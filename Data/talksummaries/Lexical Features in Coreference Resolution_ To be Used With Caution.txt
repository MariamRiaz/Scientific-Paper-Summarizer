0	7	Similar to many other tasks, lexical features are a major source of information in current coreference resolvers.
1	41	Coreference resolution is a set partitioning problem in which each resulting partition refers to an entity.
4	10	The introduction of the CoNLL dataset enabled a significant boost in the performance of coreference resolvers, i.e. about 10 percent difference between the CoNLL score of the currently best coreference resolver, deep-coref by Clark and Manning (2016b), and the winner of the CoNLL 2011 shared task, the Stanford rule-based system by Lee et al. (2013).
20	12	Clark and Manning (2016b) capture the required information for resolving coreference relations by using a large number of lexical features and a small set of nonlexical features including string match, distance, mention type, speaker and genre features.
21	10	The main difference is that Clark and Manning (2016b) use word embeddings instead of the exact surface forms that are used by Durrett and Klein (2013).
25	16	Aside from the evident success of lexical features, it is debatable how well the knowledge that is mainly captured by the lexical information of the training data can be generalized to other domains.
26	9	As reported by Ghaddar and Langlais (2016b), state-of-the-art coreference resolvers trained on the CoNLL dataset perform poorly, i.e. worse than the rule-based system (Lee et al., 2013), on the new dataset, WikiCoref (Ghaddar and Langlais, 2016b), even though WikiCoref is annotated with the same annotation guidelines as the CoNLL dataset.
28	36	The results are reported using MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), CEAFe (Luo, 2005), the average F1 score of these three metrics, i.e. CoNLL score, and LEA (Moosavi and Strube, 2016).
29	33	berkeley is the mention-ranking model of Durrett and Klein (2013) with the FINAL feature set including the head, first, last, preceding and following words of a mention, the ancestry, length, gender and number of a mention, distance of two mentions, whether the anaphor and antecedent are nested, same speaker and a small set of string match features.
35	9	deep-coref also incorporates type, length, and position of a mention, whether the mention is nested in any other mention, distance of two mentions, speaker features and a small set of string match features.
45	12	In this section, we investigate how much lexical features contribute to the fact that current improvements in coreference resolution do not properly apply to a new domain.
46	67	Table 2 shows the ratio of non-pronominal coreferent mentions in the CoNLL test set that also appear as coreferent mentions in the training data.
47	14	These high ratios indicate a high degree of overlap between the mentions of the CoNLL datasets.
52	47	Table 3 shows the results of the examined coreference resolvers when the test set only includes one genre, i.e. pt or wb, in two different settings: (1) the training set includes all genres (in-domain evaluation), and (2) the corresponding genre of the test set is excluded from the training and development sets (out-of-domain evaluation).
55	9	For in-domain evaluations we train deep-coref ’s ranking model for 100 iterations, i.e. the setting used by Clark and Manning (2016a).
56	10	However, based on the performance on the development set, we only train the model for 50 iterations in out-ofdomain evaluations.
64	16	Interestingly, the performance of berkeleyfinal, cort and cort−lexical increases for the wb genre when this genre is excluded from the training set.
65	20	deep-coref, which uses a complex deep neural network and mainly lexical features, has the highest gain from the redundancy in the training and test datasets.
68	10	The classifier may also memorize other properties of the seen mentions in the training data.
72	15	The seen rows show the ratio of each category of links for which the (antecedent head, anaphor head) pair is seen in the training set.
76	25	The results of Table 5 show that most of the incorrect links are also made between the mentions that are both seen in the training data.
78	19	We analyze the links that are created by Stanford’s rule-based system and compute the ratio of the links that exist in the training set.
83	9	We also compute the ratios of Table 5 for the missing links that are associated with the recall er- rors of deep-coref.
97	36	The effect of lexical features is also analyzed by Levy et al. (2015) for tasks like hypernymy and entailment.
98	33	They show that state-of-the-art classifiers memorize words from the training data.
100	69	We show the extensive use of lexical features biases coreference resolvers towards seen mentions.
103	228	Coreference resolvers are going to be used in tasks and domains for which coreference annotated corpora may not be available.
104	26	Therefore, generalizability should be brought into attention in developing coreference resolvers.
105	39	Moreover, we show that there is a significant overlap between the training and validation sets in the CoNLL dataset.
106	27	The LEA metric (Moosavi and Strube, 2016) is introduced as an attempt to make coreference evaluations more reliable.
