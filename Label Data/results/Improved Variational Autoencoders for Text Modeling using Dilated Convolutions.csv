0,1,label2,summary_sentences
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1829–1838 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1829",text,[0],[0]
"Networks are ubiquitous, with prominent examples including social networks (e.g., Facebook, Twitter) or citation networks of research papers (e.g., arXiv).",1 Introduction,[0],[0]
"When analyzing data from these real-world networks, traditional methods often represent vertices (nodes) as one-hot representations (containing the connectivity information of each vertex with respect to all other vertices), usually suffering from issues related to the inherent sparsity of large-scale networks.",1 Introduction,[0],[0]
"This results in models that are not able to fully capture the relationships between vertices of the network (Perozzi et al., 2014; Tu et al., 2016).",1 Introduction,[0],[0]
"Alternatively, network embedding (i.e., network representation learning) has been considered, representing each vertex of a network with a low-dimensional vector that preserves information on its similarity rel-
ative to other vertices.",1 Introduction,[0],[0]
"This approach has attracted considerable attention in recent years (Tang and Liu, 2009; Perozzi et al., 2014; Tang et al., 2015; Grover and Leskovec, 2016; Wang et al., 2016; Chen et al., 2016; Wang et al., 2017a; Zhang et al., 2018).
",1 Introduction,[0],[0]
"Traditional network embedding approaches focus primarily on learning representations of vertices that preserve local structure, as well as internal structural properties of the network.",1 Introduction,[0],[0]
"For instance, Isomap (Tenenbaum et al., 2000), LINE (Tang et al., 2015), and Grarep (Cao et al., 2015) were proposed to preserve first-, second-, and higher-order proximity between nodes, respectively.",1 Introduction,[0],[0]
"DeepWalk (Perozzi et al., 2014), which learns vertex representations from random-walk sequences, similarly, only takes into account structural information of the network.",1 Introduction,[0],[0]
"However, in realworld networks, vertices usually contain rich textual information (e.g., user profiles in Facebook, paper abstracts in arXiv, user-generated content on Twitter, etc.), which may be leveraged effectively for learning more informative embeddings.
",1 Introduction,[0],[0]
"To address this opportunity, Yang et al. (2015) proposed text-associated DeepWalk, to incorporate textual information into the vectorial representations of vertices (embeddings).",1 Introduction,[0],[0]
"Sun et al. (2016) employed deep recurrent neural networks to integrate the information from vertex-
associated text into network representations.",1 Introduction,[0],[0]
"Further, Tu et al. (2017) proposed to more effectively model the semantic relationships between vertices using a mutual attention mechanism.
",1 Introduction,[0],[0]
"Although these methods have demonstrated performance gains over structure-only network embeddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings.",1 Introduction,[0],[0]
"However, as shown in Figure 1, to assess the similarity between two research papers, a more effective strategy would compare and align (via localweighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (downweighted).",1 Introduction,[0],[0]
"This alignment mechanism is difficult to accomplish in models where text sequences are first embedded into a common space and then compared in pairs (He and Lin, 2016; Parikh et al., 2016; Wang and Jiang, 2017; Wang et al., 2017b; Shen et al., 2018a).
",1 Introduction,[0],[0]
We propose to learn a semantic-aware Network Embedding (NE) that incorporates wordlevel alignment features abstracted from text sequences associated with vertex pairs.,1 Introduction,[0],[0]
"Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors.",1 Introduction,[0],[0]
"These features are then accumulated via a simple but efficient aggregation function, obtaining the final representation for the sentence.",1 Introduction,[0],[0]
"As a result, the word-by-word alignment features (as illustrated in Figure 1) are explicitly and effectively captured by our model.",1 Introduction,[0],[0]
"Further, the learned network embeddings under our framework are adaptive to the specific (local) vertices that are considered, and thus are context-aware and especially suitable for downstream tasks, such as link prediction.",1 Introduction,[0],[0]
"Moreover, since the word-by-word matching procedure introduced here is highly parallelizable and does not require any complex encoding networks, such as Long Short-Term Memory (LSTM) or Convolutional Neural Networks (CNNs), our framework requires significantly less time for training, which is attractive for large-scale network applications.
",1 Introduction,[0],[0]
"We evaluate our approach on three real-world datasets spanning distinct network-embeddingbased applications: link prediction, vertex classi-
fication and visualization.",1 Introduction,[0],[0]
"We show that the proposed word-by-word alignment mechanism efficiently incorporates textual information into the network embedding, and consistently exhibits superior performance relative to several competitive baselines.",1 Introduction,[0],[0]
Analyses considering the extracted word-by-word pairs further validate the effectiveness of the proposed framework.,1 Introduction,[0],[0]
"A network (graph) is defined as G = {V ,E}, where V and E denote the set of N vertices (nodes) and edges, respectively, where elements of E are two-element subsets of V .",2.1 Problem Definition,[0],[0]
"Here we only consider undirected networks, however, our approach (introduced below) can be readily extended to the directed case.",2.1 Problem Definition,[0],[0]
"We also define W , the symmetric RN×N matrix whose elements,wij , denote the weights associated with edges in V , and T , the set of text sequences assigned to each vertex.",2.1 Problem Definition,[0],[0]
"Edges and weights contain the structural information of the network, while the text can be used to characterize the semantic properties of each vertex.",2.1 Problem Definition,[0],[0]
"Given network G, with the network embedding we seek to encode each vertex into a low-dimensional vector h (with dimension much smaller than N ), while preserving structural and semantic features of G.",2.1 Problem Definition,[0],[0]
"To incorporate both structural and semantic information into the network embeddings, we specify two types of (latent) embeddings: (i) hs, the structural embedding; and (ii) ht, the textual embedding.",2.2 Framework Overview,[0],[0]
"Specifically, each vertex in G is encoded into a low-dimensional embedding h =",2.2 Framework Overview,[0],[0]
[hs;ht].,2.2 Framework Overview,[0],[0]
"To learn these embeddings, we specify an objective that leverages the information from both W and T , denoted as
L = ∑ e∈E Lstruct(e) + Ltext(e) + Ljoint(e) , (1)
where Lstruct, Ltext and Ljoint denote structure, text, and joint structure-text training losses, respectively.",2.2 Framework Overview,[0],[0]
"For a vertex pair {vi, vj} weighted by wij , Lstruct(vi, vj) in (1) is defined as (Tang et al., 2015)
Lstruct(vi, vj) = wij log p(his|hjs) , (2)
where p(his|hjs) denotes the conditional probability between structural embeddings for vertices {vi, vj}.",2.2 Framework Overview,[0],[0]
"To leverage the textual information in T , similar text-specific and joint structure-text training objectives are also defined
Ltext(vi, vj) = wijα1 log p(hit|h j t ) , (3)
Ljoint(vi, vj) =",2.2 Framework Overview,[0],[0]
wijα2 log p(hit|hjs) (4) + wijα3 log p(h,2.2 Framework Overview,[0],[0]
"i s|h j t ) , (5)
where p(hit|h j t ) and p(h i t|hjs) (or p(his|h j t ))",2.2 Framework Overview,[0],[0]
"denote the conditional probability for a pair of text embeddings and text embedding given structure embedding (or vice versa), respectively, for vertices {vi, vj}.",2.2 Framework Overview,[0],[0]
"Further, α1, α2 and α3 are hyperparameters that balance the impact of the different training-loss components.",2.2 Framework Overview,[0],[0]
"Note that structural embeddings, hs, are treated directly as parameters, while the text embeddings ht are learned based on the text sequences associated with vertices.
",2.2 Framework Overview,[0],[0]
"For all conditional probability terms, we follow Tang et al. (2015) and consider the second-order proximity between vertex pairs.",2.2 Framework Overview,[0],[0]
"Thus, for vertices {vi, vj}, the probability of generating hi conditioned on hj may be written as
p(hi|hj) = exp
( hj T hi )
∑N k=1 exp ( hj T hk ) .",2.2 Framework Overview,[0],[0]
"(6)
Note that (6) can be applied to both structural and text embeddings in (2) and (3).
",2.2 Framework Overview,[0],[0]
"Inspired by Tu et al. (2017), we further assume that vertices in the network play different roles depending on the vertex with which they interact.",2.2 Framework Overview,[0],[0]
"Thus, for a given vertex, the text embedding, ht, is adaptive (specific) to the vertex it is being conditioned on.",2.2 Framework Overview,[0],[0]
"This type of contextaware textual embedding has demonstrated superior performance relative to context-free embeddings (Tu et al., 2017).",2.2 Framework Overview,[0],[0]
"In the following two sections, we describe our strategy for encoding the text sequence associated with an edge into its adaptive textual embedding, via word-by-context and word-by-word alignments.",2.2 Framework Overview,[0],[0]
"We first introduce our base model, which reweights the importance of individual words within a text sequence in the context of the edge being considered.",2.3 Word-by-Context Alignment,[0],[0]
"Consider text sequences associated with two vertices connected by an edge, de-
noted ta and tb and contained in T .",2.3 Word-by-Context Alignment,[0],[0]
"Text sequences ta and tb are of lengths Ma and Mb, respectively, and are represented by Xa ∈ Rd×Ma",2.3 Word-by-Context Alignment,[0],[0]
"and Xb ∈ Rd×Mb , respectively, where d is the dimension of the word embedding.",2.3 Word-by-Context Alignment,[0],[0]
"Further, x(i)a denotes the embedding of the i-th word in sequence ta.
",2.3 Word-by-Context Alignment,[0],[0]
Our goal is to encode text sequences ta and tb into counterpart-aware vectorial representations ha and hb.,2.3 Word-by-Context Alignment,[0],[0]
"Thus, while inferring the adaptive textual embedding for sentence ta, we propose reweighting the importance of each word in ta to explicitly account for its alignment with sentence tb.",2.3 Word-by-Context Alignment,[0],[0]
"The weight αi, corresponding to the i-th word in ta, is generated as:
αi = exp(tanh(W1cb",2.3 Word-by-Context Alignment,[0],[0]
+W2x (i) a )),2.3 Word-by-Context Alignment,[0],[0]
"∑Ma
j=1 exp(tanh(W1cb",2.3 Word-by-Context Alignment,[0],[0]
+,2.3 Word-by-Context Alignment,[0],[0]
"W2x (j) a ))
, (7)
where W1 and W2 are model parameters and cb = ∑Mb i=1",2.3 Word-by-Context Alignment,[0],[0]
"x b i is the context vector of sequence tb, obtained by simply averaging over all the word embeddings in the sequence, similar to fastText (Joulin et al., 2016).",2.3 Word-by-Context Alignment,[0],[0]
"Further, the word-by-context embedding for sequence ta is obtained by taking the weighted average over all word embeddings
ha = ∑Ma i=1αix (i) a .",2.3 Word-by-Context Alignment,[0],[0]
"(8)
Intuitively, αi may be understood as the relevance score between the ith word in ta and sequence tb.",2.3 Word-by-Context Alignment,[0],[0]
"Specifically, keywords within ta, in the context of tb, should be assigned larger weights, while less important words will be correspondingly downweighted.",2.3 Word-by-Context Alignment,[0],[0]
"Similarly, hb is encoded as a weighted embedding using (7) and (8).",2.3 Word-by-Context Alignment,[0],[0]
"With the alignment in the previous section, wordby-context matching features αi are modeled; however, the word-by-word alignment information (fine-grained), which is key to characterize the relationship between two vertices (as discussed in the above), is not explicitly captured.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"So motivated, we further propose an architecture to explicitly abstract word-by-word alignment information from ta and tb, to learn the relationship between the two vertices.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"This is inspired by the recent success of Relation Networks (RNs) for relational reasoning (Santoro et al., 2017).
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"As illustrated in Figure 2, given two input embedding matrices Xa and Xb, we first compute the affinity matrix A ∈ RMb×Ma , whose elements represent the affinity scores corresponding to all word pairs between sequences ta and tb
A = XTb Xa .",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"(9)
Subsequently, we compute the context-aware matrix for sequence tb as
Ab = softmax(A) , X̃b = XbAb , (10)
where the softmax(·) function is applied columnwise to A, and thus Ab contains the attention weights (importance scores) across sequence tb (columns), which account for each word in sequence ta (rows).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Thus, matrix X̃b ∈ Rd×Ma in (10) constitutes an attention-weighted embedding for Xb.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Specifically, the i-th column of X̃b, denoted as x̃(i)b , can be understood as a weighted average over all the words in tb, where higher attention weights indicate better alignment (match) with the i-th word in ta.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"To abstract the word-by-word alignments, we compare x(i)a with x̃ (i) b , for i = 1, 2, ...,Ma, to obtain the corresponding matching vector
m(i)a = falign ( x(i)a , x̃ (i) b ) , (11)
where falign(·) represents the alignment function.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Inspired by the observation in Wang and Jiang (2017) that simple comparison/alignment functions based on element-wise operations exhibit excellent performance in matching text sequences, here we use a combination of element-wise subtraction and multiplication as
falign(x (i) a , x̃ (i) a ) =",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"[x (i) a − x̃ (i) a ;x (i) a x̃ (i) a ] ,
where denotes the element-wise Hadamard product, then these two operations are concatenated to produce the matching vector m(i)a .",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Note these operators may be used individually or combined as we will investigate in our experiments.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Subsequently, matching vectors from (11) are aggregated to produce the final textual embedding hat for sequence ta as
hat = faggregate ( m(1)a ,m (2) a , ...,m (Ma) a ) , (12)
where faggregate denotes the aggregation function, which we specify as the max-pooling pooling operation.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Notably, other commutative operators, such as summation or average pooling, can be otherwise employed.",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Although these aggregation functions are simple and invariant to the order of words in input sentences, they have been demonstrated to be highly effective in relational reasoning (Parikh et al., 2016; Santoro et al., 2017).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"To further explore this, in Section 5.3, we conduct an ablation study comparing different choices of alignment and aggregation functions.
",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"The representation hb can be obtained in a similar manner through (9), (10), (11) and (12), but replacing (9) with A = XTaXb (its transpose).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"Note that this word-by-word alignment is more computationally involved than word-by-context; however, the former has substantially fewer parameters to learn, provided we no longer have to estimate the parameters in (7).",2.4 Fine-Grained Word-by-Word Alignment,[0],[0]
"For large-scale networks, computing and optimizing the conditional probabilities in (1) using (6) is computationally prohibitive, since it requires the summation over all vertices V in G. To address this limitation, we leverage the negative sampling strategy introduced by Mikolov et al. (2013), i.e., we perform computations by sampling a subset of negative edges.",2.5 Training and Inference,[0],[0]
"As a result, the conditional in (6) can be rewritten as:
p(hi|hj) = log σ",2.5 Training and Inference,[0],[0]
"( hj T hi )
+ K∑ i=1",2.5 Training and Inference,[0],[0]
Ehi∼P,2.5 Training and Inference,[0],[0]
"(v) [ log σ ( −hjThi )] ,
where σ(x) = 1/(1 + exp(−x)) is the sigmoid function.",2.5 Training and Inference,[0],[0]
"Following Mikolov et al. (2013), we set the noise distribution P (v) ∝",2.5 Training and Inference,[0],[0]
"d3/4v , where dv is the out-degree of vertex v ∈ V .",2.5 Training and Inference,[0],[0]
The number of negative samples K is treated as a hyperparameter.,2.5 Training and Inference,[0],[0]
"We
use Adam (Kingma and Ba, 2014) to update the model parameters while minimizing the objective in (1).",2.5 Training and Inference,[0],[0]
"Network embedding methods can be divided into two categories: (i) methods that solely rely on the structure, e.g., vertex information; and (ii) methods that leverage both the structure the network and the information associated with its vertices.
",3 Related Work,[0],[0]
"For the first type of models, DeepWalk (Perozzi et al., 2014) has been proposed to learn node representations by generating node contexts via truncated random walks; it is similar to the concept of Skip-Gram (Mikolov et al., 2013), originally introduced for learning word embeddings.",3 Related Work,[0],[0]
"LINE (Tang et al., 2015) proposed a principled objective to explicitly capture first-order and second-order proximity information from the vertices of a network.",3 Related Work,[0],[0]
"Further, Grover and Leskovec (2016) introduced a biased random walk procedure to generate the neighborhood for a vertex, which infers the node representations by maximizing the likelihood of preserving the local context information of vertices.",3 Related Work,[0],[0]
"However, these algorithms generally ignore rich heterogeneous information associated with vertices.",3 Related Work,[0],[0]
"Here, we focus on incorporating textual information into network embeddings.
",3 Related Work,[0],[0]
"To learn semantic-aware network embeddings, Text-Associated DeepWalk (TADW) (Yang et al., 2015) proposed to integrate textual features into network representations with matrix factorization, by leveraging the equivalence between DeepWalk and matrix factorization.",3 Related Work,[0],[0]
"CENE (ContentEnhanced Network Embedding) (Sun et al., 2016) used bidirectional recurrent neural networks to abstract the semantic information associated with vertices, which further demonstrated the advantages of employing textual information.",3 Related Work,[0],[0]
"To capture the interaction between sentences of vertex pairs, Tu et al. (2017) further proposed ContextAware Network Embedding (CANE), that employs a mutual attention mechanism to adaptively account for the textual information from neighboring vertices.",3 Related Work,[0],[0]
"Despite showing improvement over structure-only models, these semantic-aware methods cannot capture word-level alignment information, which is important for inferring the relationship between node pairs, as previously discussed.",3 Related Work,[0],[0]
"In this work, we introduce a WordAlignment-based Network Embedding (WANE)
framework, which aligns and aggregates word-byword matching features in an explicit manner, to obtain more informative network representations.",3 Related Work,[0],[0]
"Datasets We investigate the effectiveness of the proposed WANE model on two standard networkembedding-based tasks, i.e., link prediction and multi-label vertex classification.",4 Experimental Setup,[0],[0]
"The following three real-world datasets are employed for quantitative evaluation: (i) Cora, a standard paper citation network that contains 2,277 machine learning papers (vertices) grouped into 7 categories and connected by 5,214 citations (edges) (ii) HepTh, another citation network of 1,038 papers with abstract information and 1,990 citations; (iii) Zhihu, a network of 10,000 active users from Zhihu, the largest Q&A website in China, where 43,894 vertices and descriptions of the Q&A topics are available.",4 Experimental Setup,[0],[0]
"The average lengths of the text in the three datasets are 90, 54, and 190, respectively.",4 Experimental Setup,[0],[0]
"To make direct comparison with existing work, we employed the same preprocessing procedure1 of Tu et al. (2017).
",4 Experimental Setup,[0],[0]
"Training Details For fair comparison with CANE (Tu et al., 2017), we set the dimension of network embedding for our model to 200.",4 Experimental Setup,[0],[0]
"The number of negative samples K is selected from {1, 3, 5} according to performance on the validation set.",4 Experimental Setup,[0],[0]
"We set the batch size as 128, and the model is trained using Adam (Kingma and Ba, 2014), with a learning rate of 1× 10−3 for all parameters.",4 Experimental Setup,[0],[0]
"Dropout regularization is employed on the word embedding layer, with rate selected from {0.5, 0.7, 0.9}, also on the validation set.",4 Experimental Setup,[0],[0]
"Our code will be released to encourage future research.
",4 Experimental Setup,[0],[0]
"Baselines To evaluate the effectiveness of our framework, we consider several strong baseline methods for comparisons, which can be categorized into two types: (i) models that only exploit structural information: MMB (Airoldi et al., 2008), DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015), and node2vec (Grover and Leskovec, 2016).",4 Experimental Setup,[0],[0]
"(ii) Models that take both structural and textual information into account: Naive combination (which simply concatenates the structure-based embedding with CNN-based text embeddings, as explored in (Tu et al., 2017), TADW (Yang et al., 2015), CENE (Sun et al.,
1https://github.com/thunlp/CANE
2016), and CANE (Tu et al., 2017).",4 Experimental Setup,[0],[0]
"It is worth noting that unlike all these baselines, WANE explicitly captures word-by-word interactions between text sequence pairs.
",4 Experimental Setup,[0],[0]
Evaluation Metrics,4 Experimental Setup,[0],[0]
"We employ AUC (Hanley and McNeil, 1982) as the evaluation metric for link prediction, which measures the probability that vertices within an existing edge, randomly sampled from the test set, are more similar than those from a random pair of non-existing vertices, in terms of the inner product between their corresponding embeddings.
",4 Experimental Setup,[0],[0]
"For multi-label vertex classification and to ensure fair comparison, we follow Yang et al. (2015) and employ a linear SVM on top of the learned network representations, and evaluate classification accuracy with different training ratios (varying from 10% to 50%).",4 Experimental Setup,[0],[0]
The experiments for each setting are repeated 10 times and the average test accuracy is reported.,4 Experimental Setup,[0],[0]
"We experiment with three variants for our WANE model: (i) WANE: where the word embeddings of each text sequence are simply average to obtain the sentence representations, similar to (Joulin et al., 2016; Shen et al., 2018c).",5 Experimental Results,[0],[0]
"(ii) WANE-
wc: where the textual embeddings are inferred with word-by-context alignment.",5 Experimental Results,[0],[0]
(iii) WANE-ww: where the word-by-word alignment mechanism is leveraged to capture word-by-word matching features between available sequence pairs.,5 Experimental Results,[0],[0]
"Table 1 presents link prediction results for all models on Cora dataset, where different ratios of edges are used for training.",5.1 Link Prediction,[0],[0]
"It can be observed that when only a small number of edges are available, e.g., 15%, the performances of structure-only methods is much worse than semantic-aware models that have taken textual information into consideration The perfromance gap tends to be smaller when a larger proportion of edges are employed for training.",5.1 Link Prediction,[0],[0]
"This highlights the importance of incorporating associated text sequences into network embeddings, especially in the case of representing a relatively sparse network.",5.1 Link Prediction,[0],[0]
"More importantly, the proposed WANE-ww model consistently outperforms other semantic-aware NE models by a substantial margin, indicating that our model better abstracts word-by-word alignment features from the text sequences available, thus yields more informative network representations.
",5.1 Link Prediction,[0],[0]
"Further, WANE-ww also outperforms WANE or WANE-wc on a wide range of edge training pro-
portions.",5.1 Link Prediction,[0],[0]
This suggests that: (i) adaptively assigning different weights to each word within a text sequence (according to its paired sequence) tends to be a better strategy than treating each word equally (as in WANE).,5.1 Link Prediction,[0],[0]
(ii) Solely considering the context-by-word alignment features (as in WANE-wc) is not as efficient as abstracting word-by-word matching information from text sequences.,5.1 Link Prediction,[0],[0]
"We observe the same trend and the superiority of our WANE-ww models on the other two datasets, HepTh and Zhihu datasets, as shown in Table 2 and 3, respectively.",5.1 Link Prediction,[0],[0]
We further evaluate the effectiveness of proposed framework on vertex classification tasks with the Cora dataset.,5.2 Multi-label Vertex Classification,[0],[0]
"Similar to Tu et al. (2017), we generate the global embedding for each vertex by taking the average over its context-aware embeddings with all other connected vertices.",5.2 Multi-label Vertex Classification,[0],[0]
"As shown in Figure 3(c), semantic-aware NE methods (including naive combination, TADW, CENE, CANE) exhibit higher test accuracies than semantic-agnostic models, demonstrating the advantages of incorporating textual information.",5.2 Multi-label Vertex Classification,[0],[0]
"Moreover, WANEww consistently outperforms other competitive semantic-aware models on a wide range of labeled proportions, suggesting that explicitly capturing word-by-word alignment features is not only use-
ful for vertex-pair-based tasks, such as link prediction, but also results in better global embeddings which are required for vertex classification tasks.",5.2 Multi-label Vertex Classification,[0],[0]
"These observations further demonstrate that WANE-ww is an effective and robust framework to extract informative network representations.
",5.2 Multi-label Vertex Classification,[0],[0]
"Semi-supervised classification We further consider the case where the training ratio is less than 10%, and evaluate the learned network embedding with a semi-supervised classifier.",5.2 Multi-label Vertex Classification,[0],[0]
"Following Yang et al. (2015), we employ a Transductive SVM (TSVM) classifier with a linear kernel (Joachims, 1998) for fairness.",5.2 Multi-label Vertex Classification,[0],[0]
"As illustrated in Table 4, the proposed WANE-ww model exhibits superior performances in most cases.",5.2 Multi-label Vertex Classification,[0],[0]
"This may be due to the fact that WANE-ww extracts information from the vertices and text sequences jointly, thus the obtained vertex embeddings are less noisy and perform more consistently with relatively small training ratios (Yang et al., 2015).",5.2 Multi-label Vertex Classification,[0],[0]
"Motivated by the observation in Wang and Jiang (2017) that the advantages of different functions to match two vectors vary from task to task, we further explore the choice of alignment and aggregation functions in our WANE-ww model.",5.3 Ablation Study,[0],[0]
"To match the word pairs between two sequences, we experimented with three types of operations: sub-
traction, multiplication, and Sub & Multi (the concatenation of both approaches).",5.3 Ablation Study,[0],[0]
"As shown in Figure 3(a) and 3(b), element-wise subtraction tends to be the most effective operation performancewise on both Cora and Zhihu datasets, and performs comparably to Sub & Multi on the HepTh dataset.",5.3 Ablation Study,[0],[0]
"This finding is consistent with the results in Wang and Jiang (2017), where they found that simple comparison functions based on elementwise operations work very well on matching text sequences.
",5.3 Ablation Study,[0],[0]
"In terms of the aggregation functions, we compare (one-layer) CNN, mean-pooling, and maxpooling operations to accumulate the matching vectors.",5.3 Ablation Study,[0],[0]
"As shown in Figure 3(b), max-pooling has the best empirical results on all three datasets.",5.3 Ablation Study,[0],[0]
"This may be attributed to the fact that the maxpooling operation is better at selecting important word-by-word alignment features, among all matching vectors available, to infer the relationship between vertices.",5.3 Ablation Study,[0],[0]
"Embedding visualization To visualize the learned network representations, we further employ t-SNE to map the low-dimensional vectors of the vertices to a 2-D embedding space.",5.4 Qualitative Analysis,[0],[0]
"We use the Cora dataset because there are labels associated with each vertex and WANE-ww to obtain the network embeddings.
",5.4 Qualitative Analysis,[0],[0]
"As shown in Figure 4 where each point indicates one paper (vertex), and the color of each point indicates the category it belongs to, the embeddings of the same label are indeed very close in the 2-D plot, while those with different labels are relatively farther from each other.",5.4 Qualitative Analysis,[0],[0]
"Note that the model is not trained with any label information, indicating that WANE-ww has extracted meaningful patterns from the text and vertex information available.
",5.4 Qualitative Analysis,[0],[0]
Case study The proposed word-by-word alignment mechanism can be used to highlight the most informative words (and the corresponding matching features) wrt the relationship between vertices.,5.4 Qualitative Analysis,[0],[0]
We visualize the norm of matching vector obtained in (11) in Figure 5 for the Cora dataset.,5.4 Qualitative Analysis,[0],[0]
"It can be observed that matched key words, e.g., ‘MCMC’, ‘convergence’, between the text sequences are indeed assigned higher values in the matching vectors.",5.4 Qualitative Analysis,[0],[0]
These words would be selected preferentially by the final max-pooling aggregation operation.,5.4 Qualitative Analysis,[0],[0]
This indicates that WANEww is able to abstract important word-by-word alignment features from paired text sequences.,5.4 Qualitative Analysis,[0],[0]
We have presented a novel framework to incorporate the semantic information from vertexassociated text sequences into network embeddings.,6 Conclusions,[0],[0]
"An align-aggregate framework is introduced, which first aligns a sentence pair by capturing the word-by-word matching features, and then adaptively aggregating these word-level alignment
information with an efficient max-pooling function.",6 Conclusions,[0],[0]
"The semantic features abstracted are further encoded, along with the structural information, into a shared space to obtain the final network embedding.",6 Conclusions,[0],[0]
Compelling experimental results on several tasks demonstrated the advantages of our approach.,6 Conclusions,[0],[0]
"In future work, we aim to leverage abundant unlabeled text data to abstract more informative sentence representations (Dai and Le, 2015; Zhang et al., 2017; Shen et al., 2017; Tang and de Sa, 2018) .",6 Conclusions,[0],[0]
"Another interesting direction is to learn binary and compact network embedding, which could be more efficient in terms of both computation and memory, relative to its continuous counterpart (Shen et al., 2018b).
",6 Conclusions,[0],[0]
"Acknowledgments This research was supported in part by DARPA, DOE, NIH, ONR and NSF.",6 Conclusions,[0],[0]
"Network embeddings, which learn lowdimensional representations for each vertex in a large-scale network, have received considerable attention in recent years.",abstractText,[0],[0]
"For a wide range of applications, vertices in a network are typically accompanied by rich textual information such as user profiles, paper abstracts, etc.",abstractText,[0],[0]
We propose to incorporate semantic features into network embeddings by matching important words between text sequences for all pairs of vertices.,abstractText,[0],[0]
"We introduce a word-by-word alignment framework that measures the compatibility of embeddings between word pairs, and then adaptively accumulates these alignment features with a simple yet effective aggregation function.",abstractText,[0],[0]
"In experiments, we evaluate the proposed framework on three real-world benchmarks for downstream tasks, including link prediction and multi-label vertex classification.",abstractText,[0],[0]
Results demonstrate that our model outperforms state-of-the-art network embedding methods by a large margin.,abstractText,[0],[0]
Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment,title,[0],[0]
At the heart of natural language parsing is the challenge of representing the “state” of an algorithm— what parts of a parse have been built and what parts of the input string are not yet accounted for— as it incrementally constructs a parse.,1 Introduction,[0],[0]
"Traditional approaches rely on independence assumptions, decomposition of scoring functions, and/or greedy approximations to keep this space manageable.",1 Introduction,[0],[0]
"Continuous-state parsers have been proposed, in which the state is embedded as a vector (Titov and Henderson, 2007; Stenetorp, 2013; Chen and Manning, 2014; Dyer et al., 2015; Zhou et al., 2015; Weiss et al., 2015).",1 Introduction,[0],[0]
"Dyer et al. reported state-of-the-art performance on English and Chinese benchmarks using a transition-based parser whose continuous-state embeddings were constructed using LSTM recurrent neural networks (RNNs) whose parameters were estimated to maximize the probability of a gold-standard sequence of parse actions.
",1 Introduction,[0],[0]
"The primary contribution made in this work is to take the idea of continuous-state parsing a step further by making the word embeddings that are used to construct the parse state sensitive to the morphology of the words.1 Since it it is well known that a word’s form often provides strong evidence regarding its grammatical role in morphologically rich languages (Ballesteros, 2013, inter alia), this has promise to improve accuracy and statistical efficiency relative to traditional approaches that treat each word type as opaque and independently modeled.",1 Introduction,[0],[0]
"In the traditional parameterization, words with similar grammatical roles will only be embedded near each other if they are observed in similar contexts with sufficient frequency.",1 Introduction,[0],[0]
"Our approach reparameterizes word embeddings using the same RNN machinery used in the parser: a word’s vector is calculated based on the sequence of orthographic symbols representing it (§3).
",1 Introduction,[0],[0]
"Although our model is provided no supervision in the form of explicit morphological annotation, we find that it gives a large performance increase when parsing morphologically rich languages in the SPMRL datasets (Seddah et al., 2013; Seddah and Tsarfaty, 2014), especially in agglutinative languages and the ones that present extensive case systems (§4).",1 Introduction,[0],[0]
"In languages that show little morphology, performance remains good, showing that the RNN composition strategy is capable of capturing both morphological regularities and arbitrariness in the sense of Saussure (1916).",1 Introduction,[0],[0]
"Finally, a particularly noteworthy result is that we find that character-based word embeddings in some cases obviate explicit POS information, which is usually found to be indispensable for accurate parsing.
",1 Introduction,[0],[0]
A secondary contribution of this work is to show that the continuous-state parser of Dyer et al. (2015) can learn to generate nonprojective trees.,1 Introduction,[0],[0]
"We do this by augmenting its transition operations
1Software for replicating the experiments is available from https://github.com/clab/lstm-parser.
",1 Introduction,[0],[0]
"ar X
iv :1
50 8.
00 65
7v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
1 A
ug 2
01 5
with a SWAP operation (Nivre, 2009) (§2.4), enabling the parser to produce nonprojective dependencies which are often found in morphologically rich languages.",1 Introduction,[0],[0]
"We begin by reviewing the parsing approach of Dyer et al. (2015) on which our work is based.
",2 An LSTM Dependency Parser,[0],[0]
"Like most transition-based parsers, Dyer et al.’s parser can be understood as the sequential manipulation of three data structures: a buffer B initialized with the sequence of words to be parsed, a stack S containing partially-built parses, and a list A of actions previously taken by the parser.",2 An LSTM Dependency Parser,[0],[0]
"In particular, the parser implements the arc-standard parsing algorithm (Nivre, 2004).
",2 An LSTM Dependency Parser,[0],[0]
"At each time step t, a transition action is applied that alters these data structures by pushing or popping words from the stack and the buffer; the operations are listed in Figure 1.
",2 An LSTM Dependency Parser,[0],[0]
"Along with the discrete transitions above, the parser calculates a vector representation of the states of B, S, and A; at time step t these are denoted by bt, st, and at, respectively.",2 An LSTM Dependency Parser,[0],[0]
"The total parser state at t is given by
pt = max {0,W[st;bt;at] + d} (1)
where the matrix W and the vector d are learned parameters.",2 An LSTM Dependency Parser,[0],[0]
"This continuous-state representation pt is used to decide which operation to apply next, updating B, S, and A (Figure 1).
",2 An LSTM Dependency Parser,[0],[0]
"We elaborate on the design of bt, st, and at using RNNs in §2.1, on the representation of partial parses in S in §2.2, and on the parser’s decision mechanism in §2.3.",2 An LSTM Dependency Parser,[0],[0]
We discuss the inclusion of SWAP in §2.4.,2 An LSTM Dependency Parser,[0],[0]
RNNs are functions that read a sequence of vectors incrementally; at time step t the vector xt is read in and the hidden state ht computed using xt and the previous hidden state ht−1.,2.1 Stack LSTMs,[0],[0]
"In principle, this allows retaining information from time steps in the distant past, but the nonlinear “squashing” functions applied in the calcluation of each ht result in a decay of the error signal used in training with backpropagation.",2.1 Stack LSTMs,[0],[0]
"LSTMs are a variant of RNNs designed to cope with this “vanishing gradient” problem using an extra memory “cell” (Hochreiter and Schmidhuber, 1997; Graves, 2013).
",2.1 Stack LSTMs,[0],[0]
Past work explains the computation within an LSTM through the metaphors of deciding how much of the current input to pass into memory (it) or forget (ft).,2.1 Stack LSTMs,[0],[0]
"We refer interested readers to the original papers and present only the recursive equations updating the memory cell ct and hidden state ht given xt, the previous hidden state ht−1, and the memory cell ct−1:
it = σ(Wixxt +Wihht−1 +Wicct−1 + bi)
ft = 1− it ct = ft ct−1+
it tanh(Wcxxt",2.1 Stack LSTMs,[0],[0]
+Wchht−1 + bc) ot = σ(Woxxt,2.1 Stack LSTMs,[0],[0]
"+Wohht−1 +Wocct + bo)
ht = ot tanh(ct),
where σ is the component-wise logistic sigmoid function and is the component-wise (Hadamard) product.",2.1 Stack LSTMs,[0],[0]
Parameters are all represented using W and b.,2.1 Stack LSTMs,[0],[0]
"This formulation differs slightly from the classic LSTM formulation in that it makes use of “peephole connections” (Gers et al., 2002) and defines the forget gate so that it sums with the input gate to 1 (Greff et al., 2015).",2.1 Stack LSTMs,[0],[0]
"To improve the representational capacity of LSTMs (and RNNs generally), they can be stacked in “layers.”",2.1 Stack LSTMs,[0],[0]
"In these architectures, the input LSTM at higher layers at time t is the value of ht computed by the lower layer (and xt is the input at the lowest layer).
",2.1 Stack LSTMs,[0],[0]
The stack LSTM augments the left-to-right sequential model of the conventional LSTM with a stack pointer.,2.1 Stack LSTMs,[0],[0]
"As in the LSTM, new inputs are added in the right-most position, but the stack pointer indicates which LSTM cell provides ct−1 and ht−1 for the computation of the next iterate.",2.1 Stack LSTMs,[0],[0]
"Further, the stack LSTM provides a pop operation that moves the stack pointer to the previous element.",2.1 Stack LSTMs,[0],[0]
"Hence each of the parser data structures (B, S, and A) is implemented with its own stack LSTM, each with its own parameters.",2.1 Stack LSTMs,[0],[0]
"The values of bt, st, and at are the ht vectors from their respective stack LSTMs.",2.1 Stack LSTMs,[0],[0]
"Whenever a REDUCE operation is selected, two tree fragments are popped off of S and combined to form a new tree fragment, which is then popped back onto S (see Figure 1).",2.2 Composition Functions,[0],[0]
"This tree must be embedded as an input vector xt.
",2.2 Composition Functions,[0],[0]
"To do this, Dyer et al. (2015) use a recursive neural network gr (for relation r) that composes
the representations of the two subtrees popped from S (we denote these by u and v), resulting in a new vector gr(u,v) or gr(v,u), depending on the direction of attachment.",2.2 Composition Functions,[0],[0]
The resulting vector embeds the tree fragment in the same space as the words and other tree fragments.,2.2 Composition Functions,[0],[0]
"This kind of composition was thoroughly explored in prior work (Socher et al., 2011; Socher et al., 2013b; Hermann and Blunsom, 2013; Socher et al., 2013a); for details, see Dyer et al. (2015).",2.2 Composition Functions,[0],[0]
"The parser uses a probabilistic model of parser decisions at each time step t. Letting A(S,B) denote the set of allowed transitions given the stack S and buffer S (i.e., those where preconditions are met; see Figure 1), the probability of action z ∈ A(S,B) defined using a log-linear distribution:
p(z | pt) = exp
( g>z pt + qz )∑ z′∈A(S,B) exp",2.3 Predicting Parser Decisions,[0],[0]
"( g>z′pt + qz′
) (2) (where gz and qz are parameters associated with each action type z).
",2.3 Predicting Parser Decisions,[0],[0]
"Parsing proceeds by always choosing the most probable action from A(S,B).",2.3 Predicting Parser Decisions,[0],[0]
"The probabilistic definition allows parameter estimation for all of the parameters (W∗, b∗ in all three stack LSTMs, as well as W, d, g∗, and q∗) by maximizing the conditional likelihood of each correct parser decisions given the state.",2.3 Predicting Parser Decisions,[0],[0]
"Dyer et al. (2015)’s parser implemented the most basic version of the arc-standard algorithm, which is capable of producing only projective parse trees.",2.4 Adding the SWAP Operation,[0],[0]
"In order to deal with nonprojective trees, we also add the SWAP operation which allows nonprojective trees to be produced.
",2.4 Adding the SWAP Operation,[0],[0]
"The SWAP operation, first introduced by Nivre (2009), allows a transition-based parser to produce
nonprojective trees.",2.4 Adding the SWAP Operation,[0],[0]
"Here, the inclusion of the SWAP operation requires breaking the linearity of the stack by removing tokens that are not at the top of the stack.",2.4 Adding the SWAP Operation,[0],[0]
This is easily handled with the stack LSTM.,2.4 Adding the SWAP Operation,[0],[0]
"Figure 1 shows how the parser is capable of moving words from the stack (S) to the buffer (B), breaking the linear order of words.",2.4 Adding the SWAP Operation,[0],[0]
"Since a node that is swapped may have already been assigned as the head of a dependent, the buffer (B) can now also contain tree fragments.",2.4 Adding the SWAP Operation,[0],[0]
The main contribution of this paper is to change the word representations.,3 Word Representations,[0],[0]
"In this section, we present the standard word embeddings as in Dyer et al. (2015), and the improvements we made generating word embeddings designed to capture morphology based on orthographic strings.",3 Word Representations,[0],[0]
"Dyer et al.’s parser generates a word representation for each input token by concatenating two vectors: a vector representation for each word type (w) and a representation (t) of the POS tag of the token (if it is used), provided as auxiliary input to the parser.2 A linear map (V) is applied to the resulting vector and passed through a component-wise ReLU:
x = max {0,V[w; t] + b}
For out-of-vocabulary words, the parser uses an “UNK” token that is handled as a separate word during parsing time.",3.1 Baseline: Standard Word Embeddings,[0],[0]
"This mapping can be shown schematically as in Figure 2.
2Dyer et al. (2015), included a third input representation learned from a neural language model (w̃LM).",3.1 Baseline: Standard Word Embeddings,[0],[0]
"We do not include these pretrained representations in our experiments, focusing instead on character-based representations.",3.1 Baseline: Standard Word Embeddings,[0],[0]
"Following Ling et al. (2015), we compute character-based continuous-space vector embeddings of words using bidirectional LSTMs (Graves and Schmidhuber, 2005).",3.2 Character-Based Embeddings of Words,[0],[0]
"When the parser initiates the learning process and populates the buffer with all the words from the sentence, it reads the words character by character from left to right and computes a continuous-space vector embedding the character sequence, which is the h vector of the LSTM; we denote it by → w.",3.2 Character-Based Embeddings of Words,[0],[0]
"The same process is also applied in reverse (albeit with different parameters), computing a similar continuous-space vector embedding starting from the last character and finishing at the first ( ← w); again each character is represented with an LSTM cell.",3.2 Character-Based Embeddings of Words,[0],[0]
"After that, we concatenate these vectors and a (learned) representation of their tag to produce the representation w.",3.2 Character-Based Embeddings of Words,[0],[0]
"As in §3.1, a linear map (V) is applied and passed through a component-wise ReLU.
",3.2 Character-Based Embeddings of Words,[0],[0]
"x = max { 0,V",3.2 Character-Based Embeddings of Words,[0],[0]
[ → w; ← w; t] + b },3.2 Character-Based Embeddings of Words,[0],[0]
"This process is shown schematically in Figure 3.
",3.2 Character-Based Embeddings of Words,[0],[0]
"Note that under this representation, out-ofvocabulary words are treated as bidirectional LSTM encodings and thus they will be “close” to other words that the parser has seen during training, ideally close to their more frequent, syntactically similar morphological relatives.",3.2 Character-Based Embeddings of Words,[0],[0]
"We conjecture that this will give a clear advantage over a single “UNK” token for all the words that the parser does not see during training, as done by Dyer et al. (2015) and other parsers without additional resources.",3.2 Character-Based Embeddings of Words,[0],[0]
In §4 we confirm this hypothesis.,3.2 Character-Based Embeddings of Words,[0],[0]
"We applied our parsing model and several variations of it to several parsing tasks and report re-
sults below.",4 Experiments,[0],[0]
"In order to find out whether the character-based representations are capable of learning the morphology of words, we applied the parser to morphologically rich languages specifically the treebanks of the SPMRL shared task (Seddah et al., 2013; Seddah and Tsarfaty, 2014): Arabic (Maamouri et al., 2004), Basque (Aduriz et al., 2003), French (Abeillé et al., 2003), German (Seeker and Kuhn, 2012), Hebrew (Sima’an et al., 2001), Hungarian (Vincze et al., 2010), Korean (Choi, 2013), Polish (Świdziński and Woliński, 2010) and Swedish (Nivre et al., 2006b).",4.1 Data,[0],[0]
"For all the corpora of the SPMRL Shared Task we used predicted POS tags as provided by the shared task organizers.3 For these datasets, evaluation is calculated using eval07.pl, which includes punctuation.
",4.1 Data,[0],[0]
"We also experimented with the Turkish dependency treebank4 (Oflazer et al., 2003) of the CoNLL-X Shared Task (Buchholz and Marsi, 2006).",4.1 Data,[0],[0]
"We used gold POS tags, as is common with the CoNLL-X data sets.
",4.1 Data,[0],[0]
"To put our results in context with the most recent neural network transition-based parsers, we run the parser in the same Chinese and English
3The POS tags were calculated with the MarMot tagger (Müller et al., 2013) by the best performing system of the SPMRL Shared Task (Björkelund et al., 2013).",4.1 Data,[0],[0]
Arabic: 97.38.,4.1 Data,[0],[0]
Basque: 97.02.,4.1 Data,[0],[0]
French: 97.61.,4.1 Data,[0],[0]
German: 98.10.,4.1 Data,[0],[0]
Hebrew: 97.09.,4.1 Data,[0],[0]
Hungarian: 98.72.,4.1 Data,[0],[0]
Korean: 94.03.,4.1 Data,[0],[0]
Polish: 98.12.,4.1 Data,[0],[0]
"Swedish: 97.27.
4Since the Turkish dependency treebank does not have a development set, we extracted the last 150 sentences from the 4996 sentences of the training set as a development set.
setups as Chen and Manning (2014) and Dyer et al. (2015).",4.1 Data,[0],[0]
"For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5) following Zhang and Clark (2008b),5 with gold POS tags.",4.1 Data,[0],[0]
"For English, we used the Stanford Dependency (SD) representation of the Penn Treebank6 (Marcus et al., 1993; Marneffe et al., 2006).7.",4.1 Data,[0],[0]
"Results for Turkish, Chinese, and English are calculated using the CoNLL-X eval.pl script, which ignores punctuation symbols.",4.1 Data,[0],[0]
"In order to isolate the improvements provided by the LSTM encodings of characters, we run the stack LSTM parser in the following configurations:
• Words: words only, as in §3.1 (but without POS tags)
",4.2 Experimental Configurations,[0],[0]
"• Chars: character-based representations of words with bidirectional LSTMs, as in §3.2 (but without POS tags)
•",4.2 Experimental Configurations,[0],[0]
"Words + POS: words and POS tags (§3.1)
• Chars + POS: character-based representations of words with bidirectional LSTMs plus POS tags (§3.2)
",4.2 Experimental Configurations,[0],[0]
None of the experimental configurations include pretrained word-embeddings or any additional data resources.,4.2 Experimental Configurations,[0],[0]
"All experiments include the SWAP transition, meaning that nonprojective trees can be produced in any language.
Dimensionality.",4.2 Experimental Configurations,[0],[0]
The full version of our parsing model sets dimensionalities as follows.,4.2 Experimental Configurations,[0],[0]
"LSTM hidden states are of size 100, and we use two layers of LSTMs for each stack.",4.2 Experimental Configurations,[0],[0]
"Embeddings of the parser actions used in the composition functions have 20 dimensions, and the output embedding size is 20 dimensions.",4.2 Experimental Configurations,[0],[0]
"The learned word representations embeddings have 32 dimensions when used, while the character-based representations have 100 dimensions, when used.",4.2 Experimental Configurations,[0],[0]
Part of speech embeddings have 12 dimensions.,4.2 Experimental Configurations,[0],[0]
"These dimensionalities were chosen after running several tests with different values, but a more careful selection of these values would probably further improve results.
",4.2 Experimental Configurations,[0],[0]
"5Training: 001–815, 1001–1136.",4.2 Experimental Configurations,[0],[0]
"Development: 886– 931, 1148–1151.",4.2 Experimental Configurations,[0],[0]
"Test: 816–885, 1137–1147.
",4.2 Experimental Configurations,[0],[0]
6Training: 02–21.,4.2 Experimental Configurations,[0],[0]
Development: 22.,4.2 Experimental Configurations,[0],[0]
Test: 23.,4.2 Experimental Configurations,[0],[0]
"7The POS tags are predicted by using the Stanford Tagger
(Toutanova et al., 2003) with an accuracy of 97.3%.",4.2 Experimental Configurations,[0],[0]
Parameters are initialized randomly—refer to Dyer et al. (2015) for specifics—and optimized using stochastic gradient descent (without minibatches) using derivatives of the negative log likelihood of the sequence of parsing actions computed using backpropagation.,4.3 Training Procedure,[0],[0]
"Training is stopped when the learned model’s UAS stops improving on the development set, and this model is used to parse the test set.",4.3 Training Procedure,[0],[0]
No pretraining of any parameters is done.,4.3 Training Procedure,[0],[0]
"Tables 1 and 2 show the results of the parsers for the development sets and the final test sets, respectively.",4.4 Results and Discussion,[0],[0]
"Most notable are improvements for agglutinative languages—Basque, Hungarian, Korean, and Turkish—both when POS tags are included and when they are not.",4.4 Results and Discussion,[0],[0]
"Consistently, across all languages, Chars outperforms Words, suggesting that the character-level LSTMs are learning representations that capture similar information to parts of speech.",4.4 Results and Discussion,[0],[0]
"On average, Chars is on par with Words + POS, and the best average of labeled attachment scores is achieved with Chars + POS.
",4.4 Results and Discussion,[0],[0]
"It is common practice to encode morphological information in treebank POS tags; for instance, the Penn Treebank includes English number and tense (e.g., NNS is plural noun and VBD is verb in past tense).",4.4 Results and Discussion,[0],[0]
"Even if our character-based representations are capable of encoding the same kind of information, existing POS tags suffice for high accuracy.",4.4 Results and Discussion,[0],[0]
"However, the POS tags in treebanks for morphologically rich languages do not seem to be enough.
",4.4 Results and Discussion,[0],[0]
"Swedish, English, and French use suffixes for the verb tenses and number,8 while Hebrew uses prepositional particles rather than grammatical case.",4.4 Results and Discussion,[0],[0]
"Tsarfaty (2006) and Cohen and Smith (2007) argued that, for Hebrew, determining the correct morphological segmentation is dependent on syntactic context.",4.4 Results and Discussion,[0],[0]
"Our approach sidesteps this step, capturing the same kind of information in the vectors, and learning it from syntactic context.",4.4 Results and Discussion,[0],[0]
"Even for Chinese, which is not morphologically rich, Chars shows a benefit over Words, perhaps by capturing regularities in syllable structure within words.
",4.4 Results and Discussion,[0],[0]
"8Tense and number features provide little improvement in a transition-based parser, compared with other features such as case, when the POS tags are included (Ballesteros, 2013).",4.4 Results and Discussion,[0],[0]
Figure 4 visualizes a sample of the characterbased bidirectional LSTMs’s learned representations (Chars).,4.4.1 Learned Word Representations,[0],[0]
"Clear clusters of past tense verbs, gerunds, and other syntactic classes are visible.",4.4.1 Learned Word Representations,[0],[0]
The colors in the figure represent the most common POS tag for each word.,4.4.1 Learned Word Representations,[0],[0]
The character-based representation for words is notably beneficial for out-of-vocabulary (OOV) words.,4.4.2 Out-of-Vocabulary Words,[0],[0]
We tested this specifically by comparing Chars to a model in which all OOVs are replaced by the string “UNK” during parsing.,4.4.2 Out-of-Vocabulary Words,[0],[0]
"This always has a negative effect on LAS (average−4.5 points,
−2.8 UAS).",4.4.2 Out-of-Vocabulary Words,[0],[0]
"Figure 5 shows how this drop varies with the development OOV rate across treebanks; most extreme is Korean, which drops 15.5 LAS.",4.4.2 Out-of-Vocabulary Words,[0],[0]
"A similar, but less pronounced pattern, was observed for models that include POS.
",4.4.2 Out-of-Vocabulary Words,[0],[0]
"Interestingly, this artificially impoverished model is still consistently better than Words for all languages (e.g., for Korean, by 4 LAS).",4.4.2 Out-of-Vocabulary Words,[0],[0]
"This implies that not all of the improvement is due to OOV words; statistical sharing across orthographically close words is beneficial, as well.",4.4.2 Out-of-Vocabulary Words,[0],[0]
"The character-based representations make the parser slower, since they require composing the character-based bidirectional LSTMs for each
word of the input sentence; however, at test time these results could be cached.",4.4.3 Computational Requirements,[0],[0]
"On average, Words parses a sentence in 44 ms, whileChars needs 130 ms.9 Training time is affected by the same cons-
9We are using a machine with 32 Intel Xeon CPU E52650 at 2.00GHz; the parser runs on a single core.
tant, needing some hours to have a competitive model.",4.4.3 Computational Requirements,[0],[0]
"In terms of memory, Words requires on average 300 MB of main memory for both training and parsing, while Chars requires 450 MB.",4.4.3 Computational Requirements,[0],[0]
Table 3 shows a comparison with state-of-theart parsers.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"We include greedy transition-based parsers that, like ours, do not apply a beam search (Zhang and Clark, 2008b) or a dynamic oracle (Goldberg and Nivre, 2013).",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For all the SPMRL languages we show the results of Ballesteros (2013), who reported results after carrying out a careful automatic morphological feature selection experiment.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For Turkish, we show the results of Nivre et al. (2006a) which also carried out a careful manual morphological feature selection.",4.4.4 Comparison with State-of-the-Art,[0],[0]
Our parser outperforms these in most cases.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"Since those systems rely on morphological features, we believe that this comparison shows even more that the character-based representations are capturing morphological information, though without explicit morphological features.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For English and Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings.
",4.4.4 Comparison with State-of-the-Art,[0],[0]
We also show the best reported results on these datasets.,4.4.4 Comparison with State-of-the-Art,[0],[0]
"For the SPMRL data sets, the best performing system of the shared task is either Björkelund et al. (2013) or Björkelund et al. (2014), which are consistently better than our sys-
tem for all languages.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"Note that the comparison is harsh to our system, which does not use unlabeled data or explicit morphological features nor any combination of different parsers.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For Turkish, we report the results of Koo et al. (2010), which only reported unlabeled attachment scores.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"For English, we report (Weiss et al., 2015) and for Chinese, we report (Dyer et al., 2015) which is Words + POS but with pretrained word embeddings.",4.4.4 Comparison with State-of-the-Art,[0],[0]
"Character-based representations have been explored in other NLP tasks; for instance, dos Santos and Zadrozny (2014) and dos Santos and Guimarães (2015)",5 Related Work,[0],[0]
"learned character-level neural representations for POS tagging and named entity recognition, getting a large error reduction in both tasks.",5 Related Work,[0],[0]
Our approach is similar to theirs.,5 Related Work,[0],[0]
Others have used character-based models as features to improve existing models.,5 Related Work,[0],[0]
"For instance, Chrupała (2014) used character-based recurrent neural networks to normalize tweets.
",5 Related Work,[0],[0]
"Botha and Blunsom (2014) show that stems, prefixes and suffixes can be used to learn useful word representations but relying on an external morphological analyzer.",5 Related Work,[0],[0]
"That is, they learn the morpheme-meaning relationship with an additive model, whereas we do not need a morphological analyzer.",5 Related Work,[0],[0]
"Similarly, Chen et al. (2015) proposed joint learning of character and word embeddings for Chinese, claiming that characters contain rich information.
",5 Related Work,[0],[0]
Methods for joint morphological disambiguation and parsing have been widely explored Tsarfaty (2006; Cohen and Smith (2007; Goldberg and Tsarfaty (2008; Goldberg and Elhadad (2011).,5 Related Work,[0],[0]
"More recently, Bohnet et al. (2013) presented an arc-standard transition-based parser that performs competitively for joint morphological tagging and dependency parsing for richly inflected languages, such as Czech, Finnish, German, Hungarian, and Russian.",5 Related Work,[0],[0]
"Our model seeks to achieve a similar benefit to parsing without explicitly reasoning about the internal structure of words.
",5 Related Work,[0],[0]
"Zhang et al. (2013) presented efforts on Chinese parsing with characters showing that Chinese can be parsed at the character level, and that Chinese word segmentation is useful for predicting the correct POS tags (Zhang and Clark, 2008a).
",5 Related Work,[0],[0]
"To the best of our knowledge, previous work has not used character-based embeddings to improve dependency parsers, as done in this paper.",5 Related Work,[0],[0]
We have presented several interesting findings.,6 Conclusion,[0],[0]
"First, we add new evidence that character-based representations are useful for NLP tasks.",6 Conclusion,[0],[0]
"In this paper, we demonstrate that they are useful for transition-based dependency parsing, since they are capable of capturing morphological information crucial for analyzing syntax.
",6 Conclusion,[0],[0]
"The improvements provided by the characterbased representations using bidirectional LSTMs are strong for agglutinative languages, such as
Basque, Hungarian, Korean, and Turkish, comparing favorably to POS tags as encoded in those languages’ currently available treebanks.",6 Conclusion,[0],[0]
"This outcome is important, since annotating morphological information for a treebank is expensive.",6 Conclusion,[0],[0]
"Our finding suggests that the best investment of annotation effort may be in dependencies, leaving morphological features to be learned implicitly from strings.
",6 Conclusion,[0],[0]
"The character-based representations are also a way of overcoming the out-of-vocabulary problem; without any additional resources, they enable the parser to substantially improve the performance when OOV rates are high.",6 Conclusion,[0],[0]
"We expect that, in conjunction with a pretraing regime, or in conjunction with distributional word embeddings, further improvements could be realized.",6 Conclusion,[0],[0]
MB was supported by the European Commission under the contract numbers FP7-ICT610411 (project MULTISENSOR) and H2020RIA-645012 (project KRISTINA).,Acknowledgments,[0],[0]
This research was supported by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533 and NSF IIS-1054319.,Acknowledgments,[0],[0]
This work was completed while NAS was at CMU.,Acknowledgments,[0],[0]
"Thanks to Joakim Nivre, Bernd Bohnet, Fei Liu and Swabha Swayamdipta for useful comments.",Acknowledgments,[0],[0]
We present extensions to a continuousstate dependency parsing method that makes it applicable to morphologically rich languages.,abstractText,[0],[0]
"Starting with a highperformance transition-based parser that uses long short-term memory (LSTM) recurrent neural networks to learn representations of the parser state, we replace lookup-based word representations with representations constructed from the orthographic representations of the words, also using LSTMs.",abstractText,[0],[0]
This allows statistical sharing across word forms that are similar on the surface.,abstractText,[0],[0]
Experiments for morphologically rich languages show that the parsing model benefits from incorporating the character-based encodings of words.,abstractText,[0],[0]
Improved Transition-Based Parsing by Modeling Characters instead of Words with LSTMs,title,[0],[0]
"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015). This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder. In this paper, we experiment with a new type of decoder for VAE: a dilated CNN. By changing the decoder’s dilation architecture, we control the size of context from previously generated words. In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information. We show that when carefully managed, VAEs can outperform LSTM language models. We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE. Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",text,[0],[0]
"Generative models play an important role in NLP, both in their use as language models and because of their ability to effectively learn from unlabeled data.",1. Introduction,[0],[0]
"By parameterzing generative models using neural nets, recent work has proposed model classes that are particularly expressive and can pontentially model a wide range of phenomena in language and other modalities.",1. Introduction,[0],[0]
"We focus on a specific instance
1Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Zichao Yang <zichaoy@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
of this class: the variational autoencoder1 (VAE) (Kingma & Welling, 2013).
",1. Introduction,[0],[0]
"The generative story behind the VAE (to be described in detail in the next section) is simple: First, a continuous latent representation is sampled from a multivariate Gaussian.",1. Introduction,[1.0],"['The generative story behind the VAE (to be described in detail in the next section) is simple: First, a continuous latent representation is sampled from a multivariate Gaussian.']"
"Then, an output is sampled from a distribution parameterized by a neural decoder, conditioned on the latent representation.",1. Introduction,[0],[0]
"The latent representation (treated as a latent variable during training) is intended to give the model more expressive capacity when compared with simpler neural generative models–for example, conditional language models.",1. Introduction,[0],[0]
"The choice of decoding architecture and final output distribution, which connect the latent representation to output, depends on the kind of data being modeled.",1. Introduction,[1.0],"['The choice of decoding architecture and final output distribution, which connect the latent representation to output, depends on the kind of data being modeled.']"
"The VAE owes its name to an accompanying variational technique (Kingma & Welling, 2013) that has been successfully used to train such models on image data (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).
",1. Introduction,[0.9999999529588348],"['The VAE owes its name to an accompanying variational technique (Kingma & Welling, 2013) that has been successfully used to train such models on image data (Gregor et al., 2015; Salimans et al., 2015; Yan et al., 2016).']"
"The application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).",1. Introduction,[1.0],"['The application of VAEs to text data has been far less successful (Bowman et al., 2015; Miao et al., 2016).']"
"The obvious choice for decoding architecture for a textual VAE is an LSTM, a typical workhorse in NLP.",1. Introduction,[0],[0]
"However, Bowman et al. (2015) found that using an LSTM-VAE for text modeling yields higher perplexity on held-out data than using an LSTM language model.",1. Introduction,[0],[0]
"In particular, they observe that the LSTM decoder in VAE does not make effective use of the latent representation during training and, as a result, VAE collapses into a simple language model.",1. Introduction,[0],[0]
"Related work (Miao et al., 2016; Larochelle & Lauly, 2012; Mnih & Gregor, 2014) has used simpler decoders that model text as a bag of words.",1. Introduction,[0],[0]
"Their results indicate better use of latent representations, but their decoders cannot effectively model longer-range dependencies in text and thus underperform in terms of final perplexity.
",1. Introduction,[0],[0]
"Motivated by these observations, we hypothesize that the contextual capacity of the decoder plays an important role in whether VAEs effectively condition on the latent representation when trained on text data.",1. Introduction,[0],[0]
"We propose the use of a dilated CNN as a decoder in VAE, inspired by the recent success of using CNNs for audio, image and language
1The name VAE is often used to refer to both a model class and an associated inference procedure.
modeling (van den Oord et al., 2016a; Kalchbrenner et al., 2016a; van den Oord et al., 2016b).",1. Introduction,[0],[0]
"In contrast with prior work where extremely large CNNs are used, we exploit the dilated CNN for its flexibility in varying the amount of conditioning context.",1. Introduction,[0],[0]
"In the two extremes, depending on the choice of dilation, the CNN decoder can reproduce a simple MLP using a bags of words representation of text, or can reproduce the long-range dependence of recurrent architectures (like an LSTM) by conditioning on the entire history.",1. Introduction,[0],[0]
"Thus, by choosing a dilated CNN as the decoder, we are able to conduct experiments where we vary contextual capacity, finding a sweet spot where the decoder can accurately model text but does not yet overpower the latent representation.
",1. Introduction,[0],[0]
"We demonstrate that when this trade-off is correctly managed, textual VAEs can perform substantially better than simple LSTM language models, a finding consistent with recent image modeling experiments using variational lossy autoencoders (Chen et al., 2016).",1. Introduction,[0],[0]
"We go on to show that VAEs with carefully selected CNN decoders can be quite effective for semi-supervised classification and unsupervised clustering, outperforming several strong baselines (from (Dai & Le, 2015)) on both text categorization and sentiment analysis.
",1. Introduction,[0],[0]
"Our contributions are as follows: First, we propose the use of a dilated CNN as a new decoder for VAE.",1. Introduction,[0],[0]
"We then empirically evaluate several dilation architectures with different capacities, finding that reduced contextual capacity leads to stronger reliance on latent representations.",1. Introduction,[0],[0]
"By picking a decoder with suitable contextual capacity, we find our VAE performs better than LSTM language models on two data sets.",1. Introduction,[0],[0]
"We also explore the use of dilated CNN VAEs for semi-supervised classification and find they perform better than strong baselines from (Dai & Le, 2015).",1. Introduction,[0],[0]
"Finally, we verify that the same framework can be used effectively for unsupervised clustering.",1. Introduction,[0],[0]
"In this section, we begin by providing background on the use of variational autoencoders for language modeling.",2. Model,[0],[0]
Then we introduce the dilated CNN architecture that we will use as a new decoder for VAE in experiments.,2. Model,[0],[0]
"Finally, we describe the generalization of VAE that we will use to conduct experiments on semi-supervised classification.",2. Model,[0],[0]
"Neural language models (Mikolov et al., 2010) typically generate each token xt conditioned on the entire history of previously generated tokens:
p(x) = ∏ t p(xt|x1, x2, ..., xt−1).",2.1. Background on Variational Autoencoders,[0],[0]
"(1)
State-of-the-art language models often parametrize these conditional probabilities using RNNs, which compute an evolving hidden state over the text which is used to predict each xt.",2.1. Background on Variational Autoencoders,[0],[0]
"This approach, though effective in modeling text, does not explicitly model variance in higher-level properties of entire utterances (e.g. topic or style) and thus can have difficulty with heterogeneous datasets.
",2.1. Background on Variational Autoencoders,[0],[0]
"Bowman et al. (2015) propose a different approach to generative text modeling inspired by related work on vision (Kingma & Welling, 2013).",2.1. Background on Variational Autoencoders,[0],[0]
"Instead of directly modeling the joint probability p(x) as in Equation 1, we specify a generative process for which p(x) is a marginal distribution.",2.1. Background on Variational Autoencoders,[0],[0]
"Specifically, we first generate a continuous latent vector representation z from a multivariate Gaussian prior pθ(z), and then generate the text sequence x from a conditional distribution pθ(x|z) parameterized using a neural net (often called the generation model or decoder).",2.1. Background on Variational Autoencoders,[1.0],"['Specifically, we first generate a continuous latent vector representation z from a multivariate Gaussian prior pθ(z), and then generate the text sequence x from a conditional distribution pθ(x|z) parameterized using a neural net (often called the generation model or decoder).']"
"Because this model incorporates a latent variable that modulates the entire generation of each whole utterance, it may be better able to capture high-level sources of variation in the data.",2.1. Background on Variational Autoencoders,[0],[0]
"Specifically, in contrast with Equation 1, this generating distribution conditions on latent vector representation z:
pθ(x|z) = ∏ t pθ(xt|x1, x2, ..., xt−1, z).",2.1. Background on Variational Autoencoders,[0],[0]
"(2)
To estimate model parameters θ we would ideally like to maximize the marginal probability pθ(x) =∫ pθ(z)pθ(x|z)dz.",2.1. Background on Variational Autoencoders,[0],[0]
"However, computing this marginal is intractable for many decoder choices.",2.1. Background on Variational Autoencoders,[0],[0]
"Thus, the following variational lower bound is often used as an objective (Kingma & Welling, 2013):
log pθ(x) =",2.1. Background on Variational Autoencoders,[0],[0]
"− log ∫ pθ(z)pθ(x|z)dz
≥ Eqφ(z|x)[log pθ(x|z)]− KL(qφ(z|x)||pθ(z)).
",2.1. Background on Variational Autoencoders,[0],[0]
"Here, qφ(z|x) is an approximation to the true posterior (often called the recognition model or encoder) and is parameterized by φ.",2.1. Background on Variational Autoencoders,[0],[0]
"Like the decoder, we have a choice of neural architecture to parameterize the encoder.",2.1. Background on Variational Autoencoders,[0],[0]
"However, unlike the decoder, the choice of encoder does not change the model class – it only changes the variational approximation used in training, which is a function of both the model parameters θ and the approximation parameters φ.",2.1. Background on Variational Autoencoders,[0],[0]
Training seeks to optimize these parameters jointly using stochastic gradient ascent.,2.1. Background on Variational Autoencoders,[0],[0]
A final wrinkle of the training procedure involves a stochastic approximation to the gradients of the variational objective (which is itself intractable).,2.1. Background on Variational Autoencoders,[0],[0]
"We omit details here, noting only that the final distribution of the posterior approximation qφ(z|x) is typically assumed to be Gaussian so that a re-parametrization trick can be used, and refer readers to (Kingma & Welling, 2013).",2.1. Background on Variational Autoencoders,[0],[0]
"Together, this combination of generative model and variational inference procedure are often referred to as a variational autoencoder (VAE).",2.2. Training Collapse with Textual VAEs,[0],[0]
We can also view the VAE as a regularized version of the autoencoder.,2.2. Training Collapse with Textual VAEs,[1.0],['We can also view the VAE as a regularized version of the autoencoder.']
"Note, however, that while VAEs are valid probabilistic models whose likelihood can be evaluated on held-out data, autoencoders are not valid models.",2.2. Training Collapse with Textual VAEs,[0],[0]
"If only the first term of the VAE variational bound Eqφ(z|x)[log pθ(x|z)] is used as an objective, the variance of the posterior probability qφ(z|x) will become small and the training procedure reduces to an autoencoder.",2.2. Training Collapse with Textual VAEs,[0],[0]
"It is the KL-divergence term, KL(qφ(z|x)||pθ(z)), that discourages the VAE memorizing each x as a single latent point.
",2.2. Training Collapse with Textual VAEs,[0],[0]
"While the KL term is critical for training VAEs, historically, instability on text has been evidenced by the KL term becoming vanishingly small during training, as observed by Bowman et al. (2015).",2.2. Training Collapse with Textual VAEs,[0],[0]
"When the training procedure collapses in this way, the result is an encoder that has duplicated the Gaussian prior (instead of a more interesting posterior), a decoder that completely ignores the latent variable z, and a learned model that reduces to a simpler language model.",2.2. Training Collapse with Textual VAEs,[1.0],"['When the training procedure collapses in this way, the result is an encoder that has duplicated the Gaussian prior (instead of a more interesting posterior), a decoder that completely ignores the latent variable z, and a learned model that reduces to a simpler language model.']"
We hypothesize that this collapse condition is related to the contextual capacity of the decoder architecture.,2.2. Training Collapse with Textual VAEs,[0],[0]
The choice encoder and decoder depends on the type of data.,2.2. Training Collapse with Textual VAEs,[1.0],['The choice encoder and decoder depends on the type of data.']
"For images, these are typically MLPs or CNNs.",2.2. Training Collapse with Textual VAEs,[0],[0]
"LSTMs have been used for text, but have resulted in training collapse as discussed above (Bowman et al., 2015).",2.2. Training Collapse with Textual VAEs,[1.0],"['LSTMs have been used for text, but have resulted in training collapse as discussed above (Bowman et al., 2015).']"
"Here, we propose to use a dilated CNN as the decoder instead.",2.2. Training Collapse with Textual VAEs,[1.0],"['Here, we propose to use a dilated CNN as the decoder instead.']"
"In one extreme, when the effective contextual width of a CNN is very large, it resembles the behavior of LSTM.",2.2. Training Collapse with Textual VAEs,[0],[0]
"When the width is very small, it behaves like a bag-ofwords model.",2.2. Training Collapse with Textual VAEs,[1.0],"['When the width is very small, it behaves like a bag-ofwords model.']"
The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis: decoder contextual capacity and effective use of encoding information are directly related.,2.2. Training Collapse with Textual VAEs,[1.0],['The architectural flexibility of dilated CNNs allows us to change the contextual capacity and conduct experiments to validate our hypothesis: decoder contextual capacity and effective use of encoding information are directly related.']
We next describe the details of our decoder.,2.2. Training Collapse with Textual VAEs,[1.0],['We next describe the details of our decoder.']
"The typical approach to using CNNs used for text generation (Kalchbrenner et al., 2016a) is similar to that used for images (Krizhevsky et al., 2012; He et al., 2016), but with the convolution applied in one dimension.",2.3. Dilated Convolutional Decoders,[0],[0]
"We take this approach here in defining our decoder.
",2.3. Dilated Convolutional Decoders,[0],[0]
"One dimensional convolution: For a CNN to serve as a decoder for text, generation of xt must only condition on past tokens x<t.",2.3. Dilated Convolutional Decoders,[0],[0]
Applying the traditional convolution will break this assumption and use tokens x≥t as inputs to predict xt.,2.3. Dilated Convolutional Decoders,[0],[0]
"In our decoder, we avoid this by simply shifting the input by several slots (van den Oord et al., 2016b).",2.3. Dilated Convolutional Decoders,[0],[0]
"With a convolution with filter size of k and using n layers, our effective filter size (the number of past tokens
to condition to in predicting xt) would be (k− 1)× n+ 1.",2.3. Dilated Convolutional Decoders,[0],[0]
"Hence, the filter size would grow linearly with the depth of the network.
",2.3. Dilated Convolutional Decoders,[0],[0]
"Dilation: Dilated convolution (Yu & Koltun, 2015) was introduced to greatly increase the effective receptive field size without increasing the computational cost.",2.3. Dilated Convolutional Decoders,[1.0],"['Dilation: Dilated convolution (Yu & Koltun, 2015) was introduced to greatly increase the effective receptive field size without increasing the computational cost.']"
"With dilation d, the convolution is applied so that d − 1 inputs are skipped each step.",2.3. Dilated Convolutional Decoders,[0],[0]
Causal convolution can be seen a special case with d = 1.,2.3. Dilated Convolutional Decoders,[1.0],['Causal convolution can be seen a special case with d = 1.']
"With dilation, the effective receptive size grows exponentially with network depth.",2.3. Dilated Convolutional Decoders,[0],[0]
"In Figure 1b, we show dilation of sizes of 1 and 2 in the first and second layer, respectively.",2.3. Dilated Convolutional Decoders,[1.0],"['In Figure 1b, we show dilation of sizes of 1 and 2 in the first and second layer, respectively.']"
"Suppose the dilation size in the i-th layer is di and we use the same filter size k in all layers, then the effective filter size is (k − 1) ∑ i di + 1.",2.3. Dilated Convolutional Decoders,[0],[0]
The dilations are typically set to double every layer di+1,2.3. Dilated Convolutional Decoders,[0],[0]
=,2.3. Dilated Convolutional Decoders,[0],[0]
"2di, so the effective receptive field size can grow exponentially.",2.3. Dilated Convolutional Decoders,[0],[0]
"Hence, the contextual capacity of a CNN can be controlled across a greater range by manipulating the filter size, dilation size and network depth.",2.3. Dilated Convolutional Decoders,[0],[0]
"We use this approach in experiments.
",2.3. Dilated Convolutional Decoders,[0],[0]
"Residual connection: We use residual connection (He et al., 2016) in the decoder
ReLU 1x1, 512
ReLU 1xk, 512
conv
ReLU 1x1, 1024
+
conv
conv to speed up convergence and enable training of deeper models.",2.3. Dilated Convolutional Decoders,[0.9999999758611773],"['Residual connection: We use residual connection (He et al., 2016) in the decoder ReLU 1x1, 512 ReLU 1xk, 512 conv ReLU 1x1, 1024 + conv conv to speed up convergence and enable training of deeper models.']"
"We use a residual block (shown to the right) similar to that of (Kalchbrenner et al., 2016a).",2.3. Dilated Convolutional Decoders,[0],[0]
"We use three convolutional layers with filter size 1×1, 1×k, 1×1, respectively, and ReLU activation be-
tween convolutional layers.
",2.3. Dilated Convolutional Decoders,[0],[0]
Overall architecture: Our VAE architecture is shown in Figure 1a.,2.3. Dilated Convolutional Decoders,[0],[0]
"We use LSTM as the encoder to get the posterior probability q(z|x), which we assume to be diagonal Gaussian.",2.3. Dilated Convolutional Decoders,[0],[0]
We parametrize the mean µ and variance σ with LSTM output.,2.3. Dilated Convolutional Decoders,[0],[0]
"We sample z from q(z|x), the decoder is conditioned on the sample by concatenating z with every word embedding of the decoder input.",2.3. Dilated Convolutional Decoders,[0],[0]
"In addition to conducting language modeling experiments, we will also conduct experiments on semi-supervised classification of text using our proposed decoder.",2.4. Semi-supervised VAE,[1.0],"['In addition to conducting language modeling experiments, we will also conduct experiments on semi-supervised classification of text using our proposed decoder.']"
"In this section, we briefly review semi-supervised VAEs of (Kingma et al., 2014) that incorporate discrete labels as additional variables.",2.4. Semi-supervised VAE,[0],[0]
"Given the labeled set (x, y) ∼ DL and the unlabeled set x ∼ DU , (Kingma et al., 2014) proposed a model whose latent representation contains continuous vector z and discrete label y:
p(x,y, z) =",2.4. Semi-supervised VAE,[0],[0]
"p(y)p(z)p(x|y, z).",2.4. Semi-supervised VAE,[0],[0]
"(3)
The semi-supervised VAE fits a discriminative network q(y|x), an inference network q(z|x,y) and a generative network p(x|y, z) jointly as part of optimizing a variational lower bound similar that of basic VAE.",2.4. Semi-supervised VAE,[0.9999999682213705],"['(3) The semi-supervised VAE fits a discriminative network q(y|x), an inference network q(z|x,y) and a generative network p(x|y, z) jointly as part of optimizing a variational lower bound similar that of basic VAE.']"
"For labeled data (x,y), this bound is:
log p(x,y) ≥Eq(z|x,y)[log p(x|y, z)]",2.4. Semi-supervised VAE,[0],[0]
"− KL(q(z|x,y)||p(z)) + log p(y)
=L(x,y) + log p(y).
",2.4. Semi-supervised VAE,[0],[0]
"For unlabeled data x, the label is treated as a latent variable, yielding:
log p(x) ≥U(x) =Eq(y|x)",2.4. Semi-supervised VAE,[0],[0]
"[ Eq(z|x,y)[log p(x|y, z)]
− KL(q(z|x,y)||p(z))",2.4. Semi-supervised VAE,[0],[0]
"+ log p(y)− log q(y|x) ]
",2.4. Semi-supervised VAE,[0],[0]
"= ∑ y q(y|x)L(x,y)− KL(q(y|x)||p(y)).
",2.4. Semi-supervised VAE,[0],[0]
"Combining the labeled and unlabeled data terms, we have the overall objective as:
J =E(x,y)∼DL",2.4. Semi-supervised VAE,[0],[0]
"[L(x,y)] + Ex∼DU",2.4. Semi-supervised VAE,[0],[0]
[U(x)],2.4. Semi-supervised VAE,[0],[0]
"+ αE(x,y)∼DL",2.4. Semi-supervised VAE,[0],[0]
"[log q(y|x)],
where α controls the trade off between generative and discriminative terms.
",2.4. Semi-supervised VAE,[0],[0]
Gumbel-softmax: Jang et al. (2016); Maddison et al. (2016) propose a continuous approximation to sampling from a categorical distribution.,2.4. Semi-supervised VAE,[0],[0]
"Let u be a categorical distribution with probabilities π1, π2, ..., πc.",2.4. Semi-supervised VAE,[0],[0]
"Samples from u
can be approximated using:
yi = exp((log(πi) +",2.4. Semi-supervised VAE,[0],[0]
"gi)/τ)∑c j=1 exp((log(πj) + gj)/τ) , (4)
where gi follows Gumbel(0, 1).",2.4. Semi-supervised VAE,[0],[0]
The approximation is accurate when τ → 0 and smooth when τ > 0.,2.4. Semi-supervised VAE,[0],[0]
"In experiments, we use Gumbel-Softmax to approximate the samples from p(y|x) to reduce the computational cost.",2.4. Semi-supervised VAE,[0],[0]
"As a result, we can directly back propagate the gradients of U(x) to the discriminator network.",2.4. Semi-supervised VAE,[0],[0]
We anneal τ,2.4. Semi-supervised VAE,[0],[0]
"so that sample variance is small when training starts and then gradually decrease τ .
",2.4. Semi-supervised VAE,[0],[0]
Unsupervised clustering: In this section we adapt the same framework for unsupervised clustering.,2.4. Semi-supervised VAE,[1.0],['Unsupervised clustering: In this section we adapt the same framework for unsupervised clustering.']
"We directly minimize the objective U(x), which is consisted of two parts: reconstruction loss and KL regularization on q(y|x).",2.4. Semi-supervised VAE,[1.0],"['We directly minimize the objective U(x), which is consisted of two parts: reconstruction loss and KL regularization on q(y|x).']"
The first part encourages the model to assign x to label y such that the reconstruction loss is low.,2.4. Semi-supervised VAE,[1.0],['The first part encourages the model to assign x to label y such that the reconstruction loss is low.']
We find that the model can easily get stuck in two local optimum: the KL term is very small and q(y|x) is close to uniform distribution or the KL term is very large and all samples collapse to one class.,2.4. Semi-supervised VAE,[1.0],['We find that the model can easily get stuck in two local optimum: the KL term is very small and q(y|x) is close to uniform distribution or the KL term is very large and all samples collapse to one class.']
"In order to make the model more robust, we modify the KL term by:
KLy = max(γ,KL(q(y|x)|p(y)).",2.4. Semi-supervised VAE,[0],[0]
"(5)
That is, we only minimize the KL term when it is large enough.",2.4. Semi-supervised VAE,[0],[0]
"Since we would like to investigate VAEs for language modeling and semi-supervised classification, the data sets should be suitable for both purposes.",3.1. Data sets,[0],[0]
"We use two large scale document classification data sets: Yahoo Answer and Yelp15 review, representing topic classification and sentiment classification data sets respectively (Tang et al., 2015; Yang et al., 2016; Zhang et al., 2015).",3.1. Data sets,[0],[0]
"The original data sets contain millions of samples, of which we sample 100k as training and 10k as validation and test from the respective partitions.",3.1. Data sets,[1.0],"['The original data sets contain millions of samples, of which we sample 100k as training and 10k as validation and test from the respective partitions.']"
The detailed statistics of both data sets are in Table 1.,3.1. Data sets,[0],[0]
"Yahoo Answer contains 10 topics including Society & Culture, Science & Mathematics etc.",3.1. Data sets,[1.0],"['Yahoo Answer contains 10 topics including Society & Culture, Science & Mathematics etc.']"
"Yelp15 contains 5 level of rating, with higher rating better.",3.1. Data sets,[1.0],"['Yelp15 contains 5 level of rating, with higher rating better.']"
We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders.,3.2. Model configurations and Training details,[1.0],['We use an LSTM as an encoder for VAE and explore LSTMs and CNNs as decoders.']
"For CNNs, we explore several different configurations.",3.2. Model configurations and Training details,[0],[0]
"We set the convolution filter size to be 3 and gradually increase the depth and dilation from [1, 2, 4], [1, 2, 4, 8, 16] to [1, 2, 4, 8, 16, 1, 2, 4, 8, 16].",3.2. Model configurations and Training details,[0],[0]
"They represent small, medium and large model and we name them as SCNN, MCNN and LCNN.",3.2. Model configurations and Training details,[0],[0]
"We also explore a very large model with dilations [1, 2, 4, 8, 16, 1, 2, 4, 8, 16, 1, 2, 4, 8, 16] and name it as VLCNN.",3.2. Model configurations and Training details,[0],[0]
"The effective filter size are 15, 63, 125 and 187 respectively.",3.2. Model configurations and Training details,[0],[0]
"We use the last hidden state of the encoder LSTM and feed it though an MLP to get the mean and variance of q(z|x), from which we sample z and then feed it through an MLP to get the starting state of decoder.",3.2. Model configurations and Training details,[0],[0]
"For the LSTM decoder, we follow (Bowman et al., 2015) to use it as the initial state of LSTM and feed it to every step of LSTM.",3.2. Model configurations and Training details,[0],[0]
"For the CNN decoder, we concatenate it with the word embedding of every decoder input.
",3.2. Model configurations and Training details,[0],[0]
The architecture of the Semi-supervised VAE basically follows that of the VAE.,3.2. Model configurations and Training details,[0],[0]
We feed the last hidden state of the encoder LSTM through a two layer MLP then a softmax to get q(y|x).,3.2. Model configurations and Training details,[0],[0]
We use Gumbel-softmax to sample y from q(y|x).,3.2. Model configurations and Training details,[0],[0]
"We then concatenate y with the last hidden state of encoder LSTM and feed them throught an MLP to get the mean and variance of q(z|y,x).",3.2. Model configurations and Training details,[0],[0]
"y and z together are used as the starting state of the decoder.
",3.2. Model configurations and Training details,[0],[0]
We use a vocabulary size of 20k for both data sets and set the word embedding dimension to be 512.,3.2. Model configurations and Training details,[0],[0]
The LSTM dimension is 1024.,3.2. Model configurations and Training details,[0],[0]
"The number of channels for convolutions
in CNN decoders is 512 internally and 1024 externally, as shown in Section 2.3.",3.2. Model configurations and Training details,[0],[0]
"We select the dimension of z from [32, 64].",3.2. Model configurations and Training details,[0],[0]
"We find our model is not sensitive to this parameter.
",3.2. Model configurations and Training details,[0],[0]
"We use Adam (Kingma & Ba, 2014) to optimize all models and the learning rate is selected from [2e-3, 1e-3, 7.5e-4] and β1 is selected from [0.5, 0.9].",3.2. Model configurations and Training details,[0],[0]
"Empirically, we find learning rate 1e-3 and β1 = 0.5 to perform the best.",3.2. Model configurations and Training details,[0],[0]
"We select drop out ratio of LSTMs (both encoder and decoder) from [0.3, 0.5].",3.2. Model configurations and Training details,[0],[0]
"Following (Bowman et al., 2015), we also use drop word for the LSTM decoder, the drop word ratio is selected from [0, 0.3, 0.5, 0.7].",3.2. Model configurations and Training details,[0],[0]
"For the CNN decoder, we use a drop out ratio of 0.1 at each layer.",3.2. Model configurations and Training details,[0],[0]
We do not use drop word for CNN decoders.,3.2. Model configurations and Training details,[0],[0]
We use batch size of 32 and all model are trained for 40 epochs.,3.2. Model configurations and Training details,[0],[0]
We start to half the learning rate every 2 epochs after epoch 30.,3.2. Model configurations and Training details,[0],[0]
"Following (Bowman et al., 2015), we use KL cost annealing strategy.",3.2. Model configurations and Training details,[0],[0]
We set the initial weight of KL cost term to be 0.01 and increase it linearly until a given iteration T .,3.2. Model configurations and Training details,[0],[0]
"We treat T as a hyper parameter and select it from [10k, 40k, 80k].",3.2. Model configurations and Training details,[0],[0]
The results for language modeling are shown in Table 2.,3.3. Language modeling results,[0],[0]
We report the negative log likelihood (NLL) and perplexity (PPL) of the test set.,3.3. Language modeling results,[0],[0]
"For the NLL of VAEs, we decompose it into reconstruction loss and KL divergence and report the KL divergence in the parenthesis.",3.3. Language modeling results,[0],[0]
"To better visualize these results, we plot the results of Yahoo data set (Table 2a) in Figure 2.
",3.3. Language modeling results,[0],[0]
We first look at the LM results for Yahoo data set.,3.3. Language modeling results,[0],[0]
"As we gradually increase the effective filter size of CNN from SCNN, MCNN to LCNN, the NLL decreases from 345.3, 338.3 to 335.4.",3.3. Language modeling results,[0],[0]
The NLL of LCNN-LM is very close to the NLL of LSTM-LM 334.9.,3.3. Language modeling results,[0],[0]
"But VLCNN-LM is a little bit worse than LCNN-LM, this indicates a little bit of over-fitting.
",3.3. Language modeling results,[0],[0]
"We can see that LSTM-VAE is worse than LSTM-LM in terms of NLL and the KL term is nearly zero, which verifies the finding of (Bowman et al., 2015).",3.3. Language modeling results,[0],[0]
"When we use CNNs as the decoders for VAEs, we can see improvement over pure CNN LMs.",3.3. Language modeling results,[0],[0]
"For SCNN, MCNN and LCNN, the VAE results improve over LM results from 345.3 to 337.8, 338.3 to 336.2, and 335.4 to 333.9 respectively.",3.3. Language modeling results,[0],[0]
The improvement is big for small models and gradually decreases as we increase the decoder model contextual capacity.,3.3. Language modeling results,[0],[0]
"When the model is as large as VLCNN, the improvement diminishes and the VAE result is almost the same with LM result.",3.3. Language modeling results,[0],[0]
"This is also reflected in the KL term, SCNN-VAE has the largest KL of 13.3 and VLCNN-VAE has the smallest KL of 0.7.",3.3. Language modeling results,[0],[0]
"When LCNN is used as the decoder, we obtain an optimal trade off between using contextual information and latent representation.",3.3. Language modeling results,[0],[0]
"LCNN-VAE achieves a NLL of 333.9, which improves over LSTM-LM with NLL of 334.9.
",3.3. Language modeling results,[0],[0]
"We find that if we initialize the parameters of LSTM encoder with parameters of LSTM language model, we can improve the VAE results further.",3.3. Language modeling results,[0],[0]
This indicates better encoder model is also a key factor for VAEs to work well.,3.3. Language modeling results,[0],[0]
"Combined with encoder initialization, LCNN-VAE improves over LSTM-LM from 334.9 to 332.1 in NLL and from 66.2 to 63.9 in PPL.",3.3. Language modeling results,[0],[0]
Similar results for the sentiment data set are shown in Table 2b.,3.3. Language modeling results,[0],[0]
"LCNN-VAE improves over LSTM-LM from 362.7 to 359.1 in NLL and from 42.6 to 41.1 in PPL.
",3.3. Language modeling results,[0],[0]
"Latent representation visualization: In order to visualize the latent representation, we set the dimension of z to be 2 and plot the mean of posterior probability q(z|x), as shown in Figure 3.",3.3. Language modeling results,[0],[0]
We can see distinct different characteristics of topic and sentiment representation.,3.3. Language modeling results,[0],[0]
"In Figure 3a, we can see that documents of different topics fall into different clusters, while in Figure 3b, documents of different ratings form a continuum, they lie continuously on the xaxis as the review rating increases.",3.3. Language modeling results,[0],[0]
"Motivated by the success of VAEs for language modeling, we continue to explore VAEs for semi-supervised learning.",3.4. Semi-supervised VAE results,[0],[0]
"Following that of (Kingma et al., 2014), we set the number of labeled samples to be 100, 500, 1000 and 2000 respectively.
",3.4. Semi-supervised VAE results,[0],[0]
Ablation Study:,3.4. Semi-supervised VAE results,[0],[0]
"At first, we would like to explore the effect of different decoders for semi-supervised classification.",3.4. Semi-supervised VAE results,[0],[0]
We fix the number of labeled samples to be 500 and report both classification accuracy and NLL of the test set of Yahoo data set in Table.,3.4. Semi-supervised VAE results,[0],[0]
5.,3.4. Semi-supervised VAE results,[0],[0]
We can see that SCNN-VAESemi has the best classification accuracy of 65.5.,3.4. Semi-supervised VAE results,[0],[0]
The accuracy decreases as we gradually increase the decoder contextual capacity.,3.4. Semi-supervised VAE results,[0],[0]
"On the other hand, LCNN-VAE-Semi has the best NLL result.",3.4. Semi-supervised VAE results,[0],[0]
"This classification accuracy and NLL trade off once again verifies our conjecture: with small contextual window size, the decoder is forced to use the encoder information, hence the latent representation is better
learned.
",3.4. Semi-supervised VAE results,[0],[0]
"Comparing the NLL results of Table 5 with that of Table 2a, we can see the NLL improves.",3.4. Semi-supervised VAE results,[0],[0]
"The NLL of semisupervised VAE improves over simple VAE from 337.8 to 335.7 for SCNN, from 336.2 to 332.8 for MCNN, and from 333.9 to 332.8 for LCNN.",3.4. Semi-supervised VAE results,[0],[0]
"The improvement mainly comes from the KL divergence part, this indicates that better latent representations decrease the KL divergence, further improving the VAE results.
",3.4. Semi-supervised VAE results,[0],[0]
"Comparison with related methods: We compare Semisupervised VAE with the methods from (Dai & Le, 2015), which represent the previous state-of-the-art for semisupervised sequence learning.",3.4. Semi-supervised VAE results,[0],[0]
Dai & Le (2015) pre-trains a classifier by initializing the parameters of a classifier with that of a language model or a sequence autoencoder.,3.4. Semi-supervised VAE results,[0],[0]
They find it improves the classification accuracy significantly.,3.4. Semi-supervised VAE results,[0],[0]
"Since SCNN-VAE-Semi performs the best according to Table 5, we fix the decoder to be SCNN in this part.",3.4. Semi-supervised VAE results,[0],[0]
The detailed comparison is in Table 4.,3.4. Semi-supervised VAE results,[0],[0]
"We can see that semisupervised VAE performs better than LM-LSTM and LALSTM from (Dai & Le, 2015).",3.4. Semi-supervised VAE results,[0],[0]
We also initialize the encoder of the VAE with parameters from LM and find classification accuracy further improves.,3.4. Semi-supervised VAE results,[0],[0]
We also see the advantage of SCNN-VAE-Semi over LM-LSTM is greater when the number of labeled samples is smaller.,3.4. Semi-supervised VAE results,[0],[0]
The advantage decreases as we increase the number of labeled samples.,3.4. Semi-supervised VAE results,[0],[0]
"When we set the number of labeled samples to be 25k, the SCNN-VAE-Semi achieves an accuracy of 70.4, which is similar to LM-LSTM with an accuracy of 70.5.",3.4. Semi-supervised VAE results,[0],[0]
"Also, SCNN-VAE-Semi performs better on Yahoo data set than Yelp data set.",3.4. Semi-supervised VAE results,[0],[0]
"For Yelp, SCNN-VAE-Semi is a little bit worse than LM-LSTM if the number of labeled samples is greater than 100, but becomes better when we initialize the encoder.",3.4. Semi-supervised VAE results,[0],[0]
Figure 3b explains this observation.,3.4. Semi-supervised VAE results,[0],[0]
It shows the documents are coupled together and are harder to classify.,3.4. Semi-supervised VAE results,[0],[0]
"Also, the latent representation contains information other than sentiment, which may not be useful for classification.",3.4. Semi-supervised VAE results,[0],[0]
We also explored using the same framework for unsupervised clustering.,3.5. Unsupervised clustering results,[0],[0]
"We compare with the baselines that ex-
tract the feature with existing models and then run Gaussian Mixture Model (GMM) on these features.",3.5. Unsupervised clustering results,[0],[0]
We find empirically that simply using the features does not perform well since the features are high dimensional.,3.5. Unsupervised clustering results,[0],[0]
"We run a PCA on these features, the dimension of PCA is selected from [8, 16, 32].",3.5. Unsupervised clustering results,[0],[0]
"Since GMM can easily get stuck in poor local optimum, we run each model ten times and report the best result.",3.5. Unsupervised clustering results,[0],[0]
We find directly optimizing U(x) does not perform well for unsupervised clustering and we need to initialize the encoder with LSTM language model.,3.5. Unsupervised clustering results,[0],[0]
The model only works well for Yahoo data set.,3.5. Unsupervised clustering results,[0],[0]
This is potentially because Figure 3b shows that sentiment latent representations does not fall into clusters.,3.5. Unsupervised clustering results,[0],[0]
"γ in Equation 5 is a sensitive parameter, we select it from the range between 0.5 and 1.5 with an interval of 0.1.",3.5. Unsupervised clustering results,[0],[0]
"We use the following evaluation protocol (Makhzani et al., 2015): after we finish training, for cluster i, we find out the validation sample xn from cluster i that has the best q(yi|x) and assign the label of xn to all samples in cluster i.",3.5. Unsupervised clustering results,[0],[0]
We then compute the test accuracy based on this assignment.,3.5. Unsupervised clustering results,[0],[0]
The detailed results are in Table 5.,3.5. Unsupervised clustering results,[0],[0]
We can see SCNN-VAE-Unsup + init performs better than other baselines.,3.5. Unsupervised clustering results,[0],[0]
"LSTM+GMM performs very bad probably because the feature dimension is 1024 and is too high for GMM, even though we already used PCA to reduce the dimension.
",3.5. Unsupervised clustering results,[0],[0]
"Conditional text generation With the semi-supervised VAE, we are able to generate text conditional on the label.",3.5. Unsupervised clustering results,[0],[0]
"Due to space limitation, we only show one example of
generated reviews conditioning on review rating in Table 6.",3.5. Unsupervised clustering results,[0],[0]
"For each group of generated text, we fix z and vary the label y, while picking x via beam search with a beam size of 10.",3.5. Unsupervised clustering results,[0],[0]
"Variational inference via the re-parameterization trick was initially proposed by (Kingma & Welling, 2013; Rezende et al., 2014) and since then, VAE has been widely adopted as generative model for images (Gregor et al., 2015; Yan et al., 2016; Salimans et al., 2015; Gregor et al., 2016; Hu et al., 2017b).
",4. Related work,[0],[0]
"Our work is in line with previous works on combining variational inferences with text modeling (Bowman et al., 2015; Miao et al., 2016; Serban et al., 2016; Zhang et al., 2016; Hu et al., 2017a).",4. Related work,[0],[0]
"(Bowman et al., 2015) is the first work to combine VAE with language model and they use LSTM as the decoder and find some negative results.",4. Related work,[0],[0]
"On the other hand, (Miao et al., 2016) models text as bag of words, though improvement has been found, the model can not be used to generate text.",4. Related work,[0],[0]
Our work fills the gaps between them.,4. Related work,[0],[0]
"(Serban et al., 2016; Zhang et al., 2016) applies variational inference to dialogue modeling and machine translation and found some improvement in terms of generated text quality, but no language modeling results are reported.",4. Related work,[0],[0]
"(Chung et al., 2015; Bayer & Osendorfer, 2014; Fraccaro et al., 2016) embedded variational units in every step of a RNN, which is different from our model in using global latent variables to learn high level features.
",4. Related work,[0],[0]
"Our use of CNN as decoder is inspired by recent success of PixelCNN model for images (van den Oord et al., 2016b), WaveNet for audios (van den Oord et al., 2016a), Video Pixel Network for video modeling (Kalchbrenner et al., 2016b) and ByteNet for machine translation (Kalchbrenner et al., 2016a).",4. Related work,[0],[0]
"But in contrast to those works showing using a very deep architecture leads to better performance, CNN as decoder is used in our model to control the contextual capacity, leading to better performance.
",4. Related work,[0],[0]
"Our work is closed related the recently proposed variational lossy autoencoder (Chen et al., 2016) which is used to pre-
dict image pixels.",4. Related work,[0],[0]
"They find that conditioning on a smaller window of a pixels leads to better results with VAE, which is similar to our finding.",4. Related work,[0],[0]
"Much (Rezende & Mohamed, 2015; Kingma et al., 2016; Chen et al., 2016) has been done to come up more powerful prior/posterior distribution representations with techniques such as normalizing flows.",4. Related work,[0],[0]
We treat this as one of our future works.,4. Related work,[0],[0]
"This work is largely orthogonal and could be potentially combined with a more effective choice of decoder to yield additional gains.
",4. Related work,[0],[0]
"There is much previous work exploring unsupervised sentence encodings, for example skip-thought vectors (Kiros et al., 2015), paragraph vectors (Le & Mikolov, 2014), and sequence autoencoders (Dai & Le, 2015).",4. Related work,[0],[0]
"(Dai & Le, 2015) applies a pretrained model to semi-supervised classification and find significant gains, we use this as the baseline for our semi-supervised VAE.",4. Related work,[0],[0]
"We showed that by controlling the decoder’s contextual capacity in VAE, we can improve performance on both language modeling and semi-supervised classification tasks by preventing a degenerate collapse of the training procedure.",5. Conclusion,[0],[0]
These results indicate that more carefully characterizing decoder capacity and understanding how it relates to common variational training procedures may represent important avenues for unlocking future unsupervised problems.,5. Conclusion,[0],[0]
"Recent work on generative text modeling has found that variational autoencoders (VAE) with LSTM decoders perform worse than simpler LSTM language models (Bowman et al., 2015).",abstractText,[0],[0]
"This negative result is so far poorly understood, but has been attributed to the propensity of LSTM decoders to ignore conditioning information from the encoder.",abstractText,[0],[0]
"In this paper, we experiment with a new type of decoder for VAE: a dilated CNN.",abstractText,[0],[0]
"By changing the decoder’s dilation architecture, we control the size of context from previously generated words.",abstractText,[0],[0]
"In experiments, we find that there is a trade-off between contextual capacity of the decoder and effective use of encoding information.",abstractText,[0],[0]
"We show that when carefully managed, VAEs can outperform LSTM language models.",abstractText,[0],[0]
"We demonstrate perplexity gains on two datasets, representing the first positive language modeling result with VAE.",abstractText,[0],[0]
"Further, we conduct an in-depth investigation of the use of VAE (with our new decoding architecture) for semi-supervised and unsupervised labeling tasks, demonstrating gains over several strong baselines.",abstractText,[0],[0]
Improved Variational Autoencoders for Text Modeling using Dilated Convolutions,title,[0],[0]
