19	18	The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences.
21	59	We also found that our generator can produce high-quality utterances with correctly ordered than those in the previous methods (see Table 1).
47	73	The recurrent language generator proposed in this paper is based on a neural net language generator (Wen et al., 2016b) which consists of three components: an encoder to incorporate the target meaning representation as the model inputs, an aggregator to align and control the encoded information, and a decoder to generate output sentences.
60	19	It encodes the source information into a distributed vector representation zi which is a concatenation of embedding vector representation of each slot-value pair, and is computed by: zi = oi ⊕ vi (2) where: oi and vi are the i-th slot and value embedding, respectively.
72	21	is an alignment model,va,Wa,Ua are the weight matrices to learn.
73	72	Secondly, the Refiner calculates the new input xt based on the original input token wt and the DA representation.
74	39	There are several choices to formulate the Refiner such as gating mechanism or attention mechanism.
75	18	For each input token wt, the selected mechanism module computes the new input xt based on the dialog act representation dt and the input token embedding wt, and is formulated by: xt = fR(dt,wt) (6) where: fR is a refinement function, in which each input token is refined (or filtered) by the dialogue act attention information before putting into the RNN decoder.
76	45	By this way, we can represent the whole sentence based on this refined input using RNN model.
77	27	Attention Mechanism: Inspired by work of Cui et al. (2016), in which an attention-overattention was introduced in solving reading comprehension tasks, we place another attention applied for Refiner over the attentive Aligner, resulting in a model Attentional Refiner over Attention (ARoA).
89	59	By this way, the reset and update gates learn not only the long-term dependency but also the attention information from the dialogue act and the previous hidden state.
90	60	Secondly, the candidate activation h̃t is also modified to depend on the DA representation as follows: h̃t = tanh(Whxxt + rt Whhht−1 +Whddt) + tanh(Wdcdt) (13) The hidden state is then computed by: ht = ut ht−1 + (1− ut) h̃t (14) Finally, the output distribution is computed by applying a softmax function g, and the distribution is sampled to obtain the next token, P (wt+1 | wt, wt−1, ...w0, z) = g(Whoht) wt+1 ∼ P (wt+1 | wt, wt−1, ...w0, z) (15)
93	16	The pre-trained word vectors (Pennington et al., 2014) were used to initialize the model.
94	15	The generators were optimized by using stochastic gradient descent and back propagation through time (Werbos, 1990).
98	18	In the reranking, the cost of the generator is computed to form the reranking score R as follows: R = − T∑ t=1 y>t log pt + λERR (17) where λ is a trade off constant and is set to a large value in order to severely penalize nonsensical outputs.
99	15	The slot error rate ERR, which is the number of slots generated that is either redundant or missing, and is computed by: ERR = p+ q N (18) where: N is the total number of slots in DA, and p, q is the number of missing and redundant slots, respectively.
100	58	Note that the ERR reranking criteria cannot handle arbitrary slot-value pairs such as binary slots or slots that take the dont care value because these slots cannot be delexicalized and matched.
101	52	We conducted an extensive set of experiments to assess the effectiveness of our model using several metrics, datasets, and model architectures, in order to compare to prior methods.
102	16	We assessed the proposed models using four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television.
103	14	The Restaurant and Hotel were collected in (Wen et al., 2015b) which contain around 5K utterances and 200 distinct DAs.
104	55	The Laptop and TV datasets have been released by Wen et al. (2016a).
108	15	The generators were implemented using the TensorFlow library (Abadi et al., 2016) and trained by partitioning each of the datasets into training, validation and testing set in the ratio 3:1:1.
109	17	The hidden layer size was set to be 80 for all cases, and the generators were trained with a 70% of dropout rate.
125	31	Table 3 further shows the stable strength of our models since the results’ pattern stays unchanged compared to those in Table 2.
128	23	Figure 3 illustrates a comparison of four models (ENCDEC, SCLSTM, ARoA-M, and GR-ADD) which were trained from scratch on the laptop dataset in a variety of proportion of training data, from 10% to 100%.
130	14	Figure 4 presents a comparison performance of general models as described in Section 4.2.
132	43	Both the proposed models show their ability to generalize in the unseen domains (TV and Laptop datasets) since they consistently outperform the previous methods no matter how much training data was fed or how training method was used.
138	19	We present an extension of an Attentional RNN Encoder-Decoder model named EncoderAggregator-Decoder, in which a Refiner component is introduced to select and aggregate the semantic elements produced by the encoder.
139	13	We also present several different choices of gating and attention mechanisms which can be effectively applied to the Refiner.
143	13	In the future, it would be interesting to further investigate hybrid models which integrate gating and attention mechanisms in order to leverage the advantages of both mechanisms.
