4	65	To better understand the stories shared on social media we collect and annotate a novel dataset consisting of videos from a social media platform.
5	8	Importantly, we collect descriptions containing multiple sentences, ∗ as single sentences would typically not be able to capture the narration and plot of the video.
6	56	We introduce a large-scale multi-sentence description dataset for videos.
7	45	To build a dataset of high quality, diverse and narratively interesting videos, we choose videos that had high engagement on a social media platform.
12	11	We also show baseline results using state-of-the-art models.
14	17	Other multi-sentence description datasets are proposed for story narration of sets of images taken from a Flickr album (Huang et al., 2016; Krause et al., 2017).
18	4	We select videos from social media that are public and popular with a large number of comments and shares that triggered interactions between people.
19	5	In total, our dataset consists of 20k videos with duration ranging from 20s-180s and spanning across diverse topics that are observed on social media platforms.
20	10	We follow Krishna et al. (2017) to create temporally annotated sentences where each task is divided into two steps: (i) describing the video in multiple sentences, covering objects, situations and important details of the video; (ii) aligning each sentence in the paragraph with the corresponding timestamps in the video.
21	10	We refer to these as video segments.
23	4	We summarize the statistics of our dataset in Table 2 and compare it to prior work in Table 1.
24	22	Each of the 20k videos in our VideoStory dataset is annotated with a paragraph which has on average 4.67 temporally localized sentences.
25	11	As we have three paragraphs per video for validation and test set, we have a total of 26,245 paragraphs with a total of 123k sentences.
32	5	While the test set can be used to compare model variants in a paper, only the best model per paper should be evaluated on the blind test set annotations, which will only be possible on an evaluation server.
45	6	High quality video descriptions are more than bags of single-sentence captions; they should tell a coherent story.
48	10	We explore learning to caption the videos using ground truth video segments.
49	68	To understand if the temporal component of the video is contributing to the description, we trained image captioning models on a frame sampled from the middle of the each segment of a video.
52	26	First, we use sequence to sequence (seq-seq) recurrent neural network (RNN) model which has a two-layer encoder RNN to encode video features and a decoder RNN to generate descriptions.
53	22	In the seq-seq approach we treat each description/segment individually and use an RNN decoder to describe each segment of the video, similar to Venugopalan et al. (2015), but using Gated Recurrent Units, GRUs, (Cho et al., 2014) for both the encoder and decoder.
56	22	To capture such contextual correlations, we incorporate context from previous segment description into the captioning module.
57	9	We build a model (seq-seq + context) which takes current segment video features and hidden representation of previous segment’s sentence generation RNN at every timestamp in the decoder.
60	18	For the image caption- ing models, we used features extracted from pretrained ResNet-152 on ImageNet (He et al., 2016).
61	29	For video captioning models we extract features from pre-trained 3D convolution ResNext-101 architecture trained on Kinetics (Kay et al., 2017), denoted as R3D, which achieved state-of-the-art results on various activity recognition tasks (Hara et al., 2018).
62	21	Since a significant percentage of our videos has objects other than humans (e.g., animals) we also experiment with image-video fusion features(denoted by RNEXT, R3D) i.e., concatenation of ResNext-101 features extracted from pre-trained ImageNet with R3D features described above.
64	31	For every segment, we set the maximum number of the sequence of features to 120 (i.e., 16X120 frames from the video) and maximum sentence length to 30.
66	7	We use GRU as recurrent architecture to encode frames and decode captions with 512 dimensional hidden representation.
67	10	We measure the captioning performance with most commonly-used evaluation metrics: BLEU{3,4}, METEOR, ROUGE-L, and CIDEr following previous works of image and video captioning (Papineni et al., 2002; Lin, 2004; Banerjee and Lavie, 2005; Vedantam et al., 2015).
68	16	In Table 5, we present the performance of our baseline models on VideoStory test dataset.
73	22	However, when evaluating our ActivityNet model on our VideoStory dataset (Table 5, last row), we see significantly lower performance compared to a model trained on our dataset, highlighting the complementary nature of our dataset.
76	24	We show qualitative results from the variants of our models in Table 4.
