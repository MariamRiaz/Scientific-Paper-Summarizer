0	14	Semi-supervised learning (SSL) aims to implement learning on both labeled and unlabeled data through fully considering the supervised knowledge delivered by labeled data and unsupervised data structure under unlabeled ones (Zhu, 2011).
1	63	Co-training (Blum & Mitchell, 1998) is one of the most classical and well known SSL approaches that train classifiers on two views and exchanges labels of unlabeled instances in an iterative way.
2	16	In the recent years, co-training has been attracting much attention attributed to both its wide applications, like web classification and visual detection (Xu et al., 2009), and rational theoretical supports (Blum & Mitchell, 1998; Balcan et al., 2004; Wang & Zhou, 2010; 2013).
5	19	Based on such high-confidence assumption, most of current co-training algorithms add pseudo-labeled samples into training without replacement.
11	22	It is thus meaningful to explore whether there exists such an optimization model, which can finely interpret the co-training implementation as the process of solving this model.
13	10	To address the aforementioned issues, a new co-training method, called self-paced co-training (SPaCo) is proposed in this study.
15	14	In the method, an unlabeled instance having been added into the training pool is likely to be removed if classifiers in later training rounds identify it as a low-confidence annotated one.
16	68	Besides, the pseudo label of an unlabeled instance has chance to be rectified based on the prediction knowledge obtained by classifiers in later training rounds.
17	27	Secondly, the SPaCo method employs a serial mode to update the classifiers of two views in co-training implementation instead of the parallel mode commonly adopted by previous methods.
65	25	We first present the following SPaCo model, which extends the self-paced learning optimization model (1) to two view scenarios, by introducing importance weights of two views v (1) k , v (2) k , (k = l + 1, · · · , l + u), together with the corresponding hard self-paced regularizer f(v, λ) = λv as proposed in (Jiang et al., 2014a): min w(j),yk,v (j) k ∈[0,1] j=1,2;k=l+1,··· ,l+u E(w(j), v (j) k , yk;λ (j), γ) = 2∑ j=1 l∑ i=1 L(yi, g (j)(x (j) i ;w (j))) + 1 2 2∑ j=1 ||w(j)||2 + 2∑ j=1 l+u∑ k=l+1 (v (j) k L(yk, g (j)(x (j) k ;w (j)))− λ(j)v(j)k ) − γ(v(1))Tv(2), (2) where l and u denote the number of labeled and unlabeled instances, respectively.
66	24	x(j)i is the i th sample (i = 1, · · · , l + u) under jth view (j = 1, 2), and yi is the common label of x(j)i for every j. v (j) k denotes the weight of x (j) k where k = l + 1, ..., l + u. v (j) is an u-dimensional vector preserving all the weights of unlabeled instances under jth view where its kth element is v(j)l+k.
68	23	λ(j) is the age parameter controlling the training scale in each iteration with respect to jth view, and γ is the parameter adjusting influence from the other view when one view is going to add more training samples.
70	12	This inner product encodes the relationship of “sample easiness degree” between two views.
71	18	The new coregularizer delivers the basic assumption under co-training that different views share common knowledge of pseudolabeled sample confidence (an unlabeled sample is likely to be labeled correctly or wrongly simultaneously for both views), and thus this inner product enforces the weight penalizing the loss of one view similar to that of the other view.
72	18	This finely accords to the idea of SPCL and complies with an instructor-student-collaborative learning manner under a specific co-training curriculum.
78	11	Two classifiers are simultaneously trained on labeled samples to get initial losses of both labeled and unlabeled instances.
79	10	Update v(3−j)k (j = 1, 2): The physical meaning of this step is to prepare confident unlabeled instances (with nonzero v(3−j)k values) for the training on the j th view.
83	25	(4) In the first iteration, all the v(j)k s are zeros according to the initialization.
89	27	Update w(j): This step aims to train a classifier by virtue of the labeled and pseudo-labeled samples in the training pool of the jth view.
91	24	(2) degenerates to the standard SVM optimization problem as: min w(j) 1 2 ||w(j)||2 + l∑ i=1 L (j) i + l+u∑ k=l+1 v (j) k L (j) k , (5) where L(j)t = L(yt, g(x (j) t ,w (j))), t = 1, · · · , l + u.
93	14	For the cross entropy loss, we can employ deep learning network to train the expected classifier, and thus our model is not constrained within one single classification algorithm.
95	15	(6) It is easy to prove that the global optimum of the above problem can be obtained by directly setting the pseudolabel yi of a training sample as the weighted sum of prediction value under two classifiers.
107	23	The standard co-training method (Blum & Mitchell, 1998) requires to simultaneously train classifiers of both views, and then select highly confident samples to label for each view and feed them into the training pool of the other view.
110	13	The algorithm does not consistently keep the previously selected training pool unchanged, while a confidence sample in the pool has certain chance to be thrown out from it when the loss value of a sample is larger than a preset threshold γ + λ.
114	12	This not only will make this algorithm fully comply with the alternative updating strategy for solving an optimization model, but also leads to better performance than traditional parallel model methods (see experiment part).
126	36	Based on the SPaCo model (2) underlying Algorithm 1, we can get some new insights underlying the co-training regimes.
129	56	Specifically, in the SPaCo model (2), there is a separate SPL objective function for each view, which means that there implicitly exists a robust loss for training the classifier of each view on pseudo-labeled samples.
132	19	That is, through consistently exchanging pseudo-labels justified in different views, the robust loss functions of both views are enforced to be related by such regularization term.
136	27	To validate the performance of the proposed SPaCo method, we first employ six text classification data sets derived from three real-world domains, where each data set is associated with two naturally partitioned or artificially generated views.
137	22	Besides, we also apply our method to the person re-identification task, which is a popular research topic in the field of computer vision.
