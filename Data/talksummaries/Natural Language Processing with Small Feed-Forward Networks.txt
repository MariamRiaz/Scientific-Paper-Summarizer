2	11	These models, however, may not be feasible in all computational settings.
3	32	In particular, models running on mobile devices are often constrained in terms of memory and computation.
8	23	We begin by introducing the network model structure and the character-based representations we use throughout all tasks (§2).
9	17	The four tasks that we address are: language identification (LangID), part-of-speech (POS) tagging, word segmentation, and preordering for translation.
12	9	Quantization: Using more dimensions and less precision (Lang-ID: §3.1).
18	26	Figure 1 illustrates the model architecture: 2879 1.
22	11	A softmax function models the probability of an output class y: P (y) ∝ exp(βTy h1 + by), where βy ∈ RM and by are the weight vector and bias, respectively.
23	31	Memory needs are dominated by the embedding matrix sizes ( ∑ g VgDg, where Vg and Dg are the vocabulary sizes and dimensions respectively for each feature group g), while runtime is strongly influenced by the hidden layer dimensions.
24	37	Hashed Character n-grams Previous applications of this network structure used (pretrained) word embeddings to represent words (Chen and Manning, 2014; Weiss et al., 2015).
25	47	However, for word embeddings to be effective, they usually need to cover large vocabularies (100,000+) and dimensions (50+).
49	12	Moreover, we can apply quantization to the embedding matrix without hurting prediction accuracy: it is better to use less precision for each dimension, but to use more dimensions.
52	12	Our 16-dim Lang-ID model runs at 4450 documents/second (5.6 MB of text per second) on the preprocessed Wikipedia dataset.
53	13	Relationship to Compact Language Detector These techniques back the open-source Compact Language Detector v3 (CLD3)1 that runs in Google Chrome browsers.2 Our experimental Lang-ID model uses the same overall architecture as CLD3, but uses a simpler feature set, less involved preprocessing, and covers fewer languages.
57	10	Bloom Mapped Word Clusters It is well known that word clusters can be powerful features in linear models for a variety of tasks (Koo et al., 2008; Turian et al., 2010).
63	29	Halving all feature embedding dimensions (except for the cluster features) still gives a 12% reduction in error and trims the overall size back to 1.1x the vanilla model, staying well under 1MB in total.
76	37	The neural network with its non-linearity is in theory able to learn bigrams by conjoining unigrams, but it has been shown that explicitly using character bigram features leads to better accuracy (Zhang et al., 2016; Pei et al., 2014).
77	9	Zhang et al. (2016) suggests that embedding manually specified feature conjunctions further improves accuracy (‘Zhang et al. (2016)-combo’ in Table 4).
78	84	However, such embeddings could easily lead to a model size explosion and thus are not considered in this work.
79	81	The results in Table 4 show that spending our memory budget on small bigram embeddings is more effective than on larger character embeddings, in terms of both accuracy and model size.
80	87	Our model featuring bigrams runs at 110KB of text per second, or 39k tokens/second.
81	56	Preordering source-side words into the target-side word order is a useful preprocessing task for statistical machine translation (Xia and McCord, 2004; Collins et al., 2005; Nakagawa, 2015; de Gispert et al., 2015).
82	16	We propose a novel transition system for this task (Table 5), so that we can repeatedly apply a small network to produce these permutations.
83	15	Inspired by a non-projective parsing transition system (Nivre, 2009), the system uses a SWAP action to permute spans.
84	61	The system is sound for permutations: any derivation will end with all of the input words in a permuted order, and complete: all permutations are reachable (use SHIFT and SWAP operations to perform a bubble sort, then APPEND n − 1 times to form a single span).
85	32	For training and evaluation, we use the English-Japanese manual word alignments from Nakagawa (2015).
86	37	Pipelines For preordering, we experiment with either spending all of our memory budget on reordering, or spending some of the memory budget on features over predicted POS tags, which also requires an additional neural network to predict these tags.
88	19	As the POS tagger network uses features based on a three word window around the token, another possibility is to add all of the features that would have affected the POS tag of a token to the reorderer directly.
92	15	This paper shows that small feed-forward networks are sufficient to achieve useful accuracies on a variety of tasks.
93	53	In resource-constrained environments, speed and memory are important metrics to optimize as well as accuracies.
94	25	While large and deep recurrent models are likely to be the most accurate whenever they can be afforded, feed-foward networks can provide better value in terms of runtime and memory, and should be considered a strong baseline.
