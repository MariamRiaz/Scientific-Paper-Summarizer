{"sections": [{"text": "Proceedings of NAACL-HLT 2018, pages 1079\u20131089 New Orleans, Louisiana, June 1 - 6, 2018. c\u00a92018 Association for Computational Linguistics"}, {"heading": "1 Introduction", "text": "Topic Detection and Tracking (Allan et al., 1998) is an important area of natural language processing to find topically related ideas that evolve over time in a sequence of text collections and exhibit temporal relationships. The temporal aspects of these collections can present valuable insight into the topical structure of the collections and can be quantified by modeling the dynamics of the underlying topics discovered over time.\nProblem Statement: We aim to generate temporal topical trends or automatic overview timelines of topics for a time sequence collection of documents. This involves the following three tasks in dynamic topic analysis: (1) Topic Structure Detection (TSD): Identifying main topics in the document collection. (2) Topic Evolution Detection (TED): Detecting the emergence of a new topic\nand recognizing how it grows or decays over time (Allan, 2002). (3) Temporal Topic Characterization (TTC): Identifying the characteristics for each of the main topics in order to track the words\u2019 usage (keyword trends) for a topic over time i.e. topical trend analysis for word evolution (Fig 1, Left).\nProbabilistic static topic models, such as Latent Dirichlet Allocation (LDA) (Blei et al., 2003) and its variants (Wang and McCallum, 2006; Hall et al., 2008; Gollapalli and Li, 2015) have been investigated to examine the emergence of topics from historical documents. Another variant known as Replicated Softmax (RSM) (Hinton and Salakhutdinov, 2009) has demonstrated better generalization in log-probability and retrieval, compared to LDA. Prior works (Iwata et al., 2010; Pruteanu-Malinici et al., 2010; Saha and Sindhwani, 2012; Schein et al., 2016) have investigated Bayesian modeling of topics in time-stamped documents. Particularly, Blei and Lafferty (2006) developed a LDA based dynamic topic model (DTM) to capture the evolution of topics in a time sequence collection of documents; however they do not capture explicitly the topic popularity and usage of specific terms over time. We propose a family of probabilistic time series models with distributional estimators to explicitly model the dynamics of the underlying topics, introducing temporal latent topic dependencies (Fig 1, Right).\nTo model temporal dependencies in high dimen-\n1079\nsional sequences, such as polyphonic music, the temporal stack of RBMs (Smolensky, 1986; Hinton, 2002) has been investigated to model complex distributions. The Temporal RBM (Taylor et al., 2007; Sutskever and Hinton, 2007), Recurrent Temporal RBM (RTRBM) (Sutskever et al., 2009) and RNN-RBM (Boulanger-Lewandowski et al., 2012) show success in modeling the temporal dependencies in such symbolic sequences. In addition, RNNs (Gupta et al., 2015a; Vu et al., 2016a,b; Gupta et al., 2016) have been recognized for sentence modeling in natural language tasks. We aspire to build neural dynamic topic model called RNN-RSM to model document collections over time and learn temporal topic correlations.\nWe consider RSM for TSD and introduce the explicit latent topical dependencies for TED and TTC tasks. Fig 1 illustrates our motivation, where temporal ordering in document collection V\u0302(t) at each time step t, is modeled by conditioning the latent topic h(t) on the sequence history of latent topics h(0), ..., h(t\u22121), accumulated with temporal lag. Each RSM discovers latent topics, where the introduction of a bias term in each RSM via the time-feedback latent topic dependencies enables to explicitly model topic evolution and specific topic term usage over time. The temporal connections and RSM biases allow to convey topical information and model relation among the words, in order to deeply analyze the dynamics of the underlying topics. We demonstrate the applicability of proposed RNN-RSM by analyzing 19 years of scientific articles from NLP research.\nThe contributions in this work are: (1) Introduce an unsupervised neural dynamic topic model based on recurrent neural network and RSMs, named as RNN-RSM to explicitly model discovered latent topics (evolution) and word relations (topic characterization) over time. (2) Demonstrate better generalization (logprobability and time stamp prediction), topic interpretation (coherence), evolution and characterization, compared to the state-of-the-art. (3) It is the first work in dynamic topic modeling using undirected stochastic graphical models and deterministic recurrent neural network to model collections of different-sized documents over time, within the generative and neural network framework. The code and data are available at https://github.com/pgcool/RNN-RSM."}, {"heading": "2 The RNN-RSM model", "text": "RSM (Fig 2, Left) models are a family of differentsized Restricted Boltzmann Machines (RBMs) (Gehler et al., 2006; Xing et al., 2005; Gupta et al., 2015b,c) that models word counts by sharing the same parameters with multinomial distribution over the observable i.e. it can be interpreted as a single multinomial unit (Fig 2, Middle) sampled as many times as the document size. This facilitates in dealing with the documents of different lengths.\nThe proposed RNN-RSM model (Fig 2, Right) is a sequence of conditional RSMs1 such that at any time step t, the RSM\u2019s bias parameters bv(t)\n1Notations: U\u0302={Un}Nn=1; U:2D-Matrix; l:vector; U/l:Upper/lower-case; Scalars in unbold\nand bh(t) depend on the output of a deterministic RNN with hidden layer u(t\u22121) in the previous time step, t\u22121. Similar to RNN-RBM (BoulangerLewandowski et al., 2012), we constrain RNN hidden units (u(t)) to convey temporal information, while RSM hidden units (h(t)) to model conditional distributions. Therefore, parameters (bv(t), bh\n(t)) are time-dependent on the sequence history at time t (via a series of conditional RSMs) denoted by \u0398(t) \u2261 {V\u0302(\u03c4),u(\u03c4)|\u03c4 < t}, that captures temporal dependencies. The RNN-RSM is defined by its joint probability distribution:\nP (V\u0302,H) = P ({V\u0302(t),h(t)}Tt=1) = T\u220f\nt=1\nP (V\u0302(t),h(t)|\u0398(t))\nwhere V\u0302 = [V\u0302(1), ...V\u0302(T )] and H = [h(1), ...h(T )]. Each h(t) \u2208 {0, 1}F be a binary stochastic hidden topic vector with size F and V\u0302(t) = {V(t)n }N(t)n=1 be a collection of N documents at time step t. Let V (t) n be a K \u00d7D(t)n observed binary matrix of the nth document in the collection where, D(t)n is the document size and K is the dictionary size over all the time steps. The conditional distribution (for each unit in hidden or visible) in each RSM at time step, is given by softmax and logistic functions:\nP (v k,(t) n,i = 1|h(t)n ) =\nexp(bv,i k,(t) + \u2211F j=1 h (t) n,jW k ij)\n\u2211K q=1 exp(bv,i q,(t) + \u2211F j=1 h (t) n,jW q ij)\nP (h (t) n,j = 1|V(t)n ) = \u03c3(b (t) h,j +\nD (t) n\u2211\ni=1\nK\u2211\nk=1\nv k,(t) n,i W k ij)\nwhere P (vk,(t)n,i = 1|h (t) n ) and P (h(t)n,j = 1|V (t) n ) are conditional distributions for ith visible vn,i and jth hidden unit hn,j for the nth document at t. W kij is a symmetric interaction term between i that takes on value k and j. vk,(t)n is sampled D (t) n times with identical weights connected to binary hidden units, resulting in multinomial visibles, therefore the name Replicated Softmax. The conditionals across layers are factorized as: P (V(t)n |h(t)n ) = \u220fD(t)n i=1 P (v (t) n,i|h (t) n ); P (h(t)n |V(t)n ) = \u220f j P (h (t) n,j |V (t) n ).\nSince biases of RSM depend on the output of RNN at previous time steps, that allows to propagate the estimated gradient at each RSM backward through time (BPTT). The RSM biases and RNN hidden state u(t) at each time step t are given by-\nbv (t) = bv+Wuvu\n(t\u22121)\nbh (t) = bh+Wuhu\n(t\u22121) (1)\nu(t) = tanh(bu + Wuuu (t\u22121) + Wvu\nN(t)\u2211\nn=1\nv\u0302(t)n ) (2)\nAlgorithm 1 Training RNN-RSM with BPTT Input: Observed visibles, V\u0302 = {V\u0302(0), V\u0302(1), ..., V\u0302(t), ..., V\u0302(T )} RNN-RSM Parameters: \u03b8 = {Wuh, Wvh, Wuv, Wvu, Wuu, bv, bu, bh, bv(t), bh(t), u(0)}\n1: Propagate u(t) in RNN portion of the graph using eq 2. 2: Compute bv(t) and bh(t) using eq 1. 3: Generate negatives V(t)\u2217 using k-step Gibbs sampling. 4: Estimate the gradient of the cost C w.r.t. parameters of\nRSM Wvh, bv(t) and bh(t) using eq 5. 5: Compute gradients (eq 6) w.r.t. RNN connections (Wuh,\nWuv,Wuu,Wvu,u 0) and biases (bv, bh, bu).\n6: Goto step 1 until stopping criteria (early stopping or maximum iterations reached)\nwhere Wuv, Wuh and Wvu are weights connecting RNN and RSM portions (Figure 2). bu is the bias of u and Wuu is the weight between RNN hidden units. v\u0302(t)n is a vector of v\u0302kn (denotes the count for the kth word in nth document).\u2211N(t)\nn=1 v\u0302 (t) n refers to the sum of observed vectors across documents at time step t where each document is represented as-\nv\u0302(t)n = [{v\u0302k,(t)n }Kk=1] and v\u0302k,(t)n = D\n(t) n\u2211\ni=1\nv k,(t) n,i (3)\nwhere vk,(t)n,i =1 if visible unit i takes on k th value.\nIn each RSM, a separate RBM is created for each document in the collection at time step t with D (t) n softmax units, where D (t) n is the count of words in the nth document. Consider a document of D(t)n words, the energy of the state {V(t)n ,h(t)n } at time step, t is given by-\nE(V(t)n ,h (t) n ) =\u2212\nF\u2211\nj=1\nK\u2211\nk=1\nh (t) n,jW k j v\u0302 k,(t) n\n\u2212 K\u2211\nk=1\nv\u0302k,(t)n b k v \u2212D(t)n\nF\u2211\nj=1\nbh,jh (t) n,j\nObserve that the bias terms on hidden units are scaled up by document length to allow hidden units to stabilize when dealing with different-sized documents. The corresponding energy-probability relation in the energy-based model is-\nP (V(t)n ) = 1\nZ (t) n\n\u2211\nh (t) n\nexp(\u2212E(V(t)n ,h(t)n )) (4)\nwhere Z(t)n = \u2211\nV (t) n \u2211 h (t) n\nexp(\u2212E(V(t)n ,h(t)n )) is the normalization constant. The lower bound on the log likelihood of the data takes the form:\nlnP (V(t)n ) \u2265 \u2211\nh(t)\nQ(h(t)n |V(t)n ) lnP (V(t)n ,h(t)n ) +H(Q)\n= lnP (V(t)n )\u2212KL[Q(h(t)n |V(t)n )||P (h(t)n |V(t)n )]\nwhere H(\u00b7) is the entropy and Q is the approximating posterior. Similar to Deep Belief Networks (Hinton et al., 2006), adding an extra layer improves lower bound on the log probability of data, we introduce the extra layer via RSM biases that propagates the prior via RNN connections. The dependence analogy follows-\nE(V (t) n ,h (t) n ) \u221d 1bv(t) and E(V (t) n ,h (t) n ) \u221d 1bh(t)\nlnP (V (t) n ) \u221d 1\nE(V (t) n ,h (t) n )\n; lnP (V\u0302 (t) n ) \u221d lnP ({V\u0302\u03c4n}\u03c4<t)\nObserve that the prior is seen as the deterministic hidden representation of latent topics and injected into each hidden state of RSMs, that enables the likelihood of the data to model complex temporal densities i.e. heteroscedasticity in document collections (V\u0302) and temporal topics (H).\nGradient Approximations: The cost in RNNRSM is: C = \u2211T t=1Ct \u2261 \u2211T t=1\u2212 lnP (V\u0302(t))\nDue to intractable Z, the gradient of cost at time step t w.r.t. (with respect to) RSM parameters are approximated by k-step Contrastive Divergence (CD) (Hinton, 2002). The gradient of the negative log-likelihood of a document collection {V(t)n }N(t)n=1 w.r.t. RSM parameter Wvh,\n1\nN (t)\nN(t)\u2211\nn=1\n\u2202(\u2212 lnP (V(t)n )) \u2202Wvh\n= 1\nN (t)\nN(t)\u2211\nn=1\n\u2202F(V (t) n ) \u2202Wvh \u2212 \u2202(\u2212 lnZ (t) n ) \u2202Wvh\n= EPdata [ \u2202F(V\n(t) n )\n\u2202Wvh ]\n\ufe38 \ufe37\ufe37 \ufe38 data-dependent expectation\n\u2212EPmodel [ \u2202F(V\n(t) n )\n\u2202Wvh ]\n\ufe38 \ufe37\ufe37 \ufe38 model\u2019s expectation\n' 1 N (t)\nN(t)\u2211\nn=1\n\u2202F(V (t) n )\n\u2202Wvh \u2212 \u2202F(V\n(t)\u2217 n )\n\u2202Wvh\nThe second term is estimated by negative samples V(t)\u2217n obtained from k-step Gibbs chain starting at V(t)n samples. Pdata(V\u0302(t),h(t)) = P (h(t)|V\u0302(t))Pdata(V\u0302(t)) and Pdata(V\u0302(t)) = 1 N(t) \u2211N(t) n \u03b4(V\u0302\n(t) \u2212V(t)n ) is the empirical distribution on the observable. Pmodel(V (t)\u2217 n ,h (t) n ) is\ndefined in eq. 4. The free energy F(V(t)n ) is related to normalized probability of V(t)n as P (V (t) n ) \u2261 exp\u2212F(V(t)n ) /Z(t)n and as follows-\nF(V(t)n ) = \u2212 K\u2211\nk=1\nv\u0302k,(t)n b k v \u2212\nF\u2211\nj=1\nlog(1+\nexp(D(t)n bh,j +\nK\u2211\nk=1\nv\u0302k,(t)n W k j ))\nGradient approximations w.r.t. RSM parameters,\n\u2202Ct\n\u2202bv (t) '\nN(t)\u2211\nn=1\nv\u0302(t)\u2217n \u2212 v\u0302(t)n\n\u2202Ct\n\u2202bh (t) '\nN(t)\u2211\nn=1\n\u03c3(Wvhv\u0302 (t)\u2217 n \u2212D(t)n bh(t))\n\u2212\u03c3(Wvhv\u0302(t)n \u2212D(t)n bh(t))\n\u2202Ct \u2202Wvh\n' T\u2211\nt=1\nN(t)\u2211\nn=1\n\u03c3(Wvhv\u0302 (t)\u2217 n \u2212D(t)n bh(t))\nv\u0302(t)\u2217Tn \u2212 \u03c3(Wvhv\u0302(t)n \u2212D(t)n bh(t))v\u0302(t)Tn (5)\nThe estimated gradients w.r.t. RSM biases are back-propagated via hidden-to-bias parameters (eq 1) to compute gradients w.r.t. RNN connections (Wuh, Wuv, Wvu and Wuu) and biases (bh, bv and bu).\n\u2202C\n\u2202Wuh =\nT\u2211\nt=1\n\u2202Ct\n\u2202bh (t)\nu(t\u22121)T\n\u2202C\n\u2202Wuv =\nT\u2211\nt=1\n\u2202Ct\n\u2202bv (t)\nu(t\u22121)T\n\u2202C\n\u2202Wvu =\nT\u2211\nt=1\n\u2202Ct\n\u2202u(t) u(t)(1\u2212 u(t))\nN(t)\u2211\nn=1\nv\u0302(t)Tn\n\u2202C\n\u2202bh =\nT\u2211\nt=1\n\u2202Ct\n\u2202bh (t)\nand \u2202C\n\u2202bv =\nT\u2211\nt=1\n\u2202Ct\n\u2202bv (t)\n\u2202C\n\u2202bu =\nT\u2211\nt=1\n\u2202Ct\n\u2202u(t) u(t)(1\u2212 u(t))\n\u2202C\n\u2202Wuu =\nT\u2211\nt=1\n\u2202Ct\n\u2202u(t) u(t)(1\u2212 u(t))u(t\u22121)T\n(6)\nFor the single-layer RNN-RSM, the BPTT recurrence relation for 0 \u2264 t < T is given by-\n\u2202Ct\n\u2202u(t) = Wuu\n\u2202Ct+1\n\u2202u(t+1) u(t+1)(1\u2212 u(t+1))\n+Wuh \u2202Ct+1\n\u2202bh (t+1)\n+ Wuv \u2202Ct+1\n\u2202bv (t+1)\nwhere u(0) being a parameter and \u2202CT \u2202u(T ) = 0. See Training RNN-RSM with BPTT in Algo 1."}, {"heading": "3 Evaluation", "text": ""}, {"heading": "3.1 Dataset and Experimental Setup", "text": "We use the processed dataset (Gollapalli and Li, 2015), consisting of EMNLP and ACL conference papers from the year 1996 through 2014 (Table 1). We combine papers for each year from the two venues to prepare the document collections over time. We use ExpandRank (Wan and Xiao, 2008) to extract top 100 keyphrases for each paper, including unigrams and bigrams. We split the bigrams to unigrams to create a dictionary of all unigrams and bigrams. The dictionary size (K) and word count are 3390 and 5.19 M, respectively.\nWe evaluate RNN-RSM against static (RSM, LDA) and dynamic (DTM) topics models for topic and keyword evolution in NLP research over time. Individual 19 different RSM and LDA models are trained for each year, while DTM2 and RNNRSM are trained over the years with 19 time steps, where paper collections for a year is input at each time step. RNN-RSM is initialized with RSM (Wvh, bv, bh) trained for the year 2014.\nWe use perplexity to choose the number of topics (=30). See Table 2 for hyperparameters."}, {"heading": "3.2 Generalization in Dynamic Topic Models", "text": "Perplexity: We compute the perplexity on unobserved documents (V\u0302(t)) at each time step as PPL(V\u0302(t), t) = exp ( \u2212 1 N (t) \u2211N(t) n=1 logP (V (t) n )\n\u2211N(t) n=1 D (t) n\n)\n2https://radimrehurek.com/gensim/models/dtmmodel.html\nwhere t is the time step. N (t) is the number of documents in a collection (V\u0302(t)) at time t. Better models have lower perplexity values, suggesting less uncertainties about the documents. For heldout documents, we take 10 documents from each time step i.e. total 190 documents and compute perplexity for 30 topics. Fig 3d shows the comparison of perplexity values for unobserved documents from DTM and RNN-RSM at each time step. The SumPPL (Table 3) is the sum of PPL values for the held-out sets of each time step.\nDocument Time Stamp Prediction: To further assess the dynamic topics models, we split the document collections at each time step into 80-20% train-test, resulting in 1067 held-out documents. We predict the time stamp (dating) of a document by finding the most likely (with the lowest perplexity) location over the time line. See the mean absolute error (Err) in year for the held-out in Table 3. Note, we do not use the time stamp as observables during training."}, {"heading": "3.3 TSD, TED: Topic Evolution over Time", "text": "Topic Detection: To extract topics from each RSM, we compute posterior P (V\u0302(t)|hj = 1) by activating a hidden unit and deactivating the rest in a hidden layer. We extract the top 20 terms for every 30 topic set from 1996-2014, resulting in |Q|max = 19\u00d7 30\u00d7 20 possible topic terms.\nTopic Popularity: To determine topic popularity, we selected three popular topics (Sentiment Analysis, Word Vector and Dependency Parsing) in NLP research and create a set3 of key-terms (including unigrams and bigrams) for each topic. We compute cosine similarity of the key-terms defined for each selected topic and topics discovered by the topic models over the years. We consider the discovered topic that is the most similar to the key-terms in the target topic and plot the similarity values in Figure 3a, 3b and 3b. Observe that RNN-RSM shows better topic evolution for the three emerging topics. LDA and RSM show\n3topic-terms to be released with code\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nYears\nC os\nin e\nSi m\nila ri\nty RNN-RSM RSM LDA DTM\n(a) Topic: Sentiment Analysis\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nYears\nC os\nin e\nSi m\nila ri\nty\nRNN-RSM RSM LDA DTM\n(b) Topic: Word Vector\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\nYears\nC os\nin e\nSi m\nila ri\nty\nRNN-RSM RSM LDA DTM\n(c) Topic: Dependency Parsing\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n0\n0.2\n0.4\n0.6\n0.8\nYears\nPe rp\nle xi\nty (P\nPL )\nRNN-RSM DTM\n(d) Perplexity on Unobserved\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n14\n14.5\n15\n15.5\n16\n16.5\n17\n17.5\n18\nYears\nC O\nH (m\nea n)\nof to\npi cs\n:C O\nH \u00d7\n10 \u2212 2\nRNN-RSM DTM\n(e) COH (mean) Over Time\n96 97 98 99 00 01 02 03 04 05 06 07 08 09 10 11 12 13 14\n12 12.5\n13 13.5\n14 14.5\n15 15.5\n16 16.5\n17 17.5\n18\nYears\nC O\nH (m\ned ia\nn) of\nto pi\ncs :C\nO H \u00d7\n10 \u2212 2\nRNN-RSM DTM\n(f) COH (median) Over Time\n96 -00 00 -05 05 -10 10 -14\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nYears\nC os\nin e\nSi m\nila ri\nty\nSentiment Analysis Word Vector Dependency Parsing\n(g) RNN-RSM Adj Topic Sim\n96 -00 00 -05 05 -10 10 -14\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\nYears\nC os\nin e\nSi m\nila ri\nty\nSentiment Analysis Word Vector Dependency Parsing\n(h) DTM Adj Topic Sim\ntopical locality in Figure 3c attributed to no correlation in topic dynamics over time, while in Figure 3b, DTM does not capture evolution of topic Word Vector.\nTopic Drift (Focus Change): To compute the topic focus change over the years, we first split the time period 1996-2014 into five parts:{1996, 2000, 2005, 2010, 2014}. The cosine similarity scores are computed between the topic sets discovered in a particular year and the years preceding it in the above set, for example the similarity scores between the topic-terms in (1996, 2000), (1996, 2005), (1996, 2010) and (1996, 2014), respectively. Figure 3i, 3j, 3k and 3l demonstrate that RNN-RSM shows higher convergence in topic focus over the years, compared to LDA and RSM. In RNN-RSM, the topic similarity is gradually increased over time, however not in DTM. The higher similarities in the topic sets indicate that new/existing topics and words do not appear/disappear over time.\nWe compute topic-term drift (TTD) to show\nthe changing topics from initial to final year, as\nTTD = 1.0\u2212 cosineSimilarity(Q(t),Q(t\u2032)) where Q is the set of all topic-terms for time step t. Table 3 shows that TTD (where t=1996 and t\u2032=2014) are 0.268 and 0.084 for RNN-RSM and DTM, respectively. It suggests that the higher number of new topic-terms evolved in RNN-RSM, compared to DTM. Qualitatively, the Table 4 shows the topics observed with the highest and lowest cosine drifts in DTM and RNN-RSM.\nIn Figure 3g and 3h, we also illustrate the temporal evolution (drift) in the selected topics by computing cosine similarity on their adjacent topic vectors over time. The topic vectors are selected similarly as in computing topic popularity. We observe better TED in RNN-RSM than DTM for the three emerging topics in NLP research. For instance, for the selected topic Word Vector, the red line in DTM (Fig 3h) shows no drift (for x-axis 00-05, 05-10 and 10-14), suggesting the topicterms in the adjacent years are similar and does not evolve."}, {"heading": "3.4 Topic Interpretability", "text": "Beyond perplexities, we also compute topic coherence (Chang et al., 2009; Newman et al., 2009; Das et al., 2015) to determine the meaningful topics captured. We use the coherence measure proposed by Aletras and Stevenson (2013) that retrieves co-occurrence counts for the set of topic words using Wikipedia as a reference corpus to identify context features (window=5) for each topic word. Relatedness between topic words and context features is measured using normalized pointwise mutual information (NPMI), resulting in a single vector for every topic word. The coherence (COH) score is computed as the arithmetic mean of the cosine similarities between all word pairs. Higher scores imply more coherent topics. We use Palmetto4 library to estimate coherence. Quantitative: We compute mean and median coherence scores for each time step using the corresponding topics, as shown in Fig 3e and 3f. Table 3 shows mean-COH and median-COH scores, computed by mean and median of scores from Fig 3e and 3f, respectively. Observe that RNNRSM captures topics with higher coherence. Qualitative: Table 5 shows topics (top-10 words) with the highest and lowest coherence scores."}, {"heading": "3.5 TTC: Trending Keywords over time", "text": "We demonstrate the capability of RNN-RSM to capture word evolution (usage) in topics over time. We define: keyword-trend and SPAN. The keyword-trend is the appearance/disappearance of the keyword in topic-terms detected over time, while SPAN is the length of the longest sequence of the keyword appearance in its keyword trend.\n4github.com/earthquakesan/palmetto-py\nLet Q\u0302model = {Q(t)model}Tt=1 be a set of sets5 of topic-terms discovered by themodel (LDA, RSM, DTM and RNN-RSM) over different time steps. Let Q(t) \u2208 Q\u0302model be the topic-terms at time step t. The keyword-trend for a keyword k is a timeordered sequence of 0s and 1s, as\ntrendk(Q\u0302) = [find(k,Q(t))]Tt=1\nwhere; find(k,Q(t)) = { 1 if k \u2208 Q(t) 0 otherwise\n(7)\nAnd the SPAN (Sk) for the kth keyword isSk(Q\u0302) = length ( longestOnesSeq(trendk(Q\u0302) )\nWe compute keyword-trend and SPAN for each term from the set of some popular terms. We define average-SPAN for all the topic-terms appearing in the topics discovered over the years,\navg-SPAN(Q\u0302) = 1 ||Q\u0302|| \u2211\n{k|Q(t)\u2208Q\u0302\u2227k\u2208Q(t)}\nSk(Q\u0302)\nv\u0302k\n= 1 ||Q\u0302|| \u2211\n{k|Q(t)\u2208Q\u0302\u2227k\u2208Q(t)}\nSdictk (Q\u0302)\n5a set by bold and set of sets by b\u0302old\nwhere ||Q\u0302|| = |{k|Q(t) \u2208 Q\u0302 \u2227 k \u2208 Q(t)}| is the count of unique topic-terms and v\u0302k =\u2211T\nt=1 \u2211Dt j=1 v k j,t denotes the count of k\nth keyword. In Figure 4, the keyword-trends indicate emergence (appearance/disappearance) of the selected popular terms in topics discovered in ACL and EMNLP papers over time. Observe that RNNRSM captures longer SPANs for popular keywords and better word usage in NLP research. For example: Word Embedding is one of the top keywords, appeared locally (Figure 5) in the recent years. RNN-RSM detects it in the topics from 2010 to 2014, however DTM does not. Similarly, for Neural Language. However, Machine Translation and Language Model are globally appeared in the input document collections over time and captured in the topics by RNN-RSM and DTM. We also show keywords (Rule-set and Seed Words) that disappeared in topics over time.\nHigher SPAN suggests that the model is capable in capturing trending keywords. Table 6 shows corresponding comparison of SPANs for the 13\nselected keywords. The SPAN Sk for each keyword is computed from Figure 4. Observe that ||Q\u0302||DTM < ||Q\u0302||RNN\u2212RSM suggests new topics and words emerged over time in RNN-RSM, while higher SPAN values in RNN-RSM suggest better trends. Figure 6 shows how the word usage, captured by DTM and RNN-RSM for the topic Word Vector, changes over 19 years in NLP research. RNN-RSM captures popular terms Word Embedding and Word Representation emerged in it."}, {"heading": "4 Discussion: RNN-RSM vs DTM", "text": "Architecture: RNN-RSM treats document\u2019s stream as high dimensional sequences over time and models the complex conditional probability distribution i.e. heteroscedasticity in document collections and topics over time by a temporal stack of RSMs (undirected graphical model), conditioned on time-feedback connections using RNN (Rumelhart et al., 1985). It has two hidden layers: h (stochastic binary) to capture topical information, while u (deterministic) to convey temporal information via BPTT that models the topic dependence at a time step t on all the previous steps \u03c4 < t. In contrast, DTM is built upon\nLDA (directed model), where Dirichlet distribution on words is not amenable to sequential modeling, therefore its natural parameters (topic and topic proportion distributions) for each topic are chained, instead of latent topics that results in intractable inference in topic detection and chaining.\nTopic Dynamics: The introduction of explicit connection in latent topics in RNN-RSM allow new topics and words for the underlying topics to appear or disappear over time by the dynamics of topic correlations. As discussed, the distinction of h and u permits the latent topic h(t) to capture new topics, that may not be captured by h(t\u22121).\nDTM assumes a fixed number of global topics and models their distribution over time. However, there is no such assumption in RNN-RSM. We fixed the topic count in RNN-RSM at each time step, since Wvh is fixed over time and RSM biases turn off/on terms in each topic. However, this is fundamentally different for DTM. E.g. a unique label be assigned to each of the 30 topics at any time steps t and t\u2032. DTM follows the sets of topic labels: {TopicLabels(t)}30k=1 = {TopicLabels(t\u2032)}30k=1, due to eq (1) in Blei and Lafferty (2006) (discussed in section 5) that limits DTM to capture new (or local) topics or words appeared over time. It corresponds to the keywordtrends (section 3.5).\nOptimization: The RNN-RSM is based on Gibbs sampling and BPTT for inference while DTM employs complex variational methods, since applying Gibbs sampling is difficult due to the nonconjugacy of the Gaussian and multinomial distributions. Thus, easier learning in RNN-RSM.\nFor all models, approximations are solely used to compute the likelihood, either using variational approaches or contrastive divergence; perplexity was then computed based on the approximated likelihood. More specifically, we use variational approximations to compute the likelihood\nfor DTM (Blei and Lafferty, 2006). For RSM and RNN-RSM, the respective likelihoods are approximated using the standard Contrastive Divergence (CD). While there are substantial differences between variational approaches and CD, and thus in the manner the likelihood for different models is estimated - both approximations work well for the respective family of models in terms of approximating the true likelihood. Consequently, perplexities computed based on these approximated likelihoods are indeed comparable."}, {"heading": "5 Conclusion and Future Work", "text": "We have proposed a neural temporal topic model which we name as RNN-RSM, based on probabilistic undirected graphical topic model RSM with time-feedback connections via deterministic RNN, to capture temporal relationships in historical documents. The model is the first of its kind that learns topic dynamics in collections of different-sized documents over time, within the generative and neural network framework. The experimental results have demonstrated that RNNRSM shows better generalization (perplexity and time stamp prediction), topic interpretation (coherence) and evolution (popularity and drift) in scientific articles over time. We also introduced SPAN to illustrate topic characterization.\nIn future work, we forsee to investigate learning dynamics in variable number of topics over time. It would also be an interesting direction to investigate the effect of the skewness in the distribution of papers over all years. Further, we see a potential application of the proposed model in learning the time-aware i.e. dynamic word embeddings (Aitchison, 2001; Basile et al., 2014; Bamler and Mandt, 2017; Rudolph and Blei, 2018; Yao et al., 2018) in order to capture language evolution over time, instead of document topics."}, {"heading": "Acknowledgments", "text": "We thank Sujatha Das Gollapalli for providing us with the data sets used in the experiments. We express appreciation for our colleagues Florian Buettner, Mark Buckley, Stefan Langer, Ulli Waltinger and Usama Yaseen, and anonymous reviewers for their in-depth review comments. This research was supported by Bundeswirtschaftsministerium (bmwi.de), grant 01MD15010A (Smart Data Web) at Siemens AG- CT Machine Intelligence, Munich Germany."}], "year": 2018, "references": [{"title": "Language change: progress or decay", "authors": ["Jean Aitchison."], "venue": "Cambridge University Press.", "year": 2001}, {"title": "Evaluating topic coherence using distributional semantics", "authors": ["Nikolaos Aletras", "Mark Stevenson."], "venue": "Proceedings of the 10th International Conference on Computational Semantics (IWCS). Potsdam, Germany, pages 13\u201322.", "year": 2013}, {"title": "Introduction to topic detection and tracking", "authors": ["James Allan."], "venue": "Topic detection and tracking, Springer, pages 1\u201316.", "year": 2002}, {"title": "Topic detection and tracking pilot study final report", "authors": ["James Allan", "Jaime G Carbonell", "George Doddington", "Jonathan Yamron", "Yiming Yang."], "venue": "Proceedings of the DARPA Broadcast News Transcription and Understanding Workshop. Virginia,", "year": 1998}, {"title": "Dynamic word embeddings", "authors": ["Robert Bamler", "Stephan Mandt."], "venue": "Proceedings of the 34th International Conference on Machine Learning. Sydney, Australia, pages 380\u2013389.", "year": 2017}, {"title": "Analysing word meaning over time by exploiting temporal random indexing", "authors": ["Pierpaolo Basile", "Annalina Caputo", "Giovanni Semeraro."], "venue": "Proceedings of the 1st Italian Conference on Computational Linguistics (CLiC-it). Pisa University Press,", "year": 2014}, {"title": "Dynamic topic models", "authors": ["David M. Blei", "John D. Lafferty."], "venue": "Proceedings of the 23rd International Conference on Machine Learning. Association for Computing Machinery, Pittsburgh, Pennsylvania USA, pages 113\u2013120.", "year": 2006}, {"title": "Latent dirichlet allocation", "authors": ["David M. Blei", "Andrew Y. Ng", "Michael I. Jordan."], "venue": "Proceedings of Machine Learning Research 3(Jan):993\u20131022.", "year": 2003}, {"title": "Modeling temporal dependencies in high-dimensional sequences: Application to polyphonic music generation and transcription", "authors": ["Nicolas Boulanger-Lewandowski", "Yoshua Bengio", "Pascal Vincent."], "venue": "Proceedings of the 29th International Conference on", "year": 2012}, {"title": "Reading tea leaves: How humans interpret topic models", "authors": ["Jonathan Chang", "Sean Gerrish", "Chong Wang", "Jordan L. Boyd-Graber", "David M. Blei."], "venue": "Advances in Neural Information Processing Systems. Curran Associates, Inc., Vancouver, Canada, pages", "year": 2009}, {"title": "Gaussian lda for topic models with word embeddings", "authors": ["Rajarshi Das", "Manzil Zaheer", "Chris Dyer."], "venue": "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natu-", "year": 2015}, {"title": "Deep learning methods for the ex", "authors": ["Sch\u00fctze. 2015a"], "year": 2015}, {"title": "Training products of experts", "authors": ["Geoffrey E. Hinton"], "year": 2002}, {"title": "External evaluation of topic models", "authors": ["David Newman", "Sarvnaz Karimi", "Lawrence Cavedon."], "venue": "Proceedings of the 14th Australasian Document Computing Symposium. Citeseer, Sydney, Australia.", "year": 2009}, {"title": "Hierarchical bayesian modeling of topics in time-stamped documents", "authors": ["Iulian Pruteanu-Malinici", "Lu Ren", "John Paisley", "Eric Wang", "Lawrence Carin."], "venue": "IEEE transactions on pattern analysis and machine intelligence 32(6):996\u20131011.", "year": 2010}, {"title": "Dynamic bernoulli embeddings for language evolution", "authors": ["Maja Rudolph", "David Blei."], "venue": "Proceedings of the 27th International Conference on World Wide Web Companion. Lyon, France.", "year": 2018}, {"title": "Learning internal representations by error propagation", "authors": ["David E. Rumelhart", "Geoffrey E. Hinton", "Ronald J. Williams."], "venue": "Technical report, California Univ San Diego La Jolla Inst for Cognitive Science.", "year": 1985}, {"title": "Learning evolving and emerging topics in social media: a dynamic nmf approach with temporal regularization", "authors": ["Ankan Saha", "Vikas Sindhwani."], "venue": "Proceedings of the 5th ACM International Conference on Web Search and Data Mining. Associa-", "year": 2012}, {"title": "Poisson-gamma dynamical systems", "authors": ["Aaron Schein", "Hanna Wallach", "Mingyuan Zhou."], "venue": "Advances in Neural Information Processing Systems 29, Curran Associates, Inc., Barcelona, Spain, pages 5005\u20135013.", "year": 2016}, {"title": "Information processing in dynamical systems: Foundations of harmony theory", "authors": ["Paul Smolensky."], "venue": "Technical report, Colorado University at Boulder Department of Computer Science.", "year": 1986}, {"title": "Learning multilevel distributed representations for highdimensional sequences", "authors": ["Ilya Sutskever", "Geoffrey Hinton."], "venue": "Proceedings of the 11th International Conference on Artificial Intelligence and Statistics. San Juan, Puerto Rico, pages 548\u2013", "year": 2007}, {"title": "The recurrent temporal restricted boltzmann machine", "authors": ["Ilya Sutskever", "Geoffrey E. Hinton", "Graham W. Taylor."], "venue": "Advances in Neural Information Processing Systems 22. Curran Associates, Inc., Vancouver, Canada, pages 1601\u20131608.", "year": 2009}, {"title": "Modeling human motion using binary latent variables", "authors": ["Graham W. Taylor", "Geoffrey E. Hinton", "Sam T. Roweis."], "venue": "Advances in Neural Information Processing Systems 20. Curran Associates, Inc., Vancouver, Canada, pages 1345\u20131352.", "year": 2007}, {"title": "Combining recurrent and convolutional neural networks for relation classification", "authors": ["Ngoc Thang Vu", "Heike Adel", "Pankaj Gupta", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the North American Chapter of the Association for Computational Linguis-", "year": 2016}, {"title": "Bi-directional recurrent neural network with ranking loss for spoken language understanding", "authors": ["Ngoc Thang Vu", "Pankaj Gupta", "Heike Adel", "Hinrich Sch\u00fctze."], "venue": "Proceedings of the Acoustics, Speech and Signal Processing (ICASSP). IEEE,", "year": 2016}, {"title": "Single document keyphrase extraction using neighborhood knowledge", "authors": ["Xiaojun Wan", "Jianguo Xiao."], "venue": "Proceedings of the 23rd National Conference on Artificial Intelligence. Chicago, Illinois USA, volume 8, pages 855\u2013860.", "year": 2008}, {"title": "Topics over time: a non-markov continuous-time model of topical trends", "authors": ["Xuerui Wang", "Andrew McCallum."], "venue": "Proceedings of the 12th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. Association for Com-", "year": 2006}, {"title": "Mining associated text and images with dual-wing harmoniums", "authors": ["Eric P. Xing", "Rong Yan", "Alexander G. Hauptmann."], "venue": "Proceedings of the 21st Conference on Uncertainty in Artificial Intelligence. AUAI Press, Edinburgh, Scotland UK.", "year": 2005}, {"title": "Dynamic word embeddings for evolving semantic discovery", "authors": ["Zijun Yao", "Yifan Sun", "Weicong Ding", "Nikhil Rao", "Hui Xiong."], "venue": "Proceedings of the 11th ACM International Conference on Web Search and Data Mining (WSDM). Association for Comput-", "year": 2018}], "id": "SP:5f4743aba8f02bf8bdcc8c263332b1d5d1c9a616", "authors": [{"name": "Pankaj Gupta", "affiliations": []}, {"name": "Subburam Rajaram", "affiliations": []}, {"name": "Hinrich Sch\u00fctze", "affiliations": []}, {"name": "Bernt Andrassy", "affiliations": []}], "abstractText": "Dynamic topic modeling facilitates the identification of topical trends over time in temporal collections of unstructured documents. We introduce a novel unsupervised neural dynamic topic model named as Recurrent Neural Network-Replicated Softmax Model (RNNRSM), where the discovered topics at each time influence the topic discovery in the subsequent time steps. We account for the temporal ordering of documents by explicitly modeling a joint distribution of latent topical dependencies over time, using distributional estimators with temporal recurrent connections. Applying RNN-RSM to 19 years of articles on NLP research, we demonstrate that compared to state-of-the art topic models, RNNRSM shows better generalization, topic interpretation, evolution and trends. We also introduce a metric (named as SPAN) to quantify the capability of dynamic topic model to capture word evolution in topics over time.", "title": "Deep Temporal-Recurrent-Replicated-Softmax for Topical Trends over Time"}