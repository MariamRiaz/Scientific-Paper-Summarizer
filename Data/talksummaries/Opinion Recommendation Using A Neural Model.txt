0	38	Offering a channel for customers to share opinions and give scores to products and services, review websites have become a highly influential information source that customers refer to for making purchase decisions.
1	30	Popular examples include IMDB.com on the movie domain, Epinions.com on the product domain, and Yelp.com on the service domain.
2	20	Figure 1 shows a screenshot of a restaurant review page on Yelp.com, which offers two main types of information.
7	40	For example, the authors themselves often find highly rated movies being tedious.
10	49	To address the limitations above, we propose a new task called opinion recommendation, which is to generate a customized review score of the product that the user is likely to give, as well as a customized review that the user would have written for the target product, if the user had reviewed the product.
18	25	This poses significant challenges to statistical models, which require manually defined features to capture relevant patterns from training data.
61	43	A characteristic of our model is that YS and YR are generated on a product that the user has not reviewed.
62	29	For capturing both general and personalized information, we first build a product model, a user model, and a neighborhood model, respectively, and using a memory network model to integrate these three types of information, constructing a customized product model.
66	19	In particular, a user profile can be achieved by modeling all the reviews RU of the user, and a target product profile can be obtained by using all existing reviews RT of the product.
85	28	As a result of matrix factorization, we directly obtain the probability of each user on each topic from the person-topic matrix T .
88	26	Given the representations of existing reviews {ed(rT1), ed(rT2), ..., ed(rTnt )} of the product, we use LSTM to model their temporal orders, obtaining a sequence of hidden vectors hT = {hT1 , hT2 , ..., hTnt} by recurrently feeding {ed(rT1), ed(rT2), ..., ed(rTnt} as inputs.
91	26	We build the customised product model to integrate user information and product information (as reflected by the product model), resulting in a single vector that represents a customised product.
93	28	In particular, we use the user representation vU and the neighbour representation vN to transform the target product representation hT = {hT1 , hT2 , ..., hTnt} into a customised product representation vC , which is tailored to the taste of the user.
116	21	We thus use the customized product vector vc as a bias of the weighted average of existing rating scores.
131	22	The userproduct pairs are extracted by following criterions: for each selected user-product pair, the user should have written 10 reviews at least, and the product should contain 100 reviews at least.
147	89	Effects of various configurations of our model, are shown on Table 2, where Joint is the full model of this paper, -user ablates the user model, -neighbor ablates the neighbor model, -rating is a single-task model that generates a review without the rating score, and -generation yields only the rating.
149	38	In addition, comparison between “-Joint” and “-user”, and between “-user” and “-user, - neighbor” shows that both the user information and the neighbour user information of the user are effective for improving the results.
151	19	Finally, comparison between “Joint” and “- generation”, and between “Joint” and “-rating” shows that multi-task learning by parameter sharing is highly useful.
152	22	We show the influence of hops of memory network for customized review generation on Figure 3.
158	24	As a result, we choose 3 as the number of hops in our final test.
159	21	We show the influence of the bias weight parameter µ for rating prediction in Figure 4.
163	20	We show the final results for opinion recommendation, comparing our proposed model with the HOP Bais 0 1.342 0 1.102 1 1.102 1 0.904 2 1.046 2 1.067 3 0.904 3 1.136 4 0.987 4 1.206 5 1.102 5 1.227 6 1.045 7 1.126 8 1.172 9 1.152 10 1.167 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 0 1 2 3 4 5 M S E μ 0.90 0.95 1.00 1.05 1.10 1.15 1.20 1.25 1.30 1.35 1.40 0 1 2 3 4 5 6 7 8 9 10 M S E hop Figure 4: Influence of bias score.
177	24	Table 4 shows example outputs of rating scores and reviews.
178	18	is the rating score and review written by user her/himself, and Base is the baseline model, that generates the rating score by RS-MF, and review by Sum-LSTM-Att.
181	111	Associating reviews and ratings closely, the joint model gives a rating score that is much closer to the real user score compared to the score given by the recommendation model MF.
182	46	In addition, we can also find some habits of certain users from their customized reviews, for example, Mexican food, cheap and clean restaurant.
183	86	We proposed a novel task called opinion recommendation, which is to generate the review and rating score that a certain user would give to an unreviewed product or service.
184	22	In particular, a deep memory network was utilized to find the association between the user and the product, jointly yielding the rating score and customised review.
185	123	Results show that our methods are better results compared to several pipelines baselines using state-of-the-art sentiment rating and summarisation systems.
186	53	Review scores given by the opinion recordation system are closer to real user review scores compared to the review scores which Yelp assigns to target products.
