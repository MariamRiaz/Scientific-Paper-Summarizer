0	26	Geolocation is an essential component of applications such as traffic monitoring (Emadi et al., 2017), human mobility pattern analysis (McNeill et al., 2016; Dredze et al., 2016) and disaster response (Ashktorab et al., 2014; Wakamiya et al., 2016), as well as targeted advertising (Anagnostopoulos et al., 2016) and local recommender systems (Ho et al., 2012).
1	14	Although Twitter provides users with the means to geotag their messages, less than 1% of users opt to turn on geotagging, so thirdparty service providers tend to use profile data, text content and network information to infer the location of users.
2	10	Text content is the most widely used source of geolocation information, due to its prevalence across social media services.
3	65	Text-based geolocation systems use the geographical bias of language to infer the location of a user or message using models trained on geotagged posts.
5	49	Regression models, as a consequence of minimising squared loss for a unimodal distribution, predict inputs with multiple targets to lie between the targets (e.g. a user who mentions content in both NYC and LA is predicted to be in the centre of the U.S.).
6	91	Classification models, while eliminating this problem by predicting a more granular target, don’t provide fine-grained predictions (e.g. specific locations in NYC), and also require heuristic discretisation of locations into regions (e.g. using clustering).
7	8	Mixture Density Networks (“MDNs”: Bishop (1994)) alleviate these problems by representing location as a mixture of Gaussian distributions.
8	41	Given a text input, an MDN can generate a mixture model in the form of a probability distribution over all location points.
52	10	x is a latitude/longitude coordinate whose probability we are seeking to predict.
55	45	3.3 Mixture Density Network (MDN) A mixture density network (“MDN”: Bishop (1994)) is a latent variable model where the conditional probability of p(y|x) is modelled as a mixture of K Gaussians where the mixing coefficients π and the parameters of Gaussian distributions µ and Σ are computed as a function of input using a neural network: P(y|x) = K∑ k=1 πk(x)N ( y|µk(x),Σk(x) ) In the bivariate case of latitude/longitude, the number of parameters of each Gaussian is 6 (πk(x), µ1k(x), µ2k(x), ρk(x), σ1k(x), σ2k(x)), which are learnt in the output layer of a regular neural network as a function of input x.
59	60	After applying the transformations to enforce the range constraints, the negative log likelihood loss of each sample x given a 2d coordinate label y is computed as: L(y|x) = − log { K∑ k=1 πk(x)N ( y|µk(x),Σk(x) )} To predict a location, given an unseen input, the output of the network is reshaped into a mixture of Gaussians and µk, one of the K components’ µ is chosen as the prediction.
62	10	As a result, it might be a difficult task for the model to learn all the parameters of each sample correctly.
65	50	We use the original cost function to update the weight matrices, biases and the global shared parameters of the mixture model through backpropagation.
69	30	In problems such as lexical dialectology, the input is real-valued 2d coordinates, and the goal is to predict dialect words from a given location.
72	30	A better solution is to use aK component Gaussian mixture representation of location, where µ and Σ are shared among all samples, and the output of the layer is the probability of input in each of the mixture components.
75	35	We apply the two described MDN models on two widely-used geotagged Twitter datasets for geolocation, and compare the results with state-of-the-art classification and regression baselines.
77	38	In our experiments, we use two existing Twitter user geolocation datasets: (1) GEOTEXT (Eisenstein et al., 2010), and (2) TWITTER-US (Roller et al., 2012).
78	18	Each dataset has fixed training, devel- opment and test partitions, and a user is represented by the concatenation of their tweets, and labelled with the latitude/longitude of the first collected geotagged tweet.2 GEOTEXT and TWITTER-US cover the continental US with 9k, 449k users, respectively.3 DARE is a dialect-term dataset derived from the Dictionary of American Regional English (Cassidy, 1985) by Rahimi et al. (to appear).
93	24	The input is a latitude/longitude coordinate, the first hidden layer is a Gaussian mixture with K components which has µ and Σ as its parameters and produces a probability for each component as an activation function, the second hidden layer with tanh nonlinearity captures the association between different Gaussians, and the output is a SoftMax layer which results in a probability distribution over the vocabulary.
100	67	To come up with a ranking over words given region r as query, we use the following measure: score(wi|r) = 1 N ∑ pj∈r log(P (wi|pj)) − 1 P P∑ j=1 log(P (wi|pj)) whereN equals the number of points (out of 10000) inside the query dialect region r and P equals the total number of points (here 10000).
108	16	Also we evaluated the model using recall at k and compared it to the tanh-layer model which again is competitive with tanh-layer but with the advantage of learning dialect regions simultaneously.
109	21	Because the DARE dialect terms are not used frequently in Twitter, many of the words are not covered in our dataset, despite its size.
112	8	We also visualised the learned Gaussians of the dialectology model in Figure 2, which as expected show several smaller regions (Gaussians with higher σ) and larger regions in lower populated areas.
121	112	We also applied the Gaussian mixture representation to predict dialect words from location, and showed that it MNWA MT ID ND MEWIOR SD MI NHVT NYWY IANE MA IL PA CTRI CA NV UT OHIN NJCO WVMOKS DEMDVAKY DC AZ OKNM TN NC TX AR SC AL GAMSLA FL 0 600 1200 1800 2400 3000 3600 4200 4800 er ro ri n km (a) hella (an intensifier) mostly used in Northern California, also the name of a company in Illinois.
122	26	is competitive with simple tanh activation in terms of both perplexity of the predicted unigram model and also recall at k at retrieving DARE dialect words by location input.
123	33	Furthermore we showed that the learned Gaussian mixtures have intereting properties such as covering high population density regions (e.g. NYC and LA) with a larger number of small Gaussians, and a smaller number of larger Gaussians in low density areas (e.g. the midwest).
124	50	Although we applied the mixture of Gaussians to location data, it can be used in other settings where the input or output are from a continuous multivariate distribution.
125	31	For example it can be applied to predict financial risk (Wang and Hua, 2014) and sentiment (Joshi et al., 2010) given text.
126	82	We showed that when a global structure exists (e.g. population centres, in the case of geolocation) it is better to share the global parameters of the mixture model to improve generalisation.
127	151	In this work, we used the bivariate Gaussian distribution in the MDN’s mixture leaving the use of other distributions which might better suit the geolocation task for future research.
