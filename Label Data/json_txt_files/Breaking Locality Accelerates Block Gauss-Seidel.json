{"sections": [{"text": "showed that momentum can be used to accelerate the rate of convergence for block GaussSeidel in the setting where a fixed partitioning of the coordinates is chosen ahead of time. We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any fixed partitioning. Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix subblocks are well-conditioned. Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm."}, {"heading": "1. Introduction", "text": "The randomized Gauss-Seidel method is a commonly used iterative algorithm to compute the solution of an n\u00d7 n linear system Ax = b by updating a single coordinate at a time in a randomized order. While this approach is known to converge linearly to the true solution when A is positive definite (see e.g. (Leventhal & Lewis, 2010)), in practice it is often more efficient to update a small block of coordinates at a time due to the effects of cache locality.\nIn extending randomized Gauss-Seidel to the block setting, a natural question that arises is how one should sample the next block. At one extreme a fixed partition of the coordi-\n1UC Berkeley, Berkeley, California, USA 2Rensselaer Polytechnic Institute, Troy, New York, USA. Correspondence to: Stephen Tu <stephent@berkeley.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nnates is chosen ahead of time. The algorithm is restricted to randomly selecting blocks from this fixed partitioning, thus favoring data locality. At the other extreme we break locality by sampling a new set of random coordinates to form a block at every iteration.\nTheoretically, the fixed partition case is well understood both for Gauss-Seidel (Qu et al., 2015; Gower & Richta\u0301rik, 2015) and its Nesterov accelerated variant (Nesterov & Stich, 2016). More specifically, at most O(\u00b5\u22121part log(1/\u03b5)) iterations of Gauss-Seidel are sufficient to reach a solution with at most \u03b5 error, where \u00b5part is a quantity which measures how well the A matrix is preconditioned by the block diagonal matrix containing the sub-blocks corresponding to the fixed partitioning. When acceleration is used, Nesterov and Stich (2016) show that the rate improves to O (\u221a\nn p\u00b5 \u22121 part log(1/\u03b5)\n)\n, where p is the partition size.\nFor the random coordinate selection model, the existing literature is less complete. While it is known (Qu et al., 2015; Gower & Richta\u0301rik, 2015) that the iteration complexity with random coordinate section is O(\u00b5\u22121rand log(1/\u03b5)) for an \u03b5 error solution, \u00b5rand is another instance dependent quantity which is not directly comparable to \u00b5part. Hence it is not obvious how much better, if at all, one expects random coordinate selection to perform compared to fixed partitioning.\nOur first contribution in this paper is to show that, when compared to the random coordinate selection model, the fixed partition model can perform very poorly in terms of iteration complexity to reach a pre-specified error. Specifically, we present a family of instances (similar to the matrices recently studied by Lee and Wright (2016)) where nonaccelerated Gauss-Seidel with random coordinate selection performs arbitrarily faster than both non-accelerated and even accelerated Gauss-Seidel, using any fixed partition. Our result thus shows the importance of the sampling strategy and that acceleration cannot make up for a poor choice of sampling distribution.\nThis finding motivates us to further study the benefits of acceleration under the random coordinate selection model. Interestingly, the benefits are more nuanced under this model. We show that acceleration improves the rate from\nO(\u00b5\u22121rand log(1/\u03b5)) to O\n(\n\u221a\n\u03bd\u00b5\u22121rand log(1/\u03b5)\n)\n, where \u03bd is\na new instance dependent quantity that satisfies \u03bd \u2264 \u00b5\u22121rand. We derive a bound on \u03bd which suggests that if the subblocks of A are all well conditioned, then acceleration can provide substantial speedups. We note that this is merely a sufficient condition, and our experiments suggest that our bound is conservative.\nIn the process of deriving our results, we also develop a general proof framework for randomized accelerated methods based on Wilson et al. (2016) which avoids the use of estimate sequences in favor of an explicit Lyapunov function. Using our proof framework we are able to recover recent results (Nesterov & Stich, 2016; Allen-Zhu et al., 2016) on accelerated coordinate descent. Furthermore, our proof framework allows us to immediately transfer our results on Gauss-Seidel over to the randomized accelerated Kaczmarz algorithm, extending a recent result by Liu and Wright (2016) on updating a single constraint at a time to the block case.\nFinally, we empirically demonstrate that despite its theoretical nuances, accelerated Gauss-Seidel using random coordinate selection can provide significant speedups in practical applications over Gauss-Seidel with fixed partition sampling, as well as the classical conjugate-gradient (CG) algorithm. As an example, for a kernel ridge regression (KRR) task in machine learning on the augmented CIFAR10 dataset (n = 250, 000), acceleration with random coordinate sampling performs up to 1.5\u00d7 faster than acceleration with a fixed partitioning to reach an error tolerance of 10\u22122, with the gap substantially widening for smaller error tolerances. Furthermore, it performs over 3.5\u00d7 faster than conjugate-gradient on the same task."}, {"heading": "2. Background", "text": "We assume that we are given an n \u00d7 n matrix A which is positive definite, and an n dimensional response vector b. We also fix an integer p which denotes a block size. Under the assumption of A being positive definite, the function f(x) = 12x\nTAx\u2212 xTb is strongly convex and smooth. Recent analysis of Gauss-Seidel (Gower & Richta\u0301rik, 2015) proceeds by noting the connection between Gauss-Seidel and (block) coordinate descent on f . This is the point of view we will take in this paper."}, {"heading": "2.1. Existing rates for randomized block Gauss-Seidel", "text": "We first describe the sketching framework of (Qu et al., 2015; Gower & Richta\u0301rik, 2015) and show how it yields rates on Gauss-Seidel when blocks are chosen via a fixed partition or randomly at every iteration. While we will only focus on the special case when the sketch matrix represents column sampling, the sketching framework allows us to provide a unified analysis of both cases.\nTo be more precise, let D be a distribution over Rn\u00d7p, and let Sk \u223c D be drawn iid from D. If we perform block coordinate descent by minimizing f along the range of Sk, then the randomized block Gauss-Seidel update is given by\nxk+1 = xk \u2212 Sk(STkASk)\u2020STk (Axk \u2212 b) . (1)\nColumn sampling. Every index set J \u2286 2[n] with |J | = p induces a sketching matrix S(J) = (eJ(1), ..., eJ(p)) where ei denotes the i-th standard basis vector in R n, and J(1), ..., J(p) is any ordering of the elements of J . By equipping different probability measures on 2[n], one can easily describe fixed partition sampling as well as random coordinate sampling (and many other sampling schemes). The former puts uniform mass on the index sets J1, ..., Jn/p, whereas the latter puts uniform mass on all ( n p ) index sets of size p. Furthermore, in the sketching framework there is no limitation to use a uniform distribution, nor is there any limitation to use a fixed p for every iteration. For this paper, however, we will restrict our attention to these cases.\nExisting rates. Under the assumptions stated above, (Qu et al., 2015; Gower & Richta\u0301rik, 2015) show that for every k \u2265 0, the sequence (1) satisfies E[\u2016xk \u2212 x\u2217\u2016A] \u2264 (1\u2212 \u00b5)k/2\u2016x0 \u2212 x\u2217\u2016A , (2)\nwhere \u00b5 = \u03bbmin(E[PA1/2S ]). The expectation in (2) is taken with respect to the randomness of S0, S1, ..., and the expectation in the definition of \u00b5 is taken with respect to S \u223c D. Under both fixed partitioning and random coordinate selection, \u00b5 > 0 is guaranteed (see e.g. (Gower & Richta\u0301rik, 2015), Lemma 4.3). Thus, (1) achieves a linear rate of convergence to the true solution, with the rate governed by the \u00b5 quantity shown above.\nWe now specialize (2) to fixed partitioning and random coordinate sampling, and provide some intuition for why we expect the latter to outperform the former in terms of iteration complexity. We first consider the case when the sampling distribution corresponds to fixed partitioning. Assume for notational convenience that the fixed partitioning corresponds to placing the first p coordinates in the first partition J1, the next p coordinates in the second partition J2, and so on. Here, \u00b5 = \u00b5part corresponds to a measure of how close the product of A with the inverse of the block diagonal is to the identity matrix, defined as\n\u00b5part = p\nn \u03bbmin\n( A \u00b7 blkdiag ( A\u22121J1 , ..., A \u22121 Jn/p )) . (3)\nAbove, AJi denotes the p \u00d7 p matrix corresponding to the sub-matrix of A indexed by the i-th partition. A loose lower bound on \u00b5part is\n\u00b5part \u2265 p\nn\n\u03bbmin(A)\nmax1\u2264i\u2264n/p \u03bbmax(AJi) . (4)\nOn the other hand, in the random coordinate case, Qu et al. (2015) derive a lower bound on \u00b5 = \u00b5rand as\n\u00b5rand \u2265 p\nn\n(\n\u03b2 + (1\u2212 \u03b2)max1\u2264i\u2264n Aii \u03bbmin(A)\n)\u22121 , (5)\nwhere \u03b2 = (p\u2212 1)/(n\u2212 1). Using the lower bounds (4) and (5), we can upper bound the iteration complexity of fixed partition Gauss-Seidel Npart by O (\nn p\nmax1\u2264i\u2264n/p \u03bbmax(AJi )\n\u03bbmin(A) log(1/\u03b5)\n)\nand random coordi-\nnate Gauss-Seidel Nrand as O ( n p max1\u2264i\u2264n Aii \u03bbmin(A) log(1/\u03b5) ) . Comparing the bound on Npart to the bound on Nrand, it is not unreasonable to expect that random coordinate sampling has better iteration complexity than fixed partition sampling in certain cases. In Section 3, we verify this by constructing instances A such that fixed partition Gauss-Seidel takes arbitrarily more iterations to reach a pre-specified error tolerance compared with random coordinate Gauss-Seidel."}, {"heading": "2.2. Accelerated rates for fixed partition Gauss-Seidel", "text": "Based on the interpretation of Gauss-Seidel as block coordinate descent on the function f , we can use Theorem 1 of Nesterov and Stich (2016) to recover a procedure and a rate for accelerating (1) in the fixed partition case; the specific details are discussed in the full version of the paper (Tu et al., 2017). We will refer to this procedure as ACDM.\nThe convergence guarantee of the ACDM procedure is that for all k \u2265 0,\nE[\u2016xk \u2212 x\u2217\u2016A] \u2264 O ( ( 1\u2212 \u221a p\nn \u00b5part\n)k/2\nD0\n)\n. (6)\nAbove, D0 = \u2016x0 \u2212 x\u2217\u2016A and \u00b5part is the same quantity defined in (3). Comparing (6) to the non-accelerated Gauss-Seidel rate given in (2), we see that acceleration improves the iteration complexity to reach a solution with \u03b5 error from O(\u00b5\u22121part log(1/\u03b5)) to O (\u221a n p\u00b5 \u22121 part log(1/\u03b5) ) , as discussed in Section 1."}, {"heading": "3. Results", "text": "We now present the main results of the paper. Proofs can be found in the full version (Tu et al., 2017) of this paper."}, {"heading": "3.1. Fixed partition vs random coordinate sampling", "text": "Our first result is to construct instances where Gauss-Seidel with fixed partition sampling runs arbitrarily slower than random coordinate sampling, even if acceleration is used.\nConsider the family of n \u00d7 n positive definite matrices A given by A = {A\u03b1,\u03b2 : \u03b1 > 0, \u03b1 + \u03b2 > 0} with A\u03b1,\u03b2\ndefined as A\u03b1,\u03b2 = \u03b1I + \u03b2 n1n1 T n. The family A exhibits a crucial property that \u03a0TA\u03b1,\u03b2\u03a0 = A\u03b1,\u03b2 for every n \u00d7 n permutation matrix \u03a0. Lee and Wright (2016) recently exploited this invariance to illustrate the behavior of cyclic versus randomized permutations in coordinate descent.\nWe explore the behavior of Gauss-Seidel as the matrices A\u03b1,\u03b2 become ill-conditioned. To do this, we consider a particular parameterization which holds the minimum eigenvalue equal to one and sends the maximum eigenvalue to infinity via the sub-family {A1,\u03b2}\u03b2>0. Our first proposition characterizes the behavior of Gauss-Seidel with fixed partitions on this sub-family.\nProposition 3.1. Fix \u03b2 > 0 and positive integers n, p, k such that n = pk. Let {Ji}ki=1 be any partition of {1, ..., n} with |Ji| = p, and denote Si \u2208 Rn\u00d7p as the column selector for partition Ji. Suppose S \u2208 Rn\u00d7p takes on value Si with probability 1/k. For every A1,\u03b2 \u2208 A we have that\n\u00b5part = p\nn+ \u03b2p . (7)\nNext, we perform a similar calculation under the random column sampling model.\nProposition 3.2. Fix \u03b2 > 0 and integers n, p such that 1 < p < n. Suppose each column of S \u2208 Rn\u00d7p is chosen uniformly at random from {e1, ..., en} without replacement. For every A1,\u03b2 \u2208 A we have that\n\u00b5rand = p\nn+ \u03b2p + (p\u2212 1)\u03b2p (n\u2212 1)(n+ \u03b2p) . (8)\nThe differences between (7) and (8) are striking. Let us assume that \u03b2 is much larger than n. Then, we have that \u00b5part \u2248 1/\u03b2 for (7), whereas \u00b5rand \u2248 p\u22121n\u22121 for (8). That is, \u00b5part can be made arbitrarily smaller than \u00b5rand as \u03b2 grows.\nOur next proposition states that the rate of Gauss-Seidel from (2) is tight order-wise in that for any instance there always exists a starting point which saturates the bound.\nProposition 3.3. Let A be an n \u00d7 n positive definite matrix, and let S be a random matrix such that \u00b5 = \u03bbmin(E[PA1/2S ]) > 0. Let x\u2217 denote the solution to Ax = b. There exists a starting point x0 \u2208 Rn such that the sequence (1) satisfies for all k \u2265 0,\nE[\u2016xk \u2212 x\u2217\u2016A] \u2265 (1\u2212 \u00b5)k\u2016x0 \u2212 x\u2217\u2016A . (9)\nFrom (2) we see that Gauss-Seidel using random coordinates computes a solution xk satisfying E[\u2016xk \u2212 x\u2217\u2016A1,\u03b2 ] \u2264 \u03b5 in at most k = O(np log(1/\u03b5)) iterations. On the other hand, Proposition 3.3 states that for any fixed partition, there exists an input x0 such that k = \u2126(\u03b2 log(1/\u03b5)) iterations are required to reach the same \u03b5 error tolerance.\nFurthermore, the situation does not improve even if use ACDM from (Nesterov & Stich, 2016). Proposition 3.6, which we describe later, implies that for any fixed partition there exists an input x0 such that k = \u2126 ( \u221a n p\u03b2 log(1/\u03b5) ) iterations are required for ACDM to reach \u03b5 error. Hence as \u03b2 \u2212\u2192 \u221e, the gap between random coordinate and fixed partitioning can be made arbitrarily large. These findings are numerically verified in Section 5.1."}, {"heading": "3.2. A Lyapunov analysis of accelerated Gauss-Seidel and Kaczmarz", "text": "Motivated by our findings, our goal is to understand the behavior of accelerated Gauss-Seidel under random coordinate sampling. In order to do this, we establish a general framework from which the behavior of accelerated GaussSeidel with random coordinate sampling follows immediately, along with rates for accelerated randomized Kaczmarz (Liu & Wright, 2016) and the accelerated coordinate descent methods of (Nesterov & Stich, 2016) and (AllenZhu et al., 2016).\nFor conciseness, we describe a simpler version of our framework which is still able to capture both the GaussSeidel and Kaczmarz results, deferring the general version to the full version of the paper. Our general result requires a bit more notation, but follows the same line of reasoning.\nLet H be a random n \u00d7 n positive semi-definite matrix. Put G = E[H], and suppose that G exists and is positive definite. Furthermore, let f : Rn \u2212\u2192 R be strongly convex and smooth, and let \u00b5 denote the strong convexity constant of f w.r.t. the \u2016\u00b7\u2016G\u22121 norm. Consider the following sequence {(xk, yk, zk)}k\u22650 defined by the recurrence\nxk+1 = 1\n1 + \u03c4 yk +\n\u03c4\n1 + \u03c4 zk , (10a)\nyk+1 = xk+1 \u2212Hk\u2207f(xk+1) , (10b) zk+1 = zk + \u03c4(xk+1 \u2212 zk)\u2212 \u03c4\n\u00b5 Hk\u2207f(xk+1) , (10c)\nwhere H0, H1, ... are independent realizations of H and \u03c4 is a parameter to be chosen. Following (Wilson et al., 2016), we construct a candidate Lyapunov function Vk for the sequence (10) defined as\nVk = f(yk)\u2212 f\u2217 + \u00b5\n2 \u2016zk \u2212 x\u2217\u20162G\u22121 . (11)\nThe following theorem demonstrates that Vk is indeed a Lyapunov function for (xk, yk, zk). Theorem 3.4. Let f,G,H be as defined above. Suppose further that f has 1-Lipschitz gradients w.r.t. the \u2016\u00b7\u2016G\u22121 norm, and for every fixed x \u2208 Rn,\nf(\u03a6(x;H)) \u2264 f(x)\u2212 1 2 \u2016\u2207f(x)\u20162H , (12)\nholds for a.e. H , where \u03a6(x;H) = x\u2212H\u2207f(x). Set \u03c4 in (10) as \u03c4 = \u221a \u00b5/\u03bd, with\n\u03bd = \u03bbmax\n(\nE\n[ (G\u22121/2HG\u22121/2)2 ]) .\nThen for every k \u2265 0, we have\nE[Vk] \u2264 (1\u2212 \u03c4)kV0 .\nWe now proceed to specialize Theorem 3.4 to both the Gauss-Seidel and Kaczmarz settings."}, {"heading": "3.2.1. ACCELERATED GAUSS-SEIDEL", "text": "Let S \u2208 Rn\u00d7p denote a random sketching matrix. As suggested in Section 2, we set f(x) = 12x\nTAx \u2212 xTb and put H = S(STAS)\u2020ST. Note that G = E[S(STAS)\u2020ST] is positive definite iff \u03bbmin(E[PA1/2S ]) > 0, and is hence satisfied for both fixed partition and random coordinate sampling (c.f. Section 2). Next, the fact that f is 1-Lipschitz w.r.t. the \u2016\u00b7\u2016G\u22121 norm and the condition (12) are standard calculations. All the hypotheses of Theorem 3.4 are thus satisfied, and the conclusion is Theorem 3.5, which characterizes the rate of convergence for accelerated Gauss-Seidel (Algorithm 1).\nAlgorithm 1 Accelerated randomized block Gauss-Seidel. Require: A \u2208 Rn\u00d7n, A \u227b 0, b \u2208 Rn, sketching matrices {Sk}T\u22121k=0 \u2286 Rn\u00d7p, x0 \u2208 Rn, \u00b5 \u2208 (0, 1), \u03bd \u2265 1.\n1: Set \u03c4 = \u221a\n\u00b5/\u03bd. 2: Set y0 = z0 = x0. 3: for k = 0, ..., T \u2212 1 do 4: xk+1 = 1 1+\u03c4 yk + \u03c4 1+\u03c4 zk. 5: Hk = Sk(S T kASk) \u2020STk . 6: yk+1 = xk+1 \u2212Hk(Axk+1 \u2212 b). 7: zk+1 = zk + \u03c4(xk+1 \u2212 zk)\u2212 \u03c4\u00b5Hk(Axk+1 \u2212 b). 8: end for 9: Return yT .\nTheorem 3.5. Let A be an n \u00d7 n positive definite matrix and b \u2208 Rn. Let x\u2217 \u2208 Rn denote the unique vector satisfying Ax\u2217 = b. Suppose each Sk, k = 0, 1, 2, ... is an independent copy of a random matrix S \u2208 Rn\u00d7p. Put \u00b5 = \u03bbmin(E[PA1/2S ]), and suppose the distribution of S satisfies \u00b5 > 0. Invoke Algorithm 1 with \u00b5 and \u03bd, where\n\u03bd = \u03bbmax\n(\nE\n[ (G\u22121/2HG\u22121/2)2 ]) , (13)\nwith H = S(STAS)\u2020ST and G = E[H]. Then with \u03c4 = \u221a\n\u00b5/\u03bd, for all k \u2265 0,\nE[\u2016yk \u2212 x\u2217\u2016A] \u2264 \u221a 2(1\u2212 \u03c4)k/2\u2016x0 \u2212 x\u2217\u2016A . (14)\nNote that in the setting of Theorem 3.5, by the definition of \u03bd and \u00b5, it is always the case that \u03bd \u2264 1/\u00b5. Therefore,\nthe iteration complexity of acceleration is at least as good as the iteration complexity without acceleration.\nWe conclude our discussion of Gauss-Seidel by describing the analogue of Proposition 3.3 for Algorithm 1, which shows that our analysis in Theorem 3.5 is tight order-wise. The following proposition applies to ACDM as well; we show in the full version of the paper how ACDM can be viewed as a special case of Algorithm 1.\nProposition 3.6. Under the setting of Theorem 3.5, there exists starting positions y0, z0 \u2208 Rn such that the iterates {(yk, zk)}k\u22650 produced by Algorithm 1 satisfy\nE[\u2016yk \u2212 x\u2217\u2016A + \u2016zk \u2212 x\u2217\u2016A] \u2265 (1\u2212 \u03c4)k\u2016y0 \u2212 x\u2217\u2016A ."}, {"heading": "3.2.2. ACCELERATED KACZMARZ", "text": "The argument for Theorem 3.5 can be slightly modified to yield a result for randomized accelerated Kaczmarz in the sketching framework, for the case of a consistent overdetermined linear system.\nSpecifically, suppose we are given an m\u00d7n matrix A which has full column rank, and b \u2208 R(A). Our goal is to recover the unique x\u2217 satisfying Ax\u2217 = b. To do this, we apply a similar line of reasoning as (Lee & Sidford, 2013). We set f(x) = 12\u2016x \u2212 x\u2217\u201622 and H = PATS , where S again is our random sketching matrix. At first, it appears our choice of f is problematic since we do not have access to f and \u2207f , but a quick calculation shows that H\u2207f(x) = (STA)\u2020ST(Ax \u2212 b). Hence, with rk = Axk \u2212 b, the sequence (10) simplifies to\nxk+1 = 1\n1 + \u03c4 yk +\n\u03c4\n1 + \u03c4 zk , (15a)\nyk+1 = xk+1 \u2212 (STkA)\u2020STk rk+1 , (15b) zk+1 = zk + \u03c4(xk+1 \u2212 zk)\u2212 \u03c4\n\u00b5 (STkA)\n\u2020STk rk+1 . (15c)\nThe remainder of the argument proceeds nearly identically, and leads to the following theorem.\nTheorem 3.7. Let A be an m \u00d7 n matrix with full column rank, and b = Ax\u2217. Suppose each Sk, k = 0, 1, 2, ... is an independent copy of a random sketching matrix S \u2208 Rm\u00d7p. Put H = PATS and G = E[H]. The sequence (15) with \u00b5 = \u03bbmin(E[PATS ]), \u03bd = \u03bbmax ( E [ (G\u22121/2HG\u22121/2)2 ]) , and \u03c4 = \u221a\n\u00b5/\u03bd satisfies for all k \u2265 0,\nE[\u2016yk \u2212 x\u2217\u20162] \u2264 \u221a 2(1\u2212 \u03c4)k/2\u2016x0 \u2212 x\u2217\u20162 . (16)\nSpecialized to the setting of (Liu & Wright, 2016) where each row of A has unit norm and is sampled uniformly at every iteration, it can be shown (Section A.5.1) that\n\u03bd \u2264 m and \u00b5 = 1m\u03bbmin(ATA). Hence, the above theorem states that the iteration complexity to reach \u03b5 er-\nror is O\n(\nm\u221a \u03bbmin(ATA) log(1/\u03b5)\n)\n, which matches Theo-\nrem 5.1 of (Liu & Wright, 2016) order-wise. However, Theorem 3.7 applies in general for any sketching matrix."}, {"heading": "3.3. Specializing accelerated Gauss-Seidel to random coordinate sampling", "text": "We now instantiate Theorem 3.5 to random coordinate sampling. The \u00b5 quantity which appears in Theorem 3.5 is identical to the quantity appearing in the rate (2) of nonaccelerated Gauss-Seidel. That is, the iteration complex-\nity to reach tolerance \u03b5 is O\n(\n\u221a\n\u03bd\u00b5\u22121rand log(1/\u03b5)\n)\n, and the\nonly new term here is \u03bd. In order to provide a more intuitive interpretation of the \u03bd quantity, we present an upper bound on \u03bd in terms of an effective block condition number defined as follows. Given an index set J \u2286 2[n], define the effective block condition number of a matrix A as \u03baeff,J(A) =\nmaxi\u2208J Aii \u03bbmin(AJ ) . Note that \u03baeff,J(A) \u2264 \u03ba(AJ) always. The following lemma gives upper and lower bounds on the \u03bd quantity. Lemma 3.8. Let A be an n\u00d7n positive definite matrix and let p satisfy 1 < p < n. We have that\nn p \u2264 \u03bd \u2264 n p\n(\np\u2212 1 n\u2212 1 +\n(\n1\u2212 p\u2212 1 n\u2212 1\n)\n\u03baeff,p(A)\n)\n,\nwhere \u03baeff,p(A) = maxJ\u22862[n]:|J|=p \u03baeff,J(A), \u03bd is defined in (13), and the distribution of S corresponds to uniformly selecting p coordinates without replacement.\nLemma 3.8 states that if the p \u00d7 p sub-blocks of A are well-conditioned as defined by the effective block condition number \u03baeff,J(A), then the speed-up of accelerated Gauss-Seidel with random coordinate selection over its non-accelerate counterpart parallels the case of fixed partitioning sampling (i.e. the rate described in (6) versus the rate in (2)). This is a reasonable condition, since very illconditioned sub-blocks will lead to numerical instabilities in solving the sub-problems when implementing GaussSeidel. On the other hand, we note that Lemma 3.8 provides merely a sufficient condition for speed-ups from acceleration, and is conservative. Our numerically experiments in Section A.7.2 suggest that in many cases the \u03bd parameter behaves closer to the lower bound n/p than Lemma 3.8 suggests.\nWe can now combine Theorem 3.5 with (5) to derive the following upper bound on the iteration complexity of accelerated Gauss-Seidel with random coordinates as\nNrand,acc \u2264 O ( n\np\n\u221a\nmax1\u2264i\u2264n Aii \u03bbmin(A) \u03baeff,p(A) log(1/\u03b5)\n)\n.\nIllustrative example. We conclude our results by illustrating our bounds on a simple example. Consider the subfamily {A\u03b4}\u03b4>0 \u2286 A , with\nA\u03b4 = An+\u03b4,\u2212n , \u03b4 > 0 . (17)\nA simple calculation yields that \u03baeff,p(A\u03b4) = n\u22121+\u03b4 n\u2212p+\u03b4 , and hence Lemma 3.8 states that \u03bd(A\u03b4) \u2264 np ( 1 + p\u22121n\u22121 ) . Furthermore, by a similar calculation to Proposition 3.2, \u00b5rand = p\u03b4\nn(n\u2212p+\u03b4) . Assuming for simplicity that p =\no(n) and \u03b4 \u2208 (0, 1), Theorem 3.5 states that at most O(n 3/2\np \u221a \u03b4 log(1/\u03b5)) iterations are sufficient for an \u03b5-accurate solution. On the other hand, without acceleration (2) states that O(n 2\np\u03b4 log(1/\u03b5)) iterations are sufficient and Proposition 3.3 shows there exists a starting position for which it is necessary. Hence, as either n grows large or \u03b4 tends to zero, the benefits of acceleration become more pronounced."}, {"heading": "4. Related Work", "text": "We split the related work into two broad categories of interest: (a) work related to coordinate descent (CD) methods on convex functions and (b) randomized solvers designed for solving consistent linear systems.\nWhen A is positive definite, Gauss-Seidel can be interpreted as an instance of coordinate descent on a strongly convex quadratic function. We therefore review related work on both non-accelerated and accelerated coordinate descent, focusing on the randomized setting instead of the more classical cyclic order or Gauss-Southwell rule for selecting the next coordinate. See (Tseng & Yun, 2009) for a discussion on non-random selection rules, (Nutini et al., 2015) for a comparison of random selection versus GaussSouthwell, and (Nutini et al., 2016) for efficient implementations of Gauss-Southwell.\nNesterov\u2019s original paper in (2012) first considered randomized CD on convex functions, assuming a partitioning of coordinates fixed ahead of time. The analysis included both non-accelerated and accelerated variants for convex functions. This work sparked a resurgence of interest in CD methods for large problems. Most relevant to our paper are extensions to the block setting (Richta\u0301rik & Taka\u0301c\u0306, 2014), handling arbitrary sampling distributions (Qu & Richta\u0301rik, 2014a;b; Fountoulakis & Tappenden, 2016), and second order updates for quadratic functions (Qu et al., 2016).\nFor accelerated CD, Lee and Sidford (2013) generalize the analysis of Nesterov (2012). While the analysis of (Lee & Sidford, 2013) was limited to selecting a single coordinate at a time, several follow on works (Qu & Richta\u0301rik, 2014a; Lin et al., 2014; Lu & Xiao, 2015; Fercoq & Richta\u0301rik, 2015) generalize to block and non-smooth settings. More recently, both Allen-Zhu et al. (2016) and Nesterov and\nStich (2016) independently improve the results of (Lee & Sidford, 2013) by using a different non-uniform sampling distribution. One of the most notable aspects of the analysis in (Allen-Zhu et al., 2016) is a departure from the (probabilistic) estimate sequence framework of Nesterov. Instead, the authors construct a valid Lyapunov function for coordinate descent, although they do not explicitly mention this. In our work, we make this Lyapunov point of view explicit. The constants in our acceleration updates arise from a particular discretization and Lyapunov function outlined from Wilson et al. (2016). Using this framework makes our proof particularly transparent, and allows us to recover results for strongly convex functions from (Allen-Zhu et al., 2016) and (Nesterov & Stich, 2016) as a special case.\nFrom the numerical analysis side both the Gauss-Seidel and Kaczmarz algorithm are classical methods. Strohmer and Vershynin (2009) were the first to prove a linear rate of convergence for randomized Kaczmarz, and Leventhal and Lewis (2010) provide a similar kind of analysis for randomized Gauss-Seidel. Both of these were in the single constraint/coordinate setting. The block setting was later analyzed by Needell and Tropp (2014). More recently, Gower and Richta\u0301rik (2015) provide a unified analysis for both randomized block Gauss-Seidel and Kaczmarz in the sketching framework. We adopt this framework in this paper. Finally, Liu and Wright (2016) provide an accelerated analysis of randomized Kaczmarz once again in the single constraint setting and we extend this to the block setting."}, {"heading": "5. Experiments", "text": "In this section we experimentally validate our theoretical results on how our accelerated algorithms can improve convergence rates. Our experiments use a combination of synthetic matrices and matrices from large scale machine learning tasks.\nSetup. We run all our experiments on a 4 socket Intel Xeon CPU E7-8870 machine with 18 cores per socket and 1TB of DRAM. We implement all our algorithms in Python using numpy, and use the Intel MKL library with 72 OpenMP threads for numerical operations. We report errors as relative errors, i.e. \u2016xk \u2212 x\u2217\u20162A/\u2016x\u2217\u20162A. Finally, we use the best values of \u00b5 and \u03bd found by tuning each experiment.\nWe implement fixed partitioning by creating random blocks of coordinates at the beginning of the experiment and cache the corresponding matrix blocks to improve performance. For random coordinate sampling, we select a new block of coordinates at each iteration.\nFor our fixed partition experiments, we restrict our attention to uniform sampling. While Gower and Richta\u0301rik (2015) propose a non-uniform scheme based on Tr(STAS), for translation-invariant kernels this reduces to\nuniform sampling. Furthermore, as the kernel block Lipschitz constants were also roughly the same, other nonuniform schemes (Allen-Zhu et al., 2016) also reduce to nearly uniform sampling."}, {"heading": "5.1. Fixed partitioning vs random coordinate sampling", "text": "Our first set of experiments numerically verify the separation between fixed partitioning sampling versus random coordinate sampling.\nFigure 1 shows the progress per iteration on solving A1,\u03b2x = b, with the A1,\u03b2 defined in Section 3.1. Here we set n = 5000, p = 500, \u03b2 = 1000, and b \u223c N(0, I). Figure 1 verifies our analytical findings in Section 3.1, that the fixed partition scheme is substantially worse than uniform sampling on this instance. It also shows that in this case, acceleration provides little benefit in the case of random coordinate sampling. This is because both \u00b5 and 1/\u03bd are order-wise p/n, and hence the rate for accelerated and non-accelerated coordinate descent coincide. However we note that this only applies for matrices where \u00b5 is as large as it can be (i.e. p/n), that is instances for which GaussSeidel is already converging at the optimal rate (see (Gower & Richta\u0301rik, 2015), Lemma 4.2)."}, {"heading": "5.2. Kernel ridge regression", "text": "We next evaluate how fixed partitioning and random coordinate sampling affects the performance of Gauss-Seidel on large scale machine learning tasks. We use the popular image classification dataset CIFAR-10 and evaluate a kernel ridge regression (KRR) task with a Gaussian kernel. Specifically, given a labeled dataset {(xi, yi)}ni=1, we solve the linear system (K + \u03bbI)\u03b1 = Y with Kij = exp(\u2212\u03b3\u2016xi \u2212 xj\u201622), where \u03bb, \u03b3 > 0 are tunable parameters. The key property of KRR is that the kernel matrix K is positive semi-definite, and hence Algorithm 1 applies."}, {"heading": "GS Fixed Partition GS-Acc Fixed Partition", "text": ""}, {"heading": "GS Fixed Partition GS-Acc Fixed Partition", "text": "For the CIFAR-10 dataset, we augment the dataset1 to include five reflections, translations per-image and then apply standard pre-processing steps used in image classification (Coates & Ng, 2012; Sparks et al., 2017). We finally apply a Gaussian kernel on our pre-processed images and the resulting kernel matrix has n = 250000 coordinates. We also include experiments on a smaller MNIST kernel matrix (n = 60000) in Section A.7.\nResults from running 500 iterations of random coordinate sampling and fixed partitioning algorithms are shown in Figure 2. Comparing convergence across iterations, similar to previous section, we see that un-accelerated GaussSeidel with random coordinate sampling is better than accelerated Gauss-Seidel with fixed partitioning. However we also see that using acceleration with random sampling can further improve the convergence rates, especially to achieve errors of 10\u22123 or lower.\n1Similar to https://github.com/akrizhevsky/cuda-convnet2.\nWe also compare the convergence with respect to running time in Figure 3. Fixed partitioning has better performance in practice random access is expensive in multi-core systems. However, we see that this speedup in implementation comes at a substantial cost in terms of convergence rate. For example in the case of CIFAR-10, using fixed partitions leads to an error of 1.2\u00d7 10\u22122 after around 7000 seconds. In comparison we see that random coordinate sampling achieves a similar error in around 4500 seconds and is thus 1.5\u00d7 faster. We also note that this speedup increases for lower error tolerances."}, {"heading": "5.3. Comparing Gauss-Seidel to Conjugate-Gradient", "text": "We also compared Gauss-Seidel with random coordinate sampling to the classical conjugate-gradient (CG) algorithm. CG is an important baseline to compare with, as it is the de-facto standard iterative algorithm for solving linear systems in the numerical analysis community. While we report the results of CG without preconditioning, we remark that the performance using a standard banded preconditioner was not any better. However, for KRR specifically, there have been recent efforts (Avron et al., 2017; Rudi et al., 2017) to develop better preconditioners, and we leave a more thorough comparison for future work. The results of our experiment are shown in Figure 3. We note that Gauss-Seidel both with and without acceleration outperform CG. As an example, we note that to reach error 10\u22121 on CIFAR-10, CG takes roughly 7000 seconds, compared to less than 2000 seconds for accelerated GaussSeidel, which is a 3.5\u00d7 improvement. To understand this performance difference, we recall that our matrices A are fully dense, and hence each iteration of CG takes O(n2). On the other hand, each iteration of both non-accelerated and accelerated Gauss-Seidel takes O(np2 + p3). Hence, as long as p = O(n2/3), the time per iteration of Gauss-Seidel is order-wise no worse than\nCG. In terms of iteration complexity, standard results state that CG takes at most O( \u221a \u03ba log(1/\u03b5)) iterations to reach an \u03b5 error solution, where \u03ba denotes the condition number of A. On the other hand, Gauss-Seidel takes at most O(np\u03baeff log(1/\u03b5)), where \u03baeff = max1\u2264i\u2264n Aii \u03bbmin(A) . In the case of any (normalized) kernel matrix associated with a translation-invariant kernel such as the Gaussian kernel, we have max1\u2264i\u2264n Aii = 1, and hence generally speaking \u03baeff is much lower than \u03ba."}, {"heading": "5.4. Effect of block size", "text": "We next analyze the importance of the block size p for the accelerated Gauss-Seidel method. As the values of \u00b5 and \u03bd change for each setting of p, we use a smaller MNIST matrix for this experiment. We apply a random feature transformation (Rahimi & Recht, 2007) to generate an n \u00d7 d matrix F with d = 5000 features. We then use A = FTF and b = FTY as inputs to the algorithm. Figure 4 shows the wall clock time to converge to 10\u22125 error as we vary the block size from p = 50 to p = 1000.\nIncreasing the block-size improves the amount of progress that is made per iteration but the time taken per iteration increases as O(p3) (Line 5, Algorithm 1). However, using efficient BLAS-3 primitives usually affords a speedup from systems techniques like cache blocking. We see the effects of this in Figure 4 where using p = 500 performs better than using p = 50. We also see that these benefits reduce for much larger block sizes and thus p = 1000 is slower."}, {"heading": "6. Conclusion", "text": "In this paper, we extended the accelerated block GaussSeidel algorithm beyond fixed partition sampling. Our analysis introduced a new data-dependent parameter \u03bd which governs the speed-up of acceleration. Specializing our theory to random coordinate sampling, we derived an upper bound on \u03bd which shows that well conditioned blocks are a sufficient condition to ensure speedup. Experimentally, we showed that random coordinate sampling is readily accelerated beyond what our bound suggests.\nThe most obvious question remains to derive a sharper bound on the \u03bd constant from Theorem 3.5. Another interesting question is whether or not the iteration complexity of random coordinate sampling is always bounded above by the iteration complexity with fixed coordinate sampling.\nWe also plan to study an implementation of accelerated Gauss-Seidel in a distributed setting (Tu et al., 2016). The main challenges here are in determining how to sample coordinates without significant communication overheads, and to efficiently estimate \u00b5 and \u03bd. To do this, we wish to explore other sampling schemes such as shuffling the coordinates at the end of every epoch (Recht & Re\u0301, 2013)."}, {"heading": "Acknowledgements", "text": "We thank Ross Boczar for assisting us with Mathematica support for non-commutative algebras, Orianna DeMasi for providing useful feedback on earlier drafts of this manuscript, and the anonymous reviewers for their helpful feedback. ACW is supported by an NSF Graduate Research Fellowship. BR is generously supported by ONR awards N00014-11-1-0723 and N00014-13-1-0129, NSF award CCF-1359814, the DARPA Fundamental Limits of Learning (Fun LoL) Program, a Sloan Research Fellowship, and a Google Research Award. This research is supported in part by DHS Award HSHQDC-16-3-00083, NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, HP, Huawei, Intel, Microsoft, Mitre, Pivotal, Samsung, Schlumberger, Splunk, State Farm and VMware."}], "year": 2017, "references": [{"title": "Even Faster Accelerated Coordinate Descent Using Non-Uniform Sampling", "authors": ["Allen-Zhu", "Zeyuan", "Richt\u00e1rik", "Peter", "Qu", "Zheng", "Yuan", "Yang"], "venue": "In ICML,", "year": 2016}, {"title": "Faster Kernel Ridge Regression Using Sketching and Preconditioning", "authors": ["Avron", "Haim", "Clarkson", "Kenneth L", "Woodruff", "David P"], "year": 2017}, {"title": "Learning Feature Representations with K-Means", "authors": ["Coates", "Adam", "Ng", "Andrew Y"], "venue": "In Neural Networks: Tricks of the Trade. Springer,", "year": 2012}, {"title": "Accelerated, Parallel, and Proximal Coordinate Descent", "authors": ["Fercoq", "Olivier", "Richt\u00e1rik", "Peter"], "venue": "SIAM J. Optim.,", "year": 2015}, {"title": "A Flexible Coordinate", "authors": ["Fountoulakis", "Kimon", "Tappenden", "Rachael"], "venue": "Descent Method. arXiv,", "year": 2016}, {"title": "Randomized Iterative Methods for Linear Systems", "authors": ["Gower", "Robert M", "Richt\u00e1rik", "Peter"], "venue": "SIAM Journal on Matrix Analysis and Applications,", "year": 2015}, {"title": "Random Permutations Fix a Worst Case for Cyclic Coordinate Descent", "authors": ["Lee", "Ching-Pei", "Wright", "Stephen J"], "year": 2016}, {"title": "Efficient Accelerated Coordinate Descent Methods and Faster Algorithms for Solving Linear Systems", "authors": ["Lee", "Yin Tat", "Sidford", "Aaron"], "venue": "In FOCS,", "year": 2013}, {"title": "An Accelerated Proximal Coordinate Gradient Method", "authors": ["Lin", "Qihang", "Lu", "Zhaosong", "Xiao"], "venue": "In NIPS,", "year": 2014}, {"title": "An Accelerated Randomized Kaczmarz Algorithm", "authors": ["Liu", "Ji", "Wright", "Stephen J"], "venue": "Mathematics of Computation,", "year": 2016}, {"title": "On the Complexity Analysis of Randomized Block-Coordinate Descent Methods", "authors": ["Lu", "Zhaosong", "Xiao", "Lin"], "venue": "Mathematical Programming,", "year": 2015}, {"title": "Paved with Good Intentions: Analysis of a Randomized Block Kaczmarz Method", "authors": ["Needell", "Deanna", "Tropp", "Joel A"], "venue": "Linear Algebra and its Applications,", "year": 2014}, {"title": "Efficiency of Coordinate Descent Methods on Huge-Scale Optimization Problems", "authors": ["Nesterov", "Yurii"], "venue": "SIAM J. Optim.,", "year": 2012}, {"title": "Efficiency of Accelerated Coordinate Descent Method on Structured Optimization Problems", "authors": ["Nesterov", "Yurii", "Stich", "Sebastian"], "venue": "Technical report, Universite\u0301 catholique de Louvain, CORE Discussion Papers,", "year": 2016}, {"title": "Coordinate Descent Converges Faster with the Gauss-Southwell Rule Than Random Selection", "authors": ["Nutini", "Julie", "Schmidt", "Mark", "Laradji", "Issam H", "Friedlander", "Michael", "Koepke", "Hoyt"], "venue": "In ICML,", "year": 2015}, {"title": "Convergence Rates for Greedy Kaczmarz Algorithms, and Faster Randomized Kaczmarz Rules Using the Orthogonality Graph", "authors": ["Nutini", "Julie", "Sepehry", "Behrooz", "Laradji", "Issam", "Schmidt", "Mark", "Koepke", "Hoyt", "Virani", "Alim"], "year": 2016}, {"title": "Coordinate Descent with Arbitrary Sampling I", "authors": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": "Algorithms and Complexity. arXiv,", "year": 2014}, {"title": "Coordinate Descent with Arbitrary Sampling II: Expected", "authors": ["Qu", "Zheng", "Richt\u00e1rik", "Peter"], "venue": "Separable Overapproximation. arXiv,", "year": 2014}, {"title": "Randomized Dual Coordinate Ascent with Arbitrary Sampling", "authors": ["Qu", "Zheng", "Richt\u00e1rik", "Peter", "Zhang", "Tong"], "venue": "In NIPS,", "year": 2015}, {"title": "SDNA: Stochastic Dual Newton Ascent for Empirical Risk Minimization", "authors": ["Qu", "Zheng", "Richt\u00e1rik", "Peter", "Tak\u00e1c", "Martin", "Fercoq", "Olivier"], "venue": "In ICML,", "year": 2016}, {"title": "Random Features for Large-Scale Kernel Machines", "authors": ["Rahimi", "Ali", "Recht", "Benjamin"], "venue": "In NIPS,", "year": 2007}, {"title": "Parallel Stochastic Gradient Algorithms for Large-Scale Matrix Completion", "authors": ["Recht", "Benjamin", "R\u00e9", "Christopher"], "venue": "Mathematical Programming Computation,", "year": 2013}, {"title": "Iteration Complexity of Randomized Block-Coordinate Descent Methods for Minimizing a Composite Function", "authors": ["Richt\u00e1rik", "Peter", "Tak\u00e1c", "Martin"], "venue": "Mathematical Programming,", "year": 2014}, {"title": "FALKON: An Optimal Large Scale", "authors": ["Rudi", "Alessandro", "Carratino", "Luigi", "Rosasco", "Lorenzo"], "venue": "Kernel Method. arXiv,", "year": 2017}, {"title": "KeystoneML: Optimizing Pipelines for Large-Scale Advanced Analytics", "authors": ["Sparks", "Evan R", "Venkataraman", "Shivaram", "Kaftan", "Tomer", "Franklin", "Michael", "Recht", "Benjamin"], "year": 2017}, {"title": "A Randomized Kaczmarz Algorithm with Exponential Convergence", "authors": ["Strohmer", "Thomas", "Vershynin", "Roman"], "venue": "Journal of Fourier Analysis and Applications,", "year": 2009}, {"title": "A Coordinate Gradient Descent Method for Nonsmooth Separable Minimization", "authors": ["Tseng", "Paul", "Yun", "Sangwoon"], "venue": "Mathematical Programming,", "year": 2009}, {"title": "Large Scale Kernel Learning using Block Coordinate Descent", "authors": ["Tu", "Stephen", "Roelofs", "Rebecca", "Venkataraman", "Shivaram", "Recht", "Benjamin"], "year": 2016}, {"title": "Breaking Locality Accelerates", "authors": ["Tu", "Stephen", "Venkataraman", "Shivaram", "Wilson", "Ashia C", "Gittens", "Alex", "Jordan", "Michael I", "Recht", "Benjamin"], "venue": "Block GaussSeidel. arXiv,", "year": 2017}], "id": "SP:1014532b75ac9a81291983b5e717693abc5031f6", "authors": [{"name": "Stephen Tu", "affiliations": []}, {"name": "Shivaram Venkataraman", "affiliations": []}, {"name": "Ashia C. Wilson", "affiliations": []}, {"name": "Alex Gittens", "affiliations": []}, {"name": "Michael I. Jordan", "affiliations": []}, {"name": "Benjamin Recht", "affiliations": []}], "abstractText": "Recent work by Nesterov and Stich (2016) showed that momentum can be used to accelerate the rate of convergence for block GaussSeidel in the setting where a fixed partitioning of the coordinates is chosen ahead of time. We show that this setting is too restrictive, constructing instances where breaking locality by running non-accelerated Gauss-Seidel with randomly sampled coordinates substantially outperforms accelerated Gauss-Seidel with any fixed partitioning. Motivated by this finding, we analyze the accelerated block Gauss-Seidel algorithm in the random coordinate sampling setting. Our analysis captures the benefit of acceleration with a new data-dependent parameter which is well behaved when the matrix subblocks are well-conditioned. Empirically, we show that accelerated Gauss-Seidel with random coordinate sampling provides speedups for large scale machine learning tasks when compared to non-accelerated Gauss-Seidel and the classical conjugate-gradient algorithm.", "title": "Breaking Locality Accelerates Block Gauss-Seidel"}