6	15	To demonstrate the power of depth in neural networks, a clean and precise approach is to prove the existence of functions which can be expressed (or well-approximated) by moderately-sized networks of a given depth, yet cannot be approximated well by shallower networks, even if their size is much larger.
7	10	However, the mere existence of such functions is not enough: Ideally, we would like to show such depth separation results using natural, interpretable functions, of the type we may expect neural networks to successfully train on.
9	39	In this paper, we provide several contributions to this emerging line of work.
11	17	In this setting, we show the following: • We prove that the indicator of the Euclidean unit ball, x 7→ 1 (‖x‖ ≤ 1) in Rd, which can be easily approximated to accuracy using a 3-layer network with O(d2/ ) neurons, cannot be approximated to an accuracy higher than O(1/d4) using a 2-layer network, unless its width is exponential in d. In fact, we show the same result more generally, for any indicator of an ellipsoid x 7→ 1 (‖Ax + b‖ ≤ r) (where A is a non-singular matrix and b is a vector).
17	18	This experiment also highlights the fact that our separation result is for a natural function that is not just well-approximated by some 3-layer network, but can also be learned well from data using standard methods.
18	14	• Finally, we prove that any member of a wide family of non-linear and twice-differentiable functions (including for instance x 7→ x2 in [0, 1]), which can be approximated to accuracy using ReLU networks of depth and width O(poly(log(1/ ))), cannot be approximated to similar accuracy by constantdepth ReLU networks, unless their width is at least Ω(poly(1/ )).
19	38	We note that a similar result appeared online concurrently and independently of ours in (Yarotsky, 2016; Liang & Srikant, 2016), but the setting is a bit different (see related work below for more details).
23	11	We note that the latter distinction is important: Although L∞ bounds are more common in the approximation theory literature, L2 bounds are more natural in the context of statistical machine learning problems (where we care about the expected loss over some distribution).
28	14	On the flip side, in our paper we focus on separation in terms of the accuracy or dimension, rather than a parameter k. Moreover, the construction there relies on a highly oscillatory function, with Lipschitz constant exponential in k almost everywhere.
67	18	1 (provided below) is based on a reduction from the main result of (Eldan & Shamir, 2016), which shows the existence of a certain radial function (depending on the input x only through its norm) and a probability distribution which cannot be expressed by a 2-layer network, whose width is less than exponential in the dimension d to more than constant accuracy.
70	22	By a linear transformation argument, we show the same contradiction would have occured if we could have approximated the indicators of an non-degenerate ellipse with respect to any distribution.
93	11	Finally, with a 3-layer network, we can use the second layer to compute a continuous approximation to the threshold function z 7→ 1 (z ≤ r).
102	15	For our experiment, we sampled 5 · 105 data instances in R100, with a direction chosen uniformly at random and a norm drawn uniformly at random from the interval [0, 2].
105	26	We trained 5 ReLU networks on this dataset: • One 3-layer network, with a first hidden layer of size 100, a second hidden layer of size 20, and a linear output neuron.
117	10	Having considered functions depending on the L2 norm, we now turn to consider functions depending on the L1 norm.
120	24	Then there exists a distribution γ over {x : ‖x‖1 ≤ (1 + )r}, such that if a 2-layer ReLU network F (x) satisfies∫ x (f(‖x‖1)− F (x)) 2 γ(x)dx ≤ δ/2, then its width must be at least Ω̃(min {1/ , exp(Ω(d))}) (where the Ω̃ notation hides constants and factors logarithmic in , d).
125	42	On the flip side, f(‖x‖1) can be expressed exactly by a 3-layer, width 2d ReLU network: x 7→ [ ∑d i=1([xi]++[−xi]+)−1]+, where the output neuron is simply the identity function.
128	14	Intuitively, the proof relies on showing that any good 2- layer approximation of f(‖x‖1) must capture the nonlinear behavior of f close to “most” points x satisfying ‖x‖1 ≈ r. However, a 2-layer ReLU network x 7→∑N j=1 aj [〈wj ,x〉+ bj ]+ is piecewise linear, with nonlinearities only at the union of the N hyperplanes ∪j{x : 〈wj ,x〉 + bj = 0}.
137	22	Finally, we performed an experiment similar to the one presented in Subsection 3.1, where we verified that the bounds we derived are indeed reflected in differences in empirical performance, when training 2-layer nets versus 3-layer nets.
144	17	Instead, we consider functions which have a certain degree of non-linearity, in the sense that its Hessians are non-zero along some direction, on a significant portion of the domain.
156	15	First, we show that any strictly curved function (in a sense similar to Definition 1) cannot be well-approximated in an L2 sense by piecewise linear functions, unless the number of linear regions is large.
158	61	We then prove a result specific to the one-dimensional case, including an explicit lower bound if the target function is quadratic (Thm.
161	11	Finally, we note that any ReLU network induces a piecewise-linear function, and bound the number of linear regions induced by a ReLU network of a given width and depth (using a lemma borrowed from (Telgarsky, 2016)).
162	39	Combining this with the previous lower bound yields Thm.
163	10	We now turn to complement this lower bound with an approximability result, showing that with more depth, a wide family of functions to which Thm.
164	30	4 applies can be approximated with exponentially high accuracy.
165	181	Specifically, we consider functions which can be approximated using a moderate number of multiplications and additions, where the values of intermediate computations are bounded (for example, a special case is any function approximable by a moderately-sized Boolean circuit, or a polynomial).
166	26	The key result to show this is the following, which implies that the multiplication of two (bounded-size) numbers can be approximated by a ReLU network, with error decaying exponentially with depth: Theorem 5.
167	28	Let f : [−M,M ]2 → R, f (x, y) = x · y and let > 0 be arbitrary.
169	20	The idea of the construction is that depth allows us to compute highly-oscillating functions, which can extract highorder bits from the binary representation of the inputs.
