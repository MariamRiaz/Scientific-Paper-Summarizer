0	49	Named entity recognition (NER) is an important NLP task that automatically detects entities in text and classifies them into pre-defined entity types such as persons, organizations, geopolitical entities, locations, events, etc.
1	20	NER is a fundamental component of many information extraction and knowledge discovery applications, including relation extraction, entity linking, question answering and data mining.
5	32	However, a model could still make mistakes if its features favor a wrong entity type, which happens more frequently for unseen entities as we have observed in our experiments.
16	28	The second class of approaches use Wikipedia to generate weakly annotated data for training multilingual NER systems, e.g., (Richman and Schone, 2008; Nothman et al., 2013).
24	20	In this paper, we propose a new class of approaches that utilize Wikipedia to improve multilingual NER systems.
47	22	The system has 51 entity types, and the main motivation of deploying such a fine-grained entity type set is to build cognitive question answering applications on top of the NER system.
74	30	Notice that the automatically generated classifier training and test data are weakly labeled since the EL system may link an entity to a wrong Wikipedia page and thus the entity type assigned to that page could be wrong.
76	41	To evaluate the prediction power of different types of features, we train a number of classifiers using only title features, only infobox features, only text features, and all features respectively.
79	19	From Table 1, we can see that text features are the most important features for classifying Wikipedia pages, since the classifier trained with only text features achieves an overall F1 score of 87.2, which is better than the classifier trained with either title or infobox features alone.
85	47	We add the self-decoded Wikipedia pages with high confidence scores to the training data and train a new classifier.
94	22	As shown in Table 2, the classification accuracy under the sampling-based approach is further improved to 91.8 F1 score (the improvement is calculated by averaging over 5 random samples with q = 0.01).
95	19	We construct an English Wikipedia entity type mapping by applying the English Wikipedia entity type classifier on all the English Wikipedia pages (∼4.6M).
102	27	Therefore, we use EnglishWiki-Mapping(t, i) to denote the English Wikipedia entity type mapping that includes all the entities with confidence scores greater than or equal to t and at least i words in their names.
109	24	The rationale is that both the English and Portuguese pages are describing the same entity, even probably with different spelling (e.g., United States in English vs. Estados Unidos in Portuguese), the entity type of that entity does not change from one language to another.
120	28	We use Portuguese-Wiki-Mapping(t) to denote the mapping that includes entities with confidence scores greater than or equal to a thresh- old t. There are 525K entities in Portuguese-WikiMapping(0.9), which covers about 57% of all the Portuguese Wikipedia pages, a significant improvement of coverage compared to the direct projection approach (15%).
129	35	Under this approach, the mapping is applied as a constraint during the decoding procedure: if a sequence of words in the text form an entity name x that is included in the mapping, i.e., |M(x)| ≥ 1, then the sequence of words will be identified as an entity, and its entity type is determined by the decoding algorithm while being constrained to one of the entity types inM(x).
130	40	The second approach is to use a Wikipedia entity type mapping M to post-process the output of an NER system.
136	23	In contrast, the post-processing approach is a more conservative approach since it relies on the system entity boundaries and only changes their entity types if determined by the mapping, so it will not create new entities.
138	55	Based on the observation that the decoding constraint approach is more reliable for longer entities while the post-processing approach can better handle short entities, we have designed a joint approach that combines the two approaches as follows: it first applies Wiki-Mapping(0.9,3) as a decoding constraint for an NER system to produce system entities, and then applies Wiki-Mapping(0.9,2) to post-process the system output.
143	19	Via experiments we find that using Wiki-Mapping(0.9,1) or Wiki-Mapping(0.9,2) achieves the best improvement under the dictionary feature approach.
145	38	For each language, we compare the baseline NER system with the following approaches: • DC(i): the decoding constraint approach with mapping Language-Wiki-Mapping(0.9,i).
149	25	To evaluate the generalization capability of an NER system, we compute the F1 score on the unseen entities (Unseen) as well as on all the entities (All) in a test data set.
152	25	We have two human-annotated test data sets: the first set, Test (News), consists of 40K tokens of human-annotated news articles; and the second set, Test (Political), consists of 77K tokens of humanannotated political party articles from Wikipedia.
154	33	For Test (News) which is in the same domain as the training data, the baseline system achieves 88.2 F1 score on all the entities, and a relatively low F1 score of 78.7 on the unseen entities (38% of all the entities are unseen entities).
167	23	The baseline NER system is an MEMM model (CRF cannot handle such a big size of training data, since our NER system has 51 entity types, and the number of features and training time of CRF grow at least quadratically in the number of entity types).
171	26	In this case, the more aggressive decoding constraint approach DC(2) achieves the best improvement among the Wikipedia-based approaches, which improves the baseline by 5.9 F1 score on all the entities and by 8.6 F1 score on the unseen entities.
195	55	• In the rich-resource scenario where an NER system is well trained (e.g., 200K-300K tokens of training data for the English, Dutch and German systems), the dictionary feature approach, which uses a Wikipedia entity type mapping to create dictionary type features, achieves the best improvement.
196	37	• In both scenarios, the joint approach, which combines the decoding constraint approach and the post-processing approach in a smart way, achieves relatively robust performance among the Wikipedia-based approaches.
200	45	When a system is well trained, the dictionary feature approach achieves the best improvement over the baseline system; while when a system is trained with little human-annotated training data, a more aggressive decoding constraint approach achieves the best improvement.
201	56	The improvements are larger on unseen entities, and the approaches are especially useful when a system is applied to a new domain or it is trained with little training data.
