{"sections": [{"text": "In this work, we study the robust subspace tracking (RST) problem and obtain one of the first two provable guarantees for it. The goal of RST is to track sequentially arriving data vectors that lie in a slowly changing low-dimensional subspace, while being robust to corruption by additive sparse outliers. It can also be interpreted as a dynamic (time-varying) extension of robust PCA (RPCA), with the minor difference that RST also requires a short tracking delay. We develop a recursive projected compressive sensing algorithm that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal. We prove that NORST solves both the RST and the dynamic RPCA problems under weakened standard RPCA assumptions, two simple extra assumptions (slow subspace change and most outlier magnitudes lower bounded), and a few minor assumptions.\nOur guarantee shows that NORST enjoys a near optimal tracking delay of O(r log n log(1/ )). Its required delay between subspace change times is the same, and its memory complexity is n times this value. Thus both these are also nearly optimal. Here n is the ambient space dimension, r is the subspaces\u2019 dimension, and is the tracking accuracy. NORST also has the best outlier tolerance compared with all previous RPCA or RST methods, both theoretically and empirically (including for real videos), without requiring any model on how the outlier support is generated. This is possible because of the extra assumptions it uses."}, {"heading": "1 Introduction", "text": "Principal Components Analysis (PCA) is one of the most widely used dimension reduction techniques. It finds a small number of orthogonal basis vectors, called principal components, along which most of the variability of the dataset lies. According to its modern definition [3], robust PCA (RPCA) is the problem of decomposing a given data matrix into the sum of a low-rank matrix (true data) and a sparse matrix (outliers). The column space of the low-rank matrix then gives the desired principal subspace (PCA solution). A common application of RPCA is in video analytics in separating a video into a slow-changing background image sequence (modeled as a low-rank matrix) and a foreground image sequence consisting of moving objects or people (sparse) [3]. Robust Subspace Tracking (RST) can be simply interpreted as a time-varying extension of RPCA. It assumes that the true data lies in a low-dimensional subspace that can change with time, albeit slowly. The goal is to track this changing subspace over time in the presence of additive sparse outliers. The offline version of this problem can be called dynamic (or time-varying) RPCA. RST requires the tracking delay to be small, while dynamic RPCA does not. Time-varying subspace is a more appropriate model for long data sequences, e.g., long surveillance videos, since if a\n\u2217A shorter version of this manuscript [1] will be presented at ICML, 2018. Another small part, Corollary 5.18, will appear\nin [2].\nar X\niv :1\n71 2.\n06 06\n1v 4\n[ cs\n.I T\n] 6\nJ ul\n2 01\nsingle subspace model is used the resulting matrix may not be sufficiently low-rank. Moreover the RST problem setting (short tracking delay) is most relevant for applications where real-time or near real-time estimates are needed, e.g., video-based surveillance (object tracking) [4], monitoring seismological activity [5], or detection of anomalous behavior in dynamic social networks [6].\nIn recent years, RPCA has since been extensively studied. Many fast and provably correct approaches now exist: PCP introduced in [3] and studied in [3, 7, 8], AltProj [9], RPCA-GD [10] and NO-RMC [11]. There is much lesser work on provable dynamic RPCA and RST: original-ReProCS [12, 13, 14] for dynamic RPCA and simple-ReProCS [15] for both. The subspace tracking (ST) problem (without outliers), and with or without missing data, has been studied for much longer in [16, 17, 18, 19]. However, all existing guarantees for it only consider the statistically stationary setting of data being generated from a single unknown subspace. Of course, the most general nonstationary model that allows the subspace to change at each time is not even identifiable since at least r data points are needed to compute an r-dimensional subspace even in the no noise or missing entries case.\nIn this work, we make the subspace tracking problem identifiable by assuming a piecewise constant model on subspace change. We show that it is possible to track the changing subspace to within accuracy as long as the subspace remains constant for at least O(r log n log(1/ )) time instants, and some other assumptions hold. This is more than r by only log factors. Here n is the ambient space dimension.\nNotation. We use the interval notation [a, b] to refer to all integers between a and b, inclusive, and we use [a, b) := [a, b \u2212 1]. \u2016.\u2016 denotes the l2 norm for vectors and induced l2 norm for matrices unless specified otherwise, and \u2032 denotes transpose. We use MT to denote a sub-matrix of M formed by its columns indexed by entries in the set T . For a matrix P we use P (i) to denote its i-th row. In our algorithm statements, we use L\u0302t;\u03b1 := [ \u02c6\u0300t\u2212\u03b1+1, \u00b7 \u00b7 \u00b7 , \u02c6\u0300t] and SV Dr[M ] to refer to the matrix of top of r left singular vectors of the matrix M . A matrix P with mutually orthonormal columns is referred to as a basis matrix and is used to represent the subspace spanned by its columns. For basis matrices P1,P2, we use SE(P1,P2) := \u2016(I \u2212 P1P1\u2032)P2\u2016 as a measure of Subspace Error (distance) between their respective subspaces. This is equal to the sine of the largest principal angle between the subspaces. It is also called \u201cprojection distance\u201d [20]. If P1 and P2 are of the same dimension, SE(P1,P2) = SE(P2,P1).\nWe reuse the letters C, c to denote different numerical constants in each use. Robust Subspace Tracking (RST) and Dynamic RPCA Problem Setting. At each time t,\nwe get a data vector yt \u2208 Rn that satisfies\nyt := `t + xt + \u03bdt, for t = 1, 2, . . . , d\nwhere \u03bdt is small unstructured noise, xt is the sparse outlier vector, and `t is the true data vector that lies in a fixed or slowly changing low-dimensional subspace of Rn, i.e., `t = P(t)at where P(t) is an n\u00d7 r basis matrix with r n and with \u2016(I \u2212 P(t\u22121)P(t\u22121)\u2032)P(t)\u2016 small compared to \u2016P(t)\u2016 = 1. We use Tt to denote the support set of xt. Given an initial subspace estimate, P\u03020, the goal is to track span(P(t)) and `t either immediately or within a short delay. A by-product is that `t, xt, and Tt can also be tracked on-the-fly. The initial subspace estimate, P\u03020, can be computed by applying any of the existing RPCA solutions, e.g., PCP or AltProj, for the first roughly r data points, i.e., for Y[1,ttrain], with ttrain = Cr.\nDynamic RPCA is the offline version of the above problem. Define matrices L,X,W ,Y with L = [`1, `2, . . . `d] and Y ,X,W similarly defined. The goal is to recover the matrix L and its column space with error. We use rL to denote the rank of L. The maximum fraction of nonzeros in any row (column) of the outlier matrix X is denoted by max-outlier-frac-row (max-outlier-frac-col).\nIdentifiability and other assumptions. The above problem definition does not ensure identifiability since either of L or X can be both low-rank and sparse. Moreover, if the subspace changes at every time, it is impossible to correctly estimate all the subspaces. One way to ensure that L is not sparse is by requiring that its left and right singular vectors are dense (non-sparse) or \u201cincoherent\u201d w.r.t. a sparse vector [3, 8, 9].\nDefinition 1.1. An n\u00d7 r basis matrix P is \u00b5-incoherent if maxi=1,2,..,n \u2016P (i)\u201622 \u2264 \u00b5r/n. Here \u00b5 is called the coherence parameter. It quantifies the non-denseness of P .\nA simple way to ensure that X is not low-rank is by imposing upper bounds on max-outlier-frac-row and max-outlier-frac-col [8, 9]. One way to ensure identifiability of the changing subspaces is to assume that they are piecewise constant:\nP(t) = Pj for all t \u2208 [tj , tj+1), j = 1, 2, . . . , J,\nand to lower bound tj+1 \u2212 tj . Let t0 = 1 and tJ+1 = d. With this model, rL = rJ in general (except if subspace directions are repeated). The union of the column spans of all the Pj \u2019s is equal to the span of the left singular vectors of L. Thus, assuming that the Pj \u2019s are \u00b5-incoherent implies their incoherence. We also assume that the subspace coefficients at are mutually independent over time, have identical and diagonal covariance matrices denoted by \u039b, and are element-wise bounded. Element-wise bounded-ness of at\u2019s, along with the statistical assumptions, is similar to incoherence of right singular vectors of L (right incoherence); see Remark 3.5. Because tracking requires an online algorithm that processes data vectors one at a time or in mini-batches, we need these statistical assumptions on the at\u2019s. For the same reason, we also need to re-define max-outlier-frac-row as the maximum fraction of nonzeroes in any row of any \u03b1-consecutive-column sub-matrix of X. Here \u03b1 is the mini-batch size used by the RST algorithm. We will refer to it as max-outlier-frac-row\u03b1 to indicate this difference.\nContributions. (1) We develop a recursive projected compressive sensing (ReProCS) algorithm for RST that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal. We will refer to it as just \u201cNORST\u201d in the sequel. NORST has a significantly improved (and simpler) subspace update step compared to all previous ReProCS-based methods. Our most important contribution is one of the first two provable guarantees for RST, and the first that ensures near optimal tracking delay, needs a near optimal lower bound on how long a subspace should remain constant, and needs minimal assumptions on subspace change. Moreover, our guarantee also shows that NORST is online (after initialization), fast (has the same complexity as vanilla r-SVD), and has memory complexity of order nr log n log(1/ ). Here \u201conline\u201d means the following. After each subspace change, the algorithm detects the change in at most 2\u03b1 = Cf2r log n time instants, and after that, it improves the subspace estimate every \u03b1 time instants1. The improvement after each step is exponential and thus, one can get an -accurate estimate within K = C log(1/ ) such steps. Offline NORST also provably solves dynamic RPCA.\n(2) Our guarantees for both NORST and offline NORST (Theorem 2.2) essentially hold under \u201cweakened\u201d standard RPCA assumptions and two simple extra assumptions: (i) slow subspace change and (ii) a lower bound on most outlier magnitudes. (i) is a natural assumption for static camera videos (with no sudden scene changes) and (ii) is also easy because, by definition, an \u201coutlier\u201d is a large magnitude corruption. The small magnitude ones get classified as \u03bdt. Besides these, we also need that at\u2019s are mutually independent, have identical and diagonal covariance matrix \u039b, and are element-wise bounded. Element-wise bounded-ness, along with the statistical assumptions on at, is similar to right incoherence of L; see Remark 3.5. For the initial Cr samples, NORST needs the outlier fractions to be O(1/r) (needed to apply AltProj). As explained in Sec. 2.3, the extra assumptions help ensure that, after initialization, NORST can tolerate a constant maximum fraction of outliers per row in any \u03b1-column-sub-matrix of the data matrix without assuming any outlier support generation model. This statement assumes that the condition number of the covariance of at is a constant (with n). As is evident from Table 1, this\n1The reason that just O(r logn) samples suffice for each update is because we assume that the at\u2019s are element-wise bounded, \u03bdt is very small and with effective dimension r or smaller (see Theorem 2.2). These, along with the specific structure of the PCA problem we encounter (noise/error seen by the PCA step depends on the `t\u2019s and thus has \u201ceffective dimension\u201d r), is why so few samples suffice.\nis better than what all existing RPCA approaches can tolerate. For the video application, this implies that NORST tolerates slow moving and occasionally static foreground objects much better than all other approaches. This is also corroborated by our experiments on real videos, e.g., see Fig 1 and Sec. 6.\n(3) Unlike simple-ReProCS [15] or original-ReProCS [12, 13, 14], NORST needs only a coarse initialization which can be computed using just C log r iterations of any batch RPCA method such as AltProj applied to Cr initial samples. In fact, if the outlier magnitudes were very large for an initial set of O(r log n log r) time instants, or if the outliers were absent for this much time, even a random initialization would suffice. This simple fact has two important implications. First, NORST with the subspace change detection step removed also provides an online, fast, memory-efficient, and provably correct approach for static RPCA (our problem with J = 1, i.e., with `t = Pat). The other online solution for such a problem is ORPCA which comes with only a partial guarantee [21] (the guarantee requires intermediate algorithm estimates to be satisfying certain properties). Moreover, a direct corollary of our result is a guarantee that a minor modification of NORST-random (NORST with random initialization) also solves the subspace tracking with missing data (ST-missing) and the dynamic matrix completion (MC) problems. All existing guarantees for ST-missing [18, 19] hold only for the case of a single unknown subspace and are only partial guarantees. From the MC perspective, NORST-random does not assume any model on the set of observed entries. However, the tradeoff is that it needs many more observed entries. Both these results are given in Sec. 5.\nPaper Organization. The rest of the paper is organized as follows. In Sec. 2 we explain the main ideas of the NORST algorithm and present our main result for it (Theorem 2.2). We also discuss the implications of our guarantee, and provide detailed comparison with related work. In Sec. 3, we give the complete NORST algorithm and carefully explain the subspace change detection approach. In Sec. 4, we give the proof outline, the three main lemmas leading to the proof of Theorem 2.2, then also prove the lemmas. In Sec. 5, we provide useful corollaries for (a) Static RPCA, (b) Subspace Tracking with missing entries and (c) a simple extension to recover the guarantee of s-ReProCS from [15]. Empirical evaluation on synthetic and real-world datasets is described in Sec. 6. The complete proof of Theorem 2.2, of two auxiliary lemmas, and of the extensions is given in the Appendix."}, {"heading": "2 NORST Algorithm and Main Result", "text": ""}, {"heading": "2.1 NORST: Nearly-Optimal RST", "text": "NORST starts with a \u201cgood\u201d estimate of the initial subspace. This can be obtained by C log r iterations2 of AltProj applied to Y[1,ttrain] with ttrain = Cr. It then iterates between (a) Projected Compressive\n2Using C log r iterations helps ensure that the initialization error is O(1/ \u221a r).\nSensing (CS) / Robust Regression3 in order to estimate the sparse outliers, xt\u2019s, and hence the `t\u2019s, and (b) Subspace Update to update the subspace estimate P\u0302(t). Projected CS proceeds as follows. At time t, if the previous subspace estimate, P\u0302(t\u22121), is accurate enough, because of slow subspace change, projecting yt onto its orthogonal complement will nullify most of `t. We compute y\u0303t := \u03a8yt where \u03a8 := I \u2212 P\u0302(t\u22121)P\u0302(t\u22121)\u2032. Thus, y\u0303t = \u03a8xt + \u03a8(`t + \u03bdt) and \u2016\u03a8(`t + \u03bdt)\u2016 is small due to slow subspace change and small \u03bdt. Recovering xt from y\u0303t is now a CS / sparse recovery problem in small noise [22]. We compute x\u0302t,cs using noisy l1 minimization followed by thresholding based support estimation to obtain T\u0302t. A Least Squares (LS) based debiasing step on T\u0302t returns the final x\u0302t. We then estimate `t as \u02c6\u0300t = yt\u2212 x\u0302t. The \u02c6\u0300t\u2019s are then used for the Subspace Update step which involves (i) detecting subspace change, and (ii) obtaining improved estimates of the new subspace by K steps of r-SVD, each done with a new set of \u03b1 samples of \u02c6\u0300t. While this step is designed under the piecewise constant subspace assumption (needed for identifiability of P(t)\u2019s), if the goal is only to get good estimates of `t or xt, the method works even when this assumption may not hold, e.g., for real videos. For ease of understanding, we present a basic version of NORST in Algorithm 1. This assumes the change times tj are known. The actual algorithm, that we study and implement, detects these automatically. It is given as Algorithm 2 in Sec. 3."}, {"heading": "2.2 Main Result", "text": "Before stating the result, we precisely define max-outlier-frac-col and max-outlier-frac-row\u03b1. Since NORST is an online approach that performs outlier support recovery one data vector at a time, it needs different bounds on both. Let max-outlier-frac-col := maxt |Tt|/n. We define max-outlier-frac-row\u03b1 as the maximum fraction of outliers (nonzeros) per row of any sub-matrix of X with \u03b1 consecutive columns. To understand\nthis precisely, for a time interval, J , define \u03b3(J ) := maxi=1,2,...,n 1|J | \u2211 t\u2208J 1{i\u2208Tt} where 1S is the indicator\nfunction for statement S. Thus, \u2211\nt\u2208J 1{i\u2208Tt} counts the number of outliers (nonzeros) in row i of XJ ,\nand so \u03b3(J ) is the maximum outlier fraction in any row of the sub-matrix XJ of X. Let J \u03b1 denote a time interval of duration \u03b1. Then max-outlier-frac-row\u03b1 := maxJ \u03b1\u2286[1,d] \u03b3(J \u03b1).\nWe use t\u0302j to denote the time instant at which the j-th subspace change time is detected by Algorithm\n2.\nTheorem 2.2. Consider Algorithm 2 given in the next section. Let \u03b1 := Cf2r log n, \u039b := E[a1a1\u2032], \u03bb+ := \u03bbmax(\u039b), \u03bb \u2212 := \u03bbmin(\u039b), f := \u03bb +/\u03bb\u2212, and let xmin := mint mini\u2208Tt(xt)i denote the minimum outlier magnitude. Pick an \u03b5 \u2264 min(0.01, 0.03 minj SE(Pj\u22121,Pj)2/f). Let K := C log(1/\u03b5). If\n1. Pj\u2019s are \u00b5-incoherent; and at\u2019s are zero mean, mutually independent over time t, have identical\ncovariance matrices, i.e. E[atat\u2032] = \u039b, are element-wise uncorrelated (\u039b is diagonal), are elementwise bounded (for a numerical constant \u03b7, (at) 2 i \u2264 \u03b7\u03bbi(\u039b)), and are independent of all outlier supports Tt;\n2. \u2016\u03bdt\u20162 \u2264 cr\u2016E[\u03bdt\u03bdt\u2032]\u2016, \u2016E[\u03bdt\u03bdt\u2032]\u2016 \u2264 c\u03b52\u03bb\u2212, \u03bdt\u2019s are zero mean, mutually independent, and independent of xt, `t;\n3. max-outlier-frac-col \u2264 c1/\u00b5r, max-outlier-frac-row\u03b1 \u2264 b0 := c2f2 ;\n4. subspace change: let \u2206 := maxj SE(Pj\u22121,Pj), assume that\n3Robust Regression (with a sparsity model on the outliers) assumes that observed data vector y satisfies y = P\u0302 a+x+ b where P\u0302 is a tall matrix (given), a is the vector of (unknown) regression coefficients, x is the (unknown) sparse outliers, b is (unknown) small noise/modeling error. An obvious way to solve this is by solving mina,x \u03bb\u2016x\u20161 + \u2016y \u2212 P\u0302 a\u2212 x\u20162. In this, one can solve for a in closed form to get a\u0302 = P\u0302 \u2032(y\u2212x). Substituting this, the minimization simplifies to minx \u03bb\u2016x\u20161 +\u2016(I\u2212 P\u0302 P\u0302 \u2032)(y \u2212 x)\u20162. This is equivalent to the Lagrangian version of the projected CS problem that NORST solves (see line 7 of Algorithm 1).\n(a) tj+1 \u2212 tj > (K + 2)\u03b1, and (b) \u2206 \u2264 0.8 and C1 \u221a r\u03bb+(\u2206 + 2\u03b5) \u2264 xmin;\n5. initialization4: SE(P\u03020,P0) \u2264 0.25, C1 \u221a r\u03bb+SE(P\u03020,P0) \u2264 xmin;\nand (6) algorithm parameters are set as given in Algorithm 2; then, with probability (w.p.) at least 1\u2212 10dn\u221210,\nSE(P\u0302(t),P(t)) \u2264  (\u03b5+ \u2206) if t \u2208 [tj , t\u0302j + \u03b1), (0.3)k\u22121(\u03b5+ \u2206) if t \u2208 [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1), \u03b5 if t \u2208 [t\u0302j +K\u03b1+ \u03b1, tj+1).\nTreating f as a numerical constant, the memory complexity is O(n\u03b1) = O(nr log n) and time complexity is O(ndr log(1/\u03b5)).\nCorollary 2.3. Under Theorem 2.2 assumptions, the following also hold:\n1. \u2016x\u0302t \u2212 xt\u2016 = \u2016 \u02c6\u0300t \u2212 `t\u2016 \u2264 1.2(SE(P\u0302(t),P(t)) + \u03b5)\u2016`t\u2016 with SE(P\u0302(t),P(t)) bounded as above,\n2. at all times, t, T\u0302t = Tt,\n3. tj \u2264 t\u0302j \u2264 tj + 2\u03b1,\n4. Offline-NORST (last few lines of Algorithm 2): SE(P\u0302 offline(t) ,P(t)) \u2264 \u03b5, \u2016x\u0302 offline t \u2212xt\u2016 = \u2016 \u02c6\u0300 offline t \u2212\n`t\u2016 \u2264 \u03b5\u2016`t\u2016 at all t. Its memory complexity is O(Kn\u03b1) = O(nr log n log(1/\u03b5)).\nRemark 2.4 (Relaxing outlier magnitudes lower bound). The assumption on xmin (outlier magnitudes) required by Theorem 2.2 can be significantly relaxed to the following which only requires that most outlier magnitudes are lower bounded. Assume that the outlier magnitudes are such that the following holds: xt can be split as xt = (xt)small + (xt)large with the two components being such that, in the k-th subspace\nupdate interval 5, \u2016(xt)small\u2016 \u2264 0.3k\u22121(\u03b5 + \u2206) \u221a r\u03bb+ and the smallest nonzero entry of (xt)large is larger\nthan C1 \u00b7 0.3k\u22121(\u03b5 + \u2206) \u221a r\u03bb+. For the case of j = 0, we need the bound to hold with \u2206 replaced by \u2206init = SE(P\u03020,P0), and \u03b5 replaced by zero.\nIf there were a way to bound the element-wise error of the CS step (instead of the l2 norm error), one\ncould relax the above requirement even more.\nDiscussion. This discussion assumes that f is a constant (does not increase with n), i.e., it is O(1). Theorem 2.2 shows that, with high probability (whp), when using NORST, the subspace change gets detected within a delay of at most 2\u03b1 = Cf2(r log n) time instants, and the subspace gets estimated to \u03b5 error within at most (K + 2)\u03b1 = Cf2(r log n) log(1/\u03b5) time instants. The same is also true for the recovery error of xt and `t. Both the detection and tracking delay are within log factors of the optimal since r is the minimum delay needed even in the noise-free, i..e, xt = \u03bdt = 0, case. The fact that NORST can detect subspace change within a short delay can be an important feature for certain applications, e.g., this feature is used in [6] to detect structural changes in a dynamic social network. Moreover, if offline processing is allowed, we can guarantee recovery within normalized error \u03b5 at all time instants. This implies that offline-NORST solves the dynamic RPCA problem.\nObserve that Theorem 2.2 allows a constant maximum fraction of outliers per row (after initialization), without making any assumption on how the outlier support is generated, as long as the extra assumptions\n4This can be satisfied by applying C log r iterations of AltProj [9] on the first Cr data samples and assuming that these have outlier fractions in any row or column bounded by c/r. 5k-th subspace update interval refers to Jk := [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1) for k > 1 and J1 = [tj , t\u0302j + \u03b1) for k = 1. The first interval also includes the subspace detection interval, [tj , t\u0302j), since the analysis of the projected CS step for this interval is the same as for [t\u0302j , t\u0302j + \u03b1).\nAlgorithm 1 Basic-NORST (with tj known). The actual algorithm that detects tj automatically is Algorithm 2. Obtain P\u03020 by C(log r) iterations of AltProj on Y[1,ttrain] with ttrain = Cr followed by SVD on the output L\u0302.\n1: Input: yt, Output: x\u0302t, \u02c6\u0300t, P\u0302(t), T\u0302t 2: Parameters: K \u2190 C log(1/\u03b5), \u03b1\u2190 Cf2r log n, \u03c9supp \u2190 xmin/2, \u03be \u2190 xmin/15, r 3: Initialize: j \u2190 1, k \u2190 1 P\u0302(ttrain) \u2190 P\u03020 4: for t > ttrain do 5: \u03a8\u2190 I \u2212 P\u0302(t\u22121)P\u0302(t\u22121)\u2032 6: y\u0303t \u2190 \u03a8yt. 7: x\u0302t,cs \u2190 arg minx\u0303 \u2016x\u0303\u20161 s.t. \u2016y\u0303t \u2212\u03a8x\u0303\u2016 \u2264 \u03be. 8: T\u0302t \u2190 {i : |x\u0302t,cs| > \u03c9supp}. 9: x\u0302t \u2190 IT\u0302t(\u03a8T\u0302t \u2032\u03a8T\u0302t) \u22121\u03a8T\u0302t \u2032y\u0303t.\n10: \u02c6\u0300t \u2190 yt \u2212 x\u0302t. 11: if t = tj + k\u03b1\u2212 1 then 12: P\u0302j,k \u2190 SV Dr[L\u0302t;\u03b1], P\u0302(t) \u2190 P\u0302j,k, k \u2190 k + 1. 13: else 14: P\u0302(t) \u2190 P\u0302(t\u22121). 15: end if 16: if t = tj +K\u03b1\u2212 1 then 17: P\u0302j \u2190 P\u0302(t), k \u2190 1, j \u2190 j + 1 18: end if 19: end for\nProjected-CS\n(Robust\nRegression).\nSubspace\nUpdate.\ndiscussed below hold. We explain why this is possible in Sec. 2.3. Of course, for the initial Cr samples, NORST needs max-outlier-frac-rowCr \u2208 O(1/r) (needed to apply AltProj). Also, the memory complexity guaranteed by Theorem 2.2 is nearly d/r times better than that of all existing RPCA solutions; see Table 1. The time complexity is worse than that of only NO-RMC6, but NO-RMC needs d \u2265 cn (unreasonable requirement for videos which often have much fewer frames d than the image size n). Finally, NORST also needs outlier fraction per column to be O(1/r) instead of O(1/rL). If J is large, e.g. if J = d/(r log n), it is possible that rL r. We should clarify that NORST allows max-outlier-frac-row\u03b1 \u2208 O(1) but this does not necessarily imply that the number of outliers in each row can be this high. The reason is it only allows the fraction per column to only be O(1/r). Thus, for a matrix of size n \u00d7 \u03b1, it allows the total number of outliers to be O(min(n\u03b1, n\u03b1/r)) = O(n\u03b1/r). Thus the average fraction allowed is only O(1/r).\nNORST needs the following extra assumptions. The main extra requirement is that xmin be lower bounded as given in the last two assumptions of Theorem 2.2, or as stated in Remark 2.4. The lower bound on xmin is reasonable 7 as long as the initial subspace estimate is accurate enough and the subspace changes slowly enough so that both \u2206 and SE(P\u03020,P0) are O(1/ \u221a r). This requirement may seem restrictive on first glance but actually is not. The reason is that SE(.) is only measuring the largest principal angle. This bound on SE still allows the chordal distance between the two subspaces to be O(1). Chordal distance [20] is the l2 norm of the vector containing the sine of all principal angles. The second related extra requirement is an upper bound on \u2206 (slow subspace change) which depends on the value of xmin. We discuss this point next. Other than these two, NORST only needs simple statistical assumptions on\n6NO-RMC is so fast because it is actually a robust matrix completion solution and it deliberately undersamples the entire\ndata matrix Y to get a faster RPCA algorithm. 7requires xmin to be C \u221a \u03bb+ or larger.\nat\u2019s. The zero-mean assumption is a minor one. The assumption that \u039b be diagonal is also minor 8. In the video setting, zero-mean can be ensured by subtracting the empirical average of the background images computing using the first ttrain frames. Mutual independence of at\u2019s holds if the changes in each background image w.r.t. a \u201cmean\u201d background are independent, when conditioned on their subspace. This is valid, for example, if the background changes are due to illumination variations or due to moving curtains (see Fig. 5). Moreover, by using the approach of [14], it is possible to relax this to just requiring that the at\u2019s satisfy an autoregressive model over time. Element-wise boundedness, along with the above, is similar to right incoherence (see Remark 3.5).\nOutlier v/s Subspace Assumptions. When there are fewer outliers in the data or when outliers are easy to detect, one would expect to need weaker assumptions on the true data\u2019s subspace and/or on its rate of change. This is indeed true. The max-outlier-frac-col bound relates max-outlier-frac-col to \u00b5 (not-denseness parameter) and r (subspace dimension). The upper bound on \u2206 implies that, if xmin is\n8It only implies that Pj is the matrix of principal components of E[LjL\u2032j ] where Lj := [`tj , `tj+1, . . . , `tj+1\u22121].\nlarger (outliers are easier to detect), a larger amount of subspace change \u2206 can be tolerated. The relation of max-outlier-frac-row to rate of subspace change is not evident from the way the guarantee is stated above because we have assumed max-outlier-frac-row \u2264 b0 := c/f2 with c being a numerical constant, and used this to get a simple expression for K. If we did not do this, we would get K = Cd 1\u2212 log(\u221ab0f) log( c\u2206 0.8\u03b5)e, see Remark A.1. Since we need tj+1 \u2212 tj \u2265 (K + 2)\u03b1, a smaller b0 means a larger \u2206 can be tolerated for the same delay, or vice versa.\nAlgorithm Parameters. Algorithm 2 assumes knowledge of 4 model parameters: r, \u03bb+, \u03bb\u2212 and xmin to set the algorithm parameters. The initial dataset used for estimating P\u03020 (using AltProj) can be used to get an accurate estimate of r, \u03bb\u2212 and \u03bb+ using standard techniques. Thus one really only needs to set xmin. If continuity over time is assumed, we can let it be time-varying and set it as mini\u2208T\u0302t\u22121 |(x\u0302t\u22121)i| at t.\nRelated Work. For a summary of comparisons, see Table 1. In terms of other solutions for provably correct RST or dynamic RPCA, there is very little work. For RST, there is only one other provable algorithm, simple-ReProCS (s-ReProCS) [15]. This has the same tracking delay and memory complexity as NORST, however, it assumes that only one subspace direction can change at each change time. This is a more restrictive model than ours. Moreover, it implies that the tracking delay of s-ReProCS is r-times sub-optimal. Also, s-ReProCS uses a projection-SVD step for subspace update (as opposed to simple SVD in NORST). These two facts imply that it needs an -accurate subspace initialization in order to ensure that the later changed subspaces can be tracked with -accuracy. Thus, it does not provide a static RPCA or subspace tracking with missing data solution.\nFor dynamic RPCA, the earliest result was a partial guarantee (a guarantee that depended on intermediate algorithm estimates satisfying certain assumptions) for the original reprocs approach (originalReProCS) [12]. This was followed up by two complete guarantees for reprocs-based approaches with minor modifications [13, 14]. For simplicity we will still call these \u201coriginal-ReProCS\u201d. These guarantees needed very strong assumptions and their tracking delay was O(nr2/ 2). Since can be very small, this factor can be quite large, and hence one cannot claim that original-ReProCS solves RST. Our work is a very significant improvement over all these works. (i) The guaranteed memory complexity, tracking delay, and required delay between subspace change times of NORST are all r/ 2 times lower than that of original-ReProCS. (ii) All the original-ReProCS guarantees needed a very specific assumption on how the outlier support could change. They required an outlier support model inspired by a video moving object that moves in one direction for a long time; and whenever it moves, it must move by a fraction of s := maxt |Tt|. This is very specific model with the requirement of moving by a fraction of s being the most restrictive. Our result removes this model and replaces it with just a bound on max-outlier-frac-row. We explain in the last para of Sec. 4.1 why this is possible. (iii) The subspace change model assumed in [13, 14] required a few new directions, that were orthogonal to Pj\u22121, to be added at time tj and some others to be removed. This is an unrealistic model for slow subspace change, e.g., in 3D, it implies that the subspace needs to change from the x-y plane to the y-z plane. Moreover because of this model, their results needed the \u201cenergy\u201d (eigenvalues) along the newly added directions to be small for a period of time after each subspace change. This is a strong (and not easy to interpret) requirement. Our result removes all these requirements and replaces them with a bound on SE(Pj\u22121,Pj) which is much more realistic. Thus, in 3D, our result allows the x-y plane to change to a slightly tilted x-y plane.\nAn approach called modified-PCP (mod-PCP) was proposed to solve the problem of RPCA with partial subspace knowledge [23]. A corollary of its guarantee shows that it can also be used to solve dynamic RPCA [23]. However, since it adapted the PCP proof techniques from [3], its pros and cons are similar to those of PCP, e.g., it also needs a uniformly randomly generated outlier support. As can be seen from Table 1, its pros and cons are similar to those of the PCP result by [3] (PCP(C)) discussed below.\nWe also provide a comparison with provably correct RPCA approaches in Table 1. In summary,\nNORST has significantly better memory complexity than all of them, all of which are batch; it has the best outlier tolerance (after initialization), and the second-best time complexity, as long as its extra assumptions hold. It can also detect subspace change quickly, which can be a useful feature. Consider outlier tolerance. PCP(H), AltProj, RPCA-GD, and NO-RMC need both max-outlier-frac-row and max-outlier-frac-col to be O(1/rL); PCP(C) [3] and modified-PCP [23] need the outlier support to uniformly random (strong requirement: for video it implies that objects are very small sized and jumping around randomly); and original-ReProCS needs it to satisfy a very specific moving object model described above (restrictive). Instead, after initialization, NORST only needs max-outlier-frac-row\u03b1 \u2208 O(1) and max-outlier-frac-col \u2208 O(1/r)."}, {"heading": "2.3 The need for extra assumptions", "text": "As noted in [9], the standard RPCA problem (that only assumes left and right incoherence of L and nothing else) cannot tolerate a bound on outlier fractions in any row or any column that is larger than 1/rL 9. The reason NORST can tolerate a constant max-outlier-frac-row\u03b1 bound is because it uses extra assumptions. We explain the need for these here. It recovers xt first and then `t and does this at each time t. When recovering xt, it exploits \u201cgood\u201d knowledge of the subspace of `t (either from initialization or from the previous subspace\u2019s estimate and slow subspace change), but it has no way to deal with the residual error, bt := (I \u2212 P\u0302(t\u22121)P\u0302(t\u22121)\u2032)`t, in this knowledge. Since the individual vector bt does not have any structure that can exploited10, the error in recovering xt cannot be lower than C\u2016bt\u2016. This means that, to correctly recover the support of xt, xmin needs to be larger than C\u2016bt\u2016. This is where the xmin lower bound comes from. As we will see in Sec. 4, correct support recovery is needed to ensure that the subspace estimate can be improved with each update. In particular, it helps ensure that the error vectors et := xt\u2212 x\u0302t in a given subspace update interval are mutually independent, when conditioned on the yt\u2019s from all past intervals. This step also uses element-wise boundedness of the at\u2019s along with their mutual independence and identical covariances."}, {"heading": "3 Automatic NORST", "text": "We present the actual NORST algorithm (automatic NORST) in Algorithm 2. The main idea why automatic NORST works is the same as that of the basic algorithm with the exception of the additional subspace detection step. The subspace detection idea is borrowed from [15], although its correctness proof has differences because we assume a much simpler subspace change model. In Algorithm 2, the subspace update stage toggles between the \u201cdetect\u201d phase and the \u201cupdate\u201d phase. It starts in the \u201cupdate\u201d phase with t\u03020 = ttrain. We then perform K r-SVD steps with the k-th one done at t = t\u03020 + k\u03b1 \u2212 1. Each such step uses the last \u03b1 estimates, i.e., uses L\u0302t;\u03b1. Thus at t = t\u03020 + K\u03b1 \u2212 1, the subspace update of P0 is complete. At this point, the algorithm enters the \u201cdetect\u201d phase.\nFor any j, if the j-th subspace change is detected at time t, we set t\u0302j = t. At this time, the algorithm enters the \u201cupdate\u201d (subspace update) phase. We then perform K r-SVD steps with the k-th r-SVD step done at t = t\u0302j + k\u03b1\u2212 1 (instead of at t = tj + k\u03b1\u2212 1). Each such step uses the last \u03b1 estimates, i.e., uses 9The reason is this: let b0 = max-outlier-frac-row, one can construct a matrix X with b0 outliers in some rows that has rank equal to 1/b0. A simple way to do this would be to let the support and nonzero entries of X be constant for b0d columns before letting either of them change. Then the rank of X will be d/(b0d). A similar argument can be used for max-outlier-frac-col. 10However the bt\u2019s arranged into a matrix do form a low-rank matrix whose approximate rank is r or even lower (if not all directions change). If we try to exploit this structure we end up with a modified-PCP [23] type approach. This needs the uniform random support assumption (used in its guarantee). Or, if the [8] approach were used for its guaratee, for identifiability reasons similar to the one described above, it will still not tolerate outlier fractions larger than 1/rnew where rnew is the (approximate) rank of the matrix formed by the bt\u2019s.\nAlgorithm 2 Automatic-NORST. Obtain P\u03020 by C(log r) iterations of AltProj on Y[1,ttrain] with ttrain = Cr followed by SVD on the output L\u0302.\n1: Input: P\u03020, yt, Output: x\u0302t, \u02c6\u0300t, P\u0302(t) 2: Parameters: K \u2190 C log(1/\u03b5), \u03b1\u2190 Cf2r log n, \u03c9supp \u2190 xmin/2, \u03be \u2190 xmin/15, \u03c9evals \u2190 2\u03b52\u03bb+, r. 3: P\u0302(ttrain) \u2190 P\u03020; j \u2190 1, k \u2190 1 4: phase\u2190 update; t\u03020 \u2190 ttrain; 5: for t > ttrain do 6: Lines 5\u2212 10 of Algorithm 1 7: if phase = detect and t = t\u0302j\u22121,fin + u\u03b1 then 8: \u03a6\u2190 (I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032). 9: B \u2190 \u03a6L\u0302t,\u03b1\n10: if \u03bbmax(BB \u2032) \u2265 \u03b1\u03c9evals then 11: phase\u2190 update, t\u0302j \u2190 t, 12: end if 13: end if 14: if phase = update then 15: if t = t\u0302j + u\u03b1\u2212 1 for u = 1, 2, \u00b7 \u00b7 \u00b7 , then 16: P\u0302j,k \u2190 SV Dr[L\u0302t;\u03b1], P\u0302(t) \u2190 P\u0302j,k, k \u2190 k + 1. 17: else 18: P\u0302(t) \u2190 P\u0302(t\u22121) 19: end if 20: if t = t\u0302j +K\u03b1\u2212 1 then 21: t\u0302j,fin \u2190 t, P\u0302j \u2190 P\u0302(t) 22: k \u2190 1, j \u2190 j + 1, phase\u2190 detect. 23: end if 24: end if 25: end for 26: Offline NORST: At t = t\u0302j +K\u03b1, for all t \u2208 [t\u0302j\u22121 +K\u03b1, t\u0302j +K\u03b1\u2212 1], 27: P\u0302 offline(t) \u2190 [P\u0302j\u22121, (I \u2212 P\u0302j\u22121P\u0302j\u22121 \u2032)P\u0302j ] 28: \u03a8\u2190 I \u2212 P\u0302 offline(t) P\u0302 offline (t) \u2032 29: x\u0302offlinet \u2190 IT\u0302t(\u03a8T\u0302t \u2032\u03a8T\u0302t) \u22121\u03a8T\u0302t \u2032yt 30: \u02c6\u0300offlinet \u2190 yt \u2212 x\u0302offlinet .\nProjected CS.\nSubspace\nDetect Phase.\nSubspace\nUpdate Phase.\nOffline\nNORST.\nL\u0302t;\u03b1 Thus, at t = t\u0302j,fin = t\u0302j + K\u03b1 \u2212 1, the update is complete. At this time, the algorithm enters the \u201cdetect\u201d phase again.\nTo understand the change detection strategy, consider the j-th subspace change. Assume that the previous subspace Pj\u22121 has been accurately estimated by t = t\u0302j\u22121,fin = t\u0302j\u22121 +K\u03b1\u22121 and that t\u0302j\u22121,fin < tj . Let P\u0302j\u22121 denote this estimate. At this time, the algorithm enters the \u201cdetect\u201d phase in order to detect the next (j-th) change. Let Bt := (I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)L\u0302t;\u03b1. For every t = t\u0302j\u22121,fin + u\u03b1\u2212 1, u = 1, 2, . . . , we detect change by checking if the maximum singular value of Bt is above a pre-set threshold, \u221a \u03c9evals\u03b1, or not.\nWe claim that, whp, under assumptions of Theorem 2.2, this strategy has no \u201cfalse subspace detections\u201d and correctly detects change within a delay of at most 2\u03b1 samples. The former is true because, for any t for which [t \u2212 \u03b1 + 1, t] \u2286 [t\u0302j\u22121,fin, tj), all singular values of the matrix Bt will be close to\nzero (will be of order \u03b5 \u221a \u03bb+) and hence its maximum singular value will be below \u221a \u03c9evals\u03b1. Thus, whp, t\u0302j \u2265 tj . To understand why the change is correctly detected within 2\u03b1 samples, first consider t = t\u0302j\u22121,fin + d tj\u2212t\u0302j\u22121,fin \u03b1 e\u03b1 := tj,\u2217. Since we assumed that t\u0302j\u22121,fin < tj (the previous subspace update is complete before the next change), tj lie in the interval [tj,\u2217 \u2212 \u03b1 + 1, tj,\u2217]. Thus, not all of the `t\u2019s in this interval lie in the new subspace. Depending on where in the interval tj lies, the algorithm may or may not detect the change at this time. However, in the next interval, i.e., for t \u2208 [tj,\u2217 + 1, tj,\u2217 + \u03b1], all of the `t\u2019s lie in the new subspace. We can prove that Bt for this time t will have maximum singular value that is above the threshold. Thus, if the change is not detected at tj,\u2217, whp, it will get detected at tj,\u2217 + \u03b1. Hence one can show that, whp, either t\u0302j = tj,\u2217, or t\u0302j = tj,\u2217 + \u03b1, i.e., tj \u2264 t\u0302j \u2264 tj + 2\u03b1 (see Appendix A). Time complexity. Consider initialization. To ensure that SE(P\u03020,P0) \u2208 O(1/ \u221a r), we need to use C log r iterations of AltProj. Since there is no lower bound in the AltProj guarantee on the required number of matrix columns (except the trivial lower bound of rank) [9], we can use ttrain = Cr frames for initialization. Thus the initialization complexity is O(nttrainr 2 log( \u221a r) = O(nr3 log r) [9]. The projectedCS step complexity is equal to the cost of a matrix vector multiplication with the measurement matrix times negative logarithm of the desired accuracy in solving the l1 minimization problem. Since the measurement matrix for the CS step is I\u2212P\u0302(t\u22121)P\u0302(t\u22121)\u2032, the cost per CS step (per frame) is O(nr log(1/ )) [24] and so the total cost is O((d\u2212ttrain)nr log(1/ )). The subspace update involves at most ((d\u2212ttrain)/\u03b1) rank r-SVD\u2019s on n\u00d7\u03b1matrices all of which have constant eigen-gap (this is proved in the proof of tTheorem 4.14 from [25] which we use to show correctness of this step). Thus the total time for subspace update steps is at most ((d \u2212 ttrain)/\u03b1) \u2217 O(n\u03b1r log(1/ )) = O((d \u2212 ttrain)nr log(1/ )) [26]. Thus the running time of the complete algorithm is O(ndr log(1/ ) + nr3 log r). As long as r2 log r \u2264 d log(1/ ), the time complexity of the entire algorithm is O(ndr log(1/ )).\nRemark 3.5 (Relating our assumptions to right incoherence of Lj := L[tj ,tj+1) [8]). From our assumptions, Lj = PjAj with Aj := [atj ,atj+1, . . .atj+1\u22121], the columns of Aj are zero mean, mutually independent, have identical covariance \u039b, \u039b is diagonal, and are element-wise bounded as specified by Theorem\n2.2. Let dj := tj+1 \u2212 tj. Define a diagonal matrix \u03a3 with (i, i)-th entry \u03c3i and with \u03c32i := \u2211 t(at) 2 i /dj.\nDefine a dj \u00d7 r matrix V\u0303 with the t-th entry of the i-th column being (v\u0303i)t := (at)i/(\u03c3i \u221a dj). Then, Lj = Pj\u03a3V\u0303 \u2032 and each column of V\u0303 is unit 2-norm. Also, from the bounded-ness assumption, (v\u0303i)\n2 t \u2264 \u03b7 \u03bbi\u03c32i \u00b7 1 dj\nwhere \u03b7 is a numerical constant.\nObserve that Pj\u03a3V\u0303 \u2032 is not exactly the SVD of Lj since the columns of V\u0303 are not necessarily exactly mutually orthogonal. However, if dj is large enough, one can argue using any law of large numbers\u2019 result (e.g., Hoeffding inequality), that the columns of V\u0303 are approximately mutually orthogonal whp. Also, whp, \u03c32i \u2265 0.99\u03bbi. This also follows using Hoeffding11. Thus, our assumptions imply that, whp, (v\u0303i)2t \u2264 C/dj. If one interprets V\u0303 as an \u201capproximation\u201d to the right singular vectors of Lj, this is the right incoherence assumed by [8] and slightly stronger than what is assumed by [3, 9] and others (these require that the squared norm of each row of the matrix of right singular vectors be bounded by Cr/dj).\nThe claim that \u201cV\u0303 can be interpreted as an \u201capproximation\u201d to the right singular vectors of Lj\u201d is not rigorous. But it is also not clear how to make it rigorous since our work uses statistical assumptions on the at\u2019s. To get the exact SVD of Lj, we need the SVD of Aj. Suppose Aj SVD = R\u03a3V \u2032, then Lj SVD = (PjR)\u03a3V\n\u2032. Here R will be an r \u00d7 r orthonormal matrix. Now it is not clear how to relate the element-wise bounded-ness assumption on at\u2019s to an assumption on entries of V , since now there is no easy expression for each entry of V or of the entries of \u03a3 in terms of at (since R is unknown).\n11The first claim uses all the four assumptions on at; the second claim uses all assumptions except diagonal \u039b"}, {"heading": "4 Proof Outline and (most of the) Proof", "text": "In this section we first give the main ideas of the proof (without formal lemmas). We then state the three main lemmas and explain how they help prove Theorem 2.2. After this, we prove the three lemmas."}, {"heading": "4.1 Main idea of the proof", "text": "It is not hard to see that the \u201cnoise\u201d bt := \u03a8(`t + \u03bdt) seen by the projected CS step is proportional the error between the subspace estimate from (t \u2212 1) and the current subspace. Moreover, incoherence (denseness) of the P(t)\u2019s and slow subspace change together imply that \u03a8 satisfies the restricted isometry property (RIP) [12]. Using this, a result for noisy l1 minimization [22], and the lower bound assumption on outlier magnitudes, one can ensure that the CS step output is accurate enough and the outlier support Tt is correctly recovered. With this, we have that \u02c6\u0300t = `t + \u03bdt \u2212 et where et := xt \u2212 x\u0302t satisfies et = ITt(\u03a8Tt \u2032\u03a8Tt) \u22121ITt\n\u2032\u03a8\u2032`t and \u2016et\u2016 \u2264 C\u2016bt\u2016. Consider subspace update. Every time the subspace changes, one can show that the change can be detected within a short delay. After that, the K SVD steps help get progressively improved estimates of the changed subspace. To understand this, observe that, after a subspace change, but before the first update step, bt is the largest and hence, et, is also the largest for this interval. However, because of good initialization or because of slow subspace change and previous subspace correctly recovered (to error \u03b5), neither is too large. Both are proportional to (\u03b5+ \u2206), or to the initialization error. Using the idea below, we can show that we get a \u201cgood\u201d first estimate of the changed subspace.\nThe input to the PCA step is \u02c6\u0300t and the noise seen by it is et. Notice that et depends on the true data `t. Hence this is a setting of PCA in data-dependent noise [27, 25]. From [25], it is known that the subspace recovery error of the PCA step is proportional to the ratio between the time averaged noise\npower plus time-averaged signal-noise correlation, (\u2016 \u2211 t E[etet\u2032]\u2016 + \u2016 \u2211\nt E[`tet\u2032\u2016)/\u03b1, and the minimum signal space eigenvalue, \u03bb\u2212. The instantaneous value of noise power is (\u2206 + \u03b5)2 times \u03bb+ while that of signal-noise correlation is of order (\u2206 + \u03b5) times \u03bb+. However, using the fact that et is sparse with support Tt that changes enough over time so that max-outlier-frac-row\u03b1 is bounded, one can argue (using Cauchy-Schwartz) that their time averaged values are \u221a max-outlier-frac-row\u03b1 times smaller. As a result, after the first subspace update, the subspace recovery error is at most 4 \u221a max-outlier-frac-row\u03b1(\u03bb+/\u03bb\u2212) times (\u2206 + \u03b5). Since max-outlier-frac-row\u03b1(\u03bb+/\u03bb\u2212)2 is bounded by a constant c2 < 1, this means that, after the first subspace update, the subspace error is at most \u221a c2 times (\u2206 + \u03b5).\nThis, in turn, implies that \u2016bt\u2016, and hence \u2016et\u2016, is also \u221a c2 times smaller in the second subspace update interval compared to the first. This, along with repeating the above argument, helps show that the second estimate of the changed subspace is \u221a c2 times better than the first and hence its error is ( \u221a c2) 2 times (\u2206 + \u03b5). Repeating the argument K times, the K-th estimate has error ( \u221a c2) K times (\u2206 + \u03b5). Since K = C log(1/\u03b5), this is an \u03b5 accurate estimate of the changed subspace.\nA careful application of the result of [25] is the reason why we are able to remove the moving object model assumption on the outlier support needed by the earlier guarantees for original-ReProCS [13, 14].\nApplied to our problem, this result requires \u2016 \u2211\nt\u2208J \u03b1 ITtITt \u2032/\u03b1\u2016 to be bounded by a constant less than one. It is not hard to see that maxJ \u03b1\u2208[1,d] \u2016 \u2211 t\u2208J \u03b1 ITtITt \u2032/\u03b1\u2016 = max-outlier-frac-row\u03b1. To understand\nthis simply, the matrix \u2211\nt\u2208J \u03b1 ITtITt \u2032 is diagonal, and the i-th diagonal entry counts the number of\ntime the index i appears in the support set Tt in the interval J \u03b1 which is precisely the definition of max-outlier-frac-row\u03b1 \u00b7\u03b1. This is also why a constant bound on max-outlier-frac-row\u03b1 suffices for our setting. On the other hand the guarantees of [13, 14] required that, for any sequence of positive semi-definite\n(p.s.d.) matrices, At, \u2016 \u2211 t\u2208J \u03b1 ITtAtITt \u2032/\u03b1\u2016, be bounded by a constant less than one. This is a much more stringent requirement; one way to satisfy it is using the moving object model on outlier supports assumed there."}, {"heading": "4.2 Main Lemmas", "text": "For simplicity, we give the proof for the \u03bdt = 0 case. The changes with \u03bdt 6= 0 are minor, see Appendix A.\nFirst consider the simpler case when tj \u2019s are known, i.e., consider Algorithm 1. In this case, t\u0302j = tj .\nDefinition 4.6. Define\n1. the constants used in Theorem 2.2: c1 = 0.01, c2 = 0.01, and C1 = 15 \u221a \u03b7\n2. s := max-outlier-frac-col \u00b7 n\n3. \u03c6+ = 1.2\n4. bound on max-outlier-frac-row\u03b1: b0 := 0.01/f 2.\n5. q0 := 1.2(\u03b5+ SE(Pj\u22121,Pj)), qk = (0.3) kq0\n6. et := x\u0302t \u2212 xt. Since \u03bdt = 0, et = `t \u2212 \u02c6\u0300t\n7. Events: \u03930,0 := {assumed bound on SE(P\u03020,P0)}, \u03930,k := \u03930,k\u22121 \u2229 {SE(P\u03020,k,P0) \u2264 0.3kSE(P\u03020,P0)}, \u0393j,0 := \u0393j\u22121,K , \u0393j,k := \u0393j,k\u22121 \u2229 {SE(P\u0302j,k,Pj) \u2264 qk\u22121/4} for j = 1, 2, . . . , J and k = 1, 2, . . . ,K.\n8. Using the expression for K given in the theorem, and since P\u0302j = P\u0302j,k (from the Algorithm), it\nfollows that \u0393j,K implies SE(P\u0302j ,Pj) = SE(P\u0302j,K ,Pj) \u2264 \u03b5.\nObserve that, if we can show that Pr(\u0393J,K |\u03930,0) \u2265 1 \u2212 dn\u221210 we will have obtained all the subspace recovery bounds of Theorem 2.2. The next two lemmas applied sequentially help show that this is true for Algorithm 1 (tj known). The correctness of the actual algorithm (Algorithm 2) follows using these, Corollary 4.10, and Lemma 4.12. The Theorem\u2019s proof is in Appendix A.\nLemma 4.7 (first subspace update interval). Under the conditions of Theorem 2.2, conditioned on \u0393j,0, 1. for all t \u2208 [t\u0302j , t\u0302j + \u03b1), \u2016\u03a8`t\u2016 \u2264 (\u03b5 + \u2206) \u221a \u03b7r\u03bb+ < xmin/15, \u2016x\u0302t,cs \u2212 xt\u2016 \u2264 7xmin/15 < xmin/2,\nT\u0302t = Tt, and the error et := x\u0302t \u2212 xt = `t \u2212 \u02c6\u0300t satisfies\net = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 ITt \u2032\u03a8`t, (1)\nand \u2016et\u2016 \u2264 1.2(\u03b5+ \u2206) \u221a \u03b7r\u03bb+.\n2. w.p. at least 1 \u2212 10n\u221210, the first subspace estimate P\u0302j,1 satisfies SE(P\u0302j,1,Pj) \u2264 (q0/4), i.e., \u0393j,1 holds.\nLemma 4.8 (k-th subspace update interval). Under the conditions of Theorem 2.2, conditioned on \u0393j,k\u22121,\n1. for all t \u2208 [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1 \u2212 1), all claims of the first part of Lemma 4.7 holds, \u2016\u03a8`t\u2016 \u2264 0.3k\u22121(\u03b5+ \u2206) \u221a \u03b7r\u03bb+, and \u2016et\u2016 \u2264 (0.3)k\u22121 \u00b7 1.2(\u03b5+ \u2206) \u221a \u03b7r\u03bb+.\n2. w.p. at least 1\u221210n\u221210 the subspace estimate P\u0302j,k satisfies SE(P\u0302j,k,Pj) \u2264 (qk\u22121/4), i.e., \u0393j,k holds.\nRemark 4.9. For the case of j = 0, in both the lemmas above, \u2206 gets replaced by SE(P\u03020,P0) and \u03b5 by zero.\nCorollary 4.10. Under the conditions of Theorem 2.2 the following hold\n1. For all t \u2208 [tj , t\u0302j), conditioned on \u0393j\u22121,K , all claims of the first item of Lemma 4.7 hold.\n2. For all t \u2208 [t\u0302j +K\u03b1, tj+1), conditioned on \u0393j,K , the first item of Lemma 4.8 holds with k = K.\nThus, for all t, by the above two claims and Lemmas 4.7, 4.8, under appropriate conditioning, et satisfies (1).\nWe prove these lemmas in the next few subsections. The projected CS proof (item one of both lemmas) uses the following lemma from [12] that relates the s-Restricted Isometry Constant (RIC), \u03b4s(.), [22] of a projection matrix to the incoherence of its orthogonal complement.\nLemma 4.11. [[12]] For an n \u00d7 r basis matrix P , (1) \u03b4s(I \u2212 PP \u2032) = max|T |\u2264s \u2016IT \u2032P \u20162; and (2) max|T |\u2264s \u2016IT \u2032P \u20162 \u2264 smaxi=1,2,...,n \u2016Ii\u2032P \u20162 \u2264 s\u00b5r/n.\nThe last bound of the above lemma is a consequence of Definition 1.1. We apply this lemma with s = max-outlier-frac-col \u00b7 n. The subspace update step proof (item 2 of both the above lemmas) uses a guarantee for PCA in sparse data-dependent noise, Theorem 4.14, due to [25]. Notice that et = `t\u2212 \u02c6\u0300t is the noise/error seen by the subspace update step. By (1), this is sparse and depends on the true data `t.\nConsider the actual tj unknown case. The following lemma is used to show that, whp, we can detect subspace change within 2\u03b1 time instants. This lemmas assumes detection threshold \u03c9evals = 2\u03b5 2\u03bb+ (see Algorithm 2).\nLemma 4.12 (Subspace Change Detection). Consider an \u03b1-length time interval J \u03b1 \u2282 [tj , tj+1] (so that `t = Pjat).\n1. If \u03a6 := I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032 and SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5, with probability at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300t \u2032\u03a6\n) \u2265 0.28\u03bb\u2212SE2(Pj\u22121,Pj) > \u03c9evals\n2. If \u03a6 := I \u2212 P\u0302jP\u0302j \u2032 and SE(P\u0302j ,Pj) \u2264 \u03b5, with probability at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300t \u2032\u03a6\n) \u2264 1.37\u03b52\u03bb+ < \u03c9evals"}, {"heading": "4.3 Proof of Lemma 4.7: projected CS and subspace update in the first update interval", "text": "We first state a simple lemma. This is proved in Appendix B.\nLemma 4.13. Let Q1, Q2 and Q3 be r-dimensional subspaces in Rn such that SE(Q1,Q2) \u2264 \u22061 and SE(Q2,Q3) \u2264 \u22062. Then, SE(Q1,Q3) \u2264 \u22061 + \u22062.\nProof of Lemma 4.7. Recall from Definition 4.6 that s := max-outlier-frac-col \u00b7n and \u03c6+ = 1.2. Recall also that, for simplicity, we are considering the \u03bdt = 0 case. Proof of item 1. First consider j > 0. We have conditioned on the event \u0393j,0 := \u0393j\u22121,K . This implies that SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5. We consider the interval t \u2208 [t\u0302j , t\u0302j +\u03b1). For this interval, P\u0302(t\u22121) = P\u0302j\u22121 and thus \u03a8 = I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032 (from Algorithm). For the sparse recovery step, we first need to bound the 2s-RIC of \u03a8. To do this, we first obtain bound on max|T |\u22642s \u2016IT \u2032P\u0302j\u22121\u2016 as follows. Consider any set T such that |T | \u2264 2s. Then,\u2225\u2225\u2225IT \u2032P\u0302j\u22121\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225IT \u2032(I \u2212 Pj\u22121Pj\u22121\u2032)P\u0302j\u22121\u2225\u2225\u2225+ \u2225\u2225\u2225IT \u2032Pj\u22121Pj\u22121\u2032P\u0302j\u22121\u2225\u2225\u2225 \u2264 SE(Pj\u22121, P\u0302j\u22121) + \u2225\u2225IT \u2032Pj\u22121\u2225\u2225 = SE(P\u0302j\u22121,Pj\u22121) + \u2225\u2225IT \u2032Pj\u22121\u2225\u2225\nUsing Lemma 4.11, and the bound on max-outlier-frac-col from Theorem 2.2,\nmax |T |\u22642s \u2016IT \u2032Pj\u22121\u20162 \u2264 2smax i \u2016Ii\u2032Pj\u22121\u20162 \u2264\n2s\u00b5r\nn \u2264 0.01 (2)\nThus, using SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5,\nmax |T |\u22642s \u2225\u2225\u2225IT \u2032P\u0302j\u22121\u2225\u2225\u2225 \u2264 \u03b5+ max |T |\u22642s \u2225\u2225IT \u2032Pj\u22121\u2225\u2225 \u2264 \u03b5+ 0.1 Finally, using Lemma 4.11, \u03b42s(\u03a8) \u2264 0.112 < 0.15. Hence\u2225\u2225\u2225(\u03a8Tt \u2032\u03a8Tt)\u22121\u2225\u2225\u2225 \u2264 11\u2212 \u03b4s(\u03a8) \u2264 11\u2212 \u03b42s(\u03a8) \u2264 11\u2212 0.15 < 1.2 = \u03c6+.\nWhen j = 0, there are some minor changes. From the initialization assumption, we have SE(P\u03020,P0) \u2264 0.25. Thus, max|T |\u22642s \u2225\u2225\u2225IT \u2032P\u03020\u2225\u2225\u2225 \u2264 0.25 + 0.1 = 0.35. Thus, using Lemma 4.11, \u03b42s(\u03a80) \u2264 0.352 < 0.15. The rest of the proof given below is the same for j = 0 and j > 0.\nNext we bound norm of bt := \u03a8`t. This and the RIC bound will then be used to bound \u2016x\u0302t,cs \u2212 xt\u2016. \u2016bt\u2016 = \u2016\u03a8`t\u2016 = \u2225\u2225\u2225(I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pjat\u2225\u2225\u2225 \u2264 SE(P\u0302j\u22121,Pj) \u2016at\u2016\n(a) \u2264 (\u03b5+ SE(Pj\u22121,Pj)) \u221a \u03b7r\u03bb+ := bb\nwhere (a) follows from Lemma 4.13 with Q1 = P\u0302j\u22121, Q2 = Pj\u22121 and Q3 = Pj . Under the assumptions of Theorem 2.2, bb < xmin/15. This is why we have set \u03be = xmin/15 in the Algorithm. Using these facts, and \u03b42s(\u03a8) \u2264 0.15, the CS guarantee from [22, Theorem 1.3] implies that\n\u2016x\u0302t,cs \u2212 xt\u2016 \u2264 7\u03be = 7xmin/15 < xmin/2\nConsider support recovery. From above,\n|(x\u0302t,cs \u2212 xt)i| \u2264 \u2016x\u0302t,cs \u2212 xt\u2016 \u2264 7xmin/15 < xmin/2\nThe Algorithm sets \u03c9supp = xmin/2. Consider an index i \u2208 Tt. Since |(xt)i| \u2265 xmin,\nxmin \u2212 |(x\u0302t,cs)i| \u2264 |(xt)i| \u2212 |(x\u0302t,cs)i| \u2264 |(xt \u2212 x\u0302t,cs)i| < xmin\n2\nThus, |(x\u0302t,cs)i| > xmin2 = \u03c9supp which means i \u2208 T\u0302t. Hence Tt \u2286 T\u0302t. Next, consider any j /\u2208 Tt. Then, (xt)j = 0 and so\n|(x\u0302t,cs)j | = |(x\u0302t,cs)j)| \u2212 |(xt)j | \u2264 |(x\u0302t,cs)j \u2212 (xt)j | \u2264 bb < xmin\n2\nwhich implies j /\u2208 T\u0302t and T\u0302t \u2286 Tt implying that T\u0302t = Tt. Finally, we get an expression for et and bound it. With T\u0302t = Tt and since Tt is the support of xt, xt = ITtITt \u2032xt, and so\nx\u0302t = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 \u03a8Tt \u2032(\u03a8`t + \u03a8xt) = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 ITt \u2032\u03a8`t + xt\nsince \u03a8Tt \u2032\u03a8 = I \u2032Tt\u03a8 \u2032\u03a8 = ITt \u2032\u03a8. Thus et = x\u0302t \u2212 xt satisfies\net = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 ITt \u2032\u03a8`t and\n\u2016et\u2016 \u2264 \u2225\u2225\u2225(\u03a8Tt \u2032\u03a8Tt)\u22121\u2225\u2225\u2225\u2225\u2225ITt \u2032\u03a8`t\u2225\u2225 \u2264 \u03c6+ \u2225\u2225ITt \u2032\u03a8`t\u2225\u2225 \u2264 1.2bb\nProof of Item 2 : We will use the following result from [25, Remark 4.18].\nTheorem 4.14 (PCA in sparse data-dependent noise (PCA-SDDN)). We are given data vectors yt := `t + wt with wt = ITtMs,t`t, t = 1, 2, . . . , \u03b1, where Tt is the support set of wt, and `t = Pat with at satisfying the assumptions of Theorem 2.2. Pick an \u03b5SE > 0. Assume that maxt \u2016Ms,tP \u20162 \u2264 q < 1, the fraction of non-zeroes in any row of the noise matrix [w1,w2, . . . ,w\u03b1] is bounded by b, and 3 \u221a bqf \u2264 0.9\u03b5SE/(1 + \u03b5SE). Define\n\u03b10 := C\u03b7 q2f2\n\u03b52SE r log n\nFor an \u03b1 \u2265 \u03b10, let P\u0302 be the matrix of top r eigenvectors of D := 1\u03b1 \u2211\u03b1 t=1 yty \u2032 t. With probability at least 1\u2212 10n\u221210, SE(P\u0302 ,P ) \u2264 \u03b5SE.\nSince \u02c6\u0300t = `t \u2212 et with et satisfying (1), updating P\u0302(t) from the \u02c6\u0300t\u2019s is a problem of PCA in sparse data-dependent noise (SDDN), et. To analyze this, we use Theorem 4.14 given above. Recall from Item 1 of this lemma that, for t \u2208 [t\u0302j , t\u0302j + \u03b1), et satisfies (1). Recall from the Algorithm that we compute the first estimate of the j-th subspace, P\u0302j,1, as the top r eigenvectors of 1 \u03b1 \u2211t\u0302j+\u03b1\u22121 t=t\u0302j \u02c6\u0300 t \u02c6\u0300 t \u2032. In the notation of Theorem 4.14, yt \u2261 \u02c6\u0300t, wt \u2261 et, `t \u2261 `t, Ms,t = \u2212 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032, P\u0302 = P\u0302j,1, P = Pj , and so \u2016Ms,tP \u2016 = \u2016 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032Pj\u2016 \u2264 1.2(\u03b5 + SE(Pj\u22121,Pj)) := q0. Also, b \u2261 b0 which is the upper bound on max-outlier-frac-row\u03b1 (see Definition 4.6). Applying Theorem 4.14 with q \u2261 q0, b \u2261 b0 and using \u03b5SE = q0/4, observe that we require \u221a\nb0q0f \u2264 0.9(q0/4)\n1 + (q0/4) .\nSince q0 = 1.2(\u03b5 + SE(Pj\u22121,Pj)) < 1.2(0.01 + 0.8) < 0.98 (follows from the bounds on \u03b5 and on \u2206 given in Theorem 2.2), this holds if \u221a b0f \u2264 0.18. This is true since we have assumed b0 = 0.01/f2 (see Definition 4.6). Thus, from Theorem 4.14, with probability at least 1 \u2212 10n\u221210, SE(P\u0302j,1,Pj) \u2264 q0/4. Thus, conditioned on \u0393j,0, with this probability, \u0393j,1 holds.\nRemark 4.15 (Clarification about conditioning). In the proof above we have used Theorem 4.14 which assumes that, for t \u2208 J \u03b1, the at\u2019s are mutually independent and the matrices Ms,t are either nonrandom or are independent of the at\u2019s for this interval. When we apply the theorem for our proof, we are conditioning on \u0393j,0. This does not cause any problem since the event \u0393j,0 is a function of the random variable yold := {y1,y2, . . . ,yt\u0302j\u22121} where as our summation is over J \u03b1 := [t\u0302j , t\u0302j + \u03b1). Also, by Theorem 2.2, at\u2019s are independent of the outlier supports Tt. To be precise, we are applying Theorem 4.14 conditioned on yold, for any yold \u2208 \u0393j,0. Even conditioned on yold, clearly, the matrices Ms,t used above are independent of the at\u2019s for this interval. Also, even conditioned on yold, the at\u2019s for t \u2208 [t\u0302j , t\u0302j+\u03b1) are clearly mutually independent. Thus, the theorem can be applied. Its conclusion then tells us that, for any yold \u2208 \u0393j,0, conditioned on yold, with probability at least 1\u221210n\u221210, SE(P\u0302j,1,Pj) \u2264 q0/4. Since this holds with the same probability for all yold \u2208 \u0393j,0, it also holds with the same probability when we condition on \u0393j,0. Thus, conditioned on \u0393j,0, with this probability, \u0393j,1 holds.\nAn analogous argument will also apply to the following proofs.\n4.4 Proof of Lemma 4.8: lemma for projected CS and subspace update in k-th update interval\nProof of Lemma 4.8. We first present the proof for the k = 2 case and then generalize it for an arbitrary k.\n(A) k = 2: We have conditioned on \u0393j,1. This implies that SE(P\u0302j,1,Pj) \u2264 q0/4.\nProof of Item 1 : We consider the interval t \u2208 [t\u0302j + \u03b1, t\u0302j + 2\u03b1). For this interval, P\u0302(t\u22121) = P\u0302j,1 and thus \u03a8 = I \u2212 P\u0302j,1P\u0302j,1\u2032 (from Algorithm). For the sparse recovery step, we need to bound the 2s-RIC of \u03a8. Consider any set T such that |T | \u2264 2s. We have\u2225\u2225\u2225IT \u2032P\u0302j,1\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225IT \u2032(I \u2212 PjPj \u2032)P\u0302j,1\u2225\u2225\u2225+ \u2225\u2225\u2225IT \u2032PjPj \u2032P\u0302j,1\u2225\u2225\u2225\n\u2264 SE(Pj , P\u0302j,1) + \u2225\u2225IT \u2032Pj\u2225\u2225 = SE(P\u0302j,1,Pj) + \u2225\u2225IT \u2032Pj\u2225\u2225\nThe equality holds since SE is symmetric for subspaces of the same dimension. Using SE(P\u0302j,1,Pj) \u2264 q0/4, (2),\nmax |T |\u22642s \u2225\u2225\u2225IT \u2032P\u0302j,1\u2225\u2225\u2225 \u2264 q0/4 + max |T |\u22642s \u2225\u2225IT \u2032Pj\u2225\u2225 \u2264 q0/4 + 0.1. Finally, from using the assumptions of Theorem 2.2, q0 \u2264 0.96. Using this and Lemma 4.11,\n\u03b42s(\u03a8j) = max |T |\u22642s \u2225\u2225\u2225IT \u2032P\u0302j,1\u2225\u2225\u22252 \u2264 0.352 < 0.15. From, this \u2225\u2225\u2225(\u03a8Tt \u2032\u03a8Tt)\u22121\u2225\u2225\u2225 \u2264 11\u2212 \u03b4s(\u03a8) \u2264 11\u2212 \u03b42s(\u03a8) \u2264 11\u2212 0.15 < 1.2 = \u03c6+. Consider \u2016bt\u2016.\n\u2016bt\u2016 = \u2016\u03a8`t\u2016 = \u2225\u2225\u2225(I \u2212 P\u0302j,1P\u0302j,1\u2032)Pjat\u2225\u2225\u2225 \u2264 SE(P\u0302j,1,Pj) \u2016at\u2016 \u2264 (q0/4)\u221a\u03b7r\u03bb+\n(a) \u2264 0.3(\u03b5+ SE(Pj\u22121,Pj)) \u221a \u03b7r\u03bb+ := 0.3bb\nWe have 0.3bb < bb < xmin/15 as in the proof of Lemma 4.7. The rest of the proof is the same too. Notice here that, we could have loosened the required lower bound on xmin for this interval.\nProof of Item 2 : Again, updating P\u0302(t) using \u02c6\u0300t\u2019s is a problem of PCA in sparse data-dependent noise (SDDN), et. We use the result of Theorem 4.14. Recall from Item 1 of this lemma that, for\nt \u2208 [t\u0302j + \u03b1, t\u0302j + 2\u03b1), et satisfies (1). We compute P\u0302j,2 as the top r eigenvectors of 1\u03b1 \u2211t\u0302j+2\u03b1\u22121 t=t\u0302j+\u03b1 \u02c6\u0300 t \u02c6\u0300 t \u2032. In notation of Theorem 4.14, yt \u2261 \u02c6\u0300t, wt \u2261 et, `t \u2261 `t, P \u2261 Pj , P\u0302 \u2261 Pj,2, and Ms,t = \u2212 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032. So \u2016Ms,tPj\u2016 = \u2016 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032Pj\u2016 \u2264 (\u03c6+/4)q0 := q1. Applying Theorem 4.14 with q \u2261 q1, b \u2261 b0 (b0 bounds max-outlier-frac-row\u03b1), and setting \u03b5SE = q1/4, observe that we require\u221a\nb0q1f \u2264 0.9(q1/4)\n1 + (q1/4)\nwhich holds if \u221a b0f \u2264 0.18. This is ensured since b0 = 0.01/f2. Thus, from Theorem 4.14, with probability at least 1\u2212 10n\u221210, SE(P\u0302j,2,Pj) \u2264 (q1/4) = 0.25 \u00b7 0.3q0. Thus, with this probability, conditioned on \u0393j,1, \u0393j,2 holds.\n(B) General k: We have conditioned on \u0393j,k\u22121. This implies that SE(P\u0302j,k\u22121,Pj) \u2264 qk\u22121/4. Proof of Item 1 : Consider the interval [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1). In this interval, P\u0302(t\u22121) = P\u0302j,k\u22121 and thus \u03a8 = I \u2212 P\u0302j,k\u22121P\u0302j,k\u22121\u2032. Using the same idea as for the k = 2 case, we have that for the k-th interval, qk\u22121 = (\u03c6 +/4)k\u22121q0. Pick \u03b5SE = (qk\u22121/4). From this it is easy to see that\n\u03b42s(\u03a8) \u2264 (\nmax |T |\u22642s \u2225\u2225\u2225IT \u2032P\u0302j,k\u22121\u2225\u2225\u2225)2 \u2264 (SE(P\u0302j,k\u22121,Pj) + max |T |\u22642s \u2225\u2225IT \u2032Pj\u2225\u2225)2 (a) \u2264 (SE(P\u0302j,k\u22121,Pj) + 0.1)2 \u2264 ((\u03c6+/4)k\u22121(\u03b5+ SE(Pj\u22121,Pj) + 0.1)2 < 0.15\nwhere (a) follows from (2). Using the approach Lemma 4.7, \u2016\u03a8`t\u2016 \u2264 SE(P\u0302j,k\u22121,Pj) \u2016at\u2016 \u2264 (\u03c6+/4)k\u22121(\u03b5+ SE(Pj\u22121,Pj)) \u221a \u03b7r\u03bb+\n(a) \u2264 (\u03c6+/4)k\u22121(\u03b5+ \u2206) \u221a \u03b7r\u03bb+ := (\u03c6+/4)k\u22121bb\nProof of Item 2 : Again, updating P\u0302(t) from \u02c6\u0300t\u2019s is a problem of PCA in sparse data-dependent noise given in Theorem 4.14. From Item 1 of this lemma that, for t \u2208 [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1], et satisfies (1). We update the subspace, P\u0302j,k as the top r eigenvectors of 1 \u03b1 \u2211t\u0302j+k\u03b1\u22121 t=t\u0302j+(k\u22121)\u03b1 \u02c6\u0300 t \u02c6\u0300 t \u2032. In the setting above yt \u2261 \u02c6\u0300t, wt \u2261 et, `t \u2261 `t, and Ms,t = \u2212 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032, and so \u2016Ms,tPj\u2016 = \u2016 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032Pj\u2016 \u2264 (\u03c6+/4)k\u22121q0 := qk\u22121. Applying Theorem 4.14 with q \u2261 qk\u22121, b \u2261 b0 (b0 bounds max-outlier-frac-row\u03b1), and setting \u03b5SE = qk\u22121/4, we require \u221a\nb0qk\u22121f \u2264 0.9(qk\u22121/4)\n1 + (qk\u22121/4)\nwhich holds if \u221a b0f \u2264 0.12. This is true by our assumption. Thus, from Theorem 4.14, with probability at least 1\u2212 10n\u221210, SE(P\u0302j,k,Pj) \u2264 (\u03c6+/4)k\u22121q1. Thus, with this probability, conditioned on \u0393j,k\u22121, \u0393j,k holds."}, {"heading": "4.5 Proof of Lemma 4.12: subspace change detection lemma", "text": "Proof of Lemma 4.12. The proof uses the following lemma. It is proved in Appendix B. The proof uses Cauchy-Schwartz for sums of matrices, followed by either matrix Bernstein [28] or Vershynin\u2019s subGaussian result [29].\nLemma 4.16 (Concentration Bounds). Assume that the assumptions of Theorem 2.2 hold. For this lemma assume that `t = Pat, \u03a6 := I \u2212 P\u0302 P\u0302 \u2032, et = M2,tM1,t`t, with \u2016 1\u03b1 \u2211 t\u2208J \u03b1M2,tM2,t \u2032\u2016 \u2264 b0 and \u2016M1,tP \u2016 \u2264 q. Assume that event E0 holds. Then,\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t atat \u2032 \u2212\u039b \u2225\u2225\u2225\u2225\u2225 \u2264 0\u03bb\u2212 ) \u2265 1\u2212 10n\u221210\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264 (1 + 1)SE(P\u0302 ,P )\u221ab0q\u03bb+ ) \u2265 1\u2212 10n\u221210\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264 (1 + 2)\u221ab0q2\u03bb+ ) \u2265 1\u2212 10n\u221210\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6`t\u03bdt \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264 l,v\u03bb\u2212 ) \u2265 1\u2212 2n\u221210\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6\u03bdt\u03bdt \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264 (\u03b52 + v,v)\u03bb\u2212 ) \u2265 1\u2212 2n\u221210\nProof of Item (a): First from Corollary 4.10, note that for t \u2208 [tj , t\u0302j ], the error et satisfies (1). We\nhave\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6[Pjatat \u2032Pj \u2032 + etet \u2032 + `tet \u2032 + et`t \u2032]\u03a6 ) (a) \u2265 \u03bbmax ( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6Pjatat \u2032Pj \u2032\u03a6\n) + \u03bbmin ( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6[etet \u2032 + `tet \u2032 + et`t \u2032]\u03a6\n)\n\u2265 \u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6Pjatat \u2032Pj \u2032\u03a6\n) \u2212 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225\u2212 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 := \u03bbmax(T )\u2212\n\u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225\u2212 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 (3) where (a) follows from Weyl\u2019s Inequality. Now we bound the second and third terms by invoking Lemma 4.16 with E0 := {SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5}, P\u0302 \u2261 P\u0302j\u22121, P \u2261 Pj , M2,t \u2261 ITt and M1,t \u2261 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032, where \u03a8 = I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032. Thus, b0 \u2261 b0, q \u2261 q0. Thus, with probability at least 1\u2212 4n\u221210,\u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225+ 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264\u221ab0q20\u03bb+(1 + 2) + 2\u221ab0q0(SE(Pj\u22121,Pj) + \u03b5)\u03bb+(1 + 1) (4)\nThe above equation uses the fact that SE(P\u0302j\u22121,Pj) \u2264 \u03b5 + SE(Pj\u22121,Pj) which is a direct consequence of using Lemma 4.13. We bound the first term of (3) as follows. Let \u03a6Pj QR = EjRj be its reduced QR decomposition. Thus Ej is an n\u00d7r matrix with orthonormal columns and Rj is an r\u00d7r upper triangular matrix. Let\nA := Rj\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 atat \u2032\n) Rj \u2032.\nObserve that T can also be written as T = [ Ej Ej,\u22a5 ] [A 0 0 0 ] [ Ej \u2032 Ej,\u22a5 \u2032 ] (5)\nand thus \u03bbmax(A) = \u03bbmax(T ). We work with \u03bbmax(A) in the sequel. We will use the following simple claim.\nClaim 4.17. If X 0 (i.e., X is a p.s.d matrix), where X \u2208 Rr\u00d7r, then RXR\u2032 0 for all R \u2208 Rr\u00d7r.\nProof. Since X is p.s.d., y\u2032Xy \u2265 0 for any vector y. Use this with y = R\u2032z for any z \u2208 Rr. We get z\u2032RXR\u2032z \u2265 0. Since this holds for all z, RXR\u2032 0.\nUsing Lemma 4.16, it follows that\nPr\n( 1\n\u03b1 \u2211 t atat \u2032 \u2212 (\u03bb\u2212 \u2212 0)I 0\n) \u2265 1\u2212 2n\u221210.\nUsing Claim 4.17, with probability 1\u2212 2n\u221210,\nRj\n( 1\n\u03b1 \u2211 t atat \u2032 \u2212 (\u03bb\u2212 \u2212 0)I\n) Rj \u2032 0\n=\u21d2 \u03bbmin ( Rj ( 1\n\u03b1 \u2211 t atat \u2032 \u2212 (\u03bb\u2212 \u2212 0)I\n) Rj \u2032 ) \u2265 0\nUsing Weyl\u2019s inequality [30], with the same probability,\n\u03bbmin ( Rj ( 1\n\u03b1 \u2211 t atat \u2032 \u2212 (\u03bb\u2212 \u2212 0)I\n) Rj \u2032 ) \u2264 \u03bbmax ( Rj ( 1\n\u03b1 \u2211 t atat \u2032\n) Rj \u2032 ) \u2212 (\u03bb\u2212 \u2212 0)\u03bbmax ( RjRj\n\u2032) and so,\n\u03bbmax(A) \u2265 (\u03bb\u2212 \u2212 0)\u03bbmax(RjRj \u2032).\nWe now obtain a lower bound on the second term in the rhs above.\n\u03bbmax(RjRj \u2032) = \u03bbmax(Pj \u2032(I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pj)) = \u03bbmax(Pj \u2032(I \u2212 Pj\u22121Pj\u22121\u2032 + Pj\u22121Pj\u22121\u2032 \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pj)) \u2265 \u03bbmax(Pj \u2032(I \u2212 Pj\u22121Pj\u22121\u2032)Pj) + \u03bbmin(Pj \u2032(Pj\u22121Pj\u22121\u2032 \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pj) = \u03c32max((I \u2212 Pj\u22121Pj\u22121\u2032)Pj) + \u03bbmin(Pj \u2032(Pj\u22121Pj\u22121\u2032 \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pj)\n\u2265 SE2(Pj\u22121,Pj)\u2212 \u2225\u2225\u2225Pj \u2032(Pj\u22121Pj\u22121\u2032 \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)Pj\u2225\u2225\u2225\n(a) \u2265 SE2(Pj\u22121,Pj)\u2212 \u2225\u2225\u2225Pj\u22121Pj\u22121\u2032 \u2212 P\u0302j\u22121P\u0302j\u22121\u2032\u2225\u2225\u2225 \u2265 SE2(Pj\u22121,Pj)\u2212 2\u03b5 (6)\nwhere we have used [12, Lemma 2.10]. Thus, combining (3), (4), (6), and using 0 = 0.01, 1 = 2 = 0.01, with probability at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300t \u2032\u03a6\n) \u2265 0.99\u03bb\u2212(SE2(Pj\u22121,Pj)\u2212 2\u03b5)\u2212 \u03bb+[ \u221a b0q0(1.01q0 + 2.02(\u03b5+ SE(Pj\u22121,Pj)))]\n(a) \u2265 \u03bb\u2212 [ 0.91SE2(Pj\u22121,Pj)\u2212 2.7 \u221a b0f(\u03b5+ SE(Pj\u22121,Pj) 2 ]\n(b) \u2265 \u03bb\u2212 [ 0.91SE2(Pj\u22121,Pj)\u2212 0.54(\u03b52 + SE2(Pj\u22121,Pj) ] \u2265 \u03bb\u2212SE2(Pj\u22121,Pj)(0.91\u2212 0.54 \u00b7 1.16) \u2265 0.28\u03bb\u2212SE2(Pj\u22121,Pj)\nwhere (a) uses q0 = 1.2(\u03b5 + SE(Pj\u22121,Pj)) and \u03b5 \u2264 0.03SE2(Pj\u22121,Pj)/f2 < 0.4SE2(Pj\u22121,Pj), (b) uses\u221a b0f = 0.1 and (a+ b)\n2 \u2264 2(a2 + b2). and the last inequality again uses \u03b5 \u2264 0.03SE2(Pj\u22121,Pj)/f2. Proof of Item (b): First, we recall that from Corollary 4.10, for t \u2208 [t\u0302j + K\u03b1, tj+1), the error et satisfies (7).\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300t \u2032\u03a6\n) \u2264 \u03bbmax ( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6`t`t \u2032\u03a6\n) + \u03bbmax ( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6[`tet \u2032 + et`t \u2032 + etet \u2032]\u03a6\n)\n\u2264 \u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6`t`t \u2032\u03a6\n) + \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225+ 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 := \u03bbmax(T ) +\n\u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225+ 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 To obtain bounds on the second and third terms in the equation above we invoked Lemma 4.16 with E0 := {SE(P\u0302j ,Pj) \u2264 \u03b5}, P\u0302 \u2261 P\u0302j , P \u2261 Pj , M2,t \u2261 ITt , M1,t \u2261 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032, where \u03a8 = I \u2212 P\u0302jP\u0302j \u2032 and b0 \u2261 b0, \u2261 qK , . Thus, with probability at least 1\u2212 10n\u221210,\u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211\nt\u2208J \u03b1 \u03a6etet\n\u2032\u03a6 \u2225\u2225\u2225\u2225\u2225+ 2 \u2225\u2225\u2225\u2225\u2225 1\u03b1 \u2211\nt\u2208J \u03b1 \u03a6`tet\n\u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264\u221ab0qK\u03bb+(qK(1 + 2) + 2(1 + 1)\u03b5)\nThe above equation also uses SE(P\u0302j ,Pj) \u2264 \u03b5. Proceeding as before to bound \u03bbmax(T ), define \u03a6Pj QR = EjRj , define A as before, we know \u03bbmax(T ) = \u03bbmax(Ej \u2032TEj) = \u03bbmax(A). Further,\n\u03bbmax(A) = \u03bbmax ( Rj ( 1\n\u03b1 \u2211 t\u2208J \u03b1 atat \u2032\n) Rj \u2032 ) (a) \u2264 \u03bbmax ( 1\n\u03b1 \u2211 t\u2208J \u03b1 atat\n) \u03bbmax(RjRj \u2032)\nwhere (a) uses Ostrowski\u2019s theorem [30, Theorem 5.4.9]. We have\n\u03bbmax(RjRj \u2032) = \u03c32max(Rj) = \u03c3 2 max((I \u2212 P\u0302jP\u0302j \u2032)Pj) \u2264 \u03b52\nand we can bound \u03bbmax( 1 \u03b1 \u2211 t\u2208J \u03b1 atat \u2032) using the first item of Lemma 4.16 with 0 = 0.01. Combining all of the above, and setting 1 = 2 = 0.01, when the subspace has not changed, with probability at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300t \u2032\u03a6\n) \u2264 \u03bb+[1.01\u03b52 + \u221a b0qK(1.01qK + 2.01\u03b5)] (a) \u2264 1.37\u03b52\u03bb+\nwhere (a) uses qK \u2264 \u03b5 and b0f2 = 0.01. Under the condition of Theorem 2.2, recall that \u03c9evals = 2\u03b52\u03bb+ and thus, with high probability, 1.37\u03b52\u03bb+ < \u03c9evals < 0.28\u03bb \u2212SE2(Pj\u22121,Pj)."}, {"heading": "5 Extensions", "text": ""}, {"heading": "5.1 Static Robust PCA", "text": "A useful corollary of our result for RST is that NORST is also the first online algorithm that provides a provable finite sample guarantee for the static Robust PCA problem. Static RPCA is our problem setting with J = 1, or in other words, with `t = Pat. A recent work, [21], developed an online stochastic optimization based reformulation of PCP, called ORPCA, to solve this. Their paper provides only a partial guarantee because the guarantee assumes that the intermediate algorithm estimates, P\u0302(t), are fullrank. Moreover the guarantee is only asymptotic. Instead our result given below is a complete guarantee and is non-asymptotic.\nCorollary 5.18 (Static RPCA). Consider Algorithm 1 with t2 = \u221e. Theorem 2.2 holds with following modification: replace the slow subspace change assumption with a fixed subspace P . Everything else remains as is, but with r \u2261 rL. Under the assumptions of Theorem 2.2, all the conclusions hold with same probability. The time and memory complexity are O(ndr log(1/ )) and O(nr log n log(1/ )).\nIn applications such as \u201crobust\u201d dimensionality reduction [31, 32] where the objective is to just obtain the top-r directions along which the variability of data is maximized, we only need the first K\u03b1 = Cf2r log n log(1/\u03b5) samples to obtain an \u03b5-accurate subspace estimate. If only these are used, the time complexity reduces to O(nK\u03b1r log(1/\u03b5)) = O(nr2 log n log2(1/ )). This is faster than even NO-RMC [11] and does not require d \u2248 n; of course it requires the other extra assumptions discussed earlier."}, {"heading": "5.2 Subspace Tracking with Missing Data (ST-missing) and Dynamic Matrix Completion", "text": "Another useful corollary of our result is a guarantee for the ST-missing problem. Consider the subspace tracking with missing data (ST-missing) problem. By setting the missing entries at time t to zero, and by defining Tt to be the set of missing entries at time t, we observe n-dimensional vectors that satisfy\nyt := `t \u2212 ITtITt \u2032`t, for t = 1, 2, . . . , d. (7)\nAlgorithm 3 NORST-Random for subspace tracking with missing data (ST-missing) Algorithm 2 with the following changes\n1. Replace line 3 with: compute P\u03020 \u2190 P\u0302init \u2190 Generate an n\u00d7 r basis matrix from the random orthogonal model; j \u2190 1, k \u2190 1\n2. Replace line 6 with the following\n\u2022 \u03a8\u2190 I \u2212 P\u0302(t\u22121)P\u0302(t\u22121)\u2032; y\u0303t \u2190 \u03a8yt; x\u0302t \u2190 ITt(\u03a8Tt \u2032\u03a8Tt)\u22121\u03a8Tt \u2032y\u0303t; \u02c6\u0300t \u2190 yt \u2212 x\u0302t.\nwith xt \u2261 ITtITt \u2032`t. This can be interpreted as a special case of the RST problem where the set Tt is known. Because there are no sparse corruptions (outliers), there is no xmin. Thus the initialization error need not be O(1/ \u221a r) (needed in the RST result to ensure a reasonable lower bound on xmin) and so one can even use random initialization. We assume that the initialization is obtained using the Random Orthogonal Model described in [33]. As explained in [33], a basis matrix generated from this model is already \u00b5-incoherent. We have the following corollary. The only change in its proof is the proof of the first subspace update interval for the j = 0 case.\nCorollary 5.19 (ST-missing). Consider NORST-Random (Algorithm 3). If the assumptions of Theorem 2.2 on at and \u03bdt hold, Pj\u2019s are \u00b5-incoherent, tj+1 \u2212 tj > (K + 2)\u03b1, \u2206 < 0.8, the outlier fraction bounds given in Theorem 2.2 hold, and if, for t \u2208 [t0, t1], max-outlier-frac-col \u2264 c/(log n), then all conclusions of Theorem 2.2 on P\u0302(t) and on `t hold with the same probability.\nTo our knowledge, the above is the first complete non-asymptotic guarantee for ST-missing; and the first result that allows changing subspaces. All existing guarantees are either partial guarantees (make assumptions on intermediate algorithm estimates), e.g., [19], or provide only asymptotic results [16, 18]. Moreover, from a dynamic matrix completion viewpoint, it is also giving a matrix completion solution without assuming that the set of observed entries is generated from a uniform or a Bernoulli model. Of course the tradeoff is that it needs many more observed entries. All these points will be discussed in detail in follow-up work where we will also numerically evaluate NORST-random for this problem."}, {"heading": "5.3 Fewer than r directions change", "text": "It is possible to relax the lower bound on outlier magnitudes if not all of the subspace directions change at a given subspace change time. Suppose that only rch < r directions change. When rch = 1, we recover the guarantee of [15] but for NORST (which is a simpler algorithm than s-reprocs).\nLet Pj\u22121,fix denote a basis for the fixed component of Pj\u22121 and let Pj\u22121,ch denote a basis for its\nchanging component. Thus, Pj\u22121R = [Pj\u22121,fix,Pj\u22121,ch], where R is a r \u00d7 r rotation matrix. We have\nPj = [Pj\u22121,fix,Pj,chd] (8)\nwhere Pj,chd is the changed component and has the same dimension as Pj\u22121,ch. Thus,\nSE(Pj\u22121,Pj) = SE(Pj\u22121,ch,Pj,chd) (9)\nand so \u2206 = maxj SE(Pj\u22121,Pj) = maxj SE(Pj\u22121,ch,Pj,chd). Let \u03bb + ch denote the largest eigenvalue along any direction in Pj,chd.\nCorollary 5.20. In Algorithm 2, replace line 17 by P\u0302(t) \u2190 basis(P\u0302j\u22121, P\u0302j,k). For basis matrices P1,P2, we use P = basis(P1,P2) to mean that P is a basis matrix with column span equal to the column span of\n[P1,P2]. Assume that (8) and (9) hold. Also assume that the conditions of Theorem 2.2 holds with the lower bound on xmin relaxed to xmin \u2265 C(\u03b5 \u221a \u03b7(r \u2212 rch)\u03bb+ + (\u03b5+ \u2206) \u221a \u03b7rch\u03bb + ch). Then, all conclusions of Theorem 2.2 hold."}, {"heading": "6 Empirical Evaluation", "text": "In this section we present the results for extensive numerical experiments on synthetic and real data to validate our theoretical claims. All time comparisons are performed on a Desktop Computer with Intel R\u00a9 Xeon E3-1240 8-core CPU @ 3.50GHz and 32GB RAM and all synthetic data experiments are averaged over 100 independent trials. The codes are available at https://github.com/praneethmurthy/NORST."}, {"heading": "6.1 Synthetic Data", "text": "We perform three experiments on synthetic data to corroborate our theoretical claims.\nExperiment 1. We compare the results of NORST and Offline-NORST with static RPCA algorithms, and Robust Subspace Tracking/Online RPCA methods proposed in literature. For our first experiment, we generate the changing subspaces using Pj = e \u03b3jBjPj\u22121 as done in [34] where \u03b3j controls the subspace change and Bj \u2019s are skew-symmetric matrices. In the first experiment we used the following parameters. n = 1000, d = 12000, J = 2, t1 = 3000, t2 = 8000, r = 30, \u03b31 = 0.001, \u03b32 = \u03b31 and the matrices B1 and B2 are generated as B1 = (B\u03031 \u2212 B\u03031) and B2 = (B\u03032 \u2212 B\u03032) where the entries of B\u03031, B\u03032 are generated independently from a standard normal distribution. We set \u03b1 = 300. This gives us the basis matrices P(t) for all t. To obtain the low-rank matrix L from this we generate the coefficients at \u2208 Rr as independent zero-mean, bounded random variables. They are (at)i i.i.d\u223c unif [\u2212qi, qi] where qi = \u221a f \u2212 \u221a f(i \u2212 1)/2r for i = 1, 2, \u00b7 \u00b7 \u00b7 , r \u2212 1 and qr = 1. thus the condition number is f and we selected f = 50. For the sparse supports, we considered two models according to which the supports are generated. First we use Model G.24 [15] which simulates a moving object pacing in the video. For the first ttrain = 100 frames, we used a smaller fraction of outliers with parameters s/n = 0.01, b0 = 0.01. For t > ttrain we used s/n = 0.05 and b0 = 0.3. Secondly, we used the Bernoulli model to simulate sampling uniformly at random, i.e., each entry of the matrix, is independently selected with probability \u03c1 or not selected with probability 1\u2212\u03c1. We generate the sparse supports using the Bernoulli model using \u03c1 = 0.01 for the first ttrain frames and \u03c1 = 0.3 for the subsequent frames. The sparse outlier magnitudes for both support models are generated uniformly at random from the interval [xmin, xmax] with xmin = 10 and xmax = 20.\nWe initialized the s-ReProCS and NORST algorithms using AltProj applied to Y[1,ttrain] with ttrain = 100. For the parameters to AltProj we used used the true value of r, 15 iterations and a threshold of 0.01. This, and the choice of \u03b31 and \u03b32 ensure that SE(P\u0302init,P0) \u2248 SE(P1,P0) \u2248 SE(P2,P1) \u2248 0.01. The other algorithm parameters are set as mentioned in the theorem, i.e., K = dlog(c/\u03b5)e = 8, \u03b1 = Cr log n = 300, \u03c9 = xmin/2 = 5 and \u03be = 7xmin/15 = 0.67, \u03c9evals = 2\u03b5\n2\u03bb+ = 7.5 \u00d7 10\u22124. For l1 minimization we used the YALL-1 toolbox [35] and set the tolerance to 10\u22124. For the least-squares step we use the Conjugate Gradient Least Squares instead of the well-known \u201cbackslash\u201d operator in MATLAB since this is a well conditioned problem. For this we set the tolerance as 10\u221210 and the number of iterations as 10. We have not done any code optimization such as use of MEX files for various sub-routines to speed up our algorithm. For the other online methods we implement the algorithms without modifications. The regularization parameter for ORPCA was set as with \u03bb1 = 1/ \u221a n and \u03bb2 = 1/ \u221a d according to [21]. Wherever possible we set the tolerance as 10\u22126 and 100 iterations to match that of our algorithm. As shown in Fig. 2, NORST is significantly better than all the RST methods - s-ReProCS [15], and two popular heuristics from literature - ORPCA [21] and GRASTA [34].\nWe also provide a comparison of offline techniques in Table 2. We must mention here that we implemented the static RPCA methods once on the entire data matrix, Y . We do this to provide a roughly equal comparison of the time taken. In principle, we could also implement the static techniques on disjoint batches of size \u03b1, but we observed that this did not yield significant improvement in terms of reconstruction accuracy, while being considerably slower, and thus we report only the latter setting. As can be seen, offline NORST outperforms all static RPCA methods, both for the moving object outlier support model and for the commonly used random Bernoulli support model. For the batch comparison we used PCP, AltProj and RPCA-GD. We set the regularization parameter for PCP 1/ \u221a n in accordance with [3]. The other known parameters, r for Alt-Proj, outlier-fraction for RPCA-GD, are set using the true values. Furthermore, for all algorithms (the IALM solver in case of PCP) we set the threshold as 10\u22126 and the number of iterations to 100 as opposed to 10\u22123 and 50 iterations which were set as default to provide a fair comparison with NORST and Offline-NORST. All results are averaged over 100 independent runs.\nExperiment 2. Next we perform an experiment to validate our claim of NORST admitting a higher fraction of outliers per row than the state of the art. In particular, since AltProj has the highest tolerance to max-outlier-frac-row, we only compare with it. The experiment proceeded as follows. We chose 10 different values of each of r and b0 (we slightly misuse notation here to let b0 := max-outlier-frac-row for this section only). For each pair of b0 and r we implemented NORST and ALtProj over 100 independent trials and computed the relative error in recovering L, i.e., we computed \u2016L\u0302 \u2212 L\u2016F /\u2016L\u2016F for each run. We computed the empirical probability of success, i.e., we enumerated the number of times out of 100 the error seen by each algorithm was less than a threshold, 0.5.\nFor each pair of {b0, r} we used the Bernoulli model for sparse support generation, the low rank matrix is generated exactly as done in the previous experiments with the exception that again to provide an equal footing, we increased the \u201csubspace change\u201d by setting \u03b31 and \u03b32 to 10 times the value that was used in the previous experiment. For the first ttrain frames we used b0 = 0.02. We provide the phase transition\nplots for both algorithm in Fig. 2. Here, white represents success while black represents failure. As can be seen, NORST is able to tolerate a much larger fraction of outlier-per-row as compared to AltProj.\nExperiment 3. Finally we perform an experiment to analyze the effect of the lower bound on the outlier magnitude xmin with the performance of NORST and AltProj. We show the results in Fig. 4. In the first stage, we generate the data exactly as done in the Moving Object sparse support model of the first experiment. The only change to the data generation parameters is that we now choose three different values of xmin = {0.5, 5, 10}. Furthermore, we set all the non-zero entries of the sparse matrix to be equal to xmin. This is actually harder than allowing the sparse outliers to take on any value since for a moderately low value of xmin the outlier-lower magnitude bound of Theorem 2.2 is violated. This is indeed confirmed by the numerical results presented in Fig. 4. (i) When xmin = 0.5, NORST works well since now all the outliers get classified as the small unstructured noise \u03bdt. (ii) When xmin = 10, NORST still works well because now xmin is large enough so that the outlier support is mostly correctly recovered. (iii) But when xmin = 5 the NORST reconstruction error stagnates around 10 \u22123.\nAll AltProj errors are much worse than those of NORST because the outlier fraction per row is the same as in the first experiment. What can be noticed though is that the variation with varying xmin is not that significant."}, {"heading": "6.2 Real Data", "text": "In this section we provide empirical results on real video for the task of Background Subtraction. For the AltProj algorithm we set r = 40. The remaining parameters were used with default setting. For NORST, we set \u03b1 = 60, K = 3, \u03bet = \u2016\u03a8 \u02c6\u0300t\u22121\u20162. We found that these parameters work for most videos that we\nverified our algorithm on. For RPCA-GD we set the \u201ccorruption fraction\u201d \u03b1 = 0.2 as described in their paper.\nMeeting Room (MR) dataset: The meeting room sequence is set of 1964 images of resolution 64\u00d7 80. The first 1755 frames consists of outlier-free data. Henceforth, we consider only the last 1209 frames. For NORST, we used ttrain = 400. In the first 400 frames, a person wearing a black shirt walks in, writes something on the board and goes back. In the subsequent frames, the person walks in with a white shirt. This is a challenging video sequence because the color of the person and the color of the curtain are hard to distinguish. NORST is able to perform the separation at around 43 frames per second. We present the results in Fig. 5\nLobby (LB) dataset: This dataset contains 1555 images of resolution 128\u00d7 160. The first 341 frames are outlier free. Here we use the first 400 \u201cnoisy\u201d frames as training data. The Alt Proj algorithm is used to obtain an initial estimate with rank, r = 40. The parameters used in all algorithms are exactly the same as above. NORST achieves a \u201ctest\u201d processing rate of 16 frames-per-second. We present the results in Fig. 6"}, {"heading": "Acknowledgments", "text": "The authors would like to thank Praneeth Netrapalli and Prateek Jain of Microsoft Research India for fruitful discussions on strengthening the guarantee by removing assumptions on subspace change model."}, {"heading": "A Proof of Theorem 2.2", "text": "We divide the proof into 3 parts for better clarity. We first prove the \u03bdt = 0 case for NORST (Algorithm 2), then prove the correctness of Offline NORST, and finally explain the changes needed when \u03bdt 6= 0.\nA.1 Proof with \u03bdt = 0\nRemark A.1 (Deriving the long expression forK given in the discussion). We have used max-outlier-frac-row\u03b1 \u2264 b0 with b0 = 0.01/f 2 throughout the analysis in order to simplify the proof. If we were not to do this, and if we used [25], it is possible to show that the \u201cdecay rate\u201d qk is of the form qk = (c2 \u221a b0f) kq0 from which it follows that to obtain an \u03b5-accurate approximation of the subspace it suffices to have\nK =\n\u2308 log (c1\u2206/\u03b5)\n\u2212 log(c2 \u221a b0f)\n\u2309 .\nWe first prove Theorem 2.2 for the case when tj \u2019s are known, i.e., correctness of Algorithm 1.\nProof of Theorem 2.2 with assuming tj known. In this case t\u0302j = tj . The proof is an easy consequence of Lemmas 4.7 and 4.8. Recall that \u0393j,K \u2286 \u0393j,K\u22121 \u2286 \u00b7 \u00b7 \u00b7\u0393j,0 and \u0393J,K \u2286 \u0393J\u22121,K \u2286 \u00b7 \u00b7 \u00b7 \u2286 \u03931,K . To show that the conclusions of the Theorem hold, it suffices to show that Pr(\u0393J,K |\u03930,0) \u2265 1\u2212 10dn\u221210. Using the chain rule of probability,\nPr(\u0393J,K |\u03931,0) = Pr(\u0393J,K ,\u0393J\u22121,K , \u00b7 \u00b7 \u00b7 ,\u03931,K |\u03931,0)\n= J\u220f j=1 Pr(\u0393j,K |\u0393j,0) = J\u220f j=1 Pr(\u0393j,K ,\u0393j,K\u22121, \u00b7 \u00b7 \u00b7 ,\u0393j,1|\u0393j,0) = J\u220f j=1 K\u220f k=1 Pr(\u0393j,k|\u0393j,k\u22121) (a) \u2265 (1\u2212 10n\u221210)JK \u2265 1\u2212 10JKn\u221210.\nwhere (a) used Pr(\u0393j,1|\u0393j,0) \u2265 1\u221210n\u221210 from Lemma 4.7 and Pr(\u0393j,k|\u0393j,k\u22121) \u2265 1\u221210n\u221210 from Lemma 4.8.\nProof of Theorem 2.2. Define\nt\u0302j\u22121,fin := t\u0302j\u22121 +K\u03b1, tj,\u2217 = t\u0302j\u22121,fin +\n\u2308 tj \u2212 t\u0302j\u22121,fin\n\u03b1\n\u2309 \u03b1\nThus, t\u0302j\u22121,fin is the time at which the (j \u2212 1)-th subspace update is complete; w.h.p., this occurs before tj . With this assumption, tj,\u2217 is such that tj lies in the interval [tj,\u2217 \u2212 \u03b1+ 1, tj,\u2217]. Recall from the algorithm that we increment j to j + 1 at t = t\u0302j +K\u03b1 := t\u0302j,fin. Define the events\n1. Det0 := {t\u0302j = tj,\u2217} = {\u03bbmax( 1\u03b1 \u2211tj,\u2217 t=tj,\u2217\u2212\u03b1+1 \u03a6 \u02c6\u0300 t \u02c6\u0300\u2032 t\u03a6) > \u03c9evals} and\nDet1 := {t\u0302j = tj,\u2217 + \u03b1} = {\u03bbmax( 1\u03b1 \u2211tj,\u2217+\u03b1 t=tj,\u2217+1 \u03a6 \u02c6\u0300t \u02c6\u0300 \u2032 t\u03a6) > \u03c9evals},\n2. SubUpd := \u2229Kk=1SubUpdk where SubUpdk := {SE(P\u0302j,k,Pj) \u2264 qk},\n3. NoFalseDets := {for all J \u03b1 \u2286 [t\u0302j,fin, tj+1), \u03bbmax( 1\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300 t \u02c6\u0300\u2032 t\u03a6) \u2264 \u03c9evals}\n4. \u03930,end := {SE(P\u03020,P0) \u2264 0.25}, 5. \u0393j,end := \u0393j\u22121,end \u2229 ( (Det0 \u2229 SubUpd \u2229NoFalseDets) \u222a (Det0 \u2229Det1 \u2229 SubUpd \u2229NoFalseDets) ) .\nLet p0 denote the probability that, conditioned on \u0393j\u22121,end, the change got detected at t = tj,\u2217, i.e.,\nlet\np0 := Pr(Det0|\u0393j\u22121,end).\nThus, Pr(Det0|\u0393j\u22121,end) = 1 \u2212 p0. It is not easy to bound p0. However, as we will see, this will not be needed. Assume that \u0393j\u22121,end\u2229Det0 holds. Consider the interval J \u03b1 := [tj,\u2217, tj,\u2217+\u03b1). This interval starts at or after tj , so, for all t in this interval, the subspace has changed. For this interval, \u03a6 = I\u2212 P\u0302j\u22121P\u0302j\u22121\u2032. Applying the first item of Lemma 4.12, w.p. at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300 \u2032 t\u03a6\n) \u2265 \u03c9evals\nand thus t\u0302j = tj,\u2217 + \u03b1. In other words,\nPr(Det1|\u0393j\u22121,end \u2229Det0) \u2265 1\u2212 10n\u221210.\nConditioned on \u0393j\u22121,end \u2229 Det0 \u2229 Det1, the first SVD step is done at t = t\u0302j + \u03b1 = tj,\u2217 + 2\u03b1 and the subsequent steps are done every \u03b1 samples. We can prove Lemma 4.7 with \u0393j,0 replaced by \u0393j,end\u2229Det0\u2229 Det1 and Lemma 4.8 with \u0393j,k\u22121 replaced by \u0393j,end \u2229Det0\u2229Det1\u2229SubUpd1 \u2229 \u00b7 \u00b7 \u00b7 \u2229SubUpdk\u22121 and with the k-th SVD interval being Jk := [t\u0302j + (k \u2212 1)\u03b1, t\u0302j + k\u03b1). Applying Lemmas 4.7, and 4.8 for each k, we get\nPr(SubUpd|\u0393j\u22121,end \u2229Det0 \u2229Det1) \u2265 (1\u2212 10n\u221210)K+1.\nWe can also do a similar thing for the case when the change is detected at tj,\u2217, i.e. when Det0 holds. In this case, we replace \u0393j,0 by \u0393j,end \u2229 Det0 and \u0393j,k by \u0393j,end \u2229 Det0 \u2229 SubUpd1 \u2229 \u00b7 \u00b7 \u00b7 \u2229 SubUpdk\u22121 and conclude that\nPr(SubUpd|\u0393j\u22121,end \u2229Det0) \u2265 (1\u2212 10n\u221210)K .\nFinally consider the NoFalseDets event. First, assume that \u0393j\u22121,end \u2229Det0\u2229SubUpd holds. Consider any interval J \u03b1 \u2286 [t\u0302j,fin, tj+1). In this interval, P\u0302(t) = P\u0302j , \u03a6 = I \u2212 P\u0302jP\u0302j \u2032 and SE(P\u0302j ,Pj) \u2264 \u03b5. Using the second part of Lemma 4.12 we conclude that w.p. at least 1\u2212 10n\u221210,\n\u03bbmax\n( 1\n\u03b1 \u2211 t\u2208J \u03b1 \u03a6 \u02c6\u0300t \u02c6\u0300 \u2032 t\u03a6\n) < \u03c9evals\nSince Det0 holds, t\u0302j = tj,\u2217. Thus, we have a total of b tj+1\u2212tj,\u2217\u2212K\u03b1\u2212\u03b1\u03b1 c intervals J \u03b1 that are subsets of [t\u0302j,fin, tj+1). Moreover, b tj+1\u2212tj,\u2217\u2212K\u03b1\u2212\u03b1 \u03b1 c \u2264 b tj+1\u2212tj\u2212K\u03b1\u2212\u03b1 \u03b1 c \u2264 b tj+1\u2212tj \u03b1 c \u2212 (K + 1) since \u03b1 \u2264 \u03b1. Thus,\nPr(NoFalseDets|\u0393j\u22121,end \u2229Det0 \u2229 SubUpd) \u2265 (1\u2212 10n\u221210)b tj+1\u2212tj \u03b1 c\u2212(K)\nOn the other hand, if we condition on \u0393j\u22121,end \u2229Det0 \u2229Det1 \u2229 SubUpd, then t\u0302j = tj,\u2217 + \u03b1. Thus,\nPr(NoFalseDets|\u0393j\u22121,end \u2229Det0 \u2229Det1 \u2229 SubUpd) \u2265 (1\u2212 10n\u221210)b tj+1\u2212tj \u03b1 c\u2212(K+1)\nWe can now combine the above facts to bound Pr(\u0393j,end|\u0393j\u22121,end). Recall that p0 := Pr(Det0|\u0393j\u22121,end). Clearly, the events (Det0\u2229SubUpd\u2229NoFalseDets) and (Det0\u2229Det1\u2229SubUpd\u2229NoFalseDets) are disjoint.\nThus,\nPr(\u0393j,end|\u0393j\u22121,end) = p0 Pr(SubUpd \u2229NoFalseDets|\u0393j\u22121,end \u2229Det0) + (1\u2212 p0) Pr(Det1|\u0393j\u22121,end \u2229Det0) Pr(SubUpd \u2229NoFalseDets|\u0393j\u22121,end \u2229Det0 \u2229Det1) \u2265 p0(1\u2212 10n\u221210)K(1\u2212 10n\u221210)b tj+1\u2212tj \u03b1 c\u2212(K) + (1\u2212 p0)(1\u2212 10n\u221210)(1\u2212 10n\u221210)K(1\u2212 10n\u221210)b tj+1\u2212tj \u03b1 c\u2212(K+1) = (1\u2212 10n\u221210)b tj+1\u2212tj \u03b1 c \u2265 (1\u2212 10n\u221210)tj+1\u2212tj .\nSince the events \u0393j,end are nested, the above implies that\nPr(\u0393J,end|\u03930,end) = \u220f j Pr(\u0393j,end|\u0393j\u22121,end) \u2265 \u220f j (1\u2212 10n\u221210)tj+1\u2212tj = (1\u2212 10n\u221210)d\n\u2265 1\u2212 10dn\u221210.\nA.2 Proof of Offline NORST\nWe now provide the proof of the Offline Algorithm (lines 26-30 of Algorithm 2).\nProof of Offline NORST. The proof of this follows from the conclusions of the online counterpart. Note that the subspace estimate in this case is not necessarily r dimensional. This is essentially done to ensure that in the time intervals when the subspace has changed, but has not yet been updated, the output of the algorithm is still an \u03b5-approximate solution to the true subspace. In other words, for t \u2208 [t\u0302j\u22121 +K\u03b1, tj ], the true subspace is Pj\u22121 and so in this interval\nSE(P\u0302 offline(t) ,Pj\u22121) = SE([P\u0302j\u22121, (I \u2212 P\u0302j\u22121P\u0302j\u22121 \u2032)P\u0302j ],Pj\u22121) (a) = \u2225\u2225\u2225[I \u2212 (I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)P\u0302jP\u0302j \u2032(I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)][I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032]Pj\u22121\u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225[I \u2212 (I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)P\u0302jP\u0302j \u2032(I \u2212 P\u0302j\u22121P\u0302j\u22121\u2032)]\u2225\u2225\u2225SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5\nwhere (a) follows because for orthogonal matrices P1 and P2,\nI \u2212 P1P1\u2032 \u2212 P2P2\u2032 = (I \u2212 P1P1\u2032)(I \u2212 P2P2\u2032) = (I \u2212 P2P2\u2032)(I \u2212 P1P1\u2032)\nNow consider the interval t \u2208 [tj , t\u0302j + K\u03b1]. In this interval, the true subspace is Pj and we have back propagated the \u03b5-approximate subspace P\u0302j in this interval. We first note that span([P\u0302j\u22121, (I \u2212 P\u0302j\u22121P\u0302j\u22121\n\u2032)P\u0302j ]) = span([P\u0302j , (I \u2212 P\u0302jP\u0302j \u2032)P\u0302j\u22121]). And so we use the latter to quantify the error in this interval as\nSE(P\u0302 offline(t) ,Pj) = SE([P\u0302j , (I \u2212 P\u0302jP\u0302j \u2032)P\u0302j\u22121],Pj) = \u2225\u2225\u2225[I \u2212 (I \u2212 P\u0302jP\u0302j \u2032)P\u0302j\u22121P\u0302j\u22121\u2032(I \u2212 P\u0302jP\u0302j \u2032)][I \u2212 P\u0302jP\u0302j \u2032]Pj\u2225\u2225\u2225\n\u2264 \u2225\u2225\u2225[I \u2212 (I \u2212 P\u0302jP\u0302j \u2032)P\u0302j\u22121P\u0302j\u22121\u2032(I \u2212 P\u0302jP\u0302j \u2032)]\u2225\u2225\u2225SE(P\u0302j ,Pj) \u2264 \u03b5\nA.3 Proof with \u03bdt 6= 0\nIn this section we analyze the \u201cstable\u201d version of RST, i.e., we let \u03bdt 6= 0.\nProof. The proof is very similar to that of the noiseless case but there are two differences due to the additional noise term. The first is the effect of the noise on the sparse recovery step. The approach to address this is straightforward. We note that the error now seen in the sparse recovery step is bounded by \u2016\u03a8(`t + \u03bdt)\u2016 and using the bound on \u2016\u03bdt\u2016, we observe that the error only changes by a constant factor. In particular, we can show that \u2016et\u2016 \u2264 2.4(2\u03b5 + \u2206) \u221a \u03b7r\u03bb+. The other crucial difference is in updating subspace estimate. To deal with the additional uncorrelated noise, we use the following result.\nRemark 4.18 of [25] states the following for the case where the data contains unstructured noise \u03bdt that satisfies the assumptions of Theorem 2.2. Thus, in the notation of [25], \u03bb+v \u2264 c\u03b52\u03bb+ and rv = r. The following result also assumes r, n large enough so that (r + log n) \u2264 r log n.\nCorollary A.2 (Noisy PCA-SDDN). Given data vectors yt := `t + wt + zt = `t + ITtMs,t`t + zt, t = 1, 2, . . . , \u03b1, where Tt is the support set of wt, and `t satisfying the model detailed above. Furthermore, maxt \u2016Ms,tP \u20162 \u2264 q < 1. zt is small uncorrelated noise such that E[ztzt\u2032] = \u03a3z, maxt \u2016zt\u20162 := b2z < \u221e. Define \u03bb+z := \u03bbmax(\u03a3z) and rz as the \u201ceffective rank\u201d of zt such that b 2 z = rz\u03bb + z . Then for any \u03b1 \u2265 \u03b10, where\n\u03b10 := C\n\u03b52SE max\n{ \u03b7q2f2r log n,\nb2z \u03bb\u2212 f log n } the fraction of nonzeroes in any row of the noise matrix [w1,w2, . . . ,w\u03b1] is bounded by b, and\n3 \u221a bqf + \u03bb+z /\u03bb\n\u2212 \u2264 0.9\u03b5SE 1 + \u03b5SE\nFor an \u03b1 \u2265 \u03b10, let P\u0302 be the matrix of top r eigenvectors of D := 1\u03b1 \u2211 t yty \u2032 t. With probability at least 1\u2212 10n\u221210, SE(P\u0302 ,P ) \u2264 \u03b5SE.\nWe illustrate how applying Corollary A.2 changes the subspace update step. Consider the first subspace estimate, i.e., we are trying to get an estimate P\u0302j,1 in the j-th subspace change time interval. Define (e`)t = ITt \u2032 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032`t and (e\u03bd)t = ITt \u2032 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032\u03bdt. We estimate the new subspace, P\u0302j,1\nas the top r eigenvectors of 1\u03b1 \u2211t\u0302j+\u03b1\u22121 t=t\u0302j \u02c6\u0300 t \u02c6\u0300 t \u2032. In the setting above, yt \u2261 \u02c6\u0300t, wt \u2261 (e`)t, zt \u2261 (e\u03bd)t, `t \u2261 `t and Ms,t = \u2212 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032 and so \u2016Ms,tP \u2016 = \u2016 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032Pj\u2016 \u2264 \u03c6+(\u03b5+ SE(Pj\u22121,Pj)) := q0. Applying Corollary A.2 with q \u2261 q0, and recalling that the support, Tt satisfies the assumptions similar to that of the noiseless case and hence b0 \u2261 max-outlier-frac-row\u03b1. Now, setting \u03b5SE,1 = q0/4, observe that we require\n(i) \u221a b0q0f \u2264\n0.5 \u00b7 0.9\u03b5SE,1 1 + \u03b5SE,1 , and, (ii) \u03bb+z \u03bb\u2212 \u2264 0.5 \u00b7 0.9\u03b5SE,1 1 + \u03b5SE,1 .\nwhich holds if (i) \u221a b0f \u2264 0.12, and (ii) is satisfied as follows from using the assumptions on \u03bdt as follows. It is immediate to see that \u03bb+z /\u03bb \u2212 \u2264 \u03b52 \u2264 .2\u03b5SE,1. Furthermore, the sample complexity term remains unchanged due to the choice of \u03bdt. To see this, notice that the only extra term in the \u03b10 expression is b2zf log n/(\u03b5 2 SE\u03bb \u2212) which simplifies to \u03b52f2r log n/\u03b52SE which is what was required even in the noiseless case. Thus, from Corollary A.2, with probability at least 1 \u2212 10n\u221210, SE(P\u0302j,1,Pj) \u2264 \u03b5SE,1 = q0/4. The argument in other subspace update stages will require the same changes and follows without any further differences.\nThe final difference is in the subspace detection step. Notice that here too, in general, there will be some extra assumption required to provably detect the subspace change. However, due to the bounds assumed on \u2016\u03bdt\u2016 and the bounds on using l,v = v,v = 0.01\u03b5, we see that (i) the extra sample complexity term is the same as that required in the noiseless case."}, {"heading": "B Proof of Lemmas 4.13 and 4.16", "text": "Proof of Lemma 4.13. The proof follows from triangle inequality as SE(Q1,Q3) = \u2225\u2225(I \u2212Q1Q1\u2032)Q3\u2225\u2225 = \u2225\u2225(I \u2212Q1Q1\u2032)(I \u2212Q2Q2\u2032 +Q2Q2\u2032)Q3\u2225\u2225\n\u2264 \u2225\u2225(I \u2212Q1Q1\u2032)(I \u2212Q2Q2\u2032)Q3\u2225\u2225+ \u2225\u2225(I \u2212Q1Q1\u2032)Q2Q2\u2032Q3\u2225\u2225\n\u2264 \u2225\u2225(I \u2212Q1Q1\u2032)\u2225\u2225SE(Q2,Q3) + SE(Q1,Q2)\u2225\u2225Q2\u2032Q3\u2225\u2225 \u2264 \u22061 + \u22062\nWe need the following results for proving Lemma 4.16.\nTheorem B.1 (Cauchy-Schwartz for sums of matrices [12]). For matrices X and Y we have\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t XtYt \u2032 \u2225\u2225\u2225\u2225\u2225 2 \u2264 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t XtXt \u2032 \u2225\u2225\u2225\u2225\u2225 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t YtYt \u2032 \u2225\u2225\u2225\u2225\u2225 (10) The following theorem is adapted from [28].\nTheorem B.2 (Matrix Bernstein [28]). Given an \u03b1-length sequence of n1\u00d7 n2 dimensional random matrices and a r.v. X. Assume the following holds. For all X \u2208 C, (i) conditioned on X, the matrices Zt are mutually independent, (ii) P(\u2016Zt\u2016 \u2264 R|X) = 1, and (iii) max {\u2225\u2225 1 \u03b1 \u2211 t E[Zt\u2032Zt|X] \u2225\u2225 , \u2225\u2225 1\u03b1\u2211t E[ZtZt\u2032|X]\u2225\u2225} \u2264 \u03c32. Then, for an > 0 and for all X \u2208 C,\nP (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t Zt \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E[Zt|X] \u2225\u2225\u2225\u2225\u2225+ \u2223\u2223\u2223\u2223X ) \u2265 1\u2212 (n1 + n2) exp ( \u2212\u03b1 2 2 (\u03c32 +R ) ) . (11)\nThe following theorem is adapted from [29].\nTheorem B.3 (Sub-Gaussian Rows [29]). Given an N -length sequence of sub-Gaussian random vectors wi in Rnw , an r.v X, and a set C. Assume the following holds. For all X \u2208 C, (i) wi are conditionally independent given X; (ii) the sub-Gaussian norm of wi is bounded by K for all i. Let W := [w1,w2, . . . ,wN ] \u2032. Then for an \u2208 (0, 1) and for all X \u2208 C\nP (\u2225\u2225\u2225\u2225 1NW \u2032W \u2212 1N E[W \u2032W |X] \u2225\u2225\u2225\u2225 \u2264 \u2223\u2223\u2223\u2223X) \u2265 1\u2212 2 exp(nw log 9\u2212 c 2N4K4 ) . (12)\nProof of Lemma 4.16. The proof approach is similar to that of [15, Lemma 7.17] but the details are different since we use a simpler subspace model.\nItem 1 : Recall that the (at)i are bounded r.v.\u2019s satisfying |(at)i| \u2264 \u221a \u03b7\u03bbi. Thus, the vectors, at are sub-Gaussian with \u2016at\u2016\u03c82 = maxi \u2016(at)i\u2016\u03c82 = \u221a \u03b7\u03bb+. We now apply Theorem B.3 with K \u2261 \u221a \u03b7\u03bb+,\n= 0\u03bb \u2212, N \u2261 \u03b1 and nw \u2261 r to conclude the following: For an \u03b1 \u2265 \u03b1(0) := C(r log 9 + 10 log n)f2,\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t atat \u2032 \u2212\u039b \u2225\u2225\u2225\u2225\u2225 \u2264 0\u03bb\u2212 ) \u2265 1\u2212 10n\u221210\nThe Lemma statement assumes \u03b1 = Cf2r log n. For large r, n, this \u03b1 > \u03b1(0) = Cf 2(r+ log n). Thus, the above holds under the Lemma statement.\nItem 2 : For the second term, we proceed as follows. Since \u2016\u03a6\u2016 = 1,\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6`tet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6`tet \u2032 \u2225\u2225\u2225\u2225\u2225 .\nTo bound the RHS above, we will apply Theorem B.2 with Zt = \u03a6`tet \u2032. Conditioned on {P\u0302\u2217, Z}, the Zt\u2019s are mutually independent. We first bound obtain a bound on the expected value of the time average of the Zt\u2019s and then compute R and \u03c3 2. By Cauchy-Schwartz,\u2225\u2225\u2225\u2225\u2225E [ 1 \u03b1 \u2211 t \u03a6`tet \u2032 ]\u2225\u2225\u2225\u2225\u2225 2 = \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6P\u039bP \u2032M1,t \u2032M2,t \u2032 \u2225\u2225\u2225\u2225\u2225 2\n(a) \u2264 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t ( \u03a6P\u039bP \u2032M1,t \u2032) (M1,tP\u039bP \u2032\u03a6) \u2225\u2225\u2225\u2225\u2225 \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t M2,tM2,t \u2032 \u2225\u2225\u2225\u2225\u2225 (b)\n\u2264 b0 [ max t \u2225\u2225\u03a6P\u039bP \u2032M1,t\u2032\u2225\u22252] \u2264 b0SE2(P\u0302 ,P )q2(\u03bb+)2 (13)\nwhere (a) follows by Cauchy-Schwartz (Theorem B.1) withXt = \u03a6P\u039bP \u2032M1,t \u2032 and Yt = M2,t, (b) follows from the assumption on M2,t. To compute R\n\u2016Zt\u2016 \u2264 \u2016\u03a6`t\u2016 \u2016et\u2016 \u2264 SE(P\u0302 ,P )q\u03b7r\u03bb+ := R\nNext we compute \u03c32. Since wt\u2019s are bounded r.v.\u2019s, we have\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E[ZtZt\u2032] \u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E [ \u03a6`tet \u2032et`t \u2032\u03a6 ]\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 1\u03b1E[\u2016et\u20162 \u03a6`t`t\u2032\u03a6] \u2225\u2225\u2225\u2225\n\u2264 (\nmax et \u2016et\u20162 )\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E [ \u03a6`t`t \u2032\u03a6 ]\u2225\u2225\u2225\u2225\u2225\n\u2264 q2SE2(P\u0302 ,P )\u03b7r(\u03bb+)2\u00b7 := \u03c32\nit can also be seen that \u2225\u2225 1 \u03b1 \u2211 t E[Zt\u2032Zt] \u2225\u2225 evaluates to the same expression. Thus, applying Theorem B.2 Pr\n(\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6`tet \u2032 \u2225\u2225\u2225\u2225\u2225 \u2264 SE(P\u0302 ,P )\u221ab0q\u03bb+ + )\n\u2265 1\u2212 2n exp  \u2212\u03b1 4 max { \u03c32 2 , R }  .\nLet = 1\u03bb \u2212, then, \u03c32/ 2 = c\u03b7f2r and R/ = c\u03b7fr. Hence, for the probability to be of the form 1\u22122n\u221210 we require that \u03b1 \u2265 \u03b1(1) := C \u00b7 \u03b7f2(r log n). Item 3 : We use Theorem B.2 with Zt := \u03a6etet \u2032\u03a6. The proof is analogous to the previous item. First we bound the norm of the expectation of the time average of Zt:\u2225\u2225\u2225\u2225E [ 1\u03b1\u2211\u03a6etet\u2032\u03a6 ]\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225 1\u03b1\u2211\u03a6M2,tM1,tP\u039bP \u2032M1,t\u2032M2,t\u2032\u03a6\n\u2225\u2225\u2225\u2225 \u2264 \u2225\u2225\u2225\u2225 1\u03b1\u2211M2,tM1,tP\u039bP \u2032M1,t\u2032M2,t\u2032\n\u2225\u2225\u2225\u2225 (a) \u2264\n(\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t M2,tM2,t \u2032 \u2225\u2225\u2225\u2225\u2225 [maxt \u2225\u2225M2,tM1,tP\u039bP \u2032M1,t(\u00b7)\u2032\u2225\u22252] )1/2\n(b) \u2264 \u221a b0 [ max t \u2225\u2225M1,tP\u039bP \u2032M1,t\u2032M2,t\u2032\u2225\u2225] \u2264\u221ab0q2\u03bb+\nwhere (a) follows from Theorem B.1 with Xt = M2,t and Yt = M1,tP\u039bP \u2032M1,t \u2032M2,t \u2032 and (b) follows from the assumption on M2,t. To obtain R,\n\u2016Zt\u2016 = \u2225\u2225\u03a6etet\u2032\u03a6\u2225\u2225 \u2264 max\nt \u2016\u03a6MtPat\u20162 \u2264 q2r\u03b7\u03bb+ := R\nTo obtain \u03c32, \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E [ \u03a6et(\u03a6et) \u2032(\u03a6et)et \u2032\u03a6 ]\u2225\u2225\u2225\u2225\u2225 = \u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t E [ \u03a6etet \u2032\u03a6 \u2016\u03a6et\u20162 ]\u2225\u2225\u2225\u2225\u2225\n\u2264 (\nmax et \u2016\u03a6et\u20162 )\u2225\u2225\u03a6MtP\u039bP \u2032Mt\u2032\u03a6\u2225\u2225 \u2264 q2r\u03b7\u03bb+ \u00b7 q2\u03bb+ := \u03c32\nApplying Theorem B.2, we have\nPr (\u2225\u2225\u2225\u2225\u2225 1\u03b1\u2211 t \u03a6etet \u2032\u03a6 \u2225\u2225\u2225\u2225\u2225 \u2264\u221ab0q2\u03bb+ + ) \u2265 1\u2212 n exp ( \u2212\u03b1 2 2(\u03c32 +R ) ) Letting = 2\u03bb \u2212 we get R/ = c\u03b7rf and \u03c32/ 2 = c\u03b7rf2. For the success probability to be of the form 1\u2212 2n\u221210 we require \u03b1 \u2265 \u03b1(2) := C\u03b7 \u00b7 11f2(r log n).\nThe proof of the last two items follow from using [25, Lemma 7.19]."}, {"heading": "C Proof of Extensions", "text": "In this section we present the proof of the extensions stated in Sec. 5.\nC.1 Static Robust PCA\nThe proof follows directly from Theorem 2.2 by setting J = 1.\nC.2 Subspace Tracking with missing data and dynamic Matrix Completion\nHere we present the proof of the subspace tracking with missing data problem. The only changes needed for this proof are in the initialization step, i.e., for j = 0. For this we use the following lemma.\nLemma C.1 (Lemma 2.1, [33]). Set r\u0304 = max(r, log n). Then there exist constants C and c such that\nthe random orthogonal model with left singular vectors P\u0302init obeys Pr ( maxi \u2225\u2225\u2225Ii\u2032P\u0302init\u2225\u2225\u22252 \u2264 Cr\u0304/n) \u2265 1 \u2212 cn\u2212\u03b2 log n.\nThus,\nPr ( max i \u2225\u2225\u2225Ii\u2032P\u0302init\u2225\u2225\u22252 \u2264 \u00b5r\u0304/n) \u2265 1\u2212 n\u221210 Consider the two scenarios (i) if r \u2265 log n, then everything discussed before remains true, whereas, if (ii) r \u2264 log n, we redefine \u00b52 = C log n/r and thus in the interval [t0, t1] we require max-outlier-frac-col \u2264 0.01logn . Further, using the bound on max-outlier-frac-col it follows from triangle inequality that\nmax T \u22642s \u2225\u2225\u2225IT \u2032P\u0302init\u2225\u2225\u22252 \u2264 2smax i \u2225\u2225\u2225Ii\u2032P\u0302init\u2225\u2225\u22252 \u2264 2s\u00b5r n < 0.01\nWe only mention the changes needed for Lemma 4.7 for when j = 0 since the initialization is different. The rest of the argument of recursively applying the lemmas hold exactly as before. First, ttrain = 1 since we use random initialization. Thus from the Algorithm, t\u03020 = ttrain = 1.\nProof. Proof of item 1. Since the support of xt is known, the LS step gives x\u0302t = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 \u03a8Tt \u2032(\u03a8`t + \u03a8xt) = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 ITt \u2032\u03a8`t + xt\nThus et = x\u0302t \u2212 xt satisfies\net = ITt ( \u03a8Tt \u2032\u03a8Tt )\u22121 ITt \u2032\u03a8`t\nNow, from the incoherence assumption on P\u0302init, Lemma 4.11, the bound on max-outlier-frac-col, and recalling that in this interval, \u03a8 = I \u2212 P\u0302initP\u0302init\u2032 we have\nmax |T |\u22642s \u2225\u2225\u2225IT \u2032P\u0302init\u2225\u2225\u22252 \u2264 2s\u00b5r n \u2264 0.09 =\u21d2 \u03b42s(\u03a8) \u2264 0.32 < 0.15,\u2225\u2225\u2225(\u03a8Tt \u2032\u03a8Tt)\u22121\u2225\u2225\u2225 \u2264 11\u2212 \u03b4s(\u03a8) \u2264 11\u2212 \u03b42s(\u03a8) \u2264 11\u2212 0.15 < 1.2 = \u03c6+.\nSecondly, \u2225\u2225ITt \u2032\u03a8P0\u2225\u2225 \u2264 (\u2225\u2225ITt \u2032P0\u2225\u2225+ \u2225\u2225\u2225ITt \u2032P\u0302init\u2225\u2225\u2225) \u2264 0.3 + 0.3 = 0.6 (14) Thus, \u2016ITt \u2032\u03a8`t\u2016 \u2264 0.6 \u221a \u03b7r\u03bb+.\nProof of Item 2 : Since \u02c6\u0300t = `t \u2212 et with et satisfying the above equation, updating P\u0302(t) from the \u02c6\u0300t\u2019s is a problem of PCA in sparse data-dependent noise (SDDN), et. To analyze this, we use the PCA-SDDN result of Theorem 4.14 (this is taken from [25]). Recall from above that, for t \u2208 [t\u03020, t\u03020 + \u03b1], T\u0302t = Tt, and \u02c6\u0300 t = `t \u2212 et. Recall from the algorithm that we compute the first estimate of the j-th subspace, P\u0302j,1,\nas the top r eigenvectors of 1\u03b1 \u2211t0+\u03b1\u22121 t=t0 \u02c6\u0300 t \u02c6\u0300 t \u2032. In the notation of Theorem 4.14, yt \u2261 \u02c6\u0300t, wt \u2261 et, `t \u2261 `t and Ms,t = \u2212 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt \u2032 and so \u2016Ms,tP \u2016 = \u2016 (\u03a8Tt \u2032\u03a8Tt) \u22121 \u03a8Tt\n\u2032P0\u2016 \u2264 \u03c6+ \u00b7 0.6 = 0.72 := q0. This follows using (14). Also, b0 \u2261 max-outlier-frac-row\u03b1.\nApplying Theorem 4.14 with q \u2261 q0, b0 \u2261 max-outlier-frac-row\u03b1 and setting \u03b5SE = q0/4, observe that we require \u221a\nb0q0f \u2264 0.9(q0/4)\n1 + (q0/4) .\nThis holds if \u221a b0f \u2264 0.12 as provided by Theorem 2.2. Thus, from Corollary 4.14, with probability at least 1\u2212 10n\u221210, SE(P\u0302j,1,P0) \u2264 q0/4.\nC.3 Fewer than r directions change\nProof of Corollary 5.20. The proof is very similar to that of Theorem 2.2. The only changes occur in the (a) Projected CS step. With the subspace change model, we define `t = P(t)at := [ Pj\u22121,fix Pj,chd ] [at,fix at,chd ] where at,fix is a (r \u2212 rch)\u00d7 1 dimensional vector and at,chd is a rch \u00d7 1 dimensional vector. In the first \u03b1 frames after the j-th subspace changes (or, the j-th subspace change is detected in the automatic case), recall that P\u0302(t) = P\u0302j\u22121. Thus, SE(P\u0302(t),Pj\u22121,fix) = SE(P\u0302j\u22121,Pj\u22121,fix) \u2264 SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5 and so, the error can be bounded as\n\u2016\u03a8`t\u2016 \u2264 \u2016\u03a8Pj\u22121,fixat,fix\u2016+ \u2016\u03a8Pj,chdat,chd\u2016 \u2264 \u03b5 \u221a \u03b7(r \u2212 rch)\u03bb+ + (\u03b5+ SE(Pj\u22121,Pj)) \u221a \u03b7rch\u03bb + ch\nIn the second \u03b1 frames, have P\u0302(t) = basis(P\u0302j\u22121, P\u0302j,1). Thus SE(P\u0302(t),Pj\u22121,fix) \u2264 SE(P\u0302j\u22121,Pj\u22121,fix) \u2264 SE(P\u0302j\u22121,Pj\u22121) \u2264 \u03b5 and SE(P\u0302(t),Pj,chd) \u2264 SE(P\u0302j,1,Pj,chd) \u2264 SE(P\u0302j,1,Pj) \u2264 0.3 \u00b7 (\u03b5 + SE(Pj\u22121,Pj)). Thus, the error in the sparse recovery step in the interval after the first subspace update is performed is given as\n\u2016\u03a8`t\u2016 \u2264 \u03b5 \u221a \u03b7(r \u2212 rch)\u03bb+ + 0.3 \u00b7 (\u03b5+ SE(Pj\u22121,Pj)) \u221a \u03b7rch\u03bb + ch\nThe rest of the proof follows as before. The error after the k-th subspace update is also bounded using the above idea.\n(b) Subspace Detection step: The proof of the subspace detection step follows exactly analogous to Lemma 4.12. One minor observation is noting that SE(Pj\u22121,PJ) = SE(Pj\u22121,ch,Pj,chd) in the proof of part (a) of Lemma 4.12. The rest of the argument is exactly the same."}], "year": 2018, "references": [{"title": "Nearly optimal robust subspace tracking", "authors": ["P. Narayanamurthy", "N. Vaswani"], "venue": "ICML, pp. 3698\u2013 3706, 2018.", "year": 2018}, {"title": "A Fast and Memory-Efficient Algorithm for Robust PCA (MERoP)", "authors": ["P. Narayanamurthy", "N. Vaswani"], "venue": "IEEE Intl. Conf. Acoustics, Speech, Sig. Proc. (ICASSP), 2018.", "year": 2018}, {"title": "Robust principal component analysis", "authors": ["E.J. Cand\u00e8s", "X. Li", "Y. Ma", "J. Wright"], "venue": "J. ACM, vol. 58, no. 3, 2011.", "year": 2011}, {"title": "A multibody factorization method for independently moving objects", "authors": ["J.P. Costeira", "T. Kanade"], "venue": "International Journal of Computer Vision, vol. 29, no. 3, pp. 159\u2013179, 1998.", "year": 1998}, {"title": "Signal detection using multi-channel seismic data", "authors": ["G.S. Wagner", "T.J. Owens"], "venue": "Bulletin of the Seismological Society of America, vol. 86, no. 1A, pp. 221\u2013231, 1996.", "year": 1996}, {"title": "Recursive tensor subspace tracking for dynamic brain network analysis", "authors": ["A. Ozdemir", "E.M. Bernat", "S. Aviyente"], "venue": "IEEE Transactions on Signal and Information Processing over Networks, 2017.", "year": 2017}, {"title": "Rank-sparsity incoherence for matrix decomposition", "authors": ["V. Chandrasekaran", "S. Sanghavi", "P.A. Parrilo", "A.S. Willsky"], "venue": "SIAM Journal on Optimization, vol. 21, 2011.", "year": 2011}, {"title": "Robust matrix decomposition with sparse corruptions", "authors": ["D. Hsu", "S.M. Kakade", "T. Zhang"], "venue": "IEEE Trans. Info. Th., Nov. 2011.", "year": 2011}, {"title": "Non-convex robust pca", "authors": ["P. Netrapalli", "U N Niranjan", "S. Sanghavi", "A. Anandkumar", "P. Jain"], "venue": "NIPS, 2014.", "year": 2014}, {"title": "Fast algorithms for robust pca via gradient descent", "authors": ["X. Yi", "D. Park", "Y. Chen", "C. Caramanis"], "venue": "NIPS, 2016.", "year": 2016}, {"title": "Nearly-optimal robust matrix completion", "authors": ["Y. Cherapanamjeri", "K. Gupta", "P. Jain"], "venue": "ICML, 2016.", "year": 2016}, {"title": "Recursive robust pca or recursive sparse recovery in large but structured noise", "authors": ["C. Qiu", "N. Vaswani", "B. Lois", "L. Hogben"], "venue": "IEEE Trans. Info. Th., pp. 5007\u20135039, August 2014.", "year": 2014}, {"title": "Online matrix completion and online robust pca", "authors": ["B. Lois", "N. Vaswani"], "venue": "IEEE Intl. Symp. Info. Th. (ISIT), 2015.", "year": 2015}, {"title": "Online (and Offline) Robust PCA: Novel Algorithms and Performance Guarantees", "authors": ["J. Zhan", "B. Lois", "H. Guo", "N. Vaswani"], "venue": "Intnl. Conf. Artif. Intell. Stat. (AISTATS), 2016.", "year": 2016}, {"title": "Provable dynamic robust pca or robust subspace tracking", "authors": ["P. Narayanamurthy", "N. Vaswani"], "venue": "arXiv:1705.08948 (submitted to IEEE Trans. Info. Theory), 2017.", "year": 2017}, {"title": "Projection approximation subspace tracking", "authors": ["B. Yang"], "venue": "IEEE Trans. Sig. Proc., pp. 95\u2013107, 1995. 28", "year": 1995}, {"title": "Petrels: Parallel subspace estimation and tracking by recursive least squares from partial observations", "authors": ["Y. Chi", "Y.C. Eldar", "R. Calderbank"], "venue": "IEEE Trans. Sig. Proc., December 2013.", "year": 2013}, {"title": "Local convergence of an algorithm for subspace identification from partial data", "authors": ["L. Balzano", "S. Wright"], "venue": "Found. Comput. Math., vol. 15, no. 5, 2015.", "year": 2015}, {"title": "Schubert varieties and distances between subspaces of different dimensions", "authors": ["K. Ye", "L.H. Lim"], "venue": "SIAM Journal on Matrix Analysis and Applications, vol. 37, no. 3, pp. 1176\u20131197, 2016.", "year": 2016}, {"title": "Online robust pca via stochastic optimization", "authors": ["J. Feng", "H. Xu", "S. Yan"], "venue": "NIPS, 2013.", "year": 2013}, {"title": "The restricted isometry property and its implications for compressed sensing", "authors": ["E. Candes"], "venue": "C. R. Math. Acad. Sci. Paris Serie I, 2008.", "year": 2008}, {"title": "Robust pca with partial subspace knowledge", "authors": ["J. Zhan", "N. Vaswani"], "venue": "IEEE Trans. Sig. Proc., July 2015.", "year": 2015}, {"title": "A proximal-gradient homotopy method for the l1-regularized leastsquares problem", "authors": ["Lin Xiao", "Tong Zhang"], "venue": "ICML, 2012.", "year": 2012}, {"title": "Finite sample guarantees for pca in non-isotropic and datadependent noise", "authors": ["N. Vaswani", "P. Narayanamurthy"], "venue": "arXiv:1709.06255, 2017.", "year": 2017}, {"title": "Randomized block krylov methods for stronger and faster approximate singular value decomposition", "authors": ["Cameron Musco", "Christopher Musco"], "venue": "Advances in Neural Information Processing Systems, 2015, pp. 1396\u20131404.", "year": 2015}, {"title": "Correlated-pca: Principal components\u2019 analysis when data and noise are correlated", "authors": ["N. Vaswani", "H. Guo"], "venue": "Adv. Neural Info. Proc. Sys. (NIPS), 2016.", "year": 2016}, {"title": "User-friendly tail bounds for sums of random matrices", "authors": ["J.A. Tropp"], "venue": "Found. Comput. Math., vol. 12, no. 4, 2012.", "year": 2012}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["R. Vershynin"], "venue": "Compressed sensing, pp. 210\u2013268, 2012.", "year": 2012}, {"title": "Dimensionality reduction for fast similarity search in large time series databases", "authors": ["E. Keogh", "K. Chakrabarti", "M. Pazzani", "S. Mehrotra"], "venue": "Knowledge and information Systems, vol. 3, no. 3, pp. 263\u2013286, 2001.", "year": 2001}, {"title": "Hyperspectral image classification and dimensionality reduction: an orthogonal subspace projection approach", "authors": ["J.C. Harsanyi", "C.I. Chang"], "venue": "IEEE Transactions on geoscience and remote sensing, vol. 32, no. 4, pp. 779\u2013785, 1994.", "year": 1994}, {"title": "Exact matrix completion via convex optimization", "authors": ["E.J. Candes", "B. Recht"], "venue": "Found. of Comput. Math, , no. 9, pp. 717\u2013772, 2008.", "year": 2008}, {"title": "Incremental gradient on the grassmannian for online foreground and background separation in subsampled video", "authors": ["J. He", "L. Balzano", "A. Szlam"], "venue": "IEEE Conf. on Comp. Vis. Pat. Rec. (CVPR), 2012. 29", "year": 2012}], "id": "SP:cff617462752879038a64bf0f9e56a3d4ab58320", "authors": [{"name": "Praneeth Narayanamurthy", "affiliations": []}, {"name": "Namrata Vaswani", "affiliations": []}], "abstractText": "In this work, we study the robust subspace tracking (RST) problem and obtain one of the first two provable guarantees for it. The goal of RST is to track sequentially arriving data vectors that lie in a slowly changing low-dimensional subspace, while being robust to corruption by additive sparse outliers. It can also be interpreted as a dynamic (time-varying) extension of robust PCA (RPCA), with the minor difference that RST also requires a short tracking delay. We develop a recursive projected compressive sensing algorithm that we call Nearly Optimal RST via ReProCS (ReProCS-NORST) because its tracking delay is nearly optimal. We prove that NORST solves both the RST and the dynamic RPCA problems under weakened standard RPCA assumptions, two simple extra assumptions (slow subspace change and most outlier magnitudes lower bounded), and a few minor assumptions. Our guarantee shows that NORST enjoys a near optimal tracking delay of O(r log n log(1/ )). Its required delay between subspace change times is the same, and its memory complexity is n times this value. Thus both these are also nearly optimal. Here n is the ambient space dimension, r is the subspaces\u2019 dimension, and is the tracking accuracy. NORST also has the best outlier tolerance compared with all previous RPCA or RST methods, both theoretically and empirically (including for real videos), without requiring any model on how the outlier support is generated. This is possible because of the extra assumptions it uses.", "title": "Nearly Optimal Robust Subspace Tracking"}