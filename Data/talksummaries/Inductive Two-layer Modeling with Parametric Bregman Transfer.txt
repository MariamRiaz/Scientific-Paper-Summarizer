8	74	Although empirical performance has been the major focus of deep learning, recently substantial progress has been made towards the analysis of global training and the structure of the optimization problem.
12	31	However most analyses are still limited, especially with assumptions on the model and data distribution that are hard to verify in practice.
14	27	Examples include boosting (Bengio et al., 2005), spectral methods (Anandkumar et al., 2014; Zhong et al., 2017), kernel methods (Zhang et al., 2016; 2017), polynomial networks and sum-product networks (Livni et al., 2014; Gens & Domingos, 2012), and semidefinite relaxations (Fogel et al., 2015).
16	23	A framework based on reformulation that accommodates more general latent variable structures was proposed by Aslan et al. (2013; 2014), where each pair of adjacent layers are conjoined through a prediction loss that favors nonlinear connections.
17	36	A similar approach was designed by Carreira-Perpinnan & Wang (2014), which introduced “auxiliary coordinates” to allow deviation from layer-wise outputs with a penalty.
21	20	As a result the model is restricted to a transductive setting, in that training examples are required to establish the data-dependent context of nonparametric kernel learning.
22	37	This restriction significantly slows down predictions at test time, which is more important than the training cost.
24	53	The goal of this paper, therefore, is to develop an inductive and efficient learning strategy for two-layer conditional models with global optimality guarantees.
34	31	Here x ∈ Rn is the raw input feature, and W ∈ Rh×n is the hidden layer weights.
46	22	These transfers are illustrated in Figure 1.
49	32	To cope with it, a natural approach is to replace the exact connection of φ = f(z) by a loss function that penalizes the deviation between φ and f(z).
54	22	Unfortunately, it can be shown that such a loss does not exist, unless f is affine (see the proof in Appendix A): Theorem 1.
57	27	Interestingly, the matching loss (Auer et al., 1996) meets the first and third conditions, and satisfies a weakened version of convexity by imposing a very mild condition on f .
58	24	In particular, we assume that the transfer function is the gradient of a strictly convex function F : f = ∇F , with F : Rh → R. If f is elementwise, this just means the constituent f is continuous and strictly increasing.
66	21	However, the only nonconvex part is the bilinear term φ′z, while both F ∗ and F are convex.
67	46	Such a decoupling of nonconvex terms from the transfer functions is the key enabler for our convex reformulation.
68	26	Suppose we have t training pairs {(xj ,yj)}tj=1, stacked in two matrices X = (x1, .
69	42	The corresponding set of latent layer outputs are stacked into Φ = (φ1, .
84	25	Such a strong duality is indeed not trivial because the celebrated Sion’s minimax lemma requires that the domain of (W,U) be compact, which is not assumed here.
94	27	As a result, we may perform change of variable via Λ = ΦA, where A ∈ Rt×t+ and is not necessarily symmetric.
96	65	Although this is still not convex, all occurrences of Φ are now in the form of Φ′Φ, leading to the natural idea of optimizing over Φ′Φ directly.
105	65	More interestingly, it is not hard to show that T∞ is the tightest convex relaxation of Th, i.e. the convex hull of Th for any h. Letting T := T∞ yields our final objective min T∈T max R1=0,A≥0 1 2 tr(T )− 1 2 tr(T (I −A)X ′X(I −A′)) − 12 tr(TR ′R)− 12 tr(TAA ′)− `∗(R).
107	29	We will see in the sequel that the formulation does implicitly favor a low-rank solution through a gauge regularizer (Lemma 1), although a manual assignment of h can always be incorporated through truncation after optimization.
111	39	Then we get the same objective function as in (6), only with Th changed into {Φ′Φ : ‖Φ‖∞ ≤ 1} and the domain of A changed into {A : ∑ i |Aij | ≤ 1, ∀ j}.
115	52	Optimization over completely positive matrices is known hard (Berman & ShakedMonderer, 2003), and even projection to T is NP-hard (Dickinson & Gijben, 2014).1 Therefore we resort to conditional gradient (Frank-Wolfe) methods that are free of projection (CG, Jaggi, 2013; Harchaoui et al., 2015).
122	32	Furthermore, GCG is robust to inexactness in polar operators, and one of our key contributions below is to show that it can efficiently solve (6) with a multiplicative approximation bound of 14 .
129	23	S is convex, bounded, and closed.
132	56	In fact, it is easy to show that for any convex cone C, the gauge function of its intersection with a half-space tr(A′T ) ≤ 1 is exactly tr(A′T ) overC.
133	49	The significance of Lemma 1 is that it provides the cornerstone for solving the problem (6) by GCG.
134	47	Indeed, (6) can be equivalently rewritten as min T J(T ) := 12γS(T ) + g(T ) where (11) g(T ) := max R1=0,A≥0 − 12 tr(T (I −A)X ′X(I −A′)) (12) − 12 tr(TR ′R)− 12 tr(TAA ′)− `∗(R).
