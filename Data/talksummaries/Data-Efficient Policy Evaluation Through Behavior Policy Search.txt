0	18	Many sequential decision problems, including diabetes treatment (Bastani, 2014), digital marketing (Theocharous et al., 2015), and robot control (Lillicrap et al., 2015), are modeled as Markov decision processes (MDPs) and solved using reinforcement learning (RL) algorithms.
1	36	One important problem when applying RL to real problems is policy evaluation.
2	26	The goal in policy evaluation is to estimate the expected return (sum of rewards) produced by a policy.
3	33	We refer to this policy as the evaluation policy, πe.
20	32	Empirically we demonstrate behavior policy search with our method lowers the mean squared error of estimates compared to on-policy estimates.
26	51	Let ρ(π) := E[g(H)|H ∼ π] be the expected discounted return when the stochastic policy π is used from S0 sampled from the initial state distribution.
27	30	In this work, we consider parameterized policies, πθ, where the distribution over actions is determined by the vector θ.
31	27	We consider an incremental setting where, at iteration i, we sample a single trajectory Hi with a policy πθi and add {Hi,θi} to a set D. We use Di to denote the set at iteration i.
33	35	A policy evaluation method, PE, uses all trajectories in Di to estimate ρ(πe).
35	18	Formally, the goal of policy evaluation with PE is to minimize (PE(Di)− ρ(πe))2.
42	33	The estimate of ρ(πe) at iteration i is the average return: MC(Di) := 1 i+ 1 i∑ j=0 L∑ t=0 γtRt = 1 i+ 1 i∑ j=0 g(Hj).
52	33	Notice that, like the MC estimator, the ASE estimator is on-policy, in that the behavior policy is always the policy that we wish to evaluate.
55	54	Importance Sampling is a method for reweighting returns from a behavior policy, θ, such that they are unbiased returns from the evaluation policy.
58	16	In RL, importance sampling allows off-policy data to be used as if it were on-policy.
81	16	Unfortunately, the optimal behavior policy depends on the unknown value ρ(πe) as well as the unknown reward function R (via g(H)).
84	29	Since the optimal behavior policy cannot be analytically determined, we instead propose the behavior policy search (BPS) problem for finding πb that lowers the MSE of estimates of ρ(πe).
91	16	At the ith iteration, a BPS algorithm selects a behavior policy that will be used to generate a trajectory, Hi.
113	21	At iteration i, BPG samples a batch, Bi, of k trajectories and adds {(θi, Hi)ki=0} to a data set D (Lines 4-5).
124	23	We denote this new method DR-BPG for Doubly Robust Behavior Policy Gradient.
127	39	We can reduce the mean squared error of DR with gradient descent using unbiased estimates of the following corollary to Theorem 1: Corollary 1.
139	16	BPG can be interpreted as REINFORCE where the return of a trajectory is the square of its importance-sampled return.
162	48	In comparison to a Monte Carlo estimate with 100 trajectories from π1, MSE is 85.48 % lower with this improved behavior policy.
203	26	The intuition for why behavior policy search can lower the variance of on-policy estimates is that a well selected behavior policy can cause rare and high magnitude events to occur.
234	137	Our experiments demonstrate that behavior policy search with BPG can lower the variance of policy evaluation.
235	57	One open question is characterizing the settings where adapting the behavior policy substantially improves over on-policy estimates.
236	70	Towards answering this question, our Gridworld experiment showed that when πe has little variance, BPG can only offer marginal improvement.
237	25	BPG increases the probability of observing rare events with a high magnitude.
243	24	No behavior policy adaptation can lower the variance due to this event.
247	32	Future work could consider methods for adaptive step-sizes, second order methods, or natural behavior policy gradients.
248	40	One interesting direction for future work is incorporating behavior policy search into policy improvement.
