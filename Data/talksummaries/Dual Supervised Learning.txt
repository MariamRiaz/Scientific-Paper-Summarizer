1	59	Interestingly, we find that many of the aforementioned AI tasks are emerged in dual forms, i.e., the input and output of one task are exactly the output and input of the other task respectively.
2	44	Examples include translation from language A to language B vs. translation from language B to A, image classification vs. image generation, and speech recognition vs. speech synthesis.
6	15	In this work, we give a positive answer to the question.
8	17	The primal task takes a sample from space X as input and maps to space Y , and the dual task takes a sample from space Y as input and maps to space X .
11	19	We name this new scheme as dual supervised learning (briefly, DSL).
13	27	In this paper, we use it as a regularization term to govern the training process.
17	19	By doing so, the intrinsic probabilistic connection between θyx and θxy are explicitly strengthened, which is supposed to push the learning process towards the right direction.
41	64	Given any (x, y), let `1(f(x), y) and `2(g(y), x) denote the loss functions for f and g respectively, both of which are mappings from X × Y to R. A common practice to design (f, g) is the maximum likelihood estimation based on the parameterized conditional distributions P (·|x; θxy) and P (·|y; θyx): f(x; θxy) , arg max y′∈Y P (y′|x; θxy), g(y; θyx) , arg max x′∈X P (x′|y; θyx), where θxy and θyx are the parameters to be learned.
42	33	By standard supervised learning, the primal model f is learned by minimizing the empirical risk in space Y: minθxy (1/n) ∑n i=1`1(f(xi; θxy), yi); and dual model g is learned by minimizing the empirical risk in space X : minθyx(1/n) ∑n i=1`2(g(yi; θyx), xi).
43	32	Given the duality of the primal and dual tasks, if the learned primal and dual models are perfect, we should have P (x)P (y|x; θxy) = P (y)P (x|y; θyx) = P (x, y),∀x, y.
44	35	We call this property probabilistic duality, which serves as a necessary condition for the optimality of the learned two dual models.
45	23	By the standard supervised learning scheme, probabilistic duality is not considered during the training, and the primal and the dual models are trained independently and separately.
52	16	As an alternative, we use the empirical marginal distributions P̂ (x) and P̂ (y) to fulfill the constraint in Eqn.(2).
93	20	Note that, following the common practice, the Zh→En is evaluated by case-insensitive BLEU score.
122	17	(1) does not hold, which causes the declination of the performance of both models as they play as the regularizer for each other.
136	21	Marginal Distributions In our experiments, we simply use the uniform distribution to set the marginal distribution P̂ (y) of 10-class labels, which means the marginal distribution of each class equals 0.1.
138	20	Note that the model can predict xi only based on the previous pixels xj with index j < i.
153	16	The whole training process takes about two weeks before convergence.
155	29	Table 4 compares the error rates of two image classification models, i.e., DSL vs. Baseline, on the test set.
156	32	From this table, we find that, with using either ResNet-32 or ResNet-110, DSL achieves better accuracy than the baseline method.
168	71	But, there are still some cases that neither the baseline model nor DSL can perform well, like deers it Row 5 and frogs in Row 7.
172	79	In this domain, the primal task, sentiment classification (Maas et al., 2011; Dai & Le, 2015), is to predict the sentiment polarity label of a given sentence; and the dual task, though not quite apparent but really existed, is sentence generation based on a sentiment polarity.
176	19	We randomly sample a subset of 3750 sentences from the training data as the validation set for hyperparameter tuning and use the remaining training data for model training.
177	45	Marginal Distributions We simply use the uniform distribution to set the marginal distribution P̂ (y) of polarity labels, which means the marginal distribution of positive or negative class equals 0.5.
180	55	Model Implementation We leverage the widely used LSTM (Dai & Le, 2015) modeling approach for sentiment classification8 model.
184	42	Note theW ’s and theE’s are the parameters to learn in training.
191	24	The whole training process of DSL takes less than two days.
195	15	First, DSL can reduce the classification error by 0.90 without modifying the LSTMbased model structure.
197	37	We hypothesize the reason is that the sentiment label can merely supply at most 1 bit information such that the perplexity difference between the language model (i.e., the marginal distribution P̂ (x)) and CLM (i.e., the conditional distribution P (x|y)) are not large, which limits the improvement brought by DSL.
203	15	Meanwhile, since the training of CLM in DSL can leverage the signals provided by the classifier, DSL makes it more possible to select those words, phrases, or textual patterns that can present more specific and more intense sentiment, such as “nothing but good, 10/10, don’t waste your time”, etc.
