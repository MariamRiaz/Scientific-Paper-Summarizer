{"sections": [{"heading": "1. Introduction", "text": "Clustering is a fundamental problem in the analysis and understanding of data, and is used widely in different areas of science. The broad goal of clustering is to divide a (typically large) dataset into groups that such that data points within a group are \u201csimilar\u201d to one another. In most applications, there is a measure of similarity between any two objects, which typically forms a metric. The problem can be formalized in many different ways, depending on the properties desired of the obtained clustering. While a \u201cperfect\u201d formulation may not exist (see (Kleinberg, 2002)),\n*Equal contribution 1School of Computing, University of Utah. Correspondence to: Aditya Bhaskara <bhaskaraaditya@gmail.com>, Maheshakya Wijewardena <pmaheshakya4@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nmany formulations have been very successful in applications, including k-means, k-median, k-center, and various notions of hierarchical clustering (see (Hastie et al., 2009; Dasgupta, 2016) and references there-in).\nIn this paper, we focus on k-means clustering, in which the input is a set of n points in Euclidean space. Here the goal is to partition the points into k clusters, so as to minimize the sum of squared distances from the points to the respective cluster centers (see Section 2 for a formal definition). k-means is one of the most well-studied clustering variants. Lloyd\u2019s algorithm (Lloyd, 1982), developed over 35 years ago, has been extremely successful in practice (the success has been \u2018explained\u2019 in many recent works; see (Arthur et al., 2011; Awasthi & Sheffet, 2012; Kumar & Kannan, 2010) and references there-in). Despite the success, Lloyd\u2019s algorithm can have an arbitrarily bad approximation ratio in the worst case. To address this, constant factor approximation algorithms have been developed, which are more involved but have worst case guarantees (see (Kanungo et al., 2004) and (Ahmadian et al., 2017)). In another direction, works by (Ostrovsky et al., 2006; Jaiswal et al., 2012; Arthur & Vassilvitskii, 2007) have shown how to obtain simple bicriteria approximation algorithms for k-means. (Arthur & Vassilvitskii, 2007) also proposed a variant of Lloyd\u2019s algorithm, termed \u201ck-means++\u201d, which also comes with a theoretical approximation factor guarantee of O(log k) approximation.\nAll the algorithms above assume that data fits in a single machine. However, with the ubiquity of large data sets, there has been a lot of interest in distributed algorithms where data is spread across several machines. The goal is to use available distributed models of computation to design algorithms that can (a) work with machines having access only to their local data set, (b) use small amount of memory and only a few \u201crounds\u201d of communication, and (c) have approximation guarantees for the solution they output.\nFor k-means and related objectives, the paradigm of iterative \u2018data reduction\u2019 has been remarkably successful. The main idea is that in each round, a machine chooses a small subset of its input, and only this subset is carried to the next round. Thus the total number of points reduces by a significant factor in every round, and this results in a small number of rounds overall. Such an algorithm can be implemented\nefficiently in the MapReduce framework, introduced by (Dean & Ghemawat, 2004), and formalized by (Karloff et al., 2010)). (Ene et al., 2011) gave one of the first such implementations (for the k-median problem), and showed theoretical guarantees. This line of work has subsequently been developed in (Kumar et al., 2013; Balcan et al., 2013a; Awasthi et al., 2017). The last work also gives a summary of the known results in this space.\nThe high level ideas used in these works are similar to those used in streaming algorithms for clustering. The literature here is very rich; one of the earliest works is that of (Charikar et al., 1997), for the k-center problem. The work of (Guha et al., 2001) introduced many ideas crucial to the distributed algorithms mentioned above. Indeed, all of these algorithms can be viewed as implicitly constructing coresets (or summaries) for the underlying clustering problem. We refer to the works of (Agarwal et al., 2004; 2012; Balcan et al., 2013b; Indyk et al., 2014) for more on this connection.\nMotivation for our work. While iterative data reduction is powerful, it has a key bottleneck: in order to have approximation guarantees, machines always need to store > k data points. Indeed, all the algorithms we are aware of require a memory of kn if they are to useO(1/ ) rounds of MAPREDUCE computation.1 The high level reason for this is that if a machine sees k points that are all very far from one another, it needs to keep all of them, or else we might lose all the information about one of the clusters, and this could lead to a large objective value. This is also the reason each machine needs to communicate \u2265 k points to the others (such a lower bound was proved formally in (Chen et al., 2016), as we will discuss later). The natural question is thus to ask: can we partition the data across machines so that different machines work in different \u201cregions of space\u201d, and thus focus on finding different clusters? This would result in a smaller space requirement per machine, and lesser communication between machines. Our main result is to show that this is possible, as long as we have a rough estimate of the optimum objective value (up to an arbitrary polynomial factor). We give an algorithm based on a variant of locality sensitive hashing, and prove that this yields a bi-criteria approximation guarantee.\nLocality sensitive hashing was introduced in the seminal work of (Indyk & Motwani, 1998), which gave an efficient algorithm for nearest neighbor search in high dimensional space. The idea has found several applications in machine learning and data science, ranging from the early applications of similarity search to the speeding up of neural networks (Spring & Shrivastava, 2017). (Datar et al., 2004)\n1All the algorithms mentioned above can be naturally implemented in the MAPREDUCE framework.\ngeneralized the original result of (Indyk & Motwani, 1998) to the case of `p norms, and (Andoni & Indyk, 2006) gave an improved analysis. Extensions of LSH are still an active area of research, but a discussion is beyond the scope of this paper. Our contribution here is to understand the behavior of clusters of points under LSH and its variants."}, {"heading": "1.1. Our results", "text": "Our focus in the paper will be on the k-means objective (defined formally in Section 2). The data set is assumed to be a collection of points in a Euclidean space Rd for some d, and distance refers to the `2 distance.\nOur first contribution is an analysis of \u201cproduct LSH\u201d (PLSH), a hash obtained by concatenating independent copies of an LSH. For each LSH, we consider the implementation of (Andoni & Indyk, 2006).\nInformal theorem 1 (See Lemmas 1 and 2). Let C be any cluster of points with diameter \u03c3. Then PLSH with appropriate parameters yields the same hash for all the points in C, with probability \u2265 3/4. Furthermore, for any two points u, v such that \u2016u \u2212 v\u2016 \u2265 \u03b1 \u00b7 \u03c3, where \u03b1 \u2248 log n log log n, the probability that u and v have the same hash is < 1/n2.\nThus, PLSH has a \u201ccluster preserving\u201d property. We show the above by extending the analyses of (Indyk & Motwani, 1998) and (Andoni & Indyk, 2006). Then, we use this observation to give a simple bi-criteria approximation algorithm for k-means clustering. (A bi-criteria algorithm is one that is allowed to output a slightly larger number of centers; see Section 2.) We assume knowledge of k, as well as a very rough estimate of the objective value. The algorithm returns a polylogarithmically larger number of clusters, while obtaining a polylogarithmic factor approximation. We refer to Theorem 2 for the statement. As we note below, if s > k polylog(()n), then we can avoid violating the bound on the number of clusters (and obtain a \u201ctrue\u201d guarantee as opposed to a bi-criteria one).\nThe algorithm can be implemented in a distributed manner, specifically in the MAPREDUCE model, with dlogs ne+ 2 rounds, using machines of memory s (when we say memory s, we mean that each machine can store at most s of the points. This will be roughly the same as measuring s in bytes, as we see in Section 2. The formal result is stated in Theorem 3. We highlight that the distributed algorithm works for any s \u2265 \u03c9(log n), even s k (in which case the standard reduce-and-merge framework has no non-trivial guarantees).\nFinally, we prove that for any MapReduce algorithm that uses poly(n) machines of space s, the number of rounds necessary to obtain any non-trivial approximation to k-means is at least dlogs ne. Thus the \u2018round/memory tradeoff\u2019 we\nobtain is nearly optimal. This is based on ideas from the recent remarkable result of (Roughgarden et al., 2016). (See Theorem 4.)"}, {"heading": "1.2. Discussion, extensions and limitations", "text": "Going beyond communication lower bounds. The recent result of (Chen et al., 2016) shows lower bounds on the total amount of communication necessary for distributed clustering. They show that for a worst-case partition of points across machines, \u2126(Mk) bits are necessary, where M is the number of machines. Our result in Section 4.2 implies that if points have been partitioned across machines according to PLSH hashes, we can bypass this lower bound.\nRound lower bound. In light of Theorem 4, one way to interpret our algorithmic result is as saying that as far as obtaining polylogarithmic bi-criteria approximations go, clustering is essentially as easy as \u201caggregation\u201d (i.e., summing a collection of n numbers \u2013 which also has the same logs n upper and lower bounds).\nPrecisely k clusters. Theorem 3 gives only a bi-criteria guarantee, so it is natural to ask if we can obtain any guarantee when we desire precisely k centers. In the case when s \u2265 k log2 n, we can apply known results to obtain this. (Guha et al., 2001) showed (in our notation) that:\nTheorem 1. Let U be a set of points, and let S be a set of centers with the property that \u2211 u\u2208U d(u, S)\n2 \u2264 \u03b3 \u00b7 OPT, where OPT is the optimum k-means objective value on U . Let g : U 7\u2192 S map every u \u2208 U to its closest point in S, breaking ties arbitrarily. Now, consider a new weighted instance I of k-means where we have points in S, with weight of v \u2208 S being |g\u22121(v)|. Then, any set of centers that \u03c1-approximate the optimum objective for I give a (4\u03b3+ 2\u03c1) approximation to the original instance (given by U ).\nThus, if s \u2265 k log2 n, we can aggregate the output of our bi-criteria algorithm onto one machine, and solve k-means approximately. In essence, we are using the output of our algorithm as a coreset for k-means. We demonstrate this in our experimental results.\nBalanced clustering. A common constraint for clustering algorithms is that of the clusters being balanced. This is often captured by requiring an upper bound on the size of a cluster. (Bateni et al., 2014) showed that balanced clustering can also be solved in a distributed setting. Specifically, they showed that any bi-criteria algorithm for k-means can be used to solve the balanced clustering problem, via a result analogous to 1. In our context, this implies that if s > k log2 n, our method also gives a distributed algorithm for balanced clustering with a k-means objective.\nLimitations and lower bounds. There are two key limita-\ntions to our result. First, the polylogarithmic approximation factor in the approximation ratio seems difficult to avoid (although our experiments show that the guarantees are very pessimistic). In our argument, it arises as a by-product of being able to detect very small clusters. This is in contrast with single machine algorithms (e.g., (Kanungo et al., 2004; Ahmadian et al., 2017)) and the prior work in MapReduce algorithms, (Ene et al., 2011), which give constant factor approximations. Another restriction is that our algorithms assume a Euclidean setting for the points. The algorithms of (Ene et al., 2011) and related works can handle the case of arbitrary metric spaces. The bottleneck here is the lack of locality sensitive hashing for such spaces. A very interesting open problem is to develop new methods in this case, or prove stronger lower bounds."}, {"heading": "2. Notation and Preliminaries", "text": "We now introduce some notation and definitions that will be used for the rest of the paper. We will denote by U the set of points in the input. We denote n = |U |. All of our algorithms are for the Euclidean setting, where the points in U are in Rd, and the distance between x, y \u2208 U is the `2 norm \u2016x\u2212 y\u20162 = \u221a\u2211 i(xi \u2212 yi)2.\nA k-clustering of the points U is a partition C of U into subsets C1, C2, . . . , Ck. The centroid of a cluster Ci is the point \u00b5i := 1|Ci| \u2211 u\u2208Ci u. The k-means objective for the clustering C is now defined as\u2211 i\u2208[k] \u2211 u\u2208Ci \u2016u\u2212 \u00b5i\u201622. (1)\nThe problem of k-means clustering is to find C that minimizes the objective defined above. The minimum objective value will be denoted by OPT(U). (When the U is clear from context, we simply write OPT.) A \u03c1-approximation algorithm for k-means clustering is a polynomial time algorithm that outputs a clustering C\u2032 whose objective value is at most \u03c1 \u00b7OPT(U). We will be interested in \u03c1 being a constant or polylog(n). A (\u03c1, \u03b2) bi-criteria approximation (where \u03b2 \u2265 1) is an efficient algorithm that outputs a clustering C\u2032 that has at most \u03b2k clusters and has an objective value at most \u03c1 \u00b7OPT(U). Note that the optimum still has k clusters.\nNote on the dimension d. We assume that d = O(log n). This is without loss of generality, because we may assume that we have pre-processed the data by applying JohnsonLindenstrauss transform. As the JL transform preserves all pairwise `2 distances (Johnson & Lindenstrauss, 1984; Indyk & Motwani, 1998), clustering in the transformed space gives a (1 + ) approximation to clustering in the original one. Furthermore, the transform can be applied in parallel, individually to each point. Thus we henceforth assume that the space required to store a point is O(log n).\nMapReduce model. To illustrate our ideas, we use the well-studied MapReduce model (Dean & Ghemawat, 2004; Karloff et al., 2010). The details of the map and reduce operations are not important for our purpose. We will view it as a model of computation that proceeds in levels. At each level, we have M machines that can perform computation on their local data (the input is distributed arbitrarily among machines in the first layer). Once all the machines are done with computation, they send information to machines in the next layer. The information received by a machine acts as its local data for the next round of computation. We assume that each machine has a memory of s.\nConstants. For the sake of easier exposition, we do not attempt to optimize the constants in our bounds."}, {"heading": "3. Two Step Hashing", "text": "The key to our distributed algorithm is a two step hashing scheme. The first step is a \u2018product\u2019 of locality sensitive hashes (LSH), and the second is a random hash that maps the tuples obtained from the product-LSH to a bin with a label in the range 1, . . . , Lk, for an appropriate constant L."}, {"heading": "3.1. Product LSH", "text": "We begin with a short discussion of LSH. We follow the presentation of (Andoni & Indyk, 2006).2\nSuppose we have a collection of points U in Rd.\nLocality sensitive hashing (LSH). Let t, w be parameters. LSHt,w is a procedure that takes a u \u2208 U , and produces a (t + 1)-tuple of integers. The hash uses as parameters a matrix A of dimensions t \u00d7 d, whose entries are i.i.d. N (0, 1) Gaussians, and a collection of shift vectors S = {s1, . . . , sU}, where si is picked uniformly from [0, 4w]t. The shifts are used to generate a collection of shifted grids Gti := G\nt + si, where Gt is the integer grid Zt, scaled by 4w. Now to compute the hash of a point u, first its projection to Rt is computed by u\u2032 = Au. Next, one searches for the smallest index i \u2208 [U ] for which the ball B(u\u2032, w) contains a point of the shifted grid Gti. (Alternately one could imagine radius-w balls around the points in the shifted grids, and we look for the smallest i for which the point u\u2032 is contained in one such ball.) The hash of the point is then (i, x1, . . . , xt), where i is the index as above, and (x1, . . . , xt) are the integer coordinates corresponding to the grid point in Gti that is at distance \u2264 w from u\u2032.\n(Andoni & Indyk, 2006) show that to cover all of Rt (and thus to have a well-defined hash for every point), the number of shifts that suffice is 2O(t log t). Consequently, this is also\n2The earlier LSH schemes of (Indyk & Motwani, 1998) and (Datar et al., 2004) can also be used; however, they give a weaker approximation factor.\nthe time required to compute hash for a point, as we may need to go through all the shifts. In our setting, we will choose t = o(log n/ log log n), and thus the time needed to hash is no(1).\nProduct LSH (PLSH). Given an integer `, the product LSH PLSHt,w,` is a hashing scheme that maps a point u to a concatenation of ` independent copies of LSHt,w; it thus outputs an `(t+ 1)-tuple of integers.\nWe show the following properties of PLSH. In what follows, let \u03c3 be a parameter. Let\nw = 8\u03c3(log n)3/2; t = log n\n(log log n)2 ; ` = 32(log log n)2.\n(2)\nLemma 1. Suppose C \u2286 U has diameter \u2264 \u03c3. Then with probability at least 3/4, PLSHt,w,` maps all the points in C to the same hash value. Lemma 2. Let u, v be points that are at distance\u2265 4w/ \u221a t (= O(log n log log n) \u00b7 \u03c3). Then the probability that they have the same hash value is < 1n4 .\nProof of Lemma 1. Let C \u2032 = {Ax : x \u2208 C}. First, we claim that the diameter ofC \u2032 is at most 4\u03c3 \u221a t+ log n, w.h.p. over the choice of A. This is because for any x, y \u2208 C, the quantity \u2016A(x\u2212 y)\u201622/\u2016x\u2212 y\u201622 is distributed as a \u03c72 with t degrees of freedom. It is known (e.g., (Laurent & Massart, 2000), Lemma 1) that the tail of a chi-square statistic Y with t degrees of freedom satisfies: for any z > 0, Pr[Y \u2265 t + 2 \u221a tz + 2z] \u2264 e\u2212z . Setting z = 4 log n, and using 2 \u221a tz \u2264 t+ z, we get that Pr[Y \u2265 16(t+ log n)] < 1/n4. Thus by taking union bound over all pairs of points x, y, we have that with probability \u2265 1\u2212 1n2 , the diameter of C\n\u2032 is \u2264 4\u03c3 \u221a t+ log n.\nConditioned on this, let us calculate the probability that the points in C all have the same hash value. (The conditioning does not introduce any dependencies, as the above argument depended only on the choice of A, while the next step will depend on the choice of the shifts.) Now, consider a ball B\u2217 \u2286 Rt of radius r\u2032 := 4\u03c3 \u221a t+ log n that contains C \u2032 (as C \u2032 is of small diameter, such a ball exists).\nBefore analyzing the probability of interest, let us understand when a shifted grid Gti contains a point that has distance \u2264 w to a given point x. This is equivalent to saying that (x\u2212si) is w-close to a lattice point inGt. This happens iff si is in the ball B(x,w), where the ball has been reduced modulo [0, 4w]t (see Figure 1).\nNow, we can see how it could happen that some x \u2208 B\u2217 is w-close to a lattice point in Gti but the entirety of B\n\u2217 does not have this property. Geometrically, the bad choices of si are shown in Figure 1 (before reducing moulo [0, 4w]t). Thus, we have that the probability that all points in B\u2217 are w-close to a lattice point in Gti conditioned on there existing\na point x \u2208 B\u2217 that is close to a lattice point is at least\np1 := (w \u2212 r\u2032)t\n(w + r\u2032)t =\n( 1\u2212 2r \u2032\nw + r\u2032\n)t .\nThus p1 is a lower bound on a single LSH giving the same hash value for all the points inC. Repeating this ` times, and plugging in our choice of values for r\u2032, w, t, `, the desired claim follows.\nNext, we get to the proof of Lemma 2.\nProof. Let u, v be points as in the statement of the lemma. We show that the probability that \u2016A(u\u2212 v)\u2016 \u2264 2w in all the ` hashes is < 1/n4. This clearly implies what we need, because if in even one LSH we haveAu andAv being> 2w away, they cannot have the same PLSH.3\nNow, for a randomA, the quantity \u2016A(u\u2212v)\u201622/\u2016u\u2212v\u201622 is distributed as a \u03c72 distribution with t degrees of freedom (as we saw earlier), and thus using the lower tail from (Laurent & Massart, 2000), we get that for any z > 0, for such a random variable Y , we have Pr[Y \u2264 t \u2212 2 \u221a tz] < e\u2212z . Thus Pr[Y \u2264 (1\u2212 1\u221a 2 )t] \u2264 e\u2212t/8. Now, for our choice of parameters, we have 4w2/\u2016u \u2212 v\u201622 \u2264 t/4 < (1 \u2212 1\u221a2 )t, and thus the probability that u and v have the same PLSH is upper bounded by e\u2212t`/8 = 1/n4, as desired."}, {"heading": "3.2. Number of tuples for a cluster", "text": "We have shown that the probability that a cluster of diameter \u2264 \u03c3 hashes to precisely one tuple (for appropriate parameters) is \u2265 3/4. We now show something slightly stronger (as we will need it later).\nLemma 3. Let C be a cluster of diameter \u03c3, and let t, w, ` be set as in Eq. (2). The expected number of distinct tuples\n3This reasoning allows us to get a bound slightly better than (Andoni & Indyk, 2006).\nfor points in C (produced by PLSHt,w,`) is O(1)."}, {"heading": "3.3. Second step of hashing", "text": "The PLSH maps each point u \u2208 U to an `(t + 1) tuple of integers. The second step of hashing is very simple \u2013 we simply hash each tuple independently and uniformly to an integer in [Lk], for a prescribed parameter L."}, {"heading": "4. Approximation Algorithm", "text": "We start by describing our algorithm in a single machine setting. Then in Section 4.2, we describe how it can be implemented in parallel, with a small number of machines, and a small memory per machine."}, {"heading": "4.1. Main algorithm", "text": "The high level idea in our algorithm is to perform the twolevel hashing above, and choose a random subset of points from each bin.\nNow, in order to choose the w parameter in the hash, we need a rough scale of the optimum. To this end, we will assume that we know a D such that the optimum objective value is in the range (D/f,D), for some f = poly(n). Note that f can be something like n2, so this is a very mild assumption. With this assumption, we have that the average contribution of a point to the objective (i.e., its squared distance to its center) is\u2265 D/(n \u00b7 f). Let us denote r0 := \u221a D/(n \u00b7 f). Also, observe that no point can have a contribution more than D to the objective (as it is an upper bound on the sum of the contributions). Thus, inuitively, all the clusters have a radius (formally defined below) \u2264 \u221a D. Let \u03ba = dlog(nf)e, and let ri := 2i/2r0, for 1 \u2264 i \u2264 \u03ba. These will be the different radius \u201cscales\u201d for our clusters of interest. Note that \u03ba = O(log n), as f = poly(n).\nThe algorithm can now be described (see Algorithm 4.1). Algorithm 1 Find-Cover\nInput: set of points U , rough estimate of optimum D. Output: a subset of points S. for i = 1 . . . \u03ba do\n- Hash every point in U to a bin (range [Lk]) using the two layer hash with params t, wi, `, Lk, where wi := 8ri(log n)\n3/2. Let Uj be the points hashed to bin j. - Let Gj be the group of machines assigned for bin j. For each j, assign points in Uj uniformly at random to a machine in Gj . - For each j, select a uniformly random subset of Uj of size O(1) from Gj and add them to S. (If the number of points in the group is O(1), add all of them to S.)\nend for\nIn the remainder of this section, we analyze this algorithm. We start with a definition.\nDefinition 1 (Cluster radius). For a cluster C with centroid \u00b5, we define the radius to be the quantity \u03c1 :=\u221a\n1 |C| \u2211 p\u2208C\u2016p\u2212 \u00b5\u201622, i.e., the \u201c`22 average\u201d radius.\nObservation 1. In any clusterC with centroid \u00b5 and radius \u03c1, the number of points p in C such that \u2016p\u2212 \u00b5\u20162 > 2\u03c1 is at most |C|/4.\nThe proof follows by an averaging argument. Now, a candidate goal is to prove that for every optimum cluster Ci (center \u00b5i, radius \u03c1i), the algorithm chooses at least one point at a distance \u2264 \u03b1\u03c1i from the center \u00b5i with high probability, for some small \u03b1.\nUnfortunately this statement is false in general. Suppose the instance has an optimal cluster with small \u03c1i and a small |Ci| that is really far from the rest of the points (thus it is essential to \u201cfind\u201d that cluster). In this case, for rj that is roughly \u03c1i (which is the scale at which we hope to find a point close to this cluster), the bin containing Ci may contain many other points that are far away; thus a random sample is unlikely to choose any point close to Ci.\nThe fix for this problem comes from the observation that small clusters (i.e. small |Ci|) can afford to pay more per point to the objective. We thus define the notion of \u201cadjusted radius\u201d of a cluster. First, we define \u03b8 to be the real number satisfying OPT = n\u03b82, i.e., the typical distance of a point to its cluster center in the optimal clustering.4 Now, we have:\nDefinition 2 (Adjusted radius). For a cluster C with radius \u03c1, we define the adjusted radius to be the quantity \u03c1 satisfying \u03c12 = \u03c12 + \u03b82 + n\u03b8 2\nk|C| .\nOur main lemma about the algorithm is the following.\nLemma 4. Let C be a cluster in the optimal clustering with adjusted radius \u03c1. With probability \u2265 1/4, Algorithm 4.1 outputs a point that is at a distance \u2264 \u03b1 \u00b7 \u03c1 from the center of the cluster C, where \u03b1 = O(log n log log n).\nThis is used to show the main result of this section.\nTheorem 2. Let S\u2032 be the union of the sets output by O(log k) independent runs of Algorithm 4.1. For \u03b1 = O(log n log log n), S\u2032 gives an (\u03b12, O(log n log k)) bicriteria approximation for k-means, w.p. at least 9/10.\nProof of Theorem 2 assuming Lemma 4. First, let us analyze the number of points output. Note that in each run of the algorithm, we output O(Lk) = O(k) points for each radius range. There areO(log n) radius ranges andO(log k) independent runs. Thus we have the desired bound.\nNext, consider the approximation factor. As we take S\u2032 to be the union of O(log k) independent runs of Algorithm 4.1,\n4We note that \u03b8 is used solely for analysis purposes \u2013 the algorithm is not assumed to know it.\nthe success probability in Lemma 4 can be boosted to 1\u2212 1 10k , and by a union bound, we have that the conclusion of the lemma holds for all clusters, with probability > 1/10. Thus for every optimal cluster Ci of adjusted radius \u03c1i, Algorithm 4.1 outputs at least one point at a distance\u2264 \u03b1\u00b7\u03c1i, for \u03b1 as desired. Thus, assigning all the points in Ci to one such point would imply that the points of Ci contribute at most |Ci|\u03c12i +\u03b12|Ci|\u03c12i to the objective.5 Thus the objective value is at most\u2211\ni\n|Ci|\u03c12i + \u03b12|Ci| ( \u03c12i + \u03b8 2 + n\u03b82\nk|Ci| ) = (1 + \u03b12)OPT + \u03b12 \u00b7 2n\u03b82 \u2264 4\u03b12OPT.\nThis completes the proof."}, {"heading": "4.2. Distributed implementation", "text": "We now see how to implement algorithm from Theorem 2 in a distributed setting with a small number of rounds and machines, while also using memory \u2264 s per machine. Our final result is the following.\nTheorem 3. There is a distributed algorithm that performs dlogs ne + 2 rounds of MAPREDUCE computation, and outputs a bi-criteria approximation to k-means, with the same guarantee as Theorem 2. The number of machines needed is O\u0303 ( n\nmin{k,s} \u00b7 k s\n) , and the space per machine is\ns.\nNote. Whenever s \u2265 k, the bound on the number of machines is O\u0303(n/s), which is essentially optimal, because we need n/s machines to hold n points, if each machine has a memory of s.\nWhile most parts of the algorithm from Theorem 2 can be immediately parallelized, sampling from Uj (which may need to be split across machines) is the tricky part and requires some work. The proof is deferred to Section 3 of the supplement."}, {"heading": "5. Lower Bound", "text": "We now show that even in the very simple setting of points on a line, we have a tradeoff between the number of rounds and memory. This matches the behavior of our algorithm, up to an additive constant.\nTheorem 4. Let \u03b1 be any parameter that is poly(n). Then, any \u03b1 factor approximation algorithm for k-means with k \u2265 2 that uses poly(n) machines of memory \u2264 s requires at least logs n rounds of MAPREDUCE. The proof is by a simple reduction from Boolean-OR,6 a\n5This follows from a \u201ccenter-of-mass\u201d theorem that is standard: for a set T of points with centroid \u00b5, and any other point \u00b5\u2032,\u2211\nu\u2208T \u2016u\u2212 \u00b5 \u2032\u20162 = \u2211 u\u2208T \u2016u\u2212 \u00b5\u2016\n2 + |T |\u2016\u00b5\u2212 \u00b5\u2032\u20162. 6The input is the set of bits x1, . . . , xn, and the desired output\nproblem for which a round-memory trade-off was established in (Roughgarden et al., 2016).\nProof. Suppose we have inputs x1, . . . , xn, the inputs for Boolean OR. We produce an instance of clustering with k + n points, all on the line.\nFirst, we place points at 1, 2, . . . , k. Additionally, for 1 \u2264 i \u2264 n, if xi = 1, we add a point at k+\u03b1+1. If xi = 0, add a point at 1. Now if the OR of the xi\u2019s is TRUE, then the optimum solution places centers at 1, 2, . . . , k\u22121, k+\u03b1+1. This results in an objective value of 1. If the OR is FALSE, the optimum solution is to place centers at 1, 2, . . . , k (0 cost). Thus an \u03b1 factor approximation should be able to distinguish between the two cases (because in the NO case, it needs to have error 0, and in the YES case, this solution will be a factor > \u03b1 off.\nNote. The parameter \u03b1 implicitly comes up in the reduction. The number of bits necessary to write down the points xi is n log\u03b1. This is why we insist on \u03b1 = poly(n).\nThe lower bound above can be extended in order to rule out both (a) the case in which we have a rough estimate of the optimum (as in our algorithm), and (b) bi-criteria approximations. To handle (a), we can perturb the NO case so as to make the objective 1/p(n\u03b1) for a large polynomial p(\u00b7). In order to handle (b), i.e., to rule out an (\u03b1, \u03b2) bicriteria approximation, we need to add a multiplicity of \u03b2k for each of the points in the proof above. This leads to a slightly weaker lower bound of logs n k\u03b2 rounds. The details of these steps are straightforward, so we omit them."}, {"heading": "6. Empirical Study", "text": "We evaluate our algorithmic ideas on two synthetic and two real datasets, of varying sizes. In the former case, we know the ground truth clustering, the \u201cright k\u201d, and the optimum objective value. We use it to demonstrate how the quality of clustering depends on the parameter ` \u2013 the number of independent hashes we concatenate. In all the datasets, we compare the objective value obtained by our algorithm with the one obtained by k-means++ (part of scikit-learn (Pedregosa et al., 2011)). This will only be possible for small enough datasets, as k-means++ is a single machine algorithm that uses \u2126(nk) memory."}, {"heading": "6.1. Synthetic datasets", "text": "Both the datasets we consider are mixtures of Gaussians. The first has n = 105 points in R50 and k = 100, while the second has n = 106 point in R50 and k = 1000. For i \u2208 [k], the centers are chosen uniformly from a box of length 400 in each dimension, maintaining a distance of 20 from one\nis simply the OR of the bits.\nanother. To form cluster Ci, a random number of points are chosen from the Gaussian N (\u00b5i, 1).\nFor the first dataset, we produce PLSH tuples using w = 15, t = 2, and vary `.We call the set of points that hash to a given tuple a bucket of points. We measure the following quantities: (a) the total number of non-empty buckets, (b) the \u201cpurity\u201d of the buckets, i.e., the number of distinct clusters at intersect a non-empty bucket (on average), and (c) the \u201cspread\u201d of a cluster, i.e., the number of buckets that points of a cluster go to. The plots for these quantities as ` varies are shown in Figure 2. Note that it makes sense for the spread to increase as ` increases, as even a difference in one of the ` independent hashes results in unequal hashes.\nNext, we study the objective value. For this we choose ` = 3. This results in 257 non-empty buckets. Now, from each bucket, we output j points uniformly at random to form a set S (and do this for different j). Even for j = 1, the objective value is 41079, which is less than a factor 2 away from the optimum, 26820. This is significantly better than the guarantee we get from Theorem 2. It is also significantly better than a random subset of 257 points, for which it turns out that the objective is 5897317.\nIntuitively, a random sample will be bad for this instance, because there are many clusters of size n/k, and no points from such clusters will be chosen. This motivates us to measure the cluster recall of our procedure \u2013 how many clusters contribute to the 257 size set we output? Interestingly, all 100 of the clusters do, for the above values of the parameters. These results are consistent with the theoretical observations that PLSH finds small-cardinality clusters while a random sample does not.\nNext, consider the larger synthetic dataset. Modulo n, k, the data is generated as before. Here, we produce PLSH tuples using w = 15, t = 3, and ` = 4. For these choices of n and k, the single-machine k-means++ runs out of memory. However, as we know the \u00b5i, we can estimate the optimum\nobjective value, which is 251208.\nIn this dataset, we illustrate the use of our algorithm to generate a coreset, as discussed in Section 1.2. We obtain 5722 buckets, from each of which we sample j points to obtain a coreset S. We then run k-means on S with k = 1000, thus obtaining 1000 centers. We evaluate the k-means objective with these centers. Results for different j are shown in Figure 3. Note that even with j = 10, the objective is within a 1.1 factor of the optimum."}, {"heading": "6.2. Real datasets", "text": "We show our results on two datasets, both available via the UC Irvine dataset repository.\nSUSY. The first dataset is SUSY (see (P. Baldi, 2014)), which contains 5M points with 18 features. In order to efficiently compare with k-means++, we use a sub-sample of 100000 points. In this case we use the values of t, ` as in our theoretical results. We also try different values for w. We start with a guess of w = \u03c3(log n)3/2, where \u03c3 was obtained from k-means++ with k = 10 (which is very fast). We then scale \u03c3 from 2\u22124 to 22 in order to perform the hashing in different radius ranges. After hashing and finding S, we use it as a coreset and compute k-means. Figure 4 shows the results, and also compares against a fully random subset of points. Unlike the synthetic examples, here a random set of points is not orders of magnitude worse, but is still considerably worse than the output of our algorithm. We also note that our values are within a factor 1.2 of k-means++ (which is sequential and significantly slower). The number of buckets per cluster when k = 600 for ` = 1, . . . , 6 are 0.03, 0.31, 1.21, 3.97, 7, 10.55.\nFMA: A Dataset For Music Analysis This dataset (see (Defferrard et al., 2017)) contains 518 features extracted from audio files available in the free music archive (FMA). It has 106574 points. We perform the same experiment we did for the SUSY dataset. Figure 5 shows\nthe results, comparing the outputs with the output of kmeans++, as well as a random subset. The number of buckets per cluster when k = 512 for ` = 1, . . . , 6 are 0.08, 0.68, 2.29, 5.27, 9.43, 14.09 respectively."}], "year": 2018, "references": [{"title": "Approximating extent measures of points", "authors": ["P.K. Agarwal", "S. Har-Peled", "K.R. Varadarajan"], "venue": "Journal of the ACM (JACM),", "year": 2004}, {"title": "Better guarantees for k-means and euclidean k-median by primal-dual algorithms", "authors": ["S. Ahmadian", "A. Norouzi-Fard", "O. Svensson", "J. Ward"], "venue": "IEEE 58th Annual Symposium on Foundations of Computer Science (FOCS),", "year": 2017}, {"title": "K-means++: The advantages of careful seeding", "authors": ["D. Arthur", "S. Vassilvitskii"], "venue": "In Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2007}, {"title": "Smoothed analysis of the k-means method", "authors": ["D. Arthur", "B. Manthey", "H. R\u00f6glin"], "venue": "J. ACM,", "year": 2011}, {"title": "Improved spectral-norm bounds for clustering. In Approximation, Randomization, and Combinatorial Optimization", "authors": ["P. Awasthi", "O. Sheffet"], "venue": "Algorithms and Techniques - 15th International Workshop,", "year": 2012}, {"title": "General and robust communication-efficient algorithms for distributed clustering", "authors": ["P. Awasthi", "M. Balcan", "C. White"], "year": 2017}, {"title": "Distributed kmeans and k-median clustering on general communication topologies", "authors": ["M. Balcan", "S. Ehrlich", "Y. Liang"], "venue": "In Advances in Neural Information Processing Systems", "year": 2013}, {"title": "Distributed clustering on graphs", "authors": ["Balcan", "M.-F", "S. Ehrlich", "Y. Liang"], "venue": "In NIPS, pp. to appear,", "year": 2013}, {"title": "Distributed balanced clustering via mapping coresets", "authors": ["M. Bateni", "A. Bhaskara", "S. Lattanzi", "V.S. Mirrokni"], "venue": "In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems", "year": 2014}, {"title": "Incremental clustering and dynamic information retrieval", "authors": ["M. Charikar", "C. Chekuri", "T. Feder", "R. Motwani"], "venue": "In Proceedings of the Twenty-ninth Annual ACM Symposium on Theory of Computing,", "year": 1997}, {"title": "A cost function for similarity-based hierarchical clustering", "authors": ["S. Dasgupta"], "venue": "In Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing,", "year": 2016}, {"title": "Locality-sensitive hashing scheme based on p-stable distributions", "authors": ["M. Datar", "N. Immorlica", "P. Indyk", "V.S. Mirrokni"], "venue": "In Proceedings of the Twentieth Annual Symposium on Computational Geometry,", "year": 2004}, {"title": "Mapreduce: Simplified data processing on large clusters", "authors": ["J. Dean", "S. Ghemawat"], "venue": "In OSDI, pp", "year": 2004}, {"title": "Fma: A dataset for music analysis", "authors": ["M. Defferrard", "K. Benzi", "P. Vandergheynst", "X. Bresson"], "year": 2017}, {"title": "Fast clustering using mapreduce", "authors": ["A. Ene", "S. Im", "B. Moseley"], "venue": "In KDD, pp", "year": 2011}, {"title": "The elements of statistical learning: data mining, inference and prediction", "authors": ["T. Hastie", "R. Tibshirani", "J. Friedman"], "year": 2009}, {"title": "Approximate nearest neighbors: towards removing the curse of dimensionality", "authors": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the thirtieth annual ACM symposium on Theory of computing,", "year": 1998}, {"title": "Composable core-sets for diversity and coverage maximization", "authors": ["P. Indyk", "S. Mahabadi", "M. Mahdian", "V. Mirrokni"], "venue": "In unpublished,", "year": 2014}, {"title": "A simple d\u02c62-sampling based PTAS for k-means and other clustering problems", "authors": ["R. Jaiswal", "A. Kumar", "S. Sen"], "venue": "CoRR, abs/1201.4206,", "year": 2012}, {"title": "Extensions of Lipschitz mappings into a Hilbert space. In Conference in modern analysis and probability (New Haven, Conn., 1982), volume 26 of Contemp", "authors": ["W.B. Johnson", "J. Lindenstrauss"], "year": 1984}, {"title": "A local search approximation algorithm for k-means clustering", "authors": ["T. Kanungo", "D.M. Mount", "N.S. Netanyahu", "C.D. Piatko", "R. Silverman", "A.Y. Wu"], "venue": "Computational Geometry,", "year": 2004}, {"title": "A model of computation for mapreduce", "authors": ["H.J. Karloff", "S. Suri", "S. Vassilvitskii"], "venue": "In SODA, pp", "year": 2010}, {"title": "An impossibility theorem for clustering", "authors": ["J. Kleinberg"], "venue": "In Proceedings of the 15th International Conference on Neural Information Processing Systems,", "year": 2002}, {"title": "Clustering with spectral norm and the k-means algorithm", "authors": ["A. Kumar", "R. Kannan"], "venue": "In 51th Annual IEEE Symposium on Foundations of Computer Science,", "year": 2010}, {"title": "Fast greedy algorithms in mapreduce and streaming", "authors": ["R. Kumar", "B. Moseley", "S. Vassilvitskii", "A. Vattani"], "venue": "In SPAA, pp", "year": 2013}, {"title": "Adaptive estimation of a quadratic functional by model selection", "authors": ["B. Laurent", "P. Massart"], "venue": "Ann. Statist., 28 (5):1302\u20131338,", "year": 2000}, {"title": "Least squares quantization in pcm", "authors": ["S.P. Lloyd"], "venue": "IEEE Trans. Information Theory,", "year": 1982}, {"title": "The effectiveness of lloyd-type methods for the kmeans problem", "authors": ["R. Ostrovsky", "Y. Rabani", "L.J. Schulman", "C. Swamy"], "venue": "In 47th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2006),", "year": 2006}, {"title": "Searching for exotic particles in high-energy physics with deep learning", "authors": ["P. Baldi", "D.W.P. Sadowski"], "venue": "Nature Communications,", "year": 2014}, {"title": "Shuffles and circuits: (on lower bounds for modern parallel computation)", "authors": ["T. Roughgarden", "S. Vassilvitskii", "J.R. Wang"], "venue": "In Proceedings of the 28th ACM Symposium on Parallelism in Algorithms and Architectures,", "year": 2016}], "id": "SP:5856d8c16dce7d6aeb3836b476ff704d0770c58c", "authors": [{"name": "Aditya Bhaskara", "affiliations": []}, {"name": "Maheshakya Wijewardena", "affiliations": []}], "abstractText": "Given the importance of clustering in the analysis of large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied. A successful approach here has been the \u201creduce and merge\u201d paradigm, in which each machine reduces its input size to \u00d5(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally. This approach has the intrinsic bottleneck that each machine must solve a problem of size \u2265 k, and needs to communicate at least \u03a9(k) points to the other machines. We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on \u201cfinding different clusters\u201d. Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method. We see that our algorithm results in lower communication as well as a near-optimal number of \u2018rounds\u2019 of computation (in the popular MapReduce framework).", "title": "Distributed Clustering via LSH Based Data Partitioning"}