38	14	Furthermore, the novel architecture proposed here is not limited to morphological information alone and is flexible enough to provide other types of information for the decoder.
43	29	Luong et al. (2015) addressed the problem of rare words and OOVs with the help of a post-translation phase to exchange unknown tokens with their potential translations.
45	20	The model relies on frequent subword units instead of words.
50	47	In their model, unseen and complex words are encoded with a character-based representation, with other words encoded via the usual surface-form embeddings.
53	49	On the encoder side, words are segmented into subunits with the byte-pair segmentation model (bpe) (Sennrich et al., 2016), and on the decoder side, one target character is produced at each time step.
54	18	Accordingly, the target sequence is treated as a long chain of characters without explicit segmentation.
57	39	Among all the models reviewed in this section, the network proposed by Chung et al. (2016) could be seen as the best alternative for translating into MRLs as it works at the character level on the decoder side and it was evaluated in different settings on different languages.
58	15	Consequently, we consider it as a baseline model in our experiments.
78	22	In our first extension, the prediction probability is conditioned on one more constraint in addition to those three existing ones, as in p(yi|y1, ..., yi−1,x) = g(hi, yi−1, ci, cmi ), where cmi is the morphological context vector and carries information from those useful affixes which can enrich the decoder’s information.
79	13	cmi is generated via an attention module over the morphology table which works in a similar manner to wordbased attention model.
80	29	The attention procedure for generating cmi is formulated as in (2): cmi = |A|∑ u=1 βiufu βiu = exp (emiu)∑ |A| v=1 exp (eiv) ; emiu = a m(fu, hi−1) (2) where fu represents the embedding of the u-th affix (u-th column) in the morphology/affix tableA, βiu is the weight assigned to fu when predicting the i-th target token, and am is a feed-forward connection between the morphology table and the decoder.
81	27	The attention module in general can be considered as a search mechanism, e.g. in the original encoder-decoder architecture the basic attention module finds the most relevant input words to make the prediction.
84	13	In this scenario, the morphology table including the target language’s affixes can be considered as an external knowledge repository that sends auxiliary signals which accompany the main input sequence at all time steps.
99	16	For the same word, the second channel is supposed to predict stem-C for the fist 7 steps as the first 7 characters ‘terbiye’ belong to the stem of the word.
112	30	In the second scenario we also have a similar problem as the last layer requires some information to predict the correct morphological class through the second channel, but there is no guarantee to ensure that information in the decoder is sufficient for this sort of prediction.
113	12	In order to address these problems, in the third extension we combine both scenarios as they are complementary and can potentially help each other.
133	27	For English–Turkish (En–Tr) we use the OpenSubtitle2016 collection (Lison and Tiedemann, 2016).
135	12	We randomly select 3K sentences for each of the development and test sets for En–Tr.
150	38	Using the neural language model we train word, stem, and affix embeddings, and initialize the look-up table (but not other parts) of the decoder using those affixes.
152	12	Clearly, such an affix table is an additional knowledge source for the decoder.
159	21	unchanged and evaluated the extended model in the same setting.
160	60	Table 3 reports BLEU scores (Papineni et al., 2002) of our NMT models.
164	12	• It seems that there is a direct relation between the size of the morphology table and the gain provided for the decoder, because Russian and Turkish have bigger tables and benefit from the table more than German which has fewer affixes.
169	17	To further study our models’ behaviour and ensure that our extensions do not generate random improvements we visualized some attention weights when generating ‘terbiyesizlik’.
170	21	In Figure 4, the upper figure shows attention weights for all Turkish affixes, where the y axis shows different time steps and the x axis includes attention weights of all affixes (304 columns) for those time steps, e.g. the first row and the first column represents the attention weight assigned to the first Turkish affix when sampling t in ‘terbiyesizlik’.
181	18	However, the weights assigned to this class for t1 and i5 are much higher than those of affix characters (as they are part of the stem).
191	24	Finally, in order to complete our evaluation study we feed the English-to-German NMT model with the sentence ‘Terms and conditions for sending contributions to the BBC’, to show how the model behaves differently and generates a better target sentence.
201	29	As our model is supposed to detect this sort of intra-word relation, it treats the whole structure as two compounds which are connected to one another via an infix.
211	12	Experimental results show that our techniques were useful for NMT of MRLs.
213	45	i) We try to find more efficient ways to supply morphological information for both the encoder and decoder.
