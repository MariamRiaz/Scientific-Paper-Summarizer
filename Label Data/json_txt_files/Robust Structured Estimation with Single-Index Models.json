{"sections": [{"heading": "1. Introduction", "text": "In machine learning and statistics, a linear model of the form y = \u27e8\u03b8\u2217,x\u27e9+\u03f5 is widely used to find the relationship between feature and response, which has gained overwhelming popularity for a very long time. Here y \u2208 R and x \u2208 Rp is the pair of observed response and feature/measurement vector, \u03f5 is a zero-mean noise, and \u03b8\u2217 \u2208 Rp is the unknown parameter to be estimated. The simplicity of linear model leads to its great interpretability and computational efficiency, which are often favored in practical applications. On theoretical side, even in high-dimensional regime where sample size is smaller than the problem dimension p, strong statistical guarantees have been established under mild assumptions for various estimators, such as Lasso (Tibshirani, 1996) and Dantzig selector (Candes & Tao, 2007). Despite its attractive merits, one main drawback of linear models is the stringent assumption of linear relationship between x and y, which may fail to hold in com-\n1Department of Computer Science & Engineering, University of Minnesota-Twin Cities, Minnesota, USA. Correspondence to: Sheng Chen <shengc@cs.umn.edu>, Arindam Banerjee <banerjee@cs.umn.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nplicated scenarios. To introduce more flexibility, one option is to consider the general single-index models (SIMs) (Ichimura, 1993; Horowitz & Hardle, 1996),\nE[y|x] = f\u2217(\u27e8\u03b8\u2217,x\u27e9) , (1)\nwhere f\u2217 : R 7\u2192 R is an unknown univariate transfer function (a.k.a. link function). This class of models enjoys rich modeling power in the sense that it encompasses several useful models as special cases, which are briefly described below:\n\u2022 One-bit Compressed Sensing: In one-bit compressed sensing (1-bit CS) (Boufounos & Baraniuk, 2008; Plan & Vershynin, 2013), the response y is restricted to be binary, i.e., y \u2208 {+1,\u22121}, and the range of transfer function f\u2217 is [\u22121, 1]. Given the measurement vector x, one can generate y from the Bernoulli model,\ny + 1\n2 \u223c Ber\n( f\u2217(\u27e8\u03b8\u2217,x\u27e9) + 1\n2\n) . (2)\nIn the noiseless case, f\u2217(z) = sign(z) and y always reflects the true sign of \u27e8\u03b8\u2217,x\u27e9, while y can be incorrect for other f\u2217 whose shape determines the noise level in some way. \u2022 Generalized Linear Models: In generalized linear models (GLMs) (McCullagh, 1984), the transfer function is assumed to be monotonically increasing and conditional distribution of y|x belongs to exponential family. Different choices of f\u2217 give rise to different members in GLMs. If f\u2217 is identity function f\u2217(z) = z, one has the simple linear models, while the sigmoid function f\u2217(z) = 11+e\u2212z results in the logistic model for binary classification. In this work, however, we have no access to exact f\u2217 other than knowing it is monotonic. \u2022 Noise in Monotone Transfer: Instead of having the general expectation form of y as GLMs, one could directly introduce the noise inside monotone transfer f\u0303 to model the randomness of y (Plan et al., 2016),\ny = f\u0303 (\u27e8\u03b8\u2217,x\u27e9+ \u03f5) . (3)\nIn this setting, the transfer function f\u0303 is slightly different from the f\u2217 in (1), which are related by f\u2217(z) = E\u03f5[f\u0303(z + \u03f5)|z].\nA key advantage of SIM is its robustness. First, allowing unknown f\u2217 prevents the mis-specification of transfer function, which could otherwise lead to a poor estimate of \u03b8\u2217. Secondly, the model in (1) makes minimal assumption on the distribution of y, thus being able to tolerate potentially heavy-tailed noise.\nIn order to estimate \u03b8\u2217, we are given n measurements of (x, y) \u2208 Rp \u00d7 R, denoted by {(xi, yi)}ni=1. In this work, we focus on the n < p regime. In such high-dimensional setting, the recovery of \u03b8\u2217 is quite challenging as the problem is ill-posed even when f\u2217 is given. Over the last decade, substantial progress has been made to address the challenge by exploiting the apriori structure of parameter \u03b8\u2217, like sparsity (Tibshirani, 1996). For simple linear models or GLMs with known transfer, extensive studies have shown that sparse \u03b8\u2217 can be consistently estimated under mild assumptions, with much lower sample complexity than p (Candes & Tao, 2007; Wainwright, 2009; Bickel et al., 2009; Kakade et al., 2010; Negahban et al., 2012; Yang et al., 2016). Recently the notion of structure has been suitably generalized beyond the unstructured sparsity (Bach et al., 2012), and Gaussian width (Gordon, 1985) has emerged as a useful measure to characterize the structural complexity which further determines the recovery guarantee of \u03b8\u2217 (Chandrasekaran et al., 2012; Rao et al., 2012; Oymak et al., 2013; Amelunxen et al., 2014; Banerjee et al., 2014; Chatterjee et al., 2014; Vershynin, 2015; Tropp, 2015; Chen & Banerjee, 2016).\nIn the absence of exact f\u2217, though 1-bit CS and related variants were well-studied in recent years (Boufounos & Baraniuk, 2008; Jacques et al., 2013; Plan & Vershynin, 2013; Gopi et al., 2013; Zhang et al., 2014; Chen & Banerjee, 2015a; Zhu & Gu, 2015; Yi et al., 2015; Slawski & Li, 2015; Li, 2016; Slawski & Li, 2016), the exploration of general SIMs or the cases with monotone transfers is relatively limited, especially in the high-dimensional regime. Kalai & Sastry (2009) and Kakade et al. (2011) investigated the low-dimensional SIMs with monotone transfers, and they proposed perceptron-type algorithms to estimate both f\u2217 and \u03b8\u2217, with provable guarantees on prediction error. In high dimension, general SIMs were studied by Alquier & Biau (2013) and Radchenko (2015), in which only unstructured sparsity of \u03b8\u2217 is considered. The algorithm developed in (Alquier & Biau, 2013) relies on reversible jump MCMC, which could be slow. In Radchenko (2015), a path fitting algorithm is designed to recover f\u2217 and \u03b8\u2217, but only asymptotic guarantees are provided. Ganti et al. (2015) considered the high-dimensional setting with monotone transfer, and their iterative algorithm is based on non-convex optimization, for which it is hard to establish the convergence. Besides, the prediction error bound they derived is also weak (in the sense that it\nis even worse than the initialization of the algorithm). Recently Oymak & Soltanolkotabi (2016) proposed a constrained least-squares method to estimate \u03b8\u2217, with recovery error characterized by Gaussian width and related quantities. Though their analysis considered the general structure of \u03b8\u2217, it only holds for noiseless setting where y = f(\u27e8\u03b8\u2217,x\u27e9). General structure of \u03b8\u2217 was also explored in Vershynin (2015) and Plan et al. (2016). Other types of statistical guarantees for high-dimensional SIMs is also available, such as support recovery of \u03b8\u2217 in Neykov et al. (2016). It is worth noting that all the aforementioned statistical analyses rely on sub-Gaussian noise or the transfer function being bounded or Lipschitz, which indicates that none of the results can immediately hold for heavy-tailed noise (or without Lipschitzness and boundedness).\nIn this paper, we focus on the parameter estimation of \u03b8\u2217 instead of the prediction of y given new x. In particular, we propose two families of generalized estimators, constrained and regularized, for model (1) under Gaussian measurement. The parameter \u03b8\u2217 is assumed to possess certain lowcomplexity structure, which can be either captured by a constraint \u03b8\u2217 \u2208 K or a norm regularization term \u2225\u03b8\u2217\u2225. Our general approach is inspired by U -statistics and the advances in 1-bit CS, and subsumes several existing 1-bit CS algorithms as special cases. Similar to those algorithms, our estimator is simple and often admits closed-form solutions. Regarding the recovery analysis, there are two appealing aspects. First our results work for general structure, with error bound characterized by Gaussian width and some other easy-to-compute geometric measures. Instantiating our results with specific structure of \u03b8\u2217 recovers previously established error bounds for 1-bit CS (Zhang et al., 2014; Chen & Banerjee, 2015a), which are sharper than those yielded by the general analysis in Plan & Vershynin (2013). Second, our analysis works with limited assumptions on the condition distribution of y. In particular, our estimator is robust to heavy-tailed noise and permit unbounded transfer functions f\u2217 as well as non-Lipschitz ones. At the heart of our analysis is the generic chaining method (Talagrand, 2014), an advanced tool in probability theory, which has been successfully applied to sparse recovery (Koltchinskii, 2011) and dimensionality reduction (Dirksen, 2016), etc. Another key ingredient in our proof is a Hoeffding-type concentration inequality for U -statistics (Lee, 1990) with sub-Gaussian tails, which is less known yet generalizes the popular one for bounded U -statistics (Hoeffding, 1963). Apart from 1-bit CS, we particularly investigate the model (3), for which the generalized estimator is specialized in a novel way. The resulting estimator better leverages the monotonicity of the transfer function, which is also demonstrated through experiments. For the ease of exposition, whenever we say \u201cmonotone\u201d, it means \u201cmonotonically increasing\u201d by default. Throughout the\npaper, we will use c, C,C \u2032, C0, C1 and so on to denote absolute constants, which may differ from context to context. Detailed proofs are deferred to the supplementary material due to page limit.\nThe rest of the paper is organized as follows. In Section 2, we introduce our estimators for SIMs along with their recovery guarantees. We also provide a few examples in 1-bit CS for illustration. Section 3 is focused on model (3), for which we instantiate the general results in a new way. Other structures of \u03b8\u2217 beyond unstructured sparsity are also discussed. Section 4 provides the proof of our main results and the related lemmas. In Section 5, we complement our theoretical developments with some experiment results. The final section is dedicated to conclusions."}, {"heading": "2. Generalized Estimation for Structured Parameter", "text": ""}, {"heading": "2.1. Assumptions and Preliminaries", "text": "For the sake of identifiability, we assume w.l.o.g. that \u2225\u03b8\u2217\u22252 = 1 throughout the paper. At the first glimpse of model (1), we may realize that it is difficult to recover \u03b8\u2217 due to unknown f\u2217. In contrast, when f\u2217 is given, the recovery guarantees of \u03b8\u2217 can be established under mild assumptions of x and y, such as boundedness or subGaussianity. If we know certain properties of the transfer function like the monotonicity introduced in GLMs and (3), the structure of f\u2217 is largely restricted, and it is tempting to expect that similar results will continue to hold. Unfortunately, we first have the following claim, which indicates that without other constraints on f\u2217 beyond strict monotonicity, \u03b8\u2217 cannot be consistently estimated under general sub-Gaussian (or bounded) measurement, even in the noiseless setting of (3).\nClaim 1 Suppose that each element xi of x is sampled i.i.d. from Rademacher distribution, i.e., P(xi = 1) = P(xi = \u22121) = 0.5. Under model (3) with noise \u03f5 = 0, there exists a \u03b8\u0304 \u2208 Sp\u22121 together with a monotone f\u0304 , such that supp(\u03b8\u0304) = supp(\u03b8\u2217) and yi = f\u0304(\u27e8\u03b8\u0304,xi\u27e9) for data {(xi, yi)}ni=1 with arbitrarily large sample size n, while \u2225\u03b8\u0304 \u2212 \u03b8\u2217\u22252 > \u03b4 for some constant \u03b4.\nNow that consistent estimation of \u03b8\u2217 is not possible for general sub-Gaussian measurement, it might be reasonable to focus on certain special cases. For this work, we assume that x is standard Gaussian N (0, I). For SIM (1), we additionally assume that the distribution of y depends on x only through the value of \u27e8\u03b8\u2217,x\u27e9, i.e., the distribution of y|x is fixed if \u27e8\u03b8\u2217,x\u27e9 is given (no matter what the exact x is). This assumption is quite minimal, and it turns out that the examples we provide in Section 1 all satisfy it (if noise \u03f5 is independent of x in (3)). The same assumption is used\nin Plan et al. (2016) as well.\nUnder the assumptions above, given m i.i.d. observations (x1, y1), . . . , (xm, ym), we define\nu ((x1, y1), . . . , (xm, ym)) = m\u2211 i=1 qi (y1, . . . , ym) \u00b7 xi ,\n(4) where all qi : Rm 7\u2192 R are bounded functions with |qi| \u2264 1, which are chosen along with m based on the properties of the transfer function. In Section 2.4 and 3.1, we will see examples for their choices. The vector u \u2208 Rp is critical due to the key observation below.\nLemma 1 Suppose the distribution of y in model (1) depends on x through \u27e8\u03b8\u2217,x\u27e9 and we define accordingly\nbi (z1, . . . , zm;\u03b8 \u2217) = (5)\nE [qi (y1, . . . , ym) |\u27e8\u03b8\u2217,x1\u27e9 = z1, . . . , \u27e8\u03b8\u2217,xm\u27e9 = zm] .\nWith x being standard Gaussian N (0, I), u defined in (4) satisfies\nE [u ((x1, y1), . . . , (xm, ym))] = \u03b2\u03b8\u2217 , (6)\nwhere \u03b2 = \u2211m i=1 E[bi (g1, . . . , gm;\u03b8\u2217) \u00b7 gi], and g1, . . . , gm are i.i.d. standard Gaussian.\nNote that Lemma 1 is true for all choices of qi, and the proof is given in the supplement. This lemma presents an insight towards the design of our estimator, that is, the direction of \u03b8\u2217 can be approximated if we have a good sense about Eu. As we will see in the sequel, the scalar \u03b2 plays a key role in the estimation error bound, which can give us clues to the choice of qi. We can assume w.l.o.g. that \u03b2 \u2265 0 since we can flip the sign of each qi.\nThe recovery analysis is built on the notion of Gaussian width (Gordon, 1985), which is defined for any A \u2286 Rp as w(A) = E[supv\u2208A\u27e8g,v\u27e9], where g is a standard Gaussian random vector. Roughly speaking, w(A) measures the scaled width of set A averaged over each direction."}, {"heading": "2.2. Generalized Estimator", "text": "Inspired by Lemma 1, we define the vector u\u0302 for the observed data {(xi, yi)}ni=1,\nu\u0302 = (n\u2212m)!\nn!\n\u2211 1\u2264i1,...,im\u2264n i1 \u0338=... \u0338=im u ((xi1 , yi1), . . . , (xim , yim)) ,\n(7) which is an unbiased estimator of Eu, meaning that Eu\u0302 = Eu = \u03b2\u03b8\u2217. When m = 2, we essentially have\nu\u0302 = 1 n(n\u2212 1) \u2211\n1\u2264i,j\u2264n i \u0338=j\nu ((xi, yi), (xj , yj)) (8)\nIn fact, u\u0302 can be treated as a vector version of U -statistics with order m. Given u\u0302, a naive way to estimate \u03b8\u2217 is to simply normalize u\u0302, i.e., \u03b8\u0302 = u\u0302/\u2225u\u0302\u22252. In highdimensional setting, \u03b8\u2217 is often structured, but the naive estimator fails to take such information into account, which would lead to large error. To incorporate the prior knowledge on \u03b8\u2217, we design two types of estimator, the constrained one and the regularized one.\nConstrained Estimator: If we assume that \u03b8\u2217 belongs to some structured set K \u2286 Sp\u22121, then the estimation of \u03b8\u2217 is carried out via the constrained optimization\n\u03b8\u0302 = argmin \u03b8\u2208Rp\n\u2212 \u27e8u\u0302,\u03b8\u27e9 s.t. \u03b8 \u2208 K . (9)\nHere the set K can be non-convex, as long as the optimization can be solved globally. Since the objective function is very simple, we can often end up with a global minimizer. Similar estimator has been used in Plan et al. (2016), but they only focused on specific u\u0302.\nRegularized Estimator: If we assume that the structure of \u03b8\u2217 can be captured by certain norm \u2225 \u00b7 \u2225, we may alternatively use the regularized estimator to find \u03b8\u2217,\n\u03b8\u0302 = argmin \u03b8\u2208Rp\n\u2212 \u27e8u\u0302,\u03b8\u27e9+ \u03bb\u2225\u03b8\u2225 s.t. \u2225\u03b8\u22252 \u2264 1 . (10)\nThe optimization is convex, thus the global minimum is always attained. Previously this estimator was used in 1-bit CS scenario with L1 norm (Zhang et al., 2014)."}, {"heading": "2.3. Recovery Analysis", "text": "Regarding the constrained estimator, the recovery of \u03b8\u2217 relies on the geometry of \u03b8\u0302, which is described by\nAK(\u03b8\u2217) = cone { v \u2223\u2223\u2223 v = \u03b8\u0302 \u2212 \u03b8\u2217, \u03b8\u0302 \u2208 K} \u2229 Sp\u22121\n(11) The set AK(\u03b8\u2217) essentially contains all possible directions that error \u03b8\u0302 \u2212 \u03b8\u2217 could lie in. The following theorem characterizes the error of \u03b8\u0302.\nTheorem 1 Suppose that the optimization (9) can be solved to global minimum. Then the following error bound holds for the minimizer \u03b8\u0302 with probability at least 1 \u2212 C \u2032\u2032 exp ( \u2212w2 (AK(\u03b8\u2217)) ) ,\n\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u2225 2 \u2264 C\u03bam 3 2 \u03b2 \u00b7 w(AK(\u03b8 \u2217)) + C \u2032\u221a n , (12)\nwhere \u03ba is the sub-Gaussian norm of a standard Gaussian random variable, and C, C \u2032, C \u2032\u2032 are all absolute constant.\nRemark: Note that estimator is consistent as long as \u03b2 \u0338= 0. The error bound inversely depends on the scale of \u03b2,\nwhich implies that we should construct suitable qi such that \u03b2 is large according to its definition in Lemma 1. The choice of qi further depends on the assumed property of f\u2217. Though dependency on m may prevent us from using higher-order u, m is typically small in practice and can be treated as constant.\nFor regularized estimator, we can similarly establish the recovery guarantee in terms of Gaussian width.\nTheorem 2 Define the following set for any \u03c1 > 1, A\u03c1 (\u03b8\u2217) = cone { v \u2223\u2223\u2223 \u2225v + \u03b8\u2217\u2225 \u2264 \u2225\u03b8\u2217\u2225+ \u2225v\u2225\n\u03c1\n} \u2229 Sp\u22121\nIf we set \u03bb = \u03c1 \u2225u\u0302\u2212 \u03b2\u03b8\u2217\u2225\u2217 = O(\u03c1m3/2w(B\u2225\u00b7\u2225)/ \u221a n) and it satisfies \u03bb < \u2225u\u0302\u2225\u2217, then with probability at least 1\u2212 C \u2032 exp ( \u2212w2 ( B\u2225\u00b7\u2225\n)) , \u03b8\u0302 in (10) satisfies\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u2225\n2 \u2264 C(1 + \u03c1)\u03bam\n3 2 \u03b2 \u00b7 \u03a8(A\u03c1(\u03b8\u2217)) \u00b7 w\n( B\u2225\u00b7\u2225 ) \u221a n ,\n(13) where \u03a8(A\u03c1(\u03b8\u2217)) = supv\u2208A\u03c1(\u03b8\u2217) \u2225v\u2225 and B\u2225\u00b7\u2225 = {v | \u2225v\u2225 \u2264 1} is the unit ball of norm \u2225 \u00b7 \u2225.\nRemark: The geometry of the regularized estimator is slightly different from the constrained one. Instead of having AK(\u03b8\u2217), here the set A\u03c1(\u03b8\u2217) depends on the choice of the regularization parameter \u03bb. The same phenomenon also appears in the (Banerjee et al., 2014). The geometric measure \u03a8(A\u03c1(\u03b8\u2217)) is called restricted norm compatibility, which is non-random. For many interesting cases, it is easy to calculate (Negahban et al., 2012; Chen & Banerjee, 2015b)."}, {"heading": "2.4. Application to 1-bit CS", "text": "For 1-bit CS problem (2), the u defined in (4) can be chosen with m = 1 and qi = yi, ending up with\nu ((x, y)) = yx and u\u0302 = 1\nn n\u2211 i=1 yixi . (14)\nBy such choice of u, the \u03b2 defined in Lemma 1 is simply \u03b2 = E[f\u2217(g)g] with g being standard Gaussian random vector. Under reasonably mild noise, y is likely to take the sign of the linear measurement, which means that f\u2217(g) should be close to 1 (or -1) if g is positive (or negative). Thus we expect f\u2217(g)g to be positive most of time and \u03b2 to be large. Given the choice of u, we can specialize our generalized constrained/regularized estimator to obtain previous results. If \u03b8\u2217 is assumed to be s-sparse, for constrained estimator, we can choose a straightforward K = {\u03b8 | \u2225\u03b8\u22250 \u2264 s}\u2229Sp\u22121, which results in the k-support norm estimator (Chen & Banerjee, 2015a),\n\u03b8\u0302ks = argmin \u03b8\u2208Rp \u2212 \u27e8u\u0302,\u03b8\u27e9 s.t. \u2225\u03b8\u22250 \u2264 s, \u2225\u03b8\u22252 = 1 (15)\nThough K is non-convex, the global minimizer can actually be obtained in closed form,\n\u03b8\u0302ksj =\n{ u\u0302j / \u2225|u\u0302|\u21931:s\u22252 , if |u\u0302j | is in |u\u0302| \u2193 1:s\n0 , otherwise (16)\nwhere |u\u0302|\u2193 is the absolute-value counterpart of u\u0302 with entries sorted in descending order, and the subscript takes the top s entries. Similarly if the regularized estimator is instantiated with L1 norm \u2225 \u00b7 \u22251, we obtain the so-called passive algorithm introduced in Zhang et al. (2014),\n\u03b8\u0302ps = argmin \u03b8\u2208Rp \u2212 \u27e8u\u0302,\u03b8\u27e9+ \u03bb\u2225\u03b8\u22251 s.t. \u2225\u03b8\u22252 \u2264 1 , (17)\nwhose solution is given by \u03b8\u0302ps = S (u\u0302, \u03bb) /\u2225S (u\u0302, \u03bb) \u22252, where S(\u00b7, \u00b7) is the elementwise soft-thresholding operator, Si(u\u0302, \u03bb) = max{sign(u\u0302i)(|u\u0302i|\u2212\u03bb), 0}. Based on Theorem 1 and 2, we can easily obtain the error bound for both ksupport norm estimator and passive algorithm.\nCorollary 1 Assume that {(xi, yi)}ni=1 follow 1-bit CS model in (2) and u\u0302 is given as (14). For any s-sparse \u03b8\u2217, with high probability, \u03b8\u0302 produced by both (15) and (17) (i.e., \u03b8\u0302ks and \u03b8\u0302ps) satisfy\n\u2225\u2225\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u2225\u2225\u2225 2 \u2264 O (\u221a s log p n ) (18)\nThe proof is included in the supplementary material. The above result was shown by Slawski & Li (2015) and Zhang et al. (2014), but their analyses do not consider the general structure. Compared with O( 4 \u221a s log p/n) yielded by the general result in Plan & Vershynin (2013), our bound is much sharper."}, {"heading": "3. A New Estimator for Monotone Transfer", "text": "In this section, we specifically study model (3). Here we further assume that f\u0303 is strictly increasing. What is worth mentioning is that the estimator we develop here can be applied to GLMs as well. To avoid the confusion with u and u\u0302 defined previously, we instead use new notations h and h\u0302 respectively in this section."}, {"heading": "3.1. Estimator with Second-Order h\u0302", "text": "To motivate the design of h, it is helpful to rewrite model (3) by applying the inverse of f\u0303 on both sides,\nf\u0303\u22121(y) = \u27e8\u03b8\u2217,x\u27e9+ \u03f5 . (19)\nNote that the new formulation resembles the linear model except that we have no access to the value of f\u0303\u22121(y). Instead, all we know about r = [f\u0303\u22121(y1), . . . , f\u0303\u22121(yn)]T \u2208 Rn is that it preserves the ordering of y = [y1, . . . , yn]T .\nPut in another way, r needs to satisfy the constraint that ri > rj iff. yi > yj and ri < rj iff. yi < yj . To move one step further, it is equivalent to sign(yi \u2212 yj) = sign(ri\u2212rj) = sign(\u27e8\u03b8\u2217,xi\u2212xj\u27e9+\u03f5i\u2212\u03f5j) based on model assumption. Hence the information contained in sample {(xi, yi)}ni=1 can be interpreted from the perspective of 1- bit CS, where sign(yi \u2212 yj) reflects the perturbed sign of linear measurement \u27e8\u03b8\u2217,xi \u2212 xj\u27e9. Inspired by the u for 1-bit CS, we may choose m = 2 and define h, h\u0302 as\nh ((x1, y1), (x2, y2)) = sign(y1 \u2212 y2) \u00b7 (x1 \u2212 x2) , (20)\nh\u0302 = 1 n(n\u2212 1) \u2211\n1\u2264i,j\u2264n i \u0338=j\nh ((xi, yi), (xj , yj)) , (21)\nGiven the definition of h\u0302, Lemma 1 directly implies the following corollary.\nCorollary 2 Suppose that (x1, y2) and (x2, y2) are generated by model (3), where x1,x2 follow Gaussian distribution N (0, I), and the noise \u03f51, \u03f52 are independent of x1,x2 and identically (but arbitrarily) distributed. Then the expectation of h ((x1, y1), (x2, y2)) satisfies\nE [h ((x1, y1), (x2, y2))] = \u221a 2\u03b2\u2032\u03b8\u2217 , (22)\nwhere \u03b2\u2032 = Eg\u223cN (0,1) [ sign ( g + (\u03f51 \u2212 \u03f52)/ \u221a 2 ) \u00b7 g ] .\nRemark: The scalar \u221a 2\u03b2\u2032 serves as the role of \u03b2 in Lemma 1, and \u03b2\u2032 is always guaranteed to be strictly positive regardless how the noise is distributed, which keeps \u03b8\u2217 distinguishable all the time. To see this, let \u03be = (\u03f51\u2212 \u03f52)/ \u221a 2. Note that \u03be is symmetric, thus \u03b5\u03be has the same distribution as \u03be, where \u03b5 is a Rademacher random variable. Therefore\n\u03b2\u2032 = E [sign (g + \u03be) \u00b7 g] = Eg,\u03beE\u03b5 [sign (g + \u03b5\u03be) \u00b7 g] = E\u03beEg [\nsign (g \u2212 \u03be) + sign (g + \u03be) 2\n\u00b7 g ]\nSince g(g \u2212 \u03be) + g(g + \u03be) = 2g2 \u2265 0, it follows that sign(g(g\u2212 \u03be))+ sign(g(g+ \u03be)) = (sign(g\u2212 \u03be)+ sign(g+ \u03be)) \u00b7 sign(g) \u2265 0, thus (sign(g \u2212 \u03be) + sign(g + \u03be)) \u00b7 g is always nonnegative. Find a large enough M > 0 such that P(|\u03be| \u2264 M) = 0.5 > 0, and we have\n\u03b2\u2032 = E [sign (g + \u03be) \u00b7 g] \u2265 E\u03beEg [|g| \u00b7 I{|g| > |\u03be|}]\n\u2265 0.5Eg [|g| \u00b7 I{|g| > M}] = M\n2 \u00b7 P(|g| > M) > 0 .\nIn the ideal noiseless case, \u03b2\u2032 achieve its maximum, \u03b2\u2032max = E[sign(g)g] = E[|g|] = \u221a 2/\u03c0. In the worst case, if \u03f51 and \u03f52 are heavy-tailed and dominate g, then \u03b2\u2032 \u2248 E [ sign ( (\u03f51 \u2212 \u03f52)/ \u221a 2 ) \u00b7 g ] \u2248 0.\nNow we can instantiate the generalized estimator based on h\u0302. For example, if \u03b8\u2217 is s-sparse, we estimate it by\n\u03b8\u0302 = argmin \u03b8\u2208Rp\n\u2212 \u27e8h\u0302,\u03b8\u27e9 s.t. \u2225\u03b8\u22250 \u2264 s, \u2225\u03b8\u22252 = 1 (23)\nwhich enjoys O (\u221a s log p/n )\nerror rate as shown in Corollary 1. The regularized estimator can also be obtained with the same h\u0302 according to (17). The bottleneck of computing \u03b8\u0302 lies in the calculation of h\u0302. A simple proposition below enables us to get h\u0302 in a fast manner.\nProposition 1 Given {(xi, yi)}ni=1, let \u03c0\u2193 be the permutation of {1, . . . , n} such that y\u03c0\u21931 > y\u03c0\u21932 > . . . > y\u03c0\u2193n . Then we have\nh\u0302 = 2\nn(n\u2212 1) n\u2211 i=1 (n+ 1\u2212 2i) \u00b7 x\u03c0\u2193i (24)\nRemark: Based on the proposition above, h\u0302 can be efficiently computed in O(np+ n log n) time, i.e., O(n log n) time for sorting y and O(np) time for the weighted sum of all xi. This is a significant improvement compared with the the naive calculation using (21), which takes O(n2p) time."}, {"heading": "3.2. Beyond Unstructured Sparsity", "text": "So far we have illustrated the Gaussian width based error bounds, viz (12) and (13), only through unstructured sparsity of \u03b8\u2217. Here we provide two more examples, nonoverlapping group sparsity and fused sparsity.\nNon-Overlapping Group Sparsity: Suppose the coordinates of \u03b8\u2217 has been partitioned into K predefined disjoint groups G1, . . . ,GK \u2286 {1, 2, . . . , p}, out of which only k groups are non-zero. If we use the regularized estimator with L2,1 norm \u2225\u03b8\u22252,1 = \u2211K i=1 \u2225\u03b8Gi\u22252, the optimal solution can be similarly obtained as (17), with elementwise soft-thresholding replaced by the groupwise one. The related geometric measures that appears in (13) can be found in Banerjee et al. (2014), which are given by\n\u03a8(A\u03c1(\u03b8\u2217)) \u2264 O( \u221a k) (25)\nw ( B\u2225\u00b7\u22252,1 ) \u2264 O( \u221a logK + \u221a G) (26)\nFused Sparsity: \u03b8\u2217 is said to be s-fused-sparse if the cardinality of the set F(\u03b8\u2217) = {1 \u2264 i < p | \u03b8\u2217i \u0338= \u03b8\u2217i+1} is smaller than s. If we resort to the constrained estimator (9) with K = {\u03b8 | |F(\u03b8)| \u2264 s, \u2225\u03b8\u22252 = 1}, the associated optimization can be solved by dynamic programming (Bellman, 1961). The proposition below upper bounds the corresponding Gaussian width w(AK(\u03b8\u2217)) in (12).\nProposition 2 For s-fused-sparse \u03b8\u2217, the Gaussian width of set AK(\u03b8\u2217) with K = {\u03b8 | |F(\u03b8)| \u2264 s, \u2225\u03b8\u22252 = 1} satisfies\nw(AK(\u03b8\u2217)) \u2264 O( \u221a s log p) (27)\nThe proof can be found in (Slawski & Li, 2016), and we provide a different one in supplementary material."}, {"heading": "4. Lemmas and Proof Sketch of Theorem 1", "text": "Here we first present the important technical lemmas that will be used in the proof of Theorem 1. The first one is the Hoeffding-type inequality for sub-Gaussian U -statistics. In the literature, most of the studies are centered around bounded U -statistics, for which the celebrated concentration is established by Hoeffding (1963). Yet it is not easy to locate the counterpart for sub-Gaussian case. Therefore we provide the following result and attach a proof in the supplementary material.\nLemma 2 (Concentration for sub-Gaussian U -statistics) Define the U -statistic\nUn,m(h) = (n\u2212m)!\nn!\n\u2211 1\u2264i1,...,im\u2264n i1 \u0338=i2 \u0338=... \u0338=im h (zi1 , . . . , zim) (28)\nwith order m and kernel h : Rd\u00d7m 7\u2192 R based on n independent copies of random vector z \u2208 Rd, denoted by z1, \u00b7 \u00b7 \u00b7 , zn. If h(\u00b7, . . . , \u00b7) is sub-Gaussian with \u2225h\u2225\u03c82 \u2264 \u03ba, then the following inequality holds for Un,m(h) with any \u03b4 > 0,\nP (|Un,m(h)\u2212 EUn,m(h)| > \u03b4) \u2264 2 exp ( \u2212C \u230a n m \u230b \u00b7 \u03b4 2 \u03ba2 ) , (29) in which C is an absolute constant.\nAs mentioned earlier in Section 1, generic chaining is the key tool that our analysis relies on. Specifically we utilize Theorem 2.2.27 from (Talagrand, 2014).\nLemma 3 (Generic chaining concentration) Given metric space (T , s), if an associated stochastic process {Zt}t\u2208T has sub-Gaussian incremental, i.e., satisfies the condition\nP (|Zu \u2212 Zv| \u2265 \u03b4) \u2264 C exp ( \u2212 C \u2032\u03b42\ns2(u,v)\n) , \u2200 u,v \u2208 T ,\n(30) then the following inequality holds\nP (\nsup u,v\u2208T\n|Zu \u2212 Zv| \u2265 C1 (\u03b32(T , s) + \u03b4 \u00b7 diam (T , s)) )\n\u2264 C2 exp ( \u2212\u03b42 ) , (31)\nwhere C,C \u2032, C1 and C2 are all absolute constants.\nThe definition of the above \u03b32-functional \u03b32(\u00b7, \u00b7) is complicated, and is not of great importance. We refer interested\nreader to the books, Talagrand (2005; 2014). Loosely speaking, \u03b32(T , s) can be thought of as a measure for the size of set T under metric s. What really matters is the following relationship between \u03b32-functional and Gaussian width. (see Theorem 2.4.1 in Talagrand (2014))\nLemma 4 (Majorizing measures theorem) For any set T \u2286 Rp, the \u03b32-functional w.r.t. L2-metric and Gaussian width satisfy the following inequality with an absolute constant C0,\n\u03b32 (T , \u2225 \u00b7 \u22252) \u2264 C0 \u00b7 w(T ) (32)\nEquipped with these lemmas, we are ready to present the proof sketch of Theorem 1. A complete proof is deferred to the supplementary material.\nProof Sketch of Theorem 1: We use the shorthand notation AK for the set AK(\u03b8\u2217). As \u03b8\u0302 attains the global minimum of (9), we have\n\u27e8\u03b8\u0302 \u2212 \u03b8\u2217, u\u0302\u27e9 \u2265 0 \u21d0\u21d2 \u27e8 \u03b8\u0302 \u2212 \u03b8\u2217, u\u0302\n\u03b2 \u2212 \u03b8\u2217 + \u03b8\u2217\n\u27e9 \u2265 0\n=\u21d2 \u27e8\u03b8\u0302,\u03b8\u2217\u27e9 \u2265 1\u2212 \u2225\u03b8\u0302 \u2212 \u03b8\u2217\u22252 \u00b7 sup v\u2208AK\u222a{0}\n\u27e8 v, u\u0302\n\u03b2 \u2212 \u03b8\u2217 \u27e9 In order to bound the supremum above, we use the result from generic chaining. We define the stochastic process {Zv = \u27e8v, u\u0302/\u03b2 \u2212 \u03b8\u2217\u27e9}v\u2208AK\u222a{0}. First, we need to check the process has sub-Gaussian incremental. For simplicity, we denote u ((xi1 , yi1), . . . , (xim , yim)) by ui1,...,im . By the definitions and properties of sub-Gaussian norm (Vershynin, 2012), it is not difficult to show that \u2225\u27e8ui1,...,im ,v \u2212w\u27e9\u2225\u03c82 \u2264 \u03bam \u00b7 \u2225v \u2212w\u22252 for any v,w \u2208 AK \u222a {0}. By Lemma 2, we have\nP (|Zv \u2212 Zw| > \u03b4) \u2264 2 exp ( \u2212C \u2032 \u00b7 n\u03b2 2\u03b42\nm3\u03ba2 \u00b7 \u2225v \u2212w\u222522\n) .\nTherefore we can conclude that {Zv} has sub-Gaussian incremental w.r.t. the metric s(v,w) , \u03bam 32 \u00b7 \u2225v \u2212 w\u22252/\u03b2 \u221a n. Now applying Lemma 3 to {Zv} with a bit calculation, we can obtain\nP (\nsup v\u2208AK\u222a{0}\n|Zv| \u2265 C1\u03bam\n3 2\n\u03b2 \u221a n\n\u00b7 ( \u03b32 (AK \u222a {0}, \u2225 \u00b7 \u22252)\n+ 2\u03b4 )) \u2264 C2 exp ( \u2212\u03b42 ) Using \u03b32 (AK \u222a {0}, \u2225 \u00b7 \u22252) \u2264 C0 \u00b7w (AK \u222a {0}) implied by Lemma 4 and taking \u03b4 = w (AK \u222a {0}), we get\nsup v\u2208AK\u222a{0}\n\u27e8 v, u\u0302\n\u03b2 \u2212 \u03b8\u2217\n\u27e9 \u2264 C3\u03bam 3 2\n\u03b2 \u00b7 w (AK) + C4\u221a n\nwith probability at least 1 \u2212 C2 exp ( \u2212w2 (AK) ) . The inequality also uses the fact that w (AK \u222a {0}) \u2264 w (AK) +\nC4, which is a result of Lemma 2 in Maurer et al. (2014) (See Lemma A in supplementary material). Lastly we turn to the quantity \u2225\u03b8\u0302 \u2212 \u03b8\u2217\u22252,\n\u2225\u03b8\u0302 \u2212 \u03b8\u2217\u22252 \u2264 \u221a 2\u2212 2\u27e8\u03b8\u0302,\u03b8\u2217\u27e9 \u2264 2C3\u03bam 3 2\n\u03b2 \u00b7 w (AK) + C4\u221a n .\nWe finish the proof by letting C = 2C3, C \u2032 = C4 and C \u2032\u2032 = C2."}, {"heading": "5. Experiment", "text": "In the experiment, we focus on model (3) with sparse \u03b8\u2217. The problem dimension is fixed as p = 1000, and the sparsity of \u03b8\u2217 is set to 10. Essentially we generate our data (x, y) from\ny = f\u0303 (\u27e8\u03b8\u2217,x\u27e9+ \u03f5) ,\nwhere x \u223c N (0, I) and \u03f5 \u223c N (0, \u03c32). \u03c3 ranges from 0.3 to 1.5. We choose three monotonically increasing f\u0303 , f\u0303(z) = 1/(1 + exp(\u2212z)) (which is bounded and Lipschitz), f\u0303(z) = z3 (which is unbounded and non-Lipschitz), and f\u0303(z) = log(1 + exp(z)) (which is unbounded but Lipschitz). The sample size n varies from 200 to 1000. We use the estimator (23) in Section 3. The baselines we compare with is the SILO and iSILO algorithm introduced in (Ganti et al., 2015). SILO does not quite take the monotonicity in account. In fact, it is the special case of our generalized constrained estimator which uses the same choice of u as 1-bit CS. The original SILO use the constraint set {\u03b8 | \u2225\u03b8\u22251 \u2264 \u221a s, \u2225\u03b8\u22252 \u2264 1}, which is computationally less efficient and statistically no better than K = {\u03b8 | \u2225\u03b8\u22250 \u2264 s} \u2229 Sp\u22121 (Zhang et al., 2014; Chen & Banerjee, 2015a). Hence we also use K in SILO for a fair comparison. iSILO relies on a specific implementation of isotonic regression which explicitly restricts the Lipschitz constant of f\u0303 to be one. To fit iSILO into our setting, we remove the Lipschitzness constraint and perform the standard isotonic regression. Since the convergence is not guaranteed for the iterative procedure of iSILO, the number of its iterations is fixed to 100. The best tuning parameter of iSILO is obtained by grid search.\nThe experiment results are shown in Figure 1. Overall the iSILO algorithm works well under small noise, while our estimator has better performance when the variance of noise increases. To better demonstrate the robustness of our estimator to heavy-tailed noise, instead of Gaussian noise, we sample \u03f5 from the Student\u2019s t distribution with degrees of freedom equal to 3. We repeat the experiments for f\u0303(z) = z3, and obtain the plots in Figure 2. We can see that the error of our estimator consistently decreases for all choice of \u03c3 as n increases. For SILO and iSILO, the errors are relatively large, and unable to shrink for large \u03c3 even when more data are provided."}, {"heading": "6. Conclusion", "text": "In this paper, we study the parameter estimation for the high-dimensional single-index models. We propose two classes of robust estimators, which generalize previous works in two aspects. First we allow the diverse structure (e.g., binary, monotone and etc.) of the transfer function, which can help us customize the estimators. Secondly the structure of the true parameter can be general, either encoded by a constraint or a norm. With limited assumption on the noise, we can show that the estimation error can be bounded by simple geometric measures under Gaussian\nmeasurement, which subsumes the existing results for specific settings. The experiment results also validate our theoretical analyses."}, {"heading": "Acknowledgements", "text": "We thank Sreangsu Acharyya for helpful discussions related to the paper. The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS1422557, CCF-1451986, CNS- 1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo."}], "year": 2017, "references": [{"title": "Sparse single-index model", "authors": ["P. Alquier", "G. Biau"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Living on the edge: Phase transitions in convex programs with random data", "authors": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Inform. Inference,", "year": 2014}, {"title": "Estimation with norm regularization", "authors": ["A. Banerjee", "S. Chen", "F. Fazayeli", "V. Sivakumar"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "On the approximation of curves by line segments using dynamic programming", "authors": ["R. Bellman"], "venue": "Communications of the ACM,", "year": 1961}, {"title": "Simultaneous analysis of Lasso and Dantzig selector", "authors": ["P.J. Bickel", "Y. Ritov", "A.B. Tsybakov"], "venue": "The Annals of Statistics,", "year": 2009}, {"title": "1-bit compressive sensing", "authors": ["Boufounos", "P. T", "R.G. Baraniuk"], "venue": "In Information Sciences and Systems,", "year": 2008}, {"title": "The Dantzig selector: statistical estimation when p is much larger than n", "authors": ["E. Candes", "T. Tao"], "venue": "The Annals of Statistics,", "year": 2007}, {"title": "The convex geometry of linear inverse problems", "authors": ["V. Chandrasekaran", "B. Recht", "P.A. Parrilo", "A.S. Willsky"], "venue": "Foundations of Computational Mathematics,", "year": 2012}, {"title": "Generalized dantzig selector: Application to the k-support norm", "authors": ["S. Chatterjee", "S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2014}, {"title": "One-bit compressed sensing with the k-support norm", "authors": ["S. Chen", "A. Banerjee"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2015}, {"title": "Structured estimation with atomic norms: General bounds and applications", "authors": ["S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Structured matrix recovery via the generalized dantzig selector", "authors": ["S. Chen", "A. Banerjee"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Dimensionality reduction with subgaussian matrices: A unified theory", "authors": ["S. Dirksen"], "venue": "Foundations of Computational Mathematics,", "year": 2016}, {"title": "Learning single index models in high dimensions", "authors": ["R. Ganti", "N. Rao", "Willett", "R. M", "R. Nowak"], "venue": "arXiv preprint arXiv:1506.08910,", "year": 2015}, {"title": "One-bit compressed sensing: Provable support and vector recovery", "authors": ["S. Gopi", "P. Netrapalli", "P. Jain", "A. Nori"], "venue": "In Proceedings of The 30th International Conference on Machine Learning,", "year": 2013}, {"title": "Some inequalities for gaussian processes and applications", "authors": ["Y. Gordon"], "venue": "Israel Journal of Mathematics,", "year": 1985}, {"title": "Probability inequalities for sums of bounded random variables", "authors": ["W. Hoeffding"], "venue": "Journal of the American statistical association,", "year": 1963}, {"title": "Direct semiparametric estimation of single-index models with discrete covariates", "authors": ["J.L. Horowitz", "W. Hardle"], "venue": "Journal of the American Statistical Association,", "year": 1996}, {"title": "Semiparametric least squares (sls) and weighted sls estimation of single-index models", "authors": ["H. Ichimura"], "venue": "Journal of Econometrics,", "year": 1993}, {"title": "Robust 1-bit compressive sensing via binary stable embeddings of sparse vectors", "authors": ["L. Jacques", "Laska", "J. N", "Boufounos", "P. T", "R.G. Baraniuk"], "venue": "IEEE Transactions on Information Theory,", "year": 2013}, {"title": "Learning exponential families in high-dimensions: Strong convexity and sparsity", "authors": ["S. Kakade", "O. Shamir", "K. Sindharan", "A. Tewari"], "venue": "In Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics,", "year": 2010}, {"title": "Efficient learning of generalized linear and single index models with isotonic regression", "authors": ["Kakade", "S. M", "V. Kanade", "O. Shamir", "A. Kalai"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "The isotron algorithm: Highdimensional isotonic regression", "authors": ["Kalai", "A. T", "R. Sastry"], "venue": "In COLT,", "year": 2009}, {"title": "Oracle Inequalities in Empirical Risk Minimization and Sparse Recovery Problems. Lecture Notes in Mathematics", "authors": ["V. Koltchinskii"], "year": 2011}, {"title": "One scan 1-bit compressed sensing", "authors": ["P. Li"], "venue": "In Proceedings of the 19th International Conference on Artificial Intelligence and Statistics,", "year": 2016}, {"title": "An Inequality with Applications to Structured Sparsity and Multitask Dictionary Learning", "authors": ["A. Maurer", "M. Pontil", "B. Romera-Paredes"], "venue": "In Conference on Learning Theory (COLT),", "year": 2014}, {"title": "Generalized linear models", "authors": ["P. McCullagh"], "venue": "European Journal of Operational Research,", "year": 1984}, {"title": "A unified framework for the analysis of regularized M -estimators", "authors": ["S. Negahban", "P. Ravikumar", "M.J. Wainwright", "B. Yu"], "venue": "Statistical Science,", "year": 2012}, {"title": "L1-regularized least squares for support recovery of high dimensional single index models with gaussian designs", "authors": ["M. Neykov", "J.S. Liu", "T. Cai"], "venue": "J. Mach. Learn. Res.,", "year": 2016}, {"title": "Fast and reliable parameter estimation from nonlinear observations", "authors": ["S. Oymak", "M. Soltanolkotabi"], "venue": "arXiv preprint arXiv:1610.07108,", "year": 2016}, {"title": "The squared-error of generalized lasso: A precise analysis", "authors": ["S. Oymak", "C. Thrampoulidis", "B. Hassibi"], "venue": "In Communication, Control, and Computing (Allerton),", "year": 2013}, {"title": "Robust 1-bit compressed sensing and sparse logistic regression: A convex programming approach", "authors": ["Y. Plan", "R. Vershynin"], "venue": "IEEE Transactions on Information Theory,", "year": 2013}, {"title": "Highdimensional estimation with geometric constraints", "authors": ["Y. Plan", "R. Vershynin", "E. Yudovina"], "venue": "Information and Inference,", "year": 2016}, {"title": "High dimensional single index models", "authors": ["P. Radchenko"], "venue": "Journal of Multivariate Analysis,", "year": 2015}, {"title": "Universal Measurement Bounds for Structured Sparse Signal Recovery", "authors": ["N. Rao", "B. Recht", "R. Nowak"], "venue": "In International Conference on Artificial Intelligence and Statistics (AISTATS),", "year": 2012}, {"title": "b-bit marginal regression", "authors": ["M. Slawski", "P. Li"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Linear signal recovery from bbit-quantized linear measurements: precise analysis of the trade-off between bit depth and number of measurements", "authors": ["M. Slawski", "P. Li"], "venue": "arXiv preprint arXiv:1607.02649,", "year": 2016}, {"title": "The Generic Chaining", "authors": ["M. Talagrand"], "year": 2005}, {"title": "Upper and Lower Bounds for Stochastic Processes", "authors": ["M. Talagrand"], "year": 2014}, {"title": "Regression shrinkage and selection via the Lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society, Series B,", "year": 1996}, {"title": "Convex recovery of a structured signal from independent random linear measurements", "authors": ["J.A. Tropp"], "venue": "In Sampling Theory, a Renaissance", "year": 2015}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["R. Vershynin"], "venue": "Compressed Sensing,", "year": 2012}, {"title": "Estimation in High Dimensions: A Geometric Perspective, pp. 3\u201366", "authors": ["R. Vershynin"], "venue": "Springer International Publishing,", "year": 2015}, {"title": "Sharp thresholds for noisy and highdimensional recovery of sparsity using l1-constrained quadratic programming(Lasso)", "authors": ["M.J. Wainwright"], "venue": "IEEE Transactions on Information Theory,", "year": 2009}, {"title": "Sparse nonlinear regression: Parameter estimation under nonconvexity", "authors": ["Z. Yang", "Z. Wang", "H. Liu", "Y.C. Eldar", "T. Zhang"], "venue": "In Proceedings of the 33nd International Conference on Machine Learning,", "year": 2016}, {"title": "Optimal linear estimation under unknown nonlinear transform", "authors": ["X. Yi", "Z. Wang", "C. Caramanis", "H. Liu"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Efficient algorithms for robust one-bit compressive sensing", "authors": ["L. Zhang", "J. Yi", "R. Jin"], "venue": "In Proceedings of the 31st International Conference on Machine Learning", "year": 2014}, {"title": "Towards a Lower Sample Complexity for Robust One-bit Compressed Sensing", "authors": ["R. Zhu", "Q. Gu"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning,", "year": 2015}], "id": "SP:3516222cdab2e0d19bb0477af53815e0cbe1e116", "authors": [{"name": "Sheng Chen", "affiliations": []}, {"name": "Arindam Banerjee", "affiliations": []}], "abstractText": "In this paper, we investigate general single-index models (SIMs) in high dimensions. Based on U -statistics, we propose two types of robust estimators for the recovery of model parameters, which can be viewed as generalizations of several existing algorithms for one-bit compressed sensing (1-bit CS). With minimal assumption on noise, the statistical guarantees are established for the generalized estimators under suitable conditions, which allow general structures of underlying parameter. Moreover, the proposed estimator is novelly instantiated for SIMs with monotone transfer function, and the obtained estimator can better leverage the monotonicity. Experimental results are provided to support our theoretical analyses.", "title": "Robust Structured Estimation with Single-Index Models"}