0,1,label2,summary_sentences
Neural networks trained through stochastic gradient descent (SGD) can memorize their training data.,1. Introduction,[0],[0]
"Although practitioners have long been aware of this phenomenon, Zhang et al. (2017) recently brought attention to it by showing that standard SGD-based training on AlexNet gets close to zero training error on a modification of the ImageNet dataset even when the labels are randomly permuted.",1. Introduction,[0],[0]
"This leads to an interesting question: If neural nets have sufficient capacity to memorize random training sets why do
1Two Sigma, New York, NY, USA.",1. Introduction,[0],[0]
"Correspondence to: Satrajit Chatterjee <satrajit.chatterjee@twosigma.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
they generalize on real data?,1. Introduction,[0],[0]
A natural hypothesis is that nets behave differently on real data than on random data.,1. Introduction,[0],[0]
Arpit et al. (2017) study this question experimentally and show that there are apparent differences in behavior.,1. Introduction,[0],[0]
"They conclude that generalization and memorization depend not just on the network architecture and optimization procedure but on the dataset itself.
",1. Introduction,[0],[0]
"But what if networks fundamentally do not behave differently on real data than on random data, and, in both cases, are simply memorizing?",1. Introduction,[0],[0]
This is a difficult question to explore for two reasons.,1. Introduction,[0],[0]
"First, it is hard to provide a direct answer.",1. Introduction,[0],[0]
Whereas it is easy to tell when a net is memorizing random data (the training error goes to zero!),1. Introduction,[0],[0]
", there is no easy way to tell when a network is memorizing real data as opposed to “learning”.",1. Introduction,[0],[0]
"Second, and perhaps more importantly, it contradicts the intuitive notion—inherent in the preceding discussion—that memorization and generalization are at odds.",1. Introduction,[0],[0]
This work attempts to shed light on this second difficulty by investigating the following: How much can you learn if memorization is all you can do?,1. Introduction,[0],[0]
"Is generalization even possible in this setting?
",1. Introduction,[0],[0]
"At first, generalization in such a setting of pure memorization may seem hopeless: the simplest way to memorize would be to build a lookup table from the training data.",1. Introduction,[0],[0]
"Although this approach works for special cases where the input population is finite and small, it fails in general since the examples seen during training are unlikely to match test examples.",1. Introduction,[0],[0]
One way to get around this limitation is to use k-Nearest Neighbors (k-NN) or any of its variants at test time.,1. Introduction,[0],[0]
"While k-NNs work well on many problems, they fail on problems where it is not easy to construct a semantically meaningful distance function on the input space.",1. Introduction,[0],[0]
"In such cases, the obvious syntactic distance functions (e.g., say Euclidean distance between images viewed as vectors in Rd) do not work well.",1. Introduction,[0],[0]
"Indeed some of the most interesting results from deep learning have been the discovery—through learning—of semantically meaningful distance functions (via embeddings).
",1. Introduction,[0],[0]
"Therefore, in this work we do not allow ourselves a distance function.",1. Introduction,[0],[0]
"Instead, we get around the problem by applying the notion of depth, which has been wildly successful in improving the performance of neural networks, to direct memorization.",1. Introduction,[0],[0]
"We build a network of lookup tables (also
called “luts”) where the luts are arranged in successive layers much like a neural network.",1. Introduction,[0],[0]
"However, unlike a neural network, training happens through memorization and does not involve backpropagation, gradient descent, or any explicit search.",1. Introduction,[0],[0]
"Now, since in contrast to a neuron, the function implemented by a lut can be arbitrarily complex, without some means to control the complexity, the notion of depth is vacuous.",1. Introduction,[0],[0]
We control the complexity of a function learned by a lut in the simplest possible way: we limit the support (and thereby the size) of the lut.,1. Introduction,[0],[0]
"Each lut in a layer receives inputs from only a few luts in the previous layer, which are picked at random when the network is constructed.",1. Introduction,[0],[0]
This kind of restriction on local function complexity is similar to what is found to work well in deep neural networks.,1. Introduction,[0],[0]
"For example, a convolutional filter is obviously support-limited, and a fully connected layer although not support-limited is nevertheless limited in expressivity.",1. Introduction,[0],[0]
"Furthermore, the learned weight matrices in neural networks are often sparse or can be made so with no loss in accuracy (Han et al., 2015).
",1. Introduction,[0],[0]
We need two restrictions before we can proceed to an algorithm.,1. Introduction,[0],[0]
"First, for simplicity, we focus our attention on binary classification problems.",1. Introduction,[0],[0]
"Second, because lookup tables work naturally with discrete inputs, in this work we limit ourselves to discrete signals.",1. Introduction,[0.9634799123374169],"['In contrast to previous work, we assume that we do not need to select all message pairs in the first stage, thereby reducing computational time without sacrificing performance too much.']"
"In fact, the inputs and all intermediate signals in the network of lookup tables are binary.",1. Introduction,[0],[0]
The restriction is not as extreme as it may appear.,1. Introduction,[0],[0]
"There are a number of results in quantized and binary neural networks showing that limited precision is often sufficient (e.g. Rastegari et al., 2016).",1. Introduction,[0],[0]
"Furthermore, even in real-valued neural networks, we need mechanisms such as convolution and pooling to ensure that certain types of small changes in the inputs (e.g., a small displacement) do not lead to large changes in output.",1. Introduction,[0],[0]
"In principle, similar mechanisms could be used in a fully discrete setting to handle real-valued quantities.
",1. Introduction,[0],[0]
"With these restrictions in place, we are now ready to proceed.",1. Introduction,[0],[0]
"Let B = {0, 1} and consider the problem of learning a function f :",2. A Single Lookup Table,[0],[0]
"Bk → B from a list of training examples where each example is an (x, y) pair.",2. A Single Lookup Table,[0],[0]
"Since we want to learn by memorizing, we construct a lookup table with 2k rows (one for each possible bit pattern p ∈",2. A Single Lookup Table,[0],[0]
Bk that can appear at the input) and two columns y0 and y1.,2. A Single Lookup Table,[0],[0]
"The y0 entry for the row corresponding to pattern p (denoted by cp0) counts how many times p is associated with output 0 in the training set, i.e., the number of occurrences of (p, 0) in the training set.",2. A Single Lookup Table,[0],[0]
"Similarly, the y1 entry for row p (denoted by cp1) counts how many times the pattern p is associated with the output 1 in the training set, i.e., the number of occurrences of (p, 1)
in the training set.",2. A Single Lookup Table,[0],[0]
"Note that for a pattern p it is possible for both cp0 and cp1 to be greater than zero since due to Bayes error both (p, 0) and (p, 1) may be present in the training examples.",2. A Single Lookup Table,[0],[0]
It is also possible for both cp0 and cp1 to be zero if the input p never appears in the training examples.,2. A Single Lookup Table,[0],[0]
"We call such a lookup table a k-input lookup table or a k-lut since the inputs are bit vectors of length k.1
Next, we associate a boolean function f̂ : Bk → B with the lookup table in the following manner:
f̂(p) =  1 if cp1 > cp0,0 if cp1 < cp0, b if cp1 = cp0
where b ∈ B is picked uniformly at random when fixing f̂ in order to break ties.",2. A Single Lookup Table,[0],[0]
"In other words, f̂ maps an input p to the output that is most often associated with it in the training set (breaking ties randomly).",2. A Single Lookup Table,[0],[0]
"We say that f̂ is the function learned by the lookup table.
",2. A Single Lookup Table,[0],[0]
Example 1.,2. A Single Lookup Table,[0],[0]
Let k = 3 and consider learning a function f : B3 → B from 7 examples shown on the left below.,2. A Single Lookup Table,[0],[0]
"The lookup table that we learn is shown in the middle, and the truth table of the learned function f̂ is shown on the right.",2. A Single Lookup Table,[0],[0]
"The entries in the truth table which have been picked randomly to break ties are indicated by an asterisk.
",2. A Single Lookup Table,[0],[0]
"x x0x1x2
y
000 0 000 1 000 1 001 1 100 0 110 0 110 1
p x0x1x2
y0 y1
000 1 2 001 0 1 010 0 0 011 0 0 100 1 0 101 0 0 110 1 1 111 0 0
p f̂
000 1 001 1 010 0∗ 011 1∗ 100 0 101 1∗ 110 1∗ 111 0∗
Note that f̂ gets all training examples correct except for the first and sixth.",2. A Single Lookup Table,[0],[0]
"This is the best we can do on this set of training examples because the Bayes error rate is non-zero.
",2. A Single Lookup Table,[0],[0]
"If we measure training error as the average 0–1 loss on the training set, this procedure to learn f̂ has the following properties:
1.",2. A Single Lookup Table,[0],[0]
Optimality.,2. A Single Lookup Table,[0],[0]
"The learned function f̂ is Bayes-optimal on the training set, i.e., there is no function g :",2. A Single Lookup Table,[0],[0]
Bk → B with training error strictly less than that of f̂ .,2. A Single Lookup Table,[0],[0]
"In particular, the training error is zero iff the training set has zero Bayes error.
2.",2. A Single Lookup Table,[0],[0]
Monotonicity.,2. A Single Lookup Table,[0],[0]
"If we have more information for each x in the training set, i.e., we augment each training
1 Typically k is small (less than 10) and so the the table can be stored explicitly.",2. A Single Lookup Table,[0],[0]
"The input bit vector (viewed as an integer) can be used to directly index into the table.
example with m extra bits of information (keeping the labels fixed) and use the above procedure to now learn a new function ĝ : Bk+m → B, then the training error of ĝ is no more than that of f̂ .
",2. A Single Lookup Table,[0],[0]
Proof Sketch.,2. A Single Lookup Table,[0],[0]
Optimality is easy to see since the total training error is the sum of the training error for each possible pattern p which is minimized by choosing the majority class for each p.,2. A Single Lookup Table,[0],[0]
"Monotonicity holds since if not, then we can compose the obvious projection Bk+m",2. A Single Lookup Table,[0],[0]
"→ Bk with f̂ to get a contradiction with the optimality of ĝ.
Note that monotonicity implies in particular that the training accuracy at the output of a lut is no worse than that at any of its inputs.",2. A Single Lookup Table,[0],[0]
"Furthermore, as we make the luts larger, the training error cannot increase but only decrease.",2. A Single Lookup Table,[0],[0]
This is interesting since there are no restrictions on the m extra bits: they could be completely non-informative.,2. A Single Lookup Table,[0],[0]
"These properties will prove useful in the next section as we consider networks of luts.
",2. A Single Lookup Table,[0],[0]
"To summarize, the procedure described to learn a single lookup table in this section is essentially memorization in the presence of Bayes error, where the idea is to simply remember the output that is most commonly associated with an input in the training set.",2. A Single Lookup Table,[0],[0]
"Now consider a binary classification task on MNIST (LeCun & Cortes, 2010) of separating the digits ‘0’ through ‘4’ (we map these to the 0 class) from the digits ‘5’ through ‘9’ (the 1 class) where the pixels are 1-bit quantized.",3. A Network of Lookup Tables,[0],[0]
"Thus the task is to learn a function f : B28×28 → B. We call this the Binary-MNIST task (overloading binary here to mean both binary classification and binary inputs).
",3. A Network of Lookup Tables,[0],[0]
"In principle, we could use the procedure in Section 2 to learn this function.",3. A Network of Lookup Tables,[0],[0]
"However, since we have only 60,000 training examples in MNIST, most of the 228×28 rows in the lookup table would have 0 entries in both columns, and hence the function learned would be mostly random and have very poor generalization to inputs outside the training set.
",3. A Network of Lookup Tables,[0],[0]
"As discussed in the introduction, we get around this problem by introducing depth.",3. A Network of Lookup Tables,[0],[0]
"Instead of learning a giant lookup table with 228×28 entries, we learn a network of (much) smaller lookup tables.",3. A Network of Lookup Tables,[0],[0]
The network consists of d layers with each layer l (1 ≤ l ≤ d) having nl k-input lookup tables.,3. A Network of Lookup Tables,[0],[0]
Each lut in first layer (l = 1) receives its inputs from a krandom subset of the network inputs.,3. A Network of Lookup Tables,[0],[0]
A lut in a layer,3. A Network of Lookup Tables,[0],[0]
l > 1 receives inputs from a k-random subset of the luts in layer l − 1.,3. A Network of Lookup Tables,[0],[0]
The connectivity is fixed at network creation time and does not change during training or inference.,3. A Network of Lookup Tables,[0],[0]
"The final layer of the network has a single lookup table (i.e., nd = 1) which is the output of the network.",3. A Network of Lookup Tables,[0],[0]
"By analogy with neural
networks, we call the final layer the output layer and the other layers hidden layers.
",3. A Network of Lookup Tables,[0],[0]
"We train the lookup tables layer by layer, where the target of each lookup table is the final output.",3. A Network of Lookup Tables,[0],[0]
We start from the first layer and work our way to the output.,3. A Network of Lookup Tables,[0],[0]
"Once a layer has been learned, we use the functions associated with its luts (the f̂s of Section 2) to map its inputs to outputs.",3. A Network of Lookup Tables,[0],[0]
"These outputs serve as the inputs for the next layer, which is learned next.",3. A Network of Lookup Tables,[0],[0]
"Continuing our analogy with neural networks, we call the output values of a layer activations.
",3. A Network of Lookup Tables,[0],[0]
"Inference is similar to training: We start from the inputs and evaluate each layer in order using the functions learned at each lut to map inputs to outputs.
",3. A Network of Lookup Tables,[0],[0]
Example 2.,3. A Network of Lookup Tables,[0],[0]
We modify Example 1.,3. A Network of Lookup Tables,[0],[0]
"Instead of learning a single lut with k = 3 inputs, we learn a network of k = 2 luts.",3. A Network of Lookup Tables,[0],[0]
The network shown in Figure 1 has d = 2 layers.,3. A Network of Lookup Tables,[0],[0]
"The first layer has 2 luts (i.e., n1 = 2) which are connected to inputs x0 and x1 of the network.",3. A Network of Lookup Tables,[0],[0]
"The second layer (which is also the output layer) has 1 lut (i.e., n2 = 1) which is connected to the outputs of the two luts in the first layer.",3. A Network of Lookup Tables,[0],[0]
(The connections were made randomly when the network was created.),3. A Network of Lookup Tables,[0],[0]
"Using the procedure in Section 2, the two lookup tables learned in the first layer (using y as the target) along with their corresponding functions f̂10 and f̂11 are:
p x0x1
y0 y1 f̂10
00 1 3 1 01 0 0 1∗ 10 1 0 0 11 1 1 1∗
p x0x2
y0 y1 f̂11
00 1 2 1 01 0 1 1 10 2 1 0 11 0 0 1∗
Let the output of the luts in the first layer be w10 and w11, i.e., w10 = f̂10(x0x1) and w11 = f̂11(x0x2).",3. A Network of Lookup Tables,[0],[0]
The learning problem for the lut in the second layer is shown in the tables below.,3. A Network of Lookup Tables,[0],[0]
"For convenience, on the left we show the primary inputs x0, x1 and x2, the first layer activations w10 and w11 (which are the inputs of the lut), and the target output y.",3. A Network of Lookup Tables,[0],[0]
"On the right we show the table and the learned function f̂20:
x x0x1x2
w10w11 y
000 11 0 000 11 1 000 11 1 001 11 1 100 00 0 110 10 0 110 10 1
p w10w11
y0 y1 f̂20
00 1 0 0 01 0 0 1∗ 10 1 1 0∗ 11 1 3 1
In this case the function implemented by the network of 2-luts has the same performance on the training set as the function learned by the 3-lut in Example 1.",3. A Network of Lookup Tables,[0],[0]
"Since there are fewer possible patterns in the case of smaller luts, we expect better pattern coverage during training and hence better generalization.
",3. A Network of Lookup Tables,[0],[0]
Implementation.,3. A Network of Lookup Tables,[0],[0]
"The memorization procedure described here is linear in the size of the training data, requiring two passes over the training set.",3. A Network of Lookup Tables,[0],[0]
It is computationally efficient since it only involves counting and dense table lookups and does not require floating point.,3. A Network of Lookup Tables,[0],[0]
"It is also easy to parallelize since each lut in a given layer is independent, and the counts can be computed on disjoint subsets of the training data and then combined (using, for example, a reduction tree).",3. A Network of Lookup Tables,[0],[0]
Note that using this property it is possible to execute the algorithm on extremely large datasets where all the training examples may not fit on a single machine with only the summary statistics of the data (the counts in the lookup tables) being exchanged across machines.,3. A Network of Lookup Tables,[0],[0]
Experiment 1.,4. Experiments,[0],[0]
"In the first experiment, we apply the above procedure to the Binary-MNIST task (as defined in Section 3) to see if this approach to memorization can generalize.",4. Experiments,[0],[0]
"For this experiment, we construct a network with 5 hidden layers of 1024 luts and 1 lut in the output layer.",4. Experiments,[0],[0]
"We set k = 8, i.e., each lut in the network takes 8 inputs.
",4. Experiments,[0],[0]
"The network achieves a training accuracy of 0.89 on this task, which is perhaps not so surprising since we are memorizing the training data after all.",4. Experiments,[0],[0]
"But what is surprising is that the network achieves an accuracy of 0.87 on a heldout set (the 10,000 test images in MNIST) which indicates generalization.
",4. Experiments,[0],[0]
"This result is not state-of-the-art on this variant of MNIST (see Experiment 4), but that is not the point.",4. Experiments,[0],[0]
"It is significantly above the 0.5 accuracy that would be expected by chance, and this is achieved by an algorithm that only memorizes and performs no explicit search.
",4. Experiments,[0],[0]
The training and test accuracies are stable: there is very little variation from run to run.,4. Experiments,[0],[0]
"In other words, very little depends on the actual random choices made when deciding the topology of the network.",4. Experiments,[0],[0]
"To understand why this is
the case, we look at training accuracies of the luts in the network.",4. Experiments,[0],[0]
"Since the target for each lut in the network is the final classification target, we can examine the accuracy of a lut as a function of its layer.
",4. Experiments,[0],[0]
Table 1 shows the summary statistics for the accuracies of luts in each layer.,4. Experiments,[0],[0]
We observe that as depth increases the average accuracy of the luts in a layer goes up.,4. Experiments,[0],[0]
"In other words, depth helps.",4. Experiments,[0],[0]
"Some intuition for this is provided by the monotonicity property of the luts: the output of a lut cannot have lower accuracy than any of its inputs (Section 2).
",4. Experiments,[0],[0]
"Furthermore, we observe in Table 1 the dispersion in accuracy across the luts (measured either by standard deviation (std) or the difference between max and min) goes down.",4. Experiments,[0],[0]
"Therefore, as depth increases the specifics of the connectivity matters less and the network automatically becomes more stable with respect to the random choices made during construction.",4. Experiments,[0],[0]
"Indeed we can say something stronger: we have seen in our experiments (not shown in Table 1) that as depth increases the activations of the luts in a layer become more correlated with each other, and hence become more interchangeable.",4. Experiments,[0],[0]
"While this correlation is good for stability with respect to connectivity, it causes diminishing returns with additional depth.
",4. Experiments,[0],[0]
Remark.,4. Experiments,[0],[0]
The perceptive reader looking at Table 1 will also notice that we are wasting computation: the single output lut in layer 6 receives input from only 8 of the 1024 luts in layer 5 and these in turn can at most receive inputs from 64 luts from layer 4.,4. Experiments,[0],[0]
"Although a different topology would be more computationally efficient, this specific choice allows us to compare the different layers more easily.",4. Experiments,[0],[0]
"We have not optimized this aspect since it typically takes less than 30 seconds using a single threaded unoptimized implementation (Python with NumPy) to run an experiment.
",4. Experiments,[0],[0]
Experiment 2.,4. Experiments,[0],[0]
"As discussed in the introduction and in Section 3, we do not expect unbridled memorization in the form of a large lookup table (say k = 28 × 28 in the case of Binary-MNIST) to generalize at all.",4. Experiments,[0],[0]
"This motivated our
exploration of a network of smaller lookup tables parameterized by k (the number of inputs of each lut).",4. Experiments,[0],[0]
We now vary k to see if we can control the amount of memorization and to see the effect it has on generalization.,4. Experiments,[0],[0]
"To avoid changing too much at once, we keep the number of layers and the number of luts per layer the same as in Experiment 1.
",4. Experiments,[0],[0]
The results are shown in the first 3 columns of Table 2.,4. Experiments,[0],[0]
"With small values of k, the network finds it difficult to memorize the training data.",4. Experiments,[0],[0]
"As intuitively expected (see also the monotonicity property in Section 2), as k increases the training accuracy goes up with perfect memorization at k = 14, i.e., long before 28×28.",4. Experiments,[0],[0]
"However, larger luts generalize less well, and the best test accuracy of 0.90 is achieved at k = 12 though with substantially good memorization of the training data (0.99).",4. Experiments,[0],[0]
"Interestingly, there is a clear monotonic increase in the generalization gap measured as the difference between training and test accuracy with increasing k.
Experiment 3.",4. Experiments,[0],[0]
In this experiment—along the lines of those performed in Zhang et al. (2017)—we randomly permute the labels in the training set and repeat Experiment 2 on this “random” dataset.,4. Experiments,[0],[0]
The results are shown in columns 4 and 5 of Table 2.,4. Experiments,[0],[0]
"As expected, with increasing k the network gets better at memorizing the training data, and the test accuracy hovers around chance (0.5) though with significant variation (± 0.05).",4. Experiments,[0],[0]
"This may be viewed as empirical evidence that the Rademacher complexity goes up with k.
However, and this may be surprising for a pure memorization algorithm, memorizing random data turns out to be harder than memorizing real data (columns 2 and 3 of Table 2) in the sense that a larger k is required to get the same accuracy with random data than with real data.",4. Experiments,[0],[0]
"For example, it takes until k = 12 to get comparable training accuracy on random data as k = 4 gets on real data.",4. Experiments,[0],[0]
"This result corroborates the findings in Arpit et al. (2017, §3 and §4) that real data is easier to fit than random data.",4. Experiments,[0],[0]
But it also means that we cannot conclude that any such difference observed in neural networks is because they do not use brute force memorization on real data.,4. Experiments,[0],[0]
"As this experiment shows, such
differences can appear even with brute force memorization.
",4. Experiments,[0],[0]
"Finally, at k = 12 we have a network that is able to memorize random data (random training accuracy of 0.82) and yet generalizes to test data when trained on real data (real test accuracy of 0.90).",4. Experiments,[0],[0]
This is very similar to findings of Zhang et al. (2017) in the context of neural networks.,4. Experiments,[0],[0]
"Kawaguchi et al. (2017, §3) argue that this phenomenon is universal and our result may be viewed as further empirical evidence for their claim showing that this phenomenon can happen even in the simplified setting of just memorization.
",4. Experiments,[0],[0]
Experiment 4.,4. Experiments,[0],[0]
"For completeness, we compare memorization with several standard methods and the results are shown in Table 3.",4. Experiments,[0],[0]
We have not specifically tuned the other methods since our goal is not to beat the state-of-the-art but to get a sense of how memorization alone does when compared to the standard methods.,4. Experiments,[0],[0]
"The best performance is obtained by a LENET-style convolutional network with 2 convolutions (64 and 32 filters respectively) each followed by a corresponding max pool layer, and 3 fully connected layers (256, 128 and 2 units respectively) with softmax output.",4. Experiments,[0],[0]
"The net is trained for 6 epochs with stochastic gradient descent and dropout.
",4. Experiments,[0],[0]
"Once again, compared to random guessing which has 0.50 test accuracy, memorization does quite well with a test accuracy of 0.90 (using the k = 12 configuration from Experiment 2) and beats logistic regression and naı̈ve Bayes.",4. Experiments,[0],[0]
"Interestingly, 1- and 5-Nearest Neighbors do well too (test accuracy of 0.97) though recall that they are provided with a distance function which memorization does not have access to and must in a sense discover.
",4. Experiments,[0],[0]
Experiment 5.,4. Experiments,[0],[0]
"We now consider the task of separating the i-th digit in MNIST from the j-th digit, which gives us( 10 2 ) = 45 binary classification tasks, which we collectively call Pairwise-MNIST.",4. Experiments,[0],[0]
"The images are binarized as before.
",4. Experiments,[0],[0]
"Figure 2 shows the training accuracy and the test accuracy for each of those 45 experiments for 8 different values of k.
As in Experiment 2, we find that as k increases, the training accuracy increases (reaching 1.0), but the test accuracy falls off.",4. Experiments,[0],[0]
"If we look at the best test accuracies for a given task (across k), on 31 out of the 45 tasks, we do better than 0.98.",4. Experiments,[0],[0]
The worst of these is 0.95 which is the best memorization can do for separating ‘4’ and ‘9’.,4. Experiments,[0],[0]
This is still significantly better than the 0.5 we would expect by chance.,4. Experiments,[0],[0]
"Typically the best test accuracies are achieved at k = 6 and k = 8.
",4. Experiments,[0],[0]
Experiment 6.,4. Experiments,[0],[0]
In Experiment 5 we notice that the variation is quite high for k = 2.,4. Experiments,[0],[0]
This indicates that the depth of the network is insufficient for proper mixing.,4. Experiments,[0],[0]
"To investigate this further, we keep k = 2 and vary the number of hidden layers from 20 to 25.",4. Experiments,[0],[0]
Each hidden layer still has 1024 luts.,4. Experiments,[0],[0]
Figure 3 shows how the training and test accuracies vary with the depth of the network.,4. Experiments,[0],[0]
"It is interesting to note that the test accuracy continues to improve even for relatively deep networks (16 or 32 hidden layers), and we get very high test accuracies even with such small lookup tables.",4. Experiments,[0],[0]
"Furthermore, we note that the variation in the generalization error (difference between training and test accuracies) decreases with increasing depth.
",4. Experiments,[0],[0]
Experiment 7.,4. Experiments,[0],[0]
Next we look at memorization on CIFAR-10 which is a collection of 32 pixel by 32 pixel color images belonging to 10 classes.,4. Experiments,[0],[0]
"As with Binary-MNIST, we quantize each color channel to 1 bit and try to separate the classes 0 through 4 from classes 5 through 9.",4. Experiments,[0],[0]
"This gives us the Binary-CIFAR-10 task where we have to learn a function f : B3×32×32 → B from 50,000 images.",4. Experiments,[0],[0]
"Incidentally, the quantization of each color channel to 1-bit significantly degrades the signal making it a difficult task for humans.
",4. Experiments,[0],[0]
"For this task, we construct a network with 5 hidden layers each with 1024 luts and one output layer with 1 output.",4. Experiments,[0],[0]
We set k = 10 for the luts.,4. Experiments,[0],[0]
This network is able to achieve a training accuracy of 0.79 and a test accuracy of 0.63.,4. Experiments,[0],[0]
"Although not as impressive in absolute terms as the memoriza-
tion result on Binary-MNIST, it is still significantly above chance (0.50).",4. Experiments,[0],[0]
"Furthermore, as before, the result is very stable and does not depend on a specific random topology chosen when the network is constructed.
",4. Experiments,[0],[0]
We compare memorization with several standard methods in Table 4.,4. Experiments,[0],[0]
By comparing Table 4 with Table 3 it is clear that Binary-CIFAR-10 is a harder task than Binary-MNIST since all the methods perform significantly worse on it.,4. Experiments,[0],[0]
"The best test accuracy of 0.71 is again from a LENET-style network similar to the one used in Experiment 4, but with 40 epochs of training.",4. Experiments,[0],[0]
"We believe a ResNet-style architecture (He et al., 2016) may potentially do better here but since our goal is not to achieve state-of-the-art but see how memorization does, we leave this to future work.",4. Experiments,[0],[0]
"For the same reason we don’t explore data augmentation here which is a standard technique for CIFAR-10.
Once again, memorization compares favorably on test accuracy with the other methods, and compared to Binary-MNIST it does relatively better here since it ties with the nearest neighbor searches.
",4. Experiments,[0],[0]
Experiment 8.,4. Experiments,[0],[0]
"In this experiment, we consider the Pairwise-CIFAR-10 tasks which are defined analogously to Pairwise-MNIST.",4. Experiments,[0],[0]
We use the same network architecture as in Experiment 7 instead of optimizing specifically for these tasks.,4. Experiments,[0],[0]
Training accuracies are generally 0.95 and above whereas the test accuracies range from 0.61 (CAT v/s DOG) to 0.85,4. Experiments,[0],[0]
"(FROG v/s SHIP) with an average test accuracy of 0.76 which is significantly above chance.
",4. Experiments,[0],[0]
Experiment 9.,4. Experiments,[0],[0]
"To get qualitative insight into the decision boundaries learned with different levels of memorization, we classify points in the region [−2, 2] ×",4. Experiments,[0],[0]
"[−2, 2] ∈ R2 as being inside or outside the circle",4. Experiments,[0],[0]
x2,4. Experiments,[0],[0]
+,4. Experiments,[0],[0]
y2 ≤ 1.62.,4. Experiments,[0],[0]
"Our dataset consists of points on a 100 × 100 grid in this region which has been partitioned into equal test and training sets (Figure 4, leftmost column).",4. Experiments,[0],[0]
To make this a hard problem we encode each point as pair of 10-bit fixed-point numbers.,4. Experiments,[0],[0]
"We learn this function f : B20 → B using networks with 32 layers each with 2048 luts and vary k. With k = 10 (rightmost column), the training set is memorized perfectly but (as seen on test)",4. Experiments,[0],[0]
the concept is not learned.,4. Experiments,[0],[0]
"However, memorizing with k = 2, we learn a simpler concept that is not faithful around the “corners” (as can be seen by zooming in) but one that generalizes almost perfectly to test.",4. Experiments,[0],[0]
"Finally, k = 6 provides a satisfactory compromise between the two extremes.",4. Experiments,[0],[0]
"Thus, once again, we see that memorization if done carefully can lead to good generalization.",4. Experiments,[0],[0]
"It is instructive to compare our memorization procedure with a few commonly used procedures for learning.
",5. Comparison with Other Methods,[0],[0]
k-Nearest Neighbors.,5. Comparison with Other Methods,[0],[0]
"The key difference, as noted in the introduction, is that k-NNs require a user-specified distance function which is often syntactic notion of distance such that induced by treating an image as a vector in Rd.",5. Comparison with Other Methods,[0],[0]
These syntactic notions of distance do not work well on more challenging tasks and one may view such a learning problem as essentially that of discovering a semantically meaningful distance function.,5. Comparison with Other Methods,[0],[0]
"We see this in our experiments: the
tr ai
ni ng
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
te st
−2.0 −1.5",5. Comparison with Other Methods,[0],[0]
−1.0,5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
"−0.5 0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
−2.0 −1.5 −1.0",5. Comparison with Other Methods,[0],[0]
−0.5,5. Comparison with Other Methods,[0],[0]
"0.0 0.5 1.0 1.5 2.0
−2.0
−1.5
−1.0
−0.5
0.0
0.5
1.0
1.5
2.0
ground truth k = 2 k = 6 k = 10
Figure 4.",5. Comparison with Other Methods,[0],[0]
"The decision boundaries learned in Experiment 9.
distance function helps more with Binary-MNIST (Experiment 4) than it does with Binary-CIFAR-10 (Experiment 7).",5. Comparison with Other Methods,[0],[0]
"Furthermore, in a separate experiment we found that augmenting the table lookup with 1-NN search at test time did not significantly improve test accuracy for Binary-CIFAR-10 where memorization was already tied with k-NNs.
Additionally, k-NN requires storing the entire training set and is typically computationally more expensive at test time.",5. Comparison with Other Methods,[0],[0]
"For example, on Binary-MNIST the standard k-NN implementation in scikit-learn (Pedregosa et al., 2011) took more than an hour to evaluate performance on the training and test sets (as opposed to seconds with memorization).",5. Comparison with Other Methods,[0],[0]
"There has been work on speeding up nearest neighbor search by using locality sensitive hashing (Indyk & Motwani, 1998) and, more recently, with random projections (Li & Malik, 2016).",5. Comparison with Other Methods,[0],[0]
"In that context, one may view each lookup table as implementing a trivial locality sensitive hash function where the distance metric arises from exact equality, and the network as an ensemble through cascading of such nearest neighbors classifiers.
",5. Comparison with Other Methods,[0],[0]
Neural Networks.,5. Comparison with Other Methods,[0],[0]
"The initial motivation for this work was to understand neural networks better; particularly to explore with a model the idea that perhaps SGD is a sophisticated way to memorize training data in a manner that generalizes and that perhaps there are simpler ways to memorize data
as well that may yet generalize.",5. Comparison with Other Methods,[0],[0]
"However, a key difference is that gradient descent-based training can learn useful intermediate representations or targets for hidden layers.",5. Comparison with Other Methods,[0],[0]
"In this work we have side stepped that question, by simply setting the intermediate target to be the final output.",5. Comparison with Other Methods,[0],[0]
It is an interesting line of research to see if we can find a way to learn useful intermediate signals in this setting perhaps by purely combinatorial methods.,5. Comparison with Other Methods,[0],[0]
"Practically, that would give us a method to learn purely binary neural networks without using floating point at all, which is useful in resource constrained environments.
",5. Comparison with Other Methods,[0],[0]
Random Forests.,5. Comparison with Other Methods,[0],[0]
"Trees in a random forest are constructed over a subset of the data by iteratively evaluating different input variables to optimize purity after splitting on the variable (Breiman, 2001).",5. Comparison with Other Methods,[0],[0]
"In contrast, memorization uses the whole dataset and does not solve any optimization problem (which makes it more computationally efficient).",5. Comparison with Other Methods,[0],[0]
"Furthermore, random forests combine the tree predictions using voting whereas memorization uses cascading.
",5. Comparison with Other Methods,[0],[0]
Cascading and Stacked Generalization.,5. Comparison with Other Methods,[0],[0]
"A recent extension of random forests are Deep Forests (Zhi-Hua Zhou, 2017) where multiple random forests are constructed at each level and then cascaded using the idea of stacked generalization (Wolpert, 1992) which is a generalization of cross-validation.",5. Comparison with Other Methods,[0],[0]
"In contrast, layers of luts are far simpler, and memorization propagates outputs based on what has been memorized over the entire training data.",5. Comparison with Other Methods,[0],[0]
"Due to the manner in which we construct the lookup tables and the corresponding functions (using the counts of the patterns) it is not clear to us that stacked generalization will help.
",5. Comparison with Other Methods,[0],[0]
Spectral Methods.,5. Comparison with Other Methods,[0],[0]
"There is a rich literature on the theory of learning boolean functions (f : Bk → B in our notation) (Mansour, 1994) which looks at theoretical learning guarantees under assumptions on the input distribution (typically uniform) and on the spectrum of the function (e.g. f can be approximated by a sparse and low degree polynomial in the boolean fourier basis).",5. Comparison with Other Methods,[0],[0]
"Recently, Hazan et al. (2017) have used these techniques in hyperparameter optimization where they find them to be practically useful (the distributional assumption is not fatal for this application).",5. Comparison with Other Methods,[0],[0]
"This line of work does not deal with depth, but only linear combinations of the basis functions.",5. Comparison with Other Methods,[0],[0]
"However, there is similarity in having a low degree in the fourier basis and our notion of support-limited memorization.",5. Comparison with Other Methods,[0],[0]
"These are similar structural priors and our results and those of Hazan et al. may be viewed as evidence that real world functions satisfy these priors.
",5. Comparison with Other Methods,[0],[0]
Learning Boolean Circuits.,5. Comparison with Other Methods,[0],[0]
"There is relatively little prior work in directly learning boolean circuits (Oliveira & Sangiovanni-Vincentelli, 1994; Tapp, 2014).",5. Comparison with Other Methods,[0],[0]
"However, it is interesting to note that the memorization algorithm in Section 3 although developed independently and from different
considerations is similar to the greedy algorithm described by Tapp.2",5. Comparison with Other Methods,[0],[0]
"An important difference is that instead of learning a single tree, we learn a network which makes learning more stable (as seen in Experiment 1).",5. Comparison with Other Methods,[0],[0]
"The experiments of Zhang et al. (2017) and Arpit et al. (2017) on training with random data lead naturally to the question that if neural networks can memorize random data and yet generalize on real data, are they perhaps doing something different in the two cases.",6. Conclusion,[0],[0]
This work started with the opposite thought: What if in both cases they are simply memorizing?,6. Conclusion,[0],[0]
"This, in turn, leads to the question of whether it is even possible to generalize from pure memorization.",6. Conclusion,[0],[0]
Naı̈ve memorization with a lookup table is too simplistic a model,6. Conclusion,[0],[0]
"but, as we saw, a slightly more complex model in the form of a network of support-limited lookup tables does significantly better than chance and is closer to the standard algorithms on a number of binary classification problems from MNIST and CIFAR-10.",6. Conclusion,[0],[0]
"(To investigate if this result holds on other datasets is an important area of future work.)
",6. Conclusion,[0],[0]
"Furthermore, this model replicates some of the key observations with neural networks: the performance of a network improves with depth; it memorizes random data and yet generalizes on real data; and memorizing random data is harder than real data.",6. Conclusion,[0],[0]
"In particular, the last observation implies that we cannot rule out memorization based on differences in the hardness of learning between real and random data.
",6. Conclusion,[0],[0]
"For future work, we would like to understand why memorization generalizes.",6. Conclusion,[0],[0]
"Now, since the size of the hypothesis space is bounded by 2n2 k
(where n is the number of k-luts in the network), we can use results from PAC-learning to bound the generalization gap, but these bounds are typically weak or vacuous.3 Rademacher complexity may be useful for small k (say 2), but for moderate k—where the Rademacher complexity is high yet there is generalization— we would need a different approach, perhaps one based on stability (Bousquet & Elisseeff, 2002).",6. Conclusion,[0],[0]
"In this connection, we expect the results in Devroye & Wagner (1979) to apply to a single lut, but extensions are needed to handle networks of luts, i.e., depth.",6. Conclusion,[0],[0]
"Furthermore, these would have to incorporate details of the construction since not every network of luts generalizes (even for k = 2).
",6. Conclusion,[0],[0]
"Finally, given the computational efficiency of memorization, we would like to extend it to a practically useful algorithm for learning, but that would likely involve introducing some form of explicit optimization or search.
",6. Conclusion,[0],[0]
2We thank David Krueger for noticing the connection.,6. Conclusion,[0],[0]
3,6. Conclusion,[0],[0]
"For example, using Theorem 2.2 in Mohri et al. (2012) for the experiments in Table 2 (with δ = 0.01 for concreteness) bounds the gap to 0.34 for k = 2.",6. Conclusion,[0],[0]
The bound doubles as k increases by 2.,6. Conclusion,[0],[0]
"I thank Ben Rossi, Vinod Valsalam, Rhys Ulerich, and Eric Allen for many useful discussions and Larry Rudolph and Steve Heller for their feedback on the paper.",Acknowledgments,[0],[0]
"In the machine learning research community, it is generally believed that there is a tension between memorization and generalization.",abstractText,[0],[0]
"In this work, we examine to what extent this tension exists, by exploring if it is possible to generalize by memorizing alone.",abstractText,[0],[0]
"Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10.",abstractText,[0],[0]
"Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data.",abstractText,[0],[0]
The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.,abstractText,[0],[0]
Learning and Memorization,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 332–344 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1031",text,[0],[0]
"There is a growing interest in automated processing of historical documents, as evidenced by the growing field of digital humanities and the increasing number of digitally available collections of historical documents.",1 Introduction,[0],[0]
"A common approach to deal with the high amount of variance often found in this type of data is to perform spelling normalization (Piotrowski, 2012), which is the mapping of historical spelling variants to standardized/modernized forms (e.g. vnd→ und ‘and’).
",1 Introduction,[0],[0]
"Training data for supervised learning of historical text normalization is typically scarce, making it a challenging task for neural architectures, which typically require large amounts of labeled data.",1 Introduction,[0],[0]
"Nevertheless, we explore framing the
spelling normalization task as a character-based sequence-to-sequence transduction problem, and use encoder–decoder recurrent neural networks (RNNs) to induce our transduction models.",1 Introduction,[0],[0]
"This is similar to models that have been proposed for neural machine translation (e.g., Cho et al. (2014)), so essentially, our approach could also be considered a specific case of character-based neural machine translation.
",1 Introduction,[0],[0]
"By basing our model on individual characters as input, we keep the vocabulary size small, which in turn reduces the model’s complexity and the amount of data required to train it effectively.",1 Introduction,[0],[0]
Using an encoder–decoder architecture removes the need for an explicit character alignment between historical and modern wordforms.,1 Introduction,[0],[0]
"Furthermore, we explore using an auxiliary task for which data is more readily available, namely grapheme-tophoneme mapping (word pronunciation), to regularize the induction of the normalization models.
",1 Introduction,[0],[0]
"We propose several architectures, including multi-task learning architectures taking advantage of the auxiliary data, and evaluate them across 44 small datasets from Early New High German.
",1 Introduction,[0],[0]
Contributions,1 Introduction,[0],[0]
"Our contributions are as follows:
• We are, to the best of our knowledge, the first to propose and evaluate encoder-decoder architectures for historical text normalization.
",1 Introduction,[0],[0]
"• We evaluate several such architectures across 44 datasets of Early New High German.
",1 Introduction,[0],[0]
"• We show that such architectures benefit from bidirectional encoding, beam search, and attention.
",1 Introduction,[0],[0]
"• We also show that MTL with pronunciation as an auxiliary task improves the performance of architectures without attention.
",1 Introduction,[0],[0]
"332
• We analyze the above architectures and show that the MTL architecture learns attention from the auxiliary task, making the attention mechanism largely redundant.
",1 Introduction,[0],[0]
"• We make our implementation publicly available at https://bitbucket.org/ mbollmann/acl2017.
",1 Introduction,[0],[0]
"In sum, we both push the state-of-the-art in historical text normalization and present an analysis that, we believe, brings us a step further in understanding the benefits of multi-task learning.",1 Introduction,[0],[0]
"Normalization For the normalization task, we use a total of 44 texts from the Anselm corpus (Dipper and Schultz-Balluff, 2013) of Early New High German.1",2 Datasets,[0],[0]
"The corpus is a collection of manuscripts and prints of the same core text, a religious treatise.",2 Datasets,[0],[0]
"Although the texts are semi-parallel and share some vocabulary, they were written in different time periods (between the 14th and 16th century) as well as different dialectal regions, and show quite diverse spelling characteristics.",2 Datasets,[0],[0]
"For example, the modern German word Frau ‘woman’ can be spelled as fraw/vraw (Me), frawe (N2), frauwe (St), fraüwe (B2), frow (Stu), vrowe (Ka), vorwe (Sa), or vrouwe (B), among others.2
All texts in the Anselm corpus are manually annotated with gold-standard normalizations following guidelines described in Krasselt et al. (2015).",2 Datasets,[0],[0]
"For our experiments, we excluded texts from the corpus that are shorter than 4,000 tokens, as well as a few for which annotations were not yet available at the time of writing (mostly Low German and Dutch versions).",2 Datasets,[0],[0]
"Nonetheless, the remaining 44 texts are still quite short for machine-learning standards, ranging from about 4,200 to 13,200 tokens, with an average length of 7,350 tokens.
",2 Datasets,[0],[0]
"For all texts, we removed tokens that consisted solely of punctuation characters.",2 Datasets,[0],[0]
"We also lowercase all characters, since it helps keep the size of the vocabulary low, and uppercasing of words is usually not very consistent in historical texts.",2 Datasets,[0],[0]
"Tokenization was not an issue for pre-processing these texts, since modern token boundaries have already been marked by the transcribers.
",2 Datasets,[0],[0]
"1https://www.linguistics.rub.de/ anselm/
2We refer to individual texts using the same internal IDs that are found in the Anselm corpus (cf.",2 Datasets,[0],[0]
"the website).
",2 Datasets,[0],[0]
Grapheme-to-phoneme mappings We use learning to pronounce as our auxiliary task.,2 Datasets,[0],[0]
This task consists of learning mappings from sequences of graphemes to the corresponding sequences of phonemes.,2 Datasets,[0],[0]
"We use the German part of the CELEX lexical database (Baayen et al., 1995), particularly the database of phonetic transcriptions of German wordforms.",2 Datasets,[0],[0]
"The database contains a total of 365,530 wordforms with transcriptions in DISC format, which assigns one character to each distinct phonological segment (including affricates and diphthongs).",2 Datasets,[0],[0]
"For example, the word Jungfrau ‘virgin’ is represented as ’jUN-frB.",2 Datasets,[0],[0]
"We propose several architectures that are extensions of a base neural network architecture, closely following the sequence-to-sequence model proposed by Sutskever et al. (2014).",3.1 Base model,[0],[0]
"It consists of the following:
• an embedding layer that maps one-hot input vectors to dense vectors;
• an encoder RNN that transforms the input sequence to an intermediate vector of fixed dimensionality;
• a decoder RNN whose hidden state is initialized with the intermediate vector, and which is fed the output prediction of one timestep as the input for the next one; and
• a final dense layer with a softmax activation which takes the decoder’s output and generates a probability distribution over the output classes at each timestep.
",3.1 Base model,[0],[0]
"For the encoder/decoder RNNs, we use long short-term memory units (LSTM) (Hochreiter and Schmidhuber, 1997).",3.1 Base model,[0],[0]
"LSTMs are designed to allow recurrent networks to better learn long-term dependencies, and have proven advantageous to standard RNNs on many tasks.",3.1 Base model,[0],[0]
"We found no significant advantage from stacking multiple LSTM layers for our task, so we use the simplest competitive model with only a single LSTM unit for both encoder and decoder.
",3.1 Base model,[0],[0]
"By using this encoder–decoder model, we avoid the need to generate explicit alignments between the input and output sequences, which would bring up the question of how to deal with input/output
pairs of different lengths.",3.1 Base model,[0],[0]
"Another important property is that the model does not start to generate any output until it has seen the full input sequence, which in theory allows it to learn from any part of the input, without being restricted to fixed context windows.",3.1 Base model,[0],[0]
An example illustration of the unrolled network is shown in Fig. 1.,3.1 Base model,[0],[0]
"During training, the encoder inputs are the historical wordforms, while the decoder inputs correspond to the correct modern target wordforms.",3.2 Training,[0],[0]
"We then train each model by minimizing the crossentropy loss across all output characters; i.e., if y = (y1, ..., yn) is the correct output word (as a list of one-hot vectors of output characters) and ŷ",3.2 Training,[0],[0]
"= (ŷ1, ..., ŷn) is the model’s output, we minimize the mean loss−∑ni=1 yi log ŷi over all training samples.",3.2 Training,[0],[0]
"For the optimization, we use the Adam algorithm (Kingma and Ba, 2015) with a learning rate of 0.003.
",3.2 Training,[0],[0]
"To reduce computational complexity, we also set a maximum word length of 14, and filter all training samples where either the input or output word is longer than 14 characters.",3.2 Training,[0],[0]
"This only affects 172 samples across the whole dataset, and is only done during training.",3.2 Training,[0],[0]
"In other words, we evaluate our models across all the test examples.",3.2 Training,[0],[0]
"For prediction, our base model generates output character sequences in a greedy fashion, selecting the character with the highest probability at each timestep.",3.3 Decoding,[0],[0]
"This works fairly well, but the greedy approach can yield suboptimal global picks, in which each individual character is sensibly derived from the input, but the overall word is non-
sensical.",3.3 Decoding,[0],[0]
"We therefore also experiment with beam search decoding, setting the beam size to 5.
",3.3 Decoding,[0],[0]
"Finally, we also experiment with using a lexical filter during the decoding step.",3.3 Decoding,[0],[0]
"Here, before picking the next 5 most likely characters during beam search, we remove all characters that would lead to a string not covered by the lexicon.",3.3 Decoding,[0],[0]
This is again intended to reduce the occurrence of nonsensical outputs.,3.3 Decoding,[0],[0]
"For the lexicon, we use all word forms from CELEX (cf. Sec. 2) plus the target word forms from the training set.3",3.3 Decoding,[0],[0]
"In our base architecture, we assume that we can decode from a single vector encoding of the input sequence.",3.4 Attention,[0],[0]
"This is a strong assumption, especially with long input sequences.",3.4 Attention,[0],[0]
Attention mechanisms give us more flexibility.,3.4 Attention,[0],[0]
"The idea is that instead of encoding the entire input sequence into a fixedlength vector, we allow the decoder to “attend” to different parts of the input character sequence at each time step of the output generation.",3.4 Attention,[0],[0]
"Importantly, we let the model learn what to attend to based on the input sequence and what it has produced so far.
",3.4 Attention,[0],[0]
Our implementation is identical to the decoder with soft attention described by Xu et al. (2015).,3.4 Attention,[0],[0]
"If a = (a1, ..., an) is the encoder’s output and ht is the decoder’s hidden state at timestep t, we first calculate a context vector ẑt as a weighted combination of the output vectors ai:
ẑt =
n∑
i=1
αiai (1)
3We observe that due to this filtering, we cannot reach 2.25% of the targets in our test set, most of which are Latin word forms.
",3.4 Attention,[0],[0]
"The weights αi are derived by feeding the encoder’s output and the decoder’s hidden state from the previous timestep into a multilayer perceptron, called the attention model (fatt):
α = softmax(fatt(a, ht−1))",3.4 Attention,[0],[0]
"(2)
We then modify the decoder by conditioning its internal states not only on the previous hidden state ht−1 and the previously predicted output character yt−1, but also on the context vector ẑt:
it = σ(Wi[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bi)
ft = σ(Wf [ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bf )
ot = σ(Wo[ht−1, yt−1, ẑt]",3.4 Attention,[0],[0]
"+ bo)
gt = tanh(Wg[ht−1, yt−1, ẑt] + bg)
ct = ft ct−1",3.4 Attention,[0],[0]
"+ it gt ht = ot tanh(ct)
(3)
",3.4 Attention,[0],[0]
"In Eq. 3, we follow the traditional LSTM description consisting of input gate it, forget gate ft, output gate ot, cell state ct and hidden state ht, where W and b are trainable parameters.
",3.4 Attention,[0],[0]
"For all experiments including an attentional decoder, we use a bi-directional encoder, comprised of one LSTM layer that reads the input sequence normally and another LSTM layer that reads it backwards, and attend over the concatenated outputs of these two layers.
",3.4 Attention,[0],[0]
"While a precise alignment of input and output sequences is sometimes difficult, most of the time the sequences align in a sequential order, which can be exploited by an attentional component.",3.4 Attention,[0],[0]
"Finally, we introduce a variant of the base architecture, with or without beam search, that does multi-task learning (Caruana, 1993).",3.5 Multi-task learning,[0.9571487065909215],"['The embedding-based clustering method, i.e., Doc2Vec, applies affinity propagation (Frey and Dueck, 2007) to cluster messages embedded using Doc2Vec without being given the number of clusters, with the idea that messages in the same conversation would form a cluster.']"
"The multitask architecture only differs from the base architecture in having two classifier functions at the outer layer, one for each of our two tasks.",3.5 Multi-task learning,[0],[0]
Our auxiliary task is to predict a sequence of phonemes as the correct pronunciation of an input sequence of graphemes.,3.5 Multi-task learning,[0],[0]
"This choice is motivated by the relationship between phonology and orthography, in particular the observation that spelling variation often stems from phonological variation.
",3.5 Multi-task learning,[0],[0]
"We train our multi-task learning architecture by alternating between the two tasks, sampling one instance of the auxiliary task for each training sample of the main task.",3.5 Multi-task learning,[0],[0]
"We use the encoderdecoder to generate a corresponding output se-
quence, whether a modern word form or a pronunciation.",3.5 Multi-task learning,[0],[0]
"Doing so, we suffer a loss with respect to the true output sequence and update the model parameters.",3.5 Multi-task learning,[0],[0]
"The update for a sample from a specific task affects the parameters of corresponding classifier function, as well as all the parameters of the shared hidden layers.",3.5 Multi-task learning,[0],[0]
We used a single manuscript (B) for manually evaluating and setting the hyperparameters.,3.6 Hyperparameters,[0],[0]
This manuscript is left out of the averages reported below.,3.6 Hyperparameters,[0],[0]
"We believe that using a single manuscript for development, and using the same hyperparameters across all manuscripts, is more realistic, as we often do not have enough data in historical text normalization to reliably tune hyperparameters.
",3.6 Hyperparameters,[0],[0]
"For the final evaluation, we set the size of the embedding and the recurrent LSTM layers to 128, applied a dropout of 0.3 to the input of each recurrent layer, and trained the model on mini-batches with 50 samples each for a total of 50 epochs (in the multi-task learning setup, mini-batches contain 50 samples of each task, and epochs are counted by the size of the training set for the main task only).",3.6 Hyperparameters,[0],[0]
All these parameters were set on the B manuscript alone.,3.6 Hyperparameters,[0],[0]
"We implemented all of the models in Keras (Chollet, 2015).",3.7 Implementation,[0],[0]
Any parameters not explicitly described here were left at their default values in Keras v1.0.8.,3.7 Implementation,[0],[0]
"We split up each text into three parts, using 1,000 tokens each for a test set and a development set (that is not currently used), and the remainder of the text (between 2,000 and 11,000 tokens) for training.",4 Evaluation,[0],[0]
"We then train and evaluate on each of the 43 texts (excluding the B text that was used for hyper-parameter tuning) individually.
",4 Evaluation,[0],[0]
Baselines We compare our architectures to several competitive baselines.,4 Evaluation,[0],[0]
"Our first baseline is an averaged perceptron model trained to predict output character n-grams for each input character, after using Levenshtein alignment with generated segment distances (Wieling et al., 2009, Sec. 3.3) to align input and output characters.",4 Evaluation,[0],[0]
"Our second baseline uses the same alignment, but trains a
deep bi-LSTM sequential tagger, following Bollmann and Søgaard (2016).",4 Evaluation,[0],[0]
We evaluate this tagger using both standard and multi-task learning.,4 Evaluation,[0],[0]
"Finally, we compare our model to the rule-based and Levenshtein-based algorithms provided by the Norma tool (Bollmann, 2012).4",4 Evaluation,[0],[0]
We use word-level accuracy as our evaluation metric.,4.1 Word accuracy,[0],[0]
"While we also measure character-level metrics, minor differences on character level can cause large differences in downstream applications, so we believe that perfectly matching the output sequences is more useful.",4.1 Word accuracy,[0],[0]
"Average scores across all 43 texts are presented in Table 1 (see Appendix A for individual scores).
",4.1 Word accuracy,[0],[0]
We first see that almost all our encoder-decoder architectures perform significantly better than the four state-of-the-art baselines.,4.1 Word accuracy,[0],[0]
"All our architectures perform better than Norma and the averaged perceptron, and all the MTL architectures outperform Bollmann and Søgaard (2016).
",4.1 Word accuracy,[0],[0]
"We also see that beam search, filtering, and attention lead to cumulative gains in the context of the single-task architecture – with the best architecture outperforming the state-of-the-art by almost 3% in absolute terms.
",4.1 Word accuracy,[0],[0]
"For our multi-task architecture, we also observe gains when we add beam search and filtering, but
4https://github.com/comphist/norma
importantly, adding attention does not help.",4.1 Word accuracy,[0],[0]
"In fact, attention hurts the performance of our multitask architecture quite significantly.",4.1 Word accuracy,[0],[0]
"Also note that the multi-task architecture without attention performs on-par with the single-task architecture with attention.
",4.1 Word accuracy,[0],[0]
"We hypothesize that the reason for this pattern, which is not only observed in the average scores in Table 1, but also quite consistent across the individual results in Appendix A, is that our multi-task learning already learns how to focus attention.
",4.1 Word accuracy,[0],[0]
"This is the hypothesis that we will try to validate in Sec. 5: That multi-task learning can induce strategies for focusing attention comparable to attention strategies for recurrent neural networks.
",4.1 Word accuracy,[0],[0]
Sample predictions A small selection of predictions from our models is shown in Table 2.,4.1 Word accuracy,[0],[0]
"They serve to illustrate the effects of the various settings; e.g., the base model with greedy search tends to produce more nonsense words (ters, ünsget) than the others.",4.1 Word accuracy,[0],[0]
"Using a lexical filter helps the most in this regard: the base model with filtering correctly normalizes ergieng to erging ‘(he) fared’, while decoding without a filter produces the non-word erbiggen.",4.1 Word accuracy,[0],[0]
"Even for herczenlichen (modern herzlichen ‘heartfelt’), where no model finds the correct target form, only the model with filtering produces a somewhat reasonable alternative (herzgeliebtes ‘heartily loved’).
",4.1 Word accuracy,[0],[0]
"In some cases (such as gewarnet ‘warned’),
only the models with attention or multi-task learning produce the correct normalization, but even when they are wrong, they often agree on the prediction (e.g. dicke, herzel).",4.1 Word accuracy,[0],[0]
We will investigate this property further in Sec. 5.,4.1 Word accuracy,[0],[0]
"To gain further insights into our model, we created t-SNE projections (Maaten and Hinton, 2008) of vector representations learned on the M4 text.
",4.2 Learned vector representations,[0],[0]
Fig. 2 shows the learned character embeddings.,4.2 Learned vector representations,[0],[0]
"In the representations from the base model (Fig. 2a), characters that are often normalized to the same target character are indeed grouped closely together: e.g., historical <v> and <u> (and, to a smaller extent, <f>) are often used interchangeably in the M4 text.",4.2 Learned vector representations,[0],[0]
Note the wide separation of <n>,4.2 Learned vector representations,[0],[0]
"and <m>, which is a feature of M4 that does not hold true for all of the texts, as these do not always display a clear distinction between nasals.",4.2 Learned vector representations,[0],[0]
"On the other hand, the MTL model shows a better generalization of the training data (Fig. 2b): here, <u> is grouped closer to other vowel characters and far away from <v>/<f>.",4.2 Learned vector representations,[0],[0]
"Also, <n> and <m> are now in close proximity.
",4.2 Learned vector representations,[0],[0]
We can also visualize the internal word representations that are produced by the encoder (Fig. 3).,4.2 Learned vector representations,[0],[0]
"Here, we chose words that demonstrate the interchangeable use of <u> and <v>.",4.2 Learned vector representations,[0],[0]
"Historical vnd, vns, vmb become modern und, uns, um, changing the <v> to <u>.",4.2 Learned vector representations,[0],[0]
"However, the representation of vmb learned by the base model is closer to forms like von, vor, uor, all starting with <v> in the target normalization.",4.2 Learned vector representations,[0],[0]
"In the MTL model, however, these examples are indeed clustered together.",4.2 Learned vector representations,[0],[0]
Table 1 shows that models which employ either an attention mechanism or multi-task learning obtain similar improvements in word accuracy.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"However, we observe a decline in word accuracy for models that combine multi-task learning with attention.
",5 Analysis: Multi-task learning helps focus attention,[0],[0]
"A possible interpretation of this counterintuitive pattern might be that attention and MTL, to some degree, learn similar functions of the input data, a conjecture by Caruana (1998).",5 Analysis: Multi-task learning helps focus attention,[0],[0]
We put this hypothesis to the test by closely investigating properties of the individual models below.,5 Analysis: Multi-task learning helps focus attention,[0],[0]
"First, we are interested in the weight parameters of the final layer that transforms the decoder output to class probabilities.",5.1 Model parameters,[0],[0]
"We consider these parameters for our standard encoder-decoder model and compare them to the weights that are learned by the attention and multi-task models, respectively.5
Note that hidden layer parameters are not necessarily comparable across models, but with a fixed seed, differences in parameters over a reference model may be (and are, in our case).",5.1 Model parameters,[0],[0]
"With a fixed seed, and iterating over data points in the same order, it is conceivable the two non-baselines end up in roughly the same alternative local optimum (or at least take comparable routes).
",5.1 Model parameters,[0],[0]
"We observe that the weight differences between the standard and the attention model correlate with the differences between the standard and multitask model by a Pearson’s r of 0.346, averaged across datasets, with a standard deviation of 0.315; on individual datasets, correlation coefficient is as
5For the multi-task models, this analysis disregards those dimensions that do not correspond to classes in the main task.
high as 96.",5.1 Model parameters,[0.9551382849694281],"['To estimate conversation-level similarity, a Siamese Hierarchical Convolutional Neural Network, SHCNN, is proposed to minimize the estimation error as well as preserve both the lowand high-level semantics of messages.']"
Figure 4 illustrates these highly parallel weight changes for the different models when trained on the N4 dataset.,5.1 Model parameters,[0],[0]
"Next, we compare the effect that employing either an attention mechanism or multi-task learning has on the actual output of our system.",5.2 Final output,[0],[0]
"We find that out of the 210.9 word errors that the base model produces on average across all test sets (comprising 1,000 tokens each), attention resolves 47.7, while multi-task learning resolves an average of 45.4 errors.",5.2 Final output,[0],[0]
"Crucially, the overlap of errors that are resolved by both the attention and the MTL model amounts to 27.7 on average.
",5.2 Final output,[0],[0]
"Attention and multi-task also introduce new errors compared to the base model (26.6 and 29.5 per test set, respectively), and again we can observe a relatively high agreement of the models (11.8 word errors are introduced by both models).
",5.2 Final output,[0],[0]
"Finally, the attention and multi-task models display a word-level agreement of κ=0.834 (Cohen’s kappa), while either of these models is less strongly correlated with the base model (κ=0.817 for attention and κ=0.814 for multi-task learning).",5.2 Final output,[0],[0]
Our last analysis regards the saliency of the input timesteps with respect to the predictions of our models.,5.3 Saliency analysis,[0],[0]
We follow Li et al. (2016) in calculating first-derivative saliency for given input/output pairs and compare the scores from the different models.,5.3 Saliency analysis,[0],[0]
"The higher the saliency of an input timestep, the more important it is in determining the model’s prediction at a given output timestep.",5.3 Saliency analysis,[0],[0]
"Therefore, if two models produce similar saliency
matrices for a given input/output pair, they have learned to focus on similar parts of the input during the prediction.",5.3 Saliency analysis,[0],[0]
"Our hypothesis is that the attentional and the multi-task learning model should be more similar in terms of saliency scores than either of them compared to the base model.
",5.3 Saliency analysis,[0],[0]
Figure 5 shows a plot of the saliency matrices generated from the word pair czeychen – zeichen ‘sign’.,5.3 Saliency analysis,[0],[0]
"Here, the scores for the attentional and the MTL model indeed correlate by ρ = 0.615, while those for the base model do not correlate with either of them.",5.3 Saliency analysis,[0],[0]
"A systematic analysis across 19,000 word pairs (where all models agree on the output) shows that this effect only holds for longer input sequences (≥ 7 characters), with a mean ρ = 0.303 (±0.177) for attentional vs. MTL model, while the base model correlates with either of them by ρ < 0.21.",5.3 Saliency analysis,[0],[0]
"Many traditional approaches to spelling normalization of historical texts use edit distances or some form of character-level rewrite rules, handcrafted (Baron and Rayson, 2008) or learned automatically (Bollmann, 2013; Porta et al., 2013).
",6 Related Work,[0],[0]
"A more recent approach is based on characterbased statistical machine translation applied to historical text (Pettersson et al., 2013; SánchezMartínez et al., 2013; Scherrer and Erjavec, 2013; Ljubešić et al., 2016) or dialectal data (Scherrer and Ljubešić, 2016).",6 Related Work,[0],[0]
"This is conceptually very similar to our approach, except that we substitute the classical SMT algorithms for neural networks.",6 Related Work,[0],[0]
"Indeed, our models can be seen as a form of character-based neural MT (Cho et al., 2014).
",6 Related Work,[0],[0]
"Neural networks have rarely been applied to
historical spelling normalization so far.",6 Related Work,[0],[0]
Azawi et al. (2013) normalize old Bible text using bidirectional LSTMs with a layer that performs alignment between input and output wordforms.,6 Related Work,[0],[0]
"Bollmann and Søgaard (2016) also use bi-LSTMs to frame spelling normalization as a characterbased sequence labelling task, performing character alignment as a preprocessing step.
",6 Related Work,[0],[0]
"Multi-task learning was shown to be effective for a variety of NLP tasks, such as POS tagging, chunking, named entity recognition (Collobert et al., 2011) or sentence compression (Klerke et al., 2016).",6 Related Work,[0],[0]
"It has also been used in encoderdecoder architectures, typically for machine translation (Dong et al., 2015; Luong et al., 2016), though so far not with attentional decoders.",6 Related Work,[0],[0]
"We presented an approach to historical spelling normalization using neural networks with an encoder-decoder architecture, and showed that it consistently outperforms several existing baselines.",7 Conclusion and Future Work,[0],[0]
"Encouragingly, our work proves to be fully competitive with the sequence-labeling approach by Bollmann and Søgaard (2016), without requiring a prior character alignment.
",7 Conclusion and Future Work,[0],[0]
"Specifically, we demonstrated the aptitude of multi-task learning to mitigate the shortage of training data for the named task.",7 Conclusion and Future Work,[0],[0]
We included a multifaceted analysis of the effects that MTL introduces to our models and the resemblance that it bears to attention mechanisms.,7 Conclusion and Future Work,[0],[0]
"We believe that this analysis is a valuable contribution to the understanding of MTL approaches also beyond spelling normalization, and we are confident that our observations will stimulate further research into the relationship between MTL and attention.
",7 Conclusion and Future Work,[0],[0]
"Finally, many improvements to the presented approach are conceivable, most notably introducing some form of token context to the model.",7 Conclusion and Future Work,[0],[0]
"Currently, we only consider word forms in isolation, which is problematic for ambiguous cases (such as jn, which can normalize to in ‘in’ or ihn ‘him’) and conceivably makes the task harder for others.",7 Conclusion and Future Work,[0],[0]
Reranking the predictions with a language model could be one possible way to improve on this.,7 Conclusion and Future Work,[0],[0]
Ljubešić,7 Conclusion and Future Work,[0],[0]
"et al. (2016), for example, experiment with segment-based normalization, using a character-based SMT model with character input derived from segments (essentially, token ngrams) instead of single tokens, which also intro-
duces context.",7 Conclusion and Future Work,[0],[0]
"Such an approach could also deal with the issue of tokenization differences between the historical and the modern text, which is another challenge often found in datasets of historical text.",7 Conclusion and Future Work,[0],[0]
"Marcel Bollmann was supported by Deutsche Forschungsgemeinschaft (DFG), Grant DI 1558/4.",Acknowledgments,[0],[0]
This research is further supported by ERC Starting Grant LOWLANDS,Acknowledgments,[0],[0]
"No. 313695, as well as by Trygfonden.",Acknowledgments,[0],[0]
"For interested parties, we provide our full evaluation results for each single text in our dataset.",A Supplementary Material,[0],[0]
"Table 3 shows token counts, a rough classification of each text’s dialectal region, and the results for the baseline methods.",A Supplementary Material,[0],[0]
Table 4 presents the full results for our encoder-decoder models.,A Supplementary Material,[0],[0]
Automated processing of historical texts often relies on pre-normalization to modern word forms.,abstractText,[0],[0]
"Training encoder-decoder architectures to solve such problems typically requires a lot of training data, which is not available for the named task.",abstractText,[0],[0]
"We address this problem by using several novel encoder-decoder architectures, including a multi-task learning (MTL) architecture using a grapheme-to-phoneme dictionary as auxiliary data, pushing the state-of-theart by an absolute 2% increase in performance.",abstractText,[0],[0]
We analyze the induced models across 44 different texts from Early New High German.,abstractText,[0],[0]
"Interestingly, we observe that, as previously conjectured, multi-task learning can learn to focus attention during decoding, in ways remarkably similar to recently proposed attention mechanisms.",abstractText,[0],[0]
"This, we believe, is an important step toward understanding how MTL works.",abstractText,[0],[0]
Learning attention for historical text normalization by learning to pronounce,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 313–322 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Today, machine learning is centered around algorithms that can be trained on available taskspecific labeled and unlabeled training samples.",1 Introduction,[0],[0]
"Although learning paradigms like Transfer Learning (Pan and Yang, 2010) attempt to incorporate
∗equal contribution †Main work done during internship at Accenture Technol-
ogy Labs
knowledge from one task into another, these techniques are limited in scalability and are specific to the task at hand.",1 Introduction,[0],[0]
"On the other hand, humans have the intrinsic ability to elicit required past knowledge from the world on demand and infuse it with newly learned concepts to solve problems.
",1 Introduction,[0],[0]
"The question that we address in this paper is the following: Is it possible to develop learning models that can be trained in a way that it is able to infuse a general body of world knowledge for prediction apart from learning based on training data?
",1 Introduction,[0],[0]
"By world knowledge, we mean structured general purpose knowledge that need not be domain specific.",1 Introduction,[0],[0]
"Knowledge Graphs (Nickel et al., 2016a) are a popular source of such structured world knowledge.",1 Introduction,[0],[0]
"Knowledge Graphs represent information in the form of fact triplets, consisting of a subject entity, relation and object entity (example: <Italy, capital, Rome>).",1 Introduction,[0],[0]
The entities represent the nodes of the graph and their relations act as edges.,1 Introduction,[0],[0]
"A fact triple (subject entity, relation, object relation) is represented as (h, r, t).",1 Introduction,[0],[0]
"Practical knowledge bases congregate information from secondary databases or extract facts from unstructured text using various statistical learning mechanisms, examples of such systems are NELL (Mitchell et al., 2015) and DeepDive (Niu
313
et al., 2012).",1 Introduction,[0],[0]
"There are human created knowledge bases as well, like Freebase (FB15k) (Bollacker et al., 2008) and WordNet (Miller et al., 1990).",1 Introduction,[0],[0]
"The knowledge present in these knowledge bases includes common knowledge and partially covers common-sense knowledge and domain knowledge (Song and Roth, 2017).",1 Introduction,[0],[0]
"Knowledge Graphs and Knowledge Bases are conceptually equivalent for our purpose and we will use the name interchangeably in this paper.
",1 Introduction,[0],[0]
We illustrate the significance of world knowledge using a few examples.,1 Introduction,[0],[0]
"For the example of a Natural Language Inference (NLI) problem (MacCartney, 2009), consider the two following statements, A: The couple is walking on the sea shore and B: The man and woman are wide awake.",1 Introduction,[0],[0]
"Here, for a learning model to infer B from A, it should have access to the common knowledge that “The man and woman and The couple means the same” since this information may not be specific for a particular inference.",1 Introduction,[0],[0]
"Further, it is not possible for a model to learn all such correlations from just the labeled training data available for the task.
",1 Introduction,[0],[0]
"Consider another example of classifying the news snippet, Donald Trump offered his condolences towards the hurricane victims and their families in Texas.",1 Introduction,[0],[0]
"We cannot classify it as a political news unless we know the facts <Donald Trump, president, United States> and <Texas, state, United States>.",1 Introduction,[0],[0]
"We posit that machine learning models, apart from training them on data with the ground-truth can also be trained to fetch relevant information from structured knowledge bases in order to enhance their performance.
",1 Introduction,[0],[0]
"In this work, we propose a deep learning model that can extract relevant support facts on demand from a knowledge base (Mitchell et al., 2015) and incorporate it in the feature space along with the features learned from the training data (shown in Figure 1).",1 Introduction,[0],[0]
"This is a challenging task, as knowledge bases typically have millions of fact triples.",1 Introduction,[0],[0]
Our proposed model involves a deep learning mechanism to jointly model this look-up scheme along with the task specific training of the model.,1 Introduction,[0],[0]
The look-up mechanism and model is generic enough so that it can be augmented to any task specific learning model to boost the learning performance.,1 Introduction,[0],[0]
"In this paper, we have established superior performance of the proposed KG-augmented models
over vanilla model on text classification and natural language inference.
",1 Introduction,[0],[0]
"Although there is a plethora of work on knowledge graph representation (Nickel et al., 2016a)",1 Introduction,[0],[0]
"(Mitchell et al., 2015) (Niu et al., 2012) from natural language text, no attempt to augment learning models with knowledge graph information have been done.",1 Introduction,[0],[0]
To the best of our knowledge this is the first attempt to incorporate world knowledge from a knowledge base for learning models.,1 Introduction,[0],[0]
Knowledge Graph entities/relations need to be encoded into a numerical representation for processing.,2 Knowledge Graph Representations,[0],[0]
"Before describing the model, we provide a brief overview of graph encoding techniques.",2 Knowledge Graph Representations,[0],[0]
"Various KG embedding techniques can be classified at a high level into: Structure-based embeddings and Semantically-enriched embeddings.
",2 Knowledge Graph Representations,[0],[0]
"Structure-based embeddings: TransE (Bordes et al., 2013) is the introductory work on knowledge graph representation, which translated subject entity to object entity using one-dimensional relation vector (h + r = t).",2 Knowledge Graph Representations,[0],[0]
"Variants of the TransE (Bordes et al., 2013) model uses translation of the entity vectors over relation specific subspaces.",2 Knowledge Graph Representations,[0],[0]
"TransH (Wang et al., 2014b) introduced the relation-specific hyperplane to translate the entities.",2 Knowledge Graph Representations,[0],[0]
Similar work utilizing only the structure of the graph include ManifoldE,2 Knowledge Graph Representations,[0],[0]
"(Xiao et al., 2015b), TransG (Xiao et al., 2015a), TransD",2 Knowledge Graph Representations,[0],[0]
"(Ji et al., 2015), TransM (Fan et al., 2014), HolE (Nickel et al., 2016b) and ProjE (Shi and Weninger, 2017).
",2 Knowledge Graph Representations,[0],[0]
Semantically-enriched embeddings: These embedding techniques learn to represent entities/relations of the KG along with its semantic information.,2 Knowledge Graph Representations,[0],[0]
"Neural Tensor Network(NTN) (Socher et al., 2013) was the pioneering work in this field which initialized entity vectors with the average word embeddings followed by tensor-based operations.",2 Knowledge Graph Representations,[0],[0]
"Recent works involving this idea are “Joint Alignment” (Zhong et al., 2015) and SSP (Xiao et al., 2017).",2 Knowledge Graph Representations,[0],[0]
"DKRL (Xie et al., 2016) is a KG representation technique which also takes into account the descriptive nature of text keeping the simple structure of TransE model.",2 Knowledge Graph Representations,[0],[0]
Pretrained word2vec,2 Knowledge Graph Representations,[0],[0]
"(Mikolov et al., 2013) is used to form the entity representation by passing through a Convolutional Neural Network (CNN) (Kim, 2014) architecture constraining the relationships to hold.
",2 Knowledge Graph Representations,[0],[0]
"In our experiments we have used the DKRL (Xie et al., 2016) encoding scheme as it emphasizes on the semantic description of the text.",2 Knowledge Graph Representations,[0],[0]
"Moreover, DKRL fundamentally uses TransE (Bordes et al., 2013) method for encoding structural information.",2 Knowledge Graph Representations,[0],[0]
"Therefore, we can retrieve relevant entities & relation and obtain the complete the fact using t = h+",2 Knowledge Graph Representations,[0],[0]
r.,2 Knowledge Graph Representations,[0],[0]
"This reduces the complexity of fact retrieval as the number of entities/relations is much less compared to the number of facts, thus making the retrieval process faster.",2 Knowledge Graph Representations,[0],[0]
"Conventional supervised learning models with parameters Θ, given training data x and label y, tries to maximize the following function
max Θ
P (y|x,Θ)
The optimized parameters Θ are given as,
Θ = argmax Θ
logP (y|x,Θ)
",3 The Proposed Model,[0],[0]
"In this work, we propose to augment the supervised learning process by incorporation of world knowledge features xw.",3 The Proposed Model,[0],[0]
"The world knowledge features are retrieved using the data x, using a separate model where, xw = F (x,Θ(2)).",3 The Proposed Model,[0],[0]
"Thus, our modified objective function can be expressed as
max Θ
P (y|x, xw,Θ(1))
where, Θ = {Θ(1),Θ(2)}.",3 The Proposed Model,[0],[0]
"The optimized parameters can be obtained using the equation
Θ = argmax Θ
logP (y|x, F (x,Θ(2)),Θ(1))
",3 The Proposed Model,[0],[0]
The subsequent sections focus on the formulation of the function F which is responsible for fact triple retrieval using the data sample x.,3 The Proposed Model,[0],[0]
"Here it is important to note that, we are not assuming any structural form for P based on F .",3 The Proposed Model,[0],[0]
"So the method is generic and applicable to augment any supervised learning setting with any form for P , only constraint being P should be such that the error gradient can be computed with respect to F .",3 The Proposed Model,[0],[0]
"In the experiments we have used softmax using the LSTM (Greff et al., 2015) encodings of the input as the form for P .",3 The Proposed Model,[0],[0]
"As for F , we use soft attention (Luong et al., 2015; Bahdanau et al., 2014) using the LSTM encodings of the input and appropriate representations of the fact(s).",3 The Proposed Model,[0],[0]
"Based on the
representation used for the facts, we propose two models (a) Vanilla Model (b) Convolution-based entity/relation cluster representation, for fact retrieval in the subsequent sections.",3 The Proposed Model,[0],[0]
"The entities and relationships of KG are encoded using DKRL, explained earlier.",3.1 Vanilla Model,[0],[0]
Let ei ∈ Rm stand for the encoding of the entity i and rj ∈ Rm stands for jth relationship in the KG.,3.1 Vanilla Model,[0],[0]
"The input text in the form of concatenated word vectors, x = (x1, x2, . . .",3.1 Vanilla Model,[0],[0]
", xT ) is first encoded using an LSTM (Greff et al., 2015) module as follows,
ht = f(xt, ht−1)
and
o = 1
T
T∑
t=1
ht,
ht ∈",3.1 Vanilla Model,[0],[0]
"Rn is the hidden state of the LSTM at time t, f is a non-linear function and T is the sequence length.",3.1 Vanilla Model,[0],[0]
"Then a context vector is formed from o as follows,
C = ReLU(oTW ),
where, W ∈ Rn×m represent the weight parameters.",3.1 Vanilla Model,[0],[0]
"The same procedure is duplicated with separate LSTMs to form two seperate context vectors, one for entity retrieval (CE) and one for relationship retrieval (CR).
",3.1 Vanilla Model,[0],[0]
"As the number of fact triples in a KG is in the order of millions in the vanilla model, we resort to generating attention over the entity and relation space separately.",3.1 Vanilla Model,[0],[0]
The fact is then formed using the retrieved entity and relation.,3.1 Vanilla Model,[0],[0]
"The attention for the entity, ei using entity context vector is given by
αei = exp(CTEei)",3.1 Vanilla Model,[0],[0]
|E|∑ j=0,3.1 Vanilla Model,[0],[0]
"exp(CTEej)
where |E| is the number of entities in the KG.",3.1 Vanilla Model,[0],[0]
"Similarly the attention for a relation vector ri is computed as
αri = exp(CTRri) |R|∑ j=0 exp(CTRrj)
where |R| is the number of relations in the KG.",3.1 Vanilla Model,[0],[0]
"The final entity and relation vector retrieval is computed by the weighted sum with the attention
values of individual retrieved entity/relation vectors.
",3.1 Vanilla Model,[0],[0]
"e =
|E|∑
i=0
αeiei r =
|R|∑
i=0
αriri
Figure 2 shows the schematic diagram for entity/relation retrieval.",3.1 Vanilla Model,[0],[0]
"After the final entity and relation vectors are computed, we look forward to completion of the fact triple.",3.1 Vanilla Model,[0],[0]
The KG embedding technique used for the experiment is DKRL which inherently uses the TransE model assumption (h+r ≈ t).,3.1 Vanilla Model,[0],[0]
"Therefore, using the subject entity and relation we form the object entity as t = e+r.",3.1 Vanilla Model,[0],[0]
Thus the fact triplet retrieved is F =,3.1 Vanilla Model,[0],[0]
"[e, r, e + r], where F ∈ R3m. This retrieved fact information is concatenated along with the context vector (C) of input x obtained using LSTM module.",3.1 Vanilla Model,[0],[0]
"The final classification label y is computed as follows,
F ′ = ReLU(FTV )
y = softmax([F ′",3.1 Vanilla Model,[0],[0]
": C]TU) where, V ∈ R3m×u and U ∈ R2u×u are model parameters to be learned.",3.1 Vanilla Model,[0],[0]
y is used to compute the cross entropy loss.,3.1 Vanilla Model,[0],[0]
"We minimize this loss averaged across the training samples, to learn the various model parameters using stochastic gradient descent (Bottou, 2012).",3.1 Vanilla Model,[0],[0]
"The final prediction y, now includes information from both dataset specific samples and world knowledge to aid in en-
hanced performance.",3.1 Vanilla Model,[0],[0]
While jointly training the attention mechanism tunes itself to retrieve relevant facts that are required to do the final classification.,3.1 Vanilla Model,[0],[0]
The vanilla model attends over the entire entity/relation space which is not a good approach as we observe that the gradient for each attention value gets saturated easily.,3.2 Pre-training KG Retrieval,[0],[0]
"While training the classification and retrieval module together, the model tends to ignore the KG part and gradient propagates only through the classification module.",3.2 Pre-training KG Retrieval,[0],[0]
"This is expected to an extent as the most pertinent information for the task at hand comes from the training samples, only background aiding information comes from KG.",3.2 Pre-training KG Retrieval,[0],[0]
"After few epochs of training, the KG retrieved fact always converged to a fixed vector.",3.2 Pre-training KG Retrieval,[0],[0]
"To overcome this problem, we attempted pretraining KG retrieval part separately.",3.2 Pre-training KG Retrieval,[0],[0]
"A pre-trained KG model is used to retrieve the facts and then concatenate with the classification module, while we allow error to be propagate through the pretrained model, at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
We infer that KG doesn’t return noise and has essential information for the task as the separate KG part alone shows significant performance (59% for News20 & 66% for SNLI).,3.2 Pre-training KG Retrieval,[0],[0]
Figure 3 depicts the entire training scheme.,3.2 Pre-training KG Retrieval,[0],[0]
"This procedure solved the issue of gradient saturation in the KG retrieval part
at the time of joint training.",3.2 Pre-training KG Retrieval,[0],[0]
"However, the key problem of attention mechanism having to cover a large span of entities/relation, remained.",3.2 Pre-training KG Retrieval,[0],[0]
"In this section, we propose a mechanism to reduce the large number of entities/relationships over which attention has to be generated in the knowledge graph.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We propose to reduce the attention space by learning the representation of similar entity/relation vectors and attending over them.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In order to cluster similar entity/relation vectors, we used k-means clustering (Bishop, 2006) and formed l clusters with equal number of entity/relation vectors in each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Each of the clusters were then encoded using convolutional filters.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the k-means clustering is a sequence of entity/relation vectors {eT1 , eT2 , · · · , eTq }, where ei ∈ Rm.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"For each cluster these vectors were stacked to form E as the 2- D input to the CNN encoder, where E ∈ Rm×q.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"During experimentation for finding a suitable fil-
ter shape, it was observed that using 2-D filters the model failed to converge at all.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Therefore, we inferred that the latent representation of two different indices in the vector ei, should not be tampered using convolution.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We then resorted to use 1-D convolution filters which slide along only the columns of E , as shown Figure 4.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
The stride length along y-axis is the window length k.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"The output of the convolution layer is expressed as,
E ′(i, j) = W T",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"[ei,j , ei+1,j , . . .",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
", ei+k−1,j ]T
where, E ′(i, j) is the (i, j)th element of the output matrix E ′ and W ∈",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Rk is the convolution weight filter.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"A pooling layer followed the convolution layer in order to reduce the parameter space, we used 1-D window only along the y-axis similar to the convolutional kernel mentioned above.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We used a two layered convolution network with the stride length k & max-pool windows n is adjusted to obtain output Ei ∈ Rm, where i is the cluster index.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Similar procedure of clustering followed by the encoding of the cluster entities is done for relations as well.,3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Thus both the entity and relation space were reduced to contain fewer elements, one each for each cluster.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"After the compact entity space E and relation space R is formed, we followed the same steps as earlier for forming the attention, but now the training was more effective as the gradient was propagating effectively and was not choked by the large space.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"As the convolution architecture is also simultaneously trained, attention mechanism was not burdened as before, to learn over the large space of entities and relations.
",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Another point that needs to be mentioned here is regarding ranking/ordering items in the clusters, we have done experiments to verify the ordering does not affect the final result.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"We have verified this by randomly shuffling the entities/relationships in every clusters and the ac-
curacy output remained within an error bound of ±0.5%.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"In various permutations, the representations learned by the convolution operator for clusters varies, but it does not affect the overall results.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"Regarding the interpretation of what convolution operator learns, the operator is applied along each dimension of the entity/relationship vector, to learn a representation of the clusters.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"This representation includes information from relevant entities in the cluster, as the relevant entities varies across tasks, the representation learned using convolution also adapts accordingly.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
"It is analogous to learning relevant features from an image, in our case the convolution layer learns the features focusing on relevant entities/relations in a cluster pertaining to the task.",3.3 Convolution-based Entity and Relation Cluster Representation,[0],[0]
Our experiments were designed to analyze whether a deep learning model is being improved when it has access to KG facts from a relevant source.,4 Experiments and Evaluations,[0],[0]
"The selection of knowledge graph has to be pertinent to the task at hand, as currently there is no single knowledge base that contains multiple kinds of information and can cater to all tasks.",4 Experiments and Evaluations,[0],[0]
We illustrate with results that the performance of a deep learning model improves when it has access to relevant facts.,4 Experiments and Evaluations,[0],[0]
"We also illustrate that as the model learns faster with access to knowledge bases, we can train deep learning models with substantially less training data, without compromising on the accuracy.",4 Experiments and Evaluations,[0],[0]
"In the subsequent section we briefly describe the datasets and associated Knowledge Bases used.
Datasets and Relevant Knowledge Graphs
In our experiments, we have mainly used the popular text classification dataset 20Newsgroups (Lichman, 2013) and the Natural Language Inference dataset, Stanford Natural Language Inference (SNLI) corpus (Bowman et al., 2015).",4 Experiments and Evaluations,[0],[0]
"We have also done experiments on DBPedia ontology classification dataset1, with a very strong baseline.",4 Experiments and Evaluations,[0],[0]
"These datasets are chosen as they share domain knowledge with two most popular knowledge bases, Freebase (FB15k) (Bollacker et al., 2008) and WordNet (WN18) (Bordes et al., 2013).",4 Experiments and Evaluations,[0],[0]
"The training and test size of the datasets are mentioned in Table 1.
1http://wiki.dbpedia.org/ services-resources/dbpedia-data-set-2014
Freebase (FB15k) (Bollacker et al., 2008) contains facts about people, places and things (contains 14904 entities, 1345 relations & 4.9M fact triples), which is useful for text classification in 20Newsgroups (Lichman, 2013) dataset.",4 Experiments and Evaluations,[0],[0]
"On the other hand, WordNet (WN18) (Bordes et al., 2013) (has 40943 entities, 18 relations & 1.5M fact triples) contains facts about common day-to-day things (example: furniture includes bed), which can help in inference tasks like SNLI.",4 Experiments and Evaluations,[0],[0]
"Both the knowledge bases are directed graphs, due to fewer number of relations WN18 the entities are more likely to be connected using the same type of relations.",4 Experiments and Evaluations,[0],[0]
For experiments relating to both the datasets 20Newsgroups & SNLI we used the standard LSTM as the classification module.,4 Experiments and Evaluations,[0],[0]
"As iterated earlier, our KG based fact retrieval is independent of the base model used.",4 Experiments and Evaluations,[0],[0]
We show improvement in performance using the proposed models by KG fact retrieval.,4 Experiments and Evaluations,[0],[0]
We use classification accuracy of the test set as our evaluation metric.,4 Experiments and Evaluations,[0],[0]
All experiments were carried on a Dell Precision Tower 7910 server with Quadro M5000 GPU with 8 GB of memory.,4.1 Experimental Setup,[0],[0]
"The models were trained using the Adam’s Optimizer (Kingma and Ba, 2014) in a stochastic gradient descent (Bottou, 2012) fashion.",4.1 Experimental Setup,[0],[0]
"The models were implemented using TensorFlow (Abadi et al., 2015).",4.1 Experimental Setup,[0],[0]
The relevant hyperparameters are listed in Table 2.,4.1 Experimental Setup,[0],[0]
"The word embeddings for the experiments were obtained using the pre-trained GloVe (Pennington et al., 2014)2 vectors.",4.1 Experimental Setup,[0],[0]
"For words missing in the pre-trained vectors, the local GloVe vectors which was trained on the corresponding dataset was used.",4.1 Experimental Setup,[0],[0]
Table 3 shows the results of test accuracy of the various methods proposed on the datasets News20 & SNLI.,4.2 Results & Discussion,[0],[0]
"We observe that incorporation of KG facts using the basic vanilla model improves the performance slightly, as the retrieval model was
2http://nlp.stanford.edu/data/glove.840B.300d.zip
not getting trained effectively.",4.2 Results & Discussion,[0],[0]
The convolutionbased model shows significant improvement over the normal LSTM classification.,4.2 Results & Discussion,[0],[0]
While tuning the parameters of the convolution for clustered entities/relations it was observed that smaller stride length and longer max-pool window improved performance.,4.2 Results & Discussion,[0],[0]
"For News20 dataset we show an improvement of almost 3% and for SNLI an improvement of almost 5%.
",4.2 Results & Discussion,[0],[0]
The work is motivated more from the perspective of whether incorporation of world knowledge will improve any deep learning model rather than beating the state-of-the-art performance.,4.2 Results & Discussion,[0],[0]
"Although LSTM is used to encode the input for the model as well as the retrieval vector, as mentioned earlier, these two modules need not be same.",4.2 Results & Discussion,[0],[0]
For encoding the input any complex state-of-the-art model can be used.,4.2 Results & Discussion,[0],[0]
LSTM has also been used to generate the retrieval vector.,4.2 Results & Discussion,[0],[0]
"For DBPedia ontology classification dataset, we have used a strong baseline of 98.6%, and after augmenting it with KG (Freebase) using convolution based model we saw an improvement of ∼0.2%.",4.2 Results & Discussion,[0],[0]
"As the baseline is stronger, the improvement quantum has decreased.",4.2 Results & Discussion,[0],[0]
This is quite intuitive as complex models are selfsufficient in learning from the data by itself and therefore the room available for further improvement is relatively less.,4.2 Results & Discussion,[0],[0]
"The improvement as observed in the experiments is significant in weaker learning models, however it is also capable of improving stronger baselines as is evident from the results of DBPedia dataset.",4.2 Results & Discussion,[0],[0]
"We hypothesized that as Knowledge Graph is feeding more information to the model, we can achieve better performance with less training data.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
To verify this we have performed experiments on varying dataset fractions for 20Newsgroups dataset as shown in Figure 5.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"From the plot, we observe that KG augmented LSTM with 70% data outperforms the baseline model with full dataset support, thereby reducing the dependency on labeled data by 30%.
",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
We also designed an experiment to compare the accuracy of the baseline model trained on full training data and compared it with the accuracy of the KG augmented model trained with just 70% of the training data for 20Newsgroups and SNLI datasets.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
The accuracy and training loss plots across training epochs is given in Figure 6.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"Even with just 70% of the data, KG augmented model is able to achieve better accuracy compared to the vanilla LSTM model trained on the full data.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
This clearly indicates that relevant information pertaining to the task is retrieved from the knowledge graph and the training loss reduction is not due to lesser data only.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
Also note that training loss is substantially less for KG LSTM compared to normal LSTM when the dataset size is reduced.,4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"This result is very promising, to reduce the large labeled training data requirement of large deep learning
models, which is hard to come by.",4.3 Reducing Dataset Size Requirements for Training Deep Learning Models,[0],[0]
"The basic idea of infusing general world knowledge for learning tasks, especially for natural language processing, has not been attempted before.",5 Relevant Previous Work,[0],[0]
"For multi-label image classification, the use of KGs has been pursued recently by (Marino et al., 2016).",5 Relevant Previous Work,[0],[0]
"In this work, they first obtain labels of the input data (using a different model), use these labels to populate features from the KG and in turn use these features back for the final classification.",5 Relevant Previous Work,[0],[0]
"For NLP tasks the information needed may not necessarily depend on the final class, and we are directly using all the information available in the input for populating the relevant information from the knowledge graphs.",5 Relevant Previous Work,[0],[0]
"Our attempt is very different from Transfer Learning (Pan and Yang, 2010).
",5 Relevant Previous Work,[0],[0]
In Transfer Learning the focus is on training the model for one task and tuning the trained model to use it for another task.,5 Relevant Previous Work,[0],[0]
This is heavily dependent on the alignment between source task and destination task and transferred information is in the model.,5 Relevant Previous Work,[0],[0]
"In our case, general world knowledge is being infused into the learning model for any given task.",5 Relevant Previous Work,[0],[0]
"By the same logic, our work is different from domain adaptation (Glorot et al., 2011) as well.",5 Relevant Previous Work,[0],[0]
"There has been attempts to use world knowledge (Song and Roth, 2017) for creating more labeled training data and providing distant supervision etc.",5 Relevant Previous Work,[0],[0]
"Incorporating Inductive Biases (Ridgeway, 2016) based on the known information about a domain onto the structure of the learned models, is an active area of research.",5 Relevant Previous Work,[0],[0]
However our motivation and approach is fundamentally different from these works.,5 Relevant Previous Work,[0],[0]
In this work we have illustrated the need for incorporating world knowledge in training task specific models.,6 Conclusion & Future Work,[0],[0]
We presented a novel convolutionbased architecture to reduce the attention space over entities and relations that outperformed other models.,6 Conclusion & Future Work,[0],[0]
"With significant improvements over the vanilla baselines for two well known datasets, we have illustrated the efficacy of our proposed methods in enhancing the performance of deep learning models.",6 Conclusion & Future Work,[0],[0]
We showcased that the proposed method can be used to reduce labeled training data requirements of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Although in this work we focused only on NLP tasks and using LSTM as the baseline model, the proposed approach is applicable for other domain tasks as well, with more complicated deep learning models as baseline.",6 Conclusion & Future Work,[0],[0]
To the best of our knowledge this is the first attempt at infusing general world knowledge for task specific training of deep learning models.,6 Conclusion & Future Work,[0],[0]
"Being the first work of its kind, there is a lot of scope for improvement.",6 Conclusion & Future Work,[0],[0]
A more sophisticated model which is able to retrieve facts more efficiently from millions of entries can be formulated.,6 Conclusion & Future Work,[0],[0]
"Currently we have focused only on a flat attention structure, a hierarchical attention mechanism would be more suitable.",6 Conclusion & Future Work,[0],[0]
The model uses soft attention to enable training by simple stochastic gradient descent.,6 Conclusion & Future Work,[0],[0]
Hard attention over facts using reinforcement learning can be pursued further.,6 Conclusion & Future Work,[0],[0]
"This will further help in selection of multifacts, that are not of similar type but relevant to the task.",6 Conclusion & Future Work,[0],[0]
"The convolution based model, helped to reduce the space over entities and relationships over which attention had to be generated.",6 Conclusion & Future Work,[0],[0]
"However more sophisticated techniques using similarity based search (Wang et al., 2014a; Mu and Liu, 2017) can be pursued towards this purpose.",6 Conclusion & Future Work,[0],[0]
"The results from the initial experiments illustrates the effectiveness of our proposed approach, advocating further investigations in these directions.",6 Conclusion & Future Work,[0],[0]
"Machine Learning has been the quintessential solution for many AI problems, but learning models are heavily dependent on specific training data.",abstractText,[0],[0]
"Some learning models can be incorporated with prior knowledge using a Bayesian setup, but these learning models do not have the ability to access any organized world knowledge on demand.",abstractText,[0],[0]
"In this work, we propose to enhance learning models with world knowledge in the form of Knowledge Graph (KG) fact triples for Natural Language Processing (NLP) tasks.",abstractText,[0],[0]
Our aim is to develop a deep learning model that can extract relevant prior support facts from knowledge graphs depending on the task using attention mechanism.,abstractText,[0],[0]
We introduce a convolutionbased model for learning representations of knowledge graph entity and relation clusters in order to reduce the attention space.,abstractText,[0],[0]
We show that the proposed method is highly scalable to the amount of prior information that has to be processed and can be applied to any generic NLP task.,abstractText,[0],[0]
"Using this method we show significant improvement in performance for text classification with 20Newsgroups (News20) & DBPedia datasets, and natural language inference with Stanford Natural Language Inference (SNLI) dataset.",abstractText,[0],[0]
"We also demonstrate that a deep learning model can be trained with substantially less amount of labeled training data, when it has access to organized world knowledge in the form of a knowledge base.",abstractText,[0],[0]
Learning beyond datasets: Knowledge Graph Augmented Neural Networks for Natural language Processing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 451–462 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1042",text,[0],[0]
Multilingual word embeddings have attracted a lot of attention in recent times.,1 Introduction,[0],[0]
"In addition to having a direct application in inherently crosslingual tasks like machine translation (Zou et al., 2013) and crosslingual entity linking (Tsai and Roth, 2016), they provide an excellent mechanism for transfer learning, where a model trained in a resource-rich language is transferred to a less-resourced one, as shown with part-of-speech tagging (Zhang et al., 2016), parsing (Xiao and Guo, 2014) and document classification (Klementiev et al., 2012).
",1 Introduction,[0],[0]
"Most methods to learn these multilingual word embeddings make use of large parallel corpora (Gouws et al., 2015; Luong et al., 2015), but there have been several proposals to relax this requirement, given its scarcity in most language pairs.",1 Introduction,[0],[0]
"A possible relaxation is to use document-aligned or label-aligned comparable corpora (Søgaard et al.,
2015; Vulić and Moens, 2016; Mogadala and Rettinger, 2016), but large amounts of such corpora are not always available for some language pairs.
",1 Introduction,[0],[0]
"An alternative approach that we follow here is to independently train the embeddings for each language on monolingual corpora, and then learn a linear transformation to map the embeddings from one space into the other by minimizing the distances in a bilingual dictionary, usually in the range of a few thousand entries (Mikolov et al., 2013a; Artetxe et al., 2016).",1 Introduction,[0],[0]
"However, dictionaries of that size are not readily available for many language pairs, specially those involving less-resourced languages.
",1 Introduction,[0],[0]
"In this work, we reduce the need of large bilingual dictionaries to much smaller seed dictionaries.",1 Introduction,[0],[0]
"Our method can work with as little as 25 word pairs, which are straightforward to obtain assuming some basic knowledge of the languages involved.",1 Introduction,[0],[0]
"The method can also work with trivially generated seed dictionaries of numerals (i.e. 1-1, 2-2, 3-3, 4-4...) making it possible to learn bilingual word embeddings without any real bilingual data.",1 Introduction,[0],[0]
"In either case, we obtain very competitive results, comparable to other state-of-the-art methods that make use of much richer bilingual resources.
",1 Introduction,[0],[0]
"The proposed method is an extension of existing mapping techniques, where the dictionary is used to learn the embedding mapping and the embedding mapping is used to induce a new dictionary iteratively in a self-learning fashion (see Figure 1).",1 Introduction,[0],[0]
"In spite of its simplicity, our analysis of the implicit optimization objective reveals that the method is exploiting the structural similarity of independently trained embeddings.
",1 Introduction,[0],[0]
We analyze previous work in Section 2.,1 Introduction,[0],[0]
"Section 3 describes the self-learning framework, while Section 4 presents the experiments.",1 Introduction,[0],[0]
"Section 5 analyzes the underlying optimization objective, and Section 6 presents an error analysis.
",1 Introduction,[0],[0]
451,1 Introduction,[0],[0]
"We will first focus on bilingual embedding mappings, which are the basis of our proposals, and then on other unsupervised and weakly supervised methods to learn bilingual word embeddings.",2 Related work,[0],[0]
"Methods to induce bilingual mappings work by independently learning the embeddings in each language using monolingual corpora, and then learning a transformation from one embedding space into the other based on a bilingual dictionary.
",2.1 Bilingual embedding mappings,[0],[0]
"The first of such methods is due to Mikolov et al. (2013a), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries.",2.1 Bilingual embedding mappings,[0],[0]
"The same optimization objective is used by Zhang et al. (2016), who constrain the transformation matrix to be orthogonal.",2.1 Bilingual embedding mappings,[0],[0]
"Xing et al. (2015) incorporate length normalization in the training of word embeddings and maximize the cosine similarity instead, enforcing the orthogonality constraint to preserve the length normalization after the mapping.",2.1 Bilingual embedding mappings,[0],[0]
"Finally, Lazaridou et al. (2015) use max-margin optimization with intruder negative sampling.
",2.1 Bilingual embedding mappings,[0],[0]
"Instead of learning a single linear transformation from the source language into the target language, Faruqui and Dyer (2014) use canonical correlation analysis to map both languages to a shared vector space.",2.1 Bilingual embedding mappings,[0],[0]
"Lu et al. (2015) extend this work and apply deep canonical correlation analysis to learn non-linear transformations.
",2.1 Bilingual embedding mappings,[0],[0]
"Artetxe et al. (2016) propose a general framework that clarifies the relation between Mikolov et al. (2013a), Xing et al. (2015), Faruqui and Dyer (2014) and Zhang et al. (2016) as variants of the
same core optimization objective, and show that a new variant is able to surpass them all.",2.1 Bilingual embedding mappings,[0],[0]
"While most of the previous methods use gradient descent, Artetxe et al. (2016) propose an efficient analytical implementation for those same methods, recently extended by Smith et al. (2017) to incorporate dimensionality reduction.
",2.1 Bilingual embedding mappings,[0],[0]
"A prominent application of bilingual embedding mappings, with a direct application in machine translation (Zhao et al., 2015), is bilingual lexicon extraction, which is also the main evaluation method.",2.1 Bilingual embedding mappings,[0],[0]
"More specifically, the learned mapping is used to induce the translation of source language words that were missing in the original dictionary, usually by taking their nearest neighbor word in the target language according to cosine similarity, although Dinu et al. (2015) and Smith et al. (2017) propose alternative retrieval methods to address the hubness problem.",2.1 Bilingual embedding mappings,[0],[0]
"As mentioned before, our method works with as little as 25 word pairs, while the methods discussed previously use thousands of pairs.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The only exception in this regard is the work by Zhang et al. (2016), who only use 10 word pairs with good results on transfer learning for part-of-speech tagging.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our experiments will show that, although their method captures coarse-grained relations, it fails on finer-grained tasks like bilingual lexicon induction.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Bootstrapping methods similar to ours have been previously proposed for traditional countbased vector space models (Peirsman and Padó, 2010; Vulić and Moens, 2013).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, while previous techniques incrementally build a high-
Algorithm 1 Traditional framework Input: X (source embeddings)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: W ← LEARN MAPPING(X , Z, D) 2: D ← LEARN DICTIONARY(X , Z, W ) 3: EVALUATE DICTIONARY(D)
dimensional model where each axis encodes the co-occurrences with a specific word and its equivalent in the other language, our method works with low-dimensional pre-trained word embeddings, which are more widely used nowadays.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
A practical aspect for reducing the need of bilingual supervision is on the design of the seed dictionary.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"This is analyzed in depth by Vulić and Korhonen (2016), who propose using documentaligned corpora to extract the training dictionary.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"A more common approach is to rely on shared words and cognates (Peirsman and Padó, 2010; Smith et al., 2017), eliminating the need of bilingual data in practice.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Our use of shared numerals exploits the same underlying idea, but relies on even less bilingual evidence and should thus generalize better to distant language pairs.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Miceli Barone (2016) and Cao et al. (2016) go one step further and attempt to learn bilingual embeddings without any bilingual evidence.,2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"The former uses adversarial autoencoders (Makhzani et al., 2016), combining an encoder that maps the source language embeddings into the target language, a decoder that reconstructs the original embeddings, and a discriminator that distinguishes mapped embeddings from real target language embeddings, whereas the latter adds a regularization term to the training of word embeddings that pushes the mean and variance of each dimension in different languages close to each other.",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Although promising, the reported performance in both cases is poor in comparison to other methods.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Finally, the induction of bilingual knowledge from monolingual corpora is closely related to the decipherment scenario, for which models that incorporate word embeddings have also been proposed (Dou et al., 2015).",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"However, decipherment is only concerned with translating text from one language to another and relies on complex statistical models that are designed specifically for that purpose, while our approach is more general and learns task-independent multilingual embeddings.
",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Algorithm 2 Proposed self-learning framework Input: X (source embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
Input: Z (target embeddings),2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"Input: D (seed dictionary)
1: repeat 2:",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"W ← LEARN MAPPING(X , Z, D) 3: D ← LEARN DICTIONARY(X , Z, W ) 4: until convergence criterion 5: EVALUATE DICTIONARY(D)",2.2 Unsupervised and weakly supervised bilingual embeddings,[0],[0]
"As discussed in Section 2.1, a common evaluation task (and practical application) of bilingual embedding mappings is to induce bilingual lexicons, that is, to obtain the translation of source words that were missing in the training dictionary, which are then compared to a gold standard test dictionary for evaluation.",3 Proposed self-learning framework,[0],[0]
"This way, one can say that the seed (train) dictionary is used to learn a mapping, which is then used to induce a better dictionary (at least in the sense that it is larger).",3 Proposed self-learning framework,[0],[0]
"Algorithm 1 summarizes this framework.
",3 Proposed self-learning framework,[0],[0]
"Following this observation, we propose to use the output dictionary in Algorithm 1 as the input of the same system in a self-learning fashion which, assuming that the output dictionary was indeed better than the original one, should serve to learn a better mapping and, consequently, an even better dictionary the second time.",3 Proposed self-learning framework,[0],[0]
The process can then be repeated iteratively to obtain a hopefully better mapping and dictionary each time until some convergence criterion is met.,3 Proposed self-learning framework,[0],[0]
"Algorithm 2 summarizes this alternative framework that we propose.
",3 Proposed self-learning framework,[0],[0]
Our method can be combined with any embedding mapping and dictionary induction technique (see Section 2.1).,3 Proposed self-learning framework,[0],[0]
"However, efficiency turns out to be critical for a variety of reasons.",3 Proposed self-learning framework,[0],[0]
"First of all, by enclosing the learning logic in a loop, the total training time is increased by the number of iterations.",3 Proposed self-learning framework,[0],[0]
"Even more importantly, our framework requires to explicitly build the entire dictionary at each iteration, whereas previous work tends to induce the translation of individual words ondemand later at runtime.",3 Proposed self-learning framework,[0],[0]
"Moreover, from the second iteration onwards, it is this induced, full dictionary that has to be used to learn the embedding mapping, and not the considerably smaller seed dictionary as it is typically done.",3 Proposed self-learning framework,[0],[0]
"In the following two subsections, we respectively describe the embedding mapping method and the dictionary in-
duction method that we adopt in our work with these efficiency requirements in mind.",3 Proposed self-learning framework,[0],[0]
"As discussed in Section 2.1, most previous methods to learn embedding mappings use variants of gradient descent.",3.1 Embedding mapping,[0],[0]
"Among the more efficient exact alternatives, we decide to adopt the one by Artetxe et al. (2016) for its simplicity and good results as reported in their paper.",3.1 Embedding mapping,[0],[0]
"We next present their method, adapting the formalization to explicitly incorporate the dictionary as required by our self-learning algorithm.
",3.1 Embedding mapping,[0],[0]
Let X and Z denote the word embedding matrices in two languages so that Xi∗ corresponds to the ith source language word embedding and Zj∗ corresponds to the jth target language embedding.,3.1 Embedding mapping,[0],[0]
"While Artetxe et al. (2016) assume these two matrices are aligned according to the dictionary, we drop this assumption and represent the dictionary explicitly as a binary matrix D, so that Dij = 1 if the ith source language word is aligned with the jth target language word.",3.1 Embedding mapping,[0],[0]
The goal is then to find the optimal mapping matrix W ∗ so that the sum of squared Euclidean distances between the mapped source embeddings Xi∗W and target embeddings Zj∗,3.1 Embedding mapping,[0],[0]
"for the dictionary entries Dij is minimized:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg min W
∑
i
∑
j
Dij ||Xi∗W",3.1 Embedding mapping,[0],[0]
"− Zj∗||2
Following Artetxe et al. (2016), we length normalize and mean center the embedding matrices X and Z in a preprocessing step, and constrain W to be an orthogonal matrix (i.e. WW T = W TW = I), which serves to enforce monolingual invariance, preventing a degradation in monolingual performance while yielding to better bilingual mappings.",3.1 Embedding mapping,[0],[0]
"Under such orthogonality constraint, minimizing the squared Euclidean distance becomes equivalent to maximizing the dot product, so the above optimization objective can be reformulated as follows:
",3.1 Embedding mapping,[0],[0]
"W ∗ = arg max W
Tr ( XWZTDT )
where Tr (·) denotes the trace operator (the sum of all the elements in the main diagonal).",3.1 Embedding mapping,[0],[0]
"The optimal orthogonal solution for this problem is given by W ∗ = UV T , where",3.1 Embedding mapping,[0],[0]
XTDZ = UΣV T is the singular value decomposition of XTDZ.,3.1 Embedding mapping,[0],[0]
"Since the dictionary matrix D is sparse, this can be efficiently computed in linear time with respect to the number of dictionary entries.",3.1 Embedding mapping,[0],[0]
"As discussed in Section 2.1, practically all previous work uses nearest neighbor retrieval for word translation induction based on embedding mappings.",3.2 Dictionary induction,[0],[0]
"In nearest neighbor retrieval, each source language word is assigned the closest word in the target language.",3.2 Dictionary induction,[0],[0]
"In our work, we use the dot product between the mapped source language embeddings and the target language embeddings as the similarity measure, which is roughly equivalent to cosine similarity given that we apply length normalization followed by mean centering as a preprocessing step (see Section 3.1).",3.2 Dictionary induction,[0],[0]
"This way, following the notation in Section 3.1, we set Dij = 1 if j = argmaxk (Xi∗W )",3.2 Dictionary induction,[0],[0]
·Zk∗ and,3.2 Dictionary induction,[0],[0]
"Dij = 0 otherwise1.
",3.2 Dictionary induction,[0],[0]
"While we find that independently computing the similarity measure between all word pairs is prohibitively slow, the computation of the entire similarity matrix XWZT can be easily vectorized using popular linear algebra libraries, obtaining big performance gains.",3.2 Dictionary induction,[0],[0]
"However, the resulting similarity matrix is often too large to fit in memory when using large vocabularies.",3.2 Dictionary induction,[0],[0]
"For that reason, instead of computing the entire similarity matrix XWZT in a single step, we iteratively compute submatrices of it using vectorized matrix multiplication, find their corresponding maxima each time, and then combine the results.",3.2 Dictionary induction,[0],[0]
"In this section, we experimentally test the proposed method in bilingual lexicon induction and crosslingual word similarity.",4 Experiments and results,[0],[0]
"Subsection 4.1 describes the experimental settings, while Subsections 4.2 and 4.3 present the results obtained in each of the tasks.",4 Experiments and results,[0],[0]
The code and resources necessary to reproduce our experiments are available at https://github.com/artetxem/ vecmap.,4 Experiments and results,[0],[0]
"For easier comparison with related work, we evaluated our mappings on bilingual lexicon induction using the public English-Italian dataset by Dinu et al. (2015), which includes monolingual word embeddings in both languages together with a bilingual dictionary split in a training set and a
1Note that we induce the dictionary entries starting from the source language words.",4.1 Experimental settings,[0],[0]
"We experimented with other alternatives in development, with minor differences.
test set2.",4.1 Experimental settings,[0],[0]
"The embeddings were trained with the word2vec toolkit with CBOW and negative sampling (Mikolov et al., 2013b)3, using a 2.8 billion word corpus for English (ukWaC + Wikipedia + BNC) and a 1.6 billion word corpus for Italian (itWaC).",4.1 Experimental settings,[0],[0]
"The training and test sets were derived from a dictionary built form Europarl word alignments and available at OPUS (Tiedemann, 2012), taking 1,500 random entries uniformly distributed in 5 frequency bins as the test set and the 5,000 most frequent of the remaining word pairs as the training set.
",4.1 Experimental settings,[0],[0]
"In addition to English-Italian, we selected two other languages from different language families with publicly available resources.",4.1 Experimental settings,[0],[0]
We thus created analogous datasets for English-German and English-Finnish.,4.1 Experimental settings,[0],[0]
"In the case of German, the embeddings were trained on the 0.9 billion word corpus SdeWaC, which is part of the WaCky collection (Baroni et al., 2009) that was also used for English and Italian.",4.1 Experimental settings,[0],[0]
"Given that Finnish is not included in this collection, we used the 2.8 billion word Common Crawl corpus provided at WMT 20164 instead, which we tokenized using the Stanford Tokenizer (Manning et al., 2014).",4.1 Experimental settings,[0],[0]
"In addition to that, we created training and test sets for both pairs from their respective Europarl dictionaries from OPUS following the exact same procedure used for English-Italian, and the word embeddings were also trained using the same configuration as Dinu et al. (2015).
",4.1 Experimental settings,[0],[0]
"Given that the main focus of our work is on small seed dictionaries, we created random subsets of 2,500, 1,000, 500, 250, 100, 75, 50 and 25 entries from the original training dictionaries of 5,000 entries.",4.1 Experimental settings,[0],[0]
"This was done by shuffling once the training dictionaries and taking their first k entries, so it is guaranteed that each dictionary is a strict subset of the bigger dictionaries.
",4.1 Experimental settings,[0],[0]
"In addition to that, we explored using automatically generated dictionaries as a shortcut to practical unsupervised learning.",4.1 Experimental settings,[0],[0]
"For that purpose, we created numeral dictionaries, consisting of words matching the [0-9]+ regular expression in both vocabularies (e.g. 1-1, 2-2, 3-3, 1992-1992
2http://clic.cimec.unitn.it/ ˜georgiana.dinu/down/
3The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10, and the vocabulary was restricted to the 200,000 most frequent words
4http://www.statmt.org/wmt16/ translation-task.html
etc.).",4.1 Experimental settings,[0],[0]
"The resulting dictionary had 2772 entries for English-Italian, 2148 for English-German, and 2345 for English-Finnish.",4.1 Experimental settings,[0],[0]
"While more sophisticated approaches are possible (e.g. involving the edit distance of all words), we believe that this method is general enough that should work with practically any language pair, as Arabic numerals are often used even in languages with a different writing system (e.g. Chinese and Russian).
",4.1 Experimental settings,[0],[0]
"While bilingual lexicon induction is a standard evaluation task for seed dictionary based methods like ours, it is unsuitable for bilingual corpus based methods, as statistical word alignment already provides a reliable way to derive dictionaries from bilingual corpora and, in fact, this is how the test dictionary itself is built in our case.",4.1 Experimental settings,[0],[0]
"For that reason, we carried out some experiments in crosslingual word similarity as a way to test our method in a different task and allowing to compare it to systems that use richer bilingual data.",4.1 Experimental settings,[0],[0]
"There are no many crosslingual word similarity datasets, and we used the RG-65 and WordSim353 crosslingual datasets for English-German and the WordSim-353 crosslingual dataset for EnglishItalian as published by Camacho-Collados et al. (2015) 5.
",4.1 Experimental settings,[0],[0]
"As for the convergence criterion, we decide to stop training when the improvement on the average dot product for the induced dictionary falls below a given threshold from one iteration to the next.",4.1 Experimental settings,[0],[0]
"After length normalization, the dot product ranges from -1 to 1, so we decide to set this threshold at 1e-6, which we find to be a very conservative value yet enough that training takes a reasonable amount of time.",4.1 Experimental settings,[0],[0]
"The curves in the next section confirm that this was a reasonable choice.
",4.1 Experimental settings,[0],[0]
"This convergence criterion is usually met in less than 100 iterations, each of them taking 5 minutes on a modest desktop computer (Intel Core i5-4670 CPU with 8GiB of RAM), including the induction of a dictionary of 200,000 words at each iteration.",4.1 Experimental settings,[0],[0]
"For the experiments on bilingual lexicon induction, we compared our method with those proposed by Mikolov et al. (2013a), Xing et al. (2015), Zhang et al. (2016) and Artetxe et al. (2016), all of them implemented as part of the framework proposed by the latter.",4.2 Bilingual lexicon induction,[0],[0]
"The results ob-
5http://lcl.uniroma1.it/ similarity-datasets/
tained with the 5,000 entry, 25 entry and the numerals dictionaries for all the 3 language pairs are given in Table 1.
",4.2 Bilingual lexicon induction,[0],[0]
"The results for the 5,000 entry dictionaries show that our method is comparable or even better than the other systems.",4.2 Bilingual lexicon induction,[0],[0]
"As another reference, the best published results using nearest-neighbor retrieval are due to Lazaridou et al. (2015), who report an accuracy of 40.20% for the full EnglishItalian dictionary, almost at pair with our system (39.67%).
",4.2 Bilingual lexicon induction,[0],[0]
"In any case, the main focus of our work is on smaller dictionaries, and it is under this setting that our method really stands out.",4.2 Bilingual lexicon induction,[0],[0]
"The 25 entry and numerals columns in Table 1 show the results for this setting, where all previous methods drop dramatically, falling below 1% accuracy in all cases.",4.2 Bilingual lexicon induction,[0],[0]
"The method by Zhang et al. (2016) also obtains poor results with small dictionaries, which reinforces our hypothesis in Section 2.2 that their method can only capture coarse-grain bilingual relations for small dictionaries.",4.2 Bilingual lexicon induction,[0],[0]
"In contrast, our proposed method obtains very competitive results for all dictionaries, with a difference of only 1-2 points between the full dictionary and both the 25 entry dictionary and the numerals dictionary in all three languages.",4.2 Bilingual lexicon induction,[0],[0]
"Figure 2 shows the curve of the English-Italian accuracy for different seed dictionary sizes, confirming this trend.
",4.2 Bilingual lexicon induction,[0],[0]
"Finally, it is worth mentioning that, even if all the three language pairs show the same general behavior, there are clear differences in their absolute accuracy numbers, which can be attributed to the linguistic proximity of the languages involved.",4.2 Bilingual lexicon induction,[0],[0]
"In particular, the results for English-Finnish are about 10 points below the rest, which is explained by the fact that Finnish is a non-indoeuropean agglutinative language, making the task considerably more difficult for this language pair.",4.2 Bilingual lexicon induction,[0],[0]
"In this regard, we believe that the good results with small dictionaries are a strong indication of the robustness of our method, showing that it is able to learn good bilingual mappings from very little bilingual ev-
idence even for distant language pairs where the structural similarity of the embedding spaces is presumably weaker.",4.2 Bilingual lexicon induction,[0],[0]
"In addition to the baseline systems in Section 4.2, in the crosslingual similarity experiments we also tested the method by Luong et al. (2015), which is the state-of-the-art for bilingual word embeddings based on parallel corpora (Upadhyay et al., 2016)6.",4.3 Crosslingual word similarity,[0],[0]
"As this method is an extension of word2vec, we used the same hyperparameters as for the monolingual embeddings when possible (see Section 4.1), and leave the default ones otherwise.",4.3 Crosslingual word similarity,[0],[0]
"We used Europarl as our parallel corpus to train this method as done by the authors, which consists of nearly 2 million parallel sentences.
",4.3 Crosslingual word similarity,[0],[0]
"As shown in the results in Table 2, our method obtains the best results in all cases, surpassing the rest of the dictionary-based methods by 1-3 points depending on the dataset.",4.3 Crosslingual word similarity,[0],[0]
"But, most importantly, it does not suffer from any significant degradation for using smaller dictionaries and, in fact, our method gets better results using the 25 entry dictionary or the numeral list as the only bilingual evidence than any of the baseline systems using much richer resources.
",4.3 Crosslingual word similarity,[0],[0]
"The relatively poor results of Luong et al. (2015) can be attributed to the fact that the dictionary based methods make use of much bigger monolingual corpora, while methods based on parallel corpora are restricted to smaller corpora.",4.3 Crosslingual word similarity,[0],[0]
"However, it is not clear how to introduce monolingual corpora on those methods.",4.3 Crosslingual word similarity,[0],[0]
"We did run some experiments with BilBOWA (Gouws et al., 2015), which supports training in monolingual corpora in addition to bilingual corpora, but obtained very poor results7.",4.3 Crosslingual word similarity,[0],[0]
"All in all, our experiments show
6We also tested English-German pre-trained embeddings from Klementiev et al. (2012) and Chandar A P et al. (2014).",4.3 Crosslingual word similarity,[0],[0]
"They both had coverage problems that made the results hard to compare, and, when considering the correlations for the word pairs in their vocabulary, their performance was poor.
",4.3 Crosslingual word similarity,[0],[0]
"7Upadhyay et al. (2016) report similar problems using
that it is better to use large monolingual corpora in combination with very little bilingual data rather than a bilingual corpus of a standard size alone.",4.3 Crosslingual word similarity,[0],[0]
"It might seem somehow surprising at first that, as seen in the previous section, our simple selflearning approach is able to learn high quality bilingual embeddings from small seed dictionaries instead of falling in degenerated solutions.",5 Global optimization objective,[0],[0]
"In this section, we try to shed light on our approach, and give empirical evidence supporting our claim.
",5 Global optimization objective,[0],[0]
"More concretely, we argue that, for the embedding mapping and dictionary induction methods described in Section 3, the proposed selflearning framework is implicitly solving the following global optimization problem8:
W ∗ = arg max W
∑
i
max j
(Xi∗W ) · Zj∗
s.t. WW T = W TW",5 Global optimization objective,[0],[0]
=,5 Global optimization objective,[0],[0]
"I
Contrary to the optimization objective for W in Section 3.1, the global optimization objective does not refer to any dictionary, and maximizes the similarity between each source language word and its closest target language word.",5 Global optimization objective,[0],[0]
"Intuitively, a random solution would map source language embeddings to seemingly random locations in the target language space, and it would thus be unlikely that
BilBOWA.",5 Global optimization objective,[0],[0]
"8While we restrict our formal analysis to the embedding mapping and dictionary induction method that we use, the general reasoning should be valid for other choices as well.
",5 Global optimization objective,[0],[0]
"they have any target language word nearby, making the optimization value small.",5 Global optimization objective,[0],[0]
"In contrast, a good solution would map source language words close to their translation equivalents in the target language space, and they would thus have their corresponding embeddings nearby, making the optimization value large.",5 Global optimization objective,[0],[0]
"While it is certainly possible to build degenerated solutions that take high optimization values for small subsets of the vocabulary, we think that the structural similarity between independently trained embedding spaces in different languages is strong enough that optimizing this function yields to meaningful bilingual mappings when the size of the vocabulary is much larger than the dimensionality of the embeddings.
",5 Global optimization objective,[0],[0]
The reasoning for how the self-learning framework is optimizing this objective is as follows.,5 Global optimization objective,[0],[0]
"At the end of each iteration, the dictionary D is updated to assign, for the current mapping W , each source language word to its closest target language word.",5 Global optimization objective,[0],[0]
"This way, when we update W to maximize the average similarity of these dictionary entries at the beginning of the next iteration, it is guaranteed that the value of the optimization objective will improve (or at least remain the same).",5 Global optimization objective,[0],[0]
"The reason is that the average similarity between each word and what were previously the closest words will be improved if possible, as this is what the updated W directly optimizes (see Section 3.1).",5 Global optimization objective,[0],[0]
"In addition to that, it is also possible that, for some source words, some other target words get closer after the update.",5 Global optimization objective,[0],[0]
"Thanks to this, our self-learning algorithm is guaranteed to converge to a local optimum of the above global objective, behaving like an alternating optimization algorithm for it.
",5 Global optimization objective,[0],[0]
"It is interesting to note that the above reasoning is valid no matter what the the initial solution is, and, in fact, the global optimization objective does not depend on the seed dictionary nor any other
bilingual resource.",5 Global optimization objective,[0],[0]
"For that reason, it should be possible to use a random initialization instead of a small seed dictionary.",5 Global optimization objective,[0],[0]
"However, we empirically observe that this works poorly in practice, as our algorithm tends to get stuck in poor local optima when the initial solution is not good enough.
",5 Global optimization objective,[0],[0]
"The general behavior of our method is reflected in Figure 3, which shows the learning curve for different seed dictionaries according to both the objective function and the accuracy on bilingual lexicon induction.",5 Global optimization objective,[0],[0]
"As it can be seen, the objective function is improved from iteration to iteration and converges to a local optimum just as expected.",5 Global optimization objective,[0],[0]
"At the same time, the learning curves show a strong correlation between the optimization objective and the accuracy, as it can be clearly observed that improving the former leads to an improvement of the latter, confirming our explanations.",5 Global optimization objective,[0],[0]
"Regarding random initialization, the figure shows that the algorithm gets stuck in a poor local optimum of the objective function, which is the reason of the bad performance (0% accuracy) on bilingual lexicon induction, but the proposed optimization objective itself seems to be adequate.
",5 Global optimization objective,[0],[0]
"Finally, we empirically observe that our algorithm learns similar mappings no matter what the seed dictionary was.",5 Global optimization objective,[0],[0]
"We first repeated our experiments on English-Italian bilingual lexicon induction for 5 different dictionaries of 25 entries, obtaining an average accuracy of 38.15% and a standard deviation of only 0.75%.",5 Global optimization objective,[0],[0]
"In addition to that, we observe that the overlap between the predictions made when starting with the full dictionary and the numerals dictionary is 76.00% (60.00% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"At the same time,
37.00% of the test cases are correctly solved by both instances, and it is only 5.07% of the test cases that one of them gets right and the other wrong (34.00% and 8.94% for the 25 entry dictionary).",5 Global optimization objective,[0],[0]
"This suggests that our algorithm tends to converge to similar solutions even for disjoint seed dictionaries, which is in line with our view that we are implicitly optimizing an objective that is independent from the seed dictionary, yet a seed dictionary is necessary to build a good enough initial solution to avoid getting stuck in poor local optima.",5 Global optimization objective,[0],[0]
"For that reason, it is likely that better methods to tackle this optimization problem would allow learning bilingual word embeddings without any bilingual evidence at all and, in this regard, we believe that our work opens exciting opportunities for future research.",5 Global optimization objective,[0],[0]
"So as to better understand the behavior of our system, we performed an error analysis of its output in English-Italian bilingual lexicon induction when starting with the 5,000 entry, the 25 entry and the numeral dictionaries in comparison with the baseline method of Artetxe et al. (2016) with the 5,000 entry dictionary.",6 Error analysis,[0],[0]
"For that purpose, we took 100 random examples from the test set in the [1-5K] frequency bin, another 100 from the [5K20K] frequency bin and 30 from the [100K-200K] frequency bin, and manually analyzed each of the errors made by all the 4 different variants.
",6 Error analysis,[0],[0]
"Our analysis first reveals that, in all the cases, about a third of the translations taken as erroneous according to the gold standard are not so in real-
ity.",6 Error analysis,[0],[0]
This corresponds to both different morphological variants of the gold standard translations (e.g. dichiarato/dichiarò) and other valid translations that were missing in the gold standard (e.g. climb → salita instead of the gold standard scalato).,6 Error analysis,[0],[0]
"This phenomenon is considerably more pronounced in the first frequency bins, which already have a much higher accuracy according to the gold standard.
",6 Error analysis,[0],[0]
"As for the actual errors, we observe that nearly a third of them correspond to named entities for all the different variants.",6 Error analysis,[0],[0]
"Interestingly, the vast majority of the proposed translations in these cases are also named entities (e.g. Ryan→ Jason, John→ Paolo), which are often highly related to the original ones (e.g. Volvo→ BMW, Olympus→ Nikon).",6 Error analysis,[0],[0]
"While these are clear errors, it is understandable that these methods are unable to discriminate between named entities to this degree based solely on the distributional hypothesis, in particular when it comes to common proper names (e.g. John, Andy), and one could design alternative strategies to address this issue like taking the edit distance as an additional signal.
",6 Error analysis,[0],[0]
"For the remaining errors, all systems tend to propose translations that have some degree of relationship with the correct ones, including nearsynonyms (e.g. guidelines → raccomandazioni), antonyms (e.g. sender→ destinatario) and words in the same semantic field (e.g. nominalism→ intuizionismo / innatismo, which are all philosophical doctrines).",6 Error analysis,[0],[0]
"However, there are also a few instances where the relationship is weak or unclear (e.g. loch→ giardini, sweep→ serrare).",6 Error analysis,[0],[0]
"We also observe a few errors that are related to multiwords or collocations (e.g. carrier→ aereo, presumably related to the multiword air carrier / linea aerea), as well as some rare word that is repeated across many translations (Ferruzzi), which could be attributed to the hubness problem (Dinu et al., 2015; Lazaridou et al., 2015).
",6 Error analysis,[0],[0]
"All in all, our error analysis reveals that the baseline method of Artetxe et al. (2016) and the proposed algorithm tend to make the same kind of errors regardless of the seed dictionary used by the latter, which reinforces our interpretation in the previous section regarding an underlying optimization objective that is independent from any training dictionary.",6 Error analysis,[0],[0]
"Moreover, it shows that the quality of the learned mappings is much better than what the raw accuracy numbers might sug-
gest, encouraging the incorporation of these techniques in other applications.",6 Error analysis,[0],[0]
"In this work, we propose a simple self-learning framework to learn bilingual word embedding mappings in combination with any embedding mapping and dictionary induction technique.",7 Conclusions and future work,[0],[0]
"Our experiments on bilingual lexicon induction and crosslingual word similarity show that our method is able to learn high quality bilingual embeddings from as little bilingual evidence as a 25 word dictionary or an automatically generated list of numerals, obtaining results that are competitive with state-of-the-art systems using much richer bilingual resources like larger dictionaries or parallel corpora.",7 Conclusions and future work,[0],[0]
"In spite of its simplicity, a more detailed analysis shows that our method is implicitly optimizing a meaningful objective function that is independent from any bilingual data which, with a better optimization method, might allow to learn bilingual word embeddings in a completely unsupervised manner.
",7 Conclusions and future work,[0],[0]
"In the future, we would like to delve deeper into this direction and fine-tune our method so it can reliably learn high quality bilingual word embeddings without any bilingual evidence at all.",7 Conclusions and future work,[0],[0]
"In addition to that, we would like to explore non-linear transformations (Lu et al., 2015) and alternative dictionary induction methods (Dinu et al., 2015; Smith et al., 2017).",7 Conclusions and future work,[0],[0]
"Finally, we would like to apply our model in the decipherment scenario (Dou et al., 2015).",7 Conclusions and future work,[0],[0]
"We thank the anonymous reviewers for their insightful comments and Flavio Merenda for his help with the error analysis.
",Acknowledgements,[0],[0]
"This research was partially supported by a Google Faculty Award, the Spanish MINECO (TUNER TIN2015-65308-C5-1-R, MUSTER PCIN-2015-226 and TADEEP TIN2015-70214-P, cofunded by EU FEDER), the Basque Government (MODELA KK-2016/00082) and the UPV/EHU (excellence research group).",Acknowledgements,[0],[0]
Mikel Artetxe enjoys a doctoral grant from the Spanish MECD.,Acknowledgements,[0],[0]
"Most methods to learn bilingual word embeddings rely on large parallel corpora, which is difficult to obtain for most language pairs.",abstractText,[0],[0]
"This has motivated an active research line to relax this requirement, with methods that use document-aligned corpora or bilingual dictionaries of a few thousand words instead.",abstractText,[0],[0]
"In this work, we further reduce the need of bilingual resources using a very simple self-learning approach that can be combined with any dictionary-based mapping technique.",abstractText,[0],[0]
"Our method exploits the structural similarity of embedding spaces, and works with as little bilingual evidence as a 25 word dictionary or even an automatically generated list of numerals, obtaining results comparable to those of systems that use richer resources.",abstractText,[0],[0]
Learning bilingual word embeddings with (almost) no bilingual data,title,[0],[0]
"In this paper we propose a spectral method for learning the following binary latent variable model, shown in Figure 1.",1. Introduction,[0],[0]
"The hidden layer, h = (h1, . . .",1. Introduction,[0],[0]
", hd), consists of d binary random variables with an unknown joint distribution Ph : {0, 1}d",1. Introduction,[0],[0]
→,1. Introduction,[0],[0]
"[0, 1].",1. Introduction,[0],[0]
"The observed vector x ∈ Rm with m ≥ d features is modeled as
x = W>h+ σξ, (1)
whereW ∈ Rd×m is an unknown weight matrix assumed to be full rank d. Here, σ",1. Introduction,[0],[0]
"≥ 0 is the noise level and ξ is an additive noise vector independent of h, whose m coordinates are all i.i.d.",1. Introduction,[0],[0]
"zero mean and unit variance random variables.
",1. Introduction,[0],[0]
1Dept.,1. Introduction,[0],[0]
"of Computer Science and Applied Mathematics, Weizmann Institute of Science, Rehovot 7610001, Israel",1. Introduction,[0],[0]
.,1. Introduction,[0],[0]
"2Braun School of Public Health and Community Medicine, The Hebrew University of Jerusalem, Jerusalem 9112102, Israel.",1. Introduction,[0],[0]
"3Program of Applied Mathematics, Yale University, New Haven, CT 06511, USA.",1. Introduction,[0],[0]
"Correspondence to: Ariel Jaffe <ariel.jaffe@weizmann.ac.il>, Roi Weiss <roi.weiss@weizmann.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"For simplicity we assume it is Gaussian, though our method can be modified to handle other noise distributions.
",1. Introduction,[0],[0]
"The model in (1) appears, for example, in overlapping clustering (Banerjee et al., 2005; Baadel et al., 2016), in various problems in bioinformatics (Segal et al., 2002; Becker et al., 2011; Slawski et al., 2013), and in blind source separation (Van der Veen, 1997).",1. Introduction,[0],[0]
"A special instance of model (1) is the Gaussian-Bernoulli restricted Boltzmann machine (GRBM) where the distribution Ph is further assumed to have a parametric energy-based structure (Hinton & Salakhutdinov, 2006; Cho et al., 2011; Wang et al., 2012).",1. Introduction,[0],[0]
"G-RBMs were used, e.g., in modeling human motion (Taylor et al., 2007) and natural image patches (Melchior et al., 2017).
",1. Introduction,[0],[0]
Given n i.i.d.,1. Introduction,[0],[0]
"samples x1, . . .",1. Introduction,[0],[0]
",xn from model (1), the goal is to estimate the weight matrixW .",1. Introduction,[0],[0]
A common approach for learning W is by maximum likelihood.,1. Introduction,[0],[0]
"As this function is non-convex, common optimization schemes include the EM algorithm and alternating least squares (ALS).",1. Introduction,[0],[0]
"In addition, several works developed iterative methods specialized to GRBMs (Hinton, 2010; Cho et al., 2011).",1. Introduction,[0],[0]
"All these methods, however, often lack consistency guarantees and may not be well suited for large datasets due to their potential slow convergence.",1. Introduction,[0],[0]
"This is not surprising, as learning W under model (1) is believed to be computationally hard; see for example Mossel & Roch (2005).
",1. Introduction,[0],[0]
"Over the past years, several works considered variants and specific instances of model (1) under additional assumptions on the distribution Ph or on the weight matrix W .",1. Introduction,[0],[0]
"For example, when Ph is a product distribution, the learning problem becomes that of independent component analysis (ICA) with binary signals (Hyvärinen et al., 2004).",1. Introduction,[0],[0]
"In this case, several methods were derived for estimating W and under suitable non-degeneracy conditions were proven to be both computationally efficient and statistically consistent (Shalvi & Weinstein, 1993; Frieze et al., 1996; Regalia & Kofidis, 2003; Hyvärinen et al., 2004; Anandkumar et al., 2014; Jain & Oh, 2014).",1. Introduction,[0],[0]
"Similarly, when the hidden units are mutually exclusive, namely Ph has support h ∈ {ei}di=1, the model is a Gaussian mixture (GMM) with d spherical components with linearly independent means.",1. Introduction,[0],[0]
"Efficient and consistent algorithms were derived for this case as well (Moitra & Valiant, 2010; Anandkumar et al., 2012a;b; Hsu & Kakade, 2013).",1. Introduction,[0],[0]
"Among those, most relevant to this work
are orthogonal tensor decomposition methods (Anandkumar et al., 2014).",1. Introduction,[0],[0]
"Interestingly, these methods can learn some additional latent models, with hidden units that are not necessarily binary, such as Dirichlet allocation and other correlated topic models (Arabshahi & Anandkumar, 2017).
",1. Introduction,[0],[0]
Learning W given the observed data {xj}nj=1 can also be viewed as a noisy matrix factorization problem.,1. Introduction,[0],[0]
"If W is known to be non-negative, then various non-negative matrix factorization methods can be used.",1. Introduction,[0],[0]
"Moreover, under appropriate conditions, some of these methods were proven to be computationally efficient and consistent (Donoho & Stodden, 2004; Arora et al., 2012).",1. Introduction,[0],[0]
"For general full rank W , the matrix factorization method in Slawski et al. (2013) (SHL) exactly recovers W when σ = 0 with a runtime exponential in d. This method, however, can handle only low levels of noise and has no consistency guarantees when σ > 0.
",1. Introduction,[0],[0]
A tensor eigenpair approach In this paper we propose a novel spectral method for learning W which is based on the eigenvectors of both the second order moment matrix and the third order moment tensor of the observed data.,1. Introduction,[0],[0]
"We prove that our method is consistent under mild non-degeneracy conditions and achieves the parametric rate OP (n − 12 ) for any noise level σ ≥ 0.
",1. Introduction,[0],[0]
The non-degeneracy conditions we pose are significantly weaker than those required by the previous tensor decomposition methods mentioned above.,1. Introduction,[0],[0]
"In particular, their assumptions and resulting methods can be viewed as specific cases of our more general approach.
",1. Introduction,[0],[0]
"Similarly to the matrix factorization method in Slawski et al. (2013), our algorithm has runtime linear in n, polynomial in m, and in general exponential in d. With our current Matlab implementation, most of the runtime is spent on computing the eigenpairs of a d× d× d tensor.",1. Introduction,[0],[0]
"Practically, our method, implemented without any particular optimization, can learn a model with 12 hidden units in less than ten minutes on a standard PC.",1. Introduction,[0],[0]
"Furthermore, the overall runtime can be significantly reduced, since the step of computing the tensor eigenpairs can be embarrassingly parallelized.
",1. Introduction,[0],[0]
Paper outline In the next section we give necessary background on tensor eigenpairs.,1. Introduction,[0],[0]
"In Section 3 we introduce our
method in the case σ = 0.",1. Introduction,[0],[0]
The case σ ≥ 0 is treated in Section 4.,1. Introduction,[0],[0]
Experiments with our method and comparison to other approaches appear in Section 5.,1. Introduction,[0],[0]
All proofs are deferred to the supplementary material.,1. Introduction,[0],[0]
Notation Denote,2. Preliminaries,[0],[0]
"[d] = {1, . . .",2. Preliminaries,[0],[0]
", d} and ei as the i-th unit vector.",2. Preliminaries,[0],[0]
"We slightly abuse notation and view a matrixW also as the set of its columns, namely w ∈ W is some column of W and span(W ) is the span of all its columns.",2. Preliminaries,[0],[0]
"The unit sphere is denoted by Sd−1 = {u ∈ Rd : ‖u‖ = 1}.
",2. Preliminaries,[0],[0]
"A tensor T ∈ Rd×d×d is symmetric if Tijk = Tπ(i,j,k) for all permutations π of i, j, k.",2. Preliminaries,[0],[0]
"Here, we consider only symmetric tensors.",2. Preliminaries,[0],[0]
"T can also be seen as a multi-linear operator: for matrices W 1,W 2,W 3 with W i ∈ Rd×di , the tensor-mode product, denoted T (W 1,W 2,W 3), is a d1 × d2 × d3 tensor whose (i1, i2, i3)-th entry is∑
j1,j2,j3∈[d]
W 1j1i1W 2 j2i2W 3 j3i3Tj1j2j3 .
",2. Preliminaries,[0],[0]
Tensor eigenpairs Several types of eigenpairs of a tensor have been proposed.,2. Preliminaries,[0],[0]
"Here, we consider the following definition, termed Z-eigenpairs by Qi (2005) and l2-eigenpairs by Lim (2005).",2. Preliminaries,[0],[0]
"Henceforth we just call them eigenpairs.
",2. Preliminaries,[0],[0]
Definition 1.,2. Preliminaries,[0],[0]
"(u, λ) ∈ Rd × R is an eigenpair of T if
T (I,u,u) =",2. Preliminaries,[0],[0]
λu and ‖u‖ = 1.,2. Preliminaries,[0],[0]
"(2)
Note that if (u, λ) is an eigenpair then the eigenvalue is simply λ = T (u,u,u).",2. Preliminaries,[0],[0]
"In addition, (−u,−λ) is also an eigenpair.",2. Preliminaries,[0],[0]
"Following common practice we treat these two pairs as one and make the convention that λ ≥ 0.
",2. Preliminaries,[0],[0]
"In contrast to the matrix case, the number of eigenvalues {λ} of a tensor T ∈ Rd×d×d can be much larger than d. As shown by Cartwright & Sturmfels (2013), for a d× d× d tensor, there can be at most 2d",2. Preliminaries,[0],[0]
− 1 of them.,2. Preliminaries,[0],[0]
"With precise definitions appearing in Cartwright & Sturmfels (2013), for a generic tensor, all its eigenvalues have multiplicity one and the number of eigenpairs {(u, λ)} is at most 2d",2. Preliminaries,[0],[0]
"− 1.
",2. Preliminaries,[0],[0]
"In principle, enumerating the set of all eigenpairs of a general symmetric tensor is a #P problem (Hillar & Lim, 2013).",2. Preliminaries,[0],[0]
"Nevertheless, several methods have been proposed for computing at least some eigenpairs, including iterative higherorder power methods (Kolda & Mayo, 2011; 2014), homotopy continuation (Chen et al., 2016), semidefinite programming (Cui et al., 2014), and iterative Newton-based methods (Jaffe et al., 2017; Guo et al., 2017).",2. Preliminaries,[0],[0]
"We conclude this section with the definition of Newton-stable eigenpairs (Jaffe et al., 2017) which are most relevant to our work.
",2. Preliminaries,[0],[0]
"Newton-stable eigenpairs Equivalently to (2), eigenpairs of T can also be characterized by the function g : Rd → Rd,
g(u)",2. Preliminaries,[0],[0]
"= T (I,u,u)− T (u,u,u) · u. (3)
It is easy to verify that a pair (u, λ) with ‖u‖ = 1 is an eigenpair of T if and only if g(u) = 0 and λ = T (u,u,u).",2. Preliminaries,[0],[0]
"The stability of an eigenpair is determined by its Jacobian matrix ∇g(u) ∈ Rd×d, more precisely, by its projection into the d− 1 dimensional subspace orthogonal to u. Formally, let Lu ∈ Rd×(d−1) be a matrix with d− 1 orthonormal columns that span the subspace orthogonal to u and define the (d− 1)× (d− 1) projected Jacobian matrix
Jp(u) = L > u∇g(u)Lu.",2. Preliminaries,[0],[0]
"(4)
Definition 2.",2. Preliminaries,[0],[0]
"An eigenpair (u, λ) of T ∈ Rd×d×d is Newton-stable if the matrix Jp(u) has full rank d− 1.
",2. Preliminaries,[0],[0]
The homotopy continuation method in Chen et al. (2016) is guaranteed to compute all the Newton-stable eigenpairs of a tensor.,2. Preliminaries,[0],[0]
"Alternatively, all the Newton-stable eigenpairs can be computed by the iterative orthogonal Newton correction method (O–NCM) in Jaffe et al. (2017) as these are the attracting fixed points for this algorithm.",2. Preliminaries,[0],[0]
"Moreover, O–NCM converges to any Newton-stable eigenpair at a quadratic rate given a sufficiently close initial guess.",2. Preliminaries,[0],[0]
"Finally, for a generic tensor, all its eigenpairs are Newton-stable.",2. Preliminaries,[0],[0]
To motivate our approach for estimating the matrix W it is instructive to first consider the ideal noiseless case where σ = 0.,3. Learning in the noiseless case,[0],[0]
"In this case, model (1) takes the form x = W>h.",3. Learning in the noiseless case,[0],[0]
Our problem then becomes that of factorizing the observed matrix X =,3. Learning in the noiseless case,[0],[0]
"[x1, . . .",3. Learning in the noiseless case,[0],[0]
",xn] ∈",3. Learning in the noiseless case,[0],[0]
"Rm×n of n samples into a product of real and binary low-rank matrices,1
Find W ∈ Rd×m, H ∈ {0, 1}d×n s.t. X",3. Learning in the noiseless case,[0],[0]
"= W>H. (5)
To be able to recover W we first need conditions under which the decomposition of X into W and H is unique.",3. Learning in the noiseless case,[0],[0]
"Clearly, such a factorization can be unique at most up to a permutation of its components; we henceforth ignore this degeneracy.",3. Learning in the noiseless case,[0],[0]
"A sufficient condition for uniqueness, similar to the one posed in Slawski et al. (2013), is that H is rigid.",3. Learning in the noiseless case,[0],[0]
"Formally, H ∈ {0, 1}d×n is rigid if any non-trivial linear combination of its rows yields a non-binary vector: ∀u 6= 0,
u>H ∈ {0, 1}n ⇔ u ∈ {ei}di=1.",3. Learning in the noiseless case,[0],[0]
"(6)
Condition (6) is satisfied, for example, when the columns of H include ei and ei + ej for all i",3. Learning in the noiseless case,[0],[0]
6= j ∈,3. Learning in the noiseless case,[0],[0]
[d].,3. Learning in the noiseless case,[0],[0]
"If there
1Note that this is different from the problem known as “Boolean matrix factorization”, where X and W are assumed to be binary as well; see Miettinen & Vreeken (2014) and references therein.
exists a positive constant p0 > 0 such that Ph(ei) ≥ p0 and Ph(ei + ej) ≥ p0, then for a sample size n >",3. Learning in the noiseless case,[0],[0]
"2 log(d)/p0 the matrix H is rigid with high probability.
",3. Learning in the noiseless case,[0],[0]
"The following proposition, similar in nature to the (affine constrained) uniqueness guarantee in Slawski et al. (2013), shows that under condition (6) the factorization in (5) is unique and fully characterized by the binary constraints.
",3. Learning in the noiseless case,[0],[0]
Proposition 1.,3. Learning in the noiseless case,[0],[0]
"Let X = W>H with H ∈ {0, 1}d×n rigid and W ∈ Rd×m full rank with m ≥ d.",3. Learning in the noiseless case,[0],[0]
Let W † ∈ Rm×d be the unique right pseudo-inverse of W so WW † = Id.,3. Learning in the noiseless case,[0],[0]
"Then W and H are unique and for all v ∈ span(X) \ {0},
v>X ∈ {0, 1}n ⇔ v ∈W †.",3. Learning in the noiseless case,[0],[0]
"(7)
Hence, under the rigidity condition (6), the matrix factorization problem in (5) is equivalent to the problem of finding the unique set W † = {v∗1 , . .",3. Learning in the noiseless case,[0],[0]
.,3. Learning in the noiseless case,[0],[0]
",v∗d} ⊆ span(X) of d non-zero vectors that satisfy the binary constraints v∗i",3. Learning in the noiseless case,[0],[0]
">X ∈ {0, 1}n.",3. Learning in the noiseless case,[0],[0]
The weight matrix is then W =,3. Learning in the noiseless case,[0],[0]
"(W †)†.
Algorithm outline We recover W † via a two step procedure.",3. Learning in the noiseless case,[0],[0]
"First, a finite set V = {v1,v2, . . .",3. Learning in the noiseless case,[0],[0]
} ⊆ span(X) of candidate vectors is computed with a guarantee that W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"Specifically, V is computed from the set of eigenpairs of a d× d× d tensor, constructed from the low order moments of X .",3. Learning in the noiseless case,[0],[0]
"Typically, the size of V will be much larger than d, so in the second step V is filtered by selecting all v ∈ V that satisfy v>X ∈ {0, 1}n.
",3. Learning in the noiseless case,[0],[0]
Before describing the two steps in more detail we first state the additional non-degeneracy conditions we pose.,3. Learning in the noiseless case,[0],[0]
"To this end, denote the unknown first, second, and third order moments of the latent binary vector h by
p = E[h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd, C = E[h⊗ h] ∈",3. Learning in the noiseless case,[0],[0]
"Rd×d, C = E[h⊗ h⊗ h] ∈ Rd×d×d.",3. Learning in the noiseless case,[0],[0]
"(8)
Non-degeneracy conditions We assume the following:
(I) H is rigid.
",3. Learning in the noiseless case,[0],[0]
"(II) rank(2C(I, I, ei)− C) =",3. Learning in the noiseless case,[0],[0]
d for all i ∈,3. Learning in the noiseless case,[0],[0]
"[d].
",3. Learning in the noiseless case,[0],[0]
Condition (I) implies that both rank(HH>),3. Learning in the noiseless case,[0],[0]
= d and rank(C) = d. This in turn implies pi = E[hi],3. Learning in the noiseless case,[0],[0]
> 0,3. Learning in the noiseless case,[0],[0]
for all i ∈,3. Learning in the noiseless case,[0],[0]
[d] and that at most one variable hi has pi = 1.,3. Learning in the noiseless case,[0],[0]
"Such an “always on” variable can model a fixed bias to x. As far as we know, condition (II) is new and its nature will become clear shortly.
",3. Learning in the noiseless case,[0],[0]
"We now describe each step of our algorithm in more detail.
",3. Learning in the noiseless case,[0],[0]
"Computing the candidate set To compute a set V that is guaranteed to include the columns of W † we make use of
the second and third order moments of x,
M = E[x⊗ x] ∈ Rm×m, M = E[x⊗ x⊗ x] ∈ Rm×m×m.
(9)
",3. Learning in the noiseless case,[0],[0]
"Given a large number of samples n 1, these can be easily and accurately estimated from the sample X .",3. Learning in the noiseless case,[0],[0]
"For simplicity, in this section we consider the population setting where n → ∞, so M andM are known exactly.",3. Learning in the noiseless case,[0],[0]
"M andM are related to the unknown second and third order moments of h in (8) via (Anandkumar et al., 2014)
M = W>CW, M = C(W,W,W ).",3. Learning in the noiseless case,[0],[0]
"(10)
Since both C and W are full rank, the number of latent units can be deduced by rank(M) =",3. Learning in the noiseless case,[0],[0]
"d. Since C is positive definite, there is a whitening matrix K ∈ Rm×d such that
K>MK = Id. (11)
Such a K can be computed, for example, by an eigendecomposition of M .",3. Learning in the noiseless case,[0],[0]
"Although K is not unique, any K ⊆ span(M) that satisfies (11) suffices for our purpose.",3. Learning in the noiseless case,[0],[0]
"Define the d× d× d lower dimensional whitened tensor
W =M(K,K,K).",3. Learning in the noiseless case,[0],[0]
"(12)
Denote the set of eigenpairs ofW by
U = {(u, λ) ∈",3. Learning in the noiseless case,[0],[0]
"Sd−1 × R+ :W(I,u,u) = λu}.",3. Learning in the noiseless case,[0],[0]
"(13)
Our set of candidates is then
V = {Ku/λ : (u, λ) ∈ U with λ ≥ 1} ⊆ Rm. (14)
",3. Learning in the noiseless case,[0],[0]
The following lemma shows that under condition (I) the set V is guaranteed to contain the d columns of W †.,3. Learning in the noiseless case,[0],[0]
Lemma 1.,3. Learning in the noiseless case,[0],[0]
LetW be the tensor in (12) corresponding to model (1) with σ = 0 and let V be as in (14).,3. Learning in the noiseless case,[0],[0]
If condition (I) holds then W † ⊆ V .,3. Learning in the noiseless case,[0],[0]
"In particular, each (ui, λi) in the set of d relevant eigenpairs
U∗ = {(u, λ) ∈ U : Ku/λ ∈W †} (15)
has the eigenvalue λi = 1/ √ pi ≥ 1 where pi = E[hi] > 0.
",3. Learning in the noiseless case,[0],[0]
"Computing the tensor eigenpairs By Lemma 1, we may construct a candidate set V that contains W † by first calculating the set U of eigenpairs ofW .",3. Learning in the noiseless case,[0],[0]
"Unfortunately, computing the set of all eigenpairs of a general symmetric tensor is computationally hard (Hillar & Lim, 2013).",3. Learning in the noiseless case,[0],[0]
"Moreover, besides the d columns of W †, the set V in (14) may contain many spurious candidates, as the number of eigenpairs of W is typically O(2d)",3. Learning in the noiseless case,[0],[0]
"d (Cartwright & Sturmfels, 2013).
",3. Learning in the noiseless case,[0],[0]
"Nevertheless, as discussed in Section 2, several methods have been proposed for computing some eigenpairs of a tensor under appropriate stability conditions.",3. Learning in the noiseless case,[0],[0]
"The following lemma highlights the importance of condition (II) for the stability of the eigenpairs in U∗. Note that conditions (I)-(II) do not depend on W , but only on the distribution of h.
Lemma 2.",3. Learning in the noiseless case,[0],[0]
Let W be the whitened tensor in (12) corresponding to model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then all (u, λ) ∈ U∗ are Newton-stable eigenpairs ofW .
",3. Learning in the noiseless case,[0],[0]
"Hence, under conditions (I)-(II), the homotopy method in Chen et al. (2016), or alternatively the O–NCM with a sufficiently large number of random initializations (Jaffe et al., 2017), are guaranteed to compute a candidate set V which includes all the columns of W †.",3. Learning in the noiseless case,[0],[0]
"The next step is to extract W † out of V .
",3. Learning in the noiseless case,[0],[0]
Filtering As suggested by Eq.,3. Learning in the noiseless case,[0],[0]
"(7) we select the subset of vectors V̄ ⊆ V that satisfy the binary constraints,
V̄",3. Learning in the noiseless case,[0],[0]
"= {v ∈ V : vTX ∈ {0, 1}n}.",3. Learning in the noiseless case,[0],[0]
"(16)
Indeed, under condition (I), Proposition 1 implies that V̄ = W † and the weight matrix is thus W = V̄ †.
Algorithm 2 in Appendix C summarizes our method for the noiseless case and has the following recovery guarantee.",3. Learning in the noiseless case,[0],[0]
Theorem 1.,3. Learning in the noiseless case,[0],[0]
Let X be a matrix of n samples from model (1) with σ = 0.,3. Learning in the noiseless case,[0],[0]
"If conditions (I)-(II) hold, then the above method recovers W exactly.
",3. Learning in the noiseless case,[0],[0]
"We note that when σ = 0 and conditions (I)-(II) hold for the empirical latent moments Ĉ and Ĉ (rather than C and C), the above procedure exactly recovers W when M and M are replaced by their finite sample estimates.",3. Learning in the noiseless case,[0],[0]
The matrix factorization method SHL in Slawski et al. (2013) also exactly recovers W in the case σ = 0.,3. Learning in the noiseless case,[0],[0]
"While its runtime is also exponential in d, practically it may be much faster than our proposed tensor based approach.",3. Learning in the noiseless case,[0],[0]
"This is because SHL constructs a candidate set of size 2d that can be computed by a suitable linear transformation of the fixed set {0, 1}d, as opposed to our candidate set which is constructed by eigenpairs of a d × d × d tensor.",3. Learning in the noiseless case,[0],[0]
"However, SHL does not take advantage of the large number of samples n, since only m× d sub-matrices of the m×n sample matrix X are used for constructing its candidate set.",3. Learning in the noiseless case,[0],[0]
"Indeed, in the noisy case where σ > 0, SHL has no consistency guarantees and as demonstrated by the simulation results in Section 5 it may fail at high levels of noise.",3. Learning in the noiseless case,[0],[0]
In the next section we derive a modified version of our method that consistently estimates W for any noise level σ ≥ 0.,3. Learning in the noiseless case,[0],[0]
The method in Section 3 to estimateW is clearly inadequate when σ > 0.,4. Learning in the presence of noise,[0],[0]
"However, we now show that by making several adjustments, the two steps of computing the candidate set and its filtering can be both made robust to noise, yielding a consistent estimator of W for any σ ≥ 0.
",4. Learning in the presence of noise,[0],[0]
"Computing the candidate set As in the case σ = 0, our goal in the first step is to compute a finite candidate set
Vσ ⊆ Rm that is guaranteed to contain accurate estimates for the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To this end, in addition to the second and third order moments M andM in (9), we also consider the first order moment µ = E[x] and define the following noise corrected moments,
Mσ = M − σ2Im,
Mσ = M− σ2 m∑ i=1",4. Learning in the presence of noise,[0],[0]
"( µ⊗ ei ⊗ ei
+ ei ⊗ µ⊗ ei + ei ⊗ ei ⊗ µ ) .
",4. Learning in the presence of noise,[0],[0]
"(17)
By assumption, the noise satisfies E[ξ3i ] = 0.",4. Learning in the presence of noise,[0],[0]
"Thus, similarly to the moment equations in (10), the modified moments in (17) are related to these ofh by (Anandkumar et al., 2014)
Mσ = W >CW, Mσ = C(W,W,W ).",4. Learning in the presence of noise,[0],[0]
"(18)
Hence, if Mσ andMσ were known exactly, a candidate set Vσ that contains W † could be obtained exactly as in the noiseless case, but with M andM replaced with Mσ and Mσ; namely, first calculate the whitening matrix Kσ such that K>σMσKσ =",4. Learning in the presence of noise,[0],[0]
Id,4. Learning in the presence of noise,[0],[0]
"and then compute the eigenpairs of the population whitened tensor
Wσ =Mσ(Kσ,Kσ,Kσ).",4. Learning in the presence of noise,[0],[0]
"(19)
In practice, σ2, d, µ,M andM are all unknown and need to be estimated from the sample matrix X .",4. Learning in the presence of noise,[0],[0]
"Assuming m > d, the parameters σ2 and d can be consistently estimated, for example, by the methods in Kritchman & Nadler (2009).",4. Learning in the presence of noise,[0],[0]
"For simplicity, we assume they are known exactly.",4. Learning in the presence of noise,[0],[0]
"Similarly, µ, M ,M are consistently estimated by their empirical means, µ̂, M̂ , and M̂.",4. Learning in the presence of noise,[0],[0]
"So, after computing the plugin estimates K̂σ such that K̂>σ M̂σK̂σ = Id and Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ), we compute the set Ûσ of eigenpairs of Ŵσ and for some small 0 < τ",4. Learning in the presence of noise,[0],[0]
"= O(n− 1 2 ) take our candidate set as
V̂σ = {K̂σu/λ : (u, λ) ∈ Ûσ with λ ≥ 1−τ}.",4. Learning in the presence of noise,[0],[0]
"(20)
The following lemma shows that under conditions (I)-(II) the above procedure is stable to small perturbations.",4. Learning in the presence of noise,[0],[0]
"Namely, for perturbations of order δ 1 inWσ and Kσ , the method computes a candidate set V̂σ that contains a subset of d vectors that are O(δ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"Furthermore, these d vectors all correspond to Newton-stable eigenpairs of the perturbed tensor and are Ω(1) separated from the other candidates in V̂σ .
",4. Learning in the presence of noise,[0],[0]
Lemma 3.,4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let K̂σ, Ŵσ be their perturbed versions, inducing the candidate set V̂σ in (20).",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) hold, then there are c, δ0, δ1 > 0",4. Learning in the presence of noise,[0],[0]
"such that for all 0 ≤ δ ≤ δ0 the following holds: If the perturbed versions satisfy
max{‖Ŵσ −Wσ‖F , ‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F } ≤ δ, (21)
",4. Learning in the presence of noise,[0],[0]
"then any v∗ ∈W † has a unique v̂ ∈ V̂σ such that
‖v̂ − v∗‖ ≤ cδ.",4. Learning in the presence of noise,[0],[0]
"(22)
",4. Learning in the presence of noise,[0],[0]
"Moreover, v̂ corresponds to a Newton-stable eigenpair of Ŵσ with eigenvalue λ ≥ 1− cδ and for all ṽ ∈ V̂σ \ {v̂},
‖ṽ",4. Learning in the presence of noise,[0],[0]
− v∗‖ ≥ δ1 > 2cδ.,4. Learning in the presence of noise,[0],[0]
"(23)
",4. Learning in the presence of noise,[0],[0]
"The proof is based on the implicit function theorem (Hubbard & Hubbard, 2015); small perturbations to a tensor result in small perturbations to its Newton-stable eigenpairs.
",4. Learning in the presence of noise,[0],[0]
"Now, by the delta method, the plugin estimates K̂σ and Ŵσ are both OP (n− 1 2 ) close to their population quantities,
‖K̂σ",4. Learning in the presence of noise,[0],[0]
"−Kσ‖F = OP (n − 12 ),
‖Ŵσ −Wσ‖F = OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
"(24)
By (24), we have that (21) holds with δ = OP (n− 1 2 ).",4. Learning in the presence of noise,[0],[0]
"Hence, by Lemma 3, the eigenpairs of Ŵσ provide a candidate set V̂σ that contains d vectors that are OP (n− 1 2 ) close to the columns of W †.",4. Learning in the presence of noise,[0],[0]
"In addition, any irrelevant candidate is ΩP (1) far away from W †.",4. Learning in the presence of noise,[0],[0]
"As we show next, these properties ensure that with high probability the d relevant candidates can be identified in V̂σ .
",4. Learning in the presence of noise,[0],[0]
"Filtering Given the candidate set V̂σ computed in the first step, our goal now is to find a set V̄σ ⊆ V̂σ of d vectors that accurately estimate the d columns of W †.",4. Learning in the presence of noise,[0],[0]
"To simplify the theoretical analysis, we assume the filtering step is done using a sample X of size n that is independent of V̂σ.",4. Learning in the presence of noise,[0],[0]
"This can be achieved by first splitting a given sample of size 2n into two sets of size n, one for each step.
",4. Learning in the presence of noise,[0],[0]
"Recall that for x from model (1) and any v ∈ Rm,
v>x = v>W>h+",4. Learning in the presence of noise,[0],[0]
"σv>ξ. (25)
",4. Learning in the presence of noise,[0],[0]
"Obviously, when σ > 0, the filtering procedure in (16) for the noiseless case is inadequate, as typically no v∗ ∈ W † will exactly satisfy v∗>X ∈ {0, 1}n.",4. Learning in the presence of noise,[0],[0]
"Nevertheless, we expect that for a sufficiently small noise level σ, any v ∈ V̂σ that is close to some v∗ ∈ W † will result in v>X that is close to being binary, while any v sufficiently far from W † will result in v>X that is far from being binary.",4. Learning in the presence of noise,[0],[0]
"A natural measure for how v>X is “far from being binary”, similar to the one used for filtering in Slawski et al. (2013), is simply its deviation from its binary rounding,
min b∈{0,1}n
‖vTX − b‖2
n‖v‖2 .",4. Learning in the presence of noise,[0],[0]
"(26)
Eq. (26) works extremely well for small σ, but fails for high noise levels.",4. Learning in the presence of noise,[0],[0]
"Here we instead propose a filtering procedure based on the classical Kolmogorov-Smirnov goodness of fit
test (Lehmann & Romano, 2006).",4. Learning in the presence of noise,[0],[0]
"As we show below, this approach gives consistent estimates of W for any σ > 0.
",4. Learning in the presence of noise,[0],[0]
"Before describing the test, we first introduce the probabilistic analogue of the rigidity condition (6).",4. Learning in the presence of noise,[0],[0]
"For any u ∈ Rd, define its corresponding expected binary rounding error,
r(u) = Eh∼Ph",4. Learning in the presence of noise,[0],[0]
"[
min b∈{0,1}
(u>h− b)2 ] .
",4. Learning in the presence of noise,[0],[0]
"Clearly, r(0) = 0 and r(ei) = 0 for all i ∈",4. Learning in the presence of noise,[0],[0]
[d].,4. Learning in the presence of noise,[0],[0]
"We pose the following expected rigidity condition: for all u 6= 0,
r(u)",4. Learning in the presence of noise,[0],[0]
= 0 ⇔ u ∈ {ei}di=1.,4. Learning in the presence of noise,[0],[0]
"(27)
Analogously to the deterministic rigidity condition in (6), condition (27) is satisfied, for example, when Ph(ei) > 0",4. Learning in the presence of noise,[0],[0]
and Ph(ei + ej) > 0,4. Learning in the presence of noise,[0],[0]
for all i 6=,4. Learning in the presence of noise,[0],[0]
j ∈,4. Learning in the presence of noise,[0],[0]
"[d].
To introduce our filtering test, recall that under model (1), ξ ∼ N (0, Im).",4. Learning in the presence of noise,[0],[0]
"Hence, for any fixed v, the random variable v>x in (25) is distributed according to the following univariate Gaussian mixture model (GMM),
v>x ∼ ∑
h∈{0,1}d Ph(h) · N",4. Learning in the presence of noise,[0],[0]
"(v>W>h, σ2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(28)
",4. Learning in the presence of noise,[0],[0]
Denote the cumulative distribution function of v>x by Fv .,4. Learning in the presence of noise,[0],[0]
"For general v, this mixture may have up to 2d distinct components.",4. Learning in the presence of noise,[0],[0]
"However, for v∗ ∈W †, it reduces to a mixture of two components with means at 0 and 1.",4. Learning in the presence of noise,[0],[0]
"More precisely, for any candidate v with corresponding eigenvalue λ(v)",4. Learning in the presence of noise,[0],[0]
"≥ 1, define the GMM with two components
(1− 1λ(v)2 ) · N (0, σ 2‖v‖2)",4. Learning in the presence of noise,[0],[0]
"+ 1λ(v)2 · N (1, σ 2‖v‖2).",4. Learning in the presence of noise,[0],[0]
"(29)
Denote its cumulative distribution function by Gv.",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that under condition (27), Gv fully characterizes the columns of W †.
Lemma 4.",4. Learning in the presence of noise,[0],[0]
"Let Kσ,Wσ be the population quantities in (19) and let Vσ be the set of population candidates as computed from the eigenpairs of Wσ.",4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then for any v ∈",4. Learning in the presence of noise,[0],[0]
"Vσ with corresponding eigenvalue λ(v),
Fv = Gv ⇔ v ∈W †.
",4. Learning in the presence of noise,[0],[0]
"Given the empirical candidate set V̂σ, Lemma 4 suggests ranking all v̂ ∈ V̂σ according to their goodness of fit to Gv̂ and taking the d candidates with the best fit.",4. Learning in the presence of noise,[0],[0]
"More precisely, given a sample X =",4. Learning in the presence of noise,[0],[0]
"[x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn] that is independent of V̂σ , for each candidate v̂ ∈ V̂σ we compute the empirical cumulative distribution function, F̂v̂(t) =",4. Learning in the presence of noise,[0],[0]
1n,4. Learning in the presence of noise,[0],[0]
"∑n j=1 1{v̂>xj ≤ t}, t ∈ R, and calculate its Kolmogorov-Smirnov score
∆n(v̂) = sup t∈R |F̂v̂(t)−Gv̂(t)|.",4. Learning in the presence of noise,[0],[0]
"(30)
Algorithm 1 Estimate W when σ > 0 and n <∞",4. Learning in the presence of noise,[0],[0]
"Input: sample matrix X ∈ Rm×n and 0 < τ 1
1: estimate number of hidden units d and noise level σ2 2: compute empirical moments µ̂, M̂ and M̂ and plugin moments M̂σ and M̂σ of (17) 3: compute K̂σ such that K̂>σ M̂σK̂σ = Id 4: construct Ŵσ = M̂σ(K̂σ, K̂σ, K̂σ) 5: compute the set Ûσ of eigenpairs of Ŵσ 6: compute the candidate set V̂σ in (20) 7: for each v̂ ∈ V̂σ compute its KS score ∆n(v̂) in (30) 8: select V̄σ ⊆ V̂σ of d vectors with smallest ∆n(v̂) 9: return the pseudo-inverse Ŵ = V̄ †σ
Our estimator V̄σ ⊆ V̂σ for W † is then the set of d vectors with the smallest scores ∆n(v̂).",4. Learning in the presence of noise,[0],[0]
"The estimator for W is the pseudo-inverse, Ŵ = V̄ †σ .
",4. Learning in the presence of noise,[0],[0]
"The following lemma shows that for sufficiently large n, ∆n(v̂) accurately distinguishes between v̂ ∈ V̂σ that are close to the columns of W † from these that are not.
",4. Learning in the presence of noise,[0],[0]
Lemma 5.,4. Learning in the presence of noise,[0],[0]
"Let v∗ ∈ W † and v̂(1), v̂(2), . . .",4. Learning in the presence of noise,[0],[0]
"a sequence of random vectors such that ‖v̂(n) − v∗‖ = OP (n
− 12 ).",4. Learning in the presence of noise,[0],[0]
"Then, ∆n(v̂(n))",4. Learning in the presence of noise,[0],[0]
= oP (1).,4. Learning in the presence of noise,[0],[0]
"In contrast, if minv∗∈W † ‖v̂(n) − v∗‖ = ΩP (1), then ∆n(v̂(n)) = ΩP (1), provided the expected rigidity condition (27) holds.
",4. Learning in the presence of noise,[0],[0]
"Lemma 5 follows from classical and well studied properties of the Kolmogorov-Smirnov test, see for example Lehmann & Romano (2006); Billingsley (2013).
",4. Learning in the presence of noise,[0],[0]
Algorithm 1 summarizes our method for estimating W in the general case where σ > 0 and n <,4. Learning in the presence of noise,[0],[0]
"∞. The following theorem establishes its consistency.
",4. Learning in the presence of noise,[0],[0]
Theorem 2.,4. Learning in the presence of noise,[0],[0]
"Let x1, . . .",4. Learning in the presence of noise,[0],[0]
",xn be n i.i.d.",4. Learning in the presence of noise,[0],[0]
samples from model (1).,4. Learning in the presence of noise,[0],[0]
"If conditions (I)-(II) and the expected rigidity condition (27) hold, then the estimator Ŵ computed by Algorithm 1 is consistent, achieving the parametric rate,
Ŵ = W +OP (n − 12 ).
",4. Learning in the presence of noise,[0],[0]
Runtime The runtime of Algorithm 1 is composed of three main parts.,4. Learning in the presence of noise,[0],[0]
"First, O(nm3) operations are needed to compute all the relevant moments from the data and to construct the d× d× d whitened tensor Ŵσ .",4. Learning in the presence of noise,[0],[0]
"The most time consuming task is computing the eigenpairs of Ŵσ, which can be done by either the homotopy method or O–NCM.",4. Learning in the presence of noise,[0],[0]
"Currently, no runtime guarantees are available for either of these methods.",4. Learning in the presence of noise,[0],[0]
"In practice, since there are O(2d) eigenpairs, these methods spend O(2d · poly(d)) operations in total.",4. Learning in the presence of noise,[0],[0]
"Finally, since there are O(2d) candidates and each KS test takes O(dn) operations (Gonzalez et al., 1977), the filtering procedure runtime is O(d2dn).
",4. Learning in the presence of noise,[0],[0]
Power-stability and orthogonal decomposition The exponential runtime of our algorithm stems from the fact that the set UN of Newton-stable eigenpairs ofWσ is typically O(2d).,4. Learning in the presence of noise,[0],[0]
"However, in some cases, the set U∗ of d relevant eigenpairs has additional structure so that a smaller candidate set may be computed instead of UN .",4. Learning in the presence of noise,[0],[0]
Consider the subset UP ⊆ UN of power-stable eigenpairs ofWσ:,4. Learning in the presence of noise,[0],[0]
Definition 3.,4. Learning in the presence of noise,[0],[0]
"An eigenpair (u, λ) is power-stable if its projected Jacobian Jp(u) is either positive or negative definite.
",4. Learning in the presence of noise,[0],[0]
"Typically, the number of power-stable eigenpairs is significantly smaller than the number of Newton-stable eigenpairs.2 In addition, UP can be computed by the shifted higher-order power method (Kolda & Mayo, 2011; 2014).
",4. Learning in the presence of noise,[0],[0]
"Similarly to Lemma 2, one can show that UP is guaranteed to contain U∗ whenever the following stronger version of condition (II) holds: for all (ui, λi) ∈ U∗, the matrix
(WKLui) >(2C(I, I, ei)− C)(WKLui) (31)
is either positive-definite or negative-definite.
",4. Learning in the presence of noise,[0],[0]
"As an example, consider the case where Ph has the support h ∈",4. Learning in the presence of noise,[0],[0]
Id.,4. Learning in the presence of noise,[0],[0]
Then model (1) corresponds to a GMM with d spherical components with linearly independent means.,4. Learning in the presence of noise,[0],[0]
"In this case, bothC and C are diagonal with p on their diagonal.",4. Learning in the presence of noise,[0],[0]
"Thus, the matrices in (31) take the form −L>ei diag(p)Lei , which are all negative-definite when p > 0.",4. Learning in the presence of noise,[0],[0]
"In fact, in this case, Wσ has an orthogonal CP decomposition and the d orthogonal eigenpairs in U∗ are the only negative-definite power-stable eigenpairs ofWσ (Anandkumar et al., 2014).",4. Learning in the presence of noise,[0],[0]
"Similarly, when Ph is a product distribution, the same orthogonal structure appears if the centered moments of x are used instead of M andM. As shown in Anandkumar et al. (2014), the power method, accompanied with a deflation procedure, decomposes an orthogonal tensor in polynomial time, thus implying an efficient algorithm in these cases.",4. Learning in the presence of noise,[0],[0]
"However, under the much weaker conditions we pose on Ph, the relevant eigenpairs in U∗ are not necessarily powerstable and the CP decomposition ofWσ does not necessarily include U∗.",4. Learning in the presence of noise,[0],[0]
We demonstrate our method in three scenarios: (I) simulations from the exact binary model (1); (II) learning a common population genetic admixture model; (III) learning the proportion matrix of a cell mixture from DNA methylation levels.,5. Experiments,[0],[0]
"Due to lack of space, (III) is deferred to Appendix N. Code to reproduce the simulation results can be found at https://github.com/arJaffe/ BinaryLatentVariables.
",5. Experiments,[0],[0]
2We currently do not know whether the number of power-stable eigenpairs of a generic tensor is polynomial or exponential in d.,5. Experiments,[0],[0]
"We generated n samples from model (1) with d = 6 hidden units, m = 30 observable features, and Gaussian noise ξ ∼ N (0, Im).",5.1. Simulations,[0],[0]
The m columns of W were drawn uniformly from the unit sphere Sd−1.,5.1. Simulations,[0],[0]
"Fixing a mean vector a ∈ Rd and a covariance matrix R ∈ Rd×d, each hidden vector h was generated independently by first drawing r ∼ N (a, R) and then taking its binary rounding.
",5.1. Simulations,[0],[0]
"Figure 2 shows the error, in Frobenius norm, averaged over 50 independent realizations of X as a function of n (upper panel) and σ (lower panel) for 5 methods: (i) our spectral approach, Algorithm 1 (Spectral); (ii) Algorithm 1 followed by a single weighted least squares step (Appendix K) (Spectral+WLS); (iii) SHL, the matrix decomposition method of Slawski et al. (2013)3; (iv) ALS with a random initialization (Appendix L); and (v) an oracle estimator that is given the exact matrix H and computes W via least squares.
",5.1. Simulations,[0],[0]
"As one can see, as opposed to SHL, our method is consistent for σ > 0 and achieves an error rate O(n−",5.1. Simulations,[0],[0]
1 2 ) corresponding to a slope of −1 in the upper panel of Fig. 2.,5.1. Simulations,[0],[0]
"In addition, as seen in the lower panel of Fig. 2, at low levels of noise our method is comparable to SHL, whereas at high levels it is far more accurate.",5.1. Simulations,[0],[0]
"Finally, adding a weighted least squares step reduces the error for low noise levels, but increases the error
3Code from https://sites.google.com/site/ slawskimartin/code.",5.1. Simulations,[0],[0]
"For each realization X , we made 50 runs of SHL and chose H,W minimizing",5.1. Simulations,[0],[0]
‖X −W>,5.1. Simulations,[0],[0]
"H‖F .
for high noise levels.",5.1. Simulations,[0],[0]
A comparison between the runtime of SHL and the spectral method appears in Appendix I.,5.1. Simulations,[0],[0]
"We present an application of our method to a fundamental problem in population genetics, known as admixture (see Fig. 3).",5.2. Population genetic admixture,[0],[0]
"Admixture refers to the mixing of d ≥ 2 ancestral populations that were long separated, e.g., due to geographical or cultural barriers (Pritchard et al., 2000; Alexander et al., 2009; Li et al., 2008).",5.2. Population genetic admixture,[0],[0]
"The observed data X is an m× n matrix where m is the number of modern “admixed” individuals and n is the number of relevant locations in their DNA, known as SNPs.",5.2. Population genetic admixture,[0],[0]
Each SNP corresponds to two alleles and individuals may have different alleles.,5.2. Population genetic admixture,[0],[0]
"Fixing a reference allele for each location, Xij takes values in {0, 12 , 1} according to the number of reference alleles appearing in the genotype of individual",5.2. Population genetic admixture,[0],[0]
i ∈,5.2. Population genetic admixture,[0],[0]
[m] at locus j ∈,5.2. Population genetic admixture,[0],[0]
"[n].
",5.2. Population genetic admixture,[0],[0]
"Given the genotypes X , an important problem in population genetics is to estimate the following two quantities.",5.2. Population genetic admixture,[0],[0]
The allele frequency matrix H ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×n whose entry Hkj is the frequency of the reference allele at locus j ∈",5.2. Population genetic admixture,[0],[0]
[n] in ancestral population k ∈,5.2. Population genetic admixture,[0],[0]
[d]; and the admixture proportion matrix W ∈,5.2. Population genetic admixture,[0],[0]
"[0, 1]d×m whose columns sum to 1 and its entry Wki is the proportion of individual",5.2. Population genetic admixture,[0],[0]
"i’s genome that was inherited from population k.
A common model for X in terms of W and H is to assume that the number of alleles 2Xij ∈ {0, 1, 2} is the sum of two i.i.d.",5.2. Population genetic admixture,[0],[0]
"Bernoulli random variables with success probability Fij = ∑d k=1WkiHkj , namely, Xij |H ∼ 1 2 · Binomial(2, Fij).",5.2. Population genetic admixture,[0],[0]
"Note that under this model
E[X|H] = F = W>H. (32)
",5.2. Population genetic admixture,[0],[0]
"Although (32) has similar form to model (1), there are two main differences; the noise is not normally distributed and the matrix H is non-binary.",5.2. Population genetic admixture,[0],[0]
"Yet, the binary model (1) serves as a good approximation whenever various alleles are rare in some populations but abundant in others.",5.2. Population genetic admixture,[0],[0]
"Specifically, for ancestral populations that have been long separated, some alleles may become fixed in one population (i.e., reach frequency of 1) while being totally absent in others.
",5.2. Population genetic admixture,[0],[0]
"Simulating genetic admixture We followed a standard simulation scheme apllied, for example, in Xue et al. (2017); Gravel (2012); Price et al. (2009).",5.2. Population genetic admixture,[0],[0]
"First, using SCRM (Staab et al., 2015), we simulated d = 3 ancestral populations separated for 4000 generations and generated the genomes of 40 individuals for each.",5.2. Population genetic admixture,[0],[0]
H was then computed as the frequency of the reference alleles in each population.,5.2. Population genetic admixture,[0],[0]
"Next, the columns of W were sampled from a symmetric Dirichlet distribution with parameter α ≥ 0.",5.2. Population genetic admixture,[0],[0]
"Finally, the genomes of m = 50 admixed individuals were generated as mosaics of genomic segments of individuals from the ancestral populations with proportions W .",5.2. Population genetic admixture,[0],[0]
"The mosaic nature of the admixed genomes is an important realistic detail, due to the linkage (correlation) between SNPs (Xue et al., 2017).",5.2. Population genetic admixture,[0],[0]
"A detailed description is in Appendix M.
We compare our algorithm to two methods.",5.2. Population genetic admixture,[0],[0]
"The first is Admixture (Alexander et al., 2009), one of the most widely used algorithms in population genetics, which aims to maximize the likelihood of X .",5.2. Population genetic admixture,[0],[0]
"The second is the recently proposed spectral method ALStructure (Cabreros & Storey, 2017), where an estimation of span(W>) via Chen & Storey (2015) is followed by constrained ALS iterations of W and H .",5.2. Population genetic admixture,[0],[0]
"For our method, two modification are needed for Algorithm 1.",5.2. Population genetic admixture,[0],[0]
"First, since the distribution ofXij−wTi hj is not Gaussian, the corrected moments M̂σ,M̂σ as calculated by (17) do not satisfy (18).",5.2. Population genetic admixture,[0],[0]
"Instead, we implemented a matrix completion algorithm derived in (Jain & Oh, 2014) for a similar setup, see Appendix J for more details.",5.2. Population genetic admixture,[0],[0]
"In addition, the filtering process described in Section 4 is no longer valid.",5.2. Population genetic admixture,[0],[0]
"However, as d is relatively small, we performed exhaustive search over all candidate subsets of size d and choose the one that maximized the likelihood.
",5.2. Population genetic admixture,[0],[0]
"Figure 4 compares the results of the 3 methods for α = 0.1, 1, 10.",5.2. Population genetic admixture,[0],[0]
"The spectral method outperforms Admixture and ALStructure for α = 1, 10 and performs similarly to Admixture for α = 0.1.
",5.2. Population genetic admixture,[0],[0]
Acknowledgements This research was funded in part by NIH Grant 1R01HG008383-01A1.,5.2. Population genetic admixture,[0],[0]
Uniqueness of the factorization readily follows from (7) so we proceed to prove (7).,A. Proof of Proposition 1,[0],[0]
First note that span(X) = span(W>) =,A. Proof of Proposition 1,[0],[0]
span(W †).,A. Proof of Proposition 1,[0],[0]
"Since W is full rank, we have WW † = Id.",A. Proof of Proposition 1,[0],[0]
"Hence,
(W †)>X = (WW †)>H = H ∈ {0, 1}d×n.
",A. Proof of Proposition 1,[0],[0]
So any v∗ ∈ W † satisfies the binary constraint,A. Proof of Proposition 1,[0],[0]
"v∗>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"For the other direction, let v ∈ span(X)",A. Proof of Proposition 1,[0],[0]
"\ {0} be such that v>X ∈ {0, 1}n.",A. Proof of Proposition 1,[0],[0]
"Since v>X = (Wv)>H , the rigidity condition (6) implies Wv ∈ {ei}di=1.",A. Proof of Proposition 1,[0],[0]
"Since W is full rank and v ∈ span(W †), v must be a column of W †.",A. Proof of Proposition 1,[0],[0]
"Since the vector h is binary, its second and third order moments are related as follows.",B. Proof of Lemma 1,[0],[0]
"For all i, j ∈",B. Proof of Lemma 1,[0],[0]
"[d],
Ciij = Ciji = Cjii = E[h2ihj",B. Proof of Lemma 1,[0],[0]
] = E[hihj ] = Cij .,B. Proof of Lemma 1,[0],[0]
"(33)
",B. Proof of Lemma 1,[0],[0]
"Since W is full rank, WW † = Id.",B. Proof of Lemma 1,[0],[0]
"Hence, applying W † multi-linearly on the moment equations in (10) we obtain
C =",B. Proof of Lemma 1,[0],[0]
"(W †)>MW †,
C = M(W †,W †,W †).
",B. Proof of Lemma 1,[0],[0]
"Thus, the equality in (33) is equivalent to
[M(W †,W †,W †)]iij =",B. Proof of Lemma 1,[0],[0]
[(W †)>MW †]ij .,B. Proof of Lemma 1,[0],[0]
"(34)
Let Y ∗ ∈ Rd×d be the full rank matrix that satisfies W † = KY ∗ where K is the whitening matrix in (11).",B. Proof of Lemma 1,[0],[0]
"Then,
M(W †,W †,W †) = M(KY ∗,KY ∗,KY ∗) = W(Y ∗, Y ∗, Y ∗)
whereW is the whitened tensor in (12).",B. Proof of Lemma 1,[0],[0]
"Similarly, by (11),
(W †)>MW † = (Y ∗)>(KTMK)(Y ∗) =",B. Proof of Lemma 1,[0],[0]
"(Y ∗)>Y ∗.
Inserting these into (34), the matrix Y ∗ must satisfy
[W(Y ∗, Y ∗, Y ∗)]iij =",B. Proof of Lemma 1,[0],[0]
"[(Y ∗)>(Y ∗)]ij , ∀i, j ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"(35)
The following lemma, proved in Appendix H, shows that Eq. (35) is nothing but a tensor eigen-problem.",B. Proof of Lemma 1,[0],[0]
"Specifically, the columns of Y ∗, up to scaling, are eigenvectors ofW .",B. Proof of Lemma 1,[0],[0]
Lemma 6.,B. Proof of Lemma 1,[0],[0]
Let W ∈ Rd×d×d be an arbitrary symmetric tensor.,B. Proof of Lemma 1,[0],[0]
"Then, a matrix Y =",B. Proof of Lemma 1,[0],[0]
"[y1, . . .",B. Proof of Lemma 1,[0],[0]
",yd] ∈ Rd×d of rank d satisfies (35) if and only if for all k ∈",B. Proof of Lemma 1,[0],[0]
"[d], yk = uk/λk, where (uk, λk)dk=1 are d eigenpairs of W with linearly independent {uk}dk=1.
",B. Proof of Lemma 1,[0],[0]
"By Lemma 6, the set of scaled eigenpairs {y = u/λ} of W is guaranteed to contain the d columns of Y ∗. Since W † = KY ∗, the set {Ky} is guaranteed to contain W †.
To show that each y = u/λ",B. Proof of Lemma 1,[0],[0]
∈,B. Proof of Lemma 1,[0],[0]
"Y ∗ has λ ≥ 1, note that the vector Ky is a column of W †, so WKy = ei for some i ∈",B. Proof of Lemma 1,[0],[0]
[d].,B. Proof of Lemma 1,[0],[0]
"Hence, by the definition of the whitened tensor (12) and the moment equation (10),
W(y,y,y) = M(Ky,Ky,Ky) = C(WKy,WKy,WKy) = C(ei, ei, ei) = Ciii = E[hi] ≤ 1.
",B. Proof of Lemma 1,[0],[0]
"On the other hand, since (u, λ) is an eigenpair ofW with eigenvalue λ =W(u,u,u),
W(y,y,y) = 1 λ3 W(u,u,u) = 1 λ2 .
",B. Proof of Lemma 1,[0],[0]
"By convention, λ ≥ 0.",B. Proof of Lemma 1,[0],[0]
"Hence, λ = 1/ √ E[hi] ≥ 1,
concluding the proof.",B. Proof of Lemma 1,[0],[0]
Algorithm 2 Recover W when σ = 0,C. Recovery algorithm - noiseless case,[0],[0]
"Input: sample matrix X
1: estimate second and third order moments M ,M 2: set d = rank(M) 3: compute K ⊆ span(M) such that K>MK",C. Recovery algorithm - noiseless case,[0],[0]
=,C. Recovery algorithm - noiseless case,[0],[0]
"Id 4: compute whitened tensorW =M(K,K,K) 5: compute the set U of eigenpairs ofW 6: compute the candidate set V in (14) 7: filter V̄ = {v ∈ V : v>X ∈ {0, 1}n} 8: return the pseudo-inverse W = V̄ †",C. Recovery algorithm - noiseless case,[0],[0]
"Let (u, λ) ∈ U∗ be an eigenpair of W such that v∗ = Ku/λ ∈W †.",D. Proof of Lemma 2,[0],[0]
"To show Newton-stability we need to show that under conditions (I)-(II) the projected Jacobian matrix Jp(u) = L > u∇g(u)Lu in (4) is full rank d− 1.
",D. Proof of Lemma 2,[0],[0]
"The Jacobian matrix∇g(u) is
∇g(u) = 2W(I, I,u)− 3uW(I,u,u)>
−W(u,u,u)Id = 2W(I, I,u)− 3λuu>",D. Proof of Lemma 2,[0],[0]
− λId.,D. Proof of Lemma 2,[0],[0]
"(36)
Since L>uu = 0, the second term in (36) does not contribute to Jp(u).",D. Proof of Lemma 2,[0],[0]
"For the first term in (36), by (12) and (10),
W(I, I,u) =M(K,K,Ku) = C(WK,WK,WKu).
",D. Proof of Lemma 2,[0],[0]
"Since v∗ = Ku/λ is a column of W †, WKu =",D. Proof of Lemma 2,[0],[0]
λei for some,D. Proof of Lemma 2,[0],[0]
i ∈,D. Proof of Lemma 2,[0],[0]
[d].,D. Proof of Lemma 2,[0],[0]
"Thus,
W(I, I,u) = λC(WK,WK, ei) = λK>W>C(I, I, ei)WK.
",D. Proof of Lemma 2,[0],[0]
"For the third term in (36), by the definition of K in (11),
Id = K >MK = K>W>CWK.
Putting the last two equalities in (36) and applying the projection Lu we obtain
Jp(u) = L > u∇g(u)Lu
= λL>uK >W>(2C(I, I, ei)− C)WKLu.
",D. Proof of Lemma 2,[0],[0]
"Since λ ≥ 1 and W and K are full rank, condition (II) implies that Jp(u) is full rank as well.",D. Proof of Lemma 2,[0],[0]
"Thus, (u, λ) is a Newton-stable eigenpair ofW .",D. Proof of Lemma 2,[0],[0]
Lemma 3 follows from the following lemma which establishes the stability of Newton-stable eigenpairs of a tensor W to small perturbations W̃ =W + ∆W .,E. Proof of Lemma 3,[0],[0]
Lemma 7.,E. Proof of Lemma 3,[0],[0]
"Let (u, λ) be a Newton-stable eigenpair ofW with λ ≥ 1.",E. Proof of Lemma 3,[0],[0]
"There are c1, c2, ε0 > 0 such that for all sufficiently small ε > 0",E. Proof of Lemma 3,[0],[0]
the following holds.,E. Proof of Lemma 3,[0],[0]
"For any W̃ such that ‖W̃ −W‖F ≤ ε there exists a unique eigenpair (ũ, λ̃) of W̃ such that
‖u− ũ‖ ≤ c1ε and |λ̃− λ| ≤ c2ε.
",E. Proof of Lemma 3,[0],[0]
"In addition, (ũ, λ̃) is Newton-stable and any other eigenvector ṽ of W̃ satisfies ‖ṽ",E. Proof of Lemma 3,[0],[0]
"− u‖ ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
Proof of Lemma 7.,E. Proof of Lemma 3,[0],[0]
For a tensor T ∈ Rd×d×d let t ∈ Rs be the vector of s = d3 entries {Tijk}.,E. Proof of Lemma 3,[0],[0]
"Define the function Q : Rd+s → Rd by
Q(v, t)",E. Proof of Lemma 3,[0],[0]
= T,E. Proof of Lemma 3,[0],[0]
"(I,v,v)− T (v,v,v) ·",E. Proof of Lemma 3,[0],[0]
"v.
Note that for any t ∈ Rs and (v, β) ∈ Rd × R with v 6= 0 and β 6= 0, we have thatQ(v, t) = 0",E. Proof of Lemma 3,[0],[0]
"if and only if (v, β) is an eigenpair of t with eigenvalue β",E. Proof of Lemma 3,[0],[0]
"= T (v,v,v).4 Denote the gradients ofQ with respect to v and t by
A(v, t) = ∇vQ(v, t) ∈ Rd×d, B(v, t) = ∇tQ(v, t) ∈ Rd×s.
",E. Proof of Lemma 3,[0],[0]
"Let w ∈ Rs be the vectorization of W and let (u, λ) ∈",E. Proof of Lemma 3,[0],[0]
Sd−1 × R+ be a Newton-stable eigenpair of w with λ ≥ 1.,E. Proof of Lemma 3,[0],[0]
Since u is Newton-stable and λ,E. Proof of Lemma 3,[0],[0]
"> 0, A(u,w) is invertible.",E. Proof of Lemma 3,[0],[0]
"In addition, the following (d+ s)× (d+ s) matrix is invertible,
D(u,w) =
( A(u,w) B(u,w)
0",E. Proof of Lemma 3,[0],[0]
"Is
) .
",E. Proof of Lemma 3,[0],[0]
"4This does not precisely hold when β = 0 since Q(v, t) = 0 does not imply ‖v‖ = 1 in this case, but only that v is proportional to an eigenvector.
",E. Proof of Lemma 3,[0],[0]
"Let γD = 1/‖D(u,w)−1‖ > 0 be the smallest singular value of D(u,w) and let LD < ∞ be the Lipschitz constant of ∇Q(v, t) =",E. Proof of Lemma 3,[0],[0]
"[A(v, t), B(v, t)] ∈ Rd×(d+s) in a small neighborhood of (u,w), namely, ∀(v, t), (ṽ, t̃) in the neighborhood,
‖∇Q(v, t)−∇Q(ṽ, t̃)‖ ≤ LD‖(v, t)− (ṽ, t̃)‖.
Let Bε(w) ⊂ Rs be the ball of radius ε centered at w.",E. Proof of Lemma 3,[0],[0]
"Then by the implicit function theorem (Hubbard & Hubbard, 2015), for any ε ≤ ε1 := γ2D/(2LD), there exists a unique continuously differentiable mapping ũ : Bε(w)→ B2ε/γD (u) such that Q(ũ(w̃), w̃) = 0 for all w̃ ∈ Bε(w).",E. Proof of Lemma 3,[0],[0]
"In other words, for any w̃ such that ‖w̃ −w‖ ≤ ε, there exist a unique vector ũ in all B2ε/γD (u) that is an eigenpair of w̃. Equivalently, for W̃ such that ‖W̃ −W‖F ≤ ε, there exists a unique eigenvector ũ of W̃ such that
‖ũ− u‖ ≤ 2ε/γD",E. Proof of Lemma 3,[0],[0]
:= c1ε.,E. Proof of Lemma 3,[0],[0]
"(37)
",E. Proof of Lemma 3,[0],[0]
The bound on |λ̃ − λ| readily follows from (37).,E. Proof of Lemma 3,[0],[0]
"Indeed, let q : Rd+s → R be q(v, t) = T (v,v,v) and let Lλ be the Lipschitz constant of q in the neighborhood of (u,w).",E. Proof of Lemma 3,[0],[0]
"Then,
|λ̃− λ| = |q(ũ, w̃)− q(u,w)| ≤ Lλ √ ‖ũ− u‖2 + ‖w̃ −w‖2
≤",E. Proof of Lemma 3,[0],[0]
"Lλ √ 2
γD + 1 · ε",E. Proof of Lemma 3,[0],[0]
":= c2ε.
",E. Proof of Lemma 3,[0],[0]
"As for the Newton-stability of ũ, let r : Rd+s → R+ be r(v, t) = 1/‖A(v, t)−1‖, the minimal singular value of A(v, t).",E. Proof of Lemma 3,[0],[0]
"Since (u, λ) is a Newton-stable eigenpair of w, ∃γA > 0",E. Proof of Lemma 3,[0],[0]
"such that r(u,w) ≥ γA.",E. Proof of Lemma 3,[0],[0]
"Let Lγ be the Lipschitz constant of r(v, t) in the neighborhood (Golub & Van Loan, 2012).",E. Proof of Lemma 3,[0],[0]
"Then, for ε ≤ ε2 := γ/(2Lγ), we have r(ũ, w̃) ≥",E. Proof of Lemma 3,[0],[0]
"γA/2 > 0, so (ũ, λ̃) is a Newton-stable eigenpair of w̃.
Finally, we show that any other eigenvector ṽ of W̃ is apart from u. Since ũ is Newton-stable, there exists ε0 > 0",E. Proof of Lemma 3,[0],[0]
such that ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ ≥ 2ε0 for any other eigenvector ṽ. Hence, for ε ≤ ε0,
‖ṽ",E. Proof of Lemma 3,[0],[0]
− u‖ ≥ ∣∣‖ṽ − ũ‖ − ‖ũ−,E. Proof of Lemma 3,[0],[0]
u‖∣∣ ≥ ‖ṽ,E. Proof of Lemma 3,[0],[0]
"− ũ‖ − ε ≥ ε0.
",E. Proof of Lemma 3,[0],[0]
"Taking ε ≤ min{ε0, ε1, ε2} and c1, c2, ε0 as above concludes the proof of the lemma.
",E. Proof of Lemma 3,[0],[0]
"Lastly, for completeness, we show that γD ≥ γA√ γ2A+d .
γ−1D = ‖D(u,w) −1‖",E. Proof of Lemma 3,[0],[0]
"≤ √ ‖A(u,w)−1‖2(1 + ‖B(u,w)‖2) + ‖Is‖2
≤ √ 1 + 1 + ‖B(v,w)‖2
γ2A .",E. Proof of Lemma 3,[0],[0]
"(38)
",E. Proof of Lemma 3,[0],[0]
"To bound ‖B(u,w)‖, note thatQ(u,w) is linear in w and its i-th entry is given by
[Q(u,w)]i = ∑ k,l wiklukul",E. Proof of Lemma 3,[0],[0]
"− ( ∑ j,k,l wjklujukul)ui.
",E. Proof of Lemma 3,[0],[0]
"Thus, the d×m matrix B(u,w) has entries
[B(u,w)]i,(jkl) =",E. Proof of Lemma 3,[0],[0]
"[∇wQ(u,w)]i,(jkl) = (δij−uiuj)ukul,
which is independent of w. Recalling that ‖u‖ = 1,
‖B(u)‖2 ≤",E. Proof of Lemma 3,[0],[0]
"‖B(u)‖2F = d∑
i,j,k,l=1
(δij",E. Proof of Lemma 3,[0],[0]
"− uiuj)2u2ku2l
= d∑",E. Proof of Lemma 3,[0],[0]
"i,j=1 (δ2ij − 2δijuiuj + u2iu2j ) = d− 1.
Putting this bound in (38), we obtain γD ≥ γA√ γ2A+d .",E. Proof of Lemma 3,[0],[0]
Let v∗ ∈ W †.,F. Proof of Lemma 4,[0],[0]
Then ∃i ∈,F. Proof of Lemma 4,[0],[0]
"[d] such that v∗>W>h = hi ∈ {0, 1}.",F. Proof of Lemma 4,[0],[0]
"Hence, by (28), the c.d.f.",F. Proof of Lemma 4,[0],[0]
"Fv∗ of v∗>x corresponds to the two component GMM
(1− pi) · N (0, σ2‖v∗‖2) + pi · N (1, σ2‖v∗‖2).
",F. Proof of Lemma 4,[0],[0]
By Lemma 1 we have pi = 1/λ(v∗)2.,F. Proof of Lemma 4,[0],[0]
"Thus, Fv∗ = Gv∗ .
",F. Proof of Lemma 4,[0],[0]
"For the other direction, let v ∈",F. Proof of Lemma 4,[0],[0]
Vσ \W †.,F. Proof of Lemma 4,[0],[0]
"Since W is full rank, the d-dimensional vector u> = v>W> /∈",F. Proof of Lemma 4,[0],[0]
{e>i }di=1.,F. Proof of Lemma 4,[0],[0]
"Moreover, by Eq. (23) of Lemma 3,
inf v∈Vσ\W † min v∗∈W †
‖v − v∗‖ ≥ δ1 > 0.
",F. Proof of Lemma 4,[0],[0]
"Hence, there exists ε0 > 0",F. Proof of Lemma 4,[0],[0]
"such that
min i∈[d] ‖u− ei‖ ≥ ε0.
",F. Proof of Lemma 4,[0],[0]
"So by the expected rigidity condition (27), there exists η0 > 0",F. Proof of Lemma 4,[0],[0]
such that r(u) ≥ η0.,F. Proof of Lemma 4,[0],[0]
It follows that Fv has a component with mean that is bounded away from both 0 and 1 and thus Fv 6=,F. Proof of Lemma 4,[0],[0]
Gv .,F. Proof of Lemma 4,[0],[0]
"In particular, there exists η1 > 0 such that
sup t∈R |Fv(t)−Gv(t)| ≥ η1.",F. Proof of Lemma 4,[0],[0]
Recall that our sample of size 2n was split into two separate parts each of size,G. Proof of Lemma 5,[0],[0]
"n. The first n samples were used to estimate the tensor eigenvectors, and the last n samples to estimate the empirical cdf’s of their projections onto the eigenvectors.
",G. Proof of Lemma 5,[0],[0]
"For any v̂ that is close to a vector v, we bound ∆n(v̂) = ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
−Gv̂‖∞,G. Proof of Lemma 5,[0],[0]
"by the triangle inequality,
‖F̂v̂ −Gv̂‖∞ ≤ ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ + ‖Fv̂,G. Proof of Lemma 5,[0],[0]
− Fv‖∞ (39) + ‖Fv −Gv‖∞ + ‖Gv,G. Proof of Lemma 5,[0],[0]
"−Gv̂‖∞.
We now consider each of the four terms separately, starting with the first one.",G. Proof of Lemma 5,[0],[0]
"Since σ > 0, the cdf Fv̂ : R → [0, 1] is continuous and the distribution of ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
"− Fv̂‖∞ is independent of v̂. Then, by the Dvoretzky-Kiefer-Wolfowitz inequality, ‖F̂v̂",G. Proof of Lemma 5,[0],[0]
− Fv̂‖∞ is w.h.p.,G. Proof of Lemma 5,[0],[0]
"of order O(1/ √ n) for any v̂, and in particular tends to zero as n→ 0.
",G. Proof of Lemma 5,[0],[0]
"As for the second term, write v̂ = v + η.",G. Proof of Lemma 5,[0],[0]
"Then,
v̂>x = v>x+ η>x.
Recall that x = W>h + σξ.",G. Proof of Lemma 5,[0],[0]
"Hence, |η>x| ≤",G. Proof of Lemma 5,[0],[0]
‖W‖2 √ d‖η‖ + σ|η>ξ|.,G. Proof of Lemma 5,[0],[0]
The term η>ξ is simply a zero mean Gaussian random variable with standard deviation σ‖η‖.,G. Proof of Lemma 5,[0],[0]
"So, there exists",G. Proof of Lemma 5,[0],[0]
Kn > √ d‖W‖2 +,G. Proof of Lemma 5,[0],[0]
"σn1/3 such that with probability tending to one as n→∞, for all n samples xj ∈ X , |η>xj | ≤ Kn‖η‖.",G. Proof of Lemma 5,[0],[0]
"Thus, |v̂>x − v>x| can be bounded by Kn‖v̂ − v‖.",G. Proof of Lemma 5,[0],[0]
"This, in turn, implies that
‖Fv̂ − Fv‖∞ ≤ LKn‖v̂",G. Proof of Lemma 5,[0],[0]
"− v‖,
where L = maxt F ′v(t), which is finite for any σ > 0.",G. Proof of Lemma 5,[0],[0]
"Now, suppose the sequence v̂(n) converges to some v at rate OP (1/ √ n).",G. Proof of Lemma 5,[0],[0]
"Since Kn grows much more slowly with n, this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Let us next consider the fourth term, and leave the third term to the end.",G. Proof of Lemma 5,[0],[0]
Here note that Gv is continuous in its parameter v.,G. Proof of Lemma 5,[0],[0]
"So if the sequence v̂(n) converges to some v, then this term tends to zero.
",G. Proof of Lemma 5,[0],[0]
"Finally, consider the third term.",G. Proof of Lemma 5,[0],[0]
"If the limiting vector v belongs to the correct set, namely v∗ ∈W †, then Fv = Gv , and thus overall ‖F̂v̂ −Gv̂‖∞ tends to zero as required.
",G. Proof of Lemma 5,[0],[0]
"In contrast, if v̂ converges to a vector v /∈W †, then instead of Eq.",G. Proof of Lemma 5,[0],[0]
"(39) we invoke the following inequality:
‖F̂v̂ −Gv̂‖∞ ≥ ‖Fv −Gv‖∞",G. Proof of Lemma 5,[0],[0]
− ‖Fv − Fv̂‖∞ −‖Fv̂ − F̂v̂‖∞,G. Proof of Lemma 5,[0],[0]
"− ‖Gv̂ −Gv‖∞.
Here ‖Fv −Gv‖∞ is strictly larger than zero whereas the three other remaining terms tend to zero as n → ∞ as above.",G. Proof of Lemma 5,[0],[0]
Multiplying (35) from the right by the full rank matrix Y −1,H. Proof of Lemma 6,[0],[0]
"we obtain the equations
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[Y >]ij , ∀i, j ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Note that for all i ∈",H. Proof of Lemma 6,[0],[0]
"[d],
[W(Y, Y, I)]iij =",H. Proof of Lemma 6,[0],[0]
"[W(yi,yi, I)]j .
SinceW is symmetric, we thus have
W(I,yi,yi) = yi, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
Writing yi = ui/λi we obtain the eigenpair equation
W(I,ui,ui) = λiui, ∀i ∈",H. Proof of Lemma 6,[0],[0]
"[d].
",H. Proof of Lemma 6,[0],[0]
The other direction readily follows from the definition of eigenpairs.,H. Proof of Lemma 6,[0],[0]
Figure 5 shows the simulation runtime of the spectral approach and that of SHL vs. the number of samples n.,I. Simulation runtime,[0],[0]
The setup is similar to the one described in section 5.,I. Simulation runtime,[0],[0]
"The runtime of SHL increases linearly with n, as expected.",I. Simulation runtime,[0],[0]
"For our spectral method, for lower values of n the dominant factor is the computation of tensor eigenvectors, which does not depend on n. For large n, the dominant factor is the computation of the correlation tensor, linear in n.",I. Simulation runtime,[0],[0]
"In Algorithm 1, we modify the diagonal elements of M,M by (17).",J. Matrix and tensor denoising,[0],[0]
"This modification is suited for additive Gaussian noise, but is not applicable for the case where X = binomial(2,WTH).",J. Matrix and tensor denoising,[0],[0]
"Instead, we implemented a method derived in (Jain & Oh, 2014) for a similar setup.
",J. Matrix and tensor denoising,[0],[0]
"First, we treat the diagonal elements of Mσ as missing data, and complete them with the following iterative steps.",J. Matrix and tensor denoising,[0],[0]
"(i) compute the first d eigenpairs {vi, λi} of R(k); and (ii) update the diagonal elements by R(k+1)jj =",J. Matrix and tensor denoising,[0],[0]
"( ∑ i λiviv > i )jj .
",J. Matrix and tensor denoising,[0],[0]
"Next, instead of computingMσ via (17) and thenWσ via (19), we compute Wσ directly by solving the following system of linear equations.",J. Matrix and tensor denoising,[0],[0]
"Let K† be the pseudo-inverse
matrix of K, and PΩ(T ) denote a masking operation over the tensor T such that,
PΩ(T ) =",J. Matrix and tensor denoising,[0],[0]
{ Tijk i 6=,J. Matrix and tensor denoising,[0],[0]
j 6= k 0,J. Matrix and tensor denoising,[0],[0]
"o.w
We estimateW by the following minimization problem,
Ŵ = argmin W
‖PΩ ( W(K†,K†,K†) )",J. Matrix and tensor denoising,[0],[0]
"− PΩ(M)‖2F
This method depends only on the off-diagonal elements of M and M and hence is applicable whenever E[X|H] = WTH and the noise has bounded variance.",J. Matrix and tensor denoising,[0],[0]
"In section 5, we compare the results of algorithm 1 with and without an additional single weighted least square step.",K. Adding a weighted least square step to the spectral method,[0],[0]
"Given an estimate Ŵ , for each observed instance xj we calculate the conditional likelihood L(xj |h) for the 2d possible binary vectors h ∈ {0, 1}d,
L(xj |h) = 1√
2πσ2 exp
( − ‖xj − ŴTh‖2 / (2σ2) )",K. Adding a weighted least square step to the spectral method,[0],[0]
"For each instance xj , we keep the top K = 6 vectors h1j , . . .",K. Adding a weighted least square step to the spectral method,[0],[0]
",hKj with the highest likelihood.",K. Adding a weighted least square step to the spectral method,[0],[0]
Let Π ∈,K. Adding a weighted least square step to the spectral method,[0],[0]
"[0, 1]K×n be a weight matrix such that Πkj is proportional to L(xj |hkj), and ∑ k Πkj = 1 for all j. The new estimate Ŵwls is the minimizer of the weighted least square problem,
Ŵwls = argmin W n∑ j=1 K∑ k=1 Πkj‖xj −WThkj‖2.",K. Adding a weighted least square step to the spectral method,[0],[0]
"In section 5, we compare the results of the spectral approach to the following ALS iterations, with a random starting point.
",L. Alternating least squares for W and H,[0],[0]
W (k) = argmin,L. Alternating least squares for W and H,[0],[0]
"W∈Rd×m ‖X −WTH(k−1)‖2F
Ĥ(k) = argmin H∈Rd×n",L. Alternating least squares for W and H,[0],[0]
‖X,L. Alternating least squares for W and H,[0],[0]
"− (W (k))TH‖2F
H(k) = argmin H∈{0,1}d×n ‖H − Ĥ(k)‖2F ,",L. Alternating least squares for W and H,[0],[0]
"The simulated admixture data was generated via the following steps:
1.",M. Genetic admixture simulations,[0],[0]
"We used SCRM (Staab et al., 2015) to simulate a split between d = 3 ancestral populations, with separation
time of 4000 generations.",M. Genetic admixture simulations,[0],[0]
The simulator generated 40 chromosomes of length 250 · 106 for each of the three populations.,M. Genetic admixture simulations,[0],[0]
"The simulation parameters were determined as N0 = 104 effective population size, 10−8 mutation rate (per base pair per generation), and 10−8 recombination rate (per base pair per generation).
2.",M. Genetic admixture simulations,[0],[0]
"We sampled the proportion matrix W from a Dirichlet distribution with parameter α.
3.",M. Genetic admixture simulations,[0],[0]
"Two chromosomes of length 250 · 106 were created for each of the m = 50 admixed individuals with the following steps: (i) An ancestral population was sampled according to W, say, population hA. (ii)",M. Genetic admixture simulations,[0],[0]
"One of the 40 chromosomes was sampled from hA, say hA(k) (iii) A block length l was sampled from an exponential distribution with rate 20 per Morgan corresponding admixture event happening 20 generations ago (in our case, 1 Morgan was 108 base pairs).",M. Genetic admixture simulations,[0],[0]
(iv) A block of length l was copied from chromosome hA(k) to the corresponding locations in the new admixed chromosome.,M. Genetic admixture simulations,[0],[0]
"We repeated steps (i)-(iv) until completion of the chromosome.
",M. Genetic admixture simulations,[0],[0]
"N. Analysis of DNA methylation data The dataset is part of the supplementary material of (Houseman et al., 2012).",M. Genetic admixture simulations,[0],[0]
"The observed matrix X ∈
[0, 1]m×n consists of m = 12 blood samples, each with the DNA methylation measurements in n = 500 sites (called CpGs).
",M. Genetic admixture simulations,[0],[0]
The statistical model for X is similar to that of Eq.,M. Genetic admixture simulations,[0],[0]
(1).,M. Genetic admixture simulations,[0],[0]
"We assume that each blood cell is a mixture of d = 4 cell types, with unknown proportions.",M. Genetic admixture simulations,[0],[0]
The latent variables h correspond to the presence or absence of methylation in each site for the 4 cell types.,M. Genetic admixture simulations,[0],[0]
"Given the DNA methylation array, the task is to estimate the proportion matrix W .
",M. Genetic admixture simulations,[0],[0]
The upper and lower panels of Figure 6 correspond to the real and estimated mixture matrix.,M. Genetic admixture simulations,[0],[0]
"For comparison, we performed the steps described in detail for this dataset in Slawski et al. (2013, Section 4).",M. Genetic admixture simulations,[0],[0]
"For both methods we measured the l1 distance between the real and estimated mixture matrices,
1
mn ∑ ij |Wij",M. Genetic admixture simulations,[0],[0]
"− Ŵij |
The l1 distance were equal to 0.003 for SHL and 0.00056 for the spectral approach.",M. Genetic admixture simulations,[0],[0]
Latent variable models with hidden binary units appear in various applications.,abstractText,[0],[0]
"Learning such models, in particular in the presence of noise, is a challenging computational problem.",abstractText,[0],[0]
"In this paper we propose a novel spectral approach to this problem, based on the eigenvectors of both the second order moment matrix and third order moment tensor of the observed data.",abstractText,[0],[0]
"We prove that under mild non-degeneracy conditions, our method consistently estimates the model parameters at the optimal parametric rate.",abstractText,[0],[0]
"Our tensor-based method generalizes previous orthogonal tensor decomposition approaches, where the hidden units were assumed to be either statistically independent or mutually exclusive.",abstractText,[0],[0]
We illustrate the consistency of our method on simulated data and demonstrate its usefulness in learning a common model for population mixtures in genetics.,abstractText,[0],[0]
Learning Binary Latent Variable Models: A Tensor Eigenpair Approach,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 377–387 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1035",text,[0],[0]
"Detection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems.",1 Introduction,[0],[0]
"Traditional sentiment analyzers and
sarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c).",1 Introduction,[0],[0]
"Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns).",1 Introduction,[0],[0]
"It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels.",1 Introduction,[0],[0]
"For example, the sentence I really love my job.",1 Introduction,[0],[0]
I work 40 hours a week to be this poor.,1 Introduction,[0],[0]
requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job.,1 Introduction,[0],[0]
"In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.
",1 Introduction,[0],[0]
"Mishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better.",1 Introduction,[0],[0]
The hypothesis here is that human gaze activities are related to the cognitive processes in the brain that combine the “external knowledge” that the reader possesses with textual clues that she / he perceives.,1 Introduction,[0],[0]
"While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification, and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c).",1 Introduction,[0],[0]
"These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately
377
capture all forms of textual subtleties (b) Eyemovement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult.",1 Introduction,[0],[0]
"So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Network (CNN).",1 Introduction,[0],[0]
"We test our technique on two publicly available datasets enriched with eyemovement information, used for binary classification tasks of sentiment polarity and sarcasm detection.",1 Introduction,[0],[0]
Our experiments show that the automatically extracted features often help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.,1 Introduction,[0],[0]
"The datasets used in our experiments, resources and other relevant pointers are available at http://www.cfilt.iitb.ac.in/ cognitive-nlp
The rest of the paper is organized as follows.",1 Introduction,[0],[0]
Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting.,1 Introduction,[0],[0]
"In Section 3, we argue why CNN is preferred over other available alternatives for feature extraction.",1 Introduction,[0],[0]
The CNN architecture is proposed and discussed in Section 4.,1 Introduction,[0],[0]
Section 5 describes our experimental setup and results are discussed in Section 6.,1 Introduction,[0],[0]
We provide a detailed analysis of the results along with some insightful observations in Section 7.,1 Introduction,[0],[0]
"Section 8 points to relevant literature followed by Section 9 that concludes the paper.
",1 Introduction,[0],[0]
Terminology A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest.,1 Introduction,[0],[0]
Forward and backward saccades are called progressions and regressions respectively.,1 Introduction,[0],[0]
A scanpath is a line graph that contains fixations as nodes and saccades as edges.,1 Introduction,[0],[0]
"Presence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity",2 Eye-movement and Linguistic Subtleties,[0],[0]
"or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive
load results in longer fixation duration (Kliegl et al., 2004).
",2 Eye-movement and Linguistic Subtleties,[0],[0]
Mishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1).,2 Eye-movement and Linguistic Subtleties,[0],[0]
"For sentiment bearing texts, highly subtle eyemovement patterns are observed for semantically/pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations, etc.) than the simple ones (Mishra et al., 2016b).",2 Eye-movement and Linguistic Subtleties,[0],[0]
The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs.,2 Eye-movement and Linguistic Subtleties,[0],[0]
"In our work, CNN takes the onus of feature engineering.",2 Eye-movement and Linguistic Subtleties,[0],[0]
"CNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012).",3 Why Convolutional Neural Network?,[0],[0]
"Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours, and removal of redundant backgrounds.",3 Why Convolutional Neural Network?,[0],[0]
"We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features.",3 Why Convolutional Neural Network?,[0],[0]
"For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical il-
lustration).",3 Why Convolutional Neural Network?,[0],[0]
"With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to.",3 Why Convolutional Neural Network?,[0],[0]
Figure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The components are explained below.,4 Learning Feature Representations: The CNN Architecture,[0],[0]
The text component is quite similar to the one proposed by Kim (2014) for sentence classification.,4.1 Text Component,[0],[0]
Words (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK).,4.1 Text Component,[0],[0]
"As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static throughout training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT).",4.1 Text Component,[0],[0]
"We separately experiment with static, non-static and multi-channel variants.
",4.1 Text Component,[0],[0]
"For each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever
necessary to tackle length variations) by concatenating the word embeddings.
",4.1 Text Component,[0],[0]
"x1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)
where ⊕ is the concatenation operator.",4.1 Text Component,[0],[0]
"To extract local features1, convolution operation is applied.",4.1 Text Component,[0],[0]
"Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words.",4.1 Text Component,[0],[0]
"A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output.",4.1 Text Component,[0],[0]
"Mathematically,
ci = f(W.xi:i+H−1 + b) (2)
where b ∈ R is the bias and f is the non-linear function.",4.1 Text Component,[0],[0]
"This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .
",4.1 Text Component,[0],[0]
c =,4.1 Text Component,[0],[0]
"[c1, c2, c3, ..., cN−H+1] (3)
A global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map.",4.1 Text Component,[0],[0]
"The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.
",4.1 Text Component,[0],[0]
We have described the process by which one feature is extracted from one filter (red bordered portions in Figure 3 illustrate the case of H = 2).,4.1 Text Component,[0],[0]
The model uses multiple filters for each filter size to obtain multiple features representing the text.,4.1 Text Component,[0],[0]
"In the MULTICHANNELTEXT variant, for a window of H words, the convolution operation is separately applied on both the embedding channels.",4.1 Text Component,[0],[0]
Local features learned from both the channels are concatenated before applying max-pooling.,4.1 Text Component,[0],[0]
The gaze component deals with scanpaths of multiple participants annotating the same text.,4.2 Gaze Component,[0],[0]
"Scanpaths can be pre-processed to extract two sequences3 of gaze data to form separate channels of input: (1) A sequence of normalized4 durations of fixations (in milliseconds) in the order in which
1features specific to a region in case of images or window of words in case of text
2mean pooling does not perform well.",4.2 Gaze Component,[0],[0]
"3like text-input, gaze sequences are padded where necessary 4scaled across participants using min-max normalization to reduce subjectivity
they appear in the scanpath, and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath.",4.2 Gaze Component,[0],[0]
These channels are related to two fundamental gaze attributes such as fixation and saccade respectively.,4.2 Gaze Component,[0],[0]
"With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.
",4.2 Gaze Component,[0],[0]
"For each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence).",4.2 Gaze Component,[0],[0]
"Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of the ith participant.",4.2 Gaze Component,[0],[0]
"Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2).",4.2 Gaze Component,[0],[0]
"For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,
cij = f(W.gij:(i+J−1)(j+J−1) + b) (4)
where b ∈ R is the bias and f is a non-linear func-
tion.",4.2 Gaze Component,[0],[0]
"This operation is applied to each possible window of size J2 to produce a feature map (c),
c =[c11, c12, c13, ..., c1(G−J+1),
c21, c22, c23, ..., c2(G−J+1), ...,
c(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]
(5)
",4.2 Gaze Component,[0],[0]
A global feature is then obtained by applying max pooling operation.,4.2 Gaze Component,[0],[0]
"Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2).",4.2 Gaze Component,[0],[0]
"For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible",4.2 Gaze Component,[0],[0]
"i, j.
We have described the process by which one feature is extracted from one filter (of 2D window size J2 and the max-pooling window size of M2).",4.2 Gaze Component,[0],[0]
"In Figure 3, red and blue bordered portions illustrate the cases of J2 =",4.2 Gaze Component,[0],[0]
"[3, 3] and M2 = [2, 2] respectively.",4.2 Gaze Component,[0],[0]
"Like the text component, the gaze component also uses multiple filters for each filter size to obtain multiple features representing the gaze input.",4.2 Gaze Component,[0],[0]
"In the MULTICHANNELGAZE variant, for a 2D window of J2, the convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before maxpooling is applied.
",4.2 Gaze Component,[0],[0]
"Once the global features are learned from both the text and gaze components, they are merged
and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.
",4.2 Gaze Component,[0],[0]
"The gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others.",4.2 Gaze Component,[0],[0]
"The only way we can think of for addressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering.",4.2 Gaze Component,[0],[0]
"Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge.",4.2 Gaze Component,[0],[0]
"As of now, training and testing are carried out by keeping the order of the input constant.",4.2 Gaze Component,[0],[0]
We now share several details regarding our experiments below.,5 Experiment Setup,[0],[0]
We conduct experiments for two binaryclassification tasks of sentiment and sarcasm using two publicly available datasets enriched with eye-movement information.,5.1 Dataset,[0],[0]
Dataset 1 has been released by Mishra et al. (2016a).,5.1 Dataset,[0],[0]
It contains 994 text snippets with 383 positive and 611 negative examples.,5.1 Dataset,[0],[0]
"Out of the 994 snippets, 350 are sarcastic.",5.1 Dataset,[0],[0]
"Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive, and 400 are negative.",5.1 Dataset,[0],[0]
Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively.,5.1 Dataset,[0],[0]
"With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1).",5.2 CNN Variants,[0],[0]
"Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2).",5.2 CNN Variants,[0],[0]
"With both text and gaze components, 9 more variants could thus beexperimented with.",5.2 CNN Variants,[0],[0]
"For text component, we experiment with filter widths (H) of [3, 4].",5.3 Hyper-parameters,[0],[0]
"For the gaze component, 2D filters (J2) set to [3× 3], [4× 4] respectively.",5.3 Hyper-parameters,[0],[0]
"The
max pooling 2D window, M2, is set to [2× 2].",5.3 Hyper-parameters,[0],[0]
"In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window.",5.3 Hyper-parameters,[0],[0]
These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system.,5.3 Hyper-parameters,[0],[0]
"Tuning of hyper-parameters might help in improving the performance of our framework, which is on our future research agenda.",5.3 Hyper-parameters,[0],[0]
"For regularization dropout is employed both on the embedding and the penultimate layers with a constraint on l2-norms of the weight vectors (Hinton et al., 2012).",5.4 Regularization,[0],[0]
"Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units during forward propagation.",5.4 Regularization,[0],[0]
We set p to 0.25.,5.4 Regularization,[0],[0]
"We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1.",5.5 Training,[0],[0]
The input batch size is set to 32 and number of training iterations (epochs) is set to 200.,5.5 Training,[0],[0]
10% of the training data is used for validation.,5.5 Training,[0],[0]
"Initializing the embedding layer with of pretrained embeddings can be more effective than random initialization (Kim, 2014).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"In our experiments, we have used embeddings learned using the movie reviews with one sentence per review dataset (Pang and Lee, 2005).",5.6 Use of Pre-trained Embeddings:,[0],[0]
"It is worth noting that, for a small dataset like ours, using a small data-set like the one from (Pang and Lee, 2005) helps in reducing the number model parameters resulting in faster learning of embeddings.",5.6 Use of Pre-trained Embeddings:,[0],[0]
The results are also quite close to the ones obtained using word2vec facilitated by Mikolov et al. (2013).,5.6 Use of Pre-trained Embeddings:,[0],[0]
"For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features.",5.7 Comparison with Existing Work,[0],[0]
"For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm).",5.7 Comparison with Existing Work,[0],[0]
We follow the same 10-fold train-test configuration as these existing works for consistency.,5.7 Comparison with Existing Work,[0],[0]
"In this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks.",6 Results,[0],[0]
Table 1 presents results for sentiment analysis task.,6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%.",6.1 Results for Sentiment Analysis Task,[0],[0]
This improvement is statistically significant of p < 0.05 as confirmed by McNemar test.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).
",6.1 Results for Sentiment Analysis Task,[0],[0]
"For dataset 2, CNN based approaches do not perform better than manual feature based approaches.",6.1 Results for Sentiment Analysis Task,[0],[0]
"However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%.",6.1 Results for Sentiment Analysis Task,[0],[0]
"We observe that for dataset 2, training accuracy reaches 100 within 25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting.",6.1 Results for Sentiment Analysis Task,[0],[0]
Tuning the regularization parameters specific to dataset 2 may help here.,6.1 Results for Sentiment Analysis Task,[0],[0]
"Even though CNN might
not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants.",6.1 Results for Sentiment Analysis Task,[0],[0]
"For sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2).",6.2 Results for Sarcasm Detection Task,[0],[0]
"However, the improvement by adding the gaze component to the CNN network is just 1.34%, which is statistically insignificant over CNN with text component.",6.2 Results for Sarcasm Detection Task,[0],[0]
"While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset.",6.2 Results for Sarcasm Detection Task,[0],[0]
"This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings.",6.2 Results for Sarcasm Detection Task,[0],[0]
"In this section, some important observations from our experiments are discussed.",7 Discussion,[0],[0]
"Embedding dimension has proven to have a deep impact on the performance of neural systems (dos Santos and Gatti, 2014; Collobert et al., 2011).
",7.1 Effect of Embedding Dimension Variation,[0],[0]
We repeated our experiments by varying the embedding dimensions in the range of [50-300]5 and observed that reducing embedding dimension improves the F-scores by a little margin.,7.1 Effect of Embedding Dimension Variation,[0],[0]
Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small.,7.1 Effect of Embedding Dimension Variation,[0],[0]
"We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component.",7.1 Effect of Embedding Dimension Variation,[0],[0]
"Non-static embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples).",7.2 Effect of Static / Non-static Text Channels,[0],[0]
"For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due
5a standard range (Liu et al., 2015; Melamud et al., 2016)
to the presence of irony / sarcasm) better.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Fixation channel does not help much, may be because of higher variance in fixation duration.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"For sarcasm detection, fixation and saccade channels perform with similar accuracy when employed separately.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and nonsarcastic classes, as opposed to sentiment classes.",7.3 Effect of Fixation / Saccade Channels,[0],[0]
"To examine how good the features learned by the CNN are, we analyzed the features for a few example cases.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Figure 4 presents some of the example test cases for the task of sarcasm detection.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Output of the hidden layer after merge layer is considered as features learned by the network.,7.4 Effectiveness of the CNN-learned Features,[0],[0]
"We plot the features, in the form of color-bars, following Li et al. (2016) - denser col-
ors representing feature with higher magnitude.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In Figure 4, we show only two representative model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"As one can see, addition of gaze information helps to generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"This difference is not clear in feature vectors learned by text-only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"Example 4 is incorrectly classified by both the systems, perhaps due to lack of context.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
"In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts.",7.4 Effectiveness of the CNN-learned Features,[0],[0]
Sentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time.,8 Related Work,[0],[0]
"Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc.",8 Related Work,[0],[0]
"(Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties (Balamurali et al., 2011).",8 Related Work,[0],[0]
"For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015)",8 Related Work,[0],[0]
"(b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014).",8 Related Work,[0],[0]
Recent systems are based on variants of deep neural network built on the top of embeddings.,8 Related Work,[0],[0]
"A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined archi-
tecture (Wang et al., 2016).",8 Related Work,[0],[0]
"Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.
",8 Related Work,[0],[0]
"Eye-tracking technology is a relatively new NLP, with very few systems directly making use of gaze data in prediction frameworks.",8 Related Work,[0],[0]
"Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features.",8 Related Work,[0],[0]
The closest works to ours are by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze and text data for sentiment and sarcasm detection tasks.,8 Related Work,[0],[0]
These recent advancements motivate us to explore the cognitive NLP paradigm.,8 Related Work,[0],[0]
"In this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification.",9 Conclusion and Future Directions,[0],[0]
"On multiple published datasets for which gaze information is available, our systems could often achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.",9 Conclusion and Future Directions,[0],[0]
An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers.,9 Conclusion and Future Directions,[0],[0]
"Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions, etc.) to obtain better results, (b) exploring the applicability of our technique for documentlevel sentiment analysis and (c) applying our framework to related problems, such as emotion analysis, text summarization, and questionanswering, where considering textual clues alone may not prove to be sufficient.",9 Conclusion and Future Directions,[0],[0]
"We thank Anoop Kunchukuttan, Joe Cheri Ross, and Sachin Pawar, research scholars of the Center for Indian Language Technology (CFILT), IIT Bombay for their valuable inputs.",Acknowledgments,[0],[0]
"Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text-based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc..",abstractText,[0],[0]
Such extraction of features is typically manual.,abstractText,[0],[0]
"We contend that manual extraction of features may not be the best way to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system.",abstractText,[0],[0]
We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection.,abstractText,[0],[0]
Our proposed framework is based on Convolutional Neural Network (CNN).,abstractText,[0],[0]
The CNN learns features from both gaze and text and uses them to classify the input text.,abstractText,[0],[0]
"We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features often yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",abstractText,[0],[0]
Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1839–1848 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1839",text,[0],[0]
"In the last few years, convolutional neural networks (CNNs) have demonstrated remarkable progress in various natural language processing applications (Collobert et al., 2011), including sentence/document classification (Kim, 2014; Zhang et al., 2015; Wang et al., 2018), text sequence matching (Hu et al., 2014; Yin et al., 2016; Shen et al., 2017), generic text representations (Gan et al., 2016; Tang et al., 2018), language modeling (Dauphin et al., 2017), machine translation (Gehring et al., 2017) and abstractive sentence summarization (Gehring et al., 2017).",1 Introduction,[0],[0]
"CNNs are typically applied to tasks where feature extrac-
tion and a corresponding supervised task are approached jointly (LeCun et al., 1998).",1 Introduction,[0],[0]
"As an encoder network for text, CNNs typically convolve a set of filters, of window size n, with an inputsentence embedding matrix obtained via word2vec (Mikolov et al., 2013) or Glove (Pennington et al., 2014).",1 Introduction,[0],[0]
"Different filter sizes n may be used within the same model, exploiting meaningful semantic features from different n-gram fragments.
",1 Introduction,[0],[0]
"The learned weights of CNN filters, in most cases, are assumed to be fixed regardless of the input text.",1 Introduction,[0],[0]
"As a result, the rich contextual information inherent in natural language sequences may not be fully captured.",1 Introduction,[0],[0]
"As demonstrated in Cohen and Singer (1999), the context of a word tends to greatly influence its contribution to the final supervised tasks.",1 Introduction,[0],[0]
"This observation is consistent with the following intuition: when reading different types of documents, e.g., academic papers or newspaper articles, people tend to adopt distinct strategies for better and more effective understanding, leveraging the fact that the same words or phrases may have different meaning or imply different things, depending on context.
",1 Introduction,[0],[0]
Several research efforts have sought to incorporate contextual information into CNNs to adaptively extract text representations.,1 Introduction,[0],[0]
"One common strategy is the attention mechanism, which is typically employed on top of a CNN (or Long ShortTerm Memory (LSTM)) layer to guide the extraction of semantic features.",1 Introduction,[0],[0]
"For the embedding of a single sentence, Lin et al. (2017) proposed a selfattentive model that attends to different parts of a sentence and combines them into multiple vector representations.",1 Introduction,[0],[0]
"However, their model needs considerably more parameters to achieve performance gains over traditional CNNs.",1 Introduction,[0],[0]
"To match sentence pairs, Yin et al. (2016) introduced an attentionbased CNN model, which re-weights the convolution inputs or outputs, to extract interdepen-
dent sentence representations.",1 Introduction,[0],[0]
Wang et al. (2016); Wang and Jiang (2017) explore a compare and aggregate framework to directly capture the wordby-word matching between two paired sentences.,1 Introduction,[0],[0]
"However, these approaches suffer from the problem of high matching complexity, since a similarity matrix between pairwise words needs to be computed, and thus it is computationally inefficient or even prohibitive when applied to long sentences (Mou et al., 2016).
",1 Introduction,[0],[0]
"In this paper, we propose a generic approach to learn context-aware convolutional filters for natural language understanding.",1 Introduction,[0],[0]
"In contrast to traditional CNNs, the convolution operation in our framework does not have a fixed set of filters, and thus provides the network with stronger modeling flexibility and capacity.",1 Introduction,[0],[0]
"Specifically, we introduce a meta network to generate a set of contextaware filters, conditioned on specific input sentences; these filters are adaptively applied to either the same (Section 3.2) or different (Section 3.3) text sequences.",1 Introduction,[0],[0]
"In this manner, the learned filters vary from sentence to sentence and allow for more fine-grained feature abstraction.
",1 Introduction,[0],[0]
"Moreover, since the generated filters in our framework can adapt to different conditional information available (labels or paired sentences), they can be naturally generalized to model sentence pairs.",1 Introduction,[0],[0]
"In this regard, we propose a novel bidirectional filter generation mechanism to allow interactions between sentence pairs while constructing context-aware representations.
",1 Introduction,[0],[0]
"We investigate the effectiveness of our Adaptive Context-sensitive CNN (ACNN) framework on several text processing tasks: ontology classification, sentiment analysis, answer sentence selection and paraphrase identification.",1 Introduction,[0],[0]
We show that the proposed methods consistently outperforms the standard CNN and attention-based CNN baselines.,1 Introduction,[0],[0]
"Our work provides a new perspective on how to incorporate contextual information into text representations, which can be combined with more sophisticated structures to achieve even better performance in the future.",1 Introduction,[0],[0]
"Learning deep text representations has attracted much attention recently, since they can potentially benefit a wide range of NLP applications (Collobert et al., 2011; Kim, 2014; Wang et al., 2017a; Shen et al., 2018a; Tang and de Sa, 2018; Zhang
et al., 2018).",2 Related Work,[0],[0]
CNNs have been extensively investigated as the encoder networks of natural language.,2 Related Work,[0],[0]
"Our work is in line with previous efforts on improving the adaptivity and flexibility of convolutional neural networks (Jeon and Kim, 2017; De Brabandere et al., 2016).",2 Related Work,[0],[0]
Jeon and Kim (2017) proposed to enhance the transformation modeling capacity of CNNs by adaptively learning the filter shapes through backpropagation.,2 Related Work,[0],[0]
"De Brabandere et al. (2016) introduced an architecture to generate the future frames conditioned on given image(s), by adapting the CNN filter weights to the motion within previous video frames.",2 Related Work,[0],[0]
"Although CNNs have been widely adopted in a large number of NLP applications, improving the adaptivity of vanilla CNN modules has been considerably less studied.",2 Related Work,[0],[0]
"To the best of our knowledge, the work reported in this paper is the first attempt to develop more flexible and adjustable CNN architecture for modeling sentences.
",2 Related Work,[0],[0]
"Our use of a meta network to generate parameters for another network is directly inspired by the recent success of hypernetworks for textgeneration tasks (Ha et al., 2017), and dynamic parameter-prediction for video-frame generation (De Brabandere et al., 2016).",2 Related Work,[0],[0]
"In contrast to these works that focus on generation problems, our model is based on context-aware CNN filters and is aimed at abstracting more informative and predictive sentence features.",2 Related Work,[0],[0]
"Most similar to our work, Liu et al. (2017) designed a meta network to generate compositional functions over tree-structured neural networks for encapsulating sentence features.",2 Related Work,[0],[0]
"However, their model is only suitable for encoding individual sentences, while our framework can be readily generalized to capture the interactions between sentence pairs.",2 Related Work,[0],[0]
"Moreover, our framework is based on CNN models, which is advantageous due to fewer parameters and highly parallelizable computations relative to sequential-based models.",2 Related Work,[0],[0]
"The CNN architectures in (Kim, 2014; Collobert et al., 2011) are typically utilized for extracting sentence representations, by a composition of a convolutional layer and a max-pooling operation over all resulting feature maps.",3.1 Basic CNN for text representations,[0],[0]
"Let the words of a sentence of length T (padded where necessary) be x1, x2, ... , xT .",3.1 Basic CNN for text representations,[0],[0]
"The sentence can be represented
as a matrix X ∈ Rd×T , where each column represents a d-dimensional embedding of the corresponding word.
",3.1 Basic CNN for text representations,[0],[0]
"In the convolutional layer, a set of filters with weights W ∈ RK×h×d is convolved with every window of h words within the sentence, i.e., {x1:h, x2:h+1, . . .",3.1 Basic CNN for text representations,[0],[0]
", xT−h+1:T }, where K is the number of output feature maps (and filters).",3.1 Basic CNN for text representations,[0],[0]
"In this manner, feature maps p for these h-gram text fragments are generated as:
pi = f(W × xi:i+h−1 + b) (1)
where i = 1, 2, ..., T − h + 1 and × denotes the convolution operator at the ith shift location.",3.1 Basic CNN for text representations,[0],[0]
"Parameter b ∈ RK is the bias term and f(·) is a non-linear function, implemented as a rectified linear unit (ReLU) in our experiments.",3.1 Basic CNN for text representations,[0],[0]
"The output feature maps of the convolutional layer, i.e., p ∈ RK×(T−h+1) are then passed to the pooling layer, which takes the maximum value in every row of p, forming a K-dimensional vector, z.",3.1 Basic CNN for text representations,[0],[0]
This operation attempts to keep the most salient feature detected by every filter and discard the information from less fundamental text fragments.,3.1 Basic CNN for text representations,[0],[0]
"Moreover, the max-over-time nature of the pooling operation (Collobert et al., 2011) guarantees that the size of the obtained representation is independent of the sentence length.
",3.1 Basic CNN for text representations,[0],[0]
"Note that in basic CNN sentence encoders, filter weights are the same for different inputs, which may be suboptimal for feature extraction (De Brabandere et al., 2016), especially in the case where conditional information is available.",3.1 Basic CNN for text representations,[0],[0]
"The proposed architecture to learn contextsensitive filters is composed of two principal modules: (i) a filter generation module, which produces a set of filters conditioned on the input sentence; and (ii) an adaptive convolution module, which applies the generated filters to an input sentence (this sentence may be either the same as or different from the first input, as discussed further in Section 3.3).",3.2 Learning context-sensitive filters,[0],[0]
"The two modules are jointly differentiable, and the overall architecture can be trained in an end-to-end manner.",3.2 Learning context-sensitive filters,[0],[0]
"Since the generated filters are sample-specific, our ACNN feature extractor for text tends to have stronger predictive power than a basic CNN encoder.",3.2 Learning context-sensitive filters,[0],[0]
"The general ACNN framework is shown schematically in Figure 1.
Filter generation module Instead of utilizing fixed filter weightsW for different inputs (as (1)), our model generates a set of filters conditioned on the input sentence X .",3.2 Learning context-sensitive filters,[0],[0]
"Given an input X , the filter-generation module can be implemented, in principle, as any deep (differentiable) architecture.",3.2 Learning context-sensitive filters,[0],[0]
"However, in order to handle input sentences of variable length common in natural language, we design a generic filter generation module to produce filters with a predefined size.
",3.2 Learning context-sensitive filters,[0],[0]
"First, the input X is encapsulated into a fixedlength vector (code) z with the dimension of l, via a basic CNN model, where one convolutional layer is employed along with the pooling operation (as described in Section 3.1).",3.2 Learning context-sensitive filters,[0],[0]
"On top of this hidden representation z, a deconvolutional layer, which performs transposed operations of convolutions (Radford et al., 2016), is further applied to produce a unique set of filters forX (as illustrated in Figure 1):
z = CNN(X;θe), (2)
f = DCNN(z;θd) , (3)
where θe and θd are the learned parameters in each layer of the filter-generating module, respectively.",3.2 Learning context-sensitive filters,[0],[0]
"Specifically, we convolve z with a filter of size (fs, l, kx, ky), where fs is the number of generated filters and the kernel size is (kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"The output will be a tensor of shape (fs, kx, ky).",3.2 Learning context-sensitive filters,[0],[0]
"Since the dimension of hidden representation z is independent of input-sentence length, this framework guarantees that the generated filters are of the same shape and size for every sentence.",3.2 Learning context-sensitive filters,[0],[0]
"Intuitively, the encoding part of filter generation module abstracts the information from sentenceX into z. Based on this representation, the deconvolutional up-sampling layer determines a set of fixedsize, fine-grained filters f for the specific input.
",3.2 Learning context-sensitive filters,[0],[0]
Adaptive convolution module The adaptive convolution module takes as inputs the generated filters f and an input sentence.,3.2 Learning context-sensitive filters,[0],[0]
This sentence and the input to the filter-generation module may be identical (as in Figure 1) or different (as in Figure 2).,3.2 Learning context-sensitive filters,[0],[0]
"With the sample-specific filters, the input sentence is adaptively encoded, again, via a basic CNN architecture as in Section 3.1, i.e., one convolutional and one pooling layer.",3.2 Learning context-sensitive filters,[0],[0]
"Notably, there are no additional parameters in the adaptive convolution module (no bias term is employed).
",3.2 Learning context-sensitive filters,[0],[0]
"Our ACNN framework can be seen as a generalization of the basic CNN, which can be represented as an ACNN by setting the outputs of the filter-generation module to a constant, regardless of the contextual information from input sentence(s).",3.2 Learning context-sensitive filters,[0],[0]
"Because of the learning-to-learn (Thrun and Pratt, 2012) nature of the proposed ACNN framework, it tends to have greater representational power than the basic CNN.",3.2 Learning context-sensitive filters,[0],[0]
"Considering the ability of our ACNN framework to generate context-aware filters, it can be naturally generalized to the task of text sequence matching.",3.3 Extension to text sequence matching,[0],[0]
"In this section, we will describe the proposed Adaptive Question Answering (AdaQA) model in the context of answer sentence selection task.",3.3 Extension to text sequence matching,[0],[0]
"Note that the corresponding model can be readily adapted to other sentence matching problems as well (see Section 5.2).
",3.3 Extension to text sequence matching,[0],[0]
"Given a factual question q (associated with a list of candidate answers {a1, a2, . . .",3.3 Extension to text sequence matching,[0],[0]
", am} and their corresponding labels y = {y1, y2, . . .",3.3 Extension to text sequence matching,[0],[0]
", ym}), the goal of the model is to identify the correct answers from the set of candidates.",3.3 Extension to text sequence matching,[0],[0]
"For i = 1, 2, . . .",3.3 Extension to text sequence matching,[0],[0]
",m, if ai correctly answers q, then yi = 1, and otherwise yi = 0.",3.3 Extension to text sequence matching,[0],[0]
"Therefore, the task can be cast as a classification problem where, given an unlabeled question-answer pair (qi, ai), we seek to predict the judgement yi.
",3.3 Extension to text sequence matching,[0],[0]
"Conventionally, a question q and an answer a are independently encoded by two basic CNNs to fixed-length vector representations, denoted hq and ha, respectively.",3.3 Extension to text sequence matching,[0],[0]
They are then directly employed to predict the judgement y.,3.3 Extension to text sequence matching,[0],[0]
"This strategy could be suboptimal, since no communication (information sharing) occurs between the questionanswer pair until the top prediction layer.",3.3 Extension to text sequence matching,[0],[0]
"Intuitively, while the model is inferring the representation for a question, if the meaning of the answer is
taken into account, those features that are relevant for final prediction are more likely to be extracted.",3.3 Extension to text sequence matching,[0],[0]
"So motivated, we propose an adaptive CNN-based question-answer (AdaQA) model for this problem.",3.3 Extension to text sequence matching,[0],[0]
"The AdaQA model can be divided into three modules: filter generation, adaptive convolution, and matching modules, as depicted schematically in Figure 2.",3.3 Extension to text sequence matching,[0],[0]
"Assume there is a question-answer pair to be matched, represented by word-embedding matrices, i.e. Q ∈ RTq×d and A ∈ RTa×d, where d is the embedding dimension and Tq and Ta are respective sentence lengths.",3.3 Extension to text sequence matching,[0],[0]
"First, they are passed to two filter-generation modules, to produce two sets of filters that encapsulate features of the corresponding input sentences.",3.3 Extension to text sequence matching,[0],[0]
"Similar to the setup in Section 3.2, we also employ a two-step process to produce the filters.",3.3 Extension to text sequence matching,[0],[0]
"For a question Q, the generating process is:
zq = CNN(Q;θqe), (4) f q = DCNN(zq;θ q d) (5)
where CNN and DCNN denote the basic CNN unit and deconvolution layer, respectively, as discussed in Section 2.1.",3.3 Extension to text sequence matching,[0],[0]
Parameters θqe and θ q d are to be learned.,3.3 Extension to text sequence matching,[0],[0]
"The same process can be utilized to produce encodings za and filters fa for the answer input,A, with parameters θae and θ a d, respectively.
",3.3 Extension to text sequence matching,[0],[0]
"The two sets of filter weights are then passed to adaptive convolution modules, along with Q and A, to obtain the extracted question and answer embeddings.",3.3 Extension to text sequence matching,[0],[0]
"That is, the question embedding is convolved with the filters produced by the answer and vise versa (ψq and ψa are the bias terms to be learned).",3.3 Extension to text sequence matching,[0],[0]
"The key idea is to abstract information from the answer (or question) that is pertinent to the corresponding question (or answer).
",3.3 Extension to text sequence matching,[0],[0]
"Compared to a Siamese CNN architecture (Bromley et al., 1994), our model selectively encapsulates the most important features for judgement prediction, removing less vital information.",3.3 Extension to text sequence matching,[0],[0]
We then employ the question and answer representations hq ∈,3.3 Extension to text sequence matching,[0],[0]
"Rnh , ha ∈",3.3 Extension to text sequence matching,[0],[0]
Rnh as inputs to the matching module (where nh is the dimension of question/answer embeddings).,3.3 Extension to text sequence matching,[0],[0]
"Following Mou et al. (2016), the matching function is defined as:
t =",3.3 Extension to text sequence matching,[0],[0]
"[hq;ha;hq − ha;hq ha] (6) p(y = 1|hq,ha) = MLP(t;η′) (7)
where − and denote an element-wise subtraction and element-wise product, respectively.",3.3 Extension to text sequence matching,[0],[0]
[ha;hb] indicates that ha and hb are stacked as column vectors.,3.3 Extension to text sequence matching,[0],[0]
"The resulting matching vector t ∈ R4nh is then sent through an MLP layer (with sigmoid activation function and parameters η′ to be learned) to model the desired conditional distribution p(yi = 1|hq,ha).
",3.3 Extension to text sequence matching,[0],[0]
"Notably, we share the weights of filter generating networks for both the question and answer, so that the model adaptivity for answer selection can be improved without an excessive increase in the number of parameters.",3.3 Extension to text sequence matching,[0],[0]
All three modules in AdaQA model are jointly trained end-to-end.,3.3 Extension to text sequence matching,[0],[0]
"Note that the AdaQA model proposed can be readily adapted to other sentence matching tasks, such as paraphrase identification (see Section 5.2).",3.3 Extension to text sequence matching,[0],[0]
"The adaptive context-aware filter generation mechanism proposed here bears close resemblance to attention mechanism (Yin et al., 2016; Bahdanau et al., 2015; Xiong et al., 2017) widely adopted in the NLP community, in the sense that both methods intend to incorporate rich contextual information into text representations.",3.4 Connections to attention mechanism,[0],[0]
"However, attention is typically operated on top of the hidden units preprocessed by CNN or LSTM layers, and assigns different weights to each unit according to a context vector.",3.4 Connections to attention mechanism,[0],[0]
"By contrast, in our context-aware filter generation mechanism, the contextual information is inherently encoded into the convolutional filters, which directly interact with the input sentence during the convolution encoding operation.",3.4 Connections to attention mechanism,[0],[0]
"Notably, according to our experiments, the proposed filter generation module can be readily combined with (standard) attention mechanisms to further enhance the modeling expressiveness of CNN encoder.",3.4 Connections to attention mechanism,[0],[0]
Datasets We investigate the effectiveness of the proposed ACNN framework on both document classification and text sequence matching tasks.,4 Experimental Setup,[0],[0]
"Specifically, we consider two large-scale document classification datasets: Yelp Reviews Polarity, and DBPedia ontology datasets (Zhang et al., 2015).",4 Experimental Setup,[0],[0]
"For Yelp reviews, we seek to predict a binary label (positive or negative) regarding one review about a restaurant.",4 Experimental Setup,[0],[0]
"DBpedia is extracted from Wikipedia by crowd-sourcing and is categorized into 14 non-overlapping ontology classes, including Company, Athlete, Natural Place, etc.",4 Experimental Setup,[0],[0]
"We sample 15% of the training data as the validation set, to select hyperparameters for our models and perform early stopping.",4 Experimental Setup,[0],[0]
"For sentence matching, we evaluate the AdaQA model on two datasets for open-domain question answering: WikiQA (Yang et al., 2015) and SelQA (Jurczyk et al., 2016).",4 Experimental Setup,[0],[0]
"Given a question, the task is to rank the corresponding candidate answers, which, in the case of WikiQA, are sentences extracted from the summary section of a related Wikipedia article.",4 Experimental Setup,[0],[0]
"To facilitate comparison with existing results (Yin et al., 2016; Yang et al., 2015; Shen et al., 2018b), we truncate the candidate answers to a maximum length of 40 tokens for all experiments on the WikiQA dataset.",4 Experimental Setup,[0],[0]
"We also consider the task of paraphrase identification with the Quora Question Pairs dataset, with the same data splits as in (Wang et al., 2017b).",4 Experimental Setup,[0],[0]
"A summary of all datasets is presented in Table 1.
",4 Experimental Setup,[0],[0]
"Training Details For the document classification experiments, we randomly initialize the word embeddings uniformly within [−0.001, 0.001] and update them during training.",4 Experimental Setup,[0],[0]
"For the generated filters, we set the window size as h = 5, with K = 100 feature maps (the dimension of z is set as 100).",4 Experimental Setup,[0],[0]
"For direct comparison, we employ the same filter shape/size settings as in our basic CNN implementation.",4 Experimental Setup,[0],[0]
"A one-layer architecture is utilized for both the CNN baseline and the ACNN model, since we did not observe significant
performance gains with a multilayer architecture.",4 Experimental Setup,[0],[0]
"The minibatch size is set as 128, and a dropout rate of 0.2 is utilized on the embedding layer.",4 Experimental Setup,[0],[0]
"We observed that a larger dropout rate (e.g., 0.5) will hurt performance on document classifications and make training significantly slower.
",4 Experimental Setup,[0],[0]
"For the sentence matching tasks, we initialized the word embeddings with 50-dimensional Glove (Pennington et al., 2014) word vectors pretrained from Wikipedia 2014 and Gigaword 5 (Pennington et al., 2014) for all model variants.",4 Experimental Setup,[0],[0]
"As for the filters, we set the window size as h = 5, with K = 300 feature maps.",4 Experimental Setup,[0],[0]
"As described in Section 3.3, the vector t, output from the matching module, is fed to the prediction layer, implemented as a one-layer MLP followed by the sigmoid function.",4 Experimental Setup,[0],[0]
"We use Adam (Kingma and Ba, 2014) to train the models, with a learning rate of 3 × 10−4.",4 Experimental Setup,[0],[0]
"Dropout (Srivastava et al., 2014), with a rate of 0.5, is employed on the word embedding layer.",4 Experimental Setup,[0],[0]
The hyperparameters are selected by choosing the best model on the validation set.,4 Experimental Setup,[0],[0]
"All models are implemented with TensorFlow (Abadi et al., 2016) and are trained using one NVIDIA GeForce GTX TITAN X GPU with 12GB memory.
",4 Experimental Setup,[0],[0]
"Baselines For document classification, we consider several baseline models: (i) ngrams (Zhang et al., 2015), a bag-of-means method based on TFIDF representations built by choosing the 500,000 most frequent n-grams (up to 5-grams) from the training set and use their corresponding counts as features; (ii) small/large word CNN (Zhang et al., 2015): 6 layer word-based convolutional networks, with 256/1024 features at each layer, denoted as small/large, respectively; (iii) deep CNN (Conneau et al., 2016): deep convolutional neural networks with 9/17/29 layers.",4 Experimental Setup,[0],[0]
"To evaluate the effectiveness of proposed AdaQA model, we compare it with several CNN-based sequence matching baselines, including Vanilla CNN (Jurczyk et al., 2016; Santos et al., 2017), attentive pooling networks (dos Santos et al., 2016), and ABCNN (Yin et al., 2016) (where an attention mechanism is employed over the two sentence representations).
",4 Experimental Setup,[0],[0]
"Evaluation Metrics For document categorization and paraphrase identification tasks, we employ the percentage of correct predictions on the test set to evaluate and compare different models.
",4 Experimental Setup,[0],[0]
"For the answer sentence selection task, mean average precision (MAP) and mean reciprocal rank (MRR) are utilized as the corresponding evaluation metrics.",4 Experimental Setup,[0],[0]
"To explicitly explore whether our ACNN model can leverage the input-aware filter weights for better sentence representation, we perform a comparison between the basic CNN and ACNN models with only a single filter, which are denoted as SCNN, S-ACNN, respectively (this setting may not yield best overall performance, since only a single filter is used, but it allows us to isolate the impact of adaptivity).",5.1 Document Classification,[0],[0]
"As illustrated in Table 2, SACNN significantly outperforms S-CNN on both datasets, demonstrating the advantage of the filtergeneration module in our ACNN framework.",5.1 Document Classification,[0],[0]
"As a result, with only one convolutional filter and thus very limited modeling capacity, our S-ACNN model tends to be much more expressive than the basic CNN model, due to the flexibility of applying different filters to different sentences.
",5.1 Document Classification,[0],[0]
We further experiment on both ACNN and CNN models with multiple filters.,5.1 Document Classification,[0],[0]
The corresponding document categorization accuracies are presented in Table 2.,5.1 Document Classification,[0],[0]
"Although we only use one convolution layer for our ACNN model, it already outperforms other CNN baseline methods with much deeper architectures.",5.1 Document Classification,[0],[0]
"Moreover, our method ex-
hibits higher accuracy than n-grams, which is a very strong baseline as shown in (Zhang et al., 2015).",5.1 Document Classification,[0],[0]
We attribute the superior performance of the ACNN framework to its stronger (adaptive) feature-extraction ability.,5.1 Document Classification,[0],[0]
"Moreover, our MACNN also achieves slightly better performance than self-attentive sentence embeddings proposed in Lin et al. (2017), which requires significant more parameters than our method.
",5.1 Document Classification,[0],[0]
"Effect of number of filters To further demonstrate that the performance gains in document categorization experiments originates from the improved adaptivity of our ACNN framework, we implement the basic CNN model with different numbers of filter sizes, ranging from 1 to 1000.",5.1 Document Classification,[0],[0]
"As illustrated in Figure 3(a), when the filter size is larger than 100, the test accuracy of the standard CNN model does not show any noticeable improvement with more filters.",5.1 Document Classification,[0],[0]
"More importantly, even with a filter size of 1000, the classification accuracy of the CNN is worse than that of the ACNN model with the filter number restricted to 100.",5.1 Document Classification,[0],[0]
"Given these observations, we believe that the boosted categorization accuracy does come from the improved flexibility and thus better feature extraction of our ACNN framework.",5.1 Document Classification,[0],[0]
"To elucidate the role of different parts (modules) in our AdaQA model, we implement several model variants for comparison: (i) a “vanilla” CNN model that independently encodes two sentence representations for matching; (ii) a self-adaptive ACNN-based model where the question/answer sentence generates adaptive filters only to convolve with the input itself; (iii) a one-way ACNN model where only the answer sentence representation is extracted with adaptive filters, which
are generated conditioned on the question; (iv) a two-way AdaQA model as described in Section 2.4, where both sentences are adaptively encoded, with filters generated conditioned on the other sequence; (v) considering that the proposed filter generation mechanism is complementary to the attention layer typically employed in sequence matching tasks (see Section 3.4), we experiment with another model variant that combines the proposed context-aware filter generation mechanism with the multi-perspective attention layer introduced in (Wang et al., 2017b).
",5.2 Answer Sentence Selection,[0],[0]
"Tables 3 and 4 show experimental results of our models on WikiQA and SelQA datasets, along with other state-of-the-art methods.",5.2 Answer Sentence Selection,[0],[0]
"Note that the self-adaptive ACNN model variant, which generates filters only for the input itself (without any interactions before the top matching module), slightly outperforms the vanilla CNN Siamese model.",5.2 Answer Sentence Selection,[0],[0]
"Combined with the results in document categorization experiments, we believe that our ACNN framework, in its simplest form, can be utilized as a powerful feature extractor for transforming natural language sentences into fixed-length vectors.",5.2 Answer Sentence Selection,[0],[0]
"More importantly, our two-way AdaQA model exhibits superior results compared with the one-way variant as well as other CNN-based baseline models on the WikiQA dataset.",5.2 Answer Sentence Selection,[0],[0]
This observation indicates that the bidirectional filter generation mechanism is strongly associated with the performance gains.,5.2 Answer Sentence Selection,[0],[0]
"While combined with the multi-perspective attention layers, adopted after the ACNN encoding layer, our two-way AdaQA model achieves even better performance.",5.2 Answer Sentence Selection,[0],[0]
"This suggests that the proposed strategy is complemen-
tary, in terms of the incorporation of rich contextual information, to the standard attention mechanism.",5.2 Answer Sentence Selection,[0],[0]
"The same trend is also observed on the SelQA dataset (as shown in Table 4), which is a much larger dataset than WikiQA.
",5.2 Answer Sentence Selection,[0],[0]
"Notably, our model yields significantly better results than an attentive pooling network and ABCNN (attention-based CNN) baselines.",5.2 Answer Sentence Selection,[0],[0]
"We attribute the improvement to two potential advantages of our AdaQA model: (i) for the two previous baseline methods, the interaction between question and answer takes place either before or after convolution.",5.2 Answer Sentence Selection,[0],[0]
"However, in our AdaQA model, the communication between two sentences is inherent in the convolution operation, and thus can provide the abstracted features with more flexibility; (ii) the bidirectional filter generation mechanism in our AdaQA model generates co-dependent representations for the question and candidate answer, which could enable the model to recover from initial local maxima corresponding to incorrect predictions (Xiong et al., 2017).
",5.2 Answer Sentence Selection,[0],[0]
"Paragraph Identification Considering that the proposed AdaQA model can be readily generalized to other text sequence matching problems, we further evaluate the proposed framework on the paraphrase identification task with the Quora question pairs dataset.",5.2 Answer Sentence Selection,[0],[0]
"To ensure a fair comparison, we employ the same data splits as in (Wang et al., 2017b).",5.2 Answer Sentence Selection,[0],[0]
"As illustrated in Table 5, our twoway AdaQA model again exhibits superior performances compared with basic CNN models (as reported in (Wang et al., 2017b)).",5.2 Answer Sentence Selection,[0],[0]
"Reasoning ability To associate the improved answer sentence selection results with the reasoning capabilities of our AdaQA model, we further categorize the questions in the WikiQA test set into 5 types containing: ‘What’, ‘Where’, ‘How’, ‘When’ or ‘Who’.",5.3 Discussion,[0],[0]
We then calculate the MAP scores of the basic CNN and our AdaQA model on different question types.,5.3 Discussion,[0],[0]
"Similar to the findings in (Miao et al., 2016), we observe that the ‘How’ question is the hardest to answer, with the lowest MAP scores.",5.3 Discussion,[0],[0]
"However, our AdaQA model improves most over the basic CNN on the ‘How’ type question, see Figure 3(b).",5.3 Discussion,[0],[0]
"Further comparing our results with NASM in (Miao et al., 2016), our AdaQA model (with a MAP score of 0.579) outperforms their reported ‘How’ question MAP scores (0.524) by a large margin, indicating that the adaptive convolutional filter-generation mechanism improves the model’s ability to read and reason over natural language sentences.
",5.3 Discussion,[0],[0]
"Filter visualization To better understand what information has been encoded into our contextaware filters, we visualize one of the filters for sentences within the test set (on the DBpedia dataset) with t-SNE.",5.3 Discussion,[0],[0]
The corresponding results are shown in Figure 3(c).,5.3 Discussion,[0],[0]
"It can be observed that the filters for documents with the same label (ontology) are grouped into clusters, indicating that for different types of document, ACNN has leveraged distinct convolutional filters for better feature extraction.",5.3 Discussion,[0],[0]
"We presented a context-aware convolutional filtergeneration mechanism, introducing a meta network to adaptively produce a set of input-aware filters.",6 Conclusions,[0],[0]
"In this manner, the filter weights vary from sample to sample, providing the CNN encoder network with more modeling flexibility and capacity.
",6 Conclusions,[0],[0]
"This framework is further generalized to model question-answer sentence pairs, leveraging a twoway feature abstraction process.",6 Conclusions,[0],[0]
"We evaluate our models on several document-categorization and sentence matching benchmarks, and they consistently outperform the standard CNN and attentionbased CNN baselines, demonstrating the effectiveness of our framework.
",6 Conclusions,[0],[0]
"Acknowledgments This research was supported in part by DARPA, DOE, NIH, ONR and NSF.",6 Conclusions,[0],[0]
Convolutional neural networks (CNNs) have recently emerged as a popular building block for natural language processing (NLP).,abstractText,[0],[0]
"Despite their success, most existing CNN models employed in NLP share the same learned (and static) set of filters for all input sentences.",abstractText,[0],[0]
"In this paper, we consider an approach of using a small meta network to learn contextaware convolutional filters for text processing.",abstractText,[0],[0]
The role of meta network is to abstract the contextual information of a sentence or document into a set of input-aware filters.,abstractText,[0],[0]
"We further generalize this framework to model sentence pairs, where a bidirectional filter generation mechanism is introduced to encapsulate co-dependent sentence representations.",abstractText,[0],[0]
"In our benchmarks on four different tasks, including ontology classification, sentiment analysis, answer sentence selection, and paraphrase identification, our proposed model, a modified CNN with context-aware filters, consistently outperforms the standard CNN and attentionbased CNN baselines.",abstractText,[0],[0]
"By visualizing the learned context-aware filters, we further validate and rationalize the effectiveness of proposed framework.",abstractText,[0],[0]
Learning Context-Aware Convolutional Filters for Text Processing,title,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",1. Introduction,[0],[0]
This is particularly important while dealing with exponentially large domains such as source code and logical expressions.,1. Introduction,[0],[0]
Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.,1. Introduction,[0],[0]
"Although symbolic reasoning is very powerful, it also tends to be hard.",1. Introduction,[0],[0]
"For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse.",1. Introduction,[0],[0]
"This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se-
Work started when M. Allamanis was at Edinburgh.",1. Introduction,[0],[0]
This work was done while P. Kohli was at Microsoft.,1. Introduction,[0],[0]
"1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
mantic relationships.",1. Introduction,[0],[0]
"However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.",1. Introduction,[0],[0]
"In this work, we explore the direction of learning continuous semantic representations of symbolic expressions.",1. Introduction,[0],[0]
"The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.",1. Introduction,[0],[0]
"Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space.
",1. Introduction,[0],[0]
"In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions.",1. Introduction,[0],[0]
"Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.",1. Introduction,[0],[0]
"This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature.",1. Introduction,[0],[0]
"As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.
",1. Introduction,[0],[0]
"Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.",1. Introduction,[0],[0]
"They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",1. Introduction,[0],[0]
"While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",1. Introduction,[0],[0]
"In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different.",1. Introduction,[0],[0]
"The underlying conceptual problem is how to develop a con-
1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.
",1. Introduction,[0],[0]
"tinuous representation that follows syntax but not too much, that respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.
",1. Introduction,[0],[0]
"To tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET).",1. Introduction,[0],[0]
"EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.",1. Introduction,[0],[0]
"As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions.",1. Introduction,[0],[0]
"The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree.",1. Introduction,[0],[0]
"To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors.",1. Introduction,[0],[0]
"Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.
",1. Introduction,[0],[0]
"To summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task.",1. Introduction,[0],[0]
"(b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space.",1. Introduction,[0],[0]
"(c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives.",1. Introduction,[0],[0]
Code and data are available at groups.inf.ed.ac.uk/cup/semvec.,1. Introduction,[0],[0]
"In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class.",2. Model,[0],[0]
"Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.
",2. Model,[0],[0]
Problem Hardness.,2. Model,[0],[0]
Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse.,2. Model,[0],[0]
"For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability — an NPcomplete problem.",2. Model,[0],[0]
"Of course, we do not expect to circum-
vent an NP-complete problem with neural networks.",2. Model,[0],[0]
A network for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP .,2. Model,[0],[0]
"Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set.",2. Model,[0],[0]
"The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.
",2. Model,[0],[0]
Notation and Framework.,2. Model,[0],[0]
"To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN)",2. Model,[0],[0]
"(Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.",2. Model,[0],[0]
"Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network.",2. Model,[0],[0]
We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes.,2. Model,[0],[0]
"We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type “a” referring to the variable a or of type “and” referring to a node of the logical AND (∧) operation.",2. Model,[0],[0]
We refer to the type of a node n as τn.,2. Model,[0],[0]
"In pseudocode, TREENNs retrieve the representation of a tree T rooted at node ρ, by invoking the function TREENN(ρ) that returns a vector representation rρ ∈ RD, i.e., a SEMVEC.",2. Model,[0],[0]
"The function is defined as TREENN (current node n)
if n is not a leaf then rn ← COMBINE(TREENN(c0), . . .",2. Model,[0],[0]
", TREENN(ck), τn), where (c0, . . .",2. Model,[0],[0]
", ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn)
return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.",2. Model,[0],[0]
"Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.",2. Model,[0],[0]
"As discussed next, these will both prove to be serious limitations in our setting.",2. Model,[0],[0]
"To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).",2. Model,[0],[0]
"Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.",2.1. Neural Equivalence Networks,[0],[0]
"In this work, we find that standard neural architectures do not handle well this challenge.",2.1. Neural Equivalence Networks,[0],[0]
"To represent semantics from syntax, we need to learn to recursively
compose and decompose semantic representations and remove syntactic “noise”.",2.1. Neural Equivalence Networks,[0],[0]
"Any syntactic operation may significantly change semantics (e.g. negation, or appending ∧FALSE) while we may reach the same semantic state through many possible operations.",2.1. Neural Equivalence Networks,[0],[0]
This necessitates using high-curvature operations over the semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. ¬¬A and A should have an identical SEMVECs).",2.1. Neural Equivalence Networks,[0],[0]
"Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).",2.1. Neural Equivalence Networks,[0],[0]
"Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements.",2.1. Neural Equivalence Networks,[0],[0]
"The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.
",2.1. Neural Equivalence Networks,[0],[0]
The first extension that we introduce is to the network structure at each layer in the tree.,2.1. Neural Equivalence Networks,[0],[0]
"Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.",2.1. Neural Equivalence Networks,[0],[0]
"During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations.",2.1. Neural Equivalence Networks,[0],[0]
Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear.,2.1. Neural Equivalence Networks,[0],[0]
For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Instead, we turn to multi-layer neural
networks.",2.1. Neural Equivalence Networks,[0],[0]
"In particular, we define the network as shown in the function COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children.,2.1. Neural Equivalence Networks,[0],[0]
"Each node type τn, e.g., each logical operator, has a different set of weights.",2.1. Neural Equivalence Networks,[0],[0]
"We experimented with deeper networks but this did not yield any improvements.
",2.1. Neural Equivalence Networks,[0],[0]
"However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.",2.1. Neural Equivalence Networks,[0],[0]
"This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to “echo” its own errors and producing unstable feedback loops.",2.1. Neural Equivalence Networks,[0],[0]
"We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.",2.1. Neural Equivalence Networks,[0],[0]
We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.,2.1. Neural Equivalence Networks,[0],[0]
"That is, we set LOOKUPLEAFEMBEDDING(τn) = Cτn/ ‖Cτn‖2 , and we normalize the output of the final layer of COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
"The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).",2.1. Neural Equivalence Networks,[0],[0]
"Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.",2.1. Neural Equivalence Networks,[0],[0]
"As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.
",2.1. Neural Equivalence Networks,[0],[0]
"Although these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good
performance.",2.1. Neural Equivalence Networks,[0],[0]
"In our early experiments, we noticed that the networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar.",2.1. Neural Equivalence Networks,[0],[0]
"At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.
",2.1. Neural Equivalence Networks,[0],[0]
"Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.",2.1. Neural Equivalence Networks,[0],[0]
We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE).,2.1. Neural Equivalence Networks,[0],[0]
We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility.,2.1. Neural Equivalence Networks,[0],[0]
"Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information.",2.1. Neural Equivalence Networks,[0],[0]
"To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example.",2.1. Neural Equivalence Networks,[0],[0]
"Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree)",2.1. Neural Equivalence Networks,[0],[0]
it is often possible to completely determine or at least place strong constraints on the semantics of the third.,2.1. Neural Equivalence Networks,[0],[0]
"For example, consider a boolean formula F (a, b) = F1(a, b) ∨ F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b.",2.1. Neural Equivalence Networks,[0],[0]
"Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true.",2.1. Neural Equivalence Networks,[0],[0]
"More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.
",2.1. Neural Equivalence Networks,[0],[0]
"Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children.",2.1. Neural Equivalence Networks,[0],[0]
"More specifically, given any node p in the tree with children c0 . . .",2.1. Neural Equivalence Networks,[0],[0]
"ck, we can define a parent-children tuple",2.1. Neural Equivalence Networks,[0],[0]
"[rc0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", rck , rp] containing the (computed) SEMVECs of the children and parent nodes.",2.1. Neural Equivalence Networks,[0],[0]
What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder.,2.1. Neural Equivalence Networks,[0],[0]
"We then seek to minimize the reconstruction error of the child representations (r̃c0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from the reconstructed children.",2.1. Neural Equivalence Networks,[0],[0]
"More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with κ percent of its elements set to zero.",2.1. Neural Equivalence Networks,[0],[0]
Note that the encoder is specific to the parent node type τp.,2.1. Neural Equivalence Networks,[0],[0]
"Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs
in two major ways.",2.1. Neural Equivalence Networks,[0],[0]
"First, SUBEXPAE autoencodes on the entire parent-children representation tuple, rather than the child representations alone.",2.1. Neural Equivalence Networks,[0],[0]
"Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.
",2.1. Neural Equivalence Networks,[0],[0]
Subexpression autoencoding has several desirable effects.,2.1. Neural Equivalence Networks,[0],[0]
"First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions.",2.1. Neural Equivalence Networks,[0],[0]
"Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.",2.1. Neural Equivalence Networks,[0],[0]
"Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.",2.1. Neural Equivalence Networks,[0],[0]
"The goal is to nudge the network to learn consistent, reversible semantics.",2.1. Neural Equivalence Networks,[0],[0]
"Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class.",2.1. Neural Equivalence Networks,[0],[0]
"To illustrate this point, imagine two semantically equivalent c′0 and c ′′ 0 child nodes of different expressions that
have distant SEMVECs, i.e. ∥∥rc′0 − rc′′0 ∥∥2 although COMBINE(rc′0 , . . . )",2.1. Neural Equivalence Networks,[0],[0]
"≈ COMBINE(rc′′0 , . . . ).",2.1. Neural Equivalence Networks,[0],[0]
"In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).",2.1. Neural Equivalence Networks,[0],[0]
"Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.",2.1. Neural Equivalence Networks,[0],[0]
We train EQNETs from a dataset of expressions whose semantic equivalence is known.,2.2. Training,[0],[0]
Given a training set T = {T1 . . .,2.2. Training,[0],[0]
"TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . .",2.2. Training,[0],[0]
eJ}.,2.2. Training,[0],[0]
"We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N .",2.2. Training,[0],[0]
"Given an expression tree T that belongs to the equivalence class ei ∈ E , we compute the probability
P (ei|T ) =",2.2. Training,[0],[0]
"exp
( TREENN(T )>qei + bi )∑ j exp ( TREENN(T )>",2.2. Training,[0],[0]
qej,2.2. Training,[0],[0]
"+ bj
) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms.",2.2. Training,[0],[0]
"Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions.",2.2. Training,[0],[0]
"This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to
the training set.",2.2. Training,[0],[0]
"To train the model, we use a max-margin objective that maximizes classification accuracy, i.e.
LACC(T, ei) = max (
0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2)
where m > 0 is a scalar margin.",2.2. Training,[0],[0]
"And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei ∈ E is
L(T, ei) = LACC(T, ei) + µ |Q| ∑ n∈Q SUBEXPAE(ch(n), n)
(3)
",2.2. Training,[0],[0]
"where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight.",2.2. Training,[0],[0]
"We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics.",2.2. Training,[0],[0]
"So, for each epoch t, we set µ = 1− 10−νt with ν ≥ 0.",2.2. Training,[0],[0]
"Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.",2.2. Training,[0],[0]
"In practice, we found the optimization to be very unstable, yielding suboptimal performance.",2.2. Training,[0],[0]
We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity.,2.2. Training,[0],[0]
Datasets.,3. Evaluation,[0],[0]
We generate datasets of expressions grouped into equivalence classes from two domains.,3. Evaluation,[0],[0]
The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions.,3. Evaluation,[0],[0]
"In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression.",3. Evaluation,[0],[0]
"When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c · (a · a+ b) and f ·(d·d+e) are not equivalent.",3. Evaluation,[0],[0]
"For each domain, we generate “simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more complex operators.",3. Evaluation,[0],[0]
We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.,3. Evaluation,[0],[0]
All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly.,3. Evaluation,[0],[0]
Table 1 shows the datasets we generated.,3. Evaluation,[0],[0]
In the supplementary material we present some sample expressions.,3. Evaluation,[0],[0]
"For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) — although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.",3. Evaluation,[0],[0]
"Learning SEMVECs for boolean expressions
is already a hard problem; with n boolean variables, there are 22 n
equivalence classes (i.e. one for each possible truth table).",3. Evaluation,[0],[0]
"We split the datasets into training, validation and test sets.",3. Evaluation,[0],[0]
"We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS).",3. Evaluation,[0],[0]
It is easiest to describe UNSEENEQCLASS first.,3. Evaluation,[0],[0]
"To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set.",3. Evaluation,[0],[0]
We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class.,3. Evaluation,[0],[0]
We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset.,3. Evaluation,[0],[0]
"Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%–15%–25%.",3. Evaluation,[0],[0]
"We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.
Baselines.",3. Evaluation,[0],[0]
"To compare the performance of our model, we train the following baselines.",3. Evaluation,[0],[0]
"TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses).",3. Evaluation,[0],[0]
This captures topical/declarative knowledge but is unable to capture procedural knowledge.,3. Evaluation,[0],[0]
GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.,3. Evaluation,[0],[0]
Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.,3. Evaluation,[0],[0]
We also include two recursive neural networks (TREENN).,3. Evaluation,[0],[0]
The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014).,3. Evaluation,[0],[0]
"We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.",3. Evaluation,[0],[0]
"This shows the effect of SEMVEC normalization and subexpression autoencoder.
Hyperparameters.",3. Evaluation,[0],[0]
"We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . .",3. Evaluation,[0],[0]
", 15) statistics (described next).",3. Evaluation,[0],[0]
The selected hyperparameters are detailed in the supplementary material.,3. Evaluation,[0],[0]
Metrics.,3.1. Quantitative Evaluation,[0],[0]
To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class.,3.1. Quantitative Evaluation,[0],[0]
"More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the
score as
scorek(q) = |Nk(q) ∩ c| min(k, |c|) .",3.1. Quantitative Evaluation,[0],[0]
"(4)
To report results for a given testset, we simply average scorek(q) for all expressions q in the testset.",3.1. Quantitative Evaluation,[0],[0]
"We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation.,3.1. Quantitative Evaluation,[0],[0]
Figure 2 presents the average per-model precision-recall curves across the datasets.,3.1. Quantitative Evaluation,[0],[0]
Table 1 shows score5 of UNSEENEQCLASS.,3.1. Quantitative Evaluation,[0],[0]
Detailed plots are found in the supplementary material.,3.1. Quantitative Evaluation,[0],[0]
"EQNET performs better for all datasets, by a large margin.",3.1. Quantitative Evaluation,[0],[0]
"The only exception is POLY5, where the 2-L TREENN performs better.",3.1. Quantitative Evaluation,[0],[0]
"However, this may have to do with the small size of the dataset.",3.1. Quantitative Evaluation,[0],[0]
The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn.,3.1. Quantitative Evaluation,[0],[0]
"Understandably, introducing more variables increases the size of the represented space reducing performance.",3.1. Quantitative Evaluation,[0],[0]
"The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used.",3.1. Quantitative Evaluation,[0],[0]
Similar observations can be made for sequence models.,3.1. Quantitative Evaluation,[0],[0]
The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs.,3.1. Quantitative Evaluation,[0],[0]
"Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.",3.1. Quantitative Evaluation,[0],[0]
"Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation of Compositionality.,3.1. Quantitative Evaluation,[0],[0]
"We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of
a small size.",3.1. Quantitative Evaluation,[0],[0]
"To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8.",3.1. Quantitative Evaluation,[0],[0]
We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b.,3.1. Quantitative Evaluation,[0],[0]
"These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.
",3.1. Quantitative Evaluation,[0],[0]
"Impact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here.",3.1. Quantitative Evaluation,[0],[0]
"First, SUBEXPAE improves performance.",3.1. Quantitative Evaluation,[0],[0]
"When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS.",3.1. Quantitative Evaluation,[0],[0]
"This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average.",3.1. Quantitative Evaluation,[0],[0]
"However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets.",3.1. Quantitative Evaluation,[0],[0]
The second key difference to traditional TREENNs is the output normalization and the residual connections.,3.1. Quantitative Evaluation,[0],[0]
"Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC).",3.1. Quantitative Evaluation,[0],[0]
"We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs.",3.1. Quantitative Evaluation,[0],[0]
Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class.,3.2. Qualitative Evaluation,[0],[0]
"Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is
Table 2.",3.2. Qualitative Evaluation,[0],[0]
Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8.,3.2. Qualitative Evaluation,[0],[0]
"A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.
",3.2. Qualitative Evaluation,[0],[0]
Expr a ∧ (a ∧ (a ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (a+ c)) ((a+ c) ·,3.2. Qualitative Evaluation,[0],[0]
"c) + a (b · b)− b
tfidf c",3.2. Qualitative Evaluation,[0],[0]
∧,3.2. Qualitative Evaluation,[0],[0]
((a ∧ a) ∧ (¬a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
a+ (c+ a) ·,3.2. Qualitative Evaluation,[0],[0]
c (c · a) + (a+ c) b · (b− b) GRU X a ∧ (a ∧ (c ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
b+ (c · (a+ c)) ((b+ c) · c) + a (b+ b) · b− b 1L-TREENN a ∧ (a ∧ (a ∧ (¬b))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬b))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬b)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (b+ c)) ((b+ c) ·,3.2. Qualitative Evaluation,[0],[0]
c) + a (a− c) · b− b EQNET X X (¬(b⇒ (b ∨ c))),3.2. Qualitative Evaluation,[0],[0]
"∧ a X X (b · b) · b− b
involved.",3.2. Qualitative Evaluation,[0],[0]
"In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10.",3.2. Qualitative Evaluation,[0],[0]
"As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.
",3.2. Qualitative Evaluation,[0],[0]
Figure 4 shows a visualization of score5 for each node in the expression tree.,3.2. Qualitative Evaluation,[0],[0]
"One may see that as EQNET knows how
¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b)))",3.2. Qualitative Evaluation,[0],[0]
"((c ∨ (¬b))⇒ a) ∧ (a ⇒ a)
((b ⊕ (¬c))",3.2. Qualitative Evaluation,[0],[0]
"∧ b)⊕ (a ∨ b) ((b · a)− a) · b
a − ((a + b) · a) ((c · b) · c) · a b + ((b · b) · b)
Figure 4.",3.2. Qualitative Evaluation,[0],[0]
Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET.,3.2. Qualitative Evaluation,[0],[0]
"The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.
to compose expressions that achieve good score, even if the subexpressions achieve a worse score.",3.2. Qualitative Evaluation,[0],[0]
"This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network.",3.2. Qualitative Evaluation,[0],[0]
"Larger scale interactive t-SNE visualizations can be found online.
",3.2. Qualitative Evaluation,[0],[0]
Figure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives.,3.2. Qualitative Evaluation,[0],[0]
It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space.,3.2. Qualitative Evaluation,[0],[0]
"Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and −a, topbottom between polynomials with−b and b and the diagonal parellelt to y = −x between c and−c.",3.2. Qualitative Evaluation,[0],[0]
We observe a similar behavior in Figure 5a for boolean expressions.,3.2. Qualitative Evaluation,[0],[0]
"Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-
mann, 1994).",4. Related Work,[0],[0]
One can consider a serialized version of the resulting neural network as a representation of the expression.,4. Related Work,[0],[0]
"However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.
",4. Related Work,[0],[0]
"Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.",4. Related Work,[0],[0]
Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.,4. Related Work,[0],[0]
"EQNET’s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.",4. Related Work,[0],[0]
"In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.
",4. Related Work,[0],[0]
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.,4. Related Work,[0],[0]
"Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics.",4. Related Work,[0],[0]
Piech et al. (2015) also learn distributed matrix representations of student code submissions.,4. Related Work,[0],[0]
"However, to learn the representations, they use input and output program states and do not test for program equivalence.",4. Related Work,[0],[0]
"Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs.",4. Related Work,[0],[0]
"Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.
",4. Related Work,[0],[0]
"More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.",4. Related Work,[0],[0]
"In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better
represent equivalence, and do not use search when looking for equivalent expressions.",4. Related Work,[0],[0]
Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.,4. Related Work,[0],[0]
"In the future, SEMVECs may be useful within this area.
",4. Related Work,[0],[0]
"Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).",4. Related Work,[0],[0]
"In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated.",4. Related Work,[0],[0]
"In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.
",5. Discussion & Conclusions,[0],[0]
We show that EQNETs perform significantly better than state-of-the-art alternatives.,5. Discussion & Conclusions,[0],[0]
"But further improvements are needed, especially for more robust training of compositional models.",5. Discussion & Conclusions,[0],[0]
"In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented.",5. Discussion & Conclusions,[0],[0]
"Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"In the future, to represent more complex procedures, variable-sized representations would seem to be required.",5. Discussion & Conclusions,[0],[0]
This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].,Acknowledgments,[0],[0]
We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources.,Acknowledgments,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",abstractText,[0],[0]
"As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions.",abstractText,[0],[0]
"These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different.",abstractText,[0],[0]
"The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures.",abstractText,[0],[0]
"We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",abstractText,[0],[0]
Learning Continuous Semantic Representations of Symbolic Expressions,title,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",1. Introduction,[0],[0]
This is particularly important while dealing with exponentially large domains such as source code and logical expressions.,1. Introduction,[0],[0]
Symbolic notation allows us to abstractly represent a large set of states that may be perceptually very different.,1. Introduction,[0],[0]
"Although symbolic reasoning is very powerful, it also tends to be hard.",1. Introduction,[0],[0]
"For example, problems such as the satisfiablity of boolean expressions and automated formal proofs tend to be NP-hard or worse.",1. Introduction,[0],[0]
"This raises the exciting opportunity of using pattern recognition within symbolic reasoning, that is, to learn patterns from datasets of symbolic expressions that approximately represent se-
Work started when M. Allamanis was at Edinburgh.",1. Introduction,[0],[0]
This work was done while P. Kohli was at Microsoft.,1. Introduction,[0],[0]
"1Microsoft Research, Cambridge, UK 2University of Edinburgh, UK 3DeepMind, London, UK 4The Alan Turing Institute, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Miltiadis Allamanis <t-mialla@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
mantic relationships.",1. Introduction,[0],[0]
"However, apart from some notable exceptions (Alemi et al., 2016; Loos et al., 2017; Zaremba et al., 2014), this area has received relatively little attention in machine learning.",1. Introduction,[0],[0]
"In this work, we explore the direction of learning continuous semantic representations of symbolic expressions.",1. Introduction,[0],[0]
"The goal is for expressions with similar semantics to have similar continuous representations, even if their syntactic representation is very different.",1. Introduction,[0],[0]
"Such representations have the potential to allow a new class of symbolic reasoning methods based on heuristics that depend on the continuous representations, for example, by guiding a search procedure in a symbolic solver based on a distance metric in the continuous space.
",1. Introduction,[0],[0]
"In this paper, we make a first essential step of addressing the problem of learning continuous semantic representations (SEMVECs) for symbolic expressions.",1. Introduction,[0],[0]
"Our aim is, given access to a training set of pairs of expressions for which semantic equivalence is known, to assign continuous vectors to symbolic expressions in such a way that semantically equivalent, but syntactically diverse expressions are assigned to identical (or highly similar) continuous vectors.",1. Introduction,[0],[0]
"This is an important but hard problem; learning composable SEMVECs of symbolic expressions requires that we learn about the semantics of symbolic elements and operators and how they map to the continuous representation space, thus encapsulating implicit knowledge about symbolic semantics and its recursive abstractive nature.",1. Introduction,[0],[0]
"As we show in our evaluation, relatively simple logical and polynomial expressions present significant challenges and their semantics cannot be sufficiently represented by existing neural network architectures.
",1. Introduction,[0],[0]
"Our work in similar in spirit to the work of Zaremba et al. (2014), who focus on learning expression representations to aid the search for computationally efficient identities.",1. Introduction,[0],[0]
"They use recursive neural networks (TREENN)1 (Socher et al., 2012) for modeling homogenous, single-variable polynomial expressions.",1. Introduction,[0],[0]
"While they present impressive results, we find that the TREENN model fails when applied to more complex symbolic polynomial and boolean expressions.",1. Introduction,[0],[0]
"In particular, in our experiments we find that TREENNs tend to assign similar representations to syntactically similar expressions, even when they are semantically very different.",1. Introduction,[0],[0]
"The underlying conceptual problem is how to develop a con-
1To avoid confusion, we use TREENN for recursive neural networks and RNN for recurrent neural networks.
",1. Introduction,[0],[0]
"tinuous representation that follows syntax but not too much, that respects compositionality while also representing the fact that a small syntactic change can be a large semantic one.
",1. Introduction,[0],[0]
"To tackle this problem, we propose a new architecture, called neural equivalence networks (EQNET).",1. Introduction,[0],[0]
"EQNETs learn how syntactic composition recursively composes SEMVECs, like a TREENN, but are also designed to model large changes in semantics as the network progresses up the syntax tree.",1. Introduction,[0],[0]
"As equivalence is transitive, we formulate an objective function for training based on equivalence classes rather than pairwise decisions.",1. Introduction,[0],[0]
"The network architecture is based on composing residual-like multi-layer networks, which allows more flexibility in modeling the semantic mapping up the syntax tree.",1. Introduction,[0],[0]
"To encourage representations within an equivalence class to be tightly clustered, we also introduce a training method that we call subexpression autoencoding, which uses an autoencoder to force the representation of each subexpression to be predictable and reversible from its syntactic neighbors.",1. Introduction,[0],[0]
"Experimental evaluation on a highly diverse class of symbolic algebraic and boolean expression types shows that EQNETs dramatically outperform existing architectures like TREENNs and RNNs.
",1. Introduction,[0],[0]
"To summarize, the main contributions of our work are: (a) We formulate the problem of learning continuous semantic representations (SEMVECs) of symbolic expressions and develop benchmarks for this task.",1. Introduction,[0],[0]
"(b) We present neural equivalence networks (EQNETs), a neural network architecture that learns to represent expression semantics onto a continuous semantic representation space and how to perform symbolic operations in this space.",1. Introduction,[0],[0]
"(c) We provide an extensive evaluation on boolean and polynomial expressions, showing that EQNETs perform dramatically better than state-of-the-art alternatives.",1. Introduction,[0],[0]
Code and data are available at groups.inf.ed.ac.uk/cup/semvec.,1. Introduction,[0],[0]
"In this work, we are interested in learning semantic, compositional representations of mathematical expressions, which we call SEMVECs, and in learning to generate identical representations for expressions that are semantically equivalent, i.e. they belong to the same equivalence class.",2. Model,[0],[0]
"Equivalence is a stronger property than similarity, which has been the focus of previous work in neural network learning (Chopra et al., 2005), since equivalence is additionally a transitive relationship.
",2. Model,[0],[0]
Problem Hardness.,2. Model,[0],[0]
Finding the equivalence of arbitrary symbolic expressions is a NP-hard problem or worse.,2. Model,[0],[0]
"For example, if we focus on boolean expressions, reducing an expression to the representation of the false equivalence class amounts to proving its non-satisfiability — an NPcomplete problem.",2. Model,[0],[0]
"Of course, we do not expect to circum-
vent an NP-complete problem with neural networks.",2. Model,[0],[0]
A network for solving boolean equivalence would require an exponential number of nodes in the size of the expression if P 6= NP .,2. Model,[0],[0]
"Instead, our goal is to develop architectures that efficiently learn to solve the equivalence problems for expressions that are similar to a smaller number of expressions in a given training set.",2. Model,[0],[0]
"The supplementary material shows a sample of such expressions that illustrate the hardness of this problem.
",2. Model,[0],[0]
Notation and Framework.,2. Model,[0],[0]
"To allow our representations to be compositional, we employ the general framework of recursive neural networks (TREENN)",2. Model,[0],[0]
"(Socher et al., 2012; 2013), in our case operating on tree structures of the syntactic parse of a formula.",2. Model,[0],[0]
"Given a tree T , TREENNs learn distributed representations for each node in the tree by recursively combining the representations of its subtrees using a neural network.",2. Model,[0],[0]
We denote the children of a node n as ch(n) which is a (possibly empty) ordered tuple of nodes.,2. Model,[0],[0]
"We also use par(n) to refer to the parent node of n. Each node in our tree has a type, e.g. a terminal node could be of type “a” referring to the variable a or of type “and” referring to a node of the logical AND (∧) operation.",2. Model,[0],[0]
We refer to the type of a node n as τn.,2. Model,[0],[0]
"In pseudocode, TREENNs retrieve the representation of a tree T rooted at node ρ, by invoking the function TREENN(ρ) that returns a vector representation rρ ∈ RD, i.e., a SEMVEC.",2. Model,[0],[0]
"The function is defined as TREENN (current node n)
if n is not a leaf then rn ← COMBINE(TREENN(c0), . . .",2. Model,[0],[0]
", TREENN(ck), τn), where (c0, . . .",2. Model,[0],[0]
", ck) = ch(n) else rn ← LOOKUPLEAFEMBEDDING(τn)
return rn The general framework of TREENN allows two points of variation, the implementation of LOOKUPLEAFEMBEDDING and COMBINE.",2. Model,[0],[0]
"Traditional TREENNs (Socher et al., 2013) define LOOKUPLEAFEMBEDDING as a simple lookup operation within a matrix of embeddings and COMBINE as a single-layer neural network.",2. Model,[0],[0]
"As discussed next, these will both prove to be serious limitations in our setting.",2. Model,[0],[0]
"To train these networks to learn SEMVECs, we will use a supervised objective based on a set of known equivalence relations (see Section 2.2).",2. Model,[0],[0]
"Our domain requires that the network learns to abstract away syntax, assigning identical representations to expressions that may be syntactically different but semantically equivalent, and also assigning different representations to expressions that may be syntactically very similar but nonequivalent.",2.1. Neural Equivalence Networks,[0],[0]
"In this work, we find that standard neural architectures do not handle well this challenge.",2.1. Neural Equivalence Networks,[0],[0]
"To represent semantics from syntax, we need to learn to recursively
compose and decompose semantic representations and remove syntactic “noise”.",2.1. Neural Equivalence Networks,[0],[0]
"Any syntactic operation may significantly change semantics (e.g. negation, or appending ∧FALSE) while we may reach the same semantic state through many possible operations.",2.1. Neural Equivalence Networks,[0],[0]
This necessitates using high-curvature operations over the semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Furthermore, some operations are semantically reversible and thus we need to learn reversible semantic representations (e.g. ¬¬A and A should have an identical SEMVECs).",2.1. Neural Equivalence Networks,[0],[0]
"Based on these, we define neural equivalence networks (EQNET), which learn to compose representations of equivalence classes into new equivalence classes (Figure 1a).",2.1. Neural Equivalence Networks,[0],[0]
"Our network follows the TREENN architecture, i.e. is implemented using TREENN to model the compositional nature of symbolic expressions but is adapted based on the domain requirements.",2.1. Neural Equivalence Networks,[0],[0]
"The extensions we introduce have two aims: first, to improve the network training; and second, and more interestingly, to encourage the learned representations to abstract away surface level information while retaining semantic content.
",2.1. Neural Equivalence Networks,[0],[0]
The first extension that we introduce is to the network structure at each layer in the tree.,2.1. Neural Equivalence Networks,[0],[0]
"Traditional TREENNs (Socher et al., 2013) use a single-layer neural network at each tree node.",2.1. Neural Equivalence Networks,[0],[0]
"During our preliminary investigations and in Section 3, we found that single layer networks are not adequately expressive to capture all operations that transform the input SEMVECs to the output SEMVEC and maintain semantic equivalences, requiring high-curvature operations.",2.1. Neural Equivalence Networks,[0],[0]
Part of the problem stems from the fact that within the Euclidean space of SEMVECs some operations need to be non-linear.,2.1. Neural Equivalence Networks,[0],[0]
For example a simple XOR boolean operator requires high-curvature operations in the continuous semantic representation space.,2.1. Neural Equivalence Networks,[0],[0]
"Instead, we turn to multi-layer neural
networks.",2.1. Neural Equivalence Networks,[0],[0]
"In particular, we define the network as shown in the function COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
This uses a twolayer MLP with a residual-like connection to compute the SEMVEC of each parent node in that syntax tree given that of its children.,2.1. Neural Equivalence Networks,[0],[0]
"Each node type τn, e.g., each logical operator, has a different set of weights.",2.1. Neural Equivalence Networks,[0],[0]
"We experimented with deeper networks but this did not yield any improvements.
",2.1. Neural Equivalence Networks,[0],[0]
"However, as TREENNs become deeper, they suffer from optimization issues, such as diminishing and exploding gradients.",2.1. Neural Equivalence Networks,[0],[0]
"This is essentially because of the highly compositional nature of tree structures, where the same network (i.e. the COMBINE non-linear function) is used recursively, causing it to “echo” its own errors and producing unstable feedback loops.",2.1. Neural Equivalence Networks,[0],[0]
"We observe this problem even with only two-layer MLPs, as the overall network can become quite deep when using two layers for each node in the syntax tree.",2.1. Neural Equivalence Networks,[0],[0]
We resolve this issue in the training procedure by constraining each SEMVEC to have unit norm.,2.1. Neural Equivalence Networks,[0],[0]
"That is, we set LOOKUPLEAFEMBEDDING(τn) = Cτn/ ‖Cτn‖2 , and we normalize the output of the final layer of COMBINE in Figure 1b.",2.1. Neural Equivalence Networks,[0],[0]
"The normalization step of l̄out and Cτn is somewhat similar to weight normalization (Salimans & Kingma, 2016) and vaguely resembles layer normalization (Ba et al., 2016).",2.1. Neural Equivalence Networks,[0],[0]
"Normalizing the SEMVECs partially resolves issues with diminishing and exploding gradients, and removes a spurious degree of freedom in the semantic representation.",2.1. Neural Equivalence Networks,[0],[0]
"As simple as this modification may seem, we found it vital for obtaining good performance, and all of our multi-layer TREENNs converged to low-performing settings without it.
",2.1. Neural Equivalence Networks,[0],[0]
"Although these modifications seem to improve the representation capacity of the network and its ability to be trained, we found that they were not on their own sufficient for good
performance.",2.1. Neural Equivalence Networks,[0],[0]
"In our early experiments, we noticed that the networks were primarily focusing on syntax instead of semantics, i.e., expressions that were nearby in the continuous space were primarily ones that were syntactically similar.",2.1. Neural Equivalence Networks,[0],[0]
"At the same time, we observed that the networks did not learn to unify representations of the same equivalence class, observing multiple syntactically distinct but semantically equivalent expressions to have distant SEMVECs.
",2.1. Neural Equivalence Networks,[0],[0]
"Therefore we modify the training objective in order to encourage the representations to become more abstract, reducing their dependence on surface-level syntactic information.",2.1. Neural Equivalence Networks,[0],[0]
We add a regularization term on the SEMVECs that we call a subexpression autoencoder (SUBEXPAE).,2.1. Neural Equivalence Networks,[0],[0]
We design this regularization to encourage the SEMVECs to have two properties: abstraction and reversibility.,2.1. Neural Equivalence Networks,[0],[0]
"Because abstraction arguably means removing irrelevant information, a network with a bottleneck layer seems natural, but we want the training objective to encourage the bottleneck to discard syntactic information rather than semantic information.",2.1. Neural Equivalence Networks,[0],[0]
"To achieve this, we introduce a component that aims to encourage reversibility, which we explain by an example.",2.1. Neural Equivalence Networks,[0],[0]
"Observe that given the semantic representation of any two of the three nodes of a subexpression (by which we mean the parent, left child, right child of an expression tree)",2.1. Neural Equivalence Networks,[0],[0]
it is often possible to completely determine or at least place strong constraints on the semantics of the third.,2.1. Neural Equivalence Networks,[0],[0]
"For example, consider a boolean formula F (a, b) = F1(a, b) ∨ F2(a, b) where F1 and F2 are arbitrary propositional formulae over the variables a, b.",2.1. Neural Equivalence Networks,[0],[0]
"Then clearly if we know that F implies that a is true but F1 does not, then F2 must imply that a is true.",2.1. Neural Equivalence Networks,[0],[0]
"More generally, if F belongs to some equivalence class e0 and F1 belongs to a different class e1, we want the continuous representation of F2 to reflect that there are strong constraints on the equivalence class of F2.
",2.1. Neural Equivalence Networks,[0],[0]
"Subexpression autoencoding encourages abstraction by employing an autoencoder with a bottleneck, thereby removing irrelevant information from the representations, and encourages reversibility by autoencoding the parent and child representations together, to encourage dependence in the representations of parents and children.",2.1. Neural Equivalence Networks,[0],[0]
"More specifically, given any node p in the tree with children c0 . . .",2.1. Neural Equivalence Networks,[0],[0]
"ck, we can define a parent-children tuple",2.1. Neural Equivalence Networks,[0],[0]
"[rc0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", rck , rp] containing the (computed) SEMVECs of the children and parent nodes.",2.1. Neural Equivalence Networks,[0],[0]
What SUBEXPAE does is to autoencode this representation tuple into a low-dimensional space with a denoising autoencoder.,2.1. Neural Equivalence Networks,[0],[0]
"We then seek to minimize the reconstruction error of the child representations (r̃c0 , . . .",2.1. Neural Equivalence Networks,[0],[0]
", r̃ck ) as well as the reconstructed parent representation r̃p that can be computed from the reconstructed children.",2.1. Neural Equivalence Networks,[0],[0]
"More formally, we minimize the return value of SUBEXPAE in Figure 1c where n is a binary noise vector with κ percent of its elements set to zero.",2.1. Neural Equivalence Networks,[0],[0]
Note that the encoder is specific to the parent node type τp.,2.1. Neural Equivalence Networks,[0],[0]
"Although our SUBEXPAE may seem similar to the recursive autoencoders of Socher et al. (2011), it differs
in two major ways.",2.1. Neural Equivalence Networks,[0],[0]
"First, SUBEXPAE autoencodes on the entire parent-children representation tuple, rather than the child representations alone.",2.1. Neural Equivalence Networks,[0],[0]
"Second, the encoding is not used to compute the parent representation, but only serves as a regularizer.
",2.1. Neural Equivalence Networks,[0],[0]
Subexpression autoencoding has several desirable effects.,2.1. Neural Equivalence Networks,[0],[0]
"First, it forces each parent-children tuple to lie in a lowdimensional space, requiring the network to compress information from the individual subexpressions.",2.1. Neural Equivalence Networks,[0],[0]
"Second, because the denoising autoencoder is reconstructing parent and child representations together, this encourages child representations to be predictable from parents and siblings.",2.1. Neural Equivalence Networks,[0],[0]
"Putting these two together, the goal is that the information discarded by the autoencoder bottleneck will be more syntactic than semantic, assuming that the semantics of child node is more predictable from its parent and sibling than its syntactic realization.",2.1. Neural Equivalence Networks,[0],[0]
"The goal is to nudge the network to learn consistent, reversible semantics.",2.1. Neural Equivalence Networks,[0],[0]
"Additionally, subexpression autoencoding has the potential to gradually unify distant representations that belong to the same equivalence class.",2.1. Neural Equivalence Networks,[0],[0]
"To illustrate this point, imagine two semantically equivalent c′0 and c ′′ 0 child nodes of different expressions that
have distant SEMVECs, i.e. ∥∥rc′0 − rc′′0 ∥∥2 although COMBINE(rc′0 , . . . )",2.1. Neural Equivalence Networks,[0],[0]
"≈ COMBINE(rc′′0 , . . . ).",2.1. Neural Equivalence Networks,[0],[0]
"In some cases due to the autoencoder noise, the differences between the input tuple x′,x′′ that contain rc′0 and rc′′0 will be non-existent and the decoder will predict a single location r̃c0 (possibly different from rc′0 and rc′′0 ).",2.1. Neural Equivalence Networks,[0],[0]
"Then, when minimizing the reconstruction error, both rc′0 and rc′′0 will be attracted to r̃c0 and eventually should merge.",2.1. Neural Equivalence Networks,[0],[0]
We train EQNETs from a dataset of expressions whose semantic equivalence is known.,2.2. Training,[0],[0]
Given a training set T = {T1 . . .,2.2. Training,[0],[0]
"TN} of parse trees of expressions, we assume that the training set is partitioned into equivalence classes E = {e1 . . .",2.2. Training,[0],[0]
eJ}.,2.2. Training,[0],[0]
"We use a supervised objective similar to classification; the difference between classification and our setting is that whereas standard classification problems consider a fixed set of class labels, in our setting the number of equivalence classes in the training set will vary with N .",2.2. Training,[0],[0]
"Given an expression tree T that belongs to the equivalence class ei ∈ E , we compute the probability
P (ei|T ) =",2.2. Training,[0],[0]
"exp
( TREENN(T )>qei + bi )∑ j exp ( TREENN(T )>",2.2. Training,[0],[0]
qej,2.2. Training,[0],[0]
"+ bj
) (1) where qei are model parameters that we can interpret as representations of each equivalence class that appears in the training class, and bi are scalar bias terms.",2.2. Training,[0],[0]
"Note that in this work, we only use information about the equivalence class of the whole expression T , ignoring available information about subexpressions.",2.2. Training,[0],[0]
"This is without loss of generality, because if we do know the equivalence class of a subexpression of T , we can simply add that subexpression to
the training set.",2.2. Training,[0],[0]
"To train the model, we use a max-margin objective that maximizes classification accuracy, i.e.
LACC(T, ei) = max (
0, arg max ej 6=ei,ej∈E log P (ej |T ) P (ei|T ) +m ) (2)
where m > 0 is a scalar margin.",2.2. Training,[0],[0]
"And therefore the optimized loss function for a single expression tree T that belongs to equivalence class ei ∈ E is
L(T, ei) = LACC(T, ei) + µ |Q| ∑ n∈Q SUBEXPAE(ch(n), n)
(3)
",2.2. Training,[0],[0]
"where Q = {n ∈ T : | ch(n)| > 0}, i.e. contains the nonleaf nodes of T and µ ∈ (0, 1] a scalar weight.",2.2. Training,[0],[0]
"We found that subexpression autoencoding is counterproductive early in training, before the SEMVECs begin to represent aspects of semantics.",2.2. Training,[0],[0]
"So, for each epoch t, we set µ = 1− 10−νt with ν ≥ 0.",2.2. Training,[0],[0]
"Instead of the supervised objective that we propose, an alternative option for training EQNET would be a Siamese objective (Chopra et al., 2005) that learns about similarities (rather than equivalence) between expressions.",2.2. Training,[0],[0]
"In practice, we found the optimization to be very unstable, yielding suboptimal performance.",2.2. Training,[0],[0]
We believe that this has to do with the compositional and recursive nature of the task that creates unstable dynamics and the fact that equivalence is a stronger property than similarity.,2.2. Training,[0],[0]
Datasets.,3. Evaluation,[0],[0]
We generate datasets of expressions grouped into equivalence classes from two domains.,3. Evaluation,[0],[0]
The datasets from the BOOL domain contain boolean expressions and the POLY datasets contain polynomial expressions.,3. Evaluation,[0],[0]
"In both domains, an expression is either a variable, a binary operator that combines two expressions, or a unary operator applied to a single expression.",3. Evaluation,[0],[0]
"When defining equivalence, we interpret distinct variables as referring to different entities in the domain, so that, e.g., the polynomials c · (a · a+ b) and f ·(d·d+e) are not equivalent.",3. Evaluation,[0],[0]
"For each domain, we generate “simple” datasets which use a smaller set of possible operators and “standard” datasets which use a larger set of more complex operators.",3. Evaluation,[0],[0]
We generate each dataset by exhaustively generating all parse trees up to a maximum tree size.,3. Evaluation,[0],[0]
All expressions are symbolically simplified into a canonical from in order to determine their equivalence class and are grouped accordingly.,3. Evaluation,[0],[0]
Table 1 shows the datasets we generated.,3. Evaluation,[0],[0]
In the supplementary material we present some sample expressions.,3. Evaluation,[0],[0]
"For the polynomial domain, we also generated ONEV-POLY datasets, which are polynomials over a single variable, since they are similar to the setting considered by Zaremba et al. (2014) — although ONEV-POLY is still a little more general because it is not restricted to homogeneous polynomials.",3. Evaluation,[0],[0]
"Learning SEMVECs for boolean expressions
is already a hard problem; with n boolean variables, there are 22 n
equivalence classes (i.e. one for each possible truth table).",3. Evaluation,[0],[0]
"We split the datasets into training, validation and test sets.",3. Evaluation,[0],[0]
"We create two test sets, one to measure generalization performance on equivalence classes that were seen in the training data (SEENEQCLASS), and one to measure generalization to unseen equivalence classes (UNSEENEQCLASS).",3. Evaluation,[0],[0]
It is easiest to describe UNSEENEQCLASS first.,3. Evaluation,[0],[0]
"To create the UNSEENEQCLASS, we randomly select 20% of all the equivalence classes, and place all of their expressions in the test set.",3. Evaluation,[0],[0]
We select equivalence classes only if they contain at least two expressions but less than three times the average number of expressions per equivalence class.,3. Evaluation,[0],[0]
We thus avoid selecting very common (and hence trivial to learn) equivalence classes in the testset.,3. Evaluation,[0],[0]
"Then, to create SEENEQCLASS, we take the remaining 80% of the equivalence classes, and randomly split the expressions in each class into training, validation, SEENEQCLASS test in the proportions 60%–15%–25%.",3. Evaluation,[0],[0]
"We provide the datasets online at groups.inf.ed.ac.uk/cup/semvec.
Baselines.",3. Evaluation,[0],[0]
"To compare the performance of our model, we train the following baselines.",3. Evaluation,[0],[0]
"TF-IDF: learns a representation given the expression tokens (variables, operators and parentheses).",3. Evaluation,[0],[0]
This captures topical/declarative knowledge but is unable to capture procedural knowledge.,3. Evaluation,[0],[0]
GRU refers to the token-level gated recurrent unit encoder of Bahdanau et al. (2015) that encodes the token-sequence of an expression into a distributed representation.,3. Evaluation,[0],[0]
Stack-augmented RNN refers to the work of Joulin & Mikolov (2015) which was used to learn algorithmic patterns and uses a stack as a memory and operates on the expression tokens.,3. Evaluation,[0],[0]
We also include two recursive neural networks (TREENN).,3. Evaluation,[0],[0]
The 1- layer TREENN which is the original TREENN also used by Zaremba et al. (2014).,3. Evaluation,[0],[0]
"We also include a 2-layer TREENN, where COMBINE is a classic two-layer MLP without residual connections.",3. Evaluation,[0],[0]
"This shows the effect of SEMVEC normalization and subexpression autoencoder.
Hyperparameters.",3. Evaluation,[0],[0]
"We tune the hyperparameters of all models using Bayesian optimization (Snoek et al., 2012) on a boolean dataset with 5 variables and maximum tree size of 7 (not shown in Table 1) using the average k-NN (k = 1, . . .",3. Evaluation,[0],[0]
", 15) statistics (described next).",3. Evaluation,[0],[0]
The selected hyperparameters are detailed in the supplementary material.,3. Evaluation,[0],[0]
Metrics.,3.1. Quantitative Evaluation,[0],[0]
To evaluate the quality of the learned representations we count the proportion of k nearest neighbors of each expression (using cosine similarity) that belong to the same equivalence class.,3.1. Quantitative Evaluation,[0],[0]
"More formally, given a test query expression q in an equivalence class c we find the k nearest neighbors Nk(q) of q across all expressions, and define the
score as
scorek(q) = |Nk(q) ∩ c| min(k, |c|) .",3.1. Quantitative Evaluation,[0],[0]
"(4)
To report results for a given testset, we simply average scorek(q) for all expressions q in the testset.",3.1. Quantitative Evaluation,[0],[0]
"We also report the precision-recall curves for the problem of clustering the SEMVECs into their appropriate equivalence classes.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation.,3.1. Quantitative Evaluation,[0],[0]
Figure 2 presents the average per-model precision-recall curves across the datasets.,3.1. Quantitative Evaluation,[0],[0]
Table 1 shows score5 of UNSEENEQCLASS.,3.1. Quantitative Evaluation,[0],[0]
Detailed plots are found in the supplementary material.,3.1. Quantitative Evaluation,[0],[0]
"EQNET performs better for all datasets, by a large margin.",3.1. Quantitative Evaluation,[0],[0]
"The only exception is POLY5, where the 2-L TREENN performs better.",3.1. Quantitative Evaluation,[0],[0]
"However, this may have to do with the small size of the dataset.",3.1. Quantitative Evaluation,[0],[0]
The reader may observe that the simple datasets (containing fewer operations and variables) are easier to learn.,3.1. Quantitative Evaluation,[0],[0]
"Understandably, introducing more variables increases the size of the represented space reducing performance.",3.1. Quantitative Evaluation,[0],[0]
"The tf-idf method performs better in settings with more variables, because it captures well the variables and operations used.",3.1. Quantitative Evaluation,[0],[0]
Similar observations can be made for sequence models.,3.1. Quantitative Evaluation,[0],[0]
The one and two layer TREENNs have mixed performance; we believe that this has to do with exploding and diminishing gradients due to the deep and highly compositional nature of TREENNs.,3.1. Quantitative Evaluation,[0],[0]
"Although Zaremba et al. (2014) consider a different problem to us, they use data similar to the ONEV-POLY datasets with a traditional TREENN architecture.",3.1. Quantitative Evaluation,[0],[0]
"Our evaluation suggests that EQNETs perform much better within the ONEV-POLY setting.
",3.1. Quantitative Evaluation,[0],[0]
Evaluation of Compositionality.,3.1. Quantitative Evaluation,[0],[0]
"We evaluate whether EQNETs successfully learn to compute compositional representations, rather than overfitting to expression trees of
a small size.",3.1. Quantitative Evaluation,[0],[0]
"To do this we consider a type of transfer setting, in which we train on simpler datasets, but test on more complex ones; for example, training on the training set of BOOL5 but testing on the testset of BOOL8.",3.1. Quantitative Evaluation,[0],[0]
We average over 11 different train-test pairs (full list in supplementary material) and show the results in Figure 3a and Figure 3b.,3.1. Quantitative Evaluation,[0],[0]
"These graphs again show that EQNETs are better than any of the other methods, and indeed, performance is only a bit worse than in the non-transfer setting.
",3.1. Quantitative Evaluation,[0],[0]
"Impact of EQNET Components EQNETs differ from traditional TREENNs in two major ways, which we analyze here.",3.1. Quantitative Evaluation,[0],[0]
"First, SUBEXPAE improves performance.",3.1. Quantitative Evaluation,[0],[0]
"When training the network with and without SUBEXPAE, on average, the area under the curve (AUC) of scorek decreases by 16.8% on the SEENEQCLASS and 19.7% on the UNSEENEQCLASS.",3.1. Quantitative Evaluation,[0],[0]
"This difference is smaller in the transfer setting, where AUC decreases by 8.8% on average.",3.1. Quantitative Evaluation,[0],[0]
"However, even in this setting we observe that SUBEXPAE helps more in large and diverse datasets.",3.1. Quantitative Evaluation,[0],[0]
The second key difference to traditional TREENNs is the output normalization and the residual connections.,3.1. Quantitative Evaluation,[0],[0]
"Comparing our model to the one-layer and two-layer TREENNs again, we find that output normalization results in important improvements (the two-layer TREENNs have on average 60.9% smaller AUC).",3.1. Quantitative Evaluation,[0],[0]
"We note that only the combination of the residual connections and the output normalization improve the performance, whereas when used separately, there are no significant improvements over the two-layer TREENNs.",3.1. Quantitative Evaluation,[0],[0]
Table 2 shows expressions whose SEMVEC nearest neighbor is of an expression of another equivalence class.,3.2. Qualitative Evaluation,[0],[0]
"Manually inspecting boolean expressions, we find that EQNET confusions happen more when a XOR or implication operator is
Table 2.",3.2. Qualitative Evaluation,[0],[0]
Non semantically equivalent first nearest-neighbors from BOOL8 and POLY8.,3.2. Qualitative Evaluation,[0],[0]
"A checkmark indicates that the method correctly results in the nearest neighbor being from the same equivalence class.
",3.2. Qualitative Evaluation,[0],[0]
Expr a ∧ (a ∧ (a ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (a+ c)) ((a+ c) ·,3.2. Qualitative Evaluation,[0],[0]
"c) + a (b · b)− b
tfidf c",3.2. Qualitative Evaluation,[0],[0]
∧,3.2. Qualitative Evaluation,[0],[0]
((a ∧ a) ∧ (¬a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
c⇒ (¬((c ∧ a) ∧ a)),3.2. Qualitative Evaluation,[0],[0]
a+ (c+ a) ·,3.2. Qualitative Evaluation,[0],[0]
c (c · a) + (a+ c) b · (b− b) GRU X a ∧ (a ∧ (c ∧ (¬c))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬c)),3.2. Qualitative Evaluation,[0],[0]
b+ (c · (a+ c)) ((b+ c) · c) + a (b+ b) · b− b 1L-TREENN a ∧ (a ∧ (a ∧ (¬b))),3.2. Qualitative Evaluation,[0],[0]
a ∧ (a ∧ (c⇒ (¬b))),3.2. Qualitative Evaluation,[0],[0]
(a ∧ a) ∧ (c⇒ (¬b)),3.2. Qualitative Evaluation,[0],[0]
a+ (c · (b+ c)) ((b+ c) ·,3.2. Qualitative Evaluation,[0],[0]
c) + a (a− c) · b− b EQNET X X (¬(b⇒ (b ∨ c))),3.2. Qualitative Evaluation,[0],[0]
"∧ a X X (b · b) · b− b
involved.",3.2. Qualitative Evaluation,[0],[0]
"In fact, we fail to find any confused expressions for EQNET not involving these operations in BOOL5 and in the top 100 expressions in BOOL10.",3.2. Qualitative Evaluation,[0],[0]
"As expected, tf-idf confuses expressions with others that contain the same operators and variables ignoring order.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, GRU and TREENN tend to confuse expressions with very similar symbolic representations, i.e. that differ in one or two deeply nested variables or operators.",3.2. Qualitative Evaluation,[0],[0]
"In contrast, EQNET tends to confuse fewer expressions (as we previously showed) and the confused expressions tend to be more syntactically diverse and semantically related.
",3.2. Qualitative Evaluation,[0],[0]
Figure 4 shows a visualization of score5 for each node in the expression tree.,3.2. Qualitative Evaluation,[0],[0]
"One may see that as EQNET knows how
¬(c ⊕ (a ∧ ((a ⊕ c) ∧ b)))",3.2. Qualitative Evaluation,[0],[0]
"((c ∨ (¬b))⇒ a) ∧ (a ⇒ a)
((b ⊕ (¬c))",3.2. Qualitative Evaluation,[0],[0]
"∧ b)⊕ (a ∨ b) ((b · a)− a) · b
a − ((a + b) · a) ((c · b) · c) · a b + ((b · b) · b)
Figure 4.",3.2. Qualitative Evaluation,[0],[0]
Visualization of score5 for all expression nodes for three BOOL10 and four POLY8 test sample expressions using EQNET.,3.2. Qualitative Evaluation,[0],[0]
"The darker the color, the lower the score, i.e. white implies a score of 1 and dark red a score of 0.
to compose expressions that achieve good score, even if the subexpressions achieve a worse score.",3.2. Qualitative Evaluation,[0],[0]
"This suggests that for common expressions, (e.g. single variables and monomials) the network tends to select a unique location, without merging the equivalence classes or affecting the upstream performance of the network.",3.2. Qualitative Evaluation,[0],[0]
"Larger scale interactive t-SNE visualizations can be found online.
",3.2. Qualitative Evaluation,[0],[0]
Figure 5 presents two PCA visualizations of the SEMVECs of simple expressions and their negations/negatives.,3.2. Qualitative Evaluation,[0],[0]
It can be discerned that the black dots and their negations (in red) are discriminated in the semantic representation space.,3.2. Qualitative Evaluation,[0],[0]
"Figure 5b shows this property in a clear manner: left-right discriminates between polynomials with 1 and −a, topbottom between polynomials with−b and b and the diagonal parellelt to y = −x between c and−c.",3.2. Qualitative Evaluation,[0],[0]
We observe a similar behavior in Figure 5a for boolean expressions.,3.2. Qualitative Evaluation,[0],[0]
"Researchers have proposed compilation schemes that can transform any given program or expression to an equivalent neural network (Gruau et al., 1995; Neto et al., 2003; Siegel-
mann, 1994).",4. Related Work,[0],[0]
One can consider a serialized version of the resulting neural network as a representation of the expression.,4. Related Work,[0],[0]
"However, it is not clear how we could compare the serialized representations corresponding to two expressions and whether this mapping preserves semantic distances.
",4. Related Work,[0],[0]
"Recursive neural networks (TREENN) (Socher et al., 2012; 2013) have been successfully used in NLP with multiple applications.",4. Related Work,[0],[0]
Socher et al. (2012) show that TREENNs can learn to compute the values of some simple propositional statements.,4. Related Work,[0],[0]
"EQNET’s SUBEXPAE may resemble recursive autoencoders (Socher et al., 2011) but differs in form and function, encoding the whole parent-children tuple to force a clustering behavior.",4. Related Work,[0],[0]
"In addition, when encoding each expression our architecture does not use a pooling layer but directly produces a single representation for the expression.
",4. Related Work,[0],[0]
Mou et al. (2016) design tree convolutional networks to classify code into student submission tasks.,4. Related Work,[0],[0]
"Although they learn representations of the student tasks, these representations capture task-specific syntactic features rather than code semantics.",4. Related Work,[0],[0]
Piech et al. (2015) also learn distributed matrix representations of student code submissions.,4. Related Work,[0],[0]
"However, to learn the representations, they use input and output program states and do not test for program equivalence.",4. Related Work,[0],[0]
"Additionally, these representations do not necessarily represent program equivalence, since they do not learn the representations over all possible input-outputs.",4. Related Work,[0],[0]
"Allamanis et al. (2016) learn variable-sized representations of source code snippets to summarize them with a short function-like name but aim learn summarization features in code rather than representations of symbolic expression equivalence.
",4. Related Work,[0],[0]
"More closely related is the work of Zaremba et al. (2014) who use a TREENN to guide the search for more efficient mathematical identities, limited to homogeneous singlevariable polynomial expressions.",4. Related Work,[0],[0]
"In contrast, EQNETs consider at a much wider set of expressions, employ subexpression autoencoding to guide the learned SEMVECs to better
represent equivalence, and do not use search when looking for equivalent expressions.",4. Related Work,[0],[0]
Alemi et al. (2016) use RNNs and convolutional neural networks to detect features within mathematical expressions to speed the search for premise selection in automated theorem proving but do not explicitly account for semantic equivalence.,4. Related Work,[0],[0]
"In the future, SEMVECs may be useful within this area.
",4. Related Work,[0],[0]
"Our work is also related to recent work on neural network architectures that learn controllers/programs (Gruau et al., 1995; Graves et al., 2014; Joulin & Mikolov, 2015; Grefenstette et al., 2015; Dyer et al., 2015; Reed & de Freitas, 2016; Neelakantan et al., 2015; Kaiser & Sutskever, 2016).",4. Related Work,[0],[0]
"In contrast to this work, we do not aim to learn how to evaluate expressions or execute programs with neural network architectures but to learn continuous semantic representations (SEMVECs) of expression semantics irrespectively of how they are syntactically expressed or evaluated.",4. Related Work,[0],[0]
"In this work, we presented EQNETs, a first step in learning continuous semantic representations (SEMVECs) of procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"SEMVECs have the potential of bridging continuous representations with symbolic representations, useful in multiple applications in artificial intelligence, machine learning and programming languages.
",5. Discussion & Conclusions,[0],[0]
We show that EQNETs perform significantly better than state-of-the-art alternatives.,5. Discussion & Conclusions,[0],[0]
"But further improvements are needed, especially for more robust training of compositional models.",5. Discussion & Conclusions,[0],[0]
"In addition, even for relatively small symbolic expressions, we have an exponential explosion of the semantic space to be represented.",5. Discussion & Conclusions,[0],[0]
"Fixed-sized SEMVECs, like the ones used in EQNET, eventually limit the capacity that is available to represent procedural knowledge.",5. Discussion & Conclusions,[0],[0]
"In the future, to represent more complex procedures, variable-sized representations would seem to be required.",5. Discussion & Conclusions,[0],[0]
This work was supported by Microsoft Research through its PhD Scholarship Programme and the Engineering and Physical Sciences Research Council [grant number EP/K024043/1].,Acknowledgments,[0],[0]
We thank the University of Edinburgh Data Science EPSRC Centre for Doctoral Training for providing additional computational resources.,Acknowledgments,[0],[0]
"Combining abstract, symbolic reasoning with continuous neural reasoning is a grand challenge of representation learning.",abstractText,[0],[0]
"As a step in this direction, we propose a new architecture, called neural equivalence networks, for the problem of learning continuous semantic representations of algebraic and logical expressions.",abstractText,[0],[0]
"These networks are trained to represent semantic equivalence, even of expressions that are syntactically very different.",abstractText,[0],[0]
"The challenge is that semantic representations must be computed in a syntax-directed manner, because semantics is compositional, but at the same time, small changes in syntax can lead to very large changes in semantics, which can be difficult for continuous neural architectures.",abstractText,[0],[0]
"We perform an exhaustive evaluation on the task of checking equivalence on a highly diverse class of symbolic algebraic and boolean expression types, showing that our model significantly outperforms existing architectures.",abstractText,[0],[0]
Learning Continuous Semantic Representations of Symbolic Expressions,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1285–1295, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Monolingual word embeddings have had widespread success in many NLP tasks including sentiment analysis (Socher et al., 2013), dependency parsing (Dyer et al., 2015), machine translation (Bahdanau et al., 2014).",1 Introduction,[0],[0]
"Crosslingual word embeddings are a natural extension facilitating various crosslingual tasks, e.g. through transfer learning.",1 Introduction,[0],[0]
"A model built in a source resource-rich language can then applied to the target resource poor languages (Yarowsky and Ngai, 2001; Das and Petrov, 2011; Täckström",1 Introduction,[0],[0]
"et al., 2012; Duong et al., 2015).",1 Introduction,[0],[0]
A key barrier for crosslingual transfer is lexical matching between the source and the target language.,1 Introduction,[0],[0]
"Crosslingual word embeddings are a natural remedy where both source and target language lexicon are presented as dense vectors in the same vector space (Klementiev et al., 2012).
",1 Introduction,[0],[0]
Most previous work has focused on down-stream crosslingual applications such as document classification and dependency parsing.,1 Introduction,[0],[0]
We argue that good crosslingual embeddings should preserve both monolingual and crosslingual quality which we will use as the main evaluation criterion through monolingual word similarity and bilingual lexicon induction tasks.,1 Introduction,[0],[0]
"Moreover, many prior work (Chandar A P et al., 2014; Kočiský et al., 2014) used bilingual or comparable corpus which is also expensive for many low-resource languages.",1 Introduction,[0],[0]
"Søgaard et al. (2015) impose a less onerous data condition in the form of linked Wikipedia entries across several languages, however this approach tends to underperform other methods.",1 Introduction,[0],[0]
"To capture the monolingual distributional properties of words it is crucial to train on large monolingual corpora (Luong et al., 2015).",1 Introduction,[0],[0]
"However, many previous approaches are not capable of scaling up either because of the complicated objective functions or the nature of the algorithm.",1 Introduction,[0],[0]
"Other methods use a dictionary as the bridge between languages (Mikolov et al., 2013a; Xiao and Guo, 2014), however they do not adequately handle translation ambiguity.
",1 Introduction,[0],[0]
"Our model uses a bilingual dictionary from Panlex (Kamholz et al., 2014) as the source of bilingual signal.",1 Introduction,[0],[0]
"Panlex covers more than a thousand languages and therefore our approach applies to many languages, including low-resource languages.",1 Introduction,[0],[0]
"Our method selects the translation based on the context in an Expectation-Maximization style training algorithm which explicitly handles polysemy through incorporating multiple dictionary translations (word sense and translation are closely linked (Resnik and Yarowsky, 1999)).",1 Introduction,[0],[0]
"In addition to the dictionary,
1285
our method only requires monolingual data.",1 Introduction,[0],[0]
"Our approach is an extension of the continuous bag-ofwords (CBOW) model (Mikolov et al., 2013b) to inject multilingual training signal based on dictionary translations.",1 Introduction,[0],[0]
"We experiment with several variations of our model, whereby we predict only the translation or both word and its translation and consider different ways of using the different learned center-word versus context embeddings in application tasks.",1 Introduction,[0],[0]
We also propose a regularisation method to combine the two embedding matrices during training.,1 Introduction,[0],[0]
"Together, these modifications substantially improve the performance across several tasks.",1 Introduction,[0],[0]
"Our final model achieves state-of-the-art performance on bilingual lexicon induction task, large improvement over word similarity task compared with previous published crosslingual word embeddings, and competitive result on cross-lingual document classification task.",1 Introduction,[0],[0]
"Notably, our embedding combining techniques are general, yielding improvements also for monolingual word embedding.
",1 Introduction,[0],[0]
"This paper makes the following contributions:
• Proposing a new crosslingual training method for learning vector embeddings, based only on monolingual corpora and a bilingual dictionary;
• Evaluating several methods for combining embeddings, which are shown to help in both crosslingual and monolingual evaluations; and
• Achieving consistent results which are competitive in monolingual, bilingual and crosslingual transfer settings.",1 Introduction,[0],[0]
"There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource.",2 Related work,[0],[0]
"This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors (Luong et al., 2015; Klementiev et al., 2012).",2 Related work,[0],[0]
"These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level (Chandar A P et al., 2014; Hermann and Blunsom, 2014; Gouws et al., 2015) through learning compositional vector representations of sentences,
in order that sentences and their translations representations closely match.",2 Related work,[0],[0]
"The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments.",2 Related work,[0],[0]
"Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs.
Vulić and Moens (2015) use bilingual comparable text, sourced from Wikipedia.",2 Related work,[0],[0]
Their approach creates a psuedo-document by forming a bag-ofwords from the lemmatized nouns in each comparable document concatenated over both languages.,2 Related work,[0],[0]
These pseudo-documents are then used for learning vector representations using Word2Vec.,2 Related work,[0],[0]
"Their system, despite its simplicity, performed surprisingly well on a bilingual lexicon induction task (we compare our method with theirs on this task.)",2 Related work,[0],[0]
"Their approach is compelling due to its lesser resource requirements, although comparable bilingual data is scarce for many languages.",2 Related work,[0],[0]
"Related, Søgaard et al. (2015) exploit the comparable part of Wikipedia.",2 Related work,[0],[0]
"They represent word using Wikipedia entries which are shared for many languages.
",2 Related work,[0],[0]
A bilingual dictionary is an alternative source of bilingual information.,2 Related work,[0],[0]
"Gouws and Søgaard (2015) randomly replace the text in a monolingual corpus with a random translation, using this corpus for learning word embeddings.",2 Related work,[0],[0]
"Their approach doesn’t handle polysemy, as very few of the translations for each word will be valid in context.",2 Related work,[0],[0]
For this reason a high coverage or noisy dictionary with many translations might lead to poor outcomes.,2 Related work,[0],[0]
"Mikolov et al. (2013a), Xiao and Guo (2014) and Faruqui and Dyer (2014) filter a bilingual dictionary for one-to-one translations, thus side-stepping the problem, however discarding much of the information in the dictionary.",2 Related work,[0],[0]
"Our approach also uses a dictionary, however we use all the translations and explicitly disambiguate translations during training.
",2 Related work,[0],[0]
Another distinguishing feature on the above-cited research is the method for training embeddings.,2 Related work,[0],[0]
Mikolov et al. (2013a) and Faruqui and Dyer (2014) use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary.,2 Related work,[0],[0]
"Most of the other works train multlingual models jointly, which appears to have better performance over cascade training (Gouws et al., 2015).
",2 Related work,[0],[0]
For this reason we also use a form of joint training in our work.,2 Related work,[0],[0]
"Our model is an extension of the contextual bag of words (CBOW) model of Mikolov et al. (2013b), a method for learning vector representations of words based on their distributional contexts.",3 Word2Vec,[0],[0]
"Specifically, their model describes the probability of a token wi at position i using logistic regression with a factored parameterisation,
p(wi|wi±k\i) = exp(u>wihi)∑ w∈W exp(u > whi) , (1)
where hi = 12k ∑k
j=−k;j",3 Word2Vec,[0],[0]
6=0 vwi+j is a vector encoding the context over a window of size k centred around position,3 Word2Vec,[0],[0]
"i, W is the vocabulary and the parameters V and U ∈ R|W |×d are matrices referred to as the context and word embeddings.",3 Word2Vec,[0],[0]
"The model is trained to maximise the log-pseudo likelihood of a training corpus, however due to the high complexity of computing the denominator of equation (1), Mikolov et al. (2013b) propose negative sampling as an approximation, by instead learning to differentiate data from noise (negative examples).",3 Word2Vec,[0],[0]
"This gives rise to the following optimisation objective
∑
i∈D
( log σ(u>wihi)+",3 Word2Vec,[0],[0]
"p∑
j=1
Ewj∼Pn(w) log σ(−u>wjhi) ) ,
(2) where D is the training data and p is the number of negative examples randomly drawn from a noise distribution Pn(w).",3 Word2Vec,[0],[0]
"Our approach extends CBOW to model bilingual text, using two monolingual corpora and a bilingual dictionary.",4 Our Approach,[0],[0]
We believe this data condition to be less stringent than requiring parallel or comparable texts as the source of the bilingual signal.,4 Our Approach,[0],[0]
"It is common for field linguists to construct a bilingual dictionary when studying a new language, as one of the first steps in the language documentation process.",4 Our Approach,[0],[0]
"Translation dictionaries are a rich information source, capturing much of the lexical ambiguity in a language through translation.",4 Our Approach,[0],[0]
"For example, the word bank in English might mean the river bank
Algorithm 1 EM algorithm for selecting translation during training, where θ = (U,V) are the model parameters and η is the learning rate.
",4 Our Approach,[0],[0]
"1: randomly initialize V, U 2: for i < Iter do 3: for i ∈ De ∪Df do 4: s← vwi",4 Our Approach,[0],[0]
+,4 Our Approach,[0],[0]
"hi 5: w̄i = argmaxw∈dict(wi) cos(s,vw) 6: θ ← θ + η ∂O(w̄i,wi,hi)∂θ {see (3) or (5)} 7: end for 8: end for
or financial bank which corresponds to two different translations sponda and banca in Italian.",4 Our Approach,[0],[0]
"If we are able to learn to select good translations, then this implicitly resolves much of the semantic ambiguity in the language, and accordingly we seek to use this idea to learn better semantic vector representations of words.",4 Our Approach,[0],[0]
"To learn bilingual relations, we use the context in one language to predict the translation of the centre word in another language.",4.1 Dictionary replacement,[0],[0]
This is motivated by the fact that the context is an excellent means of disambiguating the translation for a word.,4.1 Dictionary replacement,[0],[0]
"Our method is closely related to Gouws and Søgaard (2015), however we only replace the middle word wi with a translation w̄i while keeping the context fixed.",4.1 Dictionary replacement,[0],[0]
"We replace each centre word with a translation on the fly during training, predicting instead p(w̄i|wi±k\i) but using the same formulation as equation (1) albeit with an augmented U matrix to cover word types in both languages.
",4.1 Dictionary replacement,[0],[0]
The translation w̄i is selected from the possible translations of wi listed in the dictionary.,4.1 Dictionary replacement,[0],[0]
"The problem of selecting the correct translation from the many options is reminiscent of the problem faced in expectation maximisation (EM), in that crosslingual word embeddings will allow for accurate translation, however to learn these embeddings we need to know the translations.",4.1 Dictionary replacement,[0],[0]
"We propose an EMinspired algorithm, as shown in Algorithm 1, which operates over both monolingual corpora, De and Df .",4.1 Dictionary replacement,[0],[0]
"The vector s is the semantic representation combining both the centre word, wi, and the con-
text,1 which is used to choose the best translation into the other language from the bilingual dictionary dict(wi).2 After selecting the translation, we use w̄i together with the context vector h to make a stochastic gradient update of the CBOW log-likelihood.",4.1 Dictionary replacement,[0],[0]
Words and their translations should appear in very similar contexts.,4.2 Joint Training,[0],[0]
One way to enforce this is to jointly learn to predict both the word and its translation from its monolingual context.,4.2 Joint Training,[0],[0]
"This gives rise to the following joint objective function,
O = ∑
i∈De∪Df
( α log σ(u>wihi)+(1−α) log σ(u>w̄ihi)
+
p∑
j=1
Ewj∼Pn(w) log σ(−u>wjhi) ) , (3)
where α controls the contribution of the two terms.",4.2 Joint Training,[0],[0]
"For our experiments, we set α = 0.5.",4.2 Joint Training,[0],[0]
The negative examples are drawn from combined vocabulary unigram distribution calculated from combined data De ∪Df .,4.2 Joint Training,[0],[0]
Many vector learning methods learn two embedding spaces V and U. Usually only V is used in application.,4.3 Combining Embeddings,[0],[0]
"The use of U, on the other hand, is understudied (Levy and Goldberg, 2014) with the exception of Pennington et al. (2014) who use a linear combination U + V, with minor improvement over V alone.
",4.3 Combining Embeddings,[0],[0]
"We argue that with our model, V is better at capturing the monolingual regularities and U is better at capturing bilingual signal.",4.3 Combining Embeddings,[0],[0]
The intuition for this is as follows.,4.3 Combining Embeddings,[0],[0]
"Assuming that we are predicting the word finance and its Italian translation finanze from the context (money, loan, bank, debt, credit) as shown in figure 1.",4.3 Combining Embeddings,[0],[0]
"In V only the context word representations are updated and in U only the representations of finance, finanze and negative samples such as tree and dog are updated.",4.3 Combining Embeddings,[0],[0]
"CBOW learns good embeddings because each time it updates the parameters, the words in the contexts are pushed closer to each
1Using both embeddings gives a small improvement compared to just using context vector h alone.
",4.3 Combining Embeddings,[0],[0]
"2We also experimented with using expectations over translations, as per standard EM, with slight degredation in results.
",4.3 Combining Embeddings,[0],[0]
other in the V space.,4.3 Combining Embeddings,[0],[0]
"Similarly, the target word wi and the translation w̄i are also pushed closer in the U space.",4.3 Combining Embeddings,[0],[0]
This is directly related to poitwise mutual information values of each pair of word and context explained in Levy and Goldberg (2014).,4.3 Combining Embeddings,[0],[0]
"Thus, U is bound to better at bilingual lexicon induction task and V is better at monolingual word similarity task.
",4.3 Combining Embeddings,[0],[0]
"The simple question is, how to combine both V and U to produce a better representation.",4.3 Combining Embeddings,[0],[0]
"We experiment with several ways to combine V and U. First, we can follow Pennington et al. (2014) to interpolate V and U in the post-processing step.",4.3 Combining Embeddings,[0],[0]
"i.e.
γV +",4.3 Combining Embeddings,[0],[0]
"(1− γ)U (4)
where γ controls the contribution of each embedding space.",4.3 Combining Embeddings,[0],[0]
"Second, we can also concatenate V and U instead of interpolation such that C =",4.3 Combining Embeddings,[0],[0]
"[V : U] where C ∈ R|W |×2d and W is the combined vocabulary from De ∪Df .
",4.3 Combining Embeddings,[0],[0]
"Moreover, we can also fuse V and U during training.",4.3 Combining Embeddings,[0],[0]
"For each word in the combined dictionary Ve ∪ Vf , we encourage the model to learn similar representation in both V and U by adding a regularization term to the objective function in equation (3) during training.
O′ =",4.3 Combining Embeddings,[0],[0]
"O + δ ∑
w∈Ve∪Vf ‖uw − vw‖22 (5)
where δ controls to what degree we should bind two spaces together.3",4.3 Combining Embeddings,[0],[0]
"Our experimental evaluation seeks to determine how well lexical distances in the learned embedding
3In the stochastic gradient update for a given word in context, we only compute the gradient of the regularisation term in (5) with respect to the words in the set of positive and negative examples.
",5 Experimental Setup,[0],[0]
spaces match with known lexical similarity judgements from bilingual and monolingual lexical resources.,5 Experimental Setup,[0],[0]
"To this end, in §6 we test crosslingual distances using a bilingual lexicon induction task in which we evaluate the embeddings in terms of how well nearby pairs of words from two languages in the embedding space match with human judgements.",5 Experimental Setup,[0],[0]
"Next, to evaluate the monolingual embeddings we evaluate word similarities in a single language against standard similarity datasets (§7).",5 Experimental Setup,[0],[0]
"Lastly, to demonstrate the usefulness of our embeddings in a task-based setting, we evaluate on crosslingual document classification (§9).
",5 Experimental Setup,[0],[0]
Monolingual Data,5 Experimental Setup,[0],[0]
The monolingual data is taken from the pre-processed Wikipedia dump from AlRfou et al. (2013).,5 Experimental Setup,[0],[0]
The data is already cleaned and tokenized.,5 Experimental Setup,[0],[0]
We additionally lower-case all words.,5 Experimental Setup,[0],[0]
Normally monolingual word embeddings are trained on billions of words.,5 Experimental Setup,[0],[0]
"However, obtaining that much monolingual data for a low-resource language is infeasible.",5 Experimental Setup,[0],[0]
"Therefore, we only select the first 5 million sentences (around 100 million words) for each language.
",5 Experimental Setup,[0],[0]
Dictionary A bilingual dictionary is the only source of bilingual correspondence in our technique.,5 Experimental Setup,[0],[0]
"We prefer a dictionary that covers many languages, such that our approach can be applied widely to many low-resource languages.",5 Experimental Setup,[0],[0]
"We use Panlex, a dictionary which currently covers around 1300 language varieties with about 12 million expressions.",5 Experimental Setup,[0],[0]
"The translations in PanLex come from various sources such as glossaries, dictionaries, automatic inference from other languages, etc.",5 Experimental Setup,[0],[0]
"Accordingly, Panlex has high language coverage but often noisy translations.4 Table 1 summarizes the sizes of monolingual corpora and dictionaries for each pair of language in our experiments.
",5 Experimental Setup,[0],[0]
4We also experimented with a crowd-sourced dictionary from Wiktionary.,5 Experimental Setup,[0],[0]
Our initial observation was that the translation quality was better but with a lower-coverage.,5 Experimental Setup,[0],[0]
"For example, for en-it dictionary, Panlex and Wiktionary have a coverage of 42.1% and 16.8% respectively for the top 100k most frequent English words from Wikipedia.",5 Experimental Setup,[0],[0]
The average number of translations are 5.2 and 1.9 respectively.,5 Experimental Setup,[0],[0]
We observed similar trend using Panlex and Wiktionary dictionary in our model.,5 Experimental Setup,[0],[0]
"However, using Panlex results in much better performance.",5 Experimental Setup,[0],[0]
We can run the model on the combined dictionary from both Panlex and Wiktionary but we leave it for future work.,5 Experimental Setup,[0],[0]
"Given a word in a source language, the bilingual lexicon induction (BLI) task is to predict its translation in the target language.",6 Bilingual Lexicon Induction,[0],[0]
Vulić and Moens (2015) proposed this task to test crosslingual word embeddings.,6 Bilingual Lexicon Induction,[0],[0]
The difficulty of this is that it is evaluated using the recall of the top ranked word.,6 Bilingual Lexicon Induction,[0],[0]
"The model must be very discriminative in order to score well.
",6 Bilingual Lexicon Induction,[0],[0]
"We build the CLWE for 3 language pairs: it-en, es-en and nl-en, using similar parameters setting with Vulić and Moens (2015).5 The remaining tunable parameters in our system are δ from Equation (5), and the choice of algorithm for combining embeddings.",6 Bilingual Lexicon Induction,[0],[0]
"We use the regularization technique from §4.3 for combining context and word embeddings with δ = 0.01, and word embeddings U are used as the output for all experiments (but see comparative experiments in §8.)
",6 Bilingual Lexicon Induction,[0],[0]
"Qualitative evaluation We jointly train the model to predict both wi and the translation w̄i, combine V and U during training for each language pair.",6 Bilingual Lexicon Induction,[0],[0]
Table 2 shows the top 10 closest words in both source and target languages according to cosine similarity.,6 Bilingual Lexicon Induction,[0],[0]
"Note that the model correctly identifies the translation in en as the top candidate, and the top 10 words in both source and target languages are highly related.",6 Bilingual Lexicon Induction,[0],[0]
"This qualitative evaluation initially demonstrates the ability of our CLWE to capture both the bilingual and monolingual relationship.
",6 Bilingual Lexicon Induction,[0],[0]
Quantitative evaluation Table 3 shows our results compared with prior work.,6 Bilingual Lexicon Induction,[0],[0]
"We reimple-
5Default learning rate of 0.025, negative sampling with 25 samples, subsampling rate of value 1e−4, embedding dimension d = 200, window size cs = 48 and run for 15 epochs.
",6 Bilingual Lexicon Induction,[0],[0]
ment Gouws and Søgaard (2015) using Panlex and Wiktionary dictionaries.,6 Bilingual Lexicon Induction,[0],[0]
The result with Panlex is substantially worse than with Wiktionary.,6 Bilingual Lexicon Induction,[0],[0]
This confirms our hypothesis in §2.,6 Bilingual Lexicon Induction,[0],[0]
"That is the context might be corrupted if we just randomly replace the training data with the translation from noisy dictionary such as Panlex.
",6 Bilingual Lexicon Induction,[0],[0]
"Our model when randomly picking the translation is similar to Gouws and Søgaard (2015), using the Panlex dictionary.",6 Bilingual Lexicon Induction,[0],[0]
The biggest difference is that they replace the training data (both context and middle word) while we fix the context and only replace the middle word.,6 Bilingual Lexicon Induction,[0],[0]
"For a high coverage yet noisy dictionary such as Panlex, our approach gives better average score.",6 Bilingual Lexicon Induction,[0],[0]
"Comparing our two most basic models (EM selection and random selection), it is clear that the model using EM to select the translation outperforms random selection by a significant margin.
",6 Bilingual Lexicon Induction,[0],[0]
"Our joint model, as described in equation (3) which predicts both target word and the translation, further improves the performance, especially for nl-en.",6 Bilingual Lexicon Induction,[0],[0]
We use equation (5) to combine both context embeddings V and word embeddings U for all three language pairs.,6 Bilingual Lexicon Induction,[0],[0]
This modification during training substantially improves the performance.,6 Bilingual Lexicon Induction,[0],[0]
"More importantly, all our improvements are consistent for all three language pairs and both evaluation metrics, showing the robustness of our models.
",6 Bilingual Lexicon Induction,[0],[0]
Our combined model out-performed previous approaches by a large margin.,6 Bilingual Lexicon Induction,[0],[0]
"Vulić and Moens (2015)
used bilingual comparable data, but this might be hard to obtain for some language pairs.",6 Bilingual Lexicon Induction,[0],[0]
Their performance on nl-en is poor because their comparable data between en and nl is small.,6 Bilingual Lexicon Induction,[0],[0]
"Besides, they also use POS tagger and lemmatizer to filter only Noun and reduce the morphology complexity during training.",6 Bilingual Lexicon Induction,[0],[0]
These tools might not be available for many languages.,6 Bilingual Lexicon Induction,[0],[0]
"For a fairer comparison to their work, we also use the same Treetagger (Schmid, 1995) to lemmatize the output of our combined model before evaluation.",6 Bilingual Lexicon Induction,[0],[0]
Table 3 (+lemmatization) shows some improvements but minor.,6 Bilingual Lexicon Induction,[0],[0]
It demonstrates that our model is already good at disambiguating morphology.,6 Bilingual Lexicon Induction,[0],[0]
"For example, the top 2 translations for es word lenguas in en are languages and language which correctly prefer the plural translation.",6 Bilingual Lexicon Induction,[0],[0]
Now we consider the efficacy of our CLWE on monolingual word similarity.,7 Monolingual Word Similarity,[0],[0]
"We evaluate on English monolingual similarity on WordSim353 (WSen), RareWord (RW-en) and German version of WordSim353 (WS-de) (Finkelstein et al., 2001; Luong et al., 2013; Luong et al., 2015).",7 Monolingual Word Similarity,[0],[0]
"Each of those datasets contain many tuples (w1, w2,s) where s is a scalar denoting the semantic similarity between w1 and w2 given by human annotators.",7 Monolingual Word Similarity,[0],[0]
"Good system should produce the score correlated with human judgement.
",7 Monolingual Word Similarity,[0],[0]
"We train the model as described in §4, which is the combine embeddings setting from Table 3.",7 Monolingual Word Similarity,[0],[0]
"Since the evaluation involves de and en word similarity, we train the CLWE for en-de pair.",7 Monolingual Word Similarity,[0],[0]
Table 4 shows the performance of our combined model compared with several baselines.,7 Monolingual Word Similarity,[0.9572505503253621],['Table 3 shows the performance of conversation disentanglement.']
"Our combined model out-performed both Luong et al. (2015) and Gouws and Søgaard (2015)6 which represent the best published crosslingual embeddings trained on bitext and monolingual data respectively.
",7 Monolingual Word Similarity,[0],[0]
"We also compare our system with the monolingual CBOW model trained on the monolingual data for each language, using the same parameter settings from earlier (§6).",7 Monolingual Word Similarity,[0],[0]
"Surprisingly, our combined model performs better than the monolingual CBOW baseline which makes our result close to the monolingual state-of-the-art on each different dataset.",7 Monolingual Word Similarity,[0],[0]
"However, the best monolingual methods use much larger
6trained using the Panlex dictionary
monolingual corpora (Shazeer et al., 2016), WordNet or the output of commercial search engines (Yih and Qazvinian, 2012).
",7 Monolingual Word Similarity,[0],[0]
Next we explain the gain of our combined model compared with the monolingual CBOW model.,7 Monolingual Word Similarity,[0],[0]
"First, we compare the combined model with the joint-model with respect to monolingual CBOW model (Table 4).",7 Monolingual Word Similarity,[0],[0]
"It shows that the improvement seems mostly come from combining V and U. If we apply the combining algorithm to the monolingual CBOW model (CBOW + combine), we also ob-
serve an improvement.",7 Monolingual Word Similarity,[0],[0]
"Clearly most of the improvement is from combining V and U, however our V and U are more complementary as the gain is more marked.",7 Monolingual Word Similarity,[0],[0]
"Other improvements can be explained by the observation that a dictionary can improve monolingual accuracy through linking synonyms (Faruqui and Dyer, 2014).",7 Monolingual Word Similarity,[0],[0]
"For example, since plane, airplane and aircraft have the same Italian translation aereo, the model will encourage those words to be closer in the embedding space.",7 Monolingual Word Similarity,[0],[0]
Combining context embeddings and word embeddings results in an improvement in both monolingual similarity and bilingual lexicon induction.,8 Model selection,[0],[0]
"In §4.3, we introduce several combination methods including post-processing (interpolation and concatenation) and during training (regularization).",8 Model selection,[0],[0]
"In this section, we justify our parameter and model choices.
",8 Model selection,[0],[0]
"We use en-it pair for tuning purposes, considering the value of γ in equation 4.",8 Model selection,[0],[0]
Figure 2 shows the performances using different values of γ.,8 Model selection,[0],[0]
The two extremes where γ = 0 and γ = 1 corresponds to no interpolation where we just use U or V respectively.,8 Model selection,[0],[0]
"As γ increases, the performance on WSen increases yet BLI decreases.",8 Model selection,[0],[0]
These results confirm our hypothesis in §4.3 that U is better at capturing bilingual relations and V is better at capturing monolingual relations.,8 Model selection,[0],[0]
"As a compromise, we choose γ = 0.5 in our experiments.",8 Model selection,[0],[0]
"Similarly, we tune the regularization sensitivity δ in equation (5) which combines embeddings space during training.",8 Model selection,[0],[0]
"We test δ = 10−n with n = {0, 1, 2, 3, 4} and us-
ing V, U or the interpolation of both V+U2 as the learned embeddings, evaluated on the same BLI and WS-en.",8 Model selection,[0],[0]
"We select δ = 0.01.
",8 Model selection,[0],[0]
Table 5 shows the performance with and without using combining algorithms mentioned in §4.3.,8 Model selection,[0],[0]
"As the compromise between both monolingual and crosslingual tasks, we choose regularization + U as the combination algorithm.",8 Model selection,[0],[0]
"All in all, we apply the regularization algorithm for combining V and U with δ = 0.01 and U as the output for all language pairs without further tuning.",8 Model selection,[0],[0]
"In this section, we evaluate our CLWE on a downstream crosslingual document classification (CLDC)
task.",9 Crosslingual Document Classification,[0],[0]
"In this task, the document classifier is trained on a source language and then applied directly to classify a document in the target language.",9 Crosslingual Document Classification,[0],[0]
This is convenient for a target low-resource language where we do not have document annotations.,9 Crosslingual Document Classification,[0],[0]
"The experimental setup is the same as Klementiev et al. (2012)7 with the training and testing data sourced from Reuter RCV1/RCV2 corpus (Lewis et al., 2004).
",9 Crosslingual Document Classification,[0],[0]
The documents are represented as the bag of word embeddings weighted by tf.idf.,9 Crosslingual Document Classification,[0],[0]
A multi-class classifier is trained using the average perceptron algorithm on 1000 documents in the source language and tested on 5000 documents in the target language.,9 Crosslingual Document Classification,[0],[0]
"We use the CLWE, such that the document representation in the target language embeddings is in the same space with the source language.
",9 Crosslingual Document Classification,[0],[0]
We build the en-de CLWE using combined models as described in section §4.,9 Crosslingual Document Classification,[0],[0]
"Following prior work, we also use monolingual data8 from the RCV1/RCV2 corpus (Klementiev et al., 2012; Gouws et al., 2015; Chandar A P et al., 2014).
",9 Crosslingual Document Classification,[0],[0]
Table 6 shows the CLDC results for various CLWE.,9 Crosslingual Document Classification,[0],[0]
"Despite its simplicity, our model achieves competitive performance.",9 Crosslingual Document Classification,[0],[0]
"Note that aside from our model, all other models in Table 6 use a large bitext (Europarl) which may not exist for many lowresource languages, limiting their applicability.
7The data split and code are kindly provided by the authors.",9 Crosslingual Document Classification,[0],[0]
8We randomly sample documents in RCV1 and RCV2 corpora and selected around 85k documents to form 400k monolingual sentences for both en and de.,9 Crosslingual Document Classification,[0],[0]
"For each document, we perform basic pre-processing including: lower-casing, remove html tags and tokenization.",9 Crosslingual Document Classification,[0],[0]
These monolingual data are then concatenated with the monolingual data from Wikipedia to form the final training data.,9 Crosslingual Document Classification,[0],[0]
"Our model exploits dictionaries, which are more widely available than parallel corpora.",10 Low-resource languages,[0],[0]
"However the question remains as to how well this performs of a real low-resource language, rather than a simulated condition like above, whereupon the quality of the dictionary is likely to be worse.",10 Low-resource languages,[0],[0]
"To test this, we evaluation on Serbian, a language with few annotated language resources.",10 Low-resource languages,[0],[0]
Table 1 shows the relative size of monolingual data and dictionary for en-sr compared with other language pairs.,10 Low-resource languages,[0],[0]
Both the Serbian monolingual data and the dictionary size is more than 10 times smaller than other language pairs.,10 Low-resource languages,[0],[0]
We build the en-sr CLWE using our best model (joint + combine) and evaluate on the bilingual word induction task using 939 gold translation pairs.9 We achieved recall score of 35.8% and 45.5% at 1 and 5 respectively.,10 Low-resource languages,[0],[0]
"Although worse than the earlier results, these numbers are still well above chance.
",10 Low-resource languages,[0],[0]
We can also simulate low-resource setting using our earlier datasets.,10 Low-resource languages,[0],[0]
"For estimating the performance loss on all three tasks, we down sample the dictionary for en-it and en-de based on en word frequency.",10 Low-resource languages,[0],[0]
Figure 3 shows the performance with different dictionary sizes for all three tasks.,10 Low-resource languages,[0],[0]
The monolingual similarity performance is very similar across various sizes.,10 Low-resource languages,[0],[0]
"For BLI and CLDC, dictionary size is more important, although performance levels off at around 80k dictionary pairs.",10 Low-resource languages,[0],[0]
"We conclude that this size is sufficient for decent performance.
",10 Low-resource languages,[0],[0]
"9The sr→en translations are sourced from Google Translate by translating one word at a time, followed by manually verification, after which 61 translation pairs were ruled out as being bad or questionable.",10 Low-resource languages,[0],[0]
Previous CLWE methods often impose high resource requirements yet have low accuracy.,11 Conclusion,[0],[0]
We introduce a simple framework based on a large noisy dictionary.,11 Conclusion,[0],[0]
We model polysemy using EM translation selection during training to learn bilingual correspondences from monolingual corpora.,11 Conclusion,[0],[0]
"Our algorithm allows to train on massive amount of monolingual data efficiently, representing monolingual and bilingual properties of language.",11 Conclusion,[0],[0]
"This allows us to achieve state-of-the-art performance on bilingual lexicon induction task, competitive result on monolingual word similarity and crosslingual document classification task.",11 Conclusion,[0],[0]
"Our combination techniques during training, especially using regularization, are highly effective and could be used to improve monolingual word embeddings.",11 Conclusion,[0],[0]
This work was conducted during Duong’s internship at IBM Research – Tokyo and partially supported by the University of Melbourne and National ICT Australia (NICTA).,Acknowledgments,[0],[0]
"We are grateful for support from NSF Award 1464553 and the DARPA/I2O, Contract No. HR0011-15-C-0114.",Acknowledgments,[0],[0]
"We thank Yuta Tsuboi and Alvin Grissom II for helpful discussions, Jan Šnajder for helping with sr-en evaluation.",Acknowledgments,[0],[0]
"Crosslingual word embeddings represent lexical items from different languages in the same vector space, enabling transfer of NLP tools.",abstractText,[0],[0]
"However, previous attempts had expensive resource requirements, difficulty incorporating monolingual data or were unable to handle polysemy.",abstractText,[0],[0]
We address these drawbacks in our method which takes advantage of a high coverage dictionary in an EM style training algorithm over monolingual corpora in two languages.,abstractText,[0],[0]
"Our model achieves state-of-theart performance on bilingual lexicon induction task exceeding models using large bilingual corpora, and competitive results on the monolingual word similarity and cross-lingual document classification task.",abstractText,[0],[0]
Learning Crosslingual Word Embeddings without Bilingual Corpora,title,[0],[0]
"Deep neural networks (DNNs) have improved performances of many applications, as the non-linearity of DNNs provides expressive modeling capacity, but it also makes DNNs difficult to train and easy to overfit the training data.
",1. Introduction,[0],[0]
"Whitened neural network (WNN) (Desjardins et al., 2015), a recent advanced deep architecture, is ideally to solve the above difficulties.",1. Introduction,[0],[0]
"WNN extends batch normalization (BN) (Ioffe & Szegedy, 2015) by normalizing the internal hidden representation using whitening transformation instead of standardization.",1. Introduction,[0],[0]
"Whitening helps regularize each diagonal block of the Fisher Information Matrix (FIM) to be an
1Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China 2Multimedia Laboratory, The Chinese University of Hong Kong, Hong Kong.",1. Introduction,[0],[0]
"Correspondence to: Ping Luo <pluo@ie.cuhk.edu.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
approximation of the identity matrix.,1. Introduction,[0],[0]
"This is an appealing property, as training WNN using stochastic gradient descent (SGD) mimics the fast convergence of natural gradient descent (NGD) (Amari & Nagaoka, 2000).",1. Introduction,[0],[0]
The whitening transformation also improves generalization.,1. Introduction,[0],[0]
"As demonstrated in (Desjardins et al., 2015), WNN exhibited superiority when being applied to various network architectures, such as autoencoder and convolutional neural network, outperforming many previous works including SGD, RMSprop (Tieleman & Hinton, 2012), and BN.
",1. Introduction,[0],[0]
"Although WNN is able to reduce the number of training iterations and improve generalization, it comes with a price of increasing training time, because eigen-decomposition occupies large computations.",1. Introduction,[0],[0]
The runtime scales up when the number of hidden layers that require whitening transformation increases.,1. Introduction,[0],[0]
"We revisit WNN by breaking down its performance and show that its main runtime comes from two aspects, 1) computing full covariance matrix for whitening and 2) solving singular value decomposition (SVD).",1. Introduction,[0],[0]
"Previous work (Desjardins et al., 2015) suggests to overcome these problems by a) using a subset of training data to estimate the full covariance matrix and b) solving the SVD every hundreds or thousands of training iterations.",1. Introduction,[0],[0]
"Both of them rely on the assumption that the SVD holds in this period, but it is generally not true.",1. Introduction,[0],[0]
"When this period becomes large, WNN degenerates to canonical SGD due to ill conditioning of FIM.
",1. Introduction,[0],[0]
"We propose generalized WNN (GWNN), which possesses the beneficial properties of WNN, but significantly reduces its runtime and improves its generalization.",1. Introduction,[0],[0]
"We introduce two variants of GWNN, including pre-whitening and postwhitening GWNNs.",1. Introduction,[0],[0]
"The former one whitens a hidden layer’s input values, whilst the latter one whitens the preactivation values (hidden features).",1. Introduction,[0],[0]
GWNN has three appealing characteristics.,1. Introduction,[0],[0]
"First, compared to WNN, GWNN is capable of learning more compact hidden representation, such that the SVD can be approximated by a few top eigenvectors to reduce computation.",1. Introduction,[0],[0]
This compact representation also improves generalization.,1. Introduction,[0],[0]
"Second, it enables the whitening transformation to be performed in a short period, maintaining conditioning of FIM.",1. Introduction,[0],[0]
"Third, by exploiting knowledge of the distribution of the hidden features, we calculate the covariance matrix in an analytical form to further improve computational efficiency.",1. Introduction,[0],[0]
We begin by defining the basic notation for feed-forward neural network.,2. Notation and Background,[0],[0]
A neural network transforms an input vector o0 to an output vector o` through a series of ` hidden layers {oi}`i=1.,2. Notation and Background,[0],[0]
We assume each layer has identical dimension for the simplicity of notation i.e. ∀oi ∈ Rd×1.,2. Notation and Background,[0],[0]
"In this case, all vectors and matrixes in the following should have d rows unless otherwise stated.",2. Notation and Background,[0],[0]
"As shown in Fig.1 (a), each fully-connected (fc) layer consists of a weight matrix, W i, and a set of hidden neurons, hi, each of which receives as input a weighted sum of outputs from the previous layer.",2. Notation and Background,[0],[0]
We have hi = W ioi−1.,2. Notation and Background,[0],[0]
"In this work, we take fully-connected network as an example.",2. Notation and Background,[0],[0]
"Note that the above computation can be also applied to a convolutional network, where an image patch is vectorized as a column vector and represented by oi−1 and each row of W i represents a filter.
",2. Notation and Background,[0],[0]
"As the recent deep architectures typically stack a batch normalization (BN) layer before the pre-activation values, we do not explicitly include a bias term when computing hi, because it is normalized in BN, such that φi = hi−E[hi]√
Var[hi] , where the expectation and variance are computed over a minibatch of samples.",2. Notation and Background,[0],[0]
LeCun et al. (2002) showed that such normalization speeds up convergence even when the hidden features are not decorrelated.,2. Notation and Background,[0],[0]
"Furthermore, output of each layer is calculated by a nonlinear activation function.",2. Notation and Background,[0],[0]
"A popular choice is the rectified linear unit, relu(x) = max(0, x).",2. Notation and Background,[0],[0]
"The precise computation for an output is oi = max(0,diag(αi)φi + βi), where diag(x) represents a matrix whose diagonal entries are x. αi and βi are two vectors that scale and shift the normalized features, in order to maintain the network’s representation capacity.",2. Notation and Background,[0],[0]
This section revisits whitened neural networks (WNN).,2.1. Whitened Neural Networks,[0],[0]
Any neural architecture can be adapted to a WNN by stacking a whitening transformation layer after the layer’s input.,2.1. Whitened Neural Networks,[0],[0]
"For example, Fig.1 (b) adapts a fc layer as shown in
(a) into a whitened fc layer.",2.1. Whitened Neural Networks,[0],[0]
"Its information flow becomes
õi−1 = P i−1(oi−1 − µi−1), ĥi = Ŵ iõi−1, (1)
",2.1. Whitened Neural Networks,[0],[0]
"φi = ĥ i√
Var[ĥi] , oi = max(0,diag(αi)φi + βi),
where µi−1 represents a centering variable, µi−1 = E[oi−1].",2.1. Whitened Neural Networks,[0],[0]
"P i−1 is a whitening matrix whose rows are obtained from eigen-decomposition of Σi−1, which is the covariance matrix of the input, Σi−1 = E[(oi−1 − µi−1)(oi−1 − µi−1)T].",2.1. Whitened Neural Networks,[0],[0]
"The input is decorrelated by P i−1 in the sense that its covariance matrix becomes an identity matrix, i.e. E[õi−1õi−1T ] = I .",2.1. Whitened Neural Networks,[0],[0]
"To avoid ambiguity, we use ‘ˆ’ to distinguish the variables in WNN and the canonical fc layer whenever necessary.",2.1. Whitened Neural Networks,[0],[0]
"For instance, Ŵ i represents a whitened weight matrix.",2.1. Whitened Neural Networks,[0],[0]
"In Eqn.(1), computation of the BN layer has been simplified because we have E[ĥi] = Ŵ iP i−1(E[oi−1]− µi−1) = 0.
",2.1. Whitened Neural Networks,[0],[0]
"We define θ to be a vector consisting of all the whitened weight matrixes concatenated together, θ = {vec(Ŵ 1)T, vec(Ŵ 2)T, ..., vec(Ŵ `)T}, where vec(·) is an operator that vectorizes a matrix by stacking its columns.",2.1. Whitened Neural Networks,[0],[0]
"Let L(o`, y; θ) denote a loss function of WNN, which measures the disagreement between a prediction o` made by the network, and a target y. WNN is trained by minimizing the loss function with respect to the parameter vector θ and two constraints
min θ
L(o`, y; θ) (2)
s.t. E[õi−1õi−1T ]",2.1. Whitened Neural Networks,[0],[0]
"= I, hi − E[hi] = ĥi, i = 1...`.
To satisfy the first constraint, P i−1 is obtained by decomposing the covariance matrix, Σi−1 = U i−1Si−1U i−1T. We choose P i−1 =",2.1. Whitened Neural Networks,[0],[0]
"(Si−1)− 1 2U i−1
T, where Si−1 is a diagonal matrix whose diagonal elements are eigenvalues and U i−1 is an orthogonal matrix of eigenvectors.",2.1. Whitened Neural Networks,[0],[0]
"The first constraint holds under the construction of eigendecomposition.
",2.1. Whitened Neural Networks,[0],[0]
"The second constraint, hi − E[hi] = ĥi, enforces that the centered hidden features are the same, before and after
adapting a fc layer to WNN, as shown in Fig.1 (a) and (b).",2.1. Whitened Neural Networks,[0],[0]
"In other words, it ensures that their representation powers are identical.",2.1. Whitened Neural Networks,[0],[0]
"By combing the computations in Fig.1 (a) and Eqn.(1), the second constraint implies that ‖(hi−E[hi])− ĥi‖22",2.1. Whitened Neural Networks,[0],[0]
= ‖(W ioi−1,2.1. Whitened Neural Networks,[0],[0]
− W iµi−1),2.1. Whitened Neural Networks,[0],[0]
"− Ŵ iõi−1‖22 = 0, which has a closed-form solution, Ŵ i = W i(P i−1)−1.",2.1. Whitened Neural Networks,[0],[0]
"To see this, we have ĥi = W i(P i−1)−1P",2.1. Whitened Neural Networks,[0],[0]
i−1(oi−1 − µi−1) = W i(oi−1−µi−1) = hi−E[hi].,2.1. Whitened Neural Networks,[0],[0]
"The representation capacity can be preserved by mapping the whitened weight matrix from the ordinary weight matrix.
",2.1. Whitened Neural Networks,[0],[0]
Conditioning of the FIM.,2.1. Whitened Neural Networks,[0],[0]
"Here we show that WNN improves training efficiency by conditioning the Fisher information matrix (FIM) (Amari & Nagaoka, 2000).",2.1. Whitened Neural Networks,[0],[0]
"A FIM, denoted as F , consists of `× ` blocks.",2.1. Whitened Neural Networks,[0],[0]
"Each block is indexed by Fij , representing the covariance (co-adaptation) between the whitened weight matrixes of the i-th and j-th layers.",2.1. Whitened Neural Networks,[0],[0]
"We have Fij = E[vec(δŴ i)vec(δŴ j)T], where δŴ i indicates the gradient of the i-th whitened weight matrix.",2.1. Whitened Neural Networks,[0],[0]
"For example, the gradient of Ŵ i is achieved by õi−1(δĥi)T, as illustrated in Eqn.(1).",2.1. Whitened Neural Networks,[0],[0]
"We have vec(δŴ i) = vec(õi−1(δĥi)T) = δĥi ⊗ õi−1, where ⊗ denotes the Kronecker product.",2.1. Whitened Neural Networks,[0],[0]
"In this case, Fij can be rewritten as E[(δĥi ⊗ õi−1)(δĥj ⊗ õj−1)T] = E[δĥi(δĥj)T ⊗ õi−1(õj−1)T].",2.1. Whitened Neural Networks,[0],[0]
"By assuming δĥ and õ are independent as demonstrated in (Raiko et al., 2012), Fij can be approximated by E[δĥi(δĥj)T] ⊗ E[õi−1(õj−1)T].",2.1. Whitened Neural Networks,[0],[0]
"As a result, when i = j, each diagonal block of F , Fii, has a block diagonal structure because we have E[õi−1(õi−1)T] = I as shown in Eqn.(2), which improves conditioning of FIM and thus speeds up training.",2.1. Whitened Neural Networks,[0],[0]
"In general, WNN regularizes the diagonal blocks of FIM and achieves stronger conditioning than those methods (LeCun et al., 2002; Tieleman & Hinton, 2012) that regularized the diagonal entries.
",2.1. Whitened Neural Networks,[0],[0]
Training WNN.,2.1. Whitened Neural Networks,[0],[0]
Alg.1 summarizes training of WNN.,2.1. Whitened Neural Networks,[0],[0]
"At the 1st line, the whitened weight matrix Ŵ i0 is initialized by W i of the ordinary fc layer, which can be pretrained or sampled from a Gaussian distribution.",2.1. Whitened Neural Networks,[0],[0]
The 4th line shows that Ŵ it is updated in each iteration t using SGD.,2.1. Whitened Neural Networks,[0],[0]
The first and second constraints are achieved in the 7th and 8th lines respectively.,2.1. Whitened Neural Networks,[0],[0]
"For example, the 8th line ensures that the hidden features are the same before and after updating the whitening matrix.",2.1. Whitened Neural Networks,[0],[0]
"As the distribution of the hidden representation changes after every update of the whitened weight matrix, to maintain good conditioning of FIM, the whitening matrix, P i−1, needs to be reconstructed frequently by performing eigen-decomposition on Σi−1, which is estimated using N samples.",2.1. Whitened Neural Networks,[0],[0]
N is typically 104 in experiments.,2.1. Whitened Neural Networks,[0],[0]
"However, this raw strategy increases computation time.",2.1. Whitened Neural Networks,[0],[0]
"Desjardins et al. (2015) performed whitening in every τ iterations as shown in the 5th line of Alg.1 to reduce computations, e.g. τ = 103.
",2.1. Whitened Neural Networks,[0],[0]
"How good is the conditioning of the FIM by using Al-
Algorithm 1 Training WNN 1: Init: initial network parameters θ, αi, βi; whitening matrix P i−1 = I; iteration t = 0; Ŵ it = W i; ∀i ∈ {1...`}.
g.1?",2.1. Whitened Neural Networks,[0],[0]
"We measure the similarity of the covariance matrix, E[õi−1(õi−1)T], with the identity matrix I .",2.1. Whitened Neural Networks,[0],[0]
This is called the orthogonality.,2.1. Whitened Neural Networks,[0],[0]
We employ Pearson’s correlation1 as the similarity between two matrixes.,2.1. Whitened Neural Networks,[0],[0]
"Intuitively, this measure has a value between −1 and +1, representing negative and positive correlations.",2.1. Whitened Neural Networks,[0],[0]
Larger values indicate higher orthogonality.,2.1. Whitened Neural Networks,[0],[0]
"Fig.2 visualizes four randomly generated covariance matrixes, where (a,b) are sampled from a uniform distribution between 0 and 1.",2.1. Whitened Neural Networks,[0],[0]
"Fig.2 (c,d) are generated by truncating different numbers of columns of a randomly generated orthogonal matrix.",2.1. Whitened Neural Networks,[0],[0]
"For instance, (a,b) have small similarity with respect to the identity matrix.",2.1. Whitened Neural Networks,[0],[0]
"In contrast, when the correlation equals 0.65 as shown in (c), all entries in the diagonal are larger than 0.9 and more than 80% off-diagonal entries have values smaller than 0.1.",2.1. Whitened Neural Networks,[0],[0]
"Furthermore, Pearson’s correlation is insensitive to the size of matrix, such that orthogonality of different layers can be compared together.",2.1. Whitened Neural Networks,[0],[0]
"For example, although matrixes in
1Given an identity matrix, I , and a covariance matrix, Σ, the Pearson’s correlation between them is defined as corr(Σ, I) = vec(Σ)Tvec(I)√ vec(Σ)Tvec(Σ)·vec(I)Tvec(I) , where vec(Σ) is a normalized vector by subtracting mean of all entries.
",2.1. Whitened Neural Networks,[0],[0]
"(a) and (b) have different sizes, they have similar value of orthogonality when they are sampled from the same distribution.
",2.1. Whitened Neural Networks,[0],[0]
"As shown in Fig.3 (a), we adopt network-in-network (NIN) (Lin et al., 2014) that is trained on CIFAR-10 (Krizhevsky, 2009), and plot the orthogonalities of three different convolutional layers, which are whitened every τ",2.1. Whitened Neural Networks,[0],[0]
= 103 iterations by using Alg.1.,2.1. Whitened Neural Networks,[0],[0]
"We see that orthogonality values during training have large fluctuations except those of the first convolutional layer, abbreviated as ‘conv1’.",2.1. Whitened Neural Networks,[0],[0]
"This is because the distributions of deeper layers’ inputs change after the whitened weight matrixes have been updated, leading to ill-conditions of the whitening matrixes, which are estimated in a large interval.",2.1. Whitened Neural Networks,[0],[0]
"In fact, large τ will degenerate WNN to canonical SGD.",2.1. Whitened Neural Networks,[0],[0]
"However, ‘conv1’ uses image data as inputs, whose distribution is typically stable during training.",2.1. Whitened Neural Networks,[0],[0]
"Its whitening matrix can be estimated once at the beginning and fixed in the entire training stage.
",2.1. Whitened Neural Networks,[0],[0]
"In the section below, we present generalized whitened neural networks to improve conditioning of FIM while reducing computation time.",2.1. Whitened Neural Networks,[0],[0]
"We present two types of generalized WNN (GWNN), including pre-whitening and post-whitening GWNNs.",3. Generalized Whitened Neural Networks,[0],[0]
"Both models share beneficial properties of WNN, but have lower computation time.",3. Generalized Whitened Neural Networks,[0],[0]
"This section introduces pre-whitening GWNN, abbreviated as pre-GWNN, which performs whitening transformation before applying the weight matrix (i.e. whiten the input),
as illustrated in Fig.1 (c).",3.1. Pre-whitening GWNN,[0],[0]
"When adapting a fc layer to pre-GWNN, the whitening matrix is truncated by removing those eigenvectors that have small eigenvalues, in order to learn compact representation.",3.1. Pre-whitening GWNN,[0],[0]
"This allows the input vector to vary its length, so as to gradually adapt the learned representation to informative patterns with high variations, but not noises.",3.1. Pre-whitening GWNN,[0],[0]
"Learning pre-GWNN is formulated analogously to learning WNN in Eqn.(2), but with one additional constraint truncated the rank of the whitening matrix,
min θ
L(o`, y; θ) (3)
s.t. rank(P i−1) ≤ d′, E[õi−1d′ õ i−1T d′ ]",3.1. Pre-whitening GWNN,[0],[0]
"= I,
hi − E[hi] = ĥi, i = 1...`.
Let d be the dimension of the original fc layer.",3.1. Pre-whitening GWNN,[0],[0]
"By combining Eqn.(2), we have P i−1 =",3.1. Pre-whitening GWNN,[0],[0]
"(Si−1)− 1 2U i−1
T ∈ Rd×d, which is truncated by using P i−1d′ = (S i−1 d′ )",3.1. Pre-whitening GWNN,[0],[0]
"− 12U i−1d′ T ∈ Rd′×d, where Si−1d′ is achieved by keeping rows and columns associated with the first d′ large eigenvalues, whilst U i−1d′ contains the corresponding d
′ eigenvectors.",3.1. Pre-whitening GWNN,[0],[0]
The value of d′ can be tuned using a validation set.,3.1. Pre-whitening GWNN,[0],[0]
"For simplicity, we choose d′ = d2 , which works well throughout our experiments.",3.1. Pre-whitening GWNN,[0],[0]
"This is inspired by the finding in (Zhang et al., 2015), who disclosed that the first 50% eigenvectors contribute over 95% energy in a deep model.
",3.1. Pre-whitening GWNN,[0],[0]
"More specifically, pre-GWNN first projects an input vector to a d′ low-dimensional space, õi−1d′ = P i−1 d′",3.1. Pre-whitening GWNN,[0],[0]
"(o
i−1 − µi−1) ∈ Rd′×1.",3.1. Pre-whitening GWNN,[0],[0]
"The whitened weight matrix then produces a hidden feature vector of d dimensions, which has the same length as the ordinary fc layer, i.e. ĥi = Ŵ iõi−1d′ ∈ Rd×1, where Ŵ i = W i(P i−1 d′ )
−1",3.1. Pre-whitening GWNN,[0],[0]
∈ Rd×d′ .,3.1. Pre-whitening GWNN,[0],[0]
"The computations of BN and the nonlinear activation are identical to Eqn.(1).
",3.1. Pre-whitening GWNN,[0],[0]
Training pre-GWNN is similar to Alg.1.,3.1. Pre-whitening GWNN,[0],[0]
The main modification is produced at the 7th line in order to reduce runtime.,3.1. Pre-whitening GWNN,[0],[0]
"Although Alg.1 decreases number of iterations when training converged, each iteration has additional computation time for eigen-decomposition.",3.1. Pre-whitening GWNN,[0],[0]
"For example, in WNN, the required computation of full singular value decomposition (SVD) is typically O(Nd2), where N represents the number of samples employed to estimate the covariance matrix.",3.1. Pre-whitening GWNN,[0],[0]
"In particular, when we have ` whitened layers and T is the number of iterations, all whitening transformations occupy O(Nd
2T` τ ) runtime in
the entire training stage.",3.1. Pre-whitening GWNN,[0],[0]
"In contrast, pre-GWNN performs the popular online estimation for the top d′ eigenvectors in P i−1d′ such as online SVD (Shamir, 2015; Povey et al., 2015), instead of using full SVD as WNN did.",3.1. Pre-whitening GWNN,[0],[0]
"This difference reduces runtime to O( (N+M)d
′T` τ ′ ), where τ ′
represents the whitening interval in GWNN and M is the number of samples used to estimate the top eigenvectors.",3.1. Pre-whitening GWNN,[0],[0]
"We have M = N as employed in previous works.
",3.1. Pre-whitening GWNN,[0],[0]
"For pre-GWNN, reducing runtime and improving conditioning is a tradeoff, since the former requires to increase τ ′",3.1. Pre-whitening GWNN,[0],[0]
but the latter requires to decrease it.,3.1. Pre-whitening GWNN,[0],[0]
"When M = N and d′ = d2 , we compare the runtime complexity of preGWNN to that of WNN, and obtain a ratio of dτ ′
τ , which tells us that whitening can be performed in a short interval without increasing runtime.",3.1. Pre-whitening GWNN,[0],[0]
"For instance, as shown in Fig.3 (b) when τ ′",3.1. Pre-whitening GWNN,[0],[0]
"= 20, orthogonalities are well preserved and more stable than those in (a).",3.1. Pre-whitening GWNN,[0],[0]
"In this case, preGWNN reduces computations of WNN by at least 20× when d > τ , which is a typical choice in recent deep architectures (Krizhevsky et al., 2012; Lin et al., 2014) where d ∈ {1024, 2048, 4096}.",3.1. Pre-whitening GWNN,[0],[0]
"Another variant we propose is post-whitening GWNN, abbreviated as post-GWNN.",3.2. Post-whitening GWNN,[0],[0]
"Unlike WNN and pre-GWNN, post-GWNN performs whitening transformation after applying the weight matrix (i.e. whiten the feature), as illustrated in Fig.1 (d).",3.2. Post-whitening GWNN,[0],[0]
"In general, post-GWNN reduces runtime to O( (N
′+M)d′T` τ ′ ), where N ′ N .
",3.2. Post-whitening GWNN,[0],[0]
Fig.1 (d) shows how to adapt a fc layer to post-GWNN.,3.2. Post-whitening GWNN,[0],[0]
"Suppose oi−1d′ has been whitened by P i−1 d′ in the previous layer, at the i-th layer we have
ĥi = Ŵ i(oi−1d′ − µ i−1 d′ ), h i d′ = P i d′ ĥ i, (4)
φid′ =",3.2. Post-whitening GWNN,[0],[0]
"hi d′√
Var[hi d′",3.2. Post-whitening GWNN,[0],[0]
],3.2. Post-whitening GWNN,[0],[0]
", oid′ = max(0,diag(α",3.2. Post-whitening GWNN,[0],[0]
i d′)φ i d′,3.2. Post-whitening GWNN,[0],[0]
+ β,3.2. Post-whitening GWNN,[0],[0]
"i d′),
where µi−1d′ = E[o i−1 d′ ].",3.2. Post-whitening GWNN,[0],[0]
"In Eqn.(4), a feature vector ĥi ∈ Rd×1 is first produced by applying a whitened weight matrix on the input, in order to recover the original feature length as the fc layer.",3.2. Post-whitening GWNN,[0],[0]
A whitening matrix then projects ĥi to a decorrelated feature vector hid′ ∈,3.2. Post-whitening GWNN,[0],[0]
"Rd
′×1.",3.2. Post-whitening GWNN,[0],[0]
"We have Ŵ i = W i(P i−1d′ )
−1",3.2. Post-whitening GWNN,[0],[0]
"∈ Rd×d′ , where P i−1d′ = (Si−1d′ ) − 12U i−1d′ T ∈ Rd′×d, and U i−1 and Si−1 contain eigenvectors and eigenvalues of the hidden features at the i− 1-th layer.
",3.2. Post-whitening GWNN,[0],[0]
Conditioning.,3.2. Post-whitening GWNN,[0],[0]
Here we disclose that whitening hidden features also enforces good conditioning of FIM.,3.2. Post-whitening GWNN,[0],[0]
"At this point, we have decorrelated the hidden features by satisfying E[hid′hi T d′ ] = I .",3.2. Post-whitening GWNN,[0],[0]
"Then h i d′ follows a standard multivariate Gaussian distribution, hid′ ∼ N (0, I).",3.2. Post-whitening GWNN,[0],[0]
"As a result, the layer’s output follows a rectified Gaussian distribution, which is uncorrelated as presented in remark 1.",3.2. Post-whitening GWNN,[0],[0]
"In post-GWNN, whitening hidden features of the i−",3.2. Post-whitening GWNN,[0],[0]
1- th layer improves conditioning for the i-th layer.,3.2. Post-whitening GWNN,[0],[0]
"To see this, by following the description in Sec.2.1, the diagonal block of FIM associated with the i-th layer can be written as Fii ≈ E[δĥi(δĥi)T]⊗E[(oi−1d′ −µ i−1 d′ )",3.2. Post-whitening GWNN,[0],[0]
"(o i−1 d′ −µ i−1 d′ )
T], where the parameters have low correlations since Fii has a block diagonal structure.
",3.2. Post-whitening GWNN,[0],[0]
"Algorithm 2 Training post-GWNN 1: Init: initial θ, αi, βi; and t = 0, tw = k, λ = twk ; P
i−1 = I , Ŵ it = W
i, ∀i ∈ {1...`}.",3.2. Post-whitening GWNN,[0],[0]
"2: repeat 3: for i = 1 to ` do 4: update Ŵ it , αit, and βit by SGD. 5: if mod(t, τ) = 0",3.2. Post-whitening GWNN,[0],[0]
then 6: store old P i−1o = P i−1 d′ .,3.2. Post-whitening GWNN,[0],[0]
7: estimate mean and variance of ĥi by a minibatch of N ′ samples or following remark 2 when N ′,3.2. Post-whitening GWNN,[0],[0]
= 1. 8: update P i−1d′ by online SVD.,3.2. Post-whitening GWNN,[0],[0]
"9: transform Ŵ it = Ŵ itP i−1o (P i−1 d′ )
−1.",3.2. Post-whitening GWNN,[0],[0]
10: tw = 1 and λ = twk .,3.2. Post-whitening GWNN,[0],[0]
"11: end if 12: end for 13: t = t+ 1. 14: if tw < k then tw = tw + 1 end if 15: until convergence
Remark 1.",3.2. Post-whitening GWNN,[0],[0]
"Let h ∼ N (0, I) and o = max(0, Ah + b).",3.2. Post-whitening GWNN,[0],[0]
Then E[(oj − E[oj ])(ok − E[ok])],3.2. Post-whitening GWNN,[0],[0]
≈ 0,3.2. Post-whitening GWNN,[0],[0]
"if A is a diagonal matrix, where j, k index any two entries of o and j 6= k.
For remark 1, we have A = diag(αid′) and b = β i d′ .",3.2. Post-whitening GWNN,[0],[0]
It tells us three things.,3.2. Post-whitening GWNN,[0],[0]
"First, by using whitening and BN, covariance of any two different entries of oid′ approaches zero.",3.2. Post-whitening GWNN,[0],[0]
"Second, at the iteration when we construct P id′ , we can estimate the full covariance matrix of ĥi using the mean and variance of oi−1d′ , E[ĥiĥi T
] = Ŵ iE[(oi−1d′ − µi−1d′ )(o i−1 d′",3.2. Post-whitening GWNN,[0],[0]
− µ i−1 d′ ),3.2. Post-whitening GWNN,[0],[0]
"T]Ŵ i T
.",3.2. Post-whitening GWNN,[0],[0]
The mean and variance can be estimated with a minibatch of samples i.e. N ′ N .,3.2. Post-whitening GWNN,[0],[0]
"Third, to the extreme, when N ′",3.2. Post-whitening GWNN,[0],[0]
"= 1, these statistics can still be computed in analytical forms leveraging remark 2.
",3.2. Post-whitening GWNN,[0],[0]
Remark 2.,3.2. Post-whitening GWNN,[0],[0]
"Let a random variable x ∼ N (0, 1) and y = max(0, ax + b).",3.2. Post-whitening GWNN,[0],[0]
"Then E[y] = a√
2π e−
b2
2a2 + b2Ψ(− b√ 2a )
and E[y2] = ab√ 2π e−
b2
2a2 + a 2+b2
2 Ψ(− b√ 2a ), where Ψ(x) = 1− erf(x) and erf(x) is the error function.
",3.2. Post-whitening GWNN,[0],[0]
The above remark derives the mean and variance of a rectified output unit that has shift and scale parameters.,3.2. Post-whitening GWNN,[0],[0]
"It generalizes (Arpit et al., 2016) that presented a special case when a = 1 and b = 0.",3.2. Post-whitening GWNN,[0],[0]
"In that case, we have E[y] = 1√
2π
and Var[y] = E[y2]−E[y]2 = 12− 1 2π , which are consistent with previous work.
Extensions.",3.2. Post-whitening GWNN,[0],[0]
"Remark 1 and 2 can be extended to other nonlinear activation functions, such as leaky rectified unit defined as leakyrelu(x) = max(0, x)+amin(0, x), where the slope of the negative part is controlled by the coefficient a, which is fixed in (Maas et al., 2013) and is learned in (He et al., 2015).",3.2. Post-whitening GWNN,[0],[0]
"Similar to pre-GWNN, the learning problem can be formulated as
min θ
λL(o`, y; θ) + (1− λ) ∑` i=1",3.3. Training post-GWNN,[0],[0]
"L feat(hi, ĥi; θ) (5)
s.t. rank(P i) ≤",3.3. Training post-GWNN,[0],[0]
"d′, E[hid′hi T d′ ]",3.3. Training post-GWNN,[0],[0]
"= I, i = 1...`.
Eqn.(5) has two loss functions.",3.3. Training post-GWNN,[0],[0]
"Different from WNN and pre-GWNN where the feature equality constraint can be satisfied in a closed form, this constraint is treated as an auxiliary loss function in post-GWNN, defined as Lfeat(hi, ĥi) = 12‖(h
i − E[hi])",3.3. Training post-GWNN,[0],[0]
− ĥi‖22 and minimized in the training stage.,3.3. Training post-GWNN,[0],[0]
It does not have an analytical solution because there is a nonlinear activation function between the weight matrix and the whitening matrix (i.e. in the previous layer).,3.3. Training post-GWNN,[0],[0]
"In Eqn.(5), λ is a coefficient that balances the contribution of two loss functions, and 1− λ is linearly decayed as 1− λ = k−twk , where tw = 1, 2, ...,",3.3. Training post-GWNN,[0],[0]
k.,3.3. Training post-GWNN,[0],[0]
"At each time after we update the whitening matrix, we start decay by setting tw = 1",3.3. Training post-GWNN,[0],[0]
"and k indicates the iterations at which we stop annealing.
",3.3. Training post-GWNN,[0],[0]
Alg.2 summarizes the training procedure.,3.3. Training post-GWNN,[0],[0]
It preforms online update of the top d′ eigenvectors of the whitening matrix similar to pre-GWNN.,3.3. Training post-GWNN,[0],[0]
"In comparison, it decreases the runtime of whitening transformation to O( (N
′+M)d′T` τ ′ ),
which is N+MN ′+M fold reduction with respect to pre-GWNN.",3.3. Training post-GWNN,[0],[0]
"For example, when N = M and N ′",3.3. Training post-GWNN,[0],[0]
"= 1, post-GWNN is capable of reducing computations of pre-GWNN and WNN by 2× and (2τ ′)× respectively, while maintaining better conditioning than these alternatives by choosing small τ ′.",3.3. Training post-GWNN,[0],[0]
"We compare WNN, pre-GWNN, and post-GWNN in the following aspects, including a) number of iterations when training converged, b) computation times for training, and c) generalization capacities on various datasets.",4. Empirical Studies,[0],[0]
We also conduct ablation studies with respect to 1) effect of the number of samples N to estimate the covariance matrix for pre-GWNN and 2) effect of N ′ for post-GWNN.,4. Empirical Studies,[0],[0]
"Finally, we try to tune the value of d′.
Datasets.",4. Empirical Studies,[0],[0]
We employ the following datasets.,4. Empirical Studies,[0],[0]
"a) MNIST (Lecun et al., 1998) has 60, 000 28× 28 images of 10 handwritten digits (0-9) for training and another 10, 000 test images.",4. Empirical Studies,[0],[0]
"5, 000 images from the training set are randomly selected as a validation set.",4. Empirical Studies,[0],[0]
"b) CIFAR-10 (Krizhevsky, 2009) consists of 50, 000 32 × 32 color images for training and 10, 000 images for testing.",4. Empirical Studies,[0],[0]
Each image is categorized into one of the 10 object labels.,4. Empirical Studies,[0],[0]
"For CIFAR-10, 5, 000 images are chosen for validation.",4. Empirical Studies,[0],[0]
c) CIFAR-100,4. Empirical Studies,[0],[0]
"(Krizhevsky, 2009) has the same number of
images as CIFAR-10, but each image is classified into 100 categories.",4. Empirical Studies,[0],[0]
"For CIFAR-100, we select 5, 000 images from training set for validation.",4. Empirical Studies,[0],[0]
"d) SVHN (Netzer et al., 2011) consists of color images of house numbers collected by Google Street View.",4. Empirical Studies,[0],[0]
"The task is to predict the center digit (0-9) of each image, which is of size 32×32.",4. Empirical Studies,[0],[0]
"There are 73, 257 images in the training set, 26, 032 images for test, and 531, 131 additional examples.",4. Empirical Studies,[0],[0]
"We follow (Sermanet et al., 2012) to build a validation set by selecting 400 samples per class from the training set and 200 samples per class from the additional set.",4. Empirical Studies,[0],[0]
"We didn’t train on validation, which is for tuning hyperparameters.
",4. Empirical Studies,[0],[0]
Experimental Settings.,4. Empirical Studies,[0],[0]
"We have two settings, an unsupervised and a supervised learning settings.",4. Empirical Studies,[0],[0]
"First, following (Desjardins et al., 2015), we compare the above three approaches on the task of minimizing reconstruction error of an autoencoder on MNIST.",4. Empirical Studies,[0],[0]
"The encoder consists of 4 fc sigmoidal layers, which have 1000, 500, 256, and 30 hidden neurons respectively.",4. Empirical Studies,[0],[0]
The decoder is symmetric and untied with respect to the encoder.,4. Empirical Studies,[0],[0]
"Second, for the task of image classification on CIFAR-10, -100, and SVHN, we employ the same network-in-network (NIN) (Lin et al., 2014) architecture, which has 9 convolutional layers and 3 pooling layers defined as2: conv(192, 5)-conv(160, 1)-maxpool(3, 2)-conv(96, 1)- conv(192, 5)-conv(192, 1)-avgpool(3, 2)-conv(192, 1)- conv(192, 5)-conv(192, 1)-conv(l, 1)-avgpool(8, 8), where l = 10 for CIFAR-10 and SVHN and l = 100 for CIFAR-100.",4. Empirical Studies,[0],[0]
"For all models, we use SGD with momentum of 0.9.",4. Empirical Studies,[0],[0]
"We record the number of epochs and computation time, when training WNN, pre-, and post-GWNN on MNIST and CIFAR-100, respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
We employ the first setting above for MNIST and the second setting for CIFAR100.,4.1. Comparisons of Convergence and Computations,[0],[0]
"For both settings, hyperparamters are chosen by grid search on the validation sets.",4.1. Comparisons of Convergence and Computations,[0],[0]
"The search specifications of minibatch size, learning rate, and whitening interval τ are {64, 128, 256}, {0.1, 0.01, 0.001}, and {20, 50, 100, 103}, respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
"In particular, for WNN and pre-GWNN, the number of samples used to estimate the covariance matrix, N , is picked up from {103, 10 4
2 , 10 4}.",4.1. Comparisons of Convergence and Computations,[0],[0]
"For post-GWNN,
N ′ is chosen to be the same as the minibatch size and the decay period k = 0.1τ .",4.1. Comparisons of Convergence and Computations,[0],[0]
"For a fair comparison, we report the best performance on validation set for each approach, and didn’t employ any data augmentation such as random image cropping and flipping.
2The ‘conv’, ‘maxpool’, and ‘avgpool’ represent convolution, max pooling, and average pooling respectively.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Each convolutional layer is defined as conv(number of filters, filter size).",4.1. Comparisons of Convergence and Computations,[0],[0]
"For each pooling layer, we have pool(kernel size, stride).",4.1. Comparisons of Convergence and Computations,[0],[0]
"All convolutions have stride 1.
",4.1. Comparisons of Convergence and Computations,[0],[0]
The convergence and computation time are reported in Fig.4 (a-d).,4.1. Comparisons of Convergence and Computations,[0],[0]
We have several important observations.,4.1. Comparisons of Convergence and Computations,[0],[0]
"First, all three approaches converge much faster than the canonical network trained by SGD.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Second, pre- and postGWNN achieve better convergence than WNN on both datasets as shown in (a) and (c).",4.1. Comparisons of Convergence and Computations,[0],[0]
"Moreover, post-GWNN outperforms pre-GWNN.",4.1. Comparisons of Convergence and Computations,[0],[0]
"Third, post-GWNN significantly reduces computation time compared to all the other methods, as illustrated in (b) and (d).",4.1. Comparisons of Convergence and Computations,[0],[0]
"We see that although WNN reduces the number of epochs, it takes long time to train because its whitening transformation occupies large computations.",4.1. Comparisons of Convergence and Computations,[0],[0]
"We evaluate WNN, pre-, and post-GWNN on CIFAR-10, -100, and SVHN datasets, and compare their classification accuracies to existing state-of-the-art methods.",4.2. Performances on various Datasets,[0],[0]
"For all the datasets and approaches, we utilize the same network structure as mentioned in the second setting above.",4.2. Performances on various Datasets,[0],[0]
"For two CIFAR datasets, we adopt minibatch size 64 and initial learning rate 0.1, which is reduced by half after every 25 epochs.",4.2. Performances on various Datasets,[0],[0]
We train for 250 epochs.,4.2. Performances on various Datasets,[0],[0]
"As SVHN is a large dataset, we train for 100 epochs with minibatch size 128 and initial learning rate 0.05, which is reduced by half after every 10 epochs.",4.2. Performances on various Datasets,[0],[0]
"We train on CIFAR-10 and -100 using both without and with data augmentation, which includes random cropping and horizontal flipping.",4.2. Performances on various Datasets,[0],[0]
"For SVHN, we didn’t augment data following (Sermanet et al., 2012).",4.2. Performances on various Datasets,[0],[0]
"For all the methods, we shuffle samples at the beginning of every epoch.",4.2. Performances on various Datasets,[0],[0]
"We use N = 10 4
2 for WNN and preGWNN and N ′",4.2. Performances on various Datasets,[0],[0]
= 64 for post-GWNN.,4.2. Performances on various Datasets,[0],[0]
"For both preand post-GWNN, we have M = N and d′ = d2 .",4.2. Performances on various Datasets,[0],[0]
The other experimental settings are similar to Sec.4.1.,4.2. Performances on various Datasets,[0],[0]
Table 1 shows the results.,4.2. Performances on various Datasets,[0],[0]
"We see that pre- and post-GWNN consistently achieve better results than those of WNN, and also outperform previous state-of-the-art approaches.",4.2. Performances on various Datasets,[0],[0]
The following experiments are conducted on CIFAR-100 using pre- or post-GWNN.,4.3. Ablation Studies,[0],[0]
The first two experiments follow the setting as mentioned in Sec.4.1.,4.3. Ablation Studies,[0],[0]
"First, we evaluate the effect of the number of samples, N , used to estimate the covariance matrix in pre-GWNN.",4.3. Ablation Studies,[0],[0]
"We compare performances of using different values of N picked up from {102, 103, 3 × 103, 5 × 103, 104}.",4.3. Ablation Studies,[0],[0]
Fig.5 (a) plots the results.,4.3. Ablation Studies,[0],[0]
We see that performance can drop because of ill conditioning when N is small e.g. N = 100.,4.3. Ablation Studies,[0],[0]
"When it is too large e.g. N = 104, we observe slightly overfitting.",4.3. Ablation Studies,[0],[0]
"Second, Fig.5 (b) highlights the effect of N ′",4.3. Ablation Studies,[0],[0]
in postGWNN.,4.3. Ablation Studies,[0],[0]
"We see that post-GWNN can work reasonably well when N ′ is small.
",4.3. Ablation Studies,[0],[0]
"Finally, instead of treating d′ = d2 as a constant in training, we study the effect of tuning its value on the validation set using a simple heuristic strategy.",4.3. Ablation Studies,[0],[0]
"If the validation error reduces more than 2% over 4 consecutive evaluations, we have d′ = d′",4.3. Ablation Studies,[0],[0]
"− rate × d′.
If the error has no reduction over this period, d′ is increased by the same rate as above.",4.3. Ablation Studies,[0],[0]
We use post-GWNN and follow experimental setting in Sec.4.2.,4.3. Ablation Studies,[0],[0]
"We take two different rates {0.1, 0.2} as examples.",4.3. Ablation Studies,[0],[0]
Fig.6 plots the variations of dimensions when d = 192 and shows their test errors.,4.3. Ablation Studies,[0],[0]
"We find
that keeping d′ as a constant generally produces better result than those obtained by the above strategy, but this strategy yields less runtime because more dimensions are pruned.",4.3. Ablation Studies,[0],[0]
We presented generalized WNN (GWNN) to reduce runtime and improve generalization of WNN.,5. Conclusion,[0],[0]
"Different from WNN that reduces computation time by whitening with a large period, leading to ill conditioning of FIM, GWNN learns compact internal representation, such that SVD is approximated by the top eigenvectors in an online manner, making GWNN not only reduces computations but also improves generalization.",5. Conclusion,[0],[0]
"By exploiting the knowledge of the hidden representation’s distribution, we showed that post-GWNN is able to compute the covariance matrix in a closed form, which can be also extended to the other activation function.",5. Conclusion,[0],[0]
Extensive experiments demonstrated the effectiveness of GWNN.,5. Conclusion,[0],[0]
"This work is partially supported by the National Natural Science Foundation of China (61503366, 61472410, U1613211), the National Key Research and Development Program of China (No.2016YFC1400700), the External Cooperation Program of BIC, Chinese Academy of Sciences (No.172644KYSB20160033), and the Science and Technology Planning Project of Guangdong Province (2015B010129013, 2014B050505017).",Acknowledgements,[0],[0]
"Whitened Neural Network (WNN) is a recent advanced deep architecture, which improves convergence and generalization of canonical neural networks by whitening their internal hidden representation.",abstractText,[0],[0]
"However, the whitening transformation increases computation time.",abstractText,[0],[0]
"Unlike WNN that reduced runtime by performing whitening every thousand iterations, which degenerates convergence due to the ill conditioning, we present generalized WNN (GWNN), which has three appealing properties.",abstractText,[0],[0]
"First, GWNN is able to learn compact representation to reduce computations.",abstractText,[0],[0]
"Second, it enables whitening transformation to be performed in a short period, preserving good conditioning.",abstractText,[0],[0]
"Third, we propose a data-independent estimation of the covariance matrix to further improve computational efficiency.",abstractText,[0],[0]
Extensive experiments on various datasets demonstrate the benefits of GWNN.,abstractText,[0],[0]
Learning Deep Architectures via Generalized Whitened Neural Networks,title,[0],[0]
"ar X
iv :1
70 6.
04 96
4v 4
[ cs
.L G
] 1
4 Ju
n 20
18
ing theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures. Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures. Our method only requires the relatively inexpensive sequential training of T “shallow ResNets”. We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline. In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition. A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights.",text,[0],[0]
"Why do residual neural networks (ResNets) (He et al., 2016) and the related highway networks (Srivastava et al., 2015) work?",1. Introduction,[0],[0]
"And if we study closely why they work, can we come up with new understandings of how to train them and how to define working algorithms?
",1. Introduction,[0],[0]
"Deep neural networks have elicited breakthrough successes in machine learning, especially in image classification and object recognition (Krizhevsky et al., 2012; Sermanet et al., 2013; Simonyan & Zisserman, 2014; Zeiler & Fergus, 2014) in recent years.",1. Introduction,[0],[0]
"As the number of layers increases, the nonlinear network becomes more powerful, deriving richer features from input data.",1. Introduction,[0],[0]
"Em-
1Department of Computer Science, University of Maryland; 2Department of Computer Science, Princeton University; 3Microsoft Research.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Furong Huang <furongh@cs.umd.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"pirical studies suggest that challenging tasks in image classification (He et al., 2015; Ioffe & Szegedy, 2015; Simonyan & Zisserman, 2014; Szegedy et al., 2015) and object recognition (Girshick, 2015; Girshick et al., 2014; He et al., 2014; Long et al., 2015; Ren et al., 2015) often require “deep” networks, consisting of tens or hundreds of layers.",1. Introduction,[0],[0]
"Theoretical analyses have further justified the power of deep networks (Mhaskar & Poggio, 2016) compared to shallow networks.
",1. Introduction,[0],[0]
"However, deep neural networks are difficult to train despite their intrinsic representational power.",1. Introduction,[0],[0]
"Stochastic gradient descent with back-propagation (BP) (LeCun et al., 1989) and its variants are commonly used to solve the non-convex optimization problems.",1. Introduction,[0],[0]
"A major challenge that exists for training both shallow and deep networks is vanishing or exploding gradients (Bengio et al., 1994; Glorot & Bengio, 2010).",1. Introduction,[0],[0]
"Recent works have proposed normalization techniques (Glorot & Bengio, 2010; LeCun et al., 2012; Ioffe & Szegedy, 2015; Saxe et al., 2013) to effectively ease the problem and achieve convergence.",1. Introduction,[0],[0]
"In training deep networks, however, a surprising training performance degradation is observed (He & Sun, 2015; Srivastava et al., 2015; He et al., 2016): the training performance degrades rapidly with increased network depth after some saturation point.",1. Introduction,[0],[0]
This training performance degradation is representationally surprising as one can easily construct a deep network identical to a shallow network by forcing any part of the deep network to be the same as the shallow network with the remaining layers functioning as identity maps.,1. Introduction,[0],[0]
He et al.,1. Introduction,[0],[0]
"(He et al., 2016) presented a residual network (ResNet) learning framework to ease the training of networks that are substantially deeper than those used previously.",1. Introduction,[0],[0]
And they explicitly reformulate the layers as learning residual functions with reference to the layer inputs by adding identity loops to the layers.,1. Introduction,[0],[0]
"It is shown in (Hardt & Ma, 2016) that identity loops ease the problem of spurious local optima in shallow networks.",1. Introduction,[0],[0]
"Srivastava et al. (Srivastava et al., 2015) introduce a novel architecture that enables the optimization of networks with virtually arbitrary depth through the use of a learned gating mechanism for regulating information flow.
",1. Introduction,[0],[0]
Empirical evidence overwhelmingly shows that these deep residual networks are easier to optimize than non-residual ones.,1. Introduction,[0],[0]
"Can we develop a theoretical justification for this
observation?",1. Introduction,[0],[0]
And does that justification point us towards new algorithms with better characteristics?,1. Introduction,[0],[0]
"We propose a new framework, multi-channel telescoping sum boosting (defined in Section 4), to characterize a feed forward ResNet in Section 3.",1.1. Summary of Results,[0],[0]
We show that the top level (final) output of a ResNet can be thought of as a layer-bylayer boosting method (defined in Section 2).,1.1. Summary of Results,[0],[0]
"Traditional boosting, which ensembles “estimated score functions” or “estimated labels” from weak learners, does not work in the ResNet setting because of two reasons: (1) ResNet is a telescoping sum boosting of weak learners, not a naive (weighted) ensemble; (2) ResNet boosts over “representations”, not “estimated labels”.",1.1. Summary of Results,[0],[0]
We provide the first error bound for telescoping sum boosting over features.,1.1. Summary of Results,[0],[0]
Boosting over features and boosting over labels are different.,1.1. Summary of Results,[0],[0]
There is no existing work that proves a boosting theory (guaranteed 0 training error) for boosting features.,1.1. Summary of Results,[0],[0]
"Moreover, the special structure of a ResNet entails more complicated analysis: telescoping sum boosting, which has never been introduced before in the existing literature.
",1.1. Summary of Results,[0],[0]
We introduce a learning algorithm (BoostResNet) guaranteed to reduce error exponentially as depth increases so long as a weak learning assumption is obeyed.,1.1. Summary of Results,[0],[0]
BoostResNet adaptively selects training samples or changes the cost function (Section 4 Theorem 4.2).,1.1. Summary of Results,[0],[0]
"In Section 4.4, we analyze the generalization error of BoostResNet and provide advice to avoid overfitting.",1.1. Summary of Results,[0],[0]
"The procedure trains each residual block sequentially, only requiring that each provides a better-than-a-weak-baseline in predicting labels.
",1.1. Summary of Results,[0],[0]
BoostResNet requires radically lower computational complexity for training than end-to-end back propagation (e2eBP).,1.1. Summary of Results,[0],[0]
The number of gradient updates required by BoostResNet is much smaller than e2eBP as discussed in Section 4.3.,1.1. Summary of Results,[0],[0]
"Memorywise, BoostResNet requires only individual layers of the network to be in the graphics processing unit (GPU) while e2eBP inevitably keeps all layers in the GPU.",1.1. Summary of Results,[0],[0]
"For example, in a state-of-the-art deep ResNet, this might reduce the RAM requirements for GPU by a factor of the depth of the network.",1.1. Summary of Results,[0],[0]
"Similar improvements in computation are observed since each e2eBP step involves back propagating through the entire deep network.
",1.1. Summary of Results,[0],[0]
"Experimentally, we compare BoostResNet with e2eBP over two types of feed-forward ResNets, multilayer perceptron residual network (MLP-ResNet) and convolutional neural network residual network (CNN-ResNet), on multiple datasets.",1.1. Summary of Results,[0],[0]
BoostResNet shows substantial computational performance improvements and accuracy improvement under the MLP-ResNet architecture.,1.1. Summary of Results,[0],[0]
"Under CNN-ResNet, a faster convergence for BoostResNet is observed.
",1.1. Summary of Results,[0],[0]
One of the hallmarks of our approach is to make an explicit distinction between the classes of the multiclass learning problem and channels that are constructed by the learning procedure.,1.1. Summary of Results,[0],[0]
A channel here is essentially a scalar value modified by the rounds of boosting so as to implicitly minimize the multiclass error rate.,1.1. Summary of Results,[0],[0]
"Our multi-channel telescoping sum boosting learning framework is not limited to ResNet and can be extended to other, even non-differentiable, nonlinear hypothesis units, such as decision trees or tensor decompositions.",1.1. Summary of Results,[0],[0]
"Our contribution does not limit to explaining ResNet in the boosting framework, we have also developed a new boosting framework for other relevant tasks that require multi-channel telescoping sum structure.",1.1. Summary of Results,[0],[0]
Training deep neural networks has been an active research area in the past few years.,1.2. Related Works,[0],[0]
The main optimization challenge lies in the highly non-convex nature of the loss function.,1.2. Related Works,[0],[0]
"There are two main ways to address this optimization problem: one is to select a loss function and network architecture that have better geometric properties (details refer to appendix A.1), and the other is to improve the network’s learning procedure (details refer to appendix A.2).
",1.2. Related Works,[0],[0]
"Many authors have previously looked into neural networks and boosting, each in a different way.",1.2. Related Works,[0],[0]
"Bengio et al. (2006) introduce single hidden layer convex neural networks, and propose a gradient boosting algorithm to learn the weights of the linear classifier.",1.2. Related Works,[0],[0]
The approach has not been generalized to deep networks with more than one hidden layer.,1.2. Related Works,[0],[0]
Shalev-Shwartz (2014) proposes a selfieBoost algorithm which boosts the accuracy of an entire network.,1.2. Related Works,[0],[0]
Our algorithm is different as we instead construct ensembles of classifiers.,1.2. Related Works,[0],[0]
Veit et al. (2016) interpret residual networks as a collection of many paths of differing length.,1.2. Related Works,[0],[0]
"Their empirical study shows that residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks.
",1.2. Related Works,[0],[0]
Comparison with AdaNet,1.2. Related Works,[0],[0]
"The authors of AdaNet (Cortes et al., 2016) consider ensembles of neural layers with a boosting-style algorithm and provide a method for structural learning of neural networks by optimizing over the generalization bound, which consists of the training error and the complexity of the AdaNet architecture.",1.2. Related Works,[0],[0]
AdaNet uses the traditional boosting framework where weak classifiers are being boosted.,1.2. Related Works,[0],[0]
"Therefore, to obtain low training error guarantee, AdaNet maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers.",1.2. Related Works,[0],[0]
"In AdaNet, features (representations) from each lower layer have to be fed into a classifier (in other words, be transferred to score function in the label space).",1.2. Related Works,[0],[0]
"This is because AdaNet uses traditional boosting, which ensembles score functions or labels.",1.2. Related Works,[0],[0]
"As
a result, the top classifier in AdaNet has to be connected to all lower layers, making the structure bushy.",1.2. Related Works,[0],[0]
"Therefore AdaNet chooses its own structure during learning, and its boosting theory does not necessarily work for a ResNet structure.
",1.2. Related Works,[0],[0]
"Our BoostResNet, instead, boosts features (representations) over multiple channels, and thus produces a less “bushy” architecture.",1.2. Related Works,[0],[0]
"We are able to boost features by developing this new “telescoping-sum boosting” framework, one of our main contributions.",1.2. Related Works,[0],[0]
We come up with the new weak learning condition for the telescoping-sum boosting framework.,1.2. Related Works,[0],[0]
"The algorithm is also very different from AdaNet and is explained in details in section 3 and 4.
",1.2. Related Works,[0],[0]
"BoostResNet focuses on a ResNet architecture, provides a new training algorithm for ResNet, and proves a training error guarantee for deep ResNet architecture.",1.2. Related Works,[0],[0]
"A ResNetstyle architecture is a special case of AdaNet, so AdaNet generalization guarantee applies here and our generalization analysis is built upon their work.",1.2. Related Works,[0],[0]
A residual neural network (ResNet) is composed of stacked entities referred to as residual blocks.,2. Preliminaries,[0],[0]
Each residual block consists of a neural network module and an identity loop (shortcut).,2. Preliminaries,[0],[0]
Commonly used modules include MLP and CNN.,2. Preliminaries,[0],[0]
"Throughout this paper, we consider training and test examples generated i.i.d.",2. Preliminaries,[0],[0]
"from some distribution D over X ×Y , where X is the input space and Y is the label space.",2. Preliminaries,[0],[0]
"We denote by S = ((x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym))",2. Preliminaries,[0],[0]
"a training set of m examples drawn according to Dm.
",2. Preliminaries,[0],[0]
A Residual Block of ResNet ResNet consists of residual blocks.,2. Preliminaries,[0],[0]
Each residual block contains a module and an identity loop.,2. Preliminaries,[0],[0]
"Let each module map its input x̃ to ft(x̃) where
t denotes the level of the modules.",2. Preliminaries,[0],[0]
"Each module ft is a nonlinear unit with n channels, i.e., ft(·) ∈ Rn.",2. Preliminaries,[0],[0]
"In multilayer perceptron residual network (MLP-ResNet), ft is a shallow MLP, for instance, ft(x̃) = Ṽ ⊤",2. Preliminaries,[0],[0]
t σ(W̃ ⊤ t x̃),2. Preliminaries,[0],[0]
where W̃t ∈,2. Preliminaries,[0],[0]
"Rn×k, Ṽt ∈ Rk×n and σ is a nonlinear operator such as sigmoidal function or relu function.",2. Preliminaries,[0],[0]
"Similarly, in convolutional neural network residual network (CNN-ResNet), ft(·) represents the t-th convolutional module.",2. Preliminaries,[0],[0]
"Then the t-th residual block outputs gt+1(x)
",2. Preliminaries,[0],[0]
gt+1(x) = ft(gt(x)),2. Preliminaries,[0],[0]
"+ gt(x), (1)
where x is the input fed to the ResNet.",2. Preliminaries,[0],[0]
"See Figure 1 for an illustration of a ResNet, which consists of stacked residual blocks (each residual block contains a nonlinear module and an identity loop).
",2. Preliminaries,[0],[0]
"Output of ResNet Due to the recursive relation specified in Equation (1), the output of the T -th residual block is equal to the summation over lower module outputs, i.e., gT+1(x) = ∑T
t=0 ft(gt(x)), where g0(x) = 0 and f0(g0(x))",2. Preliminaries,[0],[0]
= x.,2. Preliminaries,[0],[0]
"For binary classification tasks, the final output of a ResNet given input x is rendered after a linear classifier w ∈ Rn on representation gT+1(x) (In the multiclass setting, let C be the number of classes; the linear classifier W ∈ Rn×C is a matrix instead of a vector.):
ŷ",2. Preliminaries,[0],[0]
= σ̃ (F (x)),2. Preliminaries,[0],[0]
"= σ̃(w⊤gT+1(x)) = σ̃ ( w⊤ T∑
t=0
ft(gt(x))
)
(2)
where F (x) = w⊤gT+1(x) and σ̃(·) denotes a map from classifier outputs (scores) to labels.",2. Preliminaries,[0],[0]
For instance σ̃(z) = sign(z) for binary classification (σ̃(z) =,2. Preliminaries,[0],[0]
"argmax
i zi
for multiclass classification).",2. Preliminaries,[0],[0]
"The parameters of a depthT ResNet are {w, {ft(·), ∀t ∈ T }}.",2. Preliminaries,[0],[0]
A ResNet training involves training the classifier w and the weights of modules ft(·) ∀t ∈,2. Preliminaries,[0],[0]
"[T ] when training examples (x1, y1), (x2, y2), . . .",2. Preliminaries,[0],[0]
", (xm, ym) are available.
",2. Preliminaries,[0],[0]
"Boosting Boosting (Freund & Schapire, 1995) assumes the availability of a weak learning algorithm which, given labeled training examples, produces a weak classifier (a.k.a. base classifier).",2. Preliminaries,[0],[0]
The goal of boosting is to improve the performance of the weak learning algorithm.,2. Preliminaries,[0],[0]
The key idea behind boosting is to choose training sets for the weak classifier in such a fashion as to force it to infer something new about the data each time it is called.,2. Preliminaries,[0],[0]
"The weak learning algorithm will finally combine many weak classifiers into a single strong classifier whose prediction power is strong.
",2. Preliminaries,[0],[0]
"From empirical experience, ResNet remedies the problem of training error degradation (instability of solving nonconvex optimization problem using SGD) in deeper neural networks.",2. Preliminaries,[0],[0]
"We are curious about whether there is a
theoretical justification that identity loops help in training.",2. Preliminaries,[0],[0]
"More importantly, we are interested in proposing a new algorithm that avoids end-to-end back-propagation (e2eBP) through the deep network and thus is immune to the instability of SGD for non-convex optimization of deep neural networks.",2. Preliminaries,[0],[0]
"As we recall from Equation (2), ResNet indeed has a similar form as the strong classifier in boosting.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"The key difference is that boosting is an ensemble of estimated hypotheses whereas ResNet is an ensemble of estimated feature representations ∑T
t=0 ft(gt(x)).",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"To solve this problem, we introduce an auxiliary linear classifier wt on top of each residual block to construct a hypothesis module.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Formally, a hypothesis module is defined as
ot(x) def = w⊤t gt(x) ∈ R (3)
in the binary classification setting.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Therefore ot+1(x) = w⊤t+1[ft(gt(x)),3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
+ gt(x)] as gt+1(x) = ft(gt(x)) + gt(x).,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We emphasize that given gt(x), we only need to train ft and wt+1 to train ot+1(x).",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"In other words, we feed the output of previous residual block (gt(x)) to the current module and train the weights of current module ft(·) and the auxiliary classifier wt+1.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Now the input, gt+1(x), of the t + 1-th residual block is the output, ft(gt(x))+gt(x), of the t-th residual block.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"As a result, ot(x) = ∑t−1 t′=0 w ⊤ t ft′(gt′(x))",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"In other words, the auxiliary linear classifier is common for all modules underneath.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"It would not be realistic to assume a common auxiliary linear classifier, as such an assumption prevents us from training the T hypothesis module sequentially.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We design a weak module classifier using the idea of telescoping sum as follows.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Definition 3.1.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"A weak module classifier is defined as
ht(x) def = αt+1ot+1(x) − αtot(x) (4)
where ot(x) def = w⊤t gt(x) is a hypothesis module, and αt is a scalar.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"We call it a “telescoping sum boosting” framework if the weak learners are restricted to the form of the weak module classifier.
",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
ResNet:,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Ensemble of Weak Module Classifiers Recall that the T -th residual block of a ResNet outputs gT+1(x), which is fed to the top/final linear classifier for the final classification.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We show that an ensemble of the weak module classifiers is equivalent to a ResNet’s final output.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We state it formally in Lemma 3.2.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"For purposes of exposition,
we will call F (x) the output of ResNet although a σ̃ function is applied on top of F (x), mapping the output to the label space Y .",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Lemma 3.2.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Let the input gt(x) of the t-th module be the output of the previous module, i.e., gt+1(x) = ft(gt(x))",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
+ gt(x).,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Then the summation of T weak module classifiers divided by αT+1 is identical to the output, F (x), of the depth-T ResNet,
F (x) = w⊤gT+1(x) ≡",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"1
αT+1
T∑
t=0
ht(x),",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"(5)
where the weak module classifier ht(x) is defined in Equation (4).
See Appendix B for the proof.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Overall, our proposed ensemble of weak module classifiers is a new framework that allows for sequential training of ResNet.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
Note that traditional boosting algorithm results do not apply here.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
We now analyze our telescoping sum boosting framework in Section 4.,3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Our analysis applies to both binary and multiclass, but we will focus on the binary class for simplicity in the main text and defer the multiclass analysis to the Appendix F.",3. ResNet in Telescoping Sum Boosting Framework,[0],[0]
"Below, we propose a learning algorithm whose training error decays exponentially with the number of weak module classifiers T under a weak learning condition.",4. Telescoping Sum Boosting for Binary Classification,[0],[0]
"We restrict to bounded hypothesis modules, i.e., |ot(x)| ≤ 1.",4. Telescoping Sum Boosting for Binary Classification,[0],[0]
The weak module classifier involves the difference between (scaled version of) ot+1(x) and ot(x).,4.1. Weak Learning Condition,[0],[0]
Let γ̃t def = Ei∼Dt−1,4.1. Weak Learning Condition,[0],[0]
[yiot(xi)],4.1. Weak Learning Condition,[0],[0]
"> 0 be the edge of the hypothesis module ot(x), where Dt−1 is the weight of the examples.",4.1. Weak Learning Condition,[0],[0]
"As the hypothesis module ot(x) is bounded by 1, we obtain |γ̃t| ≤ 1.",4.1. Weak Learning Condition,[0],[0]
So γ̃t characterizes the performance of the hypothesis module ot(x).,4.1. Weak Learning Condition,[0],[0]
"A natural requirement would be that ot+1(x) improves slightly upon ot(x), and thus γ̃t+1 − γ̃t ≥ γ′ > 0 could serve as a weak learning condition.",4.1. Weak Learning Condition,[0],[0]
"However this weak learning condition is too strong: even when current hypothesis module is performing almost ideally (γ̃t is close to 1), we still seek a hypothesis module which performs consistently better than the previous one by γ′. Instead, we consider a much weaker learning condition, inspired by training error analysis, as follows.
",4.1. Weak Learning Condition,[0],[0]
Definition 4.1 (γ-Weak Learning Condition).,4.1. Weak Learning Condition,[0],[0]
"A weak module classifier ht(x) = αt+1ot+1 − αtot satisfies the γweak learning condition if γ̃2t+1−γ̃ 2 t
1−γ̃2t ≥ γ2 > 0",4.1. Weak Learning Condition,[0],[0]
"and the
Algorithm 1 BoostResNet: telescoping sum boosting for binary-class classification Input: m labeled samples",4.1. Weak Learning Condition,[0],[0]
"[(xi, yi)]m",4.1. Weak Learning Condition,[0],[0]
"where yi ∈ {−1,+1} and a threshold γ Output: {ft(·), ∀t} and wT+1 ⊲ Discard wt+1, ∀t 6= T
1: Initialize t← 0, γ̃0",4.1. Weak Learning Condition,[0],[0]
"← 0, α0 ← 0, o0(x)← 0 2: Initialize sample weights at round 0: D0(i)← 1/m, ∀i ∈",4.1. Weak Learning Condition,[0],[0]
"[m] 3: while γt > γ do 4: ft(·), αt+1,wt+1, ot+1(x)← Algorithm 2(gt(x), Dt, ot(x), αt)
5: Compute γt ← √
γ̃2t+1−γ̃2t 1−γ̃2t
⊲ where γ̃t+1 ← Ei∼Dt",4.1. Weak Learning Condition,[0],[0]
"[yiot+1(xi)]
6: Update Dt+1(i)← Dt(i) exp(−yiht(xi))m∑ i=1",4.1. Weak Learning Condition,[0],[0]
Dt(i) exp[−yiht(xi)],4.1. Weak Learning Condition,[0],[0]
"⊲ where ht(x) = αt+1ot+1(x) − αtot(x) 7: t← t+ 1 8: end while 9: T ← t− 1
Algorithm 2 BoostResNet:",4.1. Weak Learning Condition,[0],[0]
"oracle implementation for training a ResNet block
Input: gt(x),Dt,ot(x) and αt Output: ft(·), αt+1, wt+1 and ot+1(x)
1: (ft, αt+1,wt+1)← arg min (f,α,v) m∑ i=1 Dt(i) exp ( −yiαv⊤ [f(gt(xi))",4.1. Weak Learning Condition,[0],[0]
+ gt(xi)],4.1. Weak Learning Condition,[0],[0]
+ yiαtot(xi) ),4.1. Weak Learning Condition,[0],[0]
2: ot+1(x)← w⊤t+1 [ft(gt(x)),4.1. Weak Learning Condition,[0],[0]
"+ gt(x)]
covariance between exp(−yot+1(x)) and exp(yot(x)) is non-positive.
",4.1. Weak Learning Condition,[0],[0]
"The weak learning condition is motivated by the learning theory and it is met in practice (refer to Figure 4).
",4.1. Weak Learning Condition,[0],[0]
"Interpretation of weak learning condition For each weak
module classifier ht(x), γt def = √ γ̃2t+1−γ̃2t 1−γ̃2t characterizes the normalized improvement of the correlation between the true labels y and the hypothesis modules ot+1(x) over the correlation between the true labels y and the hypothesis modules ot(x).",4.1. Weak Learning Condition,[0],[0]
The condition specified in Definition 4.1 is mild as it requires the hypothesis module ot+1(x) to perform only slightly better than the previous hypothesis module ot(x).,4.1. Weak Learning Condition,[0],[0]
"In residual network, since ot+1(x) represents a depth-(t + 1) residual network which is a deeper counterpart of the depth-t residual network ot(x), it is natural to assume that the deeper residual network improves slightly upon the shallower residual network.",4.1. Weak Learning Condition,[0],[0]
"When γ̃t is close to 1, γ̃2t+1 only needs to be slightly better than γ̃ 2 t as the denominator 1 − γ̃2t is small.",4.1. Weak Learning Condition,[0],[0]
"The assumption of the covariance between exp(−yot+1(x)) and exp(yot(x)) being non-positive is suggesting that the weak module classifiers should not be adversarial, which may be a reasonable assumption for ResNet.",4.1. Weak Learning Condition,[0],[0]
"We now propose a novel training algorithm for telescoping sum boosting under binary-class classification as in Algo-
rithm 1.",4.2. BoostResNet,[0],[0]
"In particular, we introduce a training procedure for deep ResNet in Algorithm 1 & 2, BoostResNet, which only requires sequential training of shallow ResNets.
",4.2. BoostResNet,[0],[0]
The training algorithm is a module-by-module procedure following a bottom-up fashion as the outputs of the t-th module gt+1(x) are fed as the training examples to the next t + 1-th module.,4.2. BoostResNet,[0],[0]
Each of the shallow ResNet ft(gt(x)),4.2. BoostResNet,[0],[0]
+ gt(x) is combined with an auxiliary linear classifier wt+1 to form a hypothesis module ot+1(x).,4.2. BoostResNet,[0],[0]
The weights of the ResNet are trained on these shallow ResNets.,4.2. BoostResNet,[0],[0]
The telescoping sum construction is the key for successful interpretation of ResNet as ensembles of weak module classifiers.,4.2. BoostResNet,[0],[0]
The innovative introduction of the auxiliary linear classifiers (wt+1) is the key solution for successful multi-channel representation boosting with theoretical guarantees.,4.2. BoostResNet,[0],[0]
"Auxiliary linear classifiers are only used to guide training, and they are not included in the model (proved in Lemma 3.2).",4.2. BoostResNet,[0],[0]
This is the fundamental difference between BoostResNet and AdaNet.,4.2. BoostResNet,[0],[0]
"AdaNet (Cortes et al., 2016) maps the feature vectors (hidden layer representations) to a classifier space and boosts the weak classifiers.",4.2. BoostResNet,[0],[0]
Our framework is a multi-channel representation (or information) boosting rather than a traditional classifier boosting.,4.2. BoostResNet,[0],[0]
"Traditional boosting theory does not apply in our setting.
",4.2. BoostResNet,[0],[0]
Theorem 4.2.,4.2. BoostResNet,[0],[0]
[ Training error bound ],4.2. BoostResNet,[0],[0]
"The training error of a T -module telescoping sum boosting framework using Algorithms 1 and 2 decays exponentially with the number
of modules T ,
Pr i∼S
( σ̃ ( ∑
t
ht (xi) )",4.2. BoostResNet,[0],[0]
"6= yi ) ≤ e− 12Tγ2
if ∀t ∈",4.2. BoostResNet,[0],[0]
"[T ] the weak module classifier ht(x) satisfies the γ-weak learning condition defined in Definition 4.1.
",4.2. BoostResNet,[0],[0]
"The training error of Algorithms 1 and 2 is guaranteed to decay exponentially with the ResNet depth even when each hypothesis module ot+1(x) performs slightly better than its previous hypothesis module ot(x) (i.e., γ > 0).",4.2. BoostResNet,[0],[0]
Refer to Appendix F for the algorithm and theoretical guarantees for multiclass classification.,4.2. BoostResNet,[0],[0]
"In Algorithm 2, the implementation of the oracle at line 1 is equivalent to
(ft, αt+1,wt+1) =
arg min (f,α,v)
1 m
m ∑
i=1
exp ( −yiαv ⊤",4.3. Oracle Implementation for ResNet,[0],[0]
"[f(gt(xi)) + gt(xi)] ) (6)
",4.3. Oracle Implementation for ResNet,[0],[0]
The minimization problem over f corresponds to finding the weights of the t-th nonlinear module of the residual network.,4.3. Oracle Implementation for ResNet,[0],[0]
Auxiliary classifier wt+1 is used to help solve this minimization problem with the guidance of training labels yi.,4.3. Oracle Implementation for ResNet,[0],[0]
"However, the final neural network model includes none of the auxiliary classifiers, and still follows a standard ResNet structure (proved in Lemma 3.2).",4.3. Oracle Implementation for ResNet,[0],[0]
"In practice, there are various ways to implement Equation (6).",4.3. Oracle Implementation for ResNet,[0],[0]
"For instance, Janzamin et. al. (Janzamin et al., 2015) propose a tensor decomposition technique which decomposes a tensor formed by some transformation of the features x combined with labels y and recovers the weights of a one-hidden layer neural network with guarantees.",4.3. Oracle Implementation for ResNet,[0],[0]
"One can also use backpropagation as numerous works have shown that gradient based training are relatively stable on shallow networks with identity loops (Hardt & Ma, 2016; He et al., 2016).
",4.3. Oracle Implementation for ResNet,[0],[0]
Computational & Memory Efficiency BoostResNet training is memory efficient as the training process only requires parameters of two consecutive residual blocks to be in memory.,4.3. Oracle Implementation for ResNet,[0],[0]
"Given that the limited GPU memory being one of the main bottlenecks for computational efficiency, BoostResNet requires significantly less training time than e2eBP in deep networks as a result of reduced communication overhead and the speed-up in shallow gradient forwarding and back-propagation.",4.3. Oracle Implementation for ResNet,[0.9516542845486979],"['CISIR performs better than all baseline methods for all datasets, and achieves excellent performance in IRC, due in part to the high-performing similarity estimates from the first stage.']"
"Let M1 be the memory required for one module, and M2 be the memory required for one linear classifier, the memory consumption is M1 +M2 by BoostResNet and M1T +M2 by e2eBP.",4.3. Oracle Implementation for ResNet,[0],[0]
"Let the flops needed for gradient update over one module and one linear classifier be C1 and C2 respectively, the computation cost is C1+C2 by BoostResNet and C1T + C2 by e2eBP.",4.3. Oracle Implementation for ResNet,[0],[0]
"In this section, we analyze the generalization error to understand the possibility of overfitting under Algorithm 1.",4.4. Generalization Error Analysis,[0],[0]
"The strong classifier or the ResNet is F (x) = ∑ t ht(x)
αT+1 .",4.4. Generalization Error Analysis,[0],[0]
"Now
we define the margin for example (x, y) as yF (x).",4.4. Generalization Error Analysis,[0],[0]
"For simplicity, we consider MLP-ResNet with n multiple channels and assume that the weight vector connecting a neuron at layer t with its preceding layer neurons is l1 norm bounded by Λt,t−1.",4.4. Generalization Error Analysis,[0],[0]
"Recall that there exists a linear classifier w on top, and we restrict to l1 norm bounded classifiers, i.e., ‖w‖1 ≤ C0 <∞. The expected training examples are l∞ norm bounded r∞ def = ES∼D",4.4. Generalization Error Analysis,[0],[0]
[ maxi∈[m]‖xi‖∞ ],4.4. Generalization Error Analysis,[0],[0]
"< ∞. We introduce Corollary 4.3 which follows directly from Lemma 2 of (Cortes et al., 2016).",4.4. Generalization Error Analysis,[0],[0]
Corollary 4.3.,4.4. Generalization Error Analysis,[0],[0]
"(Cortes et al., 2016) Let D be a distribution over X × Y and S be a sample of m examples chosen independently at random according to D. With probability at least 1−δ, for θ > 0, the strong classifier F (x) (ResNet) satisfies that
Pr D (yF (x) ≤ 0) ≤",4.4. Generalization Error Analysis,[0],[0]
"Pr S (yF (x) ≤ θ)+
4C0r∞ θ
√
log(2n)
2m
T ∑
t=0
Λt + 2
θ
√
log T
m + β(θ,m, T, δ) (7)
where Λt def = ∏t t′=0 2Λt′,t′−1 and β(θ,m, T, δ)
def =√⌈
4 θ2
log (
θ2m log T )⌉ log T m + log 2 δ 2m .
",4.4. Generalization Error Analysis,[0],[0]
"From Corollary 4.3, we obtain a generalization error bound in terms of margin bound PrS (yF (x) ≤ θ) and network complexity 4C0r∞
θ
√ log(2n)
2m ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
"Λt + 2 θ √ log T m +
β(θ,m, T, δ).",4.4. Generalization Error Analysis,[0],[0]
"Larger margin bound (larger θ) contributes positively to generalization accuracy, and l1 norm bounded weights (smaller ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
Λt ) are beneficial to control network complexity and to avoid overfitting.,4.4. Generalization Error Analysis,[0],[0]
"The dominant term in the network complexity is
4C0r∞ θ
√ log(2n)
2m ∑T t=0 Λt which scales as least linearly
with the depth T .",4.4. Generalization Error Analysis,[0],[0]
"See Appendix D for the proof.
",4.4. Generalization Error Analysis,[0],[0]
"This corollary suggests that stronger weak module classifiers which produce higher accuracy predictions and larger edges, will yield larger margins and suffer less from overfitting.",4.4. Generalization Error Analysis,[0],[0]
"The larger the value of θ, the smaller the term 4C0r∞
θ
√ log(2n)
2m ∑T t=0",4.4. Generalization Error Analysis,[0],[0]
"Λt + 2 θ √ log T m + β(θ,m, T, δ) is.
",4.4. Generalization Error Analysis,[0],[0]
"With larger edges on the training set and when γ̃T+1 < 1, we are able to choose larger values of θ while keeping the error term zero or close to zero.",4.4. Generalization Error Analysis,[0],[0]
"We compare our proposed BoostResNet algorithm with e2eBP training a ResNet on the MNIST (LeCun et al.,
0 5 10 15 20 25 30 0
0.2
0.4
0.6
0.8 1 T ra in in g A cc u ra cy PSfrag replacements Training Acc
Number of Residual Blocks
BoostResNet e2eBP
(a) depth vs training accuracy
0 5 10 15 20 25 30 0
0.2
0.4
0.6
0.8
1
T e
A cc
u ra
cy
PSfrag replacements
Test Acc
BoostResNet
Number of Residual Blocks
e2eBP
(b) depth vs test accuracy
Figure 2:",5. Experiments,[0],[0]
"Comparison of BoostResNet (ours, blue) and e2eBP (baseline, red) on multilayer perceptron residual network on MNIST dataset.
10 7
10 8
10 9
Number of Gradient Updates
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
A cc
u ra
cy
BoostResNet Training BoostResNet Test e2eBP Training e2eBP Test PSfrag replacements
BoostResNet Training
BoostResNet Test
e2eBP Training (a) SVHN
10 7
10 8
10 9
10 10
Number of Gradient Updates
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
A cc
u ra
cy
BoostResNet Training BoostResNet Test e2eBP Training e2eBP Test
(b) CIFAR-10
Figure 3: Convergence performance comparison between e2eBP and BoostResNet on convolutional neural network residual network on the SVHN and CIFAR-10 dataset.",5. Experiments,[0],[0]
"The vertical dotted line shows when BoostResNet training stopped, and we began refining the network with standard e2eBP training.
0",5. Experiments,[0],[0]
"10 20 30 40 50 0
0.1
0.2
PSfrag replacements γt
depth T (a) γt
0 10 20 30 40 50 0.7
0.8
0.9
",5. Experiments,[0],[0]
"PSfrag replacementsγ̃t
depth T (b) γ̃t
Figure 4: Visualization of edge γt and edge for each residual block γ̃t.",5. Experiments,[0],[0]
"The x-axis represents depth, and the y-axis represents γt or γ̃t values.",5. Experiments,[0],[0]
"The plots are for a convolutional network composed of 50 residual blocks and trained on the SVHN dataset.
1998), street view house numbers (SVHN) (Netzer et al., 2011), and CIFAR-10 (Krizhevsky & Hinton, 2009) benchmark datasets.",5. Experiments,[0],[0]
"Two different types of architectures are tested: a ResNet where each module is a fully-connected multi-layer perceptron (MLP-ResNet) and a more common, convolutional neural network residual network (CNNResNet).",5. Experiments,[0],[0]
"In each experiment the architecture of both algorithms is identical, and they are both initialized with the same random seed.",5. Experiments,[0],[0]
"As a baseline, we also experiment with standard boosting (AdaBoost.",5. Experiments,[0],[0]
"MM (Mukherjee & Schapire, 2013)) of convolutional modules for SVHN and CIFAR-10 datasets.",5. Experiments,[0],[0]
Our experiments are programmed in the Torch deep learning framework for Lua and executed on NVIDIA Tesla P100 GPUs.,5. Experiments,[0],[0]
"All models are trained using the Adam variant of SGD (Kingma & Ba, 2014).
",5. Experiments,[0],[0]
"Hyperparameters are selected via random search for high-
est accuracy on a validation set.",5. Experiments,[0],[0]
"They are specified in Appendix H. In BoostResNet, the most important hyperparameters, according to our experiments, are those that govern when the algorithm stops training the current module and begins training its successor.
",5. Experiments,[0],[0]
"MLP-ResNet on MNISTThe MNIST database (LeCun et al., 1998) of handwritten digits has a training set of 60,000 examples, and a test set of 10,000 examples.",5. Experiments,[0],[0]
The data contains ten classes.,5. Experiments,[0],[0]
"We test the performance of BoostResNet on MLP-ResNet using MNIST dataset, and compare it with e2eBP baseline.",5. Experiments,[0],[0]
"Each residual block is composed of an MLP with a single, 1024-dimensional hidden layer.",5. Experiments,[0],[0]
The training and test error between BoostResNet and e2eBP is in Figure 2 as a function of depth.,5. Experiments,[0],[0]
"Surprisingly, we find that training error degrades for e2eBP, although the ResNet’s identity
loop is supposed to alleviate this problem.",5. Experiments,[0],[0]
"Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.
",5. Experiments,[0],[0]
"CNN-ResNet on SVHN SVHN (Netzer et al., 2011) is a real-world image dataset, obtained from house numbers in Google Street View images.",5. Experiments,[0],[0]
"The dataset contains over 600,000 training images, and about 20,000 test images.",5. Experiments,[0],[0]
"We fit a 50-layer, 25-residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3a).",5. Experiments,[0],[0]
Each residual block is composed of a CNN using 15 3 × 3 filters.,5. Experiments,[0],[0]
We refine the result of BoostResNet by initializing the weights using the result of BoostResNet and run end-to-end back propagation (e2eBP).,5. Experiments,[0],[0]
"From figure 3a, our BoostResNet converges much faster (requires much fewer gradient updates) than e2eBP.",5. Experiments,[0],[0]
"The test accuracy of BoostResNet is comparable with e2eBP.
",5. Experiments,[0],[0]
"CNN-ResNet on CIFAR-10 The CIFAR-10 dataset is a benchmark dataset composed of 10 classes of small images, such as animals and vehicles.",5. Experiments,[0],[0]
"It consists of 50,000 training images and 10,000 test images.",5. Experiments,[0],[0]
"We again fit a 50-layer, 25- residual-block CNN-ResNet using both BoostResNet and e2eBP (figure 3b).",5. Experiments,[0],[0]
BoostResNet training converges to the optimal solution faster than e2eBP.,5. Experiments,[0],[0]
"Unlike in the previous two datasets, the efficiency of BoostResNet comes at a cost when training with CIFAR-10.",5. Experiments,[0],[0]
"We find that the test accuracy of the e2eBP refined BoostResNet to be slightly lower than that produced by e2eBP.
",5. Experiments,[0],[0]
Weak Learning Condition Check The weak learning condition (Definition 4.1) inspired by learning theory is checked in Figure 4.,5. Experiments,[0],[0]
"The required better than random guessing edge γt is depicted in Figure 4a, it is always greater than 0 and our weak learning condition is thus nonvacuous.",5. Experiments,[0],[0]
"In Figure 4b, the representations we learned using BoostResNet is increasingly better (for this classification task) as the depth increases.
",5. Experiments,[0],[0]
"Comparison of BoostResNet, e2eBP and AdaBoost Besides e2eBP, we also experiment with standard boosting (AdaBoost.",5. Experiments,[0],[0]
"MM (Mukherjee & Schapire, 2013)), as another baseline, of convolutional modules.",5. Experiments,[0],[0]
"In this experiment, each weak learner is a residual block of the ResNet,
paired with a classification layer.",5. Experiments,[0],[0]
We do 25 rounds of AdaBoost.,5. Experiments,[0],[0]
MM and train each weak learner to convergence.,5. Experiments,[0],[0]
"Table 1 and table 2 exhibit a comparison of BoostResNet, e2eBP and AdaBoost performance on SVHN and CIFAR10 dataset respectively.
",5. Experiments,[0],[0]
"On SVHN dataset, the advantage of BoostResNet over e2eBP is obvious.",5. Experiments,[0],[0]
"Using 3 × 108 number of gradient updates, BoostResNet achieves 93.8% test accuracy whereas e2eBP obtains a test accuracy of 83%.",5. Experiments,[0],[0]
The training and test accuracies of SVHN are listed in Table 1.,5. Experiments,[0],[0]
"BoostResNet training allows the model to train much faster than end-to-end training, and still achieves the same test accuracy when refined with e2eBP.",5. Experiments,[0],[0]
"To list the hyperparameters we use in our BoostResNet training after searching over candidate hyperparamters, we optimize learning rate to be 0.004 with a 9 × 10−5 learning rate decay.",5. Experiments,[0],[0]
The gamma threshold is optimized to be 0.001 and the initial gamma value on SVHN is 0.75.,5. Experiments,[0],[0]
"On CIFAR-10 dataset, the main advantage of BoostResNet over e2eBP is the speed of training.",5. Experiments,[0],[0]
BoostResNet refined with e2eBP obtains comparable results with e2eBP.,5. Experiments,[0],[0]
This is because we are using a suboptimal architecture of ResNet which overfits the CIFAR-10 dataset.,5. Experiments,[0],[0]
"AdaBoost, on the other hand, is known to be resistant to overfitting.",5. Experiments,[0],[0]
"In BoostResNet training, we optimize learning rate to be 0.014 with a 3.46 × 10−5 learning rate decay.",5. Experiments,[0],[0]
The gamma threshold is optimized to be 0.007 and the initial gamma value on CIFAR-10 is 0.93.,5. Experiments,[0],[0]
"We find that a standard ResNet, to its credit, is quite robust to hyperparameters, namely learning rate and learning rate decay, provided that we use an optimization procedure that automatically modulates these values.",5. Experiments,[0],[0]
Our proposed BoostResNet algorithm achieves exponentially decaying (with the depth T ) training error under the weak learning condition.,6. Conclusions and Future Works,[0],[0]
BoostResNet is much more computationally efficient compared to end-to-end backpropagation in deep ResNet.,6. Conclusions and Future Works,[0],[0]
"More importantly, the memory required by BoostResNet is trivial compared to end-toend back-propagation.",6. Conclusions and Future Works,[0],[0]
It is particularly beneficial given the limited GPU memory and large network depth.,6. Conclusions and Future Works,[0],[0]
"Our learn-
ing framework is natural for non-differentiable data.",6. Conclusions and Future Works,[0],[0]
"For instance, our learning framework is amenable to take weak learning oracles using tensor decomposition techniques.",6. Conclusions and Future Works,[0],[0]
"Tensor decomposition, a spectral learning framework with theoretical guarantees, is applied to learning one layer MLP in (Janzamin et al., 2015).",6. Conclusions and Future Works,[0],[0]
We plan to extend our learning framework to non-differentiable data using general weak learning oracles.,6. Conclusions and Future Works,[0],[0]
A.1.,A. Related Works,[0],[0]
"Loss function and architecture selection
In neural network optimization, there are many commonly-used loss functions and criteria, e.g., mean squared error, negative log likelihood, margin criterion, etc.",A. Related Works,[0],[0]
"There are extensive works (Girshick, 2015; Rubinstein & Kroese, 2013; Tygert et al., 2015) on selecting or modifying loss functions to prevent empirical difficulties such as exploding/vanishing gradients or slow learning (Balduzzi et al., 2017).",A. Related Works,[0],[0]
"However, there are no rigorous principles for selecting a loss function in general.",A. Related Works,[0],[0]
"Other works consider variations of the multilayer perceptron (MLP) or convolutional neural network (CNN) by adding identity skip connections (He et al., 2016), allowing information to bypass particular layers.",A. Related Works,[0],[0]
"However, no theoretical guarantees on the training error are provided despite breakthrough empirical successes.",A. Related Works,[0],[0]
"Hardt et al. (Hardt & Ma, 2016) have shown the advantage of identity loops in linear neural networks with theoretical justifications; however the linear setting is unrealistic in practice.
",A. Related Works,[0],[0]
A.2.,A. Related Works,[0],[0]
"Learning algorithm design
There have been extensive works on improving BP (LeCun et al., 1989).",A. Related Works,[0],[0]
"For instance, momentum (Qian, 1999), Nesterov accelerated gradient (Nesterov, 1983), Adagrad (Duchi et al., 2011) and its extension Adadelta (Zeiler, 2012).",A. Related Works,[0],[0]
"Most recently, Adaptive Moment Estimation (Adam) (Kingma & Ba, 2014), a combination of momentum and Adagrad, has received substantial success in practice.",A. Related Works,[0],[0]
"All these methods are modifications of stochastic gradient descent (SGD), but our method only requires an arbitrary oracle, which does not necessarily need to be an SGD solver, that solves a relatively simple shallow neural network.",A. Related Works,[0],[0]
Proof.,B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"In our algorithm, the input of the next module is the output of the current module
gt+1(x) = ft(gt(x))",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"+ gt(x), (8)
we thus obtain that each weak learning module is
ht(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αt+1w ⊤ t+1(ft(gt(x)),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"+ gt(x))− αtw⊤t gt(x) (9)
= αt+1w ⊤ t+1gt+1(x)− αtw⊤t gt(x), (10)
and similarly
ht+1 = αt+2w ⊤ t+2gt+2(x)− αt+1w⊤t+1gt+1(x).",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"(11)
Therefore the sum over ht(x) and ht+1(x) is
ht(x) + ht+1(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αt+2w ⊤ t+2gt+2(x),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"− αtw⊤t gt(x) (12)
",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
"And we further see that the weighted summation over all ht(x) is a telescoping sum (note that g0(x) = 0):
T∑
t=0
ht(x) =",B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
αT+1w ⊤ T+1gT+1(x),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
− α0w⊤0 g0(x) = αT+1w⊤T+1gT+1(x).,B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
(13),B. Proof for Lemma 3.2: the strong learner is a ResNet,[0],[0]
Proof.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
We will use a 0-1 loss to measure the training error.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"In our analysis, the 0-1 loss is bounded by exponential loss.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"The training error is therefore bounded by
Pr i∼D1
(p(αT+1w ⊤ T+1gT+1(xi))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi) (14)
=
m∑
i=1
D1(i)1{σ̃(αT+1w⊤T+1gT+1(xi))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
6=,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"yi} (15)
=
m∑
i=1
D1(i)1
{ σ̃ ( T∑
t=0
ht(xi) )",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi } (16)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
D1(i)",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"exp { −yi T∑
t=0
ht(xi)
} (17)
=
m∑
i=1
DT+1(i)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"T∏
t=0
Zt (18)
=
T∏
t=0
Zt (19)
where Zt = m∑ i=1 Dt(i) exp (−yiht(xi)).
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We choose αt+1 to minimize Zt.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"∂Zt ∂αt+1
=",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− m∑
i=1
Dt(i)yiot+1 exp (−yiht(xi)) (20)
= −Zt m∑
i=1
Dt+1(i)yiot+1(i) = 0 (21)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
Furthermore each learning module is bounded as we see in the following analysis.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We obtain
Zt =
m∑
i=1
Dt(i)e −yiht(xi) (22)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
m∑
i=1
Dt(i)e −αt+1yiot+1(xi)+αtyiot(xi) (23)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e −αt+1yiot+1(xi)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e αtyiot(xi) (24)
=",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)e −αt+1
1+yiot+1(xi) 2 +αt+1 1−yiot+1(xi) 2
m∑
i=1
Dt(i)e αt
1+yiot(xi) 2 −αt 1−yiot(xi)
2 (25)
≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)
( 1 + yiot+1(xi)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"2 e−αt+1 + 1− yiot+1(xi) 2
eαt+1 ) ·",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"m∑
i=1
Dt(i)
( 1 + yiot(xi)
2 eαt + 1− yiot(xi) 2
e−αt )
(26)
=
m∑
i=1
Dt(i)
( 1 + yiot+1(xi)
2 e−αt+1 + 1− yiot+1(xi) 2
eαt+1 ) eαt + e−αt
2 (27)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
m∑
i=1
Dt(i)
( e−αt+1 + eαt+1
2 +",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
e−αt+1,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 yiot+1(xi)
)",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"eαt + e−αt
2 (28)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"=
( e−αt+1 + eαt+1
2 + e−αt+1",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2 (29)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
Equation (24) is due to the non-positive correlation between exp(−yot+1(x)) and exp(yot(x)).,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Jensen’s inequality in Equation (26) holds only when |yiot+1(xi)| ≤ 1 which is satisfied by the definition of the weak learning module.
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
The algorithm chooses αt+1 to minimize Zt.,C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"We achieve an upper bound on Zt,
√ 1−γ̃2t
1−γ̃2t−1 by minimizing the bound in
Equation (29)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Zt|αt+1=argminZt ≤ Zt|αt+1= 12 ln( 1+γ̃t1−γ̃t ) (30)
≤ ( e−αt+1 + eαt+1
2 + e−αt+1",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2
∣∣∣∣ αt+1= 1 2 ln( 1+γ̃t 1−γ̃t ) (31)
= √ 1− γ̃2t 1− γ̃2t−1 = √ 1− γ2t (32)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Therefore over the T modules, the training error is upper bounded as follows
Pr i∼D
(p(αT+1w ⊤ T+1gT+1(xi)))",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"6= yi) ≤
T∏
t=0
√ 1− γ2t ≤",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"T∏
t=0
√ 1− γ2 = exp ( −1 2 Tγ2 ) (33)
",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Overall, Algorithm 1 leads us to consistent learning of ResNet.",C. Proof for Theorem 4.2: binary class telescoping sum boosting theory,[0],[0]
"Rademacher complexity technique is powerful for measuring the complexity of H any family of functions h : X → R, based on easiness of fitting any dataset using classifiers inH (whereX is any space).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let S =< x1, . . .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
", xm > be a sample of m points in X .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"The empirical Rademacher complexity ofH with respect to S is defined to be
RS(H) def=",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Eσ [ sup h∈H 1 m m∑
i=1
σih(xi)
]",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(34)
where σ is the Rademacher variable.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
The Rademacher complexity on m data points drawn from distribution D is defined by Rm(H) =,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
ES∼D,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
[RS(H)] .,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
(35) Proposition D.1.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(Theorem 1 (Cortes et al., 2014))",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Let H be a hypothesis set admitting a decomposition H = ∪li=1Hi for some l > 1.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Hi are distinct hypothesis sets.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let S be a random sequence of m points chosen independently from X according to some distribution D. For θ > 0 and any H =∑Tt=0 ht, with probability at least 1− δ,
Pr D (yH(x) ≤ 0) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yH(x) ≤ θ) + 4 θ
T∑
t=0
Rm(Hkt)",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"+ 2
θ
√ log l
m
+
√
⌈ 4 θ2 log
( θ2m
log l
) ⌉",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"log l
m +
log 2 δ
2m (36)
for all ht ∈ Hkt .",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Lemma D.2.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let h̃ = w̃⊤f̃ , where w̃ ∈ Rn, f̃ ∈ Rn.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Let H̃ and F̃ be two hypothesis sets, and h̃ ∈ H̃ , f̃j ∈ F̃ , ∀j ∈",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
[n].,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"The Rademacher complexity of H̃ and F̃ with respect to m points from D are related as follows
Rm(H̃) = ‖w̃‖1Rm(F̃).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(37)
D.1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"ResNet Module Hypothesis Space
Let n be the number of channels in ResNet, i.e., the number of input or output neurons in a module ft(gt(x)).",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"We have proved that ResNet is equivalent as
F (x) =",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"w⊤ T∑
t=0
f(gt(x))",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"(38)
We define the family of functions that each neuron ft,j , ∀j ∈",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"[n] belong to as
Ft = {x→ ut−1,j(σ ◦ ft−1)(x) : ut−1,j ∈ Rn, ‖ut−1,j‖1 ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Λt,t−1, ft−1,i ∈ Ft−1} (39)
where ut−1,j denotes the vector of weights for connections from unit j to a lower layer t−1, σ◦ ft−1 denotes element-wise nonlinear transformation on ft−1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
The output layer of each module is connected to the output layer of previous module.,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"We consider 1-layer modules for convenience of analysis.
",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Therefore in ResNet with probability at least 1− δ,
Pr D (yF (x) ≤ 0)",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
≤,D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yF (x) ≤ θ) + 4 θ
T∑
t=0
‖w‖1Rm(Ft) + 2
θ
√ logT
m
+
√
⌈ 4 θ2 log
( θ2m
logT
) ⌉ logT
m +
log 2 δ
2m (40)
for all ft ∈ Ft.
Define the maximum infinity norm over samples as r∞ def = ES∼D",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"[ maxi∈[m]‖xi‖∞ ] and the product of l1 norm bound on weights as Λt def = ∏t
t′=0 2Λt′,t′−1.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"According to lemma 2 of (Cortes et al., 2016), the empirical Rademacher complexity is bounded as a function of r∞, Λt and n:
Rm(Ft) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"r∞Λt √ log(2n)
2m (41)
",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Overall, with probability at least 1− δ,
Pr D (yF (x) ≤ 0) ≤",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
"Pr S (yF (x) ≤ θ) +
4‖w‖1r∞ √ log(2n) 2m
θ
T∑
t=0
Λt
+ 2
θ
√ logT
m +
√
⌈ 4 θ2 log
( θ2m
logT
) ⌉ logT
m +
log 2 δ
2m (42)
for all ft ∈ Ft.",D. Proof for Corollary 4.3: Generalization Bound,[0],[0]
Theorem E.1.,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
[ Generalization error bound ],E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Given algorithm 1, the fraction of training examples with margin at most θ is at most (1 + 21√ γ̃T+1 −1 )",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
θ 2 exp(− 12γ2T ).,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"And the generalization error PrD(yF (x) ≤ 0) satisfies
Pr D (yF (x) ≤ 0) ≤",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(1 + 21
γ̃T+1 − 1)
θ 2 exp(−1
2 γ2T )
+ 4C0r∞
θ
√ log(2n)
2m
T∑
t=0
Λt + 2
θ
√ logT
m + β(θ,m, T, δ) (43)
with probability at least 1− δ for β(θ,m, T, δ) def= √⌈
4 θ2
log (
θ2m log T )⌉ log T m + log 2 δ 2m .
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Now the proof for Theorem E is the following.
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
Proof.,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"The fraction of examples in sample set S being smaller than θ is bounded
Pr S (yF (x) ≤ θ) ≤ 1 m
m∑
i=1
1{yiF (xi) ≤ θ} (44)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"= 1
m
m∑
i=1
1{yi T∑
t=0
ht(xi) ≤ θαT+1} (45)
≤ 1 m
m∑
i=1
exp(−yi T∑
t=0
ht(xi)",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"+ θαT+1) (46)
= exp(θαT+1) 1
m
m∑
i=1
exp(−yi T∑
t=0
ht(xi))",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(47)
= exp(θαT+1)
T∏
t=0
Zt (48)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
To bound exp(θαT+1) = √ (1+γ̃T+11−γ̃T+1 ),E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"θ , we first bound γ̃T+1: We know that ∏T
t′=t+1(1− γ2t′)γ2t ≤ (1− γ2)T−tγ2 for all ∀γt ≥ γ2 + ǫ",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
if γ2 ≥ 1−ǫ2 .,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Therefore ∀ γt ≥ γ2 + ǫ and γ2 ≥ 1−ǫ2
γ̃2T+1 =",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(1− γ2T )γ̃2T + γ2T (49)
=
T∑
t=1
T∏
t′=t+1
(1 − γ2t′)γ2t + T∏
t=1
(1− γ2t )γ̃21",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(50)
≤ T∑
t=1
(1− γ2)T−tγ2 + (1− γ2)T γ̃21",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"(51)
=
T−1∑
t=0
(1− γ2)tγ2 + (1− γ2)T γ̃21 (52)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"= 1− (1− γ2)T + (1− γ2)T γ̃21 (53) = 1− (1− γ̃21)(1− γ2)T (54)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Therefore
Pr S (yF (x) ≤ θ) ≤ exp(θαT+1)
T∏
t=1
Zt (55)
= ( 1 + γ̃T+1 1− γ̃T+1 ) θ 2
T∏
t=1
Zt (56)
= ( 1 + γ̃T+1 1− γ̃T+1 ) θ 2
T∏
t=1
√ 1− γ2t (57)
= (1 + 2
1 γ̃T+1
− 1) θ 2 exp(−1 2 γ2T ) (58)
≤ (1 + 21√ 1−(1−γ̃21)(1−γ2)T",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"− 1) θ 2 exp(−1 2 γ2T ) (59)
",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"As T →∞, PrS(yF (x) ≤ θ) ≤ 0",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"as exp(− 12γ2T ) decays faster than (1 + 21√ 1−(1−γ̃2
1 )(1−γ2)T
−1 )",E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
θ 2 .,E. Proof for Theorem E: Margin and Generalization Bound,[0],[0]
"Recall that the weak module classifier is defined as
ht(x) = αt+1ot+1(x) − αtot(x) ∈ RC , (60)
where ot(x) ∈ ∆C−1.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The weak learning condition for multi-class classification is different from the binary classification stated in the previous section, although minimal demands placed on the weak module classifier require prediction better than random on any distribution over the training set intuitively.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
We now define the weak learning condition.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"It is again inspired by the slightly better than random idea, but requires a more sophisticated analysis in the multi-class setting.
F.1.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Cost Matrix
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"In order to characterize the training error, we introduce the cost matrix C ∈ Rm×C where each row denote the cost incurred by classifying that example into one of the C categories.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"We will bound the training error using exponential loss, and under the exponential loss function defined as in Definition G.1, the optimal cost function used for best possible training error is therefore determined.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Lemma F.1.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The optimal cost function under the exponential loss is
Ct(i, l) = { exp (st(xi, l)− st(xi, yi))",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− ∑
l′ 6=yi exp (st(xi, l
′)− st(xi, yi))",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"if l = yi (61)
where st(x) = t∑
τ=1 hτ (x).
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
F.2.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Weak Learning Condition
Definition F.2.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Let γ̃t+1 = −
m∑
i=1
<Ct(i,:),ot+1(xi)>
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
Ct(i,l) and γ̃t =
− m∑
i=1
<Ct−1(i,:),ot(xi)>
m∑
i=1
∑
l 6=yi
Ct−1(i,l) .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"A multi-class weak module classifier
ht(x)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"= αt+1ot+1(x) − αtot(x) satisfies the γ-weak learning condition if γ̃ 2 t+1−γ̃ 2 t
1−γ̃2t ≥ γ2 > 0, and Cov(< Ct(i, :
), ot+1(xi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
">,< Ct(i, :), ot+1(xi) >)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"≥ 0.
We propose a novel learning algorithm using the optimal edge-over-random cost function for training ResNet under multiclass classification task as in Algorithm 3.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Theorem F.3.,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The training error of a T -module ResNet using Algorithm 3and 4 decays exponentially with the depth of the ResNet T ,
C",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− 1 m
m∑
i=1
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Lexpη (sT (xi)) ≤,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(C − 1)e− 1 2Tγ 2
(62)
if the weak module classifier ht(x) satisfies the γ-weak learning condition ∀t ∈",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"[T ].
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"The exponential loss function defined as in Definition G.1
Algorithm 3 BoostResNet: telescoping sum boosting for multi-class classification Input:",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Given (x1, y1), . . .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(xm, ym) where yi ∈ Y = {1, . . .",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
", C} and a threshold γ Output: {ft(·),∀t} and WT+1 ⊲",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Discard wt+1, ∀t 6=",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"T
1: Initialize t← 0, γ̃0",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"← 1, α0 ← 0, o0 ← 0 ∈ RC , s0(xi, l) = 0, ∀i ∈",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"[m], l ∈ Y 2: Initialize cost function C0(i, l)← { 1",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi 1− C,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"if l = yi 3: while γt > γ do 4: ft(·), αt+1,Wt+1, ot+1(x)← Algorithm 4(gt(x),Ct, ot(x), αt) 5: Compute γt ← √
γ̃2t+1−γ̃2t 1−γ̃2t
⊲ where γ̃t+1 ← −
m∑
i=1 Ct(i,:)·ot+1(xi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
Ct(i,l)
6: Update st+1(xi, l)←",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"st(xi, l) +",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"ht(xi, l) ⊲",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"where ht(xi, l) = αt+1ot+1(xi, l)− αtot(xi, l) 7: Update cost function Ct+1(i, l)← { est+1(xi,l)−st+1(xi,yi)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
if l 6= yi,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"− ∑
l′ 6=yi est+1(xi,l ′)−st+1(xi,yi) if l = yi
8: t← t+ 1 9: end while
10: T ← t− 1
Algorithm 4 BoostResNet:",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"oracle implementation for training a ResNet module (multi-class)
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Input: gt(x),st,ot(x) and αt Output: ft(·), αt+1, Wt+1 and ot+1(x)
1: (ft, αt+1,Wt+1)← arg min (f,α,V ) m∑ i=1",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
∑,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
l 6=yi eαV,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)]",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
2: ot+1(x)←W⊤t+1,F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
[ft(gt(x)),F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"+ gt(x)]
F.3.",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Oracle Implementation
We implement an oracle to minimize Zt def = m∑ i=1 ∑",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"l 6=yi est(xi,l)−st(xi,yi)eht(xi,l)−ht(xi,yi) given current state st and hypothesis module ot(x).",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"Therefore minimizing Zt is equivalent to the following.
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"min (f,α,V )
m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)e−αt(ot(xi,l)−ot(xi,yi))eαV",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)] (63)
≡ min (f,α,V )
",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"m∑
i=1
∑
l 6=yi
eαV",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"⊤[f(gt(xi),l)−f(gt(xi),yi)+gt(xi,l)−gt(xi,yi)]",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
"(64)
≡ min α,f,v
m∑
i=1
e−αv ⊤[f(xi,yi)+gt(xi,yi)]
∑
l 6=yi
eαv ⊤[f(xi,l)+gt(xi,l)] (65)",F. Telescoping Sum Boosting for Multi-calss Classification,[0],[0]
Proof.,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"To characterize the training error, we use the exponential loss function
Definition G.1.",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Define loss function for a multiclass hypothesis H(xi) on a sample (xi, yi) as
Lexpη (H(xi), yi) = ∑
l 6=yi
exp ((H(xi, l)−H(xi, yi))) .",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(66)
Define the accumulated weak learner st(xi, l) = t∑
t′=0 ht′(xi, l) and the loss Zt = m∑ i=1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
∑,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"l 6=yi exp(st(xi, l)",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"−
st(xi, yi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"exp(ht(xi, l)− ht(xi, yi)).
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Recall that st(xi, l) = t∑
t′=0
ht′(xi, l) =",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"αt+1W ⊤ t+1gt+1(xi), the loss for a T -module multiclass ResNet is thus
Pr i∼D1
(p(αT+1W ⊤ T+1gT+1(xi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"6= yi) ≤
1
m
m∑
i=1
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Lexpη (sT (xi)) (67)
≤ 1 m
m∑
i=1
∑
l 6=yi
exp (η(sT (xi, l)− sT (xi, yi)))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(68)
≤ 1 m ZT (69) =",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"T∏
t=0
Zt Zt−1
(70)
Note that Z0 = 1 m as the initial accumulated weak learners s0(xi, l) = 0.
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"The loss fraction between module t and t− 1, Zt Zt−1 , is related to Zt − Zt−1 as ZtZt−1 = Zt−Zt−1 Zt−1 + 1.
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"The Zt is bounded
Zt =
m∑
i=1
∑
l 6=yi
exp(st(xi, l)− st(i, yi) + ht(xi, l)− ht(xi, yi))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"(71)
≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)eαt+1ot+1(xi,l)−αt+1ot+1(xi,yi)",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi)e−αtot(xi,l)+αtot(xi,yi) (72)
≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est(xi,l)−st(xi,yi) ( e−αt+1 + eαt+1
2 + e−αt+1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2
(ot+1(xi, yi)− ot+1(xi, l)) )
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"m∑
i=1
∑
l 6=yi
est−1(xi,l)−st−1(xi,yi) ( eαt + e−αt
2
) (73)
=(",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"e−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2
m∑
i=1
<",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Ct(xi, :), ot+1(xi, :) >)
( eαt + e−αt
2
)
≤(e",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2 m∑
i=1
<",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Ct(xi, :), Uγ̃t(xi, :) >)
( eαt + e−αt
2
) (74)
=( e−αt+1 + eαt+1 − 2
2 Zt−1 + eαt+1 − e−αt+1 2
(−γ̃t)Zt−1) ( eαt + e−αt
2
) (75)
Therefore Zt Zt−1 ≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
( e−αt+1 + eαt+1 2 + e−αt+1,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2 γ̃t )( eαt + e−αt 2 ) (76)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
The algorithm chooses αt+1 to minimize Zt.,G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"We achieve an upper bound on Zt,
√ 1−γ̃2t
1−γ̃t−12
by minimizing the bound in Equation (76)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Zt|αt+1=argminZt ≤ Zt|αt+1= 12 ln( 1+γ̃t1−γ̃t ) (77)
≤ ( e−αt+1 + eαt+1
2 + e−αt+1",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"− eαt+1 2 γ̃t
) eαt + e−αt
2
∣∣∣∣ αt+1= 1 2 ln( 1+γ̃t 1−γ̃t ) (78)
= √ 1− γ̃2t 1− γ̃2t−1 = √ 1− γ2t (79)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Therefore over the T modules, the training error is upper bounded as follows
Pr i∼D
(p(αT+1w ⊤ T+1gT+1(xi)))",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"6= yi) ≤
T∏
t=0
√ 1− γ2t ≤",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"T∏
t=0
√ 1− γ2 = exp ( −1 2 Tγ2 ) (80)
",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
"Overall, Algorithm 3 and 4 leads us to consistent learning of ResNet.",G. Proof for Theorem F.3 multiclass boosting theory,[0],[0]
H.1.,H. Experiments,[0],[0]
"Training error degradation of e2eBP on ResNet
We investigate e2eBP training performance on various depth ResNet.",H. Experiments,[0],[0]
"Surprisingly, we observe a training error degradation for e2eBP although the ResNet’s identity loop is supposed to alleviate this problem.",H. Experiments,[0],[0]
"Despite the presence of identity loops, the e2eBP eventually is susceptible to spurious local optima.",H. Experiments,[0],[0]
"This phenomenon is explored further in Figures 5a and 5b, which respectively show how training and test accuracies vary throughout the fitting process.",H. Experiments,[0],[0]
"Our proposed sequential training procedure, BoostResNet, relieves gradient instability issues, and continues to perform well as depth increases.",H. Experiments,[0],[0]
We prove a multi-channel telescoping sum boosting theory for the ResNet architectures which simultaneously creates a new technique for boosting over features (in contrast to labels) and provides a new algorithm for ResNet-style architectures.,abstractText,[0],[0]
"Our proposed training algorithm, BoostResNet, is particularly suitable in nondifferentiable architectures.",abstractText,[0],[0]
Our method only requires the relatively inexpensive sequential training of T “shallow ResNets”.,abstractText,[0],[0]
We prove that the training error decays exponentially with the depth T if the weak module classifiers that we train perform slightly better than some weak baseline.,abstractText,[0],[0]
"In other words, we propose a weak learning condition and prove a boosting theory for ResNet under the weak learning condition.",abstractText,[0],[0]
A generalization error bound based on margin theory is proved and suggests that ResNet could be resistant to overfitting using a network with l1 norm bounded weights.,abstractText,[0],[0]
Learning Deep ResNet Blocks Sequentially using Boosting Theory,title,[0],[0]
"Determinantal Point Processes (DPPs) are a family of probabilistic models that arose from the study of quantum mechanics (Macchi, 1975) and random matrix theory (Dyson, 1962).",1. Introduction,[0],[0]
"Following the seminal work of Kulesza and Taskar (Kulesza & Taskar, 2012), discrete DPPs have found numerous applications in machine learning, including in document and timeline summarization (Lin & Bilmes, 2012; Yao et al., 2016), image search (Kulesza & Taskar, 2011; Affandi et al., 2014) and segmentation (Lee et al., 2016), audio signal processing (Xu & Ou, 2016), bioinformatics (Batmanghelich et al., 2014) and neuroscience (Snoek et al., 2013).",1. Introduction,[0],[0]
"What makes such models appealing is that they exhibit repulsive behavior and lend themselves naturally to tasks where returning a diverse set of objects is important.
",1. Introduction,[0],[0]
"One way to define a DPP is through an N × N symmetric positive semidefinite matrix K, called a kernel, whose
1Department of Mathematics, MIT, USA.",1. Introduction,[0],[0]
"Correspondence to: John Urschel <urschel@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"eigenvalues are bounded in the range [0, 1].",1. Introduction,[0],[0]
"Then the DPP associated with K, which we denote by DPP(K), is the distribution on Y ⊆",1. Introduction,[0],[0]
"[N ] = {1, . . .",1. Introduction,[0],[0]
", N} that satisfies, for any J ⊆",1. Introduction,[0],[0]
"[N ],
P[J ⊆ Y ] = det(KJ),
where KJ is the principal submatrix of K indexed by the set J .",1. Introduction,[0],[0]
"The graph induced by K is the graph G = ([N ], E) on the vertex set [N ] that connects i, j ∈",1. Introduction,[0],[0]
"[N ] if and only if Ki,j 6= 0.
",1. Introduction,[0],[0]
"There are fast algorithms for sampling (or approximately sampling) from DPP(K) (Deshpande & Rademacher, 2010; Rebeschini & Karbasi, 2015; Li et al., 2016b;a).",1. Introduction,[0],[0]
Marginalizing the distribution on a subset I ⊆,1. Introduction,[0],[0]
"[N ] and conditioning on the event that J ⊆ Y both result in new DPPs and closed form expressions for their kernels are known (Borodin & Rains, 2005).
",1. Introduction,[0],[0]
There has been much less work on the problem of learning the parameters of a DPP.,1. Introduction,[0],[0]
"A variety of heuristics have been proposed, including Expectation-Maximization (Gillenwater et al., 2014), MCMC (Affandi et al., 2014), and fixed point algorithms (Mariet & Sra, 2015).",1. Introduction,[0],[0]
"All of these attempt to solve a nonconvex optimization problem, and no guarantees on their statistical performance are known.",1. Introduction,[0],[0]
"Recently, Brunel et al. (Brunel et al., 2017) studied the rate of estimation achieved by the maximum likelihood estimator, but the question of efficient computation remains open.
",1. Introduction,[0],[0]
"Apart from positive results on sampling, marginalization and conditioning, most provable results about DPPs are actually negative.",1. Introduction,[0],[0]
"It is conjectured that the maximum likelihood estimator is NP-hard to compute (Kulesza, 2012).",1. Introduction,[0],[0]
"Actually, approximating the mode of size k of a DPP to within a ck factor is known to be NP-hard for some c > 1 (Çivril & Magdon-Ismail, 2009; Summa et al., 2015).",1. Introduction,[0],[0]
"The best known algorithms currently obtain a ek+o(k) approximation factor (Nikolov, 2015; Nikolov & Singh, 2016).
",1. Introduction,[0],[0]
"In this work, we bypass the difficulties associated with maximum likelihood estimation by using the method of moments to achieve optimal sample complexity.",1. Introduction,[0],[0]
"We introduce a parameter `, which we call the cycle sparsity of the graph induced by the kernelK, which governs the number of moments that need to be considered and, thus, the sample complexity.",1. Introduction,[0],[0]
"Moreover, we use a refined version of Horton’s al-
gorithm (Horton, 1987; Amaldi et al., 2010) to implement the method of moments in polynomial time.
",1. Introduction,[0],[0]
The cycle sparsity of a graph is the smallest integer ` so that the cycles of length at most ` yield a basis for the cycle space of the graph.,1. Introduction,[0],[0]
"Even though there are in general exponentially many cycles in a graph to consider, Horton’s algorithm constructs a minimum weight cycle basis and, in doing so, also reveals the parameter ` together with a collection of at most ` induced cycles spanning the cycle space.
",1. Introduction,[0],[0]
We use such cycles in order to construct our method of moments estimator.,1. Introduction,[0],[0]
"For any fixed ` ≥ 2, our overall algorithm has sample complexity
n =",1. Introduction,[0],[0]
"O ((C α )2` + logN α2ε2 ) for some constant C > 1 and runs in time polynomial in n and N , and learns the parameters up to an additive ε with high probability.",1. Introduction,[0],[0]
The (C/α)2` term corresponds to the number of samples needed to recover the signs of the entries in K. We complement this result with a minimax lower bound (Theorem 2) to show that this sample complexity is in fact near optimal.,1. Introduction,[0],[0]
"In particular, we show that there is an infinite family of graphs with cycle sparsity ` (namely length ` cycles) on which any algorithm requires at least (C ′α)−2` samples to recover the signs of the entries of K for some constant C ′",1. Introduction,[0],[0]
> 1.,1. Introduction,[0],[0]
"Finally, we show experimental results that confirm many quantitative aspects of our theoretical predictions.",1. Introduction,[0],[0]
"Together, our upper bounds, lower bounds, and experiments present a nuanced understanding of which DPPs can be learned provably and efficiently.",1. Introduction,[0],[0]
"Let Y1, . . .",2.1. Model and definitions,[0],[0]
", Yn be n independent copies of Y ∼ DPP(K), for some unknown kernel K",2.1. Model and definitions,[0],[0]
such that 0 K IN .,2.1. Model and definitions,[0],[0]
It is well known that K is identified by DPP(K) only up to flips of the signs of its rows and columns: If K ′ is another symmetric matrix with 0 K ′,2.1. Model and definitions,[0],[0]
"IN , then DPP(K ′)=DPP(K) if and only if K ′ = DKD for some D ∈ DN , where DN denotes the class of all N × N diagonal matrices with only 1 and −1 on their diagonals (Kulesza, 2012, Theorem 4.1).",2.1. Model and definitions,[0],[0]
"We call such a transform a DN -similarity of K.
In view of this equivalence class, we define the following pseudo-distance between kernels K and K ′:
ρ(K,K ′) = inf D∈DN |DKD −K ′|∞ ,
where for any matrix K, |K|∞ = maxi,j∈[N ] |Ki,j | denotes the entrywise sup-norm.
",2.1. Model and definitions,[0],[0]
For any S ⊂,2.1. Model and definitions,[0],[0]
"[N ], we write ∆S = det(KS), where KS denotes the |S| × |S| submatrix of K obtained by keeping rows and colums with indices in S. Note that for 1 ≤",2.1. Model and definitions,[0],[0]
i 6=,2.1. Model and definitions,[0],[0]
"j ≤ N , we have the following relations:
Ki,i = P[i ∈ Y ], ∆{i,j} = P[{i, j} ⊆ Y ],
and |Ki,j | = √ Ki,iKj,j −∆{i,j}.",2.1. Model and definitions,[0],[0]
"Therefore, the principal minors of size one and two of K determine K up to the sign of its off-diagonal entries.",2.1. Model and definitions,[0],[0]
"In fact, for any K, there exists an ` depending only on the graph GK induced by K, such that K can be recovered up to a DN -similarity with only the knowledge of its principal minors of size at most `.",2.1. Model and definitions,[0],[0]
We will show that this ` is exactly the cycle sparsity.,2.1. Model and definitions,[0],[0]
"In this section, we review some of the interplay between graphs and DPPs that plays a key role in the definition of our estimator.
",2.2. DPPs and graphs,[0],[0]
We begin by recalling some standard graph theoretic notions.,2.2. DPPs and graphs,[0],[0]
Let G =,2.2. DPPs and graphs,[0],[0]
(,2.2. DPPs and graphs,[0],[0]
"[N ], E), |E| = m. A cycle C of G is any connected subgraph in which each vertex has even degree.",2.2. DPPs and graphs,[0],[0]
Each cycle C is associated with an incidence vector x ∈ GF (2)m such that xe = 1 if e is an edge in C and xe = 0 otherwise.,2.2. DPPs and graphs,[0],[0]
The cycle space C of G is the subspace of GF (2)m spanned by the incidence vectors of the cycles in G. The dimension νG,2.2. DPPs and graphs,[0],[0]
"of the cycle space is called cyclomatic number, and it is well known that νG",2.2. DPPs and graphs,[0],[0]
":= m−N + κ(G), where κ(G) denotes the number of connected components of G.
Recall that a simple cycle is a graph where every vertex has either degree two or zero and the set of vertices with degree two form a connected set.",2.2. DPPs and graphs,[0],[0]
A cycle basis is a basis of C ⊂ GF (2)m such that every element is a simple cycle.,2.2. DPPs and graphs,[0],[0]
"It is well known that every cycle space has a cycle basis of induced cycles.
",2.2. DPPs and graphs,[0],[0]
Definition 1.,2.2. DPPs and graphs,[0],[0]
"The cycle sparsity of a graph G is the minimal ` for which G admits a cycle basis of induced cycles of length at most `, with the convention that ` = 2 whenever the cycle space is empty.",2.2. DPPs and graphs,[0],[0]
"A corresponding cycle basis is called a shortest maximal cycle basis.
",2.2. DPPs and graphs,[0],[0]
"A shortest maximal cycle basis of the cycle space was also studied for other reasons by (Chickering et al., 1995).",2.2. DPPs and graphs,[0],[0]
"We defer a discussion of computing such a basis to Section 4.
",2.2. DPPs and graphs,[0],[0]
For any subset S ⊆,2.2. DPPs and graphs,[0],[0]
"[N ], denote by GK(S) =",2.2. DPPs and graphs,[0],[0]
"(S,E(S))",2.2. DPPs and graphs,[0],[0]
the subgraph of GK induced by S. A matching of GK(S) is a subset M ⊆ E(S) such that any two distinct edges in M are not adjacent in G(S).,2.2. DPPs and graphs,[0],[0]
The set of vertices incident to some edge in M is denoted by V (M).,2.2. DPPs and graphs,[0],[0]
We denote by M(S) the collection of all matchings of GK(S).,2.2. DPPs and graphs,[0],[0]
"Then, if GK(S) is an induced cycle, we can write the principal
minor ∆S = det(KS) as follows: ∆S = ∑
M∈M(S)
(−1)|M | ∏
{i,j}∈M
K2i,j ∏
i 6∈V",2.2. DPPs and graphs,[0],[0]
"(M)
Ki,i
+ 2× (−1)|S|+1 ∏
{i,j}∈E(S)
Ki,j .",2.2. DPPs and graphs,[0],[0]
"(1)
Others have considered the relationship between the principal minors of K and recovery of DPP(K).",2.2. DPPs and graphs,[0],[0]
"There has been work regarding the symmetric principal minor assignment problem, namely the problem of computing a matrix given an oracle that gives any principal minor in constant time (Rising et al., 2015).
",2.2. DPPs and graphs,[0],[0]
"In our setting, we can approximate the principal minors of K by empirical averages.",2.2. DPPs and graphs,[0],[0]
"However the accuracy of our estimator deteriorates with the size of the principal minor, and we must therefore estimate the smallest possible principal minors in order to achieve optimal sample complexity.",2.2. DPPs and graphs,[0],[0]
"Here, we prove a new result, namely, that the smallest ` such that all the principal minors of K are uniquely determined by those of size at most ` is exactly the cycle sparsity of the graph induced by K.
Proposition 1.",2.2. DPPs and graphs,[0],[0]
Let K ∈,2.2. DPPs and graphs,[0],[0]
"RN×N be a symmetric matrix, GK be the graph induced byK, and ` ≥ 3 be some integer.",2.2. DPPs and graphs,[0],[0]
"The kernelK is completely determined up toDN -similarity by its principal minors of size at most ` if and only if the cycle sparsity of GK is at most `.
Proof.",2.2. DPPs and graphs,[0],[0]
"Note first that all the principal minors of K completely determine K up to a DN -similarity (Rising et al., 2015, Theorem 3.14).",2.2. DPPs and graphs,[0],[0]
"Moreover, recall that principal minors of degree at most 2 determine the diagonal entries of K as well as the magnitude of its off-diagonal entries.",2.2. DPPs and graphs,[0],[0]
"In particular, given these principal minors, one only needs to recover the signs of the off-diagonal entries of K. Let the sign of cycle C in K be the product of the signs of the entries of K corresponding to the edges of C.
Suppose GK has cycle sparsity ` and let (C1, . . .",2.2. DPPs and graphs,[0],[0]
", Cν) be a cycle basis of GK where each Ci, i ∈",2.2. DPPs and graphs,[0],[0]
[ν] is an induced cycle of length at most `.,2.2. DPPs and graphs,[0],[0]
"By (1), the sign of any Ci, i ∈",2.2. DPPs and graphs,[0],[0]
"[ν] is completely determined by the principal minor ∆S , where S is the set of vertices of Ci and is such that |S| ≤",2.2. DPPs and graphs,[0],[0]
`.,2.2. DPPs and graphs,[0],[0]
"Moreover, for i ∈",2.2. DPPs and graphs,[0],[0]
"[ν], let xi ∈ GF",2.2. DPPs and graphs,[0],[0]
(2)m denote the incidence vector of Ci.,2.2. DPPs and graphs,[0],[0]
"By definition, the incidence vector x of any cycle C is given by ∑ i∈I xi for some subset",2.2. DPPs and graphs,[0],[0]
I ⊂,2.2. DPPs and graphs,[0],[0]
[ν].,2.2. DPPs and graphs,[0],[0]
"The sign of C is then given by the product of the signs of Ci, i ∈",2.2. DPPs and graphs,[0],[0]
I and thus by corresponding principal minors.,2.2. DPPs and graphs,[0],[0]
"In particular, the signs of all cycles are determined by the principal minors ∆S with |S| ≤",2.2. DPPs and graphs,[0],[0]
`.,2.2. DPPs and graphs,[0],[0]
"In turn, by Theorem 3.12 in (Rising et al., 2015), the signs of all cycles completely determine K, up to a DN -similarity.
",2.2. DPPs and graphs,[0],[0]
"Next, suppose the cycle sparsity of GK is at least ` + 1,
and let C` be the subspace of GF (2)m spanned by the induced cycles of length at most ` in GK .",2.2. DPPs and graphs,[0],[0]
"Let x1, . . .",2.2. DPPs and graphs,[0],[0]
", xν be a basis of C` made of the incidence column vectors of induced cycles of length at most ` inGK and form the matrix A ∈ GF (2)m×ν by concatenating the xi’s.",2.2. DPPs and graphs,[0],[0]
"Since C` does not span the cycle space of GK , ν < νGK ≤ m.",2.2. DPPs and graphs,[0],[0]
"Hence, the rank of A is less than m, so the null space of A> is non trivial.",2.2. DPPs and graphs,[0],[0]
Let x̄ be the incidence column vector of an induced cycle C̄,2.2. DPPs and graphs,[0],[0]
"that is not in C`, and let h ∈ GL(2)m with A>h = 0, h 6= 0 and x̄>h = 1.",2.2. DPPs and graphs,[0],[0]
These three conditions are compatible because C̄ /∈,2.2. DPPs and graphs,[0],[0]
C`.,2.2. DPPs and graphs,[0],[0]
We are now in a position to define an alternate kernel K ′,2.2. DPPs and graphs,[0],[0]
"as follows: Let K ′i,i = Ki,i and |K ′i,j | = |Ki,j | for all i, j ∈",2.2. DPPs and graphs,[0],[0]
[N ].,2.2. DPPs and graphs,[0],[0]
We define the signs of the off-diagonal entries of K ′,2.2. DPPs and graphs,[0],[0]
"as follows: For all edges e = {i, j}, i 6= j, sgn(K ′e)",2.2. DPPs and graphs,[0],[0]
= sgn(Ke) if he = 0 and sgn(K ′e) =,2.2. DPPs and graphs,[0],[0]
− sgn(Ke) otherwise.,2.2. DPPs and graphs,[0],[0]
We now check that K and K ′ have the same principal minors of size at most ` but differ on a principal minor of size larger than `.,2.2. DPPs and graphs,[0],[0]
"To that end, let x be the incidence vector of a cycle C in C` so that x",2.2. DPPs and graphs,[0],[0]
=,2.2. DPPs and graphs,[0],[0]
Aw for some w ∈ GL(2)ν .,2.2. DPPs and graphs,[0],[0]
"Thus the sign of C in K is given by∏
e : xe=1
Ke = (−1)x >h ∏ e : xe=1 K ′e
= (−1)w >A>h ∏ e : xe=1 K ′e = ∏ e : xe=1 K ′e
because A>h = 0.",2.2. DPPs and graphs,[0],[0]
"Therefore, the sign of any C ∈ C` is the same in K and K ′. Now, let S ⊆",2.2. DPPs and graphs,[0],[0]
[N ] with |S| ≤,2.2. DPPs and graphs,[0],[0]
"`, and let",2.2. DPPs and graphs,[0],[0]
"G = GKS = GK′S be the graph corresponding to KS (or, equivalently, to K ′S).",2.2. DPPs and graphs,[0],[0]
"For any induced cycle C in G, C is also an induced cycle in GK and its length is at most `.",2.2. DPPs and graphs,[0],[0]
"Hence, C ∈ C` and the sign of C is the same in K and K ′. By (Rising et al., 2015, Theorem 3.12), det(KS) = det(K ′S).",2.2. DPPs and graphs,[0],[0]
"Next observe that the sign of C̄ in K is given by∏
e : x̄e=1
Ke = (−1)x̄ >h ∏ e : x̄e=1 K ′e = − ∏ e : xe=1 K ′e.
",2.2. DPPs and graphs,[0],[0]
"Note also that since C̄ is an induced cycle of GK = GK′ , the above quantity is nonzero.",2.2. DPPs and graphs,[0],[0]
"Let S̄ be the set of vertices in C̄. By (1) and the above display, we have det(KS̄) 6=",2.2. DPPs and graphs,[0],[0]
"det(K ′
S̄ )",2.2. DPPs and graphs,[0],[0]
.,2.2. DPPs and graphs,[0],[0]
"Together with (Rising et al., 2015, Theorem
3.14), it yields K 6=",2.2. DPPs and graphs,[0],[0]
DK ′D for all D ∈ DN .,2.2. DPPs and graphs,[0],[0]
Our procedure is based on the previous result and can be summarized as follows.,2.3. Definition of the Estimator,[0],[0]
"We first estimate the diagonal entries (i.e., the principal minors of size one) of K by the method of moments.",2.3. Definition of the Estimator,[0],[0]
"By the same method, we estimate the principal minors of size two ofK, and we deduce estimates of the magnitude of the off-diagonal entries.",2.3. Definition of the Estimator,[0],[0]
"To use these estimates to deduce an estimate Ĝ of GK , we make the following assumption on the kernel K.
Assumption 1.",2.3. Definition of the Estimator,[0],[0]
"Fix α ∈ (0, 1).",2.3. Definition of the Estimator,[0],[0]
"For all 1 ≤ i < j ≤ N , either Ki,j = 0, or |Ki,j | ≥ α.
",2.3. Definition of the Estimator,[0],[0]
"Finally, we find a shortest maximal cycle basis of Ĝ, and we set the signs of our non-zero off-diagonal entry estimates by using estimators of the principal minors induced by the elements of the basis, again obtained by the method of moments.
",2.3. Definition of the Estimator,[0],[0]
For S ⊆,2.3. Definition of the Estimator,[0],[0]
"[N ], set ∆̂S = 1
n n∑ p=1 1S⊆Yp , and define
K̂i,i = ∆̂{i} and B̂i,j = K̂i,iK̂j,j − ∆̂{i,j},
where K̂i,i and B̂i,j are our estimators of Ki,i and K2i,j , respectively.
",2.3. Definition of the Estimator,[0],[0]
Define Ĝ =,2.3. Definition of the Estimator,[0],[0]
"([N ], Ê),",2.3. Definition of the Estimator,[0],[0]
"where, for i 6= j, {i, j} ∈ Ê if and only if B̂i,j ≥ 12α
2.",2.3. Definition of the Estimator,[0],[0]
The graph Ĝ is our estimator of GK .,2.3. Definition of the Estimator,[0],[0]
"Let {Ĉ1, ..., ĈνĜ} be a shortest maximal cycle basis of the cycle space of Ĝ.",2.3. Definition of the Estimator,[0],[0]
Let Ŝi ⊆,2.3. Definition of the Estimator,[0],[0]
"[N ] be the subset of vertices of Ĉi, for 1 ≤ i ≤",2.3. Definition of the Estimator,[0],[0]
νĜ.,2.3. Definition of the Estimator,[0],[0]
"We define
Ĥi = ∆̂Ŝi − ∑
M∈M(Ŝi)
(−1)|M | ∏
{i,j}∈M
B̂i,j ∏
i 6∈V (M)
K̂i,i,
for 1 ≤",2.3. Definition of the Estimator,[0],[0]
i ≤ νĜ.,2.3. Definition of the Estimator,[0],[0]
"In light of (1), for large enough n, this quantity should be close to
Hi = 2× (−1)|Ŝi|+1 ∏
{i,j}∈E(Ŝi)
Ki,j .
",2.3. Definition of the Estimator,[0],[0]
"We note that this definition is only symbolic in nature, and computing Ĥi in this fashion is extremely inefficient.",2.3. Definition of the Estimator,[0],[0]
"Instead, to compute it in practice, we will use the determinant of an auxiliary matrix, computed via a matrix factorization.",2.3. Definition of the Estimator,[0],[0]
"Namely, let us define the matrix K̃ ∈",2.3. Definition of the Estimator,[0],[0]
"RN×N such that K̃i,i = K̂i,i for 1 ≤",2.3. Definition of the Estimator,[0],[0]
"i ≤ N , and K̃i,j = B̂1/2i,j .",2.3. Definition of the Estimator,[0],[0]
"We have
det K̃Ŝi = ∑ M∈M (−1)|M | ∏ {i,j}∈M B̂i,j ∏ i 6∈V (M) K̂i,i
+ 2× (−1)|Ŝi|+1 ∏
{i,j}∈Ê(Ŝi)
B̂ 1/2 i,j ,
so that we may equivalently write
Ĥi = ∆̂Ŝi − det(K̃Ŝi) + 2× (−1) |Ŝi|+1",2.3. Definition of the Estimator,[0],[0]
"∏ {i,j}∈Ê(Ŝi) B̂ 1/2 i,j .
Finally, let m̂ = |Ê|.",2.3. Definition of the Estimator,[0],[0]
"Set the matrix A ∈ GF (2)νĜ×m̂ with i-th row representing Ĉi in GF (2)m, 1 ≤ i ≤ νĜ, b = (b1, . . .",2.3. Definition of the Estimator,[0],[0]
", bνĜ) ∈ GF (2)
νĜ with bi = 12",2.3. Definition of the Estimator,[0],[0]
"[sgn(Ĥi) + 1], 1 ≤ i ≤ νĜ, and let x ∈ GF (2)m be a solution to the linear system",2.3. Definition of the Estimator,[0],[0]
"Ax = b if a solution exists, x = 1m otherwise.
",2.3. Definition of the Estimator,[0],[0]
"We define K̂i,j = 0",2.3. Definition of the Estimator,[0],[0]
"if {i, j} /∈ Ê and K̂i,j = K̂j,i = (2x{i,j} − 1)B̂ 1/2",2.3. Definition of the Estimator,[0],[0]
"i,j for all {i, j} ∈ Ê.",2.3. Definition of the Estimator,[0],[0]
The main result of this subsection is the following lemma which relates the quality of estimation of K in terms of ρ to the quality of estimation of the principal minors ∆S .,2.4. Geometry,[0],[0]
Lemma 1.,2.4. Geometry,[0],[0]
"Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK .",2.4. Geometry,[0],[0]
Let ε > 0.,2.4. Geometry,[0],[0]
If |∆̂S −∆S | ≤ ε for all S ⊆,2.4. Geometry,[0],[0]
[N ] with |S| ≤ 2 and if |∆̂S −∆S | ≤ (α/4)|S| for all,2.4. Geometry,[0],[0]
S ⊆,2.4. Geometry,[0],[0]
[N ] with 3 ≤ |S| ≤,2.4. Geometry,[0],[0]
"`, then
ρ(K̂,K) <",2.4. Geometry,[0],[0]
"4ε/α .
",2.4. Geometry,[0],[0]
Proof.,2.4. Geometry,[0],[0]
"We can bound |B̂i,j −K2i,j |, namely,
B̂i,j ≤",2.4. Geometry,[0],[0]
"(Ki,i + α2/16)(Kj,j + α2/16)− (∆{i,j} − α2/16) ≤",2.4. Geometry,[0],[0]
"K2i,j + α2/4
and
B̂i,j ≥ (Ki,i − α2/16)(Kj,j − α2/16)− (∆{i,j} + α2/16) ≥",2.4. Geometry,[0],[0]
"K2i,j − 3α2/16,
giving |B̂i,j −K2i,j | < α2/4.",2.4. Geometry,[0],[0]
"Thus, we can correctly determine whether Ki,j = 0 or |Ki,j | ≥ α, yielding Ĝ = GK .",2.4. Geometry,[0],[0]
"In particular, the cycle basis Ĉ1, . . .",2.4. Geometry,[0],[0]
", ĈνĜ of Ĝ is a cycle basis of GK .",2.4. Geometry,[0],[0]
Let 1 ≤ i ≤ νĜ.,2.4. Geometry,[0],[0]
Denote by t = (α/4)|Si|.,2.4. Geometry,[0],[0]
We have∣∣∣Ĥi −Hi∣∣∣ ≤,2.4. Geometry,[0],[0]
|∆̂Ŝi −∆Ŝi |+ |M(Ŝi)|maxx∈±1 [ (1 + 4tx)|Ŝi|,2.4. Geometry,[0],[0]
"− 1
] ≤ (α/4)|Ŝi| + |M(Ŝi)|",2.4. Geometry,[0],[0]
"[ (1 + 4t)|Ŝi| − 1
] ≤ (α/4)|Ŝi| + T ( |Ŝi|, ⌊ |Ŝi| 2 ⌋) 4t T (|Ŝi|, |Ŝi|)
≤ (α/4)|Ŝi| + 4t (2 |Ŝi| 2 − 1)(2|Ŝi|",2.4. Geometry,[0],[0]
"− 1)
≤ (α/4)|Ŝi| + t22|Ŝi|
< 2α|Ŝi| ≤ |Hi|,
where, for positive integers p < q, we denote by T (q, p) = ∑p i=1",2.4. Geometry,[0],[0]
( q i ),2.4. Geometry,[0],[0]
.,2.4. Geometry,[0],[0]
"Therefore, we can deter-
mine the sign of the product ∏ {i,j}∈E(Ŝi)Ki,j for every element in the cycle basis and recover the signs of the non-zero off-diagonal entries of Ki,j .",2.4. Geometry,[0],[0]
"Hence,
ρ(K̂,K) = max1≤i,j≤N ∣∣∣|K̂i,j | − |Ki,j |∣∣∣.",2.4. Geometry,[0],[0]
"For i = j,∣∣∣|K̂i,j | − |Ki,j |∣∣∣ = |K̂i,i −Ki,i| ≤ ε.",2.4. Geometry,[0],[0]
"For i 6= j with {i, j} ∈ Ê = E, one can easily show that∣∣∣B̂i,j −K2i,j∣∣∣ ≤ 4ε, yielding
|B̂1/2i,j − |Ki,j || ≤ 4ε∣∣B̂1/2i,j + |Ki,j |∣∣ ≤",2.4. Geometry,[0],[0]
"4ε α ,
which completes the proof.
",2.4. Geometry,[0],[0]
We are now in a position to establish a sufficient sample size to estimate K within distance ε.,2.4. Geometry,[0],[0]
Theorem 1.,2.4. Geometry,[0],[0]
"Let K satisfy Assumption 1, and let ` be the cycle sparsity of GK .",2.4. Geometry,[0],[0]
Let ε > 0.,2.4. Geometry,[0],[0]
"For any A > 0, there exists C > 0",2.4. Geometry,[0],[0]
"such that
n ≥ C",2.4. Geometry,[0],[0]
"( 1 α2ε2 + ` ( 4 α )2`) logN ,
yields ρ(K̂,K) ≤ ε",2.4. Geometry,[0],[0]
"with probability at least 1−N−A.
Proof.",2.4. Geometry,[0],[0]
"Using the previous lemma, and applying a union bound,
P [ ρ(K̂,K) > ε ] ≤ ∑ |S|≤2 P [ |∆̂S −∆S | > αε/4 ]",2.4. Geometry,[0],[0]
"+
∑ 2≤|S|≤` P [ |∆̂S −∆S | > (α/4)|S| ] ≤ 2N2e−nα 2ε2/8 + 2N `+1e−2n(α/4) 2`
, (2)
where we used Hoeffding’s inequality.",2.4. Geometry,[0],[0]
We prove an information-theoretic lower bound that holds already if GK is an `-cycle.,3. Information theoretic lower bound,[0],[0]
"Let D(K‖K ′) and H(K,K ′) denote respectively the Kullback-Leibler divergence and the Hellinger distance between DPP(K) and DPP(K ′).",3. Information theoretic lower bound,[0],[0]
Lemma 2.,3. Information theoretic lower bound,[0],[0]
"For η ∈ {−,+}, letKη be the `×`matrix with elements given by
Ki,j =  1/2 if j = i α if j = i± 1 ηα",3. Information theoretic lower bound,[0],[0]
"if (i, j) ∈ {(1, `), (`, 1)} 0 otherwise .
",3. Information theoretic lower bound,[0],[0]
"Then, for any α ≤ 1/8, it holds
D(K‖K ′) ≤ 4(6α)`, and H(K,K ′) ≤ (8α2)` .
",3. Information theoretic lower bound,[0],[0]
Proof.,3. Information theoretic lower bound,[0],[0]
"It is straightforward to see that
det(K+J )",3. Information theoretic lower bound,[0],[0]
− det(K,3. Information theoretic lower bound,[0],[0]
− J ) =,3. Information theoretic lower bound,[0],[0]
{ 2α` if J =,3. Information theoretic lower bound,[0],[0]
"[`] 0 else .
",3. Information theoretic lower bound,[0],[0]
"If Y is sampled from DPP(Kη), we denote by pη(S) = P[Y = S], for S ⊆",3. Information theoretic lower bound,[0],[0]
[`].,3. Information theoretic lower bound,[0],[0]
It follows from the inclusionexclusion principle that for all S ⊆,3. Information theoretic lower bound,[0],[0]
"[`],
p+(S)− p−(S) =",3. Information theoretic lower bound,[0],[0]
"∑
J⊆[`]\S
(−1)|J|(detK+S∪J",3. Information theoretic lower bound,[0],[0]
"− detK − S∪J)
=",3. Information theoretic lower bound,[0],[0]
"(−1)`−|S|(detK+ − detK−) = ±2α` , (3)
where |J | denotes the cardinality of J .",3. Information theoretic lower bound,[0],[0]
The inclusionexclusion principle also yields that pη(S) = |det(Kη,3. Information theoretic lower bound,[0],[0]
− IS̄)| for all S ⊆,3. Information theoretic lower bound,[0],[0]
"[l], where IS̄ stands for the ` × ` diagonal matrix with ones on its entries (i, i) for i /∈ S, zeros elsewhere.
",3. Information theoretic lower bound,[0],[0]
"Denote by D(K+‖K−) the Kullback Leibler divergence between DPP(K+) and DPP(K−):
D(K+‖K−) = ∑ S⊆[`] p+(S) log ( p+(S) p−(S) )
≤ ∑ S⊆[`] p+(S) p−(S) (p+(S)− p−(S))
≤",3. Information theoretic lower bound,[0],[0]
2α` ∑ S⊆[`] |det(K+ − IS̄)| |det(K−,3. Information theoretic lower bound,[0],[0]
"− IS̄)| , (4)
by (3).",3. Information theoretic lower bound,[0],[0]
"Using the fact that 0 < α ≤ 1/8 and the Gershgorin circle theorem, we conclude that the absolute value of all eigenvalues of Kη− IS̄ are between 1/4 and 3/4.",3. Information theoretic lower bound,[0],[0]
"Thus we obtain from (4) the bound D(K+‖K−) ≤ 4(6α)`.
",3. Information theoretic lower bound,[0],[0]
"Using the same arguments as above, the Hellinger distance H(K+,K−) between DPP(K+) and DPP(K−) satisfies
H(K+,K−) =",3. Information theoretic lower bound,[0],[0]
∑ J⊆,3. Information theoretic lower bound,[0],[0]
[`] ( p+(J)− p−(J)√ p+(J),3. Information theoretic lower bound,[0],[0]
+,3. Information theoretic lower bound,[0],[0]
"√ p−(J) )2
≤ ∑ J⊆[`] 4α2` 2 · 4−` = (8α2)`
which completes the proof.
",3. Information theoretic lower bound,[0],[0]
"The sample complexity lower bound now follows from standard arguments.
",3. Information theoretic lower bound,[0],[0]
Theorem 2.,3. Information theoretic lower bound,[0],[0]
Let 0,3. Information theoretic lower bound,[0],[0]
< ε ≤ α ≤ 1/8 and 3 ≤ ` ≤ N .,3. Information theoretic lower bound,[0],[0]
There exists a constant C > 0,3. Information theoretic lower bound,[0],[0]
"such that if
n ≤ C ( 8` α2` + log(N/`) (6α)` + logN ε2 ) ,
then the following holds: for any estimator K̂ based on n samples, there exists a kernelK that satisfies Assumption 1 and such that the cycle sparsity of GK is ` and for which ρ(K̂,K) ≥ ε with probability at least 1/3.
Proof.",3. Information theoretic lower bound,[0],[0]
Recall the notation of Lemma 2.,3. Information theoretic lower bound,[0],[0]
First consider the N ×N block diagonal matrix K (resp.,3. Information theoretic lower bound,[0],[0]
K ′) where its first block isK+ (resp. K−) and its second block is IN−`.,3. Information theoretic lower bound,[0],[0]
"By a standard argument, the Hellinger distance Hn(K,K ′) between the product measures DPP(K)⊗n and DPP(K ′)⊗n satisfies
1− H 2 n(K,K ′) 2 =",3. Information theoretic lower bound,[0],[0]
"( 1− H 2(K,K ′) 2 )n",3. Information theoretic lower bound,[0],[0]
"≥ (1− α2` 2× 8` )n ,
which yields the first term in the desired lower bound.
",3. Information theoretic lower bound,[0],[0]
"Next, by padding with zeros, we can assume that L = N/` is an integer.",3. Information theoretic lower bound,[0],[0]
Let K(0) be a block diagonal matrix where each block is K+ (using the notation of Lemma 2).,3. Information theoretic lower bound,[0],[0]
"For j = 1, . . .",3. Information theoretic lower bound,[0],[0]
", L, define the N × N block diagonal matrix K(j) as the matrix obtained from K(0) by replacing its jth block with K− (again using the notation of Lemma 2).
",3. Information theoretic lower bound,[0],[0]
"Since DPP(K(j)) is the product measure of L lower dimensional DPPs that are each independent of each other, using Lemma 2 we have D(K(j)‖K(0)) ≤",3. Information theoretic lower bound,[0],[0]
4(6α)`.,3. Information theoretic lower bound,[0],[0]
"Hence, by Fano’s lemma (see, e.g., Corollary 2.6 in (Tsybakov, 2009)), the sample complexity to learn the kernel of a DPP within a distance ε ≤ α is
Ω
( log(N/`)
(6α)` ) which yields the second term.
",3. Information theoretic lower bound,[0],[0]
The third term follows from considering K0 = (1/2)IN and letting Kj be obtained from K0 by adding ε to the jth entry along the diagonal.,3. Information theoretic lower bound,[0],[0]
It is easy to see that D(Kj‖K0) ≤ 8ε2.,3. Information theoretic lower bound,[0],[0]
"Hence, a second application of Fano’s lemma yields that the sample complexity to learn the kernel of a DPP within a distance ε is Ω( logNε2 ).
",3. Information theoretic lower bound,[0],[0]
The third term in the lower bound is the standard parametric term and is unavoidable in order to estimate the magnitude of the coefficients of K.,3. Information theoretic lower bound,[0],[0]
The other terms are more interesting.,3. Information theoretic lower bound,[0],[0]
"They reveal that the cycle sparsity of GK , namely, `, plays a key role in the task of recovering the sign pattern of K.",3. Information theoretic lower bound,[0],[0]
Moreover the theorem shows that the sample complexity of our method of moments estimator is near optimal.,3. Information theoretic lower bound,[0],[0]
We first give an algorithm to compute the estimator K̂ defined in Section 2.,4.1. Horton’s algorithm,[0],[0]
"A well-known algorithm of Horton (Horton, 1987) computes a cycle basis of minimum total length in time O(m3N).",4.1. Horton’s algorithm,[0],[0]
"Subsequently, the running time was improved toO(m2N/ logN) time (Amaldi et al., 2010).",4.1. Horton’s algorithm,[0],[0]
"Also, it is known that a cycle basis of minimum total length is a shortest maximal cycle basis (Chickering et al., 1995).",4.1. Horton’s algorithm,[0],[0]
"Together, these results imply the following.
",4.1. Horton’s algorithm,[0],[0]
Lemma 3.,4.1. Horton’s algorithm,[0],[0]
Let G =,4.1. Horton’s algorithm,[0],[0]
(,4.1. Horton’s algorithm,[0],[0]
"[N ], E), |E| = m. There is an algorithm to compute a shortest maximal cycle basis in O(m2N/ logN) time.
",4.1. Horton’s algorithm,[0],[0]
"In addition, we recall the following standard result regarding the complexity of Gaussian elimination (Golub & Van Loan, 2012).
",4.1. Horton’s algorithm,[0],[0]
"Algorithm 1 Compute Estimator K̂ Input: samples Y1, ..., Yn, parameter α > 0.
",4.1. Horton’s algorithm,[0],[0]
Compute ∆̂S for all |S| ≤ 2.,4.1. Horton’s algorithm,[0],[0]
"Set K̂i,i = ∆̂{i} for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i ≤ N .,4.1. Horton’s algorithm,[0],[0]
"Compute B̂i,j for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i < j ≤ N .,4.1. Horton’s algorithm,[0],[0]
Form K̃ ∈,4.1. Horton’s algorithm,[0],[0]
RN×N and Ĝ,4.1. Horton’s algorithm,[0],[0]
=,4.1. Horton’s algorithm,[0],[0]
"([N ], Ê).",4.1. Horton’s algorithm,[0],[0]
"Compute a shortest maximal cycle basis {v̂1, ..., v̂νĜ}.",4.1. Horton’s algorithm,[0],[0]
Compute ∆̂Ŝi for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ.,4.1. Horton’s algorithm,[0],[0]
Compute ĈŜi using det K̃Ŝi for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ.,4.1. Horton’s algorithm,[0],[0]
"Construct A ∈ GF (2)νĜ×m, b ∈ GF (2)νĜ .",4.1. Horton’s algorithm,[0],[0]
Solve Ax = b using Gaussian elimination.,4.1. Horton’s algorithm,[0],[0]
"Set K̂i,j = K̂j,i = (2x{i,j}−1)B̂ 1/2",4.1. Horton’s algorithm,[0],[0]
"i,j , for all {i, j} ∈",4.1. Horton’s algorithm,[0],[0]
"Ê.
Lemma 4.",4.1. Horton’s algorithm,[0],[0]
"LetA ∈ GF (2)ν×m, b ∈ GF",4.1. Horton’s algorithm,[0],[0]
(2)ν .,4.1. Horton’s algorithm,[0],[0]
"Then Gaussian elimination will find a vector x ∈ GF (2)m such that Ax = b or conclude that none exists in O(ν2m) time.
",4.1. Horton’s algorithm,[0],[0]
We give our procedure for computing the estimator K̂ in Algorithm 1.,4.1. Horton’s algorithm,[0],[0]
"In the following theorem, we bound the running time of Algorithm 1 and establish an upper bound on the sample complexity needed to solve the recovery problem as well as the sample complexity needed to compute an estimate K̂ that is close to K.
Theorem 3.",4.1. Horton’s algorithm,[0],[0]
Let K ∈,4.1. Horton’s algorithm,[0],[0]
"RN×N be a symmetric matrix satisfying 0 K I , and satisfying Assumption 1.",4.1. Horton’s algorithm,[0],[0]
Let GK be the graph induced by K and ` be the cycle sparsity of GK .,4.1. Horton’s algorithm,[0],[0]
"Let Y1, ..., Yn be samples from DPP(K) and δ ∈ (0, 1).",4.1. Horton’s algorithm,[0],[0]
"If
n > log(N",4.1. Horton’s algorithm,[0],[0]
"`+1/δ)
(α/4) 2`
,
then with probability at least 1 − δ, Algorithm 1 computes an estimator K̂ which recovers the signs of K up to a DN - similarity and satisfies
ρ(K, K̂)",4.1. Horton’s algorithm,[0],[0]
"< 1
α
( 8 log(4N `+1/δ)
n
)1/2 (5)
in O(m3 + nN2) time.
",4.1. Horton’s algorithm,[0],[0]
Proof.,4.1. Horton’s algorithm,[0],[0]
(5) follows directly from (2) in the proof of Theorem 1.,4.1. Horton’s algorithm,[0],[0]
"That same proof also shows that with probability at least 1 − δ, the support of GK and the signs of K are recovered up to a DN -similarity.",4.1. Horton’s algorithm,[0],[0]
What remains is to upper bound the worst case run time of Algorithm 1.,4.1. Horton’s algorithm,[0],[0]
We will perform this analysis line by line.,4.1. Horton’s algorithm,[0],[0]
Initializing K̂ requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Computing ∆S for all subsets |S| ≤ 2 requires O(nN2) operations.,4.1. Horton’s algorithm,[0],[0]
"Setting K̂i,i requires O(N) operations.",4.1. Horton’s algorithm,[0],[0]
"Computing B̂i,j for 1 ≤",4.1. Horton’s algorithm,[0],[0]
i < j ≤ N requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Forming K̃ requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
Forming GK requires O(N2) operations.,4.1. Horton’s algorithm,[0],[0]
"By
Lemma 3, computing a shortest maximal cycle basis requires O(mN) operations.",4.1. Horton’s algorithm,[0],[0]
"Constructing the subsets Si, 1 ≤ i ≤ νĜ, requires O(mN) operations.",4.1. Horton’s algorithm,[0],[0]
Computing ∆̂Si for 1 ≤,4.1. Horton’s algorithm,[0],[0]
i ≤ νĜ requires O(nm) operations.,4.1. Horton’s algorithm,[0],[0]
Computing ĈSi using det(K̃[Si]) for 1 ≤,4.1. Horton’s algorithm,[0],[0]
"i ≤ νĜ requires O(m`3) operations, where a factorization of each K̃[Si] is used to compute each determinant in O(`3) operations.",4.1. Horton’s algorithm,[0],[0]
Constructing A and b requires O(m`) operations.,4.1. Horton’s algorithm,[0],[0]
"By Lemma 4, finding a solution x using Gaussian elimination requires O(m3) operations.",4.1. Horton’s algorithm,[0],[0]
"Setting K̂i,j for all edges {i, j} ∈ E requires O(m) operations.",4.1. Horton’s algorithm,[0],[0]
"Put this all together, Algorithm 1 runs in O(m3 + nN2) time.",4.1. Horton’s algorithm,[0],[0]
Here we show that it is possible to obtain faster algorithms by exploiting the structure of GK .,4.2. Chordal Graphs,[0],[0]
"Specifically, in the case where GK chordal, we give an O(m) time algorithm to determine the signs of the off-diagonal entries of the estimator K̂, resulting in an improved overall runtime of O(m + nN2).",4.2. Chordal Graphs,[0],[0]
Recall that a graph G =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E) is said to be chordal if every induced cycle in G is of length three.",4.2. Chordal Graphs,[0],[0]
"Moreover, a graph G = ([N ], E) has a perfect elimination ordering (PEO) if there exists an ordering of the vertex set {v1, ..., vN} such that, for all i, the graph induced by {vi} ∪ {vj |{i, j} ∈ E, j > i} is a clique.",4.2. Chordal Graphs,[0],[0]
It is well known that a graph is chordal if and only if it has a PEO.,4.2. Chordal Graphs,[0],[0]
"A PEO of a chordal graph with m edges can be computed in O(m) operations using lexicographic breadth-first search (Rose et al., 1976).
",4.2. Chordal Graphs,[0],[0]
Lemma 5.,4.2. Chordal Graphs,[0],[0]
Let G =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E), be a chordal graph and {v1, ..., vn} be a PEO.",4.2. Chordal Graphs,[0],[0]
"Given i, let i∗",4.2. Chordal Graphs,[0],[0]
:,4.2. Chordal Graphs,[0],[0]
= min{j|j >,4.2. Chordal Graphs,[0],[0]
"i, {vi, vj} ∈ E}.",4.2. Chordal Graphs,[0],[0]
Then the graph G′ =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E′), where E′ = {{vi, vi∗}}N−κ(G)i=1 , is a spanning forest of G.
Proof.",4.2. Chordal Graphs,[0],[0]
"We first show that there are no cycles in G′. Suppose to the contrary, that there is an induced cycle C of length k on the vertices {vj1 , ..., vjk}.",4.2. Chordal Graphs,[0],[0]
Let v be the vertex of smallest index.,4.2. Chordal Graphs,[0],[0]
Then v is connected to two other vertices in the cycle of larger index.,4.2. Chordal Graphs,[0],[0]
"This is a contradiction to the construction.
",4.2. Chordal Graphs,[0],[0]
What remains is to show that |E′| = N − κ(G).,4.2. Chordal Graphs,[0],[0]
It suffices to prove the case κ(G) = 1.,4.2. Chordal Graphs,[0],[0]
"Suppose to the contrary, that there exists a vertex vi, i < N , with no neighbors of larger index.",4.2. Chordal Graphs,[0],[0]
Let P be the shortest path in G from vi to vN .,4.2. Chordal Graphs,[0],[0]
"By connectivity, such a path exists.",4.2. Chordal Graphs,[0],[0]
Let vk be the vertex of smallest index in the path.,4.2. Chordal Graphs,[0],[0]
"However, it has two neighbors in the path of larger index, which must be adjacent to each other.",4.2. Chordal Graphs,[0],[0]
"Therefore, there is a shorter path.
",4.2. Chordal Graphs,[0],[0]
"Now, given the chordal graph GK induced by K and the estimates of principal minors of size at most three, we provide an algorithm to determine the signs of the edges of
Algorithm 2 Compute Signs of Edges in Chordal Graph
Input: GK =",4.2. Chordal Graphs,[0],[0]
"([N ], E) chordal, ∆̂S for |S| ≤ 3.
",4.2. Chordal Graphs,[0],[0]
"Compute a PEO {v1, ..., vN}.",4.2. Chordal Graphs,[0],[0]
Compute the spanning forest G′ =,4.2. Chordal Graphs,[0],[0]
(,4.2. Chordal Graphs,[0],[0]
"[N ], E′).",4.2. Chordal Graphs,[0],[0]
Set all edges in E′ to have positive sign.,4.2. Chordal Graphs,[0],[0]
"Compute Ĉ{i,j,i∗} for all {i, j} ∈ E \ E′, j < i. Order edges E \",4.2. Chordal Graphs,[0],[0]
E′,4.2. Chordal Graphs,[0],[0]
"= {e1, ..., eν} such that",4.2. Chordal Graphs,[0],[0]
i,4.2. Chordal Graphs,[0],[0]
> j,4.2. Chordal Graphs,[0],[0]
if max ei < max ej .,4.2. Chordal Graphs,[0],[0]
"Visit edges in sorted order and for e = {i, j}, j >",4.2. Chordal Graphs,[0],[0]
"i, set
sgn({i, j}) = sgn(Ĉ{i,j,i∗}) sgn({i, i∗}) sgn({j, i∗}).
",4.2. Chordal Graphs,[0],[0]
"GK , or, equivalently, the off-diagonal entries of K.
Theorem 4.",4.2. Chordal Graphs,[0],[0]
"IfGK is chordal, Algorithm 2 correctly determines the signs of the edges of GK in O(m) time.
",4.2. Chordal Graphs,[0],[0]
Proof.,4.2. Chordal Graphs,[0],[0]
We will simultaneously perform a count of the operations and a proof of the correctness of the algorithm.,4.2. Chordal Graphs,[0],[0]
Computing a PEO requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
Computing the spanning forest requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
"The edges of the spanning tree can be given arbitrary sign, because it is a cycle-free graph.",4.2. Chordal Graphs,[0],[0]
This assigns a sign to two edges of each 3-cycle.,4.2. Chordal Graphs,[0],[0]
"Computing each Ĉ{i,j,i∗} requires a constant number of operations because ` = 3, requiring a total of O(m −N) operations.",4.2. Chordal Graphs,[0],[0]
Ordering the edges requires O(m) operations.,4.2. Chordal Graphs,[0],[0]
"Setting the signs of each remaining edge requires O(m) operations.
",4.2. Chordal Graphs,[0],[0]
"Therefore, when GK is chordal, the overall complexity required by our algorithm to compute K̂ is reduced to O(m+ nN2).",4.2. Chordal Graphs,[0],[0]
Here we present experiments to supplement the theoretical results of the paper.,5. Experiments,[0],[0]
We test our algorithm on two types of random matrices.,5. Experiments,[0],[0]
"First, we consider the matrix K ∈",5. Experiments,[0],[0]
"RN×N corresponding to the cycle on N vertices,
K = 1
2 I +
1 4 A,
where A is symmetric and has non-zero entries only on the edges of the cycle, either +1 or −1, each with probability 1/2.",5. Experiments,[0],[0]
"By the Gershgorin circle theorem, 0 K I .",5. Experiments,[0],[0]
"Next, we consider the matrix K ∈",5. Experiments,[0],[0]
"RN×N corresponding to the clique on N vertices,
K = 1
2 I +
1
4 √ N A,
where A is symmetric and has all entries either +1 or −1, each with probability 1/2.",5. Experiments,[0],[0]
It is well known that −2,5. Experiments,[0],[0]
"√ N A 2 √ N with high probability, implying 0 K I .
",5. Experiments,[0],[0]
"For both cases and for a range of values of matrix dimension N and samples n, we run our algorithm on 50 randomly generated instances.",5. Experiments,[0],[0]
"We record the proportion of trials where we recover the graph induced by K, and the proportion of the trials where we recover both the graph and correctly determine the signs of the entries.
",5. Experiments,[0],[0]
"In Figure 1, the shade of each box represents the proportion of trials where recovery was successful for a given pair N,n. A completely white box corresponds to zero success rate, black to a perfect success rate.
",5. Experiments,[0],[0]
The plots corresponding to the cycle and the clique are telling.,5. Experiments,[0],[0]
"We note that for the clique, recovering the sparsity pattern and recovering the signs of the off-diagonal entries come hand-in-hand.",5. Experiments,[0],[0]
"However, for the cycle, there is a noticeable gap between the number of samples required to recover the sparsity pattern and the number of samples required to recover the signs of the off-diagonal entries.",5. Experiments,[0],[0]
"This empirically confirms the central role that cycle sparsity plays in parameter estimation, and further corroborates our theoretical results.",5. Experiments,[0],[0]
"In this paper, we gave the first provable guarantees for learning the parameters of a DPP.",6. Conclusion and open questions,[0],[0]
"Our upper and lower bounds reveal the key role played by the parameter `, which is the cycle sparsity of graph induced by the kernel of the DPP.",6. Conclusion and open questions,[0],[0]
"Our estimator does not need to know ` beforehand, but can adapt to the instance.",6. Conclusion and open questions,[0],[0]
"Moreover, our procedure outputs an estimate of `, which could potentially be used for further inference questions such as testing and confidence intervals.",6. Conclusion and open questions,[0],[0]
"An interesting open question is whether on a graph by graph basis, the parameter ` exactly determines the optimal sample complexity.",6. Conclusion and open questions,[0],[0]
"Moreover when the number of samples is too small, can we exactly characterize which signs can be learned correctly and which cannot (up to a similarity transformation by D)?",6. Conclusion and open questions,[0],[0]
"Such results would lend new theoretical insights into the output of algorithms for learning DPPs, and which individual parameters in the estimate we can be confident about and which we cannot.
Acknowledgements.",6. Conclusion and open questions,[0],[0]
"A.M. is supported in part by NSF CAREER Award CCF-1453261, NSF Large CCF1565235, a David and Lucile Packard Fellowship and an Alfred P. Sloan Fellowship.",6. Conclusion and open questions,[0],[0]
"P.R. is supported in part by NSF CAREER DMS-1541099, NSF DMS-1541100, DARPA W911NF-16-1-0551, ONR N00014-17-1-2147 and a grant from the MIT NEC Corporation.",6. Conclusion and open questions,[0],[0]
"Determinantal Point Processes (DPPs) are a family of probabilistic models that have a repulsive behavior, and lend themselves naturally to many tasks in machine learning where returning a diverse set of objects is important.",abstractText,[0],[0]
"While there are fast algorithms for sampling, marginalization and conditioning, much less is known about learning the parameters of a DPP.",abstractText,[0],[0]
"Our contribution is twofold: (i) we establish the optimal sample complexity achievable in this problem and show that it is governed by a natural parameter, which we call the cycle sparsity; (ii) we propose a provably fast combinatorial algorithm that implements the method of moments efficiently and achieves optimal sample complexity.",abstractText,[0],[0]
"Finally, we give experimental results that confirm our theoretical findings.",abstractText,[0],[0]
Learning Determinantal Point Processes with Moments and Cycles,title,[0],[0]
"For well over a decade there has been extensive work on learning social network influence models (Liben-Nowell & Kleinberg, 2003; Netrapalli & Sanghavi, 2012; Abrahao et al., 2013; Friggeri et al., 2014; Anderson et al., 2015; Subbian et al., 2017), and the independent cascade model in particular (Saito et al., 2008; Gomez Rodriguez et al., 2010; Goyal et al., 2010; Gomez Rodriguez et al., 2011; Du et al., 2014; Lemonnier et al., 2014; Bourigault et al.,
*Equal contribution 1Department of Computer Science, Harvard University 2Facebook, Menlo Park.",1. Introduction,[0],[0]
"Correspondence to: Dimitris Kalimeris <kalimeris@g.harvard.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
2014; Narasimhan et al., 2015).",1. Introduction,[0],[0]
"Independent cascade (IC) was popularized by the seminal work of Kempe, Kleinberg, and Tardos (Kempe et al., 2003) and is a stochastic model that predicts the likelihood of information diffusing from one individual to another in a social network.",1. Introduction,[0],[0]
"In this model for every pair of individuals connected in the network u, v there is a probability pu,v that v adopts the behavior of u (i.e. information is diffused from u to v).
",1. Introduction,[0],[0]
The main challenge with learning the IC model is that the sample complexity is often overwhelmingly large or simply infeasible.,1. Introduction,[0],[0]
"To illustrate this point, Figure 1 shows the cumulative distribution of edge interactions for millions of public events on Facebook over varying periods of time, ranging from one week to two months (see detailed description of the dataset in Section 5).",1. Introduction,[0],[0]
The vertical line marks the minimal number of observations per edge required to infer the likelihood of influence with error 0.1 and confidence 95%.,1. Introduction,[0],[0]
"In this data set, more than 90% of the edges do not have enough observations to learn the respective diffusion probabilities accurately, even over a period of two months.",1. Introduction,[0],[0]
"Furthermore, in a single week (the timeframe in which inference about an event is often most relevant), none of the edges in the data set have sufficiently many observations.
",1. Introduction,[0],[0]
"Given that even with the data available on Facebook there are not enough observations to learn the model, one needs to impose additional assumptions.",1. Introduction,[0],[0]
"A natural approach is to assume that the diffusion probabilities are a function of network and individuals’ characteristics and some underlying
global hyperparameter θ.",1. Introduction,[0],[0]
"In the case of events for example, it seems reasonable that influence could be estimated as a function of some global unknown multidimensional parameter θ and individuals’ characteristics such as location, gender, and age, and topological features like the ratio between the intersection and the union of the individuals’ neighborhoods.",1. Introduction,[0],[0]
"Using xu,v to denote the characteristics of u and v, the hyperparametric approach assumes that the probability of u to influence v denoted pu,v is not arbitrary and can be faithfully estimated via some function p that maps θ and xu,v to [0, 1], i.e. pu,v = p(θ,xu,v).",1. Introduction,[0],[0]
"Given a set of characteristics, learning the IC model then reduces to recovering the underlying hyperparameter θ.
Intuitively, learning a hyperparametric model necessitates far fewer samples than a general diffusion model for two main reasons.",1. Introduction,[0],[0]
"First, since the diffusion probabilities are correlated, each observation provides information about all edges in the network.",1. Introduction,[0],[0]
"Second, it seems reasonable that the sample complexity of learning the hyperparameter should largely depend on the dimension of the hyperparameter rather than the number of edges the network.
",1. Introduction,[0],[0]
A simple example.,1. Introduction,[0],[0]
"To solidify our intuition, consider a simple bipartite network G = (U, V,E) where nodes in U attempt to activate nodes from V as depicted in Figure 1.1 and each activation attempt together with its outcome (label) constitutes one sample.",1. Introduction,[0],[0]
"Our goal is to find a p̂u,v for every edge (u, v) ∈",1. Introduction,[0],[0]
"E, s.t. with prob.",1. Introduction,[0],[0]
"at least 1− δ for all edges:
|pu,",1. Introduction,[0],[0]
"v − p̂u,v| ≤
Hoeffding’s inequality and a union bound imply that Θ ( |E| 2 log |E| δ ) samples are necessary and sufficient to learn the diffusion probabilities on all the edges in the graph.",1. Introduction,[0],[0]
"In comparison, suppose that the diffusion probability of each edge is a function of a hyperparameter θ ∈",1. Introduction,[0],[0]
"[0, 1]d and some known features of the edge xu,v ∈",1. Introduction,[0],[0]
"[0, 1]d as follows:
pu,v = 1
1 + e−〈θ,xu,v〉
Then, learning the diffusion probabilities becomes a logistic regression problem and thus only O ( d 2 log d δ )",1. Introduction,[0],[0]
"samples are required, independent of the number of edges.",1. Introduction,[0],[0]
"This reduces the sample complexity by a factor of |E|d which is quite dramatic when the number of edges in the network |E| is large and the dimension of the hyperparameter d is small.
",1. Introduction,[0],[0]
"Beyond potential improvements in sample complexity, a hyperparametric model is convenient due to the structure it imposes.",1. Introduction,[0],[0]
"A recent line of work on influence maximization in bandit models (Wen et al., 2015; Vaswani et al., 2017), assumes that the diffusion probabilities are a linear function of edge features, i.e. pu,v = 〈θ, xu,v〉 and this structure is leveraged in order to develop faster algorithms.",1. Introduction,[0],[0]
Our goal in this paper is to explore a hyperparametric approach for learning the independent cascade diffusion model.,1.1. A Hyperparametric Approach,[0],[0]
"Doing so requires addressing three open questions:
• Does restriction to a low-dimensional hyperparameter substantially decrease the sample complexity?",1.1. A Hyperparametric Approach,[0],[0]
"As discussed above, the motivation for a hyperparametric approach is that intuitively its sample complexity should depend on the dimension of the hyperparameter rather than the number of edges.",1.1. A Hyperparametric Approach,[0],[0]
"While intuitive, when the indegree of nodes is greater than 1, minimizing empirical risk becomes a non-convex optimization problem and analyzing sample complexity is not trivial; • Can a hyperparametric model be learned efficiently?",1.1. A Hyperparametric Approach,[0],[0]
"As learning a hyperparametric model requires solving a non-convex optimization problem, it is not clear it can be learned efficiently, in theory or in practice; 1 • Are low-dimensional hyperparametric models predictive?",1.1. A Hyperparametric Approach,[0],[0]
"Assuming that sample complexity heavily depends on its dimension, our approach is relevant only if reasonable estimates of the IC model are achievable with a low-dimensional hypothesis class.
",1.1. A Hyperparametric Approach,[0],[0]
In this paper we address the above questions.,1.1. A Hyperparametric Approach,[0],[0]
We first show that the sample complexity can indeed be dramatically reduced when restricting the hypothesis class to a lowdimensional hyperparameter.,1.1. A Hyperparametric Approach,[0],[0]
"Specifically, when comparing with the state-of-the-art bound for learning the independent cascade model (without the hyperparametric assumption) we show that the sample complexity can be reduced by a factor of |E|/d, as foreshadowed by the example above.
",1.1. A Hyperparametric Approach,[0],[0]
Despite being a non-concave optimization problem we show that the problem has a great deal of structure.,1.1. A Hyperparametric Approach,[0],[0]
"Under mild assumptions about the distribution generating the samples, we show how this structure can be leveraged to efficiently train a model with arbitrarily small generalization error.
",1.1. A Hyperparametric Approach,[0],[0]
"Lastly, we show that the hyperparametric approach does work in practice.",1.1. A Hyperparametric Approach,[0],[0]
"To do so, we ran experiments on large scale cascades recorded on the Facebook social network.",1.1. A Hyperparametric Approach,[0],[0]
"We show that with a hyperparameter of dimension 40 one
1We note that even without the hyperparametric assumption PAC learning the IC model is a non-convex optimization problem (Narasimhan et al., 2015).
can estimate the diffusion probabilities with remarkably high accuracy.",1.1. A Hyperparametric Approach,[0],[0]
"Naturally, there are data sets that do not contain individuals’ characteristics.",1.1. A Hyperparametric Approach,[0],[0]
"Nonetheless, most social networking services include useful information about its members that help predict diffusion, and the topology of the network alone often may serve as a good proxy.",1.1. A Hyperparametric Approach,[0],[0]
"A social network is a finite directed graph G = (V,E), where the set of nodes V represents the individuals in the network and the set of edges E represents their social links.
",2. The Hyperparametric Model,[0],[0]
Independent Cascade (IC) model.,2. The Hyperparametric Model,[0],[0]
The IC model assumes that every node v ∈ V can either be active or inactive.,2. The Hyperparametric Model,[0],[0]
All nodes begin as inactive.,2. The Hyperparametric Model,[0],[0]
"At time step t = 0 a subset of the nodes X called the seed becomes active, and activations in the network continue according to the following stochastic process: Each node u that became active at time step t = τ attempts to influence every one of its neighbors v only once at time step t = τ + 1 independently, and succeeds with some probability pu,v. A node v that became active during the process will never go back to being inactive.",2. The Hyperparametric Model,[0],[0]
"Our work generalizes the standard IC model by assuming that the probabilities pu,v are not arbitrary but correlated and specifically consequences of nodes’ features.",2. The Hyperparametric Model,[0],[0]
"This is the hyperparametric assumption, as described below.
Hyperparametrization.",2. The Hyperparametric Model,[0],[0]
Every node u ∈ V is associated with a vector of features containing information about it.,2. The Hyperparametric Model,[0],[0]
"Every edge (u, v) ∈",2. The Hyperparametric Model,[0],[0]
"E, is also associated with a feature vector, the concatenation of the feature vectors of its endpoints, denoted by xu,v. The diffusion probability of each edge is a function of a global hyperparameter θ and its feature vector.",2. The Hyperparametric Model,[0],[0]
"Formally, we assume that there exists a function p :",2. The Hyperparametric Model,[0],[0]
Rd × Rd,2. The Hyperparametric Model,[0],[0]
"→ [0, 1] s.t. pu,v = p(θ, xu,v) for any edge (u, v) ∈",2. The Hyperparametric Model,[0],[0]
E.,2. The Hyperparametric Model,[0],[0]
"In this work we define p as the sigmoid function:
pu,v = σ(θ, xu,v) = 1
1 + e−〈θ,xu,v〉 .
",2. The Hyperparametric Model,[0],[0]
We restrict the hyperparameter θ to lie in a hypothesis class H =,2. The Hyperparametric Model,[0],[0]
"[−B,B]d for some constant B > 0, and w.l.o.g.",2. The Hyperparametric Model,[0],[0]
"we assume that the feature vector of every edge lies in [0, 1]d.",2. The Hyperparametric Model,[0],[0]
"Additionally, we assume that pu,v is bounded away from 0 and 1 for all edges, i.e. pu,v ∈",2. The Hyperparametric Model,[0],[0]
"[λ, 1− λ], for some λ > 0.
",2. The Hyperparametric Model,[0],[0]
"Further discussion about the hyperparametric model and the choice of the sigmoid function can be found in Appendix A.
Samples.",2. The Hyperparametric Model,[0],[0]
The input to a learning algorithm is a collection of labeled samples.,2. The Hyperparametric Model,[0],[0]
"We assume that there is some unknown distribution D0 over subset of nodes, that activates the initial seed of the cascade, V0.",2. The Hyperparametric Model,[0],[0]
"Subsequently, as we discussed before, we can partition V \ V0 into subsets of nodes V1, V2, . . .",2. The Hyperparametric Model,[0],[0]
", Vn−1 that become activated at steps
τ = 1, 2, . . .",2. The Hyperparametric Model,[0],[0]
", n− 1, respectively 2.",2. The Hyperparametric Model,[0],[0]
"Notice that the cascade can be further decomposed into a sequence of simpler samples as follows: for every τ ∈ {0, 1, . . .",2. The Hyperparametric Model,[0],[0]
", n− 1} consider all the nodes v /∈",2. The Hyperparametric Model,[0],[0]
∪τ−1t=0 Vt that are within distance of 1 from Vτ .,2. The Hyperparametric Model,[0],[0]
"For every v that became activated by Vτ (i.e. v ∈ Vτ+1) create the sample ((Vτ , v), 1), and for every v that remained inactive create the sample ((Vτ , v), 0).",2. The Hyperparametric Model,[0],[0]
"Throughout this paper we assume that the input to our learning algorithm is of the form {(Xi, vi), yi}mi=1 where Xi ⊆ V is a subset of active nodes, vi is a node in distance 1 from Xi and yi ∈ {0, 1} is its label.",2. The Hyperparametric Model,[0],[0]
"In Appendix C we map every seed-generating distribution D0 to a sample-generating distribution D.
Log-likelihood of a sample.",2. The Hyperparametric Model,[0],[0]
"For every node v, the event “v becomes influenced by X , when the hyperparameter has value θ” is a Bernoulli random variable with probability of success fθv (X) = 1− ∏ u∈X∩N(v)(1−pu,v(θ))",2. The Hyperparametric Model,[0],[0]
"whereN(v) is the set of in-neighbors of node v. Hence, the likelihood of a sample s = ((X, v), y), where v /∈",2. The Hyperparametric Model,[0],[0]
"X is fθv (X)y · ( 1−
fθv (X) )1−y , and the respective log-likelihood of s is:
L(s, θ)",2. The Hyperparametric Model,[0],[0]
= y ln(fθv (X)),2. The Hyperparametric Model,[0],[0]
+ (1− y) ln(1− fθv (X)),2. The Hyperparametric Model,[0],[0]
"(1)
We want to recover a hyperparameter θ that yields accurate estimates.",2. The Hyperparametric Model,[0],[0]
"To do so, given a training set S = {si}mi=1, we seek the most probable hyperparameter generating S by maximizing the cumulative log-likelihood function:
θ̂ = arg max θ∈H
1
m m∑ i=1 L(si, θ) (2)
",2. The Hyperparametric Model,[0],[0]
Learning a diffusion model.,2. The Hyperparametric Model,[0],[0]
"Our goal is to bound the sample complexity, i.e. the number of i.i.d.",2. The Hyperparametric Model,[0],[0]
"samples generated by a distribution D that we need to observe to PAC learn H. That is, guarantee that supθ∈H Es∼D[L(s, θ)]",2. The Hyperparametric Model,[0],[0]
"− Es∼D[L(s, θ̂)] ≤ , with probability at least 1 − δ (see definition of PAC learnability in Appendix B).
",2. The Hyperparametric Model,[0],[0]
"Notice that while there are |E| edges in the network, which translates to |E| diffusion probabilities, in the optimization problem (2) there are only d parameters to be learned.
",2. The Hyperparametric Model,[0],[0]
We would like to note that PAC learning guarantees are required to hold for any distribution D that generates the data.,2. The Hyperparametric Model,[0],[0]
"Hence, it is easy to see that the diffusion probabilities or the hyperparameter θ are not learnable without extra assumptions on D. For details refer to Appendix D.",2. The Hyperparametric Model,[0],[0]
In this section we prove Theorem 2 which is the main technical result of the paper.,3. Learning a Hyperparametric Model,[0],[0]
The main takeaway is that a hyperparameteric approach makes learning an influence model feasible.,3. Learning a Hyperparametric Model,[0],[0]
"Informally, the theorem states that the number of samples required to ( , δ)-PAC learn the model is
2The influence process terminates after at most |V",3. Learning a Hyperparametric Model,[0],[0]
"| − 1 steps.
",3. Learning a Hyperparametric Model,[0],[0]
"Õ ( ∆2 ( ∆·d+log 1δ 2 )) , where ∆ is the maximum degree in
the network and d is the dimension of the hyperparameter.",3. Learning a Hyperparametric Model,[0],[0]
"As we later show in the experiments section, very small constant values of d suffice to learn an influence model with almost no error on real data.",3. Learning a Hyperparametric Model,[0],[0]
"This is in sharp contrast to the best sample complexity guarantees due to (Narasimhan et al., 2015) for learning the model without the hyperparametric assumption which is Õ ( ∆2 ( ∆·|E|+log 1δ 2 )) .
",3. Learning a Hyperparametric Model,[0],[0]
"Furthermore, imposing assumptions on the distribution D, can reduce the dependence on ∆ making the sample complexity (almost) independent of the size of the network.",3. Learning a Hyperparametric Model,[0],[0]
The main technical challenge is due to the fact that the MLE objective in (2) is non-concave,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
and we cannot immediately derive sample complexity bounds from convergence guarantees of Stochastic Gradient Descent for example.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Instead, to analyze the sample complexity we will argue about the Rademacher complexity of our hypothesis class by using covering numbers.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Informally, the Rademacher complexity measures the expressive power of a hypothesis classH with respect to a probability distribution and the covering number of a set is the number of balls of a certain radius whose union contains the set (see Definitions 2 and 3 in Appendix B).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Recall that the sample complexity of a hypothesis class can be derived from its Rademacher complexity.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Theorem 1 ((Shalev-Shwartz & Ben-David, 2014)).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Assume that for every sample s ∼ D and every θ ∈ H we have that: |L(s, θ)| ≤ C. Let S ∼ Dm and θ̂ = arg maxθ∈H { 1 m ∑m i=1",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"L (si, θ) } .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then with probability at least 1− δ over the choice of S we have that:
Es∼D[L(s, θ̂)]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"≥ sup θ∈H Es∼D[L(s, θ)]
−R(S,H)−O C √
log 1δ m  whereR(S,H) is the Rademacher complexity of the class H with respect to S.
Hence, our goal reduces to boundingR(S,H) for a training set S of size m. We do so by discretizingH by , and prove that if the discretization is dense enough, then we do not sacrifice a lot by searching for the most likely hyperparameter in the discrete spaceH instead of the continuousH.
To this end, we construct an -cover of the hypothesis class H =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"[−B,B]d.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Proving that the log-likelihood of any fixed sample s, is bounded and Lipschitz3 in θ with respect to
3intuitively for a Lipschitz function a small change in the argument cannot lead to a large change in the value of the function, see Definition 4, Appendix B.
the `1-norm, where the Lipschitz parameter depends on λ (Lemma 3 in Appendix E), allows us to translate the cover of the space of the hyperparameter, into a cover of the space of the log-likelihood functions, by slightly increasing the number of points we include in it, as stated in Lemma 1.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"The proof is deferred to Appendix E.
Lemma 1.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Let S = {((Xi, vi), yi)}mi=1 be a non-empty set of samples and let ∆S = maxs∈S |X∩N(v)| (maximum indegree of a node that was activated, across all samples).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"The covering number of the class of all log-likelihood functions for S is O (( Bρd λ∆S )d) , i.e. we can choose a discrete cover
H ⊆ H of size",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"O ((
Bρd λ∆S
)d) , such that for all θ ∈ H,
there exists a θ ∈ H with sup s∈S |L(s, θ)− L(s, θ )| ≤ .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Given the above lemma, we can invoke Massart’s lemma (Lemma 5 in Appendix E) onH which upper bounds the Rademacher complexity of finite hypothesis classes.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Subsequently, we use Lemma 4 (Appendix E) to upper bound the Rademacher complexity ofH from that ofH .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"We are now ready to prove the main theorem of the section.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Theorem 2 (Sample Complexity of MLE).,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Let G = (V,E) be a directed graph and D be a distribution that generates samples of the form s = ((X, v), y).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Let ∆ = maxs∼D |X ∩N(v)|.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then, for any , δ ∈ (0, 1) , if we use Maximum Likelihood Estimation on a training set of size m ≥ m( , δ) =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
O ( ∆2 log2(1/λ)d log(Bρd/λ ∆ ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
+log(1/δ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"2
) samples drawn i.i.d.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"from D, with probability at least 1− δ (over the draw of the training set) it holds:
sup θ∈H
Es∼D[L ( s, θ ) ]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"− Es∼D[L ( s, θ̂ ) ] ≤ .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
Proof.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Define ∆ := maxs∼D |X ∩N(v)|, i.e. the maximum active in-degree that any sample generated by D can have.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Then, applying Lemma 1 we can create a discrete -cover of the space of the log-likelihoods, H ⊆ H, of size |H",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
| =,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
O (( Bρd λ∆ )d) for any training set S of any size.,3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Invoking Lemma 4 (Appendix E), we can associate the Rademacher complexity ofH with that of its coverH , for any S and any > 0, as follows:
R(S,H) ≤",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"R(S,H ) + 2 .
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Hence, we can focus on bounding the Rademacher complexity of H instead of that of H. Since H is finite, the well-known Massart’s lemma apply yielding:
R(S,H) ≤",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"R(S,H ) + 2
≤ 2 max θ∈H ∣∣∣∣∣∣(L(si, θ))mi=1∣∣∣∣∣∣2",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"√ 2 log(|H |) m + 2
≤ 2 √ m∆",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"ln 1 λ · √ 2 log(|H |) m + 2
= 2∆ ln 1
λ
√ 2 log(|H |)
m + 2
= O ( ∆ log 1
λ
√ d log(Bρd/λ∆ )
m
)",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"+ 2
where the first inequality holds because of Lemma 4 (discretization), the second because of Massart’s lemma (finite hypothesis class), the third because of Lemma 3 (bounded L), and the last one because of Lemma 1 (covering number).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Setting = 1/m yields: R(S,H) =",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"O ( ∆ log(1/λ) √
d log(Bρdm/λ∆) m
) .",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Now using the
generalization bound of Theorem 1, one can see that in order to achieve Es∼D[L(s, θ̂)]",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"≥ supθ∈H Es∼D[L(s, θ)]− , with probability at least 1 − δ, we need S to be of size m = O",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
( ∆2 log2(1/λ)d log(Bρd/λ ∆ ),3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"+log(1/δ) 2 ) , which
concludes the proof.
",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Given that B and λ are constants the above bound simplifies to Õ ( ∆2 ( ∆·d+log 1δ 2 )) , which allows immediate compar-
ison with the bounds derived in (Narasimhan et al., 2015).",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"Additionally, when the degree of every node is constant (which is the case for real social networks like Facebook) or when the distribution D activates only seeds of constant size, ∆ is a constant and the sample complexity becomes Õ ( d+log 1δ 2 ) , independent of the size of the network.",3.1. Sample Complexity via Radamacher Complexity,[0],[0]
"As we mentioned in the previous section, the maximization problem (2) is non-concave and it cannot be solved efficiently.",4. Algorithms,[0],[0]
"However, the cumulative log-likelihood function we aim to optimize has a great deal of structure we can utilize.
",4. Algorithms,[0],[0]
"To understand this structure, note that there are only three distinct cases for a sample s = ((X, v), y) in the training set S: (i) node v was not influenced, (ii) node v was influenced and there is only one neighbor of v in X and (iii) node v was influenced and there is more than one neighbor of v in X .",4. Algorithms,[0],[0]
The only case that makes the respective log-likelihood non-concave is (iii) since we are unable to identify which of the parents of v actually influenced it and how to update the hyperparameter (equation (1) yields that formally).,4. Algorithms,[0],[0]
"We refer to such samples as obfuscated.
",4. Algorithms,[0],[0]
One can partition S into So and S \,4. Algorithms,[0],[0]
So where So contains the obfuscated samples.,4. Algorithms,[0],[0]
"We can then write f̃(θ) := 1m ∑ s∈S L(s, θ) = 1 m ∑ s∈S\So L(s, θ) + 1 m ∑ s∈So L(s, θ) =: f(θ) + ξ(θ).",4. Algorithms,[0],[0]
Optimizing f̃ can be perceived as optimizing a concave function f under noise ξ.,4. Algorithms,[0],[0]
"The magnitude of the noise depends on the probability of seeing an obfuscated sample, which characterizes the difficulty of the optimization problem and can be computed in simple cases (see e.g. Lemma 8 in Appendix F).
",4. Algorithms,[0],[0]
"There are three distinct approaches that we can follow:
1.",4. Algorithms,[0],[0]
"Ignore the obfuscated samples and optimize f instead of f̃ , using standard methods like Gradient Descent.",4. Algorithms,[0],[0]
The fact that the likelihood of each sample is bounded (Lemma 3 in Appendix E) will assure that the recovered solution will approximately optimize f̃ as well.,4. Algorithms,[0],[0]
2.,4. Algorithms,[0],[0]
"Optimize f̃ directly by applying techniques from (Belloni et al., 2015) for concave optimization under noise.",4. Algorithms,[0],[0]
3.,4. Algorithms,[0],[0]
"Attempt to optimize f̃ using standard concave optimization techniques (for example Stochastic Gradient Descent (SGD) which is widely used in the training of deep networks, a non-convex optimization problem).
",4. Algorithms,[0],[0]
"The first two methods provide theoretical guarantees for noise of small magnitude, if the noise is large however, they can lead to large error.",4. Algorithms,[0],[0]
See Appendix F for a detailed description of these two approaches.,4. Algorithms,[0],[0]
"The third heuristic approach works remarkably well in practice, even when the noise is large, as the experiments of Section 5.1 demonstrate.",4. Algorithms,[0],[0]
"Additionally, in Section 5.2 we include experiments indicating that, even if the noise is small, it is still in our best interest to utilize all the available samples since the shortage of the training set hurts us more than non-concavity.",4. Algorithms,[0],[0]
We conduct two sets of experiments.,5. Experiments,[0],[0]
"First, using synthetic datasets we show that if the hyperparametric assumption holds in a network, we can accurately learn the edge probabilities despite the non-concavity of (2), and significantly outperform methods that do not include information about the node features, for small training sets4.",5. Experiments,[0],[0]
We also investigate which properties of the network and the model affect the convergence rate.,5. Experiments,[0],[0]
"Secondly, we validate our approach using real Facebook data, by showing that low-dimensional hyperparametric models are predictive in practice.",5. Experiments,[0],[0]
"Real Graphs: We also use the “ego-facebook”, “wiki-Vote”, “bitcoin-otc” and “bitcoin-alpha” datasets from (Leskovec & Krevl, 2014), which are publicly available real-world social networks, enabling the reproducibility of our experiments.
",5.1. Learning the Diffusion Probabilities,[0],[0]
Graphs.,5.1. Learning the Diffusion Probabilities,[0],[0]
"We synthetically generate the social graph and the hyperparameter that determines the diffusion probabilities.
",5.1. Learning the Diffusion Probabilities,[0],[0]
Synthetic Graphs: We simulate a social network using standard graph models.,5.1. Learning the Diffusion Probabilities,[0],[0]
"Since different models yield graphs with different topology, we selected four of the widely used ones: Barabási-Albert, Kronecker, Erdös-Rényi and the con-
4Notice that learning the diffusion probabilities allows us to compute other quantities of interest as well, like the probability of a node becoming influenced, the final size of a cascade initiated from a given set, or the influence function.
figuration model.",5.1. Learning the Diffusion Probabilities,[0],[0]
"For a more detailed description of these models and the construction process refer to Appendix G.
Experimental setup.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We generate 15 random features in [0, 1] for every node (we found consistent results across a large range of features).",5.1. Learning the Diffusion Probabilities,[0],[0]
"We define the first 10 of them to be the ones in which the hyperparametric assumption is based, and the rest is redundant information (30 features for each edge, where only 20 of them are important).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Additionally, we generate a random hyperparameter in [−1, 1]d (d = 20).",5.1. Learning the Diffusion Probabilities,[0],[0]
"We use the sigmoid function over the important features of each edge e and the hyperparameter to compute pe, as described in Section 2, imposing correlation between the probabilities of different edges by construction.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Subsequently, we generate 100,000 samples, and attempt to solve the optimization problem (2) using SGD, initializing the hyperparameter to 0 and using a learning rate of 1/ √ T , where T is the size of the training set.",5.1. Learning the Diffusion Probabilities,[0],[0]
"Details on how we create the training set can be found in Appendix G.
Benchmarks.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We tested the hyperparametric model against the following benchmarks: • Omniscient MLE: The true diffusion probability of an
edge is approximated by p̂e = n+e /ne, where n + e is the number of activations of edge e, while ne is the total number of exposures of e (activation attempts).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Here, we assume that for every sample ((X, v), y) we observe the activation or not of all the edges e = (u, v), where u is an active neighbor of v.",5.1. Learning the Diffusion Probabilities,[0],[0]
"This is a strong benchmark since in practice, we can observe whether v became active but not which node activated it.",5.1. Learning the Diffusion Probabilities,[0],[0]
"• Non-hyperparametric MLE: We implemented the algorithm of (Narasimhan et al., 2015), that allows one
to learn the diffusion probabilities only by observing whether a node v was influenced by the seed X or not.",5.1. Learning the Diffusion Probabilities,[0],[0]
"• Hyperparametric MLE, reduced information: We com-
pare ourselves against a hyperparametric model that is unaware of the exact features that are important, and selects only a subset of them.",5.1. Learning the Diffusion Probabilities,[0],[0]
Here we select only 5 out of 10 important features of every node.,5.1. Learning the Diffusion Probabilities,[0],[0]
"• Hyperparametric MLE, augmented information: Similarly, we compare ourselves against a hyperparametric model that is unaware of the important features, thus it selects all the available ones (15 features per node).
Results.",5.1. Learning the Diffusion Probabilities,[0],[0]
"We repeat each experiment 10 times, and provide the mean and the standard deviation in Figure 3.",5.1. Learning the Diffusion Probabilities,[0],[0]
"The y-axis corresponds to 1|E| ∑ e∈E |pe − p̂e|, the average absolute error between the real probability (known by construction) and the empirical one across the network.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In all networks, the hyperparametric approach greatly outperforms the nonhyperparametric benchmarks, even assuming omniscience.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Note that in the non-hyperparametric benchmarks, since samples do not carry global information, there exist edges that have no exposures given the samples that we have seen so far.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In that case, we define p̂e = 0.",5.1. Learning the Diffusion Probabilities,[0],[0]
"This explains the initial increase in the error in the omniscient MLE since, if pe is small and the first exposure of edge e is an activation, the error on e increases from pe to 1 − pe.",5.1. Learning the Diffusion Probabilities,[0],[0]
"Once we see enough samples, p̂e converges to pe.",5.1. Learning the Diffusion Probabilities,[0],[0]
"One can also notice, the effect of omniscience since it leads to faster convergence than actual implementable non-hyperparametric methods.
",5.1. Learning the Diffusion Probabilities,[0],[0]
"Regarding the two benchmarks that involve the hyperparametric assumption it is worth noting that reduced information does not allow convergence to 0 error, while augmented
information does (see also Figure 4d for a more detailed investigation).",5.1. Learning the Diffusion Probabilities,[0],[0]
"Finally, the initial difference in the errors of different networks is related to how good predictor the initialization of SGD is (i.e. θ = 0), as well as how large the average diffusion probability in the network is.",5.1. Learning the Diffusion Probabilities,[0],[0]
"The learning effect is evident and universal though, since the error converges to 0 independently of the underlying network.",5.1. Learning the Diffusion Probabilities,[0],[0]
"In these experiments we use Erdös-Rényi graphs with 1000 nodes and 20000 edges (unless otherwise stated).
",5.2. Convergence Rate Investigation,[0],[0]
Samples vs Concavity.,5.2. Convergence Rate Investigation,[0],[0]
In Section 4 we classified the samples into categories based on whether they yield concave log-likelihood or not.,5.2. Convergence Rate Investigation,[0],[0]
"Recall that if we ignore the obfuscated samples, the optimization problem becomes concave.",5.2. Convergence Rate Investigation,[0],[0]
A natural question is whether sacrificing samples for concavity leads to faster convergence.,5.2. Convergence Rate Investigation,[0],[0]
"To this end, we generate samples and if a sample is obfuscated, we discard it with probability p ∈ {0.0, 0.25, 0.5, 0.75, 1.0}.",5.2. Convergence Rate Investigation,[0],[0]
The results can be found in Figure 4a.,5.2. Convergence Rate Investigation,[0],[0]
"It is evident that even though our optimization problem becomes concave and hence theoretically easier to solve, the price due to data shortage is huge.
",5.2. Convergence Rate Investigation,[0],[0]
Approximate models.,5.2. Convergence Rate Investigation,[0],[0]
Here we investigate how properties of the graph or the model affect the convergence rate.,5.2. Convergence Rate Investigation,[0],[0]
"• Graph Density: We create an Erdös-Rényi graph with
1000 nodes and varying number of edges, exploring how does graph density affect the convergence of the error.",5.2. Convergence Rate Investigation,[0],[0]
"As the density increases, so does the average degree in the network and, as a result, the number of obfuscated samples.",5.2. Convergence Rate Investigation,[0],[0]
"Hence, the information obtained becomes “noisier” and convergence is slower.",5.2. Convergence Rate Investigation,[0],[0]
• Noisy model: Until now we assumed that the hyperparametric model is the ground truth.,5.2. Convergence Rate Investigation,[0],[0]
Here we relax this assumption.,5.2. Convergence Rate Investigation,[0],[0]
"We generate each edge probability as before, and subsequently add noise uniform in [−N,N ], for increasing values ofN .",5.2. Convergence Rate Investigation,[0],[0]
"Now, the average error does not converge to 0 and increases with N .",5.2. Convergence Rate Investigation,[0],[0]
•,5.2. Convergence Rate Investigation,[0],[0]
"Features Effect: In many cases we might not know the exact features that support the hyperparametric model.
",5.2. Convergence Rate Investigation,[0],[0]
We explore the effect of this lack of information by including varying number of significant features in our model.,5.2. Convergence Rate Investigation,[0],[0]
"Our results show that in terms of convergence more information does not hurt, despite being more costly computationally.",5.2. Convergence Rate Investigation,[0],[0]
"However, if we fail to include all the significant features, we do not converge to 0 error, and the error grows with the removal of features.
",5.2. Convergence Rate Investigation,[0],[0]
"The results can be found in Figure 4b-d. In all the cases the way that we enforced the hyperparametric assumption, created the samples and ran SGD is the same as in Section 5.1.",5.2. Convergence Rate Investigation,[0],[0]
"Importantly, we evaluate the validity of the hyperparametric assumption on real cascade data.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"To this end, we use the following aggregated and public Facebook data sets containing only de-identified data (i.e. they don’t include personally identifying information about individuals in the dataset).
Events.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"In Facebook a user can invite a set of other users for an event, who can then forward the invite to their friends to join.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Moreover, when friends join an event a user may be notified in their Facebook feed and join in turn.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
The cascade in this scenario is an event and an exposure is either a direct invite or a feed notification.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
A user is influenced if she marked herself as “going” to the event.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
This dataset is a random sample of events that happened over a twomonth period in late 2017.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
We have included only public events that are visible to everyone and also excluded users who created the event or joined without being invited.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The number of cascades in our dataset is roughly 3 million with 90 million users participating and 130 million exposures.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Video and Photo Reshares.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
A cascade in this dataset is a video or photo content.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
Whenever a user watches a video (photo) posted from a friend we consider it as an exposure and when the user shares that video (photo) after watching we consider it as an adoption (we consider only videos that were explicitly seen and not auto-played).,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"We collected a random sample of photo/video data on a random day in January 2018, and included only public photo and video
posts.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Our data sets contain roughly 10 million cascades with more than 100 million users and 500 million exposures.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Experimental setup.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The features that we include in our model in both cases are user attributes such as Facebook age in days, friend count, number of initiated friendship requests, subscriber count and subscription count, city, country, and language, number of days active in last 7 days, and 28 days.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
The categorical features were binarized in the model.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
An important difference with the experiments of Sections 5.1 and 5.2 is that here we don’t know the true diffusion probability of every edge by construction.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Instead, we estimate it from samples as p̂e = n+e ne
.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"In order to estimate p̂e accurately, we need enough samples for edge e. Hence, we restrict our evaluation set (the set of edges where we measure the error) only to edges that have at least 67 interactions, meaning that |pe − p̂e| ≤ 0.15 with probability at least 90%.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Each experiment is repeated 50 times and the averages together with the standard deviations are reported in Figures 5 and 6.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
Results.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
Our first set of experiments is to validate the hyperparametric assumption in real data.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"We observe that using the optimization problem (2), with very few samples, the hyperparametric model achieves significant reduction in average error (up to 60%) over methods that don’t utilize node features.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The results, reported in Fig. 5, are consistent with our synthetic experiments, where the hyperparametric assumption holds by construction.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"Note that the non-hyperparametric methods will eventually converge to zero error as they correspond to the ground truth while the hyperparametric model is only a good approximation of it.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
We also included reduced and augmented hyperparametric models for comparison as in the synthetic experiments.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
In the case of the reduced model we used only 20% of the most important features of each edge (measured using Mutual Information).,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"For the augmented version on the other hand, we augment the feature vector of each node with redundant information (increase its dimension by 50% and fill the extra coordinates with random noise) and investigate whether convergence still occurs.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"As in the experiments of Section 5.1, the reduced model converges to higher average error than the models that use more information, while the augmented model successfully ignores all the redundant features.
",5.3. Are Low-dimensional Models Predictive?,[0],[0]
We also evaluated the sensitivity of the hyperparametric model when we include all versus few selected features.,5.3. Are Low-dimensional Models Predictive?,[0],[0]
"The picture that we see matches the synthetic experiments (Figure 4d), i.e. the hyperparametric model is supported on several features and if we fail to include all of them our error won’t converge to 0.",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"However, an important difference with the synthetic experiments is that here not all the features are equally important, hence by applying feature-selection algorithms we can collect a small subset that performs almost as well as using the entire feature vector (see e.g. the difference in the error between 20% and 90% of the features).",5.3. Are Low-dimensional Models Predictive?,[0],[0]
"This research was supported by NSF grant CAREER CCF1452961, BSF grant 2014389, NSF USICCS proposal 1540428, and a Facebook research award.",6. Acknowledgements,[0],[0]
In this paper we advocate for a hyperparametric approach to learn diffusion in the independent cascade (IC) model.,abstractText,[0],[0]
The sample complexity of this model is a function of the number of edges in the network and consequently learning becomes infeasible when the network is large.,abstractText,[0],[0]
We study a natural restriction of the hypothesis class using additional information available in order to dramatically reduce the sample complexity of the learning process.,abstractText,[0],[0]
In particular we assume that diffusion probabilities can be described as a function of a global hyperparameter and features of the individuals in the network.,abstractText,[0],[0]
One of the main challenges with this approach is that training a model reduces to optimizing a non-convex objective.,abstractText,[0],[0]
"Despite this obstacle, we can shrink the best-known sample complexity bound for learning IC by a factor of |E|/d where |E| is the number of edges in the graph and d is the dimension of the hyperparameter.",abstractText,[0],[0]
We show that under mild assumptions about the distribution generating the samples one can provably train a model with low generalization error.,abstractText,[0],[0]
"Finally, we use large-scale diffusion data from Facebook to show that a hyperparametric model using approximately 20 features per node achieves remarkably high accuracy.",abstractText,[0],[0]
Learning Diffusion using Hyperparameters,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 654–664 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1061",text,[0],[0]
"The dialog manager is one of the key components of dialog systems, which is responsible for modeling the decision-making process.",1 Introduction,[0],[0]
"Specifically, it typically takes a new utterance and the dialog context as input, and generates discourse-level decisions (Bohus and Rudnicky, 2003; Williams and Young, 2007).",1 Introduction,[0],[0]
"Advanced dialog managers usually have a list of potential actions that enable them to have diverse behavior during a conversation, e.g. different strategies to recover from non-understanding (Yu et al., 2016).",1 Introduction,[0],[0]
"However, the conventional approach of designing a dialog manager (Williams and Young, 2007) does not
scale well to open-domain conversation models because of the vast quantity of possible decisions.",1 Introduction,[0],[0]
"Thus, there has been a growing interest in applying encoder-decoder models (Sutskever et al., 2014) for modeling open-domain conversation (Vinyals and Le, 2015; Serban et al., 2016a).",1 Introduction,[0],[0]
"The basic approach treats a conversation as a transduction task, in which the dialog history is the source sequence and the next response is the target sequence.",1 Introduction,[0],[0]
"The model is then trained end-to-end on large conversation corpora using the maximum-likelihood estimation (MLE) objective without the need for manual crafting.
",1 Introduction,[0],[0]
"However recent research has found that encoder-decoder models tend to generate generic and dull responses (e.g., I don’t know), rather than meaningful and specific answers (Li et al., 2015; Serban et al., 2016b).",1 Introduction,[0],[0]
"There have been many attempts to explain and solve this limitation, and they can be broadly divided into two categories (see Section 2 for details): (1) the first category argues that the dialog history is only one of the factors that decide the next response.",1 Introduction,[0],[0]
"Other features should be extracted and provided to the models as conditionals in order to generate more specific responses (Xing et al., 2016; Li et al., 2016a); (2) the second category aims to improve the encoder-decoder model itself, including decoding with beam search and its variations (Wiseman and Rush, 2016), encouraging responses that have long-term payoff (Li et al., 2016b), etc.
",1 Introduction,[0],[0]
"Building upon the past work in dialog managers and encoder-decoder models, the key idea of this paper is to model dialogs as a one-to-many problem at the discourse level.",1 Introduction,[0],[0]
"Previous studies indicate that there are many factors in open-domain dialogs that decide the next response, and it is nontrivial to extract all of them.",1 Introduction,[0],[0]
"Intuitively, given a similar dialog history (and other observed inputs), there may exist many valid responses (at the
654
discourse-level), each corresponding to a certain configuration of the latent variables that are not presented in the input.",1 Introduction,[0],[0]
"To uncover the potential responses, we strive to model a probabilistic distribution over the distributed utterance embeddings of the potential responses using a latent variable (Figure 1).",1 Introduction,[0],[0]
"This allows us to generate diverse responses by drawing samples from the learned distribution and reconstruct their words via a decoder neural network.
",1 Introduction,[0],[0]
"Specifically, our contributions are three-fold: 1.",1 Introduction,[0],[0]
"We present a novel neural dialog model adapted from conditional variational autoencoders (CVAE) (Yan et al., 2015; Sohn et al., 2015), which introduces a latent variable that can capture discourse-level variations as described above 2.",1 Introduction,[0],[0]
"We propose Knowledge-Guided CVAE (kgCVAE), which enables easy integration of expert knowledge and results in performance improvement and model interpretability.",1 Introduction,[0],[0]
3.,1 Introduction,[0],[0]
"We develop a training method in addressing the difficulty of optimizing CVAE for natural language generation (Bowman et al., 2015).",1 Introduction,[0],[0]
"We evaluate our models on human-human conversation data and yield promising results in: (a) generating appropriate and discourse-level diverse responses, and (b) showing that the proposed training method is more effective than the previous techniques.",1 Introduction,[0],[0]
Our work is related to both recent advancement in encoder-decoder dialog models and generative models based on CVAE.,2 Related Work,[0],[0]
"Since the emergence of the neural dialog model, the problem of output diversity has received much attention in the research community.",2.1 Encoder-decoder Dialog Models,[0],[0]
Ideal output responses should be both coherent and diverse.,2.1 Encoder-decoder Dialog Models,[0],[0]
"However, most models end up with generic and dull responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"To tackle this problem, one line of research has focused on augmenting the input of encoder-decoder models with richer context information, in order to generate more spe-
cific responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Li et al., (2016a) captured speakers’ characteristics by encoding background information and speaking style into the distributed embeddings, which are used to re-rank the generated response from an encoder-decoder model.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Xing et al., (2016) maintain topic encoding based on Latent Dirichlet Allocation (LDA) (Blei et al., 2003) of the conversation to encourage the model to output more topic coherent responses.
",2.1 Encoder-decoder Dialog Models,[0],[0]
"On the other hand, many attempts have also been made to improve the architecture of encoderdecoder models.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Li et al,.",2.1 Encoder-decoder Dialog Models,[0],[0]
"(2015) proposed to optimize the standard encoder-decoder by maximizing the mutual information between input and output, which in turn reduces generic responses.",2.1 Encoder-decoder Dialog Models,[0],[0]
"This approach penalized unconditionally high frequency responses, and favored responses that have high conditional probability given the input.",2.1 Encoder-decoder Dialog Models,[0],[0]
Wiseman and Rush (2016) focused on improving the decoder network by alleviating the biases between training and testing.,2.1 Encoder-decoder Dialog Models,[0],[0]
They introduced a searchbased loss that directly optimizes the networks for beam search decoding.,2.1 Encoder-decoder Dialog Models,[0],[0]
"The resulting model achieves better performance on word ordering, parsing and machine translation.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Besides improving beam search, Li et al., (2016b) pointed out that the MLE objective of an encoder-decoder model is unable to approximate the real-world goal of the conversation.",2.1 Encoder-decoder Dialog Models,[0],[0]
"Thus, they initialized a encoderdecoder model with MLE objective and leveraged reinforcement learning to fine tune the model by optimizing three heuristic rewards functions: informativity, coherence, and ease of answering.",2.1 Encoder-decoder Dialog Models,[0],[0]
"The variational autoencoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) is one of the most popular frameworks for image generation.",2.2 Conditional Variational Autoencoder,[0],[0]
The basic idea of VAE is to encode the input x into a probability distribution z instead of a point encoding in the autoencoder.,2.2 Conditional Variational Autoencoder,[0],[0]
"Then VAE applies a decoder network to reconstruct the original input using samples from z. To generate images, VAE first obtains a sample of z from the prior distribution, e.g. N (0, I), and then produces an image via the decoder network.",2.2 Conditional Variational Autoencoder,[0],[0]
"A more advanced model, the conditional VAE (CVAE), is a recent modification of VAE to generate diverse images conditioned on certain attributes, e.g. generating different human faces given skin color (Yan et al., 2015; Sohn et al., 2015).",2.2 Conditional Variational Autoencoder,[0],[0]
"Inspired by CVAE, we view the dialog contexts as the conditional attributes and adapt CVAE
to generate diverse responses instead of images.",2.2 Conditional Variational Autoencoder,[0],[0]
"Although VAE/CVAE has achieved impressive results in image generation, adapting this to natural language generators is non-trivial.",2.2 Conditional Variational Autoencoder,[0],[0]
"Bowman et al., (2015) have used VAE with Long-Short Term Memory (LSTM)-based recognition and decoder networks to generate sentences from a latent Gaussian variable.",2.2 Conditional Variational Autoencoder,[0],[0]
They showed that their model is able to generate diverse sentences with even a greedy LSTM decoder.,2.2 Conditional Variational Autoencoder,[0],[0]
They also reported the difficulty of training because the LSTM decoder tends to ignore the latent variable.,2.2 Conditional Variational Autoencoder,[0],[0]
We refer to this issue as the vanishing latent variable problem.,2.2 Conditional Variational Autoencoder,[0],[0]
"Serban et al., (2016b) have applied a latent variable hierarchical encoder-decoder dialog model to introduce utterance-level variations and facilitate longer responses.",2.2 Conditional Variational Autoencoder,[0],[0]
"To improve upon the past models, we firstly introduce a novel mechanism to leverage linguistic knowledge in training end-to-end neural dialog models, and we also propose a novel training technique that mitigates the vanishing latent variable problem.",2.2 Conditional Variational Autoencoder,[0],[0]
"Each dyadic conversation can be represented via three random variables: the dialog context c (context window size k − 1), the response utterance x (the kth utterance) and a latent variable z, which is used to capture the latent distribution over the valid responses.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Further, c is composed of the dialog history: the preceding k-1 utterances; conversational floor (1 if the utterance is from the same speaker of x, otherwise 0) and meta features m (e.g. the topic).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We then define the conditional distribution p(x, z|c)",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"= p(x|z, c)p(z|c) and our goal is to use deep neural networks (parametrized by θ) to approximate p(z|c) and p(x|z, c).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We refer to pθ(z|c) as the prior network and pθ(x, |z, c) as the
response decoder.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Then the generative process of x is (Figure 2 (a)):
1.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Sample a latent variable z from the prior network pθ(z|c).
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
2.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Generate x through the response decoder pθ(x|z, c).
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"CVAE is trained to maximize the conditional log likelihood of x given c, which involves an intractable marginalization over the latent variable z. As proposed in (Sohn et al., 2015; Yan et al., 2015), CVAE can be efficiently trained with the Stochastic Gradient Variational Bayes (SGVB) framework (Kingma and Welling, 2013) by maximizing the variational lower bound of the conditional log likelihood.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We assume the z follows multivariate Gaussian distribution with a diagonal covariance matrix and introduce a recognition network qφ(z|x, c) to approximate the true posterior distribution p(z|x, c).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Sohn and et al,.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"(2015) have shown that the variational lower bound can be written as:
L(θ, φ;x, c) = −KL(qφ(z|x, c)‖pθ(z|c))",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"+Eqφ(z|c,x)[log pθ(x|z, c)] (1) ≤ log p(x|c)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
Figure 3 demonstrates an overview of our model.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"The utterance encoder is a bidirectional recurrent neural network (BRNN) (Schuster and Paliwal, 1997) with a gated recurrent unit (GRU) (Chung et al., 2014) to encode each utterance into fixedsize vectors by concatenating the last hidden states of the forward and backward RNN ui =",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"[~hi, ~hi]. x is simply uk.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The context encoder is a 1-layer GRU network that encodes the preceding k-1 utterances by taking u1:k−1 and the corresponding conversation floor as inputs.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The last hidden state hc of the context encoder is concatenated with meta features and c =,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"[hc,m].",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Since we assume z follows isotropic Gaussian distribution, the recognition network qφ(z|x, c) ∼ N (µ, σ2I) and the prior network pθ(z|c) ∼ N (µ′, σ′2I), and then we have:
[ µ
log(σ2)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"] =Wr [ x c ] + br (2)
[ µ′
log(σ′2)
] = MLPp(c) (3)
",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"We then use the reparametrization trick (Kingma and Welling, 2013) to obtain samples of z either
from N (z;µ, σ2I) predicted by the recognition network (training) or N (z;µ′, σ′2I) predicted by the prior network (testing).",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"Finally, the response decoder is a 1-layer GRU network with initial state s0 =Wi[z, c]+bi.",3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
The response decoder then predicts the words in x sequentially.,3.1 Conditional Variational Autoencoder (CVAE) for Dialog Generation,[0],[0]
"In practice, training CVAE is a challenging optimization problem and often requires large amount of data.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"On the other hand, past research in spoken dialog systems and discourse analysis has suggested that many linguistic cues capture crucial features in representing natural conversation.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"For example, dialog acts (Poesio and Traum, 1998) have been widely used in the dialog managers (Litman and Allen, 1987; Raux et al., 2005; Zhao and Eskenazi, 2016) to represent the propositional function of the system.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Therefore, we conjecture that it will be beneficial for the model to learn meaningful latent z",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"if it is provided with explicitly extracted discourse features during the training.
",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In order to incorporate the linguistic features into the basic CVAE model, we first denote the set of linguistic features as y. Then we assume that the generation of x depends on c, z and y. y relies on z and c as shown in Figure 2.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Specifically, during training the initial state of the response decoder is s0 = Wi[z, c, y] + bi and the input at every step is [et, y] where et is the word embedding of tth word in x.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In addition, there is an MLP to predict y′ = MLPy(z, c) based on z and c.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"In the testing stage, the predicted y′ is used by the response decoder instead of the oracle decoders.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"We denote the modified model as knowledge-guided
CVAE (kgCVAE) and developers can add desired discourse features that they wish the latent variable z to capture.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"KgCVAE model is trained by maximizing:
L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c))",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
+,3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"Eqφ(z|c,x,y)[log p(x|z, c, y)]",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"+Eqφ(z|c,x,y)[log p(y|z, c)] (4)
Since now the reconstruction of y is a part of the loss function, kgCVAE can more efficiently encode y-related information into z than discovering it only based on the surface-level x and c. Another advantage of kgCVAE is that it can output a highlevel label (e.g. dialog act) along with the wordlevel responses, which allows easier interpretation of the model’s outputs.",3.2 Knowledge-Guided CVAE (kgCVAE),[0],[0]
"A straightforward VAE with RNN decoder fails to encode meaningful information in z due to the vanishing latent variable problem (Bowman et al., 2015).",3.3 Optimization Challenges,[0],[0]
"Bowman et al., (2015) proposed two solutions: (1) KL annealing: gradually increasing the weight of the KL term from 0 to 1 during training; (2) word drop decoding: setting a certain percentage of the target words to 0.",3.3 Optimization Challenges,[0],[0]
We found that CVAE suffers from the same issue when the decoder is an RNN.,3.3 Optimization Challenges,[0],[0]
"Also we did not consider word drop decoding because Bowman et al,.",3.3 Optimization Challenges,[0],[0]
"(2015) have shown that it may hurt the performance when the drop rate is too high.
",3.3 Optimization Challenges,[0],[0]
"As a result, we propose a simple yet novel technique to tackle the vanishing latent variable problem: bag-of-word loss.",3.3 Optimization Challenges,[0],[0]
"The idea is to introduce
an auxiliary loss that requires the decoder network to predict the bag-of-words in the response x as shown in Figure 3(b).",3.3 Optimization Challenges,[0],[0]
"We decompose x into two variables: xo with word order and xbow without order, and assume that xo and xbow are conditionally independent given z and c: p(x, z|c)",3.3 Optimization Challenges,[0],[0]
"= p(xo|z, c)p(xbow|z, c)p(z|c).",3.3 Optimization Challenges,[0],[0]
"Due to the conditional independence assumption, the latent variable is forced to capture global information about the target response.",3.3 Optimization Challenges,[0],[0]
"Let f = MLPb(z, x) ∈ RV where V is vocabulary size, and we have:
log p(xbow|z, c) = log |x|∏
t=1
efxt ∑V
j e fj
(5)
where |x| is the length of x and xt is the word index of tth word in x.",3.3 Optimization Challenges,[0],[0]
"The modified variational lower bound for CVAE with bag-of-word loss is (see Appendix A for kgCVAE):
L′(θ, φ;x, c) = L(θ, φ;x, c) +",3.3 Optimization Challenges,[0],[0]
"Eqφ(z|c,x,y)[log p(xbow|z, c)]",3.3 Optimization Challenges,[0],[0]
"(6)
We will show that the bag-of-word loss in Equation 6 is very effective against the vanishing latent variable and it is also complementary to the KL annealing technique.",3.3 Optimization Challenges,[0],[0]
We chose the Switchboard (SW) 1,4.1 Dataset,[0],[0]
"Release 2 Corpus (Godfrey and Holliman, 1997) to evaluate the proposed models.",4.1 Dataset,[0],[0]
SW has 2400 two-sided telephone conversations with manually transcribed speech and alignment.,4.1 Dataset,[0],[0]
"In the beginning of the call, a computer operator gave the callers recorded prompts that define the desired topic of discussion.",4.1 Dataset,[0],[0]
There are 70 available topics.,4.1 Dataset,[0],[0]
We randomly split the data into 2316/60/62 dialogs for train/validate/test.,4.1 Dataset,[0],[0]
"The pre-processing includes (1) tokenize using the NLTK tokenizer (Bird et al., 2009); (2) remove non-verbal symbols and repeated words due to false starts; (3) keep the top 10K frequent word types as the vocabulary.",4.1 Dataset,[0],[0]
"The final data have 207, 833/5, 225/5, 481 (c, x) pairs for train/validate/test.",4.1 Dataset,[0],[0]
"Furthermore, a subset of SW was manually labeled with dialog acts (Stolcke et al., 2000).",4.1 Dataset,[0],[0]
"We extracted dialog act labels based on the dialog act recognizer proposed in (Ribeiro et al., 2015).",4.1 Dataset,[0],[0]
"The features include the uni-gram and bi-gram of the utterance, and the contextual features of the last 3 utterances.",4.1 Dataset,[0],[0]
"We trained a Support Vector Machine
(SVM) (Suykens and Vandewalle, 1999) with linear kernel on the subset of SW with human annotations.",4.1 Dataset,[0],[0]
There are 42 types of dialog acts and the SVM achieved 77.3% accuracy on held-out data.,4.1 Dataset,[0],[0]
Then the rest of SW data are labelled with dialog acts using the trained SVM dialog act recognizer.,4.1 Dataset,[0],[0]
We trained with the following hyperparameters (according to the loss on the validate dataset): word embedding has size 200 and is shared across everywhere.,4.2 Training,[0],[0]
"We initialize the word embedding from Glove embedding pre-trained on Twitter (Pennington et al., 2014).",4.2 Training,[0],[0]
The utterance encoder has a hidden size of 300 for each direction.,4.2 Training,[0],[0]
The context encoder has a hidden size of 600 and the response decoder has a hidden size of 400.,4.2 Training,[0],[0]
The prior network and the MLP for predicting y both have 1 hidden layer of size 400 and tanh non-linearity.,4.2 Training,[0],[0]
The latent variable z has a size of 200.,4.2 Training,[0],[0]
The context window k is 10.,4.2 Training,[0],[0]
"All the initial weights are sampled from a uniform distribution [-0.08, 0.08].",4.2 Training,[0],[0]
The mini-batch size is 30.,4.2 Training,[0],[0]
"The models are trained end-to-end using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.001 and gradient clipping at 5.",4.2 Training,[0],[0]
We selected the best models based on the variational lower bound on the validate data.,4.2 Training,[0],[0]
"Finally, we use the BOW loss along with KL annealing of 10,000 batches to achieve the best performance.",4.2 Training,[0],[0]
Section 5.4 gives a detailed argument for the importance of the BOW loss.,4.2 Training,[0],[0]
"We compared three neural dialog models: a strong baseline model, CVAE, and kgCVAE.",5.1 Experiments Setup,[0],[0]
"The baseline model is an encoder-decoder neural dialog model without latent variables similar to (Serban et al., 2016a).",5.1 Experiments Setup,[0],[0]
The baseline model’s encoder uses the same context encoder to encode the dialog history and the meta features as shown in Figure 3.,5.1 Experiments Setup,[0],[0]
The encoded context c is directly fed into the decoder networks as the initial state.,5.1 Experiments Setup,[0],[0]
"The hyperparameters of the baseline are the same as the ones reported in Section 4.2 and the baseline is trained to minimize the standard cross entropy loss of the decoder RNN model without any auxiliary loss.
",5.1 Experiments Setup,[0],[0]
"Also, to compare the diversity introduced by the stochasticity in the proposed latent variable versus the softmax of RNN at each decoding step, we generate N responses from the baseline by sam-
pling from the softmax.",5.1 Experiments Setup,[0],[0]
"For CVAE/kgCVAE, we sample N times from the latent z and only use greedy decoders so that the randomness comes entirely from the latent variable z.",5.1 Experiments Setup,[0],[0]
"Automatically evaluating an open-domain generative dialog model is an open research challenge (Liu et al., 2016).",5.2 Quantitative Analysis,[0],[0]
"Following our one-tomany hypothesis, we propose the following metrics.",5.2 Quantitative Analysis,[0],[0]
"We assume that for a given dialog context c, there existMc reference responses rj , j ∈",5.2 Quantitative Analysis,[0],[0]
"[1,Mc].",5.2 Quantitative Analysis,[0],[0]
Meanwhile a model can generateN hypothesis responses,5.2 Quantitative Analysis,[0],[0]
"hi, i ∈",5.2 Quantitative Analysis,[0],[0]
"[1, N ].",5.2 Quantitative Analysis,[0],[0]
"The generalized responselevel precision/recall for a given dialog context is:
precision(c) =
∑N i=1maxj∈[1,Mc]d(rj , hi)
N
recall(c) =
∑Mc j=1maxi∈[1,N ]d(rj , hi))
",5.2 Quantitative Analysis,[0],[0]
"Mc
where d(rj , hi) is a distance function which lies between 0 to 1 and measures the similarities between rj and hi.",5.2 Quantitative Analysis,[0],[0]
"The final score is averaged over the entire test dataset and we report the performance with 3 types of distance functions in order to evaluate the systems from various linguistic points of view:
1.",5.2 Quantitative Analysis,[0],[0]
"Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified ngram precision with a length penalty (Papineni et al., 2002; Li et al., 2015).",5.2 Quantitative Analysis,[0],[0]
"We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale.
2.",5.2 Quantitative Analysis,[0],[0]
"Cosine Distance of Bag-of-word Embedding: a simple method to obtain sentence embeddings is to take the average or extrema of all the word embeddings in the sentences (Forgues et al., 2014; Adi et al., 2016).",5.2 Quantitative Analysis,[0],[0]
"The d(rj , hi) is the cosine distance of the two embedding vectors.",5.2 Quantitative Analysis,[0],[0]
"We used Glove embedding described in Section 4 and denote the average method as A-bow and extrema method as E-bow.
3.",5.2 Quantitative Analysis,[0],[0]
"Dialog Act Match: to measure the similarity at the discourse level, the same dialogact tagger from 4.1 is applied to label all the generated responses of each model.",5.2 Quantitative Analysis,[0],[0]
"We set d(rj , hi) = 1 if rj and hi have the same dialog acts, otherwise d(rj , hi) = 0.
",5.2 Quantitative Analysis,[0],[0]
"One challenge of using the above metrics is that there is only one, rather than multiple reference responses/contexts.",5.2 Quantitative Analysis,[0],[0]
This impacts reliability of our measures.,5.2 Quantitative Analysis,[0],[0]
"Inspired by (Sordoni et al., 2015), we utilized information retrieval techniques (see Appendix A) to gather 10 extra candidate reference responses/context from other conversations with the same topics.",5.2 Quantitative Analysis,[0],[0]
"Then the 10 candidate references are filtered by two experts, which serve as the ground truth to train the reference response classifier.",5.2 Quantitative Analysis,[0],[0]
The result is 6.69 extra references in average per context.,5.2 Quantitative Analysis,[0],[0]
The average number of distinct reference dialog acts is 4.2.,5.2 Quantitative Analysis,[0],[0]
"Table 1 shows the results.
",5.2 Quantitative Analysis,[0],[0]
The proposed models outperform the baseline in terms of recall in all the metrics with statistical significance.,5.2 Quantitative Analysis,[0],[0]
This confirms our hypothesis that generating responses with discourse-level diversity can lead to a more comprehensive coverage of the potential responses than promoting only word-level diversity.,5.2 Quantitative Analysis,[0],[0]
"As for precision, we observed that the baseline has higher or similar scores than CVAE in all metrics, which is expected since the baseline tends to generate the mostly likely and safe responses repeatedly in the N hypotheses.",5.2 Quantitative Analysis,[0],[0]
"However, kgCVAE is able to achieve the highest precision and recall in the 4 metrics at the same time (BLEU1-4, E-BOW).",5.2 Quantitative Analysis,[0],[0]
"One reason
for kgCVAE’s good performance is that the predicted dialog act label in kgCVAE can regularize the generation process of its RNN decoder by forcing it to generate more coherent and precise words.",5.2 Quantitative Analysis,[0],[0]
We further analyze the precision/recall of BLEU4 by looking at the average score versus the number of distinct reference dialog acts.,5.2 Quantitative Analysis,[0],[0]
"A low number of distinct dialog acts represents the situation where the dialog context has a strong constraint on the range of the next response (low entropy), while a high number indicates the opposite (highentropy).",5.2 Quantitative Analysis,[0],[0]
Figure 4 shows that CVAE/kgCVAE achieves significantly higher recall than the baseline in higher entropy contexts.,5.2 Quantitative Analysis,[0],[0]
"Also it shows that CVAE suffers from lower precision, especially in low entropy contexts.",5.2 Quantitative Analysis,[0],[0]
"Finally, kgCVAE gets higher precision than both the baseline and CVAE in the full spectrum of context entropy.",5.2 Quantitative Analysis,[0],[0]
Table 2 shows the outputs generated from the baseline and kgCVAE.,5.3 Qualitative Analysis,[0],[0]
"In example 1, caller A begins with an open-ended question.",5.3 Qualitative Analysis,[0],[0]
The kgCVAE model generated highly diverse answers that cover multiple plausible dialog acts.,5.3 Qualitative Analysis,[0],[0]
"Further, we notice that the generated text exhibits similar dialog acts compared to the ones predicted separately by the model, implying the consistency of natural language generation based on y.",5.3 Qualitative Analysis,[0],[0]
"On the contrary, the responses from the baseline model are limited to local n-gram variations and share a similar prefix, i.e. ”I’m”.",5.3 Qualitative Analysis,[0],[0]
Example 2 is a situation where caller A is telling B stories.,5.3 Qualitative Analysis,[0],[0]
The ground truth response is a back-channel and the range of valid answers is more constrained than example 1 since B is playing the role of a listener.,5.3 Qualitative Analysis,[0],[0]
The baseline successfully predicts ”uh-huh”.,5.3 Qualitative Analysis,[0],[0]
The kgCVAE model is also able to generate various ways of back-channeling.,5.3 Qualitative Analysis,[0],[0]
"This implies that the latent z is able to capture context-sensitive variations, i.e. in low-entropy dialog contexts modeling lexical diversity while in high-entropy ones modeling discourse-level diversity.",5.3 Qualitative Analysis,[0],[0]
"Moreover, kgCVAE is occasionally able to
generate more sophisticated grounding (sample 4) beyond a simple back-channel, which is also an acceptable response given the dialog context.
",5.3 Qualitative Analysis,[0],[0]
"In addition, past work (Kingma and Welling, 2013) has shown that the recognition network is able to learn to cluster high-dimension data, so we conjecture that posterior z outputted from the recognition network should cluster the responses into meaningful groups.",5.3 Qualitative Analysis,[0],[0]
"Figure 5 visualizes the posterior z of responses in the test dataset in 2D space using t-SNE (Maaten and Hinton, 2008).",5.3 Qualitative Analysis,[0],[0]
"We found that the learned latent space is highly correlated with the dialog act and length of responses, which confirms our assumption.",5.3 Qualitative Analysis,[0],[0]
"Finally, we evaluate the effectiveness of bag-ofword (BOW) loss for training VAE/CVAE with the RNN decoder.",5.4 Results for Bag-of-Word Loss,[0],[0]
"To compare with past work (Bowman et al., 2015), we conducted the same language modelling (LM) task on Penn Treebank using VAE.",5.4 Results for Bag-of-Word Loss,[0],[0]
The network architecture is same except we use GRU instead of LSTM.,5.4 Results for Bag-of-Word Loss,[0],[0]
We compared four different training setups: (1) standard VAE without any heuristics; (2) VAE with KL annealing (KLA); (3) VAE with BOW loss; (4) VAE with both BOW loss and KLA.,5.4 Results for Bag-of-Word Loss,[0],[0]
"Intuitively, a well trained model should lead to a low reconstruction loss and small but non-trivial KL cost.",5.4 Results for Bag-of-Word Loss,[0],[0]
"For all models with KLA, the KL weight increases linearly from 0 to 1 in the first 5000 batches.
",5.4 Results for Bag-of-Word Loss,[0],[0]
Table 3 shows the reconstruction perplexity and the KL cost on the test dataset.,5.4 Results for Bag-of-Word Loss,[0],[0]
"The standard VAE fails to learn a meaningful latent variable by hav-
ing a KL cost close to 0 and a reconstruction perplexity similar to a small LSTM LM (Zaremba et al., 2014).",5.4 Results for Bag-of-Word Loss,[0],[0]
"KLA helps to improve the reconstruction loss, but it requires early stopping since the models will fall back to the standard VAE after the KL weight becomes 1.",5.4 Results for Bag-of-Word Loss,[0],[0]
"At last, the models with BOW loss achieved significantly lower perplexity and larger KL cost.
",5.4 Results for Bag-of-Word Loss,[0],[0]
Figure 6 visualizes the evolution of the KL cost.,5.4 Results for Bag-of-Word Loss,[0],[0]
"We can see that for the standard model, the KL cost crashes to 0 at the beginning of training and never recovers.",5.4 Results for Bag-of-Word Loss,[0],[0]
"On the contrary, the model with only KLA learns to encode substantial information in latent z when the KL cost weight is small.",5.4 Results for Bag-of-Word Loss,[0],[0]
"However, after the KL weight is increased to 1 (after 5000 batch), the model once again decides to ignore the latent z and falls back to the naive implementation.",5.4 Results for Bag-of-Word Loss,[0],[0]
"The model with BOW loss, however, consistently converges to a non-trivial KL cost even without KLA, which confirms the importance of BOW loss for training latent variable models with the RNN decoder.",5.4 Results for Bag-of-Word Loss,[0],[0]
"Last but not least, our experiments showed that the conclusions drawn from LM using VAE also apply to training CVAE/kgCVAE, so we used BOW loss together with KLA for all previous experiments.",5.4 Results for Bag-of-Word Loss,[0],[0]
"In conclusion, we identified the one-to-many nature of open-domain conversation and proposed two novel models that show superior performance in generating diverse and appropriate responses at the discourse level.",6 Conclusion and Future Work,[0],[0]
"While the current paper addresses diversifying responses in respect to dialogue acts, this work is part of a larger research direction that targets leveraging both past linguistic findings and the learning power of deep neural networks to learn better representation of the latent factors in dialog.",6 Conclusion and Future Work,[0],[0]
"In turn, the output of this novel neural dialog model will be easier to explain and control by humans.",6 Conclusion and Future Work,[0],[0]
"In addition to dialog acts, we plan to apply our kgCVAE model to capture other different linguistic phenomena including sentiment, named entities,etc.",6 Conclusion and Future Work,[0],[0]
"Last but not least, the recognition network in our model will serve as the foundation for designing a datadriven dialog manager, which automatically discovers useful high-level intents.",6 Conclusion and Future Work,[0],[0]
All of the above suggest a promising research direction.,6 Conclusion and Future Work,[0],[0]
"Variational Lower Bound for kgCVAE We assume that even with the presence of linguistic feature y regarding x, the prediction of xbow still only depends on the z and c. Therefore, we have:
L(θ, φ;x, c, y) = −KL(qφ(z|x, c, y)‖Pθ(z|c))",A Supplemental Material,[0],[0]
+,A Supplemental Material,[0],[0]
"Eqφ(z|c,x,y)[log p(x|z, c, y)]",A Supplemental Material,[0],[0]
"+Eqφ(z|c,x,y)[log p(y|z, c)]",A Supplemental Material,[0],[0]
"+Eqφ(z|c,x,y)[log p(xbow|z, c)]
(7)
Collection of Multiple Reference Responses We collected multiple reference responses for each dialog context in the test set by information retrieval techniques combining with traditional a machine learning method.",A Supplemental Material,[0],[0]
"First, we encode the dialog history using Term Frequency-Inverse Document Frequency (TFIDF) (Salton and Buckley, 1988) weighted bag-of-words into vector representation h.",A Supplemental Material,[0],[0]
"Then we denote the topic of the conversation as t and denote f as the conversation floor, i.e. if the speakers of the last utterance in the dialog history and response utterance are the same f = 1 otherwise f = 0.",A Supplemental Material,[0],[0]
"Then we computed the similarity d(ci, cj) between two dialog contexts using:
d(ci, cj) =",A Supplemental Material,[0],[0]
"1(ti = tj)1(ti = tj) hi · hj ||hi||||hj || (8)
Unlike past work (Sordoni et al., 2015), this similarity function only cares about the distance in the context and imposes no constraints on the response, therefore is suitbale for finding diverse responses regarding to the same dialog context.",A Supplemental Material,[0],[0]
"Secondly, for each dialog context in the test set, we retrieved the 10 nearest neighbors from the training set and treated the responses from the training set as candidate reference responses.",A Supplemental Material,[0],[0]
"Thirdly, we further sampled 240 context-responses pairs from 5481 pairs in the total test set and post-processed the selected candidate responses by two human computational linguistic experts who were told to give a binary label for each candidate response about whether the response is appropriate regarding its dialog context.",A Supplemental Material,[0],[0]
The filtered lists then served as the ground truth to train our reference response classifier.,A Supplemental Material,[0],[0]
"For the next step, we extracted bigrams, part-of-speech bigrams and word part-of-speech
pairs from both dialogue contexts and candidate reference responses with rare threshold for feature extraction being set to 20.",A Supplemental Material,[0],[0]
Then L2-regularized logistic regression with 10-fold cross validation was applied as the machine learning algorithm.,A Supplemental Material,[0],[0]
Cross validation accuracy on the human-labelled data was 71%.,A Supplemental Material,[0],[0]
"Finally, we automatically annotated the rest of test set with this trained classifier and the resulting data were used for model evaluation.",A Supplemental Material,[0],[0]
"While recent neural encoder-decoder models have shown great promise in modeling open-domain conversations, they often generate dull and generic responses.",abstractText,[0],[0]
"Unlike past work that has focused on diversifying the output of the decoder at word-level to alleviate this problem, we present a novel framework based on conditional variational autoencoders that captures the discourse-level diversity in the encoder.",abstractText,[0],[0]
Our model uses latent variables to learn a distribution over potential conversational intents and generates diverse responses using only greedy decoders.,abstractText,[0],[0]
We have further developed a novel variant that is integrated with linguistic prior knowledge for better performance.,abstractText,[0],[0]
"Finally, the training procedure is improved by introducing a bag-of-word loss.",abstractText,[0],[0]
Our proposed models have been validated to generate significantly more diverse responses than baseline approaches and exhibit competence in discourse-level decision-making.,abstractText,[0],[0]
Learning Discourse-level Diversity for Neural Dialog Models using Conditional Variational Autoencoders,title,[0],[0]
"The task of unsupervised discrete representation learning is to obtain a function that maps similar (resp. dissimilar) data into similar (resp. dissimilar) discrete representations, where the similarity of data is defined according to applications of interest.",1. Introduction,[0],[0]
"It is a central machine learning task because of the compactness of the representations and ease
1University of Tokyo, Japan 2RIKEN AIP, Japan 3Preferred Networks, Inc., Japan 4ATR Cognitive Mechanism Laboratories, Japan.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Weihua Hu <hu@ms.k.utokyo.ac.jp>, Takeru Miyato <takeru.miyato@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
of interpretation.",1. Introduction,[0],[0]
The task includes two important machine learning tasks as special cases: clustering and unsupervised hash learning.,1. Introduction,[0],[0]
"Clustering is widely applied to data-driven application domains (Berkhin, 2006), while hash learning is popular for an approximate nearest neighbor search for large scale information retrieval (Wang et al., 2016).
",1. Introduction,[0],[0]
"Deep neural networks are promising to be used thanks to their scalability and flexibility of representing complicated, non-linear decision boundaries.",1. Introduction,[0],[0]
"However, their model complexity is huge, and therefore, regularization of the networks is crucial to learn meaningful representations of data.",1. Introduction,[0],[0]
"Particularly, in unsupervised representation learning, target representations are not provided and hence, are unconstrained.",1. Introduction,[0],[0]
"Therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest (e.g., invariance to small perturbations or affine transformation).",1. Introduction,[0],[0]
"Naı̈ve regularization to use is a weight decay (Erin Liong et al., 2015).",1. Introduction,[0],[0]
"Such regularization, however, encourages global smoothness of the function prediction; thus, may not necessarily impose the intended invariance on the predicted discrete representations.
",1. Introduction,[0],[0]
"Instead, in this paper, we use data augmentation to model the invariance of learned data representations.",1. Introduction,[0],[0]
"More specifically, we map data points into their discrete representations by a deep neural network and regularize it by encouraging its prediction to be invariant to data augmentation.",1. Introduction,[0],[0]
The predicted discrete representations then exhibit the invariance specified by the augmentation.,1. Introduction,[0],[0]
Our proposed regularization method is illustrated as red arrows in Figure 1.,1. Introduction,[0],[0]
"As depicted, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion.",1. Introduction,[0],[0]
We term such regularization Self-Augmented Training (SAT).,1. Introduction,[0],[0]
"SAT is inspired by the recent success in regularization of neural networks in semi-supervised learning (Bachman et al., 2014; Miyato et al., 2016; Sajjadi et al., 2016).",1. Introduction,[0],[0]
SAT is flexible to impose various types of invariances on the representations predicted by neural networks.,1. Introduction,[0],[0]
"For example, it is generally preferred for data representations to be locally invariant, i.e., remain unchanged under local perturbations on data points.",1. Introduction,[0],[0]
"Using SAT, we can impose the local invariance on the representations by pushing the predictions of perturbed data points to be close to those of the original data points.",1. Introduction,[0],[0]
"For image data, it may also be preferred for data representations to be invariant under affine distortion, e.g., rotation, scaling and parallel movement.",1. Introduction,[0],[0]
"We can similarly impose the invariance via SAT by using the affine distortion for the data augmentation.
",1. Introduction,[0],[0]
"We then combine the SAT with the Regularized Information Maximization (RIM) for clustering (Gomes et al., 2010; Bridle et al., 1991), and arrive at our Information Maximizing Self-Augmented Training (IMSAT), an information-theoretic method for learning discrete representations using deep neural networks.",1. Introduction,[0],[0]
We illustrate the basic idea of IMSAT in Figure 1.,1. Introduction,[0],[0]
"Following the RIM, we maximize information theoretic dependency between inputs and their mapped outputs, while regularizing the mapping function.",1. Introduction,[0],[0]
IMSAT differs from the original RIM in two ways.,1. Introduction,[0],[0]
"First, IMSAT deals with a more general setting of learning discrete representations; thus, is also applicable to hash learning.",1. Introduction,[0],[0]
"Second, it uses a deep neural network for the mapping function and regularizes it in an end-to-end fashion via SAT.",1. Introduction,[0],[0]
"Learning with our method can be performed by stochastic gradient descent (SGD); thus, scales well to large datasets.
",1. Introduction,[0],[0]
"In summary, our contributions are: 1) an informationtheoretic method for unsupervised discrete representation learning using deep neural networks with the end-to-end regularization, and 2) adaptations of the method to clustering and hash learning to achieve the state-of-the-art performance on several benchmark datasets.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
"Related work is summarized in Section 2, while our method, IMSAT, is
presented in Section 3.",1. Introduction,[0],[0]
Experiments are provided in Section 4 and conclusions are drawn in Section 5.,1. Introduction,[0],[0]
Various methods have been proposed for clustering and hash learning.,2. Related Work,[0],[0]
"The representative ones include K-means clustering and hashing (He et al., 2013), Gaussian mixture model clustering, iterative quantization (Gong et al., 2013), and minimal-loss hashing (Norouzi & Blei, 2011).",2. Related Work,[0],[0]
"However, these methods can only model linear boundaries between different representations; thus, cannot fit to non-linear structures of data.",2. Related Work,[0],[0]
"Kernel-based (Xu et al., 2004; Kulis & Darrell, 2009) and spectral (Ng et al., 2001; Weiss et al., 2009) methods can model the non-linearity of data, but they are difficult to scale to large datasets.
",2. Related Work,[0],[0]
"Recently, clustering and hash learning using deep neural networks have attracted much attention.",2. Related Work,[0],[0]
"In clustering, Xie et al. (2016) proposed to use deep neural networks to simultaneously learn feature representations and cluster assignments, while Dilokthanakul et al. (2016) and Zheng et al. (2016) proposed to model the data generation process by using deep generative models with Gaussian mixture models as prior distributions.
",2. Related Work,[0],[0]
"Regarding hashing learning, a number of studies have used deep neural networks for supervised hash learning and achieved state-of-the-art results on image and text retrievals (Xia et al., 2014; Lai et al., 2015; Zhang et al., 2015; Xu et al., 2015; Li et al., 2015).",2. Related Work,[0],[0]
Relatively few studies have focused on unsupervised hash learning using deep neural networks.,2. Related Work,[0],[0]
"The pioneering work is semantic hashing, which uses stacked RBM models to learn compact binary representations (Salakhutdinov & Hinton, 2009).",2. Related Work,[0],[0]
Erin Liong et al. (2015) recently proposed to use deep neural networks for the mapping function and achieved stateof-the-art results.,2. Related Work,[0],[0]
"These unsupervised methods, however, did not explicitly intended impose the invariance on the learned representations.",2. Related Work,[0],[0]
"Consequently, the predicted representations may not be useful for applications of interest.
",2. Related Work,[0],[0]
"In supervised and semi-supervised learning scenarios, data augmentation has been widely used to regularize neural networks.",2. Related Work,[0],[0]
Leen (1995) showed that applying data augmentation to a supervised learning problem is equivalent to adding a regularization to the original cost function.,2. Related Work,[0],[0]
"Bachman et al. (2014); Miyato et al. (2016); Sajjadi et al. (2016) showed that such regularization can be adapted to semi-supervised learning settings to achieve state-of-theart performance.
",2. Related Work,[0],[0]
"In unsupervised representation learning scenarios, Dosovitskiy et al. (2014) proposed to use data augmentation to model the invariance of learned representations.",2. Related Work,[0],[0]
"Our IMSAT is different from Dosovitskiy et al. (2014)
in two important aspects: 1) IMSAT directly imposes the invariance on the learned representations, while Dosovitskiy et al. (2014) imposes invariance on surrogate classes, not directly on the learned representations.",2. Related Work,[0],[0]
"2) IMSAT focuses on learning discrete representations that are directly usable for clustering and hash learning, while Dosovitskiy et al. (2014) focused on learning continuous representations that are then used for other tasks such as classification and clustering.",2. Related Work,[0],[0]
"Relation of our work to denoising and contractive auto-encoders (Vincent et al., 2008; Rifai et al., 2011) is discussed in Appendix A.",2. Related Work,[0],[0]
"Let X and Y denote the domains of inputs and discrete representations, respectively.",3. Method,[0],[0]
"Given training samples, {x1, x2, . . .",3. Method,[0],[0]
", xN}, the task of discrete representation learning is to obtain a function, f :",3. Method,[0],[0]
"X → Y , that maps similar inputs into similar discrete representations.",3. Method,[0],[0]
"The similarity of data is defined according to applications of interest.
",3. Method,[0],[0]
We organize Section 3 as follows.,3. Method,[0],[0]
"In Section 3.1, we review the RIM for clustering (Gomes et al., 2010).",3. Method,[0],[0]
"In Section 3.2, we present our proposed method, IMSAT, for discrete representation learning.",3. Method,[0],[0]
"In Sections 3.3 and 3.4, we adapt IMSAT to the tasks of clustering and hash learning, respectively.",3. Method,[0],[0]
"In Section 3.5, we discuss an approximation technique for scaling up our method.",3. Method,[0],[0]
"The RIM (Gomes et al., 2010) learns a probabilistic classifier pθ(y|x) such that mutual information (Cover & Thomas, 2012) between inputs and cluster assignments is maximized.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"At the same time, it regularizes the complexity of the classifier.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
Let X ∈ X and Y ∈,3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Y ≡ {0, . . .",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
",K − 1} denote random variables for data and cluster assignments, respectively, where K is the number of clusters.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"The RIM minimizes the objective:
R(θ)− λI(X;Y ), (1)
where R(θ) is the regularization penalty, and I(X;Y ) is mutual information between X and Y , which depends on θ through the classifier, pθ(y|x).",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Mutual information measures the statistical dependency between X and Y , and is 0 iff they are independent.",3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
Hyper-parameter λ ∈ R trades off the two terms.,3.1. Review of Regularized Information Maximization for Clustering,[0],[0]
"Here, we present two components that make up our IMSAT.",3.2. Information Maximizing Self-Augmented Training,[0],[0]
We present the Information Maximization part in Section 3.2.1 and the SAT part in Section 3.2.2 .,3.2. Information Maximizing Self-Augmented Training,[0],[0]
We extend the RIM and consider learning M -dimensional discrete representations of data.,3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Let the output domain be Y = Y1× · · ·×YM , where Ym ≡ {0, 1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", Vm−1}, 1 ≤ m ≤ M .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Let Y = (Y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", YM ) ∈ Y be a random variable for the discrete representation.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Our goal is to learn a multioutput probabilistic classifier pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x) that maps similar inputs into similar representations.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"For simplicity, we model the conditional probability pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x) by using the deep neural network depicted in Figure 1.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"Under the model, {y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM} are conditionally independent given x:
pθ(y1, . . .",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
", yM |x)",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"= M∏
m=1
pθ(ym|x).",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"(2)
Following the RIM (Gomes et al., 2010), we maximize the mutual information between inputs and their discrete representations, while regularizing the multi-output probabilistic classifier.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
The resulting objective to minimize looks exactly the same as Eq.,3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"(1), except that Y is multi-dimensional in our setting.",3.2.1. INFORMATION MAXIMIZATION FOR LEARNING DISCRETE REPRESENTATIONS,[0],[0]
"VIA SELF-AUGMENTED TRAINING
We present an intuitive and flexible regularization objective, termed Self-Augmented Training (SAT).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
SAT uses data augmentation to impose the intended invariance on the data representations.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Essentially, SAT penalizes representation dissimilarity between the original data points and augmented ones.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
Let T : X → X denote a pre-defined data augmentation under which the data representations should be invariant.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The regularization of SAT made on data point x is
RSAT(θ;x, T (x))
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"= − M∑
m=1
Vm−1∑
ym=0
pθ̂(ym|x) log pθ(ym|T (x)), (3)
where pθ̂(ym|x) is the prediction of original data point x, and θ̂ is the current parameter of the network.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
In Eq.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(3), the representations of the augmented data are pushed to be close to those of the original data.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Since probabilistic classifier pθ(y|x) is modeled using a deep neural network, it is flexible enough to capture a wide range of invariances specified by the augmentation function T .",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The regularization by SAT is then the average of RSAT(θ;x, T (x)) over all the training data points:
RSAT(θ;T )",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"= 1
N
N∑
n=1
RSAT(θ;xn, T (xn)).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(4)
The augmentation function T can either be stochastic or deterministic.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
It can be designed specifically for the applications of interest.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"For example, for image data, affine distortion such as rotation, scaling and parallel movement can be used for the augmentation function.
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Alternatively, more general augmentation functions that do not depend on specific applications can be considered.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"A representative example is local perturbations, in which the augmentation function is
T (x) = x+ r, (5)
where r is a small perturbation that does not alter the meaning of the data point.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
The use of local perturbations in SAT encourages the data representations to be locally invariant.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
The resulting decision boundaries between different representations tend to lie in low density regions of a data distribution.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"Such boundaries are generally preferred and follow the low-density separation principle (Grandvalet et al., 2004).
",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"The two representative regulariztion methods based on local perturbations are: Random Perturbation Training (RPT) (Bachman et al., 2014) and Virtual Adversarial Training (VAT) (Miyato et al., 2016).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"In RPT, perturbation r is sampled randomly from hyper-sphere ||r||2 = ϵ, where ϵ is a hyper-parameter that controls the range of the local perturbation.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"On the other hand, in VAT, perturbation r is chosen to be an adversarial direction:
r = argmax r′
{RSAT(θ̂;x, x+ r′); ||r′||2 ≤ ϵ}.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"(6)
The solution of Eq.",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
(6) can be approximated efficiently by a pair of forward and backward passes.,3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"For further details, refer to Miyato et al. (2016).",3.2.2. REGULARIZATION OF DEEP NEURAL NETWORKS,[0],[0]
"In clustering, we can directly apply the RIM (Gomes et al., 2010) reviewed in Section 3.1.",3.3. IMSAT for Clustering,[0],[0]
"Unlike the original RIM, however, our method, IMSAT, uses deep neural networks for the classifiers and regularizes them via SAT.",3.3. IMSAT for Clustering,[0],[0]
"By representing mutual information as the difference between marginal entropy and conditional entropy (Cover & Thomas, 2012), we have the objective to minimize:
RSAT(θ;T )",3.3. IMSAT for Clustering,[0],[0]
− λ,3.3. IMSAT for Clustering,[0],[0]
"[H(Y )−H(Y |X)] , (7)
where H(·) and H(·|·) are entropy and conditional entropy, respectively.",3.3. IMSAT for Clustering,[0],[0]
"The two entropy terms can be calculated as
H(Y ) ≡ h(pθ(y))",3.3. IMSAT for Clustering,[0],[0]
"= h ( 1
N
N∑
i=1
pθ(y|x) ) , (8)
H(Y |X) ≡",3.3. IMSAT for Clustering,[0],[0]
"1 N
N∑
i=1
h(pθ(y|xi)), (9)
where h(p(y)) ≡",3.3. IMSAT for Clustering,[0],[0]
"− ∑
y′ p(y ′) log p(y′) is the entropy
function.",3.3. IMSAT for Clustering,[0],[0]
"Increasing the marginal entropy H(Y ) encourages the cluster sizes to be uniform, while decreasing the conditional entropy H(Y |X) encourages unambiguous cluster assignments (Bridle et al., 1991).
",3.3. IMSAT for Clustering,[0],[0]
"In practice, we can incorporate our prior knowledge on cluster sizes by modifying H(Y )",3.3. IMSAT for Clustering,[0],[0]
"(Gomes et al., 2010).",3.3. IMSAT for Clustering,[0],[0]
"Note that H(Y ) = logK − KL[pθ(y)|| U ], where K is the number of clusters, KL[·||·] is the Kullback-Leibler divergence, and U is a uniform distribution.",3.3. IMSAT for Clustering,[0],[0]
"Hence, maximization of H(Y ) is equivalent to minimization of KL[pθ(y)|| U ], which encourages predicted cluster distribution pθ(y) to be close to U .",3.3. IMSAT for Clustering,[0],[0]
Gomes et al. (2010) replaced U in KL[pθ(y)|| U ] with any specified class prior q(y) so that pθ(y) is encouraged to be close to q(y).,3.3. IMSAT for Clustering,[0],[0]
"In our preliminary experiments, we found that the resulting pθ(y) could still be far apart from pre-specified q(y).",3.3. IMSAT for Clustering,[0],[0]
"To ensure that pθ(y) is actually close to q(y), we consider the following constrained optimization problem:
min θ RSAT(θ;T ) +",3.3. IMSAT for Clustering,[0],[0]
"λH(Y |X),
subject to KL[pθ(y)|| q(y)]",3.3. IMSAT for Clustering,[0],[0]
"≤ δ, (10)
where δ > 0 is a tolerance hyper-parameter that is set sufficiently small so that predicted cluster distribution pθ(y) is the same as class prior q(y) up to δ-tolerance.",3.3. IMSAT for Clustering,[0],[0]
"Eq. (10) can be solved by using the penalty method (Bertsekas, 1999), which turns the original constrained optimization problem into a series of unconstrained optimization problems.",3.3. IMSAT for Clustering,[0],[0]
Refer to Appendix B for the detail.,3.3. IMSAT for Clustering,[0],[0]
"In hash learning, each data point is mapped into a D-bitbinary code.",3.4. IMSAT for Hash Learning,[0],[0]
"Hence, the original RIM is not directly applicable.",3.4. IMSAT for Hash Learning,[0],[0]
"Instead, we apply our method for discrete representation learning presented in Section 3.2.1.
",3.4. IMSAT for Hash Learning,[0],[0]
"The computation of mutual information I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X), however, is intractable for large D because it involves a summation over an exponential number of terms, each of which corresponds to a different configuration of hash bits.
",3.4. IMSAT for Hash Learning,[0],[0]
"Brown (2009) showed that mutual information I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X) can be expanded as the sum of interaction information (McGill, 1954):
I(Y1, . . .",3.4. IMSAT for Hash Learning,[0],[0]
", YD;X) =",3.4. IMSAT for Hash Learning,[0],[0]
"∑
C⊆SY
I(C ∪X), |C| ≥ 1, (11)
where SY ≡ {Y1, . .",3.4. IMSAT for Hash Learning,[0],[0]
.,3.4. IMSAT for Hash Learning,[0],[0]
", YD}.",3.4. IMSAT for Hash Learning,[0],[0]
Note that I denotes interaction information when its argument is a set of random variables.,3.4. IMSAT for Hash Learning,[0],[0]
Interaction information is a generalization of mutual information and can take a negative value.,3.4. IMSAT for Hash Learning,[0],[0]
"When the argument is a set of two random variables, the interaction information reduces to mutual information between the two
random variables.",3.4. IMSAT for Hash Learning,[0],[0]
"Following Brown (2009), we only retain terms involving pairs of output dimensions in Eq.",3.4. IMSAT for Hash Learning,[0],[0]
"(11), i.e., all terms where |C| ≤ 2.",3.4. IMSAT for Hash Learning,[0],[0]
"This gives us
D∑
d=1
I(Yd;X)",3.4. IMSAT for Hash Learning,[0],[0]
"+ ∑
1≤d ̸=d′≤D I({Yd, Yd′ , X}).",3.4. IMSAT for Hash Learning,[0],[0]
"(12)
",3.4. IMSAT for Hash Learning,[0],[0]
This approximation ignores the interactions among hash bits beyond the pairwise interactions.,3.4. IMSAT for Hash Learning,[0],[0]
"It is related to the orthogonality constraint that is widely used in the literature to remove redundancy among hash bits (Wang et al., 2016).",3.4. IMSAT for Hash Learning,[0],[0]
"In fact, the orthogonality constraint encourages the covariance between a pair of hash bits to 0.",3.4. IMSAT for Hash Learning,[0],[0]
"Thus, it also takes into account the pairwise interactions.
",3.4. IMSAT for Hash Learning,[0],[0]
It follows from the definition of interaction information and the conditional independence in Eq.,3.4. IMSAT for Hash Learning,[0],[0]
"(2) that
I({Yd, Yd′ , X}) ≡ I(Yd;Yd′ |X)− I(Yd;Yd′) = −I(Yd;Yd′).",3.4. IMSAT for Hash Learning,[0],[0]
"(13)
In summary, our approximated objective to minimize is
RSAT(θ;T )− λ
⎛ ⎝ D∑
d=1
I(X;Yd)− ∑
1≤d",3.4. IMSAT for Hash Learning,[0],[0]
"̸=d′≤D I(Yd;Yd′)
⎞
⎠ .
",3.4. IMSAT for Hash Learning,[0],[0]
"(14)
The first term regularizes the neural network.",3.4. IMSAT for Hash Learning,[0],[0]
"The second term maximizes the mutual information between data and each hash bit, and the third term removes the redundancy among the hash bits.",3.4. IMSAT for Hash Learning,[0],[0]
"To scale up our method to large datasets, we would like the objective in Eq.",3.5. Approximation of the Marginal Distribution,[0],[0]
(1) to be amenable to optimization based on mini-batch SGD.,3.5. Approximation of the Marginal Distribution,[0],[0]
"For the regularization term, we use the SAT in Eq.",3.5. Approximation of the Marginal Distribution,[0],[0]
"(4), which is the sum of per sample penalties and can be readily adapted to mini-batch computation.",3.5. Approximation of the Marginal Distribution,[0],[0]
"For the approximated mutual information in Eq. (14), we can decompose it into three parts: (i) conditional entropy H(Yd|X), (ii) marginal entropy H(Yd), and (iii) mutual information between a pair of output dimensions I(Yd;Yd′).",3.5. Approximation of the Marginal Distribution,[0],[0]
"The conditional entropy only consists of a sum over per example entropies (see Eq. (9)); thus, can be easily adapted to mini-batch computation.",3.5. Approximation of the Marginal Distribution,[0],[0]
"However, the marginal entropy (see Eq. (8)) and the mutual information involve the marginal distribution over a subset of target dimensions, i.e., pθ(c) ≡",3.5. Approximation of the Marginal Distribution,[0],[0]
"1N ∑N n=1 pθ(c|xn), where c ⊆ {y1, . .",3.5. Approximation of the Marginal Distribution,[0],[0]
.,3.5. Approximation of the Marginal Distribution,[0],[0]
", yM}.",3.5. Approximation of the Marginal Distribution,[0],[0]
"Hence, the marginal distribution can only be calculated using the entire dataset and is not amenable to the mini-batch setting.",3.5. Approximation of the Marginal Distribution,[0],[0]
"Following Springenberg (2015), we approximate the marginal distributions using mini-batch data:
pθ(c)",3.5. Approximation of the Marginal Distribution,[0],[0]
"≈ 1 |B| ∑
x∈B pθ(c|x) ≡ p̂θ(B)(c), (15)
where B is a set of data in the mini-batch.",3.5. Approximation of the Marginal Distribution,[0],[0]
"In the case of clustering, the approximated objective that we actually minimize is an upper bound of the exact objective that we try to minimize.",3.5. Approximation of the Marginal Distribution,[0],[0]
Refer to Appendix C of the supplementary material for the detailed discussion.,3.5. Approximation of the Marginal Distribution,[0],[0]
"In this section, we evaluate IMSAT for clustering and hash learning using benchmark datasets.",4. Experiments,[0],[0]
"In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation.",4.1. Implementation,[0],[0]
"Therefore, in all the experiments with benchmark datasets, we used commonly reported parameter values for deep neural networks and avoided dataset-specific tuning as much as possible.",4.1. Implementation,[0],[0]
"Specifically, inspired by Hinton et al. (2012), we set the network dimensionality to d-1200-1200-M for clustering across all the datasets, where d and M are input and output dimensionality, respectively.",4.1. Implementation,[0],[0]
"For hash learning, we used smaller network sizes to ensure fast computation of mapping data into hash codes.",4.1. Implementation,[0],[0]
"We used rectified linear units (Jarrett et al., 2009; Nair & Hinton, 2010; Glorot et al., 2011) for all the hidden activations and applied batch normalization (Ioffe & Szegedy, 2015) to each layer to accelerate training.",4.1. Implementation,[0],[0]
"For the output layer, we used the softmax for clustering and the sigmoids for hash learning.",4.1. Implementation,[0],[0]
"Regarding optimization, we used Adam (Kingma & Ba, 2015) with the step size 0.002.",4.1. Implementation,[0],[0]
Refer to Appendix D for further details.,4.1. Implementation,[0],[0]
"Our implementation based on Chainer (Tokui et al., 2015) is available at https://github.com/weihua916/imsat.",4.1. Implementation,[0],[0]
We evaluated our method for clustering presented in Section 3.3 on eight benchmark datasets.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We performed experiments with two variants of the RIM and three variants of IMSAT, each of which uses different classifiers and regularization.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
Table 1 summarizes these variants.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We also compared our IMSAT with existing clustering methods including K-means, DEC (Xie et al., 2016), denoising AutoEncoder (dAE)+K-means (Xie et al., 2016).
",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
A brief summary of dataset statistics is given in Table 2.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"In the experiments, our goal was to discover clusters that correspond well with the ground-truth categories.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For the STL, CIFAR10 and CIFAR100 datasets, raw pixels are not suited for our goal because color information is dominant.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"We therefore applied 50-layer pre-trained deep residual networks (He et al., 2016) to extract features and used them for clustering.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Note that since the residual network was trained on ImageNet, each class of the STL dataset (which is a subset of ImageNet) was expected to be well-separated in the feature space.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For Omniglot, 100 types of characters were sampled, each containing 20 data points.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Each data point was augmented 20 times by the stochastic affine distortion described in Appendix F. For SVHN, each image was represented as a 960-dimensional GIST feature (Oliva & Torralba, 2001).",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"For Reuters and 20news, we removed stop words and retained the 2000 most frequent words.",4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
We then used tf-idf features.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
Refer to Appendix E of the supplementary material for further details.,4.2.1. DATASETS AND COMPARED METHODS,[0],[0]
"Following Xie et al. (2016), we set the number of clusters to the number of ground-truth categories and evaluated clustering performance with unsupervised clustering accuracy (ACC):
ACC = max m
∑N n=1 1{ln = m(cn)}
N , (16)
where ln and cn are the ground-truth label and cluster assignment produced using the algorithm for xn, respectively.",4.2.2. EVALUATION METRIC,[0],[0]
The m ranges over all possible one-to-one mappings between clusters and labels.,4.2.2. EVALUATION METRIC,[0],[0]
"The best mapping can be efficiently computed using the Hungarian algorithm (Kuhn, 1955).",4.2.2. EVALUATION METRIC,[0],[0]
"In unsupervised learning, it is not straightforward to determine hyper-parameters by cross-validation.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Hence, we fixed hyper-parameters across all the datasets unless there was an objective way to select them.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For K-means, we tried 12 different initializations and reported the results with the best objectives.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For dAE+K-means and DEC (Xie et al., 2016), we used the recommended hyperparameters for the network dimensionality and annealing speed.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Inspired by the automatic kernel width selection in spectral clustering (Zelnik-Manor & Perona, 2004), we set the perturbation range, ϵ, on data point x in VAT and RPT as
ϵ(x) = α · σt(x), (17)
where α is a scalar and σt(x) is the Euclidian distance to the t-th neighbor of x.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In our experiments, we fixed t = 10.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), we fixed α = 0.4, 2.5 and 0.25, respectively, which performed well across the datasets.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For the methods shown in Table 1, we varied one hyperparameter and chose the best one that performed well across the datasets.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"More specifically, for Linear RIM and Deep RIM, we varied the decay rate over 0.0025 ·",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"2i, i = 0, 1, . . .",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
", 7.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"For the three variants of IMSAT, we varied λ in Eq.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"(19) for 0.025 · 2i, i = 0, 1, . . .",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
", 7.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
We set q to be the uniform distribution and let δ = 0.01 · h(q(y)) in Eq.,4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"(10) for the all experiments.
",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Consequently, we chose 0.005 for decay rates in both Linear RIM and Deep RIM.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"Also, we set λ = 1.6, 0.05 and 0.1 for Linear IMSAT (VAT), IMSAT (RPT) and IMSAT (VAT), respectively.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
We hereforth fixed these hyperparameters throughout the experiments for both clustering and hash learning.,4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In Appendix G, we report all the experimental results and the criteria to choose the parameters.",4.2.3. HYPER-PARAMETER SELECTION,[0],[0]
"In Table 3, we compare clustering performance across eight benchmark datasets.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We see that IMSAT (VAT) performed well across the datasets.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"The fact that our IMSAT outperformed Linear RIM, Deep RIM and Linear IMSAT (VAT) for most datasets suggests the effectiveness of using deep neural networks with an end-to-end regularization via SAT.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
Linear IMSAT (VAT) did not perform well even with the end-to-end regularization probably because the linear classifier was not flexible enough to model the intended invariance of the representations.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We also see from Table 3 that IMSAT (VAT) consistently outperformed IMSAT (RPT) in our experiments.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"This suggests that VAT is an effective regularization method in unsupervised learning scenarios.
",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We further conducted experiments on the Omniglot dataset to demonstrate that clustering performance can be improved by incorporating domain-specific knowledge in the augmentation function of SAT.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"Specifically, we used the affine distortion in addition to VAT for the augmented function of SAT.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"We compared the clustering accuracy of IMSAT with three different augmentation functions: VAT, affine distortion, and the combination of VAT & affine distortion, in which we simply set the regularization to be
1 2 · RSAT(θ;TVAT)",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"+ 1 2 · RSAT(θ;Taffine), (18)
where TVAT and Taffine are augmentation functions of VAT and affine distortion, respectively.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
"For Taffine, we used the stochastic affine distortion function defined in Appendix F.
We report the clustering accuracy of Omniglot in Table 4.",4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We see that including affine distortion in data augmentation significantly improved clustering accuracy.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
Figure 2 shows ten randomly selected clusters of the Omniglot dataset that were found using IMSAT (VAT) and IMSAT (VAT & affine distortion).,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We observe that IMSAT (VAT & affine distortion) was able to discover cluster assignments that are invariant to affine distortion as we intended.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
These results suggest that our method successfully captured the invariance in the hand-written character recognition in an unsupervised way.,4.2.4. EXPERIMENTAL RESULTS,[0],[0]
We evaluated our method for hash learning presented in Section 3.4 on two benchmark datasets: MNIST and CIFAR10 datasets.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Each data sample of CIFAR10 is represented as a 512-dimensional GIST feature (Oliva & Torralba, 2001).",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Our method was compared against several unsupervised hash learning methods: spectral hashing (Weiss et al., 2009), PCA-ITQ (Gong et al., 2013), and Deep Hash (Erin Liong et al., 2015).",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
We also compared our method to the hash versions of Linear RIM and Deep RIM.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"For our IMSAT, we used VAT for the regularization.",4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
We used the same hyper-parameters as in Section 4.2.3.,4.3.1. DATASETS AND COMPARED METHODS,[0],[0]
"Following Erin Liong et al. (2015), we used three evaluation metrics to measure the performance of the different methods: 1) mean average precision (mAP); 2) precision at N = 500 samples; and 3) Hamming look-up result where the hamming radius is set as r = 2.",4.3.2. EVALUATION METRIC,[0],[0]
We used the class labels to define the neighbors.,4.3.2. EVALUATION METRIC,[0],[0]
We repeated the experiments ten times and took the average as the final result.,4.3.2. EVALUATION METRIC,[0],[0]
"The MNIST and CIFAR10 datasets both have 10 classes, and contain 70000 and 60000 data points, respectively.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Following Erin Liong et al. (2015), we randomly sampled 1000 samples, 100 per class, as the query data and used the remaining data as the gallery set.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We tested performance for 16 and 32-bit hash codes.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"In practice, fast computation of hash codes is crucial for fast information retrieval.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Hence, small networks are preferable.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"We therefore tested our method on three different network sizes: the same ones as Deep Hash (Erin Liong et al., 2015), d-200-200-M , and d-400-400-M .",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Note that Deep Hash used d-60-30-M and d-80-50-M for learning 16 and 32-bit hash codes, respectively.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
Table 5 lists the results for 16-bit hash.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Due to the space
constraint, we report the results for 32-bit hash codes in Appendix H, but the results showed a similar tendency as that of 16-bit hash codes.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We see from Table 5 that IMSAT with the largest network sizes (400-400) achieved competitive performance in both datasets.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"The performance of IMSAT improved significantly when slightly bigger networks (200-200) were used, while the performance of Deep RIM did not improve much with the larger networks.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
We deduce that this is because we can better model the local invariance by using more flexible networks.,4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"Deep RIM, on the other hand, did not significantly benefit from the larger networks, because the additional flexibility of the networks was not used by the global function regularization via weight-decay.1 In Appendix I, our deduction is supported using a toy dataset.
",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"1Hence, we deduce that Deep Hash, which is only regularized by weight-decay, would not benefit much by using larger networks.",4.3.3. EXPERIMENTAL RESULTS,[0],[0]
"In this paper, we presented IMSAT, an informationtheoretic method for unsupervised discrete representation learning using deep neural networks.",5. Conclusion & Future Work,[0],[0]
"Through extensive experiments, we showed that intended discrete representations can be obtained by directly imposing the invariance to data augmentation on the prediction of neural networks in an end-to-end fashion.",5. Conclusion & Future Work,[0],[0]
"For future work, it is interesting to apply our method to structured data, i.e., graph or sequential data, by considering appropriate data augmentation.",5. Conclusion & Future Work,[0],[0]
We thank Brian Vogel for helpful discussions and insightful reviews on the paper.,Acknowledgements,[0],[0]
"This paper is based on results obtained from Hu’s internship at Preferred Networks, Inc.",Acknowledgements,[0],[0]
Learning discrete representations of data is a central machine learning task because of the compactness of the representations and ease of interpretation.,abstractText,[0],[0]
The task includes clustering and hash learning as special cases.,abstractText,[0],[0]
Deep neural networks are promising to be used because they can model the non-linearity of data and scale to large datasets.,abstractText,[0],[0]
"However, their model complexity is huge, and therefore, we need to carefully regularize the networks in order to learn useful representations that exhibit intended invariance for applications of interest.",abstractText,[0],[0]
"To this end, we propose a method called Information Maximizing Self-Augmented Training (IMSAT).",abstractText,[0],[0]
"In IMSAT, we use data augmentation to impose the invariance on discrete representations.",abstractText,[0],[0]
"More specifically, we encourage the predicted representations of augmented data points to be close to those of the original data points in an end-to-end fashion.",abstractText,[0],[0]
"At the same time, we maximize the informationtheoretic dependency between data and their predicted discrete representations.",abstractText,[0],[0]
Extensive experiments on benchmark datasets show that IMSAT produces state-of-the-art results for both clustering and unsupervised hash learning.,abstractText,[0],[0]
Learning Discrete Representations via Information Maximizing Self-Augmented Training,title,[0],[0]
"The goal of unsupervised learning is to uncover hidden structure in unlabelled data, often in the form of latent feature representations.",1. Introduction,[0],[0]
"One popular type of model, an autoencoder, does this by trying to reconstruct its input (Bengio et al., 2007).",1. Introduction,[0],[0]
"Autoencoders have been used in various forms to address problems in machine translation (Chandar et al., 2014; Tu et al., 2017), speech processing (Elman & Zipser, 1987; Zeiler et al., 2013), and computer vision (Rifai et al., 2011;
1Computer Science Division, Stellenbosch University, South Africa 2CSIR/SU Centre for Artificial Intelligence Research, 3Department of Electrical and Electronic Engineering, Stellenbosch University, South Africa.",1. Introduction,[0],[0]
"Correspondence to: Steve Kroon <kroon@sun.ac.za>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"*Code to reproduce all the results in this paper is available at: https://github.com/arnupretorius/lindaedynamics icml2018
Larsson, 2017), to name just a few areas.",1. Introduction,[0],[0]
"Denoising autoencoders (DAEs) are an extension of autoencoders which learn latent features by reconstructing data from corrupted versions of the inputs (Vincent et al., 2008).",1. Introduction,[0],[0]
"Although this corruption step typically leads to improved performance over standard autoencoders, a theoretical understanding of its effects remains incomplete.",1. Introduction,[0],[0]
"In this paper, we provide new insights into the inner workings of DAEs by analysing the learning dynamics of linear DAEs.
",1. Introduction,[0],[0]
"We specifically build on the work of Saxe et al. (2013a;b), who studied the learning dynamics of deep linear networks in a supervised regression setting.",1. Introduction,[0],[0]
"By analysing the gradient descent weight update steps as time-dependent differential equations (in the limit as the learning rate approaches a small value), Saxe et al. (2013a) were able to derive exact solutions for the learning trajectory of these networks as a function of training time.",1. Introduction,[0],[0]
Here we extend their approach to linear DAEs.,1. Introduction,[0],[0]
"To do this, we use the expected reconstruction loss over the noise distribution as an objective (requiring a different decomposition of the input covariance) as a tractable way to incorporate noise into our analytic solutions.",1. Introduction,[0],[0]
"This approach yields exact equations which can predict the learning trajectory of a linear DAE.
",1. Introduction,[0],[0]
"Our work here shares the motivation of many recent studies (Advani & Saxe, 2017; Pennington & Worah, 2017; Pennington & Bahri, 2017; Nguyen & Hein, 2017; Dinh et al., 2017; Louart et al., 2017; Swirszcz et al., 2017; Lin et al., 2017; Neyshabur et al., 2017; Soudry & Hoffer, 2017; Pennington et al., 2017) working towards a better theoretical understanding of neural networks and their behaviour.",1. Introduction,[0],[0]
"Although we focus here on a theory for linear networks, such networks have learning dynamics that are in fact nonlinear.",1. Introduction,[0],[0]
"Furthermore, analyses of linear networks have also proven useful in understanding the behaviour of nonlinear neural networks (Saxe et al., 2013a; Advani & Saxe, 2017).
",1. Introduction,[0],[0]
First we introduce linear DAEs (§2).,1. Introduction,[0],[0]
"We then derive analytic expressions for their nonlinear learning dynamics (§3), and verify our solutions in simulations (§4) which show how noise can influence the shape of the loss surface and change the rate of convergence for gradient descent optimisation.",1. Introduction,[0],[0]
We also find that an appropriate amount of noise can help DAEs ignore low variance directions in the input while learning the reconstruction mapping.,1. Introduction,[0],[0]
"In the remainder of
the paper, we compare DAEs to standard regularised autoencoders and show that our theoretical predictions match both simulations (§5) and experimental results on MNIST and CIFAR-10 (§6).",1. Introduction,[0],[0]
"We specifically find that while the noise in a DAE has an equivalent effect to standard weight decay, the DAE exhibits faster learning dynamics.",1. Introduction,[0],[0]
We also show that our observations hold qualitatively for nonlinear DAEs.,1. Introduction,[0],[0]
We first give the background of linear DAEs.,2. Linear Denoising Autoencoders,[0],[0]
"Given training data consisting of pairs {(x̃i,xi), i = 1, ..., N}, where x̃ represents a corrupted version of the training data x ∈ RD, the reconstruction loss for a single hidden layer DAE with activation function φ is given by
L = 1 2N N∑ i=1",2. Linear Denoising Autoencoders,[0],[0]
"||xi −W2φ(W1x̃i)||2.
",2. Linear Denoising Autoencoders,[0],[0]
"Here, W1 ∈ RH×D and W2 ∈ RD×H are the weights of the network with hidden dimensionality H .",2. Linear Denoising Autoencoders,[0],[0]
"The learned feature representations correspond to the latent variable z = φ(W1x̃).
",2. Linear Denoising Autoencoders,[0],[0]
"To corrupt an input x, we sample a noise vector , where each component is drawn i.i.d.",2. Linear Denoising Autoencoders,[0],[0]
from a pre-specified noise distribution with mean zero and variance s2.,2. Linear Denoising Autoencoders,[0],[0]
We define the corrupted version of the input as x̃ = x + .,2. Linear Denoising Autoencoders,[0],[0]
"This ensures that the expectation over the noise remains unbiased, i.e. E (x̃) =",2. Linear Denoising Autoencoders,[0],[0]
"x.
Restricting our scope to linear neural networks, with φ(a) = a, the loss in expectation over the noise distribution is
E",2. Linear Denoising Autoencoders,[0],[0]
"[L] = 1
2N N∑ i=1",2. Linear Denoising Autoencoders,[0],[0]
"||xi −W2W1xi||2
whitece+ s2
2 tr(W2W1WT1 W T 2 ),",2. Linear Denoising Autoencoders,[0],[0]
"(1)
See the supplementary material for the full derivation.",2. Linear Denoising Autoencoders,[0],[0]
"Here we derive the learning dynamics of linear DAEs, beginning with a brief outline to build some intuition.
",3. Learning Dynamics of Linear DAEs,[0],[0]
"The weight update equations for a linear DAE can be formulated as time-dependent differential equations in the limit as the gradient descent learning rate becomes small (Saxe et al., 2013a).",3. Learning Dynamics of Linear DAEs,[0],[0]
The task of an ordinary (undercomplete) linear autoencoder is to learn the identity mapping that reconstructs the original input data.,3. Learning Dynamics of Linear DAEs,[0],[0]
The matrix corresponding to this learned map will essentially be an approximation of the full identity matrix that is of rank equal to the input dimension.,3. Learning Dynamics of Linear DAEs,[0],[0]
"It turns out that tracking the temporal updates of this mapping represents a difficult problem that involves dealing with
coupled differential equations, since both the on-diagonal and off-diagonal elements of the weight matrices need to be considered in the approximation dynamics at each time step.
",3. Learning Dynamics of Linear DAEs,[0],[0]
"To circumvent this issue and make the analysis tractable, we follow the methodology introduced in Saxe et al. (2013a), which is to: (1) decompose the input covariance matrix using an eigenvalue decomposition; (2) rotate the weight matrices to align with these computed directions of variation; and (3) use an orthogonal initialisation strategy to diagonalise the composite weight matrix W = W2W1.",3. Learning Dynamics of Linear DAEs,[0],[0]
"The important difference in our setting, is that additional constraints are brought about through the injection of noise.
",3. Learning Dynamics of Linear DAEs,[0],[0]
The remainder of this section outlines this derivation for the exact solutions to the learning dynamics of linear DAEs.,3. Learning Dynamics of Linear DAEs,[0],[0]
Consider a continuous time limit approach to studying the learning dynamics of linear DAEs.,3.1. Gradient descent update,[0],[0]
This is achieved by choosing a sufficiently small learning rate α for optimising the loss in (1) using gradient descent.,3.1. Gradient descent update,[0],[0]
"The update for W1 in a single gradient descent step then takes the form of a time-dependent differential equation
τ",3.1. Gradient descent update,[0],[0]
"d
dt W1 = N∑ i=1",3.1. Gradient descent update,[0],[0]
WT2 ( xix T,3.1. Gradient descent update,[0],[0]
i −W2W1xixTi ) whitesp−,3.1. Gradient descent update,[0],[0]
"εWT2 W2W1 = WT2 (Σxx −W2W1Σxx)− εWT2 W2W1.
",3.1. Gradient descent update,[0],[0]
"Here t is the time measured in epochs, τ =",3.1. Gradient descent update,[0],[0]
"Nα , ε = Ns 2 and Σxx = ∑N i=1",3.1. Gradient descent update,[0],[0]
"xix T i , represents the input covariance matrix.",3.1. Gradient descent update,[0],[0]
"Let the eigenvalue decomposition of the input covariance be Σxx = V ΛV
T , where V is an orthogonal matrix and denote the eigenvalues",3.1. Gradient descent update,[0],[0]
λj =,3.1. Gradient descent update,[0],[0]
"[Λ]jj , with λ1 ≥ λ2 ≥ · · · ≥ λD. The update can then be rewritten as
τ",3.1. Gradient descent update,[0],[0]
"d
dt W1 = W
T 2 V ( Λ− V TW2W1V Λ ) V T
morewhitespace− εWT2 W2W1.
",3.1. Gradient descent update,[0],[0]
The weight matrices can be rotated to align with the directions of variation in the input by performing the rotations W 1 = W1V andW 2 = V TW2.,3.1. Gradient descent update,[0],[0]
"Following a similar derivation for W2, the weight updates become
τ",3.1. Gradient descent update,[0],[0]
"d
dt W 1 = W
T 2 ( Λ−W 2W 1Λ )",3.1. Gradient descent update,[0],[0]
"− εWT2W 2W 1
τ d
dt",3.1. Gradient descent update,[0],[0]
"W 2 =
( Λ−W 2W 1Λ )",3.1. Gradient descent update,[0],[0]
W T 1 − εW 2W 1W T 1 .,3.1. Gradient descent update,[0],[0]
"To decouple the dynamics, we can set W2 = V D2RT and W1 = RD1V T , where R is an arbitrary orthogonal matrix
and D2 and D1 are diagonal matrices.",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"This results in the product of the realigned weight matrices
W 2W 1 = V TV D2R TRD1V TV = D2D1
to become diagonal.",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"The updates now reduce to the following scalar dynamics that apply independently to each pair of diagonal elements w1j and w2j of D1 and D2 respectively:
τ",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"d
dt w1j = w2jλj (1− w2jw1j)− εw22jw1j (2)
τ",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"d
dt w2j = w1jλj (1− w2jw1j)− εw2jw21j .",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"(3)
Note that the same dynamics stem from gradient descent on the loss given by
` = D∑ j=1 λj 2τ (1− w2jw1j)2 + D∑ j=1 ε 2τ (w2jw1j) 2. (4)
By examining (4), it is evident that the degree to which the first term will be reduced will depend on the magnitude of the associated eigenvalue λj .",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"However, for directions in the input covariance Σxx with relatively little variation the decrease in the loss from learning the identity map will be negligible and is likely to result in overfitting (since little to no signal is being captured by these eigenvalues).",3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
The second term in (4) is the result of the input corruption and acts as a suppressant on the magnitude of the weights in the learned mapping.,3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
Our interest is to better understand the interplay between these two terms during learning by studying their scalar learning dynamics.,3.2. Orthogonal initialisation and scalar dynamics,[0],[0]
"As noted above, the dynamics of learning are dictated by the value ofw = w2w1",3.3. Exact solutions to the dynamics of learning,[0],[0]
over time.,3.3. Exact solutions to the dynamics of learning,[0],[0]
"An expression can be derived for w(t) by using a hyperbolic change of coordinates in (2) and (3), letting θ parameterise points along a dynamics trajectory represented by the conserved quantity w22−w21 = ±c0.",3.3. Exact solutions to the dynamics of learning,[0],[0]
This relies on the fact that ` is invariant under a scaling of the weights such that w = (w1/c)(cw2) =,3.3. Exact solutions to the dynamics of learning,[0],[0]
"w2w1 for any constant c (Saxe et al., 2013a).",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Starting at any initial point (w1, w2) the dynamics are
w(t) = c0 2 sinh (θt) , (5)
with θt = 2tanh−1",3.3. Exact solutions to the dynamics of learning,[0],[0]
[ (1− E) ( ζ2 − β2 − 2βδ ),3.3. Exact solutions to the dynamics of learning,[0],[0]
"− 2(1 + E)ζδ
(1− E) (2β + 4δ)− 2(1 + E)ζ
]
where β = c0 ( 1 + ελ ) , ζ = √ β2 + 4, δ = tanh ( θ0 2 ) and E = eζλt/τ .",3.3. Exact solutions to the dynamics of learning,[0],[0]
Here θ0 depends on the initial weights w1 and w2 through the relationship θ0 = sinh−1(2w/c0).,3.3. Exact solutions to the dynamics of learning,[0],[0]
"The
derivation for θt involves rewriting τ ddtw in terms of θ, integrating over the interval θ0 to θt, and finally rearranging terms to get an expression for θ(t) ≡ θt (see the supplementary material for full details).",3.3. Exact solutions to the dynamics of learning,[0],[0]
"To derive the learning dynamics for different noise distributions, the corresponding ε must be computed and used to determine β and ζ .",3.3. Exact solutions to the dynamics of learning,[0],[0]
"For example, sampling noise from a Gaussian distribution such that ∼ N (0, σ2I), gives ε = Nσ2.",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Alternatively, if is distributed according to a zero-mean Laplace distribution with scale parameter b, then ε = 2Nb2.",3.3. Exact solutions to the dynamics of learning,[0],[0]
"Since the expression for the learning dynamics of a linear DAE in (5) evolve independently for each direction of variation in the input, it is enough to study the effect that noise has on learning for a single eigenvalue λ.",4. The Effects of Noise: a Simulation Study,[0],[0]
To do this we trained a scalar linear DAE to minimise the loss `λ = λ 2 (1−w2w1) 2+ ε2 (w2w1) 2 with λ = 1 using gradient descent.,4. The Effects of Noise: a Simulation Study,[0],[0]
"Starting from several different randomly initialised weights w1 and w2, we compare the simulated dynamics with those predicted by equation (5).",4. The Effects of Noise: a Simulation Study,[0],[0]
"The top row in Figure 1 shows the exact fit between the predictions and numerical simulations for different noise levels, ε = 0, 1, 5.
",4. The Effects of Noise: a Simulation Study,[0],[0]
The trajectories in the top row of Figure 1 converge to the optimal solution at different rates depending on the amount of injected noise.,4. The Effects of Noise: a Simulation Study,[0],[0]
"Specifically, adding more noise results in faster convergence.",4. The Effects of Noise: a Simulation Study,[0],[0]
"However, the trade-off in (4) ensures that the fixed point solution also diminishes in magnitude.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"To gain further insight, we also visualise the associated loss surfaces for each experiment in the bottom row of Figure 1.",4. The Effects of Noise: a Simulation Study,[0],[0]
"Note that even though the scalar product w2w1 defines a linear mapping, the minimisation of `λ with respect to w1 and w2 is a non-convex optimisation problem.",4. The Effects of Noise: a Simulation Study,[0],[0]
The loss surfaces in Figure 1 each have an unstable saddle point at w2 = w1 = 0,4. The Effects of Noise: a Simulation Study,[0],[0]
(red star) with all remaining fixed points lying on a minimum loss manifold (cyan curve).,4. The Effects of Noise: a Simulation Study,[0],[0]
This manifold corresponds to the different possible combinations ofw2 and w1 that minimise `λ.,4. The Effects of Noise: a Simulation Study,[0],[0]
"The paths that gradient descent follow from various initial starting weights down to points situated on the manifold are represented by dashed orange lines.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"For a fixed value of λ, adding noise warps the loss surface making steeper slopes and pulling the minimum loss manifold in towards the saddle point.",4. The Effects of Noise: a Simulation Study,[0],[0]
"Therefore, steeper descent directions cause learning to converge at a faster rate to fixed points that are smaller in magnitude.",4. The Effects of Noise: a Simulation Study,[0],[0]
"This is the result of a sharper curving loss surface and the minimum loss manifold lying closer to the origin.
",4. The Effects of Noise: a Simulation Study,[0],[0]
"We can compute the fixed point solution for any pair of initial starting weights (not on the saddle point) by taking
the derivative
d`λ dw = −λ τ (1− w) + ε τ w,
and setting it equal to zero to find w∗ = λλ+ε .",4. The Effects of Noise: a Simulation Study,[0],[0]
This solution reveals the interaction between the input variance associated with λ and the noise ε.,4. The Effects of Noise: a Simulation Study,[0],[0]
"For large eigenvalues for which λ ε, the fixed point will remain relatively unaffected by adding noise, i.e., w∗ ≈ 1.",4. The Effects of Noise: a Simulation Study,[0],[0]
"In contrast, if λ ε, the noise will result in w∗ ≈ 0.",4. The Effects of Noise: a Simulation Study,[0],[0]
"This means that over a distribution of eigenvalues, an appropriate amount of noise can help a DAE to ignore low variance directions in the input data while learning the reconstruction.",4. The Effects of Noise: a Simulation Study,[0],[0]
"In a practical setting, this motivates the tuning of noise levels on a development set to prevent overfitting.",4. The Effects of Noise: a Simulation Study,[0],[0]
"It is well known that adding noise to the inputs of a neural network is equivalent to a form of regularisation (Bishop, 1995).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Therefore, to further understand the role of noise in linear DAEs we compare the dynamics of noise to those of explicit regularisation in the form of weight decay (Krogh & Hertz, 1992).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The reconstruction loss for a linear weight
decayed autoencoder (WDAE) is given by
1
2N N∑ i=1",5. The Relationship Between Noise and Weight Decay,[0],[0]
||xi,5. The Relationship Between Noise and Weight Decay,[0],[0]
"−W2W1xi||2 + γ 2 ( ||W1||2 + ||W2||2 ) (6)
where γ is the penalty parameter that controls the amount of regularisation applied during learning.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Provided that the weights of the network are initialised to be small, it is also possible (see supplementary material) to derive scalar dynamics of learning from (6) as
wγ(t) =",5. The Relationship Between Noise and Weight Decay,[0],[0]
"ξEγ
Eγ − 1 + ξ/w0 , (7)
where ξ = (1−Nγ/λ) and Eγ = e2ξt/τ .
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Figure 2 compares the learning trajectories of linear DAEs and WDAEs over time (as measured in training epochs) for λ = 2.5, 1, 0.5 and 0.1.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The dynamics for both noise and weight decay exhibit a sigmoidal shape with an initial period of inactivity followed by rapid learning, finally reaching a plateau at the fixed point solution.",5. The Relationship Between Noise and Weight Decay,[0],[0]
Figure 2 illustrates that the learning time associated with an eigenvalue is negatively correlated with its magnitude.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"Thus, the eigenvalue corresponding to the largest amount of variation explained is the quickest to escape inactivity during learning.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The colour intensity of the lines in Figure 2 correspond to the amount of noise or regularisation applied in each run,
with darker lines indicating larger amounts.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In the continuous time limit with equal learning rates, when compared with noise dynamics, weight decay experiences a delay in learning such that the initial inactive period becomes extended for every eigenvalue, whereas adding noise has no effect on learning time.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In other words, starting from small weights, noise injected learning is capable of providing an equivalent regularisation mechanism to that of weight decay in terms of a constrained fixed point mapping, but with zero time delay.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"However, this analysis does not take into account the practice of using well-tuned stable learning rates for discrete optimisation steps.",5. The Relationship Between Noise and Weight Decay,[0],[0]
We therefore consider the impact on training time when using optimised learning rates for each approach.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"By using second order information from the Hessian as in Saxe et al. (2013a), (here of the expected reconstruction loss with respect to the scalar weights), we relate the optimal learning rates for linear DAEs and WDAEs,
where each optimal rate is inversely related to the amount of noise/regularisation applied during training (see supplementary material).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The ratio of the optimal DAE rate to that for the WDAE is
R = 2λ+ γ
2λ+ 3ε .",5. The Relationship Between Noise and Weight Decay,[0],[0]
"(8)
Note that the ratio in (8) will essentially be equal to one for eigenvalues that are significantly larger than both ε and γ, with deviations from unity only manifesting for smaller values of λ.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Furthermore, weight decay and noise injected learning result in equivalent scalar solutions when their parameters are related by γ = λελ+ε (see supplementary material).",5. The Relationship Between Noise and Weight Decay,[0],[0]
This leads to the following two observations.,5. The Relationship Between Noise and Weight Decay,[0],[0]
"First, it shows that adding noise during learning can be interpreted as a form of weight decay where the penalty parameter γ adapts to each direction of variation in the data.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In other words, noise essentially makes use of the statistical structure of
the input data to influence the amount of shrinkage that is being applied in various directions during learning.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Second, together with (8), we can theoretically compare the learning dynamics of DAEs and WDAEs, when both equivalent regularisation and the relative differences in optimal learning rates are taken into account.
",5. The Relationship Between Noise and Weight Decay,[0],[0]
"The effects of optimal learning rates (for λ = 1), are shown in Figure 3.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"DAEs still exhibit faster dynamics (left panel), even when taking into account the difference in the learning rate as a function of noise, or equivalent weight decay (middle panel).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"In addition, for equivalent regularisation effects, the ratio of the optimal rates R can be shown to be a monotonically decreasing function of the noise level, where the rate of decay depends on the size of λ.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"This means that for any amount of added noise, the DAE will require a slower learning rate than that of the WDAE.",5. The Relationship Between Noise and Weight Decay,[0],[0]
"Even so, a faster rate for the WDAE does not seem to compensate for its slower dynamics and the difference in learning time is also shown to grow as more noise (regularisation) is applied during training (right panel).",5. The Relationship Between Noise and Weight Decay,[0],[0]
"A primary motivation for weight decay as a regulariser is that it provides solutions with smaller weight norms, producing smoother models that have better generalisation performance.",5.1. Exploiting invariance in the loss function,[0],[0]
Figure 4 shows the effect of noise (top row) compared to weight decay (bottom row) on the norm of the weights during learning.,5.1. Exploiting invariance in the loss function,[0],[0]
"Looking at the loss surface for weight decay (bottom left panel), the penalty on the size of the weights acts by shrinking the minimum loss manifold down from a long curving valley to a single point (associ-
ated with a small norm solution).",5.1. Exploiting invariance in the loss function,[0],[0]
"Interestingly, this results in gradient descent following a trajectory towards an “invisible” minimum loss manifold similar to the one associated with noise.",5.1. Exploiting invariance in the loss function,[0],[0]
"However, once on this manifold, weight decay begins to exploit invariances in the loss function to changes in the weights, so as to move along the manifold down towards smaller norm solutions.",5.1. Exploiting invariance in the loss function,[0],[0]
"This means that even when the two approaches learn the exact same mapping over time (as shown by the learning dynamics in the middle column of Figure 4), additional epochs will cause weight decay to further reduce the size of the weights (bottom right panel).",5.1. Exploiting invariance in the loss function,[0],[0]
This happens in a stage-like manner where the optimisation first focuses on reducing the reconstruction loss by learning the optimal mapping and then reduces the regularisation loss through invariance.,5.1. Exploiting invariance in the loss function,[0],[0]
It is common practice to initialise the weights of a network with small values.,5.2. Small weight initialisation and early stopping,[0],[0]
"In fact, this strategy has recently been theoretically shown to help, along with early stopping, to ensure good generalisation performance for neural networks in certain high-dimensional settings (Advani & Saxe, 2017).",5.2. Small weight initialisation and early stopping,[0],[0]
"In our analysis however, what we find interesting about small weight initialisation is that it removes some of the differences in the learning behaviour of DAEs compared to regularised autoencoders that use weight decay.
",5.2. Small weight initialisation and early stopping,[0],[0]
"To see this, the magenta lines in Figure 4 show the learning dynamics for the two approaches where the weights of both the networks were initialised to small random starting values.",5.2. Small weight initialisation and early stopping,[0],[0]
"The learning dynamics are almost identical in terms of their temporal trajectories and have equal fixed
points.",5.2. Small weight initialisation and early stopping,[0],[0]
"However, what is interesting is the implicit regularisation that is brought about through the small initialisation.",5.2. Small weight initialisation and early stopping,[0],[0]
"By starting small and making incremental updates to the weights, the scalar solution in both cases end up being equal to the minimum norm solution.",5.2. Small weight initialisation and early stopping,[0],[0]
"In other words, the path that gradient descent takes from the initialisation to the minimum loss manifold, reaches the manifold where the norm of the weights happen to also be small.",5.2. Small weight initialisation and early stopping,[0],[0]
"This means that the second phase of weight decay (where the invariance of the loss function would be exploited to reduce the regularisation penalty), is not only no longer necessary, but also does not result in a norm that is appreciably smaller than that obtained by learning with added noise.",5.2. Small weight initialisation and early stopping,[0],[0]
"Therefore in this case, learning with explicit regularisation provides no additional benefit over that of learning with noise in terms of reducing the norm of the weights during training.
",5.2. Small weight initialisation and early stopping,[0],[0]
"When initialising small, early stopping can also serve as a form of implicit regularisation by ensuring that the weights do not change past the point where the validation loss starts to increase (Bengio et al., 2007).",5.2. Small weight initialisation and early stopping,[0],[0]
"In the context of learning dynamics, early stopping for DAEs can be viewed as a method that effectively selects only the directions of variation deemed useful for generalisation during reconstruction, considering the remaining eigenvalues to carry no additional signal.",5.2. Small weight initialisation and early stopping,[0],[0]
To verify the dynamics of learning on real-world data sets we compared theoretical predictions with actual learning on MNIST and CIFAR-10.,6. Experimental Results,[0],[0]
"In our experiments we considered the following linear autoencoder networks: a regular AE, a WDAE and a DAE.
",6. Experimental Results,[0],[0]
"For MNIST, we trained each autoencoder with small randomly initialised weights, using N = 50000 training samples for 5000 epochs, with a learning rate α = 0.01 and a hidden layer width ofH = 256.",6. Experimental Results,[0],[0]
"For the WDAE, the penalty parameter was set at γ = 0.5 and for the DAE, σ2 = 0.5.",6. Experimental Results,[0],[0]
"The results are shown in Figure 5 (left column).
",6. Experimental Results,[0],[0]
The theoretical predictions (solid lines) in Figure 5 show good agreement with the actual learning dynamics (points).,6. Experimental Results,[0],[0]
"As predicted, both regularisation (orange) and noise (green) suppress the fixed point value associated with the different eigenvalues and, whereas regularisation delays learning (fewer fixed points are reached by the WDAE during training when compared to the DAE), the use of noise has no effect on training time.
",6. Experimental Results,[0],[0]
Similar agreement is shown for CIFAR-10 in the right column of Figure 5.,6. Experimental Results,[0],[0]
"Here, we trained each network with small randomly initialised weights using N = 30000 training samples for 5000 epochs, with a learning rate α = 0.001 and a hidden dimension H = 512.",6. Experimental Results,[0],[0]
"For the WDAE, the
penalty parameter was set at γ = 0.5 and for the DAE, σ2 = 0.5.
",6. Experimental Results,[0],[0]
"Next, we investigated whether these dynamics are at least also qualitatively present in nonlinear autoencoder networks.",6. Experimental Results,[0],[0]
"Figure 6 shows the dynamics of learning for nonlinear AEs, WDAEs and DAEs, using ReLU activations, trained on MNIST (N = 50000) and CIFAR-10 (N = 30000) with equal learning rates.",6. Experimental Results,[0],[0]
"For the DAE, the input was corrupted using sampled Gaussian noise with mean zero and σ2 = 3.",6. Experimental Results,[0],[0]
"For the WDAE, the amount of weight decay was manually tuned to γ = 0.0045, to ensure that both autoencoders displayed roughly the same degree of regularisation in terms of the fixed points reached.",6. Experimental Results,[0],[0]
"During the course of training, the identity mapping associated with each eigenvalue was estimated (see supplementary material), at equally spaced intervals of size 10 epochs.
",6. Experimental Results,[0],[0]
"The learning dynamics are qualitatively similar to the dy-
namics observed in the linear case.",6. Experimental Results,[0],[0]
Both noise and weight decay result in a shrinkage of the identity mapping associated with each eigenvalue.,6. Experimental Results,[0],[0]
"Furthermore, in terms of the number of training epochs, the DAE is seen to learn as quickly as a regular AE, whereas the WDAE incurs a delay in learning time.",6. Experimental Results,[0.95616784720582],"['As shown in Figure 3, the percentage of messages in the same conversation as a given message becomes significantly lower with a longer elapsed time between consecutive messages.']"
"Although these experimental results stem from a single training run for each autoencoder, we note that wall-clock times for training may still differ because DAEs require some additional time for sampling noise.",6. Experimental Results,[0],[0]
Similar results were observed when using a tanh nonlinearity and are provided in the supplementary material.,6. Experimental Results,[0],[0]
There have been many studies aiming to provide a better theoretical understanding of DAEs.,7. Related Work,[0],[0]
"Vincent et al. (2008) analysed DAEs from several different perspectives, including manifold learning and information filtering, by establishing an equivalence between different criteria for learning and the original training criterion that seeks to minimise the reconstruction loss.",7. Related Work,[0],[0]
"Subsequently, Vincent (2011) showed that under a particular set of conditions, the training of DAEs can also be interpreted as a type of score matching.",7. Related Work,[0],[0]
This connection provided a probabilistic basis for DAEs.,7. Related Work,[0],[0]
"Following this, a more in-depth analysis of DAEs as a possible generative model suitable for arbitrary loss functions and multiple types of data was given by Bengio et al. (2013).
",7. Related Work,[0],[0]
"In contrast to a probabilistic understanding of DAEs, we present here an analysis of the learning process.",7. Related Work,[0],[0]
"Specifically inspired by Saxe et al. (2013a), as well as by earlier work on supervised neural networks (Opper, 1988; Sanger, 1989; Baldi & Hornik, 1989; Saad & Solla, 1995), we provide a theoretical investigation of the temporal behaviour of linear DAEs using derived equations that exactly describe their dynamics of learning.",7. Related Work,[0],[0]
"Specifically for the linear case, the squared error loss for the reconstruction contractive autoencoder (RCAE) introduced in Alain & Bengio (2014) is equivalent to the expected loss (over the noise) for the DAE.",7. Related Work,[0],[0]
"Therefore, the learning dynamics described in this paper also apply to linear RCAEs.
",7. Related Work,[0],[0]
For our analysis to be tractable we used a marginalised reconstruction loss where the gradient descent dynamics are viewed in expectation over the noise distribution.,7. Related Work,[0],[0]
"Whereas our motivation is analytical in nature, marginalising the reconstruction loss tends to be more commonly motivated from the point of view of learning useful and robust feature representations at a significantly lower computational cost (Chen et al., 2014; 2015).",7. Related Work,[0],[0]
"This approach has also been investigated in the context of supervised learning (van der Maaten et al., 2013; Wang & Manning, 2013; Wager et al., 2013).",7. Related Work,[0],[0]
"Also related to our work is the analysis by Poole et al. (2014), who showed that training autoencoders with noise (added at different levels of the network architecture),
is closely connected to training with explicit regularisation and proposed a marginalised noise framework for noisy autoencoders.",7. Related Work,[0],[0]
This paper analysed the learning dynamics of linear denoising autoencoders (DAEs) with the aim of providing a better understanding of the role of noise during training.,8. Conclusion and Future Work,[0],[0]
"By deriving exact time-dependent equations for learning, we showed how noise influences the shape of the loss surface as well as the rate of convergence to fixed point solutions.",8. Conclusion and Future Work,[0],[0]
"We also compared the learning behaviour of added input noise to that of weight decay, an explicit form of regularisation.",8. Conclusion and Future Work,[0],[0]
"We found that while the two have similar regularisation effects, the use of noise for regularisation results in faster training.",8. Conclusion and Future Work,[0],[0]
"We compared our theoretical predictions with actual learning dynamics on real-world data sets, observing good agreement.",8. Conclusion and Future Work,[0],[0]
"In addition, we also provided evidence (on both MNIST and CIFAR-10) that our predictions hold qualitatively for nonlinear DAEs.
",8. Conclusion and Future Work,[0],[0]
This work provides a solid basis for further investigation.,8. Conclusion and Future Work,[0],[0]
"Our analysis could be extended to nonlinear DAEs, potentially using the recent work on nonlinear random matrix theory for neural networks (Pennington & Worah, 2017; Louart et al., 2017).",8. Conclusion and Future Work,[0],[0]
Our findings indicate that appropriate noise levels help DAEs ignore low variance directions in the input; we also obtained new insights into the training time of DAEs.,8. Conclusion and Future Work,[0],[0]
"Therefore, future work might consider how these insights could actually be used for tuning noise levels and predicting the training time of DAEs.",8. Conclusion and Future Work,[0],[0]
"This would require further validation and empirical experiments, also on other datasets.",8. Conclusion and Future Work,[0],[0]
"Finally, our analysis only considers the training dynamics, while a better understanding of generalisation and what influences the quality of feature representations during testing, are also of prime importance.",8. Conclusion and Future Work,[0],[0]
"We would like to thank Andrew Saxe for early discussions that got us interested in this work, as well as the reviewers for insightful comments and suggestions.",Acknowledgements,[0],[0]
"We would like to thank the CSIR/SU Centre for Artificial Intelligence Research (CAIR), South Africa, for financial support.",Acknowledgements,[0],[0]
AP would also like to thank the MIH Media Lab at Stellenbosch University and Praelexis (Pty) Ltd for providing stimulating working environments for a portion of this work.,Acknowledgements,[0],[0]
"Denoising autoencoders (DAEs) have proven useful for unsupervised representation learning, but a thorough theoretical understanding is still lacking of how the input noise influences learning.",abstractText,[0],[0]
Here we develop theory for how noise influences learning in DAEs.,abstractText,[0],[0]
"By focusing on linear DAEs, we are able to derive analytic expressions that exactly describe their learning dynamics.",abstractText,[0],[0]
We verify our theoretical predictions with simulations as well as experiments on MNIST and CIFAR-10.,abstractText,[0],[0]
"The theory illustrates how, when tuned correctly, noise allows DAEs to ignore low variance directions in the inputs while learning to reconstruct them.",abstractText,[0],[0]
"Furthermore, in a comparison of the learning dynamics of DAEs to standard regularised autoencoders, we show that noise has a similar regularisation effect to weight decay, but with faster training dynamics.",abstractText,[0],[0]
We also show that our theoretical predictions approximate learning dynamics on real-world data and qualitatively match observed dynamics in nonlinear DAEs.,abstractText,[0],[0]
*,abstractText,[0],[0]
Learning Dynamics of Linear Denoising Autoencoders,title,[0],[0]
"Hospitalized patients are vulnerable to a wide range of adverse events, including cardiopulmonary arrests (Kause et al., 2004; Hogan et al., 2012; Yoon et al., 2016), acute respiratory failures (Mokart, 2013), septic shocks (Henry et al., 2015), and post-operative complications (Clifton
*Equal contribution 1University of California, Los Angeles, US.",1. Introduction,[0],[0]
"2University of Oxford, UK.",1. Introduction,[0],[0]
"3Alan Turing Institute, UK..",1. Introduction,[0],[0]
"Correspondence to: Ahmed M. Alaa <ahmedmalaa@ucla.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"et al., 2012).",1. Introduction,[0],[0]
"For a patient in a regular ward, the occurrence of any such event entails an unplanned transfer to an intensive care unit (ICU), the timing of which is a major determinant of the eventual outcome.",1. Introduction,[0],[0]
"Indeed, recent medical studies have confirmed that delayed transfer to the ICU is strongly correlated with morbidity and mortality (Mardini L, 2012; Mokart, 2013).",1. Introduction,[0],[0]
"The problem of delayed ICU transfer is enormous and acute: over 750,000 septic shocks and 200,000 cardiac arrests occur in the U.S. each year with mortality rates of 28.6% and 75% respectively (Merchant et al., 2011; Kumar et al., 2011).",1. Introduction,[0],[0]
"Fortunately, experts believe that much of these events could be prevented with accurate prognosis and early warning (Nguyen et al., 2007).
",1. Introduction,[0],[0]
"Motivated by the proliferation of electronic health records (EHRs) (currently available in more than 75% of hospitals in the U.S. (Charles et al., 2016))",1. Introduction,[0],[0]
we develop a datadriven real-time risk score that can promptly assess a hospitalized patient’s risk of clinical deterioration.,1. Introduction,[0],[0]
"Our risk score hinges on a novel continuous-time semi-Markovmodulated marked Hawkes process model for a monitored patient’s episode, i.e. the patient’s evolving (latent) clinical states and her corresponding (observed) physiological data.",1. Introduction,[0],[0]
"With the guidance of critical care experts, we conducted experiments on a dataset for a cohort of criticallyill patients admitted to a major academic medical center.",1. Introduction,[0],[0]
Results show that our risk score offers significant gains in the accuracy (and timeliness) of predicting clinical deterioration; our risk score attains a 23% improvement in the Area Under Receiver Operating Characteristic (AUROC) as compared to the technology currently deployed in our medical center.,1. Introduction,[0],[0]
"Since it confers a significant prognostic value in subacute care in wards, the proposed risk score is currently being installed in our medical center as a replacement for the current technology.
",1. Introduction,[0],[0]
The proposed probabilistic model (based on which our risk score is computed) captures a hospitalized patient’s entire episode as recorded in the EHR.,1. Introduction,[0],[0]
"A typical (critical care) patient episode comprises the time of her admission to the ward, the time of her admission to the ICU or discharge from the ward, and a temporal sequence of irregularly sampled physiological data that are collected during her stay in the ward (Johnson, 2016; Ghassemi et al., 2015).",1. Introduction,[0],[0]
"We model
a patient’s episode as being driven by a latent clinical state process, which we represent as a semi-Markov jump process (Yu, 2010), describing the evolution of the patient’s “severity of illness” over time.",1. Introduction,[0],[0]
"All the observable physiological variables are modulated by this process: the times at which clinicians decide to observe the patient’s physiological data are drawn from a Hawkes point process (Hawkes & Oakes, 1974), the intensity of which is modulated by the patient’s clinical state process, whereas the observed physiological data is drawn from a switching multi-task Gaussian process with hyper-parameters that depend on the patient’s clinical state.",1. Introduction,[0],[0]
The patient episode is thus a marked Hawkes process –with the physiological data serving as the marks– that is modulated by the patient’s clinical states.,1. Introduction,[0],[0]
"We provide a detailed description of the model in Section 3, and then propose an EM-based algorithm for learning its parameters from the EHR data in Section 4.
",1. Introduction,[0],[0]
A distinctive feature of our model is its ability to incorporate informative clinical judgments into the generative process for the patient’s episode.,1. Introduction,[0],[0]
The manifestation of informative clinical judgments in the EHR episodes is doublefaceted: the patients’ episodes are both “informatively sampled” and “informatively censored”.,1. Introduction,[0],[0]
"Informative sampling results from the fact that clinicians decide to observe the patient’s physiological data more intensely if they believe that the patient is in a “bad” clinical state (Moskovitch et al., 2015; Qin & Shelton, 2015)– a belief that is based on either the clinician’s own assessment of the patient’s state, or the communication between the patient and the ward staff (Kyriacos et al., 2014).",1. Introduction,[0],[0]
"Informative censoring results from the clinicians’ decision on when to send the patient to the ICU or discharge her from the ward, which is indeed informative of the “ clinical deterioration” or “clinical stability” onsets.",1. Introduction,[0],[0]
"In our model, informative censoring is taken into account by adopting an absorbing semi-Markov chain as a model for the patient’s latent clinical states; a patient’s risk score at any point of time is thus defined as the probability of eventual absorption in a “clinical deterioration” state.
",1. Introduction,[0],[0]
Related work:,1. Introduction,[0],[0]
"Marked point processes have been recently used in a very different context to model check-in data (Du et al., 2016; Pan et al., 2016), but we are not aware of any attempts for their deployment in the medical context.",1. Introduction,[0],[0]
"Most of the previous works on risk prognosis for critical care patients viewed informative censoring as a “surrogate label” for a patient’s clinical deterioration, and hence used those labels to train a supervised (regression) model using the physiological data in a fixed-size time window before censoring.",1. Introduction,[0],[0]
"The supervised models used in the literature included logistic regression (Ho et al., 2012; Saria et al., 2010), SVMs (Wiens et al., 2012), Gaussian processes (Ghassemi et al., 2015; Yoon et al., 2016) and recurrent neural networks (Che et al., 2016).",1. Introduction,[0],[0]
"The main limitation of this approach is that, in addition to the fact that it gener-
ally does not deal with informatively sampled episodes, it does not model the entire patient’s physiological trajectory, and hence it does not accurately capture intermediate (subtle) deterioration stages that are indicative of future severe deterioration stages, which leads to a sluggish risk assessment and delayed ICU alarms.
",1. Introduction,[0],[0]
"Another strand of literature has focused on building probabilistic models, usually variants of Hidden Markov Models (HMMs), for the entire patient’s physiological trajectories; applications have ranged from disease progression modeling to neonatal sepsis prediction (Wang et al., 2014; Stanculescu et al., 2014).",1. Introduction,[0],[0]
"These models are not capable of dealing with irregularly-sampled data, do not deal with informatively sampled episodes, and are restricted to the Markovianity assumption which entails unrealistically memoryless clinical state transitions.",1. Introduction,[0],[0]
"In (Henry et al., 2015), a ranking algorithm was used to construct a risk score for sepsis shocks; however, the approach therein requires the clinicians to provide assessments that order the disease severity at different time instances– we typically do not have such data in the EHR for ward patients.
",1. Introduction,[0],[0]
"Various works in the medical literature have proposed “expert-based” medical risk scores for prognosis in hospital wards (Morgan et al., 1997; Parshuram et al., 2009), some of which are currently used in practice.",1. Introduction,[0],[0]
"The medical literature has also suggested the use of mortality risk scores that are normally used in the ICU, such as APACHE-II and SOFA, as risk scores for ward patients (Yu et al., 2014).",1. Introduction,[0],[0]
"However, recent systematic reviews have demonstrated the modest net clinical utility of all these scores (Cvach, 2012).",1. Introduction,[0],[0]
"More recently, a data-driven medical risk score based on a simple regression model, known as the Rothman index, has been developed and commercialized (Finlay et al., 2014; Rothman et al., 2013).",1. Introduction,[0],[0]
"The Rothman index is currently deployed in various major hospitals in the U.S. including our medical center; in Section 5, we show that our risk score significantly outperforms the Rothman index in terms of AUROC and timeliness of ICU admission alarms.",1. Introduction,[0],[0]
The subacute care data in an EHR typically comprises a set of episodes; each episode is a sequence of vital signs and lab tests (physiological data) that have been gathered (by clinicians) for a hospitalized patient at irregularly spaced time instances during her stay in a ward.,2. Structure of the EHR Data,[0],[0]
"The episode starts at the time of admission to the ward, and is concluded by either an unplanned admission to the ICU, which means that the patient was clinically deteriorating, or a discharge from the ward, which means that the patient was clinically stable.",2. Structure of the EHR Data,[0],[0]
"We denote an EHR dataset D that comprises the episodes for D patients as D = {Ed}Dd=1, where Ed is the episode for the dth patient, and is defined as
Ed = ({ydm, tdm} Md m=1, T d c , l d), with ydm being the m th Qdimensional physiological variable (vital signs and lab test outcomes) for patient d observed at time tdm, and the total number of samples observed for that patient during her episode is Md.",2. Structure of the EHR Data,[0],[0]
"The duration of patient d’s stay in the ward is denoted by T dc , whereas her endpoint outcome (clinical deterioration and ICU admission, or clinical stability and discharge) is declared via a binary variable ld; the realization ld = 1 means that patient d was admitted to the ICU, and ld = 0 means that the patient was discharged home.",2. Structure of the EHR Data,[0],[0]
"We stress that the labels ld associated with every episode are neither noisy nor entirely subjective: we assign a label ld = 1 for patients who actually needed a therapeutic intervention in the ICU after an unplanned admission, and assign a label ld = 0 for patients who were discharged and not re-admitted shortly after.",2. Structure of the EHR Data,[0],[0]
We excluded all post-surgical ward patients for whom an ICU admission was preordained since for those patients the prognosis problem is not relevant.,2. Structure of the EHR Data,[0],[0]
"Our dataset comprises thousands of episodes for patients admitted to a large medical center over a 3-year period; all the episodes display the structure described above.
",2. Structure of the EHR Data,[0],[0]
The clinicians were right!,2. Structure of the EHR Data,[0],[0]
"Clinical judgments manifest in the dth episode of D through informative sampling (encoded in the observation times {tdm} Md m=1), and informative censoring (encoded in the episode duration T dc and the endpoint outcome l d).",2. Structure of the EHR Data,[0],[0]
Figure 1 is a depiction for both informative sampling and informative censoring.,2. Structure of the EHR Data,[0],[0]
"In Figure 1, we estimate the physiological data (time-varying) sampling rate using all the episodes in our dataset over a time horizon of 35 hours before ICU admission for deteriorating patients (i.e. patients with ld = 1, with t = 0 being the ICU admission time), and we compute the same estimates for stable patients (i.e. patients with ld = 0, with t = 0 being the discharge time).
",2. Structure of the EHR Data,[0],[0]
"We can see from the trends in Figure 1 that as the deteriorating patient approaches the ICU admission time, the clinicians tend to sample her physiological data more intensely, whereas as the stable patient approaches the discharge time, the clinicians tend to have a more relaxed schedule for observing her vital signs and lab tests.",2. Structure of the EHR Data,[0],[0]
The divergence between the sampling rates for deteriorating and stable patient groups increases as the patients approach their ICU admission and discharge onsets– that is because the clinicians become less uncertain about the patient’s state as time progresses.,2. Structure of the EHR Data,[0],[0]
"We have tested the hypothesis that the sampling rate of deteriorating patients is –on average– larger than that for stable patients in the last 24 hours before ICU admission or discharge via a two-sample t-test with a significance level of 0.05, and the hypothesis was accepted.
",2. Structure of the EHR Data,[0],[0]
"The take-away from Figure 1 is that the clinician’s judgment of the patient’s clinical state –manifesting in the vital signs and lab tests sampling rates– is very predictive of the
endpoint outcomes.",2. Structure of the EHR Data,[0],[0]
"This implies that there is a room for “learning from the informative clinical judgments”; that is, one can infer the patients’ latent states over time by using the clinicians’ observable sampling patterns as proximal noisy labels for those latent states.",2. Structure of the EHR Data,[0],[0]
Now we present a probabilistic model for the episodes {Ed}Dd=1 that captures the informative sampling and censoring effects discussed in Section 2.,3. The Semi-Markov-Modulated Marked Hawkes Process Model,[0],[0]
"We start by modeling the patient’s latent clinical state process in Subsection 3.1, before modeling the observable variables in Subsection 3.2.",3. The Semi-Markov-Modulated Marked Hawkes Process Model,[0],[0]
We assume that each patient’s episode is governed by an underlying latent clinical state process X(t) that represents the evolution of her “clinical well-being” over time.,3.1. Latent Clinical States,[0],[0]
The patient’s latent clinical state X(t) at any point of time t ∈ R+,3.1. Latent Clinical States,[0],[0]
"(t = 0 corresponds to the time of admission to the ward) belongs to a finite space X comprising N states, i.e. X = {1, 2, . .",3.1. Latent Clinical States,[0],[0]
", ., N}.",3.1. Latent Clinical States,[0],[0]
"We model informative censoring by assuming that states 1 and N are absorbing states; state 1 is the state of clinically stability, at which the patient can be safely discharged home, whereas state N is the state of clinical deterioration, at which the patient needs to be admitted to the ICU.",3.1. Latent Clinical States,[0],[0]
"Whenever the patient is in state 1 or N , her episode is terminated by the clinicians shortly after.",3.1. Latent Clinical States,[0],[0]
"All other states in X/{1, N} are transient states in which the patient needs vigilant monitoring by the ward staff.
",3.1. Latent Clinical States,[0],[0]
"The clinical state process X(t) is a semi-Markov jump process (Yu, 2010), i.e. X(t) =",3.1. Latent Clinical States,[0],[0]
∑K n=1 Xn · 1{τn≤t<τn+1} is a semi-Markov process for which every new state realization,3.1. Latent Clinical States,[0],[0]
"Xn starts at a jump time τn, where τ1 = 0, and lasts for a random sojourn time Sn = τn+1",3.1. Latent Clinical States,[0],[0]
− τn.,3.1. Latent Clinical States,[0],[0]
"A total number of K states are realized in the path X(t), where K is indeed random, and XK ∈ {1, N}, i.e. the patient’s episode is concluded by either clinical deterioration or stability.",3.1. Latent Clinical States,[0],[0]
"The advantage of adopting a semi-Markovian model for the clinical state process is that unlike Marko-
vian models, semi-Markovianity does not imply memoryless transitions– the transition probability from one clinical state to another at any time depends on the time spent in the current state, a property that has been recently validated in various clinical state models (Taghipour et al., 2013).",3.1. Latent Clinical States,[0],[0]
"We adopt an explicit-duration model for the state sojourn times (Johnson & Willsky, 2013): the nth state sojourn time Sn is drawn from a Gamma distribution1 with state-specific parameters as follows
Sn|(Xn = i) ∼ Gamma(γi), ∀i ∈ X .",3.1. Latent Clinical States,[0],[0]
"(1)
Since the model includes two absorbing states (1 and N ) for which the notion of sojourn time is inapplicable, we define the variables Sn of such states as the clinicians’ “response times” for admitting patients to the ICU or discharging them upon the clinical deterioration/stability onset.",3.1. Latent Clinical States,[0],[0]
"The transitions among clinical states are governed by a semiMarkov transition kernel matrix P = (pij)i,j , i.e.
P(Xn+1 = j|Xn = i) =",3.1. Latent Clinical States,[0],[0]
"pij , (2)
where self-transitions are eliminated for all transient states (Yu, 2010; Johnson & Willsky, 2013), i.e. pii = 0, ∀i ∈ X/{1, N}, and enforced for the two absorbing states pii = 1, ∀i ∈ {1, N}.",3.1. Latent Clinical States,[0],[0]
"The initial state distribution is given by π = (πi) N i=1, where πi = P(X(0) = i).",3.1. Latent Clinical States,[0],[0]
"Every episode Ed in an EHR dataset D is associated with a latent clinical state trajectory {Xdn, Sdn}K d
n=1, but we can only observe the absorbing state realization XKd",3.1. Latent Clinical States,[0],[0]
= ld in the EHR data.,3.1. Latent Clinical States,[0],[0]
"The patient’s latent clinical state process X(t) manifests in two ways: (1) it modulates the intensity of sampling the patient’s physiological variables (informative sampling), and (2) it modulates the distributional properties of the observed physiological variables.",3.2. Observable Physiological Data,[0],[0]
"We capture these two effects via a marked point process model for the patient’s
1We model the sojourn time via a Gamma distribution since it encompasses memoryless exponential distributions of Markov models as a special case (Liu et al., 2015).
episode E : the marked point process {(ym, tm)}m∈N+ comprises an observation process {tm}m∈N+ , which represents the physiological variables’ sampling times, and a mark process {ym}m∈N+ , which represents the realized physiological variables at these sampling times.",3.2. Observable Physiological Data,[0],[0]
The distributional specifications of our marked point process are given in the following Subsections.,3.2. Observable Physiological Data,[0],[0]
"We model the observation process generating the physiological variables’ observation times {tm}m∈N+ as a doubly stochastic point process whose intensity, λ(t), is a stochastic process modulated by the latent clinical state process X(t).",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In particular, the observation process {tm}m∈N+ is modeled as a one-dimensional Hawkes process with a linear self-exciting intensity function λ(t,X(t))",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"(Lee et al., 2016), i.e.
λ(t,X(t) = i) = λoi + αi ∑
τ<tm<t
e−βi(t−tm), (3)
∀",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"i ∈ X ,, where λoi , αi and βi are the state-dependent intensity parameters, e−βi(t−tm) is an exponential triggering kernel, and τ <",3.2.1. THE OBSERVATION PROCESS,[0],[0]
t is the time of the most recent jump in X(t).,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In order to ensure the local stationarity of the Hawkes process within the sojourn time of every latent state, we assume that αiβi < 1, ∀i ∈ X (Roueff et al., 2016); the expected value of the intensity function is therefore given by E[λ(t,X(t) = i)]",3.2.1. THE OBSERVATION PROCESS,[0],[0]
= λ,3.2.1. THE OBSERVATION PROCESS,[0],[0]
o,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"i
",3.2.1. THE OBSERVATION PROCESS,[0],[0]
1−αiβi .,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"For βi = ∞
or αi = 0, we recover a modulated Poisson process as a special case (Pan et al., 2016).
",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In Figure 2, we depict the observation times in two patients’ episodes: patient A being a clinically deteriorating patient, and patient B being a clinically stable patient.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
We can see that patient A’s episode had its sampling rate escalating as her condition was worsening; the sampling rate remained intense after she was admitted to the ICU.,3.2.1. THE OBSERVATION PROCESS,[0],[0]
"On the other hand, patient B’s episode exhibited a decelerating sampling rate on her path to clinical stability.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"We can also notice that
the observation times display a subtle “clustered” pattern that point out to their temporal dependencies– indeed, the clinicians are not memoryless, and the times at which they observe the physiological data are dependent.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
"In the light of the above, the modulated Hawkes process described above appears to be a sensible model for the observation process {tm}m∈N+ as it captures both the time-varying intensity and the temporal dependencies illustrated in Figure 2.",3.2.1. THE OBSERVATION PROCESS,[0],[0]
Now we provide the distributional specification of the mark process {ym}m∈N+ .,3.2.2. THE MARK PROCESS,[0],[0]
"Since the physiological data are irregularly sampled from an underlying continuous-time physiological process Y (t) at the sampling times determined by the observation process {tm}m∈N+ , a convenient model for Y (t) is a switching Gaussian Process defined as follows: Y (t) = ∑K n=1 Yn(t)1{τn≤t<τn+1}, with
Yn(t)|(Xn = i) ∼ GP(mi(t), ki(t, t′)), (4)
where mi(t) and ki(t, t′) are the state-dependent mean function and covariance kernel, respectively.",3.2.2. THE MARK PROCESS,[0],[0]
"We use a constant mean function mi(t) = mi and a Matérn kernel given by
ki(t, t ′)",3.2.2. THE MARK PROCESS,[0],[0]
"=
(√ 2vi−1 |t−t′|
ℓi
)vi− 12 Kvi− 12 (√ 2vi−1 |t−t′| ℓi ) 2vi−
3 2 Γ(vi − 12 )
,
(5) where vi ∈ N+, ℓi ∈ R+, Γ(.) is the Gamma function and Kvi− 12 (.) is a modified Bessel function (Rasmussen, 2006).
",3.2.2. THE MARK PROCESS,[0],[0]
"Our choice for the Matérn kernel is motivated by its ability to represent various commonly used stochastic processes; for instance, when vi = 1, then Yn(t)|(Xn = i) is an Ornstein-Uhlenbeck process (Rasmussen, 2006), whereas for a general integer value of vi, Yn(t)|(Xn = i) is a continuous-time analogue of the Auto-regressive process AR(vi)– a process that has been widely used to model physiological time-series data (Stanculescu et al., 2014).",3.2.2. THE MARK PROCESS,[0],[0]
"By constructing Yn(t) as a continuous-time analog of the AR model, the process Y (t) = ∑K n=1 Yn(t)1{τn≤t≤τn+1} becomes a continuous-time switching AR model that is modulated by the patient’s latent clinical state process X(t).",3.2.2. THE MARK PROCESS,[0],[0]
"We observe the continuous-time process Y (t) only at the sampling times dictated by the observation process (tm)m∈N+ , and the resulting process {ym}m∈N+ defines the mark process.",3.2.2. THE MARK PROCESS,[0],[0]
"The observation process together with the mark process, both modulated by the latent clinical state process X(t), constitute a marked Hawkes process, which completely describes a patient’s episode.
",3.2.2. THE MARK PROCESS,[0],[0]
"The mark process defined above is one-dimensional, and hence we need to extend the definition to handle a multidimensional process that represents multiple lab tests and
vital signs.",3.2.2. THE MARK PROCESS,[0],[0]
"In other words, we seek a continuous-time analog of the switching Vector Auto-regressive (VAR) model rather than an AR model2.",3.2.2. THE MARK PROCESS,[0],[0]
"This is achieved by adopting a multi-task Gaussian Process as a model for the Qdimensional physiological process Y (t) ∈ RQ (Durichen et al., 2015).",3.2.2. THE MARK PROCESS,[0],[0]
"That is, we assume that Yn(t)|(Xn = i) ∼ GP(mi(t),Ki(t, t′)), where the covariance kernel Ki(t, t
′) = {ki(r, g, t, t′)}Qr,g=1 is based on the intrinsic correlation model (Bonilla et al., 2007), i.e. Ki(t, t′) can be written in the following separable form
ki(r, g, t, t ′)",3.2.2. THE MARK PROCESS,[0],[0]
"= Σi(r, g) · ki(t, t′), (6)
where ki(r, g, t, t′) is the covariance between the rth physiological variable at time t and the gth physiological variable at time t′,",3.2.2. THE MARK PROCESS,[0],[0]
"Σi is a Q × Q intrinsic correlation matrix, and ki(t, t′) is the Matérn kernel in (5).",3.2.2. THE MARK PROCESS,[0],[0]
"For every state i, we denote the multi-task GP parameter set as Θi = (mi(t),Ki(t, t
′)), and the Hawkes process parameter set as Λi.",3.2.2. THE MARK PROCESS,[0],[0]
"The entire model parameters can be bundled in the parameter set Ω as follows
Ω = {P, (πi, γi,Λi,Θi)i∈X }.
",3.2.2. THE MARK PROCESS,[0],[0]
"Given a parameter set Ω, we can easily generate sample patient episodes from our model by first sampling a state sequence {X1, . .",3.2.2. THE MARK PROCESS,[0],[0]
.,3.2.2. THE MARK PROCESS,[0],[0]
", XK} using the semi-Markov transition kernel, then sampling a corresponding sequence of sojourn times {S1, . .",3.2.2. THE MARK PROCESS,[0],[0]
.,3.2.2. THE MARK PROCESS,[0],[0]
", SK} from the state-dependent Gamma distributions, then sampling a set of multi-task Gaussian process {Y1(t), . .",3.2.2. THE MARK PROCESS,[0],[0]
"., YK(t)}, and finally sampling a sequence of observation times {tm}Mm=1 using Ogata’s modified thinning algorithm (Ogata, 1981).",3.2.2. THE MARK PROCESS,[0],[0]
Figure 3 depicts one patient episode sampled from our model.,3.2.2. THE MARK PROCESS,[0],[0]
(An algorithm for sampling episodes from our model is provided in Appendix B in the supplementary material.),3.2.2. THE MARK PROCESS,[0],[0]
"In this Section, we develop an offline learning algorithm that learns the model parameter Ω using the offline training episodes in D, and a real-time risk scoring algorithm that computes a hospitalized patient’s risk over time.",4. Learning and Inference,[0],[0]
"The learning algorithm operates by first detecting change-points in the physiological data and the observation process; using the detected change-points, the algorithm segments each episode into a sequence of states, and uses an EM algorithm to estimate the model parameters.",4. Learning and Inference,[0],[0]
"The real-time risk scoring algorithm operates by inferring the patient’s current state via forward-filtering, and then computing the probability of eventual absorption in the deteriorating state.
",4. Learning and Inference,[0],[0]
"2In Appendix A of the supplementary material, we establish the connection between the switching multi-task Gaussian Process model described herein and the conventional VAR model, showing that the former is the continuous-time analog of the latter.
0",4. Learning and Inference,[0],[0]
"5 10 15 20 25 0
2
4
6
8
10
Time t
L a te n t S ta te s a n d",4. Learning and Inference,[0],[0]
In te n si ty F u,4. Learning and Inference,[0],[0]
"n ct io n
Clinical state space is X = {1,2,3,4} (state 4 is the “clinical deterioration” state)
0 5 10 15 20 25 0
5
10
15
20
25
Time t
P",4. Learning and Inference,[0],[0]
h y si o lo,4. Learning and Inference,[0],[0]
"g ic a l V a ri a b le s
Y (t) {ym} M m=1
Intensity function λ(t,X(t))",4. Learning and Inference,[0],[0]
"Clinical state process X(t) Observation process {tm}
Informative censoring l = 1
X1 = 2 X2 = 3
X3 = 4
Figure 3.",4. Learning and Inference,[0],[0]
An episode sampled from the proposed model.,4. Learning and Inference,[0],[0]
"Estimating the model parameters Ω from the dataset D = {Ed}Dd=1 is a daunting task due to the hiddenness of the patients’ clinical state trajectories {Xdn, Sdn}Dd=1; an application of MCMC-based inference methods, such as the method in (Qin & Shelton, 2015), will incur an excessive computational cost for a complex model like ours.",4.1. The Offline Learning Algorithm,[0],[0]
We therefore developed an efficient three-step learning algorithm that capitalizes on the structure of the patients’ episodes in order to find a point estimate Ω̂ for Ω.,4.1. The Offline Learning Algorithm,[0],[0]
"The three steps of the our learning algorithm are listed hereunder3.
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 1: Change-point detection We first estimate the jump times {τd1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
"., τdKd} for every episode Ed in D.",4.1. The Offline Learning Algorithm,[0],[0]
"This is achieved by using the E-divisive change-point detection algorithm (Matteson & James, 2014).",4.1. The Offline Learning Algorithm,[0],[0]
"Since the E-divisive algorithm is nonparametric, we are able to estimate the onsets of all clinical states, i.e. the jump times of Xd(t), prior to finding the estimate Ω̂.",4.1. The Offline Learning Algorithm,[0],[0]
"We let the E-divisive algorithm jointly detect changes in the distributions of the observable variables {ydm} Md m=1 and the observation process {tdm} Md m=1 by creating an augmented vector of observables that comprises both the physiological observations and a “differential” observation process, i.e.
{τ̂d1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
"., τ̂dKd}",4.1. The Offline Learning Algorithm,[0],[0]
"= E-divisive((y d 1 ,∆t d 1), . . .",4.1. The Offline Learning Algorithm,[0],[0]
", (y d Md ,∆t d Md)),
where ∆tdm = t d m − tdm−1, with ∆td1 = 0.",4.1. The Offline Learning Algorithm,[0],[0]
"By the end of this step, we obtain an estimate for the start and end times of all clinical state realizations for every episode Ed.
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 2: Maximum Likelihood Estimation (MLE) of the absorbing states’ parameters By virtue of informative censoring, we know the identities of all the absorbing states, i.e. XdKd = l
d, ∀d.",4.1. The Offline Learning Algorithm,[0],[0]
"Now that we have estimates for the onsets of the absorbing states, obtained from step 1, then we can estimate the response
3The details for all the algorithms in this Section are provided in Appendix C in the supplementary material.
times as ŜdKd = T d c",4.1. The Offline Learning Algorithm,[0],[0]
"− τ̂dKd , ∀1",4.1. The Offline Learning Algorithm,[0],[0]
"≤ d ≤ D. Define the (fully observable) sub-dataset Di as follows
Di = { (ym, tm){tm≥τ̂d
Kd }, Ŝ
d",4.1. The Offline Learning Algorithm,[0],[0]
"Kd : l
d = i, Ed ∈ D } ,
∀i ∈ {0, 1}.",4.1. The Offline Learning Algorithm,[0],[0]
"Given such a fully fledged specification of the absorbing states’ onsets, identities, and the corresponding physiological variables, we can directly apply MLE to estimate the parameters Θ1,ΘN , γ1, γN ,Λ1 and ΛN .",4.1. The Offline Learning Algorithm,[0],[0]
"Using the dataset Di, the parameter Θi is estimated using the gradient method for Gaussian processes as in (Bonilla et al., 2007), γi is estimated using the standard MLE estimating equations, and Λi is estimated by maximizing the recursive likelihood formula in (Ogata, 1981) using the Nelder-Mead simplex method (Nelder & Mead, 1965).
",4.1. The Offline Learning Algorithm,[0],[0]
"Step 3: Estimation of the transient states’ parameters using the EM algorithm While the absorbing states are observable, the transient states are all hidden.",4.1. The Offline Learning Algorithm,[0],[0]
"In order to estimate the parameters P, {Θi}N−1i=2 , {γi} N−1 i=2 , and {Λi} N−1 i=2 , we use the jump times’ estimates {τ̂d1 , . .",4.1. The Offline Learning Algorithm,[0],[0]
".,",4.1. The Offline Learning Algorithm,[0],[0]
"τ̂dKd−1} (obtained from step 1) in order to segment every episode d into a set of finite transition,",4.1. The Offline Learning Algorithm,[0],[0]
and hence we obtain a discrete-time HMM-like process.,4.1. The Offline Learning Algorithm,[0],[0]
"We truncate all the episodes by removing the data belonging to the absorbing state, and run the EM algorithm in order to estimate the transient state parameters.",4.1. The Offline Learning Algorithm,[0],[0]
The EM algorithm takes advantage of informative censoring in the forward-backward message passing stage by computing the backward messages conditioned on the identity of the endpoint absorbing state ld for every episode d.,4.1. The Offline Learning Algorithm,[0],[0]
"Having learned the model parameters Ω̂ from an offline dataset D, we now explain how risk scoring is conducted in real-time for a newly hospitalized patient.",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"The patient risk score at time t is denoted by R(t), and is defined as R(t) = P(X(∞)",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
= N,4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"| {ym, tm}, tm ≤ t, Ω̂).",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"That is, the risk score R(t) is the probability of being eventually absorbed in the deteriorating state N given the observable physiological data up to time t. Using Bayes’ rule, the risk score R(t) is given by∑ i∈X P(X(t) =",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"i | {ym, tm})︸ ︷︷ ︸",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
Current state · P(X(∞) = N |X(t) = i)︸ ︷︷ ︸,4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"Future transition ,
where the “current state” term is computed in real-time using the efficient (dynamic programming) forward-filtering algorithm, whereas the “future transition” term is computed offline using the estimated model parameters.",4.2. The Real-time Risk Scoring Algorithm,[0],[0]
"In order to evaluate the prognostic utility of our model, we conducted experiments on a dataset D comprising informa-
tion on patient admissions to a major medical center over a 3-year period, and compared the proposed risk score defined in Subsection 4.2 with other competing baselines.",5. Experiments,[0],[0]
"We briefly describe our dataset in the next Subsection, and then present the experimental results.",5. Experiments,[0],[0]
A very detailed description for our dataset and the implementation of the baselines is provided in Appendix D in the supplementary material.,5. Experiments,[0],[0]
Each patient record in D is an episode that is formatted as described in Section 2.,5.1. Data Description,[0],[0]
"The patients’ cohort in D is very heterogeneous; diagnoses included sepsis, hypertension, renal failure, leukemia, septicemia and pneumonia.",5.1. Data Description,[0],[0]
"Each patient’s episode in D comprises 21 vital signs and lab tests that are collected for the patient over time, along with the time instances at which they where collected.",5.1. Data Description,[0],[0]
"The vital signs include diastolic and systolic blood pressure, Glasgow coma scale score, heart rate, eye opening, respiratory rate, temperature, O2 saturation and device assistance, best motor and verbal responses.",5.1. Data Description,[0],[0]
"The lab tests included measurements of chloride, glucose, urea nitrogen, white blood cell count, creatinine, hemoglobin, platelet count, potassium, sodium and CO2.",5.1. Data Description,[0],[0]
"In all the experiments conducted in this Section, we split D into a training set with admissions over a 2.5-year period (5,000 episodes) and a testing set with admissions over a 6-month period (1,094 episodes).",5.1. Data Description,[0],[0]
"We ran the offline learning algorithm in Subsection 4.1 (with 1000 EM iterations) on the 5,000 training episodes in D, and obtained an estimate Ω̂ for the semi-Markovmodulated marked Hawkes process that describes the patient cohort.",5.2. Results,[0],[0]
"Using the Bayesian information criterion, we selected an instantiation of our model with 4 clinical states, where state 1 is the clinical stability (absorbing) state, and state 4 is the clinical deterioration (absorbing) state.",5.2. Results,[0],[0]
"The learned Hawkes process intensity functions for these two states are given by
λ(t, 1) = 0.55︸︷︷︸ Baseline intensity ↓",5.2. Results,[0],[0]
"+ 0.2 ∑ tm
e−8.46(t−tm)︸ ︷︷ ︸",5.2. Results,[0],[0]
"Temporal dependencies ↓ ,
λ(t, 4) = 0.82︸︷︷︸ Baseline intensity ↑ + 0.16 ∑ tm
e−1.36(t−tm)︸ ︷︷ ︸",5.2. Results,[0],[0]
"Temporal dependencies ↑ ,
where λ(t,X(t)) is measured in samples per hour.",5.2. Results,[0],[0]
"We note that the estimated Hawkes process parameters accurately describe the clinicians’ judgments; when the patient is in the deteriorating state (state 4), the clinicians tend to observe her physiological measurements more frequently.",5.2. Results,[0],[0]
"This manifests in the baseline intensity of λ(t, 4) being
50% higher than that of λ(t, 1).",5.2. Results,[0],[0]
"Moreover, we note that when the patient is clinically stable (state 1), the temporal dependencies between the observation times almost disappear as the exponential triggering kernel plays little role in determining the observation times, i.e. λ(t, 1) ≈ 0.55, which renders the observation process closer to a Poisson process.",5.2. Results,[0],[0]
"Contrarily, when the patient is deteriorating, strong temporal dependencies are displayed in the observation times– this is intuitive since for a deteriorating patient, the follow-up times decided by the clinicians strongly depend on what have been observed in the past.",5.2. Results,[0],[0]
"These distinguishing state-specific features of the clinical judgments are the essence of informative sampling, which allows us to integrate physiological data together with clinical experience while learning the patient’s physiological model.
",5.2. Results,[0],[0]
We illustrate the value of informative sampling in Figure 4 through an episode for a cardiac patient who was admitted in the ward for 1 week before being sent to the ICU upon a cardiac arrest.,5.2. Results,[0],[0]
"When running the offline learning algorithm in Section 4.1 while assuming that the observation process {tdm}m is uninformative (i.e. λ(t, i) = λ(t, j), ∀i, j ∈ X ), the detected clinical deterioration onset is only 10 hours ahead of the cardiac arrest event (onset (2) in Figure 4): this is because the states are estimated solely based on the physiological data, and hence the clinical deterioration state is detected only when the patient’s heart rate fell below 60 beats per minute (BPM).",5.2. Results,[0],[0]
"When running the learning again but with informative sampling taken into account, the detected clinical deterioration onset is 40 hours ahead of the cardiac arrest event (onset (1) in Figure 4); this is the time instance at which the clinicians decided to monitor the patient more vigilantly (i.e. more intense sampling rate) even though the evidence for an upcoming cardiac arrest in her heart rate trajectory was rather subtle.",5.2. Results,[0],[0]
"The clinicians’ decision to intensely observe the patient’s physiological trajectory is based either on their experience, or on apparent symptoms or complains from the patient that were not recorded in the EHR.",5.2. Results,[0],[0]
"By integrating the clinicians’ judgments into our model, we are able to capture the subtleties in the patients’ temporal physiological parameters, and hence learn more accurate representations for the clinical states.
",5.2. Results,[0],[0]
"We then computed the real-time risk score (as described in Subsection 4.2) for the testing episodes, and compared its sensitivity-precision AUCROC with that of the baseline risk scoring methods listed hereunder.
",5.2. Results,[0],[0]
"Medical Risk Scores: we considered the two most commonly used medical risk scores in regular wards– the MEWS score (Morgan et al., 1997) and the Rothman index4 (Rothman et al., 2013).",5.2. Results,[0],[0]
"We implemented the MEWS score and the Rothman index as described in (Kyriacos et al., 2014) and (Rothman et al., 2013), respectively.",5.2. Results,[0],[0]
"We also compared with the APACHE-II and SOFA scores.
",5.2. Results,[0],[0]
"Machine Learning Algorithms: we considered the traditional approach for real-time risk scoring, which treats informative censoring as a surrogate label based on which a supervised regression model is learned offline, and then risk scoring is applied in real-time using the temporal data within a sliding window– we call these methods “slidingwindow methods”.",5.2. Results,[0],[0]
"We implemented sliding-window methods based on logistic regression (Ho et al., 2012; Saria et al., 2010), random forests, and Gaussian process regression (Ghassemi et al., 2015; Yoon et al., 2016).",5.2. Results,[0],[0]
"In addition, we compared our risk score with a standard Hidden Markov Model with Gaussian emissions.",5.2. Results,[0],[0]
"The relevant physiological measurements for every baseline were selected through the correlated feature selection method (Yu & Liu, 2003).",5.2. Results,[0],[0]
"The hyper-parameters of all the baselines, including the size of the sliding window for the supervised learning methods, were optimized via cross-validation.",5.2. Results,[0],[0]
"To handle the irregularly sampled data, we discretized the time horizon into 1-hour steps and fed the baselines with interpolated, discrete-time episodes.
",5.2. Results,[0],[0]
"In Table 1, we compare the performance of our risk score with the baselines in terms of the sensitivity-precision AU-
4At the time of conducting these experiments, the Rothman index was deployed in more than 60 major hospitals in the US.
ROC.",5.2. Results,[0],[0]
"As we can see, the proposed risk score offers a 23% AUROC improvement as compared to the best performing medical risk score –the Rothman index– which was the score deployed in our medical center at the time of conducting this experiment.",5.2. Results,[0],[0]
"Moreover, our risk score also provides significant gains over discriminative sliding-window regression models; the proposed risk score achieves a 11.5% AUROC improvement as compared to the best performing ML algorithm (random forest).",5.2. Results,[0],[0]
"In addition, the proposed risk score achieves a 16% AUROC improvement as compared to a standard HMM with Gaussian emission variables5.",5.2. Results,[0],[0]
"On average, our risk score prompts ICU alarms 8 hours before the censoring time at a sensitivity of 50% and precision of 35%.",5.2. Results,[0],[0]
"We stress that while computing our risk score for the testing episodes, we did not use the information conveyed in the observation process {tdm}m to infer the patients’ clinical states.",6. Discussion: Chicken-and-egg,[0],[0]
"This is because in practice, the value of the realtime risk score R(t) itself influences the clinician’s behavior and hence impacts the observation process, creating a chicken-and-egg dilemma in which one cannot clearly conceptualize the causal relation between the risk score and the clinicians’ judgments.",6. Discussion: Chicken-and-egg,[0],[0]
"A very interesting research direction is to consider an observation process that is modulated by both the patient’s state and the real-time risk score through an intensity function λ(t,X(t), R(t)), where the algorithm learns the clinical state representation online by “sharing experience” with the clinicians.",6. Discussion: Chicken-and-egg,[0],[0]
"That is, the algorithm uses the clinician’s judgments to refine its clinical state model, which leads to a refined risk score R(t) that would in turn allow the clinician to exhibit more accurate judgments; an online learning process that would ideally converge to a state of “shared knowledge” between the clinician and the system.
",6. Discussion: Chicken-and-egg,[0],[0]
The significant prognostic value offered by our risk score promises a great improvement in the quality of subacute care in wards.,6. Discussion: Chicken-and-egg,[0],[0]
"By utilizing the proposed score instead of the current technology, clinicians in a crowded ward can better focus their attention on patients at real risk of deterioration, and can also plan for timely ICU admissions and effective therapeutic interventions.",6. Discussion: Chicken-and-egg,[0],[0]
"With the high in-hospital mortality rates in wards, deploying our risk score may help save thousands of lives annually– we are currently working towards installing the proposed risk score in our medical center.
",6. Discussion: Chicken-and-egg,[0],[0]
"5The adoption of a semi-Markovian model for the clinical state process protects our model from the overtly rapid state switching behavior that is introduced by memoryless HMMs (Matteson & James, 2014).",6. Discussion: Chicken-and-egg,[0],[0]
Critically ill patients in regular wards are vulnerable to unanticipated adverse events which require prompt transfer to the intensive care unit (ICU).,abstractText,[0],[0]
"To allow for accurate prognosis of deteriorating patients, we develop a novel continuoustime probabilistic model for a monitored patient’s temporal sequence of physiological data.",abstractText,[0],[0]
"Our model captures “informatively sampled” patient episodes: the clinicians’ decisions on when to observe a hospitalized patient’s vital signs and lab tests over time are represented by a marked Hawkes process, with intensity parameters that are modulated by the patient’s latent clinical states, and with observable physiological data (mark process) modeled as a switching multi-task Gaussian process.",abstractText,[0],[0]
"In addition, our model captures “informatively censored” patient episodes by representing the patient’s latent clinical states as an absorbing semi-Markov jump process.",abstractText,[0],[0]
The model parameters are learned from offline patient episodes in the electronic health records via an EM-based algorithm.,abstractText,[0],[0]
Experiments conducted on a cohort of patients admitted to a major medical center over a 3-year period show that risk prognosis based on our model significantly outperforms the currently deployed medical risk scores and other baseline machine learning algorithms.,abstractText,[0],[0]
Learning from Clinical Judgments: Semi-Markov-Modulated Marked  Hawkes Processes for Risk Prognosis,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2390–2400 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
Learning sentence representations is central to many natural language modeling applications.,1 Introduction,[0],[0]
The aim of a model for this task is to learn fixedlength feature vectors that encode the semantic and syntactic properties of sentences.,1 Introduction,[0],[0]
"Deep learning techniques have shown promising performance on sentence modeling, via feedforward neural networks (Huang et al., 2013), recurrent neural networks (RNNs) (Hochreiter and Schmidhuber, 1997), convolutional neural networks (CNNs) (Kalchbrenner et al., 2014; Kim, 2014; Shen et al., 2014), and recursive neural networks (Socher et al., 2013).",1 Introduction,[0],[0]
Most of these models are task-dependent: they are trained specifically for a certain task.,1 Introduction,[0],[0]
"However, these methods may be-
come inefficient when we need to repeatedly learn sentence representations for a large number of different tasks, because they may require retraining a new model for each individual task.",1 Introduction,[0],[0]
"In this paper, in contrast, we are primarily interested in learning generic sentence representations that can be used across domains.
",1 Introduction,[0],[0]
Several approaches have been proposed for learning generic sentence embeddings.,1 Introduction,[0],[0]
"The paragraphvector model of Le and Mikolov (2014) incorporates a global context vector into the log-linear neural language model (Mikolov et al., 2013) to learn the sentence representation; however, at prediction time, one needs to perform gradient descent to compute a new vector.",1 Introduction,[0],[0]
"The sequence autoencoder of Dai and Le (2015) describes an encoder-decoder model to reconstruct the input sentence, while the skip-thought model of Kiros et al. (2015) extends the encoder-decoder model to reconstruct the surrounding sentences of an input sentence.",1 Introduction,[0],[0]
"Both the encoder and decoder of the methods above are modeled as RNNs.
",1 Introduction,[0.9523638036654128],['Three datasets from Reddit and one dataset of IRC are used as the experimental datasets.']
"CNNs have recently achieved excellent results in various task-dependent natural language applications as the sentence encoder (Kalchbrenner et al., 2014; Kim, 2014; Hu et al., 2014).",1 Introduction,[0],[0]
This motivates us to propose a CNN encoder for learning generic sentence representations within the framework of encoder-decoder models proposed by Sutskever et al. (2014); Cho et al. (2014).,1 Introduction,[0],[0]
"Specifically, a CNN encoder performs convolution and pooling operations on an input sentence, then uses a fullyconnected layer to produce a fixed-length encoding of the sentence.",1 Introduction,[0],[0]
This encoding vector is then fed into a long short-term memory (LSTM) recurrent network to produce a target sentence.,1 Introduction,[0],[0]
"Depending on the task, we propose three models: (i)",1 Introduction,[0],[0]
"CNNLSTM autoencoder: this model seeks to reconstruct the original input sentence, by capturing the intra-sentence information; (ii) CNN-LSTM future
2390
predictor: this model aims to predict a future sentence, by leveraging inter-sentence information; (iii) CNN-LSTM composite model: in this case, there are two LSTMs, decoding the representation to the input sentence itself and a future sentence.",1 Introduction,[0],[0]
"This composite model aims to learn a sentence encoder that captures both intra- and inter-sentence information.
",1 Introduction,[0],[0]
The proposed CNN-LSTM future predictor model only considers the immediately subsequent sentence as context.,1 Introduction,[0],[0]
"In order to capture longerterm dependencies between sentences, we further introduce a hierarchical encoder-decoder model.",1 Introduction,[0],[0]
This model abstracts the RNN language model of Mikolov et al. (2010) to the sentence level.,1 Introduction,[0],[0]
"That is, instead of using the current word in a sentence to predict future words (sentence continuation), we encode a sentence to predict multiple future sentences (paragraph continuation).",1 Introduction,[0],[0]
"This model is termed hierarchical CNN-LSTM model.
",1 Introduction,[0],[0]
"As in Kiros et al. (2015), we first train our proposed models on a large collection of novels.",1 Introduction,[0],[0]
"We then evaluate the CNN sentence encoder as a generic feature extractor for 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking and 5 standard classification benchmarks.",1 Introduction,[0],[0]
"In these experiments, we train a linear classifier on top of the extracted sentence features, without additional fine-tuning of the CNN.",1 Introduction,[0],[0]
"We show that our trained sentence encoder yields generic repre-
sentations that perform as well as, or better, than those of Kiros et al. (2015); Hill et al. (2016), in all the tasks considered.
",1 Introduction,[0],[0]
"Summarizing, the main contribution of this paper is a new class of CNN-LSTM encoder-decoder models that is able to leverage the vast quantity of unlabeled text for learning generic sentence representations.",1 Introduction,[0],[0]
"Inspired by the skip-thought model (Kiros et al., 2015), we have further explored different variants: (i) CNN is used as the sentence encoder rather than RNN; (ii) larger context windows are considered: we propose the hierarchical CNN-LSTM model to encode a sentence for predicting multiple future sentences.",1 Introduction,[0],[0]
"Consider the sentence pair (sx, sy).",2.1 CNN-LSTM model,[0],[0]
"The encoder, a CNN, encodes the first sentence sx into a feature vector z, which is then fed into an LSTM decoder that predicts the second sentence sy.",2.1 CNN-LSTM model,[0],[0]
Let wtx ∈,2.1 CNN-LSTM model,[0],[0]
"{1, . . .",2.1 CNN-LSTM model,[0],[0]
", V } represent the t-th word in sentences sx, where wtx indexes one element in a V - dimensional set (vocabulary); wty is defined similarly w.r.t.",2.1 CNN-LSTM model,[0],[0]
sy.,2.1 CNN-LSTM model,[0],[0]
"Each word wtx is embedded into a k-dimensional vector xt = We[wtx], where We ∈ Rk×V is a word embedding matrix (learned), and notation We[v] denotes the v-th column of matrix We.",2.1 CNN-LSTM model,[0],[0]
"Similarly, we let yt = We[wty].
CNN encoder The CNN architecture in Kim (2014); Collobert et al. (2011) is used for sentence encoding, which consists of a convolution layer and a max-pooling operation over the entire sentence for each feature map.",2.1 CNN-LSTM model,[0],[0]
"A sentence of length T (padded where necessary) is represented as a matrix X ∈ Rk×T , by concatenating its word embeddings as columns, i.e., the t-th column of X is xt.
",2.1 CNN-LSTM model,[0],[0]
A convolution operation involves a filter Wc ∈,2.1 CNN-LSTM model,[0],[0]
"Rk×h, applied to a window of h words to produce a new feature.",2.1 CNN-LSTM model,[0],[0]
"According to Collobert et al. (2011), we can induce one feature map c = f(X ∗Wc + b) ∈ RT−h+1, where f(·) is a nonlinear activation function such as the hyperbolic tangent used in our experiments, b ∈ RT−h+1 is a bias vector, and ∗ denotes the convolutional operator.",2.1 CNN-LSTM model,[0],[0]
Convolving the same filter with the h-gram at every position in the sentence allows the features to be extracted independently of their position in the sentence.,2.1 CNN-LSTM model,[0],[0]
"We then apply a max-over-time pooling operation (Collobert et al., 2011) to the feature map and take its maximum value, i.e., ĉ = max{c}, as the feature corresponding to this particular filter.",2.1 CNN-LSTM model,[0],[0]
"This pooling scheme tries to capture the most important feature, i.e., the one with the highest value, for each feature map, effectively filtering out less informative compositions of words.",2.1 CNN-LSTM model,[0],[0]
"Further, pooling also guarantees that the extracted features are independent of the length of the input sentence.
",2.1 CNN-LSTM model,[0],[0]
The above process describes how one feature is extracted from one filter.,2.1 CNN-LSTM model,[0],[0]
"In practice, the model uses multiple filters with varying window sizes (Kim, 2014).",2.1 CNN-LSTM model,[0],[0]
"Each filter can be considered as a linguistic feature detector that learns to recognize a specific class of n-grams (or h-grams, in the above notation).",2.1 CNN-LSTM model,[0],[0]
"However, since the h-grams are computed in the embedding space, the model naturally handles similar h-grams composed of synonyms.",2.1 CNN-LSTM model,[0],[0]
"Assume we have m window sizes, and for each window size, we use d filters; then we obtain a md-dimensional vector to represent a sentence.
",2.1 CNN-LSTM model,[0],[0]
"Compared with the LSTM encoders used in Kiros et al. (2015); Dai and Le (2015); Hill et al. (2016), a CNN encoder may have the following advantages.",2.1 CNN-LSTM model,[0],[0]
"First, the sparse connectivity of a CNN, which indicates fewer parameters are required, typically improves its statistical efficiency as well as reduces memory requirements (Goodfellow et al., 2016).",2.1 CNN-LSTM model,[0],[0]
"For example, excluding the number of parameters used in the word embeddings, our trained CNN sentence encoder has 3 million parameters,
while the skip-thought vector of Kiros et al. (2015) contains 40 million parameters.",2.1 CNN-LSTM model,[0],[0]
"Second, a CNN is easy to implement in parallel over the whole sentence, while an LSTM needs sequential computation.
",2.1 CNN-LSTM model,[0],[0]
LSTM decoder The CNN encoder maps sentence sx into a vector z.,2.1 CNN-LSTM model,[0],[0]
"The probability of a length-T sentence sy given the encoded feature vector z is defined as
p(sy|z) =",2.1 CNN-LSTM model,[0],[0]
"T∏
t=1
p(wty|w0y, . . .",2.1 CNN-LSTM model,[0],[0]
", wt−1y , z) (1)
where w0y is defined as a special start-of-thesentence token.",2.1 CNN-LSTM model,[0],[0]
"All the words in the sentence are sequentially generated using the RNN, until the end-of-the-sentence symbol is generated.",2.1 CNN-LSTM model,[0],[0]
"Specifically, each conditional p(wty|w<ty , z), where < t = {0, . . .",2.1 CNN-LSTM model,[0],[0]
", t− 1}, is specified as softmax(Vht), where ht, the hidden units, are recursively updated through ht = H(yt−1,ht−1, z), and h0 is defined as a zero vector (h0 is not updated during training).",2.1 CNN-LSTM model,[0],[0]
V is a weight matrix used for computing a distribution over words.,2.1 CNN-LSTM model,[0],[0]
Bias terms are omitted for simplicity throughout the paper.,2.1 CNN-LSTM model,[0],[0]
"The transition function H(·) is implemented with an LSTM (Hochreiter and Schmidhuber, 1997).
",2.1 CNN-LSTM model,[0],[0]
"Given the sentence pair (sx, sy), the objective function is the sum of the log-probabilities of the target sentence conditioned on the encoder representation in (1): ∑T t=1 log p(w t y|w<ty , z).",2.1 CNN-LSTM model,[0],[0]
"The total objective is the above objective summed over all the sentence pairs.
",2.1 CNN-LSTM model,[0],[0]
"Applications Inspired by Srivastava et al. (2015), we propose three models: (i) an autoencoder, (ii) a future predictor, and (iii) the composite model.",2.1 CNN-LSTM model,[0],[0]
"These models share the same CNN-LSTM model architecture, but are different in terms of the choices of the target sentence.",2.1 CNN-LSTM model,[0],[0]
"An illustration of the proposed encoder-decoder models is shown in Figure 1(left).
",2.1 CNN-LSTM model,[0],[0]
The autoencoder (i) aims to reconstruct the same sentence as the input.,2.1 CNN-LSTM model,[0],[0]
"The intuition behind this is that an autoencoder learns to represent the data using features that explain its own important factors of variation, and hence model the internal structure of sentences, effectively capturing the intrasentence information.",2.1 CNN-LSTM model,[0],[0]
Another natural task is encoding an input sentence to predict the subsequent sentence.,2.1 CNN-LSTM model,[0],[0]
"The future predictor (ii) achieves this, effectively capturing the inter-sentence information,
which has been shown to be useful to learn the semantics of a sentence (Kiros et al., 2015).",2.1 CNN-LSTM model,[0],[0]
"These two tasks can be combined to create a composite model (iii), where the CNN encoder is asked to learn a feature vector that is useful to simultaneously reconstruct the input sentence and predict a future sentence.",2.1 CNN-LSTM model,[0],[0]
This composite model encourages the sentence encoder to incorporate contextual information both within and beyond the sentence.,2.1 CNN-LSTM model,[0],[0]
The future predictor described in Section 2.1 only considers the immediately subsequent sentence as context.,2.2 Hierarchical CNN-LSTM model,[0],[0]
"By utilizing a larger surrounding context, it is likely that we can learn even higher-quality sentence representations.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Inspired by the standard RNN-based language model (Mikolov et al., 2010) that uses the current word to predict future words, we propose a hierarchical encoder-decoder model that encodes the current sentence to predict multiple future sentences.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"An illustration of the hierarchical model is shown in Figure 1(right), with details provided in Figure 2.
",2.2 Hierarchical CNN-LSTM model,[0],[0]
Our proposed hierarchical model characterizes the hierarchy word-sentence-paragraph.,2.2 Hierarchical CNN-LSTM model,[0],[0]
"A paragraph is modeled as a sequence of sentences, and each sentence is modeled as a sequence of words.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Specifically, assume we are given a paragraph D = (s1, . . .",2.2 Hierarchical CNN-LSTM model,[0],[0]
", sL), that consists of L sentences.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"The probability for paragraph D is then defined as
p(D) = L∏
`=1
p(s`|s<`) (2)
where s0 is defined as a special start-of-theparagraph token.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"As shown in Figure 2(left), each p(s`|s<`) in (2) is calculated as
p(s`|s<`) = p(s`|h(p)` ) (3) h
(p) ` = LSTMp(h (p) `−1, z`−1) (4)
z`−1 = CNN(s`−1) (5)
where h(p)` denotes the `-th hidden state of the LSTM paragraph generator, and h(p)0 is fixed as a zero vector.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"The CNN in (5) is as described in Section 2.1, encoding the sentence s`−1 into a vector representation z`−1.
",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Equation (4) serves as the paragraph-level language model (Mikolov et al., 2010), which encodes all the previous sentence representations z<` into a vector representation h(p)` .",2.2 Hierarchical CNN-LSTM model,[0],[0]
"This hidden state h (p) `
is used to guide the generation of the `-th sentence through the decoder (3), which is defined as
p(s`|h(p)` )",2.2 Hierarchical CNN-LSTM model,[0],[0]
"= T∏̀
t=1
p(w`,t|w`,<t,h(p)` ) (6)
where w`,0 is defined as a special start-of-thesentence token.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"T` is the length of sentence `, and w`,t denotes the t-th word in sentence `.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"As shown in Figure 2(right), each p(w`,t|w`,<t,h(p)` ) in (6) is calculated as
p(w`,t|w`,<t,h(p)` )",2.2 Hierarchical CNN-LSTM model,[0],[0]
"= softmax(Vh(s)`,t ) (7) h
(s) `,t = LSTMs(h (s) `,t−1,x`,t−1,h (p) ` ) (8)
where h(s)`,t denotes the t-th hidden state of the LSTM decoder for sentence `, x`,t−1 denotes the word embedding for w`,t−1, and h (s) `,0 is fixed as a zero vector for all ` = 1, . . .",2.2 Hierarchical CNN-LSTM model,[0],[0]
", L. V is a weight matrix used for computing distribution over words.",2.2 Hierarchical CNN-LSTM model,[0],[0]
"Various methods have been proposed for sentence modeling, which generally fall into two categories.",3 Related work,[0],[0]
"The first consists of models trained specifically for a certain task, typically combined with downstream applications.",3 Related work,[0],[0]
"Several models have been proposed along this line, ranging from simple additional composition of the word vectors (Mitchell and Lapata, 2010; Yu and Dredze, 2015; Iyyer et al., 2015) to those based on complex nonlinear functions like recursive neural networks (Socher et al., 2011, 2013), convolutional neural networks (Kalchbrenner et al., 2014; Hu et al., 2014; Johnson and Zhang, 2015; Zhang et al., 2015; Gan et al., 2017), and recurrent neural networks (Tai et al., 2015; Lin et al., 2017).
",3 Related work,[0],[0]
The other category consists of methods aiming to learn generic sentence representations that can be used across domains.,3 Related work,[0],[0]
"This includes the paragraph vector (Le and Mikolov, 2014), skip-thought vector (Kiros et al., 2015), and the sequential denoising autoencoders (Hill et al., 2016).",3 Related work,[0],[0]
"Hill et al. (2016) also proposed a sentence-level log-linear bag-of-words (BoW) model, where a BoW representation of an input sentence is used to predict adjacent sentences that are also represented as BoW. Most recently, Wieting et al. (2016); Arora et al. (2017); Pagliardini et al. (2017) proposed methods in which sentences are represented as a weighted average of fixed (pre-trained) word vectors.",3 Related work,[0],[0]
"Our model falls into this category, and is most related to Kiros et al. (2015).
",3 Related work,[0],[0]
"However, there are two key aspects that make our model different from Kiros et al. (2015).",3 Related work,[0],[0]
"First, we use CNN as the sentence encoder.",3 Related work,[0],[0]
"The combination of CNN and LSTM has been considered in image captioning (Karpathy and Fei-Fei, 2015), and in some recent work on machine translation (Kalchbrenner and Blunsom, 2013; Meng et al., 2015; Gehring et al., 2016).",3 Related work,[0],[0]
"Our utilization of a CNN is different, and more importantly, the ultimate goal of our model is different.",3 Related work,[0],[0]
"Our work aims to use a CNN to learn generic sentence embeddings.
",3 Related work,[0],[0]
"Second, we use the hierarchical CNN-LSTM model to predict multiple future sentences, rather than the surrounding two sentences as in Kiros et al. (2015).",3 Related work,[0],[0]
"Utilizing a larger context window aids our model to learn better sentence representations, capturing longer-term dependencies between sentences.",3 Related work,[0],[0]
Similar work to this hierarchical language modeling can be found in Li et al. (2015); Sordoni et al. (2015); Lin et al. (2015); Wang and Cho (2016).,3 Related work,[0],[0]
"Specifically, Li et al. (2015); Sordoni et al. (2015) uses an LSTM for the sentence encoder, while Lin et al. (2015) uses a bag-of-words to represent sentences.",3 Related work,[0],[0]
"We first provide qualitative analysis of our CNN encoder, and then present experimental results on 8 tasks: 5 classification benchmarks, paraphrase detection, semantic relatedness and image-sentence ranking.",4 Experiments,[0],[0]
"As in Kiros et al. (2015), we evaluate the capabilities of our encoder as a generic feature extractor.",4 Experiments,[0],[0]
"To further demonstrate the advantage of our learned generic sentence representations, we also fine-tune our trained sentence encoder on the 5 clas-
sification benchmarks.",4 Experiments,[0],[0]
"All the CNN-LSTM models are trained using the BookCorpus dataset (Zhu et al., 2015), which consists of 70 million sentences from over 7000 books.
",4 Experiments,[0],[0]
"We train four models in total: (i) an autoencoder, (ii) a future predictor, (iii) the composite model, and (iv) the hierarchical model.",4 Experiments,[0],[0]
"For the CNN encoder, we employ filter windows (h) of sizes {3,4,5} with 800 feature maps each, hence each sentence is represented as a 2400-dimensional vector.",4 Experiments,[0],[0]
"For both, the LSTM sentence decoder and paragraph generator, we use one hidden layer of 600 units.
",4 Experiments,[0],[0]
"The CNN-LSTM models are trained with a vocabulary size of 22,154 words.",4 Experiments,[0],[0]
"In order to learn a generic sentence encoder that can encode a large number of possible words, we use two methods of considering words not in the training set.",4 Experiments,[0],[0]
"Suppose we have a large pretrained word embedding matrix, such as the publicly available word2vec vectors (Mikolov et al., 2013), in which all test words are assumed to reside.
",4 Experiments,[0],[0]
"The first method learns a linear mapping between the word2vec embedding space Vw2v and the learned word embedding space Vcnn by solving a linear regression problem (Kiros et al., 2015).",4 Experiments,[0],[0]
"Thus, any word from Vw2v can be mapped into Vcnn for encoding sentences.",4 Experiments,[0],[0]
"The second method fixes the word vectors in Vcnn as the corresponding word vectors in Vw2v , and we do not update the word embedding parameters during training.",4 Experiments,[0],[0]
"Thus, any word vector from Vw2v can be naturally used to encode sentences.",4 Experiments,[0],[0]
"By doing this, our trained sentence encoder can successfully encode 931,331 words.
",4 Experiments,[0],[0]
"For training, all weights in the CNN and nonrecurrent weights in the LSTM are initialized from a uniform distribution in [-0.01,0.01].",4 Experiments,[0],[0]
Orthogonal initialization is employed on the recurrent matrices in the LSTM.,4 Experiments,[0],[0]
All bias terms are initialized to zero.,4 Experiments,[0],[0]
The initial forget gate bias for LSTM is set to 3.,4 Experiments,[0],[0]
"Gradients are clipped if the norm of the parameter vector exceeds 5 (Sutskever et al., 2014).",4 Experiments,[0],[0]
"The Adam algorithm (Kingma and Ba, 2015) with learning rate 2× 10−4 is utilized for optimization.",4 Experiments,[0],[0]
"For all the CNN-LSTM models, we use mini-batches of size 64.",4 Experiments,[0],[0]
"For the hierarchical CNN-LSTM model, we use mini-batches of size 8, and each paragraph is composed of 8 sentences.",4 Experiments,[0],[0]
"We do not perform any regularization other than dropout (Srivastava et al., 2014).",4 Experiments,[0],[0]
"All experiments are implemented
in Theano (Bastien et al., 2012), using a NVIDIA GeForce GTX TITAN X GPU with 12GB memory.",4 Experiments,[0],[0]
"We first demonstrate that the sentence representation learned by our model exhibits a structure that makes it possible to perform analogical reasoning using simple vector arithmetics, as illustrated in Table 1.",4.1 Qualitative analysis,[0],[0]
It demonstrates that the arithmetic operations on the sentence representations correspond to wordlevel addition and subtractions.,4.1 Qualitative analysis,[0],[0]
"For instance, in the 3rd example, our encoder captures that the difference between sentence B and C is “you"" and “him"", so that the former word in sentence A is replaced by the latter (i.e., “you”-“you”+“him”=“him”), resulting in sentence D.
Table 2 shows nearest neighbors of sentences from a CNN-LSTM autoencoder trained on the BookCorpus dataset.",4.1 Qualitative analysis,[0],[0]
Nearest neighbors are scored by cosine similarity from a random sample of 1 million sentences from the BookCorpus dataset.,4.1 Qualitative analysis,[0],[0]
"As can be seen, our encoder learns to accurately
capture semantic and syntax of the sentences.",4.1 Qualitative analysis,[0],[0]
"Classification benchmarks We first study the task of sentence classification on 5 datasets: MR (Pang and Lee, 2005), CR (Hu and Liu, 2004), SUBJ (Pang and Lee, 2004), MPQA (Wiebe et al., 2005) and TREC (Li and Roth, 2002).",4.2 Quantitative evaluations,[0],[0]
"On all the datasets, we separately train a logistic regression model on top of the extracted sentence features.",4.2 Quantitative evaluations,[0],[0]
We restrict our comparison to methods that also aims to learn generic sentence embeddings for fair comparison.,4.2 Quantitative evaluations,[0],[0]
We also provide the state-of-the-art results using task-dependent learning methods for reference.,4.2 Quantitative evaluations,[0],[0]
Results are summarized in Table 3.,4.2 Quantitative evaluations,[0],[0]
"Our CNN encoder provides better results than the combine-skip model of Kiros et al. (2015) on all the 5 datasets.
",4.2 Quantitative evaluations,[0],[0]
We highlight some observations.,4.2 Quantitative evaluations,[0],[0]
"First, the autoencoder performs better than the future predictor, indicating that the intra-sentence information may be more important for classification than the inter-sentence information.",4.2 Quantitative evaluations,[0],[0]
"Second, the hierarchi-
cal model performs better than the future predictor, demonstrating the importance of capturing longterm dependencies across multiple sentences.",4.2 Quantitative evaluations,[0],[0]
"Our combined model, which concatenates the feature vectors learned from both the hierarchical model and the composite model, performs the best.",4.2 Quantitative evaluations,[0],[0]
This may be due to that: (i) both intra- and long-term inter-sentence information are leveraged; (ii) it is easier to linearly separate the feature vectors in higher dimensional spaces.,4.2 Quantitative evaluations,[0],[0]
"Further, using (fixed) pre-trained word embeddings consistently provides better performance than using the learned word embeddings.",4.2 Quantitative evaluations,[0],[0]
"This may be due to that word2vec provides more generic word representations, since it is trained on the large Google News dataset (containing 100 billion words) (Mikolov et al., 2013).
",4.2 Quantitative evaluations,[0],[0]
"To further demonstrate the advantage of the learned generic representations, we train a CNN classifier (i.e., a CNN encoder with a logistic regression model on top) with two different initialization strategies: random initialization and initialization with trained parameters from the CNN-LSTM composite model.",4.2 Quantitative evaluations,[0],[0]
Results are shown in Figure 3(left).,4.2 Quantitative evaluations,[0],[0]
"The pretraining provides substantial improvements
(3.52% on average) over random initialization of CNN parameters.",4.2 Quantitative evaluations,[0],[0]
Figure 3(right) shows the effect of pretraining as the number of labeled sentences is varied.,4.2 Quantitative evaluations,[0],[0]
"For the TREC dataset, the performance improves from 79.7% to 84.1% when only 10% sentences are labeled.",4.2 Quantitative evaluations,[0],[0]
"As the size of the set of labeled sentences grows, the improvement becomes smaller, as expected.",4.2 Quantitative evaluations,[0],[0]
"For future work, our CNNLSTM model can be also used for semi-supervised
learning, with the autoencoder on all the data (labeled and unlabled), and the classifier only on the labeled data.
",4.2 Quantitative evaluations,[0],[0]
"Paraphrase detection Now we consider paraphrase detection on the MSRP dataset (Dolan et al., 2004).",4.2 Quantitative evaluations,[0],[0]
"On this task, one needs to predict whether or not two sentences are paraphrases.",4.2 Quantitative evaluations,[0],[0]
"The training set consists of 4076 sentence pairs, and the test set has 1725 pairs.",4.2 Quantitative evaluations,[0],[0]
"As in Tai et al. (2015), given two sentence representations zx and zy, we first compute their element-wise product zx zy and their absolute difference |zx",4.2 Quantitative evaluations,[0],[0]
"− zy|, and then concatenate them together.",4.2 Quantitative evaluations,[0],[0]
A logistic regression model is trained on top of the concatenated features to predict whether two sentences are paraphrases.,4.2 Quantitative evaluations,[0],[0]
We present our results on the last column of Table 3.,4.2 Quantitative evaluations,[0],[0]
"Our best result is better than the other results that use task-independent methods.
",4.2 Quantitative evaluations,[0],[0]
"Image-sentence ranking We consider the task of image-sentence ranking, which aims to retrieve items in one modality given a query from the other.",4.2 Quantitative evaluations,[0],[0]
"We use the COCO dataset (Lin et al., 2014), which contains 123,287 images each with 5 captions.",4.2 Quantitative evaluations,[0],[0]
For development and testing we use the same splits as Karpathy and Fei-Fei (2015).,4.2 Quantitative evaluations,[0],[0]
The development and test sets each contain 5000 images.,4.2 Quantitative evaluations,[0],[0]
"We further split them into 5 random sets of 1000 images, and report the average performance over the 5 splits.",4.2 Quantitative evaluations,[0],[0]
"Performance is evaluated using Recall@K, which measures the average times a correct item is found within the top-K retrieved results.",4.2 Quantitative evaluations,[0],[0]
"We also report the median rank of the closest ground truth result
in the ranked list.",4.2 Quantitative evaluations,[0],[0]
"We represent images using 4096-dimensional feature vectors from VggNet (Simonyan and Zisserman, 2015).",4.2 Quantitative evaluations,[0],[0]
Each caption is encoded using our trained CNN encoder.,4.2 Quantitative evaluations,[0],[0]
"The training objective is the same pairwise ranking loss as used in Kiros et al. (2015), which takes the form of max(0, α− f(xn, yn) +",4.2 Quantitative evaluations,[0],[0]
"f(xn, ym)), where f(·, ·) is the image-sentence score.",4.2 Quantitative evaluations,[0],[0]
"(xn, yn) denotes the related image-sentence pair, and (xn, ym) is the randomly sampled unrelated image-sentence pair with n 6= m. For image retrieval from sentences, x denotes the caption, y denotes the image, and vice versa.",4.2 Quantitative evaluations,[0],[0]
"The objective is to force the matching score of the related pair (xn, yn) to be greater than the unrelated pair (xn, ym) by a margin α, which is set to 0.1 in our experiments.
",4.2 Quantitative evaluations,[0],[0]
Table 4 shows our results.,4.2 Quantitative evaluations,[0],[0]
"Consistent with previous experiments, we empirically found that the encoder trained using the fixed word embedding performed better on this task, hence only results using this method are reported.",4.2 Quantitative evaluations,[0],[0]
"As can be seen, we obtain the same median rank as in Kiros et al. (2015), indicating that our encoder is as competitive as the skip-thought vectors (Kiros et al., 2015).",4.2 Quantitative evaluations,[0],[0]
"The performance gain between our encoder and the combine-skip model of Kiros et al. (2015) on the R@1 score is significant, which shows that the CNN encoder has more discriminative power on re-
trieving the most correct item than the skip-thought vector.
",4.2 Quantitative evaluations,[0],[0]
"Semantic relatedness For our final experiment, we consider the task of semantic relatedness on the SICK dataset (Marelli et al., 2014), consisting of 9927 sentence pairs.",4.2 Quantitative evaluations,[0],[0]
"Given two sentences, our goal is to produce a real-valued score between [1, 5] to indicate how semantically related two sentences are, based on human generated scores.",4.2 Quantitative evaluations,[0],[0]
We compute a feature vector representing the pair of sentences in the same way as on the MSRP dataset.,4.2 Quantitative evaluations,[0],[0]
"We follow the method in Tai et al. (2015), and use the crossentropy loss for training.",4.2 Quantitative evaluations,[0],[0]
Results are summarized in Table 5.,4.2 Quantitative evaluations,[0],[0]
Our result is better than the combineskip model of Kiros et al. (2015).,4.2 Quantitative evaluations,[0],[0]
This suggests that CNN also provides competitive performance at matching human relatedness judgements.,4.2 Quantitative evaluations,[0],[0]
We presented a new class of CNN-LSTM encoderdecoder models to learn sentence representations from unlabeled text.,5 Conclusion,[0],[0]
"Our trained convolutional encoder is highly generic, and can be an alternative to the skip-thought vectors of Kiros et al. (2015).",5 Conclusion,[0],[0]
Compelling experimental results on several tasks demonstrated the advantages of our approach.,5 Conclusion,[0],[0]
"In future work, we aim to use more advanced CNN architectures (Conneau et al., 2016) for learning generic sentence embeddings.",5 Conclusion,[0],[0]
"This research was supported by ARO, DARPA, DOE, NGA, ONR and NSF.",Acknowledgments,[0],[0]
We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes.,abstractText,[0],[0]
"The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder.",abstractText,[0],[0]
"Several tasks are considered, including sentence reconstruction and future sentence prediction.",abstractText,[0],[0]
"Further, a hierarchical encoderdecoder model is proposed to encode a sentence to predict multiple future sentences.",abstractText,[0],[0]
"By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice.",abstractText,[0],[0]
"Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.",abstractText,[0],[0]
Learning Generic Sentence Representations Using Convolutional Neural Networks,title,[0],[0]
"Real-world interactions among multiple entities are often recorded as asynchronous event sequences, such as user behaviors in social networks, job hunting and hopping among companies, and diseases and their complications.",1. Introduction,[0],[0]
The entities or event types in the sequences often exhibit selftriggering and mutually-triggering patterns.,1. Introduction,[0],[0]
"For example, a tweet of a twitter user may trigger further responses from her friends (Zhao et al., 2015).",1. Introduction,[0],[0]
"A disease of a patient may trigger other complications (Choi et al., 2015).",1. Introduction,[0],[0]
"Hawkes processes, an important kind of temporal point process model (Hawkes & Oakes, 1974), have capability to describe the triggering patterns quantitatively and capture the infectivity network of the entities.
",1. Introduction,[0],[0]
"1Georgia Institute of Technology, Atlanta, Georgia, USA 2University of Toronto, Toronto, Ontario, Canada.",1. Introduction,[0],[0]
"Correspondence to: Hongteng Xu <hxu42@gatech.edu>, Dixin Luo <dixin.luo@utoronto.ca>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"Despite the usefulness of Hawkes processes, robust learning of Hawkes processes often needs many event sequences with events occurring over a long observation window.",1. Introduction,[0],[0]
"Unfortunately, the observation window is likely to be very short and sequence-specific in many important practical applications, i.e., within an imagined universal window, each sequence is only observed with a corresponding short subinterval of it, and the events outside this sub-interval are not observed — we call them short doubly-censored (SDC) event sequences.",1. Introduction,[0],[0]
"Existing learning algorithms of Hawkes processes directly applied to SDCs may suffer from overfitting, and what is worse, the triggering patterns between historical events and current ones are lost, so that the triggering patterns learned from SDC event sequences are often unreliable.",1. Introduction,[0],[0]
"This problem is a thorny issue in several practical applications, especially in those having timevarying triggering patterns.",1. Introduction,[0],[0]
"For example, the disease networks of patients should evolve with the increase of age.",1. Introduction,[0],[0]
"However, it is very hard to track and record people’s diseases on a life-time scale.",1. Introduction,[0],[0]
"Instead, we can only obtain their several admissions (even only one admission) in a hospital during one or two years, which are just SDC event sequences.",1. Introduction,[0],[0]
"Therefore, it is highly desirable to propose a method to learn Hawkes processes having a longtime support from a collection of SDC event sequences
In this paper, we propose a novel and simple data synthesis method to enhance the robustness of learning algorithms for Hawkes processes.",1. Introduction,[0],[0]
Fig. 1 illustrates the principle of our method.,1. Introduction,[0],[0]
"Given a set of SDC event sequences, we sample predecessor for each event sequence from potential candidates and stitch them together as new training data.",1. Introduction,[0],[0]
"In the sampling step, the distribution of predecessor (and successor) is estimated according to the similarities between
current sequence and its candidates, and the similarity is defined based on the information of time stamps and (optional) features of event sequences.",1. Introduction,[0],[0]
We analyze the rationality and the feasibility of our data synthesis method and discuss the necessary condition for using the method.,1. Introduction,[0],[0]
Experimental results show that our data synthesis method indeed helps to improve the robustness of various learning algorithms for Hawkes processes.,1. Introduction,[0],[0]
"Especially in the case of time-varying Hawkes processes, applying our method in the learning phase achieves much better results than learning directly from SDC event sequences, which is meaningful for many practical applications, e.g., constructing dynamical disease network, and learning long-term infectivity among different IT companies.",1. Introduction,[0],[0]
"An event sequence can be represented as s = {(ti, ci)}Mi=1, where time stamps ti’s are in an observation window",2. Related Work,[0],[0]
"[Tb, Te] and events ci’s are in a set of event types C = {1, ..., C}.",2. Related Work,[0],[0]
"A point process {Nc}c∈C is a random process model taking event sequences as instances, where Nc = {Nc(t)|t ∈",2. Related Work,[0],[0]
"[Tb, Te]} and Nc(t) is the number of type-c events occurring at or before time t. A point process can be characterized via its conditional intensity function λc(t) = E[dNc(t)|HCt ]/dt, where c ∈ C and HCt",2. Related Work,[0],[0]
"= {(ti, ci)|ti < t, ci ∈ C} is the set of history.",2. Related Work,[0],[0]
"It represents the expected instantaneous happening rate of events given historical record (Daley & Vere-Jones, 2007).",2. Related Work,[0],[0]
"The intensity is often modeled with certain parameters Θ to capture the phenomena of interests, i.e., self-triggering (Hawkes & Oakes, 1974) or self-correcting (Xu et al., 2015).",2. Related Work,[0],[0]
"Based on {λc(t)}c∈C , the likelihood of an event sequence s is
L(s; Θ) =",2. Related Work,[0],[0]
"∏
i λci(ti) exp
( − ∑
c ∫ Te Tb λc(s)ds ) .",2. Related Work,[0],[0]
"(1)
Hawkes Processes.",2. Related Work,[0],[0]
"Hawkes processes (Hawkes & Oakes, 1974) have a particular form of intensity:
λc(t) = µc + ∑C
c′=1 ∫ t 0 φcc′(t, s)dNc′(s), (2)
where µc is the exogenous base intensity independent of the history while ∫ t 0 φcc′(t, s)dNc′(s) is the endogenous intensity capturing the influence of historical events on type-c ones at time t (Xu et al., 2016a).",2. Related Work,[0],[0]
"Here, φcc′(t, s) ≥ 0 is called impact function.",2. Related Work,[0],[0]
"It quantifies the influence of the type-c′ event at time s to the type-c event at time t. Hawkes processes provide us with a physically-meaningful model to capture the infectivity among various events, which are used in social network analysis (Zhou et al., 2013b; Zhao et al., 2015), behavior analysis (Yang & Zha, 2013; Luo et al., 2015) and financial analysis (Bacry et al., 2013).",2. Related Work,[0],[0]
"However, the methods in these references assume
that the impact function is shift-invariant (i.e., φcc′(t, s) = φcc′(t− s), t ≥ s), which limits their applications on longtime scale.",2. Related Work,[0],[0]
"Recently, the time-dependent Hawkes process (TiDeH) in (Kobayashi & Lambiotte, 2016) and the neural network-based Hawkes process in (Mei & Eisner, 2016) learn very flexible Hawkes processes with complicated intensity functions.",2. Related Work,[0],[0]
"Because they highly depend on the size and the quality of data, they may fail in the case of SDC event sequences.
",2. Related Work,[0],[0]
Learning from Imperfect Observations.,2. Related Work,[0],[0]
"In practice, we need to learn sequential models from imperfect observations (e.g., interleaved (Xu et al., 2016b), aggregated (Luo et al., 2016) and extremely-short sequences (Xu et al., 2016c)).",2. Related Work,[0],[0]
"Multiple imputation (MI) (Rubin, 2009) is a general framework to build surrogate observations from the current model.",2. Related Work,[0],[0]
"For time series, bootstrap method (Efron, 1982; Politis & Romano, 1994; Gonçalves & Kilian, 2004) and its variants (Paparoditis & Politis, 2001; Guan & Loh, 2007) have been used to improve learning results when observations are insufficient.",2. Related Work,[0],[0]
"In survival analysis, many techniques have been made to deal with truncated and censored data (Turnbull, 1974; De Gruttola & Lagakos, 1989; Klein & Moeschberger, 2005; Van den Berg & Drepper, 2016).",2. Related Work,[0],[0]
"For point processes, the global (Streit, 2010) or local (Fan, 2009) likelihood maximization estimators (MLE) are used to learn Poisson processes.",2. Related Work,[0],[0]
"Nonparametric approaches for non-homogeneous Poisson processes use the pseudo MLE (Sun & Kalbfleisch, 1995) or full MLE (Wellner & Zhang, 2000).",2. Related Work,[0],[0]
"The bootstrap methods above are also used to learn point processes (Cowling et al., 1996; Guan & Loh, 2007; Kirk & Stumpf, 2009).",2. Related Work,[0],[0]
"To learn Hawkes processes robustly, structural constraints, e.g., low-rank (Luo et al., 2015) and group-sparse regularizers (Xu et al., 2016a), are introduced.",2. Related Work,[0],[0]
"However, all of these methods do not consider the case of SDC event sequences for Hawkes processes.",2. Related Work,[0],[0]
Suppose that the original complete event sequences are in a long observation window.,3. Learning from SDC Event Sequences,[0],[0]
"However, the observation window in practice might be segmented into several intervals {Tnb , Tne }Nn=1, and we can only observe Kn SDC sequences {snk} Kn k=1 in the n-th interval, n = 1, ..., N .",3. Learning from SDC Event Sequences,[0],[0]
"Although we can still apply maximum likelihood estimator to learn Hawkes processes, i.e.,
minΘ− ∑
n,k logL(snk ; Θ), (3)
the SDC event sequences would lead to over-fitting problem and the loss of triggering patterns.",3. Learning from SDC Event Sequences,[0],[0]
Can we do better in such a situation?,3. Learning from SDC Event Sequences,[0],[0]
"In this work, we propose a data synthesis method based on a sampling-stitching mechanism, which extends SDC event sequences to longer ones and enhances the robustness of learning algorithms.",3. Learning from SDC Event Sequences,[0],[0]
Denote the k-th SDC event sequence in the n-th interval as snk .,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Because its predecessor is unavailable, if we learn the parameters of our model via (3) directly, we actually impose a strong assumption on our data that there is no event happening before snk (or previous events are too far away from snk to have influences on s n k ).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Obviously, this assumption is questionable — it is likely that there are influential events happening before snk .",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"A more reasonable strategy is enumerating potential predecessors and maximizing the expected log-likelihood over the whole observation window:
minΘ− ∑
n,k Es∼HC
Tn b
[logL([s, snk ]; Θ)].",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"(4)
Here Ex∼D[f(x)] represents the expectation of function f(x) with random variable x yielding to a distribution D. s ∼ HCTnb means all possible history before T n b , and L([s, snk ]; Θ) is the likelihood of stitched sequence [s, snk ].
",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"The stitched sequence [s, snk ] can be generated via sampling SDC sequence s from previous 1st, ..., (k− 1)-th intervals and stitching s to snk .",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
The sampling process yields to the probabilistic distribution of the stitched sequences.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Given snk , we can compute its similarity between its potential predecessor sn ′
k′ in [T n′ b , T n′ e ] as
w(sn ′
k′ , s n k )",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"=
{ 0, when Tn ′
e ≥",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Tnb S(Tnb , T n′ e )",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"S(f n k , f n′ k′ ), o.w. (5)
Here, S(a, b) = exp(−‖b",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
− a‖22/σs) is a predefined similarity function with parameter σs.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"fnk is the feature of snk , which is available in some applications.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Note that the availability of feature is optional — even if the feature of sequence is unavailable, we can still define the similarity measurement purely based on time stamps.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"The normalized {w(sn′k′ , snk )} provides us with the probability that sn ′ k′ appears before snk , i.e., p(s n′ k′ |snk ) ∝",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"w(sn ′ k′ , s n k ).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Then, we can sample sn ′
k′ according to the categorical distribution, i.e., sn ′
k′ ∼ Category(w(·, snk )).
",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
We can apply such a sampling-stitching mechanism L times iteratively to the SDC sequences in both backward and forward directions and get long stitched event sequences.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Specifically, we represent a stitched event sequence as sstitch =",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"[s1, ..., s2L+1], sl ∈ {snk}, l = 1, ..., 2L+ 1, whose probability is
p(sstitch) ∝",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"∏2L
l=1 w(sl, sl+1).",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"(6)
Note that our data synthesis method naturally contains two variants.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"When the starting (the ending) point of time window is unavailable, we use the time stamp of the first (the last) event of SDC sequence instead.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"Additionally, we can
relax the constraint Tn ′
e ≤",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
Tnb in (5) and allow a SDC sequence to have an overlap with its predecessor/successor.,3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"In this case, we preserve the overlap part randomly either from itself or its predecessor/successor before applying our sampling-stitching method.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"These two variants ensure that our data synthesis method is doable in practice, which are used in the following experiments on real-world data.",3.1. Data Synthesis via Sampling-Stitching,[0],[0]
"After applying our data synthesis method, we obtain many stitched event sequences, which can be used as instances for estimating Es∼HC
Tn b",3.2. Justification,[0],[0]
"[logL([s, snk ]; Θ)].",3.2. Justification,[0],[0]
"Specifically, taking advantage of stitched sequences, we can rewrite the learning problem in (4) approximately as
minΘ− ∑
sstitch∈S p(sstitch) logL(sstitch; Θ), (7)
which is actually the minimum cross-entropy estimation.",3.2. Justification,[0],[0]
"p(sstitch) represents the “true” probability that the stitched sequence happens, which is estimated via the predefined similarity measurement and the sampling mechanism.",3.2. Justification,[0],[0]
"The likelihood L(sstitch; Θ) represents the “unnatural” probability that the stitched sequence happens, which is estimated based on the definition in (1).",3.2. Justification,[0],[0]
Our data synthesis method takes advantage of the information of time stamps and (optional) features and makes p(sstitch) suitable for practical situations.,3.2. Justification,[0],[0]
"For example, the likelihood of a sequence generally reduces with the increase of observation time window.",3.2. Justification,[0],[0]
"The proposed probability p(sstitch) yields to the same pattern — according to (6), the longer a stitched sequence is, the smaller its probability becomes.
",3.2. Justification,[0],[0]
"The set of all possible stitched sequences, i.e., the S in (7), is very large, whose cardinality is |S| =",3.2. Justification,[0],[0]
O( ∏N n=1Kn).,3.2. Justification,[0],[0]
"In practice, we cannot and do not need to enumerate all possible combinations.",3.2. Justification,[0],[0]
"An empirical setting is making the number of stitched sequences comparable to that of original SDC event sequences, i.e., generating O( ∑N n=1Kn) stitched sequences.",3.2. Justification,[0],[0]
"In the following experiments, we just apply 5(= U ) trials and generate 5 stitched sequences for each original SDC event sequence, which achieves a tradeoff between computational complexity and performance.",3.2. Justification,[0],[0]
It should be noted that our data synthesis method is only suitable for those complicated point processes whose historical events have influences on current and future ones.,3.3. Feasibility,[0],[0]
"Specifically, we analyze the feasibility of our method for several typical point processes.
",3.3. Feasibility,[0],[0]
Poisson Processes.,3.3. Feasibility,[0],[0]
Our data synthesis method cannot improve learning results if the event sequences are generated via Poisson processes.,3.3. Feasibility,[0],[0]
"For Poisson processes, the happening rate of future events is independent of historical
events.",3.3. Feasibility,[0],[0]
"In other words, the intensity function of each interval can be learned independently based on the SDC event sequences.",3.3. Feasibility,[0],[0]
"The stitched sequences do not provide us with any additional information.
",3.3. Feasibility,[0],[0]
Hawkes Processes.,3.3. Feasibility,[0],[0]
"For Hawkes processes, whose intensity function is defined as (2), our data synthesis method can enhance the robustness of learning algorithm generally.",3.3. Feasibility,[0],[0]
"In particular, consider a “long” event sequence generated via a Hawkes process in the time window",3.3. Feasibility,[0],[0]
"[Tb, Te].",3.3. Feasibility,[0],[0]
"If we divide the time window into 2 intervals, i.e., [Tb, T ] and (T, Te], the intensity function corresponding to the second interval can be written as
λc(t) = µc + ∑ ti≤T φcci(t, ti) + ∑ T<ti≤Te φcci(t, ti).",3.3. Feasibility,[0],[0]
"(8)
If the events in the first interval are unobserved, we just have a SDC event sequence, and the second term in (8) is unavailable.",3.3. Feasibility,[0],[0]
"Learning Hawkes processes directly from the SDC event sequence ignores the information of the second term, which has a negative influence on learning results.",3.3. Feasibility,[0],[0]
Our data synthesis method leverages the information from other potential predecessors and generates multiple candidate long sequences.,3.3. Feasibility,[0],[0]
"As a result, we obtain multiple intensity functions sharing the second interval and maximize the weighted sum of their log-likelihood functions (i.e., an estimated expectation of the log-likelihood of the real long sequence), as (7) does.
",3.3. Feasibility,[0],[0]
"Compared with learning from SDC event sequences directly, applying our data synthesis method can improve learning results in general, unless the term∑ ti≤T φcci(t, ti) is ignorable.",3.3. Feasibility,[0],[0]
"Specifically, we can model the impact functions {φcc′(t, s)}c,c′∈C of Hawkes processes based on basis representation:
φcc′(t, s) = ψcc′(t)︸ ︷︷ ︸",3.3. Feasibility,[0],[0]
Infectivity × g(t−,3.3. Feasibility,[0],[0]
s)︸ ︷︷ ︸,3.3. Feasibility,[0],[0]
"Triggering kernel
= ∑M
m=1 acc′mκm(t)g(t− s).
",3.3. Feasibility,[0],[0]
"(9)
Here, we decompose impact functions into two parts: 1)",3.3. Feasibility,[0],[0]
Infectivity ψcc′(t) = ∑M m=1 acc′mκm(t) represents the infectivity of event type c′ to c at time t.1 2),3.3. Feasibility,[0],[0]
Triggering kernel g(t) = exp(−βt) measures the time decay of infectivity.,3.3. Feasibility,[0],[0]
It means that the infectivity of a historical event to current one reduces exponentially with the increase of temporal distance between them.,3.3. Feasibility,[0],[0]
"When β is very large, φcc′(t, s) decays rapidly with the increase of t − s, and the events happening long ago can be ignored.",3.3. Feasibility,[0],[0]
"In such a situation, our data synthesis method is unable to improve learning results.
",3.3. Feasibility,[0],[0]
"1When M = 1 and κm(t) ≡ 1, we obtain the simplest timeinvariant Hawkes process.",3.3. Feasibility,[0],[0]
"Relaxing the shift-invariant assumption, i.e., M > 1 and κm(t) is Gaussian, we obtain a flexible time-varying Hawkes process model.",3.3. Feasibility,[0],[0]
Hawkes process is a kind of physically-interpretable model for many natural and social phenomena.,4. Implementation for Hawkes Processes,[0],[0]
The proposed model in (9) reflects many common properties of realworld event sequences.,4. Implementation for Hawkes Processes,[0],[0]
"First, the infectivity among various event types often changes smoothly in practice: in social networks, the interaction between two users changes smoothly, which is not established or blocked suddenly; in disease networks, the infectivity among diseases should change smoothly with the increase of patient’s age.",4. Implementation for Hawkes Processes,[0],[0]
Applying Gaussian basis representation guarantees the smoothness of infectivity function.,4. Implementation for Hawkes Processes,[0],[0]
"Second, the triggering kernel measures the decay of infectivity over time.",4. Implementation for Hawkes Processes,[0],[0]
"According to existing work, the decay of infectivity is exponential approximately, which has been verified in many real-world data (Zhou et al., 2013a; Kobayashi & Lambiotte, 2016; Choi et al., 2015).",4. Implementation for Hawkes Processes,[0],[0]
"For learning Hawkes processes from SDC event sequences, we combine our data synthesis method with an EM-based learning algorithm of Hawkes processes.",4. Implementation for Hawkes Processes,[0],[0]
"Applying our data synthesis method, we obtain a set of stitched event sequences S = {sn} and their appearance probabilities {pn}, where sn = {(tni , cni )",4. Implementation for Hawkes Processes,[0],[0]
In i=1|tni ∈,4. Implementation for Hawkes Processes,[0],[0]
"[Tnb , Tne ], cni ∈ C} and pn is calculated based on (5).",4. Implementation for Hawkes Processes,[0],[0]
"According to (7, 9), we can learn target Hawkes process via
min µ≥0, A≥0
− ∑|S|
n=1 pn logL(sn; Θ) + γR(A).",4. Implementation for Hawkes Processes,[0],[0]
"(10)
",4. Implementation for Hawkes Processes,[0],[0]
"Here Θ = {µ, A} represents the parameters of our model.",4. Implementation for Hawkes Processes,[0],[0]
The vector µ =,4. Implementation for Hawkes Processes,[0],[0]
[µc] and the tensor A = [acc′m] are nonnegative.,4. Implementation for Hawkes Processes,[0],[0]
"Based on (1, 9), the log-likelihood function is
logL(sn; Θ)
= ∑In
i=1",4. Implementation for Hawkes Processes,[0],[0]
log λci(ti)− ∑C c=1 ∫,4. Implementation for Hawkes Processes,[0],[0]
"Tne Tnb λc(s)ds
= ∑In
i=1",4. Implementation for Hawkes Processes,[0],[0]
"log
[ µcni + ∑",4. Implementation for Hawkes Processes,[0],[0]
j<i g(τnij) ∑ m acni cnjmκm(t n,4. Implementation for Hawkes Processes,[0],[0]
i ) ],4. Implementation for Hawkes Processes,[0],[0]
"− ∑C
c=1
( ∆nµc − ∑ m ∑In i=1",4. Implementation for Hawkes Processes,[0],[0]
∑,4. Implementation for Hawkes Processes,[0],[0]
"j≤i accnjmGij ) ,
where τnij = t n",4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
"− tnj , Gij = ∫ tni+1 tni
κm(s)g(s",4. Implementation for Hawkes Processes,[0],[0]
"− tnj )ds, and ∆n = Tne",4. Implementation for Hawkes Processes,[0],[0]
− Tnb .,4. Implementation for Hawkes Processes,[0],[0]
"R(A) represents the regularizer of parameters, whose weight is γ.",4. Implementation for Hawkes Processes,[0],[0]
"Following existing work in (Luo et al., 2015; Zhou et al., 2013a; Xu et al., 2016a), we assume the infectivity connections among different event types to be sparse and impose a `1-norm regularizer on the coefficient tensor A, i.e., R(A) = ‖A‖1 =∑ c,c′,m |acc′m|.
",4. Implementation for Hawkes Processes,[0],[0]
We can solve the problem via an EM algorithm.,4. Implementation for Hawkes Processes,[0],[0]
"Specifically, when sparse regularizer is applied, we take advantage of ADMM method, introducing auxiliary variable Z =",4. Implementation for Hawkes Processes,[0],[0]
[zcc′m] and dual variable U =,4. Implementation for Hawkes Processes,[0],[0]
"[ucc′m] for A and
rewriting the objective function in (10) as − ∑
n pn logL(sn; Θ) + 0.5ρ‖A−Z‖2F
+ ρtr(U>(A−Z))",4. Implementation for Hawkes Processes,[0],[0]
"+ γ‖Z‖1.
",4. Implementation for Hawkes Processes,[0],[0]
"Here ρ controls the weights of regularization terms, which increases with the number of EM iterations.",4. Implementation for Hawkes Processes,[0],[0]
tr(·) computes the trace of matrix.,4. Implementation for Hawkes Processes,[0],[0]
"Then, we can update {µ,A}, Z, and U alternatively.
",4. Implementation for Hawkes Processes,[0],[0]
Update µ,4. Implementation for Hawkes Processes,[0],[0]
"and A: Given the parameters in the k-th iteration, we apply Jensen’s inequality to − ∑ n logL(sn; Θ) and obtain a surrogate objective function for µ andA:
Q(µ,A; µk,Ak,Zk,Uk)
",4. Implementation for Hawkes Processes,[0],[0]
=− N∑ n=1 pn { In∑ i=1,4. Implementation for Hawkes Processes,[0],[0]
"[∑ j<i M∑ m=1 qijm log g(τnij)acni cnjmκm(t n i ) qijm
+ qi log µcni qi",4. Implementation for Hawkes Processes,[0],[0]
− C∑ c=1 M∑ m=1,4. Implementation for Hawkes Processes,[0],[0]
∑ j≤i accnjmGij ] −∆n C∑ c=1 µc } + 0.5ρ‖A−Zk,4. Implementation for Hawkes Processes,[0],[0]
"+Uk‖2F ,
where qi = µkcn i
λk cn",4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
(,4. Implementation for Hawkes Processes,[0],[0]
"tni )
and qijm = g(τnij)a k cn",4. Implementation for Hawkes Processes,[0],[0]
i cn j mκm(t n,4. Implementation for Hawkes Processes,[0],[0]
"i )
",4. Implementation for Hawkes Processes,[0],[0]
λk cn,4. Implementation for Hawkes Processes,[0],[0]
i,4. Implementation for Hawkes Processes,[0],[0]
(,4. Implementation for Hawkes Processes,[0],[0]
"tni )
, and
λkcni (t n i ) is calculated based on µ k and Ak.",4. Implementation for Hawkes Processes,[0],[0]
"Then, we can update µ andA via solving ∂Q∂µ = 0 and ∂Q ∂A = 0.",4. Implementation for Hawkes Processes,[0],[0]
"Both of these two equations have closed-form solution:
µk+1c =
∑ n pn ∑ cni =c
qi∑ n pn∆n ,
ak+1cc′m =
√ B2 − 4ρC −B
2ρ ,
(11)
where B = ρ(ukcc′m − zkcc′m) +",4. Implementation for Hawkes Processes,[0],[0]
∑,4. Implementation for Hawkes Processes,[0],[0]
n,4. Implementation for Hawkes Processes,[0],[0]
"pn ∑
cni =c, c n",4. Implementation for Hawkes Processes,[0],[0]
j,4. Implementation for Hawkes Processes,[0],[0]
"=c
′, j≤i Gij , C =",4. Implementation for Hawkes Processes,[0],[0]
"− ∑ n pn ∑
cni =c, c n j =c
′, j≤i qijm.
",4. Implementation for Hawkes Processes,[0],[0]
Update Z:,4. Implementation for Hawkes Processes,[0],[0]
"Given Ak+1 and Uk, we can update Z via solving the following optimization problem:
minZ γ‖Z‖1 + 0.5ρ‖Ak+1 −Z +Uk‖2F .
",4. Implementation for Hawkes Processes,[0],[0]
"Applying soft-thresholding method, we have
Zk+1 =",4. Implementation for Hawkes Processes,[0],[0]
"F γ ρ (Ak+1 +Uk), (12)
where Fη(x) = sign(x) min{|x|",4. Implementation for Hawkes Processes,[0],[0]
"− η, 0} is the softthresholding function.
",4. Implementation for Hawkes Processes,[0],[0]
Update U :,4. Implementation for Hawkes Processes,[0],[0]
"Given Ak+1 and Zk+1, we can further update dual variable as
Uk+1 = Uk + (Ak+1 −Zk+1).",4. Implementation for Hawkes Processes,[0],[0]
"(13)
Algorithm 1 Learning Algorithm of Hawkes Processes 1: Input: Event sequences S.",4. Implementation for Hawkes Processes,[0],[0]
"The threshold V . Prede-
fined parameters β, σκ, and γ. 2: Output: ParametersA and µ. 3: Initialize Ak and µk randomly.",4. Implementation for Hawkes Processes,[0],[0]
"Zk = Ak, Uk = 0.",4. Implementation for Hawkes Processes,[0],[0]
k,4. Implementation for Hawkes Processes,[0],[0]
"= 0, ρ = 1. 4: repeat 5: ObtainAk+1 and µk+1 via (11).",4. Implementation for Hawkes Processes,[0],[0]
6: Obtain Zk+1 via (12).,4. Implementation for Hawkes Processes,[0],[0]
7: Obtain Uk+1 via (13).,4. Implementation for Hawkes Processes,[0],[0]
"8: k = k + 1, ρ = 1.5ρ.",4. Implementation for Hawkes Processes,[0],[0]
"9: until ‖Ak −Ak−1‖F < V
10: A = Ak, µ = µk.
",4. Implementation for Hawkes Processes,[0],[0]
"In summary, Algorithm 1 shows the scheme of our learning method.",4. Implementation for Hawkes Processes,[0],[0]
Note that the algorithm can be applied to SDC event sequences directly via ignoring pn’s.,4. Implementation for Hawkes Processes,[0],[0]
"To demonstrate the usefulness of our data synthesis method, we combine it with various learning algorithms of Hawkes processes and learn different models accordingly from SDC event sequences.",5.1. Implementation Details,[0],[0]
"For time-invariant Hawkes processes, we consider two learning algorithms — our EMbased learning algorithm and the least squares (LS) algorithm in (Eichler et al., 2016).",5.1. Implementation Details,[0],[0]
"For time-varying Hawkes processes, we apply our EM-based learning algorithm.",5.1. Implementation Details,[0],[0]
"In the following experiments, we use Gaussian basis functions: κm(t) =",5.1. Implementation Details,[0],[0]
exp((t − tm)2/σκ) with center tm and bandwidth σκ.,5.1. Implementation Details,[0],[0]
"The number and the bandwidth of basis can be set according to the basis selection method proposed in (Xu et al., 2016a).",5.1. Implementation Details,[0],[0]
"Additionally, we set V = 10−4, γ = 1, and σs = 1 in our algorithm.",5.1. Implementation Details,[0],[0]
"Given SDC event sequences, we learn Hawkes processes in three ways: 1) learning directly from SDC event sequences; 2) applying the stationary bootstrap method in (Politis & Romano, 1994) to generate more synthetic SDC event sequences and learning from these sequences accordingly; 3) learning from stitched sequences generated via our data synthesis method.",5.1. Implementation Details,[0],[0]
"For real-world data, whose SDC sequences do not have predefined starting and ending time stamps, we applied the variants of our method mentioned in the end of Section 3.1.",5.1. Implementation Details,[0],[0]
The synthetic SDC event sequences are generated via the following method: 2000 complete event sequences are simulated in the time window,5.2. Synthetic Data,[0],[0]
"[0, 50] based on a 2-dimensional Hawkes process.",5.2. Synthetic Data,[0],[0]
"The base intensity {µc}2c=1 are randomly generated in the range [0.1, 0.2].",5.2. Synthetic Data,[0],[0]
"The parameter of trigger-
ing kernel, β, is set to be 0.2.",5.2. Synthetic Data,[0],[0]
"For time-invariant Hawkes processes, we set the infectivity {ψcc′(t)} to be 4 constants randomly generated in the range [0, 0.2].",5.2. Synthetic Data,[0],[0]
"For time-varying Hawkes processes, we set ψcc′(t) = 0.2 cos(2π ωcc′ 50 t), where {ωcc′} are randomly generated in the range [1, 4].",5.2. Synthetic Data,[0],[0]
"Given these complete event sequences, we select 1000 sequences as testing set while the remaining 1000 sequences as training set.",5.2. Synthetic Data,[0],[0]
"To generate SDC event sequences, we segment time window into 10 intervals, and just randomly preserve the data in one interval for each training sequences.",5.2. Synthetic Data,[0],[0]
"We test all methods in 10 trials and compare them on the relative error between real parameters Θ and their estimation results Θ̂, i.e., ‖Θ−Θ̂‖2‖Θ‖2 , and the log-likelihood of testing sequences.
",5.2. Synthetic Data,[0],[0]
Time-invariant Hawkes Processes.,5.2. Synthetic Data,[0],[0]
Fig. 2 shows the comparisons on log-likelihood and relative error for various methods.,5.2. Synthetic Data,[0],[0]
"In Fig. 2(a) we can find that compared with the learning results based on complete event sequences, the results based on SDC event sequences degrade a lot (lower log-likelihood and higher relative error) because of the loss of information.",5.2. Synthetic Data,[0],[0]
"Our data synthesis method improves the learning results consistently with the increase of training sequences, which outperforms its bootstrap-based competitor (Politis & Romano, 1994) as well.",5.2. Synthetic Data,[0],[0]
"To demonstrate the universality of our method, besides our EM-based algorithm, we apply our method to the Least Squares (LS) algo-
rithm (Eichler et al., 2016).",5.2. Synthetic Data,[0],[0]
Fig. 2(b) shows that our method also improves the learning results of the LS algorithm in the case of SDC event sequences.,5.2. Synthetic Data,[0],[0]
"Both the log-likelihood and the relative error obtained from the stitched sequences approach to the results learned from complete sequences.
",5.2. Synthetic Data,[0],[0]
Time-varying Hawkes Processes.,5.2. Synthetic Data,[0],[0]
Fig. 3 shows the comparisons on log-likelihood and relative error for various methods.,5.2. Synthetic Data,[0],[0]
"Similarly, the learning results are improved because of applying our method — higher log-likelihood and lower relative error are obtained and their standard deviation (the error bars associated with curves) is shrunk.",5.2. Synthetic Data,[0],[0]
"In this case, applying our method twice achieves better results than applying once, which verifies the usefulness of the iterative framework in our sampling-stitching algorithm.",5.2. Synthetic Data,[0],[0]
"Besides objective measurements, in Fig. 4 we visualize the infectivity functions {ψcc′(t)}.",5.2. Synthetic Data,[0],[0]
"It is easy to find that the infectivity functions learned from stitched sequences (red curves) are comparable to those learned from complete event sequences (yellow curves), which have small estimation errors of the ground truth (black curves).
",5.2. Synthetic Data,[0],[0]
"Note that our iterative framework is useful, especially for time-varying Hawkes processes, when the number of stitches is not very large.",5.2. Synthetic Data,[0],[0]
"In our experiments, we fixed the maximum number of synthetic sequences.",5.2. Synthetic Data,[0],[0]
"As a result, Figs. 2 and 3 show that the likelihoods first increase (i.e., stitching once or twice) and then decrease (i.e., stitching more than three times) while the relative errors have opponent changes w.r.t.",5.2. Synthetic Data,[0],[0]
the number of stitches.,5.2. Synthetic Data,[0],[0]
These phenomena imply that too many stitches introduce too much unreliable interdependency among events.,5.2. Synthetic Data,[0],[0]
"Therefore, we fix
the number of stitches to 2 in the following experiments.",5.2. Synthetic Data,[0],[0]
"Besides synthetic data, we also test our method on realworld data, including the LinkedIn data collected by ourselves and the MIMIC III data set (Johnson et al., 2016).
",5.3. Real-World Data,[0],[0]
LinkedIn Data.,5.3. Real-World Data,[0],[0]
"The LinkedIn data we collected online contain job hopping records of 3, 000 LinkedIn users in 82 IT companies.",5.3. Real-World Data,[0],[0]
"For each user, her/his check-in time stamps corresponding to different companies are recorded as an event sequence, and her/his profile (e.g., education background, skill list, etc.) is treated as the feature associated with the sequence.",5.3. Real-World Data,[0],[0]
"For each person, the attractiveness of a company is always time-varying.",5.3. Real-World Data,[0],[0]
"For example, a young man may be willing to join in startup companies and increase his income via jumping between different companies.",5.3. Real-World Data,[0],[0]
"With the increase of age, he would more like to stay in the same company and achieve internal promotions.",5.3. Real-World Data,[0],[0]
"In other words, the infectivity network among different companies should be dynamical w.r.t.",5.3. Real-World Data,[0],[0]
the age of employees.,5.3. Real-World Data,[0],[0]
"Unfortunately, most of the records in LinkedIn are short and doubly-censored — only the job hopping events in recent years are recorded.",5.3. Real-World Data,[0],[0]
"How to construct the dynamical infectivity network among different companies from the SDC event sequences is still an open problem.
",5.3. Real-World Data,[0],[0]
"Applying our data synthesis method, we can stitch different users’ job hopping sequences based on their ages (time stamps) and their profile (feature) and learn the dynamical network of company over time.",5.3. Real-World Data,[0],[0]
"In particular, we select 100 users with relatively complete job hopping history (i.e., the range of their working experience is over 25 years) as testing set.",5.3. Real-World Data,[0],[0]
"The remaining 2, 900 users are randomly selected as training set.",5.3. Real-World Data,[0],[0]
The log-likelihood of testing set in 10 trials is shown in Fig. 5(a).,5.3. Real-World Data,[0],[0]
"We can find that the log-likelihood obtained from stitched sequences is higher than that obtained from original SDC sequences or that obtained from the sequences generated via the bootstrap method (Politis & Romano, 1994), and its standard deviation is bounded
stably.",5.3. Real-World Data,[0],[0]
Fig. 6(a) visualizes the adjacent matrix of infectivity network.,5.3. Real-World Data,[0],[0]
"The properties of the network verifies the rationality of our results: 1) the diagonal elements of the adjacent matrix are larger than other elements in general, which reflects the fact that most employees would like to stay in the same company and achieve a series of internal promotions; 2) with the increase of age, the infectivity network becomes sparse, which reflects the fact that users are more likely to try different companies in the early stages of their careers.
",5.3. Real-World Data,[0],[0]
MIMIC III Data.,5.3. Real-World Data,[0],[0]
"The MIMIC III data contain admission records of over 40, 000 patients in the Beth Israel Deaconess Medical Center between 2001 and 2012.",5.3. Real-World Data,[0],[0]
"For each patient, her/his admission time stamps and diseases (represented via the ICD-9 codes (Deyo et al., 1992)) are recorded as an event sequence, and her/his profile (including gender, race and chronic history) is represented as binary feature of the sequence.",5.3. Real-World Data,[0],[0]
"As aforementioned, some work (Choi et al., 2015) has been done to extract timeinvariant disease network from admission records.",5.3. Real-World Data,[0],[0]
"However, the real disease network should be time-varying w.r.t.",5.3. Real-World Data,[0],[0]
the age of patient.,5.3. Real-World Data,[0],[0]
"Similar to the LinkedIn data, here we only obtain SDC event sequences — the ranges of most admission records are just 1 or 2 years.
",5.3. Real-World Data,[0],[0]
"Applying our data synthesis method, we can leverage the information from different patients and stitch their sequences based on their ages and their profile.",5.3. Real-World Data,[0],[0]
"Focusing on 600 common diseases in 12 categories, we select 15, 000 patients’ admission records randomly as training set and 1, 000 patients with relatively complete records as testing set.",5.3. Real-World Data,[0],[0]
Fig.,5.3. Real-World Data,[0],[0]
5(b) shows that applying our data synthesis method indeed helps to improve log-likelihood of testing data.,5.3. Real-World Data,[0],[0]
"Compared with our bootstrap-based competitor, our data synthesis method gets more obvious improvements.
",5.3. Real-World Data,[0],[0]
"Furthermore, we visualize the adjacent matrix of dynamical network of disease categories in Fig. 6(b).",5.3. Real-World Data,[0],[0]
"We can find that: 1) with the increase of age the disease network becomes dense, which reflects the fact that the complications of diseases are more and more common when people become old; 2) the networks show that neoplasms and the diseases of circulatory, respiratory, and digestive systems have strong self-triggering patterns because the treatments of these diseases often include several phases and require patients to make admission multiple times; 3) for kids and teenagers, their disease networks (i.e., “Age 0” and “Age 10” networks) are very sparse, and their common diseases mainly include neoplasms and the diseases of circulatory, respiratory, and digestive systems; 4) for middle-aged people, the reasons for their admissions are diverse and complicated so that their disease networks are dense and include many mutually-triggering patterns; 5) for longevity people, their disease networks (i.e., “Age 80” and “Age 90” networks) are relatively sparser than those of middle-aged people, because their admissions are generally caused by elderly chronic diseases.
",5.3. Real-World Data,[0],[0]
"Additionally, we visualize the dynamical networks of the diseases of circulatory systems in Fig. 7, and find some interesting triggering patterns.",5.3. Real-World Data,[0],[0]
"For example, for kids (“Age 0” network), the typical circulatory diseases are “diseases of mitral and aortic valves” (ICD-9 396) and “cardiac dysrhythmias” (ICD-9 427), which are common for premature babies and the kids having congenital heart disease.",5.3. Real-World Data,[0],[0]
"For the old (“Age 80” network), the network becomes dense.",5.3. Real-World Data,[0],[0]
"We can find that 1) as a main cause of death, “heart failure” (ICD-9 428) is triggered via multiple other diseases, especially “secondary hypertension” (ICD-9 405); 2) “sec-
ondary hypertension” is also likely to cause “other and illdefined cerebrovascular disease” (ICD-9 437); 3) “Hemorrhoids” (ICD-9 455), as a common disease with strong self-triggering pattern, will cause frequent admissions of patients.",5.3. Real-World Data,[0],[0]
"In summary, the analysis above verifies the rationality of our result — the dynamical disease networks we learned indeed reflect the properties of human’s health trajectory.",5.3. Real-World Data,[0],[0]
The list of ICD-9 codes and the complete enlarged network over age are shown in the supplementary file.,5.3. Real-World Data,[0],[0]
"In this paper, we propose a novel data synthesis method to learn Hawkes processes from SDC event sequences.",6. Conclusion,[0],[0]
"With the help of temporal information and optional features, we measure the similarities among different SDC event sequences and estimate the distribution of potential long event sequences.",6. Conclusion,[0],[0]
"Applying a sampling-stitching mechanism, we successfully synthesize a large amount of long event sequences and learn point processes robustly.",6. Conclusion,[0],[0]
We test our method for both time-invariant and time-varying Hawkes processes.,6. Conclusion,[0],[0]
Experiments show that our data synthesis method improves the robustness of learning algorithms for various models.,6. Conclusion,[0],[0]
"In the future, we plan to provide more theoretical and quantitative analysis to our data synthesis method and apply it to more applications.",6. Conclusion,[0],[0]
"This work is supported in part via NSF IIS-1639792, DMS-1317424, NIH R01 GM108341, NSFC 61628203, U1609220 and the Key Program of Shanghai Science and Technology Commission 15JC1401700.",Acknowledgements,[0],[0]
Many real-world applications require robust algorithms to learn point processes based on a type of incomplete data — the so-called short doublycensored (SDC) event sequences.,abstractText,[0],[0]
We study this critical problem of quantitative asynchronous event sequence analysis under the framework of Hawkes processes by leveraging the idea of data synthesis.,abstractText,[0],[0]
"Given SDC event sequences observed in a variety of time intervals, we propose a sampling-stitching data synthesis method, sampling predecessors and successors for each SDC event sequence from potential candidates and stitching them together to synthesize long training sequences.",abstractText,[0],[0]
The rationality and the feasibility of our method are discussed in terms of arguments based on likelihood.,abstractText,[0],[0]
Experiments on both synthetic and real-world data demonstrate that the proposed data synthesis method improves learning results indeed for both timeinvariant and time-varying Hawkes processes.,abstractText,[0],[0]
Learning Hawkes Processes from Short Doubly-Censored Event Sequences,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 595–605 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"For most Natural Language Processing (NLP) tasks, obtaining sufficient annotated text for training accurate models is a critical bottleneck.",1 Introduction,[0],[0]
"Thus active learning has been applied to NLP tasks to minimise the expense of annotating data (Thompson et al., 1999; Tong and Koller, 2001; Settles and Craven, 2008).",1 Introduction,[0],[0]
"Active learning aims to reduce cost by identifying a subset of unlabelled data for annotation, which is selected to maximise the accuracy of a supervised model trained on the data (Settles, 2010).",1 Introduction,[0],[0]
"There have been many successful applications to NLP, e.g., Tomanek et al. (2007) used an active learning algorithm for CoNLL corpus to get an F1 score 84% with a reduction of annotation cost of about 48%.",1 Introduction,[0],[0]
"In prior work most active learning algorithms are designed for English based on
heuristics, such as using uncertainty or informativeness.",1 Introduction,[0],[0]
"There has been comparatively little work done about how to learn the active learning strategy itself.
",1 Introduction,[0],[0]
"It is no doubt that active learning is extremely important for other languages, particularly lowresource languages, where annotation is typically difficult to obtain, and annotation budgets more modest (Garrette and Baldridge, 2013).",1 Introduction,[0],[0]
"Such settings are a natural application for active learning, however there is little work to this end.",1 Introduction,[0],[0]
"A potential reason is that most active learning algorithms require a substantial ‘seed set’ of data for learning a basic classifier, which can then be used for active data selection.",1 Introduction,[0],[0]
"However, given the dearth of data in the low-resource setting, this assumption can make standard approaches infeasible.
",1 Introduction,[0],[0]
"In this paper,1 we propose PAL, short for Policy based Active Learning, a novel approach for learning a dynamic active learning strategy from data.",1 Introduction,[0],[0]
"This allows for the strategy to be applied in other data settings, such as cross-lingual applications.",1 Introduction,[0],[0]
"Our algorithm does not use a fixed heuristic, but instead learns how to actively select data, formalised as a reinforcement learning (RL) problem.",1 Introduction,[0],[0]
"An intelligent agent must decide whether or not to select data for annotation in a streaming setting, where the decision policy is learned using a deep Q-network (Mnih et al., 2015).",1 Introduction,[0],[0]
"The policy is informed by observations including sentences’ content information, the supervised model’s classifications and its confidence.",1 Introduction,[0],[0]
"Accordingly, a rich and dynamic policy can be learned for annotating new data based on the past sequence of annotation decisions.
",1 Introduction,[0],[0]
"Furthermore, in order to reduce the dependence on the data in the target language, which may be low resource, we first learn the policy of active
1Source code available at https://github.com/ mengf1/PAL
595
learning on another language and then transfer it to the target language.",1 Introduction,[0],[0]
"It is easy to learn a policy on a high resource language, where there is plentiful data, such as English.",1 Introduction,[0],[0]
"We use cross-lingual word embeddings to learn compatible data representations for both languages, such that the learned policy can be easily ported into the other language.
",1 Introduction,[0],[0]
Our work is different for prior work in active learning for NLP.,1 Introduction,[0],[0]
Most previous active learning algorithms developed for NER tasks is based on one language and then applied to the language itself.,1 Introduction,[0],[0]
"Another main difference is that many active learning algorithms use a fixed data selection heuristic, such as uncertainty sampling (Settles and Craven, 2008; Stratos and Collins, 2015; Zhang et al., 2016).",1 Introduction,[0],[0]
"However, in our algorithm, we implicitly use uncertainty information as one kind of observations to the RL agent.
",1 Introduction,[0],[0]
The remainder of this paper is organised as follows.,1 Introduction,[0],[0]
"In Section 2, we briefly review some related work.",1 Introduction,[0],[0]
"In Section 3, we present active learning algorithms, which cross multiple languages.",1 Introduction,[0],[0]
The experimental results are presented in Section 4.,1 Introduction,[0],[0]
We conclude our work in Section 5.,1 Introduction,[0],[0]
"As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classifier.",2 Related work,[0],[0]
"Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling (Lewis and Gale, 1994; Tong and Koller, 2001), which selects the data about which the current classifier is the most uncertain; 2) query by committee (Seung et al., 1992), which selects the data about which the “committee” disagree most; and 3) expected error reduction (Roy and McCallum, 2001), which selects the data that can contribute the largest model loss reduction for the current classifier once labelled.",2 Related work,[0],[0]
"Applications of active learning to NLP include text classification (McCallumzy and Nigamy, 1998; Tong and Koller, 2001), relation classification (Qian et al., 2014), and structured prediction (Shen et al., 2004; Settles and Craven, 2008; Stratos and Collins, 2015; Fang and Cohn, 2017).",2 Related work,[0],[0]
Qian et al. used uncertainty sampling to jointly perform on English and Chinese.,2 Related work,[0],[0]
"Stratos and Collins and Zhang et al. deployed uncertainty-based AL algorithms for languages with the minimal supervision.
",2 Related work,[0],[0]
Deep reinforcement learning (DRL) is a general-purpose framework for decision making based on representation learning.,2 Related work,[0],[0]
"Recently, there are some notable examples include deep Qlearning (Mnih et al., 2015), deep visuomotor policies (Levine et al., 2016), attention with recurrent networks (Ba et al., 2015), and model predictive control with embeddings (Watter et al., 2015).",2 Related work,[0],[0]
"Other important works include massively parallel frameworks (Nair et al., 2015), dueling architecture (Wang et al., 2016) and expert move prediction in the game of Go (Maddison et al., 2015), which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search (Silver et al., 2016).",2 Related work,[0],[0]
DRL has been also studied in NLP tasks.,2 Related work,[0],[0]
"For example, recently, DRL has been studied for information extraction problem (Narasimhan et al., 2016).",2 Related work,[0],[0]
They designed a framework that can decide to acquire external evidence and the framework is under the reinforcement learning method.,2 Related work,[0],[0]
"However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings.
",2 Related work,[0],[0]
"Recent deep learning work has also looked at transfer learning (Bengio, 2012).",2 Related work,[0],[0]
"More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments (Parisotto et al., 2016; Rusu et al., 2016), using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.",2 Related work,[0],[0]
"We now show how active learning can be formalised as as a decision process, and then show how this allows for the active learning selection policy to be learned from data using deep reinforcement learning.",3 Methodology,[0],[0]
Later we introduce a method for transferring the policy between languages.,3 Methodology,[0],[0]
"Active learning is a simple technique for labelling data, which involves first selecting some instances from an unlabelled dataset, which are then annotated by a human oracle, which is then repeated many times until a termination criterion is satisfied, e.g., the annotation budget is exhausted.",3.1 Active learning as a decision process,[0],[0]
"Most often the selection function is based on the pre-
dictions of a trained model, which has been fit to the labelled dataset at each stage in the algorithm, where datapoints are selected based on the model’s predictive uncertainty (Lewis and Gale, 1994), or divergence in predictions over an ensemble (Seung et al., 1992).",3.1 Active learning as a decision process,[0],[0]
"The key idea of these methods is to find the instances on which the model is most likely to make errors, such that after their labelling and inclusion in the training set, the model becomes more robust to these types of errors on unseen data.
",3.1 Active learning as a decision process,[0],[0]
"The steps in active learning can be viewed as a decision process, a means of formalising the active learning algorithm as a sequence of decisions, where the stages of active learning correspond to the state of the system.",3.1 Active learning as a decision process,[0],[0]
"Accordingly, the state corresponds to the selected data for labelling and their labels, and each step in the active learning algorithm corresponds to a selection action, wherein the heuristic selects the next items from a pool.",3.1 Active learning as a decision process,[0],[0]
"This process terminates when the budget is exhausted.
",3.1 Active learning as a decision process,[0],[0]
"Effectively the active learning heuristic is operating as a decision policy, a form of function taking as input the current state — comprising the labelled data, from which a model is trained — and a candidate unlabelled data point — e.g., the model uncertainty.",3.1 Active learning as a decision process,[0],[0]
"This raises the opportunity to consider general policy functions, based on the state and data point inputs, and resulting in a labelling decision, and, accordingly a mechanism for learning such functions from data.",3.1 Active learning as a decision process,[0],[0]
"We now elaborate on the components of this process, namely the formulation of the decision process, architecture of the policy function, and means of learning the decision policy automatically from data.",3.1 Active learning as a decision process,[0],[0]
"For simplicity, we make a streaming assumption, whereby unlabelled data (sentences) arrive in a stream (Lewis and Gale, 1994).2 As each instance arrives, an agent must decide the action to take, namely whether or not the instance should be manually annotated.",3.2 Stream-based learning,[0],[0]
"This process is illustrated in Figure 1, which illustrates the space of decision sequences for a small corpus.",3.2 Stream-based learning,[0],[0]
"As part of this process, a separate model, pφ, is trained on the labelled data, and updated accordingly as the labelled dataset is expanded as new annotations ar-
2This is different to pool-based active learning, where one of several options is chosen for annotation.",3.2 Stream-based learning,[0],[0]
"Our setup permits simpler learning, while remaining sufficiently general.
rive.",3.2 Stream-based learning,[0],[0]
"This model is central to the policy for choosing the labelling actions at each stage, and for determining the reward for a sequence of actions.
",3.2 Stream-based learning,[0],[0]
"This is a form of Markov Decision Process (MDP), which allows the learning of a policy that can dynamically select instances that are most informative.",3.2 Stream-based learning,[0],[0]
"As illustrated in Figure 1 at each time, the agent observes the current state si which includes the sentence xi, and the learned model φ.",3.2 Stream-based learning,[0],[0]
"The agent selects a binary action ai, denoting whether to label xi, according to the policy π.",3.2 Stream-based learning,[0],[0]
"For ai = 1, the corresponding sentence is labelled and added to the labelled data, and the model pφ updated to include this new training point.",3.2 Stream-based learning,[0],[0]
"The process then repeats, terminating when either the dataset is exhausted or a fixed annotation budget is reached.",3.2 Stream-based learning,[0],[0]
"After termination a reward is computed based on the accuracy of the final model, φ.",3.2 Stream-based learning,[0],[0]
"We represent the MDP framework as a tuple 〈S,A, Pr(si+1|si, a), R〉, where S = {s} is the space of all possible states, A = {0, 1} is the set of actions, R(s, a) is the reward function, and Pr(si+1|si, a) is the transition function.",3.2 Stream-based learning,[0],[0]
The state at time i comprises the candidate instance being considered for annotation and the labelled dataset constructed in steps 1 . . .,3.2.1 State,[0],[0]
"i. We represent the state using a continuous vector, using the concatenation of the vector representation of xi, and outputs of the model pφ trained over the labelled data.",3.2.1 State,[0],[0]
"These outputs use both the predictive marginal distributions of the model on the instance, and a representation of the model’s confidence.",3.2.1 State,[0],[0]
"We now elaborate on each component.
",3.2.1 State,[0],[0]
"Content representation A key input to the agent is the content of the sentence, xi, which we encode using a convolutional neural network to arrive at a fixed sized vector representation, following Kim (2014).",3.2.1 State,[0],[0]
"This involves embedding each of the n words in the sentence to produce a matrix Xi = {xi,1, xi,2, · · · , xi,n}, after which a series of wide convolutional filters is applied, using multiple filters with different gram sizes.",3.2.1 State,[0],[0]
Each filter uses a linear transformation with a rectified linear unit activation function.,3.2.1 State,[0],[0]
"Finally the filter outputs are merged using a max-pooling operation to yield a hidden state hc, which is used to represent the sentence.
",3.2.1 State,[0],[0]
"Representation of marginals The prediction outputs of the training model, pφ(y|xi), are central to all active learning heuristics, and accordingly, we include this in our approach.",3.2.1 State,[0],[0]
"In order to generalise existing techniques, we elect to use the predictive marginals directly, rather than only using statistics thereof, e.g., entropy.",3.2.1 State,[0],[0]
"This generality allows for different and more nuanced concepts to be learned, including patterns of probabilities that span several adjacent positions in the sentence (e.g., the uncertainty about the boundary of a named entity).
",3.2.1 State,[0],[0]
"We use another convolutional neural network to process the predictive marginals, as shown in Figure 2.",3.2.1 State,[0],[0]
"The convolutional layer contains j filters with ReLU activation, based on a window of width 3 and height equal to the number of classes, and with a stride of one token.",3.2.1 State,[0],[0]
"We use a wide convolution, by padding the input matrix to either size with vectors of zeros.",3.2.1 State,[0],[0]
"These j feature maps are then subsampled with mean pooling, such that the network is easily able to capture the average uncertainty in each window.",3.2.1 State,[0],[0]
"The final hidden layer he is used to represent the predictive marginals.
",3.2.1 State,[0],[0]
Confidence of sequential prediction The last component is a score C which indicates the confidence of the model prediction.,3.2.1 State,[0],[0]
"This is defined based on the most probable label sequence under the model, e.g., using Viterbi algorithm with a CRF, and the probability of this sequence is used to represent the confidence, C = n √
maxy pφ(y|xi), where n = |xi| is the length of the sentence.",3.2.1 State,[0],[0]
"We now turn to the action, which denotes whether the human oracle must annotate the current sentence.",3.2.2 Action,[0],[0]
"The agent selects either to annotate xi, in which case ai = 1, or not, with ai = 0, after which the agent proceeds to consider the next instance, xi+1.",3.2.2 Action,[0],[0]
"When action ai = 1 is chosen, an oracle is requested to annotate the sentence, and the newly annotated sentence is added to the training data, and φ updated accordingly.",3.2.2 Action,[0],[0]
"A special ‘terminate’ option applies when no further data remains or the annotation budget is exhausted, which concludes the active learning run (referred to as an ‘episode’ or ‘game’ herein).",3.2.2 Action,[0],[0]
"The training signal for learning the policy takes the form of a scalar ‘reward’, which provides feedback on the quality of the actions made by the agent.",3.2.3 Reward,[0],[0]
"The most obvious reward is to wait for a game to conclude, then measure the held-out performance of the model, which has been trained on the labelled data.",3.2.3 Reward,[0],[0]
"However, this reward is delayed, and is difficult to related to individual actions after a long game.",3.2.3 Reward,[0],[0]
"To compensate for this,
we use reward shaping, whereby small intermediate rewards are assigned which speeds up the learning process (Ng, 2003; Lample and Chaplot, 2016).",3.2.3 Reward,[0],[0]
"At each step, the intermediate reward is defined as the change in held-out performance, i.e., R(si−1, a) = Acc(φi)",3.2.3 Reward,[0],[0]
"− Acc(φi−1), where Acc denotes predictive accuracy (here F1 score), and φi is the trained model after action a has take place, which may include an additional training instance.",3.2.3 Reward,[0],[0]
"Accordingly, when considering the aggregate reward over a game, the intermediate terms cancel, such that the total reward measures the performance improvement over the whole game.",3.2.3 Reward,[0],[0]
"Note that the value of R(s, a) can be positive or negative, indicating a beneficial or detrimental effect on the performance.",3.2.3 Reward,[0],[0]
"There is a fixed budget B for the total number of instances annotated, which corresponds to the terminal state in the MDP.",3.2.4 Budget,[0],[0]
It is a predefined number and chosen according to time and cost constraints.,3.2.4 Budget,[0],[0]
"A game is finished when the data is exhausted or the budget reached, and with the final result being the dataset thus created, upon which the final model is trained.",3.2.4 Budget,[0],[0]
The remaining question is how the above components can be used to learn a good policy.,3.2.5 Reinforcement learning,[0],[0]
"Different policies make different data selections, and thus result in models with different performance.",3.2.5 Reinforcement learning,[0],[0]
"We adopt a reinforcement learning (RL) approach to learn a policy resulting a highly accurate model.
",3.2.5 Reinforcement learning,[0],[0]
"Having represented the problem as a MDP, episode as a sequence of transitions (si, a, r, si+1).",3.2.5 Reinforcement learning,[0],[0]
"One episode of active learning produces a finite sequence of states, actions and rewards.",3.2.5 Reinforcement learning,[0],[0]
"We use a deep Q-learning approach (Mnih et al., 2015), which formalises the policy using function Qπ(s, a)→ Rwhich determines the utility of taking a from state s according to a policy π.",3.2.5 Reinforcement learning,[0],[0]
"In Qlearning, the agent iteratively updates Q(s, a) using rewards obtained from each episode, with updates based on the recursive Bellman equation for the optimal Q:
Qπ(s, a) =",3.2.5 Reinforcement learning,[0],[0]
"E[Ri|si = s, ai = a, π].",3.2.5 Reinforcement learning,[0],[0]
"(1)
Here, Ri = ∑T t=i γ t−irt is the discounted future reward and γ ∈",3.2.5 Reinforcement learning,[0],[0]
"[0, 1] is a factor discounting the value of future rewards and the expectation is
Algorithm 1 Learn an active learning policy Input: data D, budget B Output: π
1: for episode = 1, 2, . . .",3.2.5 Reinforcement learning,[0],[0]
", N do 2:",3.2.5 Reinforcement learning,[0],[0]
Dl ← ∅ and shuffle D 3: φ←,3.2.5 Reinforcement learning,[0],[0]
"Random 4: for i ∈ {0, 1, 2, . . .",3.2.5 Reinforcement learning,[0],[0]
", |D|} do 5: Construct the state si using xi 6:",3.2.5 Reinforcement learning,[0],[0]
"The agent makes a decision according to ai = arg maxQπ(si, a) 7: if ai = 1 then 8: Obtain the annotation yi 9:",3.2.5 Reinforcement learning,[0],[0]
"Dl ← Dl + (xi,yi) 10: Update model φ based on Dl 11: end if 12: Receive a reward ri using held-out set 13: if |Dl| = B then 14:",3.2.5 Reinforcement learning,[0],[0]
"Store (si, ai, ri,Terminate) inM 15:",3.2.5 Reinforcement learning,[0],[0]
"Break 16: end if 17: Construct the new state si+1 18: Store transition (si, ai, ri, si+1) inM 19: Sample random minibatch of transitions
{(sj , aj , rj , sj+1)} from M, and perform gradient descent step on L(θ)
20: Update policy π with θ 21: end for 22: end for 23: return the latest policy π
taken over all transitions involving state s and action a.
Following Deep Q-learning (Mnih et al., 2015), we make use of a deep neural network to compute the expected Q-value, in order to update the parameters.",3.2.5 Reinforcement learning,[0],[0]
"We implement the Q-function using a single hidden layer neural network, taking as input the state representation (hc,he, C) (defined in §3.2.1), and outputting two scalar values corresponding to the values Q(s, a) for a ∈ {0, 1}.",3.2.5 Reinforcement learning,[0],[0]
"This network uses a rectified linear unit (ReLU) activation function in its hidden layer.
",3.2.5 Reinforcement learning,[0],[0]
"The parameters in the DQN are learnt using stochastic gradient descent, based on a regression objective to match the Q-values predicted by the DQN and the expected Q-values from the Bellman equation, ri + γmaxaQ(si+1, a; θ).",3.2.5 Reinforcement learning,[0],[0]
"Following (Mnih et al., 2015), we use an experience replay memory M to store each transition (s, a, r, s′) as it is used in an episode, after which
Algorithm 2 Active learning by policy transfer Input: unlabelled data D, budget B, policy π Output: Dl
1: Dl ← ∅ 2: φ←",3.2.5 Reinforcement learning,[0],[0]
"Random 3: for |Dl| 6= B and D not empty do 4: Randomly sample xi from the data pool D and construct the state si 5: The agent chooses an action ai according to ai = arg maxQπ(si, a) 6: if ai = 1 then 7: Obtain the annotation yi 8:",3.2.5 Reinforcement learning,[0],[0]
"Dl ← Dl + (xi,yi) 9: Update model φ based on Dl
10: end if 11: D ← D\xi 12: Receive a reward ri using held-out set 13:",3.2.5 Reinforcement learning,[0],[0]
Update policy π 14: end for 15: return,3.2.5 Reinforcement learning,[0],[0]
"Dl
we sample a mini-batch of transitions from the memory and then minimize the loss function:
L(θ) = Es,a,r,s′",3.2.5 Reinforcement learning,[0],[0]
"[( yi(r, s′)−Q(s, a; θ) )2] , (2)
where yi(r, s′) =",3.2.5 Reinforcement learning,[0],[0]
r + γmaxa′,3.2.5 Reinforcement learning,[0],[0]
"Q(s′, a′; θi−1) is the target Q-value, based on the current parameters θi−1, and the expectation is over the minibatch.",3.2.5 Reinforcement learning,[0],[0]
"Learning updates are made every training step, based on stochastic gradient descent to minimise Eq. 2 w.r.t.",3.2.5 Reinforcement learning,[0],[0]
"parameters θ.
",3.2.5 Reinforcement learning,[0],[0]
The algorithm for learning is summarised in Algorithm 1.,3.2.5 Reinforcement learning,[0],[0]
"We train the policy by running multiple active learning episodes over the training data, where each episode is a simulated active learning run.",3.2.5 Reinforcement learning,[0],[0]
"For each episode, we shuffle the data, and hide the known labels, which are revealed as requested during the run.",3.2.5 Reinforcement learning,[0],[0]
"A disjoint held-out set is used to compute the reward, i.e., model accuracy, which is fixed over the episodes.",3.2.5 Reinforcement learning,[0],[0]
"Between each episode the model is reset to its initialisation condition, with the main changes being the different (random) data ordering and the evolving policy function.",3.2.5 Reinforcement learning,[0],[0]
We now turn to the question of how the learned policy can be applied to another dataset.,3.3 Cross-lingual policy transfer,[0],[0]
"Given the extensive use of the training dataset, the policy application only makes sense when employed in a
Algorithm 3 Active learning by policy and model transfer, for ‘cold-start’ scenario Input: unlabelled data D, budget B, policy π,
model φ Output: Dl
1: Dl ← ∅ 2: for |Dl| 6= B and D not empty do 3: Randomly sample xi from the data pool D and construct the state si 4: The agent chooses an action ai according to ai = arg maxQπ(si, a) 5: if ai = 1 then 6:",3.3 Cross-lingual policy transfer,[0],[0]
"Dl ← Dl + (xi,−) 7: end if 8: D ← D\xi 9: end for
10:",3.3 Cross-lingual policy transfer,[0],[0]
"Obtain all the annotations for Dl 11: return Dl
different data setting, e.g., where the domain, task or language is different.",3.3 Cross-lingual policy transfer,[0],[0]
"For this paper, we consider a cross-lingual application of the same task (NER), where we train a policy on a source language (e.g., English), and then transfer the learned policy to a different target language.",3.3 Cross-lingual policy transfer,[0],[0]
"Cross-lingual word embeddings provide a common shared representation to facilitate application of the policy to other languages.
",3.3 Cross-lingual policy transfer,[0],[0]
We illustrate the policy transfer algorithm in Algorithm 2.,3.3 Cross-lingual policy transfer,[0],[0]
"This algorithm is broadly similar to Algorithm 1, but has two key differences.",3.3 Cross-lingual policy transfer,[0],[0]
"Firstly, Algorithm 2 makes only one pass over the data, rather than several passes, as befits an application to a low-resource language where oracle labelling is costly.",3.3 Cross-lingual policy transfer,[0],[0]
"Secondly, the algorithm also assumes an initial policy, π, which is fine tuned during the episode based on held-out performance such that the policy can adapt to the test scenario.3",3.3 Cross-lingual policy transfer,[0],[0]
"The above transfer algorithm has some limitations, which may not be realistic for low-resource settings: the requirement for held-out evaluation data and the embedding of the oracle annotator inside the learning loop.",3.4 Cold-start transfer,[0],[0]
"The former implies more supervision than is ideal in a low-resource setting,
3Moreover, the algorithm can be extended to a traditional batch setting by evaluating a batch of data instances and selectinag the best k instances for labelling under the policy.",3.4 Cold-start transfer,[0],[0]
"This could be applied in either the transfer step (Algorithm 2) or initial policy training (Algorithm 1), or both.
while the latter places limitations on the communication with annotator as well as a necessity for real-time processing, both which are unlikely in a field linguistics setting.
",3.4 Cold-start transfer,[0],[0]
"For this data and- communication-impoverished setting, denoted as cold-start, we allow only one chance to request labels for the target data, and, having no held-out data, do not allow policy updates.",3.4 Cold-start transfer,[0],[0]
"The agent needs to select a batch of unlabelled target instances for annotations, but cannot use these resulting annotations or any other feedback to refine the selection.",3.4 Cold-start transfer,[0],[0]
"In this, more difficult cold-start setting, we bootstrap the process with an initial model, such that the agent can make informative decisions in the absence of feedback.
",3.4 Cold-start transfer,[0],[0]
The procedure is outlined in Algorithm 3.,3.4 Cold-start transfer,[0],[0]
"Using the cross-lingual word embeddings, we transfer both a policy and a model into the target language.",3.4 Cold-start transfer,[0],[0]
"The model, φ, is trained on one source language, and the policy is learned on a different source language.",3.4 Cold-start transfer,[0],[0]
"Policy learning uses Alg 1, with the small change that in step 3 the model is initialised using φ.",3.4 Cold-start transfer,[0],[0]
"Consequently the learned policy can exploit the knowledge from cross-lingual initialisation, such that it can figure out which aspects that need to be corrected using target annotated data.",3.4 Cold-start transfer,[0],[0]
"Overall this allows for estimates and confidence values to be produced by the model, thus providing the agent with sufficient information for data selection.",3.4 Cold-start transfer,[0],[0]
"We conduct experiments to validate the proposed active learning method in a cross-lingual setting, whereby an active learning policy trained on a source language is transferred to a target language.",4 Experiments,[0],[0]
"We allow repeated active learning simulations on the source language, where annotated corpora are plentiful, to learn a policy, while for target languages we only permit a single episode, to mimic a language without existing resources.
",4 Experiments,[0],[0]
"We use NER corpora from CoNLL2002/2003 shared tasks,4 which comprise NER annotated text in English (en), German (de), Spanish (es), and Dutch (nl), each annotated using the IOB1 labelling scheme, which we convert to the IO labeling scheme.",4 Experiments,[0],[0]
"We use the existing corpus partions, with train used for policy training, testb used
4 http://www.cnts.ua.ac.be/conll2002/ ner/, http://www.cnts.ua.ac.be/conll2003/ ner/
as held-out for computing rewards, and final results are reported on testa.
",4 Experiments,[0],[0]
"We consider three experimental conditions, as illustrated in Table 1:
bilingual where English is the source (used for policy learning) and we vary the target language;
multilingual where several source languages are the used in joint learning of the policy, and a separate language is used as target; and
cold-start where a pretrained English NER tagger is used to initialise policy learning on a source language, and in cold-start application to a separate target language.
",4 Experiments,[0],[0]
Configuration We now outline the parameter settings for the experimental runs.,4 Experiments,[0],[0]
"For learning an active learning policy, we run N = 10, 000 episodes with budget B = 200 sentences using Alg. 1.",4 Experiments,[0],[0]
"Content representations use three convolutional filters of size 3, 4 and 5, using 128 filters for each size, while for predictive marginals, the convolutional filters are of width 3, using 20 filters.",4 Experiments,[0],[0]
The size of the last hidden layer is 256.,4 Experiments,[0],[0]
The discount factor is set to γ = 0.99.,4 Experiments,[0],[0]
We used the ADAM algorithm with mini-batches of size 32 for training the neural network.,4 Experiments,[0],[0]
"To report performance, we apply the learned policy to the target training set (using Alg. 2 or 3, again with budget 200),5 after which we use the final trained model for which we report F1 score.
",4 Experiments,[0],[0]
"For word embeddings, we use off the shelf CCA trained multilingual embeddings (Ammar et al.,
5Although it is possible the policy may learn not to use the full budget, this does not occur in practise.
",4 Experiments,[0],[0]
"2016),6 using a 40 dimensional embedding and fixing these during training of both the policy and model.",4 Experiments,[0],[0]
"As the model, we use a standard linear chain CRF (Lafferty et al., 2001) for the first two sets of experiments, while for cold-start case we use a basic RNN classifier with the same multilingual embeddings as before, and a 128 dimensional hidden layer.
",4 Experiments,[0],[0]
"The proposed method is referred to as PAL, as shorthand Policy based Active Learning.",4 Experiments,[0],[0]
"Subscripts b,m, c are used to denote the bilingual, multilingual and cold-start experimental configurations.",4 Experiments,[0],[0]
"For comparative baselines, we use the following methods:
Uncertainty sampling we use the total token entropy measure (Settles and Craven, 2008), which takes the instance x maximising∑|x|
t=1H(yt|x, φ), where H is the token entropy.",4 Experiments,[0],[0]
"We use the whole training set as the data pool, and select a single instance for labelling in each active learning step.",4 Experiments,[0],[0]
"This method was shown to achieve the best result among model-independent active learning methods on the CoNLL data.
",4 Experiments,[0],[0]
"Random sampling which randomly selects examples from the unlabelled pool.
",4 Experiments,[0],[0]
"Results Figure 3 shows results the bilingual case, where PALb consistently outperforms the Random and Uncertainty baselines across the three target languages.",4 Experiments,[0],[0]
"Uncertainty sampling is ineffective, particularly towards the start of the run,
6http://128.2.220.95/multilingual
as a consequence of its dependence on a high quality model.",4 Experiments,[0],[0]
"The use of content information allows PALb to make a stronger start, despite the poor initial model.
",4 Experiments,[0],[0]
"Also shown in Figure 3 are results for multilingual policy learning, PALm, which outperform all other approaches including PALb.",4 Experiments,[0],[0]
"This illustrates that the additional training over several languages gives rise to a better policy, than only using one source language.",4 Experiments,[0],[0]
"The superior performance is particularly marked in the early stages of the runs for Spanish and Dutch, which may indicate that the approach was better able to learn to exploit the sentence content information.
",4 Experiments,[0],[0]
We evaluate the cold-start setting in Figure 4.,4 Experiments,[0],[0]
"Recall that in this setting there are no policy or model updates, as no heldout data is used, and all annotations arrive in a batch.",4 Experiments,[0],[0]
"The model, however, is initialised with a NER tagger trained on a different language, which explains why the performance for all methods starts from around 40% rather than 0%.",4 Experiments,[0],[0]
"Even in this challenging evaluation setting, our algorithm PALc outperforms both baseline methods, showing that deep Q learning allows for better exploitation of the pretrained classifier, alongside the sentence content.
",4 Experiments,[0],[0]
"Lastly, we report the results for all approaches in Table 2, based on training on the full 200 labelled sentences as selected under the different methods.",4 Experiments,[0],[0]
"It is clear that the PAL methods all outperform the baselines, and among these the multilingual training of PALm outperforms the bilingual setting in PALb.",4 Experiments,[0],[0]
"Surprisingly, PALc gives the overall best results, despite using a static policy and model during target application, underscoring the importance of model pretraining.",4 Experiments,[0],[0]
"Table 2 also re-
ports the cost reduction versus random sampling, showing that the PAL methods can reduce the annotation burden to as low as 10%.",4 Experiments,[0],[0]
"In this paper, we have proposed a new active learning algorithm capable of learning active learning strategies from data.",5 Conclusion,[0],[0]
"We formalise active learning under a Markov decision framework, whereby active learning corresponds to a sequence of binary annotation decisions applied to a stream of data.",5 Conclusion,[0],[0]
"Based on this, we design an active learning algorithm as a policy based on deep reinforcement learning.",5 Conclusion,[0],[0]
"We show how these learned active learning policies can be transferred between languages, which we empirically show provides consistent and sizeable improvements over baseline methods, including traditional uncertainty sampling.",5 Conclusion,[0],[0]
"This
holds true even in a very difficult cold-start setting, where no evaluation data is available, and there is no ability to react to annotations.",5 Conclusion,[0],[0]
This work was sponsored by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) under the Low Resource Languages for Emergent Incidents (LORELEI) program issued by DARPA/I2O under Contract No. HR0011-15-C-0114.,Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,Acknowledgments,[0],[0]
Trevor Cohn was supported by an Australian Research Council Future Fellowship.,Acknowledgments,[0],[0]
Active learning aims to select a small subset of data for annotation such that a classifier learned on the data is highly accurate.,abstractText,[0],[0]
"This is usually done using heuristic selection methods, however the effectiveness of such methods is limited and moreover, the performance of heuristics varies between datasets.",abstractText,[0],[0]
"To address these shortcomings, we introduce a novel formulation by reframing the active learning as a reinforcement learning problem and explicitly learning a data selection policy, where the policy takes the role of the active learning heuristic.",abstractText,[0],[0]
"Importantly, our method allows the selection policy learned using simulation on one language to be transferred to other languages.",abstractText,[0],[0]
"We demonstrate our method using cross-lingual named entity recognition, observing uniform improvements over traditional active learning.",abstractText,[0],[0]
Learning how to Active Learn: A Deep Reinforcement Learning Approach,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1874–1883 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1874",text,[0],[0]
"For many real-world NLP tasks, labeled data is rare while unlabelled data is abundant.",1 Introduction,[0],[0]
Active learning (AL) seeks to learn an accurate model with minimum amount of annotation cost.,1 Introduction,[0],[0]
It is inspired by the observation that a model can get better performance if it is allowed to choose the data points on which it is trained.,1 Introduction,[0],[0]
"For example, the learner can identify the areas of the space where it does not have enough knowledge, and query those data points which bridge its knowledge gap.
",1 Introduction,[0],[0]
"Traditionally, AL is performed using engineered heuristics in order to estimate the usefulness of unlabeled data points as queries to an annotator.",1 Introduction,[0],[0]
"Recent work (Fang et al., 2017; Bachman et al., 2017; Woodward and Finn, 2017) have focused on learning the AL querying strategy, as engineered heuristics are not flexible to exploit char-
acteristics inherent to a given problem.",1 Introduction,[0],[0]
"The basic idea is to cast AL as a decision process, where the most informative unlabeled data point needs to be selected based on the history of previous queries.",1 Introduction,[0],[0]
"However, previous works train for the AL policy by a reinforcement learning (RL) formulation, where the rewards are provided at the end of sequences of queries.",1 Introduction,[0],[0]
"This makes learning the AL policy difficult, as the policy learner needs to deal with the credit assignment problem.",1 Introduction,[0],[0]
"Intuitively, the learner needs to observe many pairs of query sequences and the resulting end-rewards to be able to associate single queries with their utility scores.
",1 Introduction,[0],[0]
"In this work, we formulate learning AL strategies as an imitation learning problem.",1 Introduction,[0],[0]
"In particular, we consider the popular pool-based AL scenario, where an AL agent is presented with a pool of unlabelled data.",1 Introduction,[0],[0]
"Inspired by the Dataset Aggregation (DAGGER) algorithm (Ross et al., 2011), we develop an effective AL policy learning method by designing an efficient and effective algorithmic expert, which provides the AL agent with good decisions in the encountered states.",1 Introduction,[0],[0]
We then use a deep feedforward network to learn the AL policy to associate states to actions.,1 Introduction,[0],[0]
"Unlike the RL approach, our method can get observations and actions directly from the expert’s trajectory.",1 Introduction,[0],[0]
"Therefore, our trained policy can make better rankings of unlabelled datapoints in the pool, leading to more effective AL strategies.
",1 Introduction,[0],[0]
We evaluate our method on text classification and named entity recognition.,1 Introduction,[0],[0]
"The results show our method performs better than strong AL methods using heuristics and reinforcement learning, in that it boosts the performance of the underlying model with fewer labelling queries.",1 Introduction,[0],[0]
An open source implementation of our model is available at: https://github.com/Grayming/ ALIL.,1 Introduction,[0],[0]
"We consider the popular pool-based AL setting where we are given a small set of initial labeled data and a large pool of unlabelled data, and a budget for getting the annotation of some unlabelled data by querying an oracle, e.g. a human annotator.",2 Pool-based AL as a Decision Process,[0],[0]
"The goal is to intelligently pick those unlabelled data for which if the annotations were available, the performance of the underlying re-trained model would be improved the most.
",2 Pool-based AL as a Decision Process,[0],[0]
The main challenge in AL is how to identify and select the most beneficial unlabelled data points.,2 Pool-based AL as a Decision Process,[0],[0]
"Various heuristics have been proposed to guide the unlabelled data selection (Settles, 2010).",2 Pool-based AL as a Decision Process,[0],[0]
"However, there is no one AL heuristic which performs best for all problems.",2 Pool-based AL as a Decision Process,[0],[0]
"The goal of this paper is to provide an approach to learn an AL strategy which is best suited for the problem at hand, instead of resorting to ad-hoc heuristics.
",2 Pool-based AL as a Decision Process,[0],[0]
"The AL strategy can be learned by attempting to actively learn on tasks sampled from a distribution over the tasks (Bachman et al., 2017).",2 Pool-based AL as a Decision Process,[0],[0]
"The idea is to simulate the AL scenario on instances of the problem created using available labeled data, where the label of some part of the data is kept hidden.",2 Pool-based AL as a Decision Process,[0],[0]
"This allows to have an automatic oracle to reveal the labels of the queried data, resulting in an efficient way to quickly evaluate a hypothesised AL strategy.",2 Pool-based AL as a Decision Process,[0],[0]
"Once the AL strategy is learned on simulations, it is then applied to real AL scenarios.",2 Pool-based AL as a Decision Process,[0],[0]
"The more related are the tasks in the real scenario to those used to train the AL strategy, the more effective the AL strategy would be.
",2 Pool-based AL as a Decision Process,[0],[0]
We are interested to train a model mφ which maps an inputx ∈ X to its label y ∈,2 Pool-based AL as a Decision Process,[0],[0]
"Yx , where Yx is the set of labels for the input x and φ is the parameter vector of the underling model.",2 Pool-based AL as a Decision Process,[0],[0]
"For example, in the named entity recognition (NER) task, the input is a sentence and the output is its label sequence, e.g. in the IBO format.",2 Pool-based AL as a Decision Process,[0],[0]
"Let D = {(x,y)} be a support set of labeled data, which is randomly partitioned into labeledDlab, unlabelledDunl, and evaluation Devl datasets.",2 Pool-based AL as a Decision Process,[0],[0]
Repeated random partitioning creates multiple instances of the AL problem.,2 Pool-based AL as a Decision Process,[0],[0]
"At each time step t of an AL problem, the algorithm interacts with the oracle and queries the label of a datapoint xt ∈ Dunlt .",2 Pool-based AL as a Decision Process,[0],[0]
"As the result of this action, the followings happen:
• The automatic oracle reveals the label yt;
• The labeled and unlabelled datasets are up-
dated to include and exclude the recently queried data point, respectively;
• The underlying model is re-trained based on the enlarged labeled data to update φ; and
• The AL algorithm receives a reward −loss(mφ, Devl), which is the negative loss of the current trained model on the evaluation set, defined as
loss(mφ, D evl) := ∑ (x,y)∈Devl loss(mφ(x), y)
where loss(y ′, y) is the loss incurred due to predicting y ′ instead of the ground truth y .
",2 Pool-based AL as a Decision Process,[0],[0]
"More formally, a pool-based AL problem is a Markov decision process (MDP), denoted by (S,A, Pr(st+1|st, at), R) where S is the state space, A is the set of actions, Pr(st+1|st, at) is the transition function, and R is the reward function.",2 Pool-based AL as a Decision Process,[0],[0]
The state st ∈ S at time t consists of the labeled Dlabt and unlabelled D unl t datasets paired with the parameters of the currently trained model φt.,2 Pool-based AL as a Decision Process,[0],[0]
An action at ∈,2 Pool-based AL as a Decision Process,[0],[0]
"A corresponds to the selection of a query datapoint, and the reward function R(st, at, st+1) := −loss(mφt , Devl).
",2 Pool-based AL as a Decision Process,[0],[0]
We aim to find the optimal AL policy prescribing which datapoint needs to be queried in a given state to get the most benefit.,2 Pool-based AL as a Decision Process,[0],[0]
"The optimal policy is found by maximising the following objective over the parameterised policies:
E(Dlab,Dunl,Devl)∼D [ Eπθ [ B∑ t=1 R(st, at, st+1) ]] (1)
where πθ is the policy network parameterised by θ,D is a distribution over possible AL problem instances, and B is the maximum number of queries made in an AL run, a.k.a. an episode.",2 Pool-based AL as a Decision Process,[0],[0]
"Following (Bachman et al., 2017), we maximise the sum of the rewards after each time step to encourage the anytime behaviour, i.e. the model should perform well after each label query.",2 Pool-based AL as a Decision Process,[0],[0]
"AL Policy
The question remains as how can we train the policy network to maximise the training objective in eqn 1.",3 Deep Imitation Learning to Train the,[0],[0]
"Typical learning approaches resort to deep reinforcement learning (RL) and provide training signal at the end of each episode to learn the optimal policy (Fang et al., 2017; Bachman
et al., 2017)",3 Deep Imitation Learning to Train the,[0],[0]
"e.g., using policy gradient methods.",3 Deep Imitation Learning to Train the,[0],[0]
"These approaches, however, need a large number of training episodes to learn a reasonable policy as they need to deal with the credit assignment problem, i.e. discovery of the utility of individual actions in the sequence based on the achieved reward at the end of the episode.",3 Deep Imitation Learning to Train the,[0],[0]
"This exacerbates the difficulty of finding a good AL policy.
",3 Deep Imitation Learning to Train the,[0],[0]
We formulate learning for the AL policy as an imitation learning problem.,3 Deep Imitation Learning to Train the,[0],[0]
"At each state, we provide the AL agent with a correct action which is computed by an algorithmic expert.",3 Deep Imitation Learning to Train the,[0],[0]
The AL agent uses the sequence of states observed in an episode paired with the expert’s sequence of actions to update its policy.,3 Deep Imitation Learning to Train the,[0],[0]
"This directly addresses the credit assignment problem, and reduces the complexity of the problem compared to the RL approaches.",3 Deep Imitation Learning to Train the,[0],[0]
"In what follows, we describe the ingredients of our deep imitation learning (IL) approach, which is summarised in Algorithm 1.
Algorithmic Expert.",3 Deep Imitation Learning to Train the,[0],[0]
"At a given AL state st, our algorithmic expert computes an action by evaluating the current pool of unlabeled data.",3 Deep Imitation Learning to Train the,[0],[0]
"More concretely, for each x′ ∈ Dpoolrnd and its correct label y ′, the underlying model mφt is re-trained to get mx ′
φt , where Dpoolrnd ⊂ D unl t is a small subset of the current large pool of unlabeled data.",3 Deep Imitation Learning to Train the,[0],[0]
"The expert action is then computed as:
arg min x′∈Dpoolrnd
loss(mx ′
φt (x), Devl).",3 Deep Imitation Learning to Train the,[0],[0]
"(2)
In other words, our algorithmic expert tries a subset of actions to roll-out one step from the current state, in order to efficiently compute a reasonable action.",3 Deep Imitation Learning to Train the,[0],[0]
"Searching for the optimal action would be O(|Dunl|B), which is computationally challenging due to (i) the large action set, and (ii) the exponential dependence on the length of the roll out.",3 Deep Imitation Learning to Train the,[0],[0]
"We will see in the experiments that our method efficiently learns effective AL policies.
",3 Deep Imitation Learning to Train the,[0],[0]
Policy Network.,3 Deep Imitation Learning to Train the,[0],[0]
Our policy network is a feedforward network with two fully-connected hidden layers.,3 Deep Imitation Learning to Train the,[0],[0]
"It receives the current AL state, and provides a preference score for a given unlabeled data point, allowing to select the most beneficial one corresponding to the highest score.",3 Deep Imitation Learning to Train the,[0],[0]
"The input to our policy network consists of three parts: (i) a fixed dimensional representation of the content and the predicted label of the unlabeled data point under consideration, (ii) a fixed-dimensional rep-
resentation of the content and the labels of the labeled dataset, and (iii) a fixed-dimensional representation of the content of the unlabeled dataset.
",3 Deep Imitation Learning to Train the,[0],[0]
Imitation Learning Algorithm.,3 Deep Imitation Learning to Train the,[0],[0]
A typical approach to imitation learning (IL) is to train the policy network so that it mimics the expert’s behaviour given training data of the encountered states (input) and actions (output) performed by the expert.,3 Deep Imitation Learning to Train the,[0],[0]
The policy network’s prediction affects future inputs during the execution of the policy.,3 Deep Imitation Learning to Train the,[0],[0]
"This violates the crucial independent and identically distributed (iid) assumption, inherent to most statistical supervised learning approaches for learning a mapping from states to actions.
",3 Deep Imitation Learning to Train the,[0],[0]
"We make use of Dataset Aggregation (DAGGER) (Ross et al., 2011), an iterative algorithm for IL which addresses the non-iid nature of the encountered states during the AL process (see Algorithm 1).",3 Deep Imitation Learning to Train the,[0],[0]
"In round τ of DAGGER, the learned policy network π̂τ is applied to the AL problem to collect a sequence of states which are paired with the expert actions.",3 Deep Imitation Learning to Train the,[0],[0]
"The collected pair of states and actions are aggregated to the dataset of such pairs M , collected from the previous iterations of the algorithm.",3 Deep Imitation Learning to Train the,[0],[0]
"The policy network is then re-trained on the aggregated set, resulting in π̂τ+1 for the next iteration of the algorithm.",3 Deep Imitation Learning to Train the,[0],[0]
"The intuition is to build up the set of states that the algorithm is likely to encounter during its execution, in order to increase the generalization of the policy network.",3 Deep Imitation Learning to Train the,[0],[0]
"To better leverage the training signal from the algorithmic expert, we allow the algorithm to collect state-action pairs according to a modified policy which is a mixture of π̂τ and the expert policy π̃∗τ , i.e.
πτ = βτ π̃ ∗ +",3 Deep Imitation Learning to Train the,[0],[0]
"(1− βτ )π̂τ
where βτ ∈",3 Deep Imitation Learning to Train the,[0],[0]
"[0, 1] is a mixing coefficient.",3 Deep Imitation Learning to Train the,[0],[0]
"This amounts to tossing a coin with parameter βτ in
each iteration of the algorithm to decide one of these two policies for data collection.
",3 Deep Imitation Learning to Train the,[0],[0]
Re-training the Policy Network.,3 Deep Imitation Learning to Train the,[0],[0]
"To train our policy network, we turn the preference scores to probabilities, and optimise the parameters such that the probability of the action prescribed by the expert is maximized.",3 Deep Imitation Learning to Train the,[0],[0]
"More specifically, let M := {(si, ai)}Ii=1 be the collected states paired with their expert’s prescribed actions.",3 Deep Imitation Learning to Train the,[0],[0]
"LetDpooli be the set of unlabelled datapoints in the pool within the state, and ai denote the datapoint selected by the expert in the set.",3 Deep Imitation Learning to Train the,[0],[0]
"Our training objective is∑I
i=1",3 Deep Imitation Learning to Train the,[0],[0]
"logPr(ai|D pool i ) where
Pr(ai|Dpooli ) := exp π̂(ai;si)∑
x∈Dpooli exp π̂(x;si)
.
",3 Deep Imitation Learning to Train the,[0],[0]
The above can be interpreted as the probability of ai being the best action among all possible actions in the state.,3 Deep Imitation Learning to Train the,[0],[0]
"Following (Mnih et al., 2015), we randomly sample multiple1 mini-batches from the replay memoryM, in addition to the current round’s stat-action pair, in order to retrain the policy network.",3 Deep Imitation Learning to Train the,[0],[0]
"For each mini-batch, we make one SGD step to update the policy, where the gradients of the network parameters are calculated using the backpropagation algorithm.
",3 Deep Imitation Learning to Train the,[0],[0]
Transferring the Policy.,3 Deep Imitation Learning to Train the,[0],[0]
We now apply the policy learned on the source task to AL in the target task.,3 Deep Imitation Learning to Train the,[0],[0]
We expect the learned policy to be effective for target tasks which are related to the source task in terms of the data distribution and characteristics.,3 Deep Imitation Learning to Train the,[0],[0]
Algorithm 2 illustrates the policy transfer.,3 Deep Imitation Learning to Train the,[0],[0]
"The pool-based AL scenario in Algorithm 2 is cold-start; however, extending to incorporate initially available labeled data is straightforward.",3 Deep Imitation Learning to Train the,[0],[0]
We conduct experiments on text classification and named entity recognition (NER).,4 Experiments,[0],[0]
"The AL scenarios include cross-domain sentiment classification, cross-lingual authorship profiling, and crosslingual named entity recognition (NER), whereby an AL policy trained on a source domain/language is transferred to the target domain/language.
",4 Experiments,[0],[0]
"We compare our proposed AL method using imitation learning (ALIL) with the followings:
• Random sampling:",4 Experiments,[0],[0]
"The query datapoint is chosen randomly.
",4 Experiments,[0],[0]
"1In our experiments, we use 10 mini-bathes, each of which of size 100.
",4 Experiments,[0],[0]
"Algorithm 1 Learn active learning policy via imitation learning Input: large labeled data D, max episodes T , budget B,
sample size K, the coin parameter β Output: The learned policy 1: M ← ∅ .",4 Experiments,[0],[0]
"the aggregated dataset 2: initialise π̂1 with a random policy 3: for τ=1, . . .",4 Experiments,[0],[0]
", T do 4: Dlab, Dunl, Devl ← dataPartition(D) 5: φ1 ← trainModel(Dlab) 6: c← coinToss(β) 7: for t ∈ 1, . . .",4 Experiments,[0],[0]
",B do 8:",4 Experiments,[0],[0]
"Dpoolrnd ← sampleUniform(D unl,K)
9: st ← (Dlab, Dpoolrnd ,φt) 10: at ← argminx′∈Dpool
rnd loss(mx
′ φt , Devl)
11: if c is head then .",4 Experiments,[0],[0]
the expert 12: xt ← at 13: else .,4 Experiments,[0],[0]
"the policy 14: xt ← argmaxx′∈Dpool
rnd π̂τ (x
′;st)
15: end if 16: Dlab ← Dlab + {(xt, yt)} 17:",4 Experiments,[0],[0]
"Dunl ← Dunl − {xt} 18: M ←M + {(st, at)} 19: φt+1 ← retrainModel(φt, Dlab) 20: end for 21: π̂τ+1 ← retrainPolicy(π̂τ ,M) 22: end for 23: return π̂T+1
Algorithm 2 Active learning by policy transfer Input: unlabeled pool Dunl, budget B, policy π̂",4 Experiments,[0],[0]
"Output: labeled dataset and trained model 1: Dlab ← ∅ 2: initialise φ randomly 3: for t ∈ 1, . . .",4 Experiments,[0],[0]
",B do 4: st ← (Dlab, Dunl,φ) 5: xt ← argmaxx′∈Dunl π̂(x′;st) 6: yt ← askAnnotation(xt) 7: Dlab ← Dlab + {(xt, yt)} 8:",4 Experiments,[0],[0]
Dunl ← Dunl − {xt} 9: φ←,4 Experiments,[0],[0]
"retrainModel(φ,Dlab) 10: end for 11: return Dlab and φ
• Diversity sampling: The query datapoint is argminx ∑ x′∈Dlab Jaccard(x,x
′), where the Jaccard coefficient between the unigram features of the two given texts is used as the similarity measure.
",4 Experiments,[0],[0]
"• Uncertainty-based sampling: For text classification, we use the datapoint with the highest predictive entropy, argmaxx − ∑ y p(y|x,D
lab) log p(y|x,Dlab) where p(y|x,Dlab) comes from the underlying model.",4 Experiments,[0],[0]
"We further use a state-of-the-art extension of this method, called uncertainty with rationals (Sharma et al., 2015), which not only considers uncertainty but also looks whether
the unlabelled document contains sentiment words or phrases that were returned as rationales for any of the existing labeled documents.",4 Experiments,[0],[0]
"For NER, we use the Total Token Entropy (TTE) as the uncertainty sampling method, argmaxx − ∑|x| i=1",4 Experiments,[0],[0]
"∑ yi p(yi|x,Dlab) log p(yi|x,Dlab) which has been shown to be the best heuristic for this task among 17 different heuristics (Settles and Craven, 2008).
",4 Experiments,[0],[0]
"• PAL: A reinforcement learning based approach (Fang et al., 2017), which makes use a deep Q-network to make the selection decision for stream-based active learning.",4 Experiments,[0],[0]
Datasets and Setup.,4.1 Text Classification,[0],[0]
"The first task is sentiment classification, in which product reviews express either positive or negative sentiment.",4.1 Text Classification,[0],[0]
"The data comes from the Amazon product reviews (McAuley and Yang, 2016); see Table 1 for data statistics.
",4.1 Text Classification,[0],[0]
"The second task is Authorship Profiling, in which we aim to predict the gender of the text author.",4.1 Text Classification,[0],[0]
"The data comes from the gender profiling task in PAN 2017 (Rangel et al., 2017), which consists of a large Twitter corpus in multiple languages: English (en), Spanish (es) and Portuguese (pt).",4.1 Text Classification,[0],[0]
"For each language, all tweets collected from a user constitute one document; Table 1 shows data statistics.",4.1 Text Classification,[0],[0]
"The multilingual embeddings for this task come from off-the-shelf CCA-trained embeddings (Ammar et al., 2016) for twelve languages, including English, Spanish and Portuguese.",4.1 Text Classification,[0],[0]
"We fix these word embeddings during training of both the policy and the underlying classification model.
",4.1 Text Classification,[0],[0]
"For training, 10% of the source data is used as the evaluation set for computing the best action in imitation learning.",4.1 Text Classification,[0],[0]
"We run T = 100 episodes with the budget B = 100 documents in each episode, set the sample size K = 5, and fix the mixing coefficient βτ = 0.5.",4.1 Text Classification,[0],[0]
"For testing, we take 90% of the target data as the unlabeled pool, and the
remaining 10% as the test set.",4.1 Text Classification,[0],[0]
We show the test accuracy w.r.t.,4.1 Text Classification,[0],[0]
"the number of labelled documents selected in the AL process.
",4.1 Text Classification,[0],[0]
"As the underlying model mφ , we use a fast and efficient text classifier based on convolutional neural networks.",4.1 Text Classification,[0],[0]
"More specifically, we apply 50 convolutional filters with ReLU activation on the embedding of all words in a document x, where the width of the filters is 3.",4.1 Text Classification,[0],[0]
"The filter outputs are averaged to produce a 50-dimensional document representation h(x), which is then fed into a softmax to predict the class.
",4.1 Text Classification,[0],[0]
Representing state-action.,4.1 Text Classification,[0],[0]
"The input to the policy network, i.e. the feature vector representing a state-action pair, includes: the candidate document represented by the convolutional net h(x), the distribution over the document’s class labels mφ(x), the sum of all document vector representations in the labeled set ∑ x′∈Dlab h(x
′), the sum of all document vectors in the random pool of unlabelled data
∑ x′∈Dpoolrnd
h(x′), and the empirical distribution of class labels in the labeled dataset.
Results.",4.1 Text Classification,[0],[0]
"Fig 2 shows the results on product sentiment prediction and authorship profiling, in cross-domain and cross-lingual AL scenarios2.",4.1 Text Classification,[0],[0]
"Our ALIL method consistently outperforms both heuristic-based and RL-based (PAL) (Fang et al., 2017) approaches across all tasks.",4.1 Text Classification,[0],[0]
"ALIL tends to convergence faster than other methods, which indicates its policy can quickly select the most informative datapoints.",4.1 Text Classification,[0],[0]
"Interestingly, the uncertainty and diversity sampling heuristics perform worse than random sampling on sentiment classification.",4.1 Text Classification,[0],[0]
We speculate this may be due to these two heuristics not being able to capture the polarity information during the data selection process.,4.1 Text Classification,[0],[0]
"PAL performs on-par with uncertainty with rationals on musical device, both of which outperform the traditional diversity and uncertainty sampling heuristics.",4.1 Text Classification,[0],[0]
"Interestingly, PAL is outperformed by random sampling on movie reviews, and by the traditional uncertainty sampling heuristic on authorship profiling tasks.",4.1 Text Classification,[0],[0]
"We attribute this to ineffectiveness of the RL-based approach for learning a reasonable AL query strategy.
",4.1 Text Classification,[0],[0]
We further investigate combining the transfer of the policy network with the transfer of the underlying classifier.,4.1 Text Classification,[0],[0]
"That is, we first train a classi-
2Uncertainty with rationale cannot be done for authorship profiling as the rationales come from a sentiment dictionary.
fier on all of the annotated data from the source domain/language.",4.1 Text Classification,[0],[0]
"Then, this classifier is ported to the target domain/language; for cross-language transfer, we make use of multilingual word embeddings.",4.1 Text Classification,[0],[0]
"We start the AL process starting from the transferred classifier, referred to as the warmstart AL.",4.1 Text Classification,[0],[0]
We compare the performance of the directly transferred classifier with those obtained after the AL process in the warm-start and cold-start scenarios.,4.1 Text Classification,[0],[0]
The results are shown in Table 2.,4.1 Text Classification,[0],[0]
"We have run the cold-start and warm-start AL for 25 times, and reported the average accuracy in Table 2.",4.1 Text Classification,[0],[0]
"As seen from the results, both the cold and warm start AL settings outperform the direct transfer significantly, and the warm start consistently gets higher accuracy than the cold start.",4.1 Text Classification,[0],[0]
"The difference between the results are statistically significant, with a p-value of .001, according to McNemar test3 (Dietterich, 1998).",4.1 Text Classification,[0],[0]
"Data and setup We use NER corpora from the CONLL2002/2003 shared tasks, which include annotated text in English (en), German (de), Spanish (es), and Dutch (nl).",4.2 Named Entity Recognition,[0],[0]
"The original annotation is based on IOB1, which we convert to the IO
3As the contingency table needed for the McNemar test, we have used the average counts across the 25 runs.
labelling scheme.",4.2 Named Entity Recognition,[0],[0]
"Following Fang et al. (2017), we consider two experimental conditions: (i) the bilingual scenario where English is the source (used for policy training) and other languages are the target, and (ii) the multilingual scenario where one of the languages (except English) is the target and the remaining ones are the source used in joint training of the AL policy.",4.2 Named Entity Recognition,[0],[0]
The underlying model mφ is a conditional random field (CRF) treating NER as a sequence labelling task.,4.2 Named Entity Recognition,[0],[0]
"The prediction is made using the Viterbi algorithm.
",4.2 Named Entity Recognition,[0],[0]
"In the existing corpus partitions from CoNLL, each language has three subsets: train, testa and testb.",4.2 Named Entity Recognition,[0],[0]
"During policy training with the source language(s), we combine these three subsets, shuffle, and re-split them into simulated training, unlabelled pool, and evaluation sets in every episode.",4.2 Named Entity Recognition,[0],[0]
"We run N = 100 episodes with the budget B = 200, and set the sample size k = 5.",4.2 Named Entity Recognition,[0],[0]
"When we transfer the policy to the target language, we do one episode and select B datapoints from train (treated as the pool of unlabeled data) and report F1 scores on testa.
",4.2 Named Entity Recognition,[0],[0]
Representing state-action.,4.2 Named Entity Recognition,[0],[0]
"The input to the policy network includes the representation of the candidate sentence using the sum of its words’ embeddings h(x), the representation of the labelling marginals using the label-level convolutional network cnnlab(Emφ(y|x)[y]) (Fang et al., 2017), the representation of sentences in the labeled data ∑ (x′,y ′)∈Dlab h(x
′), the representation of sentences in the random pool of unlabelled data
∑ x′∈Dpoolrnd
h(x′), the representation of ground-truth labels in the labeled data∑
(x′,y ′)∈Dlab cnnlab(y ′) using the empirical distributions, and the confidence of the sequential pre-
diction |x|",4.2 Named Entity Recognition,[0],[0]
"√
maxy mφ(y|x), where |x| denotes the length of the sentence x.",4.2 Named Entity Recognition,[0],[0]
"For the word embeddings, we use off-the-shelf CCA trained multilingual embeddings (Ammar et al., 2016) with 40 dimensions; we fix these during policy training.
Results.",4.2 Named Entity Recognition,[0],[0]
Fig. 3 shows the results for three target languages.,4.2 Named Entity Recognition,[0],[0]
"In addition to the strong heuristicbased methods, we compare our imitation learning approach (ALIL) with the reinforcement learning approach (PAL) (Fang et al., 2017), in both bilingual (bi) and multilingual (mul) transfer settings.",4.2 Named Entity Recognition,[0],[0]
"Across all three languages, ALIL.bi and ALIL.mul outperform the heuristic methods, including Uncertainty Sampling based on TTE.",4.2 Named Entity Recognition,[0],[0]
"This is expected as the uncertainty sampling largely relies on a high quality underlying model, and diversity sampling ignores the labelling information.",4.2 Named Entity Recognition,[0],[0]
"In the bilingual case, ALIL.bi outperforms PAL.bi on Spanish (es) and Dutch (nl), and performs similarly on German (de).",4.2 Named Entity Recognition,[0],[0]
"In the multilingual case, ALIL.mul achieves the best performance on Spanish, and performs competitively with PAL.mul on German and Dutch.",4.2 Named Entity Recognition,[0],[0]
Insight on the selected data.,4.3 Analysis,[0],[0]
We compare the data selected by ALIL to other methods.,4.3 Analysis,[0],[0]
"This will confirm that ALIL learns policies which are suitable for the problem at hand, without resorting to a fixed engineered heuristics.",4.3 Analysis,[0],[0]
"For this analysis, we report the mean reciprocal rank (MRR) of the data points selected by the ALIL policy under rankings of the unlabelled pool generated by the uncertainty and diversity sampling.",4.3 Analysis,[0],[0]
"Furthermore, we measure the fraction of times the decisions made by the ALIL policy agrees with those which would
have been made by the heuristic methods, which is measured by the accuracy (acc).",4.3 Analysis,[0],[0]
Table 3 report these measures.,4.3 Analysis,[0],[0]
"As we can see, for sentiment classification since uncertainty and diversity sampling perform badly, ALIL has a big disagreement with them on the selected data points.",4.3 Analysis,[0],[0]
"While for gender classification on Portuguese and NER on Spanish, ALIL shows much more agreement with other three heuristics.
",4.3 Analysis,[0],[0]
"Lastly, we compare chosen queries by ALIL to those by PAL, to investigate the extent of the agreement between these two methods.",4.3 Analysis,[0],[0]
This is simply measure by the fraction of identical query data points among the total number of queries (i.e. accuracy).,4.3 Analysis,[0],[0]
"Since PAL is stream-based and sensitive to the order in which it receives the data points, we report the average accuracy taken over multiple runs with random input streams.",4.3 Analysis,[0],[0]
The expected accuracy numbers are reported in Table 3.,4.3 Analysis,[0],[0]
"As seen, ALIL has higher overlap with PAL than the heuristic-based methods, in terms of the selected queries.
",4.3 Analysis,[0],[0]
Sensitivity toK.,4.3 Analysis,[0],[0]
"As seen in Algorithm 1, we resort to an approximate algorithmic expert, which selects the best action in a random subset of the
pool of unlabelled data with size K, in order to make the policy training efficient.",4.3 Analysis,[0],[0]
"Note that, in policy training, settingK to one and the size of the unlabelled data pool correspond to stream-based and pool-based AL scenarios, respectively.",4.3 Analysis,[0],[0]
"By changingK to values between these two extremes, we can analyse the effect of the quality of the algorithmic expert on the trained policy; Figure 4 shows the results.",4.3 Analysis,[0],[0]
"A larger candidate set may correspond to a better learned policy, needed to be traded off with the training time growing linearly with K. Interestingly, even small candidate sets lead to strong AL policies as increasing K beyond 10 does not change the performance significantly.
",4.3 Analysis,[0],[0]
Dynamically changing β.,4.3 Analysis,[0],[0]
"In our algorithm, β plays an important role as it trades off exploration versus exploitation.",4.3 Analysis,[0],[0]
"In the above experiments, we fix it to 0.5; however, we can change its value throughout trajectory collection as a function of τ (see Algorithm 1).",4.3 Analysis,[0],[0]
"We investigate schedules which tend to put more emphasis on exploration and exploitation towards the beginning and end of data collection, respectively.",4.3 Analysis,[0],[0]
"We investigate the following schedules: (i) linear βτ = max(0.5, 1− 0.01τ), (ii) exponential βτ = 0.9τ , and (iii) and inverse sigmoid βτ = 55+exp(τ/5) , as a function of iterations.",4.3 Analysis,[0],[0]
Fig. 5 shows the comparisons of these schedules.,4.3 Analysis,[0],[0]
The learned policy seems to perform competitively with either a fixed or an exponential schedule.,4.3 Analysis,[0],[0]
"We have also investigated tossing the coin in each step within the trajectory roll out, but found that it is more effective to have it before the full trajectory roll out (as currently done in Algorithm 1).",4.3 Analysis,[0],[0]
"Traditional active learning algorithms rely on various heuristics (Settles, 2010), such as uncertainty sampling (Settles and Craven, 2008; Houlsby et al., 2011), query-by-committee (GiladBachrach et al., 2006), and diversity sampling (Brinker, 2003; Joshi et al., 2009; Yang et al., 2015).",5 Related Work,[0],[0]
"Apart from these, different heuristics can be combined, thus creating integrated strategy which consider one or more heuristics at the same time.",5 Related Work,[0],[0]
"Combined with transfer learning, pre-existing labeled data from related tasks can help improve the performance of an active learner (Xiao and Guo, 2013; Kale and Liu, 2013; Huang and Chen, 2016; Konyushkova et al., 2017).",5 Related Work,[0],[0]
"More recently, deep reinforcement learning is used as the framework for learning active learning algorithms, where the active learning cycle is considered as a decision process.",5 Related Work,[0],[0]
"(Woodward and Finn, 2017) extended one shot learning to active learning and combined reinforcement learning with a deep recurrent model to make labeling decisions.",5 Related Work,[0],[0]
"(Bachman et al., 2017) introduced a policy gradient based method which jointly learns data representation, selection heuristic as well as the model prediction function.",5 Related Work,[0],[0]
"(Fang et al., 2017) designed an active learning algorithm based on a deep Qnetwork, in which the action corresponds to binary annotation decisions applied to a stream of data.",5 Related Work,[0],[0]
"The learned policy can then be transferred between languages or domains.
",5 Related Work,[0],[0]
Imitation learning (IL) refers to an agent’s acquisition of skills or behaviours by observing an expert’s trajectory in a given task.,5 Related Work,[0],[0]
It helps reduce sequential prediction tasks into supervised learning by employing a (near) optimal oracle at training time.,5 Related Work,[0],[0]
"Several IL algorithms has been proposed in sequential prediction tasks, including SEARA (Daumé et al., 2009), AggreVaTe (Ross and Bagnell, 2014), DaD (Venkatraman et al., 2015), LOLS(Chang et al., 2015), DeeplyAggreVaTe (Sun et al., 2017).",5 Related Work,[0],[0]
"Our work is closely related to Dagger (Ross et al., 2011), which can guarantee to find a good policy by addressing the dependency nature of encountered states in a trajectory.",5 Related Work,[0],[0]
"In this paper, we have proposed a new method for learning active learning algorithms using deep imitation learning.",6 Conclusion,[0],[0]
"We formalize pool-based active
learning as a Markov decision process, in which active learning corresponds to the selection decision of the most informative data points from the pool.",6 Conclusion,[0],[0]
Our efficient algorithmic expert provides state-action pairs from which effective active learning policies can be learned.,6 Conclusion,[0],[0]
"We show that the algorithmic expert allows direct policy learning, while at the same time, the learned policies transfer successfully between domains and languages, demonstrating improvement over previous heuristic and reinforcement learning approaches.",6 Conclusion,[0],[0]
We would like to thank the feedback from anonymous reviewers.,Acknowledgments,[0],[0]
G. H. is grateful to Trevor Cohn for interesting discussions.,Acknowledgments,[0],[0]
This work was supported by computational resources from the Multimodal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) at Monash University.,Acknowledgments,[0],[0]
Heuristic-based active learning (AL) methods are limited when the data distribution of the underlying learning problems vary.,abstractText,[0],[0]
We introduce a method that learns an AL policy using imitation learning (IL).,abstractText,[0],[0]
"Our IL-based approach makes use of an efficient and effective algorithmic expert, which provides the policy learner with good actions in the encountered AL situations.",abstractText,[0],[0]
"The AL strategy is then learned with a feedforward network, mapping situations to most informative query datapoints.",abstractText,[0],[0]
We evaluate our method on two different tasks: text classification and named entity recognition.,abstractText,[0],[0]
Experimental results show that our IL-based AL strategy is more effective than strong previous methods using heuristics and reinforcement learning.,abstractText,[0],[0]
Learning How to Actively Learn: A Deep Imitation Learning Approach,title,[0],[0]
"As neural networks become increasingly popular, their black box reputation is a barrier to adoption when interpretability is paramount.",1. Introduction,[0],[0]
"Here, we present DeepLIFT (Deep Learning Important FeaTures), a novel algorithm to assign importance score to the inputs for a given output.",1. Introduction,[0],[0]
Our approach is unique in two regards.,1. Introduction,[0],[0]
"First, it frames the question of importance in terms of differences from a ‘reference’ state, where the ‘reference’ is chosen according to the problem at hand.",1. Introduction,[0],[0]
"In contrast to most gradient-based methods, using a difference-from-reference allows DeepLIFT to propagate an importance signal even in situations where the gradient is zero and avoids artifacts caused by discontinuities in the gradient.",1. Introduction,[0],[0]
"Second, by op-
1Stanford University, Stanford, California, USA.",1. Introduction,[0],[0]
"Correspondence to: A Kundaje <akundaje@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"tionally giving separate consideration to the effects of positive and negative contributions at nonlinearities, DeepLIFT can reveal dependencies missed by other approaches.",1. Introduction,[0],[0]
"As DeepLIFT scores are computed using a backpropagationlike algorithm, they can be obtained efficiently in a single backward pass after a prediction has been made.",1. Introduction,[0],[0]
This section provides a review of existing approaches to assign importance scores for a given task and input example.,2. Previous Work,[0],[0]
These approaches make perturbations to individual inputs or neurons and observe the impact on later neurons in the network.,2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Zeiler & Fergus (Zeiler & Fergus, 2013) occluded different segments of an input image and visualized the change in the activations of later layers.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"“In-silico mutagenesis” (Zhou & Troyanskaya, 2015) introduced virtual mutations at individual positions in a genomic sequence and quantified the their impact on the output.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Zintgraf et al. (Zintgraf et al., 2017) proposed a clever strategy for analyzing the difference in a prediction after marginalizing over each input patch.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"However, such methods can be computationally inefficient as each perturbation requires a separate forward propagation through the network.",2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
They may also underestimate the importance of features that have saturated their contribution to the output (Fig. 1).,2.1. Perturbation-Based Forward Propagation Approaches,[0],[0]
"Unlike perturbation methods, backpropagation approaches propagate an importance signal from an output neuron backwards through the layers to the input in one pass, making them efficient.",2.2. Backpropagation-Based Approaches,[0],[0]
DeepLIFT is one such approach.,2.2. Backpropagation-Based Approaches,[0],[0]
"Simonyan et al. (Simonyan et al., 2013) proposed using the gradient of the output w.r.t.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
pixels of an input image to compute a “saliency map” of the image in the context of image classification tasks.,"2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"The authors showed that this was similar to deconvolutional networks (Zeiler & Fergus,
2013) except for the handling of the nonlinearity at rectified linear units (ReLUs).","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"When backpropagating importance using gradients, the gradient coming into a ReLU during the backward pass is zero’d out if the input to the ReLU during the forward pass is negative.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"By contrast, when backpropagating an importance signal in deconvolutional networks, the importance signal coming into a ReLU during the backward pass is zero’d out if and only if it is negative, with no regard to sign of the input to the ReLU during the forward pass.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Springenberg et al., (Springenberg et al., 2014) combined these two approaches into Guided Backpropagation, which zero’s out the importance signal at a ReLU if either the input to the ReLU during the forward pass is negative or the importance signal during the backward pass is negative.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Guided Backpropagation can be thought of as equivalent to computing gradients, with the caveat that any gradients that become negative during the backward pass are discarded at ReLUs.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Due to the zero-ing out of negative gradients, both guided backpropagation and deconvolutional networks can fail to highlight inputs that contribute negatively to the output.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Additionally, none of the three approaches would address the saturation problem illustrated in Fig. 1, as the gradient of y w.r.t.","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"h is negative (causing Guided Backprop and deconvolutional networks to assign zero importance), and the gradient of h w.r.t both i1 and i2 is zero when i1 + i2 > 1 (causing both gradients and Guided Backprop to be zero).","2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
Discontinuities in the gradients can also cause undesirable artifacts (Fig. 2).,"2.2.1. GRADIENTS, DECONVOLUTIONAL NETWORKS AND GUIDED BACKPROPAGATION",[0],[0]
"Bach et al. (Bach et al., 2015) proposed an approach for propagating importance scores called Layerwise Relevance Propagation (LRP).",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"Shrikumar et al. and Kindermans et al. (Shrikumar et al., 2016; Kindermans et al., 2016) showed that absent modifications to deal with numerical stability, the original LRP rules were equivalent within a scaling fac-
tor to an elementwise product between the saliency maps of Simonyan et al. and the input (in other words, gradient × input).",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"In our experiments, we compare DeepLIFT to gradient × input as the latter is easily implemented on a GPU, whereas LRP does not currently have GPU implementations available to our knowledge.
",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"While gradient × input is often preferable to gradients alone as it leverages the sign and strength of the input, it still does not address the saturation problem in Fig. 1 or the thresholding artifact in Fig. 2.",2.2.2. LAYERWISE RELEVANCE PROPAGATION AND GRADIENT × INPUT,[0],[0]
"Instead of computing the gradients at only the current value of the input, one can integrate the gradients as the inputs are scaled up from some starting value (eg: all zeros) to their current value (Sundararajan et al., 2016).",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"This addressess the saturation and thresholding problems of Fig. 1 and Fig. 2, but numerically obtaining high-quality integrals adds computational overhead.",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"Further, this approach can still give misleading results (see Section 3.4.3).",2.2.3. INTEGRATED GRADIENTS,[0],[0]
"Grad-CAM (Selvaraju et al., 2016) computes a coarsegrained feature-importance map by associating the feature maps in the final convolutional layer with particular classes based on the gradients of each class w.r.t.",2.3. Grad-CAM and Guided CAM,[0],[0]
"each feature map, and then using the weighted activations of the feature maps as an indication of which inputs are most important.",2.3. Grad-CAM and Guided CAM,[0],[0]
"To obtain more fine-grained feature importance, the authors proposed performing an elementwise product between the scores obtained from Grad-CAM and the scores obtained from Guided Backpropagation, termed Guided Grad-CAM.",2.3. Grad-CAM and Guided CAM,[0],[0]
"However, this strategy inherits the limitations of Guided Backpropagation caused by zero-ing out negative gradients during backpropagation.",2.3. Grad-CAM and Guided CAM,[0],[0]
It is also specific to convolutional neural networks.,2.3. Grad-CAM and Guided CAM,[0],[0]
DeepLIFT explains the difference in output from some ‘reference’ output in terms of the difference of the input from some ‘reference’ input.,3.1. The DeepLIFT Philosophy,[0],[0]
The ‘reference’ input represents some default or ‘neutral’ input that is chosen according to what is appropriate for the problem at hand (see Section 3.3 for more details).,3.1. The DeepLIFT Philosophy,[0],[0]
"Formally, let t represent some target output neuron of interest and let x1, x2, ..., xn represent some neurons in some intermediate layer or set of layers that are necessary and sufficient to compute t. Let t0 represent the reference activation of t. We define the quantity ∆t to be the difference-from-reference, that is ∆t = t− t0.",3.1. The DeepLIFT Philosophy,[0],[0]
"DeepLIFT assigns contribution scores C∆xi∆t to ∆xi s.t.:
n∑ i=1",3.1. The DeepLIFT Philosophy,[0],[0]
"C∆xi∆t = ∆t (1)
",3.1. The DeepLIFT Philosophy,[0],[0]
We call Eq. 1 the summation-to-delta property.,3.1. The DeepLIFT Philosophy,[0],[0]
C∆xi∆t can be thought of as the amount of difference-fromreference in t that is attributed to or ‘blamed’ on the difference-from-reference of xi.,3.1. The DeepLIFT Philosophy,[0],[0]
"Note that when a neuron’s transfer function is well-behaved, the output is locally linear in its inputs, providing additional motivation for Eq. 1.
C∆xi∆t can be non-zero even when ∂t ∂xi is zero.",3.1. The DeepLIFT Philosophy,[0],[0]
"This allows DeepLIFT to address a fundamental limitation of gradients because, as illustrated in Fig. 1, a neuron can be signaling meaningful information even in the regime where its gradient is zero.",3.1. The DeepLIFT Philosophy,[0],[0]
"Another drawback of gradients addressed by DeepLIFT is illustrated in Fig. 2, where the discontinuous nature of gradients causes sudden jumps in the importance score over infinitesimal changes in the input.",3.1. The DeepLIFT Philosophy,[0],[0]
"By contrast, the difference-from-reference is continuous, allowing DeepLIFT to avoid discontinuities caused by bias terms.",3.1. The DeepLIFT Philosophy,[0],[0]
"For a given input neuron x with difference-from-reference ∆x, and target neuron t with difference-from-reference ∆t that we wish to compute the contribution to, we define the multiplier m∆x∆t as:
m∆x∆t = C∆x∆t
∆x (2)
",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"In other words, the multiplier m∆x∆t is the contribution of ∆x to ∆t divided by ∆x.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"Note the close analogy to the
idea of partial derivatives: the partial derivative ∂t∂x is the infinitesimal change in t caused by an infinitesimal change in x, divided by the infinitesimal change in x.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"The multiplier is similar in spirit to a partial derivative, but over finite differences instead of infinitesimal ones.",3.2.1. DEFINITION OF MULTIPLIERS,[0],[0]
"Assume we have an input layer with neurons x1, ..., xn, a hidden layer with neurons y1, ..., yn, and some target output neuron t. Given values for m∆xi∆yj and m∆yj∆t, the following definition of m∆xi∆t is consistent with the summation-to-delta property in Eq. 1 (see Appendix A for the proof):
m∆xi∆t = ∑ j m∆xi∆yjm∆yj∆t",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"(3)
We refer to Eq. 3 as the chain rule for multipliers.",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"Given the multipliers for each neuron to its immediate successors, we can compute the multipliers for any neuron to a given target neuron efficiently via backpropagation - analogous to how the chain rule for partial derivatives allows us to compute the gradient w.r.t.",3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
the output via backpropagation.,3.2.2. THE CHAIN RULE FOR MULTIPLIERS,[0],[0]
"When formulating the DeepLIFT rules described in Section 3.5, we assume that the reference of a neuron is its activation on the reference input.",3.3. Defining the Reference,[0],[0]
"Formally, say we have a neuron y with inputs x1, x2, ... such that y = f(x1, x2, ...).",3.3. Defining the Reference,[0],[0]
"Given the reference activations x01, x 0 2, ... of the inputs, we can calculate the reference activation y0 of the output as:
y0 = f(x01, x 0 2, ...)",3.3. Defining the Reference,[0],[0]
"(4)
i.e. references for all neurons can be found by choosing a reference input and propagating activations through the net.
",3.3. Defining the Reference,[0],[0]
The choice of a reference input is critical for obtaining insightful results from DeepLIFT.,3.3. Defining the Reference,[0],[0]
"In practice, choosing a good reference would rely on domain-specific knowledge, and in some cases it may be best to compute DeepLIFT scores against multiple different references.",3.3. Defining the Reference,[0],[0]
"As a guiding principle, we can ask ourselves “what am I interested in measuring differences against?”.",3.3. Defining the Reference,[0],[0]
"For MNIST, we use a reference input of all-zeros as this is the background of the images.",3.3. Defining the Reference,[0],[0]
"For the binary classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}), we obtained sensible results using either a reference input containing the expected frequencies of ACGT in the background (Fig. 5), or by averaging the results over multiple reference inputs for each sequence that are generated by shuffling each original sequence (Appendix J).",3.3. Defining the Reference,[0],[0]
"For CIFAR10 data, we found that using a blurred version of the original image as the
reference highlighted outlines of key objects, while an allzeros reference highlighted hard-to-interpret pixels in the background (Appendix L).
",3.3. Defining the Reference,[0],[0]
It is important to note that gradient×input implicitly uses a reference of all-zeros (it is equivalent to a first-order Taylor approximation of gradient×∆input where ∆ is measured w.r.t.,3.3. Defining the Reference,[0],[0]
an input of zeros).,3.3. Defining the Reference,[0],[0]
"Similary, integrated gradients (Section 2.2.3) requires the user to specify a starting point for the integral, which is conceptually similar to specifying a reference for DeepLIFT.",3.3. Defining the Reference,[0.9503074414984285],"['One solution for conversation disentanglement is to model the task as a topic detection and tracking (TDT) (Allan, 2002) task by deciding whether each incoming message starts a new topic or belongs to an existing conversation.']"
"While Guided Backprop and pure gradients don’t use a reference, we argue that this is a limitation as these methods only describe the local behaviour of the output at the specific input value, without considering how the output behaves over a range of inputs.",3.3. Defining the Reference,[0],[0]
"We will see in Section 3.5.3 that, in some situations, it is essential to treat positive and negative contributions differently.",3.4. Separating Positive and Negative Contributions,[0],[0]
"To do this, for every neuron y, we will introduce ∆y+ and ∆y− to represent the positive and negative components of ∆y, such that:
∆y = ∆y+ + ∆y−
C∆y∆t = C∆y+∆t + C∆y−∆t
For linear neurons, ∆y+ and ∆y− are found by writing ∆y as a sum of terms involving its inputs ∆xi and grouping positive and negative terms together.",3.4. Separating Positive and Negative Contributions,[0],[0]
"The importance of this will become apparent when applying the RevealCancel rule (Section 3.5.3), where for a given target neuron t we may find thatm∆y+∆t andm∆y−∆t differ.",3.4. Separating Positive and Negative Contributions,[0],[0]
"However, when applying only the Linear or Rescale rules (Section 3.5.1 and Section 3.5.2), m∆y∆t = m∆y+∆t = m∆y−∆t.",3.4. Separating Positive and Negative Contributions,[0],[0]
We present the rules for assigning contribution scores for each neuron to its immediate inputs.,3.5. Rules for Assigning Contribution Scores,[0],[0]
"In conjunction with the chain rule for multipliers (Section 3.2), these rules can be used to find the contributions of any input (not just the immediate inputs) to a target output via backpropagation.",3.5. Rules for Assigning Contribution Scores,[0],[0]
This applies to Dense and Convolutional layers (excluding nonlinearities).,3.5.1. THE LINEAR RULE,[0],[0]
Let y be a linear function of its inputs xi such that y = b + ∑ i wixi.,3.5.1. THE LINEAR RULE,[0],[0]
We have ∆y = ∑ i wi∆xi.,3.5.1. THE LINEAR RULE,[0],[0]
"We define the positive and negative parts of ∆y as:
∆y+ = ∑ i 1{wi∆xi >",3.5.1. THE LINEAR RULE,[0],[0]
"0}wi∆xi
= ∑ i 1{wi∆xi > 0}wi(∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
+ ∆x,3.5.1. THE LINEAR RULE,[0],[0]
"− i )
",3.5.1. THE LINEAR RULE,[0],[0]
∆y−,3.5.1. THE LINEAR RULE,[0],[0]
"= ∑ i 1{wi∆xi < 0}wi∆xi
= ∑ i 1{wi∆xi < 0}wi(∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
+ ∆x,3.5.1. THE LINEAR RULE,[0],[0]
"− i )
Which leads to the following choice for the contributions: C∆x+i ∆y+
= 1{wi∆xi > 0}wi∆x+i C∆x−i ∆y+
= 1{wi∆xi > 0}wi∆x−i",3.5.1. THE LINEAR RULE,[0],[0]
"C∆x+i ∆y−
",3.5.1. THE LINEAR RULE,[0],[0]
= 1{wi∆xi < 0}wi∆x+i C∆x−i,3.5.1. THE LINEAR RULE,[0],[0]
"∆y− = 1{wi∆xi < 0}wi∆x−i
We can then find multipliers using the definition in Section 3.2.1, which givesm∆x+i ∆y+ = m∆x−i ∆y+",3.5.1. THE LINEAR RULE,[0],[0]
= 1{wi∆xi > 0}wi and m∆x+i ∆y− = m∆x−i,3.5.1. THE LINEAR RULE,[0],[0]
∆y−,3.5.1. THE LINEAR RULE,[0],[0]
"= 1{wi∆xi < 0}wi.
",3.5.1. THE LINEAR RULE,[0],[0]
What about when ∆xi = 0?,3.5.1. THE LINEAR RULE,[0],[0]
"While setting multipliers to 0 in this case would be consistent with summation-to-delta, it is possible that ∆x+i and ∆x",3.5.1. THE LINEAR RULE,[0],[0]
"− i are nonzero (and cancel each other out), in which case setting the multiplier to 0 would fail to propagate importance to them.",3.5.1. THE LINEAR RULE,[0],[0]
"To avoid this, we set m∆x+i ∆y+ = m∆x+i",3.5.1. THE LINEAR RULE,[0],[0]
∆y− = 0.5wi when ∆xi is 0 (similarly for ∆x−).,3.5.1. THE LINEAR RULE,[0],[0]
See Appendix B for how to compute these multipliers using standard neural network ops.,3.5.1. THE LINEAR RULE,[0],[0]
"This rule applies to nonlinear transformations that take a single input, such as the ReLU, tanh or sigmoid operations.",3.5.2. THE RESCALE RULE,[0],[0]
Let neuron y be a nonlinear transformation of its input x such that y = f(x).,3.5.2. THE RESCALE RULE,[0],[0]
"Because y has only one input, we have by summation-to-delta that C∆x∆y = ∆y, and consequently m∆x∆y = ∆y∆x .",3.5.2. THE RESCALE RULE,[0],[0]
"For the Rescale rule, we set ∆y + and ∆y− proportional to ∆x+ and ∆x− as follows:
∆y+ = ∆y
∆x ∆x+ = C∆x+∆y+
∆y− = ∆y
∆x",3.5.2. THE RESCALE RULE,[0],[0]
"∆x− = C∆x−∆y−
Based on this, we get:
m∆x+∆y+ = m∆x−∆y− = m∆x∆y = ∆y
∆x
In the case where x→ x0, we have ∆x→ 0 and ∆y → 0.",3.5.2. THE RESCALE RULE,[0],[0]
"The definition of the multiplier approaches the derivative, i.e. m∆x∆y → dydx , where the dy dx is evaluated at x = x
0.",3.5.2. THE RESCALE RULE,[0],[0]
"We can thus use the gradient instead of the multiplier when x is close to its reference to avoid numerical instability issues caused by having a small denominator.
",3.5.2. THE RESCALE RULE,[0],[0]
Note that the Rescale rule addresses both the saturation and the thresholding problems illustrated in Fig. 1 and Fig. 2.,3.5.2. THE RESCALE RULE,[0],[0]
"In the case of Fig. 1, if i01 = i02 = 0, then at i1 + i2 > 1 we have ∆h = −1 and ∆y = 1, giving
m∆h∆y = ∆y ∆h = −1",3.5.2. THE RESCALE RULE,[0],[0]
"even though dy dh = 0 (in other words, using difference-from-reference allows information to flow even when the gradient is zero).",3.5.2. THE RESCALE RULE,[0],[0]
"In the case of Fig. 2, assuming x0 = y0 = 0, at x = 10 + we have ∆y = , giving m∆x∆y = 10+ and C∆x∆y = ∆x ×m∆x∆y = .",3.5.2. THE RESCALE RULE,[0],[0]
"By contrast, gradient×input assigns a contribution of 10+ to x and−10 to the bias term (DeepLIFT never assigns importance to bias terms).
",3.5.2. THE RESCALE RULE,[0],[0]
"As revealed in previous work (Lundberg & Lee, 2016), there is a connection between DeepLIFT and Shapely values.",3.5.2. THE RESCALE RULE,[0],[0]
"Briefly, the Shapely values measure the average marginal effect of including an input over all possible orderings in which inputs can be included.",3.5.2. THE RESCALE RULE,[0],[0]
"If we define “including” an input as setting it to its actual value instead of its reference value, DeepLIFT can be thought of as a fast approximation of the Shapely values.",3.5.2. THE RESCALE RULE,[0],[0]
"At the time, Lundberg & Lee cited a preprint of DeepLIFT which described only the Linear and Rescale rules with no separate treatment of positive and negative contributions.",3.5.2. THE RESCALE RULE,[0],[0]
"SHAPELY VALUES: THE REVEALCANCEL RULE
While the Rescale rule improves upon simply using gradients, there are still some situations where it can provide misleading results.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Consider the min(i1, i2) operation depicted in Fig. 3, with reference values of i1 = 0 and i2 = 0.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Using the Rescale rule, all importance would be assigned either to i1 or to i2 (whichever is smaller).",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This can obscure the fact that both inputs are relevant for the min operation.
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"To understand why this occurs, consider the case when i1 > i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"We have h1 = (i1 − i2) > 0 and h2 = max(0, h1) = h1.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"By the Linear rule, we calculate that C∆i1∆h1 = i1 and C∆i2∆h1 = −i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"By the Rescale rule, the multiplier m∆h1∆h2 is ∆h2 ∆h1
= 1, and thus C∆i1∆h2 = m∆h1∆h2C∆i1∆h1 = i1 and C∆i2∆h2 = m∆h1∆h2C∆i2∆h1 = −i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
The total contribution of i1 to the output o becomes (i1 − C∆i1∆h2) =,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"(i1 − i1) = 0, and the total contribution of i2 to o is −C∆i2∆h2 = i2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This calculation is misleading as it discounts the fact that C∆i2∆h2 would be 0 if i1 were 0 - in other words, it ignores a dependency induced between i1 and i2 that comes from i2 canceling out i1 in the nonlinear neuron h2.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
A similar failure occurs when i1 < i2; the Rescale rule results in C∆i1∆o = i1 and C∆i2∆o = 0.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Note that gradients, gradient×input, Guided Backpropagation and integrated gradients would also assign all importance to either i1 or i2, because for any given input the gradient is zero for one of i1 or i2 (see Appendix C for a detailed calculation).
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
One way to address this is by treating the positive and negative contributions separately.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
We again consider the nonlinear neuron y = f(x).,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Instead of assuming that ∆y+ and ∆y− are proportional to ∆x+ and ∆x− and that
m∆x+∆y+ = m∆x−∆y− = m∆x∆y (as is done for the Rescale rule), we define them as follows:
∆y+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"= 1
2
( f(x0 + ∆x+)− f(x0) )",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"+ 1
2
( f(x0 + ∆x− + ∆x+)−",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
f(x0 + ∆x−) ),3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"∆y− = 1
2
( f(x0 + ∆x−)− f(x0) )",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"+ 1
2
( f(x0 + ∆x+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
+ ∆x−)−,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
f(x0 + ∆x+) ),3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"m∆x+∆y+ = C∆x+y+
∆x+ =
∆y+ ∆x+",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"; m∆x−∆y− = ∆y− ∆x−
In other words, we set ∆y+ to the average impact of ∆x+ after no terms have been added and after ∆x− has been added, and we set ∆y− to the average impact of ∆x− after no terms have been added and after ∆x+ has been added.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"This can be thought of as the Shapely values of ∆x+ and ∆x− contributing to y.
By considering the impact of the positive terms in the absence of negative terms, and the impact of negative terms in the absence of positive terms, we alleviate some of the issues that arise from positive and negative terms canceling each other out.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"In the case of Fig. 3, RevealCancel would assign a contribution of 0.5 min(i1, i2) to both inputs (see Appendix C for a detailed calculation).
",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"While the RevealCancel rule also avoids the saturation and thresholding pitfalls illustrated in Fig. 1 and Fig. 2, there are some circumstances where we might prefer to use the Rescale rule.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"Specifically, consider a thresholded ReLU where ∆y > 0",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
iff ∆x,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
≥ b.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"If ∆x < b merely indicates noise, we would want to assign contributions of 0 to both ∆x+ and ∆x− (as done by the Rescale rule) to mitigate the noise.",3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
RevealCancel may assign nonzero contributions by considering ∆x+ in the absence of ∆x− and vice versa.,3.5.3. AN IMPROVED APPROXIMATION OF THE,[0],[0]
"In the case of softmax or sigmoid outputs, we may prefer to compute contributions to the linear layer preceding the final nonlinearity rather than the final nonlinearity itself.",3.6. Choice of Target Layer,[0],[0]
"This would be to avoid an attentuation caused by the
summation-to-delta property described in Section 3.1.",3.6. Choice of Target Layer,[0],[0]
"For example, consider a sigmoid output o = σ(y), where y is the logit of the sigmoid function.",3.6. Choice of Target Layer,[0],[0]
Assume y,3.6. Choice of Target Layer,[0],[0]
"= x1 + x2, where x01 = x 0 2 = 0.",3.6. Choice of Target Layer,[0],[0]
"When x1 = 50 and x2 = 0, the output o saturates at very close to 1 and the contributions of x1 and x2 are 0.5 and 0 respectively.",3.6. Choice of Target Layer,[0],[0]
"However, when x1 = 100 and x2 = 100, the output o is still very close to 0, but the contributions of x1 and x2 are now both 0.25.",3.6. Choice of Target Layer,[0],[0]
This can be misleading when comparing scores across different inputs because a stronger contribution to the logit would not always translate into a higher DeepLIFT score.,3.6. Choice of Target Layer,[0],[0]
"To avoid this, we compute contributions to y rather than o.
Adjustments for Softmax Layers
If we compute contributions to the linear layer preceding the softmax rather than the softmax output, an issue that could arise is that the final softmax output involves a normalization over all classes, but the linear layer before the softmax does not.",3.6. Choice of Target Layer,[0],[0]
"To address this, we can normalize the contributions to the linear layer by subtracting the mean contribution to all classes.",3.6. Choice of Target Layer,[0],[0]
"Formally, if n is the number of classes, C∆x∆ci represents the unnormalized contribution to class ci in the linear layer and C ′∆x∆ci represents the normalized contribution, we have:
C ′∆x∆ci = C∆x∆ci − 1
n n∑ j=1 C∆x∆cj (5)
As a justification for this normalization, we note that subtracting a fixed value from all the inputs to the softmax leaves the output of the softmax unchanged.",3.6. Choice of Target Layer,[0],[0]
"We train a convolutional neural network on MNIST (LeCun et al., 1999) using Keras (Chollet, 2015) to perform digit classification and obtain 99.2% test-set accuracy.",4.1. Digit Classification (MNIST),[0],[0]
"The architecture consists of two convolutional layers, followed by a fully connected layer, followed by the softmax output layer (see Appendix D for full details on model architecture and training).",4.1. Digit Classification (MNIST),[0],[0]
"We used convolutions with stride > 1 instead of pooling layers, which did not result in a drop in performance as is consistent with previous work (Springenberg et al., 2014).",4.1. Digit Classification (MNIST),[0],[0]
"For DeepLIFT and integrated gradients, we used a reference input of all zeros.
",4.1. Digit Classification (MNIST),[0],[0]
"To evaluate importance scores obtained by different methods, we design the following task: given an image that originally belongs to class co, we identify which pixels to erase to convert the image to some target class ct.",4.1. Digit Classification (MNIST),[0],[0]
"We do this by finding Sxidiff = Sxico − Sxict (where Sxic is the score for pixel xi and class c) and erasing up to 157 pixels (20% of the image) ranked in descending order of Sxidiff for which
Sxidiff > 0.",4.1. Digit Classification (MNIST),[0],[0]
"We then evaluate the change in the log-odds score between classes co and ct for the original image and the image with the pixels erased.
",4.1. Digit Classification (MNIST),[0],[0]
"As shown in Fig. 4, DeepLIFT with the RevealCancel rule outperformed the other backpropagation-based methods.",4.1. Digit Classification (MNIST),[0],[0]
"Integrated gradients (Section 2.2.3) computed numerically over either 5 or 10 intervals produced results comparable to each other, suggesting that adding more intervals would not change the result.",4.1. Digit Classification (MNIST),[0],[0]
"Integrated gradients also performed comparably to gradient*input, suggesting that saturation and thresholding failure modes are not common on MNIST data.",4.1. Digit Classification (MNIST),[0],[0]
"Guided Backprop discards negative gradients during backpropagation, perhaps explaining its poor performance at discriminating between classes.",4.1. Digit Classification (MNIST),[0],[0]
We also explored using the Rescale rule instead of RevealCancel on various layers and found that it degraded performance (Appendix E).,4.1. Digit Classification (MNIST),[0],[0]
"Next, we compared the importance scoring methods when applied to classification tasks on DNA sequence inputs (strings over the alphabet {A,C,G,T}).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
The human genome has millions of DNA sequence elements ( 200-1000 in length) containing specific combinations of short functional words to which regulatory proteins (RPs) bind to regulate gene activity.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Each RP (e.g. GATA1) has binding affinity to specific collections of short DNA words (motifs) (e.g. GATAA and GATTA).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
A key problem in computational genomics is the discovery of motifs in regulatory DNA elements that give rise to distinct molecular signatures (labels) which can be measured experimentally.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Here, in order to benchmark DeepLIFT and competing methods to uncover predictive patterns in DNA sequences, we design a simple simulation that captures the essence of the motif discovery problem described above.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Background DNA sequences of length 200 were generated by sampling the letters ACGT at each position with
probabilities 0.3, 0.2, 0.2 and 0.3 respectively.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Motif instances were randomly sampled from previously known probabilistic motif models (See Appendix F) of two RPs named GATA1 and TAL1 (Fig.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"6a)(Kheradpour & Kellis, 2014), and 0-3 instances of a given motif were inserted at random non-overlapping positions in the DNA sequences.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We trained a multi-task neural network with two convolutional layers, global average pooling and one fullyconnected layer on 3 binary classification tasks.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Positive labeled sequences in task 1 represented “both GATA1 and TAL1 present”, task 2 represented “GATA1 present” and in task 3 represented “TAL1 present”.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"14 of sequences had both GATA1 and TAL1 motifs (labeled 111), 14 had only GATA1 (labeled 010), 14 had only TAL1 (labeled 001), and 1 4 had no motifs (labeled 000).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Details of the simulation, network architecture and predictive performance are given in Appendix F. For DeepLIFT and integrated gradients, we used a reference input that had the expected frequencies of ACGT at each position (i.e. we set the ACGT channel axis to 0.3, 0.2, 0.2, 0.3; see Appendix J for results using
shuffled sequences as a reference).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"For fair comparison, this reference was also used for gradient×input and Guided Backprop×input (“input” is more accurately called ∆input where ∆ measured w.r.t the reference).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"For DNA sequence inputs, we found Guided Backprop×input performed better than vanilla Guided Backprop; thus, we used the former.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Given a particular subsequence, it is possible to compute the log-odds score that the subsequence was sampled from a particular motif vs. originating from the background distribution of ACGT.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"To evaluate different importancescoring methods, we found the top 5 matches (as ranked by their log-odds score) to each motif for each sequence from the test set, as well as the total importance allocated to the match by different importance-scoring methods for each task.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
The results are shown in Fig. 5 (for TAL1) and Appendix E (for GATA1).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Ideally, we expect an importance scoring method to show the following properties: (1) high scores for TAL1 motifs on task 2 and (2) low scores for TAL1 on task 1, with (3) higher scores corresponding to stronger log-odds matches; analogous pattern for GATA1 motifs (high for task 1, low for task 2); (4) high scores for both TAL1 and GATA1 motifs for task 0, with (5) higher scores on sequences containing both kinds of motifs vs. sequences containing only one kind (revealing cooperativity; corresponds to red dots lying above green dots in Fig. 5).
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
We observe Guided Backprop×input fails (2) by assigning positive importance to TAL1 on task 1 (see Appendix H for an example sequence).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
It fails property (4) by failing to identify cooperativity in task 0,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
(red dots overlay green dots).,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Both Guided Backprop×input and gradient×input show suboptimal behavior regarding property (3), in that there is a sudden increase in importance when the log-odds score is around 7, but little differentiation at higher logodds scores (by contrast, the other methods show a more gradual increase).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"As a result, Guided Backprop×input and gradient×input can assign unduly high importance to weak motif matches (Fig. 6).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
This is a practical consequence of the thresholding problem from Fig. 2.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"The large discontinuous jumps in gradient also result in inflated scores (note the scale on the y-axes) relative to other methods.
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We explored three versions of DeepLIFT: Rescale at all nonlinearities (DeepLIFT-Rescale), RevealCancel at all nonlinearities (DeepLIFT-RevealCancel), and Rescale at convolutional layers with RevealCancel at the fully connected layer (DeepLIFT-fc-RC-conv-RS).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"In contrast to the results on MNIST, we found that DeepLIFT-fc-RC-convRS reduced noise relative to pure RevealCancel.",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We think this is because of the noise-suppression property discussed in Section 3.5.3; if the convolutional layers act like motif detectors, the input to convolutional neurons that do not fire may just represent noise and importance should not be propagated to them (see Fig. 6 for an example sequence).
",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"Gradient×inp, integrated gradients and DeepLIFT-Rescale occasionally miss relevance of TAL1 for Task 0",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"(Fig. 5b), which is corrected by using RevealCancel on the fully connected layer (see example sequence in Fig. 6).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
Note that the RevealCancel scores seem to be tiered.,4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"As illustrated in Appendix I, this is related to having multiple instances of a given motif in a sequence (eg: when there are multiple TAL1 motifs, the importance assigned to the presence of TAL1 is distributed across all the motifs).",4.2. Classifying Regulatory DNA (Genomics),[0],[0]
"We have presented DeepLIFT, a novel approach for computing importance scores based on explaining the difference of the output from some ‘reference’ output in terms of differences of the inputs from their ‘reference’ inputs.",5. Conclusion,[0],[0]
"Using the difference-from-reference allows information to propagate even when the gradient is zero (Fig. 1), which could prove especially useful in Recurrent Neural Networks where saturating activations like sigmoid or tanh are popular.",5. Conclusion,[0],[0]
DeepLIFT avoids placing potentially misleading importance on bias terms (in contrast to gradient*input - see Fig. 2).,5. Conclusion,[0],[0]
"By allowing separate treatment of positive and negative contributions, the DeepLIFT-RevealCancel rule can identify dependencies missed by other methods (Fig. 3).",5. Conclusion,[0],[0]
"Open questions include how to apply DeepLIFT to RNNs, how to compute a good reference empirically from the data, and how best to propagate importance through ‘max’ operations (as in Maxout or Maxpooling neurons) beyond simply using the gradients.",5. Conclusion,[0],[0]
We thank Anna Shcherbina for early experiments applying DeepLIFT to image data and beta-testing.,6. Acknowledgements,[0],[0]
AS is supported by a Howard Hughes Medical Institute International Student Research Fellowship and a Bio-X Bowes Fellowship.,7. Funding,[0],[0]
PG is supported by a Bio-X Stanford Interdisciplinary Graduate Fellowship.,7. Funding,[0],[0]
AK was supported by NIH grants DP2-GM-123485 and 1R01ES025009-02.,7. Funding,[0],[0]
AS & PG conceptualized DeepLIFT.,8. Author Contributions,[0],[0]
AS implemented DeepLIFT.,8. Author Contributions,[0],[0]
AS ran experiments on MNIST.,8. Author Contributions,[0],[0]
AS & PG ran experiments on genomic data.,8. Author Contributions,[0],[0]
AK provided guidance and feedback.,8. Author Contributions,[0],[0]
"AS, PG and AK wrote the manuscript.",8. Author Contributions,[0],[0]
The purported “black box” nature of neural networks is a barrier to adoption in applications where interpretability is essential.,abstractText,[0],[0]
"Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input.",abstractText,[0],[0]
DeepLIFT compares the activation of each neuron to its ‘reference activation’ and assigns contribution scores according to the difference.,abstractText,[0],[0]
"By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches.",abstractText,[0],[0]
Scores can be computed efficiently in a single backward pass.,abstractText,[0],[0]
"We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods.",abstractText,[0],[0]
"Video tutorial: http://goo.gl/ qKb7pL, code: http://goo.gl/RM8jvH.",abstractText,[0],[0]
Learning Important Features Through Propagating Activation Differences,title,[0],[0]
"The Partially Observable Markov Decision Processes (POMDP) [8] is a general model for sequential decision making in stochastic and partially observable environments, which are ubiquitous in real-world problems.",1 Introduction,[0],[0]
A key shortcoming of POMDP methods is the assumption that the dynamics of the environment are known a priori.,1 Introduction,[0],[0]
"In real-world applications, however, it may be impossible to obtain a complete and accurate description of the system.",1 Introduction,[0],[0]
"Instead, we may have uncertain prior knowledge about the model.",1 Introduction,[0],[0]
"When lacking a model, a prior can be incorporated into the POMDP problem in a principled way, as demonstrated by the Bayes-Adaptive POMDP framework [13].
",1 Introduction,[0],[0]
"The BA-POMDP framework provides a Bayesian approach to decision making by maintaining a probability distribution over possible models as the agent acts in an online reinforcement learning setting [5, 19].",1 Introduction,[0],[0]
This method casts the Bayesian reinforcement learning problem into a POMDP planning problem where the hidden model of the environment is part of the state space.,1 Introduction,[0],[0]
"Unfortunately, this planning problem becomes very large, with a countably infinite state space over all possible models, and as such, current solution methods are not scalable or perform poorly [13].
",1 Introduction,[0],[0]
Online and sample-based planning has shown promising performance on non-trivial POMDP problems [12].,1 Introduction,[0],[0]
"Online methods reduce the complexity by considering the relevant (i.e., reachable) states only, and sample-based approaches tackle the complexity issues through approximations in the form of simulated interactions with the environment.",1 Introduction,[0],[0]
"Here we modify one of those methods, Partial Observable Monte-Carlo Planning (POMCP)",1 Introduction,[0],[0]
"[15] and extend it to the Bayes-Adaptive case, leading to a novel approach: BA-POMCP.
",1 Introduction,[0],[0]
"In particular, we improve the sampling approach by exploiting the structure of the BA-POMDP resulting in root sampling and expected models methods.",1 Introduction,[0],[0]
"We also present an approach for more efficient model representation, which we call linking states.",1 Introduction,[0],[0]
"Lastly, we prove the correctness of our improvements, showing that they converge to the true BA-POMDP solution.",1 Introduction,[0],[0]
"As a result, we present methods that significantly improve the scalability of learning in BAPOMDPs, making them practical for larger problems.",1 Introduction,[0],[0]
"First, we discuss POMDPs and BA-POMDPs in respectively Section 2.1 and 2.2.",2 Background,[0],[0]
"Formally, a POMDP is described by a tuple (S, A, Z, D, R, γ, h), where S is the set of states of the environment; A is the set of actions; Z is the set of observations; D is the ‘dynamics function’ that describes the dynamics of the ∗This is an extended version of a paper that was published at ICML’2017.
",2.1 POMDPs,[0],[0]
"ar X
iv :1
80 6.
",2.1 POMDPs,[0],[0]
"05 63
1v 1
[ cs
.A",2.1 POMDPs,[0],[0]
"I]
1 4
Ju n
20 18
system in the form of transition probabilities D(s′,z|s,a);1 R is the immediate reward function R(s, a) that describes the reward of selecting a in s; γ ∈",2.1 POMDPs,[0],[0]
"[0,1) is the discount factor; and h is the horizon of an episode in the system.
",2.1 POMDPs,[0],[0]
"The goal of the agent in a POMDP is to maximize the expected cumulative (discounted) reward, also called the expected return.",2.1 POMDPs,[0],[0]
"The agent has no direct access to the system’s state, so it can only rely on the action-observation history ht = 〈a0,z1, . . .",2.1 POMDPs,[0],[0]
",at−1,zt〉 up to the current step t. It can use this history to maintain a probability distribution over the state, also called a belief, b(s).",2.1 POMDPs,[0],[0]
"A solution to a POMDP is then a mapping from a belief b to an action a, which is called a policy π.",2.1 POMDPs,[0],[0]
"Solution methods aim to find an optimal policy, a mapping from a belief to an action with the highest possible expected return.
",2.1 POMDPs,[0],[0]
The agent maintains its belief during execution through belief updates.,2.1 POMDPs,[0],[0]
"A belief update calculates the posterior probability of the state s′ given the previous belief over s and action-observation pair 〈a,z〉: b′(s) = P (s′|b(s), a, z).",2.1 POMDPs,[0],[0]
This operation is infeasible for large spaces because it enumerates over the entire state space.,2.1 POMDPs,[0],[0]
A common approximation method is to represent the belief with a (unweighted) particle filter [17].,2.1 POMDPs,[0],[0]
A particle filter is a collection of K particles (states).,2.1 POMDPs,[0],[0]
"Each particle represents a probability of 1K ; if a specific state x occurs n times in a particle filter, then P (x) = nK .",2.1 POMDPs,[0],[0]
"The precision of the filter is determined by the number of particles K. To update such a belief after execution of action a and observation z, a standard approach is to utilize rejection sampling: the agent repeatedly samples a state s from its belief, then simulates the execution of a on s through D, and receives a (simulated) new state s′sim and observation zsim.",2.1 POMDPs,[0],[0]
"s
′ is added to the new belief only when zsim equals z, and rejected otherwise.",2.1 POMDPs,[0],[0]
"This process repeats until the new belief contains K particles.
",2.1 POMDPs,[0],[0]
Partially Observable Monte-Carlo Planning (POMCP),2.1 POMDPs,[0],[0]
"[15], is a scalable method which extends Monte Carlo tree search (MCTS) to solve POMDPs.",2.1 POMDPs,[0],[0]
POMCP is one of the leading algorithms for solving general POMDPs.,2.1 POMDPs,[0],[0]
"At each time step, the algorithm performs online planning by incrementally building a lookahead tree that contains Q(h,a), where h is the action-observation history-path to reach that node.",2.1 POMDPs,[0],[0]
It samples hidden states s at the root node (called ‘root sampling’) and uses that state to sample a trajectory that first traverses the lookahead tree and then performs a (random) rollout.,2.1 POMDPs,[0],[0]
The return of this trajectory is used to update the statistics for all visited nodes.,2.1 POMDPs,[0],[0]
"These statistics include the number of times an action has been taken at a history (N(h,a)) and estimated value of being in that node (Q(h,a)), based on an average over the returns.
",2.1 POMDPs,[0],[0]
"Because this lookahead tree can be very large, the search is directed to the relevant parts by selecting the actions inside the tree that maximize the ‘upper confidence bounds’",2.1 POMDPs,[0],[0]
"[2]: U(h,a) = Q(h, a) + c √ log(N(h) + 1)/N(h,a).",2.1 POMDPs,[0],[0]
"Here, N(h) is the number of times the history has been visited.",2.1 POMDPs,[0],[0]
"At the end of each simulation, the discounted accumulated return is used to update the estimated value of all the nodes in the tree that have been visited during that simulation.",2.1 POMDPs,[0],[0]
"POMCP terminates after some criteria has been met, typically defined by a maximum number of simulations or allocated time.",2.1 POMDPs,[0],[0]
"The agent then picks the action with the highest estimated value (maxaQ(b,a)).",2.1 POMDPs,[0],[0]
POMCP can be shown to converge to an -optimal value function.,2.1 POMDPs,[0],[0]
"Moreover, the method has demonstrated good performance in large domains with a limited number of simulations.",2.1 POMDPs,[0],[0]
The extension of POMCP that is used in this work is discussed in Section 3.,2.1 POMDPs,[0],[0]
"Most research concerning POMDPs has considered the task of planning: given a full specification of the model, determine an optimal policy (e.g., [8, 14]).",2.2 BA-POMDPs,[0],[0]
"However, in many real-world applications, the model is not (perfectly) known in advance, which means that the agent has to learn about its environment during execution.",2.2 BA-POMDPs,[0],[0]
This is the task considered in reinforcement learning (RL),2.2 BA-POMDPs,[0],[0]
"[16].
",2.2 BA-POMDPs,[0],[0]
"A fundamental RL problem is the difficulty of deciding whether to select actions in order to learn a better model of the environment, or to exploit the current knowledge about the rewards and effects of actions.",2.2 BA-POMDPs,[0],[0]
"In recent years, Bayesian RL methods have become popular because they can provide a principled solution to this exploration/exploitation tradeoff",2.2 BA-POMDPs,[0],[0]
"[19, 5, 6, 10, 18].
",2.2 BA-POMDPs,[0],[0]
"In particular, we consider the framework of Bayes-Adaptive POMDPs [11, 13].",2.2 BA-POMDPs,[0],[0]
BA-POMDPs use Dirichlet distributions to model uncertainty over transitions and observations2 (typically assuming the reward function is chosen by the designer and is known).,2.2 BA-POMDPs,[0],[0]
"In particular, if the agent could observe both states and observations, it could maintain a vector χ with the counts of the occurrences for all 〈s, a, s′, z〉 tuples.",2.2 BA-POMDPs,[0],[0]
"We write χs′zsa for the number of times that 〈s,a〉 is followed by 〈s′,z〉.
",2.2 BA-POMDPs,[0],[0]
"While the agent cannot observe the states and has uncertainty about the actual count vector, this uncertainty can be represented using regular POMDP formalisms.",2.2 BA-POMDPs,[0],[0]
"That is, the count vector is included as part of the hidden state of a specific POMDP, called a BA-POMDP.",2.2 BA-POMDPs,[0],[0]
"Formally, a BA-POMDP is a tuple 〈S̄, A, D̄, R̄, Z, γ, h〉 with some modified
1 This formulation allows for easier notation and generalizes the typical formulation with separate transition T and observation functions",2.2 BA-POMDPs,[0],[0]
"O: D = 〈T,O〉.",2.2 BA-POMDPs,[0],[0]
"In our experiments, we do employ this typical factorization.
2 [11, 13] follow the standard T & O POMDP representations, but we use our combined D formalism.
",2.2 BA-POMDPs,[0],[0]
"Algorithm 1 BA-POMCP(b̄,num sims)
1: //b̄ is an augmented belief (e.g., particle filter) 2: h0 ← () .",2.2 BA-POMDPs,[0],[0]
"The empty history (i.e., now) 3: for i← 1 . . .",2.2 BA-POMDPs,[0],[0]
"num sims do 4: //First, we root sample an (augmented) state: 5: s̄← SAMPLE(b̄) .",2.2 BA-POMDPs,[0],[0]
"reference to a particle 6: s̄′ ← COPY(s̄) 7: SIMULATE(s̄′, 0, h0) 8: end for 9: a← GREEDYACTIONSELECTION(h0)
10: return a
components in comparison to the POMDP.",2.2 BA-POMDPs,[0],[0]
"While the observation and action space remain unchanged, the state (space) of the BA-POMDP now includes Dirichlet parameters: s̄ = 〈s, χ〉, which we will refer to as augmented states.",2.2 BA-POMDPs,[0],[0]
"The reward model remains the same, since it is assumed to be known, R̄(〈s′,χ′),a) =",2.2 BA-POMDPs,[0],[0]
"R(s,a).",2.2 BA-POMDPs,[0],[0]
"The dynamics functions, D̄, however, is described in terms of the counts in s̄, and is defined as follows
Dχ(s ′,z|s, a) , E[D(s′,z|s, a)|χ] = χ s′z sa∑
s′z χ s′z sa
.",2.2 BA-POMDPs,[0],[0]
"(1)
These expectations can now be used to define the transitions for the BA-POMDP.",2.2 BA-POMDPs,[0],[0]
If we let δs ′z,2.2 BA-POMDPs,[0],[0]
"sa denote a vector of the length of χ containing all zeros except for the position corresponding to 〈s,a,s′,z〉 (where it has a one), and if we let Ia,b denote the Kronecker delta that indicates (is 1 when) a = b, then we can define D̄ as D̄(s′,χ′,z|s,χ, a) = Dχ(s
′,z|s, a)Iχ′,χ+δs′zsa .",2.2 BA-POMDPs,[0],[0]
"Remember that these counts are not observed by the agent, since that would require observations of the state.",2.2 BA-POMDPs,[0],[0]
The agent can only maintain belief over these count vectors.,2.2 BA-POMDPs,[0],[0]
"Still, when interacting with the environment, the ratio of the true—but unknown—count vectors will converge to coincide with the true transition and observation probabilities in expectation.",2.2 BA-POMDPs,[0],[0]
"It is important to realize, however, that this convergence of count vector ratios does not directly imply learnability by the agent: even though the ratio of the count vectors of the true hidden state will converge, the agent’s belief over count vectors might not.
",2.2 BA-POMDPs,[0],[0]
BA-POMDPs are infinite state POMDP models and thus extremely difficult to solve.,2.2 BA-POMDPs,[0],[0]
"Ross et al. [13] introduced a technique to convert such models to finite models, but these are still very large.",2.2 BA-POMDPs,[0],[0]
"Therefore, Ross et al. propose a simple lookahead planner to solve BA-POMDPs in an online manner.",2.2 BA-POMDPs,[0],[0]
"This approach approximates the expected values associated with each action at the belief by applying a lookahead search of depth d. This method will function as the comparison baseline in our experiments, as no other BA-POMDP solution methods have been proposed.",2.2 BA-POMDPs,[0],[0]
"Powerful methods, such as POMCP [15], have significantly improved the scalability of POMDP solution methods.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"At the same time the most practical solution method for BA-POMDPs, the aforementioned lookahead algorithm, is quite limited in dealing with larger problems.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"POMDP methods have rarely been applied to BA-POMDPs [1], and no systematic investigation of their performance has been conducted.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In this paper, we aim to address this void, by extending POMCP to BA-POMDPs, in an algorithm that we refer to as BA-POMCP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Moreover, we propose a number of novel adaptations to BA-POMCP that exploit the structure of the BA-POMDP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In this section, we first lay out the basic adaptation of POMCP to BA-POMDPs and then describe the proposed modifications that improve its efficiency.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"BA-POMCP BA-POMCP, just like POMCP, constructs a lookahead tree through simulated experiences (Algorithm 1).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In BA-POMDPs, however, the dynamics of the system are inaccessible during simulations, and the belief is a probability distribution over augmented states.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"BA-POMCP, as a result, must sample augmented states from the belief b̄, and use copies of those states (s̄ = 〈s,χ〉) for each simulation (Algorithm 2).",3 BA-POMDPs via Sample-based Planning,[0],[0]
We will refer to this as root sampling of the state (line 6).,3 BA-POMDPs via Sample-based Planning,[0],[0]
"The copy is necessary, as otherwise the STEP function in Algorithm 2 would alter the belief b̄.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"It is also expensive, for χ grows with the state, action and observation space, to |S|2 × |A| × |Ω| parameters.",3 BA-POMDPs via Sample-based Planning,[0],[0]
3,3 BA-POMDPs via Sample-based Planning,[0],[0]
"In practice, this operation becomes a bottleneck to the runtime of BA-POMCP in larger domains.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
3It is |S|2 × |A|+ |S| ×,3 BA-POMDPs via Sample-based Planning,[0],[0]
"|A| × |Ω| when assuming D is factored in T & O
Algorithm 2 SIMULATE(s̄, d, h) 1: if ISTERMINAL(h) ||",3 BA-POMDPs via Sample-based Planning,[0],[0]
"d == max depth then 2: return 0 3: end if 4: //Action selection uses statistics stored at node h: 5: a← UCBACTIONSELECTION(h) 6: R← R(s̄,a) 7: z ← STEP(s̄, a) //modifies",3 BA-POMDPs via Sample-based Planning,[0],[0]
"s̄ to sampled next state 8: h′ ← (h,a,z) 9: if h′ ∈ Tree then 10: r ← R+ γ SIMULATE(s̄, d+ 1, h′) 11: else 12: CONSTRUCTNODE(h′) //Initializes statistics 13:",3 BA-POMDPs via Sample-based Planning,[0],[0]
"r ← R+ γ ROLLOUT(s̄, d+ 1, h′) 14: end if 15: //Update statistics: 16: N(h,a)← N(h,a) + 1 17: Q(h,a)← N(h,a)−1
N(h,a) Q(h,a) + 1 N(h,a) r
18: return r
Algorithm 3 BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: Dsa ∼ χsa 2: 〈s′,z〉 ∼ Dsa 3: //In place updating of s̄ = 〈s, χ〉 4: χs ′z",3 BA-POMDPs via Sample-based Planning,[0],[0]
sa ← χs ′z,3 BA-POMDPs via Sample-based Planning,[0],[0]
"sa + 1
5: s← s′ 6: return z
To apply POMCP on BA-POMDPs, where the dynamics are unknown, we modify the STEP function, proposing several variants.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The most straightforward one, BA-POMCP-STEP is employed in what we refer to as ‘BA-POMCP’.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This method, shown in Algorithm 3, is similar to BA-MCP [7]: essentially, it samples a dynamic model Dsa which specifies probabilities Pr(s′,z|s,a) and subsequently samples an actual next state and observation from that distribution.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Note that the underlying states and observations are all represented simply as an index, and hence the assignment on line 5 is not problematic.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"However, the cost of the model sampling operation in line 1 is.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Root Sampling of the Model BA-MCP [7] addresses the fully observable BRL problem by using POMCP on an augmented state s̄ = 〈s, T 〉, consisting of the observable state, as well as the hidden true transition function T .",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Application of POMCP’s root sampling of state in this case leads to ‘root sampling of a transition function’: Since the true transition model T does not change during the simulation, one is sampled at the root and used during the entire simulation.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In the BA-POMCP case, root sampling of a state s̄ = 〈s, χ〉 does not lead to a same interpretation: no model, but counts are root sampled and they do change over time.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We use this as inspiration to introduce a similar, but clearly different, (since this is not root sampling of state) technique called root sampling of the model (which we will refer to as just ‘root sampling’).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The idea is simple: every time we root sample a state s̄ = 〈s, χ〉 ∼ b̄ at the beginning of a simulation (line 5 in Algorithm 1), we directly sample a Ḋ ∼ Dir(χ), which we will refer to as the root-sampled model Ḋ and it is used for the rest of the simulation.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
We denote this root sampling in BA-POMCP as ‘R-BA-POMCP’.,3 BA-POMDPs via Sample-based Planning,[0],[0]
The approach is formalized by R-BA-POMCPSTEP (Algorithm 4).,3 BA-POMDPs via Sample-based Planning,[0],[0]
Note that no count updates take place (cf. line 4 in Algorithm 3).,3 BA-POMDPs via Sample-based Planning,[0],[0]
"This highlights an important advantage of this technique: since the counts are not used in the remainder of the simulation, the copy of counts (as part of line 6 of Algorithm 1) can be avoided altogether.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Since this copy operation is costly, especially in larger domains, where the number of states, action and observations and the number of counts is large, this can lead to significant savings.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Finally, we point out that, similar to what Guez et al. [7] propose, Ḋ can be constructed lazily: the part of the model Ḋ is only sampled when it becomes necessary.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The transition probabilities during R-BA-POMCP differ from those in BA-POMCP, and it is not obvious that a policy based on R-BA-POMCP maintains the same guarantees.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We prove in Section 4 that the solution of R-BAPOMCP in the limit converges to that of BA-POMCP.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Algorithm 4 R-BA-POMCP-STEP (s̄ = 〈s, χ〉, a) 1: //Sample from the root sampled model 2: s′,z ∼ Ḋs,a 3: s← s′ 4: return z
Algorithm 5 E-BA-POMCP-STEP(s̄ = 〈s, χ〉, a) 1: //Sample from the Expected model 2:",3 BA-POMDPs via Sample-based Planning,[0],[0]
"s′,z ∼ Dχ(·, ·|s,a) 3: χs ′z",3 BA-POMDPs via Sample-based Planning,[0],[0]
sa ← χs ′z,3 BA-POMDPs via Sample-based Planning,[0],[0]
"sa + 1
4: s← s′ 5: return z
Expected models during simulations The second, complementary, adaptation modifies the way models are sampled from the root-sampled counts in STEP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This version samples the transitions from the expected dynamics Dχ given in (1), rather than from a sampled dynamics function D ∼ Dir(χ).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The latter operation is relatively costly, while constructing Dχ is very cheap.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"In fact, this operation is so cheap, that it is more efficient to (re-)calculate it on the fly rather than to actually store Dχ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"This approach is shown in Algorithm 5.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
Linking States,3 BA-POMDPs via Sample-based Planning,[0],[0]
"Lastly, we propose a specialized data structure to encode the augmented BA-POMDP states.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The structure aims to optimize for the complexity of the count-copy operation in line 6 of Algorithm 1 while allowing modifications to s̄. The linking state sl is a tuple of a system state, a pointer (or link) to an unmodifiable set of counts χ and a set of updated counts 〈s, l, δ〉.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"l is a pointer to some set of counts χ, which remain unchanged during count updates (such as in the STEP function), and instead are stored in the set of updated counts, δ, as shown in Algorithm 6.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"The consequence is that the linking state copy-operation can safely perform a shallow copy of the counts χ, and must only consider δ, which is assumed to be much smaller.
",3 BA-POMDPs via Sample-based Planning,[0],[0]
Linking states can be used during the (rejection-sample-based) belief update at the beginning of each real time step.,3 BA-POMDPs via Sample-based Planning,[0],[0]
"While the root-sampled augmented states (including δ in linking states) are typically deleted at the end of each simulation during L-BA-POMCP, each belief update potentially increases the size of δ of each particle.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Theoretically, the number of updated counts represented in δ increases and the size of δ may (eventually) grow similar to the size of χ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"Therefore, at some point, it is necessary to construct a new χ′ that combines χ and δ (after which δ can be safely emptied).",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We define a new parameter for the maximum size of δ, λ, and condition to merge only if the size of δ exceeds λ.",3 BA-POMDPs via Sample-based Planning,[0],[0]
"We noticed that, in practice, the number of merges is much smaller than the amount of copies in BA-POMCP.",3 BA-POMDPs via Sample-based Planning,[0],[0]
We also observed in our experiments that it is often the case that a specific (small) set of transitions are notably more popular than others and that δ grows quite slowly.,3 BA-POMDPs via Sample-based Planning,[0],[0]
"Here, we analyze the proposed root sampling of the dynamics function and expected transition techniques, and demonstrate they converge to the solution of the BA-POMDP.",4 Theoretical Analysis,[0],[0]
These main steps of this proof are similar to those in [15].,4 Theoretical Analysis,[0],[0]
"We point out however, that the technicalities of proving the components are far more involved.
",4 Theoretical Analysis,[0],[0]
"The convergence guarantees of the original POMCP method are based on showing that, for an arbitrary rollout policy π, the POMDP rollout distribution (the distribution over full histories when performing root sampling of state) is equal to the derived MDP rollout distribution (the distribution over full histories when sampling in the belief MDP).",4 Theoretical Analysis,[0],[0]
Given that these are identical it is easy to see that the statistics maintained in the search tree will converge to the same number in expectation.,4 Theoretical Analysis,[0],[0]
"As such, we will show a similar result here for expected transitions (‘expected’ for short) and root sampling of the dynamics function (‘root sampling’ below).
",4 Theoretical Analysis,[0],[0]
"We define H0 as the full history (also including states) at the root of simulation, Hd as the full history of a node at depth d in the simulation tree, and χ(Hd) as the counts induced by Hd.",4 Theoretical Analysis,[0],[0]
"We then define the rollout distributions:
Definition 1.",4 Theoretical Analysis,[0],[0]
"The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π.",4 Theoretical Analysis,[0],[0]
"It is given by
Pπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (2)
with Pπ(H0) = b0(〈s0,χ0〉)",4 Theoretical Analysis,[0],[0]
"the belief ‘now’ (at the root of the online planning).
",4 Theoretical Analysis,[0],[0]
"Algorithm 6 L-BA-POMCP-STEP(sl = 〈s, l, δ〉, a) 1: D ∼ 〈l, δ〉 2: s′,z ∼ Ds,a 3: s← s′ 4: δs ′z",4 Theoretical Analysis,[0],[0]
sa ← δs ′z,4 Theoretical Analysis,[0],[0]
"sa + 1
5: return z
Note that there are two expectations in the above definition: ‘expected transitions’ mean that transitions for a history Hd are sampled from Dχ(Hd).",4 Theoretical Analysis,[0],[0]
"The other ‘expected’ is the expectation of those samples (and it is easy to see that this will converge to the expected transition probabilities Dχ(Hd)(sd+1,zd+1|as,sd)).",4 Theoretical Analysis,[0],[0]
"For root sampling of the dynamics model, this is less straightforward, and we give the definition in terms of the empirical distribution:
Definition 2.",4 Theoretical Analysis,[0],[0]
"The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by
P̃πK(Hd) , 1
Kd Kd∑ i=1",4 Theoretical Analysis,[0],[0]
"I{ Hd=H (i) d
}, where
• K is the number of simulations that comprise the empirical distribution, • Kd is the number of simulations that reach depth d (not all simulations might be equally long), • H(i)d is the history specified by the i-th particle at stage d.
Now, our main theoretical result is that these distributions are the same in the limit of the number of simulations:
Theorem 3.",4 Theoretical Analysis,[0],[0]
"The full-history RS-BA-POMDP rollout distribution (Def. 2) converges in probability to the quantity of Def. 1:
∀Hd P̃πKd(Hd) p→ Pπ(Hd).",4 Theoretical Analysis,[0],[0]
"(3)
Proof.",4 Theoretical Analysis,[0],[0]
"The proof is listed in appendix A.
Corollary 4.",4 Theoretical Analysis,[0],[0]
"Given suitably chosen exploration constant (e.g., c > Rmax1−γ ), BA-POMCP with root-sampling of dynamics function converges in probability to the expected transition solution.
",4 Theoretical Analysis,[0],[0]
Proof.,4 Theoretical Analysis,[0],[0]
"Since Theorem 3 guarantees the distributions over histories are the same in the limit, they will converge to the same values maintained in the tree.
",4 Theoretical Analysis,[0],[0]
"Finally, we see that these are solutions for the BA-POMDP:
Corollary 5.",4 Theoretical Analysis,[0],[0]
"BA-POMCP with expected transitions sampling, as well as with root sampling of dynamics function converge to an -optimal value function of a BA-POMDP: V (〈s,χ〉 ,h) p→ V ∗ (〈s,χ〉 ,h), where =
precision 1−γ .
",4 Theoretical Analysis,[0],[0]
Proof.,4 Theoretical Analysis,[0],[0]
"A BA-POMDP is a POMDP, so the analysis from Silver and Veness [15] applies to the BA-POMDP, which means that the stated guarantees hold for BA-POMCP.",4 Theoretical Analysis,[0],[0]
"The BA-POMDP is stated in terms of expected transitions, so the theoretical guarantees extend to the expected transition BA-POMCP, which in turn via corollary 4 implies that the theoretical guarantees extend to RS-BA-POMCP.
",4 Theoretical Analysis,[0],[0]
"Finally, we note that linking states does not affect they way that sampling is performed at all:
Proposition 6.",4 Theoretical Analysis,[0],[0]
Linking states does not affect convergence of BA-POMCP.,4 Theoretical Analysis,[0],[0]
"Experimental setup In this section, we evaluate our algorithms on a small toy problem, the well-known Tiger problem [3] and test scalability on a larger domain: the Partially Observable Sysadmin (POSysadmin) problem.",5 Empirical Evaluation,[0],[0]
"In POSysadmin, the agent acts as a system administrator with the task of maintaining a network of n computers.",5 Empirical Evaluation,[0],[0]
"Computers are either ‘working’ or ‘failing’, which can be deterministically resolved by ‘rebooting’ the computer.",5 Empirical Evaluation,[0],[0]
"The agent does not know the state of any computer, but can ‘ping’ any individual computer.",5 Empirical Evaluation,[0],[0]
"At each step, any of the computers can ‘fail’ with some probability f .",5 Empirical Evaluation,[0],[0]
"This leads to a state space of size 2n, an action space of 2n+ 1, where the agent can ‘ping’ or ‘reboot’ any of the computers, or ‘do nothing’, and an observation space of 3 ({NULL, failing, working}).",5 Empirical Evaluation,[0],[0]
"The ‘ping’ action has a cost of 1 associated with it, while rebooting a computer costs 20 and switches the computer to ‘working’.",5 Empirical Evaluation,[0],[0]
"Lastly, each ‘failing’ computer has a cost of 10 at each time step.
",5 Empirical Evaluation,[0],[0]
We conducted an empirical evaluation with aimed for 3 goals: The first goal attempts to support the claims made in Section 4 and show that the adaptations to BA-POMCP do not decrease the quality of the resulting policies.,5 Empirical Evaluation,[0],[0]
"Second, we investigate the runtime of those modifications to demonstrate their contribution to the efficiency of BA-POMCP.",5 Empirical Evaluation,[0],[0]
The last part contains experiments that directly compare the performance per action selection time with the baseline approach of Ross et al. [13].,5 Empirical Evaluation,[0],[0]
"For brevity, Table 1 describes the default parameters for the following experiments.",5 Empirical Evaluation,[0],[0]
"It will be explicitly mentioned whenever different values are used.
",5 Empirical Evaluation,[0],[0]
"BA-POMCP variants Section 4 proves that the solutions of the proposed modifications (root-sampling (R-), expected models (E-) and linking states (L-)) in the limit converge to the solution of BA-POMCP.",5 Empirical Evaluation,[0],[0]
"Here, we investigate the behaviour of these methods in practice.",5 Empirical Evaluation,[0],[0]
"For the Tiger problem, the agent’s initial belief over the transition model is correct (i.e., counts that correspond to the true probabilities with high confidence), but it provides an uncertain belief that underestimates the reliability of the observations.",5 Empirical Evaluation,[0],[0]
"Specifically, it assigns 5 counts to hearing the correct observation and 3 counts to incorrect: the agent initially beliefs it will hear correctly with a probability of 62.5%.",5 Empirical Evaluation,[0],[0]
"The experiment is run for with 100, 1000 & 1000 simulations and all combinations of BA-POMCP adaptations.
",5 Empirical Evaluation,[0],[0]
Figure 1a plots the average return over 10000 runs for a learning period of 100 episodes for Tiger.,5 Empirical Evaluation,[0],[0]
"The key
observation here is two-fold.",5 Empirical Evaluation,[0],[0]
"First, all methods improve over time through refining their knowledge about D. Second, there are three distinct clusters of lines, each grouped by the number of simulations.",5 Empirical Evaluation,[0],[0]
"This shows that all 3 variants (R/L/E-BA-POMCP) lead to the same results.
",5 Empirical Evaluation,[0],[0]
We repeat this investigation with the (3-computer),5 Empirical Evaluation,[0],[0]
"POSysadmin problems, where we allow 100 simulations per time step.",5 Empirical Evaluation,[0],[0]
"In this configuration, the network was fully connected with a failure probability f = 0.1.",5 Empirical Evaluation,[0],[0]
"The (deterministic) observation function is assumed known a priori, but the prior over the transition function is noisy as follows: for each count c, we take the true probability of that transition (called p) and (randomly) either subtract or add .15.",5 Empirical Evaluation,[0],[0]
Note that we do not allow transitions with non-zero probability to fall below 0 by setting those counts to 0.001.,5 Empirical Evaluation,[0],[0]
Each Dirichlet distribution is then normalized the counts to sum to 20.,5 Empirical Evaluation,[0],[0]
"With 3 computers, this results in |S| × |A| = 8× 7 = 56 noisy Dirichlet distributions of |S| = 8 parameters.
",5 Empirical Evaluation,[0],[0]
Figure 1b shows how each method is able to increase its performance over time for POSysadmin.,5 Empirical Evaluation,[0],[0]
"Again, the proposed modifications do not seem to alter the solution quality for a specific number of simulations.
",5 Empirical Evaluation,[0],[0]
BA-POMCP scalability,5 Empirical Evaluation,[0],[0]
"While the previous experiments indicate that the three adaptations produce equally good policies, they do not support any of the efficiency claims made in Section 3.",5 Empirical Evaluation,[0],[0]
"Here, we compare the scalability of BA-POMCP on the POSysadmin problem.",5 Empirical Evaluation,[0],[0]
"The proposed BA-POMCP variants are repeatedly run for 100 episodes on instances of POSysadmin of increasing network size (3 to 10 computers), and we measure the average action selection time required for 1000 simulations.",5 Empirical Evaluation,[0],[0]
"Note that the experiments are capped to allow up to 5 seconds per action selection, demonstrating the problem size that a specific method can perform 1000 simulations in under 5 seconds.
",5 Empirical Evaluation,[0],[0]
"Figure 2 shows that BA-POMCP takes less than 0.5 seconds to perform 1000 simulations on an augmented state with approximately 150 parameters (3 computers), but is quickly unable to solve larger problems, as it requires more than 4 seconds to plan for a BA-POMDP with 200000 counts.",5 Empirical Evaluation,[0],[0]
"BA-POMCP versions with a single adaptation are able to solve the same problems twice as fast, while combinations are able to solve much larger problems with up to 5 million parameters (10 computers).",5 Empirical Evaluation,[0],[0]
"This implies not only that each individual adaptation is able to speed up BA-POMCP, but also that they complement one another.
",5 Empirical Evaluation,[0],[0]
Performance The previous experiments first show that the adaptations do not decrease the policy quality of BAPOMCP and second that the modified BA-POMCP methods improve scalability.,5 Empirical Evaluation,[0],[0]
Here we put those thoughts together and directly consider the performance relative to the action selection time.,5 Empirical Evaluation,[0],[0]
In these experiments we take the average return over multiple repeats of 100 episodes and plot them according to the time required to reach such performance.,5 Empirical Evaluation,[0],[0]
"Here BA-POMCP is also directly compared to the baseline lookahead planner by Ross et al. [13].
",5 Empirical Evaluation,[0],[0]
"First, we apply lookahead with depth 1&2 on the Tiger problem under the same circumstance as the first experiment for increasing number of particles (25, 50, 100, 200 & 500), which determines the runtime.",5 Empirical Evaluation,[0],[0]
"The resulting average episode return is plotted against the action selection time in Figure 3a.
",5 Empirical Evaluation,[0],[0]
The results show that most methods reach near optimal performance after 0.5 seconds action selection time.,5 Empirical Evaluation,[0],[0]
"R-BAPOMCP and E-R-BA-POMCP perform worse than their counterparts BA-POMCP and E-BAPOMCP, which suggests that root sampling of the dynamics actually slows down BA-POMCP slightly.",5 Empirical Evaluation,[0],[0]
"This phenomenon is due to the fact that the Tiger problem is so small, that the overhead of copying the augmented state and re-sampling of dynamics (during
(a) The average return over 100 episodes per action selection time of on the Tiger problem
(b) The average return over 100 episodes per action selection time of BA-POMCP on the POSysadmin problem
STEP function) that root sampling avoids is negligible and does overcome the additional complexity of root sampling.",5 Empirical Evaluation,[0],[0]
"Also note that, even though the Tiger problem is so trivial that a lookahead of depth 1 suffices to solve the POMDP problem optimally, BA-POMCP still consistently outperforms this baseline.
",5 Empirical Evaluation,[0],[0]
The last experiment shows BA-POMCP and lookahead on the POSysadmin domain with 6 computers (which contains 55744 counts) with a failure rate of 0.05.,5 Empirical Evaluation,[0],[0]
The agent was provided with an accurate belief χ.4,5 Empirical Evaluation,[0],[0]
"The results are shown in Figure 3b.
",5 Empirical Evaluation,[0],[0]
We were unable to get lookahead search to solve this problem: the single instance which returned results in a reasonable amount of time (the single dot in the lower right corner) was with a lookahead depth of 1 (which is insufficient for this domain) with just 50 particles.,5 Empirical Evaluation,[0],[0]
"BA-POMCP, however, was able to perform up to 4096 simulations within 5 seconds and reach an average return of approximately −198, utilizing a belief of 1000 particles.",5 Empirical Evaluation,[0],[0]
"The best performing method, L-R-E-BA-POMCP requires less than 2 seconds for similar results, and is able to reach approximately −190 in less than 3 seconds.",5 Empirical Evaluation,[0],[0]
"Finally, we see that each of the individual modifications outperform the original BA-POMCP, where Expected models seems to be the biggest contributor.",5 Empirical Evaluation,[0],[0]
This paper provides a scalable framework for learning in Bayes-Adaptive POMDPs.,6 Conclusion,[0],[0]
"BA-POMDPs give a principled way of balancing exploration and exploiting in RL for POMDPs, but previous solution methods have not scaled to non-trivial domains.",6 Conclusion,[0],[0]
"We extended the Monte Carlo Tree Search method POMCP to BA-POMDPs and described three modifications—Root Sampling, Linking States and Expected Dynamics models— to take advantage of BA-POMDP structure.",6 Conclusion,[0],[0]
We proved convergence of the techniques and demonstrated that our methods can generate high-quality solutions on significantly larger problems than previous methods in the literature.,6 Conclusion,[0],[0]
"Research supported by NSF grant #1664923 and NWO Innovational Research Incentives Scheme Veni #639.021.336.
4 We do not use the same prior as in the first BA-POMCP variants experiments since this gives uninformative results due to the fact that solution methods convergence to the optimal policy with respect to the (noisy) belief, which is different from the one with respect to the true model.",Acknowledgements,[0],[0]
"While RS-BA-POMCP is potentially more efficient, it is not directly clear whether it still converges to an -optimal value function.",A Proof of Theorem 3,[0],[0]
"Here we show that the method is sound by showing that, when using root sampling of the model, the distribution over full histories (including states, actions and observations) will converge in probability to the same distribution when not using this additional root sampling step.",A Proof of Theorem 3,[0],[0]
We will give an concise itemized description of the used notation.,Notation,[0],[0]
• hd is an action-observation history at depth d of a simulation.,Action-observation histories.,[0],[0]
"• hd = (a0,z1, . . .",Action-observation histories.,[0],[0]
",ad−1,zd).
",Action-observation histories.,[0],[0]
‘Full’ histories.,Action-observation histories.,[0],[0]
"In addition to actions and observations, full histories also include the states.",Action-observation histories.,[0],[0]
"• H0 is the (unknown) full history (of real experience) at the root of the simulation: i.e., if there have been k steps of
‘real’ experience H0 = (s−k,a−k,s−k+1,z−k−1, . . .",Action-observation histories.,[0],[0]
",a−1,s0,z0).",Action-observation histories.,[0],[0]
•,Action-observation histories.,[0],[0]
"Hd is a full history (of simulated experience) at depth d in the lookahead tree: Hd = (H0,a0,s1,z1,a1,s2,z2, . . .",Action-observation histories.,[0],[0]
",ad−1,sd,zd) =
(Hd−1,ad−1,sd,zd) = 〈H0,s0:d,hd〉.",Action-observation histories.,[0],[0]
• H(i)d is the full history at depth d corresponding to simulation i. •,Action-observation histories.,[0],[0]
"In our proof, we will also need to indicate if a particular full history",Action-observation histories.,[0],[0]
"Hd is consistent with a full history at the root
of simulation:
Cons(H0,Hd)",Action-observation histories.,[0],[0]
"= { 1 if Hd is consistent with the full history at the root H0 , 0 otherwise.
",Action-observation histories.,[0],[0]
Dynamics Function.,Action-observation histories.,[0],[0]
We fold transition and observations function into one: • D denotes the dynamics model.,Action-observation histories.,[0],[0]
• Dstztst−1at−1,Action-observation histories.,[0],[0]
"= D s′z sa = Dst−1,at−1(st,zt) = D(st,zt|st−1,at−1) =",Action-observation histories.,[0],[0]
"Pr(st,zt|st−1,at−1).
",Action-observation histories.,[0],[0]
"• Dsa denotes the vector: 〈 Ds 1z1 sa , . . .",Action-observation histories.,[0],[0]
",D s|S|z|Z|",Action-observation histories.,[0],[0]
sa 〉 .,Action-observation histories.,[0],[0]
"• χs′zsa denotes how often 〈s′,z〉 occurred after 〈s,a〉.",Counts.,[0],[0]
"• χsa is the vector of counts for 〈s,a〉.",Counts.,[0],[0]
"• χ = 〈χs1a1 , . . .",Counts.,[0],[0]
",χs|S|a|A|〉 is the total collection of all such count vectors.",Counts.,[0],[0]
• χ(Hd) denotes the vector of counts at simulated full history Hd. •,Counts.,[0],[0]
"If χ0 = χ(H0) is the count vector at the root of simulation, we have that χ(Hd) = χ0 + ∆(Hd), with ∆(Hd) the
vector of counts of all (s,a,s′,z) quadruples occurring in Hd since the root of simulation (after H0).",Counts.,[0],[0]
• Let x = 〈x1 . . .,Dirichlet distributions.,[0],[0]
xK〉 ∈ ∆K and α = 〈α1 . .,Dirichlet distributions.,[0],[0]
.,Dirichlet distributions.,[0],[0]
"αK〉 be a count vector, then we write Dir(x|α) = Pr(x;α) = B(α) ∏K i=1",Dirichlet distributions.,[0],[0]
x,Dirichlet distributions.,[0],[0]
"αi−1 i , with B(α) =",Dirichlet distributions.,[0],[0]
"Γ( ∑ i αi)∏
i Γ(αi) the Dirichlet normalization constant, with Γ the gamma function.
",Dirichlet distributions.,[0],[0]
"• So, in translated in terms of dynamics function and counts, we have:
– for a particular s,a: Dir(Dsa|χsa) = Pr(Dsa;χsa) = B(χsa) ∏",Dirichlet distributions.,[0],[0]
"〈s′,z〉∈S×Z ( Ds ′z sa )χs′zsa −1 .
– we will also abuse notation and write Dir(D|χ) =",Dirichlet distributions.,[0],[0]
"∏ 〈s,a〉Dir(Dsa|χsa).",Dirichlet distributions.,[0],[0]
• ẋ denotes a root sampled quantity x. • I{condition} is the indicator function which is 1 iff condition is true and 0 otherwise.,Var.,[0],[0]
Definition 7.,Definitions,[0],[0]
"The expected full-history expected transition BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π.",Definitions,[0],[0]
"It is given by
Pπ(Hd+1) = Dχ(Hd)(sd+1,zd+1|as,sd)π(ad|hd)P π(Hd) (4)
with Pπ(H0) = b0(〈s0,χ0〉)",Definitions,[0],[0]
"the belief ‘now’ (at the root of the online planning).
",Definitions,[0],[0]
Definition 8.,Definitions,[0],[0]
"The empirical full-history root-sampling (RS) BA-POMDP rollout distribution is the distribution over full histories of a BA-POMDP, when performing Monte-Carlo simulations according to a policy π in combination with root sampling of the dynamics model D. This distribution, for a particular stage d, is given by
P̃πK(Hd) , 1
Kd Kd∑ i=1",Definitions,[0],[0]
"I{ Hd=H (i) d
}, where
• K is the number of simulations that comprise the empirical distribution.",Definitions,[0],[0]
•,Definitions,[0],[0]
Kd is the number of simulations that reach depth d (not all simulations might be equally long).,Definitions,[0],[0]
"• H(i)d is the history specified by the i-th particle at stage d.
Remark: throughout this proof we assume that there is only 1 initial count vector at the root.",Definitions,[0],[0]
Or put better: we assume that there is one unique H0 at which all simulations start.,Definitions,[0],[0]
"However, for ‘real’ steps t > 0 we could be in different Hrealt all corresponding to the same observed real history h real t .",Definitions,[0],[0]
"In this case, root sampling from the belief can be thought of root sampling the initial full history H0 ∼ b(Hrealt ).",Definitions,[0],[0]
"As such, our proof shows convergence in probability of
∀H0∀Hd P̃πKd(Hd|H0) p→ Pπ(Hd|H0).
",Definitions,[0],[0]
for each such sampled H0.,Definitions,[0],[0]
It is clear that that directly implies that ∀Hd P̃πKd(Hd) = EH0 [ P̃πKd(Hd|H0) ],Definitions,[0],[0]
p→ EH0,Definitions,[0],[0]
[Pπ(Hd|H0)],Definitions,[0],[0]
"= Pπ(Hd).
",Definitions,[0],[0]
"In the below, we omit the explicit conditioning on H0.",Definitions,[0],[0]
"The proof depends on a lemma that follows below.
",Proof of Main Theorem,[0],[0]
Theorem 9.,Proof of Main Theorem,[0],[0]
"The full-history RS-BA-POMDP rollout distribution (Def. 8) converges in probability to full-history BAPOMDP rollout distribution (Def. 7):
∀Hd P̃πKd(Hd) p→ Pπ(Hd).",Proof of Main Theorem,[0],[0]
"(5)
Proof.",Proof of Main Theorem,[0],[0]
For ease of notation we prove this for stage d+ 1.,Proof of Main Theorem,[0],[0]
Note that a history Hd+1 =,Proof of Main Theorem,[0],[0]
"(Hd,ad,sd+1,zd+1), only differs from Hd in that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple, implying that χ(Hd+1) only differs from χ(Hd) in the counts χsdad for sdad.",Proof of Main Theorem,[0],[0]
"Therefore, the expression for P̃ π",Proof of Main Theorem,[0],[0]
"Kd
(Hd) derived in Lemma 10 below (cf. equation (23)) can be written in recursive form as
P̃π(Hd+1) = Cons(H0,Hd) d∏ t=0 π(at|ht) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd+1))
",Proof of Main Theorem,[0],[0]
"= Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd)) B(χsa(Hd)) B(χsa(Hd+1))",Proof of Main Theorem,[0],[0]
"= Cons(H0,Hd) d−1∏ t=0 π(at|ht)π(ad|hd) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd))",Proof of Main Theorem,[0],[0]
"∏ 〈s,a〉 B(χsa(Hd)) B(χsa(Hd+1))
 =
Cons(H0,Hd) d−1∏ t=0 π(at|ht) ∏",Proof of Main Theorem,[0],[0]
"〈s,a〉 B(χsa(H0)) B(χsa(Hd)) π(ad|hd) B(χsdad(Hd)) B(χsdad(Hd+1))
",Proof of Main Theorem,[0],[0]
"= P̃π(Hd)π(ad|hd) B(χsdad(Hd))
B(χsdad(Hd+1))
",Proof of Main Theorem,[0],[0]
"with base case P̃π(H0) = 1, and
B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
B(χsdad(H0))",Proof of Main Theorem,[0],[0]
B(χsdad(Hd+1)) ·,Proof of Main Theorem,[0],[0]
B(χsdad(Hd)),Proof of Main Theorem,[0],[0]
B(χsdad(H0)),Proof of Main Theorem,[0],[0]
= B(χsdad(H0))/B(χsdad(Hd+1)) B(χsdad(H0))/B(χsdad(Hd)),Proof of Main Theorem,[0],[0]
"(6)
the result of dividing out the contribution of the old counts for sdad and multiplying in the new contribution.",Proof of Main Theorem,[0],[0]
"Now, we investigate these terms more closely.
",Proof of Main Theorem,[0],[0]
Again remember that the sole difference between Hd+1 =,Proof of Main Theorem,[0],[0]
"(Hd,ad,sd+1,zd+1) and Hd is that it has one extra transition for the (sd,ad,sd+1,zd+1) quadruple.",Proof of Main Theorem,[0],[0]
"Let us write T = ∑ (s′,z) χ",Proof of Main Theorem,[0],[0]
"s′z sdad
(Hd) for the total of the counts for sd,ad and N = χsd+1zd+1sdad",Proof of Main Theorem,[0],[0]
(Hd) for the number of counts for that such a transition was to (sd+1zd+1).,Proof of Main Theorem,[0],[0]
"Because Hd+1 only has 1 extra transition, we also know that for this history, the total counts is one higher: ∑ (s′,z) χ s′z sdad (Hd+1)",Proof of Main Theorem,[0],[0]
= T,Proof of Main Theorem,[0],[0]
+ 1 and since that transition was to (sd+1zd+1) the counts χ sd+1zd+1 sdad (Hd+1),Proof of Main Theorem,[0],[0]
= N + 1.,Proof of Main Theorem,[0],[0]
"Now let us expand the term from (6):
B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
Γ(T )/",Proof of Main Theorem,[0],[0]
∏ s′z Γ(χ s′z sdad,Proof of Main Theorem,[0],[0]
"(Hd))
Γ(T + 1)/ ∏",Proof of Main Theorem,[0],[0]
"s′z Γ(χ s′z sdad (Hd+1))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
",Proof of Main Theorem,[0],[0]
"∏ s′z Γ(χ s′z sdad
(Hd+1))∏ s′z Γ(χ s′ sdad (Hd))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(χ sd+1zd+1 sdad (Hd+1))",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
s′z 6=(sd+1zd+1) Γ(χ,Proof of Main Theorem,[0],[0]
"s′z sdad (Hd+1))
Γ(χ sd+1zd+1 sdad (Hd)) ∏",Proof of Main Theorem,[0],[0]
s′z 6=(sd+1zd+1) Γ(χ,Proof of Main Theorem,[0],[0]
"s′z sdad (Hd))
",Proof of Main Theorem,[0],[0]
"= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(χ sd+1zd+1 sdad (Hd+1))
Γ(χ sd+1zd+1 sdad (Hd))
= Γ(T )
Γ(T",Proof of Main Theorem,[0],[0]
"+ 1)
Γ(N + 1)
Γ(N)
",Proof of Main Theorem,[0],[0]
"Now, the gamma function has the property that Γ(x+ 1) = xΓ(x)",Proof of Main Theorem,[0],[0]
"[4], which means that we get
= Γ(T )
TΓ(T )
NΓ(N) Γ(N) = N T .
",Proof of Main Theorem,[0],[0]
"Therefore we get B(χsdad(Hd))
B(χsdad(Hd+1))",Proof of Main Theorem,[0],[0]
"=
χ sd+1zd+1 sdad (Hd)∑
(s′,z) χ s′z sdad (Hd)
and thus
P̃π(Hd+1) = P̃ π(Hd)π(ad|hd)
χ sd+1zd+1 sdad (Hd)∑
(s′,z) χ s′z sdad (Hd) .",Proof of Main Theorem,[0],[0]
"(7)
the r.h.s.",Proof of Main Theorem,[0],[0]
of this equation is identical to (4) except for the difference in between P̃π(Hd) and Pπ(Hd).,Proof of Main Theorem,[0],[0]
"This can be resolved by forward induction with base step: P̃π(H0) = b0(〈s0,χ0,ψ0〉) = Pπ(H0), and the induction step (show P̃π(Hd+1) = P
π(Hd+1) given P̃π(Hd) = Pπ(Hd)) directly following from (4) and (7).",Proof of Main Theorem,[0],[0]
"Therefore we can conclude that ∀d P̃π(Hd) = Pπ(Hd).
",Proof of Main Theorem,[0],[0]
"Since Lemma 10 establishes that ∀Hd P̃πKd(Hd) p→ P̃π(Hd), we directly have
∀Hd P̃πKd(Hd) p→ Pπ(Hd),
thus proving the result.
",Proof of Main Theorem,[0],[0]
"The proof depends on the following lemma:
Lemma 10.",Proof of Main Theorem,[0],[0]
"The full-history RS-BA-POMDP rollout distribution converges in probability to the following quantity:
∀Hd P̃πKd(Hd) p→ b0(s0) [ d∏ t=1 π(at−1|ht−0) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)  (8) with B(α) = Γ(α1+...·+αk)Γ(α1)·...·Γ(αk) the normalization term of a Dirichlet distribution with parametric vector α.
",Proof of Main Theorem,[0],[0]
Proof.,Proof of Main Theorem,[0],[0]
"Via the weak law of large numbers, we have that the empirical mean of a random variable converges in probability to its expectation.
",Proof of Main Theorem,[0],[0]
∀Hd P̃πKd(Hd) p→ 1 Kd Kd∑ i=1,Proof of Main Theorem,[0],[0]
I{ Hd=H (i) d } p→ E [ I{ Hd=H (i) d }],Proof of Main Theorem,[0],[0]
"This expectation can be rewritten as follows
E",Proof of Main Theorem,[0],[0]
"[ I{ Hd=H (i) d }] = ∑ H
(i) d
P̃π ( H
(i) d )",Proof of Main Theorem,[0],[0]
"I{ Hd=H (i) d } = P̃π (Hd) (9)
where P̃π(Hd) denotes the (true, non-empirical) probability that the RS-BA-POMDP rollout generates full historyHd.",Proof of Main Theorem,[0],[0]
"This is an expectation over the root sampled model Ḋ:
P̃π(Hd) = ∫",Proof of Main Theorem,[0],[0]
"P̃π ( Hd|Ḋ ) Dir(Ḋ|χ̇)dḊ (10)
= ∫ [ Cons(H0,Hd)
d∏ t=1",Proof of Main Theorem,[0],[0]
"Ḋ(st,zt|st−1,at−1)π(at−1|ht−1)
] Dir(Ḋ|χ̇)dḊ (11)
= Cons(H0,Hd)",Proof of Main Theorem,[0],[0]
"[ d∏ t=1 π(at−1|ht−1) ](∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ]",Proof of Main Theorem,[0],[0]
"Dir(Ḋ|χ̇)dḊ ) (12)
Where Cons(H0,Hd) is a term that indicates whether (takes value 1 if)",Proof of Main Theorem,[0],[0]
"Hd is consistent with the full history at the root H0.5
5An earlier version of this proof ([9]) contained a term b0(s0) instead of Cons(H0,Hd), which fails to recognize that this proof assumes H0 to be fixed.",Proof of Main Theorem,[0],[0]
"See also the remark on page 11.
",Proof of Main Theorem,[0],[0]
"Now we can exploit the fact that only the Dirichlet for the transitions specified by Hd matter.∫ [ d∏ t=1 Ḋ(st,zt|st−1,at−1) ] Dir(Ḋ|χ0)dḊ (13) ={split up the integral over one big vector into integrals over smaller vectors}∫ · · · ∫",Proof of Main Theorem,[0],[0]
"[ d∏
t=1
Ḋst,ztst−1,at−1 ]∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  dḊs1a1 . . .",Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (14) ={reorder the transition probabilities: ∆sas ′z χ,Proof of Main Theorem,[0],[0]
"(Hd)is the number of occurences of (s,a,s
′,z)in Hd}∫ · · · ∫ ∏
〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
"∆sas′zχ (Hd)∏ 〈s,a〉 Dir(Ḋsa|χsa(H0))  ",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (15) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
"∆sas′zχ (Hd)∏ 〈s,a〉 B(χ̇sa)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (16) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉  ∏ 〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
∆sas′zχ (Hd)B(χ̇sa) ∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (17) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 B(χ̇sa)  ",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )",Proof of Main Theorem,[0],[0]
∆sas′zχ (Hd) ∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1",Proof of Main Theorem,[0],[0]
dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (18) = ∫ · · · ∫,Proof of Main Theorem,[0],[0]
"∏
〈s,a〉 B(χ̇sa)",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋs ′z sa )χsas′z0 −1+∆sas′zχ",Proof of Main Theorem,[0],[0]
(Hd) dḊs1a1 . . .,Proof of Main Theorem,[0],[0]
dḊs|S|a|A| (19),Proof of Main Theorem,[0],[0]
"Now we reverse the order of integration and multiplication, which is possible since the different s,a pairs over which we integrate are disjoint.6 We obtain:
= ∏ 〈s,a〉 B(χsa(H0)) ∫",Proof of Main Theorem,[0],[0]
∏,Proof of Main Theorem,[0],[0]
"〈s′,z〉 ( Ḋsa(s ′,z) )",Proof of Main Theorem,[0],[0]
χsas′z0,Proof of Main Theorem,[0],[0]
+∆sas′zχ,Proof of Main Theorem,[0],[0]
"(Hd)−1 dḊsa (20)
={since we integrate over the entire vector Ḋsa, the integral equals 1/B(χsa(H0) +",Proof of Main Theorem,[0],[0]
"∆saχ (Hd))}∏ 〈s,a〉 B(χsa(H0)) 1 B(χsa(H0) + ∆saχ (Hd))",Proof of Main Theorem,[0],[0]
"(21)
= ∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd)) (22)
Therefore
P̃π(Hd) = Cons(H0,Hd) [ d−1∏ t=0 π(at|ht) ]∏ 〈s,a〉 B(χsa(H0)) B(χsa(Hd))  , (23) proving (8).
",Proof of Main Theorem,[0],[0]
"6E.g, consider two sets A1 = { a (1) 1 ,a (2) 1 } and A2 = { a (1) 2 ,a (2) 2 ,a (3) 2 } .",Proof of Main Theorem,[0],[0]
"Equation (19) is of the same form as
∑ a1∈A1 ∑ a2∈A2 2∏ i=1",Proof of Main Theorem,[0],[0]
"ai = ∑ a1∈A1 ∑ a2∈A2 a1a2 = a (1) 1 a (1) 2 + a (1) 1 a (2) 2 + a (1) 1 a (3) 2 + a (2) 1 a (1) 2 + a (2) 1 a (2) 2 + a (2) 1 a (3) 2
= a (1) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) + a (2) 1 ( a (1) 2 + a (2) 2 + a (3) 2 ) =",Proof of Main Theorem,[0],[0]
( a (1) 1 + a (2) 1 )( a (1) 2 + a (2) 2 + a (3) 2 ),Proof of Main Theorem,[0],[0]
"=
 ∑ a1∈A1",Proof of Main Theorem,[0],[0]
a1  ∑ a2∈A2 a2  = 2∏ i=1 ∑ ai∈Ai ai,Proof of Main Theorem,[0],[0]
"The POMDP is a powerful framework for reasoning under outcome and information uncertainty, but constructing an accurate POMDP model is difficult.",abstractText,[0],[0]
Bayes-Adaptive Partially Observable Markov Decision Processes (BAPOMDPs) extend POMDPs to allow the model to be learned during execution.,abstractText,[0],[0]
"BA-POMDPs are a Bayesian RL approach that, in principle, allows for an optimal trade-off between exploitation and exploration.",abstractText,[0],[0]
"Unfortunately, BAPOMDPs are currently impractical to solve for any non-trivial domain.",abstractText,[0],[0]
"In this paper, we extend the Monte-Carlo Tree Search method POMCP to BA-POMDPs and show that the resulting method, which we call BA-POMCP, is able to tackle problems that previous solution methods have been unable to solve.",abstractText,[0],[0]
"Additionally, we introduce several techniques that exploit the BA-POMDP structure to improve the efficiency of BA-POMCP along with proof of their convergence.",abstractText,[0],[0]
Learning in POMDPs with Monte Carlo Tree Search∗,title,[0],[0]
"With the increasing success of highly non-convex and complex learning architectures such as neural networks, there is an increasing effort to further understand and explain the limits of training such hierarchical structures.
",1. Introduction,[0],[0]
"Recently there have been attempts to draw mathematical insight from kernel methods in order to better understand deep learning, as well as come up with new computationally learnable architectures.",1. Introduction,[0],[0]
"One such line of work consists of learning classifiers that are linear functions of a very large or infinite collection of non-linear functions (Bach,
1University of Princeton, Princeton, New Jersey, USA 2Tel-Aviv University, Tel-Aviv, Israel.",1. Introduction,[0],[0]
"Correspondence to: Roi Livni <rlivni@cs.princeton.edu>, Daniel Carmon <carmonda@mail.tau.ac.il>, Amir Globerson <gamir@mail.tau.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2014; Daniely et al., 2016; Cho & Saul, 2009; Heinemann et al., 2016; Williams, 1997).",1. Introduction,[0],[0]
"Such models can be interpreted as a neural network with infinitely many nodes in a hidden layer, and we thus refer to them as “Infinite Layer Networks” (ILN).",1. Introduction,[0],[0]
"They are of course also related to kernel based classifiers, as will be discussed later.
",1. Introduction,[0],[0]
"A target function in an ILN class will be of the form: x→ ∫ ψ(x;w)f(w)dµ(w), (1)
",1. Introduction,[0],[0]
"Here ψ is some function of the input x and parameters w, and dµ(w) is a prior over the parameter space.",1. Introduction,[0],[0]
"For example, ψ(x;w) can be a single sigmoidal neuron or a complete convolutional network.",1. Introduction,[0],[0]
"The integral can be thought of as an infinite sum over all such possible networks, and f(w) can be thought of as an infinite output weight vector to be trained.
",1. Introduction,[0],[0]
A Standard 1–hidden layer network with a finite set of units can be obtained from the above formalism as follows.,1. Introduction,[0],[0]
"First, choose ψ(x;w) = σ(x ·w) where σ is an activation function (e.g., sigmoid or relu).",1. Introduction,[0],[0]
"Next, set dµ(w) to be a discrete measure over a finite set w1, . . .",1. Introduction,[0],[0]
",wd.1 In this case, the integral results in a network with d hidden units, and the function f is the linear weights of the output layer.",1. Introduction,[0],[0]
"Namely:
x→ 1 d d∑ i=1 f(wi) · σ(x ·wi).
",1. Introduction,[0],[0]
"The main challenge when training 1–hidden layer networks is of course to find the w1, . . .",1. Introduction,[0],[0]
",wd on which we wish to support our distribution.",1. Introduction,[0],[0]
"It is known (Livni et al., 2014), that due to hardness of learning intersection of halfspaces (Klivans & Sherstov, 2006; Daniely et al., 2014), 1–hidden layer neural networks are computationally hard for a wide class of activation functions.",1. Introduction,[0],[0]
"Therefore, as the last example illustrates, the choice of µ is indeed crucial for performance.
",1. Introduction,[0],[0]
"For a fixed prior µ, the class of ILN functions is highly expressive, since f can be chosen to approximate any 1- hidden layer architecture to arbitrary precision (by setting f to delta functions around the weights of the network, as
1In δ function notation dµ(w)",1. Introduction,[0],[0]
= 1 d ∑d i=1,1. Introduction,[0],[0]
"δ(w −wi)dw
we did above for µ).",1. Introduction,[0],[0]
"However, this expressiveness comes at a cost.",1. Introduction,[0],[0]
"As argued in Heinemann et al. (2016), ILN will generalize well when there is a large probability mass of w parameters that attain a small loss.
",1. Introduction,[0],[0]
The key observation that makes certain ILN tractable to learn is that Eq. 1 is a linear functional in f .,1. Introduction,[0],[0]
In that sense it is a linear classifier and enjoys the rich theory and algorithmic toolbox for such classifiers.,1. Introduction,[0],[0]
"In particular, one can use the fact that linear classifiers can be learned via the kernel trick in a batch (Cortes & Vapnik, 1995) as well as online settings (Kivinen et al., 2004).",1. Introduction,[0],[0]
"In other words, we can reduce learning ILN to the problem of computing the kernel function between two examples.",1. Introduction,[0],[0]
"Specifically the problem reduces to computing integrals of the following form:
k(x1,x2) = ∫ ψ(x1;w) · ψ(x2;w)dµ(w) (2)
= E w̄∼µ",1. Introduction,[0],[0]
[ψ(x1; w̄) · ψ(x2; w̄)] .,1. Introduction,[0],[0]
"(3)
In this work we extend this result to the case where no closed form kernel is available, and thus the kernel trick is not directly applicable.",1. Introduction,[0],[0]
"We thus turn our attention to the setting where features (i.e., w vectors) can be randomly sampled.",1. Introduction,[0],[0]
"In this setting, our main result shows that for the squared loss, we can efficiently learn the above class.",1. Introduction,[0],[0]
"Moreover, we can surprisingly do this with a computational cost comparable to that of methods that have access to the closed form kernel k(x1,x2).
",1. Introduction,[0],[0]
"The observation we begin with is that sampling random features (i.e., w above), leads to an unbiased estimate of the kernel in Eq. 2.",1. Introduction,[0],[0]
"Thus, if for example, we ignore complexity issues and can sample infinitely many w’s, it is not surprising that we can avoid the need for exact computation of the kernel.",1. Introduction,[0],[0]
"However, our results provide a much stronger and practical result.",1. Introduction,[0],[0]
"Given T training samples, the lower bound on achievable accuracy is O(1/ √ T ) (see Shamir, 2015).",1. Introduction,[0],[0]
"We show that we can in fact achieve this rate, using Õ(T 2) calls2 to the random feature generator.",1. Introduction,[0],[0]
"For comparison, note that O(T 2) is the size of the kernel matrix, and is thus likely to be the cost of any algorithm that uses an explicit kernel matrix, where one is available.",1. Introduction,[0],[0]
"As we discuss later, our approach improves on previous random features based learning (Dai et al., 2014; Rahimi & Recht, 2009) in terms of sample/computational complexity, and expressiveness.",1. Introduction,[0],[0]
We consider algorithms that learn a mapping from input instances x ∈ X to labels y ∈ Y .,2. Problem Setup,[0],[0]
"We focus on the regression case where Y is the interval [−1, 1].",2. Problem Setup,[0],[0]
Our starting point is a class of feature functions ψ(w;x) :,2. Problem Setup,[0],[0]
"Ω×X → R,
2We use Õ notation to suppress logarithmic factors
parametrized by vectors w ∈",2. Problem Setup,[0],[0]
"Ω. The functions ψ(w;x) may contain highly complex non linearities, such as multilayer networks consisting of convolution and pooling layers.",2. Problem Setup,[0],[0]
"Our only assumption on ψ(w;x) is that for all w ∈ Ω and x ∈ X it holds that |ψ(w;x)| < 1.
",2. Problem Setup,[0],[0]
"Given a distribution µ on Ω, we denote by L2(Ω, µ) the class of square integrable functions over Ω.
L2(Ω, µ) =
{ f : ∫ f2(w)dµ(w)",2. Problem Setup,[0],[0]
"<∞ } .
",2. Problem Setup,[0],[0]
We will use functions f ∈,2. Problem Setup,[0],[0]
"L2(Ω, µ) as mixture weights over the class Ω, where each f naturally defines a new regression function from x to R as follows:
x→ ∫ ψ(w;x)f(w)dµ(w).",2. Problem Setup,[0],[0]
"(4)
Our key algorithmic assumption is that the learner can efficiently sample random w",2. Problem Setup,[0],[0]
"according to the distribution µ. Denote the time to generate one such sample by ρ.
",2. Problem Setup,[0],[0]
In what follows it will be simpler to express the integrals as scalar products.,2. Problem Setup,[0],[0]
Define the following scalar product on functions f ∈,2. Problem Setup,[0],[0]
"L2(Ω, µ).
",2. Problem Setup,[0],[0]
"〈f, g〉 = ∫ f(w)g(w)dµ(w) (5)
We denote the corresponding `2 norm by ‖f‖ = √ 〈f, f〉.",2. Problem Setup,[0],[0]
"Also, given features x denote by Φ(x) the function in L2(Ω, µ) given by Φ(x)[w] = ψ(w;x).",2. Problem Setup,[0],[0]
The regression functions we are considering are then of the form x→,2. Problem Setup,[0],[0]
"〈f,Φ(x)〉.
",2. Problem Setup,[0],[0]
"A subclass of norm bounded elements in L2(Ω, µ) induces a natural subclass of regression functions.",2. Problem Setup,[0],[0]
"Namely, we consider the following class:
HBµ = {x→ 〈f,Φ(x)〉 : ‖f‖ < B} .
",2. Problem Setup,[0],[0]
"Our ultimate goal is to output a predictor f ∈ L2(Ω, µ) that is competitive, in terms of prediction, with the best target function in the classHBµ .
",2. Problem Setup,[0],[0]
"We will consider an online setting, and use it to derive generalization bounds via standard online to batch conversion.",2. Problem Setup,[0],[0]
"In our setting, at each round a learner chooses a target function ft ∈ L2(Ω, µ) and an adversary then reveals a sample xt and label yt.",2. Problem Setup,[0],[0]
"The learner then incurs a loss of
`t(ft)",2. Problem Setup,[0],[0]
"= 1
2 (〈ft,Φ(xt)〉 − yt)2 .",2. Problem Setup,[0],[0]
"(6)
The use of squared loss might seem restrictive if one is interested in classification.",2. Problem Setup,[0],[0]
"However, L2 loss is common by now in classification with support vector machines and kernel methods since (Suykens & Vandewalle, 1999; Suykens
et al., 2002).",2. Problem Setup,[0],[0]
"More recently Zhang et al. (2016) showed that when using a large number of features regression achieves performance comparable to the corresponding linear classifiers (see Section 5 therein).
",2. Problem Setup,[0],[0]
"The objective of the learner is to minimize her T round regret w.r.t norm bounded elements in L2(Ω, µ).",2. Problem Setup,[0],[0]
"Namely:
T∑ t=1",2. Problem Setup,[0],[0]
`t(ft)− min f∗∈HBµ T∑ t=1 `t(f ∗).,2. Problem Setup,[0],[0]
"(7)
In the statistical setting we assume that the sequence S = {(xi, yi)}Ti=1 is generated IID according to some unknown distribution P. We then define the expected loss of a predictor as
L(f) = E (x,y)∼P
[ 1
2 (〈f,Φ(x)〉 − y)2
] .",2. Problem Setup,[0],[0]
(8),2. Problem Setup,[0],[0]
Theorem 1 states our result for the online model.,3. Main Results,[0],[0]
The corresponding result for the statistical setting is given in Corollary 1.,3. Main Results,[0],[0]
"We will elaborate on the structure of the Algorithm later, but first provide the main result.
",3. Main Results,[0],[0]
"Algorithm 1: The SHRINKING GRADIENT algorithm.
",3. Main Results,[0],[0]
"Data: T, B > 1, η,m Result: Weights α(1), . . .",3. Main Results,[0],[0]
", α(T+1) ∈ RT .",3. Main Results,[0],[0]
"Functions
ft ∈ L2(Ω, µ) defined as ft = ∑t i=1",3. Main Results,[0],[0]
"α (t) i Φ(xi);
Initialize α(1) = 0̄ ∈ RT ; for t = 1, . . .",3. Main Results,[0],[0]
", T do
Observe xt, yt; Set Et = EST SCALAR PROD(α(t),x1:t−1,xt,m); if |Et| < 16B then
α(t+1) = α(t); α
(t+1) t = −η(yt − Et);
else α(t+1) = 14α (t);
Theorem 1.",3. Main Results,[0],[0]
"Run Algorithm 1 with parameters T , B ≥ 1, η = B√
T and m = O
( B4T log (BT ) ) .",3. Main Results,[0],[0]
"Then:
1.",3. Main Results,[0],[0]
"For every sequence of squared losses `1, . . .",3. Main Results,[0],[0]
", `T observed by the algorithm we have for f1, . .",3. Main Results,[0],[0]
.,3. Main Results,[0],[0]
", fT :
E",3. Main Results,[0],[0]
[ T∑ t=1 `t(ft)− min f∗∈HBµ T∑ t=1 `t(f ∗) ],3. Main Results,[0],[0]
"= O(B √ T )
Algorithm 2: EST SCALAR PROD Data: α, x1:t−1, x, m Result:",3. Main Results,[0],[0]
"Estimated scalar product E if α = 0̄ then
Set E = 0 else
for k=1.. . .",3. Main Results,[0],[0]
",m do Sample i from the distribution q(i) =",3. Main Results,[0],[0]
"|αi|∑ |αi| ; Sample parameter w̄ from µ. Set E(k) = sgn(αi)ψ(xi; w̄)ψ(x; w̄);
",3. Main Results,[0],[0]
"Set E = ‖α‖1m ∑m k=1E (k)
2.",3. Main Results,[0],[0]
"The run-time of the algorithm is Õ ( ρB4T 2 ) .3
3.",3. Main Results,[0],[0]
For each t = 1 . . .,3. Main Results,[0],[0]
"T and a new test example x, we can with probability ≥ 1 − δ",3. Main Results,[0],[0]
"estimate 〈ft,Φ(x)〉 within accuracy 0 by running Algorithm 2 with parameters α(t), {xi}ti=1, ,x and m = O(B
4T 20
log 1/δ).",3. Main Results,[0],[0]
"The resulting running time for a test point is then O(ρm).
",3. Main Results,[0],[0]
"We next turn to the statistical setting, where we provide bounds on the expected performance.",3. Main Results,[0],[0]
"Following standard online to batch conversion and Theorem 1 we can obtain the following Corollary (e.g., see Shalev-Shwartz, 2011):
Corollary 1 (Statistical Setting).",3. Main Results,[0],[0]
The following holds for any > 0.,3. Main Results,[0],[0]
"Run Algorithm 1 as in Theorem 1, with T = O(B 2
2 ).",3. Main Results,[0],[0]
"Let S = {(xt, yt)} T t=1, be an IID sample drawn from some unknown distribution P. Let fS = 1T ∑ ft.",3. Main Results,[0],[0]
"Then the expected loss satisfies:
E S∼P",3. Main Results,[0],[0]
[L(fS)],3. Main Results,[0],[0]
"< inf f∗∈HBµ
L(f∗) + .
",3. Main Results,[0],[0]
"The runtime of the algorithm, as well as estimation time on a test example are as defined in Theorem 1.
",3. Main Results,[0],[0]
Proofs of the results are provided in Section 5.1 and the appendix.,3. Main Results,[0],[0]
"Learning with random features can be traced to the early days of learning (Minsky & Papert, 1988), and infinite networks have also been introduced more than 20 years ago (Williams, 1997; Hornik, 1993).",4. Related Work,[0],[0]
"More recent works have considered learning neural nets (also multi-layer) with infinite hidden units using the kernel trick (Cho & Saul, 2009; Deng et al., 2012; Hazan & Jaakkola, 2015; Heinemann et al., 2016).",4. Related Work,[0],[0]
"These works take a similar approach
3Ignoring logarithmic factors in B and T .
to ours but focus on computing the kernel for certain feature classes in order to invoke the kernel trick.",4. Related Work,[0],[0]
Our work in contrast avoids using the kernel trick and applies to any feature class that can be randomly generated.,4. Related Work,[0],[0]
"All the above works are part of a broader effort of trying to circumvent hardness in deep learning by mimicking deep nets through kernels (Mairal et al., 2014; Bouvrie et al., 2009; Bo et al., 2011; 2010), and developing general duality between neural networks and kernels (Daniely et al., 2016).
",4. Related Work,[0],[0]
From a different perspective the relation between random features and kernels has been noted by Rahimi & Recht (2007) who show how to represent translation invariant kernels in terms of random features.,4. Related Work,[0],[0]
"This idea has been further studied (Bach, 2015; Kar & Karnick, 2012) for other kernels as well.",4. Related Work,[0],[0]
"The focus of these works is mainly to allow scaling down of the feature space and representation of the final output classifier.
",4. Related Work,[0],[0]
"Dai et al. (2014) focus on tractability of large scale kernel methods, and their proposed doubly stochastic algorithm can also be used for learning with random features as we have here.",4. Related Work,[0],[0]
"In Dai et al. (2014) the objective considered is of the regularized form:γ2 ‖f‖
2 +R(f), with a corresponding sample complexity of O(1/(γ2 2)) samples needed to achieve approximation with respect to the risk of the optimum of the regularized objective.
",4. Related Work,[0],[0]
"To relate the above results to ours, we begin by emphasizing that the bound in (Dai et al., 2014) holds for fixed γ, and refers to optimization of the regularized objective.",4. Related Work,[0],[0]
"Our objective is to minimize the risk R(f) which is the expected squared loss, for which we need to choose γ = O( B2 ) in order to attain accuracy (Sridharan et al., 2009).",4. Related Work,[0],[0]
"Plugging this γ into the generalization bound in Dai et al. (2014) we obtain that the algorithm in Dai et al. (2014) needs O(B 4
4 ) samples to compete with the optimal target function in the B-ball.",4. Related Work,[0],[0]
"Our algorithm needs O(B 2
2 ) examples which is considerably better.",4. Related Work,[0],[0]
"We note that their method does extend to a larger class of losses, whereas our is restricted to the quadratic loss.
",4. Related Work,[0],[0]
"In Rahimi & Recht (2009), the authors consider embedding the domain into the feature space x →",4. Related Work,[0],[0]
"[ψ(w1;x), . . .",4. Related Work,[0],[0]
", ψ(wm;x)], where wi are IID random variables sampled according to some prior µ(w).",4. Related Work,[0],[0]
"They show that with O(B 2 log 1/δ 2 ) random features estimated on O(B 2 log 1/δ 2 ) samples they can compete with the class:
",4. Related Work,[0],[0]
"HBµ max = { x→ ∫ ψ(w;x)f(w)dµ(w) : |f(w)| ≤ B } Our algorithm relates to the mean square error cost function which does not meet the condition in Rahimi & Recht (2009), and is hence formally incomparable.",4. Related Work,[0],[0]
"Yet we can invoke our algorithm to compete against a larger class
of target functions.",4. Related Work,[0],[0]
"Our main result shows that Algorithm 1, using Õ(B 8 4 ) estimated features and using O( B2
2 ) samples will, in expectation, output a predictor that is close to the best in HBµ .",4. Related Work,[0],[0]
Note that |f(w)| < B implies Ew∼µ(f2(w)),4. Related Work,[0],[0]
< B2.,4. Related Work,[0],[0]
Hence HBµ max ⊆ H B µ .,4. Related Work,[0],[0]
"Note however, that the number of estimated features (as a function of B) is worse in our case.
",4. Related Work,[0],[0]
Our approach to the problem is to consider learning with a noisy estimate of the kernel.,4. Related Work,[0],[0]
"A related setting was studied in Cesa-Bianchi et al. (2011b), where the authors considered learning with kernels when the data is corrupted.",4. Related Work,[0],[0]
Noise in the data and noise in the scalar product estimation are not equivalent when there is non-linearity in the kernel space embedding.,4. Related Work,[0],[0]
"There is also extensive research on linear regression with actively chosen attributes (CesaBianchi et al., 2011a; Hazan & Koren, 2012).",4. Related Work,[0],[0]
The convergence rates and complexity of the algorithms are dimension dependent.,4. Related Work,[0],[0]
It would be interesting to see if their method can be extended from finite set of attributes to a continuum set of attributes.,4. Related Work,[0],[0]
"We next turn to present Algorithm 1, from which our main result is derived.",5. Algorithm,[0],[0]
"The algorithm is similar in spirit to Online Gradient Descent (OGD) (Zinkevich, 2003), but with some important modifications that are necessary for our analysis.
",5. Algorithm,[0],[0]
"We first introduce the problem in the terminology of online convex optimization, as in Zinkevich (2003).",5. Algorithm,[0],[0]
At iteration t our algorithm outputs a hypothesis ft.,5. Algorithm,[0],[0]
"It then receives as feedback (xt, yt), and suffers a loss `t(ft) as in Eq. 6.",5. Algorithm,[0],[0]
"The objective of the algorithm is to minimize the regret against a benchmark of B-bounded functions, as in Eq. 7.
",5. Algorithm,[0],[0]
A classic approach to the problem is to exploit the OGD algorithm.,5. Algorithm,[0],[0]
"Its simplest version would be to update ft+1 → ft − η∇t where η is a step size, and ∇t is the gradient of the loss w.r.t.",5. Algorithm,[0],[0]
f at ft.,5. Algorithm,[0],[0]
"In our case,∇t is given by:
∇t = (〈ft,Φ(xt)〉 − yt) Φ(xt) (9)
",5. Algorithm,[0],[0]
Applying this update would also result in a function ft =∑t i=1 αiΦ(xt) as we have in Algorithm 1 (but with different αi from ours).,5. Algorithm,[0],[0]
"However, in our setting this update is not applicable since the scalar product 〈ft,Φ(xt)〉 is not available.",5. Algorithm,[0],[0]
One alternative is to use a stochastic unbiased estimate of the gradient that we denote by ∇̄t.,5. Algorithm,[0],[0]
This induces an update step ft+1 → ft − η∇̄t.,5. Algorithm,[0],[0]
"One can show that OGD with such an estimated gradient enjoys the following upper bound on the regret E [ ∑ `t(ft)− `t(f∗)] for every ‖f∗‖ ≤ B (e.g., see Shalev-Shwartz, 2011):
B2
η + η T∑ i=1",5. Algorithm,[0],[0]
E,5. Algorithm,[0],[0]
[ ‖∇t‖2 ] + η T∑ i=1,5. Algorithm,[0],[0]
"V [ ∇̄t ] , (10)
where V [ ∇̄t ]",5. Algorithm,[0],[0]
= E [ ‖∇̄t −∇t‖2 ] .,5. Algorithm,[0],[0]
"We can bound the first two terms using standard techniques applicable for the squared loss (e.g., see Zhang, 2004; Srebro et al., 2010).",5. Algorithm,[0],[0]
The third term depends on our choice of gradient estimate.,5. Algorithm,[0],[0]
"There are various choices for such an estimate, and we use a version which facilitates our analysis, as explained below.",5. Algorithm,[0],[0]
"Assume that at iteration t, our function ft is given by ft =∑t i=1",5. Algorithm,[0],[0]
α (t) i Φ(xt).,5. Algorithm,[0],[0]
"We now want to use sampling to obtain an unbiased estimate of 〈ft,Φ(xt)〉.",5. Algorithm,[0],[0]
"This will be done via a two step sampling procedure, as described in Algorithm 2.",5. Algorithm,[0],[0]
"First, sample an index",5. Algorithm,[0],[0]
i ∈,5. Algorithm,[0],[0]
"[1, . . .",5. Algorithm,[0],[0]
", t] by sampling according to the distribution q(i) ∝ |α(t)i",5. Algorithm,[0],[0]
|.,5. Algorithm,[0],[0]
"Next, for the chosen i, sample w̄ according to µ, and use ψ(x; w̄)ψ(xi; w̄) to construct an estimate of 〈Φ(xi),Φ(xt)〉.",5. Algorithm,[0],[0]
"The resulting unbiased estimate of 〈Φ(xi),Φ(xt)〉 is denoted by Et and given by:
Et = ‖α(t)‖1 m m∑ i=1",5. Algorithm,[0],[0]
"sgn(α(t)i )ψ(xi; w̄)ψ(xt; w̄) (11)
",5. Algorithm,[0],[0]
"The corresponding unbiased gradient estimate is:
∇̄t = (Et − yt)xt (12)
",5. Algorithm,[0],[0]
The variance of ∇̄ affects the convergence rate and depends on both ‖α‖1 and the number of estimationsm.,5. Algorithm,[0],[0]
"We wish to maintain m = O(T ) estimations per round, while achieving O( √ T ) regret.
",5. Algorithm,[0],[0]
"To effectively regularize ‖α‖1, we modify the OGD algorithm so that whenever Et is larger then 16B, we do not perform the usual update.",5. Algorithm,[0],[0]
"Instead, we perform a shrinking step that divides α(t) (and hence ft) by 4.",5. Algorithm,[0],[0]
"Treating B as constant, this guarantees that ‖α‖1 = O(ηT ), and in turn Var(∇̄t) = O(η 2T 2 m ).",5. Algorithm,[0],[0]
"Setting η = O(1/ √ T ), we have that m = O(T ) estimations are sufficient.
",5. Algorithm,[0],[0]
"The rationale for the shrinkage is that wheneverEt is large, it indicates that ft is “far away” from the B-ball, and a shrinkage step, similar to projection, brings ft closer to the optimal element in the B-ball.",5. Algorithm,[0],[0]
"However, due to stochasticity, the shrinkage step does add a further term to the regret bound that we would need to take care of.",5. Algorithm,[0],[0]
"In what follows we analyze the regret for Algorithm 1, and provide a high level proof of Theorem 1.",5.1. Analysis,[0],[0]
The appendix provides the necessary lemmas and a more detailed proof.,5.1. Analysis,[0],[0]
"We begin by modifying the regret bound for OGD in Eq. 10 to accommodate for steps that differ from the standard gradient update, such as shrinkage.",5.1. Analysis,[0],[0]
"We use the following notation for the regret at iteration t:
Rt(f ∗)",5.1. Analysis,[0],[0]
= E,5.1. Analysis,[0],[0]
[ T∑ t=1 `t(ft)− `t(f∗) ],5.1. Analysis,[0],[0]
"(13)
Lemma 1.",5.1. Analysis,[0],[0]
"Let `1, . . .",5.1. Analysis,[0],[0]
", `T be an arbitrary sequence of convex loss functions, and let f1, . . .",5.1. Analysis,[0],[0]
", fT be random vectors, produced by an online algorithm.",5.1. Analysis,[0],[0]
Assume ‖fi‖ ≤,5.1. Analysis,[0],[0]
BT for all i ≤ T .,5.1. Analysis,[0],[0]
For each t let ∇̄t be an unbiased estimator of ∇`t(ft).,5.1. Analysis,[0],[0]
"Denote f̂t = ft−1 − η∇̄t−1 and let
Pt(f ∗)",5.1. Analysis,[0],[0]
= P [ ‖ft,5.1. Analysis,[0],[0]
− f∗‖ > ‖f̂t − f∗‖ ] .,5.1. Analysis,[0],[0]
"(14)
",5.1. Analysis,[0],[0]
"For every ‖f∗‖ ≤ B it holds that :
Rt(f ∗) ≤",5.1. Analysis,[0],[0]
"B
2
η + η T∑ t=1",5.1. Analysis,[0],[0]
E,5.1. Analysis,[0],[0]
[ ‖∇t‖2 ],5.1. Analysis,[0],[0]
+ η T∑ t=1 V [ ∇̄t ],5.1. Analysis,[0],[0]
"+
T∑ t=1",5.1. Analysis,[0],[0]
(BT +B),5.1. Analysis,[0],[0]
2 η E,5.1. Analysis,[0],[0]
[Pt(f∗)],5.1. Analysis,[0],[0]
"(15)
See Appendix B.1 for proof of the lemma.",5.1. Analysis,[0],[0]
"As discussed earlier, the first three terms on the RHS are the standard bound for OGD from Eq. 10.",5.1. Analysis,[0],[0]
"Note that in the standard OGD it holds that ft = f̂t, and therefore Pt(f∗) = 0",5.1. Analysis,[0],[0]
"and the last term disappears.
",5.1. Analysis,[0],[0]
The third term will be bounded by controlling ‖α‖1.,5.1. Analysis,[0],[0]
The last term Pt(f∗) is a penalty that results from updates that stir ft away from the standard update step f̂t.,5.1. Analysis,[0],[0]
This will indeed happen for the shrinkage step.,5.1. Analysis,[0],[0]
The next lemma bounds this term.,5.1. Analysis,[0],[0]
"See Appendix B.2 for proof.
",5.1. Analysis,[0],[0]
Lemma 2.,5.1. Analysis,[0],[0]
"Run Algorithm 1 with parameters T , B ≥ 1 and η < 1/8.",5.1. Analysis,[0],[0]
Let ∇̄t be the unbiased estimator of ∇`t(ft) of the form ∇̄t =,5.1. Analysis,[0],[0]
(Et − yt)Φ(xt).,5.1. Analysis,[0],[0]
Denote f̂t = ft − η∇̄t and define Pt(f∗) as in Eq. 14.,5.1. Analysis,[0],[0]
"Then:
Pt(f ∗)",5.1. Analysis,[0],[0]
"≤ 2 exp ( − m
(3ηt)2
)
",5.1. Analysis,[0],[0]
The following lemma (see Appendix B.3 for proof) bounds the second and third terms of Eq. 15.,5.1. Analysis,[0],[0]
Lemma 3.,5.1. Analysis,[0],[0]
Consider the setting as in Lemma 2.,5.1. Analysis,[0],[0]
Then V [ ∇̄t ] ≤ ((16B+1)ηt) 2 m and E,5.1. Analysis,[0],[0]
[ ‖∇t‖2 ] ≤ 2E,5.1. Analysis,[0],[0]
"[`t(ft)].
",5.1. Analysis,[0],[0]
"Proof of Theorem 1 Combining Lemmas 1, 2 and 3 and rearranging we get:
(1− 2η)E [Rt(f∗)]",5.1. Analysis,[0],[0]
"≤ B2
η + 2η T∑ t=1 `t(f ∗)",5.1. Analysis,[0],[0]
"+ (16)
η ((16B + 1)ηT )2T
m +
(BT +B) 2
η
T∑ t=1 Pt(f ∗)
To bound the second term in Eq. 16",5.1. Analysis,[0],[0]
"we note that:
min ‖f∗‖<B T∑ t=1 `t(f ∗) ≤",5.1. Analysis,[0],[0]
T∑ t=1 `t(0) ≤,5.1. Analysis,[0],[0]
"T. (17)
We next set η and m as in the statement of the theorem.",5.1. Analysis,[0],[0]
"Namely: η = B
2 √ T , andm = ((16B+1)B)2T log γ, where γ = max (
((16B+1)ηT+B)2) η2 , e
) .",5.1. Analysis,[0],[0]
"This choice of m implies
that m > ((16B + 1)ηT )2, and hence the third term in Eq.",5.1. Analysis,[0],[0]
"16 is upper bounded by T .
",5.1. Analysis,[0],[0]
"Next we have that m > (3ηt)2 log γ for every t, and by the bound on BT we have that γ > (B+BT ) 2
η2 .",5.1. Analysis,[0],[0]
"Taken together with Lemma 2 we have that:
(BT +B) 2
η
T∑ t=1",5.1. Analysis,[0],[0]
"Pt(f ∗) ≤ ηT. (18)
",5.1. Analysis,[0],[0]
"The above bounds imply that:
(1− 2η)E [Rt(f∗)]",5.1. Analysis,[0],[0]
"≤ B2
η + 2ηT + ηT + ηT
Finally by choice of η, and dividing both sides by (1− 2η) we obtain the desired result.",5.1. Analysis,[0],[0]
In this section we provide a toy experiment to compare our Shrinking Gradient algorithm to other random feature based methods.,6. Experiments,[0],[0]
"In particular, we consider the following three algorithms: Fixed-Random: Sample a set of r features w1, . . .",6. Experiments,[0],[0]
",wr and evaluate these on all the train and test points.",6. Experiments,[0],[0]
"Namely, all x points will be evaluated on the same features.",6. Experiments,[0],[0]
This is the standard random features approach proposed in Rahimi & Recht (2007; 2009).,6. Experiments,[0],[0]
"Doubly Stochastic Gradient Descent (Dai et al., 2014):",6. Experiments,[0],[0]
"Here each training point x samples k features w1, . . .",6. Experiments,[0],[0]
",wk.",6. Experiments,[0],[0]
These features will from that point on be used for evaluating dot products with x.,6. Experiments,[0],[0]
"Thus, different x points will use different features.",6. Experiments,[0],[0]
Shrinking Gradient:,6. Experiments,[0],[0]
This is the approach proposed here in Section 3.,6. Experiments,[0],[0]
"Namely, each training point x samples m features in order to calculate the dot product with the current regression function.
",6. Experiments,[0],[0]
"In comparing the algorithms we choose r, k,m so that the same overall number of features is calculated.",6. Experiments,[0],[0]
"For all methods we explored different initial step sizes and schedules for changing the step size.
",6. Experiments,[0],[0]
The key question in comparing the three algorithms is how well they use a given budget of random features.,6. Experiments,[0],[0]
To explore this we perform an experiments to simulate the high dimensional feature case.,6. Experiments,[0],[0]
"We consider vectors x ∈ RD, where a random feature w corresponds to a uniform choice of coordinate w in x.",6. Experiments,[0],[0]
"We work in the regime where D is large in the sense that D > T , where T is the size of the training data.",6. Experiments,[0],[0]
Thus random sampling of T features will not reveal all coordinates of x.,6. Experiments,[0],[0]
The training set is generated as follows.,6. Experiments,[0],[0]
"First, a training set x1, . . .",6. Experiments,[0],[0]
",xT ∈",6. Experiments,[0],[0]
"RD is
sampled from a standard Gaussian.",6. Experiments,[0],[0]
"We furthermore clip negative values to zero, in order to make the data sparser and more challenging for feature sampling.",6. Experiments,[0],[0]
Next a weight vector a ∈,6. Experiments,[0],[0]
RD is chosen as a random sparse linear combination of the training points.,6. Experiments,[0],[0]
This is done in order for the true function to be in the corresponding RKHS.,6. Experiments,[0],[0]
"Finally, the training set is labeled using yi = a · xi.
",6. Experiments,[0],[0]
During training we do not assume that the algorithms have access to x.,6. Experiments,[0],[0]
"Rather they can uniformly sample coordinates from it, which mimics our setting of random features.",6. Experiments,[0],[0]
"For the experiment we take D = 550, 600, . . .",6. Experiments,[0],[0]
", 800 and T = 200.",6. Experiments,[0],[0]
"All algorithms perform one pass over the data, to emulate the online regret setting.",6. Experiments,[0],[0]
The results shown in Figure 1 show that our method indeed achieves a lower loss while working with the same feature budget.,6. Experiments,[0],[0]
We presented a new online algorithm that employs kernels implicitly but avoids the kernel trick assumption.,7. Discussion,[0],[0]
"Namely, the algorithm can be invoked even when one has access to only estimations of the scalar product.",7. Discussion,[0],[0]
"The problem was motivated by kernels resulting from neural nets, but it can of course be applied to any scalar product of the form we described.",7. Discussion,[0],[0]
"As an example of an interesting extension, consider a setting where a learner can observe an unbiased estimate of a coordinate in a kernel matrix, or alternatively the scalar product between any two observations.",7. Discussion,[0],[0]
"Our results imply that in this setting the above rates are applicable, and at least for the square loss, having no access to the true values in the kernel matrix is not necessarily prohibitive during training.
",7. Discussion,[0],[0]
"The results show that with sample size T we can achieve error of O( B√
T ).",7. Discussion,[0],[0]
"As demonstrated in Shamir (2015) these
rates are optimal, even when the scalar product is computable.",7. Discussion,[0],[0]
To achieve this rate our algorithm needs to perform Õ(B4T 2) scalar product estimations.,7. Discussion,[0],[0]
"When the scalar product can be computed, existing kernelized algorithms need to observe a fixed proportion of the kernel matrix, hence they observe order of Ω(T 2) scalar products.",7. Discussion,[0],[0]
"In Cesa-Bianchi et al. (2015) it was shown that when the scalar product can be computed exactly, one would need access to at least Ω(T ) entries to the kernel matrix.",7. Discussion,[0],[0]
It is still an open problem whether one has to access Ω(T 2) entries when the kernel can be computed exactly.,7. Discussion,[0],[0]
"However, as we show here, for fixed B even if the kernel can only be estimated Õ(T 2) estimations are enough.",7. Discussion,[0],[0]
"It would be interesting to further investigate and improve the performance of our algorithm in terms of the norm bound B.
To summarize, we have shown that the kernel trick is not strictly necessary in terms of sample complexity.",7. Discussion,[0],[0]
"Instead, simply sampling random features via our proposed algorithm results in a similar sample complexity.",7. Discussion,[0],[0]
"Recent empirical results by Zhang et al. (2016) show that using a large number of random features and regression comes close to the performance of the first successful multilayer CNNs (Krizhevsky et al., 2012) on CIFAR-10.",7. Discussion,[0],[0]
"Although deep learning architectures still substantially outperform random features, it is conceivable that with the right choice of random features, and scalable learning algorithms like we present here, considerable improvement in performance is possible.",7. Discussion,[0],[0]
In this section we provide concentration bounds for the estimation procedure in Algorithm 2.,A. Estimation Concentration Bounds,[0],[0]
Lemma 4.,A. Estimation Concentration Bounds,[0],[0]
"Run Algorithm 2 with α and, {xi}Ti=1, x, and m. Let f = ∑ αiΦ(xi).",A. Estimation Concentration Bounds,[0],[0]
Assume that |ψ(x;w)| < 1 for all w and x.,A. Estimation Concentration Bounds,[0],[0]
Let E be the output of Algorithm 2.,A. Estimation Concentration Bounds,[0],[0]
"Then E is an unbiased estimator for 〈f,Φ(x)〉 and:
P [|E − 〈f,Φ(x)〉| > ] ≤ exp ( − m 2
‖α‖21
) (19)
Proof.",A. Estimation Concentration Bounds,[0],[0]
Consider the random variables ‖α‖1E(k),A. Estimation Concentration Bounds,[0],[0]
(where E(k) is as defined in Algorithm 2) and note that they are IID.,A. Estimation Concentration Bounds,[0],[0]
One can show that E [ ‖α‖1E(k) ],A. Estimation Concentration Bounds,[0],[0]
"=∑
αiE",A. Estimation Concentration Bounds,[0],[0]
"[ψ(xi;w)ψ(x;w)] = 〈f,Φ(x)〉.",A. Estimation Concentration Bounds,[0],[0]
By the bound on ψ(x;w),A. Estimation Concentration Bounds,[0],[0]
we have that ∣∣‖α‖1E(k)∣∣ < ‖α‖1 with probability 1.,A. Estimation Concentration Bounds,[0],[0]
"Since E = 1m ∑ E(k) the result follows directly from Hoeffding’s inequality.
",A. Estimation Concentration Bounds,[0],[0]
"Next, we bound the α(t) coeffcients and obtain a concentration bound for the estimated dot product Et.",A. Estimation Concentration Bounds,[0],[0]
Lemma 5.,A. Estimation Concentration Bounds,[0],[0]
"The α(t) obtained in Algorithm 1 satisfies:
‖α(t)‖1 ≤ (16B + 1)ηt.
",A. Estimation Concentration Bounds,[0],[0]
"As a corollary of this and Lemma 4 we have that the function ft satisfies:
",A. Estimation Concentration Bounds,[0],[0]
P,A. Estimation Concentration Bounds,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| >",A. Estimation Concentration Bounds,[0],[0]
"] ≤ exp ( −
2m
((16B + 1)ηt)2 ) (20)
Proof.",A. Estimation Concentration Bounds,[0],[0]
We prove the statement by induction.,A. Estimation Concentration Bounds,[0],[0]
"We separate into two cases, depending on whether the shrinkage step was performed or not.
",A. Estimation Concentration Bounds,[0],[0]
"If |Et| ≥ 16B the algorithm sets α(t+1) = 14α (t), and:
‖α(t+1)‖1 = 1
4 ‖α(t)‖1 ≤ (16B + 1)η(t+ 1)
",A. Estimation Concentration Bounds,[0],[0]
If |Et| < 16B the gradient update is performed.,A. Estimation Concentration Bounds,[0],[0]
Since |yt| ≤ 1 we have that |Et,A. Estimation Concentration Bounds,[0],[0]
−,A. Estimation Concentration Bounds,[0],[0]
"yt| < 16B + 1 and:
‖α(t+1)‖1 ≤ ‖α(t)‖1 + η|Et",A. Estimation Concentration Bounds,[0],[0]
− yi| ≤ (16B + 1)η(t+ 1).,A. Estimation Concentration Bounds,[0],[0]
B.1.,B. Proofs of Lemmas,[0],[0]
"Proof of Lemma 1
First, by convexity we have that
2(`t(ft)− `t(f∗))",B. Proofs of Lemmas,[0],[0]
"≤ 2 〈∇t, ft − f∗〉 .",B. Proofs of Lemmas,[0],[0]
"(21)
",B. Proofs of Lemmas,[0],[0]
"Next we upper bound 〈∇t, ft − f∗〉.",B. Proofs of Lemmas,[0],[0]
Denote by E the event ‖ft+1,B. Proofs of Lemmas,[0],[0]
− f∗‖ > ‖f̂t+1,B. Proofs of Lemmas,[0],[0]
"− f∗‖. Note that:
E [ ‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖2 ] ≤ E [ ‖f̂t+1,B. Proofs of Lemmas,[0],[0]
"− f∗‖2 ] +
E [ ‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖2 ∣∣E] · Pt+1(f∗) ≤ E,B. Proofs of Lemmas,[0],[0]
"[ ‖f̂t+1 − f∗‖2 ] + (B +BT ) 2Pt+1(f ∗)
",B. Proofs of Lemmas,[0],[0]
"Plugging in f̂t+1 = ft − η∇̄t, summing over t and using Eq. 21 and E",B. Proofs of Lemmas,[0],[0]
"[ ‖∇̄t‖2 ] = E [ ‖∇t‖2 ] + V [ ∇̄t ] , we obtain the desired result.
",B. Proofs of Lemmas,[0],[0]
B.2.,B. Proofs of Lemmas,[0],[0]
"Proof for Lemma 2
To prove the bound in the lemma, we first bound the event Pt(f ∗)",B. Proofs of Lemmas,[0],[0]
"w.r.t to two possible events:
Lemma 6.",B. Proofs of Lemmas,[0],[0]
Consider the setting as in Lemma 2.,B. Proofs of Lemmas,[0],[0]
"Run Algorithm 1 and for each t consider the following two events:
• Et1 : |Et| > 16B and |Et| > 14η‖ft‖.
• Et2 : |Et| > 16B and ‖ft‖ < 8B.
For every ‖f∗‖",B. Proofs of Lemmas,[0],[0]
<,B. Proofs of Lemmas,[0],[0]
B,B. Proofs of Lemmas,[0],[0]
we have that Pt(f∗),B. Proofs of Lemmas,[0],[0]
< P,B. Proofs of Lemmas,[0],[0]
"[Et1 ∪ Et2].
",B. Proofs of Lemmas,[0],[0]
Proof.,B. Proofs of Lemmas,[0],[0]
Denote the event |Et| > 16B by Et0.,B. Proofs of Lemmas,[0],[0]
"Note that if Et0 does not happen, then ft = f̂t.",B. Proofs of Lemmas,[0],[0]
"Hence trivially
Pt(f ∗)",B. Proofs of Lemmas,[0],[0]
= P [ ‖ft,B. Proofs of Lemmas,[0],[0]
− f∗‖ > ‖f̂t − f∗‖ ∧ Et0 ],B. Proofs of Lemmas,[0],[0]
"We will assume that: (1) |Et| > 16B., (2) |Et| < 14η‖ft‖., (3) ‖ft‖",B. Proofs of Lemmas,[0],[0]
>,B. Proofs of Lemmas,[0],[0]
8B.,B. Proofs of Lemmas,[0],[0]
"We then show ‖ft+1−f∗‖ ≤ ‖f̂t+1−f∗‖.
",B. Proofs of Lemmas,[0],[0]
"In other words, we will show that if Et0 happens and ‖ft+1− f∗‖ > ‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖, then either Et2 or Et1 happened.",B. Proofs of Lemmas,[0],[0]
"This will conclude the proof.
",B. Proofs of Lemmas,[0],[0]
"Fix t, note that since |ψ(x;w)| < 1 we have that ‖Φ(x)‖ < 1.",B. Proofs of Lemmas,[0],[0]
"We then have:
‖f̂t+1‖ = ‖ft",B. Proofs of Lemmas,[0],[0]
− η(Et,B. Proofs of Lemmas,[0],[0]
"− y)Φ(xt)‖ (22)
≥ ‖ft‖ −",B. Proofs of Lemmas,[0],[0]
"η|Et| − η ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η
where the last inequality is due to assumption (2) above.",B. Proofs of Lemmas,[0],[0]
"We therefore have the following for every ‖f∗‖ < B:
‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖ ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η −B
On the other hand, if ft+1 6= f̂t+1 then by construction of the algorithm ft+1 = 14ft:
‖ft+1",B. Proofs of Lemmas,[0],[0]
− f∗‖ ≤ ‖ft+1‖+ ‖f∗‖ ≤,B. Proofs of Lemmas,[0],[0]
"‖ft‖
4 +B.
",B. Proofs of Lemmas,[0],[0]
Next note that η < 2B and assumption (3) states ‖ft‖ >,B. Proofs of Lemmas,[0],[0]
"8B. Therefore: 12‖ft‖ > 4B > η + 2B, and:
‖f̂t+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖ ≥ 3
4 ‖ft‖",B. Proofs of Lemmas,[0],[0]
"− η −B
= 1
4 ‖ft‖+
( 1
2 ‖ft‖",B. Proofs of Lemmas,[0],[0]
− η,B. Proofs of Lemmas,[0],[0]
"− 2B
)",B. Proofs of Lemmas,[0],[0]
"+B
≥ 1 4 ‖ft‖+B ≥ ‖ft+1",B. Proofs of Lemmas,[0],[0]
"− f∗‖
Next we upper bound P [Et1 ∪ Et2].",B. Proofs of Lemmas,[0],[0]
In what follows the superscript t is dropped.,B. Proofs of Lemmas,[0],[0]
"A bound for P [E1 ∩ Ec2 ]: Assume that
|Et − 〈ft,Φ(xt)〉|",B. Proofs of Lemmas,[0],[0]
<,B. Proofs of Lemmas,[0],[0]
"( 1
4η − 1)8B.
We assume T is sufficiently large and η < 18 .",B. Proofs of Lemmas,[0],[0]
We have 1 4η − 1 > 1.,B. Proofs of Lemmas,[0],[0]
"Since we assume E2 did not happen we must have ‖ft‖ > 8B and |Et − 〈ft,Φ(xt)〉| <",B. Proofs of Lemmas,[0],[0]
"( 14η − 1)‖f‖, and therefore:
Et − ‖f‖ <",B. Proofs of Lemmas,[0],[0]
"|Et − 〈ft,Φ(xt)〉| <",B. Proofs of Lemmas,[0],[0]
"( 1
4η − 1)‖f‖.
Which implies Et < 14η‖f‖, and we get that E1 did not happen.",B. Proofs of Lemmas,[0],[0]
"We conclude that if E1 and not E2 then:
|Et − 〈ft,Φ(xt)〉| ≥ ( 1
4η − 1)8B.
Since 14η − 1 > 1 we have that: |Et − 〈ft,Φ(xt)〉| ≥ 8B, leading to:
P [E1 ∩ Ec2 ]",B. Proofs of Lemmas,[0],[0]
≤ P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥ 8B] .",B. Proofs of Lemmas,[0],[0]
"(23)
A bound for P [E2]: If |Et| > 16B and ‖ft‖ < 8B then by normalization of Φ(xt)",B. Proofs of Lemmas,[0],[0]
"we have that 〈ft,Φ(xt)〉 < 8B and trivially we have that |Et − 〈ft,Φ(xt)〉| ≥ 8B, and therefore:
",B. Proofs of Lemmas,[0],[0]
P [E2] ≤ P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥ 8B] .",B. Proofs of Lemmas,[0],[0]
"(24)
Taking Eq. 23 and Eq. 24 we have that
P [E2 ∪ E1] ≤",B. Proofs of Lemmas,[0],[0]
2P,B. Proofs of Lemmas,[0],[0]
"[|Et − 〈ft,Φ(xt)〉| ≥",B. Proofs of Lemmas,[0],[0]
8B] .,B. Proofs of Lemmas,[0],[0]
"(25)
By Lemma 5 we have that:
P (|Et − 〈ft,Φ(xt)〉|) > 8B)",B. Proofs of Lemmas,[0],[0]
"<
exp(− m(8B) 2
((16B + 1)ηt)2 ) < exp
( − m
(3ηt)2 )",B. Proofs of Lemmas,[0],[0]
"Taking the above upper bounds together with Lemma 6 we can prove Lemma 2.
B.3.",B. Proofs of Lemmas,[0],[0]
"Proof of Lemma 3
Begin by noting that since ‖Φ(x)‖ < 1, it follows from the definitions of ∇, ∇̄ that V [ ∇̄t ]",B. Proofs of Lemmas,[0],[0]
"= E [ ‖∇̄t −∇t‖2 ] and therefore
V [ ∇̄t ] ≤",B. Proofs of Lemmas,[0],[0]
"E [ (Et − 〈ft,Φ(xt)〉)2 ] = V [Et]
By construction (see Algorithm 2) we have that:
V [Et] = 1 m V [ ‖α(t)‖21ψ(xi;w)ψ(xt;w) ] where the index i is sampled as in Algorithm 2, and ψ(xi;w)ψ(xt;w) is bounded by 1.",B. Proofs of Lemmas,[0],[0]
"By Lemma 5 we have that
V [Et] ≤ ((16B + 1)ηt)2
m .",B. Proofs of Lemmas,[0],[0]
This provides the required bound on V [ ∇̄t ] .,B. Proofs of Lemmas,[0],[0]
"Additionally, we have that
‖∇t‖2 = (〈ft,Φ(xt)〉 − yt)2‖Φ(xt)‖2 ≤ 2`t(ft)
and the result follows by taking expectation.
",B. Proofs of Lemmas,[0],[0]
Acknowledgements The authors would like to thank Tomer Koren for helpful discussions.,B. Proofs of Lemmas,[0],[0]
Roi Livni was supported by funding from Eric and Wendy Schmidt Fund for Strategic Innovation.,B. Proofs of Lemmas,[0],[0]
"This work was supported by the Blavatnik Computer Science Research Fund, the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI), and an ISF Centers of Excellence grant.",B. Proofs of Lemmas,[0],[0]
Infinite Layer Networks (ILN) have been proposed as an architecture that mimics neural networks while enjoying some of the advantages of kernel methods.,abstractText,[0],[0]
ILN are networks that integrate over infinitely many nodes within a single hidden layer.,abstractText,[0],[0]
"It has been demonstrated by several authors that the problem of learning ILN can be reduced to the kernel trick, implying that whenever a certain integral can be computed analytically they are efficiently learnable.",abstractText,[0],[0]
"In this work we give an online algorithm for ILN, which avoids the kernel trick assumption.",abstractText,[0],[0]
"More generally and of independent interest, we show that kernel methods in general can be exploited even when the kernel cannot be efficiently computed but can only be estimated via sampling.",abstractText,[0],[0]
"We provide a regret analysis for our algorithm, showing that it matches the sample complexity of methods which have access to kernel values.",abstractText,[0],[0]
"Thus, our method is the first to demonstrate that the kernel trick is not necessary, as such, and random features suffice to obtain comparable performance.",abstractText,[0],[0]
Learning Infinite Layer Networks Without the Kernel Trick,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1492–1502 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Semantic parsing aims to automatically predict formal representations of meaning underlying natural language, and has been useful in question answering (Shen and Lapata, 2007), text-to-scene generation (Coyne et al., 2012), dialog systems (Chen et al., 2013) and social-network extraction (Agarwal et al., 2014), among others.",1 Introduction,[0],[0]
"Various formal meaning representations have been developed corresponding to different semantic theories (Fillmore, 1982; Palmer et al., 2005; Flickinger et al., 2012; Banarescu et al., 2013).",1 Introduction,[0],[0]
"The distributed nature of these efforts results in a set of annotated resources that are similar in spirit, but not strictly compatible.",1 Introduction,[0],[0]
"A major axis of structural divergence in semantic formalisms is whether based on spans (Baker et al., 1998; Palmer et al., 2005) or dependencies (Surdeanu et al., 2008; Oepen et al., 2014; Banarescu et al., 2013; Copestake et al., 2005, inter alia).",1 Introduction,[0],[0]
"Depending on application requirements, either might be most useful in a given situation.
",1 Introduction,[0],[0]
"Learning from a union of these resources seems promising, since more data almost always translates into better performance.",1 Introduction,[0],[0]
"This is indeed the case for two prior techniques—parameter sharing
(FitzGerald et al., 2015; Kshirsagar et al., 2015), and joint decoding across multiple formalisms using cross-task factors that score combinations of substructures from each (Peng et al., 2017).",1 Introduction,[0],[0]
"Parameter sharing can be used in a wide range of multitask scenarios, when there is no data overlap or even any similarity between the tasks (Collobert and Weston, 2008; Søgaard and Goldberg, 2016).",1 Introduction,[0],[0]
"But techniques involving joint decoding have so far only been shown to work for parallel annotations of dependency-based formalisms, which are structurally very similar to each other (Lluı́s et al., 2013; Peng et al., 2017).",1 Introduction,[0],[0]
"Of particular interest is the approach of Peng et al., where three kinds of semantic graphs are jointly learned on the same input, using parallel annotations.",1 Introduction,[0],[0]
"However, as new annotation efforts cannot be expected to use the same original texts as earlier efforts, the utility of this approach is limited.
",1 Introduction,[0],[0]
"We propose an extension to Peng et al.’s formulation which addresses this limitation by considering disjoint resources, each containing only a single kind of annotation.",1 Introduction,[0],[0]
"Moreover, we consider structurally divergent formalisms, one dealing with semantic spans and the other with semantic
1492
dependencies.",1 Introduction,[0],[0]
"We experiment on frame-semantic parsing (Gildea and Jurafsky, 2002; Das et al., 2010), a span-based semantic role labeling (SRL) task (§2.1), and on a dependency-based minimum recursion semantic parsing (DELPH-IN MRS, or DM; Flickinger et al., 2012) task (§2.2).",1 Introduction,[0],[0]
"See Figure 1 for an example sentence with gold FrameNet annotations, and author-annotated DM representations.
",1 Introduction,[0],[0]
"Our joint inference formulation handles missing annotations by treating the structures that are not present in a given training example as latent variables (§3).1 Specifically, semantic dependencies are treated as a collection of latent variables when training on FrameNet examples.
",1 Introduction,[0],[0]
"Using this latent variable formulation, we present an approach for relating spans and dependencies, by explicitly scoring affinities between pairs of potential spans and dependencies.",1 Introduction,[0],[0]
"Because there are a huge number of such pairs, we limit our consideration to only certain pairs—our design is inspired by the head rules of Surdeanu et al. (2008).",1 Introduction,[0],[0]
Further possible span-dependency pairs are pruned using an `1-penalty technique adapted from sparse structure learning (§5).,1 Introduction,[0],[0]
"Neural network architectures are used to score framesemantic structures, semantic dependencies, as well as cross-task structures (§4).
",1 Introduction,[0],[0]
"To summarize, our contributions include: • using a latent variable formulation to ex-
tend cross-task scoring techniques to scenarios where datasets do not overlap; • learning cross-task parts across structurally
divergent formalisms; and • using an `1-penalty technique to prune the
space of cross task parts.",1 Introduction,[0],[0]
"Our approach results in a new state-of-the-art in frame-semantic parsing, improving prior work by 0.8% absolute F1 points (§6), and achieves competitive performance on semantic dependency parsing.",1 Introduction,[0],[0]
Our code is available at https://github.com/Noahs-ARK/ NeurboParser.,1 Introduction,[0],[0]
"We describe the two tasks addressed in this work—frame-semantic parsing (§2.1) and semantic dependency parsing (§2.2)—and discuss how
1Following past work on support vector machines with latent variables (Yu and Joachims, 2009), we use the term “latent variable,” even though the model is not probabilistic.
",2 Tasks and Related Work,[0],[0]
their structures relate to each other (§2.3).,2 Tasks and Related Work,[0],[0]
"Frame-semantic parsing is a span-based task, under which certain words or phrases in a sentence evoke semantic frames.",2.1 Frame-Semantic Parsing,[0],[0]
"A frame is a group of events, situations, or relationships that all share the same set of participant and attribute types, called frame elements or roles.",2.1 Frame-Semantic Parsing,[0],[0]
"Gold supervision for frame-semantic parses comes from the FrameNet lexicon and corpus (Baker et al., 1998).
",2.1 Frame-Semantic Parsing,[0],[0]
"Concretely, for a given sentence, x, a framesemantic parse y consists of:
• a set of targets, each being a short span (usually a single token2) that evokes a frame; • for each target t, the frame f that it evokes;
and • for each frame f , a set of non-overlapping ar-
gument spans in the sentence, each argument a = (i, j, r) having a start token index",2.1 Frame-Semantic Parsing,[0],[0]
"i, end token index j and role label r.
The lemma and part-of-speech tag of a target comprise a lexical unit (or LU).",2.1 Frame-Semantic Parsing,[0],[0]
"The FrameNet ontology provides a mapping from an LU ` to the set of possible frames it could evoke, F`.",2.1 Frame-Semantic Parsing,[0],[0]
"Every frame f ∈ F` is also associated with a set of roles, Rf under this ontology.",2.1 Frame-Semantic Parsing,[0],[0]
"For example, in Figure 1, the LU “fall.v” evokes the frame MOTION DIRECTIONAL.",2.1 Frame-Semantic Parsing,[0],[0]
"The roles THEME and PLACE (which are specific to MOTION DIRECTIONAL), are filled by the spans “Only a few books” and “in the reading room” respectively.",2.1 Frame-Semantic Parsing,[0],[0]
"LOCATIVE RELATION has other roles (PROFILED REGION, ACCESSIBILITY, DEIXIS, etc.) which are not realized in this sentence.
",2.1 Frame-Semantic Parsing,[0],[0]
"In this work, we assume gold targets and LUs are given, and parse each target independently, following the literature (Johansson and Nugues, 2007; FitzGerald et al., 2015; Yang and Mitchell, 2017; Swayamdipta et al., 2017, inter alia).",2.1 Frame-Semantic Parsing,[0],[0]
"Moreover, following Yang and Mitchell (2017), we perform frame and argument identification jointly.",2.1 Frame-Semantic Parsing,[0],[0]
"Most prior work has enforced the constraint that a role may be filled by at most one argument span, but following Swayamdipta et al. (2017) we do not impose this constraint, requiring only that arguments for the same target do not overlap.
",2.1 Frame-Semantic Parsing,[0],[0]
296.5% of targets in the training data are single tokens.,2.1 Frame-Semantic Parsing,[0],[0]
"Broad-coverage semantic dependency parsing (SDP; Oepen et al., 2014, 2015, 2016) represents sentential semantics with labeled bilexical dependencies.",2.2 Semantic Dependency Parsing,[0],[0]
"The SDP task mainly focuses on three semantic formalisms, which have been converted to dependency graphs from their original annotations.",2.2 Semantic Dependency Parsing,[0],[0]
"In this work we focus on only the DELPHIN MRS (DM) formalism.
",2.2 Semantic Dependency Parsing,[0],[0]
"Each semantic dependency corresponds to a labeled, directed edge between two words.",2.2 Semantic Dependency Parsing,[0],[0]
"A single token is also designated as the top of the parse, usually indicating the main predicate in the sentence.",2.2 Semantic Dependency Parsing,[0],[0]
"For example in Figure 1, the left-most arc has head “Only”, dependent “few”, and label arg1.",2.2 Semantic Dependency Parsing,[0],[0]
"In semantic dependencies, the head of an arc is analogous to the target in frame semantics, the destination corresponds to the argument, and the label corresponds to the role.",2.2 Semantic Dependency Parsing,[0],[0]
"The same set of labels are available for all arcs, in contrast to the frame-specific roles in FrameNet.",2.2 Semantic Dependency Parsing,[0],[0]
"Early semantic role labeling was span-based (Gildea and Jurafsky, 2002; Toutanova et al., 2008, inter alia), with spans corresponding to syntactic constituents.",2.3 Spans vs. Dependencies,[0],[0]
"But, as in syntactic parsing, there are sometimes theoretical or practical reasons to prefer dependency graphs.",2.3 Spans vs. Dependencies,[0],[0]
"To this end, Surdeanu et al. (2008) devised heuristics based on syntactic head rules (Collins, 2003) to transform PropBank (Palmer et al., 2005) annotations into dependencies.",2.3 Spans vs. Dependencies,[0],[0]
"Hence, for PropBank at least, there is a very direct connection (through syntax) between spans and dependencies.
",2.3 Spans vs. Dependencies,[0],[0]
"For many other semantic representations, such a direct relationship might not be present.",2.3 Spans vs. Dependencies,[0],[0]
"Some semantic representations are designed as graphs from the start (Hajič et al., 2012; Banarescu et al., 2013), and have no gold alignment to spans.",2.3 Spans vs. Dependencies,[0],[0]
"Conversely, some span-based formalisms are not annotated with syntax (Baker et al., 1998; He et al., 2015),3 and so head rules would require using (noisy and potentially expensive) predicted syntax.
",2.3 Spans vs. Dependencies,[0],[0]
"Inspired by the head rules of Surdeanu et al. (2008), we design cross-task parts, without relying
3 In FrameNet, phrase types of arguments and their grammatical function in relation to their target have been annotated.",2.3 Spans vs. Dependencies,[0],[0]
"But in order to apply head rules, the internal structure of arguments (or at least their semantic heads) would also require syntactic annotations.
on gold or predicted syntax (which may be either unavailable or error-prone) or on heuristics.",2.3 Spans vs. Dependencies,[0],[0]
"Given an input sentence x, and target t with its LU `, denote the set of valid frame-semantic parses (§2.1) as Y(x, t, `), and valid semantic dependency parses as Z(x).4 We learn a parameterized function S that scores candidate parses.",3 Model,[0],[0]
"Our goal is to jointly predict a frame-semantic parse and a semantic dependency graph by selecting the highest scoring candidates:
",3 Model,[0],[0]
"(ŷ, ẑ) = arg max (y,z)∈Y(x,t,`)×Z(x) S(y, z,x, t, `).",3 Model,[0],[0]
"(1)
The overall score S can be decomposed into the sum of frame SRL score Sf, semantic dependency score Sd, and a cross-task score",3 Model,[0],[0]
"Sc:
S(y, z,x, t, `) = Sf(y,x, t, `) + Sd(z,x)
",3 Model,[0],[0]
"+Sc(y, z,x, t,`).",3 Model,[0],[0]
"(2)
Sf and Sc require access to the target and LU, in addition to x, but Sd does not.",3 Model,[0],[0]
"For clarity, we omit the dependence on the input sentence, target, and lexical unit, whenever the context is clear.",3 Model,[0],[0]
Below we describe how each of the scores is computed based on the individual parts that make up the candidate parses.,3 Model,[0],[0]
Frame SRL score.,3 Model,[0],[0]
"The score of a framesemantic parse consists of • the score for a predicate part, sf (p) where
each predicate is defined as a combination of a target t, the associated LU, `, and the frame evoked by the LU, f ∈ F`; • the score for argument parts, sf (a), each as-
sociated with a token span and semantic role fromRf .
",3 Model,[0],[0]
"Together, this results in a set of frame-semantic parts of size O(n2 |F`| |Rf |).5",3 Model,[0],[0]
"The score for a frame semantic structure y is the sum of local scores of parts in y:
Sf(y) = ∑
yi∈y sf(yi).",3 Model,[0],[0]
"(3)
The computation of sf is described in §4.2.",3 Model,[0],[0]
"4For simplicity, we consider only a single target here; handling of multiple targets is discussed in §6.",3 Model,[0],[0]
"5With pruning (described in §6) we reduce this to a number of parts linear in n. Also, |F`| is usually small (averaging 1.9), as is |Rf | (averaging 9.5).
",3 Model,[0],[0]
"includes include.v Inclusion Evidence to support this argument Total …
Figure 2: An example of cross-task parts from the FrameNet 1.5 development set.",3 Model,[0],[0]
We enumerate all unlabeled semantic dependencies from the first word of the target (includes) to any token inside the span.,3 Model,[0],[0]
"The red bolded arc indicates the prediction of our model.
",3 Model,[0],[0]
Semantic dependency score.,3 Model,[0],[0]
"Following Martins and Almeida (2014), we consider three types of parts in a semantic dependency graph: semantic heads, unlabeled semantic arcs, and labeled semantic arcs.",3 Model,[0],[0]
"Analogous to Equation 3, the score for a dependency graph z is the sum of local scores:
Sd(z) = ∑
zj∈z sd(zj), (4)
The computation of sd is described in §4.3.",3 Model,[0],[0]
Cross task score.,3 Model,[0],[0]
"In addition to task-specific parts, we introduce a set C of cross-task parts.",3 Model,[0],[0]
Each cross-task part relates an argument part from y to an unlabeled dependency arc from z.,3 Model,[0],[0]
"Based on the head-rules described in §2.3, we consider unlabeled arcs from the target to any token inside the span.6",3 Model,[0],[0]
"Intuitively, an argument in FrameNet would be converted into a dependency from its target to the semantic head of its span.",3 Model,[0],[0]
"Since we do not know the semantic head of the span, we consider all tokens in the span as potential modifiers of the target.",3 Model,[0],[0]
Figure 2 shows examples of crosstask parts.,3 Model,[0],[0]
"The cross-task score is given by
Sc(y, z) = ∑
(yi,zj)∈(y×z)∩C sc(yi, zj).",3 Model,[0],[0]
"(5)
The computation of sc is described in §4.4.",3 Model,[0],[0]
"In contrast to previous work (Lluı́s et al., 2013; Peng et al., 2017), where there are parallel annotations for all formalisms, our input sentences contain only one of the two—either the span-based frame SRL annotations, or semantic dependency graphs from DM.",3 Model,[0],[0]
"To handle missing annotations, we treat semantic dependencies z as latent when
6Most targets are single-words (§2.1).",3 Model,[0],[0]
"For multi-token targets, we consider only the first token, which is usually content-bearing.
",3 Model,[0],[0]
"decoding frame-semantic structures.7 Because the DM dataset we use does not have target annotations, we do not use latent variables for frame semantic structures when predicting semantic dependency graphs.",3 Model,[0],[0]
"The parsing problem here reduces to
ẑ = arg max z∈Z Sd(z), (6)
in contrast with Equation 1 .",3 Model,[0],[0]
This section describes the parametrization of the scoring functions from §3.,4 Parameterizations of Scores,[0],[0]
"At a very high level: we learn contextualized token and span vectors using a bidirectional LSTM (biLSTM; Graves, 2012) and multilayer perceptrons (MLPs) (§4.1); we learn lookup embeddings for LUs, frames, roles, and arc labels; and to score a part, we combine the relevant representations into a single scalar score using a (learned) low-rank multilinear mapping.",4 Parameterizations of Scores,[0],[0]
"Scoring frames and arguments is detailed in §4.2, that of dependency structures in §4.3, and §4.4 shows how to capture interactions between arguments and dependencies.",4 Parameterizations of Scores,[0],[0]
"All parameters are learned jointly, through the optimization of a multitask objective (§5).",4 Parameterizations of Scores,[0],[0]
Tensor notation.,4 Parameterizations of Scores,[0],[0]
The order of a tensor is the number of its dimensions—an order-2 tensor is a matrix and an order-1 tensor is a vector.,4 Parameterizations of Scores,[0],[0]
"Let ⊗ denote tensor product; the tensor product of two order-2 tensors A and B yields an order-4 tensor where (A ⊗B)i,j,k,l = Ai,jBk,l. We use 〈·, ·〉 to denote inner products.",4 Parameterizations of Scores,[0],[0]
"The representations of tokens and spans are formed using biLSTMs followed by MLPs.
",4.1 Token and Span Representations,[0],[0]
Contextualized token representations.,4.1 Token and Span Representations,[0],[0]
Each token in the input sentence x is mapped to an embedding vector.,4.1 Token and Span Representations,[0],[0]
"Two LSTMs (Hochreiter and Schmidhuber, 1997) are run in opposite directions over the input vector sequence.",4.1 Token and Span Representations,[0],[0]
"We use the concatenation of the two hidden representations at each position i as a contextualized word embedding for each token:
hi =",4.1 Token and Span Representations,[0],[0]
[−→ h i; ←− h i ] .,4.1 Token and Span Representations,[0],[0]
"(7)
7Semantic dependency parses over a sentence are not constrained to be identical for different frame-semantic targets.
",4.1 Token and Span Representations,[0],[0]
Span representations.,4.1 Token and Span Representations,[0],[0]
"Following Lee et al. (2017), span representations are computed based on boundary word representations and discrete length and distance features.",4.1 Token and Span Representations,[0],[0]
"Concretely, given a target t and its associated argument a = (i, j, r) with boundary indices i and j, we compute three features φt(a) based on the length of a, and the distances from i and j to the start of t.",4.1 Token and Span Representations,[0],[0]
We concatenate the token representations at a’s boundary with the discrete features φt(a).,4.1 Token and Span Representations,[0],[0]
"We then use a two-layer tanh-MLP to compute the span representation:
gspan(i, j) = MLPspan",4.1 Token and Span Representations,[0],[0]
( [hi;hj ;φt(a)] ) .,4.1 Token and Span Representations,[0],[0]
"(8)
The target representation gtgt(t) is similarly computed using a separate MLPtgt, with a length feature but no distance features.",4.1 Token and Span Representations,[0],[0]
"As defined in §3, the representation for a predicate part incorporates representations of a target span, the associated LU and the frame evoked by the LU.",4.2 Frame and Argument Scoring,[0],[0]
"The score for a predicate part is given by a multilinear mapping:
gpred(f) =",4.2 Frame and Argument Scoring,[0],[0]
"gfr(f)⊗ gtgt(t)⊗ glu(`) (9a) sf(p) = 〈 W,gpred(f) 〉 , (9b)
where W is a low-rank order-3 tensor of learned parameters, and gfr(f) and glu(`) are learned lookup embeddings for the frame and LU.
",4.2 Frame and Argument Scoring,[0],[0]
"A candidate argument consists of a span and its role label, which in turn depends on the frame, target and LU.",4.2 Frame and Argument Scoring,[0],[0]
"Hence the score for argument part, a = (i, j, r) is given by extending definitions from Equation 9:
garg(a) =",4.2 Frame and Argument Scoring,[0],[0]
"gspan(i, j)⊗ grole(r), (10a) sf(a) = 〈 W⊗U,gpred(f)⊗ garg(a) 〉 , (10b)
where U is a low-rank order-2 tensor of learned parameters and grole(r) is a learned lookup embedding of the role label.",4.2 Frame and Argument Scoring,[0],[0]
"Local scores for dependencies are implemented with two-layer tanh-MLPs, followed by a final linear layer reducing the represenation to a single scalar score.",4.3 Dependency Scoring,[0],[0]
"For example, let u = i→j denote an unlabeled arc (ua).",4.3 Dependency Scoring,[0],[0]
"Its score is:
gua(u) = MLPua ( [hi;hj ] ) (11a)
sd(u) = w ua · gua(u), (11b)
where wua is a vector of learned weights.",4.3 Dependency Scoring,[0],[0]
"The scores for other types of parts are computed similarly, but with separate MLPs and weights.",4.3 Dependency Scoring,[0],[0]
"As shown in Figure 2, each cross-task part c consists of two first-order parts: a frame argument part a, and an unlabeled dependency part, u.",4.4 Cross-Task Part Scoring,[0],[0]
"The score for a cross-task part incorporates both:
sc (c) = 〈 W⊗U⊗V,gpred(f)⊗ garg(a)
⊗ wua ⊗ gua(u) 〉 , (12)
where V is a low-rank order-2 tensor of parameters.",4.4 Cross-Task Part Scoring,[0],[0]
"Following previous work (Lei et al., 2014; Peng et al., 2017), we construct the parameter tensors W, U, and V so as to upper-bound their ranks.",4.4 Cross-Task Part Scoring,[0],[0]
All parameters from the previous sections are trained using a max-margin training objective (§5.1).,5 Training and Inference,[0],[0]
"For inference, we use a linear programming procedure, and a sparsity-promoting penalty term for speeding it up (§5.2).",5 Training and Inference,[0],[0]
"Let y∗ denote the gold frame-semantic parse, and let δ (y,y∗) denote the cost of predicting y with respect to y∗.",5.1 Max-Margin Training,[0],[0]
"We optimize the latent structured hinge loss (Yu and Joachims, 2009), which gives a subdifferentiable upper-bound on δ:
L (y∗) = max (y,z)∈Y×Z {S (y, z) + δ",5.1 Max-Margin Training,[0],[0]
"(y,y∗)}
−max z∈Z {S (y∗, z)} .
",5.1 Max-Margin Training,[0],[0]
"(13)
",5.1 Max-Margin Training,[0],[0]
"Following Martins and Almeida (2014), we use a weighted Hamming distance as the cost function, where, to encourage recall, we use costs 0.6 for false negative predictions and 0.4 for false positives.",5.1 Max-Margin Training,[0],[0]
"Equation 13 can be evaluated by applying the same max-decoding algorithm twice—once with cost-augmented inference (Crammer et al., 2006), and once more keeping y∗ fixed.",5.1 Max-Margin Training,[0],[0]
"Training then aims to minimize the average loss over all training instances.8
Another potential approach to training a model on disjoint data would be to marginalize out the
8We do not use latent frame structures when decoding semantic dependency graphs (§3).",5.1 Max-Margin Training,[0],[0]
"Hence, the loss reduces to structured hinge (Tsochantaridis et al., 2004) when training on semantic dependencies.
latent structures and optimize the conditional loglikelihood (Naradowsky et al., 2012).",5.1 Max-Margin Training,[0],[0]
"Although max-decoding and computing marginals are both NP-hard in general graphical models, there are more efficient off-the-shelf implementations for approximate max-decoding, hence, we adopt a max-margin formulation.",5.1 Max-Margin Training,[0],[0]
"We formulate the maximizations in Equation 13 as 0–1 integer linear programs and use AD3 to solve them (Martins et al., 2011).",5.2 Inference,[0],[0]
"We only enforce a non-overlapping constraint when decoding FrameNet structures, so that the argument identification subproblem can be efficiently solved by a dynamic program (Kong et al., 2016; Swayamdipta et al., 2017).",5.2 Inference,[0],[0]
"When decoding semantic dependency graphs, we enforce the determinism constraint (Flanigan et al., 2014), where certain labels may appear on at most one arc outgoing from the same token.",5.2 Inference,[0],[0]
Inference speedup by promoting sparsity.,5.2 Inference,[0],[0]
"As discussed in §3, even after pruning, the number of within-task parts is linear in the length of the input sentence, so the number of cross-task parts is quadratic.",5.2 Inference,[0],[0]
This leads to potentially very slow inference.,5.2 Inference,[0],[0]
"We address this problem by imposing an `1 penalty on the cross-task part scores:
L ( y∗ )",5.2 Inference,[0],[0]
"+ λ ∑
(yi,zj)∈C
∣∣sc(yi, zj) ∣∣, (14)
where λ is a hyperparameter, set to 0.01 as a practical tradeoff between efficiency and development set performance.",5.2 Inference,[0],[0]
"Whenever the score for a crosstask part is driven to zero, that part’s score no longer needs to be considered during inference.",5.2 Inference,[0],[0]
"It is important to note that by promoting sparsity this way, we do not prune out any candidate solutions.",5.2 Inference,[0],[0]
"We are instead encouraging fewer terms in the scoring function, which leads to smaller, faster inference problems even though the space of feasible parses is unchanged.
",5.2 Inference,[0],[0]
"The above technique is closely related to a line of work in estimating the structure of sparse graphical models (Yuan and Lin, 2007; Friedman et al., 2008), where an `1 penalty is applied to the inverse covariance matrix in order to induce a smaller number of conditional dependencies between variables.",5.2 Inference,[0],[0]
"To the best of our knowledge, we are the first to apply this technique to the output of neural scoring functions.",5.2 Inference,[0],[0]
"Here, we are interested in learn-
ing sparse graphical models only because they result in faster inference, not because we have any a priori belief about sparsity.",5.2 Inference,[0],[0]
"This results in roughly a 14× speedup in our experiments, without any significant drop in performance.",5.2 Inference,[0],[0]
Datasets.,6 Experiments,[0],[0]
"Our model is evaluated on two different releases of FrameNet: FN 1.5 and FN 1.7,9 using splits from Swayamdipta et al. (2017).",6 Experiments,[0],[0]
"Following Swayamdipta et al. (2017) and Yang and Mitchell (2017), each target annotation is treated as a separate training instance.",6 Experiments,[0],[0]
"We also include as training data the exemplar sentences, each annotated for a single target, as they have been reported to improve performance (Kshirsagar et al., 2015; Yang and Mitchell, 2017).",6 Experiments,[0],[0]
"For semantic dependencies, we use the English DM dataset from the SemEval 2015 Task 18 closed track (Oepen et al., 2015).10 DM contains instances from the WSJ corpus for training and both in-domain (id) and out-of-domain (ood) test sets, the latter from the Brown corpus.11 Table 1 summarizes the sizes of the datasets.",6 Experiments,[0],[0]
Baselines.,6 Experiments,[0],[0]
We compare FN performance of our joint learning model (FULL) to two baselines: BASIC:,6 Experiments,[0],[0]
"A single-task frame SRL model, trained using a structured hinge objective.",6 Experiments,[0],[0]
NOCTP:,6 Experiments,[0],[0]
"A joint model without cross-task parts.
",6 Experiments,[0],[0]
It demonstrates the effect of sharing parameters in word embeddings and LSTMs (like in FULL).,6 Experiments,[0],[0]
"It does not use latent semantic dependency structures, and aims to minimize the sum of training losses from both tasks.
",6 Experiments,[0],[0]
"We also compare semantic dependency parsing performance against the single task model by Peng
9https://FN.icsi.berkeley.edu/ fndrupal/
10http://sdp.delph-in.net/. The closed track does not have access to any syntactic analyses.",6 Experiments,[0],[0]
"The impact of syntactic features on SDP performance is extensively studied in Ribeyre et al. (2015).
",6 Experiments,[0],[0]
11Our FN training data does not overlap with the DM test set.,6 Experiments,[0],[0]
"We remove the 3 training sentences from DM which appear in FN test data.
",6 Experiments,[0],[0]
"et al. (2017), denoted as NeurboParser (BASIC).",6 Experiments,[0],[0]
"To ensure fair comparison with our FULL model, we made several modifications to their implementation (§6.3).",6 Experiments,[0],[0]
"We observed performance improvements from our reimplementation, which can be seen in Table 5.",6 Experiments,[0],[0]
Pruning strategies.,6 Experiments,[0],[0]
"For frame SRL, we discard argument spans longer than 20 tokens (Swayamdipta et al., 2017).",6 Experiments,[0],[0]
"We further pretrain an unlabeled model and prune spans with posteriors lower than 1/n2, with n being the input sentence length.",6 Experiments,[0],[0]
"For semantic dependencies, we generally follow Martins and Almeida (2014), replacing their feature-rich pruner with neural networks.",6 Experiments,[0],[0]
"We observe that O(n) spans/arcs remain after pruning, with around 96% FN development recall, and more than 99% for DM.12",6 Experiments,[0],[0]
FN parsing results.,6.1 Empirical Results,[0],[0]
Table 2 compares our full frame-semantic parsing results to previous systems.,6.1 Empirical Results,[0],[0]
"Among them, Täckström et al. (2015) and Roth (2016) implement a two-stage pipeline and use the method from Hermann et al. (2014) to predict frames.",6.1 Empirical Results,[0],[0]
"FitzGerald et al. (2015) uses the
12On average, around 0.8n argument spans, and 5.7n unlabeled dependency arcs remain after pruning.
same pipeline formulation, but improves the frame identification of Hermann et al. (2014) with better syntactic features.",6.1 Empirical Results,[0],[0]
"open-SESAME (Swayamdipta et al., 2017) uses predicted frames from FitzGerald et al. (2015), and improves argument identification using a softmax-margin segmental RNN.",6.1 Empirical Results,[0],[0]
"They observe further improvements from product of experts ensembles (Hinton, 2002).
",6.1 Empirical Results,[0],[0]
The best published FN 1.5 results are due to Yang and Mitchell (2017).,6.1 Empirical Results,[0],[0]
Their relational model (REL) formulates argument identification as a sequence of local classifications.,6.1 Empirical Results,[0],[0]
They additionally introduce an ensemble method (denoted as ALL) to integrate the predictions of a sequential CRF.,6.1 Empirical Results,[0],[0]
They use a linear program to jointly predict frames and arguments at test time.,6.1 Empirical Results,[0],[0]
"As shown in Table 2, our single-model performance outperforms their REL model, and is on par with their ALL model.",6.1 Empirical Results,[0],[0]
"For a fair comparison, we build an ensemble (FULL, 2×) by separately training two models, differing only in random seeds, and averaging their part scores.",6.1 Empirical Results,[0],[0]
"Our ensembled model outperforms previous best results by 0.8% absolute.
",6.1 Empirical Results,[0],[0]
Table 3 compares our frame identification results with previous approaches.,6.1 Empirical Results,[0],[0]
Hermann et al. (2014) and Hartmann et al. (2017) use distributed word representations and syntax features.,6.1 Empirical Results,[0],[0]
"We follow the FULL LEXICON setting (Hermann et al., 2014) and extract candidate frames from the offi-
13Our comparison to Hermann et al. (2014) is based on their updated version: http://www.aclweb.org/ anthology/P/P14/P14-1136v2.pdf.",6.1 Empirical Results,[0],[0]
Ambiguous frame identification results by Yang and Mitchell (2017) and Hartmann et al. (2017) are 75.7 and 73.8.,6.1 Empirical Results,[0],[0]
"Their ambiguous lexical unit sets are different from the one extracted from the official frame directory, and thus the results are not comparable to those in Table 3.
cial directories.",6.1 Empirical Results,[0],[0]
The Ambiguous setting compares lexical units with more than one possible frames.,6.1 Empirical Results,[0],[0]
"Our approach improves over all previous models under both settings, demonstrating a clear benefit from joint learning.
",6.1 Empirical Results,[0],[0]
We observe similar trends on FN 1.7 for both full structure extraction and for frame identification only (Table 4).,6.1 Empirical Results,[0],[0]
FN 1.7 extends FN 1.5 with more consistent annotations.,6.1 Empirical Results,[0],[0]
"Its test set is different from that of FN 1.5, so the results are not directly comparable to Table 2.",6.1 Empirical Results,[0],[0]
"We are the first to report frame-semantic parsing results on FN 1.7, and we encourage future efforts to do so as well.
",6.1 Empirical Results,[0],[0]
Semantic dependency parsing results.,6.1 Empirical Results,[0],[0]
Table 5 compares our semantic dependency parsing performance on DM with the baselines.,6.1 Empirical Results,[0],[0]
Our reimplementation of the BASIC model slightly improves performance on in-domain test data.,6.1 Empirical Results,[0],[0]
"The NOCTP model ties parameters from word embeddings and LSTMs when training on FrameNet and DM, but does not use cross-task parts or joint prediction.",6.1 Empirical Results,[0],[0]
"NOCTP achieves similar in-domain test performance, and improves over BASIC on outof-domain data.",6.1 Empirical Results,[0],[0]
"By jointly predicting FrameNet
structures and semantic dependency graphs, the FULL model outperforms the baselines by more than 0.6% absolute F1 scores under both settings.
",6.1 Empirical Results,[0],[0]
"Previous state-of-the-art results on DM are due to the joint learning model of Peng et al. (2017), denoted as NeurboParser (FREDA3).",6.1 Empirical Results,[0],[0]
"They adopted a multitask learning approach, jointly predicting three different parallel semantic dependency annotations.",6.1 Empirical Results,[0],[0]
"Our FULL model’s in-domain test performance is on par with FREDA3, and improves over it by 0.6% absolute F1 on out-ofdomain test data.",6.1 Empirical Results,[0],[0]
Our ensemble of two FULL models achieves a new state-of-the-art in both indomain and out-of-domain test performance.,6.1 Empirical Results,[0],[0]
Error type breakdown.,6.2 Analysis,[0],[0]
"Similarly to He et al. (2017), we categorize prediction errors made by the BASIC and FULL models in Table 6.",6.2 Analysis,[0],[0]
"Entirely missing an argument accounts for most of the errors for both models, but we observe fewer errors by FULL compared to BASIC in this category.",6.2 Analysis,[0],[0]
"FULL tends to predict more arguments in general, including more incorrect arguments.
",6.2 Analysis,[0],[0]
"Since candidate roles are determined by frames, frame and role errors are highly correlated.",6.2 Analysis,[0],[0]
"Therefore, we also show the role errors when frames are correctly predicted (parenthesized numbers in the second row).",6.2 Analysis,[0],[0]
"When a predicted argument span matches a gold span, predicting the semantic role is less challenging.",6.2 Analysis,[0],[0]
"Role errors account for only around 13% of all errors, and half of them are due to mispredictions of frames.",6.2 Analysis,[0],[0]
Performance by argument length.,6.2 Analysis,[0],[0]
Figure 3 plots dev. precision and recall of both BASIC and FULL against binned argument lengths.,6.2 Analysis,[0],[0]
"We ob-
serve two trends: (a) FULL tends to predict longer arguments (averaging 3.2) compared to BASIC (averaging 2.9), while keeping similar precision;14 (b) recall improvement in FULL mainly comes from arguments longer than 4.",6.2 Analysis,[0],[0]
"Our implementation is based on DyNet (Neubig et al., 2017).15 We use predicted part-of-speech tags and lemmas using NLTK (Bird et al., 2009).16
Parameters are optimized with stochastic subgradient descent for up to 30 epochs, with `2 norms of gradients clipped to 1.",6.3 Implementation Details,[0],[0]
"We use 0.33 as initial learning rate, and anneal it at a rate of 0.5 every 10 epochs.",6.3 Implementation Details,[0],[0]
Early stopping is applied based on FN development F1.,6.3 Implementation Details,[0],[0]
"We apply logarithm with base 2 to all discrete features, e.g., log2(d+1) for distance feature valuing d. To speed up training, we randomly sample a 35% subset from the FN exemplar instances each epoch.",6.3 Implementation Details,[0],[0]
Hyperparameters.,6.3 Implementation Details,[0],[0]
"Each input token is represented as the concatenation a word embedding vector, a learned lemma vector, and a learned vector for part-of speech, all updated during training.",6.3 Implementation Details,[0],[0]
"We use 100-dimensional GloVe (Pennington et al., 2014) to initialize word embeddings.",6.3 Implementation Details,[0],[0]
"We apply word dropout (Iyyer et al., 2015) and randomly replace a word w with a special UNK symbol with probability α1+#(w) , with #(w) being the count of w in the training set.",6.3 Implementation Details,[0],[0]
"We follow the default parameters initialization procedure by DyNet, and an `2
14Average gold span length is 3.4 after discarding those longer than 20.
15https://github.com/clab/dynet 16http://www.nltk.org/
penalty of 10−6 is applied to all weights.",6.3 Implementation Details,[0],[0]
See Table 7 for other hyperparameters.,6.3 Implementation Details,[0],[0]
Modifications to Peng et al. (2017).,6.3 Implementation Details,[0],[0]
"To ensure fair comparisons, we note two implementation modifications to Peng et al.’s basic model.",6.3 Implementation Details,[0],[0]
"We use a more recent version (2.0) of the DyNet toolkit, and we use 50-dimensional lemma embeddings instead of their 25-dimensional randomly-initialized learned word embeddings.",6.3 Implementation Details,[0],[0]
We presented a novel multitask approach to learning semantic parsers from disjoint corpora with structurally divergent formalisms.,7 Conclusion,[0],[0]
"We showed how joint learning and prediction can be done with scoring functions that explicitly relate spans and dependencies, even when they are never observed together in the data.",7 Conclusion,[0],[0]
We handled the resulting inference challenges with a novel adaptation of graphical model structure learning to the deep learning setting.,7 Conclusion,[0],[0]
"We raised the state-ofthe-art on DM and FrameNet parsing by learning from both, despite their structural differences and non-overlapping data.",7 Conclusion,[0],[0]
"While our selection of factors is specific to spans and dependencies, our general techniques could be adapted to work with more combinations of structured prediction tasks.",7 Conclusion,[0],[0]
We have released our implementation at https://github.com/ Noahs-ARK/NeurboParser.,7 Conclusion,[0],[0]
"We thank Kenton Lee, Luheng He, and Rowan Zellers for their helpful comments, and the anonymous reviewers for their valuable feedback.",Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS1562364.,Acknowledgments,[0],[0]
"We present a new approach to learning semantic parsers from multiple datasets, even when the target semantic formalisms are drastically different, and the underlying corpora do not overlap.",abstractText,[0],[0]
We handle such “disjoint” data by treating annotations for unobserved formalisms as latent structured variables.,abstractText,[0],[0]
"Building on state-of-the-art baselines, we show improvements both in frame-semantic parsing and semantic dependency parsing by modeling them jointly.",abstractText,[0],[0]
Our code is open-source and available at https://github.com/ Noahs-ARK/NeurboParser.,abstractText,[0],[0]
Learning Joint Semantic Parsers from Disjoint Data,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3761–3771 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3761",text,[0],[0]
The meaning of natural language should always be accompanied by a context.,1 Introduction,[0],[0]
Grounded language acquisition aims at learning the meaning of language in the context of an observed world state.,1 Introduction,[0],[0]
"A solution framework typically addresses the following subproblems: segmenting the text into meaningful phrasal units, determining which world state information is being referred to, and finding proper alignments from these units to the events of values in the world state.
",1 Introduction,[0],[0]
"The task has attracted much attention from the NLP community with a special focus on aligning text descriptions onto processed, structured event records (Snyder and Barzilay, 2007; Liang et al., 2009; Hajishirzi et al., 2011).",1 Introduction,[0],[0]
"Various statistical models have been proposed, attempting at
∗ Contribution during internship at Microsoft Research Asia.
",1 Introduction,[0],[0]
"1Our implementation is available at https: //github.com/hiaoxui/D2T-Grounding.
characterizing the interaction between text spans and categorical values (e.g., direction=‘East’) or strings (e.g., person names).",1 Introduction,[0],[0]
"The previously addressed term semantic correspondences narrowly describes the process of aligning natural language spans to different data fields.
",1 Introduction,[0],[0]
"However, there still exists a gap between alignment results and the underlying semantics.",1 Introduction,[0],[0]
"People tend to use various phrases to describe information that are inferred from different amounts of numerical values in a data field, or values derived from additional operations over fields.",1 Introduction,[0],[0]
Consider the example description for a basketball game shown in Figure 1.,1 Introduction,[0],[0]
The phrase edged out in the first sentence implies the fact that the Toronto Raptors had beaten their opponent by a relatively narrow margin.,1 Introduction,[0],[0]
"This could only be derived from an operation of subtraction between two scores that correspond to the field PTS for both teams in the event table, which leads to a relatively small difference of only four points.",1 Introduction,[0],[0]
"Previous efforts on learning semantic correspondences relying on categorical distributions (Liang et al., 2009) or string pattern features (Hajishirzi et al., 2012; Koncel-Kedziorski et al., 2014) do not have the capability to accurately capture numerical information, especially for the part that does not appear explicitly in the table and needs to be inferred.
",1 Introduction,[0],[0]
Such kind of language grounding is important both for natural language understanding and for natural language generation.,1 Introduction,[0],[0]
"For language understanding, establishing explicit connections between symbols and values beyond ungrounded symbolic meaning representations will be useful for acquisition and inference of numerical commonsense (Narisawa et al., 2013).",1 Introduction,[0],[0]
"For language generation, properly aligned information is the key to acquiring patterns of various lexical choices under different world states (Roy and Reiter, 2005).
",1 Introduction,[0],[0]
"In this work, we make a step towards more explicit semantic correspondences between structured data and texts.",1 Introduction,[0],[0]
"Rather than only producing coarse alignments between data fields and text spans, we try to detect the latent semantics underlying these alignments by prompting explicit semantic annotations.",1 Introduction,[0],[0]
"We make the first attempt at utilizing publicly available datasets originally prepared for data-to-text language generation to produce such annotations for words and phrases in natural language without additional supervision.
",1 Introduction,[0],[0]
"Specifically, we conduct our study on a recently released dataset of descriptions for NBA basketball games with structured tables of game records.",1 Introduction,[0],[0]
"Different from a few popular datasets that have been well conjectured to be produced from rules (Reiter, 2017), the summaries are all written by humans.",1 Introduction,[0],[0]
"The text contains some numbers and proper nouns, which are easier to establish correspondence with data.",1 Introduction,[0],[0]
"However, the majority of texts contain many informative words, some of which need to be inferred indirectly from various types of values in the data cells.",1 Introduction,[0],[0]
"We want to establish explicit correspondences for them.
",1 Introduction,[0],[0]
We derive a set of semantic labels from original data fields (Sec 4.1).,1 Introduction,[0],[0]
These labels could be executed to establish direct correspondences to one or more values in the structured table.,1 Introduction,[0],[0]
No annotation on the original dataset means unsupervised learning from weak distant supervision should be conducted.,1 Introduction,[0],[0]
"We design a semi-hidden Markov model to address this problem (Sec 4.2), which could align a semantic label to a word span.",1 Introduction,[0],[0]
"In the model, we leverage continuous probability distributions to model the correspondences between numerical values and lexical terms, which has not been well captured in previous work.",1 Introduction,[0],[0]
"To address the emerged issue of “garbage collection” that commonly appears in statistical alignment models (Sec 4.5), we add a soft statistical con-
straint via posterior regularization (Ganchev et al., 2010).",1 Introduction,[0],[0]
"As a by-product, we also show how the derived semantic annotations could be used to induce descriptive templates for data-to-text generation (Sec 5).",1 Introduction,[0],[0]
"Experimental results (Sec 6) suggest the feasibility of the setting in this study, and show the effectiveness of our proposed framework.",1 Introduction,[0],[0]
"Grounded language acquisition has aroused wide interest in various disciplines (Siskind, 1996; Yu and Ballard, 2004; Gorniak and Roy, 2007; Yu and Siskind, 2013; Chrupała et al., 2015).",2 Related work,[0],[0]
"Later work in the community of natural language processing also moved in this direction by relaxing the amount of supervision to enable a model to learn from ambiguous alignments (Kate and Mooney, 2007; Chen and Mooney, 2008).",2 Related work,[0],[0]
"Some research aimed at establishing coarse alignments between simulated robot soccer game records and commentary sentences (Chen and Mooney, 2008; Chen et al., 2010; Bordes et al., 2010; Hajishirzi et al., 2011).",2 Related work,[0],[0]
"For weather forecast domain, Liang et al. (2009) used a hierarchical hidden Markov model in order to map utterances to world states, which coped with segmentation and alignment together.",2 Related work,[0],[0]
"More recently, Koncel-Kedziorski et al. (2014) tried to obtain the correspondences between real commentaries and structured football (soccer) events in multiple resolutions.",2 Related work,[0],[0]
We are distinct from this line of work in the fact that we aim at producing explicit semantic annotations that could capture information from structured tables.,2 Related work,[0],[0]
"To achieve this goal, we need additional scaling or operations to enable data fields and values to be faithfully mapped onto texts.",2 Related work,[0],[0]
This will address the issue of the lack of consideration for the relationship between lexical terms and numerical values.,2 Related work,[0],[0]
"Our approach makes a significant difference in that our framework could generalize to numerical values or value combinations that are unseen in training, and will not be simply reciting cooccurrence patterns of exact values in the training data.
",2 Related work,[0],[0]
Our work relates to learning executable semantic parsers under weak supervision.,2 Related work,[0],[0]
"Early semantic parsing started from fully supervised training with annotated meaning representations available (Zettlemoyer and Collins, 2005; Ge and Mooney, 2006; Snyder and Barzilay, 2007), but more recent work focused on reducing the amount of supervision required (Artzi and Zettlemoyer,
2013).",2 Related work,[0],[0]
"The intuition behind weakly supervised executable semantic parsing is that once the latent semantic representation has been executed, one could test whether the execution results could match the information with available weak supervision signals such as answers to natural language queries (Clarke et al., 2010; Liang et al., 2011), or task completion from instructional navigations (Misra et al., 2017).",2 Related work,[0],[0]
"Such formulations have been adapted for question answering over structured tables (Pasupat and Liang, 2015; Krishnamurthy et al., 2017).",2 Related work,[0],[0]
"However, the current research focus is to convert a natural language question into executable table queries and to directly retrieve results.",2 Related work,[0],[0]
They do not have the need of inference involving numerical commonsense implied by various lexical patterns.,2 Related work,[0],[0]
"A few unsupervised approaches exist (Poon and Domingos, 2009; Poon, 2013) but only specific to translating language into queries in the highly structured database and cannot be applied to our domain.
",2 Related work,[0],[0]
"Our approach is implemented as assigning tag annotations over text spans, which is conceptually related to fine-grained named entity tagging (Ling and Weld, 2012).",2 Related work,[0],[0]
Our setting only requires a rather weak and distant form of supervision from paired tables and texts without annotations for fine-grained alignments between phrases and data cells.,2 Related work,[0],[0]
"Similar modeling and learning strategies could potentially be useful for considerably large tag space derived from structured knowledge bases in the future (Choi et al., 2018).
",2 Related work,[0],[0]
"The feasibility of this work is partly due to the availability of data, mostly comes from the field of data-to-text language generation.",2 Related work,[0],[0]
"Related work in data-to-text generation mainly focused on directly generating summary descriptions for structured data (Mei et al., 2016; Kiddon et al., 2016; Murakami et al., 2017; Wiseman et al., 2017), without establishing underlying semantic correspondences.",2 Related work,[0],[0]
"Texts generated thereby can be fluent but not conforming to the input data, unlike templatebased approaches where lexical choices could be directly controlled.",2 Related work,[0],[0]
"In our work, we find the derived semantic correspondences between data and texts to be useful for template induction, either with simple heuristics to automatically extract description patterns (how to say) and corresponding triggers (what & when to say), or with more crafted discriminative learning approaches (cf.",2 Related work,[0],[0]
Angeli et al. (2010)).,2 Related work,[0],[0]
"Task Let S be the set of all world states, W be the set of all texts, O be the set of all executable operators, and V be the output space of O. A world state s ∈ S is a table storing some information, or more specifically in this work, a tabular recording for a sports game.",3 Technical overview,[0],[0]
An operator o ∈,3 Technical overview,[0],[0]
"O can be executed on a world state to retrieve values, i.e., each o could be treated as a mapping of S → V .",3 Technical overview,[0],[0]
"The result of an operation can be a string, a continuous values or a discrete value.",3 Technical overview,[0],[0]
"Meanwhile, each world state s is accompanied with a piece of description w ∈ W .",3 Technical overview,[0],[0]
Here w consists of a sequence of word tokens {wi ∈ w}.,3 Technical overview,[0],[0]
"We further define a segmentation variable π, which could convert w into a sequence of word spans c, containing each span of tokens ct ∈ c that could be interpreted as a phrase.",3 Technical overview,[0],[0]
"Note that we use superscript t to denote indices of phrases, and subscript i to denote indices of individual words.",3 Technical overview,[0],[0]
"We further define l as a sequence of latent labels, and value of each lt is an operator oi ∈",3 Technical overview,[0],[0]
O. 2,3 Technical overview,[0],[0]
"For each world state s and corresponding description w, we want to jointly find a proper segmentation π to obtain c, and assign labels to every word span ct.
",3 Technical overview,[0],[0]
"We conduct this study on the ROTOWIRE subset of the openly available dataset released by Wiseman et al. (2017), containing text descriptions for NBA basketball games with structured tables of game statistics.",3 Technical overview,[0],[0]
Take Figure 1 as an example.,3 Technical overview,[0],[0]
"The proper nouns (e.g. Toronto Raptors) appeared in the sentence can be assigned with a tag Team Name in our tag set, which could then be aligned to the Team Name field in the table.",3 Technical overview,[0],[0]
"Some numbers appearing in the text, such as 15 in the example, can be assigned with the label Team Losses, with the executable annotation to extract the number of previously lost matches of the mentioned team.",3 Technical overview,[0],[0]
What we are more interested in is where the phrase edge out comes from.,3 Technical overview,[0],[0]
"We are aiming at a model which is capable to capture the semantics behind the phrase edge out, which is used to describe an event that one team has beaten the other with close scores.",3 Technical overview,[0],[0]
"In our annotation scheme, this phrase should be assigned with the tag Team Points Delta, and executing that will return the score difference between two teams.
",3 Technical overview,[0],[0]
"The task is challenging in that there does not
2We will interchangeably use labels, operators, tags to refer to the latent executable semantic annotations in this paper.
exist any other additional supervision signal.",3 Technical overview,[0],[0]
The learning process will mostly rely on statistical cooccurrences of information between the structured data and its text descriptions.,3 Technical overview,[0],[0]
Note that we assume a consistent structure (schema) throughout the whole dataset upon which the learning process will be performed.,3 Technical overview,[0],[0]
"Model To jointly learn word segmentations c and latent semantic annotations l between world state s and text w in a unified framework, we propose a generative model to characterize the joint distribution Ps(l, π,w; θ), parameterized by θ.",3 Technical overview,[0],[0]
"Learning The data contain paired texts and tables only, thus our model must learn segmentations and latent semantic annotations in an unsupervised fashion.",3 Technical overview,[0],[0]
"The target is to maximize the complete data likelihood
L(θ) = ∏
(s,w)∈D ∑",3 Technical overview,[0],[0]
"l,π Ps(l, π,w; θ),
where D represents the whole training data.",3 Technical overview,[0],[0]
"To reduce the search space in inference and to capture some patterns of content planning in the text descriptions, we adopt a Markov assumption over phrase segments, which leads to a hidden semi-Markov model (semi-HMM) (Murphy, 2002; Sarawagi and Cohen, 2005).",3 Technical overview,[0],[0]
The key part is to characterize different types of correspondences (Sec 4.2).,3 Technical overview,[0],[0]
"We derive an expectation-maximization (EM) algorithm to perform maximum likelihood estimation, and introduce a soft statistical regularization to guide the model towards a better solution (Sec 4.5).",3 Technical overview,[0],[0]
"Inference Once the model has been trained, we use a Viterbi-like dynamic programming process to perform MAP inference to segment the texts and to assign the most likely tags for each span.",3 Technical overview,[0],[0]
We describe the process of how we derived our set of semantic annotations here.,4.1 The set of annotations,[0],[0]
"There are two kinds of specific tables for each NBA game in the dataset: Box-Scores and Line-Scores, respectively
showing the performance statistics for individual players and the whole teams.",4.1 The set of annotations,[0],[0]
"The types of possible values for data fields are strings, categorical values and numerical values.",4.1 The set of annotations,[0],[0]
"Box-Score consists of 24 fields, of which four are string-values, one is categorical, the other 19 being numerical.",4.1 The set of annotations,[0],[0]
"LineScore consists of 16 fields, containing two stringvalued, one categorical, and 13 numerical fields.",4.1 The set of annotations,[0],[0]
"A semantic tag works on a specified kind of field type, from either team statistics or player statistics.",4.1 The set of annotations,[0],[0]
"For a tag that could align to one single field in the table, we let it return the exact value in the cell that could maximize the likelihood.",4.1 The set of annotations,[0],[0]
"For example, the tag Team City is used to extract the name of the team in the table that corresponds to the current word span.",4.1 The set of annotations,[0],[0]
"For fields taking numerical values, we additionally allow tags to be able to perform mathematical operations, such as subtraction, between two values in the same field.",4.1 The set of annotations,[0],[0]
3,4.1 The set of annotations,[0],[0]
"For example, the tag Team Points Delta can be executed to return the score difference between two teams.",4.1 The set of annotations,[0],[0]
We list the different types of tags with examples shown in Table 1 and leave the entire tag set to Appendix A.,4.1 The set of annotations,[0],[0]
"Along with all these tags derived from the original data fields, we also include a special NULL tag which are supposed to be assigned to non-informative words or words containing information not contained in the given table.
",4.1 The set of annotations,[0],[0]
Note that we impose little prior knowledge in this step.,4.1 The set of annotations,[0],[0]
"We simply over-generate all possible labels, and let the model figure out which part of them should be eventually used.",4.1 The set of annotations,[0],[0]
"Although the only compositional operation we used in this work is numeric subtraction, common operations that could produce string, categorical values or numbers could be easily introduced for other domains.",4.1 The set of annotations,[0],[0]
"As previously mentioned, we will be modeling the joint distribution of word segmentation c and the latent semantic annotations l between paired world state s and text w, which could be factorized as:
Ps(l, π,w) = Ps(w, π|l) · Ps(l),
and we write Ps(w, π|l) as Ps(c|l).",4.2 Semi-HMMs with continuous values,[0],[0]
"In this section, we focus on the probability of the alignments between word spans and labels, namely, Ps(c|l).
",4.2 Semi-HMMs with continuous values,[0],[0]
"Following Liang et al. (2009), we consider two aspects.",4.2 Semi-HMMs with continuous values,[0],[0]
"One is salience that captures the intuition
3We limit the number of arguments within two in this work and leave more complex operators for future study.
that some fields should be more frequently mentioned than others (henceforth some latent tags should be more frequently triggered).",4.2 Semi-HMMs with continuous values,[0],[0]
"The other is (local) coherence, which refers to the order in which the writer mentioning certain information tends to follow some patterns.",4.2 Semi-HMMs with continuous values,[0],[0]
"To capture these two phenomena, we define a Markov model:
Ps(c, l) =",4.2 Semi-HMMs with continuous values,[0],[0]
"∏ t P (lt|lt−1) · Ps(ct|lt),
where lt is the annotated label at time stamp t, and we assume that the transition probabilities are independent of world state s. It resembles a standard form of HMMs, despite the subscript s in Ps(c|l).",4.2 Semi-HMMs with continuous values,[0],[0]
"For different types of correspondences between lt and ct, we define different probability distributions to model Ps(ct|lt):
(1) Numerics-to-numerics:",4.2 Semi-HMMs with continuous values,[0],[0]
"The numbers in texts could sometimes be inaccurate due to some rounding customs, thus we use a Gaussian model for this type: SoftIndicator(x, y|σ)",4.2 Semi-HMMs with continuous values,[0],[0]
"= N (x − y|0, σ), where N is the Gaussian density.",4.2 Semi-HMMs with continuous values,[0],[0]
"When the output type of tag lt is numeric and the word span ct is a number, we set Ps(c|l) = SoftIndicator(c, vl|σl), where σl is different for different tags, and vl is the corresponded value in the table for tag l. Note that when σ → 0, SoftIndicator reduces to an indicator function that only allows exact matching.
",4.2 Semi-HMMs with continuous values,[0],[0]
"(2) String-to-string: Similarly for strings, since simple matching could fail if the text contains Bob to refer to Bob Smith.",4.2 Semi-HMMs with continuous values,[0],[0]
We simply use string matching to model the probability: Ps(c|l) ∝,4.2 Semi-HMMs with continuous values,[0],[0]
"Match(vl, c), where the Match function returns the number of shared words between cell value vl and word c.
(3) Category-to-string:",4.2 Semi-HMMs with continuous values,[0],[0]
"For labels correspond to discrete categorical values, such as Sunday, PG (point guard, a basketball position), we adopt the same method used by Liang et al. (2009): using a multinomial distribution over all word spans for each possible category:
Ps(c|l) = νc,vl , ∑ c νc,v = 1, (1)
where vl is again the output value of tag l.
(4) Numerics-to-string: When the tag correspond to a numeric value vl while the word span c is not a number, the problem resembles speech modeling (Huang et al., 1990).",4.2 Semi-HMMs with continuous values,[0],[0]
"Applying the Bayes rule, we get: 4
Ps(c|l) = P (c|l, vl) ∝",4.2 Semi-HMMs with continuous values,[0],[0]
"P (vl|c, l) · P (c|l),
where we collapse the relevant part from world state s into vl.",4.2 Semi-HMMs with continuous values,[0],[0]
"The intuition behind P (vl|c, l) is that when an informative word (e.g. routed) appears in the text, the corresponding values should have different chances to happen in the world, e.g. P (30|routed,Points Delta)",4.2 Semi-HMMs with continuous values,[0],[0]
"> P (3|routed,Points Delta).",4.2 Semi-HMMs with continuous values,[0],[0]
"Due to the lack of prior knowledge on the distribution, we also model this term as Gaussian.",4.2 Semi-HMMs with continuous values,[0],[0]
5,4.2 Semi-HMMs with continuous values,[0],[0]
"The result resembles a Gaussian mixture:
Ps(c|l) ∝",4.2 Semi-HMMs with continuous values,[0],[0]
"N (vl;µc,l, σc,l) · ηc,l,∑ c ηc,l = 1,
where P (c|l) = ηc,l is also multinomial.",4.2 Semi-HMMs with continuous values,[0],[0]
Our model can enable phrase segmentation.,4.3 Modeling phrasal spans,[0],[0]
"Previously, Liang et al. (2009) treated the words inside a phrase individually and independently.",4.3 Modeling phrasal spans,[0],[0]
This could be problematic in our scenario.,4.3 Modeling phrasal spans,[0],[0]
"For example, take down is used to describe a team defeating another, while separately both take and down are frequent words in general, making them difficult to be jointly assigned with the correct label as a whole.",4.3 Modeling phrasal spans,[0],[0]
"Instead, in our model we treat the phrasal
4We noticed that in parallel with our work, another study (Zhang et al., 2018) on verb selection for data-to-text generation also use the same strategy of Bayes rule to estimate parameters, and provide a noisy-channel interpretation.
",4.3 Modeling phrasal spans,[0],[0]
"5The double tails in Gaussian pdf have different interpretations: unlikely to occur, or unlikely to occur conditionally.
",4.3 Modeling phrasal spans,[0],[0]
word span as a unit.,4.3 Modeling phrasal spans,[0],[0]
"The probability is assigned to the whole span of phrase instead of individual words, which will break the token-wise Markov property (Fig. 2, henceforth Semi-HMM).",4.3 Modeling phrasal spans,[0],[0]
"For efficient parameter estimation, We use a variant of the standard forward-backward algorithm by adding a parameter k, which is the maximum length of word spans, onto the Markov chain.",4.3 Modeling phrasal spans,[0],[0]
We leave detailed descriptions to Appendix B.,4.3 Modeling phrasal spans,[0],[0]
Preliminary experiments suggest that the initial model have too many words assigned to the NULL tag.,4.4 Skipping null labels,[0],[0]
"Informative alignments may not be adjacent, which breaks the simplest Markov assumptions.",4.4 Skipping null labels,[0],[0]
"In our model, the transition score of two non-NULL labels can be calculated by skipping all the NULLs in between, as shown in Figure 3.",4.4 Skipping null labels,[0],[0]
"This is implemented without breaking the overall Markov property with the following trick used in earlier work on statistical alignments (Brown et al., 1993):",4.4 Skipping null labels,[0],[0]
"Suppose we have m labels (i.e., m latent states), we can design m different NULL labels that share the same emission score, while preserving their original outward-transition probabilities.",4.4 Skipping null labels,[0],[0]
The types of NULL labels are inherited from the previous label.,4.4 Skipping null labels,[0],[0]
"This might seem to be wasteful at first sight as we use two-fold latent states, but the Markov property is successfully preserved, therefore simplifying our implementation.",4.4 Skipping null labels,[0],[0]
The structured tables and text descriptions of the dataset were originally crawled from different sources.,4.5 Posterior regularization,[0],[0]
"As a consequence, a non-negligible proportion of texts is in fact describing information outside the given table, such as historical records (e.g., “win streak”).",4.5 Posterior regularization,[0],[0]
"Ideally, words in these parts of the text should remain unaligned, or in our setting, be annotated with the NULL tag.",4.5 Posterior regularization,[0],[0]
"However, due to the notorious effect of garbage collection
from statistical alignment (Brown et al., 1993), these words tend to be aligned to some irrelevant fields in the table which are rarely mentioned.
",4.5 Posterior regularization,[0],[0]
"We address this issue by adding a soft statistical constraint in the form of posterior regularization (Ganchev et al., 2010; Graça et al., 2010).",4.5 Posterior regularization,[0],[0]
"With posterior regularization, we could add certain types of statistical constraints to the E-step in the EM procedure, while keeping the inference tractable.",4.5 Posterior regularization,[0],[0]
"The constraints should be in the form of:
E[f(w, l)]",4.5 Posterior regularization,[0],[0]
"= ∑ i E[f(w, li)]",4.5 Posterior regularization,[0],[0]
"≤ bw, (2)
where the features f should be defined on local cliques for tractable inference.
",4.5 Posterior regularization,[0],[0]
We use projected gradient descent to solve the E-step sub-problem in this work.,4.5 Posterior regularization,[0],[0]
"The statistical constraint we add to the posterior is rather simple: For each sentence, we “encourage” at least a proportion of words to be aligned to NULL labels:
E[−f(w, l)] ≤",4.5 Posterior regularization,[0],[0]
"−r0 · n, (3)
f(w, l) = n∑ i=1",4.5 Posterior regularization,[0],[0]
"1(li = NULL), (4)
where r0 is a adjustable ratio, n is the length of w. We also tried other constraints but found this simple soft regularization performing well.",4.5 Posterior regularization,[0],[0]
"Intuitively, the assigned semantic correspondences could be useful to derive templates and trigger rules for language generation.",5 By-product: template induction,[0],[0]
"In this work, we use the most straightforward heuristics to perform template induction, utilizing the established correspondences and inferred parameters.",5 By-product: template induction,[0],[0]
"Specifically, we first blank out the correspondences of numerics-to-numerics and string-to-string to be empty slots and replace with the tag names.",5 By-product: template induction,[0],[0]
"In the example of Figure 1, we could replace Raptors with Team Name, and 120 with Team Points, if they have been correctly aligned.
",5 By-product: template induction,[0],[0]
We also need to know when to use each template.,5 By-product: template induction,[0],[0]
"We define a template trigger to be a quadruple (c, l, µc,l, σc,l), where c is a phrase, l is a tag, µc,l and σc,l are estimated Gaussian parameters.",5 By-product: template induction,[0],[0]
"6 We assign each template with a score to be the minimum probability for all triggers inside:
score(s, t) =",5 By-product: template induction,[0],[0]
min,5 By-product: template induction,[0],[0]
"i N (ti.l(s); ti.µ, ti.σ), (5)
6To use a unified notation, for categorical-value triggers we set µc,l = argmaxvl νc,vl (defined in (1)) and σc,l = .
where t = {ti} denotes all possible triggers in the template, and the tag l can be executed over the world state s to retrieve a value l(s).",5 By-product: template induction,[0],[0]
"We only consider sentences satisfying both of the following conditions in order to extract templates with high quality: (1) sentences aligned to ≤ two teams or one player, and (2) sentences with triggers derived from continuous distributions.
",5 By-product: template induction,[0],[0]
"Now that the templates and triggers are ready for use, we will experiment with the following straightforward rules to perform data-to-text generation: For every game, we first generate a sentence describing the scoreline result, followed by three sentences describing other information about team performance.",5 By-product: template induction,[0],[0]
"While keeping that no template is repeatedly used, we will then choose the template with the highest score for top ten players sorted by their game points.",5 By-product: template induction,[0],[0]
We conducted experiments on the ROTOWIRE subset of the Wiseman et al. (2017) dataset.,6.1 Experimental setup,[0],[0]
"In our experiment, we restricted the maximum length of word span to two as a trade-off of speed and performance.",6.1 Experimental setup,[0],[0]
"Empirically, most of the phrases in the dataset are at a length of at most two.",6.1 Experimental setup,[0],[0]
"We empirically set the expected NULL ratio to be r0 = 0.5.
",6.1 Experimental setup,[0],[0]
"We did the following pre-processing steps for all systems in comparison: we lemmatized all tokens in the sentences, and filtered out sentences containing less than five words since they are meaningless short sentences.",6.1 Experimental setup,[0],[0]
"To utilize the game dates, we converted them from calendar date to the day of week, e.g. 11/28/2016 is converted to Monday as a categorical value.",6.1 Experimental setup,[0],[0]
"Due to the huge noise in the ROTOWIRE dataset, containing many sentences irrelevant with their corresponding tables, we filtered out the sentences that contain no team or player names, or those that mention more than 2 players, as most of them are irrelevant texts.
",6.1 Experimental setup,[0],[0]
"Following Liang et al. (2009), we also used the parameters of a simpler model without Markov dependency (which was uniformly initialized) to initialize our complete model with obtained parameters, and then trained it until convergence.",6.1 Experimental setup,[0],[0]
"We adapted Liang et al. (2009)’s framework to the table schema in the ROTOWIRE dataset, and ran experiments accordingly as our baseline model.",6.1 Experimental setup,[0],[0]
"It is difficult to evaluate the accuracy of tag assignments for the entire dataset, since the tags are not annotated in the original data.",6.2.1 Intrinsic evaluation,[0],[0]
"We recruited three human annotators with familiarity in the domain of basketball games to label 300 sentences (around 8,000 tokens in total, and 30% of them are annotated with tags) from the test set.",6.2.1 Intrinsic evaluation,[0],[0]
"There exists a fraction (18%) where agreements were not made, we included all the proposed tags from the annotators to be correct.",6.2.1 Intrinsic evaluation,[0],[0]
"Also, because of the ambiguity of annotations, we use the base names of derived tags (e.g. Rebounds Delta) for numerics-to-string relationship evaluation.",6.2.1 Intrinsic evaluation,[0],[0]
(e.g. Rebounds Delta is reduced to Rebounds),6.2.1 Intrinsic evaluation,[0],[0]
"Finally, we calculated the precision and recall for non-NULL tag assignments at word-level.
",6.2.1 Intrinsic evaluation,[0],[0]
The results are shown in Table 2.,6.2.1 Intrinsic evaluation,[0],[0]
"The Liang et al. (2009) framework could still achieve around 65% recall, because there exist a large proportion of correspondences that could be easily captured by exact matches and simple categorical distributions.",6.2.1 Intrinsic evaluation,[0],[0]
"Our model without PR achieves lower precision than the baseline, because the baseline did not model numerics-to-string relationships and encountered less severe issues of garbage collection.",6.2.1 Intrinsic evaluation,[0],[0]
"We can observe that our initial model indeed outperforms the baseline system in recall, while PR helps a lot to avoid distraction from irrelevant information that should be tagged as NULL.
",6.2.1 Intrinsic evaluation,[0],[0]
"We also include more fine-grained results for different types of correspondences, shown in Table 3.",6.2.1 Intrinsic evaluation,[0],[0]
"As expected, numerics-to-string correspon-
dences are the most difficult part in this study.",6.2.1 Intrinsic evaluation,[0],[0]
"Another notable thing is that although we found that around 40% of numerics-to-numerics correspondences were ambiguous due to the appearance of identical values from different table cells, our model could still achieve a high accuracy of 95.0%.",6.2.1 Intrinsic evaluation,[0],[0]
"We also tested how the derived templates could perform in language generation, when compared with the baseline using the same heuristics described in Sec 5.",6.2.2 Extrinsic evaluation,[0],[0]
"We report automatic metrics including BLEU scores and those based on relation extraction as proposed by Wiseman et al. (2017): precision & number of unique relations in generation (RG), precision & recall for content selection (CS), and content ordering (CO) score.",6.2.2 Extrinsic evaluation,[0],[0]
"These automatic metrics were designed for various aspects in NLG and may not all suit our main focus well, so we also conducted human evaluation on information correctness (1-5 scale ratings, the higher the better).",6.2.2 Extrinsic evaluation,[0],[0]
We asked four human raters who are fluent in English and with familiarity in basketball terms to rate over outputs for 30 random games.,6.2.2 Extrinsic evaluation,[0],[0]
Results are shown in Table 4.,6.2.2 Extrinsic evaluation,[0],[0]
"We can observe that templates derived from our model indeed outper-
form those from the baseline.",6.2.2 Extrinsic evaluation,[0],[0]
We put some inducted templates and generated text examples in the Appendix.,6.2.2 Extrinsic evaluation,[0],[0]
Figure 4 shows some examples produced from our methods.,6.3 Analysis,[0],[0]
"Some of the alignments are meaningful, for example, the model assigned the word perfect with the annotation FT Percent, which represents the percentage of free throws.",6.3 Analysis,[0],[0]
"Without PR, our model performed poorly by aligning many common words to those rarely mentioned cells.",6.3 Analysis,[0],[0]
"In this example, the FT Made and FT Attempt fields in the input table both have the same value 8, making it difficult for a model without proper local coherence modeling to distinguish between them.",6.3 Analysis,[0],[0]
"Because our initial model without PR cannot annotate NULL correctly, the Markov transition between these two numbers was intercepted by three meaningless tokens.",6.3 Analysis,[0],[0]
"However, after injecting the PR constraint, most of the unmentioned words were successfully identified.",6.3 Analysis,[0],[0]
"The model captured the pattern that FT Attempt almost always follows FT Made, making it correctly assigned these two labels.
",6.3 Analysis,[0],[0]
"We conducted ablation experiments for some
of the components (Table 5).",6.3 Analysis,[0],[0]
"When setting the maximum phrase length to be k = 1, the model degenerates to a normal HMM.",6.3 Analysis,[0],[0]
The performance measured by F1-score drops for only a little.,6.3 Analysis,[0],[0]
"One possible reason is that Semi-HMMs tend to output some meaningless combinations of words as phrases, such as the phrase points on in Figure 4 (b), which could lead to many redundant annotations that hampers precision albeit its help to recall.",6.3 Analysis,[0],[0]
"We also tried to disable the transition probabilities during both training and inference, which led to lower precision and lower recall naturally as there was no modeling for local coherence.",6.3 Analysis,[0],[0]
"Finally, by canceling the NULL-skipping mechanism, we found that the numerics-to-numerics annotation accuracy dropped from 95.0% to 88.8%.",6.3 Analysis,[0],[0]
"Many of the spurious numerics-to-numerics annotations, such as the 8 - of - 8 in Figure 4 (d), could be corrected using transition probabilities under the skipping-NULL mechanism (Figure 4 (c)).
",6.3 Analysis,[0],[0]
One additional advantage of our model is that we can easily verify what the model has captured.,6.3 Analysis,[0],[0]
"For the latent annotation Team Points Delta, we sort its corresponding phrases by weights P (c|l)P (c) and we list the top 12 weighted words in Figure 5.",6.3 Analysis,[0],[0]
We can observe that most of the displayed phrases have strong semantic relationship with score differences.,6.3 Analysis,[0],[0]
"More interestingly, we found the mean and variance values estimated by the Gaussian distributions rather informative.",6.3 Analysis,[0],[0]
"When l = Teams Points Delta, we observed that µl,narrowly ≈ 2 while µl,blow out ≈ 26.",6.3 Analysis,[0],[0]
"We could infer the conditions under which some phrases should be used, providing useful insights for lexical choices in language generation.",6.3 Analysis,[0],[0]
"In this paper, we attempt to learn executable latent semantic annotations from paired structured tables and texts.",7 Conclusion,[0],[0]
"We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model and impose a soft statistical constraint via posterior regularization.",7 Conclusion,[0],[0]
"Experimental results suggest the feasibility of the setting in study and the effectiveness of our framework.
",7 Conclusion,[0],[0]
This is a preliminary study for using weak supervision from structured data and texts to address the challenging problem of language grounding.,7 Conclusion,[0],[0]
"For future study, one could collect large-scale data and texts in other domains where more complex grounding on phrases such as “increasing trends” should be done.",7 Conclusion,[0],[0]
"To enhance modeling power, unsupervised discriminative models that utilize rich features (Berg-kirkpatrick et al., 2010) could also be explored.",7 Conclusion,[0],[0]
We are also interested in collecting more high-quality parallel data to induce grounded compositional logic representations.,7 Conclusion,[0],[0]
We would like to thank Hongyuan Mei and all the anonymous reviewers for giving helpful comments on an earlier draft of this paper.,Acknowledgements,[0],[0]
"Previous work on grounded language learning did not fully capture the semantics underlying the correspondences between structured world state representations and texts, especially those between numerical values and lexical terms.",abstractText,[0],[0]
"In this paper, we attempt at learning explicit latent semantic annotations from paired structured tables and texts, establishing correspondences between various types of values and texts.",abstractText,[0],[0]
"We model the joint probability of data fields, texts, phrasal spans, and latent annotations with an adapted semi-hidden Markov model, and impose a soft statistical constraint to further improve the performance.",abstractText,[0],[0]
"As a by-product, we leverage the induced annotations to extract templates for language generation.",abstractText,[0],[0]
"Experimental results suggest the feasibility of the setting in this study, as well as the effectiveness of our proposed framework.",abstractText,[0],[0]
1,abstractText,[0],[0]
Learning Latent Semantic Annotations for Grounding Natural Language to Structured Data,title,[0],[0]
"Latent space models (LSMs), such as sparse coding (Olshausen & Field, 1997), topic models (Blei et al., 2003) and neural networks, are widely used in machine learning to extract hidden patterns and learn latent representations of data.",1. Introduction,[0],[0]
An LSM consists of a set of components.,1. Introduction,[0],[0]
"Each component aims at capturing one latent pattern and is pa-
1Machine Learning Department, Carnegie Mellon University 2Petuum Inc.",1. Introduction,[0],[0]
"3School of Engineering and Applied Sciences, Harvard University 4College of Engineering and Computer Science, Syracuse University 5Groupon Inc.",1. Introduction,[0],[0]
"6School of Computer Science, University of Waterloo 7Department of Biomedical Data Science, Stanford University.",1. Introduction,[0],[0]
"Correspondence to: Pengtao Xie <pengtaox@cs.cmu.edu>, Eric P. Xing <eric.xing@petuum.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
rameterized by a weight vector.",1. Introduction,[0],[0]
"For instance, in a topic model (Blei et al., 2003), the components are referred to as topics, aiming at discovering the semantics underlying documents.",1. Introduction,[0],[0]
Each topic is associated with a weight vector.,1. Introduction,[0],[0]
The modeling power of LSMs can be very large when the number of components is large and the dimension of weight vectors is high.,1. Introduction,[0],[0]
"For instance, in the LightLDA (Yuan et al., 2015) topic model, the number of topics is 1 million and the dimension of topic vector is 50000, resulting in a topic matrix with 50 billion parameters.",1. Introduction,[0],[0]
"The vast model capacity of LSMs enables them to flexibly adapt to the complex patterns underlying data and achieve great predictive performance therefrom.
",1. Introduction,[0],[0]
"While highly expressive, LSMs are prone to overfitting, because of their large amount of model parameters.",1. Introduction,[0],[0]
"A key ingredient to successfully train LSMs is regularization, which imposes certain control over the model parameters to reduce model complexity and improve the generalization performance on unseen data.",1. Introduction,[0],[0]
"Many regularizers have been proposed, including `2 regularization, `1 regularization (Tibshirani, 1996), nuclear norm (Recht et al., 2010),",1. Introduction,[0],[0]
"Dropout (Srivastava et al., 2014) and so on.
",1. Introduction,[0],[0]
"Recently, a new type of regularization approaches (Yu et al., 2011; Zou & Adams, 2012a; Xie et al., 2015; 2016a; Rodrı́guez et al., 2016), which aim at encouraging the weight vectors of components in LSMs to be “diverse”, are emerging.",1. Introduction,[0],[0]
"Zou & Adams (2012b) apply Determinantal Point Process (Kulesza & Taskar, 2012) to encourage the topic vectors in LDA to be “diverse”.",1. Introduction,[0],[0]
Bao et al. (2013) develop a softmax regularizer to promote incoherence among hidden units in neural network.,1. Introduction,[0],[0]
Xie et al. (2015) propose an angle-based regularizer to “diversify” the weight vectors in Restricted Boltzmann Machine (RBM).,1. Introduction,[0],[0]
"While these approaches have demonstrated promising effectiveness on a wide range of empirical studies, in theory how they reduce overfitting is still unclear.",1. Introduction,[0],[0]
"One intuitive explanation could be: promoting diversity imposes a structural constraint on model parameters, which reduces the model capacity of LSMs and therefore alleviates overfitting.",1. Introduction,[0],[0]
"However, how to make this formal is challenging.",1. Introduction,[0],[0]
"In this paper, we aim to bridge this gap, by proposing a diversitypromoting approach that is both empirically effective and theoretically analyzable.",1. Introduction,[0],[0]
"We use near-orthogonality to represent “diversity” and propose to learn LSMs with angular
constraints (ACs) where the angle between components is constrained to be close to π2 , which hence encourages the components to be close to orthogonal (therefore “diverse”).",1. Introduction,[0],[0]
"Using sparse coding and neural network as study cases, we analyze how ACs affect the generalization performance of these two LSMs.",1. Introduction,[0],[0]
"The analysis shows that the more close to π2 the angles are, the smaller the estimation error is and the larger the approximation error is.",1. Introduction,[0],[0]
The best tradeoffs of these two errors can be explored by properly tuning the angles.,1. Introduction,[0],[0]
"We develop an alternating direction method of multipliers (ADMM) (Boyd et al., 2011) algorithm to solve the angle-constrained LSM (AC-LSM) problems.",1. Introduction,[0],[0]
"In various experiments, we demonstrate that ACs improve the generalization performance of LSMs and outperform other diversity-promoting regularization approaches.
",1. Introduction,[0],[0]
"The major contributions of this work are:
• We propose a new approach to promote diversity in LSMs, by imposing angular constraints (ACs) on components, for the sake of alleviating overfitting.
",1. Introduction,[0],[0]
"• We perform theoretical analysis on how ACs affect the generalization error of two exemplar LSMs: sparse coding and neural networks.
",1. Introduction,[0],[0]
"• We develop an efficient ADMM algorithm to solve the AC-LSM problems.
",1. Introduction,[0],[0]
"• Empirical evaluation demonstrates that ACs are very effective in reducing overfitting and outperforms other diversity-promoting approaches.
",1. Introduction,[0],[0]
The rest of the paper is organized as follows.,1. Introduction,[0],[0]
Section 2 reviews related works.,1. Introduction,[0],[0]
Second 3 introduces the angleconstrained LSMs and Section 4 gives the theoretical analysis.,1. Introduction,[0],[0]
Section 5 presents experimental results and Section 6 concludes the paper.,1. Introduction,[0],[0]
Diversity-promoting learning of latent space models has been widely studied recently.,2. Related Works,[0],[0]
Ramirez et al. (2010) define a regularizer based on squared Frobenius norm to encourage the dictionary in sparse coding to be incoherent.,2. Related Works,[0],[0]
"Zou & Adams (2012a) use the determinantal point process (DPP) (Kulesza & Taskar, 2012) to encourage the location vectors in Gaussian mixture model (GMM) and topics in latent Dirichlet allocation (Blei et al., 2003) to be “diverse”.",2. Related Works,[0],[0]
"Given m vectors {wj}mj=1, DPP is defined as log det(G).",2. Related Works,[0],[0]
"G is a kernel matrix whereGij = k(wi,wj) and k(·, ·) is a kernel function.",2. Related Works,[0],[0]
"det(G) is the volume of the parallelepiped formed by {φ(wj)}mj=1, where φ(·) denotes the reproducing kernel feature map associated with kernel k. Vectors with larger volume are considered to be more diverse since they are more spread out.",2. Related Works,[0],[0]
Xie (2015) develop an anglebased regularizer to encourage the weight vectors of hidden units in restricted Boltzmann machine to be close to orthogonal.,2. Related Works,[0],[0]
"The non-obtuse angle between each pair of weight
vectors is measured and the regularizer is defined as the mean of these angles minus their variance.",2. Related Works,[0],[0]
A larger mean encourages the vectors to have larger angles overall and a smaller variance encourages the vectors to be evenly different from each other.,2. Related Works,[0],[0]
Xie (2015) apply this regularizer to encourage the projection vectors in distance metric learning to be diverse and show that promoting diversity can reduce model size without sacrificing modeling power.,2. Related Works,[0],[0]
"Besides frequentist-style regularization, diversity-promoting learning is also investigated in Bayesian learning where the components are random variables.",2. Related Works,[0],[0]
Affandi et al. (2013) apply DPP as a repulsive prior to encourage the location vectors in GMM to be far apart.,2. Related Works,[0],[0]
"Xie et al. (2016a) propose a mutual angular prior that has an inductive bias towards vectors having larger angles.
",2. Related Works,[0],[0]
"In the literature of neural networks, many works have studied the “diversification” of hidden units.",2. Related Works,[0],[0]
Le et al. (2010) apply a strict-orthogonality constraint over the weight parameters to make the hidden units uncorrelated (therefore “diverse”).,2. Related Works,[0],[0]
"In practice, this hard constraint might be too restrictive and hurts performance, as we will confirm in experiments.",2. Related Works,[0],[0]
Bao et al. (2013) propose a softmax regularizer to encourage the weight vectors of hidden units to have small cosine similarity.,2. Related Works,[0],[0]
Cogswell et al. (2015) propose to decorrelate hidden activations by minimizing their covariance.,2. Related Works,[0],[0]
"In convolutional neural networks (CNNs) where the number of activations is much larger than that of weight parameters, this regularizer is computationally prohibitive since it is defined over activations rather than weights.",2. Related Works,[0],[0]
Henaff et al. (2016) perform a study to show that random orthogonal initialization of the weight matrices in recurrent neural networks improves its ability to perform longmemory tasks.,2. Related Works,[0],[0]
Xiong et al. (2016) propose a structured decorrelation constraint which groups hidden units and encourages units within the same group to have strong connections during the training procedure and forces units in different groups to learn nonredundant representations by minimizing the cross-covariance between them.,2. Related Works,[0],[0]
Rodrı́guez et al. (2016) show that regularizing negatively correlated features inhibits effective diversity and propose a solution which locally enforces feature orthogonality.,2. Related Works,[0],[0]
Chen et al. (2017) propose a group orthogonal CNN which leverages side information to learn diverse feature representations.,2. Related Works,[0],[0]
Mao et al. (2017) impose a stochastic decorrelation constraint based on covariance to reduce the co-adaptation of hidden units.,2. Related Works,[0],[0]
Xie et al. (2017) define a kernel-based regularizer to promote diversity and analyze how it affects the generalize performance of neural networks (NNs).,2. Related Works,[0],[0]
"It is unclear how to generalize the analysis to other LSMs.
",2. Related Works,[0],[0]
Diversity-promoting learning has been investigated in nonLSM models as well.,2. Related Works,[0],[0]
"In multi-class classification, Malkin & Bilmes (2008) encourage the coefficient vectors of different classes to be diverse by maximizing the determi-
nant of the covariance matrix of the coefficient vectors.",2. Related Works,[0],[0]
"In classifiers ensemble, Yu et al. (2011) develop a regularizer to encourage the coefficient vectors of support vector machines (SVMs) to have small cosine similarity and analyze how this regularizer affects the generalization performance.",2. Related Works,[0],[0]
The analysis is specific to SVM ensemble.,2. Related Works,[0],[0]
It is unclear how to generalize it to latent space models.,2. Related Works,[0],[0]
"In this section, we propose Angle-Constrained Latent Space Models (AC-LSMs) and present an ADMM algorithm to solve them.",3. Methods,[0],[0]
An LSM consists of m components and these components are parameterized by vectors W = {wj}mj=1.,3.1. Latent Space Models with Angular Constraints,[0],[0]
Let L(W) denote the objective function of this LSM.,3.1. Latent Space Models with Angular Constraints,[0],[0]
"Similar to (Bao et al., 2013; Xie et al., 2015; Rodrı́guez et al., 2016), we use angle to characterize diversity: the components are considered to be more diverse if they are close to being orthogonal, i.e., their angles are close to π2 .",3.1. Latent Space Models with Angular Constraints,[0],[0]
"To encourage this, we require the absolute value of cosine similarity between each pair of components to be less than a small value τ , which leads to the following angle-constrained LSM (AC-LSM) problem
min W L(W) s.t. 1 ≤",3.1. Latent Space Models with Angular Constraints,[0],[0]
"i < j ≤ m, |wi·wj |‖wi‖2‖wj‖2 ≤ τ",3.1. Latent Space Models with Angular Constraints,[0],[0]
"(1)
The parameter τ controls the level of near-orthogonality (or diversity).",3.1. Latent Space Models with Angular Constraints,[0],[0]
"A smaller τ indicates that the vectors are more close to being orthogonality, and hence are more diverse.",3.1. Latent Space Models with Angular Constraints,[0],[0]
"As will be shown later, representing diversity using the angular constraints facilitates theoretical analysis and is empirically effective as well.",3.1. Latent Space Models with Angular Constraints,[0],[0]
"In this section, we apply the ACs to two LSMs.
",3.2. Case Studies,[0],[0]
"Sparse Coding Given a set of data samples {xi}ni=1, where x ∈ Rd, sparse coding (SC) (Olshausen & Field, 1997) aims to use a set of “basis” vectors (referred to as dictionary)",3.2. Case Studies,[0],[0]
W = {wj}mj=1 to reconstruct the data samples.,3.2. Case Studies,[0],[0]
Each data sample x is reconstructed by taking a sparse linear combination of the basis vectors x,3.2. Case Studies,[0],[0]
≈ ∑m j=1 αjwj where {αj}mj=1 are the linear coefficients (referred to as sparse codes) and most of them are zero.,3.2. Case Studies,[0],[0]
The reconstruction error is measured using the squared `2 norm ‖x− ∑m j=1 αjwj‖22.,3.2. Case Studies,[0],[0]
"To achieve sparsity among the coeffi-
cients, `1-regularization is utilized: ∑m j=1 |αj |1.",3.2. Case Studies,[0],[0]
"To avoid the degenerated case where most coefficients are zero and the basis vectors are of large magnitude, `2-regularization is applied to the basis vectors: ‖wj‖22.",3.2. Case Studies,[0],[0]
"Putting these pieces together, we learn the basis vectors and sparse codes
(denoted by A) by minimizing the following objective function: L(W,A) = 12 ∑n i=1(‖xi − ∑m j=1 αijwj‖22 +
λ1 ∑m j=1 |αij |1)",3.2. Case Studies,[0],[0]
+,3.2. Case Studies,[0],[0]
λ2 ∑m j=1 ‖wj‖22.,3.2. Case Studies,[0],[0]
"Applying ACs to the basis vectors, we obtain the following AC-SC problem:
minW,A L(W,A) s.t. 1 ≤",3.2. Case Studies,[0],[0]
"i < j ≤ m, |wi·wj |‖wi‖2‖wj‖2 ≤",3.2. Case Studies,[0],[0]
"τ
(2)
Neural Networks In a neural network (NN) with L hidden layers, each hidden layer l is equipped with m(l) units and each unit i is connected with all units in layer l − 1.",3.2. Case Studies,[0],[0]
"Hidden unit i at layer l is parameterized by a weight vector w
(l) i .",3.2. Case Studies,[0],[0]
These hidden units aim at capturing latent features underlying data.,3.2. Case Studies,[0],[0]
"Applying ACs to the weight vectors of hidden units, we obtain the following AC-NN problem
min W L(W) s.t. ∀ 1 ≤",3.2. Case Studies,[0],[0]
"l ≤ L, 1 ≤",3.2. Case Studies,[0],[0]
"i < j ≤ m(l), |w (l) i ·w (l) j |
‖w(l)i ‖2‖w (l) j ‖2
≤ τ
whereW denotes weight vectors in all layers and L(W) is the objective function of this NN.",3.2. Case Studies,[0],[0]
"In this section, we develop an ADMM-based algorithm to solve the AC-LSM problem.",3.3. Algorithm,[0],[0]
"To make it amenable for optimization, we first factorize each weight vector w into its `2 norm g =",3.3. Algorithm,[0],[0]
‖w‖2 and direction,3.3. Algorithm,[0],[0]
w̃ = w‖w‖2 .,3.3. Algorithm,[0],[0]
"Under such a factorization, w can be reparameterized as w = gw̃, where g > 0",3.3. Algorithm,[0],[0]
and ‖w̃‖2 = 1.,3.3. Algorithm,[0],[0]
"Then the problem defined in Eq.(1) can be transformed into
min W̃,G L(W̃,G) s.t. ∀j, gj ≥ 0, ‖w̃j‖2 = 1 ∀i 6= j, |w̃i · w̃j",3.3. Algorithm,[0],[0]
"| ≤ τ
(3)
where W̃ = {w̃j}mj=1 and G = {gj}mj=1.",3.3. Algorithm,[0],[0]
"We solve this new problem by alternating between W̃ and G. Fixing W̃ , the problem defined over G is: minG L(G) s.t. ∀j, gj ≥ 0, which can be solved using projected gradient descent.",3.3. Algorithm,[0],[0]
"Fixing G, the sub-problem defined over W̃ is
min W̃ L(W̃) s.t. ∀j, ‖w̃j‖2 = 1 ∀i",3.3. Algorithm,[0],[0]
"6= j, |w̃i · w̃j",3.3. Algorithm,[0],[0]
"| ≤ τ
(4)
which we solve using an ADMM algorithm.",3.3. Algorithm,[0],[0]
There are R = m(m,3.3. Algorithm,[0],[0]
− 1) pairwise constraints |w̃i · w̃j,3.3. Algorithm,[0],[0]
| ≤ τ .,3.3. Algorithm,[0],[0]
"For the r-th constraint, let p(r) and q(r) be the index of the first and second vector respectively, i.e., the r-th constraint is |w̃p(r) ·",3.3. Algorithm,[0],[0]
w̃q(r)| ≤ τ .,3.3. Algorithm,[0],[0]
"First, we introduce auxiliary variables {v(r)1 }Rr=1 and {v (r) 2 }Rr=1, to rewrite the problem in
Eq.(4) into an equivalent form.",3.3. Algorithm,[0],[0]
"For each pairwise constraint: |w̃p(r) · w̃q(r)| ≤ τ , we introduce two auxiliary vectors v(r)1 and v (r) 2 , and let w̃p(r) = v (r) 1 , w̃q(r) = v (r) 2 , ‖v(r)1 ‖2 = 1, ‖v (r) 2 ‖2 = 1, |v (r) 1 · v (r) 2 | ≤ τ .",3.3. Algorithm,[0],[0]
"To this end, we obtain the following problem
min W̃,V L(W̃) s.t. ∀j, ‖w̃j‖2 = 1 ∀r, w̃p(r) = v (r) 1 , w̃q(r) = v",3.3. Algorithm,[0],[0]
"(r) 2
∀r, ‖v(r)1 ‖2 = 1, ‖v (r) 2 ‖2 = 1, |v (r) 1 · v (r) 2 | ≤ τ
where V = {(v(r)1 ,v (r) 2 )}Rr=1.",3.3. Algorithm,[0],[0]
"Then we define the augmented Lagrangian, with Lagrange multipliers Y = {(y(r)1 ,y (r) 2 )}Rr=1 and parameter ρ
min W̃,V,Y L(W̃) +",3.3. Algorithm,[0],[0]
"R∑ r=1 (y (r) 1 · (w̃p(r) − v (r) 1 )
+y (r) 2 · (w̃q(r)",3.3. Algorithm,[0],[0]
− v (r) 2 ) + ρ 2‖w̃p(r) − v (r) 1 ‖22 +ρ2‖w̃q(r),3.3. Algorithm,[0],[0]
"− v (r) 2 ‖22)
s.t. ∀j, ‖w̃j‖2 = 1 ∀r, ‖v(r)1 ‖2 = 1, ‖v (r) 2 ‖2 = 1, |v (r) 1 · v (r) 2 | ≤ τ
which can be solved by alternating between W̃ , V , Y .
",3.3. Algorithm,[0],[0]
"Solve W̃ The sub-problem defined over W̃ is
min W̃ L(W̃) + R∑ r=1",3.3. Algorithm,[0],[0]
"(y (r) 1 · w̃p(r) + y (r) 2 · w̃q(r)
+ρ2‖w̃p(r)",3.3. Algorithm,[0],[0]
− v (r) 1 ‖22 + ρ 2‖w̃q(r),3.3. Algorithm,[0],[0]
"− v (r) 2 ‖22)
s.t. ∀j, ‖w̃j‖2 = 1
(5)
For sparse coding, we solve this sub-problem using coordinate descent.",3.3. Algorithm,[0],[0]
"At each iteration, we update w̃j by fixing the other variables.",3.3. Algorithm,[0],[0]
Please refer to the supplements for details.,3.3. Algorithm,[0],[0]
"For neural network, this sub-problem can be solved using projected gradient descent which iteratively performs the following three steps: (1) compute the gradient of w̃j using backpropagation; (2) perform a gradient descent update of w̃j ; (3) project each vector onto the unit sphere: w̃j",3.3. Algorithm,[0],[0]
"← w̃j/‖w̃j‖2.
",3.3. Algorithm,[0],[0]
"Solve v(r)1 ,v (r) 2",3.3. Algorithm,[0],[0]
"The corresponding sub-problem is
min v (r) 1 ,v (r) 2
−y(r)1 · v (r) 1",3.3. Algorithm,[0],[0]
"− y (r) 2 · v (r) 2
+ρ2‖w̃p(r)",3.3. Algorithm,[0],[0]
− v (r) 1 ‖22 + ρ 2‖w̃q(r),3.3. Algorithm,[0],[0]
"− v (r) 2 ‖22
s.t. ‖v(r)1 ‖2 = 1, ‖v (r) 2 ‖2 = 1,
v (r) 1 · v (r) 2 ≤ τ,−v (r) 1 · v (r) 2 ≤ τ
Let γ1, γ2, λ1 ≥ 0, λ2 ≥ 0 be the KKT multipliers associated with the four constraints in this sub-problem.",3.3. Algorithm,[0],[0]
"According to the KKT conditions, we have
−y(r)1 +ρ(v (r) 1 −w̃p(r))+2γ1v (r) 1 +(λ1−λ2)v (r) 2 = 0 (6)
−y(r)2 +ρ(v (r) 2 −w̃q(r))+2γ2v (r) 2 +(λ1−λ2)v (r) 1 = 0 (7)
We solve these two equations by examining four cases.
",3.3. Algorithm,[0],[0]
"Case 1 First, we assume λ1 = 0, λ2 = 0, then (ρ + 2γ1)v (r) 1 = y (r) 1 + ρw̃p(r) and (ρ + 2γ2)v (r) 2 = y (r) 2 + ρw̃q(r).",3.3. Algorithm,[0],[0]
"According to the primal feasibility ‖v (r) 1 ‖2 = 1 and ‖v(r)2 ‖2 = 1, we know
v (r) 1 =
y (r) 1 + ρw̃p(r)
‖y(r)1 + ρw̃p(r)‖2 , v
(r) 2 =
y (r) 2 + ρw̃q(r)
‖y(r)2",3.3. Algorithm,[0],[0]
"+ ρw̃q(r)‖2
Then we check whether the constraint |v(r)1 · v (r) 2 | ≤ τ is satisfied.",3.3. Algorithm,[0],[0]
"If so, then v(r)1 and v (r) 2 are the optimal solution.
",3.3. Algorithm,[0],[0]
Case 2,3.3. Algorithm,[0],[0]
"We assume λ1 > 0 and λ2 = 0, then
(ρ+ 2γ1)v (r) 1 + λ1v (r) 2 = y (r) 1 + ρw̃p(r) (8)
(ρ+ 2γ2)v (r) 2 + λ1v (r) 1 = y (r) 2 + ρw̃q(r) (9)
",3.3. Algorithm,[0],[0]
"According to the complementary slackness condition, we know v(r)1 · v (r) 2 = τ .",3.3. Algorithm,[0],[0]
"For the vectors on both sides of Eq.(8), taking the square of their `2 norm, we get
(ρ+2γ1) 2+λ21+2(ρ+2γ1)λ1τ = ‖y",3.3. Algorithm,[0],[0]
"(r) 1 +ρw̃p(r)‖22 (10)
Similarly, from Eq.(9), we get
(ρ+2γ2) 2+λ21+2(ρ+2γ2)λ1τ = ‖y",3.3. Algorithm,[0],[0]
"(r) 2 +ρw̃q(r)‖22 (11)
Taking the inner product of the two vectors on the left hand sides of Eq.(8,9), and that on the right hand sides, we get
(2ρ+ 2γ1 +",3.3. Algorithm,[0],[0]
2γ2)λ1,3.3. Algorithm,[0],[0]
+ ((ρ+ 2γ1)(ρ+ 2γ2),3.3. Algorithm,[0],[0]
+ λ 2 1)τ,3.3. Algorithm,[0],[0]
=,3.3. Algorithm,[0],[0]
(y (r) 1 + ρw̃p(r)),3.3. Algorithm,[0],[0]
>,3.3. Algorithm,[0],[0]
"(y (r) 2 + ρw̃q(r))
(12) Solving the system of equations consisting of Eq.(10-12), we obtain the optimal values of γ1, γ2 and λ1.",3.3. Algorithm,[0],[0]
"Plugging them into Eq.(8) and Eq.(9), we obtain a solution of v(r)1 and v(r)2 .",3.3. Algorithm,[0],[0]
Then we check whether this solution satisfies −v(r)1 · v (r) 2 ≤ τ .,3.3. Algorithm,[0],[0]
"If so, this is an optimal solution.
",3.3. Algorithm,[0],[0]
"In Case 3, we discuss λ1 = 0, λ2 > 0.",3.3. Algorithm,[0],[0]
"In Case 4, we discuss λ1 > 0, λ2 > 0.",3.3. Algorithm,[0],[0]
The corresponding problems can be solved in a similar way as Case 2.,3.3. Algorithm,[0],[0]
"Please refer to the supplements for details.
",3.3. Algorithm,[0],[0]
"Solve y(r)1 ,y (r) 2",3.3. Algorithm,[0],[0]
"We simply perform the following:
y (r) 1 = y (r) 1 + ρ(w̃p(r)",3.3. Algorithm,[0],[0]
"− v (r) 1 ) (13)
y (r) 2 = y (r) 2 + ρ(w̃q(r)",3.3. Algorithm,[0],[0]
"− v (r) 2 ) (14)
",3.3. Algorithm,[0],[0]
"Compared with a vanilla backpropagation algorithm, the major extra cost in this ADMM algorithm comes from solving the R = m(m − 1) pairs of vectors {v(r)1 ,v (r) 2 }Rr=1.",3.3. Algorithm,[0],[0]
Solving each pair incurs O(m) cost.,3.3. Algorithm,[0],[0]
The R pairs bring in a total cost of O(m3).,3.3. Algorithm,[0],[0]
"Such a cubic cost is also incurred in other decorrelation methods such as (Le et al., 2010; Bao et al., 2013).",3.3. Algorithm,[0],[0]
"In practice,m is typically less than 1000.",3.3. Algorithm,[0],[0]
"This O(m3) cost does not substantially bottleneck computation, as we will validate in experiments.",3.3. Algorithm,[0],[0]
"In this section, we discuss how the parameter τ which controls the level of diversity affects the generalization performance of sparse coding and neural network.",4. Analysis,[0],[0]
"Following (Vainsencher et al., 2011), we assume the data example x ∈ Rd and basis vector w ∈",4.1. Sparse Coding,[0],[0]
"Rd are both of unit length, and the linear coefficient vector",4.1. Sparse Coding,[0],[0]
"a ∈ Rm is at most k sparse, i.e., ‖a‖0 ≤",4.1. Sparse Coding,[0],[0]
k.,4.1. Sparse Coding,[0],[0]
"The estimation error of dictionary W is defined as
L(W) = Ex∼p∗",4.1. Sparse Coding,[0],[0]
"[min‖a‖0≤k ‖x− ∑m
j=1 ajwj‖2].",4.1. Sparse Coding,[0],[0]
"(15)
Let L̃(W) =",4.1. Sparse Coding,[0],[0]
1n ∑n i=1 min‖a‖0≤k ‖xi − ∑m j=1 ajwj‖2 be the empirical reconstruction error on n samples.,4.1. Sparse Coding,[0],[0]
"We have the following theorem.
",4.1. Sparse Coding,[0],[0]
Theorem 1,4.1. Sparse Coding,[0],[0]
Assume τ,4.1. Sparse Coding,[0],[0]
"< 1k , with probability at least 1−δ:
L(W) ≤",4.1. Sparse Coding,[0],[0]
"L̃(W)+
√ dm ln 4 √ nk
1−kτ 2n",4.1. Sparse Coding,[0],[0]
"+
√ ln 1/δ
2n +
√ 4
n .",4.1. Sparse Coding,[0],[0]
"(16)
Note that the right hand side is an increasing function w.r.t",4.1. Sparse Coding,[0],[0]
τ .,4.1. Sparse Coding,[0],[0]
"As expected, a smaller τ (implying more diversity) would induce a lower estimation error bound.",4.1. Sparse Coding,[0],[0]
"The generalization error of a hypothesis f represented with a neural network is defined as L(f) = E(x,y)∼p∗",4.2. Neural Network,[0],[0]
"[`(f(x), y)], where p∗ is the distribution of inputoutput pair (x, y) and `(·, ·) is the loss function.",4.2. Neural Network,[0],[0]
The training error is L̂(f) =,4.2. Neural Network,[0],[0]
1n,4.2. Neural Network,[0],[0]
∑n i=1,4.2. Neural Network,[0],[0]
"`(f(x
(i)), y(i)), where n is the number of training samples.",4.2. Neural Network,[0],[0]
"Let f∗ ∈ argminf∈FL(f) be the true risk minimizer and f̂ ∈ argminf∈F L̂(f) be the
empirical risk minimizer.",4.2. Neural Network,[0],[0]
We aim to analyze the generalization error L(f̂) of the empirical risk minimizer f̂ .,4.2. Neural Network,[0],[0]
L(f̂) can be decomposed into L(f̂) = L(f̂),4.2. Neural Network,[0],[0]
"− L(f∗) + L(f∗), where L(f̂) − L(f∗) is the estimation error and L(f∗) is the approximation error.
",4.2. Neural Network,[0],[0]
"For simplicity, we start with a “simple” fully connected network with one hidden layer of m units, used for univariate regression (one output unit) with squared loss.",4.2. Neural Network,[0],[0]
Analysis for more complicated fully connected NNs with multiple hidden layers can be achieved in a straightforward way by cascading our analysis for this “simple” fully connected NN.,4.2. Neural Network,[0],[0]
Let x ∈ Rd be the input vector and y be the response value.,4.2. Neural Network,[0],[0]
"For simplicity, we assume max{‖x‖2, |y|} ≤ 1.",4.2. Neural Network,[0],[0]
"Let wj ∈ Rd be the weights connecting the j-th hidden unit with input units, with ‖wj‖2 ≤",4.2. Neural Network,[0],[0]
"C.
Let α be a vector where αj is the weight connecting hidden unit j to the output unit, with ‖α‖2 ≤",4.2. Neural Network,[0],[0]
"B. We assume the activation function h(t) applied on the hidden units is Lipschitz continuous with constant L. Commonly used activation functions such as rectified linear h(t) = max(0, t), tanh h(t) =",4.2. Neural Network,[0],[0]
"(et − e−t)/(et + e−t), and sigmoid h(t) = 1/(1+ e−t) are all Lipschitz continuous with L = 1, 1, 0.25, respectively.",4.2. Neural Network,[0],[0]
"Consider the hypothesis set
F = {x 7→ m∑ j=1 αjh(w > j x) | ‖α‖2",4.2. Neural Network,[0],[0]
"≤ B, ‖wj‖2 ≤ C,
∀i",4.2. Neural Network,[0],[0]
"6= j, |wi ·wj | ≤ τ‖wi‖2‖wj‖2}.
",4.2. Neural Network,[0],[0]
"The estimation error given in Theorem 2 below indicates how well the algorithm is able to learn from the samples.
",4.2. Neural Network,[0],[0]
"Theorem 2 Let the activation function h be L-Lipschitz continuous and the loss `(ŷ, y) = 12 (ŷ − y)
2.",4.2. Neural Network,[0],[0]
"Then, with probability at least 1− δ:
L(f̂)−L(f∗) ≤",4.2. Neural Network,[0],[0]
"γ2
√ 2 ln(4/δ)",4.2. Neural Network,[0],[0]
"+ 4γB(2CL+ |h(0)|)
√ m√
n",4.2. Neural Network,[0],[0]
"(17) where γ = 1 +BCL √ (m− 1)τ + 1 + √ mB|h(0)|.
Note that γ, hence the above bound on estimation error, decreases as τ becomes smaller.",4.2. Neural Network,[0],[0]
The bound goes to zero as n (sample size) goes to infinite.,4.2. Neural Network,[0],[0]
"The inverse square root dependence on nmatches existing results (Bartlett & Mendelson, 2003).",4.2. Neural Network,[0],[0]
"We note that it is straightforward to extend our bound to any bounded Lipschitz continuous loss `.
",4.2. Neural Network,[0],[0]
"The approximation error indicates how capable the hypothesis set F is to approximate a target function g = E[y|x], where the error is measured by minf∈F‖f − g‖L2 and ‖f − g‖2L2 = ∫ (f(x) − g(x))2P (dx).",4.2. Neural Network,[0],[0]
"Following (Barron, 1993), we assume the target function g satisfies certain smoothness condition that is expressed in the first moment of its Fourier representation: ∫ ‖ω‖2|g̃(ω)|dω ≤ B/2
where g̃(ω) is the Fourier representation of g(x).",4.2. Neural Network,[0],[0]
For such function the following theorem states its approximation error.,4.2. Neural Network,[0],[0]
"(In order to derive explicit constants we restrict h to be the sigmoid function, but other Lipschitz continuous activation function can be similarly handled.)
",4.2. Neural Network,[0],[0]
"Theorem 3 Let C > 1, m ≤ 2(b π 2−θ θ c + 1), where θ = arccos(τ), and h(t) = 1/(1 + e−t).",4.2. Neural Network,[0],[0]
"Then, there is a function f ∈ F such that
‖f − g‖L2 ≤ B( 1√m + 1+2 lnC C )+
+ 2 √ mBC sin(min(3mθ,π)2 ).",4.2. Neural Network,[0],[0]
"(18)
",4.2. Neural Network,[0],[0]
"This theorem implies that whether to use the angular constraint (AC) or not has a significant influence on the approximate error bound: without using AC (τ = 1), the bound is a decreasing function of m (the number of hidden units); using AC (τ < 1), the bound increases with m. This striking phrase-change indicates the impact of AC.",4.2. Neural Network,[0],[0]
"Given a fixed m, the bound decreases with τ , implying that a stronger regularization (smaller τ ) incurs larger approximation error.",4.2. Neural Network,[0],[0]
"When τ = 1, the second term in the bound vanishes and the bound is reduced to the one in (Barron, 1993), which is a decreasing function of m (and C, the upper bound on the weights).",4.2. Neural Network,[0],[0]
"When τ < 1, the second term increases withm up to the upper bound 2(b
π 2−θ θ c+1).",4.2. Neural Network,[0],[0]
"This
is because a larger number of hidden units bear a larger difficulty in satisfying the pairwise ACs, which causes the function space F to shrink rapidly; accordingly, the approximation power of F decreases quickly.
",4.2. Neural Network,[0],[0]
The analysis in the two theorems shows that τ incurs a tradeoff between the estimation error and the approximation error: decreasing τ reduces the estimation error and enlarges the approximation error.,4.2. Neural Network,[0],[0]
"Since the generalization error is the sum of the estimation error and the approximation error, τ has an optimal value to yield the minimal generalization error.",4.2. Neural Network,[0],[0]
"In this section, we present experimental results.",5. Experiments,[0],[0]
"Due to space limit, we put some results into supplements.",5. Experiments,[0],[0]
"Following (Yang et al., 2009), we applied sparse coding for image feature learning.",5.1. Sparse Coding,[0],[0]
"We used three datasets in the experiments: Scenes-15 (Lazebnik et al., 2006), Caltech256 (Griffin et al., 2007) and UIUC-Sport (Li & Fei-Fei, 2007).",5.1. Sparse Coding,[0],[0]
"For each dataset, five random train/test splits are performed and the results are averaged over the five runs.",5.1. Sparse Coding,[0],[0]
"We extract pixel-level dense SIFT (Lowe, 2004) features where the step size and patch size are 8 and 16 respectively.",5.1. Sparse Coding,[0],[0]
"On top of the SIFT features, we use sparse coding methods to learn a dictionary and represent each SIFT
feature into a sparse code.",5.1. Sparse Coding,[0],[0]
"To obtain image-level features, we apply max-pooling (Yang et al., 2009) and spatial pyramid matching (Lazebnik et al., 2006; Yang et al., 2009) over the pixel-level sparse codes.",5.1. Sparse Coding,[0],[0]
Then a linear SVM is applied to classify the images.,5.1. Sparse Coding,[0],[0]
"We compare with other diversity-promoting regularizers including determinant of covariance matrix (DCM) (Malkin & Bilmes, 2008), cosine similarity (CS) (Yu et al., 2011), determinantal point process (DPP) (Kulesza & Taskar, 2012; Zou & Adams, 2012b), InCoherence (IC) (Bao et al., 2013) and mutual angles (MA) (Xie et al., 2015).",5.1. Sparse Coding,[0],[0]
"We use 5-fold cross validation to tune τ in {0.3, 0.4, · · · , 1} and the number of basis vectors in {50, 100, 200, · · · , 500}.",5.1. Sparse Coding,[0],[0]
"The parameter ρ in ADMM is set to 1.
",5.1. Sparse Coding,[0],[0]
"Table 1 shows the classification accuracy on three datasets, from which we can see that compared with unregularized SC, AC-SC greatly improves performance.",5.1. Sparse Coding,[0],[0]
"For example, on the Sports dataset, AC improves the accuracy from 87.4% to 90.9%.",5.1. Sparse Coding,[0],[0]
This suggests that AC is effective in reducing overfitting and improving generalization performance.,5.1. Sparse Coding,[0],[0]
"Compared with other diversity-promoting regularizers, AC achieves better performance, demonstrating its better efficacy in promoting diversity.",5.1. Sparse Coding,[0],[0]
"We evaluate AC on three types of neural networks: fullyconnected NN (FNN) for phone recognition (Hinton et al., 2012), CNN for image classification (Krizhevsky et al., 2012), and RNN for question answering (Seo et al., 2017).",5.2. Neural Networks,[0],[0]
"In the main paper, we report results on four datasets: TIMIT1, CIFAR-102, CNN (Hermann et al., 2015), Daily Mail (Hermann et al., 2015).",5.2. Neural Networks,[0],[0]
"Please refer to the supplements for results on other datasets.
",5.2. Neural Networks,[0],[0]
"FNN for Phone Recognition The TIMIT dataset contains a total of 6300 sentences (5.4 hours), divided into a training set (462 speakers), a validation set (50 speakers) and a core test set (24 speakers).",5.2. Neural Networks,[0],[0]
"We used the Kaldi (Povey et al., 2011) toolkit to train the monophone system which was utilized to do forced alignment and to get labels for speech frames.",5.2. Neural Networks,[0],[0]
The toolkit was also utilized to preprocess the data into log-filter banks.,5.2. Neural Networks,[0],[0]
"Among methods based on FNN, Karel’s recipe in Kaldi achieves state
1https://catalog.ldc.upenn.edu/LDC93S1 2https://www.cs.toronto.edu/ kriz/cifar.html
of the art performance.",5.2. Neural Networks,[0],[0]
We apply AC to the FNN in this recipe.,5.2. Neural Networks,[0],[0]
"The inputs of the FNN are the FMLLR (Gales, 1998) features of the neighboring 21 frames, which are mean centered and normalized to have unit variance.",5.2. Neural Networks,[0],[0]
The number of hidden layers is 4.,5.2. Neural Networks,[0],[0]
Each layer has 1024 hidden units.,5.2. Neural Networks,[0],[0]
Stochastic gradient descent (SGD) is used to train the network.,5.2. Neural Networks,[0],[0]
The learning rate is set to 0.008.,5.2. Neural Networks,[0],[0]
"We compare with four diversity-promoting regularizers: CS, IC, MA and DeCorrelation (DC) (Cogswell et al., 2015).",5.2. Neural Networks,[0],[0]
"The regularization parameter in these methods are tuned in {10−6, 10−5, · · · , 105}.",5.2. Neural Networks,[0],[0]
"The β parameter in IC is set to 1.
",5.2. Neural Networks,[0],[0]
Table 2 shows state of the art phone error rate (PER) on the TIMIT core test set.,5.2. Neural Networks,[0],[0]
"Methods in the first panel are mostly based on FNN, which perform less well than Kaldi.",5.2. Neural Networks,[0],[0]
Methods in the third panel are all based on RNNs which in general perform better than FNN since they are able to capture the temporal structure in speech data.,5.2. Neural Networks,[0],[0]
"In the second panel, we compare AC with other diversity-promoting regularizers.",5.2. Neural Networks,[0],[0]
"Without regularization, the error is 18.53%.",5.2. Neural Networks,[0],[0]
"With AC, the error is reduced to 18.41%, which is very close to a strong RNN-based baseline Connectionist Temporal Classification (CTC) (Graves et al., 2013).",5.2. Neural Networks,[0],[0]
"Besides, AC outperforms other regularizers.
",5.2. Neural Networks,[0],[0]
"CNN for Image Classification The CIFAR-10 dataset contains 32x32 color images from 10 categories, with 50,000 images for training and 10,000 for testing.",5.2. Neural Networks,[0],[0]
We used 5000 training images as the validation set to tune hyperparameters.,5.2. Neural Networks,[0],[0]
"The data is augmented by first zero-padding the images with 4 pixels on each side, then randomly cropping
the padded images to reproduce 32x32 images.",5.2. Neural Networks,[0],[0]
"We apply AC to wide residual network (WideResNet) (Zagoruyko & Komodakis, 2016) where the depth is set to 28 and the width is set to 10.",5.2. Neural Networks,[0],[0]
"SGD is used for training, with epoch number 200, initial learning rate 0.1, minibatch size 128, Nesterov momentum 0.9, dropout probability 0.3 and weight decay 0.0005.",5.2. Neural Networks,[0],[0]
"The learning rate is dropped by 0.2 at 60, 120 and 160 epochs.",5.2. Neural Networks,[0],[0]
The performance is the median of 5 runs.,5.2. Neural Networks,[0],[0]
"We compare with CS, IC, MA, DC and an Orthogonality-Promoting (OP) regularizer (Rodrı́guez et al., 2016).
",5.2. Neural Networks,[0],[0]
Table 3 shows state of the art classification error on the test set.,5.2. Neural Networks,[0],[0]
"Compared with the unregularized WideResNet which achieves an error of 3.89%, applying AC reduces the error to 3.63%.",5.2. Neural Networks,[0],[0]
"AC achieves lower error than other regularizers.
",5.2. Neural Networks,[0],[0]
"LSTM for Question Answering We apply AC to long short-term memory (LSTM) (Hochreiter & Schmidhuber, 1997) network, which is a type of RNN.",5.2. Neural Networks,[0],[0]
"Given the input xt at timestamp t, LSTM produces a hidden state ht based on the following transition equations:
it = σ(W",5.2. Neural Networks,[0],[0]
(i)xt +U (i)ht−1 + b,5.2. Neural Networks,[0],[0]
(i)),5.2. Neural Networks,[0],[0]
ft = σ(W (f)xt +U (f)ht−1 + b (f)),5.2. Neural Networks,[0],[0]
ot = σ(W (o)xt +U (o)ht−1 + b (o)),5.2. Neural Networks,[0],[0]
ct = it tanh(W(c)xt +U(c)ht−1 + b(c)),5.2. Neural Networks,[0],[0]
"+ ft ct−1 ht = ot tanh(ct)
where Ws are Us are gate-specific weight matrices.",5.2. Neural Networks,[0],[0]
"On the row vectors of each weight matrix, the AC is applied.",5.2. Neural Networks,[0],[0]
"The LSTM is used for a question answering (QA) task on two datasets: CNN and DailyMail (Hermann et al., 2015),
each containing a training, development and test set with 300k/4k/3k and 879k/65k/53k examples respectively.",5.2. Neural Networks,[0],[0]
"Each example consists of a passage, a question and an answer.",5.2. Neural Networks,[0],[0]
The question is a cloze-style task where an entity is replaced by a placeholder and the goal is to infer this missing entity (answer) from all the possible entities appearing in the passage.,5.2. Neural Networks,[0],[0]
"The LSTM architecture and experimental settings follow the Bidirectional Attention Flow (BIDAF) (Seo et al., 2017) model, which consists of the following layers: character embedding, word embedding, contextual embedding, attention flow, modeling and output.",5.2. Neural Networks,[0],[0]
LSTM is applied in the contextual embedding and modeling layer.,5.2. Neural Networks,[0],[0]
"Character embedding is based on onedimensional convolutional neural network, where the number of filters is set to 100 and the width of receptive field is set to 5.",5.2. Neural Networks,[0],[0]
"In LSTM, the size of hidden state is set to 100.",5.2. Neural Networks,[0],[0]
"Optimization is based on AdaDelta (Zeiler, 2012), where the minibatch size and initial learning rate are set to 48 and 0.5.",5.2. Neural Networks,[0],[0]
The model is trained for 8 epochs.,5.2. Neural Networks,[0],[0]
"Dropout (Srivastava et al., 2014) with probability 0.2 is applied.",5.2. Neural Networks,[0],[0]
"We compare with four diversity promoting regularizers: CS, IC, MA and DC.
",5.2. Neural Networks,[0],[0]
Table 4 shows state of the art accuracy on the two datasets.,5.2. Neural Networks,[0],[0]
"As can be seen, after applying AC to BIDAF, the accuracy is improved from 76.94% to 77.23% on the CNN test set and from 79.63% to 79.88% on the DailyMail test set.",5.2. Neural Networks,[0],[0]
"Among the diversity-promoting regularizers, AC achieves the highest accuracy.",5.2. Neural Networks,[0],[0]
"In the theoretical analysis presented in Section 4, we have shown that the parameter τ which controls the level of near-
orthogonality (or diversity) incurs a tradeoff between estimation error and approximation error.",5.3. Sensitivity to Parameter τ,[0],[0]
"In this section, we provide an empirical verification, using FNN on TIMIT as a study case.",5.3. Sensitivity to Parameter τ,[0],[0]
Figure 1 shows how the phone error rates vary on the TIMIT core test set.,5.3. Sensitivity to Parameter τ,[0],[0]
"As can be seen, the lowest test error is achieved under a moderate τ (= 0.75).",5.3. Sensitivity to Parameter τ,[0],[0]
Either a smaller or a larger τ degrades the performance.,5.3. Sensitivity to Parameter τ,[0],[0]
This empirical observation is aligned with the theoretical analysis that the best generalization performance is achieved under a properly chosen τ .,5.3. Sensitivity to Parameter τ,[0],[0]
"When τ is close to 0, the hidden units are close to orthogonality, which yields much poorer performance.",5.3. Sensitivity to Parameter τ,[0],[0]
"This confirms that the strict-orthogonality constraint proposed by (Le et al., 2010) is too restrictive and is less favorable than a “soft” regularization approach.",5.3. Sensitivity to Parameter τ,[0],[0]
We compare the computational time of neural networks under different regularizers.,5.4. Computational Time,[0],[0]
"Table 5 shows the total runtime time of FNNs on TIMIT and CNNs on CIFAR-10 with a single GTX TITAN X GPU, and the runtime of LSTM networks on the CNN dataset with 2 TITAN X GPUs.",5.4. Computational Time,[0],[0]
"Compared with no regularization, AC incurs a 18.2% extra time on TIMIT, 12.7% on CIFAR-10 and 14.8% on CNN.",5.4. Computational Time,[0],[0]
The runtime of AC is comparable to that under other diversitypromoting regularizers.,5.4. Computational Time,[0],[0]
"In this paper, we propose Angled-Constrained Latent Space Models (AC-LSMs) that aim at promoting diversity among components in LSMs for the sake of alleviating overfitting.",6. Conclusions,[0],[0]
"Compared with previous diversity-promoting methods, AC has two benefits.",6. Conclusions,[0],[0]
"First, it is theoretically analyzable: the generalization error analysis shows that larger diversity leads to smaller estimation error and larger approximation error.",6. Conclusions,[0],[0]
"Second, it is empirically effective, as validated in various experiments.",6. Conclusions,[0],[0]
"We would like to thank the anonymous reviewers for the suggestions and comments that help to improve this work a lot, and thank Yajie Miao for helping with some of the experiments.",Acknowledgements,[0],[0]
"P.X and E.X are supported by National Institutes of Health P30DA035778, R01GM114311, National Science Foundation IIS1617583, DARPA FA872105C0003 and Pennsylvania Department of Health BD4BH4100070287.",Acknowledgements,[0],[0]
"The large model capacity of latent space models (LSMs) enables them to achieve great performance on various applications, but meanwhile renders LSMs to be prone to overfitting.",abstractText,[0],[0]
"Several recent studies investigate a new type of regularization approach, which encourages components in LSMs to be diverse, for the sake of alleviating overfitting.",abstractText,[0],[0]
"While they have shown promising empirical effectiveness, in theory why larger “diversity” results in less overfitting is still unclear.",abstractText,[0],[0]
"To bridge this gap, we propose a new diversitypromoting approach that is both theoretically analyzable and empirically effective.",abstractText,[0],[0]
"Specifically, we use near-orthogonality to characterize “diversity” and impose angular constraints (ACs) on the components of LSMs to promote diversity.",abstractText,[0],[0]
A generalization error analysis shows that larger diversity results in smaller estimation error and larger approximation error.,abstractText,[0],[0]
An efficient ADMM algorithm is developed to solve the constrained LSM problems.,abstractText,[0],[0]
Experiments demonstrate that ACs improve generalization performance of LSMs and outperform other diversitypromoting approaches.,abstractText,[0],[0]
Learning Latent Space Models with Angular Constraints,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 141–147 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2022",text,[0],[0]
"Across social media, thousands of posts daily take the form of informal FIRST-PERSON NARRATIVES.",1 Introduction,[0],[0]
These narratives provide a rich resource for computational modeling of how people feel about the events they report on.,1 Introduction,[0],[0]
"Being able to reliably predict the affect a person may feel towards events they encounter has a range of potential applications, including monitoring mood and mental health (Isaacs et al., 2013) and getting conversational assistants to respond appropriately (Bowden et al., 2017).",1 Introduction,[0],[0]
"Moreover, as these narratives are told from the perspective of a protagonist, this research could be used to understand other types of protagonist-framed narratives, like those in fiction.
",1 Introduction,[0],[0]
"We are interested in the opinions that a protagonist has, not the author per se.",1 Introduction,[0],[0]
This is sometimes referred to as internal sentiment or self reflective sentiment.,1 Introduction,[0],[0]
"While in many situations that is overlaid with the author’s opinions, in first-personal narratives, because the author is the protagonist, the two perspectives align.",1 Introduction,[0],[0]
"Here, we use the term affect to reference this protagonist-centered notion of opinion.
",1 Introduction,[0],[0]
"A central obstacle to reliable affect prediction is that that people tend not to explicitly flag their affective state, by saying I am happy.",1 Introduction,[0],[0]
"Large-scale sentiment dictionaries focus on compiling lexical items that bear a consistent affect all on their own (Wilson et al., 2005).",1 Introduction,[0],[0]
"But people tend to describe situations, such as My friend bought me flowers, or I got a parking ticket, from which other humans can readily infer their implicit affective reactions.
",1 Introduction,[0],[0]
"One approach to this problem aims to directly learn units larger than a lexical item that reliably bear some marker of polarity or emotion (Vu et al., 2014; Li et al., 2014; Ding and Riloff, 2016; Goyal et al., 2010; Russo et al., 2015; Kiritchenko et al., 2014; Reckman et al., 2013).
",1 Introduction,[0],[0]
"Another approach aims to model the speaker’s affect to an event compositionally, e.g. Anand and Reschke (2010) (A&R) proposed that the affect a lexical predicate communicates should be modeled as an n-ary function, taking as inputs the affect that the speaker bears towards each partici-
141
pant.",1 Introduction,[0],[0]
Table 1 contains A&R’s functions for verbs of possession: a state in which X has Y or X lacks Y does not convey a clear affect unless we know what the speaker thinks of both X and Y .,1 Introduction,[0],[0]
"If the speaker has positive affect toward both X and Y (Row 1), then we infer that her attitude toward the event is positive, but if either is negative, then we infer that the speaker is negative toward the event.",1 Introduction,[0],[0]
"Similarly, Rashkin et al. (2016) represent the typical affect communicated by particular predicates via connotation frames.",1 Introduction,[0],[0]
"Here we are finding the internal sentiment of the speaker, or, as Rashkin et al. refer to it, the ”mental state” of the speaker.
",1 Introduction,[0],[0]
"Inspired by A&R’s framework, our work learns lexico-functional patterns (patterns involving lexical items or pairs of lexical items in specific grammatical relations that we show to capture functorargument relations in A&R’s sense), about the effects of combining particular arguments with particular verbs (event types) from first-person narratives.",1 Introduction,[0],[0]
Our novel observation is that learning these compositional functions is greatly simplified in the case of first-person affect.,1 Introduction,[0],[0]
"People bear positive affect to themselves, so sentences with first-person elements, e.g. I/we/me, reduce the problem for an approach like A&R’s to learning the polarity that results from composing the verb with only one of its arguments, i.e. only Rows 1, 2 in Table 1 need to be learned for first person subjects.",1 Introduction,[0],[0]
Firstperson narratives are full of such sentences.,1 Introduction,[0],[0]
See Table 2.,1 Introduction,[0],[0]
"We show that the learned patterns are often consonant with A&R’s predictions, but are richer, including e.g. many private state descriptions (Wiebe et al., 2004; Wiebe, 1990).
",1 Introduction,[0],[0]
"In addition, we demonstrate that these lexicofunctional patterns improve the performance of several off-the-shelf sentiment analyzers.",1 Introduction,[0],[0]
"We show that Stanford sentiment (Socher et al., 2013) has a best performance of 0.67 macro F on our test set.",1 Introduction,[0],[0]
We then supplement it with our learned patterns and demonstrate significant improvements.,1 Introduction,[0],[0]
"Our final ensemble achieves 0.75 F on the test set.
",1 Introduction,[0],[0]
We discuss related work in more detail in Sec. 5.,1 Introduction,[0],[0]
"We start with a set of first-person narratives (weblogs) drawn from the Spinn3r corpus, that cover a wide range of topics (Burton et al., 2009; Gordon and Swanson, 2009).",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"To reduce noise, we restrict the blogs to those from well-known blogging sites (Ding and Riloff, 2016), and select 15,466 stories whose length ranges from 225 to 375 words.
",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"We hand-annotate a set of 477 positive and 440 negative stories, and use these to bootstrap a larger set of 1,420 negative and 2,288 positive stories.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"To bootstrap, we apply AutoSlog-TS, a weakly supervised pattern learner that only requires training sets os stories labeled broadly as POSITIVE or NEGATIVE (Riloff, 1996; Riloff and Wiebe, 2003).",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
AutoSlog uses a set of syntactic templates to define different types of linguistic expressions.,2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"The left-hand side of Table 3 lists examples of AutoSlog patterns and the right-hand side illustrates a specific lexical-syntactic pattern that corresponds to each general pattern template, as instantiated in first-person stories.1 When bootstrapping a larger positive and negative story corpus, we use the whole story, not just the first person sentences.
",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"The left-hand-side of Table 3 shows that the learned patterns can involve syntactic arguments
1The examples are shown as general expressions for readability, but the actual patterns must match the syntactic constraints associated with the pattern template.
of the verbal predicate, which means that these patterns are proxies for one column of verbal function tables like those in Table 1.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"However, they can also include verb-particle constructions, such as cheated on, or verb-head-of-preposition constructions.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"In each case though, because these patterns are localized to a verb and only one element, they allow us to learn highly specific patterns that could be incorporated into a dictionary such as +- Effect (Choi and Wiebe, 2014).",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"AutoSlog simultaneously harvests both (syntactically constrained) MWE patterns and more compositionally regular verb-argument groups at the same time.
",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"AutoSlog-TS computes statistics on the strength of association of each pattern with each class, i.e. P(POSITIVE | p) and P(NEGATIVE | p), along with the pattern’s overall frequency.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"We define three parameters for each class: θf , the frequency with which a pattern occurs, θp, the probability with which a pattern is associated with the given class and θn, the number of patterns that must occur in the text for it to be labeled.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"These parameters are tuned on the dev set (Riloff, 1996; Oraby et al., 2015; Riloff and Wiebe, 2003).
",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"To bootstrap a larger corpus, we want settings that have lower recall but very high precision.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"We select θp = 0.7, θf = 10 and θn = 3 for the positive class and θp = 0.85, θf = 10 and θn = 4 for the negative class for bootstrapping.",2 Bootstrapping a First-Person Sentiment Corpus,[0],[0]
"Our experimental setup involves first creating a corpus of training and test sentences, then applying AutoSlog-TS a second time to learn linguistic patterns.",3 Experimental Setup,[0],[0]
We then set up methods for cascading classifiers to explore whether ensemble classifiers improve our results.,3 Experimental Setup,[0],[0]
"Training Set: From the bootstrapped set of stories, we create a corpus of sentences.",3 Experimental Setup,[0],[0]
"A critical simplifying assumption of our method is that a multi-sentence story can be labelled as a whole as positive or negative, and that each of its sentences inherit this polarity.",3 Experimental Setup,[0],[0]
This means we can learn the polarity of events in such narratives from their (noisy) inherited polarity without labelling individual sentences.,3 Experimental Setup,[0],[0]
"Our training set consists of 46,255 positive and 25,069 negative sentences.",3 Experimental Setup,[0],[0]
Test Set: We create the test set by selecting 4k random first-person sentences.,3 Experimental Setup,[0],[0]
"First-person sentences either contain an explicit first person marker, i.e. we and my or start with either a progressive verb or pleonastic it.",3 Experimental Setup,[0],[0]
"To collect gold labels, we designed a qualifier and a HIT for Mechanical Turk, and put
these out for annotation by 5 Turkers, who label each instance as positive, negative, or neutral.",3 Experimental Setup,[0],[0]
"To ensure the high quality of the test set, we select sentences that were labelled consistently positive or negative by 4 or 5 Turkers.",3 Experimental Setup,[0],[0]
"We collected 1,266 positive and 1,440 negative sentences.
",3 Experimental Setup,[0],[0]
"Dev Set: We created the dev set using the same method as the test set, having Turkers annotate 2k random first-person sentences.",3 Experimental Setup,[0],[0]
We collected 498 positive and 754 negative sentences.,3 Experimental Setup,[0],[0]
"The 4k test and dev sentences available for download at https://nlds.soe.ucsc.edu/first-person-sentiment.
AutoSlog First-Person Sentence Classifier.",3 Experimental Setup,[0],[0]
"In order to learn new affect functions, we develop a second sentence-level classifier using AutoSlogTS.",3 Experimental Setup,[0],[0]
"We run AutoSlog over the training corpus, using the dev set to tune the parameters θf , θp and θn (?), in order to maximize macro F-score.",3 Experimental Setup,[0],[0]
"Our best parameters on the dev set for positive is θf=18, θp=0.85 and θn=1 and for negative is θf=1, θp=0.5 and θn=1.",3 Experimental Setup,[0],[0]
We specify that if the sentence is in both classes we rename it as neutral.,3 Experimental Setup,[0],[0]
"We will refer to this classifier as the AutoSlog classifier.
",3 Experimental Setup,[0],[0]
Baseline First-Person Sentence Classifiers.,3 Experimental Setup,[0],[0]
Our goal is to see whether the knowledge we learn using AutoSlog-TS complements existing sentiment classifiers.,3 Experimental Setup,[0],[0]
"We thus experiment with a number of baseline classifiers: the default SVM classifier from Weka with unigram features (Hall et al., 2005), a version of the NRC-Canada sentiment classifier (Mohammad et al., 2013), provided to us by Qadir and Riloff (2014), and the Stanford Sentiment classifier (Socher et al., 2013).
",3 Experimental Setup,[0],[0]
Retrained Stanford.,3 Experimental Setup,[0],[0]
"The Stanford Sentiment classifier is a based on Recursive Neural Networks, and trained on a compositional Sentiment Treebank, which includes fine-grained sentiment labels for 215,154 phrases from 11,855 sentences from movie reviews.",3 Experimental Setup,[0],[0]
It can accurately predict some compositional semantic effects and handle negation.,3 Experimental Setup,[0],[0]
"However since it was trained on movie reviews, it is likely to be missing labelled data for some common phrases in our blogs.",3 Experimental Setup,[0],[0]
Thus we also retrained it (RETRAINED STANFORD) on high precision phrases from AutoSlog extracted from our training data of positive and negative blogs.,3 Experimental Setup,[0],[0]
"This provides 67,710 additional phrases, including 58,972 positive phrases and 8,738 negative phrases.",3 Experimental Setup,[0],[0]
The retrained model includes both the labels from the original Sentiment Treebank and the AutoSlog high precision phrases.,3 Experimental Setup,[0],[0]
We present our experimental results and analyze the results in terms of the lexico-functional linguistic patterns we learn.,4 Results and Analysis,[0],[0]
Baseline Classifiers.,4 Results and Analysis,[0],[0]
"Rows 1-3 of Table 4 show the results for the three baselines, in terms of Fscore for each class and the macro F. Stanford outperforms both NRC and SVM, but misses many cases of positive sentiment.",4 Results and Analysis,[0],[0]
AutoSlog Classifier.,4 Results and Analysis,[0],[0]
Row 4 of Table 4 shows the results for the AutoSlog classifier.,4 Results and Analysis,[0],[0]
"Although AutoSlog itself does not perform highly, the patterns that it learns represent a different type of knowledge than what is contained in many sentiment analysis tools.",4 Results and Analysis,[0],[0]
"We therefore hypothesized that a cascading classifier, which supplements one of the baseline sentiment classifiers with the lexicofunctional patterns that AutoSlog learns might yield higher performance.",4 Results and Analysis,[0],[0]
Retrained Stanford.,4 Results and Analysis,[0],[0]
Row 5 of Table 4 shows the results for RETRAINED STANFORD.,4 Results and Analysis,[0],[0]
The F-scores for RETRAINED STANFORD are almost identical to the standard Stanford classifier.,4 Results and Analysis,[0],[0]
This may be because our data is a small percentage of the entire number of phrases used in training Stanford.,4 Results and Analysis,[0],[0]
"Although RETRAINED STANFORD prioritizes our phrases, it would not make sense to remove the original training data.",4 Results and Analysis,[0],[0]
Cascading Classifiers.,4 Results and Analysis,[0],[0]
We implement cascading classifiers to test our hypothesis.,4 Results and Analysis,[0],[0]
"The cascade classifier has primary and secondary classifiers, and we invoke the secondary classifiers only if the primary assigns a prediction of neutral to a test instance, which reflects the lack of sentimentbearing lexical items.",4 Results and Analysis,[0],[0]
"We also have a cascade classifier with a tertiary classifier, which is invoked in the same fashion as the secondary classifier after the primary and secondary classifiers have been run.",4 Results and Analysis,[0],[0]
"The cascading classifiers are named in the order the classifier is employed, primary, secondary or primary, secondary, tertiary.",4 Results and Analysis,[0],[0]
"For our cascading classifiers, we combine our baseline classifiers (NRC and Stanford), with our AutoSlog classifier.",4 Results and Analysis,[0],[0]
We do not use SVM as a primary classifier since it has no neutral label.,4 Results and Analysis,[0],[0]
"The results for the cascading experiments are in Rows 6-9 of Table 4.
",4 Results and Analysis,[0],[0]
"Cascading NRC and AutoSlog provides the best performance, improving both the positive and negative classes, for a macro F of 0.71.",4 Results and Analysis,[0],[0]
"This shows that the learned implicit polarity information from AutoSlog improves NRC’s performance.
",4 Results and Analysis,[0],[0]
"Since our best two-classifier cascade comes
from combining NRC and AutoSlog, we also test a cascade that adds Stanford or SVM.",4 Results and Analysis,[0],[0]
"We achieve our best macro F of 0.75 for the combination with SVM.
Analysis and Discussion.",4 Results and Analysis,[0],[0]
"Here we discuss how the patterns we learned from AutoSlog can supplement the knowledge encoded in current sentiment classifiers, and in newly evolving sentiment resources (Goyal et al., 2010; Choi and Wiebe, 2014; Balahur et al., 2012; Ruppenhofer and Brandes, 2015).
",4 Results and Analysis,[0],[0]
Tables 5 and 6 illustrate several learned lexicofunctional patterns for positive events used in the AutoSlog classifier.,4 Results and Analysis,[0],[0]
"The patterns shown in Table 5 are predicted by A&R’s framework, some functions of which can be seen in Table 1.",4 Results and Analysis,[0],[0]
"For example, we find a range of basic state descriptions (have party, have cancer) whose basic entailment category is either possessive or property state.",4 Results and Analysis,[0],[0]
"Since Ehave is positive for a first-person subject only if the object is positive, and negative if the object is negative, we predict that parties are good to possess and that cancer is a bad property to have.",4 Results and Analysis,[0],[0]
"In this way, we can recruit the existing function for have to induce new positive or negative things to “possess.”",4 Results and Analysis,[0],[0]
"In line with A&R’s claims, many events are identified with their final results: headed for results in being at a desired location, while not coming home results in something failing to be at a desired location.",4 Results and Analysis,[0],[0]
"We find it a welcome result that our semi-supervised methods
yield patterns that correspond to the A&R classes, thus validating our suspicion that first-person sentences furnish a simplifying test ground for discovering functional patterns in the wild.
",4 Results and Analysis,[0],[0]
"However, many patterns are not covered by A&R’s general classes, see Table 6.",4 Results and Analysis,[0],[0]
"Looking first at verbs, one major correlation is between positive classes and public events and negative classes and private states.",4 Results and Analysis,[0],[0]
"Verbs extracted from the positive class tend to be eventive and agentive describing more dynamic activities and interactions, such as played, swim, enjoyed, and danced.",4 Results and Analysis,[0],[0]
"Even many positive have uses are light verbs describing an activity such as have lunch.
",4 Results and Analysis,[0],[0]
Verbs from the negative class are strikingly different.,4 Results and Analysis,[0],[0]
"They are very often stative, where the author is the experiencer (cognitive subject) of that private state.",4 Results and Analysis,[0],[0]
"While this state vs. event distinction is not one existing computational models of sentiment or affect discuss explicitly, it replicates a finding that consistently emerges in clinical psychology, one that is explicitly argued for in cognitive-behavioral accounts of the mood that particular activities evoke (Lewinsohn et al., 1985; MacPhillamy and Lewinsohn, 1982; Russo et al., 2015).",4 Results and Analysis,[0],[0]
"In addition, Table 6 reveals several novel result state categories.",4 Results and Analysis,[0],[0]
"The success, planning, and unmet desire frames are all ultimately about goalfulfillment (or lack thereof).",4 Results and Analysis,[0],[0]
"While the success and unmet desire cases could be understood as having or lacking something, the planning cases indicate steps achieved toward a desired end-state.",4 Results and Analysis,[0],[0]
Previous work on learning affect from eventuality descriptions has largely focused on actions.,4 Results and Analysis,[0],[0]
Our results indicate that private state descriptions are another rich source of evidence.,4 Results and Analysis,[0],[0]
"Previous work learns phrasal markers of implicit polarity via bootstrapping from largescale text sources, e.g. Vu et al. (2014)
learn emotion-specific event types by extracting emotion,event pairs on Twitter.",5 Related Work,[0],[0]
"Li et al. (2014) uses Twitter to bootstrap ‘major life events’ and typical replies to those events.
",5 Related Work,[0],[0]
Ding and Riloff (2016) extract subj-verb-obj triples from blog posts.,5 Related Work,[0],[0]
They then apply label propagation to spread polarity from sentences to events.,5 Related Work,[0],[0]
"However, the triples they learn do not focus on first-person experiencers.",5 Related Work,[0],[0]
"They also filter private states out of the verbs used to learn their triples, whereas we have found that verbs relating to private states such as need, want and realize are important indicators of first-person affect.
",5 Related Work,[0],[0]
"Balahur et al. (2012) use the narratives produced by the ISEAR questionnaire (Scherer et al., 1986) for first-person examples of particular emotions and extract sequences of subject-verb-object triples, which they annotate for basic emotions.
",5 Related Work,[0],[0]
"Recent work has built on this idea, and developed methods to automatically expand Anand & Reschke’s verb classes to create completely new lexical resources (Balahur et al., 2012; Choi and Wiebe, 2014; Deng et al., 2013; Deng and Wiebe, 2014; Ruppenhofer and Brandes, 2015).",5 Related Work,[0],[0]
"Choi & Wiebe’s work comes closest to ours in trying to induce (not annotate) lexical functions, but we attempt to infer these from stories directly, whereas they use a structured lexical resource.",5 Related Work,[0],[0]
We show that we can learn lexico-functional linguistic patterns that reliably predict first-person affect.,6 Conclusion,[0],[0]
We constructed a dataset of positive and negative first-person experiencer sentences and used them to learn such patterns.,6 Conclusion,[0],[0]
We then showed that the performance of current sentiment classifiers can be enhanced by augmenting them with these patterns.,6 Conclusion,[0],[0]
"By adding our AutoSlog classifier’s results to existing classifiers we were able to improve from a baseline 0.67 to 0.75 Macro F with a cascading classifier of NRC, AutoSlog and SVM.",6 Conclusion,[0],[0]
"In addition, we analyze the linguistic functions that indicate positivity and negativity for the first person experiencer, and show that they are very different.",6 Conclusion,[0],[0]
"In first-person descriptions, positivity is often signaled by active participations in events, while negativity involves private states.",6 Conclusion,[0],[0]
"In future work, we plan to explore the integration of these observations into sentiment resources such as the +-Effect lexicon (Choi and Wiebe, 2014).",6 Conclusion,[0],[0]
We plan to apply these high precision first-person lexical patterns beyond blog data and with other personmarking.,6 Conclusion,[0],[0]
Informal first-person narratives are a unique resource for computational models of everyday events and people’s affective reactions to them.,abstractText,[0],[0]
People blogging about their day tend not to explicitly say I am happy.,abstractText,[0],[0]
Instead they describe situations from which other humans can readily infer their affective reactions.,abstractText,[0],[0]
However current sentiment dictionaries are missing much of the information needed to make similar inferences.,abstractText,[0],[0]
We build on recent work that models affect in terms of lexical predicate functions and affect on the predicate’s arguments.,abstractText,[0],[0]
We present a method to learn proxies for these functions from firstperson narratives.,abstractText,[0],[0]
"We construct a novel fine-grained test set, and show that the patterns we learn improve our ability to predict first-person affective reactions to everyday events, from a Stanford sentiment baseline of .67F to .75F.",abstractText,[0],[0]
Learning Lexico-Functional Patterns for First-Person Affect,title,[0],[0]
"Many real-world processes of interest, ranging from climate variables to brain signals, are spatio-temporal in nature, cf.",1. Introduction,[0],[0]
Cressie & Wikle (2011).,1. Introduction,[0],[0]
"That is, they can be described as a random quantity that varies over some fixed spatial and temporal domain.",1. Introduction,[0],[0]
"Suppose we obtain n training points from a real-valued spatio-temporal process,
Dn = { (s1, t1, y1), . . .",1. Introduction,[0],[0]
", (sn, tn, yn) } ,
where yi denotes the quantity of interest observed at the ith training point, with spatial coordinate si and time ti.",1. Introduction,[0],[0]
"For notational convenience, let (s, t, y) denote an unobserved test point in space-time where y is unknown.",1. Introduction,[0],[0]
"Then a common goal is to predict y in unobserved space-time regions (s, t) using Dn.",1. Introduction,[0],[0]
"Specifically, certain spatial regions may have limited data coverage over extended periods of time, as illustrated in Figure 1.",1. Introduction,[0],[0]
"In real-world applications, Dn need not be gathered in a single batch but obtained in parts over time from various sensors, stations, satellites,
*Equal contribution 1Uppsala University, Sweden.",1. Introduction,[0],[0]
"Correspondence to: Muhammad Osama <muhammad.osama@it.uu.se>, Dave Zachariah <dave.zachariah@it.uu.se>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
etc.",1. Introduction,[0],[0]
"That is, the dataset is augmented sequentially, i.e., n = 1, 2, . . .",1. Introduction,[0],[0]
", N .",1. Introduction,[0],[0]
"In these streaming data scenarios, we are interested in continuous refinement of the prediction of y at (s, t) as new data is augmented into Dn+1.
",1. Introduction,[0],[0]
The unknown data-generating process is often assumed to belong to a class of data models indexed by a parameter θ.,1. Introduction,[0],[0]
"Each model θ in the class yields a predictor ŷθ(s, t) of y at test point (s, t).",1. Introduction,[0],[0]
A specific set of model parameters θ̂ is learned using Dn.,1. Introduction,[0],[0]
"Examples of commonly used model classes include Gaussian Processes (GP) (Rasmussen & Williams, 2006), spatio-temporal random effects models (Cressie et al., 2010), dynamic factor analysis models (Lopes et al., 2008; Fox & Dunson, 2015), spatial random effect models extended to incorporate time as an additional dimension (Zammit-Mangion & Cressie, 2017) (cf. related work section below).",1. Introduction,[0],[0]
"For many spatio-temporal applications, the model class should be capable of expressing temporal patterns that change across different spatial regions.",1. Introduction,[0],[0]
"Moreover, for streaming data scenarios, the learned parameter θ̂ and the resulting predictor ŷθ̂(s, t) should be updated in a sequential manner.
",1. Introduction,[0],[0]
"Our contribution in this paper is two-fold:
• we develop a non-stationary, localized covariance model capable of capturing temporal patterns that change across space, as illustrated in Figure 2 below.
",1. Introduction,[0],[0]
"• we show how to sequentially learn the covariance model parameters and update the predictor from streaming spatio-temporal data, with a runtime that is linear in n.
",1. Introduction,[0],[0]
"In Section 2, we relate our work to already existing approaches and introduce a commonly used model class in Section 3.",1. Introduction,[0],[0]
In Section 4 we develop a localized spatiotemporal covariance model to be used in conjunction with a covariance-fitting learning approach.,1. Introduction,[0],[0]
"Finally, the proposed method is evaluated using synthetic and real climate data in Sections 5 and 6, respectively.
",1. Introduction,[0],[0]
"Notation: col{s1, s2} stacks both elements into a single column vector.",1. Introduction,[0],[0]
"⊗, δ(·), ‖ · ‖W and † denote the Kronecker product, Kronecker delta function, weighted `2-norm and Moore-Penrose inverse, respectively.",1. Introduction,[0],[0]
"Finally, the sample mean is denoted by Ê[si] = 1n",1. Introduction,[0],[0]
∑n i=1 si.,1. Introduction,[0],[0]
"A popular model class is the family of GPs, specified by a mean and covariance function (Rasmussen & Williams, 2006).",2. Related work,[0],[0]
"This approach is computationally prohibitive in its basic form since both learning the model parameters θ and implementing the predictor ŷθ(s, t) requires a runtime on the order of O(N3), where N is typically large in spatio-temporal applications.",2. Related work,[0],[0]
The predictor implementation can be approximated using various techniques.,2. Related work,[0],[0]
"One popular approach is to approximate the training data using m N inducing points which reduces the runtime toO(m2N) (Quiñonero-Candela & Rasmussen, 2005; Bijl et al., 2015).",2. Related work,[0],[0]
"Moreover, by assuming Kronecker covariance functions it is possible to obtain even shorter runtimes by utilizing the Kronecker structure of the GP covariance matrix (Saatçi, 2012).",2. Related work,[0],[0]
"If the model class is restricted to stationary covariance functions, the runtimes can be reduced further, cf. (Saatçi, 2012; Wilson et al., 2014).",2. Related work,[0],[0]
"In the spacetime domain, such models are also equivalent to dynamical system models so that ŷθ(s, t) can be approximated using a basis expansion and implemented by a Kalman smoother (Särkkä et al., 2013).",2. Related work,[0],[0]
"In the above cases, however, θ and ŷθ(s, t) are not updated jointly when obtaining streaming data.
",2. Related work,[0],[0]
"The restriction to stationary covariance models is, moreover, not always adequate to capture temporal patterns that differ across spatial regions.",2. Related work,[0],[0]
"This modeling limitation is addressed by Cressie et al. (2010), where a discrete-time model class is partially specified using a spatial basis function expansion with time-varying expansion coefficients.",2. Related work,[0],[0]
These are modeled as a first-order vector auto-regressive process.,2. Related work,[0],[0]
The coefficients thus determine a spatial pattern of the process that evolves at each discrete time-instant.,2. Related work,[0],[0]
"This model class can capture patterns localized to specific regions in space, unlike stationary covariance models.",2. Related work,[0],[0]
"The predictor ŷθ(s, t) can be viewed as a spatial fixed-rank kriging method that is updated via a Kalman filter and thus applicable to streaming data (cf. Cressie & Johannesson (2008)).",2. Related work,[0],[0]
"The model parameter θ, however, is learned us-
ing a moment-fitting approach and operates on batch rather than streaming datasets.",2. Related work,[0],[0]
"Other work using dynamic factor analysis models (Lopes et al., 2008; Fox & Dunson, 2015) similarly allow for time-varying coefficients but with more flexible data-adaptive basis.",2. Related work,[0],[0]
"However, they are implemented using Markov Chain Monte Carlo methods which are computationally prohibitive for the scenarios considered herein.
",2. Related work,[0],[0]
"Moreover, a first-order auto-regressive structure may not accurately capture more complex temporal patterns observed in real spatio-temporal processes.",2. Related work,[0],[0]
The approach taken by Zammit-Mangion & Cressie (2017) circumvents this limitation using basis functions that are localized in both space and time.,2. Related work,[0],[0]
"Time locality cannot, however, capture periodic patterns or trends necessary for interpolation over longer periods.",2. Related work,[0],[0]
The model parameters are learned using an expectation-maximization method which is not readily applicable to streaming data scenarios.,2. Related work,[0],[0]
"We begin by defining the data vector y = col{y1, y2, . . .",3. Spatio-temporal model class,[0],[0]
", yn} obtained from Dn.",3. Spatio-temporal model class,[0],[0]
"For the test point (s, t), we consider the unbiased predictor of y as a linear combination of the data (Stein, 2012):
ŷ(s, t) = λ>(s, t)y, (1)
where λ>(s, t) is a vector of n weights which naturally depend on the test point (s, t).",3. Spatio-temporal model class,[0],[0]
The weight vector is defined as the minimizer of the conditional mean square prediction error.,3. Spatio-temporal model class,[0],[0]
"That is,
λ(s, t) , arg min λ
E",3. Spatio-temporal model class,[0],[0]
"[ (y − λ>y)2 ∣∣ s, t ].",3. Spatio-temporal model class,[0],[0]
"(2)
Since the conditional error is determined by the unknown distribution p(y,y|s, t, s1, t1, . . .",3. Spatio-temporal model class,[0],[0]
", sn, tn), we specify a class of data-generating models, using only the mean and covariance (Cressie & Wikle, 2011):{
E[y] = u>(s, t)η, Cov[y, y′] = φ>(s, t)Θφ(s′, t′) + θ0δ(s, s ′)δ(t, t′).
",3. Spatio-temporal model class,[0],[0]
"(3) The function u(s, t) captures the expected trend of the entire spatio-temporal process y, and when there is no such general trend we set u(s, t) ≡ 1.",3. Spatio-temporal model class,[0],[0]
"The function φ(s, t) captures the smoothness of the process in space-time and is of dimension p × 1.",3. Spatio-temporal model class,[0],[0]
"The parameter matrix Θ is diagonal and specifies the relevance of each dimension of φ(s, t) similar to the way in which automatic relevance determination is sometimes used within the GP (Tipping, 2001; Faul & Tipping, 2002).",3. Spatio-temporal model class,[0],[0]
"Taken together, (3) specifies a class of models, each of which is indexed by the parameters (η,Θ, θ0).",3. Spatio-temporal model class,[0],[0]
"The p+1 covariance parameters (Θ, θ0), which we collectively
denote by θ = col{θ0, θ1, . . .",3. Spatio-temporal model class,[0],[0]
", θp} for notational convenience, determine the spatio-temporal covariance structure Covθ[y, y
′] which depends on the function φ(s, t).",3. Spatio-temporal model class,[0],[0]
"In the next section, we will specify φ(s, t) to develop a suitable covariance model to capture local spatial and periodic temporal patterns.
",3. Spatio-temporal model class,[0],[0]
"For a given model in the class, the optimal weights (2) are given in closed form (Stein, 2012) as
λθ(s, t) = K −11(1>K−11)† + K−1Π⊥ΦΘφ(s, t),
where the subindex highlights the model parameter dependence.",3. Spatio-temporal model class,[0],[0]
"The quantities in λθ(s, t) are determined by the regressor matrix
Φ = [ φ(s1, t1) . . .",3. Spatio-temporal model class,[0],[0]
"φ(sn, tn) ]> and the following covariance matrix
Kθ = Cov[y,y] = ΦΘΦ > + θ0I 0, (5)
with Π⊥ = I− 1(1>K−11)†1K−1 being an oblique projector onto span(1)⊥.
The optimal weights",3. Spatio-temporal model class,[0],[0]
are invariant to the mean parameters ∼ η and to uniform scaling of the p + 1 covariance parameters θ.,3. Spatio-temporal model class,[0],[0]
"By learning θ up to an arbitrary scale factor, the predictor (1) is given by the linear combiner weights λθ(s, t).",3. Spatio-temporal model class,[0],[0]
"If we assume that the process is Gaussian, the model can be learned using the maximum likelihood framework.",3. Spatio-temporal model class,[0],[0]
"However, this yields neither a convex problem nor one that is readily solved in a sequential manner as Dn is augmented sequentially.",3. Spatio-temporal model class,[0],[0]
"In the next section, we apply a convex covariance-fitting framework to learn the spatiotemporal model using streaming data.",3. Spatio-temporal model class,[0],[0]
"Below we specify the function φ(s, t) in (3) such that the spatio-temporal covariance structure Covθ[y, y′] can express local spatial patterns with varying temporal periodicities as illustrated in Figure 2.",4. Proposed method,[0],[0]
"Subsequently, we apply a covariance-fitting methodology for learning the model parameters such that the predictor (1) can be updated sequentially for each new observation (Zachariah et al., 2017).",4. Proposed method,[0],[0]
"The function φ(s, t) varies over a space-time domain S × T ⊂ Rd+1 and its elements can be thought of as basis functions.",4.1. Local-periodic space-time basis,[0],[0]
"It is formulated as a Kronecker product of a time and space bases,
φ(s, t) = ψ(t)⊗ϕ(s), (6)
for compactness.
",4.1. Local-periodic space-time basis,[0],[0]
"We begin by specifying the spatial function as
ϕ(s) = ϕ1(s1)⊗ · · · ⊗ϕd(sd), (7)
where the basis vector for the ith spatial dimension,
ϕi(si) = col{ ϕi,1(si), · · · , ϕi,Ns(si) } (8)
is composed of Ns localized components with a finite support L. For notational simplicity, we consider Ns and L to be same for each dimension i.",4.1. Local-periodic space-time basis,[0],[0]
"Based on their computational attractiveness and local approximation properties we use a cubic spline basis (Rasmussen & Williams, 2006; Wasserman, 2006).",4.1. Local-periodic space-time basis,[0],[0]
"Then (8) is given by (4), where c determines the location of a component.",4.1. Local-periodic space-time basis,[0],[0]
Figure 3a illustrates the components as a function of its spatial dimension.,4.1. Local-periodic space-time basis,[0],[0]
"We place the centers c of each component uniformly across the spatial dimensions.
",4.1. Local-periodic space-time basis,[0],[0]
Using ϕ(s) allows for covariance structures that are localized in space in such a way that neighbouring points have a nonnegative correlation and points far from each other have no correlation as determined by the support size,4.1. Local-periodic space-time basis,[0],[0]
"L. Hence for a given L, the resulting covariance structure can capture local spatial patterns of a certain scale and can easily be extended to cover multiple scales by replacing (6) with for example
φ(s, t) = ψ(t)⊗",4.1. Local-periodic space-time basis,[0],[0]
[ ϕL1(s) ϕL2(s) ],4.1. Local-periodic space-time basis,[0],[0]
that accommodates two different support sizes L1 and L2.,4.1. Local-periodic space-time basis,[0],[0]
The number of basis functions Ns is chosen such that adjacent localized components ϕi(s) have overlapping support to cater for points in between them.,4.1. Local-periodic space-time basis,[0],[0]
"This requirement is
ϕ(s) =  1 6f(s) 3 (c−2)L 4 ≤ s <",4.1. Local-periodic space-time basis,[0],[0]
(c−1)L 4 −1 2 f(s) 3 + 2f(s)2,4.1. Local-periodic space-time basis,[0],[0]
− 2f(s) + 23 (c−1)L 4 ≤ s <,4.1. Local-periodic space-time basis,[0],[0]
Lc 4 1 2f(s) 3,4.1. Local-periodic space-time basis,[0],[0]
− 4f(s)2 + 10f(s)− 223 Lc 4 ≤ s <,4.1. Local-periodic space-time basis,[0],[0]
(c+1)L 4 −1 6 f(s) 3 + 2f(s)2,4.1. Local-periodic space-time basis,[0],[0]
"− 8f(s) + 323 (c+1)L 4 ≤ s ≤ (c+2)L 4
0 otherwise
where f(s) = 4s
L −c+2
(4)
fulfilled by choosing Ns > RsL where Rs is the range of the spatial dimension.",4.1. Local-periodic space-time basis,[0],[0]
"For example when Ns = 2RsL , the adjacent component ϕi(s) have 50 percent overlap.",4.1. Local-periodic space-time basis,[0],[0]
"The maximum value of Ns is limited by the number of training points and the computational resources that are available.
",4.1. Local-periodic space-time basis,[0],[0]
"The temporal function ψ(t) is also specified by a basis
ψ(t) = col{ ψ0(t), ψ1(t), . . .",4.1. Local-periodic space-time basis,[0],[0]
", ψNt(t) }.",4.1. Local-periodic space-time basis,[0],[0]
"(9)
However, to be able to predict missing data of the type illustrated in Figure 1 we cannot rely on a localized basis for extended interpolations over space-time.",4.1. Local-periodic space-time basis,[0],[0]
Due to its good approximating properties we instead apply the periodic basis developed by Solin & Särkkä (2014) defined over a range T =,4.1. Local-periodic space-time basis,[0],[0]
"[0, Rt]:
ψk(t) =
{ 1, k = 0,
1√ Rt sin (kπ t+Rt2Rt ), otherwise, (10)
Similar to a Fourier basis, ψ(t) allows for periodic covariance structures that capture both fixed and periodic patterns in the data along time with different frequencies.",4.1. Local-periodic space-time basis,[0],[0]
"Moreover, as Nt grows, any temporally stationary covariance structure can be captured, cf.",4.1. Local-periodic space-time basis,[0],[0]
"(Solin & Särkkä, 2014).",4.1. Local-periodic space-time basis,[0],[0]
"Using (10), the maximum frequency in the model is Nt4Rt .",4.1. Local-periodic space-time basis,[0],[0]
"Hence,
depending on the data and the highest frequency periodic patterns we may expect in it, an appropriate value of Nt can be chosen.
",4.1. Local-periodic space-time basis,[0],[0]
"In summary, the proposed spatio-temporal basis φ(s, t) in (6) is of dimension p = Nds (Nt+1) and yields a covariance function Covθ[y, y′]",4.1. Local-periodic space-time basis,[0],[0]
"that may vary temporally with different frequencies specific to different spatial regions, as illustrated in Figure 2.",4.1. Local-periodic space-time basis,[0],[0]
"The covariance structure is determined by the parameter θ, which we learn using a covariancefitting methodology considered next.",4.1. Local-periodic space-time basis,[0],[0]
"We describe a covariance-fitting approach for learning the model parameter θ, up to an arbitrary scale factor, from streaming data.",4.2. Learning method for streaming data,[0],[0]
"Given a training dataset Dn, this approach enables us to update the predictor ŷθ(s, t) =",4.2. Learning method for streaming data,[0],[0]
"λ>θ (s, t)y from (1) in a streaming fashion as n = 1, 2, . . .",4.2. Learning method for streaming data,[0],[0]
.,4.2. Learning method for streaming data,[0],[0]
"We consider fitting the model covariance structure of the training data y, which is parameterized by θ in (5), to the empirical structure.",4.2. Learning method for streaming data,[0],[0]
"Let us first define a normalized sample covariance matrix of the training data,
K̃ =",4.2. Learning method for streaming data,[0],[0]
"(y − 1η)(y − 1η)>
‖y",4.2. Learning method for streaming data,[0],[0]
"− 1η‖2 .
",4.2. Learning method for streaming data,[0],[0]
"Here 1 corresponds to using u(s, t) ≡ 1.",4.2. Learning method for streaming data,[0],[0]
Then the optimal model parameters are given by a covariance-fitting criterion (cf.,4.2. Learning method for streaming data,[0],[0]
"(Cressie, 1985; Anderson, 1989; Cressie & Johannesson, 2008; Stoica et al., 2011)) with minimizer:
θ̂ = arg min θ ∥∥K̃−Kθ∥∥2K−1θ",4.2. Learning method for streaming data,[0],[0]
(11) Here the matrix norm corresponds to a weighted norm which penalizes correlated residuals.,4.2. Learning method for streaming data,[0],[0]
"The learned parameter θ̂ is invariant with respect to the mean parameter η and can be rescaled by an arbitrary scale factor (Zachariah et al., 2017).",4.2. Learning method for streaming data,[0],[0]
"Moreover, the resulting predictor corresponding to θ̂ in equation (1) can be written in the equivalent form:
ŷθ̂(s, t) ≡",4.2. Learning method for streaming data,[0],[0]
α >,4.2. Learning method for streaming data,[0],[0]
"(s, t)w?",4.2. Learning method for streaming data,[0],[0]
"(12)
where α(s, t) = col{1,φ(s, t)}.",4.2. Learning method for streaming data,[0],[0]
"The (p + 1)-dimensional weight vector w? is defined as the minimizer
w? = arg min w
√ Ê [ |yi −α>(si, ti)w|2 ]",4.2. Learning method for streaming data,[0],[0]
"+
1√ N ‖ζ w‖1
(13) where the elements of ζ are given by
ζj =
{ 1√ N ‖[Φ]j−1‖2, j > 1,
0, otherwise,
For proofs of these relations and a derivation of its computational properties, see Zachariah et al. (2017).
",4.2. Learning method for streaming data,[0],[0]
The resulting predictor in (12) is called the SPICE (sparse iterative covariance-based estimation) predictor.,4.2. Learning method for streaming data,[0],[0]
"It is computed via a convex and sparsifying regularized minimization problem that can be solved using coordinate descent with recursively updated quantities at each new training point (sn, tn, yn).",4.2. Learning method for streaming data,[0],[0]
"By exploiting this structure, our predictor ŷθ̂(s, t) can now be updated with streaming data as n = 1, 2, . . . .",4.2. Learning method for streaming data,[0],[0]
A pseudocode implementation is provided in Algorithm 1.,4.2. Learning method for streaming data,[0],[0]
The key recursively updated quantities passed from one update to the next are the symmetric matrix Γ and the vectors ρ and w̌ of dimension p+1 along with the scalar ∼ κ.,4.2. Learning method for streaming data,[0],[0]
"Here w̌ is the weight vector at sample n− 1, which is initialized at zero along with the above variables in Algorithm 1.",4.2. Learning method for streaming data,[0],[0]
The runtime is linear in n and constant in memory.,4.2. Learning method for streaming data,[0],[0]
"That is, for a fixed training data sizeN , the total runtime of the algorithm is on the order O(Np2) and its memory requirement is O(p2).",4.2. Learning method for streaming data,[0],[0]
"For further details, we refer the reader to the supplementary material.",4.2. Learning method for streaming data,[0],[0]
Code available at github.,4.2. Learning method for streaming data,[0],[0]
The proposed method has been derived for predictions using large and/or streaming data sets.,5. Synthetic data,[0],[0]
"We now demonstrate its predictive properties using synthetic data and for the sake of reference compare it with a GPR (Gaussian process regression) method using different covariance functions Cov[y, y′].
",5. Synthetic data,[0],[0]
"Algorithm 1 Learning from streaming datasets Input: (sn, tn, yn) and w̌ Γ := Γ +α(sn, tn)α
>",5. Synthetic data,[0],[0]
"(sn, tn) ρ := ρ+α(sn, tn)yn κ := κ+",5. Synthetic data,[0],[0]
y2n := κ+ w̌>Γw̌ − 2w̌>ρ τ,5. Synthetic data,[0],[0]
":= ρ− Γw̌ repeat j = 1, . . .",5. Synthetic data,[0],[0]
", p+ 1 cj := τj + Γjjw̌j if j = 1 then wj :=
cj Γjj
else aj := + Γjjw̌ 2 j + 2w̌jτj
ŝj",5. Synthetic data,[0],[0]
":= sign(cj) r̂j := |cj | Γjj − 1Γjj
√ ajΓjj−|cj |2
n−1
wj :=
{ ŝj r̂j √ n− 1|cj | >",5. Synthetic data,[0],[0]
"√ ajΓjj − |cj |2
0 otherwise end if := + Γjj(w̌j",5. Synthetic data,[0],[0]
− w?j )2 + 2(w̌j,5. Synthetic data,[0],[0]
− w?j )τj τ := τ +,5. Synthetic data,[0],[0]
"[Γ]j(w̌j − w?j )
until number of iterations equal L Output: w? = w̌",5. Synthetic data,[0],[0]
"To illustrate a dynamically evolving process, we consider planar a wave in one-dimensional space and time, cf.",5.1. Damped planar wave,[0],[0]
Figure 4a.,5.1. Damped planar wave,[0],[0]
"The unknown process is generated according to:
y(s, t) =",5.1. Damped planar wave,[0],[0]
"cos
( 2π
λs (s− vst)
)",5.1. Damped planar wave,[0],[0]
"exp ( − s
20
) + ε, (14)
where vs is the speed of the wave along space in units per second, λs is the wavelength in units of space and ε is a zero-mean white Gaussian process with standard deviation σ.
",5.1. Damped planar wave,[0],[0]
Note that the process decays exponentially as it propagates through space.,5.1. Damped planar wave,[0],[0]
"For our experiments, we set vs = 3",5.1. Damped planar wave,[0],[0]
"[spatial units/sec], λs = 9",5.1. Damped planar wave,[0],[0]
[spatial units] and σ = 0.3.,5.1. Damped planar wave,[0],[0]
Synthetic data is generated over a uniform grid and a subset of N = 700 training points are used.,5.1. Damped planar wave,[0],[0]
"Different contiguous space-time blocks are selected as test regions to resemble realistic scenarios in which the coverage of sensors, satellites or other measurement equipment is incomplete.",5.1. Damped planar wave,[0],[0]
"For example, the dashed white boxes in Figure 4 emulate cases where data over a small region is missing most of the time.",5.1. Damped planar wave,[0],[0]
"By contrast, the dashed black boxes correspond to cases when data over large spatial regions is missing some of the time.
",5.1. Damped planar wave,[0],[0]
"The process in these test regions as well as at other randomly missing points is predicted using the proposed
method with Nt = 25, Ns = 15 and a spatial basis support set to L = 5 spatial units.",5.1. Damped planar wave,[0],[0]
"This results in φ(s, t) being of dimension p = Ns(Nt + 1) = 390.",5.1. Damped planar wave,[0],[0]
The mean-square error (MSE) of the prediction is shown in Figure 4b and evaluated using 25 Monte Carlo simulations.,5.1. Damped planar wave,[0],[0]
"The region in the white box extends over almost the entire time dimension, hence there are very few neighbouring training points in time to draw upon for prediction and no information about the periodicities in the region.",5.1. Damped planar wave,[0],[0]
Instead our method leverages the neighbouring spatial information to obtain a good prediction resulting in a low MSE.,5.1. Damped planar wave,[0],[0]
Both black boxes are test regions that have neighbouring training points that provide temporal information about the process.,5.1. Damped planar wave,[0],[0]
"However, left region has training points both before and after whereas the right region only has points before, yielding a more challenging prediction problem.",5.1. Damped planar wave,[0],[0]
"Nevertheless, the proposed method is able to learn both the periodic and the local damping patterns to provide accurate predictions in both regions.
",5.1. Damped planar wave,[0],[0]
We include also the MSE of GPR using two different covariance functions learned by a numerical maximum likelihood search.,5.1. Damped planar wave,[0],[0]
"While this method is not applicable to the streaming data of interest here, it provides a performance reference.",5.1. Damped planar wave,[0],[0]
"First, we use a Matérn ARD covariance model (Rasmussen & Nickisch, 2010) to carefully adapt both space and time dimensions.",5.1. Damped planar wave,[0],[0]
In Figure 4c it is seen that the resulting prediction errors are markedly worse for the large missing spatial regions and the method naturally fails to capture the periodic pattern of the process.,5.1. Damped planar wave,[0],[0]
"Next, we use a periodic Matérn ARD covariance model to also capture space-time periodicity.",5.1. Damped planar wave,[0],[0]
"However, the MSE (Figure 4d) is degraded throughout, which is possibly due to the non-convex optimization problem used to learn the model parameters.",5.1. Damped planar wave,[0],[0]
"It may lead to local minima issues, including learning erroneous periods.",5.1. Damped planar wave,[0],[0]
Here we generate a process that emulates scenarios of temporal periodicities which may vary across spatial regions.,5.2. Varying seasonalities across space,[0],[0]
This occurs e.g. in climate data.,5.2. Varying seasonalities across space,[0],[0]
"Figure 5a shows a realization of a process generated according to
y(s, t) =",5.2. Varying seasonalities across space,[0],[0]
"cos
( 2π
T (s) t
) + ε (15)
where the period T (s) differs across space and ε is zeromean white Gaussian process with standard deviation σ = 0.3.",5.2. Varying seasonalities across space,[0],[0]
"In the upper region of the spatial domain T (s) = ∞, i.e., the process has a constant mean.",5.2. Varying seasonalities across space,[0],[0]
"In the middle and bottom regions T (s) is large and small, respectively.",5.2. Varying seasonalities across space,[0],[0]
The data is generated over a uniform grid and a subset of N = 600 points is used for training.,5.2. Varying seasonalities across space,[0],[0]
"A contiguous space-time block, marked by the dashed black box in Figure 5, forms a test region to emulate scenarios where data can be missing
over a large spatial region for some time.
",5.2. Varying seasonalities across space,[0],[0]
"For the proposed method we use Nt = 35, Ns = 15 and a support of L = 3 for the spatial basis, so that p = Ns(Nt + 1) = 540.",5.2. Varying seasonalities across space,[0],[0]
For the GPR we use the periodic Matérn ARD kernel.,5.2. Varying seasonalities across space,[0],[0]
Figures 5b and 5c show the MSE performance of the proposed method and GPR respectively which were obtained using 25 Monte Carlo simulations.,5.2. Varying seasonalities across space,[0],[0]
"The MSE of the proposed method is overall lower than that of GPR, both in the dashed test region as well as outside it.",5.2. Varying seasonalities across space,[0],[0]
"Unlike the proposed method, GPR has one parameter to fit to an overall periodic pattern and is thus unable to learn spatially localized patterns.",5.2. Varying seasonalities across space,[0],[0]
"Thus after learning, the process is predicted to be be nearly constant along time for all parts of the spatial region.",5.2. Varying seasonalities across space,[0],[0]
"We now demonstrate the proposed method for much larger, and possibly streaming, real-world datasets.",6. Real data,[0],[0]
"As a first application example, we use tropical pacific Sea Surface Temperature (SST) data (Wikle, 2011).",6.1. Pacific Sea Surface Temperature,[0],[0]
"These data represent gridded monthly SST anomalies, in ◦C, from January 1970 through March 2003 over a spatial region from 29◦S to 29◦N and 124◦E to 70◦W.",6.1. Pacific Sea Surface Temperature,[0],[0]
"The spatial resolution of the data is 2◦ in both latitude and longitude.
",6.1. Pacific Sea Surface Temperature,[0],[0]
"Here we consider data from the first 36 months, making the total number of space-time data points equal to 36 × 2 520 = 90 720.",6.1. Pacific Sea Surface Temperature,[0],[0]
"In the first experiment, training points are sampled randomly across space-time and the missing data constitute the test points.",6.1. Pacific Sea Surface Temperature,[0],[0]
Here we set N = 63 503 as the number of training points.,6.1. Pacific Sea Surface Temperature,[0],[0]
"For the proposed method we set Nt = 100, Ns = 8 and the spatial support L to be half of each spatial dimension.",6.1. Pacific Sea Surface Temperature,[0],[0]
Then p = N2s (Nt+ 1) = 6 464.,6.1. Pacific Sea Surface Temperature,[0],[0]
Figure 6a shows the prediction error histogram of all test points across the spatio-temporal domain.,6.1. Pacific Sea Surface Temperature,[0],[0]
"We see that it is centered around zero and its dispersion is considerably narrower than the dynamic range of the data.
",6.1. Pacific Sea Surface Temperature,[0],[0]
"In the second experiment, we select a contiguous spacetime block as a test region in addition to other test points to evaluate the performance in scenarios where data over entire spatial regions are missing for a period of time.",6.1. Pacific Sea Surface Temperature,[0],[0]
"Data falling within the spatial region marked by the black dashed box in Figure 6c is missing beyond month 26, as indicated by the black dashed line in Figure 6d.",6.1. Pacific Sea Surface Temperature,[0],[0]
Here N = 18 144 are the number of training points.,6.1. Pacific Sea Surface Temperature,[0],[0]
The prediction error histogram for this second experiment is shown in Figure 6b and remains fairly narrow.,6.1. Pacific Sea Surface Temperature,[0],[0]
Figure 6c illustrates the predicted SST anomalies [◦C] for a spatial slice at month t = 30.,6.1. Pacific Sea Surface Temperature,[0],[0]
"We pick a spatial point in a region where the El Niño effect, i.e., the periodic warming of the equatorial Pa-
cific Sea Surface (Sarachik & Cane, 2010), is known to be noticeable.",6.1. Pacific Sea Surface Temperature,[0],[0]
The prediction of the SST anomalies at this spatial location across time along with the true SST is illustrated with Figure 6d.,6.1. Pacific Sea Surface Temperature,[0],[0]
Note that the predictor is able to track the rising temperature deviation also for the missing data.,6.1. Pacific Sea Surface Temperature,[0],[0]
"As a second application example, we use precipitation data from the Climate Research Unit (CRU) time series datasets of climate variations (Jones & Harris, 2013).",6.2. Precipitation data,[0],[0]
The precipitation data consists of monthly rainfall in millimeter over a period from 1901 to 2012 obtained with high spatial resolution (0.5 by 0.5 degree) over the whole planet.,6.2. Precipitation data,[0],[0]
Here we consider a five year period from 2001 to 2005 and between spatial coordinates 95◦W to 107◦W and 40◦N to 50◦N.,6.2. Precipitation data,[0],[0]
"This yields a total number of 28 800 data points.
",6.2. Precipitation data,[0],[0]
"The spatial region indicated by the black dashed box in Figure 7b beyond month t = 47, as seen in Figure 7c, constitutes a contiguous test region, in addition to other randomly selected test points.",6.2. Precipitation data,[0],[0]
"The remaining N = 14 400 points are used for training.
",6.2. Precipitation data,[0],[0]
"For the proposed method we set Nt = 300, Ns = 6 and the spatial support L to be half of each spatial dimension.",6.2. Precipitation data,[0],[0]
Then p = N2s (Nt + 1) = 10 836.,6.2. Precipitation data,[0],[0]
Figure,6.2. Precipitation data,[0],[0]
"7a shows the
prediction error histogram for the precipitation test data.",6.2. Precipitation data,[0],[0]
It is centered around zero and its dispersion is narrower than the dynamic range of the data.,6.2. Precipitation data,[0],[0]
Figure,6.2. Precipitation data,[0],[0]
7b shows the contour plot of predicted precipitation for a spatial slice at month t = 54.,6.2. Precipitation data,[0],[0]
"The red cross and plus marker indicate spatial points whose actual and predicted time series are compared in Figures 7c and 7d, respectively.",6.2. Precipitation data,[0],[0]
Note that the estimated precipitation tracks the true precipitation well everywhere even to the right of the black dashed line where the data was not seen during training.,6.2. Precipitation data,[0],[0]
Note the ability of the predictor to track the different seasonal patterns in the missing regions.,6.2. Precipitation data,[0],[0]
"We proposed a method in which a spatio-temporal predictor ŷθ̂(s, t) can be learned and updated sequentially as spatio-temporal data is obtained as a stream.",7. Conclusion,[0],[0]
"It is capable of capturing spatially varying temporal patterns, using a non-stationary covariance model that is learned using a covariance-fitting approach.",7. Conclusion,[0],[0]
"We demonstrated, using both simulated and real climate data, that it is capable of producing accurate predictions in large unobserved space-time test regions.",7. Conclusion,[0],[0]
"In future work, we intend to further improve the computational efficiency of the method by exploiting the spatially localized structure of the covariance model.
",7. Conclusion,[0],[0]
-2,7. Conclusion,[0],[0]
-1.5 -1,7. Conclusion,[0],[0]
-0.5 0,7. Conclusion,[0],[0]
"0.5 1 1.5 2
Prediction error
0
500
1000
1500
2000
2500
N u m
b e
r o
f p o
in ts
(a)
-2 -1.5 -1",7. Conclusion,[0],[0]
-0.5 0,7. Conclusion,[0],[0]
"0.5 1 1.5 2
Prediction error
0
100
200
300
400
500
600
700
800
900
N u
m b
e r
o f
p o
in ts
(b)
140",7. Conclusion,[0],[0]
o E 160 o,7. Conclusion,[0],[0]
E 180 o,7. Conclusion,[0],[0]
E 160 o,7. Conclusion,[0],[0]
W 140 o,7. Conclusion,[0],[0]
W 120 o,7. Conclusion,[0],[0]
W 100 o,7. Conclusion,[0],[0]
W 80 o,7. Conclusion,[0],[0]
"W
Longitude
25 o S
20 o S
15 o S
10 o S
5 o S
0
5 o N
10 o N
15 o N
20 o N
25 o N
L a
ti tu
d e
-0.5
0
0.5
1",7. Conclusion,[0],[0]
"NewLEADS - New Directions in Learning Dynamical Systems (contract number: 621-2016-06079), funded by the Swedish Research Council and the project ASSEMBLE (contract number: RIT15-0012), funded by the Swedish Foundation for Strategic Research (SSF).",8. Acknowledgements,[0],[0]
"We address the problem of predicting spatiotemporal processes with temporal patterns that vary across spatial regions, when data is obtained as a stream.",abstractText,[0],[0]
"That is, when the training dataset is augmented sequentially.",abstractText,[0],[0]
"Specifically, we develop a localized spatio-temporal covariance model of the process that can capture spatially varying temporal periodicities in the data.",abstractText,[0],[0]
We then apply a covariance-fitting methodology to learn the model parameters which yields a predictor that can be updated sequentially with each new data point.,abstractText,[0],[0]
The proposed method is evaluated using both synthetic and real climate data which demonstrate its ability to accurately predict data missing in spatial regions over time.,abstractText,[0],[0]
Learning Localized Spatio-Temporal Models From Streaming Data,title,[0],[0]
"Multivariate temporal sequences arise in a wide range of applications, where the pattern of interest is represented as a sequence of local feature vectors.",1. Introduction,[0],[0]
The local features may be high-dimensional and contain noisy information.,1. Introduction,[0],[0]
"Thus it is desirable to reduce the dimension of the features in sequences by projecting them to a discriminative low-dimensional subspace, in which sequence classification would become faster and more accurate.
",1. Introduction,[0],[0]
"Various supervised dimensionality reduction (DR) methods
1Science & Technology on Integrated Information System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China 2Department of Electrical Engineering and Computer Science, Northwestern University, Evanston, IL, USA.",1. Introduction,[0],[0]
"Correspondence to: Bing Su <subingats@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
have been developed for vector data under the i.i.d. assumption, but they cannot be applied to the features in sequences by omitting the temporal dependencies.",1. Introduction,[0],[0]
"DR for sequence data aims at learning a subspace by maximizing the separability among sequence classes, where the separability embodies in the differences on temporal structures.",1. Introduction,[0],[0]
"The temporal structures reflect the common evolutions of all sequences from the same class, and they depend on temporal alignments to establish correspondences among sequences with local temporal differences.",1. Introduction,[0],[0]
The separability and objective are more difficult to formulate and manipulate by nature.,1. Introduction,[0],[0]
"For these reasons, DR for sequence data has received much less attention.
",1. Introduction,[0],[0]
"Existing methods such as Linear Sequence Discriminant Analysis (LSDA) (Su & Ding, 2013; Su et al., 2018) and Max-Min inter-Sequence Distance Analysis (MMSDA) (Su et al., 2017a) construct the separability based on generative models.",1. Introduction,[0],[0]
"For each class, they train a leftto-right Hidden Markov Model (HMM) (Rabiner, 1989)) from the original sequences.",1. Introduction,[0],[0]
"The mean of the features aligned to each hidden state is calculated, and the means of all ordered states form a mean sequence.",1. Introduction,[0],[0]
"The interclass distance is measured as the Dynamic Time Warping (DTW) (Sakoe & Chiba, 1978) distance between the mean sequences.",1. Introduction,[0],[0]
"Such separability depends on the alignments between the sequences and the hidden states, which further rely on the similarities of the features.",1. Introduction,[0],[0]
"When projecting the features to a subspace, the local similarities among the transformed features may change, and hence the alignments may change accordingly.",1. Introduction,[0],[0]
"On the other hand, the projection is determined by maximizing the separability, where the separability should be constructed based on the alignments in the subspace.",1. Introduction,[0],[0]
"Therefore, learning the projection and inferring the alignments are entangled.",1. Introduction,[0],[0]
"To make it tractable, existing methods simply fix the alignments in the underlying subspace to those in the original space.",1. Introduction,[0],[0]
"However, the resulting separability cannot reflect the real confusion relationship between classes in the subspace.",1. Introduction,[0],[0]
"Also, HMM-based separability requires a large number of sequences for training and is poor in scalability.
",1. Introduction,[0],[0]
"In this paper, we propose a supervised DR method for sequence data called Latent Temporal Linear Discriminant Analysis (LT-LDA).",1. Introduction,[0],[0]
"We learn an abstract template for each class to discover the temporal structures via employ-
ing the modified DTW barycenter (Petitjean et al., 2011; Su et al., 2016).",1. Introduction,[0],[0]
We then construct the separability among sequence classes based on the alignments between the abstract templates and the training sequences.,1. Introduction,[0],[0]
"Although determining the alignments by learning the abstract templates and learning the subspace by maximizing the constructed separability still rely on each other, we show that their objectives are actually connected, which allows us to jointly learn the most discriminative subspace together with the associated latent alignments, resulting in sequences of lowdimensional discriminative temporal representations.
",1. Introduction,[0],[0]
The main contributions are as follows.,1. Introduction,[0],[0]
"(1) Different from the HMM-based separability, our new construction of separability does not require lots of training data.",1. Introduction,[0],[0]
It can be performed even when only one training sequence per class is available.,1. Introduction,[0],[0]
"(2) Different from previous methods where the subspace can only be learned through pre-fixing the alignments, we propose to learn the subspace and the latent alignments simultaneously and develop an efficient iterative solution.",1. Introduction,[0],[0]
The learned subspace is thereby holistically optimal.,1. Introduction,[0],[0]
"(3) We establish a connection between our objective formulation and the abstract template learning, which ensures the convergence of our solution.",1. Introduction,[0],[0]
We further provide theoretical insight on the subspace selection.,1. Introduction,[0],[0]
"Various supervised linear DR methods have been proposed for vector data, such as Linear Discriminant Analysis (LDA) (Fisher, 1936), Marginal Fisher Analysis (Yan et al., 2007), and Max-min Distance Analysis (Bian & Tao, 2011; Zhang & Yeung, 2010).",2. Related Work,[0],[0]
"LDA optimizing the Fisher criterion is perhaps the most widely used method for its simpleness, effectiveness and the well-established theory, and is getting consistent interest (De la Torre & Kanade, 2006; Ding & Li, 2007; Ye et al., 2007; Nikitidis et al., 2014) in machine learning.",2. Related Work,[0],[0]
"These methods cannot be applied to vectors in sequences, which violate the basic i.i.d.",2. Related Work,[0],[0]
assumption.,2. Related Work,[0],[0]
"Our method performs DR for sequence data by lifting the inherent temporal dependencies.
",2. Related Work,[0],[0]
"In (Zhou & De la Torre, 2012; Trigeorgis et al., 2018), linear and non-linear transformations were learned for each sequence pair to perform multi-modal alignments.",2. Related Work,[0],[0]
The transformations for different sequence pairs are different.,2. Related Work,[0],[0]
"In our method, the projection is for discriminating different classes and stays the same for all sequences from all classes.",2. Related Work,[0],[0]
"In (Shyr et al., 2010), a sufficient DR approach was proposed for sequence labeling by building sequence kernels.",2. Related Work,[0],[0]
"The labels are associated with the vectors in sequences rather than the whole sequences, and the task is to predict a class label for each vector in the sequences.",2. Related Work,[0],[0]
"In (Flamary et al., 2012), the features are transformed by unidimensional convolutions of all dimensions
for sequence labeling.",2. Related Work,[0],[0]
Our method focuses on linear projection and the task is to predict a label for each entire sequence.,2. Related Work,[0],[0]
"In (Lajugie et al., 2014), a Mahalanobis distance was learned given the ground-truth alignments of training samples for multivariate sequence alignment, while in our method the alignments of both the training sequences and the test sequences are unavailable.
",2. Related Work,[0],[0]
"LSDA (Su & Ding, 2013; Su et al., 2018) and MMSDA (Su et al., 2017a) targeted at the same problem as this paper, where the projection was learned by maximizing the separability defined on HMM-based temporal structures.",2. Related Work,[0],[0]
The alignments of the sequences to the hidden states in the original space and the underlying subspace were assumed to be the same.,2. Related Work,[0],[0]
"LSDA optimized the Fisher criterion and made further approximations on the inter-class scatter to make the optimization tractable; MMSDA optimized the max-min distance criterion, resulting in solving a series of time-consuming semi-definite programming problems and cannot scale to high dimension.",2. Related Work,[0],[0]
"Differently, in our method, the discovery of temporal structures is DTW-based and only depends on deterministic operations, which avoids the estimation of massive parameters of HMM;",2. Related Work,[0],[0]
"The latent alignments in the subspace can be jointly learned with the projection owing to our construction of separability.
",2. Related Work,[0],[0]
"Recurrent Neural Networks (RNNs) (Graves et al., 2013; Sutskever et al., 2014) have seldom been used for DR but often as classifiers.",2. Related Work,[0],[0]
"The sequences can be projected first by our method, and then input to RNNs for classification.",2. Related Work,[0],[0]
"This way, the input sequences are more discriminative and RNNs need to learn fewer parameters.",2. Related Work,[0],[0]
We learn an abstract template M consisting of ordered temporal structures for each sequence class from all its training sequence samples.,3.1. Learning Abstract Templates,[0],[0]
Each sequence X =,3.1. Learning Abstract Templates,[0],[0]
"[x1,x2, · · · ,xT ] consists of a series of ordered frame-wide feature vectors, where xt is the feature vector extracted from the t-th frame, and T is the length of the sequence.",3.1. Learning Abstract Templates,[0],[0]
"For a specific sequence class, we denote its training sequence sample set by {X1,X2, · · · ,XN}, where N is the number of training sequences in the set, and Tn is the length of Xn.",3.1. Learning Abstract Templates,[0],[0]
"Different sequence samples may have different lengths.
",3.1. Learning Abstract Templates,[0],[0]
We define the abstract template as a sequence of the abstracted temporal structures M =,3.1. Learning Abstract Templates,[0],[0]
"[m1,m2, · · · ,mL], where the element mj captures the average frame-wide features of a temporal structure or stage that each sequence must go through.",3.1. Learning Abstract Templates,[0],[0]
Hence M can be considered as an atomic sequence.,3.1. Learning Abstract Templates,[0],[0]
"L is the length of M, which is generally shorter than any sequence sample, because the learned template on-
ly contains the essential temporal structures and each structure will last several frames in a sequence.
",3.1. Learning Abstract Templates,[0],[0]
M can be used to divide a sequence sample X into different temporal regions.,3.1. Learning Abstract Templates,[0],[0]
"This is achieved by aligning X to M with a warping function, which can be defined by a warping path P =",3.1. Learning Abstract Templates,[0],[0]
"[p1,p2, · · · ,pL].",3.1. Learning Abstract Templates,[0],[0]
pt =,3.1. Learning Abstract Templates,[0],[0]
"[st, et]T means that the {st, st +1, · · · , et}-th elements in X are aligned to the t-th element of M. Similar to DTW, several constraints are applied to P: (1) s1 = 1, eL = T ; (2) et < st+1, ∀t = 1, · · · , L − 1; (3) et ≥ st; (4) lt ≤ aTL , where lt=et−st+1 is the number of elements in X that are aligned to the t-th element in M, a ≥ 1 is a factor that controls the allowed degree of warping.",3.1. Learning Abstract Templates,[0],[0]
"This constraint means that the number of elements in X aligned to any element in M should not exceed a multiple of the average number, and hence prevents extremely unbalanced partitioning.",3.1. Learning Abstract Templates,[0],[0]
"Therefore, only salient temporal structures that are universal in all training sequences can be captured by M.
We employ the modified DTW algorithm (Su et al., 2016; 2017b) to compute the optimal warping path.",3.1. Learning Abstract Templates,[0],[0]
"We denote the cost of a partial path of aligning the first i elements in X to the first j elements in M as c(i, j, l), where the last l elements of the first i elements in X are aligned to the j-th element in M. c(i, j, l) can be determined recurrently:
c(i, j, l) =  d(i, j), l = 1, i = j = 1 d(i, j) + aT/L min k=1 c(i− 1, j − 1, k), l = 1 d(i, j) + c(i− 1, j, l − 1), l ≤ aTL Inf, otherwise ,
(1) where d(i, j) is the Euclidean distance between the i-th element of X and the j-th element of M. The minimum alignment cost can be found by such a dynamic programming and is achieved at the end of recursion.",3.1. Learning Abstract Templates,[0],[0]
"The corresponding optimal warping path is obtained by back tracking.
",3.1. Learning Abstract Templates,[0],[0]
Based on the dynamic alignment Eq.,3.1. Learning Abstract Templates,[0],[0]
"(1), M can be obtained by employing the DTW barycenter averaging (DBA) (Petitjean et al., 2011) as follows.",3.1. Learning Abstract Templates,[0],[0]
"We first use the uniform alignments to initialize M. Specially, in the n-th training sequence Xn, lnj = Tn L elements in Xn are aligned to the j-th element of M, ∀j = 1, · · · , L. The initial j-th element mj of M can be computed as:
mj = 1 N∑ n=1 lnj
N∑ n=1 enj∑",3.1. Learning Abstract Templates,[0],[0]
"k=snj xnk , (2)
where Pn = [pn1 , · · · ,pnL] is the alignment path that aligns Xn to M, pnj =",3.1. Learning Abstract Templates,[0],[0]
"[s n j , e n",3.1. Learning Abstract Templates,[0],[0]
"j ]
T records the start and end indexes of elements in Xn that are aligned to mj .",3.1. Learning Abstract Templates,[0],[0]
We then align each training sequence Xn to the initial M using Eq.,3.1. Learning Abstract Templates,[0],[0]
"(1) to update the alignment path Pn, for n = 1, · · · , N .",3.1. Learning Abstract Templates,[0],[0]
"We
Algorithm 1 Abstract template learning Input: {X1, · · · ,XN}; L; a; Output: M; Pn, n = 1, · · · , N ;
1: Initialize the uniform alignment path Pn for the training sequence Xn, for n = 1, · · · , N ; 2: Compute the initial abstract template M using Eq.",3.1. Learning Abstract Templates,[0],[0]
"(2); 3: while M has not converged do 4: Update the alignment paths Pn by aligning Xn to M, n = 1, · · · , N using Eq.",3.1. Learning Abstract Templates,[0],[0]
"(1); 5: Update the abstract template M with the alignment paths Pn, n = 1, · · · , N using Eq.",3.1. Learning Abstract Templates,[0],[0]
"(2); 6: end while
finally recompute the elements in M using Eq.",3.1. Learning Abstract Templates,[0],[0]
(2) with the updated Pn again.,3.1. Learning Abstract Templates,[0],[0]
This process can be repeated until the difference of M in the current iteration and M in the previous iteration is below a threshold or a maximum number of iterations is reached.,3.1. Learning Abstract Templates,[0],[0]
"We summarize the abstract template learning algorithm in Alg. 1.
",3.1. Learning Abstract Templates,[0],[0]
"Alg. 1 extends DBA to multi-dimensional sequences with a uniform initialization, and imposes stricter constraints on the warping path.",3.1. Learning Abstract Templates,[0],[0]
"As a result, any vector in any sequence can only be aligned to one element of M, which facilitates the invariant property of the separability in Sec. 3.2.
",3.1. Learning Abstract Templates,[0],[0]
Convergence.,3.1. Learning Abstract Templates,[0],[0]
Alg.,3.1. Learning Abstract Templates,[0],[0]
1,3.1. Learning Abstract Templates,[0],[0]
"actually minimizes the following objective function:
min Pn,n=1,··· ,N L∑ j=1 N∑ n=1 enj∑",3.1. Learning Abstract Templates,[0],[0]
k=snj ∥xnk −mj∥ 2 2.,3.1. Learning Abstract Templates,[0],[0]
"(3)
The value of the objective function in Eq.(3) decreases by both alternative procedures in Alg.1.",3.1. Learning Abstract Templates,[0],[0]
The objective function also has a lower bound 0.,3.1. Learning Abstract Templates,[0],[0]
Thus Alg.1 is guaranteed to converge to a local minimum.,3.1. Learning Abstract Templates,[0],[0]
We measure the separability among sequence classes based on their abstract templates in two aspects: the within-class scatter and the inter-class distance.,3.2. Separability Construction,[0],[0]
"We define the intraclass scatter of a sequence class as the sum of variances of all component temporal structures in the abstract template:
S = L∑
j=1
( N∑
n=1
lnj / N∑
n=1
Tn)Sj .",3.2. Separability Construction,[0],[0]
"(4)
lnj is the number of features in the n-th sequence aligned to the j-th temporal structure in the abstract template.",3.2. Separability Construction,[0],[0]
"Sj is the variance of the j-th temporal structure, which can be estimated as the variance matrix of all feature vectors in all training sequences aligned to the j-th element of M.
For sequence class i, we denote its intra-class scatter by Si.",3.2. Separability Construction,[0],[0]
"Assuming there are C sequence classes, we define the within-class scatter as the sum of intra-class scatters of all classes weighted by the prior probability pi of class i:
Sw = C∑ i=1",3.2. Separability Construction,[0],[0]
"piSi, (5)
where pi can be estimated as the number of sequences from class i divided by the number of sequences from all classes.
",3.2. Separability Construction,[0],[0]
The learned abstract template M of a sequence class represents the temporal structures and their general evolution of the class.,3.2. Separability Construction,[0],[0]
The separability between two sequence classes can be reflected by the difference between the two corresponding abstract templates.,3.2. Separability Construction,[0],[0]
"For two sequence classes i and j, we define the separability between them as:
Sb = ∑
1≤i<j≤C ∑ 1≤u,v≤L piup j v(m i u −mjv)(miu −mjv) T .
",3.2. Separability Construction,[0],[0]
"(6) miu and m j v denote the u-th element of M
i and the v-th element of Mj , respectively.",3.2. Separability Construction,[0],[0]
"piu and p j v denote the prior probabilities of miu and m j v , respectively.",3.2. Separability Construction,[0],[0]
p,3.2. Separability Construction,[0],[0]
"i u is estimated as the number of vectors in sequences from class i that are aligned to miu divided by the number of all vectors in all sequences from all classes.
",3.2. Separability Construction,[0],[0]
Constructing the inter-class scatter by Eq.,3.2. Separability Construction,[0],[0]
(6) is equivalent to viewing each temporal structure as a subclass.,3.2. Separability Construction,[0],[0]
"Since each sequence class is abstracted by several ordered temporal structures, if all temporal structures from all classes are maximally separated, the separability of different sequence classes increases accordingly.",3.2. Separability Construction,[0],[0]
"Thus Eq. (6) can indeed reflect the separability between sequence classes.
",3.2. Separability Construction,[0],[0]
"Note that both Sw (5) and Sb (6) rely on the alignments of sequence samples to the corresponding abstract templates: P = {Pin, n = 1, 2, · · · , N i, i = 1, 2, · · · , C}.",3.2. Separability Construction,[0],[0]
"We denote them by Sw(P) and Sb(P)1, respectively, to emphasize the dependencies on alignments.
",3.2. Separability Construction,[0],[0]
"Compared with HMM-based separability (Su & Ding, 2013; Su et al., 2018), our separability construction has several advantages.",3.2. Separability Construction,[0],[0]
(1).,3.2. Separability Construction,[0],[0]
It does not require a large amount of training data.,3.2. Separability Construction,[0],[0]
"Even when each class has only one sequence sample, Alg. 1 can still be performed and meaningful scatters can thereby be constructed.",3.2. Separability Construction,[0],[0]
"In this case, Alg. 1 degrades to the temporal clustering algorithm (Su et al., 2016).",3.2. Separability Construction,[0],[0]
(2).,3.2. Separability Construction,[0],[0]
"It does not need to estimate any parameter, thus has better scalability.",3.2. Separability Construction,[0],[0]
(3).,3.2. Separability Construction,[0],[0]
"Owing to the constraints on the warping path, calculating Sw by Eq. (5) is also equivalent to viewing all temporal structures in all classes as subclasses.",3.2. Separability Construction,[0],[0]
"Thus Sb(P) + Sw(P) = St, where St is the total s-
1Strictly speaking, they also depend on M, but M and P are closely associated, so we omit M for brevity.
catter of all features in all sequences and is independent of P. This invariant property ensures the joint optimization in Sec. 3.3.",3.2. Separability Construction,[0],[0]
"Our goal is to learn a linear transformation matrix W ∈ Rd×d′ to project feature vectors in sequences from the original d-dimensional space to the most discriminative d′dimensional subspace, in which the separability among different sequence classes are maximized.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"The separability depends on the alignments between the sequences and the abstract templates, which are inferred based on the pairwise distances between feature vectors in the space.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"When the features are projected to a subspace, the distances among the transformed features may change.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"The alignments may change accordingly, which should be re-calculated using Alg. 1 in the subspace.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
The updates of the alignments in turn affect the determination of the transformation.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"Existing methods (Su & Ding, 2013; Su et al., 2018) tackle such entanglement by fixing the alignments obtained in the original space, which may lead to sub-optimal solutions.
",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
We consider the joint learning of the transformation and the abstract templates together with the corresponding temporal alignments in the latent subspace simultaneously.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
We optimize the Fisher criterion that maximizes the inter-class separability and minimizes the within-class scatter.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"Due to the invariant property: Sb(P) + Sw(P) = St, the optimal projections of maximizing the ratio of Sb and Sw and maximizing the ration of Sb and St are the same (Fukunaga, 1990).",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"Therefore, we formulate our objective function as follows:
max W,P
tr((WTStW) −1WTSb(P)W).",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(7)
We solve Eq.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
(7) by alternatively updating W and P to obtain a local optimal solution.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"We call this method Latent Temporal Linear Discriminant Analysis or LT-LDA, which is summarized in Alg. 2.
",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"In the first stage, LT-LDA optimizes over P by fixing W. The first inverse matrix item (WTStW)−1 in Eq.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
(7) does not depend on P. We omit this item for the moment to derive an intuitive solution and will explain its effect later.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"The objective then becomes
max P
tr(WTSb(P)W).",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(8)
Since Sb(P) = St − Sw(P), Eq. (8) is equivalent to:
min P
tr(WTSw(P)W).",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(9)
Substituting Eq. (5) to Eq. (9) and expanding, Eq. (9) is
Algorithm 2 LT-LDA Input: the training sequences of each class c = 1, · · · , C, the length of the abstract template L, the control factor a; Output: the projection W;
1: Initialize the abstract template Mc and the associated alignments",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"Pc in the original space using Alg. 1, for c = 1, · · · , C; Calculate Sw (5) and Sb (6) according to Pc and Mc; 2: Initialize W by solving Eq.(11) 3: while W has not converged do 4: Project the training sequences into a subspace by W;
Update Mc and Pc in this subspace using Alg. 1, for c = 1, · · · , C;
5: Re-calculate Sw and Sb with the updated alignments P by Eq.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(5) and Eq. (6), respectively; 6: Update W by solving Eq. (11); 7: end while
transformed to:
C∑ i=1",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"pi min Pin,n=1,··· ,N L∑ j=1 Ni∑ n=1 einj∑ k=sinj ∥∥x̂ink",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"− m̂ij∥∥22, (10) where x̂ink = W Txink and m̂ i j are the projected feature and the element of the abstract template in the subspace, respectively.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
The superscript i is to indicate that the variable belongs to class i. To ensure the convergence and compensate the omitted item when deriving Eq.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(8) which will be more clear in Section 3.4, the features should first be centered before the start of the iterations, and a whitening preprocessing should be applied to all features in all sequences in this stage.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"That is, the mean of all xink is zero, and x̂ink =",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"WwW Txink , where Ww = Γ",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
− 12 w is whitening transformation and Γw is the total scatter of all projected features in all sequences.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"In our experiments, we found that the two procedures can be neglected, and the LT-LDA still converges while the computational complexity is reduced.
",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
Each of the C components of minimization is exactly the same with Eq.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
(3) in the subspace associated with W instead of the original space.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"These minimizations are independent from each other, and hence we can learn the abstract template and the corresponding alignments of training sequences for each of the C classes using Alg. 1 individually.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
The learned alignments for all the sequences in all the classes are used to update Sw and Sb using Eq.,3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"(5) and Eq. (6), respectively.
",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
"In the second stage, LT-LDA optimizes over W for given P. In this case, both Sw and Sb are fixed, and the objective function becomes a standard LDA problem:
max W
tr((WTStW) −1WTSbW)
⇔ max W tr((WTSwW) −1WTSbW).
(11)
The columns of the updated W are given by the eigenvectors of S−1w Sb with respect to the d ′ largest eigenvalues.",3.3. Joint Learning of the Transformation and the Latent Alignments,[0],[0]
We theoretically provide more insights by proving 1) that the abstract template learning algorithm (Alg. 1) can be linked to a trace maximization formulation; 2) that the LTLDA algorithm (Alg. 2) is guaranteed the converge; 3) that it is possible to simplify the joint optimization of Eq.,3.4. Theoretical Analysis,[0],[0]
(13) under certain conditions.,3.4. Theoretical Analysis,[0],[0]
"All proofs are given in the supplementary material.
",3.4. Theoretical Analysis,[0],[0]
Let Z be the matrix consisting of all frame-wise feature vectors in all training sequences.,3.4. Theoretical Analysis,[0],[0]
"Let T be the alignment indicator matrix, which is defined as follows: T = {πi,k}Nt×CL, where πi,k = 1 if the frame-wide feature vector zi in the i-th column of Z is in the sequence from the c = ⌈ (k − 12 )/L",3.4. Theoretical Analysis,[0],[0]
⌉ -th class and is aligned to the l = k,3.4. Theoretical Analysis,[0],[0]
"− (c − 1)L-th stage of class c, and πi,l = 0 otherwise.",3.4. Theoretical Analysis,[0],[0]
Nt is the total number of vectors in all the training sequences from all the samples.,3.4. Theoretical Analysis,[0],[0]
"Following (Dhillon et al., 2005; Ye et al., 2007), the weighted indicator matrix is defined as F = T(TTT)− 1 2 .
",3.4. Theoretical Analysis,[0],[0]
"It can be shown that
Fi,k =
{ 1/ √ nk, if zi ∈ (c, l)
0, otherwise ,
where nk is the number of 1 in the k-th column of F, i.e., the number of vectors that are aligned to stage l of class c. Lemma 1.",3.4. Theoretical Analysis,[0],[0]
"Objective function (3) is equivalent to the trace maximization problem
max F
tr(FTZTZF).",3.4. Theoretical Analysis,[0],[0]
"(12)
Lemma 2.",3.4. Theoretical Analysis,[0],[0]
"Objective function (7) is equivalent to the trace maximization problem
max W,F
tr(FTZTW(WTZZTW)−1WTZF).",3.4. Theoretical Analysis,[0],[0]
"(13)
Theorem 1.",3.4. Theoretical Analysis,[0],[0]
"The LT-LDA algorithm (Alg. 2) is guaranteed the converge.
",3.4. Theoretical Analysis,[0],[0]
"Similar to (Ye et al., 2007), in some specific cases, the joint optimization of Eq.",3.4. Theoretical Analysis,[0],[0]
(13) can be simplified by factoring out the projection matrix W. The result is summarized as follows: Theorem 2.,3.4. Theoretical Analysis,[0],[0]
Let G = ZTZ be the Gram matrix.,3.4. Theoretical Analysis,[0],[0]
"When the dimensionality is reduced to a specific value d′ = min(CL, d,Nt) and a regularization term δINt is added to the total scatter St, where INt is the Nt-order identity matrix, if W∗ and F∗ are the optimal solutions of the trace maximization problem (13):
max W,F
tr(FTZTW(WT (ZZT + δINt)W) −1WTZF)
(14)
then F∗ is also the optimal solution of the problem
max F
tr(FT (INt − (INt + 1
δ G)−1)F) (15)
",3.4. Theoretical Analysis,[0],[0]
"This theorem provides an upper bound of the objective for the stage of learning the partitions, which provides additional insights on the subspace selection of LT-LDA.",3.4. Theoretical Analysis,[0],[0]
"From Lemma 1, in the original space, the objective function (12) of the abstract template leaning actually maximizes tr(FTZTZF) = tr(FTGF).",3.4. Theoretical Analysis,[0],[0]
"While in the d′dimensional subspace, the objective (15) of LT-LDA actually maximizes a kernel version of Eq.",3.4. Theoretical Analysis,[0],[0]
"(12), where the kernel Gram matrix",3.4. Theoretical Analysis,[0],[0]
"Gk = INt − (INt + 1δG)
−1 is used instead of the original Gram matrix G in Eq.",3.4. Theoretical Analysis,[0],[0]
(12).,3.4. Theoretical Analysis,[0],[0]
Gk → G/δ,3.4. Theoretical Analysis,[0],[0]
"when δ → ∞, and hence objective (15) is equivalent to standard objective (12).",3.4. Theoretical Analysis,[0],[0]
"Gk → UrUTr when δ → 0, Ur is the set of the largest r principal components of all the features in all the sequences w.r.t.",3.4. Theoretical Analysis,[0],[0]
"the non-zero eigenvalues of G. Thus objective (15) is equivalent to learning the abstract templates in the subspace determined by PCA.
",3.4. Theoretical Analysis,[0],[0]
"Gk can be further expressed as
Gk = Udiag(λ1/(λ1 + δ), · · · , λNt/(λNt + δ))UT .
",3.4. Theoretical Analysis,[0],[0]
"This means that, the iterative procedures of LT-LDA essentially construct a kernel matrix for learning the latent alignments w.r.t.",3.4. Theoretical Analysis,[0],[0]
the abstract templates.,3.4. Theoretical Analysis,[0],[0]
"The construction is achieved by performing a transformation to G, such that each eigenvalue λ of G is transformed to λ/(λ + δ), while the eigenvectors of G remain unchanged.",3.4. Theoretical Analysis,[0],[0]
The subspace can be determined easily given the alignments without the need of the iterative procedures.,3.4. Theoretical Analysis,[0],[0]
"The nature of the subspace selection by LT-LDA indicates that it may be possible to accelerate the LT-LDA algorithm by fixing the partitions learned by optimizing (15) and hence getting rid of the time-consuming iterative procedures, without significant degradation in performance.",3.4. Theoretical Analysis,[0],[0]
The complexity of updating P using Alg.,3.5. Computational Complexity,[0],[0]
"1 for all the C classes is O(ICNLTd), I is the number of iterations in Alg.",3.5. Computational Complexity,[0],[0]
1..,3.5. Computational Complexity,[0],[0]
"The complexity of re-calculating Sw, Sb and W is O(CNTd2 + C2L2d2 + d3).",3.5. Computational Complexity,[0],[0]
Thus the overall complexity of Alg. 2 is O(I ′(ICNLTd+,3.5. Computational Complexity,[0],[0]
CNTd2 + C2L2d2 + d3)),3.5. Computational Complexity,[0],[0]
", I ′ is the number of iterations in Alg. 2.",3.5. Computational Complexity,[0],[0]
In this section we evaluate the proposed LT-LDA in comparison with several supervised DR methods for sequences on three real-world datasets.,4. Experimental Results,[0],[0]
Evaluations on another dataset are presented in the supplementary material.,4. Experimental Results,[0],[0]
Datasets.,4.1. Experimental Setup,[0],[0]
"ChaLearn Gesture dataset (Escalera et al., 2013b;a) contains Kinect videos from 20 Italian gestures.",4.1. Experimental Setup,[0],[0]
"The dataset has been split into training, validation and test sets.",4.1. Experimental Setup,[0],[0]
"MSR Sports Action3D dataset (Li et al., 2010) consists of depth sequences from 20 sports actions.",4.1. Experimental Setup,[0],[0]
"We follow the same experimental setup as in (Wang et al., 2012; Wang & Wu, 2013) to split the dataset into training and test set.",4.1. Experimental Setup,[0],[0]
"Olympic Sports dataset (Niebles et al., 2010) consists of 783 video sequences from 16 actions.",4.1. Experimental Setup,[0],[0]
"The dataset has been split into training and test sets, where 649 videos are used for training and 134 videos are used for testing.
",4.1. Experimental Setup,[0],[0]
Frame-wise features.,4.1. Experimental Setup,[0],[0]
"We extract a feature vector from each frame, and hence every action video is represented by a sequence of frame-wise features.",4.1. Experimental Setup,[0],[0]
"For the Chalearn dataset, we employ the frame-wise features provided by the authors of (Fernando et al., 2015), which are body-jointsbased features with a dimensionality of 100.",4.1. Experimental Setup,[0],[0]
"For the MSR Action3D dataset, we employ the frame-wise features provided by the authors of (Wang & Wu, 2013), which are the relative positions of all the 3D joints with a dimensionality of 192.",4.1. Experimental Setup,[0],[0]
"For the Olympic Sports dataset, we employ the improved dense trajectories (Wang & Schmid, 2013) based frame-wise features.",4.1. Experimental Setup,[0],[0]
"MBH descriptors are extracted at densely sampled points from each frame and then encoded by Bag-of-Words with a codebook of 4, 000 visual words.",4.1. Experimental Setup,[0],[0]
"The frame-wise feature is the histogram of the quantized descriptors with a dimensionality of 4, 000.
",4.1. Experimental Setup,[0],[0]
Classification and evaluation measures.,4.1. Experimental Setup,[0],[0]
"We adopt three classifiers in the learned subspace, including the HMM classifier, the DTW classifier, and the SVM classifier.",4.1. Experimental Setup,[0],[0]
"For the HMM classifier, a left-to-right HMM with 4 states and self-loops is trained for each sequence class, and a test sequence is classified to the class whose HMM has the highest probability to generate it.",4.1. Experimental Setup,[0],[0]
"For the DTW classifier, the training sequence that has the smallest sum of DTW distances with all other sequences from the same class is selected as the template of this class.",4.1. Experimental Setup,[0],[0]
A test sequence is classified to the class whose temple has the smallest DTW distance to it.,4.1. Experimental Setup,[0],[0]
"The two classifiers directly take sequences as input, and we use the accuracy as the performance measure.",4.1. Experimental Setup,[0],[0]
"For the SVM classifier, we encode each sequence into a vector by rank pooling (Fernando et al., 2015).",4.1. Experimental Setup,[0],[0]
"Linear SVMs are trained on these encoded vectors, and the parameter C of SVM is selected by cross-validation.",4.1. Experimental Setup,[0],[0]
We use the accuracy and the Mean Average Precision (MAP) as the evaluation measures for the SVM classifier.,4.1. Experimental Setup,[0],[0]
The proposed LT-LDA has two preset parameters: the length of each abstract template L and the factor a controlling the allowed degree of warping.,4.2. Influence of Parameters,[0],[0]
"In this section we eval-
uate the influence of them on the MSR Action3D dataset.",4.2. Influence of Parameters,[0],[0]
Evaluations on other datasets are presented in the supplementary material.,4.2. Influence of Parameters,[0],[0]
"Different performance measures including accuracy, MAP, precision and recall with the SVM classifier are evaluated by increasing L from 3 to 21 with an interval of 3 while fixing a to 2, and increasing a from 1 to 5 with an interval of 0.5 while fixing L to 8, respectively.",4.2. Influence of Parameters,[0],[0]
The reduced dimension is fixed to 20.,4.2. Influence of Parameters,[0],[0]
The results are shown in Fig. 1.,4.2. Influence of Parameters,[0],[0]
"The optimal parameters are generally the same for multi-class indicators including accuracy, precision, recall and F-score, but are different for MAP.
",4.2. Influence of Parameters,[0],[0]
LT-LDA achieves the highest multi-class performances when L = 9 on this dataset.,4.2. Influence of Parameters,[0],[0]
"The larger the L, the longer the template, the finer the captured temporal structures, but the less accurate the estimated statistics of structures, and the more likely to cause overfitting.",4.2. Influence of Parameters,[0],[0]
"Therefore, the performances decrease if the length L is too long or too short.",4.2. Influence of Parameters,[0],[0]
"Generally, setting L within the range of 6 to 9 leads to satisfactory results.",4.2. Influence of Parameters,[0],[0]
A too large a easily leads to unbalanced alignments.,4.2. Influence of Parameters,[0],[0]
"If a is too small, the flexibility of alignments may be restricted.",4.2. Influence of Parameters,[0],[0]
Allowing appropriate warping leads to satisfactory results.,4.2. Influence of Parameters,[0],[0]
"We fix a to 2 in the following experiments, and fix L to 8 except on the Olympic Sports dataset, where we set L to 20 such that LT-LDA can preserve 20× C − 1 = 319 dimensions at most.",4.2. Influence of Parameters,[0],[0]
"In LT-LDA, the latent alignments are jointly learned with the underlying subspace.",4.3. Effects of the Joint Learning,[0],[0]
If instead we use the alignments in the original space calculated by Alg. 1,4.3. Effects of the Joint Learning,[0],[0]
"directly, LT-LDA degenerates to the initialization of W in LT-LDA.",4.3. Effects of the Joint Learning,[0],[0]
We denote this algorithm by ini-LT-LDA and compare it with LT-LDA on the large-scale Olympic Sports dataset.,4.3. Effects of the Joint Learning,[0],[0]
The comparisons by using different classifiers and evaluation measures are shown in Fig. 2.,4.3. Effects of the Joint Learning,[0],[0]
We can observe that LTLDA significantly outperforms ini-LT-LDA by a large margin.,4.3. Effects of the Joint Learning,[0],[0]
Learning the latent alignments associated with the subspace jointly does help to improve the classification performance in the subspace.,4.3. Effects of the Joint Learning,[0],[0]
This is because the temporal structures and the alignments may change from those in the original space.,4.3. Effects of the Joint Learning,[0],[0]
"In the learned subspace of ini-LT-LDA, al-
though different classes get better separated under the alignments in the original space, additional confusions may be introduced due to the changes of alignments.",4.3. Effects of the Joint Learning,[0],[0]
"While for LTLDA, since the separability is maximized in the subspace under the corresponding alignments, the learned subspace gets joint optimality among all possible subspaces.",4.3. Effects of the Joint Learning,[0],[0]
More complete evaluations and analysis on more datasets are presented in the supplementary material.,4.3. Effects of the Joint Learning,[0],[0]
"We compare the proposed LT-LDA with LDA and kernel LDA (kLDA) by viewing the features in sequences as independent samples, as well as LSDA.",4.4. Comparison with Different DR Methods,[0],[0]
The performances of the original feature sequences are also presented as baselines.,4.4. Comparison with Different DR Methods,[0],[0]
"We use the drtoolbox (van der Maaten & Hinton, 2008) to perform LDA and kLDA.",4.4. Comparison with Different DR Methods,[0],[0]
"For kLDA, it is impracticable to use all features in all training sequences, because this will lead to a huge size of the kernel matrix and very large space and computational overhead.",4.4. Comparison with Different DR Methods,[0],[0]
"Following (Su et al., 2018), we sample 1 to 5 features randomly from each sequence for training.",4.4. Comparison with Different DR Methods,[0],[0]
"We use the same parameters of LSDA as in (Su & Ding, 2013).
",4.4. Comparison with Different DR Methods,[0],[0]
"Fig. 3 and Fig. 4 depict the performances as functions of the dimensionality of the learned subspace on the ChaLearn dataset and the Action3D dataset, respectively.",4.4. Comparison with Different DR Methods,[0],[0]
We can observe that the proposed LT-LDA achieves the best performances among all these DR methods by all the three classifiers with different evaluation measures on both datasets.,4.4. Comparison with Different DR Methods,[0],[0]
"Especially by the DTW classier, LT-LDA outperforms the second LSDA by a margin of more than 10%.",4.4. Comparison with Different DR Methods,[0],[0]
"By the HMM classifier and the DTW classifier, the accuracies of LTLDA are consistently better on all the reduced dimensions, and LT-LDA with less than 15 dimensions achieves much better results than the original features with hundreds of dimensions.",4.4. Comparison with Different DR Methods,[0],[0]
"For the SVM classifier with the rank pooling, LT-LDA achieves comparable MAPs with original features using only 15 or 25 dimensions.",4.4. Comparison with Different DR Methods,[0],[0]
"The worse performances of LDA and kLDA are caused by the dependency of features in sequences, which violates the basic assumption of
the two methods, while LT-LDA well exploits such temporal dependencies by learning the latent alignments.
",4.4. Comparison with Different DR Methods,[0],[0]
"For the Olympic Sports dataset, there are only less than 35 training videos per class.",4.4. Comparison with Different DR Methods,[0],[0]
"Each video generally has hundreds of frames, and the dimensionality of the feature for each frame is 4, 000.",4.4. Comparison with Different DR Methods,[0],[0]
"Therefore, it is impracticable to train a HMM for each class and hence LSDA cannot be employed.",4.4. Comparison with Different DR Methods,[0],[0]
kLDA is also computational prohibited.,4.4. Comparison with Different DR Methods,[0],[0]
"We compare LTLDA with PCA and LDA on this dataset, as shown in Fig. 5.",4.4. Comparison with Different DR Methods,[0],[0]
LDA can only preserve C−1 = 19 dimensions at most.,4.4. Comparison with Different DR Methods,[0],[0]
"LTLDA consistently outperforms PCA, and further improves the performances when more than 19 dimensions are preserved.",4.4. Comparison with Different DR Methods,[0],[0]
"With only 250 dimensions, LT-LDA achieves comparable accuracy and MAP with the original BoW-based distributed features with 4, 000 dimensions.",4.4. Comparison with Different DR Methods,[0],[0]
"This implies that the BoW features can be greatly compressed by LTLDA while the discriminative information is maintained.
To compare with the state-of-the-art gesture recognition methods, we also evaluate the multi-class precision, recall, and F-score by LT-LDA with fine-tuned a via the rank pooling and the SVM classifier on the ChaLearn dataset.",4.4. Comparison with Different DR Methods,[0],[0]
The comparisons are shown in Tab. 1.,4.4. Comparison with Different DR Methods,[0],[0]
LT-LDA outperforms the state-of-the-art results using only 45 dimensions.,4.4. Comparison with Different DR Methods,[0],[0]
"In this paper, we have presented a DR method for sequence data, called LT-LDA, which learns the subspace and infers
the latent alignments within it simultaneously.",5. Conclusion,[0],[0]
"We formulate the learning of the subspace, the latent alignments, and the temporal structures into a joint objective function, and solve it by iteratively repeating the two alternative procedures of applying LDA and learning the abstract templates.",5. Conclusion,[0],[0]
The effectiveness of the proposed method is demonstrated on three action datasets with various evaluation measures and classifiers.,5. Conclusion,[0],[0]
"This work was supported in part by the National Natural Science Foundation of China under Grant No.61603373, the National Science Foundation grant IIS-1217302, IIS1619078, and the Army Research Office ARO W911NF16-1-0138.",Acknowledgements,[0],[0]
"Low-dimensional discriminative representations enhance machine learning methods in both performance and complexity, motivating supervised dimensionality reduction (DR) that transforms high-dimensional data to a discriminative subspace.",abstractText,[0],[0]
"Most DR methods require data to be i.i.d., however, in some domains, data naturally come in sequences, where the observations are temporally correlated.",abstractText,[0],[0]
We propose a DR method called LT-LDA to learn low-dimensional temporal representations.,abstractText,[0],[0]
"We construct the separability among sequence classes by lifting the holistic temporal structures, which are established based on temporal alignments and may change in different subspaces.",abstractText,[0],[0]
"We jointly learn the subspace and the associated alignments by optimizing an objective which favors easily-separable temporal structures, and show that this objective is connected to the inference of alignments, thus allows an iterative solution.",abstractText,[0],[0]
We provide both theoretical insight and empirical evaluation on real-world sequence datasets to show the interest of our method.,abstractText,[0],[0]
Learning Low-Dimensional Temporal Representations,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 420–425 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
420",text,[0],[0]
"Recently, more and more attention from both academia and industry is paying to building nontask-oriented chatbots that can naturally converse with humans on any open domain topics.",1 Introduction,[0],[0]
"Existing approaches can be categorized into generationbased methods (Shang et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Sordoni et al., 2015; Xing et al., 2017; Serban et al., 2017; Xing et al., 2018) which synthesize a response with natural language generation techniques, and retrievalbased methods (Hu et al., 2014; Lowe et al., 2015; Yan et al., 2016; Zhou et al., 2016; Wu et al., 2017) which select a response from a pre-built index.",1 Introduction,[0],[0]
"In this work, we study response selection for retrieval-based chatbots, not only because retrieval-based methods can return fluent and informative responses, but also because they have been successfully applied to many real products such as the social-bot XiaoIce from Microsoft (Shum et al., 2018) and the E-commerce assistant AliMe Assist from Alibaba Group (Li et al., 2017).
∗",1 Introduction,[0],[0]
"Corresponding Author
A key step to response selection is measuring the matching degree between a response candidate and an input which is either a single message (Hu et al., 2014) or a conversational context consisting of multiple utterances (Wu et al., 2017).",1 Introduction,[0],[0]
"While existing research focuses on how to define a matching model with neural networks, little attention has been paid to how to learn such a model when few labeled data are available.",1 Introduction,[0],[0]
"In practice, because human labeling is expensive and exhausting, one cannot have large scale labeled data for model training.",1 Introduction,[0],[0]
"Thus, a common practice is to transform the matching problem to a classification problem with human responses as positive examples and randomly sampled ones as negative examples.",1 Introduction,[0],[0]
"This strategy, however, oversimplifies the learning problem, as most of the randomly sampled responses are either far from the semantics of the messages or the contexts, or they are false negatives which pollute the training data as noise.",1 Introduction,[0],[0]
"As a result, there often exists a significant gap between the performance of a model in training and the same model in practice (Wang et al., 2015; Wu et al., 2017).1
We propose a new method that can effectively leverage unlabeled data for learning matching models.",1 Introduction,[0],[0]
"To simulate the real scenario of a retrieval-based chatbot, we construct an unlabeled data set by retrieving response candidates from an index.",1 Introduction,[0],[0]
"Then, we employ a weak annotator to provide matching signals for the unlabeled inputresponse pairs, and leverage the signals to supervise the learning of matching models.",1 Introduction,[0],[0]
"The weak annotator is pre-trained from large scale humanhuman conversations without any annotations, and thus a Seq2Seq model becomes a natural choice.",1 Introduction,[0],[0]
"Our approach is compatible with any matching models, and falls in a teacher-student framework
1The model performs well on randomly sampled data, but badly on human labeled data.
",1 Introduction,[0],[0]
"(Hinton et al., 2015) where the Seq2Seq model transfers the knowledge from human-human conversations to the learning process of the matching models.",1 Introduction,[0],[0]
"Broadly speaking, both of (Hinton et al., 2015) and our work let a neural network supervise the learning of another network.",1 Introduction,[0],[0]
An advantage of our method is that it turns the hard zero-one labels in the existing learning paradigm to soft (weak) matching scores.,1 Introduction,[0],[0]
"Hence, the model can learn a large margin between a true response with a true negative example, and the semantic distance between a true response and a false negative example is short.",1 Introduction,[0],[0]
"Furthermore, due to the simulation of real scenario, harder examples can been seen in the training phase that makes the model more robust in the testing.
",1 Introduction,[0],[0]
"We conduct experiments on two public data sets, and experimental results on both data sets indicate that models learned with our method can significantly outperform their counterparts learned with the random sampling strategy.
",1 Introduction,[0],[0]
Our contributions include: (1) proposal of a new method that can leverage unlabeled data to learn matching models for retrieval-based chatbots; and (2) empirical verification of the effectiveness of the method on public data sets.,1 Introduction,[0],[0]
"Given a data setD = {xi, (yi,1, . . .",2.1 The Existing Learning Approach,[0],[0]
", yi,n)}Ni=1 with xi a message or a conversational context and yi,j a response candidate of xi, we aim to learn a matching modelM(·, ·) from D. Thus, for any new pair (x, y),M(x, y) measures the matching degree between x and y.
To obtain a matching model, one has to deal with two problems: (1) how to defineM(·, ·); and (2) how to perform learning.",2.1 The Existing Learning Approach,[0],[0]
"Existing work focuses on Problem (1) where state-of-the-art methods include dual LSTM (Lowe et al., 2015), MultiView LSTM (Zhou et al., 2016), CNN (Yan et al., 2016), and Sequential Matching Network (Wu et al., 2017), but adopts a simple strategy for Problem (2): ∀xi, a human response is designated as yi,1 with a label 1, and some randomly sampled responses are treated as (yi,2, . . .",2.1 The Existing Learning Approach,[0],[0]
", yi,n) with labels 0.",2.1 The Existing Learning Approach,[0],[0]
"M(·, ·) is then learned by maximizing the following objective: ∑N i=1",2.1 The Existing Learning Approach,[0],[0]
∑n j=1,2.1 The Existing Learning Approach,[0],[0]
"[ri,j log(M(xi, yi,j)) + (1− ri,j) log(1−M(xi, yi,j))] ,
(1)
where ri,j ∈ {0, 1} is a label.",2.1 The Existing Learning Approach,[0],[0]
"While matching accuracy can be improved by carefully designing M(·, ·) (Wu et al., 2017), the bottleneck becomes the learning approach which suffers obvious problems: most of the randomly sampled yi,j are semantically far from xi which may cause an undesired decision boundary at the end of optimization; some yi,j are false negatives.",2.1 The Existing Learning Approach,[0],[0]
"As hard zero-one labels are adopted in Equation (1), these false negatives may mislead the learning algorithm.",2.1 The Existing Learning Approach,[0],[0]
"The problems remind us that besides good architectures of matching models, we also need a good approach to learn such models from data.",2.1 The Existing Learning Approach,[0],[0]
"As human labeling is infeasible when training complicated neural networks, we propose a new method that can leverage unlabeled data to learn a matching model.",2.2 A New Learning Method,[0],[0]
"Specifically, instead of random sampling, we construct D by retrieving (yi,2, . . .",2.2 A New Learning Method,[0],[0]
", yi,n) from an index (yi,1 is the human response of xi).",2.2 A New Learning Method,[0],[0]
"By this means, some yi,j are true positives, and some are negatives but semantically close to xi.",2.2 A New Learning Method,[0],[0]
"After that, we employ a weak annotatorG(·, ·) to indicate the matching degree of every (xi, yi,j) in D as weak supervision signals.",2.2 A New Learning Method,[0],[0]
"Let sij = G(xi, yi,j), then the learning approach can be formulated as:
argmin M(·,·) N∑ i=1",2.2 A New Learning Method,[0],[0]
"n∑ j=1 max(0,M(xi, yi,j)−M(xi, yi,1)+ s′i,j),
(2)
where s′ij is a normalized weak signal defined as max(0,
si,j si,1 − 1).",2.2 A New Learning Method,[0],[0]
"The normalization here elimi-
nates bias from different xi.",2.2 A New Learning Method,[0],[0]
"Objective (2) encourages a large margin between the matching of an input and its human response and the matching of the input and a negative response judged by G(·, ·) (as will be seen later, si,jsi,1 > 1).",2.2 A New Learning Method,[0],[0]
"The learning approach simulates how we build a matching model in a retrievalbased chatbot: given {xi}, some response candidates are first retrieved from an index.",2.2 A New Learning Method,[0],[0]
Then human annotators are hired to judge the matching degree of each pair.,2.2 A New Learning Method,[0],[0]
"Finally, both the data and the human labels are fed to an optimization program for model training.",2.2 A New Learning Method,[0],[0]
"Here, we replace the expensive human labels with cheap judgment from G(·, ·).
",2.2 A New Learning Method,[0],[0]
"We define G(·, ·) as a sequence-to-sequence architecture (Vinyals and Le, 2015) with an attention mechanism (Bahdanau et al., 2015), and pre-train it with large amounts of human-human conversa-
tion data.",2.2 A New Learning Method,[0],[0]
"The Seq2Seq model can capture the semantic correspondence between an input and a response, and then transfer the knowledge to the learning of a matching model in the optimization of (2).",2.2 A New Learning Method,[0],[0]
"sij is then defined as the likelihood of generating yi,j from xi:
sij = ∑ k log[p(wyi,j ,k, |xi, wyi,j ,l<k)], (3)
where wyi,j ,k is the k-th word of yi,j and wyi,j ,",2.2 A New Learning Method,[0],[0]
"l<k is the word sequence before wyi,j ,k.
Since negative examples are retrieved by a search engine, the oversimplification problem of the negative sampling approach can be partially mitigated.",2.2 A New Learning Method,[0],[0]
We leverage a weak annotator to assign a score for each example to distinguish false negative examples and true negative examples.,2.2 A New Learning Method,[0],[0]
"Equation (2) turns the hard zero-one labels in Equation (1) to soft matching degrees, and thus our method encourages the model to be more confident to classify a response with a high si,j score as a negative one.",2.2 A New Learning Method,[0],[0]
"In this way, we can avoid false negative examples and true negative examples are treated equally during training, and update the model toward a correct direction.
",2.2 A New Learning Method,[0],[0]
"It is noteworthy that although our approach also involves an interaction between a generator and a discriminator, it is different from the GANs (Goodfellow et al., 2014) in principle.",2.2 A New Learning Method,[0],[0]
"GANs try to learn a better generator via an adversarial process, while our approach aims to improve the discriminator with supervision from the generator, which also differentiates it from the recent work on transferring knowledge from a discriminator to a generative visual dialog model (Lu et al., 2017).",2.2 A New Learning Method,[0],[0]
"Our approach is also different from those semi-supervised approaches in the teacher-student framework (Dehghani et al., 2017a,b), as there are no labeled data in learning.",2.2 A New Learning Method,[0],[0]
"We conduct experiments on two public data sets: STC data set (Wang et al., 2013) for single-turn response selection and Douban Conversation Corpus (Wu et al., 2017) for multi-turn response selection.",3 Experiment,[0],[0]
"Note that we do not test the proposed approach on Ubuntu Corpus (Lowe et al., 2015), because both training and test data in the corpus are constructed by random sampling.",3 Experiment,[0],[0]
We implement our approach with TensorFlow.,3.1 Implementation Details,[0],[0]
"In both experiments, the same Seq2Seq model is exploited which is trained with 3.3 million inputresponse pairs extracted from the training set of the Douban data.",3.1 Implementation Details,[0],[0]
"Each input is a concatenation of consecutive utterances in a context, and the response is the next turn ({u<i}, ui).",3.1 Implementation Details,[0],[0]
"We set the vocabulary size as 30, 000, the hidden vector size as 1024, and the embedding size as 620.",3.1 Implementation Details,[0],[0]
"Optimization is conducted with stochastic gradient descent (Bottou, 2010), and is terminated when perplexity on a validation set (170k pairs) does not decrease in 3 consecutive epochs.",3.1 Implementation Details,[0],[0]
"In optimization of Objective (2), we initialize M(·, ·) with a model trained under Objective (1) with the (random) negative sampling strategy, and fix word embeddings throughout training.",3.1 Implementation Details,[0],[0]
This can stabilize the learning process.,3.1 Implementation Details,[0],[0]
The learning rate is fixed as 0.1.,3.1 Implementation Details,[0],[0]
"Experiment settings: in the STC (stands for Short Text Conversation) data set, the task is to select a proper response for a post in Weibo2.",3.2 Single-turn Response Selection,[0],[0]
The training set contains 4.8 million post-response (true response) pairs.,3.2 Single-turn Response Selection,[0],[0]
The test set consists of 422 posts with each one associated with around 30 responses labeled by human annotators in “good” and “bad”.,3.2 Single-turn Response Selection,[0],[0]
"In total, there are 12, 402 labeled pairs in the test data.",3.2 Single-turn Response Selection,[0],[0]
"Following (Wang et al., 2013, 2015), we combine the score from a matching model with TF-IDF based cosine similarity using RankSVM",3.2 Single-turn Response Selection,[0],[0]
whose parameters are chosen by 5-fold cross validation.,3.2 Single-turn Response Selection,[0],[0]
Precision at position 1 (P@1) is employed as an evaluation metric.,3.2 Single-turn Response Selection,[0],[0]
"In addition to the models compared on the data in the existing literatures, we also implement dual LSTM (Lowe et al., 2015) as a baseline.",3.2 Single-turn Response Selection,[0],[0]
"As case studies, we learn a dual LSTM and an CNN (Hu et al., 2014) with the proposed approach, and denote them as LSTM+WS (Weak Supervision) and CNN+WS, respectively.",3.2 Single-turn Response Selection,[0],[0]
"When constructing D, we build an index with the training data using Lucene3 and retrieve 9 candidates (i.e., {yi,2, . .",3.2 Single-turn Response Selection,[0],[0]
.,3.2 Single-turn Response Selection,[0],[0]
", yi,n}) for each post with the inline algorithm of the index.",3.2 Single-turn Response Selection,[0],[0]
"We form a validation set by randomly sampling 10 thousand posts associated with the responses from D (human response is positive and others are treated as negative).
",3.2 Single-turn Response Selection,[0],[0]
Results: Table 1 reports the results.,3.2 Single-turn Response Selection,[0],[0]
"We can see 2http://weibo.sina.com 3https://lucenenet.apache.org/
that CNN and LSTM consistently get improved when learned with the proposed approach, and the improvements over the models learned with random sampling are statistically significant (ttest with p-value < 0.01).",3.2 Single-turn Response Selection,[0],[0]
"LSTM+WS even surpasses the best performing model, DeepMatchtree, reported on this data.",3.2 Single-turn Response Selection,[0],[0]
These results indicate the usefulness of the proposed approach in practice.,3.2 Single-turn Response Selection,[0],[0]
One can expect improvements to models like DeepMatchtree with the new learning method.,3.2 Single-turn Response Selection,[0],[0]
We leave the verification as future work.,3.2 Single-turn Response Selection,[0],[0]
Experiment settings: Douban Conversation Corpus contains 0.5 million context-response (true response) pairs for training and 1000 contexts for test.,3.3 Multi-turn Response Selection,[0],[0]
"In the test set, every context has 10 response candidates, and each of the response has a label “good” or “bad” judged by human annotators.",3.3 Multi-turn Response Selection,[0],[0]
"Mean average precision (MAP) (BaezaYates et al., 1999), mean reciprocal rank (MRR) (Voorhees, 1999), and precision at position 1 (P@1) are employed as evaluation metrics.",3.3 Multi-turn Response Selection,[0],[0]
"We copy the numbers reported in (Wu et al., 2017) for the baseline models, and learn LSTM, Multi-View, and SMN with the proposed approach.",3.3 Multi-turn Response Selection,[0],[0]
"We build an index with the training data, and retrieve 9 candidates with the method in (Wu et al., 2017) for each context when constructing D. 10 thousand pairs are sampled from D as a validation set.
",3.3 Multi-turn Response Selection,[0],[0]
Results: Table 2 reports the results.,3.3 Multi-turn Response Selection,[0],[0]
"Consistent with the results on the STC data, every model (+WS one) gets improved with the new learning approach, and the improvements are statistically significant (t-test with p-value < 0.01).",3.3 Multi-turn Response Selection,[0],[0]
"Ablation studies: we first replace the weak supervision s′i,j in Equation (2) with a constant selected from {0.1, 0.2, . . .",3.4 Discussion,[0],[0]
", 0.9} on validation, and denote the models as model+const.",3.4 Discussion,[0],[0]
"Then, we keep
everything the same as our approach but replace D with a set constructed by random sampling, denoted as model+WSrand.",3.4 Discussion,[0],[0]
Table 3 reports the results.,3.4 Discussion,[0],[0]
We can conclude that both the weak supervision and the strategy of training data construction are important to the success of the proposed learning approach.,3.4 Discussion,[0],[0]
"Training data construction plays a more crucial role, because it involves more true positives and negatives with different semantic distances to the positives into learning.
",3.4 Discussion,[0],[0]
Does updating the Seq2Seq model help?,3.4 Discussion,[0],[0]
"It is well known that Seq2Seq models suffer from the “safe response” (Li et al., 2016a) problem, which may bias the weak supervision signals to high-frequency responses.",3.4 Discussion,[0],[0]
"Therefore, we attempt to iteratively optimize the Seq2Seq model and the matching model and check if the matching model can be further improved.",3.4 Discussion,[0],[0]
"Specifically, we update the Seq2Seq model every 20 mini-batches with the policy-based reinforcement learning approach proposed in (Li et al., 2016b).",3.4 Discussion,[0],[0]
The reward is defined as the matching score of a context and a response given by the matching model.,3.4 Discussion,[0],[0]
"Unfortunately, we do not observe significant improvement on the matching model.",3.4 Discussion,[0],[0]
"The result is attributed to two factors: (1) it is difficult to significantly im-
prove the Seq2Seq model with a policy gradient based method; and (2) eliminating “safe response” for Seq2Seq model cannot help a matching model to learn a better decision boundary.
",3.4 Discussion,[0],[0]
"How the number of response candidates affects learning: we vary the number of {yi,j}nj=1 in D in {2, 5, 10, 20} and study how the hyperparameter influences learning.",3.4 Discussion,[0],[0]
We study with LSTM on the STC data and SMN on the Douban data.,3.4 Discussion,[0],[0]
Table 4 reports the results.,3.4 Discussion,[0],[0]
"We can see that as the number of candidates increases, the performance of the the learned models becomes better.",3.4 Discussion,[0],[0]
"Even with 2 candidates (one from human and the other from retrieval), our approach can still improve the peformance of matching models.",3.4 Discussion,[0],[0]
"Previous studies focus on architecture design for retrieval-based chatbots, but neglect the problems brought by random negative sampling in the learning process.",4 Conclusion and Future Work,[0],[0]
"In this paper, we propose leveraging a Seq2Seq model as a weak annotator on unlabeled data to learn a matching model for response selection.",4 Conclusion and Future Work,[0],[0]
"By this means, we can mine hard instances for matching model and give them scores with a weak annotator.",4 Conclusion and Future Work,[0],[0]
Experimental results on public data sets verify the effectiveness of the new learning approach.,4 Conclusion and Future Work,[0],[0]
"In the future, we will investigate how to remove bias from the weak supervisors, and further improve the matching model performance with a semi-supervised approach.",4 Conclusion and Future Work,[0],[0]
Yu Wu is supported by Microsoft Fellowship Scholarship and AdeptMind Scholarship.,Acknowledgment,[0],[0]
"This work is supported by the National Natural Science Foundation of China (Grand Nos. 61672081,U1636211,61370126), Beijing Advanced Innovation Center for Imaging Technology (No.BAICIT-2016001).",Acknowledgment,[0],[0]
We propose a method that can leverage unlabeled data to learn a matching model for response selection in retrieval-based chatbots.,abstractText,[0],[0]
"The method employs a sequence-tosequence architecture (Seq2Seq) model as a weak annotator to judge the matching degree of unlabeled pairs, and then performs learning with both the weak signals and the unlabeled data.",abstractText,[0],[0]
Experimental results on two public data sets indicate that matching models get significant improvements when they are learned with the proposed method.,abstractText,[0],[0]
Learning Matching Models with Weak Supervision for Response Selection in Retrieval-based Chatbots,title,[0],[0]
"Structured prediction can be thought of as a generalization of binary classification to structured outputs, where the goal is to jointly predict several dependent variables.",1. Introduction,[0],[0]
"Predicting complex, structured data is of great significance in various application domains including computer vision (e.g., image segmentation, multiple object tracking), natural language processing (e.g., part-of-speech tagging, named entity recognition) and computational biology (e.g. protein structure prediction).",1. Introduction,[0],[0]
"However, unlike binary classification, structured prediction presents a set of unique computational and statistical challenges.",1. Introduction,[0],[0]
The chief being that the number of structured outputs is exponential in the input size.,1. Introduction,[0],[0]
"For instance, in translation tasks, the number of parse trees of a sentence is exponential in the length of the sentence.",1. Introduction,[0],[0]
"Second, it is very common in such domains to have very few training examples as compared to the size of the output space thereby making generalization to unseen inputs difficult.
1Department of Computer Science, Purdue University, West Lafayette, IN - 47906.",1. Introduction,[0],[0]
"Correspondence to: Asish Ghoshal <aghoshal@purdue.edu>, Jean Honorio <jhonorio@purdue.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"The key computational challenge in structured prediction stems from the inference problem, where a decoder, parameterized by a vector w of weights, predicts (or decodes) the latent structured output y given an observed input x. With the exception of a few special cases, the general inference problem in structured prediction is intractable.",1. Introduction,[0],[0]
"For instance in many cases the inference problem reduces to the maximum acyclic subgraph problem which is NP-hard and hard to approximate to within a factor of 1/2 of the optimal solution (Guruswami et al., 2008), or cardinality-constrained submodular maximization, which is also NP-hard and hard to compute a solution better than the (1 1/"")-approximate solution returned by a greedy algorithm (Nemhauser et al., 1978).",1. Introduction,[0],[0]
"The learning problem, where the goal is to learn the parameter w of the decoder from a set of labeled training instances, and which involves solving the inference problem as a subroutine, is therefore intractable for all but a few special cases.",1. Introduction,[0],[0]
"Hardness of max-margin learning (SVM) was shown by (Sontag et al., 2010).
",1. Introduction,[0],[0]
"Hardness results notwithstanding, various methods — which are worst-case exponential-time — have been developed over the last decade for predicting structured data including conditional random fields (Lafferty et al., 2001), and maxmargin approaches (Taskar et al., 2003), to name a few.",1. Introduction,[0],[0]
"In these approaches, learning the parameter w of the decoder involves minimizing a loss function L(w, S) over a data set S of m training pairs {(x
i , y i )}m i=1
.",1. Introduction,[0],[0]
"One could also take a Bayesian approach and learn a posterior distribution Q over decoder parameters w by minimizing the Gibbs loss E w⇠Q [L(w, S)].",1. Introduction,[0],[0]
"McAllester (McAllester, 2007) showed, using the PAC-Bayesian framework, that the commonly used max-margin loss (Taskar et al., 2003) upper bounds the expected Gibbs loss over the data distribution, upto statistical error.",1. Introduction,[0],[0]
"Therefore, minimizing the max-margin loss provides a principled way for learning the parameters of a structured decoder.",1. Introduction,[0],[0]
"More recently, (Honorio & Jaakkola, 2016) showed that minimizing a surrogate randomized loss, where the max-margin loss is computed over a small number of randomly sampled structured outputs, also bounds the Gibbs loss from above upto statistical error.
",1. Introduction,[0],[0]
The above can be thought of as weight based perturbation models.,1. Introduction,[0],[0]
"The perturb-and-MAP framework introduced by
(Papandreou & Yuille, 2011), and henceforth referred to as MAP perturbation, provides an efficient way to generate samples from the Gibbs distribution by injecting random noise (that do not depend on the weights of the decoder w) in the potential or score function of the decoder and then computing the most likely assignment or energy configuration (MAP).",1. Introduction,[0],[0]
"MAP perturbation models are an attractive alternative to expensive Markov Chain Monte Carlo simulations for drawing samples from the Gibbs distribution, in that the former facilitates one-shot sampling.",1. Introduction,[0],[0]
"Moreover, learning MAP predictors for structured prediction problems is particularly attractive because the predictions are robust to random noise.",1. Introduction,[0],[0]
"However, learning the parameters of such MAP predictors involves solving the MAP problem, which in general is intractable.",1. Introduction,[0],[0]
In this paper we obtain a provably polynomial time algorithm for learning the parameters of perturbed MAP predictors with structure based perturbations.,1. Introduction,[0],[0]
"In the following paragraph we summarize the main technical contributions of our paper.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"To the best of our knowledge, we are the first to obtain generalization bounds for MAPperturbation models with structure-based (Gumbel) perturbations — for detailed comparison with existing literature see Section 6.",1. Introduction,[0],[0]
"While it is well known that Gumbel perturbations induce a conditional random field (CRF) distribution over the structured outputs, we show that the generalization error is upper bounded by a CRF loss up to statistical error.",1. Introduction,[0],[0]
We obtain Rademacher based uniform convergence guarantees for the latter.,1. Introduction,[0],[0]
"However, the main contribution of our paper is to obtain a provably polynomial time algorithm for learning MAP-perturbation models for general structured prediction problems.",1. Introduction,[0],[0]
"We propose a novel randomized surrogate loss that lower bounds the CRF loss and still upper bounds the expected loss over data distribution, upto approximation and statistical error terms that decay as eO (1/pm) with m being the number of samples.",1. Introduction,[0],[0]
"While it is NP-Hard to compute and approximate the CRF loss in general (Barahona, 1982; Chandrasekaran et al., 2008), our surrogate loss can be computed in polynomial time.",1. Introduction,[0],[0]
Our results also imply that one can learn parameters of CRF models for structured prediction in polynomial time under certain conditions.,1. Introduction,[0],[0]
"Our work is inspired by the work of (Honorio & Jaakkola, 2016) who also propose a polynomial time algorithm for learning the parameters of a structured decoder in the max-margin framework.",1. Introduction,[0],[0]
"In contrast to prior work which consider weight based perturbations, our work is concerned with structure based perturbations.",1. Introduction,[0],[0]
"Previous algorithms for learning MAP perturbation models, for instance, the hard-EM algorithm by (Gane et al., 2014) and the moment-matching algorithm by (Papandreou & Yuille, 2011), are in general intractable and have no generalization guarantees.",1. Introduction,[0],[0]
"Lastly, the main conceptual contribution of our work is to demonstrate that it is possible to efficiently learn the parameters of a structured
decoder with generalization guarantees without solving the inference problem exactly.",1. Introduction,[0],[0]
We begin this section by introducing our notations and formalizing the problem of learning MAP-perturbation models.,2. Preliminaries,[0],[0]
"In structured prediction, we have an input x 2 X and a set of feasible decodings of the input Y(x).",2. Preliminaries,[0],[0]
"Without loss of generality, we assume that |Y(x)|  r for all x 2 X. Input-output pairs (x, y) are represented by a joint feature vector (x, y) 2 Rd.",2. Preliminaries,[0],[0]
"For instance, when x is a sentence and y is a parse tree, the joint feature map (x, y) can be a vector of 0/1-indicator variables representing if a particular word is present in x and a particular edge is present in y.",2. Preliminaries,[0],[0]
"We will assume that min{
j (x, y) 6= 0",2. Preliminaries,[0],[0]
| j 2,2. Preliminaries,[0],[0]
"[d]} 1 which commonly holds for structured prediction problems, for instance, when using binary features, or features that “count” number of components, edges, parts, etc.
",2. Preliminaries,[0],[0]
"A decoder f w : X! Y, parameterized by a vector w 2 Rd, returns an output y 2 Y(x) given an input x.",2. Preliminaries,[0],[0]
"We consider linear decoders of the form:
",2. Preliminaries,[0],[0]
"f w (x) = argmax y2Y(x)
h (x, y), wi, (1)
which return the highest scoring structured output for a particular input x, where the score is linear in the weights w.",2. Preliminaries,[0],[0]
"As is traditionally the case in high-dimensional statistics, we will assume that the weight vectors are s-sparse, i.e., have at most s non-zero coordinates.",2. Preliminaries,[0],[0]
"We will denote the set of s-sparse d-dimensional vectors by Rd,s.
In the perturb and MAP framework, a stochastic decoder first perturbs the linear score by injecting some independent noise for each structured output y, and then returns the structured output that maximizes the perturbed score.",2. Preliminaries,[0],[0]
Gumbel perturbations are commonly used owing to the max-stability property of the Gumbel distribution.,2. Preliminaries,[0],[0]
"Denoting G( ) as the Gumbel distribution with location and scale parameters 0 and respectively, we have the following stochastic decoder, where ⇠ Gr denotes a collection of r i.i.d.",2. Preliminaries,[0],[0]
"Gumbeldistributed random variables and
y denotes the Gumbel random variable associated with structured output y:
f w, (x) = argmax y2Y(x)
h (x, y), wi+ y .",2. Preliminaries,[0],[0]
"(2)
",2. Preliminaries,[0],[0]
"For any weight vector w, and data set S = {(x i , y i )} i.i.d.⇠ Dm, we consider the following expected and empirical zeroone loss:
L(w,D) =",2. Preliminaries,[0],[0]
"E (x,y)⇠D [E ⇠",2. Preliminaries,[0],[0]
"Gr [1 [y 6= fw, (x)]",2. Preliminaries,[0],[0]
"]] , (3)
L(w, S) = 1
m
mX
i=1
E ⇠",2. Preliminaries,[0],[0]
"Gr [1 [yi 6= fw, (xi)]] , (4)
where 1 [ · ] denotes the indicator function and D is the unknown data distribution.",2. Preliminaries,[0],[0]
"We will let the scale parameter depend on the number of samples m and the weight vector w, and write (m,w) > 0.",2. Preliminaries,[0],[0]
"The reason for this will become clear later, but intuitively one would expect that as the number of samples increases, the magnitude of perturbations should decrease in order to control the generalization error.",2. Preliminaries,[0],[0]
"Under Gumbel perturbations, f
w, (x i ) is distributed according to following conditional random field (CRF) distribution Q(x
i , w) with pmf q( · ;x",2. Preliminaries,[0],[0]
"i
, w) (Gumbel, 1954; Papandreou & Yuille, 2011):
q(y i ;x i , w) = Pr ⇠Gr( ) {fw, (xi) = yi}
=
exp( h (xi,yi),wi/ )
Z(w, x i )
, (5)
where Z(w, x i ) = P y2Y(x) exp(
h (xi,y),wi/ ) is the partition function.",2. Preliminaries,[0],[0]
"The empirical loss in (4) can then be computed as:
(CRF loss) L(w, S) = 1 m
mX
i=1
Pr {f w, (x i )",2. Preliminaries,[0],[0]
6=,2. Preliminaries,[0],[0]
y i } .,2. Preliminaries,[0],[0]
"(6)
The ultimate objective of a learning algorithm is to learn a weight vector w that generalizes to unseen data.",2. Preliminaries,[0],[0]
"Therefore, minimizing the expected loss given by (3) is the best strategy towards that end.",2. Preliminaries,[0],[0]
"However, since the data distribution is unknown, one instead minimizes the empirical loss (4) on a finite number of labeled examples S.",2. Preliminaries,[0],[0]
"As a first step we will show that the empirical loss (6) indeed bounds the expected perturbed loss (3) from above, upto statistical error that decays as eO (1/pm).",3. Generalization Bound,[0],[0]
"We have the following generalization bound.
",3. Generalization Bound,[0],[0]
Theorem 1 (Rademacher based generalization bound).,3. Generalization Bound,[0],[0]
"With probability at least 1 over the choice of m samples S:
(8w 2 Rd,s) L(w,D)  L(w, S) + ""(d, s,m, r, ), where
""(d, s,m, r, ) = 2
r s(ln d+ 2 ln(mr))
m + 3
r ln 2/
2m .
Proof.",3. Generalization Bound,[0],[0]
"Let
g w (x, y) def = Pr ⇠Gr( ) {y 6= fw, (x)} , G def = {g w
| w 2 Rd,s}.",3. Generalization Bound,[0],[0]
"Then by Rademacher based uniform convergence, with probability at least 1 over the choice of m samples, we have
that: (8w 2 Rd,s) L(w,D)  L(w, S) + 2bRS(G)",3. Generalization Bound,[0],[0]
"+ 3 r log 2/
2m ,
(7)
where bRS(G) denotes the empirical Rademacher complexity of G.",3. Generalization Bound,[0],[0]
"Let = (
i
)
m
i=1 be independent Rademacher variables.",3. Generalization Bound,[0],[0]
"Also define W def= {w/ (w,m) | w 2 Rd,s}.",3. Generalization Bound,[0],[0]
"Then, bRS(G)
= E "" sup
w2Rd,s
1
m
mX
i=1
",3. Generalization Bound,[0],[0]
i g w,3. Generalization Bound,[0],[0]
"(x i , y i )
#
=
1 m E
"" sup
w2Rd,s
mX
i=1
",3. Generalization Bound,[0],[0]
"i
Pr ⇠Gr( ) {yi 6=",3. Generalization Bound,[0],[0]
"fw, (xi)} #
(a)
=
1 m E
"" sup
w2W
mX
i=1
",3. Generalization Bound,[0],[0]
"i
Pr ⇠Gr(1) {yi 6=",3. Generalization Bound,[0],[0]
"fw, (xi)} #
 1 m E ⇠Gr(1)
"" E "" sup
w2W
mX
i=1
",3. Generalization Bound,[0],[0]
i 1,3. Generalization Bound,[0],[0]
"[y i 6= f w, (x i )]
##
(b)  1 m E ⇠Gr(1)
"" E "" sup
w2Rd,s
mX
i=1
",3. Generalization Bound,[0],[0]
i 1,3. Generalization Bound,[0],[0]
"[y i 6= f w, (x i )]",3. Generalization Bound,[0],[0]
"## ,
where step (a) follows from Pr ⇠Gr( ) {yi 6=",3. Generalization Bound,[0],[0]
"fw, (xi)}
= Pr ⇠Gr(1) y",3. Generalization Bound,[0],[0]
i 6=,3. Generalization Bound,[0],[0]
"fw / , (xi) , and step (b) follows from W ✓ Rd,s. We will enumerate the structured outputs Y(x
i ) as y i,1 , . . .",3. Generalization Bound,[0],[0]
", y i,r .",3. Generalization Bound,[0],[0]
"For any fixed , the weight vector w induces a linear ordering ⇡
i ( · ; ) over the structured outputs Y(x
i ), i.e., h (x i , y i,⇡i(1; ) ), wi + 1
> h",3. Generalization Bound,[0],[0]
"(x
i , y i,⇡i(2; ) ), wi + 2 > . . .",3. Generalization Bound,[0],[0]
> h,3. Generalization Bound,[0],[0]
"(x i , y i,⇡i(r; ) ), wi + r .",3. Generalization Bound,[0],[0]
"Let ⇡( ) = {⇡ i
} be the orderings over all m data points induced by a fixed weight vector w and fixed , and let ⇧( ) be the collection of all orderings ⇡( ) over all w 2 Rd,s for a fixed .",3. Generalization Bound,[0],[0]
"Since w is s-sparse we have, from results by (Bennett, 1956; Bennett & Hays, 1960; Cover, 1967), that the number of possible linear orderings is |⇧( )|  d
s
(mr)2s  ds(mr)2s .",3. Generalization Bound,[0],[0]
"Therefore we have:
bRS(G)
 1 m E ⇠Gr( )
"" E "" sup
⇡( )2⇧( )
mX
i=1
i 1 ⇥",3. Generalization Bound,[0],[0]
y,3. Generalization Bound,[0],[0]
i 6=,3. Generalization Bound,[0],[0]
"y i,⇡i(1; )
",3. Generalization Bound,[0],[0]
⇤,3. Generalization Bound,[0],[0]
"##
(a)  1 m
p s(log d+ 2 log(mr)) p m
=
r s(log d+",3. Generalization Bound,[0],[0]
"2 log(mr))
",3. Generalization Bound,[0],[0]
"m ,
where the inequality (a) follows from the Massart’s finite class lemma.
",3. Generalization Bound,[0],[0]
"As a direct consequence of the uniform convergence bound given by Theorem 1, we have that minimizing the CRF loss
(6) is a consistent procedure for learning MAP-perturbation models.",3. Generalization Bound,[0],[0]
"While Theorem 1 provides theoretical justification for learning MAP-perturbation models by minimizing the CRF loss (6), with the exception of a few special cases, computing the loss function is in general intractable.",4. Towards an efficient learning algorithm,[0],[0]
"This is due to the need for computing the partition function Z(w, x) which is an NP-hard problem (Barahona, 1982).",4. Towards an efficient learning algorithm,[0],[0]
"Further, even approximating Z(w, x) with high probability and arbitrary precision is also known to be NP-hard (Chandrasekaran et al., 2008).
",4. Towards an efficient learning algorithm,[0],[0]
"To counter this computational bottleneck, we propose an efficient stochastic decoder that decodes over a randomly sampled set of structured outputs.",4. Towards an efficient learning algorithm,[0],[0]
"To elaborate further, given some x 2 X, let R(x,w) be some proposal distribution, parameterized by x and w, over the structured outputs Y(x).",4. Towards an efficient learning algorithm,[0],[0]
"We generate a set T0 of n structured outputs sampled independently from the distribution R and define the following efficient stochastic decoder:
f w, ,T0(x) =",4. Towards an efficient learning algorithm,[0],[0]
"argmax y2T0 h (x, y), wi+ y .",4. Towards an efficient learning algorithm,[0],[0]
"(8)
Therefore f w, ,T0(x) is distributed according to the CRF distribution Q(x,w,T0) with pmf q( · ;x,w,T0) and support on T0 as follows:
q(y;x,w,T0) =",4. Towards an efficient learning algorithm,[0],[0]
"Pr ⇠Gn {fw, ,T0(x) = y}
= 1",4. Towards an efficient learning algorithm,[0],[0]
"[y 2 T0] Z w,x,T0 exp( h (x,y),wi/ ),
where Z w,x,T0 = P y 02T0 exp( h (x,y0),wi/ ).",4. Towards an efficient learning algorithm,[0],[0]
"Note that the partition function Z w,x,T0 can be computed in time linear in n, since |T0| =",4. Towards an efficient learning algorithm,[0],[0]
n.,4. Towards an efficient learning algorithm,[0],[0]
"Now, let T = {T i | x i
2 S} be the collection of n structured outputs sampled for each x
i
in the data set, from the product distribution R(S, w) def= ⇥",4. Towards an efficient learning algorithm,[0],[0]
"m
i=1
(R(x i ) n ).",4. Towards an efficient learning algorithm,[0],[0]
"Note that the distribution R(S, w) does not depend on the {y
i }’s in S. We denote the distribution over the collection of sets {T
i } by R(S, w) to keep the notation light.",4. Towards an efficient learning algorithm,[0],[0]
"Additionally, we consider proposal distributions R(x,w) that are equivalent upto linearly inducible orderings of the structured output.
",4. Towards an efficient learning algorithm,[0],[0]
"Definition 1 (Equivalence of proposal distributions (Honorio & Jaakkola, 2016)).",4. Towards an efficient learning algorithm,[0],[0]
"For any x 2 X, two proposal distributions R(x,w) and R(x,w0), with probability mass functions p(·;x,w) and p(·;x,w0), are equivalent if:
8y, y0 2 Y(x) : h (x, y), wi  h (x, y0), wi and h (x, y), w0i  h (x, y0), w0i
() 8y 2 Y(x) p(y;x,w) =",4. Towards an efficient learning algorithm,[0],[0]
"p(y;x,w0).
",4. Towards an efficient learning algorithm,[0],[0]
"We then write R(x,w) ⌘",4. Towards an efficient learning algorithm,[0],[0]
"R(x,w0) ⌘ R(x,⇡(x)), where ⇡(x) is the linear ordering over Y(x) induced by w (and w0).
",4. Towards an efficient learning algorithm,[0],[0]
"Intuitively speaking, the above definition requires proposal distributions to depend only on the orderings of the values h (x, y
1 ), wi, . . .",4. Towards an efficient learning algorithm,[0],[0]
", h (x, y r ), wi and not on the actual value of h (x, y
j ), wi.",4. Towards an efficient learning algorithm,[0],[0]
"To obtain an efficient learning algorithm with generalization guarantees, we will use augmented sets ¯T = {¯T
i }m i=1
, where ¯T
i = T",4. Towards an efficient learning algorithm,[0],[0]
i,4. Towards an efficient learning algorithm,[0],[0]
[{y i }.,4. Towards an efficient learning algorithm,[0],[0]
"Then, given a random collection of structured outputs T, we consider the following augmented randomized empirical loss for learning the parameters of the MAP-perturbation model:
L(w, S, ¯T) = 1
m
mX
i=1
Pr ⇠Gn f w, , ¯Ti(xi) 6=",4. Towards an efficient learning algorithm,[0],[0]
yi .,4. Towards an efficient learning algorithm,[0],[0]
"(9)
As opposed to the loss function given by (6), the loss in (9) can be computed efficiently for small n.",4. Towards an efficient learning algorithm,[0],[0]
"Our next result shows that the randomized augmented loss lower bounds the full CRF loss L(w, S) as long as ¯T
i is a set, i.e., contains only unique elements.",4. Towards an efficient learning algorithm,[0],[0]
Lemma 1.,4. Towards an efficient learning algorithm,[0],[0]
"For all data sets S, T
i ✓ Y(x i
), and weight vectors w:
L(w, S, ¯T) L(w, S) =
1 m
mX
i=1
",4. Towards an efficient learning algorithm,[0],[0]
"Pr f w, , ¯Ti(xi) = yi ⇥
Pr f w, (x i ) 2 (Y(x i ) \",4. Towards an efficient learning algorithm,[0],[0]
¯T i )  0,4. Towards an efficient learning algorithm,[0],[0]
"(10)
Proof.",4. Towards an efficient learning algorithm,[0],[0]
"For any x 2 X, T ✓ Y(x), y 2 T and weight vector w:
Pr {f w, (x) = y} Pr {f w, ,T(x) = y}
= eh (x,y),wi ⇢ Z(w, x,T) Z(w, x) Z(w, x)Z(w, x,T)
=
eh (x,y),wi
Z(w, x,T)
1
Z(w, x)
8 <
: X
y 02Y(x)\T
eh (x,y 0 ),wi
9 =
;
= Pr {f w, ,T(x) =",4. Towards an efficient learning algorithm,[0],[0]
"y}Pr {fw, (x) 2 Y(x) \ T} .
",4. Towards an efficient learning algorithm,[0],[0]
Since by construction y i,4. Towards an efficient learning algorithm,[0],[0]
2 ¯T,4. Towards an efficient learning algorithm,[0],[0]
"i , the final claim follows.
",4. Towards an efficient learning algorithm,[0],[0]
Remark 1.,4. Towards an efficient learning algorithm,[0],[0]
"If ¯T i = Y(x i ) then L(w, S) = L(w, S, ¯T i ).
",4. Towards an efficient learning algorithm,[0],[0]
"Next, we will show that an algorithm that learns the parameter w of the MAP-perturbation model, by sampling a small number of structured outputs for each x
i and minimizing the empirical loss given by (9), generalizes under various choices of the proposal distribution R. Our first step in that direction would be to obtain uniform convergence guarantees for the stochastic loss (9).",4. Towards an efficient learning algorithm,[0],[0]
"To obtain our generalization bound, we decompose the difference L(w, S) L(w, S, ¯T) as follows:
L(w, S) L(w, S, ¯T) =",4.1. Generalization bound,[0],[0]
"A(w, S) +B(w, S, ¯T), (11) A(w, S) = L(w, S) ET⇠R(S) ⇥",4.1. Generalization bound,[0],[0]
"L(w, S, ¯T)",4.1. Generalization bound,[0],[0]
"⇤ , (12)
B(w, S, ¯T) = ET⇠R(S) ⇥",4.1. Generalization bound,[0],[0]
"L(w, S, ¯T)",4.1. Generalization bound,[0],[0]
"⇤ L(w, S, ¯T), (13) where A(w, S) can be thought of as the approximation error due to using a small number of structured outputs T
i ’s instead of the full sets Y(x
i ), while B(w, S, ¯T) be is the statistical error.",4.1. Generalization bound,[0],[0]
"In what follows, we will bound each of these errors from above.
",4.1. Generalization bound,[0],[0]
"From Lemma 1 it is clear that the proposal distribution plays a crucial role in determining how far the surrogate loss L(w, S, ¯T) is from the CRF loss L(w, S).",4.1. Generalization bound,[0],[0]
"To bound the approximation error, we make the following assumption about the proposal distributions R(x,w).",4.1. Generalization bound,[0],[0]
Assumption 1.,4.1. Generalization bound,[0],[0]
"For all (x
i , y i ) 2 S and weight vectors w 2 Rd,s, the proposal distribution satisfies the following condition with probability at least 1 kwk
1 /pm, for a constant c 2",4.1. Generalization bound,[0],[0]
"[0, 1]:
(i) T i",4.1. Generalization bound,[0],[0]
= {y i },4.1. Generalization bound,[0],[0]
if 8 y 6=,4.1. Generalization bound,[0],[0]
y,4.1. Generalization bound,[0],[0]
i h,4.1. Generalization bound,[0],[0]
"(x i , y i ), wi > h",4.1. Generalization bound,[0],[0]
"(x i , y), wi, (ii) 1
n P y2Tih (xi, y), wi h (xi, yi), wi+c kwk1 otherwise,
where the probability is taken over the set T i .
",4.1. Generalization bound,[0],[0]
"Intuitively, Assumption 1 states that, if y i is not the highest scoring structure under w, then the proposal distribution should return structures T = {y} whose average score is an additive constant factor away from the score of the observed structure",4.1. Generalization bound,[0],[0]
"y
i with high probability.",4.1. Generalization bound,[0],[0]
"Otherwise, the proposal distribution should return the singleton set T = {y
i } with high probability.",4.1. Generalization bound,[0],[0]
"Note that Assumption 1 is in comparison much weaker than the low-norm assumption of (Honorio & Jaakkola, 2016), which requires that, in expectation, the norm of the difference between (x, y) and (x, y
i ) (where y is sampled from the proposal distribution) should decay as 1/pm.",4.1. Generalization bound,[0],[0]
"The following lemma bounds the approximation error from above.
",4.1. Generalization bound,[0],[0]
Lemma 2 (Approximation Error).,4.1. Generalization bound,[0],[0]
"If the scale parameter of the Gumbel perturbations satisfies:  min( kwk 1
/logm,wmin/log((r 1)(pm 1))) for all w 6= 0, and n m0.5 c, then under Assumption 1 A(w, S)  "" 1 (m,n,w), where
"" 1 (m,n,w) def =
kwk",4.1. Generalization bound,[0],[0]
"1p
m +
1
1 + p m ,
and w min = min{|w j",4.1. Generalization bound,[0],[0]
"| | |w j | 6= 0, j 2",4.1. Generalization bound,[0],[0]
"[d]}.
",4.1. Generalization bound,[0],[0]
Proof.,4.1. Generalization bound,[0],[0]
"Let A i (w, S) def = Pr ⇠G( ) {fw, (xi) 6= yi} ETi ⇥",4.1. Generalization bound,[0],[0]
"Pr ⇠G( ) f w, , ¯Ti(xi) 6=",4.1. Generalization bound,[0],[0]
"yi ⇤
be the i-th term of A(w, S).",4.1. Generalization bound,[0],[0]
"We will consider two cases.
",4.1. Generalization bound,[0],[0]
"Case I: y i is strictly the highest scoring structure for x i under w, i.e., 8y 6=",4.1. Generalization bound,[0],[0]
y,4.1. Generalization bound,[0],[0]
i h,4.1. Generalization bound,[0],[0]
"(x i , y i ), wi > h",4.1. Generalization bound,[0],[0]
"(x i
, y), wi.",4.1. Generalization bound,[0],[0]
"First note that:
",4.1. Generalization bound,[0],[0]
"A i (w, S)  ",4.1. Generalization bound,[0],[0]
"Pr ⇠G( ) {fw, (xi) 6= yi} .",4.1. Generalization bound,[0],[0]
"(14)
We will prove that Pr ⇠G( ) {fw, (xi) 6= yi}  1/pm.",4.1. Generalization bound,[0],[0]
"Assume instead that Pr ⇠G( ) {fw, (xi) 6= yi} > 1/pm.",4.1. Generalization bound,[0],[0]
"Then X
y 6=yi
( p m 1)eh (xi,y),wi/",4.1. Generalization bound,[0],[0]
>,4.1. Generalization bound,[0],[0]
"eh (xi,yi),wi/
Let y0 2 Y(x i )",4.1. Generalization bound,[0],[0]
"\ {y i } be such that h (x i , y0), wi is maximized.",4.1. Generalization bound,[0],[0]
"Then, (r 1)(pm 1)eh (xi,y0),wi/ upper bounds the left-hand side of the above equation.",4.1. Generalization bound,[0],[0]
"Taking log on both sides we get:
> h",4.1. Generalization bound,[0],[0]
"(x i , y i )",4.1. Generalization bound,[0],[0]
"(x i , y0), wi log((r 1)(pm 1))
",4.1. Generalization bound,[0],[0]
Since y i is the unique maximizer of the score h,4.1. Generalization bound,[0],[0]
"(x i , y i ), wi, (x
i , y0) and (x i , y i
) must differ on at least one element in the support set of w. This implies, from above and the assumption that the minimum non-zero element of (x, y) is at least 1:
>",4.1. Generalization bound,[0],[0]
"w min
log((r 1)(pm 1)) ,
which violates Assumption 1.",4.1. Generalization bound,[0],[0]
"Therefore from (14) we have that A
i
(w, S)  1/pm.
",4.1. Generalization bound,[0],[0]
Case II: 9y 6=,4.1. Generalization bound,[0],[0]
"y i : h (x i , y), wi h",4.1. Generalization bound,[0],[0]
"(x i , y i ), wi.",4.1. Generalization bound,[0],[0]
"Let
i
(y) def = (x i , y) (x i , y i ).",4.1. Generalization bound,[0],[0]
"In this case,
A i
(w, S) (a)  ETi ⇥",4.1. Generalization bound,[0],[0]
"Pr f w, , ¯Ti(xi) = yi ⇤
= ETi  exp(h (x i , y i
), wi/ ) Z(w, x
i , ¯T i )
",4.1. Generalization bound,[0],[0]
"(b) = ETi
"" 1
1 + P y2Ti e h i(y),wi/
#
(c)  E Si
 1
1 + neSi/
, (15)
where we have defined S i def = 1
n P y2Tih i(y), wi.",4.1. Generalization bound,[0],[0]
"In the
above, in step (a) we dropped the term Pr {f w, (x i ) =",4.1. Generalization bound,[0],[0]
y i } to get an upper bound.,4.1. Generalization bound,[0],[0]
"Step (b) follows from dividing the numerator and denominator by exp(h (x
i , y i ), wi) and that y i 2 ¯T i .",4.1. Generalization bound,[0],[0]
Step (c) follows from Jensen’s inequality.,4.1. Generalization bound,[0],[0]
"Now,
E Si
 1
1 + neSi/
= E Si
 1
1 + neSi/ | S i kwk1 2 Pr
⇢ S i
kwk1 2
+ E Si
 1
1 + neSi/ | S",4.1. Generalization bound,[0],[0]
"i
< kwk 1
2
Pr ⇢ S i < kwk 1
2
(a)  E Si
 1
1 + neSi/ | S",4.1. Generalization bound,[0],[0]
i kwk1 2 + kwk,4.1. Generalization bound,[0],[0]
"1p
m (b)  E Si  1
1 +",4.1. Generalization bound,[0],[0]
neSi log m/kwk1 | S,4.1. Generalization bound,[0],[0]
i kwk1 2 +,4.1. Generalization bound,[0],[0]
kwk,4.1. Generalization bound,[0],[0]
"1p m
= E Si
 1
1 + nmSi/kwk1 | S",4.1. Generalization bound,[0],[0]
i,4.1. Generalization bound,[0],[0]
kwk1 2 + kwk,4.1. Generalization bound,[0],[0]
"1p m
 1 1 + n",4.1. Generalization bound,[0],[0]
"p m +
kwk",4.1. Generalization bound,[0],[0]
"1p
m , (16)
where inequality (a) follows from Assumption 1 and (b) follows from the fact that  kwk
1 /logm.",4.1. Generalization bound,[0],[0]
"Thus from (15) and (16) we have that A
i
(w, S)  1/(1+npm) + kwk 1 /pm.
",4.1. Generalization bound,[0],[0]
"The final claim follows from Case I and II.
",4.1. Generalization bound,[0],[0]
"Note that for c 0.5 the number of structured outputs needed is n = 1, while in the worst case (c = 0) n",4.1. Generalization bound,[0],[0]
= p m.,4.1. Generalization bound,[0],[0]
"Furthermore, n needs to grow polynomially with respect to m in order to achieve O (1/pm) generalization error.",4.1. Generalization bound,[0],[0]
Lemma 3 (Statistical Error).,4.1. Generalization bound,[0],[0]
"For any fixed data set S, the statistical error B(w, S, ¯T) is bounded, simultaneously for all proposal distributions R(x
i , w) over {T i }, as follows:",4.1. Generalization bound,[0],[0]
"PrT (8w 2 Rd,s) B(w, S, ¯T)  "" 2 (d, s, n, r,m, ) | S
1 , (17) where
"" 2 (d, s, n, r,m, ) def = 2
r s(ln d+ 2 ln(nr))
",4.1. Generalization bound,[0],[0]
m,4.1. Generalization bound,[0],[0]
"+
r ln 1/
2m +
r s(ln d+ 2 ln(mr))",4.1. Generalization bound,[0],[0]
+,4.1. Generalization bound,[0],[0]
"ln 1/
2m .
",4.1. Generalization bound,[0],[0]
"The proof of the above lemma is adapted from the proof of Rademacher based uniform convergence, and can be found in Appendix A in the supplementary material.
",4.1. Generalization bound,[0],[0]
"Now, we are ready to present our main result proving uniform convergence of the randomized loss L(w, S, ¯T).",4.1. Generalization bound,[0],[0]
"More specifically, we provide eO (1/pm) generalization error.",4.1. Generalization bound,[0],[0]
Theorem 2.,4.1. Generalization bound,[0],[0]
"With probability at least 1 2 over the choice of the data set S and the set of random structured outputs T, and simultaneously for all w 2 Rd,s and proposal distributions R(x,w):
L(w,D)  L(w, S, ¯T) + "" 1 + "" 2 , (18)
where "" 1 and "" 2 are defined in Lemma 2 and 3 respectively.
",4.1. Generalization bound,[0],[0]
Proof.,4.1. Generalization bound,[0],[0]
The claim follows directly from Lemma 2 and Lemma 3 by taking an expectation with respect to S.,4.1. Generalization bound,[0],[0]
"Having proved uniform convergence of our randomized procedure for learning the parameters of a MAP decoder, we turn our attention to the proposal distribution.",4.2. Examples of proposal distributions,[0],[0]
"We want to construct proposal distributions of the form given by Definition 1 that satisfy Assumption 1 with a large enough constant c. Additionally, for our randomized procedure to run in polynomial time we want the proposal distribution to sample a structured output in constant time.",4.2. Examples of proposal distributions,[0],[0]
"The following algorithm is directly motivated by Assumption 1 where the set neighbors
k (y) for an input x is defined as: neighbors
k
(y) def = {y0 2 Y(x) \",4.2. Examples of proposal distributions,[0],[0]
"{y} | H(y, y0)  k}, with H( · , ·) being the Hamming distance.
",4.2. Examples of proposal distributions,[0],[0]
Algorithm 1,4.2. Examples of proposal distributions,[0],[0]
"An example algorithm implementing a proposal distribution that depends on y
i 2 S. 1: Input: Weight vector w 2 Rd,s, (x
i , y i ) 2 S, parameter ↵ 2 [0, 1] and k 1.",4.2. Examples of proposal distributions,[0],[0]
2: Output: A structured output y 2 Y(x).,4.2. Examples of proposal distributions,[0],[0]
"3: With probability ↵ pick y0 uniformly at random from
Y(x i ), and with probability 1 ↵ set y0 to y i
.",4.2. Examples of proposal distributions,[0],[0]
4: y y0.,4.2. Examples of proposal distributions,[0],[0]
"5: for y0 2 neighbors
k (y) do 6: if h (x, y0), wi h (x, y), wi then 7: y y0.",4.2. Examples of proposal distributions,[0],[0]
"8: end if 9: end for
10:",4.2. Examples of proposal distributions,[0],[0]
"Return y.
Remark 2.",4.2. Examples of proposal distributions,[0],[0]
"Setting ↵ = kwk 1 /pm, Algorithm 1 satisfies the condition given in Definition 1 as well as Assumption 1.",4.2. Examples of proposal distributions,[0],[0]
"Since, for any w,w0 2 Rd,s that induce the same linear ordering over Y(x), conditioned on the y0 sampled in Step 3, the algorithm returns the same y for both w and w0 with probability 1.
",4.2. Examples of proposal distributions,[0],[0]
"Also note that using a larger k ensures that the above algorithm satisfies Assumption 1 with a larger constant c, thereby reducing the number of structured outputs that need to be sampled (n), at the cost of increased computation for sampling a single structured output.
",4.2. Examples of proposal distributions,[0],[0]
The parameter ↵ in Algorithm 1 controls exploration vs exploitation.,4.2. Examples of proposal distributions,[0],[0]
"As ↵ becomes smaller Algorithm 1 returns a proposal from within the neighborhood of y
i while for larger ↵ it explores high scoring structures in the entire set of candidate structures.
",4.2. Examples of proposal distributions,[0],[0]
"Lastly, note that our results do not violate the hardness results of (Sontag et al., 2010), who essentially show that it is NP-hard to decide if the training data is linearly separable.",4.2. Examples of proposal distributions,[0],[0]
"Depending on whether or not the data is linearly separable, the loss L(w, S) (6) can be large or small (for all or some weight vector).",4.2. Examples of proposal distributions,[0],[0]
"While computing L(w, S) is intractable in
general, we merely provide an efficiently computable lower bound L(w, S,T) ((9)) that still upper bounds the expected loss L(w,D).",4.2. Examples of proposal distributions,[0],[0]
"In this section we discuss strategies for minimizing the (randomized) CRF loss L(w, S, ¯T).",4.3. Minimizing the CRF loss,[0],[0]
"Minimizing the randomized CRF loss L(w, S, ¯T) is equivalent to maximizing the randomized CRF gain U(w, S, ¯T)",4.3. Minimizing the CRF loss,[0],[0]
"= 1
m
P m
i=1
",4.3. Minimizing the CRF loss,[0],[0]
"Pr f w, , ¯Ti(xi) = yi , which in turn is equivalent to maximizing logU(w, S, ¯T).",4.3. Minimizing the CRF loss,[0],[0]
"The latter can be accomplished by gradient based methods with the gradient of logU(w, S, ¯T) given by:
r w logU(w, S, ¯T) =
P m
i=1
q",4.3. Minimizing the CRF loss,[0],[0]
"i ( (x i , y i )",4.3. Minimizing the CRF loss,[0],[0]
E,4.3. Minimizing the CRF loss,[0],[0]
"[ (x i , y)])P m
i=1
q",4.3. Minimizing the CRF loss,[0],[0]
"i
,
(19) where q
i
def = Pr f w, , ¯Ti(xi) = yi , and the expectation is taken with respect to y ⇠ Q(x
i , w, ¯T i ).",4.3. Minimizing the CRF loss,[0],[0]
"The exact CRF loss (L(w, S)) can similarly be minimized by using ¯T
i = Y(x i ), for all x
i 2 S, in the above.",4.3. Minimizing the CRF loss,[0],[0]
"Note that by Jensen’s inequality logU(w, S, ¯T) 1
m
P m
i=1
log Pr f w, , ¯Ti(xi) = yi , where the latter can be identified as the log likelihood of the data set S under the CRF distributions {Q(x
i , w, ¯T i )}.",4.3. Minimizing the CRF loss,[0],[0]
"Therefore, L(w, S, ¯T) can be equivalently minimized by minimizing the negative log-likelihood of the data, which in turn gives rise to the well known moment-matching rule known in the literature (Papandreou & Yuille, 2011).",4.3. Minimizing the CRF loss,[0],[0]
"Thus, Algorithm 1 can be used with standard moment matching where the expectation is approximated by averaging over y’s drawn from the distribution Q(x
i , w, ¯T i ).",4.3. Minimizing the CRF loss,[0],[0]
"While standard moment matching is in general intractable, moment matching in conjunction with Algorithm 1 is always efficient.",4.3. Minimizing the CRF loss,[0],[0]
"Indeed, (19) can be thought of as a “weighted” moment matching rule with weights q
i
.",4.3. Minimizing the CRF loss,[0],[0]
"In this section, we evaluate our proposed method (CRF_RAND) on synthetic data against three other methods: CRF_ALL, SVM_RAND, and SVM.",5. Experiments,[0],[0]
"The CRF_RAND method minimizes the randomized loss L(w, S, ¯T) (9) subject to `
1 penalty (as prescribed by Lemma 2) by sampling structured outputs from the proposal distribution given by Algorithm 1.",5. Experiments,[0],[0]
"The CRF_ALL method minimizes the exact (exponential-time) loss L(w, S) (6).",5. Experiments,[0],[0]
"Lastly, SVM is the widely used max-margin method of (Taskar et al., 2003), while SVM_RAND is the randomized SVM method proposed by (Honorio & Jaakkola, 2016).
",5. Experiments,[0],[0]
We generate a ground truth parameter w⇤ 2 Rd with random entries sampled independently from a zero mean Gaussian distribution with variance 100.,5. Experiments,[0],[0]
We then randomly set all but s = p d entries to be zero.,5. Experiments,[0],[0]
"We then generate
a training set of S of 100 samples.",5. Experiments,[0],[0]
"We used the following joint feature map (x, y) for an input output pair.",5. Experiments,[0],[0]
"For every pair of possible edges or elements i and j, we set (x, y)
i,j = 1",5. Experiments,[0],[0]
"[x i,j = 1 ^",5. Experiments,[0],[0]
i 2 y ^ j 2 y].,5. Experiments,[0],[0]
"For instance, for directed spanning trees of v nodes, we have x 2 {0, 1}(v2) and (x, y) 2 R(v2).",5. Experiments,[0],[0]
"We considered directed spanning trees of 6 nodes, directed acyclic graphs of 5 nodes and 2 parents per node, and sets of 4 elements chosen from 15 possible elements.",5. Experiments,[0],[0]
"In order to generate each training sample (x, y) 2 S, we generated a random vector x with independent Bernoulli entries with parameter 1/2.",5. Experiments,[0],[0]
"After generating x, we set y = f
w
⇤ (x), i.e., we solved (1) in order to produce
the latent structured output y from the observed input x and the parameter w⇤.
",5. Experiments,[0],[0]
We set the ` 1 regularization parameter to be 0.01 for all methods.,5. Experiments,[0],[0]
"We used 20 iterations of gradient descent with step size of 1/pt for all algorithms, where t is the iteration, to learn the parameter w for both the exact method and our randomized algorithm.",5. Experiments,[0],[0]
"In order to simplify gradient calculations, we simply set = 1/ log((r 1))(pm 1)) during training.",5. Experiments,[0],[0]
"For CRF_RAND, we used Algorithm 1 with ↵ = kwk
1
/pm and invoke the algorithm p m num-
ber of times to generate the set T i for each i 2",5. Experiments,[0],[0]
"[m] and w. This results in n = |T
i |  pm.",5. Experiments,[0],[0]
"To evaluate the generalization performance of our algorithm we generated a test set S0 = {x0
i , y0 i }m i=1
of 100 samples and calculated two losses.",5. Experiments,[0],[0]
"The first was the full CRF loss (6) on the test set S0, and the second was the test set hamming loss 1
m
P m
i=1
ˆH(f ŵ",5. Experiments,[0],[0]
"(x0 i ), y0 i ), where ˆH( · , ·) is the normalized Hamming distance, and ŵ is the learned parameter.",5. Experiments,[0],[0]
"Hamming distance is a popular distortion function used in structured prediction, and provides a more realistic assessment of the performance of a decoder, since in most cases it suffices to recover most of the structure rather than predicting the structure exactly.",5. Experiments,[0],[0]
"For DAGs and trees the Hamming distance counts the number of different edges between the structured outputs, while for sets it counts the number of different elements.",5. Experiments,[0],[0]
We normalize the Hamming distance to be between 0 and 1.,5. Experiments,[0],[0]
"We computed the mean and 95% confidence intervals of each of these metrics by repeating the above procedure 30 times.
",5. Experiments,[0],[0]
Figure 1 shows the training and test set errors and the training time of the four different algorithms.,5. Experiments,[0],[0]
"CRF_RAND significantly outperformed other algorithms in both the test set loss and test set hamming loss, while being ⇡ 6 times faster than the exact method (CRF_ALL) for DAGs, ⇡ 20 times faster for trees, and⇡ 3 times faster for sets.",5. Experiments,[0],[0]
"The exact CRF method (CRF_ALL) was also significantly faster than the exact SVM (SVM) method while achieving similar test set loss and test set hamming loss.
",5. Experiments,[0],[0]
7rDLn lRVV 7HVt lRVV 7HVt HDmm.,5. Experiments,[0],[0]
"lRVV
DAGV
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
7rDLn lRVV 7HVt lRVV 7HVt HDmm.",5. Experiments,[0],[0]
"lRVV
7rHHV
7rDLn lRVV 7HVt lRVV 7HVt HDmm.",5. Experiments,[0],[0]
"lRVV
6HtV
C5F_5A1D C5F_ALL",5. Experiments,[0],[0]
"6V0_5A1D 6V0_ALL
7rDining time DAGs
0.0
50.0
100.0
150.0
200.0
6 e c o
n G
s
10.04
58.10
10.19
149.91
7rDining time 7rees
16.91
339.76
15.22
934.94
7rDining time 6ets
21.15
57.59
7.32
147.70
Figure 1.",5. Experiments,[0],[0]
"(Left) Training and test set loss (6), and test set hamming loss of the exact method (CRF_ALL) and our randomized algorithm (CRF_RAND), the randomized SVM method by (Honorio & Jaakkola, 2016) (SVM_RAND), and the exact SVM (SVM_ALL), a.k.a max-margin, method of (Taskar et al., 2003).",5. Experiments,[0],[0]
"For the randomized algorithms, i.e., CRF_RAND and SVM_RAND, the training loss is the randomized training loss, i.e., L(w, S, T̄) and L(w, S,T) respectively.",5. Experiments,[0],[0]
(Right) Training time in seconds for the various methods.,5. Experiments,[0],[0]
"Significant body of work exists in computing a single MAP estimate by exploiting problem specific structure, for instance, super-modularity, linear programming relaxations to name a few.",6. Related Work,[0],[0]
"However, in this paper we are concerned with the problem of learning the parameters of MAP perturbation models.",6. Related Work,[0],[0]
"Among generalization bounds for MAP perturbation models, (Hazan et al., 2013b) prove PAC-Bayesian generalization bounds for weight based perturbations.",6. Related Work,[0],[0]
"(Hazan et al., 2013b) additionally propose learning weight based MAP-perturbation models by minimizing the PAC-Bayesian upper bound on the generalization error.",6. Related Work,[0],[0]
"However, their method for learning the parameters involves constructing restricted families of posterior distributions over the weights w that lead to smooth, but not necessarily convex, generalization bounds that can be optimized using gradient based methods.",6. Related Work,[0],[0]
"For learning MAP-perturbation models with structure based (Gumbel) perturbations, (Gane et al., 2014) propose a hard-EM algorithm which is both worstcase exponential time and has no theoretical guarantees.",6. Related Work,[0],[0]
"(Papandreou & Yuille, 2011) on the other hand, propose learning Gumbel MAP-perturbation models by using the moment matching method.",6. Related Work,[0],[0]
"However, such an approach is tractable only for energy functions for which the global minimum can be computed efficiently.",6. Related Work,[0],[0]
"Lastly, (Hazan et al., 2013a; Orabona et al., 2014) consider the problem of efficiently sampling from MAP perturbation models using low dimensional perturbations.",6. Related Work,[0],[0]
"(Hazan & Jaakkola, 2012; Hazan et al., 2013a) additionally propose ways to approximate and bound the partition function.",6. Related Work,[0],[0]
"While such bounds on the partition function can be used, in principle, to approximately minimize the CRF loss (6), it is unclear if one can obtain uniform convergence guarantees for the same, given that computing or even approximating the partition function
is NP-hard (Barahona, 1982; Chandrasekaran et al., 2008).",6. Related Work,[0],[0]
We conclude with some directions for future work.,7. Concluding remarks,[0],[0]
"While in this work we showed that one can learn with approximate inference, it would be interesting to analyze approximate inference for prediction on an independent test set.",7. Concluding remarks,[0],[0]
Another avenue for future work would be to develop more powerful proposal distributions that allow for more finer-grained control over the parameter c by exploiting problem specific structure like submodularity.,7. Concluding remarks,[0],[0]
This material is based upon work supported by the National Science Foundation under Grant No. 1716609-IIS.,Acknowledgements,[0],[0]
MAP perturbation models have emerged as a powerful framework for inference in structured prediction.,abstractText,[0],[0]
Such models provide a way to efficiently sample from the Gibbs distribution and facilitate predictions that are robust to random noise.,abstractText,[0],[0]
"In this paper, we propose a provably polynomial time randomized algorithm for learning the parameters of perturbed MAP predictors.",abstractText,[0],[0]
"Our approach is based on minimizing a novel Rademacher-based generalization bound on the expected loss of a perturbed MAP predictor, which can be computed in polynomial time.",abstractText,[0],[0]
We obtain conditions under which our randomized learning algorithm can guarantee generalization to unseen examples.,abstractText,[0],[0]
Learning Maximum-A-Posteriori Perturbation Models for Structured Prediction in Polynomial Time,title,[0],[0]
"The proliferation of machine learning, and more recently deep learning, in real-world applications has been made possible by an exponential increase in compute capabilities, largely driven by advancements in hardware design.",1. Introduction,[0],[0]
"To maximize the effectiveness of a given design, computer architecture often involves the use of prediction and heuristics.",1. Introduction,[0],[0]
"Prefetching is a canonical example of this, where instructions or data are brought into much faster storage well in
*Work done while at Google 1Google 2Stanford University 3University of California, Santa Cruz.",1. Introduction,[0],[0]
"Correspondence to: Milad Hashemi <miladh@google.com>, Kevin Swersky <kswersky@google.com>.
advance of their required usage.
",1. Introduction,[0],[0]
Prefetching addresses a critical bottleneck in von Neumann computers: computation is orders of magnitude faster than accessing memory.,1. Introduction,[0],[0]
"This problem is known as the memory wall (Wulf & McKee, 1995), and modern applications can spend over 50% of all compute cycles waiting for data to arrive from memory (Kozyrakis et al., 2010; Ferdman et al., 2012; Kanev et al., 2015).",1. Introduction,[0],[0]
"To mitigate the memory wall, microprocessors use a hierarchical memory system, with small and fast memory close to the processor (i.e., caches), and large yet slower memory farther away.",1. Introduction,[0],[0]
"Prefetchers predict when to fetch what data into cache to reduce memory latency, and the key towards effective prefetching is to attack the difficult problem of predicting memory access patterns.
",1. Introduction,[0],[0]
Predictive optimization such as prefetching is one form of speculation.,1. Introduction,[0],[0]
Modern microprocessors leverage numerous types of predictive structures to issue speculative requests with the aim of increasing performance.,1. Introduction,[0],[0]
"Historically, most predictors in hardware are table-based.",1. Introduction,[0],[0]
"That is, future events are expected to correlate with past history tracked in lookup tables (implemented as memory arrays).",1. Introduction,[0],[0]
"These memory arrays are sized based on the working set, or amount of information that the application actively uses.",1. Introduction,[0],[0]
"However, the working sets of modern datacenter workloads are orders of magnitude larger than those of traditional workloads such as SPEC CPU2006 and continue to grow (Ayers et al., 2018; Ferdman et al., 2012; Gutierrez et al., 2011; Hashemi et al., 2016).",1. Introduction,[0],[0]
"This trend poses a significant challenge, as prediction accuracy drops sharply when the working set is larger than the predictive table.",1. Introduction,[0],[0]
"Scaling predictive tables with fast-growing working sets is difficult and costly for hardware implementation.
",1. Introduction,[0],[0]
"Neural networks have emerged as a powerful technique to address sequence prediction problems, such as those found in natural language processing (NLP) and text understanding (Bengio et al., 2003; Mikolov et al., 2010; 2013).",1. Introduction,[0],[0]
"Simple perceptrons have even been deployed in hardware (e.g., SPARC T4 processor (Golla & Jordan, 2011)), to handle branch prediction (Jiménez & Lin, 2001).",1. Introduction,[0],[0]
"Yet, exploring the effectiveness of sequential learning algorithms in microarchitectural designs is still an open area of research.
",1. Introduction,[0],[0]
"In this paper, we explore the utility of sequence-based neu-
ar X
iv :1
80 3.
02 32
9v 1
[ cs
.L G
] 6
M ar
2 01
8
ral networks in microarchitectural systems.",1. Introduction,[0],[0]
"In particular, given the challenge of the memory wall, we apply sequence learning to the difficult problem of prefetching.
",1. Introduction,[0],[0]
Prefetching is fundamentally a regression problem.,1. Introduction,[0],[0]
"The output space, however, is both vast and extremely sparse, making it a poor fit for standard regression models.",1. Introduction,[0],[0]
"We take inspiration from recent works in image and audio generation that discretize the space, namely PixelRNN and Wavenet (Oord et al., 2016a;b).",1. Introduction,[0],[0]
"Discretization makes prefetching more analogous to neural language models, and we leverage it as a starting point for building neural prefetchers.",1. Introduction,[0],[0]
We find that we can successfully model the output space to a degree of accuracy that makes neural prefetching a very distinct possibility.,1. Introduction,[0],[0]
"On a number of benchmark datasets, we find that recurrent neural networks significantly outperform the stateof-the-art of traditional hardware prefetchers.",1. Introduction,[0],[0]
We also find that our results are interpretable.,1. Introduction,[0],[0]
"Given a memory access trace, we show that the RNN is able to discern semantic information about the underlying application.",1. Introduction,[0],[0]
Prefetchers are hardware structures that predict future memory accesses from past history.,2.1. Microarchitectural Data Prefetchers,[0],[0]
They can largely be separated into two categories: stride prefetchers and correlation prefetchers.,2.1. Microarchitectural Data Prefetchers,[0],[0]
"Stride prefetchers are commonly implemented in modern processors and lock onto stable, repeatable deltas (differences between subsequent memory addresses) (Gindele, 1977; Jouppi, 1990; Palacharla & Kessler, 1994).",2.1. Microarchitectural Data Prefetchers,[0],[0]
"For example, given an access pattern that adds four to a memory address every time (0, 4, 8, 12), a stride prefetcher will learn that delta and try to prefetch ahead of the demand stream, launching parallel accesses to potential future address targets (16, 20, 24) up to a set prefetch distance.
",2.1. Microarchitectural Data Prefetchers,[0],[0]
"Correlation prefetchers try to learn patterns that may repeat, but are not as consistent as a single stable delta (Charney & Reeves, 1995; Lai et al., 2001; Somogyi et al., 2006; Roth et al., 1998).",2.1. Microarchitectural Data Prefetchers,[0],[0]
They store the past history of memory accesses in large tables and are better at predicting more irregular patterns than stride prefetchers.,2.1. Microarchitectural Data Prefetchers,[0],[0]
"Examples of correlation prefetchers include Markov prefetchers (Joseph & Grunwald, 1997), GHB prefetchers (Nesbit & Smith, 2004), and more recent work that utilizes larger in-memory structures (Jain & Lin, 2013).",2.1. Microarchitectural Data Prefetchers,[0],[0]
"Correlation prefetchers require large, costly tables, and are typically not implemented in modern multi-core processors.",2.1. Microarchitectural Data Prefetchers,[0],[0]
Deep learning has become the model-class of choice for many sequential prediction problems.,2.2. Recurrent Neural Networks,[0],[0]
"Notably, speech recognition (Hinton et al., 2012) and natural language pro-
cessing (Mikolov et al., 2010).",2.2. Recurrent Neural Networks,[0],[0]
"In particular, RNNs are a preferred choice for their ability to model long-range dependencies.",2.2. Recurrent Neural Networks,[0],[0]
"LSTMs (Hochreiter & Schmidhuber, 1997) have emerged as a popular RNN variant that deals with training issues in standard RNNs, by propagating the internal state additively instead of multiplicatively.",2.2. Recurrent Neural Networks,[0],[0]
"An LSTM is composed of a hidden state h and a cell state c, along with input i, forget f , and output gates o that dictate what information gets stored and propagated to the next timestep.",2.2. Recurrent Neural Networks,[0],[0]
"At timestep N , input xN is presented to the LSTM, and the LSTM states are computed using the following process:
1.",2.2. Recurrent Neural Networks,[0],[0]
"Compute the input, forget, and output gates
iN = σ(Wi[xN ,hN−1] + bi)
fN = σ(Wf",2.2. Recurrent Neural Networks,[0],[0]
"[xN ,hN−1] + bf )
",2.2. Recurrent Neural Networks,[0],[0]
"oN = σ(Wo[xN ,hN−1] + bo)
2.",2.2. Recurrent Neural Networks,[0],[0]
"Update the cell state
cN = fN cN−1",2.2. Recurrent Neural Networks,[0],[0]
"+ iN tanh(Wc[xN ,hN−1] + bc)
3.",2.2. Recurrent Neural Networks,[0],[0]
"Compute the LSTM hidden (output) state
hN = oN tanh(cN )
",2.2. Recurrent Neural Networks,[0],[0]
"Where [xN ,hN−1] represents the concatenation of the current input and previous hidden state, represents elementwise multiplication, and σ(u) = 11+exp(−u) is the sigmoid non-linearity.
",2.2. Recurrent Neural Networks,[0],[0]
"The above process forms a single LSTM layer, where W{i,f,o,c} are the weights of the layer, and b{i,f,o,c} are the biases.",2.2. Recurrent Neural Networks,[0],[0]
LSTM layers can be further stacked so that the output of one LSTM layer at time N becomes the input to another LSTM layer at time N .,2.2. Recurrent Neural Networks,[0],[0]
"It is analogous to having multiple layers in a feed-forward neural network, and allows greater modeling flexibility with relatively few extra parameters.",2.2. Recurrent Neural Networks,[0],[0]
Prefetching is the process of predicting future memory accesses that will miss in the on-chip cache and access memory based on past history.,3.1. Prefetching as a Prediction Problem,[0],[0]
Each of these memory addresses are generated by a memory instruction (a load/store).,3.1. Prefetching as a Prediction Problem,[0],[0]
"Memory instructions are a subset of all instructions that interact with the addressable memory of the computer system.
",3.1. Prefetching as a Prediction Problem,[0],[0]
"Many hardware proposals use two features to make these prefetching decisions: the sequence of caches miss addresses that have been observed so far and the sequence of instruction addresses, also known as program counters
(PCs), that are associated with the instruction that generated each of the cache miss addresses.
",3.1. Prefetching as a Prediction Problem,[0],[0]
"PCs are unique tags, that is each PC is unique to a particular instruction that has been compiled from a particular function in a particular code file.",3.1. Prefetching as a Prediction Problem,[0],[0]
"PC sequences can inform the model of patterns in the control flow of higher level code, while the miss address sequence informs the model of which address to prefetch next.",3.1. Prefetching as a Prediction Problem,[0],[0]
"In modern computer systems, both of these features are represented as 64-bit integers.
",3.1. Prefetching as a Prediction Problem,[0],[0]
"Therefore, an initial model could use two input features at a given timestep N .",3.1. Prefetching as a Prediction Problem,[0],[0]
"It could use the address and PC that generated a cache miss at that timestep to predict the address of the miss at timestep N + 1.
",3.1. Prefetching as a Prediction Problem,[0],[0]
"However, one concern quickly becomes apparent: the address space of an application is extremely sparse.",3.1. Prefetching as a Prediction Problem,[0],[0]
"In our training data with O(100M) cache misses, only O(10M) unique cache block miss addresses appear on average out of the entire 264 physical address space.",3.1. Prefetching as a Prediction Problem,[0],[0]
"This is further displayed when we plot an example trace from omnetpp (a benchmark from the standard SPEC CPU2006 benchmark suite (Sta, 2006)) in Figure 1, where the red datapoints are cache miss addresses1.",3.1. Prefetching as a Prediction Problem,[0],[0]
The wide range and severely multi-modal nature of this space makes it a challenge for time-series regression models.,3.1. Prefetching as a Prediction Problem,[0],[0]
"For example, neural networks tend work best with normalized inputs, however when normalizing this data, the finite precision floating-point representation results in a significant loss of information.",3.1. Prefetching as a Prediction Problem,[0],[0]
"This issue affects modeling at both the input and output levels, and we will describe several approaches to deal with both aspects.
",3.1. Prefetching as a Prediction Problem,[0],[0]
"1Cache miss addresses are from a three level simulated cache hierarchy with a 32 KB L1, 256 KB L2, and 1.25 MB Last Level Cache (LLC), similar to a single thread context from an Intel Broadwell microprocessor.",3.1. Prefetching as a Prediction Problem,[0],[0]
"Rather than treating the prefetching problem as regression, we opt to treat the address space as a large, discrete vocabulary, and perform classification.",3.2. Prefetching as Classification,[0],[0]
This is analogous to next-word or character prediction in natural language processing.,3.2. Prefetching as Classification,[0],[0]
"The extreme sparsity of the space, and the fact that some addresses are much more commonly accessed than others, means that the effective vocabulary size can actually be manageable for RNN models.",3.2. Prefetching as Classification,[0],[0]
"Additionally, the model gains flexibility by being able to produce multi-modal outputs, compared to unimodal regression techniques that assume e.g., a Gaussian likelihood.",3.2. Prefetching as Classification,[0],[0]
"This idea of treating the output prediction problem as one of classification instead of regression has been successfully used in image (Oord et al., 2016a) and audio generation (Oord et al., 2016b).
",3.2. Prefetching as Classification,[0],[0]
"However, there are 264 possible softmax targets, so a quantization scheme is necessary.",3.2. Prefetching as Classification,[0],[0]
"Importantly, in order to be useful, a prefetch must be within a cache line to be completely accurate, usually within 64 bytes.",3.2. Prefetching as Classification,[0],[0]
"There is a second order benefit if it is within a page, usually 4096 bytes, but even predicting at the page level would leave 252 possible targets.",3.2. Prefetching as Classification,[0],[0]
"In (Oord et al., 2016b), they predict 16-bit integer values from an acoustic signal.",3.2. Prefetching as Classification,[0],[0]
"To avoid having to apply a softmax over 216 values, they apply a non-linear quantization scheme to reduce the space to 256 categories.",3.2. Prefetching as Classification,[0],[0]
"This form of quantization is inappropriate for our purposes, as it decreases the resolution of addresses towards the extremes of the address space, whereas in prefetching we need high resolution in every area where addresses are used.
",3.2. Prefetching as Classification,[0],[0]
"Luckily, programs tend to behave in predictable ways, so only a relatively small (but still large in absolute numbers), and consistent set of addresses are ever seen.",3.2. Prefetching as Classification,[0],[0]
"Our primary quantization scheme is to therefore create a vocabulary of common addresses during training, and to use this as the set of targets during testing.",3.2. Prefetching as Classification,[0],[0]
"This reduces the coverage, as there may be addresses at test time that are not seen during training time, however we will show that for reasonablysized vocabularies, we capture a significant proportion of the space.",3.2. Prefetching as Classification,[0],[0]
The second approach we explore is to cluster the addresses using clustering on the address space.,3.2. Prefetching as Classification,[0],[0]
"This is akin to an adaptive form of non-linear quantization.
",3.2. Prefetching as Classification,[0],[0]
"Due to dynamic side-effects such as address space layout randomization (ASLR), different runs of the same program will lead to different raw address accesses (Team, 2003).",3.2. Prefetching as Classification,[0],[0]
"However, given a layout, the program will behave in a consistent manner.",3.2. Prefetching as Classification,[0],[0]
"Therefore, one potential strategy is to predict deltas, ∆N = AddrN+1−AddrN , instead of addresses directly.",3.2. Prefetching as Classification,[0],[0]
"These will remain consistent across program executions, and come with the benefit that the number of uniquely occurring deltas is often orders of magnitude smaller than uniquely occurring addresses.",3.2. Prefetching as Classification,[0],[0]
"This is shown in Table 1, where we show the number of unique PCs, addresses, and
deltas across a suite of program trace datasets.",3.2. Prefetching as Classification,[0],[0]
We also show the number of unique addresses and deltas required to achieve 50% coverage.,3.2. Prefetching as Classification,[0],[0]
"In almost all cases, this is much smaller when considering deltas.",3.2. Prefetching as Classification,[0],[0]
"In our models, we therefore use deltas as inputs instead of raw addresses.",3.2. Prefetching as Classification,[0],[0]
In this section we introduce two LSTM-based prefetching models.,4. Models,[0],[0]
"The first version is analogous to a standard language model, while the second exploits the structure of the memory access space in order to reduce the vocabulary size and reduce the model memory footprint.",4. Models,[0],[0]
Suppose we restricted the output vocabulary size in order to only model the most frequently occurring deltas.,4.1. Embedding LSTM,[0],[0]
"According to Table 1, the size of the vocabulary required in order to obtain at best 50% accuracy is usually O(1000) or less, well within the capabilities of standard language models.",4.1. Embedding LSTM,[0],[0]
"Our first model therefore restricts the output vocabulary size to a large, but feasible 50,000 of the most frequent, unique deltas.",4.1. Embedding LSTM,[0],[0]
"For the input vocabulary, we include all deltas as long as they appear in the dataset at least 10 times.",4.1. Embedding LSTM,[0],[0]
"Expanding the vocabulary beyond this is challenging, both computationally and statistically.",4.1. Embedding LSTM,[0],[0]
"We leave an exploration of approaches like the hierarchical softmax (Mnih & Hinton, 2009) to future work.
",4.1. Embedding LSTM,[0],[0]
"We refer to this model as the embedding LSTM, as illustrated in Figure 2.",4.1. Embedding LSTM,[0],[0]
It uses a categorical (one-hot) representation for both the input and output deltas.,4.1. Embedding LSTM,[0],[0]
"At timestep N , the input PCN and ∆N are individually embedded and then the embeddings are concatenated and fed as inputs to a twolayer LSTM.",4.1. Embedding LSTM,[0],[0]
"The LSTM then performs classification over the delta vocabulary, and the K highest-probability deltas are chosen for prefetching 2.
",4.1. Embedding LSTM,[0],[0]
"In a practical implementation, a prefetcher can return several predictions.",4.1. Embedding LSTM,[0],[0]
"This creates a trade-off, where more predictions increases the probability of a cache hit at the next timestep, but potentially removes other useful items from the cache 3.",4.1. Embedding LSTM,[0],[0]
We opt to prefetch the top-10 predictions of the LSTM at each timestep.,4.1. Embedding LSTM,[0],[0]
"Other possibilities that we do not explore here include using a beam-search to predict the next n deltas, or to learn to directly predict N to N +n steps ahead in one forward pass of the LSTM.
",4.1. Embedding LSTM,[0],[0]
There are several limitations to this approach.,4.1. Embedding LSTM,[0],[0]
"First, a large vocabulary increases the model’s computational and storage
2Directly predicting probabilites is another advantage that classification provides over traditional hardware.
",4.1. Embedding LSTM,[0],[0]
"3This trade-off is subtle, because some of the addresses may end up being useful at future timesteps.
footprint.",4.1. Embedding LSTM,[0],[0]
"Second, truncating the vocabulary necessarily puts a ceiling on the accuracy of the model.",4.1. Embedding LSTM,[0],[0]
"Finally, dealing with rarely occurring deltas is non-trivial, as they will be seen relatively few times during training.",4.1. Embedding LSTM,[0],[0]
"This is known in NLP as the rare word problem (Luong et al., 2015).",4.1. Embedding LSTM,[0],[0]
We hypothesize that much of the interesting interaction between addresses occurs locally in address space.,4.2. Clustering + LSTM,[0],[0]
"As one example, data structures like structs and arrays tend to be stored in contiguous blocks, and accessed repeatedly.",4.2. Clustering + LSTM,[0],[0]
"In this model, we exploit this idea to design a prefetcher that very carefully models local context, whereas the embedding LSTM models both local and global context.
",4.2. Clustering + LSTM,[0],[0]
"By looking at narrower regions of the address space, we can see that there is indeed rich local context.",4.2. Clustering + LSTM,[0],[0]
We took the set of addresses from omnetpp and clustered them into 6 different regions using k-means.,4.2. Clustering + LSTM,[0],[0]
"We show two of the clusters in Figure 3, and the rest can be found in the appendix.
",4.2. Clustering + LSTM,[0],[0]
"To assess the relative accuracy of modeling local addressspace regions, we first cluster the raw address space using k-means.",4.2. Clustering + LSTM,[0],[0]
"The data is then partitioned into these clusters, and deltas are computed within each cluster.",4.2. Clustering + LSTM,[0],[0]
A visual example of this is shown in Figure 4a.,4.2. Clustering + LSTM,[0],[0]
"We found that one of the major advantages of this approach is that the set of deltas within a cluster is significantly smaller than the global vocabulary, alleviating some of the issues with the embedding LSTM.
",4.2. Clustering + LSTM,[0],[0]
"To reduce the size of the model, we use a multi-task LSTM to model all of the clusters.",4.2. Clustering + LSTM,[0],[0]
"Stated another way, we use an LSTM to model each cluster independently, but tie the weights of the LSTMs.",4.2. Clustering + LSTM,[0],[0]
"However, we provide the cluster ID as an additional feature, which effectively gives each LSTM
Table 1.",4.2. Clustering + LSTM,[0],[0]
Program trace dataset statistics.,4.2. Clustering + LSTM,[0],[0]
"M stands for million.
",4.2. Clustering + LSTM,[0],[0]
"Dataset # Misses # PC # Addrs # Deltas # Addrs 50% mass # Deltas 50% mass gems 500M 3278 13.11M 2.47M 4.28M 18 astar 500M 211 0.53M 1.77M 0.06M 15 bwaves 491M 893 14.20M 3.67M 3.03M 2 lbm 500M 55 6.60M 709 3.06M 9
leslie3d 500M 2554 1.23M 0.03M 0.23M 15 libquantum 470M 46 0.52M 30 0.26M 1
mcf 500M 174 27.41M 30.82M 0.07M 0.09M milc 500M 898 3.74M 9.68M 0.87M 46 omnetpp 449M 976 0.71M 5.01M 0.12M 4613 soplex 500M 1218 3.49M 5.27M 1.04M 10 sphinx 283M 693 0.21M 0.37M 0.03M 3 websearch 500M 54600 77.76M 96.41M 0.33M 5186
Figure 3.",4.2. Clustering + LSTM,[0],[0]
Two of six k-means clusters on the omnetpp benchmark dataset.,4.2. Clustering + LSTM,[0],[0]
"Memory accesses are colored according to the PC that generated them.
",4.2. Clustering + LSTM,[0],[0]
"a different set of biases.
",4.2. Clustering + LSTM,[0],[0]
"The partitioning of the address space into narrower regions also means that the set of addresses within each cluster will take on roughly the same order of magnitude, meaning that the resulting deltas can be effectively normalized and used as real-valued inputs to the LSTM.",4.2. Clustering + LSTM,[0],[0]
"This allows us to further reduce the size of the model, as we do not need to keep around a large matrix of embeddings.",4.2. Clustering + LSTM,[0],[0]
"Importantly, we still treat next-delta prediction as a classification problem, as we found that regression is still too inaccurate to be practical 4.
",4.2. Clustering + LSTM,[0],[0]
This version of the LSTM addresses some of the issues of the embedding LSTM.,4.2. Clustering + LSTM,[0],[0]
"The trade-offs are that it requires an additional step of pre-processing to cluster the address space, and that it only models local context.",4.2. Clustering + LSTM,[0],[0]
"That is, it
4The reason for this is after de-normalization, small inaccuracies become dramatically magnified.
cannot model the dynamics that cause the program to access different regions of the address space.",4.2. Clustering + LSTM,[0],[0]
A necessary condition for neural networks to be effective prefetchers is that they must be able to accurately predict cache misses.,5. Experiments,[0],[0]
Our experiments primarily measure their effectiveness in this task when compared with traditional hardware.,5. Experiments,[0],[0]
There are many design choices to be made in prefetching beyond the model itself.,5. Experiments,[0],[0]
"Creating a fair comparison is a subtle process, and we outline our choices here.",5. Experiments,[0],[0]
The data used in our evaluation is a dynamic trace that contains the sequence of memory addresses that an application computes.,5.1. Data Collection,[0],[0]
"This trace is captured by using a dynamic instrumentation tool, Pin (Luk et al., 2005), that attaches to the process and emits a ”PC, Virtual Address” tuple into a file every time the instrumented application accesses memory (every load or store instruction).
",5.1. Data Collection,[0],[0]
"This raw access trace mostly contains accesses that hit in the cache (such as stack accesses, which are present in the data cache).",5.1. Data Collection,[0],[0]
"Since we are focused on predicting cache misses, we obtain the sequence of cache misses by simulating this trace through a simple cache simulator that emulates an Intel Broadwell microprocessor (Section 3.1).
",5.1. Data Collection,[0],[0]
"To evaluate our proposals, we use the memory intensive applications of SPEC CPU2006.",5.1. Data Collection,[0],[0]
This is a standard benchmark suite that is used pervasively to evaluate the performance of computer systems.,5.1. Data Collection,[0],[0]
"However, SPEC CPU2006 also has small working sets when compared to modern datacenter workloads.",5.1. Data Collection,[0],[0]
"Therefore in addition to SPEC benchmarks, we also include Google’s web search workload.",5.1. Data Collection,[0],[0]
Web search is a unique application that exemplifies enterprise-scale software development and drives industrial hardware platforms.,5.1. Data Collection,[0],[0]
"We split each trace into a training and testing set, using 70% for training and 30% for evaluation, and train each LSTM on each dataset independently.",5.2. Experimental Setup,[0],[0]
"The embedding LSTM was trained with ADAM (Kingma & Ba, 2015) while the clustering LSTM was trained with Adagrad (Duchi et al., 2011).",5.2. Experimental Setup,[0],[0]
We report the specific hyperparameters used in the appendix.,5.2. Experimental Setup,[0],[0]
"Precision We measure precision-at-10, which makes the assumption that each model is allowed to make 10 predictions at a time.",5.3. Metrics,[0],[0]
The model predictions are deemed correct if the true delta is within the set of deltas given by the top-10 predictions.,5.3. Metrics,[0],[0]
"A label that is outside of the output vocabulary of the model is automatically deemed to be a failure.
",5.3. Metrics,[0],[0]
Recall We measure recall-at-10.,5.3. Metrics,[0],[0]
"Each time the model makes predictions, we record this set of 10 deltas.",5.3. Metrics,[0],[0]
"At the end, we measure the recall as the cardinality of the set of predicted deltas over the entire set seen at test-time.",5.3. Metrics,[0],[0]
"This measures the ability of the prefetcher to make diverse predictions, but does not give any weight to the relative frequency of the different deltas.
",5.3. Metrics,[0],[0]
One subtlety involving the clustering + LSTM model is how it is used at test-time.,5.3. Metrics,[0],[0]
"In practice, if an address generates a cache miss, then we identify the region of this miss, feed it as an input to the appropriate LSTM, and retrieve predictions.",5.3. Metrics,[0],[0]
"Therefore, the bandwidth required to make a prediction is nearly identical between the two LSTM variants.",5.3. Metrics,[0],[0]
We compare our LSTM-based prefetchers to two hardware prefetchers.,5.4. Model Comparison,[0],[0]
The first is a standard stream prefetcher.,5.4. Model Comparison,[0],[0]
"We simulate a hardware structure that supports up to 10 simul-
taneous streams to maintain parity between the ML and traditional predictors.",5.4. Model Comparison,[0],[0]
"The second is a GHB PC/DC prefetcher (Nesbit & Smith, 2004).",5.4. Model Comparison,[0],[0]
This is a correlation prefetcher that uses two tables.,5.4. Model Comparison,[0],[0]
"The first table stores PCs, these PCs then serve as a pointer into the second table where delta history is recorded.",5.4. Model Comparison,[0],[0]
"On every access, the GHB prefetcher jumps through the second table in order to prefetch deltas that it has recorded in the past.",5.4. Model Comparison,[0],[0]
"This prefetcher excels at more complex memory access patterns, but has much lower recall than the stream prefetcher.
",5.4. Model Comparison,[0],[0]
Figure 5 shows the comparison of the different prefetchers across a range of benchmark datasets.,5.4. Model Comparison,[0],[0]
"While the stream prefetcher is able to achieve a high recall due to its dynamic vocabulary, the LSTM models otherwise dominate, especially in terms of precision.
",5.4. Model Comparison,[0],[0]
"Comparing the embedding LSTM to the cluster + LSTM models, neither model obviously outperforms the other in terms of precision.",5.4. Model Comparison,[0],[0]
The clustering +,5.4. Model Comparison,[0],[0]
"LSTM tends to generate much higher recall, likely the result of having multiple vocabularies.",5.4. Model Comparison,[0],[0]
"An obvious direction is to ensemble these models, which we leave for future work.",5.4. Model Comparison,[0],[0]
"In this experiment, we remove one of the ∆s or PCs from the embedding LSTM inputs, and measure the change in predictive ability.",5.5. Predictive information of ∆s vs PCs,[0],[0]
"This allows us to determine the relative information content contained in each input modality.
",5.5. Predictive information of ∆s vs PCs,[0],[0]
"As Figure 6 shows, both PCs and deltas contain a good amount of predictive information.",5.5. Predictive information of ∆s vs PCs,[0],[0]
"Most of the information required for high precision is contained within the delta sequence, however the PC sequence helps improve recall.",5.5. Predictive information of ∆s vs PCs,[0],[0]
One of the key advantages of using a model to learn patterns that generalize (as opposed to lookup tables) is that the model can then be introspected in order to gain insights into the data.,5.6. Interpreting Program Semantics,[0],[0]
"In Figure 7, we show a t-SNE (Maaten & Hinton, 2008) visualization of the final state of the concatenated (∆, PC) embeddings on mcf, colored according to PCs.
",5.6. Interpreting Program Semantics,[0],[0]
There is clearly a lot of structure to the space.,5.6. Interpreting Program Semantics,[0],[0]
"Linking PCs back to the source code in mcf, we observe one cluster that consists of repetitions of the same code statement, caused by the compiler unrolling a loop.",5.6. Interpreting Program Semantics,[0],[0]
"A different cluster consists only of pointer dereferences, as the application traverses a linked list.",5.6. Interpreting Program Semantics,[0],[0]
Applications besides mcf show this learned structure as well.,5.6. Interpreting Program Semantics,[0],[0]
In omnetpp we find that inserting and removing into a data structure are mapped to the same cluster and data comparisons are mapped into a different cluster.,5.6. Interpreting Program Semantics,[0],[0]
"We show these code examples in the appendix, and leave further inspection for future work, but the model appears to be learning about the higher level structure of the application.",5.6. Interpreting Program Semantics,[0],[0]
"Machine learning in microarchitecture and computer systems is not new, however the application of machine learning as a complete replacement for traditional systems, especially using deep learning, is a relatively new and largely uncharted area.",6.1. Machine Learning in Microarchitecture,[0],[0]
"Here we outline several threads of interaction
between machine learning and microarchitecture research.
",6.1. Machine Learning in Microarchitecture,[0],[0]
Prior work has also directly applied machine learning techniques to microarchitectural problems.,6.1. Machine Learning in Microarchitecture,[0],[0]
"Notably, the perceptron branch predictor (Jiménez & Lin, 2001) uses a linear classifier to predict whether a branch is taken or not-taken.",6.1. Machine Learning in Microarchitecture,[0],[0]
The perceptron learns in an online fashion by incrementing or decrementing weights based on taken/not-taken outcome.,6.1. Machine Learning in Microarchitecture,[0],[0]
"The key benefit of the perceptron is its simplicity, eschewing more complicated training algorithms such as back-propagation to meet tight latency requirements.
",6.1. Machine Learning in Microarchitecture,[0],[0]
"Other applications of machine learning in microarchitecture include applying reinforcement learning for optimizing the long-term performance of memory controller scheduling algorithms (Ipek et al., 2008), tuning performance knobs (Blanton et al., 2015), and using bandits to identify patterns in hardware and software features that relate to a memory access (Peled et al., 2015).
",6.1. Machine Learning in Microarchitecture,[0],[0]
"Recent work also proposed an LSTM-based prefetcher and evaluated it with generated traces following regular expressions (Zeng, October 2017).",6.1. Machine Learning in Microarchitecture,[0],[0]
"Using the squared loss, the model caters to capturing regular, albeit non-stride, patterns.",6.1. Machine Learning in Microarchitecture,[0],[0]
"Irregular memory reference patterns, either due to workload behavior or multi-core processor reordering/interleaving, pose challenges to such regression-based approaches.",6.1. Machine Learning in Microarchitecture,[0],[0]
Zeng (October 2017) evaluate their model on randomly generated patterns.,6.1. Machine Learning in Microarchitecture,[0],[0]
"As detailed in Section 3, we have found regression models to be a poor fit on real workloads.
",6.1. Machine Learning in Microarchitecture,[0],[0]
"Very recent work has also explored the usage of machine learning to replace conventional database index structures such as b-trees and bloom filters (Kraska et al., 2017).",6.1. Machine Learning in Microarchitecture,[0],[0]
"Although the nature of this problem differs from cache prefetching, there are many similarities as well.",6.1. Machine Learning in Microarchitecture,[0],[0]
"Specifically, the idea of using the distribution of the data to learn specific models as opposed to deploying generic data structures.",6.1. Machine Learning in Microarchitecture,[0],[0]
Our clustering approach is also reminiscent of the hierarchical approach that they deploy.,6.1. Machine Learning in Microarchitecture,[0],[0]
"Importantly, they find that neural network models are faster to query than conventional data structures.",6.1. Machine Learning in Microarchitecture,[0],[0]
Memory traces can be thought of as a representation of program behavior.,6.2. Machine Learning of Program Behavior,[0],[0]
"Specifically, they represent a bottom-up view of the dynamic interaction of a pre-specified program with a particular set of data.",6.2. Machine Learning of Program Behavior,[0],[0]
"From a machine learning perspective, researchers have taken a top-down approach to explore whether neural networks can understand program behavior and structure.
",6.2. Machine Learning of Program Behavior,[0],[0]
"One active area of research is program synthesis, where a full program is generated from a partial specification– usually input/output examples.",6.2. Machine Learning of Program Behavior,[0],[0]
Zaremba & Sutskever (2014) use an LSTM to estimate the output of a randomly generated program.,6.2. Machine Learning of Program Behavior,[0],[0]
"The model can only see the sequence of ASCII characters representing the program, and must also generate the resulting output as a sequence of characters.",6.2. Machine Learning of Program Behavior,[0],[0]
"Zaremba & Sutskever (2014) falls into the category of sequence-tosequence models (Sutskever et al., 2014).",6.2. Machine Learning of Program Behavior,[0],[0]
"Another example
in this category is the Neural Turing Machine, which augments an LSTM with an external memory and an attention mechanism to form a differentiable analog of a Turing machine (Graves et al., 2014).",6.2. Machine Learning of Program Behavior,[0],[0]
"This is used to solve simple problems such as sorting, copying, and associative recall.
",6.2. Machine Learning of Program Behavior,[0],[0]
"There is also work on modeling source code directly, as opposed to modeling properties of the resulting program.",6.2. Machine Learning of Program Behavior,[0],[0]
"For example, (Maddison & Tarlow, 2014) creates a generative model of source code using a probabilistic context-free grammar.",6.2. Machine Learning of Program Behavior,[0],[0]
"(Hindle et al., 2012) models source code as if it were natural language using an n-gram model.",6.2. Machine Learning of Program Behavior,[0],[0]
"This approach has been extended to neural language models for source code (White et al., 2015).",6.2. Machine Learning of Program Behavior,[0],[0]
"Lastly, Cummins et al. use neural networks to mine online code repositories to automatically synthesize applications (Cummins et al., 2017).",6.2. Machine Learning of Program Behavior,[0],[0]
Computer architects have long exploited the benefits of learning and predicting program behaviors to unlock control and data parallelism.,7. Conclusion and Future Work,[0],[0]
"The conventional approach of table-based predictors, however, is too costly to scale for data-intensive irregular workloads and showing diminishing returns.",7. Conclusion and Future Work,[0],[0]
The models described in this paper demonstrate significantly higher precision and recall than table-based approaches.,7. Conclusion and Future Work,[0],[0]
"This study also motivates a rich set of questions that this initial exploration does not solve, and we leave these for future research.
",7. Conclusion and Future Work,[0],[0]
"We have focused on a train-offline test-online model, using precision and recall as evaluation metrics.",7. Conclusion and Future Work,[0],[0]
"A prefetcher, through accurate prefetching, can change the distribution of cache misses, which could make a static RNN model less effective.",7. Conclusion and Future Work,[0],[0]
"There are several ways to alleviate this, such as adapting the RNN online, however this could increase the computational and memory burden of the prefetcher.",7. Conclusion and Future Work,[0],[0]
"One could also try training on hits and misses, however this can significantly change the distribution and size of the dataset.
",7. Conclusion and Future Work,[0],[0]
There is a notion of timeliness that is also an important consideration.,7. Conclusion and Future Work,[0],[0]
"If the RNN prefetches a line too early, it risks evicting data from the cache that the processor hasn’t used yet.",7. Conclusion and Future Work,[0],[0]
"If it prefetches too late, the performance impact of the request is minimal, as much of the latency cost of accessing main memory has already been paid.",7. Conclusion and Future Work,[0],[0]
"One simple heuristic is to predict several steps ahead, instead of just the next step.",7. Conclusion and Future Work,[0],[0]
"This would be similar to the behavior of stream prefetchers.
",7. Conclusion and Future Work,[0],[0]
"Finally, the effectivenss of an RNN prefetcher must eventually be measured in terms of its performance impact within a program.",7. Conclusion and Future Work,[0],[0]
Ideally the RNN would be directly optimized for this.,7. Conclusion and Future Work,[0],[0]
"This and the previous issues motivate the use of reinforcement learning techniques (Sutton & Barto, 1998) as a method to train these RNNs in dynamic environments.",7. Conclusion and Future Work,[0],[0]
"Indeed, modern microarchitectures also employ control sys-
tems to control prefetcher aggressiveness, and this provides yet another area in which neural networks could be used.
",7. Conclusion and Future Work,[0],[0]
"Additionally, we have not evaluated the hardware design aspect of our models.",7. Conclusion and Future Work,[0],[0]
Correlation based prefetchers are difficult to implement in hardware because of their memory size.,7. Conclusion and Future Work,[0],[0]
"While it is unclear if RNNs can meet the latency demands required for a hardware accelerator, neural networks also significantly compress learned representations during training, and shift the problem to a compute problem rather than a memory capacity problem.",7. Conclusion and Future Work,[0],[0]
"Given the recent proliferation of ML accelerators, this shift towards compute leaves us optimistic at the prospects of neural networks in this domain.
",7. Conclusion and Future Work,[0],[0]
Prefetching is not the only domain where computer systems employ speculative execution.,7. Conclusion and Future Work,[0],[0]
Branch prediction is the process of predicting the direction of branches that an application will take.,7. Conclusion and Future Work,[0],[0]
Branch target buffers predict the address that a branch will redirect control flow to.,7. Conclusion and Future Work,[0],[0]
Cache replacement algorithms predict the best line to evict from a cache when a replacement decision needs to be made.,7. Conclusion and Future Work,[0],[0]
One consequence of replacing microarchitectural heuristics with learned systems is that we can introspect those systems in order to better understand their behavior.,7. Conclusion and Future Work,[0],[0]
"Our t-SNE experiments only scratch the surface and show an opportunity to leverage much of the recent work in understanding RNN systems (Murdoch & Szlam, 2017; Murdoch et al., 2018).
",7. Conclusion and Future Work,[0],[0]
The t-SNE results also indicate that an interesting view of memory access traces is that they are a reflection of program behavior.,7. Conclusion and Future Work,[0],[0]
"A trace representation is necessarily different from e.g., input-output pairs of functions, as in particular, traces are a representation of an entire, complex, human-written program.",7. Conclusion and Future Work,[0],[0]
This view of learning dynamic behavior provides a different path towards building neural systems that learn and replicate program behavior.,7. Conclusion and Future Work,[0],[0]
The experimental results for precision/recall are given in Table 2/Table 3 respectively.,B. Experimental Results,[0],[0]
The hyperparameters for both LSTM models are given in Table 4,C. LSTM Hyperparameters,[0],[0]
In Figure 9 we show the results of running k-means with 6 clusters on 106 addresses from omnetpp.,D. K-Means Clustering on an Address Trace,[0],[0]
The explosion in workload complexity and the recent slow-down in Moore’s law scaling call for new approaches towards efficient computing.,abstractText,[0],[0]
"Researchers are now beginning to use recent advances in machine learning in software optimizations, augmenting or replacing traditional heuristics and data structures.",abstractText,[0],[0]
"However, the space of machine learning for computer hardware architecture is only lightly explored.",abstractText,[0],[0]
"In this paper, we demonstrate the potential of deep learning to address the von Neumann bottleneck of memory performance.",abstractText,[0],[0]
"We focus on the critical problem of learning memory access patterns, with the goal of constructing accurate and efficient memory prefetchers.",abstractText,[0],[0]
"We relate contemporary prefetching strategies to n-gram models in natural language processing, and show how recurrent neural networks can serve as a drop-in replacement.",abstractText,[0],[0]
"On a suite of challenging benchmark datasets, we find that neural networks consistently demonstrate superior performance in terms of precision and recall.",abstractText,[0],[0]
"This work represents the first step towards practical neural-network based prefetching, and opens a wide range of exciting directions for machine learning in computer architecture research.",abstractText,[0],[0]
Learning Memory Access Patterns,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2812–2817 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2812",text,[0],[0]
"Alzheimer’s disease (AD) is a neurodegenerative progressive disease whose symptoms include memory loss, disorientation, and behavioral issues (Ballard et al., 2011).",1 Introduction,[0],[0]
"In 2017, 5.7 million Americans were living with AD, and the disease accounted for $11.4 billion in healthcare costs in the United States (Alzheimer’s Association, 2018).",1 Introduction,[0],[0]
"AD is diagnosed through clinician-administered questionnaires, such as the Mini-Mental State Examination (MMSE), which assigns a score between 0 and 30 based on responses to questions testing memory, recall, and orientation (Folstein et al., 1975).",1 Introduction,[0],[0]
"For context, a MMSE score of 23 and below is associated with cognitive decline.
",1 Introduction,[0],[0]
AD affects language and some of its symptoms include difficulties in word-finding and changes in the voice.,1 Introduction,[0],[0]
Detecting these subtle changes can help identify AD at an early stage.,1 Introduction,[0],[0]
"Indeed, many studies have applied a combination of natural language processing and machine learning techniques
to detect AD.",1 Introduction,[0],[0]
"On the DementiaBank (DB) dataset, which includes audio files and corresponding transcripts of participants completing a picture description task, Wankerl et al. (2017) employed an n-gram based approach to classify between participants with and without AD.",1 Introduction,[0],[0]
"On the same dataset, Fraser et al. (2015) extracted an extensive list of lexicosyntactic features from the transcripts and identified participants with AD with an accuracy of 81%.",1 Introduction,[0],[0]
"More recently, HernándezDomı́nguez et al. (2018) looked at the information content units of the pictures and compared them to healthy population-specific references to achieve an F-score of 0.81.
",1 Introduction,[0],[0]
"Predicting clinical scores is a harder task and is more common in image processing, where researchers make use of brain scans.",1 Introduction,[0],[0]
"For example, Huang et al. (2016) used MRI scans from 805 subjects and relied on the longitudinal aspect of their dataset to predict MMSE scores.",1 Introduction,[0],[0]
"Specific to the DB dataset, Yancheva et al. (2015) extracted linguistic features and used a bivariate dynamic Bayes net to represent the longitudinal nature of the data, and obtained a mean absolute error (MAE) of 3.83.",1 Introduction,[0],[0]
"Focusing on subjects with larger samples of data yielded a MAE of 2.91.
",1 Introduction,[0],[0]
"In instances where multiple views of the same data are available, it makes sense to learn a vector representation (an embedding) that encapsulates the different sources of information.",1 Introduction,[0],[0]
"Benton et al. (2016) used different representations of their data (e.g., bag-of-words, word vectors) to learn multiview embeddings for Twitter users, and obtained promising results when evaluating their embeddings in downstream prediction tasks.
",1 Introduction,[0],[0]
"In this work, we leverage the multiview nature of DB to learn an embedding for each user.",1 Introduction,[0],[0]
"We evaluate the utility of the multiview embedding in two downstream tasks: classification of AD vs non-AD participants, and clinical score prediction.",1 Introduction,[0],[0]
"We use the DementiaBank (DB) corpus (Becker et al., 1994), which consists of adults aged 44 and older, assigned to either the ‘Dementia’ (N = 167) or ‘Healthy’ (N = 97) group based on a battery of neuropsychological tests and on their medical histories.",2.1 Dataset,[0],[0]
"In DB, participants performed the “Cookie Theft” picture description task from the Boston Diagnostic Aphasia Examination (Goodglass and Kaplan, 1983), in which they verbally describe the contents of a picture.",2.1 Dataset,[0],[0]
"Additionally, participants in the ‘Dementia’ group completed the category fluency (i.e., naming words belonging to a given category), letter fluency (i.e., naming words that start with a given letter), sentence construction, and story recall tasks.",2.1 Dataset,[0],[0]
The picture description and both fluency tasks were professionally transcribed and annotated with instances of filled pauses.,2.1 Dataset,[0],[0]
"Previous experiments in the literature on DB have been limited to the picture description task, most likely because the other tasks are not available for all participants.",2.1 Dataset,[0],[0]
"From transcripts of the picture description, category fluency and letter fluency tasks, we extract 565 linguistic features1.",2.2 Linguistic features,[0],[0]
"We compute lexical features (e.g., the mean number of syllables per word, the vocabulary richness as measured by the type-token-ratio2), semantic features (e.g., the mean specificity of words as measured by their depth in WordNet3), and syntactic features (e.g., the proportion of various parts-of-speech tags, such as nouns and adjectives).",2.2 Linguistic features,[0],[0]
"We also automatically extract various subjective measures, such as the mean imageability (i.e., a word’s ability to evoke a mental image) and the mean age-ofacquisition of words using norms derived from the Bristol (Stadthagen-Gonzalez and Davis, 2006) and Gilhoolie-Logie (Gilhooly and Logie, 1980) norms.",2.2 Linguistic features,[0],[0]
"Finally, we train an LDA model of 100 topics (Blei et al., 2003) using a Wikipedia snap-
1The code to extract these is being made available at https://github.com/SPOClab-ca/COVFEFE.
2The type-token ratio is obtained by dividing the number of types (i.e., the total number of different words) by the number of tokens (i.e., the total number of words).
",2.2 Linguistic features,[0],[0]
"3WordNet (Miller, 1995) is a lexical database which groups English words into collections of synonyms.",2.2 Linguistic features,[0],[0]
"The database is ordered from most generic (e.g., “plant”) to most specific (e.g., “rose”).
shot, and compute the topic probabilities for each transcript.",2.2 Linguistic features,[0],[0]
We apply generalized canonical correlation analysis (GCCA) to our dataset to obtain a multiview embedding.,2.3 Learning a multiview embedding,[0],[0]
"We use GCCA as described by Benton et al. 2016 to learn linear transformations Uj which project different views of our data into the embedding G. In our experiments, we consider the following views of DB: linguistic features of the picture description, category fluency and letter fluency tasks, and demographic information.
",2.3 Learning a multiview embedding,[0],[0]
"Given X ∈ Rd×N , X ′ ∈ Rd′×N ′",2.3 Learning a multiview embedding,[0],[0]
", where N is the total number of data points, N ′ is the total number of data points for which all views J are available, and d and d′ are the dimensions of X and X ′; let Xj and X ′j denote views j of X and X ′. Here, j ∈ {PD,CAT,LET,DEM}, which correspond to the picture description, category fluency, and letter fluency linguistic features, and demographic information, respectively.
1.",2.3 Learning a multiview embedding,[0],[0]
"We use GCCA to learn Uj from X ′PD, X ′CAT , X ′",2.3 Learning a multiview embedding,[0],[0]
"LET , X ′",2.3 Learning a multiview embedding,[0],[0]
"DEM , such that:
minimize Uj ,G′ ∑ j∈J ||G′",2.3 Learning a multiview embedding,[0],[0]
"− UTj X ′j ||2F
Uj ∈ Rdj×k, G′ ∈ Rk×N ′ .
",2.3 Learning a multiview embedding,[0],[0]
2.,2.3 Learning a multiview embedding,[0],[0]
We compute G = UTPDXPD.,2.3 Learning a multiview embedding,[0],[0]
"Since UPD ∈ RdPD×k and XPD ∈ RdPD×N , then G ∈ Rk×N .
3.",2.3 Learning a multiview embedding,[0],[0]
"We concatenate G to a subset of the picture description linguistic features, X∗PD, to obtain C = (X∗PD, G), where C ∈ R(k+d∗PD)×N .
4.",2.3 Learning a multiview embedding,[0],[0]
We use the augmented set of features C for two downstream tasks: AD classification and clinical score prediction.,2.3 Learning a multiview embedding,[0],[0]
"We run all experiments with 10-fold cross validation and test various settings of k, the dimension of the multiview embedding.",3 Results,[0],[0]
"We select the top n linguistic features, ordered through a one-way ANOVA and concatenate them with multiview embeddings of size k.",3.1 GCCA and classification,[0],[0]
"The n + k features are then given as input to a random forest
classifier with 100 decision trees, and we report the F1 scores in Figure 1.",3.1 GCCA and classification,[0],[0]
Our best classification result (F1 = 0.823 ± 0.032) is achieved with a multiview embedding of size k = 35 using the best n = 75 linguistic features.,3.1 GCCA and classification,[0],[0]
"Adding GCCA embeddings improves classification results: an ANOVA test reveals a significant difference between F1 results with and without GCCA (F = 15.85, p = 0.00018), and a post-hoc Tukey’s honest significant difference test reveals that F1 scores are significantly higher in experiments using GCCA embeddings (p = 0.00018).
",3.1 GCCA and classification,[0],[0]
"Next, we look at multiview embeddings generated from different combinations of DB views, and report our F1 scores in Table 1 for embeddings of size k = 35 and using the top n = 75 features.",3.1 GCCA and classification,[0],[0]
"Adding multiview embeddings always improves classification, and we obtain our best results by learning an embedding from the picture description and category fluency views.",3.1 GCCA and classification,[0],[0]
"To predict MMSE scores, we select the top n best features, ordered through a continuous one-way
ANOVA, and concatenate them with our multiview embedding of size k. The n+ k features are then given as input to a linear regression model and we report the mean absolute error (MAE).",3.2 GCCA and regression,[0],[0]
10- fold cross-validation results are given in Figure 2.,3.2 GCCA and regression,[0],[0]
Our lowest MAE of 3.412 ± 0.300 was obtained using a GCCA embedding of size k = 5 and retaining the top n = 75 linguistics features.,3.2 GCCA and regression,[0],[0]
"Adding multiview embeddings yields the best results, but an ANOVA test reveals no significant difference (F = 0.41, p = 0.53).",3.2 GCCA and regression,[0],[0]
"We then perform the same experiments as described in sections 3.1 and 3.2, but we learn our UPD linear projection matrix with a different dataset.",3.3 Learning a multiview embedding from another dataset,[0],[0]
"We use Talk2Me4, an online language assessment from the University of Toronto, in which participants use the web to complete a variety of language tasks, including the picture description task, the vocabulary task, the Winograd task (Levesque et al., 2011), and the word fluency task (including both category and letter fluency).",3.3 Learning a multiview embedding from another dataset,[0],[0]
"For all tasks in Talk2Me, we transcribe the audio recordings using the Kaldi open-source automatic speech recognition engine (Povey et al., 2011), and extract the same set of text features as in Section 2.2.",3.3 Learning a multiview embedding from another dataset,[0],[0]
"Next, we apply GCCA to learn a
4https://www.cs.toronto.edu/talk2me
multiview embedding from the following views: picture description, story recall, vocabulary, fluency, and image naming tasks, and demographics.
",3.3 Learning a multiview embedding from another dataset,[0],[0]
"As in previous experiments, we concatenate the multiview embedding with the DB picture description linguistic features, and use these to classify AD participants and to predict MMSE scores.",3.3 Learning a multiview embedding from another dataset,[0],[0]
"In the regression task, the GCCA features from Talk2Me greatly hinder performance.",3.3 Learning a multiview embedding from another dataset,[0],[0]
The best result we obtain with Talk2Me multiview embeddings is an MAE of 3.929±1.37.,3.3 Learning a multiview embedding from another dataset,[0],[0]
"In classification, we observe improvements, as shown in Figure 3, and obtain an F1 of 0.793 ± 0.052.",3.3 Learning a multiview embedding from another dataset,[0],[0]
"However, an ANOVA test reveals no significant difference with multiview embeddings (F = 0.45, p = 0.50).",3.3 Learning a multiview embedding from another dataset,[0],[0]
"In our experiments, we use GCCA to learn a multiview embedding and augment our existing set of features.",4 Discussion,[0],[0]
"The multiview embedding consists of a vector representation which encapsulates information from various sources of information (i.e., the picture description task, the category and letter fluency tasks, and demographic data).",4 Discussion,[0],[0]
"We hypothesize that the additional information contained in this embedding would be useful in downstream
tasks such as classification and regression.",4 Discussion,[0],[0]
"Indeed, the multiview embedding obtained from DB improves AD detection and clinical score prediction.",4 Discussion,[0],[0]
"Similarly, we also observe an improvement in classification when using a multiview embedding learned from a normative dataset.
",4 Discussion,[0],[0]
"Our results are better in the classification task than in the regression task, since the MMSE score is mainly used as a screening tool (i.e., determining if a person has AD or not) and has restricted sensitivity, especially for identifying milder stages of AD (Trzepacz et al., 2015).",4 Discussion,[0],[0]
We have shown that we can make use of the multiview aspect of a small dataset such as DB to learn a multiview embedding.,5 Conclusion,[0],[0]
This embedding can subsequently be used to improve models for classification and regression.,5 Conclusion,[0],[0]
"In our experiments, multiview embeddings allowed the use of both the category and letter fluency data in DB, even though they were only available for the ‘Dementia’ participants.",5 Conclusion,[0],[0]
"Benefits are also possible using secondary datasets to learn multiview embeddings.
",5 Conclusion,[0],[0]
"Extracting acoustic features – such as pause ratio, pitch, and Mel-frequency cepstral coefficients (MFCCs) – and treating them as an additional view is part of our future work.",5 Conclusion,[0],[0]
"Furthermore, we will look into other secondary datasets as well as different approaches of obtaining multiview embeddings.",5 Conclusion,[0],[0]
"While GCCA allows for an arbitrary number of views, it is limited in that it only learns linear projections to the embedding space.",5 Conclusion,[0],[0]
"A possible alternative is deep generalized canonical correlation analysis (DGCCA), which makes use of neural networks to learn non-linear mappings to the embedding space (Benton et al., 2017).",5 Conclusion,[0],[0]
"This work was partially funded by an NSERC Discovery grant (RGPIN 435874) and by a Young Investigator award by the Alzheimer Society of Canada, both held by Rudzicz.",Acknowledgments,[0],[0]
"As the incidence of Alzheimer’s Disease (AD) increases, early detection becomes crucial.",abstractText,[0],[0]
"Unfortunately, datasets for AD assessment are often sparse and incomplete.",abstractText,[0],[0]
"In this work, we leverage the multiview nature of a small AD dataset, DementiaBank, to learn an embedding that captures different modes of cognitive impairment.",abstractText,[0],[0]
We apply generalized canonical correlation analysis (GCCA) to our dataset and demonstrate the added benefit of using multiview embeddings in two downstream tasks: identifying AD and predicting clinical scores.,abstractText,[0],[0]
"By including multiview embeddings, we obtain an F1 score of 0.82 in the classification task and a mean absolute error of 3.42 in the regression task.",abstractText,[0],[0]
"Furthermore, we show that multiview embeddings can be obtained from other datasets as well.",abstractText,[0],[0]
Learning multiview embeddings for assessing dementia,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1861–1870 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1861",text,[0],[0]
Text representation is a crucial problem in most natural language processing (NLP) and information retrieval (IR) tasks.,1 Introduction,[0],[0]
"In monolingual IR, early research works mostly use vector space models for query-document semantic matching (Salton et al., 1975), which suffer from the problem of synonymy and polysemy.",1 Introduction,[0],[0]
"In order to bridge the lexical gaps, latent semantic models such as latent semantic analysis (LSA) (Deerwester et al., 1990) have been proposed to abstract away from surface text
forms to approximate semantics.",1 Introduction,[0],[0]
"More recently, text representation learned with neural networks is attracting increasing attention of the IR community (Mitra and Craswell, 2017) and positive results have been reported on various evaluation data sets (Fan et al., 2018).
",1 Introduction,[0],[0]
"Compared to the prosperity in monolingual IR, there have been less advancements in CLIR where documents are written in a language different from that of queries.",1 Introduction,[0],[0]
"In addition to document ranking, CLIR models need to cross the language barriers, which makes the task intuitively more difficult than monolingual IR.",1 Introduction,[0],[0]
Traditional approaches reduce CLIR to its monolingual counterpart via performing some way of translation on queries or/and documents.,1 Introduction,[0],[0]
"The typical translation process is performed with either off-the-shelf machine translation (MT) systems or multilingual dictionaries (Nie, 2010).",1 Introduction,[0],[0]
"However, MT based approaches are far from being a suitable solution for solving CLIR problems (refer to detailed analysis in (Zhou et al., 2012)).",1 Introduction,[0],[0]
"Dictionary-based approaches suffer from the same problem of lexical gaps as in the monolingual case (Gupta et al., 2017).",1 Introduction,[0],[0]
"An efficient cross-language representation is in need for CLIR, which is expected to be able to cross both the language and lexical gaps.
",1 Introduction,[0],[0]
The most intuitive idea one can have so as to represent text in cross-language settings is to extend those models in monolingual environment.,1 Introduction,[0],[0]
"For instance, we note studies such as the extension of LSA in (Littman et al., 1998), the extension of principle component analysis (PCA) in (Platt et al., 2010), the extension of autoencoder model in (Chandar et al., 2014), and the extension of word2vec (Mikolov et al., 2013) in (Vulić and Moens, 2015).",1 Introduction,[0],[0]
"These approaches construct crosslanguage and semantic-rich representation of text, which can be applied to CLIR directly.",1 Introduction,[0],[0]
"However, all the models listed here aim to learn general text
representation where the objective is to capture term proximity rather than relevance that is essential for retrieval task (Zamani and Croft, 2017).",1 Introduction,[0],[0]
"A recent work (Gupta et al., 2017) tries to learn taskspecific representation for CLIR.",1 Introduction,[0],[0]
"However, their model only captures ranking signals in monolingual settings, which does not necessarily generalize well in CLIR.
",1 Introduction,[0],[0]
"In this paper, we propose to learn task-specific text representation for CLIR via a novel adversarial learning framework.",1 Introduction,[0],[0]
"Following the convention in generative adversarial networks (GAN) (Goodfellow et al., 2014), our representation learning model is realized as an interplay between two processes, an embedding generator (G) and an adversarial discriminator (D), conducted as a minmax game.",1 Introduction,[0],[0]
"With the GAN framework, we design three constraints to direct the representation learning process.",1 Introduction,[0],[0]
CLIR is essentially a ranking problem and we develop a matching constraint to make sure that documents can be ranked in the right order given a query in another language.,1 Introduction,[0],[0]
"The matching constraint considers both cross-language and monolingual pairwise ranking signals, which is superior to previous studies (e.g., (Gupta et al., 2017)) only considering monolingual matching signals.",1 Introduction,[0],[0]
"Meanwhile, a translation constraint is imposed on the latent representation to bridge the language gaps.",1 Introduction,[0],[0]
These two constraints direct the encoding networks to generate a language-invariant and task-specific representation in the embedding space.,1 Introduction,[0],[0]
"Lastly, an adversarial constraint is proposed to force both language and source invariant to be reached more efficiently and effectively.",1 Introduction,[0],[0]
"Through the joint exploitation of these constraints in an adversarial manner, the embedding space being optimal for CLIR will then result through the convergence of this process.",1 Introduction,[0],[0]
Comprehensive CLIR experiments reveal that our model is superior to state-of-the-art continuous space models and approaches the machine translation and monolingual baselines.,1 Introduction,[0],[0]
Text representation has been a long-standing research question in IR.,2 Related work,[0],[0]
"Classic methods such as vector space model are not able to deal with lexical gaps between queries and documents, resulting in inferior retrieval performance.",2 Related work,[0],[0]
"Latent semantic approaches such as LSA (Deerwester et al., 1990) and latent dirichlet allocation (LDA) (Blei et al.,
2003) abstract away from surface text forms to alleviate sparsity and approximate semantics.",2 Related work,[0],[0]
"More recently, learning based approaches with neural networks have gained great success in NLP (Baroni et al., 2014) and started to attract increasing interests of the IR community.",2 Related work,[0],[0]
"In terms of word level embedding, word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) are two models that have been cited frequently in recent literature.",2 Related work,[0],[0]
"These two models provide semantic-rich representations to bridge lexical gaps between queries and documents, which have been used broadly in neural IR studies (Ganguly et al., 2015; Zheng and Callan, 2015; Zamani and Croft, 2016).
",2 Related work,[0],[0]
"The above studies deal with monolingual text representation, which are related to the crosslanguage models presented below.",2 Related work,[0],[0]
"As for CLIR, typical approaches reduce CLIR to its monolingual counterparts via performing some way of translation.",2 Related work,[0],[0]
"Machine translation systems such as Google translator1 have been widely used to translate queries or documents, which serve as a default and convenient translation option in CLIR.",2 Related work,[0],[0]
"It is however far from being a suitable solution for solving CLIR problems (a detailed analysis can be found in (Zhou et al., 2012)).",2 Related work,[0],[0]
"An alternative solution is to rely on multilingual dictionaries to perform lexicon-level translation, which is mostly in combination with either language modeling strategy (Kraaij et al., 2003) or query structuring framework (Pirkola, 1998).",2 Related work,[0],[0]
"However, dictionary-based methods still suffer from the lexical gap problem which reduces their performance in CLIR.
",2 Related work,[0],[0]
"In fact, researchers have extended the models in monolingual settings and developed continuous space models for cross-language tasks to capture rich semantics.",2 Related work,[0],[0]
These cross-language extensions can be applied to CLIR directly.,2 Related work,[0],[0]
"For instance, Littman et al. (1998) extend LSA to its cross-language version CL-LSA by concatenating document-term matrix of parallel data which acts as dual-language documents to be learned by LSA.",2 Related work,[0],[0]
Such a methodology leads to a dual-language semantic space in which terms from both languages are represented.,2 Related work,[0],[0]
Vinokourov et al. (2002) use parallel data to find most likely correlations between projected vectors based on canonical component analysis technique.,2 Related work,[0],[0]
"The OPCA model (Platt et al., 2010) starts with the basic model
1https://translate.google.com
PCA that is then made discriminative by encouraging comparable document pairs to have similar vector representation.",2 Related work,[0],[0]
"Compared to CL-LSA, OPCA avoids the use of artificial concatenated documents.",2 Related work,[0],[0]
"More recently, neural models have been employed to learn cross-language representations.",2 Related work,[0],[0]
"For instance, autoencoder is extended to a bilingual version BAE in (Chandar et al., 2014) which learns vectorial word representations from aligned sentences.",2 Related work,[0],[0]
Yih et al. (2011) develop S2Net to learn a projection matrix to map the corresponding term vectors into a latent space where similar documents are close.,2 Related work,[0],[0]
S2Net is implemented with Siamese neural network framework.,2 Related work,[0],[0]
Vulić and Moens (2015) first merge two documents from the aligned document pair in a comparable corpus and then train word2vec on the pseudo-bilingual document to obtain cross-language embeddings.,2 Related work,[0],[0]
"The above approaches learn general text representation that captures term proximity rather than relevance which is important for retrieval task (Zamani and Croft, 2017).",2 Related work,[0],[0]
"A recent work (Gupta et al., 2017) tries to learn task-specific embeddings for CLIR.",2 Related work,[0],[0]
"However, it learns ranking signals by preserving pairwise ranking in monolingual settings prior to a transfer learning process to another language, which does not necessarily generalize well in CLIR.
",2 Related work,[0],[0]
"One can find from above analysis that, most existing approaches, either based on neural networks or not, learn general embeddings irrelevant to CLIR.",2 Related work,[0],[0]
"We argue that task-specific embeddings are superior, a fact that is inspired by monolingual IR studies and that will actually be validated by CLIR experiments in this paper.",2 Related work,[0],[0]
"To this end, we will learn cross-language and task-specific embeddings for CLIR via a novel text representation model based on adversarial learning (Goodfellow et al., 2014).",2 Related work,[0],[0]
We will present in this section a neural representation learning framework for CLIR.,3 Representation learning framework,[0],[0]
"As discussed before, the framework is realized based on adversarial learning as an interplay between the generator process and the discriminator process.",3 Representation learning framework,[0],[0]
"We will develop three constraints, namely a matching constraint, a translation constraint and an adversarial constraint, to direct the learning of cross-language and target-specific text embeddings.",3 Representation learning framework,[0],[0]
"For ease of presentation, let us assume in CLIR we have a
source language query qs and a target language document dt.",3 Representation learning framework,[0],[0]
The translation of qs in the target language is qt.,3 Representation learning framework,[0],[0]
"The learning framework is illustrated in figure 1, which consists of an adversarial network NNadv, three dimension adaptation networks NNdim and three encoding networks respectively for qt, dt and qs.",3 Representation learning framework,[0],[0]
There have been various approaches one can use to encode sentences/documents into dense vectors.,3.1 Text representation networks,[0],[0]
"For instance, models based on convolutional neural networks (Kalchbrenner et al., 2014) and models based on recurrent neural networks (Liu et al., 2016) have been popular choices.
",3.1 Text representation networks,[0],[0]
"In order to map queries and documents into the embedding space, we make use of recurrent neural network with the long short-term memory (LSTM) architecture that can deal with vanishing and exploring gradient problems (Hochreiter and Schmidhuber, 1997).",3.1 Text representation networks,[0],[0]
We present here derivation details of LSTM for clarification sake.,3.1 Text representation networks,[0],[0]
The LSTM framework consists of several gates to control the cell state in the network.,3.1 Text representation networks,[0],[0]
"Firstly, a forget gate f (a
sigmoid layer) functions according to:
f τ = σ(Wf ·",3.1 Text representation networks,[0],[0]
"[hτ−1, xτ ] + bf )
",3.1 Text representation networks,[0],[0]
"Then, an input gate i (a sigmoid layer) and a tanh layer work together as follows:
iτ = σ(Wi · [hτ−1, xτ ] + bi)
",3.1 Text representation networks,[0],[0]
C̃τ = tanh(Wc ·,3.1 Text representation networks,[0],[0]
"[hτ−1, xτ ] + bc)
With the forget gate f , the input gate i and the new value C̃, one can update the cell state C as:
Cτ = f τ",3.1 Text representation networks,[0],[0]
∗ Cτ−1,3.1 Text representation networks,[0],[0]
+ iτ ∗,3.1 Text representation networks,[0],[0]
"C̃τ
Lastly, an output gate o (a sigmoid layer) outputs:
oτ = σ(Wo · [hτ−1, xτ ] + bo)",3.1 Text representation networks,[0],[0]
hτ = oτ ∗,3.1 Text representation networks,[0],[0]
"tanh(Cτ )
",3.1 Text representation networks,[0],[0]
"In above equations, xτ is the input at time step τ .",3.1 Text representation networks,[0],[0]
hτ and hτ−1 denote the hidden states at time steps τ,3.1 Text representation networks,[0],[0]
and τ,3.1 Text representation networks,[0],[0]
− 1.,3.1 Text representation networks,[0],[0]
All W and b are parameters.,3.1 Text representation networks,[0],[0]
"For brevity, we can write the update process as:
hτ = LSTM(hτ−1, xτ )
",3.1 Text representation networks,[0],[0]
Given a text sequence x =,3.1 Text representation networks,[0],[0]
"(x1, x2, . . .",3.1 Text representation networks,[0],[0]
", xl), typical methods take the output hl of LSTM at the last time step l as the concentrated representation of the whole sequence x (Sutskever et al., 2014).",3.1 Text representation networks,[0],[0]
"Since queries in IR tasks tend to be short and noisy, we make use of Bidirectional LSTM with pooling (Tan et al., 2015) to obtain a more effective text representation from all the hidden states h1:l. The sequence x is fed from left to right into LSTMa and from right to left into LSTMb.",3.1 Text representation networks,[0],[0]
The new hidden state hτab at time step τ is obtained by concatenating the hidden states of LSTMa and LSTMb at their respective time step τ .,3.1 Text representation networks,[0],[0]
"Since max-pooling has been proven to be efficient in similar tasks (Tan et al., 2015), the latent representation zx of x can be formulated as:
zx = NNdim(MaxPooling(h 1:l ab ))
",3.1 Text representation networks,[0],[0]
"where x can be qs, qt or dt. NNdim is designed to adapt the output dimension and to allow further flexibility for representation learning.",3.1 Text representation networks,[0],[0]
Document ranking is the central problem in both monolingual IR and CLIR tasks.,3.2 Matching constraint and Translation constraint,[0],[0]
"CLIR differs itself from its monolingual counterpart in that the
language gap needs to be crossed prior to the retrieval process.",3.2 Matching constraint and Translation constraint,[0],[0]
"Since the choice of translation strategies (query, document or both) affects the design of other components in our model, we will discuss the translation constraint in section 3.2.1 prior to matching constraints in sections 3.2.2 and 3.2.3.",3.2 Matching constraint and Translation constraint,[0],[0]
"The translation constraint is developed to minimize the differences between a pair of parallel texts, which serves as a basic requirement in the translation scenario.",3.2.1 Translation constraint,[0],[0]
Such a constraint directs the learning of language-invariant text representation for CLIR.,3.2.1 Translation constraint,[0],[0]
"We follow the arguments in previous studies (Vilares et al., 2016) and choose to translate queries in our model, since it is computationally expensive to translate large-scale document collections in practice.",3.2.1 Translation constraint,[0],[0]
"In this paper, we directly employ Google translator to translate queries, which is a popular choice for machine translation that leads to state-of-the-art translation performance.",3.2.1 Translation constraint,[0],[0]
The translation constraint is then imposed on the embedding vectors zqs and zqt of the queries qs and qt.,3.2.1 Translation constraint,[0],[0]
"The translation lossLtra on a set QP of query pairs can be defined with the squared L2 norm, which is:
Ltra = ∑
(qs,qt)∈QP
‖zqs",3.2.1 Translation constraint,[0],[0]
− zqt‖22,3.2.1 Translation constraint,[0],[0]
The matching constraint captures essential characteristics of cross-language ranking.,3.2.2 Cross-language matching constraint,[0],[0]
"Following the practice in learning to rank (Liu, 2009), we model document ranking in the pairwise style where the relevance information is in the form of preferences between pairs of documents with respect to individual queries.",3.2.2 Cross-language matching constraint,[0],[0]
"In the model for CLIR, since we have matching signals from both monolingual text pairs and cross-language text pairs, the model can benefit from complementary knowledge from two resources.",3.2.2 Cross-language matching constraint,[0],[0]
"The monolingual pairwise matching constraint will be introduced in section 3.2.3.
",3.2.2 Cross-language matching constraint,[0],[0]
"Similar to neural models in monolingual settings (Huang et al., 2013), the cross-language pairwise matching constraint is placed on top of the embedding vectors of source language query and target language documents.",3.2.2 Cross-language matching constraint,[0],[0]
"In figure 1, let us assume xqs has a relevant document xdt+ and an irrelevant document xdt− according to annotated text pairs.",3.2.2 Cross-language matching constraint,[0],[0]
"In training, the positive sample xdt+
for xqs can be chosen as the most relevant texts according to annotation, and the negative sample xdt− is picked randomly from the data collection.",3.2.2 Cross-language matching constraint,[0],[0]
The cross-language matching constraint encourages the hidden representation of xdt+ to be near to the hidden representation of xqs in the semanticrich embedding space.,3.2.2 Cross-language matching constraint,[0],[0]
"Meanwhile, it asks the hidden representation of xdt− to be far from that of xqs .",3.2.2 Cross-language matching constraint,[0],[0]
We follow typical neural IR models and make use of cosine as the distance measure of hidden vectors.,3.2.2 Cross-language matching constraint,[0],[0]
"The probability that dt+ is ranked higher than dt− given qs can be derived as:
P̂ (qs) = σ[βc · (cos(zqs , zdt+)− cos(zqs , zdt−))",3.2.2 Cross-language matching constraint,[0],[0]
"]
where σ is the sigmoid function with a hyperparameter βc controlling its shape.",3.2.2 Cross-language matching constraint,[0],[0]
"The crosslanguage matching loss Lmatc on cross-language triplet set QDc can be defined with cross-entropy loss as:
Lmatc = ∑
(qs,dt+,dt−)∈QDc
CE[P (qs), P̂ (qs)]
where CE denotes the cross-entropy operator between two distributions and P (qs) is the actual counterpart of P̂ (qs) estimated from annotation with a strategy similar to that in (Dehghani et al., 2017).",3.2.2 Cross-language matching constraint,[0],[0]
The monolingual matching constraint Lmatm can be built in a way similar to that of Lmatc.,3.2.3 Monolingual matching constraint,[0],[0]
"Lmatm is imposed on a set QDm of monolingual triplet (qt, dt+, dt−) as:
Lmatm = ∑
(qt,dt+,dt−)∈QDm
CE[P (qt), P̃ (qt)]
where P (qt) is the actual counterpart of P̃ (qt) estimated from annotation.",3.2.3 Monolingual matching constraint,[0],[0]
P̃ (qt) denotes the probability that dt+ is ranked higher than dt− given qt.,3.2.3 Monolingual matching constraint,[0],[0]
"It can be computed with the sigmoid function as:
P̃ (qt) =",3.2.3 Monolingual matching constraint,[0],[0]
"σ[βm · (cos(zqt , zdt+)− cos(zqt , zdt−))",3.2.3 Monolingual matching constraint,[0],[0]
"]
where βm is a hyper-parameter.",3.2.3 Monolingual matching constraint,[0],[0]
"Since our model is implemented with adversarial framework, we propose to model the representation generator G, which embodies the process of language-invariant and task-specific embedding of queries and documents into a latent
subspace, under a combination of three constraints introduced above.",3.2.4 Embedding generator constraint,[0],[0]
The translation constraint aims to guarantee language invariant when translating queries.,3.2.4 Embedding generator constraint,[0],[0]
The cross-language matching constraint explicitly captures cross-language ranking signals from cross-language text pairs.,3.2.4 Embedding generator constraint,[0],[0]
"The monolingual matching constraint takes monolingual ranking into account so as to complement the crosslanguage ranking signals.
",3.2.4 Embedding generator constraint,[0],[0]
"Combing the three constraints above, we obtain a comprehensive constraint that should be obeyed by the embedding generator process.",3.2.4 Embedding generator constraint,[0],[0]
"With the regularization term Lreg equaling to the sum of Frobenius norms of all weight matrices in the text embedding phase, we can write the embedding generator constraint LG as:
LG(θG) = γ1 ·Ltra+γ2 ·Lmatc+γ3 ·Lmatm+Lreg
where θG denotes the set of parameters in the generator networks, and γ1, γ2, γ3 are hyperparameters.",3.2.4 Embedding generator constraint,[0],[0]
We will introduce the adversarial constraint in this part.,3.3 Adversarial constraint,[0],[0]
"GAN (Goodfellow et al., 2014) simultaneously trains a generative model G and a disriminative model D in a competing way.",3.3 Adversarial constraint,[0],[0]
G generates samples from a source of noisew that satisfies w ∼ Pn(w) and tries to capture the real data distribution,3.3 Adversarial constraint,[0],[0]
"Pr. D learns to distinguish between the generated samples from G and the true data sampled from Pr (in practice, from training data).",3.3 Adversarial constraint,[0],[0]
The training procedure for G is to try its best to fool D. Let us assume that G generates samples satisfying the distribution Pg that is implicitly decided by G(w).,3.3 Adversarial constraint,[0],[0]
"The GAN value function V (G,D) on which D and G play the minmax game can be written as:
min G max D V (D,G) =Ex∼Pr",3.3 Adversarial constraint,[0],[0]
"[logD(x)] (1)
+ Ex∼Pg",3.3 Adversarial constraint,[0],[0]
"[log(1−D(x))]
Theoretical analysis has indicated that playing the minmax game as above amounts to minimizing the Jensen-Shannon divergence between Pg and Pr.
We follow the general idea of GAN and develop an adversarial component on top of the embedding space in figure 1.",3.3 Adversarial constraint,[0],[0]
"We note that GAN has been used in representation learning in a similar way as in (Bousmalis et al., 2016; Liu et al., 2017).",3.3 Adversarial constraint,[0],[0]
"In our model in figure 1, the adversarial component NNadv acts as the discriminator D
which tries its best to detect whether the embedding vector z is encoded from xqt , xdt or xqs .",3.3 Adversarial constraint,[0],[0]
"In this paper, NNadv is implemented as a neural network with a softmax output layer.",3.3 Adversarial constraint,[0],[0]
The output of NNadv then corresponds to a probability distribution vector over the input sources.,3.3 Adversarial constraint,[0],[0]
Let us denote the ground truth label of the current input z to NNadv as lz which indicates the source that z is encoded from.,3.3 Adversarial constraint,[0],[0]
"We can adjust equation 1 to our settings and obtain the adversarial loss Ladv on a query set Qt and a document set Dt in the target language, as well as a query set Qs in the source language, which can be written as:
Ladv = min G max D ∑ x∈Qt,Dt,Qs logNNadv(zx) ◦",3.3 Adversarial constraint,[0],[0]
"lzx
where ◦ is the inner product operator.",3.3 Adversarial constraint,[0],[0]
"Following the training convention of GAN (Goodfellow et al., 2014), the process of learning the language-invariant and task-specific text representation for CLIR should be conducted by jointly minimizing the generator constraint LG and the adversarial loss Ladv, which leads us to the combined objective function L as:
L = LG + Ladv
According to the rule of playing the minmax game in GAN, G tries its best to maximize the probability that D makes a mistake and D tries its best to distinguish between real data and generated data (in our case, various input sources).",3.4 Training procedure,[0],[0]
The theoretical requirement behind GAN that D is maintained near its optimal solution as long as G changes slowly enough motivates us to update the discriminator part k steps per update of the generator part in the iterative optimization process.,3.4 Training procedure,[0],[0]
"Based on these discussions, the minmax optimization process can be derived as:
1.",3.4 Training procedure,[0],[0]
"Optimize D when fixing G through: θ̂D = argmaxθD L(θ̂G, θD)
2.",3.4 Training procedure,[0],[0]
"Optimize G when fixing D through: θ̂G = argminθG L(θG, θ̂D)
",3.4 Training procedure,[0],[0]
The optimization can be implemented with mini-batch gradient ascent (for θD) and descent (for θG).,3.4 Training procedure,[0],[0]
"In this section, we conduct CLIR experiments so as to compare our text representation model with several other models.",4 Experiments and results,[0],[0]
"To perform CLIR experiments, we rely on broadly used data sets released in the bilingual tasks of the cross-language evaluation forum (CLEF) 2.",4.1.1 CLIR evaluation sets,[0],[0]
We choose to use the data from the year 2000 to 2004.,4.1.1 CLIR evaluation sets,[0],[0]
"Table 1 lists the characteristics of the data set, which include number of documents (Nd), number of distinct words (Nw), the average document length (DLavg) and the number of queries (Nq) in each task.",4.1.1 CLIR evaluation sets,[0],[0]
"We use source language queries in French (Fr), German (De) and Italian (It) to retrieve target language documents in English (En).",4.1.1 CLIR evaluation sets,[0],[0]
Queries from year 2000 to 2002 are combined to a single task in table 1 since they have the same target set.,4.1.1 CLIR evaluation sets,[0],[0]
"In order to train the representation learning model, we need to construct a data set consisting of annotated text pairs.",4.1.2 Training set,[0],[0]
"We combine AOL queries (Pass et al., 2006) and a set of news titles downloaded from the news sites3 to constitute training query set of diversity.",4.1.2 Training set,[0],[0]
"Following the previous work (Gupta et al., 2017), we sample a balanced subset (1M) from such query set and use these queries to retrieve the data collection with BM25.",4.1.2 Training set,[0],[0]
"For each training query, we take the top retrieved texts as positive samples, and the negative samples are selected randomly from the data collection.",4.1.2 Training set,[0],[0]
"In addition to the pseudo-labeled text pairs of low quality, we combine the LETOR4.0 dataset (Qin and Liu, 2013) that is developed for evaluating learning to rank models.",4.1.2 Training set,[0],[0]
"The LETOR4.0 dataset consists of relevance judgments of higher quality compared to
2http://www.clef-initiative.eu 3We fetch 2.8M web pages from several news websites such as ChinaDaily (www.chinadaily.com.cn) and XinhuaNews (www.xinhuanet.com).
pseudo-labeled data.",4.1.2 Training set,[0],[0]
"The two data resources can complement each other in the training process.
",4.1.2 Training set,[0],[0]
"In our experiments, the pseudo-labeled data is used to train the whole model and the LETOR dataset is employed to fine tune the parameters relevant to the source queries and target documents which are more important for the cross-language retrieval task.",4.1.2 Training set,[0],[0]
The terms are initialized as the 512d word2vec vectors trained on Wikipedia dump corpus4.,4.2.1 Experimental setup,[0],[0]
"The term embeddings are fed into the LSTM model of which the hidden unit number is chosen from {64, 128, 256, 512}.",4.2.1 Experimental setup,[0],[0]
The adversarial network NNadv is as a three-layer feed-forward network with softmax on top of the last layer.,4.2.1 Experimental setup,[0],[0]
"NNdim is implemented as a feed-forward network with layer dimension chosen from {32, 64, 128, 256} and hidden layer number chosen from {1, 2}.",4.2.1 Experimental setup,[0],[0]
"The values of hyper-parameters γ1, γ2 and γ3 are chosen from {0.01, 0.1, 1, 10, 100}.",4.2.1 Experimental setup,[0],[0]
"The learning rate is selected from {10−1, 10−2, 10−3, 10−4, 10−5}.",4.2.1 Experimental setup,[0],[0]
"Those hyper-parameters are tuned on the validation set which is 20% of the training queries randomly selected.
",4.2.1 Experimental setup,[0],[0]
"For evaluation, we present results in terms of mean average precision (MAP).",4.2.1 Experimental setup,[0],[0]
Statistically significant differences between various models are determined using the paired t-test with p < 0.05.,4.2.1 Experimental setup,[0],[0]
"We make use of three categories of baselines for CLIR experiments.
",4.2.2 Baseline approaches,[0],[0]
1.,4.2.2 Baseline approaches,[0],[0]
"Monolingual run (MON): a baseline with target language queries that are strictly parallel to source language queries.
2.",4.2.2 Baseline approaches,[0],[0]
"Machine translation (MT): a baseline with target-language queries translated by machine translation system from sourcelanguage queries.
3.",4.2.2 Baseline approaches,[0],[0]
Cross-language text representation models: baselines that rely on continuous space models for cross-language text representation.,4.2.2 Baseline approaches,[0],[0]
"We make use here of S2Net (Yih et al., 2011), BAE (Chandar et al., 2014), and XCNN (Gupta et al., 2017) for the CLIR task.
",4.2.2 Baseline approaches,[0],[0]
4https://dumps.wikimedia.org,4.2.2 Baseline approaches,[0],[0]
Table 2 lists the experimental results on CLEF dataset for our model (the column OURS) and all baseline models.,4.3.1 Comparisons to state-of-the-art,[0],[0]
"There are three data collections and three language pairs, amounting to nine cross-language retrieval tasks.",4.3.1 Comparisons to state-of-the-art,[0],[0]
"Except the strong baselines MON and MT, our model shows the best overall performance among all CLIR strategies.",4.3.1 Comparisons to state-of-the-art,[0],[0]
"Indeed, our model outperforms all continuous space baselines (i.e., S2Net, BAE and XCNN) with statistical significance in almost all cases.",4.3.1 Comparisons to state-of-the-art,[0],[0]
Our model decreases slightly from the strong MT baseline in most retrieval tasks with only one degradation being significant on 03(De-En).,4.3.1 Comparisons to state-of-the-art,[0],[0]
"Furthermore, one can find that our model approaches the monolingual baseline very much in all retrieval tasks with all MAP ratios around or over 90%.",4.3.1 Comparisons to state-of-the-art,[0],[0]
"In our experiments, we have not performed comparisons to CL-LSA (Littman et al., 1998) and its variant OPCA (Platt et al., 2010), because they have been consistently outperformed by other CLIR strategies with a large margin (Schauble and Sheridan, 1997; Nie, 2010; Vulić et al., 2011).
",4.3.1 Comparisons to state-of-the-art,[0],[0]
"Among all continuous space baselines, the most recent model XCNN shows the best performance.",4.3.1 Comparisons to state-of-the-art,[0],[0]
XCNN always outperforms linear projection methods S2Net with significance.,4.3.1 Comparisons to state-of-the-art,[0],[0]
It also significantly outperforms the non-linear model BAE in all cases.,4.3.1 Comparisons to state-of-the-art,[0],[0]
"This is coincident with previous conclusions in (Gupta et al., 2017) due to the fact that XCNN learns target-specific representation for CLIR but the other models do not.",4.3.1 Comparisons to state-of-the-art,[0],[0]
"Our model also tries to learn task-specific representation for CLIR, which significantly outperforms XCNN in most cases according to the results in table 2.",4.3.1 Comparisons to state-of-the-art,[0],[0]
The reasons might be that (1) our method is modeled in a more effective adversarial learning framework.,4.3.1 Comparisons to state-of-the-art,[0],[0]
(2) we explicitly capture crosslanguage ranking signals in embedding generator in addition to monolingual ranking signals used in XCNN.,4.3.1 Comparisons to state-of-the-art,[0],[0]
(3) our model can jointly capture the translation knowledge and document ranking knowledge in a unified framework.,4.3.1 Comparisons to state-of-the-art,[0],[0]
Our model can be customized easily by altering the constraints to direct the representation learning process.,4.3.2 Variant of our model,[0],[0]
"Since the specificity of our model comes from the adversarial learning framework that has never been investigated in CLIR, we re-
move the constraint Ladv from the original model M and obtain the variantMadv.",4.3.2 Variant of our model,[0],[0]
"In this case,Madv can be optimized with standard mini-batch gradient descent approach, without playing the minmax game.",4.3.2 Variant of our model,[0],[0]
"We redo above CLIR experiments with the same settings as above and obtain the retrieval results of Madv in table 3.
",4.3.2 Variant of our model,[0],[0]
"From the results one can find that when removing the adversarial component from the original model, Madv decreases from the original model M in all retrieval tasks.",4.3.2 Variant of our model,[0],[0]
The differences that are significant appear in 5 out of 9 retrieval tasks.,4.3.2 Variant of our model,[0],[0]
"The results demonstrate that learning generator and discriminator in a competing style within the adversarial learning framework leads to representation of higher quality, which eventually supports efficient CLIR.",4.3.2 Variant of our model,[0],[0]
"If we compare the variant Madv with the XCNN model in table 2, we find that Madv still performs better than XCNN in most
cases.",4.3.2 Variant of our model,[0],[0]
"Such a comparison implicitly indicates that the joint exploitation of monolingual matching constraint, cross-language matching constraint and translation constraint in a single model is more efficient than using them separately as in the XCNN model.",4.3.2 Variant of our model,[0],[0]
"In this paper, we propose a novel text representation approach for CLIR based on the adversarial learning framework.",5 Conclusions,[0],[0]
"The learning framework is implemented as an interplay between an embedding generator process and an adversarial discriminator process, which leads to an optimal representation that is both language invariant and domain specific.",5 Conclusions,[0],[0]
The embedding generator is learned such that it explicitly considers both cross-language and monolingual pairwise ranking signals.,5 Conclusions,[0],[0]
"In this way, it can ensure that the learned embeddings benefit from both sources and are directly optimized for CLIR.",5 Conclusions,[0],[0]
"To the best of our knowledge, it is the first time adversarial learning has been applied to CLIR.",5 Conclusions,[0],[0]
Experiments on various language pairs in CLEF data collection show that our model is significantly better than other latent semantic models for CLIR.,5 Conclusions,[0],[0]
"Indeed, our model approaches the performance of machine translation and monolingual baselines.",5 Conclusions,[0],[0]
We thank the anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
This work was supported by the Fundamental Research Funds for Central Universities of CCNU (No. CCNU15A05062).,Acknowledgments,[0],[0]
"The existing studies in cross-language information retrieval (CLIR) mostly rely on general text representation models (e.g., vector space model or latent semantic analysis).",abstractText,[0],[0]
These models are not optimized for the target retrieval task.,abstractText,[0],[0]
"In this paper, we follow the success of neural representation in natural language processing (NLP) and develop a novel text representation model based on adversarial learning, which seeks a task-specific embedding space for CLIR.",abstractText,[0],[0]
Adversarial learning is implemented as an interplay between the generator process and the discriminator process.,abstractText,[0],[0]
"In order to adapt adversarial learning to CLIR, we design three constraints to direct representation learning, which are (1) a matching constraint capturing essential characteristics of cross-language ranking, (2) a translation constraint bridging language gaps, and (3) an adversarial constraint forcing both language and source invariant to be reached more efficiently and effectively.",abstractText,[0],[0]
"Through the joint exploitation of these constraints in an adversarial manner, the underlying cross-language semantics relevant to retrieval tasks are better preserved in the embedding space.",abstractText,[0],[0]
Standard CLIR experiments show that our model significantly outperforms state-of-the-art continuous space models and approaches the strong machine translation and monolingual baselines.,abstractText,[0],[0]
Learning Neural Representation for CLIR with Adversarial Framework,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3174–3187 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3174",text,[0],[0]
"With the continued success of encoder-decoder models for machine translation and related tasks, there has been great interest in extending these methods to build general-purpose, data-driven natural language generation (NLG) systems (Mei et al., 2016; Dušek and Jurcıcek, 2016; Lebret et al., 2016; Chisholm et al., 2017; Wiseman et al., 2017).",1 Introduction,[0],[0]
"These encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015) use a neural encoder model to represent a source knowledge base, and a decoder model to emit a textual description word-by-word, conditioned on the source encoding.",1 Introduction,[0],[0]
"This style of generation contrasts with the more traditional division of labor in NLG, which famously emphasizes addressing the two questions of “what to say” and “how to say it” separately, and which leads to systems with explicit content selection, macro- and micro-planning, and surface realization components (Reiter and Dale, 1997; Jurafsky and Martin, 2014).
",1 Introduction,[0],[0]
"Encoder-decoder generation systems appear to have increased the fluency of NLG outputs, while reducing the manual effort required.",1 Introduction,[0],[0]
"However, due to the black-box nature of generic encoderdecoder models, these systems have also largely sacrificed two important desiderata that are often found in more traditional systems, namely (a) interpretable outputs that (b) can be easily controlled in terms of form and content.
",1 Introduction,[0],[0]
"This work considers building interpretable and controllable neural generation systems, and proposes a specific first step: a new data-driven generation model for learning discrete, template-like
structures for conditional text generation.",1 Introduction,[0],[0]
"The core system uses a novel, neural hidden semimarkov model (HSMM) decoder, which provides a principled approach to template-like text generation.",1 Introduction,[0],[0]
We further describe efficient methods for training this model in an entirely data-driven way by backpropagation through inference.,1 Introduction,[0],[0]
"Generating with the template-like structures induced by the neural HSMM allows for the explicit representation of what the system intends to say (in the form of a learned template) and how it is attempting to say it (in the form of an instantiated template).
",1 Introduction,[0],[0]
"We show that we can achieve performance competitive with other neural NLG approaches, while making progress satisfying the above two desiderata.",1 Introduction,[0],[0]
"Concretely, our experiments indicate that we can induce explicit templates (as shown in Figure 1) while achieving competitive automatic scores, and that we can control and interpret our generations by manipulating these templates.",1 Introduction,[0],[0]
"Finally, while our experiments focus on the data-to-text regime, we believe the proposed methodology represents a compelling approach to learning discrete, latent-variable representations of conditional text.",1 Introduction,[0],[0]
A core task of NLG is to generate textual descriptions of knowledge base records.,2 Related Work,[0],[0]
"A common approach is to use hand-engineered templates (Kukich, 1983; McKeown, 1992; McRoy et al., 2000), but there has also been interest in creating templates in an automated manner.",2 Related Work,[0],[0]
"For instance, many authors induce templates by clustering sentences and then abstracting templated fields with hand-engineered rules (Angeli et al., 2010; Kondadadi et al., 2013; Howald et al., 2013), or with a pipeline of other automatic approaches (Wang and Cardie, 2013).
",2 Related Work,[0],[0]
"There has also been work in incorporating probabilistic notions of templates into generation models (Liang et al., 2009; Konstas and Lapata, 2013), which is similar to our approach.",2 Related Work,[0],[0]
"However, these approaches have always been conjoined with discriminative classifiers or rerankers in order to actually accomplish the generation (Angeli et al., 2010; Konstas and Lapata, 2013).",2 Related Work,[0],[0]
"In addition, these models explicitly model knowledge base field selection, whereas the model we present is fundamentally an end-to-end model over generation segments.
",2 Related Work,[0],[0]
"Recently, a new paradigm has emerged around neural text generation systems based on machine translation (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015).",2 Related Work,[0],[0]
Most of this work has used unconstrained black-box encoderdecoder approaches.,2 Related Work,[0],[0]
"There has been some work on discrete variables in this context, including extracting representations (Shen et al., 2018), incorporating discrete latent variables in text modeling (Yang et al., 2018), and using non-HSMM segmental models for machine translation or summarization (Yu et al., 2016; Wang et al., 2017; Huang et al., 2018).",2 Related Work,[0],[0]
"Dai et al. (2017) develop an approximate inference scheme for a neural HSMM using RNNs for continuous emissions; in contrast we maximize the exact log-marginal, and use RNNs to parameterize a discrete emission distribution.",2 Related Work,[0],[0]
"Finally, there has also been much recent interest in segmental RNN models for non-generative tasks in NLP (Tang et al., 2016; Kong et al., 2016; Lu et al., 2016).
",2 Related Work,[0],[0]
"The neural text generation community has also recently been interested in “controllable” text generation (Hu et al., 2017), where various aspects of the text (often sentiment) are manipulated or transferred (Shen et al., 2017; Zhao et al., 2018; Li et al., 2018).",2 Related Work,[0],[0]
"In contrast, here we focus on controlling either the content of a generation or the way it is expressed by manipulating the (latent) template used in realizing the generation.",2 Related Work,[0],[0]
Our focus is on generating a textual description of a knowledge base or meaning representation.,3 Overview: Data-Driven NLG,[0],[0]
"Following standard notation (Liang et al., 2009; Wiseman et al., 2017), let x= {r1 . . .",3 Overview: Data-Driven NLG,[0],[0]
rJ} be a collection of records.,3 Overview: Data-Driven NLG,[0],[0]
"A record is made up of a type (r.t), an entity (r.e), and a value (r.m).",3 Overview: Data-Driven NLG,[0],[0]
"For example, a knowledge base of restaurants might have a record with r.t = Cuisine, r.e = Denny’s, and r.m = American.",3 Overview: Data-Driven NLG,[0],[0]
"The aim is to generate an adequate and fluent text description ŷ1:T = ŷ1, . . .",3 Overview: Data-Driven NLG,[0],[0]
", ŷT of x. Concretely, we consider the E2E Dataset (Novikova et al., 2017) and the WikiBio Dataset (Lebret et al., 2016).",3 Overview: Data-Driven NLG,[0],[0]
We show an example E2E knowledge base x in the top of Figure 1.,3 Overview: Data-Driven NLG,[0],[0]
"The top of Figure 2 shows an example knowledge base x from the WikiBio dataset, where it is paired with a reference text y= y1:T at the bottom.
",3 Overview: Data-Driven NLG,[0],[0]
"The dominant approach in neural NLG has been
3176
sopoulos, 2007; Turner et al., 2010).",3 Overview: Data-Driven NLG,[0],[0]
"Generation is divided into modular, yet highly interdependent, decisions: (1) content planning defines which parts of the input fields or meaning representations should be selected; (2) sentence planning determines which selected fields are to be dealt with in each output sentence; and (3) surface realization generates those sentences.
",3 Overview: Data-Driven NLG,[0],[0]
Data-driven approaches have been proposed to automatically learn the individual modules.,3 Overview: Data-Driven NLG,[0],[0]
"One approach first aligns records and sentences and then learns a content selection model (Duboue and McKeown, 2002; Barzilay and Lapata, 2005).",3 Overview: Data-Driven NLG,[0],[0]
"Hierarchical hidden semi-Markov generative models have also been used to first determine which facts to discuss and then to generate words from the predicates and arguments of the chosen facts (Liang et al., 2009).",3 Overview: Data-Driven NLG,[0],[0]
"Sentence planning has been formulated as a supervised set partitioning problem over facts where each partition corresponds to a sentence (Barzilay and Lapata, 2006).",3 Overview: Data-Driven NLG,[0],[0]
"End-to-end approaches have combined sentence planning and surface realization by using explicitly aligned sentence/meaning pairs as training data (Ratnaparkhi, 2002; Wong and Mooney, 2007; Belz, 2008; Lu and Ng, 2011).",3 Overview: Data-Driven NLG,[0],[0]
"More recently, content selection and surface realization have been combined (Angeli et al., 2010; Kim and Mooney, 2010; Konstas and Lapata, 2013).
",3 Overview: Data-Driven NLG,[0],[0]
"At the intersection of rule-based and statistical methods, hybrid systems aim at leveraging human contributed rules and corpus statistics (Langkilde and Knight, 1998; Soricut and Marcu, 2006; Mairesse and Walker, 2011).
",3 Overview: Data-Driven NLG,[0],[0]
"Our approach is inspired by the recent success of neural language models for image captioning (Kiros et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Fang et al., 2015; Xu et al., 2015), machine translation (Devlin et al., 2014; Bahdanau et al., 2015; Luong et al., 2015), and modeling conversations and dialogues (Shang et al., 2015; Wen et al., 2015; Yao et al., 2015).
",3 Overview: Data-Driven NLG,[0],[0]
Our model is most similar to Mei et al. (2016) who use an encoder-decoder style neural network model to tackle the WEATHERGOV and ROBOCUP tasks.,3 Overview: Data-Driven NLG,[0],[0]
"Their architecture relies on LSTM units and an attention mechanism which reduces scalability compared to our simpler design.
",3 Overview: Data-Driven NLG,[0],[0]
Figure 1: Wikipedia infobox of Frederick Parker-Rhodes.,3 Overview: Data-Driven NLG,[0],[0]
"The introduction of his article reads: “Frederick Parker-Rhodes (21 March 1914 – 21 November 1987) was an English linguist,
plant pathologist, computer scientist, mathematician, mystic, and mycologist.”.
",3 Overview: Data-Driven NLG,[0],[0]
"3 Language Modeling for Constrained Sentence generation
Conditional language models are a popular choice to generate sentences.",3 Overview: Data-Driven NLG,[0],[0]
"We introduce a tableconditioned language model for constraining text generation to include elements from fact tables.
3.1 Language model Given a sentence s = w1, . . .",3 Overview: Data-Driven NLG,[0],[0]
", wT with T words from vocabularyW , a language model estimates:
P (s) =
T∏
t=1
P (wt|w1, . . .",3 Overview: Data-Driven NLG,[0],[0]
", wt−1) .",3 Overview: Data-Driven NLG,[0],[0]
"(1)
Let ct = wt−(n−1), . . .",3 Overview: Data-Driven NLG,[0],[0]
", wt−1 be the sequence of n",3 Overview: Data-Driven NLG,[0],[0]
− 1 context words preceding wt.,3 Overview: Data-Driven NLG,[0],[0]
"An n-gram language model makes an order n Markov assumption,
P (s)",3 Overview: Data-Driven NLG,[0],[0]
"≈ T∏
t=1
P (wt|ct) .",3 Overview: Data-Driven NLG,[0],[0]
"(2)
3.2 Language model conditioned on tables A table is a set of field/value pairs, where values are sequences of words.",3 Overview: Data-Driven NLG,[0],[0]
"We therefore propose language models that are conditioned on these pairs.
",3 Overview: Data-Driven NLG,[0],[0]
"Local conditioning refers to the information from the table that is applied to the description of the words which have already generated, i.e. the previous words that constitute the context of the language
Frederick Parker-Rhodes (21 March 1914 - 21 November
1987) was an English linguist, plant pathologist, computer
scientist, mathematician, mystic, and mycologist.
",3 Overview: Data-Driven NLG,[0],[0]
"Figure 2: An example from the WikiBio dataset (Lebret et al., 2016), with a database x (top) for Frederick ParkerRhodes and corresponding reference generation y (bottom).
to use an encoder network over x and then a conditio al decoder network to generate y, training the whole system in an end-to-end manner.",3 Overview: Data-Driven NLG,[0],[0]
"To generate a description for a given example, a black-box network (such as an RNN) is used to produce a distribution over the n xt w rd, from which a choice is made and fed back into the system.",3 Overview: Data-Driven NLG,[0],[0]
"The entire distribution is driven by the internal states of the neural network.
",3 Overview: Data-Driven NLG,[0],[0]
"While effective, relying on a neural decoder makes it difficult to understand what aspects of x are correlated with a particular system output.",3 Overview: Data-Driven NLG,[0],[0]
"This leads to problems both in controlling finegrained aspects of the generation process and in interpreting model mistakes.
",3 Overview: Data-Driven NLG,[0],[0]
"As an example of why controllability is important, consider the records in Figure 1.",3 Overview: Data-Driven NLG,[0],[0]
"Given these inputs an end-user might ant to gene te an output meeting specific co straints, such as not mentioning any information relating to customer rating.",3 Overview: Data-Driven NLG,[0],[0]
"Under a standard encoder-decoder style model, one could filter out this information either from the encoder or decoder, but in practice this would lead to unexpected changes in output that might propagate through the whole system.
",3 Overview: Data-Driven NLG,[0],[0]
"As n xampl of the difficulty of interpreting mistakes, consider following actual generation from an encoder-decoder style system for
the records in Figure 2: ”frederick parker-rhodes (21 november 1914 - 2 march 1987) was an english mycology and plant pathology, mathematics at the university of uk.”",3 Overview: Data-Driven NLG,[0],[0]
"In addition to not being fluent, it is unclear what the end of this sentence is even attempting to convey: it may be attempting to convey a fact not actually in the knowledge base (e.g., where Parker-Rhodes studied), or perhaps it is simply failing to fluently realize information that is in the knowledge base (e.g., ParkerRhodes’s country of residence).
",3 Overview: Data-Driven NLG,[0],[0]
"Traditional NLG systems (Kukich, 1983; McKeown, 1992; Belz, 2008; Gatt and Reiter, 2009), in contrast, largely avoid these problems.",3 Overview: Data-Driven NLG,[0],[0]
"Since they typically employ an explicit planning component, which decides which knowledge base records to
focus on, and a surface realization component,
which realizes the chosen records, the intent of the
ystem is always explicit, and it may be modified
to meet constraints.",3 Overview: Data-Driven NLG,[0],[0]
The goal of this work is to propose an approach to neural NLG that addresses these issues in a principled way.,3 Overview: Data-Driven NLG,[0],[0]
We target this goal by proposing a new model that generates with template-like objects induced by a neural HSMM (see Figure 1).,3 Overview: Data-Driven NLG,[0],[0]
"Templates are useful here because they represent a fixed plan for the generation’s content, and because they make it clear what part of the generation is associated with which record in the knowledge base.",3 Overview: Data-Driven NLG,[0],[0]
What does it mean to learn a template?,4 Background: Semi-Markov Models,[0],[0]
"It is natural to think of a template as a sequence of typed text-segments, perhaps with some segments acting as the template’s “backbone” (Wang and Cardie, 2013), and the remaining segments filled in from the knowledge base.
",4 Background: Semi-Markov Models,[0],[0]
"A natural probabilistic model conforming with this intuition is the hidden semi-markov model (HSMM) (Gales and Young, 1993; Ostendorf et al., 1996), which models latent segmentations in an output sequence.",4 Background: Semi-Markov Models,[0],[0]
"Informally, an HSMM is much like an HMM, except emissions may last multiple time-steps, and multi-step emissions need not be independent of each other conditioned on the state.
",4 Background: Semi-Markov Models,[0],[0]
We briefly review HSMMs following Murphy (2002).,4 Background: Semi-Markov Models,[0],[0]
Assume we have a sequence of obs rved tokens y1 . . .,4 Background: Semi-Markov Models,[0],[0]
"yT and a discrete, latent state zt ∈{1, . .",4 Background: Semi-Markov Models,[0],[0]
.,4 Background: Semi-Markov Models,[0],[0]
",K} for each timestep.",4 Background: Semi-Markov Models,[0],[0]
"We addition-
ally use two per-timestep variables to model multistep segments: a length variable lt ∈{1, . . .",4 Background: Semi-Markov Models,[0],[0]
", L} specifying the length of the current segment, and a deterministic binary variable ft indicating whether a segment finishes at time t. We will consider in particular conditional HSMMs, which condition on a source x, essentially giving us an HSMM decoder.
",4 Background: Semi-Markov Models,[0],[0]
An HSMM specifies a joint distribution on the observations and latent segmentations.,4 Background: Semi-Markov Models,[0],[0]
"Letting θ denote all the parameters of the model, and using the variables introduced above, we can write the corresponding joint-likelihood as follows
p(y, z, l, f |x; θ) = T−1∏ t=0 p(zt+1, lt+1 | zt, lt, x)ft
× T∏ t=1 p(yt−lt+1:",4 Background: Semi-Markov Models,[0],[0]
"t | zt, lt, x)ft ,
where we take z0 to be a distinguished startstate, and the deterministic ft variables are used for excluding non-segment log probabilities.",4 Background: Semi-Markov Models,[0],[0]
"We further assume p(zt+1, lt+1 | zt, lt, x) factors as p(zt+1 | zt, x) × p(lt+1 | zt+1).",4 Background: Semi-Markov Models,[0],[0]
"Thus, the likelihood is given by the product of the probabilities of each discrete state transition made, the probability of the length of each segment given its discrete state, and the probability of the observations in each segment, given its state and length.",4 Background: Semi-Markov Models,[0],[0]
"We use a novel, neural parameterization of an HSMM to specify the probabilities in the likelihood above.",5 A Neural HSMM Decoder,[0],[0]
"This full model, sketched out in Figure 3, allows us to incorporate the modeling components, such as LSTMs and attention, that make neural text generation effective, while maintaining the HSMM structure.",5 A Neural HSMM Decoder,[0],[0]
"Since our model must condition on x, let rj ∈Rd represent a real embedding of record rj ∈x, and let xa ∈Rd represent a real embedding of the entire knowledge base x, obtained by max-pooling coordinate-wise over all the rj .",5.1 Parameterization,[0],[0]
"It is also useful to have a representation of just the unique types of records that appear in x, and so we also define xu ∈Rd to be the sum of the embeddings of the unique types appearing in x, plus a bias vector and followed by a ReLU nonlinearity.
",5.1 Parameterization,[0],[0]
"Transition Distribution The transition distribution p(zt+1 | zt, x) may be viewed as aK ×K matrix of probabilities, where each row sums to 1.",5.1 Parameterization,[0],[0]
"We define this matrix to be
p(zt+1 | zt, x) ∝",5.1 Parameterization,[0],[0]
"AB +C(xu)D(xu),
where A∈RK×m1 , B ∈Rm1×K are state embeddings, and where C : Rd → RK×m2 and D :",5.1 Parameterization,[0],[0]
Rd → Rm2×K are parameterized non-linear functions of xu.,5.1 Parameterization,[0],[0]
"We apply a row-wise softmax to the resulting matrix to obtain the desired probabilities.
",5.1 Parameterization,[0],[0]
"Length Distribution We simply fix all length probabilities p(lt+1 | zt+1) to be uniform up to a maximum length L.1
Emission Distribution The emission model models the generation of a text segment conditioned on a latent state and source information, and so requires a richer parameterization.",5.1 Parameterization,[0],[0]
"Inspired by the models used for neural NLG, we base this model on an RNN decoder, and write a segment’s probability as a product over token-level probabilities,
p(yt−lt+1:",5.1 Parameterization,[0],[0]
"t | zt= k, lt=",5.1 Parameterization,[0],[0]
"l, x) = lt∏ i=1",5.1 Parameterization,[0],[0]
p(yt−lt+i |,5.1 Parameterization,[0],[0]
"yt−lt+1:t−lt+i−1, zt= k, x)
×",5.1 Parameterization,[0],[0]
p(</seg> | yt−lt+1:,5.1 Parameterization,[0],[0]
"t, zt= k, x)× 1{lt = l},
1We experimented with parameterizing the length distribution, but found that it led to inferior performance.",5.1 Parameterization,[0],[0]
"Forcing the length probabilities to be uniform encourages the model to cluster together functionally similar emissions of different lengths, while parameterizing them can lead to states that specialize to specific emission lengths.
where </seg> is an end of segment token.",5.1 Parameterization,[0],[0]
"The RNN decoder uses attention and copy-attention over the embedded records rj , and is conditioned on zt= k by concatenating an embedding corresponding to the k’th latent state to the RNN’s input; the RNN is also conditioned on the entire x by initializing its hidden state with xa.
More concretely, let hki−1 ∈Rd be the state of an RNN conditioned on x and zt= k (as above) run over the sequence yt−lt+1:t−lt+i−1.",5.1 Parameterization,[0],[0]
"We let the model attend over records rj using hki−1 (in the style of Luong et al. (2015)), producing a context vector cki−1.",5.1 Parameterization,[0],[0]
"We may then obtain scores vi−1 for each word in the output vocabulary,
vi−1=W tanh(g k 1 ◦",5.1 Parameterization,[0],[0]
"[hki−1, cki−1]),
with parameters gk1 ∈R2d and W ∈RV×2d.",5.1 Parameterization,[0],[0]
Note that there is a gk1 vector for each of K discrete states.,5.1 Parameterization,[0],[0]
"To additionally implement a kind of slot filling, we allow emissions to be directly copied from the value portion of the records rj using copy attention (Gülçehre et al., 2016; Gu et al., 2016; Yang et al., 2016).",5.1 Parameterization,[0],[0]
"Define copy scores,
ρj = r T j tanh(g k 2 ◦ hki−1),
where gk2 ∈Rd.",5.1 Parameterization,[0],[0]
"We then normalize the outputvocabulary and copy scores together, to arrive at
ṽi−1=softmax([vi−1, ρ1, . . .",5.1 Parameterization,[0],[0]
", ρJ ]),
and thus
p(yt−lt+i=w | yt−lt+1:t−lt+i−1, zt= k, x)",5.1 Parameterization,[0],[0]
"= ṽi−1,w + ∑
j:rj .m=w
ṽi−1,V+j .
",5.1 Parameterization,[0],[0]
An Autoregressive Variant The model as specified assumes segments are independent conditioned on the associated latent state and x.,5.1 Parameterization,[0],[0]
"While this assumption still allows for reasonable performance, we can tractably allow interdependence between tokens (but not segments) by having each next-token distribution depend on all the previously generated tokens, giving us an autoregressive HSMM.",5.1 Parameterization,[0],[0]
"For this model, we will in fact use p(yt−lt+i=w | y1:t−lt+i−1, zt= k, x) in defining our emission model, which is easily implemented by using an additional RNN run over all the preceding tokens.",5.1 Parameterization,[0],[0]
We will report scores for both non-autoregressive and autoregressive HSMM decoders below.,5.1 Parameterization,[0],[0]
The model requires fitting a large set of neural network parameters.,5.2 Learning,[0],[0]
"Since we assume z, l, and f are unobserved, we marginalize over these variables to maximize the log marginal-likelihood of the observed tokens y given x. The HSMM marginal-likelihood calculation can be carried out efficiently with a dynamic program analogous to either the forward- or backward-algorithm familiar from HMMs (Rabiner, 1989).
",5.2 Learning,[0],[0]
"It is actually more convenient to use the backward-algorithm formulation when using RNNs to parameterize the emission distributions, and we briefly review the backward recurrences here, again following Murphy (2002).",5.2 Learning,[0],[0]
"We have:
βt(j) = p(yt+1:T | zt= j, ft=1, x)
= K∑ k=1 β∗t",5.2 Learning,[0],[0]
"(k) p(zt+1= k | zt = j)
β∗t (k) = p(yt+1:T | zt+1 = k, ft = 1, x)
= L∑ l=1",5.2 Learning,[0],[0]
[ βt+l(k),5.2 Learning,[0],[0]
"p(lt+1= l | zt+1= k)
p(yt+1:t+l | zt+1= k, lt+1= l) ] ,
with base case βT (j)= 1.",5.2 Learning,[0],[0]
"We can now obtain the marginal probability of y as p(y |x)= ∑K k=1 β ∗ 0(k) p(z1= k), where we have used the fact that f0 must be 1, and we therefore train to maximize the log-marginal likelihood of the observed y:
ln p(y |x; θ) = ln K∑ k=1 β∗0(k) p(z1= k).",5.2 Learning,[0],[0]
"(1)
Since the quantities in (1) are obtained from a dynamic program, which is itself differentiable, we may simply maximize with respect to the parameters θ by back-propagating through the dynamic program; this is easily accomplished with automatic differentiation packages, and we use pytorch (Paszke et al., 2017) in all experiments.",5.2 Learning,[0],[0]
"After training, we could simply condition on a new database and generate with beam search, as is standard with encoder-decoder models.",5.3 Extracting Templates and Generating,[0],[0]
"However, the structured approach we have developed allows us to generate in a more template-like way, giving us more interpretable and controllable generations.
",5.3 Extracting Templates and Generating,[0],[0]
[The Golden Palace]55 [is a]59 [coffee shop]12,5.3 Extracting Templates and Generating,[0],[0]
[providing]3 [Indian]50 [food]1 [in the]17 [£20- 25]26,5.3 Extracting Templates and Generating,[0],[0]
[price range]16,5.3 Extracting Templates and Generating,[0],[0]
[.]2 [It is]8 [located in the]25,5.3 Extracting Templates and Generating,[0],[0]
"[riverside]40 [.]53 [Its customer rating is]19 [high]23 [.]2
Figure 4: A sample Viterbi segmentation of a training text; subscripted numbers indicate the corresponding latent state.",5.3 Extracting Templates and Generating,[0],[0]
"From this we can extract a template with S=17 segments; compare with the template used at the bottom of Figure 1.
",5.3 Extracting Templates and Generating,[0],[0]
"First, note that given a database x and reference generation y we can obtain the MAP assignment to the variables z, l, and f with a dynamic program similar to the Viterbi algorithm familiar from HMMs.",5.3 Extracting Templates and Generating,[0],[0]
"These assignments will give us a typed segmentation of y, and we show an example Viterbi segmentation of some training text in Figure 4.",5.3 Extracting Templates and Generating,[0],[0]
"Computing MAP segmentations allows us to associate text-segments (i.e., phrases) with the discrete labels zt that frequently generate them.",5.3 Extracting Templates and Generating,[0],[0]
"These MAP segmentations can be used in an exploratory way, as a sort of dimensionality reduction of the generations in the corpus.",5.3 Extracting Templates and Generating,[0],[0]
"More importantly for us, however, they can also be used to guide generation.
",5.3 Extracting Templates and Generating,[0],[0]
"In particular, since each MAP segmentation implies a sequence of hidden states z, we may run a template extraction step, where we collect the most common “templates” (i.e., sequences of hidden states) seen in the training data.",5.3 Extracting Templates and Generating,[0],[0]
"Each “template” z(i) consists of a sequence of latent states, with z(i)= z(i)1 , . . .",5.3 Extracting Templates and Generating,[0],[0]
"z (i) S representing the S distinct segments in the i’th extracted template (recall that we will technically have a zt for each time-step, and so z(i) is obtained by collapsing adjacent zt’s with the same value); see Figure 4 for an example template (with S=17) that can be extracted from the E2E corpus.",5.3 Extracting Templates and Generating,[0],[0]
"The bottom of Figure 1 shows a visualization of this extracted template, where discrete states are replaced by the phrases they frequently generate in the training data.
",5.3 Extracting Templates and Generating,[0],[0]
"With our templates z(i) in hand, we can then restrict the model to using (one of) them during generation.",5.3 Extracting Templates and Generating,[0],[0]
"In particular, given a new input x, we may generate by computing
ŷ(i)",5.3 Extracting Templates and Generating,[0],[0]
=,5.3 Extracting Templates and Generating,[0],[0]
"argmax y′ p(y′, z(i) |x), (2)
which gives us a generation ŷ(i) for each extracted template z(i).",5.3 Extracting Templates and Generating,[0],[0]
"For example, the generation in Figure 1 is obtained by maximizing (2) with x set to the database in Figure 1 and z(i) set to the template
extracted in Figure 4.",5.3 Extracting Templates and Generating,[0],[0]
"In practice, the argmax in (2) will be intractable to calculate exactly due to the use of RNNs in defining the emission distribution, and so we approximate it with a constrained beam search.",5.3 Extracting Templates and Generating,[0],[0]
"This beam search looks very similar to that typically used with RNN decoders, except the search occurs only over a segment, for a particular latent state k.",5.3 Extracting Templates and Generating,[0],[0]
"Returning to the discussion of controllability and interpretability, we note that with the proposed model (a) it is possible to explicitly force the generation to use a chosen template z(i), which is itself automatically learned from training data, and (b) that every segment in the generated ŷ(i) is typed by its corresponding latent variable.",5.4 Discussion,[0],[0]
"We explore these issues empirically in Section 7.1.
",5.4 Discussion,[0],[0]
"We also note that these properties may be useful for other text applications, and that they offer an additional perspective on how to approach latent variable modeling for text.",5.4 Discussion,[0],[0]
"Whereas there has been much recent interest in learning continuous latent variable representations for text (see Section 2), it has been somewhat unclear what the latent variables to be learned are intended to capture.",5.4 Discussion,[0],[0]
"On the other hand, the latent, template-like structures we induce here represent a plausible, probabilistic latent variable story, and allow for a more controllable method of generation.
",5.4 Discussion,[0],[0]
"Finally, we highlight one significant possible issue with this model – the assumption that segments are independent of each other given the corresponding latent variable and x.",5.4 Discussion,[0],[0]
Here we note that the fact that we are allowed to condition on x is quite powerful.,5.4 Discussion,[0],[0]
"Indeed, a clever encoder could capture much of the necessary interdependence between the segments to be generated (e.g., the correct determiner for an upcoming noun phrase) in its encoding, allowing the segments themselves to be decoded more or less independently, given x.",5.4 Discussion,[0],[0]
"Our experiments apply the approach outlined above to two recent, data-driven NLG tasks.",6 Data and Methods,[0],[0]
"Experiments use the E2E (Novikova et al., 2017) and WikiBio (Lebret et al., 2016) datasets, examples of which are shown in Figures 1 and 2, respectively.",6.1 Datasets,[0],[0]
"The former dataset, used for the
2018 E2E-Gen Shared Task, contains approximately 50K total examples, and uses 945 distinct word types, and the latter dataset contains approximately 500K examples and uses approximately 400K word types.",6.1 Datasets,[0],[0]
"Because our emission model uses a word-level copy mechanism, any record with a phrase consisting of n words as its value is replaced with n positional records having a single word value, following the preprocessing of Lebret et al. (2016).",6.1 Datasets,[0],[0]
"For example, “type[coffee shop]” in Figure 1 becomes “type-1[coffee]” and “type2[shop].”
",6.1 Datasets,[0],[0]
"For both datasets we compare with published encoder-decoder models, as well as with direct template-style baselines.",6.1 Datasets,[0],[0]
"The E2E task is evaluated in terms of BLEU (Papineni et al., 2002), NIST (Belz and Reiter, 2006), ROUGE (Lin, 2004), CIDEr (Vedantam et al., 2015), and METEOR (Banerjee and Lavie, 2005).2",6.1 Datasets,[0],[0]
"The benchmark system for the task is an encoder-decoder style system followed by a reranker, proposed by Dušek and Jurcıcek (2016).",6.1 Datasets,[0],[0]
"We compare to this baseline, as well as to a simple but competitive non-parametric template-like baseline (“SUB” in tables), which selects a training sentence with records that maximally overlap (without including extraneous records) the unseen set of records we wish to generate from; ties are broken at random.",6.1 Datasets,[0],[0]
"Then, word-spans in the chosen training sentence are aligned with records by string-match, and replaced with the corresponding fields of the new set of records.3
The WikiBio dataset is evaluated in terms of BLEU, NIST, and ROUGE, and we compare with the systems and baselines implemented by Lebret et al. (2016), which include two neural, encoderdecoder style models, as well as a Kneser-Ney, templated baseline.",6.1 Datasets,[0],[0]
"We first emphasize two additional methodological details important for obtaining good performance.
",6.2 Model and Training Details,[0],[0]
Constraining Learning,6.2 Model and Training Details,[0],[0]
We were able to learn more plausible segmentations of y by constraining the model to respect word spans yt+1:t+l that appear in some record rj ∈x.,6.2 Model and Training Details,[0],[0]
"We accomplish this by giving zero probability (within the backward re-
2We use the official E2E NLG Challenge scoring scripts at https://github.com/tuetschek/e2e-metrics.
",6.2 Model and Training Details,[0],[0]
"3For categorical records, like “familyFriendly”, which cannot easily be aligned with a phrase, we simply select only candidate training sentences with the same categorical value.
currences in Section 5) to any segmentation that splits up a sequence yt+1:t+l that appears in some rj , or that includes yt+1:t+l as a subsequence of another sequence.",6.2 Model and Training Details,[0],[0]
"Thus, we maximize (1) subject to these hard constraints.
",6.2 Model and Training Details,[0],[0]
"Increasing the Number of Hidden States While a larger K allows for a more expressive latent model, computing K emission distributions over the vocabulary can be prohibitively expensive.",6.2 Model and Training Details,[0],[0]
"We therefore tie the emission distribution between multiple states, while allowing them to have a different transition distributions.
",6.2 Model and Training Details,[0],[0]
"We give additional architectural details of our model in the Supplemental Material; here we note that we use an MLP to embed rj ∈Rd, and a 1- layer LSTM (Hochreiter and Schmidhuber, 1997) in defining our emission distributions.",6.2 Model and Training Details,[0],[0]
"In order to reduce the amount of memory used, we restrict our output vocabulary (and thus the height of the matrix W in Section 5) to only contain words in y that are not present in x; any word in y present in x is assumed to be copied.",6.2 Model and Training Details,[0],[0]
"In the case where a word yt appears in a record rj (and could therefore have been copied), the input to the LSTM at time t+1 is computed using information from rj ; if there are multiple rj from which yt could have been copied, the computed representations are simply averaged.
",6.2 Model and Training Details,[0],[0]
"For all experiments, we set d=300 and L=4.",6.2 Model and Training Details,[0],[0]
"At generation time, we select the 100 most common templates z(i), perform beam search with a beam of size 5, and select the generation with the highest overall joint probability.
",6.2 Model and Training Details,[0],[0]
"For our E2E experiments, our best nonautoregressive model has 55 “base” states, duplicated 5 times, for a total of K =275 states, and our best autoregressive model uses K =60 states, without any duplication.",6.2 Model and Training Details,[0],[0]
"For our WikiBio experiments, both our best non-autoregressive and autoregressive models uses 45 base states duplicated 3 times, for a total of K =135 states.",6.2 Model and Training Details,[0],[0]
"In all cases, K was chosen based on BLEU performance on held-out validation data.",6.2 Model and Training Details,[0],[0]
Code implementing our models is available at https://github.com/ harvardnlp/neural-template-gen.,6.2 Model and Training Details,[0],[0]
Our results on automatic metrics are shown in Tables 1 and 2.,7 Results,[0],[0]
"In general, we find that the templated baselines underperform neural models, whereas our proposed model is fairly competitive with neural models, and sometimes even out-
performs them.",7 Results,[0],[0]
"On the E2E data, for example, we see in Table 1 that the SUB baseline, despite having fairly impressive performance for a nonparametric model, fares the worst.",7 Results,[0],[0]
"The neural HSMM models are largely competitive with the encoder-decoder system on the validation data, despite offering the benefits of interpretability and controllability; however, the gap increases on test.
",7 Results,[0],[0]
"Table 2 evaluates our system’s performance on the test portion of the WikiBio dataset, comparing with the systems and baselines implemented by Lebret et al. (2016).",7 Results,[0],[0]
"Again for this dataset we see that their templated Kneser-Ney model underperforms on the automatic metrics, and that neural models improve on these results.",7 Results,[0],[0]
"Here the HSMMs are competitive with the best model of Lebret et al. (2016), and even outperform it on ROUGE.",7 Results,[0],[0]
"We emphasize, however, that recent, sophisticated approaches to encoder-decoder style
Travellers Rest Beefeater
name[Travellers Rest Beefeater], customerRating[3 out of 5], area[riverside], near[Raja Indian Cuisine]
1.",7 Results,[0],[0]
[Travellers Rest Beefeater]55,7 Results,[0],[0]
[is a]59 [3 star]43 [restaurant]11 [located near]25 [,7 Results,[0],[0]
Raja Indian Cuisine]40,7 Results,[0],[0]
[.]53 2.,7 Results,[0],[0]
"[Near]31 [riverside]29 [,]44 [Travellers Rest Beefeater]55",7 Results,[0],[0]
[serves]3 [3 star]50,7 Results,[0],[0]
[food]1 [.]2 3.,7 Results,[0],[0]
[Travellers Rest Beefeater]55,7 Results,[0],[0]
[is a]59,7 Results,[0],[0]
[restaurant]12 [providing]3,7 Results,[0],[0]
[riverside]50 [food]1,7 Results,[0],[0]
[and has a]17 [3 out of 5]26 [customer rating]16,7 Results,[0],[0]
[.]2 [It is]8 [near]25 [Raja Indian Cuisine]40,7 Results,[0],[0]
[.]53 4.,7 Results,[0],[0]
[Travellers Rest Beefeater]55,7 Results,[0],[0]
[is a]59 [place to eat]12 [located near]25 [Raja Indian Cuisine]40,7 Results,[0],[0]
[.]53 5.,7 Results,[0],[0]
[Travellers Rest Beefeater]55,7 Results,[0],[0]
[is a]59 [3 out of 5]5,7 Results,[0],[0]
[rated]32 [riverside]43 [restaurant]11 [near]25 [Raja Indian Cuisine]40,7 Results,[0],[0]
"[.]53
Table 3: Impact of varying the template z(i) for a single x from the E2E validation data; generations are annotated with the segmentations of the chosen z(i).",7 Results,[0],[0]
"Results were obtained using the NTemp+AR model from Table 1.
database-to-text generation have since surpassed the results of Lebret et al. (2016) and our own, and we show the recent seq2seq style results of Liu et al. (2018), who use a somewhat larger model, at the bottom of Table 2.",7 Results,[0],[0]
"We now qualitatively demonstrate that our generations are controllable and interpretable.
",7.1 Qualitative Evaluation,[0],[0]
"Controllable Diversity One of the powerful aspects of the proposed approach to generation is that we can manipulate the template z(i) while leaving the database x constant, which allows for easily controlling aspects of the generation.",7.1 Qualitative Evaluation,[0],[0]
"In Table 3 we show the generations produced by our model for five different neural template sequences z(i), while fixing x. There, the segments in each generation are annotated with the latent states determined by the corresponding z(i).",7.1 Qualitative Evaluation,[0],[0]
"We see that these templates can be used to affect the wordordering, as well as which fields are mentioned in the generated text.",7.1 Qualitative Evaluation,[0],[0]
"Moreover, because the discrete states align with particular fields (see below), it is generally simple to automatically infer to which fields particular latent states correspond, allowing users to choose which template best meets their requirements.",7.1 Qualitative Evaluation,[0],[0]
"We emphasize that this level of controllability is much harder to obtain for encoderdecoder models, since, at best, a large amount of sampling would be required to avoid generating around a particular mode in the conditional distribution, and even then it would be difficult to control the sort of generations obtained.
",7.1 Qualitative Evaluation,[0],[0]
"Interpretable States Discrete states also provide a method for interpreting the generations produced by the system, since each segment is explicitly typed by the current hidden state of the model.",7.1 Qualitative Evaluation,[0],[0]
Table 4 shows the impact of varying the template z(i) for a single x from the WikiBio dataset.,7.1 Qualitative Evaluation,[0],[0]
"While there is in general surprisingly little stylistic variation in the WikiBio data itself, there is variation in the information discussed, and the templates capture this.",7.1 Qualitative Evaluation,[0],[0]
"Moreover, we see that particular discrete states correspond in a consistent way to particular pieces of information, allowing us to align states with particular field types.",7.1 Qualitative Evaluation,[0],[0]
"For instance, birth names have the same hidden state (132), as do names (117), nationalities (82), birth dates (101), and occupations (20).
",7.1 Qualitative Evaluation,[0],[0]
"To demonstrate empirically that the learned states indeed align with field types, we calculate the average purity of the discrete states learned for both datasets in Table 5.",7.1 Qualitative Evaluation,[0],[0]
"In particular, for each discrete state for which the majority of its generated words appear in some rj , the purity of a state’s record type alignment is calculated as the percentage of the state’s words that come from the most frequent record type the state represents.",7.1 Qualitative Evaluation,[0],[0]
This calculation was carried out over training examples that belonged to one of the top 100 most frequent templates.,7.1 Qualitative Evaluation,[0],[0]
Table 5 indicates that discrete states learned on the E2E data are quite pure.,7.1 Qualitative Evaluation,[0],[0]
"Discrete states learned on the WikiBio data are less pure, though still rather impressive given that there are approximately 1700 record types represented in the WikiBio data, and we limit the number of states to 135.",7.1 Qualitative Evaluation,[0],[0]
"Unsurprisingly, adding autoregressiveness to the model decreases purity on both datasets, since the model may rely on the autoregressive RNN for typing, in addition to the state’s identity.",7.1 Qualitative Evaluation,[0],[0]
"We have developed a neural, template-like generation model based on an HSMM decoder, which can be learned tractably by backpropagating through a dynamic program.",8 Conclusion and Future Work,[0],[0]
"The method allows us to extract template-like latent objects in a principled way in the form of state sequences, and then generate with them.",8 Conclusion and Future Work,[0],[0]
This approach scales to large-scale text datasets and is nearly competitive with encoder-decoder models.,8 Conclusion and Future Work,[0],[0]
"More importantly, this approach allows for controlling the diversity of generation and for producing interpretable states during generation.",8 Conclusion and Future Work,[0],[0]
"We view this work both as the first step towards learning discrete latent variable template models for more difficult generation tasks, as well as a different perspective on learning latent variable text models in general.",8 Conclusion and Future Work,[0],[0]
"Future work will examine encouraging the model to learn maximally different (or minimal) templates, which our objective does not explicitly encourage, templates of larger textual phenomena, such as paragraphs and documents, and hierarchical templates.",8 Conclusion and Future Work,[0],[0]
SW gratefully acknowledges the support of a Siebel Scholars award.,Acknowledgments,[0],[0]
"AMR gratefully acknowledges the support of NSF CCF-1704834, Intel Research, and Amazon AWS Research grants.",Acknowledgments,[0],[0]
A.1 Additional Model and Training Details Computing,A Supplemental Material,[0],[0]
rj,A Supplemental Material,[0],[0]
"A record rj is represented by embedding a feature for its type, its position, and its word value in Rd, and applying an MLP with ReLU nonlinearity (Nair and Hinton, 2010) to form rj ∈Rd, similar to Yang et al. (2016) and Wiseman et al. (2017).
",A Supplemental Material,[0],[0]
LSTM Details,A Supplemental Material,[0],[0]
"The initial cell and hiddenstate values for the decoder LSTM are given by Q1xa and tanh(Q2xa), respectively, where Q1,Q2 ∈Rd×d.
",A Supplemental Material,[0],[0]
"When a word yt appears in a record rj , the input to the LSTM at time t + 1 is computed using an MLP with ReLU nonlinearity over the concatenation of the embeddings for rj’s record type, word value, position, and a feature for whether it is the final position for the type.",A Supplemental Material,[0],[0]
"If there are multiple rj from which yt could have been copied, the computed representations are averaged.",A Supplemental Material,[0],[0]
"At test time, we use the MAP rj to compute the input, even if there are multiple matches.",A Supplemental Material,[0],[0]
"For yt which could not have been copied, the input to the LSTM at time t+1 is computed using the same MLP over yt and three dummy features.
",A Supplemental Material,[0],[0]
"For the autoregressive HSMM, an additional 1- layer LSTM with d hidden units is used.",A Supplemental Material,[0],[0]
"We experimented with having the autoregressive HSMM consume either tokens y1:t in predicting yt+1, or the average embedding of the field types corresponding to copied tokens in y1:",A Supplemental Material,[0],[0]
"t. The former worked slightly better for the WikiBio dataset (where field types are more ambiguous), while the latter worked slightly better for the E2E dataset.
",A Supplemental Material,[0],[0]
"Transition Distribution The function C(xu), which produces hidden state embeddings conditional on the source, is defined as C(xu)=U2(ReLU(U1xu)), where
U1 ∈Rm3×d and U2 ∈RK×m2×m3 ; D(x) is defined analogously.",A Supplemental Material,[0],[0]
"For all experiments, m1=64, m2=32, and m3=64.
",A Supplemental Material,[0],[0]
"Optimization We train with SGD, using a learning rate of 0.5 and decaying by 0.5 each epoch after the first epoch in which validation loglikelihood fails to increase.",A Supplemental Material,[0],[0]
"When using an autoregressive HSMM, the additional LSTM is optimized only after the learning rate has been decayed.",A Supplemental Material,[0],[0]
"We regularize with Dropout (Srivastava et al., 2014).
",A Supplemental Material,[0],[0]
"A.2 Additional Learned Templates In Tables 6 and 7 we show visualizations of additional templates learned on the E2E and WikiBio data, respectively, by both the non-autoregressive and autoregressive HSMM models presented in the paper.",A Supplemental Material,[0],[0]
"For each model, we select a set of five dissimilar templates in an iterative way by greedily selecting the next template (out of the 200 most frequent) that has the highest percentage of states that do not appear in the previously selected templates; ties are broken randomly.",A Supplemental Material,[0],[0]
"Individual states within a template are visualized using the three most common segments they generate.
1.",A Supplemental Material,[0],[0]
"| The Waterman
The Golden Palace Browns Cambridge
...
| is a
is an is a family friendly
...
",A Supplemental Material,[0],[0]
"| Italian French
fast food ...
|",A Supplemental Material,[0],[0]
"restaurant
pub place ...
",A Supplemental Material,[0],[0]
"| with a with
with an ...
| average
high low ...
| customer rating
price range rating ...",A Supplemental Material,[0],[0]
"|.
2.",A Supplemental Material,[0],[0]
"| There is a
There is a cheap There is an
...
| restaurant
coffee shop French restaurant
...
",A Supplemental Material,[0],[0]
"| The Mill
Bibimbap House",A Supplemental Material,[0],[0]
"The Twenty Two
...
| located in the located on the
located north of the ...
| centre of the city
river city centre
...
| that serves
serving that provides
...
| fast food
sushi take-away deliveries
...
",A Supplemental Material,[0],[0]
"|.
3.",A Supplemental Material,[0],[0]
"| The Olive Grove
The Punter The Cambridge Blue
...
| restaurantpub ...",A Supplemental Material,[0],[0]
| serves offers has ...,A Supplemental Material,[0],[0]
"|
fast food sushi
take-away deliveries ...
|.
4.",A Supplemental Material,[0],[0]
"| The
Child friendly The average priced
...
| restaurant
coffee shop French restaurant
...
| The Mill
Bibimbap House",A Supplemental Material,[0],[0]
"The Twenty Two
...
| serves offers has ... | English Indian Italian ...",A Supplemental Material,[0],[0]
| food cuisine dishes ...,A Supplemental Material,[0],[0]
"|.
5.",A Supplemental Material,[0],[0]
"| The Strada
The Dumpling Tree Alimentum
...
| provides serves offers ... | Indian Chinese English ... | food in the food at a food and has a ... | customer rating of price range of rating of
...
| 1 out of 5 average
5 out of 5 ...
|.
1.",A Supplemental Material,[0],[0]
"| The Eagle
The Golden Curry Zizzi ...
| provides providing
serves ...
| Indian
Chinese English
...
| food
cuisine Food ...
",A Supplemental Material,[0],[0]
"| in the with a
and has a ...
| high
moderate average
...
| price range
customer rating rating ...
|.",A Supplemental Material,[0],[0]
"| It is
They are It’s ...
| near
located in the located near
...
| riverside
city centre Cafe Sicilia
...
|.",A Supplemental Material,[0],[0]
"| Its customer rating is
It has a The price range is
...
",A Supplemental Material,[0],[0]
"| 1 out of 5 average
high ...
|.
2.",A Supplemental Material,[0],[0]
"| Located near
Located in the Near ...
",A Supplemental Material,[0],[0]
"| The Portland Arms
riverside city centre
...
| is an
is a family friendly there is",A Supplemental Material,[0],[0]
"a
...
| Italian
fast food French ...
| restaurant called
place called restaurant named
...
| The Waterman
Cocum Loch Fyne
...
|.
3.",A Supplemental Material,[0],[0]
"| A An
A family friendly ...
| Italian
fast food French ...
| restaurant
pub coffee shop
...
| is
called named ...
|",A Supplemental Material,[0],[0]
"The Waterman
Cocum Loch Fyne
...
|.
4.",A Supplemental Material,[0],[0]
"| Located near
Located in the Near ...
",A Supplemental Material,[0],[0]
"| The Portland Arms
riverside city centre
...
| , | The Eagle
The Golden Curry Zizzi ...
| is a
is a family friendly is an ...
| cheap
family-friendly family friendly
...
",A Supplemental Material,[0],[0]
"| Italian
fast food French ...
| restaurant
pub coffee shop
...
|.
5.",A Supplemental Material,[0],[0]
"| A An
A family friendly ...
| Italian
fast food French ...
| restaurant
pub coffee shop
...
| near
located in the located near ...",A Supplemental Material,[0],[0]
"|
riverside city centre Cafe Sicilia ...",A Supplemental Material,[0],[0]
| is called named ...,A Supplemental Material,[0],[0]
| The Waterman Cocum Loch Fyne ...,A Supplemental Material,[0],[0]
"|.
",A Supplemental Material,[0],[0]
"Table 6: Five templates extracted from the E2E data with the NTemp model (top) and the Ntemp+AR model (bottom).
1.",A Supplemental Material,[0],[0]
"| william henry
george augustus frederick marie anne de bourbon
...
| (
was ( ; ...
| born
born on born 1 ...",A Supplemental Material,[0],[0]
| 1968 1960 1970 ...,A Supplemental Material,[0],[0]
| ) ]) ] ...,A Supplemental Material,[0],[0]
| is an american is a russian was an american ...,A Supplemental Material,[0],[0]
| politician actor football player ...,A Supplemental Material,[0],[0]
"|.
2.",A Supplemental Material,[0],[0]
"| sir
captain lieutenant
...
| john herbert
hartley donald charles cameron
...
| was a
was a british was an english ...",A Supplemental Material,[0],[0]
"|
world war i world war
",A Supplemental Material,[0],[0]
"first world war ...
| national team organization super league
...
|.
3.",A Supplemental Material,[0],[0]
"| john herbert
hartley donald charles cameron
...
| is a
was a is an ...
| indie rock
death metal ska ...
",A Supplemental Material,[0],[0]
"| band
midfielder defenceman
...
| from for
based in ...
| australia
los angeles, california chicago
...
|.
4.",A Supplemental Material,[0],[0]
"| john herbert
hartley donald charles cameron
...
| was a is a
is a former ...
",A Supplemental Material,[0],[0]
"| american
major league baseball australian
...
| football
professional baseball professional ice hockey
...
| midfielder defender
goalkeeper ...
",A Supplemental Material,[0],[0]
"|.
5.",A Supplemental Material,[0],[0]
"| james
william john william
...
| “ billy ” wilson
smith “ jack ” henry
...
| ( | 1900
c. 1894 1913 ...
| – | france
budapest buenos aires
...
| )",A Supplemental Material,[0],[0]
"| is an american is an english
was an american ...
| footballer
professional footballer rules footballer
...
| who plays for
who currently plays for who played with
...
",A Supplemental Material,[0],[0]
"| paganese
south melbourne fc dynamo kyiv
...
| in the of the
and the ...
| vicotiral football league national football league
australian football league ...",A Supplemental Material,[0],[0]
| ( | vfl nfl afl ... | ),A Supplemental Material,[0],[0]
"|.
1.",A Supplemental Material,[0],[0]
"| aftab ahmed
anderson da silva david jones
...
| (; ...",A Supplemental Material,[0],[0]
| born born on born 1 ...,A Supplemental Material,[0],[0]
| 1951 1970 1974 ...,A Supplemental Material,[0],[0]
| )] ...,A Supplemental Material,[0],[0]
| is an american was an american is an english ...,A Supplemental Material,[0],[0]
| actor actress cricketer ...,A Supplemental Material,[0],[0]
"|.
2.",A Supplemental Material,[0],[0]
"| aftab ahmed
anderson da silva david jones
...
| was a
is a former is a ...
| world war",A Supplemental Material,[0],[0]
"i
liberal baseball
...
| member of the
party member of the recipient of the
...
| austrian
pennsylvania montana
...
| house of representatives
legislature senate ...
|.
3.",A Supplemental Material,[0],[0]
"| adjutant
lieutenant captain
...
| aftab ahmed
anderson da silva david jones
...
",A Supplemental Material,[0],[0]
"| was a
is a former is a ...
| world war",A Supplemental Material,[0],[0]
"i
liberal baseball
...
| member of the
party member of the recipient of the
...
| knesset
scottish parliament fc lokomotiv liski
...
|.
4.",A Supplemental Material,[0],[0]
"| william
john william james “
...
| “ billy ” watson
smith jim ” edward
...
| ( | 1913
c. 1900 1913 ...",A Supplemental Material,[0],[0]
| – in - ...,A Supplemental Material,[0],[0]
"| 1917 surrey, england british columbia ...",A Supplemental Material,[0],[0]
| ) | was an american was an australian is an american ...,A Supplemental Material,[0],[0]
"| football player rules footballer defenceman ...
| who plays for
who currently plays for who played with
...
| collingwood
st kilda carlton ...
",A Supplemental Material,[0],[0]
"| in the of the
and the ...
| victorial football league national football league
australian football league ...",A Supplemental Material,[0],[0]
| ( | vfl afl nfl ...,A Supplemental Material,[0],[0]
| ),A Supplemental Material,[0],[0]
"|.
5.",A Supplemental Material,[0],[0]
"| aftab ahmed
anderson da silva david jones
...
| is a
is a former is a female
...
",A Supplemental Material,[0],[0]
"| member of the
party member of the recipient of the
...
| knesset
scottish parliament fc lokomotiv liski
...
|.
",A Supplemental Material,[0],[0]
Table 7: Five templates extracted from the WikiBio data with the NTemp model (top) and the Ntemp+AR model (bottom).,A Supplemental Material,[0],[0]
"While neural, encoder-decoder models have had significant empirical success in text generation, there remain several unaddressed problems with this style of generation.",abstractText,[0],[0]
"Encoderdecoder models are largely (a) uninterpretable, and (b) difficult to control in terms of their phrasing or content.",abstractText,[0],[0]
"This work proposes a neural generation system using a hidden semimarkov model (HSMM) decoder, which learns latent, discrete templates jointly with learning to generate.",abstractText,[0],[0]
"We show that this model learns useful templates, and that these templates make generation both more interpretable and controllable.",abstractText,[0],[0]
"Furthermore, we show that this approach scales to real data sets and achieves strong performance nearing that of encoderdecoder text generation models.",abstractText,[0],[0]
Learning Neural Templates for Text Generation,title,[0],[0]
Developing provably efficient algorithms for learning commonly used neural network architectures continues to be a core challenge in machine learning.,1. Introduction,[0],[0]
The underlying difficulty arises from the highly non-convex nature of the optimization problems posed by neural networks.,1. Introduction,[0],[0]
"Obtaining provable guarantees for learning even very basic architectures remains open.
",1. Introduction,[0],[0]
"In this paper we consider a simple convolutional neural network with a single filter and overlapping patches fol-
*Equal contribution 1Department of Computer Science, University of Texas at Austin 2Department of Computer Science, UCLA.",1. Introduction,[0],[0]
"Correspondence to: Surbhi Goel <surbhi@cs.utexas.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
lowed by average pooling (Figure 1).,1. Introduction,[0],[0]
"More formally, for an input image x, we consider k patches of size r indicated by selection matrices P1, . . .",1. Introduction,[0],[0]
", Pk ∈ {0, 1}r×n where each matrix has exactly one 1 in each row and at most one 1 in each column.",1. Introduction,[0],[0]
The neural network is computed as fw(x) = 1k ∑k i=1,1. Introduction,[0],[0]
"σ(w
TPix)",1. Introduction,[0],[0]
where σ is the activation function and w ∈,1. Introduction,[0],[0]
Rr is the weight vector corresponding to the convolution filter.,1. Introduction,[0],[0]
We focus on ReLU and leaky ReLU activation functions.,1. Introduction,[0],[0]
"The main contribution of this paper is a simple, stochastic update algorithm Convotron (Algorithm 1) for provably learning the above convolutional architecture.",1.1. Our Contributions,[0],[0]
"The algorithm has the following properties:
• Works for general classes of overlapping patches and requires mild distributional conditions.
",1.1. Our Contributions,[0],[0]
"• Proper recovery of the unknown weight vector.
",1.1. Our Contributions,[0],[0]
"• Stochastic in nature with a “gradient-like” update step.
• Requires no special/random initialization scheme or tuning of the learning rate.
",1.1. Our Contributions,[0],[0]
"• Tolerates noise and succeeds in the probabilistic concept model of learning.
",1.1. Our Contributions,[0],[0]
"• Logarithmic convergence in 1/ , the error parameter, in the realizable setting.
",1.1. Our Contributions,[0],[0]
This is the first efficient algorithm for learning general classes of overlapping patches (and the first algorithm for any class of patches that succeeds under mild distributional assumptions).,1.1. Our Contributions,[0],[0]
"Prior work has focused on analyzing SGD in the realizable/noiseless setting with the caveat of requiring either disjoint patches (Brutzkus & Globerson, 2017; Du et al., 2017b) with Gaussian inputs or technical conditions linking the underlying true parameters and the “closeness of patches” (Du et al., 2017a).
",1.1. Our Contributions,[0],[0]
"In contrast, our conditions depend only on the patch structure itself and can be efficiently verified.",1.1. Our Contributions,[0],[0]
Commonly used patch structures in computer vision applications such as 1D/2D grids satisfy our conditions.,1.1. Our Contributions,[0],[0]
"Additionally, we require only that the underlying distribution on samples is symmetric and induces a covariance matrix on the patches with polynomially bounded condition number1.",1.1. Our Contributions,[0],[0]
All prior work handles only continuous distributions.,1.1. Our Contributions,[0],[0]
Another major difference from prior work is that we give guarantees using purely empirical updates.,1.1. Our Contributions,[0],[0]
"That is, we do not require an assumption that we have access to exact quantities such as the population gradient of the loss function.
",1.1. Our Contributions,[0],[0]
"We further show that in the commonly studied setting of Gaussian inputs and non-overlapping patches, updating with respect to a single non-overlapping patch is sufficient to guarantee convergence.",1.1. Our Contributions,[0],[0]
This indicates that the Gaussian/nooverlap assumption is quite strong.,1.1. Our Contributions,[0],[0]
Our approach is to exploit the monotonicity of the activation function instead of the strong convexity of the loss surface.,1.2. Our Approach,[0],[0]
We use ideas from isotonic regression and extend them in the context of convolutional networks.,1.2. Our Approach,[0],[0]
"These ideas have been successful for learning generalized linear models (Kakade et al., 2011), improperly learning fully connected, depththree neural networks (Goel & Klivans, 2017b), and learning graphical models (Klivans & Meka, 2017).",1.2. Our Approach,[0],[0]
"It is known that in the worst case, learning even simple neural networks is computationally intractable.",1.3. Related Work,[0],[0]
"For example, in the non-realizable (agnostic) setting, it is known that learning a single ReLU (even for bounded distributions and unit norm hidden weight vectors) with respect to square-loss is as hard as learning sparse parity with noise (Goel et al., 2016), a notoriously difficult problem from computational learning theory.",1.3. Related Work,[0],[0]
"For learning one hidden layer convolutional networks, Brutzkus and Globerson (Brutzkus & Glober-
1Brutzkus and Globerson (Brutzkus & Globerson, 2017) proved that the problem, even with disjoint patches, is NP-hard in general, and so some distributional assumption is needed for efficient learning.
son, 2017) proved that distribution-free recoverability of the unknown weight vector is NP-hard, even if we restrict to disjoint patch structures.
",1.3. Related Work,[0],[0]
"As such, a major open question is to discover the mildest assumptions that lead to polynomial-time learnability for simple neural networks.",1.3. Related Work,[0],[0]
"In this paper, we consider the very popular class of convolutional neural networks (for a summary of other recent approaches for learning more general architectures see (Goel & Klivans, 2017a)).",1.3. Related Work,[0],[0]
"For convolutional networks, all prior research has focused on analyzing conditions under which (Stochastic) Gradient Descent converges to the hidden weight vector in polynomial-time.
",1.3. Related Work,[0],[0]
"Along these lines, Brutzkus and Globerson (Brutzkus & Globerson, 2017) proved that with respect to the spherical Gaussian distribution and for disjoint (non-overlapping) patch structures, gradient descent recovers the weight vector in polynomial-time.",1.3. Related Work,[0],[0]
"Zhong et al. (Zhong et al., 2017a) showed that gradient descent combined with tensor methods can recover one hidden layer involving multiple weight vectors but still require a Gaussian distribution and nonoverlapping patches.",1.3. Related Work,[0],[0]
"Du et al. (Du et al., 2017b) proved that gradient descent recovers a hidden weight vector involved in a type of two-layer convolutional network under the assumption that the distribution is a spherical Gaussian, the patches are disjoint, and the learner has access to the true population gradient of the loss function.
",1.3. Related Work,[0],[0]
"We specifically highlight the work of Du, Lee, and Tian (Du et al., 2017a), who proved that gradient descent recovers a hidden weight vector in a one-layer convolutional network under certain technical conditions that are more general than the Gaussian/no-overlap patch scenario.",1.3. Related Work,[0],[0]
"Their conditions involve a certain “alignment” of the unknown patch structure, the hidden weight vector, and the (continuous) marginal distribution.",1.3. Related Work,[0],[0]
"However, it is unclear which concrete patch-structure/distributional combinations their framework captures.",1.3. Related Work,[0],[0]
"We also note that all of the above results assume there is no noise; i.e.,, they work in the realizable setting.
",1.3. Related Work,[0],[0]
"Other related works analyzing gradient descent with respect to the Gaussian distribution (but for non-convolutional networks) include (Soltanolkotabi, 2017; Ge et al., 2017; Zhong et al., 2017b; Tian, 2016; Li & Yuan, 2017; Zhang et al., 2017).
",1.3. Related Work,[0],[0]
"In contrast, we consider an alternative to gradient descent, namely Convotron, that is based on isotonic regression.",1.3. Related Work,[0],[0]
"The exploration of alternative algorithms to gradient descent is a feature of our work, as it may lead to new algorithms for learning deeper networks.",1.3. Related Work,[0],[0]
|| ·,2. Preliminaries,[0],[0]
|| corresponds to the l2 -norm for vectors and the spectral norm for matrices.,2. Preliminaries,[0],[0]
The identity matrix is denoted by I .,2. Preliminaries,[0],[0]
We denote the input-label distribution by D over input drawn from X and label drawn from Y .,2. Preliminaries,[0],[0]
"The marginal distribution on the input is denoted by DX and the corresponding probability density function is denoted by PX .
",2. Preliminaries,[0],[0]
In this paper we consider a simple convolution neural network with one hidden layer and average pooling.,2. Preliminaries,[0],[0]
"Given input x ∈ Rn, the network computes k patches of size r where each patch’s location is indicated by matrices P1, . . .",2. Preliminaries,[0],[0]
", Pk ∈ {0, 1}r×n.",2. Preliminaries,[0],[0]
Each matrix Pi has exactly one 1 in each row and at most one 1 in every column.,2. Preliminaries,[0],[0]
"As before, the neural network is computed as follows:
fw(x) = 1
k k∑ i=1",2. Preliminaries,[0],[0]
σ,2. Preliminaries,[0],[0]
( wTPix ) where σ is the activation function and w ∈,2. Preliminaries,[0],[0]
"Rr is the weight vector corresponding to the convolution filter.
",2. Preliminaries,[0],[0]
"We study the problem of learning the teacher network with true weight w∗ under the square loss from noisy labels, that is, we wish to find a w such that
L(w) :",2. Preliminaries,[0],[0]
"= Ex∼DX [ (fw(x)− fw∗(x))2 ] ≤ .
Assumptions 1.",2. Preliminaries,[0],[0]
"We make the following assumptions:
(a) Learning Model: Probabilistic Concept Model (Kearns & Schapire, 1990), that is, for all (x, y) ∼ D, y = fw∗(x)+ξ, for some unknownw∗ where ξ is noise with E[ξ|x] = 0 and E[ξ4|x] ≤ ρ for some ρ > 0.",2. Preliminaries,[0],[0]
"Note we do not require that the noise is independent of the instance.2
(b) Distribution: The marginal distribution on the input space DX is a symmetric distribution about the origin, that is, for all x, PX (x) = PX (−x).",2. Preliminaries,[0],[0]
"(c) Patch Structure: The minimum eigenvalue of PΣ :=∑k i,j=1 PiΣP T j where Σ = Ex∼DX [xxT ] and the max-
imum eigenvalue of P := ∑k i,j=1 PiP T j are polynomially bounded.
",2. Preliminaries,[0],[0]
(d) Activation Function:,2. Preliminaries,[0],[0]
"The activation function has the following form:
σ(x) =",2. Preliminaries,[0],[0]
"{ x if x ≥ 0 αx otherwise
for some constant α ∈",2. Preliminaries,[0],[0]
"[0, 1].",2. Preliminaries,[0],[0]
"2In the realizable setting, as in previous works, it is assumed
that ξ = 0.
",2. Preliminaries,[0],[0]
"The distributional assumption includes common assumptions such as Gaussian inputs, but is far less restrictive.",2. Preliminaries,[0],[0]
"For example, we do not require the distribution to be continuous nor do we require it to have identity covariance.",2. Preliminaries,[0],[0]
"In Section 4, we show that commonly used patch schemes from computer vision satisfy our patch requirements.",2. Preliminaries,[0],[0]
The assumption on activation functions is satisfied by popular activations such as ReLU (α = 0) and leaky ReLU (α > 0).,2. Preliminaries,[0],[0]
"The activations we consider in this paper have the following useful property under the stated distributional assumption:
Lemma 1.",2.1. Some Useful Properties,[0],[0]
"For all a, b ∈ R,
Ex∼DX [σ(aTx)(bTx)] = 1 + α
2 Ex∼DX [(aTx)(bTx)].
",2.1. Some Useful Properties,[0],[0]
"The loss function can be upper bounded by the l2-norm distance of weight vectors using the following lemma.
",2.1. Some Useful Properties,[0],[0]
Lemma 2.,2.1. Some Useful Properties,[0],[0]
"For any w, we have
L(w) ≤ 1 + α 2 λmax(Σ)||w∗ − w||2.
Lemma 3.",2.1. Some Useful Properties,[0],[0]
"For all w and x,
(fw∗(x)− fw(x))2 ≤ ||w∗ − w||2||x||2
The Gershgorin Circle Theorem, stated below, is useful for bounding the eigenvalues of matrices.
",2.1. Some Useful Properties,[0],[0]
"Theorem 1 ((Weisstein, 2003)).",2.1. Some Useful Properties,[0],[0]
"For a n × n matrix A, define Ri := ∑n j=1,j 6=i |Ai,j |.",2.1. Some Useful Properties,[0],[0]
"Each eigenvalue of A must lie in at least one of the disks {z : |z −Ai,i| ≤ Ri}.
",2.1. Some Useful Properties,[0],[0]
Note: The proofs of lemmas in this section have been deferred to the Supplemental section.,2.1. Some Useful Properties,[0],[0]
In this section we describe our main algorithm Convotron and give a proof of its correctness.,3. The Convotron Algorithm,[0],[0]
Convotron is an iterative algorithm similar in flavor to SGD with a modified (aggressive) gradient update.,3. The Convotron Algorithm,[0],[0]
"Unlike SGD (Algorithm 3), Convotron comes with provable guarantees and also does not need a good initialization scheme for convergence.
",3. The Convotron Algorithm,[0],[0]
"The following theorem describes the convergence rate of our algorithm:
Theorem 2.",3. The Convotron Algorithm,[0],[0]
"If Assumptions 1 are satisfied then for
η = Ω ( λmin(PΣ) kλmax(P ) min ( 1 Ex[||x||4] , δ||w∗||2√ ρEx[||x||4] )) and
T =",3. The Convotron Algorithm,[0],[0]
O,3. The Convotron Algorithm,[0],[0]
"(
k ηλmin(PΣ)
log (
1 δ
)) , with probability 1 − δ, the
weight vector w computed by Convotron satisfies
||w",3. The Convotron Algorithm,[0],[0]
"− w∗||2 ≤ ||w∗||2.
",3. The Convotron Algorithm,[0],[0]
Algorithm 1 Convotron Initialize w1 := 0 ∈,3. The Convotron Algorithm,[0],[0]
"Rr. for t = 1 to T do
Draw (xt, yt) ∼ D",3. The Convotron Algorithm,[0],[0]
Let Gt = (yt − fwt(xt)),3. The Convotron Algorithm,[0],[0]
(∑k i=1,3. The Convotron Algorithm,[0],[0]
Pixt ),3. The Convotron Algorithm,[0],[0]
"Set wt+1 = wt + ηGt
end for Return wT+1
Proof.",3. The Convotron Algorithm,[0],[0]
"Define St = {(x1, y1), . . .",3. The Convotron Algorithm,[0],[0]
", (xt, yt)}",3. The Convotron Algorithm,[0],[0]
"The dynamics of Convotron can be expressed as follows:
Ext,yt",3. The Convotron Algorithm,[0],[0]
[||wt − w∗||2 − ||wt+1,3. The Convotron Algorithm,[0],[0]
"− w∗||2|St−1] = 2ηExt,yt",3. The Convotron Algorithm,[0],[0]
"[(w∗ − wt)TGt|St−1] − η2Ext,yt [||Gt||2|St−1]
We need to bound the RHS of the above equation.",3. The Convotron Algorithm,[0],[0]
"We have,
Ext,yt",3. The Convotron Algorithm,[0],[0]
"[(w∗ − wt)TGt|St−1]
= Ext,yt",3. The Convotron Algorithm,[0],[0]
[ (w∗ − wt)T (yt − fwt(xt)),3. The Convotron Algorithm,[0],[0]
( k∑ i=1,3. The Convotron Algorithm,[0],[0]
"Pixt )∣∣∣∣∣St−1 ]
= Ext,ξt",3. The Convotron Algorithm,[0],[0]
"[ (w∗ − wt)T (fw∗(xt) + ξt
−fwt(xt)) ( k∑ i=1",3. The Convotron Algorithm,[0],[0]
"Pixt )∣∣∣∣∣St−1 ]
= Ext [ (w∗ − wt)T (fw∗(xt)− fwt(xt))",3. The Convotron Algorithm,[0],[0]
( k∑ i=1,3. The Convotron Algorithm,[0],[0]
"Pixt )∣∣∣∣∣St−1 ]
(1)
= 1
k ∑ 1≤i,j≤k",3. The Convotron Algorithm,[0],[0]
Ext,3. The Convotron Algorithm,[0],[0]
[(σ(wT∗ Pixt)− σ(wTt Pixt)),3. The Convotron Algorithm,[0],[0]
"(2)
(wT∗ − wTt )",3. The Convotron Algorithm,[0],[0]
"Pjxt|St−1]
",3. The Convotron Algorithm,[0],[0]
"= 1 + α
2k ∑ 1≤i,j≤k Ext [((wT∗ − wTt )Pixt)
((wT∗ − wTt )",3. The Convotron Algorithm,[0],[0]
"Pjxt)|St−1] (3)
",3. The Convotron Algorithm,[0],[0]
"= 1 + α
2k (wT∗ − wTt )  ∑ 1≤i≤k Pi Ext",3. The Convotron Algorithm,[0],[0]
"[xtxTt ] ∑ 1≤j≤k PTj
 ",3. The Convotron Algorithm,[0],[0]
"(w∗ − wt) = 1 + α
2k (wT∗ − wTt )  ∑ 1≤i,j≤k PiΣP T j  (w∗ − wt) = 1 + α
2k (wT∗ − wTt )",3. The Convotron Algorithm,[0],[0]
"PΣ(w∗ − wt)
≥ 1 + α 2k",3. The Convotron Algorithm,[0],[0]
λmin(PΣ)||w∗ − wt||2.,3. The Convotron Algorithm,[0],[0]
"(4)
(1) follows using linearity of expectation and the fact that that E[ξt|xt] = 0 and (3) follows from using Lemma 1.",3. The Convotron Algorithm,[0],[0]
"(4) follows from observing that PΣ is symmetric, thus ∀x, xTPΣx ≥ λmin(PΣ)||x||2.
",3. The Convotron Algorithm,[0],[0]
Now we bound the variance of Gt.,3. The Convotron Algorithm,[0],[0]
Note that E[Gt] = 0.,3. The Convotron Algorithm,[0],[0]
"Further,
Ext,yt",3. The Convotron Algorithm,[0],[0]
"[||Gt||2|St−1]
= Ext,yt (yt − fwt(xt))2 ∣∣∣∣∣ ∣∣∣∣∣",3. The Convotron Algorithm,[0],[0]
k∑ i=1,3. The Convotron Algorithm,[0],[0]
Pixt ∣∣∣∣∣,3. The Convotron Algorithm,[0],[0]
∣∣∣∣∣,3. The Convotron Algorithm,[0],[0]
2 ∣∣∣∣∣∣St−1  ≤,3. The Convotron Algorithm,[0],[0]
λmax(P ),3. The Convotron Algorithm,[0],[0]
"Ext,yt [ (yt − fwt(xt))2||xt||2
∣∣St−1] (5) =",3. The Convotron Algorithm,[0],[0]
"λmax(P )Ext,ξt [ (fw∗(xt) + ξt",3. The Convotron Algorithm,[0],[0]
"− fwt(xt))2||xt||2
∣∣St−1] = λmax(P )Ext,ξt [ ((fw∗(xt)− fwt(xt))2 + ξ2t
+ 2(fw∗(xt)− fwt(xt))ξt||xt||2 ∣∣St−1]
= λmax(P ) ( Ext [ (fw∗(xt)− fwt(xt))2||xt||2 ∣∣St−1] + Ext,ξt",3. The Convotron Algorithm,[0],[0]
"[ξ2t ||xt||2] ) (6)
≤",3. The Convotron Algorithm,[0],[0]
"λmax(P ) ( Ext [||xt||4]||w∗ − wt||2 + √ ρExt [||xt||4] ) (7)
(5) follows from observing ∣∣∣∣∣∣∑ki=1 Pix∣∣∣∣∣∣2 ≤",3. The Convotron Algorithm,[0],[0]
"λmax(P )||x||2 for all x, (6) follows from observing Eξ[ξ|x] = 0 and (7) follows from applying Lemma 3 and bounding Ext,ξt",3. The Convotron Algorithm,[0],[0]
"[ξ2t ||xt||2] using Cauchy-Schwartz inequality.
",3. The Convotron Algorithm,[0],[0]
"Combining the above equations and taking expectation over St−1, we get
ESt",3. The Convotron Algorithm,[0],[0]
[||wt+1 − w∗||2] ≤ (1− 3ηβ + η2γ)ESt−1,3. The Convotron Algorithm,[0],[0]
"[||wt − w∗||2] + η2B
for β = 1+α3k λmin(PΣ), γ =",3. The Convotron Algorithm,[0],[0]
λmax(P )Ex[||x|| 4] and B =,3. The Convotron Algorithm,[0],[0]
"λmax(P ) √ ρEx[||x||4].
",3. The Convotron Algorithm,[0],[0]
"We set η = βmin (
1 γ , δ||w∗||2 B
) and break the analysis to
two cases:
Case 1:",3. The Convotron Algorithm,[0],[0]
ESt−1 [||wt − w∗||2] > ηB,3. The Convotron Algorithm,[0],[0]
β .,3. The Convotron Algorithm,[0],[0]
This implies that ESt,3. The Convotron Algorithm,[0],[0]
[||wt+1 − w∗||2] ≤ (1− ηβ)ESt−1,3. The Convotron Algorithm,[0],[0]
"[||wt − w∗||2].
",3. The Convotron Algorithm,[0],[0]
Case 2:,3. The Convotron Algorithm,[0],[0]
ESt−1 [||wt − w∗||2] ≤ ηB β ≤,3. The Convotron Algorithm,[0],[0]
||w∗||,3. The Convotron Algorithm,[0],[0]
"2.
Observe that once Case 2 is satisfied, we have ESt [||wt+1− w∗||2] ≤ (1 − 2ηβ)ηBβ + η
2B ≤ ηBβ .",3. The Convotron Algorithm,[0],[0]
"Hence, for any iteration > t, Case 2 will continue to hold true.",3. The Convotron Algorithm,[0],[0]
This implies that either at each iteration ESt−1 [||wt − w∗||2] decreases by a factor (1−ηβ) or it is less than δ||w∗||2.,3. The Convotron Algorithm,[0],[0]
"Thus if Case 1 is not satisfied for any iteration up to T , then we have,
EST",3. The Convotron Algorithm,[0],[0]
[||wT+1 − w||2] ≤ (1− ηβ) T ||w∗||2 ≤,3. The Convotron Algorithm,[0],[0]
"e−ηβT ||w∗||2
since at initialization ||w1 − w∗|| = ||w∗||.",3. The Convotron Algorithm,[0],[0]
"Setting T = O (
1 ηβ log ( 1 δ )) and using Markov’s inequality, with probability 1 − δ, over the choice of ST , ||wT+1 − w∗|| ≤ ||w∗||2.
",3. The Convotron Algorithm,[0],[0]
"By using Lemma 2, we can get a bound on L(wT ) ≤ ||w∗||2 by appropriately scaling .",3. The Convotron Algorithm,[0],[0]
"For the realizable (no noise) setting, that is, for all (x, y) ∼ D, y = fw∗(x), for some unknown w∗, Convotron achieves faster convergence rates.
",3.1. Convotron in the Realizable Case,[0],[0]
Corollary 1.,3.1. Convotron in the Realizable Case,[0],[0]
"If Assumptions 1 are satisfied with the learning model restricted to the realizable case, then for suitably choosen η, after T = O ( k2λmax(P )Ex[||x||4] λmin(PΣ)2 log ( 1 δ )) iterations, with probability 1−δ, the weight vector w computed by Convotron satisfies
||w",3.1. Convotron in the Realizable Case,[0],[0]
"− w∗||2 ≤ ||w∗||2.
",3.1. Convotron in the Realizable Case,[0],[0]
Proof.,3.1. Convotron in the Realizable Case,[0],[0]
"Since the setting has no noise, ρ = 0.",3.1. Convotron in the Realizable Case,[0],[0]
"Setting that parameter in Theorem 2 gives us η = Ω ( λmin(PΣ)
kλmax(P )Ex[||x||4] ) as δ||w∗||
2√ ρEx[||x||4] tends to infinity as ρ tends to 0 and taking the minimum removes this dependence from η.",3.1. Convotron in the Realizable Case,[0],[0]
"Substituting this η gives us the required result.
",3.1. Convotron in the Realizable Case,[0],[0]
"Observe that the dependence of in the convergence rate is log(1/ ) for the realizable setting, compared to the 1/ dependence in the noisy setting.",3.1. Convotron in the Realizable Case,[0],[0]
"In this section, we will show that the commonly used convolutional filters in practice (“patch and stride”) have good eigenvalues giving us fast convergence by Theorem 2.",4. Which Patch Structures are Easy to Learn?,[0],[0]
We will start with the 1D case and then subsequently extend the result for the 2D case.,4. Which Patch Structures are Easy to Learn?,[0],[0]
Here we formally describe a patch and stride convolution in the one-dimensional setting.,4.1. 1D Convolution,[0],[0]
Consider a 1D image of dimension n. Let the patch size be r and stride be d. Let the patches be indexed from 1 and let patch i start at position (i−1)d+1 and be contiguous through position (i−1)d+r.,4.1. 1D Convolution,[0],[0]
"The matrix Pi of dimension r × n corresponding to patch i looks as follows,
Pi = ( 0r×((i−1)d+1)Ir0r×(n−r−(i−1)d) )",4.1. 1D Convolution,[0],[0]
"where 0a×b indicates a matrix of dimension a× b with all zeros and Ia indicates the identity matrix of size a.
Thus, the total number of patches is k = bn−rd c",4.1. 1D Convolution,[0],[0]
+ 1,4.1. 1D Convolution,[0],[0]
.,4.1. 1D Convolution,[0],[0]
We will assume that n,4.1. 1D Convolution,[0],[0]
≥ 2r− 1 and r ≥ d.,4.1. 1D Convolution,[0],[0]
"The latter condition is to ensure there is some overlap, non-overlapping case, which is easier, is handled in the next section.",4.1. 1D Convolution,[0],[0]
"We will bound the extremal eigenvalues of P =∑k i,j=1 PiP T j .",4.1. 1D Convolution,[0],[0]
"Simple algebra gives us the following structure for P ,
Pi,j = {",4.1. 1D Convolution,[0],[0]
k,4.1. 1D Convolution,[0],[0]
− a if |i− j| = ad 0,4.1. 1D Convolution,[0],[0]
"otherwise
For understanding, we show the matrix structure for d = 1 and n ≥ 2r.",4.1. 1D Convolution,[0],[0]
k k,4.1. 1D Convolution,[0],[0]
− 1 . . .,4.1. 1D Convolution,[0],[0]
k,4.1. 1D Convolution,[0],[0]
− r + 1 k,4.1. 1D Convolution,[0],[0]
− 1 k . . .,4.1. 1D Convolution,[0],[0]
k,4.1. 1D Convolution,[0],[0]
− r + 2 ... ... . . .,4.1. 1D Convolution,[0],[0]
"...
",4.1. 1D Convolution,[0],[0]
k,4.1. 1D Convolution,[0],[0]
− r + 1 k,4.1. 1D Convolution,[0],[0]
− r + 2 . . .,4.1. 1D Convolution,[0],[0]
"k
 .",4.1. 1D Convolution,[0],[0]
"The following lemmas bound the extremal eigenvalues of P .
",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Lemma 4.,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Maximum eigenvalue of P satisfies λmax(P ) ≤,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
k(p + 1),4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
− (p − p2)(p2 + 1) = O(kp),4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"where p = b r−1d c and p2 = bp2c.
Proof.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Using Theorem 1, we have λmax(P ) ≤ maxi ( Pi,i + ∑ j 6=i |Pi,j | ) = maxi ∑k j=1 Pi,j .",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Observe
that P is bisymmetric thus ∑k j=1 Pi,j = ∑k",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"j=1 Pr−i+1,j and we can restrict to the top half of the matrix.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"The structure of P indicates that in a fixed row, the diagonal entry is maximum and the non-zero entries decrease monotonically by 1 as we move away from the diagonal.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Also, there can be at most p+ 1 non-zero entries in any row.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Thus the sum is maximized when there are p+ 1 non-zero entries and the diagonal entry is the middle entry, that is at position p2d+1.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"By simple algebra,
λmax(P ) ≤ k∑ j=1 Pp2d+1,j
= k + 2 p2∑ j=1 (k − j) + (p− 2p2)(k − p2 − 1)
= k(p+ 1)− (p− p2)(p2 + 1).
",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Lemma 5.,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Minimum eigenvalue of P satisfies λmin(P ),4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"≥ 0.5.
",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Proof.,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"We break the analysis into following two cases:
d < r/2: We can show that λmax(P−1) ≥ 2 using the structure of P (see Lemma A and B in Supplemental).",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
Since λmin(P ),4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"= 1/λmax(P −1), we have λmin(P ) ≥ 0.5.
",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
d ≥ r/2: In this case we directly bound the minimum eigenvalue of P .,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Using Theorem 1, we know that λmin(P ) ≥ mini ( Pi,i − ∑ j 6=i |Pi,j | ) .",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"For Pi,j 6= 0, |i − j| = ad for some a.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
The maximum value that |i,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"− j| can take is r − 1 and since d ≥ r/2, a must be either 0 or 1.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Also, for any i, there exists a unique j such that |i − j| = d since r/2 ≤",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"d < r, thus there are exactly 2 non-zero entries in each row of P , Pi,",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"i. This gives us, for each i, ∑ j 6=i Pi,j = k − 1.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Thus, we get that λmin(P )",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"≥
mini ( Pi,i − ∣∣∣∑j 6=i Pi,j∣∣∣) = k",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
− (k − 1) = 1.,4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Combining both, we get the required result.",4.1.1. BOUNDING EXTREMAL EIGENVALUES OF P,[0],[0]
"Augmenting the above analysis with Theorem 2 gives us learnability of 1D convolution filters.
",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
Corollary 2.,4.1.2. LEARNING RESULT FOR 1D,[0],[0]
"If Assumptions 1(a),(b), and (d) are satisfied and the patches have a patch and stride structure with parameters n, r, d, then for suitably chosen η and T =
O
( n3r
d4λmin(Σ)2 max
( Ex[||x||4], √ ρEx[||x||4] ||w∗||2 ) log ( 1 δ )) ,
with probability 1 − δ, the weight vector w output by Convotron satisfies
||w",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
"− w∗||2 ≤ ||w∗||2.
",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
Proof.,4.1.2. LEARNING RESULT FOR 1D,[0],[0]
"Combining the above Lemmas gives us that λmax(P ) = O(pk) = O(nr/d
2) and λmin(P ) = Ω(1).",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
Observe that λmin(PΣ) ≥ λmin(P )λmin(Σ).,4.1.2. LEARNING RESULT FOR 1D,[0],[0]
"Substituting these values in Theorem 2 gives us the desired result.
",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
"Comparing with SGD, (Brutzkus & Globerson, 2017) showed that even for r = 2 and d = 1, Gradient descent can get stuck in a local minima with probability ≥ 1/4.",4.1.2. LEARNING RESULT FOR 1D,[0],[0]
Here we formally define stride and patch convolutions in two dimensions.,4.2. 2D Convolution,[0],[0]
Consider a 2D image of dimension n1×n2.,4.2. 2D Convolution,[0],[0]
"Let the patch size be r1× r2 and stride in both directions be d1, d2 respectively.",4.2. 2D Convolution,[0],[0]
"Enumerate patches such that patch (i, j) starts at position ((i − 1)d1 + 1, (j − 1)d2 + 1) and is a rectangle with diagonally opposite point ((i−1)d2+r1, (j− 1)d2 + r2).",4.2. 2D Convolution,[0],[0]
Let k1 = bn1−r1d1,4.2. 2D Convolution,[0],[0]
c+,4.2. 2D Convolution,[0],[0]
1 and k2 = b n2−r2 d2 c+ 1.,4.2. 2D Convolution,[0],[0]
Let us vectorize the image row-wise into a n1n2 dimension vector and enumerate each patch row-wise to get a r1r2 dimensional vector.,4.2. 2D Convolution,[0],[0]
"Let Q(i,j) be the indicator matrix of dimension r1r2 × n1n2",4.2. 2D Convolution,[0],[0]
"with 1 at (a, b) if the ath location of patch (i, j) is b. More formally, (Q(i,j))a,b = 1 for all a = pr2 + q",4.2. 2D Convolution,[0],[0]
+ 1 for 0 ≤,4.2. 2D Convolution,[0],[0]
"p < r1, 0 ≤ q < r2, and
b = ((i− 1)d1 +",4.2. 2D Convolution,[0],[0]
p)n2 + jd2 + q+ 1 else 0.,4.2. 2D Convolution,[0],[0]
"Note that there are k1 · k2 patches in total with the corresponding patch matrices being Q(i,j) for 1 ≤",4.2. 2D Convolution,[0],[0]
"i ≤ k1, 1 ≤ j ≤ k2.",4.2. 2D Convolution,[0],[0]
"We will bound the extremal eigenvalues of Q =∑k1 i,p=1 ∑k2 j,q=1Q(i,j)Q T (p,q).",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Let P (1) i ’s be the patch matrices corresponding to the 1D convolution for parameters n1, r1, d1 defined as in the previous section and let P (1) = ∑k1",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"i,j=1 P (1) i (P (1) j )
T .",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Define P (2)i ’s for 1 ≤,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"i ≤ k2 and P (2) similarly with parameters n2, r2, d2 instead of n1, r1, d1.
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Lemma 6.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Q(i,j) = P (1) i ⊗ P (2) j .
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Proof.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Intuitively P (1)i and P (2) j give the indices corresponding to the row and column of the 2D patch and the Kronecker product vectorizes it to give us the (i, j)th patch.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"More formally, we will show that (Q(i,j))a,b = 1 iff (P
(1) i ⊗ P (2) j )a,b = 1.
Let a = pr2 + q + 1 with 0 ≤",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
p,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
<,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"r1, 0 ≤ q < r2 and b = rn2 + s + 1 with 0 ≤ r < n1, 0 ≤ s < n2.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Then, (P
(1) i ⊗ P (2) j )a,b = 1 iff (P (1) i )p,r = 1 and (P (2) j )q,s = 1.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"We know that (P (1)i )p,r = 1 iff r = (i− 1)d1 + p+ 1 and (P
(2) j )q,s = 1 iff s =",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
(j − 1)d2 + q + 1.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
This gives us that b =,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
((i − 1)d1 + p)n1,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"+ (j − 1)d2 + q + 1, which is the same condition for (Q(i,j))a,b = 1.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Thus Q(i,j) = P
(1) i ⊗ P (2) j .
Lemma 7.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Q = P (1) ⊗ P (2).
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Proof.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"We have,
Q = k1∑",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"i,p=1 k2∑ j,q=1",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Q(i,j)Q",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"T (p,q)
=",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"k1∑ i,p=1 k2∑ j,q=1 (P (1) i ⊗ P (2) j )(P (1) p ⊗ P (2)q )",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"T
= k1∑ i,p=1 k2∑ j,q=1 (P (1) i ⊗ P (2) j )((P (1) p ) T ⊗",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"(P (2)q )T )
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"= k1∑ i,p=1 k2∑ j,q=1 (P (1) i (P (1) p ) T )⊗",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"(P (2)j (P (2) q ) T )
=  k1∑",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"i,p=1 P (1) i (P (1) p ) T ⊗  k2∑",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"j,q=1 P (2) j",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"(P (2) q ) T  = P (1) ⊗ P (2).
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Lemma 8.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"We have λmin(Q) ≥ 0.25 and λmax(Q) = O(k1p1k2p2) where p1 = b r1−1d1 c and p2 = b r2−1 d2 c.
Proof.",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Since Q = P (1) ⊗ P (2) and Q,P (1), P (2) are positive semi-definite, λmin(Q) = λmin(P )",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
λmin(P (2)) and λmax(Q),4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
=,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
λmax(P (1))λmax(P (2)).,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Using the lemmas from the previous section gives us the required result.
",4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
Note that this technique can be extended to higher dimensional patch structures as well.,4.2.1. BOUNDING EXTREMAL EIGENVALUES OF Q,[0],[0]
"Similar to the 1D case, combining the above analysis with Theorem 2 gives us learnability of 2D convolution filters.
",4.2.2. LEARNING RESULT FOR 2D,[0],[0]
Corollary 3.,4.2.2. LEARNING RESULT FOR 2D,[0],[0]
"If Assumptions 1(a),(b), and (d) are satisfied and the patches have a 2D patch and stride structure with parameters n1, n2, r1, r2, d1, d2, then for suitably chosen η and T =
O
( n31n 3 2r1r2
d31d 3 2λmin(Σ)
2 max ( Ex[||x||4], √ ρEx[||x||4] ||w∗||2 ) log ( 1 δ )) ,
with probability 1 − δ, the weight vector w output by Convotron satisfies
||w",4.2.2. LEARNING RESULT FOR 2D,[0],[0]
"− w∗||2 ≤ ||w∗||2.
",4.2.2. LEARNING RESULT FOR 2D,[0],[0]
Proof.,4.2.2. LEARNING RESULT FOR 2D,[0],[0]
"Lemma 8 gives us that λmax(Q) = O(n1n2r1r2/(d1d2)
2) and λmin(P ) = Ω(1).",4.2.2. LEARNING RESULT FOR 2D,[0],[0]
Observe that λmin(PΣ) ≥ λmin(P )λmin(Σ).,4.2.2. LEARNING RESULT FOR 2D,[0],[0]
Substituting these values in Theorem 2 gives us the desired result.,4.2.2. LEARNING RESULT FOR 2D,[0],[0]
"In this section, we will show that if there is one patch that does not overlap with any patch and the covariance matrix is identity then we can easily learn the filter even if the other patches have arbitrary overlaps.",5. Non-overlapping Patches are Easy,[0],[0]
This includes the commonly used Gaussian assumption.,5. Non-overlapping Patches are Easy,[0],[0]
"WLOG we assume
Algorithm 2 Convotron-No-Overlap Initialize w1 := 0 ∈",5. Non-overlapping Patches are Easy,[0],[0]
"Rr. for t = 1 to T do
Draw (xt, yt) ∼ D",5. Non-overlapping Patches are Easy,[0],[0]
Let,5. Non-overlapping Patches are Easy,[0],[0]
Gt = (yt − fwt(xt))P1xt,5. Non-overlapping Patches are Easy,[0],[0]
Set,5. Non-overlapping Patches are Easy,[0],[0]
wt+1,5. Non-overlapping Patches are Easy,[0],[0]
"= wt + ηGt
end for Return wT+1
that P1 is the patch that does not overlap with any other patch implying P1PTj = P T j P1 = 0 for all j 6= 1.
",5. Non-overlapping Patches are Easy,[0],[0]
Observe that the algorithm ignores the directions of all other patches and yet succeeds.,5. Non-overlapping Patches are Easy,[0],[0]
"This indicates that with respect to a Gaussian distribution, in order to have an interesting patch structure (for one layer networks), it is necessary to avoid having even a single disjoint patch.",5. Non-overlapping Patches are Easy,[0],[0]
"The following theorem shows the convergence of Convotron-No-Overlap.
",5. Non-overlapping Patches are Easy,[0],[0]
Theorem 3.,5. Non-overlapping Patches are Easy,[0],[0]
"If Assumptions 1 are satisfied with Σ = I , then for η = (1+α)3k min ( 1 Ex[||x||4] , δ||w∗||2√ ρEx[||x||4] ) and T ≥
1 ηδ log ( 1 δ ) , with probability 1 − δ, the weight vector w outputted by Convotron-No-Overlap satisfies
||w",5. Non-overlapping Patches are Easy,[0],[0]
"− w∗||2 ≤ ||w∗||2.
",5. Non-overlapping Patches are Easy,[0],[0]
Proof.,5. Non-overlapping Patches are Easy,[0],[0]
The proof follows the outline of the Convotron proof very closely.,5. Non-overlapping Patches are Easy,[0],[0]
We use the same definitions as in the previous proof.,5. Non-overlapping Patches are Easy,[0],[0]
"We have,
Ext,yt",5. Non-overlapping Patches are Easy,[0],[0]
"[(w∗ − wt)TGt|St−1]
= 1
k ∑ 1≤i≤k",5. Non-overlapping Patches are Easy,[0],[0]
Ext,5. Non-overlapping Patches are Easy,[0],[0]
[(σ(wT∗ Pixt)− σ(wTt,5. Non-overlapping Patches are Easy,[0],[0]
"Pixt))(wT∗
− wTt )P1xt|St−1]
= 1 + α
2k ∑ 1≤i≤k",5. Non-overlapping Patches are Easy,[0],[0]
Ext,5. Non-overlapping Patches are Easy,[0],[0]
"[((wT∗ − wTt )Pixt)((wT∗
− wTt )",5. Non-overlapping Patches are Easy,[0],[0]
"P1xt)|St−1]
= 1 + α
2k (wT∗ − wTt )  ∑ 1≤i≤k Pi Ext",5. Non-overlapping Patches are Easy,[0],[0]
"[xtxTt ]P1(w∗ − wt) = 1 + α
2k ||wT∗",5. Non-overlapping Patches are Easy,[0],[0]
"− wTt ||2
The last equality follows since PTi P1 = 0 for all i 6= 1 and PT1 P1 is a permutation of identity.
",5. Non-overlapping Patches are Easy,[0],[0]
"Similarly,
Ext,yt",5. Non-overlapping Patches are Easy,[0],[0]
"[||Gt||2|St−1] = Ext,yt [ (yt − fwt(xt))2 ||Pixt|| 2 ∣∣∣St−1]
≤",5. Non-overlapping Patches are Easy,[0],[0]
"Ext,yt [ (yt − fwt(xt))2||xt||2 ∣∣St−1]
Algorithm 3 SGD Randomly initialize w1 ∈",5. Non-overlapping Patches are Easy,[0],[0]
"Rr. for t = 1 to T do
Draw (xt, yt) ∼ D",5. Non-overlapping Patches are Easy,[0],[0]
Let Gt = (yt − fwt(xt)),5. Non-overlapping Patches are Easy,[0],[0]
(∑k i=1 σ,5. Non-overlapping Patches are Easy,[0],[0]
′(wTt Pixt)Pixt ),5. Non-overlapping Patches are Easy,[0],[0]
"Set wt+1 = wt + ηGt
end for Return wT+1
≤",5. Non-overlapping Patches are Easy,[0],[0]
Ext,5. Non-overlapping Patches are Easy,[0],[0]
"[||xt||4]||w∗ − wt||2 + √ ρExt [||xt||4]
Following the rest of the analysis for η and T as in the theorem statement gives us the required result.",5. Non-overlapping Patches are Easy,[0],[0]
"To further support our theoretical findings, we empirically compare the performance of SGD (Algorithm 3) with our algorithm Convotron.",6. Experiments: SGD vs Convotron,[0],[0]
"We measure performance based on the failure probability, that is, the fraction of runs the algorithm fails to converge on randomly initialized runs (the randomness is over both the choice of initialization for SGD and the draws from the distribution).",6. Experiments: SGD vs Convotron,[0],[0]
"More formally, we say that the algorithm fails if the closeness in l2-norm of the difference of the final weight vector obtained (wT ) and the true weight parameter (w∗), that is, ||wT − w∗|| is greater than a threshold θ.",6. Experiments: SGD vs Convotron,[0],[0]
"We choose this measure because in practice, due to the high computation time of training neural networks, random restarts are expensive.
",6. Experiments: SGD vs Convotron,[0],[0]
"In the experiments, given a fixed true weight vector, for varying learning rates (increments of 0.01), we choose 50 random initializations and run the two algorithms with them as starting points.",6. Experiments: SGD vs Convotron,[0],[0]
We plot the failure probability (θ = 0.1) with varying learning rate.,6. Experiments: SGD vs Convotron,[0],[0]
"Note that the lowest learning rate we use is 0.01 as making the learning rate too small requires high number of iterations for convergence for both algorithms.
",6. Experiments: SGD vs Convotron,[0],[0]
"We first test the performance on a simple 1D convolution case with (n, k, d, T ) = (8, 4, 1, 6000) and 2D case with (n1, n2, k1, k2, d1, d2, T ) = (5, 5, 3, 3, 1, 1, 15000) on inputs drawn from a normalized (l2 norm 1) Gaussian distribution with identity covariance matrix.",6. Experiments: SGD vs Convotron,[0],[0]
We adversarially choose a fixed weight vector (l2-norm 1).,6. Experiments: SGD vs Convotron,[0],[0]
"For example, we take the vector to be [1,−1, 1,−1] in the 1D case and normalize.",6. Experiments: SGD vs Convotron,[0],[0]
"This weight vector can be viewed as an edge detection filter, that is, counting the number of times image goes from black (negative) to white (positive).",6. Experiments: SGD vs Convotron,[0],[0]
"Figure 3 (Top) shows that SGD has a small data dependent range where it succeeds but may fail with almost 0.5 probability outside this region whereas Convotron always returns a good solution for small enough η chosen according to
Theorem 2.",6. Experiments: SGD vs Convotron,[0],[0]
"The failure points observed for SGD show the prevalence of bad local minima where SGD gets stuck.
",6. Experiments: SGD vs Convotron,[0],[0]
"For the second experiment, we choose a fixed weight vector for which SGD performs well with very high probability on a normalized Gaussian input distribution with identity covariance matrix (see Figure 3 (Bottom-left)).",6. Experiments: SGD vs Convotron,[0],[0]
"However, on choosing a different covariance matrix with higher condition number ∼ 60, the performance of SGD worsens whereas Convotron always succeeds (see Figure 3 (Bottom-Right)).",6. Experiments: SGD vs Convotron,[0],[0]
The covariance matrix is generated by choosing random matrices followed by symmetrizing them and adding cI for c > 0,6. Experiments: SGD vs Convotron,[0],[0]
"to make the eigenvalues positive.
",6. Experiments: SGD vs Convotron,[0],[0]
"These experiments demonstrate that techniques for finetuning SGD’s learning rate are necessary, even for very simple architectures.",6. Experiments: SGD vs Convotron,[0],[0]
"In contrast, no fine-tuning is necessary for Convotron: the correct learning rate can be easily computed given the learner’s desired patch structure and estimate of the covariance martix.",6. Experiments: SGD vs Convotron,[0],[0]
"We have given the first efficient algorithm with provable guarantees for learning general one layer convolutional networks under symmetric, well-conditioned distributions.",7. Conclusions and Future Work,[0],[0]
The obvious open question is to extend our algorithm to higher depth networks and weaken the distributional assumptions.,7. Conclusions and Future Work,[0],[0]
We thank Jessica Hoffmann and Philipp Krähenbühl for useful discussions.,Acknowledgments,[0],[0]
We give the first provably efficient algorithm for learning a one hidden layer convolutional network with respect to a general class of (potentially overlapping) patches under mild conditions on the underlying distribution.,abstractText,[0],[0]
"We prove that our framework captures commonly used schemes from computer vision, including one-dimensional and twodimensional “patch and stride” convolutions.",abstractText,[0],[0]
Our algorithm– Convotron– is inspired by recent work applying isotonic regression to learning neural networks.,abstractText,[0],[0]
"Convotron uses a simple, iterative update rule that is stochastic in nature and tolerant to noise (requires only that the conditional mean function is a one layer convolutional network, as opposed to the realizable setting).",abstractText,[0],[0]
"In contrast to gradient descent, Convotron requires no special initialization or learning-rate tuning to converge to the global optimum.",abstractText,[0],[0]
We also point out that learning one hidden convolutional layer with respect to a Gaussian distribution and just one disjoint patch P (the other patches may be arbitrary) is easy in the following sense: Convotron can efficiently recover the hidden weight vector by updating only in the direction of P .,abstractText,[0],[0]
Learning One Convolutional Layer with Overlapping Patches,title,[0],[0]
Intelligent agents rarely act in isolation in the real world and often seek to achieve their goals through interaction with other agents.,1. Introduction,[0],[0]
"Such interactions give rise to rich, complex behaviors formalized as per-agent policies in a multiagent system (Ferber, 1999; Wooldridge, 2009).",1. Introduction,[0],[0]
"Depending on the underlying motivations of the agents, interactions could be directed towards achieving a shared goal in a collaborative setting, opposing another agent in a competitive setting, or be a mixture of these in a setting where agents collaborate in teams to compete against other teams.",1. Introduction,[0],[0]
"Learning useful representations of the policies of agents based on their interactions is an important step towards characterization of the agent behavior and more generally inference and reasoning in multiagent systems.
",1. Introduction,[0],[0]
1Stanford University 2Carnegie Mellon University 3OpenAI.,1. Introduction,[0],[0]
Correspondence to: Aditya Grover,1. Introduction,[0],[0]
"<adityag@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this work, we propose an unsupervised encoder-decoder framework for learning continuous representations of agent policies given access to only a few episodes of interaction.",1. Introduction,[0],[0]
"For any given agent, the representation function is an encoder that learns a mapping from an interaction (i.e., one or more episodes of observation and action pairs involving the agent) to a continuous embedding vector.",1. Introduction,[0],[0]
"Using such embeddings, we condition a policy network (decoder) and train it simultaneously with the encoder to imitate other interactions involving the same (or a coupled) agent.",1. Introduction,[0],[0]
"Additionally, we can explicitly discriminate between the embeddings corresponding to different agents using triplet losses.
",1. Introduction,[0],[0]
"For the embeddings to be useful, the representation function should generalize to both unseen interactions and unseen agents for novel downstream tasks.",1. Introduction,[0],[0]
Generalization is wellunderstood in the context of supervised learning where a good model is expected to attain similar train and test performance.,1. Introduction,[0],[0]
"For multiagent systems, we consider a notion of generalization based on agent-interaction graphs.",1. Introduction,[0],[0]
"An agentinteraction graph provides an abstraction for distinguishing the agents (nodes) and interactions (edges) observed during training, validation, and testing.
",1. Introduction,[0],[0]
"Our framework is agnostic to the nature of interactions in multiagent systems, and hence broadly applicable to competitive and cooperative environments.",1. Introduction,[0],[0]
"In particular, we consider two multiagent environments: (i) a competitive continuous control environment, RoboSumo (Al-Shedivat et al., 2018), and (ii) a ParticleWorld environment of cooperative communication where agents collaborate to achieve a common goal (Mordatch & Abbeel, 2018).",1. Introduction,[0],[0]
"For evaluation, we show how representations learned by our framework are effective for downstream tasks that include clustering of agent policies (unsupervised), classification such as win or loss outcomes in competitive systems (supervised), and policy optimization (reinforcement).",1. Introduction,[0],[0]
"In the case of policy optimization, we show how these representations can serve as privileged information for better training of agent policies.",1. Introduction,[0],[0]
"In RoboSumo, we train agent policies that can condition on the opponent’s representation and achieve superior win rates much more quickly as compared to an equally expressive baseline policy with the same number of parameters.",1. Introduction,[0],[0]
"In ParticleWorld, we train speakers that can communicate more effectively with a much wider range of listeners given knowledge of their representations.",1. Introduction,[0],[0]
"In this section, we present the necessary background and notation relevant to the problem setting of this work.
",2. Preliminaries,[0],[0]
Markov games.,2. Preliminaries,[0],[0]
"We use the classical framework of Markov games (Littman, 1994) to represent multiagent systems.",2. Preliminaries,[0],[0]
A Markov game extends the general formulation of partially observable Markov decision processes (POMDP) to the multiagent setting.,2. Preliminaries,[0],[0]
"In a Markov game, we are given a set of n agents on a state-space S with action spaces A1,A2, · · · ,An and observation spaces O1,O2, · · · ,On respectively.",2. Preliminaries,[0],[0]
"At every time step t, an agent i receives an observation",2. Preliminaries,[0],[0]
o(t)i ∈,2. Preliminaries,[0],[0]
Oi and executes an action a (t),2. Preliminaries,[0],[0]
i ∈,2. Preliminaries,[0],[0]
Ai based on a stochastic policy π(i) :,2. Preliminaries,[0],[0]
"Oi × Ai → [0, 1].",2. Preliminaries,[0],[0]
"Based on the executed action, the agent receives a reward r (t) i :",2. Preliminaries,[0],[0]
S × Ai → R and the next observation o (t+1),2. Preliminaries,[0],[0]
i .,2. Preliminaries,[0],[0]
The state dynamics are determined by a transition function T : S×A1×· · ·×An → S .,2. Preliminaries,[0],[0]
The agent policies are trained to maximize their own expected reward r̄i = ∑H t=1 r (t),2. Preliminaries,[0],[0]
"i over a time horizon H .
",2. Preliminaries,[0],[0]
Extended Markov games.,2. Preliminaries,[0],[0]
"In this work, we are interested in interactions that involve not all but only a subset of agents.",2. Preliminaries,[0],[0]
"For this purpose, we generalize Markov games as follows.",2. Preliminaries,[0],[0]
"First, we augment the action space of each agent with a NO-OP (i.e., no action).",2. Preliminaries,[0],[0]
"Then, we introduce a problem parameter, 2 ≤ k ≤ n, with the following semantics.",2. Preliminaries,[0],[0]
"During every rollout of the Markov game, all but k agents deterministically execute the NO-OP operator while the k agents execute actions as per the policies defined on the original observation and action spaces.",2. Preliminaries,[0],[0]
"Accordingly, we assume that each agent receives rewards only in the interaction episode it participates in.",2. Preliminaries,[0],[0]
"Informally, the extension allows for multiagent systems where all agents do not necessarily have to participate simultaneously in an interaction.",2. Preliminaries,[0],[0]
"For instance, this allows to consider one-vs-one multiagent tournaments where only two players participate in any given match.
",2. Preliminaries,[0],[0]
"To further introduce the notation, consider a multiagent system as a generalized Markov game.",2. Preliminaries,[0],[0]
"We denote the set of agent policies with P = {π(i)}ni=1, interaction episodes with E = {EMj}mj=1 where Mj ⊆ {1, 2, · · · , n}, |Mj | = k is the set of k agents participating in episodeEMj .",2. Preliminaries,[0],[0]
"To simplify presentation for the rest of the paper, we assume k = 2 and, consequently, denote the set of interaction episodes between agents i and j as Eij .",2. Preliminaries,[0],[0]
"A single episode, eij ∈ Eij , consists of a sequence of observations and actions for the specified time horizon, H .
",2. Preliminaries,[0],[0]
Imitation learning.,2. Preliminaries,[0],[0]
"Our approach to learning policy representations relies on behavioral cloning (Pomerleau, 1991)— a type of imitation learning where we train a mapping from observations to actions in a supervised manner.",2. Preliminaries,[0],[0]
"Although there exist other imitation learning algorithms (e.g., inverse reinforcement learning, Abbeel & Ng, 2004), our frame-
work is largely agnostic to the choice of the algorithm, and we restrict our presentation to behavioral cloning, leaving other imitation learning paradigms to future work.",2. Preliminaries,[0],[0]
The dominant paradigm for unsupervised representation learning is to optimize the parameters of a representation function that can best explain or generate the observed data.,3. Learning framework,[0],[0]
"For instance, the skip-gram objective used for language and graph data learns representations of words and nodes predictive of representations of surrounding context (Mikolov et al., 2013; Grover & Leskovec, 2016).",3. Learning framework,[0],[0]
"Similarly, autoencoding objectives, often used for image data, learn representations that can reconstruct the input (Bengio et al., 2009).
",3. Learning framework,[0],[0]
"In this work, we wish to learn a representation function that maps episode(s) from an agent policy, π(i) ∈ Π to a real-valued vector embedding where Π is a class of representable policies.",3. Learning framework,[0],[0]
"That is, we optimize for the parameters θ for a function fθ : E → Rd where E denotes the space of episodes corresponding to a policy and d is the dimension of the embedding.",3. Learning framework,[0],[0]
"Here, we have assumed the agent policies are black-boxes, i.e., we can only access them based on interaction episodes with other agents in a Markov game.",3. Learning framework,[0],[0]
"Hence, for every agent i, we wish to learn policies using Ei = ∪jE(i)ij .",3. Learning framework,[0],[0]
"Here, E (i) ij refers the episode data for interactions between agent i and j, but consisting of only the observation and action pairs of agent i. For a multiagent system, we propose the following auxiliary tasks for learning a good representation of an agent’s policy:
1.",3. Learning framework,[0],[0]
Generative representations.,3. Learning framework,[0],[0]
"The representation should be useful for simulating the agent’s policy.
",3. Learning framework,[0],[0]
2.,3. Learning framework,[0],[0]
Discriminative representations.,3. Learning framework,[0],[0]
"The representation should be able to distinguish the agent’s policy with the policies of other agents.
",3. Learning framework,[0],[0]
"Accordingly, we now propose generative and discriminative objectives for representation learning in multiagent systems.",3. Learning framework,[0],[0]
"Imitation learning does not require direct access to the reward signal, making it an attractive task for unsupervised representation learning.",3.1. Generative representations via imitation learning,[0],[0]
"Formally, we are interested in learning a policy π(i)φ : S×A → [0, 1] for an agent i given access to observation and action pairs from interaction episode(s) involving the agent.",3.1. Generative representations via imitation learning,[0],[0]
"For behavioral cloning, we maximize the following (negative) cross-entropy objective:
Ee∼Ei  ∑ 〈o,a〉∼e [ log π (i) φ (a|o) ] where the expectation is over interaction episodes of agent i and the optimization is over the parameters φ.
",3.1. Generative representations via imitation learning,[0],[0]
"Algorithm 1 Learn Policy Embedding Function (fθ) input {Ei}ni=1 – interaction episodes, λ – hyperparameter.
1: Initialize θ and φ 2: for i = 1, 2, . . .",3.1. Generative representations via imitation learning,[0],[0]
", n",3.1. Generative representations via imitation learning,[0],[0]
do 3: Sample a positive episode pe ← e+ ∼ Ei 4: Sample a reference episode re ← e∗ ∼ Ei\e+ 5:,3.1. Generative representations via imitation learning,[0],[0]
Compute Im loss←,3.1. Generative representations via imitation learning,[0],[0]
"−
∑ 〈o,a〉∼e+ log πφ,θ(a|o, e∗)
6: for j = 1, 2, . .",3.1. Generative representations via imitation learning,[0],[0]
.,3.1. Generative representations via imitation learning,[0],[0]
", n do 7: if j 6=",3.1. Generative representations via imitation learning,[0],[0]
i then 8: Sample a negative episode ne ← e− ∼,3.1. Generative representations via imitation learning,[0],[0]
"Ej 9: Compute Id loss← dθ(e+, e−, e∗)
10: Set Loss← Im loss + λ · Id loss 11: Update θ and φ to minimize Loss 12: end if 13: end for 14: end for output θ
Learning individual policies for every agent can be computationally and statistically prohibitive for large-scale multiagent systems, especially when the number of interaction episodes per agent is small.",3.1. Generative representations via imitation learning,[0],[0]
"Moreover, it precludes generalization across the behaviors of such agents.",3.1. Generative representations via imitation learning,[0],[0]
"On the other hand, learning a single policy for all agents increases sample efficiency but comes at the cost of reduced modeling flexibility in simulating diverse agent behaviors.",3.1. Generative representations via imitation learning,[0],[0]
We offset this dichotomy by learning a single conditional policy network.,3.1. Generative representations via imitation learning,[0],[0]
"To do so, we first specify a representation function, fθ : E → Rd, with parameters θ, where E represents the space of episodes.",3.1. Generative representations via imitation learning,[0],[0]
We use this embedding to condition the policy network.,3.1. Generative representations via imitation learning,[0],[0]
"Formally, the policy network is denoted by πφ,θ : S ×",3.1. Generative representations via imitation learning,[0],[0]
A × E,3.1. Generative representations via imitation learning,[0],[0]
"→ [0, 1] and φ are parameters for the function mapping the agent observation and embedding to a distribution over the agent’s actions.
",3.1. Generative representations via imitation learning,[0],[0]
"The parameters θ and φ for the conditional policy network are learned jointly by maximizing the following objective:
1
n n∑ i=1",3.1. Generative representations via imitation learning,[0],[0]
"Ee1∼Ei, e2∼Ei\e1  ∑ 〈o,a〉∼e1 log πφ,θ(a|o, e2)  (1) For every agent, the objective function samples two distinct episodes e1 and e2.",3.1. Generative representations via imitation learning,[0],[0]
The observation and action pairs from e2 are used to learn an embedding fθ(e2) that conditions the policy network trained on observation and action pairs from e1.,3.1. Generative representations via imitation learning,[0],[0]
The conditional policy network shares statistical strength through a common set of parameters for the policy network and the representation function across all agents.,3.1. Generative representations via imitation learning,[0],[0]
"An intuitive requirement for any representation function learned for a multiagent system is that the embeddings should reflect characteristics of an agent’s behavior that dis-
tinguish it from other agents.",3.2. Discriminative representations via identification,[0],[0]
"To do so in an unsupervised manner, we propose an objective for agent identification based on the triplet loss directly in the space of embeddings.
",3.2. Discriminative representations via identification,[0],[0]
"To learn a representation for agent i based on interaction episodes, we use the representation function fθ to compute three sets of embeddings: (i) a positive embedding for an episode e+ ∼ Ei involving agent i, (ii) a negative embedding for an episode e− ∼ Ej involving a random agent j 6=",3.2. Discriminative representations via identification,[0],[0]
"i, and (iii) a reference embedding for an episode e∗ ∼ Ei again involving agent i, but different from e+.",3.2. Discriminative representations via identification,[0],[0]
"Given these embeddings, we define the triplet loss:
dθ(e+, e−, e∗) =
(1 + exp {‖re − ne‖2 − ‖re",3.2. Discriminative representations via identification,[0],[0]
"− pe‖2})−2 (2)
where pe = fθ(e+), ne = fθ(e−), re = fθ(e∗).",3.2. Discriminative representations via identification,[0],[0]
"Intuitively, the loss encourages the positive embedding to be closer to the reference embedding than the negative embedding, which makes the embeddings of the same agent tend to cluster together and be further away from embeddings of other agents.",3.2. Discriminative representations via identification,[0],[0]
We note that various other notions of distance can also be used.,3.2. Discriminative representations via identification,[0],[0]
"The one presented above corresponding to a squared softmax objective (Hoffer & Ailon, 2015).",3.2. Discriminative representations via identification,[0],[0]
Conditional imitation learning encourages fθ to learn representations that can learn and simulate the entire policy of the agents and agent identification incentivizes representations that can distinguish between agent policies.,3.3. Hybrid generative-discriminative representations,[0],[0]
"Both objectives are complementary, and we combine Eq.",3.3. Hybrid generative-discriminative representations,[0],[0]
"(1) and Eq. (2) to get the final objective used for representation learning:
1
n n∑ i=1",3.3. Hybrid generative-discriminative representations,[0],[0]
"Ee+∼Ei, e∗∼Ei\e+  ∑ 〈o,a〉∼e+ log πφ,θ(a|o, e∗)︸ ︷︷ ︸ imitation",3.3. Hybrid generative-discriminative representations,[0],[0]
"− λ ∑ j 6=i
Ee−∼Ej",3.3. Hybrid generative-discriminative representations,[0],[0]
"[dθ(e+, e−, e∗)]︸ ︷︷ ︸ agent identification
 (3)
where λ > 0 is a tunable hyperparameter that controls the relative weights of the discriminative and generative terms.",3.3. Hybrid generative-discriminative representations,[0],[0]
The pseudocode for the proposed algorithm is given in Algorithm 1.,3.3. Hybrid generative-discriminative representations,[0],[0]
"In experiments, we parameterize the conditional policy πθ,φ using neural networks and use stochastic gradient-based methods for optimization.",3.3. Hybrid generative-discriminative representations,[0],[0]
Generalization is well-understood for supervised learning— models that shows similar train and test performance exhibit good generalization.,4. Generalization in MAS,[0],[0]
"To measure the quality of the learned representations for a multiagent system (MAS), we introduce a graphical formalism for reasoning about agents and their interactions.",4. Generalization in MAS,[0],[0]
"In many scenarios, we are interested in generalization of the policy representation function fθ across novel agents and interactions in a multiagent system.",4.1. Generalization across agents & interactions,[0],[0]
"For instance, we would like fθ to output useful embeddings for a downstream task, even when evaluated with respect to unseen agents and interactions.",4.1. Generalization across agents & interactions,[0],[0]
"This notion of generalization is best understood using agent-interaction graphs (Grover et al., 2018).
",4.1. Generalization across agents & interactions,[0],[0]
"The agent-interaction graph describes interactions between a set of agent policies P and a set of interaction episodes I through a graph G = (P, I).1",4.1. Generalization across agents & interactions,[0],[0]
An example graph is shown in Figure 1a.,4.1. Generalization across agents & interactions,[0],[0]
"The graph represents a multiagent system consisting of interactions between pairs of agents, and we will especially focus on the interactions involving Alice, Bob, Charlie, and Davis.",4.1. Generalization across agents & interactions,[0],[0]
"The interactions could be competitive (e.g., a match between two agents) or cooperative (e.g., two agents communicating for a navigation task).
",4.1. Generalization across agents & interactions,[0],[0]
"We learn the representation function fθ on a subset of the interactions, denoted by the solid black edges in Figure 1a.",4.1. Generalization across agents & interactions,[0],[0]
"At test time, fθ is evaluated on some downstream task of interest.",4.1. Generalization across agents & interactions,[0],[0]
The agents and interactions observed at test time can be different from those used for training.,4.1. Generalization across agents & interactions,[0],[0]
"In particular, we consider the following cases:
Weak generalization.2 Here, we are interested in the generalization performance of the representation function on an unseen interaction between existing agents, all of which are observed during training.",4.1. Generalization across agents & interactions,[0],[0]
This corresponds to the red edge representing the interaction between Alice and Bob in Figure 1a.,4.1. Generalization across agents & interactions,[0],[0]
"From the context of an agent-interaction graph, the test graph adds only edges to the train graph.
",4.1. Generalization across agents & interactions,[0],[0]
Strong generalization.,4.1. Generalization across agents & interactions,[0],[0]
Generalization can also be evaluated with respect to unseen agents (and their interactions).,4.1. Generalization across agents & interactions,[0],[0]
This corresponds to the addition of agents Charlie and Davis in Figure 1a.,4.1. Generalization across agents & interactions,[0],[0]
"Akin to a few shot learning setting, we observe a few of their interactions with existing agents Alice and
1If we have more than two participating agents per interaction episode, we could represent the interactions using a hypergraph.
",4.1. Generalization across agents & interactions,[0],[0]
"2Also referred to as intermediate generalization by Grover et al. (2018).
",4.1. Generalization across agents & interactions,[0],[0]
Bob (green edges) and generalization is evaluated on unseen interactions involving Charlie and Davis (blue edges).,4.1. Generalization across agents & interactions,[0],[0]
"The test graph adds both nodes and edges to the train graph.
",4.1. Generalization across agents & interactions,[0],[0]
"For brevity, we skip discussion of weaker forms of generalization that involves evaluation of the test performance on unseen episodes of an existing training edge (black edge).",4.1. Generalization across agents & interactions,[0],[0]
"Since the representation function is learned using an unsupervised auxiliary objective, we test its generalization performance by evaluating the usefulness of these embeddings for various kinds downstream tasks described below.
",4.2. Generalization across tasks,[0],[0]
Unsupervised.,4.2. Generalization across tasks,[0],[0]
"These embeddings can be used for clustering, visualization, and interpretability of agent policies in a low-dimensional space.",4.2. Generalization across tasks,[0],[0]
"Such semantic associations between the learned embeddings can be defined for a single agent wherein we expect representations for the same agent based on distinct episodes to be embedded close to each other, or across agents wherein agents with similar policies will have similar embeddings on average.
",4.2. Generalization across tasks,[0],[0]
Supervised.,4.2. Generalization across tasks,[0],[0]
Deep neural network representations are especially effective for predictive modeling.,4.2. Generalization across tasks,[0],[0]
"In a multiagent setting, the embeddings serve as useful features for learning agent properties and interactions, including assignment of role categories to agents with different skills in a collaborative setting, or prediction of win or loss outcomes of interaction matches between agents in a competitive setting.
",4.2. Generalization across tasks,[0],[0]
Reinforcement.,4.2. Generalization across tasks,[0],[0]
"Finally, we can use the learned representation functions to improve generalization of the policies learned from a reinforcement signal in competitive and cooperative settings.",4.2. Generalization across tasks,[0],[0]
"We design policy networks that, in addition to observations, take embedding vectors of the opposing agents as inputs.",4.2. Generalization across tasks,[0],[0]
The embeddings are computed from the past interactions of the opposing agent either with the agent being trained or with other agents using the representation function (Figure 2).,4.2. Generalization across tasks,[0],[0]
Such embeddings play the role of privileged information and allow us to train a policy network that uses this information to learn faster and generalize better to opponents or cooperators unseen at training time.,4.2. Generalization across tasks,[0],[0]
We evaluate the proposed framework for both competitive and collaborative environments on various downstream machine learning tasks.,5. Evaluation methodology & results,[0],[0]
"In particular, we use the RoboSumo and ParticleWorld environments for the competitive and collaborative scenarios, respectively.",5. Evaluation methodology & results,[0],[0]
We consider the embedding objectives in Eq.,5. Evaluation methodology & results,[0],[0]
"(1), Eq. (2), and Eq. (3) independently and refer to them as Emb-Im, Emb-Id, and Emb-Hyb respectively.",5. Evaluation methodology & results,[0],[0]
"The hyperparameter λ for Emb-Hyb is chosen by grid search over λ ∈ {0.01, 0.05, 0.1, 0.5} on a held-out set of interactions.
",5. Evaluation methodology & results,[0],[0]
"In all our experiments, the representation function fθ is specified through a multi-layer perceptron (MLP) that takes as input an episode and outputs an embedding of that episode.",5. Evaluation methodology & results,[0],[0]
"In particular, the MLP takes as input a single (observation, action) pair to output an intermediate embedding.",5. Evaluation methodology & results,[0],[0]
"We average the intermediate embeddings for all (observation, action) pairs in an episode to output an episode embedding.",5. Evaluation methodology & results,[0],[0]
"To condition a policy network on the embedding, we simply concatenate the observation fed as input to the network with the embedding.",5. Evaluation methodology & results,[0],[0]
"Experimental setup and other details beyond what we state below are deferred to the Appendix.
5.1.",5. Evaluation methodology & results,[0],[0]
"The RoboSumo environment
For the competitive environment, we use RoboSumo (AlShedivat et al., 2018)—a 3D environment with simulated physics (based on MuJoCo (Todorov et al., 2012))",5. Evaluation methodology & results,[0],[0]
that allows agents to control multi-legged 3D robots and compete against each other in continuous-time wrestling games (Figure 1b).,5. Evaluation methodology & results,[0],[0]
"For our analysis, we train a diverse collection of 25 agents, some of which are trained via self-play and others are trained in pairs concurrently using Proximal Policy Optimization (PPO) algorithm (Schulman et al., 2017).
",5. Evaluation methodology & results,[0],[0]
We start with a fully connected agent-interaction graph (clique) of 25 agents.,5. Evaluation methodology & results,[0],[0]
"Every edge in this graph corresponds to 10 rollout episodes involving the corresponding agents.
",5. Evaluation methodology & results,[0],[0]
"The maximum length (or horizon) of any episode is 500 time steps, after which the episode is declared a draw.",5. Evaluation methodology & results,[0],[0]
"To evaluate weak generalization, we sample a connected subgraph for training with approximately 60% of the edges preserved for training, and remaining split equally for validation and testing.",5. Evaluation methodology & results,[0],[0]
"For strong generalization, we preserve 15 agents and their interactions with each other for training, and similarly, 5 agents and their within-group interactions each for validation and testing.",5. Evaluation methodology & results,[0],[0]
"To evaluate the robustness of the embeddings, we compute multiple embeddings for each policy based on different episodes of interaction at test time.",5.1.1. EMBEDDING ANALYSIS,[0],[0]
Our evaluation metric is based on the intra- and inter-cluster Euclidean distances between embeddings.,5.1.1. EMBEDDING ANALYSIS,[0],[0]
The intra-cluster distance for an agent is the average pairwise distance between its embeddings computed on the set of test interaction episodes involving the agent.,5.1.1. EMBEDDING ANALYSIS,[0],[0]
"Similarly, the inter-cluster distance is the average pairwise distance between the embeddings of an agent with those of other agents.",5.1.1. EMBEDDING ANALYSIS,[0],[0]
Let Ti = {t(i)c }nic=1 denote the set of test interactions involving,5.1.1. EMBEDDING ANALYSIS,[0],[0]
"agent i. We define the intra-inter cluster ratio (IICR) as:
IICR =
1 n ∑n i=1 1 n2i ∑ni a=1 ∑ni b=1",5.1.1. EMBEDDING ANALYSIS,[0],[0]
"‖t (i) a − t(i)b ‖2
1 n(n−1) n∑ i=1",5.1.1. EMBEDDING ANALYSIS,[0],[0]
n∑ j 6=i 1 ninj ni∑ a=1,5.1.1. EMBEDDING ANALYSIS,[0],[0]
"nj∑ b=1 ‖t(i)a − t(j)b ‖2 .
",5.1.1. EMBEDDING ANALYSIS,[0],[0]
The intra-inter clustering ratios are reported in Table 1.,5.1.1. EMBEDDING ANALYSIS,[0],[0]
"A ratio less than 1 suggests that there is signal that identifies the agent, and the signal is stronger for lower ratios.",5.1.1. EMBEDDING ANALYSIS,[0],[0]
"Even though this task might seem especially suited for the agent identification objective, we interestingly find that the Emb-Im attains lower clustering ratios than Emb-Id for both weak and strong generalization.",5.1.1. EMBEDDING ANALYSIS,[0],[0]
Emb-Hyb outperforms both these methods.,5.1.1. EMBEDDING ANALYSIS,[0],[0]
"We qualitatively visualize the embeddings learned using Emb-Hyb by projecting them on the leading principal components, as shown in Figures 3a and 3b for 10 test interaction episodes of 5 randomly selected agents in the weak and strong generalization settings respectively.",5.1.1. EMBEDDING ANALYSIS,[0],[0]
We can use these embeddings directly for training a classifier to predict the outcome of an episode (win/loss/draw).,5.1.2. OUTCOME PREDICTION,[0],[0]
"For classification, we use an MLP with 3 hidden layers
of 100 units each and the learning objective minimizes the cross entropy error.",5.1.2. OUTCOME PREDICTION,[0],[0]
The input to the classifier are the embeddings of the two agents involved in the episode.,5.1.2. OUTCOME PREDICTION,[0],[0]
The results are reported in Table 1.,5.1.2. OUTCOME PREDICTION,[0],[0]
"Again, imitation based methods seem more suited for this task with Emb-Hyb and Emb-Im outperforming other methods for weak and strong generalization respectively.",5.1.2. OUTCOME PREDICTION,[0],[0]
Here we ask whether embeddings can be used to improve learned policies in a reinforcement learning setting both in terms of end performance and generalization.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"To this end, we select 5 training, 5 validation, and 5 testing opponents from the pool of 25 pre-trained agents.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Next, we train a new agent with reinforcement learning to compete against the selected 5 training opponents; the agent is trained concurrently against all 5 opponents using a distributed version of PPO algorithm, as described in Al-Shedivat et al. (2018).",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Throughout training, we evaluate new agents on the 5 testing opponents and record the average win and draw rates.
",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Using this setup, we compare a baseline agent with MLP-based policy with an agent whose policy takes 100- dimensional embeddings of the opponents as additional inputs at each time step and uses that information to condition its behavior on the opponent’s representation.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"The embed-
dings for each opponent are either computed online, i.e., based on an interaction episode rolled out during training at a previous time step (Figure 2), or offline, i.e., pre-computed before training the new agent using only interactions between the pre-trained opponents.
",5.1.3. POLICY OPTIMIZATION,[0],[0]
Figure 4 shows the average win rates against the set of training and testing opponents for the baseline and our agents that use different types of embeddings.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"While every new agent is able to achieve almost 100% win rate against the training opponents, we see that the agents that condition their policies on the opponent’s embeddings perform better on the held-out set of opponents, i.e., generalize better, with the best performance achieved with Emb-Hyb.",5.1.3. POLICY OPTIMIZATION,[0],[0]
We also note that embeddings computed offline turn out to lead to better performance than if computed online3.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"As an ablation test, we also evaluate our agents when they are provided an incorrect embedding (either all zeros, Emb-zero, or an embedding selected for a different random opponent, Emb-rand) and observe that such embeddings lead to a degradation in performance4.
3Perhaps, this is due to differences in the interactions of the opponents between themselves and with the new agent that the embedding network was not able to capture entirely.
",5.1.3. POLICY OPTIMIZATION,[0],[0]
"4Performance decrease is most significant for Emb-zero, which is an out-of-distribution all-zeros vector.
",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Finally, to evaluate strong generalization in the RL setting, we pit the newly trained baseline and agents with embedding-conditional policies against each other.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Since the embedding network has never seen the new agents, it must exhibit strong generalization to be useful in such setting.",5.1.3. POLICY OPTIMIZATION,[0],[0]
The results are give in Figures 5 and 6.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"Even though the margin is not very large, the agents that use Emb-Hyb perform the best on average.
5.2.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"The ParticleWorld environment
For the collaborative setting, we evaluate the framework on the ParticleWorld environment for cooperative communication (Mordatch & Abbeel, 2018; Lowe et al., 2017).",5.1.3. POLICY OPTIMIZATION,[0],[0]
The environment consists of a continuous 2D grid with 3 landmarks and two kinds of agents collaborating to navigate to a common landmark goal (Figure 1c).,5.1.3. POLICY OPTIMIZATION,[0],[0]
"At the beginning of every episode, the speaker agent is shown the RGB color of a single target landmark on the grid.",5.1.3. POLICY OPTIMIZATION,[0],[0]
The speaker then communicates a fixed length binary message to the listener agent.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"Based on the received messages, the listener agent the moves in a particular direction.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"The final reward, shared across the speaker and listener agents, is the distance of the listener to the target landmark after a fixed time horizon.
",5.1.3. POLICY OPTIMIZATION,[0],[0]
The agent-interaction graph for this environment is bipartite with only cross edges between speaker and listener agents.,5.1.3. POLICY OPTIMIZATION,[0],[0]
Every interaction edge in this graph corresponds to 1000 rollout episodes where the maximum length of any episode is 25 steps.,5.1.3. POLICY OPTIMIZATION,[0],[0]
We pretrain 28,5.1.3. POLICY OPTIMIZATION,[0],[0]
MLP parameterized speaker and listener agent policies.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"Every speaker learns through communication with only two different listeners
and vice-versa, giving an extremely sparse agent-interaction graph.",5.1.3. POLICY OPTIMIZATION,[0],[0]
We explicitly encoded diversity in these speakers and listener agents by masking bits in the communication channel.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"In particular, we masked 1 or 2 randomly selected bits for every speaker agent in the graph to give a total of( 7 1 ) + ( 7 2 ) = 28 distinct speaker agents.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"Depending on the neighboring speaker agents in the agent-interaction graph, the listener agents also show diversity in the learned policies.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"The policies are learned using multiagent deep deterministic policy gradients (MADDPG, Lowe et al., 2017).
",5.1.3. POLICY OPTIMIZATION,[0],[0]
"In this environment, the speakers and listeners are tightly coupled.",5.1.3. POLICY OPTIMIZATION,[0],[0]
Hence we vary the setup used previously in the competitive scenario.,5.1.3. POLICY OPTIMIZATION,[0],[0]
We wish to learn embeddings of listeners based on their interactions with speakers.,5.1.3. POLICY OPTIMIZATION,[0],[0]
"Since the agent-interaction graph is bipartite, we use the embeddings of listener agents to condition a shared policy network for the respective speaker agents.",5.1.3. POLICY OPTIMIZATION,[0],[0]
"For the weak generalization setting, we remove an outgoing edge from every listener agent in the original graph to obtain the training graph.",5.2.1. EMBEDDING ANALYSIS,[0],[0]
"In the case of strong generalization, we set aside 7 listener agents (and their outgoing edges) each for validation and testing while the representation function is learned on the remaining 14 listener agents and their interactions.",5.2.1. EMBEDDING ANALYSIS,[0],[0]
"The intra-inter clustering ratios are shown
in Table 2, and the projections of the embeddings learned using Emb-Hyb are visualized in Figure 3c and Figure 3d for weak and strong generalization respectively.",5.2.1. EMBEDDING ANALYSIS,[0],[0]
"In spite of the high degree of sparsity in the training graph, the intrainter clustering ratio for the test interaction embeddings is less than unity suggesting an agent-specific signal.",5.2.1. EMBEDDING ANALYSIS,[0],[0]
"Emb-id works particularly well in this environment, achieving best results for both weak and strong generalization.",5.2.1. EMBEDDING ANALYSIS,[0],[0]
"Here, we are interested in learning speaker agents that can communicate more effectively with a much wider range of listeners given knowledge of their embeddings.",5.2.2. POLICY OPTIMIZATION,[0],[0]
"Referring back to Figure 2, we learn a policy πψ for a speaker agent that conditions on the representation function fθ for the listener agents.",5.2.2. POLICY OPTIMIZATION,[0],[0]
"For cooperative communication, we consider interactions with 14 pre-trained listener agents split as 6 training, 4 validation, and 4 test agents.5 Similar to the competitive setting, we compare performance against a baseline speaker agent that does not have access to any privilege information about the listeners.",5.2.2. POLICY OPTIMIZATION,[0],[0]
We summarize the results for the best validated models during training and 100 interaction episodes per test listener agent across 5 initializations in Table 3.,5.2.2. POLICY OPTIMIZATION,[0],[0]
"From the results, we observe that online embedding based methods can generalize better than the baseline methods.",5.2.2. POLICY OPTIMIZATION,[0],[0]
"The baseline MADDPG achieves the lowest training error, but fails to generalize well enough and incurs a low average reward for the test listener agents.",5.2.2. POLICY OPTIMIZATION,[0],[0]
Agent modeling is a well-studied topic within multiagent systems.,6. Discussion & Related Work,[0],[0]
See Albrecht & Stone (2017) for an excellent recent survey on this subject.,6. Discussion & Related Work,[0],[0]
The vast majority of literature concerns with learning models for a specific predictive task.,6. Discussion & Related Work,[0],[0]
"Predictive tasks are typically defined over actions, goals, and beliefs of other agents (Stone & Veloso, 2000).",6. Discussion & Related Work,[0],[0]
"In competitive domains such as Poker and Go, such tasks are often integrated with domain-specific heuristics to model opponents and learn superior policies (Rubin & Watson, 2011; Mnih et al., 2015).",6. Discussion & Related Work,[0],[0]
"Similarly, intelligent tutoring systems take into account pedagogical features of students and teachers to accelerate learning of desired behaviors in a collaborative environment (McCalla et al., 2000).
",6. Discussion & Related Work,[0],[0]
"In this work, we proposed an approach for modeling agent behavior in multiagent systems through unsupervised representational learning of agent policies.",6. Discussion & Related Work,[0],[0]
"Since we sidestep any domain specific assumptions and learn in an unsupervised manner, our framework learns representations that are
5None of the methods considered were able to learn a nontrivial speaker agent when trained simultaneously with all 28 listener agents.",6. Discussion & Related Work,[0],[0]
"Hence, we simplified the problem by considering the 14 listener agents that attained the best rewards during pretraining.
useful for several downstream tasks.",6. Discussion & Related Work,[0],[0]
"This extends the use of deep neural networks in multiagent systems to applications beyond traditional reinforcement learning and predictive modeling (Mnih et al., 2015; Hoshen, 2017).
",6. Discussion & Related Work,[0],[0]
Both the generative and discriminative components of our framework have been explored independently in prior work.,6. Discussion & Related Work,[0],[0]
Imitation learning has been extensively studied in the singleagent setting and recent work by Le et al. (2017) proposes an algorithm for imitation in a coordinated multiagent system.,6. Discussion & Related Work,[0],[0]
"Wang et al. (2017) proposed an imitation learning algorithm for learning robust controllers with few expert demonstrations in a single-agent setting that conditions the policy network on an inference network, similar to the encoder in our framework.",6. Discussion & Related Work,[0],[0]
"In another recent work, Li et al. (2017) propose an algorithm for learning interpretable representations using generative adversarial imitation learning.",6. Discussion & Related Work,[0],[0]
"Agent identification which represents the discriminative term in the learning objective is inspired from triplet losses and Siamese networks that are used for learning representations of data using distance comparisons (Hoffer & Ailon, 2015).
",6. Discussion & Related Work,[0],[0]
A key contribution of this work is a principled methodology for evaluating generalization of representations in multiagent systems based on the graphs of the agent interactions.,6. Discussion & Related Work,[0],[0]
"Graphs are a fundamental abstraction for modeling relational data, such as the interactions arising in multiagent systems (Zhou et al., 2016a;b; Chen et al., 2017; Battaglia et al., 2016; Hoshen, 2017) and concurrent work proposes to learn such graphs directly from data (Kipf et al., 2018).",6. Discussion & Related Work,[0],[0]
"In this work, we presented a framework for learning representations of agent policies in multiagent systems.",7. Conclusion & Future Work,[0],[0]
The agent policies are accessed using a few interaction episodes with other agents.,7. Conclusion & Future Work,[0],[0]
Our learning objective is based on a novel combination of a generative component based on imitation learning and a discriminative component for distinguishing the embeddings of different agent policies.,7. Conclusion & Future Work,[0],[0]
"Our overall framework is unsupervised, sample-efficient, and domainagnostic, and hence can be readily extended to many environments and downstream tasks.",7. Conclusion & Future Work,[0],[0]
"Most importantly, we showed the role of these embeddings as privileged information for learning more adaptive agent policies in both collaborative and competitive settings.
",7. Conclusion & Future Work,[0],[0]
"In the future, we would like to explore multiagent systems with more than two agents participating in the interactions.",7. Conclusion & Future Work,[0],[0]
Semantic interpolation of policies directly in the embedded space in order to obtain a policy with desired behaviors quickly is another promising direction.,7. Conclusion & Future Work,[0],[0]
"Finally, it would be interesting to extend and evaluate the proposed framework to learn representations for history dependent policies such as those parameterized by long short-term memory networks.",7. Conclusion & Future Work,[0],[0]
"We are thankful to Lisa Lee, Daniel Levy, Jiaming Song, and everyone at OpenAI for helpful comments and discussion.",Acknowledgements,[0],[0]
AG is supported by a Microsoft Research PhD Fellowship.,Acknowledgements,[0],[0]
MA is partially supported by NIH R01GM114311.,Acknowledgements,[0],[0]
JKG is partially supported by the Army Research Laboratory through the Army High Performance Computing Research Center under Cooperative Agreement W911NF-07-2-0027.,Acknowledgements,[0],[0]
Modeling agent behavior is central to understanding the emergence of complex phenomena in multiagent systems.,abstractText,[0],[0]
Prior work in agent modeling has largely been task-specific and driven by handengineering domain-specific prior knowledge.,abstractText,[0],[0]
We propose a general learning framework for modeling agent behavior in any multiagent system using only a handful of interaction data.,abstractText,[0],[0]
Our framework casts agent modeling as a representation learning problem.,abstractText,[0],[0]
"Consequently, we construct a novel objective inspired by imitation learning and agent identification and design an algorithm for unsupervised learning of representations of agent policies.",abstractText,[0],[0]
"We demonstrate empirically the utility of the proposed framework in (i) a challenging highdimensional competitive environment for continuous control and (ii) a cooperative environment for communication, on supervised predictive tasks, unsupervised clustering, and policy optimization using deep reinforcement learning.",abstractText,[0],[0]
Learning Policy Representations in Multiagent Systems,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 674–679, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Topic models (Blei et al., 2003) have become standard tools for analyzing document collections, and topic analyses are quite common for social media (Paul and Dredze, 2011; Zhao et al., 2011; Hong and Davison, 2010; Ramage et al., 2010; Eisenstein et al., 2010).",1 Introduction,[0],[0]
"Their popularity owes in part to their data driven nature, allowing them to adapt to new corpora and languages.",1 Introduction,[0],[0]
"In social media especially, there is a large diversity in terms of both the topic and language, necessitating the modeling of multiple languages simultaneously.",1 Introduction,[0],[0]
"A good candidate for multi-lingual topic analyses are polylingual topic models (Mimno et al., 2009), which learn topics for multiple languages, creating tuples of language specific distributions over monolingual vocabularies for each topic.",1 Introduction,[0],[0]
"Polylingual topic models enable cross language analysis by grouping documents by topic regardless of language.
",1 Introduction,[0],[0]
Training of polylingual topic models requires parallel or comparable corpora: document tuples from multiple languages that discuss the same topic.,1 Introduction,[0],[0]
"While additional non-aligned documents
can be folded in during training, the “glue” documents are required to aid in the alignment across languages.",1 Introduction,[0],[0]
"However, the ever changing vocabulary and topics of social media (Eisenstein, 2013) make finding suitable comparable corpora difficult.",1 Introduction,[0],[0]
Standard techniques – such as relying on machine translation parallel corpora or comparable documents extracted from Wikipedia in different languages – fail to capture the specific terminology of social media.,1 Introduction,[0],[0]
"Alternate methods that rely on bilingual lexicons (Jagarlamudi and Daumé, 2010) similarly fail to adapt to shifting vocabularies.",1 Introduction,[0],[0]
"The result: an inability to train polylingual models on social media.
",1 Introduction,[0],[0]
"In this paper, we offer a solution: utilize codeswitched social media to discover correlations across languages.",1 Introduction,[0],[0]
"Social media is filled with examples of code-switching, where users switch between two or more languages, both in a conversation and even a single message (Ling et al., 2013).",1 Introduction,[0],[0]
"This mixture of languages in the same context suggests alignments between words across languages through the common topics discussed in the context.
",1 Introduction,[0],[0]
We learn from code-switched social media by extending the polylingual topic model framework to infer the language of each token and then automatically processing the learned topics to identify aligned topics.,1 Introduction,[0],[0]
"Our model improves both in terms of perplexity and a human evaluation, and we provide some example analyses of social media that rely on our learned topics.
674",1 Introduction,[0],[0]
Code-switched documents has received considerable attention in the NLP community.,2 Code-Switching,[0],[0]
"Several tasks have focused on identification and analysis, including mining translations in code-switched documents (Ling et al., 2013), predicting codeswitched points (Solorio and Liu, 2008a), identifying code-switched tokens (Lignos and Marcus, 2013; Yu et al., 2012; Elfardy and Diab, 2012), adding code-switched support to language models (Li and Fung, 2012), linguistic processing of code switched data (Solorio and Liu, 2008b), corpus creation (Li et al., 2012; Diab and Kamboj, 2011), and computational linguistic analyses and theories of code-switching (Sankofl, 1998; Joshi, 1982).
",2 Code-Switching,[0],[0]
Code-switching specifically in social media has also received some recent attention.,2 Code-Switching,[0],[0]
Lignos and Marcus (2013) trained a supervised token level language identification system for Spanish and English code-switched social media to study codeswitching behaviors.,2 Code-Switching,[0],[0]
"Ling et al. (2013) mined translation spans for Chinese and English in codeswitched documents to improve a translation system, relying on an existing translation model to aid in the identification and extraction task.",2 Code-Switching,[0],[0]
"In contrast to this work, we take an unsupervised approach, relying only on readily available document level language ID systems to utilize code-switched data.",2 Code-Switching,[0],[0]
"Additionally, our focus is not on individual messages, rather we aim to train a model that can be used to analyze entire corpora.
",2 Code-Switching,[0],[0]
"In this work we consider two types of codeswitched documents: single messages and conversations, and two language pairs: Chinese-English and Spanish-English.",2 Code-Switching,[0],[0]
"Figure 1 shows an example of a code-switched Spanish-English conversation, in which three users discuss Mexico’s football team advancing to the Gold medal game in the 2012 Summer Olympics.",2 Code-Switching,[0],[0]
"In this conversation, some tweets are code-switched and some are in a single language.",2 Code-Switching,[0],[0]
By collecting the entire conversation into a single document we provide the topic model with additional content.,2 Code-Switching,[0],[0]
"An example of a Chinese-English code-switched messages is given by Ling et al. (2013):
watup Kenny Mayne!!",2 Code-Switching,[0],[0]
-,2 Code-Switching,[0],[0]
"Kenny Mayne 最近这么样啊!!
",2 Code-Switching,[0],[0]
Here a user switches between languages in a single message.,2 Code-Switching,[0],[0]
"We empirically evaluate our model on
both conversations and messages.",2 Code-Switching,[0],[0]
In the model presentation we will refer to both as “documents.”,2 Code-Switching,[0],[0]
"To train a polylingual topic model on social media, we make two modifications to the model of Mimno et al. (2009): add a token specific language variable, and a process for identifying aligned topics.
",3 csLDA,[0],[0]
"First, polylingual topic models require parallel or comparable corpora in which each document has an assigned language.",3 csLDA,[0],[0]
"In the case of code-switched social media data, we require a pertoken language variable.",3 csLDA,[0],[0]
"However, while document level language identification (LID) systems are common place, very few languages have pertoken LID systems (King and Abney, 2013; Lignos and Marcus, 2013).
",3 csLDA,[0],[0]
"To address the lack of available LID systems, we add a per-token latent language variable to the polylingual topic model.",3 csLDA,[0],[0]
"For documents that are not code-switched, we observe these variables to be the output of a document level LID system.",3 csLDA,[0],[0]
"In the case of code-switched documents, these variables are inferred during model inference.
",3 csLDA,[0],[0]
"Second, polylingual topic models assume the aligned topics are from parallel or comparable corpora, which implicitly assumes that a topics popularity is balanced across languages.",3 csLDA,[0],[0]
Topics that show up in one language necessarily show up in another.,3 csLDA,[0],[0]
"However, in the case of social media, we can make no such assumption.",3 csLDA,[0],[0]
"The topics discussed are influenced by users, time, and location, all factors intertwined with choice of language.",3 csLDA,[0],[0]
"For example, English speakers will more likely discuss Olympic basketball while Spanish speakers football.",3 csLDA,[0],[0]
"There may be little or no documents on a given topic in one language, while they are plentiful in another.",3 csLDA,[0],[0]
"In this case, a polylingual topic model, which necessarily infers a topicspecific word distribution for each topic in each language, would learn two unrelated word distributions in two languages for a single topic.",3 csLDA,[0],[0]
"Therefore, naively using the produced topics as “aligned” across languages is ill-advised.
",3 csLDA,[0],[0]
Our solution is to automatically identify aligned polylingual topics after learning by examining a topic’s distribution across code-switched documents.,3 csLDA,[0],[0]
"Our metric relies on distributional properties of an inferred topic across the entire collection.
",3 csLDA,[0],[0]
"To summarize, based on the model of Mimno et al. (2009) we will learn:
• For each topic, a language specific word distribution.",3 csLDA,[0],[0]
•,3 csLDA,[0],[0]
"For each (code-switched) token, a language.",3 csLDA,[0],[0]
•,3 csLDA,[0],[0]
"For each topic, an identification as to whether
the topic captures an alignment across languages.
",3 csLDA,[0],[0]
The first two goals are achieved by incorporating new hidden variables in the traditional polylingual topic model.,3 csLDA,[0],[0]
The third goal requires an automated post-processing step.,3 csLDA,[0],[0]
We call the resulting model Code-Switched LDA (csLDA).,3 csLDA,[0],[0]
The generative process is as follows:,3 csLDA,[0],[0]
"• For each topic z ∈ T
• For each language l ∈",3 csLDA,[0],[0]
L,3 csLDA,[0],[0]
•,3 csLDA,[0],[0]
"Draw word distribution φlz∼Dir(βl)
",3 csLDA,[0],[0]
• For each document d ∈ D: • Draw a topic distribution θd ∼ Dir(α) •,3 csLDA,[0],[0]
Draw a language distribution ψd∼Dir(γ),3 csLDA,[0],[0]
•,3 csLDA,[0],[0]
For each token,3 csLDA,[0],[0]
i ∈,3 csLDA,[0],[0]
d: • Draw a topic zi ∼ θd •,3 csLDA,[0],[0]
"Draw a language li ∼ ψd • Draw a word wi ∼ φlz For monolingual documents, we fix li to the LID tag for all tokens.",3 csLDA,[0],[0]
"Additionally, we use a single background distribution for each language to capture stopwords; a control variable π, which follows a Dirichlet distribution with prior parameterized by δ, is introduced to decide the choice between background words and topic words following (Chemudugunta et al., 2006)1.",3 csLDA,[0],[0]
"We use asymmetric Dirichlet priors (Wallach et al., 2009), and let the optimization process learn the hyperparameters.",3 csLDA,[0],[0]
The graphical model is shown in Figure 2.,3 csLDA,[0],[0]
Inference for csLDA follows directly from LDA.,3.1 Inference,[0],[0]
A Gibbs sampler learns the word distributions φlz for each language and topic.,3.1 Inference,[0],[0]
We use a block Gibbs sampler to jointly sample topic and language variables for each token.,3.1 Inference,[0],[0]
"As is customary, we collapse out φ, θ and ψ.",3.1 Inference,[0],[0]
"The sampling posterior is:
P (zi, li|w, z−i, l−i, α, β, γ) ∝",3.1 Inference,[0],[0]
"(nl,zwi)−i + β
nl,z−i +Wβ × m
z,d −i + α
md−i + T α × o
l,d −i + γ
od−i + Lγ (1)
where (nl,zwi)−i is the number of times the type for word wi assigned to topic z and language l (ex-
1Omitted from the generative process but shown in Fig. 2.
cluding current word wi), m z,d −i is the number of tokens assigned to topic z in document d (excluding current word wi), o",3.1 Inference,[0],[0]
"l,d −i is the number of tokens assigned to language l in document d (excluding current word wi), and these variables with superscripts or subscripts omitted are totals across all values for the variable.",3.1 Inference,[0],[0]
W is the number of words in the corpus.,3.1 Inference,[0],[0]
All counts omit words assigned to the background.,3.1 Inference,[0],[0]
"During sampling, words are first assigned to the background/topic distribution and then topic and language are sampled for nonbackground words.
",3.1 Inference,[0],[0]
"We optimize the hyperparameters α, β, γ and δ by interleaving sampling iterations with a NewtonRaphson update to obtain the MLE estimate for the hyperparameters.",3.1 Inference,[0],[0]
"Taking α as an example, one step of the Newton-Raphson update is:
αnew = αold −H−1∂L ∂α
(2)
where H is the Hessian matrix and ∂L∂α is the gradient of the likelihood function with respect to the optimizing hyperparameter.",3.1 Inference,[0],[0]
We interleave 200 sampling iterations with one Newton-Raphson update.,3.1 Inference,[0],[0]
"We next identify learned topics (a set of related word-distributions) that truly represent an aligned topic across languages, as opposed to an unrelated set of distributions for which there is no supporting alignment evidence in the corpus.",3.2 Selecting Aligned Topics,[0],[0]
We begin by measuring how often each topic occurs in codeswitched documents.,3.2 Selecting Aligned Topics,[0],[0]
"If a topic never occurs in a code-switched document, then there can be no evidence to support alignment across languages.",3.2 Selecting Aligned Topics,[0],[0]
"For the topics that appear at least once in a codeswitched document, we estimate their probability
in the code-switched documents by a MAP estimate of θ.",3.2 Selecting Aligned Topics,[0],[0]
Topics appearing in at least one codeswitched document with probability greater than a threshold p are selected as candidates for true cross-language topics.,3.2 Selecting Aligned Topics,[0],[0]
"We used two datasets: a Sina Weibo ChineseEnglish corpus (Ling et al., 2013) and a SpanishEnglish Twitter corpus.
",4 Data,[0],[0]
"Weibo Ling et al. (2013) extracted over 1m Chinese-English parallel segments from Sina Weibo, which are code-switched messages.",4 Data,[0],[0]
"We randomly sampled 29,705 code-switched messages along with 42,116 Chinese and 42,116 English messages from the the same time frame.",4 Data,[0],[0]
We used these data for training.,4 Data,[0],[0]
"We then sampled an additional 2475 code-switched messages, 4221 English and 4211 Chinese messages as test data.
Olympics We collected tweets from July 27, 2012 to August 12, 2012, and identified 302,775 tweets about the Olympics based on related hashtags and keywords (e.g. olympics, #london2012, etc.)",4 Data,[0],[0]
We identified code-switched tweets using the Chromium Language Detector2.,4 Data,[0],[0]
This system provides the top three possible languages for a given document with confidence scores; we identify a tweet as code-switched if two predicted languages each have confidence greater than 33%.,4 Data,[0],[0]
"We then used the tagger of Lignos and Marcus (2013) to obtain token level LID tags, and only tweets with tokens in both Spanish and English are used as code-switched tweets.",4 Data,[0],[0]
In total we identified 822 Spanish-English code-switched tweets.,4 Data,[0],[0]
"We further expanded the mined tweets to full conversations, yielding 1055 Spanish-English codeswitched documents (including both tweets and conversations), along with 4007 English and 4421 Spanish tweets composes our data set.",4 Data,[0],[0]
We reserve 10% of the data for testing.,4 Data,[0],[0]
We evaluated csLDA on the two datasets and evaluated each model using perplexity on held out data and human judgements.,5 Experiments,[0],[0]
"While our goal is to learn polylingual topics, we cannot compare to previous polylingual models since they require comparable data, which we lack.",5 Experiments,[0],[0]
"Instead, we constructed a baseline from LDA run on the entire dataset (no
2https://code.google.com/p/chromium-compact-language-detector/
language information.)",5 Experiments,[0],[0]
"For each model, we measured the document completion perplexity (RosenZvi et al., 2004) on the held out data.",5 Experiments,[0],[0]
We experimented with different numbers of topics (T ).,5 Experiments,[0],[0]
"Since csLDA duplicates topic distributions (T ×L) we used twice as many topics for LDA.
",5 Experiments,[0],[0]
Figure 3 shows test perplexity for varying T and perplexity for the best setting of csLDA (T =60) and LDA (T =120).,5 Experiments,[0],[0]
"The table lists both monolingual and code-switched test data; csLDA improves over LDA in almost every case, and across all values of T .",5 Experiments,[0],[0]
"The background distribution (-bg) has mixed results for LDA, whereas for csLDA it shows consistent improvement.",5 Experiments,[0],[0]
Table 4 shows some csLDA topics.,5 Experiments,[0],[0]
"While there are some mistakes, overall the topics are coherent and aligned.
",5 Experiments,[0],[0]
"We use the available per-token LID system (Lignos and Marcus, 2013) for Spanish/English to justify csLDA’s ability to infer the hidden language variables.",5 Experiments,[0],[0]
"We ran csLDA-bg with li set to the value provided by the LID system for codeswitched documents (csLDA-bg with LID), which gives csLDA high quality LID labels.",5 Experiments,[0],[0]
"While we see gains for the code-switched data, overall the results for csLDA-bg and csLDA-bg with LID are similar, suggesting that the model can operate effectively even without a supervised per-token LID system.",5 Experiments,[0],[0]
"We evaluate topic alignment quality through a human judgements (Chang et al., 2009).",5.1 Human Evaluation,[0],[0]
"For each aligned topic, we show an annotator the 20 most frequent words from the foreign language topic (Chinese or Spanish) with the 20 most frequent words from the aligned English topic and two random English topics.",5.1 Human Evaluation,[0],[0]
The annotators are asked to select the most related English topic among the three; the one with the most votes is considered the aligned topic.,5.1 Human Evaluation,[0],[0]
"We count how often the model’s alignments agree.
",5.1 Human Evaluation,[0],[0]
LDA may learn comparable topics in different languages but gives no explicit alignments.,5.1 Human Evaluation,[0],[0]
We create alignments by classifying each LDA topic by language using the KL-divergence between the topic’s words distribution and a word distribution for the English/foreign language inferred from the monolingual documents.,5.1 Human Evaluation,[0],[0]
Language is assigned to a topic by taking the minimum KL.,5.1 Human Evaluation,[0],[0]
"For Weibo data, this was not effective since the vocabularies of each language are highly unbalanced.",5.1 Human Evaluation,[0],[0]
"Instead,
we manually labeled the topics by language.",5.1 Human Evaluation,[0],[0]
We then pair topics across languages using the cosine similarity of their co-occurrence statistics in codeswitched documents.,5.1 Human Evaluation,[0],[0]
Topic pairs with similarity above t are considered aligned topics.,5.1 Human Evaluation,[0],[0]
We also used a threshold p (§3.2) to select aligned topics in csLDA.,5.1 Human Evaluation,[0],[0]
"To ensure a fair comparison, we select the same number of aligned topics for LDA and csLDA.3.",5.1 Human Evaluation,[0],[0]
"We used the best performing setting: csLDA T =60, LDA T =120, which produced 12 alignments from Olympics and 28 from Weibo.
",5.1 Human Evaluation,[0],[0]
Using Mechanical Turk we collected multiple judgements per alignment.,5.1 Human Evaluation,[0],[0]
"For Spanish, we removed workers who disagreed with the majority more than 50% of the time (83 deletions), leaving 6.5 annotations for each alignment (85.47% inter-annotator agreement.)",5.1 Human Evaluation,[0],[0]
"For Chinese, since quality of general Chinese turkers is low (Pavlick et al., 2014)",5.1 Human Evaluation,[0],[0]
we invited specific workers and obtained 9.3 annotations per alignment (78.72% inter-annotator agreement.),5.1 Human Evaluation,[0],[0]
"For Olympics, LDA alignments matched the judgements 25% of the time, while csLDA matched 50% of the time.",5.1 Human Evaluation,[0],[0]
"While csLDA found 12 alignments and LDA 29, the 12 topics evaluated from both models show that csLDA’s alignments are higher quality.",5.1 Human Evaluation,[0],[0]
"For the Weibo data, LDA matched judgements 71.4%, while csLDA matched 75%.",5.1 Human Evaluation,[0],[0]
"Both obtained high
3We used thresholds p = 0.2 and t = 0.0001.",5.1 Human Evaluation,[0],[0]
"We limited the model with more alignments to match the one with less.
quality alignments – likely due both to the fact that the code-switched data is curated to find translations and we hand labeled topic language – but csLDA found many more alignments: 60 as compared to 28.",5.1 Human Evaluation,[0],[0]
These results confirm our automated results: csLDA finds higher quality topics that span both languages.,5.1 Human Evaluation,[0],[0]
"Code-switched documents are common in social media, providing evidence for polylingual topic models to infer aligned topics across languages.",abstractText,[0],[0]
"We present Code-Switched LDA (csLDA), which infers language specific topic distributions based on code-switched documents to facilitate multi-lingual corpus analysis.",abstractText,[0],[0]
"We experiment on two code-switching corpora (English-Spanish Twitter data and English-Chinese Weibo data) and show that csLDA improves perplexity over LDA, and learns semantically coherent aligned topics as judged by human anno-",abstractText,[0],[0]
Learning Polylingual Topic Models from Code-Switched Social Media Documents,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2289–2294, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Bilingual word embeddings have attracted a lot of attention in recent times (Zou et al., 2013; Kočiský et al., 2014; Chandar A P et al., 2014; Gouws et al., 2014; Gouws and Søgaard, 2015; Luong et al., 2015; Wick et al., 2016).",1 Introduction,[0],[0]
A common approach to obtain them is to train the embeddings in both languages independently and then learn a mapping that minimizes the distances between equivalences listed in a bilingual dictionary.,1 Introduction,[0],[0]
"The learned transformation can also be applied to words missing in the dictionary, which can be used to induce new translations with a direct application in machine translation (Mikolov et al., 2013b; Zhao et al., 2015).
",1 Introduction,[0],[0]
"The first method to learn bilingual word embedding mappings was proposed by Mikolov et al. (2013b), who learn the linear transformation that minimizes the sum of squared Euclidean distances for the dictionary entries.",1 Introduction,[0],[0]
"Subsequent work has proposed alternative optimization objectives to learn
better mappings.",1 Introduction,[0],[0]
"Xing et al. (2015) incorporate length normalization in the training of word embeddings and try to maximize the cosine similarity instead, introducing an orthogonality constraint to preserve the length normalization after the projection.",1 Introduction,[0],[0]
"Faruqui and Dyer (2014) use canonical correlation analysis to project the embeddings in both languages to a shared vector space.
",1 Introduction,[0],[0]
"Beyond linear mappings, Lu et al. (2015) apply deep canonical correlation analysis to learn a nonlinear transformation for each language.",1 Introduction,[0],[0]
"Finally, additional techniques have been used to address the hubness problem in Mikolov et al. (2013b), both through the neighbor retrieval method (Dinu et al., 2015) and the training itself (Lazaridou et al., 2015).",1 Introduction,[0],[0]
"We leave the study of non-linear transformation and other additions for further work.
",1 Introduction,[0],[0]
"In this paper, we propose a general framework to learn bilingual word embeddings.",1 Introduction,[0],[0]
"We start with a basic optimization objective (Mikolov et al., 2013b) and introduce several meaningful and intuitive constraints that are equivalent or closely related to previously proposed methods (Faruqui and Dyer, 2014; Xing et al., 2015).",1 Introduction,[0],[0]
"Our framework provides a more general view of bilingual word embedding mappings, showing the underlying connection between the existing methods, revealing some flaws in their theoretical justification and providing an alternative theoretical interpretation for them.",1 Introduction,[0],[0]
"Our experiments on an existing English-Italian word translation induction and an English word analogy task give strong empirical evidence in favor of our theoretical reasoning, while showing that one of our models clearly outperforms previous alternatives.
2289",1 Introduction,[0],[0]
Let X and Z denote the word embedding matrices in two languages for a given bilingual dictionary so that their ith row Xi∗ and Zi∗ are the word embeddings of the ith entry in the dictionary.,2 Learning bilingual mappings,[0],[0]
"Our goal is to find a linear transformation matrix W so that XW best approximates Z, which we formalize minimizing the sum of squared Euclidean distances following Mikolov et al. (2013b):
arg min W
∑
i
‖Xi∗W",2 Learning bilingual mappings,[0],[0]
"− Zi∗‖2
Alternatively, this is equivalent to minimizing the (squared) Frobenius norm of the residual matrix:
arg min W
‖XW",2 Learning bilingual mappings,[0],[0]
"− Z‖2F
Consequently, W will be the so called leastsquares solution of the linear matrix equation XW = Z.",2 Learning bilingual mappings,[0],[0]
This is a well-known problem in linear algebra and can be solved by taking the MoorePenrose pseudoinverse X+ =,2 Learning bilingual mappings,[0],[0]
( XTX )−1,2 Learning bilingual mappings,[0],[0]
"XT as W = X+Z, which can be computed using SVD.",2 Learning bilingual mappings,[0],[0]
"Monolingual invariance is needed to preserve the dot products after mapping, avoiding performance degradation in monolingual tasks (e.g. analogy).",2.1 Orthogonality for monolingual invariance,[0],[0]
This can be obtained requiring W to be an orthogonal matrix (W TW = I).,2.1 Orthogonality for monolingual invariance,[0],[0]
"The exact solution under such orthogonality constraint is given by W = V UT , where ZTX = UΣV T is the SVD factorization of ZTX (cf. Appendix A).",2.1 Orthogonality for monolingual invariance,[0],[0]
"Thanks to this, the optimal transformation can be efficiently computed in linear time with respect to the vocabulary size.",2.1 Orthogonality for monolingual invariance,[0],[0]
"Note that orthogonality enforces an intuitive property, and as such it could be useful to avoid degenerated solutions and learn better bilingual mappings, as we empirically show in Section 3.",2.1 Orthogonality for monolingual invariance,[0],[0]
Normalizing word embeddings in both languages to be unit vectors guarantees that all training instances contribute equally to the optimization goal.,2.2 Length normalization for maximum cosine,[0],[0]
"As long as W is orthogonal, this is equivalent to maximizing the sum of cosine similarities for the dictionary
entries, which is commonly used for similarity computations:
arg min W
∑
i
∥∥∥∥ Xi∗",2.2 Length normalization for maximum cosine,[0],[0]
‖Xi∗‖ W − Zi∗‖Zi∗‖ ∥∥∥∥,2.2 Length normalization for maximum cosine,[0],[0]
"2
= arg max W
∑
i
cos (Xi∗W,Zi∗)
",2.2 Length normalization for maximum cosine,[0],[0]
"This last optimization objective coincides with Xing et al. (2015), but their work was motivated by an hypothetical inconsistency in Mikolov et al. (2013b), where the optimization objective to learn word embeddings uses dot product, the objective to learn mappings uses Euclidean distance and the similarity computations use cosine.",2.2 Length normalization for maximum cosine,[0],[0]
"However, the fact is that, as long as W is orthogonal, optimizing the squared Euclidean distance of length-normalized embeddings is equivalent to optimizing the cosine, and therefore, the mapping objective proposed by Xing et al. (2015) is equivalent to that used by Mikolov et al. (2013b) with orthogonality constraint and unit vectors.",2.2 Length normalization for maximum cosine,[0],[0]
"In fact, our experiments show that orthogonality is more relevant than length normalization, in contrast to Xing et al. (2015), who introduce orthogonality only to ensure that unit length is preserved after mapping.",2.2 Length normalization for maximum cosine,[0],[0]
"Dimension-wise mean centering captures the intuition that two randomly taken words would not be expected to be semantically similar, ensuring that the expected product of two random embeddings in any dimension and, consequently, their cosine similarity, is zero.",2.3 Mean centering for maximum covariance,[0],[0]
"As long as W is orthogonal, this is equivalent to maximizing the sum of dimensionwise covariance for the dictionary entries:
arg min W
‖CmXW",2.3 Mean centering for maximum covariance,[0],[0]
"− CmZ‖2F
= arg max W
∑
i
cov (XW∗i, Z∗i)
where Cm denotes the centering matrix This equivalence reveals that the method proposed by Faruqui and Dyer (2014) is closely related to our framework.",2.3 Mean centering for maximum covariance,[0],[0]
"More concretely, Faruqui and Dyer (2014) use Canonical Correlation Analysis (CCA) to project the word embeddings in both languages to a shared vector space.",2.3 Mean centering for maximum covariance,[0],[0]
"CCA maximizes
the dimension-wise covariance of both projections (which is equivalent to maximizing the covariance of a single projection if the transformations are constrained to be orthogonal, as in our case) but adds an implicit restriction to the two mappings, making different dimensions have the same variance and be uncorrelated among themselves1:
arg max A,B
∑
i
cov (",2.3 Mean centering for maximum covariance,[0],[0]
"XA∗i, ZB∗i)
s.t. ATXTCmXA = BTZTCmZB = I
Therefore, the only fundamental difference between both methods is that, while our model enforces monolingual invariance, Faruqui and Dyer (2014) do change the monolingual embeddings to meet this restriction.",2.3 Mean centering for maximum covariance,[0],[0]
"In this regard, we think that the restriction they add could have a negative impact on the learning of the bilingual mapping, and it could also degrade the quality of the monolingual embeddings.",2.3 Mean centering for maximum covariance,[0],[0]
Our experiments (cf. Section 3) show empirical evidence supporting this idea.,2.3 Mean centering for maximum covariance,[0],[0]
"In this section, we experimentally test the proposed framework and all its variants in comparison with related methods.",3 Experiments,[0],[0]
"For that purpose, we use the translation induction task introduced by Mikolov et al. (2013b), which learns a bilingual mapping on a small dictionary and measures its accuracy on predicting the translation of new words.",3 Experiments,[0],[0]
"Unfortunately, the dataset they use is not public.",3 Experiments,[0],[0]
"For that reason, we use the English-Italian dataset on the same task provided by Dinu et al. (2015)2.",3 Experiments,[0],[0]
"The dataset contains monolingual word embeddings trained with the word2vec toolkit using the CBOW method with negative sampling (Mikolov et al., 2013a)3.",3 Experiments,[0],[0]
"The English embeddings were trained on a 2.8 billion word corpus (ukWaC + Wikipedia + BNC), while the 1.6 billion word corpus itWaC was used to train the Italian
1While CCA is typically defined in terms of correlation (thus its name), correlation is invariant to the scaling of variables, so it is possible to constrain the canonical variables to have a fixed variance, as we do, in which case correlation and covariance become equivalent
2http://clic.cimec.unitn.it/˜georgiana.",3 Experiments,[0],[0]
"dinu/down/
3The context window was set to 5 words, the dimension of the embeddings to 300, the sub-sampling to 1e-05 and the number of negative samples to 10
embeddings.",3 Experiments,[0],[0]
"The dataset also contains a bilingual dictionary learned from Europarl, split into a training set of 5,000 word pairs and a test set of 1,500 word pairs, both of them uniformly distributed in frequency bins.",3 Experiments,[0],[0]
"Accuracy is the evaluation measure.
",3 Experiments,[0],[0]
"Apart from the performance of the projected embeddings in bilingual terms, we are also interested in the monolingual quality of the source language embeddings after the projection.",3 Experiments,[0],[0]
"For that purpose, we use the word analogy task proposed by Mikolov et al. (2013a), which measures the accuracy on answering questions like “what is the word that is similar to small in the same sense as biggest is similar to big?” using simple word vector arithmetic.",3 Experiments,[0],[0]
"The dataset they use consists of 8,869 semantic and 10,675 syntactic questions of this type, and is publicly available4.",3 Experiments,[0],[0]
"In order to speed up the experiments, we follow the authors and perform an approximate evaluation by reducing the vocabulary size according to a frequency threshold of 30,000 (Mikolov et al., 2013a).",3 Experiments,[0],[0]
Since the original embeddings are the same in all the cases and it is only the transformation that is applied to them,3 Experiments,[0],[0]
"that changes, this affects all the methods in the exact same way, so the results are perfectly comparable among themselves.",3 Experiments,[0],[0]
"With these settings, we obtain a coverage of 64.98%.
",3 Experiments,[0],[0]
"We implemented the proposed method in Python using NumPy, and make it available as an open source project5.",3 Experiments,[0],[0]
"The code for Mikolov et al. (2013b) and Xing et al. (2015) is not publicly available, so we implemented and tested them as part of the proposed framework, which only differs from the original systems in the optimization method (exact solution instead of gradient descent) and the length normalization approach in the case of Xing et al. (2015) (postprocessing instead of constrained training).",3 Experiments,[0],[0]
"As for the method by Faruqui and Dyer (2014), we used their original implementation in Python and MATLAB6, which we extended to cover cases where the dictionary contains more than one entry for the same word.
",3 Experiments,[0],[0]
"4https://code.google.com/archive/p/ word2vec/
5https://github.com/artetxem/vecmap 6https://github.com/mfaruqui/
crosslingual-cca",3 Experiments,[0],[0]
"The rows in Table 1 show, respectively, the results for the original embeddings, the basic mapping proposed by Mikolov et al. (2013b) (cf. Section 2) and the addition of orthogonality constraint (cf. Section 2.1), with and without length normalization and, incrementally, mean centering.",3.1 Results of our framework,[0],[0]
"In all the cases, length normalization and mean centering were applied to all embeddings, even if missing from the dictionary.
",3.1 Results of our framework,[0],[0]
"The results show that the orthogonality constraint is key to preserve monolingual performance, and it also improves bilingual performance by enforcing a relevant property (monolingual invariance) that the transformation to learn should intuitively have.",3.1 Results of our framework,[0],[0]
"The contribution of length normalization alone is marginal, but when followed by mean centering we obtain further improvements in bilingual performance without hurting monolingual performance.",3.1 Results of our framework,[0],[0]
Table 2 shows the results for our best performing configuration in comparison to previous work.,3.2 Comparison to other work,[0],[0]
"As discussed before, (Mikolov et al., 2013b) and (Xing et al., 2015) were implemented as part of our framework, so they correspond to our uncostrained mapping with no preprocessing and orthogonal mapping with length normalization, respectively.
",3.2 Comparison to other work,[0],[0]
"As it can be seen, the method by Xing et al. (2015) performs better than that of Mikolov et al. (2013b) in the translation induction task, which is in line with what they report in their paper.",3.2 Comparison to other work,[0],[0]
"Moreover, thanks to the orthogonality constraint their monolingual performance in the word analogy task does not degrade, whereas the accuracy of Mikolov et al. (2013b) drops by 2.86% in absolute terms with respect to the original embeddings.
",3.2 Comparison to other work,[0],[0]
"Since Faruqui and Dyer (2014) take advantage of
CCA to perform dimensionality reduction, we tested several values for it and report the best (180 dimensions).",3.2 Comparison to other work,[0],[0]
"This beats the method by Xing et al. (2015) in the bilingual task, although it comes at the price of a considerable degradation in monolingual quality.
",3.2 Comparison to other work,[0],[0]
"In any case, it is our proposed method with the orthogonality constraint and a global preprocessing with length normalization followed by",3.2 Comparison to other work,[0],[0]
dimensionwise mean centering that achieves the best accuracy in the word translation induction task.,3.2 Comparison to other work,[0],[0]
"Moreover, it does not suffer from any considerable degradation in monolingual quality, with an anecdotal drop of only 0.07% in contrast with 2.86% for Mikolov et al. (2013b) and 7.02% for Faruqui and Dyer (2014).
",3.2 Comparison to other work,[0],[0]
"When compared to Xing et al. (2015), our results in Table 1 reinforce our theoretical interpretation for their method (cf. Section 2.2), as it empirically shows that its improvement with respect to Mikolov et al. (2013b) comes solely from the orthogonality constraint, and not from solving any inconsistency.
",3.2 Comparison to other work,[0],[0]
It should be noted that the implementation by Faruqui and Dyer (2014) also length-normalizes the word embeddings in a preprocessing step.,3.2 Comparison to other work,[0],[0]
"Following the discussion in Section 2.3, this means that our best performing configuration is conceptually very close to the method by Faruqui and Dyer (2014), as they both coincide on maximizing the average dimension-wise covariance and length-normalize the embeddings in both languages first, the only difference being that our model enforces monolingual invariance after the normalization while theirs does change the monolingual embeddings to make different dimensions have the same variance and be uncorrelated among themselves.",3.2 Comparison to other work,[0],[0]
"However, our model performs considerably better than any configuration from Faruqui and Dyer (2014) in both the monolingual and the bilingual task, supporting our hypothesis that these two constraints that are implicit in their method are not only conceptually confusing,
but also have a negative impact.",3.2 Comparison to other work,[0],[0]
"This paper develops a new framework to learn bilingual word embedding mappings, generalizing previous work and providing an efficient exact method to learn the optimal transformation.",4 Conclusions,[0],[0]
Our experiments show the effectiveness of the proposed model and give strong empirical evidence in favor of our reinterpretation of Xing et al. (2015) and Faruqui and Dyer (2014).,4 Conclusions,[0],[0]
"It is the proposed method with the orthogonality constraint and a global preprocessing with length normalization and dimension-wise mean centering that achieves the best overall results both in monolingual and bilingual terms, surpassing those previous methods.",4 Conclusions,[0],[0]
"In the future, we would like to study non-linear mappings (Lu et al., 2015) and the additional techniques in (Lazaridou et al., 2015).",4 Conclusions,[0],[0]
"This research was partially supported by the European Commision (QTLeap FP7-ICT-2013-10610516), a Google Faculty Award, and the Spanish Ministry of Economy and Competitiveness (TADEEP TIN2015-70214-P).",Acknowledgments,[0],[0]
"Mikel Artetxe enjoys a doctoral grant from the Spanish Ministry of Education, Culture and Sports.",Acknowledgments,[0],[0]
"Constraining W to be orthogonal (W TW = I), the original minimization problem can be reformulated as follows (cf. Section 2.1):
arg min W
∑
i
‖Xi∗W",A Proof of solution under orthogonality,[0],[0]
"− Zi∗‖2
= arg min W
∑
i
( ‖Xi∗W‖2 + ‖Zi∗‖2 − 2Xi∗WZTi∗ )
= arg max W
∑
i
Xi∗WZTi∗
= arg max W
Tr ( XWZT )
",A Proof of solution under orthogonality,[0],[0]
"= arg max W
Tr ( ZTXW )
",A Proof of solution under orthogonality,[0],[0]
"In the above expression, Tr(·) denotes the trace operator (the sum of all the elements in the main diagonal), and the last equality is given by its cyclic
property.",A Proof of solution under orthogonality,[0],[0]
"At this point, we can take the SVD of ZTX as ZTX = UΣV T , so Tr ( ZTXW ) =",A Proof of solution under orthogonality,[0],[0]
Tr ( UΣV TW ) =,A Proof of solution under orthogonality,[0],[0]
Tr ( ΣV TWU ) .,A Proof of solution under orthogonality,[0],[0]
"Since V T , W and U are orthogonal matrices, their product V TWU will also be an orthogonal matrix.",A Proof of solution under orthogonality,[0],[0]
"In addition to that, given that Σ is a diagonal matrix, its trace after an orthogonal transformation will be maximal when the values in its main diagonal are preserved after the mapping, that is, when the orthogonal transformation matrix is the identity matrix.",A Proof of solution under orthogonality,[0],[0]
"This will happen when V TWU = I in our case, so the optimal solution will be W = V UT .",A Proof of solution under orthogonality,[0],[0]
Mapping word embeddings of different languages into a single space has multiple applications.,abstractText,[0],[0]
"In order to map from a source space into a target space, a common approach is to learn a linear mapping that minimizes the distances between equivalences listed in a bilingual dictionary.",abstractText,[0],[0]
"In this paper, we propose a framework that generalizes previous work, provides an efficient exact method to learn the optimal linear transformation and yields the best bilingual results in translation induction while preserving monolingual performance in an analogy task.",abstractText,[0],[0]
Learning principled bilingual mappings of word embeddings while preserving monolingual invariance,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1297–1307 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1297",text,[0],[0]
"Every day, people go to different places to accomplish goals.",1 Introduction,[0],[0]
"People go to stores to buy clothing, go to restaurants to eat, and go to the doctor for medical services.",1 Introduction,[0],[0]
"People travel to specific destinations to enjoy the beach, go skiing, or see historical sites.",1 Introduction,[0],[0]
"For most places, people typically go there for a common set of reasons, which we will refer to as prototypical goal activities (goal-acts) for a location.",1 Introduction,[0],[0]
"For example, a prototypical goal-act for restaurants would be “eat food” and for IKEA would be “buy furniture”.
",1 Introduction,[0],[0]
"Previous research has established that recognizing people’s goals is essential for narrative text understanding and story comprehension (Schank and Abelson, 1977; Wilensky, 1978; Lehnert, 1981; Elson and McKeown, 2010; Goyal et al., 2013).
",1 Introduction,[0],[0]
Goals and plans are essential to understand people’s behavior and we use our knowledge of prototypical goals to make inferences when reading.,1 Introduction,[0],[0]
"For example, consider the following pair of sentences: “Mary went to the supermarket.",1 Introduction,[0],[0]
She needed milk.”,1 Introduction,[0],[0]
"Most people will infer that Mary purchased milk, unless told otherwise.",1 Introduction,[0],[0]
But a purchase event is not explicitly mentioned.,1 Introduction,[0],[0]
"In contrast, a similar sentence pair “Mary went to the theatre.",1 Introduction,[0],[0]
She needed milk.” feels incongruent and does not produce that inference.,1 Introduction,[0],[0]
Recognizing goals is also critical for conversational dialogue systems.,1 Introduction,[0],[0]
"For example, if a friend tells you that they went to a restaurant, you might reply “What did you eat?”, but if a friend says that they went to Yosemite, a more appropriate response might be “Did you hike?” or “Did you see the waterfalls?”.
",1 Introduction,[0],[0]
Our knowledge of prototypical goal activities also helps us resolve semantic ambiguity.,1 Introduction,[0],[0]
"For example, consider the following sentences:
(a) She went to the kitchen and got chicken.",1 Introduction,[0],[0]
(b) She went to the supermarket and got chicken.,1 Introduction,[0],[0]
"(c) She went to the restaurant and got chicken.
",1 Introduction,[0],[0]
"In sentence (a), we infer that she retrieved chicken (e.g., from the refrigerator) but did not pay for it.",1 Introduction,[0],[0]
"In (b), we infer that she paid for the chicken but probably did not eat it at the supermarket.",1 Introduction,[0],[0]
"In (c), we infer that she ate the chicken at the restaurant.",1 Introduction,[0],[0]
"Note how the verb “got” maps to different presumed events depending on the location.
",1 Introduction,[0],[0]
Our research aims to learn the prototypical goalacts for locations using a text corpus.,1 Introduction,[0],[0]
"First, we extract activities that co-occur with locations in goaloriented syntactic patterns.",1 Introduction,[0],[0]
"Next, we construct an activity profile matrix that consists of an activity vector (profile) for each of the locations.",1 Introduction,[0],[0]
"We then apply a semi-supervised label propagation algorithm to iteratively revise the activity profile strengths based on a small set of labeled locations.
",1 Introduction,[0],[0]
We also incorporate external resources to measure similarity between different activity expressions.,1 Introduction,[0],[0]
Our results show that this semi-supervised learning approach outperforms several baseline methods in identifying the prototypical goal activities for locations.,1 Introduction,[0],[0]
"Recognizing plans and goals is fundamental to narrative story understanding (Schank and Abelson, 1977; Bower, 1982).",2 Related Work,[0],[0]
"Conceptual knowledge structures developed in prior work have shown the importance of this type of knowledge, including plans (Wilensky, 1978), goal trees (Carbonell, 1979), and plot units (Lehnert, 1981).",2 Related Work,[0],[0]
"Wilensky’s research aimed to understand the actions of characters in stories by analyzing their goals, and their plans to accomplish those goals.",2 Related Work,[0],[0]
"For example, someone’s goal might be to obtain food with a plan to go to a restaurant.",2 Related Work,[0],[0]
"Our work aims to learn prototypical goals associated with a location, to support similar inference capabilities during story understanding.
",2 Related Work,[0],[0]
"Goals and plans can also function to trigger scripts (Cullingford, 1978), such as the $RESTAURANT script.",2 Related Work,[0],[0]
"There has been growing interest in learning narrative event chains and script knowledge from large text corpora (e.g., (Chambers and Jurafsky, 2008, 2009; Jans et al., 2012; Pichotta and Mooney, 2014, 2016)).",2 Related Work,[0],[0]
"In addition, Goyal et al. (2010; 2013) developed a system to automatically produce plot unit representations for short stories.",2 Related Work,[0],[0]
"A manual analysis of their stories revealed that 61% of Positive/Negative Affect States originated from completed plans and goals, and 46% of Mental Affect States originated from explicitly stated or inferred plans and goals.
",2 Related Work,[0],[0]
Elson & McKeown (2010) included plans and goals in their work on creating extensive story bank annotations that capture the knowledge needed to understand narrative structure.,2 Related Work,[0],[0]
"Researchers have also begun to explore NLP methods for recognizing the goals, desires, and plans
of characters in stories.",2 Related Work,[0],[0]
"Recent work has explored techniques to detect wishes (desires) in natural language text (Goldberg et al., 2009) and identify desire fulfillment (Chaturvedi et al., 2016; Rahimtoroghi et al., 2017).
",2 Related Work,[0],[0]
"Graph-based semi-supervised learning has been successfully used for many tasks, including sentiment analysis (Rao and Ravichandran, 2009; Feng et al., 2013), affective event recognition (Ding and Riloff, 2016) and class-instance extraction (Talukdar and Pereira, 2010).",2 Related Work,[0],[0]
"The semi-supervised learning algorithm used in this paper is modeled after a framework developed by Zhu et al. (2003) based on harmonic energy minimization and a label propagation algorithm described in (Zhu and Ghahramani, 2002).",2 Related Work,[0],[0]
Our aim is to learn the most prototypical goal-acts for locations.,3 Learning Prototypical Goal Activities,[0],[0]
"To tackle this problem, we first extract locations and related activities from a large text corpus.",3 Learning Prototypical Goal Activities,[0],[0]
Then we use a semi-supervised learning method to identify the goal activities for individual locations.,3 Learning Prototypical Goal Activities,[0],[0]
In the following sections we describe these processes in detail.,3 Learning Prototypical Goal Activities,[0],[0]
"To collect information about locations and activities, we use the 2011 Spinn3r dataset (Burton et al., 2011).",3.1 Location and Activity Extraction,[0],[0]
"Since our interest is learning about the activities of ordinary people in their daily lives, we use the Weblog subset of the Spinn3r corpus, which contains over 133 million blog posts.
",3.1 Location and Activity Extraction,[0],[0]
We use the text data to identify activities that are potential goal-acts for a location.,3.1 Location and Activity Extraction,[0],[0]
"However we also need to identify locations and want to include both proper names (e.g., Disneyland) as well as nominals (e.g., store, beach), so Named Entity Recognition will not suffice.",3.1 Location and Activity Extraction,[0],[0]
"Consequently, we extract (Loc,Act) pairs using syntactic patterns.
",3.1 Location and Activity Extraction,[0],[0]
"First, we apply the Stanford dependency parser (Manning et al., 2014).",3.1 Location and Activity Extraction,[0],[0]
"We then extract sentences that match the pattern “go to X to Y ” with the
following conditions: (1) there exists a subject connecting to “go”, (2) X has an nmod (nominal modifier) relation to “go” (lemma), (3)X is a noun or noun compound, (4) Y has an xcomp relation (open clausal complement) with “go”, and (5) Y is a verb.",3.1 Location and Activity Extraction,[0],[0]
"Figure 1 depicts the intended syntactic structure, which we will informally call the “go to” pattern.",3.1 Location and Activity Extraction,[0],[0]
"For sentences that match this pattern, we extract X as a location and Y as an activity.",3.1 Location and Activity Extraction,[0],[0]
"If the verb is followed by a particle and/or noun phrase (NP), then we also include the particle and head noun of the NP.",3.1 Location and Activity Extraction,[0],[0]
"For example, we extract activities such as “pray”, “clean up”, and “buy sweater”.
",3.1 Location and Activity Extraction,[0],[0]
This syntactic structure was chosen to identify activities that are described as being the reason why someone went to the location.,3.1 Location and Activity Extraction,[0],[0]
However it is not perfect.,3.1 Location and Activity Extraction,[0],[0]
"In some cases, X is not a location (e.g., “go to great lengths to ...”",3.1 Location and Activity Extraction,[0],[0]
"yields “lengths” as a location), or Y is not a goal-act for X (e.g., “go to the office to retrieve my briefcase ...”",3.1 Location and Activity Extraction,[0],[0]
yields “retrieve briefcase” which is not a prototypical goal for “office”).,3.1 Location and Activity Extraction,[0],[0]
"Interestingly, the pattern extracts some nominals that are not locations in a strict sense, but behave as locations.",3.1 Location and Activity Extraction,[0],[0]
"For example, “go to the doctor” extracts “doctor” as a location.",3.1 Location and Activity Extraction,[0],[0]
"Literally a doctor is a person, but in this context it really refers to the doctor’s office, which is a location.",3.1 Location and Activity Extraction,[0],[0]
"The pattern also extracts entities such as “roof”, which are not generally thought of as locations but do have a fixed physical location.",3.1 Location and Activity Extraction,[0],[0]
"Other extracted entities are virtual but function as locations, such as “Internet”.",3.1 Location and Activity Extraction,[0],[0]
"For the purposes of this work, we use the term location in a general sense to include any place or object that has a physical, virtual or implied location.
",3.1 Location and Activity Extraction,[0],[0]
"The “go to” pattern worked quite well at extracting (Loc,Act) pairs, but in relatively small quantities due to the very specific nature of the syntactic structure.",3.1 Location and Activity Extraction,[0],[0]
So we tried to find additional activities for those locations.,3.1 Location and Activity Extraction,[0],[0]
"Initially, we tried harvesting activities that occurred in close proximity (within 5 words) to a known location, but the results were
too noisy.",3.1 Location and Activity Extraction,[0],[0]
"Instead, we used the pattern “Y in/at X” with the same syntactic constraints for Y (the extracted activity) and X (a location extracted by the “go to” pattern).
",3.1 Location and Activity Extraction,[0],[0]
"We discovered many sentences in the corpus that were exactly or nearly the same, differing only by a few words, which resulted in artificially high frequency counts for some (Loc,Act) pairs.",3.1 Location and Activity Extraction,[0],[0]
"So we filtered duplicate or near-duplicate sentences by computing the longest common substring of sentence pairs that extracted the same (Loc,Act).",3.1 Location and Activity Extraction,[0],[0]
"If the shared substring had length ≥ 5, then we discarded the “duplicate” sentence.
",3.1 Location and Activity Extraction,[0],[0]
"Finally, we applied three filters.",3.1 Location and Activity Extraction,[0],[0]
"To keep the size of the data manageable, we discarded locations and activities that were each extracted with frequency < 30 by our patterns.",3.1 Location and Activity Extraction,[0],[0]
"And we manually filtered locations that are Named Entities corresponding to cities or larger geo-political regions (e.g., provinces or countries).",3.1 Location and Activity Extraction,[0],[0]
Large regions defined by government boundaries fall outside the scope of our task because the set of activities that typically occur in (say) a city or country is so broad.,3.1 Location and Activity Extraction,[0],[0]
"Finally, we added a filter to try to remove extremely general activities that can occur almost anywhere (e.g., visit).",3.1 Location and Activity Extraction,[0],[0]
"If an activity co-occurred with > 20% of the extracted (distinct) locations, then we discarded it.
",3.1 Location and Activity Extraction,[0],[0]
"After these filters, we extracted 451 distinct locations, 5143 distinct activities, roughly 200, 000 distinct (Loc,Act) pairs, and roughly 500, 000 instances of (Loc,Act) pairs.",3.1 Location and Activity Extraction,[0],[0]
"We define an activity profile matrix Y of size n×m, where n is the number of distinct locations andm is the number of distinct activities.",3.2 Activity Profiles for Locations,[0],[0]
"Yi,j represents the strength of the jth activity aj being a goal-act for li.",3.2 Activity Profiles for Locations,[0],[0]
We use yi ∈ Rm to denote the ith row of Y .,3.2 Activity Profiles for Locations,[0],[0]
"Table 1 shows an illustration of (partial) activity profiles for four locations.1 Our goal is
1Not actual values, for illustration only.
to learn the Yi,j values so that activities with high strength are truly goal-acts for location li.
",3.2 Activity Profiles for Locations,[0],[0]
We could build the activity profile for location li using the co-occurrence data extracted from the blog corpus.,3.2 Activity Profiles for Locations,[0],[0]
"For example, we could estimate P (aj | li) directly from the frequency counts of the activities extracted for li.",3.2 Activity Profiles for Locations,[0],[0]
"However, a high co-occurrence frequency doesn’t necessarily mean that the activity represents a prototypical goal.",3.2 Activity Profiles for Locations,[0],[0]
"For example, the activity “have appointment” frequently co-occurs with “clinic” but doesn’t reveal the underlying reason for going to the clinic (e.g., probably to see a doctor or undergo a medical test).",3.2 Activity Profiles for Locations,[0],[0]
"To appreciate the distinction, imagine that you asked a friend why she went to a health clinic, and she responded with “because I had an appointment”.",3.2 Activity Profiles for Locations,[0],[0]
"You would likely view her response as being snarky or evasive (i.e., she didn’t want to tell you the reason).",3.2 Activity Profiles for Locations,[0],[0]
"In Section 4, we will evaluate this approach as a baseline and show that it does not perform well.",3.2 Activity Profiles for Locations,[0],[0]
"Our aim is to learn the activity profiles for locations using a small amount of labeled data, so we frame this problem as a semi-supervised learning task.",3.3 Semi-Supervised Learning of Goal-Act Probabilities,[0],[0]
"Given a small number of “seed” locations coupled with predefined goal-acts, we want to learn the goal-acts for new locations.",3.3 Semi-Supervised Learning of Goal-Act Probabilities,[0],[0]
"We use li ∈ L to represent location li, where |L| = n.",3.3.1 Location Similarity Graph,[0],[0]
"We define an undirected graph G = (V,E) with vertices representing locations (|V | = n) and edgesE = V ×V , such that each pair of vertices vi",3.3.1 Location Similarity Graph,[0],[0]
"and vk is connected with an edge eik whose weight represents the similarity between li and lk.
",3.3.1 Location Similarity Graph,[0],[0]
We can then represent the edge weights as an n × n symmetric weight matrix W indicating the similarity between locations.,3.3.1 Location Similarity Graph,[0],[0]
"There could be many ways to define the weights, but for now we use the following definition from (Zhu et al., 2003), where σ2 is a hyper-parameter2:
Wi,k = exp ( − 1 σ2 (1− sim (li, lk)) )
",3.3.1 Location Similarity Graph,[0],[0]
"(1)
To assess the similarity between locations, we measure the cosine similarity between vectors of their co-occurrence frequencies with activities.",3.3.1 Location Similarity Graph,[0],[0]
"Specifically, let matrix Fn×m = [f1, ..., fn]T
2We use the same value σ2 = 0.03 as (Zhu et al., 2003).
",3.3.1 Location Similarity Graph,[0],[0]
"where fi is a vector of length m capturing the co-occurrence frequencies between location li and each activity aj in the extracted data (i.e., Fi,j is the number of times",3.3.1 Location Similarity Graph,[0],[0]
that activity aj occurred with location li).,3.3.1 Location Similarity Graph,[0],[0]
"We then define location similarity as:
sim(li, lk) = fTi",3.3.1 Location Similarity Graph,[0],[0]
"fk ‖fi‖‖fk‖
(2)",3.3.1 Location Similarity Graph,[0],[0]
"We use semi-supervised learning with a set of “seed” locations from human annotations, and another set of locations that are unlabeled.",3.3.2 Initializing Activity Profiles,[0],[0]
"So we subdivide the set of locations into S = {l1, ..., ls}, which are the seed locations, and U = {ls+1, ..., ls+u}, which are the unlabeled locations, such that s + u = n.",3.3.2 Initializing Activity Profiles,[0],[0]
"For an unlabeled location li ∈ U , the initial activity profile is the normalized co-occurrence frequency vector f i.
For each seed location li ∈ S, we first automatically construct an activity profile vector hi based on the gold goal-acts which were obtained from human annotators as described in Section 4.1.",3.3.2 Initializing Activity Profiles,[0],[0]
All activities not in the gold set are assigned a value of zero.,3.3.2 Initializing Activity Profiles,[0],[0]
Each activity aj in the gold set is assigned a probability P (aj | li) based on the gold answers.,3.3.2 Initializing Activity Profiles,[0],[0]
"However, the gold goal-acts may not match the activity phrases found in the corpus (see discussion in Section 4.3), so we smooth the vector created with the gold goal-acts by averaging it with the normalized co-occurrence frequency vector f i extracted from the corpus.
",3.3.2 Initializing Activity Profiles,[0],[0]
The activity profiles of seed locations stay constant through the learning process.,3.3.2 Initializing Activity Profiles,[0],[0]
We use y0i to denote the initial activity profiles.,3.3.2 Initializing Activity Profiles,[0],[0]
"So when li ∈ S, y0i = (f i + hi)/2.",3.3.2 Initializing Activity Profiles,[0],[0]
"We apply a learning framework developed by (Zhu et al., 2003) based on harmonic energy minimization and extend it to multiple labels.",3.3.3 Learning Goal-Act Strengths,[0],[0]
"Intuitively, we assume that similar locations should share similar activity profiles,3 which motivates the following objective function over matrix Y :
argmin Y ∑ i,k Wi,k‖yi − yk‖2,
s.t. yi = y0i for each li ∈ S (3)
Let D = (di) denote an n × n diagonal matrix where di = ∑n k=1Wi,k. Let’s split Y by the sth
3This is a heuristic but is not always true.
row:",3.3.3 Learning Goal-Act Strengths,[0],[0]
Y =,3.3.3 Learning Goal-Act Strengths,[0],[0]
"[ Ys Yu ] , then split W (similarly for D) into four blocks by the sth row and column:
W =",3.3.3 Learning Goal-Act Strengths,[0],[0]
"[ Wss Wsu Wus Wuu ] (4)
",3.3.3 Learning Goal-Act Strengths,[0],[0]
"From (Zhu et al., 2003), Eq (3) is given by:
Yu =",3.3.3 Learning Goal-Act Strengths,[0],[0]
"(Duu −Wuu)−1WusYs (5)
We then use the label propagation algorithm described in (Zhu and Ghahramani, 2002) to compute Y :
Algorithm 1 repeat Y ← D−1WY",3.3.3 Learning Goal-Act Strengths,[0],[0]
"Clamp yi = y0i for each li ∈ S
until convergence",3.3.3 Learning Goal-Act Strengths,[0],[0]
"One problem with the above algorithm is that it only takes advantage of relations between vertices (i.e., locations).",3.3.4 Activity Similarity,[0],[0]
"If there are intrinsic relations between activities, they could be exploited as a complementary source of information to benefit the learning.",3.3.4 Activity Similarity,[0],[0]
"Intuitively, different pairs of activities share different similarities, e.g., “eat burgers” should be more similar to “have lunch” than “read books”.
",3.3.4 Activity Similarity,[0],[0]
"Under this idea, similar to the previous location similarity weight matrixW , we want to define an activity similarity weight matrix Am×m where Ai,k indicates the similarity weight between activity ai and ak:
Ai,k = exp",3.3.4 Activity Similarity,[0],[0]
"( − 1 σ2 (1− sim (ai, ak)) )
(6)
where σ2 is the same as in Eq (1).",3.3.4 Activity Similarity,[0],[0]
"We explore 3 different similarity functions sim(ai, ak) based on co-occurrence with locations, word matching, and embedding similarities.
",3.3.4 Activity Similarity,[0],[0]
"First, similar to Eq (2), we can use each activity’s co-occurrence frequency with all locations as its location profile and define a similarity score based on cosine values of location profile vectors:
simL(ai, ak) = gTi gk ‖gi‖‖gk‖
(7)
where the predefined co-occurrence frequency matrix F =",3.3.4 Activity Similarity,[0],[0]
"[f1, ..., fn]T =",3.3.4 Activity Similarity,[0],[0]
"[g1, ...,gm].
",3.3.4 Activity Similarity,[0],[0]
"As a second option, the similarity between activities can often be implied by their lexical overlap, e.g., two activities sharing the same verb or noun might be related.",3.3.4 Activity Similarity,[0],[0]
"For each word belonging to any of our activities, we use WordNet (Miller, 1995) to find its synonyms.",3.3.4 Activity Similarity,[0],[0]
We also include the word itself in the synonym set.,3.3.4 Activity Similarity,[0],[0]
"If the synonym sets of two words overlap, we call these two words “match”.",3.3.4 Activity Similarity,[0],[0]
"Then we define the lexical overlap similarity function between ai and ak:
simO(ai, ak) =  1 if verb and noun match 0.5 if verb or noun match 0",3.3.4 Activity Similarity,[0],[0]
"otherwise
(8) As a third option, we can use 300-dimension word embedding vectors (Pennington et al., 2014) trained on 840 billion tokens of web data to compute semantic similarity.",3.3.4 Activity Similarity,[0],[0]
We compute an activity’s embedding as the average of its words’ embeddings.,3.3.4 Activity Similarity,[0],[0]
"Let simE(ai, ak) be the cosine value between the embedding vectors of ai and ak:
simE(ai, ak) = cos〈Embed(ai),Embed(ak)〉 (9)
",3.3.4 Activity Similarity,[0],[0]
"Finally, we can plug these similarity functions into Eq (6).",3.3.4 Activity Similarity,[0],[0]
"We use AL, AO, AE to denote the corresponding matrix.",3.3.4 Activity Similarity,[0],[0]
We can also plug in multiple similarity metrics such as (simL + simE)/2 and use combination symbols AL+E to denote the matrix.,3.3.4 Activity Similarity,[0],[0]
"Once we have a similarity matrix for activities, the next question is how will it help with the activity profile computation?",3.3.5 Injecting Activity Similarity,[0],[0]
"Recall from Eq (5), we know that the activity profile of an unlabeled location can be represented by a linear combination of other locations’ activity profiles.",3.3.5 Injecting Activity Similarity,[0],[0]
The activity profile matrix Y is an n ×m matrix where each row is the activity profile for a location.,3.3.5 Injecting Activity Similarity,[0],[0]
We can also view Y as a matrix whose each column is the location profile for an activity.,3.3.5 Injecting Activity Similarity,[0],[0]
"Using the same idea, we can make each column approximate a linear combination of its highly related columns (i.e., the location profile of an activity will become more similar to the location profiles of its similar activities).",3.3.5 Injecting Activity Similarity,[0],[0]
"Our expectation is that this approximation will help improve the quality of Y .
",3.3.5 Injecting Activity Similarity,[0],[0]
"By being right multiplied by matrix A, Y gets updated from manipulating its columns (activities) as well.",3.3.5 Injecting Activity Similarity,[0],[0]
"We modify the algorithm accordingly as below:
Algorithm 2 repeat Y ←",3.3.5 Injecting Activity Similarity,[0],[0]
D−1WYA Clamp yi,3.3.5 Injecting Activity Similarity,[0],[0]
"= y0i for each li ∈ S
until convergence",3.3.5 Injecting Activity Similarity,[0],[0]
"Since this is a new task and there is no existing dataset for evaluation, we use crowd-sourcing via Amazon Mechanical Turk (AMT) to acquire gold standard data.",4.1 Gold Standard Data,[0],[0]
"First, we released a qualification test containing 15 locations along with detailed annotation guidelines.",4.1 Gold Standard Data,[0],[0]
"25 AMT workers finished our assignment, and we chose 15 of them who did the best job following our guidelines to continue.",4.1 Gold Standard Data,[0],[0]
"We gave the 15 qualified workers 200 new locations, consisting of 152 nominals and 48 proper names,4 randomly selected from our extracted data and set aside as test data.",4.1 Gold Standard Data,[0],[0]
"For each location, we asked the AMT workers to complete the following sentence:
People go to LOC to VERB NOUN
LOC was replaced by one of the 200 locations.",4.1 Gold Standard Data,[0],[0]
"Annotators were asked to provide an activity that is the primary reason why a person would go to that location, in the form of just a VERB or a VERB NOUN pair.",4.1 Gold Standard Data,[0],[0]
"Annotators also had the option to label a location as an “ERROR” if they felt that the provided term is not a location, since our location extraction was not perfect.
",4.1 Gold Standard Data,[0],[0]
"4Same distribution as in the whole location set.
",4.1 Gold Standard Data,[0],[0]
"Only 10 annotators finished labeling our test cases, so we used their answers as the gold standard.",4.1 Gold Standard Data,[0],[0]
We discarded 12 locations that were labeled as an “ERROR” by ≥ 3 workers.5,4.1 Gold Standard Data,[0],[0]
"This resulted in a test set of 188 locations paired with 10 manually defined goal-acts for each one.
",4.1 Gold Standard Data,[0],[0]
A key question that we wanted to investigate through this manual annotation effort is to know whether people truly do associate the same prototypical goal activities with locations.,4.1 Gold Standard Data,[0],[0]
To what extent do people agree when asked to list goalacts?,4.1 Gold Standard Data,[0],[0]
"Also, some places clearly have a smaller set of goal-acts than others.",4.1 Gold Standard Data,[0],[0]
"For example, the primary reason to go to an airport is to catch a flight, but there’s a larger set of common reasons why people go to Yosemite (e.g.,“hiking camping”, “rock climbing”, “see waterfalls”, etc.).
",4.1 Gold Standard Data,[0],[0]
"Complicating matters, the AMT workers often described the same activity with different words (e.g., “buy book” vs. “purchase book”).",4.1 Gold Standard Data,[0],[0]
"Automatically recognizing synonymous event phrases is a difficult NLP problem in its own right.6 So solely for the purpose of analysis, we manually merged activities that have a nearly identical meaning.",4.1 Gold Standard Data,[0],[0]
"We were extremely conservative and did not merge similar or related phrases that were not synonymous because the granularity of terms may matter for this task (e.g., we did not merge “eat burger” and “eat lunch” because one may apply to a specific location while the other does not).
",4.1 Gold Standard Data,[0],[0]
Figure 2 shows the results of our analysis.,4.1 Gold Standard Data,[0],[0]
Only 1 location was assigned exactly the same goal-act by all 10 annotators.,4.1 Gold Standard Data,[0],[0]
But at least half (5) of the annotators listed the same goal-act for 40% of the locations.,4.1 Gold Standard Data,[0],[0]
And nearly 80% of locations had one or more goal-acts listed by ≥ 3 people.,4.1 Gold Standard Data,[0],[0]
These results show that people often do share the same associations between prototypical goal-acts and locations.,4.1 Gold Standard Data,[0],[0]
"These results are also very conservative because many different answers were also similar (e.g. “eat burger”, “eat meal”).
",4.1 Gold Standard Data,[0],[0]
In Table 2 we show examples of locations and the goal-acts listed for them by the human annotators.,4.1 Gold Standard Data,[0],[0]
"If multiple people gave the same answer, we show the number in parentheses.",4.1 Gold Standard Data,[0],[0]
"For example, given the location “Toys R Us”, 9 people listed “buy toys” as a goal-act and 1 person listed “browse gifts”.",4.1 Gold Standard Data,[0],[0]
"We see from Table 2 that
5We found that the workers rarely used the “ERROR” label, so setting this threshold to be 3 was a strong signal.
",4.1 Gold Standard Data,[0],[0]
"6We tried using WordNet synsets to conflate phrases, but it didn’t help much.
",4.1 Gold Standard Data,[0],[0]
"some locations yield very similar sets of goal-acts (e.g., sink, airport, bookstore), while other locations show more diversity (e.g., lake, chiropractor, Chinatown).",4.1 Gold Standard Data,[0],[0]
"To assess the difficulty of this NLP task, we created 3 baseline systems for comparison with our learning approach.",4.2 Baselines,[0],[0]
"All of these methods take the list of activities that co-occurred with a location li in our extracted data and rank them.
",4.2 Baselines,[0],[0]
"The first baseline, FREQ, ranks the activities based on the co-occurrence frequency Fi,j between li and aj in our patterns.",4.2 Baselines,[0],[0]
"The second baseline, PMI, ranks the activities using point-wise mutual information.",4.2 Baselines,[0],[0]
"The third baseline, EMBED, ranks the activities based on the cosine similarity of the semantic embedding vectors for li and aj .",4.2 Baselines,[0],[0]
"We use GloVe (Pennington et al., 2014) 300- dimension embedding vectors pre-trained on 840 billion tokens of web data.",4.2 Baselines,[0],[0]
"For locations and activities with multiple words, we create an embedding by averaging the vectors of their constituent words.",4.2 Baselines,[0],[0]
The gold standard contains a set of goal-acts for each location.,4.3 Matching Activities,[0],[0]
"Since the same activity can be expressed with many different phrases, the only way to truly know whether two phrases refer to the same activity is manual evaluation, which is expensive.",4.3 Matching Activities,[0],[0]
"Furthermore, many activities are very similar or highly related, but not exactly the same.",4.3 Matching Activities,[0],[0]
"For example, “eat burger” and “eat food” both describe eating activities, but the latter is more general than the former.",4.3 Matching Activities,[0],[0]
"Considering them to be the same is not always warranted (e.g., “eat
burger” is a logical goal-act for McDonald’s but not for Baskin-Robbins which primarily sells ice cream).",4.3 Matching Activities,[0],[0]
"As another example, “buy chicken” and “eat chicken” refer to different events (buying and eating)",4.3 Matching Activities,[0],[0]
so they are clearly not the same semantically.,4.3 Matching Activities,[0],[0]
"But at a place like KFC, buying chicken implies eating chicken, and vice versa, so they seem like equally good answers as goal-acts for KFC.",4.3 Matching Activities,[0],[0]
"Due to the complexities of determining which gold standard answers belong in equivalence classes, we considered all of the goal-acts provided by the human annotators to be acceptable answers.
",4.3 Matching Activities,[0],[0]
"To determine whether an activity aj produced by our system matches any of the gold goal-acts for a location li, we report results using two types of matching criteria.",4.3 Matching Activities,[0],[0]
"Exact Match judges aj to be a correct answer for li if (1) it exactly matches (after lemmatization) any activity in li’s gold set, or (2) aj’s verb and noun both appear in li’s gold set, though possibly in different phrases.",4.3 Matching Activities,[0],[0]
"For example, if a gold set contains “buy novels” and “browse books”, then “buy books” will be a match.
",4.3 Matching Activities,[0],[0]
"Since Exact Match is very conservative, we also define a Partial Match criterion to give 50% credit for answers that partially overlap with a gold answer.",4.3 Matching Activities,[0],[0]
An activity aj is a partial match for li if either its verb or noun matches any of the activities in li’s gold set of goal-acts.,4.3 Matching Activities,[0],[0]
"For example, “buy burger” would be a partial match with “buy food” because their verbs match.",4.3 Matching Activities,[0],[0]
All of our methods produce a ranked list of hypothesized goal-acts for a location.,4.4 Evaluation Metrics,[0],[0]
So we use Mean Reciprocal Rank (MRR) to judge the quality of the top 10 activities in each ranked list.,4.4 Evaluation Metrics,[0],[0]
"We report two types of MRR scores.
",4.4 Evaluation Metrics,[0],[0]
"MRR based on the Exact Match criteria (MRRE) is computed as follows, where n is the
number of locations in the test set:
MRRE = 1
n n∑ i=1
1
rank of 1st Exact Match (10)
",4.4 Evaluation Metrics,[0],[0]
We also compute MRR using both the Exact Match and Partial Match criteria.,4.4 Evaluation Metrics,[0],[0]
"First, we need to identify the “best” answer among the 10 activities in the ranked list, which depends both on each activity’s ranking and its matching score.",4.4 Evaluation Metrics,[0],[0]
"The matching score for activity aj is defined as:
score(aj) =  1 if aj is an Exact Match 0.5 if aj is a Partial Match 0",4.4 Evaluation Metrics,[0],[0]
"otherwise
Given 10 ranked activities a1 ... a10 for li, we then compute:
best score(li) = max j=1..10 score(aj) rank(aj)
",4.4 Evaluation Metrics,[0],[0]
"And then finally define MRRP as follows:
MRRP = 1
n n∑ i=1",4.4 Evaluation Metrics,[0],[0]
best score(li) (11),4.4 Evaluation Metrics,[0],[0]
"Unless otherwise noted, all of our experiments report results using 4-fold cross-validation on the 200 locations in our test set.",4.5 Experimental Results,[0],[0]
"We used 4 folds to ensure 50 seed locations for each run (i.e., 1 fold for training and 3 folds for testing).
",4.5 Experimental Results,[0],[0]
The first two columns of Table 3 show the MRR results under Exact Match and Partial Match conditions.,4.5 Experimental Results,[0],[0]
"The first 3 rows show the results for the baseline systems, and the remaining rows show results for our Activity Profile (AP) semi-supervised learning method.",4.5 Experimental Results,[0],[0]
"We show results for 5 variations of the algorithm: AP uses Algorithm 1, and the others use Algorithm 2 with different Activity Similarity measures: AP+AL (location profile similarity), AP+AO (overlap similarity), AP+AE (embedding similarity), and AP+AL+E (location profiles plus embeddings).
",4.5 Experimental Results,[0],[0]
Table 3 shows that our AP algorithm outperforms all 3 baseline methods.,4.5 Experimental Results,[0],[0]
"When adding Activity Similarity into the algorithm, we find that AL slightly improves performance, butAO andAE do not.",4.5 Experimental Results,[0],[0]
"However, we also tried combining them and obtained improved results by usingAL andAE together, yielding an MRRP score of 0.42.
",4.5 Experimental Results,[0],[0]
"To gain more insight about the behavior of the models, Table 3 also shows results for the topranked 1, 2, and 3 answers.",4.5 Experimental Results,[0],[0]
"For these experiments, the system gets full credit if any of its top k answers exactly matches the gold standard, or 50% credit if a partial match is among its top k answers.",4.5 Experimental Results,[0],[0]
"These results show that our AP method produces more correct answers at the top of the list than the baseline methods.
",4.5 Experimental Results,[0],[0]
Table 4 shows six locations with their gold answers and the Top 3 goal-acts hypothesized by our best AP system and the PMI and FREQ baselines.,4.5 Experimental Results,[0],[0]
The activities in boldface were deemed correct (including Partial Match).,4.5 Experimental Results,[0],[0]
"For “bookstore” and “pharmacy”, all of the methods perform well.",4.5 Experimental Results,[0],[0]
"Note the challenge of recognizing that different phrases mean essentially the same thing (e.g., “fill prescription”, “pick up prescription”, “find medicine”).",4.5 Experimental Results,[0],[0]
"For “university” and “Meijer”, the AP method produces more appropriate answers than the baseline methods.",4.5 Experimental Results,[0],[0]
"For “market” and “phone”, all three methods struggle to produce good answers.",4.5 Experimental Results,[0],[0]
"Since “market” is polysemous, we see activities related to both stores and financial markets.",4.5 Experimental Results,[0],[0]
"And “phone” arguably is not a location at all, but most human annotators treated it as a virtual location, listing goal-acts related to telephones.",4.5 Experimental Results,[0],[0]
"However our algorithm considered phones to be similar to computers, which makes sense for today’s smartphones.",4.5 Experimental Results,[0],[0]
"In general, we also observed that Internet sites behave as virtual locations in language (e.g., “I went to YouTube...”).",4.5 Experimental Results,[0],[0]
"The goal-acts learned by our system were extracted from the Spinn3r dataset, while the gold standard answers were provided by human annotators, so the same (or very similar) activities are often expressed in different ways (see Section 4.3).",4.6 Discussion,[0],[0]
This raises the question: what is the upper bound on system performance when evaluating against human-provided goal-acts?,4.6 Discussion,[0],[0]
"To answer this, we compared all of the activities that co-occurred with each location in the corpus against its gold goalacts.",4.6 Discussion,[0],[0]
Only 36% of locations had at least one gold goal-act among its extracted activities when matching identical strings (after lemmatization).,4.6 Discussion,[0],[0]
"Because of this issue, our Exact Match criteria also allowed for combining verbs and nouns from different gold answers.",4.6 Discussion,[0],[0]
"Under this Exact Match criteria, 73% of locations had at least one gold goal-act
among the extracted activities, so this represents an upper bound on performance using this metric.",4.6 Discussion,[0],[0]
"Under the Partial Match criteria, 98% of locations had at least one gold goal-act among the extracted activities, but only 50% credit was awarded for these cases so the maximum score possible would be ∼86%.
",4.6 Discussion,[0],[0]
We also manually inspected 200 gold locations to analyze their types.,4.6 Discussion,[0],[0]
"We discovered some related groups, but substantial diversity overall.",4.6 Discussion,[0],[0]
"The largest group contains ∼20% of the locations, which are many kinds of stores (e.g., Ikea, WalMart, Apple store, shoe store).",4.6 Discussion,[0],[0]
"Even within a group, different locations often have quite different sets of co-occurring activities.",4.6 Discussion,[0],[0]
"In fact, we discovered some spelling variants (e.g., “WalMart” and “wal mart”), but they also have substantially different activity vectors (e.g., because one spelling is much more frequent), so the model learns about them independently.8 Other groups include restaurants (∼5%), home-related (e.g., bathroom) (∼5%), education (∼5%), virtual (e.g., Wikipedia) (∼3%), medical (∼3%) and landscape (e.g., hill) (∼3%).",4.6 Discussion,[0],[0]
"It is worth noting that our locations were extracted by two syntactic patterns and it remains to be seen if this has brought in any bias — detecting location nouns (especially nominals)
7A lemmatization error for the verb “enrolled”.",4.6 Discussion,[0],[0]
"8Of course normalizing location names beforehand may
be beneficial in future work.
",4.6 Discussion,[0],[0]
is a challenging problem in its own right.,4.6 Discussion,[0],[0]
We introduced the problem of learning prototypical goal activities for locations.,5 Conclusions and Future Work,[0],[0]
We obtained human annotations and showed that people do associate prototypical goal-acts with locations.,5 Conclusions and Future Work,[0],[0]
We then created an activity profile framework and applied a semi-supervised label propagation algorithm to iteratively update the activity strengths for locations.,5 Conclusions and Future Work,[0],[0]
"We demonstrated that our learning algorithm identifies goal-acts for locations more accurately than several baseline methods.
",5 Conclusions and Future Work,[0],[0]
"However, this problem is far from solved.",5 Conclusions and Future Work,[0],[0]
Challenges also remain in how to evaluate the accuracy of goal knowledge extracted from text corpora.,5 Conclusions and Future Work,[0],[0]
"Nevertheless, our work represents a first step toward learning goal knowledge about locations, and we believe that learning knowledge about plans and goals is an important direction for natural language understanding research.",5 Conclusions and Future Work,[0],[0]
"In future work, we hope to see if we can take advantage of more contextual information as well as other external knowledge to improve the recognition of goalacts.",5 Conclusions and Future Work,[0],[0]
We are grateful to Haibo Ding for valuable comments on preliminary versions of this work.,Acknowledgments,[0],[0]
People go to different places to engage in activities that reflect their goals.,abstractText,[0],[0]
"For example, people go to restaurants to eat, libraries to study, and churches to pray.",abstractText,[0],[0]
We refer to an activity that represents a common reason why people typically go to a location as a prototypical goal activity (goal-act).,abstractText,[0],[0]
Our research aims to learn goal-acts for specific locations using a text corpus and semi-supervised learning.,abstractText,[0],[0]
"First, we extract activities and locations that co-occur in goal-oriented syntactic patterns.",abstractText,[0],[0]
"Next, we create an activity profile matrix and apply a semi-supervised label propagation algorithm to iteratively revise the activity strengths for different locations using a small set of labeled data.",abstractText,[0],[0]
We show that this approach outperforms several baseline methods when judged against goal-acts identified by human annotators.,abstractText,[0],[0]
Learning Prototypical Goal Activities for Locations,title,[0],[0]
Semantically similar adjectives are not fully interchangeable in context.,1 Introduction,[0],[0]
"Although hot and scalding are related, the statement “the coffee was hot” does not imply the coffee was scalding.",1 Introduction,[0],[0]
"Hot and scalding are scalar adjectives that describe temperature, but they are not interchangeable because they vary in intensity.",1 Introduction,[0],[0]
A native English speaker knows that their relative intensities are given by the ranking hot < scalding.,1 Introduction,[0],[0]
"Understanding this distinction is important for language understanding tasks such as sentiment analysis (Pang et al., 2008), question answering (de Marneffe et al., 2010), and textual inference (Dagan et al., 2006).
",1 Introduction,[0],[0]
"Existing lexical resources such as WordNet (Miller, 1995; Fellbaum, 1998) do not include the relative intensities of adjectives.",1 Introduction,[0],[0]
"As a result, there have been efforts to automate the process of learning intensity relations (e.g. Sheinman and Tokunaga (2009), de Melo and Bansal (2013), Wilkinson (2017), etc.).",1 Introduction,[0],[0]
"Many existing approaches rely
on pattern-based or lexicon-based methods to predict the intensity ranking of adjectives.",1 Introduction,[0],[0]
"Patternbased approaches search large corpora for lexical patterns that indicate an intensity relationship – for example, “not just X, but Y” implies X < Y.",1 Introduction,[0],[0]
"As with pattern-based approaches for other tasks (such as hypernym discovery (Hearst, 1992)), they are precise but have relatively sparse coverage of comparable adjectives, even when using webscale corpora (de Melo and Bansal, 2013; Ruppenhofer et al., 2014).",1 Introduction,[0],[0]
"Lexicon-based approaches employ resources that map an adjective to a realvalued number that encodes both intensity and polarity (e.g. good might map to 1 and phenomenal to 5, while bad maps to -1 and awful to -3).",1 Introduction,[0],[0]
"They can also be precise, but may not cover all adjectives of interest.
",1 Introduction,[0],[0]
We propose paraphrases as a new source of evidence for the relative intensity of scalar adjectives.,1 Introduction,[0],[0]
"A paraphrase is a pair of words or phrases with approximately similar meaning, such as really great↔ phenomenal.",1 Introduction,[0],[0]
Adjectival paraphrases can be exploited to uncover intensity relationships.,1 Introduction,[0],[0]
"A paraphrase pair of the above form, where one phrase is composed of an intensifying adverb and an adjective (really great) and the other is a single-word adjective (phenomenal), provides evidence that great < phenomenal.",1 Introduction,[0],[0]
"By drawing this evidence from large, automatically-generated
paraphrase resources like the Paraphrase Database (PPDB) 1 (Ganitkevitch et al., 2013; Pavlick et al., 2015), it is possible to obtain high-coverage pairwise adjective intensity predictions at reasonably high accuracy.
",1 Introduction,[0],[0]
"We demonstrate the usefulness of paraphrase evidence for inferring relative adjective intensity in two tasks: ordering sets of adjectives along an intensity scale, and inferring the polarity of indirect answers to yes/no questions.",1 Introduction,[0],[0]
"In both cases, we find that combining the relatively noisy, but highcoverage, paraphrase evidence with more precise but low-coverage pattern- or lexicon-based evidence improves overall quality.",1 Introduction,[0],[0]
"Noting that adding adjective intensity relations to WordNet (Miller, 1995; Fellbaum, 1998) would be useful, Sheinman et al. (2013) propose a system for automatically extracting sets of same-attribute adjectives from WordNet ‘dumbbells’ – consisting of two direct antonyms at the poles and satellites of synonymous/related adjectives incident to each antonym (Gross and Miller, 1990) – and ordering them by intensity.",2 Related Work,[0],[0]
"The annotations, however, are not in WordNet as of its latest version (3.1).
",2 Related Work,[0],[0]
"Work on adjective intensity generally focuses on two related tasks: clustering adjectives based on the attributes they modify, and ranking sameattribute adjectives by intensity.",2 Related Work,[0],[0]
"With respect to the former, common approaches involve clustering adjectives by their contexts (Hatzivassiloglou and McKeown, 1993; Shivade et al., 2015).",2 Related Work,[0],[0]
"We do not focus on the clustering task in this paper, but concentrate on the ranking task.
",2 Related Work,[0],[0]
Approaches to the task of ranking scalar adjectives by their intensity mostly fall under the paradigms of pattern-based or lexicon-based approaches.,2 Related Work,[0],[0]
"Pattern-based approaches work by extracting lexical (Sheinman and Tokunaga, 2009; de Melo and Bansal, 2013; Sheinman et al., 2013) or syntactic (Shivade et al., 2015) patterns indicative of an intensity relationship from large corpora.",2 Related Work,[0],[0]
"For example, the patterns “X, but not Y” and “not just X but Y” provide evidence that X is an adjective less intense than Y.
Lexicon-based approaches are derived from the premise that adjectives can provide information about the sentiment of a text (Hatzivassiloglou and McKeown, 1993).",2 Related Work,[0],[0]
"These methods draw upon a
1www.paraphrase.org
lexicon that maps adjectives to real-valued scores encoding both sentiment polarity and intensity.",2 Related Work,[0],[0]
"The lexicon might be compiled automatically – for example, from analyzing adjectives’ appearance in star-valued product or movie reviews (de Marneffe et al., 2010; Rill et al., 2012; Sharma et al., 2015; Ruppenhofer et al., 2014) – or manually.",2 Related Work,[0],[0]
"In our experiments we utilize the manually-compiled SO-CAL lexicon (Taboada et al., 2011).
",2 Related Work,[0],[0]
Our paraphrase-based approach to inferring relative adjective intensity is based on paraphrases that combine adjectives with adverbial modifiers.,2 Related Work,[0],[0]
"A tangentially related approach is Collex (Ruppenhofer et al., 2014), which is motivated by the intuition that adjectives with extreme intensities are modified by different adverbs from adjectives with more moderate intensities: extreme adverbs like absolutely are more likely to modify extreme adjectives like brilliant than are moderate adverbs like very.",2 Related Work,[0],[0]
"Unlike Collex, which requires predetermined sets of ‘end-of-scale’ and ‘normal’ adverbial modifiers, our approach learns the identity and relative importance of intensifying adverbs.
",2 Related Work,[0],[0]
Relative intensity is just one of several dimensions of gradable adjective semantics.,2 Related Work,[0],[0]
"In addition to intensity scales, a comprehensive model of scalar adjective semantics might also incorporate notions of intensity range (Morzycki, 2015), adjective class (Kamp and Partee, 1995), and scale membership according to meaning (Hatzivassiloglou and McKeown, 1993).",2 Related Work,[0],[0]
"In this paper we take the position that relative intensity is worth studying on its own because it is an important component of adjective semantics, usable directly for some NLP tasks such as sentiment analysis (Pang et al., 2008), and as part of a more comprehensive model for other tasks like question answering (de Marneffe et al., 2010).",2 Related Work,[0],[0]
Adjectival paraphrases provide evidence about the relative intensity of adjectives.,3 Paraphrase-based Intensity Evidence,[0],[0]
A paraphrase of the form RB JJu ↔,3 Paraphrase-based Intensity Evidence,[0],[0]
"JJv – where one phrase is comprised of an adjective modified by an intensifying adverb (RB JJu), and the other is a single-word adjective (JJv) – is evidence that the first adjective is less intense than the second (JJu < JJv).",3 Paraphrase-based Intensity Evidence,[0],[0]
We propose a new method for encoding this evidence and using it to make pairwise adjective intensity predictions.,3 Paraphrase-based Intensity Evidence,[0],[0]
"First, a graph (JJGRAPH) is formed to represent over 36k adjectival paraphrases hav-
ing the specified form.",3 Paraphrase-based Intensity Evidence,[0],[0]
"Next, data in the graph are used to make pairwise adjective intensity predictions.",3 Paraphrase-based Intensity Evidence,[0],[0]
"In JJGRAPH, nodes are adjectives, and each directed edge (JJu −−→
RB JJv) corresponds to an adjec-
tival paraphrase of the form RB JJu ↔",3.1 Identifying Intensifying Adverbs,[0],[0]
"JJv – for example, very tall ↔ large – where one ‘phrase’ (JJv) is an adjective and the other (RB JJu) is an adjectival phrase containing an adverb and adjective (see Figure 1 for examples).
",3.1 Identifying Intensifying Adverbs,[0],[0]
Adverbs in PPDB can be intensifying or deintensifying.,3.1 Identifying Intensifying Adverbs,[0],[0]
"An intensifying adverb (e.g. very, totally) strengthens the adjectives it modifies.",3.1 Identifying Intensifying Adverbs,[0],[0]
"In contrast, a de-intensifying adverb (e.g. slightly, somewhat) weakens the adjectives it modifies.",3.1 Identifying Intensifying Adverbs,[0],[0]
"Since edges in JJGRAPH ideally point in the direction of increasing intensity, the first step in the process of creating JJGRAPH is to identify a set of adverbs that are likely intensifiers to be included as edges.
",3.1 Identifying Intensifying Adverbs,[0],[0]
"For this purpose, we generate a set R of likely intensifying adverbs within PPDB using a bootstrapping approach (Figure 2).",3.1 Identifying Intensifying Adverbs,[0],[0]
The process starts with a small seed set of adjective pairs having a known intensity relationship.,3.1 Identifying Intensifying Adverbs,[0],[0]
"The seeds are pairs (ju, jv) from PPDB-XXL2 such that ju is a baseform adjective (e.g. hard), and jv is its comparative or superlative form (e.g. harder or hardest).",3.1 Identifying Intensifying Adverbs,[0],[0]
"Using the seeds, we identify intensifying ad-
2PPDB comes in six increasingly large sizes from S to XXXL; larger collections have wider coverage but lower precision.",3.1 Identifying Intensifying Adverbs,[0],[0]
"Our work uses XXL.
verbs by finding adjectival paraphrases in PPDB of the form (riju ↔ jv); because ju < jv, adverb ri is inferred to be intensifying (Round 1).",3.1 Identifying Intensifying Adverbs,[0],[0]
All such ri are added to initial adverb set R1.,3.1 Identifying Intensifying Adverbs,[0],[0]
"The process continues by extracting paraphrases (riju′ ↔ jv′) with ri ∈ R1, indicating additional adjective pairs (ju′ , jv′) with intensity direction inferred by ri (Round 2).",3.1 Identifying Intensifying Adverbs,[0],[0]
"Finally, the adjective pairs extracted in this second iteration are used to identify additional intensifying adverbs R3, which are added to the final set R = R1 ∪R3 (Round 3).
",3.1 Identifying Intensifying Adverbs,[0],[0]
"In all, this process generates a set of 610 adverbs.",3.1 Identifying Intensifying Adverbs,[0],[0]
"Examination of the set shows that the process does capture many intensifying adverbs like very and abundantly, and excludes many deintensifying adverbs appearing in PPDB like far less and not as.",3.1 Identifying Intensifying Adverbs,[0],[0]
"However, due to the noise inherent in PPDB itself and in the bootstrapping process, there are also a few de-intensifying adverbs included in R (e.g. hardly, kind of ) as well as adverbs that are neither intensifying nor deintensifying (e.g. ecologically).",3.1 Identifying Intensifying Adverbs,[0],[0]
It will be important to take this noise into consideration when using JJGRAPH to make pairwise intensity predictions.,3.1 Identifying Intensifying Adverbs,[0],[0]
"JJGRAPH is built by extracting all 36,756 adjectival paraphrases in PPDB of the specified form RB JJu ↔",3.2 Building JJGRAPH,[0],[0]
"JJv, where the adverb belongs to R. The resulting graph has 3,704 unique adjective nodes.",3.2 Building JJGRAPH,[0],[0]
"JJGRAPH is a multigraph, as there are frequently multiple intensifying relationships between pairs of adjectives.",3.2 Building JJGRAPH,[0],[0]
"For example, the paraphrases pretty hard ↔ tricky and really hard ↔ tricky are both present in PPDB.",3.2 Building JJGRAPH,[0],[0]
"There can also be contradictory or cyclic edges in JJGRAPH, as in the example depicted in the JJGRAPH subgraph in Figure 3, where the adverb really connects tasty to lovely and vice versa.",3.2 Building JJGRAPH,[0],[0]
Self-edges are allowed (e.g. really hard↔ hard).,3.2 Building JJGRAPH,[0],[0]
Examining the directed adverb edges between two adjectives ju and jv in JJGRAPH provides evidence about the relative intensity relationship between them.,3.3 Pairwise Intensity Prediction,[0],[0]
"However, it has just been noted that JJGRAPH is noisy, containing both contradictory/cyclic edges and adverbs that are not uniformly intensifying.",3.3 Pairwise Intensity Prediction,[0],[0]
"Rather than try to eliminate cycles, or manually annotate each adverb with a weight corresponding to its intensity and polarity
(Ruppenhofer et al., 2015; Taboada et al., 2011), we aim to learn these weights automatically in the process of predicting pairwise intensity.
",3.3 Pairwise Intensity Prediction,[0],[0]
"Given adjective pair (ju, jv), we build a classifier that outputs a score from 0 to 1 indicating the predicted likelihood that ju< jv.",3.3 Pairwise Intensity Prediction,[0],[0]
Its binary features correspond to adverb edges from ju to jv and from jv to ju in JJGRAPH.,3.3 Pairwise Intensity Prediction,[0],[0]
"The feature space includes only adverbs from R that appear at least 10 times in JJGRAPH, resulting in features for m = 259 unique adverbs in each direction (i.e. from ju to jv and vice versa) for 2m = 518 binary features total.",3.3 Pairwise Intensity Prediction,[0],[0]
"Note that while all adverb features correspond to predicted intensifiers from R, there are some features that are actually de-intensifying due to the noise inherent in the bootstrapping process (Section 3.1).
",3.3 Pairwise Intensity Prediction,[0],[0]
"We train the classifier on all 36.7k edges in JJGRAPH, based on a simplifying assumption that all adverbs in R are indeed intensifiers.",3.3 Pairwise Intensity Prediction,[0],[0]
"For each adjective pair (ju, jv) with one or more direct edges from ju to jv, a positive training instance for pair (ju, jv) and a negative training instance for pair (jv, ju) are added to the training set.",3.3 Pairwise Intensity Prediction,[0],[0]
"A logistic regression classifier is trained on the data, using elastic net regularization and 10-fold cross validation to tune parameters.
",3.3 Pairwise Intensity Prediction,[0],[0]
"The model parameters output by the training process are in a feature weights vector w ∈ R2m (with no bias term) which can be used to generate a paraphrase-based score for each adjective pair:
scorepp(ju, jv) = 1
1 + exp−wxuv",3.3 Pairwise Intensity Prediction,[0],[0]
"− 0.5 (1)
where xuv is the binary feature vector for adjective pair (ju, jv).",3.3 Pairwise Intensity Prediction,[0],[0]
"The decision boundary 0.5 is subtracted from the sigmoid activation function so that pairs predicted to have the directed relation ju< jv will have a positive score, and those predicted to have the opposite directional relation will have a negative score.",3.3 Pairwise Intensity Prediction,[0],[0]
Our experiments compare the proposed paraphrase approach with existing pattern- and lexicon-based approaches.,4 Other Intensity Evidence,[0],[0]
We experiment with the pattern-based approach of de Melo and Bansal (2013).,4.1 Pattern-based Evidence,[0],[0]
"Given a pair of adjectives to be ranked by their intensity, de Melo and Bansal (2013) cull intensity patterns from Google n-Grams (Brants and Franz, 2009) as evidence of their intensity order.",4.1 Pattern-based Evidence,[0],[0]
"Specifically, they identify 8 types of weak-strong patterns (e.g. “X, but not Y”) and 7 types of strong-weak patterns (e.g. “not X, but still Y”) that are used as evidence about the directionality of the intensity relationship between adjectives.",4.1 Pattern-based Evidence,[0],[0]
"Given an adjective pair (ju, jv), an overall pattern-based weak-strong score is calculated:
scorepat(ju, jv) =",4.1 Pattern-based Evidence,[0],[0]
"(Wu − Su)− (Wv − Sv)
count(ju) · count(jv) (2)
where Wu and Su quantify the pattern evidence for the weak-strong and strong-weak intensity relations respectively for the pair (ju, jv), and Wv and Sv quantify the pattern evidence for the pair (jv, ju).",4.1 Pattern-based Evidence,[0],[0]
"Wu and Su are calculated as:
Wu = 1
P1 ∑ p1∈Pws count(p1(ju, jv))
",4.1 Pattern-based Evidence,[0],[0]
"Su = 1
P2 ∑ p2∈Psw count(p2(ju, jv)) (3)
Wv and Sv are calculated similarly by swapping the positions of ju and jv.",4.1 Pattern-based Evidence,[0],[0]
"For example, given pair (good, great), Wu might incorporate evidence from patterns “good, but not great” and “not only good but great”, while Sv might incorporate evidence from the pattern “not great, just good”.",4.1 Pattern-based Evidence,[0],[0]
"Pws denotes the set of weak-strong patterns, Psw denotes the set of strong-weak patterns, and P1 and P2 give the total counts of all occurrences of any pattern in Pws and Psw respectively.",4.1 Pattern-based Evidence,[0],[0]
The score is normalized by the frequencies of ju and jv in order to avoid bias due to high-frequency adjectives.,4.1 Pattern-based Evidence,[0],[0]
"As with the paraphrase-based scoring mechanism (Equation 1), scores output by this method can be positive or negative, with positive scores being indicative of a weak-strong relationship from ju to jv.",4.1 Pattern-based Evidence,[0],[0]
"Note that score(ju, jv) = −score(jv, ju).",4.1 Pattern-based Evidence,[0],[0]
"We use the manually-compiled SO-CAL3 lexicon as our third, lexicon-based method for inferring intensity.",4.2 Lexicon-based Evidence,[0],[0]
"The SO-CAL lexicon assigns an integer weight in the range [−5, 5] to 2,826 adjectives.",4.2 Lexicon-based Evidence,[0],[0]
"The sign of the weight encodes sentiment polarity (positive or negative), and the value encodes intensity (e.g. atrocious, with a weight of -5, is more intense than unlikable, with a weight of -3).",4.2 Lexicon-based Evidence,[0],[0]
"SO-CAL is used to derive a pairwise intensity prediction for adjectives (ju,jv) as follows:
scoresocal(ju, jv) = |L(jv)| − |L(ju)|, iff sign(ju) = sign(jv)
(4)
where L(jv) gives the lexicon weight for jv.",4.2 Lexicon-based Evidence,[0],[0]
Note that scoresocal is computed only for adjectives having the same polarity direction in the lexicon,4.2 Lexicon-based Evidence,[0],[0]
; otherwise the score is undefined.,4.2 Lexicon-based Evidence,[0],[0]
"This is because adjectives belonging to different half scales, such as freezing and steaming, are frequently incomparable in terms of intensity (de Marneffe et al., 2010).",4.2 Lexicon-based Evidence,[0],[0]
"While the pattern-based and lexicon-based pairwise intensity scores are known to be precise but low-coverage (de Melo and Bansal, 2013; Ruppenhofer et al., 2015), we expect that the paraphrase-based score will produce higher coverage at lower accuracy.",4.3 Combining Evidence,[0],[0]
Thus we also experiment with scoring methods that combine two or three score types.,4.3 Combining Evidence,[0],[0]
"When combining two metrics x and y to generate a score for a pair (ju, jv), we simply use the first metric x if it can be reliably calculated for the pair, and back off to metric y otherwise.",4.3 Combining Evidence,[0],[0]
"More formally, the combined score for metrics x and y is given by:
scorex+y(ju, jv) = αx · gx(scorex(ju, jv))",4.3 Combining Evidence,[0],[0]
"+ (1− αx) · gy(scorey(ju, jv))
(5)
where αx ∈ {0, 1} is a binary indicator corresponding to the condition that scorex can be reliably calculated for the adjective pair, and gx(·) is a scaling function (see below).",4.3 Combining Evidence,[0],[0]
"If αx = 1, then scorex is used.",4.3 Combining Evidence,[0],[0]
"Otherwise, if αx = 0, then we default to scorey.",4.3 Combining Evidence,[0],[0]
"When combining three metrics x, y, and z, the combined score is given by:
3https://github.com/sfu-discourse-lab/ SO-CAL
scorex+y+z(ju, jv) = αx · gx(scorex(ju, jv))",4.3 Combining Evidence,[0],[0]
"+ (1− αx) · scorey+z(ju, jv)
(6)
",4.3 Combining Evidence,[0],[0]
The criteria for having αx = 1 varies depending on the metric type.,4.3 Combining Evidence,[0],[0]
"For pattern-based evidence (x=‘pat’), αx = 1 when adjectives ju and jv appear together in any of the intensity patterns culled from Google n-grams (e.g. a pattern like “ju, but not jv” exists).",4.3 Combining Evidence,[0],[0]
"For lexicon-based evidence (x=‘socal’), αx = 1 when both ju and jv are in the SO-CAL vocabulary, and have the same polarity (i.e. are both positive or both negative).",4.3 Combining Evidence,[0],[0]
"For paraphrase-based evidence (x=‘pp’), αx = 1 when ju and jv have one or more edges directly connecting them in JJGRAPH.
",4.3 Combining Evidence,[0],[0]
"Since the metrics to be combined may have different ranges, we use a scaling function gx(·) to make the scores output by each metric directly comparable:
gx(w) = sign(w) · (
log(|w|)− µx σx + γ
) (7)
where µx and σx are the estimated population mean and standard deviation of log(scorex) (estimated over all adjective pairs in the dataset), and γ is an offset that ensures positive scores remain positive, and negative scores remain negative.",4.3 Combining Evidence,[0],[0]
In our experiments we set γ = 5.,4.3 Combining Evidence,[0],[0]
The first experimental application for the different paraphrase evidence is an existing model for predicting a global intensity ordering within a set of adjectives.,5 Ranking Adjective Sets by Intensity,[0],[0]
Global ranking models are useful for inferring intensity comparisons between adjectives for which there is no explicit evidence.,5 Ranking Adjective Sets by Intensity,[0],[0]
"For example, in ranking three adjectives like warm, hot, and scalding, there may be direct evidence indicating warm < hot and hot < scalding, but no way of directly comparing warm to scalding.",5 Ranking Adjective Sets by Intensity,[0],[0]
Global ranking models infer that warm< scalding based on evidence from the other adjective pairs in the scale.,5 Ranking Adjective Sets by Intensity,[0],[0]
We adopt the mixed-integer linear programming (MILP) approach of de Melo and Bansal (2013) for generating a global intensity ranking.,5.1 Global Ranking Model,[0],[0]
"This model takes a set of adjectives A = {a1, . . .",5.1 Global Ranking Model,[0],[0]
", an}
and directed, pairwise adjective intensity scores score(ai, aj) as input, and assigns each adjective ai a place along a linear scale xi ∈",5.1 Global Ranking Model,[0],[0]
"[0, 1].",5.1 Global Ranking Model,[0],[0]
The adjectives’ assigned values define the global ordering.,5.1 Global Ranking Model,[0],[0]
"If the predicted weights used as input are inconsistent, containing cycles, the model resolves these by choosing the globally optimal solution.
",5.1 Global Ranking Model,[0],[0]
"Recall that all pairwise scoring metrics produce a positive score for adjective pair (ju, jv) when it is likely that ju< jv, and a negative score otherwise.",5.1 Global Ranking Model,[0],[0]
"Consequently, the MILP approach should result in xu < xv when score(ju, jv) is positive, and xu > xv otherwise.",5.1 Global Ranking Model,[0],[0]
"This goal is achieved by maximizing the objective function:∑
u,v
sign(xv − xu) ·",5.1 Global Ranking Model,[0],[0]
"score(ju, jv) (8)
de Melo and Bansal (2013) propose a MILP formulation for maximizing this objective, which we utilize in our experiments.",5.1 Global Ranking Model,[0],[0]
"Note that while de Melo and Bansal (2013) incorporate synonymy evidence from WordNet in their ranking method, we do not implement this part of the model.",5.1 Global Ranking Model,[0],[0]
"We experiment with using each of the paraphrase-, pattern-, and lexicon-based pairwise scores as input to the global ranking model in isolation.",5.2 Experiments,[0],[0]
"To examine how the scoring methods perform when used in combination, we also test all possible ordered combinations of 2 and 3 scores.
",5.2 Experiments,[0],[0]
Experiments are run over three distinct test sets (Table 1).,5.2 Experiments,[0],[0]
Each dataset contains ordered sets of scalar adjectives belonging to the same scale.,5.2 Experiments,[0],[0]
"In general, scalar adjectives describing the same attribute can be ordered along a full scale (e.g. freezing to sweltering), or a half scale (warm to sweltering); all three test sets group adjectives into half scales.",5.2 Experiments,[0],[0]
"The three datasets are described here, and their characteristics are given in Table 1.",5.2 Experiments,[0],[0]
"deMelo (de Melo and Bansal, 2013)4.",5.2 Experiments,[0],[0]
"87 adjective
4http://demelo.org/gdm/intensity/
sets are extracted from WordNet ‘dumbbell’ structures (Gross and Miller, 1990), and partitioned into half-scale sets based on their pattern-based evidence in the Google N-Grams corpus (Brants and Franz, 2009).",5.2 Experiments,[0],[0]
"Sets are manually annotated for intensity relations (<, >, and =).",5.2 Experiments,[0],[0]
"Wilkinson (Wilkinson and Oates, 2016).",5.2 Experiments,[0],[0]
"Twelve adjective sets are generated by presenting crowd workers with small seed sets (e.g. huge, small, microscopic), and eliciting similar adjectives.",5.2 Experiments,[0],[0]
"Sets are automatically cleaned for consistency, and then annotated for intensity by crowd workers.",5.2 Experiments,[0],[0]
"While the original dataset contains full scales, we manually sub-divide these into 21 half-scales for use in this study.",5.2 Experiments,[0],[0]
Details on the modification from full- to half-scales are in the Supplemental Material.,5.2 Experiments,[0],[0]
Crowd.,5.2 Experiments,[0],[0]
We also crowdsourced a new set of adjective scales with high coverage of the PPDB vocabulary.,5.2 Experiments,[0],[0]
"In a three-step process, we first asked crowd workers whether pairs of adjectives describe the same attribute (e.g. temperature) and therefore should belong along the same scale.",5.2 Experiments,[0],[0]
"Second, sets of same-scale adjectives were refined over multiple rounds.",5.2 Experiments,[0],[0]
"Finally, workers ranked the adjectives in each set by intensity.",5.2 Experiments,[0],[0]
"The final dataset includes 293 adjective pairs along 79 scales.
",5.2 Experiments,[0],[0]
We measure the agreement between the gold standard ranking of adjectives along each scale and the predicted ranking using three commonlyused metrics: Pairwise accuracy.,5.2 Experiments,[0],[0]
"For each pair of adjectives along the same scale, we compare the predicted ordering of the pair after global ranking (<, >, or =) to the gold-standard ordering of the pair, and report overall accuracy of the pairwise predictions.",5.2 Experiments,[0],[0]
Kendall’s tau (τb).,5.2 Experiments,[0],[0]
This metric computes the rank correlation between the predicted (rP (J)) and gold-standard (rG(J)),5.2 Experiments,[0],[0]
"ranking permutations of each adjective scale J , incorporating a correction for ties.",5.2 Experiments,[0],[0]
"Values for τb range from −1 to 1, with extreme values indicating a perfect negative
or positive correlation, and a value of 0 indicating no correlation between predicted and gold rankings.",5.2 Experiments,[0],[0]
We report τb,5.2 Experiments,[0],[0]
"as a weighted average over scales in each dataset, where weights correspond to the number of adjective pairs in each scale.",5.2 Experiments,[0],[0]
Spearman’s rho (ρ).,5.2 Experiments,[0],[0]
We report the Spearman’s ρ rank correlation coefficient between predicted (rP (J)) and gold-standard (rG(J)),5.2 Experiments,[0],[0]
ranking permutations.,5.2 Experiments,[0],[0]
"For each dataset, we calculate this metric just once by treating each adjective in a particular scale as a single data point, and calculating an overall ρ for all adjectives from all scales.",5.2 Experiments,[0],[0]
"The results of the global ordering experiment, reported in Table 2, are organized as follows:",5.3 Experimental Results,[0],[0]
Score Accuracy pertains to performance of the scoring methods alone – prior to global ranking – while Global Ranking Results pertains to performance of each scoring method as used in the global ranking algorithm.,5.3 Experimental Results,[0],[0]
Within Score Accuracy there are two metrics.,5.3 Experimental Results,[0],[0]
Coverage gives the percent of unique same-scale adjective pairs from the test set that can be directly scored using the given method.,5.3 Experimental Results,[0],[0]
"For scorepat, covered pairs are all those that appear together in any recognized pattern;
for scorepp, covered pairs are those directly connected in JJGRAPH by one or more direct edges; for scoresocal, covered pairs are all those for which both adjectives are in the SO-CAL lexicon and the metric is defined.",5.3 Experimental Results,[0],[0]
"Pairwise Accuracy gives the accuracy of the scoring method (before global ranking) on just the covered pairs, meaning that the subset of pairs scored by each method varies.",5.3 Experimental Results,[0],[0]
"Within Global Ranking Results, we report pairwise accuracy, weighted average τb, and ρ calculated over all pairs after ranking – including both pairs that are covered by the scoring method, and those whose pairwise intensity relationship has been inferred by the ranking algorithm.
",5.3 Experimental Results,[0],[0]
"The results indicate that the pairwise score accuracies (before ranking) for scorepat and scoresocal are higher than those of scorepp for all datasets, but that their coverage is relatively limited.",5.3 Experimental Results,[0],[0]
"The one exception is the deMelo dataset, where scorepat has high coverage because the dataset was compiled specifically by finding adjective pairs that matched lexical patterns in the corpus.",5.3 Experimental Results,[0],[0]
"For all datasets, highest coverage is achieved using one of the combined metrics that incorporates paraphrase-based evidence.
",5.3 Experimental Results,[0],[0]
"The impact of these trends is visible on the
Global Ranking Results.",5.3 Experimental Results,[0],[0]
"When using pairwise intensity scores to compute the global ranking, higher coverage by a metric drives better results, as long as the metric’s accuracy is reasonably high.",5.3 Experimental Results,[0],[0]
"Thus the paraphrase-based scorepp, with its high coverage, gets better global ranking results than the other single-method scores for two of the three datasets.",5.3 Experimental Results,[0],[0]
"Further, we find that boosting coverage with a combined metric that incorporates paraphrase evidence produces the highest post-ranking pairwise accuracy scores overall for all three datasets, and the highest average τb and ρ on the Crowd and Wilkinson datasets.",5.3 Experimental Results,[0],[0]
"We conclude that incorporating paraphrase evidence can improve the quality of this model for ordering adjectives along a scale because it gives high coverage with reasonably high quality.
",5.3 Experimental Results,[0],[0]
The performance trends on the deMelo dataset differ from those on the Crowd and Wilkinson datasets.,5.3 Experimental Results,[0],[0]
"In particular, scorepp and scoresocal have substantially lower pre-ranking pairwise accuracy on the pairs they cover in the deMelo dataset than they do for Crowd and Wilkinson: scorepp has an accuracy of just 0.458 on covered pairs in the deMelo dataset, compared with 0.676 and 0.753 on the Crowd and Wilkinson datasets, and score differences for scoresocal are similar.",5.3 Experimental Results,[0],[0]
The near-random prediction accuracies of scorepp and scoresocal on deMelo before ranking lead to nearzero correlation values on this dataset after global ranking.,5.3 Experimental Results,[0],[0]
"To explore possible reasons for these results, we assessed the level of human agreement with each dataset in terms of pairwise accuracy.",5.3 Experimental Results,[0],[0]
"For each test set, we asked five crowd workers to classify the intensity direction for each adjective pair (ju, jv) in all scales as less than (<), greater than (>), or equal (=).",5.3 Experimental Results,[0],[0]
"We found that humans agreed with the ‘gold standard’ direction 65% of the time on the Bansal dataset, versus 70% of the time on the Crowd and Wilkinson datasets.",5.3 Experimental Results,[0],[0]
"It is possible that the more difficult nature of the Bansal dataset, coupled with its method of compilation (i.e. favoring adjective pairs that co-occur with pre-defined intensity patterns), lead to the lower coverage and lower accuracy of scorepp and scoresocal on this dataset.",5.3 Experimental Results,[0],[0]
The second task that we address is answering indirect yes or no questions.,6 Indirect Question Answering,[0],[0]
"de Marneffe et al. (2010) observed that answers to such polar questions fre-
quently omit an explicit yes or no response.",6 Indirect Question Answering,[0],[0]
In some cases the implied answer depends on the relative intensity of adjective modifiers in the question and answer.,6 Indirect Question Answering,[0],[0]
"For example, in the exchange:
Q: Was he a successful ruler?",6 Indirect Question Answering,[0],[0]
"A: Oh, a tremendous ruler.
",6 Indirect Question Answering,[0],[0]
"the implied answer is yes, which is inferred because successful≤ tremendous in terms of relative intensity.",6 Indirect Question Answering,[0],[0]
"Conversely, in the exchange:
Q: Does it have a large impact?",6 Indirect Question Answering,[0],[0]
"A: It has a medium-sized impact.
",6 Indirect Question Answering,[0],[0]
"the implied answer is no because large>mediumsized.
",6 Indirect Question Answering,[0],[0]
de Marneffe et al. (2010) compiled an evaluation set for this task by extracting 123 examples of such indirect question-answer pairs (IQAP) from dialogue corpora.,6 Indirect Question Answering,[0],[0]
"In each exchange, the implied answer (annotated by crowd workers to be yes or no5) depends on the relative intensity relationship between modifiers in the question and answer texts.",6 Indirect Question Answering,[0],[0]
"In their original paper, the authors utilize an automatically-compiled lexicon to make a polarity prediction for each IQAP.",6 Indirect Question Answering,[0],[0]
Our goal is to see whether paraphrase-based scores are useful for predicting the polarity of answers in the IQAP dataset.,6.1 Predicting Answer Polarity,[0],[0]
"As before, we compare the quality of predictions made using the paraphrase-based evidence with predictions made using pattern-based, lexicon-based, and combined scoring metrics.
",6.1 Predicting Answer Polarity,[0],[0]
"To use the pairwise scores for inference, we employ a decision procedure nearly identical to that of de Marneffe et al. (2010).",6.1 Predicting Answer Polarity,[0],[0]
"If jq and ja are scorable (i.e. have a scorable intensity relationship along the same half-scale), then jq≤ ja implies the answer is yes (first example above), and jq> ja implies the answer is no (second example).",6.1 Predicting Answer Polarity,[0],[0]
"If the pair of adjectives is not scorable, then the predicted answer is no, as the pair could be antonyms or completely unrelated.",6.1 Predicting Answer Polarity,[0],[0]
"If either jq or ja is missing from the scoring vocabulary, the adjectives are impossible to compare and therefore the prediction is uncertain.",6.1 Predicting Answer Polarity,[0],[0]
"The full decision procedure is given in Figure 4.
",6.1 Predicting Answer Polarity,[0],[0]
"5The original dataset contains two additional examples where the answer is annotated as uncertain, but de Marneffe et al. (2010) exclude them from the results and so do we.",6.1 Predicting Answer Polarity,[0],[0]
"The decision procedure in Figure 4 is carried out for the 123 IQAP instances in the dataset, varying the score type.",6.2 Experiments,[0],[0]
"We report the accuracy, and macro-averaged precision, recall, and F1-score of the 85 yes and 38 no instances, in Table 3 alongside the percent of instances with adjectives out of vocabulary.",6.2 Experiments,[0],[0]
"Only the combined scores for the two best-scoring combinations, scoresocal+pp and scoresocal+pat+pp, are reported.
",6.2 Experiments,[0],[0]
"The simplest baseline of predicting all answers to be “YES” gets highest accuracy in this imbalanced test set, but all score types perform better than the all-“YES” baseline in terms of precision and F1-score.",6.2 Experiments,[0],[0]
"Bouyed by its high precision, the scoresocal – which is derived from a manuallycompiled lexicon – scored higher than scorepp and scorepat.",6.2 Experiments,[0],[0]
"But it mis-predicted 33% of pairs
as uncertain because of its limited overlap with the IQAP vocabulary.",6.2 Experiments,[0],[0]
"Meanwhile, scorepp had relatively high coverage and a mid-level F-score, while scorepat scored poorly on this dataset due to its sparsity; while all modifiers in the IQAP dataset are in the Google N-grams vocabulary, most do not have observed patterns and therefore return predictions of “NO” (item 2 in Figure 4).",6.2 Experiments,[0],[0]
"As in the global ranking experiments, the paraphrase-based evidence is complementary to the lexicon-based evidence, and thus the combined scoresocal+pp and scoresocal+pat+pp produce significantly better accuracy than any score in isolation (McNemar’s test, p < .01), and also out-perform the original expected ranking method of de Marneffe et al. (2010) (although they do not beat the best-reported score on this dataset, F-score=0.706 (Kim and de Marneffe, 2013)).",6.2 Experiments,[0],[0]
We have proposed adjectival paraphrases as a new source of evidence for predicting intensity relationships between scalar adjectives.,7 Conclusion,[0],[0]
"While paraphrase-based intensity evidence produces pairwise predictions that are less precise than those produced by pattern- or lexicon-based evidence, the coverage is substantially higher.",7 Conclusion,[0],[0]
Thus paraphrases can be successfully used as a complementary source of information for reasoning about adjective intensity.,7 Conclusion,[0],[0]
"This material is based in part on research sponsored by the following organizations: the Allen Institute for Artificial Intelligence (AI2) Key Scientific Challenges program, the Google Ph.D. Fellowship program, the French National Research Agency under project ANR-16-CE33-0013, and DARPA under grant numbers FA8750-13-2-0017 (the DEFT program) and HR0011-15-C-0115 (the LORELEI program).",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.,Acknowledgments,[0],[0]
"The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA and the U.S. Government.
",Acknowledgments,[0],[0]
We are grateful to our anonymous reviewers for their thoughtful and constructive comments.,Acknowledgments,[0],[0]
"Adjectives like warm, hot, and scalding all describe temperature but differ in intensity.",abstractText,[0],[0]
Understanding these differences between adjectives is a necessary part of reasoning about natural language.,abstractText,[0],[0]
We propose a new paraphrasebased method to automatically learn the relative intensity relation that holds between a pair of scalar adjectives.,abstractText,[0],[0]
"Our approach analyzes over 36k adjectival pairs from the Paraphrase Database under the assumption that, for example, paraphrase pair really hot↔ scalding suggests that hot < scalding.",abstractText,[0],[0]
"We show that combining this paraphrase evidence with existing, complementary patternand lexicon-based approaches improves the quality of systems for automatically ordering sets of scalar adjectives and inferring the polarity of indirect answers to yes/no questions.",abstractText,[0],[0]
Learning Scalar Adjective Intensity from Paraphrases,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1733–1742, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Multiword Expressions (MWEs) are sequences of words that exhibit some kind of idiosyncrasy.,1 Introduction,[0],[0]
"This idiosyncrasy can be semantic, statistical, or syntactic1.",1 Introduction,[0],[0]
"Ivory tower, speed limit, and at large
1MWEs can have other less significant kinds of idiosyncrasies.",1 Introduction,[0],[0]
"For instance lexical idiosyncrasy as in ad hoc, and pragmatic idiosyncrasy as in good morning (Baldwin and Kim, 2010)
are examples of semantically, statistically and syntactically idiosyncratic MWEs respectively.",1 Introduction,[0],[0]
Note that an MWE can be idiosyncratic at several levels.,1 Introduction,[0],[0]
"In general, semantically idiosyncratic MWEs are commonly referred to as non-compositional (Baldwin and Kim, 2010) and statistically idiosyncratic MWEs are commonly referred to as collocations (Sag et al., 2002).",1 Introduction,[0],[0]
Non-compositional MWEs are those whose meaning can not be readily inferred from the meaning of their constituents and collocations are those MWEs whose constituents co-occur more than expected by chance.,1 Introduction,[0],[0]
"Collocations constitute the largest subset of all kinds of MWEs, however, non-compositional ones cause more problems in various NLP tasks, for example word sense disambiguation (McCarthy et al., 2003) and machine translation (Lin, 1999).",1 Introduction,[0],[0]
It may also be more challenging to model noncompositionality than collocational weight as the former has to do with modelling the semantics and the latter can to some extent be modeled by conventional statistical measures such as mutual information.,1 Introduction,[0],[0]
"Detecting non-compositionality in an automatic fashion has been the aim of much previous research.
",1 Introduction,[0],[0]
"In this paper, we capture non-compositionality of English Noun Compounds (NCs)2 based on the assumption that the majority of the compounds are compositional, for which a composition function can be learned.",1 Introduction,[0],[0]
"This implies that the compounds for which a composition function cannot be learned with a relatively low error are noncompositional.
",1 Introduction,[0],[0]
"In previous work on vector-space models of distributional semantics, semantic composition has been commonly assumed to be a trivial predetermined function such as addition, multiplication,
2MWEs have various syntactic categories such as noun compounds, verb particle constructions, light verb constructions, etc., with noun compounds and verb particle constructions constituting the most prominent categories of MWEs.
1733
and their weighted variations (Mitchell and Lapata, 2008; Reddy et al., 2011; Salehi et al., 2015).",1 Introduction,[0],[0]
Nevertheless there is some work that regards composition as a more complex function.,1 Introduction,[0],[0]
"For instance Widdows (2008) who propose (but doesn’t empirically test) the use of Tensor and Convolution products for modelling non-compositionality, Baroni and Zamparelli (2010) who regard adjectives in adjectival-noun compositions as matrices that can be learned by linear regression, and Socher et al. (2012) who present a model that learns phrase composition by means of a recursive neural network.",1 Introduction,[0],[0]
The two latter works show that complex composition models significantly outperform additive and multiplicative functions.,1 Introduction,[0],[0]
"In this work, we too assume that composition is arguably a complex function.",1 Introduction,[0],[0]
"We believe simplified composition functions, such as additive and multiplicative functions and their weighted variations, while having advantages such as being impervious to overfitting, can not completely capture semantic composition.",1 Introduction,[0],[0]
Nevertheless modelling composition by means of a powerful function can be equally inadequate for our purposes.,1 Introduction,[0],[0]
"An overly powerful composition function memorizes all compositional and non-compositional compounds, resulting in overfitting and low learning error that hinders discrimination between compositional and non-compositional compounds.",1 Introduction,[0],[0]
"We examine various classes of composition functions, ranging from the least to the most powerful (in terms of learning capacity).",1 Introduction,[0],[0]
"We show that complex functions clearly do a better job in modelling semantic composition and in detecting noncompositionality compared to commonly used additive and multiplicative functions.
",1 Introduction,[0],[0]
"Compositional compounds are also decomposable; intuitively, their semantics is the union of the semantics of their components.",1 Introduction,[0],[0]
"More formally, conditioned on the vector of the compound, vectors of the component words should be independently predictable.",1 Introduction,[0],[0]
"This principle, together with the assumption that most of the compounds are compositional, leads to the conclusion that a model of composition should be able to be auto-reconstructive: the composition function that maps component-words’ vectors to their compound vector should have an associated decomposition function that independently predicts each of the component-words’ vectors from this compound vector.",1 Introduction,[0],[0]
"An auto-reconstructive model en-
ables us to exploit more data in order to learn semantic composition and predict compositionality.",1 Introduction,[0],[0]
"We show that auto-reconstruction can improve the accuracy of composition functions and improve detecting non-compositionality.
",1 Introduction,[0],[0]
"To further improve non-compositionality detection, we propose an EM-like detection algorithm based on hidden compositionality annotations.",1 Introduction,[0],[0]
The best composition is the one that is the best fit on all the data points except the noncompositional ones.,1 Introduction,[0],[0]
"Since we don’t use annotated data at training time, we assume annotations to be hidden variables and iteratively alternate between optimizing the composition function and optimizing the hidden compositionality annotations.",1 Introduction,[0],[0]
"We show that this iterative algorithm increases the accuracy of non-compositionality detection compared to the case when training is done on all examples.
",1 Introduction,[0],[0]
We run our experiments on the data set of Farahmand et al. (2015) who provide a set of English NCs which are annotated with noncompositionality judgments.,1 Introduction,[0],[0]
We show that quadratic regression significantly outperforms additive and multiplicative baselines and all other models in modelling semantic composition and identifying the non-compositional NCs.,1 Introduction,[0],[0]
"In short, the contributions of our work are: to empirically evaluate various composition functions ranging from simple to overly complex in order to find the most accurate function; to propose, to the best of our knowledge for the first time, a method of identifying non-compositional phrases as phrases for which a composition function cannot be readily learned; to propose learning decomposability as another criterion to detect non-compositionality; and to examine possible ways of improving the accuracy of the models by means of EM on hidden compositionality annotations.",1 Introduction,[0],[0]
"To the best of our knowledge, attempts to extract non-compositionality in computational linguistics go back to 1998.",2 Related Work,[0],[0]
Tapanainen et al. (1998) propose a method to identify non-compositional verbobject collocations based on the semantic asymmetry of verb-object relation.,2 Related Work,[0],[0]
"They assume that in a verb-object idiomatic expression, the object is a more interesting element in the sense that if the object appears with one (or only a few) verbs in a large corpus, it presumably has an id-
iomatic nature.",2 Related Work,[0],[0]
Lin (1999) argues that the mutual information between the constituents of a non-compositional phrase is significantly different from that of a phrase created by substituting the constituents of that phrase with their similar words.,2 Related Work,[0],[0]
Their evaluation reveals a low precision (16 − 39%) and recall (14 − 21%).,2 Related Work,[0],[0]
In any case this method is not able to discriminate non-compositional MWEs from collocational MWEs as they share the same property of nonsubstitutability (their constituents cannot be replaced with their synonyms).,2 Related Work,[0],[0]
Baldwin et al. (2003) present a method that decides about the non-compositionality of English NCs and verb particle constructions by using latent semantic analysis to calculate the similarity between a MWE and its components.,2 Related Work,[0],[0]
They argue that a higher similarity indicates a higher degree of compositionality.,2 Related Work,[0],[0]
McCarthy et al. (2003) devise a number of measures based on comparison of the neighbors of phrasal verbs and their corresponding simplex verbs.,2 Related Work,[0],[0]
They evaluate these measures by calculating their correlation with human compositionality judgments on a set of phrasal verbs.,2 Related Work,[0],[0]
They show that some of the measures have significant correlations with human judgments.,2 Related Work,[0],[0]
Venkatapathy and Joshi (2005) present a supervised model that benefits from both collocational and contextual information and ranks the MWE candidates based on their non-compositionality.,2 Related Work,[0],[0]
Katz and Giesbrecht (2006) use distributional semantics and LSA as a model of context similarity to test whether the local context of a MWE can distinguish its idiomatic use from literal use.,2 Related Work,[0],[0]
They further compare the context of a MWE with the context of its components and show that this can be used to decide whether the expression is idiomatic or not.,2 Related Work,[0],[0]
Cook et al. (2007) is a relatively different work where the authors propose a syntactic approach to identify semantic non-compositionality of verb-noun MWEs.,2 Related Work,[0],[0]
"McCarthy et al. (2007) use various models of selectional preferences for detecting non-compositional verb-object pairs.
",2 Related Work,[0],[0]
"Reddy et al. (2011) employ the additive and multiplicative composition functions presented by Mitchell and Lapata (2008)3 and several similar-
3Mitchell and Lapata (2008) present an analysis of vectorbased additive and multiplicative semantic composition models where each words is represented by its distributional vector.",2 Related Work,[0],[0]
"They conclude that multiplicative and combined models do a better job in modelling vector-based semantic composition than other models.
",2 Related Work,[0],[0]
ity based models to measure the compositionality of MWEs.,2 Related Work,[0],[0]
Similarity based models measure the similarity of a MWE vector and sum/product of its constituents’ vectors.,2 Related Work,[0],[0]
"Their evaluation (which is carried out on a set of 90 annotated NCs) shows that there is a relatively high correlation (Spearman ρ of between 0.51 and 0.71) between their models’ predictions and human judgments on noncompositionality of English NCs, with weighted additive function outperforming all the other models.",2 Related Work,[0],[0]
Kiela and Clark (2013) present a model of detecting non-compositionality based on the hypothesis that the average distance between a phrase vector and its substituted phrase vectors is related to its compositionality.,2 Related Work,[0],[0]
In particular compositional phrases are less similar to their neighbors in semantic space.,2 Related Work,[0],[0]
"The distributional vectors representing the semantics of words were created using the standard window method and 50,000 most frequent context words.",2 Related Work,[0],[0]
"They show that their model slightly (+0.014 and +0.007) outperforms their baselines (Venkatapathy and Joshi, 2005; McCarthy et al., 2007).
",2 Related Work,[0],[0]
All of the models mentioned so far are based on conventional4 or count based vector space representation of the words.,2 Related Work,[0],[0]
More recent works however are based on representation learning of word embeddings.,2 Related Work,[0],[0]
Baroni and Zamparelli (2010) regard adjective as matrices and nouns as real-valued vectors for Italian adjective noun composition.,2 Related Work,[0],[0]
They learn the adjective matrices by linear regression.,2 Related Work,[0],[0]
"In this work, however, every adjective is presented by a new matrix which leads to a large number of parameters.",2 Related Work,[0],[0]
"Socher et al. (2012) suggest that composition function is a matrix that multiplies on the word vectors, and Mikolov et al. (2013b) present a model of learning non-compositional phrases by calculating a data-driven score for certain frequent phrases (up to size two) and learn them as a whole.",2 Related Work,[0],[0]
"Salehi et al. (2015) borrow the word embeddings from (Mikolov et al., 2013a) to model the semantics of words and use several composition functions from (Mitchell and Lapata, 2008; Reddy et al., 2011) to predict the non-compositionality of MWEs.",2 Related Work,[0],[0]
"They compare the performance of word embeddings with conventional distributional vector representations and discover the superiority of word embeddings in predicting non-compositionality of MWEs.
4Conventional or count based models of distributional similarity as oppose to word embeddings (Salehi et al., 2015; Baroni et al., 2014).",2 Related Work,[0],[0]
"In order to represent words and compounds we use word embeddings, which are a form of vector space models.",3 Representation of Words and Compounds,[0],[0]
Vector space models represent the semantics of words and phrases with real valued vectors.,3 Representation of Words and Compounds,[0],[0]
"Word embeddings have proven to be effective models of semantic representation of words and outperform the count-based models in various NLP tasks (Baroni et al., 2014; Collobert et al., 2011; Collobert and Weston, 2008; Yazdani and Popescu-Belis, 2013; Huang et al., 2012; Mikolov et al., 2013c).",3 Representation of Words and Compounds,[0],[0]
"They have been successfully applied to semantic composition (Mikolov et al., 2013b) and outperformed the conventional count based contextual models in predicting noncompositionality of MWEs (Salehi et al., 2015).
",3 Representation of Words and Compounds,[0],[0]
In this work we use word embeddings of Mikolov et al. (2013a) to represent the semantics of words and compounds.,3 Representation of Words and Compounds,[0],[0]
We chose an English Wikipedia dump as our corpus.,3 Representation of Words and Compounds,[0],[0]
After filtering HTML tags and noise we POS-tagged the corpus and extracted ≈ 70k compounds whose frequency of occurrence was above 50.,3 Representation of Words and Compounds,[0],[0]
We learn the embeddings of these compounds as single tokens using the word2vec5 bag-of-word model.,3 Representation of Words and Compounds,[0],[0]
"We also learn the embeddings of the compounds of the evaluation set, plus the embeddings of all the compounds’ component words.",3 Representation of Words and Compounds,[0],[0]
Compounds’ sizes are restricted to two (i.e. bigrams) for the sake of simplicity and to respect the evaluation set standards.,3 Representation of Words and Compounds,[0],[0]
The compounds and word embeddings are then used as supervised signals to learn a composition function.,3 Representation of Words and Compounds,[0],[0]
"After the unsupervised learning of word embeddings and candidate compound embeddings (see section 3), we use these embeddings as supervised signals in order to train our composition functions.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"The term supervised might be misleading as the models do not have any information about the compositionality of the compounds during the training phase, and in that respect it is unsupervised.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"To describe the models in a formal way, throughout the paper we use the following notations: d represents the size of embeddings, φ(wi) represents embedding of wi, and φ̃(wi−wj) =
5https://code.google.com/p/word2vec/
f(φ(wi), φ(wj)) represents the learned embedding of bigram wi−wj by the composition function f .",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"The training error of bigram wi−wj by f is eij = ‖φ̃(wi−wj)−φ(wi, wj)‖, and ‖‖ is norm 2.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"The composition functions are described in the following sections.
",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"Given unsupervised embeddings for both words and compounds, a composition model is trained to map the word embeddings to the compound embeddings, with norm 2 error eij defined above.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
Then this same error for this same task (norm 2 between predicted and unsupervised compound embeddings) is used to measure noncompositionality.,4 Supervised Models of Composition on Word Embeddings,[0],[0]
"In other words, we learn a composition function (with several models) and identify non-compositional expressions as those for which the error of this composition function is high.
",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"We explore various classes of composition functions of word embeddings, ranging from simple to complex, to find the most effective one.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"We want a composition function that is powerful enough to learn composition for compositional compounds, but simple enough that it fails to learn composition for non-compositional compounds.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"To this end, we investigate linear projections, polynomial projections, and neural networks.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"We try these models with and without sparsity regularisation, which reduces the number of non-zero parameters while otherwise keeping the complexity of the function that can be learned.",4 Supervised Models of Composition on Word Embeddings,[0],[0]
"In this model we assume that the embedding of a bigram is a linear projection of its component words’ embeddings.
f(φ(wi), φ(wj)) =",4.1 Linear Projection,[0],[0]
"[φ(wi), φ(wj)]θ2d×d
To train this function we optimize the least square error, which gives us a multi-variant linear regression.
",4.1 Linear Projection,[0],[0]
min,4.1 Linear Projection,[0],[0]
"θ ‖[φ(wi), φ(wj)]θ2d×d",4.1 Linear Projection,[0],[0]
"− φ(wi, wj)‖
As mentioned earlier, a composition function that doesn’t overfit the training data and induces a more meaningful error is more suitable for our purpose.",4.1 Linear Projection,[0],[0]
"One effective way of reducing overfitting and increasing generalization is by keeping only the important parameters of the model, which is done by enforcing sparsity on model parameters.
",4.1 Linear Projection,[0],[0]
"In case of sparse linear projections, only a few elements of the projection matrix θ are non-zero.",4.1 Linear Projection,[0],[0]
"This means that not all dimensions of the latent space has a role in all dimensions of the compound embedding.
",4.1 Linear Projection,[0],[0]
"To apply sparsity on θ, we add a norm 1 penalty on it and add that to the least square optimization.",4.1 Linear Projection,[0],[0]
"This forms a multi-variant lasso regression (Tibshirani, 2011).
",4.1 Linear Projection,[0],[0]
min,4.1 Linear Projection,[0],[0]
"θ ‖[φ(wi), φ(wj)]θ",4.1 Linear Projection,[0],[0]
"− φ(wi, wj)‖+ λ|θ|
Figure 1 shows the transformation matrices of linear projection and sparse linear projection.",4.1 Linear Projection,[0],[0]
"The two diagonals of the matrices correspond to the sum of the two embeddings, which we can see are the main component of the sparse function, and play an important role in the non-sparse one.",4.1 Linear Projection,[0],[0]
"We will see that despite being an important component of these functions, sum alone is not capable of accurately modelling semantic composition.",4.1 Linear Projection,[0],[0]
"Polynomial projection is a non-linear projection that assumes the relation between compound embedding and the component words’ embeddings
should be a polynomial of degree",4.2 Polynomial Projection,[0],[0]
n. This can be viewed as a form of linear regression where first a polynomial transformation is applied to the input vector and then a linear projection is fitted.,4.2 Polynomial Projection,[0],[0]
"If ψ shows the polynomial transformation then we have:
f(φ(wi), φ(wj))",4.2 Polynomial Projection,[0],[0]
"= ψ([φ(wi), φ(wj)])θ
We couldn’t successfully apply any polynomial beyond quadratic transformation without overfitting.",4.2 Polynomial Projection,[0],[0]
"The case of a quadratic ψ transformation is:
ψ(x) = x21, · · ·x2n︸ ︷︷ ︸",4.2 Polynomial Projection,[0],[0]
"Pure quadratic , x1x2, · · ·xn−1xn︸ ︷︷ ︸ interaction terms , x1, · · ·xn︸ ︷︷ ︸ linear terms
Similar to the linear case we can have sparse version of the polynomial regression in which we allow the presence of only a few non-zero elements in the θ matrix.",4.2 Polynomial Projection,[0],[0]
The sparsity regularizer is more important in the case of polynomial regression as we have many more parameters.,4.2 Polynomial Projection,[0],[0]
The quadratic model is similar to Recursive Neural Tensor compositionality model of Socher et al. (2013).,4.2 Polynomial Projection,[0],[0]
But in our model the tensor is symmetric around the diagonal.,4.2 Polynomial Projection,[0],[0]
Figure 2 shows the pure quadratic transformation matrices.,4.2 Polynomial Projection,[0],[0]
"A feed forward neural network is a universal approximator (Cybenko, 1989): feed-forward network with a single hidden layer can approximate any continuous function, provided it has enough hidden units.",4.3 Neural Networks,[0],[0]
Therefore we use neural networks as a powerful class of learning models to learn semantic composition.,4.3 Neural Networks,[0],[0]
"The number of hidden units gives us a measure to control expressiveness of our model.
f(φ(wi), φ(wj))",4.3 Neural Networks,[0],[0]
"= σ([φ(wi), φ(wj)]Wih)Who
Similar to the previous models, we optionally impose sparsity over weight matrices of the neural network to be able to induce more meaningful learning errors.",4.3 Neural Networks,[0],[0]
We evaluated the above models on the data set of Farahmand et al. (2015).,4.4 Experimental Results,[0],[0]
They provide a set of 1042 English NCs with four non-compositionality judgments.,4.4 Experimental Results,[0],[0]
The judgments are binary decisions taken by four experts about whether or not a compound is non-compositional.,4.4 Experimental Results,[0],[0]
We calculate a votebased non-compositionality score for each of the data set compounds by summing over its noncompositionality judgments.,4.4 Experimental Results,[0],[0]
The neural network models are trained using stochastic gradient descent.,4.4 Experimental Results,[0],[0]
"We use the additive and multiplicative models of modelling composition and detecting noncompositionality presented by Salehi et al. (2015) and (Reddy et al., 2011) as state of the art baselines.
",4.4 Experimental Results,[0],[0]
The results are shown in Table 1.,4.4 Experimental Results,[0],[0]
The second column shows the correlation between different models’ predictions and the annotated data in terms of Spearman ρ.,4.4 Experimental Results,[0],[0]
"The last three columns show the performance of different models in terms of Normalized Discounted Cumulative Gain (NDCG), F1 score and and Precision at 100 (P@100).",4.4 Experimental Results,[0],[0]
For these three scores we consider the problem of predicting noncompositional NCs a problem with a binary solution where we assume compounds (of the evaluation set) with at least two non-compositionality votes are non-compositional.,4.4 Experimental Results,[0],[0]
NDCG assigns a higher score to a ranked list of compounds if the non-compositional ones are ranked higher in the list.,4.4 Experimental Results,[0],[0]
"F1 column represents the maximum F1 score on the top-n elements of the ranked list returned by the corresponding model for all n in
[1 − size-of-ranked-list].",4.4 Experimental Results,[0],[0]
P@100 shows the precision at the first 100 compounds ranked as noncompositional.,4.4 Experimental Results,[0],[0]
The models are listed in the order of complexity of the composition function.,4.4 Experimental Results,[0],[0]
The addition-based baseline which was explored in a variety of previous work does not seem to be as powerful as the other models.,4.4 Experimental Results,[0],[0]
It is outperformed by almost all learned models.,4.4 Experimental Results,[0],[0]
"In general, we can see that more complex functions tend to learn compositionality in a more effective way.
",4.4 Experimental Results,[0],[0]
"As mentioned earlier, overly powerful learners overfit and do not produce meaningful errors for the detection task.",4.4 Experimental Results,[0],[0]
Sparsity seems to address this issue by reducing the number of non-zero parameters while the function can still keep the complex terms if needed.,4.4 Experimental Results,[0],[0]
"In general sparse models show improvement over their non-sparse counterparts, specifically for more powerful models.",4.4 Experimental Results,[0],[0]
"In this section, we investigate the hypothesis that we can detect non-compositionality better by not only modelling a composition function, but also modelling a decomposition function.",5 Auto-reconstructive Models,[0],[0]
"For compositional compounds, given the meaning of the compound, the meaning of the two component words should be conditionally independent.",5 Auto-reconstructive Models,[0],[0]
We therefore assume that the decomposition function predicts the component words’ vectors independently.,5 Auto-reconstructive Models,[0],[0]
Let us illustrate this assumption by examining the non-compositional compound flag stop.,5 Auto-reconstructive Models,[0],[0]
"Given the semantics of this compound (a point at which a vehicle in public transportation stops only
on prearrangement or signal6), we can not readily predict one of its component words without knowing the other.",5 Auto-reconstructive Models,[0],[0]
Now consider the compositional compound hip injury.,5 Auto-reconstructive Models,[0],[0]
"Given the semantics of this compound it is much easier to predict each of its component words independently.
",5 Auto-reconstructive Models,[0],[0]
"In the previous section, the training signals came from the embeddings of the candidate compounds and their component words.",5 Auto-reconstructive Models,[0],[0]
In this section we extend our model such that it can benefit from more training signals.,5 Auto-reconstructive Models,[0],[0]
"To this end, we formalize the assumption that a compositional compound is also decomposable as an auto-reconsructive model.",5 Auto-reconstructive Models,[0],[0]
"We thus add this hypothesis to the learning process: a good composition function not only builds the semantics of the compound from the semantics of its component words, but it also allows the independent prediction of the semantics of its component words from the compound semantics.",5 Auto-reconstructive Models,[0],[0]
In the following sections we add this assumption to both linear projection (which encompasses polynomial) and Neural Network models.,5 Auto-reconstructive Models,[0],[0]
"Let YM×d be a matrix whose rows are the precomputed compound embeddings, and XM×2d be a matrix whose rows are the concatenation of the embeddings for the words of these compounds.",5.1 Auto-reconstructive Linear Models,[0],[0]
"Let AN×2d be another matrix where every row contains the concatenation of the embeddings for the words of a compound, but this matrix includes many compounds for which we did not precompute compound embeddings.",5.1 Auto-reconstructive Linear Models,[0],[0]
We assume that the rows of matrixA include the rows of matrixX .,5.1 Auto-reconstructive Linear Models,[0],[0]
"In linear models the auto-reconstructive objective function is as follows:
min θ,θ′ ‖Xθ",5.1 Auto-reconstructive Linear Models,[0],[0]
− Y ‖+,5.1 Auto-reconstructive Linear Models,[0],[0]
"λ‖Aθθ′ −A‖
where λ is a meta-parameter for the importance of the auto-reconstruction in the objective.",5.1 Auto-reconstructive Linear Models,[0],[0]
"A schematic of this model is shown in Figure 3a.
",5.1 Auto-reconstructive Linear Models,[0],[0]
"We can look at this problem as the following weighted least square problem:
min θ,θ′ ‖ ( X A ) θ",5.1 Auto-reconstructive Linear Models,[0],[0]
− ( Y Aθ′T (θ′θ′T )−1 ),5.1 Auto-reconstructive Linear Models,[0],[0]
"‖ 1... λ  In the above matrix formula we transformed the auto-reconstruction part of the objective to a
6Definition taken from Merriam-Webster Dictionary
pseudo regressand of the least square.",5.1 Auto-reconstructive Linear Models,[0],[0]
"To solve this optimization we design an efficient alternating least squares algorithm.
",5.1 Auto-reconstructive Linear Models,[0],[0]
"First we initialize θ0 to be the answer of the original multi-variant linear regression, θ0=X \Y where X \ =(XTX)−1XT is the pseudoinverse of X .",5.1 Auto-reconstructive Linear Models,[0],[0]
Let us assume W is the diagonal matrix with first M elements of the diagonal being 1 and the remaining N being λ.,5.1 Auto-reconstructive Linear Models,[0],[0]
We alternate between the following formulas until the algorithm converges.,5.1 Auto-reconstructive Linear Models,[0],[0]
"First we approximate the next θ′ based on the current approximation of θ, then we use this value of θ′ to calculate the pseudo regressand part of the least square.",5.1 Auto-reconstructive Linear Models,[0],[0]
"In the final step we solve the weighted least square for this new regressand matrix and continue iterating these stages until the algorithm converges.
",5.1 Auto-reconstructive Linear Models,[0],[0]
θ′t =,5.1 Auto-reconstructive Linear Models,[0],[0]
"(Aθt) \A (1)
",5.1 Auto-reconstructive Linear Models,[0],[0]
X2 =,5.1 Auto-reconstructive Linear Models,[0],[0]
( X A ) Y2 =,5.1 Auto-reconstructive Linear Models,[0],[0]
"( Y
Aθ′Tt−1(θ′t−1θ′Tt−1)−1
) (2)
",5.1 Auto-reconstructive Linear Models,[0],[0]
"θt = (XT2 WX2) −1(X2)TWY2 (3)
",5.1 Auto-reconstructive Linear Models,[0],[0]
The above algorithm can also be used in the case of polynomial regression.,5.1 Auto-reconstructive Linear Models,[0],[0]
The only thing that needs to be done is to replace X and A by their polynomial transformations.,5.1 Auto-reconstructive Linear Models,[0],[0]
The auto-reconstructive neural network follows the same idea.,5.2 Auto-reconstructive Neural Networks,[0],[0]
"The objective function changes to:
min Wih,Whi,Woh ‖σ(XWih)Who − Y ‖+",5.2 Auto-reconstructive Neural Networks,[0],[0]
λ‖σ(AWih)Whi −A‖,5.2 Auto-reconstructive Neural Networks,[0],[0]
"(4)
Figure 3b shows the schematic of this model.",5.2 Auto-reconstructive Neural Networks,[0],[0]
We optimize this objective using stochastic gradient descent with early stopping.,5.2 Auto-reconstructive Neural Networks,[0],[0]
The results are shown in Table 2.,5.2 Auto-reconstructive Neural Networks,[0],[0]
We choose the first 300K frequent noun-noun compounds from the corpus in order to build matrix A.,5.2 Auto-reconstructive Neural Networks,[0],[0]
Each row of A created by concatenating the component words vectors.,5.2 Auto-reconstructive Neural Networks,[0],[0]
The results show that the auto-reconstructive models generally improve over their counterparts.,5.2 Auto-reconstructive Neural Networks,[0],[0]
"As mentioned earlier, the improvement comes from two facts.",5.2 Auto-reconstructive Neural Networks,[0],[0]
On the one hand we increase the training signals by implementing the decomposability hypothesis.,5.2 Auto-reconstructive Neural Networks,[0],[0]
"On the other hand, the auto-reconstructive model enables us to exploit more data in addition to the candidate compounds.",5.2 Auto-reconstructive Neural Networks,[0],[0]
There is almost no improvement in the case of linear model because this model does not have enough learning capacity to benefit from a higher number of training signals.,5.2 Auto-reconstructive Neural Networks,[0],[0]
All the models discussed in this paper are unsupervised since they don’t have any access to labels specifying compositionality of compounds.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"The above models simply assume that most compounds are compositional, and therefore train their composition and decomposition functions on all compounds.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"In this section we incorporate in the models an intrinsic uncertainty about the compositionality annotation of the training set.
",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
The best (optimum) composition function is the one that fits well all the compositional compounds and does not fit the non-compositional ones.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
But we assume that we do not have training labels indicating compositionality.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"To overcome this uncertainty and improve the learning process, we introduce latent compositionality labels to the model.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"We assume each candidate compound has a latent annotation, 1 or 0, showing
whether or not it is compositional.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
Let us assume a non-compositionality detection system that returns B non-compositional candidates that should have their own lexical unit and parameters.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
The objective of this composition function training is to minimize the error of compositional compounds and not the error of non-compositional ones.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"In order to implement this objective we use the following loss function:
min λij ,θ ∑ ij λije 2 ij
s.t λij ∈ {0, 1},∑ ij λij = N −B
where λij represents the hidden compositionality annotation and eij is again the learning error for the pairwi−wj .",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
We want to find theB points such that annotating them as non-compositional results in the minimum error of this objective.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"The algorithm that alternates between optimizing the composition learning and the hidden annotations eventually converges to this solution.
",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"If the errors are fixed, the B compounds with the biggest errors are the answers to the noncompositional annotation optimization of that iteration.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
Therefore to solve this optimization we follow an EM-like algorithm:,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
First we set all λij to 1 and perform the optimization on the composition function.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"Then we sort the compounds by their error and set the λij of the biggest B elements to 0, and the rest to 1.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
In other words we assume the compounds with big error are presumably non-compositional according to what we know until that iteration.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
We continue alternating between training the composition function and annotating high error points until the algorithm reaches convergence.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
The results are shown in Table 3.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
"Models that use latent annotations clearly outperform their counterparts, especially in terms
of precision at 100.",6 Non-compositionality Detection Using Latent Annotations,[0],[0]
This is expected since at training time we consider a model that returns B noncompositional compounds and therefore precision at 100 is optimized.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
The latent annotations do not improve the linear model since the model is simple and there is not much room to improve its learning.,6 Non-compositionality Detection Using Latent Annotations,[0],[0]
We proposed a framework to detect noncompositional compounds as the compounds that stand out as outliers in the process of learning compositionality of English noun compounds.,7 Conclusions,[0],[0]
We proposed and evaluated a range of functions with a variety of complexities that model semantic composition.,7 Conclusions,[0],[0]
We showed that learners such as polynomial projection and neural networks which are distinctly more complex than commonly used additive and multiplicative functions can model semantic composition more effectively.,7 Conclusions,[0],[0]
We showed that a function as complex as quadratic projection is a better learner of compositionality than simpler models.,7 Conclusions,[0],[0]
We further showed that enforcing sparsity is an effective way of learning a complex composition function while avoiding overfitting and producing meaningful learning errors.,7 Conclusions,[0],[0]
"Furthermore, we improved our models by incorporating an autoreconstructive loss function that enables us to benefit from more training signals and cover more data.",7 Conclusions,[0],[0]
"Finally, we addressed the intrinsic label uncertainty in training data by considering latent annotations, and showed that it can further improve the results.",7 Conclusions,[0],[0]
"This research was partially funded by Hasler foundation project no. 15019, “Deep Neural Network Dependency Parser for Context-aware Representation Learning”.",Acknowledgments,[0],[0]
"Non-compositionality of multiword expressions is an intriguing problem that can be the source of error in a variety of NLP tasks such as language generation, machine translation and word sense disambiguation.",abstractText,[0],[0]
We present methods of non-compositionality detection for English noun compounds using the unsupervised learning of a semantic composition function.,abstractText,[0],[0]
Compounds which are not well modeled by the learned semantic composition function are considered noncompositional.,abstractText,[0],[0]
"We explore a range of distributional vector-space models for semantic composition, empirically evaluate these models, and propose additional methods which improve results further.",abstractText,[0],[0]
"We show that a complex function such as polynomial projection can learn semantic composition and identify non-compositionality in an unsupervised way, beating all other baselines ranging from simple to complex.",abstractText,[0],[0]
We show that enforcing sparsity is a useful regularizer in learning complex composition functions.,abstractText,[0],[0]
We show further improvements by training a decomposition function in addition to the composition function.,abstractText,[0],[0]
"Finally, we propose an EM algorithm over latent compositionality annotations that also improves the performance.",abstractText,[0],[0]
Learning Semantic Composition to Detect Non-compositionality of Multiword Expressions,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1391–1400, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Hierarchical phrase-based translation (Chiang, 2007) explores formal synchronous context free grammar (SCFG) rules for translation.",1 Introduction,[0],[0]
Two types of nonterminal symbols are used in translation rules: nonterminal X in ordinary SCFG rules and nonterminal S in glue rules that are specially introduced to concatenate nonterminal Xs in a monotonic manner.,1 Introduction,[0],[0]
"The same generic symbol X for all ordinary nonterminals makes it difficult to distinguish and select proper translation rules.
",1 Introduction,[0],[0]
"In order to address this issue, researchers either use syntactic labels to annotate nonterminal Xs (Zollmann and Venugopal, 2006; Zollmann and Vogel, 2011; Li et al., 2012; Hanneman and Lavie, 2013), or employ syntactic information
∗Corresponding author
from parse trees to refine nonterminals with realvalued vectors (Venugopal et al., 2009; Huang et al., 2013).",1 Introduction,[0],[0]
"In addition to syntactic knowledge, semantic structures are also leveraged to refine nonterminals (Gao and Vogel, 2011).",1 Introduction,[0],[0]
"All these efforts focus on incorporating linguistic knowledge into hierarchical translation rules.
",1 Introduction,[0],[0]
"Unfortunately, syntactic or semantic parsers for many languages are not accessible due to the lack of labeled training data.",1 Introduction,[0],[0]
"In contrast, a large amount of unlabeled data are easily available.",1 Introduction,[0],[0]
"Therefore, can we mine syntactic or semantic properties for nonterminals from unlabeled data?",1 Introduction,[0],[0]
"Or can we exploit these data to refine nonterminals for SMT?
",1 Introduction,[0],[0]
"Learning semantic representations for terminals (words, multi-word phrases or sentences) from unlabeled data has achieved substantial progress in recent years (Mitchell and Lapata, 2008; Turian et al., 2010; Socher et al., 2010; Mikolov et al., 2013c; Blunsom et al., 2014).",1 Introduction,[0],[0]
These representations have been used successfully in various NLP tasks.,1 Introduction,[0],[0]
"However, there is no attempt to learn semantic representations for nonterminals from unlabeled data.",1 Introduction,[0],[0]
In this paper we propose a framework to learn semantic representations for nonterminal Xs in translation rules.,1 Introduction,[0],[0]
"Our framework is established on the basis of realvalued vector representations learned for multiword phrases, which are substituted with nonterminal Xs during hierarchical rule extraction.",1 Introduction,[0],[0]
We propose a weighted mean value and a minimum distance method to obtain nonterminal representations from representations of their phrasal substitutions.,1 Introduction,[0],[0]
We further build a semantic nonterminal refinement model with semantic representations of nonterminals to compute similarities between phrasal substitutions and nonterminals.,1 Introduction,[0],[0]
"In doing so, we want to enhance phrasal substitution and translation rule selection during decoding.
",1 Introduction,[0],[0]
"The big challenge here is that thousands of tar-
1391
get phrasal substitutions will be generated for one single nonterminal during decoding.",1 Introduction,[0],[0]
Computing vector representations for all these phrases will be very time-consuming.,1 Introduction,[0],[0]
We therefore introduce two different methods to handle it.,1 Introduction,[0],[0]
"In the first method, we project representations of source phrases onto their target counterparts linearly/nonlinearly via a neural network.",1 Introduction,[0],[0]
These projected vectors are used as approximations to real target representations to compute semantic similarities.,1 Introduction,[0],[0]
"In the second method, we decode sentences in two passes.",1 Introduction,[0],[0]
The first pass collects target phrase candidates from n-best translations of sentences generated by the baseline.,1 Introduction,[0],[0]
"The second pass calculates vector representations of these collected target phrases and then computes similarities between them and target-side nonterminals.
",1 Introduction,[0],[0]
Our contributions are two-fold.,1 Introduction,[0],[0]
"First, we learn semantic representations for nonterminals from their phrasal substitutions with two different methods.",1 Introduction,[0],[0]
"This is the first time, to the best of our knowledge, to induce semantic representations for nonterminals from unlabeled data in the context of SMT.",1 Introduction,[0],[0]
"Second, we successfully address the issue of time-consuming target-side phrase-nonterminal similarity computation mentioned above.",1 Introduction,[0],[0]
We incorporate both source-/target-side semantic nonterminal refinement model and their combination based on learned nonterminal representations into translation system.,1 Introduction,[0],[0]
"Experiment results show that our method can achieve an improvement of 1.16 BLEU points over the baseline system on NIST MT evaluation test sets.
",1 Introduction,[0],[0]
The rest of this paper is organized as follows.,1 Introduction,[0],[0]
Section 2 briefly reviews related work.,1 Introduction,[0],[0]
"Section 3 presents our approach of learning semantic vectors for nonterminals, followed by Section 4 describing the details of our semantic nonterminal refinement model.",1 Introduction,[0],[0]
Section 5 introduces the integration of the proposed model into SMT.,1 Introduction,[0],[0]
Experiment results are reported in Section 6.,1 Introduction,[0],[0]
"Finally, we conclude our work in Section 7.",1 Introduction,[0],[0]
A variety of approaches have been explored for nonterminal refinement in hierarchical phrasebased translation.,2 Related Work,[0],[0]
"These approaches can be categorized into two groups: 1) augmenting the nonterminal symbol X with informative labels, and 2) attaching distributional linguistic knowledge to each nonterminal in hierarchical rules.",2 Related Work,[0],[0]
"The former
only allows substitution operations with matched labels.",2 Related Work,[0],[0]
"The latter normally builds an additional model as a new feature of the log-linear model to incorporate attached knowledge.
",2 Related Work,[0],[0]
"Among approaches which directly refine the single label to more fine-grained labels, syntactic and semantic knowledge are explored in various ways.",2 Related Work,[0],[0]
The syntactically augmented translation model (SAMT) proposed by Zollmann and Venugopal (2006) uses syntactic categories extracted from target-side parse trees to augment nonterminals in hierarchical rules.,2 Related Work,[0],[0]
"Unfortunately, there is a data sparseness problem in this model due to thousands of extracted syntactic categories.",2 Related Work,[0],[0]
One solution to address this issue is to reduce the number of syntactic categories.,2 Related Work,[0],[0]
"Zollmann and Vogel (2011) use word tags, generated by either POS tagger or unsupervised word class induction, instead of syntactic categories.",2 Related Work,[0],[0]
"Hanneman and Lavie (2013) coarsen the label set by introducing a label collapsing algorithm to SAMT grammars (Zollmann and Venugopal, 2006).",2 Related Work,[0],[0]
Yet another solution is easing restrictions on label matching.,2 Related Work,[0],[0]
Shen et al. (2009) penalize substitution with unmatched labels while Chiang (2010) uses soft match features to model substitutions with various labels.,2 Related Work,[0],[0]
"Similar to Zollmann and Venugopal (2006), Hoang and Koehn (2010) decorate some hierarchical rules with source-side syntax information and use undecorated, decorated, and partially decorated rules in their translation model.",2 Related Work,[0],[0]
Mylonakis and Sima’an (2011) employ source-side syntax-based labels to define a joint probability synchronous grammar.,2 Related Work,[0],[0]
"Combinatory Categorial Grammar (CCG) labels or CCG contextual labels are also used to enrich nonterminals (Almaghout et al., 2011; Weese et al., 2012).",2 Related Work,[0],[0]
Li et al. (2012) incorporate head information extracted from source-side dependency structures into translation rules.,2 Related Work,[0],[0]
"Besides, semantic knowledge is also used to refine nonterminals.",2 Related Work,[0],[0]
Gao and Vogel (2011) utilize target-side semantic roles to form SRL-aware SCFG rules.,2 Related Work,[0],[0]
"Most of approaches introduced here explicitly require syntactic or semantic parsers trained on manually labeled data.
",2 Related Work,[0],[0]
"On the other hand, efforts have also been directed towards attaching distributional linguistic knowledge to nonterminals.",2 Related Work,[0],[0]
Venugopal et al. (2009) propose a preference grammar to annotate nonterminals based on preference distributions of syntactic categories.,2 Related Work,[0],[0]
"Huang et al. (2010) learn la-
tent syntactic distributions for each nonterminal.",2 Related Work,[0],[0]
They use these distributions to decorate nonterminal Xs in SCFG rules with a real-valued feature vectors and utilize these vectors to measure the similarities between source phrases and applied rules.,2 Related Work,[0],[0]
"Similar to this work, Huang et al. (2013) utilize treebank tags based on dependency parsing to learn latent distributions.",2 Related Work,[0],[0]
"Cao et al. (2014) attach translation rules with dependency knowledge, which contains both dependency relations inside rules and dependency relations between rules and their contexts.
",2 Related Work,[0],[0]
"The difference of our work from these studies is that our semantic representations are learned from unlabeled bilingual (or monolingual) data and do not depend on any linguistic resources, e.g., parsers.",2 Related Work,[0],[0]
"We also believe that our model is able to exploit both syntactic and semantic information for nonterminals since vector representations learned in our way are able to capture both syntactic and semantic properties (Turian et al., 2010; Socher et al., 2010).",2 Related Work,[0],[0]
"In our framework, semantic representations for nonterminal Xs are automatically induced from word-aligned parallel corpus.",3 Learning Semantic Representations for Nonterminals,[0],[0]
"In this section, we detail the essential component of our approach, i.e., how to learn semantic vectors for nonterminals and how to project source semantic vectors onto target language semantic space.",3 Learning Semantic Representations for Nonterminals,[0],[0]
"Before discussing nonterminal representations, we briefly introduce vector representations for words and phrases.",3 Learning Semantic Representations for Nonterminals,[0],[0]
"We employ a neural method, specifically the continuous bag-of-words model (Mikolov et al., 2013a) to learn high-quality vector representations for words.",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"Once we complete the training of the continuous bag-of-words model, word embeddings form an embedding matrix M ∈ Rd×|V |, where d is a pre-determined embedding dimensionality and each word w in the vocabulary V corresponds to a vector ~v ∈ Rd.",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"Given the embedding matrix M , mapping words to vectors can be done by simply looking up their respective columns in M .
",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"We further feed these learned word embeddings
to recursive autoencoders (RAE) (Socher et al., 2011) for learning phrase representations.",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"In traditional RAE (shown in Figure 1), given two input children representation vectors ~c1 ∈ Rd and ~c2 ∈ Rd , their parent representation ~p can be calculated as follows:
~p = f (1)(W (1)[~c1; ~c2] + b(1))",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"(1)
where [~c1; ~c2] ∈ R2d is the concatenation of vectors of two children,",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"W (1) ∈ Rd×2d is a weight matrix, b(1) ∈",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"Rd is a bias term, and f (1) is an element-wise activation function such as tanh.",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
The above output representation ~p can be used as a child vector to construct the representation for a larger subphrase.,3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"This process is repeated until a binary tree covering the whole input phrase is generated.
",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"In order to evaluate how well the parent vector represents its children, we can reconstruct the children in a reconstruction layer:
",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
[~c1 ′ ; ~c2 ′ ],3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
= f (2)(W (2)~p+ b(2)),3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"(2)
where ~c1 ′ and ~c2 ′
are the reconstructed children, W (2) is a weight matrix for reconstruction, b(2) is a bias term for reconstruction, and f (2) is an element-wise activation function.
",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"For each node in the generated binary tree, we compute Euclidean distance between the original input vectors and the reconstructed vectors to measure the reconstruction error:
Erec([~c1; ~c2]) = 1 2 ‖[~c1; ~c2]− [~c1′ ; ~c2′ ]‖2 (3)
By minimizing the total reconstruction error over all nonterminal nodes, we can learn parameters of RAE.
",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
Socher et al. (2011) propose a greedy unsupervised RAE as an extension to the above traditional RAE.,3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
The main difference is that in the unsupervised RAE there is no tree structure which is given for traditional RAE.,3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
It can learn both representations and tree structures of phrases or sentences.,3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"In this work, we adopt the unsupervised RAE to learn vector representations for phrases.",3.1 Prerequisite: Learning Words and Phrases Representations,[0],[0]
"As we extract hierarchical rules from phrases by replacing subphrases with nonterminal symbols, a nonterminal X is generalized from a number of
subphrases.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
We believe that these subphrases determine syntactic and semantic properties of the nonterminal X .,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"We therefore enrich each nonterminalX with a semantic vector induced from vector representations of phrases that are replaced by the nonterminal during rule extraction.
",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"For an SCFG rule, we can learn semantic vectors for nonterminals on both the source and target side.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Due to the space limitation, we introduce the procedure of learning nonterminal vectors on the source side.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Semantic vectors on the target side can be learned analogically.
",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"For each source-side nonterminal X of a hierarchical rule, we collect all source subphrases replaced by X in a source subphrase set P = {p1, p2, · · · , pm}.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
We also count the number of times of these phrases being replaced by nonterminal X on training data during rule extraction.,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"We collect these numbers in a count set C = {c1, c2, · · · , cm}.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Based on the phrase set P , count set C and learned phrase vector representations in P , we can compute a semantic vector ~vx for nonterminal X in each SCFG rule.
",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
We propose two general approaches to obtain semantic vectors for nonterminals: a weighted mean value method and a minimum distance method.,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Given phrase vector representations ~Pr = {~p1, ~p2, . . .",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
", ~pm} , we calculate the semantic vector for a nonterminal generalized from these phrases as follows.
",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Weighted mean value method (MV) computes semantic vector ~vx as:
~vx = ∑m
i=1 ci · ~pi∑m i=1",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"ci
(4)
Minimum distance method (MD) finds a point in semantic space to minimize the sum of Eu-
clidean distances of vectors in ~Pr to this point.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Formally,
~vx = argmin ~vx",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
m∑ i=1,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
√√√√ d∑ j=1 (pij − vxj)2,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"(5)
We use the stochastic gradient descent algorithm to find the minimal distance and the point ~vx.",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
The component vxj can be updated by vxj ← vxj,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
+ λ ∂f∂vxj where f is ∑m i=1 √∑d,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
j=1(pij,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
− vxj)2 and λ is the learning rate.,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"Similar to the center of gravity, the semantic vector ~vx learned by this method acts as a semantic centroid for all vectors of phrases that are substituted by X .",3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
Nonterminals in different hierarchical translation rules will have different semantic centroids.,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
These centroids will help translation model capture semantic diversity to a certain degree.,3.2 Inducing Nonterminal Representations from Phrase Representations,[0],[0]
"As we discussed in Section 1, directly learning vector representations for target phrases is very costly in practice.",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"Inspired by Mikolov et al. (2013b), we adopt vector projection to alleviate this problem.",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"Different from mapping representations from the source side to the target side by learning a linear matrix on word alignments (Mikolov et al., 2013b), we project source multiword phrase representations onto the target semantic space in a nonlinear manner as we believe that nonlinear relations between languages are more reasonable.",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"Specifically, we use a neural network to achieve this goal.",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
Our neural network is a multilayer feed-forward neural network with one hidden layer.,3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"The functional form can be written in the following equation:
~p = tanh(W (4)(tanh(W (3) ~src) + b(3))",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
+,3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
b(4)),3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"(6) where ~src is the input vector which is learned in the source semantic space, W (3) denotes the weight matrix for connections between input and hidden neurons and W (4) denotes the weight matrix for links between hidden neurons and output, b(3) and b(4) are bias terms.",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"To train the neural network, we optimize the following objective:",3.3 Mapping Source-Side Representations onto Target-Side Semantic Space,[0],[0]
"W (3),W (4) 1 N N∑ i=1 ‖",J = argmin,[0],[0]
"~trgi − ~pi‖2 +R(θ) (7)
where N is the number of training examples, ~trgi is the target vector representation for the ith example learned by RAE and ~pi is the output of the neural network for the source vector representation ~srci of ith example.",J = argmin,[0],[0]
"R(θ) is the regularizer on parameters:
R(θ) =",J = argmin,[0],[0]
"λL 2 ‖W‖2 (8)
where W denotes parameters for parameter matrices W (3), W (4) and bias terms b(3) , b(4).",J = argmin,[0],[0]
"In this section, we describe our semantic nonterminal refinement model on the basis of induced real-valued semantic vectors for nonterminals.",4 Semantic Nonterminal Refinement Model,[0],[0]
We incorporate learned semantic representations of nonterminals into hierarchical rules.,4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
"In particular, ordinary hierarchical rules take the following form:
X → 〈aXsb, cXtd〉 (9)
where a/b, c/d are strings of terminals on the source and target side, s and t are placeholders denoting the nonterminal X on the source or target side, Xs and Xt are aligned to each other.
",4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
Representations for nonterminals can be on either the source or target side.,4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
"They are attached to hierarchical rules as follows:
",4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
"X → 〈aXsb, cXtd, ~vxs, ~vxt〉 (10)
where ~vx. is the source- or target-side semantic representation for nonterminal.",4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
"In this way, we keep original translation rules intact and decorate nonterminals with their semantic representations.",4.1 Nonterminal Representations in Hierarchical Rules,[0],[0]
The proposed semantic nonterminal refinement model estimates the semantic similarity between a phrase p and nonterminal X .,4.2 The Model,[0],[0]
The phrase p and nonterminal X will have a high similarity score in the representation space if they are semantically similar.,4.2 The Model,[0],[0]
"The higher semantic similarity scores are, the more compatible nonterminals are with corresponding phrases.
",4.2 The Model,[0],[0]
"There is another nonterminal S in glue rules, which are formalized as follows:
S → 〈S1X2, S1X2〉 (11)
S → 〈X1, X1〉 (12)",4.2 The Model,[0],[0]
This nonterminal S is different from X .,4.2 The Model,[0],[0]
"We therefore treat it as a special case in the computation of semantic similarity.
",4.2 The Model,[0],[0]
"In this work, we explore two approaches to compute similarity: one based on cosine similarity and the other based on Euclidean distance.
",4.2 The Model,[0],[0]
"Given a phrase vector representation ~p and nonterminalX semantic vector ~vx, Cosine Similarity (CS) is computed as:
cos(~p, ~vx) = ~p · ~vx ‖~p‖‖ ~vx‖ (13)
We set α for the Cosine Similarity between the glue rule and its corresponding phrase as follows:
SeSim = { cos(~p, ~vx) hierarchical rules
α glue rules (14)
",4.2 The Model,[0],[0]
"As for Euclidean Distance (ED), it is computed according to the following formula:
dist(~p, ~vx) = √√√√ d∑ i=1",4.2 The Model,[0],[0]
"(pi − vxi)2 (15)
and similarly we set β for glue rules:
",4.2 The Model,[0],[0]
"SeSim = { dist(~p, ~vx) hierarchical rules
β glue rules (16)",4.2 The Model,[0],[0]
We incorporate the proposed model as a new feature into the hierarchical phrase-based translation system.,5 Decoding,[0],[0]
"Specifically, two features are added into the baseline system:
1.",5 Decoding,[0],[0]
"Source-side semantic similarity between source phrases and nonterminals
2.",5 Decoding,[0],[0]
"Target-side semantic similarity between target phrases and nonterminals
We compute source- and target-side similarities based on representations of nonterminals and phrasal substitutions for each applied rule, and sum up these similarities to calculate the total score of a derivation on the two features.
",5 Decoding,[0],[0]
The integration of the source-side semantic nonterminal refinement model into the decoder is trivial.,5 Decoding,[0],[0]
"For the target-side model, however, we have to consider the efficiency issue as we mentioned in Section 1.",5 Decoding,[0],[0]
We introduce two different methods to integrate the target-side model into the decoder: 1) projection and 2) two-pass decoding.,5 Decoding,[0],[0]
"In the first integration method, a mapping neural network is trained to map source phrase representations onto the target semantic space as described in Section 3.3.",5 Decoding,[0],[0]
The projection can be linear if we remove the hidden layer in the projection neural network.,5 Decoding,[0],[0]
This is similar to the mapping matrix learned by Mikolov et al. (2013b).,5 Decoding,[0],[0]
We calculate semantic similarities between projected representations of phrases and those of nonterminals.,5 Decoding,[0],[0]
"In the two-pass decoding, we collect target phrase candidates from 100-best translations for each source sentence generated by the baseline in the first pass and learn vector representations for these target phrase candidates.",5 Decoding,[0],[0]
"Then in the second pass, we decode source sentence with our target semantic nonterminal refinement model using learned target phrase vector representations.",5 Decoding,[0],[0]
"If a target phrase appears in the collected set, the target-side semantic nonterminal refinement model will calculate the semantic similarity between the target phrase and the corresponding nonterminal on the target semantic space; otherwise the model will give a penalty.",5 Decoding,[0],[0]
"This is because this phrase is not a desirable phrase as it is not used in 100-best translations.
",5 Decoding,[0],[0]
"The weights of these two features are tuned by the Minimum Error Rate Training (MERT)(Och, 2003), together with weights of other sub-models on a development set.",5 Decoding,[0],[0]
Figure 2 shows the architecture of SMT system with the proposed semantic nonterminal refinement model.,5 Decoding,[0],[0]
"In this section, we conducted a series of experiments on Chinese-to-English translation using large-scale bilingual training data, aiming at the following questions:
1.",6 Experiment,[0],[0]
"Which approach is better for learning nonterminal representations, weighted mean value or minimum distance?
2.",6 Experiment,[0],[0]
Can the target-side semantic nonterminal refinement model improve translation quality?,6 Experiment,[0],[0]
"And which method is better for integrating the target-side semantic model into translation, projection or two-pass decoding?
3.",6 Experiment,[0],[0]
Does the combination of source and target semantic nonterminal refinement models provide further improvement?,6 Experiment,[0],[0]
Our training corpus contains 2.9M sentence pairs with 80.9M Chinese words and 86.4M English words from LDC data1.,6.1 Setup,[0],[0]
"We used NIST MT03 as our development set, NIST MT06 as our development test set and MT08 as our final test set.
",6.1 Setup,[0],[0]
"We ran Giza++ on the training corpus in both Chinese-to-English and English-to-Chinese directions and applied the “grow-diag-final” refinement rule (Koehn et al., 2003) to obtain word alignments.",6.1 Setup,[0],[0]
"We used the SRI Language Modeling Toolkit2 (Stolcke and others, 2002) to train our language models.",6.1 Setup,[0],[0]
"MERT (Och, 2003) was adopted to tune feature weights of the decoder.",6.1 Setup,[0],[0]
We used the case-insensitive BLEU3 as our evaluation metric.,6.1 Setup,[0],[0]
"In order to alleviate the instability of MERT , we followed Clark et al. (2011) to perform three runs of MERT and reported average BLEU scores over the three runs for all our experiments.
",6.1 Setup,[0],[0]
We used word2vec toolkit4 to train our word embeddings and set the vector dimension d to 30.,6.1 Setup,[0],[0]
"In our training experiment, we used the continuous bag-of-words model with a context window of size 5.",6.1 Setup,[0],[0]
"The monolingual corpus, which was used to pre-train word embeddings, is extracted from
1The corpora include LDC2003E14, LDC2004T07, LDC2005T06, LDC2005T10 and LDC2004T08 (Hong Kong Hansards/Laws/News).
2http://www.speech.sri.com/projects/srilm/download.html 3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl 4https://code.google.com/p/word2vec/
the above parallel corpus in SMT.",6.1 Setup,[0],[0]
"To train vector representations for multi-word phrases, we randomly selected 1M bilingual sentences 5 as training set and used the unsupervised greedy RAE following (Socher et al., 2011).",6.1 Setup,[0],[0]
"We used a learning rate of 10−3 for our minimum distance method that learned the centroid of phrase representations as the vector representation of the corresponding nonterminal.
",6.1 Setup,[0],[0]
"For projection neural network in Section 3.3, we set 300 units for the hidden layer and dimensionality of 30 for both input and output vectors.",6.1 Setup,[0],[0]
Learning rate was set to 10−3 and the regularization coefficient λL was set to 10−3.,6.1 Setup,[0],[0]
"To construct the training set for the projection neural network, we selected phrase pairs from our rule table and used their representations on the source and target side as training examples.",6.1 Setup,[0],[0]
"We randomly selected 5M examples as training set, 10k examples as development set and 10k examples as test set.",6.1 Setup,[0],[0]
"The multi-layer projection neural network was trained with the back-propagation and stochastic gradient descent algorithm with a mini-batch size of 5k.
",6.1 Setup,[0],[0]
"Our baseline system is an in-house hierarchical phrase-based system (Chiang, 2007).",6.1 Setup,[0],[0]
"The features used in the baseline system includes a 4-gram language model trained on the Xinhua section of the English Gigaword corpus, a 3-gram language model trained on the target part of the bilingual training data, bidirectional translation probabilities, bidirectional lexical weights, a word count, a phrase count and a glue rule count.
",6.1 Setup,[0],[0]
"In order to compare our proposed models with previous methods on nonterminal refinement, we re-implemented a syntax mismatch model (SynMis) which was used by Huang et al. (2013) and integrated it into hierarchical phrase-based system.",6.1 Setup,[0],[0]
Syn-Mis model decorates each nonterminal with a distribution of head POS tags and uses this distribution to measure the degree of syntactic compatibility of translation rules with corresponding source spans.,6.1 Setup,[0],[0]
"In order to obtain head POS tags for Syn-Mis model, we used the Stanford dependency parser 6 (Chang et al., 2009) to parse Chinese sentences in our training corpus and NIST development/test sets.
",6.1 Setup,[0],[0]
"5We choose bilingual sentences because we want to obtain bilingual training examples to train our projection neural network as described in Section 3.3.
6http://nlp.stanford.edu/software/lex-parser.shtml",6.1 Setup,[0],[0]
Our first group of experiments were carried out to investigate which approach is more appropriate to learn semantic vectors for nonterminals.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
We only used the source-side semantic nonterminal refinement model in these experiments.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"In order to validate the effectiveness of the proposed approaches for learning nonterminal semantic vectors, we combined the minimum distance method (MD) with the Euclidean Distance (ED) because both of them are distance-based, and combined the weighted mean value method (MV) with the Cosine Similarity model (CS) as they belong to vector-based approaches.",6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"We chose α = 1.0, 0, -1.0 and β = 0, 0.5, 1.0 for glue rules to study the impact of these parameters.",6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"We compared our model with the baseline and Syn-Mis model.
",6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
Results are shown in Table 1.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"From Table 1, we observe that the proposed two approaches are able to achieve significant improvements over the baseline.",6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
(MV + CS) and (MD + ED) achieve up to an absolute improvement of 1.09 and 0.81 (when α = 0 and β = 0.5),6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
BLEU points respectively over the baseline on the development test set MT06.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
And the approach (MV + CS) with α = 0 outperforms Syn-Mis by 0.4 BLEU points on MT06 without using any syntactic information.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
The approach (MV + CS) achieves better performance and it is more efficient than (MD + ED) where the computation of semantic centroids is time-consuming.,6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"Therefore, we adopt the approach (MV + CS) with α = 0 to learn semantic vectors for nonterminals and compute semantic similarities in the following experiments.",6.2 Different Approaches to Learn Vector Representations for Nonterminals,[0],[0]
"In the second set of experiments, we further validate the effectiveness of semantic nonterminal vectors learned on the target side.",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"In these experiments, learning vector representations and computing semantic similarities were performed on the target language semantic space.",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
We also compared the two integration methods discussed in Section 5 for the target-side model.,6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"With regard to the projection method, we further compared the linear projection (the projection neural network without hidden layer) with the nonlinear projection (with hidden layer).",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"Experiment results are shown in Table 2.
",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"From Table 2, we can see that
• Two-pass decoding achieves the highest BLEU scores, which are higher than those of the baseline by 0.75 and 0.66 BLEU points on MT06 and MT08 respectively.",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
The reason may be that noisy translation candidates are filtered out in the first pass.,6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"This finding is consistent with many other multiple-pass systems in natural language processing, e.g., two-pass parsing (Zettlemoyer and Collins, 2007).
",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
• Nonlinear projection achieves an improvement of 0.62 BLEU points over the baseline on MT06.,6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
It outperforms linear projection method on both sets.,6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"These empirical results support our assumption that nonlinear relations between languages are more reasonable than linear relations.
",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"• The results prove that the target-side semantic nonterminal refinement model is also able
to improve the baseline system, although the gain is less than that of the source-side counterpart.",6.3 Effect of the Target Semantic Nonterminal Refinement Models,[0],[0]
"Finally, we integrated both the source- and targetside semantic nonterminal refinement models into the baseline system.",6.4 Combination of the Source and Target Models,[0],[0]
"In this experiment, we adopted nonlinear projection to obtain target semantic vector representations for target phrases.",6.4 Combination of the Source and Target Models,[0],[0]
"These two models collectively achieve a gain of up to 1.16 BLEU points over the baseline and 0.41 BLEU points over Syn-Mis model on average, which is shown in Table 3.",6.4 Combination of the Source and Target Models,[0],[0]
We have presented a framework to refine nonterminal X in hierarchical translation rules with semantic representations.,7 Conclusion,[0],[0]
"The semantic vectors are derived from vector representations of phrasal substitutions, which are automatically learned using an unsupervised RAE.",7 Conclusion,[0],[0]
"As the semantic nonterminal refinement model is capable of selecting more semantically similar translation rules, it achieves statistically significant improvements over the baseline on Chinese-to-English translation.",7 Conclusion,[0],[0]
"Experiment results have shown that
• Using (MV + CS) approach to learn semantic representations for nonterminals can achieve better performance than (MD + ED) in terms of BLEU scores.
",7 Conclusion,[0],[0]
• Target-side semantic nonterminal refinement model is able to substantially improve translation quality over the baseline.,7 Conclusion,[0],[0]
"Two-pass de-
coding method is superior to the projection method.
",7 Conclusion,[0],[0]
"• The simultaneous incorporation of the source- and target-side models can achieve further improvements over a single-side model.
",7 Conclusion,[0],[0]
"For the future work, we are interested in learning bilingual representations (Lauly et al., 2014; Gouws et al., 2014) for nonterminals.",7 Conclusion,[0],[0]
We also would like to extend our work by using more contextual lexical information to derive semantic vectors for nonterminals.,7 Conclusion,[0],[0]
"The work was sponsored by the National Natural Science Foundation of China (Grants No. 61403269, 61432013 and 61333018) and Natural Science Foundation of Jiangsu Province (Grant No. BK20140355).",Acknowledgment,[0],[0]
We would like to thank three anonymous reviewers for their insightful comments.,Acknowledgment,[0],[0]
"In hierarchical phrase-based translation, coarse-grained nonterminal Xs may generate inappropriate translations due to the lack of sufficient information for phrasal substitution.",abstractText,[0],[0]
In this paper we propose a framework to refine nonterminals in hierarchical translation rules with real-valued semantic representations.,abstractText,[0],[0]
The semantic representations are learned via a weighted mean value and a minimum distance method using phrase vector representations obtained from large scale monolingual corpus.,abstractText,[0],[0]
"Based on the learned semantic vectors, we build a semantic nonterminal refinement model to measure semantic similarities between phrasal substitutions and nonterminal Xs in translation rules.",abstractText,[0],[0]
Experiment results on ChineseEnglish translation show that the proposed model significantly improves translation quality on NIST test sets.,abstractText,[0],[0]
Learning Semantic Representations for Nonterminals in Hierarchical Phrase-Based Translation,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 236–246, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010).",1 Introduction,[0],[0]
One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain.,1 Introduction,[0],[0]
This is because sentiments are often expressed with domain-specific words and expressions.,1 Introduction,[0],[0]
"For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain.",1 Introduction,[0],[0]
"Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain.
",1 Introduction,[0],[0]
"Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer
et al., 2007; Pan et al., 2010; Bollegala et al., 2011; Ponomareva and Thelwall, 2012; Bollegala et al., 2016).",1 Introduction,[0],[0]
"Among them, an appealing method is the Structural Correspondence Learning (SCL) method (Blitzer et al., 2007), which uses pivot feature prediction tasks to induce a projected feature space that works well for both the source and the target domains.",1 Introduction,[0],[0]
The intuition behind is that these pivot prediction tasks are highly correlated with the original task.,1 Introduction,[0],[0]
"For sentiment classification, Blitzer et al. (2007) first chose pivot words which have high mutual information with the sentiment labels, and then set up the pivot prediction tasks to be the predictions of each of these pivot words using the other words.
",1 Introduction,[0],[0]
"However, the original SCL method is based on traditional discrete feature representations and linear classifiers.",1 Introduction,[0],[0]
"In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015).",1 Introduction,[0],[0]
"In these models, dense, real-valued feature vectors and non-linear classification functions are used.",1 Introduction,[0],[0]
"By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words.",1 Introduction,[0],[0]
"And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space.",1 Introduction,[0],[0]
"Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification.
236
There has been some recent studies on neural network-based domain adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014).",1 Introduction,[0],[0]
They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains.,1 Introduction,[0],[0]
"However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task.",1 Introduction,[0],[0]
"In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation.",1 Introduction,[0],[0]
"Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features (Yang and Eisenstein, 2015; Bollegala et al., 2015).",1 Introduction,[0],[0]
"Different from these methods, we aim to directly learn sentence embeddings that work well across domains.
",1 Introduction,[0],[0]
"In this paper, we aim to extend the main idea behind SCL to neural network-based solutions to sentiment classification to address the domain adaptation problem.",1 Introduction,[0],[0]
"Specifically, we borrow the idea of using pivot prediction tasks from SCL.",1 Introduction,[0],[0]
"But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights, which all relies on linear transformations, we introduce only two auxiliary binary prediction tasks and directly learn a non-linear transformation that maps an input to a dense embedding vector.",1 Introduction,[0],[0]
"Moreover, different from SCL and the auto-encoderbased methods, in which the hidden feature representation and the final classifier are learned sequentially, we propose to jointly learn the hidden feature representation together with the sentiment classification model itself, and we show that joint learning works better than sequential learning.
",1 Introduction,[0],[0]
We conduct experiments on a number of different source and target domains for sentence-level sentiment classification.,1 Introduction,[0],[0]
We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs.,1 Introduction,[0],[0]
"Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daumé III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011; Yu and Jiang, 2015).",2 Related Work,[0],[0]
"For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers.",2 Related Work,[0],[0]
"One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011).",2 Related Work,[0],[0]
"Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012).",2 Related Work,[0],[0]
"Our proposed method is similar to the first line of work in that we also aim to learn a general, cross-domain representation (sentence embeddings in our case).",2 Related Work,[0],[0]
"Neural Networks for Sentiment Classification: A recent trend of deep learning enhances various kinds of neural network models for sentiment classification, including Convolutional Neural Networks (CNNs), Recursive Neural Network (ReNNs) and Recurrent Neural Network (RNNs), which have been shown to achieve competitive results across different benchmarks (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015).",2 Related Work,[0],[0]
"Inspired by their success in standard indomain settings, it is intuitive for us to apply these neural network models to domain adaptation settings.",2 Related Work,[0],[0]
"Denoising Auto-encoders for Domain Adaptation: Denoising Auto-encoders have been extensively studied in cross-domain sentiment classification, since the representations learned through multilayer neural networks are robust against noise during domain adaptation.",2 Related Work,[0],[0]
"The initial application of this idea is to directly employ stacked denoising autoencoders (SDA) by reconstructing the original features from data that are corrupted with noise (Glorot et al., 2011), and Chen et al. (2012) proposed to analytically marginalize out the corruption during SDA training.",2 Related Work,[0],[0]
Later Yang and Eisenstein (2014) further showed that their proposed structured dropout noise strategy can dramatically improve the efficiency without sacrificing the accuracy.,2 Related Work,[0],[0]
"However, these methods are still based on traditional discrete representation and do not exploit the idea of using auxiliary tasks that are related to the end task.",2 Related Work,[0],[0]
"In contrast, the sentence embeddings learned from our method are derived from real-valued feature vectors and rely on related auxiliary tasks.",2 Related Work,[0],[0]
In this section we present our sentence embeddingbased domain adaptation method for sentiment classification.,3 Method,[0],[0]
We first introduce the necessary notation and an overview of our method.,3 Method,[0],[0]
we then delve into the details of the method.,3 Method,[0],[0]
We assume that each input is a piece of text consisting of a sequence of words.,3.1 Notation and Method Overview,[0],[0]
"For the rest of this paper, we assume each input is a sentence, although our method is general enough for longer pieces of text.",3.1 Notation and Method Overview,[0],[0]
"Let x = (x1, x2, . . .)",3.1 Notation and Method Overview,[0],[0]
"denote a sentence where each xi ∈ {1, 2, . . .",3.1 Notation and Method Overview,[0],[0]
", V } is a word in the vocabulary and V is the vocabulary size.",3.1 Notation and Method Overview,[0],[0]
"Let the sentiment label ofx be y ∈ {+,−}where + denotes a positive sentiment and − a negative sentiment.",3.1 Notation and Method Overview,[0],[0]
"We further assume that we are given a set of labeled training sentences from a source domain, denoted by Ds = {(xsi , ysi )}N s
i=1.",3.1 Notation and Method Overview,[0],[0]
"Also, we have a set of unlabeled sentences from a target domain, denoted by Dt = {xti}N t
i=1.",3.1 Notation and Method Overview,[0],[0]
"Our goal is to learn a good sentiment classifier from both Ds and Dt such that the classifier works well on the target domain.
",3.1 Notation and Method Overview,[0],[0]
"A baseline solution without considering any domain difference is to simply train a classifier using Ds, and with the recent advances in neural networkbased methods to sentence classification, we consider a baseline that uses a multi-layer neural network such as a CNN or an RNN to perform the classification task.",3.1 Notation and Method Overview,[0],[0]
"To simplify the discussion and focus on the domain adaptation ideas we propose, we will leave the details of the neural network model we use in Section 3.5.",3.1 Notation and Method Overview,[0],[0]
"For now, we assume that a multilayer neural network is used to transform each input x into a sentence embedding vector z. Let us use fΘ to denote the transformation function parameterized by Θ, that is, z = fΘ(x).",3.1 Notation and Method Overview,[0],[0]
"Next, we assume that a linear classifier such as a softmax classifier is learned to map z to a sentiment label y.
We introduce two auxiliary tasks which presumably are highly correlated with the sentiment classification task itself.",3.1 Notation and Method Overview,[0],[0]
Labels for these auxiliary tasks can be automatically derived from unlabeled data in both the source and the target domains.,3.1 Notation and Method Overview,[0],[0]
"With the help of the two auxiliary tasks, we learn a non-linear transformation function fΘ′ from unlabeled data and
use it to derive a sentence embedding vector z′ from sentence x, which supposedly works better across domains.",3.1 Notation and Method Overview,[0],[0]
Finally we use the source domain’s training data to learn a linear classifier on the representation z ⊕,3.1 Notation and Method Overview,[0],[0]
"z′, where ⊕ is the operator that concatenates two vectors.",3.1 Notation and Method Overview,[0],[0]
Figure 1 gives the outline of our method.,3.1 Notation and Method Overview,[0],[0]
Our two auxiliary tasks are about whether an input sentence contains a positive or negative domainindependent sentiment word.,3.2 Auxiliary Tasks,[0],[0]
The intuition is the following.,3.2 Auxiliary Tasks,[0],[0]
"If we have a list of domain-independent positive sentiment words, then an input sentence that contains one of these words, regardless of the domain the sentence is from, is more likely to contain an overall positive sentiment.",3.2 Auxiliary Tasks,[0],[0]
"For example, a sentence containing the word good is likely to be overall positive.",3.2 Auxiliary Tasks,[0],[0]
"Moreover, the rest of the sentence excluding the word good may contain domain-specific words or expressions that also convey a positive sentiment.",3.2 Auxiliary Tasks,[0],[0]
"For example, in the sentence “The laptop is good and goes really fast,” we can see that the word fast is a domain-specific sentiment word, and its sentiment polarity correlates with that of the word good, which is domain-independent.",3.2 Auxiliary Tasks,[0],[0]
"Therefore, we can hide the domain-independent positive words in a sentence and try to use the other words in the sentence to predict whether the original sentence contains a domain-independent positive word.",3.2 Auxiliary Tasks,[0],[0]
There are two things to note about this auxiliary task: (1) The label of the task can be automatically derived provided that we have the domain-independent positive word list.,3.2 Auxiliary Tasks,[0],[0]
(2) The task is closely related to the original task of sentence-level sentiment classification.,3.2 Auxiliary Tasks,[0],[0]
"Similarly, we can introduce a task to predict the existence of a domain-independent negative sentiment word in a sentence.
",3.2 Auxiliary Tasks,[0],[0]
"Formally, let us assume that we have two domainindependent sentiment word lists, one for the positive sentiment and the other for the negative sentiment.",3.2 Auxiliary Tasks,[0],[0]
Details of how these lists are obtained will be given in Section 3.5.,3.2 Auxiliary Tasks,[0],[0]
"Borrowing the term from SCL, we refer to these sentiment words as pivot words.",3.2 Auxiliary Tasks,[0],[0]
"For each sentence x, we replace all the occurrences of these pivot words with a special token UNK.",3.2 Auxiliary Tasks,[0],[0]
"Let g(·) be a function that denotes this procedure, that is, g(x) is the resulting sentence with
31
UNK tokens.",3.2 Auxiliary Tasks,[0],[0]
We then introduce two binary labels for g(x).,3.2 Auxiliary Tasks,[0],[0]
"The first label u indicates whether the original sentence x contains at least one domainindependent positive sentiment word, and the second label v indicates whether x contains at least one domain-independent negative sentiment word.",3.2 Auxiliary Tasks,[0],[0]
"Figure 1 shows an example sentence x, its modified version g(x) and the labels u and v for x.",3.2 Auxiliary Tasks,[0],[0]
"We further use Da = {(xi, ui, vi)}Nai=1 to denote a set of training sentences for the auxiliary tasks.",3.2 Auxiliary Tasks,[0],[0]
"Note that the sentences in Da can be from the sentences in Ds and Dt, but they can also be from other unlabeled sentences.",3.2 Auxiliary Tasks,[0],[0]
"With the two auxiliary tasks, we can learn a neural network model in a standard way to produce sentence embeddings that work well for the auxiliary tasks.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Specifically, we still use Θ′ to denote the parameters of the neural network that produces the sentence embeddings (and fΘ′ the corresponding transformation function), and we use β+ and β− to denote the parameters of two softmax classifiers for the two auxiliary tasks, respectively.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Using crossentropy loss, we can learn Θ′ by minimizing the following loss function:
J(Θ′,β+,β−)
=",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"− ∑
(x,u,v)∈Da
( log p(u|fΘ′(g(x));β+)
+ log p(v|fΘ′(g(x));β−) ) ,
where p(y|z;β) is the probability of label y given vector z and parameter β under softmax regression.
",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"With the learned Θ′, we can derive a sentence embedding z′ from any sentence.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Although we could simply use this embedding z′ for sentiment classification through another softmax classifier, this may not be ideal because z′ is transformed from g(x), which has the domain-independent sentiment words removed.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Similar to SCL and some other previous work, we concatenate the embedding vector z′ with the standard embedding vector z for the final classification.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Although we can learn Θ′ using Da as a first step, here we also explore a joint learning setting.",3.4 Joint Learning,[0],[0]
"In this setting, Θ′ is learned together with the neural network model used for the end task, i.e., sentiment classification.",3.4 Joint Learning,[0],[0]
"This way, the learning of Θ′ depends not only on Da but also on Ds, i.e., the sentimentlabeled training data from the source domain.
",3.4 Joint Learning,[0],[0]
"Specifically, we use Θ to denote the parameters for a neural network that takes the original sentence x and transforms it to a sentence embedding (and fΘ the corresponding transformation function).",3.4 Joint Learning,[0],[0]
We use γ to denote the parameters of a softmax classifier that operates on the concatenated sentence embedding z ⊕ z′ for sentiment classification.,3.4 Joint Learning,[0],[0]
"With joint learning, we try to minimize the following loss func-
tion:
J(Θ,Θ′,γ,β+,β−)
=",3.4 Joint Learning,[0],[0]
"− ∑
(x,y)∈Ds
( log p(y|fΘ(x)⊕ fΘ′(g(x));γ) )
",3.4 Joint Learning,[0],[0]
"− ∑
(x,u,v)∈Da
( log p(u|fΘ′(g(x));β+)
+ log p(v|fΘ′(g(x));β−) ) .
",3.4 Joint Learning,[0],[0]
We can see that this loss function contains two parts.,3.4 Joint Learning,[0],[0]
The first part is the cross-entropy loss based on the true sentiment labels of the sentences in Ds.,3.4 Joint Learning,[0],[0]
"The second part is the loss based on the auxiliary tasks and the data Da, which are derived from unlabeled sentences.
",3.4 Joint Learning,[0],[0]
"Finally, to make a prediction on a sentence, we use the learned Θ and Θ′ to derive a sentence embedding fΘ(x) ⊕ fΘ′(g(x)), and then use the softmax classifier parameterized by the learned γ to make the final prediction.",3.4 Joint Learning,[0],[0]
"In this section we explain some of the model details.
",3.5 Implementation Details,[0],[0]
"Pivot Word Selection Recall that the two auxiliary tasks depend on two domain-independent sentiment word lists, i.e., pivot word lists.",3.5 Implementation Details,[0],[0]
"Different from Blitzer et al. (2007), we employ weighted log-likelihood ratio (WLLR) to select the most positive and negative words in both domains as pivots.",3.5 Implementation Details,[0],[0]
The reason is that in our preliminary experiments we observe that mutual information (used by Blitzer et al. (2007)) is biased towards low frequency words.,3.5 Implementation Details,[0],[0]
Some high frequency words including good and great are scored low.,3.5 Implementation Details,[0],[0]
"In comparison, WLLR does not have this issue.",3.5 Implementation Details,[0],[0]
"The same observation was also reported previously by Li et al. (2009).
",3.5 Implementation Details,[0],[0]
"More specifically, we first tokenize the sentences in Ds and Dt and perform part-of-speech tagging using the NLTK toolkit.",3.5 Implementation Details,[0],[0]
"Next, we extract only adjectives, adverbs and verbs with a frequency of at least 3 in the source domain and at least 3 in the target domain.",3.5 Implementation Details,[0],[0]
We also remove negation words such as not and stop words using a stop word list.,3.5 Implementation Details,[0],[0]
"We then measure each remaining candidate word’s relevance to the positive and the negative classes based on Ds
by computing the following scores:
r(w, y) = p̃(w|y) log p̃(w|y) p̃(w|ȳ) ,
where w is a word, y ∈ {+,−} is a sentiment label, ȳ is the opposite label of y, and p̃(w|y) is the empirical probability of observing w in sentences labeled with y.",3.5 Implementation Details,[0],[0]
"We can then rank the candidate words in decreasing order of r(w,+) and r(w,−).",3.5 Implementation Details,[0],[0]
"Finally, we select the top 25% from each ranked list as the final lists of pivot words for the positive and the negative sentiments.",3.5 Implementation Details,[0],[0]
"Some manual inspection shows that most of these words are indeed domain-independent sentiment words.
",3.5 Implementation Details,[0],[0]
Neural Network Model,3.5 Implementation Details,[0],[0]
Our framework is general and potentially we can use any neural network model to transform an input sentence to a sentence embedding vector.,3.5 Implementation Details,[0],[0]
"In this paper, we adopt a CNN-based approach because it has been shown to work well for sentiment classification.",3.5 Implementation Details,[0],[0]
"Specifically, each word (including the token UNK) is represented by a word embedding vector.",3.5 Implementation Details,[0],[0]
"Let W ∈ Rd×V denote the lookup table for words, where each column is a d-dimensional embedding vector for a word type.",3.5 Implementation Details,[0],[0]
"Two separate CNNs are used to process x and g(x), and their mechanisms are the same.",3.5 Implementation Details,[0],[0]
"For a word xi in each CNN, the embedding vectors inside a window of size n centered at i are concatenated into a new vector, which we refer to as ei ∈ Rnd.",3.5 Implementation Details,[0],[0]
"A convolution operation is then performed by applying a filter F ∈ Rh×nd on ei to produce a hidden vector hi = m(Fei + b), where b ∈ Rh is a bias vector and m is an elementwise non-linear transformation function.",3.5 Implementation Details,[0],[0]
Note that we pad the original sequence in front and at the back to ensure that at each position i we have n vectors to be combined into hi.,3.5 Implementation Details,[0],[0]
"After the convolution operation is applied to the whole sequence, we obtain H =",3.5 Implementation Details,[0],[0]
"[h1,h2, . . .",3.5 Implementation Details,[0],[0]
"], and we apply a max-over-time pooling operator to take the maximum value of each row of H to obtain an overall hidden vector, i.e., z for x and z′ for g(x).
",3.5 Implementation Details,[0],[0]
It is worth noting that the two neural networks corresponding to fΘ and fΘ′ share the same word embedding lookup table.,3.5 Implementation Details,[0],[0]
"This lookup table is initialized with word embeddings from word2vec1 and
1https://code.google.com/p/word2vec/
is updated during our learning process.",3.5 Implementation Details,[0],[0]
Note that the token UNK is initialized as a zero vector and never updated.,3.5 Implementation Details,[0],[0]
"Although our method is inspired by SCL, there are a number of major differences: (1) Our method is based on neural network models with continuous, dense feature representations and non-linear transformation functions.",3.6 Differences from SCL,[0],[0]
"SCL is based on discrete, sparse feature vectors and linear transformations.",3.6 Differences from SCL,[0],[0]
"(2) Although our pivot word selection is similar to that of SCL, in the end we only use two auxiliary tasks while SCL uses much more pivot prediction tasks.",3.6 Differences from SCL,[0],[0]
"(3) We can directly learn the transformation function f ′Θ that produces the hidden representation, while SCL relies on SVD to learn the projection function.",3.6 Differences from SCL,[0],[0]
"(4) We perform joint learning of the auxiliary tasks and the end task, i.e., sentiment classification, while SCL performs the learning in a sequential manner.",3.6 Differences from SCL,[0],[0]
"To evaluate our proposed method, we conduct experiments using five benchmark data sets.",4.1 Data Sets and Experiment Settings,[0],[0]
The data sets are summarized in Table 1.,4.1 Data Sets and Experiment Settings,[0],[0]
"Movie12 and Movie23 are movie reviews labeled by Pang and Lee (2005) and Socher et al. (2013), respectively.",4.1 Data Sets and Experiment Settings,[0],[0]
"Camera4 are reviews of digital products such as MP3 players and cameras (Hu and Liu, 2004).",4.1 Data Sets and Experiment Settings,[0],[0]
"Laptop and Restaurant5 are laptop and restaurant reviews taken
2https://www.cs.cornell.edu/people/pabo/ movie-review-data/
3http://nlp.stanford.edu/sentiment/ 4http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html 5Note that the original data set is for aspect-level sentiment analysis.",4.1 Data Sets and Experiment Settings,[0],[0]
"We remove sentences with opposite polarities towards different aspects, and use the consistent polarity as the sentencelevel sentiment of each remaining sentence.
from SemEval 2015 Task 12.",4.1 Data Sets and Experiment Settings,[0],[0]
We consider 18 pairs of data sets where the two data sets come from different domains.6,4.1 Data Sets and Experiment Settings,[0],[0]
"For neural network-based methods, we randomly pick 200 sentences from the target domain as the development set for parameter tuning, and the rest of the data from the target domain as the test data.",4.1 Data Sets and Experiment Settings,[0],[0]
We consider the following baselines: Naive is a non-domain-adaptive baseline based on bag-of-word representations.,4.2 Baselines and Hyperparameters,[0],[0]
SCL is our implementation of the Structural Correspondence Learning method.,4.2 Baselines and Hyperparameters,[0],[0]
"We set the number of induced features K to 100 and rescale factor α = 5, and we use 1000 pivot words based on our preliminary experiments.",4.2 Baselines and Hyperparameters,[0],[0]
"mDA is our implementation of marginalized Denoising Auto-encoders (Chen et al., 2012), one of the state-of-the-art domain adaptation methods, which learns a shared hidden representation by reconstructing pivot features from corrupted inputs.",4.2 Baselines and Hyperparameters,[0],[0]
"Following Yang and Eisenstein (2014), we employ the efficient and effective structured dropout noise strategy without any parameter.",4.2 Baselines and Hyperparameters,[0],[0]
The top 500 features are chosen as pivots based on our preliminary experiments.,4.2 Baselines and Hyperparameters,[0],[0]
"NaiveNN is a non-domain-adaptive baseline based on CNN, as described in Section 3.5.",4.2 Baselines and Hyperparameters,[0],[0]
"Aux-NN is a simple combination of our auxiliary tasks with NaiveNN, which treats the derived label of two auxiliary tasks as two features and then appends them to the hidden representation learned from CNN, followed by a softmax classifier.",4.2 Baselines and Hyperparameters,[0],[0]
"SCL-NN is a naive combination of SCL with NaiveNN, which appends the induced representation from SCL to the hidden representation learned from CNN, followed by a softmax classifier.",4.2 Baselines and Hyperparameters,[0],[0]
mDA-NN is similar to SCL-NN but uses the hidden representation derived from mDA.,4.2 Baselines and Hyperparameters,[0],[0]
"Sequential is our proposed method without joint learning, which first learns Θ′ based on Da and then learns Θ and γ based on Ds with fixed Θ′. Joint is our proposed joint learning method, that is, we jointly learn Θ and Θ′.
6Because Movie1 and Movie2 come from the same domain, we do not take this pair.
",4.2 Baselines and Hyperparameters,[0],[0]
"For Naive, SCL and mDA, we use LibLinear7 to train linear classifiers and use its default hyperparameters.",4.2 Baselines and Hyperparameters,[0],[0]
"In all the tasks, we use unigrams and bigrams with a frequency of at least 4 as features for classification.",4.2 Baselines and Hyperparameters,[0],[0]
"For the word embeddings, we set the dimension d to 300.",4.2 Baselines and Hyperparameters,[0],[0]
"For CNN, we set the window size to 3.",4.2 Baselines and Hyperparameters,[0],[0]
"Also, the size of the hidden representations z",4.2 Baselines and Hyperparameters,[0],[0]
and z′ is set to 100.,4.2 Baselines and Hyperparameters,[0],[0]
"Following Kim (2014), the non-linear activation function in CNN is Relu, the mini-batch size is 50, the dropout rate α equals 0.5, and the hyperparameter for the l2 norms is set to be 3.",4.2 Baselines and Hyperparameters,[0],[0]
"For Naive, SCL and mDA, we do not use the 200 sentences in the development set for tuning parameters.",4.2 Baselines and Hyperparameters,[0],[0]
"Hence, for fair comparison, we also include settings where the 200 sentences are added to the training set.",4.2 Baselines and Hyperparameters,[0],[0]
We denote these settings by ++.,4.2 Baselines and Hyperparameters,[0],[0]
"In Table 2, we report the results of all the methods.",4.3 Results,[0],[0]
"It is easy to see that the performance of Naive is very limited, and the incorporation of 200 reviews in the development set (Naive++) brings in 4.3% of improvement on average.",4.3 Results,[0],[0]
"SCL++ and mDA++ can further improve the average accuracy respectively
7http://www.csie.ntu.edu.tw/cjlin/ liblinear/
by 0.8% and 1.9%, which verifies the usefulness of these two domain adaptation methods.",4.3 Results,[0],[0]
"However, we can easily see that the performance of these domain adaptation methods based on discrete, bag-of-word representations is even much lower than the nondomain-adaptive method on continuous representations (NaiveNN).",4.3 Results,[0],[0]
"This confirms that it is useful to develop domain adaptation methods based on embedding vectors and neural network models.
",4.3 Results,[0],[0]
"Moreover, we can find that the performance of simply appending two features from auxiliary tasks to NaiveNN (i.e., Aux-NN) is quite close to that of NaiveNN on most data set pairs, which shows that it is not ideal for domain adaptation.",4.3 Results,[0],[0]
"In addition, although the shared hidden representations derived from SCL and mDA are based on traditional bag-of-word representations, SCL-NN and mDANN can still improve the performance of NaiveNN on most data set pairs, which indicates that the derived shared hidden representations by SCL and by mDA can generalize better across domains and are generally useful for domain adaptation.
",4.3 Results,[0],[0]
"Finally, it is easy to see that our method with joint learning outperforms SCL-NN on almost all the data set pairs.",4.3 Results,[0],[0]
"And in comparison with mDANN, our method with joint learning can also outper-
form it on most data set pairs, especially when the size of the labeled data in the source domain is relatively large.",4.3 Results,[0],[0]
"Furthermore, we can easily observe that for our method, joint learning generally works better than sequential learning.",4.3 Results,[0],[0]
"All these observations show the advantage of our joint learning method.
",4.3 Results,[0],[0]
"In Table 3, we also show the comparison between mDA-NN and our model under a setting some labeled target data is used.",4.3 Results,[0],[0]
"Specifically, we randomly select 100 sentences from the development set and mix them with the training set.",4.3 Results,[0],[0]
"We can observe that our method Joint outperforms NaiveNN and mDANN by 1.2% and 0.6%, respectively, which further confirms the effectiveness of our model.",4.3 Results,[0],[0]
"But, in comparison with the setting where no target data is available, the average improvement of our method over NaiveNN is relatively small.
",4.3 Results,[0],[0]
"Hence, to give a deeper analysis, we further show the comparison of Joint and NaiveNN with respect to the number of labeled target data in Figure 2.",4.3 Results,[0],[0]
"Note that for space limitation, we only present the results on MV2→ RT and MV2→ CR.",4.3 Results,[0],[0]
Similar trends have been observed on other data set pairs.,4.3 Results,[0],[0]
"As we can see from Figure 2, the difference between the performance of NaiveNN and that of Joint gradually decreases with the increase of the number of labeled
target data.",4.3 Results,[0],[0]
This indicates that our joint model is much more effective when no or small number of labeled target data is available.,4.3 Results,[0],[0]
"To obtain a better understanding of our method, we conduct a case study where the source is CR and the target is RT.
",4.4 Case Study,[0],[0]
"For each sentiment polarity, we try to extract the most useful trigrams for the final predictions.",4.4 Case Study,[0],[0]
"Recall that our CNN models use a window size of 3, which corresponds to trigrams.",4.4 Case Study,[0],[0]
"By tracing the final prediction scores back through the neural network, we are able to locate the trigrams which have contributed the most through max-pooling.",4.4 Case Study,[0],[0]
"In Table 4, we present the most useful trigrams of each polarity extracted by NaiveNN and by the two components of our sequential and joint method.",4.4 Case Study,[0],[0]
"Sequentialoriginal and Joint-original refer to the CNN corresponding to fΘ while Sequential-auxiliary and Joint-auxiliary refer to the CNN corresponding to fΘ′ , which is related to the auxiliary tasks.
",4.4 Case Study,[0],[0]
"In Table 4, we can easily observe that for NaiveNN, the most important trigrams are domainindependent, which contain some general sentiment words like good, great and disappointing.",4.4 Case Study,[0],[0]
"For our sequential model, the most important trigrams captured by Sequential-original are similar to NaiveNN, but due to the removal of the pivot words in each sentence, the most important trigrams extracted by Sequential-auxiliary are domain-specific, including target-specific sentiment words like oily, friendly and target-specific aspect words like flavor, atmosphere.",4.4 Case Study,[0],[0]
"But since aspect words are irrelevant to our sentiment classification
task, it might bring in some noise and affect the performance of our sequential model.",4.4 Case Study,[0],[0]
"In contrast to Sequential-auxiliary, Joint-auxiliary is jointly learnt with the sentiment classification task, and it is easy to see that most of its extracted trigrams are target-specific sentiment words.",4.4 Case Study,[0],[0]
"Also, for Jointoriginal, since we share the word embeddings of two components and do not remove any pivot, it is intuitive to see that the extracted trigrams contain both domain-independent and domain-specific sentiment words.",4.4 Case Study,[0],[0]
"These observations agree with our motivations behind the model.
",4.4 Case Study,[0],[0]
"Finally, we also sample several sentences from the test dataset, i.e., RT, to get a deeper insight of our joint model.",4.4 Case Study,[0],[0]
Although NaiveNN and Sequential correctly predict sentiments of the following two sentences:,4.4 Case Study,[0],[0]
and now a Lasagna Menu which is to die for!”,"1. “I’ve also been amazed at all the new additions in the past few years: A new Jazz Bar, the most fantastic Dining Garden, the Best Thin Crust Pizzas,",[0],[0]
and lemon and lime juice and mint leaves that is to die for!”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
"Both of them give wrong predictions on another three sentences containing to die for:
3.",2. “The have a great cocktail with Citrus Vodka,[0],[0]
“Try their chef’s specials– they are to die for.”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
4.,2. “The have a great cocktail with Citrus Vodka,[0],[0]
“Their tuna tartar appetizer is to die for.”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
"However, since to die for co-occurs with some
general sentiment words like fantastic, best and great in previous two sentences, our joint model can implicitly learn that to die for is highly correlated with the positive sentiment via our auxiliary tasks, and ultimately make correct predictions for the latter three sentences.",5. “It’s to die for!”.,[0],[0]
"This further indicates that our joint model can identify more domain-specific sentiment words in comparison with NaiveNN and Sequential, and therefore improve the performance.",5. “It’s to die for!”.,[0],[0]
We presented a domain adaptation method for sentiment classification based on sentence embeddings.,5 Conclusions,[0],[0]
"Our method induces a sentence embedding that works well across domains, based on two auxiliary tasks.",5 Conclusions,[0],[0]
We also jointly learn the cross-domain sentence embedding and the sentiment classifier.,5 Conclusions,[0],[0]
Experiment results show that our proposed joint method can outperform several highly competitive domain adaptation methods on 18 source-target pairs using five benchmark data sets.,5 Conclusions,[0],[0]
"Moreover, further analysis confirmed that our method is able to pick up domain-specific sentiment words.",5 Conclusions,[0],[0]
"This research is supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme Office, Media Development Authority (MDA).",Acknowledgment,[0],[0]
"In this paper, we study cross-domain sentiment classification with neural network architectures.",abstractText,[0],[0]
We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification.,abstractText,[0],[0]
We also propose to jointly learn this sentence embedding together with the sentiment classifier itself.,abstractText,[0],[0]
Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets.,abstractText,[0],[0]
Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 236–246, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"With the growing need of correctly identifying the sentiments expressed in subjective texts such as product reviews, sentiment classification has received continuous attention in the NLP community for over a decade (Pang et al., 2002; Pang and Lee, 2004; Hu and Liu, 2004; Choi and Cardie, 2008; Nakagawa et al., 2010).",1 Introduction,[0],[0]
One of the big challenges of sentiment classification is how to adapt a sentiment classifier trained on one domain to a different new domain.,1 Introduction,[0],[0]
This is because sentiments are often expressed with domain-specific words and expressions.,1 Introduction,[0],[0]
"For example, in the Movie domain, words such as moving and engaging are usually positive, but they may not be relevant in the Restaurant domain.",1 Introduction,[0],[0]
"Since labeled data is expensive to obtain, it would be very useful if we could adapt a model trained on a source domain to a target domain.
",1 Introduction,[0],[0]
"Much work has been done in sentiment analysis to address this domain adaptation problem (Blitzer
et al., 2007; Pan et al., 2010; Bollegala et al., 2011; Ponomareva and Thelwall, 2012; Bollegala et al., 2016).",1 Introduction,[0],[0]
"Among them, an appealing method is the Structural Correspondence Learning (SCL) method (Blitzer et al., 2007), which uses pivot feature prediction tasks to induce a projected feature space that works well for both the source and the target domains.",1 Introduction,[0],[0]
The intuition behind is that these pivot prediction tasks are highly correlated with the original task.,1 Introduction,[0],[0]
"For sentiment classification, Blitzer et al. (2007) first chose pivot words which have high mutual information with the sentiment labels, and then set up the pivot prediction tasks to be the predictions of each of these pivot words using the other words.
",1 Introduction,[0],[0]
"However, the original SCL method is based on traditional discrete feature representations and linear classifiers.",1 Introduction,[0],[0]
"In recent years, with the advances of deep learning in NLP, multi-layer neural network models such as RNNs and CNNs have been widely used in sentiment classification and achieved good performance (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015).",1 Introduction,[0],[0]
"In these models, dense, real-valued feature vectors and non-linear classification functions are used.",1 Introduction,[0],[0]
"By using real-valued word embeddings pre-trained from a large corpus, these models can take advantage of the embedding space that presumably better captures the syntactic and semantic similarities between words.",1 Introduction,[0],[0]
"And by using non-linear functions through multi-layer neural networks, these models represent a more expressive hypothesis space.",1 Introduction,[0],[0]
"Therefore, it would be interesting to explore how these neural network models could be extended for cross-domain sentiment classification.
236
There has been some recent studies on neural network-based domain adaptation (Glorot et al., 2011; Chen et al., 2012; Yang and Eisenstein, 2014).",1 Introduction,[0],[0]
They use Stacked Denoising Auto-encoders (SDA) to induce a hidden representation that presumably works well across domains.,1 Introduction,[0],[0]
"However, SDA is fully unsupervised and does not consider the end task we need to solve, i.e., the sentiment classification task.",1 Introduction,[0],[0]
"In contrast, the idea behind SCL is to use carefullychosen auxiliary tasks that correlate with the end task to induce a hidden representation.",1 Introduction,[0],[0]
"Another line of work aims to learn a low dimensional representation for each feature in both domains based on predicting its neighboring features (Yang and Eisenstein, 2015; Bollegala et al., 2015).",1 Introduction,[0],[0]
"Different from these methods, we aim to directly learn sentence embeddings that work well across domains.
",1 Introduction,[0],[0]
"In this paper, we aim to extend the main idea behind SCL to neural network-based solutions to sentiment classification to address the domain adaptation problem.",1 Introduction,[0],[0]
"Specifically, we borrow the idea of using pivot prediction tasks from SCL.",1 Introduction,[0],[0]
"But instead of learning thousands of pivot predictors and performing singular value decomposition on the learned weights, which all relies on linear transformations, we introduce only two auxiliary binary prediction tasks and directly learn a non-linear transformation that maps an input to a dense embedding vector.",1 Introduction,[0],[0]
"Moreover, different from SCL and the auto-encoderbased methods, in which the hidden feature representation and the final classifier are learned sequentially, we propose to jointly learn the hidden feature representation together with the sentiment classification model itself, and we show that joint learning works better than sequential learning.
",1 Introduction,[0],[0]
We conduct experiments on a number of different source and target domains for sentence-level sentiment classification.,1 Introduction,[0],[0]
We show that our proposed method is able to achieve the best performance compared with a number of baselines for most of these domain pairs.,1 Introduction,[0],[0]
"Domain Adaptation: Domain adaptation is a general problem in NLP and has been well studied in recent years (Blitzer et al., 2006; Daumé III, 2007; Jiang and Zhai, 2007; Dredze and Crammer, 2008; Titov, 2011; Yu and Jiang, 2015).",2 Related Work,[0],[0]
"For sentiment classification, most existing domain adaptation methods are based on traditional discrete feature representations and linear classifiers.",2 Related Work,[0],[0]
"One line of work focuses on inducing a general lowdimensional cross-domain representation based on the co-occurrences of domain-specific and domainindependent features (Blitzer et al., 2007; Pan et al., 2010; Pan et al., 2011).",2 Related Work,[0],[0]
"Another line of work tries to derive domain-specific sentiment words (Bollegala et al., 2011; Li et al., 2012).",2 Related Work,[0],[0]
"Our proposed method is similar to the first line of work in that we also aim to learn a general, cross-domain representation (sentence embeddings in our case).",2 Related Work,[0],[0]
"Neural Networks for Sentiment Classification: A recent trend of deep learning enhances various kinds of neural network models for sentiment classification, including Convolutional Neural Networks (CNNs), Recursive Neural Network (ReNNs) and Recurrent Neural Network (RNNs), which have been shown to achieve competitive results across different benchmarks (Socher et al., 2013; Dong et al., 2014a; Dong et al., 2014b; Kim, 2014; Tang et al., 2015).",2 Related Work,[0],[0]
"Inspired by their success in standard indomain settings, it is intuitive for us to apply these neural network models to domain adaptation settings.",2 Related Work,[0],[0]
"Denoising Auto-encoders for Domain Adaptation: Denoising Auto-encoders have been extensively studied in cross-domain sentiment classification, since the representations learned through multilayer neural networks are robust against noise during domain adaptation.",2 Related Work,[0],[0]
"The initial application of this idea is to directly employ stacked denoising autoencoders (SDA) by reconstructing the original features from data that are corrupted with noise (Glorot et al., 2011), and Chen et al. (2012) proposed to analytically marginalize out the corruption during SDA training.",2 Related Work,[0],[0]
Later Yang and Eisenstein (2014) further showed that their proposed structured dropout noise strategy can dramatically improve the efficiency without sacrificing the accuracy.,2 Related Work,[0],[0]
"However, these methods are still based on traditional discrete representation and do not exploit the idea of using auxiliary tasks that are related to the end task.",2 Related Work,[0],[0]
"In contrast, the sentence embeddings learned from our method are derived from real-valued feature vectors and rely on related auxiliary tasks.",2 Related Work,[0],[0]
In this section we present our sentence embeddingbased domain adaptation method for sentiment classification.,3 Method,[0],[0]
We first introduce the necessary notation and an overview of our method.,3 Method,[0],[0]
we then delve into the details of the method.,3 Method,[0],[0]
We assume that each input is a piece of text consisting of a sequence of words.,3.1 Notation and Method Overview,[0],[0]
"For the rest of this paper, we assume each input is a sentence, although our method is general enough for longer pieces of text.",3.1 Notation and Method Overview,[0],[0]
"Let x = (x1, x2, . . .)",3.1 Notation and Method Overview,[0],[0]
"denote a sentence where each xi ∈ {1, 2, . . .",3.1 Notation and Method Overview,[0],[0]
", V } is a word in the vocabulary and V is the vocabulary size.",3.1 Notation and Method Overview,[0],[0]
"Let the sentiment label ofx be y ∈ {+,−}where + denotes a positive sentiment and − a negative sentiment.",3.1 Notation and Method Overview,[0],[0]
"We further assume that we are given a set of labeled training sentences from a source domain, denoted by Ds = {(xsi , ysi )}N s
i=1.",3.1 Notation and Method Overview,[0],[0]
"Also, we have a set of unlabeled sentences from a target domain, denoted by Dt = {xti}N t
i=1.",3.1 Notation and Method Overview,[0],[0]
"Our goal is to learn a good sentiment classifier from both Ds and Dt such that the classifier works well on the target domain.
",3.1 Notation and Method Overview,[0],[0]
"A baseline solution without considering any domain difference is to simply train a classifier using Ds, and with the recent advances in neural networkbased methods to sentence classification, we consider a baseline that uses a multi-layer neural network such as a CNN or an RNN to perform the classification task.",3.1 Notation and Method Overview,[0],[0]
"To simplify the discussion and focus on the domain adaptation ideas we propose, we will leave the details of the neural network model we use in Section 3.5.",3.1 Notation and Method Overview,[0],[0]
"For now, we assume that a multilayer neural network is used to transform each input x into a sentence embedding vector z. Let us use fΘ to denote the transformation function parameterized by Θ, that is, z = fΘ(x).",3.1 Notation and Method Overview,[0],[0]
"Next, we assume that a linear classifier such as a softmax classifier is learned to map z to a sentiment label y.
We introduce two auxiliary tasks which presumably are highly correlated with the sentiment classification task itself.",3.1 Notation and Method Overview,[0],[0]
Labels for these auxiliary tasks can be automatically derived from unlabeled data in both the source and the target domains.,3.1 Notation and Method Overview,[0],[0]
"With the help of the two auxiliary tasks, we learn a non-linear transformation function fΘ′ from unlabeled data and
use it to derive a sentence embedding vector z′ from sentence x, which supposedly works better across domains.",3.1 Notation and Method Overview,[0],[0]
Finally we use the source domain’s training data to learn a linear classifier on the representation z ⊕,3.1 Notation and Method Overview,[0],[0]
"z′, where ⊕ is the operator that concatenates two vectors.",3.1 Notation and Method Overview,[0],[0]
Figure 1 gives the outline of our method.,3.1 Notation and Method Overview,[0],[0]
Our two auxiliary tasks are about whether an input sentence contains a positive or negative domainindependent sentiment word.,3.2 Auxiliary Tasks,[0],[0]
The intuition is the following.,3.2 Auxiliary Tasks,[0],[0]
"If we have a list of domain-independent positive sentiment words, then an input sentence that contains one of these words, regardless of the domain the sentence is from, is more likely to contain an overall positive sentiment.",3.2 Auxiliary Tasks,[0],[0]
"For example, a sentence containing the word good is likely to be overall positive.",3.2 Auxiliary Tasks,[0],[0]
"Moreover, the rest of the sentence excluding the word good may contain domain-specific words or expressions that also convey a positive sentiment.",3.2 Auxiliary Tasks,[0],[0]
"For example, in the sentence “The laptop is good and goes really fast,” we can see that the word fast is a domain-specific sentiment word, and its sentiment polarity correlates with that of the word good, which is domain-independent.",3.2 Auxiliary Tasks,[0],[0]
"Therefore, we can hide the domain-independent positive words in a sentence and try to use the other words in the sentence to predict whether the original sentence contains a domain-independent positive word.",3.2 Auxiliary Tasks,[0],[0]
There are two things to note about this auxiliary task: (1) The label of the task can be automatically derived provided that we have the domain-independent positive word list.,3.2 Auxiliary Tasks,[0],[0]
(2) The task is closely related to the original task of sentence-level sentiment classification.,3.2 Auxiliary Tasks,[0],[0]
"Similarly, we can introduce a task to predict the existence of a domain-independent negative sentiment word in a sentence.
",3.2 Auxiliary Tasks,[0],[0]
"Formally, let us assume that we have two domainindependent sentiment word lists, one for the positive sentiment and the other for the negative sentiment.",3.2 Auxiliary Tasks,[0],[0]
Details of how these lists are obtained will be given in Section 3.5.,3.2 Auxiliary Tasks,[0],[0]
"Borrowing the term from SCL, we refer to these sentiment words as pivot words.",3.2 Auxiliary Tasks,[0],[0]
"For each sentence x, we replace all the occurrences of these pivot words with a special token UNK.",3.2 Auxiliary Tasks,[0],[0]
"Let g(·) be a function that denotes this procedure, that is, g(x) is the resulting sentence with
31
UNK tokens.",3.2 Auxiliary Tasks,[0],[0]
We then introduce two binary labels for g(x).,3.2 Auxiliary Tasks,[0],[0]
"The first label u indicates whether the original sentence x contains at least one domainindependent positive sentiment word, and the second label v indicates whether x contains at least one domain-independent negative sentiment word.",3.2 Auxiliary Tasks,[0],[0]
"Figure 1 shows an example sentence x, its modified version g(x) and the labels u and v for x.",3.2 Auxiliary Tasks,[0],[0]
"We further use Da = {(xi, ui, vi)}Nai=1 to denote a set of training sentences for the auxiliary tasks.",3.2 Auxiliary Tasks,[0],[0]
"Note that the sentences in Da can be from the sentences in Ds and Dt, but they can also be from other unlabeled sentences.",3.2 Auxiliary Tasks,[0],[0]
"With the two auxiliary tasks, we can learn a neural network model in a standard way to produce sentence embeddings that work well for the auxiliary tasks.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Specifically, we still use Θ′ to denote the parameters of the neural network that produces the sentence embeddings (and fΘ′ the corresponding transformation function), and we use β+ and β− to denote the parameters of two softmax classifiers for the two auxiliary tasks, respectively.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Using crossentropy loss, we can learn Θ′ by minimizing the following loss function:
J(Θ′,β+,β−)
=",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"− ∑
(x,u,v)∈Da
( log p(u|fΘ′(g(x));β+)
+ log p(v|fΘ′(g(x));β−) ) ,
where p(y|z;β) is the probability of label y given vector z and parameter β under softmax regression.
",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"With the learned Θ′, we can derive a sentence embedding z′ from any sentence.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Although we could simply use this embedding z′ for sentiment classification through another softmax classifier, this may not be ideal because z′ is transformed from g(x), which has the domain-independent sentiment words removed.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Similar to SCL and some other previous work, we concatenate the embedding vector z′ with the standard embedding vector z for the final classification.",3.3 Sentence Embeddings for Domain Adaptation,[0],[0]
"Although we can learn Θ′ using Da as a first step, here we also explore a joint learning setting.",3.4 Joint Learning,[0],[0]
"In this setting, Θ′ is learned together with the neural network model used for the end task, i.e., sentiment classification.",3.4 Joint Learning,[0],[0]
"This way, the learning of Θ′ depends not only on Da but also on Ds, i.e., the sentimentlabeled training data from the source domain.
",3.4 Joint Learning,[0],[0]
"Specifically, we use Θ to denote the parameters for a neural network that takes the original sentence x and transforms it to a sentence embedding (and fΘ the corresponding transformation function).",3.4 Joint Learning,[0],[0]
We use γ to denote the parameters of a softmax classifier that operates on the concatenated sentence embedding z ⊕ z′ for sentiment classification.,3.4 Joint Learning,[0],[0]
"With joint learning, we try to minimize the following loss func-
tion:
J(Θ,Θ′,γ,β+,β−)
=",3.4 Joint Learning,[0],[0]
"− ∑
(x,y)∈Ds
( log p(y|fΘ(x)⊕ fΘ′(g(x));γ) )
",3.4 Joint Learning,[0],[0]
"− ∑
(x,u,v)∈Da
( log p(u|fΘ′(g(x));β+)
+ log p(v|fΘ′(g(x));β−) ) .
",3.4 Joint Learning,[0],[0]
We can see that this loss function contains two parts.,3.4 Joint Learning,[0],[0]
The first part is the cross-entropy loss based on the true sentiment labels of the sentences in Ds.,3.4 Joint Learning,[0],[0]
"The second part is the loss based on the auxiliary tasks and the data Da, which are derived from unlabeled sentences.
",3.4 Joint Learning,[0],[0]
"Finally, to make a prediction on a sentence, we use the learned Θ and Θ′ to derive a sentence embedding fΘ(x) ⊕ fΘ′(g(x)), and then use the softmax classifier parameterized by the learned γ to make the final prediction.",3.4 Joint Learning,[0],[0]
"In this section we explain some of the model details.
",3.5 Implementation Details,[0],[0]
"Pivot Word Selection Recall that the two auxiliary tasks depend on two domain-independent sentiment word lists, i.e., pivot word lists.",3.5 Implementation Details,[0],[0]
"Different from Blitzer et al. (2007), we employ weighted log-likelihood ratio (WLLR) to select the most positive and negative words in both domains as pivots.",3.5 Implementation Details,[0],[0]
The reason is that in our preliminary experiments we observe that mutual information (used by Blitzer et al. (2007)) is biased towards low frequency words.,3.5 Implementation Details,[0],[0]
Some high frequency words including good and great are scored low.,3.5 Implementation Details,[0],[0]
"In comparison, WLLR does not have this issue.",3.5 Implementation Details,[0],[0]
"The same observation was also reported previously by Li et al. (2009).
",3.5 Implementation Details,[0],[0]
"More specifically, we first tokenize the sentences in Ds and Dt and perform part-of-speech tagging using the NLTK toolkit.",3.5 Implementation Details,[0],[0]
"Next, we extract only adjectives, adverbs and verbs with a frequency of at least 3 in the source domain and at least 3 in the target domain.",3.5 Implementation Details,[0],[0]
We also remove negation words such as not and stop words using a stop word list.,3.5 Implementation Details,[0],[0]
"We then measure each remaining candidate word’s relevance to the positive and the negative classes based on Ds
by computing the following scores:
r(w, y) = p̃(w|y) log p̃(w|y) p̃(w|ȳ) ,
where w is a word, y ∈ {+,−} is a sentiment label, ȳ is the opposite label of y, and p̃(w|y) is the empirical probability of observing w in sentences labeled with y.",3.5 Implementation Details,[0],[0]
"We can then rank the candidate words in decreasing order of r(w,+) and r(w,−).",3.5 Implementation Details,[0],[0]
"Finally, we select the top 25% from each ranked list as the final lists of pivot words for the positive and the negative sentiments.",3.5 Implementation Details,[0],[0]
"Some manual inspection shows that most of these words are indeed domain-independent sentiment words.
",3.5 Implementation Details,[0],[0]
Neural Network Model,3.5 Implementation Details,[0],[0]
Our framework is general and potentially we can use any neural network model to transform an input sentence to a sentence embedding vector.,3.5 Implementation Details,[0],[0]
"In this paper, we adopt a CNN-based approach because it has been shown to work well for sentiment classification.",3.5 Implementation Details,[0],[0]
"Specifically, each word (including the token UNK) is represented by a word embedding vector.",3.5 Implementation Details,[0],[0]
"Let W ∈ Rd×V denote the lookup table for words, where each column is a d-dimensional embedding vector for a word type.",3.5 Implementation Details,[0],[0]
"Two separate CNNs are used to process x and g(x), and their mechanisms are the same.",3.5 Implementation Details,[0],[0]
"For a word xi in each CNN, the embedding vectors inside a window of size n centered at i are concatenated into a new vector, which we refer to as ei ∈ Rnd.",3.5 Implementation Details,[0],[0]
"A convolution operation is then performed by applying a filter F ∈ Rh×nd on ei to produce a hidden vector hi = m(Fei + b), where b ∈ Rh is a bias vector and m is an elementwise non-linear transformation function.",3.5 Implementation Details,[0],[0]
Note that we pad the original sequence in front and at the back to ensure that at each position i we have n vectors to be combined into hi.,3.5 Implementation Details,[0],[0]
"After the convolution operation is applied to the whole sequence, we obtain H =",3.5 Implementation Details,[0],[0]
"[h1,h2, . . .",3.5 Implementation Details,[0],[0]
"], and we apply a max-over-time pooling operator to take the maximum value of each row of H to obtain an overall hidden vector, i.e., z for x and z′ for g(x).
",3.5 Implementation Details,[0],[0]
It is worth noting that the two neural networks corresponding to fΘ and fΘ′ share the same word embedding lookup table.,3.5 Implementation Details,[0],[0]
"This lookup table is initialized with word embeddings from word2vec1 and
1https://code.google.com/p/word2vec/
is updated during our learning process.",3.5 Implementation Details,[0],[0]
Note that the token UNK is initialized as a zero vector and never updated.,3.5 Implementation Details,[0],[0]
"Although our method is inspired by SCL, there are a number of major differences: (1) Our method is based on neural network models with continuous, dense feature representations and non-linear transformation functions.",3.6 Differences from SCL,[0],[0]
"SCL is based on discrete, sparse feature vectors and linear transformations.",3.6 Differences from SCL,[0],[0]
"(2) Although our pivot word selection is similar to that of SCL, in the end we only use two auxiliary tasks while SCL uses much more pivot prediction tasks.",3.6 Differences from SCL,[0],[0]
"(3) We can directly learn the transformation function f ′Θ that produces the hidden representation, while SCL relies on SVD to learn the projection function.",3.6 Differences from SCL,[0],[0]
"(4) We perform joint learning of the auxiliary tasks and the end task, i.e., sentiment classification, while SCL performs the learning in a sequential manner.",3.6 Differences from SCL,[0],[0]
"To evaluate our proposed method, we conduct experiments using five benchmark data sets.",4.1 Data Sets and Experiment Settings,[0],[0]
The data sets are summarized in Table 1.,4.1 Data Sets and Experiment Settings,[0],[0]
"Movie12 and Movie23 are movie reviews labeled by Pang and Lee (2005) and Socher et al. (2013), respectively.",4.1 Data Sets and Experiment Settings,[0],[0]
"Camera4 are reviews of digital products such as MP3 players and cameras (Hu and Liu, 2004).",4.1 Data Sets and Experiment Settings,[0],[0]
"Laptop and Restaurant5 are laptop and restaurant reviews taken
2https://www.cs.cornell.edu/people/pabo/ movie-review-data/
3http://nlp.stanford.edu/sentiment/ 4http://www.cs.uic.edu/˜liub/FBS/
sentiment-analysis.html 5Note that the original data set is for aspect-level sentiment analysis.",4.1 Data Sets and Experiment Settings,[0],[0]
"We remove sentences with opposite polarities towards different aspects, and use the consistent polarity as the sentencelevel sentiment of each remaining sentence.
from SemEval 2015 Task 12.",4.1 Data Sets and Experiment Settings,[0],[0]
We consider 18 pairs of data sets where the two data sets come from different domains.6,4.1 Data Sets and Experiment Settings,[0],[0]
"For neural network-based methods, we randomly pick 200 sentences from the target domain as the development set for parameter tuning, and the rest of the data from the target domain as the test data.",4.1 Data Sets and Experiment Settings,[0],[0]
We consider the following baselines: Naive is a non-domain-adaptive baseline based on bag-of-word representations.,4.2 Baselines and Hyperparameters,[0],[0]
SCL is our implementation of the Structural Correspondence Learning method.,4.2 Baselines and Hyperparameters,[0],[0]
"We set the number of induced features K to 100 and rescale factor α = 5, and we use 1000 pivot words based on our preliminary experiments.",4.2 Baselines and Hyperparameters,[0],[0]
"mDA is our implementation of marginalized Denoising Auto-encoders (Chen et al., 2012), one of the state-of-the-art domain adaptation methods, which learns a shared hidden representation by reconstructing pivot features from corrupted inputs.",4.2 Baselines and Hyperparameters,[0],[0]
"Following Yang and Eisenstein (2014), we employ the efficient and effective structured dropout noise strategy without any parameter.",4.2 Baselines and Hyperparameters,[0],[0]
The top 500 features are chosen as pivots based on our preliminary experiments.,4.2 Baselines and Hyperparameters,[0],[0]
"NaiveNN is a non-domain-adaptive baseline based on CNN, as described in Section 3.5.",4.2 Baselines and Hyperparameters,[0],[0]
"Aux-NN is a simple combination of our auxiliary tasks with NaiveNN, which treats the derived label of two auxiliary tasks as two features and then appends them to the hidden representation learned from CNN, followed by a softmax classifier.",4.2 Baselines and Hyperparameters,[0],[0]
"SCL-NN is a naive combination of SCL with NaiveNN, which appends the induced representation from SCL to the hidden representation learned from CNN, followed by a softmax classifier.",4.2 Baselines and Hyperparameters,[0],[0]
mDA-NN is similar to SCL-NN but uses the hidden representation derived from mDA.,4.2 Baselines and Hyperparameters,[0],[0]
"Sequential is our proposed method without joint learning, which first learns Θ′ based on Da and then learns Θ and γ based on Ds with fixed Θ′. Joint is our proposed joint learning method, that is, we jointly learn Θ and Θ′.
6Because Movie1 and Movie2 come from the same domain, we do not take this pair.
",4.2 Baselines and Hyperparameters,[0],[0]
"For Naive, SCL and mDA, we use LibLinear7 to train linear classifiers and use its default hyperparameters.",4.2 Baselines and Hyperparameters,[0],[0]
"In all the tasks, we use unigrams and bigrams with a frequency of at least 4 as features for classification.",4.2 Baselines and Hyperparameters,[0],[0]
"For the word embeddings, we set the dimension d to 300.",4.2 Baselines and Hyperparameters,[0],[0]
"For CNN, we set the window size to 3.",4.2 Baselines and Hyperparameters,[0],[0]
"Also, the size of the hidden representations z",4.2 Baselines and Hyperparameters,[0],[0]
and z′ is set to 100.,4.2 Baselines and Hyperparameters,[0],[0]
"Following Kim (2014), the non-linear activation function in CNN is Relu, the mini-batch size is 50, the dropout rate α equals 0.5, and the hyperparameter for the l2 norms is set to be 3.",4.2 Baselines and Hyperparameters,[0],[0]
"For Naive, SCL and mDA, we do not use the 200 sentences in the development set for tuning parameters.",4.2 Baselines and Hyperparameters,[0],[0]
"Hence, for fair comparison, we also include settings where the 200 sentences are added to the training set.",4.2 Baselines and Hyperparameters,[0],[0]
We denote these settings by ++.,4.2 Baselines and Hyperparameters,[0],[0]
"In Table 2, we report the results of all the methods.",4.3 Results,[0],[0]
"It is easy to see that the performance of Naive is very limited, and the incorporation of 200 reviews in the development set (Naive++) brings in 4.3% of improvement on average.",4.3 Results,[0],[0]
"SCL++ and mDA++ can further improve the average accuracy respectively
7http://www.csie.ntu.edu.tw/cjlin/ liblinear/
by 0.8% and 1.9%, which verifies the usefulness of these two domain adaptation methods.",4.3 Results,[0],[0]
"However, we can easily see that the performance of these domain adaptation methods based on discrete, bag-of-word representations is even much lower than the nondomain-adaptive method on continuous representations (NaiveNN).",4.3 Results,[0],[0]
"This confirms that it is useful to develop domain adaptation methods based on embedding vectors and neural network models.
",4.3 Results,[0],[0]
"Moreover, we can find that the performance of simply appending two features from auxiliary tasks to NaiveNN (i.e., Aux-NN) is quite close to that of NaiveNN on most data set pairs, which shows that it is not ideal for domain adaptation.",4.3 Results,[0],[0]
"In addition, although the shared hidden representations derived from SCL and mDA are based on traditional bag-of-word representations, SCL-NN and mDANN can still improve the performance of NaiveNN on most data set pairs, which indicates that the derived shared hidden representations by SCL and by mDA can generalize better across domains and are generally useful for domain adaptation.
",4.3 Results,[0],[0]
"Finally, it is easy to see that our method with joint learning outperforms SCL-NN on almost all the data set pairs.",4.3 Results,[0],[0]
"And in comparison with mDANN, our method with joint learning can also outper-
form it on most data set pairs, especially when the size of the labeled data in the source domain is relatively large.",4.3 Results,[0],[0]
"Furthermore, we can easily observe that for our method, joint learning generally works better than sequential learning.",4.3 Results,[0],[0]
"All these observations show the advantage of our joint learning method.
",4.3 Results,[0],[0]
"In Table 3, we also show the comparison between mDA-NN and our model under a setting some labeled target data is used.",4.3 Results,[0],[0]
"Specifically, we randomly select 100 sentences from the development set and mix them with the training set.",4.3 Results,[0],[0]
"We can observe that our method Joint outperforms NaiveNN and mDANN by 1.2% and 0.6%, respectively, which further confirms the effectiveness of our model.",4.3 Results,[0],[0]
"But, in comparison with the setting where no target data is available, the average improvement of our method over NaiveNN is relatively small.
",4.3 Results,[0],[0]
"Hence, to give a deeper analysis, we further show the comparison of Joint and NaiveNN with respect to the number of labeled target data in Figure 2.",4.3 Results,[0],[0]
"Note that for space limitation, we only present the results on MV2→ RT and MV2→ CR.",4.3 Results,[0],[0]
Similar trends have been observed on other data set pairs.,4.3 Results,[0],[0]
"As we can see from Figure 2, the difference between the performance of NaiveNN and that of Joint gradually decreases with the increase of the number of labeled
target data.",4.3 Results,[0],[0]
This indicates that our joint model is much more effective when no or small number of labeled target data is available.,4.3 Results,[0],[0]
"To obtain a better understanding of our method, we conduct a case study where the source is CR and the target is RT.
",4.4 Case Study,[0],[0]
"For each sentiment polarity, we try to extract the most useful trigrams for the final predictions.",4.4 Case Study,[0],[0]
"Recall that our CNN models use a window size of 3, which corresponds to trigrams.",4.4 Case Study,[0],[0]
"By tracing the final prediction scores back through the neural network, we are able to locate the trigrams which have contributed the most through max-pooling.",4.4 Case Study,[0],[0]
"In Table 4, we present the most useful trigrams of each polarity extracted by NaiveNN and by the two components of our sequential and joint method.",4.4 Case Study,[0],[0]
"Sequentialoriginal and Joint-original refer to the CNN corresponding to fΘ while Sequential-auxiliary and Joint-auxiliary refer to the CNN corresponding to fΘ′ , which is related to the auxiliary tasks.
",4.4 Case Study,[0],[0]
"In Table 4, we can easily observe that for NaiveNN, the most important trigrams are domainindependent, which contain some general sentiment words like good, great and disappointing.",4.4 Case Study,[0],[0]
"For our sequential model, the most important trigrams captured by Sequential-original are similar to NaiveNN, but due to the removal of the pivot words in each sentence, the most important trigrams extracted by Sequential-auxiliary are domain-specific, including target-specific sentiment words like oily, friendly and target-specific aspect words like flavor, atmosphere.",4.4 Case Study,[0],[0]
"But since aspect words are irrelevant to our sentiment classification
task, it might bring in some noise and affect the performance of our sequential model.",4.4 Case Study,[0],[0]
"In contrast to Sequential-auxiliary, Joint-auxiliary is jointly learnt with the sentiment classification task, and it is easy to see that most of its extracted trigrams are target-specific sentiment words.",4.4 Case Study,[0],[0]
"Also, for Jointoriginal, since we share the word embeddings of two components and do not remove any pivot, it is intuitive to see that the extracted trigrams contain both domain-independent and domain-specific sentiment words.",4.4 Case Study,[0],[0]
"These observations agree with our motivations behind the model.
",4.4 Case Study,[0],[0]
"Finally, we also sample several sentences from the test dataset, i.e., RT, to get a deeper insight of our joint model.",4.4 Case Study,[0],[0]
Although NaiveNN and Sequential correctly predict sentiments of the following two sentences:,4.4 Case Study,[0],[0]
and now a Lasagna Menu which is to die for!”,"1. “I’ve also been amazed at all the new additions in the past few years: A new Jazz Bar, the most fantastic Dining Garden, the Best Thin Crust Pizzas,",[0],[0]
and lemon and lime juice and mint leaves that is to die for!”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
"Both of them give wrong predictions on another three sentences containing to die for:
3.",2. “The have a great cocktail with Citrus Vodka,[0],[0]
“Try their chef’s specials– they are to die for.”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
4.,2. “The have a great cocktail with Citrus Vodka,[0],[0]
“Their tuna tartar appetizer is to die for.”,2. “The have a great cocktail with Citrus Vodka,[0],[0]
"However, since to die for co-occurs with some
general sentiment words like fantastic, best and great in previous two sentences, our joint model can implicitly learn that to die for is highly correlated with the positive sentiment via our auxiliary tasks, and ultimately make correct predictions for the latter three sentences.",5. “It’s to die for!”.,[0],[0]
"This further indicates that our joint model can identify more domain-specific sentiment words in comparison with NaiveNN and Sequential, and therefore improve the performance.",5. “It’s to die for!”.,[0],[0]
We presented a domain adaptation method for sentiment classification based on sentence embeddings.,5 Conclusions,[0],[0]
"Our method induces a sentence embedding that works well across domains, based on two auxiliary tasks.",5 Conclusions,[0],[0]
We also jointly learn the cross-domain sentence embedding and the sentiment classifier.,5 Conclusions,[0],[0]
Experiment results show that our proposed joint method can outperform several highly competitive domain adaptation methods on 18 source-target pairs using five benchmark data sets.,5 Conclusions,[0],[0]
"Moreover, further analysis confirmed that our method is able to pick up domain-specific sentiment words.",5 Conclusions,[0],[0]
"This research is supported by the Singapore National Research Foundation under its International Research Centre@Singapore Funding Initiative and administered by the IDM Programme Office, Media Development Authority (MDA).",Acknowledgment,[0],[0]
"In this paper, we study cross-domain sentiment classification with neural network architectures.",abstractText,[0],[0]
We borrow the idea from Structural Correspondence Learning and use two auxiliary tasks to help induce a sentence embedding that supposedly works well across domains for sentiment classification.,abstractText,[0],[0]
We also propose to jointly learn this sentence embedding together with the sentiment classifier itself.,abstractText,[0],[0]
Experiment results demonstrate that our proposed joint model outperforms several state-of-theart methods on five benchmark datasets.,abstractText,[0],[0]
Learning Sentence Embeddings with Auxiliary Tasks for Cross-Domain Sentiment Classification,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4816–4821 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4816",text,[0],[0]
"Knowledge graphs (KGs) are used to organize, manage, and retrieve structured information.",1 Introduction,[0],[0]
The incompleteness of most real-world KGs has stimulated research on predicting missing relations between entities.,1 Introduction,[0],[0]
"A KG is of the form G = (E , R), where E is a set of entities and, R is a set of relation types or predicates.",1 Introduction,[0],[0]
"One can represent G as a set of triples of the form (subject, predicate, object), denoted as (s, p, o).",1 Introduction,[0],[0]
"The link prediction problem seeks the most probable completion of a triple (s, p, ?) or (?, p, o) (Nickel et al., 2016).",1 Introduction,[0],[0]
We focus on temporal KGs where some triples are augmented with time information and the link prediction problem asks for the most probable completion given time information.,1 Introduction,[0],[0]
"More formally, a temporal KG G = (E , R, T ) is a KG where
∗Work done while interning at NEC Labs Europe 1https://github.com/nle-ml/mmkb
facts can also have the form (subject, predicate, object, timestamp) or (subject, predicate, object, time predicate, timestamp), in addition to (s, p, o) triples.",1 Introduction,[0],[0]
"For instance, facts such as (Barack Obama, born, US, 1961) or (Barack Obama, president, US, occursSince, 2009-01) express temporal information about the facts associated with Barack Obama.",1 Introduction,[0],[0]
"While the former expresses that a relation type occurred at a specific point in time, the latter expresses an (open) time interval using the time predicate “occursSince.”",1 Introduction,[0],[0]
"The latter example also illustrates a common challenge posed by the heterogeneity of time expressions due to variations in language and serialization standards.
",1 Introduction,[0],[0]
"Most approaches to link prediction are characterized by a scoring function that operates on the entity and relation type embeddings of a triple (Bordes et al., 2013; Yang et al., 2014; Guu et al., 2015).",1 Introduction,[0],[0]
Learning representations that carry temporal information is challenging due to the sparsity and irregularities of time expressions.,1 Introduction,[0],[0]
"It is possible, however, to turn time expressions into sequences of tokens expressing said temporal information.",1 Introduction,[0],[0]
"Moreover, character-level architectures for language modeling (Zhang et al., 2015; Kim et al., 2016) operate on characters as atomic units to derive word embeddings.",1 Introduction,[0],[0]
"Inspired by these models, we propose a method to incorporate time information into standard embedding approaches for link prediction.",1 Introduction,[0],[0]
"We learn time-aware representations by training a recursive neural network with sequences of tokens representing the time predicate and the digits of the timestamp, if they exist.",1 Introduction,[0],[0]
The last hidden state of the recurrent network is combined with standard scoring functions from the KG completion literature.,1 Introduction,[0],[0]
"Reasoning with temporal information in knowledge bases has a long history and has resulted in numerous temporal logics (van Benthem, 1995).",2 Related Work,[0],[0]
"Several recent approaches extend statistical relational learning frameworks with temporal reasoning capabilities (Chekol et al., 2017; Chekol and Stuckenschmidt, 2018; Dylla et al., 2013).
",2 Related Work,[0],[0]
There is also prior work on incorporating temporal information in knowledge graph completion methods.,2 Related Work,[0],[0]
Jiang et al. (2016) capture the temporal ordering that exists between some relation types as well as additional common-sense constraints to generate more accurate link predictions.,2 Related Work,[0],[0]
Esteban et al. (2016) introduce a prediction model for link prediction that assumes that changes to a KG are introduced by incoming events.,2 Related Work,[0],[0]
These events are modeled as a separate event graph and used to predict the existence of links in the future.,2 Related Work,[0],[0]
Trivedi et al. (2017) model the occurrence of a fact as a point process whose intensity function is influenced by the score assigned to the fact by an embedding function.,2 Related Work,[0],[0]
Leblay and Chekol (2018) develop scoring functions that incorporate time representations into a TransE-type scoring function.,2 Related Work,[0],[0]
"Prior work has also incorporated numerical but non-temporal entity information for knowledge base completion (Garcia-Duran and Niepert, 2017).
",2 Related Work,[0],[0]
"Contrary to all previous approaches, we encode sequences of temporal tokens with an RNN.",2 Related Work,[0],[0]
"This facilitates the encoding of relation types with temporal tokens such as “since,” ”until,” and the digits of timestamps.",2 Related Work,[0],[0]
"Moreover, the RNN encoding provides an inductive bias for parameter sharing among similar timestamps (e.g., those occurring in the same century).",2 Related Work,[0],[0]
"Finally, our method can be combined with all existing scoring functions.",2 Related Work,[0],[0]
"Embedding approaches for KG completion learn a scoring function f that operates on the embeddings of the subject es, the object eo, and the predicate ep of the triples.",3 Time-Aware Representations,[0],[0]
"The value of this scoring function on a triple (s, p, o), f(s, p, o), is learned to be proportional to the likelihood of the triples being true.",3 Time-Aware Representations,[0],[0]
"Popular examples of scoring functions are
• TRANSE (Bordes et al., 2013)
",3 Time-Aware Representations,[0],[0]
"f(s, p, o)",3 Time-Aware Representations,[0],[0]
= ||es + ep − eo||2.,3 Time-Aware Representations,[0],[0]
"(1)
• DISTMULT (Yang et al., 2014):
f(s, p, o) =",3 Time-Aware Representations,[0],[0]
"(es ◦ eo)eTp , (2)
where es, eo ∈ Rd are the embeddings of the subject and object entities, ep ∈ Rd is the embedding of the relation type predicate, and ◦ is the elementwise product.",3 Time-Aware Representations,[0],[0]
"These scoring functions do not take temporal information into account.
",3 Time-Aware Representations,[0],[0]
"Given a temporal KG where some triples are augmented with temporal information, we can decompose a given (possibly incomplete) timestamp into a sequence consisting of some of the following temporal tokens
year︷ ︸︸ ︷ 0 · 1 · 2 · 3 · 4 · 5 · 6 · 7 · 8 · 9
month︷ ︸︸ ︷ 01 · 02 · 03 · 04 · 05 · 06 · 07 · 08 · 09 · 10 · 11 · 12
day︷ ︸︸",3 Time-Aware Representations,[0],[0]
︷ 0 ·,3 Time-Aware Representations,[0],[0]
"1 · 2 · 3 · 4 · 5 · 6 · 7 · 8 · 9
Hence, temporal tokens have a vocabulary size of 32.",3 Time-Aware Representations,[0],[0]
"Moreover, for each triple we can extract a sequence of predicate tokens that always consists of the relation type token and, if available, a temporal modifier token such as “since” or “until.”",3 Time-Aware Representations,[0],[0]
We refer to the concatenation of the predicate token sequence and (if available) the sequence of temporal tokens as the predicate sequence pseq.,3 Time-Aware Representations,[0],[0]
"Now, a temporal KG can be represented as a collection of triples of the form (s, pseq, o), wherein the predicate sequence may include temporal information.",3 Time-Aware Representations,[0],[0]
Table 1 lists some examples of such facts from a temporal KG and their corresponding predicate sequence.,3 Time-Aware Representations,[0],[0]
"We use the suffix y, m and d to indicate whether the digit corresponds to year, month or day information.",3 Time-Aware Representations,[0],[0]
It is these sequences of tokens that are used as input to a recurrent neural network.,3 Time-Aware Representations,[0],[0]
A long short-term memory (LSTM) is a neural network architecture particularly suited for modeling sequential data.,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"The equations defining an
LSTM are
i = σg(hn−1Ui + xnWi)
f = σg(hn−1Uf +",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"xnWf )
",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"o = σg(hn−1Uo + xnWo)
g = σc(hn−1Ug + xnWg)",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
cn = f ◦,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
cn−1,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
+ i ◦,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"g hn = o ◦ σh(cn)
(3)
",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"where i, f , o and g are the input, forget, output and input modulation gates, respectively.",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"c and h are the cell and hidden state, respectively.",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
All vectors are in Rh. xn ∈,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
Rd is the representation of the n-th element of a sequence.,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"In this paper we set h = d. σg, σc and σh are activation functions.
",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
Each token of the input sequence pseq is first mapped to its corresponding d-dimensional embedding via a linear layer and the resulting sequence of embeddings used as input to the LSTM.,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"Each predicate sequence of length N is represented by the last hidden state of the LSTM, that is, epseq = hN .",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"The predicate sequence representation, which carries time information, can now be used in conjunction with subject and object embeddings in standard scoring functions.",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"For instance, temporal-aware versions of TRANSE and DISTMULT, which we refer to as TA-TRANSE and TA-DISTMULT, have the following scoring function for triples (s, pseq, o):
TA-TRANSE: f(s, pseq, o) = ||es + epseq − eo||2 TA-DISTMULT: f(s, pseq, o) =",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"(es ◦ eo)eTpseq .
",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"All parameters of the scoring functions are learned jointly with the parameters of the LSTMs using stochastic gradient descent.
",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
"The advantages of character level models to encode time information for link prediction are: (1) the usage of digits and modifiers such as “since” as atomic tokens facilitates the transfer of information across similar timestamps, leading to higher efficiency (e.g. small vocabulary size); (2) at test time, one can obtain a representation for a timestamp even though it is not part of the training set;
(3) the model can use triples with and without temporal information as training data.",3.1 LSTMs for Time-Encoding Sequences,[0],[0]
Figure 1 illustrates the generic working of our approach.,3.1 LSTMs for Time-Encoding Sequences,[0],[0]
We conducted experiments on four different KG completion data sets where a subset of the facts are augmented with time information.,4 Experiments,[0],[0]
Integrated Crisis Early Warning System (ICEWS) is a repository that contains political events with a specific timestamp.,4.1 Datasets,[0],[0]
"These political events relate entities (e.g. countries, presidents...) to a number of other entities via logical predicates (e.g. ’Make a visit’ or ’Express intent to meet or negotiate’).",4.1 Datasets,[0],[0]
Additional information can be found at http://www.icews.com/. The repository is organized in dumps that contain the events that occurred each year from 1995 to 2015.,4.1 Datasets,[0],[0]
"We created two temporal KGs out of this repository, i) a short-range version that contains all events in 2014, and ii) a long-range version that contains all events occurring between 2005-2015.",4.1 Datasets,[0],[0]
"We refer to these two data sets as ICEWS 2014 and ICEWS 2005-15, respectively.",4.1 Datasets,[0],[0]
Due to the large number of entities we selected a subset of the most frequently occurring entities in the graph and all facts where both the subject and object are part of this subset of entities.,4.1 Datasets,[0],[0]
"We split the facts into training, validation and test in a proportion of 80%/10%/10%, respectively.",4.1 Datasets,[0],[0]
"The protocol for the creation of these data sets is identical to the onw followed in previous work (Bordes et al., 2013).",4.1 Datasets,[0],[0]
"To create YAGO15K, we used FREEBASE15K (Bordes et al., 2013) (FB15K) as a blueprint.",4.1 Datasets,[0],[0]
"We aligned entities from FB15K to YAGO (Hoffart et al., 2013) with SAMEAS relations contained in a YAGO dump2, and kept all facts involving those entities.",4.1 Datasets,[0],[0]
"Finally, we augment this collection of facts with time information from the “yagoDateFacts”3 dump.",4.1 Datasets,[0],[0]
"Contrary to the
2/yago-naga/yago3.1/yagoDBpediaInstances.ttl.7z 3/yago-naga/yago3.1/yagoDateFacts.ttl.7z
ICEWS data sets, YAGO15K does contain temporal modifiers; namely, ’occursSince’ and ’occursUntil’.",4.1 Datasets,[0],[0]
"Contrary to previous work (Leblay and Chekol, 2018), all facts maintain time information in the same level of granularity as one can find in the original dumps these data sets come from.
",4.1 Datasets,[0],[0]
"We also experimented with the temporal facts from the WIKIDATA data set4 extracted in (Leblay and Chekol, 2018).",4.1 Datasets,[0],[0]
"Only information regarding the year is available for these facts, since the authors discarded information of finer granularity.",4.1 Datasets,[0],[0]
All facts are framed in a time interval (i.e. they contain the temporal modifiers ’occursSince’ and ’occursUntil’).,4.1 Datasets,[0],[0]
Facts annotated with a single point-in-time are associated with that time-point as start and end time.,4.1 Datasets,[0],[0]
"Due to the large number of entities of this data set, which hinders the computation of standard KG completion metrics, we selected a subset of the most frequent entities and
4http://staff.aist.go.jp/julien.leblay/datasets
kept all facts where both the subject and object are part of this subset of entities.",4.1 Datasets,[0],[0]
"This set of filtered facts was split into training, validation and test in the same proportion as before.
",4.1 Datasets,[0],[0]
Table 2 lists some statistics of the temporal KGs.,4.1 Datasets,[0],[0]
"All four data sets, with their corresponding training, validation, and test splits are available at https://github.com/nle-ml/mmkb.",4.1 Datasets,[0],[0]
"We evaluate various methods by their ability to answer completion queries where i) all the arguments of a fact are known except the subject entity, and ii) all the arguments of a fact are known except the object entity.",4.2 General Set-up,[0],[0]
"For the former we replace the subject by each of the KBs entities E in turn, sort the triples based on the scores returned by the different methods, and computed the rank of the correct entity.",4.2 General Set-up,[0],[0]
"We repeated the same process for the second completion task and average the results.
",4.2 General Set-up,[0],[0]
This is standard procedure in the KG completion literature.,4.2 General Set-up,[0],[0]
"We also report the filtered setting as described in (Bordes et al., 2013).",4.2 General Set-up,[0],[0]
The mean of all computed ranks is the Mean Rank (lower is better) and the fraction of correct entities ranked in the top n is called hits@n (higher is better).,4.2 General Set-up,[0],[0]
"We also compute the Mean Reciprocal Rank (higher is better) which is less susceptible to outliers.
",4.2 General Set-up,[0],[0]
"Recent work (Leblay and Chekol, 2018) evaluates different approaches for performing link prediction in temporal KGs.",4.2 General Set-up,[0],[0]
"The approach that learns independent representations for each timestamp and use these representations as translation vectors, similarly to (Bordes et al., 2013), leads to the best results.",4.2 General Set-up,[0],[0]
"This approach is called VECTORBASED TTRANSE, though for the shake of simplicity in the paper we refer to it as TTRANSE.",4.2 General Set-up,[0],[0]
"We compare our approaches TA-TRANSE and TADISTMULT against TTRANSE, and the standard embedding methods TRANSE and DISTMULT.",4.2 General Set-up,[0],[0]
"For all approaches, we used ADAM (Kingma and Ba, 2014) for parameter learning in a mini-batch setting with a learning rate of 0.001, the categorical cross-entropy (Kadlec et al., 2017) as loss function and the number of epochs was set to 500.",4.2 General Set-up,[0],[0]
We validated every 20 epochs and stopped learning whenever the MRR values on the validation set decreased.,4.2 General Set-up,[0],[0]
The batch size was set to 512 and the number of negative samples to 500 for all experiments.,4.2 General Set-up,[0],[0]
The embedding size is d=100.,4.2 General Set-up,[0],[0]
"We apply dropout (Srivastava et al., 2014) for all embeddings.",4.2 General Set-up,[0],[0]
"We validated the dropout from the values {0, 0.4} for all experiments.",4.2 General Set-up,[0],[0]
"For TA-TRANSE and TA-DISTMULT, the activation gate σg is the sigmoid function; σc and σh were chosen to be linear activation functions.",4.2 General Set-up,[0],[0]
Table 3 and 4 list the results for the KG completion tasks.,4.3 Results,[0],[0]
"TA-TRANSE and TA-DISTMULT systematically improve TRANSE and DISTMULT in MRR, hits@10 and hits@1 in almost all cases.",4.3 Results,[0],[0]
Mean rank is a metric that is very susceptible to outliers and hence these improvements are not consistent.,4.3 Results,[0],[0]
TTRANSE learns independent representations for each timestamp contained in the training set.,4.3 Results,[0],[0]
"At test time, timestamps unseen during training are represented by null vectors.",4.3 Results,[0],[0]
"This explains that TTRANSE is only competitive in YAGO15K, wherein the number of distinct timestamps is very small (see #Distinct TS in Table 2) and thus enough training examples exist to learn robust timestamp embeddings.",4.3 Results,[0],[0]
"TTRANSE’s performance is similar to that of TA-TRANSE, our time-aware version of TRANSE, in WIKIDATA.",4.3 Results,[0],[0]
"Similarly, TTRANSE can learn robust timestamp representations because of the small number of distinct timestamps of this data set.
",4.3 Results,[0],[0]
"Figure 3 shows a comparison of the training loss of TRANSE and TA-TRANSE for YAGO15K. Under the same set-up, TA-TRANSE’s ability to learn from time information leads to a training loss lower than that of TRANSE.
",4.3 Results,[0],[0]
"Figure 2 shows a t-SNE (Maaten and Hinton, 2008) visualization of the embeddings learned for the predicate sequence pseq =",4.3 Results,[0],[0]
"[playsFor, occursSince, date], where date corresponds to the date token sequence.",4.3 Results,[0],[0]
This illustrates that the learned relation type embeddings carry temporal information.,4.3 Results,[0],[0]
We propose a digit-level LSTM to learn representations for time-augmented KG facts that can be used in conjunction with existing scoring functions for link prediction.,5 Conclusions,[0],[0]
Experiments in four temporal knowledge graphs show the effectiveness of the approach.,5 Conclusions,[0],[0]
Research on link prediction in knowledge graphs has mainly focused on static multirelational data.,abstractText,[0],[0]
In this work we consider temporal knowledge graphs where relations between entities may only hold for a time interval or a specific point in time.,abstractText,[0],[0]
"In line with previous work on static knowledge graphs, we propose to address this problem by learning latent entity and relation type representations.",abstractText,[0],[0]
"To incorporate temporal information, we utilize recurrent neural networks to learn timeaware representations of relation types which can be used in conjunction with existing latent factorization methods.",abstractText,[0],[0]
The proposed approach is shown to be robust to common challenges in real-world KGs: the sparsity and heterogeneity of temporal expressions.,abstractText,[0],[0]
Experiments show the benefits of our approach on four temporal KGs.,abstractText,[0],[0]
The data sets are available under a permissive BSD-3 license1.,abstractText,[0],[0]
Learning Sequence Encoders for Temporal Knowledge Graph Completion,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 712–718 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
712",text,[0],[0]
Text simplification (TS) is the task of modifying an original text into a simpler version of it.,1 Introduction,[0],[0]
One of the main parameters for defining a suitable simplification is the target audience.,1 Introduction,[0],[0]
"Examples include elderly, children, cognitively impaired users, nonnative speakers and low-literacy readers.
",1 Introduction,[0],[0]
"Traditionally, work on TS has been divided in lexical simplification (LS) and syntactic simplification (SS).",1 Introduction,[0],[0]
"LS (Paetzold, 2016) deals with the identification and replacement of complex words or phrases.",1 Introduction,[0],[0]
"SS (Siddharthan, 2011) performs
structural transformations such as changing a sentence from passive to active voice.",1 Introduction,[0],[0]
"However, most recent approaches learn transformations from corpora, addressing simplification at lexical and syntactic levels altogether.",1 Introduction,[0],[0]
"These include either learning tree-based transformations (Woodsend and Lapata, 2011; Paetzold and Specia, 2013) or using machine translation (MT)-based techniques (Zhu et al., 2010; Coster and Kauchak, 2011a; Wubben et al., 2012; Narayan and Gardent, 2014; Nisioi et al., 2017; Zhang and Lapata, 2017).",1 Introduction,[0],[0]
"This paper uses the latter type of technique, which treats TS as a monolingual MT task, where an original text is “translated” into its simplified version.
",1 Introduction,[0],[0]
"In order to build MT-based models, a parallel corpus of original texts with their simplified counterparts is needed.",1 Introduction,[0],[0]
"For English, two main such corpora are available: Wikipedia-Simple Wikipedia (W-SW) (Zhu et al., 2010) and the Newsela Article Corpus.1 The former is a collection of original Wikipedia articles and their simplified versions created by volunteers.",1 Introduction,[0],[0]
The latter consists of news articles professionally simplified for various specific audiences following the US school grade system.,1 Introduction,[0],[0]
"To build simplification models, the pairs of articles in these corpora have been aligned at the level of smaller units using standard algorithms (Coster and Kauchak, 2011b; Paetzold and Specia, 2016; Štajner et al., 2017).",1 Introduction,[0],[0]
"Based on the number of sentences involved in these alignments, one can categorise alignments into four types of coarse-grained simplification operations: • Identical: an original sentence is aligned to
itself, i.e. no simplification is performed.",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"Elaboration: an original sentence is aligned
to a single, rewritten simplified sentence.",1 Introduction,[0],[0]
"• One-to-many: splitting – an original sentence
is aligned to 2+ simplified sentences.
",1 Introduction,[0],[0]
"1https://newsela.com/data, v.2016-01-29.
",1 Introduction,[0],[0]
"• Many-to-one: joining – 2+ original sentences are aligned to a single simplified sentence.
",1 Introduction,[0],[0]
"We hereafter refer to the unit of simplification, i.e. one or more original or simplified sentences, as instances.
",1 Introduction,[0],[0]
"The Newsela corpus is seen as having higher quality than W-SW because its simplifications are created by professionals, following well defined guidelines (Xu et al., 2015).",1 Introduction,[0],[0]
It is also larger which is preferable for training corpus-based models.,1 Introduction,[0],[0]
"More interestingly, the Newsela corpus has a feature that has been ignored thus far: Each instance in the corpus was created for readers with a certain school grade level.",1 Introduction,[0],[0]
"Each original article has a label indicating its corresponding grade level (from 12 to 2), and may have various simplified versions, each for a different grade level.",1 Introduction,[0],[0]
"For example, a level 12 article may have simplified counterparts for levels 8 and 4.",1 Introduction,[0],[0]
"In other words, the corpus contains instances where the same input leads to different outputs.",1 Introduction,[0],[0]
Disregarding this factor may lead to suboptimal models.,1 Introduction,[0],[0]
"To avoid this problem, previous work (Alva-Manchego et al., 2017; Zhang and Lapata, 2017; Scarton et al., 2018b) has used subsets of the corpus with only certain combinations of complex-simplified article pairs, e.g. adjacent or non-adjacent pairs.",1 Introduction,[0],[0]
"This however reduces the amount of data available for training.
",1 Introduction,[0],[0]
"We propose a way of making use of this information to build more informed TS models that are aware of different types of target audiences, while still making use of the full dataset for learning.",1 Introduction,[0],[0]
"Inspired by the work of Johnson et al. (2017) for MT, we add to each original instance an artificial token that represents the target grade level of that instance in order to guide a sequence-to-sequence attentional encoder-decoder neural approach (Bahdanau et al., 2015) (§2).",1 Introduction,[0],[0]
"In a similar vein, we also annotate the coarse-grained type of operation that should be performed to simplify the original instance, under the hypothesis that certain operations are more often used to simplify into certain grade levels.",1 Introduction,[0],[0]
Deciding on the operation is an easier problem than performing the actual operation.,1 Introduction,[0],[0]
"We rely on both gold and predicted operation types.
",1 Introduction,[0],[0]
"Experiments with models built with these artificial tokens outperform state-of-the-art neural models for TS, with the best approach combining grade level and type of operation (§3).",1 Introduction,[0],[0]
"Interestingly, such an approach also enables zero-shot TS, where a simplification for a grade level pair unseen
at training time can still be generated during testing.",1 Introduction,[0],[0]
We show that our zero-shot learning models perform virtually as well as our grade/operationinformed models (§4).,1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to build TS models for specific target audiences and to explore zero-shot learning for this application.",1 Introduction,[0],[0]
"Our approach follows that of Johnson et al. (2017), a multilingual MT approach that adds an artificial token to encode the target language to the beginning of each source sentence in the parallel corpus.",2 System architecture,[0],[0]
"With this modified version of the corpus, a single encoder-decoder architecture is used to deal with different language pairs.",2 System architecture,[0],[0]
"Based on the tokens, the source sentences are encoded differently according to the target language they have been paired with in the corpus.",2 System architecture,[0],[0]
"Such an approach enables zeroshot MT, where a model is able to provide translations for language pairs it has not seem at training time.
",2 System architecture,[0],[0]
"We apply three types of data manipulation, where artificial tokens are added to the beginning of original side of both training and test instances: • to-grade: the token corresponds to the grade
level of the target instance, • operation: the token is one of the four possi-
ble coarse-grained operations that transforms the original into the simplified instance, • to-grade-operation: concatenation of the
two above tokens.",2 System architecture,[0],[0]
"Different from the grade level, which can be available at test time simply by knowing the intended reader of the text, information about the operations to be performed, which we extracted from the parallel corpus, will not be available at test time.",2 System architecture,[0],[0]
We use gold labels extracted from the parallel corpus for an oracle experiment but also use a classifier that predicts the operations for the test set based on those in the training data.,2 System architecture,[0],[0]
"We built a simple Naive Bayes classifier using the scikit-learn toolkit (Pedregosa et al., 2011) and nine features (Scarton et al., 2017): • number of tokens / punctuation / content
words / clauses, • ratio of the number of verbs / nouns / adjec-
tives / adverbs / connectives to the number of content words.
",2 System architecture,[0],[0]
"Table 1 shows examples of the tokens used when an original instance is marked to be simpli-
fied to grade level 4 or grade level 2.",2 System architecture,[0],[0]
"Since the reference for grade level 4 is a copy of the original, the operation token for this case is <identical>.",2 System architecture,[0],[0]
"For level 2 the reference is a rewrite and, therefore, the operation token is <elaboration>.
",2 System architecture,[0],[0]
We use OpenNMT2 as our encoder-decoder architecture.,2 System architecture,[0],[0]
"Both encoder and decoder have two LSTM layers, hidden states of size 500 and dropout = 0.3.",2 System architecture,[0],[0]
"Global attention combined with input-feeding is used, as describe in (Luong et al., 2015).",2 System architecture,[0],[0]
A model is trained for each dataset constructed with different artificial tokens for 13 epochs.,2 System architecture,[0],[0]
The best model is selected according to perplexity on the development set.,2 System architecture,[0],[0]
"Figure 1 shows the architecture of the neural network, including attention and input-feeding.",2 System architecture,[0],[0]
"In this example, <token> represents the artificial token added to the pre-processed data.
",2 System architecture,[0],[0]
"We evaluate our models with BLEU3 (Papineni et al., 2002) (a proxy for grammaticality assessment), SARI (Xu et al., 2016)4 (a proxy for simplicity assessment) and Flesch Reading Ease5 (a
2Torch version: http://opennmt.net/OpenNMT/ 3The multi-blue.perl script from https://github.
com/moses-smt/mosesdecoder 4https://github.com/cocoxu/ simplification 5https://github.com/mmautner/ readability
proxy for readability assessment).",2 System architecture,[0],[0]
"According to Xu et al. (2016), BLEU shows high correlation with human scores for grammaticality and meaning preservation, whilst SARI shows high correlation with human scores for simplicity.",2 System architecture,[0],[0]
"Although previous work have also relied on human judgements of grammaticality, meaning preservation and simplicity, in our case such a type of evaluation is infeasible: we would need to involve judges with specific grade levels or rely on professionals who are experts in grade level-specific simplification to make such assessments.",2 System architecture,[0],[0]
"Our version of the Newsela corpus has 550, 644 instance pairs (11M original tokens and 10M target tokens), which we randomly divided into training (440, 516 instances: 80%), development (55, 064 instances: 10%) and test (55, 064 instances: 10%) sets.",3 Reader-specific TS models,[0],[0]
Instances were aligned using the method by Paetzold and Specia (2016).,3 Reader-specific TS models,[0],[0]
Xu et al. (2015) report over 56K original sentences and approximately 305K sentences including the original ones and all simplification types.,3 Reader-specific TS models,[0],[0]
Our number of instance pairs is higher because we allowed alignments from original to all simplified versions and among simplified versions.,3 Reader-specific TS models,[0],[0]
"An original article 0 may be aligned to up to four simplified versions: 1, 2, 3 and 4.",3 Reader-specific TS models,[0],[0]
"For each article, the alignments were extracted between 0-{1,2,3,4}, 1- {2,3,4}, 2-{3,4} and 3-4, where available.",3 Reader-specific TS models,[0],[0]
"Our corpus is also larger than the ones used in (AlvaManchego et al., 2017; Scarton et al., 2018b) and (Zhang and Lapata, 2017).",3 Reader-specific TS models,[0],[0]
"While the former use only adjacent levels (e.g. 0-1, 1-2) and the latter only non-adjacent levels (e.g. 0-2, 1-4), we make use of the full dataset.
",3 Reader-specific TS models,[0],[0]
As baseline we trained a model using OpenNMT and the same hyperparameters as described in §2 on the entire Newsela corpus but without artificial tokens (s2s model).,3 Reader-specific TS models,[0],[0]
"The state-of-the-
art model is represented by NTS, which was also trained on the entire corpus using a similar OpenNMT architecture with the same hyperparameters but additional pre-trained word embeddings as described in Nisioi et al. (2017).6
As shown in Table 2 the NTS system performs slightly worse than the baseline system according to BLEU and SARI.",3 Reader-specific TS models,[0],[0]
"Although concatenating global and local embeddings has led to improvements for the W-SW corpus in (Nisioi et al., 2017), this does not seem to be the case for the Newsela corpus.",3 Reader-specific TS models,[0],[0]
Our models outperform both the baseline and NTS systems by a large margin.,3 Reader-specific TS models,[0],[0]
"Examples of outputs from all systems can be found in the Supplementary Material.
",3 Reader-specific TS models,[0],[0]
The best model is the one built with the <tograde+operation> token with gold operations annotations (last row).,3 Reader-specific TS models,[0],[0]
The second best system uses the gold <operation> token only.,3 Reader-specific TS models,[0],[0]
"Therefore, knowing the operation type to be performed for a given instance provides valuable information.",3 Reader-specific TS models,[0],[0]
"Even though the models with predicted operations (‘pred’ in Table 2) still outperform the baseline, they lag behind their counterparts built using gold operations.",3 Reader-specific TS models,[0],[0]
"The main reason for that is the very simplistic classifier we used (average accuracy = 0.51, calculated using 10-fold crossvalidation).",3 Reader-specific TS models,[0],[0]
"In summary, s2s+to-grade is the best performing model in a real world scenario, given the low performance of ‘pred’ systems.",3 Reader-specific TS models,[0],[0]
"A more informed classifier should lead to better results, but this left for future work; our goal was to show the potential of this information.
",3 Reader-specific TS models,[0],[0]
The improvements in SARI are substantial: 7 points over the baseline even with the predicted operations.,3 Reader-specific TS models,[0],[0]
"However, SARI aims to measure simplicity in general (not for specific grade levels).",3 Reader-specific TS models,[0],[0]
"Since human evaluation of the targeted simplification performed by our models is not feasible, we can only approximate the usefulness of our models by using readability metrics such
6Equivalent to their best performing “NTS-w2v” version.
",3 Reader-specific TS models,[0],[0]
as the Flesch-Kincaid Grade Level.,3 Reader-specific TS models,[0],[0]
"This metric maps a text into a US grade level, which is the same grading provided in the Newsela corpus and, therefore, relevant for our study.",3 Reader-specific TS models,[0],[0]
"Table 3 shows the Flesch-Kincaid results for the test set divided into the appropriate grade levels considering the outputs of s2s, s2s+to-grade and s2s+to-grade+operation (gold) models.",3 Reader-specific TS models,[0],[0]
"Simplifications generated by s2s+to-grade and s2s+tograde+operation are scored consistently closer to the appropriate grade, which does not happen with s2s.
",3 Reader-specific TS models,[0],[0]
The last row of Table 3 shows the Mean Absolute Error (MAE) considering the Flesch-Kincaid Grade Level scores for the system outputs as the hypothesis and the expected grade level as the gold scores.,3 Reader-specific TS models,[0],[0]
"Our s2s+to-grade and s2s+tograde+operation (gold) models show lower error scores than the baseline system, which supports our hypothesis that such models produce more adequate outputs for targeted grade levels.",3 Reader-specific TS models,[0],[0]
The main advantage of s2s+to-grade is that a user can inform their grade level and retrieve a personalised simplification.,3.1 Usefulness of the s2s+to-grade model,[0],[0]
"Table 4 shows an example with different simplifications for an out-of-domain instance from the SimPA corpus (Scarton et al., 2018a).",3.1 Usefulness of the s2s+to-grade model,[0],[0]
The same instance was given as input to the s2s+to-grade model with different artificial tokens according to the grade level that we want to achieve.,3.1 Usefulness of the s2s+to-grade model,[0],[0]
The s2s system (second row) repeats the original instance (first row).,3.1 Usefulness of the s2s+to-grade model,[0],[0]
"Conversely, our s2s+to-grade model is capable of distinguishing among different levels and produces personalised simplifications for each grade level.",3.1 Usefulness of the s2s+to-grade model,[0],[0]
"To show that zero-shot TS is possible, we build models on training data without instances of a certain grade level pair and test them on instances of that grade level pair.",4 Zero-shot TS models,[0],[0]
"Consider the grade level pair < go, gt >, where go is the grade level of an original instance o, gt of a target instance t, and t is aligned to o. We test if our “s2s+to-grade” model can generalise for instances of < ĝo, ĝt > that have not been seen at training time.
",4 Zero-shot TS models,[0],[0]
"Due to space restrictions, we only show results for three representative grade level pairs.",4 Zero-shot TS models,[0],[0]
These pairs have a large enough number of training and test instances and cover levels that are closer or further apart from each other.,4 Zero-shot TS models,[0],[0]
"In addition, after removing them the training corpus still has enough instances of the ĝt as target grade level.",4 Zero-shot TS models,[0],[0]
Instances of the target but not the original level (or of the target language in MT) must exist for zero-shot to be possible.,4 Zero-shot TS models,[0],[0]
"The distributions of the selected grade level pairs is shown in Table 5.
",4 Zero-shot TS models,[0],[0]
"In Table 6, the s2s and s2s+to-grade models are the same as in Section 3, i.e. trained with the entire dataset without artificial tokens (s2s) or with artificial tokens (s2s+to-grade).",4 Zero-shot TS models,[0],[0]
"The zero-shot models (s2s+to-grade+zs) are trained with <to-grade> data, but after removing instances of the grade level pair < ĝo, ĝt > under investigation, i.e. on a smaller dataset.",4 Zero-shot TS models,[0],[0]
"For < 12, 7 > and < 12, 4 >, the zero-shot models outperform the baseline according to all metrics.",4 Zero-shot TS models,[0],[0]
"In terms of SARI, for < 12, 7 > the zero-shot model is only marginally worse than the s2s+to-grade model.",4 Zero-shot TS models,[0],[0]
"Conversely, s2s+to-grade+zs outperforms s2s+to-grade for <
12, 4 >, which is an impressive result.",4 Zero-shot TS models,[0],[0]
"Finally, for < 6, 5 > all three models perform similarly.",4 Zero-shot TS models,[0],[0]
"This may be explained by the proximity of ĝo and ĝt, which means that instances must be considerably close to each other and therefore simplifications will be minor and have little impact in the scores.",4 Zero-shot TS models,[0],[0]
We have presented an approach for TS that benefits from corpora built for various target audiences and allows building better models than generalpurpose ones.,5 Conclusions,[0],[0]
"We have also shown that zero-shot learning is possible for TS, where instances of the original-target audience do not exist.",5 Conclusions,[0],[0]
As future work we intend to investigate (i) better classifiers to predict operation types and (ii) multi-task learning as an alternative way of building a single TS model for various specific target audiences.,5 Conclusions,[0],[0]
We also plan to run experiments with the W-SW corpus and using an improved classifier to train models with information on operations.,5 Conclusions,[0],[0]
"This work was supported by the EC project SIMPATICO (H2020-EURO-6-2015, grant number 692819).",Acknowledgements,[0],[0]
Text simplification (TS) is a monolingual text-to-text transformation task where an original (complex) text is transformed into a target (simpler) text.,abstractText,[0],[0]
Most recent work is based on sequence-to-sequence neural models similar to those used for machine translation (MT).,abstractText,[0],[0]
"Different from MT, TS data comprises more elaborate transformations, such as sentence splitting.",abstractText,[0],[0]
"It can also contain multiple simplifications of the same original text targeting different audiences, such as school grade levels.",abstractText,[0],[0]
We explore these two features of TS to build models tailored for specific grade levels.,abstractText,[0],[0]
Our approach uses a standard sequenceto-sequence architecture where the original sequence is annotated with information about the target audience and/or the (predicted) type of simplification operation.,abstractText,[0],[0]
"We show that it outperforms stateof-the-art TS approaches (up to 3 and 12 BLEU and SARI points, respectively), including when training data for the specific complex-simple combination of grade levels is not available, i.e. zero-shot learning.",abstractText,[0],[0]
Learning Simplifications for Specific Target Audiences,title,[0],[0]
Sleep plays a vital role in an individual’s health and wellbeing.,1. Introduction,[0],[0]
Sleep progresses in cycles that involve multiple sleep stages:,1. Introduction,[0],[0]
"Awake, Light sleep, Deep sleep and REM (Rapid Eye Movement).",1. Introduction,[0],[0]
Different stages are associated with different physiological functions.,1. Introduction,[0],[0]
"For example, deep sleep is essential for tissue growth, muscle repair, and memory consolidation, while REM helps procedural memory and emotional health.",1. Introduction,[0],[0]
"At least, 40 million Americans each year suffer from chronic sleep disorders (National Institute of Health).",1. Introduction,[0],[0]
Most sleep disorders can be managed once they are correctly diagnosed (National Institute of Health).,1. Introduction,[0],[0]
"Monitoring sleep stages is beneficial for diagnosing sleep disorders, and tracking the response to treatment (Carskadon & Rechtschaffen, 2000).
",1. Introduction,[0],[0]
Prevailing approaches for monitoring sleep stages are inconvenient and intrusive.,1. Introduction,[0],[0]
"The medical gold standard relies on Polysomnography (PSG), which is typically con-
1MIT CSAIL, Cambridge, MA, USA 2Massachusetts General Hospital, Boston, MA, USA.",1. Introduction,[0],[0]
"Correspondence to: Mingmin Zhao <mingmin@mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
ducted in a hospital or sleep lab, and requires the subject to wear a plethora of sensors, such as EEG-scalp electrodes, an ECG monitor, multiple chest bands, and nasal probes.",1. Introduction,[0],[0]
"As a result, patients can experience sleeping difficulties, which renders the measurements unrepresentative (Herbst, 2010).",1. Introduction,[0],[0]
"Furthermore, the cost and discomfort of PSG limit the potential for long term sleep studies.
",1. Introduction,[0],[0]
"Recent advances in wireless systems have demonstrated that radio technologies can capture physiological signals without body contact (Kaltiokallio et al., 2014; Adib et al., 2015; Zhao et al., 2016).",1. Introduction,[0],[0]
"These technologies transmit a low power radio signal (i.e., 1000 times lower power than a cell phone transmission) and analyze its reflections.",1. Introduction,[0],[0]
They extract a person’s breathing and heart beats from the radio frequency (RF) signal reflected off her body.,1. Introduction,[0],[0]
"Since the cardio-respiratory signals are correlated with sleep stages, in principle, one could hope to learn a subject’s sleep stages by analyzing the RF signal reflected off her body.",1. Introduction,[0],[0]
"Such a system would significantly reduce the cost and discomfort of today’s sleep staging, and allow for long term sleep stage monitoring.
",1. Introduction,[0],[0]
There are multiple challenges in realizing the potential of RF measurements for sleep staging.,1. Introduction,[0],[0]
"In particular, we must learn RF signal features that capture the sleep stages and their temporal progression, and the features should be transferable to new subjects and different environments.",1. Introduction,[0],[0]
"The problem is that RF signals carry much information that is irrelevant to sleep staging, and are highly dependent on the individuals and the measurement conditions.",1. Introduction,[0],[0]
"Specifically, they reflect off all objects in the environment including walls and furniture, and are affected by the subject’s position and distance from the radio device.",1. Introduction,[0],[0]
"These challenges were not addressed in past work which used hand-crafted signal features to train a classifier (Zaffaroni et al., 2014; Tataraidze et al., 2016b).",1. Introduction,[0],[0]
"The accuracy was relatively low (∼64%) and the model did not generalize beyond the environment where the measurements were collected.
",1. Introduction,[0],[0]
This paper presents a new model that delivers a significantly higher accuracy and generalizes well to new environments and subjects.,1. Introduction,[0],[0]
"The model adapts a convolutional neural network (CNN) to extract stage-specific features from RF spectrograms, and couples it with a recurrent
neural network (RNN) to capture the temporal dynamics of sleep stages.
",1. Introduction,[0],[0]
"However, a CNN-RNN combination alone would remain liable to distracting features pertaining to specific individuals or measurement conditions (i.e., the source domains), and hence would not generalize well.",1. Introduction,[0],[0]
"To address this issue, we introduce a new adversarial training regime that discards extraneous information specific to individuals or measurement conditions, while retaining all information relevant to the predictive task –i.e., the adversary ensures conditional independence between the learned representation and the source domains.
",1. Introduction,[0],[0]
"Our training regime involves 3 players: the feature encoder (CNN-RNN), the sleep stage predictor, and the source discriminator.",1. Introduction,[0],[0]
"The encoder plays a cooperative game with the predictor to predict sleep stages, and a minimax game against the source discriminator.",1. Introduction,[0],[0]
Our source discriminator deviates from the standard domain-adversarial discriminator in that it takes as input also the predicted distribution of sleep stages in addition to the encoded features.,1. Introduction,[0],[0]
"This dependence facilitates accounting for inherent correlations between stages and individuals, which cannot be removed without degrading the performance of the predictive task.
",1. Introduction,[0],[0]
"We analyze this game and demonstrate that at equilibrium, the encoded features discard all extraneous information that is specific to the individuals or measurement conditions, while preserving all information relevant to predicting the sleep stages.",1. Introduction,[0],[0]
We also evaluate our model on a dataset of RF measurements and corresponding sleep stages1.,1. Introduction,[0],[0]
Experimental results show that our model significantly improves the prediction accuracy of sleep stages.,1. Introduction,[0],[0]
"In particular, our model has a prediction accuracy of 79.8% and a Cohen’s Kappa of 0.70, whereas the best prior result for predicting sleep stages from RF signals (Tataraidze et al., 2016b) has an accuracy of 64% and a Cohen’s Kappa of 0.49.",1. Introduction,[0],[0]
(a) Sleep Staging: The gold standard in sleep staging is based on Polysomnography (PSG) conducted overnight in a hospital or sleep lab.,2. Related Work,[0],[0]
"The subject has to sleep while wearing multiple sensors including an EEG monitor, an EMG monitor, an EOG monitor, nasal probes, etc.",2. Related Work,[0],[0]
"A sleep technologist visually inspects the output of the sensors and assigns to each 30-second window a stage label (Rechtschaffen & Kales, 1968).
",2. Related Work,[0],[0]
A few past proposals have tried to automate the process and reduce the number of sensors.,2. Related Work,[0],[0]
"These solutions can be classified into four categories according to their source
1Dataset is available at: http://sleep.csail.mit.edu/
signal: EEG-based, Cardiorespiratory-based, Actigraphybased, or RF-based.",2. Related Work,[0],[0]
Table 1 summarizes the state of the art performance in each category.,2. Related Work,[0],[0]
"The table shows both the classification accuracy and the Cohen’s Kappa coefficient, κ.",2. Related Work,[0],[0]
"The most accurate methods rely on EEG signals (Ebrahimi et al., 2008; Fraiwan et al., 2012; Popovic et al., 2014; Shambroom et al., 2012).",2. Related Work,[0],[0]
"However, EEG monitors are also the most intrusive because they require the subject to sleep with a skullcap or a head-band equipped with multiple electrodes, which is uncomfortable and can cause headaches and skin irritation.",2. Related Work,[0],[0]
The second category requires the subject to wear a chest-band and analyzes the resulting cardiorespiratory signals.,2. Related Work,[0],[0]
"It is more comfortable than the prior method but also less accurate (Tataraidze et al., 2016a; Long et al., 2014).",2. Related Work,[0],[0]
"The third approach is based on actigraphy; it leverages accelerometers in FitBit or smart phones (Hao et al., 2013; Gu et al., 2014) to monitor body movements and infer sleep quality.",2. Related Work,[0],[0]
"Yet, motion is known to be a poor metric for measuring sleep stages and quality (Pollak et al., 2001).",2. Related Work,[0],[0]
The last approach relies on RF signals reflected off the subject body during her sleep.,2. Related Work,[0],[0]
It allows the subject to sleep comfortably without any onbody sensors.,2. Related Work,[0],[0]
"Yet past approaches in this category have the worst performance in comparison to other solutions.
",2. Related Work,[0],[0]
This paper builds on the above literature but delivers significant new contributions.,2. Related Work,[0],[0]
"In comparison to methods that use sources other than RF signals, the paper enables accurate monitoring of sleep stages while allowing the subject to sleep comfortably in her own bed without sensors on her body.",2. Related Work,[0],[0]
"Furthermore, due to differences between RF signals and other signal sources, our model has to eliminate extraneous information that are specific to the environment and irrelevant to sleep stage classification.",2. Related Work,[0],[0]
"In comparison to past work on learning sleep stages from RF signals (Rahman et al., 2015; Tataraidze et al., 2016b; Liu et al., 2014), our approach significantly improves the prediction accuracy as shown in Table 1.",2. Related Work,[0],[0]
"This improvement is due to intrinsic differences between past models and the model in this paper, which avoids hand-crafted features, and learns
features that capture the temporal dependencies and transfer well to new subjects and different environments.
",2. Related Work,[0],[0]
"(b) Representation Learning: We build on a rich body of literature on CNNs and RNNs which have been successfully used to model spatial patterns (Szegedy et al., 2015; He et al., 2016) and temporal dynamics (Sutskever et al., 2014), including combinations of the two (Pigou et al., 2015).",2. Related Work,[0],[0]
"Our CNN differs slightly in terms of convolutions that are adapted to our domain while, architecturally, our RNN is a standard variety LSTM.
",2. Related Work,[0],[0]
Our work also contributes to learning invariant representations in deep adversarial networks.,2. Related Work,[0],[0]
"Adversarial networks were introduced to effectively train complex generative models of images (Goodfellow et al., 2014; Radford et al., 2015; Chen et al., 2016) where the adversary (discriminator) was introduced so as to match generated samples with observed ones.",2. Related Work,[0],[0]
"The broader approach has since been adopted as the training paradigm across a number of other tasks as well, from learning representations for semisupervised learning (Makhzani et al., 2015), and modeling dynamic evolution (Vondrick et al., 2016; Purushotham et al., 2017) to inverse maps for inference (Donahue et al., 2017; Dumoulin et al., 2017), and many others.",2. Related Work,[0],[0]
"Substantial work has also gone into improving the stability of adversarial training (Metz et al., 2016; Arjovsky et al., 2017; Arjovsky & Bottou, 2017).
",2. Related Work,[0],[0]
"On a technical level, our work is most related to adversarial architectures for domain adaptation (Ganin & Lempitsky, 2015; Ganin et al., 2016; Tzeng et al., 2015; 2016).",2. Related Work,[0],[0]
"Yet, there are key differences between our approach and the above references, beyond the main application of sleep staging that we introduce.",2. Related Work,[0],[0]
"First, our goal is to remove conditional dependencies rather than making the representation domain independent.",2. Related Work,[0],[0]
"Thus, unlike the above references which do not involve conditioning in the adversary, our adversary takes the representation but is also conditioned on the predicted label distribution.",2. Related Work,[0],[0]
"Second, our game theoretic setup controls the information flow differently, ensuring that only the representation encoder is modified based on the adversary performance.",2. Related Work,[0],[0]
"Specifically, the predicted distribution over stages is strategically decoupled from the adversary (conditioning is uni-directional).",2. Related Work,[0],[0]
"Third, we show that this new conditioning guarantees an equilibrium solution that fully preserves the ability to predict sleep staging while removing, conditionally, extraneous information specific to the individuals or measurement conditions.",2. Related Work,[0],[0]
"Guarantees of this kind are particularly important for healthcare data where the measurements are noisy with a variety of dependencies that need to be controlled.
",2. Related Work,[0],[0]
"Finally, our work is naturally also related to other non-adversarial literature on multi-source domain adaptation (Crammer et al., 2008; Long et al., 2015), and work on
metrics for measuring distance between distributions (BenDavid et al., 2010; Fernando et al., 2013).",2. Related Work,[0],[0]
Let x ∈,3. Model,[0],[0]
"Ωx be an input sample, and y ∈ {1, 2, ..., ny} an output label.",3. Model,[0],[0]
"Let s ∈ {1, 2, ..., ns} denote an auxiliary label that refers to the source of a specific input sample.",3. Model,[0],[0]
We define x =,3. Model,[0],[0]
"[x1, x2..., xt] ∈ Ωx as the sequence of input samples from the beginning of time until the current time t.
In the context of our application, the above notation translates into the following: The input sample x is a 30-second RF spectrogram, and the output label y is a sleep stage that takes one of four values: Awake, Light Sleep, Deep Sleep, or REM.",3. Model,[0],[0]
The vector x refers to the sequence of RF spectrograms from the beginning of the night until the current time.,3. Model,[0],[0]
"Since RF signals carry information about the subject and the measurement environment, we assign each input x an auxiliary label s which identifies the subjectenvironment pair, hereafter referred to as the source.
",3. Model,[0],[0]
"Our goal is to learn a latent representation (i.e., an encoder) that can be used to predict label y; yet, we want this representation to generalize well to predict sleep stages for new subjects without having labeled data from them.",3. Model,[0],[0]
Simply making the representation invariant to the source domains could hamper the accuracy of the predictive task.,3. Model,[0],[0]
"Instead we would like to remove conditional dependencies between the representation and the source domains.
",3. Model,[0],[0]
We introduce a multi-domain adversarial model that achieves the above goal.,3. Model,[0],[0]
Our model is shown in Fig. 1(a).,3. Model,[0],[0]
"It has three components: An encoder E, a label predictor F , and a source discriminator D. Our model is set up as a game, where the representation encoder plays a cooperative game with the label predictor to allow it to predict the correct labels using the encoded representation.",3. Model,[0],[0]
"The encoder also plays a minimax game against the source discrimina-
tor to prevent it from decoding the source label from the encoded representation.
",3. Model,[0],[0]
"A key characteristic of our model is the conditioning of the source discriminator on the label distribution, Py(·|x) (see Fig. 1(a)).",3. Model,[0],[0]
"This conditioning of the adversary allows the learned representation to correlate with the domains, but only via the label distribution –i.e., removes conditional dependencies between the representation and the sources.
",3. Model,[0],[0]
The rest of this section is organized as follows.,3. Model,[0],[0]
"We first formally define three players E, F , and D and the representation invariance they are trained to achieve.",3. Model,[0],[0]
"In Sec. 3.1, we analyze the game and prove that at equilibrium the encoder discards all extraneous information about the source that is not beneficial for label prediction (i.e., predicting y).",3. Model,[0],[0]
Training the ideal model in Fig. 1(a) is challenging because it requires access to the label distribution Py(·|x).,3. Model,[0],[0]
"To drive an efficient training algorithm, we define in Sec. 3.2 an extended game where the source discriminator uses the output of the label predictor as an approximation of the posterior probabilities, as shown in Fig. 1(b).",3. Model,[0],[0]
"We prove that the equilibriums of the original game are also equilibriums in the extended one.
Encoder E: An encoder E(·) :",3. Model,[0],[0]
"Ωx → Ωz is a function that takes a sequence of input samples x, and returns a vector summary of x as z = E(x).
",3. Model,[0],[0]
Label Predictor F : A label predictor F (·) :,3. Model,[0],[0]
"Ωz → [0, 1]ny takes a latent representationE(x) as input and predicts the probability of each label y associated with input x as QF (y|E(x)).",3. Model,[0],[0]
The goal of an ideal predictor F is to approximate Py(·|x) with QF (·|E(x)).,3. Model,[0],[0]
"The loss of the label predictor, F , given the encoder E, is defined as the cross-entropy between the label distribution Py(·|x) and QF (·|E(x)):
Lf (F ;E) =",3. Model,[0],[0]
"Ex,y[− logQF (y|E(x))]",3. Model,[0],[0]
"(1) During training, the encoder E and predictor F play a cooperative game to minimize the label prediction loss.
",3. Model,[0],[0]
"Source Discriminator D: We define a source discriminator as D(·, ·) :",3. Model,[0],[0]
Ωz ×,3. Model,[0],[0]
"[0, 1]ny",3. Model,[0],[0]
"→ [0, 1]ns .",3. Model,[0],[0]
"It takes the latent representation E(x) and the label distribution Py(·|x) as inputs, and predicts which source domain (i.e., subject and environment) they are sampled from as QD(·|E(x), Py(·|x)).",3. Model,[0],[0]
"Next, we define the desired representation invariance.",3. Model,[0],[0]
Definition 1 (Representation invariance).,3. Model,[0],[0]
"We say that representation E is invariant if E(x) contains no information about s beyond what is already contained in Py(·|x); that is, QD(·|E(x), Py(·|x))",3. Model,[0],[0]
"= QD(·|Py(·|x)) for the optimal D.
To measure the invariance of an encoder E, we define the
loss of the source discriminator D as the cross-entropy between Ps(·|x) and QD(·|E(x), Py(·|x)):
Ld(D;E) =",3. Model,[0],[0]
"Ex,s[− logQD(s|E(x), Py(·|x))]",3. Model,[0],[0]
"(2)
During training, encoder E and discriminator D play a minimax game: while D is trained to minimize the source prediction loss, encoderE is trained to maximize it in order to achieve the above invariance.",3. Model,[0],[0]
"During training, encoder E plays a co-operative game with predictor F , and a minimax game with discriminator D. We define a value function of E, F and D with λ > 0:
V(E,F,D) =",3.1. Ideal Game,[0],[0]
"Lf (F ;E)− λ · Ld(D;E) (3)
",3.1. Ideal Game,[0],[0]
"The training procedure can be viewed as a three-player minimax game of E, F and D:
min E min F max D V(E,F,D) =",3.1. Ideal Game,[0],[0]
"min E,F max D V(E,F,D) (4)
",3.1. Ideal Game,[0],[0]
Proposition 2 (Optimal predictor).,3.1. Ideal Game,[0],[0]
"Given encoder E,
Lf (E) , min F Lf (F ;E) ≥ H(y|E(x)), (5)
where H(·) is entropy.",3.1. Ideal Game,[0],[0]
"The optimal predictor F ∗ that achieves equality is:
QF∗(y|E(x))",3.1. Ideal Game,[0],[0]
= p(y|E(x)),3.1. Ideal Game,[0],[0]
"(6)
Proof.
",3.1. Ideal Game,[0],[0]
"Lf (F ;E) =Ex,y[− logQF (y|E(x))]",3.1. Ideal Game,[0],[0]
"=EE(x),y[− logQF (y|E(x))]",3.1. Ideal Game,[0],[0]
=Ez∼P (E(x)),3.1. Ideal Game,[0],[0]
Ey∼P (y|z)[− logQF (y|z)],3.1. Ideal Game,[0],[0]
"=Ez∼P (E(x))[H(y|z) +DKL(P (y|z) ‖QF (y|z))] ≥Ez∼P (E(x))[H(y|z)] =H(y|E(x))
",3.1. Ideal Game,[0],[0]
The equality holds when DKL(P (y|E(x)) ‖QF,3.1. Ideal Game,[0],[0]
(y|E(x))),3.1. Ideal Game,[0],[0]
= 0 for almost every x ∈ Supp(Px).,3.1. Ideal Game,[0],[0]
That is QF∗(y|E(x)),3.1. Ideal Game,[0],[0]
"= p(y|E(x)) for almost every y and x ∈ Supp(Px).
",3.1. Ideal Game,[0],[0]
"Similarly we can prove the following Proposition.
",3.1. Ideal Game,[0],[0]
Proposition 3 (Optimal discriminator).,3.1. Ideal Game,[0],[0]
"Given encoder E,
Ld(E) , min D Ld(D;E) ≥ H(s|E(x), Py(·|x))",3.1. Ideal Game,[0],[0]
"(7)
The optimal discriminator D∗ that achieves this value is:
QD∗(s|E(x), Py(·|x))",3.1. Ideal Game,[0],[0]
"= P (s|E(x), Py(·|x)) (8)
Corollary 3.1. H(s) is an upper bound of the loss of the optimal discriminator D∗ for any encoder E.
Next, we state the virtual training criterion of the encoder.
",3.1. Ideal Game,[0],[0]
Proposition 4.,3.1. Ideal Game,[0],[0]
"If predictor F and discriminator D have enough capacity and are trained to achieve their optimal losses, the minimax game (4) can be rewritten as the following training procedure of encoder E:
min E
",3.1. Ideal Game,[0],[0]
"[H(y|E(x))− λ ·H(s|E(x), Py(·|x))]",3.1. Ideal Game,[0],[0]
"(9)
Proof.",3.1. Ideal Game,[0],[0]
"Based on the losses of the optimal predictor F ∗ and the optimal discriminator D∗ in Proposition 2 and Proposition 3, the minimax game (4) can be rewritten as (9).",3.1. Ideal Game,[0],[0]
"Thus, encoderE is trained to minimize a virtual training criterion C(E) = H(y|E(x))−",3.1. Ideal Game,[0],[0]
"λ ·H(s|E(x), Py(·|x)).
",3.1. Ideal Game,[0],[0]
"Next, we describe the optimal encoder.
",3.1. Ideal Game,[0],[0]
Theorem 5 (Optimal encoder).,3.1. Ideal Game,[0],[0]
"If encoder E, predictor F and discriminatorD have enough capacity and are trained to reach optimum, any global optimal encoder E∗ has the following properties:
H(y|E∗(x))",3.1. Ideal Game,[0],[0]
"= H(y|x) (10a) H(s|E∗(x), Py(·|x))",3.1. Ideal Game,[0],[0]
= H(s|Py(·|x)),3.1. Ideal Game,[0],[0]
"(10b)
",3.1. Ideal Game,[0],[0]
Proof.,3.1. Ideal Game,[0],[0]
"Since E(x) is a function of x:
Lf (E) = H(y|E(x))",3.1. Ideal Game,[0],[0]
"≥ H(y|x) (11a) Ld(E) = H(s|E(x), Py(·|x))",3.1. Ideal Game,[0],[0]
≤,3.1. Ideal Game,[0],[0]
H(s|Py(·|x)),3.1. Ideal Game,[0],[0]
"(11b)
",3.1. Ideal Game,[0],[0]
"Hence, C(E) = H(y|E(x))",3.1. Ideal Game,[0],[0]
"− λ ·H(s|E(x), Py(·|x))",3.1. Ideal Game,[0],[0]
≥ H(y|x)−λ ·H(s|Py(·|x)).,3.1. Ideal Game,[0],[0]
The equality holds if and only if both (10a) and (10b) are satisfied.,3.1. Ideal Game,[0],[0]
"Therefore, we only need to prove that the optimal value of C(E) is equal to H(y|x)−λ·H(s|Py(·|x))",3.1. Ideal Game,[0],[0]
"in order to prove that any global encoder E∗ satisfies both (10a) and (10b).
",3.1. Ideal Game,[0],[0]
We show thatC(E) can achieveH(y|x)−λ·H(s|Py(·|x)),3.1. Ideal Game,[0],[0]
by considering the following encoder E0: E0(x) = Py(·|x).,3.1. Ideal Game,[0],[0]
It can be examined that H(y|E0(x)),3.1. Ideal Game,[0],[0]
"= H(y|x) and H(s|E0(x), Py(·|x))",3.1. Ideal Game,[0],[0]
"= H(s|Py(·|x)).
",3.1. Ideal Game,[0],[0]
"Adversarial training of D can be viewed as a regularizer, which leads to a common representation space for multiple source domains.",3.1. Ideal Game,[0],[0]
"From Theorem 5, the optimal encoder E∗ using adversarial training satisfies H(y|E∗(x))",3.1. Ideal Game,[0],[0]
"= H(y|x), which is the maximal discriminative capability that any encoder E can achieve.",3.1. Ideal Game,[0],[0]
"Thus, we have the following corollary.
",3.1. Ideal Game,[0],[0]
Corollary 5.1.,3.1. Ideal Game,[0],[0]
"Adversarial training of the discriminator does not reduce the discriminative capability of the representation.
",3.1. Ideal Game,[0],[0]
Remark 5.1.,3.1. Ideal Game,[0],[0]
"During the proof of Theorem 5, we construct an encoder E0(x) = Py(·|x) that can achieve the optimal value of C(E).",3.1. Ideal Game,[0],[0]
"However, we argue that training will not converge to this trivial encoder in practice.",3.1. Ideal Game,[0],[0]
"This is because Py(·|x) is a mapping from the full signal history to the distribution over stages at the current step, therefore itself highly complex.",3.1. Ideal Game,[0],[0]
"Since we use the RNN state as the encodingE(x), and it feeds into the LSTM gates, distribution over stages at previous step does not represent a sufficient summary of the history until the current one.",3.1. Ideal Game,[0],[0]
"Therefore, E(x) must be able to anticipate the temporal evolution of the signal and contain a more effective summary than Py(·|x) would be.",3.1. Ideal Game,[0],[0]
Corollary 5.2.,3.1. Ideal Game,[0],[0]
"If encoder E and predictor F have enough capacity and are trained to reach optimum, the output of F is equal to Py(·|x).
",3.1. Ideal Game,[0],[0]
Proof.,3.1. Ideal Game,[0],[0]
"When predictor F is optimal (Proposition 2), QF (y|E(x))",3.1. Ideal Game,[0],[0]
= p(y|E(x)).,3.1. Ideal Game,[0],[0]
"When E is optimal (Theorem 5), H(y|E(x))",3.1. Ideal Game,[0],[0]
"= H(y|x), that is p(y|E(x))",3.1. Ideal Game,[0],[0]
= p(y|x).,3.1. Ideal Game,[0],[0]
"Therefore, QF (y|E(x)) = p(y|x).",3.1. Ideal Game,[0],[0]
"In practice, estimating the posterior label distribution Py(·|x) from labeled data is a non-trivial task.",3.2. Extended Game,[0],[0]
Fortunately however our predictor F and encoderE are playing a cooperative game to approximate this posterior label distribution Py(·|x),3.2. Extended Game,[0],[0]
withQF (·|E(x)),3.2. Extended Game,[0],[0]
.,3.2. Extended Game,[0],[0]
"Therefore, we useQF (·|E(x)), the output of predictor F , as a proxy of Py(·|x) and feed it as input to discriminator D (Fig. 1(b)).
",3.2. Extended Game,[0],[0]
"An extended three-player game arises: while encoder E still plays a cooperative game with predictor F and a minimax game with discriminator D, discriminator D depends strategically on predictor F but not vice versa.",3.2. Extended Game,[0],[0]
"The dotted line in Fig. 1(b) illustrates this dependency.
",3.2. Extended Game,[0],[0]
"The relationship between the ideal minimax game (Sec. 3.1) and the extended one is stated below.
",3.2. Extended Game,[0],[0]
Proposition 6.,3.2. Extended Game,[0],[0]
"If encoder E, predictor F and discriminator D have enough capacity, the solution that encompasses the optimal encoder, E∗, predictor, F ∗ and discriminator, D∗, in the ideal minimax game is also an equilibrium solution of the extended game.
",3.2. Extended Game,[0],[0]
Proof.,3.2. Extended Game,[0],[0]
"By Corollary 5.2, when encoder E and predictor F are optimal, QF (·|E(x)) is equal to Py(·|x).",3.2. Extended Game,[0],[0]
"Thus, the extended game becomes equivalent to the ideal game, and E∗, F ∗ and D∗ is an equilibrium solution of both games.
",3.2. Extended Game,[0],[0]
"Algorithm 1 Encoder, predictor and discriminator training Input: Labeled data {(xi, yi, si)}Mi=1, learning rate η.",3.2. Extended Game,[0],[0]
Compute stop criterion for inner loop:,3.2. Extended Game,[0],[0]
"δd ← H(s) for number of training iterations do
Sample a mini-batch of training data {(xi, yi, si)}mi=1",3.2. Extended Game,[0],[0]
Lif ←,3.2. Extended Game,[0],[0]
"− logQF (yi|E(xi))
",3.2. Extended Game,[0],[0]
wi ← QF (·|E(xi)),3.2. Extended Game,[0],[0]
"I stop gradient along this link Lid ← − logQD(si|E(xi),wi)
Vi = Lif",3.2. Extended Game,[0],[0]
"− λ · Lid Update encoder E:
θe ← θe − ηe∇θe 1m ∑m i=1",3.2. Extended Game,[0],[0]
"Vi
Update predictor F : θf ← θf − ηf∇θf 1m ∑m i=1",3.2. Extended Game,[0],[0]
"Vi repeat Update discriminator D:
θd ← θd + ηd∇θd 1m ∑m i=1",3.2. Extended Game,[0],[0]
"Vi
until 1m ∑m i=1",3.2. Extended Game,[0],[0]
Lid ≤,3.2. Extended Game,[0],[0]
"δd
end for",3.2. Extended Game,[0],[0]
We implement the extended three-player game with iterative updates of the players (Algorithm 1).,3.3. Training Algorithm,[0],[0]
"Note that, since the output of the label predictor is a proxy of the underlying posterior, and since the source discriminator depends strategically on the predictor but not vice versa, the gradient does not back-propagate from the discriminator to the predictor (i.e., the dotted link in Fig. 1(b)).
",3.3. Training Algorithm,[0],[0]
"The number of training steps in the inner loop usually needs to be carefully chosen (Goodfellow et al., 2014).",3.3. Training Algorithm,[0],[0]
A large number of steps is computationally inefficient but a small one will cause the model to collapse.,3.3. Training Algorithm,[0],[0]
"This is because the outer players, E and F , can be over-trained against a nonoptimal inner player D, and they will try to maximize Ld at the cost of increasing Lf .",3.3. Training Algorithm,[0],[0]
"To prevent the model collapse phenomenon, we use an adaptive number of training steps in the inner loop and adjust it dynamically based on Ld (Algorithm 1).",3.3. Training Algorithm,[0],[0]
The idea is to use the upper bound in Corollary 3.1 as the stopping criterion for the inner loop.,3.3. Training Algorithm,[0],[0]
"While we described our model in the context of sleep staging, we believe the model can be applied more broadly.",3.4. Discussion of the Model Benefits,[0],[0]
Our model is characterized by the 3-way game and the adversarial conditioning on the label distribution.,3.4. Discussion of the Model Benefits,[0],[0]
This combination yields the following benefits: 1) It guarantees an equilibrium solution that fully preserves the ability to perform the predictive task while removing any distracting information specific to the source domains.,3.4. Discussion of the Model Benefits,[0],[0]
Guarantees of this kind are particularly important in healthcare where the measurements are noisy and have a variety of dependencies that need to be controlled.,3.4. Discussion of the Model Benefits,[0],[0]
2),3.4. Discussion of the Model Benefits,[0],[0]
"It allows to properly leverage the
adversarial feedback even when the target labels are uncertain.",3.4. Discussion of the Model Benefits,[0],[0]
"For example, in the sleep staging problem, each 30-second window is given one label.",3.4. Discussion of the Model Benefits,[0],[0]
"Yet, many such windows include transitions between sleep stages, e.g., a transition from light to deep sleep.",3.4. Discussion of the Model Benefits,[0],[0]
These transitions are gradual and hence the transition windows can be intrinsically different from both light and deep sleep.,3.4. Discussion of the Model Benefits,[0],[0]
It would be desirable to have the learned representation capture the concept of transition and make it invariant to the source (see the results in Sec. 4.5).,3.4. Discussion of the Model Benefits,[0],[0]
3),3.4. Discussion of the Model Benefits,[0],[0]
It allows the conditioning to remain available for additional guiding of representations based on unlabeled data.,3.4. Discussion of the Model Benefits,[0],[0]
The model can incorporate unlabeled data for either semi-supervised learning or transductive learning within a unified framework.,3.4. Discussion of the Model Benefits,[0],[0]
"In this section, we empirically evaluate our model.",4. Experiments,[0],[0]
RF-Sleep is a dataset of RF measurements during sleep with corresponding sleep stage labels.,4.1. RF-Sleep Dataset,[0],[0]
"All studies that involve human subjects were approved by our IRB.
Study setup: The sleep studies are done in the bedroom of each subject.",4.1. RF-Sleep Dataset,[0],[0]
We install a radio device in the bedroom.,4.1. RF-Sleep Dataset,[0],[0]
"It transmits RF signals and measure their reflections while the subject is sleeping alone in the bed.
",4.1. RF-Sleep Dataset,[0],[0]
"Ground truth: During the study, each subject sleeps with an FDA-approved EEG-based sleep monitor (Popovic et al., 2014), which collects 3-channel frontal EEG.",4.1. RF-Sleep Dataset,[0],[0]
The monitor labels every 30-second of sleep with the subject’s sleep stage.,4.1. RF-Sleep Dataset,[0],[0]
"This system has human-level comparable accuracy (Popovic et al., 2014), and has already been used in several sleep studies(Lucey et al., 2016; Shah et al., 2016).
",4.1. RF-Sleep Dataset,[0],[0]
Size of dataset:,4.1. RF-Sleep Dataset,[0],[0]
The dataset collects 100 nights of sleep from 25 young healthy subjects (40% females).,4.1. RF-Sleep Dataset,[0],[0]
It contains over 90k 30-second epochs of RF measurements and their corresponding sleep stages provided by the EEG-based sleep monitor.,4.1. RF-Sleep Dataset,[0],[0]
"Each epochs has one of four labels Awake, REM, Light Sleep (N1 or N2) and Deep Sleep (N3).",4.1. RF-Sleep Dataset,[0],[0]
"We parameterize encoder E, predictor F and discriminator D as neural networks.",4.2. Parameterization,[0],[0]
Encoder E is parameterized by a hybrid CNN-RNN model.,4.2. Parameterization,[0],[0]
"We adapt a residual networks architecture (He et al., 2016) with 24 convolutional layers to extract features from each 30-second RF spectrogram, and an RNN with LSTM cell (Hochreiter & Schmidhuber, 1997) that takes sequences of CNN features as input.",4.2. Parameterization,[0],[0]
Both predictor F and discriminator D are parameterized by networks with two fully-connected layers.,4.2. Parameterization,[0],[0]
"We evaluate the model on every subject while training on the data collected from the other subjects (i.e., the model is never trained on data from the test subject).",4.3. Classification Results,[0],[0]
"The training data is randomly split into a training set and validation set (75%/25%).
",4.3. Classification Results,[0],[0]
"We use two metrics commonly used in automated sleep staging, namely Accuracy and Cohen’s Kappa.",4.3. Classification Results,[0],[0]
"While accuracy measures the percent agreement with ground truth, Cohen’s Kappa coefficient κ (Cohen, 1960) takes into account the possibility of the agreement occurring by chance and is usually a more robust metric.",4.3. Classification Results,[0],[0]
"κ > 0.4, κ > 0.6, κ > 0.8 are considered to be moderate, substantial and almost perfect agreement (Landis & Koch, 1977).
",4.3. Classification Results,[0],[0]
Table 2 shows the accuracy and Cohen’s Kappa of our model compared to the state-of-the-art in classifying sleep stages using RF reflections.,4.3. Classification Results,[0],[0]
"Since neither the dataset nor the code used in past papers is publicly available, we compare with their published results.",4.3. Classification Results,[0],[0]
We note however that the Cohen’s Kappa provides some normalization since it accounts for the underlying uncertainty in the data.,4.3. Classification Results,[0],[0]
"The table shows that our model has an accuracy of 79.8% and a κ = 0.70, which significantly outperforms past solutions.
",4.3. Classification Results,[0],[0]
Fig. 2(a) shows the confusion matrix of our model.,4.3. Classification Results,[0],[0]
Fig. 2(b) also shows the accuracy on each subject.,4.3. Classification Results,[0],[0]
"It has a standard deviation of 2.9%, suggesting that our model is capable of adapting to different subjects and environments.
",4.3. Classification Results,[0],[0]
"Finally, we show in Fig. 3",4.3. Classification Results,[0],[0]
"the full-night predictions along with the ground truth for the average, best, and worst classification accuracy.",4.3. Classification Results,[0],[0]
We analyze the role of CNN and RNN in predicting sleep stages.,4.4. Understanding the Role of CNN & RNN,[0],[0]
"To do so, we use t-SNE embedding (Maaten & Hinton, 2008) to visualize the response of our network after CNN and RNN, respectively.",4.4. Understanding the Role of CNN & RNN,[0],[0]
Fig. 4 shows the visualization results from one of the subjects.,4.4. Understanding the Role of CNN & RNN,[0],[0]
Data points are randomly sub-sampled for better viewing.,4.4. Understanding the Role of CNN & RNN,[0],[0]
"The result shows that the CNN succeeds at separating the Wake, REM from Light and Deep Sleep.",4.4. Understanding the Role of CNN & RNN,[0],[0]
However it fails at separate Light Sleep and Deep Sleep from each other.,4.4. Understanding the Role of CNN & RNN,[0],[0]
"In contrast, Light Sleep and Deep Sleep form different clusters in the RNN response.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"These results demonstrate the role of CNN and RNN in our model: CNN learns stage-specific features that can distinguish Wake, REM and from Deep and Light Sleep.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"RNN captures the dynamics of those features to fur-
ther determine whether the sleep is light or deep.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"Note that Light and Deep Sleep are more similar to each other and are typically referred to as NREM, i.e., non-REM.
",4.4. Understanding the Role of CNN & RNN,[0],[0]
We have trained a similar model without the RNN layer on top of CNN.,4.4. Understanding the Role of CNN & RNN,[0],[0]
"In this case, the overall accuracy decreases by 12.8%, specifically the precision light and deep sleep decreases by 23.5%.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"This suggests that there are stagespecific information embedded in the temporal dynamics of the RF measurements, and therefore can only be captured and exploited with RNN.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"Moreover, these temporal dynamics are particularly crucial for distinguishing light and deep sleep.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"Indeed, there are known temporal patterns that govern the progression of light and deep sleep through the night (Carskadon et al., 2005).",4.4. Understanding the Role of CNN & RNN,[0],[0]
"For example, the probability of being in deep sleep decreases as sleep progresses.",4.4. Understanding the Role of CNN & RNN,[0],[0]
"Also, people usually need to go through light sleep before they can get into deep sleep.",4.4. Understanding the Role of CNN & RNN,[0],[0]
These temporal dynamics of sleep stages can be captured by RNN and might be exploited to distinguish light and deep sleep.,4.4. Understanding the Role of CNN & RNN,[0],[0]
We evaluate the role of our adversarial discriminator in learning transferable features for predicting sleep stages.,4.5. Role of Our Adversarial Discriminator,[0],[0]
We first look at the losses on the validation set as training progresses to check whether the extraneous information specific to the individuals and environments can be removed.,4.5. Role of Our Adversarial Discriminator,[0],[0]
"As a baseline, we compare with a version of our model without the source discriminator.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"For this baseline,
we train a (non-adversarial) discriminator to determine the source of features.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"Fig. 5 shows that the loss of the source discriminator in the baseline model decreases very quickly while ours stays high (upper bounded by H(s) = 2.81 in this case), suggesting that our learned representation is invariant across sources.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"The figure also shows that adding an adversarial discriminator increases the performance on the test set and can be helpful in reducing over-fitting.
",4.5. Role of Our Adversarial Discriminator,[0],[0]
"To check that our adversarial model has learned transferable features, we visualize the learned featuresE(x) on the test data for both models.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"Color-coding the sources, Fig. 6 shows that our learned features have almost the same distribution on different sources, while the baseline model learns features that are separable.
",4.5. Role of Our Adversarial Discriminator,[0],[0]
"Next, we illustrate the benefits of conditioning on the posterior distribution, and that it can recover underlying concepts not specified in the labels.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"We consider the learned features for transition periods between light and deep sleep, which might be a class that is different from both light and deep sleep.",4.5. Role of Our Adversarial Discriminator,[0],[0]
We define transition periods as epochs that have both light and deep sleep as neighbors.,4.5. Role of Our Adversarial Discriminator,[0],[0]
We visualize it with a different color.,4.5. Role of Our Adversarial Discriminator,[0],[0]
"Color-coding stages and shape-coding sources, Fig. 7 shows the learned features from transition periods are segregated, as those from light sleep and deep sleep.",4.5. Role of Our Adversarial Discriminator,[0],[0]
"This indicates that our learned features have recovered the concept of a transition period, which is helpful in understanding and predicting sleep stages.",4.5. Role of Our Adversarial Discriminator,[0],[0]
This paper introduce a new predictive model that learns sleep stages from RF signals and achieves a significant improvement over the state-of-the-art.,5. Conclusion,[0],[0]
We believe this work marks an important step in sleep monitoring.,5. Conclusion,[0],[0]
"We also believe that the proposed adversarial setup, which extracts task-specific domain-invariant features, is applicable to other predictive tasks, particularly in health sensing where variations across subjects and measurement conditions could be a major challenge.",5. Conclusion,[0],[0]
The authors thank the anonymous reviewers for their helpful comments in revising the paper.,Acknowledgments,[0],[0]
We are grateful to the members of the CSAIL for their insightful discussions and to all the human subjects for their participation in our experiments.,Acknowledgments,[0],[0]
We focus on predicting sleep stages from radio measurements without any attached sensors on subjects.,abstractText,[0],[0]
We introduce a new predictive model that combines convolutional and recurrent neural networks to extract sleep-specific subjectinvariant features from RF signals and capture the temporal progression of sleep.,abstractText,[0],[0]
"A key innovation underlying our approach is a modified adversarial training regime that discards extraneous information specific to individuals or measurement conditions, while retaining all information relevant to the predictive task.",abstractText,[0],[0]
We analyze our game theoretic setup and empirically demonstrate that our model achieves significant improvements over state-of-the-art solutions.,abstractText,[0],[0]
Learning Sleep Stages from Radio Signals: A Conditional Adversarial Architecture,title,[0],[0]
An accurate identification of the system dynamics is the first and very crucial step to many modern control methods.,1. Introduction,[0],[0]
"Although reinforcement learning also allows model-free search for optimal policies, it is known to be less efficient and difficult to analyze.",1. Introduction,[0],[0]
"Therefore, classical control engineers employ system identification techniques to obtain parametric model descriptions of dynamical systems from observation data, e.g., in the linear case ARX and ARMAX models.",1. Introduction,[0],[0]
"The identification focuses on model selection, i.e., finding the model structure and the corresponding set of parameters.",1. Introduction,[0],[0]
"But often this set of model candidates is difficult to find, especially for complex, possibly non-deterministic, systems (Ljung, 1998).",1. Introduction,[0],[0]
"Therefore, the need for data-driven models has emerged recently as control engineering is increasingly applied in areas without analytic description of the dynamical system.",1. Introduction,[0],[0]
"We consider the following two application scenarios: First, assume a set of trajectories for a
1Chair of Information-oriented Control, Technical University of Munich, Munich, Germany.",1. Introduction,[0],[0]
"Correspondence to: Jonas Umlauft <jonas.umlauft@tum.de>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"robotic task is given through human demonstrations, e.g., object grasping.",1. Introduction,[0],[0]
The goal is to represent the motion with a dynamical system.,1. Introduction,[0],[0]
"To ensure the reproduction terminates at the desired final point (object to grasp), we introduce the stability constraint.",1. Introduction,[0],[0]
"Second, consider a dynamical system which is known to be stable, e.g., a pendulum which rests in hanging position.",1. Introduction,[0],[0]
"The goal is to identify the dynamics precisely without further physical insights.
",1. Introduction,[0],[0]
"Bayesian non-parametric methods, more particularly Gaussian Processes (GPs) where successfully employed by Kocijan et al. (2005) and Wang et al. (2005) for system identification.",1. Introduction,[0],[0]
"Other approaches focus on learning switching linear systems (Fox et al., 2009) or employ an EM algorithm (Ghahramani & Roweis, 1999) for nonlinear systems.",1. Introduction,[0],[0]
"However, these approaches neglect the prior assumption that the dynamical system is stable, which becomes crucial when the learned model is used as a generative process such as in movement generation for robotics (Ijspeert et al., 2002).",1. Introduction,[0],[0]
"If stability is not considered during learning, the identified model suffers from spurious attractors which are not part of the true dynamics or instability.
",1. Introduction,[0],[0]
Only little work has merged the extensive knowledge on stability theory from control engineering with the powerful data-driven approaches for system identification:,1. Introduction,[0],[0]
For example Boots et al. (2008) and Chiuso & Pillonetto (2010) take stability constraints for learning dynamical systems into account but are limited to linear systems.,1. Introduction,[0],[0]
The work by Khansari-Zadeh & Billard (2011) ensures stability of the system by constraining the optimization of a Gaussian Mixture Model (GMM) to stability conditions derived from Lyapunov methods.,1. Introduction,[0],[0]
"The work by Paraschos et al. (2013) relies on a phase variable to ensure stability, which makes the approach time-dependent and therefore less robust.",1. Introduction,[0],[0]
Control Lyapunov functions are used by KhansariZadeh & Billard (2014) to ensure global stability for the learned system.,1. Introduction,[0],[0]
"These approaches partially employ probabilistic models (GP, GMM), but limit the analysis to the deterministic part by only considering the mean regression.",1. Introduction,[0],[0]
"By discarding the true underlying probability distribution, information regarding reliability of the model provided by the data is lost.",1. Introduction,[0],[0]
"This leads to overconfident conclusions regarding performance or safety on the real system.
",1. Introduction,[0],[0]
"Therefore, this work proposes a framework for learning
probabilistic nonlinear dynamical systems from observation, which takes the prior assumption of stability into account.",1. Introduction,[0],[0]
The required stochastic stability conditions of the discrete-time Markov processes are derived from Lyapunov theory.,1. Introduction,[0],[0]
We provide simulation results to validate the proposed approach and compare it to previously mentioned methods for identifying dynamical systems.,1. Introduction,[0],[0]
"We consider an autonomous, dynamical, discrete-time system with continuous-valued state xk ∈ X = Rd.",2. Problem Formulation,[0],[0]
"The state evolves according to an unknown stochastic process1
xk+1 = f̂(xk, ω̂k), (1)
with initial value x0 ∈ X and ω̂k is a random variable from the probability space (Ω,F ,P) with sample space Ω, the corresponding σ-algebra F and the probability measure P .",2. Problem Formulation,[0],[0]
"Since xk ∈ X is fixed at each step, (1) describes a state dependent distribution overxk+1.",2. Problem Formulation,[0],[0]
"A realization of ω̂k ∈ Ω, is drawn at every time step, yielding a realization of the next step.",2. Problem Formulation,[0],[0]
"As the distribution for xk+1 only depends on the state at time step k, f̂ is a Markov process, denoted by {xk}.",2. Problem Formulation,[0],[0]
"We assume that consecutive measurements of the state are available, thus N data pairs are given in the trainings set D = {x̄n, x̄n+1}Nn=1.",2. Problem Formulation,[0],[0]
"Based on these measurements, we model the unknown dynamics f̂ including the distribution ω̂k using the prior knowledge, that the stochastic process (1) converges to the origin xk = 0.",2. Problem Formulation,[0],[0]
The model consists of the mapping fψ and a encoding of the random variable ω defined by a finite parameter vector ψ ∈ Ψ.,2. Problem Formulation,[0],[0]
"As the model fψ must best possibly explain the data D, the problem is formulated as constrained likelihood maximization
ψ∗ = arg max ψ N∑ n=1 logP ( x̄n+1|x̄n,fψ ) , (2a) s.t. {xk} converges to the origin for k →∞. (2b)
",2. Problem Formulation,[0],[0]
"As different stochastic stability concepts exist, the convergence in (2b) is defined as convergence with probability one (w.p.1) (Kushner, 1971):
Definition 1 (Convergence w.p.1).",2. Problem Formulation,[0],[0]
"{xk} converges to the origin w.p.1 if, for each > 0, ‖xk‖ ≥ only finitely often.
",2. Problem Formulation,[0],[0]
1Notation:,2. Problem Formulation,[0],[0]
"Bold symbols denote vectors or multivariate functions, capital letters matrices and Ip the p × p identity matrix.",2. Problem Formulation,[0],[0]
"A 0 denotes positive definiteness of the matrix A, E",2. Problem Formulation,[0],[0]
"[·] the expected value, V [·] the variance of a random variable and C",2. Problem Formulation,[0],[0]
"[·, ·] the covariance between two random variables, where C [a] = C",2. Problem Formulation,[0],[0]
"[a,a].",2. Problem Formulation,[0],[0]
X̄ denotes a realization of the random variable X .,2. Problem Formulation,[0],[0]
"Imitating Matlab indexing, A(:,i) denotes the i-th column, A(j,:) the j-th row and A(1:2,i) the first and second element in the i-the column of A.",2. Problem Formulation,[0],[0]
"The i-th entry of the vector xk is denoted xk,i.
This also implies the following type of convergence, which might be more intuitive to the reader.",2. Problem Formulation,[0],[0]
Definition 2 (Convergence in probability).,2. Problem Formulation,[0],[0]
"The chain {xk} converges to the origin in probability if P(‖xk‖ ≥ )→ 0, for each > 0.
",2. Problem Formulation,[0],[0]
"We do not consider any control input here, thus the identification takes place for the closed-loop system for a existing controller or an uncontrolled system.",2. Problem Formulation,[0],[0]
"Consider the state-dependent coefficient form of fψ
xk+1 = A(xk)xk, (3)
where, for a fixedxk,A is a random variable from the probability space (ΩA,FA,PA) with the sample space ΩA ⊆ Rd×d.",3.1. The Model,[0],[0]
"The probability density function of A is specified by the vector θ ∈ Θ, which is state dependent through θψ : X →",3.1. The Model,[0],[0]
Θ. This mapping is itself parametrized by a vector ψ.,3.1. The Model,[0],[0]
"At each step, a realization of A, denoted by Ā, is drawn and multiplied by the state xk to proceed by one step.",3.1. The Model,[0],[0]
"This is visualized in Figure 1 along with the two-layer model structure: The first layer maps current state xk ∈ X onto the parameter θ ∈ Θ, denoted by θψ : X",3.1. The Model,[0],[0]
→ Θ. The mapping is parametrized by ψ.,3.1. The Model,[0],[0]
"The second layer is the probability distribution onAwhich assigns to each element in the sample space ΩA a probability based on θ.
To illustrate this multilayer design, we give a brief example in the scalar case d = 1: Assume A(xk) follows, for a given xk, a Gaussian distribution A ∼ N (µ,σ).",3.1. The Model,[0],[0]
"Therefore, the parameter vector is θN =",3.1. The Model,[0],[0]
"[µ σ]ᵀ with µ ∈ R,σ ∈ R+, thus ΘN ⊂",3.1. The Model,[0],[0]
R× R+.,3.1. The Model,[0],[0]
"The dependency of these parameters on the current state xk is expressed in θ
ψ N , e.g.,[
µ(xk) σ(xk)
] = θψN (xk) =",3.1. The Model,[0],[0]
"[ wxk zx2k ] , (4)
where linear dependency of the mean on the state and a quadratic relation between variance and the state is assumed.",3.1. The Model,[0],[0]
The parameters defining θψN here are ψ =,3.1. The Model,[0],[0]
"[w z]
ᵀ. Generally, the first layer θψ : X → Θ can be any state of the art parametric regression method which is parametrized by ψ.",3.1. The Model,[0],[0]
"For layer two, any probability distribution with a fixed set of parameters is applicable for A.
Leaving the stochastic aspect aside, model (3) is the statedependent coefficient (SDC) form which is reached by factorizing a nonlinear system into a linear-like structure.",3.1. The Model,[0],[0]
"It was shown, that for a any continuous differentiable function f with f(0) = 0, their exists a matrix-valued function A(x) such that f(x) = A(x)x, see Cimen (2008).",3.1. The Model,[0],[0]
"Thus, the SDC form is not limiting the expressive power of our model.",3.1. The Model,[0],[0]
"It also reflects the setup of many real-world system, e.g., consider an actuator whose output is generally noisy and the magnitude of the noise is dependent on the temperature.",3.1. The Model,[0],[0]
"By modeling the temperature as a state, the model (3) allows to capture this varying precision of the actuator.
",3.1. The Model,[0],[0]
The structure of the model (3) combines two important criteria.,3.1. The Model,[0],[0]
"First, it provides more flexibility than a linear system with random parameters, so it encodes also nonlinear dynamics.",3.1. The Model,[0],[0]
"Second, it is simple enough to allow a quadratic Lyapunov function analysis and therefore the derivation of analytic constraints for convergence as needed for the optimization in (2).",3.1. The Model,[0],[0]
"For approaching the problem as formulated in Section 2, an analytic condition for the constraint in the optimization problem (2b), given that fψ is of the form (3), is needed.",3.2. Stability Analysis,[0],[0]
The literature on stability criteria for dynamical systems is very rich and for nonlinear systems,3.2. Stability Analysis,[0],[0]
Lyapunov type methods are often used.,3.2. Stability Analysis,[0],[0]
"They are based on the following idea: If there is a function representing the ”energy” in the system (called Lyapunov function) which constantly decreases over time, the state will converge to a ”zero energy” state, the origin.",3.2. Stability Analysis,[0],[0]
"More precise, the Lyapunov function must be positive definite and it must be strictly decreasing over time, except in the origin.",3.2. Stability Analysis,[0],[0]
"Using the stochastic discretetime version of Lyapunov methods and the Borel-Cantelli Lemma leads to the following conditions for exponential stability (which implies convergence w.p.1 as defined in Definition 1)
Theorem 1 (Exponential Stability, (Kushner, 1971)).",3.2. Stability Analysis,[0],[0]
"Given a positive definite function V (xk) ≥ 0 for which
E [V (xk+1)|xk]−V (xk) ≤ −αV (xk), ∀xk ∈ X \ 0, (5)
for some α > 0 then
E [V (xk+m)|xk] ≤ (1− α)mV (xk) and (6) V (xk+m)→ 0 for m→∞ (w.p.1).",3.2. Stability Analysis,[0],[0]
"(7)
For the class of systems in (3) a quadratic function V (xk) is a proper Lyapunov function to derive sufficient stability constraints for arbitrary distributions on A as shown in the following proposition: Proposition 1 (Stability of the model (3)).",3.2. Stability Analysis,[0],[0]
Consider a stochastic process generated from (3) where in each step a realization of A is drawn from sample space ΩA ⊂ Rd×d.,3.2. Stability Analysis,[0],[0]
The process is globally exponentially stable at xk = 0 if there exists a P 0 such that E,3.2. Stability Analysis,[0],[0]
[Aᵀ(xk)]P E [A(xk)],3.2. Stability Analysis,[0],[0]
"+Q− (1− α)P 0, ∀xk ∈ X ,
(8)
for some α > 0, where Q is defined as Q(i,j)(xk) = ∑ l P(l,:) C [ A(:,i)(xk),A(l,j)(xk) ] , (9)
for any x0 ∈ X .
",3.2. Stability Analysis,[0],[0]
Proof.,3.2. Stability Analysis,[0],[0]
"Considering a quadratic Lyapunov function V (xk) = x ᵀ kPxk with P 0, the inequality from Theorem 1 in (5) is given as
E",3.2. Stability Analysis,[0],[0]
[ xᵀk+1Pxk+1|xk+1 ],3.2. Stability Analysis,[0],[0]
"−xᵀkPxk ≤ −αx ᵀ kPxk,
which yields for the stochastic2 process xk+1 =",3.2. Stability Analysis,[0],[0]
"A(xk)xk
xᵀk E",3.2. Stability Analysis,[0],[0]
[A ᵀ]P E,3.2. Stability Analysis,[0],[0]
[A]xk +,3.2. Stability Analysis,[0],[0]
"Tr (P C [Axk])− (10) − (1− α)xᵀkPxk ≤ 0, ∀xk ∈ X .",3.2. Stability Analysis,[0],[0]
"Now, an expression for the trace is derived as follows
Tr (P C",3.2. Stability Analysis,[0],[0]
[Axk]),3.2. Stability Analysis,[0],[0]
= Tr ( P C,3.2. Stability Analysis,[0],[0]
"[∑ i A(:,i)xk,i ])
=",3.2. Stability Analysis,[0],[0]
Tr P∑,3.2. Stability Analysis,[0],[0]
"i,j xk,ixk,j C",3.2. Stability Analysis,[0],[0]
"[ A(:,i),A(:,j) ]",3.2. Stability Analysis,[0],[0]
"= ∑ i,j,l P(l,:) C [ A(:,i),A(l,j) ] xk,ixk,j , = x ᵀ kQxk
where definition of Q in (9) was substituted.",3.2. Stability Analysis,[0],[0]
"Using this simplification, (10) is rewritten as
xᵀk
( ᵀ E",3.2. Stability Analysis,[0],[0]
[A]P E,3.2. Stability Analysis,[0],[0]
"[A] +Q− (1− α)P ) xk ≤ 0,
which must hold for ∀xk ∈ X .",3.2. Stability Analysis,[0],[0]
"To ensure this, the matrix E [A]ᵀ P E",3.2. Stability Analysis,[0],[0]
"[A] +Q− (1−α)P must be negative semidefinite, which concludes the proof.
2The xk dependency of the random process A as been dropped for notational convenience.
",3.2. Stability Analysis,[0],[0]
The interpretation of Proposition 1 is analogue to the linear deterministic case xk+1 = Axk which is stable if there exists a matrix P for which AᵀPA − P ≺ 0:,3.2. Stability Analysis,[0],[0]
"In the nonlinear case in (3) the negative definiteness must be fulfilled for A(xk), ∀xk ∈ X .",3.2. Stability Analysis,[0],[0]
"The probabilistic nature of the system (3) in addition requires ”a buffer”, which here is Q. The deterministic case is reconstructed if A has zero variance.",3.2. Stability Analysis,[0],[0]
"The scalar case, considered in the following remark, also allows an intuitive insight to the Proposition 1: There is a trade-off between the magnitude of the expected value and the variance of A as follows:
Remark 1.",3.2. Stability Analysis,[0],[0]
"In the scalar case3, i.e. d = 1 in (3), with Q = P V [A(xk)] condition (8) simplifies for any P > 0",3.2. Stability Analysis,[0],[0]
"to
E",3.2. Stability Analysis,[0],[0]
"[A(xk)]2 + V [A(xk)] ≤ 1− α, ∀xk ∈ X .",3.2. Stability Analysis,[0],[0]
(11),3.2. Stability Analysis,[0],[0]
"Our learning framework consists of three major steps:
1.",4. Stable Learning with Various Distributions,[0],[0]
Chose any probability distribution for the random variable A in (3) which is given by a fixed set of parameters θ ∈ Θ and whose first two moments are available.,4. Stable Learning with Various Distributions,[0],[0]
"It is assumed that subset Θ∗ ⊆ Θ for which (8) is fulfilled is non-empty, thus Θ∗ 6=",4. Stable Learning with Various Distributions,[0],[0]
"∅.
2.",4. Stable Learning with Various Distributions,[0],[0]
Chose any parametric regression method to represent the mapping θψ : X → Θ. The parameters of this mapping are denoted by ψ ∈ Ψ.,4. Stable Learning with Various Distributions,[0],[0]
"The set of all ψ for which all xk ∈ X map to Θ∗ is denoted by Ψ∗.
3.",4. Stable Learning with Various Distributions,[0],[0]
"The likelihood maximization under constraints
ψ∗ = arg max ψ∈Ψ∗ N∑ n=1",4. Stable Learning with Various Distributions,[0],[0]
"logP ( xn+1|xn,θψ ) , (12)
is solved, whereψ ∈ Ψ∗",4. Stable Learning with Various Distributions,[0],[0]
"is equivalent to constraint (8) with P 0 and α > 0.
",4. Stable Learning with Various Distributions,[0],[0]
The optimization (12) is a general constrained nonlinear program in a rather high dimensional space (depending on number of parameters of the regression method in step 2).,4. Stable Learning with Various Distributions,[0],[0]
"However, independent of the optimality, the model fψ∗ of the form (3) is exponentially stable, thus any sample path of the system converges.",4. Stable Learning with Various Distributions,[0],[0]
"For computational simplicity, we focus on two types of distribution which naturally fulfill the constraints as explained in the next sections.",4. Stable Learning with Various Distributions,[0],[0]
"For certain choices of distributions, constraint (8) is fulfilled for all possible parameter θ, thus Θ∗ = Θ, which
3Even though A,Q,P are scalars here, we keep them capitalized for notational consistency.
makes the optimization unconstrained.",4.1. Stability with Beta Distribution,[0],[0]
One example of such a distribution is the Beta distribution as given in the following corollary.,4.1. Stability with Beta Distribution,[0],[0]
Corollary 1.,4.1. Stability with Beta Distribution,[0],[0]
"The scalar system xk+1 = A(xk)xk where A(xk) = κ(Ã(xk)− η) with Beta distributed Ã(xk) ∼ B(a(xk), b(xk)) and κ = 2, η = 0.5 with state dependent parameters [a(xk) b(xk)]ᵀ = θ ψ B (xk), with any θψB : X → ΘB = R2+ is exponentially stable.
",4.1. Stability with Beta Distribution,[0],[0]
Proof.,4.1. Stability with Beta Distribution,[0],[0]
"Applying the affine transformation to mean and variance leads to4
E [A(xk)]",4.1. Stability with Beta Distribution,[0],[0]
= κ ( E [ Ã(xk) ],4.1. Stability with Beta Distribution,[0],[0]
−η ) = κ,4.1. Stability with Beta Distribution,[0],[0]
"( a a+ b −η ) ,
V [A(xk)] = κ2 V [ Ã(xk) ]",4.1. Stability with Beta Distribution,[0],[0]
"= κ2
ab
(a+ b)2(a+ b+ 1) .
",4.1. Stability with Beta Distribution,[0],[0]
"Condition (11) is rewritten to
(κ(E [A(xk)]−η))2 + κ2 V [A(xk)] ≤",4.1. Stability with Beta Distribution,[0],[0]
"1− α, ∀xk ∈ X , where the best possible choice for η minimizes (E",4.1. Stability with Beta Distribution,[0],[0]
"[A(xk)]−η)2, because it leaves the largest possible range for κ.",4.1. Stability with Beta Distribution,[0],[0]
"As E [A] is in the interval ]0, 1",4.1. Stability with Beta Distribution,[0],[0]
[ the minimization is achieved with the choice η = 12 .,4.1. Stability with Beta Distribution,[0],[0]
"Then, condition (11), divided by κ2 on both sides, evaluates to
a2
(a+ b)2 − a a+ b + 1 4 +
ab
(a+ b)2(a+ b+ 1) =
− ab (a+ b)(a+ b+ 1)︸ ︷︷ ︸
0... 14
+ 1 4 ≤ 1− α κ2 .
",4.1. Stability with Beta Distribution,[0],[0]
As α > 0 can be chosen arbitrarily small this condition holds for every |κ| ≤ 2.,4.1. Stability with Beta Distribution,[0],[0]
"Hence, according to Theorem 1 the system xk+1 = A(xk)xk is exponentially stable.
",4.1. Stability with Beta Distribution,[0],[0]
"To ensure maximal flexibility of the model, κ = 2 is set for further considerations.",4.1. Stability with Beta Distribution,[0],[0]
This leads to the conclusion that Θ∗B = ΘB = R2+.,4.1. Stability with Beta Distribution,[0],[0]
"Therefore, in the optimization, no constraints on ψ must be considered, thus Ψ = Ψ∗.",4.1. Stability with Beta Distribution,[0],[0]
ConstructingA from a Dirichlet distribution also allows for unconstrained optimization as it also leads to stable behavior as shown in the following corollary.,4.2. Stability with Dirichlet Distribution,[0],[0]
Corollary 2.,4.2. Stability with Dirichlet Distribution,[0],[0]
The d-dimensional system xk+1 =,4.2. Stability with Dirichlet Distribution,[0],[0]
"A(xk)xk where each row of A(xk) consists of the first d elements of a d+ 1 dimensional Dirichlet distributed vector, thus,
A(i,:) = a (i) (1:d), with a (i) ∼ D ( θ ψi D (xk) ) , ∀i = 1 . . .",4.2. Stability with Dirichlet Distribution,[0],[0]
"d,
with any θψiD : X → ΘD,∀i, is asymptotically stable w.p.1.",4.2. Stability with Dirichlet Distribution,[0],[0]
"4The state dependency of a, b is dropped for notational convenience.
",4.2. Stability with Dirichlet Distribution,[0],[0]
Proof.,4.2. Stability with Dirichlet Distribution,[0],[0]
"By construction the sample space ofA, ΩA contains only elements for which
A(i,j) > 0, A(i,j) < 1, ∀i, j = 1 . . .",4.2. Stability with Dirichlet Distribution,[0],[0]
"d and d∑
j=1
A(i,j) < 1, ∀i = 1 . . .",4.2. Stability with Dirichlet Distribution,[0],[0]
"d. (13)
Consider now a realization of A(xk) denoted by Ā and since the following statements hold for any realization in the sample space, we omit writing ∀Ā ∈ ΩA.",4.2. Stability with Dirichlet Distribution,[0],[0]
"It follows
d∑ j=1",4.2. Stability with Dirichlet Distribution,[0],[0]
"Ā(i,j) < 1 ∀i ⇒ max i=1",4.2. Stability with Dirichlet Distribution,[0],[0]
":d d∑ j=1 Ā(i,j) < 1
⇒ ‖Ā‖∞ = max i=1",4.2. Stability with Dirichlet Distribution,[0],[0]
":d d∑ j=1 |Ā(i,j)| < 1,
where the last inequality holds because all elements of Ā are strictly positive and ‖Ā‖∞ denotes the Maximum Absolute Row Sum Norm.",4.2. Stability with Dirichlet Distribution,[0],[0]
"Consider now M consecutive realizations Ā(i) with i = 1, . . .",4.2. Stability with Dirichlet Distribution,[0],[0]
",M .",4.2. Stability with Dirichlet Distribution,[0],[0]
"For the maximum norm of state in the M -th step holds
‖xk+M‖∞ = ∥∥∥∥∥ M∏",4.2. Stability with Dirichlet Distribution,[0],[0]
m=1 Ā(m)xk ∥∥∥∥∥ ∞ ≤ M∏,4.2. Stability with Dirichlet Distribution,[0],[0]
m=1,4.2. Stability with Dirichlet Distribution,[0],[0]
"∥∥∥Ā(m)∥∥∥ ∞ ‖xk‖∞
≤ (
max m ∥∥∥Ā(m)∥∥∥ ∞ )",4.2. Stability with Dirichlet Distribution,[0],[0]
"M ‖xk‖∞ M→∞−−−−→ 0,
where the submultiplicativity property of induced matrices (Horn & Johnson, 2013) is used.",4.2. Stability with Dirichlet Distribution,[0],[0]
"As convergence towards the origin holds for each element in the sample space, the system is stable with probability one.",4.2. Stability with Dirichlet Distribution,[0],[0]
"Therefore, the parameter space is unrestricted Θ∗D = ΘD = R d+1",4.2. Stability with Dirichlet Distribution,[0],[0]
"+ .
",4.2. Stability with Dirichlet Distribution,[0],[0]
Remark 2.,4.2. Stability with Dirichlet Distribution,[0],[0]
Note that this approach only allows to represent the special class of positive systems.,4.2. Stability with Dirichlet Distribution,[0],[0]
"Nevertheless, positive systems play an important role in control engineering for modeling the evolution of strictly positive quantities as shown in (Farina & Rinaldi, 2011).
",4.2. Stability with Dirichlet Distribution,[0],[0]
Remark 3.,4.2. Stability with Dirichlet Distribution,[0],[0]
"An affine transformation, as shown for Beta distribution is not possible here because absolute values are taken in the row sum.",4.2. Stability with Dirichlet Distribution,[0],[0]
"Therefore, from
∑d j=1 |Ā(i,j)| < 1 one cannot conclude∑d
j=1 |κ(Ā(i,j) − 0.5)| < 1 for any κ > 1.",4.2. Stability with Dirichlet Distribution,[0],[0]
"We validate our approach, labeled LeSSS (for Learning Stable Stochastic Systems), using synthetic and human motion data and the simulation of a chemical reactor.",5.1. Setup,[0],[0]
"For the Beta distribution, Gaussian Mixture Regression (GMR)
is used for the mapping from the state to the parameters θB : X → ΘB.",5.1. Setup,[0],[0]
"Thus, the parameter vector ψ, is the concatenation of the prior πl, the meansµl and the covariances Σl for l = 1, . . .",5.1. Setup,[0],[0]
L .,5.1. Setup,[0],[0]
The code (based on Calinon (2009)) includes k-means clustering initialization and a transformation of Σl and πl to make it an unconstrained optimization.,5.1. Setup,[0],[0]
"To evaluate the likelihood function for each training point {x̄n+1, x̄n}, the Beta distribution parameters are computed [an bn]
ᵀ = θψB (x̄n) using GMR.",5.1. Setup,[0],[0]
"Then, the log likelihood of Ān = x̄n+1/x̄n given the parameters [an bn] is evaluated using the density function of the Beta distribution.",5.1. Setup,[0],[0]
As all possible parameter θB,5.1. Setup,[0],[0]
"= [a b]ᵀ ∈ R2+ lead to stability, finding ψ∗ is an unconstrained optimization problem.
",5.1. Setup,[0],[0]
"For the Dirichlet distribution, the mapping from the state to the parameters θψD : X → ΘD uses a nearest neighbor approach for computational simplicity.",5.1. Setup,[0],[0]
The 2d = 4 closest data points are considered for fitting the training parameters of the Dirichlet distribution locally.,5.1. Setup,[0],[0]
Then a training point is placed at the center of these four points.,5.1. Setup,[0],[0]
"At reproduction, the closest such training point and its Dirichlet parameters are taken for regression.",5.1. Setup,[0],[0]
"This does not necessarily maximize the likelihood, but shows accurate results for reproduction.",5.1. Setup,[0],[0]
"We compare the following models from literature regarding reproduction precision and convergence properties:
•",5.1. Setup,[0],[0]
The approach introduced by Boots et al. (2008) learns stable linear dynamical system (stable LDS) from data.,5.1. Setup,[0],[0]
"It constraints the search of the deterministic dynamic matrix A to ensure the stability
of xk+1 = Axk.
",5.1. Setup,[0],[0]
"• Gaussian Process Dynamical Models (GPDM) (Wang et al., 2005) represent dynamical system in the general form xk+1 = f(xk), with Gaussian Process f ∼ GP(0, k(xk,x′k)).",5.1. Setup,[0],[0]
We employ a zero prior mean function and a squared exponential kernel.,5.1. Setup,[0],[0]
The hyperparameters of the kernel are optimized using the likelihood as described by Rasmussen & Williams (2006).,5.1. Setup,[0],[0]
"In reproduction, this method can either be used in deterministic setting by only taking the posterior mean prediction µGP(xk) thus xk+1 = µGP(xk) or the stochastic setting xk+1 ∼ N (µGP(xk), ΣGP(xk)), where ΣGP(xk) is the posterior variance.",5.1. Setup,[0],[0]
"GPDMs are bounded (Beckers & Hirche, 2016a;b) but not stable.
",5.1. Setup,[0],[0]
• The Stable Estimator of Dynamical Systems (SEDS) as introduced by Khansari-Zadeh & Billard (2011) constraints the likelihood optimization of GMR parameters to a class of mean stable dynamical timecontinuous systems.,5.1. Setup,[0],[0]
The GMR maps from current state x to the time derivative ẋ.,5.1. Setup,[0],[0]
"It focuses on deterministic systems by only considering stability criteria for the mean prediction of the GMR, µGMM(x), while ignoring the stochastic nature of GMMs, (its variance prediction ΣGMM).",5.1. Setup,[0],[0]
"We also run this method in a stochastic setting, where ẋ ∼ N (µGMM(x), ΣGMM(x)).",5.1. Setup,[0],[0]
"For our simulations, five mixtures are employed.
",5.1. Setup,[0],[0]
"Before starting the comparison to existing approaches, LeSSS is demonstrated on a synthetic dataset.",5.1. Setup,[0],[0]
"For the first simulation, the task is to identify the stable nonlinear stochastic system given by
xk+1 = A(xk)xk, (14) where A(xk) ∼ B ( (xk − 5)2, (xk + 5)2 ) .
",5.2. Simulation 1: Synthetic Data,[0],[0]
"The learning algorithm is given 100 training points {x̄n, x̄n+1}100n=1 equally spaced in the state space interval",5.2. Simulation 1: Synthetic Data,[0],[0]
"[−8, 8] which are drawn from the state dependent Beta distribution (14).",5.2. Simulation 1: Synthetic Data,[0],[0]
Here L = 3 was chosen for the number of mixtures in the GMR for the mapping θψB : X → ΘB.,5.2. Simulation 1: Synthetic Data,[0],[0]
Figure 2 compares the mean and variance of the original system (14) to the one inferred by our model.,5.2. Simulation 1: Synthetic Data,[0],[0]
It clearly shows that the model offers sufficient flexibility to reconstruct the original system.,5.2. Simulation 1: Synthetic Data,[0],[0]
"Note: It is also possible to verify the parameter functions a(xk), b(xk) as given in (14), but we directly look at the mean and variance functions as there exists a unique mapping and it is more intuitive for interpretation.",5.2. Simulation 1: Synthetic Data,[0],[0]
"It must be omitted, that the data was generated from the same model which the algorithm is learning.",5.2. Simulation 1: Synthetic Data,[0],[0]
"This
explains the good fitting, but is of course not often the case in practical application.",5.2. Simulation 1: Synthetic Data,[0],[0]
"Therefore, we continue with a real world dataset in the following.",5.2. Simulation 1: Synthetic Data,[0],[0]
"For the next simulation, we use the data set for lettershaped motions provided by Khansari-Zadeh & Billard (2011).",5.3. Simulation 2: Human Motion Data,[0],[0]
The 225 trainings points of 3 trajectories of the two dimensional Z-shaped motion are projected on the y-axis.,5.3. Simulation 2: Human Motion Data,[0],[0]
The GMR for θψB : X → ΘB is trained with two mixtures.,5.3. Simulation 2: Human Motion Data,[0],[0]
Figure 3 shows the training data along with the fit of the mean and variance functions.,5.3. Simulation 2: Human Motion Data,[0],[0]
The mean function shows a smoothed estimate of the training data.,5.3. Simulation 2: Human Motion Data,[0],[0]
"The model identifies properly that the training data has higher variability (around xk = 0) and captures this in its variance function.
",5.3. Simulation 2: Human Motion Data,[0],[0]
"Figure 4 compares the reproduction of the models stable LDS, GPDM, SEDS and LeSSS if taking the deterministic (mean) output of each model (all starting from the same
initial point).",5.3. Simulation 2: Human Motion Data,[0],[0]
"The stable LDS approach leads to a converging trajectory, but fails to capture the complexity of the dynamic (as the true dynamic is nonlinear).",5.3. Simulation 2: Human Motion Data,[0],[0]
The GPDM converges to a spurious attractor at x ≈ −9.3 which is undesired but not surprising.,5.3. Simulation 2: Human Motion Data,[0],[0]
SEDS and LeSSS both lead to asymptotic stable reproductions of the movement.,5.3. Simulation 2: Human Motion Data,[0],[0]
"Since the data does not contain the full state (due to the projection on the y-axis), it is not possible to reproduce the movement precisely with a dynamical system model.
",5.3. Simulation 2: Human Motion Data,[0],[0]
"Figure 5 compares the reproduction of the three stochastic dynamical models GPDM, SEDS and LeSSS based on three sample paths drawn from each model.",5.3. Simulation 2: Human Motion Data,[0],[0]
The GPDM again converges to the spurious attractor.,5.3. Simulation 2: Human Motion Data,[0],[0]
"SEDS clearly shows that convergence of the mean is not sufficient for converging trajectories of a stochastic system, as the drawn sample paths are strongly oscillating around the origin without tendency to converge.",5.3. Simulation 2: Human Motion Data,[0],[0]
"In the stochastic case only LeSSS generates converging trajectories.
",5.3. Simulation 2: Human Motion Data,[0],[0]
Figure 6 shows an example for the human motion imitation in the 2D case on a different training data set.,5.3. Simulation 2: Human Motion Data,[0],[0]
"It shows the deterministic trajectory and 5 sample path realizations, where all of them show high reproduction precision and convergence to the orign.",5.3. Simulation 2: Human Motion Data,[0],[0]
"For the last validation, we utilize simulated data from a simplified chemical reactor (Einarsson, 1998).",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
The closedloop reactor is modeled by a piecewise affine system with two states: the fluid level x1 and the temperature x2.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"Both states are physically positive quantities, therefore the approach in Section 4.2 is suitable.",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"The switching between different dynamic matrices is state dependent and occurs at x1 = 3 and x2 = 50, which corresponds to a discrete change of the control inputs.",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"The training data consists of 8 trajectories of 15 steps each, which are pairwise initialized at the 4 different regions of the dynamics and perturbed
with white noise with σ = 0.01 for both states.
",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"Figure 7 shows the training data along with the reproduction using stable LDS, GPDM, SEDS and LeSSS in the deterministic setting.",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
The initial points in the test case were set close to the one in the training data.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
The stable LDS is not capable to capture the varying behavior in the different regions of the piecewise affine system and therefore fails in accuracy of the reproduction.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"GPDM leads again to convergence outside the origin, which is undesirable.",5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
SEDS and LeSSS are both converging as it is enforced by design.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
Figure 8 shows that similar to the 1D case GPDM and SEDS fail to converge in the stochastic case while LeSSS is stable in all sample paths.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
Table 1 compares the methods with regard to the reproduction precision quantitatively.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
It shows that LeSSS outperforms other methods in this measure while providing the necessary guarantees regarding convergence.,5.4. Simulation 3: Chemical Reactor Simulation,[0],[0]
"The simulations show that LeSSS is powerful enough to represent various nonlinear dynamics, while capturing the probabilistic nature of the process.",5.5. Discussion,[0],[0]
"The incorporation of the prior knowledge on goal convergence ensures that the learned model is stable in probability.
",5.5. Discussion,[0],[0]
"The computational complexity for learning the parameters of the model using interior-point methods, is mainly determined by the employed mapping in the first layer θψ .",5.5. Discussion,[0],[0]
"The
computation times on a i5 CPU 2.30GHz, 2 Cores and 8GB RAM are given for Simulation 2 and 3 in Table 2.",5.5. Discussion,[0],[0]
"Since the GPDM, SEDS and LeSSS all solve non-convex optimization problems, their commutation times are in the same order of magnitude.",5.5. Discussion,[0],[0]
The linear model has advantage here.,5.5. Discussion,[0],[0]
"Regarding the scalability with more training points, the parameter fitting performs similarly to other approaches requiring likelihood computation since this is the major factor.",5.5. Discussion,[0],[0]
"However, the scalability strongly depends on the employed distribution and the mapping in the first layer θψ .
",5.5. Discussion,[0],[0]
"This work only deals with system with a single equilibrium point, but could be extended to system with more complex attractor dynamics.",5.5. Discussion,[0],[0]
"However, further knowledge is required because - in addition to the position of all equilibrium points - their regions of attraction must be known.",5.5. Discussion,[0],[0]
This work proposes a framework for learning nonlinear stable stochastic dynamical systems from data.,6. Conclusion,[0],[0]
"We introduce a flexible model, which builds on the state-dependent coefficient form and derive exponential stability conditions based on stochastic Lyapunov methods.",6. Conclusion,[0],[0]
"The criteria is applicable to various probability distributions, while we focus to investigate the application to Beta and Dirichlet distributions.",6. Conclusion,[0],[0]
Simulation results verify sufficient flexibility of the model and the correct identification of the system’s uncertainty.,6. Conclusion,[0],[0]
In comparison to existing approaches it showed advantages in reproduction precision and convergence properties on human motion data and simulated data from a real system.,6. Conclusion,[0],[0]
The research leading to these results has received funding from the European Research Council under the European Union Seventh Framework Program (FP7/2007-2013),Acknowledgment,[0],[0]
ERC Starting Grant ”Control based on Human Models (conhumo)” agreement number 337654.,Acknowledgment,[0],[0]
We also would like to thank the reviewers for very constructive feedback.,Acknowledgment,[0],[0]
"A data-driven identification of dynamical systems requiring only minimal prior knowledge is promising whenever no analytically derived model structure is available, e.g., from first principles in physics.",abstractText,[0],[0]
"However, meta-knowledge on the system’s behavior is often given and should be exploited: Stability as fundamental property is essential when the model is used for controller design or movement generation.",abstractText,[0],[0]
"Therefore, this paper proposes a framework for learning stable stochastic systems from data.",abstractText,[0],[0]
We focus on identifying a state-dependent coefficient form of the nonlinear stochastic model which is globally asymptotically stable according to probabilistic Lyapunov methods.,abstractText,[0],[0]
We compare our approach to other state of the art methods on real-world datasets in terms of flexibility and stability.,abstractText,[0],[0]
Learning Stable Stochastic Nonlinear Dynamical Systems,title,[0],[0]
Kernel-based methods are a staple machine learning approach in Natural Language Processing (NLP).,1 Introduction,[0],[0]
"Frequentist kernel methods like the Support Vector Machine (SVM) pushed the state of the art in many NLP tasks, especially classification and regression.",1 Introduction,[0],[0]
"One interesting aspect of kernels is their ability to be defined directly on structured objects like strings, trees and graphs.",1 Introduction,[0],[0]
This approach has the potential to move the modelling effort from feature engineering to kernel engineering.,1 Introduction,[0],[0]
"This is useful when we do not have much prior knowledge about how the data
behaves, as we can more readily define a similarity metric between inputs instead of trying to characterize which features are the best for the task at hand.
",1 Introduction,[0],[0]
Kernels are a very flexible framework: they can be combined and parameterized in many different ways.,1 Introduction,[0],[0]
"Complex kernels, however, lead to the problem of model selection, where the aim is to obtain the best kernel configuration in terms of hyperparameter values.",1 Introduction,[0],[0]
The usual approach for model selection in frequentist methods is to employ grid search on some development data disjoint from the training data.,1 Introduction,[0],[0]
This approach can rapidly become impractical when using complex kernels which increase the number of model hyperparameters.,1 Introduction,[0],[0]
"Grid search also requires the user to explicitly set the grid values, making it difficult to fine tune the hyperparameters.",1 Introduction,[0],[0]
"Recent advances in model selection tackle some of these issues, but have several limitations (see §6 for details).
",1 Introduction,[0],[0]
"Our proposed approach for model selection relies on Gaussian Processes (GPs) (Rasmussen and Williams, 2006), a widely used Bayesian kernel machine.",1 Introduction,[0],[0]
"GPs allow efficient and fine-grained model selection by maximizing the evidence on the training data using gradient-based methods, dropping the requirement for development data.",1 Introduction,[0],[0]
"As a Bayesian procedure, GPs also naturally balance between model capacity and generalization.",1 Introduction,[0],[0]
"GPs have been shown to achieve state of the art performance in various regression tasks (Hensman et al., 2013; Cohn and Specia, 2013).",1 Introduction,[0],[0]
"Therefore, we base our approach on this framework.
",1 Introduction,[0],[0]
"While prediction performance is important to consider (as we show in our experiments), we are
461
Transactions of the Association for Computational Linguistics, vol. 3, pp.",1 Introduction,[0],[0]
"461–473, 2015.",1 Introduction,[0],[0]
Action Editor: Stefan Riezler.,1 Introduction,[0],[0]
"Submission batch: 2/2015; Revision batch 7/2015; Published 8/2015.
",1 Introduction,[0],[0]
c©2015 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
mainly interested in two other significant aspects that are enabled by our approach:
• Gradient-based methods are more efficient than grid search for high dimensional spaces.",1 Introduction,[0],[0]
"This allows us to easily propose new rich kernel extensions that rely on a large number of hyperparameters, which in turn can result in better modelling capacity.
",1 Introduction,[0],[0]
"• Since the model selection process is now finegrained, we can interpret the resulting hyperparameter values, depending on how the kernel is defined.
",1 Introduction,[0],[0]
"In this work we focus on tree kernels, which have been successfully used in a number of NLP tasks (see §6).",1 Introduction,[0],[0]
"In most cases, these kernels are used as an SVM component and model selection is not considered an important issue.",1 Introduction,[0],[0]
"Hyperparameters are usually set to default values, which work reasonably well in terms of prediction performance.",1 Introduction,[0],[0]
"However, this is only possible due to the small number of hyperparameters these kernels contain.
",1 Introduction,[0],[0]
We perform experiments comprising synthetic data (§4) and two real NLP regression tasks: Emotion Analysis (§5.1) and Translation Quality Estimation (§5.2).,1 Introduction,[0],[0]
Our findings show that our approach outperforms SVMs using the same kernels.,1 Introduction,[0],[0]
Our definition of GPs closely follows that of Rasmussen and Williams (2006).,2 Gaussian Process Regression,[0],[0]
"Consider a setting where we have a dataset X = {(x1, y1), (x2, y2), . . .",2 Gaussian Process Regression,[0],[0]
", (xn, yn)}, where xi is a ddimensional input and yi the corresponding output.",2 Gaussian Process Regression,[0],[0]
Our goal is to infer an underlying function,2 Gaussian Process Regression,[0],[0]
"f : Rd → R to explain this data, i.e. f(xi)",2 Gaussian Process Regression,[0],[0]
≈ yi.,2 Gaussian Process Regression,[0],[0]
"Formally, f is drawn from a GP prior,
f(x) ∼ GP(µ(x), k(x,x′)),
where µ(x) is the mean function, which is usually the 0 constant, and k(x,x′) is the kernel function.
",2 Gaussian Process Regression,[0],[0]
"In a regression setting, we assume that the response variables are noisy latent function evaluations, i.e., yi = f(xi) + η, where η ∼ N (0, σ2n) is added white noise.",2 Gaussian Process Regression,[0],[0]
"We assume a Gaussian likelihood, which allows us to obtain a closed formula
solution for the posterior, namely
y∗ ∼ N (k∗(K + σnI)−1yT , k(x∗,x∗)− kT∗ (K + σnI)−1k∗),
where x∗ and y∗ are respectively the test input and its response variable, K is the Gram matrix corresponding to the training inputs and k∗ =",2 Gaussian Process Regression,[0],[0]
"[〈x1,x∗〉, 〈x2,x∗〉, . . .",2 Gaussian Process Regression,[0],[0]
", 〈xn,x∗〉] is the vector of kernel evaluations between the test input and each training input.
",2 Gaussian Process Regression,[0],[0]
A key property of GP models is their ability to perform efficient model selection.,2 Gaussian Process Regression,[0],[0]
"This is achieved by employing gradient-based methods to maximize the marginal likelihood,
p(y|X,θ) = ∫ p(y|X,θ, f)p(f)df,
where θ represents the vector of model hyperparameters and y is the vector of response variables from the training data.",2 Gaussian Process Regression,[0],[0]
"For a Gaussian likelihood, we can take the log of the expression above to obtain in closed-form1,
log p(y|X,θ) =
−1 2 yTG−1y
︸ ︷︷ ︸ data fit
−1 2
log |G| ︸",2 Gaussian Process Regression,[0],[0]
"︷︷ ︸
complexity penalty
−n 2
log 2π ︸ ︷︷ ︸
constant
where G = K+σnI.",2 Gaussian Process Regression,[0],[0]
"The data fit term is dependent on the training response variables, while the complexity penalty term relies only on the kernel and training inputs.",2 Gaussian Process Regression,[0],[0]
"Since the first two terms have conflicting objectives, optimizing the log marginal likelihood will naturally achieve a compromise and thus limit overfitting (without the need for any validation step or additional data).
",2 Gaussian Process Regression,[0],[0]
To enable gradient-based optimization we need to derive the gradients w.r.t.,2 Gaussian Process Regression,[0],[0]
"the hyperparameters:
∂ ∂θj log p(y|X,θ) =1 2 yTG−1 ∂G ∂θj G−1y
− 1 2
trace ( G−1 ∂G
∂θj
) .
1See Rasmussen and Williams (2006, pp. 113-114) for details on the derivation of this formula and also its correspondent gradient calculation.
",2 Gaussian Process Regression,[0],[0]
The gradients of G depend on the underlying kernel.,2 Gaussian Process Regression,[0],[0]
Therefore we can employ any kind of valid kernel in this procedure as long as its gradients can be computed.,2 Gaussian Process Regression,[0],[0]
This not only allows for fine-tuning of hyperparameters but also allows for kernel extensions which are richly parameterized.,2 Gaussian Process Regression,[0],[0]
The seminal work on Convolution Kernels by Haussler (1999) defines a broad class of kernels on discrete structures by counting and weighting the number of substructures they share.,3 Tree Kernels,[0],[0]
"Applying Haussler’s formulation to trees we reach a general formula for a tree kernel between two trees t1 and t2, namely
k(t1, t2) = ∑
f∈F w(f)c1(f)c2(f), (1)
where F is the set of all tree fragments, c1(f) and c2(f) return the counts for fragment f in trees t1 and t2, respectively, and w(f) assigns a weight to fragment f .",3 Tree Kernels,[0],[0]
"In other words, we can consider the kernel a weighted dot product over vectors of fragment counts.",3 Tree Kernels,[0],[0]
"The actual fragment set F is deliberately left undefined: different concepts of tree fragments define different tree kernels.
",3 Tree Kernels,[0],[0]
"In this paper, we will focus on Subset Tree Kernels (henceforth SSTK), first introduced by Collins and Duffy (2001).",3 Tree Kernels,[0],[0]
This kernel considers tree fragments that contains complete grammar rules (see Figure 1 for an example).,3 Tree Kernels,[0],[0]
Consider the set of nodes in the two trees as N1 and N2 respectively.,3 Tree Kernels,[0],[0]
We define Ii(n) as an indicator function that returns 1 if fragment fi ∈ F has root n and 0 otherwise.,3 Tree Kernels,[0],[0]
"A SSTK can then be defined as:
k(t1, t2) = ∑
n1∈N1
∑
n2∈N2 ∆(n1, n2) , (2)
where ∆(n1, n2) = |F|∑
i=1
λ s(i) 2 Ii(n1)Ii(n2)
and s(i) is the number of fragments in i with at least one child2.
",3 Tree Kernels,[0],[0]
"The formulation in Equation 2 is the same as the one shown in Equation 1, except that we are now restricting the weights w(f) to be a function of a
2See Pighin and Moschitti (2010) for details and a proof on this derivation.
",3 Tree Kernels,[0],[0]
hyperparameter λ.,3 Tree Kernels,[0],[0]
"The original goal of λ is to act as a decay factor that penalizes contributions from larger fragments cf smaller ones (and therefore, it should be in the [0, 1] interval).",3 Tree Kernels,[0],[0]
"Without this factor, the resulting distribution over tree pairs is skewed, giving extremely large values when trees are equal and rapidly decreasing for small differences over fragment counts.",3 Tree Kernels,[0],[0]
"The decay factor helps to spread this distribution, effectively giving smaller weights to larger fragments.
",3 Tree Kernels,[0],[0]
"The function ∆ can be defined recursively,
∆(n1, n2) =    0",3 Tree Kernels,[0],[0]
pr(n1) 6=,3 Tree Kernels,[0],[0]
"pr(n2) λ pr(n1) = pr(n2) ∧
preterm(n1)",3 Tree Kernels,[0],[0]
"λg(n1, n2)",3 Tree Kernels,[0],[0]
"otherwise,
where pr(n) is the grammar production at node n and preterm(n) returns true if n is a pre-terminal node.",3 Tree Kernels,[0],[0]
"The function g is defined as follows:
g(n1, n2)",3 Tree Kernels,[0],[0]
"=
|n1|∏
i=1
(α+ ∆(cin1 , c i n2)) , (3)
where |n| is the number of children of node n and cin is the i
th child of node n.",3 Tree Kernels,[0],[0]
"This recursive definition is calculated efficiently by employing dynamic programming to cache intermediate ∆ results.
",3 Tree Kernels,[0],[0]
"Equation 3 also adds another hyperparameter, α.",3 Tree Kernels,[0],[0]
This hyperparameter was introduced by Moschitti (2006b)3 as a way to select between two different tree kernels.,3 Tree Kernels,[0],[0]
"If α = 1, we get the original SSTK, if α = 0, then we obtain the Subtree Kernel, which only allows fragments with terminal symbols
3In his original formulation, this hyperparameter was named σ",3 Tree Kernels,[0],[0]
"but here we use α to not confuse it with the GP noise hyperparameter.
",3 Tree Kernels,[0],[0]
as leaves.,3 Tree Kernels,[0],[0]
"We can also interpret the Subtree Kernel as a “sparse” version of the SSTK, where the “nonsubtree” fragments have their weights equal to zero.
",3 Tree Kernels,[0],[0]
"Even though fragment weights are affected by both kernel hyperparameters, previous work did not discuss their effects.",3 Tree Kernels,[0],[0]
The usual procedure fixes α to 1 (selecting the original SSTK) and sets λ to a default value (around 0.4).,3 Tree Kernels,[0],[0]
"As explained in §2, the GP model selection procedure enables us to learn finegrained values for these hyperparameters, which can lead to better performing models and aid interpretation.",3 Tree Kernels,[0],[0]
"Furthermore, it also allows us to extend these kernels by adding new hyperparameters.",3 Tree Kernels,[0],[0]
We propose one such kernel in the next Section.,3 Tree Kernels,[0],[0]
"While varying the SSTK hyperparameters can lead to different weight schemes, they do that in a very coarse way.",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"For some applications, it may be necessary to give more weight to specific fragments or set of fragments (e.g., NPs being more important than ADVP in an information extraction setting).",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"The Symbol-aware Subset Tree Kernel (henceforth, SASSTK), which we introduce here, allows a more fine-grained control over the weights by employing one λ and one α hyperparameter for each non-terminal symbol in the training data.",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"The calculation uses a similar recursive formula to the SSTK, namely:
∆(n1, n2) =    0 pr(n1) 6= pr(n2) λx pr(n1) = pr(n2) ∧
preterm(n1) λxgx(n1, n2) otherwise,
where x is the symbol at node n1 and
gx(n1, n2) =
|n1|∏
i=1
(αx + ∆(c i n1 , c i n2)) .",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"(4)
The SASSTK can be interpreted as a generalization of the SSTK: we can recover the latter by tying all λ and setting all α = 1.",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"By employing different hyperparameter values for each specific symbol, we can effectively modify the weights of all fragments where the symbol appears.",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
"Table 1 shows an example where we unrolled a kernel computation into its corresponding feature space, showing the resulting weighted counts for each feature.",3.1 Symbol-aware Subset Tree Kernel,[0],[0]
To enable hyperparameter optimization via gradient descent we must provide gradients for the kernels.,3.2 Kernel Gradients,[0],[0]
"In this Section we derive the gradients for SASSTK.
",3.2 Kernel Gradients,[0],[0]
From Equation 2 we know that the kernel is a double summation over the ∆ function.,3.2 Kernel Gradients,[0],[0]
"Therefore all gradients are also double summations, but over the gradients of ∆. We can obtain these in a vectorized way, by considering the gradients of the hyperparameter vectors λ and α over ∆.",3.2 Kernel Gradients,[0],[0]
"Let k be the number of symbols considered in the model and λ and α be k-dimensional vectors containing the respective hyperparameters.
",3.2 Kernel Gradients,[0],[0]
"In the following, we use the notation ∆i as a shorthand for ∆(cin1 , c i n2) and we also omit the parameters of gx.",3.2 Kernel Gradients,[0],[0]
"We start with the λ gradient:
∂∆ ∂λ =    0",3.2 Kernel Gradients,[0],[0]
pr(n1),3.2 Kernel Gradients,[0],[0]
"6= pr(n2) u pr(n1) = pr(n2) ∧ preterm(n1) ∂(λxgx)
",3.2 Kernel Gradients,[0],[0]
"∂λ otherwise,
where x is the symbol at n1, gx is defined in Equation 4 and u is the k-dimensional unit vector with the element corresponding to symbol x equal to 1 and all others equal to 0.",3.2 Kernel Gradients,[0],[0]
"The gradient in the third case is defined recursively,
∂(λxgx)
",3.2 Kernel Gradients,[0],[0]
"∂λ = ugx + λx ∂gx ∂λ
= ugx + λx
|n1|∑
i=1
gx αx + ∆i ∂∆i",3.2 Kernel Gradients,[0],[0]
"∂λ .
",3.2 Kernel Gradients,[0],[0]
"The α gradient is derived in a similar way,
∂∆ ∂α =    0",3.2 Kernel Gradients,[0],[0]
"pr(n1) 6= pr(n2) ∨ preterm(n1)
∂(λxgx)",3.2 Kernel Gradients,[0],[0]
"∂α otherwise,
and the gradient at the second case is also defined recursively,
∂(λxgx)
",3.2 Kernel Gradients,[0],[0]
"∂α = λx ∂gx ∂α
= λx
|n1|∑
i=1
gx αx + ∆i
( u+
∂∆i ∂α
) .
",3.2 Kernel Gradients,[0],[0]
Gradients can be efficiently obtained using dynamic programming.,3.2 Kernel Gradients,[0],[0]
"In fact, they can be calculated at the same time as ∆ to improve performance since they all share many terms in their derivations.",3.2 Kernel Gradients,[0],[0]
"Finally, we can easily obtain the gradients for the original SSTK by letting u = 1.",3.2 Kernel Gradients,[0],[0]
It is common practice when using tree kernels to normalize the kernel.,3.3 Kernel Normalization,[0],[0]
This helps reduce the random effect of tree size.,3.3 Kernel Normalization,[0],[0]
"Normalization can be achieved using the following, where k̂ is the normalized kernel:
k̂(t1, t2) = k(t1, t2)√
k(t1, t1)k(t2, t2) .
",3.3 Kernel Normalization,[0],[0]
To apply this normalized version in the optimization procedure we must also derive gradients for the normalization function.,3.3 Kernel Normalization,[0],[0]
"In the following equation, we use kij and k̂ij as a shorthand for k(ti, tj) and k̂(ti, tj), respectively:
∂k̂12 ∂θ = ∂k12 ∂θ√ k11k22",3.3 Kernel Normalization,[0],[0]
− k̂12 ∂k11 ∂θ k22 + k11 ∂k22 ∂θ 2k11k22 .,3.3 Kernel Normalization,[0],[0]
Many other structural kernels rely on recursive definitions and dynamic programming to perform their calculations.,3.4 Other Extensions,[0],[0]
"Examples include other tree kernels like the Partial Tree Kernel (Moschitti, 2006a) and string kernels like the ones defined on character ngrams (Lodhi et al., 2002) or word sequences (Cancedda et al., 2003).",3.4 Other Extensions,[0],[0]
"While in this paper we focus
on the SSTK (and our proposed SASSTK), our approach can easily be extended to these other kernels, as long as all the corresponding recursive definitions are differentiable.",3.4 Other Extensions,[0],[0]
A natural question that arises in the proposed method is how much data is needed to accurately learn the kernel hyperparameters.,4 Synthetic Data Experiments,[0],[0]
"To answer this question, we run a set of experiments using synthetic data.",4 Synthetic Data Experiments,[0],[0]
"We generate this data by using a set of 1000 natural language syntactic trees, where we fix a random subset of 200 instances for testing and use the remaining 800 instances as training.",4 Synthetic Data Experiments,[0],[0]
"For each training set size we define a GP over the full dataset, sample a function from it and use the function output as the response variable for each tree.",4 Synthetic Data Experiments,[0],[0]
"We try two different GP priors, one using the SSTK and another one using the SASSTK.
",4 Synthetic Data Experiments,[0],[0]
The conditions above provide a controlled environment to check the modelling capacities of our approach since we know the exact distribution where the data comes from.,4 Synthetic Data Experiments,[0],[0]
"The reasoning behind these experiments is that to be able to provide benefits in real tasks, where the data distribution is not known, our models have to be learnable in this controlled setting as well using a reasonable amount of data.
",4 Synthetic Data Experiments,[0],[0]
"Finally, we also provide an empirical evaluation comparing the speed performance between our approach and grid search.",4 Synthetic Data Experiments,[0],[0]
"Our first experiments use a SSTK as the kernel with λ = 0.001, α = 1 and σ2n = 0.01.",4.1 SSTK Prior,[0],[0]
"After obtaining the input trees and their sampled labels, we define a new GP model using only the training data plus the obtained response variables, this time using a SSTK with randomized hyperparameter values.",4.1 SSTK Prior,[0],[0]
"Then we optimize the GP and check if the learned hyperparameters are close to the original ones, using 10 random restarts to limit the effect of local optima.",4.1 SSTK Prior,[0],[0]
We also use the optimized GP to predict response variables on the test set and measure Root Mean Squared Error (RMSE).,4.1 SSTK Prior,[0],[0]
Our hypothesis is that with a reasonable sample size we can retrieve the original hyperparameter values and obtain low RMSE.,4.1 SSTK Prior,[0],[0]
"For each training set size, we repeat the experiment 20 times.
",4.1 SSTK Prior,[0],[0]
Figure 2 shows the results of these experiments.,4.1 SSTK Prior,[0],[0]
For small sizes the variance in the resulting hyperparameter values is large but as soon as we reach 200 instances we are able to retrieve the original values with high confidence.,4.1 SSTK Prior,[0],[0]
"In other words, in an ideal setting 200 instances are enough to learn the kernel.",4.1 SSTK Prior,[0],[0]
It is also interesting to note that test RMSE after optimization steadily decreases as we increase training data size.,4.1 SSTK Prior,[0],[0]
"This shows that if one is more interested in predictions themselves, it is still worth optimizing hyperparameters even if the training data is small.",4.1 SSTK Prior,[0],[0]
The large number of hyperparameters of the SASSTK makes it more prone to optimization and overfitting issues when compared to the SSTK.,4.2 SASSTK Prior,[0],[0]
This raises the question of how much data is needed to justify its use.,4.2 SASSTK Prior,[0],[0]
"To address this question, we run similar experiments to those above for the SSTK, except that now we sample from a GP using a SASSTK as the kernel.
",4.2 SASSTK Prior,[0],[0]
"Instead of optimizing all hyperparameters freely we use a simpler version where we tie λ and α for each symbol to the same value, except for the symbol ’S’.",4.2 SASSTK Prior,[0],[0]
Effectively this version has one extra λ and one extra α (henceforth λS and αS) when compared to the SSTK.,4.2 SASSTK Prior,[0],[0]
"The GP prior hyperparameter values are set to λ = 0.001, λS = 0.5, α = 0.1, αS = 1 and σ2n = 0.01.",4.2 SASSTK Prior,[0],[0]
"For each training set size, we train two GPs, one using this SASSTK and one using the original SSTK, optimize them using 10 random restarts and measure RMSE on the test set.
",4.2 SASSTK Prior,[0],[0]
Results are shown in Figure 3.,4.2 SASSTK Prior,[0],[0]
"For all training set sizes the SASSTK reaches lower RMSE than SSTK, with a substantial difference after reaching 100 instances.",4.2 SASSTK Prior,[0],[0]
This shows that even for small datasets our proposed kernel manages to capture aspects which can not be explained by the original SSTK.,4.2 SASSTK Prior,[0],[0]
"Note that this is an ideal setting, and real datasets may need to be larger to realize gains from SASSTK.",4.2 SASSTK Prior,[0],[0]
"Nevertheless, these are promising results since they give evidence of a small lower bound on the dataset size for SASSTK to be effective.",4.2 SASSTK Prior,[0],[0]
To provide an overview of how efficient is the gradient-based method compared to grid search we also run a set of experiments measuring wall clock training time vs. RMSE on a test set.,4.3 Performance Experiments,[0],[0]
For both GP and SVM models we employ the SSTK as the kernel and we use the same synthetic data from the previous experiments4.,4.3 Performance Experiments,[0],[0]
"We perform 20 runs, keeping the test set as the same 200 instances for all runs and randomly sampling 200 instances from the remaining instances as training data.
",4.3 Performance Experiments,[0],[0]
Figure 4 shows the curves for both GP and SVM models.,4.3 Performance Experiments,[0],[0]
"The GP curve is obtained by increasing the maximum number of iterations of the gradient-based method (in this case, L-BFGS) and the SVM curve is obtained by increasing the granularity of the grid size.
",4.3 Performance Experiments,[0],[0]
"We can see that optimizing the GP model is consistently much faster than doing grid search on the SVM model (notice the logarithmic scale), even though it shows some variance when letting L-BFGS run for a larger number of iterations.",4.3 Performance Experiments,[0],[0]
The GP model also is able to better predictions in general.,4.3 Performance Experiments,[0],[0]
"Even when taking the variances into account, grid search would still need around 10 times more computation
4For specific details on the SVM models used in all experiments performed in this paper we refer the reader to Appendix A.
time to achieve the same predictions obtained by the GP model.",4.3 Performance Experiments,[0],[0]
"In real settings, SVMs predictions tend to be more on par with the ones provided by a GP (as shown in §5) but nevertheless these figures show that the GP can be much more time efficient when optimizing hyperparameters of a tree kernel.
",4.3 Performance Experiments,[0],[0]
An important performance aspect to take into account is parallelization.,4.3 Performance Experiments,[0],[0]
Grid search is embarassingly parallelizable since each grid point can run in a different core.,4.3 Performance Experiments,[0],[0]
"However, the GP optimization can also benefit from multiple cores by running each kernel computation inside the Gram matrix in parallel.",4.3 Performance Experiments,[0],[0]
"To keep the comparisons simpler, the results shown in this section use a single core but all experiments in §5 employ parallelization in the Gram matrix computation level (for both SVM and GP models).",4.3 Performance Experiments,[0],[0]
Our experiments with NLP data address two regression tasks: Emotion Analysis and Quality Estimation.,5 NLP Experiments,[0],[0]
"For both tasks, we use the Stanford parser (Manning et al., 2014) to obtain constituency trees for all sentences.",5 NLP Experiments,[0],[0]
"Also, rather than using data official splits, we perform 5-fold cross-validation in order to obtain more reliable results.",5 NLP Experiments,[0],[0]
"The goal of Emotion Analysis is to automatically detect emotions in a text (Strapparava and Mihalcea, 2008).",5.1 Emotion Analysis,[0],[0]
"This problem is closely related to Opinion Mining (Pang and Lee, 2008), with similar applications, but it is usually done at a more fine-grained level and involves the prediction of a set of labels for each text (one for each emotion) instead of a single label.
",5.1 Emotion Analysis,[0],[0]
Beck et al. (2014a) used a multi-task GP for this task with a bag-of-words feature representation.,5.1 Emotion Analysis,[0],[0]
"In theory, it is possible to combine their multi-task kernel with our tree kernels, but to keep the focus of the experiments on testing tree kernel approaches, here we use independently trained models, one per emotion.
",5.1 Emotion Analysis,[0],[0]
"Dataset We use the dataset provided by the “Affective Text” shared task in SemEval2007 (Strapparava and Mihalcea, 2007), which is composed of 1000 news headlines annotated in terms of six emotions: Anger, Disgust, Fear, Joy, Sadness and Sur-
prise.",5.1 Emotion Analysis,[0],[0]
"For each emotion, a score between 0 and 100 is given, 0 meaning total lack of emotion and 100, maximally emotional.",5.1 Emotion Analysis,[0],[0]
"Scores are mean-normalized before training the models.
",5.1 Emotion Analysis,[0],[0]
"Models We perform experiments using the following tree kernels:
• SSTK:",5.1 Emotion Analysis,[0],[0]
"the SSTK formulation introduced by Moschitti (2006b);
• SASSTKfull: our proposed Symbol-Aware SSTK;
• SASSTKS: same as before, but using only two λ and two α hyperparameters: one for symbols corresponding to full sentences5 and another for all other symbols.",5.1 Emotion Analysis,[0],[0]
"This configuration is similar to that in Section 4.2.
",5.1 Emotion Analysis,[0],[0]
"For all kernels, we also use a variation fixing the α hyperparameters to 1 to emulate the original SSTK.
Baselines and evaluation Our results are compared against three baselines:
• SVM SSTK: a SVM using an SSTK kernel.
",5.1 Emotion Analysis,[0],[0]
"• SVM BOW: same as before, but using an RBF kernel with a bag-of-words representation.
",5.1 Emotion Analysis,[0],[0]
"• GP BOW: same as SVM BOW but using a GP instead.
",5.1 Emotion Analysis,[0],[0]
"The SVM models are trained using a wrapper for LIBSVM6 (Chang and Lin, 2001) provided by the scikit-learn toolkit7",5.1 Emotion Analysis,[0],[0]
"(Pedregosa et al., 2011) and optimized via grid search.",5.1 Emotion Analysis,[0],[0]
"Following previous work, we use Pearson’s correlation coefficient as evaluation metric.",5.1 Emotion Analysis,[0],[0]
"Pearson’s scores are obtained by concatenating all six emotions outputs together.
",5.1 Emotion Analysis,[0],[0]
Table 2 shows the results.,5.1 Emotion Analysis,[0],[0]
"The best GP model with tree kernels outperforms the SVMs, showing that the fine-grained model selection procedure provided by the GP models is helpful when dealing with tree kernels.",5.1 Emotion Analysis,[0],[0]
"However, using the SASSTK models do not help in the case of free α and the SASSTKfull actually performs worse than the original SSTK,
5In this dataset, symbols are S, SQ, SBARQ and SINV .",5.1 Emotion Analysis,[0],[0]
"6http://www.csie.ntu.edu.tw/˜cjlin/
libsvm 7http://scikit-learn.org
even though the optimized marginal likelihood was higher.",5.1 Emotion Analysis,[0],[0]
"This is evidence that the SASSTKfull model is overfitting the training data, probably due to its large number of hyperparameters.
",5.1 Emotion Analysis,[0],[0]
Another interesting finding in Table 2 is that fixing the α values often harms performance.,5.1 Emotion Analysis,[0],[0]
Inspecting the free α models showed that the values found by the optimizer were very close to zero.,5.1 Emotion Analysis,[0],[0]
This indicates that the model selection procedure prefer towards giving smaller weights to incomplete tree fragments.,5.1 Emotion Analysis,[0],[0]
"We can interpret this as the model selecting a more lexicalized feature space, which also explains why the GP RBF model on bag-of-words performed the best in this task.
",5.1 Emotion Analysis,[0],[0]
"Finally, to understand how the optimized kernels could provide more interpretability, Table 3 shows the top 15 λ values obtained by the SASSTKfull (fixed α variant) with their corresponding symbols.",5.1 Emotion Analysis,[0],[0]
In this specific case the kernel does not give the best performance so there are limitations in doing a full linguistic analysis.,5.1 Emotion Analysis,[0],[0]
"Nevertheless, we believe this example shows the potential for developing more interpretable kernels.",5.1 Emotion Analysis,[0],[0]
This is especially interesting because these models take into account a much richer feature space than what it is allowed by parametric models.,5.1 Emotion Analysis,[0],[0]
"The goal of Quality Estimation is to provide a quality prediction for new, unseen machine translated texts (Blatz et al., 2004; Bojar et al., 2014).",5.2 Quality Estimation,[0],[0]
"Exam-
ples of applications include filtering machine translated sentences that would require more post-editing effort than translation from scratch (Specia et al., 2009), selecting the best translation from different MT systems (Specia et al., 2010) or between an MT system and a translation memory (He et al., 2010), and highlighting segments that need revision (Bach et al., 2011).",5.2 Quality Estimation,[0],[0]
"While various quality metrics exist, here we focus on post-editing time prediction.
",5.2 Quality Estimation,[0],[0]
Tree kernels have been used before in this task (with SVMs) by Hardmeier (2011) and Hardmeier et al. (2012).,5.2 Quality Estimation,[0],[0]
"While their best models combine tree kernels with a set of explicit features, they also show good results using only the tree kernels.",5.2 Quality Estimation,[0],[0]
"This makes Quality Estimation a good benchmark task to test our models.
",5.2 Quality Estimation,[0],[0]
Datasets We use two publicly available datasets containing post-edited machine translated sentences.,5.2 Quality Estimation,[0],[0]
"Both are composed of a set of source sentences, their machine translated outputs and the corresponding post-editing time.
",5.2 Quality Estimation,[0],[0]
"• French-English (fr-en): This dataset, described in (Specia, 2011), contains 2524 French sentences translated into English and postedited by a novice translator.
",5.2 Quality Estimation,[0],[0]
"• English-Spanish (en-es): This dataset was used in the WMT14 Quality Estimation shared task (Bojar et al., 2014), containing 858 sentences translated from English into Spanish and post-edited by an expert translator.
",5.2 Quality Estimation,[0],[0]
"For each dataset, post-editing times are first divided by the translation output length (obtaining the post-editing time per word) and then mean normalized.
",5.2 Quality Estimation,[0],[0]
"Models Since our data consists of pairs of trees, our models in this task use a pair of tree kernels.",5.2 Quality Estimation,[0],[0]
We combine these two kernels by either summing or multiplying them.,5.2 Quality Estimation,[0],[0]
"As for underlying tree kernels, we try both SSTK and SASSTKS .",5.2 Quality Estimation,[0],[0]
"As in the Emotion Analysis task, we also experiment with a set of kernel configurations with the α hyperparameters fixed at 1.",5.2 Quality Estimation,[0],[0]
"We also test models that combine our tree kernels with an RBF kernel on a set of 17 features extracted using the QuEst framework (Specia et al., 2013).",5.2 Quality Estimation,[0],[0]
"These features are part of a strong baseline model used by the WMT14 shared task.
",5.2 Quality Estimation,[0],[0]
"Baselines and evaluation We compare our results with a number of SVM models:
• SVM SSTK: same as in the Emotion Analysis task, using either a sum (+) or a product (×) of SSTKs.
",5.2 Quality Estimation,[0],[0]
"• SVM RBF: this is an SVM trained on the 17 features extracted by Quest.
• SVM RBF",5.2 Quality Estimation,[0],[0]
"SSTK: a combination of the two models above.
",5.2 Quality Estimation,[0],[0]
"For further comparison, we also show results obtained using a GP model and an RBF kernel on the QuEst-only features.",5.2 Quality Estimation,[0],[0]
"Following previous work, we measure prediction performance using both Mean Absolute Error (MAE) and RMSE.
",5.2 Quality Estimation,[0],[0]
The prediction results are given in Table 4.,5.2 Quality Estimation,[0],[0]
"They indicate a number of interesting findings:
• For the fr-en dataset, the GP models combining tree kernels with an RBF kernel outperform all other models.",5.2 Quality Estimation,[0],[0]
"Results for the en-es dataset are less consistent, probably due to the small size of the dataset, but on average they are better than their SVM counterparts.
",5.2 Quality Estimation,[0],[0]
• The SVMs using a combination of kernels performs worse than using the RBF kernel alone.,5.2 Quality Estimation,[0],[0]
"Inspecting the models, we found that grid search actually harms performance.",5.2 Quality Estimation,[0],[0]
"For instance, for the fr-en dataset, MAE and RMSE for the RBF + SSTK × model before grid search are 0.4681 and 0.6016, respectively.",5.2 Quality Estimation,[0],[0]
"On the other hand, for this dataset all GP models achieve better results after optimization.
",5.2 Quality Estimation,[0],[0]
"• Unlike in the Emotion Analysis task, fixing α results in better performance, even though the resulting models have lower marginal likelihood than the ones with free α.",5.2 Quality Estimation,[0],[0]
The same effect happened when comparing the SASSTK models with the SSTK ones for the en-es dataset.,5.2 Quality Estimation,[0],[0]
"Both cases are evidence of model overfitting.
",5.2 Quality Estimation,[0],[0]
We also inspect the resulting hyperparameters to obtain insights about the features used by the model.,5.2 Quality Estimation,[0],[0]
Table 5 shows the optimized λ values for the GP SSTK models with fixed α for the fr-en dataset.,5.2 Quality Estimation,[0],[0]
The λ values obtained are higher for the target sentence kernels than for the source sentence ones.,5.2 Quality Estimation,[0],[0]
"We can interpret this as the model giving preference to features from the target trees instead of the source trees, which is what we would expect for this task.",5.2 Quality Estimation,[0],[0]
Both NLP tasks show evidence that the GP models with large number of hyperparameters (SASSTKfull in the case of Emotion Analysis and the free α models in Quality Estimation) are overfitting the corresponding datasets.,5.3 Overfitting,[0],[0]
"While the Bayesian formula-
tion for the marginal likelihood does help limiting overfitting, it does not prevent it completely.",5.3 Overfitting,[0],[0]
Small datasets or invalid assumptions about the Gaussian distribution of the data may still lead to poorly fitting models.,5.3 Overfitting,[0],[0]
"Another means of reducing overfitting is by taking a fully Bayesian approach in which hyperparameters are considered as random variables and are marginalized out (Osborne, 2010); this is a research direction we plan to pursue in the future.8",5.3 Overfitting,[0],[0]
The GP framework introduced in Section 2 can be extended to non-regression problems by changing the likelihood function.,5.4 Extensions to Other Tasks,[0],[0]
"For instance, models for classification (Rasmussen and Williams, 2006, Chap. 3), ordinal regression (Chu and Ghahramani, 2005) and structured prediction (Altun et al., 2004; Bratières et al., 2013) were proposed in the literature.",5.4 Extensions to Other Tasks,[0],[0]
"Since the likelihood is independent of the kernel, a natural future step is to apply the kernels and models introduced in this paper to different NLP tasks.
",5.4 Extensions to Other Tasks,[0],[0]
"In light of that, we did initial experiments with constituency parsing",5.4 Extensions to Other Tasks,[0],[0]
reranking.9 The first results were inconclusive but we do believe this is because we employed naive approaches using classification (1-best result vs. all) and regression (using PARSEVAL metrics as the response variable) models.,5.4 Extensions to Other Tasks,[0],[0]
A more appropriate way to tackle this task is by employing a reranking-based likelihood and this is a direction we plan to pursue in the future.,5.4 Extensions to Other Tasks,[0],[0]
"Interest in model selection procedures for kernelbased methods has been growing in the last years.
8See also Rasmussen and Williams (2006, Chap. 5) for an in-depth discussion on this issue.
",6 Related Work,[0],[0]
"9We thank the anonymous reviewers for this suggestion.
",6 Related Work,[0],[0]
"One widely used approach for that is Multiple Kernel Learning (MKL) (Gönen and Alpaydın, 2011).",6 Related Work,[0],[0]
MKL is based on the idea of using combinations of kernels to model the data and developing algorithms to tune the kernel coefficients.,6 Related Work,[0],[0]
"This is different from our method, where we focus on learning the hyperparameters of a single structural kernel.",6 Related Work,[0],[0]
An approach similar to ours was proposed by Igel et al. (2007).,6 Related Work,[0],[0]
"They combine oligo kernels (a kind of ngram kernel) with MKL, derive their gradients and optimize towards a kernel alignment metric.",6 Related Work,[0],[0]
"Compared to our approach, they restrict the length of the n-grams being considered, while we rely on dynamic programming to explore the whole substructure space.",6 Related Work,[0],[0]
"Also, their method does not take into account the underlying learning algorithm.",6 Related Work,[0],[0]
"Another recent approach proposed for model selection is random search (Bergstra and Bengio, 2012).",6 Related Work,[0],[0]
"Like grid search, it has the drawback of not employing gradient information, as it is designed for any kind of hyperparameters (including categorical ones).
",6 Related Work,[0],[0]
Structural kernels have been successfully employed in a number of NLP tasks.,6 Related Work,[0],[0]
The original SSTK proposed by Collins and Duffy (2001) was used to rerank the output of syntactic parsers.,6 Related Work,[0],[0]
"Recently, this reranking idea was also applied to discourse parsing (Joty and Moschitti, 2014).",6 Related Work,[0],[0]
"Other tree kernel applications include Semantic Role Labelling (Moschitti et al., 2008) and Relation Extraction (Plank and Moschitti, 2013).",6 Related Work,[0],[0]
"String kernels were mostly used in Text Classification (Lodhi et al., 2002; Cancedda et al., 2003), while graph kernels have been used for recognizing Textual Entailment (Zanzotto and Dell’Arciprete, 2009).",6 Related Work,[0],[0]
"However, these previous works focused on frequentist methods like SVM or voted perceptron while we employ a Bayesian approach.
",6 Related Work,[0],[0]
"Gaussian Processes are a major framework in machine learning nowadays: applications include Robotics (Ko et al., 2007), Geolocation (Schwaighofer et al., 2004) and Computer Vision (Sinz et al., 2004).",6 Related Work,[0],[0]
"Only very recently they have been successfully employed in a few NLP tasks such as translation quality estimation (Cohn and Specia, 2013; Beck et al., 2014b), detection of temporal patterns in text (Preoţiuc-Pietro and Cohn, 2013), semantic similarity (Rios and Specia, 2014) and emotion analysis (Beck et al., 2014a).",6 Related Work,[0],[0]
"In terms of feature
representations, previous work focused on the vectorial inputs and applied well-known kernels for these inputs, e.g. the RBF kernel.",6 Related Work,[0],[0]
"As shown on §5.2, our approach is orthogonal to these previous ones, since kernels can be easily combined in different ways.
",6 Related Work,[0],[0]
It is important to note that we are not the first ones to combine GPs with kernels on structured inputs.,6 Related Work,[0],[0]
Driessens et al. (2006) employed a combination of GPs and graph kernels for reinforcement learning.,6 Related Work,[0],[0]
"However, unlike our approach, they did not attempt model selection, evaluating only a few hyperparameter values empirically.",6 Related Work,[0],[0]
"This paper describes a Bayesian approach for structural kernel learning, based on Gaussian Processes for easy model selection.",7 Conclusions,[0],[0]
Experiments applying our models to synthetic data showed that it is possible to learn structural kernel hyperparameters using a fairly small amount of data.,7 Conclusions,[0],[0]
"Furthermore we obtained promising results in two NLP tasks, including Quality Estimation, where we beat the state of the art.",7 Conclusions,[0],[0]
"Finally, we showed how these rich parameterizations can lead to more interpretable kernels.
",7 Conclusions,[0],[0]
"Beyond empirical improvements, an important goal of this paper is to present a method that enables new kernel developments through the extension of the number of hyperparameters.",7 Conclusions,[0],[0]
"We focused on the Subset Tree Kernel, proposing an extension and then deriving its gradients.",7 Conclusions,[0],[0]
"This approach can be applied to any structural kernel, as long as gradients are available.",7 Conclusions,[0],[0]
It is our hope that this work will serve as a starting point for future developments in these research directions.,7 Conclusions,[0],[0]
Daniel Beck was supported by funding from CNPq/Brazil (No. 237999/2012-9).,Acknowledgements,[0],[0]
Dr. Cohn is the recipient of an Australian Research Council Future Fellowship (project number FT130101105).,Acknowledgements,[0],[0]
The authors would also like to thank the three anonymous reviewers for their helpful comments and suggestions.,Acknowledgements,[0],[0]
All SVM baselines employ the -insensitve loss function.,A Details on SVM Baselines,[0],[0]
Grid search optimization is done via 3- fold cross-validation on the respective training set and use RMSE as the metric to be minimized.,A Details on SVM Baselines,[0],[0]
"After obtained the best hyperparameter values, the SVM is retrained using these values on the full respective training set.",A Details on SVM Baselines,[0],[0]
"The specific intervals used in grid search depend on the task.
",A Details on SVM Baselines,[0],[0]
"For the performance experiments on synthetic data, we employed an interval of [10−2, 10] for C (regularization coefficient) and , [10−8, 1] for λ and [10−4, 2] for α.",A Details on SVM Baselines,[0],[0]
In each run we incrementally increase the size of the grid by adding intermediate values on each interval.,A Details on SVM Baselines,[0],[0]
We keep a linear scale for the SSTK hyperparameters and a logarithmic scale for C and .,A Details on SVM Baselines,[0],[0]
"As an example, Table 6 shows the resulting grids when the grid value is 4 for each hyperparameter.",A Details on SVM Baselines,[0],[0]
"For all NLP experiments the grid is fixed for all hyperparameters (including γ, the lengthscale value in the RBF kernel), with its corresponding values shown on Table 7.",A Details on SVM Baselines,[0],[0]
Structural kernels are a flexible learning paradigm that has been widely used in Natural Language Processing.,abstractText,[0],[0]
"However, the problem of model selection in kernel-based methods is usually overlooked.",abstractText,[0],[0]
"Previous approaches mostly rely on setting default values for kernel hyperparameters or using grid search, which is slow and coarse-grained.",abstractText,[0],[0]
"In contrast, Bayesian methods allow efficient model selection by maximizing the evidence on the training data through gradient-based methods.",abstractText,[0],[0]
In this paper we show how to perform this in the context of structural kernels by using Gaussian Processes.,abstractText,[0],[0]
Experimental results on tree kernels show that this procedure results in better prediction performance compared to hyperparameter optimization via grid search.,abstractText,[0],[0]
"The framework proposed in this paper can be adapted to other structures besides trees, e.g., strings and graphs, thereby extending the utility of kernel-based methods.",abstractText,[0],[0]
Learning Structural Kernels for Natural Language Processing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 44–55 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1005",text,[0],[0]
Semantic parsing is the task of mapping natural language utterances to machine interpretable meaning representations.,1 Introduction,[0],[0]
"Despite differences in the choice of meaning representation and model structure, most existing work conceptualizes semantic parsing following two main approaches.",1 Introduction,[0],[0]
"Under the first approach, an utterance is parsed and grounded to a meaning representation directly via learning a task-specific grammar (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2006; Kwiatkowksi et al., 2010; Liang et al., 2011; Berant et al., 2013; Flanigan et al., 2014; Pasupat and Liang, 2015; Groschwitz et al., 2015).",1 Introduction,[0],[0]
"Under the second approach, the utterance is first parsed to an intermediate task-independent representation tied to a syntactic parser and then mapped to a grounded
1Our code is available at https://github.com/ cheng6076/scanner.
",1 Introduction,[0],[0]
"representation (Kwiatkowski et al., 2013; Reddy et al., 2016, 2014; Krishnamurthy and Mitchell, 2015; Gardner and Krishnamurthy, 2017).",1 Introduction,[0],[0]
"A merit of the two-stage approach is that it creates reusable intermediate interpretations, which potentially enables the handling of unseen words and knowledge transfer across domains (Bender et al., 2015).
",1 Introduction,[0],[0]
"The successful application of encoder-decoder models (Bahdanau et al., 2015; Sutskever et al., 2014) to a variety of NLP tasks has provided strong impetus to treat semantic parsing as a sequence transduction problem where an utterance is mapped to a target meaning representation in string format (Dong and Lapata, 2016; Jia and Liang, 2016; Kočiský et al., 2016).",1 Introduction,[0],[0]
"Such models still fall under the first approach, however, in contrast to previous work (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Liang et al., 2011)",1 Introduction,[0],[0]
"they reduce the need for domain-specific assumptions, grammar learning, and more generally extensive feature engineering.",1 Introduction,[0],[0]
But this modeling flexibility comes at a cost since it is no longer possible to interpret how meaning composition is performed.,1 Introduction,[0],[0]
Such knowledge plays a critical role in understand modeling limitations so as to build better semantic parsers.,1 Introduction,[0],[0]
"Moreover, without any taskspecific prior knowledge, the learning problem is fairly unconstrained, both in terms of the possible derivations to consider and in terms of the target output which can be ill-formed (e.g., with extra or missing brackets).
",1 Introduction,[0],[0]
"In this work, we propose a neural semantic parser that alleviates the aforementioned problems.",1 Introduction,[0],[0]
Our model falls under the second class of approaches where utterances are first mapped to an intermediate representation containing natural language predicates.,1 Introduction,[0],[0]
"However, rather than using an external parser (Reddy et al., 2014, 2016) or manually specified CCG grammars (Kwiatkowski et al., 2013), we induce intermediate representations in the form of predicate-argument structures
44
from data.",1 Introduction,[0],[0]
"This is achieved with a transition-based approach which by design yields recursive semantic structures, avoiding the problem of generating ill-formed meaning representations.",1 Introduction,[0],[0]
"Compared to most existing semantic parsers which employ a CKY style bottom-up parsing strategy (Krishnamurthy and Mitchell, 2012; Cai and Yates, 2013; Berant et al., 2013; Berant and Liang, 2014), the transition-based approach we proposed does not require feature decomposition over structures and thereby enables the exploration of rich, non-local features.",1 Introduction,[0],[0]
"The output of the transition system is then grounded (e.g., to a knowledge base) with a neural mapping model under the assumption that grounded and ungrounded structures are isomorphic.2 As a result, we obtain a neural model that jointly learns to parse natural language semantics and induce a lexicon that helps grounding.
",1 Introduction,[0],[0]
The whole network is trained end-to-end on natural language utterances paired with annotated logical forms or their denotations.,1 Introduction,[0],[0]
"We conduct experiments on four datasets, including GEOQUERY (which has logical forms; Zelle and Mooney 1996), SPADES (Bisk et al., 2016), WEBQUESTIONS (Berant et al., 2013), and GRAPHQUESTIONS (Su et al., 2016) (which have denotations).",1 Introduction,[0],[0]
"Our semantic parser achieves the state of the art on SPADES and GRAPHQUESTIONS, while obtaining competitive results on GEOQUERY and WEBQUESTIONS.",1 Introduction,[0],[0]
"A side-product of our modeling framework is that the induced intermediate representations can contribute to rationalizing neural predictions (Lei et al., 2016).",1 Introduction,[0],[0]
"Specifically, they can shed light on the kinds of representations (especially predicates) useful for semantic parsing.",1 Introduction,[0],[0]
"Evaluation of the induced predicate-argument relations against syntax-based ones reveals that they are interpretable and meaningful compared to heuristic baselines, but they sometimes deviate from linguistic conventions.",1 Introduction,[0],[0]
"Problem Formulation Let K denote a knowledge base or more generally a reasoning system, and x an utterance paired with a grounded meaning representationG or its denotation y.",2 Preliminaries,[0],[0]
Our problem is to learn a semantic parser that maps x to G via an intermediate ungrounded representation U .,2 Preliminaries,[0],[0]
"When G is executed against K, it outputs denota-
2We discuss the merits and limitations of this assumption in Section 5
tion y.
Grounded Meaning Representation We represent grounded meaning representations in FunQL (Kate et al., 2005) amongst many other alternatives such as lambda calculus (Zettlemoyer and Collins, 2005), λ-DCS (Liang, 2013) or graph queries (Holzschuher and Peinl, 2013; Harris et al., 2013).",2 Preliminaries,[0],[0]
"FunQL is a variable-free query language, where each predicate is treated as a function symbol that modifies an argument list.",2 Preliminaries,[0],[0]
"For example, the FunQL representation for the utterance which states do not border texas is:
answer(exclude(state(all), next to(texas)))
where next to is a domain-specific binary predicate that takes one argument (i.e., the entity texas) and returns a set of entities (e.g., the states bordering Texas) as its denotation.",2 Preliminaries,[0],[0]
all is a special predicate that returns a collection of entities.,2 Preliminaries,[0],[0]
"exclude is a predicate that returns the difference between two input sets.
",2 Preliminaries,[0],[0]
An advantage of FunQL is that the resulting s-expression encodes semantic compositionality and derivation of the logical forms.,2 Preliminaries,[0],[0]
"This property makes FunQL logical forms convenient to be predicted with recurrent neural networks (Vinyals et al., 2015; Choe and Charniak, 2016; Dyer et al., 2016).",2 Preliminaries,[0],[0]
"However, FunQL is less expressive than lambda calculus, partially due to the elimination of variables.",2 Preliminaries,[0],[0]
"A more compact logical formulation which our method also applies to is λ-DCS (Liang, 2013).",2 Preliminaries,[0],[0]
"In the absence of anaphora and composite binary predicates, conversion algorithms exist between FunQL and λ-DCS.",2 Preliminaries,[0],[0]
"However, we leave this to future work.
",2 Preliminaries,[0],[0]
Ungrounded Meaning Representation We also use FunQL to express ungrounded meaning representations.,2 Preliminaries,[0],[0]
The latter consist primarily of natural language predicates and domain-general predicates.,2 Preliminaries,[0],[0]
"Assuming for simplicity that domaingeneral predicates share the same vocabulary
in ungrounded and grounded representations, the ungrounded representation for the example utterance is:
answer(exclude(states(all), border(texas)))
where states and border are natural language predicates.",2 Preliminaries,[0],[0]
In this work we consider five types of domain-general predicates illustrated in Table 1.,2 Preliminaries,[0],[0]
"Notice that domain-general predicates are often implicit, or represent extra-sentential knowledge.",2 Preliminaries,[0],[0]
"For example, the predicate all in the above utterance represents all states in the domain which are not mentioned in the utterance but are critical for working out the utterance denotation.",2 Preliminaries,[0],[0]
"Finally, note that for certain domain-general predicates, it also makes sense to extract natural language rationales (e.g., not is indicative for exclude).",2 Preliminaries,[0],[0]
"But we do not find this helpful in experiments.
",2 Preliminaries,[0],[0]
In this work we constrain ungrounded representations to be structurally isomorphic to grounded ones.,2 Preliminaries,[0],[0]
"In order to derive the target logical forms, all we have to do is replacing predicates in the ungrounded representations with symbols in the knowledge base.",2 Preliminaries,[0],[0]
"In this section, we discuss our neural model which maps utterances to target logical forms.",3 Modeling,[0],[0]
"The semantic parsing task is decomposed in two stages: we first explain how an utterance is converted to an intermediate representation (Section 3.1), and then describe how it is grounded to a knowledge base (Section 3.2).",3 Modeling,[0],[0]
"At this stage, utterances are mapped to intermediate representations with a transition-based algorithm.",3.1 Generating Ungrounded Representations,[0],[0]
"In general, the transition system generates the representation by following a derivation tree (which contains a set of applied rules) and some canonical generation order (e.g., depth-first).",3.1 Generating Ungrounded Representations,[0],[0]
"For FunQL, a simple solution exists since the representation itself encodes the derivation.",3.1 Generating Ungrounded Representations,[0],[0]
"Consider again answer(exclude(states(all), border(texas)))",3.1 Generating Ungrounded Representations,[0],[0]
which is tree structured.,3.1 Generating Ungrounded Representations,[0],[0]
"Each predicate (e.g., border) can be visualized as a non-terminal node of the tree and each entity (e.g., texas) as a terminal.",3.1 Generating Ungrounded Representations,[0],[0]
The predicate all is a special case which acts as a terminal directly.,3.1 Generating Ungrounded Representations,[0],[0]
"We can generate the tree with a top-down, depth first transition system reminiscent of recurrent neural network grammars (RNNGs; Dyer et al. 2016).",3.1 Generating Ungrounded Representations,[0],[0]
"Similar to RNNG, our
algorithm uses a buffer to store input tokens in the utterance and a stack to store partially completed trees.",3.1 Generating Ungrounded Representations,[0],[0]
A major difference in our semantic parsing scenario is that tokens in the buffer are not fetched in a sequential order or removed from the buffer.,3.1 Generating Ungrounded Representations,[0],[0]
This is because the lexical alignment between an utterance and its semantic representation is hidden.,3.1 Generating Ungrounded Representations,[0],[0]
"Moreover, some predicates cannot be clearly anchored to a token span.",3.1 Generating Ungrounded Representations,[0],[0]
"Therefore, we allow the generation algorithm to pick tokens and combine logical forms in arbitrary orders, conditioning on the entire set of sentential features.",3.1 Generating Ungrounded Representations,[0],[0]
"Alternative solutions in the traditional semantic parsing literature include a floating chart parser (Pasupat and Liang, 2015) which allows to construct logical predicates out of thin air.
",3.1 Generating Ungrounded Representations,[0],[0]
"Our transition system defines three actions, namely NT, TER, and RED, explained below.
NT(X) generates a Non-Terminal predicate.",3.1 Generating Ungrounded Representations,[0],[0]
"This predicate is either a natural language expression such as border, or one of the domain-general predicates exemplified in Table 1 (e.g., exclude).",3.1 Generating Ungrounded Representations,[0],[0]
"The type of predicate is determined by the placeholder X and once generated, it is pushed onto the stack and represented as a non-terminal followed by an open bracket (e.g., ‘border(’).",3.1 Generating Ungrounded Representations,[0],[0]
"The open bracket will be closed by a reduce operation.
",3.1 Generating Ungrounded Representations,[0],[0]
TER(X) generates a TERminal entity or the special predicate all.,3.1 Generating Ungrounded Representations,[0],[0]
"Note that the terminal choice does not include variable (e.g., $0, $1), since FunQL is a variable-free language which sufficiently captures the semantics of the datasets we work with.",3.1 Generating Ungrounded Representations,[0],[0]
"The framework could be extended to generate directly acyclic graphs by incorporating variables with additional transition actions for handling variable mentions and co-reference.
",3.1 Generating Ungrounded Representations,[0],[0]
RED stands for REDuce and is used for subtree completion.,3.1 Generating Ungrounded Representations,[0],[0]
It recursively pops elements from the stack until an open non-terminal node is encountered.,3.1 Generating Ungrounded Representations,[0],[0]
"The non-terminal is popped as well, after which a composite term representing the entire subtree, e.g., border(texas), is pushed back to the stack.",3.1 Generating Ungrounded Representations,[0],[0]
"If a RED action results in having no more open non-terminals left on the stack, the transition system terminates.",3.1 Generating Ungrounded Representations,[0],[0]
"Table 2 shows the transition actions used to generate our running example.
",3.1 Generating Ungrounded Representations,[0],[0]
The model generates the ungrounded representation U conditioned on utterance x by recursively calling one of the above three actions.,3.1 Generating Ungrounded Representations,[0],[0]
"Note that U is defined by a sequence of actions (denoted
by a) and a sequence of term choices (denoted by u) as shown in Table 2.",3.1 Generating Ungrounded Representations,[0],[0]
"The conditional probability p(U |x) is factorized over time steps as:
p(U |x) =",3.1 Generating Ungrounded Representations,[0],[0]
"p(a, u|x)
=
T∏
t=1
p(at|a<t, x)p(ut|a<t, x)I(at 6=RED) (1)
where I is an indicator function.",3.1 Generating Ungrounded Representations,[0],[0]
"To predict the actions of the transition system, we encode the input buffer with a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) and the output stack with a stack-LSTM (Dyer et al., 2015).",3.1 Generating Ungrounded Representations,[0],[0]
"At each time step, the model uses the representation of the transition system et to predict an action:
p(at|a<t, x) ∝",3.1 Generating Ungrounded Representations,[0],[0]
"exp(Wa · et) (2)
where et is the concatenation of the buffer representation bt and the stack representation st.",3.1 Generating Ungrounded Representations,[0],[0]
"While the stack representation st is easy to retrieve as the top state of the stack-LSTM, obtaining the buffer representation bt is more involved.",3.1 Generating Ungrounded Representations,[0],[0]
This is because we do not have an explicit buffer representation due to the non-projectivity of semantic parsing.,3.1 Generating Ungrounded Representations,[0],[0]
"We therefore compute at each time step an adaptively weighted representation of bt (Bahdanau et al., 2015) conditioned on the stack representation st.",3.1 Generating Ungrounded Representations,[0],[0]
"This buffer representation is then concatenated with the stack representation to form the system representation et.
",3.1 Generating Ungrounded Representations,[0],[0]
"When the predicted action is either NT or TER, an ungrounded term ut (either a predicate or an
entity) needs to be chosen from the candidate list depending on the specific placeholder X. To select a domain-general term, we use the same representation of the transition system et to compute a probability distribution over candidate terms:
p(uGENERALt |a<t, x) ∝ exp(Wp · et) (3)
To choose a natural language term, we directly compute a probability distribution of all natural language terms (in the buffer) conditioned on the stack representation st and select the most relevant term (Jia and Liang, 2016):
p(uNLt |a<t, x) ∝ exp(st) (4)
When the predicted action is RED, the completed subtree is composed into a single representation on the stack.",3.1 Generating Ungrounded Representations,[0],[0]
"For the choice of composition function, we use a single-layer neural network as in Dyer et al. (2015), which takes as input the concatenated representation of the predicate and argument of the subtree.",3.1 Generating Ungrounded Representations,[0],[0]
"Since we constrain the network to learn ungrounded structures that are isomorphic to the target meaning representation, converting ungrounded representations to grounded ones becomes a simple lexical mapping problem.",3.2 Generating Grounded Representations,[0],[0]
"For simplicity, hereafter we do not differentiate natural language and domain-general predicates.
",3.2 Generating Grounded Representations,[0],[0]
"To map an ungrounded term ut to a grounded term gt, we compute the conditional probability
of gt given ut with a bi-linear neural network: p(gt|ut) ∝",3.2 Generating Grounded Representations,[0],[0]
"exp ~ut ·Wug · ~gt> (5) where ~ut is the contextual representation of the ungrounded term given by the bidirectional LSTM, ~gt is the grounded term embedding, and Wug is the weight matrix.
",3.2 Generating Grounded Representations,[0],[0]
The above grounding step can be interpreted as learning a lexicon: the model exclusively relies on the intermediate representation U to predict the target meaning representation G without taking into account any additional features based on the utterance.,3.2 Generating Grounded Representations,[0],[0]
"In practice, U may provide sufficient contextual background for closed domain semantic parsing where an ungrounded predicate often maps to a single grounded predicate, but is a relatively impoverished representation for parsing large open-domain knowledge bases like Freebase.",3.2 Generating Grounded Representations,[0],[0]
"In this case, we additionally rely on a discriminative reranker which ranks the grounded representations derived from ungrounded representations (see Section 3.4).",3.2 Generating Grounded Representations,[0],[0]
"When the target meaning representation is available, we directly compare it against our predictions and back-propagate.",3.3 Training Objective,[0],[0]
"When only denotations are available, we compare surrogate meaning representations against our predictions (Reddy et al., 2014).",3.3 Training Objective,[0],[0]
Surrogate representations are those with the correct denotations.,3.3 Training Objective,[0],[0]
"When there exist multiple surrogate representations,3 we select one randomly and back-propagate.",3.3 Training Objective,[0],[0]
"The global effect of the above update rule is close to maximizing the marginal likelihood of denotations, which differs from recent work on weakly-supervised semantic parsing based on reinforcement learning (Neelakantan et al., 2017).
",3.3 Training Objective,[0],[0]
"Consider utterance x with ungrounded meaning representation U , and grounded meaning representation G. Both U and G are defined with a sequence of transition actions (same for U and G) and a sequence of terms (different for U and G).",3.3 Training Objective,[0],[0]
Recall that a =,3.3 Training Objective,[0],[0]
"[a1, · · · , an] denotes the transition action sequence defining U and G; let u =",3.3 Training Objective,[0],[0]
"[u1, · · · , uk] denote the ungrounded terms (e.g., predicates), and g =",3.3 Training Objective,[0],[0]
"[g1, · · · , gk] the grounded terms.",3.3 Training Objective,[0],[0]
We aim to maximize the likelihood of the grounded meaning representation p(G|x) over all training examples.,3.3 Training Objective,[0],[0]
"This
3The average Freebase surrogate representations obtained with highest denotation match (F1) is 1.4.
likelihood can be decomposed into the likelihood of the grounded action sequence p(a|x) and the grounded term sequence p(g|x), which we optimize separately.
",3.3 Training Objective,[0],[0]
"For the grounded action sequence (which by design is the same as the ungrounded action sequence and therefore the output of the transition system), we can directly maximize the log likelihood log p(a|x) for all examples:
La = ∑
x∈T log p(a|x) =
∑
x∈T
n∑
t=1
log p(at|x) (6)
where T denotes examples in the training data.",3.3 Training Objective,[0],[0]
"For the grounded term sequence g, since the intermediate ungrounded terms are latent, we maximize the expected log likelihood of the grounded terms ∑ u [p(u|x) log p(g|u, x)] for all examples, which is a lower bound of the log likelihood log p(g|x):",3.3 Training Objective,[0],[0]
"Lg = ∑
x∈T
∑
u
[p(u|x) log p(g|u, x)]
= ∑
x∈T
∑
u
[ p(u|x) k∑
t=1
log p(gt|ut) ]",3.3 Training Objective,[0],[0]
"(7)
The final objective is the combination of La and Lg, denoted as LG = La + Lg.",3.3 Training Objective,[0],[0]
We optimize this objective with the method described in Lei et al. (2016).,3.3 Training Objective,[0],[0]
"As discussed above, for open domain semantic parsing, solely relying on the ungrounded representation would result in an impoverished model lacking sentential context useful for disambiguation decisions.",3.4 Reranker,[0],[0]
"For all Freebase experiments, we followed previous work (Berant et al., 2013; Berant and Liang, 2014; Reddy et al., 2014) in additionally training a discriminative ranker to re-rank grounded representations globally.
",3.4 Reranker,[0],[0]
"The discriminative ranker is a maximumentropy model (Berant et al., 2013).",3.4 Reranker,[0],[0]
"The objective is to maximize the log likelihood of the correct answer y given x by summing over all grounded candidates G with denotation y (i.e.,[[G]]K = y):
Ly = ∑
(x,y)∈T log
∑
[[G]]K=y
p(G|x) (8)
p(G|x) ∝ exp{f(G, x)} (9) where f(G, x) is a feature function that maps pair (G, x) into a feature vector.",3.4 Reranker,[0],[0]
We give details on the features we used in Section 4.2.,3.4 Reranker,[0],[0]
"In this section, we verify empirically that our semantic parser derives useful meaning representations.",4 Experiments,[0],[0]
We give details on the evaluation datasets and baselines used for comparison.,4 Experiments,[0],[0]
We also describe implementation details and the features used in the discriminative ranker.,4 Experiments,[0],[0]
"We evaluated our model on the following datasets which cover different domains, and use different types of training data, i.e., pairs of natural language utterances and grounded meanings or question-answer pairs.
",4.1 Datasets,[0],[0]
"GEOQUERY (Zelle and Mooney, 1996) contains 880 questions and database queries about US geography.",4.1 Datasets,[0],[0]
"The utterances are compositional, but the language is simple and vocabulary size small.",4.1 Datasets,[0],[0]
The majority of questions include at most one entity.,4.1 Datasets,[0],[0]
"SPADES (Bisk et al., 2016) contains 93,319 questions derived from CLUEWEB09 (Gabrilovich et al., 2013) sentences.",4.1 Datasets,[0],[0]
"Specifically, the questions were created by randomly removing an entity, thus producing sentence-denotation pairs (Reddy et al., 2014).",4.1 Datasets,[0],[0]
"The sentences include two or more entities and although they are not very compositional, they constitute a large-scale dataset for neural network training.",4.1 Datasets,[0],[0]
"WEBQUESTIONS (Berant et al., 2013) contains 5,810 question-answer pairs.",4.1 Datasets,[0],[0]
"Similar to SPADES, it is based on Freebase and the questions are not very compositional.",4.1 Datasets,[0],[0]
"However, they are real questions asked by people on the Web.",4.1 Datasets,[0],[0]
"Finally, GRAPHQUESTIONS (Su et al., 2016) contains 5,166 question-answer pairs which were created by showing 500 Freebase graph queries to Amazon Mechanical Turk workers and asking them to paraphrase them into natural language.",4.1 Datasets,[0],[0]
"Amongst the four datasets described above, GEOQUERY has annotated logical forms which we directly use for training.",4.2 Implementation Details,[0],[0]
"For the other three datasets, we treat surrogate meaning representations which lead to the correct answer as gold standard.",4.2 Implementation Details,[0],[0]
"The surrogates were selected from a subset of candidate Freebase graphs, which were obtained by entity linking.",4.2 Implementation Details,[0],[0]
"Entity mentions in SPADES have been automatically annotated with Freebase entities (Gabrilovich et al., 2013).",4.2 Implementation Details,[0],[0]
"For WEBQUESTIONS and GRAPHQUESTIONS, we follow the procedure described in Reddy et al. (2016).",4.2 Implementation Details,[0],[0]
"We identify po-
tential entity spans using seven handcrafted partof-speech patterns and associate them with Freebase entities obtained from the Freebase/KG API.4",4.2 Implementation Details,[0],[0]
We use a structured perceptron trained on the entities found in WEBQUESTIONS and GRAPHQUESTIONS to select the top 10 non-overlapping entity disambiguation possibilities.,4.2 Implementation Details,[0],[0]
"We treat each possibility as a candidate input utterance, and use the perceptron score as a feature in the discriminative reranker, thus leaving the final disambiguation to the semantic parser.
",4.2 Implementation Details,[0],[0]
"Apart from the entity score, the discriminative ranker uses the following basic features.",4.2 Implementation Details,[0],[0]
The first feature is the likelihood score of a grounded representation aggregating all intermediate representations.,4.2 Implementation Details,[0],[0]
"The second set of features include the embedding similarity between the relation and the utterance, as well as the similarity between the relation and the question words.",4.2 Implementation Details,[0],[0]
"The last set of features includes the answer type as indicated by the last word in the Freebase relation (Xu et al., 2016).
",4.2 Implementation Details,[0],[0]
"We used the Adam optimizer for training with an initial learning rate of 0.001, two momentum parameters [0.99, 0.999], and batch size 1.",4.2 Implementation Details,[0],[0]
"The dimensions of the word embeddings, LSTM states, entity embeddings and relation embeddings are [50, 100, 100, 100].",4.2 Implementation Details,[0],[0]
"The word embeddings were initialized with Glove embeddings (Pennington et al., 2014).",4.2 Implementation Details,[0],[0]
All other embeddings were randomly initialized.,4.2 Implementation Details,[0],[0]
Experimental results on the four datasets are summarized in Tables 3–6.,4.3 Results,[0],[0]
"We present comparisons of our system which we call SCANNER (as a shorthand for SymboliC meANiNg rEpResentation) against a variety of models previously described in the literature.
",4.3 Results,[0],[0]
GEOQUERY results are shown in Table 5.,4.3 Results,[0],[0]
"The first block contains symbolic systems, whereas neural models are presented in the second block.",4.3 Results,[0],[0]
We report accuracy which is defined as the proportion of the utterance that are correctly parsed to their gold standard logical forms.,4.3 Results,[0],[0]
"All previous neural systems (Dong and Lapata, 2016; Jia and Liang, 2016) treat semantic parsing as a sequence transduction problem and use LSTMs to directly map utterances to logical forms.",4.3 Results,[0],[0]
"SCANNER yields performance improvements over these
4http://developers.google.com/ freebase/
systems when using comparable data sources for training.",4.3 Results,[0],[0]
"Jia and Liang (2016) achieve better results with synthetic data that expands GEOQUERY; we could adopt their approach to improve model performance, however, we leave this to future work.
",4.3 Results,[0],[0]
Table 6 reports SCANNER’s performance on SPADES.,4.3 Results,[0],[0]
"For all Freebase related datasets we use average F1 (Berant et al., 2013) as our evaluation metric.",4.3 Results,[0],[0]
Previous work on this dataset has used a semantic parsing framework similar to ours where natural language is converted to an intermediate syntactic representation and then grounded to Freebase.,4.3 Results,[0],[0]
"Specifically, Bisk et al. (2016) evaluate the effectiveness of four different CCG parsers on the semantic parsing task when varying the amount of supervision required.",4.3 Results,[0],[0]
"As can be seen, SCANNER outperforms all CCG variants (from unsupervised to fully supervised) without having access to any manually annotated derivations or lexicons.",4.3 Results,[0],[0]
"For fair comparison, we also built a neural baseline that encodes an utterance with a recurrent neural network and then predicts a grounded meaning representation directly (Ture and Jojic, 2016; Yih et al., 2016).",4.3 Results,[0],[0]
"Again, we observe that SCANNER outperforms this baseline.
",4.3 Results,[0],[0]
Results on WEBQUESTIONS are summarized in Table 3.,4.3 Results,[0],[0]
SCANNER obtains performance on par with the best symbolic systems (see the first block in the table).,4.3 Results,[0],[0]
"It is important to note that Bast and Haussmann (2015) develop a question answering system, which contrary to ours can-
not produce meaning representations whereas Berant and Liang (2015) propose a sophisticated agenda-based parser which is trained borrowing ideas from imitation learning.",4.3 Results,[0],[0]
SCANNER is conceptually similar to Reddy et al. (2016) who also learn a semantic parser via intermediate representations which they generate based on the output of a dependency parser.,4.3 Results,[0],[0]
SCANNER performs competitively despite not having access to any linguistically-informed syntactic structures.,4.3 Results,[0],[0]
The second block in Table 3 reports the results of several neural systems.,4.3 Results,[0],[0]
Xu et al. (2016) represent the state of the art on WEBQUESTIONS.,4.3 Results,[0],[0]
Their system uses Wikipedia to prune out erroneous candidate answers extracted from Freebase.,4.3 Results,[0],[0]
Our model would also benefit from a similar post-processing step.,4.3 Results,[0],[0]
"As in previous experiments, SCANNER outperforms the neural baseline, too.
",4.3 Results,[0],[0]
"Finally, Table 4 presents our results on GRAPHQUESTIONS.",4.3 Results,[0],[0]
"We report F1 for SCANNER, the neural baseline model, and three symbolic systems presented in Su et al. (2016).",4.3 Results,[0],[0]
SCANNER achieves a new state of the art on this dataset with a gain of 4.23 F1 points over the best previously reported model.,4.3 Results,[0],[0]
"Since a central feature of our parser is that it learns intermediate representations with natural language predicates, we conducted additional experiments in order to inspect their quality.",4.4 Analysis of Intermediate Representations,[0],[0]
"For GEOQUERY
which contains only 280 test examples, we manually annotated intermediate representations for the test instances and evaluated the learned representations against them.",4.4 Analysis of Intermediate Representations,[0],[0]
The experimental setup aims to shows how humans can participate in improving the semantic parser with feedback at the intermediate stage.,4.4 Analysis of Intermediate Representations,[0],[0]
"In terms of evaluation, we use three metrics shown in Table 7.",4.4 Analysis of Intermediate Representations,[0],[0]
The first row shows the percentage of exact matches between the predicted representations and the human annotations.,4.4 Analysis of Intermediate Representations,[0],[0]
"The second row refers to the percentage of structure matches, where the predicted representations have the same structure as the human annotations, but may not use the same lexical terms.",4.4 Analysis of Intermediate Representations,[0],[0]
"Among structurally correct predictions, we additionally compute how many tokens are correct, as shown in the third row.",4.4 Analysis of Intermediate Representations,[0],[0]
"As can be seen, the induced meaning representations overlap to a large extent with the human gold standard.
",4.4 Analysis of Intermediate Representations,[0],[0]
We also evaluated the intermediate representations created by SCANNER on the other three (Freebase) datasets.,4.4 Analysis of Intermediate Representations,[0],[0]
"Since creating a manual gold standard for these large datasets is time-consuming, we compared the induced representations against the output of a syntactic parser.",4.4 Analysis of Intermediate Representations,[0],[0]
"Specifically, we converted the questions to event-argument structures with EASYCCG (Lewis and Steedman, 2014), a high coverage and high accuracy CCG parser.",4.4 Analysis of Intermediate Representations,[0],[0]
EASYCCG extracts predicate-argument structures with a labeled F-score of 83.37%.,4.4 Analysis of Intermediate Representations,[0],[0]
"For further comparison, we built a simple baseline which identifies predicates based on the output of the Stanford POStagger (Manning et al., 2014) following the ordering VBD VBN VB VBP VBZ MD.
",4.4 Analysis of Intermediate Representations,[0],[0]
"As shown in Table 8, on SPADES and WEBQUESTIONS, the predicates learned by our model match the output of EASYCCG more closely than the heuristic baseline.",4.4 Analysis of Intermediate Representations,[0],[0]
"But for GRAPHQUESTIONS which contains more compositional questions, the mismatch is higher.",4.4 Analysis of Intermediate Representations,[0],[0]
"However, since the key idea of our model is to capture salient meaning for the task at hand rather than strictly obey syntax, we would not expect the
predicates induced by our system to entirely agree with those produced by the syntactic parser.",4.4 Analysis of Intermediate Representations,[0],[0]
"To further analyze how the learned predicates differ from syntax-based ones, we grouped utterances in SPADES into four types of linguistic constructions: coordination (conj), control and raising (control), prepositional phrase attachment (pp), and subordinate clauses (subord).",4.4 Analysis of Intermediate Representations,[0],[0]
"Table 8 also shows the breakdown of matching scores per linguistic construction, with the number of utterances in each type.",4.4 Analysis of Intermediate Representations,[0],[0]
"In Table 9, we provide examples of predicates identified by SCANNER, indicating whether they agree or not with the output of EASYCCG.",4.4 Analysis of Intermediate Representations,[0],[0]
"As a reminder, the task in SPADES is to predict the entity masked by a blank symbol ( ).
",4.4 Analysis of Intermediate Representations,[0],[0]
"As can be seen in Table 8, the matching score is relatively high for utterances involving coordination and prepositional phrase attachments.",4.4 Analysis of Intermediate Representations,[0],[0]
"The model will often identify informative predicates (e.g., nouns) which do not necessarily agree with linguistic intuition.",4.4 Analysis of Intermediate Representations,[0],[0]
"For example, in the utterance wilhelm maybach and his son started maybach in 1909 (see Table 9), SCANNER identifies the predicateargument structure son(wilhelm maybach) rather than started(wilhelm maybach).",4.4 Analysis of Intermediate Representations,[0],[0]
We also observed that the model struggles with control and subordinate constructions.,4.4 Analysis of Intermediate Representations,[0],[0]
"It has difficulty distinguishing control from raising predicates as exemplified in the utterance ceo john thain agreed to leave from Table 9, where it identifies the raising predicate agreed.",4.4 Analysis of Intermediate Representations,[0],[0]
"For subordinate clauses, SCANNER tends to take shortcuts identifying as predicates words closest to the blank symbol.",4.4 Analysis of Intermediate Representations,[0],[0]
We presented a neural semantic parser which converts natural language utterances to grounded meaning representations via intermediate predicate-argument structures.,5 Discussion,[0],[0]
"Our model
essentially jointly learns how to parse natural language semantics and the lexicons that help grounding.",5 Discussion,[0],[0]
"Compared to previous neural semantic parsers, our model is more interpretable as the intermediate structures are useful for inspecting what the model has learned and whether it matches linguistic intuition.
",5 Discussion,[0],[0]
An assumption our model imposes is that ungrounded and grounded representations are structurally isomorphic.,5 Discussion,[0],[0]
An advantage of this assumption is that tokens in the ungrounded and grounded representations are strictly aligned.,5 Discussion,[0],[0]
"This allows the neural network to focus on parsing and lexical mapping, sidestepping the challenging structure mapping problem which would result in a larger search space and higher variance.",5 Discussion,[0],[0]
"On the negative side, the structural isomorphism assumption restricts the expressiveness of the model, especially since one of the main benefits of adopting a two-stage parser is the potential of capturing domain-independent semantic information via the intermediate representation.",5 Discussion,[0],[0]
"While it would be challenging to handle drastically non-isomorphic structures in the current model, it is possible to perform local structure matching, i.e., when the mapping between natural language and domainspecific predicates is many-to-one or one-to-many.
",5 Discussion,[0],[0]
"For instance, Freebase does not contain a relation representing daughter, using instead two relations representing female and child.",5 Discussion,[0],[0]
"Previous work (Kwiatkowski et al., 2013) models such cases by introducing collapsing (for many-to-one mapping) and expansion (for one-to-many mapping) operators.",5 Discussion,[0],[0]
"Within our current framework, these two types of structural mismatches can be handled with semi-Markov assumptions (Sarawagi and Cohen, 2005; Kong et al., 2016) in the parsing (i.e., predicate selection) and the grounding steps, respectively.",5 Discussion,[0],[0]
"Aside from relaxing strict isomorphism, we would also like to perform crossdomain semantic parsing where the first stage of the semantic parser is shared across domains.
",5 Discussion,[0],[0]
"Acknowledgments We would like to thank three anonymous reviewers, members of the Edinburgh ILCC and the IBM Watson, and Abulhair Saparov for feedback.",5 Discussion,[0],[0]
The support of the European Research Council under award number 681760 “Translating Multiple Modalities into Text” is gratefully acknowledged.,5 Discussion,[0],[0]
We introduce a neural semantic parser which is interpretable and scalable.,abstractText,[0],[0]
"Our model converts natural language utterances to intermediate, domain-general natural language representations in the form of predicate-argument structures, which are induced with a transition system and subsequently mapped to target domains.",abstractText,[0],[0]
The semantic parser is trained end-to-end using annotated logical forms or their denotations.,abstractText,[0],[0]
We achieve the state of the art on SPADES and GRAPHQUESTIONS and obtain competitive results on GEOQUERY and WEBQUESTIONS.,abstractText,[0],[0]
The induced predicate-argument structures shed light on the types of representations useful for semantic parsing and how these are different from linguistically motivated ones.1,abstractText,[0],[0]
Learning Structured Natural Language Representations for Semantic Parsing,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 47–57, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"This paper studies and extends previous work using the structured perceptron (Collins, 2002) for complex NLP tasks.",1 Introduction,[0],[0]
"We show that for the task of coreference resolution the straightforward combination of beam search and early update (Collins and Roark, 2004) falls short of more limited feature sets that allow for exact search.",1 Introduction,[0],[0]
"This contrasts with previous work on, e.g., syntactic parsing (Collins and Roark, 2004; Huang, 2008; Zhang and Clark, 2008) and linearization (Bohnet et al., 2011), and even simpler structured prediction problems, where early updates are not even necessary, such as part-of-speech tagging (Collins, 2002) and named entity recognition (Ratinov and Roth, 2009).
",1 Introduction,[0],[0]
The main reason why early updates underperform in our setting is that the task is too difficult and that the learning algorithm is not able to profit from all training data.,1 Introduction,[0],[0]
"Put another way, early updates happen too early, and the learning algorithm rarely reaches the end of the instances as it halts, updates, and moves on to the next instance.
",1 Introduction,[0],[0]
"An alternative would be to continue decoding the same instance after the early updates,
which is equivalent to Learning as Search Optimization (LaSO; Daumé III and Marcu (2005b)).",1 Introduction,[0],[0]
The learning task we are tackling is however further complicated since the target structure is under-determined by the gold standard annotation.,1 Introduction,[0],[0]
"Coreferent mentions in a document are usually annotated as sets of mentions, where all mentions in a set are coreferent.",1 Introduction,[0],[0]
"We adopt the recently popularized approach of inducing a latent structure within these sets (Fernandes et al., 2012; Chang et al., 2013; Durrett and Klein, 2013).",1 Introduction,[0],[0]
"This approach provides a powerful boost to the performance of coreference resolvers, but we find that it does not combine well with the LaSO learning strategy.",1 Introduction,[0],[0]
"We therefore propose a modification to LaSO, which delays updates until after each instance.",1 Introduction,[0],[0]
"The combination of this modification with non-local features leads to further improvements in the clustering accuracy, as we show in evaluation results on all languages from the CoNLL 2012 Shared Task – Arabic, Chinese, and English.",1 Introduction,[0],[0]
We obtain the best results to date on these data sets.1,1 Introduction,[0],[0]
Coreference resolution is the task of grouping referring expressions (or mentions) in a text into disjoint clusters such that all mentions in a cluster refer to the same entity.,2 Background,[0],[0]
"An example is given in Figure 1 below, where mentions from two clusters are marked with brackets:
1Our system is available at http://www.ims.",2 Background,[0],[0]
"uni-stuttgart.de/˜anders/coref.html
47
",2 Background,[0],[0]
"In recent years much work on coreference resolution has been devoted to increasing the expressivity of the classical mention-pair model, in which each coreference classification decision is limited to information about two mentions that make up a pair.",2 Background,[0],[0]
"This shortcoming has been addressed by entity-mention models, which relate a candidate mention to the full cluster of mentions predicted to be coreferent so far (for more discussion on the model types, see, e.g., (Ng, 2010)).
",2 Background,[0],[0]
"Nevertheless, the two best systems in the latest CoNLL Shared Task on coreference resolution (Pradhan et al., 2012) were both variants of the mention-pair model.",2 Background,[0],[0]
"While the second best system (Björkelund and Farkas, 2012) followed the widely used baseline of Soon et al. (2001), the winning system (Fernandes et al., 2012) proposed the use of a tree representation.
",2 Background,[0],[0]
The tree-based model of Fernandes et al. (2012) construes the representation of coreference clusters as a rooted tree.,2 Background,[0],[0]
Figure 2 displays an example tree over the clusters from Figure 1.,2 Background,[0],[0]
"Every mention corresponds to a node in the tree, and arcs between mentions indicate that they are coreferent.",2 Background,[0],[0]
The tree additionally has a dummy root node.,2 Background,[0],[0]
"Every subtree under the root node corresponds to a cluster of coreferent mentions.
",2 Background,[0],[0]
"Since coreference training data is typically not annotated with trees, Fernandes et al. (2012) proposed the use of latent trees that are induced during the training phase of a coreference resolver.",2 Background,[0],[0]
"The latent tree provides more meaningful antecedents for training.2 For instance, the popular pair-wise instance creation method suggested by Soon et al. (2001) assumes non-branching trees, where the antecedent of every mention is its linear predecessor (i.e., heb2 is the antecedent of Gary Wilberb3).",2 Background,[0],[0]
"Comparing the two alternative antecedents of Gary Wilberb3 , the tree in Figure 2 provides a more reliable basis for training a coreference resolver, as the two mentions of Gary Wilber are both proper names and have an exact string match.",2 Background,[0],[0]
"LetM = {m0,m1, ...,mn} denote the set of mentions in a document, including the artificial root mention (denoted by m0).",3 Representation and Learning,[0],[0]
"We assume that the
2We follow standard practice and overload the terms anaphor and antecedent to be any type of mention, i.e., names as well as pronouns.",3 Representation and Learning,[0],[0]
"An antecedent is simply the mention to the left of the anaphor.
",3 Representation and Learning,[0],[0]
"mentions are ordered ascendingly with respect to the linear order of the document, where the document root precedes",3 Representation and Learning,[0],[0]
"all other mentions.3 For each mention mj , let Aj denote the set of potential antecedents.",3 Representation and Learning,[0],[0]
"That is, the set of all mentions that precede mj according to the linear order including the root node, or, Aj = {mi | i < j}.",3 Representation and Learning,[0],[0]
"Finally, let A denote the set of all antecedent sets {A0, A1, ..., An}.
",3 Representation and Learning,[0],[0]
"In the tree model, each mention corresponds to a node, and an antecedent-anaphor pair 〈ai,mi〉, where ai ∈ Ai, corresponds to a directed edge (or arc) pointing from antecedent to anaphor.
",3 Representation and Learning,[0],[0]
"The score of an arc 〈ai,mi〉 is defined as the scalar product between a weight vector w and a feature vector Φ(〈ai,mi〉), where Φ is a feature extraction function over an arc (thus extracting features from the antecedent and the anaphor).",3 Representation and Learning,[0],[0]
"The score of a coreference tree y = {〈a1,m1〉, 〈a2,m2〉, ..., 〈an,mn〉} is defined as the sum of the scores of all the mention pairs:
score(〈ai,mi〉) = w · Φ(〈ai,mi〉) (1) score(y) = ∑ 〈ai,mi〉∈y score(〈ai,mi〉)
",3 Representation and Learning,[0],[0]
The objective is to find the output ŷ,3 Representation and Learning,[0],[0]
"that maximizes the scoring function:
ŷ = arg max y∈Y(A) score(y) (2)
where Y(A) denotes the set of possible trees given the antecedent sets A. By treating the mentions as nodes in a directed graph and assigning scores to the arcs according to (1), Fernandes et al. (2012) solved the search problem using the Chu-LiuEdmonds (CLE) algorithm (Chu and Liu, 1965;
3We impose a total order on mentions.",3 Representation and Learning,[0],[0]
"In case of nested mentions, the mention that begins first is assumed to precede the embedded one.",3 Representation and Learning,[0],[0]
"If two mentions begin at the same token, the longer one is taken to precede the shorter one.
",3 Representation and Learning,[0],[0]
"Edmonds, 1967), which is a maximum spanning tree algorithm that finds the optimal tree over a connected directed graph.",3 Representation and Learning,[0],[0]
"CLE, however, has the drawback that the scores of the arcs must remain fixed and can not change depending on other arcs and it is not clear how to include non-local features in a CLE decoder.",3 Representation and Learning,[0],[0]
"We find the weight vector w by online learning using a variant of the structured perceptron (Collins, 2002).",3.1 Online learning,[0],[0]
"Specifically, we use the passive-aggressive (PA) algorithm (Crammer et al., 2006), since we found that this performed slightly better in preliminary experiments.4
The structured perceptron iterates over training instances 〈xi, yi〉, where xi are inputs and yi are outputs.",3.1 Online learning,[0],[0]
For each instance it uses the current weight vector w to make a prediction ŷi given the input xi.,3.1 Online learning,[0],[0]
"If the prediction is incorrect, the weight vector is updated in favor of the correct structure.",3.1 Online learning,[0],[0]
Otherwise the weight vector is left untouched.,3.1 Online learning,[0],[0]
In our setting inputs xi correspond to documents and outputs yi are trees over mentions in a document.,3.1 Online learning,[0],[0]
"The training data is, however, not annotated with trees, but only with clusters of mentions.",3.1 Online learning,[0],[0]
"That is, the yi’s are not defined a priori.",3.1 Online learning,[0],[0]
"In order to have a tree structure to update against, we use the current weight vector and apply the decoder to a constrained antecedent set and obtain a latent tree over the mentions in a document, where each mention is assigned a single correct antecedent (Fernandes et al., 2012).",3.2 Latent antecedents,[0],[0]
We constrain the antecedent sets such that only trees that correspond to the correct clustering can be built.,3.2 Latent antecedents,[0],[0]
"Specifically, let Ãj denote the set of correct antecedents for a mention mj , or
Ãj",3.2 Latent antecedents,[0],[0]
=,3.2 Latent antecedents,[0],[0]
"{ {m0} if mj has no correct antecedent {ai | COREF(ai,mj), ai ∈ Aj} otherwise
that is, if mention mj is non-referential or the first mention of its cluster, Ãj contains only the document root.",3.2 Latent antecedents,[0],[0]
Otherwise it is the set of all mentions to the left that belong to the same cluster as mj .,3.2 Latent antecedents,[0],[0]
"Analogously to A, let Ã denote the set of constrained antecedent sets.",3.2 Latent antecedents,[0],[0]
"The latent tree ỹ needed
4We also implement the feature mapping function Φ as a hash kernel (Bohnet, 2010) and apply averaging (Collins, 2002), though for brevity we omit this from the pseudocode.
for updates is then defined to be the optimal tree over Y(Ã), subject to the current weight vector:
ỹ = arg max y∈Y(Ã) score(y)
",3.2 Latent antecedents,[0],[0]
"The intuition behind the latent tree is that during online learning, the weight vector will start favoring latent trees that are easier to learn (such as the one in Figure 2).
",3.2 Latent antecedents,[0],[0]
"Algorithm 1 PA algorithm with latent trees Input: Training data D, number of iterations T Output: Weight vector w 1: w = −→ 0
2: for t ∈ 1..T do 3: for 〈Mi,Ai, Ãi〉 ∈ D",3.2 Latent antecedents,[0],[0]
do 4: ŷi = arg maxY(A) score(y) .,3.2 Latent antecedents,[0],[0]
Predict 5: if ¬CORRECT(ŷi) then 6: ỹi = arg maxY(Ã) score(y) .,3.2 Latent antecedents,[0],[0]
Latent tree 7: ∆,3.2 Latent antecedents,[0],[0]
= Φ(ŷi)− Φ(ỹi) 8: τ = ∆·w+LOSS(ŷi)‖∆‖2 .,3.2 Latent antecedents,[0],[0]
PA weight 9: w = w + τ∆ .,3.2 Latent antecedents,[0],[0]
"PA update
10: return w
Algorithm 1 shows pseudocode for the learning algorithm, which we will refer to as the baseline learning algorithm.",3.2 Latent antecedents,[0],[0]
"Instead of looping over pairs 〈x, y〉 of documents and trees, it loops over triples 〈M,A, Ã〉 that comprise the set of mentions M and the two sets of antecedent candidates (line 3).",3.2 Latent antecedents,[0],[0]
"Moreover, rather than checking that the tree is identical to the latent tree, it only requires the tree to correctly encode the gold clustering (line 5).",3.2 Latent antecedents,[0],[0]
The update that occurs in lines 7-9 is the passive-aggressive update.,3.2 Latent antecedents,[0],[0]
A loss function LOSS that quantifies the error in the prediction is used to compute a scalar τ that controls how much the weights are moved in each update.,3.2 Latent antecedents,[0],[0]
"If τ is set to 1, the update reduces to the standard structured perceptron update.",3.2 Latent antecedents,[0],[0]
The loss function can be an arbitrarily complex function that returns a numerical value of how bad the prediction is.,3.2 Latent antecedents,[0],[0]
"In the simplest case, Hamming loss can be used, i.e., for each incorrect arc add 1.",3.2 Latent antecedents,[0],[0]
"We follow Fernandes et al. (2012) and penalize erroneous root attachments, i.e., mentions that erroneously get the root node as their antecedent, with a loss of 1.5.",3.2 Latent antecedents,[0],[0]
For all other arcs we use Hamming loss.,3.2 Latent antecedents,[0],[0]
"We now show that the search problem in (2) can equivalently be solved by the more intuitive bestfirst decoder (Ng and Cardie, 2002), rather than using the CLE decoder.",4 Incremental Search,[0],[0]
"The best-first decoder
works incrementally by making a left-to-right pass over the mentions, selecting for each mention the highest scoring antecedent.
",4 Incremental Search,[0],[0]
"The key aspect that makes the best-first decoder equivalent to the CLE decoder is that all arcs point from left to right, both in this paper and in the work of Fernandes et al. (2012).",4 Incremental Search,[0],[0]
"We sketch a proof that this decoder also returns the highest scoring tree.
",4 Incremental Search,[0],[0]
"First, note that this algorithm indeed returns a tree.",4 Incremental Search,[0],[0]
"This can be shown by assuming the opposite, in which case the tree has to have a cycle.",4 Incremental Search,[0],[0]
Then there must be a mention that has its antecedent to the right.,4 Incremental Search,[0],[0]
"Though this is not possible since all arcs point from left to right.
",4 Incremental Search,[0],[0]
"Second, this tree is the highest scoring tree.",4 Incremental Search,[0],[0]
"Again, assume the contrary, i.e., that there is a higher scoring tree in Y(A).",4 Incremental Search,[0],[0]
This implies that for some mention there is a higher scoring antecedent than the one selected by the decoder.,4 Incremental Search,[0],[0]
This contradicts the fact that the best-first decoder selects the highest scoring antecedent for each mention.5,4 Incremental Search,[0],[0]
"Since the best-first decoder makes a left-to-right pass, it is possible to extract features on the partial structure on the left.",5 Introducing Non-local Features,[0],[0]
"Such non-local features are able to capture information beyond that of a mention and its potential antecedent, e.g., the size of a partially built cluster, or features extracted from the antecedent of the antecedent.
",5 Introducing Non-local Features,[0],[0]
"When only local features are used, greedy search (either with CLE or the best-first decoder) suffices to find the highest scoring tree.",5 Introducing Non-local Features,[0],[0]
"That is, greedy search provides an exact solution to equation 2.",5 Introducing Non-local Features,[0],[0]
"Non-local features, however, render the exact search problem intractable.",5 Introducing Non-local Features,[0],[0]
"This is because with non-local features, locally suboptimal (i.e., non-greedy) antecedents for some mentions may lead to a higher total score over a whole document.
",5 Introducing Non-local Features,[0],[0]
"In order to keep some options around during search, we extend the best-first decoder with beam search.",5 Introducing Non-local Features,[0],[0]
Beam search works incrementally by keeping an agenda of state items.,5 Introducing Non-local Features,[0],[0]
"At each step, all items on the agenda are expanded.",5 Introducing Non-local Features,[0],[0]
The subset of size k (the beam size) of the highest scoring expansions are retained and put back into the agenda for the next step.,5 Introducing Non-local Features,[0],[0]
"The feature extraction function Φ
5In case there are multiple maximum spanning trees, the best-first decoder will return one of them.",5 Introducing Non-local Features,[0],[0]
This also holds for the CLE algorithm.,5 Introducing Non-local Features,[0],[0]
"With proper definitions, the proof can be constructed to show that both search algorithms return trees belonging to the set of maximum spanning trees over a graph.
",5 Introducing Non-local Features,[0],[0]
"is also extended such that it also receives the current state s as an argument: Φ(〈mi,mj〉, s).",5 Introducing Non-local Features,[0],[0]
"The state encodes the previous decisions and enables Φ to extract features from the partial tree on the left.
",5 Introducing Non-local Features,[0],[0]
We now outline three different ways of learning the weight vector w with non-local features.,5 Introducing Non-local Features,[0],[0]
"The beam search decoder can be plugged into the training algorithm, replacing the calls to arg max.",5.1 Early updates,[0],[0]
"Since state items leading to the best tree may be pruned from the agenda before the decoder reaches the end of the document, the introduction of non-local features may cause the decoder to return a non-optimal tree.",5.1 Early updates,[0],[0]
This is problematic as it might cause updates although the correct tree has a higher score than the predicted one.,5.1 Early updates,[0],[0]
"It has previously been observed (Huang et al., 2012) that substantial gains can be made by applying an early update strategy (Collins and Roark, 2004): if the correct item is pruned before reaching the end of the document, then stop and update.
",5.1 Early updates,[0],[0]
"While beam search and early updates have been successfully applied to other NLP applications, our task differs in two important aspects: First, coreference resolution is a much more difficult task, which relies on more (world) knowledge than what is available in the training data.",5.1 Early updates,[0],[0]
"In other words, it is unlikely that we can devise a feature set that is informative enough to allow the weight vector to converge towards a solution that lets the learning algorithm see the entire documents during training, at least in the situation when no external knowledge sources are used.
",5.1 Early updates,[0],[0]
"Second, our gold structure is not known but is induced latently, and may vary from iteration to iteration.",5.1 Early updates,[0],[0]
"With non-local features this is troublesome since the best latent tree of a complete document may not necessarily coincide with the best partial tree at some intermediate mentionmj , j < n, i.e., a mention before the last in a document.",5.1 Early updates,[0],[0]
"We therefore also apply beam search to find the latent tree to have a partial gold structure for every mention in a document.
",5.1 Early updates,[0],[0]
Algorithm 2 shows pseudocode for the beam search and early update training procedure.,5.1 Early updates,[0],[0]
"The algorithm maintains two parallel agendas, one for gold items and one for predicted items.",5.1 Early updates,[0],[0]
"At every mention, both agendas are expanded and thus cover the same set of mentions.",5.1 Early updates,[0],[0]
"Then the predicted agenda is checked to see if it contains any correct
Algorithm 2 Beam search and early update",5.1 Early updates,[0],[0]
"Input: Data set D, epochs T , beam size k Output: weight vector w 1: w = −→ 0
2: for t ∈ 1..T do 3: for 〈Mi,Ai, Ãi〉 ∈ D",5.1 Early updates,[0],[0]
do 4: AgendaG = {} 5: AgendaP,5.1 Early updates,[0],[0]
= {} 6: for j ∈ 1..n do 7:,5.1 Early updates,[0],[0]
"AgendaG = EXPAND(AgendaG , Ãj ,mj , k) 8: AgendaP = EXPAND(AgendaP , Aj ,mj , k) 9: if ¬CONTAINSCORRECT(AgendaP ) then
10: ỹ = EXTRACTBEST(AgendaG) 11: ŷ = EXTRACTBEST(AgendaP ) 12: update .",5.1 Early updates,[0],[0]
PA update 13: GOTO 3 .,5.1 Early updates,[0],[0]
Skip and move to next instance 14: ŷ = EXTRACTBEST(AgendaP ) 15,5.1 Early updates,[0],[0]
: if ¬CORRECT(ŷ) then 16: ỹ = EXTRACTBEST(AgendaG) 17: update .,5.1 Early updates,[0],[0]
"PA update
item.",5.1 Early updates,[0],[0]
"If there is no correct item in the predicted agenda, search is halted and an update is made against the best item from the gold agenda.",5.1 Early updates,[0],[0]
The algorithm then moves on to the next document.,5.1 Early updates,[0],[0]
"If the end of a document is reached, the top scoring predicted item is checked for correctness.",5.1 Early updates,[0],[0]
"If it is not, an update is made against the best gold item.
",5.1 Early updates,[0],[0]
"A drawback of early updates is that the remainder of the document is skipped when an early update is applied, effectively discarding some training data.6 An alternative strategy that makes better use of the training data is to apply the maxviolation procedure suggested by Huang et al. (2012).",5.1 Early updates,[0],[0]
"However, since our gold trees change from iteration to iteration, and even inside of a single document, it is not entirely clear with respect to what gold tree the maximum violation should be computed.",5.1 Early updates,[0],[0]
"Initial experiments with max-violation updates indicated that they did not improve much over early updates, and also had a tendency to only consider a smaller portion of the training data.",5.1 Early updates,[0],[0]
"To make full use of the training data we implemented Learning as Search Optimization (LaSO; Daumé III and Marcu, 2005b).",5.2 LaSO,[0],[0]
"It is very similar to early updates, but differs in one crucial respect: When an early update is made, search is continued rather than aborted.",5.2 LaSO,[0],[0]
"Thus the learning algorithm always reaches the end of a document, avoiding the problem that early updates discard parts of the training data.
",5.2 LaSO,[0],[0]
"6In fact, after 50 iterations about 70% of the mentions in the training data are still being ignored due to early updates.
",5.2 LaSO,[0],[0]
"Correct items are computed the same way as with early updates, where an agenda of gold items is maintained in parallel.",5.2 LaSO,[0],[0]
"When search is resumed after an intermediate LaSO update, the prediction agenda is re-seeded with gold items (i.e., items that are all correct).",5.2 LaSO,[0],[0]
"This is necessary since the update influences what the partial gold structure looks like, and the gold agenda therefore needs to be recreated from the beginning of the document.",5.2 LaSO,[0],[0]
"Specifically, after each intermediate LaSO update, the gold agenda is expanded repeatedly from the beginning of the document to the point where the update was made, and is then copied over to seed the prediction agenda.",5.2 LaSO,[0],[0]
"In terms of pseudocode, this is accomplished by replacing lines 12 and 13 in Algorithm 2 with the following: 12: update .",5.2 LaSO,[0],[0]
"PA update 13: AgendaG = {} 14: for mi ∈ {m1, ...,mj} .",5.2 LaSO,[0],[0]
"Recreate gold agenda 15: AgendaG = EXPAND(AgendaG , Ãi,mi, k) 16: AgendaP = COPY(AgendaG) 17: GOTO 6 .",5.2 LaSO,[0],[0]
Continue,5.2 LaSO,[0],[0]
"When we applied LaSO, we noticed that it performed worse than the baseline learning algorithm when only using local features.",5.3 Delayed LaSO updates,[0],[0]
We believe that the reason is that updates are made in the middle of documents which means that lexical forms of antecedents are “fresh in memory” of the weight vector.,5.3 Delayed LaSO updates,[0],[0]
This results in fewer mistakes during training and leads to fewer updates.,5.3 Delayed LaSO updates,[0],[0]
"While this feedback makes it easier during training, such feedback is not available during test time, and the LaSO learning setting therefore mimics the testing setting to a lesser extent.
",5.3 Delayed LaSO updates,[0],[0]
We also found that LaSO updates change the shape of the latent tree and that the average distance between mentions connected by an arc increased.,5.3 Delayed LaSO updates,[0],[0]
This problem can also be attributed to how lexical items are fresh in memory.,5.3 Delayed LaSO updates,[0],[0]
Such trees tend to deviate from the intuition that the latent trees are easier to learn.,5.3 Delayed LaSO updates,[0],[0]
"They also render distancebased features (which are standard practice and generally rather useful) less powerful, as distance in sentences or mentions becomes less of a reliable indicator for coreference.
",5.3 Delayed LaSO updates,[0],[0]
"To cope with this problem, we devised the delayed LaSO update, which differs from LaSO only in the respect that it postpones the actual updates until the end of a document.",5.3 Delayed LaSO updates,[0],[0]
This is accomplished by summing the distance vectors ∆ at every point where LaSO would make an update.,5.3 Delayed LaSO updates,[0],[0]
"At
Algorithm 3 Delayed LaSO update Input: Data set D, iterations T , beam size k Output: weight vector w 1: w = −→ 0
2: for t ∈ 1..T do 3: for 〈Mi,Ai, Ãi〉 ∈ D",5.3 Delayed LaSO updates,[0],[0]
do 4: AgendaG = {} 5: AgendaP,5.3 Delayed LaSO updates,[0],[0]
= {} 6: ∆acc = −→ 0 7: lossacc = 0 8: for j ∈ 1..n,5.3 Delayed LaSO updates,[0],[0]
"do 9: AgendaG = EXPAND(AgendaG , Ãj ,mj , k) 10: AgendaP = EXPAND(AgendaP , Aj ,mj , k) 11: if ¬CONTAINSCORRECT(AgendaP ) then 12: ỹ = EXTRACTBEST(AgendaG) 13: ŷ = EXTRACTBEST(AgendaP ) 14: ∆acc = ∆acc + Φ(ŷ)− Φ(ỹ) 15: lossacc = lossacc + LOSS(ŷ) 16: AgendaP",5.3 Delayed LaSO updates,[0],[0]
= AgendaG 17: ŷ = EXTRACTBEST(AgendaP ) 18: if ¬CORRECT(ŷ) then 19: ỹ = EXTRACTBEST(AgendaG) 20: ∆acc = ∆acc + Φ(ŷ)− Φ(ỹ) 21: lossacc = lossacc + LOSS(ŷ) 22: if ∆acc 6=,5.3 Delayed LaSO updates,[0],[0]
"−→0 then 23: update w.r.t. ∆acc and lossacc
the end of a document, an update is made with respect to the sum of all ∆’s.",5.3 Delayed LaSO updates,[0],[0]
"Similarly, a running sum of the partial loss is maintained within a document.",5.3 Delayed LaSO updates,[0],[0]
"Since the PA update only depends on the distance vector ∆ and the loss, it can be applied with respect to these sums at the end of the document.",5.3 Delayed LaSO updates,[0],[0]
"When only local features are used, this update is equivalent to the updates in the baseline learning algorithm.",5.3 Delayed LaSO updates,[0],[0]
This follows because greedy search finds the optimal tree when only local features are used.,5.3 Delayed LaSO updates,[0],[0]
"Similarly, using only local features, the beam-based best-first decoder will also return the optimal tree.",5.3 Delayed LaSO updates,[0],[0]
Algorithm 3 shows the pseudocode for the delayed LaSO learning algorithm.,5.3 Delayed LaSO updates,[0],[0]
In this section we briefly outline the type of features we use.,6 Features,[0],[0]
The feature sets are customized for each language.,6 Features,[0],[0]
"As a baseline we use the features from Björkelund and Farkas (2012), who ranked second in the 2012 CoNLL shared task and is publicly available.",6 Features,[0],[0]
The exact definitions and feature sets that we use are available as part of the download package of our system.,6 Features,[0],[0]
"Basic features that can be extracted on one or both mentions in a pair include (among others): Mention type, which is either root, pro-
noun, name, or common; Distance features, e.g., the distance in sentences or mentions; Rule-based features, e.g., StringMatch or SubStringMatch; Syntax-based features, e.g., category labels or paths in the syntax tree; Lexical features, e.g., the head word of a mention or the last word of a mention.
",6.1 Local features,[0],[0]
"In order to have a strong local baseline, we applied greedy forward/backward feature selection on the training data using a large set of local feature templates.",6.1 Local features,[0],[0]
"Specifically, the training set of each language was split into two parts where 75% was used for training, and 25% for testing.",6.1 Local features,[0],[0]
"Feature templates were incrementally added or removed in order to optimize the mean of MUC, B3, and CEAFe (i.e., the CoNLL average).",6.1 Local features,[0],[0]
"We experimented with non-local features drawn from previous work on entity-mention models (Luo et al., 2004; Rahman and Ng, 2009), however they did not improve performance in preliminary experiments.",6.2 Non-local Features,[0],[0]
"The one exception is the size of a cluster (Culotta et al., 2007).",6.2 Non-local Features,[0],[0]
Additional features we use are Shape encodes the linear “shape” of a cluster in terms of mention type.,6.2 Non-local Features,[0],[0]
"For instance, the clusters representing Gary Wilber and Drug Emporium Inc. from the example in Figure 1, would be represented as RNPN and RNCCC, respectively.",6.2 Non-local Features,[0],[0]
"Where R, N, P, and C denote the root node, names, pronouns, and common noun phrases, respectively.",6.2 Non-local Features,[0],[0]
"Local syntactic context is inspired by the Entity Grid (Barzilay and Lapata, 2008), where the basic assumption is that references to an entity follow particular syntactic patterns.",6.2 Non-local Features,[0],[0]
"For instance, an entity may be introduced as an object in one sentence, whereas in subsequent sentences it is referred to in subject position.",6.2 Non-local Features,[0],[0]
Grammatical functions are approximated by the path in the syntax tree from a mention to its closest S node.,6.2 Non-local Features,[0],[0]
"The partial paths of a mention and its linear predecessor, given the cluster of the current antecedent, informs the model about the local syntactic context.",6.2 Non-local Features,[0],[0]
"Cluster start distance denotes the distance in mentions from the beginning of the document where the cluster of the antecedent in consideration begins.
",6.2 Non-local Features,[0],[0]
"Additionally, the non-local model also has access to the basic properties of other mentions in the partial tree structure, such as head words.",6.2 Non-local Features,[0],[0]
"The
non-local features were selected with the same greedy forward strategy as the local features, starting from the optimized local feature sets.",6.2 Non-local Features,[0],[0]
"We apply our model to the CoNLL 2012 Shared Task data, which includes a training, development, and test set split for three languages: Arabic, Chinese and English.",7 Experimental Setup,[0],[0]
"We follow the closed track setting where systems may only be trained on the provided training data, with the exception of the English gender and number data compiled by Bergsma and Lin (2006).",7 Experimental Setup,[0],[0]
"We use automatically extracted mentions using the same mention extraction procedure as Björkelund and Farkas (2012).
",7 Experimental Setup,[0],[0]
"We evaluate our system using the CoNLL 2012 scorer, which computes several coreference metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFe and CEAFm (Luo, 2005).",7 Experimental Setup,[0],[0]
"We also report the CoNLL average (also known as MELA; Denis and Baldridge (2009)), i.e., the arithmetic mean of MUC, B3, and CEAFe.",7 Experimental Setup,[0],[0]
"It should be noted that for B3 and the CEAF metrics, multiple ways of handling twinless mentions7 have been proposed (Rahman and Ng, 2009; Stoyanov et al., 2009).",7 Experimental Setup,[0],[0]
"We use the most recent version of the CoNLL scorer (version 7), which implements the original definitions of these metrics.8
Our system is evaluated on the version of the data with automatic preprocessing information (e.g., predicted parse trees).",7 Experimental Setup,[0],[0]
Unless otherwise stated we use 25 iterations of perceptron training and a beam size of 20.,7 Experimental Setup,[0],[0]
We did not attempt to tune either of these parameters.,7 Experimental Setup,[0],[0]
"We experiment with two feature sets for each language: the optimized local feature sets (denoted local), and the optimized local feature sets extended with non-local features (denoted non-local).",7 Experimental Setup,[0],[0]
Learning strategies.,8 Results,[0],[0]
We begin by looking at the different learning strategies.,8 Results,[0],[0]
"Since early updates do not always make use of the complete documents during training, it can be expected that it will require either a very wide beam or more iterations to get up to par with the baseline learning algorithm.",8 Results,[0],[0]
"Figure 3 shows the CoNLL average on
7i.e., mentions that appear in the prediction but not in gold, or the other way around
8Available at http://conll.cemantix.org/ 2012/software.html
the English development set as a function of number of training iterations with two different beam sizes, 20 and 100, over the local and non-local feature sets.",8 Results,[0],[0]
"The figure shows that even after 50 iterations, early update falls short of the baseline, even when the early update system has access to more informative non-local features.9
In Figure 4 we compare early update with LaSO and delayed LaSO on the English development set.",8 Results,[0],[0]
"The left half uses the local feature set, and the right the extended non-local feature set.",8 Results,[0],[0]
"Recall that with only local features, delayed LaSO is equivalent to the baseline learning algorithm.",8 Results,[0],[0]
"As before, early update is considerably worse than other learning strategies.",8 Results,[0],[0]
"We also see that delayed LaSO outperforms LaSO, both with and without non-local features.",8 Results,[0],[0]
"Note that plain LaSO with non-local features only barely outperforms the delayed LaSO with only local features (i.e., the baseline), which indicates that only delayed LaSO is able to fully leverage non-local features.",8 Results,[0],[0]
"From these results we conclude that we are better off when the learning algorithm handles one document at a time, instead of getting feedback within documents.
",8 Results,[0],[0]
Local vs. Non-local feature sets.,8 Results,[0],[0]
Table 1 displays the differences in F-measures and CoNLL average between the local and non-local systems when applied to the development sets for each language.,8 Results,[0],[0]
All metrics improve when more informative non-local features are added to the local feature set.,8 Results,[0],[0]
"Arabic and English show considerable improvements, and the CoNLL average increases
9Although the Early systems still seem to show slight increases after 50 iterations, it needs a considerable number of iterations to catch up with the baseline – after 100 iterations the best early system is still more than half a point behind the baseline.
about one point.",8 Results,[0],[0]
"For Chinese the gains are generally not as pronounced, though the MUC metric goes up by more than half a point.
",8 Results,[0],[0]
Final results.,8 Results,[0],[0]
"In Table 2 we compare the results of the non-local system (This paper) to the best results from the CoNLL 2012 Shared Task.10 Specifically, this includes Fernandes et al.’s (2012) system for Arabic and English (denoted Fernandes), and Chen and Ng’s (2012) system for Chinese (denoted C&N).",8 Results,[0],[0]
"For English we also compare it to the Berkeley system (Durrett and Klein, 2013), which, to our knowledge, is the best publicly available system for English coreference resolution (denoted D&K).",8 Results,[0],[0]
"As a general baseline, we also include Björkelund and Farkas’ (2012) system (denoted B&F), which was the second best system in the shared task.",8 Results,[0],[0]
For almost all metrics our system is significantly better than the best competitor.,8 Results,[0],[0]
"For a few metrics the best competitor outperforms our results for either precision or recall, but in terms of F-measures and the CoNLL average our system is the best for all languages.
",8 Results,[0],[0]
10Thanks to Sameer Pradhan for providing us with the outputs of the other systems for significance testing.,8 Results,[0],[0]
On the machine learning side Collins and Roark’s (2004) work on the early update constitutes our starting point.,9 Related Work,[0],[0]
"The LaSO framework was introduced by Daumé III and Marcu (2005b), but has, to our knowledge, only been applied to the related task of entity detection and tracking (Daumé III and Marcu, 2005a).",9 Related Work,[0],[0]
"The theoretical motivation for early updates was only recently explained rigorously (Huang et al., 2012).",9 Related Work,[0],[0]
"The delayed LaSO update that we propose decomposes the prediction task of a complex structure into a number of subproblems, each of which guarantee violation, using Huang et al.’s (2012) terminology.",9 Related Work,[0],[0]
"We believe this is an interesting novelty, as it leverages the complete structures for every training instance during every iteration, and expect it to be applicable also to other structured prediction tasks.
",9 Related Work,[0],[0]
"Our approach also resembles imitation learning techniques such as SEARN (Daumé III et al., 2009) and DAGGER (Ross et al., 2011), where the search problem is reduced to a sequence of classification steps that guide the search algorithm through the search space.",9 Related Work,[0],[0]
"These frameworks, however, rely on the notion of an expert policy which provides an optimal decision at each point during search.",9 Related Work,[0],[0]
"In our context that would require antecedents for every mention to be given a priori, rather than using latent antecedents as we do.
Perceptrons for coreference.",9 Related Work,[0],[0]
"The perceptron has previously been used to train coreference resolvers either by casting the problem as a binary classification problem that considers pairs of mentions in isolation (Bengtson and Roth, 2008; Stoyanov et al., 2009; Chang et al., 2012, inter alia) or in the structured manner, where a clustering for an entire document is predicted in one go (Fernandes et al., 2012).",9 Related Work,[0],[0]
"However, none of these works use non-local features.",9 Related Work,[0],[0]
Stoyanov and Eisner (2012) train an Easy-First coreference system with the perceptron to learn a sequence of join operations between arbitrary mentions in a document and accesses non-local features through previous merge operations in later stages.,9 Related Work,[0],[0]
"Culotta et al. (2007) also apply online learning in a first-order logic framework that enables non-local features, though using a greedy search algorithm.
",9 Related Work,[0],[0]
Latent antecedents.,9 Related Work,[0],[0]
"The use of latent antecedents goes back to the work of Yu and Joachims (2009), although the idea of determining
meaningful antecedents for mentions can be traced back to Ng and Cardie (2002) who used a rulebased approach.",9 Related Work,[0],[0]
"Latent antecedents have recently gained popularity and were used by two systems in the CoNLL 2012 Shared Task, including the winning system (Fernandes et al., 2012; Chang et al., 2012).",9 Related Work,[0],[0]
"Durrett and Klein (2013) present a coreference resolver with latent antecedents that predicts clusterings over entire documents and fit a loglinear model with a custom task-specific loss function using AdaGrad (Duchi et al., 2011).",9 Related Work,[0],[0]
Chang et al. (2013) use a max-margin approach to learn a pairwise model and rely on stochastic gradient descent to circumvent the costly operation of decoding the entire training set in order to compute the gradients and the latent antecedents.,9 Related Work,[0],[0]
"None of the aforementioned works use non-local features in their models, however.
",9 Related Work,[0],[0]
Entity-mention models.,9 Related Work,[0],[0]
"Entity-mention models that compare a single mention to a (partial) cluster have been studied extensively and several works have evaluated non-local entity-level features (Luo et al., 2004; Yang et al., 2008; Rahman and Ng, 2009).",9 Related Work,[0],[0]
"Luo et al. (2004) also apply beam search at test time, but use a static assignment of antecedents and learns log-linear model using batch learning.",9 Related Work,[0],[0]
"Moreover, these works alter the basic feature definitions from their pairwise models when introducing entity-level features.",9 Related Work,[0],[0]
"This contrasts with our work, as our mention-pair model simply constitutes a special case of the non-local system.",9 Related Work,[0],[0]
We presented experiments with a coreference resolver that leverages non-local features to improve its performance.,10 Conclusion,[0],[0]
The application of non-local features requires the use of an approximate search algorithm to keep the problem tractable.,10 Conclusion,[0],[0]
We evaluated standard perceptron learning techniques for this setting both using early updates and LaSO.,10 Conclusion,[0],[0]
"We found that the early update strategy is considerably worse than a local baseline, as it is unable to exploit all training data.",10 Conclusion,[0],[0]
"LaSO resolves this issue by giving feedback within documents, but still underperforms compared to the baseline as it distorts the choice of latent antecedents.
",10 Conclusion,[0],[0]
"We introduced a modification to LaSO, where updates are delayed until each document is processed.",10 Conclusion,[0],[0]
"In the special case where only local features are used, this method coincides with standard structured perceptron learning that uses exact search.",10 Conclusion,[0],[0]
"Moreover, it is also able to profit from nonlocal features resulting in improved performance.",10 Conclusion,[0],[0]
We evaluated our system on all three languages from the CoNLL 2012 Shared Task and present the best results to date on these data sets.,10 Conclusion,[0],[0]
We are grateful to the anonymous reviewers as well as Christian Scheible and Wolfgang Seeker for comments on earlier versions of this paper.,Acknowledgments,[0],[0]
"This research has been funded by the DFG via SFB 732, project D8.",Acknowledgments,[0],[0]
We investigate different ways of learning structured perceptron models for coreference resolution when using non-local features and beam search.,abstractText,[0],[0]
Our experimental results indicate that standard techniques such as early updates or Learning as Search Optimization (LaSO) perform worse than a greedy baseline that only uses local features.,abstractText,[0],[0]
By modifying LaSO to delay updates until the end of each instance we obtain significant improvements over the baseline.,abstractText,[0],[0]
"Our model obtains the best results to date on recent shared task data for Arabic, Chinese, and English.",abstractText,[0],[0]
Learning Structured Perceptrons for Coreference Resolution with Latent Antecedents and Non-local Features,title,[0],[0]
"Document modeling is a fundamental task in Natural Language Processing useful to various downstream applications including topic labeling (Xie and Xing, 2013), summarization (Chen et al., 2016; Wolf and Gibson, 2006), sentiment analysis (Bhatia et al., 2015), question answering (Verberne et al., 2007), and machine translation (Meyer and Webber, 2013).
",1 Introduction,[0],[0]
"Recent work provides strong evidence that better document representations can be obtained by incorporating structural knowledge (Bhatia et al., 2015; Ji and Smith, 2017; Yang et al., 2016).",1 Introduction,[0],[0]
"Inspired by existing theories of discourse, representations of document structure have assumed several guises in the literature, such as trees in the style of Rhetorical Struc-
ture Theory (RST; Mann and Thompson, 1988), graphs (Lin et al., 2011; Wolf and Gibson, 2006), entity transitions (Barzilay and Lapata, 2008), or combinations thereof (Lin et al., 2011; Mesgar and Strube, 2015).",1 Introduction,[0],[0]
"The availability of discourse annotated corpora (Carlson et al., 2001; Prasad et al., 2008) has led to the development of off-the-shelf discourse parsers (e.g., Feng and Hirst, 2012; Liu and Lapata, 2017), and the common use of trees as representations of document structure.",1 Introduction,[0],[0]
"For example, Bhatia et al. (2015) improve document-level sentiment analysis by reweighing discourse units based on the depth of RST trees, whereas Ji and Smith (2017) show that a recursive neural network built on the output of an RST parser benefits text categorization in learning representations that focus on salient content.
",1 Introduction,[0],[0]
"Linguistically motivated representations of document structure rely on the availability of annotated corpora as well as a wider range of standard NLP tools (e.g., tokenizers, pos-taggers, syntactic parsers).",1 Introduction,[0],[0]
"Unfortunately, the reliance on labeled data, which is both difficult and highly expensive to produce, presents a major obstacle to the widespread use of discourse structure for document modeling.",1 Introduction,[0],[0]
"Moreover, despite recent advances in discourse processing, the use of an external parser often leads to pipeline-style architectures where errors propagate to later processing stages, affecting model performance.
",1 Introduction,[0],[0]
It is therefore not surprising that there have been attempts to induce document representations directly from data without recourse to a discourse parser or additional annotations.,1 Introduction,[0],[0]
"The main idea is
ar X
iv :1
70 5.
09 20
7v 4
[ cs
.C",1 Introduction,[0],[0]
"L
] 3
F eb
to obtain hierarchical representations by first building representations of sentences, and then aggregating those into a document representation (Tang et al., 2015a,b).",1 Introduction,[0],[0]
"Yang et al. (2016) further demonstrate how to implicitly inject structural knowledge onto the representation using an attention mechanism (Bahdanau et al., 2015) which acknowledges that sentences are differentially important in different contexts.",1 Introduction,[0],[0]
"Their model learns to pay more or less attention to individual sentences when constructing the representation of the document.
",1 Introduction,[0],[0]
"Our work focus on learning deeper structureaware document representations, drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016).",1 Introduction,[0],[0]
"Kim et al. (2017) introduce structured attention networks which are generalizations of the basic attention procedure, allowing to learn sentential representations while attending to partial segmentations or subtrees.",1 Introduction,[0],[0]
"Specifically, they take into account the dependency structure of a sentence by viewing the attention mechanism as a graphical model over latent variables.",1 Introduction,[0],[0]
They first calculate unnormalized pairwise attention scores for all tokens in a sentence and then use the inside-outside algorithm to normalize the scores with the marginal probabilities of a dependency tree.,1 Introduction,[0],[0]
"Without recourse to an external parser, their model learns meaningful task-specific dependency structures, achieving competitive results in several sentence-level tasks.",1 Introduction,[0],[0]
"However, for document modeling, this approach has two drawbacks.",1 Introduction,[0],[0]
"Firstly, it does not consider non-projective dependency structures, which are common in documentlevel discourse analysis (Hayashi et al., 2016; Lee et al., 2006).",1 Introduction,[0],[0]
"As illustrated in Figure 1, the tree structure of a document can be flexible and the dependency edges may cross.",1 Introduction,[0],[0]
"Secondly, the inside-outside algorithm involves a dynamic programming process which is difficult to parallelize, making it impractical for modeling long documents.1
In this paper, we propose a new model for representing documents while automatically learning richer structural dependencies.",1 Introduction,[0],[0]
"Using a variant of Kirchhoff’s Matrix-Tree Theorem (Tutte, 1984), our model implicitly considers non-projective depen-
1In our experiments, adding the inside-outside pass increases training time by a factor of 10.
dency tree structures.",1 Introduction,[0],[0]
"We keep each step of the learning process differentiable, so the model can be trained in an end-to-end fashion and induce discourse information that is helpful to specific tasks without an external parser.",1 Introduction,[0],[0]
The inside-outside model of Kim et al. (2017) and our model both have a O(n3) worst case complexity.,1 Introduction,[0],[0]
"However, major operations in our approach can be parallelized efficiently on GPU computing hardware.",1 Introduction,[0],[0]
"Although our primary focus is on document modeling, there is nothing inherent in our model that prevents its application to individual sentences.",1 Introduction,[0],[0]
"Advantageously, it can induce non-projective structures which are required for representing languages with free or flexible word order (McDonald and Satta, 2007).
",1 Introduction,[0],[0]
Our contributions in this work are threefold: a model for learning document representations whilst taking structural information into account; an efficient training procedure which allows to compute document level representations of arbitrary length; and a large scale evaluation study showing that the proposed model performs competitively against strong baselines while inducing intermediate structures which are both interpretable and meaningful.,1 Introduction,[0],[0]
"In this section, we describe how previous work uses the attention mechanism for representing individual sentences.",2 Background,[0],[0]
"The key idea is to capture the interaction between tokens within a sentence, generating a context representation for each word with weak structural information.",2 Background,[0],[0]
"This type of intra-sentence attention encodes relationships between words within
each sentence and differs from inter-sentence attention which has been widely applied to sequence transduction tasks like machine translation (Bahdanau et al., 2015) and learns the latent alignment between source and target sequences.
",2 Background,[0],[0]
Figure 2 provides a schematic view of the intrasentential attention mechanism.,2 Background,[0],[0]
"Given a sentence represented as a sequence of n word vectors [u1,u2, · · · ,un], for each word pair 〈ui,uj〉, the attention score aij is estimated as:
fij = F (ui,uj) (1)
aij = exp(fij)∑n",2 Background,[0],[0]
"k=1 exp(fik)
(2)
where F () is a function for computing the unnormalized score fij which is then normalized by calculating a probability distribution aij .",2 Background,[0],[0]
"Individual words collect information from their context based on aij and obtain a context representation:
ri = n∑
j=1
aijuj (3)
where attention score aij indicates the (dependency) relation between the i-th and the j-th-words and how information from uj should be fed into ui.
",2 Background,[0],[0]
"Despite successful application of the above attention mechanism in sentiment analysis (Cheng et al., 2016) and entailment recognition (Parikh et al., 2016), the structural information under consideration is shallow, limited to word-word dependencies.",2 Background,[0],[0]
"Since attention is computed as a simple probability distribution, it cannot capture more elaborate
structural dependencies such as trees (or graphs).",2 Background,[0],[0]
Kim et al. (2017) induce richer internal structure by imposing structural constraints on the probability distribution computed by the attention mechanism.,2 Background,[0],[0]
"Specifically, they normalize fij with a projective dependency tree using the inside-outside algorithm (Baker, 1979):
fij = F (ui,uj) (4)
a = inside-outside(f) (5)
ri = n∑
j=1
aijuj (6)
",2 Background,[0],[0]
"This process is differentiable, so the model can be trained end-to-end and learn structural information without relying on a parser.",2 Background,[0],[0]
"However, efficiency is a major issue, since the inside-outside algorithm has time complexity O(n3) (where n represents the number of tokens) and does not lend itself to easy parallelization.",2 Background,[0],[0]
The high order complexity renders the approach impractical for real-world applications.,2 Background,[0],[0]
In this section we present our document representation model.,3 Encoding Text Representations,[0],[0]
"We follow previous work (Tang et al., 2015a; Yang et al., 2016) in modeling documents hierarchically by first obtaining representations for sentences and then composing those into a document representation.",3 Encoding Text Representations,[0],[0]
Structural information is taken into account while learning representations for both sentences and documents and an attention mechanism is applied on both words within a sentence and sentences within a document.,3 Encoding Text Representations,[0],[0]
"The general idea is to force pair-wise attention between text units to form a non-projective dependency tree, and automatically induce this tree for different natural language processing tasks in a differentiable way.",3 Encoding Text Representations,[0],[0]
"In the following, we first describe how the attention mechanism is applied to sentences, and then move on to present our document-level model.",3 Encoding Text Representations,[0],[0]
Let T =,3.1 Sentence Model,[0],[0]
"[u1,u2, · · · ,un] denote a sentence containing a sequence of words, each represented by a vector u, which can be pre-trained on a large corpus.",3.1 Sentence Model,[0],[0]
"Long Short-Term Memory Neural Networks (LSTMs; Hochreiter and Schmidhuber, 1997) have
been successfully applied to various sequence modeling tasks ranging from machine translation (Bahdanau et al., 2015), to speech recognition (Graves et al., 2013), and image caption generation (Xu et al., 2015).",3.1 Sentence Model,[0],[0]
"In this paper we use bidirectional LSTMs as a way of representing elements in a sequence (i.e., words or sentences) together with their contexts, capturing the element and an “infinite” window around it.",3.1 Sentence Model,[0],[0]
"Specifically, we run a bidirectional LSTM over sentence T , and take the output vectors [h1,h2, · · · ,hn] as the representations of words in T , where ht ∈ Rk is the output vector for word ut based on its context.
",3.1 Sentence Model,[0],[0]
We then exploit the structure of T which we induce based on an attention mechanism detailed below to obtain more precise representations.,3.1 Sentence Model,[0],[0]
"Inspired by recent work (Daniluk et al., 2017; Miller et al., 2016), which shows that the conventional way of using LSTM output vectors for calculating both attention and encoding word semantics is overloaded and likely to cause performance deficiencies, we decompose the LSTM output vector in two parts:
",3.1 Sentence Model,[0],[0]
"[et,dt] = ht (7)
where et ∈ Rkt , the semantic vector, encodes semantic information for specific tasks, and dt ∈ Rks , the structure vector, is used to calculate structured attention.
",3.1 Sentence Model,[0],[0]
"We use a series of operations based on the MatrixTree Theorem (Tutte, 1984) to incorporate the struc-
tural bias of non-projective dependency trees into the attention weights.",3.1 Sentence Model,[0],[0]
We constrain the probability distributions aij (see Equation (2)) to be the posterior marginals of a dependency tree structure.,3.1 Sentence Model,[0],[0]
"We then use the normalized structured attention, to build a context vector for updating the semantic vector of each word, obtaining new representations [r1, r2, · · · , rn].",3.1 Sentence Model,[0],[0]
An overview of the model is presented in Figure 3.,3.1 Sentence Model,[0],[0]
We describe the attention mechanism in detail in the following section.,3.1 Sentence Model,[0],[0]
Dependency representations of natural language are a simple yet flexible mechanism for encoding words and their syntactic relations through directed graphs.,3.2 Structured Attention Mechanism,[0],[0]
"Much work in descriptive linguistics (Melc̆uk, 1988; Tesniére, 1959) has advocated their suitability for representing syntactic structure across languages.",3.2 Structured Attention Mechanism,[0],[0]
"A primary advantage of dependency representations is that they have a natural mechanism for representing discontinuous constructions arising from long distance dependencies or free word order through nonprojective dependency edges.
",3.2 Structured Attention Mechanism,[0],[0]
"More formally, building a dependency tree amounts to finding latent variables zij for all i 6= j, where word i is the parent node of word j, under some global constraints, amongst which the single-head constraint is the most important, since it forces the structure to be a rooted tree.",3.2 Structured Attention Mechanism,[0],[0]
"We use a variant of Kirchhoff’s Matrix-Tree Theorem (Koo et al., 2007; Tutte, 1984) to calculate the marginal probability of each dependency edge P (zij = 1) of a non-projective dependency tree, and this probability is used as the attention weight that decides how much information is collected from child unit j to the parent unit i.
We first calculate unnormalized attention scores fij with structure vector d (see Equation (7)) via a bilinear function:
tp = tanh(Wpdi) (8)
tc = tanh(Wcdj) (9)
fij = t T pWatc (10)
where Wp ∈ Rks∗ks and Wc ∈ Rks∗ks are the weights for building the representation of parent and child nodes.",3.2 Structured Attention Mechanism,[0],[0]
Wa ∈ Rks∗ks is the weight for the bilinear transformation.,3.2 Structured Attention Mechanism,[0],[0]
"f ∈ Rn∗n can be viewed as
a weighted adjacency matrix for a graph G with n nodes where each node corresponds to a word in a sentence.",3.2 Structured Attention Mechanism,[0],[0]
"We also calculate the root score f ri , indicating the unnormalized possibility of a node being the root:
f ri = Wrdi (11)
where Wr ∈ R1∗ks .",3.2 Structured Attention Mechanism,[0],[0]
"We calculate P (zij = 1), the marginal probability of the dependency edge, following Koo et al. (2007):
",3.2 Structured Attention Mechanism,[0],[0]
"Aij = { 0 if i = j exp(fij) otherwise
(12)
",3.2 Structured Attention Mechanism,[0],[0]
"Lij =
{∑n i′=1Ai′j if i = j
−Aij otherwise (13)
L̄ij =
{ exp(f ri )",3.2 Structured Attention Mechanism,[0],[0]
"i = 1
Lij i > 1 (14)
P (zij = 1) =",3.2 Structured Attention Mechanism,[0],[0]
"(1− δ1,j)Aij",3.2 Structured Attention Mechanism,[0],[0]
"[L̄−1]jj − (1− δi,1)Aij [L̄−1]ji (15) P (root(i))",3.2 Structured Attention Mechanism,[0],[0]
=,3.2 Structured Attention Mechanism,[0],[0]
"exp(f ir)[L̄ −1]i1
where 1 ≤",3.2 Structured Attention Mechanism,[0],[0]
"i ≤ n, 1 ≤ j ≤ n. L ∈ Rn∗n is the Laplacian matrix for graph G and L̄",3.2 Structured Attention Mechanism,[0],[0]
"∈ Rn∗n is a variant of L that takes the root node into consideration, and δ is the Kronecker delta.",3.2 Structured Attention Mechanism,[0],[0]
"The key for the calculation to hold is for Lii, the minor of the Laplacian matrix L with respect to row i and column i, to be equal to the sum of the weights of all directed spanning trees of G which are rooted at i. P (zij = 1) is the marginal probability of the dependency edge between the i-th and j-th words.",3.2 Structured Attention Mechanism,[0],[0]
P (root(i) = 1) is the marginal probability of the ith word headed by the root of the tree.,3.2 Structured Attention Mechanism,[0],[0]
"Details of the proof can be found in Koo et al. (2007).
",3.2 Structured Attention Mechanism,[0],[0]
We denote the marginal probabilities P (zij = 1) as aij and P (root(i)),3.2 Structured Attention Mechanism,[0],[0]
as ari .,3.2 Structured Attention Mechanism,[0],[0]
"This can be interpreted as attention scores which are constrained to converge to a structured object, a non-projective dependency tree, in our case.",3.2 Structured Attention Mechanism,[0],[0]
"We update the semantic
vector ei of each word with structured attention:
pi = n∑
k=1
akiek + a r ieroot (16)
ci = n∑
k=1
aikei (17)
ri = tanh(Wr[ei,pi, ci]) (18)
where pi ∈ Rke is the context vector gathered from possible parents of ui and",3.2 Structured Attention Mechanism,[0],[0]
"ci ∈ Rke the context vector gathered from possible children, and eroot is a special embedding for the root node.",3.2 Structured Attention Mechanism,[0],[0]
The context vectors are concatenated with ei and transformed with weights Wr ∈ Rke∗3ke to obtain the updated semantic vector ri ∈ Rke with rich structural information (see Figure 3).,3.2 Structured Attention Mechanism,[0],[0]
We build document representations hierarchically: sentences are composed of words and documents are composed of sentences.,3.3 Document Model,[0],[0]
Composition on the document level also makes use of structured attention in the form of a dependency graph.,3.3 Document Model,[0],[0]
"Dependencybased representations have been previously used for developing discourse parsers (Hayashi et al., 2016; Li et al., 2014) and in applications such as summarization (Hirao et al., 2013).
",3.3 Document Model,[0],[0]
"As illustrated in Figure 4, given a document with n sentences [s1, s2, · · · , sn], for each sentence si, the input is a sequence of word embeddings [ui1,ui2, · · · ,uim], where m is the number of tokens in si.",3.3 Document Model,[0],[0]
"By feeding the embeddings into a sentence-level bi-LSTM and applying the proposed structured attention mechanism, we obtain the updated semantic vector [ri1, ri2, · · · , rim].",3.3 Document Model,[0],[0]
Then a pooling operation produces a fixed-length vector vi for each sentence.,3.3 Document Model,[0],[0]
"Analogously, we view the document as a sequence of sentence vectors",3.3 Document Model,[0],[0]
"[v1,v2, · · · ,vn] whose embeddings are fed to a document-level bi-LSTM.",3.3 Document Model,[0],[0]
"Application of the structured attention mechanism creates new semantic vectors [q1, q2, · · · , qn] and another pooling operation yields the final document representation y.",3.3 Document Model,[0],[0]
"Our model can be trained in an end-to-end fashion since all operations required for computing structured attention and using it to update the semantic
vectors are differentiable.",3.4 End-to-End Training,[0],[0]
"In contrast to in Kim et al. (2017), training can be done efficiently.",3.4 End-to-End Training,[0],[0]
The major complexity of our model lies in the computation of the gradients of the the inverse matrix.,3.4 End-to-End Training,[0],[0]
"Let A denote a matrix depending on a real parameter x; assuming all component functions in A are differentiable, and A is invertible for all possible values, the gradient of A with respect respect to x is:
dA−1
dx = −A−1dA",3.4 End-to-End Training,[0],[0]
"dx A−1 (19)
Multiplication of the three matrices and matrix inversion can be computed efficiently on modern parallel hardware architectures such as GPUs.",3.4 End-to-End Training,[0],[0]
"In our experiments, computation of structured attention takes only 1/10 of training time.",3.4 End-to-End Training,[0],[0]
In this section we present our experiments for evaluating the performance of our model.,4 Experiments,[0],[0]
"Since sentence representations constitute the basic building blocks of our document model, we first evaluate the performance of structured attention on a sentence-level task, namely natural language inference.",4 Experiments,[0],[0]
"We then assess the document-level representations obtained by our model on a variety of classification tasks representing documents of different length, subject matter, and language.",4 Experiments,[0],[0]
"Our
code is available at https://github.com/ nlpyang/structured.",4 Experiments,[0],[0]
The ability to reason about the semantic relationship between two sentences is an integral part of text understanding.,4.1 Natural Language Inference,[0],[0]
"We therefore evaluate our model on recognizing textual entailment, i.e., whether two premise-hypothesis pairs are entailing, contradictory, or neutral.",4.1 Natural Language Inference,[0],[0]
"For this task we used the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), which contains premise-hypothesis pairs and target labels indicating their relation.",4.1 Natural Language Inference,[0],[0]
"After removing sentences with unknown labels, we obtained 549,367 pairs for training, 9,842 for development and 9,824 for testing.
",4.1 Natural Language Inference,[0],[0]
Sentence-level representations obtained by our model (with structured attention) were used to encode the premise and hypothesis by modifying the model of Parikh et al. (2016) as follows.,4.1 Natural Language Inference,[0],[0]
Let,4.1 Natural Language Inference,[0],[0]
"[xp1, · · · ,x p n]",4.1 Natural Language Inference,[0],[0]
"and [xh1 , · · · ,xhm] be the input vectors for the premise and hypothesis, respectively.",4.1 Natural Language Inference,[0],[0]
"Application of structured attention yields new vector representations [rp1, · · · , r p n] and",4.1 Natural Language Inference,[0],[0]
"[rh1 , · · · , rhm].",4.1 Natural Language Inference,[0],[0]
"Then we combine these two vectors with inter-sentential attention, and apply an average pooling operation:
oij =",4.1 Natural Language Inference,[0],[0]
MLP (r p i ),4.1 Natural Language Inference,[0],[0]
"TMLP (rhj ) (20)
r̄pi",4.1 Natural Language Inference,[0],[0]
=,4.1 Natural Language Inference,[0],[0]
[r p,4.1 Natural Language Inference,[0],[0]
"i , m∑ j=1 exp(oij)∑m",4.1 Natural Language Inference,[0],[0]
k=1 exp(oik) ],4.1 Natural Language Inference,[0],[0]
"(21)
r̄hi =",4.1 Natural Language Inference,[0],[0]
[r h,4.1 Natural Language Inference,[0],[0]
"i , m∑ i=1 exp(oij)∑m",4.1 Natural Language Inference,[0],[0]
k=1 exp(okj) ],4.1 Natural Language Inference,[0],[0]
"(22)
rp = n∑
i=1
g(r̄pi ), r h = m∑ i=1 g(r̄hi )",4.1 Natural Language Inference,[0],[0]
"(23)
where MLP () is a two-layer perceptron with a ReLU activation function.",4.1 Natural Language Inference,[0],[0]
"The new representations rp, rh are then concatenated and fed into another two-layer perceptron with a softmax layer to obtain the predicted distribution over the labels.
",4.1 Natural Language Inference,[0],[0]
The hidden size of the LSTM was set to 150.,4.1 Natural Language Inference,[0],[0]
The dimensions of the semantic vector were 100 and the dimensions of structure vector were 50.,4.1 Natural Language Inference,[0],[0]
"We used pretrained 300-D Glove 840B (Pennington et al., 2014) vectors to initialize the word embeddings.",4.1 Natural Language Inference,[0],[0]
"All parameters (including word embeddings) were updated with Adagrad (Duchi et al., 2011), and the
learning rate was set to 0.05.",4.1 Natural Language Inference,[0],[0]
The hidden size of the two-layer perceptron was set to 200 and dropout was used with ratio 0.2.,4.1 Natural Language Inference,[0],[0]
"The mini-batch size was 32.
",4.1 Natural Language Inference,[0],[0]
We compared our model (and variants thereof) against several related systems.,4.1 Natural Language Inference,[0],[0]
Results (in terms of 3-class accuracy) are shown in Table 1.,4.1 Natural Language Inference,[0],[0]
Most previous systems employ LSTMs and do not incorporate a structured attention component.,4.1 Natural Language Inference,[0],[0]
Exceptions include Cheng et al. (2016) and Parikh et al. (2016) whose models include intra-attention encoding relationships between words within each sentence (see Equation (2)).,4.1 Natural Language Inference,[0],[0]
"It is also worth noting that some models take structural information into account in the form of parse trees (Bowman et al., 2016; Chen et al., 2017).",4.1 Natural Language Inference,[0],[0]
"The second block of Table 1 presents a version of our model without an intra-sentential attention mechanism as well as three variants with attention, assuming the structure of word-to-word re-
lations and dependency trees.",4.1 Natural Language Inference,[0],[0]
In the latter case we compare our matrix inversion based model against Kim et al.’s (2017) inside-outside attention model.,4.1 Natural Language Inference,[0],[0]
"Consistent with previous work (Cheng et al., 2016; Parikh et al., 2016), we observe that simple attention brings performance improvements over no attention.",4.1 Natural Language Inference,[0],[0]
Structured attention further enhances performance.,4.1 Natural Language Inference,[0],[0]
"Our own model with tree matrix inversion slightly outperforms the inside-outside model of Kim et al. (2017), overall achieving results in the same ballpark with related LSTM-based models (Chen et al., 2017; Cheng et al., 2016; Parikh et al., 2016).
",4.1 Natural Language Inference,[0],[0]
Table 2 compares the running speed of the models shown in the second block of Table 1.,4.1 Natural Language Inference,[0],[0]
As can be seen matrix inversion does not increase running speed over the simpler attention mechanism and is considerably faster compared to inside-outside.,4.1 Natural Language Inference,[0],[0]
The latter is 10–20 times slower than our model on the same platform.,4.1 Natural Language Inference,[0],[0]
"In this section, we evaluate our document-level model on a variety of classification tasks.",4.2 Document Classification,[0],[0]
We selected four datasets which we describe below.,4.2 Document Classification,[0],[0]
"Table 3 summarizes some statistics for each dataset.
",4.2 Document Classification,[0],[0]
Yelp reviews were obtained from the 2013 Yelp Dataset Challenge.,4.2 Document Classification,[0],[0]
"This dataset contains restaurant reviews, each associated with human ratings on a scale from 1 (negative) to 5 (positive) which we used as gold labels for sentiment classification; we followed the preprocessing introduced in Tang et al. (2015a) and report experiments on their training, development, and testing partitions (80/10/10).
",4.2 Document Classification,[0],[0]
"IMDB reviews were obtained from Diao et al. (2014), who randomly crawled reviews for 50K movies.",4.2 Document Classification,[0],[0]
"Each review is associated with user ratings ranging from 1 to 10.
",4.2 Document Classification,[0],[0]
Czech reviews were obtained from Brychcın and Habernal (2013).,4.2 Document Classification,[0],[0]
"The dataset contains reviews from the Czech Movie Database2 each labeled as positive, neutral, or negative.",4.2 Document Classification,[0],[0]
"We include Czech in our experiments since it has more flexible word order compared to English, with non-projective dependency structures being more frequent.",4.2 Document Classification,[0],[0]
"Experiments on this dataset perform 10-fold cross-validation following previous work (Brychcın and Habernal, 2013).
",4.2 Document Classification,[0],[0]
"2http://www.csfd.cz/
Congressional floor debates were obtained from a corpus originally created by Thomas et al. (2006) which contains transcripts of U.S. floor debates in the House of Representatives for the year 2005.",4.2 Document Classification,[0],[0]
"Each debate consists of a series of speech segments, each labeled by the vote (“yea” or “nay”) cast for the proposed bill by the the speaker of each segment.",4.2 Document Classification,[0],[0]
"We used the pre-processed corpus from Yogatama and Smith (2014).3
Following previous work (Yang et al., 2016), we only retained words appearing more than five times in building the vocabulary and replaced words with lesser frequencies with a special UNK token.",4.2 Document Classification,[0],[0]
"Word embeddings were initialized by training word2vec (Mikolov et al., 2013) on the training and validation splits of each dataset.",4.2 Document Classification,[0],[0]
"In our experiments, we set the word embedding dimension to be 200 and the hidden size for the sentence-level and documentlevel LSTMs to 100 (the dimensions of the semantic and structure vectors were set to 75 and 25, respectively).",4.2 Document Classification,[0],[0]
We used a mini-batch size of 32 during training and documents of similar length were grouped in one batch.,4.2 Document Classification,[0],[0]
"Parameters were optimized with Adagrad (Duchi et al., 2011), the learning rate was set to 0.05.",4.2 Document Classification,[0],[0]
We used L2 regularization for all parameters except word embeddings with regularization constant set to 1e−4.,4.2 Document Classification,[0],[0]
"Dropout was applied on the input and output
3http://www.cs.cornell.edu/˜ainur/data.",4.2 Document Classification,[0],[0]
"html
layers with dropout rate 0.3.",4.2 Document Classification,[0],[0]
Our results are summarized in Table 4.,4.2 Document Classification,[0],[0]
"We compared our model against several related models covering a wide spectrum of representations including word-based ones (e.g., paragraph vector and CNN models) as well as hierarchically composed ones (e.g., a CNN or LSTM provides a sentence vector and then a recurrent neural network combines the sentence vectors to form a document level representation for classification).",4.2 Document Classification,[0],[0]
"Previous state-of-the-art results on the three review datasets were achieved by the hierarchical attention network of Yang et al. (2016), which models the document hierarchically with two GRUs and uses an attention mechanism to weigh the importance of each word and sentence.",4.2 Document Classification,[0],[0]
"On the debates corpus, Ji and Smith (2017) obtained best results with a recursive neural network model operating on the output of an RST parser.",4.2 Document Classification,[0],[0]
"Table 4 presents three variants4 of our model, one with structured attention on the sentence level, another one with structured attention on the document level and a third model which employs attention on both levels.",4.2 Document Classification,[0],[0]
"As can be seen, the combination is beneficial achieving best results on three out of four datasets.",4.2 Document Classification,[0],[0]
"Furthermore, structured attention is superior to the simpler word-to-word attention mechanism, and both types of attention bring improvements over no attention.",4.2 Document Classification,[0],[0]
"The structured attention approach is also very efficient, taking only 20 minutes for one training epoch on the largest dataset.",4.2 Document Classification,[0],[0]
"To gain further insight on structured attention, we inspected the dependency trees it produces.",4.3 Analysis of Induced Structures,[0],[0]
"Specifically, we used the Chu-Liu-Edmonds algo-
4We do not report comparisons with the inside-outside approach on document classification tasks due to its prohibitive computation cost leading to 5 hours of training for one epoch.
rithm (Chu and Liu, 1965; Edmonds, 1967) to extract the maximum spanning tree from the attention scores.",4.3 Analysis of Induced Structures,[0],[0]
We report various statistics on the characteristics of the induced trees across different tasks and datasets.,4.3 Analysis of Induced Structures,[0],[0]
"We also provide examples of tree output, in an attempt to explain how our model uses dependency structures to model text.
",4.3 Analysis of Induced Structures,[0],[0]
Sentence Trees We compared the dependency trees obtained from our model with those produced by a state-of-the-art dependency parser trained on the English Penn Treebank.,4.3 Analysis of Induced Structures,[0],[0]
"Table 5 presents various statistics on the depth of the trees produced by our model on the SNLI test set and the Stanford dependency parser (Manning et al., 2014).",4.3 Analysis of Induced Structures,[0],[0]
"As can be seen, the induced dependency structures are simpler compared to those obtained from the Stanford parser.",4.3 Analysis of Induced Structures,[0],[0]
"The trees are generally less deep (their height is 5.78 compared to 8.99 for the Stanford parser), with the majority being of depth 2–4.",4.3 Analysis of Induced Structures,[0],[0]
"Almost half of the induced trees have a projective structure, although there is nothing in the model to enforce this constraint.",4.3 Analysis of Induced Structures,[0],[0]
"We also calculated the percentage of headdependency edges that are identical between the two
sets of trees.",4.3 Analysis of Induced Structures,[0],[0]
"Although our model is not exposed to annotated trees during training, a large number of edges agree with the output of the Stanford parser.
",4.3 Analysis of Induced Structures,[0],[0]
Figure 5 shows examples of dependency trees induced on the SNLI dataset.,4.3 Analysis of Induced Structures,[0],[0]
"Although the model is trained without ever being exposed to a parse tree, it is able to learn plausible dependency structures via the attention mechanism.",4.3 Analysis of Induced Structures,[0],[0]
Overall we observe that the induced trees differ from linguistically motivated ones in the types of dependencies they create which tend to be of shorter length.,4.3 Analysis of Induced Structures,[0],[0]
The dependencies obtained from structured attention are more direct as shown in the first premise sentence in Figure 5 where words at and bar are directly connected to the verb drink.,4.3 Analysis of Induced Structures,[0],[0]
"This is perhaps to be expected since the attention mechanism uses the dependency structures to collect information from other words, and the direct links will be more effective.
",4.3 Analysis of Induced Structures,[0],[0]
Document Trees,4.3 Analysis of Induced Structures,[0],[0]
We also used the Chu-LiuEdmonds algorithms to obtain document-level dependency trees.,4.3 Analysis of Induced Structures,[0],[0]
Table 6 summarizes various characteristics of these trees.,4.3 Analysis of Induced Structures,[0],[0]
"For most datasets, documentlevel trees are not very deep, they mostly contain up to nodes of depth 3.",4.3 Analysis of Induced Structures,[0],[0]
This is not surprising as the documents are relatively short (see Table 3) with the exception of debates which are longer and the induced trees more complex.,4.3 Analysis of Induced Structures,[0],[0]
"The fact that most documents exhibit simple discourse structures is further corroborated by the large number (over 70%) of projective trees induced on Yelp, IMBD, and CZ Movies datasets.",4.3 Analysis of Induced Structures,[0],[0]
"Unfortunately, our trees cannot be directly compared with the output of a discourse parser which typically involves a segmentation process splitting sentences into smaller units.",4.3 Analysis of Induced Structures,[0],[0]
"Our trees are constructed over entire sentences, and there is no mechanism currently in the model to split sentences
into discourse units.
",4.3 Analysis of Induced Structures,[0],[0]
Figure 6 shows examples of document-level trees taken from Yelp and the Czech Movie dataset.,4.3 Analysis of Induced Structures,[0],[0]
"In the first tree, most edges are examples of the “elaboration” discourse relation, i.e., the child presents
additional information about the parent.",4.3 Analysis of Induced Structures,[0],[0]
"The second tree is non-projective, the edges connecting sentences 1 and 4 and 3 and 5 cross.",4.3 Analysis of Induced Structures,[0],[0]
"The third review, perhaps due to its colloquial nature, is not entirely coherent.",4.3 Analysis of Induced Structures,[0],[0]
"However, the model manages to link sentences 1 and 3 to sentence 2, i.e., the movie being discussed; it also relates sentence 6 to 4, both of which express highly positive sentiment.",4.3 Analysis of Induced Structures,[0],[0]
In this paper we proposed a new model for representing documents while automatically learning rich structural dependencies.,5 Conclusions,[0],[0]
Our model normalizes intra-attention scores with the marginal probabilities of a non-projective dependency tree based on a matrix inversion process.,5 Conclusions,[0],[0]
"Each operation in this process is differentiable and the model can be trained efficiently end-to-end, while inducing structural information.",5 Conclusions,[0],[0]
"We applied this approach to model documents hierarchically, incorporating both sentenceand document-level structure.",5 Conclusions,[0],[0]
Experiments on sentence and document modeling tasks show that the representations learned by our model achieve competitive performance against strong comparison systems.,5 Conclusions,[0],[0]
"Analysis of the induced tree structures revealed that they are meaningful, albeit different from linguistics ones, without ever exposing the model to linguistic annotations or an external parser.
Directions for future work are many and varied.",5 Conclusions,[0],[0]
"Given appropriate training objectives (Linzen et al., 2016), it should be possible to induce linguistically meaningful dependency trees using the proposed attention mechanism.",5 Conclusions,[0],[0]
"We also plan to explore how document-level trees can be usefully employed in summarization, e.g., as a means to represent or even extract important content.
",5 Conclusions,[0],[0]
Acknowledgments The authors gratefully acknowledge the support of the European Research Council (award number 681760).,5 Conclusions,[0],[0]
"We also thank the anonymous TACL reviewers and the action editor whose feedback helped improve the present paper, members of EdinburghNLP for helpful discussions and suggestions, and Barbora Skarabela for translating the Czech document for us.",5 Conclusions,[0],[0]
"In this paper, we focus on learning structureaware document representations from data without recourse to a discourse parser or additional annotations.",abstractText,[0],[0]
"Drawing inspiration from recent efforts to empower neural networks with a structural bias (Cheng et al., 2016; Kim et al., 2017), we propose a model that can encode a document while automatically inducing rich structural dependencies.",abstractText,[0],[0]
"Specifically, we embed a differentiable non-projective parsing algorithm into a neural model and use attention mechanisms to incorporate the structural biases.",abstractText,[0],[0]
Experimental evaluations across different tasks and datasets show that the proposed model achieves state-of-the-art results on document modeling tasks while inducing intermediate structures which are both interpretable and meaningful.,abstractText,[0],[0]
Learning Structured Text Representations,title,[0],[0]
"Supervised machine learning traditionally depends on access to labeled training data, a major bottleneck in developing new methods and applications.",1. Introduction,[0],[0]
"In particular, deep learning methods require tens of thousands or more labeled data points for each specific task.",1. Introduction,[0],[0]
"Collecting these labels is often prohibitively expensive, especially when specialized domain expertise is required, and major technology companies are investing heavily in hand-curating labeled training data (Metz, 2016; Eadicicco, 2017).",1. Introduction,[0],[0]
"Aiming to overcome this bottleneck, there is growing interest in using generative models to synthesize training data from weak super-
1Stanford University, Stanford, California.",1. Introduction,[0],[0]
"Correspondence to: Stephen Bach <bach@cs.stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
vision sources such as heuristics, knowledge bases, and weak classifiers trained directly on noisy sources.",1. Introduction,[0],[0]
"Rather than treating training labels as gold-standard inputs, such methods model training set creation as a process in order to generate training labels at scale.",1. Introduction,[0],[0]
"The true class label for a data point is modeled as a latent variable that generates the observed, noisy labels.",1. Introduction,[0],[0]
"After fitting the parameters of this generative model on unlabeled data, a distribution over the latent, true labels can be inferred.
",1. Introduction,[0],[0]
"The structure of such generative models directly affects the inferred labels, and prior work assumes that the structure is user-specified (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013b; Ratner et al., 2016).",1. Introduction,[0],[0]
One option is to assume that the supervision sources are conditionally independent given the latent class label.,1. Introduction,[0],[0]
"However, statistical dependencies are common in practice, and not taking them into account leads to misjudging the accuracy of the supervision.",1. Introduction,[0],[0]
"We cannot rely in general on users to specify the structure of the generative model, because supervising heuristics and classifiers might be independent for some data sets but not others.",1. Introduction,[0],[0]
"We therefore seek an efficient method for automatically learning the structure of the generative model from weak supervision sources alone.
",1. Introduction,[0],[0]
"While structure learning in the supervised setting is wellstudied (e.g., Meinshausen & Bühlmann, 2006; Zhao & Yu, 2006; Ravikumar et al., 2010, see also Section 6), learning the structure of generative models for weak supervision is challenging because the true class labels are latent.",1. Introduction,[0],[0]
"Although we can learn the parameters of generative models for a given structure using stochastic gradient descent and Gibbs sampling, modeling all possible dependencies does not scale as an alternative to model selection.",1. Introduction,[0],[0]
"For example, estimating all possible correlations for a modestly sized problem of 100 weak supervision sources takes over 40 minutes.",1. Introduction,[0],[0]
"(For comparison, our proposed approach solves the same problem in 15 seconds.)",1. Introduction,[0],[0]
"As users develop their supervision heuristics, rerunning parameter learning to identify dependencies becomes a prohibitive bottleneck.
",1. Introduction,[0],[0]
We propose an estimator to learn the dependency structure of a generative model without using any labeled training data.,1. Introduction,[0],[0]
"Our method maximizes the `1-regularized marginal pseudolikelihood of each supervision source’s output independently, selecting those dependencies that have nonzero
weights.",1. Introduction,[0],[0]
"This estimator is analogous to maximum likelihood for logistic regression, except that we marginalize out our uncertainty about the latent class label.",1. Introduction,[0],[0]
"Since the pseudolikelihood is a function of one free variable and marginalizes over one other variable, we compute the gradient of the marginal pseudolikelihood exactly, avoiding the need for approximating the gradient with Gibbs sampling, as is done for maximum likelihood estimation.
",1. Introduction,[0],[0]
Our analysis shows that the amount of data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models.,1. Introduction,[0],[0]
"Intuitively, this follows from the fact that learning the generative model’s parameters is possible when there are a sufficient number of better-than-random supervision sources available.",1. Introduction,[0],[0]
"With enough signal to estimate the latent class labels better than random guessing, those estimates can be refined until the model is identified.
",1. Introduction,[0],[0]
We run experiments to confirm these predictions.,1. Introduction,[0],[0]
We also compare against the alternative approach of considering all possible dependencies during parameter learning.,1. Introduction,[0],[0]
We find that our method is 100⇥ faster.,1. Introduction,[0],[0]
"In addition, our method returns 1/4 as many extraneous correlations on synthetic data when tuned for comparable recall.",1. Introduction,[0],[0]
"Finally, we demonstrate that on real-world applications of weak supervision, using generative models with automatically learned dependencies improves performance.",1. Introduction,[0],[0]
"We find that our method provides on average 1.5 F1 points of improvement over existing, user-developed information extraction applications on PubMed abstracts and hardware specification sheets.",1. Introduction,[0],[0]
"When developing machine learning systems, the primary bottleneck is often curating a sufficient amount of labeled training data.",2. Background,[0],[0]
"Hand labeling training data is expensive, time consuming, and often requires specialized knowledge.",2. Background,[0],[0]
Recently researchers have proposed methods for synthesizing labels from noisy label sources using generative models.,2. Background,[0],[0]
(See Section 6 for a summary.),2. Background,[0],[0]
"We ground our work in one framework, data programming (Ratner et al., 2016), that generalizes many approaches in the literature.
",2. Background,[0],[0]
"In data programming, weak supervision sources are encoded as labeling functions, heuristics that label data points (or abstain).",2. Background,[0],[0]
A generative probabilistic model is fit to estimate the accuracy of the labeling functions and the strength of any user-specified statistical dependencies among their outputs.,2. Background,[0],[0]
"In this model, the true class label for a data point is a latent variable that generates the labeling function outputs.",2. Background,[0],[0]
"After fitting the parameters of the generative model, a distribution over the latent, true labels can be estimated and be used to train a discriminative model by minimizing the expected loss with respect to that distribution.
",2. Background,[0],[0]
"We formally describe this setup by first specifying for each data point x
i a latent random variable y",2. Background,[0],[0]
"i 2 { 1, 1} that is its true label.",2. Background,[0],[0]
"For example, in an information extraction task, x
i might be a span of text.",2. Background,[0],[0]
"Then, y i can represent whether it is a mention of a company or not (entity tagging).",2. Background,[0],[0]
"Alternatively, x
i might be a more complex structure, such as a tuple of canonical identifiers along with associated mentions in a document, and then y
i can represent whether a relation of interest over that tuple is expressed in the document (relation extraction).
",2. Background,[0],[0]
"We do not have access to y i (even at training time), but we do have n user-provided labeling functions 1, . . .",2. Background,[0],[0]
", n that can be applied to x
i to produce outputs ⇤ i1, . . .",2. Background,[0],[0]
",⇤in.
",2. Background,[0],[0]
"For example, for the company-tagging task mentioned above, a labeling function might apply the regular expression .+\sInc\. to a span of text and return whether it matched.",2. Background,[0],[0]
"The domain of each ⇤
ij is { 1, 0, 1}, corresponding to false, abstaining, and true.",2. Background,[0],[0]
"Generalizing to the multiclass case is straightforward.
",2. Background,[0],[0]
"Our goal is to estimate a probabilistic model that generates the labeling-function outputs ⇤ 2 { 1, 0, 1}m⇥",2. Background,[0],[0]
n.,2. Background,[0],[0]
"A common assumption is that the outputs are conditionally independent given the true label, and that the relationship between ⇤ and y is governed by n accuracy dependencies
Acc j",2. Background,[0],[0]
"(⇤ i , y i ) :",2. Background,[0],[0]
"= y i ⇤ ij
with a parameter ✓Acc j modeling how accurate each labeling function
j is.",2. Background,[0],[0]
"We refer to this structure as the conditionally independent model, and specify it as
p ✓
(⇤, Y ) / exp
0 @ mX
i=1
nX
j=1
✓Acc j Acc j",2. Background,[0],[0]
"(⇤ i , y i )
1
A , (1)
where Y := y1, . . .",2. Background,[0],[0]
", ym.
",2. Background,[0],[0]
"We estimate the parameters ✓ by minimizing the negative log marginal likelihood p
✓
(
¯ ⇤) for an observed matrix of labeling function outputs ¯⇤:
argmin
✓
log X
Y
p ✓ ( ¯ ⇤, Y ) .",2. Background,[0],[0]
"(2)
Optimizing the likelihood is straightforward using stochastic gradient descent.",2. Background,[0],[0]
"The gradient of objective (2) with respect to parameter ✓Acc
j
is
mX
i=1
",2. Background,[0],[0]
"E⇤,Y⇠✓ ⇥",2. Background,[0],[0]
Acc j,2. Background,[0],[0]
"(⇤ i , y i )",2. Background,[0],[0]
⇤ E Y⇠✓|⇤̄ ⇥,2. Background,[0],[0]
"Acc j ( ¯ ⇤ i , y i )",2. Background,[0],[0]
"⇤ ,
the difference between the corresponding sufficient statistic of the joint distribution p
✓ and the same distribution conditioned on ¯⇤.",2. Background,[0],[0]
"In practice, we can interleave samples to estimate the gradient and gradient steps very tightly, taking
a small step after each sample of each variable ⇤ ij",2. Background,[0],[0]
"or y i , similarly to contrastive divergence (Hinton, 2002).
",2. Background,[0],[0]
"The conditionally independent model is a common assumption, and using a more sophisticated generative model currently requires users to specify its structure.",2. Background,[0],[0]
"In the rest of the paper, we address the question of automatically identifying the dependency structure from the observations ¯⇤ without observing Y .",2. Background,[0],[0]
Statistical dependencies arise naturally among weak supervision sources.,3. Structure Learning without Labels,[0],[0]
"In data programming, users often write labeling functions with directly correlated outputs or even labeling functions deliberately designed to reinforce others with narrow, more precise heuristics.",3. Structure Learning without Labels,[0],[0]
"To address this issue, we generalize the conditionally independent model as a factor graph with additional dependencies, including higher-order factors that connect multiple labeling function outputs for each data point x
i and label y i .",3. Structure Learning without Labels,[0],[0]
"We specify the general model as
p ✓
(⇤, Y ) / exp
mX
i=1
X
t2T
X
s2St
✓t s t s (⇤ i , y i )
! .",3. Structure Learning without Labels,[0],[0]
"(3)
Here T is the set of dependency types of interest, and S t is a set of index tuples, indicating the labeling functions that participate in each dependency of type t 2 T .",3. Structure Learning without Labels,[0],[0]
"We start by defining standard correlation dependencies of the form
Cor jk",3. Structure Learning without Labels,[0],[0]
"(⇤ i , y i ) :",3. Structure Learning without Labels,[0],[0]
"= {⇤ ij = ⇤ ik } .
",3. Structure Learning without Labels,[0],[0]
We refer to such dependencies as pairwise among labeling functions because they depend only on two labeling function outputs.,3. Structure Learning without Labels,[0],[0]
"We can also consider higher-order dependencies that involve more variables, such as conjunction dependencies of the form
",3. Structure Learning without Labels,[0],[0]
"And jk (⇤ i , y i ) : = {⇤ ij = y i ^",3. Structure Learning without Labels,[0],[0]
"⇤ ik = y i } .
",3. Structure Learning without Labels,[0],[0]
"Estimating the structure of the distribution p ✓ (⇤, Y ) is challenging because Y is latent; we never observe its value, even during training.",3. Structure Learning without Labels,[0],[0]
"We must therefore work with the marginal likelihood p
✓ (⇤).",3. Structure Learning without Labels,[0],[0]
Learning the parameters of the generative model jointly requires Gibbs sampling to estimate gradients.,3. Structure Learning without Labels,[0],[0]
"As the number of possible dependencies increases at least quadratically in the number of labeling functions, this heavyweight approach to learning does not scale (see Section 5.2).",3. Structure Learning without Labels,[0],[0]
"We can scale up learning over many potentially irrelevant dependencies by optimizing a different objective: the log
marginal pseudolikelihood of the outputs of a single labeling function
j , i.e., conditioned on the outputs of the others \j , using `1 regularization to induce sparsity.",3.1. Learning Objective,[0],[0]
"The objective is
argmin
✓
log p ✓ ( ¯ ⇤ j | ¯⇤\j) +",3.1. Learning Objective,[0],[0]
"✏k✓k1 (4)
= argmin
✓
mX
i=1
log
X
yi
p ✓ ( ¯ ⇤ ij , y i | ¯⇤ i\j) +",3.1. Learning Objective,[0],[0]
"✏k✓k1,
where ✏ > 0 is a hyperparameter.
",3.1. Learning Objective,[0],[0]
"By conditioning on all other labeling functions in each term log
P yi p ✓ (",3.1. Learning Objective,[0],[0]
"¯ ⇤ ij , y i | ¯⇤ i\j), we ensure that the gradient can be computed in polynomial time with respect to the number of labeling functions, data points, and possible dependencies; without requiring any sampling or variational approximations.",3.1. Learning Objective,[0],[0]
"The gradient of the log marginal pseudolikelihood is the difference between two expectations: the sufficient statistics conditioned on all labeling functions but
j , and conditioned on all labeling functions:
@ log p(¯⇤
j | ¯⇤\j) @✓t
s
= ↵ , (5)
where
↵ := mX
i=1
X
⇤ij ,yi
p ✓ (⇤ ij ,",3.1. Learning Objective,[0],[0]
y i | ¯⇤ i\j),3.1. Learning Objective,[0],[0]
"t
s
((⇤
ij , ¯⇤ i\j), yi)
:= mX
i=1
X
yi
p(y i | ¯⇤ i )",3.1. Learning Objective,[0],[0]
"t s ( ¯ ⇤ i , y i ).
",3.1. Learning Objective,[0],[0]
"Note that in the definition of ↵, t s operates on the value of ⇤
ij set in the summation and the observed values of ¯⇤ i\j .
",3.1. Learning Objective,[0],[0]
"We optimize for each labeling function j in turn, selecting those dependencies with parameters that have a sufficiently large magnitude and adding them to the estimated structure.",3.1. Learning Objective,[0],[0]
"We implement our method as Algorithm 1, a stochastic gradient descent (SGD) routine.",3.2. Implementation,[0],[0]
"At each step of the descent, the gradient (5) is estimated for a single data point, which can be computed in closed form.",3.2. Implementation,[0],[0]
Using SGD has two advantages.,3.2. Implementation,[0],[0]
"First, it requires only first-order gradient information.",3.2. Implementation,[0],[0]
"Other methods for `1-regularized regression like interior-point methods (Koh et al., 2007) usually require computing second-order information.",3.2. Implementation,[0],[0]
"Second, the observations ¯⇤ can be processed incrementally.",3.2. Implementation,[0],[0]
"Since data programming operates on unlabeled data, which is often abundant, scalability is crucial.",3.2. Implementation,[0],[0]
"To implement `1 regularization as part of SGD, we use an online truncated gradient method (Langford et al., 2009).
",3.2. Implementation,[0],[0]
"In practice, we find that the only parameter that requires tuning is ✏, which controls the threshold and regularization
Algorithm 1 Structure Learning for Data Programming Input: Observations ¯⇤ 2 { 1, 0, 1}m⇥n, threshold ✏, distribution p with parameters ✓, initial parameters ✓0, step size ⌘, epoch count T , truncation frequency K D ; for j = 1 to n do ✓ ✓0 for ⌧ = 1 to T do
for i = 1 to m do for ✓t
s in ✓ do ↵
P ⇤ij ,yi p",3.2. Implementation,[0],[0]
"(⇤ ij , y i |¯⇤ i\j)",3.2. Implementation,[0],[0]
"t s ((⇤ ij , ¯⇤ i\j), yi) P
yi p(y i | ¯⇤ i )",3.2. Implementation,[0],[0]
"t s ( ¯ ⇤ i , y i )
",3.2. Implementation,[0],[0]
"✓t s
✓t s ⌘(↵ ) if ⌧m+ i mod K is 0 then
for ✓t s in ✓ where ✓t s > 0",3.2. Implementation,[0],[0]
"do ✓t s max{0, ✓t s
K⌘✏} for ✓t
s in ✓ where ✓t s < 0",3.2. Implementation,[0],[0]
"do ✓t s min{0, ✓t s +K⌘✏}
for ✓t s in ✓ where j 2 s do if |✓t
s",3.2. Implementation,[0],[0]
| >,3.2. Implementation,[0],[0]
✏ then D D,3.2. Implementation,[0],[0]
"[ {(s, t)}
return D
strength.",3.2. Implementation,[0],[0]
Higher values induce more sparsity in the selected structure.,3.2. Implementation,[0],[0]
"For the other parameters, we use the same values in all of our experiments: step size ⌘ = m 1, epoch count T = 10, and truncation frequency K = 10.",3.2. Implementation,[0],[0]
We provide guarantees on the probability that Algorithm 1 successfully recovers the exact dependency structure.,4. Analysis,[0],[0]
"We first provide a general recovery guarantee for all types of possible dependencies, including both pairwise and higherorder dependencies.",4. Analysis,[0],[0]
"However, in many cases, higher-order dependencies are not necessary to model the behavior of the labeling functions.",4. Analysis,[0],[0]
"In fact, as we demonstrate in Section 5.3, in many useful models there are only accuracy dependencies and pairwise correlations.",4. Analysis,[0],[0]
"In this case, we show as a corollary to our general result that the number of samples required is sublinear in the number of possible dependencies, specifically O(n log n).
",4. Analysis,[0],[0]
Previous analyses for the supervised case do not carry over to the unsupervised setting because the problem is no longer convex.,4. Analysis,[0],[0]
"For example, analysis of an analogous method for supervised Ising models (Ravikumar et al., 2010) relies on Lagrangian duality and a tight duality gap, which does not hold for our estimation problem.",4. Analysis,[0],[0]
"Instead, we reason about a region of the parameter space in which we can estimate Y well enough that we can eventually ap-
proach the true model.
",4. Analysis,[0],[0]
We now state the conditions necessary for our guarantees.,4. Analysis,[0],[0]
First are two standard conditions that are needed to guarantee that the dependency structure can be recovered with any number of samples.,4. Analysis,[0],[0]
"One, we must have some set ⇥ ⇢ RM of feasible parameters.",4. Analysis,[0],[0]
"Two, the true model is in ⇥, i.e., there exists some choice of ✓⇤ 2 ⇥ such that
⇡⇤(⇤, Y ) =",4. Analysis,[0],[0]
"p ✓ ⇤ (⇤, Y ), 8⇤ 2 { 1, 0, 1}m⇥n, Y 2 { 1, 1}m (6)
where ⇡⇤ is the true distribution.
",4. Analysis,[0],[0]
"Next, let j denote the set of dependencies that involve either labeling function
j or the true label y.",4. Analysis,[0],[0]
"For any feasible parameter ✓ 2 ⇥ and j 2 {1, . . .",4. Analysis,[0],[0]
", n}, there must exist c > 0",4. Analysis,[0],[0]
"such that
cI + mX
i=1
Cov(⇤,Y )⇠p✓ ( j(⇤, Y ) | ⇤i = ¯⇤i)
mX
i=1
Cov(⇤,Y )⇠p✓ ( j(⇤, Y ) | ⇤i\j = ¯⇤i\j).",4. Analysis,[0],[0]
"(7)
",4. Analysis,[0],[0]
"This means that for each labeling function, we have a better estimate of the dependencies with the labeling function than without.",4. Analysis,[0],[0]
"It is analogous to assumptions made to analyze parameter learning in data programming.
",4. Analysis,[0],[0]
"Finally, we require that all non-zero parameters be bounded away from zero.",4. Analysis,[0],[0]
"That is, for all ✓
i 6= 0, and some  > 0, we have that
|✓ i | .",4. Analysis,[0],[0]
"(8)
Under these conditions, we are able to provide guarantees on the probability of finding the correct dependency structure.",4. Analysis,[0],[0]
"First, we present guarantees for all types of possible dependencies in Theorem 1, proved in Appendix A.2.",4. Analysis,[0],[0]
"For this theorem, we define d
j to be the number of possible dependencies involving either ⇤
j or y, and we define d as the largest of d1, . . .",4. Analysis,[0],[0]
", dn. Theorem 1.",4. Analysis,[0],[0]
"Suppose we run Algorithm 1 on a problem where conditions (6), (7), and (8) are satisfied.",4. Analysis,[0],[0]
"Then, for any > 0, an unlabeled input dataset of size
m 32d c22 log
✓ 2nd ◆
is sufficient to recover the exact dependency structure with a probability of at least 1 .
",4. Analysis,[0],[0]
"For general dependencies, d can be as large as the number of possible dependencies due to the fact that higher-order dependencies can connect the true label and many labeling functions.",4. Analysis,[0],[0]
"The rate of Theorem 1 rate is therefore not directly comparable to that of Ravikumar et al. (2010), which applies to Ising models with pairwise dependencies.
",4. Analysis,[0],[0]
"As we demonstrate in Section 5.3, however, real-world applications can be improved by modeling just pairwise correlations among labeling functions.",4. Analysis,[0],[0]
"If only considering these dependencies, then d will only be 2n 1, rather than the number of potential dependencies.",4. Analysis,[0],[0]
"In Corollary 2, we show that a number of samples needed in this case is O(n log n).",4. Analysis,[0],[0]
"Notice that this is sublinear in the number of possible dependencies, which is O(n2).
",4. Analysis,[0],[0]
Corollary 2.,4. Analysis,[0],[0]
"Suppose we run Algorithm 1 on a problem where conditions (6), (7), and (8) are satisfied.",4. Analysis,[0],[0]
"Additionally, assume that the only potential dependencies are accuracy and correlation dependencies.",4. Analysis,[0],[0]
"Then, for any > 0, an unlabeled input dataset of size
m 64n c22 log
✓ 4n ◆
is sufficient to recover the exact dependency structure with a probability of at least 1 .
",4. Analysis,[0],[0]
"In this case, we see the difference in analyses between the unsupervised and supervised settings.",4. Analysis,[0],[0]
"Whereas the rate of Corollary 2 depends on the maximum number of dependencies that could affect a variable in the model class, the rate of Ravikumar et al. (2010) depends cubically on the maximum number of dependencies that actually affect any variable in the true model and only logarithmically in the maximum possible degree.",4. Analysis,[0],[0]
"In the supervised setting, the guaranteed rate is therefore tighter for very sparse models.",4. Analysis,[0],[0]
"However, as we show in Section 5.1, the guaranteed rates in both settings are pessimistic, and in practice they appear to scale at the same rate.",4. Analysis,[0],[0]
We implement our method as part of the open source framework Snorkel1 and evaluate it in three ways.,5. Experiments,[0],[0]
"First, we measure how the probability of returning the exact correlation structure is affected by the problem parameters using synthetic data, confirming our analysis that its sample complexity is sublinear in the number of possible dependencies.",5. Experiments,[0],[0]
"In fact, we find that in practice the sample complexity is lower than the theoretically guaranteed rate, matching the rate seen in practice for fully supervised structure learning.",5. Experiments,[0],[0]
"Second, we compare our method to estimating structures via parameter learning over all possible dependencies.",5. Experiments,[0],[0]
"We demonstrate using synthetic data that our method is 100⇥ faster and more accurate, selecting 1/4 as many extraneous correlations on average.",5. Experiments,[0],[0]
"Third, we apply our method to real-world applications built using data programming, such as information extraction from PubMed journal abstracts and hardware specification sheets.",5. Experiments,[0],[0]
"In these applications, users did not specify any dependencies between the label-
1snorkel.stanford.edu
ing functions they authored; however, as we detail in Section 5.3, these dependencies naturally arise, for example due to explicit composing, relaxing, or tightening of labeling function heuristics; related distant supervision sources; or multiple concurrent developers writing labeling functions.",5. Experiments,[0],[0]
"We show that learning this structure improves performance over the conditionally independent model, giving an average 1.5 F1 point boost.",5. Experiments,[0],[0]
We test how the probability that Algorithm 1 returns the correct correlation structure depends on the true distribution.,5.1. Sample Complexity,[0],[0]
Our analysis in Section 4 guarantees that the sample complexity grows at worst on the order O(n log n) for n labeling functions.,5.1. Sample Complexity,[0],[0]
"In practice, we find that structure learning performs better than this guaranteed rate, depending linearly on the number of true correlations and logarithmically on the number of possible correlations.",5.1. Sample Complexity,[0],[0]
"This matches the observed behavior for fully supervised structure learning for Ising models (Ravikumar et al., 2010), which is also tighter than the best known theoretical guarantees.
",5.1. Sample Complexity,[0],[0]
"To demonstrate this behavior, we attempt to recover the true dependency structure using a number of samples defined as
m := 750 · · d⇤ · log n (9)
where d⇤ is the maximum number of dependencies that affect any one labeling function.",5.1. Sample Complexity,[0],[0]
"For example, in the conditionally independent model d⇤ = 1 and in a model with one correlation d⇤ = 2.",5.1. Sample Complexity,[0],[0]
We vary the control parameter from 0.1 to 2.0 to determine the point at which m is sufficiently large for Algorithm 1 to recover the true dependency structure.,5.1. Sample Complexity,[0],[0]
"(The constant 750 was selected so that it succeeds with high probability around = 1.0.)
",5.1. Sample Complexity,[0],[0]
"We first test the effect of varying n, the number of labeling functions.",5.1. Sample Complexity,[0],[0]
"For n 2 {25, 50, 75, 100}, we set two pairs of
labeling functions to be correlated with ✓Cor jk = 0.25.",5.1. Sample Complexity,[0],[0]
"We set ✓Acc j
= 1.0 for all j. We then generate m samples for each setting of over 100 trials.",5.1. Sample Complexity,[0],[0]
Figure 1 shows the fraction of times Algorithm 1 returns the correct correlation structure as a function of the control parameter .,5.1. Sample Complexity,[0],[0]
"That the curves are aligned for different values of n shows that the sample complexity in practice scales logarithmically in n.
We next test the effect of varying d⇤, the maximum number of dependencies that affect a labeling function in the true distribution.",5.1. Sample Complexity,[0],[0]
"For 25 labeling functions, we add correlations to the true model to form cliques of increasing degree.",5.1. Sample Complexity,[0],[0]
All parameters are the same as in the previous experiment.,5.1. Sample Complexity,[0],[0]
"Figure 2 shows that for increasing values of d⇤, (9) again predicts the number of samples for Algorithm 1 to succeed.",5.1. Sample Complexity,[0],[0]
That the curves are aligned for different values of d⇤ shows that the sample complexity in practice scales linearly in d⇤.,5.1. Sample Complexity,[0],[0]
We next compare Algorithm 1 with an alternative approach.,5.2. Comparison with Maximum Likelihood,[0],[0]
"Without an efficient structure learning method, one could maximize the marginal likelihood of the observations ¯⇤ while considering all possible dependencies.",5.2. Comparison with Maximum Likelihood,[0],[0]
"To measure the benefits of maximizing the marginal pseudolikelihood, we compare its performance against an analogous maximum likelihood estimation routine that also uses stochastic gradient descent, but instead uses Gibbs sampling to estimate the intractable gradient of the objective.
",5.2. Comparison with Maximum Likelihood,[0],[0]
We create different distributions over n labeling functions by selecting with probability 0.05 pairs of labeling functions to make correlated.,5.2. Comparison with Maximum Likelihood,[0],[0]
"Again, the strength of correlation is set at ✓Cor
jk
= 0.25 and accuracy is set at ✓Acc j = 1.0.",5.2. Comparison with Maximum Likelihood,[0],[0]
"We generate 100 distributions for n 2 {25, 30, 35, . . .",5.2. Comparison with Maximum Likelihood,[0],[0]
", 100}.",5.2. Comparison with Maximum Likelihood,[0],[0]
"For each distribution we generate 10,000 samples and attempt to recover the true correlation structure.
",5.2. Comparison with Maximum Likelihood,[0],[0]
We first compare running time between the two methods.,5.2. Comparison with Maximum Likelihood,[0],[0]
"Our implementation of maximum likelihood estimation is designed for speed: for every sample taken to estimate the gradient, a small update to the parameters is performed.",5.2. Comparison with Maximum Likelihood,[0],[0]
"This approach is state-of-the-art for high-speed learning for factor graphs (Zhang & Ré, 2014).",5.2. Comparison with Maximum Likelihood,[0],[0]
"However, the need for sampling the variables ⇤ and Y is still computationally expensive.",5.2. Comparison with Maximum Likelihood,[0],[0]
"Figure 3 shows that by avoiding variable sampling, using pseudolikelihood is 100⇥ faster.
",5.2. Comparison with Maximum Likelihood,[0],[0]
"We next compare the accuracy of the two methods, which depends on the regularization ✏.",5.2. Comparison with Maximum Likelihood,[0],[0]
"The ideal is to maximize the probability of perfect recall with few extraneous correlations, because subsequent parameter estimation can reduce the influence of an extraneous correlation but cannot discover a missing correlation.",5.2. Comparison with Maximum Likelihood,[0],[0]
We tune ✏ independently for each method.,5.2. Comparison with Maximum Likelihood,[0],[0]
Figure 4 (top) shows that maximum pseudolikelihood is able to maintain higher levels of recall than maximum likelihood as the problem size increases.,5.2. Comparison with Maximum Likelihood,[0],[0]
"Figure 4 (bottom) shows that even tuned for better recall, maximum pseudolikelihood is more precise, returning 1/4 as many extraneous correlations.",5.2. Comparison with Maximum Likelihood,[0],[0]
"We interpret this improved accuracy as a benefit of computing the gradient for a data point exactly, as opposed to using Gibbs sampling to estimate it as in maximum likelihood estimation.",5.2. Comparison with Maximum Likelihood,[0],[0]
"We evaluate our method on several real-world information extraction applications, comparing the performance of data programming using dependencies selected by our method with the conditionally independent model (Table 1).",5.3. Real-World Applications,[0],[0]
"In the data programming method, users express a variety of weak supervision rules and sources such as regular expression patterns, distant supervision from dictionaries and existing knowledge bases, and other heuristics as labeling functions.",5.3. Real-World Applications,[0],[0]
"Due to the noisy and overlapping nature of these labeling functions, correlations arise.",5.3. Real-World Applications,[0],[0]
"Learning this correlation structure gives an average improvement of 1.5 F1 points.
",5.3. Real-World Applications,[0],[0]
"Extracting structured information from unstructured text by identifying mentioned entities and relations is a challenging task that is well studied in the context of weak supervision (Bunescu & Mooney, 2007; Alfonseca et al., 2012; Ratner et al., 2016).",5.3. Real-World Applications,[0],[0]
We consider three tasks: extracting mentions of specific diseases from the scientific literature (Disease Tagging); extracting mentions of chemicals inducing diseases from the scientific literature (ChemicalDisease); and extracting mentions of electronic device polarity from PDF parts sheet tables (Device Polarity).,5.3. Real-World Applications,[0],[0]
"In the first two applications, we consider a training set of 500 unlabeled abstracts from PubMed, and in the third case 100 PDF parts sheets consisting of mixed text and tabular data.",5.3. Real-World Applications,[0],[0]
"We use hand-labeled test sets to evaluate on the candidatemention-level performance, which is the accuracy of the classifier in identifying correct mentions of specific entities or relations, given a set of candidate mentions.",5.3. Real-World Applications,[0],[0]
"For example, in Chemical-Disease, we consider as candidates all pairs of co-occurring chemical-disease mention pairs as identified by standard preprocessing tools2.
",5.3. Real-World Applications,[0],[0]
We see that modeling the correlations between labeling functions gives gains in performance which appear to be correlated with the total number of sources.,5.3. Real-World Applications,[0],[0]
"For example, in the disease tagging application, we have 233 labeling
2 ncbi.nlm.nih.gov/CBBresearch/Lu/Demo/PubTator/index.cgi
functions, the majority of which check for membership in specific subtrees of a reference disease ontology using different matching heuristics.",5.3. Real-World Applications,[0],[0]
"There is overlap in the labeling functions which check identical subtrees of the ontology, and we see that our method increases end performance by a significant 2.6 F1 points by modeling this structure.
",5.3. Real-World Applications,[0],[0]
"Examining the Chemical-Disease task, we see that our method identifies correlations that are both obviously true and ones that are more subtle.",5.3. Real-World Applications,[0],[0]
"For example, our method learns dependencies between labeling functions that are compositions of one another, such as one labeling function checking for the pattern [CHEM] induc.",5.3. Real-World Applications,[0],[0]
*,5.3. Real-World Applications,[0],[0]
"[DIS], and a second labeling function checking for this pattern plus membership in an external knowledge base of known chemical-disease relations.",5.3. Real-World Applications,[0],[0]
"Our method also learns more subtle correlations: for example, it selected a correlation between a labeling function that checks for the presence of a chemical mention in between the chemical and disease mentions comprising the candidate, and one that checks for the pattern .*-induced appearing in between.",5.3. Real-World Applications,[0],[0]
"Our method is in large part motivated by the new programming model introduced by weak supervision, and the novel hurdles that developers face.",5.4. Accelerating Application Development,[0],[0]
"For example in the Disease Tagging application above, we observed developers significantly slowed down in trying to to leverage the rich disease ontologies and matching heuristics they had available without introducing too many dependencies between their labeling functions.",5.4. Accelerating Application Development,[0],[0]
"In addition to being slowed down, we also observed developers running into significant pitfalls due to unnoticed correlations between their weak supervision sources.",5.4. Accelerating Application Development,[0],[0]
"In one collaborator’s application, for every labeling function that referenced the words in a sentence, a corresponding labeling function referenced the lemmas, which were often identical, and this significantly degraded performance.",5.4. Accelerating Application Development,[0],[0]
"By automatically learning dependencies, we were able to significantly mitigate the effects of such correlations.",5.4. Accelerating Application Development,[0],[0]
"We therefore envision an accelerated development process enabled by our method.
",5.4. Accelerating Application Development,[0],[0]
"To further explore the way in which our method can protect against such types of failure modes, we consider adding correlated, random labeling functions to those used in the Chemical-Disease task.",5.4. Accelerating Application Development,[0],[0]
Figure 5 shows the average estimated accuracy of copies of a random labeling function.,5.4. Accelerating Application Development,[0],[0]
An independent model grows more confident that the random noise is accurate.,5.4. Accelerating Application Development,[0],[0]
"However, with structure learning, we identify that the noisy sources are not independent and they therefore do not outvote the real labeling functions.",5.4. Accelerating Application Development,[0],[0]
"In this way, structure learning can protect against failures as users experiment with sources of weak supervision.
",5.4. Accelerating Application Development,[0],[0]
Table 1.,5.4. Accelerating Application Development,[0],[0]
"Candidate-mention scores of information extraction applications trained with data programming using generative models with no dependency structure (Independent) and learned dependency structure (Structure).
",5.4. Accelerating Application Development,[0],[0]
APPLICATION INDEPENDENT STRUCTURE F1 DIFF.,5.4. Accelerating Application Development,[0],[0]
#,5.4. Accelerating Application Development,[0],[0]
LFS # COR.,5.4. Accelerating Application Development,[0],[0]
% CORR.P,5.4. Accelerating Application Development,[0],[0]
"R F1 P R F1
DISEASE TAGGING 60.4 73.3 66.3 68.0 69.8 68.9 2.6 233 315 1.17% CHEMICAL-DISEASE 45.1 69.2 54.6 46.8 69.0 55.9 1.3 33 21 3.98% DEVICE POLARITY 78.9 99.6 88.1 80.5 98.6 88.7 0.6 12 32 48.49%
Figure 5.",5.4. Accelerating Application Development,[0],[0]
"Structure learning identifies and corrects correlated, random labeling functions added to the Chemical-Disease task.",5.4. Accelerating Application Development,[0],[0]
"Structure learning is a well-studied problem, but most work has assumed access to hand-labeled training data.",6. Related Work,[0],[0]
Some of the earliest work has focused on generalized linear models.,6. Related Work,[0],[0]
"The lasso (Tibshirani, 1996), linear regression with `1 regularization, is a classic technique.",6. Related Work,[0],[0]
Zhao & Yu (2006) showed that the lasso is a consistent structure estimator.,6. Related Work,[0],[0]
"The Dantzig selector (Candes & Tao, 2007) is another structure estimator for linear models that uses `1, which can learn in the high-dimensional setting where there are more possible dependencies than samples.",6. Related Work,[0],[0]
Ng (2004) showed that `1-regularized logistic regression has sample complexity logarithmic in the number of features.,6. Related Work,[0],[0]
"`1 regularization has also been used as a prior for compressed sensing (e.g., Donoho & Elad, 2003; Tropp, 2006; Wainwright, 2009).
",6. Related Work,[0],[0]
Regularized estimators have also been used to select structures for graphical models.,6. Related Work,[0],[0]
Meinshausen & Bühlmann (2006) showed that parameter learning with `1 regularization for Gaussian graphical models under similar assumptions also consistently selects the correct structure.,6. Related Work,[0],[0]
"Most similar to our proposed estimator, Ravikumar et al. (2010) propose a fully supervised pseudolikelihood estimator for Ising models.",6. Related Work,[0],[0]
"Also related is the work of Chandrasekaran et al. (2012), which considers learning the structure of Gaussian graphical models with latent variables.",6. Related Work,[0],[0]
"Other techniques for learning the structure of graphical models include grafting (Perkins et al., 2003; Zhu et al., 2010) and the information bottleneck approach for learning Bayesian
networks with latent variables (Elidan & Friedman, 2005).
",6. Related Work,[0],[0]
Using heuristic sources of labels is increasingly common.,6. Related Work,[0],[0]
"Treating labels from a single heuristic source as gold labels is called distant supervision (Craven & Kumlien, 1999; Mintz et al., 2009).",6. Related Work,[0],[0]
"Some methods use multi-instance learning to reduce the noise in a distant supervision source (Riedel et al., 2010; Hoffmann et al., 2011).",6. Related Work,[0],[0]
"Others use hierarchical topic models to generate additional training data for weak supervision, but they do not support user-provided heuristics (Alfonseca et al., 2012; Takamatsu et al., 2012; Roth & Klakow, 2013a;b).",6. Related Work,[0],[0]
"Previous methods that support heuristics for weak supervision (e.g., Bunescu & Mooney, 2007; Shin et al., 2015) do not model the noise inherent in these sources.",6. Related Work,[0],[0]
"Also, Downey & Etzioni (2008) showed that PAC learning is possible without hand-labeled data if the features monotonically order data by class probability.
",6. Related Work,[0],[0]
"Estimating the accuracy of multiple label sources without a gold standard is a classic problem (Dawid & Skene, 1979), and many proposed approaches are generalized in the data programming framework.",6. Related Work,[0],[0]
Parisi et al. (2014) proposed a spectral approach to estimating the accuracy of members of classifier ensembles.,6. Related Work,[0],[0]
"Many methods for crowdsourcing estimate the accuracy of workers without hand-labeled data (e.g., Dalvi et al., 2013; Joglekar et al., 2015; Zhang et al., 2016).",6. Related Work,[0],[0]
"In data programming, the scaling of data to label sources is different from crowdsourcing; a relatively small number of sources label all the data.",6. Related Work,[0],[0]
We can therefore learn rich dependency structures among the sources.,6. Related Work,[0],[0]
We showed that learning the structure of a generative model enables higher quality data programming results.,7. Conclusion and Future Directions,[0],[0]
Our method for structure learning is also 100⇥ faster than a maximum likelihood approach.,7. Conclusion and Future Directions,[0],[0]
"If data programming and other forms of weak supervision are to make machine learning tools easier to develop, selecting accurate structures for generative models with minimal user intervention is a necessary capability.",7. Conclusion and Future Directions,[0],[0]
Interesting questions remain.,7. Conclusion and Future Directions,[0],[0]
Can the guarantee of Theorem 1 be tightened for higher-order dependencies to match the pairwise case of Corollary 2?,7. Conclusion and Future Directions,[0],[0]
Preliminary experiments show that they converge at similar rates in practice.,7. Conclusion and Future Directions,[0],[0]
"Thanks to Christopher De Sa for helpful discussions, and Henry Ehrenberg and Sen Wu for assistance with experiments.",Acknowledgements,[0],[0]
We gratefully acknowledge the support of the Defense Advanced Research Projects Agency (DARPA) SIMPLEX program under No.,Acknowledgements,[0],[0]
"N66001-15-C-4043, the DARPA D3M program under No. FA8750-17-2-0095, the National Science Foundation (NSF) CAREER Award under No. IIS- 1353606, the Office of Naval Research (ONR) under awards",Acknowledgements,[0],[0]
No.,Acknowledgements,[0],[0]
"N000141210041 and No. N000141310129, a Sloan Research Fellowship, the Moore Foundation, an Okawa Research Grant, Toshiba, and Intel.",Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, NSF, ONR, or the U.S. government.",Acknowledgements,[0],[0]
Curating labeled training data has become the primary bottleneck in machine learning.,abstractText,[0],[0]
Recent frameworks address this bottleneck with generative models to synthesize labels at scale from weak supervision sources.,abstractText,[0],[0]
"The generative model’s dependency structure directly affects the quality of the estimated labels, but selecting a structure automatically without any labeled data is a distinct challenge.",abstractText,[0],[0]
We propose a structure estimation method that maximizes the `1regularized marginal pseudolikelihood of the observed data.,abstractText,[0],[0]
Our analysis shows that the amount of unlabeled data required to identify the true structure scales sublinearly in the number of possible dependencies for a broad class of models.,abstractText,[0],[0]
Simulations show that our method is 100⇥ faster than a maximum likelihood approach and selects 1/4 as many extraneous dependencies.,abstractText,[0],[0]
"We also show that our method provides an average of 1.5 F1 points of improvement over existing, user-developed information extraction applications on real-world data such as PubMed journal abstracts.",abstractText,[0],[0]
Learning the Structure of Generative Models without Labeled Data,title,[0],[0]
"Decentralized partially observable Markov decision processes (Dec-POMDPs) emerged as the standard framework for sequential decision making by a team of collaborative agents (Bernstein et al., 2000).",1. Introduction,[0],[0]
"A key assumption of DecPOMDPs is that agents can neither see the actual state of the system nor explicitly communicate their noisy observations with each other due to communication cost, latency or noise, hence providing a partial explanation of the double exponential growth at every control interval of the required memory in optimal algorithms (Hansen et al., 2004; Szer et al., 2005; Oliehoek et al., 2008; Amato et al., 2009; Oliehoek et al., 2013; Dibangoye et al., 2015; 2016).",1. Introduction,[0],[0]
"While planning methods for finite Dec-POMDPs made substantial progress in recent years, the formal treatment of the corresponding reinforcement learning problems received little attention so far.",1. Introduction,[0],[0]
"The literature of multi-agent reinforcement learning (MARL) can be divided into two main categories: concurrent and team approaches (Tan, 1998; Panait & Luke, 2005).
",1. Introduction,[0],[0]
"1Univ Lyon, INSA Lyon, INRIA, CITI, F-69621 Villeurbanne, France 2INRIA / Université de Lorraine, Nancy, France.",1. Introduction,[0],[0]
"Correspondence to: Jilles S. Dibangoye <jilles.dibangoye@inria.fr>, Olivier Buffet <olivier.buffet@loria.fr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Perhaps the dominant paradigm in MARL is the concurrent approach, which involves multiple simultaneous learners: typically, each agent has its learning process.",1. Introduction,[0],[0]
"Self-interested learners, for example, determine their best-response behaviors considering their opponents are part of the environment, often resulting in local optima (Brown, 1951; Hu & Wellman, 1998; Littman, 1994).",1. Introduction,[0],[0]
"While concurrent learning can apply in Dec-POMDPs, a local optimum may lead to severely suboptimal performances (Peshkin et al., 2000; Zhang & Lesser, 2011; Kraemer & Banerjee, 2016; Nguyen et al., 2017).",1. Introduction,[0],[0]
"Also, methods of this family face two conceptual issues that limit their applicability.",1. Introduction,[0],[0]
"The primary concern is that of the co-adaptation dilemma, which arises when each attempt to modify an agent behavior can ruin learned behaviors of its teammates.",1. Introduction,[0],[0]
"Another major problem is that of the multi-agent credit assignment, that is, how to split the collective reward among independent learners.
",1. Introduction,[0],[0]
"Alternatively, the team approach involves a single learner acting on behalf of all agents to discover a collective solution (Salustowicz et al., 1998; Miconi, 2003).",1. Introduction,[0],[0]
"Interestingly, this approach circumvents the difficulties arising from both the co-adaptation and the multi-agent credit assignment.",1. Introduction,[0],[0]
"Coordinated agents, for example, simultaneously learn their control choices and the other agent strategies assuming instantaneous and free explicit communications (Guestrin et al., 2002; Kok & Vlassis, 2004).",1. Introduction,[0],[0]
"While methods of this family inherit from standard single-agent techniques, they need to circumvent two significant drawbacks: the explosion in the state space size; and the centralization of all learning resources in a single place.",1. Introduction,[0],[0]
"Recently, team algorithms ranging from Q-learning to policy-search have been introduced for finite Dec-POMDPs, but with no guaranteed global optimality (Wu et al., 2013; Liu et al., 2015; 2016; Kraemer & Banerjee, 2016).",1. Introduction,[0],[0]
"So, it seems one can either compute local optima with arbitrary bad performances or calculate optimal solutions but assuming noise-free, instantaneous and explicit communications.
",1. Introduction,[0],[0]
"A recent approach to optimally solving Dec-POMDPs suggests recasting them into occupancy-state MDPs (oMDPs) and then applying (PO)MDP solution methods (Dibangoye et al., 2013; 2014a;b; 2016).",1. Introduction,[0],[0]
"In these oMDPs, the states called occupancy states are distributions over hidden states and joint histories of the original problem, and actions called decision rules are mappings from joint histories to con-
trols (Nayyar et al., 2011; Oliehoek, 2013; Dibangoye et al., 2016).",1. Introduction,[0],[0]
This approach achieves scalability gains by exploiting the piece-wise linearity and convexity of the optimal value function.,1. Introduction,[0],[0]
"Since this methodology was successfully applied for planning in Dec-POMDPs, it is natural to wonder which benefits it could bring to the corresponding MARL problem.",1. Introduction,[0],[0]
"Unfortunately, a straightforward application of standard RL methods to oMDPs will face three severe limitations.",1. Introduction,[0],[0]
"First, occupancy states are unknown, and hence must be estimated.",1. Introduction,[0],[0]
"Second, they lie in a continuum making tabular RL methods inapplicable.",1. Introduction,[0],[0]
"Finally, the greedy maximization is computationally demanding in decentralized stochastic control problems (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010).
",1. Introduction,[0],[0]
"This paper extends the methodology of Dibangoye et al. to MARL, focussing on the three major issues that limit its applicability.",1. Introduction,[0],[0]
"Our primary result is the proof that, by restricting attention to plans instead of policies, a linear function over occupancy states and joint decision rules, which is simple to store and update, can capture the optimal performance for Dec-POMDPs.",1. Introduction,[0],[0]
"We further use plans instead of policies in a policy iteration algorithm, with the plan always being improved with respect to a linear function and a linear function always being driven toward the linear function for the plan.",1. Introduction,[0],[0]
"Under accurate estimation of the occupancy states, the resulting algorithm, called occupancystate SARSA (oSARSA) (Rummery, G. A. and Niranjan, 1994), is guaranteed to converge with probability one to a near-optimal plan for any finite Dec-POMDP.",1. Introduction,[0],[0]
"To extend its applicability to higher-dimensional domains, oSARSA replaces the greedy (or soft) maximization by a mixed-integer linear program for finite settings.",1. Introduction,[0],[0]
"Altogether, we obtain a MARL algorithm that can apply to finite Dec-POMDPs.",1. Introduction,[0],[0]
"Experiments show our approach can learn to act near-optimally in many finite domains from the literature.
",1. Introduction,[0],[0]
We organize the remainder of this paper as follows.,1. Introduction,[0],[0]
"Section 2 extends a recent planning theory, starting with a formal definition of finite Dec-POMDPs.",1. Introduction,[0],[0]
We proceed with the introduction of a framework for centralized MARL in Dec-POMDPs along with solutions to the three limitations mentioned above in Section 3.,1. Introduction,[0],[0]
We further present the resulting algorithm oSARSA along with convergence guarantees in Section 4.,1. Introduction,[0],[0]
"Finally, we conduct experiments in Section 5, demonstrating our approach learns to act optimally in many finite domains from the literature.",1. Introduction,[0],[0]
"Proofs are provided in the companion technical report (Dibangoye & Buffet, 2018).
2.",1. Introduction,[0],[0]
Planning in Dec-POMDPs as oMDPs,1. Introduction,[0],[0]
A finite Dec-POMDP is a tuple M .,2.1. Finite Dec-POMDPs,[0],[0]
"“ pn,X, tU iu, tZiu, p, r, `, γ, b0q, where n denotes the
number of agents involved in the decentralized stochastic control process; X is a finite set of hidden world states, denoted x or y; U i is a finite private control set of agent i P v1;nw, where U “ U1 ˆ ¨ ¨ ¨ ˆ Un specifies the set of controls u “ pu1, . . .",2.1. Finite Dec-POMDPs,[0],[0]
", unq; Zi is a finite private observation set of agent i, where Z “ Z1 ˆ ¨ ¨ ¨ ˆ",2.1. Finite Dec-POMDPs,[0],[0]
"Zn specifies the set of observations z “ pz1, . . .",2.1. Finite Dec-POMDPs,[0],[0]
", znq; p describes a transition function with conditional density pu,zpx, yq; r is a reward model with immediate reward rpx, uq, we assume rewards are two-side bounded, i.e., for some c P R`, @x",2.1. Finite Dec-POMDPs,[0],[0]
"P X,u P U : |rpx, uq| ď c; ` is the planning horizon; γ P r0, 1s denotes the discount factor; and b0 is the initial belief state with density b0px0q.",2.1. Finite Dec-POMDPs,[0],[0]
"We shall restrict attention to finite planning horizon ` ă 8 since an infinite planning horizon solution is within a small scalar ą 0 of a finite horizon solution where ` “ rlogγpp1´ γq {cqs.
Because we are interested in MARL, we assume an incomplete knowledge aboutM , i.e., p and r are either unavailable or only through a generative model.",2.1. Finite Dec-POMDPs,[0],[0]
"Hence, the goal of solving M is to find a plan, i.e., a tuple of individual decision rules, one for each agent and time step: ρ .“",2.1. Finite Dec-POMDPs,[0],[0]
"pa10:`, . . .",2.1. Finite Dec-POMDPs,[0],[0]
", an0:`q.",2.1. Finite Dec-POMDPs,[0],[0]
A tth individual decision rule ait :,2.1. Finite Dec-POMDPs,[0],[0]
"O i t ÞÑ PpU iq of agent i prescribes private controls based on the whole information available to the agent up to the tth time step, i.e., history of controls and observations oit “ pui0:t´1, zi1:tq, where oi0 “ H and oit P Oit.",2.1. Finite Dec-POMDPs,[0],[0]
"A tth joint decision rule, denoted at : Ot ÞÑ PpUq, can be specified as atpu|oq
.“",2.1. Finite Dec-POMDPs,[0],[0]
"śn i“1 a i tpui|oiq, where Ot
.“",2.1. Finite Dec-POMDPs,[0],[0]
"O1t ˆ ¨ ¨ ¨ ˆ Ont , oi P Oit and o .“",2.1. Finite Dec-POMDPs,[0],[0]
"po1, . . .",2.1. Finite Dec-POMDPs,[0],[0]
", onq P Ot.",2.1. Finite Dec-POMDPs,[0],[0]
"From control interval t onward, agents collectively receive discounted cumulative rewards, denoted by random variable Rt
.“",2.1. Finite Dec-POMDPs,[0],[0]
"λ1rt` ¨ ¨ ¨`λ`r`, where λt denotes the time-step dependent weighting factors, often set to λt “ γt for discounted problems.",2.1. Finite Dec-POMDPs,[0],[0]
"For any control interval t, joint plans a0:t of interest are those that achieve the highest performance measure Jpa0:tq
.“",2.1. Finite Dec-POMDPs,[0],[0]
"Ea0:t tR0 | b0u starting at b0, where Ea0:tt¨u denotes the expectation with respect to the probability distribution over state-action pairs joint plan a0:t induces, in particular Jpρq
.“",2.1. Finite Dec-POMDPs,[0],[0]
Jpa0:`´1q for ρ .“,2.1. Finite Dec-POMDPs,[0],[0]
a0:`´1.,2.1. Finite Dec-POMDPs,[0],[0]
"One can show that, in Dec-POMDPs, there always exists a deterministic plan that is as good as any stochastic plan (see Puterman, 1994, Lemma 4.3.1).",2.1. Finite Dec-POMDPs,[0],[0]
"Unfortunately, there is no direct way to apply the theory developed for Markov decision processes (Bellman, 1957; Puterman, 1994) to Dec-POMDPs, including: the Bellman optimality equation; or the policy improvement theorem.",2.1. Finite Dec-POMDPs,[0],[0]
"To overcome these limitations, we rely on a recent theory by Dibangoye et al. that recasts M into an MDP, thereby allowing knowledge transfer from the MDP setting to Dec-POMDPs.",2.1. Finite Dec-POMDPs,[0],[0]
"To overcome the fact that agents can neither see the actual state of the system nor explicitly communicate their noisy observations with each other, Szer et al. (2005) and later
on Dibangoye et al. (2016) suggest formalizing M from the perspective of a centralized algorithm.",2.2. Occupancy-State MDPs,[0],[0]
"A centralized algorithm acts on behalf of the agents by selecting a joint decision rule to be executed at each control interval based on all data available about the system, namely the information state.",2.2. Occupancy-State MDPs,[0],[0]
"The information state at the end of control interval t, denoted ιt`1
.“",2.2. Occupancy-State MDPs,[0],[0]
"pb0, a0:tq, is a sequence of joint decision rules the centralized algorithm selected starting at the initial belief state.",2.2. Occupancy-State MDPs,[0],[0]
"Hence, the information state satisfies the following recursion: ι0 .“",2.2. Occupancy-State MDPs,[0],[0]
pb0q and ιt`1 .,2.2. Occupancy-State MDPs,[0],[0]
"“ pιt, atq for all control interval t, resulting in an ever-growing sequence.",2.2. Occupancy-State MDPs,[0],[0]
"To generalize the value from one information state to another one, Dibangoye et al. introduced the concept of occupancy states.",2.2. Occupancy-State MDPs,[0],[0]
"The occupancy state at control interval t, denoted st
.“",2.2. Occupancy-State MDPs,[0],[0]
"Ppxt, ot|ιtq, is a distribution over hidden states and joint histories conditional on information state ιt at control interval t. Interestingly, the occupancy state has many important properties.",2.2. Occupancy-State MDPs,[0],[0]
"First, it is a sufficient statistic of the information state when estimating the (current and future) reward to be gained by executing a joint decision rule:
Rpst, atq .“",2.2. Occupancy-State MDPs,[0],[0]
"ÿ
x
ÿ
o
stpx, oq ÿ
u
atpu|oq ¨ rpx, uq.
",2.2. Occupancy-State MDPs,[0],[0]
"In addition, it describes a deterministic and fully observable Markov decision process, where the next occupancy state depends only on the current occupancy state and next joint decision rule, for all y P X, o P O, u P U, z P Z:
T pst, atq .“",2.2. Occupancy-State MDPs,[0],[0]
"st`1
st`1py, po, u, zqq .“",2.2. Occupancy-State MDPs,[0],[0]
"atpu|oq ÿ
x
stpx, oq ¨ pu,zpx, yq.
",2.2. Occupancy-State MDPs,[0],[0]
"The process the occupancy states describe is known as the occupancy-state Markov decision process (oMDP), and denoted M 1 .“",2.2. Occupancy-State MDPs,[0],[0]
"pS,A,R, T, `, γ, s0q.",2.2. Occupancy-State MDPs,[0],[0]
"Similarly to POMDPs, it was proven that Dec-POMDPs can be recasted into MDPs, called oMDPs, and an optimal solution of the resulting oMDP is also an optimal solution for the original DecPOMDP (Dibangoye et al., 2016).",2.2. Occupancy-State MDPs,[0],[0]
"M 1 is an `-steps deterministic and continuous MDP with respect to M , where S .“",2.2. Occupancy-State MDPs,[0],[0]
YtPv0;`´1w St is the set of occupancy states up to control interval ` ´ 1; A .“,2.2. Occupancy-State MDPs,[0],[0]
"YtPv0;`´1w At is the set of joint decision rules up to control interval `´ 1; R is the reward model; and T is the transition rule; s0 is the initial occupancy state, which is essentially the initial belief in M ; γ and ` are as in M .",2.2. Occupancy-State MDPs,[0],[0]
"It is worth noticing that there is no need to construct explicitly M 1; instead we use M (when available) as a generative model for the occupancy states T pst, atq and rewards Rpst, atq, for all control intervals t.
To better understand why we use plans instead of policies and how they relate, consider the MDP case.",2.2. Occupancy-State MDPs,[0],[0]
"The solution of any finite MDP called a policy π : S ÞÑ A can be represented as a decision tree, where nodes are labelled with
actions and arcs are labelled with states.",2.2. Occupancy-State MDPs,[0],[0]
"Since an oMDP is also an MDP, its policies can also be represented as decision trees, except that actions are decision rules and states are occupancy states.",2.2. Occupancy-State MDPs,[0],[0]
"In contrast to standard MDPs, oMDPs are deterministic.",2.2. Occupancy-State MDPs,[0],[0]
"This means that only a single branch in the decision-tree representation—i.e., a sequence of actions—is necessary to act optimally in oMDPs.",2.2. Occupancy-State MDPs,[0],[0]
A single branch of a decision tree is called a plan.,2.2. Occupancy-State MDPs,[0],[0]
"Hence policies are more general than plans, but in deterministic MDPs, both can be employed while achieving optimal performance (plans inducing an open-loop approach, and policies a closed-loop approach).",2.2. Occupancy-State MDPs,[0],[0]
We shall restrict attention to plans because they are more concise than policies.,2.2. Occupancy-State MDPs,[0],[0]
"Below, we review a closedloop approach based on the dynamic programming theory (Bellman, 1957).
",2.2. Occupancy-State MDPs,[0],[0]
"For any finiteM , the Bellman equation is written as follows: for all occupancy state st P St, and some fixed policy π,
V πt pstq .",2.2. Occupancy-State MDPs,[0],[0]
"“ Rpst, πpstqq ` λ1V πt`1pT pst, πpstqqq (1)
with boundary condition V π` p¨q .“",2.2. Occupancy-State MDPs,[0],[0]
"0, describes the return of a particular occupancy state st when taking decision rule at “ πpstq prescribed by π.",2.2. Occupancy-State MDPs,[0],[0]
"The equation for an optimal policy π˚ is referred to as the Bellman optimality equation: for any control interval t, and occupancy state st,
V ˚t pstq .“",2.2. Occupancy-State MDPs,[0],[0]
"maxatPA Rpst, atq ` λ1V ˚t`1pT pst, atqq (2)
with boundary condition V ˚` p¨q .“",2.2. Occupancy-State MDPs,[0],[0]
0.,2.2. Occupancy-State MDPs,[0],[0]
"Unfortunately, occupancy states lie in a continuum, which makes exact dynamic programming methods infeasible.",2.2. Occupancy-State MDPs,[0],[0]
"Interestingly, when optimized exactly, the value function solution of (2) along with the boundary condition is always piece-wise linear and convex in the occupancy-state space (Dibangoye et al., 2016).
",2.2. Occupancy-State MDPs,[0],[0]
Lemma 1.,2.2. Occupancy-State MDPs,[0],[0]
"For any arbitrary M 1, the solution V ˚0:` of (2) is convex in the occupancy-state space.",2.2. Occupancy-State MDPs,[0],[0]
"If we restrict attention to deterministic policies and finite M (and corresponding M 1), the solution of (2) is piece-wise linear and convex in the occupancy-state space.",2.2. Occupancy-State MDPs,[0],[0]
"Hence, the optimal value at any occupancy state st is as follows:
V ˚t pstq .",2.2. Occupancy-State MDPs,[0],[0]
"“ maxαtPΓt xst, αty, (3)
where xst, αty is used to express the expectation of a linear function",2.2. Occupancy-State MDPs,[0],[0]
"αt (also called α-vectorin the probability space defined by sample space X ˆO, the σ-algebra X ˆO",2.2. Occupancy-State MDPs,[0],[0]
"and the probability distribution st; and Γt is the set of all tth α-vectors.
",2.2. Occupancy-State MDPs,[0],[0]
"Lemma 1 shows that for any arbitrary M and corresponding M 1, the solution of (2), represented by sets Γ0:`, is convex in the occupancy-state space.",2.2. Occupancy-State MDPs,[0],[0]
Each α-vector defines the value function over a bounded region of the occupancy-state space.,2.2. Occupancy-State MDPs,[0],[0]
"In addition, it is associated with a plan, defining the
optimal plan for a bounded region of the occupancy-state space.",2.2. Occupancy-State MDPs,[0],[0]
Sets Γ0:` are iteratively improved by adding a new α-vector that dominates current ones over certain regions of the occupancy-state space.,2.2. Occupancy-State MDPs,[0],[0]
"The α-vector to be added is computed using point-based Bellman backup operator H:
rHΓt`1spstq “ arg max αat : aPAt,αt`1PΓt`1 xst, αat y,
where αat px, oq .“",2.2. Occupancy-State MDPs,[0],[0]
"Etrpx, uq ` λ1αt`1py, po, u, zqq|au, for each hidden state x P X , and joint history o",2.2. Occupancy-State MDPs,[0],[0]
"P O. To keep the number of α-vectors manageable, one can prune those that are dominated over the entire occupancy-state space.",2.2. Occupancy-State MDPs,[0],[0]
"All in all, the oMDP reformulation permits us to solve finite M by means of M 1 using near-optimal planning methods leveraging on the special structure of the optimal value function (Shani et al., 2013).",2.2. Occupancy-State MDPs,[0],[0]
"This methodology results in the current state-of-the-art algorithm to optimally solving finite Dec-POMDPs (Dibangoye et al., 2016).",2.2. Occupancy-State MDPs,[0],[0]
So it seems natural to wonder if the same methodology can also succeed when applied to the corresponding reinforcement-learning problem.,2.2. Occupancy-State MDPs,[0],[0]
"In other words, how can a centralized algorithm learn to coordinate a team of agents with possibly contradicting perceptual information?
3.",2.2. Occupancy-State MDPs,[0],[0]
"Learning in Dec-POMDPs as oMDPs Using the oMDP reformulation, a natural approach to achieve centralized RL for decentralized stochastic control suggests applying exact RL methods.",2.2. Occupancy-State MDPs,[0],[0]
"In the Q-learning algorithm (Watkins & Dayan, 1992), for example, one would learn directly the Q-value function when following a fixed policy π: for any control interval t P v0; `´ 1w,
Qπt pst, atq .“",2.2. Occupancy-State MDPs,[0],[0]
"Rpst, atq ` λ1V πt`1pT pst, atqq (4)
with boundary condition Qπ` p¨, ¨q “ 0.",2.2. Occupancy-State MDPs,[0],[0]
"The policy improvement theorem provides a procedure to change a sub-optimal policy π into an improved one π̄ (Howard, 1960): for any control interval t P v0; `´ 1w,
π̄pstq .",2.2. Occupancy-State MDPs,[0],[0]
“,2.2. Occupancy-State MDPs,[0],[0]
"arg maxatPAt Q π t pst, atq. (5)
",2.2. Occupancy-State MDPs,[0],[0]
"Unfortunately, this approach has three severe limitations.",2.2. Occupancy-State MDPs,[0],[0]
"First, the occupancy states are unknown and must be estimated.",2.2. Occupancy-State MDPs,[0],[0]
"Second, even if we assume a complete knowledge of the occupancy states, they lie in a continuum, which precludes exact RL methods to accurately predict α-vectors even in the limit of infinite time and data.",2.2. Occupancy-State MDPs,[0],[0]
"Finally, the greedy maximization required to improve the value function proved to be NP-hard in finite settings (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010).",2.2. Occupancy-State MDPs,[0],[0]
"Although mappings T and R in M 1 are unknown to either agents or a centralized algorithm, one can instead estimate
on the fly both T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq for some fixed plan ρ .“ a0:`´1 through successive interactions of agents with the environment.",3.1. Addressing Estimation Issues,[0],[0]
"To this end, we shall distinguish between two settings.",3.1. Addressing Estimation Issues,[0],[0]
"The first one assumes a generative model is available during the centralized learning phase, e.g. a black box simulator; and the second does not.",3.1. Addressing Estimation Issues,[0],[0]
"In both cases, we build on the concept of replay pool (Mnih et al., 2015), except that we extend it from stationary single-agent domains to non-stationary multi-agent domains.
",3.1. Addressing Estimation Issues,[0],[0]
"If a generative model is available during the learning phase, then a Monte Carlo method can approximate T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq arbitrary closely.",3.1. Addressing Estimation Issues,[0],[0]
"To this end, the generative model allows the agents to sample experiences generated from M .",3.1. Addressing Estimation Issues,[0],[0]
An `-steps experience is a 4-tuple ξ .,3.1. Addressing Estimation Issues,[0],[0]
"“ px0:`´1, u0:`´1, r0:`´1, z1:`q, where x0:`´1 are sampled hidden states, u0:`´1 are controls made, r0:`´1 are reward signals drawn from the reward model, and z1:` are the resulting observations, drawn from the dynamics model.",3.1. Addressing Estimation Issues,[0],[0]
"If we let Dρ .“ tξrisuiPv1:Kw be the replay pool of K i.i.d random samples created through successive interactions with the generative model, then empirical occupancy state ŝt « T ps0, a0:t´1q and reward R̂t «",3.1. Addressing Estimation Issues,[0],[0]
"RpT ps0, a0:t´1q, atq corresponding to the current Dρ are given by: for any control interval t P v0 : `´ 1w,
ŝtpx, oq .“",3.1. Addressing Estimation Issues,[0],[0]
"1K řK i“1δxpx ris t q ¨ δopu ris 0:t, z ris 1:tq (6)
and R̂t .“",3.1. Addressing Estimation Issues,[0],[0]
"1K řK i“1r ris t , (7)
where δxp¨q and δop¨q denote the delta-Dirac mass located in hidden state and joint history pair, respectively.",3.1. Addressing Estimation Issues,[0],[0]
"By the law of large numbers the sequence of averages of these estimates converges to their expected values, and the standarddeviation of its error falls as 1{ ?",3.1. Addressing Estimation Issues,[0],[0]
"K (Sutton & Barto, 1998, chapter 5).",3.1. Addressing Estimation Issues,[0],[0]
"The error introduced by Monte Carlo when estimating T pŝt´1, at´1q instead of T ps0, a0:t´1q is upper bounded by 2`{ ?",3.1. Addressing Estimation Issues,[0],[0]
K.,3.1. Addressing Estimation Issues,[0],[0]
The proof follows from the performance guarantee of the policy-search algorithm by Bagnell et al. (2004).,3.1. Addressing Estimation Issues,[0],[0]
"Hence, to ensure the learned value function is within ą 0 of the optimal one, one should set the replaypool size to K “ Θp4 ` 2 2 q.
When no generative model is available, the best we can do is to store samples agents collected during the learning phase into replay pools",3.1. Addressing Estimation Issues,[0],[0]
"Dρ, one experience for each episode within the limit size of K. We maintain only the K recent experiences, and may discard1 hidden states since they are unnecessary for the updates of future replay pools and the performance measure.",3.1. Addressing Estimation Issues,[0],[0]
"The rationale behind this approach is that it achieves the same performances as a Monte Carlo method for the task of approximating T ps0, a0:t´1q and RpT ps0, a0:t´1q, atq given a fixed plan ρ
.“ a0:`´1.",3.1. Addressing Estimation Issues,[0],[0]
"In fact, if we let Dρ be a replay pool of K i.i.d. samples
1Note that one should keep hidden states when available since they often speed up the convergence.
generated according to ρ,",3.1. Addressing Estimation Issues,[0],[0]
"the empirical occupancy state ŝt « T ps0, a0:t´1q and reward R̂t «",3.1. Addressing Estimation Issues,[0],[0]
"RpT ps0, a0:t´1q, atq corresponding to Dρ are given by (6) and (7), respectively.",3.1. Addressing Estimation Issues,[0],[0]
One can further show this approach preserves performance guarantees similar to those obtained when using a generative model.,3.1. Addressing Estimation Issues,[0],[0]
"The key issue with large spaces of occupancy states and decision rules is that of generalization, that is, how experiences with a limited subset of occupancy states and decision rules can produce a good approximation over a much larger space.",3.2. Addressing Prediction Issues,[0],[0]
"Fortunately, a fundamental property of oMDPs is the convexity of the optimal value function over the occupancystate space, see Lemma 1.",3.2. Addressing Prediction Issues,[0],[0]
"Building on this property, we demonstrate a simple yet important preliminary result before stating the main result of this section.
",3.2. Addressing Prediction Issues,[0],[0]
Lemma 2.,3.2. Addressing Prediction Issues,[0],[0]
"For any arbitrary M 1 (resp. M ), the optimal Qvalue function is the upper envelope of sets Ω˚0:` of α-vectors over occupancy states and joint decision rules: for any control interval t, Q˚t pst, atq “ maxqtPΩ˚t",3.2. Addressing Prediction Issues,[0],[0]
"xsτ d aτ , qty, where qt P Ω˚t are appropriate α-vectors, and sτ d aτ denotes the Hadamard product2.
",3.2. Addressing Prediction Issues,[0],[0]
Lemma 2 generalizes the convexity property demonstrated in Lemma 1 from optimal value functions over occupancy states to optimal value functions over occupancy states and decision rules.,3.2. Addressing Prediction Issues,[0],[0]
"As a consequence, finite sets Ω˚0:`´1 of αvectors can produce solutions arbitrarily close to the optimal Q-value function Q˚0:`´1.",3.2. Addressing Prediction Issues,[0],[0]
"Though Q-value function Q ˚ 0:`´1 generalizes from a pair of occupancy state and decision rule to another one, storing and updating a convex hull is non trivial.",3.2. Addressing Prediction Issues,[0],[0]
"Instead of learning the optimal Q-value function over all occupancy states and decision rules, we explore a simpler yet tractable alternative, which will prove sufficient to preserve ability to eventually find an optimal plan starting at initial occupancy state s0.",3.2. Addressing Prediction Issues,[0],[0]
Theorem 1.,3.2. Addressing Prediction Issues,[0],[0]
"For any arbitrary M 1 (resp. M ), the Q-value functionQρ ˚
0:`´1 under an optimal plan ρ ˚ .“",3.2. Addressing Prediction Issues,[0],[0]
"a˚0:`´1 starting
at initial occupancy state s0 is linear in occupancy states and decision rules:",3.2. Addressing Prediction Issues,[0],[0]
"Qρ ˚ t pst, atq “ xst d at, q ρ˚
t y where qρ ˚
t .“",3.2. Addressing Prediction Issues,[0],[0]
arg maxqtPΩ˚t xT,3.2. Addressing Prediction Issues,[0],[0]
"ps0, a ˚ 0:t´1q",3.2. Addressing Prediction Issues,[0],[0]
"d a˚t , qty .
",3.2. Addressing Prediction Issues,[0],[0]
Theorem 1 proves that the Q-function for a given optimal joint plan achieves performance at the initial occupancy state s0 as good as the Q-value function for an optimal joint policy.,3.2. Addressing Prediction Issues,[0],[0]
"Standard policy iteration algorithms search for an optimal joint policy, which requires a finite set of αvectors to approximate V ˚/Q˚, hence the resulting PWLC approximator is tight almost everywhere.",3.2. Addressing Prediction Issues,[0],[0]
"Building upon Theorem 1, we search for an optimal ρ, which requires
2@px, o, uq : rsτ d aτ spx, o, uq .“",3.2. Addressing Prediction Issues,[0],[0]
"sτ px, oq ¨ aτ pu|oq.
only a single α-vector to approximate V ρ{Qρ, thus the resulting linear approximator is loose everywhere except in the neighborhood of a few points.",3.2. Addressing Prediction Issues,[0],[0]
"The former approach may require less iterations before convergence to an optimal joint policy, but the computational cost of each iteration shall increase with the number of α-vectors maintained.",3.2. Addressing Prediction Issues,[0],[0]
"The latter approach may require much more iterations, but all iteration shares the same computational cost.",3.2. Addressing Prediction Issues,[0],[0]
"A fundamental theorem in many RL algorithms is the policy improvement theorem, which helps improving policies over time until convergence.",3.3. Addressing Plan Improvement Issues,[0],[0]
"This section introduces a procedure to improve a plan starting with a sub-optimal one.
",3.3. Addressing Plan Improvement Issues,[0],[0]
Suppose we have determined the value function V ρ0:`´1 for any arbitrary ρ .“,3.3. Addressing Plan Improvement Issues,[0],[0]
a0:`´1.,3.3. Addressing Plan Improvement Issues,[0],[0]
"For some control interval t P v0; ` ´ 1w, we would like to know whether or not we should change decision rules a0:t to choose ā0:t ‰ a0:",3.3. Addressing Plan Improvement Issues,[0],[0]
t. We know how good it is to follow the current plan from control interval t onward—that is V ρt —but would it be better or worse to change to the new plan?,3.3. Addressing Plan Improvement Issues,[0],[0]
One way to answer this question is to consider selecting ā0:t at control interval t and thereafter following decision rules at`1:`´1 of the existing ρ.,3.3. Addressing Plan Improvement Issues,[0],[0]
The value of the resulting joint plan is given by Jpā0:t´1q `,3.3. Addressing Plan Improvement Issues,[0],[0]
"λ1V ρt`1pT ps0, ā0:t´1qq.",3.3. Addressing Plan Improvement Issues,[0],[0]
The key criterion is whether this quantity is greater or less than Jpρq.,3.3. Addressing Plan Improvement Issues,[0],[0]
"Next, we state the plan improvement theorem for oMDPs.",3.3. Addressing Plan Improvement Issues,[0],[0]
Theorem 2.,3.3. Addressing Plan Improvement Issues,[0],[0]
Let ρ .“ a0:`´1 and ρ̄ .“,3.3. Addressing Plan Improvement Issues,[0],[0]
"ā0:`´1 be any pair of plans and J0:` be a sequence of α-vectors such that, for all t, Jtpxt, otq
.“",3.3. Addressing Plan Improvement Issues,[0],[0]
Etα0r0 ` . . .,3.3. Addressing Plan Improvement Issues,[0],[0]
"` αtrt|b0, xt, ot, a0:t´1u.",3.3. Addressing Plan Improvement Issues,[0],[0]
Let s̄t .“,3.3. Addressing Plan Improvement Issues,[0],[0]
"T ps0, ā0:t´1q and st .“",3.3. Addressing Plan Improvement Issues,[0],[0]
"T ps0, a0:t´1q be occupancy states at any control interval t P v0; ` ´ 1w under ρ̄ and ρ, respectively.",3.3. Addressing Plan Improvement Issues,[0],[0]
"Then, xā0:t˚´1, at˚:`´1y such that t˚ “ arg maxtPv0;`´1w xs̄t ´ st, Jt ´ λ1V ρ t y is as good as, or better than, ρ.
Proof.",3.3. Addressing Plan Improvement Issues,[0],[0]
"The proof follows from the difference between the performance measure of ρ .“ a0:`´1 and ρ̄
.“",3.3. Addressing Plan Improvement Issues,[0],[0]
ā0:`´1.,3.3. Addressing Plan Improvement Issues,[0],[0]
"Let ςtpρ̄, ρq be the advantage of taking plan xā0:",3.3. Addressing Plan Improvement Issues,[0],[0]
"t´1, at:`´1y instead of ρ: for any control interval t P v0; `´ 1w,
ςtpρ̄, ρq “ Jpā0:t´1q `",3.3. Addressing Plan Improvement Issues,[0],[0]
λ1V ρt pT,3.3. Addressing Plan Improvement Issues,[0],[0]
"ps0, ā0:t´1qq ´ Jpρq “ Jpā0:t´1q ´",3.3. Addressing Plan Improvement Issues,[0],[0]
"Jpa0:t´1q ` λ1pV ρt ps̄tq ´ V ρ t pstqq
“ xs̄t ´ st, Jt ´",3.3. Addressing Plan Improvement Issues,[0],[0]
"λ1V ρt y.
",3.3. Addressing Plan Improvement Issues,[0],[0]
"If we let t˚ .“ arg maxt“0,1,...,`´1 ςtpρ̄, ρq, then plan xā0:t˚´1, at˚:`´1y achieves the highest advantage among plan set txā0:t´1, at:`´1yutPv0;`´1w constructed based on ρ̄.",3.3. Addressing Plan Improvement Issues,[0],[0]
"If t˚ “ 0, then xā0:t˚´1, at˚:`´1y “ ρ, and no improved plans were found from plan set generated from ρ̄.",3.3. Addressing Plan Improvement Issues,[0],[0]
"Otherwise, new xā0:t˚´1, at˚:`´1y must be better than ρ.
",3.3. Addressing Plan Improvement Issues,[0],[0]
"Theorem 2 plays the same role in the plan space as does
the policy improvement theorem in the policy space.",3.3. Addressing Plan Improvement Issues,[0],[0]
"More precisely, after sampling a plan, i.e., a sequence of decision rules, it tells us which of these decision rules will improve the current plan.",3.3. Addressing Plan Improvement Issues,[0],[0]
"More specifically, it shows how, given ρ .“ a0:`´1 and α-vector qρ0:`´1, we can easily evaluate a change in ρ at any control interval to a particular (possibly improved) plan.",3.3. Addressing Plan Improvement Issues,[0],[0]
"To ease exploration towards promising plans, we investigate the -greedy maximization (or softmaximization).",3.3. Addressing Plan Improvement Issues,[0],[0]
"At each control interval t and occupancy state st, it randomly selects ât with probability ; otherwise, it greedily selects ât w.r.t.",3.3. Addressing Plan Improvement Issues,[0],[0]
"the current Q-value function Q ρ t :
ât .",3.3. Addressing Plan Improvement Issues,[0],[0]
"“ arg maxat : a1tPA1t ,...,ant PAnt Q ρ t pst, atq,
where ρ̂ .“",3.3. Addressing Plan Improvement Issues,[0],[0]
â0:`´1.,3.3. Addressing Plan Improvement Issues,[0],[0]
"Unfortunately, this operation proved to be NP-hard for finite M (Radner, 1962; Dibangoye et al., 2009; Kumar & Zilberstein, 2009; Oliehoek et al., 2010).",3.3. Addressing Plan Improvement Issues,[0],[0]
"Searching for the best decision rule requires enumerating all of them, which is not possible in large planning horizons.",3.3. Addressing Plan Improvement Issues,[0],[0]
"Instead, we present a mixed-integer linear programming (MILP) method, which successfully performs the greedy maximization for finite M .",3.3. Addressing Plan Improvement Issues,[0],[0]
"Though MILP is NP-hard in the worst case, the solution of its LP relaxation, which is in P, is often integral in our experiments.",3.3. Addressing Plan Improvement Issues,[0],[0]
"In other words, the solution of the LP relaxation is already a solution of the MILP.",3.3. Addressing Plan Improvement Issues,[0],[0]
A similar observation was done before by MacDermed & Isbell.,3.3. Addressing Plan Improvement Issues,[0],[0]
"Mixed-Integer Linear Program 1 builds on (MacDermed & Isbell, 2013), which introduced an integer program for the greedy maximization in finite M .",3.3. Addressing Plan Improvement Issues,[0],[0]
"We also exploit the occupancy state estimation, in which ŝt replaces st, and the current α-vector q ρ t .",3.3. Addressing Plan Improvement Issues,[0],[0]
"Mixed-Integer Linear Program 1 (For finite M ).
",3.3. Addressing Plan Improvement Issues,[0],[0]
"Maximize at,a1t ,...,a n t
ř
x
ř oŝtpx, oq ÿ
u
atpu|oq ¨ qρt po, uq (8)
s.t.: ř ujatpu j , ui|oq “ aitpui|oiq, @i, ui, o",3.3. Addressing Plan Improvement Issues,[0],[0]
"(9)
ř
uatpu|oq “ 1, @o (10)
where tatpu|oqu and taitpui|oiqu are positive and boolean variables, respectively.
",3.3. Addressing Plan Improvement Issues,[0],[0]
"Mixed-Integer Linear program 1 optimizes positive variables tatpu|oquuPU,oPOt , one positive variable for each control-history pair.",3.3. Addressing Plan Improvement Issues,[0],[0]
"More precisely, each variable represents the probability atpu|oq of control u being taken given that agents experienced joint history o. Constraints must be imposed on these variables to ensure they form proper probability distributions (10), and that they result from the product of independent probability distributions (9), one independent probability distribution for each agent.",3.3. Addressing Plan Improvement Issues,[0],[0]
"In order to make the description of the conditional independence,
atpu|oq “ a1t pu1|o1q ˆ ¨ ¨ ¨ ˆ ant pun|onq, (11)
we use additional variables taitpui|oiquiPv1;nw,uiPUi,oiPOit .",3.3. Addressing Plan Improvement Issues,[0],[0]
"Marginalizing out both sides of (11) over all control-history
pairs of all agents except agent i, denoted ´i, leads to (9).",3.3. Addressing Plan Improvement Issues,[0],[0]
That is not sufficient to ensure conditional independence in general.,3.3. Addressing Plan Improvement Issues,[0],[0]
"If we further constrain taitpui|oiqu to be boolean, then system of equations (9) implies (11).",3.3. Addressing Plan Improvement Issues,[0],[0]
"Given (9) and (10), agent variables taitpui|oiquuiPUi,oiPOit describe a proper probability distribution, so we omit corresponding constraints.",3.3. Addressing Plan Improvement Issues,[0],[0]
"Our greedy maximization approach is fundamentally different from previous ones, including the integer program by (MacDermed & Isbell, 2013) and the constraint optimization program by (Kumar & Zilberstein, 2009; Dibangoye et al., 2016).",3.3. Addressing Plan Improvement Issues,[0],[0]
"First, while previous approaches made use of boolean variables, we use both positive and boolean variables instead.",3.3. Addressing Plan Improvement Issues,[0],[0]
"Next, prior approaches optimize a value function represented as a convex hull; we optimize an α-vector instead.",3.3. Addressing Plan Improvement Issues,[0],[0]
This section presents the oSARSA algorithm with tabular representations and function approximations (using either linear functions or deep neural networks) along with convergence guarantees.,4. The oSARSA Algorithm,[0],[0]
"oSARSA algorithms are specializations of Policy Iteration, except that we use plans instead of policies.",4. The oSARSA Algorithm,[0],[0]
"For the sake of conciseness, we describe a generic algorithm, which can fit to either tabular or approximate representations.
",4. The oSARSA Algorithm,[0],[0]
"In Dec-POMDPs, the goal of oSARSA is to learn q˚0:`´1, a sequence of α-vectors of an optimal plan ρ˚. In particular, we must estimate qtpx, o, uq for the current plan ρ and for all reachable state x, joint history o, control u, and any control interval t.",4. The oSARSA Algorithm,[0],[0]
"At the same time, the algorithm changes ρ towards improved plans according to the plan improvement theorem.",4. The oSARSA Algorithm,[0],[0]
The improved plans are constructed by exploring the occupancy-state space according to -greedy plans (see Section 3.3).,4. The oSARSA Algorithm,[0],[0]
"To provide good estimations, we store all experiences in data set Dρ, from which we estimate the occupancy states and returns under ρ for any control interval (see Section 3.1).",4. The oSARSA Algorithm,[0],[0]
"Upon estimating occupancy state ŝ and selecting joint decision rule a, we update parametrized αvector qt with parameter θt using qt`1, Dρ and at`1 by means of temporal difference learning: for all px, o, uq,
θ rτ`1s t .",4. The oSARSA Algorithm,[0],[0]
“,4. The oSARSA Algorithm,[0],[0]
"θrτst ` βτEŝ,a,D,at`1tδt∇q rτs t px, o, uqu (12)
δt",4. The oSARSA Algorithm,[0],[0]
"“ r ` λ1qrτst`1py, o1, u1q ´ q rτs t px, o, uq,
where βτ is a step size, and quantity ∇qtpx, o, uq denotes the gradient of qt at px, o, uq w.r.t.",4. The oSARSA Algorithm,[0],[0]
some parameter θt.,4. The oSARSA Algorithm,[0],[0]
"Using tabular representations (e.g., finite/small M ), θt “ qt and thus ∇qtpx, o, uq is a unit vector ex,o,u whose value at px, o, uq is one and zero otherwise.",4. The oSARSA Algorithm,[0],[0]
"Using linear function approximations (e.g., continuous/large M ), qtpx, o, uq
.“ φtpx, o, uqJθt, where ∇qtpx, o, uq “ φtpx, o, uq is the feature vector at px, o, uq.",4. The oSARSA Algorithm,[0],[0]
"Algorithm 1 shows the pseudocode of oSARSA.
",4. The oSARSA Algorithm,[0],[0]
"Algorithm 1 The oSARSA Algorithm Initialize ḡ “ ´8, ρ̄ and q0:`´1 arbitrary, and Dρ̄. while q0:`´1 has not converged do
Select -greedily ρ w.r.t.",4. The oSARSA Algorithm,[0],[0]
q0:`´1 and Dρ̄. Compose Dρ with N trajectories tξrτsuNτ“1.,4. The oSARSA Algorithm,[0],[0]
"Estimate pg, ςq from r
ř`´1 t“0 R̂t|Dρ, ŝ0 “ s0s.
",4. The oSARSA Algorithm,[0],[0]
"If g ´ ς ě ḡ then pρ̄, ḡ,Dρ̄q “ pρ, g ` ς,Dρq.",4. The oSARSA Algorithm,[0],[0]
"Update α-vectors q0:`´1 as described in (12).
",4. The oSARSA Algorithm,[0],[0]
"end while
To establish the convergence of oSARSA, we introduce the following assumptions.
",4. The oSARSA Algorithm,[0],[0]
Theorem 3.,4. The oSARSA Algorithm,[0],[0]
"Consider assumptions: (1) The stepsizes tβτuτ“1,2,... satisfy Robbins & Monro’s conditions; (2) The occupancy states ŝ0:`´1 and immediate returns R̂0:`´1 are accurately estimated; and (3) Every pair of reachable occupancy state and joint decision rule is visited infinitely often.",4. The oSARSA Algorithm,[0],[0]
"Under these assumptions, the sequence qrτs0:`´1 generated by oSARSA converges with probability 1 to q˚0:`´1.
",4. The oSARSA Algorithm,[0],[0]
Proof.,4. The oSARSA Algorithm,[0],[0]
"Under these assumptions, we define Hρ that maps a sequence of α-vectors q0:`´1 to a new sequence of α-vectors Hρq0:`´1 according to the formula: for all hidden state x, joint history o and control u, at control interval t,
pHρq0:`´1qpx, o, uq “ rpx, uq ` λ1Etvt`1py, o‘ pu, zqqu,
where vtpx, oq .",4. The oSARSA Algorithm,[0],[0]
"“ qtpx, o, ρpoqq and ρpoq is the control prescribed by ρ at joint history o.",4. The oSARSA Algorithm,[0],[0]
"Then, the plan evaluation step of the oSARSA algorithm is of the form
q rτ`1s t px, o, uq “ p1´ βtqq rτs t px, o, uq ` βtκ rτs t px, o, uq,
κ rτs t",4. The oSARSA Algorithm,[0],[0]
"px, o, uq “ pHρq rτs 0:`´1qpx, o, uq ` wtpx, o, uq,
where wtpx, o, uq “ rpx, uq ` λ1vrτst`1py, o ‘ pu, zqq ´ pHρqrτs0:`´1qpx, o, uq is a zero mean noise term.",4. The oSARSA Algorithm,[0],[0]
"Using this temporal-difference update-rule, see (12), we converge with probability 1 to qρ0:`´1.",4. The oSARSA Algorithm,[0],[0]
It now remains to be verified that the plan improvement step of the oSARSA algorithm changes the current plan for an improved one.,4. The oSARSA Algorithm,[0],[0]
"Initially, ḡ is arbitrarily bad, so any new plan is an improved one.",4. The oSARSA Algorithm,[0],[0]
"Then, ḡ “ Jpρq for the current best plan ρ since occupancy state and return are accurately estimated.",4. The oSARSA Algorithm,[0],[0]
"Hence, when ever g ě ḡ, we know that the new plan ρ̄ yields a performance measure Jpρ̄q superior to Jpρq, thus ρ̄ improves ρ.",4. The oSARSA Algorithm,[0],[0]
"We conclude the proof noticing that in finite M , the number of deterministic plans is finite.",4. The oSARSA Algorithm,[0],[0]
"As a consequence, by visiting infinitely often every pair of occupancy state and decision rule we are guaranteed to visit all deterministic plans, hence an optimal one.
",4. The oSARSA Algorithm,[0],[0]
It is now important to observe that we meet assumption (2) in Theorem 3 only when M is available.,4. The oSARSA Algorithm,[0],[0]
"Otherwise, we rely
on confidence bounds rg ´ ς, g ` ςs, e.g. Hoeffding’s inequality, on estimate g « Jpρq.",4. The oSARSA Algorithm,[0],[0]
"In particular, we use lowerbounds g ´ ς",4. The oSARSA Algorithm,[0],[0]
"on sample means instead of the sample means g themselves, to limit situations where g is overestimated.",4. The oSARSA Algorithm,[0],[0]
"Small data sets often lead to suboptimal solutions, but as the number of experiences in data set Dρ increases, sample means and corresponding lower bounds get close to the mean, i.e., ς tends to 0.",4. The oSARSA Algorithm,[0],[0]
We require an accurate estimation of a plan’s performance to know for sure its performance is above that of any other plans we may encounter.,4. The oSARSA Algorithm,[0],[0]
"However, an estimation of a sample plan’s performance can be refined over time until it becomes accurate.",4. The oSARSA Algorithm,[0],[0]
"For example, if the algorithm samples a promising plan—i.e., its confidence bounds suggest it might achieve a better performance than that of the current plan—the algorithm can progressively refine it until it becomes accurate.",4. The oSARSA Algorithm,[0],[0]
"Of course, having a good initial estimate can significantly speed up the convergence.",4. The oSARSA Algorithm,[0],[0]
"In the extreme case, the initial estimate is the true value.",4. The oSARSA Algorithm,[0],[0]
"It is also worth noticing that the memory complexity of the oSARSA algorithms is linear with the size of an α-vector, i.e., Θp|Dρ|q; and its time complexity is linear with the episodes.",4. The oSARSA Algorithm,[0],[0]
We ran the oSARSA algorithm on a Mac OSX machine with 3.8GHz Core i5 and 8GB of available RAM.,5. Experiments,[0],[0]
We solved the MILPs using ILOG CPLEX Optimization Studio.,5. Experiments,[0],[0]
"We define features to use sequences ofK last joint observations instead of joint histories, hence the dimension of the parameter vector θ is |X|p|U ||Z|qK for finite M .
",5. Experiments,[0],[0]
"We evaluate our algorithm on multiple 2-agent benchmarks from the literature all available at masplan.org: Mabc, Recycling, Gridsmall, Grid3x3corners, Boxpushing, and Tiger.",5. Experiments,[0],[0]
These are the largest and the most challenging benchmarks from the Dec-POMDP literature.,5. Experiments,[0],[0]
"For each of them, we compare our algorithm to the state-of-the-art algorithms based on either a complete or a generative model: FBHSVI (Dibangoye et al., 2016), RLaR (Kraemer & Banerjee, 2016), and MCEM (Wu et al., 2013).",5. Experiments,[0],[0]
"We also reported results of the state-of-the-art model-free solver: (distributed) REINFORCE (Peshkin et al., 2000).",5. Experiments,[0],[0]
"For REINFORCE and oSARSA, we used hyper-parameters and β ranging from 1 to 10´3 with a decaying factor of 104, sample size |D|",5. Experiments,[0],[0]
“ 104.,5. Experiments,[0],[0]
"We use maximum episodes and time limit 105 and 5 hours, respectively, as our stopping criteria.
",5. Experiments,[0],[0]
"Surprisingly, REINFORCE performs very well on domains that consist of weakly coupled agents, see Figure 1.",5. Experiments,[0],[0]
"However, for domains with strongly coupled agents, e.g., Tiger or BoxPushing, it often gets stuck at some local optima.",5. Experiments,[0],[0]
"In contrast, oSARSA converges to near-optimal solutions when enough resources are available over all domains, see Figure 1 and Table 1.",5. Experiments,[0],[0]
"Regarding the most challenging benchmarks, which require more resources, oSARSA stops before
the convergence to a near-optimal solution; yet, it often outperforms the other RL algorithms.",5. Experiments,[0],[0]
"RLaR can achieve near-optimal result for small domains and short planning horizon (` ď 5q, assuming there exists a unique optimal plan.",5. Experiments,[0],[0]
"As for MCEM, it can solve infinite horizon problems, but similarly to REINFORCE may get stuck in local optima; this is essentially as they both use a form of gradient descent in a parametrized policy space.",5. Experiments,[0],[0]
"This paper extends a recent but growing (deep) MARL paradigm (Szer et al., 2005; Dibangoye et al., 2016; Kraemer & Banerjee, 2016; Mordatch & Abbeel, 2017; Foerster et al., 2017), namely RL for decentralized control, from modelbased to model-free settings.",6. Discussion,[0],[0]
"This paradigm allows a centralized algorithm to learn on behalf of all agents how to select an optimal joint decision rule to be executed at each control interval based on all data available about the system during a learning phase, while still preserving ability for each agent to act based solely on its private histories at the execution phase.",6. Discussion,[0],[0]
"In particular, we introduced tabular and approximate oSARSA algorithms, which demonstrated promising results often outperforming state-of-the-art MARL approaches for Dec-POMDPs.",6. Discussion,[0],[0]
"To do so, oSARSA learns a value function that maps pairs of occupancy state and joint decision rule to reals.",6. Discussion,[0],[0]
"To ease the generalization in such high-dimensional continuous spaces, we restrict attention to plans rather than policies, which in turn restricts value functions of interest to linear functions.",6. Discussion,[0],[0]
"To speed up the greedy maximization, we used a MILP for finite settings—we shall use a gradient approach instead of a MILP for continuous settings in future works.",6. Discussion,[0],[0]
"Finally, we present a proof of optimality for a MARL algorithm when the estimation error is neglected.",6. Discussion,[0],[0]
"We shall investigate an approach to relax this somewhat restrictive assumption, perhaps within the probably approximately correct learning framework.
",6. Discussion,[0],[0]
"The RL for decentralized control paradigm is significantly different from the standard RL paradigm, in which agents have the same amount of information during both the learning and the execution phases.",6. Discussion,[0],[0]
Another major difference lies in the fact that learned value functions in standard (deep) RL algorithms are mapping from histories (or states) to reals.,6. Discussion,[0],[0]
"In contrast, oSARSA learns a value function that maps occupancy-state/decision-rule pairs to reals—spaces of occupancy states and joint decision rules are multiple orders of magnitude larger than history or state spaces.",6. Discussion,[0],[0]
"As a consequence, standard (MA)RL methods, e.g. REINFORCE and MCEM, and recent actor-critic methods (Bono et al., 2018), may converge towards a local optimum faster than oSARSA, but the latter often converges towards a nearoptimal solution.",6. Discussion,[0],[0]
oSARSA uses occupancy states instead of joint histories mainly because occupancy states are (so far minimal) sufficient statistics for optimal decision-making in Dec-POMDPs—using joint histories instead of occupancy states may lead to suboptimal solutions except in quite restrictive settings.,6. Discussion,[0],[0]
"For example, RLaR learns value functions mapping history/action pairs to reals, but convergence towards an optimal solution is guaranteed only for domains that admit a unique optimal joint plan—which essentially restricts to POMDPs (Kraemer & Banerjee, 2016).",6. Discussion,[0],[0]
We address a long-standing open problem of reinforcement learning in decentralized partially observable Markov decision processes.,abstractText,[0],[0]
"Previous attempts focussed on different forms of generalized policy iteration, which at best led to local optima.",abstractText,[0],[0]
"In this paper, we restrict attention to plans, which are simpler to store and update than policies.",abstractText,[0],[0]
"We derive, under certain conditions, the first near-optimal cooperative multi-agent reinforcement learning algorithm.",abstractText,[0],[0]
"To achieve significant scalability gains, we replace the greedy maximization by mixed-integer linear programming.",abstractText,[0],[0]
Experiments show our approach can learn to act near-optimally in many finite domains from the literature.,abstractText,[0],[0]
Learning to Act in Decentralized Partially Observable MDPs,title,[0],[0]
"Crowdsourcing has drawn increasing popularity in the field of machine learning by annotating millions of items in a short time with relatively low cost (Howe, 2006; Welinder & Perona; Deng et al., 2013; Jiang et al., 2015).",1. Introduction,[0],[0]
"This provides a great opportunity to build up large-scale training sets for complex models, such as deep neural networks (Krizhevsky et al., 2012), and to reach consensus among non-experts, such as peer grading in today’s popular massive open online course (MOOC) systems.",1. Introduction,[0],[0]
"However, the quality of the collected results is often unreliable and diverse, and there are spammers who give random labels to make easy money, or even adversaries who deliberately give wrong answers.",1. Introduction,[0],[0]
"To address this issue, most crowd-
1The Chinese University of Hong Kong, Hong Kong, China.",1. Introduction,[0],[0]
"2Shenzhen University, China.",1. Introduction,[0],[0]
"3Guangdong Provincial Key Laboratory of Computer Vision and Virtual Reality Technology, Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Shenzhen, China.",1. Introduction,[0],[0]
"Correspondence to: Shengyu Zhang <syzhang@cse.cuhk.edu.hk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
sourcing systems resort to distributing each item to a number of redundant workers.,1. Introduction,[0],[0]
"This raises a challenging question of how to aggregate such noisy and redundant labels.
",1. Introduction,[0],[0]
An intuitive and baseline approach for crowdsourcing is to identify each item following the majority voting (MV) result of workers.,1. Introduction,[0],[0]
"Unfortunately, this approach is errorprone since it treats each worker equally, and the accuracy severely deteriorates with the fraction of less qualified workers, spammers or adversaries.",1. Introduction,[0],[0]
"Weighted majority voting (WMV) (Karger et al., 2011) method tries to address this issue by associating each worker with a weight to characterize his expertise.",1. Introduction,[0],[0]
"Specially, max-margin majority voting (M3V) method (Tian & Zhu, 2015) optimizes the associated variables in WMV by maximizing the minimal difference between the aggregated score of the potential true label and the aggregated scores of others.
",1. Introduction,[0],[0]
"In a different approach, Dawid-Skene (DS) model (Dawid & Skene, 1979) represents each worker’s expertise by a confusion matrix and uses a latent variable model to generate collected labels, which implicitly assumes a worker to perform equally well across all items in a common class.",1. Introduction,[0],[0]
"This model can be iteratively inferred by the famous Expectation-Maximization (EM) method (Dempster et al., 1977), and works well in practice.",1. Introduction,[0],[0]
"In particular, (Zhang et al., 2016) employs the spectral method (Anandkumar et al., 2012) to initialize the DS model, and obtains an optimal convergence rate up to a logarithmic factor by EM method.",1. Introduction,[0],[0]
"Recently, (Zhou et al., 2012; Tian & Zhu, 2015) proposed to improve the aggregating performance by integrating the merits of MV method with DS model.",1. Introduction,[0],[0]
"The performance of DS model and its variants often relies on the specially conceived priors with some manually configured hyperparameters.
",1. Introduction,[0],[0]
All the above mentioned approaches are for aggregating general multiclass labels.,1. Introduction,[0],[0]
"In many practical applications, however, the labels have a natural ordinal structure.",1. Introduction,[0],[0]
"For instance, in MOOCs, students are often required to grade their own assignments on an ordinal scale of 5 (excellent), 4 (good), 3 (fair), 2 (pass) and 1 (failure).",1. Introduction,[0],[0]
"In medical imaging, doctors are often required to mark images on an ordinal scale of stage 1, stage 2, stage 3, and stage 4.",1. Introduction,[0],[0]
Ordinal label faces an issue of diverse standards.,1. Introduction,[0],[0]
"For example, given four assignments whose true grades are 5, 4, 3 and 2, a
strict marker may rate them as 4, 3, 2 and 1.",1. Introduction,[0],[0]
"If we ignore the ordinal structure, we may identify this marker as an adversary because all his answer labels are considered wrong.",1. Introduction,[0],[0]
"But actually this marker grades all assignments in a correct order, which should be incorporated to improve the crowdsourcing performance.
",1. Introduction,[0],[0]
This inspires us to transform the K-class ordinal labeling to K − 1 binary classifications.,1. Introduction,[0],[0]
"That is, instead of directly using a label answer k ∈",1. Introduction,[0],[0]
"[K] = {1, 2, . . .",1. Introduction,[0],[0]
",K}, we use it to answer K − 1 questions “Is the label greater than i” for all",1. Introduction,[0],[0]
i ∈,1. Introduction,[0],[0]
[K − 1].,1. Introduction,[0],[0]
"In this way, the harsh marker’s answer 4 for the first assignment would give three correct answers (on thresholds i = 1, 2, 3) and only one wrong answer (on threshold i = 4), making the marker’s answers highly useful in the aggregation.
",1. Introduction,[0],[0]
"For each binary problem, we can employ a Gibbs sampler to generate label estimations from the generative model of crowdsourcing.",1. Introduction,[0],[0]
"However, these label estimations could be error-prone especially for the difficult items, whose labels may be sampled according to the uniform distribution, and it is well known that the performance of a generative model heavily relies on the specially conceived priors.",1. Introduction,[0],[0]
"To address these issues in binary crowdsourcing tasks, we define a separating width to characterize the quality of label estimations, and solve it by optimizing a linear decision boundary.",1. Introduction,[0],[0]
"The similar idea has been previously explored in (Cortes & Vapnik, 1995) and found a lot of success for supervising learning problems.",1. Introduction,[0],[0]
"By optimizing the separating width among two classes, we can improve the sampling accuracy and update the prior distributions automatically during the learning process.",1. Introduction,[0],[0]
"To characterize the quality of aggregating ordinal labels from K classes, we introduce K − 1 decision boundaries to help optimize the separating width.",1. Introduction,[0],[0]
"As demonstrated empirically, our method achieves the best performance on the real-world datasets compared to other state-of-the-art methods.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
Sec. 2 introduces some preliminary works for crowdsourcing tasks.,1. Introduction,[0],[0]
Sec. 3 presents the generative model employed in this paper.,1. Introduction,[0],[0]
"Sec. 4.1 derives the objective function for binary aggregating problem, which is extended for the ordinal case in Sec. 4.2.",1. Introduction,[0],[0]
The derivations of inference method are discussed in Sec. 5.,1. Introduction,[0],[0]
"Sec. 6 evaluates the performance of our method on some real-world datasets, and Sec. 7 concludes this paper.",1. Introduction,[0],[0]
"In this section, we formalize the problem and survey some preliminary methods.",2. Problem Setting and Preliminary Work,[0],[0]
Suppose that there are M workers and N items taken from a total of K classes.,2. Problem Setting and Preliminary Work,[0],[0]
"For item i, define an M ×K matrix Ri by putting Rijk = 1 if worker
j labels the item as k, and Rijk = 0 otherwise.",2. Problem Setting and Preliminary Work,[0],[0]
Note that Ri is a highly sparse matrix since each item is usually assigned to a small number of workers.,2. Problem Setting and Preliminary Work,[0],[0]
"The objective of a crowdsourcing problem is to identify the true label zi of item i based on the sparse matrices {R1, . . .",2. Problem Setting and Preliminary Work,[0],[0]
", RN}.",2. Problem Setting and Preliminary Work,[0],[0]
"Majority Voting (MV) has been widely used to find the most likely label for item i by solving the following problem,
zi = argk max1 T MR iek, (1)
where 1M is a all-one column vector of dimension M and ek is the k-th standard basis.",2.1. Majority Voting Method and its Variants,[0],[0]
"Weighted majority voting (WMV) (Karger et al., 2011) generalizes MV by assigning weight vector η ∈ RM×1 to the workers and solving the following problem
zi = argk max η TRiek.",2.1. Majority Voting Method and its Variants,[0],[0]
"(2)
Specially, max-margin majority voting (M3V) (Tian & Zhu, 2015) defines the crowdsourcing margin as the minimal difference between the aggregated score of the potential true label and aggregated scores of other labels, and solves η by maximizing the sum of the crowdsourcing margins of all items.",2.1. Majority Voting Method and its Variants,[0],[0]
Dawid-Skene (DS) model has been another popular way to aggregate collected labels by capturing the uncertainties of labeling behaviors in a generative model.,2.2. Dawid-Skene Model and its Variants,[0],[0]
"Compared with WMV and M3V, both of which characterize the expertise of worker j by a scaler variable, DS model characterizes the expertise of worker j with an individual confusion matrix Aj ∈ RK×K , where the (k, d)-th entry denotes the probability that worker j labels a class k sample as class d. Denote A = {Aj}Mj=1.",2.2. Dawid-Skene Model and its Variants,[0],[0]
"DS model aims to maximize the likelihood of observed samplesR = {Ri}Ni=1 as follows,
max A L = N∑ i=1",2.2. Dawid-Skene Model and its Variants,[0],[0]
ln ∫,2.2. Dawid-Skene Model and its Variants,[0],[0]
"p(Ri|zi,A)p(zi)dzi, (3)
where p(Ri|zi,A) = ∏M j=1 ∏K",2.2. Dawid-Skene Model and its Variants,[0],[0]
d=1(A j zid ),2.2. Dawid-Skene Model and its Variants,[0],[0]
"R i jd and zi is a latent variable with p(zi) = 1K ,∀i ∈",2.2. Dawid-Skene Model and its Variants,[0],[0]
[N ].,2.2. Dawid-Skene Model and its Variants,[0],[0]
"This likelihood function can be optimized iteratively by EM method (Dempster et al., 1977) as,
E-step: q(zi = k) ∝",2.2. Dawid-Skene Model and its Variants,[0],[0]
exp M∑ j=1 K∑ d=1 Rijd lnA,2.2. Dawid-Skene Model and its Variants,[0],[0]
j,2.2. Dawid-Skene Model and its Variants,[0],[0]
"kd,
M-step:",2.2. Dawid-Skene Model and its Variants,[0],[0]
Ajkd ∝ N∑ i=1,2.2. Dawid-Skene Model and its Variants,[0],[0]
q(zi,2.2. Dawid-Skene Model and its Variants,[0],[0]
"= k)R i jd.
(4)
",2.2. Dawid-Skene Model and its Variants,[0],[0]
"Thus, the collected labels are aggregated following the rule zi = argk max exp ∑M",2.2. Dawid-Skene Model and its Variants,[0],[0]
j=1 ∑K d=1R,2.2. Dawid-Skene Model and its Variants,[0],[0]
"i jd lnA j kd, where the unknown parameters A can be updated in M-step through maximum likelihood estimation (MLE) principle.",2.2. Dawid-Skene Model and its Variants,[0],[0]
"Recently, spectral methods have been applied to obtain a better initialization of the DS model (Zhang et al., 2016), which achieves an optimal convergence rate up to a logarithmic factor.",2.2. Dawid-Skene Model and its Variants,[0],[0]
"By assuming some special structures of the confusion matrices A, (Raykar et al., 2010) studies homogeneous DS model, and (Moreno et al., 2015) studies the existence of clusters of workers.",2.2. Dawid-Skene Model and its Variants,[0],[0]
Some recent improvements have been achieved by combining MV related methods and DS related methods.,2.3. Recent Achievements,[0],[0]
"(Zhou et al., 2012) assumes labels are generated according to a distribution over workers, items and labels, which can be inferred by minimizing its entropy with constraints developed from MV method and DS model.",2.3. Recent Achievements,[0],[0]
"(Tian & Zhu, 2015) incorporates M3V method with DS model in a regularized Bayesian framework (Zhu et al., 2014), and approximates the posterior distribution over the true labels with a Gibbs sampler.",2.3. Recent Achievements,[0],[0]
"Nowadays, binary and general multi-class crowdsourcing problems have been widely studied in the literature, but the ordinal sibling has not received nearly as much attention yet.",2.3. Recent Achievements,[0],[0]
"The work (Zhou et al., 2014) tries to use the ordinal structure and makes an assumption that workers have difficulty distinguishing between two adjacent ordinal classes whereas it is much easier to distinguish between two far-away classes.",2.3. Recent Achievements,[0],[0]
"In this paper, we will develop a novel objective function to aggregate the ordinal labels, and achieve the best performance on the real-world datasets.",2.3. Recent Achievements,[0],[0]
"In this section, we present a fully Bayesian model to generate observed matrices R. First we note that some items may be intrinsically hard to label even for experts (which is not uncommon in, for example, medical imaging).",3. Generative Model of Crowdsourcing,[0],[0]
"To model such difficulty, we introduce a K-dimensional vector ωi to denote the prior distribution of true label of the item i even for experts.",3. Generative Model of Crowdsourcing,[0],[0]
"(For items clearly from category k, the vector ωi would be just the standard basis ek.)",3. Generative Model of Crowdsourcing,[0],[0]
Denote ω = {ωi}Ni=1.,3. Generative Model of Crowdsourcing,[0],[0]
"We can obtain a joint distribution as follows.
",3. Generative Model of Crowdsourcing,[0],[0]
"p(R,z|A,ω) = ∏",3. Generative Model of Crowdsourcing,[0],[0]
"i,j,d,k (Ajkd) RijdI(zi=k)ωik I(zi=k) , (5)
whereA contains the confusion matrices of all workers like DS model, z is the label vector to be solved and I(·) is an indicator function.",3. Generative Model of Crowdsourcing,[0],[0]
"Since usually most workers just annotate a few items, we may not have sufficient samples to infer z, A and ω.",3. Generative Model of Crowdsourcing,[0],[0]
"To overcome this, we formulate a fully
Bayesian framework overA and ω with prior from Dirichlet distributions (Minka, 2000), a family that has found numerous successful applications (such as topic models) to generate prior distributions.",3. Generative Model of Crowdsourcing,[0],[0]
"We assume that both workers’ expertise A and items’ difficulty ω are random variables from the family of Dirichlet distributions
D(x|α) = Γ(Kα) ΓK(α) K∏ t=1 xα−1t , (6)
where Γ(·) is the gamma function.",3. Generative Model of Crowdsourcing,[0],[0]
"As illustrated in Figure 1, the concentration parameter α controls the sparsity preference of random vector.",3. Generative Model of Crowdsourcing,[0],[0]
"A precisely described item i should have ωi be associated with a small concentration parameter, resulting in a sparse prior vector, while a vaguely described item should be associated with a large concentration parameter.",3. Generative Model of Crowdsourcing,[0],[0]
"To model the expertise Aj of the worker j, we also prefer that it has a small concentration parameter.",3. Generative Model of Crowdsourcing,[0],[0]
"We formulate the prior distributions over A and ω as follows.
p(A|α) = ∏ j,k D(Ajk:|αj), p(ω|β) = ∏",3. Generative Model of Crowdsourcing,[0],[0]
"i D(ωi|βi), (7)
whereα = {αj}Mj=1 and β = {βi}Ni=1.",3. Generative Model of Crowdsourcing,[0],[0]
"Combining Eq. (5) and (7) gives the following joint distribution,
p(R,A, z,ω|α,β) = p(R,z|A,ω)p(ω|β)p(A|α).
",3. Generative Model of Crowdsourcing,[0],[0]
"The graphical model can be found in Fig. 2.
",3. Generative Model of Crowdsourcing,[0],[0]
"Given the matricesR, we can get the posterior distribution overA,z and ω, which can be formulated as
p(A, z,ω|R,α,β) = p(R,A, z,ω|α,β)∫ p(R,A, z,ω|α,β)dAzω .
(8) We can obtain a classifier as argz max p(z|R,α,β) = argz max ∫ p(A, z,ω|R,α,β)dAω to label the item,
parametrized by the hyperparameters α and β.",3. Generative Model of Crowdsourcing,[0],[0]
"Conventionally, researchers mainly focus on how to approximate the posterior distribution with better accuracy and runningtime performance with the fixed prior distributions, or updating the prior distributions by introducing new priors over α and β (Kim & Ghahramani, 2012; Moreno et al., 2015).",3. Generative Model of Crowdsourcing,[0],[0]
"However, the performance of the above generative model heavily relies on the specially conceived priors to incorporate domain knowledge, which transmits affects on the posterior estimations through Bayes’ rules.",3. Generative Model of Crowdsourcing,[0],[0]
"Given a family of prior choices, we prefer the classifier with the more powerful discriminative capability to achieve better generalization performance.",3. Generative Model of Crowdsourcing,[0],[0]
"Before we present the objective function to aggregate ordinal labels, we firstly consider a simple case, the binary crowdsourcing problem with K = 2.",4.1. Binary Crowdsourcing Problem,[0],[0]
As shown in Eq.,4.1. Binary Crowdsourcing Problem,[0],[0]
"(8), by varying the hyperparameters α and β, we can obtain a series of posterior approximations to identify unlabeled items via Bayes rule.",4.1. Binary Crowdsourcing Problem,[0],[0]
"Moreover, by fixing the hyperparameters α and β, we can get multiple estimations of the true label of one item, which are randomly sampled from the posterior distribution over its true label.",4.1. Binary Crowdsourcing Problem,[0],[0]
"Thus, we are motivated to find the most favored set of label estimations over all items.
",4.1. Binary Crowdsourcing Problem,[0],[0]
"For a better generalization performance, we try to maximize the separation width between two classes.",4.1. Binary Crowdsourcing Problem,[0],[0]
"As shown in Fig. 3, the label set 2 is preferred to the set 1, because the set 2 has a larger separation width between two classes.",4.1. Binary Crowdsourcing Problem,[0],[0]
"To evaluate the separating width of samples with the label set z = {zi}Ni=1, with zi ∈ {−1, 1},∀i ∈",4.1. Binary Crowdsourcing Problem,[0],[0]
"[N ], we introduce a linear decision boundary f(Ri) =",4.1. Binary Crowdsourcing Problem,[0],[0]
aTRib with a ∈ RM×1 and b ∈ RK×1.,4.1. Binary Crowdsourcing Problem,[0],[0]
Our decision boundary is formulated refer to the formulas in Eq.,4.1. Binary Crowdsourcing Problem,[0],[0]
"(1) and (2), where a denote the worker expertise and b transforms worker’s label into a scale variable.",4.1. Binary Crowdsourcing Problem,[0],[0]
"Thus, we define an optimization problem as,
min a,b
L(a, b) = ‖a‖22‖b‖22, (9)
s.t. zia TRib ≥ 1, ∀i ∈",4.1. Binary Crowdsourcing Problem,[0],[0]
"[N ],
where ‖x‖22 = xTx and the minimal value L(a∗, b∗) characterizes the separating width of the label set",4.1. Binary Crowdsourcing Problem,[0],[0]
"z. This optimization problem can be understood from the objective function used in support vector machine (SVM) (Cortes & Vapnik, 1995), where the objective function is to maximize the margin width (‖baT ‖F )−1",4.1. Binary Crowdsourcing Problem,[0],[0]
=,4.1. Binary Crowdsourcing Problem,[0],[0]
(‖a‖22‖b‖22)−1 and the constrains state that all samples lie on the correct side of the margin.,4.1. Binary Crowdsourcing Problem,[0],[0]
"(The constraint in the above optimization problem can be viewed as the inner product of Ri and a rank-
1 matrix baT .",4.1. Binary Crowdsourcing Problem,[0],[0]
One may wonder why confining to rank-1 measurements.,4.1. Binary Crowdsourcing Problem,[0],[0]
"Note that MV (Eq.(1)) and WMV (Eq.(2)) are also of the rank-1 form, and our experiments also show that using higher rank measurements actually makes the generalization performance worse; see experiments in Appendix.)",4.1. Binary Crowdsourcing Problem,[0],[0]
"Since z is a random variable generated from the posterior distribution (8), we need to reformulate the objective function (9) as follows,
min a,b,α,β
L(a, b,α,β) =",4.1. Binary Crowdsourcing Problem,[0],[0]
"‖a‖22‖b‖22, (10)
s.t. Ep(zi|Ri,α,β)zia TRib ≥ 1, ∀i ∈",4.1. Binary Crowdsourcing Problem,[0],[0]
"[N ],
p(A, z,ω|R,α,β) ∝",4.1. Binary Crowdsourcing Problem,[0],[0]
"p(R,A, z,ω|α,β).
",4.1. Binary Crowdsourcing Problem,[0],[0]
"Practically, the labeled samples are often linearly inseparable by a single hyperplane; see Set 3 in Fig. 3.",4.1. Binary Crowdsourcing Problem,[0],[0]
"To cope with this issue, we relax the hard constrains by introducing non-negative slack variables ξi, one for each sample, and obtain a “soft” model as follows.
",4.1. Binary Crowdsourcing Problem,[0],[0]
"min a,b,α,β,ξ L(a, b,α,β) =",4.1. Binary Crowdsourcing Problem,[0],[0]
‖a‖22‖b‖22 + λ2 λ1 N∑ i=1,4.1. Binary Crowdsourcing Problem,[0],[0]
"ξi, (11)
s.t. Ep(zi|Ri,α,β)zia",4.1. Binary Crowdsourcing Problem,[0],[0]
"TRib ≥ 1− ξi, ∀i ∈",4.1. Binary Crowdsourcing Problem,[0],[0]
"[N ],
p(A, z,ω|R,α,β) ∝",4.1. Binary Crowdsourcing Problem,[0],[0]
"p(R,A, z,ω|α,β),
where λ2λ1 is used as a positive regularization parameter for later convenience, and 1 − ξi is the soft-margin for item i.",4.1. Binary Crowdsourcing Problem,[0],[0]
"If Ri lies on the correct side of the margin, ξi = 0.",4.1. Binary Crowdsourcing Problem,[0],[0]
"For sample on the wrong side, ξi is proportional to the distance to the margin.",4.1. Binary Crowdsourcing Problem,[0],[0]
"Thus, the value of ξi reflects the difficulty of identifying item",4.1. Binary Crowdsourcing Problem,[0],[0]
"i, or the error allowed to misclassify the item i.",4.1. Binary Crowdsourcing Problem,[0],[0]
"The calculation of p(A, z,ω|R,α,β) is intractable because it involves that of the marginal distribution p(R|α,β).",4.1. Binary Crowdsourcing Problem,[0],[0]
"To address this issue, we introduce a redundant distribution q(A, z,ω) and rewrite the optimization problem as follows.
min a,b,α,β,q,ξ L(a, b, α, β, ξ) =",4.1. Binary Crowdsourcing Problem,[0],[0]
‖a‖22‖b‖22 + λ2 λ1 N∑ i=1,4.1. Binary Crowdsourcing Problem,[0],[0]
"ξi,
(12)
s.t. Eq(zi)zia",4.1. Binary Crowdsourcing Problem,[0],[0]
"TRib ≥ 1− ξi, ∀i ∈",4.1. Binary Crowdsourcing Problem,[0],[0]
"[N ],
KL(q‖p) = 0.
where q and p are shorthand for q(A, z,ω) and p(A,z,ω|R,α,β), respectively, to simplify the presentations in the rest of the paper.",4.1. Binary Crowdsourcing Problem,[0],[0]
"Let ζi = 1−ziaTRib, we can turn the optimization problem into the following one with two regularization terms: mina,b,α,β,q,ξ L(a, b,α,β, q), where L is defined by
L(a, b,α,β, q) = KL(q‖p) +",4.1. Binary Crowdsourcing Problem,[0],[0]
"λ1‖a‖22‖b‖22
+ 2λ2 N∑ i=1",4.1. Binary Crowdsourcing Problem,[0],[0]
"∑ zi∈{−1,1} q(zi)(ζi)+, (13)
where (ζi)+ = max{0, ζi} is the hinge loss function widely used in training classifiers .",4.1. Binary Crowdsourcing Problem,[0],[0]
The factor 2λ2 is introduced here to simplify the derivations of inference methods later.,4.1. Binary Crowdsourcing Problem,[0],[0]
"By optimizing the unknown parameters in the objective function in Eq.(13), we can obtain the estimated labels with the largest separating width.",4.1. Binary Crowdsourcing Problem,[0],[0]
"As introduced in Section 1, transforming a K-class ordinal labeling problem (“what is the label of this item?”) to (K−1) binary classification problems (“Is the label of this item greater than k?” for k ∈",4.2. Ordinal Crowdsourcing Problem,[0],[0]
[K − 1]) allows us to exploit more useful information from workers.,4.2. Ordinal Crowdsourcing Problem,[0],[0]
"The transform is illustrated in Set 4 of Fig. 3, where we have items coming fromK ordered classes, C1, . . .",4.2. Ordinal Crowdsourcing Problem,[0],[0]
", CK .",4.2. Ordinal Crowdsourcing Problem,[0],[0]
"We look forK−1 decision boundaries, with boundary t discriminating classes C1 ∪ · · · ∪ Ct and classes Ct+1 ∪ · · · ∪ CK .",4.2. Ordinal Crowdsourcing Problem,[0],[0]
"For the t-th binary question, we introduce a linear decision boundary as ft(Ri) = aTt R
ibt.",4.2. Ordinal Crowdsourcing Problem,[0],[0]
It is easily verified that all boundaries intersecting at the zero point.,4.2. Ordinal Crowdsourcing Problem,[0],[0]
"With a = {at}K−1t=1 and b = {bt}K−1t=1 , the loss function in Eq.",4.2. Ordinal Crowdsourcing Problem,[0],[0]
"(13) becomes
L(a, b,α,β, q) =KL(q‖p)",4.2. Ordinal Crowdsourcing Problem,[0],[0]
"+ λ1 K−1∑ t=1 ‖at‖22‖bt‖22
+ 2λ2",4.2. Ordinal Crowdsourcing Problem,[0],[0]
N∑ i=1,4.2. Ordinal Crowdsourcing Problem,[0],[0]
K∑ zi=1 q(zi) K−1∑ t=1,4.2. Ordinal Crowdsourcing Problem,[0],[0]
"(ζit)+, (14)
where ζit = 1 − sgnt(zi)aTt",4.2. Ordinal Crowdsourcing Problem,[0],[0]
Ribt with sgnt(zi) = −1,4.2. Ordinal Crowdsourcing Problem,[0],[0]
if zi ≤ t and sgnt(zi) = 1,4.2. Ordinal Crowdsourcing Problem,[0],[0]
if zi > t. It is obvious that our ordinal model will degenerate into binary one when K = 2.,4.2. Ordinal Crowdsourcing Problem,[0],[0]
WhenK,4.2. Ordinal Crowdsourcing Problem,[0],[0]
"≥ 3, the ordinal label should be estimated by considering the predicted results from K − 1 binary problems.",4.2. Ordinal Crowdsourcing Problem,[0],[0]
"In this section, we present the implementation details to infer the true labels and all other unknown parameters involved in ordinal crowdsourcing problems.",5. Inference Details,[0],[0]
Our inference method consists of two parts.,5. Inference Details,[0],[0]
"In the first part, we employ
a Gibbs sampler to approximately sample from the posterior distribution p = p(A,z,ω|R,α,β).",5. Inference Details,[0],[0]
"In the second part, we update the hyperparameters (α,β) and linear decision boundaries based on the gradient method to achieve the largest separating width.
",5. Inference Details,[0],[0]
"To approximate the intractable posterior distribution p, there are two standard approaches, which are Variational Bayesian (VB) and Gibbs sampling.",5. Inference Details,[0],[0]
"Compared with the Gibbs sampling method, VB is usually difficult in its functional optimization, especially hard in our case due to the hinge loss function.",5. Inference Details,[0],[0]
"Moreover, VB often suffers from inaccuracy because of the potentially impractical assumption of independence of variables.",5. Inference Details,[0],[0]
"Gibbs sampling is applicable here, because it provides numerical approximations to the integration problems in large dimensional spaces by generating an instance from the conditional distribution of each variable in turn.",5. Inference Details,[0],[0]
"It can be shown that the sequence of samples constitutes a Markov chain, which finally converges to the targeted posterior distribution as the stationary distribution.
",5. Inference Details,[0],[0]
"Since the sampling process of the confusion matrices A and the items’ difficulties ω can be developed in the standard way, we leave their derivations in Appendix and mainly discuss the sampling process of true labels z here.",5. Inference Details,[0],[0]
The difficulty of sampling z is mainly due to the hinge loss function (ζit)+.,5. Inference Details,[0],[0]
"We employ data augmented technique (Polson & Scott, 2011) to approximate the hinge loss function.",5. Inference Details,[0],[0]
"According to the equality (Andrews & Mallows, 1974),
exp(−2λ2(ζit)+) = ∫ φ(zi, γit|Ri)dγit, (15)
with φ(zi, γit|Ri) = (2πγit)− 1 2 exp( −12γit (γit + λ2ζit) 2) and γit as a non-negative augmented variable, we can reformulate the objective function in Eq.(14) as follows.
",5. Inference Details,[0],[0]
"L(a, b,α,β, q) ≤ KL(q‖p)",5. Inference Details,[0],[0]
"+ λ1 ∑ t aTt atb T t bt (16)
+ ∑ i,zi,t ∫ q(zi)q(γit) ln
q(γit)
φ(zi, γit|Ri) dγit,
where the inequality comes from Jensen’s inequality with a new distribution q(γit) to help to approximate the hinge loss function exp(−2λ2(ζit)+).",5. Inference Details,[0],[0]
"Note that the right hand side of the inequality is tractable, minimizing which would give an upper bound of the original optimization problem.",5. Inference Details,[0],[0]
"Before we sample the true labels of items, we need to first generate augmented variables γ = {γit}N,K−1i=1,t=1.",5. Inference Details,[0],[0]
"When fixing other random variables, we can generate the (i, t)-th augmented parameter according to the following generalized inverse Gaussian (GIG) distribution,
γit ∼ 1 Z γ",5. Inference Details,[0],[0]
"− 12 it exp[− 1 2 (γit +
λ22ζ 2 it
γit )], (17)
where Z is the normalization term.",5. Inference Details,[0],[0]
"It has been shown that 1 γit
can be drawn efficiently with O(1) time complexity (Michael et al., 1976).
",5. Inference Details,[0],[0]
"Here, we can sample the true labels of all items.",5. Inference Details,[0],[0]
"Let φ(z, γ|R) = ∏N i=1",5. Inference Details,[0],[0]
"∏K−1 t=1 φ(zi, γit|Ri).",5. Inference Details,[0],[0]
Rewrite the objective function shown in Eq.,5. Inference Details,[0],[0]
"(16) with respect to q(zi) as follows,
L(q(zi)) ≤ KL(q‖p)",5. Inference Details,[0],[0]
+ ∑,5. Inference Details,[0],[0]
"i,zi,t ∫ q(zi)q(γit) ln
q(γit)
φ(zi, γit|Ri) dγit
= KL(q · q(γ)‖p · φ(z, γ)).",5. Inference Details,[0],[0]
"(18)
Thus, with all other parameters fixed, we can sample zi ∈",5. Inference Details,[0],[0]
"[K] according to the following distribution,
q(zi) ∝",5. Inference Details,[0],[0]
"p(Ri,A, zi, ωi|α,β)",5. Inference Details,[0],[0]
"K−1∏ t=1 φ(zi, γit|Rit).",5. Inference Details,[0],[0]
"(19)
Let us examine the two terms on the right hand side.",5. Inference Details,[0],[0]
"The first term comes from the generative model of crowdsourcing, while the second term maximizes the separating width of the estimated ordinal labels.",5. Inference Details,[0],[0]
"For the binary crowdsourcing problem, we have only one decision boundary to measure the separating width, while for the ordinal crowdsourcing problem with K classes, we get K − 1 intersected decision boundaries to measure the separating width.",5. Inference Details,[0],[0]
"After obtaining a set of random samples to approximate the joint posterior distribution q over all model parameters and augmented variables, the objective function shown in Eq.",5. Inference Details,[0],[0]
"(14) becomes a parametric function with respect to a, b,α and β.",5. Inference Details,[0],[0]
"Thus, we can intuitively update these parameters based on the gradient method.",5. Inference Details,[0],[0]
"The derivations of the updating formulas over a, b,α and β can be found in Appendix.
",5. Inference Details,[0],[0]
Let 〈f(x)〉 = ∫ q(x)f(x)dx denote the expectation of f(x) with respect to the distribution of q(x).,5. Inference Details,[0],[0]
"Our method is outlined in Algorithm 1, in which each while iteration consists of two for loops, and the source code with demo can be found on the website1.",5. Inference Details,[0],[0]
"In the first for loop, we employ
1http://appsrv.cse.cuhk.edu.hk/˜gychen/
Algorithm 1",5. Inference Details,[0],[0]
"Our Ordinal Crowdsourcing Method 1: Input: R = {Ri}Ni=1, λ1, λ2 and the learning rates η. 2",5. Inference Details,[0],[0]
": Initializing z = {zi}Ni=1 by MV, a, b, α and β 3: while not convergence do 4: for i = 1 : N do 5:",5. Inference Details,[0],[0]
Ajkd ∼ D(A j kd|αj + ∑N,5. Inference Details,[0],[0]
"i=1R i jdI(zi = k))
6: ωik ∼ D(ωik|βi + I(zi = k)) 7: γit ∼ 1Z γ",5. Inference Details,[0],[0]
"− 12 it exp[− 12 (γit + λ22ζ 2 it γit )]
8: zi ∼ p(Ri,A, zi, ωi|α,β) ∏K−1 t=1 φ(zi, γit|Rit)
9: end for 10: for t = 1 : K − 1 do 11: Σat = 2λ1‖bt‖22I + ∑N i=1",5. Inference Details,[0],[0]
"λ22 〈γit〉R ibtb T t R iT
12: at = Σ−1at ( ∑N i=1(λ2 + λ22 〈γit〉 )〈sgnt(zi)〉R ibt)
13: Σbt = 2λ1‖at‖22I + ∑N i=1",5. Inference Details,[0],[0]
λ22 〈γit〉a T t R iRi T at 14: bt = Σ −1 bt,5. Inference Details,[0],[0]
"( ∑N i=1(λ2 + λ22 〈γit〉 )〈sgnt(zi)〉a T t R i) 15: end for 16: αj ← αj − η ∂L(αj)∂αj ,∀j ∈",5. Inference Details,[0],[0]
[M ] 17: βi ← βi,5. Inference Details,[0],[0]
"− η ∂L(βi)∂βi ,∀i ∈",5. Inference Details,[0],[0]
"[N ] 18: end while
a Gibbs sampler to generate the random variables to approximate the posterior distribution.",5. Inference Details,[0],[0]
"In the second part, we solve the separating width by optimizing K − 1 decision boundaries, and update the prior distributions by gradient method.",5. Inference Details,[0],[0]
"Compared with the traditional generative model of crowdsourcing, including DS model and its variants, our method introduces an augment variable γit to approximate the hinge loss function, which is further involved in the generation of true labels.",5. Inference Details,[0],[0]
This algorithm is iteratively implemented to reach a local optimum.,5. Inference Details,[0],[0]
"To fully evaluate the ideas employed in this paper, we present empirical studies of our aggregating method in comparison with competitive ones not only on ordinal crowdsourcing tasks, but also binary crowdsourcing tasks.",6. Experiments and Discussions,[0],[0]
"For our method, we configure λ1 = λ2 = 1, α = 1M ,β = 1N , η = 1 × 10−5 and initialize zi by the majority voting result.",6. Experiments and Discussions,[0],[0]
"In each run of our method, we generate 80 samples to approximate the posterior distribution and discard the first 10 samples as burn-in steps.",6. Experiments and Discussions,[0],[0]
"The reported error rate of our method is averaged over 10 runs, and all experiments are conducted in a PC with Intel Core i7 1.8GHz CPU and 8.00GB RAM.",6. Experiments and Discussions,[0],[0]
"We first evaluate our method on three binary benchmark datasets shown in Table 1, include labeling bird species
(Welinder et al., 2010)",6.1. Binary Crowdsourcing Tasks,[0],[0]
"(Bird dataset), recognizing textual entailment (Snow et al., 2008) (RTE dataset) and accessing the relevance of topic-document pairs with a binary judgment in TREC 2011 crowdsourcing track (Gabriella & Matthew, 2011) (TREC dataset).",6.1. Binary Crowdsourcing Tasks,[0],[0]
"The competitive methods include the pure majority voting estimator (refereed to as MV), the EM method for DS model initialized by majority voting (refereed to as MV-DS), the EM method for DS model initialized by spectral method (refereed to as Opt-DS) (Zhang et al., 2016), the Gibbs sampler for the Bayesian extension of M3V (Tian & Zhu, 2015) (referred to as G-CrowdSVM), the SVD-based algorithm proposed in (Ghosh et al., 2011) (referred to as Gh-SVD), and the Eigenvalues of Ratio algorithm proposed in (Dalvi et al., 2013) (referred to as Eig-Ratio).",6.1. Binary Crowdsourcing Tasks,[0],[0]
The performance of all methods are evaluated by error as l0 = 1 − 1|z|‖z,6.1. Binary Crowdsourcing Tasks,[0],[0]
"− ẑ‖0, where z contains true labels of items (available in all these datasets)",6.1. Binary Crowdsourcing Tasks,[0],[0]
and ẑ contains estimations given by our algorithm (not using any information of z).,6.1. Binary Crowdsourcing Tasks,[0],[0]
"Noted that the reported error rates of G-CrowdSVM are the average over 10 random runs as our method.
",6.1. Binary Crowdsourcing Tasks,[0],[0]
"As shown in Table 2, our method achieves the best performance among all methods on three benchmark datasets.",6.1. Binary Crowdsourcing Tasks,[0],[0]
"Without regards to the prior knowledges over workers’ expertise and items’ difficulties, we can degenerate our model into MV-DS model by setting λ2 = λ1 = 0.",6.1. Binary Crowdsourcing Tasks,[0],[0]
"Comparing with the performance of MV-DS model, especially Opt-DS method, we present a more complicated generative model, leading to better predictive results.",6.1. Binary Crowdsourcing Tasks,[0],[0]
"Compared with G-CrowdSVM method, our method updates prior distributions and improves the sampling accuracy by optimizing the separating width during the learning process, which leads to the improvements of predictive performance on all datasets.",6.1. Binary Crowdsourcing Tasks,[0],[0]
"In this part, we report empirical results of our method on ordinal benchmark datasets in comparison with competitive ones.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"We consider MV, MV-DS, and G-CrowdSVM as baselines, and compare our method with Entropy(O) (Zhou et al., 2014), which consider the ordinal structures in labels.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"As shown in Table 1, we have two ordinal datasets.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"One is to judge the relevance of query-URL pairs with a 5-level rating score (Web dataset), and the other is to identify the age of each subject with a 7-level rating score (Age dataset).",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"To evaluate the performance of aggregating ordinal labels, we define three following error measurements as: l0 = 1 − 1|z|‖z",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"− ẑ‖0, l1 = 1 |z|‖z",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
− ẑ‖1 and l2 = 1 |z|‖z − ẑ‖2.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Compared with the error rate l0, the measures l1 and l2 take precision into consideration, and may be preferred for aggregating ordinal labels when one cares about the severity of error.
",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Table 3 summarizes the performance of all methods on the ordinal datasets, and shows that our method consistently outperforms the others in predicting the ordinal labels of items.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Similar with our method, G-CrowdSVM attempts to maximize the margin between the aggregated score of potential true label and the aggregated score of others, and achieves the better performance in comparison with the state-of-the-art method to aggregate ordinal labels, Entropy(O).",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Compared with G-CrowdSVM, we treat the problem of aggregating collected labels as the classification problem, and introduce K − 1 decision boundaries to consider the ordinal relationship among categories.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"As shown in Table 3, on the Web dataset, our method significantly reduces the average l0 error rate from 7.99% to 3.22%.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"In addition, the average l1 error of our method is 3.69%, which is only slightly larger than the l0 error rate of 3.22%.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"It means that, even for the incorrect labels ẑi outputted by our algorithm, our ẑi is not far away from its true answer zi, resulting in a relatively small damage.
",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
The Web dataset has been widely used in the evaluation of ordinal crowdsourcing problem.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"On this dataset, our method introduces 4 decision boundaries to measure the separating width of generated true labels.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"To help to understand the linear decision boundaries learned by our method, we illustrate the average value over 80 samples of {bt}4t=1 as Fig. 4.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
It can be seen that the absolute value of all entries in bt is approximated to 1.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"To be more concrete, let us consider the first decision boundary with b1, which calculate Rib1 for the item i. Thus, Rib1 successfully reduces the ordinal problem into a binary one, the j-th entry inRibi would be −1 if worker j ranks item",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"i as 1 and 1 if worker j rank item i as 2, 3, 4 and 5.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
Note that at characterizes the expertise of all workers for the t-th binary problem.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Fig. 5 contains three confusion matrices, including the averaged confusion matrix of all workers, the confusion matrices of
Table 2.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"l0 error rate (%) in predicting the latent labels on three binary benchmark datasets.
",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Binary Dataset Ours G-CrowdSVM Opt-DS MV-DS MV Gh-SVD Eig-Ratio
Bird 9.25±0.17",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"10.37±0.41 10.09 11.11 24.07 27.78 27.78 RTE 7.00±0.29 7.72±0.22 7.12 7.12 10.31 49.13 9.00 TREC 29.30±0.11 31.32±0.34 29.80 30.02 34.86 42.99 43.96
Table 3.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"Errors in predicting the latent labels on two ordinal benchmark datasets.
",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
an expert and a spammer.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"It can be found that the spammer ranks all items randomly to make easy money, while the expert has a confusion matrix similar to the identical matrix.",6.2. Ordinal Crowdsourcing Tasks,[0],[0]
Our method can accurately estimate the confusion matrices of all workers even given the averaged confusion matrix acts like a spammer.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
Fig. 6 summarizes the training time and error rates after each iteration for all estimators on the Web dataset.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
It can be found that the proposed method coverages to a lower error rate and all other three methods have error convergence curves all above ours.,6.2. Ordinal Crowdsourcing Tasks,[0],[0]
"In this paper, we develop a novel method to aggregate ordinal labels by optimizing the separating width among classes.",7. Conclusions,[0],[0]
"To measure the separating width among ordinal labels, we first investigate a binary case, and then extend our achievements to the ordinal one.",7. Conclusions,[0],[0]
"With K − 1 decision boundaries, we define an optimization problem for measuring the separating width among ordinal classes.",7. Conclusions,[0],[0]
"The newly introduced boundaries not only help to optimize the hyperparameters, but also calibrate the estimated labels sampled from the generative model.",7. Conclusions,[0],[0]
"A Gibbs sampler is adopted to approximate the posterior distribution, while the gradient method is used to calculate the separating width and optimize the hyperparameters.
",7. Conclusions,[0],[0]
"As demonstrated on the ordinal datasets, which is the main focus of this paper, our method consistently achieves the best performance compared with competitive ones, and the improvements on Web dataset are significant.",7. Conclusions,[0],[0]
"As demonstrated by the experimental results on the binary datasets, our algorithm works slightly better than any previous method.",7. Conclusions,[0],[0]
"Thus, our algorithm provides a uniform method in both binary and ordinal cases, and can be practically useful for real-world applications.",7. Conclusions,[0],[0]
We would like to thank anonymous reviewers for their valuable comments to improve the presentation of this paper.,Acknowledgements,[0],[0]
This work was supported by the China 973 Program (Project No. 2015CB351706) and a grant from the National Natural Science Foundation of China (Project No. 61233012).,Acknowledgements,[0],[0]
Shengyu Zhang was supported by Research Grants Council of the Hong Kong S.A.R. (Project no.,Acknowledgements,[0],[0]
CUHK14239416).,Acknowledgements,[0],[0]
"While crowdsourcing has been a cost and time efficient method to label massive samples, one critical issue is quality control, for which the key challenge is to infer the ground truth from noisy or even adversarial data by various users.",abstractText,[0],[0]
"A large class of crowdsourcing problems, such as those involving age, grade, level, or stage, have an ordinal structure in their labels.",abstractText,[0],[0]
"Based on a technique of sampling estimated label from the posterior distribution, we define a novel separating width among the labeled observations to characterize the quality of sampled labels, and develop an efficient algorithm to optimize it through solving multiple linear decision boundaries and adjusting prior distributions.",abstractText,[0],[0]
"Our algorithm is empirically evaluated on several real world datasets, and demonstrates its supremacy over state-ofthe-art methods.",abstractText,[0],[0]
Learning to Aggregate Ordinal Labels by Maximizing Separating Width,title,[0],[0]
"The problem of aligning sequences is well studied in the literature across many domains, e.g., machine translation (Brown et al., 1993; Dyer et al., 2013; Bahdanau et al., 2014), speech recognition (Graves et al., 2006; 2013), handwriting recognition (Graves et al., 2009; Graves & Schmidhuber, 2009), alignment of books with movies made based on them (Zhu et al., 2015) and more.",1. Introduction,[0],[0]
"The alignment is often done sequentially, one step at a time.",1. Introduction,[0],[0]
"We propose a neural network architecture, capable of aligning two input sequences globally and at once.
",1. Introduction,[0],[0]
We focus on the alignment of source code and its translation to the compiled object code.,1. Introduction,[0],[0]
"During compilation, source code typically written in a human-readable high level programming language, such as C, C++ and Java, is transformed by the compiler to object code.",1. Introduction,[0],[0]
"Every object code statement stems from a specific location in the source code, and, therefore, there is a statement-level alignment
1The School of Computer Science, Tel Aviv University 2Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Dor Levy <dor.levy@cs.tau.ac.il>, Lior Wolf <wolf@cs.tau.ac.il>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
between source code and object code.
",1. Introduction,[0],[0]
"As far as we know, statement-by-statement alignment of source- and object-code is not treated in the literature.",1. Introduction,[0],[0]
"It is challenging, since the per-statement outcome of the compilation process also depends on other statements of the source code.",1. Introduction,[0],[0]
"In addition, this outcome is produced in increasing levels of sophistication that are determined by the compiler’s optimization flags.
",1. Introduction,[0],[0]
"Our compound deep neural network combines one embedding and one RNN per input sequence, a CNN applied to a grid of sequence representation pairs and multiple softmax layers.",1. Introduction,[0],[0]
Training is performed using both real-world and synthetic data that we created for this purpose.,1. Introduction,[0],[0]
"The real-world data consists of 53,000 short functions from 90 open-source projects of the GNU project.",1. Introduction,[0],[0]
"Three levels of compiler optimization are used and the ground truth alignment labels are extracted from the compiler’s output.
",1. Introduction,[0],[0]
Our experiments1 show that the neural network presented is able to predict the alignment considerably more accurately than the literature baselines.,1. Introduction,[0],[0]
"Moreover, our method is general and transcends the problem of aligning sequences.",1. Introduction,[0],[0]
We demonstrate it by using exactly the same architecture for learning the Traveling Salesman Problem.,1. Introduction,[0],[0]
"We propose a novel network architecture and challenge it with a difficult alignment problem, which has unique characteristics: the input sequences’ representations are not per token, but per statement (a subsequence of tokens).",1.1. Our Contributions,[0],[0]
"The alignment is predicted by our architecture not sequentially (e.g., by employing attention), but by considering the entire grid of potential alignments at once.",1.1. Our Contributions,[0],[0]
"This is done using an architecture that combines a top-level CNN with LSTMs (Hochreiter & Schmidhuber, 1997).
",1.1. Our Contributions,[0],[0]
"While neural networks have been shown to be capable of aligning sequences in the domain of NLP, where a sentence in one natural (human) language is aligned with its translation (Bahdanau et al., 2014), the current domain has additional challenges.",1.1. Our Contributions,[0],[0]
"First, each source or object-code statement contains both an operation (a reserved C key-
1Our code and data are publicly available at: https:// github.com/DorLevyML/learn-align
word or an opcode) and potentially multiple parameters, and are, therefore, typically more complex than natural language words.",1.1. Our Contributions,[0],[0]
"Second, highly optimized compilation means that the alignment is highly non-monotonous.",1.1. Our Contributions,[0],[0]
"Third, the alignment is very often partial, since not all source-code statements are aligned with the object-code statements.",1.1. Our Contributions,[0],[0]
"Finally, the meaning of each code statement is completely context dependent, since, for example, the variables are reused within multiple statements.",1.1. Our Contributions,[0],[0]
"In natural languages, the context helps to resolve ambiguities.",1.1. Our Contributions,[0],[0]
"However, a direct dictionary based alignment already provides a moderately accurate result.",1.1. Our Contributions,[0],[0]
"In the current domain, the alignment process has to depend entirely on the context.",1.1. Our Contributions,[0],[0]
"Extensive work was done on the problem of predicting the alignment and computing its probability given a pair consisting of a source sentence and a candidate target sentence (Brown et al., 1993; Dyer et al., 2013).",1.2. Related Work,[0],[0]
"The alignment probability is then used for re-ranking the translation candidates in the translation pipeline.
",1.2. Related Work,[0],[0]
Bahdanau et al. (2014) propose an architecture for jointly aligning and translating between two languages.,1.2. Related Work,[0],[0]
The encoder of the source language is based on a bidirectional RNN.,1.2. Related Work,[0],[0]
"During the decoding process, in which the new sentence in the target language is created, an RNN is used to predict one word at a time.",1.2. Related Work,[0],[0]
"This RNN pools as one of its inputs, a weighted combination of the representations of the various words in the source language.",1.2. Related Work,[0],[0]
The weights of this combination are pseudo-probabilities that represent the similarity of the predicted word in the translated sentence to each of the words in the source sentence.,1.2. Related Work,[0],[0]
"In contrast to our work, the model described in (Bahdanau et al., 2014) implicitly aligns an input sequence to an output sequence, as part of the translation process, while our model explicitly aligns two input sequences.",1.2. Related Work,[0],[0]
Note that most human languages are relatively similar and are constructed by similar rules.,1.2. Related Work,[0],[0]
"It is unlikely that the same translation architectures could successfully and accurately translate, for example, C code to object code.
",1.2. Related Work,[0],[0]
"Our work is close in concept to Pointer Networks (Vinyals et al., 2015), where the proposed architecture outputs discrete tokens corresponding to positions in the input sequence.",1.2. Related Work,[0],[0]
The input sequence is first encoded by an LSTM to a representation sequence.,1.2. Related Work,[0],[0]
"A second LSTM, at each time step, then points to a location in the input sequence through an attention mechanism and given the previously pointed value of the input sequence.",1.2. Related Work,[0],[0]
"Similarly, our architecture also points to locations in an input sequence.",1.2. Related Work,[0],[0]
"However, in contrast to Vinyals et al. (2015), our architecture receives two input sequences and points to locations on a grid formed by the two.
",1.2. Related Work,[0],[0]
"The approach that is most closely related to ours is MatchLSTM (Wang & Jiang, 2015).",1.2. Related Work,[0],[0]
"This architecture is used to determine, given a premise sentence and a hypothesis sentence, whether the hypothesis can be inferred from the premise.",1.2. Related Work,[0],[0]
The Match-LSTM is designed to do so by matching of the hypothesis and premise word-by-word.,1.2. Related Work,[0],[0]
"First, the two sentences are processed using two LSTM networks.",1.2. Related Work,[0],[0]
A third LSTM then processes sequentially the hypothesis representation sequence and for every word in the hypothesis sentence produces a match score for all words in the premise representation sequence using an attention mechanism.,1.2. Related Work,[0],[0]
"Finally, after the third LSTM is done processing the hypothesis sentence, its last hidden state is used to produce a single prediction for the relation between the hypothesis and the premise.",1.2. Related Work,[0],[0]
"Although our work is aimed to align two sequences, our proposed architecture is far from Match-LSTM.",1.2. Related Work,[0],[0]
"While Match-LSTM matches sequentially every word in the hypothesis sentence to all words in the premise, our architecture represents all the statement pairs as a grid and aligns all of them globally and combined, using a CNN.",1.2. Related Work,[0],[0]
"Another difference is that the alignment produced by Match-LSTM is only implicit, since the goal of the architecture is to predict the relation between the two sequences.",1.2. Related Work,[0],[0]
"In our architecture, the alignment is explicit and fully supervised during training.
",1.2. Related Work,[0],[0]
"Another aspect in which our architecture differs from the ones proposed by Bahdanau et al. (2014); Vinyals et al. (2015); Wang & Jiang (2015), is that in order to align statements, it does not learn representations that correspond to tokens in the input sequences, but learns representations that correspond to segments in the input sequence – each segment being a mini-sequence of tokens that corresponds to one statement.
",1.2. Related Work,[0],[0]
Sequence processing with CNNs The use of CNNs for sequence processing tasks has been expanding recently.,1.2. Related Work,[0],[0]
"Such tasks include sequence encoding (Zhang et al., 2015; Lee et al., 2016), sentiment prediction (Blunsom et al., 2014), document summarization (Denil et al., 2014) and translation (Gehring et al., 2017).",1.2. Related Work,[0],[0]
"One reason is the computational efficiency of CNNs compared to RNNs, which leads to faster computations both on GPU and CPU.",1.2. Related Work,[0],[0]
"Another reason is their ability to capture translation invariant features in text, as shown by, e.g., Allamanis et al. (2016), who use a convolutional attention mechanism in order to generate extreme summarization of source code functions.
",1.2. Related Work,[0],[0]
Neural networks and source code tasks Neural networks have been shown to be useful in tasks involving source code.,1.2. Related Work,[0],[0]
"For example, In (Zaremba & Sutskever, 2014) a sequence-to-sequence LSTM learns to execute simple class of python programs only from seeing input-output pairs.",1.2. Related Work,[0],[0]
"In Allamanis et al. (2016), a model learns to generate meaningful summaries to short functions written in Java.",1.2. Related Work,[0],[0]
"We consider source code written in the C programming language, in which statements are generally separated by a semicolon (;).",2. The Code Alignment Problem,[0],[0]
The compiler translates the source code to object code.,2. The Code Alignment Problem,[0],[0]
"For example, the GCC compiler (Stallman et al., 2009) is used.",2. The Code Alignment Problem,[0],[0]
"We view the object code as assembly, where each statement contains an opcode and its operands.",2. The Code Alignment Problem,[0],[0]
"Since the source code is translated to object code during compilation, there is a well-defined alignment between them, which is known to the compiler.",2. The Code Alignment Problem,[0],[0]
GCC outputs this information when it runs with a debug configuration.,2. The Code Alignment Problem,[0],[0]
"In the GCC alignment output, the statement level alignment between source- and object-code is a many-to-one map from object code statements to source code statements: while every object-code statement is aligned to some source-code statement, not all source-code statements are covered.",2.1. Problem Formalization,[0],[0]
This is due to optimization performed by the compiler.,2.1. Problem Formalization,[0],[0]
"Our definition of a statement is slightly modified, due to the convention used in the GCC alignment output.",2.1. Problem Formalization,[0],[0]
"A C statement can be one of the following: (i) a simple statement in C containing one command ending with a semicolon; (ii) curly parentheses ({,}); (iii) the signature of a function; (iv) one of if(EXP1),
for(EXP1;EXP2;EXP3), or while(EXP1), including the corresponding expressions; (v) else or do.",2.1. Problem Formalization,[0],[0]
"Note that the following code
1 do 2 { 3 a += 4 ; 4 } 5 w h",2.1. Problem Formalization,[0],[0]
i l e,2.1. Problem Formalization,[0],[0]
"( i < 5 0 0 ) ;
contains five statements (as numbered) since the do, the {, the }, and the while are all separate statements.
",2.1. Problem Formalization,[0],[0]
"The object code statements follow the conventional definition, as shown, for example, in assembly code listings.",2.1. Problem Formalization,[0],[0]
"Each statement contains a single opcode such as mov, jne, or pop, and its operands.
",2.1. Problem Formalization,[0],[0]
"An example is shown in Fig. 1, which depicts both the source code of a single C language function, which contains M = 10 statements, and the compiled object code of this function, which contains N = 14 statements.",2.1. Problem Formalization,[0],[0]
The alignment between the two is shown graphically by using a matrix of size N ×M .,2.1. Problem Formalization,[0],[0]
Each column (row) of this matrix corresponds to one source (object) code statement.,2.1. Problem Formalization,[0],[0]
"The matrix (i, j) element encodes the probability of aligning object-code statement i ∈ 1, . . .",2.1. Problem Formalization,[0],[0]
", N with the source-code statement j ∈ 1, . . .",2.1. Problem Formalization,[0],[0]
",M .",2.1. Problem Formalization,[0],[0]
"Since the matrix is the ground truth label, all probabilities are either 0 (black) or 1 (white).",2.1. Problem Formalization,[0],[0]
"In other words, each row i is a one-hot vector showing the alignment of one object-code statement i.
As can be seen in the figure, the first opcode push corresponds to the function’s statement {, that opens the function’s block.",2.1. Problem Formalization,[0],[0]
"As expected, there are also many opcodes that implement the for statement, which comprises comparing, incrementing and jumping.
",2.1. Problem Formalization,[0],[0]
The matrix representation is the target value of the neural alignment network.,2.1. Problem Formalization,[0],[0]
The network will output the rows of the alignment matrix as vectors of pseudo probabilities.,2.1. Problem Formalization,[0],[0]
We can view the resulting prediction matrix as a softalignment.,2.1. Problem Formalization,[0],[0]
"In order to obtain hard alignments, we simply take the index of the maximal element in each row.
",2.1. Problem Formalization,[0],[0]
"Another dimension in which we challenge our alignment network, is compilation optimization, which drastically changes the object code based on the level of optimization used (see Fig. 1 of supplementary material).",2.1. Problem Formalization,[0],[0]
"This optimization makes the object code more efficient and can render it shorter (more common) or longer than the code without optimization, see supplementary material.",2.1. Problem Formalization,[0],[0]
"Each statement is treated as a sequence of tokens, where the last token of each such sequence is always the end-ofstatement (EOS) token.",3. The Neural Alignment Network,[0],[0]
"A function is given by concatenating all such sequences to one sequence.
",3. The Neural Alignment Network,[0],[0]
"We employ a compound deep neural network for predicting the alignment, as explained in Sec. 3.2.",3. The Neural Alignment Network,[0],[0]
It consists of four parts: the first part is used for representing each source code statement j as a vector vj .,3. The Neural Alignment Network,[0],[0]
"The second part does the same for the object code, resulting in a representation vector ui.",3. The Neural Alignment Network,[0],[0]
"The third part processes using a convolutional neural network pairs of vector representations, one of each type, as a multi-channel grid, and produces an alignment score s(i, j).",3. The Neural Alignment Network,[0],[0]
It is not a probability.,3. The Neural Alignment Network,[0],[0]
"However, the higher the alignment value, the more likely the two statements are to be aligned.",3. The Neural Alignment Network,[0],[0]
"The alignment scores are fed to the top-most part of the network, which computes the pseudo probabilities pij of aligning object code statement i to the source code statement j. Specifically, the fourth part considers for an object-code statement i, all source-code statements j = 1, 2, . . .",3. The Neural Alignment Network,[0],[0]
",M the alignment score, and employs the softmax function: pij =
exp(s(ui,vj))∑M k=1 exp(s(ui,vk)) .",3. The Neural Alignment Network,[0],[0]
"Our model incorporates two LSTM networks to encode the sequences, one for each sequence domain: source code and object code.",3.1. Encoding the Input Statements,[0],[0]
"Therefore, we first embed each token in the input sequences in a high-dimensional space.",3.1. Encoding the Input Statements,[0],[0]
"We use different embeddings for source code and for object code, since each is composed of a different vocabulary.",3.1. Encoding the Input Statements,[0],[0]
"The vocabularies are hybrid, in the sense that they consist of both words and characters.
",3.1. Encoding the Input Statements,[0],[0]
The source code vocabulary is a hybrid of characters and the C language reserved words.,3.1. Encoding the Input Statements,[0],[0]
"A C reserved word is embedded to a single vector, while variable names, arguments and numeric values are decomposed to character by character sequences.",3.1. Encoding the Input Statements,[0],[0]
"The vocabulary contains the C language reserved words as atomic units, EOS, and the following single character elements: (i) alphanumeric characters including all letters and digits; (ii) the operators +, -, /, *, &, |, ˆ, ∼, ?; and (iii) the following punctuation marks: (, ),",3.1. Encoding the Input Statements,[0],[0]
"[, ], {, }, <, >, =, !, ,, ’, "", ;, #, \. Let ε(α) denote the embedding of a C token α to a vector.",3.1. Encoding the Input Statements,[0],[0]
"Then the C code string if (a5<42), for example, is decomposed to the following sequence: ε(if), ε((), ε(a), ε(5), ε(<), ε(4), ε(2), ε()), ε(EOS).
",3.1. Encoding the Input Statements,[0],[0]
"Similarly, the object code vocabulary is also a hybrid, and contains opcodes, registers and characters of numeric values and is based on the assembly representation of the object code.",3.1. Encoding the Input Statements,[0],[0]
The opcode of each statement is one out of dozens of possible values.,3.1. Encoding the Input Statements,[0],[0]
"The operands are either regis-
ters or numeric values.",3.1. Encoding the Input Statements,[0],[0]
"The vocabulary also includes the punctuation marks of the assembly language and, therefore, contains the following types of elements: (i) the various opcodes; (ii) the various registers; (iii) hexadecimal digits; (iv) the symbols (,),x,-,:; and (v) EOS, which ends every statement.",3.1. Encoding the Input Statements,[0],[0]
Let ε′(β) denote the embedding of an object code token β to a vector.,3.1. Encoding the Input Statements,[0],[0]
"Then the following assembly string mov %eax,-0x8(%rbp), for example, is decomposed to the following sequence: ε′(mov), ε′(%eax), ε′(-), ε′(0), ε′(x), ε′(8), ε′((), ε′(%rbp), ε′()), ε′(EOS).",3.1. Encoding the Input Statements,[0],[0]
The network architecture is depicted in Fig. 2(a).,3.2. Neural Network Architecture,[0],[0]
The input sequences introduce many complex and long-range dependencies.,3.2. Neural Network Architecture,[0],[0]
"Therefore, the network employs two LSTM encoders: one for creating a representation of the source code statements and one is used for representing the object code statements.",3.2. Neural Network Architecture,[0],[0]
"In all of our experiments, the LSTMs have one layer and 128 cells.
",3.2. Neural Network Architecture,[0],[0]
Recall that each statement in the input sequences is a sequence of tokens.,3.2. Neural Network Architecture,[0],[0]
"However, for alignment, only a single vector representation is required per statement.",3.2. Neural Network Architecture,[0],[0]
"In order to obtain a single vector representation per statement, we sample the representation sequences output by the encoders only at time steps corresponding to EOS’s.",3.2. Neural Network Architecture,[0],[0]
"It should be noted that information from other statements is not lost, since each RNN activation is affected by other activations in the sequence.",3.2. Neural Network Architecture,[0],[0]
"Moreover, since EOS is ubiquitous, its representation must be based on its context.",3.2. Neural Network Architecture,[0],[0]
"Otherwise, it is meaningless.",3.2. Neural Network Architecture,[0],[0]
"During training, the network learns to create meaningful representations at the location of the EOS inputs.
",3.2. Neural Network Architecture,[0],[0]
"The result of the LSTM encoders areM representation vectors output by the source-code encoding LSTM, denoted by {vj}j∈(1,...,M), and N representation vectors output by the object-code encoding LSTM, denoted by {ui}i∈(1,...,N).",3.2. Neural Network Architecture,[0],[0]
"The statement representation vectors are then assembled in anN×M grid, such that the (i, j) element is [ui; vj ], where ; denotes vector concatenation.",3.2. Neural Network Architecture,[0],[0]
"Since each encoder LSTM has 128 cells, the vector [ui; vj ] has 256 channels.
",3.2. Neural Network Architecture,[0],[0]
"In order to transform the statement representation pairs to alignment scores, we employ a decoding Convolutional Neural Network (CNN) over the 256-channel grid.",3.2. Neural Network Architecture,[0],[0]
"The decoding CNN has five convolutional layers, each with 32 5 × 5 filters followed by ReLU non-linearities, except for the last layer which consists of one 5× 5 filter and no nonlinearities.",3.2. Neural Network Architecture,[0],[0]
"The CNN output is, therefore, a single channel N ×M grid, s(i, j), representing the alignment score of object code statement",3.2. Neural Network Architecture,[0],[0]
"i and source code statement j.
",3.2. Neural Network Architecture,[0],[0]
"In the many-to-one alignment problem, the network’s output for each row should contain pseudo probabilities.
",3.2. Neural Network Architecture,[0],[0]
"Therefore, we add a softmax layer on top of the list of alignment scores computed for each object-code statement i: s(ui, v1), s(ui, v2), . . .",3.2. Neural Network Architecture,[0],[0]
", s(ui, vM ), i.e., there are N softmax layers, each converting M alignment scores to a vector of probabilities {pij}j∈(1,...,M) for each row i ∈",3.2. Neural Network Architecture,[0],[0]
"(1, . . .",3.2. Neural Network Architecture,[0],[0]
", N).
",3.2. Neural Network Architecture,[0],[0]
"During training, the Negative Log Likelihood (NLL) loss is used.",3.2. Neural Network Architecture,[0],[0]
"Let A be the set of N object-code to source-code alignments (i, j).",3.2. Neural Network Architecture,[0],[0]
"The training loss for a single training sample is given by 1N ∑ (i,j)∈A−log(pij) , i.e., the loss is the mean of NLL values of all N rows.",3.2. Neural Network Architecture,[0],[0]
"For comparison, we also consider a model that performs decoding directly over the statements grid.",3.3. Local Grid Decoder,[0],[0]
"In this model, the decoder consists only of a single layer network s attached to each one of the NM pairs of object code and source code statement representations (ui and vj).",3.3. Local Grid Decoder,[0],[0]
The same network weights are shared between all NM pairs and are trained jointly.,3.3. Local Grid Decoder,[0],[0]
"This network is given by:
s(ui, vj) = v T tanh(Woui +Wsvj)
where v, Wo and Ws are the network’s weights.",3.3. Local Grid Decoder,[0],[0]
"We consider another, simpler version of the Local Grid Decoder, where s(ui, vj) = uTi · vj , i.e., an inner product operation is employed, instead of the single layer network.",3.3. Local Grid Decoder,[0],[0]
"In this section, we describe the baseline methods that we compare to our architecture.",4. Literature Baseline Methods,[0],[0]
"Our architecture and all baselines use LSTM encoders to encode the input sequences, and softmax layers on top of the decoder output in order to produce an alignment probability, as explained in Sec. 3.2.",4. Literature Baseline Methods,[0],[0]
"The architectures differ only in the decoder part that produces alignment scores, i.e., in the model s(i, j).",4. Literature Baseline Methods,[0],[0]
"A profound difference between our architecture and the baselines, is that while our architecture predicts the alignment over the whole statements grid at once, the baselines predict the alignment sequentially.",4. Literature Baseline Methods,[0],[0]
"This baseline adapts the Pointer Network (Ptr-Net) architecture proposed by (Vinyals et al., 2015) in two ways.",4.1. Pointer Network,[0],[0]
PtrNet is designed to solve the task of producing a sequence of pointers to an input sequence.,4.1. Pointer Network,[0],[0]
The Ptr-Net architecture employs an encoder LSTM to represent the input sequence as a sequence of hidden states ej .,4.1. Pointer Network,[0],[0]
A second decoder LSTM then produces hidden states that are used to point to locations in the input sequence via an attention mechanism.,4.1. Pointer Network,[0],[0]
Denote the hidden states of the decoder as di.,4.1. Pointer Network,[0],[0]
"The attention mechanism is then given by:
uij = v T tanh(W1ej +W2di) j ∈ (1, . . .",4.1. Pointer Network,[0],[0]
",",4.1. Pointer Network,[0],[0]
"n)
",4.1. Pointer Network,[0],[0]
"pi = softmax(u i)
where n is the input sequence length and pi is the soft prediction at time step i of the decoder LSTM.",4.1. Pointer Network,[0],[0]
"The input to the decoder LSTM at time step i is argmax
j (ui−1j ), i.e., the
input token ”pointed” by the attention mechanism at the previous step.",4.1. Pointer Network,[0],[0]
"Thus, the output of the decoder LSTM can be considered as a sequence of pointers to locations at the input sequence.
",4.1. Pointer Network,[0],[0]
"Since in the alignment problem we need to align each object code statement to one of the source code statements, we adapt Ptr-Net to produce ”pointers” to the source code statements sequence for every object code statement.",4.1. Pointer Network,[0],[0]
"The adaptation is not trivial: our problem presents two input sequences, while Ptr-Net is originally designed to handle one.",4.1. Pointer Network,[0],[0]
"We create two such adaptations, Ptr1 and Ptr2, which are depicted in Fig. 2(b) and (c), respectively.
",4.1. Pointer Network,[0],[0]
"In Ptr1, we employ a Ptr-Net decoder at each time step i over the sequence of object code statement representations ui.",4.1. Pointer Network,[0],[0]
"The decoder is an LSTM network, whose hid-
den state hi is fed to an attention model employed over the whole sequence of source code statement representations vj : s(i, j) = vT",4.1. Pointer Network,[0],[0]
"tanh(Wsvj +Whhi).
",4.1. Pointer Network,[0],[0]
"The outputs s(i, j) of the attention model are used as the alignment scores that will be fed later to the softmax layers.",4.1. Pointer Network,[0],[0]
"The Ptr-Net decoder receives at each time step i, the source code statement representation that the attention model ”pointed” to at the previous step i − 1, i.e., vpi−1 where pi = argmax
j (s(i, j)).
",4.1. Pointer Network,[0],[0]
"Finally, in order to condition the output of the pointer decoder at the current object code statement representation ui, the input of the pointer decoder LSTM is the concatenation of ui and vpi−1 :
hi = LSTM([ui; vpi−1 ], hi−1, ci, ci−1),
where ci is the contents of the LSTM memory cells at time step i.",4.1. Pointer Network,[0],[0]
"At the first time step i = 1, the value of vp0 is the all-0 vector, and h0 is initialized with the last hidden state of the source-code statements encoding LSTM.
",4.1. Pointer Network,[0],[0]
"It should be noted, that at each step, the Ptr-Net decoder sees the current object code statement and the previous ”pointed” source code statement.",4.1. Pointer Network,[0],[0]
It means that the LSTM sees the source code statement that is aligned to the previous object code statement.,4.1. Pointer Network,[0],[0]
"A wiser adaptation would present the Ptr-Net decoder LSTM with the explicit alignment decision, i.e., the previous ”pointed” source code statement and the previous object code statement, such that the input is a pair of two statements that were predicted to align.",4.1. Pointer Network,[0],[0]
"Thus, in the second adaptation of Ptr-Net to our problem, which we call Ptr2, the input to the Ptr-Net decoder LSTM is the concatenation of ui−1 and vpi−1 :
hi = LSTM([ui−1; vpi−1 ], hi−1, ci, ci−1).
",4.1. Pointer Network,[0],[0]
"The current object code statement representation ui is then fed directly to the attention model, in addition to the PtrNet decoder output and the source code statement representation: s(i, j) = vT",4.1. Pointer Network,[0],[0]
tanh(Woui +Wsvj +Whhi).,4.1. Pointer Network,[0],[0]
"This baseline uses the matching scores of the Match-LSTM architecture (Wang & Jiang, 2015).",4.2. Match-LSTM,[0],[0]
"The architecture receives as inputs two sentences, a premise and a hypothesis.",4.2. Match-LSTM,[0],[0]
"First, the two sentences are processed using two LSTM networks, to produce the hidden representation sequences vj and ui for the premise and hypothesis, respectively.",4.2. Match-LSTM,[0],[0]
"Next, attention ai vectors are computed over the premise representation sequence as follows: ai = ∑M k=1 αijvj , where
αij are the attention weights and are given by
αij = exp(s(ui, vj))∑M k=1 exp(s(ui, vk))
s(i, j) = vT",4.2. Match-LSTM,[0],[0]
"tanh(Woui +Wsvj +Whhi−1),
where hi is the hidden state of the third LSTM that processes the hypothesis representation sequence together with the attention vector computed over the whole premise sequence: hi = LSTM([ui; ai], hi−1, ci, ci−1).
",4.2. Match-LSTM,[0],[0]
"For further details about the Match-LSTM architecture, see (Wang & Jiang, 2015).",4.2. Match-LSTM,[0],[0]
"In order to adapt Match-LSTM to our problem, we simply substitute the premise (hypothesis) representation sequence with the source (object) code statements representation sequence, and use the matching scores s(i, j) as the alignment scores.",4.2. Match-LSTM,[0],[0]
Data collection We employ both synthetic C functions generated randomly and human-written C functions from real-world projects.,5. Evaluation,[0],[0]
"In order to generate random C functions, we used pyfuzz, an open-source random program generator for python (Myint, 2013), and modified it so it will output short functions written in C rather than python.",5. Evaluation,[0],[0]
"For the real-world human-written data set, we used over 53,000 short functions from 90 open-source projects, that are part of the GNU project and are written in C. Among them are grep, nano, etc.",5. Evaluation,[0],[0]
"Before compilation, we ran only the preprocessor of GCC, in order to clean the sources of non-code text, such as comments, macros, #ifdef commands and more.
",5. Evaluation,[0],[0]
"In order to compile the source code with optimizations, we use the GCC compiler (Stallman et al., 2009) with the optimization levels -O1, -O2 or -O3.",5. Evaluation,[0],[0]
Each level turns on additional optimization flags.,5. Evaluation,[0],[0]
"Each of the datasets of generated and human-written C functions has three parts, each compiled using one of the three mentioned optimization levels.",5. Evaluation,[0],[0]
"After compilation of the human-written projects, some functions contained object code from other, inline functions.",5. Evaluation,[0],[0]
"These functions were excluded from the dataset in order to introduce the network with pure translation
pairs, i.e., source code and object code that has originated entirely from it.",5. Evaluation,[0],[0]
"In addition, we tell GCC to output debugging information that includes the statement-level alignment between each C function and the object code compiled from it.",5. Evaluation,[0],[0]
"Therefore, each sample in the resulting dataset consists of source code, object code compiled at some optimization level and the statement-by-statement alignment between them.",5. Evaluation,[0],[0]
"Tab. 1 reports the statistics of the code alignment datasets.
",5. Evaluation,[0],[0]
Training procedure For each data set,5. Evaluation,[0],[0]
", we train one network for all optimization levels.",5. Evaluation,[0],[0]
The length of all functions has been limited to 450 tokens.,5. Evaluation,[0],[0]
"The training set of synthetic functions contains 120,000 samples.",5. Evaluation,[0],[0]
"The validation and the test sets contain 15,000 samples each.",5. Evaluation,[0],[0]
"The training, validation and test sets of human-written functions contain 42,391, 5,474 and 5,253 samples, respectively.",5. Evaluation,[0],[0]
"During training, we use batches of 32 samples each.
",5. Evaluation,[0],[0]
"The weights of the LSTM and attention networks are initialized uniformly in [−1.0, 1.0].",5. Evaluation,[0],[0]
The CNN filter weights are initialized using truncated normal distribution with a standard deviation of 0.1.,5. Evaluation,[0],[0]
"The biases of the LSTM and CNN networks are initialized to 0.0, except for the biases of the LSTM forget gates, which are initialized to 1.0 in order to encourage memorization at the beginning of training (Józefowicz et al., 2015).",5. Evaluation,[0],[0]
"The Adam learning rate scheme (Kingma & Ba, 2015) is used, with a learning rate of 0.001, β1 = 0.9, β2 = 0.999, and = 1e− 08.",5. Evaluation,[0],[0]
Our proposed network and the baseline methods are trained and evaluated over the datasets of synthetic and humanwritten code.,5.1. Alignment Results,[0],[0]
"Tab. 2 and Tab. 3 present the resulting accuracy, which is computed per object-code statement as follows.",5.1. Alignment Results,[0],[0]
"First, the network predicts pseudo-probabilities of aligning source code statements to each object code statement.",5.1. Alignment Results,[0],[0]
"Second, in order to obtain hard alignments, we take the index of the maximal element in each row of the predicted soft alignment matrix.",5.1. Alignment Results,[0],[0]
"Third, for every object code statement, we count a true alignment only if the aligned source code statement is the ground truth alignment.",5.1. Alignment Results,[0],[0]
The accuracy is reported separately for the three optimization levels and for all of them combined.,5.1. Alignment Results,[0],[0]
"As can be seen in Tab. 2, all models excel over synthetic code, reaching almost perfect alignment accuracy with a slight advantage to our Convolutional and Local Grid Decoders.",5.1. Alignment Results,[0],[0]
Tab. 3 shows that the GNU code is more challenging to all methods.,5.1. Alignment Results,[0],[0]
"Our proposed Grid Decoder models outperform all baseline methods, and the Convolutional Grid Decoder is superior by a substantial margin over the local and inner product alternatives.",5.1. Alignment Results,[0],[0]
"The Ptr1, Ptr2 and Match-LSTM baselines reach about the same performance.",5.1. Alignment Results,[0],[0]
"It is an expected result, since these models are very similar: they all employ a decoding LSTM and an attention mechanism, with only small differences in performing the sequential processing of the encoded representation sequences.",5.1. Alignment Results,[0],[0]
"We perform an additional experiment based on the TSP benchmark presented in (Vinyals et al., 2015) in order to di-
rectly compare with the Pointer Network architecture (PtrNet), where it was already tested.",5.2. Traveling Salesman Problem (TSP),[0],[0]
The input of the TSP problem is a randomly ordered sequence of 2D points.,5.2. Traveling Salesman Problem (TSP),[0],[0]
"The output is a sequence of all the points reordered, such that the route length (sum of distance between adjacent points) is minimal.",5.2. Traveling Salesman Problem (TSP),[0],[0]
"For our method, we consider the connectivity matrix of the cycle graph in lieu of the alignment matrix.",5.2. Traveling Salesman Problem (TSP),[0],[0]
"As reported in (Vinyals et al., 2015), overfitting was observed here.",5.2. Traveling Salesman Problem (TSP),[0],[0]
"Therefore, we performed the following data augmentation process.",5.2. Traveling Salesman Problem (TSP),[0],[0]
"For each sample in the training set, the IDs of the 2D points are permuted randomly and independently of the other samples.",5.2. Traveling Salesman Problem (TSP),[0],[0]
It is equivalent to randomly shuffling the order of the points in the sample sequence.,5.2. Traveling Salesman Problem (TSP),[0],[0]
"The IDs in the label are then permuted accordingly, to represent the same target route.",5.2. Traveling Salesman Problem (TSP),[0],[0]
"During training, the process was repeated at the beginning of every epoch, and independently of past epochs.",5.2. Traveling Salesman Problem (TSP),[0],[0]
Fig. 5 depicts an example route and its connectivity matrix before and after permutation of the node IDs.,5.2. Traveling Salesman Problem (TSP),[0],[0]
"The results are presented in Tab. 4, along with the optimal and approximated results (see (Vinyals et al., 2015) for further details).",5.2. Traveling Salesman Problem (TSP),[0],[0]
"As can be seen, our method is comparable to the original Ptr-Net model for both n = 5 and n = 10.",5.2. Traveling Salesman Problem (TSP),[0],[0]
We present a neural network architecture for aligning two sequences.,6. Summary,[0],[0]
"We challenge our network with aligning source code to its compiled object code, sequences that in some aspects are more complex than human language sentences.",6. Summary,[0],[0]
Our experiments demonstrate that the proposed architecture is successful in predicting the alignment.,6. Summary,[0],[0]
"On this task, the network outperforms multiple literature baselines such as Pointer Networks and Match-LSTM, suggesting that a global, CNN-based approach to alignment is better than the sequential, RNN-based approach.
",6. Summary,[0],[0]
"Our model can be used for alignment of any two sequences with a many-to-one map between them, and extended to other graph problems, as demonstrated for TSP.",6. Summary,[0],[0]
"The authors would like to thank Ofir Press, Dotan Kaufman and Shimi Salant for useful advice and insightful discussions.",Acknowledgments,[0],[0]
This work was supported by a 2016 ICRC grant.,Acknowledgments,[0],[0]
We propose a new neural network architecture and use it for the task of statement-by-statement alignment of source code and its compiled object code.,abstractText,[0],[0]
Our architecture learns the alignment between the two sequences – one being the translation of the other – by mapping each statement to a context-dependent representation vector and aligning such vectors using a grid of the two sequence domains.,abstractText,[0],[0]
"Our experiments include short C functions, both artificial and human-written, and show that our neural network architecture is able to predict the alignment with high accuracy, outperforming known baselines.",abstractText,[0],[0]
We also demonstrate that our model is general and can learn to solve graph problems such as the Traveling Salesman Problem.,abstractText,[0],[0]
Learning to Align the Source Code to the Compiled Object Code,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2737–2746 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2737",text,[0],[0]
"A principle goal of asking questions is to fill information gaps, typically through clarification questions.1",1 Introduction,[0],[0]
We take the perspective that a good question is the one whose likely answer will be useful.,1 Introduction,[0],[0]
"Consider the exchange in Figure 1, in which an initial poster (who we call “Terry”) asks for help configuring environment variables.",1 Introduction,[0],[0]
"This post is underspecified and a responder (“Parker”) asks a clarifying question (a) below, but could alternatively have asked (b) or (c):
(a) What version of Ubuntu do you have?
1We define ‘clarification question’ as a question that asks for some information that is currently missing from the given context.
",1 Introduction,[0],[0]
(b) What is the make of your wifi card?,1 Introduction,[0],[0]
"(c) Are you running Ubuntu 14.10 kernel 4.4.0-59-
generic on an x86 64 architecture?",1 Introduction,[0],[0]
Parker should not ask (b) because an answer is unlikely to be useful; they should not ask (c) because it is too specific and an answer like “No” or “I do not know” gives little help.,1 Introduction,[0],[0]
"Parker’s question (a) is much better: it is both likely to be useful, and is plausibly answerable by Terry.
",1 Introduction,[0],[0]
"In this work, we design a model to rank a candidate set of clarification questions by their usefulness to the given post.",1 Introduction,[0],[0]
"We imagine a use case (more discussion in §7) in which, while Terry is writing their post, a system suggests a shortlist of questions asking for information that it thinks people like Parker might need to provide a solution, thus enabling Terry to immediately clarify their post, potentially leading to a much quicker resolution.",1 Introduction,[0],[0]
"Our model is based on the decision theoretic framework of the Expected Value of Perfect Information (EVPI) (Avriel and Williams, 1970), a measure of the value of gathering additional information.",1 Introduction,[0],[0]
"In our setting, we use EVPI to calculate which questions are most likely to elicit an answer that would make the post more informative.
",1 Introduction,[0],[0]
Our work has two main contributions: 1.,1 Introduction,[0],[0]
"A novel neural-network model for address-
ing the task of ranking clarification question built on the framework of expected value of perfect information (§2).",1 Introduction,[0],[0]
2.,1 Introduction,[0],[0]
"A novel dataset, derived from StackExchange2, that enables us to learn a model to ask clarifying questions by looking at the types of questions people ask (§3).
",1 Introduction,[0],[0]
We formulate this task as a ranking problem on a set of potential clarification questions.,1 Introduction,[0],[0]
We evaluate models both on the task of returning the original clarification question and also on the task of picking any of the candidate clarification questions marked as good by experts (§4).,1 Introduction,[0],[0]
We find that our EVPI model outperforms the baseline models when evaluated against expert human annotations.,1 Introduction,[0],[0]
We include a few examples of human annotations along with our model performance on them in the supplementary material.,1 Introduction,[0],[0]
"We have released our dataset of∼77K (p, q, a) triples and the expert annotations on 500 triples to help facilitate further research in this task.3",1 Introduction,[0],[0]
We build a neural network model inspired by the theory of expected value of perfect information (EVPI).,2 Model description,[0],[0]
"EVPI is a measurement of: if I were to acquire information X, how useful would that be to
2We use data from StackExchange; per license cc-by-sa 3.0, the data is “intended to be shared and remixed” (with attribution).
",2 Model description,[0],[0]
"3https://github.com/raosudha89/ ranking_clarification_questions
me?",2 Model description,[0],[0]
"However, because we haven’t acquired X yet, we have to take this quantity in expectation over all possible X, weighted by each X’s likelihood.",2 Model description,[0],[0]
"In our setting, for any given question qi that we can ask, there is a set A of possible answers that could be given.",2 Model description,[0],[0]
For each possible answer aj ∈,2 Model description,[0],[0]
"A, there is some probability of getting that answer, and some utility if that were the answer we got.",2 Model description,[0],[0]
"The value of this question qi is the expected utility, over all possible answers:
EVPI(qi|p) = ∑",2 Model description,[0],[0]
"aj∈A P[aj |p, qi]U(p+ aj) (1)
",2 Model description,[0],[0]
"In Eq 1, p is the post, qi is a potential question from a set of candidate questionsQ and aj is a potential answer from a set of candidate answers A.",2 Model description,[0],[0]
"Here, P[aj |p, qi] measures the probability of getting an answer aj given an initial post p and a clarifying question qi, and U(p + aj) is a utility function that measures how much more complete p would be if it were augmented with answer aj .",2 Model description,[0],[0]
"The modeling question then is how to model:
1.",2 Model description,[0],[0]
"The probability distribution P[aj |p, qi] and 2.",2 Model description,[0],[0]
"The utility function U(p+ aj).
",2 Model description,[0],[0]
"In our work, we represent both using neural networks over the appropriate inputs.",2 Model description,[0],[0]
"We train the parameters of the two models jointly to minimize a joint loss defined such that an answer that has a higher potential of increasing the utility of a post gets a higher probability.
",2 Model description,[0],[0]
Figure 2 describes the behavior of our model during test time.,2 Model description,[0],[0]
"Given a post p, we generate a set of candidate questions and a set of candidate
answers (§2.1).",2 Model description,[0],[0]
"Given a post p and a question candidate qi, we calculate how likely is this question to be answered using one of our answer candidates aj (§2.2).",2 Model description,[0],[0]
"Given a post p and an answer candidate aj , we calculate the utility of the updated post i.e. U(p+ aj) (§2.3).",2 Model description,[0],[0]
We compose these modules into a joint neural network that we optimize end-to-end over our data (§2.4).,2 Model description,[0],[0]
"Given a post p, our first step is to generate a set of question and answer candidates.",2.1 Question & answer candidate generator,[0],[0]
One way that humans learn to ask questions is by looking at how others ask questions in a similar situation.,2.1 Question & answer candidate generator,[0],[0]
Using this intuition we generate question candidates for a given post by identifying posts similar to the given post and then looking at the questions asked to those posts.,2.1 Question & answer candidate generator,[0],[0]
"For identifying similar posts, we use Lucene4, a software extensively used in information retrieval for extracting documents relevant to a given query from a pool of documents.",2.1 Question & answer candidate generator,[0],[0]
Lucene implements a variant of the term frequency-inverse document frequency (TF-IDF) model to score the extracted documents according to their relevance to the query.,2.1 Question & answer candidate generator,[0],[0]
We use Lucene to find the top 10 posts most similar to a given post from our dataset (§3).,2.1 Question & answer candidate generator,[0],[0]
We consider the questions asked to these 10 posts as our set of question candidates Q and the edits made to the posts in response to the questions as our set of answer candidates A.,2.1 Question & answer candidate generator,[0],[0]
"Since the top-most similar candidate extracted by Lucene is always the original post itself, the original question and answer paired with the post is always one of the candidates in Q and A.",2.1 Question & answer candidate generator,[0],[0]
"§3 describes in detail the process of extracting the
4https://lucene.apache.org/
(post, question, answer) triples from the StackExchange datadump.",2.1 Question & answer candidate generator,[0],[0]
"Given a post p and a question candidate qi, our second step is to calculate how likely is this question to be answered using one of our answer candidates aj .",2.2 Answer modeling,[0],[0]
"We first generate an answer representation by combining the neural representations of the post and the question using a function Fans(p̄, q̄i) (details in §2.4).",2.2 Answer modeling,[0],[0]
"Given such a representation, we measure the distance between this answer representation and one of the answer candidates aj using the function below:
dist(Fans(p̄, q̄i), âj) = 1− cos sim(Fans(p̄, q̄i), âj)
",2.2 Answer modeling,[0],[0]
"The likelihood of an answer candidate aj being the answer to a question qi on post p is finally calculated by combining this distance with the cosine similarity between the question qi and the question qj paired with the answer candidate aj :
P[aj |p, qi] = exp−dist(Fans(p̄, q̄i), âj) ∗cos sim(q̂i, q̂j) (2)
where âj , q̂i and q̂j are the average word vector of aj , qi and qj respectively (details in §2.4) and cos sim is the cosine similarity between the two input vectors.
",2.2 Answer modeling,[0],[0]
We model our answer generator using the following intuition: a question can be asked in several different ways.,2.2 Answer modeling,[0],[0]
"For e.g. in Figure 1, the question “What version of Ubuntu do you have?” can be asked in other ways like “What version of operating system are you using?”, “Version of OS?”, etc.",2.2 Answer modeling,[0],[0]
"Additionally, for a given post and a question, there can be
several different answers to that question.",2.2 Answer modeling,[0],[0]
"For instance, “Ubuntu 14.04 LTS”, “Ubuntu 12.0”, “Ubuntu 9.0”, are all valid answers.",2.2 Answer modeling,[0],[0]
"To generate an answer representation capturing these generalizations, we train our answer generator on our triples dataset (§3) using the loss function below:
lossans(pi, qi, ai, Qi) = dist(Fans(p̄i, q̄i), âi) (3) + ∑ j∈Q ( dist(Fans(p̄i, q̄i), âj) ∗ cos sim(q̂i, q̂j) )
where, â and q̂ is the average word vectors of a and q respectively (details in §2.4), cos sim is the cosine similarity between the two input vectors.
",2.2 Answer modeling,[0],[0]
This loss function can be explained using the example in Figure 3.,2.2 Answer modeling,[0],[0]
Question qi is the question paired with the given post pi.,2.2 Answer modeling,[0],[0]
"In Eq 3, the first term forces the function Fans(p̄i, q̄i) to generate an answer representation as close as possible to the correct answer ai.",2.2 Answer modeling,[0],[0]
"Now, a question can be asked in several different ways.",2.2 Answer modeling,[0],[0]
"Let Qi be the set of candidate questions for post pi, retrieved from the dataset using Lucene (§ 2.1).",2.2 Answer modeling,[0],[0]
"Suppose a question candidate qj is very similar to the correct question qi ( i.e. cos sim(q̂i, q̂j) is near zero).",2.2 Answer modeling,[0],[0]
"Then the second term forces the answer representation Fans(p̄i, q̄i) to be close to the answer aj corresponding to the question qj as well.",2.2 Answer modeling,[0],[0]
"Thus in Figure 3, the answer representation will be close to aj (since qj is similar to qi), but may not be necessarily close to ak (since qk is dissimilar to qi).",2.2 Answer modeling,[0],[0]
"Given a post p and an answer candidate aj , the third step is to calculate the utility of the updated post i.e. U(p+aj).",2.3 Utility calculator,[0],[0]
"As expressed in Eq 1, this utility function measures how useful it would be if a given post p were augmented with an answer aj paired with a different question qj in the candidate set.",2.3 Utility calculator,[0],[0]
"Although theoretically, the utility of the updated post can be calculated only using the given post (p) and the candidate answer (aj), empirically we find that our neural EVPI model performs better when the candidate question (qj) paired with the candidate answer is a part of the utility function.",2.3 Utility calculator,[0],[0]
We attribute this to the fact that much information about whether an answer increases the utility of a post is also contained in the question asked to the post.,2.3 Utility calculator,[0],[0]
"We train our utility calculator using our dataset of (p, q, a) triples (§3).",2.3 Utility calculator,[0],[0]
"We label all the (pi, qi, ai) pairs from our triples dataset with label y = 1.",2.3 Utility calculator,[0],[0]
"To get negative samples, we make use of
the answer candidates generated using Lucene as described in §2.1.",2.3 Utility calculator,[0],[0]
"For each aj ∈ Ai, where Ai is the set of answer candidates for post pi, we label the pair (pi, qj , aj) with label y = 0, except for when aj = ai.",2.3 Utility calculator,[0],[0]
"Thus, for each post pi in our triples dataset, we have one positive sample and nine negative samples.",2.3 Utility calculator,[0],[0]
It should be noted that this is a noisy labelling scheme since a question not paired with the original question in our dataset can often times be a good question to ask to the post (§4).,2.3 Utility calculator,[0],[0]
"However, since we do not have annotations for such other good questions at train time, we assume such a labelling.
",2.3 Utility calculator,[0],[0]
"Given a post pi and an answer aj paired with the question qj , we combine their neural representations using a function Futil(p̄i, q̄j , āj) (details in §2.4).",2.3 Utility calculator,[0],[0]
"The utility of the updated post is then defined as U(pi + aj) = σ(Futil(p̄i, q̄j , āj))5.",2.3 Utility calculator,[0],[0]
"We want this utility to be close to 1 for all the positively labelled (p, q, a) triples and close to 0 for all the negatively labelled (p, q, a) triples.",2.3 Utility calculator,[0],[0]
"We therefore define our loss using the binary cross-entropy formulation below:
lossutil(yi, p̄i, q̄j , āj)",2.3 Utility calculator,[0],[0]
"= yi log(σ(Futil(p̄i, q̄j , āj)))",2.3 Utility calculator,[0],[0]
(4),2.3 Utility calculator,[0],[0]
Our fundamental representation is based on recurrent neural networks over word embeddings.,2.4 Our joint neural network model,[0],[0]
"We obtain the word embeddings using the GloVe (Pennington et al., 2014) model trained on the entire datadump of StackExchange.6.",2.4 Our joint neural network model,[0],[0]
"In Eq 2 and Eq 3, the average word vector representations q̂ and â are obtained by averaging the GloVe word embeddings for all words in the question and the answer respectively.",2.4 Our joint neural network model,[0],[0]
"Given an initial post p, we generate a post neural representation p̄ using a post LSTM (long short-term memory architecture) (Hochreiter and Schmidhuber, 1997).",2.4 Our joint neural network model,[0],[0]
The input layer consists of word embeddings of the words in the post which is fed into a single hidden layer.,2.4 Our joint neural network model,[0],[0]
The output of each of the hidden states is averaged together to get our neural representation p̄.,2.4 Our joint neural network model,[0],[0]
"Similarly, given a question q and an answer a, we generate the neural representations q̄ and ā using a question LSTM and an answer LSTM respectively.",2.4 Our joint neural network model,[0],[0]
"We define the function Fans in our answer model as a feedforward neural network with five hidden layers on the inputs p̄ and q̄. Likewise, we
5σ is the sigmoid function.",2.4 Our joint neural network model,[0],[0]
"6Details in the supplementary material.
define the function Futil in our utility calculator as a feedforward neural network with five hidden layers on the inputs p̄, q̄ and ā. We train the parameters of the three LSTMs corresponding to p, q and a, and the parameters of the two feedforward neural networks jointly to minimize the sum of the loss of our answer model (Eq 3) and our utility calculator (Eq 4) over our entire dataset:∑ i ∑ j lossans(p̄i, q̄i, āi, Qi) +",2.4 Our joint neural network model,[0],[0]
"lossutil(yi, p̄i, q̄j , āj)
(5)
",2.4 Our joint neural network model,[0],[0]
"Given such an estimate P[aj |p, qi] of an answer and a utility U(p + aj) of the updated post, we rank the candidate questions by their value as calculated using Eq 1.",2.4 Our joint neural network model,[0],[0]
"The remaining question, then, is how to get data that enables us to train our answer model and our utility calculator.",2.4 Our joint neural network model,[0],[0]
"Given data, the training becomes a multitask learning problem, where we learn simultaneously to predict utility and to estimate the probability of answers.",2.4 Our joint neural network model,[0],[0]
"StackExchange is a network of online question answering websites about varied topics like academia, ubuntu operating system, latex, etc.",3 Dataset creation,[0],[0]
"The data dump of StackExchange contains timestamped information about the posts, comments on the post and the history of the revisions made to the post.",3 Dataset creation,[0],[0]
"We use this data dump to create our dataset of (post, question, answer) triples: where the post is the initial unedited post, the question is the comment containing a question and the answer is either the edit made to the post after the question or the author’s response to the question in the comments section.
",3 Dataset creation,[0],[0]
Extract posts: We use the post histories to identify posts that have been updated by its author.,3 Dataset creation,[0],[0]
"We use the timestamp information to retrieve the initial unedited version of the post.
Extract questions: For each such initial version of the post, we use the timestamp information of its comments to identify the first question comment made to the post.",3 Dataset creation,[0],[0]
We truncate the comment till its question mark ’?’ to retrieve the question part of the comment.,3 Dataset creation,[0],[0]
We find that about 7% of these are rhetoric questions that indirectly suggest a solution to the post.,3 Dataset creation,[0],[0]
For e.g. “have you considered installing X?”.,3 Dataset creation,[0],[0]
"We do a manual analysis of
these non-clarification questions and hand-crafted a few rules to remove them.",3 Dataset creation,[0],[0]
"7
Extract answers: We extract the answer to a clarification question in the following two ways: (a) Edited post: Authors tend to respond to a clarification question by editing their original post and adding the missing information.",3 Dataset creation,[0],[0]
"In order to account for edits made for other reasons like stylistic updates and grammatical corrections, we consider only those edits that are longer than four words.",3 Dataset creation,[0],[0]
"Authors can make multiple edits to a post in response to multiple clarification questions.8 To identify the edit made corresponding to the given question comment, we choose the edit closest in time following the question.",3 Dataset creation,[0],[0]
(b) Response to the question: Authors also respond to clarification questions as subsequent comments in the comment section.,3 Dataset creation,[0],[0]
"We extract the first comment by the author following the clarification question as the answer to the question.
",3 Dataset creation,[0],[0]
"In cases where both the methods above yield an answer, we pick the one that is the most semantically similar to the question, where the measure of similarity is the cosine distance between the average word embeddings of the question and the answer.
",3 Dataset creation,[0],[0]
"We extract a total of 77,097 (post, question, answer) triples across three domains in StackExchange (Table 1).",3 Dataset creation,[0],[0]
We will release this dataset along with the the nine question and answer candidates per triple that we generate using lucene (§ 2.1).,3 Dataset creation,[0],[0]
We include an analysis of our dataset in the supplementary material.,3 Dataset creation,[0],[0]
"We define our task as given a post p, and a set of candidate clarification questions Q, rank the questions according to their usefulness to the post.
7Details in the supplementary material.",4 Evaluation design,[0],[0]
"8On analysis, we find that 35%-40% of the posts get asked multiple clarification questions.",4 Evaluation design,[0],[0]
"We include only the first clarification question to a post in our dataset since identifying if the following questions are clarifications or a part of a dialogue is non-trivial.
",4 Evaluation design,[0],[0]
"Since the candidate set includes the original question q that was asked to the post p, one possible approach to evaluation would be to look at how often the original question is ranked higher up in the ranking predicted by a model.",4 Evaluation design,[0],[0]
"However, there are two problems to this approach: 1) Our dataset creation process is noisy.",4 Evaluation design,[0],[0]
The original question paired with the post may not be a useful question.,4 Evaluation design,[0],[0]
"For e.g. “are you seriously asking this question?”, “do you mind making that an answer?”9.",4 Evaluation design,[0],[0]
2),4 Evaluation design,[0],[0]
The nine other questions in the candidate set are obtained by looking at questions asked to posts that are similar to the given post.10 This greatly increases the possibility of some other question(s) being more useful than the original question paired with the post.,4 Evaluation design,[0],[0]
This motivates an evaluation design that does not rely solely on the original question but also uses human judgments.,4 Evaluation design,[0],[0]
"We randomly choose a total of 500 examples from the test sets of the three domains proportional to their train set sizes (askubuntu:160, unix:90 and superuser:250) to construct our evaluation set.",4 Evaluation design,[0],[0]
"Due to the technical nature of the posts in our dataset, identifying useful questions requires technical experts.",4.1 Annotation scheme,[0],[0]
"We recruit 10 such experts on Upwork11 who have prior experience in unix based operating system administration.12 We provide the annotators with a post and a randomized list of the ten question candidates obtained using Lucene (§2.1) and ask them to select a single “best” (B) question to ask, and additionally mark as “valid” (V ) other questions that they thought would be okay to ask in the context of the original post.",4.1 Annotation scheme,[0],[0]
We enforce that the “best” question be always marked as a “valid” question.,4.1 Annotation scheme,[0],[0]
We group the 10 annotators into 5 pairs and assign the same 100 examples to the two annotators in a pair.,4.1 Annotation scheme,[0],[0]
We calculate the inter-annotator agreement on the “best” and the “valid” annotations using Cohen’s Kappa measurement.,4.2 Annotation analysis,[0],[0]
"When calculating the agreement on the “best” in the strict sense, we get a low
9Data analysis included in the supplementary material suggests 9% of the questions are not useful.
",4.2 Annotation analysis,[0],[0]
"10Note that this setting is different from the distractorbased setting popularly used in dialogue (Lowe et al., 2015) where the distractor candidates are chosen randomly from the corpus.
11https://upwork.com 12Details in the supplementary material.
agreement of 0.15.",4.2 Annotation analysis,[0],[0]
"However, when we relax this to a case where the question marked as“best” by one annotator is marked as “valid” by another, we get an agreement of 0.87.",4.2 Annotation analysis,[0],[0]
"The agreement on the “valid” annotations, on the other hand, was higher: 0.58.",4.2 Annotation analysis,[0],[0]
"We calculate this agreement on the binary judgment of whether a question was marked as valid by the annotator.
",4.2 Annotation analysis,[0],[0]
"Given these annotations, we calculate how often is the original question marked as “best” or “valid” by the two annotators.",4.2 Annotation analysis,[0],[0]
"We find that 72% of the time one of the annotators mark the original as the “best”, whereas only 20% of the time both annotators mark it as the “best” suggesting against an evaluation solely based on the original question.",4.2 Annotation analysis,[0],[0]
"On the other hand, 88% of the time one of the two annotators mark it as a “valid” question confirming the noise in our training data.13
Figure 4 shows the distribution of the counts of questions in the intersection of “valid” annotations (blue legend).",4.2 Annotation analysis,[0],[0]
We see that about 85% of the posts have more than 2 valid questions and 50% have more than 3 valid questions.,4.2 Annotation analysis,[0],[0]
The figure also shows the distribution of the counts when the original question is removed from the intersection (red legend).,4.2 Annotation analysis,[0],[0]
"Even in this set, we find that about 60% of the posts have more than two valid questions.",4.2 Annotation analysis,[0],[0]
These numbers suggests that the candidate set of questions retrieved using Lucene (§2.1) very often contains useful clarification questions.,4.2 Annotation analysis,[0],[0]
"Our primary research questions that we evaluate experimentally are:
1.",5 Experimental results,[0],[0]
"Does a neural network architecture improve upon non-neural baselines?
1376% of the time both the annotators mark it as a “valid”.
2.",5 Experimental results,[0],[0]
Does the EVPI formalism provide leverage over a similarly expressive feedforward network?,5 Experimental results,[0],[0]
3. Are answers useful in identifying the right question?,5 Experimental results,[0],[0]
4.,5 Experimental results,[0],[0]
How do the models perform when evaluated on the candidate questions excluding the original?,5 Experimental results,[0],[0]
"We compare our model with following baselines:
Random:",5.1 Baseline methods,[0],[0]
"Given a post, we randomly permute its set of 10 candidate questions uniformly.14
Bag-of-ngrams:",5.1 Baseline methods,[0],[0]
"Given a post and a set of 10 question and answer candidates, we construct a bag-of-ngrams representation for the post, question and answer.",5.1 Baseline methods,[0],[0]
"We train the baseline on all the positive and negative candidate triples (same as in our utility calculator (§2.3)) to minimize hinge loss on misclassification error using cross-product features between each of (p, q), (q, a) and (p, a).",5.1 Baseline methods,[0],[0]
We tune the ngram length and choose n=3 which performs best on the tune set.,5.1 Baseline methods,[0],[0]
"The question candidates are finally ranked according to their predictions for the positive label.
",5.1 Baseline methods,[0],[0]
"Community QA: The recent SemEval2017 Community Question-Answering (CQA) (Nakov et al., 2017) included a subtask for ranking a set of comments according to their relevance to a given post in the Qatar Living15 forum.",5.1 Baseline methods,[0],[0]
"Nandi et al. (2017), winners of this subtask, developed a logistic regression model using features based on
14We take the average over 1000 random permutations.",5.1 Baseline methods,[0],[0]
"15http://www.qatarliving.com/forum
string similarity, word embeddings, etc.",5.1 Baseline methods,[0],[0]
"We train this model on all the positively and negatively labelled (p, q) pairs in our dataset (same as in our utility calculator (§2.3), but without a).",5.1 Baseline methods,[0],[0]
"We use a subset of their features relevant to our task.16
Neural baselines: We construct the following neural baselines based on the LSTM representation of their inputs (as described in §2.4): 1. Neural(p, q):",5.1 Baseline methods,[0],[0]
"Input is concatenation of p̄ and q̄. 2. Neural(p, a): Input is concatenation of p̄ and ā. 3. Neural(p, q, a): Input is concatenation of p̄, q̄ and ā.
",5.1 Baseline methods,[0],[0]
"Given these inputs, we construct a fully connected feedforward neural network with 10 hidden layers and train it to minimize the binary cross entropy across all positive and negative candidate triples (same as in our utility calculator (§ 2.3)).",5.1 Baseline methods,[0],[0]
"The major difference between the neural baselines and our EVPI model is in the loss function: the EVPI model is trained to minimize the joint loss between the answer model (defined on Fans(p, q) in Eq 3) and the utility calculator (defined on Futil(p, q, a) in Eq 4) whereas the neural baselines are trained to minimize the loss directly on F (p, q), F (p, a) or F (p, q, a).",5.1 Baseline methods,[0],[0]
We include the implementation details of all our neural models in the supplementary material.,5.1 Baseline methods,[0],[0]
We first describe the results of the different models when evaluated against the expert annotations we collect on 500 samples (§4).,5.2.1 Evaluating against expert annotations,[0],[0]
"Since the annotators
16Details in the supplementary material.
had a low agreement on a single best, we evaluate against the union of the “best” annotations (B1 ∪ B2 in Table 2) and against the intersection of the “valid” annotations (V 1 ∩ V 2 in Table 2).
",5.2.1 Evaluating against expert annotations,[0],[0]
"Among non-neural baselines, we find that the bag-of-ngrams baseline performs slightly better than random but worse than all the other models.",5.2.1 Evaluating against expert annotations,[0],[0]
"The Community QA baseline, on the other hand, performs better than the neural baseline (Neural (p, q)), both of which are trained without using the answers.",5.2.1 Evaluating against expert annotations,[0],[0]
"The neural baselines with answers (Neural(p, q, a) and Neural(p, a)) outperform the neural baseline without answers (Neural(p, q)), showing that answer helps in selecting the right question.
",5.2.1 Evaluating against expert annotations,[0],[0]
"More importantly, EVPI outperforms the Neural (p, q, a) baseline across most metrics.",5.2.1 Evaluating against expert annotations,[0],[0]
Both models use the same information regarding the true question and answer and are trained using the same number of model parameters.17,5.2.1 Evaluating against expert annotations,[0],[0]
"However, the EVPI model, unlike the neural baseline, additionally makes use of alternate question and answer candidates to compute its loss function.",5.2.1 Evaluating against expert annotations,[0],[0]
"This shows that when the candidate set consists of questions similar to the original question, summing over their utilities gives us a boost.",5.2.1 Evaluating against expert annotations,[0],[0]
The last column in Table 2 shows the results when evaluated against the original question paired with the post.,5.2.2 Evaluating against the original question,[0],[0]
"The bag-of-ngrams baseline performs similar to random, unlike when evaluated against human judgments.",5.2.2 Evaluating against the original question,[0],[0]
"The Community QA baseline again outperforms Neural(p, q) model and comes very close to the Neural (p, a) model.
",5.2.2 Evaluating against the original question,[0],[0]
"As before, the neural baselines that make use of the answer outperform the one that does not use the answer and the EVPI model performs significantly better than Neural(p, q, a).",5.2.2 Evaluating against the original question,[0],[0]
"In the preceding analysis, we considered a setting in which the “ground truth” original question was in the candidate set Q. While this is a common evaluation framework in dialog response selection (Lowe et al., 2015), it is overly optimistic.",5.2.3 Excluding the original question,[0],[0]
"We, therefore, evaluate against the “best” and the “valid” annotations on the nine other question candidates.",5.2.3 Excluding the original question,[0],[0]
"We find that the neural models beat the
17We use 10 hidden layers in the feedforward network of the neural baseline and five hidden layers each in the two feedforward networks Fans and Futil of the EVPI model.
non-neural baselines.",5.2.3 Excluding the original question,[0],[0]
"However, the differences between all the neural models are statistically insignificant.18",5.2.3 Excluding the original question,[0],[0]
"Most prior work on question generation has focused on generating reading comprehension questions: given text, write questions that one might find on a standardized test (Vanderwende, 2008; Heilman, 2011; Rus et al., 2011; Olney et al., 2012).",6 Related work,[0],[0]
"Comprehension questions, by definition, are answerable from the provided text.",6 Related work,[0],[0]
"Clarification questions–our interest–are not.
",6 Related work,[0],[0]
"Outside reading comprehension questions, Labutov et al. (2015) generate high-level question templates by crowdsourcing which leads to significantly less data than we collect using our method.",6 Related work,[0],[0]
Liu et al. (2010) use template question generation to help authors write better related work sections.,6 Related work,[0],[0]
"Mostafazadeh et al. (2016) introduce a Visual Question Generation task where the goal is to generate natural questions that are not about what is present in the image rather about what can be inferred given the image, somewhat analogous to clarification questions.",6 Related work,[0],[0]
"Penas and Hovy (2010) identify the notion of missing information similar to us, but they fill the knowledge gaps in a text with the help of external knowledge bases, whereas we instead ask clarification questions.",6 Related work,[0],[0]
"Artzi and Zettlemoyer (2011) use human-generated clarification questions to drive a semantic parser where the clarification questions are aimed towards simplifying a user query; whereas we generate clarification questions aimed at identifying missing information in a text.
",6 Related work,[0],[0]
"Among works that use community question answer forums, the keywords to questions (K2Q) system (Zheng et al., 2011) generates a list of candidate questions and refinement words, given a set of input keywords, to help a user ask a better question.",6 Related work,[0],[0]
Figueroa and Neumann (2013) rank different paraphrases of query for effective search on forums.,6 Related work,[0],[0]
"(Romeo et al., 2016) develop a neural network based model for ranking questions on forums with the intent of retrieving similar other question.",6 Related work,[0],[0]
"The recent SemEval-2017 Community QuestionAnswering (CQA) (Nakov et al., 2017) task included a subtask to rank the comments according to their relevance to the post.",6 Related work,[0],[0]
"Our task primarily differs from this task in that we want to identify a
18Results included in the supplementary material.
question comment which is not only relevant to the post but will also elicit useful information missing from the post.",6 Related work,[0],[0]
Hoogeveen et al. (2015) created the CQADupStack dataset using StackExchange forums for the task of duplicate question retrieval.,6 Related work,[0],[0]
"Our dataset, on the other hand, is designed for the task of ranking clarification questions asked as comments to a post.",6 Related work,[0],[0]
"We have constructed a new dataset for learning to rank clarification questions, and proposed a novel model for solving this task.",7 Conclusion,[0],[0]
"Our model integrates well-known deep network architectures with the classic notion of expected value of perfect information, which effectively models a pragmatic choice on the part of the questioner: how do I imagine the other party would answer if I were to ask this question.",7 Conclusion,[0],[0]
"Such pragmatic principles have recently been shown to be useful in other tasks as well (Golland et al., 2010; Smith et al., 2013; Orita et al., 2015; Andreas and Klein, 2016).",7 Conclusion,[0],[0]
"One can naturally extend our EVPI approach to a full reinforcement learning approach to handle multi-turn conversations.
",7 Conclusion,[0],[0]
Our results shows that the EVPI model is a promising formalism for the question generation task.,7 Conclusion,[0],[0]
"In order to move to a full system that can help users like Terry write better posts, there are three interesting lines of future work.",7 Conclusion,[0],[0]
"First, we need it to be able to generalize: for instance by constructing templates of the form “What version of are you running?” into which the system would need to fill a variable.",7 Conclusion,[0],[0]
"Second, in order to move from question ranking to question generation, one could consider sequence-to-sequence based neural network models that have recently proven to be effective for several language generation tasks (Sutskever et al., 2014; Serban et al., 2016; Yin et al., 2016).",7 Conclusion,[0],[0]
"Third is in evaluation: given that this task requires expert human annotations and also given that there are multiple possible good questions to ask, how can we automatically measure performance at this task?, a question faced in dialog and generation more broadly (Paek, 2001; Lowe et al., 2015; Liu et al., 2016).",7 Conclusion,[0],[0]
"The authors thank the three anonymous reviewers of this paper, and the anonymous reviewers of the previous versions for their helpful comments and
suggestions.",Acknowledgments,[0],[0]
"They also thank the members of the Computational Linguistics and Information Processing (CLIP) lab at University of Maryland for helpful discussions.
",Acknowledgments,[0],[0]
This work was supported by NSF grant IIS1618193.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.",Acknowledgments,[0],[0]
"Inquiry is fundamental to communication, and machines cannot effectively collaborate with humans unless they can ask questions.",abstractText,[0],[0]
"In this work, we build a neural network model for the task of ranking clarification questions.",abstractText,[0],[0]
Our model is inspired by the idea of expected value of perfect information: a good question is one whose expected answer will be useful.,abstractText,[0],[0]
"We study this problem using data from StackExchange, a plentiful online resource in which people routinely ask clarifying questions to posts so that they can better offer assistance to the original poster.",abstractText,[0],[0]
"We create a dataset of clarification questions consisting of ∼77K posts paired with a clarification question (and answer) from three domains of StackExchange: askubuntu, unix and superuser.",abstractText,[0],[0]
We evaluate our model on 500 samples of this dataset against expert human judgments and demonstrate significant improvements over controlled baselines.,abstractText,[0],[0]
Learning to Ask Good Questions: Ranking Clarification Questions using Neural Expected Value of Perfect Information,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2193–2203 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2193",text,[0],[0]
"Learning to ask questions (or, question generation) aims to generate a question to a given input.",1 Introduction,[0],[0]
"Deciding what to ask and how is an indicator of machine understanding (Mostafazadeh et al., 2016), as demonstrated in machine comprehension (Du et al., 2017; Zhou et al., 2017b; Yuan et al., 2017) and question answering (Tang et al., 2017; Wang et al., 2017).",1 Introduction,[0],[0]
"Raising good questions is essential to conversational systems because a good system can well interact with users by asking and responding (Li et al., 2016).",1 Introduction,[0],[0]
"Furthermore, asking
∗Authors contributed equally to this work.",1 Introduction,[0],[0]
"†Corresponding author: Minlie Huang.
",1 Introduction,[0],[0]
"questions is one of the important proactive behaviors that can drive dialogues to go deeper and further (Yu et al., 2016).
",1 Introduction,[0],[0]
Question generation (QG) in open-domain conversational systems differs substantially from the traditional QG tasks.,1 Introduction,[0],[0]
"The ultimate goal of this task is to enhance the interactiveness and persistence of human-machine interactions, while for traditional QG tasks, seeking information through a generated question is the major purpose.",1 Introduction,[0],[0]
"The response to a generated question will be supplied in the following conversations, which may be novel but not necessarily occur in the input as that in traditional QG (Du et al., 2017; Yuan et al., 2017; Tang et al., 2017; Wang et al., 2017; Mostafazadeh et al., 2016).",1 Introduction,[0],[0]
"Thus, the purpose of this task is to spark novel yet related information to drive the interactions to continue.
",1 Introduction,[0],[0]
"Due to the different purposes, this task is unique in two aspects: it requires to question not only in various patterns but also about diverse yet relevant topics.",1 Introduction,[0],[0]
"First, there are various questioning patterns for the same input, such as Yes-no questions and Wh-questions with different interrogatives.",1 Introduction,[0],[0]
Diversified questioning patterns make dialogue interactions richer and more flexible.,1 Introduction,[0],[0]
"Instead, traditional QG tasks can be roughly addressed by syntactic transformation (Andrenucci and Sneiders, 2005; Popowich and Winne, 2013), or implicitly modeled by neural models (Du et al., 2017).",1 Introduction,[0],[0]
"In such tasks, the information questioned on is pre-specified and usually determines the pattern of questioning.",1 Introduction,[0],[0]
"For instance, asking Whoquestion for a given person, or Where-question for a given location.
",1 Introduction,[0],[0]
"Second, this task requires to address much more transitional topics of a given input, which is the nature of conversational systems.",1 Introduction,[0],[0]
"For instance, for the input “I went to dinner with my friends”, we may question about topics such as friend, cuisine,
price, place and taste.",1 Introduction,[0],[0]
"Thus, this task generally requires scene understanding to imagine and comprehend a scenario (e.g., dining at a restaurant) that can be interpreted by topics related to the input.",1 Introduction,[0],[0]
"However, in traditional QG tasks, the core information to be questioned on is pre-specified and rather static, and paraphrasing is more required.
Undoubtedly, asking good questions in conversational systems needs to address the above issues (questioning with diversified patterns, and addressing transitional topics naturally in a generated question).",1 Introduction,[0],[0]
"As shown in Figure 1, a good question is a natural composition of interrogatives, topic words, and ordinary words.",1 Introduction,[0],[0]
"Interrogatives indicate the pattern of questioning, topic words address the key information of topic transition, and ordinary words play syntactical and grammatical roles in making a natural sentence.
",1 Introduction,[0],[0]
"We thus classify the words in a question into three types: interrogative, topic word, and ordinary word automatically.",1 Introduction,[0],[0]
"We then devise two decoders, Soft Typed Decoder (STD) and Hard Typed Decoder (HTD), for question generation in conversational systems1.",1 Introduction,[0],[0]
"STD deals with word types in a latent and implicit manner, while HTD in a more explicit way.",1 Introduction,[0],[0]
"At each decoding position, we firstly estimate a type distribution over word types.",1 Introduction,[0],[0]
STD applies a mixture of type-specific generation distributions where type probabilities are the coefficients.,1 Introduction,[0],[0]
"By contrast, HTD reshapes the type distribution by Gumbel-softmax and modulates the generation distribution by type probabilities.",1 Introduction,[0],[0]
"Our contributions are as follows:
• To the best of our knowledge, this is the first study on question generation in the setting of
1To simplify the task, as a preliminary research, we consider the one-round conversational system.
conversational systems.",1 Introduction,[0],[0]
"We analyze the key differences between this new task and other traditional question generation tasks.
",1 Introduction,[0],[0]
• We devise soft and hard typed decoders to ask good questions by capturing different roles of different word types.,1 Introduction,[0],[0]
Such typed decoders may be applicable to other generation tasks if word semantic types can be identified.,1 Introduction,[0],[0]
"Traditional question generation can be seen in task-oriented dialogue system (Curto et al., 2012), sentence transformation (Vanderwende, 2008), machine comprehension (Du et al., 2017; Zhou et al., 2017b; Yuan et al., 2017; Subramanian et al., 2017), question answering (Qin, 2015; Tang et al., 2017; Wang et al., 2017; Song et al., 2017), and visual question answering (Mostafazadeh et al., 2016).",2 Related Work,[0],[0]
"In such tasks, the answer is known and is part of the input to the generated question.",2 Related Work,[0],[0]
"Meanwhile, the generation tasks are not required to predict additional topics since all the information has been provided in the input.",2 Related Work,[0],[0]
"They are applicable in scenarios such as designing questions for reading comprehension (Du et al., 2017; Zhou et al., 2017a; Yuan et al., 2017), and justifying the visual understanding by generating questions to a given image (video) (Mostafazadeh et al., 2016).
",2 Related Work,[0],[0]
"In general, traditional QG tasks can be addressed by the heuristic rule-based reordering methods (Andrenucci and Sneiders, 2005; Ali et al., 2010; Heilman and Smith, 2010), slotfilling with question templates (Popowich and Winne, 2013; Chali and Golestanirad, 2016; Labutov et al., 2015), or implicitly modeled by recent neural models(Du et al., 2017; Zhou et al., 2017b; Yuan et al., 2017; Song et al., 2017; Subramanian et al., 2017).",2 Related Work,[0],[0]
"These tasks generally do not require to generate a question with various patterns: for a given answer and a supporting text, the question type is usually decided by the input.
",2 Related Work,[0],[0]
"Question generation in large-scale, opendomain dialogue systems is relatively unexplored.",2 Related Work,[0],[0]
Li et al. (2016) showed that asking questions in task-oriented dialogues can offer useful feedback to facilitate learning through interactions.,2 Related Work,[0],[0]
"Several questioning mechanisms were devised with handcrafted templates, but unfortunately not applicable to open-domain conversational systems.",2 Related Work,[0],[0]
"Similar to our goal, a visual QG task is proposed to generate a question to interact with other people, given
an image as input (Mostafazadeh et al., 2016).",2 Related Work,[0],[0]
"The task of question generation in conversational systems can be formalized as follows: given a user post X = x1x2 · · ·xm, the system should generate a natural and meaningful question Y = y1y2 · · · yn to interact with the user, formally as
Y ∗ =",3.1 Overview,[0],[0]
"argmax Y P(Y |X).
",3.1 Overview,[0],[0]
"As aforementioned, asking good questions in conversational systems requires to question with diversified patterns and address transitional topics naturally in a question.",3.1 Overview,[0],[0]
"To this end, we classify the words in a sentence into three types: interrogative, topic word, and ordinary word, as shown in Figure 1.",3.1 Overview,[0],[0]
"During training, the type of each word in a question is decided automatically2.",3.1 Overview,[0],[0]
We manually collected about 20 interrogatives.,3.1 Overview,[0],[0]
"The verbs and nouns in a question are treated as topic words, and all the other words as ordinary words.",3.1 Overview,[0],[0]
"During test, we resort to PMI (Church and Hanks, 1990) to predict a few topic words for a given post.
",3.1 Overview,[0],[0]
"On top of an encoder-decoder framework, we propose two decoders to effectively use word types in question generation.",3.1 Overview,[0],[0]
The first model is soft typed decoder (STD).,3.1 Overview,[0],[0]
"It estimates a type distribution over word types and three type-specific generation distributions over the vocabulary, and then obtains a mixture of type-specific distributions for word generation.
",3.1 Overview,[0],[0]
"The second one is a hard form of STD, hard typed decoder (HTD), in which we can control the decoding process more explicitly by approximating the operation of argmax with Gumbel-softmax (Jang et al., 2016).",3.1 Overview,[0],[0]
"In both decoders, the final generation probability of a word is modulated by its word type.",3.1 Overview,[0],[0]
"Our model is based on the general encoderdecoder framework (Cho et al., 2014; Sutskever et al., 2014).",3.2 Encoder-Decoder Framework,[0],[0]
"Formally, the model encodes an input sequence X = x1x2 · · ·xm into a sequence of hidden states hi, as follows,
ht = GRU(ht−1, e(xt)), 2Though there may be errors in word type classification,
we found it works well in response generation.
where GRU denotes gated recurrent units (Cho et al., 2014), and e(x) is the word vector of word x.",3.2 Encoder-Decoder Framework,[0],[0]
"The decoder generates a word sequence by sampling from the probability P(yt|y<t, X) (y<t = y1y2 · · · yt−1, the generated subsequence) which can be computed via
P(yt|y<t, X) = MLP(st, e(yt−1), ct), st = GRU(st−1, e(yt−1), ct),
where st is the state of the decoder at the time step t, and this GRU has different parameters with the one of the encoder.",3.2 Encoder-Decoder Framework,[0],[0]
The context vector ct is an attentive read of the hidden states of the encoder as ct = ∑T i=1,3.2 Encoder-Decoder Framework,[0],[0]
"αt,ihi, where the weight αt,i is scored by another MLP(st−1, hi) network.",3.2 Encoder-Decoder Framework,[0],[0]
"In a general encoder-decoder model, the decoder tends to generate universal, meaningless questions like “What’s up?” and “So what?”.",3.3 Soft Typed Decoder (STD),[0],[0]
"In order to generate more meaningful questions, we propose a soft typed decoder.",3.3 Soft Typed Decoder (STD),[0],[0]
"It assumes that each word has a latent type among the set {interrogative, topic word, ordinary word}.",3.3 Soft Typed Decoder (STD),[0],[0]
"The soft typed decoder firstly estimates a word type distribution over latent types in the given context, and then computes type-specific generation distributions over the entire vocabulary for different word types.",3.3 Soft Typed Decoder (STD),[0],[0]
"The final probability of generating a word is a mixture of type-specific generation distributions where the coefficients are type probabilities.
",3.3 Soft Typed Decoder (STD),[0],[0]
"The final generation distribution P(yt|y<t, X) from which a word can be sampled, is given by
P(yt|y<t, X) = k∑
i=1
P(yt|tyt = ci, y<t, X) ·",3.3 Soft Typed Decoder (STD),[0],[0]
"P(tyt = ci|y<t, X), (1)
where tyt denotes the word type at time step t and ci is a word type.",3.3 Soft Typed Decoder (STD),[0],[0]
"Apparently, this formulation states that the final generation probability is a mixture of the type-specific generation probabilities P(yt|tyt = ci, y<t, X), weighted by the probability of the type distributionP(tyt = ci|y<t, X).",3.3 Soft Typed Decoder (STD),[0],[0]
We name this decoder as soft typed decoder.,3.3 Soft Typed Decoder (STD),[0],[0]
"In this model, word type is latent because we do not need to specify the type of a word explicitly.",3.3 Soft Typed Decoder (STD),[0],[0]
"In other words, each word can belong to any of the three types, but with different probabilities given the current context.
",3.3 Soft Typed Decoder (STD),[0],[0]
"The probability distribution over word types C = {c1, c2, · · · , ck} (k = 3 in this paper) (termed
as type distribution) is given by
P(tyt|y<t, X) = softmax(W0st + b0), (2)
where st is the hidden state of the decoder at time step t, W0 ∈ Rk×d, and d is the dimension of the hidden state.
",3.3 Soft Typed Decoder (STD),[0],[0]
"The type-specific generation distribution is given by
P(yt|tyt = ci, y<t, X) =",3.3 Soft Typed Decoder (STD),[0],[0]
"softmax(Wcist + bci),
where Wci ∈ R|V |×d and |V | is the size of the entire vocabulary.",3.3 Soft Typed Decoder (STD),[0],[0]
"Note that the type-specific generation distribution is parameterized by Wci , indicating that the distribution for each word type has its own parameters.
",3.3 Soft Typed Decoder (STD),[0],[0]
"Instead of using a single distribution P(yt|y<t, X) as in a general Seq2Seq decoder, our soft typed decoder enriches the model by applying multiple type-specific generation distributions.",3.3 Soft Typed Decoder (STD),[0],[0]
This enables the model to express more information about the next word to be generated.,3.3 Soft Typed Decoder (STD),[0],[0]
"Also note that the generation distribution is over the same vocabulary, and therefore there is no need to specify word types explicitly.",3.3 Soft Typed Decoder (STD),[0],[0]
"In the soft typed decoder, we assume that each word is a distribution over the word types.",3.4 Hard Typed Decoder (HTD),[0],[0]
"In this sense, the type of a word is implicit.",3.4 Hard Typed Decoder (HTD),[0],[0]
We do not need to specify the type of each word explicitly.,3.4 Hard Typed Decoder (HTD),[0],[0]
"In the hard typed decoder, words in the entire vocabulary are dynamically classified into three types for each post, and the decoder first estimates a type distribution at each position and then generates a word with the highest type probability.",3.4 Hard Typed Decoder (HTD),[0],[0]
"This pro-
cess can be formulated as follows:
c∗ = arg max ci P(tyt = ci|y<t, X), (3) P(yt|y<t, X) =",3.4 Hard Typed Decoder (HTD),[0],[0]
"P(yt|tyt = c∗, y<t, X).",3.4 Hard Typed Decoder (HTD),[0],[0]
"(4)
This is essentially the hard form of Eq. 1, which just selects the type with the maximal probability.",3.4 Hard Typed Decoder (HTD),[0],[0]
"However, this argmax process may cause two problems.",3.4 Hard Typed Decoder (HTD),[0],[0]
"First, such a cascaded decision process (firstly selecting the most probable word type and secondly choosing a word from that type) may lead to severe grammatical errors if the first selection is wrong.",3.4 Hard Typed Decoder (HTD),[0],[0]
"Second, argmax is discrete and nondifferentiable, and it breaks the back-propagation path during training.
",3.4 Hard Typed Decoder (HTD),[0],[0]
"To make best use of word types in hard typed decoder, we address the above issues by applying Gumbel-Softmax (Jang et al., 2016) to approximate the operation of argmax.",3.4 Hard Typed Decoder (HTD),[0],[0]
"There are several steps in the decoder (see Figure 2):
First, the type of each word (interrogative, topic, or ordinary) in a question is decided automatically during training, as aforementioned.
",3.4 Hard Typed Decoder (HTD),[0],[0]
"Second, the generation probability distribution is estimated as usual,
P(yt|y<t, X) = softmax(W0st + b0).",3.4 Hard Typed Decoder (HTD),[0],[0]
"(5)
Further, the type probability distribution at each decoding position is estimated as follows,
P(tyt|y<t, X) = softmax(W1st + b1).",3.4 Hard Typed Decoder (HTD),[0],[0]
"(6)
Third, the generation probability for each word is modulated by its corresponding type probabil-
ity:
P ′(yt|y<t, X) = P(yt|y<t, X)·m(yt),
m(yt) =
{ 1 , c(yt) =",3.4 Hard Typed Decoder (HTD),[0],[0]
"c ∗
0 , c(yt) 6=",3.4 Hard Typed Decoder (HTD),[0],[0]
"c∗ (7)
where c(yt) looks up the word type of word yt, and c∗ is the type with the highest probability as defined in Eq. 3.",3.4 Hard Typed Decoder (HTD),[0],[0]
"This formulation has exactly the effect of argmax, where the decoder will only generate words of type with the highest probability.
",3.4 Hard Typed Decoder (HTD),[0],[0]
"To make P∗(yt|y<t, X) a distribution, we normalize these values by a normalization factor Z:
Z = 1∑
yt∈V P ′(yt|y<t, X) where V is the decoding vocabulary.",3.4 Hard Typed Decoder (HTD),[0],[0]
"Then, the final probability can be denoted by
P∗(yt|y<t, X) = Z · P ′(yt|y<t, X).",3.4 Hard Typed Decoder (HTD),[0],[0]
"(8)
As mentioned, in order to have an effect of argmax but still maintain the differentiability, we resort to Gumbel-Softmax (Jang et al., 2016), which is a differentiable surrogate to the argmax function.",3.4 Hard Typed Decoder (HTD),[0],[0]
"The type probability distribution is then adjusted to the following form:
m(yt) = GS(P(tyt = c(yt)|y<t, X)), GS(πi) =",3.4 Hard Typed Decoder (HTD),[0],[0]
"e(log(πi)+gi)/τ∑k j=1 e (log(πj)+gj)/τ , (9)
where π1, π2, · · · , πk represents the probabilities of the original categorical distribution, gj are i.i.d samples drawn from Gumbel(0,1)3 and τ is a constant that controls the smoothness of the distribution.",3.4 Hard Typed Decoder (HTD),[0],[0]
"When τ → 0, Gumbel-Softmax performs like argmax, while if τ → ∞, Gumbel-Softmax performs like a uniform distribution.",3.4 Hard Typed Decoder (HTD),[0],[0]
"In our experiments, we set τ a constant between 0 and 1, making Gumbel-Softmax smoother than argmax, but sharper than normal softmax.
",3.4 Hard Typed Decoder (HTD),[0],[0]
"Note that in HTD, we apply dynamic vocabularies for different responses during training.",3.4 Hard Typed Decoder (HTD),[0],[0]
The words in a response are classified into the three types dynamically.,3.4 Hard Typed Decoder (HTD),[0],[0]
A specific type probability will only affect the words of that type.,3.4 Hard Typed Decoder (HTD),[0],[0]
"During test, for each post, topic words are predicted with PMI, interrogatives are picked from a small dictionary, and the rest of words in the vocabulary are treated as ordinary words.
",3.4 Hard Typed Decoder (HTD),[0],[0]
"3If u ∼ Uniform(0, 1), then g = −log(−log(u))",3.4 Hard Typed Decoder (HTD),[0],[0]
"∼ Gumbel(0, 1).",3.4 Hard Typed Decoder (HTD),[0],[0]
"We adopt negative data likelihood (equivalent to cross entropy) as the loss function, and additionally, we apply supervision on the mixture weights of word types, formally as follows:
Φ1 = ∑",3.5 Loss Function,[0],[0]
t,3.5 Loss Function,[0],[0]
"− logP(yt = ỹt|y<t, X), (10)
",3.5 Loss Function,[0],[0]
"Φ2 = ∑ t − logP(tyt = t̃yt|y<t, X), (11)
",3.5 Loss Function,[0],[0]
Φ = Φ1,3.5 Loss Function,[0],[0]
"+ λΦ2, (12)
where t̃yt represents the reference word type and ỹt represents the reference word at time t. λ is a factor to balance the two loss terms, and we set λ=0.8 in our experiments.
",3.5 Loss Function,[0],[0]
"Note that for HTD, we substitute P∗(yt = wj |y<t, X) (as defined by Eq. 8) into Eq. 10.",3.5 Loss Function,[0],[0]
The only difference between training and inference is the means of choosing topic words.,3.6 Topic Word Prediction,[0],[0]
"During training, we identify the nouns and verbs in a response as topic words; whereas during inference, we adopt PMI (Church and Hanks, 1990) and Rel(ki, X) to predict a set of topic words ki for an input post X , as defined below:
PMI(wx, wy) = log p(wx, wy)
p1(wx) ∗ p2(wy) , Rel(ki, X) = ∑ wx∈X ePMI(wx,ki),
where p1(w)/p2(w) represent the probability of word w occurring in a post/response, respectively, and p(wx, wy) is the probability of word wx occurring in a post and wy in a response.
",3.6 Topic Word Prediction,[0],[0]
"During inference, we predict at most 20 topic words for an input post.",3.6 Topic Word Prediction,[0],[0]
"Too few words will affect the grammaticality since the predicted set contains infrequent topic words, while too many words introduce more common topics leading to more general responses.",3.6 Topic Word Prediction,[0],[0]
"To estimate the probabilities in PMI, we collected about 9 million post-response pairs from Weibo.",4.1 Dataset,[0],[0]
"To train our question generation models, we distilled the pairs whereby the responses are in question form with the help of around 20 hand-crafted
templates.",4.1 Dataset,[0],[0]
The templates contain a list of interrogatives and other implicit questioning patterns.,4.1 Dataset,[0],[0]
"Such patterns detect sentences led by words like what, how many, how about or sentences ended with a question mark.",4.1 Dataset,[0],[0]
"After that, we removed the pairs whose responses are universal questions that can be used to reply many different posts.",4.1 Dataset,[0],[0]
"This is a simple yet effective way to avoid situations where the type probability distribution is dominated by interrogatives and ordinary words.
",4.1 Dataset,[0],[0]
"Ultimately, we obtained the dataset comprising about 491,000 post-response pairs.",4.1 Dataset,[0],[0]
"We randomly selected 5,000 pairs for testing and another 5,000 for validation.",4.1 Dataset,[0],[0]
The average number of words in post/response is 8.3/9.3 respectively.,4.1 Dataset,[0],[0]
"The dataset contains 66,547 different words, and 18,717 words appear more than 10 times.",4.1 Dataset,[0],[0]
The dataset is available at: http://coai.cs.tsinghua.edu. cn/hml/dataset/.,4.1 Dataset,[0],[0]
We compared the proposed decoders with four state-of-the-art baselines.,4.2 Baselines,[0],[0]
Seq2Seq:,4.2 Baselines,[0],[0]
"A simple encoder-decoder with attention mechanisms (Luong et al., 2015).",4.2 Baselines,[0],[0]
MA:,4.2 Baselines,[0],[0]
"The mechanism-aware (MA) model applies multiple responding mechanisms represented by real-valued vectors (Zhou et al., 2017a).",4.2 Baselines,[0],[0]
The number of mechanisms is set to 4 and we randomly picked one response from the generated responses for evaluation to avoid selection bias.,4.2 Baselines,[0],[0]
"TA: The topic-aware (TA) model generates informative responses by incorporating topic words predicted from the input post (Xing et al., 2017).",4.2 Baselines,[0],[0]
"ERM: Elastic responding machine (ERM) adaptively selects a subset of responding mechanisms using reinforcement learning (Zhou et al., 2018a).",4.2 Baselines,[0],[0]
The settings are the same as the original paper.,4.2 Baselines,[0],[0]
"Parameters were set as follows: we set the vocabulary size to 20, 000 and the dimension of word vectors as 100.",4.3 Experiment Settings,[0],[0]
The word vectors were pretrained with around 9 million post-response pairs from Weibo and were being updated during the training of the decoders.,4.3 Experiment Settings,[0],[0]
We applied the 4-layer GRU units (hidden states have 512 dimensions).,4.3 Experiment Settings,[0],[0]
These settings were also applied to all the baselines.,4.3 Experiment Settings,[0],[0]
λ in Eq. 12 is 0.8.,4.3 Experiment Settings,[0],[0]
We set different values of τ in Gumbel-softmax at different stages of training.,4.3 Experiment Settings,[0],[0]
"At the early stage, we set τ to a small value (0.6) to obtain a sharper reformed distri-
bution (more like argmax).",4.3 Experiment Settings,[0],[0]
"After several steps, we set τ to a larger value (0.8) to apply a more smoothing distribution.",4.3 Experiment Settings,[0],[0]
Our codes are available at: https://github.com/victorywys/ Learning2Ask_TypedDecoder.,4.3 Experiment Settings,[0],[0]
"We conducted automatic evaluation over the 5, 000 test posts.",4.4 Automatic Evaluation,[0],[0]
"For each post, we obtained responses from the six models, and there are 30, 000 post-response pairs in total.",4.4 Automatic Evaluation,[0],[0]
We adopted perplexity to quantify how well a model fits the data.,4.4.1 Evaluation Metrics,[0],[0]
Smaller values indicate better performance.,4.4.1 Evaluation Metrics,[0],[0]
"To evaluate the diversity of the responses, we employed distinct-1 and distinct-2 (Li et al., 2015).",4.4.1 Evaluation Metrics,[0],[0]
"These two metrics calculates the proportion of the total number of distinct unigrams or bigrams to the total number of generated tokens in all the generated responses.
",4.4.1 Evaluation Metrics,[0],[0]
"Further, we calculated the proportion of the responses containing at least one topic word in the list predicted by PMI.",4.4.1 Evaluation Metrics,[0],[0]
This is to evaluate the ability of addressing topic words in response.,4.4.1 Evaluation Metrics,[0],[0]
We term this metric as topical response ratio (TRR).,4.4.1 Evaluation Metrics,[0],[0]
We predicted 20 topic words with PMI for each post.,4.4.1 Evaluation Metrics,[0],[0]
Comparative results are presented in Table 1.,4.4.2 Results,[0],[0]
"STD and HTD perform fairly well with lower perplexities, higher distinct-1 and distinct-2 scores, and remarkably better topical response ratio (TRR).",4.4.2 Results,[0],[0]
"Note that MA has the lowest perplexity because the model tends to generate more universal responses.
",4.4.2 Results,[0],[0]
"Our decoders have better distinct-1 and distinct2 scores than baselines do, and HTD performs much better than the strongest baseline TA.",4.4.2 Results,[0],[0]
"Noticeably, the means of using topic information in our models differs substantially from that in TA.",4.4.2 Results,[0],[0]
"Our decoders predict whether a topic word should be decoded at each position, whereas TA takes as
input topic word embeddings at all decoding positions.
",4.4.2 Results,[0],[0]
"Our decoders have remarkably better topic response ratios (TRR), indicating that they are more likely to include topic words in generation.",4.4.2 Results,[0],[0]
We resorted to a crowdsourcing service for manual annotation.,4.5 Manual Evaluation,[0],[0]
500 posts were sampled for manual annotation4.,4.5 Manual Evaluation,[0],[0]
We conducted pair-wise comparison between two responses generated by two models for the same post.,4.5 Manual Evaluation,[0],[0]
"In total, there are 4,500 pairs to be compared.",4.5 Manual Evaluation,[0],[0]
"For each response pair, five judges were hired to give a preference between the two responses, in terms of the following three metrics.",4.5 Manual Evaluation,[0],[0]
"Tie was allowed, and system identifiers were masked during annotation.",4.5 Manual Evaluation,[0],[0]
"Each of the following metrics is evaluated independently on each pair-wise comparison: Appropriateness: measures whether a question is reasonable in logic and content, and whether it is questioning on the key information.",4.5.1 Evaluation Metrics,[0],[0]
"Inappropriate questions are either irrelevant to the post, or have grammatical errors, or universal questions.",4.5.1 Evaluation Metrics,[0],[0]
Richness: measures whether a response contains topic words that are relevant to a given post.,4.5.1 Evaluation Metrics,[0],[0]
Willingness to respond: measures whether a user will respond to a generated question.,4.5.1 Evaluation Metrics,[0],[0]
This metric is to justify how likely the generated questions can elicit further interactions.,4.5.1 Evaluation Metrics,[0],[0]
"If people are willing to respond, the interactions can go further.
",4.5.1 Evaluation Metrics,[0],[0]
"4During the sampling process, we removed those posts that are only interpretable with other context or background.",4.5.1 Evaluation Metrics,[0],[0]
The label of each pair-wise comparison is decided by majority voting from five annotators.,4.5.2 Results,[0],[0]
Results shown in Table 2 indicate that STD and HTD outperform all the baselines in terms of all the metrics.,4.5.2 Results,[0],[0]
"This demonstrates that our decoders produce more appropriate questions, with richer topics.",4.5.2 Results,[0],[0]
"Particularly, our decoders have substantially better willingness scores, indicating that questions generated by our models are more likely to elicit further interactions.",4.5.2 Results,[0],[0]
"Noticeably, HTD outperforms STD significantly, indicating that it is beneficial to specify word types explicitly and apply dynamic vocabularies in generation.
",4.5.2 Results,[0],[0]
"We also observed that STD outperforms Seq2Seq and TA, but the differences are not significant in appropriateness.",4.5.2 Results,[0],[0]
"This is because STD generated about 7% non-question responses which were judged as inappropriate, while Seq2Seq and TA generated universal questions (inappropriate too but beat STD in annotation) to these posts.",4.5.2 Results,[0],[0]
"The proportion of the pair-wise annotations in which at least three of five annotators assign the same label to a record is 90.57%/93.11%/96.62% for appropriateness/ richness/willingness, respectively.",4.5.3 Annotation Statistics,[0],[0]
The values show that we have fairly good agreements with majority voting.,4.5.3 Annotation Statistics,[0],[0]
"To analyze whether the model can question with various patterns, we manually annotated the questioning patterns of the responses to 100 sampled posts.",4.6 Questioning Pattern Distribution,[0],[0]
"The patterns are classified into 11 types including Yes-No, How-, Why-, What-, When-, and Who- questions.",4.6 Questioning Pattern Distribution,[0],[0]
"We then calculated the KL diver-
gence between the pattern type distribution by a model and that by human (i.e., gold responses).
",4.6 Questioning Pattern Distribution,[0],[0]
"Results in Table 3 show that the pattern distribution by our model is closer to that in humanwritten responses, indicating that our decoders can better learn questioning patterns from human language.",4.6 Questioning Pattern Distribution,[0],[0]
"Further investigation reveals that the baselines tend to generate simple questions like What?(什么？) or Really?(真的吗), and constantly focus on using one or two question patterns whereas our decoders use more diversified patterns as appeared in the human language.",4.6 Questioning Pattern Distribution,[0],[0]
"Table 4 presents some generated questions by our decoders, which are more appropriate.",4.7 Examples of the Generated Questions,[0],[0]
"On the contrary, Seq2Seq, MA and ERM tend to generate more universal questions.",4.7 Examples of the Generated Questions,[0],[0]
"These examples also clearly show that asking questions in open-domain conversational systems requires scene understanding, which is verified by this scene example of singing at karaoke(在卡拉ok唱歌).
",4.7 Examples of the Generated Questions,[0],[0]
"To further understand how to ask good questions in conversational systems, we presented more generated question examples by human and our typed decoders.",4.7 Examples of the Generated Questions,[0],[0]
The first example (Post-1) in Table 5 shows that a good question can be composed of only interrogatives and ordinary words.,4.7 Examples of the Generated Questions,[0],[0]
This shows the complexity of this task and also the necessity of modeling interrogatives.,4.7 Examples of the Generated Questions,[0],[0]
"Post-2 indicates a typical questioning pattern which is ques-
tioning on a particular topic word (效果-effect) of the input.",4.7 Examples of the Generated Questions,[0],[0]
"While for Post-3, the questions are asking about transitional topics of the input (上 班-work → 部门-department; 体育中心-sports center → 体育学院-college of Physical Education), indicating a typical case of topic transition in our task (also seen in Post-4,寿司-sushi→日式 料理-Japanese food).",4.7 Examples of the Generated Questions,[0],[0]
"This example also demonstrates that for the same input, there are various questioning patterns: a How-question asked by human, a Which-question by STD, and a Yes-No question by HTD.",4.7 Examples of the Generated Questions,[0],[0]
"As for Post-4, the gold question requires a background that is only shared between the poster and responder, while STD and HTD tend to raise more general questions due to the lack of such shared knowledge.",4.7 Examples of the Generated Questions,[0],[0]
"To gain more insights into how a word type influence the generation process, we visualized the type probability at each decoding position in HTD.",4.8 Visualization of Type Distribution,[0],[0]
This example (Figure 3) shows that the model can capture word types well at different positions.,4.8 Visualization of Type Distribution,[0],[0]
"For instance, at the first and second positions, ordinary words have the highest probabilities for generating 你-you and 喜欢-like, and at the third position, a
topic word兔子-rabbit is predicted while the last two positions are for interrogatives (a particle and a question mark).",4.8 Visualization of Type Distribution,[0],[0]
"We presented error type distribution by manually analyzing 100 bad responses sampled from STD and HTD respectively, where bad means the response by our model is worse than that by some baseline during the pair-wise annotation.
",4.9 Error Analysis,[0],[0]
"There are 4 typical error types: no topic words (NoT) in a response (mainly universal questions), wrong topics (WrT) where topic words are irrelevant, type generation error (TGE) where a wrong word type is predicted (See Eq. 2) and it causes grammatical errors, and other errors.
",4.9 Error Analysis,[0],[0]
The error distribution is shown in Table 6.,4.9 Error Analysis,[0],[0]
"For STD, most of the errors are attributed to no topic or wrong topics, while for HTD, the majority of errors fall into wrong topics.
",4.9 Error Analysis,[0],[0]
There are typical cases for these error types: (1) Posts such as “I am so happy today!”,4.9 Error Analysis,[0],[0]
"contains
no topic words or rare topic words.",4.9 Error Analysis,[0],[0]
"In this case, our method is unable to predict the topic words so that the models tend to generate universal questions.",4.9 Error Analysis,[0],[0]
This happens more frequently in STD because the topic words are not specified explicitly.,4.9 Error Analysis,[0],[0]
"(2) Posts contains multiple topic words, but the model sometimes focuses on an inappropriate one.",4.9 Error Analysis,[0],[0]
"For instance, for Post-2 in Table 7, HTD focused on 海报-poster but 合作-cooperation is a proper one to be focused on.",4.9 Error Analysis,[0],[0]
"(3) For complex posts, the models failed to predict the correct word type in response.",4.9 Error Analysis,[0],[0]
"For Post-3, STD generated a declarative sentence and HTD generated a question which, however, is not adequate within the context.
",4.9 Error Analysis,[0],[0]
"These cases show that controlling the questioning patterns and the informativeness of the content faces with the compatibility issue, which is challenging in language generation.",4.9 Error Analysis,[0],[0]
"These errors are also partially due to the imperfect ability of topic word prediction by PMI, which is challenging itself in open-domain conversational systems.",4.9 Error Analysis,[0],[0]
We present two typed decoders to generate questions in open-domain conversational systems.,5 Conclusion and Future Work,[0],[0]
"The decoders firstly estimate a type distribution over word types, and then use the type distribution to modulate the final word generation distribution.",5 Conclusion and Future Work,[0],[0]
"Through modeling the word types in language generation, the proposed decoders are able to question with various patterns and address novel yet related transitional topics in a generated question.",5 Conclusion and Future Work,[0],[0]
"Results show that our models can generate more appropriate questions, with richer topics, thereby more likely to elicit further interactions.
",5 Conclusion and Future Work,[0],[0]
The work can be extended to multi-turn conversation generation by including an additional detector predicting when to ask a question.,5 Conclusion and Future Work,[0],[0]
The detector can be implemented by a classifier or some heuristics.,5 Conclusion and Future Work,[0],[0]
"Furthermore, the typed decoders are applicable to the settings where word types can be easily obtained, such as in emotional text generation (Ghosh et al., 2017; Zhou et al., 2018b).",5 Conclusion and Future Work,[0],[0]
This work was partly supported by the National Science Foundation of China under grant No.61272227/61332007 and the National Basic Research Program (973 Program) under grant No. 2013CB329403.,Acknowledgements,[0],[0]
We would like to thank Prof. Xiaoyan Zhu for her generous support.,Acknowledgements,[0],[0]
"Asking good questions in large-scale, open-domain conversational systems is quite significant yet rather untouched.",abstractText,[0],[0]
"This task, substantially different from traditional question generation, requires to question not only with various patterns but also on diverse and relevant topics.",abstractText,[0],[0]
"We observe that a good question is a natural composition of interrogatives, topic words, and ordinary words.",abstractText,[0],[0]
"Interrogatives lexicalize the pattern of questioning, topic words address the key information for topic transition in dialogue, and ordinary words play syntactical and grammatical roles in making a natural sentence.",abstractText,[0],[0]
We devise two typed decoders (soft typed decoder and hard typed decoder) in which a type distribution over the three types is estimated and used to modulate the final generation distribution.,abstractText,[0],[0]
Extensive experiments show that the typed decoders outperform state-of-the-art baselines and can generate more meaningful questions.,abstractText,[0],[0]
Learning to Ask Questions in Open-domain Conversational Systems with Typed Decoders,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1108–1117 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1108",text,[0],[0]
Human-computer conversation is a critical and challenging task in AI and NLP.,1 Introduction,[0],[0]
"There have been two major streams of research in this direction, namely task oriented dialog and general purpose dialog (i.e., chit-chat).",1 Introduction,[0],[0]
"Task oriented dialog aims to help people complete specific tasks such as buying tickets or shopping, while general purpose dialog attempts to produce natural and meaningful conversations with people regarding a wide range of topics in open domains (Perez-Marin, 2011; Sordoni et al.).",1 Introduction,[0],[0]
"In recent years, the latter has at-
tracted much attention in both academia and industry as a way to explore the possibility in developing a general purpose AI system in language (e.g., chatbots).
",1 Introduction,[0],[0]
A widely adopted approach to general purpose dialog is learning a generative conversational model from large scale social conversation data.,1 Introduction,[0],[0]
"Most methods in this line are constructed within the statistical machine translation (SMT) framework, where a sequence-to-sequence (Seq2Seq) model is learned to “translate” an input utterance into a response.",1 Introduction,[0],[0]
"However, general purpose dialog is intrinsically different from machine translation.",1 Introduction,[0],[0]
"In machine translation, since every sentence and its translation are semantically equivalent, there exists a 1-to-1 relationship between them.",1 Introduction,[0],[0]
"However, in general purpose dialog, a general response (e.g., “I don’t know”) could correspond to a large variety of input utterances.",1 Introduction,[0],[0]
"For example, in the chit-chat corpus used in this study (as shown in Figure 1), the top three most frequently appeared responses are “Must support!",1 Introduction,[0],[0]
"Cheer!”, “Support!",1 Introduction,[0],[0]
"It’s good.”, and “My friends and I are shocked!”, where the response “Must support!",1 Introduction,[0],[0]
Cheer!” is used for 1216 different input utterances.,1 Introduction,[0],[0]
"Previous Seq2Seq models, which treat all the utteranceresponse pairs uniformly and employ a single
model to learn the relationship between them, will inevitably favor such general responses with high frequency.",1 Introduction,[0],[0]
"Although these responses are safe for replying different utterances, they are boring and trivial since they carry little information, and may quickly lead to an end of the conversation.
",1 Introduction,[0],[0]
There have been a few efforts attempting to address this issue in literature.,1 Introduction,[0],[0]
Li et al. (2016a) proposed to use the Maximum Mutual Information (MMI) as the objective to penalize general responses.,1 Introduction,[0],[0]
It could be viewed as a post-processing approach which did not solve the generation of trivial responses fundamentally.,1 Introduction,[0],[0]
Xing et al. (2017) pre-defined a set of topics from an external corpus to guide the generation of the Seq2Seq model.,1 Introduction,[0],[0]
"However, it is difficult to ensure that the topics learned from the external corpus are consistent with that in the conversation corpus, leading to the introduction of additional noises.",1 Introduction,[0],[0]
Zhou et al. (2017) introduced latent responding factors to model multiple responding mechanisms.,1 Introduction,[0],[0]
"However, these latent factors are usually difficult in interpretation and it is hard to decide the number of the latent factors.
",1 Introduction,[0],[0]
"In our work, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity.",1 Introduction,[0],[0]
The key idea is inspired by our observation on everyday conversation between humans.,1 Introduction,[0],[0]
"In human-human conversation, people often actively control the specificity of responses depending on their own response purpose (which might be affected by a variety of underlying factors like their current mood, knowledge state and so on).",1 Introduction,[0],[0]
"For example, they may provide some interesting and specific responses if they like the conversation, or some general responses if they want to end it.",1 Introduction,[0],[0]
"They may provide very detailed responses if they are familiar with the topic, or just “I don’t know” otherwise.",1 Introduction,[0],[0]
"Therefore, we propose to simulate the way people actively control the specificity of the response.
",1 Introduction,[0],[0]
We employ a Seq2Seq framework and further introduce an explicit specificity control variable to represent the response purpose of the agent.,1 Introduction,[0],[0]
"Meanwhile, we assume that each word, beyond the semantic representation which relates to its meaning, also has another representation which relates to the usage preference under different response purpose.",1 Introduction,[0],[0]
We name this representation as the usage representation of words.,1 Introduction,[0],[0]
"The specificity
control variable then interacts with the usage representation of words through a Gaussian Kernel layer, and guides the Seq2Seq model to generate responses at different specificity levels.",1 Introduction,[0],[0]
We refer to our model as Specificity Controlled Seq2Seq model (SC-Seq2Seq).,1 Introduction,[0],[0]
"Note that unlike the work by (Xing et al., 2017), we do not rely on any external corpus to learn our model.",1 Introduction,[0],[0]
"All the model parameters are learned on the same conversation corpus in an end-to-end way.
",1 Introduction,[0],[0]
We employ distant supervision to train our SCSeq2Seq model since the specificity control variable is unknown in the raw data.,1 Introduction,[0],[0]
"We describe two ways to acquire distant labels for the specificity control variable, namely Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF).",1 Introduction,[0],[0]
"By using normalized values, we restrict the specificity control variable to be within a pre-defined continuous value range with each end has very clear meaning on the specificity.",1 Introduction,[0],[0]
"This is significantly different from the discrete latent factors in (Zhou et al., 2017) which are difficult in interpretation.
",1 Introduction,[0],[0]
"We conduct an empirical study on a large public dataset, and compare our model with several state-of-the-art response generation methods.",1 Introduction,[0],[0]
"Empirical results show that our model can generate either general or specific responses, and significantly outperform existing methods under both automatic and human evaluations.",1 Introduction,[0],[0]
"In this section, we briefly review the related work on conversational models and response specificity.",2 Related Work,[0],[0]
Automatic conversation has attracted increasing attention over the past few years.,2.1 Conversational Models,[0],[0]
"At the very beginning, people started the research using handcrafted rules and templates (Walker et al., 2001; Williams et al., 2013; Henderson et al., 2014).",2.1 Conversational Models,[0],[0]
"These approaches required little data for training but huge manual effort to build the model, which is very time-consuming.",2.1 Conversational Models,[0],[0]
"For now, conversational models fall into two major categories: retrieval-based and generation-based.",2.1 Conversational Models,[0],[0]
"Retrievalbased conversational models search the most suitable response from candidate responses using different schemas (Kearns, 2000; Wang et al., 2013; Yan et al., 2016).",2.1 Conversational Models,[0],[0]
"These methods rely on preexisting responses, thus are difficult to be exten-
ded to open domains (Zhou et al., 2017).",2.1 Conversational Models,[0],[0]
"With the large amount of conversation data available on the Internet, generation-based conversational models developed within a SMT framework (Ritter et al., 2011; Cho et al., 2014; Bahdanau et al., 2015) show promising results.",2.1 Conversational Models,[0],[0]
Shang et al. (2015) generated replies for short-text conversation by encoder-decoder-based neural network with local and global attentions.,2.1 Conversational Models,[0],[0]
Serban et al. (2016) built an end-to-end dialogue system using generative hierarchical neural network.,2.1 Conversational Models,[0],[0]
Gu et al. (2016) introduced copynet to simulate the repeating behavior of humans in conversation.,2.1 Conversational Models,[0],[0]
"Similarly, our model is also based on the encoder-decoder framework.",2.1 Conversational Models,[0],[0]
Some recent studies began to focus on generating more specific or informative responses in conversation.,2.2 Response Specificity,[0],[0]
"It is also called a diversity problem since if each response is more specific, it would be more diverse between responses of different utterances.",2.2 Response Specificity,[0],[0]
"As an early work, Li et al. (2016a) used Maximum Mutual Information (MMI) as the objective to penalize general responses.",2.2 Response Specificity,[0],[0]
"Later, Li et al. (2017) proposed a data distillation method, which trains a series of generative models at different levels of specificity and uses a reinforcement learning model to choose the model best suited for decoding depending on the conversation context.",2.2 Response Specificity,[0],[0]
"These methods circumvented the general response issue by using either a post-processing approach or a data selection approach.
",2.2 Response Specificity,[0],[0]
"Besides, Li et al. (2016b) tried to build a personalized conversation engine by adding extra personal information.",2.2 Response Specificity,[0],[0]
Xing et al. (2017) incorporated the topic information from an external corpus into the Seq2Seq framework to guide the generation.,2.2 Response Specificity,[0],[0]
"However, external dataset may not be always available or consistent with the conversation dataset in topics.",2.2 Response Specificity,[0],[0]
Zhou et al. (2017) introduced latent responding factors to the Seq2Seq model to avoid generating safe responses.,2.2 Response Specificity,[0],[0]
"However, these latent factors are usually difficult in interpretation and hard to decide the number.
",2.2 Response Specificity,[0],[0]
"Moreover, Mou et al. (2016) proposed a content-introducing approach to generate a response based on a predicted keyword.",2.2 Response Specificity,[0],[0]
Yao et al. (2016) attempted to improve the specificity with the reinforcement learning framework by using the averaged IDF score of the words in the response as a reward.,2.2 Response Specificity,[0],[0]
"Shen et al. (2017) presented a con-
ditional variational framework for generating specific responses based on specific attributes.",2.2 Response Specificity,[0],[0]
"Unlike these existing methods, we introduce an explicit specificity control variable into a Seq2Seq model to handle different utterance-response relationships in terms of specificity.",2.2 Response Specificity,[0],[0]
"In this section, we present the Specificity Controlled Seq2Seq model (SC-Seq2Seq), a novel Seq2Seq model designed for actively controlling the generated responses in terms of specificity.",3 Specificity Controlled Seq2Seq Model,[0],[0]
"The basic idea of a generative conversational model is to learn the mapping from an input utterance to its response, typically using an encoderdecoder framework.",3.1 Model Overview,[0],[0]
"Formally, given an input utterance sequence X = (x1, x2, . . .",3.1 Model Overview,[0],[0]
", xT )",3.1 Model Overview,[0],[0]
"and a target response sequence Y = (y1, y2, . . .",3.1 Model Overview,[0],[0]
", yT ′), a neural Seq2Seq model is employed to learn p(Y|X) based on the training corpus D = {(X,Y)|Y is the response of X}.",3.1 Model Overview,[0],[0]
"By maximizing the likelihood of all the utterance-response pairs with a single mapping mechanism, the learned Seq2Seq model will inevitably favor those general responses that can correspond to a large variety of input utterances.
",3.1 Model Overview,[0],[0]
"To address this issue, we assume that there are different mapping mechanisms between utteranceresponse pairs with respect to their specificity relation.",3.1 Model Overview,[0],[0]
"Rather than involving some latent factors, we propose to introduce an explicit variable s into a Seq2Seq model to handle different utteranceresponse mappings in terms of specificity.",3.1 Model Overview,[0],[0]
"By doing so, we hope that (1) s would have explicit meaning on specificity, and (2) s could not only interpret but also actively control the generation of the response Y given the input utterance X. The goal of our model becomes to learn p(Y|X, s) over the corpus D, where we acquire distant labels for s from the same corpus for learning.",3.1 Model Overview,[0],[0]
"The overall architecture of SC-Seq2Seq is depicted in Figure 2, and we will detail our model as follows.",3.1 Model Overview,[0],[0]
The encoder is to map the input utterance X into a compact vector that can capture its essential topics.,3.1.1 Encoder,[0],[0]
"Specifically, we use a bi-directional GRU (Cho et al., 2014) as the utterance encoder, and each word xi is firstly represented by its semantic representation ei mapped by semantic embedding
matrix E as the input of the encoder.",3.1.1 Encoder,[0],[0]
"Then, the encoder represents the utterance X as a series of hidden vectors {ht}Tt=1 modeling the sequence from both forward and backward directions.",3.1.1 Encoder,[0],[0]
"Finally, we use the final backward hidden state as the initial hidden state of the decoder.",3.1.1 Encoder,[0],[0]
"The decoder is to generate a response Y given the hidden representations of the input utterance X under some specificity level denoted by the control variable s. Specifically, at step t, we define the probability of generating any target word yt by a “mixture” of probabilities:
p(yt)",3.1.2 Decoder,[0],[0]
"= βpM (yt) + γpS(yt), (1)
where pM (yt) denotes the semantic-based generation probability, pS(yt) denotes the specificitybased generation probability, β and γ are the coefficients.
",3.1.2 Decoder,[0],[0]
"Specifically, pM (yt) is defined the same as that in traditional Seq2Seq model (Sutskever et al., 2014):
pM (yt = w) =",3.1.2 Decoder,[0],[0]
"wT(WhM ·hyt +WeM ·et−1+bM ), (2) where w is a one-hot indicator vector of the word w and et−1 is the semantic representation of the t − 1-th generated word in decoder. WhM , WeM and bM are parameters.",3.1.2 Decoder,[0],[0]
"hyt is the t-th hidden state in the decoder which is computed by:
hyt = f(yt−1,hyt−1 , ct), (3)
where f is a GRU unit and ct is the context vector to allow the decoder to pay different attention to different parts of input at different steps (Bahdanau et al., 2015).
",3.1.2 Decoder,[0],[0]
pS(yt) denotes the generation probability of the target word given the specificity control variable s.,3.1.2 Decoder,[0],[0]
Here we introduce a Gaussian Kernel layer to define this probability.,3.1.2 Decoder,[0],[0]
"Specifically, we assume that each word, beyond its semantic representation e, also has a usage representation u mapped by usage embedding matrix U. The usage representation of a word denotes its usage preference under different specificity.",3.1.2 Decoder,[0],[0]
"The specificity control variable s then interacts with the usage representations through the Gaussian Kernel layer to produce the specificity-based generation probability pS(yt):
pS(yt = w) = 1√ 2πσ exp(−(ΨS(U,w)− s) 2 2σ2 ),
ΨS(U,w) = σ(wT(U ·WU + bU )), (4) where σ2 is the variance, and ΨS(·) maps the word usage representation into a real value with the specificity control variable s as the mean of the Gaussian distribution.",3.1.2 Decoder,[0],[0]
WU and bU are parameters to be learned.,3.1.2 Decoder,[0],[0]
"Note here in general we can use any realvalue function to define ΨS(U,w).",3.1.2 Decoder,[0],[0]
"In this work, we use the sigmoid function σ(·) for ΨS(U,w) since we want to define s within the range [0,1] so that each end has very clear meaning on the specificity, i.e., 0 denotes the most general response while 1 denotes the most specific response.",3.1.2 Decoder,[0],[0]
"In the next section, we will also keep this property when we define the distant label for the control variable.",3.1.2 Decoder,[0],[0]
"We train our SC-Seq2Seq model by maximizing the log likelihood of generating responses over the training set D:
L = ∑
(X,Y)∈D
logP (Y|X, s; θ).",3.2 Distant Supervision,[0],[0]
"(5)
where θ denotes all the model parameters.",3.2 Distant Supervision,[0],[0]
"Note here since s is an explicit control variable in our model, we need the triples (X,Y, s) for training.",3.2 Distant Supervision,[0],[0]
"However, s is not directly available in the raw conversation corpus, thus we acquire distant labels for s to learn our model.",3.2 Distant Supervision,[0],[0]
"We introduce two ways of distant supervision on the specificity control variable s, namely Normalized Inverse Response Frequency (NIRF) and Normalized Inverse Word Frequency (NIWF).",3.2 Distant Supervision,[0],[0]
Normalized Inverse Response Frequency (NIRF) is based on the assumption that a response is more general if it corresponds to more input utterances in the corpus.,3.2.1 Normalized Inverse Response Frequency,[0],[0]
"Therefore, we use the inverse frequency of a response in a conversation corpus to indicate its specificity level.",3.2.1 Normalized Inverse Response Frequency,[0],[0]
"Specifically, we first build the response collection R by extracting all the responses from D. For a response Y ∈ R, let fY denote its corpus frequency in R, we compute its Inverse Response Frequency (IRF) as:
IRFY = log(1 + |R|)/fY, (6)
where |R| denotes the size of the response collection R. Next, we use the min-max normalization method (Jain et al., 2005) to obtain the NIRF value.",3.2.1 Normalized Inverse Response Frequency,[0],[0]
"Namely,
NIRFY = IRFY −minY′∈R(IRFY′)
maxY′∈R(IRFY′)−minY′∈R(IRFY′) .
(7) where max(IRFR) and min(IRFR) denotes the maximal and minimum IRF value in R respectively.",3.2.1 Normalized Inverse Response Frequency,[0],[0]
The NIRF value is then used as the distant label of s in training.,3.2.1 Normalized Inverse Response Frequency,[0],[0]
"Note here by using normalized values, we aim to constrain the specificity control variable s to be within the pre-defined continuous value range [0,1].",3.2.1 Normalized Inverse Response Frequency,[0],[0]
"Normalized Inverse Word Frequency (NIWF) is based on the assumption that the specificity level of a response depends on the collection of words it contains, and the sentence is more specific if it contains more specific words.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"Hence, we can use the inverse corpus frequency of the words to indicate the specificity level of a response.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"Specifically, for a word y in the response Y, we first obtain its Inverse Word Frequency (IWF) by:
IWFy = log(1 + |R|)/fy, (8)
where fy denotes the number of responses in R containing the word y. Since a response usually contains a collection of words, there would be multiple ways to define the response-level IWF value, e.g., sum, average, minimum or maximum of the IWF values of all the words.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"In our work, we find that the best performance can be achieved by using the maximum of the IWF of all the words in Y to represent the response-level IWF by
IWFY = maxy∈Y(IWFy).",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"(9)
This is reasonable since a response is specific as long as it contains some specific words.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"We do not require all the words in a response to be specific, thus sum, average, and minimum would not be appropriate operators for computing the responselevel IWF.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"Again, we use min-max normalization to obtain the NIWF value for the response Y.",3.2.2 Normalized Inverse Word Frequency,[0],[0]
"Given a new input utterance, we can employ the learned SC-Seq2Seq model to generate responses at different specificity levels by varying the control variable s.",3.3 Specificity Controlled Response Generation,[0],[0]
"In this way, we can simulate human conversations where one can actively control the response specificity depending on his/her own mind.",3.3 Specificity Controlled Response Generation,[0],[0]
"When we apply our model to a chatbot, there might be different ways to use the control variable for conversation in practice.",3.3 Specificity Controlled Response Generation,[0],[0]
"If we want the agent to always generate informative responses, we can set s to 1 or some values close to 1.",3.3 Specificity Controlled Response Generation,[0],[0]
"If we want the agent to be more dynamic, we can sample s within the range [0,1] to enrich the styles in the response.",3.3 Specificity Controlled Response Generation,[0],[0]
We may further employ some reinforcement learning technique to learn to adjust the control variable depending on users’ feedbacks.,3.3 Specificity Controlled Response Generation,[0],[0]
"This would make the agent even more vivid, and we leave this as our future work.",3.3 Specificity Controlled Response Generation,[0],[0]
"In this section, we conduct experiments to verify the effectiveness of our proposed model.",4 Experiment,[0],[0]
We conduct our experiments on the public Short Text Conversation (STC) dataset1 released in NTCIR-13.,4.1 Dataset Description,[0],[0]
"STC maintains a large repository of post-comment pairs from the Sina Weibo which is one of the popular Chinese social sites.
",4.1 Dataset Description,[0],[0]
"1http://ntcirstc.noahlab.com.hk/STC2/stc-cn.htm
STC dataset contains roughly 3.8 million postcomment pairs, which could be used to simulate the utterance-response pairs in conversation.",4.1 Dataset Description,[0],[0]
"We employ the Jieba Chinese word segmenter2 to tokenize the utterances and responses into sequences of Chinese words, and the detailed dataset statistics are shown in Table 1.",4.1 Dataset Description,[0],[0]
"We randomly selected two subsets as the development and test dataset, each containing 10k pairs.",4.1 Dataset Description,[0],[0]
The left pairs are used for training.,4.1 Dataset Description,[0],[0]
"We compare our proposed SC-Seq2Seq model against several state-of-the-art baselines: (1) Seq2Seq-att: the standard Seq2Seq model with the attention mechanism (Bahdanau et al., 2015); (2) MMI-bidi: the Seq2Seq model using Maximum Mutual Information (MMI) as the objective function to reorder the generated responses (Li et al., 2016a); (3) MARM: the Seq2Seq model with a probabilistic framework to model the latent responding mechanisms (Zhou et al., 2017); (4) Seq2Seq+IDF:",4.2 Baselines Methods,[0],[0]
"an extension of Seq2Seq-att by optimizing specificity under the reinforcement learning framework, where the reward is calculated as the sentence level IDF score of the generated response (Yao et al., 2016).",4.2 Baselines Methods,[0],[0]
We refer to our model trained using NIRF and NIWF as SCSeq2SeqNIRF and SC-Seq2SeqNIWF respectively.,4.2 Baselines Methods,[0],[0]
"As suggested in (Shang et al., 2015), we construct two separate vocabularies for utterances and responses by using 40,000 most frequent words on each side in the training data, covering 97.7% words in utterances and 96.1% words in responses respectively.",4.3 Implementation Details,[0],[0]
"All the remaining words are replaced by a special token <UNK> symbol.
",4.3 Implementation Details,[0],[0]
We implemented our model in Tensorflow3.,4.3 Implementation Details,[0],[0]
"We 2https://pypi.python.org/pypi/jieba 3https://www.tensorflow.org/
tuned the hyper-parameters via the development set.",4.3 Implementation Details,[0],[0]
"Specifically, we use one layer of bi-directional GRU for encoder and another uni-directional GRU for decoder, with the GRU hidden unit size set as 300 in both the encoder and decoder.",4.3 Implementation Details,[0],[0]
"The dimension of semantic word embeddings in both utterances and responses is 300, while the dimension of usage word embeddings in responses is 50.",4.3 Implementation Details,[0],[0]
"We apply the Adam algorithm (Kingma and Ba, 2015) for optimization, where the parameters of Adam are set as in (Kingma and Ba, 2015).",4.3 Implementation Details,[0],[0]
"The variance σ2 of the Gaussian Kernel layer is set as 1, and all other trainable parameters are randomly initialized by uniform distribution within [-0.08,0.08].",4.3 Implementation Details,[0],[0]
The mini-batch size for the update is set as 128.,4.3 Implementation Details,[0],[0]
"We clip the gradient when its norm exceeds 5.
",4.3 Implementation Details,[0],[0]
"Our model is trained on a Tesla K80 GPU card, and we run the training for up to 12 epochs, which takes approximately five days.",4.3 Implementation Details,[0],[0]
"We select the model that achieves the lowest perplexity on the development dataset, and we report results on the test dataset.",4.3 Implementation Details,[0],[0]
"For evaluation, we follow the existing work and employ both automatic and human evaluations: (1) distinct-1 & distinct-2 (Li et al., 2016a): we count numbers of distinct unigrams and bigrams in the generated responses, and divide the numbers by total number of generated unigrams and bigrams.",4.4 Evaluation Methodologies,[0],[0]
Distinct metrics (both the numbers and the ratios) can be used to evaluate the specificity/diversity of the responses.,4.4 Evaluation Methodologies,[0],[0]
"(2) BLEU (Papineni et al., 2002): BLEU has been proved strongly correlated with human evaluations.",4.4 Evaluation Methodologies,[0],[0]
BLEU-n measures the average n-gram precision on a set of reference sentences.,4.4 Evaluation Methodologies,[0],[0]
"(3) Average & Extrema (Serban et al., 2017): Average and Extrema projects the generated response and the ground truth response into two separate vectors by taking the mean over the word embeddings or taking the extremum of each dimension respectively, and then computes the cosine similarity between them.",4.4 Evaluation Methodologies,[0],[0]
(4) Human evaluation: Three labelers with rich Weibo experience were recruited to conduct evaluation.,4.4 Evaluation Methodologies,[0],[0]
Responses from different models are randomly mixed for labeling.,4.4 Evaluation Methodologies,[0],[0]
"Labelers refer to 300 random sampled test utterances and score the quality of the responses with the following criteria: 1) +2: the response is not only semantically relevant and grammatical, but also informat-
ive and interesting; 2) +1: the response is grammatical and can be used as a response to the utterance, but is too trivial (e.g., “I don’t know”);",4.4 Evaluation Methodologies,[0],[0]
"3) +0: the response is semantically irrelevant or ungrammatical (e.g., grammatical errors or UNK).",4.4 Evaluation Methodologies,[0],[0]
"Agreements to measure inter-rater consistency among three labelers are calculated with the Fleiss’ kappa (Fleiss and Cohen, 1973).",4.4 Evaluation Methodologies,[0],[0]
Model Analysis: We first analyze our models trained with different distant supervision information.,4.5 Evaluation Results,[0],[0]
"For each model, given a test utterance, we vary the control variable s by setting it to five different values (i.e., 0, 0.2, 0.5, 0.8, 1) to check whether the learned model can actually achieve different specificity levels.",4.5 Evaluation Results,[0],[0]
"As shown in Table 2, we find that: (1) The SC-Seq2Seq model trained with NIRF cannot work well.",4.5 Evaluation Results,[0],[0]
The test performances are almost the same with different s value.,4.5 Evaluation Results,[0],[0]
This is surprising since the NIRF definition seems to be directly corresponding to the specificity of a response.,4.5 Evaluation Results,[0],[0]
"By conducting further analysis, we find that even though the conversation dataset is large, it is still limited and a general response could appear very few times in this corpus.",4.5 Evaluation Results,[0],[0]
"In other words, the inverse frequency of a response is very weakly correlated with its response spe-
cificity.",4.5 Evaluation Results,[0],[0]
(2) The SC-Seq2Seq model trained with NIWF can achieve our purpose.,4.5 Evaluation Results,[0],[0]
"By varying the control variable s from 0 to 1, the generated responses turn from general to specific as measured by the distinct metrics.",4.5 Evaluation Results,[0],[0]
The results indicate that the max inverse word frequency in a response is a good distant label for the response specificity.,4.5 Evaluation Results,[0],[0]
"(3) When we compare the generated responses against ground truth data, we find the SC-Seq2SeqNIWF model with the control variable s set to 0.5 can achieve the best performances.",4.5 Evaluation Results,[0],[0]
"The results indicate that there are diverse responses in real data in terms of specificity, and it is necessary to take a balanced setting if we want to fit the ground truth.
",4.5 Evaluation Results,[0],[0]
Baseline Comparison: The performance comparisons between our model and the baselines are shown in Table 3.,4.5 Evaluation Results,[0],[0]
"We have the following observations: (1) By using MMI as the objective, MMI-bidi can improve the specificity (in terms of distinct ratios) over the traditional Seq2Seq-att model.",4.5 Evaluation Results,[0],[0]
"(2) MARM can achieve the best distinct ratios among the baseline methods, but the worst in terms of the distinct numbers.",4.5 Evaluation Results,[0],[0]
The results indicate that MARM tends to generate specific but very short responses.,4.5 Evaluation Results,[0],[0]
"Meanwhile, its low BLEU scores also show that the responses generated by MARM deviate from the ground truth significantly.",4.5 Evaluation Results,[0],[0]
"(3) By using the IDF information as the reward to train
the Seq2Seq model, the Seq2Seq+IDF does not show much advantages, but only achieves comparable results as MMI-bidi.",4.5 Evaluation Results,[0],[0]
"(4) By setting the control variable s to 1, our SC-Seq2SeqNIWF model can achieve the best specificity performance as evaluated by the distinct metrics.",4.5 Evaluation Results,[0],[0]
"By setting the control variable s to 0.5, our SC-Seq2SeqNIWF model can best fit the ground truth data as evaluated by the BLEU scores, Average and Extrema.",4.5 Evaluation Results,[0],[0]
All the improvements over the baseline models are statistically significant (p-value < 0.01).,4.5 Evaluation Results,[0],[0]
"These results demonstrate the effectiveness as well as the flexibility of our controlled generation model.
",4.5 Evaluation Results,[0],[0]
Table 4 shows the human evaluation results.,4.5 Evaluation Results,[0],[0]
"We can observe that: (1) SC-Seq2SeqNIWF,s=1 generates the most informative responses and interesting (labeled as “+2”) and the least general responses than all the baseline models.",4.5 Evaluation Results,[0],[0]
"Meanwhile, SC-Seq2SeqNIWF,s=0 generates the most general responses (labeled as “+1”); (2) MARM generates the most bad responses (labeled as “+0”), which indicates the drawbacks of the unknown latent responding mechanisms; (3) The kappa values of our models are all larger than 0.4, considered as “moderate agreement” regarding quality of responses.",4.5 Evaluation Results,[0],[0]
"The largest kappa value is achieved by SC-Seq2SeqNIWF,s=0, which seems reasonable since it is easy to reach an agreement on general responses.",4.5 Evaluation Results,[0],[0]
"Sign tests demonstrate the improvements of SC-Seq2SeqNIWF,s=1 to the baseline models are statistically significant (p-value < 0.01).",4.5 Evaluation Results,[0],[0]
All the human judgement results again demonstrate the effectiveness of our controlled generation mechanism.,4.5 Evaluation Results,[0],[0]
"To better understand how different models perform, we conduct some case studies.",4.6 Case Study,[0],[0]
"We randomly sample three utterances from the test dataset, and show the responses generated by different models.
",4.6 Case Study,[0],[0]
"Utterance: 三室一厅的帐篷，你值得拥有！A tent with three rooms and one hall, you’re worth it!",4.6 Case Study,[0],[0]
"Seq2Seq-att:这个不错，很好！It looks good, very good!",4.6 Case Study,[0],[0]
"MMI-bidi:这个不错，我喜欢。It looks good, I like it.",4.6 Case Study,[0],[0]
MARM:不错。Good.,4.6 Case Study,[0],[0]
"Seq2Seq+IDF:帐篷不错。The tent is good.
s=1",4.6 Case Study,[0],[0]
:我想知道这帐篷是什么牌子的？I want to know what the brand of this tent is? s=0.8,4.6 Case Study,[0],[0]
:我想知道这是在哪买的？I want to know where to buy it?,4.6 Case Study,[0],[0]
"s=0.5:我也想买一个,可惜买不起！I want one too, but it’s a pity that I can’t afford it.",4.6 Case Study,[0],[0]
s=0.2:,4.6 Case Study,[0],[0]
"不错不错！Good, good.",4.6 Case Study,[0],[0]
"s=0:真的吗？Really?
",4.6 Case Study,[0],[0]
Utterance:哎哟!被发现了!汤唯有必要那么美吗?,4.6 Case Study,[0],[0]
Oh!,4.6 Case Study,[0],[0]
Got caught!,4.6 Case Study,[0],[0]
Why is Tang Wei so beautiful?,4.6 Case Study,[0],[0]
Seq2Seq-att:我超喜欢她！I really like her!,4.6 Case Study,[0],[0]
MMI-bidi:好美。我超喜欢她！So beautiful.,4.6 Case Study,[0],[0]
I really like her!,4.6 Case Study,[0],[0]
MARM:我不喜欢汤唯。I don’t like Tang Wei.,4.6 Case Study,[0],[0]
Seq2Seq+IDF:,4.6 Case Study,[0],[0]
"我超喜欢她的，我喜欢。I really like her, I like.
s=1:",4.6 Case Study,[0],[0]
"我超喜欢她的眼妆，她的气质很美！I really like her eye make-up, and her temperament is so beautiful! s=0.8:我超喜欢她的，她的眼睛很雪亮！I really like her, and her eyes are so bright!",4.6 Case Study,[0],[0]
"s=0.5:我觉得她很美，很喜欢她！I think she is very beautiful, and I really like her!",4.6 Case Study,[0],[0]
s=0.2:我觉得她很美！I think she is very beautiful.,4.6 Case Study,[0],[0]
"s=0:是的。Yes.
",4.6 Case Study,[0],[0]
"Utterance:将来也要养一只这么乖的的萨摩，让它陪着孩子一起长大。I will keep such a sweet samoyed, and let it accompany my children to grow up.",4.6 Case Study,[0],[0]
Seq2Seq-att:这不是萨摩吗？Is it a samoyed?,4.6 Case Study,[0],[0]
MMI-bidi:好可爱的萨摩。What a lovely samoyed.,4.6 Case Study,[0],[0]
MARM:好可爱啊。So lovely.,4.6 Case Study,[0],[0]
Seq2Seq+IDF:,4.6 Case Study,[0],[0]
"好可爱的狗狗。What a lovely dog.
s=1: 萨摩耶好可爱，真想养一只!",4.6 Case Study,[0],[0]
"The samoyed is so lovely, and I really want to keep one! s=0.8:萨摩好可爱，好想掐掐。The samoyed is so lovely, and I really want to pinch it.",4.6 Case Study,[0],[0]
s=0.5:,4.6 Case Study,[0],[0]
"好可爱的狗狗，好可爱的狗狗。What a lovely dog, what a lovely dog.",4.6 Case Study,[0],[0]
s=0.2:好可爱！好可爱！So,4.6 Case Study,[0],[0]
"lovely, so lovely!",4.6 Case Study,[0],[0]
s=0:,4.6 Case Study,[0],[0]
"好可爱！So lovely!
Table 5:",4.6 Case Study,[0],[0]
Examples of response generation from the STC test data.,4.6 Case Study,[0],[0]
s,4.6 Case Study,[0],[0]
"= 1, 0.8, 0.5, 0.2, 0 are the outputs of our SC-Seq2SeqNIWF with different s values.
",4.6 Case Study,[0],[0]
"As shown in Table 5, we can find that: (1) The responses generated by the four baselines are often quite general and short, which may quickly lead to an end of the conversation.",4.6 Case Study,[0],[0]
"(2) SC-Seq2SeqNIWF with large control variable values (i.e., s > 0.5) can generate very long and specific responses.",4.6 Case Study,[0],[0]
"In these responses, we can find many informative words.",4.6 Case Study,[0],[0]
"For example, in case 2 with s as 1 and 0.8, we can find words like “眼妆(eye make-up)”, “气 质(temperament)” and “雪亮(bright)” which are quite specific and strongly related to the conversation topic of “beauty”.",4.6 Case Study,[0],[0]
"(3) When we decrease the control variable value, the generated responses become more and more general and shorter from our SC-Seq2SeqNIWF model.",4.6 Case Study,[0],[0]
We also conduct some analysis to understand the usage representations of words introduced in our model.,4.7 Analysis on Usage Representations,[0],[0]
"We randomly sample 500 words from our SC-Seq2SeqNIWF and apply t-SNE (Maaten and Hinton, 2008) to visualize both usage and semantic embeddings.",4.7 Analysis on Usage Representations,[0],[0]
"As shown in Figure 3, we can see that the two distributions are significantly different.",4.7 Analysis on Usage Representations,[0],[0]
"In the usage space, words like “脂 肪肝(fatty liver)” and “久坐(outsit)” lie closely which are both specific words, and both are far from the general words like “胖(fat)”.",4.7 Analysis on Usage Representations,[0],[0]
"On the contrary, in the semantic space, “脂肪肝(fatty liver)” is close to “胖(fat)” since they are semantically related, and both are far from the word “久坐(outsit)”.",4.7 Analysis on Usage Representations,[0],[0]
"Furthermore, given some sampled target words, we also show the top-5 similar words based on cosine similarity under both representations in Table 6.",4.7 Analysis on Usage Representations,[0],[0]
"Again, we can see that the nearest neighbors of a same word are quite different under two representations.",4.7 Analysis on Usage Representations,[0],[0]
"Neighbors based on semantic representations are semantically related, while neighbors based on usage representations are not so related but with similar specificity levels.",4.7 Analysis on Usage Representations,[0],[0]
We propose a novel controlled response generation mechanism to handle different utteranceresponse relationships in terms of specificity.,5 Conclusion,[0],[0]
"We introduce an explicit specificity control variable
into the Seq2Seq model, which interacts with the usage representation of words to generate responses at different specificity levels.",5 Conclusion,[0],[0]
"Empirical results showed that our model can generate either general or specific responses, and significantly outperform state-of-the-art generation methods.",5 Conclusion,[0],[0]
"This work was funded by the 973 Program of China under Grant No. 2014CB340401, the National Natural Science Foundation of China (NSFC) under Grants No. 61425016, 61472401, 61722211, and 20180290, the Youth Innovation Promotion Association CAS under Grants No. 20144310, and 2016102, and the National Key R&D Program of China under Grants No. 2016QY02D0405.",6 Acknowledgments,[0],[0]
"In conversation, a general response (e.g., “I don’t know”) could correspond to a large variety of input utterances.",abstractText,[0],[0]
"Previous generative conversational models usually employ a single model to learn the relationship between different utteranceresponse pairs, thus tend to favor general and trivial responses which appear frequently.",abstractText,[0],[0]
"To address this problem, we propose a novel controlled response generation mechanism to handle different utterance-response relationships in terms of specificity.",abstractText,[0],[0]
"Specifically, we introduce an explicit specificity control variable into a sequence-to-sequence model, which interacts with the usage representation of words through a Gaussian Kernel layer, to guide the model to generate responses at different specificity levels.",abstractText,[0],[0]
We describe two ways to acquire distant labels for the specificity control variable in learning.,abstractText,[0],[0]
Empirical studies show that our model can significantly outperform the state-of-theart response generation models under both automatic and human evaluations.,abstractText,[0],[0]
Learning to Control the Specificity in Neural Response Generation,title,[0],[0]
"Many decision problems can be phrased as coordination problems of many artificial intelligent agents (Boutilier, 1996).",1. Introduction,[0],[0]
"Examples include robot soccer (Kok et al., 2003), warehouse commissioning (Claes et al., 2017), and traffic light control (Wiering, 2000).",1. Introduction,[0],[0]
"We consider the cooperative case, where there is a single goal to be optimised.",1. Introduction,[0],[0]
"A naive approach could be to consider a super agent that decides on the actions of all agents involved, which could easily result in an action space which is prohibitively large.",1. Introduction,[0],[0]
"However, many coordination tasks have loose couplings.",1. Introduction,[0],[0]
"This means that the total reward to optimise can be decomposed into a sum of local rewards that only depend on (possibly
1Dept. of Computer Science, Vrije Universiteit Brussel, Brussels, Belgium 2Fac. of Science, Vrije Universiteit Amsterdam, The Netherlands 3DeepMind, London, UK.",1. Introduction,[0],[0]
"Correspondence to: Eugenio Bargiacchi <svalorzen@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
overlapping) subsets of agents.,1. Introduction,[0],[0]
"Then, each agent’s action can only directly affects the rewards of few small subsets of agents.",1. Introduction,[0],[0]
"Key to making coordination efficient is exploiting such loose couplings.
",1. Introduction,[0],[0]
"For an example of such a coordination task, consider an autonomously controlled wind farm in which each agent represents a wind turbine that is able to adjust the alignment of its blades to the wind (see Section 6.3).",1. Introduction,[0],[0]
"Each turbine can maximize its own power output by aligning the blades exactly perpendicular to the wind, but doing so may hinder turbines that are behind it due to turbulence (Van Dijk et al., 2016).",1. Introduction,[0],[0]
It should be possible to do better through coordination.,1. Introduction,[0],[0]
"However, considering the full joint action over all turbines leads to a high-dimensional action space, which would be hard to optimise.",1. Introduction,[0],[0]
"Instead, we can see that this problem is loosely coupled, by noting that the power output of each turbine only directly depends on a small subset of other turbines — the turbines upwind within a certain distance.",1. Introduction,[0],[0]
"This means that the total output can be phrased as a sum of local rewards that depend on small subsets of agents.
",1. Introduction,[0],[0]
"In this paper, we formalize multi-agent multi-armed bandits (MAMABs) and investigate how to balance exploration and exploitation in the joint action taken by the agents, such that the loss due to taking suboptimal joint actions during learning is bounded.",1. Introduction,[0],[0]
"Building on the upper confidence bound (UCB) framework (Auer et al., 2002) for single-agent multi-armed bandits, we formulate a new algorithm that we call multi-agent upper confidence exploration (MAUCE) (Section 4).",1. Introduction,[0],[0]
"MAUCE balances exploitation and exploration using local estimates and local upper confidence bounds.
",1. Introduction,[0],[0]
"We prove in Section 5 that MAUCE achieves a regret bound that depends on the harmonic mean of the local upper confidence bounds, rather than their sum, as we would get by applying the combinatorial bandit framework (Cesa-Bianchi & Lugosi, 2012; Chen et al., 2013).",1. Introduction,[0],[0]
This leads to a regret logarithmic in the number of arm-pulls and linear in the number of agents.,1. Introduction,[0],[0]
"In contrast, the naive approach of considering the full joint action is exponential in the number of agents.",1. Introduction,[0],[0]
"In Section 6 we compare empirically the performance of MAUCE to other approaches from the literature, and show that it achieves much less regret in various settings, including wind farm control.",1. Introduction,[0],[0]
"Multi-agent reinforcement learning and planning with loose couplings has mainly been studied in sequential problems (Guestrin et al., 2002; Kok & Vlassis, 2006; De Hauwere et al., 2010; Scharpff et al., 2016).",2. Related Work,[0],[0]
"In such sequential settings however, the value function does not permit an exact factorization.",2. Related Work,[0],[0]
"Therefore, only in the planning setting (Scharpff et al., 2016), some guarantees can be provided.",2. Related Work,[0],[0]
"For learning (Kok & Vlassis, 2006), the focus has been on empirical performance.",2. Related Work,[0],[0]
"In this paper, we focus on MAMAB, which permit an exact factorization of the value function.
",2. Related Work,[0],[0]
"This work is related to combinatorial bandits (Bubeck & Cesa-Bianchi, 2012; Cesa-Bianchi & Lugosi, 2012; Gai et al., 2012; Chen et al., 2013), in which sets of arms can be pulled simultaneously.",2. Related Work,[0],[0]
"In our setting, these variables correspond to the different agents, and similarly to the combinatorial bandit framework, the action space grows exponentially with the size of the sets of rewards.",2. Related Work,[0],[0]
"We consider a specific variant, called the semi-bandit problem (Audibert et al., 2011), in which local components of the global reward are observable.",2. Related Work,[0],[0]
Chen et al. (2013) considered this variant and constructed an algorithm.,2. Related Work,[0],[0]
"However, that algorithm assumes access to an (α, β)-oracle that provides a joint action that outputs an α fraction of the optimal expected reward with a certain probability β.",2. Related Work,[0],[0]
"Instead, we assume the availability of a coordination graph, which is often a more reasonable assumption in multi-agent settings.",2. Related Work,[0],[0]
"Before introducing our new algorithm, we first need to define our learning problem.",3. Background,[0],[0]
"This problem, the multi-agent multi-armed bandit, is a repeated fully cooperative multiagent game.",3. Background,[0],[0]
"We first define the single-agent version of our setting, and then add the multi-agent elements.",3. Background,[0],[0]
The single-agent version of our setting is commonly known as the multi-armed bandit (MAB): Definition 1.,3. Background,[0],[0]
"A single-agent multi-armed bandit (MAB) (Thompson, 1933) is a tuple 〈A, F 〉 where
• A is a set of actions or arms, and • F (a), called the reward function, is a random function
taking an arm, a ∈ A, as input.",3. Background,[0],[0]
"Specifically, for each a ∈ A, F (a) is a random variable associated with a probability distribution Pa :",3. Background,[0],[0]
R →,3. Background,[0],[0]
"[0, 1] over realvalued rewards r.
We refer to the mean reward of an arm as µa = EPa",3. Background,[0],[0]
[r] =∫∞ −∞,3. Background,[0],[0]
"rPa(r)dr, and to the optimal reward as the mean reward of the best arm µ∗ = maxa µa.
",3. Background,[0],[0]
The goal of an agent interacting with a MAB is to minimize the expected regret.,3. Background,[0],[0]
Definition 2.,3. Background,[0],[0]
"The expected cumulative regret of pulling
a sequence of arms for timestep t",3. Background,[0],[0]
= 1,3. Background,[0],[0]
"to the horizon T (following the definition of Agrawal & Goyal, 2012), is
E",3. Background,[0],[0]
"[ T∑ t=1 µ∗ − µa(t) ] ,
where a(t) is the arm pulled at time t, and na(t) is the number of times arm a is pulled until timestep t.
",3. Background,[0],[0]
"In a multi-agent multi-armed bandit (MAMAB) there are multiple agents, and the rewards are factored:
Definition 3.",3. Background,[0],[0]
"A multi-agent multi-armed bandit (MAB) is a tuple 〈A,D, F 〉 where
• D is the set of m enumerated agents, • A = A1 × · · · × Am is a set of joint actions, which is
the Cartesian product of the sets of individual actions, Ai, for each of the m agents in D, and • F (a), called the global reward function, is a random function taking a joint action, a ∈ A, as input, but with added structure.",3. Background,[0],[0]
"Specifically, there are ρ possibly overlapping subsets of agents, and the global reward is decomposed into ρ local noisy reward functions: F (a) = ∑ρ e=1",3. Background,[0],[0]
"f
e(ae) where fe(ae) ∈",3. Background,[0],[0]
"[0, remax].",3. Background,[0],[0]
"A local function fe only depends on the joint action ae of the subset De of agents.
",3. Background,[0],[0]
"We refer to the mean reward of a joint action as µa, which in turn is factorized into the same local reward components as F (a): µa = ∑ρ e=1 µ(a
e).",3. Background,[0],[0]
"For simplicity, we refer to an agent i by its index.
µa thus maps joint actions to real-valued expected rewards via real-valued local expected rewards, i.e., it is a coordination graph (CoG) (Guestrin et al., 2002; Kok & Vlassis, 2006).",3. Background,[0],[0]
"When µa and all its components are known, it can be used to extract the optimal reward µ∗. A naive way to do so would be to ‘flatten’ the CoG, i.e., enumerate all joint actions, compute their associated mean reward, and then maximize.",3. Background,[0],[0]
"However, this is typically infeasible, as the number of joint actions, A ≡ |A|, is exponential in the number of agents.",3. Background,[0],[0]
"For instance, if each agent has two actions, then A = 2m. Therefore, extracting the optimal reward and associated actions is typically done via algorithms like variable elimination (VE).",3. Background,[0],[0]
"In VE, agents are eliminated from the CoG sequentially, thus solving the maximization problem as a series of local subproblems: one per agent.",3. Background,[0],[0]
"When an agent is eliminated, VE computes its best responses to all possible actions of its neighbors, i.e., the agents with which it shares a local reward function.",3. Background,[0],[0]
"The local values of these best responses are then used to create a new local mean reward, replacing those to which the eliminated agent was connected.",3. Background,[0],[0]
"This exploits the graphical structure resulting from the factorization, and the size of the local subproblems depends only on the induced width, i.e., how many agents
Algorithm 1 MAUCE 1: Input: An MAMAB with a factorized reward function, F (a) = ∑ρ e=1 f
e(ae), a time horizon T 2: Initialize µ̂e(ae) and ne(ae) to zero.",3. Background,[0],[0]
"3: for i = 1 to T do 4: at = arg maxa µ̂t(a) + ct(a) where,
µ̂et (a) = ∑ρ e=1 µ̂t(a e) and,
ct(a) = √ 1 2 ( ∑ρ e=1",3. Background,[0],[0]
"n e t (a e)−1(remax) 2) log(tA)
5: rt= ∑ρ e=1r e t (a
e) (execute a, obtain local rewards) 6: Update µ̂et (a e) using ret (a e) for all ae ⊂ at 7: Increment net (a e) by 1 for all ae ⊂ at 8: end for
the eliminated agent shares a local reward function with at the time of its elimination.",3. Background,[0],[0]
"When the coordination graph is sparse, i.e., agents are only involved in a small number of local reward functions, the induced width is typically much smaller than the size of the joint action space, making the maximization problem tractable.
",3. Background,[0],[0]
"When we are not simply maximizing over the joint actions to extract the optimal reward, but also need to explore to learn what the values of the mean rewards are, the situation becomes more complex.",3. Background,[0],[0]
"Again, we could ‘flatten’ the MAMAB by treating each joint action as a separate arm in a single-agent MAB, but this quickly leads to too many arms to be able to learn effectively with popular algorithms such as UCB (Auer et al., 2002) of which the regret bounds depend on the number of arms.",3. Background,[0],[0]
"Furthermore, just adding the standard exploration bonuses to each of the local mean rewards leads to over-exploration, as we will show experimentally in Section 6.",3. Background,[0],[0]
"Instead, we propose to treat exploration and exploitation as separate objectives during a VE-like scheme, and taking inspiration from the multiobjective literature (Roijers et al., 2015), define a new VElike subroutine, that allows us to define a MAUCE (Section 4) for which we can prove a much tighter regret-bound.",3. Background,[0],[0]
"In this section we propose our new algorithm for MAMABs: Multi-Agent Upper Confidence Exploration (MAUCE) (Algorithm 1).
MAUCE executes a joint action at every timestep that maximizes the estimated mean reward for a given factorization of the reward function, µ̂(a), plus an exploration bonus, ct(a), that is computed using the same factorization.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"To do so, it keeps mean estimates of local rewards µ̂e(ae), and local counts net (a
e) for each subset of agents.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
These local estimates depend only on the subset of actions ae ⊂ a for this group of agents De ⊂ D.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Not all joint actions have to
be selected often, or even at all.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Note that the counts net (a e) used to compute the bonus for an action a can change over time, even if the joint action a has never been selected, because MAUCE observes and uses the local rewards, ret (a
e).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
This enables the algorithm to exploit the graphical structure to compute tighter exploration bonuses while guaranteeing a tight regret bound.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Despite not guaranteeing to explore all joint actions, the algorithm achieves guaranteed logarithmic regret.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"The proof for this regret bound is given in Section 5.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Besides the local counts, the exploration bonus also depends on the maximum value of the local rewards remax, the time index t, and A.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
We note that A is exponential in the number of agents.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Contrary to single-agent MABs, it is not trivial to maximize over µ̂(a)",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"+ ct(a), as we need to maximize over a A efficiently, and ct(a) is a non-linear function in the local counts net (a
e).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Hence, MAUCE requires a special algorithm to perform this maximization.
4.1.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Maximizing µ̂(a) + c(a)
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"We observe that we can express the estimated mean as the sum of local estimated means, and that ct(a) can be expressed as a function over the inverse counts: y( ∑ρ e=1 n e t (a e)−1(remax) 2).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Hence, when we write down the local estimates as two-element vectors: an estimated mean component and a weighted inverse counts component,
ve(ae) = (µ̂e(ae), net (a e)−1(remax) 2), (1)
we can express the mean plus exploration bonus as a function applied to the sum of these vectors:
zt(v(a))",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"= µ̂(a) + ct(a) = v[1] +
√ 1
2 v[2] log(tA), (2)
where
v(a) = ρ∑ e=1 ve(ae).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"(3)
Vector formulations of reward, as those of Equations 1–3, are often used in the multi-objective decision making literature (Roijers & Whiteson, 2017).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Consider multi-objective variable elimination (MOVE) (Rollón & Larrosa, 2006; Roijers et al., 2015), a multi-objective framework based on variable elimination that is able to handle vectors.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Instead of single best responses for eliminated agents, MOVE produces sets of vectors that are possibly optimal as intermediate results.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"At each agent elimination, MOVE computes all possible (local) value vectors for the subproblem of eliminating the agent i, and prunes away those that are locally dominated.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"After MOVE eliminates the last agent it outputs a possibly very large set of possibly optimal vectors, e.g., a Pareto front or convex coverage set.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"In contrast to MOVE, we only want to output a single vector, i.e., the one that maximizes zt (Equation 2).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"To do this we
tighten MOVE’s simple domination pruning by introducing lower and upper bounds on the exploration part of the vector.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"This results in an algorithm in which the number of vectors in the intermediate solution sets steeply decreases in the last agent eliminations (in contrast to MOVE, in which the intermediate sets typically continue to grow in size).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"We call this algorithm upper confidence variable elimination (UCVE).
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"First, we define the input of UCVE.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Specifically, to be able to work with sets of vectors as intermediate results, we first reformulate the problem of finding the optimal joint action in these terms.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Specifically, we define the input to UCVE as a set F of local upper confidence vector set functions (UCVSFs).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"For each fe of F (a), F contains an identically scoped UCVSF ue.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Each ue initially contains a singleton set, ue(ae) = {ve(ae)}, where ve(ae) is defined as in Equation 1.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Eliminating an agent i, is performed by replacing all ue(ae) which have i in scope, i.e., i ∈ De, by a new function that incorporates the possibly optimal responses of i. These possibly optimal responses are again vectors in the form of Equation 1.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
Algorithm 2 UCVE(F),4. Multi-Agent Upper Confidence Exploration,[0],[0]
Input : A set of local upper confidence vector set functions F and an elimination order q (a queue with all agents),4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Output : An optimal joint action, a∗
1: while q is not empty do 2: i← q.dequeue() 3: Fi ← the subset of UCVSFs inF that have i in scope 4: xu, xl ← compute upper and lower bounds on the
exploration part of the vectors for the remaining factors in F \ Fi
5: unew(·)← a new UCVSF 6: for all ae−i ∈ ADe\{i} do 7: V ← ⋃ ai∈Ai ⊕ ue∈Fi ue(ae−i × {ai}) 8: unew(ae−i)←prune(V, xu, xl) 9: end for 10: F ← F \ Fi ∪ {unew} 11: end while 12: u← retrieve final factor from F 13: return the optimal joint action from u
UCVE is provided in Algorithm 2.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
Note that we only describe what is traditionally known as the forward pass of the variable elimination scheme.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"This is because to retrieve the optimal joint action, we make use of the tagging scheme of MOVE (Roijers et al., 2015), where vectors are tagged with the appropriate action of an agent during its elimination.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"UCVE eliminates all agents in a predetermined order, q, in the main loop (line 1–11).",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"On line 2 the next agent i is
popped off the queue, and on line 3 the factors that have i in scope, Fi are collected.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"The functions inFi will be replaced in F by a new UCVSF, unew, incorporating the possible best responses to every possible local joint action of the neighbors of i.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"This new UCVSF has all the neighboring agents De \ {i} of agent i in scope.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"First, all possible vectors V that can be made with the UCVSFs in Fi are computed (on line 7), across all actions of i, for a given ae−i:
V = ⋃
ai∈Ai ⊕ ue∈Fi ue(ae−i × {ai}),
where Ai is the action space of agent i, and the cross-sum operator A ⊕ B is defined as A ⊕ B = {a + b",4. Multi-Agent Upper Confidence Exploration,[0],[0]
:,4. Multi-Agent Upper Confidence Exploration,[0],[0]
a ∈,4. Multi-Agent Upper Confidence Exploration,[0],[0]
A ∧ b ∈ B}.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
Note that the resulting actions always include the appropriate actions ai (which is under the union) and the appropriate actions from ae−i.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"After V is computed, the vectors in V that cannot lead to an optimal joint action need to be pruned.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
Each vector in V consists of an estimated mean and a weighted inverse counts part that will lead to the exploration bonus.,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Because the weighted inverse counts cannot be linearly added to the estimated mean, we cannot a priori tell whether a vector v ∈ V is better than another vector v′ ∈ V when v[1] >",4. Multi-Agent Upper Confidence Exploration,[0],[0]
v′[1] but v[2] < v′[2].,4. Multi-Agent Upper Confidence Exploration,[0],[0]
"We thus define a pruning operator that satisfies the two conditions necessary for correctness in a multi-objective variable elimination-scheme (Roijers, 2016), i.e., (1) no excess values are kept, and (2) no unnecessary values are returned after the last agent elimination.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"We compute an upper and a lower bound on the exploration bonus for the remaining functions in F \Fi, using the sums of the maximum, resp.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"minimum, values for the exploration part, xu = ∑ ue∈F\Fi maxae maxv∈ue(ae) v[2]
and xl = ∑ ue∈F\Fi minae minv∈ue(ae) v[2].",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Specifically, a vector v ∈ V cannot contribute to the optimal value if there is another vector v′ ∈ V such that
v[1]+
√ 1
2 (v[2]+xu) log(tA)<v
′[1]+
√ 1
2 (v′[2]+xl) log(tA)
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"Hence, prune removes those candidate local upper confidence vectors that cannot contribute to finding the maximal mean plus exploration bonus.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"This immediately satisfies correctness condition (1), as it follows from the definition of upper and lower bounds.
",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"After all agents have been eliminated, there is only one UCVSF left, containing a single local upper confidence vector.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"UCVE retrieves the optimal vector— which maximizes the µ̂(a) + ct(a) —and the associated joint action, at, from the final UCVSF, satisfying correctness condition 2.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
"We thus have defined an efficient algorithm that correctly outputs the joint action that maximizes µ̂(a) + ct(a), and can therefore be used to select joint actions inside of MAUCE.",4. Multi-Agent Upper Confidence Exploration,[0],[0]
The efficiency of our method is achieved by exploiting localized structures within the global reward.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"If there are no such structures, then a worst-case regret of O(A log T ) can be achieved by employing an upper-confidence bound (UCB) algorithm (Auer et al., 2002; Auer & Ortner, 2010).",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"As A grows exponentially with the number of agents, the global action space is simply too large to make this bound of practical use.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"However, we show that when the global reward can be decomposed into local reward functions over subsets of agents, the regret obtained when using our method becomes much smaller.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"In fact, when the local rewards all have the same range, the regret of our method becomes linear in the number of agents.
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Assume there are ρ subsets of agents, called groups, and that there is a decomposition of the global reward F (a) =∑ρ e=1 f
e(ae) where fe(ae) ∈",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"[0, remax].",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"W.l.o.g., let us also consider non-identical groups and that ∑ρ e=1 r e max = 1.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
The local function fe only depends on the actions of a group De.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"We maintain the sample mean reward µ̂et (ae) and number of pulls net (a e) for each local joint action ae taken by group De at time t. Finally, we define the gap between the true expected rewards of the optimal action a∗ and action a to be ∆(a) = E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
[F (a∗)]− E [F (a)].,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
Theorem 1.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"If at each time t we choose at such that
at = arg max a wt(a)
= arg max a µ̂t(a) + ct(a) ,
with
µ̂t(a)",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"= ρ∑ e=1 µ̂et (a e) ,
ct(a) = √√√√1 2 ( ρ∑ e=1 net (a e)−1(remax) 2 ) log(tA) ,
then the expected global regret is bounded by
E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"[ T∑ t=1 ∆(at) ] ≤ 2N ∑ρ e=1(r e max) 2 log(TA) mina ∆(a)2 +log T+1 ,
where N ≡ ∑ρ e=1",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"∏ i∈De Ai is the total number of local joint actions and Ai = |Ai|.
Proof.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
Let Ct(a) be the event that ∆(a) > 2ct(a) holds and Ct(a) its negation.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"By the law of the excluded middle, we can then write
E [∆(at)]",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
= E [∆(at) |Ct(at)]P (Ct(at)),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
+,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"E [ ∆(at) ∣∣Ct(at)]P (Ct(at))
which implies
E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
[ T∑ t=1 ∆(at) ] ≤ T∑ t=1,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"P (Ct(at))
+ E [ ∆(at) | Ct(at) ]",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
E [ I { Ct(at) }] where I{·} is the indicator function.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
We first look at all time steps on which Ct(at) holds.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Specifically, we bound the probability that this event occurs.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Using the law of total probability and chain rule, we can derive
P (Ct(at)) ≤",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"a∈A P (a = at |Ct(a)) (4)
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"By definition, action at maximizes the upper bound wt(·).",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Therefore,
P (a = at | Ct(a))",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
= P (wt(a) = wt(at) |Ct(a)),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
≤ P (wt(a) ≥ wt(a∗) |Ct(a)),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"= P (µ̂t(a)− µ̂t(a∗) ≥ ct(a∗)− ct(a) |Ct(a))
≤ exp ( − 2(∆(a) + ct(a∗)− ct(a))
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
2∑ρ e=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) ),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"In the last step, we used Hoeffding’s inequality.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"This is possible, as µ̂t(a) is a sum of i.i.d. random variables bounded within the interval [ 0,
remax net (a e)
] .",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"We now apply condition
Ct(a) such that ∆(a) > 2ct(a) and derive
P (a = at | Ct(a))",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"≤ exp ( − 2(ct(a) + ct(a∗))
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
2∑ρ e=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) ),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
≤ exp ( − 2ct(a) 2 + 2ct(a∗),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"2∑ρ
e=1(r e max) 2 (net (a e)−1 + net (a e ∗) −1) )",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
= exp (− log(tA)),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"≤ (tA)−1
Using (4), we can conclude
T∑ t=1 P (Ct(at))",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
≤,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"T∑ t=1 (tA)−1A ≤ log T + 1 (5)
where for the last step we used ∑T t=1 t
−1",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
< log T + γ + 3 6T+2 < log T + 1,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"(Chen & Qi, 2003), where γ is Euler’s constant.
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Now, we look at the time steps where Ct(at) holds.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Either at = a∗ and ∆(at) = 0, or
∆(at) ≤ 2ct(at)
∆(at) 2 ≤ 2 log(tA)",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
ρ∑ e=1 (remax),5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"2(net (a e t )) −1
1 ≤ 2 log(tA) mine net (a e t )
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"e max) 2
mina6=a∗ ∆(a) 2
min e net (a e t )",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
≤ 2,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"log(TA)
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"e max) 2
mina6=a∗ ∆(a) 2
(6)
Note that as there are at most N = ∑ρ e=1",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"∏ i∈De Ai local joint actions, the left-hand side will increase every at most N time steps.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Since the right-hand side is fixed and does not depend on t, (6) can only be true on at most 2N log(TA)",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
e max) 2/mina6=a∗ ∆(a) 2 different time steps.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
This implies that T∑ t=1,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
E [ ∆(at) | Ct(at) ],5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"E [ I { Ct(at)
}] ≤",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"E
[ T∑ t=1 I { Ct(at) ∧ at 6=",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"a∗ }]
≤ 2N log(TA)
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"e max) 2
mina6=a∗ ∆(a) 2
Together with (4) and (5), this implies
E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
[ T∑ t=1 ∆(at) ] ≤ 2N log(TA) ∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"e max) 2 mina6=a∗ ∆(a) 2 + log T + 1
Corollary 1.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"If Ai ≤ k for all agents i, and if |De| ≤",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
d,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"for all groups De, then
E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
[ T∑ t=1 ∆(at) ],5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"≤ 2ρk d (log T +m log k) mina6=a∗ ∆(a) 2 + log T + 1 .
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
Proof.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
∑ρ e=1 r e max = 1 implies ∑ρ e=1(r,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"e max)
2 ≤ 1.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Additionally, logA = ∑m i=1",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
logAi,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
≤,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
m log k.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"Finally,
N = ∑ρ e=1",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"∏ i∈De Ai ≤ ρkd.
",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"The important thing to note is that the given regret bound is linear in the number of agents m and in the number of functions ρ, which implies — since ρ ≤ ( m d )",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"< md — that it is polynomial in m, with degree at most d+ 1.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"This is a huge improvement over the naive ‘flattened’ regret bound, which is exponential in the number of agents.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
Corollary 2.,5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"If, in addition to the assumptions in Corollary 1, each local function has the same range such that remax = ρ−1, then
E",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
[ T∑ t=1 ∆(at) ],5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"≤ 2k d (log T +m log k) mina6=a∗ ∆(a) 2 + log T + 1 .
Proof.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"If remax = ρ −1 for each e, then ∑ρ e=1(r e max)
2 = ρ−1 and therefore N ∑ρ e=1(r e max) 2 = kd.
Note that this implies that under the assumption that each local function has the same range, 1) the regret no longer depends on ρ and 2) the regret is linear in the number of agents.",5. Linear Regret Bound for Collaborative Multi-Agent Settings,[0],[0]
"In order to test the performance of MAUCE, and compare it to competing approaches, we tested it on three different settings of increasing complexity, which are described below.",6. Experiments,[0],[0]
"We compared our results against several baselines: a uniformly random action selector, Sparse Cooperative QLearning (SCQL) (Kok & Vlassis, 2006), and Learning with Linear Rewards (LLR) (Gai et al., 2012).
SCQL is a multi-agent Q-Learning based algorithm that can leverage domain knowledge about agents’ interdependencies to lower its sample requirements.",6. Experiments,[0],[0]
"SCQL was originally proposed in the context of multi-agent MDPs, but does apply to MAMABs.",6. Experiments,[0],[0]
"To allow for exploration we use both optimistic initialization and an ε-greedy policy, with the ε parameter linearly decreasing over time: ε = 0.05− 10−5t.
LLR is a UCB algorithm from the combinatorial bandit literature that applies most to MAMABs, as it assumes that the rewards are a linear combination of what we refer to as local reward functions.",6. Experiments,[0],[0]
"Contrary to MAUCE however, it computes upper confidence bounds on the local reward components separately, before summing them, rather than our vector-based formulation of Equations 1–3.",6. Experiments,[0],[0]
"LLR is parameterless, aside from the knowledge of which agents depend on each other.
",6. Experiments,[0],[0]
In all experiments the rewards were normalized so that the maximum possible regret per timestep is one.,6. Experiments,[0],[0]
The maximum possible reward was computed by directly solving non-stochastic versions of the problems with Variable Elimination (or brute-force enumeration).,6. Experiments,[0],[0]
"Reward normalization enables directly comparing the output results to see how each approach performs across different settings.
",6. Experiments,[0],[0]
"We now describe each of our problem settings: the 0101- Chain, which is simple but illustrates the fast learning properties of MAUCE; Gem Mining, which is real-world inspired and adapted from an established benchmark multiobjective coordination graph; and Wind Farm, a real-world coordination problem, in which we connect our learning problem to a state-of-the-art wind farm simulator.
",6. Experiments,[0],[0]
All the code needed to run the experiments can be found at https://bitbucket.org/Svalorzen/ mauce-experiments/src/master/,6. Experiments,[0],[0]
"The 0101-Chain is a simple MAMAB, with a known optimal action.",6.1. 0101-Chain,[0],[0]
"The problem consists of n agents, and n− 1 local reward functions.",6.1. 0101-Chain,[0],[0]
"Each local reward function f i(ai, ai+1) is connected to the agent with the same index, i, and to i+1.
",6.1. 0101-Chain,[0],[0]
The optimal action in the 0101-Chain problem is ai = 0,6.1. 0101-Chain,[0],[0]
"if i is even, and ai",6.1. 0101-Chain,[0],[0]
= 1 is i is odd.,6.1. 0101-Chain,[0],[0]
"The reward tables for each local group are given in Table 1.
6.2.",6.1. 0101-Chain,[0],[0]
"Gem Mining
Our Gem Mining problem is adapted from the Mining Day problem from (Roijers et al., 2015), which is a multiobjective coordination graph benchmark problem.
",6.1. 0101-Chain,[0],[0]
"In Gem Mining, a mining company mines gems from a set of mines (local reward functions) located in the mountains (see Figure 1).",6.1. 0101-Chain,[0],[0]
The mine workers live in villages at the foot of the mountains.,6.1. 0101-Chain,[0],[0]
"The company has one van in each village (agents) for transporting workers and must determine every morning to which mine each van should go (actions), but vans can only travel to nearby mines (graph connectivity).",6.1. 0101-Chain,[0],[0]
"Workers are more efficient when there are more workers at a mine: the probability of finding a gem in a mine is x ·1.03w−1, where x is the base probability of finding a gem in a mine and w is the number of workers at the mine.",6.1. 0101-Chain,[0],[0]
"To generate an instance with v villages (agents), we randomly assign 1-5 workers to each village and connect it to 2–4 mines.",6.1. 0101-Chain,[0],[0]
"Each village is only connected to mines with a greater or equal index, i.e., if village i is connected to m mines, it is connected to mines i to i + m − 1.",6.1. 0101-Chain,[0],[0]
The last village is connected to 4 mines and thus the number of mines is v + 3.,6.1. 0101-Chain,[0],[0]
"In our wind farm experiment, we used a state-of-the-art simulator (Van Dijk et al., 2016) to mimic the energy production of a series of wind turbines when exposed to a global incoming wind vector.",6.3. Wind Farm,[0],[0]
"In the real world, turbines can often be oriented at certain angles to maximize production.",6.3. Wind Farm,[0],[0]
"This is a non-trivial control task, as the turbulence caused by a turbine will negatively affect turbines downwind.",6.3. Wind Farm,[0],[0]
"The
direction of this generated turbulence depends on the angle that the turbine has w.r.t.",6.3. Wind Farm,[0],[0]
"the incoming wind vector.
",6.3. Wind Farm,[0],[0]
We setup our simulated wind farm using 11 turbines (see Figure 2).,6.3. Wind Farm,[0],[0]
Each turbine has a choice between three different actions (angles) that it can turn to.,6.3. Wind Farm,[0],[0]
"The last 4 turbines downwind (2, 5, 8, and a) are set directly against the wind and are not controlled by agents, as they cannot generate turbulence that can impact power production.",6.3. Wind Farm,[0],[0]
"However, the remaining 7 turbines do influence the rest of the farm, and so must cooperate to maximize power production.
",6.3. Wind Farm,[0],[0]
"We vary the wind speed in the simulator at each timestep, following a truncated normal distribution with mean 8.1 m/s.",6.3. Wind Farm,[0],[0]
"The overall reward is normalized to a [0, 1] interval using the maximum possible overall reward at the highest wind strength and the minimum possible reward per turbine at the minimum wind strength.",6.3. Wind Farm,[0],[0]
"While this makes it impossible to compute the true regret, as choosing the optimal action does not result in a 0 regret in expectation, it avoids having to calculate the true expected reward for all actions in this scenario, which is non-trivial.
Differently from the previous experiments, rewards in this settings are obtained per-agent from the simulator rather than per-group.",6.3. Wind Farm,[0],[0]
"Thus, we use single-agent local groups to prevent dependencies between the reward functions of each group.",6.3. Wind Farm,[0],[0]
"The reward for agents in more than one group is given solely to their single-agent group, and none to the others (rather than splitting).",6.3. Wind Farm,[0],[0]
"We tested the performance of MAUCE on the higly structured 0101-Chain problem with 11 agents for 10,000 joint action executions, and compare its performance against random, SCQL and LLR.",6.4. Results,[0],[0]
"The results (Figure 3) indicate that both SCQL and MAUCE can learn effectively, far outclassing random joint action selection and LLR.
",6.4. Results,[0],[0]
"When comparing MAUCE and SCQL (Figure 3(b)), MAUCE achieves considerably less regret than SCQL.",6.4. Results,[0],[0]
"This is because MAUCE’s exploration strategy is based on the aggregation of local exploration bounds, while SCQL uses an ε-greedy exploration strategy.",6.4. Results,[0],[0]
"On the other hand, SCQL does learn the optimal joint action quickly, thanks to optimistic initialization and this aggressive exploration strategy.",6.4. Results,[0],[0]
"We note that after a while, we decreased ε to 0, i.e., only exploit, making the regret graph a flat line from that point onward.",6.4. Results,[0],[0]
We note that the annealing of ε needed to be fine-tuned.,6.4. Results,[0],[0]
"We thus conclude that MAUCE is an effective algorithm that can exploit the graphical structure, leading to superior performance for this highly-structured problem.
",6.4. Results,[0],[0]
"We then tested MAUCE against the other algorithms on randomly generated Gem Mining instances with 5 villages and 8 mines, to compare performances on a more challenging problem.",6.4. Results,[0],[0]
Figure 3(c) represents the average regret over multiple different scenarios.,6.4. Results,[0],[0]
"We observe that, while SCQL and LLR are all able to achieve sublinear regret curves, MAUCE handles the exploration-exploitation tradeoffs best, resulting in the lowest regret over time.
",6.4. Results,[0],[0]
"Finally, to test the performance of MAUCE on a real-world problem, we run the algorithms on a Wind Farm instance (Figure 3(d)).",6.4. Results,[0],[0]
"Due to the high computational costs of running the simulator, we perform only ten runs.",6.4. Results,[0],[0]
"As explained before, the measure shown is not an exact form of regret, as the optimal action will not result in a 0 regret in expectation.
",6.4. Results,[0],[0]
"The MAUCE algorithm once again performs best, with less cumulative regret than both LLR and SCQL.",6.4. Results,[0],[0]
The LLR algorithm also doesn’t seem to achieve any significant learning with respect to the random policy.,6.4. Results,[0],[0]
"Note that MAUCE keeps learning and fine tuning this policy over the whole duration of the experiment, which allows it to increasingly achieve lower regret than SCQL.",6.4. Results,[0],[0]
"At timestep 10000, the difference between the two is 43.258, while at timestep 40000 it is 81.373.",6.4. Results,[0],[0]
"It is important to note that SCQL could probably be made to perform better by finely tuning the initialization values and epsilon updates, but this would take significant human time and repetitive trials.",6.4. Results,[0],[0]
"MAUCE can instead directly
manage the exploration-exploitation trade-off by using its local bounds for each local joint action.
",6.4. Results,[0],[0]
"We thus conclude that MAUCE is an effective algorithm for trading off exploration versus exploitation in MAMABs, and has superior performance w.r.t.",6.4. Results,[0],[0]
the alternative algorithms.,6.4. Results,[0],[0]
"In this paper, we proposed the multi-agent upper confidence exploration (MAUCE) algorithm for multi-agent multi-armed bandits (MAMABs).",7. Conclusion,[0],[0]
"While learning, MAUCE leverages the graphical properties of the MAMAB by treating as separate objectives both exploration, expressed as a function of the sum over weighted inverse local counts, and exploitation, i.e., the sum over estimated mean local rewards.",7. Conclusion,[0],[0]
"Via a subroutine, upper confidence variable elimination (UCVE), that can handle these objectives, MAUCE selects the action that best balances exploration and exploitation according to the joint overall mean reward plus (upper confidence) exploration bound.",7. Conclusion,[0],[0]
"We have proven a regret bound for MAUCE that is only linear in the number of agents, rather than exponential, as it would be if we were to flatten the MAMAB to a single-agent MAB.",7. Conclusion,[0],[0]
"Furthermore, the regret bound is logarithmic in the number of arm pulls.",7. Conclusion,[0],[0]
"We compared MAUCE empirically to state-of-the-art algorithms in multi-agent reinforcement learning and combinatorial bandits, and have shown that MAUCE achieves much lower empirical regret than these approaches.
",7. Conclusion,[0],[0]
"We note that the range parameters remax for MAUCE, which represent the difference between the maximum and minimum possible reward for each local joint action, can be difficult to guess in advance when the problem is not exactly known, as in the Wind Farm experiments.",7. Conclusion,[0],[0]
"One way to mitigate this, could be to estimate them from the coordination graph of expected mean rewards learnt while running the algorithm, rather than running preliminary experiments as we did for the Wind Farm.",7. Conclusion,[0],[0]
We will test this in future work.,7. Conclusion,[0],[0]
"Furthermore, we aim to build on MAUCE to achieve quality guarantees for reinforcement learning in multi-agent MDPs.",7. Conclusion,[0],[0]
"The first author was supported by Flanders Innovation & Entrepreneurship (VLAIO), SBO project 140047: Stable MultI-agent LEarnIng for neTworks (SMILE-IT), second author was supported by an FWO PhD grant (Fonds Wetenschappelijk Onderzoek - Vlaanderen), third author was a Postdoctoral Fellow with the FWO (grant #12J0617N).",Acknowledgements,[0],[0]
Learning to coordinate between multiple agents is an important problem in many reinforcement learning problems.,abstractText,[0],[0]
"Key to learning to coordinate is exploiting loose couplings, i.e., conditional independences between agents.",abstractText,[0],[0]
"In this paper we study learning in repeated fully cooperative games, multi-agent multi-armed bandits (MAMABs), in which the expected rewards can be expressed as a coordination graph.",abstractText,[0],[0]
"We propose multi-agent upper confidence exploration (MAUCE), a new algorithm for MAMABs that exploits loose couplings, which enables us to prove a regret bound that is logarithmic in the number of arm pulls and only linear in the number of agents.",abstractText,[0],[0]
"We empirically compare MAUCE to sparse cooperative Q-learning, and a state-of-the-art combinatorial bandit approach, and show that it performs much better on a variety of settings, including learning control policies for wind farms.",abstractText,[0],[0]
Learning to Coordinate with Coordination Graphs in Repeated Single-Stage Multi-Agent Decision Problems,title,[0],[0]
"Sepsis is a clinical condition involving a destructive host response to the invasion of a microorganism and/or its toxin, and is associated with high morbidity and mortality.",1. Introduction,[0],[0]
"Without early intervention, this inflammatory response can progress to septic shock, organ failure and death (Bone et al., 1989).",1. Introduction,[0],[0]
"Identifying sepsis early improves patient outcomes, as mortality from septic shock increases by 7.6%
1Dept. of Statistical Science, Duke University, Durham NC, USA.",1. Introduction,[0],[0]
"Correspondence to: Joseph Futoma <jdf38@duke.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
for every hour that treatment is delayed after the onset of hypotension (Kumar et al., 2006).",1. Introduction,[0],[0]
"It was also recently shown that timely administration of a 3-hour bundle of care for patients with sepsis (i.e. blood culture, broadspectrum antibiotics, and a lactate measurement) was associated with lower in-hospital mortality (Seymour et al., 2017), further emphasizing the need for fast and aggressive treatment.",1. Introduction,[0],[0]
"Unfortunately, early and accurate identification of sepsis remains elusive even for experienced clinicians, as the symptoms associated with sepsis may be caused by many other clinical conditions (Jones et al., 2010).
",1. Introduction,[0],[0]
"Despite the difficulties associated with identifying sepsis, data that could be used to inform such a prediction is already being routinely captured in the electronic health record (EHR).",1. Introduction,[0],[0]
"To this end, data-driven early warning scores have great potential to identify early clinical deterioration using live data from the EHR.",1. Introduction,[0],[0]
"As one example, the Royal College of Physicians developed, validated, and implemented the National Early Warning Score (NEWS) to identify patients who are acutely decompensating (Smith et al., 2013).",1. Introduction,[0],[0]
Such early warning scores compare a small number of physiological variables (NEWS uses six) to normal ranges of values to generate a single composite score.,1. Introduction,[0],[0]
"NEWS is already implemented in our university health system’s EHR so that when the score reaches a defined trigger, a patient’s care nurse is alerted to potential clinical deterioration.",1. Introduction,[0],[0]
"However, a major problem with NEWS and other related early warning scores is that they are typically broad in scope and were not developed to target a specific condition such as sepsis, since many unrelated disease states (e.g. trauma, pancreatitis, alcohol withdrawal) can result in high scores.",1. Introduction,[0],[0]
"Previous measurements revealed 63.4% of the alerts triggered by the NEWS score at our hospital were cancelled by the care nurse, suggesting breakdowns in the training and education process, low specificity, and high alarm fatigue.",1. Introduction,[0],[0]
"Despite the obvious limitation of using only a small fraction of available information, these scores are also overly simplistic in assigning independent scores to each variable, ignoring both the complex relationships between different physiological variables and their evolution in time.
",1. Introduction,[0],[0]
"The goal in this work is to develop a more flexible statistical model that leverages as much available data as possible from patient admissions in order to provide earlier
and more accurate detection of sepsis.",1. Introduction,[0],[0]
"However, this task is complicated by a number of problems that arise working with real EHR data, some of them particular to sepsis.",1. Introduction,[0],[0]
"Unlike other clinical adverse events such as cardiac arrests or transfers to the Intensive Care Unit (ICU) with known event times, sepsis presents a challenge as the exact time at which it starts is generally unknown.",1. Introduction,[0],[0]
"Instead, sepsis is typically observed indirectly through abnormal labs or vitals, the administration of antibiotics, or the drawing of blood cultures to test for suspected infection.",1. Introduction,[0],[0]
"Another challenging aspect of our data source is the large degree of heterogeneity present across patient encounters, as we did not exclude certain classes of admissions.",1. Introduction,[0],[0]
"More generally, clinical time series data presents its own set of problems, as they are measured at irregularly spaced intervals and there are many (often informatively) missing values.",1. Introduction,[0],[0]
"Alignment of patient time series also presents an issue, as patients admitted to the hospital may have very different unknown clinical states, with some having sepsis already upon admission.",1. Introduction,[0],[0]
"A crucial clinical consideration to be taken into account is the timeliness of alarms raised by the model, as a clinician needs ample time to act on the prediction and quickly intervene on patients flagged as high-risk of being septic.",1. Introduction,[0],[0]
"Thus in building a system to predict sepsis we must consider timeliness of the prediction in addition to other metrics that quantify discrimination and accuracy.
",1. Introduction,[0],[0]
Our proposed methodology for detecting sepsis in multivariate clinical time series overcomes many of these limitations.,1. Introduction,[0],[0]
"Our approach hinges on constructing an end-toend classifier that takes in raw physiology time series data, transforms it through a Multitask Gaussian Process (MGP) to a more uniform representation on an evenly spaced grid, and feeds the latent function values through a deep recurrent neural network (RNN) to predict the binary outcome of whether or not the patient will become (or is already) septic.",1. Introduction,[0],[0]
"Setting up the problem in this way allows us to leverage the powerful representational abilities of RNNs, which typically requires standardized inputs at uniformly-spaced intervals, for our irregularly spaced multivariate clinical time series.",1. Introduction,[0],[0]
"As more information is made available during an encounter, the model can dynamically update its prediction about how likely it is that the patient will become septic.",1. Introduction,[0],[0]
"When the predicted probability of sepsis exceeds a predefined threshold (chosen to maximize predefined metrics such as sensitivity, positive predictive value, and timeliness), the model can be used to trigger an alarm.
",1. Introduction,[0],[0]
"We train our model with real patient data extracted from the Duke University Health System EHR, using a large cohort of heterogeneous inpatient encounters spanning 18 months.",1. Introduction,[0],[0]
"Our experiments show that using our method we can reliably predict the onset of sepsis roughly 4 hours in advance of the true occurrence, at a sensitivity of 0.85 and a precision of 0.64.",1. Introduction,[0],[0]
"The benefits of our MGP classification frame-
work are also apparent as there is a performance gain of 4.3% in area under the ROC curve and 11.1% in area under the Precision Recall curve, compared to the results of training an RNN to raw clinical data without a Gaussian Process to smoothly interpolate and impute missing values.",1. Introduction,[0],[0]
"Our overall performance is also substantially better than the most common early warnings scores from the medical literature, and in particular we perform significantly better than the NEWS score currently in use at our hospital.",1. Introduction,[0],[0]
These large gains in performance will translate to better patient outcomes and a lower burden on the overall health system when our model is deployed on the wards in the near future as part of a randomized trial.,1. Introduction,[0],[0]
There is a large body of works on the development and validation of early warning scores to predict clinical deterioration and other related outcomes.,2. Related Works,[0],[0]
"For instance, the MEWS score (Gardner-Thorpe et al., 2006) and NEWS score (Smith et al., 2013) are two of the more common scores used to assess overall clinical deterioration.",2. Related Works,[0],[0]
"In addition, the SIRS score for systemic inflammatory response syndrome was part of the original clinical definition of sepsis (Bone et al., 1992), although other scores designed for sepsis such as SOFA (Vincent et al., 1996) and qSOFA (Singer et al., 2016) have been more popular in recent years.",2. Related Works,[0],[0]
"A more sophisticated regression-based approach called the Rothman Index (Rothman et al., 2013) is also in widespread use for detecting overall deterioration.",2. Related Works,[0],[0]
"Finally, (Henry et al., 2015) used a Cox regression to predict sepsis using clinical time series data, although they do not account for temporal structure since they simply create feature and event-time pairs from raw data.
",2. Related Works,[0],[0]
There has been much recent interest within machine learning in developing models to predict future disease progression using EHRs.,2. Related Works,[0],[0]
"For instance, (Schulam & Saria, 2015) developed a longitudinal model for predicting progression of scleroderma, (Futoma et al., 2016) presented a joint model for predicting progression of chronic kidney disease and cardiac events, and (Liu et al., 2015) proposed a continuous-time hidden Markov model for progression of glaucoma.",2. Related Works,[0],[0]
"However, these models operate on a longer time scale, on the order of months to years, which is different for our setting that demands real-time predictions at an hourly level of granularity.",2. Related Works,[0],[0]
"The recent works of (Yoon et al., 2016) and (Hoiles & van der Schaar, 2016) are more relevant to our application, as they both developed models using clinical time series to predict a more general condition of clinical deterioration, as observed by admission to the Intensive Care Unit.
",2. Related Works,[0],[0]
"Although there has been some past methodological work on classification of multivariate time series, most of these
approaches rely on clustering using some form of ad-hoc distance metric between series, e.g. (Xing et al., 2012), and comparing a new series to observed clusters.",2. Related Works,[0],[0]
More similar to our work are several recent papers on using recurrent neural networks to classify clinical time series.,2. Related Works,[0],[0]
"In particular, (Lipton et al., 2016) used Long-Short Term Memory (LSTM) RNNs to predict diagnosis codes given physiological time series from the ICU, and (Choi et al., 2016) used Gated Recurrent Unit RNNs to predict onset of heart failure using categorical time series of diagnosis and procedural codes.",2. Related Works,[0],[0]
"Lastly, on a different note (Zhengping et al., 2016) also used a variant of Gated Recurrent Unit networks to investigate patterns of informative missingness in physiological ICU time series.
",2. Related Works,[0],[0]
There are several related works that also utilized multitask Gaussian processes in modeling multivariate physiological time series.,2. Related Works,[0],[0]
"For instance (Ghassemi et al., 2015) and (Durichen et al., 2015) used a similar model to ours, but instead focused more on forecasting of vitals to predict clinical instability, whereas our task is a binary classification to identify sepsis early.",2. Related Works,[0],[0]
"Finally, our end-to-end technique to discriminatively learn both the MGP and classifier parameters builds off of (Cheng-Xian Li & Marlin, 2016).",2. Related Works,[0],[0]
"However, our focus is more applied and the setting is more involved, as our time series are multivariate, of highly variable length, and may contain large amounts of missingness.",2. Related Works,[0],[0]
We frame the problem of early detection of sepsis as a multivariate time series classification problem.,3. Proposed Model,[0],[0]
"Given a new patient encounter, the goal is to continuously update the predicted probability that the encounter will result in sepsis, using all available information up until that time.",3. Proposed Model,[0],[0]
Figure 1 shows an overview of our approach.,3. Proposed Model,[0],[0]
"We first introduce
some notation, before presenting the details of the modeling framework, the learning algorithm, and the approximations to speed up learning and inference.
",3. Proposed Model,[0],[0]
"We suppose that our dataset D consists of N independent patient encounters, {Di}Ni=1.",3. Proposed Model,[0],[0]
"For each patient encounter i, we have a vector of baseline covariates available upon admission to the hospital, denoted bi ∈ RB , such as gender, age, and documented comorbidities.",3. Proposed Model,[0],[0]
At times ti =,3. Proposed Model,[0],[0]
"[ti1, ti2, . . .",3. Proposed Model,[0],[0]
", tiTi ] during the encounter we obtain information about M different types of vitals and laboratory tests that characterize the patient’s physiological state, where ti1 = 0 is the time of admission.",3. Proposed Model,[0],[0]
These longitudinal values are denoted Yi =,3. Proposed Model,[0],[0]
"[yi1, yi2, . . .",3. Proposed Model,[0],[0]
", yiM ] ∈ RTi×M , with yim ∈ RTi the vector of recorded values for variable m at each time.",3. Proposed Model,[0],[0]
"In practice, only a small fraction of this complete matrix is observed, since only a subset of the M variables are recorded at each observation time.",3. Proposed Model,[0],[0]
"We make no assumption about how long each encounter may last, so the length of the time series for each encounter is highly variable (Ti 6= Ti′ ) and these times are irregularly spaced, with each encounter having a unique set of observation times.",3. Proposed Model,[0],[0]
"Additionally, during the encounter, medications of P different classes are administered at Ui different times (and it is possible for Ui = 0).",3. Proposed Model,[0],[0]
"We denote this information as Pi = {(ui1,pi1), (ui2,pi2), . . .",3. Proposed Model,[0],[0]
", (uiUi ,piUi}, with pij ∈ {0, 1}P a binary vector denoting which of the P medications were administered at time uij .",3. Proposed Model,[0],[0]
"This infor-
mation is particularly valuable, because administration of medications provides some insight into a physician’s subjective impression of a patient’s health state by the type and quantity of medications ordered.",3. Proposed Model,[0],[0]
"Finally, each encounter in the training set is associated with a binary label oi ∈ {0, 1} denoting whether or not the patient acquired sepsis; we go into detail about how this is defined from the raw data in Section 4.1.",3. Proposed Model,[0],[0]
"Thus, the data for a single encounter can be summarized as Di = {bi, ti,Yi,Pi, oi}.",3. Proposed Model,[0],[0]
Gaussian processes (GPs) are a common choice for modeling irregularly spaced time series as they are naturally able to handle the variable spacing and differing number of observations per series.,3.1. Multitask Gaussian Processes,[0],[0]
"Additionally, they maintain uncertainty about the variance of the series at each point, which is important in our setting since the irregularity and missingness of clinical time series can lead to high uncertainty for variables that are infrequently (or perhaps never) observed, as is often the case.",3.1. Multitask Gaussian Processes,[0],[0]
"In order to account for the multivariate nature of our time series, we use a Multitask Gaussian Process (MGP) (Bonilla et al., 2008), an extension to GPs for handling multiple outputs at each time.",3.1. Multitask Gaussian Processes,[0],[0]
"Let fim(t) be a latent function representing the true values of physiological variable m for patient encounter i at time t. The MGP model places independent GP priors over the latent functions, with a shared correlation function kt over time.",3.1. Multitask Gaussian Processes,[0],[0]
"We assume each function has a prior mean of zero, so that the data has been centered.",3.1. Multitask Gaussian Processes,[0],[0]
"Then, we have:
cov(fim(t), fim′(t′))",3.1. Multitask Gaussian Processes,[0],[0]
"= KMmm′k t(t, t′) (1)
yim(t)",3.1. Multitask Gaussian Processes,[0],[0]
"∼ N (fim(t), σ2m) (2)
where yim(t) is the actual observed value.",3.1. Multitask Gaussian Processes,[0],[0]
"Equivalently, the likelihood for a fully observed multivariate time series of M measurements at T unique times is:
vec(Yi) ≡ yi ∼ N (0,Σi) (3)",3.1. Multitask Gaussian Processes,[0],[0]
Σi = K M ⊗KTi +D ⊗,3.1. Multitask Gaussian Processes,[0],[0]
"I, (4)
where yi is a stacked vector of all M longitudinal variables at the Ti observation times, and ⊗ denotes the Kronecker product.",3.1. Multitask Gaussian Processes,[0],[0]
"KM is a full-rank M × M covariance matrix specifying the relationships among the variables, crucially allowing information from more frequently sampled variables to help improve learning about the variables infrequently (or perhaps never) measured.",3.1. Multitask Gaussian Processes,[0],[0]
KTi is a Ti × Ti correlation matrix (the variance can be fully explained by KM ) for the observation times ti as specified by the correlation function kt with parameters η shared across all encounters.,3.1. Multitask Gaussian Processes,[0],[0]
"In this work we use the Ornstein-Uhlenbeck (OU) kernel function, kt(t, t′) = e−|t−t
′|/l, with a single length-scale parameter η = l.",3.1. Multitask Gaussian Processes,[0],[0]
"The OU kernel is useful for modeling noisy physiological data, as draws from the
corresponding stochastic process are only first-order continuous (Rasmussen & Williams, 2005).",3.1. Multitask Gaussian Processes,[0],[0]
"Finally, D is a diagonal matrix of noise variances {σ2m}Mm=1.",3.1. Multitask Gaussian Processes,[0],[0]
"In practice, only a subset of the M series are observed at each time, so the MTi ×MTi covariance matrix",3.1. Multitask Gaussian Processes,[0],[0]
Σi only needs to be computed at the observed values.,3.1. Multitask Gaussian Processes,[0],[0]
"This model is known in geostatistics as the intrinsic correlation model, since the covariance between different variables and between different points in time is explicitly separated, and is a special case of the linear model of coregionalization (Wackernagel, 1998).
",3.1. Multitask Gaussian Processes,[0],[0]
"The MGP can be used as a mechanism to handle the irregular spacing and missing values in the raw data, and output a uniform representation to feed into a black box classifier.",3.1. Multitask Gaussian Processes,[0],[0]
"To accomplish this, we define X to be a set of evenly spaced points in time (e.g. every hour) that will be shared across all encounters.",3.1. Multitask Gaussian Processes,[0],[0]
"For each encounter, we denote a subset of these points by xi = (xi1, xi2, . . .",3.1. Multitask Gaussian Processes,[0],[0]
", xiXi), so that xij = xi′j if both series are at least xij hours long.",3.1. Multitask Gaussian Processes,[0],[0]
"The MGP provides a posterior distribution for the M ×Xi matrix Zi of latent time series values at the grid times xi within this encounter, while also maintaining uncertainty over the values.",3.1. Multitask Gaussian Processes,[0],[0]
"If we let zi = vec(Zi), this posterior is also normally distributed with mean and covariance given by:
µzi = (K M ⊗KXiTi)Σ−1i yi (5) Σzi = (K M ⊗KXi)− (KM ⊗KXiTi)Σ−1i (KM ⊗KTiXi)
(6)
where KXiTi and KXi are correlation matrices between the grid times xi and observation times ti and between xi with itself, as specified by the correlation function kt.",3.1. Multitask Gaussian Processes,[0],[0]
"The set of MGP parameters to be learned are thus θ = (KM , {σ2m}Mm=1, η), and in this work we assume that they are shared across all encounters.",3.1. Multitask Gaussian Processes,[0],[0]
"The structured input Zi can then serve as a standardized input to the RNN, where the raw time series data has been interpolated and missing values imputed.",3.1. Multitask Gaussian Processes,[0],[0]
"We build off the ideas in (Cheng-Xian Li & Marlin, 2016) to learn a classifier that directly takes the latent function values zi at shared reference time points xi = {xij}Xij=1 as inputs.",3.2. Classification Method,[0],[0]
"The time series for each encounter i in our data can be represented as an MGP posterior distribution zi ∼ N(µzi ,Σzi ; θ) at times xi.",3.2. Classification Method,[0],[0]
"This information will be fed into a downstream black box classifier to learn the label of the time series.
",3.2. Classification Method,[0],[0]
"Since the lengths of each times series are variable, the classifier used must be able to account for variable length inputs, as the size of zi and xi will differ across encounters.",3.2. Classification Method,[0],[0]
"To this end, we turn to deep recurrent neural networks, a natural choice for learning flexible functions that
map variable-length input sequences to a single output.",3.2. Classification Method,[0],[0]
"In particular, we used a Long-Short Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997), as these classes of RNNs have been shown to be very flexible and have obtained excellent performance on a wide variety of problems.",3.2. Classification Method,[0],[0]
"At each time xij , a new set of inputs dij =",3.2. Classification Method,[0],[0]
"[z>ij ,b >",3.2. Classification Method,[0],[0]
"i ,p>ij ]> will be fed into the network, consisting of the M latent function values zij , the baseline covariates bi, and pij , a vector of counts of the S medications administered between xij and xi,j−1.",3.2. Classification Method,[0],[0]
"Thus, the RNN is able to learn complicated time-varying interactions among the static admission variables, the physiological labs and vitals, and administration of medications.
",3.2. Classification Method,[0],[0]
"If the function values zij were actually observed at each time xij , they could be directly fed into the RNN classifier along with the rest of the observed portion of the vector dij , and learning would be straightforward.",3.2. Classification Method,[0],[0]
"Let f(Di;w) denote the RNN classifier function, parameterized by w, that maps the (M + B + P )",3.2. Classification Method,[0],[0]
× Xi matrix of inputs Di to an output probability.,3.2. Classification Method,[0],[0]
"Learning the classifier given zi would involve learning the parameters w of the RNN by optimizing a loss function l(f(Di;w), oi) that compares the model’s prediction to the true label oi.",3.2. Classification Method,[0],[0]
"However, since zi is a random variable, this loss function to be optimized is itself a random variable.",3.2. Classification Method,[0],[0]
"Thus, the loss function that we will actually optimize is the expected loss Ezi∼N(µzi ,Σzi ;θ)[l(f(Di;w), oi)], with respect to the MGP posterior distribution of zi.",3.2. Classification Method,[0],[0]
"Then the overall learning problem is to minimize this loss function over the full dataset:
w∗,θ∗ = argminw,θ N∑ i=1 Ezi∼N(µzi ,Σzi ;θ)[l(f(Di;w), oi)].
(7)
",3.2. Classification Method,[0],[0]
"Given fitted model parameters w∗,θ∗, when we are given a new patient encounter Di′ for which we wish to predict whether or not it will become septic, we simply take Ezi′∼N(µzi′ ,Σzi′ ;θ∗)[f(Di′ ;w
∗)] as a risk score that can be updated continuously as more information is available.",3.2. Classification Method,[0],[0]
"This approach is “uncertainty-aware”, as the uncertainty in the MGP posterior for zi is propagated all the way through to the loss function.",3.2. Classification Method,[0],[0]
Variations on this setup exist by moving the expectation.,3.2. Classification Method,[0],[0]
"For instance, moving the expectation inside the classifier function f swaps the MGP mean vector µzi in place of zi in the RNN input Di.",3.2. Classification Method,[0],[0]
"This approach will be more computationally efficient but discards the uncertainty information in the time series, which may be undesirable in our setting of noisy clinical time series with high rates of missingness.",3.2. Classification Method,[0],[0]
The learning problem is to learn optimal parameters that minimize the loss in (7).,3.3. End to End Learning Framework,[0],[0]
"Since the expected loss
Ez∼N(µz,Σz ;θ)[l(f(D;w), o)] is intractable for our problem setup, we approximate the loss with Monte Carlo samples:
Ez∼N(µz,Σz ;θ)[l(f(D;w), o)]",3.3. End to End Learning Framework,[0],[0]
"≈ 1
S S∑ s=1 l(f(Ds;w), o),
(8)
Ds =",3.3. End to End Learning Framework,[0],[0]
"[Z>s ,B >,P>]>, vec(Zs) ≡",3.3. End to End Learning Framework,[0],[0]
"zs ∼ N(µz,Σz;θ)
(9)
where B and P are appropriately sized matrices of the baseline covariates and medication counts over time.
",3.3. End to End Learning Framework,[0],[0]
We need to compute gradients of this expression with respect to the RNN parameters w and the MGP parameters θ.,3.3. End to End Learning Framework,[0],[0]
"This can be achieved with the reparameterization trick, using the fact that z = µz +",3.3. End to End Learning Framework,[0],[0]
"Rξ, where ξ ∼ N(0, I) and R is a matrix such that Σz = RR> (Kingma & Welling, 2014).",3.3. End to End Learning Framework,[0],[0]
"This allows us to bring the gradients of (8) inside the expectation, where they can be computed efficiently.",3.3. End to End Learning Framework,[0],[0]
"Rather than choose R to be lower triangular so that it can only be computed in O(M3X3) time with a Cholesky decomposition, we follow (Cheng-Xian Li & Marlin, 2016) and letR be the symmetric matrix square root, as this leads to a scalable approximation to be discussed in Section 3.4.",3.3. End to End Learning Framework,[0],[0]
"Finally, we train our model discriminatively and end-to-end by jointly learning θ withw, as opposed to a two-stage approach that would first learn and fix θ before learning w.",3.3. End to End Learning Framework,[0],[0]
"The computation to both learn the model parameters and make predictions for a new patient encounter is dominated primarily by computing the parameters of the MGP in (5) and (6) and then drawing samples for z from it, as these are of dimension MX (where X is the number of reference time points).",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"To make this computation more amenable to large-scale datasets such as our large cohort of inpatient admissions, we use the Lanczos method to obtain approximate draws from large multivariate Gaussians.
",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"Recall that to draw from a multivariate Gaussian requires taking the product Σ1/2z ξ, where Σ 1/2 z is the symmetric matrix square root and ξ ∼ N(0, I).",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"We can approximate this product using the Lanczos method, a Krylov subspace approximation that bypasses the need to explicitly compute Σ1/2z and only requires matrix-vector products with Σz .",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"The main idea is to find an optimal approximation of Σ1/2z in the Krylov subspace Kk(Σz, ξ) = span{ξ,Σzξ, . . .",3.4. Scaling Computation with the Lanczos Method,[0],[0]
",Σk−1z ξ}; this approximation is simply the orthogonal projection of Σzξ into the subspace.",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"See (Chow & Saad, 2014) for more details on the use of Krylov methods for sampling multivariate Gaussians.",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"In practice, k is chosen to be a small constant, k <",3.4. Scaling Computation with the Lanczos Method,[0],[0]
<,3.4. Scaling Computation with the Lanczos Method,[0],[0]
"MX , so that the O(k3) operation of computing the matrix square root of a k × k tridiagonal matrix can effectively be treated as
Algorithm 1 Lanczos Method to approximate Σ1/2ξ Input: covariance matrix Σ, random vector ξ, k β1 = 0 and d0 = 0",3.4. Scaling Computation with the Lanczos Method,[0],[0]
d1 =,3.4. Scaling Computation with the Lanczos Method,[0],[0]
ξ/||ξ|| for j = 1 to k do d = Σdj − βjdj−1 αj = d > j,3.4. Scaling Computation with the Lanczos Method,[0],[0]
"d
d = d− αjdj βj+1 = ||d|| dj+1 = d/βj+1
end for D =",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"[d1, . . .",3.4. Scaling Computation with the Lanczos Method,[0],[0]
",dk] H = tridiagonal(β2",3.4. Scaling Computation with the Lanczos Method,[0],[0]
":k,α1:k,β2:k) Return: ||ξ||DH1/2e1 // e1 =",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"[1, 0, . .",3.4. Scaling Computation with the Lanczos Method,[0],[0]
.,3.4. Scaling Computation with the Lanczos Method,[0],[0]
", 0]>
O(1).",3.4. Scaling Computation with the Lanczos Method,[0],[0]
The most expensive step in the Lanczos method then becomes computation of matrix-vector products Σzd.,3.4. Scaling Computation with the Lanczos Method,[0],[0]
"To compute these we use the conjugate gradient algorithm, another Krylov method, and it usually converges in only a few iterations.",3.4. Scaling Computation with the Lanczos Method,[0],[0]
We also use conjugate gradient when computing µz in (5) to approximate Σ−1y.,3.4. Scaling Computation with the Lanczos Method,[0],[0]
"Importantly, every operation in both the Lanczos method (Algorithm 1) and the conjugate gradient algorithm are differentiable, so that it is possible to backpropagate through the entire procedure during training with automatic differentiation.",3.4. Scaling Computation with the Lanczos Method,[0],[0]
"Our dataset consists of 49,312 inpatient admissions from our university health system spanning 18 months, extracted directly from our EHR.",4.1. Data Description,[0],[0]
"After extensive data cleaning, there are M = 34 physiological variables, of which 5 are vitals, and 29 are laboratory values (see Figure 2), and they vary considerably in the number of encounters with at least one recorded measurement.",4.1. Data Description,[0],[0]
"At least one value for each of the vital variables is measured in over 99% of encounters, while some labs (e.g. Ammonia, ESR, D-Dimer) are very rarely taken, being measured in only 2-4% of encounters, with most of the rest falling somewhere in the middle.",4.1. Data Description,[0],[0]
"There were b = 35 baseline covariates reliably measured upon admission (e.g. age, race, gender, whether the admission was a transfer or urgent, comorbidities upon admission).",4.1. Data Description,[0],[0]
"Finally, we have information on P = 8 medication classes, where these classes were determined from a thorough review of the raw medication names in the EHR.",4.1. Data Description,[0],[0]
"The patient encounters range from very short admissions of only a few hours to extended stays lasting multiple months, with the mean length of stay at 121.7 hours, with a standard deviation of 108.1 hours.",4.1. Data Description,[0],[0]
"As there was no specific inclusion or exclusion criteria in the creation of this patient cohort, the resulting population is very heterogeneous and can vary
tremendously in clinical status.",4.1. Data Description,[0],[0]
"This makes the dataset representative of the real clinical setting in which our method will be used, across the entire inpatient wards.",4.1. Data Description,[0],[0]
"Before modeling we log transform allM physiological time series variables to reduce the effect of outliers, and then center and scale all continuous-valued inputs into the model.
",4.1. Data Description,[0],[0]
"For encounters that ultimately resulted in sepsis, we used a well-defined clinical definition to assess the first time at which sepsis is suspected to have been present.",4.1. Data Description,[0],[0]
"This criteria consisted of at least two consistently abnormal vitals signs, along with a blood culture drawn for a suspected infection, and at least one abnormal laboratory value indicating early signs of organ failure.",4.1. Data Description,[0],[0]
This definition was carefully reviewed and found to be sufficient by clinicians.,4.1. Data Description,[0],[0]
Thus each encounter is associated with a binary label indicating whether or not that patient ever acquired sepsis; the prevalence of sepsis in our full dataset was 21.4%.,4.1. Data Description,[0],[0]
"We train our method to 80% of the full dataset, setting aside 10% as a validation set to select hyperparameters and a final 10% for testing.",4.2. Experimental Setup,[0],[0]
"For the encounters that result in sepsis, we throw away data from after sepsis was acquired, as our clinical goal is to be able to predict sepsis before it happens for a new patient.",4.2. Experimental Setup,[0],[0]
For non-septic encounters we train on the full length of the encounter until discharge.,4.2. Experimental Setup,[0],[0]
"We choose the shared reference times X to be evenly spaced at every hour starting at admission, as clinically the desire is for a risk score that will refresh only once an hour.
",4.2. Experimental Setup,[0],[0]
"We compared our method (denoted “MGP-RNN”) against several baselines, including several common clinical scoring systems, as well as more complex methods.",4.2. Experimental Setup,[0],[0]
"In particular, we compared our model with the NEWS score currently in use at our hospital, along with the MEWS score and the SIRS score.",4.2. Experimental Setup,[0],[0]
Each of these scores are based off of a small subset of the total variables available to our methods.,4.2. Experimental Setup,[0],[0]
"In particular, MEWS uses five, NEWS uses seven, and SIRS uses four.",4.2. Experimental Setup,[0],[0]
"These clinical benchmarks all assign independent scores to each variable under consideration, with higher scores given for more abnormal values, although they each use different thresholds and different values.
",4.2. Experimental Setup,[0],[0]
"As a strong comparison to our end-to-end MGP-RNN classifier, we also trained an LSTM recurrent neural network from the raw data alone (denoted “Raw RNN” in Figure 3), with the same number of layers and hidden units as the network in our classifier (2 layers with 64 hidden units per layer).",4.2. Experimental Setup,[0],[0]
"The mean value for each vital and lab was taken in hourly windows, and windows with missing values carried the most recent value forward.",4.2. Experimental Setup,[0],[0]
"If there was no previously observed variable yet in that encounter, we imputed the mean.",4.2. Experimental Setup,[0],[0]
"In addition, we trained an L2 penalized logistic regression baseline (“PLR”) using this imputation mecha-
nism.
",4.2. Experimental Setup,[0],[0]
"We also compare against a simplified version of the end-toend MGP-RNN framework, (denoted “MGP-RNN-mean”) where we replace the latent MGP function values z with their expectation µz during both training and testing, to test the effect of discarding the extra uncertainty information.",4.2. Experimental Setup,[0],[0]
"Finally, to demonstrate the added value of using an MGP instead of independent GPs for each physiological variable, we trained two end-to-end GP-RNN baselines using (7), the same loss function as the MGP-RNN.",4.2. Experimental Setup,[0],[0]
"The first, “GPRNN-shared”, is equivalent to an MGP with KM = I , i.e. no covariances across variables, and all variables share the same length-scale in the OU kernel.",4.2. Experimental Setup,[0],[0]
"The second, “GPRNN-indep”, is considerably stronger as it also has no covariances across variables, but allows each GP prior on the latent functions fm to have its own length scale in its OU kernel, i.e. lm 6= lm′ .",4.2. Experimental Setup,[0],[0]
"To guard against overfitting we apply early stopping on the validation set, and use mild L2 regularization for all RNNbased methods.",4.2. Experimental Setup,[0],[0]
"We train all models using stochastic gradient descent with the ADAM optimizer (Kingma & Ba, 2015) using minibatches of 100 encounters at a time and a learning rate of 0.001.",4.2. Experimental Setup,[0],[0]
To approximate the expectation in (8) we draw ten Monte Carlo samples.,4.2. Experimental Setup,[0],[0]
We implemented our methods in Tensorflow1.,4.2. Experimental Setup,[0],[0]
"On a server with 63GB RAM and 12 Intel Xeon E5-2680 2.50GHz CPUs, the MGP-RNN method takes roughly 10 hours per epoch on the training set, and takes on average 0.3 seconds to evaluate a test case
1https://github.com/jfutoma/MGP-RNN
and generate a risk score.",4.2. Experimental Setup,[0],[0]
All methods converged in a small number of epochs.,4.2. Experimental Setup,[0],[0]
We use several different metrics to evaluate performance.,4.3. Evaluation Metrics,[0],[0]
"The area under the Receiver Operating Characteristic (ROC) curve (AU-ROC) is an overall measure of discrimination, and can be interpreted as the probability that the classifier correctly ranks a random sepsis encounter as higher risk than a random non-sepsis encounter.",4.3. Evaluation Metrics,[0],[0]
We also report the area under the Precision Recall (PR) curve (AUPR).,4.3. Evaluation Metrics,[0],[0]
"Importantly, we examine how these metrics vary as we change the window in which we make the prediction to see how far in advance we can reliably predict onset of sepsis.",4.3. Evaluation Metrics,[0],[0]
Our results show that the MGP-RNN classification framework yields clear performance gains when compared to the various baselines considered.,4.4. Results,[0],[0]
"It substantially outperforms the overly simplistic clinical scores, and demonstrates modest gains over the RNN trained to raw data, the MGP-RNN-mean method that discards uncertainty information, and the univariate GP baselines.
",4.4. Results,[0],[0]
Figure 3 summarizes the results.,4.4. Results,[0],[0]
"The four MGP/GP-RNN methods are in solid lines, and the other baselines are dashed.",4.4. Results,[0],[0]
"It is clear that these four methods perform considerably better than the other methods, especially in the last four hours prior to sepsis/discharge.
",4.4. Results,[0],[0]
The left and middle panes of Figure 3 display the AU-ROC and AU-PR for each method as a function of the number of hours in advance the prediction is made.,4.4. Results,[0],[0]
"Generally the MGP-RNN performs best, followed by the three other MGP/GP-RNN baselines.",4.4. Results,[0],[0]
"This is likely because it retains uncertainty information about the noisy time series (unlike MGP-RNN-mean), and can learn correlations among the different physiological variables to improve the quality of the imputation (unlike the GP-RNN methods).",4.4. Results,[0],[0]
"As expected, the GP-RNN-indep baseline consistently outperforms the simpler GP-RNN-shared.
",4.4. Results,[0],[0]
The right pane of Figure 3 shows the tradeoff between precision and timeliness for a fixed sensitivity of 0.85 across the methods.,4.4. Results,[0],[0]
"It is most useful to evaluate with such a high sensitivity as this is the setting clinicians typically want to use a risk score, in order not to miss many cases.",4.4. Results,[0],[0]
"The MGPRNN performs comparably to the MGP-RNN-mean within a few hours of sepsis, and demonstrates the biggest performance gains from about 3 to 7 hours beforehand.",4.4. Results,[0],[0]
"Throughout, it has much higher precision than NEWS, MEWS, and SIRS, especially so in the few hours immediately preceding sepsis.",4.4. Results,[0],[0]
"This is a very important clinical point, since clinicians want a method with very high precision and a low false alarm rate to reduce the alarm fatigue experienced with current solutions.",4.4. Results,[0],[0]
"Furthermore, being able to detect sepsis even a few hours early might substantially increase treatment effectiveness and improve patient outcomes.",4.4. Results,[0],[0]
We have presented a novel approach for early detection of sepsis that classifies multivariate clinical time series in a manner that is both flexible and takes into account the uncertainty in the series.,5. Conclusions and Clinical Significance,[0],[0]
"On a large dataset of inpatient encounters from our university health system, we find that our proposed method substantially outperforms strong baselines and a number of widespread clinical benchmarks.",5. Conclusions and Clinical Significance,[0],[0]
"In particular, our methods tend to have much higher precision, and thus they have much lower rates of false alarm.",5. Conclusions and Clinical Significance,[0],[0]
"For instance, at a very high sensitivity of 0.85 and when making predictions 4 hours in advance, there will be only roughly 0.5 false alarms for every true alarm generated by our approach, whereas for the NEWS score currently being used at our institution, there will be about 2.5 false alarms for every true alarm.",5. Conclusions and Clinical Significance,[0],[0]
"Thus, adoption of our method would result in a drastic reduction in the total number of false alarms.
",5. Conclusions and Clinical Significance,[0],[0]
"In addition to the initial promise of our approach, there are a number of interesting directions to extend the proposed method to better account for various aspects of our data source.",5. Conclusions and Clinical Significance,[0],[0]
"In particular, we could incorporate a clustering component with different sets of MGPs for different latent subpopulations of encounters, to address high heterogeneity across patients.",5. Conclusions and Clinical Significance,[0],[0]
"The medication data might be
better utilized to also learn the effect of medications on the physiological time series.",5. Conclusions and Clinical Significance,[0],[0]
"For instance, certain medications might have a sharp effect on certain vitals signs to help stabilize them; such treatment response curves could be learned observationally and applied to help improve predictions.",5. Conclusions and Clinical Significance,[0],[0]
"More sophisticated covariance structure in the multitask Gaussian process would allow for a more flexible model, since our assumption of a correlation function shared across all physiological streams may be overly restrictive.",5. Conclusions and Clinical Significance,[0],[0]
"Finally, use of additional approximations from the GP literature may further decrease the computational overhead and improve training times.
",5. Conclusions and Clinical Significance,[0],[0]
"This work has the potential to have a high impact in improving clinical practice in the identification of sepsis, both at our institution and elsewhere.",5. Conclusions and Clinical Significance,[0],[0]
"The underlying biological mechanism is poorly understood, and the problem has historically been very difficult for clinicians.",5. Conclusions and Clinical Significance,[0],[0]
"Use of a model such as ours to predict onset of sepsis would significantly reduce the alarm fatigue associated with current clinical scores, and could both significantly improve patient outcomes and reduce burden on the health system.",5. Conclusions and Clinical Significance,[0],[0]
"Although in this work our emphasis was on early detection of sepsis, the methods could be modified with minimal effort to apply to detection of other clinical events of interest, such as cardiac arrests, code blue events, admission to the ICU, and cardiogenic shock.",5. Conclusions and Clinical Significance,[0],[0]
We are currently working to implement our methods directly into an application that can pull live data from our health system’s EHR and present our model’s predictions to a rapid response team.,5. Conclusions and Clinical Significance,[0],[0]
"This will allow us to apply our methods in a real-time clinical setting and their utility can be proven empirically, as data is collected on how accurate the alarms it raises are and how it is used on the actual wards.",5. Conclusions and Clinical Significance,[0],[0]
Joseph Futoma is supported by an NDSEG fellowship.,Acknowledgements,[0],[0]
Katherine Heller is supported by an NSF CAREER award.,Acknowledgements,[0],[0]
This project was partially funded by the Duke Institute for Health Innovation.,Acknowledgements,[0],[0]
"Thanks to the other members of our team for their invaluable work on this project: Cara O’Brien MD, Meredith Clement MD, Armando Bedoya MD, Mark Sendak MD, Nathan Brajer, and Bryce Wolery.",Acknowledgements,[0],[0]
"We present a scalable end-to-end classifier that uses streaming physiological and medication data to accurately predict the onset of sepsis, a life-threatening complication from infections that has high mortality and morbidity.",abstractText,[0],[0]
"Our proposed framework models the multivariate trajectories of continuous-valued physiological time series using multitask Gaussian processes, seamlessly accounting for the high uncertainty, frequent missingness, and irregular sampling rates typically associated with real clinical data.",abstractText,[0],[0]
"The Gaussian process is directly connected to a black-box classifier that predicts whether a patient will become septic, chosen in our case to be a recurrent neural network to account for the extreme variability in the length of patient encounters.",abstractText,[0],[0]
We show how to scale the computations associated with the Gaussian process in a manner so that the entire system can be discriminatively trained end-to-end using backpropagation.,abstractText,[0],[0]
"In a large cohort of heterogeneous inpatient encounters at our university health system we find that it outperforms several baselines at predicting sepsis, and yields 19.4% and 55.5% improved areas under the Receiver Operating Characteristic and Precision Recall curves as compared to the NEWS score currently used by our hospital.",abstractText,[0],[0]
Learning to Detect Sepsis with a Multitask Gaussian Process RNN Classifier,title,[0],[0]
Probabilistic graphical models provide a powerful framework to describe the dependencies between a set of variables.,1 Introduction,[0],[0]
"Many applications infer the structure of a probabilis-
1ESAT-PSI, KU Leuven 2INRIA 3University of Paris-Saclay 4University of Montreal.",1 Introduction,[0],[0]
"Correspondence to: Eugene Belilovsky <eugene.belilovsky@inria.fr>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1 Introduction,[0],[0]
tic graphical model from data to elucidate the relationships between variables.,1 Introduction,[0],[0]
These relationships are often represented by an undirected graphical model also known as a Markov Random Field (MRF).,1 Introduction,[0],[0]
"We focus on a common MRF model, Gaussian graphical models (GGMs).",1 Introduction,[0],[0]
"GGMs are used in structure-discovery settings for rich data such as neuroimaging, genetics, or finance (Friedman et al., 2008; Ryali et al, 2012; Mohan et al., 2012; Belilovsky et al., 2016).",1 Introduction,[0],[0]
"Although multivariate Gaussian distributions are well-behaved, determining likely structures from few examples is a difficult task when the data is high dimensional.",1 Introduction,[0],[0]
"It requires strong priors, typically a sparsity assumption, or other restrictions on the structure of the graph, which now make the distribution difficult to express analytically and use.
",1 Introduction,[0],[0]
"A standard approach to estimating structure with GGMs in high dimensions is based on the classic result that the zeros of a precision matrix correspond to zero partial correlation, a necessary and sufficient condition for conditional independence (Lauritzen, 1996).",1 Introduction,[0],[0]
"Assuming only a few conditional dependencies corresponds to a sparsity constraint on the entries of the precision matrix, leading to a combinatorial problem.",1 Introduction,[0],[0]
Many popular approaches to learning GGMs can be seen as leveraging the `1-norm to create convex surrogates to this problem.,1 Introduction,[0],[0]
"Meinshausen & Bühlmann (2006) use nodewise `1 penalized regressions, while other estimators penalize the precision matrix directly (Cai et al., 2011; Friedman et al., 2008; Ravikumar et al., 2011), the most popular being the graphical lasso
fgl(Σ̂) = arg min Θ 0",1 Introduction,[0],[0]
− log |Θ|+,1 Introduction,[0],[0]
"Tr (Σ̂Θ) + λ‖Θ‖1 (1)
which can be seen as a penalized maximum-likelihood estimator.",1 Introduction,[0],[0]
"Here Θ and Σ̂ are the precision and sample covariance matrices, respectively.",1 Introduction,[0],[0]
"A large variety of alternative penalties extend the priors of the graphical lasso (Danaher et al., 2014; Ryali et al, 2012; Varoquaux et al., 2010).",1 Introduction,[0],[0]
"However, this strategy faces several challenges.",1 Introduction,[0],[0]
"Constructing novel surrogates for structured-sparsity assumptions on MRF structures is difficult, as priors need to be formulated and incorporated into a penalized maximum likelihood objective which then calls for the development of an efficient optimization algorithm, often within a separate research effort.",1 Introduction,[0],[0]
"Furthermore, model selection in a penalized maximum likelihood setting is difficult as regularization parameters are often unintuitive.
",1 Introduction,[0],[0]
"ar X
iv :1
60 5.
06 35
9v 3
[ st
at .M
L ]
3 A
ug 2
01 7
We propose to learn the estimator.",1 Introduction,[0],[0]
"Rather than manually designing a specific graph-estimation procedure, we frame this estimator-engineering problem as a learning problem, selecting a function from a large flexible function class by risk minimization.",1 Introduction,[0],[0]
This allows us to construct a loss function that explicitly aims to recover the edge structure.,1 Introduction,[0],[0]
"Indeed, sampling from a distribution of graphs and empirical covariances with desired properties is often possible, even when this distribution is not analytically tractable.",1 Introduction,[0],[0]
As such we can perform empirical risk minimization to select an appropriate function for edge estimation.,1 Introduction,[0],[0]
"Such a framework gives more control on the assumed level of sparsity (as opposed to graph lasso) and can impose structure on the sampling to shape the expected distribution, while optimizing a desired performance metric.
",1 Introduction,[0],[0]
"For particular cases we show that the problem of interest can be solved with a polynomial function, which is learnable with a neural network (Andoni et al., 2014).",1 Introduction,[0],[0]
"Motivated by this fact, as well as theoretical and empricial results on learning smooth functions approximating solutions to combinatorial problems (Cohen et al., 2016; Vinyals et al., 2015), we propose to use a particular convolutional neural network as the function class.",1 Introduction,[0],[0]
"We train it by sampling small datasets, generated from graphs with the prescribed properties, with a primary focus on sparse graphical models.",1 Introduction,[0],[0]
"We estimate from this data small-sample covariance matrices (n < p), where n is the number of samples and p is the dimensionality of the data.",1 Introduction,[0],[0]
Then we use them as training data for the neural network (Figure 2) where target labels are indicators of present and absent edges in the underlying GGM.,1 Introduction,[0],[0]
"The learned network can then be employed in various real-world structure discovery problems.
",1 Introduction,[0],[0]
In Section 1.1 we review the related work.,1 Introduction,[0],[0]
In Section 2 we formulate the risk minimization view of graph-structure inference and describe how it applies to sparse GGMs.,1 Introduction,[0],[0]
Section 2.3 describes and motivates the deep-learning architecture we chose to use for the sparse GGM problem in this work.,1 Introduction,[0],[0]
In Section 3 we describe the details of how we train an edge estimator for sparse GGMs.,1 Introduction,[0],[0]
We then evaluate its properties extensively on simulation data.,1 Introduction,[0],[0]
"Finally, we show that this edge estimator trained only on synthetic data can obtain state of the art performance at inference time on real neuroimaging and genetics problems, while being much faster to execute than other methods.",1 Introduction,[0],[0]
Lopez-Paz et al. (2015) analyze learning functions to identify the structure of directed graphical models in causal inference using estimates of kernel-mean embeddings.,1.1 Related Work,[0],[0]
"As in our work, they demonstrate the use of simulations for training while testing on real data.",1.1 Related Work,[0],[0]
"Unlike our work, they primarily focus on finding the causal direction in two node
graphs with many observations.
",1.1 Related Work,[0],[0]
Our learning architecture is motivated by the recent literature on deep networks.,1.1 Related Work,[0],[0]
"Vinyals et al. (2015) have shown that neural networks can learn approximate solutions to NPhard combinatorial problems, and the problem of optimal edge recovery in MRFs can be seen as a combinatorial optimization problem.",1.1 Related Work,[0],[0]
"Several recent works have proposed neural architectures for graph input data (Henaff et al., 2015; Duvenaud et al, 2015; Li et al., 2016).",1.1 Related Work,[0],[0]
"These are based on multi-layer convolutional networks, as in our work, or multistep recurrent neural networks.",1.1 Related Work,[0],[0]
"The input in our approach can be viewed as a complete graph, while the output is a sparse graph, thus none of these are directly applicable.",1.1 Related Work,[0],[0]
"Related to our work, Balan et al. (2015) use deep networks to approximate a posterior distribution.",1.1 Related Work,[0],[0]
"Finally, Gregor & LeCun (2010); Xin et al. (2016) use deep networks to approximate steps of a known sparse recovery algorithm.
",1.1 Related Work,[0],[0]
Bayesian approaches to structure learning rely on priors on the graph combined with sampling techniques to estimate the posterior of the graph structure.,1.1 Related Work,[0],[0]
"Some approaches make assumptions on the decomposability of the graph (Moghaddam et al., 2009).",1.1 Related Work,[0],[0]
"The G-Wishart distribution is a popular distribution which forms part of a framework for structure inference, and advances have been recently made in efficient sampling (Mohammadi & Wit, 2015).",1.1 Related Work,[0],[0]
"These methods can still be rather slow compared to competing methods, and in the setting of p > n",1.1 Related Work,[0],[0]
we find they are less powerful.,1.1 Related Work,[0],[0]
We consider MRF edge estimation as a learnable function.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
Let X ∈ Rn×p be a matrix whose n rows are i.i.d.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"samples x ∼ P (x) of dimension p. LetG = (V,E) be an undirected and unweighted graph associated with the set of variables in x.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Let L = {0, 1} and Ne = p(p−1)2 the maximum possible edges in E. Let Y ∈ LNe indicate the presence or absence of edges in the edge set E of G, namely
Y ij = { 0 xi ⊥ xj |xV \i,j 1",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
xi,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"6⊥ xj |xV \i,j .
(2)
We define an approximate structure discovery method gw(X), which predicts the edge structure, Ŷ = gw(X), given a sample of data X .",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
We focus on X drawn from a Gaussian distribution.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"In this case, the empirical covariance matrix, Σ̂, is a sufficient statistic of the population covariance and therefore of the conditional dependency structure.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
We thus express our structure-recovery problem as a function of Σ̂: gw(X) := fw(Σ̂).,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
fw is parametrized by w and belongs to the function class F .,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Note that the graphical lasso in Equation (1) is an fw for a specific choice of F .
",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
This view on the edge estimator now allows us to bring the selection of fw from the domain of human design to the domain of empirical risk minimization over F .,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Defining a distribution P on Rp×p × LNe such that (Σ̂, Y ) ∼ P, we would like our estimator, fw, to minimize the expected risk
R(f) = E(Σ̂,Y )∼P[l(f(Σ̂), Y )].",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"(3)
",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
Here l : LNe × LNe → R+ is the loss function.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"For graphical model selection the 0/1 loss function is the natural error metric to consider (Wang et al., 2010).",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"The estimator with minimum risk is generally not possible to compute as a closed form expression for most interesting choices of P, such as those arising from sparse graphs.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"In this setting, Eq. (1) achieves the information theoretic optimal recovery rate up to a constant for certain P corresponding to uniformly sparse graphs with a maximum degree, but only when the optimal λ is used and the non-zero precision matrix values are bounded away from zero (Wang et al., 2010; Ravikumar et al., 2011).
",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
The design of the estimator in Equation (1) is not explicitly minimizing this risk functional.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
Thus modifying the estimator to fit a different class of graphs (e.g. small-world networks) while minimizing R(f) is not obvious.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Furthermore, in practical settings the optimal λ is unknown and precision matrix entries can be very small.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
We would prefer to directly minimize the risk functional.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Desired structural assumptions on samples from P on the underlying graph, such as sparsity, may imply that the distribution is not tractable for analytic solutions.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Meanwhile, we can often devise a sampling procedure for P allowing us to select an appropriate function via empirical risk minimization.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"Thus it is sufficient to define a rich enough F over which we can minimize the empirical risk over the samples generated, giving us a learning objective over N samples {Yk,Σk}Nk=1 drawn from P: min w 1 N ∑N",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"k=1 l(fw(Σ̂k), Yk).",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"To maintain tractability, we use the standard cross-entropy loss as a convex surrogate, l̂ : RNe × LNe , given by∑
i 6=j
( Y ij log(f ijw (Σ̂))",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"+ (1− Y ij) log(1− f ijw (Σ̂)) ) .
",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
"We now need to select a sufficiently rich function class for fw and a method to produce appropriate (Y, Σ̂) which model our desired data priors.",2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
This will allow us to learn a fw that explicitly attempts to minimize errors in edge discovery.,2.1 Learning an Approximate Edge Estimation Procedure,[0],[0]
We discuss how the described approach can be applied to recover sparse Gaussian graphical models.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
A typical assumption in many modalities is that the number of edges is sparse.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"A convenient property of these GGMs is that the
precision matrix has a zero value in the (i,",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
j)th entry precisely when variables i and j are independent conditioned on all others.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Additionally, the precision matrix and partial correlation matrix have the same sparsity pattern, while the partial correlation matrix has normalized entries.
",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"We propose to simulate our a priori assumptions of sparsity and Gaussianity to learn fw(Σ̂), which can then produce predictions of edges from the input data.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"We model P (x|G) as arising from a sparse prior on the graph G and correspondingly the entries of the precision matrix Θ. To obtain a single sample of X corresponds to n i.i.d. samples from N (0,Θ−1).",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"We can now train fw(Σ̂) by generating sample pairs (Σ̂, Y ).",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
At execution time we standardize the input data and compute the covariance matrix before evaluating fw(Σ̂).,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"The process of learning fw for the sparse GGM is given in Algorithm 1.
",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Algorithm 1 Training a GGM edge estimator for i ∈ {1, .., N} do
Sample Gi ∼ P(G)",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Sample Θi ∼ P(Θ|G = Gi) Xi ← {xj ∼ N(0,Θ−1i )} n j=1
Construct (Yi, Σ̂i) pair from (Gi,Xi) end for Select Function Class F (e.g. CNN)",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Optimize: min
f∈F 1 N
∑N k=1 l̂(f(Σ̂k), Yk))
",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"A weakly-informative sparsity prior is one where each edge is equally likely with small probability, versus structured sparsity where edges have specific configurations.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"For obtaining the training samples (Σ̂, Y ) in this case we would like to create a sparse precision matrix, Θ, with the desired number of zero entries distributed uniformly.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
One strategy to do this and assure the precision matrices lie in the positive definite cone is to first construct an upper triangular sparse matrix and then multiply it by its transpose.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
This process is described in detail in the experimental section.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Alternatively, an MCMC based G-Wishart distribution sampler can be employed if specific structures of the graph are desired (Lenkoski, 2013).
",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
The sparsity patterns in real data are often not uniformly distributed.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
Many real world networks have a small-world structure: graphs that are sparse and yet have a comparatively short average distance between nodes.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
These transport properties often hinge on a small number of high-degree nodes called hubs.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Normally, such structural patterns require sophisticated adaptation when applying estimators like Eq.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
(1).,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Indeed, high-degree nodes break the smallsample, sparse-recovery properties of `1-penalized estimators (Ravikumar et al., 2011).",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
In our framework such structural assumptions appear as a prior that can be learned offline during training of the prediction function.,2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"Similarly priors on other distributions such as general exponential
families can be more easily integrated.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
"As the structure discovery model can be trained offline, even a slow sampling procedure may suffice.",2.2 Discovering Sparse GGMs and Beyond,[0],[0]
In this work we propose to use a neural network as our function fw.,2.3 Neural Network Graph Estimator,[0],[0]
To motivate this let us consider the extreme case when n,2.3 Neural Network Graph Estimator,[0],[0]
p.,2.3 Neural Network Graph Estimator,[0],[0]
In this case Σ̂ ≈ Σ and thus entries of Σ̂−1 or the partial correlation that are almost equal to zero can give the edge structure.,2.3 Neural Network Graph Estimator,[0],[0]
We can show that a neural network is consistent with this limiting case.,2.3 Neural Network Graph Estimator,[0],[0]
Definition 1 (P-consistency).,2.3 Neural Network Graph Estimator,[0],[0]
A function class F is Pconsistent,2.3 Neural Network Graph Estimator,[0],[0]
"if ∃f ∈ F such that E(Σ̂,Y )∼P[l(f(Σ̂), Y )]",2.3 Neural Network Graph Estimator,[0],[0]
→ 0 as n→∞,2.3 Neural Network Graph Estimator,[0],[0]
with high probability.,2.3 Neural Network Graph Estimator,[0],[0]
Proposition 1 (Existence of P-consistent neural network graph estimator).,2.3 Neural Network Graph Estimator,[0],[0]
There exists a feed forward neural network function class F that is P-consistent.,2.3 Neural Network Graph Estimator,[0],[0]
Proof.,2.3 Neural Network Graph Estimator,[0],[0]
"If the data is standardized, each entry of Σ corresponds to the correlation ρi,j .",2.3 Neural Network Graph Estimator,[0],[0]
"The partial correlation of edge (i, j) conditioned on nodes Z, is given recursively as
ρi,j|Z = (ρi,j|Z\zo − ρi,zo|Z\zoρj,zo|Z\zo) 1
D .",2.3 Neural Network Graph Estimator,[0],[0]
"(4)
We may ignore the denominator, D, as we are interested in I(ρi,j|Z = 0).",2.3 Neural Network Graph Estimator,[0],[0]
Thus we are left with a recursive formula that yields a high degree polynomial.,2.3 Neural Network Graph Estimator,[0],[0]
"From Andoni et al. (2014, Theorem 3.1) using gradient descent, a neural network with only two layers can learn a polynomial function of degree d to arbitrary precision given sufficient hidden units.
",2.3 Neural Network Graph Estimator,[0],[0]
Remark 1.,2.3 Neural Network Graph Estimator,[0],[0]
Naïvely the polynomial from the recursive definition of partial correlation is of degree bounded by 2p−2.,2.3 Neural Network Graph Estimator,[0],[0]
"In the worst case, this would seem to imply that we would need an exponentially growing number of hidden nodes to approximate it.",2.3 Neural Network Graph Estimator,[0],[0]
"However, this problem has a great deal of structure that can allow efficient approximation.",2.3 Neural Network Graph Estimator,[0],[0]
"Firstly, higher order monomials will go to zero quickly with a uniform prior on ρi,j , which takes values between 0 and 1, suggesting that in many cases a concentration bound exists that guarantees non-exponential growth.",2.3 Neural Network Graph Estimator,[0],[0]
"Furthermore, the existence result is shown already for a shallow network, and we expect a logarithmic decrease in the number of parameters to peform function estimation with a deep network (Cohen et al., 2016).
",2.3 Neural Network Graph Estimator,[0],[0]
"Moreover, there are a great deal of redundant computations in Eq.",2.3 Neural Network Graph Estimator,[0],[0]
(4) and an efficient dynamic programming implementation can yield polynomial computation time and require only low order polynomial computations with appropriate storage of previous computation.,2.3 Neural Network Graph Estimator,[0],[0]
Similarly we would like to design a network that would have capacity to re-use computations across edges and approximate low order polynomials.,2.3 Neural Network Graph Estimator,[0],[0]
"We also observe that the conditional independence of nodes
i, j given Z can be computed equivalently in many ways by considering many paths through the nodes Z.",2.3 Neural Network Graph Estimator,[0],[0]
"Thus we can choose any valid ordering for traversing the nodes starting from a given edge.
",2.3 Neural Network Graph Estimator,[0],[0]
We now describe an efficient architecture for this problem which uses a series of shared operations at each edge.,2.3 Neural Network Graph Estimator,[0],[0]
"We consider a feedforward network where each edge i, j is associated with a vector, oki,j , at each layer k > 0.",2.3 Neural Network Graph Estimator,[0],[0]
"For each edge, i, j, we start with a neighborhood of the 6 adjacent nodes, i, j, i-1, i+1, j-1, j+1 for which we take all corresponding edge values from the covariance matrix and construct o1i,j .",2.3 Neural Network Graph Estimator,[0],[0]
"We proceed at each layer to increase the nodes considered for each oki,j , the output at each layer progressively increasing the receptive field making sure all values associated with the considered nodes are present.",2.3 Neural Network Graph Estimator,[0],[0]
The entries used at each layer are illustrated in Figure 1.,2.3 Neural Network Graph Estimator,[0],[0]
"The receptive field here refers to the original covariance entries which are accessible by a given, oki,j (Luo et al., 2010).",2.3 Neural Network Graph Estimator,[0],[0]
The equations defining the process are shown in Figure 1.,2.3 Neural Network Graph Estimator,[0],[0]
Here a neural network fwk is applied at each edge at each layer and a dilation sequence dk is used.,2.3 Neural Network Graph Estimator,[0],[0]
"We call a network of this topology a D-Net of depth l. We use dilation here to allow the receptive field to grow fast, so the network does not need a great deal of layers.",2.3 Neural Network Graph Estimator,[0],[0]
We make the following observations: Proposition 2.,2.3 Neural Network Graph Estimator,[0],[0]
"For general P it is a necessary condition for P-consistency that the receptive field of D-Net covers all entries of the covariance, Σ̂, at any edge it is applied.",2.3 Neural Network Graph Estimator,[0],[0]
Proof.,2.3 Neural Network Graph Estimator,[0],[0]
Consider nodes i and j and a chain graph such that i and j are adjacent to each other in the matrix but are at the terminal nodes of the chain graph.,2.3 Neural Network Graph Estimator,[0],[0]
One would need to consider all other variables to be able to explain away the correlation.,2.3 Neural Network Graph Estimator,[0],[0]
Alternatively we can see this directly from expanding Eq.,2.3 Neural Network Graph Estimator,[0],[0]
"(4).
",2.3 Neural Network Graph Estimator,[0],[0]
Proposition 3.,2.3 Neural Network Graph Estimator,[0],[0]
A p × p matrix Σ̂ will be covered by the receptive field for a D-Net of depth log2(p) and dk = 2 k−1 Proof.,2.3 Neural Network Graph Estimator,[0],[0]
"The receptive field of a D-Net with dilation sequence dk = 2
k−1 of depth l is O(2l).",2.3 Neural Network Graph Estimator,[0],[0]
"We can see this as oki,j will receive input from ok−1a,b at the edge of it’s receptive field, effectively doubling it.",2.3 Neural Network Graph Estimator,[0],[0]
"It now follows that we need at least log2(p) layers to cover the receptive field.
",2.3 Neural Network Graph Estimator,[0],[0]
Intuitively adjacent edges have a high overlap in their receptive fields and can easily share information about the non-overlapping components.,2.3 Neural Network Graph Estimator,[0],[0]
This is analogous to a parametrized message passing.,2.3 Neural Network Graph Estimator,[0],[0]
"For example if edge (i, j) is explained by node k, as k enters the receptive field of edge (i, j − 1), the path through (i, j) can already be discounted.",2.3 Neural Network Graph Estimator,[0],[0]
"In terms of Eq. (4) this can correspond to storing computations that can be used by neighbor edges from lower levels in the recursion.
",2.3 Neural Network Graph Estimator,[0],[0]
"As fwk is identical for all nodes, we can simultaneously implement all edge predictions efficiently as a convolutional
network.",2.3 Neural Network Graph Estimator,[0],[0]
We make sure that to have considered all edges relevant to the current set of nodes in the receptive field which requires us to add values from filters applied at the diagonal to all edges.,2.3 Neural Network Graph Estimator,[0],[0]
In Figure 1 we illustrate the nodes and receptive field considered with respect to the covariance matrix.,2.3 Neural Network Graph Estimator,[0],[0]
"This also motivates a straightforward implementation using 2D convolutions (adding separate convolutions at i, i and j, j to each i, j at each layer to achieve the specific input pattern described) shown in Figure 2.
",2.3 Neural Network Graph Estimator,[0],[0]
Ultimately our choice of architecture that has shared computations and multiple layers is highly scalable as compared with a naive fully connected approach and allows leveraging existing optimized 2-D convolutions.,2.3 Neural Network Graph Estimator,[0],[0]
"In preliminary work we have also considered fully connected layers but this proved to be much less efficient in terms of storage and scalibility than using deep convolutional networks.
",2.3 Neural Network Graph Estimator,[0],[0]
Considering the general n p case is illustrative.,2.3 Neural Network Graph Estimator,[0],[0]
"However, the main benefit of making the computations differentiable and learned from data is that we can take advantage of the sparsity and structure assumptions to obtain more efficient results than naive computation of partial correlation or matrix inversion.",2.3 Neural Network Graph Estimator,[0],[0]
"As n decreases our estimate of ρ̂i,j becomes inexact; a data-driven model that takes better advantage of the assumptions on the underlying distribution and can more accurately recover the graph structure.
",2.3 Neural Network Graph Estimator,[0],[0]
"The convolution structure is dependent on the order of the variables used to build the covariance matrix, which is arbitrary.",2.3 Neural Network Graph Estimator,[0],[0]
Permuting the input data we can obtain another estimate of the output.,2.3 Neural Network Graph Estimator,[0],[0]
"In the experiments, we leverage these various estimate in an ensembling approach, averaging the results of several permutations of input.",2.3 Neural Network Graph Estimator,[0],[0]
"We observe that this generally yields a modest increase in accuracy, but that even a single node ordering can show substantially improved performance over competing methods in the literature.",2.3 Neural Network Graph Estimator,[0],[0]
Our experimental evaluations focus on the challenging high dimensional settings in which p > n,3 Experiments,[0],[0]
"and consider both
synthetic data and real data from genetics and neuroimaging.",3 Experiments,[0],[0]
"In our experiments we explore how well networks trained on parametric samples generalize, both to unseen synthetic data and to several real world problems.",3 Experiments,[0],[0]
"In order to highlight the generality of the learned networks, we apply the same network to multiple domains.",3 Experiments,[0],[0]
"We train networks taking in 39, 50, and 500 node graphs.",3 Experiments,[0],[0]
The former sizes are chosen based on the real data we consider in subsequent sections.,3 Experiments,[0],[0]
"We refer to these networks as DeepGraph-39, 50, and 500.",3 Experiments,[0],[0]
In all cases we have 50 feature maps of 3× 3 kernels.,3 Experiments,[0],[0]
The 39 and 50 node network with 6 convolutional layers and dk = k+ 1.,3 Experiments,[0],[0]
For the 500 node network with 8 convolutional layers and dk = 2k+1.,3 Experiments,[0],[0]
We use ReLU activations.,3 Experiments,[0],[0]
"The last layer has 1× 1 convolution and a sigmoid outputing a value of 0 to 1 for each edge.
",3 Experiments,[0],[0]
We sample P (X|G) with a sparse prior on P (G) as follows.,3 Experiments,[0],[0]
"We first construct a lower diagonal matrix, L, where each entry has α probability of being zero.",3 Experiments,[0],[0]
"Non-zero entries are set uniformly between −c and c. Multiplying LLT gives a sparse positive definite precision matrix, Θ. This gives us our P (Θ|G) with a sparse prior on P (G).",3 Experiments,[0],[0]
"We sample from the Gaussian N (0,Θ−1) to obtain samples of X .",3 Experiments,[0],[0]
"Here α corresponds to a specific sparsity level in the final precision matrix, which we set to produce matrices 92− 96% sparse and c chosen so that partial correlations range 0 to 1.
",3 Experiments,[0],[0]
Each network is trained continously with new samples generated until the validation error saturates.,3 Experiments,[0],[0]
"For a given precision matrix we generate 5 possible X samples to be used as training data, with a total of approximately 100K training samples used for each network.",3 Experiments,[0],[0]
"The networks are optimized using ADAM (Kingma & Ba, 2015) coupled with crossentropy loss as the objective function (cf. Sec. 2.1).",3 Experiments,[0],[0]
We use batch normalization at each layer.,3 Experiments,[0],[0]
"Additionally, we found that using the absolute value of the true partial correlations as labels, instead of hard binary labels, improves results.
",3 Experiments,[0],[0]
"Synthetic Data Evaluation To understand the properties of our learned networks, we evaluated them on different synthetic data than the ones they were trained on.",3 Experiments,[0],[0]
"More specifically, we used a completely different third party sam-
pler so as to avoid any contamination.",3 Experiments,[0],[0]
"We use DeepGraph39, which takes 4 hours to train, on a variety of settings.",3 Experiments,[0],[0]
The same trained network is utilized in the subsequent neuroimaging evaluations as well.,3 Experiments,[0],[0]
"DeepGraph-500 is also used to evaluate larger graphs.
",3 Experiments,[0],[0]
"We used the BDGraph R-package to produce sparse precision matrices based on the G-Wishart distribution (Mohammadi & Wit, 2015) as well as the R-package rags2ridges (Peeters et al., 2015) to generate data from small-world networks corresponding to the Watts–Strogatz model (Watts & Strogatz, 1998).",3 Experiments,[0],[0]
"We compared our learned estimator against the scikit-learn (Pedregosa et al, 2011) implementation of Graphical Lasso with regularizer chosen by cross-validation as well as the Birth-Death Rate MCMC (BDMCMC) method from Mohammadi & Wit (2015).
",3 Experiments,[0],[0]
"For each scenario we repeat the experiment for 100 different graphs and small sample observations showing the average area under the ROC curve (AUC), precision@k corresponding to 5% of possible edges, and calibration error (CE) (Mohammadi & Wit, 2015).
",3 Experiments,[0],[0]
For graphical lasso we use the partial correlations to indicate confidence in edges; BDGraph automatically returns posterior probabilities as does our method.,3 Experiments,[0],[0]
"Finally to understand the effect of the regularization parameter we additionally report the result of graphical lasso under optimal regularizer setting on the testing data.
",3 Experiments,[0],[0]
Our method dominates all other approaches in all cases with p > n (which also corresponds to the training regime).,3 Experiments,[0],[0]
"For the case of random Gaussian graphs with n=35 (as in our training data), and graph sparsity of 95%, we have superior performance and can further improve on this by averaging permutations.",3 Experiments,[0],[0]
"Next we apply the method to less straightforward synthetic data, such as that arising from small-world graphs which is typical of many applications.",3 Experiments,[0],[0]
"We found that, compared to baseline methods, our network performs particularly well with high-degree nodes and when the distribution becomes non-normal.",3 Experiments,[0],[0]
"In particular our method performs well on the relevant metrics with small-world networks, a very common family of graphs in real-world data, obtaining superior precision at the primary levels of interest.
",3 Experiments,[0],[0]
"Figure 3 shows examples of random and Watts-Strogatz small-world graphs used in these experiments.
",3 Experiments,[0],[0]
Training a new network for each number of samples can pose difficulties with our proposed method.,3 Experiments,[0],[0]
Thus we evaluted how robust the network DeepGraph-39 is to input covariances obtained from fewer or more samples.,3 Experiments,[0],[0]
"We find that overall the performance is quite good even when lowering the number of samples to n = 15, we obtain superior performance to the other approaches (Table 1).",3 Experiments,[0],[0]
"We also applied DeepGraph-39 on data from a multivariate generalization of the Laplace distribution (Gómez et al., 1998).",3 Experiments,[0],[0]
As in other experiments precision matrices were sampled from the G-Wishart at a sparsity of 95%.,3 Experiments,[0],[0]
"Gómez et al. (1998, Proposition 3.1) was applied to produce samples.",3 Experiments,[0],[0]
"We find that DeepGraph-39 performs competitively, despite the discrepancy between train and test distributions.",3 Experiments,[0],[0]
"Experiments with variable sparsity are considered in the supplementary material, which find that for very sparse graphs, the networks remain robust in performance, while for increased density performance degrades but remains competitive.
",3 Experiments,[0],[0]
"Using the small-world network data generator (Peeters et al., 2015), we demonstrate that we can update the generic sparse prior to a structured one.",3 Experiments,[0],[0]
We re-train DeepGraph-39 using only 1000 examples of small-world graphs mixed with 1000 examples from the original uniform sparsity model.,3 Experiments,[0],[0]
"We perform just one epoch of training and observe markedly improved performance on this test case as seen in the last row of Table 1.
",3 Experiments,[0],[0]
For our final scenario we consider the very challenging setting with 500 nodes and only n = 50 samples.,3 Experiments,[0],[0]
"We note that the MCMC based method fails to converge at this scale, while graphical lasso is very slow as seen in the timing performance and barely performs better than chance.",3 Experiments,[0],[0]
Our method convincingly outperforms graphical lasso in this scenario as shown in Tabel 2.,3 Experiments,[0],[0]
"Here we additionally report precision at just the first 0.05% of edges since competitors perform nearly at chance at the 5% level.
",3 Experiments,[0],[0]
We compute the average execution time of our method compared to Graph Lasso and BDGraph on a CPU in Table 3.,3 Experiments,[0],[0]
"We note that we use a production quality version of graph lasso (Pedregosa et al, 2011), whereas we have not opti-
mized the network execution, for which known strategies may be applied (Denton et al., 2014).
",3 Experiments,[0],[0]
Cancer Genome Data We perform experiments on a gene expression dataset described in Honorio et al. (2012).,3 Experiments,[0],[0]
The data come from a cancer genome atlas from 2360 subjects for various types of cancer.,3 Experiments,[0],[0]
"We used the first 50 genes from Honorio et al. (2012, Appendix C.2) of commonly regulated genes in cancer.",3 Experiments,[0],[0]
"We evaluated on two groups of subjects, one with breast invasive carcinoma (BRCA) consisting of 590 subjects and the other colon adenocarcinoma (COAD) consisting of 174 subjects.
",3 Experiments,[0],[0]
Evaluating edge selection in real-world data is challenging.,3 Experiments,[0],[0]
"We use the following methodology: for each method we
select the top-k ranked edges, recomputing the maximum likelihood precision matrix with support given by the corresponding edge selection method.",3 Experiments,[0],[0]
We then evaluate the likelihood on held-out data.,3 Experiments,[0],[0]
We repeat this procedure for a range of k. We rely on Algorithm 0,3 Experiments,[0],[0]
in Hara & Takemura (2010) to compute the maximum likelihood precision given a support.,3 Experiments,[0],[0]
The experiment is repeated for each of CODA and BRCA subject groups 150 times.,3 Experiments,[0],[0]
Results are shown in Figure 4.,3 Experiments,[0],[0]
In all cases we use 40 samples for edge selection and precision estimation.,3 Experiments,[0],[0]
"We compare with graphical lasso as well as the Ledoit-Wolf shrinkage estimator (Ledoit & Wolf, 2004).",3 Experiments,[0],[0]
We additionally consider the MCMC based approach described in previous section.,3 Experiments,[0],[0]
"For graphical lasso and Ledoit-Wolf, edge selection is based on thresholding partial correlation (Balmand & Dalalyan, 2016).
",3 Experiments,[0],[0]
"Additionally, we evaluate the stability of the solutions provided by the various methods.",3 Experiments,[0],[0]
In several applications a low variance on the estimate of the edge set is important.,3 Experiments,[0],[0]
"On Table 4, we report Spearman correlations between pairs of solutions, as it is a measure of a monotone link between two variables.",3 Experiments,[0],[0]
"DeepGraph has far better stability in the genome experiments and is competitive in the fMRI data.
",3 Experiments,[0],[0]
Resting State Functional Connectivity We evaluate our graph discovery method to study brain functional connectivity in resting-state fMRI data.,3 Experiments,[0],[0]
Correlations in brain activity measured via fMRI reveal functional interactions between remote brain regions.,3 Experiments,[0],[0]
These are an important measure to study psychiatric diseases that have no known anatomical support.,3 Experiments,[0],[0]
"Typical connectome analysis describes each subject or group by a GGM measuring functional connectivity between a set of regions (Varoquaux & Craddock, 2013).",3 Experiments,[0],[0]
"We use the ABIDE dataset (Di Martino et al, 2014), a large scale resting state fMRI dataset.",3 Experiments,[0],[0]
It gathers brain scans from 539 individuals suffering from autism spectrum disorder and 573 controls over 16 sites.1,3 Experiments,[0],[0]
"For our experiments we use an atlas with 39 regions of interest described in Varoquaux et al. (2011).
1http://preprocessed-connectomes-project.",3 Experiments,[0],[0]
"github.io/abide/
few selected edges.",3 Experiments,[0],[0]
When the number of selected edges is in the range above 25 we begin to perform significantly better in edge selection as seen in Fig. 4.,3 Experiments,[0],[0]
We evaluated stability of the results as shown in Tab. 4.,3 Experiments,[0],[0]
"DeepGraph outperformed the other methods across the board.
",3 Experiments,[0],[0]
ABIDE has high variability across sites and subjects.,3 Experiments,[0],[0]
"As a result, to resolve differences between approaches, we needed to perform 1000 folds to obtain well-separated error bars.",3 Experiments,[0],[0]
"We found that the birth-death MCMC method took very long to converge on this data, moreover the need for many folds to obtain significant results amongst the methods made this approach prohibitively slow to evaluate.
",3 Experiments,[0],[0]
"We show the edges returned by Graph Lasso and DeepGraph for a sample from 35 subjects (Fig. 5) in the control group.
",3 Experiments,[0],[0]
We also show the result of a large-sample result based on 368 subjects from graphical lasso.,3 Experiments,[0],[0]
In visual evaluation of the edges returned by DeepGraph we find that they closely align with results from a large-sample estimation procedure.,3 Experiments,[0],[0]
Furthermore we can see several edges in the subsample which were particularly strongly activated in both methods.,3 Experiments,[0],[0]
Learned graph estimation outperformed strong baselines in both accuracy and speed.,4 Discussion and Conclusions,[0],[0]
"Even in cases that deviate from standard GGM sparsity assumptions (e.g. Laplace, small-world) it performed substantially better.",4 Discussion and Conclusions,[0],[0]
When finetuning on the target distribution performance further improves.,4 Discussion and Conclusions,[0],[0]
Most importantly the learned estimator generalizes well to real data finding relevant stable edges.,4 Discussion and Conclusions,[0],[0]
"We also observed that the learned estimators generalize to variations not seen at training time (e.g. different n or sparsity), which points to this potentialy learning generic computations.",4 Discussion and Conclusions,[0],[0]
This also shows potential to more easily scale the method to different graph sizes.,4 Discussion and Conclusions,[0],[0]
"One could consider transfer learning, where a network for one size of data is used as a starting point to learn a network working on larger dimension data.
",4 Discussion and Conclusions,[0],[0]
Penalized maximum likelihood can provide performance guarantees under restrictive assumptions on the form of the distribution and not considering the regularization path.,4 Discussion and Conclusions,[0],[0]
In the proposed method one could obtain empirical bounds under the prescribed data distribution.,4 Discussion and Conclusions,[0],[0]
"Additionally, at execution time the speed of the approach can allow for resampling based uncertainty estimates and efficient model
selection (e.g. cross-validation) amongst several trained estimators.
",4 Discussion and Conclusions,[0],[0]
We have introduced the concept of learning an estimator for determining the structure of an undirected graphical model.,4 Discussion and Conclusions,[0],[0]
A network architecture and sampling procedure for learning such an estimator for the case of sparse GGMs was proposed.,4 Discussion and Conclusions,[0],[0]
"We obtained competitive results on synthetic data with various underlying distributions, as well as on challenging real-world data.",4 Discussion and Conclusions,[0],[0]
"Empirical results show that our method works particularly well compared to other approaches for small-world networks, an important class of graphs common in real-world domains.",4 Discussion and Conclusions,[0],[0]
"We have shown that neural networks can obtain improved results over various statistical methods on real datasets, despite being trained with samples from parametric distributions.",4 Discussion and Conclusions,[0],[0]
Our approach enables straightforward specifications of new priors and opens new directions in efficient graphical structure discovery from few examples.,4 Discussion and Conclusions,[0],[0]
"This work is partially funded by Internal Funds KU Leuven, FP7-MC-CIG 334380, DIGITEO 2013-0788D - SOPRANO, and ANR-11-BINF-0004 NiConnect.",Acknowledgements,[0],[0]
We thank Jean Honorio for providing pre-processed Genome Data.,Acknowledgements,[0],[0]
"A.1 Predicting Covariance Matrices
Using our framework it is possible to attempt to directly predict an accurate covariance matrix given a noisy one constructed from few observations.",A Supplementary Experiments and Analysis,[0],[0]
This is a more challenging task than predicting the edges.,A Supplementary Experiments and Analysis,[0],[0]
"In this section we show preliminay experiments which given an empirical covariance matrix from few observations attempts to predict a more accurate covariance matrix that takes into account underlying sparse data dependency structure.
",A Supplementary Experiments and Analysis,[0],[0]
"One challenge is that outputs of our covariance predictor must be on the positive semidefinite cone, thus we choose to instead predict on the cholesky decompositions, which allows us to always produce positive definite covariances.",A Supplementary Experiments and Analysis,[0],[0]
"We train a similar structure to DeepGraph-39 structure modifying the last layer to be fully connected linear layer that predicts on the cholesky decomposition of the true covariance matrices generated by our model with a squared loss.
",A Supplementary Experiments and Analysis,[0],[0]
We evaluate this network using the ABIDE dataset described in Section 3.,A Supplementary Experiments and Analysis,[0],[0]
The ABIDE data has a large number of samples allowing us to obtain a large sample estimate of the covariance and compare it to our estimator as well as graphical lasso and empirical covariance estimators.,A Supplementary Experiments and Analysis,[0],[0]
Using the large sample ABIDE empirical covariance matrix.,A Supplementary Experiments and Analysis,[0],[0]
We find that we can obtain competitive `2 and `∞ norm using few samples.,A Supplementary Experiments and Analysis,[0],[0]
"We use 403 subjects from the ABIDE Control group each with a recording of 150 − 200 samples to construct covariance matrix, totaling 77 330 samples (some correlated).",A Supplementary Experiments and Analysis,[0],[0]
"This acts as our very approximate estimate of the population Σ. We then evaluate covariance estimation on 35 samples using the empirical covariance estimator, graphical lasso, and DeepGraph trained to output covariance matrices.",A Supplementary Experiments and Analysis,[0],[0]
We repeat the experiment for 50 different subsamples of the data.,A Supplementary Experiments and Analysis,[0],[0]
We see in 5 that the prediction approach can obtain competitive results.,A Supplementary Experiments and Analysis,[0],[0]
"In terms of `2 graphical lasso performs better, however our estimate is better than empirical covariance estimation and much faster then graphical lasso.",A Supplementary Experiments and Analysis,[0],[0]
In some applications such as robust estimation a fast estimate of the covariance matrix (automatically embedding sparsity assumptions) can be of great use.,A Supplementary Experiments and Analysis,[0],[0]
"For `∞ error we see the empirical covariance estimation outperforms graphical lasso and DeepGraph for this dataset, while DeepGraph performs better in terms of this metric.
",A Supplementary Experiments and Analysis,[0],[0]
"We note these results are preliminary, as the covariance predicting networks were not heavily optimized, moreover the ABIDE dataset is very noisy even when pre-processed and thus even the large sample covariance estimate may not be accurate.",A Supplementary Experiments and Analysis,[0],[0]
"We believe this is an interesting alternate application of our paper.
",A Supplementary Experiments and Analysis,[0],[0]
"A.2 Additional Synthetic Results on Sparsity
We investigate the affect of sparsity on DeepGraph-39 which has been trained with input that has sparsity 96%− 92% sparse.",A Supplementary Experiments and Analysis,[0],[0]
We find that DeepGraph performs well at the 2% sparsity level despite not seeing this at training time.,A Supplementary Experiments and Analysis,[0],[0]
"At the same time performance
begins to degrade for 15% but is still competitive in several categories.",A Supplementary Experiments and Analysis,[0],[0]
The results are shown in Table 6.,A Supplementary Experiments and Analysis,[0],[0]
"Future investigation can consider how alternate variation of sparsity at training time will affect these results.
",A Supplementary Experiments and Analysis,[0],[0]
"A.3 Application of Larger Network on Smaller Input
We perform preliminary investigation of application of a network trained for a larger number of nodes to a smaller set of nodes.",A Supplementary Experiments and Analysis,[0],[0]
"Specifically, we consider the breast invasive carcinoma groups gene data.",A Supplementary Experiments and Analysis,[0],[0]
"We now take all 175 valid genes from Appendix C.2 of (Honorio et al., 2012).",A Supplementary Experiments and Analysis,[0],[0]
We take the network trained on 500 nodes in the synthetic experiments section.,A Supplementary Experiments and Analysis,[0],[0]
We use the same experimental setup as in the gene experiments.,A Supplementary Experiments and Analysis,[0],[0]
The 175 × 175 covariance matrix from 40 samples and padded to the appropriate size.,A Supplementary Experiments and Analysis,[0],[0]
"We observe that DeepGraph has similar performance to graph lasso while permuting the input and ensembling the result gives substantial improvement.
",A Supplementary Experiments and Analysis,[0],[0]
"A.4 Permutation as Ensemble Method
As discussed in Section 2.3, permuting the input and averaging several permutations can produce an improved result empirically.",A Supplementary Experiments and Analysis,[0],[0]
We interpret this as a typical ensembling method.,A Supplementary Experiments and Analysis,[0],[0]
This can be an advantage of the proposed architecture as we are able to easily use standard ensemble techniques.,A Supplementary Experiments and Analysis,[0],[0]
"We perform an experiment to further verify that indeed the permutation of the input (and subsequent inverse permutation) allows us to produce separate classifiers that have uncorrelated errors.
",A Supplementary Experiments and Analysis,[0],[0]
We use the setup from the synthetic experiments with DeepGraph39 in Section 3 with n = 35 and p = 39.,A Supplementary Experiments and Analysis,[0],[0]
We construct 20 permutation matrices as in the experimental section.,A Supplementary Experiments and Analysis,[0],[0]
Treating each as a separate classifier we compute the correlation coefficient of the errors on 50 synthetic input examples.,A Supplementary Experiments and Analysis,[0],[0]
"We find that the average correlation coefficient of the errors of two classifiers is 0.028±0.002, suggesting they are uncorrelated.",A Supplementary Experiments and Analysis,[0],[0]
"Finally we note the individual errors are relatively small, as can already be inferred from our extensive experimental results in Section 3.",A Supplementary Experiments and Analysis,[0],[0]
"We however compute the average absolute error of all the outputs across each permutation for this set of inputs as 0.03, notably the range of outputs is 0 to 1.",A Supplementary Experiments and Analysis,[0],[0]
"Thus since prediction error differ at each permutation but are accurate we can average and yield a lower total prediction error.
",A Supplementary Experiments and Analysis,[0],[0]
Finally we note that our method is extremely efficient computationally thus averaging the results of several permutations is practical even as the graph becomes large.,A Supplementary Experiments and Analysis,[0],[0]
We consider structure discovery of undirected graphical models from observational data.,abstractText,[0],[0]
Inferring likely structures from few examples is a complex task often requiring the formulation of priors and sophisticated inference procedures.,abstractText,[0],[0]
Popular methods rely on estimating a penalized maximum likelihood of the precision matrix.,abstractText,[0],[0]
"However, in these approaches structure recovery is an indirect consequence of the data-fit term, the penalty can be difficult to adapt for domain-specific knowledge, and the inference is computationally demanding.",abstractText,[0],[0]
"By contrast, it may be easier to generate training samples of data that arise from graphs with the desired structure properties.",abstractText,[0],[0]
"We propose here to leverage this latter source of information as training data to learn a function, parametrized by a neural network that maps empirical covariance matrices to estimated graph structures.",abstractText,[0],[0]
"Learning this function brings two benefits: it implicitly models the desired structure or sparsity properties to form suitable priors, and it can be tailored to the specific problem of edge structure discovery, rather than maximizing data likelihood.",abstractText,[0],[0]
"Applying this framework, we find our learnable graph-discovery method trained on synthetic data generalizes well: identifying relevant edges in both synthetic and real data, completely unknown at training time.",abstractText,[0],[0]
"We find that on genetics, brain imaging, and simulation data we obtain performance generally superior to analytical methods.",abstractText,[0],[0]
Learning to Discover Sparse Graphical Models,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1812–1822 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
In this paper, we propose to leverage representation learning for conversation disentanglement. A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages. With the estimated similarity scores, our algorithm for conversation identification by similarity ranking (CISIR) then derives conversations based on highconfidence message pairs and pairwise redundancy. Experiments were conducted with four publicly available datasets of conversations from Reddit and IRC channels. The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement.",text,[0],[0]
"With the growth of ubiquitous internet and mobile devices, people now commonly communicate in the virtual world.",1 Introduction,[0],[0]
"Among the various methods of communication, text-based conversational media, such as internet relay chat (IRC) (Werry, 1996) and Facebook Messenger1, has been and remains one of the most popular choices.",1 Introduction,[0],[0]
"In addition, many enterprises have started to use conversational chat platforms such as Slack2 to enhance team collaboration.",1 Introduction,[1.0],"['In addition, many enterprises have started to use conversational chat platforms such as Slack2 to enhance team collaboration.']"
"However, multiple conversations may
1Facebook Messenger: https://www.messenger.",1 Introduction,[0],[0]
"com/
2Slack: https://slack.com/
occur simultaneously when conversations involve three or more participants.",1 Introduction,[0],[0]
Aoki et al. (2006) found an average of 1.79 conversations among eight participants at a time.,1 Introduction,[1.0],['Aoki et al. (2006) found an average of 1.79 conversations among eight participants at a time.']
"Moreover, some platforms like chatrooms in Twitch may have more concurrent conversations (Hamilton et al., 2014).",1 Introduction,[1.0],"['Moreover, some platforms like chatrooms in Twitch may have more concurrent conversations (Hamilton et al., 2014).']"
Interleaved conversations can lead to difficulties in both grasping discussions and identifying messages related to a search result.,1 Introduction,[0],[0]
"For example, Figure 1 shows a segment of conversations from the real-world IRC dataset as an example.",1 Introduction,[0],[0]
Five interleaved threads are involved in only ten messages.,1 Introduction,[0],[0]
Messages in the same thread may not have identical keywords.,1 Introduction,[0],[0]
"Moreover, a user (i.e., Elma) can participate in multiple threads.",1 Introduction,[0],[0]
"Hence, a robust mechanism to disentangle interleaved conversations can improve a user’s satisfaction with a chat system.
",1 Introduction,[1.000000048174639],"['Hence, a robust mechanism to disentangle interleaved conversations can improve a user’s satisfaction with a chat system.']"
"One solution for conversation disentanglement is to model the task as a topic detection and tracking (TDT) (Allan, 2002) task by deciding whether each incoming message starts a new topic or belongs to an existing conversation.",1 Introduction,[0],[0]
"Messages in the same conversation may have higher similarity
1812
scores (Shen et al., 2006; Mayfield et al., 2012) or similar context messages (Wang and Oard, 2009).",1 Introduction,[0],[0]
"However, similarity thresholds for determining new topics vary depending on context.",1 Introduction,[0],[0]
"Embedding of earlier messages, resulting in duplication of parts of messages, can alter the similarity score.",1 Introduction,[0],[0]
"More specifically, the similarity scores obtained in previous work cannot well represent conversationlevel relationships between messages.
",1 Introduction,[0],[0]
"Several studies have examined the use of statistical (Du et al., 2017) and linguistic features (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012) for predicting user annotations of paired message similarity.",1 Introduction,[0],[0]
These studies employed bag-of-words representations which do not capture term similarity and cannot distinguish word importance and relationships between words in a message.,1 Introduction,[0],[0]
"Thus, better representations of messages and their relationships are needed.
",1 Introduction,[0],[0]
"Recent studies have demonstrated the effectiveness of deep learning methods in representation learning (Bengio et al., 2013), aiming to infer low-dimensional distributed representations for sparse data such as text (Hinton and Salakhutdinov, 2006).",1 Introduction,[0],[0]
"These representations can be derived not only for words (Mikolov et al., 2013) but also sentences and documents (Le and Mikolov, 2014).",1 Introduction,[0],[0]
"In particular, convolutional neural networks (CNNs) have been shown to efficiently and effectively preserve important semantic and syntactic information from embedded text sequences (Blunsom et al., 2014).",1 Introduction,[0],[0]
"It has been demonstrated that CNNs produce state-of-the-art results in many NLP tasks such as text classification (Kim, 2014; Lai et al., 2015; Zhang et al., 2015) and sentiment analysis (Tang et al., 2014; Poria et al., 2015).",1 Introduction,[0],[0]
"Existing approaches, however, do not take advantage of deep learning techniques to model relationships between messages for disentangling conversations.",1 Introduction,[0],[0]
"(Mehri and Carenini, 2017) defined many statistical features for use with a random forest for in-thread classification and used a recurrent neural network (RNN) only to model adjacent messages with an external dataset as a feature.
",1 Introduction,[0],[0]
"In this paper, we aim to leverage deep learning for conversation disentanglement.",1 Introduction,[0],[0]
Our proposed approach consists of two stages: (1) message pair similarity estimation and (2) conversation identification.,1 Introduction,[0],[0]
"In the first stage, we propose the Siamese hierarchical convolutional neural network (SHCNN) to estimate conversation-level similarity between pairs of closely posted messages.",1 Introduction,[1.0],"['In the first stage, we propose the Siamese hierarchical convolutional neural network (SHCNN) to estimate conversation-level similarity between pairs of closely posted messages.']"
"SHCNN is framed as a Siamese architecture (Mueller and Thyagarajan, 2016)",1 Introduction,[0],[0]
"concatenat-
ing the outputs of two hierarchical convolutional neural networks and additional features.",1 Introduction,[0],[0]
"Compared to other conventional CNN-based Siamese networks (Severyn and Moschitti, 2015; Yin et al., 2016), SHCNN models not only local information in adjacent words but also more global semantic information in a message.",1 Introduction,[0],[0]
"In the second stage, the algorithm of conversation identification by similarity ranking (CISIR) ranks messages within a time window paired with each message and constructs a message graph involving high-rank connections with strong confidence.",1 Introduction,[0],[0]
"Although only high-confidence relations are represented in the constructed graph, the redundancy of pairwise relationships can capture the connectivity of messages within a conversation.
",1 Introduction,[0],[0]
"In summary, the main contributions of this paper are threefold: (1) Deep similarity estimation for conversation disentanglement: To the best of our knowledge, this is the first study applying deep learning to estimate similarities between messages for disentangling conversations.",1 Introduction,[0],[0]
SHCNN simultaneously captures and compares local and global characteristics of two messages to estimate their similarity.,1 Introduction,[0],[0]
Message representations are also optimized towards the task of conversation disentanglement.,1 Introduction,[0],[0]
"(2) Efficient and effective method: The selection of message pairs posted closely in time and the proposed CISIR algorithm significantly reduces the computational time from O |M |2 to O (k|M |), where |M | is the number of messages, and k is the maximum number of messages posted within a fixed-length time window.",1 Introduction,[0],[0]
"When many messages are posted over a long period, the computational time of our approach could be nearlinear.",1 Introduction,[0],[0]
"(3) Empirical improvements over previous work: Extensive experiments have been conducted on four publicly available datasets, including three synthetic conversation datasets and one real conversation dataset from Reddit3 and IRC conversations.",1 Introduction,[0],[0]
Our approach outperforms all comparative baselines for both similarity estimation and conversation disentanglement.,1 Introduction,[0],[0]
Methods for conversation disentanglement can be simply categorized into unsupervised and supervised approaches.,2 Related Work,[0],[0]
"Unsupervised approaches (Wang and Oard, 2009) estimate the relationship between messages through unsupervised similarity functions such cosine similarity, and assign messages to conversations based on a predefined
3Reddit: https://www.reddit.com/
threshold.",2 Related Work,[0],[0]
"In contrast, supervised methods exploit a set of user annotations (Elsner and Charniak, 2008; Mayfield et al., 2012; Shen et al., 2006; Du et al., 2017; Mehri and Carenini, 2017) to adapt to different datasets.",2 Related Work,[0],[0]
"Our approach can be classified as a supervised approach because a small set of user annotations is used to train the SHCNN.
",2 Related Work,[0],[0]
"In addition to conversations, some studies predict the partial structure of threaded data, especially for online forums (Aumayr et al., 2011; Wang et al., 2011b,a).",2 Related Work,[0],[0]
"These studies merely classify parent-child relationships in disentangled, independent threads.",2 Related Work,[0],[0]
"Moreover, they focus only on comments to the same post.",2 Related Work,[0],[0]
"Indeed, conversation disentanglement is a more difficult task.
",2 Related Work,[0],[0]
Estimating the similarity of text pairs is an essential part in our approach.,2 Related Work,[0],[0]
"Many studies also focus on similar tasks aside from conversation disentanglement, such as entailment prediction (Mueller and Thyagarajan, 2016; Wang and Jiang, 2017) and question-answering (Severyn and Moschitti, 2015; Amiri et al., 2016; Yin et al., 2016).",2 Related Work,[0],[0]
"However, most of their models are complicated and require a larger amount of labeled training data; limited conversational data can lead to unsatisfactory performance as shown in Section 4.",2 Related Work,[0],[0]
"In this section, we formally define the objective of this work and notations used.",3 Conversation Disentanglement,[0],[0]
A two-stage approach is then proposed to address the problem.,3 Conversation Disentanglement,[0],[0]
"Given a set of speakers S, a message m is defined as a tuple m = (w, s, t), where w = hw1, w2, · · · , wni is a word sequence posted by the speaker s 2 S at time t in seconds.",3.1 Problem Statement,[0],[0]
Each message m is associated with a conversation z (m).,3.1 Problem Statement,[0],[0]
"Messages in different conversations can be posted concurrently, i.e., conversations can be interleaved.
",3.1 Problem Statement,[0],[0]
"Following the settings of previous work (Elsner and Charniak, 2008, 2010, 2011; Mayfield et al., 2012), a set of pairwise annotations",3.1 Problem Statement,[0],[0]
"A = {(mi, mj , y)}, where y 2 {0, 1}, is given for training the model.",3.1 Problem Statement,[0],[0]
"More specifically, a Boolean value y indicates whether two messages mi and mj are in the same conversation, i.e., z(mi) and z(mj) are identical.
",3.1 Problem Statement,[0],[0]
"Given a set of messages M and the pairwise annotations A as training data, the goal is to learn a model that can identify whether messages are posted in the same conversation z(m).",3.1 Problem Statement,[0],[0]
"Note that the number of conversations |Z =
{z(m) | 8m 2 M} | is always unknown to the system.",3.1 Problem Statement,[0],[0]
Figure 2 illustrates our two-stage framework.,3.2 Framework Overview,[0],[0]
The first stage aims to estimate pairwise similarity among messages.,3.2 Framework Overview,[0],[0]
Message pair selection is applied to focus on the similarity between messages that are posted closely in time and thus more likely to be in the same conversation.,3.2 Framework Overview,[0],[0]
The Siamese hierarchical CNN (SHCNN) is proposed for learning message representations and estimating pairwise similarity scores.,3.2 Framework Overview,[0],[0]
"The overlapping hierarchical structure of SHCNN models a message at multiple semantic levels and obtains representations that are more comprehensive.
",3.2 Framework Overview,[0],[0]
"In the second stage, our conversation identification by similarity ranking (CISIR) algorithm exploits the redundancy and connectivity of pairwise relationships to identify conversations as connected components in a message graph.",3.2 Framework Overview,[0],[0]
"Most of the previous work on conversation disentanglement focused on pairwise relationships between messages (Mayfield et al., 2012).",3.3 Message Pair Selection,[0],[0]
"Especially for single-pass clustering approaches, all pairs of messages need to be enumerated during similarity computation (Wang and Oard, 2009).",3.3 Message Pair Selection,[0],[0]
"However, if messages have been collected for a long time, the number of message pairs could be too mammoth to be processed in an acceptable amount of time.",3.3 Message Pair Selection,[0],[0]
"More precisely, it leads to at least O(n2) computational time, where n is the number of messages.",3.3 Message Pair Selection,[0],[0]
"As shown in Figure 3, the percentage of messages in the same conversation as a given message becomes significantly lower with a longer elapsed time between consecutive messages.",3.3 Message Pair Selection,[0],[0]
"In light of this observation, an assumption is made as follows: Assumption 1 The elapsed time between two consecutive messages posted in the same conversation is not greater than T hours, where T is a small number.",3.3 Message Pair Selection,[1.0],"['In light of this observation, an assumption is made as follows: Assumption 1 The elapsed time between two consecutive messages posted in the same conversation is not greater than T hours, where T is a small number.']"
"More specifically, in our dataset every message mi is posted within T hours earlier or later than any other message mj in the same conversation, i.e., |ti tj | 3600 < T for all pairs (mi, mj), where t is in seconds.",3.3 Message Pair Selection,[1.0],"['More specifically, in our dataset every message mi is posted within T hours earlier or later than any other message mj in the same conversation, i.e., |ti tj | 3600 < T for all pairs (mi, mj), where t is in seconds.']"
"For example, in the IRC dataset the average elapsed time between consecutive messages in a conversation is only 7 minutes.",3.3 Message Pair Selection,[0],[0]
"If a conversation is ongoing, there may not be an extended silence before a new message; conversely, an extended silence could be treated as the start of a new
conversation.",3.3 Message Pair Selection,[0],[0]
"With this assumption, the number of pairs can be reduced to O(kn), where k is the maximum number of messages posted in a T -hour time window.",3.3 Message Pair Selection,[0],[0]
"By default T is set to 1 hour in our experiments.
",3.3 Message Pair Selection,[0],[0]
"In addition, it is worth mentioning that it may be possible to include conversational structure, such as replied-to relations, into the model.",3.3 Message Pair Selection,[0],[0]
"For example, after using CISIR to identify conversational threads, structure inference may be performed using methods such as described in (Aumayr et al., 2011) or (Wang et al., 2011b) and the structure used to refine the threads.",3.3 Message Pair Selection,[0],[0]
"In this study, we focus on only conversation disentanglement.",3.3 Message Pair Selection,[0],[0]
"Given a set of message pairs, we propose the Siamese hierarchical CNN (SHCNN) to estimate the similarity between a pair of messages.
|w| words message input m
convolutional message
matrix Wc
d-dimensional word embedding
... ...
high-level message
matrix WH
low-level conv.",3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN),[0],[0]
"feature
map cLi
high-level conv.",3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN),[1.0000000664619655],['feature map cLi high-level conv.']
"feature
map cHi
64-dim low-level representation
m̂L
64-dim high-level representation
m̂H
128-dim message representation m̂
d ⇥",3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN),[0],[0]
"|w| message matrix W
d ⇥ |w| message matrix W
Figure 4: Illustration of hierarchical CNN (HCNN) for message representation.",3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN),[0],[0]
"The labels with a larger font size indicate the corresponding tensors, and the labels with a smaller font size explain the operations between tensors.",3.4 Similarity Estimation with the Siamese Hierarchical CNN (SHCNN),[0],[0]
The effectiveness of CNNs for representing text has already been addressed in previous studies.,3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"However, single-layer CNNs (Kim, 2014; Severyn and Moschitti, 2015) may not represent highlevel semantics while low-level information could be diluted with multiple-layer CNNs (Yin et al., 2016).",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"The hierarchical CNN (HCNN) is designed to simultaneously capture low- and highlevel message meanings as shown in Figure 4.
",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"A message mi is first represented by a d ⇥ |w| message matrix W 2 Rd⇥|w|, where d is the dimension of a word embedding, and |w| is the num-
ber of words in a message.",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"For low-level information, we exploit single-layer CNNs (Kim, 2014; Severyn and Moschitti, 2015) with a set of d⇥ kL kernels, where L denotes “Low”, to extract ngram semantics of kL contiguous words.",3.4.1 Hierarchical CNN for Message Representation,[1.0],"['For low-level information, we exploit single-layer CNNs (Kim, 2014; Severyn and Moschitti, 2015) with a set of d⇥ kL kernels, where L denotes “Low”, to extract ngram semantics of kL contiguous words.']"
"In this paper, 64 d ⇥ kL kernels, where kL = 5, are applied to obtain 64 low-level features m̂L. Note that the kernel row dimension is identical to the word embedding dimension to jointly consider the full embedding vector.",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"As a consequence, convolution with each kernel produces a vector cLi , which is then aggregated by max-over-time pooling (Collobert et al., 2011; Kim, 2014).
",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"To acquire high-level semantics across a message, HCNN uses another multiple-layer CNN for feature extraction.",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"A 1 ⇥ kC kernel is applied to W , thereby generating a convolutional message matrix W C .",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"Features covering broader contents are computed by applying a 1 ⇥ 2 kernel to a max-pooling layer with a stride of 2, producing a high-level message matrix W H .",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"The row sizes of the two kernels are set to 1 to capture relations within each embedding dimension, and convolution is performed on W H with 64 d⇥ kH kernels to capture relations across embedding dimensions.",3.4.1 Hierarchical CNN for Message Representation,[1.0],"['The row sizes of the two kernels are set to 1 to capture relations within each embedding dimension, and convolution is performed on W H with 64 d⇥ kH kernels to capture relations across embedding dimensions.']"
"The generated convolutional feature maps cHi are subject to max-over-time pooling, resulting in 64 features m̂H .",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"Finally, a message representation m̂ is constructed by concatenating m̂L and m̂H , i.e., creating a 128-dimensional feature vector, for characterizing both low- and high-level semantics of a message m. In this paper, both kC and kH are set to 5 while computing high-level representations.",3.4.1 Hierarchical CNN for Message Representation,[0],[0]
"A Siamese structure with two identical subnetworks is useful to exploit the affinity between representations of two instances in the same hidden space (Severyn and Moschitti, 2015; Yin et al., 2016; Wang and Jiang, 2017).",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"For similarity estimation, we propose the Siamese hierarchical CNN (SHCNN) using a Siamese structure that blends the outputs from two HCNNs as well as some context features.
",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
Figure 5 shows the structure of the SHCNN for estimating the similarity between two messages mi and mj where the message representations m̂i and m̂j are generated by two sub-networks HCNNs (See Figure 4).,3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"There are many ways to deal with two sub-networks, such as using a similarity matrix (Severyn and Moschitti, 2015) or an attention matrix (Yin et al., 2016).",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"However, both methods lead to an enormous number of parame-
ters for long messages.",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"We propose to independently compute the element-wise absolute differences (Mueller and Thyagarajan, 2016) between a pair of message representations m̂i and m̂j , each from a sub-network.",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"More formally, the absolute difference d is a vector where the k-th element is computed as |m̂i(k) m̂j(k)|.",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
This approach provides not only fewer parameters but also the flexibility to observe interactions among different dimensions in representations.,3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"Our experiments also show it outperforms the other two approaches in similarity estimation (See Section 4).
",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"In addition to message contents, contexts such as temporal and user information were also usually considered in previous studies about conversation disentanglement (Wang and Oard, 2009; Elsner and Charniak, 2010, 2011).",3.4.2 Siamese Hierarchical CNN (SHCNN),[1.0],"['In addition to message contents, contexts such as temporal and user information were also usually considered in previous studies about conversation disentanglement (Wang and Oard, 2009; Elsner and Charniak, 2010, 2011).']"
"In this paper, we focus on the performance of message content representations and only incorporate four context features: speaker identicality, absolute time difference and the number of duplicated words with and without weighting by inverse document frequency (Christopher et al., 2008).",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"SHCNN concatenates the context features x(mi, mj) with the absolute difference d as the input of a fully-connected layer of the same size.
",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
The final output of SHCNN ŷ,3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"(mi, mj) is normalized by a logistic sigmoid function (Han and Moraga, 1995), representing the probability P (z(mi) = z(mj)).",3.4.2 Siamese Hierarchical CNN (SHCNN),[0],[0]
"All convolutional layers and the fully-connected layer require activation functions, and the choice affects the performance (Maas et al., 2013).",3.4.3 Activation Functions,[0],[0]
"Popular functions include rectified linear units (ReLUs) (LeCun et al., 2015), hyperbolic tangent
units (tanh) and exponential linear units (ELUs) (Clevert et al., 2016).",3.4.3 Activation Functions,[0],[0]
"In this study, we conducted informal comparison experiments and ELU was finally chosen for all functions because it performed the best.",3.4.3 Activation Functions,[0],[0]
"Given a set of annotated message pairs A = {(mi, mj , y)}, where y is a Boolean value indicating whether two messages are in the same conversation, SHCNN is optimized with binomial cross entropy (Goodfellow et al., 2016).",3.4.4 Optimization and Implementation Details,[0],[0]
"More formally, the objective function is as follows:
X
(mi,mj ,y)2A
",3.4.4 Optimization and Implementation Details,[0],[0]
[y · log(ŷ + ✏) +,3.4.4 Optimization and Implementation Details,[0],[0]
"(1 y) · log(1 ŷ + ✏)]+ ||✓||2
where ŷ simplifies ŷ(mi, mj), and ✏ is a small number, i.e., 10 9 in our experiments, preventing underflow errors.",3.4.4 Optimization and Implementation Details,[0],[0]
"The term serves as the weight for L2-regularization for the set of parameters ✓.
",3.4.4 Optimization and Implementation Details,[0],[0]
"In our experiments, SHCNN is implemented by TensorFlow (Abadi et al., 2016) and trained by the Adam optimizer (Kingma and Ba, 2015) with an initial learning rate of 10 3.",3.4.4 Optimization and Implementation Details,[0],[0]
"The dropout technique (Srivastava et al., 2014) is utilized in the fully-connected layer with a dropout probability of 0.1.",3.4.4 Optimization and Implementation Details,[0],[0]
"Word embeddings are initialized using the publicly available fastText 300-dimensional pretrained embeddings from Facebook (Bojanowski et al., 2016).",3.4.4 Optimization and Implementation Details,[0],[0]
"The batch size is set to 512, and the maximum number of training epochs is 1,000.",3.4.4 Optimization and Implementation Details,[0],[0]
The final model is determined by evaluating the mean average precision (MAP) on a validation dataset every 100 iterations.,3.4.4 Optimization and Implementation Details,[0],[0]
"In the second stage of conversation disentanglement, i.e., part (2) in Figure 2, we aim to separate conversations based on the identified message pairs and their estimated similarity.",3.5 Conversation Identification by SImilarity Ranking (CISIR),[1.0],"['In the second stage of conversation disentanglement, i.e., part (2) in Figure 2, we aim to separate conversations based on the identified message pairs and their estimated similarity.']"
"It is intuitive to apply graph-based methods if pairwise relationships of messages are exploited (Elsner and Charniak, 2008).",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"Furthermore, methods based on single-pass clustering (Wang and Oard, 2009) can be also be treated as graph-based methods.",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"However, graph-based methods have a risky drawback: A single false positive connection between two messages can be propagated to several messages from different conversations.",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"As shown
Algorithm 1: The algorithm of conversation disentanglement by similarity ranking (CISIR).
1 CISIR (M , D, r, h); Input : Message set M , the set of selected
message pairs D, the threshold of similarity ranks r and the threshold of similarity scores h.
Output:",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"A set of conversations C 2 Let G = (M , ;) be an undirected message
graph 3 for m 2 M do 4",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"Dm = {(mi, mj , ŷ)",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
| mi = m _,3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"mj = m} 5 Rank entries in Dm by ŷ in a descending order 6 for k = 1 to min(r, |Dm|) do 7",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"Let (mi, mj , ŷ) be the k-th entry in ranked Dm 8 if ŷ <",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"h then 9 break
10 Add an edge (mi, mj) into G
11 C = ConnectedComponents(G) 12 return C
in Figure 3, a certain percentage of message pairs are in different conversations, which can lead to numerous false positive connections.
",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"False alarms may be reduced by raising the threshold that determines whether two messages are connected (Wang and Oard, 2009).",3.5.1 Graph-based Methods and Conversation Connectivity,[1.0],"['False alarms may be reduced by raising the threshold that determines whether two messages are connected (Wang and Oard, 2009).']"
"However, a high threshold can make disentangled conversations fragmented and the best threshold for each pair could vary.",3.5.1 Graph-based Methods and Conversation Connectivity,[0],[0]
"Instead of setting a high threshold, we propose the algorithm of Conversation Identification by SImilarity Ranking (CISIR).",3.5.2 The CISIR Algorithm,[0],[0]
CISIR focuses on the top messages ranked by similarity scores.,3.5.2 The CISIR Algorithm,[0],[0]
"Based on Assumption 1, for each message, there exists at least one or more other messages in the same conversation posted closely in time.",3.5.2 The CISIR Algorithm,[0],[0]
"With this redundancy, a few pairs with stronger confidence, i.e., the top-ranked pairs, can be enough to extend a correct connectivity to earlier or later messages, while the low-ranked pairs can be ignored to reduce the risk of error propagation.
",3.5.2 The CISIR Algorithm,[0],[0]
"Given a set of selected message pairs with estimated similarity scores D = {(mi, mj , ŷ)}, Algorithm 1 shows the procedure of CISIR with two parameters r and h, where r is a high threshold
of similarity ranks and h is a lower threshold of similarity scores.",3.5.2 The CISIR Algorithm,[0],[0]
Note that CISIR filters out pairs with low scores because a message can have more than r same-conversation pairs posted in its T - hour time window.,3.5.2 The CISIR Algorithm,[0],[0]
"For each message, CISIR ranks all of its associated pairs by the estimated similarity and only retrieves the top-r pairs whose similarity scores are greater than h. These retrieved high-confidence pairs are treated as the edges in a message graph G. Finally, CISIR divides G into connected components, and the messages in each connected component are treated as a conversation.",3.5.2 The CISIR Algorithm,[0],[0]
"In this paper, we use grid search to set r and h as 5 and 0.5, respectively.",3.5.2 The CISIR Algorithm,[0],[0]
The efficiency of Algorithm 1 can be further improved.,3.5.3 Improvement of Time Complexity,[0],[0]
The top-r qualified pairs for each message can be pre-processed by a scan of D with |M | min-heaps which always contain at most r+1 elements.,3.5.3 Improvement of Time Complexity,[1.0],['The top-r qualified pairs for each message can be pre-processed by a scan of D with |M | min-heaps which always contain at most r+1 elements.']
"When r is a small constant number, it only takes O(|D|) = O(k · |M |) for pre-processing, where k is the maximum number of messages posted in a T -hour time window.",3.5.3 Improvement of Time Complexity,[0],[0]
"With preprocessed top pairs, CISIR can do graph construction and find connected components in O(k|M |), which compares favorably to conventional methods in O(|M |2).",3.5.3 Improvement of Time Complexity,[0],[0]
"In this section, we conduct extensive experiments on four publicly available datasets to evaluate SHCNN and CISIR in two stages.",4 Experiments,[0],[0]
"Three datasets from Reddit and one dataset of IRC are used as the experimental datasets.
",4.1.1 Datasets,[0],[0]
"• Reddit Datasets4 The Reddit dataset is comprised of all posts and corresponding comments in all sub-reddits (i.e., forums in Reddit.com) from June 2016 to May 2017.",4.1.1 Datasets,[0],[0]
Comments under a post can be treated as messages in one conversational thread.,4.1.1 Datasets,[1.0],['Comments under a post can be treated as messages in one conversational thread.']
Here we manually merge all comments in a sub-reddit to construct a synthetic dataset of interleaved conversations.,4.1.1 Datasets,[0],[0]
"Note that although it is called a “synthetic dataset,” all messages are written by real users.",4.1.1 Datasets,[0],[0]
"Three subreddits with different popularity levels as shown in Table 1 are selected to build three datasets: gadgets, iPhone and politics.",4.1.1 Datasets,[0],[0]
"4The organized Reddit dataset is publicly available in https://files.pushshift.io/reddit/.
• IRC Dataset.",4.1.1 Datasets,[0],[0]
"An annotated IRC dataset used in (Elsner and Charniak, 2008) is also included in our experiments.",4.1.1 Datasets,[0],[0]
The IRC dataset consists of about 6 hours of messages in interleaved conversations.,4.1.1 Datasets,[0],[0]
"Even though the IRC dataset is significantly smaller and shorter than the Reddit datasets, it consists of natural, interleaved conversations with ground truth annotations, including thread id.",4.1.1 Datasets,[0],[0]
Humans may not participate in a large number of simultaneous conversations.,4.1.2 Experimental Settings,[0],[0]
"e.g., an average of 1.79 for eight people (Aoki et al., 2006), but there could be hundreds of concurrent posts in a subreddit.",4.1.2 Experimental Settings,[0],[0]
"Hence, we adjusted the datasets to be more similar to real conversations.",4.1.2 Experimental Settings,[0],[0]
Specifically we removed some conversations so that every dataset has at most ten conversations at any point in time.,4.1.2 Experimental Settings,[0],[0]
Short messages with less than five words are also removed because even for humans they are frequently ambiguous.,4.1.2 Experimental Settings,[0],[0]
"Too short conversations with less than ten messages are also discarded as outliers (Ren et al., 2011).",4.1.2 Experimental Settings,[0],[0]
"Training and validation data are randomly chosen from only 10% of the selected message pairs, respectively, because in real situations obtaining labels could be very costly.",4.1.2 Experimental Settings,[0],[0]
The remaining 80% of pairs are regarded as testing data.,4.1.2 Experimental Settings,[0],[0]
"As a result, Table 1 shows the statistics of the four datasets after pre-processing.",4.1.2 Experimental Settings,[0],[0]
"Message pair similarity estimation is treated as a ranking task and evaluated with three ranking evaluation metrics: precision at 1 (P@1), mean average precision (MAP) and mean reciprocal rank (MRR) (Christopher et al., 2008).",4.2 Pairwise Similarity Estimation,[0],[0]
"We compare the performance with six baseline methods, including the difference of posted time (TimeDiff ), sameness of speakers (Speaker), cosine similarity of text (Text-Sim), the approach proposed by Elsner and Charniak (2008) (Elsner), DeepQA (Severyn and Moschitti, 2015) and ABCNN (Yin et al., 2016).",4.2 Pairwise Similarity Estimation,[0],[0]
Note that DeepQA and ABCNN are neural network-based models for questionanswering.,4.2 Pairwise Similarity Estimation,[1.0],['Note that DeepQA and ABCNN are neural network-based models for questionanswering.']
"The approach of Mehri and Carenini
(2017) was not compared in our experiments because the RNN requires additional message sequences; moreover, its performance was only mildly better than Elsner, which performed poorly on IRC in Table 2.
",4.2 Pairwise Similarity Estimation,[0],[0]
Table 2 shows the performance of similarity estimation.,4.2 Pairwise Similarity Estimation,[0],[0]
"Among all methods, neural network approaches (Severyn and Moschitti, 2015; Yin et al., 2016) perform better than other methods in most cases, indicating that message content representation has considerable impact on estimating pairwise similarity.",4.2 Pairwise Similarity Estimation,[0],[0]
SHCNN outperforms most of the baselines even if only low-level (L) or high-level (H) representations are exploited.,4.2 Pairwise Similarity Estimation,[0],[0]
"When SHCNN captures both low- and high-level semantics, it significantly outperforms all baselines across the four datasets.",4.2 Pairwise Similarity Estimation,[0],[0]
"For example, ABCNN can outperform SHCNN using only either low- or high-level representations in the politics dataset; however, SHCNN turns the tables after using both representations.",4.2 Pairwise Similarity Estimation,[0],[0]
An interesting observation is that ABCNN is the best baseline in every dataset except for IRC; this may be because the IRC data is too small to train complicated attention structures.,4.2 Pairwise Similarity Estimation,[0],[0]
"On the contrary, our SHCNN can precisely capture semantics even with few parameters and limited data.
",4.2 Pairwise Similarity Estimation,[0],[0]
"To shed deeper insights of how SHCNN surpasses other methods, we exhibit the prediction
results of the IRC data and demonstrate the capability of SHCNN to simultaneously preserve local and more global information.",4.2 Pairwise Similarity Estimation,[0],[0]
Figure 6 presents an example to show how SHCNN is better than other methods in capturing more high-level topical information.,4.2 Pairwise Similarity Estimation,[0],[0]
"Even though the main sentences of two messages are clearly on different topics, the baseline method DeepQA (Severyn and Moschitti, 2015) still predicts a high similarity.",4.2 Pairwise Similarity Estimation,[0],[0]
"This could be attributed to the context of author mention (Wang and Oard, 2009) and a bias on the local information, i.e., the exact same term “Arlie”, in the Siamese network used in DeepQA.",4.2 Pairwise Similarity Estimation,[0],[0]
"On the contrary, SHCNN can capture more global information that differentiates the topics and correctly predicts a very low score.",4.2 Pairwise Similarity Estimation,[0],[0]
Figure 7 illustrates another example of how SHCNN outperforms other methods in preserving the similarity of local information.,4.2 Pairwise Similarity Estimation,[0],[0]
Both of the messages in the example have some segments related to software engineering.,4.2 Pairwise Similarity Estimation,[0],[0]
"A baseline method ABCNN (Yin et al., 2016) with multiple-layer CNNs, however, still predicts a low score.",4.2 Pairwise Similarity Estimation,[1.0],"['A baseline method ABCNN (Yin et al., 2016) with multiple-layer CNNs, however, still predicts a low score.']"
This might be because both sentences are long so that the local information is diluted after processing by multiple CNN layers.,4.2 Pairwise Similarity Estimation,[0],[0]
"Differently, SHCNN is able to seize local information, correctly predicting a high score.",4.2 Pairwise Similarity Estimation,[0],[0]
"For conversation identification, three clustering metrics are adopted for evaluation: normalized mutual information (NMI), adjusted rand index (ARI) and F1 score (F1).",4.3 Conversation Identification,[1.0],"['For conversation identification, three clustering metrics are adopted for evaluation: normalized mutual information (NMI), adjusted rand index (ARI) and F1 score (F1).']"
"Six methods are implemented as the baselines for conversation disentanglement, including Doc2Vec (Le and Mikolov, 2014), blocks of 10 messages (Block-10), messages of respective speakers (Speaker) (Elsner and Charniak, 2011), context-based message expansion (CBME) (Wang and Oard, 2009) and a graph-theoretical model with chat- and contentspecific features (Elsner and Charniak, 2008) (GTM).",4.3 Conversation Identification,[0],[0]
"The embedding-based clustering method, i.e., Doc2Vec, applies affinity propagation (Frey and Dueck, 2007) to cluster messages embedded using Doc2Vec without being given the number of clusters, with the idea that messages in the same conversation would form a cluster.",4.3 Conversation Identification,[0],[0]
"Note that message pairs in the training and validation data are not utilized in prediction for a fair comparison to all methods.
",4.3 Conversation Identification,[0],[0]
Table 3 shows the performance of conversation disentanglement.,4.3 Conversation Identification,[0],[0]
Note that “Oracle” represents the optimal performance for CISIR when all message pairs in identical conversations in D are correctly retrieved.,4.3 Conversation Identification,[0],[0]
"Because pairs in D may not have enough coverage to connect all messages in a coversation, the optimal performance could be lower than 1.0.",4.3 Conversation Identification,[0],[0]
"CISIR performs better than all baseline methods for all datasets, and achieves excellent performance in IRC, due in part to the high-performing similarity estimates from the first stage.",4.3 Conversation Identification,[0],[0]
"Among the baseline methods, GTM performs relatively well on all datasets except for IRC.",4.3 Conversation Identification,[0],[0]
"This is because messages are more frequently posted in the IRC dataset, thereby increasing the number of incorrect pairs in the constructed graph.",4.3 Conversation Identification,[0],[0]
"Examining the graph constructed by GTM, there are only two connected components, indicating that many conversations were in-
correctly combined; in contrast, CISIR may be exempt from error propagation because it only relies on top-ranked pairs.",4.3 Conversation Identification,[0],[0]
Doc2Vec is trained to predict words in a document in an unsupervised manner.,4.3 Conversation Identification,[1.0],['Doc2Vec is trained to predict words in a document in an unsupervised manner.']
Its lowest performance in the experiments may point out a need for supervised learning in the specific task of conversation disentanglement to tackle the variation in semantic patterns.,4.3 Conversation Identification,[0],[0]
Time and author contextual cues do help conversation disentanglement as seen in the results of Block-10 and Speaker.,4.3 Conversation Identification,[1.0],['Time and author contextual cues do help conversation disentanglement as seen in the results of Block-10 and Speaker.']
Both of these contexts are integrated into our model.,4.3 Conversation Identification,[1.0],['Both of these contexts are integrated into our model.']
"In this paper, we propose a novel framework for disentangling conversations, including similarity estimation for message pairs and conversation identification.",5 Conclusions,[0],[0]
"In contrast to previous work, we assume that we do not need to select all message pairs in the first stage, thereby reducing computational time without sacrificing performance too much.",5 Conclusions,[0],[0]
"To estimate conversation-level similarity, a Siamese Hierarchical Convolutional Neural Network, SHCNN, is proposed to minimize the estimation error as well as preserve both the lowand high-level semantics of messages.",5 Conclusions,[0],[0]
"In the second stage, we developed the Conversation Identification by SImilarity Ranking, CISIR, algorithm, which exploits the assumption made in the first stage and identifies individual, entangled conversations with high-ranked message pairs.",5 Conclusions,[0],[0]
Extensive experiments conducted on four publicly available datasets show that SHCNN and CISIR outperform several existing approaches in both similarity estimation and conversation identification.,5 Conclusions,[0],[0]
We would like to thank the anonymous reviewers for their helpful comments.,Acknowledgement,[0],[0]
"The work was partially supported by NIH U01HG008488, NIH R01GM115833, NIH U54GM114833, and NSF IIS-1313606.",Acknowledgement,[0],[0]
"An enormous amount of conversation occurs online every day, such as on chat platforms where multiple conversations may take place concurrently.",abstractText,[0],[0]
Interleaved conversations lead to difficulties in not only following discussions but also retrieving relevant information from simultaneous messages.,abstractText,[0],[0]
Conversation disentanglement aims to separate intermingled messages into detached conversations.,abstractText,[0],[0]
"In this paper, we propose to leverage representation learning for conversation disentanglement.",abstractText,[0],[0]
"A Siamese hierarchical convolutional neural network (SHCNN), which integrates local and more global representations of a message, is first presented to estimate the conversation-level similarity between closely posted messages.",abstractText,[0],[0]
"With the estimated similarity scores, our algorithm for conversation identification by similarity ranking (CISIR) then derives conversations based on highconfidence message pairs and pairwise redundancy.",abstractText,[0],[0]
Experiments were conducted with four publicly available datasets of conversations from Reddit and IRC channels.,abstractText,[0],[0]
The experimental results show that our approach significantly outperforms comparative baselines in both pairwise similarity estimation and conversation disentanglement.,abstractText,[0],[0]
Learning to Disentangle Interleaved Conversational Threads with a Siamese Hierarchical Network and Similarity Ranking,title,[0],[0]
