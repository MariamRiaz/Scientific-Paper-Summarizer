0	19	Neural Machine Translation (NMT) has shown remarkable progress in recent years.
1	25	However, it requires large amounts of bilingual data to learn a translation model with reasonable quality (Koehn and Knowles, 2017).
2	35	This requirement can be compensated by leveraging curated monolingual linguistic resources in a multi-task learning framework.
4	21	Multi-Task Learning (MTL) is an effective approach for leveraging commonalities of related tasks to improve performance.
7	68	Their method shares components of the SEQ2SEQ model among the tasks, e.g. encoder, decoder or the attention mechanism.
8	33	However, this approach has two limitations: (i) it fully shares the components, and (ii) the shared component(s) are shared among all of the tasks.
9	36	The first limitation can be addressed using deep stacked layers in encoder/decoder, and sharing the layers partially (Zaremoodi and Haffari, 2018).
10	11	The second limitation causes this MTL approach to suffer from task interference or inability to leverages commonalities among a subset of tasks.
12	7	In this paper, we address the task interference problem by learning how to dynamically control the amount of sharing among all tasks.
17	14	Sharing the parameters of the recurrent units among different tasks is indeed sharing the knowledge for controlling the information flow in the hidden states.
18	19	Sharing these parameters among all tasks may, however, lead to task interference or inability to leverages commonalities among subsets of tasks.
23	7	In MTL, the tasks can use different subsets of these shared experts.
31	26	At each time step, the routing network is responsible to softly forward the input to the shared blocks conditioning on the input xt, and the previous hidden state of the unit ht−1 as follows: st = tanh(Wx · xt +Wh · ht−1 + bs), τt = softmax(Wτ · st + bτ ), where W ’s and b’s are the parameters.
32	13	Then, the i-th shared block is fed with the input of the 1multiple recurrent units can be stacked on top of each other to consist a multi-layer component unit modulated by the corresponding output of the routing network x̃(i)t = τt[i]xt where τt[i] is the scalar output of the routing network for the i-th block.
33	7	The hidden state of the unit is the concatenation of the hidden state of the shared and taskspecific parts ht = [h (shared) t ;h (task) t ].
34	20	The state of task-specific part is the state of the corresponding block h(task)t = h (n+1) t , and the state of the shared part is the sum of states of shared blocks weighted by the outputs of the routing network h (shared) t = ∑n i=1 τt[i]h (i) t .
35	8	Each block is responsible to control its own flow of information via a standard gating mechanism.
36	29	Our recurrent units are agnostic to the internal architecture of the blocks; we use the gated-recurrent unit (Cho et al., 2014) in this paper.
49	21	It is assembled from English-Farsi parallel subtitles from the TED corpus (Tiedemann, 2012), accompanied by all the parallel news text in LDC2016E93 Farsi Representative Language Pack from the Linguistic Data Consortium.
67	11	For this task, we have used the Abstract Meaning Representation (AMR) corpus Release 2.0 (LDC2017T10)5.
68	20	This corpus contains natural language sentences from newswire, weblogs, web discussion forums and broadcast conversations.
81	14	We use Adam optimizer (Kingma and Ba, 2014) with the initial learning rate of 0.003 for all the tasks.
84	14	All models are trained for 50 epochs and the best models are saved based on the perplexity on the dev set of the translation task.
88	12	Table 1 reports the results for the baselines and our proposed method on the two aforementioned translation tasks.
90	9	As seen, partial parameter sharing is more effective than fully parameter sharing.
92	29	The improvements in the translation quality of NMT models trained by our MTL method may be attributed to less interference with multiple auxiliary tasks.
97	21	The second block is mostly used by the semantic and syntactic parsing tasks, so specialized for them.
98	27	This confirms our model leverages commonalities among subsets of tasks by dedicating common blocks to them to reduce task interference.
100	33	We address the task interference issue in previous MTL models by extending the recurrent units with multiple blocks along with a trainable routing network.
101	31	Our experimental results on low-resource English to Farsi and Vietnamese datasets, show +1 BLEU score improvements compared to strong baselines.
