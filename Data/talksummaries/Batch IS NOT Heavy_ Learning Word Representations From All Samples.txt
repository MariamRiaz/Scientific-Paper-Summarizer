0	21	Representing words using dense and real-valued vectors, aka word embeddings, has become the cornerstone for many natural language processing (NLP) tasks, such as document classification (Sebastiani, 2002), parsing (Huang et al., 2012), discourse relation recognition (Lei et al., 2017) and named entity recognition (Turian et al., 2010).
1	49	Word embeddings can be learned by optimizing that words occurring in similar contexts have similar embeddings, i.e. the well-known distributional hypothesis (Harris, 1954).
2	53	A representative method is skip-gram (SG) (Mikolov et al., 2013a,b), which realizes the hypothesis using a shallow neural network model.
8	53	These drawbacks of SGD can be attributed to its one-sample learning scheme, which updates parameters based on one training sample in each step.
13	23	Taking the word embedding learning as an example, if the vocabulary size is |V |, then evaluating the loss function and computing the full gradient takes O(|V |2k) time, where k is the embedding size.
70	32	Once specifying the loss function, the main challenge is how to perform an efficient optimization for Eq.(1).
71	23	In the following, we develop a fast batch gradient optimization algorithm that is based on a partition reformulation for the loss and a decouple operation for the inner product.
72	27	As can be seen, the major computational cost in Eq.
73	35	(1) lies in the term LN , because the size of (V×V ) \S is very huge, which typically contains over billions of negative examples.
75	36	The loss partition serves as the prerequisite for the efficient computation of full batch gradients.
76	34	LN= ∑ w∈V ∑ c∈V α−c (r −−UwŨTc )2− ∑ (w,c)∈S α−c (r −− UwŨTc )2 (6) By replacing LN in Eq.
79	32	Finally, we achieve a reformulated loss L = ∑ w∈V ∑ c∈V α−c (r −−UwŨTc ) 2 ︸ ︷︷ ︸ LA + ∑ (w,c)∈S (α+wc − α−c )(∆− UwŨTc ) 2 ︸ ︷︷ ︸ L P ′ +C (7) where ∆ = (α+wcr + wc − α−c r−)/(α+wc − α−c ).
86	21	Interestingly, we observe that the summation operator and elements in Uw and Ũc can be rearranged by the commutative property (Dai et al., 2007), as shown below.
90	20	Specifically, we define pwdd′ , p c dd′ , q w d and q c d as the pre-calculated terms pwdd′ = ∑ w∈V uwduwd′ q w d = ∑ w∈V uwd pcdd′ = ∑ c∈V α−c ũcdũcd′ q c d = ∑ c∈V α−c ũcd (10) Then the computation of L̃A can be simplified to∑k d=0 ∑k d′=0 p w dd′p c dd′ − 2r−qwd qcd.
93	36	As a result, the total time complexity of computing LA is decreased toO(2|V |k2+2|V |k+k2) ≈ O(2|V |k2), which is much smaller than the originalO(k|V |2).
100	25	Regarding the complexity of AllVec, we can see that the overall complexity of Algorithm 1 is O(4|S|k + 4|V |k2).
104	21	We conduct experiments on three popular evaluation tasks, namely word analogy (Mikolov et al., 2013a), word similarity (Faruqui and Dyer, 2014) and QVEC (Tsvetkov et al., 2015).
138	41	While both AllVec and GloVe use global contexts, AllVec performs much better than GloVe in syntactic tasks.
140	26	By contrast, local-window based methods, such as SG and SGA, are more effective to capture local sentence features, resulting in good performance on syntactic analogies.
143	28	We can see that AllVec achieves the best performance in most tasks, which admits the advantage of batch learning with all samples.
146	19	In this subsection, we investigate the impact of the proposed weighting scheme for negative (context) words.
157	34	It also helps to explain why GloVe performs poorly on syntactic tasks.
176	52	Despite this, the total run time of AllVec is still in a feasible range.
177	54	Assuming the convergence is measured by the number of parameter updates, our AllVec yields a much faster convergence rate than the one-sample SG method.
182	29	This means we can achieve the embarrassing parallelization by simply separating the updates by words; that is, letting different workers update the model parameters for disjoint sets of words.
184	117	In this paper, we presented AllVec, an efficient batch learning based word embedding model that is capable to leverage all positive and negative training examples without any sampling and approximation.
185	217	In contrast with models based on SGD and negative sampling, AllVec shows more stable convergence and better embedding quality by the all-sample optimization.
186	59	Besides, both theoretical analysis and experiments demonstrate that AllVec achieves the same time complexity with the classic SGD models.
187	142	In future, we will extend our proposed all-sample learning scheme to deep learning methods, which are more expressive than the shallow embedding model.
189	37	Lastly, we are interested in exploring the recent adversarial learning techniques to enhance the robustness of word embeddings.
