15	8	In Section 4 we report our experimental results.
18	11	The article A is also associated with a set of comments C(A) = c1, ..., cl.
19	68	The task is to link comments c ∈ C(A) with article segments s ∈ S(A).
23	7	First, we split the news article into segments.
29	10	Next, we use either words with stop-word removal (step 3)) or terms (shown in 4) where each term is split by a semicolon) to represent the article sentence and also each comment.
32	8	This work investigates a simple method for linking comments and news article sentences using a linear combination of similarity scores as computed through a number of different similarity metrics (features).
35	20	Otherwise, a similarity score is computed and articles are linked if their similarity score is above a threshold.
37	31	Each metric is computed based on the comment c ∈ C(A) and a segment s ∈ S(A) as input.
38	5	We pair every segment from S(A) with every comment from C(A).
40	23	We link all comments including quotes to the article sentences they quote.
42	12	If the quoteScore exceeds an experimentally set threshold of 0.5 (50% of consecutive article segment words are found in the same order within the comment), then the segment is regarded as quoted in the comment, the commentsegment pair is linked, their linking Score is set to quoteScore and no further linking features are considered.
44	13	If a comment does not contain a quote as described above, we compute the following features to obtain the value of the similarity score without considering the quote feature: • Cosine: The cosine similarity (Salton and Lesk, 1968) computes the cosine angle between two vectors.
45	7	We fill the vectors with terms/word frequencies extracted from the article segment/comment.
46	11	• Dice: dice = 2 ∗ len(I(S, C)) len(S) + len(C) (1) where I(S, C) is the intersection set between the terms/words in the segment and in the comment.
49	23	• NE overlap: NEoverlap = len(I(S, C)) len(U(S, C)) (3) where I(S, C) is the intersection set between the named entities (NEs) in the segment and in the comment and U(S, C) is the NEs union set.
51	8	Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences.
52	10	DISCO computes two different similarities between words: DISCO1 and DISCO2.
56	17	The weights are trained based on linear regression using the Weka package and the training data described in the following section.
78	8	The AT data set consists of articles with comments downloaded from the technology news website Ars Technica (AT).
106	12	The results showed that the overall performance of combined quote and similarity metrics is comparable to that of topic modeling method despite substantial domain difference between training and testing data sets.
120	78	More precisely, when our linking approach found a link between a sentence in the comment and an article sentence it also linked all the remaining sentences within the comment to the article sentence.
121	31	The evaluation was performed with English and Italian data.
125	176	Anything below this threshold was not linked.
127	11	For English both our runs were considered.
128	15	However, for Italian there has been some problems in the submission, so that our second run with the threshold 0.5 was not considered.
129	49	Our results for English are that using our second run we obtained better results compared to all other 8 system submissions.
130	6	With this set-up we achieved 89% precision.
132	23	With this score it became the 5th system.
133	33	For Italian our first run got the 6th position scoring 89% precision.
134	62	Since our first run also did not perform well on the English data, it is likely that the performance on the Italian data would have been better could the second run be submitted.
