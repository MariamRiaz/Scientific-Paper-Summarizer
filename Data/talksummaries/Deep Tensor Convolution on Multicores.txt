17	86	Even in the case of the most successful distributed frameworks for ConvNets (Abadi et al., 2016), GPU memory management is largely unresolved.
18	23	The TensorFlow authors propose two partial solutions warranting further investigation: (a) re-computing versus storing large tensors; and (b) transferring long-lived tensors from GPU to host CPU memory.
26	47	Consider the 1-dimension convolution s = g∗d, where the kernel and data vectors are of length G and D. This problem can be rephrased as one of polynomial multiplication by introducing the associated polynomials d(x), g(x) and s(x) = g(x)d(x), where the coefficients si = ∑ k gi−kdk of xi are the solution to the desired convolution.
29	49	With respect to Algorithm 1, Step (1) is implemented by the kernel and data trans- forms, Step (2) by their transformed element-wise product and Step (3) by the final inverse transform.
38	33	Lavin has published code that automates this procedure using the Cook-Toom algorithm to produce transformed kernels and data both of length D (Lavin, 2016).
40	83	Inappropriate selection of m(x) would yield matrices of polynomials (degree > 1) that require considerably more scalar multiplications to compute.
54	62	Below we present an alternative approach to fast convolution that removes the need to hand-craft minimal algorithms.
60	40	Instead of crafting a minimal algorithm, we show how relaxed memory constraints and efficient sparse linear algebra of CPU systems can be leveraged to amortize transform costs.
80	23	(3) In our case U is sparse and X is dense, so we implement (3) such that U is traversed in the outermost two loops.
83	23	(4) It is straightforward to show that (1) is a special case of (4) by considering the following equivalence: Y = X ×n U⇔ Y(n) = UX(n), where the matrix X(n) is the mode-n major unfolding of tensor X (Kolda & Bader, 2009).
84	53	In the 1-dimensional case, X(1) is simply x and thus X ×1 U = Ux.
88	20	The reasons are two-fold: (a) the matrix multiplication cost can be amortized across a larger number of kernels and channels due to relaxed memory constraints; and (b) CPUs are able to directly leverage the sparse structure of these matrices for further acceleration.
95	22	The game changes when one considers these approaches in the context of a ConvNet layer with multiple channels and kernels.
96	23	Without loss of generality, assume the numbers of kernels and channels are both equal to M .
97	37	As the inverse transform can be applied once over the reduced output and the data transform once across all kernels, the required number of multiplications is just 4M2 + 24M (versus 6M2 for Winograd).
98	16	Although it is also possible to restructure Winograd’s algorithm to exploit the size of the network, for larger networks the 4M2 multiplications required by the elementwise product quickly renders the linear transform cost neg- ligible.
121	31	Looking beyond a single core, a recent review demonstrates poor multicore scalability across all major ConvNet frameworks (Shi et al., 2016a).
126	46	We can compute the ratio of computations, i.e. 1 multiply and 1 accumulate operation per (g, d)-pair, to the volume of memory loaded: computations memory accesses = Little’s Law shows this is problematic for effective CPU utilization, as convolution expressed in this form is bottlenecked by memory bandwidth (Little, 1961).
127	27	To solve this problem, recall that D is one of many small, overlapping tiles that span the full-size feature map.
129	23	, DN captures a single (x, y) coordinate in the earlier G′ D′ element-wise product, which is fused with the channelwise reduction into end-to-end matrix multiplications: computations memory accesses = DN (MT +MK) = T +K .
149	36	Single-core utilization is just one dimension of performance optimization.
152	32	To avoid memory contention and other concurrency issues we adopt the Cilk Plus work-stealing scheduler supported by GCC 4.8 (Blumofe et al., 1996; Robison, 2013), simply applying its fork-join primitive to all for-loops with no iteration dependencies.
153	79	The number of tiles T per thread is empirically tuned to simultaneously maximize L3 cache utilization (T cannot be too large) and compute-to-memory ratio (T cannot be too small).
154	99	We observe that even this simple parallelization scheme yields near-optimal linear scalability.
156	85	Scalability is measured across a single convolution layer for a 1024 × 1024 image with kernels of size 4 × 4.
157	30	To avoid NUMA issues relating to expensive inter-chip communication, we spawn independent instances for each CPU in our 4-socket sharedmemory server such that all 18 threads in Figure 3 are bound to a single chip.
162	171	The only study we could find presenting thorough CPU benchmarking is that of Shi et al., comparing the throughput of Caffe, CNTK, Tensorflow and Torch for the AlexNet and ResNet architectures (Shi et al., 2016a).
165	32	They adopt an ear- lier version of TensorFlow that uses the Eigen 3.2 library (no AVX/FMA support), and otherwise use the default framework-specific implementations of convolution rather than linking to optimized packages such as Intel MKL.
166	138	We benchmark 2D ConvNet performance against two popular frameworks: TensorFlow, using the newer Eigen 3.3 library (with AVX support); and Caffe, compiled to use Intel’s optimized MKL library.
172	22	An important innovation of our approach is that it is batch size-agnostic, making it suitable for single-image autoregressive models common in generative modelling and deep reinforcement learning.
