0	59	Researchers distinguish the problem of general knowledge question answering from that of reading comprehension (Hermann et al., 2015; Hill et al., 2016).
2	31	First, reading comprehension systems must infer answers from a given unstructured passage rather than structured knowledge sources such as Freebase (Bollacker et al., 2008) or the Google Knowledge Graph (Singhal, 2012).
3	2	Second, machine comprehension systems cannot exploit the large level of redundancy present on the web to find statements that provide a strong syntactic match to the question (Yang et al., 2015).
5	27	In this paper, we describe the construction of a new reading comprehension dataset that we refer to as “Who-did-What”.
7	73	The question is formed by deleting a person named entity from the first sentence of the question article.
9	15	Our dataset differs from the CNN and Daily Mail comprehension tasks (Hermann et al., 2015) in that it forms questions from two distinct articles rather than summary points.
11	57	This also reduces the syntactic similarity between the question and the relevant sentences in the passage, increasing the need for deeper semantic analysis.
13	18	This is also intended to produce problems requiring deeper semantic analysis.
14	118	The resulting dataset yields a larger gap between human and machine performance than existing ones.
15	8	Humans can answer questions in our dataset with an 84% success rate compared to the estimates of 75% for CNN (Chen et al., 2016) and 82% for the CBT named entities task (Hill et al., 2016).
16	66	In spite of this higher level of human performance, various existing readers perform significantly worse on our dataset than they do on the CNN dataset.
17	80	For example, the Attentive Reader (Hermann et al., 2015) achieves 63% on CNN but only 55% on Who-didWhat and the Attention Sum Reader (Kadlec et al., 2016) achieves 70% on CNN but only 59% on Whodid-What.
18	28	In summary, we believe that our Who-did-What dataset is more challenging, and requires deeper semantic analysis, than existing datasets.
39	1	We now describe the construction of our Who-didWhat dataset in more detail.
40	22	To generate a problem we first generate the question by selecting a random article — the “question article” — from the Gigaword corpus and taking the first sentence of that article — the “question sentence” — as the source of the cloze question.
44	27	For each person named entity we then identify a noun phrase in the automatic parse that is headed by that person.
51	10	The article should be independent of the question article but should discuss the people and events mentioned in the question sentence.
52	13	To find a passage we search the Gigaword dataset using the Apache Lucene information retrieval system (McCandless et al., 2010), using the question sentence as the query.
53	26	The named entity to be deleted is included in the query and required to be included in the returned article.
54	17	We also restrict the search to articles published within two weeks of the date of the question article.
55	35	Articles containing sentences too similar to the question in word overlap and phrase matching near the blanked phrase are removed.
59	21	We collect all person named entities in the passage except unblanked person named entities in the question.
66	26	Our duplication-removal process ensures that no two problems have very similar questions.
71	8	• Unigram: Select the most frequent last name us- ing the unigram counts from the 5-gram model.
72	53	To minimize the number of questions removed we solve an optimization problem defined by limiting the performance of each baseline to a specified target value while removing as few problems as possible, i.e., max α(C) ∑ C∈{0,1}|b| α(C)|T (C)| (1) subject to ∀i ∑ C:Ci=1 α(C)|T (C)| N ≤ k N = ∑ C∈{0,1}|b| α(C)|T (C)| (2) where T (C) is the subset of the questions solved by the subset C of the suppressed baselines, α(C) is a keeping rate for question set T (C), Ci = 1 indicates the i-th baseline is in the subset, |b| is the number of baselines, N is a total number of questions, and k is the upper bound for the baselines after suppression.
