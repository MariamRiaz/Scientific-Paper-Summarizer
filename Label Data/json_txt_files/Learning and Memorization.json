{"sections": [{"heading": "1. Introduction", "text": "Neural networks trained through stochastic gradient descent (SGD) can memorize their training data. Although practitioners have long been aware of this phenomenon, Zhang et al. (2017) recently brought attention to it by showing that standard SGD-based training on AlexNet gets close to zero training error on a modification of the ImageNet dataset even when the labels are randomly permuted. This leads to an interesting question: If neural nets have sufficient capacity to memorize random training sets why do\n1Two Sigma, New York, NY, USA. Correspondence to: Satrajit Chatterjee <satrajit.chatterjee@twosigma.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nthey generalize on real data? A natural hypothesis is that nets behave differently on real data than on random data. Arpit et al. (2017) study this question experimentally and show that there are apparent differences in behavior. They conclude that generalization and memorization depend not just on the network architecture and optimization procedure but on the dataset itself.\nBut what if networks fundamentally do not behave differently on real data than on random data, and, in both cases, are simply memorizing? This is a difficult question to explore for two reasons. First, it is hard to provide a direct answer. Whereas it is easy to tell when a net is memorizing random data (the training error goes to zero!), there is no easy way to tell when a network is memorizing real data as opposed to \u201clearning\u201d. Second, and perhaps more importantly, it contradicts the intuitive notion\u2014inherent in the preceding discussion\u2014that memorization and generalization are at odds. This work attempts to shed light on this second difficulty by investigating the following: How much can you learn if memorization is all you can do? Is generalization even possible in this setting?\nAt first, generalization in such a setting of pure memorization may seem hopeless: the simplest way to memorize would be to build a lookup table from the training data. Although this approach works for special cases where the input population is finite and small, it fails in general since the examples seen during training are unlikely to match test examples. One way to get around this limitation is to use k-Nearest Neighbors (k-NN) or any of its variants at test time. While k-NNs work well on many problems, they fail on problems where it is not easy to construct a semantically meaningful distance function on the input space. In such cases, the obvious syntactic distance functions (e.g., say Euclidean distance between images viewed as vectors in Rd) do not work well. Indeed some of the most interesting results from deep learning have been the discovery\u2014through learning\u2014of semantically meaningful distance functions (via embeddings).\nTherefore, in this work we do not allow ourselves a distance function. Instead, we get around the problem by applying the notion of depth, which has been wildly successful in improving the performance of neural networks, to direct memorization. We build a network of lookup tables (also\ncalled \u201cluts\u201d) where the luts are arranged in successive layers much like a neural network. However, unlike a neural network, training happens through memorization and does not involve backpropagation, gradient descent, or any explicit search. Now, since in contrast to a neuron, the function implemented by a lut can be arbitrarily complex, without some means to control the complexity, the notion of depth is vacuous. We control the complexity of a function learned by a lut in the simplest possible way: we limit the support (and thereby the size) of the lut. Each lut in a layer receives inputs from only a few luts in the previous layer, which are picked at random when the network is constructed. This kind of restriction on local function complexity is similar to what is found to work well in deep neural networks. For example, a convolutional filter is obviously support-limited, and a fully connected layer although not support-limited is nevertheless limited in expressivity. Furthermore, the learned weight matrices in neural networks are often sparse or can be made so with no loss in accuracy (Han et al., 2015).\nWe need two restrictions before we can proceed to an algorithm. First, for simplicity, we focus our attention on binary classification problems. Second, because lookup tables work naturally with discrete inputs, in this work we limit ourselves to discrete signals. In fact, the inputs and all intermediate signals in the network of lookup tables are binary. The restriction is not as extreme as it may appear. There are a number of results in quantized and binary neural networks showing that limited precision is often sufficient (e.g. Rastegari et al., 2016). Furthermore, even in real-valued neural networks, we need mechanisms such as convolution and pooling to ensure that certain types of small changes in the inputs (e.g., a small displacement) do not lead to large changes in output. In principle, similar mechanisms could be used in a fully discrete setting to handle real-valued quantities.\nWith these restrictions in place, we are now ready to proceed."}, {"heading": "2. A Single Lookup Table", "text": "Let B = {0, 1} and consider the problem of learning a function f : Bk \u2192 B from a list of training examples where each example is an (x, y) pair. Since we want to learn by memorizing, we construct a lookup table with 2k rows (one for each possible bit pattern p \u2208 Bk that can appear at the input) and two columns y0 and y1. The y0 entry for the row corresponding to pattern p (denoted by cp0) counts how many times p is associated with output 0 in the training set, i.e., the number of occurrences of (p, 0) in the training set. Similarly, the y1 entry for row p (denoted by cp1) counts how many times the pattern p is associated with the output 1 in the training set, i.e., the number of occurrences of (p, 1)\nin the training set. Note that for a pattern p it is possible for both cp0 and cp1 to be greater than zero since due to Bayes error both (p, 0) and (p, 1) may be present in the training examples. It is also possible for both cp0 and cp1 to be zero if the input p never appears in the training examples. We call such a lookup table a k-input lookup table or a k-lut since the inputs are bit vectors of length k.1\nNext, we associate a boolean function f\u0302 : Bk \u2192 B with the lookup table in the following manner:\nf\u0302(p) =  1 if cp1 > cp0,0 if cp1 < cp0, b if cp1 = cp0\nwhere b \u2208 B is picked uniformly at random when fixing f\u0302 in order to break ties. In other words, f\u0302 maps an input p to the output that is most often associated with it in the training set (breaking ties randomly). We say that f\u0302 is the function learned by the lookup table.\nExample 1. Let k = 3 and consider learning a function f : B3 \u2192 B from 7 examples shown on the left below. The lookup table that we learn is shown in the middle, and the truth table of the learned function f\u0302 is shown on the right. The entries in the truth table which have been picked randomly to break ties are indicated by an asterisk.\nx x0x1x2\ny\n000 0 000 1 000 1 001 1 100 0 110 0 110 1\np x0x1x2\ny0 y1\n000 1 2 001 0 1 010 0 0 011 0 0 100 1 0 101 0 0 110 1 1 111 0 0\np f\u0302\n000 1 001 1 010 0\u2217 011 1\u2217 100 0 101 1\u2217 110 1\u2217 111 0\u2217\nNote that f\u0302 gets all training examples correct except for the first and sixth. This is the best we can do on this set of training examples because the Bayes error rate is non-zero.\nIf we measure training error as the average 0\u20131 loss on the training set, this procedure to learn f\u0302 has the following properties:\n1. Optimality. The learned function f\u0302 is Bayes-optimal on the training set, i.e., there is no function g : Bk \u2192 B with training error strictly less than that of f\u0302 . In particular, the training error is zero iff the training set has zero Bayes error.\n2. Monotonicity. If we have more information for each x in the training set, i.e., we augment each training\n1 Typically k is small (less than 10) and so the the table can be stored explicitly. The input bit vector (viewed as an integer) can be used to directly index into the table.\nexample with m extra bits of information (keeping the labels fixed) and use the above procedure to now learn a new function g\u0302 : Bk+m \u2192 B, then the training error of g\u0302 is no more than that of f\u0302 .\nProof Sketch. Optimality is easy to see since the total training error is the sum of the training error for each possible pattern p which is minimized by choosing the majority class for each p. Monotonicity holds since if not, then we can compose the obvious projection Bk+m \u2192 Bk with f\u0302 to get a contradiction with the optimality of g\u0302.\nNote that monotonicity implies in particular that the training accuracy at the output of a lut is no worse than that at any of its inputs. Furthermore, as we make the luts larger, the training error cannot increase but only decrease. This is interesting since there are no restrictions on the m extra bits: they could be completely non-informative. These properties will prove useful in the next section as we consider networks of luts.\nTo summarize, the procedure described to learn a single lookup table in this section is essentially memorization in the presence of Bayes error, where the idea is to simply remember the output that is most commonly associated with an input in the training set."}, {"heading": "3. A Network of Lookup Tables", "text": "Now consider a binary classification task on MNIST (LeCun & Cortes, 2010) of separating the digits \u20180\u2019 through \u20184\u2019 (we map these to the 0 class) from the digits \u20185\u2019 through \u20189\u2019 (the 1 class) where the pixels are 1-bit quantized. Thus the task is to learn a function f : B28\u00d728 \u2192 B. We call this the Binary-MNIST task (overloading binary here to mean both binary classification and binary inputs).\nIn principle, we could use the procedure in Section 2 to learn this function. However, since we have only 60,000 training examples in MNIST, most of the 228\u00d728 rows in the lookup table would have 0 entries in both columns, and hence the function learned would be mostly random and have very poor generalization to inputs outside the training set.\nAs discussed in the introduction, we get around this problem by introducing depth. Instead of learning a giant lookup table with 228\u00d728 entries, we learn a network of (much) smaller lookup tables. The network consists of d layers with each layer l (1 \u2264 l \u2264 d) having nl k-input lookup tables. Each lut in first layer (l = 1) receives its inputs from a krandom subset of the network inputs. A lut in a layer l > 1 receives inputs from a k-random subset of the luts in layer l \u2212 1. The connectivity is fixed at network creation time and does not change during training or inference. The final layer of the network has a single lookup table (i.e., nd = 1) which is the output of the network. By analogy with neural\nnetworks, we call the final layer the output layer and the other layers hidden layers.\nWe train the lookup tables layer by layer, where the target of each lookup table is the final output. We start from the first layer and work our way to the output. Once a layer has been learned, we use the functions associated with its luts (the f\u0302s of Section 2) to map its inputs to outputs. These outputs serve as the inputs for the next layer, which is learned next. Continuing our analogy with neural networks, we call the output values of a layer activations.\nInference is similar to training: We start from the inputs and evaluate each layer in order using the functions learned at each lut to map inputs to outputs.\nExample 2. We modify Example 1. Instead of learning a single lut with k = 3 inputs, we learn a network of k = 2 luts. The network shown in Figure 1 has d = 2 layers. The first layer has 2 luts (i.e., n1 = 2) which are connected to inputs x0 and x1 of the network. The second layer (which is also the output layer) has 1 lut (i.e., n2 = 1) which is connected to the outputs of the two luts in the first layer. (The connections were made randomly when the network was created.) Using the procedure in Section 2, the two lookup tables learned in the first layer (using y as the target) along with their corresponding functions f\u030210 and f\u030211 are:\np x0x1\ny0 y1 f\u030210\n00 1 3 1 01 0 0 1\u2217 10 1 0 0 11 1 1 1\u2217\np x0x2\ny0 y1 f\u030211\n00 1 2 1 01 0 1 1 10 2 1 0 11 0 0 1\u2217\nLet the output of the luts in the first layer be w10 and w11, i.e., w10 = f\u030210(x0x1) and w11 = f\u030211(x0x2). The learning problem for the lut in the second layer is shown in the tables below. For convenience, on the left we show the primary inputs x0, x1 and x2, the first layer activations w10 and w11 (which are the inputs of the lut), and the target output y. On the right we show the table and the learned function f\u030220:\nx x0x1x2\nw10w11 y\n000 11 0 000 11 1 000 11 1 001 11 1 100 00 0 110 10 0 110 10 1\np w10w11\ny0 y1 f\u030220\n00 1 0 0 01 0 0 1\u2217 10 1 1 0\u2217 11 1 3 1\nIn this case the function implemented by the network of 2-luts has the same performance on the training set as the function learned by the 3-lut in Example 1. Since there are fewer possible patterns in the case of smaller luts, we expect better pattern coverage during training and hence better generalization.\nImplementation. The memorization procedure described here is linear in the size of the training data, requiring two passes over the training set. It is computationally efficient since it only involves counting and dense table lookups and does not require floating point. It is also easy to parallelize since each lut in a given layer is independent, and the counts can be computed on disjoint subsets of the training data and then combined (using, for example, a reduction tree). Note that using this property it is possible to execute the algorithm on extremely large datasets where all the training examples may not fit on a single machine with only the summary statistics of the data (the counts in the lookup tables) being exchanged across machines."}, {"heading": "4. Experiments", "text": "Experiment 1. In the first experiment, we apply the above procedure to the Binary-MNIST task (as defined in Section 3) to see if this approach to memorization can generalize. For this experiment, we construct a network with 5 hidden layers of 1024 luts and 1 lut in the output layer. We set k = 8, i.e., each lut in the network takes 8 inputs.\nThe network achieves a training accuracy of 0.89 on this task, which is perhaps not so surprising since we are memorizing the training data after all. But what is surprising is that the network achieves an accuracy of 0.87 on a heldout set (the 10,000 test images in MNIST) which indicates generalization.\nThis result is not state-of-the-art on this variant of MNIST (see Experiment 4), but that is not the point. It is significantly above the 0.5 accuracy that would be expected by chance, and this is achieved by an algorithm that only memorizes and performs no explicit search.\nThe training and test accuracies are stable: there is very little variation from run to run. In other words, very little depends on the actual random choices made when deciding the topology of the network. To understand why this is\nthe case, we look at training accuracies of the luts in the network. Since the target for each lut in the network is the final classification target, we can examine the accuracy of a lut as a function of its layer.\nTable 1 shows the summary statistics for the accuracies of luts in each layer. We observe that as depth increases the average accuracy of the luts in a layer goes up. In other words, depth helps. Some intuition for this is provided by the monotonicity property of the luts: the output of a lut cannot have lower accuracy than any of its inputs (Section 2).\nFurthermore, we observe in Table 1 the dispersion in accuracy across the luts (measured either by standard deviation (std) or the difference between max and min) goes down. Therefore, as depth increases the specifics of the connectivity matters less and the network automatically becomes more stable with respect to the random choices made during construction. Indeed we can say something stronger: we have seen in our experiments (not shown in Table 1) that as depth increases the activations of the luts in a layer become more correlated with each other, and hence become more interchangeable. While this correlation is good for stability with respect to connectivity, it causes diminishing returns with additional depth.\nRemark. The perceptive reader looking at Table 1 will also notice that we are wasting computation: the single output lut in layer 6 receives input from only 8 of the 1024 luts in layer 5 and these in turn can at most receive inputs from 64 luts from layer 4. Although a different topology would be more computationally efficient, this specific choice allows us to compare the different layers more easily. We have not optimized this aspect since it typically takes less than 30 seconds using a single threaded unoptimized implementation (Python with NumPy) to run an experiment.\nExperiment 2. As discussed in the introduction and in Section 3, we do not expect unbridled memorization in the form of a large lookup table (say k = 28 \u00d7 28 in the case of Binary-MNIST) to generalize at all. This motivated our\nexploration of a network of smaller lookup tables parameterized by k (the number of inputs of each lut). We now vary k to see if we can control the amount of memorization and to see the effect it has on generalization. To avoid changing too much at once, we keep the number of layers and the number of luts per layer the same as in Experiment 1.\nThe results are shown in the first 3 columns of Table 2. With small values of k, the network finds it difficult to memorize the training data. As intuitively expected (see also the monotonicity property in Section 2), as k increases the training accuracy goes up with perfect memorization at k = 14, i.e., long before 28\u00d728. However, larger luts generalize less well, and the best test accuracy of 0.90 is achieved at k = 12 though with substantially good memorization of the training data (0.99). Interestingly, there is a clear monotonic increase in the generalization gap measured as the difference between training and test accuracy with increasing k.\nExperiment 3. In this experiment\u2014along the lines of those performed in Zhang et al. (2017)\u2014we randomly permute the labels in the training set and repeat Experiment 2 on this \u201crandom\u201d dataset. The results are shown in columns 4 and 5 of Table 2. As expected, with increasing k the network gets better at memorizing the training data, and the test accuracy hovers around chance (0.5) though with significant variation (\u00b1 0.05). This may be viewed as empirical evidence that the Rademacher complexity goes up with k.\nHowever, and this may be surprising for a pure memorization algorithm, memorizing random data turns out to be harder than memorizing real data (columns 2 and 3 of Table 2) in the sense that a larger k is required to get the same accuracy with random data than with real data. For example, it takes until k = 12 to get comparable training accuracy on random data as k = 4 gets on real data. This result corroborates the findings in Arpit et al. (2017, \u00a73 and \u00a74) that real data is easier to fit than random data. But it also means that we cannot conclude that any such difference observed in neural networks is because they do not use brute force memorization on real data. As this experiment shows, such\ndifferences can appear even with brute force memorization.\nFinally, at k = 12 we have a network that is able to memorize random data (random training accuracy of 0.82) and yet generalizes to test data when trained on real data (real test accuracy of 0.90). This is very similar to findings of Zhang et al. (2017) in the context of neural networks. Kawaguchi et al. (2017, \u00a73) argue that this phenomenon is universal and our result may be viewed as further empirical evidence for their claim showing that this phenomenon can happen even in the simplified setting of just memorization.\nExperiment 4. For completeness, we compare memorization with several standard methods and the results are shown in Table 3. We have not specifically tuned the other methods since our goal is not to beat the state-of-the-art but to get a sense of how memorization alone does when compared to the standard methods. The best performance is obtained by a LENET-style convolutional network with 2 convolutions (64 and 32 filters respectively) each followed by a corresponding max pool layer, and 3 fully connected layers (256, 128 and 2 units respectively) with softmax output. The net is trained for 6 epochs with stochastic gradient descent and dropout.\nOnce again, compared to random guessing which has 0.50 test accuracy, memorization does quite well with a test accuracy of 0.90 (using the k = 12 configuration from Experiment 2) and beats logistic regression and na\u0131\u0308ve Bayes. Interestingly, 1- and 5-Nearest Neighbors do well too (test accuracy of 0.97) though recall that they are provided with a distance function which memorization does not have access to and must in a sense discover.\nExperiment 5. We now consider the task of separating the i-th digit in MNIST from the j-th digit, which gives us( 10 2 ) = 45 binary classification tasks, which we collectively call Pairwise-MNIST. The images are binarized as before.\nFigure 2 shows the training accuracy and the test accuracy for each of those 45 experiments for 8 different values of k.\nAs in Experiment 2, we find that as k increases, the training accuracy increases (reaching 1.0), but the test accuracy falls off. If we look at the best test accuracies for a given task (across k), on 31 out of the 45 tasks, we do better than 0.98. The worst of these is 0.95 which is the best memorization can do for separating \u20184\u2019 and \u20189\u2019. This is still significantly better than the 0.5 we would expect by chance. Typically the best test accuracies are achieved at k = 6 and k = 8.\nExperiment 6. In Experiment 5 we notice that the variation is quite high for k = 2. This indicates that the depth of the network is insufficient for proper mixing. To investigate this further, we keep k = 2 and vary the number of hidden layers from 20 to 25. Each hidden layer still has 1024 luts. Figure 3 shows how the training and test accuracies vary with the depth of the network. It is interesting to note that the test accuracy continues to improve even for relatively deep networks (16 or 32 hidden layers), and we get very high test accuracies even with such small lookup tables. Furthermore, we note that the variation in the generalization error (difference between training and test accuracies) decreases with increasing depth.\nExperiment 7. Next we look at memorization on CIFAR-10 which is a collection of 32 pixel by 32 pixel color images belonging to 10 classes. As with Binary-MNIST, we quantize each color channel to 1 bit and try to separate the classes 0 through 4 from classes 5 through 9. This gives us the Binary-CIFAR-10 task where we have to learn a function f : B3\u00d732\u00d732 \u2192 B from 50,000 images. Incidentally, the quantization of each color channel to 1-bit significantly degrades the signal making it a difficult task for humans.\nFor this task, we construct a network with 5 hidden layers each with 1024 luts and one output layer with 1 output. We set k = 10 for the luts. This network is able to achieve a training accuracy of 0.79 and a test accuracy of 0.63. Although not as impressive in absolute terms as the memoriza-\ntion result on Binary-MNIST, it is still significantly above chance (0.50). Furthermore, as before, the result is very stable and does not depend on a specific random topology chosen when the network is constructed.\nWe compare memorization with several standard methods in Table 4. By comparing Table 4 with Table 3 it is clear that Binary-CIFAR-10 is a harder task than Binary-MNIST since all the methods perform significantly worse on it. The best test accuracy of 0.71 is again from a LENET-style network similar to the one used in Experiment 4, but with 40 epochs of training. We believe a ResNet-style architecture (He et al., 2016) may potentially do better here but since our goal is not to achieve state-of-the-art but see how memorization does, we leave this to future work. For the same reason we don\u2019t explore data augmentation here which is a standard technique for CIFAR-10.\nOnce again, memorization compares favorably on test accuracy with the other methods, and compared to Binary-MNIST it does relatively better here since it ties with the nearest neighbor searches.\nExperiment 8. In this experiment, we consider the Pairwise-CIFAR-10 tasks which are defined analogously to Pairwise-MNIST. We use the same network architecture as in Experiment 7 instead of optimizing specifically for these tasks. Training accuracies are generally 0.95 and above whereas the test accuracies range from 0.61 (CAT v/s DOG) to 0.85 (FROG v/s SHIP) with an average test accuracy of 0.76 which is significantly above chance.\nExperiment 9. To get qualitative insight into the decision boundaries learned with different levels of memorization, we classify points in the region [\u22122, 2] \u00d7 [\u22122, 2] \u2208 R2 as being inside or outside the circle x2 + y2 \u2264 1.62. Our dataset consists of points on a 100 \u00d7 100 grid in this region which has been partitioned into equal test and training sets (Figure 4, leftmost column). To make this a hard problem we encode each point as pair of 10-bit fixed-point numbers. We learn this function f : B20 \u2192 B using networks with 32 layers each with 2048 luts and vary k. With k = 10 (rightmost column), the training set is memorized perfectly but (as seen on test) the concept is not learned. However, memorizing with k = 2, we learn a simpler concept that is not faithful around the \u201ccorners\u201d (as can be seen by zooming in) but one that generalizes almost perfectly to test. Finally, k = 6 provides a satisfactory compromise between the two extremes. Thus, once again, we see that memorization if done carefully can lead to good generalization."}, {"heading": "5. Comparison with Other Methods", "text": "It is instructive to compare our memorization procedure with a few commonly used procedures for learning.\nk-Nearest Neighbors. The key difference, as noted in the introduction, is that k-NNs require a user-specified distance function which is often syntactic notion of distance such that induced by treating an image as a vector in Rd. These syntactic notions of distance do not work well on more challenging tasks and one may view such a learning problem as essentially that of discovering a semantically meaningful distance function. We see this in our experiments: the\ntr ai\nni ng\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\nte st\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\n\u22122.0 \u22121.5 \u22121.0 \u22120.5 0.0 0.5 1.0 1.5 2.0\n\u22122.0\n\u22121.5\n\u22121.0\n\u22120.5\n0.0\n0.5\n1.0\n1.5\n2.0\nground truth k = 2 k = 6 k = 10\nFigure 4. The decision boundaries learned in Experiment 9.\ndistance function helps more with Binary-MNIST (Experiment 4) than it does with Binary-CIFAR-10 (Experiment 7). Furthermore, in a separate experiment we found that augmenting the table lookup with 1-NN search at test time did not significantly improve test accuracy for Binary-CIFAR-10 where memorization was already tied with k-NNs.\nAdditionally, k-NN requires storing the entire training set and is typically computationally more expensive at test time. For example, on Binary-MNIST the standard k-NN implementation in scikit-learn (Pedregosa et al., 2011) took more than an hour to evaluate performance on the training and test sets (as opposed to seconds with memorization). There has been work on speeding up nearest neighbor search by using locality sensitive hashing (Indyk & Motwani, 1998) and, more recently, with random projections (Li & Malik, 2016). In that context, one may view each lookup table as implementing a trivial locality sensitive hash function where the distance metric arises from exact equality, and the network as an ensemble through cascading of such nearest neighbors classifiers.\nNeural Networks. The initial motivation for this work was to understand neural networks better; particularly to explore with a model the idea that perhaps SGD is a sophisticated way to memorize training data in a manner that generalizes and that perhaps there are simpler ways to memorize data\nas well that may yet generalize. However, a key difference is that gradient descent-based training can learn useful intermediate representations or targets for hidden layers. In this work we have side stepped that question, by simply setting the intermediate target to be the final output. It is an interesting line of research to see if we can find a way to learn useful intermediate signals in this setting perhaps by purely combinatorial methods. Practically, that would give us a method to learn purely binary neural networks without using floating point at all, which is useful in resource constrained environments.\nRandom Forests. Trees in a random forest are constructed over a subset of the data by iteratively evaluating different input variables to optimize purity after splitting on the variable (Breiman, 2001). In contrast, memorization uses the whole dataset and does not solve any optimization problem (which makes it more computationally efficient). Furthermore, random forests combine the tree predictions using voting whereas memorization uses cascading.\nCascading and Stacked Generalization. A recent extension of random forests are Deep Forests (Zhi-Hua Zhou, 2017) where multiple random forests are constructed at each level and then cascaded using the idea of stacked generalization (Wolpert, 1992) which is a generalization of cross-validation. In contrast, layers of luts are far simpler, and memorization propagates outputs based on what has been memorized over the entire training data. Due to the manner in which we construct the lookup tables and the corresponding functions (using the counts of the patterns) it is not clear to us that stacked generalization will help.\nSpectral Methods. There is a rich literature on the theory of learning boolean functions (f : Bk \u2192 B in our notation) (Mansour, 1994) which looks at theoretical learning guarantees under assumptions on the input distribution (typically uniform) and on the spectrum of the function (e.g. f can be approximated by a sparse and low degree polynomial in the boolean fourier basis). Recently, Hazan et al. (2017) have used these techniques in hyperparameter optimization where they find them to be practically useful (the distributional assumption is not fatal for this application). This line of work does not deal with depth, but only linear combinations of the basis functions. However, there is similarity in having a low degree in the fourier basis and our notion of support-limited memorization. These are similar structural priors and our results and those of Hazan et al. may be viewed as evidence that real world functions satisfy these priors.\nLearning Boolean Circuits. There is relatively little prior work in directly learning boolean circuits (Oliveira & Sangiovanni-Vincentelli, 1994; Tapp, 2014). However, it is interesting to note that the memorization algorithm in Section 3 although developed independently and from different\nconsiderations is similar to the greedy algorithm described by Tapp.2 An important difference is that instead of learning a single tree, we learn a network which makes learning more stable (as seen in Experiment 1)."}, {"heading": "6. Conclusion", "text": "The experiments of Zhang et al. (2017) and Arpit et al. (2017) on training with random data lead naturally to the question that if neural networks can memorize random data and yet generalize on real data, are they perhaps doing something different in the two cases. This work started with the opposite thought: What if in both cases they are simply memorizing? This, in turn, leads to the question of whether it is even possible to generalize from pure memorization. Na\u0131\u0308ve memorization with a lookup table is too simplistic a model but, as we saw, a slightly more complex model in the form of a network of support-limited lookup tables does significantly better than chance and is closer to the standard algorithms on a number of binary classification problems from MNIST and CIFAR-10. (To investigate if this result holds on other datasets is an important area of future work.)\nFurthermore, this model replicates some of the key observations with neural networks: the performance of a network improves with depth; it memorizes random data and yet generalizes on real data; and memorizing random data is harder than real data. In particular, the last observation implies that we cannot rule out memorization based on differences in the hardness of learning between real and random data.\nFor future work, we would like to understand why memorization generalizes. Now, since the size of the hypothesis space is bounded by 2n2 k\n(where n is the number of k-luts in the network), we can use results from PAC-learning to bound the generalization gap, but these bounds are typically weak or vacuous.3 Rademacher complexity may be useful for small k (say 2), but for moderate k\u2014where the Rademacher complexity is high yet there is generalization\u2014 we would need a different approach, perhaps one based on stability (Bousquet & Elisseeff, 2002). In this connection, we expect the results in Devroye & Wagner (1979) to apply to a single lut, but extensions are needed to handle networks of luts, i.e., depth. Furthermore, these would have to incorporate details of the construction since not every network of luts generalizes (even for k = 2).\nFinally, given the computational efficiency of memorization, we would like to extend it to a practically useful algorithm for learning, but that would likely involve introducing some form of explicit optimization or search.\n2We thank David Krueger for noticing the connection. 3 For example, using Theorem 2.2 in Mohri et al. (2012) for the experiments in Table 2 (with \u03b4 = 0.01 for concreteness) bounds the gap to 0.34 for k = 2. The bound doubles as k increases by 2."}, {"heading": "Acknowledgments", "text": "I thank Ben Rossi, Vinod Valsalam, Rhys Ulerich, and Eric Allen for many useful discussions and Larry Rudolph and Steve Heller for their feedback on the paper."}], "year": 2018, "references": [{"title": "A closer look at memorization in deep networks", "authors": ["D. Arpit", "S.K. Jastrzebski", "N. Ballas", "D. Krueger", "E. Bengio", "M.S. Kanwal", "T. Maharaj", "A. Fischer", "A.C. Courville", "Y. Bengio", "S. Lacoste-Julien"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Stability and generalization", "authors": ["O. Bousquet", "A. Elisseeff"], "venue": "J. Mach. Learn. Res.,", "year": 2002}, {"title": "Distribution-free performance bounds for potential function rules", "authors": ["L. Devroye", "T. Wagner"], "venue": "IEEE Transactions on Information Theory,", "year": 1979}, {"title": "Learning both weights and connections for efficient neural networks", "authors": ["S. Han", "J. Pool", "J. Tran", "W.J. Dally"], "venue": "In Proceedings of the 28th International Conference on Neural Information Processing Systems - Volume 1,", "year": 2015}, {"title": "Hyperparameter optimization: A spectral approach", "authors": ["E. Hazan", "A.R. Klivans", "Y. Yuan"], "venue": "CoRR, abs/1706.00764,", "year": 2017}, {"title": "Deep residual learning for image recognition", "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "IEEE Conference on Computer Vision and Pattern Recognition,", "year": 2016}, {"title": "Approximate nearest neighbors: Towards removing the curse of dimensionality", "authors": ["P. Indyk", "R. Motwani"], "venue": "In Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing,", "year": 1998}, {"title": "Generalization in Deep Learning", "authors": ["K. Kawaguchi", "L. Pack Kaelbling", "Y. Bengio"], "venue": "ArXiv e-prints,", "year": 2017}, {"title": "Fast k-nearest neighbour search via dynamic continuous indexing", "authors": ["K. Li", "J. Malik"], "venue": "In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48,", "year": 2016}, {"title": "Learning Boolean Functions via the Fourier Transform, pp. 391\u2013424", "authors": ["Y. Mansour"], "venue": "URL https://doi.org/10", "year": 1994}, {"title": "Foundations of Machine Learning", "authors": ["M. Mohri", "A. Rostamizadeh", "A. Talwalkar"], "venue": "ISBN 026201825X,", "year": 2012}, {"title": "Learning complex boolean functions: Algorithms and applications", "authors": ["A.L. Oliveira", "A. Sangiovanni-Vincentelli"], "venue": "Advances in Neural Information Processing Systems", "year": 1994}, {"title": "Xnor-net: Imagenet classification using binary convolutional neural networks", "authors": ["M. Rastegari", "V. Ordonez", "J. Redmon", "A. Farhadi"], "venue": "Computer Vision \u2013 ECCV", "year": 2016}, {"title": "A new approach in machine learning", "authors": ["A. Tapp"], "venue": "ArXiv e-prints,", "year": 2014}, {"title": "Stacked generalization", "authors": ["D.H. Wolpert"], "venue": "Neural Networks,", "year": 1992}, {"title": "Understanding deep learning requires rethinking generalization", "authors": ["C. Zhang", "S. Bengio", "M. Hardt", "B. Recht", "O. Vinyals"], "venue": "In Proceedings of the International Conference on Learning Representations ICLR,", "year": 2017}, {"title": "Deep forest: Towards an alternative to deep neural networks", "authors": ["J.F. Zhi-Hua Zhou"], "venue": "In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence,", "year": 2017}], "id": "SP:51ed7ecddc98569caa2eb8e05da5a3ee11c996e4", "authors": [{"name": "Satrajit Chatterjee", "affiliations": []}], "abstractText": "In the machine learning research community, it is generally believed that there is a tension between memorization and generalization. In this work, we examine to what extent this tension exists, by exploring if it is possible to generalize by memorizing alone. Although direct memorization with a lookup table obviously does not generalize, we find that introducing depth in the form of a network of support-limited lookup tables leads to generalization that is significantly above chance and closer to those obtained by standard learning algorithms on several tasks derived from MNIST and CIFAR-10. Furthermore, we demonstrate through a series of empirical results that our approach allows for a smooth tradeoff between memorization and generalization and exhibits some of the most salient characteristics of neural networks: depth improves performance; random data can be memorized and yet there is generalization on real data; and memorizing random data is harder in a certain sense than memorizing real data. The extreme simplicity of the algorithm and potential connections with generalization theory point to several interesting directions for future research.", "title": "Learning and Memorization"}