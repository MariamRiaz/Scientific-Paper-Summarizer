34	9	A common feature of accelerated algorithms is that they require good estimate of the strong convexity parameter.
36	9	In this paper, we show that primal-dual algorithms are capable of exploiting strong convexity from data if the algorithm parameters (such as step sizes) are set appropriately.
40	5	We follow the approach of Lan & Zhou (2015) to derive dual-free variants of the primal-dual algorithms customized for ERM problems with the linear predictor structure, and show that they can also exploit strong convexity from data with correct choices of parameters or using an adaptation scheme.
42	7	We first study batch primal-dual algorithms, by considering a “batch” version of the ERM problem (1), minx∈Rd { P (x) def = f(Ax) + g(x) } .
44	20	The functions f , g and matrix A satisfy: • f is δ-strongly convex and 1/γ-smooth where γ > 0 and δ ≥ 0, and γδ ≤ 1; • g is λ-strongly convex where λ ≥ 0; • λ+ δµ2 > 0, where µ = √ λmin(ATA).
48	38	However, they did not consider the case where additional or the sole source of strong convexity comes from f(Ax).
62	24	• If γδ = 1, i.e., f is a simple quadratic function, then θx ≈ 1− µ 2 L2 1 2µ/L+1 ≈ 1− µ2 L2 .
75	6	Therefore, if we can monitor the progress of Algorithm 4 BPD-Adapt (robust heuristic) input: previous rate estimate ρ > 0, ∆ = δµ̂2, period T , constants c < 1 and c > 1, and {P (s), D(s)}ts=t−T Compute new rate estimate ρ̂ = P (t)−D(t) P (t−T )−D(t−T ) if ρ̂ ≤ c ρ then ∆ := 2∆, ρ := ρ̂ else if ρ̂ ≥ c ρ then ∆ := ∆/2, ρ := ρ̂ else ∆ := ∆ end if σ = 1L √ λ+∆ γ , τ = 1 L √ γ λ+∆ Compute θ using (8) or set θ = 1 output: new parameters (σ, τ, θ) the convergence and compare it with the predicted convergence rate in Theorem 1, then we can adjust the estimated parameters to exploit strong convexity from data.
81	12	Algorithm 4 is a more robust heuristic.
85	6	The capability of accessing both the primal and dual objective values allows primal-dual algorithms to have good estimate of the convergence rate, which enables effective tuning heuristics.
89	5	In particular, we extend the stochastic primal-dual coordinate (SPDC) algorithm by Zhang & Xiao (2015).
116	16	Lan & Zhou (2015) developed “dual-free” variants of primal-dual algorithms that avoid computing the dual proximal mapping.
118	12	We show how to apply this approach to solve the structured ERM problems considered in this paper.
119	32	They can also exploit strong convexity from data if the algorithmic parameters are set appropriately or adapted automatically.
125	30	Let the kernel h ≡ f∗ in the Bregman divergence D. If we construct a sequence of vectors {v(t)} such that v(0) = (f∗)′(y(0)) and for all t ≥ 0, v(t+1) = v (t)+σAx̃(t) 1+σ , (12) then the solution to problem (11) is y(t+1) = f ′(v(t+1)).
126	18	The solution to (11) can be written as y(t+1)= argmin y { f∗(y)−yTAx̃(t)+ 1σ ( f∗(y)−v(t)T y )} = argmin y {( 1 + 1σ ) f∗(y)− ( Ax̃(t) + 1σv (t) )T y } = argmax y {( v(t)+σAx̃(t) 1+σ )T y − f∗(y) } = argmax y { v(t+1) T y − f∗(y) } = f ′(v(t+1)), where in the last equality we used the property of conjugate function when f is strongly convex and smooth.
130	11	Consequently, we can update y(t) in the BPD algorithm using the gradient f ′(v(t)), without the need of dual proximal mapping.
144	8	If we set T =∞ in Algorithm 7 (non adaption) and let σ = 14R √ γ(nλ+ δµ2), τ = 14R √ γ nλ+δµ2 , (15) and θ = max{θx, θy} where θx = ( 1− τσδµ2n(4+2σ) ) 1 1+τλ , θy = 1+((n−1)/n)σ/2 1+σ/2 , (16) then we have ( 1 2τ + λ 2 ) E [ ‖x(t) − x⋆‖2 ] + γ4E [ D(y⋆, y(t)) ] ≤ θtC, E [ L(x(t), y⋆)− L(x⋆, y(t)) ] ≤ θtC, where C = ( 1 2τ + λ 2 ) ‖x(0) − x⋆‖2 + ( 1 σ+ 1 2 ) D(y⋆, y(0)).
145	13	Now we discuss the results of Theorem 4 in further details.
150	7	This is not an accelerated rate, and we have the same iteration complexity as SVRG.
156	14	However, each φi is locally strongly convex in bounded domain (Bach, 2014): if z ∈ [−B,B], then we know δ = minz φi′′(z) ≥ exp(−B)/4.
160	9	The data matrix A has sizes n = 5000 and d = 3000, and its entries are sampled from multivariate normal distribution with mean zero and covariance matrix Σij = 2|i−j|/2.
164	10	As expected, the performance of Primal-AG is very similar to that of BPD, and Opt-BPD has the fastest convergence.
169	11	For Ada-SPDC and ADF-SPDC, we use the robust adaptation scheme with T = 10, c = 0.95 and c = 1.5.
170	10	We first compare these randomized algorithms for ridge regression over the cpuact data from the LibSVM website (https://www.csie.ntu.edu.tw/˜cjlin/libsvm/).
174	19	The adaptive accelerated method, ADF-SPDC, has the fastest convergence.
175	21	This indicates that our theoretical results, which predict no acceleration in this case, may be further improved.
178	84	The dualfree SPDC algorithms only use gradients of the logistic function.
179	118	The results are presented in Figure 3.
180	346	For both datasets, the strong convexity from data is very weak, and the accelerated algorithms performs better.
