0	27	In recent years, Neural Machine Translation (NMT) (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2014) has achieved remarkable performance on many translation tasks (Jean et al., 2015; Sennrich et al., 2016; Wu et al., 2016; Sennrich et al., 2017).
8	20	Similar to joint training (Cheng et al., 2016; Zhang et al., 2018), dual learning (He et al., 2016) designs a reinforcement learning framework to better capitalize on monolingual data and jointly train two models.
9	63	Instead of leveraging monolingual data (X or Z) to enrich the low-resource bilingual pair (X,Z), in this paper, we are motivated to introduce another rich language Y , by which additionally acquired bilingual data (Y,Z) and (X,Y ) can be exploited to improve the translation performance of (X,Z).
10	21	This requirement is easy to satisfy, especially when Z is a rare language but X is not.
11	26	Under this scenario, (X,Y ) can be a rich-resource pair and provide much bilingual data, while (Y,Z) would also be a low-resource pair mostly because Z is rare.
28	31	As shown in Figure 1, our method tries to leverage (X,Y ) (a rich-resource pair) and (Y, Z) to improve the translation performance of low-resource pair (X,Z), during which translation models of (X,Z) and (Y, Z) can be improved jointly.
37	23	Next, we will extend our method to two directions and give our unified bidirectional EM training in subsection 2.2.
42	23	Note that conditioned on the observed data and current model, the calculation of Q(z) is intractable, so we choose Q(z) = p(z|x) approximately.
45	22	According to Bishop (2006), we can write this gap explicitly as follows: L(Θ;D)− L(Q) = ∑ z Q(z) log Q(z) p(z|y) = KL(Q(z)||p(z|y)) = KL(p(z|x)||p(z|y)) (3) where KL(·) is the KullbackLeibler divergence, and the approximation that p(z|x, y) ≈ p(z|y) is also used above.
46	20	In the E-step, we minimize the gap between L(Q) and L(Θ;D) as follows: Θz|x = arg min Θz|x KL(p(z|x)||p(z|y)) (4) To sum it up, the E-step optimizes the model p(z|x) by minimizing the gap between L(Q) and L(Θ;D) to get a better lower bound L(Q).
48	22	Given the new model p(y|z), the E-step tries to optimize p(z|x) again to find a new lower bound, with which the M-step is re-performed.
52	39	To solve this problem, we can jointly optimize p(x|z) and p(z|y) similarly by maximizing the reverse translation probability p(x|y).
55	67	arg max Θy|z ∑ (x,y)∈D Ez∼p(z|x) log p(y|z) (6) • Direction of Y ⇒ X E: Optimize Θz|y.
75	58	Arabic (AR) and Spanish (ES) are used as two simulated rare languages Z.
76	45	We randomly choose subsets of bilingual data of (X,Z) and (Y, Z) in the original dataset to simulate low-resource situations, and make sure there is no overlap in Z between chosen data of (X,Z) and (Y,Z).
77	26	IWSLT20121: English-French is used as the rich-resource pair (X,Y ), and two rare languages Z are Hebrew (HE) and Romanian (RO) in our choice.
81	26	The monolingual data of Z (HE and RO) are taken from the web2.
84	60	The size of training data of all language pairs are shown in Table 1.
85	31	We compare our method with four baseline systems.
86	64	The first baseline is the RNNSearch model (Bahdanau et al., 2014), which is a sequence-tosequence model with attention mechanism trained with given small-scale bilingual data.
87	132	The trained translation models are also used as pre-trained models for our subsequent training processes.
88	133	The second baseline is PBSMT (Koehn et al., 2003), which is a phrase-based statistical machine translation system.
89	69	PBSMT is known to perform well on low-resource language pairs, so we want to compare it with our proposed method.
94	80	We treat this method as a second baseline because it can also be regarded as a method exploiting (Y, Z) and (X,Y ) to improve 3together with WMT2014 4together with Europarlv7  the translation of (X,Z) if we regard (X,Z) as the zero-resource pair and p(x|y) as the teacher model when training p(z|x) and p(x|z).
95	53	The fourth baseline is back-translation (Sennrich et al., 2015).
96	24	We will denote it as BackTrans.
97	72	More concretely, to train the model p(z|x), we use extra monolingual Z described in Table 1 to do back-translation; to train the model p(x|z), we use monolingual X taken from (X,Y ).
98	20	Procedures for training p(z|y) and p(y|z) are similar.
99	26	This method use extra monolingual data of Z compared with our TA-NMT method.
101	35	Experimental results on both datasets are shown in Table 3 and 4 respectively, in which RNNSearch, PBSMT, T-S and BackTrans are four baselines.
