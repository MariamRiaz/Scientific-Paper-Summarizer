0	15	Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation (Lample et al., 2017), multilingual word embeddings (Klementiev et al., 2012), and knowledge transfer to low resource languages (Guo et al., 2016).
6	43	Obtaining aligned corpora or bilingual seed dictionaries is nevertheless not straightforward for all language pairs.
7	56	This has motivated a wave of very recent research into unsupervised word translation: inducing bilingual dictionaries given only monolingual word embeddings (Conneau et al., 2018; Zhang et al., 2017b,a; Artetxe et al., 2017).
8	17	The most successful have leveraged ideas from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).
9	42	In this approach the generator provides the cross-modal mapping, taking embeddings of dictionary words in one language and ‘generating’ their translation in another.
10	15	The discriminator tries to distinguish between this ‘fake’ set of translations and the true dictionary of embeddings in the target language.
11	19	The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal mapping should be capable of inducing a complete dictionary, as per Mikolov et al. (2013).
12	26	Despite these successes, such adversarial methods have a number of well-known drawbacks (Arjovsky et al., 2017): Due to the nature of their min-max game, adversarial training is very unstable, and they are prone to divergence.
14	7	Convergence is also hard to diagnose and does not correspond well to efficacy of the generator in downstream tasks (Hoshen and Wolf, 2018).
16	28	Specifically, we propose to search for the cross-lingual word pairing that maximizes statistical dependency in terms of squared loss mutual information (SMI) (Yamada et al., 2015; Suzuki and Sugiyama, 2010).
17	32	Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) (Quadrianto et al., 2009) we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimising cross-modal pairing with representation learning within each view.
18	21	In contrast to prior work that uses a fixed representation, by non-linearly projecting monolingual world vectors before matching, we learn a new embedding where statistical dependency is easier to establish.
22	12	, n}, and Π the corresponding permutation indicator matrix: Π ∈ {0, 1}n×n,Π1n = 1n, and Π>1n = 1n.
24	18	We aim to optimize for both the permutation Π (bilingual dictionary), and non-linear transformations gx(·) and gy(·) of the respective wordvectors, that maximize statistical dependency between the views.
26	8	Our overall loss function is: min Θx,Θy ,Π Ω(D; Θx,Θy)︸ ︷︷ ︸ Regularizer −λDΠ(D; Θx,Θy)︸ ︷︷ ︸ Dependency , DΠ(D; Θx,Θy) = DΠ({gx(xi), gy(yπ(i))}ni=1), Ω(D; Θx,Θy) = n∑ i=1 ‖xi − fx(gx(xi))‖22 + ‖yi − fy(gy(yi))‖22 +R(Θx) +R(Θy).
30	6	The SMI is an f - divergence (Ali and Silvey, 1966).
34	17	In our application this corresponds to a measure of dependency between two aligned sets of monolingual wordvectors.
35	6	To exploit SMI for matching, we introduce a permutation variable Π by replacing L→ Π>LΠ in the estimator: ŜMI({(xi,yπ(i))}n1 )= 1 2n tr ( diag (α̂Π)KΠ >LΠ ) − 1 2 , that will enable optimizing Π to maximize SMI.
37	7	Then we employ an alternative optimization on Eq.
41	7	Optimization for Π To find the permutation (word matching) Π that maximizes SMI given fixed encoding parameters Θx,Θy, we only need to optimize the dependency term DΠ in Eq.
42	12	We employ the LSOM algorithm (Yamada et al., 2015).
46	26	The optimization problem is a linear assignment problem (LAP).
48	55	To get discrete Π, we solve the last step by setting η = 1.
49	11	Intuitively, this can be seen as searching for the permutation Π for which the data in the two (initially unsorted views) have a matching withinview affinity (gram) matrix, where matching is defined by maximum SMI.
50	34	In this section, we evaluate the efficacy of our proposed method against various state of the art methods for word translation.
52	15	We use polynomial kernel to compute Algorithm 1 SMI-based unsupervised word translation Input: Unpaired word embeddings D = ({xi}ni=1, {yj}nj=1).
57	6	the gram matrices K and L. For all pairs of languages, we fix the number of training epochs to 20.
63	7	Datasets We performed experiments on the publicly available English-Italian, EnglishSpanish and English-Chinese datasets released by (Dinu and Baroni, 2015; Zhang et al., 2017b; Vulic and Moens, 2013).
64	14	We name this collective set of benchmarks BLI.
65	19	We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018)1.
