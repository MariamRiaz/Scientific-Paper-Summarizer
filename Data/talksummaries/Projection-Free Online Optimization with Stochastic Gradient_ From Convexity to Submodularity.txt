20	22	This includes • Meta-Frank-Wolfe, the first projection-free algorithm for adversarial online optimization which requires only stochastic gradient estimates.
51	11	To begin defining continuous submodular functions, we first recall the definition of a submodular set function.
52	12	A real-valued set function f : 2Ω → R+ is submodular if f(A) + f(B) ≥ f(A ∪B) + f(A ∩B) for all A,B ⊂ Ω.
57	53	For efficient maximization, we also require that these functions satisfy a diminishing returns condition (Bian et al., 2017).
58	44	We say that f is continuous DR-submodular if f is differentiable and ∇f(x) ≥ ∇f(y) for all x ≤ y.
59	156	The main attraction of continuous DRsubmodular functions is that they are concave in positive directions; that is, for all x ≤ y, f(y) ≤ f(x) + 〈∇f(x),y − x〉 (Calinescu et al., 2011; Bian et al., 2017).
60	31	We now provide a brief introduction to online optimization, referring the interested reader to the excellent survey of (Hazan et al., 2016).
62	30	In each round, a player must first choose a point xt from the constraint set K. After playing xt, the value of ft(xt) is revealed to the player, along with access to the gradient∇f .
63	10	Although the player does not know the function ft while choosing xt, they may use information of previously seen functions to guide their choice.
66	24	In the adversarial setting, the goal of the player is to minimize adversarial regret, which is defined as RT , T∑ t =1 ft(xt)− inf x ∈K T∑ t =1 ft(x) for minimization problems and analogously defined for maximization problems.
67	35	Intuitively, a player’s regret is low if the accumulated value of their actions over the T rounds is close to that of the single best action in hindsight.
79	10	Nearly all optimization methods for both offline and online settings use first order information of the objective function; however, exact gradient computations can be costly, especially when the objective function is only readily expressed as a large sum of individual functions or is itself an expectation over an unknown distribution.
81	33	In this work, we assume that once a function ft is revealed, the player gains oracle access to unbiased stochastic estimates of the gradient, rather than the exact gradient.
84	9	In this work, we make a few main assumptions that allow our algorithms to be analyzed.
85	24	The constraint setK is convex and compact, with diameter D = supx,y∈K‖x − y‖ and radius R = supx∈K‖x‖.
90	10	Unlike previous work, these methods are projection-free and require only stochastic estimates of the gradients, rather than exact gradient computations.
93	16	Algorithm 1 combines the recent variance reduction technique of (Mokhtari et al., 2018a) along with the use of online linear optimization oracles to minimize the regret in each round.
94	56	An online linear optimization oracle is an instance of an online linear optimization (minimization/maximization in the convex/DR-submodular setting, respectively) algorithm that optimizes linear objectives in a sequential manner.
95	173	Both the variance reduction in the stochastic gradient estimates and the online linear oracles are crucial in the algorithm, as just one technique is not enough to get sublinear regret bounds in the adversarial setting.
96	90	At a high level, our algorithm produces iterates xt by running K steps of a Frank-Wolfe procedure, using an average of previous gradient estimates and linear online optimization oracles in place of exact optimization of the true gradient.
97	238	After a point xt is played in round t, our algorithm queries the gradient oracle ∇̃ft at K points.
98	71	Then, the gradient estimates are averaged with those from previous rounds and fed as objective functions into K linear online optimization oracles.
102	13	First, the online oracles should be minimizing in the case of convex optimization and maximizing in the case of submodular optimization.
104	25	Finally, the update rule is x (k+1) t ← (1− ηk)x (k) t + ηkv (k) t for convex problems and x (k+1) t ← x (k) t + ηkv (k) t for submodular problems.
114	12	For monotone continuous DR-submodular functions f1, .
115	11	, fT and step sizes ηk = 1K , the adversarial (1− 1/e)-regret of Algorithm 1 is at most in expectation, where Q , max{max1≤t≤T ‖∇ft(x1)‖242/3, 4σ2 + 6L2R2}.
118	13	In the stochastic online setting, where functions are sampled i.i.d.
119	27	ft ∼ D, we can develop much simpler algorithms that still achieve sublinear regret.
120	27	Algorithm 2 works without instantiating any online linear optimization oracles and requires only a single stochastic estimate of the gradient at each round.
121	13	Indeed, because the functions are not arbitrarily chosen, variance reduction along with one Frank-Wolfe step suffices to achieve a sublinear regret bound.
