33	30	Ni+(·α·) is the number of word-pairs with frequency at least i which surround α. N ′i+(α·) is the number of words coming after α to form a pattern αw for which the number of unique left contexts is at least i; it is specific to MKN and not needed in KN.
36	27	In this paper, we make use of advanced data structures to efficiently obtain the required quantities to answer probabilistic queries as they arrive.
40	27	The CST emulates the functionality of the Suffix Tree (ST) (Weiner, 1973) using substantially less space.
41	73	The suffix tree is a classical search index consisting of a rooted labelled search tree constructed from a text T of length n drawn from an alphabet of size σ.
42	29	Each root to leaf path in the suffix tree corresponds to a suffix of T .
43	53	The leaves, considered in left-toright order define the suffix array (SA) (Manber and Myers, 1993) such that the suffix T [SA[i], n − 1] is lexicographically smaller than T [SA[i+1], n−1] for i ∈ [0, n− 2].
44	51	Searching for a pattern α of length m in T can be achieved by finding the “highest” node v in the ST such that the path from the root to v is prefixed by α.
45	32	All leaf nodes in the subtree starting at v correspond to the locations of α in T .
48	23	A CST reduces the space requirements of ST by utilizing the compressibility of the BurrowsWheeler transform (BWT) (Burrows and Wheeler, 1994).
50	23	The transform is defined as BWT[i] = T[SA[i]− 1 mod n] (1) and is the core component of the FM-Index (Ferragina and Manzini, 2000) which is a subcomponent of a CST to provide efficient search for locating arbitrary length patterns (m-grams), determining occurrence frequencies etc.
51	74	The key functionality provided by the FM-Index is the ability to efficiently determine the range SA[lb, rb] matching a given pattern α described above without the need to store the ST or SA explicitly.
58	24	The structure of the wavelet tree is derived by recursively decomposing the alphabet into subsets.
61	33	Thus the space usage is proportional to the order k entropy (Hk(T)) of the text.
68	40	The number of unique right contexts of b can be determined using degree(v) (again O(1) but requires bit operations on the succinct tree representation of the ST).
99	36	Language modelling toolkits such as KenLM and SRILM precompute real valued probabilities and backoff-weights at training time, such that querying becomes largely a problem of retrieval.
115	61	The integer vectors are streamed to disk and then compressed (lines 15-17) in order to limit memory usage.
116	23	The final steps in lines 15 and 16 compress the integer and bit-vectors.
121	50	The compressed vectors are loaded into memory and when an expensive count is required for node v, the precomputed quantities can be fetched in constant time via LOOKUP(v, bv, i(x)) = i(x)RANK(bv,id(v),1).
123	29	This corresponds to v’s index in the compressed integer vectors i(x), from which its precomputed count can be fetched.
124	29	This strategy only applies for precomputed nodes; for other nodes, the values are computed on-the-fly.
135	39	tracks match for wii−k 4: p← 1/|σ| 5: for k ← 1 to m do 6: if vk−1 does not match then 7: break out of loop 8: vfullk ← back-search([lb(vfullk−1), rb(vfullk−1)], wi−k+1) 9: Dk(1),Dk(2),Dk(3+)← discounts for k-grams 10: if k = m then 11: c← size(vfullk ) 12: d← size(vk−1) 13: N1,2,3+← N123PFRONT(t, vk−1, wi−1i−k+1, 0) 14: else 15: c← N1PBACK1(t, vfullk , wi−1i−k+1) 16: d← N1PFRONTBACK1(t, vk−1, wi−1i−k+1) 17: N1,2,3+← N123PFRONT(t, vk−1, wi−1i−k+1, 1) 18: if 1 ≤ c ≤ 2 then 19: c← c− Dk(c) 20: else 21: c← c− Dk(3+) 22: γ← Dk(1)N1 + Dk(2)N2 + Dk(3+)N3+ 23: p← 1 d (c+ γp) 24: return ( p, [ vfullk ]m−1 k=0 ) grams, vfull, as the nodes matching the context in the subsequent window, denoted v. For example, in the sentence “The Force is strong with this one.”, computing the 4-gram probability of “The Force is strong” requires matches into the CST for “strong”, “is strong”, etc.
140	30	This best suits the use of backward-search in a CST, which proceeds from right-to-left over the search pattern.
170	51	In our characterlevel experiments, we used the training and test data of the benchmark 1-billion-words corpus (Chelba et al., 2013).
171	23	Small data: German Europarl First, we compare the time and memory consumption of both the SRILM and KenLM toolkits, and the CST on the small German corpus.
174	67	The construction cost is modest, requiring less memory than the benchmark systems for m ≥ 3, and running in a similar time13 (despite our method supporting queries 13 For all timings reported in the paper we manually flushed the system cache between each operation (both for construction of unlimited size).
175	82	Precomputation adds to the construction time, which rose from 173 to 299 seconds, but yielded speed improvements of several orders of magnitude for querying (218k to 98 seconds for 10- gram).
179	58	Big Data: Common Crawl Table 2 reports the perplexity results for training on 32GiB subsets of the English, Spanish, French, and German Common Crawl corpus.
186	39	(We also compare the memory optimised lazy method in Figure 7.)
199	39	In contrast, the runtime of our CST approach is much less affected by data size or model order.
202	51	Running our model with m =∞ on the largest data size did not change the memory usage and only had a minor effect on runtime, taking 645s.
