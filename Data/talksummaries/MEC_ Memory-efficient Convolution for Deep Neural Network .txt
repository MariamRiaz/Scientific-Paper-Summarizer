25	12	We specifically interpret a tensor as a matrix when it requires matrix operations, otherwise (i.e., for data movement) we keep the tensor form.
33	38	The most relevant to our work is im2col-based convolution, FFT (Fast Fourier Transform)-based convolution (Highlander & Rodriguez, 2016; Mathieu et al., 2013; Vasilache et al., 2014), and Winograd-based convolution (Lavin, 2015).
34	14	MEC provides the same functionality with reduced memory requirements.
38	17	Thus, memory-overhead becomes really high when kernels are relatively smaller (e.g., 3x3) than input matrices (Chetlur et al., 2014; He et al., 2015; Perkins, 2016; Simonyan & Zisserman, 2014).
44	15	• MEC can allow larger mini-batch sizes to speedup turn-around/per-epoch-latency during training.
45	17	• MEC can accelerate computation by improving memory sub-system efficiency (e.g. more cache hits).
51	53	In direct convolution, one element of the output matrix O is produced by a dot-product between the kernel K and a sub-matrix of the input I .
58	9	im2col) and gemm in BLAS (Chellapilla et al., 2006; Chetlur et al., 2014; Jia, 2014) by off-loading the geometry-specific specializations in convolution to a plain matrix, which is depicted in Fig.
59	14	Specifically, each sub-matrix instance w.r.t.
61	15	For example, the gray and dotted sub-matrices in (a) are transformed into the gray and dotted rows in (b), respectively.
74	7	2) of size ih × kw (which is 7× 3) into one row of L. For example,A is the first partition of I ,A = I[0 : 7, 0 : 3].
79	19	Once the lowered matrix L is formed, MEC multiplies L by K in a way significantly different from im2col-based algorithms.
83	18	2 are annotated with the corresponding source partitions.
94	195	Each execution of the body is done by one gemm call, and those matrix multiplications can be parallelized.
96	122	Due to the compact lowering in MEC, it is computationally advantageous to use I in in × ih × iw × ic (or n-h-w-c) as in Table 2, because it ensures vertical redundant pixels to be eliminated and re- covered in a contiguous memory space.
97	31	Algorithm 2 O = MEC(I,K, s) 1: Allocate O with inohowkc elements 2: Allocate L with inowihkwic elements 3: Interpret L as in × ow × ih × kw × ic tensor 4: for n ∈ 0 : in, w ∈ 0 : ow, h ∈ 0 : ih in parallel do 5: L[n, w, h, 0 : kw, 0 : ic] = I[n, h, sww : sww+kw, 0 : ic] 6: end for 7: Interpret K as khkwic × kc matrix 8: if ow ≤ T and |O| ≤ |L| then 9: Interpret L as inow × ihkwic matrix 10: Interpret O as oh × inowkc matrix 11: for h ∈ 0 : oh in parallel do 12: O[h, 0 : inowkc] = L[0 : inow, shkwich : shkwich+khkwic]×K 13: end for 14: Copy L = O 15: Interpret L as oh × in × owkc tensor 16: Interpret O as in × oh × owkc tensor 17: for n ∈ 0 : in, h ∈ 0 : oh in parallel do 18: O[n, h, 0 : owkc] = L[h, n, 0 : owkc] 19: end for 20: else 21: Interpret L as in matrices of ow × ihkwic 22: Interpret O as in matrices of oh × owkc 23: for n ∈ 0 : in, h ∈ 0 : oh in parallel do 24: O[n][h, 0 : owkc] = L[n][0 : ow, shkwich : shkwich+khkwic]×K 25: end for 26: end if 27: Return O as in × oh × owkc tensor Based on I as in × ih × iw × ic, Algorithm 2 still has the same key idea in presence of channels and mini-batches.
98	60	The lowering step lines 4-6 in Algorithm 1 is similar to lines 4-6 in Algorithm 2.
100	11	A direct extension of Algorithm 1 would interpret O as oh × inowkc matrix, and perform oh multiplications for convolution of the whole mini-batch.
108	22	This will directly generate O in n-h-w-c.
110	13	In practice, however, the size of sub-matrices can impact performance, particularly on implementation-sensitive platform like GPU.
111	13	Therefore, MEC tries to find a good trade-off between Solution A and B with a tunable parameter T in line 8.
112	8	(In addition, Solution A is available only if L can be used as an auxiliary space, i.e. it is at least as large as O).
113	40	T is a platform-dependent parameter (e.g., on CPU vs. GPU, or on GPU-compute capability), and we found T around 100 to be a good threshold for latest GPUs.
114	124	In this section, we analyze the memory saving in MEC over im2col-based convolution.
115	87	The size of the lowered matrix, L in MEC is: inowihkwkc (3) In comparison with the lowered matrix of im2col (see Eq.
116	8	(2)), there is approximately a factor of kh.
117	11	For a more exact comparison, let us form their difference R. R = inkc(ohowkhkw − owihkw) = inkcowkw(ohkh − ih) = inkcowkw( ih − kh sh kh + kh − ih) = inkcowkw(ih − kh)( kh sh − 1) (4) Since ih > kh, MEC always reduces memory footprint as long as kh > sh (i.e., there is an overlap between kernel instances).
119	55	We implemented MEC for CPU/GPU in C++ with multithreaded OpenBLAS, OpenMP, and cuBLAS (cuBLAS) using single 32-bit precision.
121	33	We compared MEC with other open-source convolution packages in C++, in order to make fair point-by-point comparison and accurately capture the memory-overhead and performance.
122	23	We downloaded an open-source FFT-based convolution (cuFFT; Theano-FFT) for GPU.
