0	26	Hospitalized patients are vulnerable to a wide range of adverse events, including cardiopulmonary arrests (Kause et al., 2004; Hogan et al., 2012; Yoon et al., 2016), acute respiratory failures (Mokart, 2013), septic shocks (Henry et al., 2015), and post-operative complications (Clifton et al., 2012).
1	30	For a patient in a regular ward, the occurrence of any such event entails an unplanned transfer to an intensive care unit (ICU), the timing of which is a major determinant of the eventual outcome.
2	13	Indeed, recent medical studies have confirmed that delayed transfer to the ICU is strongly correlated with morbidity and mortality (Mardini L, 2012; Mokart, 2013).
3	22	The problem of delayed ICU transfer is enormous and acute: over 750,000 septic shocks and 200,000 cardiac arrests occur in the U.S. each year with mortality rates of 28.6% and 75% respectively (Merchant et al., 2011; Kumar et al., 2011).
4	15	Fortunately, experts believe that much of these events could be prevented with accurate prognosis and early warning (Nguyen et al., 2007).
22	35	Most of the previous works on risk prognosis for critical care patients viewed informative censoring as a “surrogate label” for a patient’s clinical deterioration, and hence used those labels to train a supervised (regression) model using the physiological data in a fixed-size time window before censoring.
30	15	However, recent systematic reviews have demonstrated the modest net clinical utility of all these scores (Cvach, 2012).
33	31	The subacute care data in an EHR typically comprises a set of episodes; each episode is a sequence of vital signs and lab tests (physiological data) that have been gathered (by clinicians) for a hospitalized patient at irregularly spaced time instances during her stay in a ward.
40	16	Clinical judgments manifest in the dth episode of D through informative sampling (encoded in the observation times {tdm} Md m=1), and informative censoring (encoded in the episode duration T dc and the endpoint outcome l d).
43	65	We can see from the trends in Figure 1 that as the deteriorating patient approaches the ICU admission time, the clinicians tend to sample her physiological data more intensely, whereas as the stable patient approaches the discharge time, the clinicians tend to have a more relaxed schedule for observing her vital signs and lab tests.
46	21	The take-away from Figure 1 is that the clinician’s judgment of the patient’s clinical state –manifesting in the vital signs and lab tests sampling rates– is very predictive of the endpoint outcomes.
47	30	This implies that there is a room for “learning from the informative clinical judgments”; that is, one can infer the patients’ latent states over time by using the clinicians’ observable sampling patterns as proximal noisy labels for those latent states.
48	17	Now we present a probabilistic model for the episodes {Ed}Dd=1 that captures the informative sampling and censoring effects discussed in Section 2.
49	16	We start by modeling the patient’s latent clinical state process in Subsection 3.1, before modeling the observable variables in Subsection 3.2.
50	16	We assume that each patient’s episode is governed by an underlying latent clinical state processX(t) that represents the evolution of her “clinical well-being” over time.
52	16	We model informative censoring by assuming that states 1 and N are absorbing states; state 1 is the state of clinically stability, at which the patient can be safely discharged home, whereas state N is the state of clinical deterioration, at which the patient needs to be admitted to the ICU.
55	16	The clinical state processX(t) is a semi-Markov jump process (Yu, 2010), i.e. X(t) = ∑K n=1 Xn · 1{τn≤t<τn+1} is a semi-Markov process for which every new state realization Xn starts at a jump time τn, where τ1 = 0, and lasts for a random sojourn time Sn = τn+1 − τn.
62	19	Every episode Ed in an EHR dataset D is associated with a latent clinical state trajectory {Xdn, Sdn}K d n=1, but we can only observe the absorbing state realization XKd = ld in the EHR data.
66	13	We model the observation process generating the physiological variables’ observation times {tm}m∈N+ as a doubly stochastic point process whose intensity, λ(t), is a stochastic process modulated by the latent clinical state process X(t).
84	20	The mark process defined above is one-dimensional, and hence we need to extend the definition to handle a multidimensional process that represents multiple lab tests and vital signs.
86	43	This is achieved by adopting a multi-task Gaussian Process as a model for the Qdimensional physiological process Y (t) ∈ RQ (Durichen et al., 2015).
96	46	In this Section, we develop an offline learning algorithm that learns the model parameter Ω using the offline training episodes in D, and a real-time risk scoring algorithm that computes a hospitalized patient’s risk over time.
97	142	The learning algorithm operates by first detecting change-points in the physiological data and the observation process; using the detected change-points, the algorithm segments each episode into a sequence of states, and uses an EM algorithm to estimate the model parameters.
98	17	The real-time risk scoring algorithm operates by inferring the patient’s current state via forward-filtering, and then computing the probability of eventual absorption in the deteriorating state.
99	13	0 5 10 15 20 25 0 2 4 6 8 10 Time t L a te n t S ta te s a n d In te n si ty F u n ct io n Clinical state space is X = {1,2,3,4} (state 4 is the “clinical deterioration” state) 0 5 10 15 20 25 0 5 10 15 20 25 Time t P h y si o lo g ic a l V a ri a b le s Y (t) {ym} M m=1 Intensity function λ(t,X(t)) Clinical state process X(t) Observation process {tm} Informative censoring l = 1 X1 = 2 X2 = 3 X3 = 4 Figure 3.
100	14	An episode sampled from the proposed model.
101	198	Estimating the model parameters Ω from the dataset D = {Ed}Dd=1 is a daunting task due to the hiddenness of the patients’ clinical state trajectories {Xdn, Sdn}Dd=1; an application of MCMC-based inference methods, such as the method in (Qin & Shelton, 2015), will incur an excessive computational cost for a complex model like ours.
103	62	The three steps of the our learning algorithm are listed hereunder3.
105	15	., τdKd} for every episode Ed in D. This is achieved by using the E-divisive change-point detection algorithm (Matteson & James, 2014).
109	92	Step 2: Maximum Likelihood Estimation (MLE) of the absorbing states’ parameters By virtue of informative censoring, we know the identities of all the absorbing states, i.e. XdKd = l d, ∀d.
