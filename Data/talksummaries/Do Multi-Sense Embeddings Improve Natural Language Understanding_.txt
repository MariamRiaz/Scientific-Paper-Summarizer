0	25	Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks.
1	42	Most traditional embedding models associate each word type with a single embedding (e.g., Bengio et al. (2006)).
2	39	Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings.
4	68	Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).
5	23	Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).
7	27	Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.
10	37	Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.
39	20	The restaurant has a series of tables t, each of which serves a dish dt.
41	42	The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution: Pr(tw = t) ∝ { NtP (w|dt) if t already exists γP (w|dnew) if t is new (1) where Nt denotes the number of customers already sitting at table t and P (w|dt) denotes the probability of assigning the current data point to cluster dt.
48	70	When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew: p(ew|eneigh) = F (ew, eneigh) (2) F() can take different forms in different learning paradigms, e.g., F = ∏ w′∈neigh p(ew, ew′) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).
52	19	Based on CRP, the probability that assigns the current occurrence to each of the discovered senses or a new sense is given by: Pr(zw = z) ∝  Nwz P (e z w|context) if z already exists γP (w|znew) if z is new (4) where Nwz denotes the number of times already assigned to sense z for token w. P (ezw|context) denotes the probability that current occurrence belonging to (or generated by) sense z.
62	31	The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token.
70	19	Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token.
83	21	To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for this class of multi-sense models and serves as a promise for being extended to NLU tasks.
88	20	For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus: • Standard one-word-one-vector embeddings from skip-gram (50d).
89	20	• Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d) • The concatenation of global word embeddings and sense-specific embeddings (100d).
97	18	We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.
102	43	Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs.
103	34	We adopt a similar training and parameter tuning strategy as for POS tagging.
113	51	Sentiment Analysis–Stanford Treebank The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.
134	18	We adopt two different recurrent models for acquiring sentencelevel embeddings, a standard recurrent model and an LSTM model (Hochreiter and Schmidhuber, 1997).
136	29	We adopted the same training strategy as described earlier.
138	25	Performance is measured using Pearson’s r between the predicted score and goldstandard labels.
139	21	Results for different tasks are represented in Tables 3-9.
146	135	This is sensible since sentence meaning here is sensitive to the semantics of one particular word, which could vary with word sense and which would directly be reflected on the relatedness score.
147	220	(2) By contrast, for sentiment analysis (Tables 5-6), much of the task depends on correctly identifying a few sentiment words like “good” or “bad”, whose senses tend to have similar sentiment values, and hence for which multi-sense embeddings offer little help.
148	72	Multi-sense embeddings might promise to help sentiment analysis for some cases, like disambiguating the word “sound” in “safe and sound” versus “movie sound”.
150	135	Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in their capability to capture local composition like negation, and it’s not clear how helpful multi-sense embeddings are for that aspect.
152	32	Word senses have long been known to be related to POS tags.
158	45	We suggest some hypotheses: • though information about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way, we suspect that the compositional nature of neural models is able to separate the informational chaff from the wheat and choose what information to take up, bridging the gap between single vector and multi-sense paradigms.
