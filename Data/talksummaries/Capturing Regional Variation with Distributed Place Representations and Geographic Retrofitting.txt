0	28	People actively use dialects to mark their regional origin (Shoemark et al., 2017a,b), making them one of the main drivers of language variation.
11	22	These properties also allow us to measure similarities on a continuous scale, which makes represen- tation learning especially useful for the study of regional language variation along several linguistic dimensions.
16	19	In contrast to most dialectometric approaches (Nerbonne et al., 1999; Prokić and Nerbonne, 2008), and in line with common NLP practice (Doyle, 2014; Grieve, 2016; Huang et al., 2016; Rahimi et al., 2017a), we also evaluate the clustered dialect areas quantitatively.
22	40	Contributions In this paper, we make the following contributions to linguistic insights, performance improvements, and algorithmic contributions.
25	91	We use data from the social media app Jodel,2 a mobile chat application that lets people anonymously talk to other users within a 10km-radius around them.
30	13	The vast majority of posts in Jodel are written in standard German, but since it is conceptually spoken langauge (Koch and Oesterreicher, 1985; Eisenstein, 2013), regional and dialectal forms are common, especially in Switzerland, Austria, and rural areas in Southern Germany.
42	24	We try to minimize the effect of such regional topics, by excluding all named entities, as well as the names of all cities in our list, to instead focus on dialectal lexical variation.
54	41	To learn both word and city representations, we use the Doc2Vec implementation of para- graph2vec (Le and Mikolov, 2014) in gensim.7 The model is conceptually similar to word2vec (Mikolov et al., 2013), but also learns document label representations (in our case city names), embedded in the same space as the words.
57	28	The objective is to maximize the log probability of the prediction, y = arg max W log N∑ i=1 log(p(wi|k)) where k is a city, and W = wi...N a sequence of N randomly sampled words from the thread (see Figure 1 for a schematic representation).
62	52	We use non-negative matrix factorization (NMF) on the 300-dimensional city representation matrix to find the first three principal components, normalize them each to values 0.0–1.0 and interpret those as RGB values.8 I.e., we assume the first principal component signals the amount of red, the second component the amount of green, and the third component the amount of blue.
64	18	E.g., 0.5 red, 0.5 green, and 0.5 blue translates into medium gray.
77	17	These dimensions mirror the well-known strong linguistic connection between the southeast of Germany and Austria, and between most cities in the north of Germany.
79	39	However, in order to evaluate against existing dialect maps, we need to discretize the continuous representation.
103	42	To include the geographic knowledge, we retrofit the existing city embeddings C. The goal is to make the representations of cities that are in the same region more similar to each other than to cities in other regions, resulting in a retrofit embeddings matrix Ĉ.
106	16	In order to evaluate our methodology, we measure both its ability to match German dialect distinctions, and the performance of the learned embeddings in a downstream geolocation task.
110	12	This is in line with sociolinguistic findings (Plewnia and Rothe, 2012) about ubiquity of dialect use (more common in the south, therefore more varied regions, reflected in our clustering).
111	16	Due to space constraints, we have to omit further clustering stages, but find linguistically plausible solutions beyond the ones shown here.
115	18	Note that we can only assess the cities within modern-day Germany (clusters formed in Austria or Switzerland are not covered).
117	140	We report homogeneity (whether a cluster contains only data points from a single region) and completeness (how many data points of a NUTS region are in the same cluster), as well as their harmonic mean, the V-score.
120	16	The outline of dialect regions in Lameli’s map is based on the NUTS2 regions, so we compare all clustering solutions to an informed baseline that assigns each city the NUTS2 region it is located in.
121	45	Except for regions in dialect overlaps, each NUTS region is completely contained in one dialect region, so the baseline can achieve almost perfect homogeneity.
132	26	Dialect match Table 1 shows the results of clustering solutions up to 20 clusters for both retrofit and original embeddings.
136	17	Averaged k-means (over 5 runs) is much less consistent, due to random initialization, but presumably also because it cannot incorporate the geographic information.
147	29	Figure 5 shows an example of word and city similarity for the city representation of Vienna.
150	37	We can then find the most similar words to this centroid.
152	19	Figure 3), we get the solutions in Table 3.
153	23	As expected, the regional prototypes do not overlap, but feature dialectal expressions in the south, and general standard German expressions in the north.
154	137	Again, for an in-depth qualitative analysis and discussion of the socio-linguistic correlations, see Purschke and Hovy (In Preparation).
167	136	We use representation learning, structured clustering, and geographic retrofitting on city embeddings to study regional linguistic variation in German.
169	18	The learned city embeddings also capture enough regional distinction serve as input to a downstream geolocation task, outperforming a BOW baseline and producing competitive results.
