0	74	Benefited from the recent advances in neural networks (NNs) and the access to nearly unlimited corpora, neural language models are able to achieve a good perplexity score and generate highquality sentences.
1	28	These LMs automatically capture abundant linguistic information and patterns from large text corpora, and can be applied to facilitate a wide range of NLP applications (Rei, 2017; Liu et al., 2018; Peters et al., 2018).
2	61	Recently, efforts have been made on learning contextualized representations with pre-trained language models (LMs) (Peters et al., 2018).
4	24	However, due to high variability of language, gigantic NNs (e.g., LSTMs with 8,192 hidden states) are preferred to construct informative LMs and extract multifarious linguistic information (Peters et al., 2017).
5	34	Even though these models can be integrated without retraining (using their forward pass only), they still result in heavy computation workloads during inference stage, making them prohibitive for realworld applications.
6	74	In this paper, we aim to compress LMs for the end task in a plug-in-and-play manner.
8	117	However, neural language models are usually composed of RNNs, and their backpropagations require significantly more RAM than their inference.
9	28	It would become even more cumbersome when the target task equips the coupled LMs to capture information in both directions.
12	30	Intuitively, layers of different depths would capture linguistic information of different levels.
28	16	For a L-layers NN, we mark the input and output of the lth layer at the tth time stamp as xl,t and hl,t.
33	18	Specifically, dense connectivity refers to adding direct connections from any layer to all its subsequent layers.
84	22	Specifically, we choose to prepare the LMs for the pruned inputs, thus making them more robust to pruning.
86	16	As in Figure 3, a random part of layers in the LMs are randomly dropped during each batch.
88	16	3) for predicting the next word.
94	24	Following the recent study (Liu et al., 2018), we employ LSTMs to take the character-level input in a context-aware manner, and mark its output for xt as ct.
110	45	For optimization, we decompose it into two steps, i.e., model training and model pruning.
118	24	We will first discuss the capability of the LD-Net as language models, then explore the effectiveness of its contextualized representations.
122	16	The optimization is performed by the Adam algorithm (Kingma and Ba, 2014), the gradient is clipped at 5.0 and the learning rate is set to start from 0.001.
126	32	Note that, for models 3, 5, 6, 7, 8, and 9, PPL refers to the averaged perplexity of the forward and the backward LMs.
130	21	At the same time, comparing the model 8192-1024 and CNN-8192-1024, their only difference is the input method.
133	20	Since replacing embedding layer with CNN would make the training slower, we only conduct experiments with models taking word embedding as the input.
134	38	Comparing LD-Net with other baselines, we think it achieves satisfactory performance with regard to the size of hidden states.
153	16	The layer selection variables zi are initialized as 1, remained unchanged 3 https://nlp.stanford.edu/projects/glove/ during the model training and only be updated during the model pruning.
175	31	Also, as discussed in (Peters et al., 2018, 2017), the performance of the contextualized representation can be further improved by training larger models or using the CNN to represent words.
185	17	As in Table 4, we can observe that, the pruned model achieved about 5 times speed up.
186	37	Although there is still a large margin between We think it implies that ELMo works as token representations instead of sentence representations the actual speed-up and the FLOPs speed-up, we think the resulting decode speed (166K words/s) is sufficient for most real-world applications.
197	18	We further studied the layer selection patterns.
200	54	This is consistent with our intuition that some layers are more important than others and the layer selection algorithm would pick up layers meaningfully.
202	81	We conjugate that large networks trained with dropout can be viewed as a ensemble of small sub-networks (Hara et al., 2016), also there would be several sub-networks having the similar function.
203	110	Accordingly, we think the randomness mainly comes from such redundancy.
