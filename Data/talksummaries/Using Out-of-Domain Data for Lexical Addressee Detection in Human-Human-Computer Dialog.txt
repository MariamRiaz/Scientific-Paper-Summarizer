4	16	This is particularly true for systems in which explicit system addressing (e.g., push-to-talk or required keyword addressing) is undesirable.
5	37	Past research on addressee detection has focused on human-human (H-H) settings, such as meetings, sometimes with multimodal cues (op den Akker and Traum, 2009).
7	69	Some systems combine gaze with lexical and syntactic cues to detect H-H speech (Katzenmaier et al., 2004).
13	12	This is because addressee detection in the H-H-C scenario becomes even more challenging when the system is designed for natural speech, i.e., utterances that are conversational in form and not limited to command phrases with restricted syntax.
21	40	We show in this paper that precisely this training scenario is feasible and achieves results that are comparable or better than using completely matched H-H-C training data.
36	11	The ASR system used in the system was based on off-the-shelf acoustic models and had only the language model adapted to the domain, using very limited data.
46	11	Fisher telephone conversations and ICSI meetings are both corpora of human-directed speech.
48	13	The ICSI meeting corpus (Janin et al., 2003) contains multiparty face-to-face technical discussions among colleagues.
54	12	Class likelihoods are obtained from standard trigram backoff language models, using Witten-Bell discounting for smoothing (Witten and Bell, 1991).
55	32	For combining various training data sources, we use language model adaptation by interpolation (Bellegarda, 2004).
57	15	The probability estimates from in-domain and out-of-domain models are then averaged in a weighted fashion: P (wk|hk) = λPin(wk|hk) + (1− λ)Pout(wk|hk) (2) where wk is the k-th word, hk is the (n − 1)-gram history for the wordwk.
71	13	A hidden Markov model tagger using POStrigram statistics and context-independent class membership probabilities was used for tagging all LM training data.
83	23	As in Shriberg et al. (2012), we used equal error rate (EER) to compare systems, since we are interested in the discriminative power of the decision score independent of priors and costs.
86	11	We also use classification accuracy (based on data priors) in one analysis below, because EERs are not comparable for different test data subdivisions.
99	20	Somewhat surprisingly, the system trained on outof-domain data alone performs better by 3.3 EER points on ASR output and 3.1 points on transcripts compared to the in-domain baseline.
100	30	Combining in-domain and out-of-domain data (both-all, bothsmall) gives about 1 point additional EER gain.
102	18	Figure 4 shows the detection error trade-off (DET) between false alarm and miss errors for the systems in Table 3.
110	48	We note that H-H utterance are very poorly recognized in the ASR condition when only out-of-domain data is used.
111	12	This may be be- cause the human-human corpora used in training consist of transcripts, whereas the ASR output for human-directed utterances is very errorful, creating a severe train-test mismatch.
113	36	This is a standard optimization approach for interpolated language models, and can be carried out efficiently using an expectation maximization algorithm.
115	13	While this approach could theoretically give better results (since perplexity is not a discriminative criterion) we found no significant improvement in our experiments.
121	18	For example, we see that the Fisher model has a much lower perplexity on H-H utterances than the ICSI meeting model.
128	33	This is consistent with the notion that the in-domain model suffers the most from data sparseness, and therefore has the most to gain from better generalization.
130	19	The optimal N differs for ASR output versus transcripts.
131	13	The POS-based model with N = 300 improves the EER by 0.5 points on ASR output, and N = 1000 improves the EER by 0.8 points on transcripts.
134	21	We incrementally increase the window width from 0.5 seconds to 3 seconds and compare results to using full utterances.
135	16	The leveling off of the error plots indicates that most addressee information is contained in the first 1 to 1.5 seconds, although some additional information is found in the later part of utterances (the plots never level off completely).
137	14	To give an intuitive understanding of where this early addressee-relevant information comes from, we tabulated the top 15 word unigrams in each utterance class, are shown in Table 6.
140	46	Human-directed utterances are characterized by subject pronouns such as I and it, or answer particles such as yeah and okay, which likewise occur in initial position.
142	59	We explored the use of outside data for training lexical addressee detection systems for the humanhuman-computer scenario.
