{"sections": [{"heading": "1. Introduction", "text": "Anomaly detection (AD) (Chandola et al., 2009; Aggarwal, 2016) is the task of discerning unusual samples in data. Typically, this is treated as an unsupervised learning problem where the anomalous samples are not known a priori and\n*Equal contribution\nA part of the work was done while LR, RV, LD, and MK were with Department of Computer Science, Humboldt University of Berlin, Germany. 1Hasso Plattner Institute, Potsdam, Germany 2Department of Computer Science, TU Kaiserslautern, Kaiserslautern, Germany 3Machine Learning Group, Department of Electrical Engineering & Computer Science, TU Berlin, Berlin, Germany 4School of Informatics, University of Edinburgh, Edinburgh, Scotland 5German Research Center for Artificial Intelligence (DFKI GmbH), Kaiserslautern, Germany 6ISTD pillar, Singapore University of Technology and Design, Singapore. Correspondence to: Lukas Ruff <contact@lukasruff.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nit is assumed that the majority of the training dataset consists of \u201cnormal\u201d data (here and elsewhere the term \u201cnormal\u201d means not anomalous and is unrelated to the Gaussian distribution). The aim then is to learn a model that accurately describes \u201cnormality.\u201d Deviations from this description are then deemed to be anomalies. This is also known as oneclass classification (Moya et al., 1993). AD algorithms are often trained on data collected during the normal operating state of a machine or system for monitoring (Lavin & Ahmad, 2015). Other domains include intrusion detection for cybersecurity (Garcia-Teodoro et al., 2009), fraud detection (Phua et al., 2005), and medical diagnosis (Salem et al., 2013; Schlegl et al., 2017). As with many fields, the data in these domains is growing rapidly in size and dimensionality and thus we require effective and efficient ways to detect anomalies in large quantities of high-dimensional data.\nClassical AD methods such as the One-Class SVM (OCSVM) (Scho\u0308lkopf et al., 2001) or Kernel Density Estimation (KDE) (Parzen, 1962), often fail in high-dimensional, datarich scenarios due to bad computational scalability and the curse of dimensionality. To be effective, such shallow methods typically require substantial feature engineering. In comparison, deep learning (LeCun et al., 2015; Schmidhuber, 2015) presents a way to learn relevant features automatically, with exceptional successes over classical methods (Collobert et al., 2011; Hinton et al., 2012), especially in computer vision (Krizhevsky et al., 2012; He et al., 2016). How to transfer the benefits of deep learning to AD is less clear, however, since finding the right unsupervised deep objective is hard (Bengio et al., 2013). Current approaches to deep AD have shown promising results (Hawkins et al., 2002; Sakurada & Yairi, 2014; Xu et al., 2015; Erfani et al., 2016; Andrews et al., 2016; Chen et al., 2017), but none of these methods are trained by optimizing an AD based objective function and typically rely on reconstruction error based heuristics.\nIn this work we introduce a novel approach to deep AD inspired by kernel-based one-class classification and minimum volume estimation. Our method, Deep Support Vector Data Description (Deep SVDD), trains a neural network while minimizing the volume of a hypersphere that encloses the network representations of the data (see Figure 1). Minimizing the volume of the hypersphere forces the network to\nextract the common factors of variation since the network must closely map the data points to the center of the sphere."}, {"heading": "2. Related Work", "text": "Before introducing Deep SVDD we briefly review kernelbased one-class classification and present existing deep approaches to AD."}, {"heading": "2.1. Kernel-based One-Class Classification", "text": "Let X \u2286 Rd be the data space. Let k : X \u00d7 X \u2192 [0,\u221e) be a PSD kernel, Fk it\u2019s associated RKHS, and \u03c6k : X \u2192 Fk its associated feature mapping. So k(x, x\u0303) = \u3008\u03c6k(x), \u03c6k(x\u0303)\u3009Fk for all x, x\u0303 \u2208 X where \u3008\u00b7 , \u00b7\u3009Fk is the dot product in Hilbert space Fk (Aronszajn, 1950). We review two kernel machine approaches to AD.\nProbably the most prominent example of a kernel-based method for one-class classification is the One-Class SVM (OC-SVM) (Scho\u0308lkopf et al., 2001). The objective of the OC-SVM finds a maximum margin hyperplane in feature space,w \u2208 Fk, that best separates the mapped data from the origin. Given a dataset Dn = {x1, . . . ,xn} with xi \u2208 X , the OC-SVM solves the primal problem\nmin w,\u03c1,\u03be\n1 2 \u2016w\u20162Fk \u2212 \u03c1+ 1 \u03bdn n\u2211 i=1 \u03bei\ns.t. \u3008w, \u03c6k(xi)\u3009Fk \u2265 \u03c1\u2212 \u03bei, \u03bei \u2265 0, \u2200i. (1)\nHere \u03c1 is the distance from the origin to hyperplane w. Nonnegative slack variables \u03be = (\u03be1, . . . , \u03ben)\u1d40 allow the margin to be soft, but violations \u03bei get penalized. \u2016w\u20162Fk is a regularizer on the hyperplane w where \u2016 \u00b7 \u2016Fk is the norm induced by \u3008\u00b7 , \u00b7\u3009Fk . The hyperparameter \u03bd \u2208 (0, 1] controls the trade-off in the objective. Separating the data from the origin in feature space translates into finding a halfspace in which most of the data lie and points lying outside this halfspace, i.e. \u3008w, \u03c6k(x)\u3009Fk < \u03c1, are deemed\nto be anomalous.\nSupport Vector Data Description (SVDD) (Tax & Duin, 2004) is a technique related to OC-SVM where a hypersphere is used to separate the data instead of a hyperplane. The objective of SVDD is to find the smallest hypersphere with center c \u2208 Fk and radius R > 0 that encloses the majority of the data in feature space Fk. The SVDD primal problem is given by\nmin R,c,\u03be\nR2 + 1\n\u03bdn \u2211 i \u03bei\ns.t. \u2016\u03c6k(xi)\u2212 c\u20162Fk \u2264 R 2 + \u03bei, \u03bei \u2265 0, \u2200i.\n(2)\nAgain, slack variables \u03bei \u2265 0 allow a soft boundary and hyperparameter \u03bd \u2208 (0, 1] controls the trade-off between penalties \u03bei and the volume of the sphere. Points which fall outside the sphere, i.e. \u2016\u03c6k(x)\u2212 c\u20162Fk > R\n2, are deemed anomalous.\nThe OC-SVM and SVDD are closely related. Both methods can be solved by their respective duals, which are quadratic programs and can be solved via a variety of methods, e.g. sequential minimal optimization (Platt, 1998). In the case of the widely used Gaussian kernel, the two methods are equivalent and are asymptotically consistent density level set estimators (Tsybakov, 1997; Vert & Vert, 2006). Formulating the primal problems with hyperparameter \u03bd \u2208 (0, 1] as in (1) and (2) is a handy choice of parameterization since \u03bd \u2208 (0, 1] is (i) an upper bound on the fraction of outliers, and (ii) a lower bound on the fraction of support vectors (points that are either on or outside the boundary). This result is known as the \u03bd-property (Scho\u0308lkopf et al., 2001) and allows one to incorporate a prior belief about the fraction of outliers present in the training data into the model.\nApart from the necessity to perform explicit feature engineering (Pal & Foody, 2010), another drawback of the aforementioned methods is their poor computational scaling due to the construction and manipulation of the kernel\nmatrix. Kernel-based methods scale at least quadratically in the number of samples (Vempati et al., 2010) unless some sort of approximation technique is used (Rahimi & Recht, 2007). Moreover, prediction with kernel methods requires storing support vectors which can require large amounts of memory. As we will see, Deep SVDD does not suffer from these limitations."}, {"heading": "2.2. Deep Approaches to Anomaly Detection", "text": "Deep learning (LeCun et al., 2015; Schmidhuber, 2015) is a subfield of representation learning (Bengio et al., 2013) that utilizes model architectures with multiple processing layers to learn data representations with multiple levels of abstraction. Multiple levels of abstraction allow for the representation of a rich space of features in a very compact and distributed form. Deep (multi-layered) neural networks are especially well-suited for learning representations of data that are hierarchical in nature, such as images or text.\nWe categorize approaches that try to leverage deep learning for AD into either \u201cmixed\u201d or \u201cfully deep.\u201d In mixed approaches, representations are learned separately in a preceding step before these representations are then fed into classical (shallow) AD methods like the OC-SVM. Fully deep approaches, in contrast, employ the representation learning objective directly for detecting anomalies.\nWith Deep SVDD, we introduce a novel, fully deep approach to unsupervised AD. Deep SVDD learns to extract the common factors of variation of the data distribution by training a neural network to fit the network outputs into a hypersphere of minimum volume. In comparison, virtually all existing deep AD approaches rely on the reconstruction error \u2014 either in mixed approaches for just learning representations, or directly for both representation learning as well as detection.\nDeep autoencoders (Hinton & Salakhutdinov, 2006) (of various types) are the predominant approach used for deep AD. Autoencoders are neural networks which attempt to learn the identity function while having an intermediate representation of reduced dimension (or some sparsity regularization) serving as a bottleneck to induce the network to extract salient features from some dataset. Typically these networks are trained to minimize reconstruction error, i.e. \u2016x\u2212 x\u0302\u20162. Therefore these networks should be able to extract the common factors of variation from normal samples and reconstruct them accurately, while anomalous samples do not contain these common factors of variation and thus cannot be reconstructed accurately. This allows for the use of autoencoders in mixed approaches (Xu et al., 2015; Andrews et al., 2016; Erfani et al., 2016; Sabokrou et al., 2016), by plugging the learned embeddings into classical AD methods, but also in fully deep approaches, by directly employing the reconstruction error as an anomaly score (Hawkins et al.,\n2002; Sakurada & Yairi, 2014; An & Cho, 2015; Chen et al., 2017). Some variants of the autoencoder used for the purpose of AD include denoising autoencoders (Vincent et al., 2008; 2010), sparse autoencoders (Makhzani & Frey, 2013), variational autoencoders (VAEs) (Kingma & Welling, 2013), and deep convolutional autoencoders (DCAEs) (Masci et al., 2011; Makhzani & Frey, 2015), where the last variant is predominantly used in AD applications with image or video data (Seebo\u0308ck et al., 2016; Richter & Roy, 2017).\nAutoencoders have the objective of dimensionality reduction and do not target AD directly. The main difficulty of applying autoencoders for AD is given in choosing the right degree of compression, i.e. dimensionality reduction. If there was no compression, an autoencoder would just learn the identity function. In the other edge case of information reduction to a single value, the mean would be the optimal solution. That is, the \u201ccompactness\u201d of the data representation is a model hyperparameter and choosing the right balance is hard due to the unsupervised nature and since the intrinsic dimensionality of the data is often difficult to estimate (Bengio et al., 2013). In comparison, we include the compactness of representation into our Deep SVDD objective by minimizing the volume of a data-enclosing hypersphere and thus target AD directly.\nApart from autoencoders, Schlegl et al. (2017) have recently proposed a novel deep AD method based on Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) called AnoGAN. In this method one first trains a GAN to generate samples according to the training data. Given a test point AnoGAN tries to find the point in the generator\u2019s latent space that generates the sample closest to the test input considered. Intuitively, if the GAN has captured the distribution of the training data then normal samples, i.e. samples from the distribution, should have a good representation in the latent space and anomalous samples will not. To find the point in latent space, Schlegl et al. (2017) perform gradient descent in latent space keeping the learned weights of the generator fixed. AnoGAN finally defines an anomaly score also via the reconstruction error. Similar to autoencoders, a main difficulty of this generative approach is the question of how to regularize the generator for compactness."}, {"heading": "3. Deep SVDD", "text": "In this section we introduce Deep SVDD, a method for deep one-class classification. We present the Deep SVDD objective, its optimization, and theoretical properties."}, {"heading": "3.1. The Deep SVDD Objective", "text": "With Deep SVDD, we build on the kernel-based SVDD and minimum volume estimation by finding a data-enclosing hypersphere of smallest size. However, with Deep SVDD\nwe learn useful feature representations of the data together with the one-class classification objective. To do this we employ a neural network that is jointly trained to map the data into a hypersphere of minimum volume.\nFor some input space X \u2286 Rd and output space F \u2286 Rp, let \u03c6(\u00b7 ;W) : X \u2192 F be a neural network with L \u2208 N hidden layers and set of weights W = {W 1, . . . ,WL} whereW ` are the weights of layer ` \u2208 {1, . . . , L}. That is, \u03c6(x;W) \u2208 F is the feature representation of x \u2208 X given by network \u03c6 with parametersW . The aim of Deep SVDD then is to jointly learn the network parametersW together with minimizing the volume of a data-enclosing hypersphere in output space F that is characterized by radius R > 0 and center c \u2208 F which we assume to be given for now. Given some training data Dn = {x1, . . . ,xn} on X , we define the soft-boundary Deep SVDD objective as\nmin R,W\nR2 + 1\n\u03bdn n\u2211 i=1 max{0, \u2016\u03c6(xi;W)\u2212 c\u20162 \u2212R2}\n+ \u03bb\n2 L\u2211 `=1 \u2016W `\u20162F . (3)\nAs in kernel SVDD, minimizing R2 minimizes the volume of the hypersphere. The second term is a penalty term for points lying outside the sphere after being passed through the network, i.e. if its distance to the center \u2016\u03c6(xi;W) \u2212 c\u2016 is greater than radius R. Hyperparameter \u03bd \u2208 (0, 1] controls the trade-off between the volume of the sphere and violations of the boundary, i.e. allowing some points to be mapped outside the sphere. We prove in Section 3.3 that the \u03bd-parameter in fact allows us to control the proportion of outliers in a model similar to the \u03bd-property of kernel methods mentioned previously. The last term is a weight decay regularizer on the network parametersW with hyperparameter \u03bb > 0, where \u2016 \u00b7 \u2016F denotes the Frobenius norm.\nOptimizing objective (3) lets the network learn parameters W such that data points are closely mapped to the center c of the hypersphere. To achieve this the network must extract the common factors of variation of the data. As a result, normal examples of the data are closely mapped to center c, whereas anomalous examples are mapped further away from the center or outside of the hypersphere. Through this we obtain a compact description of the normal class. Minimizing the size of the sphere enforces this learning process.\nFor the case where we assume most of the training data Dn is normal, which is often the case in one-class classification tasks, we propose an additional simplified objective. We\ndefine the One-Class Deep SVDD objective as\nmin W\n1\nn n\u2211 i=1 \u2016\u03c6(xi;W)\u2212 c\u20162 + \u03bb 2 L\u2211 `=1 \u2016W `\u20162F . (4)\nOne-Class Deep SVDD simply employs a quadratic loss for penalizing the distance of every network representation \u03c6(xi;W) to c \u2208 F . The second term again is a network weight decay regularizer with hyperparameter \u03bb > 0. We can think of One-Class Deep SVDD also as finding a hypersphere of minimum volume with center c. But unlike in soft-boundary Deep SVDD, where the hypersphere is contracted by penalizing the radius directly and the data representations that fall outside the sphere, One-Class Deep SVDD contracts the sphere by minimizing the mean distance of all data representations to the center. Again, to map the data (on average) as close to center c as possible, the neural network must extract the common factors of variation. Penalizing the mean distance over all data points instead of allowing some points to fall outside the hypersphere is consistent with the assumption that the majority of training data is from one class.\nFor a given test point x \u2208 X , we can naturally define an anomaly score s for both variants of Deep SVDD by the distance of the point to the center of the hypersphere, i.e.\ns(x) = \u2016\u03c6(x;W\u2217)\u2212 c\u20162, (5)\nwhereW\u2217 are the network parameters of a trained model. For soft-boundary Deep SVDD, we can adjust this score by subtracting the final radiusR\u2217 of the trained model such that anomalies (points with representations outside the sphere) have positive scores, whereas inliers have negative scores. Note that the network parametersW\u2217 (and R\u2217) completely characterize a Deep SVDD model and no data must be stored for prediction, thus endowing Deep SVDD a very low memory complexity. This also allows fast testing by simply evaluating the network \u03c6 with learned parameters W\u2217 at some test point x \u2208 X which usually is just a concatenation of simple functions.\nWe address Deep SVDD optimization and selection of the hypersphere center c \u2208 F in the following two subsections."}, {"heading": "3.2. Optimization of Deep SVDD", "text": "We use stochastic gradient descent (SGD) and its variants (e.g., Adam (Kingma & Ba, 2014)) to optimize the parametersW of the neural network in both Deep SVDD objectives using backpropagation. Training is carried out until convergence to a local minimum. Using SGD allows Deep SVDD to scale well with large datasets as its computational complexity scales linearly in the number of training batches and each batch can be processed in parallel (e.g. by processing on multiple GPUs). SGD optimization also enables iterative or online learning.\nSince the network parameters W and radius R generally live on different scales, using one common SGD learning rate may be inefficient for optimizing the soft-boundary Deep SVDD. Instead, we suggest optimizing the network parameters W and radius R alternately in an alternating minimization/block coordinate descent approach. That is, we train the network parametersW for some k \u2208 N epochs while the radius R is fixed. Then, after every k-th epoch, we solve for radius R given the data representations from the network using the network parametersW of the latest update. R can be easily solved for via line search."}, {"heading": "3.3. Properties of Deep SVDD", "text": "For an improperly formulated network or hypersphere center c, the Deep SVDD can learn trivial, uninformative solutions. Here we theoretically demonstrate some network properties which will yield trivial solutions (and thus must be avoided). We then prove the \u03bd-property for soft-boundary Deep SVDD.\nIn the following let Jsoft(W, R) and JOC(W) be the softboundary and One-Class Deep SVDD objective functions as defined in (3) and (4). First, we show that including the hypersphere center c \u2208 F as a free optimization variable leads to trivial solutions for both objectives.\nProposition 1 (All-zero-weights solution). LetW0 be the set of all-zero network weights, i.e., W ` = 0 for every W ` \u2208 W0. For this choice of parameters, the network maps any input to the same output, i.e., \u03c6(x;W0) = \u03c6(x\u0303;W0) =: c0 \u2208 F for any x, x\u0303 \u2208 X . Then, if c = c0, the optimal solution of Deep SVDD is given byW\u2217 =W0 and R\u2217 = 0.\nProof. For every configuration (W, R) we have that Jsoft(R,W) \u2265 0 and JOC(W) \u2265 0 respectively. As the output of the all-zero-weights network \u03c6(x;W0) is constant for every input x \u2208 X (all parameters in each network unit are zero and thus the linear projection in each network unit maps any input to zero), and the center of the hypersphere is given by c = \u03c6(x;W0), all errors in the empirical sums of the objectives become zero. Thus, R\u2217 = 0 and W\u2217 = W0 are optimal solutions since Jsoft(W\u2217, R\u2217) = 0 and JOC(W\u2217) = 0 in this case.\nStated less formally, Proposition 1 implies that if we include the hypersphere center c as a free variable in the SGD optimization, Deep SVDD would likely converge to the trivial solution (W\u2217, R\u2217, c\u2217) = (W0, 0, c0). We call such a solution, where the network learns weights such that the network produces a constant function mapping to the hypersphere center, \u201chypersphere collapse\u201d since the hypersphere radius collapses to zero. Proposition 1 also implies that we require c 6= c0 when fixing c in output space F because otherwise a hypersphere collapse would again be possible. For a\nconvolutional neural network (CNN) with ReLU activation functions, for example, this would require c 6= 0. We found empirically that fixing c as the mean of the network representations that result from performing an initial forward pass on some training data sample to be a good strategy. Although we obtained similar results in our experiments for other choices of c (making sure c 6= c0), we found that fixing c in the neighborhood of the initial network outputs made SGD convergence faster and more robust.\nNext, we identify two network architecture properties, that would also enable trivial hypersphere collapse solutions. Proposition 2 (Bias terms). Let c \u2208 F be any fixed hypersphere center. If there is any hidden layer in network \u03c6(\u00b7 ;W) : X \u2192 F having a bias term, there exists an optimal solution (R\u2217,W\u2217) of the Deep SVDD objectives (3) and (4) with R\u2217 = 0 and \u03c6(x;W\u2217) = c for every x \u2208 X .\nProof. Assume layer ` \u2208 {1, . . . , L} with weightsW ` also has a bias term b`. For any input x \u2208 X , the output of layer ` is then given by\nz`(x) = \u03c3`(W ` \u00b7 z`\u22121(x) + b`),\nwhere \u201c\u00b7\u201d denotes a linear operator (e.g., matrix multiplication or convolution), \u03c3`(\u00b7) is the activation of layer `, and the output z`\u22121 of the previous layer `\u2212 1 depends on input x by the concatenation of previous layers. Then, for W ` = 0, we have that z`(x) = \u03c3`(b`), i.e., the output of layer ` is constant for every input x \u2208 X . Therefore, the bias term b` (and the weights of the subsequent layers) can be chosen such that \u03c6(x;W\u2217) = c for every x \u2208 X (assuming c is in the image of the network as a function of b` and the subsequent parametersW `+1, . . . ,WL). Hence, selectingW\u2217 in this way results in an empirical term of zero and choosing R\u2217 = 0 gives the optimal solution (ignoring the weight decay regularization terms for simplicity).\nPut differently, Proposition 2 implies that networks with bias terms can easily learn any constant function, which is independent of the input x \u2208 X . It follows that bias terms should not be used in neural networks with Deep SVDD since the network can learn the constant function mapping directly to the hypersphere center, leading to hypersphere collapse.1\nProposition 3 (Bounded activation functions). Consider a network unit having a monotonic activation function \u03c3(\u00b7) that has an upper (or lower) bound with supz \u03c3(z) 6= 0 (or infz \u03c3(z) 6= 0). Then, for a set of unit inputs {z1, . . . ,zn} that have at least one feature that is positive or negative for all inputs, the non-zero supremum (or infimum) can be uniformly approximated on the set of inputs.\n1Proposition 2 also explains why autoencoders with bias terms are vulnerable to converge to a constant mapping onto the mean, which is the optimal constant solution of the mean squared error.\nProof. W.l.o.g. consider the case of \u03c3 being upper bounded by B := supz \u03c3(z) 6= 0 and feature k being positive for all inputs, i.e. z(k)i > 0 for every i = 1, . . . , n. Then, for every \u03b5 > 0, one can always choose the weight of the k-th element wk large enough (setting all other network unit weights to zero) such that supi |\u03c3(wk z (k) i )\u2212B| < \u03b5.\nProposition 3 simply says that a network unit with bounded activation function can be saturated for all inputs having at least one feature with common sign thereby emulating a bias term in the subsequent layer, which again leads to a hypersphere collapse. Therefore, unbounded activation functions (or functions only bounded by 0) such as the ReLU should be preferred in Deep SVDD to avoid a hypersphere collapse due to \u201clearned\u201d bias terms.\nTo summarize the above analysis: the choice of hypersphere center c must be something other than the all-zero-weights solution and only neural networks without bias terms or bounded activation functions should be used in Deep SVDD to prevent a hypersphere collapse solution. Lastly, we prove that the \u03bd-property also holds for soft-boundary Deep SVDD which allows to include a prior assumption on the number of anomalies assumed to be present in the training data.\nProposition 4 (\u03bd-property). Hyperparameter \u03bd \u2208 (0, 1] in the soft-boundary Deep SVDD objective in (3) is an upper bound on the fraction of outliers and a lower bound on the fraction of samples being outside or on the boundary of the hypersphere.\nProof. Define di = \u2016\u03c6(xi;W) \u2212 c\u20162 for i = 1, . . . , n. W.l.o.g. assume d1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 dn. The number of outliers is then given by nout = |{i : di > R2}| and we can write the soft-boundary objective Jsoft (in radius R) as\nJsoft(R) = R 2 \u2212 nout\n\u03bdn R2 =\n( 1\u2212 nout\n\u03bdn\n) R2.\nThat is, radius R is decreased as long as nout \u2264 \u03bdn holds and decreasing R gradually increases nout. Thus, noutn \u2264 \u03bd must hold in the optimum, i.e. \u03bd is an upper bound on the fraction of outliers, and the optimal radius R\u2217 is given for the largest nout for which this inequality still holds. Finally, we have that R\u22172 = di for i = nout + 1 since radius R is minimal in this case and points on the boundary do not increase the objective. Hence, we also have |{i : di \u2265 R\u22172}| \u2265 nout + 1 \u2265 \u03bdn."}, {"heading": "4. Experiments", "text": "We evaluate Deep SVDD on the well-known MNIST (LeCun et al., 2010) and CIFAR-10 (Krizhevsky & Hinton, 2009) datasets. Adversarial attacks (Goodfellow et al., 2015) have seen a lot attention recently and here we examine the possibility of using anomaly detection to detect such attacks.\nTo do this we apply Boundary Attack (Brendel et al., 2018) to the GTSRB stop signs dataset (Stallkamp et al., 2011). We compare our method against a diverse collection of stateof-the-art methods from different paradigms. We use image data since they are usually high-dimensional and moreover allow for a qualitative visual assessment of detected anomalies by human observers. Using classification datasets to create one-class classification setups allows us to evaluate the results quantitatively via AUC measure by using the ground truth labels in testing (cf. Erfani et al., 2016; Emmott et al., 2016). For training, of course, we do not use any labels.2"}, {"heading": "4.1. Competing methods", "text": "Shallow Baselines (i) Kernel OC-SVM/SVDD with Gaussian kernel. We select the inverse length scale \u03b3 from \u03b3 \u2208 {2\u221210, 2\u22129, . . . , 2\u22121} via grid search using the performance on a small holdout set (10 % of randomly drawn test samples). This grants shallow SVDD a small supervised advantage. We run all experiments for \u03bd \u2208 {0.01, 0.1} and report the better result. (ii) Kernel density estimation (KDE). We select the bandwidth h of the Gaussian kernel from h \u2208 {20.5, 21, . . . , 25} via 5-fold cross-validation using the log-likelihood score. (iii) Isolation Forest (IF) (Liu et al., 2008). We set the number of trees to t = 100 and the sub-sampling size to \u03c8 = 256, as recommended in the original work. We do not compare to lazy evaluation approaches since such methods have no training phase and do not learn a model of normality (e.g. Local Outlier Factor (LOF) (Breunig et al., 2000)). For all three shallow baselines, we reduce the dimensionality of the data via PCA, where we choose the minimum number of eigenvectors such that at least 95% of the variance is retained (cf. Erfani et al., 2016).\nDeep Baselines and Deep SVDD We compare Deep SVDD to the two deep approaches described Section 2.2. We choose the DCAE from the various autoencoders since our experiments are on image data. For the DCAE encoder, we employ the same network architectures as we use for Deep SVDD. The decoder is then created symmetrically, where we substitute max-pooling with upsampling. We train the DCAE using the MSE loss. For AnoGAN we fix the architecture to DCGAN (Radford et al., 2015) and set the latent space dimensionality to 256, following Metz et al. (2017), and otherwise follow Schlegl et al. (2017). For Deep SVDD, we remove the bias terms in all network units to prevent a hypersphere collapse as explained in Section 3.3. In soft-boundary Deep SVDD, we solve for R via line search every k = 5 epochs. We choose \u03bd from \u03bd \u2208 {0.01, 0.1} and again report the best results. As was described in Sec-\n2We provide our code at https://github.com/lukasruff/DeepSVDD.\ntion 3.3, we set the hypersphere center c to the mean of the mapped data after performing an initial forward pass. For optimization, we use the Adam optimizer (Kingma & Ba, 2014) with parameters as recommended in the original work and apply Batch Normalization (Ioffe & Szegedy, 2015). For the competing deep AD methods we initialize network weights by uniform Glorot weights (Glorot & Bengio, 2010), and for Deep SVDD use the weights from the trained DCAE encoder for initialization, thus establishing a pre-training procedure. We employ a simple two-phase learning rate schedule (searching + fine-tuning) with initial learning rate \u03b7 = 10\u22124, and subsequently \u03b7 = 10\u22125. For DCAE we train 250 + 100 epochs, for Deep SVDD 150 + 100. Leaky ReLU activations are used with leakiness \u03b1 = 0.1."}, {"heading": "4.2. One-class classification on MNIST and CIFAR-10", "text": "Setup Both MNIST and CIFAR-10 have ten different classes from which we create ten one-class classification setups. In each setup, one of the classes is the normal class and samples from the remaining classes are used to represent anomalies. We use the original training and test splits in our experiments and only train with training set examples from the respective normal class. This gives training set sizes of n \u2248 6 000 for MNIST and n = 5000 for CIFAR-10. Both test sets have 10 000 samples including samples from the nine anomalous classes for each setup. We pre-process all images with global contrast normalization using the L1norm and finally rescale to [0, 1] via min-max-scaling.\nNetwork architectures For both datasets, we use LeNettype CNNs, where each convolutional module consists of a convolutional layer followed by leaky ReLU activations and 2 \u00d7 2 max-pooling. On MNIST, we use a CNN with two modules, 8\u00d7 (5\u00d75\u00d71)-filters followed by 4\u00d7 (5\u00d75\u00d71)- filters, and a final dense layer of 32 units. On CIFAR-10, we use a CNN with three modules, 32\u00d7 (5\u00d7 5\u00d7 3)-filters, 64\u00d7(5\u00d75\u00d73)-filters, and 128\u00d7(5\u00d75\u00d73)-filters, followed by a final dense layer of 128 units. We use a batch size of 200 and set the weight decay hyperparameter to \u03bb = 10\u22126.\nResults Results are presented in Table 1. Deep SVDD clearly outperforms both its shallow and deep competitors on MNIST. On CIFAR-10 the picture is mixed. Deep SVDD, however, shows an overall strong performance. It is interesting to note that shallow SVDD and KDE perform better than deep methods on three of the ten CIFAR-10 classes. Figures 2 and 3 show examples of the most normal and most anomalous in-class samples according to Deep SVDD and KDE respectively. We can see that normal examples of the classes on which KDE performs best seem to have strong global structures. For example, TRUCK images are mostly divided horizontally into street and sky, and DEER as well as FROG have similar colors globally. For these classes, choosing local CNN features can be questioned. These cases underline the importance of network architecture choice. Notably, the One-Class Deep SVDD performs slightly better than its softboundary counterpart on both datasets. This may be because the assumption of no anomalies being present in the training data is valid in our scenario. Due to SGD optimization, deep methods show higher standard deviations."}, {"heading": "4.3. Adversarial attacks on GTSRB stop signs", "text": "Setup Detecting adversarial attacks is vital in many applications such autonomous driving. In this experiment, we test how Deep SVDD compares to its competitors on detecting adversarial examples. We consider the \u201cstop sign\u201d class of the German Traffic Sign Recognition Benchmark (GTSRB) dataset, for which we generate adversarial examples from randomly drawn stop sign images of the test set using Boundary Attack. We train the models again only on normal stop sign samples and in testing check if adversarial examples are correctly detected. The training set contains n = 780 stop signs. The test set is composed of 270 normal examples and 20 adversarial examples. We pre-process the data by removing the 10% border around each sign, and then resize every image to 32\u00d7 32 pixels. After that, we again apply global contrast normalization using the L1-norm and rescale to the unit interval [0, 1].\nNetwork architecture We use a CNN with LeNet architecture having three convolutional modules, 16\u00d7(5\u00d75\u00d73)- filters, 32\u00d7 (5\u00d7 5\u00d7 3)-filters, and 64\u00d7 (5\u00d7 5\u00d7 3)-filters, followed by a final dense layer of 32 units. We train with a smaller batch size of 64, due to the dataset size and set again hyperparamter \u03bb = 10\u22126.\nResults Table 2 shows the results. The One-Class Deep SVDD shows again the best performance. Generally, the deep methods perform better. The DCGAN of AnoGAN did not converge due to the data set size which is too small for GANs. Figure 4 shows the most anomalous samples detected by One-Class Deep SVDD which are either adversarial attacks or images in odd perspectives that are cropped incorrectly. We refer to the supplementary material for more examples of the most normal images and anomalies detected."}, {"heading": "5. Conclusion", "text": "We introduced the first fully deep one-class classification objective for unsupervised AD in this work. Our method, Deep SVDD, jointly trains a deep neural network while optimizing a data-enclosing hypersphere in output space. Through this Deep SVDD extracts common factors of variation from the data. We have demonstrated theoretical properties of our method such as the \u03bd-property that allows to incorporate a prior assumption on the number of outliers being present in the data. Our experiments demonstrate quantitatively as well as qualitatively the sound performance of Deep SVDD."}, {"heading": "Acknowledgements", "text": "We kindly thank the reviewers for their constructive feedback which helped to improve this work. LR acknowledges financial support from the German Federal Ministry of Transport and Digital Infrastructure (BMVI) in the project OSIMAB (FKZ: 19F2017E). AB is grateful for support by the SUTD startup grant and the STElectronics-SUTD Cybersecurity Laboratory. MK and RV acknowledge support from the German Research Foundation (DFG) award KL 2698/2- 1 and from the Federal Ministry of Science and Education (BMBF) award 031B0187B."}], "year": 2018, "references": [{"title": "Variational Autoencoder based Anomaly Detection using Reconstruction Probability", "authors": ["J. An", "S. Cho"], "venue": "SNU Data Mining Center, Tech. Rep.,", "year": 2015}, {"title": "Representation Learning: A Review and New Perspectives", "authors": ["Y. Bengio", "A. Courville", "P. Vincent"], "venue": "IEEE TPAMI,", "year": 2013}, {"title": "Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models", "authors": ["W. Brendel", "J. Rauber", "M. Bethge"], "venue": "In ICLR,", "year": 2018}, {"title": "LOF: Identifying Density-Based Local Outliers", "authors": ["M.M. Breunig", "Kriegel", "H.-P", "R.T. Ng", "J. Sander"], "venue": "In SIGMOD Record,", "year": 2000}, {"title": "Anomaly Detection: A Survey", "authors": ["V. Chandola", "A. Banerjee", "V. Kumar"], "venue": "ACM Computing Surveys,", "year": 2009}, {"title": "Outlier Detection with Autoencoder Ensembles", "authors": ["J. Chen", "S. Sathe", "C. Aggarwal", "D. Turaga"], "venue": "In SDM,", "year": 2017}, {"title": "Natural Language Processing (Almost) from Scratch", "authors": ["R. Collobert", "J. Weston", "L. Bottou", "M. Karlen", "K. Kavukcuoglu", "P. Kuksa"], "venue": "JMLR, 12(Aug):2493\u2013", "year": 2011}, {"title": "Anomaly detection meta-analysis", "authors": ["A. Emmott", "A. Das", "T. Dietterich", "A. Fern", "Wong", "W.-K"], "year": 2016}, {"title": "High-dimensional and large-scale anomaly detection using a linear one-class SVM with deep learning", "authors": ["S.M. Erfani", "S. Rajasegarar", "S. Karunasekera", "C. Leckie"], "venue": "Pattern Recognition,", "year": 2016}, {"title": "Anomaly-based network intrusion detection: Techniques, systems and challenges", "authors": ["P. Garcia-Teodoro", "J. Diaz-Verdejo", "G. Maci\u00e1-Fern\u00e1ndez", "E. V\u00e1zquez"], "venue": "Computers & Security,", "year": 2009}, {"title": "Understanding the difficulty of training deep feedforward neural networks", "authors": ["X. Glorot", "Y. Bengio"], "venue": "In AISTATS, pp", "year": 2010}, {"title": "Generative Adversarial Nets", "authors": ["I. Goodfellow", "J. Pouget-Abadie", "M. Mirza", "B. Xu", "D. Warde-Farley", "S. Ozair", "A. Courville", "Y. Bengio"], "venue": "In NIPS,", "year": 2014}, {"title": "Explaining and harnessing adversarial examples", "authors": ["I. Goodfellow", "J. Shlens", "C. Szegedy"], "venue": "In ICLR,", "year": 2015}, {"title": "Outlier Detection Using Replicator Neural Networks", "authors": ["S. Hawkins", "H. He", "G. Williams", "R. Baxter"], "venue": "In DaWaK,", "year": 2002}, {"title": "Deep Residual Learning for Image Recognition", "authors": ["K. He", "X. Zhang", "S. Ren", "J. Sun"], "venue": "In CVPR,", "year": 2016}, {"title": "Reducing the Dimensionality of Data with", "authors": ["G.E. Hinton", "R.R. Salakhutdinov"], "venue": "Neural Networks. Science,", "year": 2006}, {"title": "Deep Neural Networks for Acoustic Modeling in Speech Recognition", "authors": ["G.E. Hinton", "L. Deng", "D. Yu", "G.E. Dahl", "Mohamed", "A.-R", "N. Jaitly", "A. Senior", "V. Vanhoucke", "P. Nguyen", "Sainath", "T. N"], "venue": "IEEE Signal Processing Magazine,", "year": 2012}, {"title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift", "authors": ["S. Ioffe", "C. Szegedy"], "venue": "In ICML, pp", "year": 2015}, {"title": "Adam: A Method for Stochastic Optimization", "authors": ["D. Kingma", "J. Ba"], "year": 2014}, {"title": "Auto-Encoding Variational Bayes", "authors": ["D.P. Kingma", "M. Welling"], "venue": "In ICLR,", "year": 2013}, {"title": "Learning multiple layers of features from tiny images", "authors": ["A. Krizhevsky", "G.E. Hinton"], "year": 2009}, {"title": "ImageNet Classification with Deep Convolutional Neural Networks", "authors": ["A. Krizhevsky", "I. Sutskever", "G.E. Hinton"], "venue": "In NIPS, pp", "year": 2012}, {"title": "Evaluating Real-time Anomaly Detection Algorithms \u2014 the Numenta Anomaly Benchmark", "authors": ["A. Lavin", "S. Ahmad"], "venue": "In 14th ICMLA, pp", "year": 2015}, {"title": "Isolation Forest", "authors": ["F.T. Liu", "K.M. Ting", "Zhou", "Z.-H"], "venue": "In ICDM, pp", "year": 2008}, {"title": "Winner-Take-All Autoencoders", "authors": ["A. Makhzani", "B.J. Frey"], "venue": "In NIPS, pp", "year": 2015}, {"title": "Stacked Convolutional Auto-Encoders for Hierarchical Feature Extraction", "authors": ["J. Masci", "U. Meier", "D. Cire\u015fan", "J. Schmidhuber"], "venue": "ICANN, pp", "year": 2011}, {"title": "One-class classifier networks for target recognition applications", "authors": ["M.M. Moya", "M.W. Koch", "L.D. Hostetler"], "venue": "In Proceedings World Congress on Neural Networks,", "year": 1993}, {"title": "Feature selection for classification of hyperspectral data by SVM", "authors": ["M. Pal", "G.M. Foody"], "venue": "IEEE Transactions on Geoscience and Remote Sensing,", "year": 2010}, {"title": "On Estimation of a Probability Density Function and Mode", "authors": ["E. Parzen"], "venue": "The annals of mathematical statistics,", "year": 1962}, {"title": "A Comprehensive Survey of Data Mining-based Fraud Detection Research", "authors": ["C. Phua", "V. Lee", "K. Smith", "R. Gayler"], "venue": "Clayton School of Information Technology,", "year": 2005}, {"title": "Sequential minimal optimization: A fast algorithm for training support vector machines", "authors": ["J. Platt"], "year": 1998}, {"title": "Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks", "authors": ["A. Radford", "L. Metz", "S. Chintala"], "year": 2015}, {"title": "Random features for large-scale kernel machines", "authors": ["A. Rahimi", "B. Recht"], "venue": "In NIPS,", "year": 2007}, {"title": "Safe Visual Navigation via Deep Learning and Novelty Detection", "authors": ["C. Richter", "N. Roy"], "venue": "In Robotics: Science and Systems Conference,", "year": 2017}, {"title": "Fully Convolutional Neural Network for Fast Anomaly Detection in Crowded Scenes", "authors": ["M. Sabokrou", "M. Fayyaz", "M Fathy"], "year": 2016}, {"title": "Anomaly detection using autoencoders with nonlinear dimensionality reduction", "authors": ["M. Sakurada", "T. Yairi"], "venue": "In Proceedings of the 2nd MLSDA Workshop,", "year": 2014}, {"title": "Sensor Fault and Patient Anomaly Detection and Classification in Medical Wireless Sensor Networks", "authors": ["O. Salem", "A. Guerassimov", "A. Mehaoua", "A. Marcus", "B. Furht"], "venue": "In ICC,", "year": 2013}, {"title": "Unsupervised Anomaly Detection with Generative Adversarial Networks to Guide Marker Discovery", "authors": ["T. Schlegl", "P. Seeb\u00f6ck", "S.M. Waldstein", "U. Schmidt-Erfurth", "G. Langs"], "venue": "In IPMI,", "year": 2017}, {"title": "Deep Learning in Neural Networks: An Overview", "authors": ["J. Schmidhuber"], "venue": "Neural networks,", "year": 2015}, {"title": "Estimating the Support of a HighDimensional Distribution", "authors": ["B. Sch\u00f6lkopf", "J.C. Platt", "J. Shawe-Taylor", "A.J. Smola", "R.C. Williamson"], "venue": "Neural computation,", "year": 2001}, {"title": "Identifying and Categorizing Anomalies", "authors": ["P. Seeb\u00f6ck", "S. Waldstein", "S. Klimscha", "B.S. Gerendas", "R. Donner", "T. Schlegl", "U. Schmidt-Erfurth", "G. Langs"], "venue": "Retinal Imaging Data", "year": 2016}, {"title": "The German Traffic Sign Recognition Benchmark: A multiclass classification competition", "authors": ["J. Stallkamp", "M. Schlipsing", "J. Salmen", "C. Igel"], "venue": "In IJCNN,", "year": 2011}, {"title": "Support Vector Data Description", "authors": ["D.M.J. Tax", "R.P.W. Duin"], "venue": "Machine learning,", "year": 2004}, {"title": "On Nonparametric Estimation of Density Level Sets", "authors": ["A.B. Tsybakov"], "venue": "The Annals of Statistics,", "year": 1997}, {"title": "Generalized RBF feature maps for Efficient Detection", "authors": ["S. Vempati", "A. Vedaldi", "A. Zisserman", "C. Jawahar"], "venue": "In 21st BMVC, pp", "year": 2010}, {"title": "Consistency and Convergence Rates of One-Class SVMs and Related Algorithms", "authors": ["R. Vert", "Vert", "J.-P"], "year": 2006}, {"title": "Extracting and Composing Robust Features with Denoising Autoencoders", "authors": ["P. Vincent", "H. Larochelle", "Y. Bengio", "Manzagol", "P.-A"], "venue": "In ICML, pp", "year": 2008}, {"title": "Stacked Denoising Autoencoders: Learning Useful Representations in a Deep Network with a Local Denoising Criterion", "authors": ["P. Vincent", "H. Larochelle", "I. Lajoie", "Y. Bengio", "Manzagol", "P.-A"], "venue": "JMLR, 11(Dec):3371\u20133408,", "year": 2010}, {"title": "Learning Deep Representations of Appearance and Motion for Anomalous Event Detection", "authors": ["D. Xu", "E. Ricci", "Y. Yan", "J. Song", "N. Sebe"], "venue": "In BMVC,", "year": 2015}], "id": "SP:6af440915b8a0718c93be1cf61905e41e620484a", "authors": [{"name": "Lukas Ruff", "affiliations": []}, {"name": "Robert A. Vandermeulen", "affiliations": []}, {"name": "Nico G\u00f6rnitz", "affiliations": []}, {"name": "Lucas Deecke", "affiliations": []}, {"name": "Shoaib A. Siddiqui", "affiliations": []}, {"name": "Alexander Binder", "affiliations": []}, {"name": "Emmanuel M\u00fcller", "affiliations": []}, {"name": "Marius Kloft", "affiliations": []}], "abstractText": "Despite the great advances made by deep learning in many machine learning problems, there is a relative dearth of deep learning approaches for anomaly detection. Those approaches which do exist involve networks trained to perform a task other than anomaly detection, namely generative models or compression, which are in turn adapted for use in anomaly detection; they are not trained on an anomaly detection based objective. In this paper we introduce a new anomaly detection method\u2014Deep Support Vector Data Description\u2014, which is trained on an anomaly detection based objective. The adaptation to the deep regime necessitates that our neural network and training procedure satisfy certain properties, which we demonstrate theoretically. We show the effectiveness of our method on MNIST and CIFAR-10 image benchmark datasets as well as on the detection of adversarial examples of GTSRB stop signs.", "title": "Deep One-Class Classification"}