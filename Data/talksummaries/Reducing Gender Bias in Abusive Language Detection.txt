0	75	Automatic detection of abusive language is an important task since such language in online space can lead to personal trauma, cyber-bullying, hate crime, and discrimination.
2	25	For this reason, using machine learning and natural language processing (NLP) systems to automatically detect abusive language is useful for many websites or social media services.
3	74	Although many works already tackled on training machine learning models to automatically detect abusive language, recent works have raised concerns about the robustness of those systems.
4	34	Hosseini et al. (2017) have shown how to easily cause false predictions with adversarial examples in Google’s API, and Dixon et al. (2017) show that classifiers can have unfair biases toward certain groups of people.
5	16	We focus on the fact that the representations of abusive language learned in only supervised learning setting may not be able to generalize well enough for practical use since they tend to overfit to certain words that are neutral but occur frequently in the training samples.
6	29	To such classifiers, sentences like “You are a good woman” are considered “sexist” probably because of the word “woman.” This phenomenon, called false positive bias, has been reported by Dixon et al. (2017).
10	20	In this work, we address model biases specific to gender identities (gender bias) existing in abusive language datasets by measuring them with a generated unbiased test set and propose three reduction methods: (1) debiased word embedding, (2) gender swap data augmentation, (3) fine-tuning with a larger corpus.
22	21	3.1 Sexist Tweets (st) This dataset consists of tweets with sexist tweets collected from Twitter by searching for tweets that contain common terms pertaining to sexism such as “feminazi.” The tweets were then annotated by experts based on criteria founded in critical race theory.
23	22	The original dataset also contained a relatively small number of “racist” label tweets, but we only retain “sexist” samples to focus on gender biases.
25	14	3.2 Abusive Tweets (abt) Recently, Founta et al. (2018) has published a large scale crowdsourced abusive tweet dataset with 60K tweets.
29	22	Gender bias cannot be measured when evaluated on the original dataset as the test sets will follow the same biased distribution, so normal evaluation set will not suffice.
30	11	Therefore, we generate a separate unbiased test set for each gender, male and female, using the identity term template method proposed in Dixon et al. (2017).
34	13	Using the released code1 of Dixon et al. (2017), we generated 1,152 samples (576 pairs) by filling the templates with common gender identity pairs (ex.
38	25	AUC), 2) AUC scores on the unbiased generated test set (Gen. AUC), and 3) the false positive/negative equality differences proposed in Dixon et al. (2017) which aggregates the difference between the overall false positive/negative rate and gender-specific false positive/negative rate.
39	23	False Positive Equality Difference (FPED) and False Negative Equality Difference (FNED) are defined as below, where T = {male, female}.
43	14	We explore three neural models used in previous works on abusive language classification: Convolutional Neural Network (CNN) (Park and Fung, 2017), Gated Recurrent Unit (GRU) (Cho et al., 2014), and Bidirectional GRU with self-attention (α-GRU) (Pavlopoulos et al., 2017), but with a simpler mechanism used in Felbo et al. (2017).
47	20	GRU: hidden dimension=512, Maximum Sequence Length=100, Embedding Size=300, Dropout=0.3 3. α-GRU: hidden dimension=256 (bidirectional, so 512 in total), Maximum Sequence Length=100, Attention Size=512, Embedding Size=300, Dropout=0.3 We also compare different pre-trained embeddings, word2vec (Mikolov et al., 2013) trained on Google News corpus, FastText (Bojanowski et al., 2017)) trained on Wikipedia corpus, and randomly initialized embeddings (random) to analyze their effects on the biases.
56	20	This is problematic since not many NLP datasets are large enough to reflect the true data distribution, more prominent in tasks like abusive language where data collection and annotation are difficult.
61	27	Models that “attend” to certain words, such as CNN’s max-pooling or αGRU’s self-attention, tended to result in higher false positive equality difference scores in st dataset.
62	11	These models show effectiveness in catching not only the discriminative features for classification, but also the “unintended” ones causing the model biases.
64	12	Debiased Word Embeddings (DE) (Bolukbasi et al., 2016) proposed an algorithm to correct word embeddings by removing gender stereotypical information.
66	22	Gender Swap (GS) We augment the training data by identifying male entities and swapping them with equivalent female entities and vice-versa.
69	15	A model is initially trained with a larger, less-biased source corpus with a same or similar task, and fine-tuned with a target corpus with a larger bias.
81	25	Table 6 shows the results of experiments using the three methods proposed.
83	25	We can see from the second rows of each section that debiased word embeddings alone do not effectively correct the bias of the whole system that well, while gender swapping significantly reduced both the equality difference scores.
93	20	We assume the performance loss happens because mitigation methods modify the data or the model in a way that sometimes deters the models from discriminating important “unbiased” features.
94	12	We discussed model biases, especially toward gender identity terms, in abusive language detection.
95	11	We found out that pre-trained word embeddings, model architecture, and different datasets all can have influence.
98	31	We believe that a meaningful extension of our work can be developing bias mitigation methods that maintain (or even increase) the classification performance and reduce the bias at the same time.
100	15	However, those works do not deal with natural language where features like gender and race are latent variables inside the language.
