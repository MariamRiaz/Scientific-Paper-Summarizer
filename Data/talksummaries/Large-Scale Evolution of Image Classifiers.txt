8	12	This, together with a set of novel and intuitive mutation operators, allowed us to reach competitive accuracies on the CIFAR-10 dataset.
10	44	We also took a small first step toward generalization and evolved networks on the CIFAR-100 dataset.
11	41	In transitioning from CIFAR-10 to CIFAR-100, we did not modify any aspect or parameter of our algorithm.
15	56	As far as we know, these are the most accurate results obtained on these datasets by automated discovery methods that start from trivial initial conditions.
17	70	In particular, it is a “oneshot” technique, producing a fully trained neural network requiring no post-processing.
19	68	Starting out with poor-performing models with no convolutions, the algorithm must evolve complex convolutional neural networks while navigating a fairly unrestricted search space: no fixed depth, arbitrary skip connections, and numerical parameters that have few restrictions on the values they can take.
22	14	We are hopeful that our explicit discussion of computation cost could spark more study of efficient model search and training.
73	70	During each evolutionary step, a computer—a worker—chooses two individuals at random from this population and compares their fitnesses.
91	18	In this case, the affected worker simply gives up and tries again.
116	10	• ALTER-NUMBER-OF-CHANNELS (of random conv.).
118	18	• INSERT-ONE-TO-ONE (inserts a one-to-one/identity connection, analogous to insert-convolution mutation).
154	13	To estimate computation costs, we identified the basic TensorFlow (TF) operations used by our model training and validation, like convolutions, generic matrix multiplications, etc.
167	23	On the other hand, 25,600 steps are not enough to fully train each individual.
173	22	To avoid over-fitting, neither the evolutionary algorithm nor the neural network training ever see the testing set.
174	51	Each time we refer to “the best model”, we mean the model with the highest validation accuracy.
175	32	However, we always report the test accuracy.
176	38	This applies not only to the choice of the best individual within an experiment, but also to the choice of the best experiment.
179	37	We want to answer the following questions: • Can a simple one-shot evolutionary process start from trivial initial conditions and yield fully trained models that rival hand-designed architectures?
180	20	• What are the variability in outcomes, the parallelizability, and the computation cost of the method?
183	10	Each experiment evolves a population in a few days, typified by the example in Figure 1.
184	15	The figure also contains examples of the architectures discovered, which turn out to be surprisingly simple.
185	30	Evolution attempts skip connections but frequently rejects them.
186	171	To get a sense of the variability in outcomes, we repeated the experiment 5 times.
187	21	Across all 5 experiment runs, the best model by validation accuracy has a testing accuracy of 94.6%.
188	31	Not all experiments reach the same accuracy, but they get close (µ=94.1%, σ=0.4).
189	50	Fine differences in the experiment outcome may be somewhat distinguishable by validation accuracy (correlation coefficient = 0.894).
190	60	The total amount of computation across all 5 experiments was 4×1020 FLOPs (or 9×1019 FLOPs on average per experiment).
191	19	Each experiment was distributed over 250 parallel workers (Section 3.1).
198	12	The low FLOP count is a consequence of random search generating many small, inadequate models that train quickly but consume roughly constant amounts of setup time (not included in the FLOP count).
200	32	We also ran a partial control where the weight-inheritance mechanism is disabled.
