0	32	Distributed machine learning is crucial for many settings where the data is possessed by multiple parties or when the quantity of data prohibits processing at a central location.
1	6	It helps to reduce the computational complexity, improve both the robustness and the scalability of data processing.
2	5	In a distributed setting, multiple entities/nodes collaboratively work toward a common optimization objective through an interactive process of local computation and message passing, which ideally should result in all nodes converging to a global optimum.
7	6	It is therefore highly desirable to ensure such iterative processes are privacy-preserving.
10	4	While a number of such studies have been done for (sub)gradient-based algorithms, the same is much harder for ADMM-based algorithms due to its computational complexity stemming from the fact that each node is required to solve an optimization problem in each iteration.
13	11	Since an attacker can potentially use all intermediate results to perform inference, the privacy loss accumulates over time through the iterative process.
15	10	In this study we propose a perturbation method that could simultaneously improve the accuracy and privacy for ADMM.
21	9	We present problem formulation and definition of differential privacy and ADMM in Section 2 and a modified ADMM algorithm along with its convergence analysis in Section 3.
26	13	Let Vi denote node i’s set of neighbors, excluding itself.
28	9	Consider the regularized empirical risk minimization (ERM) problems for binary classification defined as follows: min fc OERM (fc, Dall) = N∑ i=1 C Bi Bi∑ n=1 L (yni f T c x n i )+ρR(fc) (1) where C ≤ Bi and ρ > 0 are constant parameters of the algorithm, the loss function L (·) measures the accuracy of classifier, and the regularizer R(·) helps to prevent overfitting.
29	36	The goal is to train a (centralized) classifier fc ∈ Rd over the union of all local datasets Dall = ∪i∈N Di in a distributed manner using ADMM, while providing privacy guarantee for each data sample 2.
30	17	To decentralize (1), let fi be the local classifier of each node i.
33	10	fi = wij , wij = fj , i ∈ N , j ∈ Vi (2) where O(fi, Di) = C Bi ∑Bi n=1 L (y n i f T i x n i ) + ρ N R(fi).
34	13	The objective in (2) can be solved using ADMM.
35	12	Let {fi} be the shorthand for {fi}i∈N ; let {wij , λkij} be the shorthand for {wij , λkij}i∈N ,j∈Vi,k∈{a,b}, where λaij , λbij are dual variables corresponding to equality constraints fi = wij and wij = fj respectively.
36	15	Then the augmented Lagrangian is as follows: Lη({fi}, {wij , λkij}) = N∑ i=1 O(fi, Di) + N∑ i=1 ∑ j∈Vi (λaij) T (fi − wij) + N∑ i=1 ∑ j∈Vi (λbij) T (wij − fj) (3) + N∑ i=1 ∑ j∈Vi η 2 (||fi − wij ||22 + ||wij − fj ||22) .
38	7	(7) Using Lemma 3 in (Forero et al., 2010), if dual variables λaij(t) and λ b ij(t) are initialized to zero for all node pairs (i, j), then λaij(t) = λ b ij(t) and λ k ij(t) = −λkji(t) will hold for all iterations with k ∈ {a, b}, i ∈ N , j ∈ Vi.
39	25	Let λi(t) = ∑ j∈Vi λ a ij(t) = ∑ j∈Vi λ b ij(t), then the ADMM iterations (4)-(7) can be simplified as: fi(t+ 1) = argmin fi {O(fi, Di) + 2λi(t)T fi +η ∑ j∈Vi ||1 2 (fi(t) + fj(t))− fi||22 } ; (8) λi(t+ 1) = λi(t) + η 2 ∑ j∈Vi (fi(t+ 1)− fj(t+ 1)) .
41	6	Differential privacy (Dwork, 2006) can be used to measure the privacy risk of each individual sample in the dataset quantitatively.
42	4	Mathematically, a randomized algorithm A (·) taking a dataset as input satisfies ε-differential privacy if for any two datasets D, D̂ differing in at most one data point, and for any set of possible outputs S ⊆ range(A ), Pr(A (D) ∈ S) ≤ exp(ε)Pr(A (D̂) ∈ S) holds.
43	15	We call two datasets differing in at most one data point as neighboring datasets.
44	32	The above definition suggests that for a sufficiently small ε, an adversary will observe almost the same output regardless of the presence (or value change) of any one individual in the dataset; this is what provides privacy protection for that individual.
45	92	Two randomizations were proposed in (Zhang & Zhu, 2017): (i) dual variable perturbation, where each node i adds a random noise to its dual variable λi(t) before updating its primal variable fi(t) using (8) in each iteration; and (ii) primal variable perturbation, where after updating primal variable fi(t), each node adds a random noise to it before broadcasting to its neighbors.
46	30	Both were evaluated for a single iteration for a fixed privacy constraint.
47	51	As we will see later in numerical experiments, the privacy loss accumulates significantly when inspected over multiple iterations.
48	15	In contrast, in this study we will explore the use of the penalty parameter η to provide privacy.
49	15	In particular, we will allow this to be private information to every node, i.e., each decides its own η in every iteration and it is not exchanged among the nodes.
50	20	Below we will begin by modifying the ADMM to accommodate private penalty terms.
51	10	Conventional ADMM (Boyd et al., 2011) requires that the penalty parameter η be fixed and equal to the dual updating step size for all nodes in all iterations.
53	23	For instance, (He et al., 2002; Magnússon et al., 2014; Aybat & Iyengar, 2015; Xu et al., 2016) vary this penalty parameter in every iteration but keep it the same for different equality constraints in (2).
