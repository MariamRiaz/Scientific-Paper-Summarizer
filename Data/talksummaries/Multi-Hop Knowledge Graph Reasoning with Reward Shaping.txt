0	19	Large-scale knowledge graphs (KGs) support a variety of downstream NLP applications such as semantic search (Berant et al., 2013) and dialogue generation (He et al., 2017).
1	34	Whether curated automatically or manually, practical KGs often fail to include many relevant facts.
2	91	A popular approach for modeling incomplete KGs is knowledge graph embeddings, which map both entities and relations in the KG to a vector space and learn a truth value function for any potential KG triple parameterized by the entity and relation vectors (Yang et al., 2014; Dettmers et al., 2018).
4	20	An alternative solution for KG reasoning is to infer missing facts by synthesizing information from multi-hop paths, e.g. bornIn(Obama, Hawaii) ∧ locatedIn(Hawaii, US) ⇒ bornIn(Obama, US), as shown in Figure 1.
5	13	Path-based reasoning offers logical insights of the underlying KG and are more directly interpretable.
7	77	More recent work formulates multi-hop reasoning as a sequential decision problem, and leverages reinforcement learning (RL) to perform effective path search (Xiong et al., 2017; Das et al., 2018; Shen et al., 2018; Chen et al., 2018).
8	27	In particular, MINERVA (Das et al., 2018) uses the REINFORCE algorithm (Williams, 1992) to train an end-to-end model for multi-hop KG query answering: given a query relation and a source entity, the trained agent searches over the KG starting from the source and arrives at the candidate answers without access to any pre-computed paths.
10	22	Walk-based QA eliminates the need to precompute path features, yet this setup poses several challenges for training.
11	19	First, because practical KGs are intrinsically incomplete, the agent may arrive at a correct answer whose link to the source entity is missing from the training graph without receiving any reward (false negative targets, Figure 2).
12	16	Second, since no ground truth path is available for training, the agent may traverse spurious paths that lead to a correct answer only incidentally (false positive paths).
13	18	Because REINFORCE (Williams, 1992) is an on-policy RL algorithm (Sutton and Barto, 1998) which encourages past actions with high reward, it can bias the policy toward spurious paths found early in training (Guu et al., 2017).
16	74	As embedding-based models capture link semantics well, unobserved but correct answers would receive a higher reward score compared to a true negative entity using a well-trained model.
17	103	Second, we perform action dropout which randomly blocks some outgoing edges of the agent at each training step so as to enforce effective exploration of a diverse set of paths and dilute the negative impact of the spurious ones.
18	159	Empirically, our overall model significantly improves over state-of-the- art multi-hop reasoning approaches on four out of five benchmark KG datasets (UMLS, Kinship, FB15k-237, WN18RR).
19	201	It is also the first pathbased model that achieves consistently comparable or better performance than embedding-based models.
21	64	In this section, we first review the walk-based QA framework (§2.2) and the on-policy reinforcement learning approach proposed by Das et al. (2018) (§2.3,§2.4).
23	29	We formally represent a knowledge graph as G = (E ,R), where E is the set of entities and R is the set of relations.
24	148	Each directed link in the knowledge graph l = (es, r, eo) ∈ G represents a fact (also called a triple).
25	10	), where es is the source entity and rq is the relation of interest, the goal is to perform an efficient search over G and collect the set of possible answers Eo = {eo} where (es, rq, eo) /∈ G due to incompleteness.
27	11	Specifically, the MDP consists of the following components (Das et al., 2018).
28	15	States Each state st = (et, (es, rq)) ∈ S is a tuple where et is the entity visited at step t and (es, rq) are the source entity and query relation.
31	20	To give the agent the option to terminat a search, a self-loop edge is added to every At.
34	34	In walk-based QA, the transition is determined by G. Rewards In the default formulation, the agent receives a terminal reward of 1 if it arrives at a correct target entity when search ends and 0 otherwise.
36	34	The search policy is parameterized using state information and global context, plus the search history (Das et al., 2018).
37	15	Specifically, every entity and relation in G is assigned a dense vector embedding e ∈ d and r ∈ d. A particular action at = (rt+1, et+1) ∈ At is represented as the concatenation of the relation embedding and the end node embedding at = [r; e′t].
41	26	And the policy network π is defined as: πθ(at|st) = σ(At ×W2 ReLU(W1[et;ht; rq])), (4) where σ is the softmax operator.
42	9	The policy network is trained by maximizing the expected reward over all queries in G: J(θ) = (es,r,eo)∈G [ a1,...,aT∼πθ [R(sT |es, r)]].
44	13	According to Equation 1, the agent receives a binary reward based solely on the observed answers in G. However, G is intrinsically incomplete and this approach penalizes the false negative search attempts identically to true negatives.
45	89	To alleviate this problem, we adopt existing KG embedding models designed for the purpose of KG completion (Trouillon et al., 2016; Dettmers et al., 2018) to estimate a soft reward for target entities whose correctness is unknown.
50	15	Here we keep f in its general form and it can be replaced by any state-of-the-art model (Trouillon et al., 2016; Dettmers et al., 2018) or ensemble thereof.
