25	21	The two models have an identical structure and trained in an identical manner with a stochastic gradient method (Bottou, 2009) except that we train the forward model using timeseries in a standard (forward) manner and the backward model using the time-series from the future to the past.
28	14	For the forward model, it is hard to estimate Mi,j for j ∈ H (i.e., M:,H).
29	27	For the backward model, it is hard to estimate Mj,i for j ∈ H (i.e., MH,: = (M >):,H).
31	113	This idea of bidirectional training for learning time-series models with hidden units constitutes our first contribution.
32	30	Here, we extend a Dynamic Boltzmann Machine (DyBM) to incorporate hidden units and apply bidirectional training to learn its parameters.
34	31	The prior work on the DyBM does not consider hidden units but instead uses various features of past values to capture the long term dependency in time-series.
39	68	With bidirectional learning, we seek to learn the weight from the past visible values to the future hidden values, which corresponds to learning what features of past values are effective for prediction.
63	24	The hidden part h[t−δ] represents the values of hidden units at time t − δ.
66	16	Let θ ≡ (V,W,b) be the parameters connected to visible units x[t] (from the units in the past, x[s] or h[s] for s < t) and φ ≡ (U,Z).
70	18	(6) The energy in (5) gives the conditional probability distribution over x[t] given x[<t] and h[<t].
73	16	For real-valued time-series, one can define the conditional density pi(x [t] i |x[<t],h[<t]) with a Gaussian distribution whose mean is given from the energy associated with unit i (Dasgupta & Osogami, 2017; Osogami, 2016).
77	14	Our DyBM with binary hidden units gives the probability of a time-series, x ≡ (x[t])t=`,...,u, by pθ,φ(x) = ∑ h̃ pφ(h̃|x) u∏ t=` pθ(x [t]|x[<t], h̃[<t]) (9) where ∑ h̃ denotes the summation over all of the possible values of hidden units from time t = ` to t = u, and pφ(h̃|x) ≡ u∏ s=` pφ(h̃ [s]|x[<s], h̃[<s]), (10) where pφ(h̃[s]|x[<s], h̃[<s]) is defined analogously to (7)– (8) and provided in (36) of the supplementary material, and we arbitrarily define x[s] = 0 and h̃[s] = 0 for s < `.
81	22	Namely, at each step t, we sample h[t−1] according to pφ(h [t−1]|x[<t−1],h[<t−1]) and update θ on the basis of ∇θ log pθ(x[t]|x[<t],h[<t]).
86	27	(26) Similar to (16), the expression of (26) suggests a method of stochastic gradient: at each time t, we sample h[t−1] according to pφ(h[t−1]|x[<t−1],h[<t−1]) and update φ on the basis of the following stochastic gradient: log pθ(x [t]|x[<t],h[<t])Gt−1, (27) where Gt−1 ≡ t−1∑ s=` ∇φ log pφ(h[s]|x[<s],h[<s]).
87	46	(28) Computation of (26) involves mainly two interrelated inefficiencies.
89	18	Second, since each summand of Gt−1 is dependent on φ, Gt−1 also has to be recomputed after each update.
90	14	Thus, the computational complexity of (27) grows linearly with respect to the length of the time-series (i.e., t− `), in contrast to (17), whose complexity is independent of that length.
107	23	Recall that φ and φ′ are relatively hard to optimize, and θ and θ′ are relatively easy to optimize.
109	16	By training both the forward DyBM and the backward DyBM, we expect to effectively find appropriate values of θ and θ′.
144	15	We first demonstrate the effectiveness of our bidirectional training in a synthetic setting of learning a one-dimensional noisy sawtooth wave, which is generated according to x[t] = t C − ⌊ t C ⌋ + εt, for t = 0, 1, .
161	27	Notice also that the hidden unit can hurt the predictive accuracy without bidirectional training.
184	17	Here we train a DyBM with four hidden units or no hidden units.
205	25	We have proposed bidirectional training for time-series models with hidden units.
208	19	Numerical experiments suggest that bidirectional training has the additional effect of avoiding overfit to training data.
211	18	The second highlight is that we cannot learn the weight to hidden units (φ = (U,Z)) in the same way as the weight to visible units (θ = (V,W,b)) due to the form of the gradient in (27).
214	36	For example, one might want to use the gradients in (27)-(28), possibly with the approximation in (29), to update (U,Z) in forward learning and (V,Z) in backward learning.
215	46	It would also be interesting to apply bidirectional training to other time-series models having parameters that represent the dependency between hidden values at one time and visible values at another time.
216	28	In addition to DyBM and VAR with hidden units, these include Spiking Boltzmann Machine (Hinton & Brown, 1999) and Conditional Restricted Boltzmann Machine (Taylor et al., 2007).
217	76	Because bidirectional training is largely complementary to other techniques for learning time-series, it would be interesting to investigate how bidirectional training improves performance when it is combined with these other techniques.
218	37	We expect that this work opens up a line of research on more effective methods of bidirectional training.
