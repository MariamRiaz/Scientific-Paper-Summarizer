1	14	The alignment is often done sequentially, one step at a time.
2	16	We propose a neural network architecture, capable of aligning two input sequences globally and at once.
64	22	Since the source code is translated to object code during compilation, there is a well-defined alignment between them, which is known to the compiler.
65	30	GCC outputs this information when it runs with a debug configuration.
66	24	In the GCC alignment output, the statement level alignment between source- and object-code is a many-to-one map from object code statements to source code statements: while every object-code statement is aligned to some source-code statement, not all source-code statements are covered.
73	11	1, which depicts both the source code of a single C language function, which contains M = 10 statements, and the compiled object code of this function, which contains N = 14 statements.
75	24	Each column (row) of this matrix corresponds to one source (object) code statement.
85	53	Another dimension in which we challenge our alignment network, is compilation optimization, which drastically changes the object code based on the level of optimization used (see Fig.
123	27	The result of the LSTM encoders areM representation vectors output by the source-code encoding LSTM, denoted by {vj}j∈(1,...,M), and N representation vectors output by the object-code encoding LSTM, denoted by {ui}i∈(1,...,N).
124	16	The statement representation vectors are then assembled in anN×M grid, such that the (i, j) element is [ui; vj ], where ; denotes vector concatenation.
128	38	The CNN output is, therefore, a single channel N ×M grid, s(i, j), representing the alignment score of object code statement i and source code statement j.
131	14	, s(ui, vM ), i.e., there are N softmax layers, each converting M alignment scores to a vector of probabilities {pij}j∈(1,...,M) for each row i ∈ (1, .
136	18	In this model, the decoder consists only of a single layer network s attached to each one of the NM pairs of object code and source code statement representations (ui and vj).
139	12	We consider another, simpler version of the Local Grid Decoder, where s(ui, vj) = uTi · vj , i.e., an inner product operation is employed, instead of the single layer network.
147	19	A second decoder LSTM then produces hidden states that are used to point to locations in the input sequence via an attention mechanism.
153	25	Since in the alignment problem we need to align each object code statement to one of the source code statements, we adapt Ptr-Net to produce ”pointers” to the source code statements sequence for every object code statement.
168	12	The architecture receives as inputs two sentences, a premise and a hypothesis.
171	11	For further details about the Match-LSTM architecture, see (Wang & Jiang, 2015).
172	33	In order to adapt Match-LSTM to our problem, we simply substitute the premise (hypothesis) representation sequence with the source (object) code statements representation sequence, and use the matching scores s(i, j) as the alignment scores.
173	15	Data collection We employ both synthetic C functions generated randomly and human-written C functions from real-world projects.
174	13	In order to generate random C functions, we used pyfuzz, an open-source random program generator for python (Myint, 2013), and modified it so it will output short functions written in C rather than python.
183	13	Therefore, each sample in the resulting dataset consists of source code, object code compiled at some optimization level and the statement-by-statement alignment between them.
201	37	2, all models excel over synthetic code, reaching almost perfect alignment accuracy with a slight advantage to our Convolutional and Local Grid Decoders.
208	20	The output is a sequence of all the points reordered, such that the route length (sum of distance between adjacent points) is minimal.
216	31	5 depicts an example route and its connectivity matrix before and after permutation of the node IDs.
220	31	We present a neural network architecture for aligning two sequences.
221	22	We challenge our network with aligning source code to its compiled object code, sequences that in some aspects are more complex than human language sentences.
222	15	Our experiments demonstrate that the proposed architecture is successful in predicting the alignment.
223	90	On this task, the network outperforms multiple literature baselines such as Pointer Networks and Match-LSTM, suggesting that a global, CNN-based approach to alignment is better than the sequential, RNN-based approach.
224	38	Our model can be used for alignment of any two sequences with a many-to-one map between them, and extended to other graph problems, as demonstrated for TSP.
