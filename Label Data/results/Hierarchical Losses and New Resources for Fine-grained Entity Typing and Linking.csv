0,1,label2,summary_sentences
"ar X
iv :1
80 2.
06 09
3v 4
[ cs
.L G",text,[0],[0]
"Residual networks (He et al., 2016) are deep neural networks in which, roughly, subnetworks determine how a feature transformation should differ from the identity, rather than how it should differ from zero.",1 Introduction,[0],[0]
"After enabling the winning entry in the ILSVRC 2015 classification task, they have become established as a central idea in deep networks.
",1 Introduction,[0],[0]
Hardt & Ma (2017) provided a theoretical analysis that shed light on residual networks.,1 Introduction,[0],[0]
"They showed that (a) any linear transformation with a positive determinant and a bounded condition number can be approximated by a “deep linear network” of the form f(x) = ΘLΘL−1...Θ1x, where,
for large L, each layer Θi is close to the identity, and (b) for networks that compose near-identity transformations this way, if the excess loss is large, then the gradient is steep.",1 Introduction,[0],[0]
"Bartlett et al. (2018) extended both results to the nonlinear case, showing that any smooth, bi-Lipschitz map can be represented as a composition of near-identity functions, and that a suboptimal loss in a composition of near-identity functions implies that the functional gradient of the loss with respect to a function in the composition cannot be small.",1 Introduction,[0],[0]
"These results are interesting because they suggest that, in many cases, this non-convex objective may be efficiently optimized through gradient descent if the layers stay close to the identity, possibly with the help of a regularizer.
",1 Introduction,[0],[0]
"This paper describes and analyzes such algorithms for linear regression with d input variables and d response variables with respect to the quadratic loss, the same setting analyzed by Hardt and Ma.",1 Introduction,[0],[0]
We abstract away sampling issues by analyzing an algorithm that performs gradient descent with respect to the population loss.,1 Introduction,[0],[0]
We focus on the case that the distribution on the input patterns is isotropic.,1 Introduction,[0],[0]
(The data may be transformed through a preprocessing step to satisfy this constraint.),1 Introduction,[0],[0]
"
",1 Introduction,[0],[0]
"The traditional analysis of convex optimization algorithms (see Boyd & Vandenberghe, 2004) provides a bound in terms of the quality of the initial solution, together with bounds on the eigenvalues of the Hessian of the loss.",1 Introduction,[0],[0]
"For the non-convex problem of this paper, we show that if gradient descent starts at the identity in each layer, and if the excess loss of that initial solution is bounded by a constant, then the Hessian remains well-conditioned enough throughout training for successful learning.",1 Introduction,[0],[0]
"Specifically, there is a constant c0 such that, if the excess loss of the identity (over the least squares linear map) is at most c0, then back-propagation initialized at the identity in each layer achieves loss within at most ǫ of optimal in time polynomial in log(1/ǫ), d, and L (Section 3).",1 Introduction,[0],[0]
"On the other hand, we show that there is a constant c1 and a least squares matrix Φ such that the identity has excess loss c1 with respect to Φ, but backpropagation with identity initialization fails to learn Φ (Section 6).
",1 Introduction,[0],[0]
"We also show that if the least squares matrix Φ is symmetric positive definite then gradient descent with identity initialization achieves excess loss at most ǫ in a number of steps bounded by a polynomial in log(d/ǫ), L and the condition number of Φ (Section 4).
",1 Introduction,[0],[0]
"In contrast, for any least squares matrix Φ that is symmetric but has a negative eigenvalue, we show that no such guarantee is possible for a wide variety of algorithms of this type: the excess loss is forever bounded below by the square of this negative eigenvalue.",1 Introduction,[0],[0]
"This holds for step-and-project algorithms, and also algorithms that initialize to the identity and regularize by early stopping or penalizing ∑
i ||Θi − I|| 2 F (Section 6).",1 Introduction,[0],[0]
"Both this and the previous impossibility result can be
proved using a least squares matrix Φ with a positive determinant and a good condition number.",1 Introduction,[0],[0]
"Recall that such Φ were proved by Hardt and Ma to have a good approximation as a product of near-identity matrices – we prove that gradient descent cannot learn them, even with the help of regularizers that reward near-identity representations.
",1 Introduction,[0],[0]
"In Section 5 we provide a convergence guarantee for a least squares matrix Φ that may not be symmetric, but satisfies the positivity condition u⊤Φu",1 Introduction,[0],[0]
> γ for some γ > 0,1 Introduction,[0],[0]
that appears in the bounds.,1 Introduction,[0],[0]
We call such matrices γ-positive.,1 Introduction,[0],[0]
Such Φ include rotations by acute angles.,1 Introduction,[0],[0]
"In this case, we consider an algorithm that regularizes in addition to a near-identity initialization.",1 Introduction,[0],[0]
"After the gradient update, the algorithm performs what we call power projection, projecting its hypothesis ΘLΘL−1...Θ1 onto the set of γ-positive matrices.",1 Introduction,[0],[0]
"Second, it “balances” Θ1, ...,ΘL so that, informally, they contribute equally to ΘLΘL−1...Θ1.",1 Introduction,[0],[0]
(See Section 5 for the details.),1 Introduction,[0],[0]
"We view this
regularizer as a theoretically tractable proxy for regularizers that promote positivity and balance between layers by adding penalties.
",1 Introduction,[0],[0]
"While, in practice, deep networks are non-linear, analysis of the linear case can provide a tractable way to gain insight through rigorous theoretical analysis (Saxe et al., 2013; Kawaguchi, 2016; Hardt & Ma, 2017).",1 Introduction,[0],[0]
We might view back-propagation in the non-linear case as an approximation to a procedure that locally modifies the function computed by each layer in a manner that reduces the loss as fast as possible.,1 Introduction,[0],[0]
"If a non-linear network is obtained by composing transformations, each of which is chosen from a Hilbert space of functions (as in Daniely et al. (2016)), then a step in “function space” corresponds to a step in an (infinite-dimensional) linear space of functions.
",1 Introduction,[0],[0]
Related work.,1 Introduction,[0],[0]
The motivation for this work comes from the papers of Hardt & Ma (2017) and Bartlett et al. (2018).,1 Introduction,[0],[0]
Saxe et al. (2013) studied the dynamics of a continuous-time process obtained by taking the step size of backpropagation applied to deep linear neural networks to zero.,1 Introduction,[0],[0]
Kawaguchi (2016) showed that deep linear neural networks have no suboptimal local minima.,1 Introduction,[0],[0]
"In the case that L = 2, the problem studied here has a similar structure as problems arising from low-rank approximation of matrices, especially as regards algorithms that approximate a matrix A by iteratively improving an approximation of the form UV .",1 Introduction,[0],[0]
"For an interesting survey on the rich literature on these algorithms, please see Ge et al. (2017a); successful algorithms have included a regularizer that promotes balance in the sizes of U and V .",1 Introduction,[0],[0]
"Taghvaei et al. (2017) studied the properties of critical points on the loss when learning deep linear neural networks in the presence of a weight decay regularizer; they studied networks that transform the input to the output through a process indexed by a continuous variable, instead of through discrete layers.",1 Introduction,[0],[0]
"Lee et al. (2016) showed that, given regularity conditions, for a random initialization, gradient descent converges to a local minimizer almost surely; while their paper yields useful insights, their regularity condition does not hold for our problem.",1 Introduction,[0],[0]
Many papers have analyzed learning of neural networks with non-linearities.,1 Introduction,[0],[0]
The papers most closely related to this work analyze algorithms based on gradient descent.,1 Introduction,[0],[0]
"Some of these (Andoni et al., 2014; Brutzkus & Globerson, 2017; Ge et al., 2017b; Li & Yuan, 2017; Zhong et al., 2017; Zhang et al., 2018; Brutzkus et al., 2018; Ge et al., 2018) analyze constant-depth networks.",1 Introduction,[0],[0]
Daniely (2017) showed that stochastic gradient descent learns a subclass of functions computed by log-depth networks in polynomial time; this class includes constant-degree polynomials with polynomially bounded coefficients.,1 Introduction,[0],[0]
"Other theoretical treatments of neural network learning algorithms include Lee et al. (1996); Arora et al. (2014); Livni et al. (2014); Janzamin et al. (2015); Safran & Shamir (2016); Zhang et al. (2016); Nguyen & Hein (2017); Zhang et al. (2017); Orhan & Pitkow (2018), although these are less closely related.
",1 Introduction,[0],[0]
Our three upper bound analyses combine a new upper bound on the operator norm of the Hessian of a deep linear network with the result of Hardt and Ma that gradients are lower bounded in terms of the loss for near-identity matrices.,1 Introduction,[0],[0]
They otherwise have different outlines.,1 Introduction,[0],[0]
The bound in terms of the loss of the initial solution proceeds by showing that the distance from each layer to the identity grows slowly enough that the loss is reduced before the layers stray far enough to harm the conditioning of the Hessian.,1 Introduction,[0],[0]
"The bound for symmetric positive definite matrices proceeds by showing that, in this case, all of the layers are the same, and each of their eigenvalues converges to the Lth root of a corresponding eigenvalue of Φ. As mentioned above, the bound for γ-positive matrices Φ is for an algorithm that achieves favorable conditioning through regularization.
",1 Introduction,[0],[0]
"We expect that the theoretical analysis reported here will inform the design of practical algorithms
for learning non-linear deep networks.",1 Introduction,[0],[0]
One potential avenue for this arises from the fact that the leverage provided by regularizing toward the identity appears to already be provided by a weaker policy of promoting the property that the composition of layers is (potentially asymmetric) positive definite.,1 Introduction,[0],[0]
"Also, balancing singular values of the layers of the network aided our analysis; an analogous balancing of Jacobians associated with various layers may improve conditioning in practice in the non-linear case.",1 Introduction,[0],[0]
"For a joint distribution P with support contained in ℜd × ℜd and g : ℜd → ℜd, define ℓP (g) = E(X,Y )∼P (||g(X)",2.1 Setting,[0],[0]
− Y || 2/2).,2.1 Setting,[0],[0]
"We focus on the case that, for (X,Y ) drawn from P , the marginal on X is isotropic, with",2.1 Setting,[0],[0]
EXX⊤ = Id.,2.1 Setting,[0],[0]
"For convenience, we assume that Y = ΦX for Φ ∈ ℜd×d.",2.1 Setting,[0],[0]
This assumption is without loss of generality: if Φ is the least squares matrix (so that f defined by f(X) = ΦX minimizes ℓP,2.1 Setting,[0],[0]
"(f) among linear functions), for any linear g we have
ℓP (g) =",2.1 Setting,[0],[0]
E‖g(X),2.1 Setting,[0],[0]
− f(X)‖ 2/2 + E‖f(X)−,2.1 Setting,[0],[0]
"Y ‖2/2
+ E",2.1 Setting,[0],[0]
((g(X),2.1 Setting,[0],[0]
− f(X))(f(X),2.1 Setting,[0],[0]
"− Y ))
= E‖g(X)",2.1 Setting,[0],[0]
− f(X)‖2/2 +,2.1 Setting,[0],[0]
"E‖f(X)− Y ‖2/2
= E‖g(X)",2.1 Setting,[0],[0]
− ΦX)‖2/2 + E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2,
since f is the projection of Y onto the set of linear functions ofX.",2.1 Setting,[0],[0]
"So assuming Y = ΦX corresponds to setting Φ as the least squares matrix and replacing the loss ℓP (g) by the excess loss
E‖g(X)",2.1 Setting,[0],[0]
− ΦX‖2/2 = E‖g(X),2.1 Setting,[0],[0]
− Y ‖2/2−,2.1 Setting,[0],[0]
E‖ΦX,2.1 Setting,[0],[0]
"− Y ‖2/2.
",2.1 Setting,[0],[0]
We study algorithms that learn linear mappings parameterized by deep networks.,2.1 Setting,[0],[0]
"The network with L layers and parameters Θ = (Θ1, . . .",2.1 Setting,[0],[0]
",ΘL) computes the parameterized function fΘ(x) = ΘLΘL−1 · · ·Θ1x, where",2.1 Setting,[0],[0]
"x ∈ ℜd and Θi ∈ ℜd×d.
",2.1 Setting,[0],[0]
"We use the notation Θi:j = ΘjΘj−1 · · ·Θi for i ≤ j, so that we can write fΘ(x) = Θ1:Lx = Θi+1:LΘiΘ1:i−1x.
",2.1 Setting,[0],[0]
"When there is no possibility of confusion, we will sometimes refer to loss ℓ(fΘ) simply as ℓ(Θ).",2.1 Setting,[0],[0]
"Because the distribution of X is isotropic, ℓ(Θ) = 12 ||Θ1:L − Φ|| 2 F with respect to least squares matrix Φ. When Θ is produced by an iterative algorithm, will we also refer to loss of the tth iterate by ℓ(t).
",2.1 Setting,[0],[0]
Definition 1.,2.1 Setting,[0],[0]
"For γ > 0, a matrix A ∈ ℜd×d is γ-positive if, for all unit length u, we have u⊤Au > γ.",2.1 Setting,[0],[0]
"We use ||A||F for the Frobenius norm of matrix A, ||A||2 for its operator norm, and σmin(A) for its least singular value.",2.2 Tools and background,[0],[0]
"For vector v, we use ||v|| for its Euclidian norm.
",2.2 Tools and background,[0],[0]
"For a matrix A and a matrix-valued function B, define DAB(A) to be the matrix with
(DAB(A))i,j = ∂vec(B(A))i ∂vec(A)j ,
where vec(A) is the column vector constructed by stacking the columns of A. We use Td,d to denote the d2 × d2 permutation matrix mapping vec(A) to vec(A⊤) for A ∈ ℜd×d.",2.2 Tools and background,[0],[0]
"For A ∈ ℜn×m and B ∈ ℜp×q, A⊗B denotes the Kronecker product, that is, the np×mq matrix of n×m blocks, with the i, jth block given by AijB.
We will need the gradient and Hessian of ℓ. (The gradient, which can be computed using backprop, is of course well known.)",2.2 Tools and background,[0],[0]
"The proof is in Appendix A.
Lemma 1.
DΘiℓ (fΘ)=(vec(Id))",2.2 Tools and background,[0],[0]
"⊤ (( Θ⊤1:i−1 ⊗ (Θ1:L−Φ) ⊤Θi+1:L ))
",2.2 Tools and background,[0],[0]
"= vec(G)⊤,
where G is the d× d matrix given by
G def = Θ⊤i+1:",2.2 Tools and background,[0],[0]
"L (Θ1:L − Φ)Θ ⊤ 1:i−1. (1)
",2.2 Tools and background,[0],[0]
"For i < j,
DΘjDΘiℓ (fΘ) =",2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
",2.2 Tools and background,[0],[0]
"((Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1)Td,d + (Θ ⊤ i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L)).
DΘiDΘiℓ (fΘ)",2.2 Tools and background,[0],[0]
=,2.2 Tools and background,[0],[0]
(Id2 ⊗ (vec(Id)) ⊤),2.2 Tools and background,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",2.2 Tools and background,[0],[0]
"Td,d.",2.2 Tools and background,[0],[0]
"In this section, we prove an upper bound for gradient descent in terms of the loss of the initial solution.",3 Targets near the identity,[0],[0]
"First, set Θ(0) =",3.1 Procedure and upper bound,[0],[0]
"(I, I, ..., I), and then iteratively update
Θ (t+1)",3.1 Procedure and upper bound,[0],[0]
i = Θ,3.1 Procedure and upper bound,[0],[0]
(t),3.1 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",3.1 Procedure and upper bound,[0],[0]
"L − Φ
)
",3.1 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
Theorem 1.",3.1 Procedure and upper bound,[0],[0]
"There are positive constants c1 and c2 and polynomials p1 and p2 such that, if ℓ(Θ (0) 1:L) ≤ c1, L ≥ c2, and η ≤ 1 p1(L,d,||Φ||2) , then the above gradient descent procedure achieves
ℓ(fΘ(t)) ≤ ǫ within t = p2
(
1 η
)",3.1 Procedure and upper bound,[0],[0]
"ln (
ℓ(0) ǫ
)
iterations.",3.1 Procedure and upper bound,[0],[0]
"The following lemma, which is implicit in the proof of Theorem 2.2 in Hardt & Ma (2017), shows that the gradient is steep if the loss is large and the singular values of the layers are not too small.
",3.2 Proof of Theorem 1,[0],[0]
Lemma 2 (Hardt & Ma 2017).,3.2 Proof of Theorem 1,[0],[0]
Let ∇Θℓ(Θ) be the gradient of ℓ(Θ) with respect to any flattening of Θ.,3.2 Proof of Theorem 1,[0],[0]
"If, for all layers i, σmin(Θi) ≥ 1− a, then ||∇Θℓ(Θ)|| 2 ≥ 4ℓ(Θ)L(1− a)2L.
Next, we show that, if Θ(t) and Θ(t+1) are both close to the identity, then the gradient is not changing very fast between them, so that rapid progress continues to be made.",3.2 Proof of Theorem 1,[0],[0]
"We prove this through an upper bound on the operator norm of the Hessian that holds uniformly over members of a ball around the identity, which in turn can be obtained through a bound on the Frobenius norm.",3.2 Proof of Theorem 1,[0],[0]
"The proof is in Appendix B.
Lemma 3.",3.2 Proof of Theorem 1,[0],[0]
"Choose an arbitrary Θ with ||Θi||2 ≤ 1 + z for all i, and least squares matrix Φ with ||Φ||2 ≤ (1 + z)
L.",3.2 Proof of Theorem 1,[0],[0]
"Let ∇2 be the Hessian of ℓ(fΘ) with respect to an arbitrary flattening of the parameters of Θ. We have
||∇2||F ≤ 3Ld 5(1 + z)2L.
Armed with Lemmas 2 and 3, let us now analyze gradient descent.",3.2 Proof of Theorem 1,[0],[0]
"Very roughly, our strategy will be to show that the distance from the identity to the various layers grows slowly enough for the leverage from Lemmas 2 and 3 to enable successful learning.",3.2 Proof of Theorem 1,[0],[0]
Let R(Θ) = maxi ||Θi − I||2.,3.2 Proof of Theorem 1,[0],[0]
"From the update, we have
||Θ (t+1)",3.2 Proof of Theorem 1,[0],[0]
i − I||2 ≤ ||Θ (t) i,3.2 Proof of Theorem 1,[0],[0]
"− I||2 + η||(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤||2
≤ ||Θ (t) i",3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(Θ (t)))L||Θ (t) 1:L − Φ||2 ≤ ||Θ,3.2 Proof of Theorem 1,[0],[0]
(t),3.2 Proof of Theorem 1,[0],[0]
i,3.2 Proof of Theorem 1,[0],[0]
− I||2,3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
"+R(Θ (t)))L||Θ (t) 1:L − Φ||F .
",3.2 Proof of Theorem 1,[0],[0]
If R(t) = maxs≤tR(Θ(s)),3.2 Proof of Theorem 1,[0],[0]
(so R(0) = 0) and ℓ(t) = 12 ||Θ (t) 1:,3.2 Proof of Theorem 1,[0],[0]
"L − Φ|| 2 F , this implies
R(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
+ η(1,3.2 Proof of Theorem 1,[0],[0]
+R(t))L √ 2ℓ(t).,3.2 Proof of Theorem 1,[0],[0]
"(2)
By Lemma 3, for all Θ on the line segment from Θ(t) to Θ(t+1), we have
||∇2Θ||2 ≤ ||∇ 2 Θ||F ≤",3.2 Proof of Theorem 1,[0],[0]
"3Ld 5 max{(1 +R(t+ 1))2L, ||Φ||22},
so that
ℓ(t+ 1) ≤ ℓ(t)− η||∇Θ(t)",3.2 Proof of Theorem 1,[0],[0]
||,3.2 Proof of Theorem 1,[0],[0]
"2 +
3 2",3.2 Proof of Theorem 1,[0],[0]
η2Ld5 max{(1,3.2 Proof of Theorem 1,[0],[0]
"+R(t+ 1))2L, ||Φ||22}||∇Θ(t) || 2.
",3.2 Proof of Theorem 1,[0],[0]
"Thus, if we ensure
η ≤ 1
3Ld5 max{(1 +R(t+ 1))2L, ||Φ||22} , (3)
we have ℓ(t+ 1) ≤ ℓ(t)− (η/2)||∇Θ(t) || 2, which, using Lemma 2, gives
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).",3.2 Proof of Theorem 1,[0],[0]
"(4)
Pick any c ≥ 1.",3.2 Proof of Theorem 1,[0],[0]
"Assume that L ≥ (4/3) ln c = c2, ℓ(Θ (0) 1:L) ≤
ln(c)2
8c10 = c1 and η ≤ 1
3Ld5 max{c4,||Φ||22} .
",3.2 Proof of Theorem 1,[0],[0]
"We claim that, for all t ≥ 0,
1. R(t) ≤ ηc √ 2ℓ(0) ∑ 0≤s<t exp ( − sηLc4 )
2.",3.2 Proof of Theorem 1,[0],[0]
"ℓ(t) ≤ ( exp (
−2tηL c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"The base case holds as R(0) = 0 and ℓ(0) = ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Before starting the inductive step, notice that for any t ≥ 0,
ηc √ 2ℓ(0) ∑
0≤s<t exp
(
− sηL
c4
)
≤ ηc √ 2ℓ(0)× 1
1− exp (
−ηL c4
)
≤ ηc √ 2ℓ(0)× 2c4
ηL (since ηLc4 ≤ 1)
= 2c5 √ 2ℓ(0)
L ≤
ln c
L ≤ 3/4
where the last two inequalities follow from the constraints on ℓ(0) and L.
Using (2),
R(t+ 1) ≤ R(t) + η(1",3.2 Proof of Theorem 1,[0],[0]
"+R(t))L √ 2ℓ(t)
≤",3.2 Proof of Theorem 1,[0],[0]
R(t),3.2 Proof of Theorem 1,[0],[0]
"+ η
(
1 + ln c
L
)L √
2ℓ(t)
≤ R(t) + ηc √ 2ℓ(t)
≤ R(t) + ηc √ 2ℓ(0) exp
(
− tηL
c4
)
≤ ηc √ 2ℓ(0)",3.2 Proof of Theorem 1,[0],[0]
"∑
0≤s<t+1 exp
(
− sηL
c4
)
.
",3.2 Proof of Theorem 1,[0],[0]
"Since R(t+ 1) ≤ ln cL , the choice of η satisfies (3), so
ℓ(t+ 1) ≤ ( 1− 2ηL(1 −R(t))2L ) ℓ(t).
",3.2 Proof of Theorem 1,[0],[0]
"Now consider (1−R(t))2L:
ln ( (1−R(t))2L )",3.2 Proof of Theorem 1,[0],[0]
"= 2L ln(1−R(t))
",3.2 Proof of Theorem 1,[0],[0]
≥ 2L(−2R(t)),3.2 Proof of Theorem 1,[0],[0]
since R(t) ∈,3.2 Proof of Theorem 1,[0],[0]
"[0, 3/4]
≥ 2L
(
−2 ln c
L
)
since R(t) ≤",3.2 Proof of Theorem 1,[0],[0]
ln,3.2 Proof of Theorem 1,[0],[0]
"c
L
(1−R(t))2L ≥ 1/c4.
",3.2 Proof of Theorem 1,[0],[0]
"Using this in the bound on ℓ(t+ 1):
ℓ(t+ 1) ≤",3.2 Proof of Theorem 1,[0],[0]
"( 1− 2ηL(1 −R(t))2L ) ℓ(t)
≤
(
1− 2ηL
c4
)
ℓ(t)
≤
(
exp
(
− 2ηL
c4
))",3.2 Proof of Theorem 1,[0],[0]
"(
exp
(
− 2tηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0)
",3.2 Proof of Theorem 1,[0],[0]
"=
(
exp
(
− 2(t+ 1)ηL
c4
))
",3.2 Proof of Theorem 1,[0],[0]
"ℓ(0).
",3.2 Proof of Theorem 1,[0],[0]
"Solving ℓ(0) exp (
−2tηL c4
)
≤ ǫ for t and recalling that η < 1/c4 completes the proof of the theorem.",3.2 Proof of Theorem 1,[0],[0]
"In this section, we analyze the procedure of Section 3.1 when the least squares matrix Φ is symmetric and positive definite.
Theorem 2.",4 Symmetric positive definite targets,[0],[0]
"There is an absolute positive constant c3 such that, if Φ is symmetric and γ-positive with 0 <",4 Symmetric positive definite targets,[0],[0]
"γ < 1, and L ≥ c3 ln (||Φ||2/γ), then for all η ≤
1 L(1+||Φ||22) , gradient descent achieves
ℓ(fΘ(t))",4 Symmetric positive definite targets,[0],[0]
"≤ ǫ in poly(L, ||Φ||2/γ, 1/η) log(d/ǫ) iterations.
",4 Symmetric positive definite targets,[0],[0]
Note that a symmetric matrix is γ-positive when its minimum eigenvalue is at least γ.,4 Symmetric positive definite targets,[0],[0]
"Let Φ be a symmetric, real, γ-positive matrix with γ > 0, and let Θ(0),Θ(1), ... be the iterates of gradient descent with a step size 0 < η ≤ 1
L(1+||Φ||22) .
",4.1 Proof of Theorem 2,[0],[0]
Definition 2.,4.1 Proof of Theorem 2,[0],[0]
"Symmetric matrices A ⊆ ℜd×d are commuting normal matrices if there is a single unitary matrix U such that for all A ∈ A, U⊤AU is diagonal.
",4.1 Proof of Theorem 2,[0],[0]
"We will use the following well-known facts about commuting normal matrices.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 4 (Horn & Johnson 2013),4.1 Proof of Theorem 2,[0],[0]
.,4.1 Proof of Theorem 2,[0],[0]
"If A ⊆ ℜd×d is a set of symmetric commuting normal matrices and A,B ∈ A, the following hold:
• AB = BA;
",4.1 Proof of Theorem 2,[0],[0]
"• for all scalars α and β, A ∪ {αA+ βB,AB} are commuting normal;
• there is a unitary matrix U such that U⊤AU and U⊤BU are real and diagonal;
• the multiset of singular values of A is the same as the multiset of magnitudes of its eigenvalues;
• ||A− I||2 is the largest value of |z",4.1 Proof of Theorem 2,[0],[0]
"− 1| for an eigenvalue z of A.
Lemma 5.",4.1 Proof of Theorem 2,[0],[0]
"The matrices {Φ} ∪ {Θ (t) i : i ∈ {1, ..., L}, t ∈ Z +} are commuting normal.",4.1 Proof of Theorem 2,[0],[0]
"For all t, Θ (t) 1 = ... = Θ (t) L .
Proof.",4.1 Proof of Theorem 2,[0],[0]
The proof is by induction.,4.1 Proof of Theorem 2,[0],[0]
"The base case follows from the fact that Φ and I are commuting normal.
",4.1 Proof of Theorem 2,[0],[0]
"For the induction step, the fact that
{Φ} ∪ {
Θ (s)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
} ∪ {
Θ (s+1)",4.1 Proof of Theorem 2,[0],[0]
i :,4.1 Proof of Theorem 2,[0],[0]
"i ∈ {1, ..., L}, s ≤ t
}
are commuting normal follows from Lemma 4.",4.1 Proof of Theorem 2,[0],[0]
"The update formula now reveals that Θ (t+1) 1 = ... = Θ (t+1) L .
",4.1 Proof of Theorem 2,[0],[0]
Now we are ready to analyze the dynamics of the learning process.,4.1 Proof of Theorem 2,[0],[0]
"Let Φ = U⊤DLU be a diagonalization of Φ. Let Γ = max{1, ||Φ||2}.",4.1 Proof of Theorem 2,[0],[0]
"We next describe a sense in which gradient descent learns each eigenvalue independently.
",4.1 Proof of Theorem 2,[0],[0]
Lemma 6.,4.1 Proof of Theorem 2,[0],[0]
"For each t, there is a real diagonal matrix D̂(t) such that, for all i, Θ (t)",4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U and
D̂(t+1) = D̂(t)",4.1 Proof of Theorem 2,[0],[0]
− η(D̂(t))L−1((D̂(t))L −DL).,4.1 Proof of Theorem 2,[0],[0]
"(5)
Proof.",4.1 Proof of Theorem 2,[0],[0]
Lemma 5 implies that there is a single real U such that Θ (t),4.1 Proof of Theorem 2,[0],[0]
"i = U ⊤D̂(t)U for all i. Applying Lemma 1, recalling that Θ (t) 1 = ...",4.1 Proof of Theorem 2,[0],[0]
"= Θ (t) L , and applying the fact that Θ (t) i and Φ commute, we get
Θ (t+1)",4.1 Proof of Theorem 2,[0],[0]
i = Θ,4.1 Proof of Theorem 2,[0],[0]
(t),4.1 Proof of Theorem 2,[0],[0]
"i − η(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L−1 (
(Θ (t) i )
",4.1 Proof of Theorem 2,[0],[0]
"L − Φ ) .
",4.1 Proof of Theorem 2,[0],[0]
"Replacing each matrix by its diagonalization, we get
U⊤D̂(t+1)U = U⊤D̂(t)U",4.1 Proof of Theorem 2,[0],[0]
"− η(U⊤(D̂(t))L−1U) ( U⊤(D̂(t))LU − U⊤DLU )
",4.1 Proof of Theorem 2,[0],[0]
"= U⊤D̂(t)U − ηU⊤(D̂(t))L−1 ( (D̂(t))L −DL ) U,
and left-multiplying by U and right-multiplying by U⊤ gives (5).
",4.1 Proof of Theorem 2,[0],[0]
We will now analyze the convergence of each D̂ (t) kk to Dkk separately.,4.1 Proof of Theorem 2,[0],[0]
"Let us focus for now on an arbitrary single index k, let λ = Dkk and λ̂",4.1 Proof of Theorem 2,[0],[0]
(t) =,4.1 Proof of Theorem 2,[0],[0]
"D̂ (t) kk .
",4.1 Proof of Theorem 2,[0],[0]
Recalling that ||Φ||2 ≤,4.1 Proof of Theorem 2,[0],[0]
"Γ, we have γ 1/L ≤ λ ≤",4.1 Proof of Theorem 2,[0],[0]
"Γ1/L. Also, Γ1/L = e 1 L ln Γ ≤ e1/a ≤ 1+2/a whenever a ≥ 1 and L ≥ a ln Γ. Similarly, γ1/L ≥ 1 − a whenever L ≥ a ln(1/γ).",4.1 Proof of Theorem 2,[0],[0]
"Thus, there are absolute constants c3 and c4 such that",4.1 Proof of Theorem 2,[0],[0]
|1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ)
L < 1 for all L ≥ c3 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"We claim that, for all t, λ̂(t) lies between 1 and λ inclusive, so that |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
The base case holds because λ̂(t) = 1 and |1,4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ c4 ln(Γ/γ)L .,4.1 Proof of Theorem 2,[0],[0]
Now let us work on the induction step.,4.1 Proof of Theorem 2,[0],[0]
"Applying (5) together with Lemma 1, we get
λ̂(t+1) = λ̂(t) +",4.1 Proof of Theorem 2,[0],[0]
η(λ̂(t))L−1(λL,4.1 Proof of Theorem 2,[0],[0]
− (λ̂(t))L).,4.1 Proof of Theorem 2,[0],[0]
"(6)
By the induction hypothesis, we just need to show that sign(λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)),4.1 Proof of Theorem 2,[0],[0]
= sign(λ − λ̂(t)) and |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)| (i.e., the step is in the correct direction, and does not “overshoot”).",4.1 Proof of Theorem 2,[0],[0]
"First, to see that the step is in the right direction, note that λL ≥ (λ̂(t))L if and only if λ ≥ (λ̂(t)), and the inductive hypothesis implies that λ̂(t), and therefore (λ̂(t))L−1, is non-negative.",4.1 Proof of Theorem 2,[0],[0]
To show that |λ̂(t+1),4.1 Proof of Theorem 2,[0],[0]
− λ̂(t)| ≤ |λ,4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|, it suffices to show that η(λ̂(t))L−1 ∣ ∣ ∣ λL − (λ̂(t))L) ∣ ∣ ∣ ≤ |λ",4.1 Proof of Theorem 2,[0],[0]
"− λ̂(t)|,
which, in turn would be implied by η ≤
∣ ∣ ∣ ∣ 1 (λ̂(t))L−1( ∑L−1 i=0 (λ̂ (t))iλL−1−i) ∣ ∣ ∣ ∣ (since λL − (λ̂(t))L = (λ −
λ̂(t))",4.1 Proof of Theorem 2,[0],[0]
"∑L−1 i=0 (λ̂ (t))iλL−1−i), which follows from the inductive hypothesis and η ≤ 1 LΓ2 .",4.1 Proof of Theorem 2,[0],[0]
"We have proved that each λ̂(t) lies between λ and 1, so that |1−",4.1 Proof of Theorem 2,[0],[0]
λ̂(t)| ≤ |1−,4.1 Proof of Theorem 2,[0],[0]
"λ| ≤ c4 ln(Γ/γ).
",4.1 Proof of Theorem 2,[0],[0]
"Now, since the step is in the right direction, and does not overshoot,
|λ̂(t+1)",4.1 Proof of Theorem 2,[0],[0]
− λ| ≤ |λ̂(t) − λ| − η(λ̂(t))L−1|λL,4.1 Proof of Theorem 2,[0],[0]
"− (λ̂(t))L|
≤ |λ̂(t) − λ|
( 1− η(λ̂(t))L−1 ( L−1 ∑
i=0
(λ̂(t))iλL−1−i ))
≤ |λ̂(t)",4.1 Proof of Theorem 2,[0],[0]
"− λ| ( 1− ηLγ2 ) ,
since the fact that λ̂(t) lies between 1 and λ implies that λ̂(t) ≥ γ1/L. Thus, |λ̂(t) − λ| ≤ (
1− ηLγ2 )t c4",4.1 Proof of Theorem 2,[0],[0]
ln(Γ/γ).,4.1 Proof of Theorem 2,[0],[0]
"This implies that, for any ǫ ∈ (0, 1), for any absolute constant c5, there is a
constant c6 such that, after c6 1 ηLγ2 ln ( dL lnΓ γǫ )
steps, we have |λ̂(t)−λ| ≤ c5γ √ ǫ
LΓ √ d .Writing",4.1 Proof of Theorem 2,[0],[0]
"r = λ̂(t)−λ,
this implies, if c5 is small enough, that
((λ̂(t))L − λL)2 = ((λ+r)L−λL)2
≤ Γ2",4.1 Proof of Theorem 2,[0],[0]
"( ( 1+ r
λ
)L −1
)2
≤ Γ2 ( 2c5rL
λ
)2
≤ Γ2 ( 2c5rL
γ
)2
≤ ǫ
d .
",4.1 Proof of Theorem 2,[0],[0]
"Thus, after O (
1 ηLγ2
ln (
dL lnΓ γǫ
))
steps, (Dkk − D̂ (t) kk ) 2 ≤ ǫ/d for all k, and therefore ℓ(Θ(t))",4.1 Proof of Theorem 2,[0],[0]
"≤ ǫ,
completing the proof.",4.1 Proof of Theorem 2,[0],[0]
"We have seen that if the least squares matrix is symmetric, γ-positivity is sufficient for convergence of gradient descent.",5 Asymmetric positive definite matrices,[0],[0]
We shall see in Section 6 that positivity is also necessary for a broad family of gradient-based algorithms to converge to the optimal solution when the least squares matrix is symmetric.,5 Asymmetric positive definite matrices,[0],[0]
"Thus, in the symmetric case, positivity characterizes the success of gradient methods.
",5 Asymmetric positive definite matrices,[0],[0]
"In this section, we show that positivity suffices for the convergence of a gradient method even without the assumption that the least squares matrix is symmetric.
",5 Asymmetric positive definite matrices,[0],[0]
Note that the set of γ-positive (but not necessarily symmetric) matrices includes both rotations by an acute angle and “partial reflections” of the form ax + b refl(x) where refl(·) is a lengthpreserving reflection and 0 ≤,5 Asymmetric positive definite matrices,[0],[0]
|b| < a.,5 Asymmetric positive definite matrices,[0],[0]
"Since ( u⊤Au )⊤
= u⊤A⊤u, a matrix A is γ-positive if",5 Asymmetric positive definite matrices,[0],[0]
"and only if u⊤(A+A⊤)u ≥ 2γ for all unit length u, i.e. A+A⊤ is positive definite with eigenvalues at least 2γ.",5 Asymmetric positive definite matrices,[0],[0]
"The algorithm analyzed in this section uses a construction that is new, as far as we know, that we call a balanced factorization.",5.1 Balanced factorizations,[0],[0]
"This factorization may be of independent interest.
",5.1 Balanced factorizations,[0],[0]
Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .,5.1 Balanced factorizations,[0],[0]
The principal Lth root of a complex number whose expression in polar coordinates is reθi is r1/Leθi/L.,5.1 Balanced factorizations,[0],[0]
"The principal Lth root of a matrix A is the matrix B such that BL = A, and each eigenvalue of B is the principal Lth root of the corresponding eigenvalue of A.
Definition 3.",5.1 Balanced factorizations,[0],[0]
"If A be a matrix with polar decomposition RP , then A has the balanced factorization A = A1, ..., AL where for each i,
Ai = R 1/LPi, with Pi = R (L−i)/LP 1/LR−(L−i)/L,
and each of the Lth roots is the principal Lth root.
",5.1 Balanced factorizations,[0],[0]
The motivation for balanced factorization is as follows.,5.1 Balanced factorizations,[0],[0]
"We want each factor to do a 1/L fraction of the total amount of rotation, and a 1/L fraction of the total amount of scaling.",5.1 Balanced factorizations,[0],[0]
"However, the scaling done by the ith factor should be done in directions that take account of the partial rotations done by the other factors.",5.1 Balanced factorizations,[0],[0]
"The following is the key property of the balanced factorization; its proof is in Appendix C.
Lemma 7.",5.1 Balanced factorizations,[0],[0]
"If σ1, ..., σd are the singular values of A, and A1, ..., AL is a balanced factorization of A, then the following hold: (a) A = ∏L
i=1Ai; (b) for each i ∈ {1, ..., L}, σ 1/L 1 , ..., σ 1/L d are the singular
values of Ai.",5.1 Balanced factorizations,[0],[0]
The following is the power projection algorithm.,5.2 Procedure and upper bound,[0],[0]
"It has a positivity parameter γ > 0, and uses H = {A : ∀u s.t. ||u||",5.2 Procedure and upper bound,[0],[0]
"= 1, u⊤Au ≥ γ} as its “hypothesis space”.",5.2 Procedure and upper bound,[0],[0]
"First, it initializes Θ(0)i = γ 1/LI for all i ∈ {1, ..., L}.",5.2 Procedure and upper bound,[0],[0]
"Then, for each t, it does the following.
",5.2 Procedure and upper bound,[0],[0]
• Gradient Step.,5.2 Procedure and upper bound,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",5.2 Procedure and upper bound,[0],[0]
i = Θ,5.2 Procedure and upper bound,[0],[0]
(t),5.2 Procedure and upper bound,[0],[0]
"i − η(Θ (t) i+1:L)
⊤ (
Θ (t) 1:",5.2 Procedure and upper bound,[0],[0]
"L − Φ
)
",5.2 Procedure and upper bound,[0],[0]
"(Θ (t) 1:i−1) ⊤.
• Power Project.",5.2 Procedure and upper bound,[0],[0]
Compute the projection Ψ(t+1/2) (w.r.t.,5.2 Procedure and upper bound,[0],[0]
"the Frobenius norm) of Θ (t+1/2) 1:L
onto H.
• Factor.",5.2 Procedure and upper bound,[0],[0]
"Let Θ (t+1) 1 , ...,Θ (t+1) L be the balanced factorization of Ψ (t+1/2), so that Ψ(t+1/2) =
Θ (t+1) 1:L .
",5.2 Procedure and upper bound,[0],[0]
Theorem 3.,5.2 Procedure and upper bound,[0],[0]
For any Φ such that u⊤Φu >,5.2 Procedure and upper bound,[0],[0]
"γ for all unit-length u, the power projection algorithm produces Θ(t) with ℓ(Θ(t))",5.2 Procedure and upper bound,[0],[0]
"≤ ǫ in poly(d, ||Φ||F , 1 γ ) log(1/ǫ) iterations.",5.2 Procedure and upper bound,[0],[0]
Lemma 8.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L ∈ H.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"Θ (0) 1:L = γI ∈ H, and, for all t, Ψ (t+1/2) is obtained by projection onto H, and Θ (t+1) 1:L = Ψ(t+1/2).
",5.3 Proof of Theorem 3,[0],[0]
Definition 4.,5.3 Proof of Theorem 3,[0],[0]
The exponential of a matrix A is exp(A),5.3 Proof of Theorem 3,[0],[0]
"def = ∑∞
k=0 1 k!A k, and B is a logarithm of A if A = exp(B).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 9 (Culver 1966).,5.3 Proof of Theorem 3,[0],[0]
"A real matrix has a real logarithm if and only if it is invertible and each Jordan block belonging to a negative eigenvalue occurs an even number of times.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 10.,5.3 Proof of Theorem 3,[0],[0]
"For all t, Θ (t) 1:L has a real Lth root.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Since Θ (t) 1:L ∈ H implies u,5.3 Proof of Theorem 3,[0],[0]
⊤Θ(t)1,5.3 Proof of Theorem 3,[0],[0]
:Lu > 0,5.3 Proof of Theorem 3,[0],[0]
"for all u, Θ (t) 1:L does not have a negative eigenvalue and is invertible.",5.3 Proof of Theorem 3,[0],[0]
"By Lemma 9, Θ (t) 1:L has a real logarithm.",5.3 Proof of Theorem 3,[0],[0]
"Thus, its real Lth root can be constructed via exp(log(Θ (t) 1:L)/L).
",5.3 Proof of Theorem 3,[0],[0]
"The preceding lemma implies that the algorithm is well-defined, since all of the required roots can be calculated.
",5.3 Proof of Theorem 3,[0],[0]
Lemma 11.,5.3 Proof of Theorem 3,[0],[0]
"H is convex.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Suppose A and B are in H and λ ∈ (0, 1).",5.3 Proof of Theorem 3,[0],[0]
"We have
u⊤(λA+ (1− λ)B)u = λu⊤Au+ (1− λ)u⊤Bu ≥ γ.
Lemma 12.",5.3 Proof of Theorem 3,[0],[0]
"For all A ∈ H, σmin(A) ≥ γ.
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
"Let u and v be singular vectors such that u⊤Av = σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
"γ ≤ v⊤Av = σmin(A)v ⊤u ≤ σmin(A).
",5.3 Proof of Theorem 3,[0],[0]
Lemma 13.,5.3 Proof of Theorem 3,[0],[0]
"For all t, σmin(Θ (t) i )",5.3 Proof of Theorem 3,[0],[0]
"≥ γ 1/L.
Proof.",5.3 Proof of Theorem 3,[0],[0]
"First, σmin(Θ (0) i ) =",5.3 Proof of Theorem 3,[0],[0]
γ 1/L ≥ γ1/L. Now consider t > 0.,5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t−1/2) was projected into H, we have σmin(Ψ(t−1/2))",5.3 Proof of Theorem 3,[0],[0]
≥ γ.,5.3 Proof of Theorem 3,[0],[0]
"Lemma 7 then completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
"Define U(t) = max {
maxs≤tmaxi ||Θ (s) i ||2, ||Φ|| 1/L 2
}
, B(t) = mins≤tmini σmin(Θ (s) i ), and recall that
ℓ(t) = ||Θ (t) 1:L − Φ|| 2 F .
",5.3 Proof of Theorem 3,[0],[0]
"Arguing as in the initial portion of Section 3.2, as long as
η ≤ 1
3Ld5U(t)2L (7)
we have ℓ(t + 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLB(t)2L ) ℓ(t) (see Equation 4).,5.3 Proof of Theorem 3,[0],[0]
"Lemma 13 gives B(t) ≥ γ1/L, so ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
"( 1− ηLγ2 )
ℓ(t).",5.3 Proof of Theorem 3,[0],[0]
"Since Ψ(t+1/2) is the projection of Θ (t+1/2) 1:L onto a convex set H that
contains Φ, and Θ (t+1) 1:L = Ψ (t+1/2), (7) implies
ℓ(t+ 1) ≤ ℓ(t+ 1/2) ≤",5.3 Proof of Theorem 3,[0],[0]
( 1− ηLγ2 ) ℓ(t).,5.3 Proof of Theorem 3,[0],[0]
"(8)
Next, we prove an upper bound on U .
",5.3 Proof of Theorem 3,[0],[0]
Lemma 14.,5.3 Proof of Theorem 3,[0],[0]
"For all t, U(t) ≤ ( √
ℓ(t) + ||Φ||F
)1/L .
",5.3 Proof of Theorem 3,[0],[0]
Proof.,5.3 Proof of Theorem 3,[0],[0]
Recall that ℓ(t) = ||Θ (t) 1:L−Φ|| 2 F .,5.3 Proof of Theorem 3,[0],[0]
"By the triangle inequality, ||Θ (t) 1:L||F ≤",5.3 Proof of Theorem 3,[0],[0]
√ ℓ(t)+ ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
Thus ||Θ (t) 1:L||2 ≤ √ ℓ(t) + ||Φ||F .,5.3 Proof of Theorem 3,[0],[0]
"By Lemma 7, for all i, we have ||Θ (t)",5.3 Proof of Theorem 3,[0],[0]
i ||2 ≤ ( √ ℓ(t) + ||Φ||F )1/L .,5.3 Proof of Theorem 3,[0],[0]
"Since ||Φ||2 ≤ ||Φ||F , this completes the proof.
",5.3 Proof of Theorem 3,[0],[0]
Note that the triangle inequality implies that ℓ(0) ≤ ||Θ (0) 1:L|| 2 F + ||Φ|| 2 F ≤,5.3 Proof of Theorem 3,[0],[0]
γ 2d,5.3 Proof of Theorem 3,[0],[0]
+,5.3 Proof of Theorem 3,[0],[0]
||Φ||2F .,5.3 Proof of Theorem 3,[0],[0]
Since σmin(Φ) ≥,5.3 Proof of Theorem 3,[0],[0]
"γ, we have ||Φ|| 2 F ≥ γ 2d, so ℓ(t) ≤ 2||Φ||2F and U(t) ≤ (3||Φ||2) 1/L.",5.3 Proof of Theorem 3,[0],[0]
"Now, if we set η = 1 cLd5||Φ||2
F , for a large enough absolute constant c, then (7) is satisfied, so that (8) gives ℓ(t+1) ≤ (
1− γ 2
cd5||Φ||2 F
)
ℓ(t) and the power projection algorithm achieves ℓ(t+ 1) ≤ ǫ after
O
(
d5||Φ||2F γ2 log
(
ℓ(0)
ǫ
))
",5.3 Proof of Theorem 3,[0],[0]
"=O
(
d5||Φ||2F γ2 log
(
||Φ||2F ǫ
))
updates.",5.3 Proof of Theorem 3,[0],[0]
"In this section, we show that positive definite Φ are necessary for several gradient descent algorithms with different kinds of regularization to minimize the loss.",6 Failure,[0],[0]
"One family of algorithms that we will
analyze is parameterized by a function ψ mapping the number of inputs d and the number of layers L to a radius ψ(d, L), step sizes ηt and initialization parameter γ ≥ 0.",6 Failure,[0],[0]
"In particular, a ψ-step-and-project algorithm is any instantiation of the following algorithmic template.
",6 Failure,[0],[0]
Initialize each Θ (0),6 Failure,[0],[0]
"i = γ 1/LI for some γ ≥ 0 and iterate:
• Gradient Step.",6 Failure,[0],[0]
"For each i ∈ {1, ..., L}, update:
Θ (t+1/2)",6 Failure,[0],[0]
i = Θ,6 Failure,[0],[0]
(t),6 Failure,[0],[0]
"i − ηt(Θ (t) i+1:L)
⊤ (
Θ (t) 1:L − Φ
)
(Θ (t) 1:i−1) ⊤.
• Project.",6 Failure,[0],[0]
Set each Θt+1i,6 Failure,[0],[0]
"to the projection of Θ t+1/2 i onto {A : ||A− I||2 ≤ ψ(d, L)}.
",6 Failure,[0],[0]
We will also show that Penalty Regularized Gradient Descent which uses gradient descent with any step sizes ηt on the regularized objective ℓ(Θ) + κ 2 ∑ i ||I,6 Failure,[0],[0]
"−Θ|| 2 F also fails to minimize the loss.
",6 Failure,[0],[0]
"Both results use the simple observation that when Θ1:L and Φ are mutually diagonalizable then
||Θ1:L − Φ|| 2 F = ||U ⊤D̂U",6 Failure,[0],[0]
"− U⊤DU ||2F = d ∑
j=1
(D̂jj −Djj) 2,
where the Dii are the eigenvalues of Φ.
Theorem 4.",6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then Penalty Regularized Gradient Descent produces hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and L is even, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"For all t, Penalty Regularized Gradient Descent produces Θ (t+1) i = (1 − κ)Θ (t) i + κI",6 Failure,[0],[0]
− ηt(Θ (t) i+1:L) ⊤ ( Θ (t) 1:L −Φ ) (Θ (t) 1:i−1) ⊤.,6 Failure,[0],[0]
"Thus, by induction, the Θ(t)i are matrix polynomials of Φ, and therefore they are all commuting normal.",6 Failure,[0],[0]
As in Lemmas 5 and 6 each Θ (t) i is the same U ⊤D̃(t)U and Θ (t) 1,6 Failure,[0],[0]
:L = U ⊤(D̃(t))LU .,6 Failure,[0],[0]
"Since L is even, each (D̃(t))Ljj ≥ 0, so ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L−Φ|| 2 F ≥ λ 2/2.
To analyze step-and-project algorithms, it is helpful to first characterize the project step (see also (Lefkimmiatis et al., 2013)).
",6 Failure,[0],[0]
Lemma 15.,6 Failure,[0],[0]
"Let X be a symmetric matrix and let U⊤DU be its diagonalization.
",6 Failure,[0],[0]
"For a > 0, let Y be the Frobenius norm projection of X onto Ba = {A : A is symmetric psd and ||A−I||2 ≤ a}.",6 Failure,[0],[0]
"Then Y = U
⊤D̃U where D̃ is obtained from D by projecting all of its diagonal elements onto [1− a, 1 + a].
",6 Failure,[0],[0]
"Thus {X,Y } are symmetric commuting normal matrices.
",6 Failure,[0],[0]
Proof.,6 Failure,[0],[0]
"First, if X ∈ Ba, then Y = X and we are done.",6 Failure,[0],[0]
Assume X 6∈ Ba.,6 Failure,[0],[0]
"Clearly U ⊤D̃U ∈ Ba, so we just need to show that any member of Ba is at least as far from X as U⊤D̃U is.",6 Failure,[0],[0]
"Let Λ be the multiset of eigenvalues of X (with repetitions) that are not in [1 − a, 1 + a], and for each λ ∈ Λ, let eλ be the adjustment to λ necessary to bring it to [1− a, 1 + a]; i.e., so that λ+ eλ is the projection of λ onto [1− a, 1 + a].
",6 Failure,[0],[0]
"If uλ is the eigenvector associated with λ, we have U ⊤D̃U −X = ∑ λ∈Λ eλuλu ⊤ λ , so that ||U ⊤D̃U",6 Failure,[0],[0]
− X||2F = ∑,6 Failure,[0],[0]
λ∈Λ,6 Failure,[0],[0]
e 2 λ.,6 Failure,[0],[0]
Let Z be an arbitrary member of Ba.,6 Failure,[0],[0]
We would like to show that ||Z,6 Failure,[0],[0]
− X|| 2 F ≥ ∑ λ∈Λ e 2 λ.,6 Failure,[0],[0]
"Since Z ∈ Ba, we have ||Z − I||2 ≤ a. ||Z",6 Failure,[0],[0]
− I||2 is the largest singular value of Z,6 Failure,[0],[0]
"− I so, for any unit length vector, in particular some uλ for λ ∈ Λ, |u ⊤ λ (Z",6 Failure,[0],[0]
− I)uλ| = |u ⊤ λ,6 Failure,[0],[0]
"Zuλ − 1| ≤ a, which implies u⊤λZuλ ∈",6 Failure,[0],[0]
"[1 − a, 1 + a].",6 Failure,[0],[0]
Since U is unitary U ⊤(X,6 Failure,[0],[0]
"− Z)U has the same eigenvalues as X − Z, and, since the Frobenius norm is a function of the eigenvalues, ||U⊤(X − Z)U ||F = ||X − Z||F .",6 Failure,[0],[0]
But since u⊤λZuλ ∈,6 Failure,[0],[0]
"[1 − a, 1 + a] for all λ ∈ Λ, just summing over the diagonal elements, we get ||U⊤(X − Z)U",6 Failure,[0],[0]
"||2F ≥ ∑ λ∈Λ e 2 λ, completing the proof.
",6 Failure,[0],[0]
Theorem 5.,6 Failure,[0],[0]
If the least squares matrix Φ is symmetric then ψ-step-and-project algorithms produce hypotheses Θ (t) 1:L that are commuting normal with Φ.,6 Failure,[0],[0]
"In addition, if Φ has a negative eigenvalue −λ and either L is even or ψ(L, d) ≤ 1, then ℓ(Θ(t))",6 Failure,[0],[0]
"≥ λ2/2 for all t.
Proof.",6 Failure,[0],[0]
"As in Lemmas 5 and 6, the Θ (t+1/2)",6 Failure,[0],[0]
i are identical and mutually diagonalizable with Φ. Lemma 15 shows that this is preserved by the projection step.,6 Failure,[0],[0]
Thus there is a real diagonal D̃(t) such that each Θ (t) i = U ⊤D(t)i,6 Failure,[0],[0]
"U , so Θ (t) 1:L = U ⊤(D̃(t))LU .",6 Failure,[0],[0]
"When L is even, each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"When ψ(d, L) ≤ 1",6 Failure,[0],[0]
"then the projection ensures that the elements of D̃(t) are non-negative, and thus each (D̃(t))L)j,j ≥ 0.",6 Failure,[0],[0]
"In either case, ℓ(Θ (t))",6 Failure,[0],[0]
"= 12 ||Θ (t) 1:L− Φ||2F ≥ λ 2/2.
",6 Failure,[0],[0]
"One choice of Φ that satisfies the requirements of Theorems 4 and 5 is Φ = diag(−λ, 1, 1, ..., 1).",6 Failure,[0],[0]
"For constant λ, the loss of Θ(0) = (I, I, ..., I) is a constant for this target.",6 Failure,[0],[0]
"Another choice is Φ = diag(−λ,−λ, 1, 1, ..., 1), which has a positive determinant.
",6 Failure,[0],[0]
Our proof of failure to minimize the loss exploits the fact that the layers are initialized to multiples of the identity.,6 Failure,[0],[0]
"Since the training process is a continuous function of the initial solution, this implies that any convergence to a good solution will be very slow if the initializations are sufficiently close to the identity.",6 Failure,[0],[0]
"We thank Yair Carmon, Nigel Duffy, Matt Feiszli, Roy Frostig, Vineet Gupta, Moritz Hardt, Tomer Koren, Antoine Saliou, Hanie Sedghi, Yoram Singer and Kunal Talwar for valuable conversations.
",Acknowledgements,[0],[0]
Peter Bartlett gratefully acknowledges the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS).,Acknowledgements,[0],[0]
"We rely on the following facts (Horn, 1986; Harville, 1997).
",A Proof of Lemma 1,[0],[0]
Lemma 16.,A Proof of Lemma 1,[0],[0]
"For compatible matrices (and, where m,n, p, q, r, s are mentioned, A ∈ ℜm×n, B ∈ ℜp×q, X ∈ ℜr×s):
A⊗ (B ⊗ E) =",A Proof of Lemma 1,[0],[0]
"(A⊗B)⊗E,
AC ⊗BD = (A⊗B)(C ⊗D),
(A⊗B)⊤ = A⊤ ⊗B⊤,
vec(AXB) = (B⊤ ⊗A)vec(X),
Tm,nvec(A) def = vec(A⊤),
Tn,mTm,n = Imn,
Tm,n = T ⊤ n,m,
T1,n = Tn,1 =",A Proof of Lemma 1,[0],[0]
"In,
DX(A(B(X)))",A Proof of Lemma 1,[0],[0]
"= DB(A(B(X)))DX (B(X)),
DX(A(X)B(X))",A Proof of Lemma 1,[0],[0]
= (B(X) ⊤ ⊗ Im)DXA(X),A Proof of Lemma 1,[0],[0]
+,A Proof of Lemma 1,[0],[0]
"(Iq ⊗A(X))DXB(X),
DX(A(X) T ) = Tn,mDX(A(X)),
DX(AXB)",A Proof of Lemma 1,[0],[0]
"= B ⊤ ⊗A,
DA(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(Imn ⊗ vec(B))
=",A Proof of Lemma 1,[0],[0]
"(Inq ⊗ Tm,p)(In ⊗ vec(B)⊗",A Proof of Lemma 1,[0],[0]
"Im),
DB(A⊗B) =",A Proof of Lemma 1,[0],[0]
"(In ⊗ Tq,m ⊗ Ip)(vec(A)⊗ Ipq)
= (Tp,q ⊗ Imn)(Iq ⊗ vec(A)⊗",A Proof of Lemma 1,[0],[0]
"Ip).
",A Proof of Lemma 1,[0],[0]
"Armed with Lemma 16, we now prove Lemma 1.",A Proof of Lemma 1,[0],[0]
"We have
DΘifΘ(x) = DΘi (Θi+1:LΘiΘ1:i−1x) =",A Proof of Lemma 1,[0],[0]
"(Θ1:i−1x) ⊤ ⊗Θi+1:L.
Again, from Lemma 16
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= DΘi
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
= DΘ1:j−1x
(
(Θ1:j−1x) ⊤ ⊗Θj+1:L
)
DΘi (Θ1:j−1x)
(by the chain rule, since i < j)
= DΘ1:j−1x
(
(
(Θ1:j−1x)⊗Θ ⊤ j+1:L
)⊤ ) (
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.",A Proof of Lemma 1,[0],[0]
"(9)
Define P = Θ1:j−1x and Q = Θj+1:L, so that P ∈ ℜd×1 and Q ∈ ℜd×d.",A Proof of Lemma 1,[0],[0]
"We have
DP
(
( P ⊗Q⊤ )⊤ )
= Td2,dDP
( P ⊗Q⊤ )
",A Proof of Lemma 1,[0],[0]
"= Td2,d(I1 ⊗ Td,d ⊗ Id)(Id ⊗ vec(Q T ))",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Q ⊤)).
",A Proof of Lemma 1,[0],[0]
"Substituting back into (9), we get
DΘi ( DΘjfΘ(x) )",A Proof of Lemma 1,[0],[0]
"= Td2,d(Td,d ⊗ Id)(Id ⊗ vec(Θ ⊤ j+1:L))
",A Proof of Lemma 1,[0],[0]
"(
(Θ1:i−1x) ⊤",A Proof of Lemma 1,[0],[0]
"⊗Θi+1:j−1
)
.
",A Proof of Lemma 1,[0],[0]
"The product rule in Lemma 16 gives, for each i,
DΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= E(DΘi(ℓ(fΘ(X)))
= E(DΘi( 1
2 (fΘ(X)− ΦX)
⊤(fΘ(X)",A Proof of Lemma 1,[0],[0]
"−ΦX)))
",A Proof of Lemma 1,[0],[0]
= E(((Θ1:L − Φ)X) ⊤DΘifΘ(X)),A Proof of Lemma 1,[0],[0]
"= E ( ((Θ1:L − Φ)X) ⊤ ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
( (I1 ⊗,A Proof of Lemma 1,[0],[0]
"((Θ1:L − Φ)X) ⊤) ( (Θ1:i−1X) ⊤ ⊗Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
"= E (( (Θ1:i−1X) ⊤ ⊗ ((Θ1:L −Φ)X) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= E,A Proof of Lemma 1,[0],[0]
(( X⊤Θ⊤1:i−1 ) ⊗,A Proof of Lemma 1,[0],[0]
( X⊤(Θ1:L −Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
L )),A Proof of Lemma 1,[0],[0]
"= E ( (X⊤ ⊗X⊤) (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
))
= E ((X ⊗X)vec(1))⊤ ( Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L )
= E ( vec(XX⊤) )",A Proof of Lemma 1,[0],[0]
"⊤ (
Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L
)
= (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Hence,
(DΘiℓ (fΘ))",A Proof of Lemma 1,[0],[0]
"⊤ =
(
Θ1:i−1 ⊗Θ ⊤ i+1:L(Θ1:L − Φ)
)
(vec(Id))
",A Proof of Lemma 1,[0],[0]
"= vec ( Θ⊤i+1:L(Θ1:L − Φ)IdΘ ⊤ 1:i−1 ) .
",A Proof of Lemma 1,[0],[0]
"Also, recalling that i < j, we have
DΘjDΘiℓ (fΘ)",A Proof of Lemma 1,[0],[0]
"= DΘj
( (vec(Id))",A Proof of Lemma 1,[0],[0]
T ( Θ⊤1:i−1 ⊗ (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘj
(
Θ⊤1:i−1 ⊗",A Proof of Lemma 1,[0],[0]
"(Θ1:L − Φ) ⊤Θi+1:L
)
= (Id2 ⊗ (vec(Id))",A Proof of Lemma 1,[0],[0]
"T ) (Id ⊗ Td,d ⊗",A Proof of Lemma 1,[0],[0]
"Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘj",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L ) .
",A Proof of Lemma 1,[0],[0]
"Continuing with the subproblem,
DΘj
(
(Θ1:L −Φ) ⊤Θi+1:",A Proof of Lemma 1,[0],[0]
"L
)
",A Proof of Lemma 1,[0],[0]
"= (Θ⊤i+1:L ⊗ Id)DΘj
( (Θ1:L − Φ) ⊤ )
",A Proof of Lemma 1,[0],[0]
+ (Id ⊗ (Θ1:,A Proof of Lemma 1,[0],[0]
L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id)DΘj
( Θ⊤1:L )
+",A Proof of Lemma 1,[0],[0]
(Id ⊗ (Θ1:L − Φ) ⊤)DΘj,A Proof of Lemma 1,[0],[0]
"(Θi+1:L)
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 ) DΘj (Θ ⊤ j )
+",A Proof of Lemma 1,[0],[0]
"(Id ⊗ (Θ1:L − Φ) ⊤) ( Θ⊤i+1:j−1 ⊗Θj+1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θj+1:L ⊗Θ ⊤ 1:j−1 )",A Proof of Lemma 1,[0],[0]
"Td,d
+ (Id ⊗ (Θ1:L − Φ) ⊤)",A Proof of Lemma 1,[0],[0]
"( Θ⊤i+1:j−1 ⊗Θj+1:L )
=",A Proof of Lemma 1,[0],[0]
( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 ),A Proof of Lemma 1,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1:L ) .
",A Proof of Lemma 1,[0],[0]
"Finally,
DΘiDΘiℓ (fΘ) = DΘi
( (vec(Id)) T ( Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
"L − Φ) ⊤Θi+1:L ))
",A Proof of Lemma 1,[0],[0]
= (Id2 ⊗ (vec(Id)),A Proof of Lemma 1,[0],[0]
"T )DΘi
(
Θ⊤1:i−1 ⊗ (Θ1:",A Proof of Lemma 1,[0],[0]
L − Φ) ⊤Θi+1:,A Proof of Lemma 1,[0],[0]
"L
)
= (Id2 ⊗ (vec(Id)) T )",A Proof of Lemma 1,[0],[0]
"(Id ⊗ Td,d ⊗ Id)
( vec(Θ⊤1:i−1)⊗ Id2 ) DΘi",A Proof of Lemma 1,[0],[0]
( (Θ1:L − Φ) ⊤Θi+1,A Proof of Lemma 1,[0],[0]
":L )
and
DΘi
(
(Θ1:L − Φ) ⊤Θi+1:L
)
= (Θ⊤i+1:L ⊗ Id)DΘi
( (Θ1:L − Φ) ⊤ )
= (Θ⊤i+1:L ⊗ Id)DΘi
( Θ⊤1:L )
= (Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ) DΘi(Θ ⊤ i ) =",A Proof of Lemma 1,[0],[0]
(Θ⊤i+1:L ⊗ Id) ( Θi+1:L ⊗Θ ⊤ 1:i−1 ),A Proof of Lemma 1,[0],[0]
"Td,d = (
Θ⊤i+1:LΘi+1:",A Proof of Lemma 1,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",A Proof of Lemma 1,[0],[0]
"Td,d.",A Proof of Lemma 1,[0],[0]
"We have ||∇2||2F = 2 ∑
i<j
||DΘjDΘiℓ(fΘ)|| 2 F +
∑
i
||DΘiDΘiℓ(fΘ)|| 2 F .",B Proof of Lemma 3,[0],[0]
"(10)
Let’s start with the easier term.",B Proof of Lemma 3,[0],[0]
Choose Θ such that ||Θi − I||2 ≤ z,B Proof of Lemma 3,[0],[0]
"for all i. We have
||DΘiDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(Id)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣
F
≤ ∣ ∣
∣
∣ ∣ ∣ (Id2 ⊗ (vec(Id)) ⊤) (Id ⊗ Td,d ⊗ Id) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗Id2 )( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 )
(
Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
"L ⊗Θ ⊤ 1:i−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
∣ ∣ ∣ ∣ ∣ ∣
F
≤ d3/2 ∣ ∣
∣
∣ ∣ ∣ ( vec(Θ⊤1:i−1)⊗ Id2 ) ∣ ∣ ∣ ∣ ∣ ∣
F
× ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ∣ ∣
∣
∣ ∣
∣ vec(Θ⊤1:i−1)
∣ ∣ ∣ ∣ ∣ ∣
F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d7/2 ||Θ1:i−1||F
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:",B Proof of Lemma 3,[0],[0]
L ⊗Θ ⊤ 1:i−1 ),B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4 ||Θ1:i−1||2
∣ ∣ ∣ ∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘi+1:L ⊗Θ ⊤ 1:i−1 ) ∣ ∣ ∣ ∣ ∣ ∣
F
= d4(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ F × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ F
≤",B Proof of Lemma 3,[0],[0]
"d5(1 + z)i−1 ∣ ∣
∣
∣ ∣
∣ Θ⊤i+1:LΘi+1:L
∣ ∣ ∣ ∣ ∣ ∣ 2 × ∣ ∣ ∣ ∣ ∣ ∣ Θ⊤1:i−1 ∣ ∣ ∣ ∣ ∣ ∣ 2
≤ d5(1 + z)2(L−1).
",B Proof of Lemma 3,[0],[0]
"Similarly,
||DΘjDΘiℓ (fΘ) ||F = ∣ ∣ ∣ ∣(Id2⊗(vec(I)) ⊤) (Id⊗Td,d⊗Id)
( vec(Θ⊤1:i−1)⊗Id2 )
(
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )
)
∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1 ∣ ∣ ∣ ∣
(
Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1
)
",B Proof of Lemma 3,[0],[0]
"Td,d
+ ( Θ⊤i+1:j−1 ⊗ (Θ1:L −Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ) ∣ ∣ ∣ ∣
F
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"(∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:LΘj+1:L ⊗Θ ⊤ 1:j−1 )",B Proof of Lemma 3,[0],[0]
"Td,d ∣ ∣ ∣ ∣ ∣ ∣
F
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ∣ ∣
∣
∣ ∣ ∣ ( Θ⊤i+1:j−1 ⊗ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L )∣ ∣ ∣ ∣ ∣ ∣
F
)
= d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
"( d(1 + z)2L−1−i
+ ||Θi+1:j−1||F × ∣ ∣ ∣ ∣ ∣ ∣ (Θ1:L − Φ) ⊤Θj+1",B Proof of Lemma 3,[0],[0]
":L ∣ ∣ ∣ ∣ ∣ ∣
F
)
≤ d4(1 + z)i−1",B Proof of Lemma 3,[0],[0]
( d(1 + z)2L−1−i + 2d(1 + z)2L−1−i ),B Proof of Lemma 3,[0],[0]
"= 3d5(1 + z)2L−2.
",B Proof of Lemma 3,[0],[0]
"Putting these together with (10), we get ||∇2||2F ≤",B Proof of Lemma 3,[0],[0]
"L 29d10(1 + z)4L, so that
||∇2||F ≤ 3Ld 5(1 + z)2L.",B Proof of Lemma 3,[0],[0]
"Recall that a polar decomposition of a matrix A consists of a unitary matrix R and a positive semidefinite matrix P such that A = RP .
",C Proof of Lemma 7,[0],[0]
"Lemma 17 ((Horn & Johnson, 2013)).",C Proof of Lemma 7,[0],[0]
"A is a unitary matrix if and only if all of the (complex) eigenvalues z of A have magnitude 1.
",C Proof of Lemma 7,[0],[0]
"Lemma 18 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is unitary then A is normal.
",C Proof of Lemma 7,[0],[0]
"Lemma 19 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A is normal with eigenvalues λ1, ..., λd, the singular values of A are |λ1|, ..., |λd|.
Lemma 20.",C Proof of Lemma 7,[0],[0]
"If A is unitary, then A1/L is unitary, and thus Ai/L is unitary for any non-negative integer i.
Lemma 21.",C Proof of Lemma 7,[0],[0]
"If A is invertible and normal with singular values σ1, ..., σd, then, for any positive integer L, the singular values of A1/L are σ 1/L 1 , ..., σ 1/L d .
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"Follows from Lemma 19 together with the fact that raising a non-singular matrix to a power results in raising its eigenvalues to the same power.
",C Proof of Lemma 7,[0],[0]
"Lemma 22 ((Horn & Johnson, 2013))",C Proof of Lemma 7,[0],[0]
.,C Proof of Lemma 7,[0],[0]
"If A = RP is the polar decomposition of A, then the singular values of A are the same as the singular values of P .
",C Proof of Lemma 7,[0],[0]
Lemma 23.,C Proof of Lemma 7,[0],[0]
"If σ1, ..., σd are the principal components of A, and A = ∏L i=1Ai is a balanced factorization of A, then then σ 1/L 1 , ..., σ 1/L d are the principal components of Ai, for each i ∈ {1, ..., L}.
",C Proof of Lemma 7,[0],[0]
Proof.,C Proof of Lemma 7,[0],[0]
"The singular values of Ai = RiPi are the same as the singular values of Pi, which is similar to P 1/L, whose singular values are the Lth roots of the singular values of P , which are the same as the singular values of A.
Lemma 24.",C Proof of Lemma 7,[0],[0]
"If A1, ..., AL is a balanced factorization of A, then
A = L ∏
i=1
Ai.
Proof.",C Proof of Lemma 7,[0],[0]
"We have
A = RP
= R1/LR1−1/LP 1/LP 1−1/L = R1/LR1−1/LP 1/LR−(1−1/L)R1−1/LP 1−1/L = R1P1R 1−1/LP 1−1/L = A1R 1−1/LP 1−1/L = A1R 1/LR1−2/LP 1/LP 1−2/L
and so on.",C Proof of Lemma 7,[0],[0]
"We analyze algorithms for approximating a function f(x) = Φxmapping R to R using deep linear neural networks, i.e. that learn a function h parameterized by matrices Θ1, ...,ΘL and defined by h(x)",abstractText,[0],[0]
= ΘLΘL−1...Θ1x.,abstractText,[0],[0]
We focus on algorithms that learn through gradient descent on the population quadratic loss in the case that the distribution over the inputs is isotropic.,abstractText,[0],[0]
"We provide polynomial bounds on the number of iterations for gradient descent to approximate the least squares matrix Φ, in the case where the initial hypothesis Θ1 = ...",abstractText,[0],[0]
= ΘL = I has excess loss bounded by a small enough constant.,abstractText,[0],[0]
"On the other hand, we show that gradient descent fails to converge for Φ whose distance from the identity is a larger constant, and we show that some forms of regularization toward the identity in each layer do not help.",abstractText,[0],[0]
"If Φ is symmetric positive definite, we show that an algorithm that initializes Θi = I learns an ǫ-approximation of f using a number of updates polynomial in L, the condition number of Φ, and log(d/ǫ).",abstractText,[0],[0]
"In contrast, we show that if the least squares matrix Φ is symmetric and has a negative eigenvalue, then all members of a class of algorithms that perform gradient descent with identity initialization, and optionally regularize toward the identity in each layer, fail to converge.",abstractText,[0],[0]
"We analyze an algorithm for the case that Φ satisfies uΦu > 0 for all u, but may not be symmetric.",abstractText,[0],[0]
This algorithm uses two regularizers: one that maintains the invariant u⊤ΘLΘL−1...,abstractText,[0],[0]
Θ1u > 0,abstractText,[0],[0]
"for all u, and another that “balances” Θ1, ...,ΘL so that they have the same singular values.",abstractText,[0],[0]
"Single-task learning in computer vision has enjoyed much success in deep learning, with many single-task models now performing at or beyond human accuracies for a wide array of tasks.",1. Introduction,[0],[0]
"However, an ultimate visual system for full scene understanding must be able to perform many diverse perceptual tasks simultaneously and efficiently, especially within the limited compute environments of embedded systems
1Magic Leap, Inc. Correspondence to: Zhao Chen <zchen@magicleap.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
such as smartphones, wearable devices, and robots/drones.",1. Introduction,[0],[0]
"Such a system can be enabled by multitask learning, where one model shares weights across multiple tasks and makes multiple inferences in one forward pass.",1. Introduction,[0],[0]
"Such networks are not only scalable, but the shared features within these networks can induce more robust regularization and boost performance as a result.",1. Introduction,[0],[0]
"In the ideal limit, we can thus have the best of both worlds with multitask networks: more efficiency and higher performance.
",1. Introduction,[0],[0]
"In general, multitask networks are difficult to train; different tasks need to be properly balanced so network parameters converge to robust shared features that are useful across all tasks.",1. Introduction,[0],[0]
"Methods in multitask learning thus far have largely tried to find this balance by manipulating the forward pass of the network (e.g. through constructing explicit statistical relationships between features (Long & Wang, 2015) or optimizing multitask network architectures (Misra et al., 2016), etc.), but such methods ignore a key insight: task imbalances impede proper training because they manifest as imbalances between backpropagated gradients.",1. Introduction,[0],[0]
"A task that is too dominant during training, for example, will necessarily express that dominance by inducing gradients which have relatively large magnitudes.",1. Introduction,[0],[0]
"We aim to mitigate such issues at their root by directly modifying gradient magnitudes through tuning of the multitask loss function.
",1. Introduction,[0],[0]
"In practice, the multitask loss function is often assumed to be linear in the single task losses Li, L = ∑ i wiLi, where the sum runs over all T tasks.",1. Introduction,[0],[0]
"In our case, we propose an adaptive method, and so wi can vary at each training step t: wi = wi(t).",1. Introduction,[0],[0]
"This linear form of the loss function is convenient for implementing gradient balancing, as wi very directly and linearly couples to the backpropagated gradient magnitudes from each task.",1. Introduction,[0],[0]
The challenge is then to find the best value for each wi at each training step t that balances the contribution of each task for optimal model training.,1. Introduction,[0],[0]
"To optimize the weights wi(t) for gradient balancing, we propose a simple algorithm that penalizes the network when backpropagated gradients from any task are too large or too small.",1. Introduction,[0],[0]
"The correct balance is struck when tasks are training at similar rates; if task i is training relatively quickly, then its weight wi(t) should decrease relative to other task weights wj(t)|j 6=i to allow other tasks more influence on
training.",1. Introduction,[0],[0]
"Our algorithm is similar to batch normalization (Ioffe & Szegedy, 2015) with two main differences: (1) we normalize across tasks instead of across data batches, and (2) we use rate balancing as a desired objective to inform our normalization.",1. Introduction,[0],[0]
"We will show that such gradient normalization (hereafter referred to as GradNorm) boosts network performance while significantly curtailing overfitting.
",1. Introduction,[0],[0]
"Our main contributions to multitask learning are as follows:
1.",1. Introduction,[0],[0]
"An efficient algorithm for multitask loss balancing which directly tunes gradient magnitudes.
2.",1. Introduction,[0],[0]
"A method which matches or surpasses the performance of very expensive exhaustive grid search procedures, but which only requires tuning a single hyperparameter.
3.",1. Introduction,[0],[0]
A demonstration that direct gradient interaction provides a powerful way of controlling multitask learning.,1. Introduction,[0],[0]
"Multitask learning was introduced well before the advent of deep learning (Caruana, 1998; Bakker & Heskes, 2003), but the robust learned features within deep networks and their excellent single-task performance have spurned renewed interest.",2. Related Work,[0],[0]
"Although our primary application area is computer vision, multitask learning has applications in multiple other fields, from natural language processing (Collobert & Weston, 2008; Hashimoto et al., 2016; Søgaard & Goldberg, 2016) to speech synthesis (Seltzer & Droppo, 2013; Wu et al., 2015), from very domain-specific applications such as traffic prediction (Huang et al., 2014) to very general cross-domain work (Bilen & Vedaldi, 2017).",2. Related Work,[0],[0]
"Multitask learning has also been explored in the context of curriculum learning (Graves et al., 2017), where subsets of tasks are subsequently trained based on local rewards; we here explore the opposite approach, where tasks are jointly trained based on global rewards such as total loss decrease.
",2. Related Work,[0],[0]
"Multitask learning is very well-suited to the field of computer vision, where making multiple robust predictions is crucial for complete scene understanding.",2. Related Work,[0],[0]
"Deep networks have been used to solve various subsets of multiple vision tasks, from 3-task networks (Eigen & Fergus, 2015; Teichmann et al., 2016) to much larger subsets as in UberNet (Kokkinos, 2016).",2. Related Work,[0],[0]
"Often, single computer vision problems can even be framed as multitask problems, such as in Mask R-CNN for instance segmentation (He et al., 2017) or YOLO-9000 for object detection (Redmon & Farhadi, 2016).",2. Related Work,[0],[0]
Particularly of note is the rich and significant body of work on finding explicit ways to exploit task relationships within a multitask model.,2. Related Work,[0],[0]
"Clustering methods have shown success beyond deep models (Jacob et al., 2009; Kang et al., 2011), while constructs such as deep relationship networks (Long & Wang, 2015) and cross-stich networks (Misra et al., 2016)
give deep networks the capacity to search for meaningful relationships between tasks and to learn which features to share between them.",2. Related Work,[0],[0]
"Work in (Warde-Farley et al., 2014) and (Lu et al., 2016) use groupings amongst labels to search through possible architectures for learning.",2. Related Work,[0],[0]
"Perhaps the most relevant to the current work, (Kendall et al., 2017) uses a joint likelihood formulation to derive task weights based on the intrinsic uncertainty in each task.",2. Related Work,[0],[0]
"For a multitask loss function L(t) = ∑ wi(t)Li(t), we aim to learn the functions wi(t) with the following goals: (1) to place gradient norms for different tasks on a common scale through which we can reason about their relative magnitudes, and (2) to dynamically adjust gradient norms so different tasks train at similar rates.",3.1. Definitions and Preliminaries,[0],[0]
"To this end, we first define the relevant quantities, first with respect to the gradients we will be manipulating.
",3.1. Definitions and Preliminaries,[0],[0]
•,3.1. Definitions and Preliminaries,[0],[0]
W : The subset of the full network weights W ⊂ W where we actually apply GradNorm.,3.1. Definitions and Preliminaries,[0],[0]
"W is generally chosen as the last shared layer of weights to save on compute costs1.
",3.1. Definitions and Preliminaries,[0],[0]
• G(i)W (t) = ||∇Wwi(t)Li(t)||2: the L2 norm of the gradient of the weighted single-task loss wi(t)Li(t),3.1. Definitions and Preliminaries,[0],[0]
"with respect to the chosen weights W .
• GW (t) = Etask[G(i)W (t)]: the average gradient norm across all tasks at training time t.
We also define various training rates for each task i:
• L̃i(t) = Li(t)/Li(0): the loss ratio for task",3.1. Definitions and Preliminaries,[0],[0]
"i at time t. L̃i(t) is a measure of the inverse training rate of task i (i.e. lower values of L̃i(t) correspond to a faster training rate for task i)2.
• ri(t) = L̃i(t)/Etask[L̃i(t)]",3.1. Definitions and Preliminaries,[0],[0]
": the relative inverse training rate of task i.
With the above definitions in place, we now complete our description of the GradNorm algorithm.",3.1. Definitions and Preliminaries,[0],[0]
"As stated in Section 3.1, GradNorm should establish a common scale for gradient magnitudes, and also should balance
1In our experiments this choice of W causes GradNorm to increase training time by only ∼ 5% on NYUv2.
2Networks in this paper all had stable initializations and Li(0) could be used directly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When Li(0) is sharply dependent on initialization, we can use a theoretical initial loss instead.",3.2. Balancing Gradients with GradNorm,[0],[0]
"E.g. for Li the CE loss across C classes, we can use Li(0) = log(C).
training rates of different tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The common scale for gradients is most naturally the average gradient norm, GW (t), which establishes a baseline at each timestep t by which we can determine relative gradient sizes.",3.2. Balancing Gradients with GradNorm,[0],[0]
"The relative inverse training rate of task i, ri(t), can be used to rate balance our gradients.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Concretely, the higher the value of ri(t), the higher the gradient magnitudes should be for task i in order to encourage the task to train more quickly.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Therefore, our desired gradient norm for each task i is simply:
G (i) W (t) 7→ GW (t)× [ri(t)]",3.2. Balancing Gradients with GradNorm,[0],[0]
"α, (1)
where α is an additional hyperparameter.",3.2. Balancing Gradients with GradNorm,[0],[0]
α sets the strength of the restoring force which pulls tasks back to a common training rate.,3.2. Balancing Gradients with GradNorm,[0],[0]
"In cases where tasks are very different in their complexity, leading to dramatically different learning dynamics between tasks, a higher value of α should be used to enforce stronger training rate balancing.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When tasks are more symmetric (e.g. the synthetic examples in Section 4), a lower value of α is appropriate.",3.2. Balancing Gradients with GradNorm,[0],[0]
Note that α = 0 will always try to pin the norms of backpropagated gradients from each task to be equal at W .,3.2. Balancing Gradients with GradNorm,[0],[0]
"See Section 5.4 for more details on the effects of tuning α.
",3.2. Balancing Gradients with GradNorm,[0],[0]
"Equation 1 gives a target for each task i’s gradient norms, and we update our loss weights wi(t) to move gradient
norms towards this target for each task.",3.2. Balancing Gradients with GradNorm,[0],[0]
"GradNorm is then implemented as an L1 loss function Lgrad between the actual and target gradient norms at each timestep for each task, summed over all tasks:
Lgrad(t;wi(t))",3.2. Balancing Gradients with GradNorm,[0],[0]
"= ∑ i ∣∣∣∣G(i)W (t)−GW (t)× [ri(t)]α∣∣∣∣ 1 (2)
where the summation runs through all T tasks.",3.2. Balancing Gradients with GradNorm,[0],[0]
"When differentiating this loss Lgrad, we treat the target gradient norm GW (t)× [ri(t)]α as a fixed constant to prevent loss weights wi(t) from spuriously drifting towards zero.",3.2. Balancing Gradients with GradNorm,[0],[0]
"Lgrad is then differentiated only with respect to the wi, as the wi(t) directly control gradient magnitudes per task.",3.2. Balancing Gradients with GradNorm,[0],[0]
The computed gradients ∇wiLgrad are then applied via standard update rules to update each wi (as shown in Figure 1).,3.2. Balancing Gradients with GradNorm,[0],[0]
The full GradNorm algorithm is summarized in Algorithm 1.,3.2. Balancing Gradients with GradNorm,[0],[0]
"Note that after every update step, we also renormalize the weights wi(t) so that ∑ i wi(t) = T in order to decouple gradient normalization from the global learning rate.",3.2. Balancing Gradients with GradNorm,[0],[0]
"To illustrate GradNorm on a simple, interpretable system, we construct a common scenario for multitask networks: training tasks which have similar loss functions but different loss scales.",4. A Toy Example,[0],[0]
"In such situations, if we naı̈vely pick wi(t) = 1
Algorithm 1 Training with GradNorm Initialize wi(0) = 1 ∀i Initialize network weightsW Pick value for α > 0 and pick the weightsW (usually the
final layer of weights which are shared between tasks) for t = 0",4. A Toy Example,[0],[0]
"to max train steps do
Input batch xi to compute Li(t) ∀i and L(t) = ∑ i wi(t)Li(t)",4. A Toy Example,[0],[0]
"[standard forward pass] Compute G(i)W (t) and ri(t) ∀i Compute GW (t) by averaging the G (i) W (t)
Compute Lgrad = ∑ i|G (i) W (t)−GW (t)× [ri(t)]α|1 Compute GradNorm gradients∇wiLgrad, keeping targets GW (t)× [ri(t)]α constant Compute standard gradients∇WL(t) Update wi(t) 7→ wi(t+ 1) using ∇wiLgrad UpdateW(t) 7→ W(t+ 1) using∇WL(t)",4. A Toy Example,[0],[0]
"[standard
backward pass] Renormalize wi(t+ 1) so that ∑ i wi(t+ 1) = T
end for
for all loss weights wi(t), the network training will be dominated by tasks with larger loss scales that backpropagate larger gradients.",4. A Toy Example,[0],[0]
"We will demonstrate that GradNorm overcomes this issue.
",4. A Toy Example,[0],[0]
"Consider T regression tasks trained using standard squared loss onto the functions
fi(x) =",4. A Toy Example,[0],[0]
"σi tanh((B + i)x), (3)
where tanh(·) acts element-wise.",4. A Toy Example,[0],[0]
"Inputs are dimension 250 and outputs dimension 100, while B and i are constant matrices with their elements generated IID from N (0, 10) and N (0, 3.5), respectively.",4. A Toy Example,[0],[0]
Each task therefore shares information in B but also contains task-specific information i.,4. A Toy Example,[0],[0]
The σi are the key parameters within this problem; they are fixed scalars which set the scales of the outputs fi.,4. A Toy Example,[0],[0]
A higher scale for fi induces a higher expected value of squared loss for that task.,4. A Toy Example,[0],[0]
"Such tasks are harder to learn due to the higher variances in their response values, but they also backpropagate larger gradients.",4. A Toy Example,[0],[0]
"This scenario generally leads to suboptimal training dynamics when the higher σi tasks dominate the training across all tasks.
",4. A Toy Example,[0],[0]
"To train our toy models, we use a 4-layer fully-connected ReLU-activated network with 100 neurons per layer as a common trunk.",4. A Toy Example,[0],[0]
A final affine transformation layer gives T final predictions (corresponding to T different tasks).,4. A Toy Example,[0],[0]
"To ensure valid analysis, we only compare models initialized to the same random values and fed data generated from the same fixed random seed.",4. A Toy Example,[0],[0]
"The asymmetry α is set low to 0.12 for these experiments, as the output functions fi are all of the same functional form and thus we expect the asymmetry between tasks to be minimal.
",4. A Toy Example,[0],[0]
"In these toy problems, we measure the task-normalized testtime loss to judge test-time performance, which is the sum of the test loss ratios for each task, ∑ i Li(t)/Li(0).",4. A Toy Example,[0],[0]
We do this because a simple sum of losses is an inadequate performance metric for multitask networks when different loss scales exist; higher loss scale tasks will factor disproportionately highly in the loss.,4. A Toy Example,[0],[0]
"There unfortunately exists no general single scalar which gives a meaningful measure of multitask performance in all scenarios, but our toy problem was specifically designed with tasks which are statistically identical except for their loss scales σi.",4. A Toy Example,[0],[0]
"There is therefore a clear measure of overall network performance, which is the sum of losses normalized by each task’s variance σ2i - equivalent (up to a scaling factor) to the sum of loss ratios.
",4. A Toy Example,[0],[0]
"For T = 2, we choose the values (σ0, σ1)",4. A Toy Example,[0],[0]
"= (1.0, 100.0) and show the results of training in the top panels of Figure 2.",4. A Toy Example,[0],[0]
"If we train with equal weightswi = 1, task 1 suppresses task 0 from learning due to task 1’s higher loss scale.",4. A Toy Example,[0],[0]
"However, gradient normalization increases w0(t) to counteract the larger gradients coming from T1, and the improved task balance results in better test-time performance.
",4. A Toy Example,[0],[0]
The possible benefits of gradient normalization become even clearer when the number of tasks increases.,4. A Toy Example,[0],[0]
"For T = 10, we sample the σi from a wide normal distribution and plot the results in the bottom panels of Figure 2.",4. A Toy Example,[0],[0]
GradNorm significantly improves test time performance over naı̈vely weighting each task the same.,4. A Toy Example,[0],[0]
"Similarly to the T = 2 case, for T = 10 the wi(t) grow larger for smaller σi tasks.
",4. A Toy Example,[0],[0]
"For both T = 2 and T = 10, GradNorm is more stable and outperforms the uncertainty weighting proposed by (Kendall et al., 2017).",4. A Toy Example,[0],[0]
"Uncertainty weighting, which enforces that wi(t) ∼ 1/Li(t), tends to grow the weights wi(t) too large and too quickly as the loss for each task drops.",4. A Toy Example,[0],[0]
"Although such networks train quickly at the onset, the training soon deteriorates.",4. A Toy Example,[0],[0]
"This issue is largely caused by the fact that uncertainty weighting allows wi(t) to change without constraint (compared to GradNorm which ensures∑ wi(t) = T always), which pushes the global learning rate up rapidly as the network trains.
",4. A Toy Example,[0],[0]
The traces for each wi(t) during a single GradNorm run are observed to be stable and convergent.,4. A Toy Example,[0],[0]
"In Section 5.3 we will see how the time-averaged weightsEt[wi(t)] lie close to the optimal static weights, suggesting GradNorm can greatly simplify the tedious grid search procedure.",4. A Toy Example,[0],[0]
"We use two variants of NYUv2 (Nathan Silberman & Fergus, 2012) as our main datasets.",5. Application to a Large Real-World Dataset,[0],[0]
"Please refer to the Supplementary Materials for additional results on a 9-task facial landmark dataset found in (Zhang et al., 2014).",5. Application to a Large Real-World Dataset,[0],[0]
"The standard NYUv2 dataset carries depth, surface normals, and semantic
segmentation labels (clustered into 13 distinct classes) for a variety of indoor scenes in different room types (bathrooms, living rooms, studies, etc.).",5. Application to a Large Real-World Dataset,[0],[0]
"NYUv2 is relatively small (795 training, 654 test images), but contains both regression and classification labels, making it a good choice to test the robustness of GradNorm across various tasks.
",5. Application to a Large Real-World Dataset,[0],[0]
"We augment the standard NYUv2 depth dataset with flips and additional frames from each video, resulting in 90,000 images complete with pixel-wise depth, surface normals, and room keypoint labels (segmentation labels are, unfortunately, not available for these additional frames).",5. Application to a Large Real-World Dataset,[0],[0]
"Keypoint labels are professionally annotated by humans, while surface normals are generated algorithmically.",5. Application to a Large Real-World Dataset,[0],[0]
The full dataset is then split by scene for a 90/10 train/test split.,5. Application to a Large Real-World Dataset,[0],[0]
See Figure 6 for examples.,5. Application to a Large Real-World Dataset,[0],[0]
"We will generally refer to these two datasets as NYUv2+seg and NYUv2+kpts, respectively.
",5. Application to a Large Real-World Dataset,[0],[0]
All inputs are downsampled to 320 x 320 pixels and outputs to 80 x 80 pixels.,5. Application to a Large Real-World Dataset,[0],[0]
"We use these resolutions following (Lee et al., 2017), which represents the state-of-the-art in room keypoint prediction and from which we also derive our VGG-style model architecture.",5. Application to a Large Real-World Dataset,[0],[0]
These resolutions also allow us to keep models relatively slim while not compromising semantic complexity in the ground truth output maps.,5. Application to a Large Real-World Dataset,[0],[0]
"We try two different models: (1) a SegNet (Badrinarayanan et al., 2015; Lee et al., 2017) network with a symmetric VGG16 (Simonyan & Zisserman, 2014) encoder/decoder,
and (2) an FCN (Long et al., 2015) network with a modified ResNet-50",5.1. Model and General Training Characteristics,[0],[0]
"(He et al., 2016) encoder and shallow ResNet decoder.",5.1. Model and General Training Characteristics,[0],[0]
"The VGG SegNet reuses maxpool indices to perform upsampling, while the ResNet FCN learns all upsampling filters.",5.1. Model and General Training Characteristics,[0],[0]
"The ResNet architecture is further thinned (both in its filters and activations) to contrast with the heavier, more complex VGG SegNet:",5.1. Model and General Training Characteristics,[0],[0]
stride-2 layers are moved earlier and all 2048-filter layers are replaced by 1024-filter layers.,5.1. Model and General Training Characteristics,[0],[0]
"Ultimately, the VGG SegNet has 29M parameters versus 15M for the thin ResNet.",5.1. Model and General Training Characteristics,[0],[0]
All model parameters are shared amongst all tasks until the final layer.,5.1. Model and General Training Characteristics,[0],[0]
"Although we will focus on the VGG SegNet in our more in-depth analysis, by designing and testing on two extremely different network topologies we will further demonstrate GradNorm’s robustness to the choice of base architecture.
",5.1. Model and General Training Characteristics,[0],[0]
"We use standard pixel-wise loss functions for each task: cross entropy for segmentation, squared loss for depth, and cosine similarity for normals.",5.1. Model and General Training Characteristics,[0],[0]
"As in (Lee et al., 2017), for room layout we generate Gaussian heatmaps for each of 48 room keypoint types and predict these heatmaps with a pixel-wise squared loss.",5.1. Model and General Training Characteristics,[0],[0]
"Note that all regression tasks are quadratic losses (our surface normal prediction uses a cosine loss which is quadratic to leading order), allowing us to use ri(t) for each task i as a direct proxy for each task’s relative inverse training rate.
",5.1. Model and General Training Characteristics,[0],[0]
All runs are trained at a batch size of 24 across 4 Titan X GTX 12GB GPUs and run at 30fps on a single GPU at inference.,5.1. Model and General Training Characteristics,[0],[0]
All NYUv2 runs begin with a learning rate of 2e5.,5.1. Model and General Training Characteristics,[0],[0]
"NYUv2+kpts runs last 80000 steps with a learning rate
decay of 0.2 every 25000 steps.",5.1. Model and General Training Characteristics,[0],[0]
NYUv2+seg runs last 20000 steps with a learning rate decay of 0.2 every 6000 steps.,5.1. Model and General Training Characteristics,[0],[0]
"Updating wi(t) is performed at a learning rate of 0.025 for both GradNorm and the uncertainty weighting ((Kendall et al., 2017)) baseline.",5.1. Model and General Training Characteristics,[0],[0]
"All optimizers are Adam, although we find that GradNorm is insensitive to the optimizer chosen.",5.1. Model and General Training Characteristics,[0],[0]
We implement GradNorm using TensorFlow v1.2.1.,5.1. Model and General Training Characteristics,[0],[0]
In Table 1 we display the performance of GradNorm on the NYUv2+seg dataset.,5.2. Main Results on NYUv2,[0],[0]
"We see that GradNorm α = 1.5 improves the performance of all three tasks with respect to the equal-weights baseline (where wi(t) = 1 for all t,i), and either surpasses or matches (within statistical noise) the best performance of single networks for each task.",5.2. Main Results on NYUv2,[0],[0]
"The GradNorm Static network uses static weights derived from a GradNorm network by calculating the time-averaged weights Et[wi(t)] for each task during a GradNorm training run, and retraining a network with weights fixed to those values.",5.2. Main Results on NYUv2,[0],[0]
GradNorm thus can also be used to extract good values for static weights.,5.2. Main Results on NYUv2,[0],[0]
"We pursue this idea further in Section 5.3 and show that these weights lie very close to the optimal weights extracted from exhaustive grid search.
",5.2. Main Results on NYUv2,[0],[0]
"To show how GradNorm can perform in the presence of a larger dataset, we also perform extensive experiments on the NYUv2+kpts dataset, which is augmented to a factor of 50x more data.",5.2. Main Results on NYUv2,[0],[0]
The results are shown in Table 2.,5.2. Main Results on NYUv2,[0],[0]
"As with the NYUv2+seg runs, GradNorm networks outperform other multitask methods, and either matches (within noise) or surpasses the performance of single-task networks.
",5.2. Main Results on NYUv2,[0],[0]
Figure 3 shows test and training loss curves for GradNorm (α = 1.5) and baselines on the larger NYUv2+kpts dataset for our VGG SegNet models.,5.2. Main Results on NYUv2,[0],[0]
"GradNorm improves test-time depth error by ∼ 5%, despite converging to a much higher training loss.",5.2. Main Results on NYUv2,[0],[0]
"GradNorm achieves this by aggressively rate balancing the network (enforced by a high asymmetry α = 1.5), and ultimately suppresses the depth weight wdepth(t) to lower than 0.10 (see Section 5.4 for more details).",5.2. Main Results on NYUv2,[0],[0]
"The same
trend exists for keypoint regression, and is a clear signal of network regularization.",5.2. Main Results on NYUv2,[0],[0]
"In contrast, uncertainty weighting (Kendall et al., 2017) always moves test and training error in the same direction, and thus is not a good regularizer.",5.2. Main Results on NYUv2,[0],[0]
"Only results for the VGG SegNet are shown here, but the Thin ResNet FCN produces consistent results.",5.2. Main Results on NYUv2,[0],[0]
"For our VGG SegNet, we train 100 networks from scratch with random task weights on NYUv2+kpts.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
Weights are sampled from a uniform distribution and renormalized to sum to T = 3.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"For computational efficiency, we only train for 15000 iterations out of the normal 80000, and then compare the performance of that network to our GradNorm
α = 1.5 VGG SegNet network at the same 15000 steps.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"The results are shown in Figure 4.
",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even after 100 networks trained, grid search still falls short of our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"Even more remarkably, there is a strong, negative correlation between network performance and task weight distance to our time-averaged GradNorm weights Et[wi(t)].",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
"At an L2 distance of ∼ 3, grid search networks on average have almost double the errors per task compared to our GradNorm network.",5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
GradNorm has therefore found the optimal grid search weights in one single training run.,5.3. Gradient Normalization Finds Optimal Grid-Search Weights in One Pass,[0],[0]
The only hyperparameter in our algorithm is the asymmetry α.,5.4. Effects of tuning the asymmetry α,[0],[0]
"The optimal value of α for NYUv2 lies near α = 1.5, while in the highly symmetric toy example in Section 4 we used α = 0.12.",5.4. Effects of tuning the asymmetry α,[0],[0]
"This observation reinforces our characterization of α as an asymmetry parameter.
",5.4. Effects of tuning the asymmetry α,[0],[0]
"Tuning α leads to performance gains, but we found that for NYUv2, almost any value of 0",5.4. Effects of tuning the asymmetry α,[0],[0]
< α < 3 will improve network performance over an equal weights baseline (see Supplementary for details).,5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 5 shows that higher values of α tend to push the weights wi(t) further apart, which more aggressively reduces the influence of tasks which overfit or learn too quickly (in our case, depth).",5.4. Effects of tuning the asymmetry α,[0],[0]
"Remarkably, at α = 1.75 (not shown) wdepth(t) is suppressed to below 0.02 at no detriment to network performance on the depth task.",5.4. Effects of tuning the asymmetry α,[0],[0]
"Figure 6 shows visualizations of the VGG SegNet outputs on test set images along with the ground truth, for both the NYUv2+seg and NYUv2+kpts datasets.",5.5. Qualitative Results,[0],[0]
"Ground truth labels are juxtaposed with outputs from the equal weights network, 3 single networks, and our best GradNorm network.",5.5. Qualitative Results,[0],[0]
"Some
improvements are incremental, but GradNorm produces superior visual results in tasks for which there are significant quantitative improvements in Tables 1 and 2.",5.5. Qualitative Results,[0],[0]
"We introduced GradNorm, an efficient algorithm for tuning loss weights in a multi-task learning setting based on balancing the training rates of different tasks.",6. Conclusions,[0],[0]
"We demonstrated on both synthetic and real datasets that GradNorm improves multitask test-time performance in a variety of scenarios, and can accommodate various levels of asymmetry amongst the different tasks through the hyperparameter α.",6. Conclusions,[0],[0]
"Our empirical results indicate that GradNorm offers su-
perior performance over state-of-the-art multitask adaptive weighting methods and can match or surpass the performance of exhaustive grid search while being significantly less time-intensive.
",6. Conclusions,[0],[0]
"Looking ahead, algorithms such as GradNorm may have applications beyond multitask learning.",6. Conclusions,[0],[0]
"We hope to extend the GradNorm approach to work with class-balancing and sequence-to-sequence models, all situations where problems with conflicting gradient signals can degrade model performance.",6. Conclusions,[0],[0]
"We thus believe that our work not only provides a robust new algorithm for multitask learning, but also reinforces the powerful idea that gradient tuning is fundamental for training large, effective models on complex tasks.",6. Conclusions,[0],[0]
"Deep multitask networks, in which one neural network produces multiple predictive outputs, can offer better speed and performance than their single-task counterparts but are challenging to train properly.",abstractText,[0],[0]
We present a gradient normalization (GradNorm) algorithm that automatically balances training in deep multitask models by dynamically tuning gradient magnitudes.,abstractText,[0],[0]
"We show that for various network architectures, for both regression and classification tasks, and on both synthetic and real datasets, GradNorm improves accuracy and reduces overfitting across multiple tasks when compared to single-task networks, static baselines, and other adaptive multitask loss balancing techniques.",abstractText,[0],[0]
"GradNorm also matches or surpasses the performance of exhaustive grid search methods, despite only involving a single asymmetry hyperparameter α.",abstractText,[0],[0]
"Thus, what was once a tedious search process that incurred exponentially more compute for each task added can now be accomplished within a few training runs, irrespective of the number of tasks.",abstractText,[0],[0]
"Ultimately, we will demonstrate that gradient manipulation affords us great control over the training dynamics of multitask networks and may be one of the keys to unlocking the potential of multitask learning.",abstractText,[0],[0]
GradNorm: Gradient Normalization for Adaptive Loss Balancing in Deep Multitask Networks,title,[0],[0]
"Deep neural networks have become the state-of-the-art systems for image recognition (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Qiao et al., 2018; Simonyan & Zisserman, 2014; Szegedy et al., 2015; Wang et al., 2017; Zeiler & Fergus, 2013) as well as other vision tasks (Chen et al., 2015; Girshick et al., 2014; Long et al., 2015; Qiao et al., 2017; Ren et al., 2015; Shen et al., 2015; Xie & Tu, 2015).",1. Introduction,[0],[0]
"The architectures keep going deeper, e.g., from five convolutional layers (Krizhevsky et al., 2012) to 1001 layers (He et al., 2016b).",1. Introduction,[0],[0]
"The benefit of deep architectures is their strong learning capacities because each new layer can potentially introduce more non-linearities and typically uses larger receptive fields (Simonyan & Zisserman, 2014).",1. Introduction,[0],[0]
"In addition, adding certain types of layers (e.g. (He et al., 2016b)) will not harm the performance theoretically since they can just learn identity mapping.",1. Introduction,[0],[0]
"This makes stacking up layers more appealing in the network designs.
",1. Introduction,[0],[0]
1Johns Hopkins University 2Shanghai University 3Hikvision Research.,1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Siyuan Qiao <siyuan.qiao@jhu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Although deeper architectures usually lead to stronger learning capacities, cascading convolutional layers (e.g. VGG (Simonyan & Zisserman, 2014)) or blocks (e.g. ResNet (He et al., 2016a)) is not necessarily the only method to achieve this goal.",1. Introduction,[0],[0]
"In this paper, we present a new way to increase the depth of the networks as an alternative to stacking up convolutional layers or blocks.",1. Introduction,[0],[0]
Figure 2 provides an illustration that compares our proposed convolutional network that gradually updates the feature representations against the traditional convolutional network that computes its output simultaneously.,1. Introduction,[0],[0]
"By only adding an ordering to the channels without any additional computation, the later computed channels become deeper than the corresponding ones in the traditional convolutional network.",1. Introduction,[0],[0]
We refer to the neural networks with the proposed computation orderings on the channels as Gradually Updated Neural Networks (GUNN).,1. Introduction,[0],[0]
Figure 1 provides two examples of architecture designs based on cascading building blocks and GUNN.,1. Introduction,[0],[0]
"Without repeating the building blocks, GUNN increases the depths of the networks as well as their learning capacities.
",1. Introduction,[0],[0]
It is clear that converting plain networks to GUNN increases the depths of the networks without any additional computations.,1. Introduction,[0],[0]
"What is less obvious is that GUNN in fact eliminates the overlap singularities inherent in the loss landscapes of the cascading-based convolutional networks, which have been shown to adversely affect the training of deep neural networks as well as their performances (Wei et al., 2008;
Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"Overlap singularity is when internal neurons collapse into each other, i.e. they are unidentifiable by their activations.",1. Introduction,[0],[0]
"It happens in the networks, increases the training difficulties and degrades the performances (Orhan & Pitkow, 2018).",1. Introduction,[0],[0]
"However, if a plain network is converted to GUNN, the added computation orderings will break the symmetry between the neurons.",1. Introduction,[0],[0]
We prove that the internal neurons in GUNN are impossible to collapse into each other.,1. Introduction,[0],[0]
"As a result, the effective dimensionality can be kept during training and the model will be free from the degeneracy caused by collapsed neurons.",1. Introduction,[0],[0]
"Reflected in the training dynamics and the performances, this means that converting to GUNN will make the plain networks easier to train and perform better.",1. Introduction,[0],[0]
"Figure 3 compares the training dynamics of a 15-layer plain network on CIFAR-10 dataset (Krizhevsky & Hinton, 2009) before and after converted to GUNN.
",1. Introduction,[0],[0]
"In this paper, we test our proposed GUNN on highly competitive benchmark datasets, i.e. CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015).",1. Introduction,[0],[0]
Experimental results demonstrate that our proposed GUNNbased networks achieve the state-of-the-art performances compared with the previous cascading-based architectures.,1. Introduction,[0],[0]
"The research focuses of image recognition have moved from feature designs (Dalal & Triggs, 2005; Lowe, 2004) to architecture designs (He et al., 2016a; Huang et al., 2017b; Krizhevsky et al., 2012; Sermanet et al., 2014; Simonyan
& Zisserman, 2014; Szegedy et al., 2015; Xie et al., 2017; Zeiler & Fergus, 2013) due to the recent success of the deep neural networks.",2. Related Work,[0],[0]
"Highway Networks (Srivastava et al., 2015) proposed architectures that can be trained end-to-end with more than 100 layers.",2. Related Work,[0],[0]
The main idea of Highway Networks is to use bypassing paths.,2. Related Work,[0],[0]
"This idea was further investigated in ResNet (He et al., 2016a), which simplifies the bypassing paths by using only identity mappings.",2. Related Work,[0],[0]
"As learning ultra-deep networks became possible, the depths of the models have increased tremendously.",2. Related Work,[0],[0]
"ResNet with pre-activation (He et al., 2016b) and ResNet with stochastic depth (Huang et al., 2016) even managed to train neural networks with more than 1000 layers.",2. Related Work,[0],[0]
"FractalNet (Larsson et al., 2016) argued that in addition to summation, concatenation also helps train a deep architecture.",2. Related Work,[0],[0]
"More recently, ResNeXt (Xie et al., 2017) used group convolutions in ResNet and outperformed the original ResNet.",2. Related Work,[0],[0]
"DenseNet (Huang et al., 2017b) proposed an architecture with dense connections by feature concatenation.",2. Related Work,[0],[0]
"Dual Path Net (Chen et al., 2017) finds a middle point between ResNet and DenseNet by concatenating them in two paths.",2. Related Work,[0],[0]
"Unlike the above cascading-based methods, GUNN eliminates the overlap singularities caused by the architecture symmetry.",2. Related Work,[0],[0]
"The detailed analyses can be found in Section 4.3.
",2. Related Work,[0],[0]
"Alternative to increasing the depth of the neural networks, another trend is to increase the widths of the networks.",2. Related Work,[0],[0]
"GoogleNet (Szegedy et al., 2015; 2016) proposed an Inception module to concatenate feature maps produced by different filters.",2. Related Work,[0],[0]
"Following ResNet (He et al., 2016a), the WideResNet (Zagoruyko & Komodakis, 2016) argued that compared with increasing the depth, increasing the width of the networks can be more effective in improving the performances.",2. Related Work,[0],[0]
"Besides varying the width and the depth, there are also other design strategies for deep neural networks (Hariharan et al., 2015; Kontschieder et al., 2015; Pezeshki et al., 2016; Rasmus et al., 2015; Yang & Ramanan, 2015).",2. Related Work,[0],[0]
"Deeply-Supervised Nets (Lee et al., 2014) used auxiliary classifiers to provide direct supervisions for the internal layers.",2. Related Work,[0],[0]
"Network in Network (Lin et al., 2013) adds micro perceptrons to the convolutional layers.",2. Related Work,[0],[0]
"We consider a feature transformation F : Rm×n → Rm×n, where n denotes the channel of the features and m denotes the feature location on the 2-D feature map.",3.1. Feature Update,[0],[0]
"For example, F can be a convolutional layer with n channels for both the input and the output.",3.1. Feature Update,[0],[0]
Let x ∈,3.1. Feature Update,[0],[0]
Rm×n be the input and y ∈,3.1. Feature Update,[0],[0]
"Rm×n be the output, we have
y = F(x) (1)
Suppose that F can be decomposed into channel-wise transformation Fc(·) that are independent with eath other, then for any location k and channel c we have
ykc = Fc(xr(k))",3.1. Feature Update,[0],[0]
"(2)
where xr(k) denotes the receptive field of the location k",3.1. Feature Update,[0],[0]
"and Fc denotes the transformation on channel c.
Let UC denote a feature update on channel set C, i.e.,
UC(x) :",3.1. Feature Update,[0],[0]
"y k c = Fc(xr(k)),∀c ∈ C, k ykc",3.1. Feature Update,[0],[0]
"= x k c ,∀c",3.1. Feature Update,[0],[0]
"∈ C, k
(3)
",3.1. Feature Update,[0],[0]
"Then, UC = F when C = {1, ..., n}.",3.1. Feature Update,[0],[0]
"By defining the feature update UC on channel set C, the commonly used one-layer CNN is a special case of feature updates where every channel is updated simultaneously.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, we can also update the channels gradually.",3.2. Gradually Updated Neural Networks,[0],[0]
"For example, the proposed GUNN can be formulated by
GUNN(x) =",3.2. Gradually Updated Neural Networks,[0],[0]
(Ucl ◦ Uc(l−1),3.2. Gradually Updated Neural Networks,[0],[0]
"◦ ... ◦ Uc2 ◦ Uc1)(x)
",3.2. Gradually Updated Neural Networks,[0],[0]
"where l⋃
i=1
ci = {1, 2, ..., n} and ci ∩ cj = Φ, ∀i 6= j
(4) When l = 1, GUNN is equivalent to F .
",3.2. Gradually Updated Neural Networks,[0],[0]
"Note that the number of parameters and computation of GUNN are the same as those of the corresponding F for any partitions c1, ..., cl of {1, ..., n}.",3.2. Gradually Updated Neural Networks,[0],[0]
"However, by decomposing F into channel-wise transformations and sequentially applying them, the later computed channels are deeper than the previous ones.",3.2. Gradually Updated Neural Networks,[0],[0]
"As a result, the depth of the network can be increased, as well as the network’s learning capacity.",3.2. Gradually Updated Neural Networks,[0],[0]
"We consider the residual learning proposed by ResNet (He et al., 2016a) in our model.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Specifically, we consider the channel-wise transformation Fc : Rm×n → Rm×1 to be
Fc(x) = Gc(x) + xc (5)
",3.3. Channel-wise Update by Residual Learning,[0],[0]
Algorithm 1 Back-propagation for GUNN Input :U(·) =,3.3. Channel-wise Update by Residual Learning,[0],[0]
(Ucl ◦ Uc(l−1),3.3. Channel-wise Update by Residual Learning,[0],[0]
"◦ ... ◦ Uc1)(·), input x,
output y = U(x), gradients ∂L/∂y, and parameters Θ for U .
",3.3. Channel-wise Update by Residual Learning,[0],[0]
"Output :∂L/∂Θ, ∂L/∂x ∂L/∂x← ∂L/∂y for i←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"l to 1 do
yc ←",3.3. Channel-wise Update by Residual Learning,[0],[0]
"xc, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"∈ ci ∂L/∂y, ∂L/∂Θci ← BP(y, ∂L/∂x, Uci ,Θci) (∂L/∂x)c ← (∂L/∂y)c, ∀c ∈ ci (∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c",3.3. Channel-wise Update by Residual Learning,[0],[0]
"6∈ ci
end
where Gc is a convolutional neural network Gc : Rm×n → Rm×1.",3.3. Channel-wise Update by Residual Learning,[0],[0]
"The motivation of expressing F in a residual learning manner is to reduce overlap singularities (Orhan & Pitkow, 2018), which will be discussed in Section 4.",3.3. Channel-wise Update by Residual Learning,[0],[0]
Here we show the backpropagation algorithm for learning the parameters in GUNN that uses the same amount of computations and memory as in F .,3.4. Learning GUNN by Backpropagation,[0],[0]
"In Eq. 4, let the feature update Uci be parameterized by Θci .",3.4. Learning GUNN by Backpropagation,[0],[0]
"Let BP(x, ∂L/∂y, f,Θ) be the back-propagation algorithm for differentiable function y = f(x; Θ) with the loss L and the parameters Θ. Algorithm 1 presents the back-propagation algorithm for GUNN.",3.4. Learning GUNN by Backpropagation,[0],[0]
"Since Uci has the residual structures (He et al., 2016a), the last two steps can be merged into
(∂L/∂x)c ← (∂L/∂x)c + (∂L/∂y)c, ∀c (6)
which further simplifies the implementation.",3.4. Learning GUNN by Backpropagation,[0],[0]
It is easy to see that converting networks to GUNN-based does not increase the memory usage in feed-forwarding.,3.4. Learning GUNN by Backpropagation,[0],[0]
"Given Algorithm 1, converting networks to GUNN will not affect the memory in both the training and the evaluation.",3.4. Learning GUNN by Backpropagation,[0],[0]
Overlap singularities are inherent in the loss landscapes of some network architectures which are caused by the nonidentifiability of subsets of the neurons.,4. GUNN Eliminates Overlap Singularities,[0],[0]
"They are identified and discussed in previous work (Wei et al., 2008; Anandkumar & Ge, 2016; Orhan & Pitkow, 2018), and are shown to be harmful for the performances of deep networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Intuitively, overlap singularities exist in architectures where the internal neurons collapse into each other.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"As a result, the models are degenerate and the effective dimensionality is reduced.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"(Orhan & Pitkow, 2018) demonstrated through experiments that residual learning (see Eq. 5) helps to reduce the overlap singularities in deep networks, which partly explains the exceptional performances of ResNet (He et al., 2016a) compared with plain networks.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"In the following, we first use linear transformation as an example to demonstrate
how GUNN-based networks break the overlap singularities.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Then, we generalize the results to ReLU DNN.",4. GUNN Eliminates Overlap Singularities,[0],[0]
"Finally, we compare GUNN with the previous state-of-the-art network architectures from the perspective of singularity elimination.",4. GUNN Eliminates Overlap Singularities,[0],[0]
Consider a linear function y = f(x) :,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Rn → Rn such that
yi = n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (7)
Suppose that there exists a pair of collapsed neurons yp and yq (p < q).",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, for ∀x, yp = yq, and the equality holds after any number of gradient descents, i.e. ∆yp = ∆yq .
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
Eq. 7 describes a plain network.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"The solution for the existence of yp and yq is that ωp,j = ωq,j ,∀j.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This is the case that is mostly discussed previously, which happens in the networks and degrades the performances.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When we add the residual learning, Eq. 7 becomes
yi = xi + n∑ j=1 ωi,jxj , ∀i ∈ {1, .., n} (8)
Collapsed neurons require that ωp,p + 1 = ωq,p, ωq,q + 1 = ωp,q.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"This will make the collapse of yp and yq very hard when ω is initialized from a normal distribution N (0, √ 2/n) as in ResNet, but still possible.
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Next, we convert Eq. 8 to GUNN, i.e.,
yi = xi + i−1∑ j=1 ωi,jyj + n∑ j=i ωi,jxj , ∀i ∈ {1, .., n} (9)
Suppose that yp and yq (p < q) collapse.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Consider ∆y, the value difference at x after one step of gradient descent on ω with input x, ∂L/∂y and learning rate .",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"When → 0,
∆yi = ∂L
∂yi ( i−1∑ j=1 y2j + n∑ j=i x2j )",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"+ i−1∑ j=1 ωi,j∆yj (10)
",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"As ∆yp = ∆yq,∀x, we have ωq,j = 0, ∀j : p",4.1. Overlap Singularities in Linear Transformations,[0],[0]
< j < q.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"But this condition will be broken in the next update; thus, q = p + 1.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Then, we derive that yp = yq = 0.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
But these will also be broken in the next step of gradient descent optimization.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"Hence, yp and yq cannot collapse into each other.",4.1. Overlap Singularities in Linear Transformations,[0],[0]
The complete proof can be found in the appendix.,4.1. Overlap Singularities in Linear Transformations,[0],[0]
"In practice, architectures are usually composed of several linear layers and non-linearity layers.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Analyzing all the possible architectures is beyond our scope.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Here, we discuss the commonly used ReLU DNN, in which only linear transformations and ReLUs are used by simple layer cascading.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Following the notations in §3, we use y = G(x) + x, in which G(x) is a ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Note that G is continuous piecewise linear (PWL) function (Arora et al., 2018), which means that there exists a finite set of polyhedra whose union is Rn, and G is affine linear over each polyhedron.
",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Suppose that we convert G(x)+x to GUNN and there exists a pair of collapsed neurons yp and yq (p < q).,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, the set of polyhedra for yp is the same as for yq.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
Let P be a polyhedron for yp and yq defined above.,4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Then, ∀x,P, i,
yi = xi + i−1∑ j=1 ωi,j(P)yj + n∑ j=i ωi,j(P)xj (11)
where ω(P) denotes the parameters for polyhedron P. Note that on each P, y is a function of x in the form of Eq. 9; hence, yp and yq cannot collapse into each other.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"Since the union of all polyhedra is Rn, we conclude that GUNN eliminates the overlap singularities in ReLU DNN.",4.2. Overlap Singularities in ReLU DNN,[0],[0]
"The previous two subsections consider the GUNN conversion where |ci| = 1,∀i (see Eq. 4).",4.3. Discussions and Comparisons,[0],[0]
But this will slow down the computation on GPU due to the data dependency.,4.3. Discussions and Comparisons,[0],[0]
"Without specialized hardware or library support, we decide to increase |ci| to > 10.",4.3. Discussions and Comparisons,[0],[0]
"The resulted models run at the speed between ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b).",4.3. Discussions and Comparisons,[0],[0]
But this change introduces singularities into the channels from the same set ci.,4.3. Discussions and Comparisons,[0],[0]
"Then, the residual learning helps GUNN to reduce the singularities within the same set ci since we initialize the parameters from a normal distributionN (0, √ 2/n).",4.3. Discussions and Comparisons,[0],[0]
"We will compare the results of GUNN with and without residual learning in the experiments.
",4.3. Discussions and Comparisons,[0],[0]
We compare GUNN with the state-of-the-art architectures from the perspective of overlap singularities.,4.3. Discussions and Comparisons,[0],[0]
"ResNet (He et al., 2016a) and its variants use residual learning, which reduces but cannot eliminate the singularities.",4.3. Discussions and Comparisons,[0],[0]
"ResNeXt (Xie et al., 2017) uses group convolutions to break the symmetry between groups, which further helps to avoid neuron collapses.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet (Huang et al., 2017b) concatenates the outputs of layers as the input to the next layer.",4.3. Discussions and Comparisons,[0],[0]
"DenseNet and GUNN both create dense connections, while DenseNet reuses the outputs by concatenating and GUNN by adding them back to the inputs.",4.3. Discussions and Comparisons,[0],[0]
But the channels within the same layer of DenseNet are still possible to collapse into each other since they are symmetric.,4.3. Discussions and Comparisons,[0],[0]
"In contrast, adding back makes residual learning possible in GUNN.",4.3. Discussions and Comparisons,[0],[0]
This makes residual learning indispensable in GUNN-based networks.,4.3. Discussions and Comparisons,[0],[0]
"In this section, we will present the details of our architectures for the CIFAR (Krizhevsky & Hinton, 2009) and ImageNet (Russakovsky et al., 2015) datasets.",5. Network Architectures,[0],[0]
"Since the proposed GUNN is a method for increasing the depths of the convolutional networks, specifying the architectures to be converted is equivalent to specifying the GUNN-based architectures.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The architectures before conversion, the Simultaneously Updated Neural Networks (SUNN), become natural baselines for our proposed GUNN networks.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We first study what baseline architectures can be converted.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There are two assumptions about the feature transformation F (see Eq. 1): (1) the input and the output sizes are the same, and (2) F is channel-wise decomposable.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the first assumption, we will first use a convolutional layer with Batch Normalization (Ioffe & Szegedy, 2015) and ReLU (Nair & Hinton, 2010) to transform the feature space to a new space where the number of the channels is wanted.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To satisfy the second assumption, instead of directly specifying the transform F , we focus on designing Fci , where ci is a subset of the channels (see Eq. 4).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"To be consistent with the term update used in GUNN and SUNN, we refer to Fci as the update units for channels ci.
Bottleneck Update Units In the architectures proposed in this paper, we adopt bottleneck neural networks as shown in Figure 4 for the update units for both the SUNN and GUNN.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Suppose that the update unit maps the input features of channel size nin to the output features of size nout.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Each unit contains three convolutional layers.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The first convolutional layer transforms the input features to K × nout using a 1× 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The second convolutional layer is of kernel size 3× 3, stride 1, and padding 1, outputting the features of size K × nout.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The third layer computes the features of size nout using a 1 × 1 convolutional layer.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The output is then added back to the input, following the residual architecture proposed in ResNet (He et al., 2016a).",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We add batch normalization layer (Ioffe & Szegedy, 2015) and ReLU layer (Nair & Hinton, 2010) after the first and the second convolutional layers, while only adding batch
normalization layer after the third layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Stacking up M update units also generates a new one.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In total, we have two hyperparameters for designing an update unit: the expansion rate K and the number of the 3-layer update units M .
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"One Resolution, One Representation Our architectures will have only one representation at one resolution besides the pooling layers and the convolutional layers that initialize the needed numbers of channels.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
Take the architecture in Table 1 as an example.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
There are two processes for each resolution.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"The first one is the transition process, which computes the initial features with the dimensions of the next resolution, then down samples it to 1/4 using a 2×2 average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
A convolutional operation is needed here because F is assumed to have the same input and output sizes.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The next process is using GUNN to update this feature space gradually.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Each channel will only be updated once, and all channels will be updated after this process.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Unlike most of the previous networks, after this two processes, the feature transformations at this resolution are complete.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"There will be no more convolutional layers or blocks following this feature representation, i.e., one resolution, one representation.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, the network will compute the initial features for the next resolution, or compute the final vector representation of the entire image by a global average pooling.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"By designing networks in this way, SUNN networks usually have about 20 layers before converting to GUNN-based networks.
",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Channel Partitions With the clearly defined update units, we can easily build SUNN and GUNN layers by using the units to update the representations following Eq. 4.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
The hyperparameters for the SUNN/GUNN layer are the number of the channels N and the partition over those channels.,5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"In our proposed architectures, we evenly partition the channels into P segments.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Then, we can useN and P to represent the configuration of a layer.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"Together with the hyperparameters in the update units, we have four hyperparameters to tune for one SUNN/GUNN layer, i.e. {N,P,K,M}.",5.1. Simultaneously Updated Neural Networks and Gradually Updated Neural Networks,[0],[0]
"We have implemented two neural networks based on GUNN to compete with the previous state-of-the-art methods on CIFAR datasets, i.e., GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
Table 1 shows the big picture of GUNN-15.,5.2. Architectures for CIFAR,[0],[0]
"Here, we present the details of the hyperparameter settings for GUNN-15 and GUNN-24.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-15, we have three GUNN layers, Conv2, Conv3 and Conv4.",5.2. Architectures for CIFAR,[0],[0]
"The configuration for Conv2 is {N = 240, P = 20,K = 2,M = 1}, the configuration for Conv3 is {N = 300, P = 25,K = 2,M = 1}, and the configuration for Conv4 is {N = 360, P = 30,K = 2,M = 1}.",5.2. Architectures for CIFAR,[0],[0]
"For GUNN-24, based on GUNN-15, we change the number of output channels of Conv1 to 720, Trans1 to 900, Trans2 to 1080, and Trans3 to 1080.",5.2. Architectures for CIFAR,[0],[0]
"The hyperparameters are {N = 720, P = 20,K = 3,M = 2} for
Conv2, {N = 900, P = 25,K = 3,M = 2} for Conv3, and {N = 1080, P = 30,K = 3,M = 2} for Conv3.",5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-15 is 1585746 for CIFAR10 and 1618236 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
The number of parameters of GUNN-24 is 29534106 for CIFAR-10 and 29631396 for CIFAR-100.,5.2. Architectures for CIFAR,[0],[0]
"The GUNN-15 is aimed to compete with the methods published in an early stage by using a much smaller model, while GUNN-24 is targeted at comparing with ResNeXt (Xie et al., 2017) and DenseNet (Huang et al., 2017b) to get the state-of-the-art performance.",5.2. Architectures for CIFAR,[0],[0]
We implement a neural network GUNN-18 to compete with the state-of-the-art neural networks on ImageNet with a similar number of parameters.,5.3. Architectures for ImageNet,[0],[0]
Table 2 shows the big picture of the neural network architecture of GUNN-18.,5.3. Architectures for ImageNet,[0],[0]
"Here, we present the detailed hyperparameters for the GUNN layers in GUNN-18.",5.3. Architectures for ImageNet,[0],[0]
"The GUNN layers include Conv2, Conv3, Conv4 and Conv5.",5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 400, P = 10,K = 2,M = 1} for Conv2,
{N = 800, P = 20,K = 2,M = 1} for Conv3, {N = 1600, P = 40,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 28909736.,5.3. Architectures for ImageNet,[0],[0]
"The GUNN-18 is targeted at competing with the previous state-of-the-art methods that have similar numbers of parameters, e.g., ResNet50 (Xie et al., 2017), ResNeXt-50 (Xie et al., 2017) and DenseNet-264 (Huang et al., 2017b).
",5.3. Architectures for ImageNet,[0],[0]
We also implement a wider GUNN-based neural networks Wide-GUNN-18 for better capacities.,5.3. Architectures for ImageNet,[0],[0]
"The hyperparameters are {N = 1200, P = 30,K = 2,M = 1} for Conv2, {N = 1600, P = 40,K = 2,M = 1} for Conv3, {N = 2000, P = 50,K = 2,M = 1} for Conv4 and {N = 2000, P = 50,K = 2,M = 1} for Conv5.",5.3. Architectures for ImageNet,[0],[0]
The number of parameters is 45624936.,5.3. Architectures for ImageNet,[0],[0]
"The Wide-GUNN-18 is targeted at competing with ResNet-101, ResNext-101, DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",5.3. Architectures for ImageNet,[0],[0]
"In this section, we demonstrate the effectiveness of the proposed GUNN on several benchmark datasets.",6. Experiments,[0],[0]
"CIFAR CIFAR (Krizhevsky & Hinton, 2009) has two color image datasets: CIFAR-10 (C10) and CIFAR-100 (C100).",6.1. Benchmark Datasets,[0],[0]
Both datasets consist of natural images with the size of 32× 32 pixels.,6.1. Benchmark Datasets,[0],[0]
"The CIFAR-10 dataset has 10 categories, while the CIFAR-100 dataset has 100 categories.",6.1. Benchmark Datasets,[0],[0]
"For both of the datasets, the training and test set contain 50, 000 and 10, 000 images, respectively.",6.1. Benchmark Datasets,[0],[0]
"To fairly compare our method with the state-of-the-arts (He et al., 2016a; Huang et al., 2017b; 2016; Larsson et al., 2016; Lee et al., 2014; Lin et al., 2013; Romero et al., 2014; Springenberg et al., 2014; Srivastava et al., 2015; Xie et al., 2017), we use the same training and testing strategies, as well as the data processing methods.",6.1. Benchmark Datasets,[0],[0]
"Specifically, we adopt a commonly used data augmentation scheme, i.e., mirroring and shifting, for these two datasets.",6.1. Benchmark Datasets,[0],[0]
"We use channel means and standard derivations to normalize the images for data pre-processing.
",6.1. Benchmark Datasets,[0],[0]
"ImageNet The ImageNet dataset (Russakovsky et al., 2015) contains about 1.28 million color images for training and 50, 000 for validation.",6.1. Benchmark Datasets,[0],[0]
The dataset has 1000 categories.,6.1. Benchmark Datasets,[0],[0]
"We adopt the same data augmentation methods as in the state-of-the-art architectures (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017) for training.",6.1. Benchmark Datasets,[0],[0]
"For testing, we use single-crop at the size of 224× 224.",6.1. Benchmark Datasets,[0],[0]
"Following the
state-of-the-arts (He et al., 2016a;b; Huang et al., 2017b; Xie et al., 2017), we report the validation error rates.",6.1. Benchmark Datasets,[0],[0]
We train all of our networks using stochastic gradient descents.,6.2. Training Details,[0],[0]
"On CIFAR-10/100 (Krizhevsky & Hinton, 2009), the initial learning rate is set to 0.1, the weight decay is set to 1e−4, and the momentum is set to 0.9 without dampening.",6.2. Training Details,[0],[0]
We train the models for 300 epochs.,6.2. Training Details,[0],[0]
The learning rate is divided by 10 at 150th epoch and 225th epoch.,6.2. Training Details,[0],[0]
"We set the batch size to 64, following (Huang et al., 2017b).",6.2. Training Details,[0],[0]
"All the results reported for CIFAR, regardless of the detailed configurations, were trained using 4 NVIDIA Titan X GPUs with the data parallelism.",6.2. Training Details,[0],[0]
"On ImageNet (Russakovsky et al., 2015), the learning rate is also set to 0.1 initially, and decreases following the schedule in DenseNet (Huang et al., 2017b).",6.2. Training Details,[0],[0]
The batch size is set to 256.,6.2. Training Details,[0],[0]
"The network parameters are also initialized following (He et al., 2016a).",6.2. Training Details,[0],[0]
We use 8 Tesla V100 GPUs with the data parallelism to get the reported results.,6.2. Training Details,[0],[0]
"Our results are directly comparable with ResNet, WideResNet, ResNeXt and DenseNet.",6.2. Training Details,[0],[0]
We train two models GUNN-15 and GUNN-24 for the CIFAR-10/100 dataset.,6.3. Results on CIFAR,[0],[0]
Table 3 shows the comparisons between our method and the previous state-of-the-art methods.,6.3. Results on CIFAR,[0],[0]
Our method GUNN achieves the best results in the test of both the single model and the ensemble test.,6.3. Results on CIFAR,[0],[0]
"Here, we use Snapshot Ensemble (Huang et al., 2017a).
",6.3. Results on CIFAR,[0],[0]
Baseline Methods,6.3. Results on CIFAR,[0],[0]
Here we present the details of baseline methods in Table 3.,6.3. Results on CIFAR,[0],[0]
"The performances of ResNet (He et al., 2016a) are reported in Stochastic Depth (Huang et al., 2016) for both C10 and C100.",6.3. Results on CIFAR,[0],[0]
"The WideResNet (Zagoruyko & Komodakis, 2016) WRN-40-10 is reported in their official code repository on GitHub.",6.3. Results on CIFAR,[0],[0]
"The ResNeXt in the third group is of configuration 16 × 64d, which has the best result reported in the paper (Xie et al., 2017).",6.3. Results on CIFAR,[0],[0]
"The DenseNet is of configuration DenseNet-BC (k = 40), which achieves the best performances on CIFAR-10/100.",6.3. Results on CIFAR,[0],[0]
"The Snapshot Ensemble (Huang et al., 2017a) uses 6 DenseNet-100 to ensemble during inference.",6.3. Results on CIFAR,[0],[0]
"We do not compare with methods that use more data augmentation (e.g. (Zhang et al., 2017)) or stronger regularizations (e.g. (Gastaldi, 2017)) for the fairness of comparison.
",6.3. Results on CIFAR,[0],[0]
"Ablation Study For ablation study, we compare GUNN with SUNN, i.e., the networks before the conversion.",6.3. Results on CIFAR,[0],[0]
"Table 5 shows the comparison results, which demonstrate the effectiveness of GUNN.",6.3. Results on CIFAR,[0],[0]
We also compare the performances of GUNN with and without residual learning.,6.3. Results on CIFAR,[0],[0]
"We evaluate the GUNN on the ImageNet classification task, and compare our performances with the state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"These methods include VGGNet (Simonyan & Zisserman, 2014), ResNet (He et al., 2016a), ResNeXt (Xie
et al., 2017), DenseNet (Huang et al., 2017b), DPN (Chen et al., 2017) and SENet (Hu et al., 2017).",6.4. Results on ImageNet,[0],[0]
The comparisons are shown in Table 4.,6.4. Results on ImageNet,[0],[0]
"The results of ours, ResNeXt, and DenseNet are directly comparable as these methods use the same framework for training and testing networks.",6.4. Results on ImageNet,[0],[0]
"Table 4 groups the methods by their numbers of parameters, except VGGNet which has 1.38× 108 parameters.
",6.4. Results on ImageNet,[0],[0]
"The results presented in Table 4 demonstrate that with the similar number of parameters, GUNN can achieve comparable performances with the previous state-of-the-art methods.",6.4. Results on ImageNet,[0],[0]
"For GUNN-18, we also conduct an ablation experiment by comparing the corresponding SUNN with GUNN of the same configuration.",6.4. Results on ImageNet,[0],[0]
"Consistent with the experimental results on the CIFAR-10/100 dataset, the proposed GUNN improves the accuracy on ImageNet dataset.",6.4. Results on ImageNet,[0],[0]
"In this paper, we propose Gradually Updated Neural Network (GUNN), a novel, simple yet effective method to increase the depths of neural networks as an alternative to cascading layers.",7. Conclusions,[0],[0]
"GUNN is based on Convolutional Neural Networks (CNNs), but differs from CNNs in the way of computing outputs.",7. Conclusions,[0],[0]
The outputs of GUNN are computed gradually rather than simultaneously as in CNNs in order to increase the depth.,7. Conclusions,[0],[0]
"Essentially, GUNN assumes the input and the output are of the same size and adds a computation ordering to the channels.",7. Conclusions,[0],[0]
The added ordering increases the receptive fields and non-linearities of the later computed channels.,7. Conclusions,[0],[0]
"Moreover, it eliminates the overlap singularities inherent in the traditional convolutional networks.",7. Conclusions,[0],[0]
We test GUNN on the task of image recognition.,7. Conclusions,[0],[0]
"The evaluations are done in three highly competitive benchmarks, CIFAR10, CIFAR-100 and ImageNet.",7. Conclusions,[0],[0]
The experimental results demonstrate the effectiveness of the proposed GUNN on image recognition.,7. Conclusions,[0],[0]
"In the future, since the proposed GUNN can be used to replace CNNs in other neural networks, we will study the applications of GUNN in other visual tasks, such as object detection and semantic segmentation.",7. Conclusions,[0],[0]
"We thank Wanyu Huang, Huiyu Wang and Chenxi Liu for their insightful comments and suggestions.",Acknowledgments,[0],[0]
We gratefully acknowledge funding supports from NSF award CCF-1317376 and ONR N00014-15-1-2356.,Acknowledgments,[0],[0]
This work was also supported in part by the National Natural Science Foundation of China under Grant 61672336.,Acknowledgments,[0],[0]
Depth is one of the keys that make neural networks succeed in the task of large-scale image recognition.,abstractText,[0],[0]
The state-of-the-art network architectures usually increase the depths by cascading convolutional layers or building blocks.,abstractText,[0],[0]
"In this paper, we present an alternative method to increase the depth.",abstractText,[0],[0]
"Our method is by introducing computation orderings to the channels within convolutional layers or blocks, based on which we gradually compute the outputs in a channel-wise manner.",abstractText,[0],[0]
"The added orderings not only increase the depths and the learning capacities of the networks without any additional computation costs, but also eliminate the overlap singularities so that the networks are able to converge faster and perform better.",abstractText,[0],[0]
Experiments show that the networks based on our method achieve the state-of-the-art performances on CIFAR and ImageNet datasets.,abstractText,[0],[0]
Gradually Updated Neural Networks for Large-Scale Image Recognition,title,[0],[0]
"In recent years, there has been an explosion of interest in sequence labelling tasks.",1. Introduction,[0],[0]
"Connectionist Temporal Classification (CTC) loss (Graves et al., 2006) and Sequenceto-sequence (seq2seq) models (Cho et al., 2014; Sutskever et al., 2014) present powerful approaches to multiple applications, such as Automatic Speech Recognition (ASR) (Chan et al., 2016a; Hannun et al., 2014; Bahdanau et al.,
*Equal contribution 1Baidu Silicon Valley AI Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089, USA.",1. Introduction,[0],[0]
"Correspondence to: Hairong Liu <liuhairong@baidu.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
2016), machine translation (Sébastien et al., 2015), and parsing (Vinyals et al., 2015).",1. Introduction,[0],[0]
"These methods are based on 1) a fixed and carefully chosen set of basic units, such as words (Sutskever et al., 2014), phonemes (Chorowski et al., 2015) or characters (Chan et al., 2016a), and 2) a fixed and pre-determined decomposition of target sequences into these basic units.",1. Introduction,[0],[0]
"While these two preconditions greatly simplify the problems, especially the training processes, they are also strict and unnecessary constraints, which usually lead to suboptimal solutions.",1. Introduction,[0],[0]
"CTC models are especially harmed by fixed basic units in target space, because they build on the independence assumption between successive outputs in that space - an assumption which is often violated in practice.
",1. Introduction,[0],[0]
"The problem with fixed set of basic units is obvious: it is really hard, if not impossible, to determine the optimal set of basic units beforehand.",1. Introduction,[0],[0]
"For example, in English ASR, if we use words as basic units, we will need to deal with the large vocabulary-sized softmax, as well as rare words and data sparsity problem.",1. Introduction,[0],[0]
"On the other hand, if we use characters as basic units, the model is forced to learn the complex rules of English spelling and pronunciation.",1. Introduction,[0],[0]
"For example, the ""oh"" sound can be spelled in any of following ways, depending on the word it occurs in - { o, oa, oe, ow, ough, eau, oo, ew }.",1. Introduction,[0],[0]
"While CTC can easily model commonly co-occuring grams together, it is impossible to give roughly equal probability to many possible spellings when transcribing unseen words.",1. Introduction,[0],[0]
"Most speech recognition systems model phonemes, sub-phoneme units and senones e.g, (Xiong et al., 2016a) to get around these problems.",1. Introduction,[0],[0]
"Similarly, state-of-the-art neural machine translation systems use pre-segmented word pieces e.g, (Wu et al., 2016a) aiming to find the best of both worlds.
",1. Introduction,[0],[0]
"In reality, groups of characters are typically cohesive units for many tasks.",1. Introduction,[0],[0]
"For the ASR task, words can be decomposed into groups of characters that can be associated with sound (such as ‘tion’ and ‘eaux’).",1. Introduction,[0],[0]
"For the machine translation task, there may be values in decomposing words as root words and extensions (so that meaning may be shared explicitly between ‘paternal’ and ‘paternity’).",1. Introduction,[0],[0]
"Since this information is already available in the training data, it is perhaps, better to let the model figure it out by itself.",1. Introduction,[0],[0]
"At the same time, it raises another import question: how to de-
compose a target sequence into basic units?",1. Introduction,[0],[0]
"This is coupled with the problem of automatic selection of basic units, thus also better to let the model determine.",1. Introduction,[0],[0]
"Recently, there are some interesting attempts in these directions in the seq2seq framework.",1. Introduction,[0],[0]
"For example, Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decomposition to decompose target sequences with variable length units as a function of both input sequence and the output sequence.
",1. Introduction,[0],[0]
"In this work, we propose Gram-CTC - a strictly more general version of CTC - to automatically seek the best set of basic units from the training data, called grams, and automatically decompose target sequences into sequences of grams.",1. Introduction,[0],[0]
"Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.",1. Introduction,[0],[0]
"Since it is a loss function, it can be applied to many seq2seq tasks to enable automatic selection of grams and decomposition of target sequences without modifying the underlying networks.",1. Introduction,[0],[0]
"Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.",1. Introduction,[0],[0]
"The basic text units that previous works utilized for text prediction tasks (e.g,, automatic speech recognition, handwriting recognition, machine translation, and image captioning) can be generally divided into two categories: handcrafted ones and learning-based ones.
",2. Related Work,[0],[0]
Hand-crafted Basic Units.,2. Related Work,[0],[0]
"Fixed sets of characters (graphemes) (Graves et al., 2006; Amodei et al., 2015), word-pieces (Wu et al., 2016b; Collobert et al., 2016; Zweig et al., 2016a), words (Soltau et al., 2016; Sébastien et al., 2015), and phonemes (Lee and Hon, 1988; Sercu and Goel, 2016; Xiong et al., 2016b) have been widely used as basic units for text prediction, but all of them have drawbacks.",2. Related Work,[0],[0]
"Using these fixed deterministic decompositions of text sequences defines a prior, which is not necessarily optimal for end-to-end learning.
",2. Related Work,[0],[0]
• Word-segmented models remove the component of learning to spell and thus enable direct optimization towards reducing Word Error Rate (WER).,2. Related Work,[0],[0]
"However, these models suffer from having to handle a large vocabulary (1.7 million in (Soltau et al., 2016)), out-of-vocabulary words (Soltau et al., 2016; Sébastien et al., 2015) and data sparsity problems (Soltau et al., 2016).
",2. Related Work,[0],[0]
"• Using characters results in much smaller vocabularies (e.g, 26 for English and thousands for Chinese), but it requires much longer contexts compared to using words or word-pieces and poses the challenge of composing characters to words (Graves et al., 2006; Chan et al., 2015),
which is very noisy for languages like English.
",2. Related Work,[0],[0]
"• Word-pieces lie at the middle-ground of words and characters, providing a good trade-off between vocabulary size and context size, while the performance of using word pieces is sensitive to the choice of the word-piece set and its decomposition.
",2. Related Work,[0],[0]
"• For the ASR task, the use of phonemes was popular in the past few decades as it eases acoustic modeling (Lee and Hon, 1988) and good results were reported with phonemic models (Xiong et al., 2016b; Sercu and Goel, 2016).",2. Related Work,[0],[0]
"However, it introduces the uncertainties of mapping phonemes to words during decoding (Doss et al., 2003), which becomes less robust especially for accented speech data.
",2. Related Work,[0],[0]
Learning-based Basic Units.,2. Related Work,[0],[0]
"More recently, attempts have been made to learn basic unit sets automatically.",2. Related Work,[0],[0]
"(Luong and Manning, 2016) proposed a hybrid WordCharacter model which translates mostly at the word level and consults the character components for rare words.",2. Related Work,[0],[0]
"Chan et al (Chan et al., 2016b) proposed the Latent Sequence Decompositions framework to decomposes target sequences with variable length-ed basic units as a function of both input sequence and the output sequence.
",2. Related Work,[0],[0]
"There exist some earlier works on the “unit discovery” task (Cartwright and Brent, 1994; Goldwater et al., 2006).",2. Related Work,[0],[0]
"A standard problem with MLE solutions to this task is that there are degenerate solutions, i.e., predicting the full corpus with probability 1 at the start.",2. Related Work,[0],[0]
Often Bayesian priors or “minimum description length” constraints are used to remedy this.,2. Related Work,[0],[0]
"CTC (Graves et al., 2006) is a very popular method in seq2seq learning since it does not require the alignment information between inputs and outputs, which is usually expensive, if not impossible, to obtain.
",3.1. CTC,[0],[0]
"Since there is no alignment information, CTC marginalizes over all possible alignments.",3.1. CTC,[0],[0]
"That is, it tries to maximize p(l|x) =",3.1. CTC,[0],[0]
"∑ π p(π|x), where x is input, and π represent a valid alignment.",3.1. CTC,[0],[0]
"For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.",3.1. CTC,[0],[0]
"For the details, please refer to the original paper (Graves et al., 2006).",3.1. CTC,[0],[0]
"In CTC, the basic units are fixed, which is not desirable in some applications.",3.2. From CTC to Gram-CTC,[0],[0]
"Here we generalize CTC by considering a sequence of basic units, called gram, as a whole, which is usually more reasonable in many applications.
",3.2. From CTC to Gram-CTC,[0],[0]
"Let G be a set of n-grams of the set of basic units C of the target sequence, and τ be the length of the longest gram in G. A Gram-CTC network has a softmax output layer with |G|+1 units, that is, the probability over all grams inG and one additional symbol, blank.",3.2. From CTC to Gram-CTC,[0],[0]
"To simplify the problem, we also assume C ⊆ G. 1
For an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G′ = G ∪ {blank}, then we have
p(π|x)",3.2. From CTC to Gram-CTC,[0],[0]
"= T∏ t=1 ytπt ,∀π ∈ G ′T (1)
Just as in the case of CTC, here we refer to the elements of G′T as paths, and denote them by π, which represents a possible alignment between input and output.",3.2. From CTC to Gram-CTC,[0],[0]
"The difference is that for each word in the target sequence, it may be decomposed into different sequences of grams.",3.2. From CTC to Gram-CTC,[0],[0]
"For example, the word ‘hello’ can only be decomposed into the sequence [‘h’, ‘e’, ‘l’, ‘l’, ‘o’] for CTC (assume uni-gram CTC here), but it also can be decomposed into the sequence [‘he’, ‘ll’, ‘o’] if ‘he’ and ‘ll’ are in G.
For each π, we map it into a target sequence in the same way as CTC using the collapsing function that 1) removes all repeated labels from the path and then 2) removes all blanks.",3.2. From CTC to Gram-CTC,[0],[0]
"Note that essentially it is these rules which de-
1This is because there may be no valid decompositions for some target sequences if C 6⊆ G. Since Gram-CTC will figure out the ideal decomposition of target sequences into grams during training, this condition guarantees that there is at least one valid decomposition for every target sequence.
termine the transitions between the states of adjacent time steps in Figure 1.",3.2. From CTC to Gram-CTC,[0],[0]
This is a many-to-one mapping and we denote it by B. Note that other rules can be adopted here and the general idea presented in this paper does not depend on these specific rules.,3.2. From CTC to Gram-CTC,[0],[0]
"For a target sequence l, B−1(l) represents all paths mapped to l. Then, we have
p(l|x) = ∑
π∈B−1(l)
p(π|x) (2)
",3.2. From CTC to Gram-CTC,[0],[0]
"This equation allows for training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.",3.2. From CTC to Gram-CTC,[0],[0]
"Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.
",3.2. From CTC to Gram-CTC,[0],[0]
"Note that for each target sequence l, the set B−1(l) has O(τ2) more paths than it does in CTC.",3.2. From CTC to Gram-CTC,[0],[0]
"This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.",3.2. From CTC to Gram-CTC,[0],[0]
"The original CTC method is thus, a special case of Gram-CTC when G = C and τ = 1.",3.2. From CTC to Gram-CTC,[0],[0]
"While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.",3.2. From CTC to Gram-CTC,[0],[0]
"Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .",3.2. From CTC to Gram-CTC,[0],[0]
"To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.",3.3. The Forward-Backward Algorithm,[0],[0]
"The essence here is identifying
the states of the problem, so that we may solve future states by reusing solutions to earlier states.",3.3. The Forward-Backward Algorithm,[0],[0]
"In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.",3.3. The Forward-Backward Algorithm,[0],[0]
"Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} is the length of the last gram (li−j+1:i) used for making the prefix.",3.3. The Forward-Backward Algorithm,[0],[0]
j = 0 is valid and means that blank was used.,3.3. The Forward-Backward Algorithm,[0],[0]
"We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).",3.3. The Forward-Backward Algorithm,[0],[0]
"For readability, we will further shorten sji (l) to s j i and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i (l) to g j i .,3.3. The Forward-Backward Algorithm,[0],[0]
"For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.",3.3. The Forward-Backward Algorithm,[0],[0]
"During dynamic programming, we are dealing with sequence of states, for a state sequence ζ, its corresponding gram sequences is unique, denoted by ζg .
",3.3. The Forward-Backward Algorithm,[0],[0]
Figure 1 illustrates partially the dynamic programming process for the target sequence ‘CAT’.,3.3. The Forward-Backward Algorithm,[0],[0]
Here we suppose G contains all possible uni-grams and bi-grams.,3.3. The Forward-Backward Algorithm,[0],[0]
"Thus, for each character in ‘CAT’, there are three possible states associated with it: 1) the current character, 2) the bi-gram ending in current character, and 3) the blank after current character.",3.3. The Forward-Backward Algorithm,[0],[0]
There is also one blank at beginning.,3.3. The Forward-Backward Algorithm,[0],[0]
"In total we have 10 states.
",3.3. The Forward-Backward Algorithm,[0],[0]
"Supposing the maximum length of grams inG is τ , we first scan l to get the set S of all possible states, such that for all sji ∈ S, its corresponding g j i ∈",3.3. The Forward-Backward Algorithm,[0],[0]
"G′. i ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", |l|}",3.3. The Forward-Backward Algorithm,[0],[0]
"and j ∈ {0, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ}.",3.3. The Forward-Backward Algorithm,[0],[0]
"For a target sequence l, define the forward variable αt(s) for any s ∈ S to the total probability of all valid paths prefixes that end at state s at time t.
αt(s) def = ∑ ζ|B(ζg)=l1:e(s),ζt=s t∏ t′=1 yt ′ ζt′g (3)
",3.3. The Forward-Backward Algorithm,[0],[0]
"Following this definition, we have the following rules for initialization
α1(s) =  y1b s = s 0 0 y1 gii
s = sii ∀i ∈ {1, . . .",3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(4)
and recursion
αt(s) =  α̂it−1 ∗ ytb when s = s0i , [α̂i−jt−1 + αt−1(s)] ∗ ytgji when s = sji and g j",3.3. The Forward-Backward Algorithm,[0],[0]
i 6=,3.3. The Forward-Backward Algorithm,[0],[0]
"g j i−j , [α̂i−jt−1 + αt−1(s)− αt−1(s j i−j)] ∗ ytgji
when s = sji and g j i = g j i−j
(5)
where α̂it = ∑τ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
αt(s j i ) and y t b is the probability of blank at time,3.3. The Forward-Backward Algorithm,[0],[0]
"t.
The total probability of the target sequence l is then expressed in the following way:
p(l|x) = τ∑ j=0",3.3. The Forward-Backward Algorithm,[0],[0]
"αT (s j |l|) (6)
similarly, we can define the backward variable βt(s) as:
βt(s) def = ∑ ζ|B(ζg)=lb(s):l,ζt=s T∏ t′=t yt ′ ζt′g (7)
",3.3. The Forward-Backward Algorithm,[0],[0]
"For the initialization and recursion of βt(s), we have
βT",3.3. The Forward-Backward Algorithm,[0],[0]
"(s) =  yTb s = s 0 T yT giT
s = siT ∀i ∈ {1, . .",3.3. The Forward-Backward Algorithm,[0],[0]
.,3.3. The Forward-Backward Algorithm,[0],[0]
", τ} 0 otherwise
(8)
and
βt(s) =  β̂it+1 ∗ ytb when s = s0i , [β̂i+jt+1 + βt+1(s)] ∗ ytgji when s = sji and g j i 6=",3.3. The Forward-Backward Algorithm,[0],[0]
"g j i+j , [β̂i+jt+1 + βt+1(s)− βt+1(s j i+j)] ∗ ytgji
when s = sji and g j i = g j i+j
(9) where β̂it = ∑τ j=0 βt(s j i+j)",3.3. The Forward-Backward Algorithm,[0],[0]
"Similar to CTC, we have the following expression:
p(l|x) = ∑ s∈S αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg ∀t ∈ {1, . . .",3.4. BackPropagation,[0],[0]
",T} (10)
",3.4. BackPropagation,[0],[0]
"The derivative with regards to ytk is:
∂p(l|x) ∂ytk",3.4. BackPropagation,[0],[0]
"= 1 ytk 2 ∑ s∈lab(l,k) αt(s)βt(s) (11)
where lab(l, k) is the set of states in S whose corresponding gram is",3.4. BackPropagation,[0],[0]
k.,3.4. BackPropagation,[0],[0]
"This is because there may be multiple states corresponding to the same gram.
",3.4. BackPropagation,[0],[0]
"For the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.
∂ ln p(l|x) ∂utk = ytk",3.4. BackPropagation,[0],[0]
"− 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12)
where Zt def = ∑ s∈S
αt(s)βt(s)",3.4. BackPropagation,[0],[0]
"ytsg .
",3.4. BackPropagation,[0],[0]
(a) Training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"(b) Training curves without (blue) and with (orange) joint-training
Gram-CTC C _",3.4. BackPropagation,[0],[0]
"AT
CTC _",3.4. BackPropagation,[0],[0]
C _,3.4. BackPropagation,[0],[0]
"A T _
Gram-CTC C - AT
CTC - C - A T -
(c) Joint-training Architecture
Figure 2.",3.4. BackPropagation,[0],[0]
(Figure 2a) compares the training curves before (blue) and after (orange) auto-refinement of grams.,3.4. BackPropagation,[0],[0]
"They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.",3.4. BackPropagation,[0],[0]
Figure (2b) Training curve of model with and without joint-training.,3.4. BackPropagation,[0],[0]
"The model corresponding to the orange training curve is jointly trained together with vanilla CTC, such models are often more stable during training.",3.4. BackPropagation,[0],[0]
Figure (2c) Typical joint-training model architecture - vanilla CTC loss is best applied a few levels lower than the Gram-CTC loss.,3.4. BackPropagation,[0],[0]
Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.,4. Methodology,[0],[0]
"Although Gram-CTC can automatically select useful grams, it is challenging to train with a large G.",4.1. Iterative Gram Selection,[0],[0]
The total number of possible grams is usually huge.,4.1. Iterative Gram Selection,[0],[0]
"For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, . . .",4.1. Iterative Gram Selection,[0],[0]
", which grows exponentially and quickly becomes intractable.",4.1. Iterative Gram Selection,[0],[0]
"However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.
",4.1. Iterative Gram Selection,[0],[0]
"In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.",4.1. Iterative Gram Selection,[0],[0]
"Then, we train a model with Gram-CTC on all the remaining grams.",4.1. Iterative Gram Selection,[0],[0]
"By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.",4.1. Iterative Gram Selection,[0],[0]
"Ultimately, we choose high frequency grams together with all uni-grams as our final gram set G. Table 1 shows the impact of iterative gram selection on WSJ (without LM).",4.1. Iterative Gram Selection,[0],[0]
Figure 2a shows its corresponding training curve.,4.1. Iterative Gram Selection,[0],[0]
"For details, please refer to Section 5.2.",4.1. Iterative Gram Selection,[0],[0]
"Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.",4.2. Joint Training with Vanilla CTC,[0],[0]
"This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results
in models converging to a worse optima.",4.2. Joint Training with Vanilla CTC,[0],[0]
"To overcome this difficulty, we found it beneficial to train a model with both the Gram-CTC, as well as the vanilla CTC loss (similar to joint-training CTC together with CE loss as mentioned in (Sak et al., 2015)).",4.2. Joint Training with Vanilla CTC,[0],[0]
"Joint training of multiple objectives for sequence labelling has also been explored in previous works (Kim et al., 2016; Kim and Rush, 2016).
",4.2. Joint Training with Vanilla CTC,[0],[0]
"A typical joint-training model looks like Figure 2c, and the corresponding training curve is shown in Figure 2b.",4.2. Joint Training with Vanilla CTC,[0],[0]
The effect of joint-training are shown in Table 4 and Table 5 in the experiments.,4.2. Joint Training with Vanilla CTC,[0],[0]
"We test the Gram-CTC loss on the ASR task, while both CTC and the introduced Gram-CTC are generic techniques for other sequence labelling tasks.",5. Experiments,[0],[0]
"For all of the experiments, the model specification and training procedure are the same as in (Amodei et al., 2015) -",5. Experiments,[0],[0]
"The model is a recurrent neural network (RNN) with 2 two-dimensional convolutional input layers, followed by K forward (Fwd) or bidirectional (Bidi)",5. Experiments,[0],[0]
"Gated Recurrent layers, N cells each, and one fully connected layer before a softmax layer.",5. Experiments,[0],[0]
"In short hand, such a model is written as ‘2x2D Conv - KxN GRU’.",5. Experiments,[0],[0]
"The network is trained end-to-end with the CTC, GramCTC or a weighted combination of both.",5. Experiments,[0],[0]
"This combination is described in the earlier section.
",5. Experiments,[0],[0]
"In all experiments, audio data is is sampled at 16kHz.",5. Experiments,[0],[0]
"Linear FFT features are extracted with a hop size of 10ms and window size of 20ms, and are normalized so that each input feature has zero mean and unit variance.",5. Experiments,[0],[0]
The network inputs are thus spectral magnitude maps ranging from 0-8kHz with 161 features per 10ms frame.,5. Experiments,[0],[0]
"At each epoch, 40% of the utterances are randomly selected to add
background noise to.",5. Experiments,[0],[0]
The optimization method we use is stochastic gradient descent with Nesterov momentum.,5. Experiments,[0],[0]
"Learning hyperparameters (batch-size, learning-rate, momentum, and etc.) vary across different datasets and are tuned for each model by optimizing a hold-out set.",5. Experiments,[0],[0]
Typical values are a learning rate of 10−3 and momentum of 0.99.,5. Experiments,[0],[0]
Wall Street Journal (WSJ).,5.1. Data and Setup,[0],[0]
"This corpora consists primarily of read speech with texts drawn from a machinereadable corpus of Wall Street Journal news text, and contains about 80 hours speech data.",5.1. Data and Setup,[0],[0]
"We used the standard configuration of train si284 dataset for training, dev93 for validation and eval92 for testing.",5.1. Data and Setup,[0],[0]
"This is a relatively ‘clean’ task and often used for model prototyping (Miao et al., 2015; Bahdanau et al., 2016; Zhang et al., 2016; Chan et al., 2016b).
",5.1. Data and Setup,[0],[0]
Fisher-Switchboard.,5.1. Data and Setup,[0],[0]
"This is a commonly used English conversational telephone speech (CTS) corpora, which contains 2300 hours CTS data.",5.1. Data and Setup,[0],[0]
"Following the previous works (Zweig et al., 2016b; Povey et al., 2016; Xiong et al., 2016b; Sercu and Goel, 2016), evaluation is carried out on the NIST 2000 CTS test set, which comprises both Switchboard (SWB) and CallHome (CH) subsets.
10K Speech Dataset.",5.1. Data and Setup,[0],[0]
"We conduct large scale ASR experiments on a noisy internal dataset of 10,000 hours.",5.1. Data and Setup,[0],[0]
"This dataset contains speech collected from various scenarios, such as different background noises, far-field, different accents, and so on.",5.1. Data and Setup,[0],[0]
"Due to its inherent complexities, it is a very challenging task, and can thus validate the effectiveness of the proposed method for real-world application.",5.1. Data and Setup,[0],[0]
"We employ the WSJ dataset for demonstrating different strategies of selecting grams for Gram-CTC, since it is a widely used dataset and also small enough for rapid idea verification.",5.2. Gram Selection,[0],[0]
"However, because it is small, we cannot use large grams here due to data sparsity problem.",5.2. Gram Selection,[0],[0]
"Thus, the auto-refined gram set on WSJ is not optimal for other larger datasets, where larger grams could be effectively used, but the procedure of refinement is the same for them.
",5.2. Gram Selection,[0],[0]
"We first train a model using all uni-grams and bi-grams (29
uni-grams and 262 = 676 bi-grams, in total 705 grams), and then do decoding with the obtained model on another speech dataset to get the statistics of the usage of grams.",5.2. Gram Selection,[0],[0]
Top 100 bi-grams together with all 29 uni-grams (autorefined grams) are used for the second round of training.,5.2. Gram Selection,[0],[0]
"For comparison, we also present the result of the best handpicked grams, as well as the results on uni-grams.",5.2. Gram Selection,[0],[0]
"All the results are shown in Table 1.
",5.2. Gram Selection,[0],[0]
Some interesting observations can be found in Table 1.,5.2. Gram Selection,[0],[0]
"First, the performance of auto-refined grams is only slightly better than the combination of all uni-grams and all bigrams.",5.2. Gram Selection,[0],[0]
This is probably because WSJ is so small that gram learning suffers from the data sparsity problem here (similar to word-segmented models).,5.2. Gram Selection,[0],[0]
"The auto-refined gram set contains only a small subset of bi-grams, thus more robust.",5.2. Gram Selection,[0],[0]
"This is also why we only try bi-grams, not including higher-order grams.",5.2. Gram Selection,[0],[0]
"Second, the performance of best handpicked grams is worse than auto-refined grams.",5.2. Gram Selection,[0],[0]
This is desirable.,5.2. Gram Selection,[0],[0]
"It is time-consuming to handpick grams, especially when you consider high-order grams.",5.2. Gram Selection,[0],[0]
"The method of iterative gram selection is not only fast, but usually better.",5.2. Gram Selection,[0],[0]
"Third, the performance of Gram-CTC on auto-refined grams is only slightly better than CTC on uni-grams.",5.2. Gram Selection,[0],[0]
"This is because Gram-CTC is inherently difficult to train, since it needs to learn both decomposition and alignment.",5.2. Gram Selection,[0],[0]
WSJ is too small to provide enough data to train Gram-CTC.,5.2. Gram Selection,[0],[0]
"Using a large time stride for sequence labelling with RNNs can greatly boost the overall computation efficiency, since it effectively reduces the time steps for recurrent computation, thus speeds up the process of both forward inference and backward propagation.",5.3. Sequence Labelling in Large Stride,[0],[0]
"However, the largest stride that can be used is limited by the gram set we use.",5.3. Sequence Labelling in Large Stride,[0],[0]
The (unigram) CTC has to work in a high time resolution (small stride) in order to have enough number of frames to output every character.,5.3. Sequence Labelling in Large Stride,[0],[0]
"This is very inefficient as we know the same acoustic feature could correspond to several grams of different lengths (e.g., {‘i’, ‘igh’, ‘eye’}) .",5.3. Sequence Labelling in Large Stride,[0],[0]
"The larger the grams are, the larger stride we are potentially able to use.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"DS2 (Amodei et al., 2015) employed non-overlapping bigram outputs to allow for a larger stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
"This imposes an artificial constraint forcing the model to learn, not only the spelling of each word, but also how to split words into bigrams.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For example, part is split as [pa, rt] but the word
apart is forced to be decomposed as [ap, ar, t].",5.3. Sequence Labelling in Large Stride,[0],[0]
GramCTC removes this constraint by allowing the model to decompose words into larger units into the most convenient or sensible decomposition.,5.3. Sequence Labelling in Large Stride,[0],[0]
"Comparison results show this change enables Gram-CTC to work much better than bigram CTC, as in Table 2.
",5.3. Sequence Labelling in Large Stride,[0],[0]
"In Table 2, we compare the performance of trained model and training efficiency on two strides, 2 and 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
"For GramCTC, we use the auto-refined gram set from previous section.",5.3. Sequence Labelling in Large Stride,[0],[0]
"As expected, using stride 4 almost cuts the training time per epoch into half, compared to stride 2.",5.3. Sequence Labelling in Large Stride,[0],[0]
"From stride 2 to stride 4, the performance of uni-gram CTC drops quickly.",5.3. Sequence Labelling in Large Stride,[0],[0]
This is because small grams inherently need higher time resolutions.,5.3. Sequence Labelling in Large Stride,[0],[0]
"As for Gram-CTC, from stride 2 to stride 4, its performance decreases a little bit, while in experiments on the other datasets, Gram-CTC constantly works better in stride 4.",5.3. Sequence Labelling in Large Stride,[0],[0]
One possible explanation is that WSJ is too small for Gram-CTC to learn large grams well.,5.3. Sequence Labelling in Large Stride,[0],[0]
"In contrast, the performance of bi-gram CTC is not as good as that of Gram-CTC in either stride.",5.3. Sequence Labelling in Large Stride,[0],[0]
Figure 3 illustrates the max-decoding results of both CTC and Gram-CTC on nine utterances.,5.4. Decoding Examples,[0],[0]
"Here the label set for CTC is the set of all characters, and the label set for GramCTC is an auto-refined gram set containing all uni-grams and some high-frequency high-order grams.",5.4. Decoding Examples,[0],[0]
"Here Gram-
CTC uses stride 4 while CTC uses stride 2.
",5.4. Decoding Examples,[0],[0]
"From Figure 3, we can find that: 1) Gram-CTC does automatically find many intuitive and meaningful grams, such as ‘the’, ‘ng’, and ‘are’.",5.4. Decoding Examples,[0],[0]
2) It also decomposes the sentences into segments which are meaningful in term of pronunciation.,5.4. Decoding Examples,[0],[0]
"This decomposition resembles the phonetic decomposition, but in larger granularity and arguably more natural.",5.4. Decoding Examples,[0],[0]
"3) Since Gram-CTC predicts a chunk of characters (a gram) each time, each prediction utilizes larger context and these characters in the same predicted chunk are dependent, thus potentially more robust.",5.4. Decoding Examples,[0],[0]
"One example is the word ‘will’ in the last sentence in Figure 3. 4) Since the output of network is the probability over all grams, the decoding process is almost the same as CTC, still end-toend.",5.4. Decoding Examples,[0],[0]
This makes such decomposition superior to phonetic decomposition.,5.4. Decoding Examples,[0],[0]
"In summary, Gram-CTC combines the advantages of both CTC on characters and CTC on phonemes.",5.4. Decoding Examples,[0],[0]
"The model used here is [2x2D conv, 3x1280 Bidi GRU] with a CTC or Gram-CTC loss.",5.5.1. WSJ DATASET,[0],[0]
The results are shown in Table 3.,5.5.1. WSJ DATASET,[0],[0]
"For all models we trained, language model can greatly improve their performances, in term of WER.",5.5.1. WSJ DATASET,[0],[0]
"Though this dataset contains very limited amount of text data for learning gram selection and decomposition, Gram-
CTC can still improve the vanilla CTC notably.",5.5.1. WSJ DATASET,[0],[0]
The acoustic model trained here is composed of two 2D convolutions and six bi-directional GRU layer in 2048 dimension.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"The corresponding labels are used for training N-gram language models.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
• Switchboard,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"English speech 97S62 • Fisher English speech Part 1 - 2004S13, 2004T19 • Fisher English speech Part 2 - 2005S13, 2005T19
We use a sample of the Switchboard-1 portion of the NIST 2002 dataset (2004S11 RT-02) for tuning language model hyper-parameters.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
The evaluation is done on the NIST 2000 set.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
This configuration forms a standard benchmark for evaluating ASR models.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Results are in Table 4.
",5.5.2. FISHER-SWITCHBOARD,[0],[0]
We compare our model against best published results on in-domain data.,5.5.2. FISHER-SWITCHBOARD,[0],[0]
"These results can often be improved using out-of-domain data for training the language model, and sometimes the acoustic model as well.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Together these techniques allow (Xiong et al., 2016b) to reach a WER of 5.9 on the SWBD set.",5.5.2. FISHER-SWITCHBOARD,[0],[0]
"Finally, we experiment on a large noisy dataset collected by ourself for building large-vocabulary Continuous Speech Recognition (LVCSR) systems.",5.5.3. 10K SPEECH DATASET,[0],[0]
"This dataset contains about 10000 hours speech in a diversity of scenarios, such as farfield, background noises, accents.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In all cases, the model is [2x2D Conv, 3x2560 Fwd GRU, LA Conv] with only a change in the loss function.",5.5.3. 10K SPEECH DATASET,[0],[0]
"‘LA Conv’ refers to a look ahead convolution layer as seen in (Amodei et al., 2015) which works together with forward-only RNNs for deployment purpose.
",5.5.3. 10K SPEECH DATASET,[0],[0]
"As with the Fisher-Switchboard dataset, the optimal stride is 4 for Gram-CTC and 2 for vanilla CTC on this dataset.",5.5.3. 10K SPEECH DATASET,[0],[0]
"Thus, in both experiments, both Gram-CTC and vanilla
CTC + Gram-CTC are trained mush faster than vanilla CTC itself.",5.5.3. 10K SPEECH DATASET,[0],[0]
The result is shown in Table 5.,5.5.3. 10K SPEECH DATASET,[0],[0]
Gram-CTC performs better than CTC.,5.5.3. 10K SPEECH DATASET,[0],[0]
"After joint-training with vanilla CTC and alignment information through a CE loss, its performance is further boosted, which verifies joint-training helps training.",5.5.3. 10K SPEECH DATASET,[0],[0]
"In fact, with only a small additional cost of time, it effectively reduces the WER from 27.56% to 25.59% (without language model).",5.5.3. 10K SPEECH DATASET,[0],[0]
"In this paper, we have proposed the Gram-CTC loss to enable automatic decomposition of target sequences into learned grams.",6. Conclusions and Future Work,[0],[0]
We also present techniques to train the Gram-CTC in a clean and stable way.,6. Conclusions and Future Work,[0],[0]
"Our extensive experiments demonstrate the proposed Gram-CTC enables the models to run more efficiently than the vanilla CTC, by using larger stride, while obtaining better performance of sequence labelling.",6. Conclusions and Future Work,[0],[0]
"Comparison experiments on multiplescale datasets show the proposed Gram-CTC obtains stateof-the-art results on various ASR tasks.
",6. Conclusions and Future Work,[0],[0]
"An interesting observation is that the learning of GramCTC implicitly avoids the “degenerated solution” that occurring in the traditional “unit discovery” task, without involving any Bayesian priors or the “minimum description length” constraint.",6. Conclusions and Future Work,[0],[0]
"Using a small gram set that contains only short (up to 5 in our experiments) as well as highfrequency grams may explain the success here.
",6. Conclusions and Future Work,[0],[0]
"We will continue investigating techniques of improving the optimization of Gram-CTC loss, as well as the applications of Gram-CTC for other sequence labelling tasks.",6. Conclusions and Future Work,[0],[0]
Most existing sequence labelling models rely on a fixed decomposition of a target sequence into a sequence of basic units.,abstractText,[0],[0]
"These methods suffer from two major drawbacks: 1) the set of basic units is fixed, such as the set of words, characters or phonemes in speech recognition, and 2) the decomposition of target sequences is fixed.",abstractText,[0],[0]
These drawbacks usually result in sub-optimal performance of modeling sequences.,abstractText,[0],[0]
"In this paper, we extend the popular CTC loss criterion to alleviate these limitations, and propose a new loss function called Gram-CTC.",abstractText,[0],[0]
"While preserving the advantages of CTC, Gram-CTC automatically learns the best set of basic units (grams), as well as the most suitable decomposition of target sequences.",abstractText,[0],[0]
"Unlike CTC, Gram-CTC allows the model to output variable number of characters at each time step, which enables the model to capture longer term dependency and improves the computational efficiency.",abstractText,[0],[0]
"We demonstrate that the proposed Gram-CTC improves CTC in terms of both performance and efficiency on the large vocabulary speech recognition task at multiple scales of data, and that with Gram-CTC we can outperform the state-of-the-art on a standard speech benchmark.",abstractText,[0],[0]
Gram-CTC: Automatic Unit Selection and Target Decomposition for Sequence Labelling,title,[0],[0]
"Generative machine learning models have been used recently to produce extraordinary results, from realistic musical improvisation (Jaques et al., 2016), to changing facial expressions in images (Radford et al., 2015; Upchurch et al., 2016), to creating realistic looking artwork (Gatys et al., 2015).",1. Introduction,[0],[0]
"In large part, these generative models have been successful at representing data in continuous domains.",1. Introduction,[0],[0]
"Recently there is increased interest in training generative models to construct more complex, discrete data types such as arithmetic expressions (Kusner & Hernández-Lobato, 2016), source code (Gaunt et al., 2016; Riedel et al., 2016)
",1. Introduction,[0],[0]
*Equal contribution 1Alan Turing Institute 2University of Warwick 3University of Cambridge.,1. Introduction,[0],[0]
"Correspondence to: <mkusner@turing.ac.uk>, <bpaige@turing.ac.uk>, <jmh233@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
and molecules (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"To train generative models for these tasks, these objects are often first represented as strings.",1. Introduction,[0],[0]
"This is in large part due to the fact that there exist powerful models for text sequence modeling such as Long Short Term Memory networks (LSTMs) (Hochreiter & Schmidhuber, 1997), Gated Recurrent Units (GRUs) (Cho et al., 2014), and Dynamic Convolutional Neural Networks (DCNNs) (Kalchbrenner et al., 2014).",1. Introduction,[0],[0]
"For instance, molecules can be represented by so-called SMILES strings (Weininger, 1988) and GómezBombarelli et al. (2016b) has recently developed a generative model for molecules based on SMILES strings that uses GRUs and DCNNs.",1. Introduction,[0],[0]
"This model is able to encode and decode molecules to and from a continuous latent space, allowing one to search this space for new molecules with desirable properties (Gómez-Bombarelli et al., 2016b).
",1. Introduction,[0],[0]
"However, one immediate difficulty in using strings to represent discrete objects is that the representation is very brittle: small changes in the string can lead to completely different objects, or often do not correspond to valid objects at all.",1. Introduction,[0],[0]
"Specifically, Gómez-Bombarelli et al. (2016b) described that while searching for new molecules, the probabilistic decoder — the distribution which maps from the continuous latent space into the space of molecular structures — would sometimes accidentally put high probability on strings which are not valid SMILES strings or do not encode plausible molecules.
",1. Introduction,[0],[0]
"To address this issue, we propose to directly incorporate knowledge about the structure of discrete data using a grammar.",1. Introduction,[0],[0]
"Grammars exist for a wide variety of discrete domains such as symbolic expressions (Allamanis et al., 2016), standard programming languages such as C (Kernighan et al., 1988), and chemical structures (James et al., 2015).",1. Introduction,[0],[0]
"For instance the set of syntactically valid SMILES strings is described using a context free grammar, which can be used for parsing and validation1.
",1. Introduction,[0],[0]
"Given a grammar, every valid discrete object can be described as a parse tree from the grammar.",1. Introduction,[0],[0]
"Thus, we propose the grammar variational autoencoder (GVAE) which encodes and decodes directly from and to these parse trees.",1. Introduction,[0],[0]
"Generating parse trees as opposed to strings ensures that
1http://opensmiles.org/spec/open-smiles-2-grammar.html
all outputs are valid based on the grammar.",1. Introduction,[0],[0]
"This frees the GVAE from learning syntactic rules and allows it to wholly focus on learning other ‘semantic’ properties.
",1. Introduction,[0],[0]
We demonstrate the GVAE on two tasks for generating discrete data: 1) generating simple arithmetic expressions and 2) generating valid molecules.,1. Introduction,[0],[0]
"We show not only does our model produce a higher proportion of valid outputs than a character based autoencoder, it also produces smoother latent representations.",1. Introduction,[0],[0]
"We also show that this learned latent space is effective for searching for arithmetic expressions that fit data, for finding better drug-like molecules, and for making accurate predictions about target properties.",1. Introduction,[0],[0]
We wish to learn both an encoder and a decoder for mapping data x to and from values z in a continuous space.,2.1. Variational autoencoder,[0],[0]
"The variational autoencoder (Kingma & Welling, 2014; Rezende et al., 2014) provides a formulation in which the encoding z is interpreted as a latent variable in a probabilistic generative model; a probabilistic decoder is defined by a likelihood function p
✓ (x|z) and parameterized by ✓.",2.1. Variational autoencoder,[0],[0]
"Alongside a prior distribution p(z) over the latent variables, the posterior distribution p
✓ (z|x) / p(z)p ✓ (x|z) can then be interpreted as a probabilistic encoder.
",2.1. Variational autoencoder,[0],[0]
"To admit efficient inference, the variational Bayes approach simultaneously learns both the parameters of p
✓ (x|z) as well as those of a posterior approximation q
(z|x).",2.1. Variational autoencoder,[0],[0]
"This is achieved by maximizing the evidence lower bound (ELBO)
",2.1. Variational autoencoder,[0],[0]
"L( , ✓;x) = E q (z|x)",2.1. Variational autoencoder,[0],[0]
"[log p✓(x, z) log q (z|x)] , (1)
with L( , ✓;x)  log p ✓ (x).",2.1. Variational autoencoder,[0],[0]
"So long as p ✓ (x|z) and q
(z|x) can be computed pointwise, and are differentiable with respect to their parameters, the ELBO can be maximized via gradient descent; this allows wide flexibility in choice of encoder and decoder models.",2.1. Variational autoencoder,[0],[0]
Typically these will take the form of exponential family distributions whose parameters are the weights of a multi-layer neural network.,2.1. Variational autoencoder,[0],[0]
"A context-free grammar (CFG) is traditionally defined as a 4-tuple G = (V,⌃, R, S): V is a finite set of non-terminal symbols; the alphabet ⌃ is a finite set of terminal symbols, disjoint from V ; R is a finite set of production rules; and S is a distinct non-terminal known as the start symbol.",2.2. Context-free grammars,[0],[0]
The rules R are formally described as ↵ !,2.2. Context-free grammars,[0],[0]
"for ↵ 2 V and 2 (V [ ⌃)⇤, with ⇤ denoting the Kleene closure.",2.2. Context-free grammars,[0],[0]
"In practice, these rules are defined as a set of mappings from a single left-hand side non-terminal in V to a sequence of terminal and/or non-terminal symbols, and can be interpreted as ‘replacement’ instructions.
",2.2. Context-free grammars,[0],[0]
"Repeatedly applying production rules beginning with a non-terminal symbol defines a tree, with symbols on the right-hand side of the production rule becoming child nodes for the left-hand side parent.",2.2. Context-free grammars,[0],[0]
"The grammar G thus defines a set of possible trees extending from each nonterminal symbol in V , produced by recursively applying rules in R to leaf nodes until all leaf nodes are terminal symbols in ⌃.",2.2. Context-free grammars,[0],[0]
The language of G is the set of all terminal symbol sequences that can be generated as leaf nodes in a tree.,2.2. Context-free grammars,[0],[0]
"Given a string in the language (i.e., a sequence of terminals), a parse tree is a tree rooted at S which has this sequence of terminal symbols as its leaf nodes.",2.2. Context-free grammars,[0],[0]
The ubiquity of context-free languages in computer science is due in part to the presence of efficient parsing algorithms to generate parse trees.,2.2. Context-free grammars,[0],[0]
"For more background on CFGs and automata theory, see e.g. Hopcroft et al. (2006).
",2.2. Context-free grammars,[0],[0]
Our work builds off the work of probabilistic context-free grammars (PCFGs).,2.2. Context-free grammars,[0],[0]
"A PCFG assigns probabilities to each production rule in the grammar, and thus defines a probability distribution over parse trees (Baker, 1979; Booth & Thompson, 1973).",2.2. Context-free grammars,[0],[0]
"A string can be generated by repeatedly sampling and applying production rules, beginning at the start symbol, until no non-terminals remain.",2.2. Context-free grammars,[0],[0]
"Modern approaches allow the probabilities used at each stage to depend on the state of the parse tree (Johnson et al., 2007).",2.2. Context-free grammars,[0],[0]
In this section we describe how a grammar can improve variational autoencoders (VAE) for discrete data.,3. Methods,[0],[0]
It will do so by drastically reducing the number of invalid outputs generated from the VAE.,3. Methods,[0],[0]
"We illustrate our approach on molecular data, however it will extend to any descrete data that can be described by a grammar.
",3. Methods,[0],[0]
"One glaring issue with a character-based VAE is that it may frequently map latent points to sequences that are not valid, hoping the VAE will infer from training data what constitutes a valid sequence.",3. Methods,[0],[0]
"Instead of implicitly encouraging the VAE to produce valid sequences, we propose to give the VAE explicit knowledge about how to produce valid sequences.",3. Methods,[0],[0]
We do this by using a grammar for the sequences: given a grammar we can take any valid sequence and parse it into a parse tree.,3. Methods,[0],[0]
A pre-order traversal on this parse tree yields a sequence of production rules.,3. Methods,[0],[0]
Applying these rules in order will yield the original sequence.,3. Methods,[0],[0]
Our approach then will be to learn a VAE that produces sequences of grammar production rules.,3. Methods,[0],[0]
"The benefit is that it is trivial to generate valid sequences of production rules, as the grammar describes the valid set of rules that can be selected at any point during the generation process.",3. Methods,[0],[0]
"Thus, our model is able to focus on learning semantic properties of sequence data without also having to learn syntactic constraints.",3. Methods,[0],[0]
We propose a grammar variational autoencoder (GVAE) that encodes/decodes in the space of grammar production rules.,3.1. An illustrative example,[0],[0]
"We describe how it works with a simple example.
Encoding.",3.1. An illustrative example,[0],[0]
"Consider a subset of the SMILES grammar as shown in Figure 1, box 1 .",3.1. An illustrative example,[0],[0]
These are the possible production rules that can be used for constructing a molecule.,3.1. An illustrative example,[0],[0]
Imagine we are given as input the SMILES string for benzene: ‘c1ccccc1’.,3.1. An illustrative example,[0],[0]
"Figure 1, box 3 shows this molecule.",3.1. An illustrative example,[0],[0]
To encode this molecule into a continuous latent representation we begin by using the SMILES grammar to parse this string into a parse tree (partially shown in box 2 ).,3.1. An illustrative example,[0],[0]
This tree describes how ‘c1ccccc1’ is generated by the grammar.,3.1. An illustrative example,[0],[0]
"We decompose this tree into a sequence of production rules by performing a pre-order traversal on the branches of the parse tree from left-to-right, shown in box 4 .",3.1. An illustrative example,[0],[0]
"We convert these rules into 1-hot indicator vectors, where each dimension corresponds to a rule in the SMILES grammar, box 5 .",3.1. An illustrative example,[0],[0]
"These 1-hot vectors are concatenated into the rows of a matrix X of dimension T (X)⇥K, where K is the number of production rules in the SMILES grammar, and T (X) is the number of production rules used to generate X.
We use a deep convolutional neural network to map the collection of 1-hot vectors X to a continuous latent vector z.",3.1. An illustrative example,[0],[0]
"The architecture of the encoding network is described in the supplementary material.
",3.1. An illustrative example,[0],[0]
Decoding.,3.1. An illustrative example,[0],[0]
We now describe how we map continuous vectors back to a sequence of production rules (and thus SMILES strings).,3.1. An illustrative example,[0],[0]
"Crucially we construct the decoder so that, at any time while we are decoding a sequence, the decoder will only be allowed to select a subset of production rules that are ‘valid’.",3.1. An illustrative example,[0],[0]
"This will cause the decoder to only produce valid parse sequences from the grammar.
",3.1. An illustrative example,[0],[0]
"We begin by passing the continuous vector z through a recurrent neural network which produces a set of unnormalized log probability vectors (or ‘logits’), shown in Figure 2, box 1 and 2 .",3.1. An illustrative example,[0],[0]
"Exactly like the 1-hot vectors produced by the encoder, each dimension of the logit vectors cor-
responds to a production rule in the grammar.",3.1. An illustrative example,[0],[0]
"We can again write these collection of logit vectors as a matrix F 2 RTmax⇥K , where T
max is the maximum number of timesteps (production rules) allowed by the decoder.",3.1. An illustrative example,[0],[0]
"During the rest of the decoding operations, we will use the rows of F to select a sequence of valid production rules.
",3.1. An illustrative example,[0],[0]
"To ensure that any sequence of production rules generated from the decoder is valid, we keep track of the state of the parsing using a last-in first-out (LIFO) stack.",3.1. An illustrative example,[0],[0]
"This is shown in Figure 2, box 3 .",3.1. An illustrative example,[0],[0]
"At the beginning, every valid parse from the grammar must start with the start symbol: smiles, which is placed on the stack.",3.1. An illustrative example,[0],[0]
"Next we pop off whatever non-terminal symbol that was placed last on the stack (in this case smiles), and we use it to mask out the invalid dimensions of the current logit vector.",3.1. An illustrative example,[0],[0]
"Formally, for every non-terminal ↵ we define a fixed binary mask vector m
↵ 2 [0, 1]K .",3.1. An illustrative example,[0],[0]
"This takes the value ‘1’ for all indices in 1, . . .",3.1. An illustrative example,[0],[0]
",K corresponding to production rules that have ↵ on their left-hand-side.
",3.1. An illustrative example,[0],[0]
"In the previous example, the only production rule in the grammar beginning with smiles is the first so we maskout every dimension except the first, shown in Figure 2, box 4 .",3.1. An illustrative example,[0],[0]
"We then sample from the remaining unmasked rules, using their values in the logit vector.",3.1. An illustrative example,[0],[0]
"To sample from this masked logit at any timestep t we form the following masked distribution:
p(x t = k|↵, z) = m↵,k exp(ftk)P K
j=1 m↵,k exp(ftj) , (2)
where f tk is the (t, k)-element of the logit matrix F. As only the first rule is unmasked we will select this rule smiles !",3.1. An illustrative example,[0],[0]
"chain as the first rule in our sequence, box 5 .",3.1. An illustrative example,[0],[0]
"Now the next rule must begin with chain, so we push it onto the stack (Figure 2, box 3 ).",3.1. An illustrative example,[0],[0]
"We sample this nonterminal and, as before, use it to mask out all of the rules that cannot be applied in the current logit vector.",3.1. An illustrative example,[0],[0]
We then sample a valid rule from this logit vector: chain!,3.1. An illustrative example,[0],[0]
"chain, branched atom.",3.1. An illustrative example,[0],[0]
"Just as before we push the non-terminals on the right-hand side of this rule onto the stack, adding the individual non-terminals in from right to left, such that the leftmost non-terminal is on the top of the stack.",3.1. An illustrative example,[0],[0]
"For the
Algorithm 1 Sampling from the decoder Input: Deterministic decoder output F 2 RTmax⇥K ,
masks m ↵ for each production rule ↵ Output: Sampled productions X from p(X|z)
1: Initialize empty stack S , and push the start symbol S onto the top; set t = 0 2: while S is nonempty do 3:",3.1. An illustrative example,[0],[0]
"Pop the last-pushed non-terminal ↵ from the stack S 4: Use Eq. (2) to sample a production rule R 5: Let x
t be the 1-hot vector corresponding to R 6: Let RHS(R) denote all non-terminals on the righthand side of rule R, ordered from right to left 7: for non-terminal in RHS(R) do 8: Push on to the stack S 9: end for
10: Set X [X>,x t ]",3.1. An illustrative example,[0],[0]
"> 11: Set t t+ 1 12: end while
next state we again pop the last rule placed on the stack and mask the current logit, etc.",3.1. An illustrative example,[0],[0]
"This process continues until the stack is empty or we reach the maximum number of logit vectors T
max .",3.1. An illustrative example,[0],[0]
We describe this decoding procedure formally in Algorithm 1.,3.1. An illustrative example,[0],[0]
"In practice, because sampling from the decoder often finishes before t reaches T
max , we introduce an additional ‘no-op’ rule to the grammar that we use to pad X until the number of rows equals T
max
.
",3.1. An illustrative example,[0],[0]
We note the explicit connection between the process in Algorithm 1 and parsing algorithms for pushdown automata.,3.1. An illustrative example,[0],[0]
"A pushdown automaton is a finite state machine which has access to a single stack for long-term storage, and are equivalent to context-free grammars in the sense that every CFG can be converted into a pushdown automaton, and vice-versa (Hopcroft et al., 2006).",3.1. An illustrative example,[0],[0]
"The decoding algorithm performs the sequence of actions taken by a nondeterministic pushdown automaton at each stage of a parsing algorithm; the nondeterminism is resolved by sampling according to the probabilities in the emitted logit vector.
Contrasting the character VAE.",3.1. An illustrative example,[0],[0]
"Notice that the key difference between this grammar VAE decoder and a
character-based VAE decoder is that at every point in the generated sequence, the character VAE can sample any possible character.",3.1. An illustrative example,[0],[0]
There is no stack or masking operation.,3.1. An illustrative example,[0],[0]
"The grammar VAE however is constrained to select syntactically-valid sequences.
",3.1. An illustrative example,[0],[0]
Syntactic vs. semantic validity.,3.1. An illustrative example,[0],[0]
It is important to note that the grammar encodes syntactically valid molecules but not necessarily semantically valid molecules.,3.1. An illustrative example,[0],[0]
This is mainly because of three reasons.,3.1. An illustrative example,[0],[0]
"First, certain molecules produced by the grammar may be very unstable molecules or not chemically-valid (for instance an oxygen atom cannot bond to 3 other atoms as it only has 2 free electrons for bonding, although it would be possible to generate this in a molecule from the grammar).",3.1. An illustrative example,[0],[0]
"Second, the SMILES language has non-context free aspects, e.g. a ringbond must be opened and closed by the same digit, starting with ‘1’ (as is the case for benzene ‘c1ccccc1’).",3.1. An illustrative example,[0],[0]
"The particular challenge for matching digits, in contrast to matching grouping symbols such as parentheses, is that they do not compose in a nested manner; for example, ‘C12(CCCCC1)CCCCC2’ is a valid molecule.",3.1. An illustrative example,[0],[0]
Keeping track of which digit to use for each ringbond is not context-free.,3.1. An illustrative example,[0],[0]
"Third, we note that the GVAE can output an undetermined sequence if there are still non-terminal symbols on the stack after processing all T max
logit vectors.",3.1. An illustrative example,[0],[0]
"While this could be fixed by a procedure that converts these non-terminals to terminals, for simplicity we mark these sequences as invalid.",3.1. An illustrative example,[0],[0]
"During training, each input SMILES encoded as a sequence of 1-hot vectors X 2 {0, 1}Tmax⇥K , also defines a sequence of T
max mask vectors.",3.2. Training,[0],[0]
"Each mask at timestep t = 1, . . .",3.2. Training,[0],[0]
", T
max is selected by the left-hand side of the production rule indicated in the 1-hot vector x
t .",3.2. Training,[0],[0]
"Given these masks we can compute the decoder’s mapping
p(X|z)",3.2. Training,[0],[0]
"= T (X)Y
t=1
p(x t |z,x1:(t 1)), (3)
with the individual probabilities at each timestep defined as in Eq. (2).",3.2. Training,[0],[0]
"We pad any remaining timesteps after T (X) up
Algorithm 2 Training the Grammar VAE Input: Dataset {X(i)}N
i=1
Output: Trained VAE model p ✓ (X|z), q (z|X) 1: while VAE not converged do 2: Select element: X 2 {X(i)}N
i=1 (or minibatch) 3:",3.2. Training,[0],[0]
"Encode: z ⇠ q
(z|X) 4: Decode: given z, compute logits F 2 RTmax⇥K 5: for t in [1, . . .",3.2. Training,[0],[0]
", T
max ] do 6: Compute p
✓
(x
t |z) via Eq.",3.2. Training,[0],[0]
"(2), with mask m x
t
and logits f t
7: end for 8: Update ✓, using estimates p
✓
(X|z), q (z|X), via gradient descent on the ELBO in Eq.",3.2. Training,[0],[0]
"(4)
9: end while
to T max with a ‘no-op’ rule, a one-hot vector indicating the parse tree is complete and no actions are to be taken.
",3.2. Training,[0],[0]
"In all our experiments, q(z|X) is a Gaussian distribution whose mean and variance parameters are the output of the encoder network, with an isotropic Gaussian prior p(z)",3.2. Training,[0],[0]
"= N (0, I)",3.2. Training,[0],[0]
.,3.2. Training,[0],[0]
"At training time, we sample a value of z from q(z|X) to compute the ELBO
L( , ✓;X) = E q (z|X)",3.2. Training,[0],[0]
"[log p✓(X, z) log q (z|X)] .",3.2. Training,[0],[0]
"(4)
Following Kingma & Welling (2014), we apply a noncentered parameterization on the encoding Gaussian distribution and optimize Eq.",3.2. Training,[0],[0]
"(4) using gradient descent, learning encoder and decoder neural network parameters and ✓.",3.2. Training,[0],[0]
Algorithm 2 summarizes the training procedure.,3.2. Training,[0],[0]
We show the usefulness of our proposed grammar variational autoencoder (GVAE)2 on two sequence optimization problems: 1) searching for an arithmetic expression that best fits a dataset and 2) finding new drug molecules.,4. Experiments,[0],[0]
"We begin by showing the latent space of the GVAE and a character variational autoencoder (CVAE), similar to that of Gómez-Bombarelli et al. (2016b)3, on each of the problems.",4. Experiments,[0],[0]
"We demonstrate that the GVAE learns a smooth, meaningful latent space for arithmetic equations and molecules.",4. Experiments,[0],[0]
"Given this we perform optimization in this latent space using Bayesian optimization, inspired by the technique of Gómez-Bombarelli et al. (2016b).",4. Experiments,[0],[0]
"We demonstrate that the GVAE improves upon a previous character variational autoencoder, by selecting an arithmetic expression that matches the data nearly perfectly, and by finding novel molecules with better drug properties.
2Code available at: https://github.com/mkusner/grammarVAE 3https://github.com/maxhodak/keras-molecules",4. Experiments,[0],[0]
We describe in detail the two sequence optimization problems we seek to solve.,4.1. Problems,[0],[0]
The first consists in optimizing the fit of an arithmetic expression.,4.1. Problems,[0],[0]
"We are given a set of 100,000 randomly generated univariate arithmetic expressions from the following grammar:
S !",4.1. Problems,[0],[0]
S ‘+ ’ T | S ‘⇤ ’ T | S ‘ / ’ T | T T !,4.1. Problems,[0],[0]
‘ ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ s i n ( ’ S ‘ ) ’,4.1. Problems,[0],[0]
| ‘ exp ( ’ S ‘ ) ’ T !,4.1. Problems,[0],[0]
"‘x ’ | ‘1 ’ | ‘2 ’ | ‘3 ’
where S and T are non-terminals and the symbol | separates the possible production rules generated from each non-terminal.",4.1. Problems,[0],[0]
"By parsing this grammar we can randomly generate strings of univariate arithmetic equations (functions of x) such as the following: sin(2), x/(3+ 1), 2+ x+ sin(1/2), and x/2 ⇤ exp(x)/exp(2 ⇤ x).",4.1. Problems,[0],[0]
We limit the length of every selected string to have at most 15 production rules.,4.1. Problems,[0],[0]
Given this dataset we train both the CVAE and GVAE to learn a latent space of arithmetic expressions.,4.1. Problems,[0],[0]
We propose to perform optimization in this latent space of expressions to find an expression that best fits a fixed dataset.,4.1. Problems,[0],[0]
A common measure of best fit is the test MSE between the predictions made by a selected expression and the true data.,4.1. Problems,[0],[0]
"In the generated expressions, the presence of exponential functions can result in very large MSE values.",4.1. Problems,[0],[0]
"For this reason, we use as target variable log(1 + MSE) instead of MSE.
",4.1. Problems,[0],[0]
"For the second optimization problem, we follow (GómezBombarelli et al., 2016b) and optimize the drug properties of molecules.",4.1. Problems,[0],[0]
"Our goal is to maximize the water-octanol partition coefficient (logP), an important metric in drug design that characterizes the drug-likeness of a molecule.",4.1. Problems,[0],[0]
"As in Gómez-Bombarelli et al. (2016b) we consider a penalized logP score that takes into account other molecular properties such as ring size and synthetic accessibility (Ertl & Schuffenhauer, 2009).",4.1. Problems,[0],[0]
"The training data for the CVAE and GVAE models are 250,000 SMILES strings (Weininger, 1988) extracted at random from the ZINC database by Gómez-Bombarelli et al. (2016b).",4.1. Problems,[0],[0]
We describe the context-free grammar for SMILES strings that we use to train our GVAE in the supplementary material.,4.1. Problems,[0],[0]
Arithmetic expressions.,4.2. Visualizing the latent space,[0],[0]
"To qualitatively evaluate the smoothness of the VAE embeddings for arithmetic expressions, we attempt interpolating between two arithmetic expressions, as in Bowman et al. (2016).",4.2. Visualizing the latent space,[0],[0]
This is done by encoding two equations and then performing linear interpolation in the latent space.,4.2. Visualizing the latent space,[0],[0]
Results comparing the character and grammar VAEs are shown in Table 1.,4.2. Visualizing the latent space,[0],[0]
"Although the character VAE smoothly interpolates between the text representation of equations, it passes through intermediate points which do not decode to valid equations.",4.2. Visualizing the latent space,[0],[0]
"In contrast, the grammar VAE also provides smooth interpolation and produces valid equations for any location in the latent space.",4.2. Visualizing the latent space,[0],[0]
"A further exploration of a 2-dimensional latent space is shown in the appendix.
Molecules.",4.2. Visualizing the latent space,[0],[0]
We are interested if the GVAE produces a coherent latent space of molecules.,4.2. Visualizing the latent space,[0],[0]
To assess this we begin by encoding a molecule.,4.2. Visualizing the latent space,[0],[0]
We then generate 2 random orthogonal unit vectors in latent space (scaled down to only search the neighborhood of the molecules).,4.2. Visualizing the latent space,[0],[0]
Moving in combinations of these directions defines a grid and at each point in the grid we decode the latent vector 1000 times.,4.2. Visualizing the latent space,[0],[0]
We select the molecule that appears most often as the representative molecule.,4.2. Visualizing the latent space,[0],[0]
Figure 3 shows this latent space search surrounding two different molecules.,4.2. Visualizing the latent space,[0],[0]
Compare this to Figures 13-15 in Gómez-Bombarelli et al. (2016b).,4.2. Visualizing the latent space,[0],[0]
"We note that in each plot of the GVAE the latent space is very smooth, in many cases moving from one grid point to another will only change a single atom in a molecule.",4.2. Visualizing the latent space,[0],[0]
"In the CVAE (Gómez-Bombarelli et al., 2016b) we do not observe such fine-grained smoothness.",4.2. Visualizing the latent space,[0],[0]
We now perform a series of experiments using the autoencoders to produce novel sequences with improved properties.,4.3. Bayesian optimization,[0],[0]
"For this, we follow the approach proposed by GómezBombarelli et al. (2016b) and after training the GVAE, we
train an additional model to predict properties of sequences from their latent representation.",4.3. Bayesian optimization,[0],[0]
"To propose promising new sequences, we can start from the latent vector of an encoded sequence and then use the output of this predictor (including its gradient) to move in the latent space direction most likely to improve the property.",4.3. Bayesian optimization,[0],[0]
"The resulting new latent points can then be decoded into corresponding sequences.
",4.3. Bayesian optimization,[0],[0]
"In practice, measuring the property of each new sequence could be an expensive process.",4.3. Bayesian optimization,[0],[0]
"For example, the sequence could represent an organic photovoltaic molecule and the property could be the result of an expensive quantum mechanical simulation used to estimate the molecule’s powerconversion efficiency (Hachmann et al., 2011).",4.3. Bayesian optimization,[0],[0]
The sequence could also represent a program or expression which may be computationally expensive to evaluate.,4.3. Bayesian optimization,[0],[0]
"Therefore, ideally, we would like the optimization process to perform only a reduced number of property evaluations.",4.3. Bayesian optimization,[0],[0]
"For this, we use Bayesian optimization methods, which choose the next point to evaluate by maximizing an acquisition function that quantifies the benefit of evaluating the property at a particular location (Shahriari et al., 2016).
",4.3. Bayesian optimization,[0],[0]
"After training the GVAE, we obtain a latent feature vector for each sequence in the training data, given by the mean of the variational encoding distributions.",4.3. Bayesian optimization,[0],[0]
"We use these vectors and their corresponding property estimates to train a sparse Gaussian process (SGP) model with 500 inducing points (Snelson & Ghahramani, 2005), which is used to make predictions for the properties of new points in latent space.",4.3. Bayesian optimization,[0],[0]
"After training the SGP, we then perform 5 iterations of batch Bayesian optimization using the expected improvement (EI) heuristic (Jones et al., 1998).",4.3. Bayesian optimization,[0],[0]
"On each iteration, we select a batch of 50 latent vectors by sequentially maximizing the EI acquisition function.",4.3. Bayesian optimization,[0],[0]
"We use the Kriging Believer Algorithm to account for pending evaluations in the batch selection process (Cressie, 1990).",4.3. Bayesian optimization,[0],[0]
"That is, after selecting each new data point in the batch, we add that data point as a new inducing point in the sparse GP model with associated target variable equal to the mean of the GP predictive distribution at that point.",4.3. Bayesian optimization,[0],[0]
"Once a new batch of 50 latent vectors is selected, each point in the batch is transformed into its corresponding sequence using the decoder network in the GVAE.",4.3. Bayesian optimization,[0],[0]
The properties of the newly generated sequences are then computed and the resulting data is added to the training set before retraining the SGP and starting the next BO iteration.,4.3. Bayesian optimization,[0],[0]
"Note that some of the new sequences will be invalid and consequently, it will not be possible to obtain their corresponding property estimate.",4.3. Bayesian optimization,[0],[0]
"In this case we fix the property to be equal to the worst value observed in the original training data.
",4.3. Bayesian optimization,[0],[0]
Arithmetic expressions.,4.3. Bayesian optimization,[0],[0]
Our goal is to see if we can find an arithmetic expression that best fits a fixed dataset.,4.3. Bayesian optimization,[0],[0]
"Specifically, we generate this dataset by selecting 1000
input values, x, that are linearly-spaced between 10 and 10.",4.3. Bayesian optimization,[0],[0]
We then pass these through our true function 1/3+ x+,4.3. Bayesian optimization,[0],[0]
sin(x ⇤ x),4.3. Bayesian optimization,[0],[0]
to generate the true target observations.,4.3. Bayesian optimization,[0],[0]
We use Bayesian optimization (BO) as described above search for this equation.,4.3. Bayesian optimization,[0],[0]
We run BO for 5 iterations and average across 10 repetitions of the process.,4.3. Bayesian optimization,[0],[0]
Table 2 (rows 1 & 2) shows the results obtained.,4.3. Bayesian optimization,[0],[0]
The third column in the table reports the fraction of arithmetic sequences found by BO that are valid.,4.3. Bayesian optimization,[0],[0]
The GVAE nearly always finds valid sequences.,4.3. Bayesian optimization,[0],[0]
"The only cases in which it does not is when there are still non-terminals on the stack of
the decoder upon reaching the maximum number of timesteps T
max , however this is rare.",4.3. Bayesian optimization,[0],[0]
"Additionally, the GVAE finds squences with better scores on average when compared with the CVAE.
",4.3. Bayesian optimization,[0],[0]
"Table 3 shows the top 3 expressions found by GVAE and CVAE during the BO search, together with their associated score values.",4.3. Bayesian optimization,[0],[0]
Figure 4 shows how the best expression found by GVAE and CVAE compare to the true function.,4.3. Bayesian optimization,[0],[0]
"We note that the CVAE has failed to find the sinusoidal portion of the true expression, while the difference between the GVAE expression and the true function is negligible.
Molecules.",4.3. Bayesian optimization,[0],[0]
We now consider the problem of finding new drug-like molecules.,4.3. Bayesian optimization,[0],[0]
"We perform 5 iterations of BO, and average results across 10 trials.",4.3. Bayesian optimization,[0],[0]
Table 2 (rows 3 & 4) shows the overall BO results.,4.3. Bayesian optimization,[0],[0]
"In this problem, the GVAE produces about twice more valid sequences than the CVAE.",4.3. Bayesian optimization,[0],[0]
The valid sequences produced by the GVAE also result in higher scores on average.,4.3. Bayesian optimization,[0],[0]
The best found SMILES strings by each method and their scores are shown in Table 4; the molecules themselves are plotted in Figure 5.,4.3. Bayesian optimization,[0],[0]
We now perform a series of experiments to evaluate the predictive performance of the latent representations found by each autoencoder.,4.4. Predictive performance of latent representation,[0],[0]
"For this, we use the sparse GP model used in the previous Bayesian optimization experiments and look at its predictive performance on a left-out test set with 10% of the data, where the data is formed by the latent representation of the available sequences (these are the inputs to the sparse GP model) and the associated properties of those sequences (these are the outputs in the sparse GP model).",4.4. Predictive performance of latent representation,[0],[0]
Table 5 show the average test RMSE and test loglikelihood for the GVAE and the CVAE across 10 different splits of the data for the expressions and for the molecules.,4.4. Predictive performance of latent representation,[0],[0]
This table shows that the GVAE produces latent features that yield much better predictive performance than those produced by the CVAE.,4.4. Predictive performance of latent representation,[0],[0]
"Parse trees have been used to learn continuous representations of text in recursive neural network models (Socher et al., 2013; Irsoy & Cardie, 2014; Paulus et al., 2014).",5. Related Work,[0],[0]
These models learn a vector at every non-terminal in the parse tree by recursively combining the vectors of child nodes.,5. Related Work,[0],[0]
"Recursive autoencoders learn these representations by minimizing the reconstruction error between true child vectors and those predicted by the parent (Socher et al., 2011a;b).",5. Related Work,[0],[0]
"Recently, Allamanis et al. (2016) learn representations for symbolic expressions from their parse trees.",5. Related Work,[0],[0]
"Importantly, all of these methods are discriminative and do not learn a generative latent space.",5. Related Work,[0],[0]
"Like our decoder, re-
current neural network grammars (Dyer et al., 2016) produce sequences through a linear traversal of the parse tree, but focus on the case where the underlying grammar is unknown and not context-free.",5. Related Work,[0],[0]
Maddison & Tarlow (2014) describe generative models of natural source code based on probabilistic context free grammars and neuro-probabilistic language models.,5. Related Work,[0],[0]
"However, these works are not geared towards learning a latent representation of the data.
",5. Related Work,[0],[0]
"Learning arithmetic expressions to fit data, often called symbolic regression, are generally based on genetic programming (Willis et al., 1997) or other computationally demanding evolutionary algorithms to propose candidate expressions (Schmidt & Lipson, 2009).",5. Related Work,[0],[0]
"Alternatives include running particle MCMC inference to estimate a Bayesian posterior over parse trees (Perov & Wood, 2016).
",5. Related Work,[0],[0]
"In molecular design, searching for new molecules is traditionally done by sifting through large databases of potential molecules and then subjecting them to a virtual screening process (Pyzer-Knapp et al., 2015; Gómez-Bombarelli et al., 2016a).",5. Related Work,[0],[0]
"These databases are too large to search via exhaustive enumeration, and require novel stochastic search algorithms tailored to the domain (Virshup et al., 2013; Rupakheti et al., 2015).",5. Related Work,[0],[0]
"Segler et al. (2017) fit a recurrent neural network to chemicals represented by SMILES strings, however their goal is more akin to density estimation; they learn a simulator which can sample proposals for novel molecules, but it is not otherwise used as part of an optimization or inference process itself.",5. Related Work,[0],[0]
"Our work most closely resembles Gómez-Bombarelli et al. (2016b) for novel molecule synthesis, in that we also learn a latent variable model which admits a continuous representation of the domain.",5. Related Work,[0],[0]
"However, both Segler et al. (2017) and Gómez-Bombarelli et al. (2016b) use character-level models for molecules.",5. Related Work,[0],[0]
"Empirically, it is clear that representing molecules and equations by way of their parse tree generated from a grammar outperforms text-based representations.",6. Discussion,[0],[0]
"We believe this approach will be broadly useful for representation learning, inference, and optimization in any domain which can be represented as text in a context-free language.",6. Discussion,[0],[0]
This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.,Acknowledgements,[0],[0]
"Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as natural images, artwork, and audio.",abstractText,[0],[0]
"However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges.",abstractText,[0],[0]
"Crucially, state-of-the-art methods often produce outputs that are not valid.",abstractText,[0],[0]
"We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar.",abstractText,[0],[0]
"We propose a variational autoencoder which directly encodes from and decodes to these parse trees, ensuring the generated outputs are always syntactically valid.",abstractText,[0],[0]
"Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs.",abstractText,[0],[0]
We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecule generation.,abstractText,[0],[0]
Grammar Variational Autoencoder,title,[0],[0]
"Neural machine translation (NMT) is one of success stories of deep learning in natural language processing, with recent NMT systems outperforming traditional phrase-based approaches on many language pairs (Sennrich et al., 2016a).",1 Introduction,[0],[0]
"State-ofthe-art NMT systems rely on sequential encoderdecoders (Sutskever et al., 2014; Bahdanau et al., 2015) and lack any explicit modeling of syntax or any hierarchical structure of language.",1 Introduction,[0],[0]
"One potential reason for why we have not seen much benefit from using syntactic information in NMT is the lack of simple and effective methods for incorporating structured information in neural encoders,
including RNNs.",1 Introduction,[0],[0]
"Despite some successes, techniques explored so far either incorporate syntactic information in NMT models in a relatively indirect way (e.g., multi-task learning (Luong et al., 2015a; Nadejde et al., 2017; Eriguchi et al., 2017; Hashimoto and Tsuruoka, 2017)) or may be too restrictive in modeling the interface between syntax and the translation task (e.g., learning representations of linguistic phrases (Eriguchi et al., 2016)).",1 Introduction,[0],[0]
"Our goal is to provide the encoder with access to rich syntactic information but let it decide which aspects of syntax are beneficial for MT, without placing rigid constraints on the interaction between syntax and the translation task.",1 Introduction,[0],[0]
"This goal is in line with claims that rigid syntactic constraints typically hurt MT (Zollmann and Venugopal, 2006; Smith and Eisner, 2006; Chiang, 2010), and, though these claims have been made in the context of traditional MT systems, we believe they are no less valid for NMT.
",1 Introduction,[0],[0]
"Attention-based NMT systems (Bahdanau et al., 2015; Luong et al., 2015b) represent source sentence words as latent-feature vectors in the encoder and use these vectors when generating a translation.",1 Introduction,[0],[0]
"Our goal is to automatically incorporate information about syntactic neighborhoods of source words into these feature vectors, and, thus, potentially improve quality of the translation output.",1 Introduction,[0],[0]
"Since vectors correspond to words, it is natural for us to use dependency syntax.",1 Introduction,[0],[0]
"Dependency trees (see Figure 1) represent syntactic relations between words: for example, monkey is a subject of the predicate eats, and banana is its object.
",1 Introduction,[0],[0]
"In order to produce syntax-aware feature representations of words, we exploit graphconvolutional networks (GCNs) (Duvenaud et al., 2015; Defferrard et al., 2016; Kearnes et al., 2016; Kipf and Welling, 2016).",1 Introduction,[0],[0]
"GCNs can be regarded as computing a latent-feature representation of a node (i.e. a real-valued vector) based on its k-
1957 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1957–1967
Copenhagen, Denmark, September 7–11, 2017.",1 Introduction,[0],[0]
"c©2017 Association for Computational Linguistics
th order neighborhood (i.e. nodes at most k hops aways from the node) (Gilmer et al., 2017).",1 Introduction,[0],[0]
They are generally simple and computationally inexpensive.,1 Introduction,[0],[0]
"We use Syntactic GCNs, a version of GCN operating on top of syntactic dependency trees, recently shown effective in the context of semantic role labeling (Marcheggiani and Titov, 2017).
",1 Introduction,[0],[0]
"Since syntactic GCNs produce representations at word level, it is straightforward to use them as encoders within the attention-based encoderdecoder framework.",1 Introduction,[0],[0]
"As NMT systems are trained end-to-end, GCNs end up capturing syntactic properties specifically relevant to the translation task.",1 Introduction,[0],[0]
"Though GCNs can take word embeddings as input, we will see that they are more effective when used as layers on top of recurrent neural network (RNN) or convolutional neural network (CNN) encoders (Gehring et al., 2016), enriching their states with syntactic information.",1 Introduction,[0],[0]
"A comparison to RNNs is the most challenging test for GCNs, as it has been shown that RNNs (e.g., LSTMs) are able to capture certain syntactic phenomena (e.g., subject-verb agreement) reasonably well on their own, without explicit treebank supervision (Linzen et al., 2016; Shi et al., 2016).",1 Introduction,[0],[0]
"Nevertheless, GCNs appear beneficial even in this challenging set-up: we obtain +1.2 and +0.7 BLEU point improvements from using syntactic GCNs on top of bidirectional RNNs for EnglishGerman and English-Czech, respectively.
",1 Introduction,[0],[0]
"In principle, GCNs are flexible enough to incorporate any linguistic structure as long as they can be represented as graphs (e.g., dependency-based semantic-role labeling representations (Surdeanu et al., 2008), AMR semantic graphs (Banarescu et al., 2012) and co-reference chains).",1 Introduction,[0],[0]
"For example, unlike recursive neural networks (Socher et al., 2013), GCNs do not require the graphs to be trees.",1 Introduction,[0],[0]
"However, in this work we solely focus on dependency syntax and leave more general investigation for future work.
",1 Introduction,[0],[0]
"Our main contributions can be summarized as follows:
• we introduce a method for incorporating structure into NMT using syntactic GCNs;
• we show that GCNs can be used along with RNN and CNN encoders;
• we show that incorporating structure is beneficial for machine translation on EnglishCzech and English-German.",1 Introduction,[0],[0]
Notation.,2 Background,[0],[0]
"We use x for vectors, x1:t for a sequence of t vectors, and X for matrices.",2 Background,[0],[0]
The i-th value of vector x is denoted by xi.,2 Background,[0],[0]
We use ◦ for vector concatenation.,2 Background,[0],[0]
"In NMT (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014b), given example translation pairs from a parallel corpus, a neural network is trained to directly estimate the conditional distribution p(y1:Ty |x1:Tx) of translating a source sentence",2.1 Neural Machine Translation,[0],[0]
x1:,2.1 Neural Machine Translation,[0],[0]
Tx (a sequence of Tx words) into a target sentence y1:Ty .,2.1 Neural Machine Translation,[0],[0]
"NMT models typically consist of an encoder, a decoder and some method for conditioning the decoder on the encoder, for example, an attention mechanism.",2.1 Neural Machine Translation,[0],[0]
We will now briefly describe the components that we use in this paper.,2.1 Neural Machine Translation,[0],[0]
An encoder is a function that takes as input the source sentence and produces a representation encoding its semantic content.,2.1.1 Encoders,[0],[0]
"We describe recurrent, convolutional and bag-of-words encoders.
",2.1.1 Encoders,[0],[0]
Recurrent.,2.1.1 Encoders,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990) model sequential data.",2.1.1 Encoders,[0],[0]
They receive one input vector at each time step and update their hidden state to summarize all inputs up to that point.,2.1.1 Encoders,[0],[0]
"Given an input sequence x1:Tx = x1,x2, . . .",2.1.1 Encoders,[0],[0]
",xTx of word embeddings an RNN is defined recursively as follows:
",2.1.1 Encoders,[0],[0]
"RNN(x1:t) = f(xt,RNN(x1:t−1))
where f is a nonlinear function such as an LSTM (Hochreiter and Schmidhuber, 1997) or a GRU (Cho et al., 2014b).",2.1.1 Encoders,[0],[0]
"We will use the function RNN as an abstract mapping from an input sequence x1:T to final hidden state RNN(x1:Tx), regardless of the used nonlinearity.",2.1.1 Encoders,[0],[0]
"To not only summarize the past of a word, but also its future, a bidirectional RNN (Schuster and Paliwal, 1997; Irsoy and
Cardie, 2014) is often used.",2.1.1 Encoders,[0],[0]
"A bidirectional RNN reads the input sentence in two directions and then concatenates the states for each time step:
BIRNN(x1:Tx , t) = RNNF (x1:t)◦RNNB(xTx:t)
where RNNF and RNNB are the forward and backward RNNs, respectively.",2.1.1 Encoders,[0],[0]
"For further details we refer to the encoder of Bahdanau et al. (2015).
Convolutional.",2.1.1 Encoders,[0],[0]
"Convolutional Neural Networks (CNNs) apply a fixed-size window over the input sequence to capture the local context of each word (Gehring et al., 2016).",2.1.1 Encoders,[0],[0]
"One advantage of this approach over RNNs is that it allows for fast parallel computation, while sacrificing non-local context.",2.1.1 Encoders,[0],[0]
"To remedy the loss of context, multiple CNN layers can be stacked.",2.1.1 Encoders,[0],[0]
"Formally, given an input sequence x1:Tx , we define a CNN as follows:
",2.1.1 Encoders,[0],[0]
"CNN(x1:Tx , t) = f(xt−bw/2c, ..,xt, ..,xt+bw/2c)
where f is a nonlinear function, typically a linear transformation followed by ReLU, andw is the size of the window.
",2.1.1 Encoders,[0],[0]
Bag-of-Words.,2.1.1 Encoders,[0],[0]
In a bag-of-words (BoW) encoder every word is simply represented by its word embedding.,2.1.1 Encoders,[0],[0]
"To give the decoder some sense of word position, position embeddings (PE) may be added.",2.1.1 Encoders,[0],[0]
"There are different strategies for defining position embeddings, and in this paper we choose to learn a vector for each absolute word position up to a certain maximum length.",2.1.1 Encoders,[0],[0]
"We then represent the t-th word in a sequence as follows:
BOW(x1:Tx , t) = xt + pt
where xt is the word embedding and pt is the t-th position embedding.",2.1.1 Encoders,[0],[0]
A decoder produces the target sentence conditioned on the representation of the source sentence induced by the encoder.,2.1.2 Decoder,[0],[0]
"In Bahdanau et al. (2015) the decoder is implemented as an RNN conditioned on an additional input ci, the context vector, which is dynamically computed at each time step using an attention mechanism.
",2.1.2 Decoder,[0],[0]
"The probability of a target word yi is now a function of the decoder RNN state, the previous target word embedding, and the context vector.",2.1.2 Decoder,[0],[0]
The model is trained end-to-end for maximum log likelihood of the next target word given its context.,2.1.2 Decoder,[0],[0]
We will now describe the Graph Convolutional Networks (GCNs) of Kipf and Welling (2016).,2.2 Graph Convolutional Networks,[0],[0]
"For a comprehensive overview of alternative GCN architectures see Gilmer et al. (2017).
",2.2 Graph Convolutional Networks,[0],[0]
"A GCN is a multilayer neural network that operates directly on a graph, encoding information about the neighborhood of a node as a realvalued vector.",2.2 Graph Convolutional Networks,[0],[0]
"In each GCN layer, information flows along edges of the graph; in other words, each node receives messages from all its immediate neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"When multiple GCN layers are stacked, information about larger neighborhoods gets integrated.",2.2 Graph Convolutional Networks,[0],[0]
"For example, in the second layer, a node will receive information from its immediate neighbors, but this information already includes information from their respective neighbors.",2.2 Graph Convolutional Networks,[0],[0]
"By choosing the number of GCN layers, we regulate the distance the information travels: with k layers a node receives information from neighbors at most k hops away.
",2.2 Graph Convolutional Networks,[0],[0]
"Formally, consider an undirected graph G = (V, E), where V is a set of n nodes, and E is a set of edges.",2.2 Graph Convolutional Networks,[0],[0]
"Every node is assumed to be connected to itself, i.e. ∀v ∈ V : (v, v) ∈ E .",2.2 Graph Convolutional Networks,[0],[0]
"Now, let X ∈ Rd×n be a matrix containing all n nodes with their features, where d is the dimensionality of the feature vectors.",2.2 Graph Convolutional Networks,[0],[0]
"In our case, X will contain word embeddings, but in general it can contain any kind of features.",2.2 Graph Convolutional Networks,[0],[0]
"For a 1-layer GCN, the new node representations are computed as follows:
hv = ρ ( ∑ u∈N (v) Wxu + b )
where W ∈ Rd×d is a weight matrix and b ∈ Rd a bias vector.1 ρ is an activation function, e.g. a ReLU.N (v) is the set of neighbors of v, which we assume here to always include v itself.",2.2 Graph Convolutional Networks,[0],[0]
"As stated before, to allow information to flow over multiple hops, we need to stack GCN layers.",2.2 Graph Convolutional Networks,[0],[0]
"The recursive computation is as follows:
h(j+1)v = ρ",2.2 Graph Convolutional Networks,[0],[0]
"( ∑ u∈N (v) W (j)h(j)u + b (j) )
where j indexes the layer, and h(0)v = xv.
1We dropped the normalization factor used by Kipf and Welling (2016), as it is not used in syntactic GCNs of Marcheggiani and Titov (2017).",2.2 Graph Convolutional Networks,[0],[0]
Marcheggiani and Titov (2017) generalize GCNs to operate on directed and labeled graphs.2,2.3 Syntactic GCNs,[0],[0]
"This makes it possible to use linguistic structures such as dependency trees, where directionality and edge labels play an important role.",2.3 Syntactic GCNs,[0],[0]
They also integrate edge-wise gates which let the model regulate contributions of individual dependency edges.,2.3 Syntactic GCNs,[0],[0]
"We will briefly describe these modifications.
Directionality.",2.3 Syntactic GCNs,[0],[0]
"In order to deal with directionality of edges, separate weight matrices are used for incoming and outgoing edges.",2.3 Syntactic GCNs,[0],[0]
"We follow the convention that in dependency trees heads point to their dependents, and thus outgoing edges are used for head-to-dependent connections, and incoming edges are used for dependent-to-head connections.",2.3 Syntactic GCNs,[0],[0]
"Modifying the recursive computation for directionality, we arrive at:
h(j+1)v = ρ",2.3 Syntactic GCNs,[0],[0]
"( ∑ u∈N (v) W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) dir(u,v) )
",2.3 Syntactic GCNs,[0],[0]
"where dir(u, v) selects the weight matrix associated with the directionality of the edge connecting u and v (i.e. WIN for u-to-v, WOUT for v-to-u, and WLOOP for v-to-v).",2.3 Syntactic GCNs,[0],[0]
"Note that self loops are modeled separately,
so there are now three times as many parameters as in a non-directional GCN.
2For",2.3 Syntactic GCNs,[0],[0]
"an alternative approach to integrating labels and directions, see applications of GCNs to statistical relation learning (Schlichtkrull et al., 2017).
",2.3 Syntactic GCNs,[0],[0]
Labels.,2.3 Syntactic GCNs,[0],[0]
Making the GCN sensitive to labels is straightforward given the above modifications for directionality.,2.3 Syntactic GCNs,[0],[0]
"Instead of using separate matrices for each direction, separate matrices are now defined for each direction and label combination:
h(j+1)v = ρ ( ∑ u∈N (v) W (j) lab(u,v) h (j) u + b (j) lab(u,v) )
where we incorporate the directionality of an edge directly in its label.
",2.3 Syntactic GCNs,[0],[0]
"Importantly, to prevent over-parametrization, only bias terms are made label-specific, in other words: Wlab(u,v) = Wdir(u,v).",2.3 Syntactic GCNs,[0],[0]
"The resulting syntactic GCN is illustrated in Figure 2 (shown on top of a CNN, as we will explain in the subsequent section).
",2.3 Syntactic GCNs,[0],[0]
Edge-wise gating.,2.3 Syntactic GCNs,[0],[0]
"Syntactic GCNs also include gates, which can down-weight the contribution of individual edges.",2.3 Syntactic GCNs,[0],[0]
"They also allow the model to deal with noisy predicted structure, i.e. to ignore potentially erroneous syntactic edges.",2.3 Syntactic GCNs,[0],[0]
"For each edge, a scalar gate is calculated as follows:
g(j)u,v = σ",2.3 Syntactic GCNs,[0],[0]
"( h(j)u · ŵ (j) dir(u,v) + b̂ (j) lab(u,v) )",2.3 Syntactic GCNs,[0],[0]
"where σ is the logistic sigmoid function, and ŵ
(j) dir(u,v) ∈ R d and b̂(j)lab(u,v) ∈",2.3 Syntactic GCNs,[0],[0]
R are learned parameters for the gate.,2.3 Syntactic GCNs,[0],[0]
"The computation becomes:
h(j+1)v =ρ (∑ u∈N (v) g(j)u,v ( W (j) dir(u,v) h (j) u + b",2.3 Syntactic GCNs,[0],[0]
"(j) lab(u,v) ))",2.3 Syntactic GCNs,[0],[0]
"In this work we focus on exploiting structural information on the source side, i.e. in the encoder.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that using an encoder that incorporates syntax will lead to more informative representations of words, and that these representations, when used as context vectors by the decoder, will lead to an improvement in translation quality.",3 Graph Convolutional Encoders,[0],[0]
"Consequently, in all our models, we use the decoder of Bahdanau et al. (2015) and keep this part of the model constant.",3 Graph Convolutional Encoders,[0],[0]
"As is now common practice, we do not use a maxout layer in the decoder, but apart from this we do not deviate from the original definition.",3 Graph Convolutional Encoders,[0],[0]
"In all models we make use of GRUs (Cho et al., 2014b) as our RNN units.
",3 Graph Convolutional Encoders,[0],[0]
"Our models vary in the encoder part, where we exploit the power of GCNs to induce syntacticallyaware representations.",3 Graph Convolutional Encoders,[0],[0]
"We now define a series of encoders of increasing complexity.
BoW + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our first and simplest model, we propose a bag-of-words encoder (with position embeddings, see §2.1.1), with a GCN on top.",3 Graph Convolutional Encoders,[0],[0]
"In other words, inputs h(0) are a sum of embeddings of a word and its position in a sentence.",3 Graph Convolutional Encoders,[0],[0]
"Since the original BoW encoder captures the linear ordering information only in a very crude way (through the position embeddings), the structural information provided by GCN should be highly beneficial.
",3 Graph Convolutional Encoders,[0],[0]
Convolutional + GCN.,3 Graph Convolutional Encoders,[0],[0]
"In our second model, we use convolutional neural networks to learn word representations.",3 Graph Convolutional Encoders,[0],[0]
"CNNs are fast, but by definition only use a limited window of context.",3 Graph Convolutional Encoders,[0],[0]
"Instead of the approach used by Gehring et al. (2016) (i.e. stacking multiple CNN layers on top of each other), we use a GCN to enrich the one-layer CNN representations.",3 Graph Convolutional Encoders,[0],[0]
Figure 2 shows this model.,3 Graph Convolutional Encoders,[0],[0]
"Note that, while the figure shows a CNN with a window size of 3, we will use a larger window size of 5 in our experiments.",3 Graph Convolutional Encoders,[0],[0]
"We expect this model to perform better than BoW + GCN, because of the additional local context captured by the CNN.
BiRNN + GCN.",3 Graph Convolutional Encoders,[0],[0]
"In our third and most powerful model, we employ bidirectional recurrent neural networks.",3 Graph Convolutional Encoders,[0],[0]
"In this model, we start by encoding the source sentence using a BiRNN (i.e. BiGRU), and use the resulting hidden states as input to a GCN.",3 Graph Convolutional Encoders,[0],[0]
"Instead of relying on linear order only, the GCN will allow the encoder to ‘teleport’ over parts of the input sentence, along dependency edges, con-
necting words that otherwise might be far apart.",3 Graph Convolutional Encoders,[0],[0]
"The model might not only benefit from this teleporting capability however; also the nature of the relations between words (i.e. dependency relation types) may be useful, and the GCN exploits this information (see §2.3 for details).
",3 Graph Convolutional Encoders,[0],[0]
"This is the most challenging setup for GCNs, as RNNs have been shown capable of capturing at least some degree of syntactic information without explicit supervision (Linzen et al., 2016), and hence they should be hard to improve on by incorporating treebank syntax.
",3 Graph Convolutional Encoders,[0],[0]
Marcheggiani and Titov (2017) did not observe improvements from using multiple GCN layers in semantic role labeling.,3 Graph Convolutional Encoders,[0],[0]
"However, we do expect that propagating information from further in the tree should be beneficial in principle.",3 Graph Convolutional Encoders,[0],[0]
"We hypothesize that the first layer is the most influential one, capturing most of the syntactic context, and that additional layers only modestly modify the representations.",3 Graph Convolutional Encoders,[0],[0]
"To ease optimization, we add a residual connection (He et al., 2016) between the GCN layers, when using more than one layer.",3 Graph Convolutional Encoders,[0],[0]
"Experiments are performed using the Neural Monkey toolkit3 (Helcl and Libovický, 2017), which implements the model of Bahdanau et al. (2015) in TensorFlow.",4 Experiments,[0],[0]
"We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 (0.0002 for CNN models).4",4 Experiments,[0],[0]
The batch size is set to 80.,4 Experiments,[0],[0]
"Between layers we apply dropout with a probability of 0.2, and in experiments with GCNs5 we use the same value for edge dropout.",4 Experiments,[0],[0]
"We train for 45 epochs, evaluating the BLEU performance of the model every epoch on the validation set.",4 Experiments,[0],[0]
"For testing, we select the model with the highest validation BLEU.",4 Experiments,[0],[0]
L2 regularization is used with a value of 10−8.,4 Experiments,[0],[0]
All the model selection (incl. hyperparameter selections) was performed on the validation set.,4 Experiments,[0],[0]
"In all experiments we obtain translations using a greedy decoder, i.e. we select the output token with the highest probability at each time step.
",4 Experiments,[0],[0]
"We will describe an artificial experiment in §4.1 and MT experiments in §4.2.
",4 Experiments,[0],[0]
"3https://github.com/ufal/neuralmonkey 4Like Gehring et al. (2016) we note that Adam is too aggressive for CNN models, hence we use a lower learning rate.",4 Experiments,[0],[0]
5GCN code at https://github.com/bastings/neuralmonkey,4 Experiments,[0],[0]
Our goal here is to provide an intuition for the capabilities of GCNs.,4.1 Reordering artificial sequences,[0],[0]
We define a reordering task where randomly permuted sequences need to be put back into the original order.,4.1 Reordering artificial sequences,[0],[0]
"We encode the original order using edges, and test if GCNs can successfully exploit them.",4.1 Reordering artificial sequences,[0],[0]
Note that this task is not meant to provide a fair comparison to RNNs.,4.1 Reordering artificial sequences,[0],[0]
"The input (besides the edges) simply does not carry any information about the original ordering, so RNNs cannot possibly solve this task.
Data.",4.1 Reordering artificial sequences,[0],[0]
"From a vocabulary of 26 types, we generate random sequences of 3-10 tokens.",4.1 Reordering artificial sequences,[0],[0]
"We then randomly permute them, pointing every token to its original predecessor with a label sampled from a set of 5 labels.",4.1 Reordering artificial sequences,[0],[0]
"Additionally, we point every token to an arbitrary position in the sequence with a label from a distinct set of 5 ‘fake’ labels.",4.1 Reordering artificial sequences,[0],[0]
"We sample 25000 training and 1000 validation sequences.
",4.1 Reordering artificial sequences,[0],[0]
Model.,4.1 Reordering artificial sequences,[0],[0]
"We use the BiRNN + GCN model, i.e. a bidirectional GRU with a 1-layer GCN on top.",4.1 Reordering artificial sequences,[0],[0]
"We use 32, 64 and 128 units for embeddings, GRU units and GCN layers, respectively.
Results.",4.1 Reordering artificial sequences,[0],[0]
"After 6 epochs of training, the model learns to put permuted sequences back into order, reaching a validation BLEU of 99.2.",4.1 Reordering artificial sequences,[0],[0]
"Figure 3 shows that the mean values of the bias terms of gates (i.e. b̂) for real and fake edges are far apart, suggesting that the GCN learns to distinguish them.",4.1 Reordering artificial sequences,[0],[0]
"Interestingly, this illustrates why edge-wise gating is beneficial.",4.1 Reordering artificial sequences,[0],[0]
"A gate-less model would not understand which of the two outgoing arcs is fake and which is genuine, because only biases b would then be label-dependent.",4.1 Reordering artificial sequences,[0],[0]
"Consequently, it would only do a mediocre job in reordering.",4.1 Reordering artificial sequences,[0],[0]
"Although using label-specific matrices W would also help, this would not scale to the real scenario (see §2.3).",4.1 Reordering artificial sequences,[0],[0]
Data.,4.2 Machine Translation,[0],[0]
For our experiments we use the En-De and En-Cs News Commentary v11 data from the WMT16 translation task.6,4.2 Machine Translation,[0],[0]
For En-De we also train on the full WMT16 data set.,4.2 Machine Translation,[0],[0]
"As our validation set and test set we use newstest2015 and newstest2016, respectively.
",4.2 Machine Translation,[0],[0]
Pre-processing.,4.2 Machine Translation,[0],[0]
"The English sides of the corpora are tokenized and parsed into dependency
6http://www.statmt.org/wmt16/translation-task.html
trees by SyntaxNet,7 using the pre-trained Parsey McParseface model.8 The Czech and German sides are tokenized using the Moses tokenizer.9 Sentence pairs where either side is longer than 50 words are filtered out after tokenization.
",4.2 Machine Translation,[0],[0]
Vocabularies.,4.2 Machine Translation,[0],[0]
"For the English sides, we construct vocabularies from all words except those with a training set frequency smaller than three.",4.2 Machine Translation,[0],[0]
"For Czech and German, to deal with rare words and phenomena such as inflection and compounding, we learn byte-pair encodings (BPE) as described by Sennrich et al. (2016b).",4.2 Machine Translation,[0],[0]
"Given the size of our data set, and following Wu et al. (2016), we use 8000 BPE merges to obtain robust frequencies for our subword units (16000 merges for full data experiment).",4.2 Machine Translation,[0],[0]
"Data set statistics are summarized in Table 1 and vocabulary sizes in Table 2.
Hyperparameters.",4.2 Machine Translation,[0],[0]
"We use 256 units for word embeddings, 512 units for GRUs (800 for En-De full data set experiment), and 512 units for convolutional layers (or equivalently, 512 ‘channels’).",4.2 Machine Translation,[0],[0]
"The dimensionality of the GCN layers is equiva-
7https://github.com/tensorflow/models/tree/master/syntaxnet 8The used dependency parses can be reproduced by using
the syntaxnet/demo.sh shell script.",4.2 Machine Translation,[0],[0]
"9https://github.com/moses-smt/mosesdecoder
lent to the dimensionality of their input.",4.2 Machine Translation,[0],[0]
"We report results for 2-layer GCNs, as we find them most effective (see ablation studies below).
",4.2 Machine Translation,[0],[0]
Baselines.,4.2 Machine Translation,[0],[0]
"We provide three baselines, each with a different encoder: a bag-of-words encoder, a convolutional encoder with window size w = 5, and a BiRNN.",4.2 Machine Translation,[0],[0]
"See §2.1.1 for details.
",4.2 Machine Translation,[0],[0]
Evaluation.,4.2 Machine Translation,[0],[0]
"We report (cased) BLEU results (Papineni et al., 2002) using multi-bleu, as well as Kendall τ reordering scores.10",4.2 Machine Translation,[0],[0]
English-German.,4.2.1 Results,[0],[0]
Table 3 shows test results on English-German.,4.2.1 Results,[0],[0]
"Unsurprisingly, the bag-ofwords baseline performs the worst.",4.2.1 Results,[0],[0]
"We expected the BoW+GCN model to make easy gains over this baseline, which is indeed what happens.",4.2.1 Results,[0],[0]
"The CNN baseline reaches a higher BLEU4 score than the BoW models, but interestingly its BLEU1 score is lower than the BoW+GCN model.",4.2.1 Results,[0],[0]
"The CNN+GCN model improves over the CNN baseline by +1.9 and +1.1 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"The BiRNN, the strongest baseline, reaches a BLEU4 of 14.9.",4.2.1 Results,[0],[0]
"Interestingly, GCNs still manage to improve the result by +2.3 BLEU1 and +1.2 BLEU4 points.",4.2.1 Results,[0],[0]
"Finally, we observe a big jump in BLEU4 by using the full data set and beam search (beam 12).",4.2.1 Results,[0],[0]
"The BiRNN now reaches 23.3, while adding a GCN achieves a score of 23.9.
",4.2.1 Results,[0],[0]
English-Czech.,4.2.1 Results,[0],[0]
Table 4 shows test results on English-Czech.,4.2.1 Results,[0],[0]
"While it is difficult to obtain high absolute BLEU scores on this dataset, we can still see similar relative improvements.",4.2.1 Results,[0],[0]
"Again the BoW baseline scores worst, with the BoW+GCN easily beating that result.",4.2.1 Results,[0],[0]
"The CNN baseline scores BLEU4 of 8.1, but the CNN+GCN improves on that, this time by +1.0 and +0.6 for BLEU1 and BLEU4, respectively.",4.2.1 Results,[0],[0]
"Interestingly, BLEU1 scores for the BoW+GCN and CNN+GCN models are
10See Stanojević and Simaan (2015).",4.2.1 Results,[0],[0]
"TER (Snover et al., 2006) and BEER (Stanojević and Sima’an, 2014) metrics, even though omitted due to space considerations, are consistent with the reported results.
higher than both baselines so far.",4.2.1 Results,[0],[0]
"Finally, the BiRNN baseline scores a BLEU4 of 8.9, but it is again beaten by the BiRNN+GCN model with +1.9 BLEU1 and +0.7 BLEU4.
",4.2.1 Results,[0],[0]
Effect of GCN layers.,4.2.1 Results,[0],[0]
How many GCN layers do we need?,4.2.1 Results,[0],[0]
Every layer gives us an extra hop in the graph and expands the syntactic neighborhood of a word.,4.2.1 Results,[0],[0]
Table 5 shows validation BLEU performance as a function of the number of GCN layers.,4.2.1 Results,[0],[0]
"For English-German, using a 1-layer GCN improves BLEU-1, but surprisingly has little effect on BLEU4.",4.2.1 Results,[0],[0]
"Adding an additional layer gives improvements on both BLEU1 and BLEU4 of +1.3 and +0.73, respectively.",4.2.1 Results,[0],[0]
"For English-Czech, performance increases with each added GCN layer.
",4.2.1 Results,[0],[0]
Effect of sentence length.,4.2.1 Results,[0],[0]
We hypothesize that GCNs should be more beneficial for longer sentences: these are likely to contain long-distance syntactic dependencies which may not be adequately captured by RNNs but directly encoded in GCNs.,4.2.1 Results,[0],[0]
"To test this, we partition the validation data into five buckets and calculate BLEU for each of them.",4.2.1 Results,[0],[0]
Figure 4 shows that GCN-based models outperform their respective baselines rather uniformly across all buckets.,4.2.1 Results,[0],[0]
This is a surprising result.,4.2.1 Results,[0],[0]
"One explanation may be that syntactic parses are noisier for longer sentences, and this prevents us from obtaining extra improvements with GCNs.
",4.2.1 Results,[0],[0]
Discussion.,4.2.1 Results,[0],[0]
Results suggest that the syntaxaware representations provided by GCNs consistently lead to improved translation performance as measured by BLEU4 (as well as TER and BEER).,4.2.1 Results,[0],[0]
"Consistent gains in terms of Kendall tau and BLEU1 indicate that improvements correlate with better word order and lexical/BPE selection, two phenomena that depend crucially on syntax.",4.2.1 Results,[0],[0]
"We review various accounts to syntax in NMT as well as other convolutional encoders.
",5 Related Work,[0],[0]
Syntactic features and/or constraints.,5 Related Work,[0],[0]
"Sennrich and Haddow (2016) embed features such as POS-tags, lemmas and dependency labels and feed these into the network along with word embeddings.",5 Related Work,[0],[0]
Eriguchi et al. (2016) parse English sentences with an HPSG parser and use a Tree-LSTM to encode the internal nodes of the tree.,5 Related Work,[0],[0]
"In the decoder, word and node representations compete under the same attention mechanism.",5 Related Work,[0],[0]
"Stahlberg et al. (2016) use a pruned lattice from a hierarchical phrase-based model (hiero) to constrain NMT.
Hiero trees are not syntactically-aware, but instead constrained by symmetrized word alignments.",5 Related Work,[0],[0]
"Aharoni and Goldberg (2017) propose neural string-to-tree by predicting linearized parse trees.
",5 Related Work,[0],[0]
Multi-task Learning.,5 Related Work,[0],[0]
Sharing NMT parameters with a syntactic parser is a popular approach to obtaining syntactically-aware representations.,5 Related Work,[0],[0]
Luong et al. (2015a) predict linearized constituency parses as an additional task.,5 Related Work,[0],[0]
"Eriguchi et al. (2017) multi-task with a target-side RNNG parser (Dyer et al., 2016) and improve on various language pairs with English on the target side.",5 Related Work,[0],[0]
Nadejde et al. (2017) multi,5 Related Work,[0],[0]
"-task with CCG tagging, and also integrate syntax on the target side by predicting a sequence of words interleaved with CCG supertags.
",5 Related Work,[0],[0]
Latent structure.,5 Related Work,[0],[0]
Hashimoto and Tsuruoka (2017) add a syntax-inspired encoder on top of a BiLSTM layer.,5 Related Work,[0],[0]
They encode source words as a learned average of potential parents emulating a relaxed dependency tree.,5 Related Work,[0],[0]
"While their model is trained purely on translation data, they also experiment with pre-training the encoder using treebank annotation and report modest improvements on English-Japanese.",5 Related Work,[0],[0]
"Yogatama et al. (2016) introduce a model for language understanding and generation that composes words into sentences by inducing unlabeled binary bracketing trees.
",5 Related Work,[0],[0]
Convolutional encoders.,5 Related Work,[0],[0]
Gehring et al. (2016) show that CNNs can be competitive to BiRNNs when used as encoders.,5 Related Work,[0],[0]
To increase the receptive field of a word’s context they stack multiple CNN layers.,5 Related Work,[0],[0]
Kalchbrenner et al. (2016) use convolution in both the encoder and the decoder; they make use of dilation to increase the receptive field.,5 Related Work,[0],[0]
"In contrast to both approaches, we use a GCN informed by dependency structure to increase it.",5 Related Work,[0],[0]
"Finally, Cho et al. (2014a) propose a recursive convolutional neural network which builds a tree out of the word leaf nodes, but which ends up compressing the source sentence in a single vector.",5 Related Work,[0],[0]
We have presented a simple and effective approach to integrating syntax into neural machine translation models and have shown consistent BLEU4 improvements for two challenging language pairs: English-German and English-Czech.,6 Conclusions,[0],[0]
"Since GCNs are capable of encoding any kind of graph-based structure, in future work we would like to go be-
yond syntax, by using semantic annotations such as SRL and AMR, and co-reference chains.",6 Conclusions,[0],[0]
We would like to thank Michael Schlichtkrull and Thomas Kipf for their suggestions and comments.,Acknowledgments,[0],[0]
"This work was supported by the European Research Council (ERC StG BroadSem 678254) and the Dutch National Science Foundation (NWO VIDI 639.022.518, NWO VICI 277-89-002).",Acknowledgments,[0],[0]
We present a simple and effective approach to incorporating syntactic structure into neural attention-based encoderdecoder models for machine translation.,abstractText,[0],[0]
"We rely on graph-convolutional networks (GCNs), a recent class of neural networks developed for modeling graph-structured data.",abstractText,[0],[0]
Our GCNs use predicted syntactic dependency trees of source sentences to produce representations of words (i.e. hidden states of the encoder) that are sensitive to their syntactic neighborhoods.,abstractText,[0],[0]
"GCNs take word representations as input and produce word representations as output, so they can easily be incorporated as layers into standard encoders (e.g., on top of bidirectional RNNs or convolutional neural networks).",abstractText,[0],[0]
We evaluate their effectiveness with English-German and English-Czech translation experiments for different types of encoders and observe substantial improvements over their syntax-agnostic versions in all the considered setups.,abstractText,[0],[0]
Graph Convolutional Encoders for Syntax-aware Neural Machine Translation,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 772–776, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
The goal of relation extraction is to extract tuples of a particular relation from a corpus of natural language text.,1 Introduction,[0],[0]
"A widely employed approach to relation extraction is based on iterative bootstrapping (Brin, 1998; Agichtein and Gravano, 2000; Pasca et al., 2006; Pantel and Pennacchiotti, 2006), which can be applied with only small amounts of supervision and which scales well to very large datasets.
",1 Introduction,[0],[0]
"A well-known problem with iterative bootstrapping is a phenomenon known as semantic drift (Curran et al., 2007): as bootstrapping proceeds it is likely that unreliable patterns will lead to false extractions.",1 Introduction,[0],[0]
"These extraction errors are amplified in the following iterations and the extracted relation will drift away
from the intended target.",1 Introduction,[0],[0]
Semantic drift often results in low precision extractions and therefore poses a major limitation of iterative bootstrapping algorithms.,1 Introduction,[0],[0]
"Previous work on iterative bootstrapping has addressed the issue of reducing semantic drift for example by bagging the results of various runs employing differing seed tuples, constructing filters which identify false tuples or patterns and adding further constraints to the bootstrapping process (T. McIntosh, 2010; McIntosh and Curran, 2009; Curran et al., 2007).
",1 Introduction,[0],[0]
"However, the analysis of Komachi et al. (2008) has shown that semantic drift is an inherent property of iterative bootstrapping algorithms and therefore poses a fundamental problem.",1 Introduction,[0],[0]
"They have shown that iterative bootstrapping without pruning corresponds to an eigenvector computation and thus as the number of iterations increases the resulting ranking will always converge towards the same static ranking of tuples, regardless of the particular choice of seed instances.
",1 Introduction,[0],[0]
"In this paper, we describe an alternative method, that is not susceptible to semantic drift.",1 Introduction,[0],[0]
"We represent our data as a bipartite graph, whose vertices correspond to patterns and tuples respectively and whose edges capture cooccurrences and then measure the distance of a tuple to the seed set in terms of random walk hitting times.",1 Introduction,[0],[0]
"Experimental results confirm that semantic drift is avoided by our method and show that substantial improvements over iterative forms of bootstrapping are possible.
772",1 Introduction,[0],[0]
"From a given corpus, we extract a dataset consisting of tuples and patterns.",2 Scoring with Hitting Times,[0],[0]
"Tuples are pairs of co-occurring strings in the corpus, such as (Bill Gates, Microsoft), which potentially belong to a particular relation of interest.",2 Scoring with Hitting Times,[0],[0]
"In our case, patterns are simply the sequence of tokens occurring between tuple elements, e.g. “is the founder of”.",2 Scoring with Hitting Times,[0],[0]
"We represent all the tuple types1 X and all the extraction pattern types Y contained in a given corpus through an undirected, weighted, bipartite graph G = (V,E) with vertices V = X ∪ Y and edges E ⊂",2 Scoring with Hitting Times,[0],[0]
X,2 Scoring with Hitting Times,[0],[0]
"× Y , where an edge (x, y) ∈ E indicates that tuple x occurrs with pattern y somewhere in the corpus.",2 Scoring with Hitting Times,[0],[0]
"Edge weights are defined through a weight matrix W which holds the weight Wi,j = w(vi, vj) for edges (vi, vj) ∈ E. Specifically, we use the count of how many times a tuple occurs with a pattern in the corpus and weights for unconnected vertices are zero.
",2 Scoring with Hitting Times,[0],[0]
"Our goal is to compute a score vector σ holding a score σi = σ(xi) for each tuple xi ∈ X, which quantifies how well the tuple matches the seed tuples.",2 Scoring with Hitting Times,[0],[0]
"Higher scores indicate that the tuple is more likely to belong to the relation defined through the seeds and thus the score vector effectively provides a ranking of the tuples.
",2 Scoring with Hitting Times,[0],[0]
We define scores of tuples based on their distance2 to the seed tuples in the graph.,2 Scoring with Hitting Times,[0],[0]
"The distance of some tuple x to the seed set S can be naturally formalized in terms of the average time it takes until a random walk starting in S reaches x, the hitting time.",2 Scoring with Hitting Times,[0],[0]
The random walk is defined through the probability distribution over start vertices and through a matrix of transition probabilities.,2 Scoring with Hitting Times,[0],[0]
"Edge weights are constrained to be non-negative, which allows us to define the transition matrix P with Pi,j = p(vj |vi) = 1dviw(vi, vj), where dv =∑
vk∈V w(v, vk) is the degree of a vertex v ∈ V .",2 Scoring with Hitting Times,[0],[0]
The distance of two vertices is measured in terms of the average time of a random walk be1Note that we are using tuple and pattern types rather than particular mentions in the corpus. 2The term is used informally.,2 Scoring with Hitting Times,[0],[0]
"In particular, hitting times are not a distance metric, since they can be asymmetric.
",2 Scoring with Hitting Times,[0],[0]
tween the two.,2 Scoring with Hitting Times,[0],[0]
"Specifically, we adopt the notion of T-truncated hitting time (Sarkar and Moore, 2007) defined as the expected number of steps it takes until a random walk of at most T steps starting at vi reaches vj for the first time:
hT",2 Scoring with Hitting Times,[0],[0]
(vj |vi) = { 0 iff.,2 Scoring with Hitting Times,[0],[0]
"vj = vi or T=0 1 + ∑ vk∈V p(vk|vi)h T−1(vj |vk)
",2 Scoring with Hitting Times,[0],[0]
The truncated hitting time hT,2 Scoring with Hitting Times,[0],[0]
"(vj |vi) can be approximately computed by sampling M independent random walks starting at vi of length T and computing
ĥT",2 Scoring with Hitting Times,[0],[0]
(vj |vi),2 Scoring with Hitting Times,[0],[0]
"= 1
M m∑ k=1 tk",2 Scoring with Hitting Times,[0],[0]
"+ (1− m M )T (1)
",2 Scoring with Hitting Times,[0],[0]
where {t1 . . .,2 Scoring with Hitting Times,[0],[0]
"tm} are the sampled first-hit times of random walks which reach vj within T steps (Sarkar et al., 2008).
",2 Scoring with Hitting Times,[0],[0]
The score σHT (v) of a vertex v /∈,2 Scoring with Hitting Times,[0],[0]
"S to the seed set S is then defined as the inverse of the average T -truncated hitting time of random walks starting at a randomly chosen vertex s ∈ S:
1 σHT (v) = hT",2 Scoring with Hitting Times,[0],[0]
(v|S) = 1 |S| ∑ s∈S hT (v|s) (2),2 Scoring with Hitting Times,[0],[0]
"We extracted tuples and patterns from the fifth edition of the Gigaword corpus (Parker et al., 2011), by running a named entity tagger and extracting all pairs of named entities and extracting occurring within the same sentence which do not have another named entity standing between them.",3 Experiments,[0],[0]
"Gold standard seed and test tuples for a set of relations were obtained from YAGO (Suchanek et al., 2007).",3 Experiments,[0],[0]
"Specifically, we took all relations for which there are at least 300 tuples, each of which occurs at least once in the corpus.",3 Experiments,[0],[0]
"This resulted in the set of relations shown in Table 1, plus the development relation hasWonPrize.
",3 Experiments,[0],[0]
"For evaluation, we use the percentile rank of the median test set element (PRM, see Francois et al. 2007), which reflects the quality of the
full produced ranking, not just the top N elements and is furthermore computable with only a small set of labeled test tuples 3.
",3 Experiments,[0],[0]
We compare our proposed method based on hitting times (HT) with two variants of iterative bootstrapping.,3 Experiments,[0],[0]
The first one (IB1) does not employ pruning and corresponds to the algorithm described in Komachi et al. (2008).,3 Experiments,[0],[0]
The second one (IB2) corresponds to a standard bootstrapping algorithm which employs pruning after each step in order to reduce semantic drift.,3 Experiments,[0],[0]
"Specifically, scores are pruned after projecting from X onto Y and from Y onto X, retaining only the top N (t) =",3 Experiments,[0],[0]
N0t scores at iteration t and setting all other scores to zero.,3 Experiments,[0],[0]
The experiments in this section were conducted on the held out development relation hasWonPrize.,3.1 Parametrizations,[0],[0]
"The ranking produced by both forms of iterative bootstrapping IB1 and IB2 depend on the number of iterations, as shown in Figure 1.",3.1 Parametrizations,[0],[0]
IB1 achieves an optimal ranking after just one iteration and thereafter scores get worse due to semantic drift.,3.1 Parametrizations,[0],[0]
"In contrast, pruning helps avoid semantic drift for IB2, which attains an optimal score after 2 iterations and achieves relatively constant scores for several iterations.",3.1 Parametrizations,[0],[0]
"However, during iteration 9 an incorrect pattern is kept and this at once leads to a drastic loss in accuracy, showing that semantic drift is only deferred and not completely eliminated.
",3.1 Parametrizations,[0],[0]
"Our method HT has parameter T , corresponding to the truncation time, i.e., maximal number of steps of a random walk.",3.1 Parametrizations,[0],[0]
Figure 2 shows the PRM of our method for different values of T .,3.1 Parametrizations,[0],[0]
"Performance gets better as T increases and is optimal for T = 12, whereas for larger values, the performance gets slightly worse again.",3.1 Parametrizations,[0],[0]
"The figure shows that, if T is large enough (> 5), the PRM is relatively constant and there is no phenomenon comparable to semantic drift, which causes instability in the produced rankings.
",3.1 Parametrizations,[0],[0]
3other common metrics do not satisfy these conditions.,3.1 Parametrizations,[0],[0]
"To evaluate the methods, firstly the parameters for each method were set to the optimal values as determined in the previous section.",3.2 Method Comparison,[0],[0]
"For the experiments here, we again use 200 randomly chosen tuples as the seeds for each relation.",3.2 Method Comparison,[0],[0]
"All the remaining gold standard tuples are used for testing.
",3.2 Method Comparison,[0],[0]
Table 1 shows the PRM for the three methods.,3.2 Method Comparison,[0],[0]
"For a majority of the relations (12/16) HT attains the best, i.e. lowest, PRM, which confirms that hitting times constitute an accurate way of measuring the distance of tuples to the seed set.",3.2 Method Comparison,[0],[0]
IB1 and IB2 each perform best on 2/16 of the relations.,3.2 Method Comparison,[0],[0]
"A sign test on these results yields that
HT is better than both IB1 and IB2 at significance level α < 0.01.
",3.2 Method Comparison,[0],[0]
"Moreover, the ranking produced by HT is stable and not affected by semantic drift, given that even where results are worse than for IB1 or IB2, they are still close to the best performing method.",3.2 Method Comparison,[0],[0]
"In contrast, when semantic drift occurs, the performance of IB1 and IB2 can deteriorate drastically, e.g. for the worksAt relation, where both IB1 and IB2 produce rankings that are a lot worse than the one produced by HT.",3.2 Method Comparison,[0],[0]
Figure 3 shows the PRM for each of the three methods as a function of the size of the seed set for the relation created.,3.3 Sensitivity to Seed Set Size,[0],[0]
"For small seed sets, the performance of the iterative methods can be increased by adding more seeds.",3.3 Sensitivity to Seed Set Size,[0],[0]
"However, from a seed set size of 50 onwards, performance remains relatively constant.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In other words, iterative bootstrapping is not benefitting from the information provided by the additional labeled data, and thus has a poor learning performance.",3.3 Sensitivity to Seed Set Size,[0],[0]
"In contrast, for our method based on hitting times, the performance continually improves as the seed set size is increased.",3.3 Sensitivity to Seed Set Size,[0],[0]
"Thus, also in terms of learning performance, our method is more sound than iterative bootstrapping.",3.3 Sensitivity to Seed Set Size,[0],[0]
The paper has presented a graph-based method for seed set expansion which is not susceptible to semantic drift and on most relations outperforms iterative bootstrapping.,4 Conclusions,[0],[0]
The method measures distance between vertices through random walk hitting times.,4 Conclusions,[0],[0]
"One property which makes hitting times an appropriate distance measure is their ability to reflect the overall connectivity structure of the graph, in contrast to measures such as the shortest path between two vertices.",4 Conclusions,[0],[0]
"The hitting time will decrease when the number of paths from the start vertex to the target vertex increases, when the length of paths decreases or when the likelihood (weights) of paths increases.",4 Conclusions,[0],[0]
"These properties are particularly important when the observed graph edges must be assumed to be merely a sample of all plausible edges, possibly perturbated by noise.",4 Conclusions,[0],[0]
"This has also been asserted by previous work, which has shown that hitting times successfully capture the notion of similarity for other natural language processing problems such as learning paraphrases (Kok and Brockett, 2010) and related problems such as query suggestion (Mei et al., 2008).",4 Conclusions,[0],[0]
Future work will be aimed towards employing our hitting time based method in combination with a richer feature set.,4 Conclusions,[0],[0]
"Iterative bootstrapping methods are widely employed for relation extraction, especially because they require only a small amount of human supervision.",abstractText,[0],[0]
"Unfortunately, a phenomenon known as semantic drift can affect the accuracy of iterative bootstrapping and lead to poor extractions.",abstractText,[0],[0]
"This paper proposes an alternative bootstrapping method, which ranks relation tuples by measuring their distance to the seed tuples in a bipartite tuple-pattern graph.",abstractText,[0],[0]
"In contrast to previous bootstrapping methods, our method is not susceptible to semantic drift, and it empirically results in better extractions than iterative methods.",abstractText,[0],[0]
Graph-Based Seed Set Expansion for Relation Extraction Using Random Walk Hitting Times,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1537–1546 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Metaphor is pervasive in our everyday communication, enriching it with sophisticated imagery and helping us to reconcile our experience in the world with our conceptual system (Lakoff and Johnson, 1980).",1 Introduction,[0],[0]
"In the most influential account of metaphor to date, Lakoff and Johnson explain the phenomenon through the presence of systematic metaphorical associations between two distinct concepts or domains.",1 Introduction,[0],[0]
"For instance, when we talk about “curing juvenile delinquency” or “corruption transmitting through the government ranks”, we view the general concept of crime (the target concept) in terms of the properties of a disease (the source concept).",1 Introduction,[0],[0]
"Such metaphorical associations are broad generalisations that allow us to project knowledge and inferences across domains; and our metaphorical use of language is a reflection of this process.
",1 Introduction,[0],[0]
"Given its ubiquity, metaphorical language poses an important problem for natural language understanding (Cameron, 2003; Shutova and Teufel, 2010).",1 Introduction,[0],[0]
"A number of approaches to metaphor processing have thus been proposed, focusing pre-
dominantly on classifying linguistic expressions as literal or metaphorical.",1 Introduction,[0],[0]
"They experimented with a range of features, including lexical and syntactic information (Hovy et al., 2013; Beigman Klebanov et al., 2016) and higher-level features such as semantic roles (Gedigian et al., 2006), domain types (Dunn, 2013), concreteness (Turney et al., 2011), imageability (Strzalkowski et al., 2013) and WordNet supersenses (Tsvetkov et al., 2014).",1 Introduction,[0],[0]
"While reporting promising results, all of these approaches used hand-engineered features and relied on manually-annotated resources to extract them.",1 Introduction,[0],[0]
"In order to reduce the reliance on manual annotation, other researchers experimented with sparse distributional features (Shutova et al., 2010; Shutova and Sun, 2013) and dense neural word embeddings (Bracewell et al., 2014; Shutova et al., 2016).",1 Introduction,[0],[0]
"Their experiments have demonstrated that corpus-driven lexical representations already encode information about semantic domains needed to learn the patterns of metaphor usage from linguistic data.
",1 Introduction,[0],[0]
We take this intuition a step further and present the first deep learning architecture designed to capture metaphorical composition.,1 Introduction,[0],[0]
"Deep learning methods have already been shown successful in many other semantic tasks (e.g. Hermann et al., 2015; Kumar et al., 2015; Zhao et al., 2015), which suggests that designing a specialised neural network architecture for metaphor detection will lead to improved performance.",1 Introduction,[0],[0]
"In this paper, we present a novel architecture which (1) models the interaction between the source and target domains in the metaphor via a gating function; (2) specialises word representations for the metaphor identification task via supervised training; (3) quantifies metaphoricity via a weighted similarity function that automatically selects the relevant dimensions of similarity.",1 Introduction,[0],[0]
"We experimented with two types of word representations
1537
as inputs to the network: the standard skip-gram word embeddings (Mikolov et al., 2013a) and the cognitively-driven attribute-based vectors (Bulat et al., 2017), as well as a combination thereof.
",1 Introduction,[0],[0]
"We evaluate our method in the metaphor identification task, focusing on adjective–noun, verb– subject and verb–direct object constructions where the verbs and adjectives can be used metaphorically.",1 Introduction,[0],[0]
Our results show that our architecture outperforms both a metaphor agnostic deep learning baseline (a basic feed forward network) and the previous corpus-based approaches to metaphor identification.,1 Introduction,[0],[0]
"We also investigate the effects of training data on this task, and demonstrate that with a sufficiently large training set our method also outperforms the best existing systems based on hand-coded lexical knowledge.",1 Introduction,[0],[0]
The majority of approaches to metaphor processing cast the problem as classification of linguistic expressions as metaphorical or literal.,2 Related Work,[0],[0]
Gedigian et al. (2006) classified verbs related to MOTION and CURE within the domain of financial discourse.,2 Related Work,[0],[0]
"They used the maximum entropy classifier and the verbs’ nominal arguments and their FrameNet roles (Fillmore et al., 2003) as features, reporting encouraging results.",2 Related Work,[0],[0]
"Dunn (2013) used a logistic regression classifier and high-level properties of concepts extracted from SUMO ontology, including domain types (ABSTRACT, PHYSICAL, SOCIAL, MENTAL) and event status (PROCESS, STATE, OBJECT).",2 Related Work,[0],[0]
"Tsvetkov et al. (2014) used random forest classifier and coarse semantic features, such as concreteness, animateness, named entity types and WordNet supersenses.",2 Related Work,[0],[0]
They have shown that the model learned with such coarse semantic features is portable across languages.,2 Related Work,[0],[0]
The work of Hovy et al. (2013) is notable as they focused on compositional rather than categorical features.,2 Related Work,[0],[0]
"They trained an SVM with dependency-tree kernels to capture compositional information, using lexical, part-of-speech tag and WordNet supersense representations of sentence trees.",2 Related Work,[0],[0]
Mohler et al. (2013) aimed at modelling conceptual information.,2 Related Work,[0],[0]
They derived semantic signatures of texts as sets of highly-related and interlinked WordNet synsets.,2 Related Work,[0],[0]
"The semantic signatures served as features to train a set of classifiers (maximum entropy, decision trees, SVM, random forest) that mapped new metaphors to the semantic signatures
of the known ones.
",2 Related Work,[0],[0]
"With the aim of reducing the dependence on manually-annotated lexical resources, other research focused on modelling metaphor using corpus-driven information alone.",2 Related Work,[0],[0]
Shutova et al. (2010) pointed out that the metaphorical uses of words constitute a large portion of the dependency features extracted for abstract concepts from corpora.,2 Related Work,[0],[0]
"For example, the feature vector for politics would contain GAME or MECHANISM terms among the frequent features.",2 Related Work,[0],[0]
"As a result, distributional clustering of abstract nouns with such features identifies groups of diverse concepts metaphorically associated with the same source domain.",2 Related Work,[0],[0]
Shutova et al. (2010) exploit this property of co-occurrence vectors to identify new metaphorical mappings starting from a set of examples.,2 Related Work,[0],[0]
Shutova and Sun (2013) used hierarchical clustering to derive a network of concepts in which metaphorical associations are learned in an unsupervised way.,2 Related Work,[0],[0]
"Do Dinh and Gurevych (2016) investigated metaphors through the task of sequence labelling, detecting metaphor related words in context.",2 Related Work,[0],[0]
Gutiérrez et al. (2016) investigated metaphorical composition in the compositional distributional semantics framework.,2 Related Work,[0],[0]
"Their method learns metaphors as linear transformations in a vector space and they demonstrated that it produces superior phrase representations for both metaphorical and literal language, as compared to the traditional ”single-sense” compositional distributional model.",2 Related Work,[0],[0]
"They then used these representations in the metaphor identification task, achieving promising results.
",2 Related Work,[0],[0]
"The more recent approaches of Shutova et al. (2016) and Bulat et al. (2017) used dense skipgram word embeddings (Mikolov et al., 2013a) instead of the sparse distributional features.",2 Related Work,[0],[0]
Shutova et al. (2016) investigated a set of metaphor identification methods using linguistic and visual features.,2 Related Work,[0],[0]
"They learned linguistic and visual representations for both words and phrases, using skipgram and convolutional neural networks (Kiela and Bottou, 2014) respectively.",2 Related Work,[0],[0]
"They then measured the difference between the phrase representation and those of its component words in terms of their cosine similarity, which served as a predictor of metaphoricity.",2 Related Work,[0],[0]
"They found basic cosine similarity between the component words in the phrase to be a powerful measure – the neural embeddings of the words were compared with cosine similar-
ity and a threshold was tuned on the development set to distinguish between literal and metaphorical phrases.",2 Related Work,[0],[0]
"This approach was their best performing linguistic model, outperformed only by a multimodal system which included both linguistic and visual features.
",2 Related Work,[0],[0]
"Bulat et al. (2017) presented a metaphor identification method that uses representations constructed from human property norms (McRae et al., 2005).",2 Related Work,[0],[0]
"They first learn a mapping from the skip-gram embedding vector space to the property norm space using linear regression, which allows them to generate property norm representations for unseen words.",2 Related Work,[0],[0]
The authors then train an SVM classifier to detect metaphors using these representations as input.,2 Related Work,[0],[0]
Bulat et al. (2017) have shown that the cognitively-driven property norms outperform standard skip-gram representations in this task.,2 Related Work,[0],[0]
"Our method is inspired by the findings of Shutova et al. (2016), who showed that the cosine similarity between neural embeddings of the two words in a phrase is indicative of its metaphoricity.",3 Supervised Similarity Network,[0],[0]
"For example, the phrase ‘colourful personality’ receives a score:
s = cos(xc, xp) (1)
where xc is the embedding for colourful and xp is the embedding for personality.",3 Supervised Similarity Network,[0],[0]
"The combined phrase is classified as being metaphorical based on a threshold, which is optimised on a development dataset.",3 Supervised Similarity Network,[0],[0]
"In this paper, we propose several extensions to this general idea, creating a supervised version of the cosine similarity metric which can be optimised on training data to be more suitable for metaphor detection.",3 Supervised Similarity Network,[0],[0]
Directly comparing the vector representations of both words treats each of the embeddings as an independent unit.,3.1 Word Representation Gating,[0],[0]
"In reality, however, word meanings vary and adapt based on the context.",3.1 Word Representation Gating,[0],[0]
"In case of metaphorical language (e.g. “cure crime”), the source domain properties of the verb (e.g. cure) are projected onto the target domain noun (e.g. crime), resulting in the interaction of the two domains in the interpretation of the metaphor.
",3.1 Word Representation Gating,[0],[0]
"In order to integrate this idea into the metaphor detection method, we can construct a gating function that modulates the representation of one word based on the other.",3.1 Word Representation Gating,[0],[0]
"Given embeddings x1 and x2, the gating values are predicted as a non-linear transformation of x1 and applied to x2 through element-wise multiplication:
g = σ(Wgx1) (2)
x̃2",3.1 Word Representation Gating,[0],[0]
"= x2 g (3)
",3.1 Word Representation Gating,[0],[0]
"whereWg is a weight matrix that is optimised during training, σ is the sigmoid activation function, and represents element-wise multiplication.",3.1 Word Representation Gating,[0],[0]
"In an adjective-noun phrase, this architecture allows the network to first look at the adjective, then use its meaning to change the representation of the noun.",3.1 Word Representation Gating,[0],[0]
"The sigmoid activation function makes it act as a filter, choosing which information from the original embedding gets through to the rest of the network.",3.1 Word Representation Gating,[0],[0]
"While learning a more complex gating function could be beneficial for very large training resources, the filtering approach is more suitable for the annotated metaphor datasets which are relatively small in size.",3.1 Word Representation Gating,[0],[0]
"As the next step, we implement position-specific mappings for the word embeddings.",3.2 Vector Space Mapping,[0],[0]
"The original method uses word embeddings that have been pretrained using the distributional skip-gram objective (Mikolov et al., 2013a).",3.2 Vector Space Mapping,[0],[0]
"While this tunes the vectors for predicting context words, there is no reason to believe that the same space is also optimal for the task of metaphor detection.",3.2 Vector Space Mapping,[0],[0]
"In order to address this shortcoming, we allow the model to learn a mapping from the skip-gram vector space to a new metaphor-specific vector space:
z1 = tanh(Wz1x1) (4)
z2 = tanh(Wz2 x̃2) (5)
where Wz1 and Wz2 are weight matrices, z1 and z2 are the new position-specific word representations.",3.2 Vector Space Mapping,[0],[0]
"While the original embeddings x1 and x2 are pre-trained on a large unannotated corpus, the transformation process is optimised using annotated metaphor examples, resulting in word representations that are more suitable for this task.",3.2 Vector Space Mapping,[0],[0]
"Furthermore, the adjectives and nouns use separate mapping weights, which allows the model to better distinguish between the different functionalities of these words.",3.2 Vector Space Mapping,[0],[0]
"In contrast, the original cosine similarity is not position-specific and would give the same result regardless of the word order.",3.2 Vector Space Mapping,[0],[0]
"If the vectors x1 and x2 are normalised to unit length, the cosine similarity between them is equal to their dot product, which in turn is equal to their elementwise multiplication followed by a sum over all elements:
cos(x1, x2) ∝",3.3 Weighted Cosine,[0],[0]
"∑
i
x1,ix2,i (6)
",3.3 Weighted Cosine,[0],[0]
This calculation of cosine similarity can be formulated as a small neural network where the two unit-normalised input vectors are directly multiplied together.,3.3 Weighted Cosine,[0],[0]
"This is followed by a single output neuron, with all the intermediate weights set to value 1.",3.3 Weighted Cosine,[0],[0]
"Such a network would calculate the same sum over the element-wise multiplication, outputting the value of cosine similarity.
",3.3 Weighted Cosine,[0],[0]
"Since there is no reason to assume that all the embedding dimensions are equally important when detecting metaphors, we can explore other strategies for weighting the similarity calculation.
",3.3 Weighted Cosine,[0],[0]
Rei and Briscoe (2014) used a fixed formula to calculate weights for different dimensions of cosine similarity and showed that it helped in recovering hyponym relations.,3.3 Weighted Cosine,[0],[0]
We extend this even further and allow the network to use multiple different weighting strategies which are all optimised during training.,3.3 Weighted Cosine,[0],[0]
"This is done by first creating a vector m, which is an element-wise multiplication of the two word representations:
mi = z1,iz2,i (7)
where mi is the i-th element of vector m and z1,i is the i-th element of vector z1.",3.3 Weighted Cosine,[0],[0]
"After that, the resulting vector is used as input for a hidden neural layer:
d = γ(Wdm) (8)
whereWd is a weight matrix and γ is an activation function.",3.3 Weighted Cosine,[0],[0]
"If the length of d is 1, all the weights in Wd have value 1, and γ is a linear activation, then this formula is equivalent to a regular cosine similarity.",3.3 Weighted Cosine,[0],[0]
"However, we use a larger length for d to capture more features, use tanh as the activation function, and optimise the weights of Wd during training, giving the framework more flexibility to customise the model for the task of metaphor detection.",3.3 Weighted Cosine,[0],[0]
"Based on vector d we can output a prediction for the word pair, showing whether it is literal or metaphorical:
y = σ(Wyd) (9)
where Wy is a weight matrix, σ is the logistic activation function, and y is a real-valued prediction with values between 0 and 1.
",3.4 Prediction and Optimisation,[0],[0]
"We optimise the model based on an annotated training dataset, while minimising the following hinge loss function:
E = ∑
k
qk (10)
qk = { (ỹ − y)2 if |ỹ − y| > 0.4 0, otherwise
(11)
where y is the predicted value, ỹ is the true label, and k iterates over all training examples.",3.4 Prediction and Optimisation,[0],[0]
Equation 11 optimises the model to minimise the squared error between the predicted and true labels.,3.4 Prediction and Optimisation,[0],[0]
"However, this is only done for training examples where the predicted error is not already close enough to the desired result.",3.4 Prediction and Optimisation,[0],[0]
The condition |ỹ,3.4 Prediction and Optimisation,[0],[0]
− y| > 0.4 only updates training examples where the difference from the true label is greater than 0.4.,3.4 Prediction and Optimisation,[0],[0]
The true labels ỹ can only take values 0,3.4 Prediction and Optimisation,[0],[0]
"(literal) or 1 (metaphorical), and the threshold 0.4 is chosen so that datapoints that are on the correct side of the decision boundary by more than 0.1 would be ignored, which helps reduce overfitting and allows the model to focus on the misclassified examples.
",3.4 Prediction and Optimisation,[0],[0]
The diagram of the complete network can be seen in Figure 1.,3.4 Prediction and Optimisation,[0],[0]
"Following Bulat et al. (2017) we experiment with two types of semantic vectors: skip-gram word embeddings and attribute-based representations.
",4 Word Representations,[0],[0]
"The word embeddings are 100-dimensional and were trained using the standard log-linear skipgram model with negative sampling of Mikolov et al. (2013b) on Wikipedia for 3 epochs, using a symmetric window of 5 and 10 negative samples per word-context pair.
",4 Word Representations,[0],[0]
"We use the 2526-dimensional attribute-based vectors trained by Bulat et al. (2017), following Fagarasan et al. (2015).",4 Word Representations,[0],[0]
These representations were induced by using partial least squares regression to learn a cross-modal mapping function between the word embeddings described above and the McRae et al. (2005) property-norm semantic space.,4 Word Representations,[0],[0]
"We evaluate our method using two datasets of phrases manually annotated for metaphoricity.
",5 Datasets,[0],[0]
"Since these datasets include examples for different senses (both metaphorical and literal) of the same verbs or adjectives, they allow us to test the extent to which our model is able to discriminate between different word senses, as opposed to merely selecting the most frequent class for a given word.
",5 Datasets,[0],[0]
"Mohammad et al. dataset (MOH) Mohammad et al. (2016) used WordNet to find verbs that had between three and ten senses and extracted the sentences exemplifying them in the corresponding glosses, yielding a total of 1639 verb uses in sentences.",5 Datasets,[0],[0]
Each of these was annotated for metaphoricity by 10 annotators via the crowdsourcing platform CrowdFlower1.,5 Datasets,[0],[0]
Mohammad et al. selected the verbs that were tagged by at least 70% of the annotators as metaphorical or literal to create their dataset.,5 Datasets,[0],[0]
"We extracted verb–direct object and verb–subject relations of the annotated verbs from this dataset, discarding the instances with pronominal or clausal subject or object.",5 Datasets,[0],[0]
This resulted in a dataset of 647 verb–noun pairs (316 metaphorical and 331 literal).,5 Datasets,[0],[0]
"Some examples of annotated verb phrases from MOH are presented in Table 1.
",5 Datasets,[0],[0]
Tsvetkov et al. dataset (TSV) Tsvetkov et al. (2014) construct a dataset of adjective–noun pairs annotated for metaphoricity.,5 Datasets,[0],[0]
This is divided into a training set consisting of 884 literal and 884 metaphorical pairs (TSV-TRAIN) and a test set containing 100 literal and 100 metaphorical pairs (TSV-TEST).,5 Datasets,[0],[0]
Table 2 shows a portion of annotated adjective-noun phrases from TSV-TEST.,5 Datasets,[0],[0]
"TSV-TRAIN was collected from publicly available metaphor collections on the web and manually
1www.crowdflower.com
curated by removing duplicates and metaphorical phrases that depend on wider context for their interpretation (e.g. drowning students).",5 Datasets,[0],[0]
TSVTEST was constructed by extracting nouns that co-occur with a list of 1000 frequent adjectives in the TenTen Web Corpus2 using SketchEngine.,5 Datasets,[0],[0]
The selected adjective-noun pairs were annotated for metaphoricity by 5 annotators with an interannotator agreement of κ = 0.76.,5 Datasets,[0],[0]
"Since TSVTRAIN and TSV-TEST were constructed differently, we follow previous work (Tsvetkov et al., 2014; Shutova et al., 2016; Bulat et al., 2017) and report performance on TSV-TEST.",5 Datasets,[0],[0]
We randomly separated 200 (out of the 1536) examples from the training set to use for development experiments.,5 Datasets,[0],[0]
"The word representations in our model were initialised with either the 100-dimensional skip-gram embeddings or the 2,526-dimensional attribute vectors (Section 4).",6 Experiments and Results,[0],[0]
"These were kept fixed and not updated, which reduces overfitting on the available training examples.",6 Experiments and Results,[0],[0]
"For both word representations we use the same embeddings as Bulat et al. (2017), which makes the results directly comparable and shows that the improvements are coming from the novel architecture and are not due to a different embedding initialisation.
",6 Experiments and Results,[0],[0]
"The network was optimised using AdaDelta (Zeiler, 2012) for controlling adaptive learning rates.",6 Experiments and Results,[0],[0]
The models were evaluated after each full pass over the training data and training was stopped if the F-score on the development set had not improved for 5 epochs.,6 Experiments and Results,[0],[0]
"The transformed embeddings z1 and z2 were set to size 300, layer d was set to size 50.",6 Experiments and Results,[0],[0]
The values for these hyperparameters were chosen experimentally using the development dataset.,6 Experiments and Results,[0],[0]
"In order to avoid drawing conclusions based on outlier results due to random initialisations, we ran each experiment 25 times with random seeds and present the averaged results in this paper.",6 Experiments and Results,[0],[0]
"We implemented the framework using Theano (Al-Rfou et al., 2016) and are making the source code publicly available.3
Table 3 contains results of different system configurations on the TSV dataset.",6 Experiments and Results,[0],[0]
"The original Fscore by Tsvetkov et al. (2014) is still the highest, as they used a range of highly-engineered features that require manual annotation, such as
2https://www.sketchengine.co.uk/ententen-corpus/ 3http://www.marekrei.com/projects/ssn
the lexical abstractness, imageability scores and the relative number of supersenses for each word in the dataset.",6 Experiments and Results,[0],[0]
"Our setup is more similar to the linguistic experiments by Shutova et al. (2016), where metaphor detection is performed using pretrained word embeddings.",6 Experiments and Results,[0],[0]
They also proposed combining the linguistic model with a system using visual word representations and achieved performance improvements.,6 Experiments and Results,[0],[0]
"Recently, Bulat et al. (2017) compared different types of embeddings and showed that attribute-based representations can outperform regular skip-gram embeddings.
",6 Experiments and Results,[0],[0]
"As an additional baseline, we report the performance on metaphor detection using a basic feedforward network (FFN).",6 Experiments and Results,[0],[0]
"In this configuration, the word embeddings x1 and x2 are directly connected to the hidden layer d, skipping all the intermediate network structure.",6 Experiments and Results,[0],[0]
"The FFN achieves 74.4% F-score on TSV-TEST, showing that even such a simple model can perform relatively well in a supervised setting.",6 Experiments and Results,[0],[0]
"Using attribute vectors instead of skip-gram embeddings gives a slight improvement, especially on the recall metric, which is consistent with the findings by Bulat et al. (2017).
",6 Experiments and Results,[0],[0]
"The architecture described in Section 3, which we refer to as a supervised similarity network (SSN), outperforms the baseline and achieves 80.1% F-score using skip-gram embeddings and 80.6% with attribute-based representations.",6 Experiments and Results,[0],[0]
We also created a fusion of these two models where the predictions from both are combined as a weighted average.,6 Experiments and Results,[0],[0]
"In this setting, the two networks are trained in tandem and a real-valued weight, which is also optimised during training, is
used to combine them together.",6 Experiments and Results,[0],[0]
"This configuration achieves 81.1% F-score, indicating that the the skip-gram embeddings and attribute vectors capture somewhat complementary information.",6 Experiments and Results,[0],[0]
"Excluding the system by Tsvetkov et al. (2014) which requires hand-annotated features, the proposed similarity network outperforms all the previous systems, even improving over the multimodal system by Shutova et al. (2016) without requiring any visual information.",6 Experiments and Results,[0],[0]
"The attribute-based SSN also improves over Bulat et al. (2017) by 5.6% absolute, using the same word representations as input.
",6 Experiments and Results,[0],[0]
Table 4 contains results of different system architectures on the MOH dataset.,6 Experiments and Results,[0],[0]
"Shutova et al. (2016) reported 75% F-score on this dataset with a multimodal system, after randomly separating a subset for testing.",6 Experiments and Results,[0],[0]
"Since this corpus contains only 647 annotated examples, we instead evaluated the systems using 10-fold cross-validation.",6 Experiments and Results,[0],[0]
"The feedforward baseline with skip-gram embeddings returns an F-score that is close to the linguistic configuration of Shutova et al, whereas the best results are achieved by the similarity network with skip-gram embeddings.",6 Experiments and Results,[0],[0]
"In this setting, the attribute-based representations did not improve performance – this is expected, as the attribute norms by McRae et al. (2005) are designed for nouns, whereas the MOH dataset is centered on verbs.
",6 Experiments and Results,[0],[0]
"Table 5 contains examples from the TSV development set, together with gold annotations and predicted scores.",6 Experiments and Results,[0],[0]
"The system confidently detects literal phrases such as sunny country and meaningless discussion, along with metaphorical phrases such as unforgiving heights and blind hope.",6 Experiments and Results,[0],[0]
"The predicted output disagrees with the annotation on
cases such as humane treatment and rich programmer – some of these examples could also be argued as being metaphorical, depending on the specific sense of the words.",6 Experiments and Results,[0],[0]
"While the system was relatively unsure about the false positives (the scores were close to 0.5), it tended to assign more decisive scores to the false negatives.",6 Experiments and Results,[0],[0]
"Results in Section 6 show that performance on the TSV dataset is higher than the MOH dataset, likely due to the former having more examples available for training.",7 The Effects of Training Data,[0],[0]
"Therefore, we ran an additional experiment to investigate the effect of dataset size on the performance of metaphor detection.",7 The Effects of Training Data,[0],[0]
"Gutiérrez et al. (2016) annotated a dataset of adjective-noun phrases as being literal or metaphorical, and we are able to use this as an additional training resource.",7 The Effects of Training Data,[0],[0]
"While it contains only 23 unique adjectives, the total number of phrases reaches 8,592.",7 The Effects of Training Data,[0],[0]
"We remove any phrases that occur in the development or test data of TSV, then incrementally add the remaining examples to the TSV training data and evaluate on the TSV-TEST.
",7 The Effects of Training Data,[0],[0]
"Figure 2 shows a graph of the system performance, when increasing the training data at intervals of 500.",7 The Effects of Training Data,[0],[0]
"There is a very rapid increase in performance until around 2,000 training points, whereas the existing TSV-TRAIN is limited to 1,336 examples.",7 The Effects of Training Data,[0],[0]
Providing even more data to the system gives an additional increase that is more gradual.,7 The Effects of Training Data,[0],[0]
"The final performance of the system us-
ing both datasets is 88.3 F-score, which is the highest result reported on the TSV dataset and translates to 36% relative error reduction with respect to the same system trained only on the original dataset.
",7 The Effects of Training Data,[0],[0]
We report the exact values in Table 6 for the different training sets.,7 The Effects of Training Data,[0],[0]
"The value on the Tsvetkov training data is different from the result in Table 3, which is due to the original attribute embeddings by Bulat et al. (2017) only containing representations for the vocabulary in the TSV dataset.",7 The Effects of Training Data,[0],[0]
"In order to include the data from Gutiérrez et al. (2016), we recreated the attribute vectors for a larger vocabulary, which results in a slightly different baseline performance.",7 The Effects of Training Data,[0],[0]
"The architecture in Section 3 also acts as a semantic composition model, extracting the meaning of the phrase by combining the meanings of its component words.",8 Qualitative analysis,[0],[0]
"Therefore, we performed a qualitative experiment to investigate: (1) how well do traditional compositional methods capture metaphors, without any fine-tuning; and (2) whether the supervised representations still retain their domain-specific semantic information.",8 Qualitative analysis,[0],[0]
"For this purpose, we construct three vector spaces and visualise some examples from the TSV training set,
using t-SNE (Van Der Maaten and Hinton, 2008).",8 Qualitative analysis,[0],[0]
"Figure 3 contains examples for three different composition methods: the additive method simply sums the skip-gram embeddings for both words (top); the multiplicative method multiplies the skip-gram embeddings (middle); the final system uses layer m from the SSN model to represent the
phrases (bottom).",8 Qualitative analysis,[0],[0]
"The visualisation shows that the additive and multiplicative models are both comparable when it comes to semantic clustering of the phrases, but metaphorical examples are mixed together with literal clusters.",8 Qualitative analysis,[0],[0]
The SSN is optimised for metaphor classification and therefore it produces representations with a very clear boundary for metaphoricity.,8 Qualitative analysis,[0],[0]
"Interestingly, the graph also reveals a misannotated example in the dataset, since ‘fiery temper’ should be labeled as a metaphor.",8 Qualitative analysis,[0],[0]
"At the same time, this space also retains the general semantic information, as similar phrases with the same label are still positioned close together.",8 Qualitative analysis,[0],[0]
"Future work could investigate models of multi-task training where metaphor detection is trained together with an unsupervised objective, allowing the system to take better advantage of unlabeled data while still learning to separate metaphors.",8 Qualitative analysis,[0],[0]
"In this paper, we introduced the first deep learning architecture designed to capture metaphorical composition and evaluated it on a metaphor identification task.
",9 Conclusion,[0],[0]
"Firstly, we demonstrated that the proposed framework outperforms both a metaphor-agnostic baseline (a feed-forward neural network) as well as previous corpus-driven approaches to metaphor identification.",9 Conclusion,[0],[0]
"The results showed that it is beneficial to construct a specialised network architecture for metaphor detection, which includes a gating function for capturing the interaction between the source and target domains, word embeddings mapped to a metaphor-specific space, and optimisation using a hinge loss function.
",9 Conclusion,[0],[0]
"Secondly, our qualitative analysis indicates that our supervised similarity network learns phrase representations with a very clear boundary for metaphoricity, in contrast to traditional compositional methods.
",9 Conclusion,[0],[0]
"Finally, we show that with a sufficiently large training set our model can also outperform the state-of-the art metaphor identification systems based on hand-coded lexical knowledge.",9 Conclusion,[0],[0]
Ekaterina Shutova’s research is supported by the Leverhulme Trust Early Career Fellowship.,Acknowledgments,[0],[0]
The ubiquity of metaphor in our everyday communication makes it an important problem for natural language understanding.,abstractText,[0],[0]
"Yet, the majority of metaphor processing systems to date rely on handengineered features and there is still no consensus in the field as to which features are optimal for this task.",abstractText,[0],[0]
"In this paper, we present the first deep learning architecture designed to capture metaphorical composition.",abstractText,[0],[0]
Our results demonstrate that it outperforms the existing approaches in the metaphor identification task.,abstractText,[0],[0]
Grasping the Finer Point: A Supervised Similarity Network for Metaphor Detection,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4778–4784 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4778",text,[0],[0]
"Neural machine translation (NMT) (Kalchbrenner and Blunsom, 2013; Cho et al., 2014; Sutskever et al., 2014; Bahdanau et al., 2014) has now achieved impressive performance (Wu et al., 2016; Gehring et al., 2017; Vaswani et al., 2017; Hassan et al., 2018; Chen et al., 2018; Lample et al., 2018) and draws more attention.",1 Introduction,[0],[0]
"NMT models are built on the encoder-decoder framework where the encoder network encodes the source sentence to distributed representations and the decoder network reconstructs the target sentence form the representations word by word.
",1 Introduction,[0],[0]
"Currently, NMT models are usually trained with the word-level loss (i.e., cross-entropy) under the teacher forcing algorithm (Williams and Zipser,
*Corresponding Author
1989), which forces the model to generate translation strictly matching the ground-truth at the word level.",1 Introduction,[0],[0]
"However, in practice it is impossible to generate translation totally the same as ground truth.",1 Introduction,[0],[0]
"Once different target words are generated, the word-level loss cannot evaluate the translation properly, usually under-estimating the translation.",1 Introduction,[0],[0]
"In addition, the teacher forcing algorithm suffers from the exposure bias (Ranzato et al., 2015) as it uses different inputs at training and inference, that is ground-truth words for the training and previously predicted words for the inference.",1 Introduction,[0],[0]
"Kim and Rush (2016) proposed a method of sequence-level knowledge distillation, which use teacher outputs to direct the training of student model, but the student model still have no access to its own predicted words.",1 Introduction,[0],[0]
"Scheduled sampling(SS) (Bengio et al., 2015; Venkatraman et al., 2015) attempts to alleviate the exposure bias problem through mixing ground-truth words and previously predicted words as inputs during training.",1 Introduction,[0],[0]
"However, the sequence generated by SS may not be aligned with the target sequence, which is inconsistent with the word-level loss.
",1 Introduction,[0],[0]
"In contrast, sequence-level objectives, such as BLEU (Papineni et al., 2002), GLEU (Wu et al., 2016), TER (Snover et al., 2006), and NIST (Doddington, 2002), evaluate translation at the sentence or n-gram level and allow for greater flexibility, and thus can mitigate the above problems of the word-level loss.",1 Introduction,[0],[0]
"However, due to the nondifferentiable of sequence-level objectives, previous works on sequence-level training (Ranzato et al., 2015; Shen et al., 2016; Bahdanau et al., 2016; Wu et al., 2016; He et al., 2016; Wu et al., 2017; Yang et al., 2017) mainly rely on reinforcement learning algorithms (Williams, 1992; Sutton et al., 2000) to find an unbiased gradient estimator for the gradient update.",1 Introduction,[0],[0]
"Sparse rewards in this situation often cause the high variance of gradient estimation, which consequently leads to unstable
training and limited improvements.",1 Introduction,[0],[0]
"Lamb et al. (2016); Gu et al. (2017); Ma et al. (2018) respectively use the discriminator, critic and bag-of-words target as sequence-level training objectives, all of which are directly connected to the generation model and hence enable direct gradient update.",1 Introduction,[0],[0]
"However, these methods do not allow for direct optimization with respect to evaluation metrics.
",1 Introduction,[0],[0]
"In this paper, we propose a method to combine the strengths of the word-level and sequencelevel training, that is the direct gradient update without gradient estimation from word-level training and the greater flexibility from sequence-level training.",1 Introduction,[0],[0]
"Our method introduces probabilistic ngram matching which makes sequence-level objectives (e.g., BLEU, GLEU) differentiable.",1 Introduction,[0],[0]
"During training, it abandons teacher forcing and performs greedy search instead to take into consideration the predicted words.",1 Introduction,[0],[0]
Experiment results show that our method significantly outperforms word-level training with the cross-entropy loss and sequence-level training under the reinforcement framework.,1 Introduction,[0],[0]
The experiments also indicate that greedy search strategy indeed has superiority over teacher forcing.,1 Introduction,[0],[0]
"NMT is based on an end-to-end framework which directly models the translation probability from the source sentence x to the target sentence ŷ:
",2 Background,[0],[0]
"P (ŷ|x) = T∏ j=1 p(ŷj |ŷ<j ,x, θ), (1)
where T is the target length and θ is the model parameters.",2 Background,[0],[0]
"Given the training set D = {XM,YM} withM sentences pairs, the training objective is to maximize the log-likelihood of the training data as
θ = argmax θ {L(θ)}
L(θ) = M∑ m=1 lm∑ j=1 log(p(ŷmj |ŷm<j ,xm, θ)), (2)
where the superior m indicates the m-th sentence in the dataset and lm is the length of m-th target sentence.
",2 Background,[0],[0]
"In the above model, the probability of each target word p(ŷmj |ŷm<j ,xm, θ) is conditioned on the previous target words.",2 Background,[0],[0]
"The scenario is that in the
training time, the teacher forcing algorithm is employed and the ground truth words from the target sentence are fed as context, while during inference, the ground truth words are not available and the previous predicted words are instead fed as context.",2 Background,[0],[0]
This discrepancy is called exposure bias.,2 Background,[0],[0]
"Many automatic evaluation metrics of machine translation, such as BLEU, GLEU and NIST, are based on the n-gram matching.",3.1 Sequence-Level Objectives,[0],[0]
Assuming that y and ŷ are the output sentence and the ground truth sentence with length T and T ′,3.1 Sequence-Level Objectives,[0],[0]
"respectively, the count of an n-gram g = (g1, . . .",3.1 Sequence-Level Objectives,[0],[0]
", gn) in sentence y is calculated as
Cy(g) =",3.1 Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.1 Sequence-Level Objectives,[0],[0]
"1{gi = yt+i}, (3)
where 1{·} is the indicator function.",3.1 Sequence-Level Objectives,[0],[0]
"The matching count of the n-gram g between ŷ and y is given by
Cŷy(g) = min (Cy(g),Cŷ(g)).",3.1 Sequence-Level Objectives,[0],[0]
"(4)
Then the precision pn and the recall rn of the predicted n-grams are calculated as follows
pn =
∑ g∈y C
ŷ",3.1 Sequence-Level Objectives,[0],[0]
"y(g)∑
g∈y Cy(g) , (5)
",3.1 Sequence-Level Objectives,[0],[0]
"rn =
∑ g∈y C
ŷ y(g)∑
g∈ŷ Cŷ(g) .",3.1 Sequence-Level Objectives,[0],[0]
"(6)
BLEU, the most widely used metric for machine translation evaluation, is defined based on the n-gram precision as follows
BLEU = BP · exp( N∑ n=1",3.1 Sequence-Level Objectives,[0],[0]
"wn log pn), (7)
where BP stands for the brevity penalty",3.1 Sequence-Level Objectives,[0],[0]
and wn is the weight for the n-gram.,3.1 Sequence-Level Objectives,[0],[0]
"In contrast, GLEU is the minimum of recall and precision of 1-4 grams where 1-4 grams are counted together:
GLEU =",3.1 Sequence-Level Objectives,[0],[0]
"min(p1-4, r1-4).",3.1 Sequence-Level Objectives,[0],[0]
(8),3.1 Sequence-Level Objectives,[0],[0]
"In the output sentence y, the prediction probability varies among words.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Some words are translated by the model with high confidence while some words are translated with high uncertainty.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"However, when calculating the count of n-grams in Eq.(3), all the words in the output sentence are treated equally, regardless of their respective prediction probabilities.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"To give a more precise description of n-gram counts which considers the variety of prediction probabilities, we use the prediction probability p(yj |y<j ,x, θ) as the count of word yj , and correspondingly the count of an n-gram is the product of these probabilistic counts of all the words in the n-gram, not one anymore.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Then the probabilistic count of g = (g1, . . .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
", gn) is calculated by summing over the output sentence y as
C̃y(g) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
T−n∑ t=0 n∏ i=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"1{gi = yt+i} · p(yt+i|y<t+i,x, θ).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(9)
Now the probabilistic sequence-level objective can be got by replacing Cy(g) with C̃y(g) (the tilde over the head indicates the probabilistic version) and keeping the rest unchanged.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Here, we take BLEU as an example and show how the probabilistic BLEU (denoted as P-BLEU) is defined.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"From this purpose, the matching count of n-gram g in Eq.(4) is modified as follows
C̃ ŷ
y(g) = min(C̃y(g),Cŷ(g)).",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(10)
and the predict precision of n-grams changes into
p̃n =
∑ g∈y C̃ ŷ
y(g)∑ g∈y C̃y(g) .",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"(11)
Finally, the probabilistic BLEU (P-BLEU) is defined as
P-BLEU = BP · exp( N∑ n=1 wn log p̃n), (12)
Probabilistic GLEU (P-GLEU) can be defined in a similar way.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"Specifically, we denote the probabilistic precision of n-grams as P-Pn.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"The probabilistic precision is more reasonable than recall since the denominator in Eq.(11) plays a normalization role, so we modify the definition in Eq.(8) and define P-GLEU as simply the probabilistic precision of 1-4 grams.
",3.2 probabilistic Sequence-Level Objectives,[0],[0]
"The general probabilistic loss function is:
L(θ) =",3.2 probabilistic Sequence-Level Objectives,[0],[0]
− M∑ m=1,3.2 probabilistic Sequence-Level Objectives,[0],[0]
"P(ym, ŷm), (13)
where P represents the probabilistic sequencelevel objectives, and ym and ŷm are the predicted translation and the ground truth for the m-th sentence respectively.",3.2 probabilistic Sequence-Level Objectives,[0],[0]
The calculation of the probabilistic objective is illustrated in Figure 1.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
This probabilistic loss can work with decoding strategies such as greedy search and teacher forcing.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
In this paper we employ greedy search rather than teacher forcing so as to use the previously predicted words as context and alleviate the exposure bias problem.,3.2 probabilistic Sequence-Level Objectives,[0],[0]
We carry out experiments on Chinese-to-English translation.1 The training data consists of 1.25M pairs of sentences extracted from LDC corpora2.,4.1 Settings,[0],[0]
Sentence pairs with either side longer than 50 were dropped.,4.1 Settings,[0],[0]
We use NIST 2002 (MT 02) as the validation set and NIST 2003-2006 (MT 03-08) as the test sets.,4.1 Settings,[0],[0]
"We use the case insensitive 4-gram NIST BLEU score (Papineni et al., 2002) for the translation task.
",4.1 Settings,[0],[0]
"We apply our method to an attention-based NMT system (Bahdanau et al., 2014) implemented by Pytorch.",4.1 Settings,[0],[0]
"Both source and target vocabularies are limited to 30K. All word embedding sizes are set to 512, and the sizes of hidden units in both encoder and decoder RNNs are also set to 512.",4.1 Settings,[0],[0]
"All parameters are initialized by uniform distribution over [−0.1, 0.1].",4.1 Settings,[0],[0]
The mini-batch stochastic gradient descent (SGD) algorithm is employed to train the model with batch size of 40.,4.1 Settings,[0],[0]
"In addition, the learning rate is adjusted by adadelta optimizer (Zeiler, 2012) with ρ = 0.95 and = 1e-6.",4.1 Settings,[0],[0]
Dropout is applied on the output layer with dropout rate of 0.5.,4.1 Settings,[0],[0]
The beam size is set to 10.,4.1 Settings,[0],[0]
"Systems We first pretrain the baseline model by maximum likelihood estimation (MLE) and then refine the model using probabilistic sequencelevel objectives, including P-BLEU, P-GLEU and P-P2 (probabilistic 2-gram precision).",4.2 Performance,[0],[0]
"In addition, we reproduce previous works which train the NMT model through minimum risk training (MRT) (Shen et al., 2016) and REINFORCE algo-
1Experiment code: https://github.com/ictnlp/GS4NMT 2The corpora include LDC2002E18, LDC2003E07, LDC2003E14, Hansards portion of LDC2004T07, LDC2004T08 and LDC2005T06.
",4.2 Performance,[0],[0]
"rithm (RF) (Ranzato et al., 2015).",4.2 Performance,[0],[0]
"When reproducing their works, we set BLEU, GLEU and 2-gram precision as training objectives respectively and find out that GLEU yields the best performance.",4.2 Performance,[0],[0]
"In the following, we only report the results with training objective GLEU.",4.2 Performance,[0],[0]
Performance Table 1 shows the translation performance on test sets measured in BLEU score.,4.2 Performance,[0],[0]
"Simply training NMT model by the probabilistic 2-gram precision achieves an improvement of 1.5 BLEU points, which significantly outperforms the reinforcement-based algorithms.",4.2 Performance,[0],[0]
"We also test the precision of other n-grams and their combinations, but do not notice significant improvements over P-P2.",4.2 Performance,[0],[0]
"Notice that our method only changes the loss function, without any modification on model structure and training data.",4.2 Performance,[0],[0]
We use the probabilistic loss to finetune the baseline model rather than training from scratch.,4.3 Why Pretraining,[0],[0]
This is in line with our motivation: to alleviate the exposure bias and make the model exposed to its own output during training.,4.3 Why Pretraining,[0],[0]
"In the very beginning of the training, the model’s translation capability is nearly zero and the generated sentences are often meaningless and do not contain useful information for the training, so it is unreasonable to directly apply the greedy search strategy.",4.3 Why Pretraining,[0],[0]
"Therefore, we first apply the teacher forcing algorithm to pretrain the model, and then we let the model generate the sentences itself and learn from its own outputs.
",4.3 Why Pretraining,[0],[0]
Another reason favoring pretraining is that pretraining can lower the training cost.,4.3 Why Pretraining,[0],[0]
The training cost of the introduced probabilistic loss is about three times higher than the cost of cross entropy.,4.3 Why Pretraining,[0],[0]
"Without pretraining, the training time will be much higher than usual.",4.3 Why Pretraining,[0],[0]
"Otherwise, the training cost is acceptable if the probabilistic loss is only for finetuning.",4.3 Why Pretraining,[0],[0]
"The probabilistic loss, defined in Eq.(13), is computed from the model output y and reference ŷ.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we apply two different decoding strategies to generate y: 1.",4.4 Effect of Decoding Strategy,[0],[0]
"teacher forcing, which uses the ground truth as decoder input.",4.4 Effect of Decoding Strategy,[0],[0]
"2. greedy search, which feeds the word with maximum probability.",4.4 Effect of Decoding Strategy,[0],[0]
"By conducting this experiment, we attempt to figure out where the improvements come from: the modification of loss or the mitigation of exposure bias?
",4.4 Effect of Decoding Strategy,[0],[0]
Figure 2 shows the learning curves of the two decoding strategies with training objective P-P2.,4.4 Effect of Decoding Strategy,[0],[0]
Teacher forcing raises about 0.5 BLEU improvements and greedy search outperform the teacher forcing algorithm by nearly 1 BLEU point.,4.4 Effect of Decoding Strategy,[0],[0]
"We conclude that the probabilistic loss has its own advantage even when trained by the teacher forcing algorithm, and greedy search is effective in alleviating the exposure bias.
",4.4 Effect of Decoding Strategy,[0],[0]
Notice that the greedy search strategy highly relys on the probabilistic loss and can not be conducted independently.,4.4 Effect of Decoding Strategy,[0],[0]
Greedy search together with the word-level loss is very similar with the scheduled sampling(SS).,4.4 Effect of Decoding Strategy,[0],[0]
"However, SS is inconsistent with the word-level loss since the word-level loss requires strict alignment between hypothesis and reference, which can only be accomplished by the teacher forcing algorithm.",4.4 Effect of Decoding Strategy,[0],[0]
"In this section, we explore how the probabilistic objective correlates with the real evaluation metric.",4.5 Correlation with Evaluation Metrics,[0],[0]
"We randomly sample 100 pairs of sentences
from the training set and compute their P-GLEU and GLEU scores (Wu et al. (2016) indicates that GLEU have better performance in the sentencelevel evaluation than BLEU).
",4.5 Correlation with Evaluation Metrics,[0],[0]
"Directly computing the correlation between GLEU and P-GLEU gives the correlation coefficient 0.86, which indicates strong correlation.",4.5 Correlation with Evaluation Metrics,[0],[0]
"In addition, we draw the scatter diagram of the 100 pairs of sentences in Figure 3 with GLEU as x-axis and P-GLEU as y-axix.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Figure 3 shows that PGLEU correlates well with GLEU, suggesting that it is reasonable to directly train the NMT model with P-GLEU.",4.5 Correlation with Evaluation Metrics,[0],[0]
"Word-level loss cannot evaluate the translation properly and suffers from the exposure bias, and sequence-level objectives are usually indifferentiable and require gradient estimation.",5 Conclusion,[0],[0]
"We propose probabilistic sequence-level objectives based on ngram matching, which relieve the dependence on gradient estimation and can directly train the NMT model.",5 Conclusion,[0],[0]
Experiment results show that our method significantly outperforms previous sequence-level training works and successfully alleviates the exposure bias through performing greedy search.,5 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful comments.,6 Acknowledgments,[0],[0]
This work was supported by the National Natural Science Foundation of China (NSFC) under the project NO.61472428 and the project NO. 61662077.,6 Acknowledgments,[0],[0]
"Neural machine translation (NMT) models are usually trained with the word-level loss using the teacher forcing algorithm, which not only evaluates the translation improperly but also suffers from exposure bias.",abstractText,[0],[0]
"Sequence-level training under the reinforcement framework can mitigate the problems of the word-level loss, but its performance is unstable due to the high variance of the gradient estimation.",abstractText,[0],[0]
"On these grounds, we present a method with a differentiable sequence-level training objective based on probabilistic n-gram matching which can avoid the reinforcement framework.",abstractText,[0],[0]
"In addition, this method performs greedy search in the training which uses the predicted words as context just as at inference to alleviate the problem of exposure bias.",abstractText,[0],[0]
Experiment results on the NIST Chinese-to-English translation tasks show that our method significantly outperforms the reinforcement-based algorithms and achieves an improvement of 1.5 BLEU points on average over a strong baseline system.,abstractText,[0],[0]
Greedy Search with Probabilistic N-gram Matching for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1881–1890 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1881",text,[0],[0]
"Many key linguistic tasks, within and across languages or domains, including machine translation, rely on learning cross-lingual correspondences between words or other semantic units.",1 Introduction,[0],[0]
"While the associated alignment problem could be solved with access to large amounts of parallel data, broader applicability relies on the ability to do so with largely mono-lingual data, from Part-of-Speech (POS) tagging (Zhang et al., 2016), dependency parsing (Guo et al., 2015), to machine translation (Lample et al., 2018).",1 Introduction,[0],[0]
"The key subtask of bilingual lexical induction, for example, while long standing as a problem (Fung, 1995; Rapp, 1995, 1999), has been actively pursued recently (Artetxe et al., 2016; Zhang et al., 2017a; Conneau et al., 2018).
",1 Introduction,[0],[0]
"Current methods for learning cross-domain correspondences at the word level rely on distributed representations of words, building on the observation that mono-lingual word embeddings exhibit
similar geometric properties across languages Mikolov et al. (2013).",1 Introduction,[0],[0]
"While most early work assumed some, albeit minimal, amount of parallel data (Mikolov et al., 2013; Dinu et al., 2014; Zhang et al., 2016), recently fully-unsupervised methods have been shown to perform on par with their supervised counterparts (Conneau et al., 2018; Artetxe et al., 2018).",1 Introduction,[0],[0]
"While successful, the mappings arise from multiple steps of processing, requiring either careful initial guesses or postmapping refinements, including mitigating the effect of frequent words on neighborhoods.",1 Introduction,[0],[0]
"The associated adversarial training schemes can also be challenging to tune properly (Artetxe et al., 2018).
",1 Introduction,[0],[0]
"In this paper, we propose a direct optimization approach to solving correspondences based on recent generalizations of optimal transport (OT).",1 Introduction,[0],[0]
"OT is a general mathematical toolbox used to evaluate correspondence-based distances and establish mappings between probability distributions, including discrete distributions such as point-sets.",1 Introduction,[0],[0]
"However, the nature of mono-lingual word embeddings renders the classic formulation of OT inapplicable to our setting.",1 Introduction,[0],[0]
"Indeed, word embeddings are estimated primarily in a relational manner to the extent that the algorithms are naturally interpreted as metric recovery methods (Hashimoto et al., 2016).",1 Introduction,[0],[0]
"In such settings, previous work has sought to bypass this lack of registration by jointly optimizing over a matching and an orthogonal mapping (Rangarajan et al., 1997; Zhang et al., 2017b).",1 Introduction,[0],[0]
"Due to the focus on distances rather than points, we instead adopt a relational OT formulation based on the Gromov-Wasserstein distance that measures how distances between pairs of words are mapped across languages.",1 Introduction,[0],[0]
"We show that the resulting mapping admits an efficient solution and requires little or no tuning.
",1 Introduction,[0],[0]
"In summary, we make the following contributions:
• We propose the use of the GromovWasserstein distance to learn correspondences between word embedding spaces in a fully-unsupervised manner, leading to a theoretically-motivated optimization problem that can be solved efficiently, robustly, in a single step, and requires no post-processing or heuristic adjustments.
",1 Introduction,[0],[0]
"• To scale up to large vocabularies we realize an extended mapping to words not part of the original optimization problem.
",1 Introduction,[0],[0]
"• We show that the proposed approach performs on par with state-of-the-art neural network based methods on benchmark word translation tasks, while requiring a fraction of the computational cost and/or hyperparameter tuning.",1 Introduction,[0],[0]
"In the unsupervised bilingual lexical induction problem we consider two languages with vocabularies Vx and Vy, represented by word embeddings X = {x(i)}ni=1 and Y = {y(j)}mj=1, respectively, where x(i) ∈ X ⊂",2 Problem Formulation,[0],[0]
Rdx corresponds to wxi ∈,2 Problem Formulation,[0],[0]
Vx and y(j) ∈,2 Problem Formulation,[0],[0]
Y ⊂,2 Problem Formulation,[0],[0]
Rdy to wyj ∈ Vy.,2 Problem Formulation,[0],[0]
"For simplicity, we let m = n and dx = dy, although our methods carry over to the general case with little or no modifications.",2 Problem Formulation,[0],[0]
"Our goal is to learn an alignment between these two sets of words without any parallel data, i.e., we learn to relate x(i) ↔ y(j) with the implication that wxi translates to w y j .
",2 Problem Formulation,[0],[0]
"As background, we begin by discussing the problem of learning an explicit map between embeddings in the supervised scenario.",2 Problem Formulation,[0],[0]
The associated training procedure will later be used for extending unsupervised alignments (Section 3.2).,2 Problem Formulation,[0],[0]
"In the supervised setting, we learn a map T : X → Y such that T (x(i))",2.1 Supervised Maps: Procrustes,[0],[0]
≈ y(j) whenever wyj is a translation of wxi .,2.1 Supervised Maps: Procrustes,[0],[0]
"Let X and Y be the matrices whose columns are vectors x(i) and y(j), respectively.",2.1 Supervised Maps: Procrustes,[0],[0]
"Then we can find T by solving
min T∈F ‖X− T (Y)‖2F (1)
",2.1 Supervised Maps: Procrustes,[0],[0]
"where ‖ · ‖F is the Frobenius norm ‖A‖F =√∑ i,j |aij |2.",2.1 Supervised Maps: Procrustes,[0],[0]
"Naturally, both the difficulty of finding T and the quality of the resulting alignment depend on the choice of space F .",2.1 Supervised Maps: Procrustes,[0],[0]
"A classic
approach constrains T to be orthonormal matrices, i.e., rotations and reflections, resulting in the orthogonal Procrustes problem
min P∈O(n)
‖X−PY‖2F (2)
where O(n) = {P ∈ Rn×n | P>P = I}.",2.1 Supervised Maps: Procrustes,[0],[0]
"One key advantage of this formulation is that it has a closed-form solution in terms of a singular value decomposition (SVD), whereas for most other choices of constraint set F it does not.",2.1 Supervised Maps: Procrustes,[0],[0]
"Given an SVD decomposition UΣV> of XY>, the solution to problem (2) is P∗",2.1 Supervised Maps: Procrustes,[0],[0]
"= UV> (Schönemann, 1966).",2.1 Supervised Maps: Procrustes,[0],[0]
"Besides obvious computational advantage, constraining the mapping between spaces to be orthonormal is justified in the context of word embedding alignment because orthogonal maps preserve angles (and thus distances), which is often the only information used by downstream tasks (e.g., for nearest neighbor search) that rely on word embeddings.",2.1 Supervised Maps: Procrustes,[0],[0]
"(Smith et al., 2017) further show that orthogonality is required for self-consistency of linear transformations between vector spaces.
",2.1 Supervised Maps: Procrustes,[0],[0]
"Clearly, the Procrustes approach only solves the supervised version of the problem as it requires a known correspondence between the columns of X and Y. Steps beyond this constraint include using small amounts of parallel data (Zhang et al., 2016) or an unsupervised technique as the initial step to generate pseudo-parallel data (Conneau et al., 2018) before solving for P.",2.1 Supervised Maps: Procrustes,[0],[0]
"Optimal transport formalizes the problem of finding a minimum cost mapping between two point sets, viewed as discrete distributions.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Specifically, we assume two empirical distributions over embeddings, e.g.,
µ = n∑
i=1 piδx(i) , ν = m∑ j=1 qjδy(i) (3)
where p and q are vectors of probability weights associated with each point set.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In our case, we usually consider uniform weights, e.g., pi = 1/n and qj = 1/m, although if additional information were provided (such as in the form of word frequencies), those could be naturally incorporated via p and q (see discussion at the end of Section 3).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"We find a transportation map T realizing
inf T {∫ X c(x, T (x))dµ(x)",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"| T#µ = ν } , (4)
where the cost c(x, T (x)) is typically just ‖x − T (x)‖ and T#µ = ν implies that the source points must exactly map to the targets.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"However, such a map need not exist in general and we instead follow a relaxed Kantorovich’s formulation.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In this case, the set of transportation plans is a polytope:
Π(p,q) =",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"{Γ ∈ Rn×m+ | Γ1n = p, Γ>1n = q}.
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The cost function is given as a matrix C ∈ Rn×m, e.g., Cij = ‖x(i)",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
− y(j)‖.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The total cost incurred by Γ is 〈Γ, C〉 := ∑ ij ΓijCij .",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Thus, the discrete optimal transport (DOT) problem consists of finding a plan Γ that solves
min Γ∈Π(p,q)
〈Γ,C〉.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(5)
Problem (5) is a linear program, and thus can be solved exactly in O(n3 log n) with interior point methods.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"However, regularizing the objective leads to more efficient optimization and often better empirical results.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The most common such regularization, popularized by Cuturi (2013), involves adding an entropy penalization:
min Γ∈Π(p,q)
〈Γ,C〉 − λH(Γ).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(6)
The solution of this strictly convex optimization problem has the form Γ∗ = diag (a)Kdiag (b), with K = e− C λ (element-wise), and can be obtained efficiently via the Sinkhorn-Knopp algorithm, a matrix-scaling procedure which iteratively computes:
a← p Kb and b← q",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"K>a, (7)
where denotes entry-wise division.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The derivation of these updates is immediate from the form of Γ∗ above, combined with the marginal constraints Γ1n = p, Γ>1n",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
= q,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"(Peyré and Cuturi, 2018).
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"Although simple, efficient and theoreticallymotivated, a direct application of discrete OT for unsupervised word translation is not appropriate.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"One reason is that the mono-lingual embeddings are estimated in a relative manner, leaving, e.g., an overall rotation unspecified.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
Such degrees of freedom can dramatically change the entries of the cost matrix Cij = ‖x(i) − y(j)‖ and the resulting transport map.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"One possible solution is to simultaneously learn an optimal coupling and an orthogonal transformation (Zhang et al., 2017b).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The transport problem is then solved iteratively, using
Cij = ‖x(i) − Py(j)‖, where P is in turn chosen to minimize the transport cost (via Procrustes).",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"While promising, the resulting iterative approach is sensitive to initialization, perhaps explaining why Zhang et al. (2017b) used an adversarially learned mapping as the initial step.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"The computational cost can also be prohibitive (Artetxe et al., 2018) though could be remedied with additional development.
",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"We adopt a theoretically well-founded generalization of optimal transport for pairs of points (their distances), thus in line with how the embeddings are estimated in the first place.",2.2 Unsupervised Maps: Optimal Transport,[0],[0]
We explain the approach in detail in the next Section.,2.2 Unsupervised Maps: Optimal Transport,[0],[0]
"In this section we introduce the GromovWasserstein distance, describe an optimization algorithm for it, and discuss how to extend the approach to out-of-sample vectors.",3 Transporting across unaligned spaces,[0],[0]
The classic optimal transport requires a distance between vectors across the two domains.,3.1 The Gromov Wasserstein Distance,[0],[0]
"Such a metric may not be available, for example, when the sample sets to be matched do not belong to the same metric space (e.g., different dimension).",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein distance (Mémoli, 2011) generalizes optimal transport by comparing the metric spaces directly instead of samples across the spaces.",3.1 The Gromov Wasserstein Distance,[0],[0]
"In other words, this framework operates on distances between pairs of points calculated within each domain and measures how these distances compare to those in the other domain.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Thus, it requires a weaker but easy to define notion of distance between distances, and operates on pairs of points, turning the problem from a linear to a quadratic one.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Formally, in its discrete version, this framework considers two measure spaces expressed in terms of within-domain similarity matrices (C,p) and (C′,q) and a loss function defined between similarity pairs: L : R × R → R, where L(Cik, C ′jl) measures the discrepancy between the distances d(x(i),x(k)) and d′(y(j),y(l)).",3.1 The Gromov Wasserstein Distance,[0],[0]
"Typical choices for L are L(a, b) = 12(a",3.1 The Gromov Wasserstein Distance,[0],[0]
"− b)
2 or L(a, b) = KL(a|b).",3.1 The Gromov Wasserstein Distance,[0],[0]
"In this framework, L(Cik, C ′jl) can also be understood as the cost of “matching” i to j and k to l.
All the relevant values of L(·, ·) can be put in a 4-th order tensor L ∈ RN1×N1×N2×N2 , where Lijkl = L(Cik, C ′ jl).",3.1 The Gromov Wasserstein Distance,[0],[0]
"As before, we seek a cou-
pling Γ specifying how much mass to transfer between each pair of points from the two spaces.",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein problem is then defined as solving
GW(C,C′,p,q) = min Γ∈Π(p,q) ∑",3.1 The Gromov Wasserstein Distance,[0],[0]
"i,j,k,l LijklΓijΓkl (8)
Compared to problem (5), this version is substantially harder since the objective is now not only non-linear, but non-convex too.1 In addition, it requires operating on a fourth-order tensor, which would be prohibitive in most settings.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Surprisingly, this problem can be optimized efficiently with first-order methods, whereby each iteration involves solving a traditional optimal transport problem (Peyré et al., 2016).",3.1 The Gromov Wasserstein Distance,[0],[0]
"Furthermore, for suitable choices of loss function L, Peyré et al. (2016) show that instead of the O(N21N 2 2 ) complexity implied by naive fourthorder tensor product, this computation reduces to O(N21N2 + N1N 2 2 ) cost.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Their approach consists of solving (5) by projected gradient descent, which yields iterations that involve projecting onto Π(p,q) a pseudo-cost matrix of the form
ĈΓ(C,C ′,Γ) =",3.1 The Gromov Wasserstein Distance,[0],[0]
Cxy,3.1 The Gromov Wasserstein Distance,[0],[0]
− h1(C)Γh2(C′)>,3.1 The Gromov Wasserstein Distance,[0],[0]
"(9)
where
Cxy = f1(C)p1 > m + 1nq >f2(C ′)>
and f1, f2, h2, h2 are functions that depend on the loss L. We provide an explicit algorithm for the case L = L2 at the end of this section.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"1In fact, the discrete (Monge-type) formulation of the problem is essentially an instance of the well-known (and NP-hard) quadratic assignment problem (QAP).
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Once we have solved (8), the optimal transport coupling Γ∗ provides an explicit (soft) matching between source and target samples, which for the problem of interest can be interpreted as a probabilistic translation: for every pair of words (w (i) src, w (j) trg), Γ ∗ ij provides a likelihood that these two words are translations of each other.",3.1 The Gromov Wasserstein Distance,[0],[0]
"This itself is enough to translate, and we show in the experiments section that Γ∗ by itself, without any further post-processing, provides highquality translations.",3.1 The Gromov Wasserstein Distance,[0],[0]
"This stands in sharp contrast to mapping-based methods, which rely on nearest-neighbor computation to infer translations, and thus become prone to hub-word effects which have to be mitigated with heuristic postprocessing techniques such as Inverted Softmax (Smith et al., 2017) and Cross-Domain Similarity Scaling (CSLS) (Conneau et al., 2018).",3.1 The Gromov Wasserstein Distance,[0],[0]
"The transportation coupling Γ, being normalized by construction, requires no such artifacts.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"The Gromov-Wasserstein problem (8) possesses various desirable theoretical properties, including the fact that for a suitable choice of the loss function it is indeed a distance:
Theorem 3.1 (Mémoli 2011).",3.1 The Gromov Wasserstein Distance,[0],[0]
"With the choice L = L2, GW 1 2 is a distance on the space of metric measure spaces.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Solving problem (8) therefore yields a fascinating accompanying notion: the GromovWasserstein distance between languages, a measure of semantic discrepancy purely based on the relational characterization of their word embeddings.",3.1 The Gromov Wasserstein Distance,[0],[0]
"Owing to Theorem 3.1, such values can be
interpreted as distances, so that, e.g., the triangle inequality holds among them.",3.1 The Gromov Wasserstein Distance,[0],[0]
"In Section 4.4 we compare various languages in terms of their GWdistance.
",3.1 The Gromov Wasserstein Distance,[0],[0]
"Finally, we note that whenever word frequency counts are available, those would be used for p and q.",3.1 The Gromov Wasserstein Distance,[0],[0]
"If they are not, but words are sorted according to occurrence (as they often are in popular off-the-shelf embedding formats), one can estimate rank-probabilities such as Zipf power laws, which are known to accurately model multiple languages (Piantadosi, 2014).",3.1 The Gromov Wasserstein Distance,[0],[0]
"In order to provide a fair comparison to previous work, throughout our experiments we use uniform distributions so as to avoid providing our method with additional information not available to others.",3.1 The Gromov Wasserstein Distance,[0],[0]
"While the pure Gromov-Wasserstein approach leads to high quality solutions, it is best suited to small-to-moderate vocabulary sizes,2 since its optimization becomes prohibitive for very large problems.",3.2 Scaling Up,[0],[0]
"For such settings, we propose a twostep approach in which we first match a subset of the vocabulary via the optimal coupling, after which we learn an orthogonal mapping through a modified Procrustes problem.",3.2 Scaling Up,[0],[0]
"Formally, suppose we solve problem (8) for a reduced matrices X1:k and Yi:k consisting of the first columns k of X and Y, respectively, and let Γ∗ be the optimal coupling.",3.2 Scaling Up,[0],[0]
"We seek an orthogonal matrix that best recovers the barycentric mapping implied by Γ∗. Namely, we seek to find P which solves:
min P∈O(n)
‖XΓ∗ −PY‖22 (10)
Just as problem (2), it is easy to show that this Procrustes-type problem has a closed form solution in terms of a singular value decomposition.",3.2 Scaling Up,[0],[0]
"Namely, the solution to (10) is P∗ = UV>, where UΣV∗ = X1:mΓ
∗Y>1:m. After obtaining this projection, we can immediately map the rest of the embeddings via ŷ(j)",3.2 Scaling Up,[0],[0]
"= P∗y(j).
",3.2 Scaling Up,[0],[0]
We point out that this two-step procedure resembles that of Conneau et al. (2018).,3.2 Scaling Up,[0],[0]
"Both ultimately produce an orthogonal mapping obtained by solving a Procrustes problems, but they differ in the way they produce pseudo-matches to allow for such second-step: while their approach relies
2As shown in the experimental section, we are able to run problems of size in the order of |Vs| ≈ 105",3.2 Scaling Up,[0],[0]
"≈ |Vt| on a single machine without relying on GPU computation.
",3.2 Scaling Up,[0],[0]
"Algorithm 1 Gromov-Wasserstein Computation for Word Embedding Alignment
Input: Source and target embeddings X, Y. Regularization λ.",3.2 Scaling Up,[0],[0]
"Probability vectors p,q. //",3.2 Scaling Up,[0],[0]
Compute intra-language similarities,3.2 Scaling Up,[0],[0]
"Cs ← cos(X,X), Ct ← cos(Y,Y) Cst ← C2sp1>m + 1nq(C2t )> while not converged",3.2 Scaling Up,[0],[0]
"do
//",3.2 Scaling Up,[0],[0]
Compute pseudo-cost matrix (Eq. (9)),3.2 Scaling Up,[0],[0]
ĈΓ ← Cst − 2CsΓC>t //,3.2 Scaling Up,[0],[0]
Sinkhorn iterations (Eq. (7)),3.2 Scaling Up,[0],[0]
"a← 1, K← exp{−ĈΓ/λ} while not converged do a← p Kb, b← q",3.2 Scaling Up,[0],[0]
"K>a end while Γ← diag (a)Kdiag (b)
end while //",3.2 Scaling Up,[0],[0]
"Optional step: Learn explicit projection U,Σ,V> ← SVD(XΓY>)",3.2 Scaling Up,[0],[0]
"P = UV> return Γ,P
on an adversarially-learned transformation, we use an explicit optimization problem.
",3.2 Scaling Up,[0],[0]
We end this section by discussing parameter and configuration choices.,3.2 Scaling Up,[0],[0]
"To leverage the fast algorithm of Peyré et al. (2016), we always use the L2 distance as the loss function L between cost matrices.",3.2 Scaling Up,[0],[0]
"On the other hand, we observed throughout our experiments that the choice of cosine distance as the metric in both spaces consistently leads to better results, which agrees with common wisdom on computing distances between word embeddings.",3.2 Scaling Up,[0],[0]
This leaves us with a single hyperparameter to control: the entropy regularization term λ.,3.2 Scaling Up,[0],[0]
"By applying any sensible normalization on the cost matrices (e.g., dividing by the mean or median value), we are able to almost entirely eliminate sensitivity to that parameter.",3.2 Scaling Up,[0],[0]
"In practice, we use a simple scheme in all experiments: we first try the same fixed value (λ = 5× 10−5), and if the regularization proves too small (by leading to floating point errors), we instead use λ = 1× 10−4.",3.2 Scaling Up,[0],[0]
We never had to go beyond these two values in all our experiments.,3.2 Scaling Up,[0],[0]
We emphasize that at no point we use train (let alone test) supervision available with many datasets—model selection is done solely in terms of the unsupervised objective.,3.2 Scaling Up,[0],[0]
Pseudocode for the full method (with L = L2 and cosine similarity) is shown here as Algorithm 1.,3.2 Scaling Up,[0],[0]
"Through this experimental evaluation we seek to: (i) understand the optimization dynamics of the proposed approach (§4.2), evaluate its performance on benchmark cross-lingual word embedding tasks (§4.3), and (iii) qualitatively investigate the notion of distance-between-languages it computes (§4.4).",4 Experiments,[0],[0]
"Rather than focusing solely on prediction accuracy, we seek to demonstrate that the proposed approach offers a fast, principled, and robust alternative to state-of-the-art multi-step methods, delivering comparable performance.",4 Experiments,[0],[0]
Datasets We evaluate our method on two standard benchmark tasks for cross-lingual embeddings.,4.1 Evaluation Tasks and Methods,[0],[0]
"First, we consider the dataset of Conneau et al. (2018), which consists of word embeddings trained with FASTTEXT (Bojanowski et al., 2017) on Wikipedia and parallel dictionaries for 110 language pairs.",4.1 Evaluation Tasks and Methods,[0],[0]
"Here, we focus on the language pairs for which they report results: English (EN) from/to Spanish (ES), French (FR), German (DE), Russian (RU) and simplified Chinese (ZH).",4.1 Evaluation Tasks and Methods,[0],[0]
"We do not report results on Esperanto (EO) as dictionaries for that language were not provided with the original dataset release.
",4.1 Evaluation Tasks and Methods,[0],[0]
"For our second set of experiments, we consider the—substantially harder3—dataset of (Dinu et al., 2014), which has been extensively compared against in previous work.",4.1 Evaluation Tasks and Methods,[0],[0]
"It consists of embeddings and dictionaries in four pairs of languages; EN from/to ES, IT, DE, and FI (Finnish).
",4.1 Evaluation Tasks and Methods,[0],[0]
"3We discuss the difference in hardness of these two benchmark datasets in Section 4.3.
",4.1 Evaluation Tasks and Methods,[0],[0]
"Methods To see how our fully-unsupervised method compares with methods that require (some) cross-lingual supervision, we follow (Conneau et al., 2018) and consider a simple but strong baseline consisting of solving a procrustes problem directly using the available cross-lingual embedding pairs.",4.1 Evaluation Tasks and Methods,[0],[0]
We refer to this method simply as PROCRUSTES.,4.1 Evaluation Tasks and Methods,[0],[0]
"In addition, we compare against the fully-unsupervised methods of Zhang et al. (2017a), Artetxe et al. (2018) and Conneau et al. (2018).4 As proposed by the latter, we use CSLS whenever nearest neighbor search is required, which has been shown to improve upon naive nearest-neighbor retrieval in multiple work.",4.1 Evaluation Tasks and Methods,[0],[0]
"As previously mentioned, our approach involves only two optimization choices, one of which is required only for very large settings.",4.2 Training Dynamics of G-W,[0],[0]
"When running Algorithm 1 for the full set of embeddings is infeasible (due to memory limitations), one must decide what fraction of the embeddings to use during optimization.",4.2 Training Dynamics of G-W,[0],[0]
"In our experiments, we use the largest possible size allowed by memory constraints, which was found to be K = 20, 000 for the personal computer we used.
",4.2 Training Dynamics of G-W,[0],[0]
The other—more interesting—optimization choice involves the entropy regularization parameter λ used within the Sinkhorn iterations.,4.2 Training Dynamics of G-W,[0],[0]
"Large regularization values lead to denser optimal coupling Γ∗, while less regularization leads to sparser solutions,5 at the cost of a harder (more
4Despite its relevance, we do not include the OT-based method of Zhang et al. (2017b) in the comparison because their implementation required use of proprietary software.
",4.2 Training Dynamics of G-W,[0],[0]
"5In the limit λ→ 0, when n = m, the solution converges
non-convex) optimization problem.",4.2 Training Dynamics of G-W,[0],[0]
In Figure 2 we show the training dynamics of our method when learning correspondences between word embeddings from the dataset of Conneau et al. (2018).,4.2 Training Dynamics of G-W,[0],[0]
"As expected, larger values of λ lead to smoother improvements with faster runtime-per-iteration, at a price of some drop in performance.",4.2 Training Dynamics of G-W,[0],[0]
"In addition, we found that computing GW distances between closer languages (such as EN and FR) leads to faster convergence than for more distant ones (such as EN and RU, in Fig. 2c).
",4.2 Training Dynamics of G-W,[0],[0]
"Worth emphasizing are three desirable optimization properties that set apart the GromovWasserstein distance from other unsupervised alignment approaches, particularly adversarialtraining ones: (i) the objective decreases monotonically (ii) its value closely follows the true metric of interest (translation, which naturally is not available during training) and (iii) there is no risk of degradation due to overtraining, as is the case for adversarial-based methods trained with stochastic gradient descent (Conneau et al., 2018).",4.2 Training Dynamics of G-W,[0],[0]
We report the results on the dataset of Conneau et al. (2018) in Table 1.,4.3 Benchmark Results,[0],[0]
The strikingly high performance of all methods on this task belies the hardness of the general problem of unsupervised cross-lingual alignment.,4.3 Benchmark Results,[0],[0]
"Indeed, as pointed out by Artetxe et al. (2018), the FASTTEXT embeddings provided in this task are trained on very large and highly comparable—across languages— corpora (Wikipedia), and focuses on closely related pairs of languages.",4.3 Benchmark Results,[0],[0]
"Nevertheless, we carry out experiments here to have a broad evaluation of our approach in both easier and harder settings.
",4.3 Benchmark Results,[0],[0]
"Next, we present results on the more challeng-
to a permutation matrix, which gives a hard-matching solution to the transportation problem (Peyré and Cuturi, 2018).
",4.3 Benchmark Results,[0],[0]
"ing dataset of (Dinu et al., 2014) in Table 2.",4.3 Benchmark Results,[0],[0]
"Here, we rely on the results reported by (Artetxe et al., 2018) since by the time of writing the present work their implementation was not available yet.
",4.3 Benchmark Results,[0],[0]
"Part of what makes this dataset hard is the wide discrepancy between word distance across languages, which translates into uneven distance matrices (Figure 3), and in turn leads to poor results for G-W. To account for this, previous work has relied on an initial whitening step on the embeddings.",4.3 Benchmark Results,[0],[0]
"In our case, it suffices to normalize the pairwise similarity matrices to the same range to obtain substantially better results.",4.3 Benchmark Results,[0],[0]
"While we have observed that careful choice of the regularization parameter λ can obviate the need for this step, we opt for the normalization approach since it allows us to optimize without having to tune λ.
",4.3 Benchmark Results,[0],[0]
"We compare our method (with and without nor-
malization) against alternative approaches in Table 2.",4.3 Benchmark Results,[0],[0]
"Note that we report the runtimes of Artetxe et al. (2018) as-is, which are obtained by running on a Titan XP GPU, while our runtimes are, as before, obtained purely by CPU computation.",4.3 Benchmark Results,[0],[0]
"As mentioned earlier, Theorem 3.1 implies that the optimal value of the Gromov-Wasserstein problem can be legitimately interpreted as a distance between languages, or more explicitly, between their word embedding spaces.",4.4 Qualitative Results,[0],[0]
This distributional notion of distance is completely determined by pairwise geometric relations between these vectors.,4.4 Qualitative Results,[0],[0]
"In Figure 4 we show the values GW(Cs,Ct,p,q) computed on the FASTTEXT word embeddings of Conneau et al. (2018) corresponding to the most frequent 2000 words in each language.
",4.4 Qualitative Results,[0],[0]
"Overall, these distances conform to our intuitions: the cluster of romance languages exhibits some of the shortest distances, while classical Chinese (ZH) has the overall largest discrepancy with all other languages.",4.4 Qualitative Results,[0],[0]
"But somewhat surprisingly, Russian is relatively close to the romance languages in this metric.",4.4 Qualitative Results,[0],[0]
We conjecture that this could be due to Russian’s rich morphology (a trait shared by romance languages but not English).,4.4 Qualitative Results,[0],[0]
"Furthermore, both Russian and Spanish are prodrop languages (Haspelmath, 2001) and share syntactic phenomena, such as dative subjects (Moore and Perlmutter, 2000; Melis et al., 2013) and differential object marking (Bossong, 1991), which might explain why ES is closest to RU overall.
",4.4 Qualitative Results,[0],[0]
"On the other hand, English appears remarkably isolated from all languages, equally distant from its germanic (DE) and romance (FR) cousins.",4.4 Qualitative Results,[0],[0]
"Indeed, other aspects of the data (such as corpus size) might be underlying these observations.",4.4 Qualitative Results,[0],[0]
Study of the problem of bilingual lexical induction goes back to Rapp (1995) and Fung (1995).,5 Related Work,[0],[0]
"While the literature on this topic is extensive, we focus here on recent fully-unsupervised and minimallysupervised approaches, and refer the reader to one of various existing surveys for a broader panorama (Upadhyay et al., 2016; Ruder et al., 2017).
",5 Related Work,[0],[0]
Methods with coarse or limited parallel data.,5 Related Work,[0],[0]
"Most of these fall in one of two categories: methods that learn a mapping from one space to the other, e.g., as a least-squares objective (e.g., (Mikolov et al., 2013)) or via orthogonal transformations Zhang et al. (2016); Smith et al. (2017); Artetxe et al. (2016), and methods that find a com-
mon space on which to project both sets of embeddings (Faruqui and Dyer, 2014; Lu et al., 2015).
",5 Related Work,[0],[0]
Fully Unsupervised methods.,5 Related Work,[0],[0]
Conneau et al. (2018) and Zhang et al. (2017a) rely on adversarial training to produce an initial alignment between the spaces.,5 Related Work,[0],[0]
The former use pseudo-matches derived from this initial alignment to solve a Procrustes (2) alignment problem.,5 Related Work,[0],[0]
"Our GromovWasserstein framework can be thought of as providing an alternative to these adversarial training steps, albeit with a concise optimization formulation and producing explicit matches (via the optimal coupling) instead of depending on nearest neighbor search, as the adversarially-learnt mappings do.
",5 Related Work,[0],[0]
Zhang et al. (2017b) also leverage optimal transport distances for the cross-lingual embedding task.,5 Related Work,[0],[0]
"However, to address the issue of nonalignment of embedding spaces, their approach follows the joint optimization of the transportation and procrustes problem as outlined in Section 2.2.",5 Related Work,[0],[0]
"This formulation makes an explicit modeling assumption (invariance to unitary transformations), and requires repeated solution of Procrustes problems during alternating minimization.",5 Related Work,[0],[0]
"GromovWasserstein, on the other hand, is more flexible and makes no such assumption, since it directly deals with similarities rather than vectors.",5 Related Work,[0],[0]
"In the case where it is required, such an orthogonal mapping can be obtained by solving a single procrustes problem, as discussed in Section 3.2.",5 Related Work,[0],[0]
In this work we provided a direct optimization approach to cross-lingual word alignment.,6 Discussion and future work,[0],[0]
The Gromov-Wasserstein distance is well-suited for this task as it performs a relational comparison of word-vectors across languages rather than wordvectors directly.,6 Discussion and future work,[0],[0]
"The resulting objective is concise, and can be optimized efficiently.",6 Discussion and future work,[0],[0]
"The experimental results show that the resulting alignment framework is fast, stable and robust, yielding near stateof-the-art performance at a computational cost orders of magnitude lower than that of alternative fully unsupervised methods.
",6 Discussion and future work,[0],[0]
"While directly solving Gromov-Wasserstein problems of reasonable size is feasible, scaling up to large vocabularies made it necessary to learn an explicit mapping via Procrustes.",6 Discussion and future work,[0],[0]
GPU computations or stochastic optimization could help avoid this secondary step.,6 Discussion and future work,[0],[0]
The authors would like to thank the anonymous reviewers for helpful feedback.,Acknowledgments,[0],[0]
"The work was partially supported by MIT-IBM grant “Adversarial learning of multimodal and structured data”, and Graduate Fellowships from Hewlett Packard and CONACYT.",Acknowledgments,[0],[0]
Cross-lingual or cross-domain correspondences play key roles in tasks ranging from machine translation to transfer learning.,abstractText,[0],[0]
"Recently, purely unsupervised methods operating on monolingual embeddings have become effective alignment tools.",abstractText,[0],[0]
"Current state-of-theart methods, however, involve multiple steps, including heuristic post-hoc refinement strategies.",abstractText,[0],[0]
"In this paper, we cast the correspondence problem directly as an optimal transport (OT) problem, building on the idea that word embeddings arise from metric recovery algorithms.",abstractText,[0],[0]
"Indeed, we exploit the GromovWasserstein distance that measures how similarities between pairs of words relate across languages.",abstractText,[0],[0]
"We show that our OT objective can be estimated efficiently, requires little or no tuning, and results in performance comparable with the state-of-the-art in various unsupervised word translation tasks.",abstractText,[0],[0]
Gromov-Wasserstein Alignment of Word Embedding Spaces,title,[0],[0]
"In this paper, we focus on the multi-term nonsmooth convex composite optimization
min x∈X f(x) + n∑ i=1",1. Introduction,[0],[0]
gi(x),1. Introduction,[0],[0]
", (1)
where X is a linear space, gi : X → (−∞,+∞] is a proper, lower semicontinuous convex function for all i = 1, · · · , n, and f : X → (−∞,+∞) is a continuous
1Tencent AI Lab, China 2Sun Yat-sen University, China 3The Chinese University of Hong Kong, China.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
Li Shen,1. Introduction,[0],[0]
"<mathshenli@gmail.com>, Wei Liu <wliu@ee.columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"differentiable convex function with its gradient satisfying the inequality that
1
L ∥∥∇f(x)−∇f(y)∥∥2 ≤ 〈∇f(x)−∇f(y), x− y〉. (2)",1. Introduction,[0],[0]
"The above multi-term nonsmooth convex composite optimization problem (1) covers a large class of applications in machine learning such as simultaneous low-rank and sparsity (Richard et al., 2012; Zhou et al., 2013), overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010), graph-guided fused Lasso (Chen et al., 2012; Kim & Xing, 2009), graph-guided logistic regression (Chen et al., 2011; Zhong & Kwok, 2014), variational image restoration (Combettes & Pesquet, 2011; Dupé et al., 2009; Pustelnik et al., 2011), and other types of structure regularization paradigms (Teo et al., 2010; 2007).",1. Introduction,[0],[0]
"By introducing the multi-term nonsmooth regularization term∑n i=1 gi(x) such as structured sparsity (Huang et al., 2011; Bach et al., 2012; Bach, 2010) and nonnegativity (Chen & Plemmons, 2015; Xu & Yin, 2013), more prior information can be included to enhance the accuracy of regularization models.",1. Introduction,[0],[0]
"However, due to the multi-term nonsmooth regularization term ∑n i=1 gi(x), the optimization problem (1) is too complicated to be solved even for small n. For n ≤ 2, some existing popular first-order optimization methods are accelerated proximal gradient method (Beck & Teboulle, 2009; Nesterov, 2007), smoothing accelerated proximal gradient method (Nesterov, 2005a;b), three operator splitting method (Davis & Yin, 2015), and some primal-dual operator splitting methods such as majorized alternating direction method of multiplier (ADMM) (Cui et al., 2016; Lin et al., 2011), fast proximity method (Li & Zhang, 2016), and so on.
",1. Introduction,[0],[0]
"On the other hand, when n ≥ 3, there also exist some algorithms for solving problem (1).",1. Introduction,[0],[0]
"A directly method for (1) is smoothing accelerated proximal gradient (S-APG) proposed by Nesterov (Nesterov, 2005a;b).",1. Introduction,[0],[0]
"Then, Yu (Yu, 2013) proposed a new approximation method called PAAPG for handling (1) by combining the proximal average approximation technique and Nesterov’s acceleration technique, which has been enhanced very recently by Shen et al. (Shen et al., 2017).",1. Introduction,[0],[0]
Their proposed method called APA-APG adopts an adaptive stepsize strategy.,1. Introduction,[0],[0]
"However,
the above mentioned methods S-APG, PA-APG and its enhanced version APA-APG all need a strict restriction on the nonsmooth functions {gi(x)} that each gi(x) must be Lipschitz continuous.",1. Introduction,[0],[0]
"In addition, some primal-dual parallel splitting methods (Briceno-Arias et al., 2011; Combettes & Pesquet, 2007; 2008; Condat, 2013; Vũ, 2013) generalized from traditional operator splitting, such as forward backward splitting method (Chen & Rockafellar, 1997) and Douglas Rachford splitting method (Eckstein & Bertsekas, 1992), can also solve the multi-term nonsmooth convex composite optimization problem (1).",1. Introduction,[0],[0]
"Different from prior work, Raguet et al. (Raguet et al., 2013) proposed an efficient primal operator splitting method called generalized forward backward splitting method using the classic forward backward splitting technique, which has shown the superiority over numerous existing primal-dual splitting methods (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) in dealing with variational image restoration problems.",1. Introduction,[0],[0]
All the above mentioned methods for problem (1) with n ≥ 3 share a common feature that they all split the nonsmooth composite term∑n i=1,1. Introduction,[0],[0]
gi(x),1. Introduction,[0],[0]
"in the Jacobi iteration manner, i.e., parallelly.",1. Introduction,[0],[0]
"This is one of the main differences between existing splitting methods and our proposed method in this paper.
",1. Introduction,[0],[0]
"To split the nonsmooth composite term ∑n i=1 gi(x) more efficiently, we propose a novel operator splitting algorithm to solve problem (1) by harnessing the advantage of GaussSeidel iterations, i.e., the computation of the proximal mapping of the current function gi(x) uses the proximal mappings of gj(x) for all j < i which have already been computed ahead.",1. Introduction,[0],[0]
"In addition, to further improve the algorithm’s efficiency, we leverage the over-relaxation acceleration technique.",1. Introduction,[0],[0]
"What’s more, we provide a new strategy that the over-relaxation stepsize can be determined adaptively, ensuring a larger value to accelerate the algorithm.",1. Introduction,[0],[0]
The most important is that the convergence of our proposed GSOS algorithm is established by a newly developed analysis technique.,1. Introduction,[0],[0]
"In detail, given an invertible linear operator R, we first argue that the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of problem (1) can be recovered
by the zero point set [ (R∗)−1SR, ∂g+A◦∇f◦A,NV ]−1 (0).",1. Introduction,[0],[0]
"This is fulfilled through adopting the tool of operator optimization theory, in which the composite operator SR, ∂g+A◦∇f◦A,NV is generalized from the definition of the composite monotone operator Sλ,A,B in (Eckstein & Bertsekas, 1992).",1. Introduction,[0],[0]
"Next, by unitizing the definition of the -enlargement of maximal monotone (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), we establish a key property for SR, ∂g+A◦∇f◦A,NV , that is, gph ( SR, (∂g+A∗◦∇f◦A)[",1. Introduction,[0],[0]
"],NV )",1. Introduction,[0],[0]
"⊆
gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]",1. Introduction,[0],[0]
[ ] ) .,1. Introduction,[0],[0]
"Based on this observation, we equivalently reformulate the GSOS algorithm as a two-step iterations algorithm.",1. Introduction,[0],[0]
"Then, the
global convergence of the proposed GSOS algorithm is easily established based on this reformulation.
",1. Introduction,[0],[0]
"The closest algorithm to our proposed GSOS algorithm is the generalized forward backward splitting method proposed by Raguet et al. (Raguet et al., 2013).",1. Introduction,[0],[0]
"By carefully selecting the scaling matrix H in the forthcoming GSOS algorithm, it is easy to check that GSOS covers the generalized forward backward splitting method as a special case.",1. Introduction,[0],[0]
"Another highly related algorithm to our proposed GSOS algorithm is the matrix splitting method (Luo & Tseng, 1991; Yuan et al., 2016).",1. Introduction,[0],[0]
"Choosing the scaling matrixH suitably, the proposed GSOS algorithm can inherit the advantage of the matrix splitting technique which has shown the efficiency in (Yuan et al., 2016) for coping with a special class of coordinate separable composite optimization problems.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
"In Section 2, we first give the definitions of some useful notations which can make the paper much more readable.",1. Introduction,[0],[0]
"We also establish some lemmas and propositions based on monotone operator theory (Bauschke & Combettes, 2011), which are the key to the convergence of the GSOS algorithm.",1. Introduction,[0],[0]
"In Section 3, we present the proposed GSOS algorithm and then analyze its convergence and iteration complexity.",1. Introduction,[0],[0]
"In Section 4, we conduct numerical experiments on overlapping group Lasso and graph-guided fused Lasso problems to evaluate the efficacy of the GSOS algorithm.",1. Introduction,[0],[0]
"Finally, we draw conclusions in Section 5.",1. Introduction,[0],[0]
Let Y = ∏n i=1,2. Preliminaries and Notations,[0],[0]
"Xi be the product space of Xi with Xi = X for all i ∈ {1, 2, · · · , n}.",2. Preliminaries and Notations,[0],[0]
Let V be a linear space and V⊥ be its complementary space with the following definitions V=,2. Preliminaries and Notations,[0],[0]
{ y ∈ Y | y1 = · · ·,2. Preliminaries and Notations,[0],[0]
"= yn } , V⊥= { y ∈ Y |
n∑ i yi = 0 } .
",2. Preliminaries and Notations,[0],[0]
Let IX : X → X be the identity map and EY : X → Y be a block linear operator defined as EY =( IX · · · IX )∗ .,2. Preliminaries and Notations,[0],[0]
Let A : Y → X be a linear operator defined as Ay = 1nE ∗,2. Preliminaries and Notations,[0],[0]
Yy,2. Preliminaries and Notations,[0],[0]
= 1 n ∑n i=1 yi.,2. Preliminaries and Notations,[0],[0]
"Hence, its adjoint operator A∗ : X → Y is defined as A∗x = 1nEYx.",2. Preliminaries and Notations,[0],[0]
"Let H,R : Y → Y be block lower triangular linear invertible operators satisfying (R∗)−1 = H and H + H∗ 0.",2. Preliminaries and Notations,[0],[0]
"Moreover,H is defined as H1,1 0 · · · 0 ... . . .",2. Preliminaries and Notations,[0],[0]
"...
...",2. Preliminaries and Notations,[0],[0]
"Hn−1,1 · · · Hn−1,n−1 0",2. Preliminaries and Notations,[0],[0]
"Hn,1 · · ·",2. Preliminaries and Notations,[0],[0]
"Hn−1,n Hn,n  , (3) where Hi,j : X → X is a linear operator for all (i, j) ∈ {1, · · · , n}.",2. Preliminaries and Notations,[0],[0]
It is worthwhile to emphasize that,2. Preliminaries and Notations,[0],[0]
"Hi,i is also possible to be a lower triangular linear operator satisfying
Hi,i +H∗i,i 0.",2. Preliminaries and Notations,[0],[0]
"Next, we abuse the notation ‖ · ‖H which is induced by the inner product 〈·,H·〉 satisfying
‖ · ‖H : = √ 〈·,H·〉 = √ 〈·,H∗·〉
= √ 〈·, H+H ∗
2 ·〉 = ‖ · ‖H+H∗ 2 .",2. Preliminaries and Notations,[0],[0]
"(4)
In addition, we define the generalized proximal mapping of a proper, lower semicontinuous convex function gi(x) with respect to the invertible linear operatorHi,i.
Definition 1 For a given x, the proximal mapping denoted by ProxH−1i,i gi(x) of a proper, lower semicontinuous convex function gi with respect to an invertible linear operator",2. Preliminaries and Notations,[0],[0]
"Hi,i satisfying Hi,i + H∗i,i 0 is defined to be the zero point of the following inclusion equation
0 ∈ ∂gi(·)",2. Preliminaries and Notations,[0],[0]
"+Hi,i(· − x).",2. Preliminaries and Notations,[0],[0]
"(5)
Moreover, if Hi,i is symmetric, it can be reformulated as the following convex minimization
ProxHi,igi(x) := arg min y∈X
gi(y) + 1
2 ‖y",2. Preliminaries and Notations,[0],[0]
"− x‖2Hi,i .
",2. Preliminaries and Notations,[0],[0]
"Next, we recall the definition of -enlargement of monotone operators (Burachik et al., 1998; 1997; Burachik & Svaiter, 1999; Svaiter, 2000), which is an effective tool for establishing the convergence of the proposed GSOS algorithm.
",2. Preliminaries and Notations,[0],[0]
Definition 2,2. Preliminaries and Notations,[0],[0]
"Given a maximal monotone operator T : X ⇒ X, the (≥ 0)-enlargement of T is defined as the set T",2. Preliminaries and Notations,[0],[0]
"[ ](x) := { v ∈ Y | 〈w − v, z",2. Preliminaries and Notations,[0],[0]
− x〉 ≥,2. Preliminaries and Notations,[0],[0]
"− for all z ∈
X, w ∈ T",2. Preliminaries and Notations,[0],[0]
"(z) } .
",2. Preliminaries and Notations,[0],[0]
Recall that f(x) is a gradient Lipschitz convex function satisfying inequality (2).,2. Preliminaries and Notations,[0],[0]
"There exits 0 Σ Σ̂ LI such that the following two inequalities hold for any x, x′ ∈ X
f(x) ≤ f(x′) + 〈∇f(x′), x− x′〉+ 1 2",2. Preliminaries and Notations,[0],[0]
‖x− x′‖2,2. Preliminaries and Notations,[0],[0]
"Σ̂ , (6) f(x) ≥ f(x′) + 〈∇f(x′), x− x′〉+ 1 2 ‖x− x′‖2Σ. (7)
Actually, when f(x) is a quadratic function, it holds Σ = Σ̂ directly in inequalities (6) and (7).",2. Preliminaries and Notations,[0],[0]
"The following lemma establishes the property of the enlargement of the composite operatorA∗ ◦∇f ◦A with f satisfying inequalities (6)- (7) or (2), which is an essential ingredient for reformulating the GSOS algorithm as a two-step iterations algorithm.
",2. Preliminaries and Notations,[0],[0]
Proposition 1 Assume that f is a gradient Lipschitz continuous convex function satisfying inequality (2).,2. Preliminaries and Notations,[0],[0]
"For any x1, x2 ∈ Y , it holds that
(A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
A)(x2) ∈ (A∗ ◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A)[ ](x1) (8)
with = L4 ‖Ax1−Ax2‖ 2.",2. Preliminaries and Notations,[0],[0]
"In addition, if f further satisfies inequalities (6)-(7), it holds that
(A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
A)(x2) ∈ (A∗ ◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A)[ ](x1) (9)
with = 14‖Ax1 −Ax2‖ 2 2Σ̂−Σ .
",2. Preliminaries and Notations,[0],[0]
"Remark 1 Two comments are made for Proposition 1:
(1)",2. Preliminaries and Notations,[0],[0]
This proposition gives two types of estimations for in( A∗ ◦∇f ◦A ),2. Preliminaries and Notations,[0],[0]
[,2. Preliminaries and Notations,[0],[0]
] in (8) and (9).,2. Preliminaries and Notations,[0],[0]
"When f is a quadratic
function, it is easy to check that
1 4 ‖Ax1 −Ax2‖22Σ̂−Σ ≤",2. Preliminaries and Notations,[0],[0]
L 4,2. Preliminaries and Notations,[0],[0]
‖Ax1,2. Preliminaries and Notations,[0],[0]
"−Ax2‖2
due to Σ̂ = Σ LI.",2. Preliminaries and Notations,[0],[0]
"When f is a general gradient Lipschitz continuous function, we do not know which estimation for is tighter in (8) and (9).
",2. Preliminaries and Notations,[0],[0]
"(2) The second part of this proposition can be regarded as an intensified version of Lemma 2.2 in (Svaiter, 2014) for a specified composite operator A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A.,2. Preliminaries and Notations,[0],[0]
"The first part of the proposition coincides with the results by applying Lemma 2.2 in (Svaiter, 2014) for A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
"A.
",2. Preliminaries and Notations,[0],[0]
"Next, we generalize the notation Sλ,T1,T2 in (Eckstein & Bertsekas, 1992) for a given λ > 0 and two maximal monotone operators T1, T2 as SR,T1,T2 for a given invertible linear operatorR defined as
gphSR, T1, T2 (10) := { (x1 +Ry2, x2",2. Preliminaries and Notations,[0],[0]
− x1),2. Preliminaries and Notations,[0],[0]
"| y1 ∈ T1(x1),
y2 ∈ T2(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"By (Eckstein & Bertsekas, 1992), we know that Sλ,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone.",2. Preliminaries and Notations,[0],[0]
"However, its generalized operator SR,T1,T2 is not monotone unless the invertible linear operator R reduces to be a constant.",2. Preliminaries and Notations,[0],[0]
"Very interesting, it can be shown that its composition with (R∗)−1, i.e., (R∗)−1SR,T1,T2 , is maximal monotone for any invertible linear operatorR.
Lemma 1 For any given invertible linear operator R, operator (R∗)−1SR,T1,T2 is maximal monotone if T1 and T2 are both maximal monotone operators.
Setting T1 = ∂g + A∗",2. Preliminaries and Notations,[0],[0]
◦ ∇f ◦,2. Preliminaries and Notations,[0],[0]
"A, T2 = NV , we obtain SR, ∂g+A∗◦∇f◦A,NV , which is defined as
gph ( SR,∂g+A∗◦∇f◦A,NV ) (11)
:= { (x1+Ry2, x2−x1)",2. Preliminaries and Notations,[0],[0]
"| y1∈(∂g +A∗ ◦ ∇f ◦ A)(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"By Lemma 1, we know that (R∗)−1SR, ∂g+A∗◦∇f◦A,NV is maximal monotone due to the maximal monotonicity of ∂g + A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A and NV .,2. Preliminaries and Notations,[0],[0]
"Hence, given a constant ≥ 0, the enlargement [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV",2. Preliminaries and Notations,[0],[0]
][ ] is well defined.,2. Preliminaries and Notations,[0],[0]
"In addition, based on the definition of SR,T1,T2 again, we set T1 = ∂g + (A∗ ◦ ∇f ◦",2. Preliminaries and Notations,[0],[0]
"A)[ ], or T1 = (∂g + A∗ ◦",2. Preliminaries and Notations,[0],[0]
∇f ◦,2. Preliminaries and Notations,[0],[0]
A)[ ] and T2 = NV in (10).,2. Preliminaries and Notations,[0],[0]
"Then we have the definition of SR,∂g+(A∗◦∇f◦A)[ ],NV or SR,(∂g+A∗◦∇f◦A)[ ],NV for any given invertible linear operatorR and constant ≥ 0",2. Preliminaries and Notations,[0],[0]
"as follows
gph ( SR,∂g+(A∗◦∇f◦A)",2. Preliminaries and Notations,[0],[0]
"[ ],NV ) (12)
:= { (x1+Ry2, x2−x1)|y1∈(∂g+(A∗◦∇f ◦A)[ ])(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } ,
gph ( SR,(∂g+A∗◦∇f◦A)[ ],NV ) (13)
:= { (x1+Ry2, x2−x1)|y1∈(∂g +A∗◦∇f ◦A)[ ])(x1),
y2 ∈ NV(x2), x1 +R∗y1 = x2 −R∗y2 } .
",2. Preliminaries and Notations,[0],[0]
"In the proposition below, we will establish the relationships among the above mentioned three operators SR,∂g+(A∗◦∇f◦A)",2. Preliminaries and Notations,[0],[0]
"[ ],NV , SR,(∂g+A∗◦∇f◦A)[ ],NV and [(R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]",2. Preliminaries and Notations,[0],[0]
"[ ].
Proposition 2",2. Preliminaries and Notations,[0],[0]
"Given a constant ≥ 0 and an invertible linear operatorR, it holds that
gph ( SR, ∂g+(A∗◦∇f◦A)[ ],NV ) ⊆ gph ( SR, (∂g+A∗◦∇f◦A)[ ],NV
) ⊆ gph ( R∗[(R∗)−1SR, ∂g+A∗◦∇f◦A,NV",2. Preliminaries and Notations,[0],[0]
],2. Preliminaries and Notations,[0],[0]
"[ ] ) .
In the following, we establish the relationship between the optimal solution set [∇f + ∑n i=1 ∂gi] −1 (0) of prob-
lem (1) and [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0), which means that we can recover the solution of problem (1) through [ (R∗)−1SR, ∂g+A∗◦∇f◦A,NV ]−1 (0).
",2. Preliminaries and Notations,[0],[0]
Lemma 2 Let linear operators H and R satisfy (R∗)−1 = H and H satisfy (3).,2. Preliminaries and Notations,[0],[0]
Denote Ω =,2. Preliminaries and Notations,[0],[0]
"[ (R∗)−1SR, (∂g+A∗◦∇f◦A),NV ]−1 (0).",2. Preliminaries and Notations,[0],[0]
"It holds that[ ∇f +
n∑ i=1",2. Preliminaries and Notations,[0],[0]
"∂gi
]−1 (0) = ( ETYH∗EY
)−1ETYH∗(Ω).",2. Preliminaries and Notations,[0],[0]
"In this section, we first propose the Gauss-Seidel operator splitting algorithm for solving the multi-term nonsmooth convex composite problem (1).",3. GSOS Algorithm,[0],[0]
"Then, based on the preliminaries in Section 2, we establish the convergence and iteration complexity of the GSOS algorithm.
",3. GSOS Algorithm,[0],[0]
"Algorithm 1 GSOS Algorithm Parameters: Choose σ ∈ (0, 1), a linear operator H satisfying (3) and a starting point z0 ∈ Z .",3. GSOS Algorithm,[0],[0]
Set θfix1 ∈,3. GSOS Algorithm,[0],[0]
"( − 1, θ1 ] and θfix2 ∈",3. GSOS Algorithm,[0],[0]
"( − 1, θ2 ] , where θ1 and θ2 are
defined via equations (14a) and (14b), respectively.",3. GSOS Algorithm,[0],[0]
"for k = 0, 1, 2, · · · ,K do xk := EY ( ETYHEY )−1ETYHzk; for i = 1, 2 · · · , n do yki := ProxH−1i,i gi ( H−1i,i",3. GSOS Algorithm,[0],[0]
"[ ∑i j=1Hi,j(2xkj − zkj )",3. GSOS Algorithm,[0],[0]
"−
1 n∇f( 1 n ∑n i=1",3. GSOS Algorithm,[0],[0]
x,3. GSOS Algorithm,[0],[0]
k i ),3. GSOS Algorithm,[0],[0]
"− ∑i−1 j=1Hi,jykj ] ) ;
end for set θadap1k as (14c) and θ adap2 k as (14d); set θk ∈",3. GSOS Algorithm,[0],[0]
"[θfix1, θadap1k ]",3. GSOS Algorithm,[0],[0]
"∪ [θfix2, θ adap2 k ]; zk+1 := zk + (1 + θk)(y k − xk);
end for return ωK := ( ETYH∗EY )−1ETYH∗zK .",3. GSOS Algorithm,[0],[0]
"In Algorithm 1, parameters θ1, θ1, θ adap1 k , θ adap1 k are defined as θ1 = max { θ|(θ − σ)(H+H∗) + LA∗A 0 } ; (14a) θ2 = max { θ | (θ − σ)(H+H∗) (14b) +",3. GSOS Algorithm,[0],[0]
A∗(2Σ̂− Σ)A 0 } ; θadap1k = σ,3. GSOS Algorithm,[0],[0]
− L‖A(xk − yk)‖2 ‖xk,3. GSOS Algorithm,[0],[0]
− yk‖2H+H∗ ; (14c) θadap2k,3. GSOS Algorithm,[0],[0]
= σ,3. GSOS Algorithm,[0],[0]
− ‖A(xk,3. GSOS Algorithm,[0],[0]
"− yk)‖2
2Σ̂−Σ ‖xk",3. GSOS Algorithm,[0],[0]
− yk‖2H+H∗ .,3. GSOS Algorithm,[0],[0]
"(14d)
",3. GSOS Algorithm,[0],[0]
Remark 2,3. GSOS Algorithm,[0],[0]
"We make some comments on GSOS below.
",3. GSOS Algorithm,[0],[0]
"(1) For the updating step of xk, we obtain xk = EY (∑K i,j=1Hij )",3. GSOS Algorithm,[0],[0]
"−1∑K j=1 ∑K i=j Hijzkj by using the
notations H and EY .",3. GSOS Algorithm,[0],[0]
"Similarly, we have ωk =(∑K i,j=1Hij )−1∑K j=1 ∑j i=iH∗jizkj .",3. GSOS Algorithm,[0],[0]
"Hence, we need
to compute the inverse of ∑n i,j=1Hi,j .",3. GSOS Algorithm,[0],[0]
"However, if Hi,j is a lower triangular matrix operator, xk and ωk can be obtained easily.
",3. GSOS Algorithm,[0],[0]
"(2) By the definitions of ProxH−1i,i gi and y k, we need to
solve the following inclusion equation
Gki ∈",3. GSOS Algorithm,[0],[0]
"Hi,iyki + ∂gi(yki ),
where Gki = H −1",3. GSOS Algorithm,[0],[0]
"i,i",3. GSOS Algorithm,[0],[0]
"[∑i j=1Hi,j(2xkj − zkj )",3. GSOS Algorithm,[0],[0]
− 1 n∇f( 1 n ∑n i=1 x,3. GSOS Algorithm,[0],[0]
k i ),3. GSOS Algorithm,[0],[0]
"− ∑i−1 j=1Hi,jykj ] .",3. GSOS Algorithm,[0],[0]
"Usually, it is easy to choose a suitable Hi,i such that the solution of the above inclusion equation has a closed form.
",3. GSOS Algorithm,[0],[0]
(3) θk is the over-relaxation stepsize for accelerating the GSOS algorithm.,3. GSOS Algorithm,[0],[0]
"If the computations of θadap1k and θadap2k are time consuming, we can set θk = max{θfix1, θfix2}.
(4) WhenH is a diagonal matrix, i.e.,Hi,j = 0",3. GSOS Algorithm,[0],[0]
"andHi,i = aiI with some nonnegative constant ai, and the over relaxation stepsize θk is fixed to a smaller region, the GSOS algorithm reduces to the generalized forward backward splitting method in (Raguet et al., 2013).
",3. GSOS Algorithm,[0],[0]
"In the following, we reformulate the GSOS algorithm as a two-step iterations algorithm by utilizing monotone optimization theory established in Section 2, which is the key to the convergence of the GSOS algorithm.
",3. GSOS Algorithm,[0],[0]
"Proposition 3 Let g : Y → (−∞,+∞] be the function defined as g(x) = ∑n i=1 gi(xi).",3. GSOS Algorithm,[0],[0]
"Assume that the sequences (xk, yk) and zk are generated by Algorithm 1 with σ ∈ (0, 1).",3. GSOS Algorithm,[0],[0]
Let vk = (R∗)−1(xk − yk) and zk = yk + R(R∗)−1(zk,3. GSOS Algorithm,[0],[0]
− xk).,3. GSOS Algorithm,[0],[0]
"Then, for all k ∈ N, there exists k ≥ 0 such that the iterations in Algorithm 1 can be reformulated as the following two-step iterations algorithm:",3. GSOS Algorithm,[0],[0]
vk ∈,3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR, ∂g+(A∗◦∇f◦A),NV ]",3. GSOS Algorithm,[0],[0]
"[ k](zk), (15a) θk‖R∗vk‖2R−1 + ‖R ∗vk + zk",3. GSOS Algorithm,[0],[0]
"− zk‖2R−1
+2 k ≤",3. GSOS Algorithm,[0],[0]
σ‖zk,3. GSOS Algorithm,[0],[0]
"− zk‖2R−1 , (15b)
",3. GSOS Algorithm,[0],[0]
and zk+1 =,3. GSOS Algorithm,[0],[0]
"zk − (1 + θk)R∗vk.
",3. GSOS Algorithm,[0],[0]
"Remark 3 Based on Proposition 3, the GSOS algorithm can be regarded as an inexact over-relaxed metric proximal point algorithm for the composite inclusion
0 ∈",3. GSOS Algorithm,[0],[0]
"(R∗)−1SR,∂g+A∗◦∇f◦A,NV (z).
",3. GSOS Algorithm,[0],[0]
"By Proposition 3 and Lemma 2, we can establish the convergence of the GSOS algorithm based on the relationship
between the two zero point sets [∇f+ n∑ i=1 ∂gi] −1(0) and Ω.
Theorem 1 Let {(xk, yk, zk)} be the sequence generated by Algorithm 1.",3. GSOS Algorithm,[0],[0]
"We have:
(i) for any z∗ ∈",3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0), it holds that
‖zk+1",3. GSOS Algorithm,[0],[0]
− z∗‖2R−1 ≤,3. GSOS Algorithm,[0],[0]
‖z,3. GSOS Algorithm,[0],[0]
k,3. GSOS Algorithm,[0],[0]
"− z∗‖2R−1 (16)
− (1− σ)(1 + θk)‖xk − yk‖2R−1 ;
(ii) zk converges to a point belonging to zero point set",3. GSOS Algorithm,[0],[0]
"[(R∗)−1SR,∂g+A∗◦∇f◦A,NV ]−1(0)",3. GSOS Algorithm,[0],[0]
"and ωk converges to a point belonging to [∇f + ∑n i=1 ∂gi] −1 (0), i.e., the optimal solution set
of problem (1).
",3. GSOS Algorithm,[0],[0]
Theorem 1 indicates that ‖xk − yk‖ approaching to zero implies the convergence of the GSOS algorithm.,3. GSOS Algorithm,[0],[0]
"In the theorem below, we measure the convergence rates of two sequences ‖xk − yk‖ and ‖ωk − ωk+1‖.
Theorem 2 Let zk be the sequence generated by the GSOS algorithm.",3. GSOS Algorithm,[0],[0]
"Then, there exists i ∈ {1, 2, · · · , k} such that
‖xi − yi‖2 ≤",3. GSOS Algorithm,[0],[0]
"O (1 k ) ,",3. GSOS Algorithm,[0],[0]
∥∥ωi+1,3. GSOS Algorithm,[0],[0]
"− ωi∥∥2 ≤ O(1 k ) .
",3. GSOS Algorithm,[0],[0]
"Due to the space limit, all proofs of the propositions, lemmas and theorems are placed into the supplementary material.",3. GSOS Algorithm,[0],[0]
"In this section, we apply the proposed algorithm to the overlapping group Lasso (Zhao et al., 2009; Jacob et al., 2009; Mairal et al., 2010) and graph-guided fused Lasso problems (Chen et al., 2012; Kim & Xing, 2009), which can be formulated as
min 1
2 ‖Sx− b‖2 + K∑ i=1",4. Experiments,[0],[0]
gi(x).,4. Experiments,[0],[0]
"(17)
For overlapping group Lasso problem (21), gi(x) = ναi‖xGi‖ andK denotes the number of groups.",4. Experiments,[0],[0]
"For graphguided Lasso problem (25), gi(x) = ναij‖xi−xj‖ and K denotes the number of edges in the graph edge set E.
We describe the detailed techniques in the experimental implementation for (17).",4. Experiments,[0],[0]
"Given a > 12 and a positive definite operator D satisfying D STS, we set
Hi,j = {
1 K2D, i ≥ j ∈ {1, 2, · · · ,K}; a K2D, i = j ∈ {1, 2, · · · ,K}.
(18)
",4. Experiments,[0],[0]
"Hence, it easy to check that H + H∗ = A∗DA + 2a−1 K2 Diag ( EYD ) 0.",4. Experiments,[0],[0]
"Due to the smooth term in overlapping group Lasso (21) is quadratic, the two estimations θ2 and θadap2k in (14b) and (14d) are preferred to be used.",4. Experiments,[0],[0]
"By specific H, we obtain ∑K i,j=1Hi,j =
K(K−1)+2αK 2K2 D and∑K
j=1 ∑K i=j Hi,jzkj = D K2 ∑K j=1(a+K−j)zkj ,which fur-
ther imply xk =",4. Experiments,[0],[0]
"(∑K i,j=1Hi,j )−1∑K j=1 ∑K i=j Hi,jzkj =
2 ∑K j=1(a+K−j)z k j
K(K−1)+2aK .",4. Experiments,[0],[0]
"Moreover, by the positive definiteness of Hi,i and D, it holds that ∑n j=1 ∑j i=iH∗j,izkj =
D K2 ∑K j=1(a + j − 1)zkj .",4. Experiments,[0],[0]
"Hence, we attain ωk =
2 ∑K j=1(a+j−1)z k j
K(K−1)+2aK .",4. Experiments,[0],[0]
"In addition, by the definition of H, we reformulate the estimation (14b) for θk as the following form:
θ = max { θ | EY [ (σ − θ)D − STS ]",4. Experiments,[0],[0]
"E∗Y
+ (2a− 1) ( σ − θ ) Diag(EYD) 0 } .
",4. Experiments,[0],[0]
"Due to a ≥ 12 and the positive definiteness of D, a sufficient condition satisfying the constraint in the above set is{
(σ − θ)D − STS 0, θ ≤ σ }
.",4. Experiments,[0],[0]
"Hence, we have an alternative estimation for θ as
θ = max { θ | (σ − θ)D − STS 0, θ ≤ σ } .",4. Experiments,[0],[0]
"(19)
Similarly, the adaptive stepsize estimation (14d) is reformulated as
θadapk = σ",4. Experiments,[0],[0]
"−
1 2K2 ∥∥∥∥ K∑ i=1",4. Experiments,[0],[0]
(xk − yki ),4. Experiments,[0],[0]
"∥∥∥∥2 STS
K∑ j=1 K∑",4. Experiments,[0],[0]
i=j (xk − yki )THij(xk,4. Experiments,[0],[0]
− ykj ) .,4. Experiments,[0],[0]
"(20)
Therefore, the GSOS algorithm can be specified as the following form for solving problem (17).
",4. Experiments,[0],[0]
Algorithm 2 GSOS Algorithm for Solving Problem (,4. Experiments,[0],[0]
"17) Parameters: Choose σ ∈ (0, 1), positive definite operators D and Hi,j satisfying (18), and a starting point z0 ∈ Z .",4. Experiments,[0],[0]
"Set θ as (19) and θfix ∈ ( − 1, θ1 ] .
for k = 0, 1, 2, · · · , do xk := 2 ∑K j=1(α+K−j)z k j
K(K−1)+2αK ; for i = 1, 2 · · · ,K do yki := ProxH−1i,i gi ( H−1i,i",4. Experiments,[0],[0]
"[ ∑i j=1Hi,j(2xk − zkj )",4. Experiments,[0],[0]
"−
1 KS T (Sxk − b)− ∑i−1 j=1Hi,jykj ] ) ;
end for set θk ∈",4. Experiments,[0],[0]
"[θfixk , θ adap k",4. Experiments,[0],[0]
"], where θ adap k is defined via (20); for j = 1, 2 · · · ,K do zk+1j := z k j + (1 + θk)(y k j − xk);
end for end for return ωN = 2 ∑K j=1(α+j−1)z N j
K(K−1)+2αK .
",4. Experiments,[0],[0]
"In this paper, we compare the proposed GSOS algorithm with four state-of-the-art algorithms below.
",4. Experiments,[0],[0]
"• GFB (Raguet et al., 2013): Generalized Forward Backward (GFB) splitting algorithm is a primal firstorder operator splitting algorithm for solving (1) proposed by Raguet et al. (Raguet et al., 2013), which has been shown to outperform other competing algorithms such as (Monteiro & Svaiter, 2013; Combettes & Pesquet, 2012; Chambolle & Pock, 2011) for variational image restoration.
",4. Experiments,[0],[0]
"• PDM (Condat, 2013):",4. Experiments,[0],[0]
"A first-order Primal-Dual splitting Method (PDM) (Condat, 2013) for solving jointly the primal and dual formulations of large-scale convex minimization problems involving Lipschitz, proximal and linear composite terms.
",4. Experiments,[0],[0]
"• PA-APG (Yu, 2013): Proximal Average approximated Accelerated Proximal Gradient (PA-APG) algorithm (Yu, 2013) is a primal first-order method, which utilizes the proximal average technique (Bauschke et al., 2008) to separate the multi-term nonsmooth function in (1).",4. Experiments,[0],[0]
"It has been shown to outperform the smoothing accelerated proximal gradient method (Nesterov, 2005b;a).
",4. Experiments,[0],[0]
"• APA-APG (Shen et al., 2017):",4. Experiments,[0],[0]
"An enhanced version of PA-APG, which incorporates the Adaptive Proximal Average approximation technique with the Accelerated Proximal Gradient (APA-APG) method to improve the efficiency of the optimization procedure.
",4. Experiments,[0],[0]
"It is worthwhile to emphasize that PA-APG and APA-APG algorithms can only be applied to a specific class of problems (1), in which the multi-term nonsmooth regularization is Lipschitz continuous.",4. Experiments,[0],[0]
"Since the nonsmooth regularization terms in overlapping group Lasso and graph-guided fused Lasso are all exactly Lipschitz continuous, the two efficient solvers PG-APG (Yu, 2013) and its enhanced version APA-APG (Shen et al., 2017) are also compared with the GSOS algorithm to illustrate the efficacy of GSOS.",4. Experiments,[0],[0]
"In the implementation, the approximation parameter for PAAPG is set as 1.0e− 5.",4. Experiments,[0],[0]
"In this subsection, we apply the proposed GSOS algorithm to the overlapping group Lasso problem, which takes the following formal definition:
min 1
2 ‖Sx− b‖2 + ν K∑ i=1 αi‖xGi‖, (21)
where S ∈ Rn×d is the sampling matrix, b is the noisy observation vector, G = {G1, · · · ,GK} denotes the set of overlapping groups (Gi ⊂ {1, · · · , d} satisfying ⋃K i=1",4.1. Overlapping Group Lasso,[0],[0]
"Gi =
{1, · · · , d} and Gi ⋂ Gj 6= ∅",4.1. Overlapping Group Lasso,[0],[0]
"for some i, j), xGi ∈ Rd is a duplication of x with x{1,··· ,d}\Gi = 0, αi is the weight for the i-th group, and ν is the regularization parameter controlling group sparsity.
",4.1. Overlapping Group Lasso,[0],[0]
"During the implementation of Algorithm 2, we need to calculate the generalized proximal mapping of ‖xGi‖ in the updating step of yki .",4.1. Overlapping Group Lasso,[0],[0]
"By the positive definiteness of Hi,i, the calculation of yki in Algorithm 2 is equivalent to solving the following problem:
yki := arg min x
1 2 ‖x− bk‖2Hi,i + ναi‖xGi‖,
where bk = H−1i,i",4.1. Overlapping Group Lasso,[0],[0]
"[∑i j=1Hi,j(2xk − zkj )",4.1. Overlapping Group Lasso,[0],[0]
"− 1 KS
T (Sxk − b)− ∑i−1 j=1Hi,jykj ] .",4.1. Overlapping Group Lasso,[0],[0]
"In the proposition below, given c, diag-
onal positive definite operatprHi,i and group G, we solve
x∗",4.1. Overlapping Group Lasso,[0],[0]
":= arg min x
1 2 ‖x− c‖2Hi,i + ν‖xG‖. (22)
",4.1. Overlapping Group Lasso,[0],[0]
"When Hi,i is identity matrix I, (22) has the closed-form solution
x∗ = { x∗G , i ∈ G, ci, else,
where x∗G",4.1. Overlapping Group Lasso,[0],[0]
"= { (1− ν/‖cG‖)cG , ‖cG‖ ≥ t; 0, else.
",4.1. Overlapping Group Lasso,[0],[0]
"Proposition 4 Let (Hi,i)G be the subdiagonal matrix of Hi,i with the index set G, and t∗ be the optimal solution of the one-dimensional optimization problem
min t≥0
{ 1
2
〈 cG , [ (Hi,i)−1G + 2tI",4.1. Overlapping Group Lasso,[0],[0]
]−1 cG 〉 + tν2 } .,4.1. Overlapping Group Lasso,[0],[0]
"(23)
Hence, the optimal solution of (22) has the following form
x∗ =
{ cG",4.1. Overlapping Group Lasso,[0],[0]
"− [ I + 2t∗(Hi,i)G ]−1 cG , i ∈ G;
ci, else.",4.1. Overlapping Group Lasso,[0],[0]
"(24)
Like (Chen et al., 2012; Yu, 2013), the entries of sampling matrix S ∈ Rn×d are sampled from an i.i.d. normal distribution, and x ∈ Rd with xj = (−1)j exp−(j−1)/100 and d = 90K + 10.",4.1. Overlapping Group Lasso,[0],[0]
"Let ξ be the noise sampled from the standard normal distribution, and the noisy observation satisfies b = Sx + ξ.",4.1. Overlapping Group Lasso,[0],[0]
"In addition, we set ν = 1 and αi = 1K2 for each group Gi and the groups {Gi} are overlapped by 10 elements, that is{
G1 = {1, · · · , 100} G2 = {91, · · · , 190} · · · GK = {d− 99, · · · , d}
} .
",4.1. Overlapping Group Lasso,[0],[0]
"The sampling size and the number of groups (n,K) are chosen from the following set
(n,K) ∈ {
(1000, 20), (2000, 40), (4000, 60), (4000, 80), (5000, 80), (5000, 100)
} .
",4.1. Overlapping Group Lasso,[0],[0]
"To further reduce the computations, in Algorithm 2 we set Hi,i = ‖STS‖I and the over-relaxation stepsize θk as θ in (19).",4.1. Overlapping Group Lasso,[0],[0]
"Hence, the compared five solvers GSOS, GFB, PDM, PA-APG and APA-APG have the same computational cost in each iteration.",4.1. Overlapping Group Lasso,[0],[0]
"To be fair, all the compared algorithms start with the same initial point.",4.1. Overlapping Group Lasso,[0],[0]
"The following six pictures in Figures 1 and 2 display the comparisons of the five solvers for a variety of (n,K).",4.1. Overlapping Group Lasso,[0],[0]
It is apparent that our proposed GSOS algorithm shows great superiorities over the other four solvers.,4.1. Overlapping Group Lasso,[0],[0]
The primal-dual solver PDM is slightly faster than the primal solver GFB.,4.1. Overlapping Group Lasso,[0],[0]
"PA-APG is the slowest algorithm, because the prespecified proximal average approximation precision is 1.0e − 5 which leads to a very small stepsize.",4.1. Overlapping Group Lasso,[0],[0]
"Also, APA-APG is much faster than the other four solvers at the first 50 iterations.",4.1. Overlapping Group Lasso,[0],[0]
"However, it is slowed down since the stepsize used in AP-APG becomes smaller and smaller as the iterations go on.",4.1. Overlapping Group Lasso,[0],[0]
"In this subsection, we perform experiments on graphguided fused Lasso which is formulated as
min 1
2 ‖Sx− b‖2 + ν ∑",4.2. Graph-Guided Fused Lasso,[0],[0]
"(i,j)∈E αij |xi",4.2. Graph-Guided Fused Lasso,[0],[0]
"− xj |, (25)
where αij ≥ 0 is the weight for the fused term ‖xi − xj‖ for all (i, j) ∈ E (E is the given graph edge set), and ν is
the regularization parameter.
",4.2. Graph-Guided Fused Lasso,[0],[0]
"In the implementation of Algorithm 2 for tackling graphguided fused Lasso (25), we need to solve the following optimization in the updating step of yk:
x∗ := arg min x
1 2 ‖x− b‖2Hi,i + ν|xi",4.2. Graph-Guided Fused Lasso,[0],[0]
"− xj |, (26)
whereHi,i is a diagonal positive definite matrix, and b and ν are given constants.",4.2. Graph-Guided Fused Lasso,[0],[0]
"Let hii and hjj be the i-th and j-th diagonal elements ofHi,i, respectively.
",4.2. Graph-Guided Fused Lasso,[0],[0]
"Proposition 5 The optimal solution of (26) takes the following closed-form:
x∗ =  ",4.2. Graph-Guided Fused Lasso,[0],[0]
bl,4.2. Graph-Guided Fused Lasso,[0],[0]
− h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"ll λ ∗, l = i, bl + h −1",4.2. Graph-Guided Fused Lasso,[0],[0]
"ll λ
∗, l = j, bl, l 6=",4.2. Graph-Guided Fused Lasso,[0],[0]
"i, j,
(27)
where λ∗ is defined as
λ∗ =  bi−bj",4.2. Graph-Guided Fused Lasso,[0],[0]
h−1ii,4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj , ∣∣∣ bi−bj h−1ii",4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj ∣∣∣ ≤ ν; sign (bi − bj) ν, ∣∣∣ bi−bj h−1ii",4.2. Graph-Guided Fused Lasso,[0],[0]
+,4.2. Graph-Guided Fused Lasso,[0],[0]
h −1,4.2. Graph-Guided Fused Lasso,[0],[0]
"jj
∣∣∣ >",4.2. Graph-Guided Fused Lasso,[0],[0]
ν.,4.2. Graph-Guided Fused Lasso,[0],[0]
"In the implementation, we use the similar parameter settings of S, ν as above.",4.2. Graph-Guided Fused Lasso,[0],[0]
"The dimension parameter pair (n, d) is chosen from the following set
(n, d) ∈ {
(2000, 500), (2000, 1000), (5000, 1000), (5000, 2000), (10000, 2000), (10000, 4000)
} ,
and the parameter αi = 100/|E|2.",4.2. Graph-Guided Fused Lasso,[0],[0]
"Similarly, all the compared algorithms start with the same initial point.",4.2. Graph-Guided Fused Lasso,[0],[0]
"The following six pictures in Figures 3 and 4 display the comparisons of the five solvers for six kinds of choices of (n, d).",4.2. Graph-Guided Fused Lasso,[0],[0]
"It
is obvious that the other four solvers GFB, PDM, AP-APG and APA-APG are not as efficient as the proposed GSOS algorithm, which demonstrates that the Gauss-Seidel technique is very useful for addressing nonsmooth optimization.",4.2. Graph-Guided Fused Lasso,[0],[0]
It is worthwhile to point out that the primal solver GFB is faster than the primal-dual solver PDM on graphguided fused Lasso.,4.2. Graph-Guided Fused Lasso,[0],[0]
"One possible reason is that the number of nonsmooth terms is too large, which will lead to a large quantity of dual variables introduced in PDM and hence slow down the updating of primal variables.",4.2. Graph-Guided Fused Lasso,[0],[0]
"In this paper, we proposed a novel first-order algorithm called GSOS for addressing multi-term nonsmooth convex composite optimization.",5. Conclusions,[0],[0]
"This algorithm inherits the advantages of the Gauss-Seidel technique and the operator splitting technique, therefore being largely accelerated.",5. Conclusions,[0],[0]
"We found that the GSOS algorithm includes the generalized forward backward splitting method (Raguet et al., 2013) as a special case.",5. Conclusions,[0],[0]
"In addition, we developed a new technique to establish the global convergence and iteration complexity of the GSOS algorithm.",5. Conclusions,[0],[0]
"Last, we applied the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems, and compared it against several state-of-the-art algorithms.",5. Conclusions,[0],[0]
The experimental results show the great superiority of the GSOS algorithm in terms of both efficiency and effectiveness.,5. Conclusions,[0],[0]
Yuan is supported by NSF-China (61402182).,Acknowledgements,[0],[0]
"In this paper, we propose a fast Gauss-Seidel Operator Splitting (GSOS) algorithm for addressing multi-term nonsmooth convex composite optimization, which has wide applications in machine learning, signal processing and statistics.",abstractText,[0],[0]
"The proposed GSOS algorithm inherits the advantage of the Gauss-Seidel technique to accelerate the optimization procedure, and leverages the operator splitting technique to reduce the computational complexity.",abstractText,[0],[0]
"In addition, we develop a new technique to establish the global convergence of the GSOS algorithm.",abstractText,[0],[0]
"To be specific, we first reformulate the iterations of GSOS as a twostep iterations algorithm by employing the tool of operator optimization theory.",abstractText,[0],[0]
"Subsequently, we establish the convergence of GSOS based on the two-step iterations algorithm reformulation.",abstractText,[0],[0]
"At last, we apply the proposed GSOS algorithm to solve overlapping group Lasso and graph-guided fused Lasso problems.",abstractText,[0],[0]
Numerical experiments show that our proposed GSOS algorithm is superior to the state-of-the-art algorithms in terms of both efficiency and effectiveness.,abstractText,[0],[0]
GSOS: Gauss-Seidel Operator Splitting Algorithm for  Multi-Term Nonsmooth Convex Composite Optimization,title,[0],[0]
"↵ (1 e ↵) for cardinality constrained maximization. In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints. We experimentally validate our theoretical findings for both synthetic and real-world applications.",text,[0],[0]
"Many important problems, such as experimental design and sparse modeling, are naturally formulated as a subset selection problem, where a set function F (S) over a Kcardinality constraint is maximized, i.e.,
max S✓V,|S|K F (S), (P)
1Department of Computer Science, ETH Zurich, Zurich, Switzerland.",1. Introduction,[0],[0]
"Correspondence to: Joachim M. Buhmann <jbuhmann@inf.ethz.ch>, Andreas Krause <krausea@ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"where V = {v1, . . .",1. Introduction,[0],[0]
", vn} is the ground set.",1. Introduction,[0],[0]
"Specifically, in experimental design, the goal is to select a set of experiments to perform such that some statistical criterion is optimized.",1. Introduction,[0],[0]
This problem arises naturally in domains where performing experiments is costly.,1. Introduction,[0],[0]
"In sparse modeling, the task is to identify sparse representations of signals, enabling interpretability and robustness in high-dimensional statistical problems—properties that are crucial in modern data analysis.
",1. Introduction,[0],[0]
"Frequently, the standard GREEDY algorithm (Alg. 1) is used to (approximately) solve (P).",1. Introduction,[0],[0]
"For the case that F (S)
Algorithm 1: The GREEDY Algorithm Input: Ground set V , set function F : 2V!R+, budget K S
0 ; for t = 1, . . .",1. Introduction,[0],[0]
",K do
v ⇤ argmax v2V\St 1 F (S t 1",1. Introduction,[0],[0]
"[ {v}) F (St 1) S
t St 1 [ {v⇤} Output: SK
is a monotone nondecreasing submodular set function1, the GREEDY algorithm enjoys the multiplicative approximation guarantee of (1 1/e) (Nemhauser et al., 1978; Vondrák, 2008; Krause & Golovin, 2014).",1. Introduction,[0],[0]
"This constant factor can be improved by refining the characterization of the objective using the curvature (Conforti & Cornuéjols, 1984; Vondrák, 2010; Iyer et al., 2013), which informally quantifies how close a submodular function is to being modular (i.e., F (S) and F (S) are submodular).
",1. Introduction,[0],[0]
"However, for many applications, including experimental design and sparse Gaussian processes (Lawrence et al., 2003), F (S) is in general not submodular (Krause et al., 2008) and the above guarantee does not hold.",1. Introduction,[0],[0]
"In practice, however, the standard GREEDY algorithm often achieves very good performance on these applications, e.g., in subset selection with the R2 (squared multiple correlation) ob-
1 F (·) is monotone nondecreasing if 8A ✓ V, v 2 V , F (A [ {v})",1. Introduction,[0],[0]
F (A).,1. Introduction,[0],[0]
F (·) is submodular iff it satisfies the diminishing returns property F (A [ {v}),1. Introduction,[0],[0]
F (A) F (B [ {v}) F (B) for all A ✓ B ✓ V \{v}.,1. Introduction,[0],[0]
Assume wlog.,1. Introduction,[0],[0]
"that F (·) is normalized, i.e., F (;) = 0.
jective (Das & Kempe, 2011).",1. Introduction,[0],[0]
"To explain the good empirical performance, Das & Kempe (2011) proposed the submodularity ratio, a quantity characterizing how close a set function is to being submodular.
",1. Introduction,[0],[0]
"Another important class of non-submodular set functions comes as the auxiliary function when optimizing a continuous function f(x) s.t. combinatorial constraints, i.e., min
x2C,supp(x)2I f(x), where supp(x) := {i | xi 6= 0} is the support set of x, C is a convex set, and I is the independent sets of the combinatorial structure.",1. Introduction,[0],[0]
"One of the most popular ways to solve this problem is to use the GREEDY algorithm to maximize the auxiliary function F (S) : = max
x2C,supp(x)✓S f(x).",1. Introduction,[0],[0]
"This setting covers various important applications, to name a few, feature selection (Guyon & Elisseeff, 2003), sparse approximation (Das & Kempe, 2008; Krause & Cevher, 2010), sparse recovery (Candes et al., 2006), sparse M-estimation (Jain et al., 2014), linear programming (LP) with combinatorial constraints, and column subset selection (Altschuler et al., 2016).",1. Introduction,[0],[0]
"Recently, Elenberg et al. (2016) proved that if f(x) has L-restricted smoothness and m-restricted strong convexity, then the submodularity ratio of F (S) is lower bounded by m/L.",1. Introduction,[0],[0]
"This result significantly enlarges the domain where the GREEDY algorithm can be applied.
",1. Introduction,[0],[0]
"In this paper, we combine and generalize the ideas of curvature and submodularity ratio to derive improved constant factor approximation guarantees of the GREEDY algorithm.",1. Introduction,[0],[0]
Our guarantees allow us to better characterize the empirical success of applying GREEDY on a significantly larger class of non-submodular functions.,1. Introduction,[0],[0]
"Furthermore, we bound these characteristics for important applications, rendering the usage of GREEDY a principled choice rather than a mere heuristic.",1. Introduction,[0],[0]
"Our main contributions are:
- We prove the first tight constant-factor approximation guarantees for GREEDY on maximizing nonsubmodular nondecreasing set functions s.t.",1. Introduction,[0],[0]
"a cardinality constraint, characterized by a novel combination of the (generalized) notions of submodularity ratio and curvature ↵.
- By theoretically bounding parameters ( ,↵) for several important objectives, including Bayesian A-optimality in experimental design, the determinantal function of a square submatrix and maximization of LPs with combinatorial constraints, our theory implies the first guarantees for them.
- Lastly, we experimentally validate our theory on several real-world applications.",1. Introduction,[0],[0]
"It is worth noting that for the Bayesian A-optimality objective, GREEDY generates comparable solutions as the classically used semidefinite programming (SDP) based method, but is usually two orders of magnitude faster.
Notation.",1. Introduction,[0],[0]
"We use boldface letters, e.g., x, to represent vectors, and capital boldface letters, e.g., A, to denote matrices.",1. Introduction,[0],[0]
"x
i is the ith entry of the vector x. We refer to V = {v1, ..., vn} as the ground set.",1. Introduction,[0],[0]
"We use f(·) to denote a continuous function, and F (·) to represent a set function.",1. Introduction,[0],[0]
"supp(x) := {i 2 V | x
i 6= 0} is the support set of the vector x, and [n] := {1, ..., n} for an integer n 1.",1. Introduction,[0],[0]
We denote the marginal gain of a set ⌦ ✓ V in context of a set S ✓ V as ⇢⌦(S) := F (⌦ [ S) F (S).,1. Introduction,[0],[0]
"For v 2 V , we use the shorthand ⇢
v (S) for ⇢{v}(S).",1. Introduction,[0],[0]
"In this section we provide the submodularity ratio and curvature for general, not necessarily submodular functions2, they are natural extensions of the classical ones.",2. Submodularity Ratio and Curvature,[0],[0]
"Let S 0 = ;, St = {j1, ..., jt}, t = 1, ...,K be the successive sets chosen by GREEDY.",2. Submodularity Ratio and Curvature,[0],[0]
"For brevity, let ⇢ t : = ⇢ jt(S t 1 ) be the marginal gain of GREEDY in step t.
Definition 1 (Submodularity ratio (Das & Kempe, 2011)).",2. Submodularity Ratio and Curvature,[0],[0]
"The submodularity ratio of a non-negative set function F (·) is the largest scalar s.t.
X !2⌦\S ⇢ !",2. Submodularity Ratio and Curvature,[0],[0]
"(S) ⇢⌦(S), 8 ⌦, S ✓ V.
The greedy submodularity ratio is the largest scalar G s.t. X
!2⌦\St ⇢ !",2. Submodularity Ratio and Curvature,[0],[0]
"(S
t )",2. Submodularity Ratio and Curvature,[0],[0]
"G⇢⌦(St), 8|⌦|=K, t = 0, . .",2. Submodularity Ratio and Curvature,[0],[0]
.,2. Submodularity Ratio and Curvature,[0],[0]
",K 1.
",2. Submodularity Ratio and Curvature,[0],[0]
It is easy to see that G .,2. Submodularity Ratio and Curvature,[0],[0]
The submodularity ratio measures to what extent F (·) has submodular properties.,2. Submodularity Ratio and Curvature,[0],[0]
"We make the following observations:
Remark 1.",2. Submodularity Ratio and Curvature,[0],[0]
"For a nondecreasing function F (·), it holds a) ,
G 2",2. Submodularity Ratio and Curvature,[0],[0]
"[0, 1]; b) F (·) is submodular iff = 1.",2. Submodularity Ratio and Curvature,[0],[0]
Definition 2 (Generalized curvature).,2. Submodularity Ratio and Curvature,[0],[0]
"The curvature of a non-negative function F (·) is the smallest scalar ↵ s.t.
⇢
i (S \ {i} [ ⌦) (1 ↵)⇢",2. Submodularity Ratio and Curvature,[0],[0]
"i (S \ {i}), 8 ⌦, S ✓ V, i 2 S\⌦.
",2. Submodularity Ratio and Curvature,[0],[0]
"The greedy curvature is the smallest scalar ↵G 0 s.t.
⇢ ji(S i 1 [ ⌦) (1 ↵G)⇢",2. Submodularity Ratio and Curvature,[0],[0]
"ji(S i 1 ),
8 ⌦ : |⌦| = K, i : j i 2 SK 1\⌦.",2. Submodularity Ratio and Curvature,[0],[0]
2Curvature is commonly defined for submodular functions.,2. Submodularity Ratio and Curvature,[0],[0]
Sviridenko et al. (2013) presented a notion of curvature for monotone non-submodular functions.,2. Submodularity Ratio and Curvature,[0],[0]
We show in Appendix C the details of these notions and the relations to ours.,2. Submodularity Ratio and Curvature,[0],[0]
"Additionally, we prove in Remark 3 of Appendix C.2 that our combination of curvature and submodularity ratio is more expressive than that of Sviridenko et al. (2013) in characterizing the maximization of problem (P) using standard GREEDY.
",2. Submodularity Ratio and Curvature,[0],[0]
"When K = n or 1, SK 1\⌦ = ;, it is natural to define ↵ G
= 0.",2. Submodularity Ratio and Curvature,[0],[0]
It is easy to observe that ↵G  ↵.,2. Submodularity Ratio and Curvature,[0],[0]
"Note that the classical total curvature is ↵total := 1 min i2V ⇢i(V\{i})
⇢i(;) .
",2. Submodularity Ratio and Curvature,[0],[0]
Remark 2.,2. Submodularity Ratio and Curvature,[0],[0]
"For a nondecreasing function F (·), it holds: a) ↵,↵G 2",2. Submodularity Ratio and Curvature,[0],[0]
"[0, 1]; b) F (·) is supermodular iff ↵ = 0; c)",2. Submodularity Ratio and Curvature,[0],[0]
"If F (·) is submodular, then ↵G  ↵ = ↵total.
",2. Submodularity Ratio and Curvature,[0],[0]
"So for a submodular function, our notion of curvature is consistent with ↵total.",2. Submodularity Ratio and Curvature,[0],[0]
"Notably, ↵G usually characterizes the problem better than ↵total, as will be validated in Section 5.",2. Submodularity Ratio and Curvature,[0],[0]
We present approximation guarantee of GREEDY in Theorem 1.,3. Approximation Guarantee,[0],[0]
Note that both versions of the submodularity ratio and curvature apply in the proof.,3. Approximation Guarantee,[0],[0]
"For brevity, we use and ↵ to refer to any of these versions in the sequel.",3. Approximation Guarantee,[0],[0]
In Section 3.3 we prove tightness of the approximation guarantees.,3. Approximation Guarantee,[0],[0]
All omitted proofs are given in Appendix B. Theorem 1.,3. Approximation Guarantee,[0],[0]
Let F (·) be a non-negative nondecreasing set function with submodularity ratio 2,3. Approximation Guarantee,[0],[0]
"[0, 1] and curvature ↵ 2 [0, 1].",3. Approximation Guarantee,[0],[0]
"The GREEDY algorithm enjoys the following approximation guarantee for solving problem (P):
F (S
K ) 1 ↵
"" 1 ✓ K ↵
K
◆ K # F (⌦ ⇤ )
1 ↵
(1 e ↵ )",3. Approximation Guarantee,[0],[0]
"F (⌦⇤), (1)
where ⌦⇤ is the optimal solution of (P) and SK the output of the GREEDY algorithm.3",3. Approximation Guarantee,[0],[0]
"Before proving the theorem, we want to give the reader an intuition of the results and show how our results recover and extend several classical guarantees for the GREEDY algorithm.",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 0 (i.e., F (·) is supermodular), the approximation guarantee is lim
↵!0 1 ↵ (1 e ↵ ) = , which gives the first guarantee of greedily maximizing a nondecreasing supermodular function with bounded .",3.1. Interpreting Theorem 1,[0],[0]
"When = 1, (i.e., F (·) is submodular), we recover the guarantee of ↵ 1(1 e ↵) (Conforti & Cornuéjols, 1984).",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 1, we have a guarantee of (1 e ) (Das & Kempe, 2011).",3.1. Interpreting Theorem 1,[0],[0]
"For the case ↵ = 1, = 1, we recover the classical guarantee of (1 1/e) (Nemhauser et al., 1978).",3.1. Interpreting Theorem 1,[0],[0]
We plot the constant-factor approximation guarantees for different values of and ↵ in Fig. 1.,3.1. Interpreting Theorem 1,[0],[0]
"One interesting phenomenon is that and ↵ play different roles: Looking at = 0, the approximation factor is always 0, independent of the value ↵ takes.",3.1. Interpreting Theorem 1,[0],[0]
"In contrast, for ↵ = 0, the
3For the setting that GREEDY is allowed to pick more than K elements, e.g., pick K0 > K elements, our theory can be easily extended to show that",3.1. Interpreting Theorem 1,[0],[0]
F (SK 0 ) ↵ 1(1 e ↵ K 0/,3.1. Interpreting Theorem 1,[0],[0]
"K)F (⌦⇤).
",3.1. Interpreting Theorem 1,[0],[0]
approximation guarantee is (1 e ).,3.1. Interpreting Theorem 1,[0],[0]
This can be interpreted as the curvature boosting the guarantees.,3.1. Interpreting Theorem 1,[0],[0]
The high-level proof framework is based on Conforti & Cornuéjols (1984) (where they derive the approximation guarantee for maximizing a nondecreasing submodular function with bounded curvature).,3.2. Proof of Theorem 1,[0],[0]
"However, adapting the proof to non-submodular functions requires several changes detailed in Section 6.
",3.2. Proof of Theorem 1,[0],[0]
Proof overview.,3.2. Proof of Theorem 1,[0],[0]
"Let us denote all problem instances of maximizing a non-negative nondecreasing function F (·) s.t. K-cardinality constraint (max|S|K F (S)) to be P K,↵,
, where F (·) is parametrized by submodularity ratio and curvature ↵.",3.2. Proof of Theorem 1,[0],[0]
"Let P⌦⇤,SK 2 PK,↵, denote those problem instances with optimal solution ⌦⇤ and greedy solution SK .",3.2. Proof of Theorem 1,[0],[0]
"We group all problem instances P
K,↵, according to the set ⌦",3.2. Proof of Theorem 1,[0],[0]
"⇤ \ SK := {l1 = jm1 , l2 = jm2 , . . .",3.2. Proof of Theorem 1,[0],[0]
", ls = j
ms}, where jm1 , . . .",3.2. Proof of Theorem 1,[0],[0]
", jms are consistent with the order of greedy selection.",3.2. Proof of Theorem 1,[0],[0]
"Let us denote the problem instances with ⌦ ⇤\SK = {l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} as the group PK,↵, ({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}).
",3.2. Proof of Theorem 1,[0],[0]
"The main idea of the proof is to investigate the worst-case approximation ratio of each group of the problem instances P K,↵,
({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}), 8{l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} ✓ SK .",3.2. Proof of Theorem 1,[0],[0]
We do this by constructing LPs based on the properties of the problem instances.,3.2. Proof of Theorem 1,[0],[0]
"By studying the structures of these LPs, we will prove that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \SK = ;.",3.2. Proof of Theorem 1,[0],[0]
"Thus the desired approximation guarantee corresponds to the worst-case approximation ratio of P
K,↵,
(;).
",3.2. Proof of Theorem 1,[0],[0]
The proof.,3.2. Proof of Theorem 1,[0],[0]
"When = 0 or F (⌦⇤) = 0, (1) holds naturally.",3.2. Proof of Theorem 1,[0],[0]
"In the following, let 2 (0, 1] and F (⌦⇤) > 0.",3.2. Proof of Theorem 1,[0],[0]
"First, we present Lemma 1, which will be used to construct the LPs.
",3.2. Proof of Theorem 1,[0],[0]
Lemma 1.,3.2. Proof of Theorem 1,[0],[0]
"For any ⌦ ✓ V with |⌦| = K and any t 2 {0, . . .",3.2. Proof of Theorem 1,[0],[0]
",K 1}, let wt := |St \ ⌦|.",3.2. Proof of Theorem 1,[0],[0]
"It holds that
↵
X
i:ji2St\⌦
⇢
i
+
X
i:ji2St\⌦
⇢
i
+ 1 (K wt)⇢
t+1 F (⌦).
",3.2. Proof of Theorem 1,[0],[0]
"We now specify the constructing of the LPs: For any problem instance P⌦⇤,SK 2 PK,↵, ({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}), we know that F (SK) =",3.2. Proof of Theorem 1,[0],[0]
"P K
i=1 ⇢",3.2. Proof of Theorem 1,[0],[0]
i (telescoping sum).,3.2. Proof of Theorem 1,[0],[0]
"Hence, the approximation ratio is F (S
K) F (⌦⇤) =",3.2. Proof of Theorem 1,[0],[0]
P i ⇢,3.2. Proof of Theorem 1,[0],[0]
"i
F (⌦⇤) , which we denote as R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) =",3.2. Proof of Theorem 1,[0],[0]
P i ⇢,3.2. Proof of Theorem 1,[0],[0]
i F (⌦⇤) .,3.2. Proof of Theorem 1,[0],[0]
Define xi,3.2. Proof of Theorem 1,[0],[0]
":= ⇢i
F (⌦⇤) , i 2",3.2. Proof of Theorem 1,[0],[0]
[K].,3.2. Proof of Theorem 1,[0],[0]
"Since F is nondecreasing, x
i 0.",3.2. Proof of Theorem 1,[0],[0]
"Plugging ⌦ = ⌦⇤ into Lemma 1, and considering t = 0, . . .",3.2. Proof of Theorem 1,[0],[0]
",K 1, we have in total K constraints over the variables x
i , which constitute the constraints of the LP.",3.2. Proof of Theorem 1,[0],[0]
"So the worst-case approximation ratio of the group P
K,↵, ({l1, . .",3.2. Proof of Theorem 1,[0],[0]
.,3.2. Proof of Theorem 1,[0],[0]
", ls}) is:
R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) =",3.2. Proof of Theorem 1,[0],[0]
"min X K
i=1",3.2. Proof of Theorem 1,[0],[0]
x,3.2. Proof of Theorem 1,[0],[0]
"i
, s.t. x",3.2. Proof of Theorem 1,[0],[0]
"i 0 and,
row (0) row (1)
... row (l1 1) row (l2 1) row (q = lr) ...",3.2. Proof of Theorem 1,[0],[0]
"row (ls 1) ... row (K 1)
2
66666666666666666664
K
↵
K
... ... . . .",3.2. Proof of Theorem 1,[0],[0]
↵ ↵ · · · K 0 ↵ ↵ · · · 1 K 1 ↵ ↵ · · · 1 1 K r ... ... ... ...,3.2. Proof of Theorem 1,[0],[0]
"...
. . .",3.2. Proof of Theorem 1,[0],[0]
↵ ↵ · · · 1 1 ↵ · · · K s+1 ... ... ... ... ... ... . . .,3.2. Proof of Theorem 1,[0],[0]
"↵ ↵ · · · 1 1 ↵ · · · 1 · · · K s
3
77777777777777777775 ·
2
66666666666666664
x1
x2
...",3.2. Proof of Theorem 1,[0],[0]
"xl1 xl2 xq+1
...",3.2. Proof of Theorem 1,[0],[0]
"xls
... xK
3
77777777777777775
2
66666666666666664 1 1 ... 1 1 1 ... 1 ... 1
3
77777777777777775
(2)
",3.2. Proof of Theorem 1,[0],[0]
"The following Lemma presents the key structure of the constructed LPs, which will be used to deduce the relation between the LPs of different problem instance groups.",3.2. Proof of Theorem 1,[0],[0]
Lemma 2.,3.2. Proof of Theorem 1,[0],[0]
Assume that the optimal solution of the constructed LP is x⇤ 2 RK+,3.2. Proof of Theorem 1,[0],[0]
and that s = |⌦⇤ \ SK | 1.,3.2. Proof of Theorem 1,[0],[0]
"For all 1  r  s it holds that x⇤
q  x⇤ q+1, where q = lr.
",3.2. Proof of Theorem 1,[0],[0]
Proof sketch of Lemma 2.,3.2. Proof of Theorem 1,[0],[0]
"Assume by virture of creating a contradiction that x⇤
q
> x ⇤ q+1.",3.2. Proof of Theorem 1,[0],[0]
"We can always create a
new feasible solution y⇤ 2 RK+ by decreasing x⇤q by some ✏ > 0, while increasing all the x⇤
q+1 to x⇤ K by some proper values, s.t. y⇤ has smaller LP objective value.",3.2. Proof of Theorem 1,[0],[0]
"Specifically, we define y⇤ as: for k = 1, . . .",3.2. Proof of Theorem 1,[0],[0]
", q 1, y⇤
k
:= x ⇤ k ; y
⇤ q",3.2. Proof of Theorem 1,[0],[0]
":= x ⇤ q ✏; for k = q + 1, . . .",3.2. Proof of Theorem 1,[0],[0]
",K, y⇤ k",3.2. Proof of Theorem 1,[0],[0]
":= x ⇤ k + ✏ k where ✏
k s are defined recursively as: ✏ q+1 = ✏ K r , and
✏ q+1+u =",3.2. Proof of Theorem 1,[0],[0]
"✏q+u K r u+ 1 K r u , 1  u  K q 1.
",3.2. Proof of Theorem 1,[0],[0]
Claim 1.,3.2. Proof of Theorem 1,[0],[0]
"a) The new solution y⇤ 0; b) All of the constraints in (2) are still feasible for y⇤.
",3.2. Proof of Theorem 1,[0],[0]
"After that the change of the LP objective is,
LP = ✏+ ✏ q+1 + ✏q+2 + . .",3.2. Proof of Theorem 1,[0],[0]
.+,3.2. Proof of Theorem 1,[0],[0]
"✏K .
",3.2. Proof of Theorem 1,[0],[0]
"One can prove that the LP objective decreases:
Claim 2.",3.2. Proof of Theorem 1,[0],[0]
"For all K 1, 1  r  q < K, it holds that
LP  0, 8 2 (0, 1].",3.2. Proof of Theorem 1,[0],[0]
Equality is achieved when r = q and = 1.,3.2. Proof of Theorem 1,[0],[0]
"Therefore we reach the contradiction that x⇤ is an optimal solution of the constructed LP.
",3.2. Proof of Theorem 1,[0],[0]
"Given Lemma 2, we prove in the following Lemma, which states that the worst-case approximation ratio of all problem instances occurs when ⌦⇤ \ SK = ;.",3.2. Proof of Theorem 1,[0],[0]
Lemma 3.,3.2. Proof of Theorem 1,[0],[0]
"For all {l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls} ✓ SK , it holds that
R({l1, . . .",3.2. Proof of Theorem 1,[0],[0]
", ls}) R(;) = 1 ↵
 1 ⇣ K ↵
K
⌘ K
.
",3.2. Proof of Theorem 1,[0],[0]
"So the greedy solution has objective F (SK) 1 ↵  1 ⇣ K ↵ K ⌘ K F (⌦ ⇤ ) 1 ↵ (1 e ↵ )F (⌦⇤).
3.3.",3.2. Proof of Theorem 1,[0],[0]
"Tightness Result
We demonstrate that the approximation guarantee in Theorem 1 is tight, i.e., for every submodularity ratio and every curvature ↵, there exist set functions that achieve the bound exactly.
",3.2. Proof of Theorem 1,[0],[0]
"Assume the ground set V contains the elements in S :=
{j1, . . .",3.2. Proof of Theorem 1,[0],[0]
", jK}",3.2. Proof of Theorem 1,[0],[0]
"and the elements in ⌦ := {!1, . . .",3.2. Proof of Theorem 1,[0],[0]
",!K} (S \ ⌦ = ;) and n 2K dummy elements.",3.2. Proof of Theorem 1,[0],[0]
"The objective function we are going to construct will not depend on these dummy elements, i.e., the objective value of a set does not change if dummy elements are removed from or added to that set.",3.2. Proof of Theorem 1,[0],[0]
"Consequently, the dummy elements will not affect the submodularity ratio and the curvature.",3.2. Proof of Theorem 1,[0],[0]
"For the constants ↵ 2 [0, 1], 2 (0, 1], we define the objective function as,
F (T ) := f(|⌦ \ T |)
K
1 ↵
X
i:ji2S\T
⇠i
+
X
i:ji2S\T
⇠i, (3)
where ⇠ i : = 1 K
⇣ K ↵
K
⌘ i 1
, i 2",3.2. Proof of Theorem 1,[0],[0]
"[K]; f(x) = 1 1 K 1 x 2 +
K 1 K 1 x. Note that f(x) is convex nondecreasing over [0,K], and that f(0)",3.2. Proof of Theorem 1,[0],[0]
"= 0, f(1) = 1, f(K) = K/ .",3.2. Proof of Theorem 1,[0],[0]
It is clear that F (;) = 0 and F (·) is monotone nondecreasing.,3.2. Proof of Theorem 1,[0],[0]
"The following lemma shows that it is generally nonsubmodular and non-supermodular.
",3.2. Proof of Theorem 1,[0],[0]
Lemma 4.,3.2. Proof of Theorem 1,[0],[0]
"For the objective in (3): a) When ↵ = 0, it is supermodular; b) When = 1, it is submodular; c) F (T ) has submodularity ratio and curvature ↵.
",3.2. Proof of Theorem 1,[0],[0]
"Considering the problem of max|T |K F (T ), we claim that the GREEDY algorithm may output S. This can be proved by induction.",3.2. Proof of Theorem 1,[0],[0]
"One can see that ⇢
j1(;) =",3.2. Proof of Theorem 1,[0],[0]
"⇠1 = ⇢
!1(;), so GREEDY can choose j1 in the first step.",3.2. Proof of Theorem 1,[0],[0]
"Assume in step t 1 GREEDY has chosen St 1 = {j1, . . .",3.2. Proof of Theorem 1,[0],[0]
", jt 1}, one can verify that the marginal gains coincide, i.e., ⇢
jt(S t 1 )",3.2. Proof of Theorem 1,[0],[0]
= ⇠ t = ⇢ !,3.2. Proof of Theorem 1,[0],[0]
t(S t 1 ).,3.2. Proof of Theorem 1,[0],[0]
"However, the optimal solution is actually ⌦ with function value as F (⌦) = 1 .",3.2. Proof of Theorem 1,[0],[0]
"So the
approximation ratio is F (S) F (⌦) = 1 ↵
 1 ⇣ K ↵
K
⌘ K
, which
matches our approximation guarantee in Theorem 1.",3.2. Proof of Theorem 1,[0],[0]
We consider several important real-world applications and their corresponding objective functions.,4. Applications,[0],[0]
"We show that the submodularity ratio and the curvature of these functions can be bounded and, hence, the approximation guarantees from our theoretical results are applicable.",4. Applications,[0],[0]
All the omitted proofs are provided in Appendix D.,4. Applications,[0],[0]
"In Bayesian experimental design (Chaloner & Verdinelli, 1995), the goal is to select a set of experiments to perform s.t.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"some statistical criterion is optimized, e.g., the variance of certain parameter estimates is minimized.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Krause et al. (2008) investigated several criteria for this purpose, amongst others the Bayesian A-optimality criterion.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
This criterion is used to maximally reduce the variance in the posterior distribution over the parameters.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"In general, the criterion is not submodular as shown in Krause et al. (2008, Section 8.4).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Formally, assume there are n experimental stimuli {x1, . . .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
",xn}, each xi 2 Rd, which constitute the data matrix X 2 Rd⇥n. Let us arrange a set S ✓ V of stimuli as a matrix X
S
:
=",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"[x v1 , . . .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
",xvs ] 2 Rd⇥|S|.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Let ✓ 2 Rd be the parameter vector in the linear model y
S = X> S ✓ +w, where w is the Gaussian noise with zero mean and variance
2, i.e., w ⇠ N (0, 2I), and y S is the vector of dependent variables.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Suppose the prior takes the form of an isotropic Gaussian, i.e., ✓ ⇠ N (0,⇤ 1),⇤ = 2I. Then,  y
S
✓
⇠ N (0,⌃),⌃ =

2I+X> S ⇤ 1X S X> S ⇤ 1
⇤ 1X S
⇤ 1
.
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
This implies that ⌃,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
✓|yS =,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
(⇤ + 2X S X> S ) 1.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The Aoptimality objective is defined as,
F
A
(S)
: = tr(⌃ ✓ ) tr(⌃ ✓|yS ) (4) = tr(⇤ 1) tr((⇤+ 2X S X> S ) 1 ).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The following Proposition gives bounds on the submodularity ratio and curvature of (4).
",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
Proposition 1.,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Assume normalized stimuli, i.e., kx i k = 1, 8i 2 V .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
Let the spectral norm of X be kXk.4,4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Then, a) The objective in (4) is monotone nondecreasing.",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
b),4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"Its submodularity ratio can be lower bounded by 2
kXk2( 2+ 2kXk2) , and its curvature ↵ can be upper
bounded by 1 2
kXk2( 2+ 2kXk2) .",4.1. Bayesian A-optimality in Experimental Design,[0],[0]
"The determinantal function of a square submatrix is widely used in many areas, e.g., in determinantal point processes (Kulesza & Taskar, 2012) and active set selection for sparse Gaussian processes.",4.2. The Determinantal Function,[0],[0]
Monotone nondecreasing determinantal functions appear in the second problem.,4.2. The Determinantal Function,[0],[0]
Assume ⌃ is the covariance matrix parameterized by a positive definite kernel.,4.2. The Determinantal Function,[0],[0]
"In the Informative Vector Machine (Lawrence et al., 2003), the information gain of a subset of points S ✓ V is 1 2 logF (S), where
F (S)
:",4.2. The Determinantal Function,[0],[0]
"= det(I+ 2⌃ S ), (5)
where is the noise variance in the Gaussian process model, ⌃
S is the square submatrix with both its rows and columns indexed by S. Although logF (S) is submodular, F (S) is in general not submodular.",4.2. The Determinantal Function,[0],[0]
The approximation guarantee of GREEDY for maximizing logF (S) does not translate to a guarantee for maximizing F (S).,4.2. The Determinantal Function,[0],[0]
"The following Proposition characterizes (5).
",4.2. The Determinantal Function,[0],[0]
Proposition 2.,4.2. The Determinantal Function,[0],[0]
"a) F (S) in (5) is supermodular, its curvature is 0; b) Let the eigenvalues of A := I + 2⌃ be 1 · · · n >",4.2. The Determinantal Function,[0],[0]
1.,4.2. The Determinantal Function,[0],[0]
"The greedy submodularity ratio of F (S) can be lower bounded by K( n 1)
",4.2. The Determinantal Function,[0],[0]
( QK j=1 j) 1 .,4.2. The Determinantal Function,[0],[0]
LPs with combinatorial constraints appear frequently in practice.,4.3. LPs with Combinatorial Constraints,[0],[0]
Consider the following example: Suppose that V is the set of all products a company can produce.,4.3. LPs with Combinatorial Constraints,[0],[0]
"Given budget constraints on the raw materials needed, companies consider the LP max
x2Phd,xi, where d is the vector of profits for the individual products and where P is a polytope representing the continuous constraints.",4.3. LPs with Combinatorial Constraints,[0],[0]
The above LP can be used to assess the profit maximizing production plan.,4.3. LPs with Combinatorial Constraints,[0],[0]
Usually the company needs to consider combinatorial constraints as well.,4.3. LPs with Combinatorial Constraints,[0],[0]
"For instance, the company has at most K production lines, thus they have to select a subset of K products to produce.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Often this kind of problems can be formalized as max
x2P,supp(x)2Ihd,xi, where I is the independent set of the combinatorial structure.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Hence, a natural auxiliary set function is,
F (S) := maxsupp(x)✓S, x2Phd,xi, 8S ✓ V. (6) 4By",4.3. LPs with Combinatorial Constraints,[0],[0]
"Weyl’s inequality, a naive upper bound is kXk  p n.
Let P = {x 2",4.3. LPs with Combinatorial Constraints,[0],[0]
"Rn | 0  x  ¯u,Ax  b, ¯u 2 Rn+,A 2 Rm⇥n+ , b 2 Rm+}.",4.3. LPs with Combinatorial Constraints,[0],[0]
In general F (S) in (6) is non-submodular as illustrated by two examples in Appendix D.3.,4.3. LPs with Combinatorial Constraints,[0],[0]
"Upper bounding the curvature is equivalent to lower bounding F (S[⌦) F (S\{i}[⌦)
F (S) F (S\{i}) , which can be 0 in the worst case.",4.3. LPs with Combinatorial Constraints,[0],[0]
"However, the submodularity ratio can be lower bounded by a non-zero scalar.
",4.3. LPs with Combinatorial Constraints,[0],[0]
Proposition 3.,4.3. LPs with Combinatorial Constraints,[0],[0]
a) F (S) in (6) is a normalized nondecreasing set function.,4.3. LPs with Combinatorial Constraints,[0],[0]
b),4.3. LPs with Combinatorial Constraints,[0],[0]
"With regular non-degenerancy assumptions (details in Appendix D.3.2), its submodularity ratio can be lower bounded by 0 > 0.",4.3. LPs with Combinatorial Constraints,[0],[0]
"Many real-world applications can benefit from the theory in this work, for instance: subset selection using the R2 objective, sparse modeling and the budget allocation problem with combinatorial constraints.",4.4. More Applications,[0],[0]
Details on these applications are deferred to Appendix G.,4.4. More Applications,[0],[0]
We empirically validated approximation guarantees characterized by the submodularity ratio and the curvature for several applications.,5. Experimental Results,[0],[0]
"Since it is too time consuming to calculate the full versions of ↵ and using exhaustive search, we only calculated the greedy versions (↵G, G).",5. Experimental Results,[0],[0]
All averaged results are from 20 repeated experiments.,5. Experimental Results,[0],[0]
Source code is available at https://github.com/bianan/ non-submodular-max.5 More results are put in Appendix H.,5. Experimental Results,[0],[0]
We considered the Bayesian A-optimality objective for both synthetic and real-world data.,5.1. Bayesian Experimental Design,[0],[0]
"In all experiments, we normalized the data points to have unit `2-norm.
",5.1. Bayesian Experimental Design,[0],[0]
Real-world results: We used the Boston Housing Data.,5.1. Bayesian Experimental Design,[0],[0]
5All experiments were implemented using Matlab.,5.1. Bayesian Experimental Design,[0],[0]
"We used the SDP solver provided by CVX (Version 2.1).
",5.1. Bayesian Experimental Design,[0],[0]
"The dataset6 has 14 features (e.g., crime rate, property tax rates, etc.) and 516 samples.",5.1. Bayesian Experimental Design,[0],[0]
"To be able to quickly calculate the parameters and optimal solution by exhaustive search, the first n = 14 samples were used.",5.1. Bayesian Experimental Design,[0],[0]
"As a baseline, we used an SDP-based algorithm (abbreviated as SDP, details are available in Appendix E).",5.1. Bayesian Experimental Design,[0],[0]
Results are shown in Fig. 2 for varying values of K. In Fig.,5.1. Bayesian Experimental Design,[0],[0]
2a we can observe that both GREEDY and SDP compute near-optimal solutions.,5.1. Bayesian Experimental Design,[0],[0]
From Fig.,5.1. Bayesian Experimental Design,[0],[0]
"2b we can see that the greedy submodularity ratio G is close to 1, and that the greedy curvature ↵G is less than 1, while the classical curvature ↵total is always 1 (the worstcase value).",5.1. Bayesian Experimental Design,[0],[0]
"This implies that the classical total curvature ↵
total characterizes the considered maximization problems less accurate than the greedy curvature.
",5.1. Bayesian Experimental Design,[0],[0]
Synthetic results: We generated random observations from a multivariate Gaussian distribution with different correlations.,5.1. Bayesian Experimental Design,[0],[0]
"To be able to assess the ground truth, we used n = 12 samples with d = 6 features.",5.1. Bayesian Experimental Design,[0],[0]
"Fig. 3 shows the results with correlation 0.2 (first column) and 0.6 (second column), respectively: The first row shows the average objective values over the optimal value with error bars, and the second row shows the parameters.",5.1. Bayesian Experimental Design,[0],[0]
One can observe that GREEDY always obtains near-optimal solutions and that these solutions are roughly comparable with those obtained by the SDP.,5.1. Bayesian Experimental Design,[0],[0]
"The classical curvature ↵total is always close to 1, while ↵G take smaller values, and G takes values close to 1, thus characterize the performance of GREEDY better.
",5.1. Bayesian Experimental Design,[0],[0]
"Medium-scale synthetic experiments: To compare the runtime of SDP and GREEDY, we considered mediumscale datasets (we cannot report results on larger datasets because of the huge computational demands of the SDP).
",5.1. Bayesian Experimental Design,[0],[0]
"6https://archive.ics.uci.edu/ml/datasets/ Housing
Fig. 4 shows the objective value achieved by GREEDY and SDP for different numbers of features d and numbers of samples n, as well as the correlations.",5.1. Bayesian Experimental Design,[0],[0]
We can observe that GREEDY computes solutions that are on par or superior to those of SDP.,5.1. Bayesian Experimental Design,[0],[0]
"In Table 1 we summarize the runtime of GREEDY and SDP for different values of d and n, for correlation 0.5.",5.1. Bayesian Experimental Design,[0],[0]
"Furthermore, we show the ratio of runtimes of the two algorithms.",5.1. Bayesian Experimental Design,[0],[0]
We can observe that GREEDY is usually two orders of magnitude faster than SDP.,5.1. Bayesian Experimental Design,[0],[0]
We generated synthetic LPs as follows:,5.2. LPs with Combinatorial Constraints,[0],[0]
"Firstly, we generated the matrix A 2 Rm⇥n+ , Aij 2 [0, 1] by drawing all entries independently from a uniform distribution on
[0, 1].",5.2. LPs with Combinatorial Constraints,[0],[0]
"We set b = d = 1, and set ¯u as 1.",5.2. LPs with Combinatorial Constraints,[0],[0]
The first row of Fig. 5 plots the optimal LP objective (calculated using exhaustive search) and the LP objective returned by GREEDY.,5.2. LPs with Combinatorial Constraints,[0],[0]
The second row shows the curvature and submodularity ratio.,5.2. LPs with Combinatorial Constraints,[0],[0]
"The first column (Fig. 5a) presents the results for n = 6,m = 20, while the second column (Fig. 5b) presents that for n = 8,m = 30.",5.2. LPs with Combinatorial Constraints,[0],[0]
"Note the greedy submodularity ratio takes values between ⇠ 0.15 and 1, and that the curvature is close to the worst-case value of 1.",5.2. LPs with Combinatorial Constraints,[0],[0]
These observations are consistent with the theory in Section 4.3.,5.2. LPs with Combinatorial Constraints,[0],[0]
"We experimented with synthetic and real-world data: For synthetic data, we generated random covariance matrices ⌃ 2 Rn⇥n with uniformly distributed eigenvalues in [0, 1].",5.3. Determinantal Functions Maximization,[0],[0]
"We set n = 10, = 2.",5.3. Determinantal Functions Maximization,[0],[0]
In Fig. 6 (left) we plot the optimal determinantal objective value and the value achieved by GREEDY.,5.3. Determinantal Functions Maximization,[0],[0]
Fig. 6,5.3. Determinantal Functions Maximization,[0],[0]
"(right) traces the greedy submodularity ratio G. Since the determinantal objective is supermodular, so the approximation guarantee equals to G. We can see that G can reasonably predict the performance of GREEDY.
",5.3. Determinantal Functions Maximization,[0],[0]
"For real-world data, we considered an active set selection task on the CIFAR-107 dataset.",5.3. Determinantal Functions Maximization,[0],[0]
"The first n = 12 images in the test set were used to calculate the covariance matrix with an squared exponential kernel (k(x
i
,x
j
) =
exp( kx i x j k2/h2), h was set to be 1).",5.3. Determinantal Functions Maximization,[0],[0]
"The results in Fig. 7 shows similar results as with the synthetic data.
7https://www.cs.toronto.edu/˜kriz/cifar.",5.3. Determinantal Functions Maximization,[0],[0]
html,5.3. Determinantal Functions Maximization,[0],[0]
"In this section we briefly discuss related work on various notions of non-submodularity and the optimization of nonsubmodular functions (Further details in Appendix F).
",6. Related Work,[0],[0]
Relation to Conforti & Cornuéjols (1984) in deriving approximation guarantees.,6. Related Work,[0],[0]
"In proving Theorem 1 we use the similar proof framework (i.e., utilizing LP formulations to analyze the worst-case approximation ratios of different groups of problem instances) as that in Conforti & Cornuéjols (1984), where they derive guarantees for maximizing submodular functions.",6. Related Work,[0],[0]
"However, since we are proving guarantees for non-submodular functions, the specific techniques on how to manipulate these LPs are different.",6. Related Work,[0],[0]
"Specifically, 1)",6. Related Work,[0],[0]
The building block to construct LPs (Lemma 1) is different;,6. Related Work,[0],[0]
"2) The technique to prove the structure of the LPs (which corresponds to Lemma 2) is significantly different for a submodular function and a nonsubmodular function, and Lemma 2 is the key to investigate the worst-case approximation ratios of different groups of problem instances.",6. Related Work,[0],[0]
3),6. Related Work,[0],[0]
"The specific way to prove Lemma 3 is also different since the constraints of the LPs are different for submodular and non-submodular functions.
",6. Related Work,[0],[0]
Submodularity ratio and curvature.,6. Related Work,[0],[0]
Curvature is typically defined for submodular functions.,6. Related Work,[0],[0]
Sviridenko et al. (2013) present a notion of curvature for monotone nonsubmodular functions.,6. Related Work,[0],[0]
Appendix C provides details of that notion and relates it to our definition.,6. Related Work,[0],[0]
Yoshida (2016) prove an improved approximation ratio for knapsack-constrained maximization of submodular functions with bounded curvature.,6. Related Work,[0],[0]
"Submodularity ratio (Das & Kempe, 2011) is a quantity characterizing how close a function is to being submodular.
",6. Related Work,[0],[0]
Approximate submodularity.,6. Related Work,[0],[0]
"Krause et al. (2008) define approximately submodular functions with parameter ✏ 0 as those functions F that satisfy an approximate diminishing returns property, i.e., 8A ✓ B ✓ V \ v it holds that ⇢
v (A) ⇢",6. Related Work,[0],[0]
v (B) ✏.,6. Related Work,[0],[0]
GREEDY yields a solution with objective F (SK) (1 e 1)F,6. Related Work,[0],[0]
"(⌦⇤) K✏, for maximizing a monotone F s.t.",6. Related Work,[0],[0]
a K-cardinality constraint.,6. Related Work,[0],[0]
Du et al. (2008) study the greedy maximization of nonsubmodular potential functions with restricted submodularity and shifted submodularity.,6. Related Work,[0],[0]
"Restricted submodularity refers to functions which are submodular only over some collection of subsets of V , and shifted submodularity can be viewed as a special case of the approximate diminishing returns as defined above.",6. Related Work,[0],[0]
"Recently, Horel & Singer (2016) study ✏-approximately submodular functions, which arised from their research on “noisy” submodular functions.",6. Related Work,[0],[0]
A function F (·) is ✏-approximately submodular if there exists a submodular function G s.t. (1 ✏)G(S)  ,6. Related Work,[0],[0]
"F (S)  (1 + ✏)G(S), 8S ✓ V .
",6. Related Work,[0],[0]
Weak submodularity.,6. Related Work,[0],[0]
"Borodin et al. (2014) study weakly submodular functions, i.e., montone, nomalized functions F (·) s.t. for any S, T , it holds |T |F (S) + |S|F (T ) |S \T |F (S [T )+ |S",6. Related Work,[0],[0]
[T |F (S \T ).,6. Related Work,[0],[0]
"For a function F (·), we show in Remark 4 that the following two facts do not imply each other: i)",6. Related Work,[0],[0]
F (·) is weakly submodular; ii),6. Related Work,[0],[0]
"The submodularity ratio of F (·) is strictly larger than 0, and its curvature is strictly smaller than 1.
",6. Related Work,[0],[0]
Other notions of non-submodularity.,6. Related Work,[0],[0]
Feige & Izsak (2013) introduce the supermodular degree as a complexity measure for set functions.,6. Related Work,[0],[0]
They show that a greedy algorithm for the welfare maximization problem enjoys an approximation guarantee increasing linearly with the supermodular degree.,6. Related Work,[0],[0]
"Zhou & Spanos (2016) use the submodularity index to characterize the performance of the RANDOMGREEDY algorithm (Buchbinder et al., 2014) for maximizing a non-monotone function.
",6. Related Work,[0],[0]
Optimization of non-submodular functions.,6. Related Work,[0],[0]
"The submodular-supermodular procedure has been proposed to minimize the difference of two submodular functions (Narasimhan & Bilmes, 2005; Iyer & Bilmes, 2012).",6. Related Work,[0],[0]
"Jegelka & Bilmes (2011) present the problem of minimizing “cooperative cuts”, which are non-submodular in general, and propose efficient algorithms for optimization.",6. Related Work,[0],[0]
Kawahara et al. (2015) analyze unconstrained minimization of the sum of a submodular function and a treestructured supermodular function.,6. Related Work,[0],[0]
"Bai et al. (2016) investigate the minimization of the ratio of two submodular functions, which can be solved with bounded approximation factor.",6. Related Work,[0],[0]
We analyzed the guarantees for greedy maximization of non-submodular nondecreasing set functions.,7. Conclusion,[0],[0]
"By combining the (generalized) curvature ↵ and submodularity ratio for generic set functions, we prove the first tight approximation bounds in terms of these definitions for greedily maximizing nondecreasing set functions.",7. Conclusion,[0],[0]
These approximation bounds significantly enlarge the domain where GREEDY has guarantees.,7. Conclusion,[0],[0]
"Furthermore, we theoretically bounded the parameters ↵ and for several non-trivial applications, and validate our theory in various experiments.",7. Conclusion,[0],[0]
"The authors would like to thank Adish Singla, Kfir Y. Levy and Aurelien Lucchi for valuable discussions.",ACKNOWLEDGEMENTS,[0],[0]
This research was partially supported by ERC StG 307036 and the Max Planck ETH Center for Learning Systems.,ACKNOWLEDGEMENTS,[0],[0]
This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.,ACKNOWLEDGEMENTS,[0],[0]
We investigate the performance of the standard GREEDY algorithm for cardinality constrained maximization of non-submodular nondecreasing set functions.,abstractText,[0],[0]
"While there are strong theoretical guarantees on the performance of GREEDY for maximizing submodular functions, there are few guarantees for non-submodular ones.",abstractText,[0],[0]
"However, GREEDY enjoys strong empirical performance for many important non-submodular functions, e.g., the Bayesian A-optimality objective in experimental design.",abstractText,[0],[0]
We prove theoretical guarantees supporting the empirical performance.,abstractText,[0],[0]
Our guarantees are characterized by a combination of the (generalized) curvature ↵ and the submodularity ratio .,abstractText,[0],[0]
"In particular, we prove that GREEDY enjoys a tight approximation guarantee of 1 ↵ (1 e ↵) for cardinality constrained maximization.",abstractText,[0],[0]
"In addition, we bound the submodularity ratio and curvature for several important real-world objectives, including the Bayesian Aoptimality objective, the determinantal function of a square submatrix and certain linear programs with combinatorial constraints.",abstractText,[0],[0]
We experimentally validate our theoretical findings for both synthetic and real-world applications.,abstractText,[0],[0]
Guarantees for Greedy Maximization of Non-submodular Functions with Applications,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1308–1317 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1308",text,[0],[0]
"Acronyms are abbreviations formed from the initial components of words or phrases (e.g., “AI” from “Artificial Intelligence”).",1 Introduction,[0],[0]
"As acronyms can shorten long names and make communications
∗Work done while authors were at Microsoft Research.
more efficient, they are widely used at almost everywhere in enterprises, including notifications, emails, reports and social network posts.",1 Introduction,[0],[0]
Figure 1 shows a sample enterprise social network post.,1 Introduction,[0],[0]
"As we can see, acronyms are frequently used there.
",1 Introduction,[0],[0]
"The enterprise acronym disambiguation task is challenging due to the high ambiguity of acronyms, e.g., “SP” could stand for “Service Pack”, “SharePoint” or “Surface Pro” in Microsoft.",1 Introduction,[0],[0]
"And there is one additional challenge compared with previous disambiguation tasks: in an enterprise document, an acronym could refer
to either an internal meaning (concepts created by the enterprise that may or may not be found outside) or an external meaning (all concepts that are not internal).",1 Introduction,[0],[0]
"For example, regarding the acronym “AI”, “Asset Intelligence” is an internal meaning mainly used only in Microsoft, while “Artificial Intelligence” is an external meaning widely used in public.",1 Introduction,[0],[0]
A good acronym disambiguation system should be able to handle both internal and external meanings.,1 Introduction,[0],[0]
"As we will explain in details, it is important to make such distinction and different strategies are needed for such two cases.
",1 Introduction,[0],[0]
"For internal meanings, there are some previous work on word sense disambiguation (Navigli, 2009) and acronym disambiguation (Feng et al., 2009; Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) on a closed-domain corpus.",1 Introduction,[0],[0]
"The main challenge here is that there are rarely any domain-specific knowledge bases available in enterprises, therefore all the signals for disambiguation (including potential meanings, and their popularity scores, context representations, etc.) need to be mined from plain text.",1 Introduction,[0],[0]
Training data should also be automatically generated to make the system easily scale out to all enterprises.,1 Introduction,[0],[0]
"Compared with previous work, we developed a more comprehensive and advanced set of features in the disambiguation model, and also used a much less restrictive way to discover meaning candidates and training data, so that both precision and recall can be improved.",1 Introduction,[0],[0]
"Moreover, one main limitation of all previous work is that they do not distinguish internal and external meanings.",1 Introduction,[0],[0]
"They merely rely on the enterprise corpus to discover information about external meanings, which we observe is quite ineffective.",1 Introduction,[0],[0]
"The reason is that for popular external meaning like “Artificial Intelligence”, people often directly use its acronym in enterprises without explanation, therefore there is limited information about the connection between the acronym and the external meaning in the enterprise corpus.",1 Introduction,[0],[0]
"On the other hand, there are much more such information available in the public domain, which should be leveraged by the system.
",1 Introduction,[0],[0]
"If we consider utilizing a public knowledge base such as Wikipedia to better handle external meanings of acronyms, the problem becomes very related to the well studied Entity Linking (Ji and Grishman, 2011; Cucerzan, 2007; Dredze et al., 2010; Hoffart et al., 2011; Li et al., 2013, 2016; Ratinov et al., 2011; Shen et al., 2012) prob-
lem, which is to map entity mentions in texts to their corresponding entities in a reference knowledge base (e.g. Wikipedia).",1 Introduction,[0],[0]
"But our disambiguation task is different from the entity linking task, because the system also needs to handle internal meanings which are not covered by any knowledge bases, and ultimately needs to decide whether an acronym refers to an internal meaning or an external meaning.",1 Introduction,[0],[0]
It is nontrivial to combine the information mined from the enterprise corpus and the public knowledge base so that the system can get the best of both worlds.,1 Introduction,[0],[0]
"For instance, we have tried to run an internal disambiguator (leveraging information mined from enterprise corpus) and then resort to a public entity linking system if the internal one’s confidence is low, but the performance is very poor.",1 Introduction,[0],[0]
"Even for external meanings, it is important to leverage signals from the enterprise corpus since the context surrounding them could be quite different from that in the external world, and context is one of the most important factor for disambiguation.",1 Introduction,[0],[0]
"For example, in public world, when people mention “Operating System” they mainly talk about how to install or use it; while within Microsoft, when people mention “Operating System” most of the time they focus on how to design or implement it.
",1 Introduction,[0],[0]
"In this work, we design a novel, end-to-end framework to address all the above challenges.",1 Introduction,[0],[0]
Our framework takes the enterprise corpus and certain public knowledge base as input and produces a high-quality acronym disambiguation system as output.,1 Introduction,[0],[0]
"The models are all trained via distant supervised learning, therefore our system requires no manually labeled training examples and can be easily deployed to any enterprises.",1 Introduction,[0],[0]
The Enterprise Acronym Disambiguation problem is comprised of two sub-problems.,2 Problem Statement,[0],[0]
"The first one is Acronym Meaning Mining (Adar, 2004; Ao and Takagi, 2005; Park and Byrd, 2001; Schwartz and Hearst, 2002; Jain et al., 2007; Larkey et al., 2000; Nadeau and Turney, 2005; Taneva et al., 2013), which aims at mining acronym/meaning pairs from the enterprise corpus.",2 Problem Statement,[0],[0]
"Each meaning m should contain the full name expansion e, popularity score p (indicating how often m is used as the genuine meaning of acronym a) and context words W (i.e. words frequently used in context of the meaning).",2 Problem Statement,[0],[0]
"The popularity score and
context words can provide critical information for making disambiguation decisions.",2 Problem Statement,[0],[0]
"The second one is Meaning Candidate Ranking, whose goal is to rank the candidate meanings associated with the target acronym a and select the genuine meaning m based on the given context.
",2 Problem Statement,[0],[0]
"In this paper we assume the acronyms for disambiguation are provided as input to the system, either by the user or by an existing acronym detection module.",2 Problem Statement,[0],[0]
"We do not try to optimize the performance of acronym detection (e.g. identifying acronyms beyond the simple capitalized rule, or distinguishing cases where a capitalized term is not an acronym but a regular English word, such as “OK”).",2 Problem Statement,[0],[0]
The task of acronym detection is also interesting and important.,2 Problem Statement,[0],[0]
"But due to the space limit, it is beyond the scope of this paper.",2 Problem Statement,[0],[0]
We propose a novel end-to-end framework to solve the Enterprise Acronym Disambiguation problem.,3 Framework,[0],[0]
Our framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,3 Framework,[0],[0]
Figure 2 shows the details of our proposed framework.,3 Framework,[0],[0]
"In the mining module, we will sequentially perform Candidates Generation, Popularity Calculation, Candidates Deduplication and Context Harvesting on the input enterprise corpus.",3 Framework,[0],[0]
The details of these steps will be discussed in Section 4.,3 Framework,[0],[0]
"After mining steps, we will get an acronym/meaning repository storing all the mined acronym/meaning pairs.",3 Framework,[0],[0]
"Feed this repository together with the training data (automatically generated via distant supervision from the enterprise corpus) to the training module, we will get a candidate ranking model, a confidence estimation model and a final selection model.",3 Framework,[0],[0]
These models form the final acronym disambiguator and will be used in the testing module for actual acronym disambiguation.,3 Framework,[0],[0]
"In the testing module, given the target acronym along with some context as input, the system will output the predicted meaning.",3 Framework,[0],[0]
"Note that the mining and training module run offline once for the entire corpus or periodically when the corpus update, while the testing can be run online repeatedly for processing new documents.",3 Framework,[0],[0]
"As there is no reference dictionary or knowledge base available in enterprise telling us the potential
meanings of acronyms, we have to extract them from plain text.",4.1 Candidates Generation,[0],[0]
We propose a strategy called Hybrid Generation to balance extraction accuracy and coverage.,4.1 Candidates Generation,[0],[0]
"Namely, we treat a phrase as a meaning candidate for an acronym if: (1) the initial letters of the phrase match the acronym and the phrase and the acronym co-occur in at least one document in the enterprise corpus; or (2) it is a valid candidate for the acronym in public knowledge bases (e.g. Wikipedia).",4.1 Candidates Generation,[0],[0]
The insight of this strategy is that the valid candidates missed by condition (1) are mainly public meanings which can be found in public knowledge bases.,4.1 Candidates Generation,[0],[0]
With this strategy we can make our system understand both the internal world and the external world.,4.1 Candidates Generation,[0],[0]
"As mentioned in Section 2, for each candidate meaning, we need to calculate its popularity score, which reveals how often the candidate meaning is used as the genuine meaning of the acronym.",4.2 Popularity Calculation,[0],[0]
"In previous research on Entity Linking, popularity is calculated as the fraction of times a candidate being the target page for an anchor text in a reference knowledge base (e.g. Wikipedia).",4.2 Popularity Calculation,[0],[0]
"However, in enterprises, we do not have a knowledge base with anchor links.",4.2 Popularity Calculation,[0],[0]
Thus we cannot calculate popularity in the same way.,4.2 Popularity Calculation,[0],[0]
"Here we propose to calculate two types of popularity to mimic the effect.
1.",4.2 Popularity Calculation,[0],[0]
Marginal Popularity.,4.2 Popularity Calculation,[0],[0]
"MP (mi) = Count(mi)∑n j=1Count(mj) , (1)
where m1, m2, . . .",4.2 Popularity Calculation,[0],[0]
", mn are the meaning candidates of acronym a and Count(mi) is the number of occurrences for mi in the corpus.
2.",4.2 Popularity Calculation,[0],[0]
"Conditional Popularity.
",4.2 Popularity Calculation,[0],[0]
"CP (mi) = Count(mi, a)∑n j=1Count(mj , a) , (2) where m1, m2, . .",4.2 Popularity Calculation,[0],[0]
"., mn are the meaning candidates of acronym a and Count(mi, a) is the number of document-level cooccurrences for mi and a in the corpus.
",4.2 Popularity Calculation,[0],[0]
Conditional Popularity can more reasonably reveal how often the acronym is used to represent each meaning candidate.,4.2 Popularity Calculation,[0],[0]
"However, due to the data sparsity issue in enterprises, many valid candidates may get zero value for conditional popularity since they may never co-occur with the acronyms in the enterprise corpus.",4.2 Popularity Calculation,[0],[0]
The Marginal Popularity does not have this problem since it is calculated from the raw counts of the candidates.,4.2 Popularity Calculation,[0],[0]
"Yet on the other hand, high marginal popularity score does not necessarily indicate high correlation between the candidate and the acronym.",4.2 Popularity Calculation,[0],[0]
"It is unclear how to combine the two scores into one popularity score, so we use both of them as features in the disambiguation model.",4.2 Popularity Calculation,[0],[0]
"In enterprises, people often create many variants (including abbreviations, plurals or even misspellings) for the same meaning, therefore many mined meaning candidates are actually equivalent.",4.3 Candidates Deduplication,[0],[0]
"For example, for the meaning “Certificate Authority” of the acronym “CA”, the variants include “Cert Auth”, “Certificate Authorities” and many others.",4.3 Candidates Deduplication,[0],[0]
It is important to deduplicate these variants before sending them to the disambiguation module.,4.3 Candidates Deduplication,[0],[0]
The deduplication helps aggregate disambiguation evidences and reduce noises.,4.3 Candidates Deduplication,[0],[0]
We design several heuristic rules1 to perform the deduplication.,4.3 Candidates Deduplication,[0],[0]
Experiments show that the rules can accurately group the variants together.,4.3 Candidates Deduplication,[0],[0]
"After grouping, we sort the variants within the same group based on their marginal popularity.",4.3 Candidates Deduplication,[0],[0]
The candidate with the largest marginal popularity is selected as the canonical candidate for the group.,4.3 Candidates Deduplication,[0],[0]
Other variants in the group will be deleted from the candidate list and their popularity scores will be aggregated to the canonical candidate.,4.3 Candidates Deduplication,[0],[0]
We maintain a table to record the variants for each canonical candidate.,4.3 Candidates Deduplication,[0],[0]
"In this step, we aim to harvest context words for each meaning candidate.",4.4 Context Harvesting,[0],[0]
These context words could be used to calculate context similarity with the query context.,4.4 Context Harvesting,[0],[0]
"For each meaning candidate m, we put its canonical form and all its variants (from the variants table in Section 4.3) into set S. Then we scan the enterprise corpus, each time we find a match of any e ∈ S, we harvest the words in a
1Due to space limitations, the detailed rules are omitted.",4.4 Context Harvesting,[0],[0]
"Example rules are “word overlap percentage after stemming > 0.8”, “corresponding component words share same prefix”.
width-W word window surrounding e as the context words of m. In our experiments we set window size as 30 after trying to vary the window size from 10 to 50 and finding 30 gives the best result.
",4.4 Context Harvesting,[0],[0]
"As mentioned before, some popular public meanings might be mentioned very rarely by their full names in the enterprise corpus since people directly use their acronyms most of the time.",4.4 Context Harvesting,[0],[0]
"Therefore, the above context harvesting process can only get very few context words for those public meanings.",4.4 Context Harvesting,[0],[0]
"To alleviate this, for each public meaning we add its Wikipedia page’s content as complementary context.",4.4 Context Harvesting,[0],[0]
"By doing so, we ensure almost all valid candidates get a reasonable amount of context words.",4.4 Context Harvesting,[0],[0]
We first train a candidate ranking model to rank candidates with respect to the likelihood of being the genuine meaning for the target acronym.,5.1 Candidate Ranking,[0],[0]
"In order to train a robust ranking model, we need to get adequate amount of labeled training data.",5.1.1 Training Data Generation,[0],[0]
"Manually labeling is obviously too expensive and it requires a lot of domain knowledge, which severely limits our framework’s generalization capability.",5.1.1 Training Data Generation,[0],[0]
"To tackle this problem, we propose to automatically generate training data via distant supervision.",5.1.1 Training Data Generation,[0],[0]
"The intuition is that since acronyms and the corresponding meanings are semantically equivalent, people use them interchangeably in enterprises.",5.1.1 Training Data Generation,[0],[0]
"Therefore we can fetch documents containing the meaning, replace the meaning with the corresponding acronym and treat the meaning as ground truth.",5.1.1 Training Data Generation,[0],[0]
Figure 3 shows an example of this automatic training data generation process.,5.1.1 Training Data Generation,[0],[0]
Any learning-to-rank algorithms can be used here.,5.1.2 Training Algorithm,[0],[0]
"In our system we utilize the LambdaMART algorithm (Burges, 2010) to train the model.",5.1.2 Training Algorithm,[0],[0]
Now we explain the features we developed for the candidate ranking model.,5.1.3 Features,[0],[0]
"First, we have the Marginal Popularity score and Conditional Popularity score as two context-independent features, which could compensate for each other.",5.1.3 Features,[0],[0]
"However, as discussed in the previous section, some popular public meanings (e.g., “Artificial Intelligence”) can be rarely mentioned in enterprise corpus by their full names, therefore both their marginal popularity score and conditional popularity score can be very low.",5.1.3 Features,[0],[0]
"To address this, we add a third feature called Wiki Popularity, which is calculated from Wikipedia anchor texts to capture how often an acronym refers to a public meaning in Wikipedia.",5.1.3 Features,[0],[0]
The fourth feature we adopt is Context Similarity.,5.1.3 Features,[0],[0]
We convert the harvested context for the meaning and the query context of the target acronym into TFIDF vectors and then compute their cosine similarity2.,5.1.3 Features,[0],[0]
"We also include two features (i.e. LeftNeighborScore and RightNeighborScore) to capture the effect of the immediate neighboring words, which are more important than further context words since immediate words could form phrases with the acronym.",5.1.3 Features,[0],[0]
"For example, if we see an acronym “SP” followed by the word “2”, then likely it stands for “Service Pack”.",5.1.3 Features,[0],[0]
"However, if we see “SP” followed by “2003”, then probably its genuine meaning is “SharePoint”.",5.1.3 Features,[0],[0]
The last feature we use is FullNamePercentage.,5.1.3 Features,[0],[0]
This feature is defined as the percentage of the meaning candidate’s component words appearing in the context of the target acronym.,5.1.3 Features,[0],[0]
Table 1 summarizes the features used to train the candidate ranking model.,5.1.3 Features,[0],[0]
"After getting the ranking results, we propose to apply a confidence estimation step to decide whether to trust the top ranked answer.",5.2 Confidence Estimation,[0],[0]
There are two motivations behind.,5.2 Confidence Estimation,[0],[0]
"First, our candidate generation approach is not perfect, therefore we could encounter cases in which the genuine meaning is not in our candidates.",5.2 Confidence Estimation,[0],[0]
"For such cases, the top ranked answer is obviously incorrect.",5.2 Confidence Estimation,[0],[0]
"Second, our training data is biased towards the internal meanings since external meanings may rarely appear with full names.
",5.2 Confidence Estimation,[0],[0]
"2One popular alternative to measure context similarity is using word embeddings (Mikolov et al., 2013; Li et al., 2015).",5.2 Confidence Estimation,[0],[0]
"In our system we experimented replacing TFIDF cosine similarity with word embedding similarity, or adding word embedding similarity as an additional feature, but both hurt the disambiguation accuracy.",5.2 Confidence Estimation,[0],[0]
"So we only included the TFIDF cosine similarity as the context similarity feature in our system.
",5.2 Confidence Estimation,[0],[0]
"As a result, the learned ranking model may lack the capability to properly rank the external meanings.",5.2 Confidence Estimation,[0],[0]
"In such cases, we would better have the system return nothing rather than directly provide a wrong answer to mislead the user.",5.2 Confidence Estimation,[0],[0]
"In this step, we train a confidence estimation model, which will estimate the top result’s confidence.",5.2 Confidence Estimation,[0],[0]
"Similar to the ranker training, here the training data is also automatically generated.",5.2.1 Training Data Generation,[0],[0]
"We run the learned ranker on some distant labeled data (generated from a different corpus), and then check if the top ranked answer is correct or not.",5.2.1 Training Data Generation,[0],[0]
"If it is correct, we generate a positive training example; otherwise we make a negative training example.",5.2.1 Training Data Generation,[0],[0]
Any classification algorithms can be used here.,5.2.2 Training Algorithm,[0],[0]
"In our system we utilize the MART boosted tree algorithm (Friedman, 2000) to train the model.",5.2.2 Training Algorithm,[0],[0]
We design 7 features (summarized in Table 2) to train the confidence estimation model.,5.2.3 Features,[0],[0]
"There are two intuitions behind: (1) If the top-ranked answer’s ranking score is very small, or the topranked answer’s score is close to the secondranked answer’s score, then the ranking is not very confident; (2) If the acronym has a dominating candidate in the public domain (e.g., “Personal Computer” is the dominating candidate for “PC”), and the candidates’ Wiki popularity distribution is significantly different from their marginal/conditional popularity distributions, then the ranker’s output is not very confident.",5.2.3 Features,[0],[0]
"The first intuition covers the first 3 features, while the second intuition covers the last 4 features.",5.2.3 Features,[0],[0]
We have discussed that one particular motivation for confidence estimation is that the candidate ranking stage has some bias so it does not always rank public meanings at top when they are correct.,5.3 Final Selection,[0],[0]
"Therefore, assuming the confidence estimation model can remove incorrect top-ranked result, we still need one additional step to decide if any public meaning is correct, which we call a final selection model.",5.3 Final Selection,[0],[0]
"In this step, we determine whether to return the most popular public meaning (based on Wiki Popularity) as the final answer, and this step is only triggered when the confidence estimator judges that the ranking result is unconfident.
",5.3 Final Selection,[0],[0]
The goal of the final selection model is similar to that of the confidence estimation model.,5.3 Final Selection,[0],[0]
"In confidence estimation, we judge whether the topranked answer is correct; while in final selection, we check whether the most popular external meaning is correct.",5.3 Final Selection,[0],[0]
"Thanks to this similarity, we can reuse the data, features and training algorithm in confidence estimation model.",5.3 Final Selection,[0],[0]
"We take the same training data in Section 5.2.1 and update the labels correspondingly: if the genuine answer is the most popular external meaning, we generate a positive example; otherwise we make a negative one.",5.3 Final Selection,[0],[0]
We use both the Microsoft Answer Corpus (MAC) and the Microsoft Yammer Corpus (MYC) as the mining corpus.,6.1.1 Mining and Training Corpus,[0],[0]
These corpus are kindly shared to us by Microsoft for research purpose.,6.1.1 Mining and Training Corpus,[0],[0]
MAC contains 0.3 million web pages from a Microsoft internal question answering forum.,6.1.1 Mining and Training Corpus,[0],[0]
MYC is consisted of 6.8 million posts from Microsoft’s Yammer social network.,6.1.1 Mining and Training Corpus,[0],[0]
"In total, our mining module harvested 5287 acronyms and 17258 meaning candidates from this joint corpus.
",6.1.1 Mining and Training Corpus,[0],[0]
"For model training, the confidence estimation model and final selection model need to be trained on a different corpus than the candidate ranking model.",6.1.1 Mining and Training Corpus,[0],[0]
"So we train the candidate ranking model
on MAC, with 12500 training examples being automatically generated; and train the confidence estimation and final selection model on MYC, with 40000 training instances being generated.",6.1.1 Mining and Training Corpus,[0],[0]
We prepared four datasets3 for evaluation purposes.,6.1.2 Evaluation Datasets,[0],[0]
The first one Manual is obtained from the recent pages of Microsoft answer forum.,6.1.2 Evaluation Datasets,[0],[0]
Note these pages are disjoint from those used as mining/training corpus.,6.1.2 Evaluation Datasets,[0],[0]
We randomly sampled 300 pages and filtered out pages which do not contain ambiguous acronyms.,6.1.2 Evaluation Datasets,[0],[0]
"After filtering, 240 test cases were left and we manually labeled them.
",6.1.2 Evaluation Datasets,[0],[0]
The second one Distant is generated via distant labeling on Microsoft Office365 documents.,6.1.2 Evaluation Datasets,[0],[0]
We sampled 2000 documents which contain at least one occurrence of a meaning candidate.,6.1.2 Evaluation Datasets,[0],[0]
Then we replaced the meanings with the corresponding acronyms and treat the meanings as ground truths.,6.1.2 Evaluation Datasets,[0],[0]
"We manually checked through this dataset to remove some bad cases (e.g., “AS” for “App Store”).",6.1.2 Evaluation Datasets,[0],[0]
"This resulted in a test set of 1949 test cases.
",6.1.2 Evaluation Datasets,[0],[0]
"Comparing the Manual dataset with the Distant dataset, the Manual one, though in smaller size, can more accurately evaluate the system performance, since the target acronyms in it are sampled from the real distribution, while in the Distant dataset acronyms are artificially generated from
3Due to data confidentiality issue, we were unable to directly release these datasets.
",6.1.2 Evaluation Datasets,[0],[0]
randomly sampled meanings.,6.1.2 Evaluation Datasets,[0],[0]
We also want to compare our method with the state-of-the-art Entity Linking (EL) systems based on public knowledge bases such as Wikipedia.,6.1.2 Evaluation Datasets,[0],[0]
"However, it is unfair to directly compare as most enterprise specific meanings are unknown to them.",6.1.2 Evaluation Datasets,[0],[0]
"Therefore, we need to only consider cases where the true meaning is a public meaning covered by both our system and the compared system.",6.1.2 Evaluation Datasets,[0],[0]
"By filtering the distant dataset from Office365, we get the third dataset JoinW (1659 test cases) for comparing with the Wikifier (Ratinov et al., 2011), and the fourth dataset JoinA (237 test cases) for comparing with AIDA (Hoffart et al., 2011).",6.1.2 Evaluation Datasets,[0],[0]
"We compare the following ablations of our system, to illustrate the effectiveness of the features and components.
",6.2.1 Ablations of Our System,[0],[0]
"• Internal Popularity (IP): Only the internal popularity features (i.e., marginal popularity and conditional popularity).
",6.2.1 Ablations of Our System,[0],[0]
"• Popularity (P): The internal popularity features plus Wiki popularity features.
",6.2.1 Ablations of Our System,[0],[0]
• Popularity+Context (P+C):,6.2.1 Ablations of Our System,[0],[0]
"The popularity features plus context similarity feature.
",6.2.1 Ablations of Our System,[0],[0]
"• Popularity+Context+Neighbbor (P+C+N): The popularity features, context similarity feature and immediate neighbor features.
",6.2.1 Ablations of Our System,[0],[0]
• Popularity+ Context+,6.2.1 Ablations of Our System,[0],[0]
"Neighbbor+ Fullname (a.k.a. Candidate Ranker, or CR): Using all the features in candidate ranking module.
",6.2.1 Ablations of Our System,[0],[0]
"• Candidate Ranker + Confidence Estimator (CR+ CE): Using the candidate ranking model plus the confidence estimation model.
",6.2.1 Ablations of Our System,[0],[0]
"• Candidate Ranker + Confidence Estimator + Final Selector (a.k.a. Acronym Disambiguator, or AD):",6.2.1 Ablations of Our System,[0],[0]
"Using the candidate ranking model, the confidence estimation model and the final selection model.",6.2.1 Ablations of Our System,[0],[0]
Full version of our system.,6.2.1 Ablations of Our System,[0],[0]
"We also compare our method with two state-ofthe-art Entity Linking (EL) systems.
",6.2.2 State-of-the-art EL Systems,[0],[0]
"• Wikifier: a popular EL system using machine learning to combine various features together.
• AIDA: a robust EL system using mention-entity graph to find the best mention-entity mapping.",6.2.2 State-of-the-art EL Systems,[0],[0]
We first conduct experiments to evaluate the quality of the acronym/meaning pairs harvested through our offline mining module.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"Out of the 17258 mined pairs, we randomly sampled 2000 of them and asked 5 domain experts to manually check their validness.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
An acronym/meaning pair is considered as valid if the majority of the experts think the acronym is indeed used to abbreviate the meaning.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"For example, (AS, Analysis Service) is a valid pair, but (AS, App Store) is considered as invalid because people will not actually use AS to represent App Store.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"Among the sampled 2000 pairs, 94.5% are labeled as valid, indicating our offline mining module can accurately extract acronym/meaning pairs from enterprise corpus.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"It is hard to precisely evaluate the coverage/recall of our mining method, since it is very difficult to obtain the complete meaning list for a given acronym.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"To get a rough idea, we randomly picked up 100 acronyms and asked the 5 domain experts to enumerate the valid meanings for these acronyms.",6.3 Quality of Mined Acronyms/Meanings,[0],[0]
In total we got 230 valid meanings and all of them are covered by the mined pairs.,6.3 Quality of Mined Acronyms/Meanings,[0],[0]
"We first conduct experiments to evaluate the disambiguation performance of our ranking model, and compare the helpfulness of the features used in the model.",6.4 Disambiguation Performance,[0],[0]
"Figure 4 shows the precision (i.e., percentage of correctly disambiguated cases among all predicted cases), recall (i.e., percentage of correctly disambiguated cases among all test cases) and F1 (i.e., harmonic mean of precision and recall) of the compared methods on the Manual dataset and the Distant dataset.",6.4 Disambiguation Performance,[0],[0]
"In terms of the helpfulness of the features, the context similarity feature and the immediate neighbor features contribute most to the performance gain.",6.4 Disambiguation Performance,[0],[0]
"Other features are less helpful, yet still bring improvements to the overall performance.
",6.4 Disambiguation Performance,[0],[0]
Next we conduct experiments to illustrate the effectiveness of the confidence estimation module and the final selection module in our system.,6.4 Disambiguation Performance,[0],[0]
"Figure 5 shows the precision, recall and F1 of the compared system configurations on the Manual and Distant dataset.",6.4 Disambiguation Performance,[0],[0]
"As can be seen, the confidence estimation module can improve precision at the cost of hurting recall.",6.4 Disambiguation Performance,[0],[0]
"Fortunately, the final selection module can recover some recall losses without sacrificing too much on precision.",6.4 Disambiguation Performance,[0],[0]
"In
terms of the F1 measure, the final system achieves the best performance.
",6.4 Disambiguation Performance,[0],[0]
"Note that the ablation P+C naturally corresponds to the existing acronym disambiguation approaches (Feng et al., 2009; Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) mainly relying on context words and domain specific resources.",6.4 Disambiguation Performance,[0],[0]
These approaches do not specifically distinguish internal and external meanings.,6.4 Disambiguation Performance,[0],[0]
"They merely rely on the internal corpus to discover information about external meanings, which is quite ineffective in the scenario of enterprise acronym disambiguation (as discussed in Section 1).",6.4 Disambiguation Performance,[0],[0]
"In comparison, our system (AD) is able to leverage public resources together with the internal corpus to better handle the problem and therefore significantly outperforms them.",6.4 Disambiguation Performance,[0],[0]
We also compare our system (AD) with two stateof-the-art Entity Linking (EL) systems: Wikifier and AIDA.,6.5 Comparison with EL Systems,[0],[0]
"As explained in Section 6.1.2, we
made two datasets (i.e., JoinW and JoinA) for fair comparisons.",6.5 Comparison with EL Systems,[0],[0]
"Figure 6(a) and Figure 6(b) present the comparison of our AD system against Wikifer and AIDA, respectively.",6.5 Comparison with EL Systems,[0],[0]
"As we can see from the figures, AD significantly outperforms both Wikifier and AIDA on all three measures.",6.5 Comparison with EL Systems,[0],[0]
"The reason is that even for public meanings (e.g., Operating System) indexed by Wikifier and AIDA, the usage of them could be quite different in enterprises (e.g., inside Microsoft people talk more about designing Operating System rather than how to install it).",6.5 Comparison with EL Systems,[0],[0]
"Wikifier and AIDA utilize information from public knowledge bases (e.g., Wikipedia) to generate features, therefore can hardly capture such enterprisespecific signals.",6.5 Comparison with EL Systems,[0],[0]
"In contrast, our AD system mines disambiguation features directly from the enterprise corpus and utilizes them together with the public signals.",6.5 Comparison with EL Systems,[0],[0]
"As a result, it can more accurately represent the characteristics of the enterprise and lead to much better disambiguation performances.",6.5 Comparison with EL Systems,[0],[0]
Acronym meaning discovery has received a lot of attentions in vertical domains (mainly in biomedical).,7 Related Work,[0],[0]
"Most of the proposed approaches (Adar, 2004; Ao and Takagi, 2005; Park and Byrd, 2001; Schwartz and Hearst, 2002; Wren et al., 2002) utilized generic rules or text patterns (e.g. brackets, colons) to discover acronym meanings.",7 Related Work,[0],[0]
"These methods are usually based on the assumption that
acronyms are co-mentioned with the corresponding meanings in the same document.",7 Related Work,[0],[0]
"However, in enterprises, this assumption rarely holds.",7 Related Work,[0],[0]
"Enterprises themselves are closed ecosystems, so it is very common for people to define the acronyms somewhere and use them elsewhere.",7 Related Work,[0],[0]
"As a result, such methods cannot be used for acronym meaning discovery in enterprises.
",7 Related Work,[0],[0]
"Recently, there have been a few works (Jain et al., 2007; Larkey et al., 2000; Nadeau and Turney, 2005; Taneva et al., 2013) on automatically mining acronym meanings by leveraging Web data (e.g., query sessions, click logs).",7 Related Work,[0],[0]
"However, it is hard to apply them directly to enterprises, since most data in enterprises are raw text and therefore the query sessions/click logs are rarely available.
",7 Related Work,[0],[0]
"Acronym disambiguation can be seen as a special case for the Entity Linking (EL) (Ji and Grishman, 2011; Dredze et al., 2010) problem.",7 Related Work,[0],[0]
Approaches that link entity mentions to Wikipedia date back to Bunescu et.,7 Related Work,[0],[0]
"al’s work (Bunescu and Paşca, 2006).",7 Related Work,[0],[0]
They computed the cosine similarity between the text around the mention and the entity candidate’s Wikipedia page.,7 Related Work,[0],[0]
The referent entity with the maximum similarity score is selected as the disambiguation result.,7 Related Work,[0],[0]
"Cucerzan’s work (Cucerzan, 2007) is the first one to realize the effectiveness of using topical coherence to globally perform EL.",7 Related Work,[0],[0]
"In that work, the topical coherence between the referent entity candidate and other entities within the same context is calculated based on their overlaps in categories and incoming links in Wikipedia.",7 Related Work,[0],[0]
"Recently, several methods (Hoffart et al., 2011; Li et al., 2013, 2016; Ratinov et al., 2011; Shen et al., 2012; Cheng and Roth, 2013) also tried to enrich “context similarity” and “topical coherence” using hybrid strategies.",7 Related Work,[0],[0]
Shen et.,7 Related Work,[0],[0]
"al (Shen et al., 2015) provided a comprehensive survey for the techniques used in EL.",7 Related Work,[0],[0]
"However, these EL techniques cannot be used for acronym disambiguation in enterprises, since most enterprise meanings are not covered by public knowledge bases, and there are rarely any domain-specific knowledge bases available in enterprises.",7 Related Work,[0],[0]
"Automatic knowledge base construction (Suchanek et al., 2013) is promising, but the quality is far from applicable.",7 Related Work,[0],[0]
"Moreover, the structural information (e.g. entity taxonomy, crossdocument hyperlinks) within Wikipedia, is rarely available in enterprises.
",7 Related Work,[0],[0]
"Most of the previous work (Feng et al., 2009;
Pakhomov et al., 2005; Pustejovsky et al., 2001; Stevenson et al., 2009; Yu et al., 2006) on acronym disambiguation heavily rely on context words and domain specific resources.",7 Related Work,[0],[0]
"In comparison, our method explored a more comprehensive set of domain-independent features.",7 Related Work,[0],[0]
"Moreover, our method used a much less restrictive way to discover meaning candidates and training data, which is far more general than the methods relying on strict definition patterns (Schwartz and Hearst, 2002).",7 Related Work,[0],[0]
Another particular limitation of all these previous work is that they do not distinguish internal and external meanings.,7 Related Work,[0],[0]
"They merely rely on the internal corpus to discover information about external meanings, which is quite ineffective.",7 Related Work,[0],[0]
"In this paper, we studied the Acronym Disambiguation for Enterprises problem.",8 Conclusions,[0],[0]
"We proposed a novel, end-to-end framework to solve this problem.",8 Conclusions,[0],[0]
Our framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,8 Conclusions,[0],[0]
"The disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples.",8 Conclusions,[0],[0]
"Different from all the previous acronym disambiguation approaches, our system is capable of accurately resolving acronyms to both enterprise-specific meanings and public meanings.",8 Conclusions,[0],[0]
Experimental results on Microsoft enterprise data demonstrated that our system can effectively construct acronym/meaning repositories from scratch and accurately disambiguate acronyms to their meanings with over 90% precision.,8 Conclusions,[0],[0]
"Furthermore, our proposed framework can be easily deployed to any enterprises without requiring any domain knowledge.",8 Conclusions,[0],[0]
Acronyms are abbreviations formed from the initial components of words or phrases.,abstractText,[0],[0]
"In enterprises, people often use acronyms to make communications more efficient.",abstractText,[0],[0]
"However, acronyms could be difficult to understand for people who are not familiar with the subject matter (new employees, etc.), thereby affecting productivity.",abstractText,[0],[0]
"To alleviate such troubles, we study how to automatically resolve the true meanings of acronyms in a given context.",abstractText,[0],[0]
Acronym disambiguation for enterprises is challenging for several reasons.,abstractText,[0],[0]
"First, acronyms may be highly ambiguous since an acronym used in the enterprise could have multiple internal and external meanings.",abstractText,[0],[0]
"Second, there are usually no comprehensive knowledge bases such as Wikipedia available in enterprises.",abstractText,[0],[0]
"Finally, the system should be generic to work for any enterprise.",abstractText,[0],[0]
In this work we propose an end-to-end framework to tackle all these challenges.,abstractText,[0],[0]
The framework takes the enterprise corpus as input and produces a high-quality acronym disambiguation system as output.,abstractText,[0],[0]
"Our disambiguation models are trained via distant supervised learning, without requiring any manually labeled training examples.",abstractText,[0],[0]
"Therefore, our proposed framework can be deployed to any enterprise to support highquality acronym disambiguation.",abstractText,[0],[0]
Experimental results on real world data justified the effectiveness of our system.,abstractText,[0],[0]
Guess Me if You Can: Acronym Disambiguation for Enterprises,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 366–376 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1034",text,[0],[0]
"With the rapid growth of products reviews at the web, it has become common for people to read reviews before making a purchase decision.",1 Introduction,[0],[0]
The reviews usually contain abundant consumers’ personal experiences.,1 Introduction,[0],[0]
It has led to a significant influence on financial gains and fame for businesses.,1 Introduction,[0],[0]
"Existing studies have shown that an extra halfstar rating on Yelp causes restaurants to sell out 19% points more frequently (Anderson and Magruder, 2012), and a one-star increase in Yelp rating leads to a 5-9 % increase in revenue (Luca, 2011).",1 Introduction,[0],[0]
"This, unfortunately, gives strong incentives for imposters (called spammers) to game the system.",1 Introduction,[0],[0]
They post fake reviews or opinions (called review spam) to promote or to discredit some targeted products and services.,1 Introduction,[0],[0]
"The news from BBC has shown that around 25% of Yelp reviews could be fake.1 Therefore, it is urgent to detect review s-
1http://www.bbc.com/news/technology-24299742
pam, to ensure that the online review continues to be trusted.
",1 Introduction,[0],[0]
Jindal and Liu (2008) make the first step to detect review spam.,1 Introduction,[0],[0]
Most efforts are devoted to exploring effective linguistic and behavioral features by subsequent work to distinguish such spam from the real reviews.,1 Introduction,[0],[0]
"However, to notice such patterns or form behavioral features, developers should take a long time to observe the data, because the features are based on statistics.",1 Introduction,[0],[0]
"For instance, the feature activity window proposed by Mukherjee et al. (2013c) is to measure the activity freshness of reviewers.",1 Introduction,[0],[0]
It usually takes several months to count the difference of timestamps between the last and first reviews for reviewers.,1 Introduction,[0],[0]
"When the features show themselves finally, some major damages might have already been done.",1 Introduction,[0],[0]
"Thus, it is important to design algorithms that can detect review spam as soon as possible, ideally, right after they are posted by the new reviewers.",1 Introduction,[0],[0]
"It is a coldstart problem which is the focus of this paper.
",1 Introduction,[0],[0]
"In this paper, we assume that we must identify fake reviews immediately when a new reviewer posts just one review.",1 Introduction,[0],[0]
"Unfortunately, it is very difficult because the available information for detecting fake reviews is very poor.",1 Introduction,[0],[0]
Traditional behavioral features based on the statistics can only work well on users’ abundant behaviors.,1 Introduction,[0],[0]
"The more behavioral information obtained, the more effective the traditional behavioral features are (see experiments in Section 3 ).",1 Introduction,[0],[0]
"In the scenario of cold-start, a new reviewer only has a behavior: post a review.",1 Introduction,[0],[0]
"As a result, we can not get effective behavioral features from the data.",1 Introduction,[0],[0]
"Although, the linguistic features of reviews do not need to take much time to form, Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, where we also obtain the same observation (the details are shown in Section 3).
",1 Introduction,[0],[0]
"366
Therefore, the main difficulty of the cold-start spam problem is that there are no sufficient behaviors of the new reviewers for constructing effective behavioral features.",1 Introduction,[0],[0]
"Nevertheless, there is ample textual and behavioral information contained in the abundant reviews posted by the existing reviewers (Figure 1).",1 Introduction,[0],[0]
We could employ behavioral information of existing similar reviewers to a new reviewer to approximate his behavioral features.,1 Introduction,[0],[0]
"We argue that a reviewer’s individual characteristics such as background information, motivation, and interactive behavior style have a great influence on a reviewer’s textual and behavioral information.",1 Introduction,[0],[0]
So the textual information and the behavioral information of a reviewer are correlated with each other (similar argument in Li et al. (2016)).,1 Introduction,[0],[0]
"For example, the students of the college are likely to choose the youth hostel during summer vacation and tend to comment the room price in their reviews.",1 Introduction,[0],[0]
"But the financial analysts on a business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews.
",1 Introduction,[0],[0]
"To augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews.",1 Introduction,[0],[0]
"There are several ways to model the textual information of the review spam, such as Unigram (Mukherjee et al., 2013c), POS (Ott et al., 2011) and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003).",1 Introduction,[0],[0]
"We employ the CNN (Convolutional Neural Network) to model the review text, which has been proved that it can capture complex global semantic information that is difficult to express using traditional discrete manual features (Ren and Zhang, 2016).",1 Introduction,[0],[0]
Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer.,1 Introduction,[0],[0]
"An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s behavioral features as the new reviewers’ features (detailed in Section 5.3).",1 Introduction,[0],[0]
"However, there is abundant behavioral information in the review graph (Figure 1), it is difficult for the traditional discrete manual behavioral features to record the global behavioral information (Wang et al., 2016).",1 Introduction,[0],[0]
"Moreover, the traditional features can not capture the reviewer’s individual characteristics, because there is no explicit characteristic tag available in the review system (experi-
ments in Section 5.3).",1 Introduction,[0],[0]
"So, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem.",1 Introduction,[0],[0]
"By encoding the review graph structure (Figure 1), the proposed model can record the global footprints of the existing reviewers in an unsupervised way, and further record the reviewers’ latent characteristic information in the footprints.",1 Introduction,[0],[0]
The jointly learnt review embeddings can model the correlation of the reviewers’ textual and behavioral information.,1 Introduction,[0],[0]
"When a new reviewer posts a review, the proposed model can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings.",1 Introduction,[0],[0]
"Finally, the embeddings of the new review are fed into a classifier to identify whether it is spam or not.
",1 Introduction,[0],[0]
"In summary, our major contributions include: • To our best knowledge, this is the first work
that explores the cold-start problem in review spam detection.",1 Introduction,[0],[0]
We qualitatively and quantitatively prove that the traditional linguistic and behavioral features are not effective enough in detecting review spam for the coldstart task.,1 Introduction,[0],[0]
• We propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for the cold-start spam detection task.,1 Introduction,[0],[0]
It is an unsupervised distributional representation model which can learn from large scale unlabeled review data.,1 Introduction,[0],[0]
• Experimental results on two domains (hotel and restaurant) give good confidence that the proposed model performs effectively in the cold-start spam detection task.,1 Introduction,[0],[0]
Jindal and Liu (2008) make the first step to detect review spam.,2 Related Work,[0],[0]
"Subsequent work devoted most
efforts to explore effective features and spammerlike clues.
",2 Related Work,[0],[0]
Linguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features.,2 Related Work,[0],[0]
Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage.,2 Related Work,[0],[0]
Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews.,2 Related Work,[0],[0]
The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015).,2 Related Work,[0],[0]
Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis.,2 Related Work,[0],[0]
Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features.,2 Related Work,[0],[0]
"Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective than the RNN and the traditional discrete manual linguistic features.",2 Related Work,[0],[0]
"Hovy (2016) used N-gram generative models to produce reviews and evaluated their effectiveness.
",2 Related Work,[0],[0]
Behavioral features: Lim et al. (2010) analyzed reviewers’ rating behavioral features; Jindal et al. (2010) identified unusual review patterns which can represent suspicious behaviors of reviews; Li et al. (2011) proposed a two-view semisupervised co-training method base on behavioral features.,2 Related Work,[0],[0]
Feng et al. (2012b) study the distributions of individual spammers’ behaviors.,2 Related Work,[0],[0]
The group spammers’ behavioral features were studied in Mukherjee et al. (2012).,2 Related Work,[0],[0]
"Temporal patterns of spammers were investigated by Xie et al. (2012), Fei et al. (2013); Li et al. (2015) explored the temporal and spatial patterns.",2 Related Work,[0],[0]
"The review graph was analyzed by Wang et al. (2011), Akoglu et al. (2013); Mukherjee et al. (2013a) studied the spamicity of reviewers.",2 Related Work,[0],[0]
"Mukherjee et al. (2013c), Mukherjee et al. (2013b) proved that reviewers’ behavioral features are more effective than reviews’ linguistic features for detecting review spam.",2 Related Work,[0],[0]
"Based on this conclusion, recently, researchers (Rayana and Akoglu, 2015; KC and Mukherjee, 2016) have put more efforts in employing reviewers’ behavioral features for de-
tecting review spam, the intuition behind which is to capture the reviewers’ actions and supposes that those reviews written with spammer-like behaviors would be spam.",2 Related Work,[0],[0]
Wang et al. (2016) explored a method to learn the review representation with global behavioral information.,2 Related Work,[0],[0]
Viviani and Pasi (2017) concentrated on the aggregation process with respect to each single veracity feature.,2 Related Work,[0],[0]
"As a new reviewer posted just one review and we have to identify it immediately, the major challenge of the cold-start task is that the available information about the new reviewer is very poor.",3 Whether Traditional Features are Effective,[0],[0]
The new reviewer only provides us with one review record.,3 Whether Traditional Features are Effective,[0],[0]
"For most traditional features based on the statistics, they can not form themselves or make no sense, such as the percentage of reviews written at weekends (Li et al., 2015), the entropy of rating distribution of user’s review (Rayana and Akoglu, 2015).",3 Whether Traditional Features are Effective,[0],[0]
"To investigate whether traditional features are effective in the cold-start task, we conducted experiments on the Yelp dataset in Mukherjee et al. (2013c).",3 Whether Traditional Features are Effective,[0],[0]
"We trained SVM models with different features on the existing reviews posted before January 1, 2012, and tested on the new reviews which just posted by the new reviewers after January 1, 2012.",3 Whether Traditional Features are Effective,[0],[0]
Results are shown in Table 1.,3 Whether Traditional Features are Effective,[0],[0]
The linguistic features need not take much time to form.,3.1 Linguistic Features’ Poor Performance,[0],[0]
"But Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, compared with the performances on the crowd source datasets (Ott et al., 2011).",3.1 Linguistic Features’ Poor Performance,[0],[0]
"They showed that the word bigrams perform better than the other linguistic features, such as LIWC (Newman et al., 2003; Pennebaker et al., 2007), part-of-speech sequence patterns (Mukherjee and Liu, 2010), deep syntax (Feng et al., 2012a), information gain (Mukherjee et al., 2013c) and so on.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"So, we conduct experiments with the word bigrams feature.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"As shown in Table 1 (a, b) row 1, the word bigrams result in only around 55% in accuracy in both the hotel and restaurant domains.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"It indicates that the most effective traditional linguistic feature (i.e., the word bigrams) can’t detect the review spam effectively in the cold start task.",3.1 Linguistic Features’ Poor Performance,[0],[0]
"Abundant Information
Because there is not enough available information about the new reviewer, for most traditional behavioral features based on the statistical mechanism, they couldn’t form themselves or make no sense.",3.2 Behavioral Features only Work Well with,[0],[0]
We investigated the previous work and found that there are three behavioral features can be applied to the cold-start task.,3.2 Behavioral Features only Work Well with,[0],[0]
"They are proposed by Mukherjee et al. (2013b), i.e., 1.Review length (RL) : the length of the new review posted by the new reviewer; 2.Reviewer deviation (RD): the absolute rating deviation of the new reviewer’s review from other reviews on the same business; 3.Maximum content similarity (MCS) : the maximum content similarity (using cosine similarity) between the new reviewer’s review with other reviews on the same business.
",3.2 Behavioral Features only Work Well with,[0],[0]
"Table 1 (a, b) row 2 shows the experiment results by the combinations of the bigrams feature and the three behavioral features described above.",3.2 Behavioral Features only Work Well with,[0],[0]
The behavioral features make around 5% improvement in accuracy in the hotel domain (2.7% in the restaurant domain) as compared with only using bigrams.,3.2 Behavioral Features only Work Well with,[0],[0]
The accuracy is improved but it is just near 60% in average.,3.2 Behavioral Features only Work Well with,[0],[0]
It indicates that the traditional features are not effective enough with poor behavioral information.,3.2 Behavioral Features only Work Well with,[0],[0]
"What’s more, the behavioral features cause around 4.6% decrease in F1-
score and around 19% decrease in Recall in both hotel and restaurant domains.",3.2 Behavioral Features only Work Well with,[0],[0]
It is obvious that there is more false-positive review spam caused by the behavioral features as compared to only using bigrams.,3.2 Behavioral Features only Work Well with,[0],[0]
"It further indicates that the traditional behavioral features’ discrimination for review spam gets to be weakened by the poor behavioral information.
",3.2 Behavioral Features only Work Well with,[0],[0]
"To go a step further, we carried experiments with the three behavioral features which are formed on abundant behavioral information.",3.2 Behavioral Features only Work Well with,[0],[0]
"When the new reviewers continue to post more reviews in after weeks, their behavioral information gets to be more.",3.2 Behavioral Features only Work Well with,[0],[0]
Then the review system could obtain sufficient data to extract behavior features as compared to the poor information in the cold-start period.,3.2 Behavioral Features only Work Well with,[0],[0]
So the behavioral features with abundant information make an obvious improvement in accuracy (6.4%) in the hotel domain (Table 1 (a) row 3) as compared with the results in Table 1 (a) row 2.,3.2 Behavioral Features only Work Well with,[0],[0]
But it is only 0.6% in the restaurant domain.,3.2 Behavioral Features only Work Well with,[0],[0]
"By statistics on the datasets, we found that the new reviewers posted about 54.4 reviews in average after their first post in the hotel domain, but it is only 10 reviews in average for the new reviewers in the restaurant domain.",3.2 Behavioral Features only Work Well with,[0],[0]
The added behavioral information in the hotel domain is richer than that in the restaurant domain.,3.2 Behavioral Features only Work Well with,[0],[0]
"It indicates that:
• the traditional behavioral features can only work well with abundant behavioral information; • the more behavioral information can be obtained, the more effective the traditional behavioral features are.",3.2 Behavioral Features only Work Well with,[0],[0]
The difficulty of detecting review spam in the cold-start task is that the available behavioral information of new reviewers is very poor.,4 The Proposed Model,[0],[0]
"The new reviewer just posted one review and we have to filter it out immediately, there is not any historical review provided to us.",4 The Proposed Model,[0],[0]
"As we argued, the textual information and the behavioral information of a reviewer are correlated with each other.",4 The Proposed Model,[0],[0]
"So, to augment the behavioral information of new reviewers, we try to find the textual information which is similar with that of the new reviewer, from existing reviews.",4 The Proposed Model,[0],[0]
Then we take the behavioral information which is correlated with the found textual information as the most possible behavioral information of the new reviewer.,4 The Proposed Model,[0],[0]
"For this purpose, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem (shown in Figure 2).",4 The Proposed Model,[0],[0]
"When a new reviewer posts a review, the neural network can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings.",4 The Proposed Model,[0],[0]
"Finally, embeddings of the new review are fed into a classifier to identify whether it is spam or not.",4 The Proposed Model,[0],[0]
"In Figure 1, there is a part of review graph which is simplified from the Yelp website.",4.1 Behavioral Information Encoding,[0],[0]
"As it shows, the review graph contains the global behavioral information (footprints) of the existing reviewers.",4.1 Behavioral Information Encoding,[0],[0]
"Because the motivations of the spammers and the real reviewers are totally different, the distributions of the behavioral information of them are different (Mukherjee et al., 2013a).",4.1 Behavioral Information Encoding,[0],[0]
"There are businesses (even highly reputable ones) paying people to write fake reviews for them to promote their products/services and/or to discredit their competitors (Liu, 2015).",4.1 Behavioral Information Encoding,[0],[0]
So the behavioral footprints of the spammers are decided by the demands of the businesses.,4.1 Behavioral Information Encoding,[0],[0]
But the real reviewers only post reviews to the product or services they have actually experienced.,4.1 Behavioral Information Encoding,[0],[0]
Their behavioral footprints are influenced by their own characteristics.,4.1 Behavioral Information Encoding,[0],[0]
Previous work extracts behavioral features for reviewers from these behavioral information.,4.1 Behavioral Information Encoding,[0],[0]
But it is impractical to the new reviewers in the cold-start task.,4.1 Behavioral Information Encoding,[0],[0]
"Moreover, the traditional discrete features can not effectively record the global behavioral information (Wang et al., 2016).",4.1 Behavioral Information Encoding,[0],[0]
"Besides, there is no explicit charac-
teristic tag available in the review system, and we need to find a way to record the reviewers’ latent characters information in footprints.
",4.1 Behavioral Information Encoding,[0],[0]
"Therefore we encode these behavioral information into our model by utilizing an embedding learning model which is similar with TransE (Bordes et al., 2013).",4.1 Behavioral Information Encoding,[0],[0]
"TransE is a model which can encode the graph structure, and represent the nodes and edges (head, translation/relation, tail) in low dimension vector space.",4.1 Behavioral Information Encoding,[0],[0]
"TransE has been proved that it is good at describing the global information of the graph structure by the work about distributional representation for knowledge base (Guu et al., 2015).",4.1 Behavioral Information Encoding,[0],[0]
We consider that each reviewer in review graph describes the product in his/her own view and writes the review.,4.1 Behavioral Information Encoding,[0],[0]
"When we represent the product, reviewer, and review in low dimension vector space, the reviewer embeddings can be taken as a translation vector, which has translated the product embeddings to the review embeddings.",4.1 Behavioral Information Encoding,[0],[0]
"So, as shown in Figure 2, we take the products (hotels/restaurants) as the head part of the TransE network in our model, take the reviewers as the translation (relation) part and take the review as the tail part.",4.1 Behavioral Information Encoding,[0],[0]
"By learning from the existing large scale unlabeled reviews of the review graph, we can encode the global behavioral information into our model without extracting any traditional behavioral feature, and record reviewers’ latent characteristics information.
",4.1 Behavioral Information Encoding,[0],[0]
"More formally, we minimize a margin-based criterion over the training set:
L = ∑
(β,α,τ )",4.1 Behavioral Information Encoding,[0],[0]
"∈S
∑
(β′,α,τ ′)∈S′ max
{0, 1 + d(β + α, τ )",4.1 Behavioral Information Encoding,[0],[0]
"− d(β′ + α, τ ′)} (1)
S denotes the training set of triples (β, α, τ ) composed product β (β ∈ B, products set (head part)), reviewer α (α ∈ A, reviewers set (translation part)) and review text embeddings learnt by the CNN τ",4.1 Behavioral Information Encoding,[0],[0]
"(τ ∈ T , review texts set (tail part)).
",4.1 Behavioral Information Encoding,[0],[0]
"S′ = {(β′, α, τ )",4.1 Behavioral Information Encoding,[0],[0]
"|β′ ∈ B} ∪ {(β, α, τ ′)|τ ′",4.1 Behavioral Information Encoding,[0],[0]
"∈ T} (2)
",4.1 Behavioral Information Encoding,[0],[0]
"The set of corrupted triplets S′ (Equation (2)), is composed of training triplets with either the product or review text replaced by a random chosen one (but not both at the same time).
",4.1 Behavioral Information Encoding,[0],[0]
"d(β + α, τ )",4.1 Behavioral Information Encoding,[0],[0]
= ∥β + α,4.1 Behavioral Information Encoding,[0],[0]
"− τ∥22 , s.t.",4.1 Behavioral Information Encoding,[0],[0]
"∥β∥22 = ∥α∥22 = ∥τ∥22 = 1
(3)
d(β",4.1 Behavioral Information Encoding,[0],[0]
"+ α, τ ) is the dissimilarity function with the squared euclidean distance.",4.1 Behavioral Information Encoding,[0],[0]
"To encode the textual information into our model, we adopt a convolutional neural network (CNN) to learn to represent the existing reviews.",4.2 Textual Information Encoding,[0],[0]
"By statistics, we find that a review usually refers to several aspects of the products or services.",4.2 Textual Information Encoding,[0],[0]
"For example, a hotel review may comment the room price, the free WiFi, and the bathroom at the same time.",4.2 Textual Information Encoding,[0],[0]
"Compared with the recurrent neural network (RNN), the CNN can do a better job of modeling the different aspects of a review.",4.2 Textual Information Encoding,[0],[0]
"Ren and Zhang (2016) have proved that the CNN can capture complex global semantic information and detect review spam more effectively, compared with traditional discrete manual features and the RNN model.",4.2 Textual Information Encoding,[0],[0]
"As shown in Figure 2, we take the learnt embeddings τ of reviews by the CNN as the tail part.
",4.2 Textual Information Encoding,[0],[0]
"Specifically, we denote the review text consisting of n words as {w1, w2, ..., wn}, the word embeddings e(wi) ∈ RD, D is the word vector dimension.",4.2 Textual Information Encoding,[0],[0]
"We take the concatenation of the word embeddings in a fixed length window size Z as the input of the linear layer, which is denoted as Ii ∈ RD×Z .",4.2 Textual Information Encoding,[0],[0]
"So the output of the linear layer Hi is calculated by Hk,i = Wk · Ii + bi, where Wk ∈ RD×Z is the weight matrix of filter k.",4.2 Textual Information Encoding,[0],[0]
We utilize a max pooling layer to get the output of each filter.,4.2 Textual Information Encoding,[0],[0]
"Then we take tanh as the activation function and concatenate the outputs as the final review embeddings, which is denoted as τi.",4.2 Textual Information Encoding,[0],[0]
"To model the correlation of the textual and behavioral information, we employ the jointly information encoding.",4.3 Jointly Information Encoding,[0],[0]
"By jointly learning from the global review graph, the textual and behavioral information of existing spammers and real reviewers are embedded into the word embeddings.
",4.3 Jointly Information Encoding,[0],[0]
"In addition, the rating usually represents the sentiment polarity of a review, e.g., five star means ‘like’ and one star means ‘dislike’.",4.3 Jointly Information Encoding,[0],[0]
"The spammers often review their target products with a low rating for discredited purpose, and with a high rating for promoted purpose.",4.3 Jointly Information Encoding,[0],[0]
"To encode the semantics of the sentiment polarity into the review embeddings, we learn the embeddings of 1-5 stars rating in our model at the same time.",4.3 Jointly Information Encoding,[0],[0]
They are taken as the constraints of the review embeddings during the joint learning.,4.3 Jointly Information Encoding,[0],[0]
"They are calculated as:
C = ∑
(τ ,γ)∈Γ
∑
(τ ,γ′)∈Γ′ max{0, 1+ g(τ , γ)− g(τ , γ′)} (4)
",4.3 Jointly Information Encoding,[0],[0]
"The set of corrupted tuples Γ′ is composed of training tuples Γ with the rating of review replaced by its opposite rating (i.e., 1 by 5, 2 by 4, 3 by 1 or 5).",4.3 Jointly Information Encoding,[0],[0]
"g(τ , γ) =",4.3 Jointly Information Encoding,[0],[0]
"∥τ − γ∥22, norm constraints: ∥γ∥22 = 1.
",4.3 Jointly Information Encoding,[0],[0]
"The final joint loss function is as follows:
LJ = (1 − θ)L + θC (5)
where θ is a hyper-parameter.",4.3 Jointly Information Encoding,[0],[0]
"Datasets: To evaluate the proposed method, we conducted experiments on Yelp dataset that was used in (Mukherjee et al., 2013b,c; Rayana and Akoglu, 2015).",5.1 Datasets and Evaluation Metrics,[0],[0]
The statistics of the Yelp dataset are listed in Table 2 and Table 3.,5.1 Datasets and Evaluation Metrics,[0],[0]
The reviewed product here refers to a hotel or restaurant.,5.1 Datasets and Evaluation Metrics,[0],[0]
"We take the existing reviews posted before January 1, 2012 as the datasets for training our embedding learning model, and take the first new reviews which just posted by the new reviewers after January 1, 2012 as the test datasets.",5.1 Datasets and Evaluation Metrics,[0],[0]
Table 4 displays the statistics of the balanced datasets for training and testing the classifier.,5.1 Datasets and Evaluation Metrics,[0],[0]
Evaluation Metrics:,5.1 Datasets and Evaluation Metrics,[0],[0]
"We select precision (P), recall (R), F1-Score (F1), accuracy (A) as metrics.",5.1 Datasets and Evaluation Metrics,[0],[0]
"To illustrate the effectiveness of our model, we conduct experiments on the public datasets, and make comparison with the most effective traditional linguistic features, e.g., bigrams, and the three practicable traditional behavioral features (RL, RD, MCS (Mukherjee et al., 2013b)) referred in Section 3.2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The results are shown in Table 5.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"For our model, we set the dimension of embeddings to 100, the number of CNN filters to 100, θ to 0.1, Z to 2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The hyper-parameters are tuned by grid search on the development dataset.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"The product and reviewer embeddings are randomly ini-
tialized from a uniform distribution (Socher et al., 2013).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"The word embeddings are initialized with 100-dimensions vectors pre-trained by the CBOW model (Word2Vec) (Mikolov et al., 2013).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"As Table 5 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
Review Embeddings,5.2 Our Model v.s. the Traditional Features,[0],[0]
"Compared with the traditional linguistic features, e.g., bigrams, using the review embeddings learnt by our model, results in around 3.4% improvement in F1 and around 7.4% improvement in A in the hotel domain (1.1% in F1 and 5.0% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 1, 5).",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Compared with the combination of the bigrams and the traditional behavioral features, using the review embeddings learnt by our model, results in around 7.6% improvement in F1 and around 2.2% improvement in A in the hotel domain (6.1% in F1 and 2.3% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 2, 5).",5.2 Our Model v.s. the Traditional Features,[0],[0]
The F1-Score (F1) of the classification under the balance distribution reflects the ability to detect the review spam.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The accuracy (A) of the classification under the balance distribution reflects the ability to identify both the review spam and the real review.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The experiment results indicate that our model performs significantly better than the traditional methods in F1 and A at the same time.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"The learnt review embeddings with encoded linguistic and behavioral information are more effective in detecting review
spam for the cold-start task.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Rating Embeddings As we referred in Section 4.3, the rating of a review usually means the sentiment polarity of a real reviewer or the motivation of a spammer.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"As shown in Table 5 (a,b) rows 6, adding the rating embeddings of the products (hotel/restaurant) and reviews renders even higher F1 and A.",5.2 Our Model v.s. the Traditional Features,[0],[0]
We suppose that different rating embeddings are encoded with different semantic meanings.,5.2 Our Model v.s. the Traditional Features,[0],[0]
They reflect the semantic divergences between the average rating of the product and the review rating.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"In results, using RE+RRE+PRE which makes the best performance of our model, results in around 5.5% improvement in F1 and around 9.4% improvement in A in the hotel domain (2.9% in F1 and 6.2% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 1, 6), compared with the LF.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"Using RE+RRE+PRE results in around 9.7% improvement in F1 and around 4.2% improvement in A in the hotel domain (7.9% in F1 and 3.5% in A for the restaurant domain, shown in Tabel 5 (a,b) rows 2, 6), compared with the LF+BF.
",5.2 Our Model v.s. the Traditional Features,[0],[0]
The experiment results prove that our model is effective.,5.2 Our Model v.s. the Traditional Features,[0],[0]
The improvements in both the F1 and A prove that our model performs well in both detecting the review spam and identifying the real review.,5.2 Our Model v.s. the Traditional Features,[0],[0]
"Furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability 2.",5.2 Our Model v.s. the Traditional Features,[0],[0]
"It can learn to represent the reviews with global linguistic and behavioral information from largescale unlabeled existing reviews.
2The improvements in hotel domain are greater than that in restaurant domain.",5.2 Our Model v.s. the Traditional Features,[0],[0]
The possible reason is the proportion of the available training data in hotel domain is higher than that in restaurant domain (99.01% vs. 97.40% in Table 2).,5.2 Our Model v.s. the Traditional Features,[0],[0]
"As mentioned in Section 1, to approximate the behavioral information of the new reviewers, there are other intuitive methods.",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
So we conduct experiments with two intuitive methods as a comparison.,5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"One is finding the most similar existing review by edit distance ratio and taking the found reviewers’ behavioral features as an approximation, and then training the classifier on the behavioral features and bigrams (BF EditSim+LF).",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"The other is finding the most similar existing review by cosine similarity of review embeddings which is the average of the pre-trained word embeddings (using Word2Vec), and then training the classifier on the behavioral features and review embeddings (BF W2Vsim+W2V).",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"As shown in Table 5, our joint embeddings (Ours RE and Ours RE+RRE+PRE) obviously perform better than the intuitive methods, such as the Ours RE is 3.8% (Accuracy) and 3.2% (F1) better than BF W2Vsim+W2V in the hotel domain.",5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
The experiments indicate that our joint embeddings do a better job in capturing the reviewer’s characteristics and modeling the correlation of textual and behavioral information.,5.3 Our Jointly Embeddings v.s. the Intuitive Methods,[0],[0]
"Behavioral Information
To further evaluate the effectiveness of encoding the global behavioral information in our model, we build an independent supervised convolutional neural network which has the same structure and parameter settings with the CNN part of our model.",5.4 The Effectiveness of Encoding the Global,[0],[0]
"There is not any review graphic or behavioral information in this independent supervised CNN (Tabel 6 (a,b) row 2).",5.4 The Effectiveness of Encoding the Global,[0],[0]
"As shown in Tabel 6 (a,b) rows 2, 3, compared with the review embeddings learnt by the independent supervised CNN, using
the review embeddings learnt by our model results in around 9.0% improvement in F1 and around 3.8% improvement in A in the hotel domain (7.9% in F1 and 3.7% in A for the restaurant domain).",5.4 The Effectiveness of Encoding the Global,[0],[0]
The results show that our model can represent the new reviews posted by the new reviewers with the correlated behavioral information encoded in the word embeddings.,5.4 The Effectiveness of Encoding the Global,[0],[0]
The transE part of our model has effectively recorded the behavioral information of the review graph.,5.4 The Effectiveness of Encoding the Global,[0],[0]
"Thus, our model is more effective by jointly embedding the textual and behavioral informations, it helps to augment the possible behavioral information of the new reviewer.",5.4 The Effectiveness of Encoding the Global,[0],[0]
"Compared with the the most effective linguistic features, e.g., bigrams, our independent supervised convolutional neural network performs better in A than F1 (shown in Tabel 5 (a,b) rows 1, 2).",5.5 The Effectiveness of CNN,[0],[0]
It indicates that the CNN do a better job in identifying the real review than the review spam.,5.5 The Effectiveness of CNN,[0],[0]
We suppose that the possible reason is that the CNN is good at modeling the different semantic aspects of a review.,5.5 The Effectiveness of CNN,[0],[0]
"And the real reviewers usually tend to describe different aspects of a hotel or restaurant according to their real personal experiences, but the spammers can only forge fake reviews with their own infinite imagination.",5.5 The Effectiveness of CNN,[0],[0]
"Mukherjee et al. (2013b) also proved that different psychological states of the minds of the spammers and non-spammers, lead to significant linguistic differences between review spam and non-spam.",5.5 The Effectiveness of CNN,[0],[0]
This paper analyzes the importance and difficulty of the cold-start challenge in review spam combat.,6 Conclusion and Future Work,[0],[0]
We propose a neural network model that jointly embeds the existing textual and behavioral information for detecting review spam in the coldstart task.,6 Conclusion and Future Work,[0],[0]
It can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way.,6 Conclusion and Future Work,[0],[0]
"Then, a classifier is applied to detect the review spam.",6 Conclusion and Future Work,[0],[0]
Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability.,6 Conclusion and Future Work,[0],[0]
It is also applicable to a large-scale dataset in an unsupervised way.,6 Conclusion and Future Work,[0],[0]
"To our best knowledge, this is the first work to handle the cold-start problem in review spam detection.",6 Conclusion and Future Work,[0],[0]
"We are going to explore more effective models in fu-
ture.",6 Conclusion and Future Work,[0],[0]
This work was supported by the Natural Science Foundation of China (No. 61533018) and the National Basic Research Program of China (No. 2014CB340503).,Acknowledgments,[0],[0]
And this research work was also supported by Google through focused research awards program.,Acknowledgments,[0],[0]
"We would like to thank Prof. Bing Liu for useful advice, and the anonymous reviewers for their detailed comments and suggestions.",Acknowledgments,[0],[0]
Solving the cold-start problem in review spam detection is an urgent and significant task.,abstractText,[0],[0]
"It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work.",abstractText,[0],[0]
"This paper proposes a novel neural network model to detect review spam for the cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information.",abstractText,[0],[0]
Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability.,abstractText,[0],[0]
It is also applicable to a large-scale dataset in an unsupervised way.,abstractText,[0],[0]
Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1907–1917 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1907",text,[0],[0]
"Recently, there has been a resurgence of work in NLP on reading comprehension (Hermann et al., 2015; Rajpurkar et al., 2016; Joshi et al., 2017) with the goal of developing systems that can answer questions about the content of a given passage or document.",1 Introduction,[0],[0]
Large-scale QA datasets are indispensable for training expressive statistical models for this task and play a critical role in advancing the field.,1 Introduction,[0],[0]
And there have been a number of efforts in this direction.,1 Introduction,[0],[0]
"Miller et al. (2016), for example, develop a dataset for open-domain question answering; Rajpurkar et al. (2016) and Joshi et al. (2017) do so for reading comprehension (RC); and Hill et al. (2015) and Hermann
et al. (2015), for the related task of answering cloze questions (Winograd, 1972; Levesque et al., 2011).",1 Introduction,[0],[0]
"To create these datasets, either crowdsourcing or (semi-)synthetic approaches are used.",1 Introduction,[0],[0]
"The (semi-)synthetic datasets (e.g., Hermann et al. (2015)) are large in size and cheap to obtain; however, they do not share the same characteristics as explicit QA/RC questions (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"In comparison, high-quality crowdsourced datasets are much smaller in size, and the annotation process is quite expensive because the labeled examples require expertise and careful design (Chen et al., 2016).
",1 Introduction,[0],[0]
"Thus, there is a need for methods that can automatically generate high-quality question-answer pairs.",1 Introduction,[0],[0]
Serban et al. (2016) propose the use of recurrent neural networks to generate QA pairs from structured knowledge resources such as Freebase.,1 Introduction,[0],[0]
"Their work relies on the existence of automatically acquired KBs, which are known to have errors and suffer from incompleteness.",1 Introduction,[0],[0]
They are also nontrivial to obtain.,1 Introduction,[0],[0]
"In addition, the questions in the resulting dataset are limited to queries regarding a single fact (i.e., tuple) in the KB.
",1 Introduction,[0],[0]
"Motivated by the need for large scale QA pairs and the limitations of recent work, we investigate methods that can automatically “harvest” (generate) question-answer pairs from raw text/unstructured documents, such as Wikipediatype articles.
",1 Introduction,[0],[0]
"Recent work along these lines (Du et al., 2017; Zhou et al., 2017) (see Section 2) has proposed the use of attention-based recurrent neural models trained on the crowdsourced SQuAD dataset (Rajpurkar et al., 2016) for question generation.",1 Introduction,[0],[0]
"While successful, the resulting QA pairs are based on information from a single sentence.",1 Introduction,[0],[0]
"As described in Du et al. (2017), however, nearly 30% of the questions in the human-generated questions of SQuAD rely on information beyond a single sentence.",1 Introduction,[0],[0]
"For example, in Figure 1, the second and third questions require coreference information (i.e., recognizing that “His” in sentence 2 and “He” in sentence 3 both corefer with “Tesla” in sentence 1) to answer them.
",1 Introduction,[0],[0]
"Thus, our research studies methods for incorporating coreference information into the training of a question generation system.",1 Introduction,[0],[0]
"In particular, we propose gated Coreference knowledge for Neural Question Generation (CorefNQG), a neural sequence model with a novel gating mechanism that leverages continuous representations of coreference clusters — the set of mentions used to refer to each entity — to better encode linguistic knowledge introduced by coreference, for paragraph-level question generation.
",1 Introduction,[0],[0]
"In an evaluation using the SQuAD dataset, we find that CorefNQG enables better question generation.",1 Introduction,[0],[0]
"It outperforms significantly the baseline neural sequence models that encode information from a single sentence, and a model that encodes all preceding context and the input sentence itself.",1 Introduction,[0],[0]
"When evaluated on only the portion of SQuAD that requires coreference resolution, the gap be-
tween our system and the baseline systems is even larger.
",1 Introduction,[0],[0]
"By applying our approach to the 10,000 topranking Wikipedia articles, we obtain a question answering/reading comprehension dataset with over one million QA pairs; we provide a qualitative analysis in Section 6.",1 Introduction,[0],[0]
The dataset and the source code for the system are available at https://github.com/xinyadu/ HarvestingQA.,1 Introduction,[0],[0]
"Since the work by Rus et al. (2010), question generation (QG) has attracted interest from both the NLP and NLG communities.",2.1 Question Generation,[0],[0]
"Most early work in QG employed rule-based approaches to transform input text into questions, usually requiring the application of a sequence of well-designed general rules or templates (Mitkov and Ha, 2003; Labutov et al., 2015).",2.1 Question Generation,[0],[0]
Heilman and Smith (2010) introduced an overgenerate-and-rank approach: their system generates a set of questions and then ranks them to select the top candidates.,2.1 Question Generation,[0],[0]
"Apart from generating questions from raw text, there has also been research on question generation from symbolic representations (Yao et al., 2012; Olney et al., 2012).
",2.1 Question Generation,[0],[0]
"With the recent development of deep representation learning and large QA datasets, there has been research on recurrent neural network based approaches for question generation.",2.1 Question Generation,[0],[0]
Serban et al. (2016) used the encoder-decoder framework to generate QA pairs from knowledge base triples; Reddy et al. (2017) generated questions from a knowledge graph; Du et al. (2017) studied how to generate questions from sentences using an attention-based sequence-to-sequence model and investigated the effect of exploiting sentencevs.,2.1 Question Generation,[0],[0]
paragraph-level information.,2.1 Question Generation,[0],[0]
Du and Cardie (2017) proposed a hierarchical neural sentencelevel sequence tagging model for identifying question-worthy sentences in a text passage.,2.1 Question Generation,[0],[0]
"Finally, Duan et al. (2017) investigated how to use question generation to help improve question answering systems on the sentence selection subtask.
",2.1 Question Generation,[0],[0]
"In comparison to the related methods from above that generate questions from raw text, our method is different in its ability to take into account contextual information beyond the sentencelevel by introducing coreference knowledge.",2.1 Question Generation,[0],[0]
Recently there has been an increasing interest in question answering with the creation of many datasets.,2.2 Question Answering Datasets and Creation,[0],[0]
"Most are built using crowdsourcing; they are generally comprised of fewer than 100,000 QA pairs and are time-consuming to create.",2.2 Question Answering Datasets and Creation,[0],[0]
"WebQuestions (Berant et al., 2013), for example, contains 5,810 questions crawled via the Google Suggest API and is designed for knowledge base QA with answers restricted to Freebase entities.",2.2 Question Answering Datasets and Creation,[0],[0]
"To tackle the size issues associated with WebQuestions, Bordes et al. (2015) introduce SimpleQuestions, a dataset of 108,442 questions authored by English speakers.",2.2 Question Answering Datasets and Creation,[0],[0]
"SQuAD (Rajpurkar et al., 2016) is a dataset for machine comprehension; it is created by showing a Wikipedia paragraph to human annotators and asking them to write questions based on the paragraph.",2.2 Question Answering Datasets and Creation,[0],[0]
"TriviaQA (Joshi et al., 2017) includes 95k question-answer authored by trivia enthusiasts and corresponding evidence documents.
",2.2 Question Answering Datasets and Creation,[0],[0]
"(Semi-)synthetic generated datasets are easier to build to large-scale (Hill et al., 2015; Hermann et al., 2015).",2.2 Question Answering Datasets and Creation,[0],[0]
They usually come in the form of cloze-style questions.,2.2 Question Answering Datasets and Creation,[0],[0]
"For example, Hermann et al. (2015) created over a million examples by pairing CNN and Daily Mail news articles with their summarized bullet points.",2.2 Question Answering Datasets and Creation,[0],[0]
"Chen et al. (2016) showed that this dataset is quite noisy due to the method of data creation and concluded that performance of QA systems on the dataset is almost saturated.
",2.2 Question Answering Datasets and Creation,[0],[0]
Closest to our work is that of Serban et al. (2016).,2.2 Question Answering Datasets and Creation,[0],[0]
"They train a neural triple-to-sequence model on SimpleQuestions, and apply their system to Freebase to produce a large collection of human-like question-answer pairs.",2.2 Question Answering Datasets and Creation,[0],[0]
Our goal is to harvest high quality questionanswer pairs from the paragraphs of an article of interest.,3 Task Definition,[0],[0]
"In our task formulation, this consists of two steps: candidate answer extraction and answer-specific question generation.",3 Task Definition,[0],[0]
"Given an input paragraph, we first identify a set of question-worthy candidate answers ans = (ans1, ans2, ..., ansl), each a span of text as denoted in color in Figure 1.",3 Task Definition,[0],[0]
"For each candidate answer ansi, we then aim to generate a question Q — a sequence of tokens y1, ..., yN — based on the
sentence S that contains candidate ansi such that:
• Q asks about an aspect of ansi that is of potential interest to a human;
• Q might rely on information from sentences that precede S in the paragraph.
",3 Task Definition,[0],[0]
"Mathematically then,
Q = argmax Q
P (Q|S,C) (1)
where P (Q|S,C) = ∏N
n=1 P (yn|y<n, S, C) where C is the set of sentences that precede S in the paragraph.",3 Task Definition,[0],[0]
"In this section, we introduce our framework for harvesting the question-answer pairs.",4 Methodology,[0],[0]
"As described above, it consists of the question generator CorefNQG (Figure 2) and a candidate answer extraction module.",4 Methodology,[0],[0]
"During test/generation time, we (1) run the answer extraction module on the input text to obtain answers, and then (2) run the question generation module to obtain the corresponding questions.",4 Methodology,[0],[0]
"As shown in Figure 2, our generator prepares the feature-rich input embedding — a concatenation of (a) a refined coreference position feature embedding, (b) an answer feature embedding, and (c) a word embedding, each of which is described below.",4.1 Question Generation,[0],[0]
"It then encodes the textual input using an LSTM unit (Hochreiter and Schmidhuber, 1997).",4.1 Question Generation,[0],[0]
"Finally, an attention-copy equipped decoder is used to decode the question.
",4.1 Question Generation,[0],[0]
"More specifically, given the input sentence S (containing an answer span) and the preceding context C, we first run a coreference resolution system to get the coref-clusters for S and C and use them to create a coreference transformed input sentence: for each pronoun, we append its most representative non-pronominal coreferent mention.",4.1 Question Generation,[0],[0]
"Specifically, we apply the simple feedforward network based mention-ranking model of Clark and Manning (2016) to the concatenation of C and S to get the coref-clusters for all entities in C and S. The C&M model produces a score/representation s for each mention pair (m1,m2),
s(m1,m2) = Wmhm(m1,m2) + bm",4.1 Question Generation,[0],[0]
"(2)
…
Sentence encoder
...
where Wm is a 1 × d weight matrix and b is the bias.",4.1 Question Generation,[0],[0]
"hm(m1,m2) is representation of the last hidden layer of the three layer feedforward neural network.
",4.1 Question Generation,[0],[0]
"For each pronoun in S, we then heuristically identify the most “representative” antecedent from its coref-cluster.",4.1 Question Generation,[0],[0]
(Proper nouns are preferred.),4.1 Question Generation,[0],[0]
We append the new mention after the pronoun.,4.1 Question Generation,[0],[0]
"For example, in Table 1, “the panthers” is the most representative mention in the coref-cluster for “they”.",4.1 Question Generation,[0],[0]
The new sentence with the appended coreferent mention is our coreference transformed input sentence S ′,4.1 Question Generation,[0],[0]
"(see Figure 2).
",4.1 Question Generation,[0],[0]
Coreference Position Feature Embedding For each token in S ′,4.1 Question Generation,[0],[0]
", we also maintain one position feature fc = (c1, ..., cn), to denote pronouns (e.g., “they”) and antecedents (e.g., “the panthers”).",4.1 Question Generation,[0],[0]
We use the BIO tagging scheme to label the associated spans in S ′ .,4.1 Question Generation,[0],[0]
"“B_ANT” denotes the start of an antecedent span, tag “I_ANT” continues the antecedent span and tag “O” marks tokens that do not form part of a mention span.",4.1 Question Generation,[0],[0]
"Similarly, tags “B_PRO” and “I_PRO” denote the pronoun span.",4.1 Question Generation,[0],[0]
"(See Table 1, “coref.",4.1 Question Generation,[0],[0]
"feature”.)
",4.1 Question Generation,[0],[0]
Refined Coref.,4.1 Question Generation,[0],[0]
"Position Feature Embedding Inspired by the success of gating mecha-
nisms for controlling information flow in neural networks (Hochreiter and Schmidhuber, 1997; Dauphin et al., 2017), we propose to use a gating network here to obtain a refined representation of the coreference position feature vectors fc = (c1, ..., cn).",4.1 Question Generation,[0],[0]
The main idea is to utilize the mention-pair score (see Equation 2) to help the neural network learn the importance of the coreferent phrases.,4.1 Question Generation,[0],[0]
We compute the refined (gated) coreference position feature vector fd =,4.1 Question Generation,[0],[0]
"(d1, ..., dn) as follows,
gi = ReLU(Waci +Wbscorei + b)
",4.1 Question Generation,[0],[0]
"di = gi ci (3)
where denotes an element-wise product between two vectors and ReLU is the rectified linear activation function.",4.1 Question Generation,[0],[0]
"scorei denotes the mentionpair score for each antecedent token (e.g., “the” and “panthers”) with the pronoun (e.g., “they”); scorei is obtained from the trained model (Equation 2) of the C&M.",4.1 Question Generation,[0],[0]
"If token i is not added later as an antecedent token, scorei is set to zero.",4.1 Question Generation,[0],[0]
"Wa, Wb are weight matrices and b is the bias vector.
",4.1 Question Generation,[0],[0]
"Answer Feature Embedding We also include an answer position feature embedding to generate answer-specific questions; we denote the answer span with the usual BIO tagging scheme (see,
e.g., “the arizona cardinals” in Table 1).",4.1 Question Generation,[0],[0]
"During training and testing, the answer span feature (i.e., “B_ANS”, “I_ANS” or “O”) is mapped to its feature embedding space: fa = (a1, ..., an).
",4.1 Question Generation,[0],[0]
"Word Embedding To obtain the word embedding for the tokens themselves, we just map the tokens to the word embedding space: x = (x1, ..., xn).
",4.1 Question Generation,[0],[0]
"Final Encoder Input As noted above, the final input to the LSTM-based encoder is a concatenation of (1) the refined coreference position feature embedding (light blue units in Figure 2), (2) the answer position feature embedding (red units), and (3) the word embedding for the token (green units),
ei = concat(di, ai, xi) (4)
Encoder As for the encoder itself, we use bidirectional LSTMs to read the input e = (e1, ..., en) in both the forward and backward directions.",4.1 Question Generation,[0],[0]
"After encoding, we obtain two sequences of hidden vectors, namely, −→ h = ( −→ h1, ..., −→ hn) and ←− h = ( ←− h1, ..., ←− hn).",4.1 Question Generation,[0],[0]
"The final output state of the encoder is the concatenation of −→ h and ←− h where
hi = concat( −→ hi , ←− hi) (5)
",4.1 Question Generation,[0],[0]
"Question Decoder with Attention & Copy On top of the feature-rich encoder, we use LSTMs with attention (Bahdanau et al., 2015) as the decoder for generating the question y1, ..., ym one token at a time.",4.1 Question Generation,[0],[0]
"To deal with rare/unknown words, the decoder also allows directly copying words from the source sentence via pointing (Vinyals et al., 2015).
",4.1 Question Generation,[0],[0]
"At each time step t, the decoder LSTM reads the previous word embedding wt−1 and previous hidden state st−1 to compute the new hidden state,
st = LSTM(wt−1, st−1) (6) Then we calculate the attention distribution αt as in Bahdanau et al. (2015),
et,i = h T",4.1 Question Generation,[0],[0]
i,4.1 Question Generation,[0],[0]
"Wcst−1 αt = softmax(et) (7)
where Wc is a weight matrix and attention distribution αt is a probability distribution over the source sentence words.",4.1 Question Generation,[0],[0]
"With αt, we can obtain the context vector h∗t ,
h∗t = n∑
i=1
αithi (8)
Then, using the context vector h∗t and hidden state st, the probability distribution over the target (question) side vocabulary is calculated as,
Pvocab = softmax(Wdconcat(h∗t , st))",4.1 Question Generation,[0],[0]
"(9)
Instead of directly using Pvocab for training/generating with the fixed target side vocabulary, we also consider copying from the source sentence.",4.1 Question Generation,[0],[0]
"The copy probability is based on the context vector h∗t and hidden state st,
λcopyt = σ",4.1 Question Generation,[0],[0]
"(Weh ∗ t +Wfst) (10)
and the probability distribution over the source sentence words is the sum of the attention scores of the corresponding words,
Pcopy(w) = n∑ i=1 αit ∗ 1{w == wi} (11)
",4.1 Question Generation,[0],[0]
"Finally, we obtain the probability distribution over the dynamic vocabulary (i.e., union of original target side and source sentence vocabulary) by summing over Pcopy and Pvocab,
P (w) = λcopyt Pcopy(w) + (1− λ copy t )Pvocab(w)
(12) where σ is the sigmoid function, and Wd, We,",4.1 Question Generation,[0],[0]
Wf are weight matrices.,4.1 Question Generation,[0],[0]
"We frame the problem of identifying candidate answer spans from a paragraph as a sequence labeling task and base our model on the BiLSTM-CRF approach for named entity recognition (Huang et al., 2015).",4.2 Answer Span Identification,[0],[0]
"Given a paragraph of n tokens, instead of directly feeding the sequence of word vectors x = (x1, ..., xn) to the LSTM units, we first construct the feature-rich embedding x ′ for each token, which is the concatenation of the word embedding, an NER feature embedding, and a character-level representation of the word (Lample et al., 2016).",4.2 Answer Span Identification,[0],[0]
"We use the concatenated vector as the “final” embedding x ′ for the token,
x ′",4.2 Answer Span Identification,[0],[0]
"i = concat(xi,CharRepi,NERi) (13)
where CharRepi is the concatenation of the last hidden states of a character-based biLSTM.",4.2 Answer Span Identification,[0],[0]
"The intuition behind the use of NER features is that SQuAD answer spans contain a large number of named entities, numeric phrases, etc.
",4.2 Answer Span Identification,[0],[0]
"Then a multi-layer Bi-directional LSTM is applied to (x
′ 1, ..., x ′ n) and we obtain the output state
zt for time step t by concatenation of the hidden states (forward and backward) at time step t from the last layer of the BiLSTM.",4.2 Answer Span Identification,[0],[0]
"We apply the softmax to (z1, ..., zn) to get the normalized score representation for each token, which is of size n× k, where k is the number of tags.
",4.2 Answer Span Identification,[0],[0]
"Instead of using a softmax training objective that minimizes the cross-entropy loss for each individual word, the model is trained with a CRF (Lafferty et al., 2001) objective, which minimizes the negative log-likelihood for the entire correct sequence: − log(py),
py = exp(q(x ′ ,y))∑
y′∈Y′",4.2 Answer Span Identification,[0],[0]
"exp(q(x ′ ,y′))
",4.2 Answer Span Identification,[0],[0]
"(14)
where q(x ′ ,y) = ∑n t=1 Pt,yt + ∑n−1 t=0 Ayt,yt+1 , Pt,yt is the score of assigning tag yt to the t th token, and Ayt,yt+1 is the transition score from tag yt to yt+1, the scoring matrix A is to be learned.",4.2 Answer Span Identification,[0],[0]
Y ′ represents all the possible tagging sequences.,4.2 Answer Span Identification,[0],[0]
"We use the SQuAD dataset (Rajpurkar et al., 2016) to train our models.",5.1 Dataset,[0],[0]
It is one of the largest general purpose QA datasets derived from Wikipedia with over 100k questions posed by crowdworkers on a set of Wikipedia articles.,5.1 Dataset,[0],[0]
The answer to each question is a segment of text from the corresponding Wiki passage.,5.1 Dataset,[0],[0]
The crowdworkers were users of Amazon’s Mechanical Turk located in the US or Canada.,5.1 Dataset,[0],[0]
"To obtain high-quality articles, the authors sampled 500 articles from the top 10,000 articles obtained by Nayuki’s Wikipedia’s internal PageRanks.",5.1 Dataset,[0],[0]
"The question-answer pairs were generated by annotators from a paragraph; and although the dataset is typically used to evaluate reading comprehension, it has also been used in an open domain QA setting (Chen et al., 2017; Wang et al., 2018).",5.1 Dataset,[0],[0]
"For training/testing answer extraction systems, we pair each paragraph in the dataset with the gold answer spans that it contains.",5.1 Dataset,[0],[0]
"For the question generation system, we pair each sentence that contains an answer span with the corresponding gold question as in Du et al. (2017).
",5.1 Dataset,[0],[0]
"To quantify the effect of using predicted (rather than gold standard) answer spans on question generation (e.g., predicted answer span boundaries can be inaccurate), we also train the models on an augmented “Training set w/ noisy examples”
(see Table 2).",5.1 Dataset,[0],[0]
"This training set contains all of the original training examples plus new examples for predicted answer spans (from the top-performing answer extraction model, bottom row of Table 3) that overlap with a gold answer span.",5.1 Dataset,[0],[0]
We pair the new training sentence (w/ predicted answer span) with the gold question.,5.1 Dataset,[0],[0]
"The added examples comprise 42.21% of the noisy example training set.
",5.1 Dataset,[0],[0]
"For generation of our one million QA pair corpus, we apply our systems to the 10,000 topranking articles of Wikipedia.",5.1 Dataset,[0],[0]
"For question generation evaluation, we use BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014).1 BLEU measures average n-gram precision vs. a set of reference questions and penalizes for overly short sentences.",5.2 Evaluation Metrics,[0],[0]
"METEOR is a recall-oriented metric that takes into account synonyms, stemming, and paraphrases.
",5.2 Evaluation Metrics,[0],[0]
"For answer candidate extraction evaluation, we use precision, recall and F-measure vs. the gold standard SQuAD answers.",5.2 Evaluation Metrics,[0],[0]
"Since answer boundaries are sometimes ambiguous, we compute Binary Overlap and Proportional Overlap metrics in addition to Exact Match.",5.2 Evaluation Metrics,[0],[0]
"Binary Overlap counts every predicted answer that overlaps with a gold answer span as correct, and Proportional Overlap give partial credit proportional to the amount of overlap (Johansson and Moschitti, 2010; Irsoy and Cardie, 2014).",5.2 Evaluation Metrics,[0],[0]
"For question generation, we compare to the stateof-the-art baselines and conduct ablation tests as follows: Du et al. (2017)’s model is an attention-based RNN sequence-to-sequence neural network (without using the answer location information feature).",5.3 Baselines and Ablation Tests,[0],[0]
"Seq2seq + copyw/ answer is the attention-based sequence-to-sequence model augmented with a copy mechanism, with answer features concatenated with the word embeddings during encoding.",5.3 Baselines and Ablation Tests,[0],[0]
"Seq2seq + copyw/ full context + answer is the same model as the previous one, but we allow access to the full context (i.e., all the preceding sentences and the input sentence itself).",5.3 Baselines and Ablation Tests,[0],[0]
We denote it as ContextNQG henceforth for simplicity.,5.3 Baselines and Ablation Tests,[0],[0]
CorefNQG is the coreference-based model proposed in this paper.,5.3 Baselines and Ablation Tests,[0],[0]
"CorefNQG–gating is an
1We use the evaluation scripts of Du et al. (2017).
ablation test, the gating network is removed and the coreference position embedding is not refined.",5.3 Baselines and Ablation Tests,[0],[0]
"CorefNQG–mention-pair score is also an ablation test where all mention-pair scorei are set to zero.
",5.3 Baselines and Ablation Tests,[0],[0]
"For answer span extraction, we conduct experiments to compare the performance of an off-theshelf NER system and BiLSTM based systems.
",5.3 Baselines and Ablation Tests,[0],[0]
"For training and implementation details, please see the Supplementary Material.",5.3 Baselines and Ablation Tests,[0],[0]
"Table 2 shows the BLEU-{3, 4} and METEOR scores of different models.",6.1 Automatic Evaluation,[0],[0]
Our CorefNQG outperforms the seq2seq baseline of Du et al. (2017) by a large margin.,6.1 Automatic Evaluation,[0],[0]
"This shows that the copy mechanism, answer features and coreference resolution all aid question generation.",6.1 Automatic Evaluation,[0],[0]
"In addition, CorefNQG outperforms both Seq2seq+Copy models significantly, whether or not they have access to the full context.",6.1 Automatic Evaluation,[0],[0]
This demonstrates that the coreference knowledge encoded with the gating network explicitly helps with the training and generation: it is more difficult for the neural sequence model to learn the coreference knowledge in a latent way.,6.1 Automatic Evaluation,[0],[0]
(See input 1 in Figure 3 for an example.),6.1 Automatic Evaluation,[0],[0]
Building end-to-end models that take into account coreference knowledge in a latent way is an interesting direction to explore.,6.1 Automatic Evaluation,[0],[0]
"In the ablation tests, the performance drop of CorefNQG–gating
shows that the gating network is playing an important role for getting refined coreference position feature embedding, which helps the model learn the importance of an antecedent.",6.1 Automatic Evaluation,[0],[0]
"The performance drop of CorefNQG–mention-pair score shows the mention-pair score introduced from the external system (Clark and Manning, 2016) helps the neural network better encode coreference knowledge.
",6.1 Automatic Evaluation,[0],[0]
"To better understand the effect of coreference resolution, we also evaluate our model and the baseline models on just that portion of the test set that requires pronoun resolution (36.42% of the examples) and show the results in Table 4.",6.1 Automatic Evaluation,[0],[0]
The gaps of performance between our model and the baseline models are still significant.,6.1 Automatic Evaluation,[0],[0]
"Besides, we see that all three systems’ performance drop on this partial test set, which demonstrates the hardness of generating questions for the cases that require pronoun resolution (passage context).
",6.1 Automatic Evaluation,[0],[0]
"We also show in Table 2 the results of the QG models trained on the training set augmented with noisy examples with predicted answer spans.
",6.1 Automatic Evaluation,[0],[0]
"There is a consistent but acceptable drop for each model on this new training set, given the inaccuracy of predicted answer spans.",6.1 Automatic Evaluation,[0],[0]
"We see that CorefNQG still outperforms the baseline models across all metrics.
",6.1 Automatic Evaluation,[0],[0]
Figure 3 provides sample output for input sentences that require contextual coreference knowledge.,6.1 Automatic Evaluation,[0],[0]
We see that ContextNQG fails in all cases; our model misses only the third example due to an error introduced by coreference resolution — the “city” and “it” are considered coreferent.,6.1 Automatic Evaluation,[0],[0]
"We can also see that human-generated questions are more natural and varied in form with better paraphrasing.
",6.1 Automatic Evaluation,[0],[0]
"In Table 3, we show the evaluation results for different answer extraction models.",6.1 Automatic Evaluation,[0],[0]
"First we see that all variants of BiLSTM models outperform the off-the-shelf NER system (that proposes all NEs as answer spans), though the NER system has a higher recall.",6.1 Automatic Evaluation,[0],[0]
The BiLSTM-CRF that encodes the character-level and NER features for each token performs best in terms of F-measure.,6.1 Automatic Evaluation,[0],[0]
We hired four native speakers of English to rate the systems’ outputs.,6.2 Human Study,[0],[0]
"Detailed guidelines for the raters are listed in the supplementary materials.
",6.2 Human Study,[0],[0]
The evaluation can also be seen as a measure of the quality of the generated dataset (Section 6.3).,6.2 Human Study,[0],[0]
"We randomly sampled 11 passages/paragraphs from the test set; there are in total around 70 questionanswer pairs for evaluation.
",6.2 Human Study,[0],[0]
"We consider three metrics — “grammaticality”, “making sense” and “answerability”.",6.2 Human Study,[0],[0]
The evaluators are asked to first rate the grammatical correctness of the generated question (before being shown the associated input sentence or any other textual context).,6.2 Human Study,[0],[0]
"Next, we ask them to rate the degree to which the question “makes sense” given the input sentence (i.e., without considering the correctness of the answer span).",6.2 Human Study,[0],[0]
"Finally, evaluators rate the “answerability” of the question given the full context.
",6.2 Human Study,[0],[0]
Table 5 shows the results of the human evaluation.,6.2 Human Study,[0],[0]
Bold indicates top scores.,6.2 Human Study,[0],[0]
"We see that the original human questions are preferred over the two NQG systems’ outputs, which is understandable given the examples in Figure 3.",6.2 Human Study,[0],[0]
"The humangenerated questions make more sense and correspond better with the provided answers, particularly when they require information in the preceding context.",6.2 Human Study,[0],[0]
How exactly to capture the preceding context so as to ask better and more diverse questions is an interesting future direction for research.,6.2 Human Study,[0],[0]
"In terms of grammaticality, however, the neural models do quite well, achieving very close to human performance.",6.2 Human Study,[0],[0]
"In addition, we see that our method (CorefNQG) performs statistically significantly better across all metrics in comparison to the baseline model (ContextNQG), which has access to the entire preceding context in the passage.",6.2 Human Study,[0],[0]
"Our system generates in total 1,259,691 questionanswer pairs, nearly 126 questions per article.",6.3 The Generated Corpus,[0],[0]
"Figure 5 shows the distribution of different types of
questions in our dataset vs. the SQuAD training set.",6.3 The Generated Corpus,[0],[0]
"We see that the distribution for “In what”, “When”, “How long”, “Who”, “Where”, “What does” and “What do” questions in the two datasets is similar.",6.3 The Generated Corpus,[0],[0]
"Our system generates more “What is”, “What was” and “What percentage” questions, while the proportions of “What did”, “Why” and “Which” questions in SQuAD are larger than ours.",6.3 The Generated Corpus,[0],[0]
"One possible reason is that the “Why”, “What did” questions are more complicated to ask (sometimes involving world knowledge) and the answer spans are longer phrases of various types that are harder to identify.",6.3 The Generated Corpus,[0],[0]
"“What is” and “What was” questions, on the other hand, are often safer for the neural networks systems to ask.
",6.3 The Generated Corpus,[0],[0]
"In Figure 4, we show some examples of the generated question-answer pairs.",6.3 The Generated Corpus,[0],[0]
The answer extractor identifies the answer span boundary well and all three questions correspond to their answers.,6.3 The Generated Corpus,[0],[0]
Q2 is valid but not entirely accurate.,6.3 The Generated Corpus,[0],[0]
"For more examples, please refer to our supplementary materials.
",6.3 The Generated Corpus,[0],[0]
"Table 6 shows the performance of a topperforming system for the SQuAD dataset (Document Reader (Chen et al., 2017)) when applied to the development and test set portions of our generated dataset.",6.3 The Generated Corpus,[0],[0]
The system was trained on the training set portion of our dataset.,6.3 The Generated Corpus,[0],[0]
"We use the SQuAD evaluation scripts, which calculate exact match (EM) and F-1 scores.2 Performance of the
2F-1 measures the average overlap between the predicted answer span and ground truth answer (Rajpurkar et al., 2016).
",6.3 The Generated Corpus,[0],[0]
neural machine reading model is reasonable.,6.3 The Generated Corpus,[0],[0]
"We also train the DocReader on our training set and test the models’ performance on the original dev set of SQuAD; for this, the performance is around 45.2% on EM and 56.7% on F-1 metric.",6.3 The Generated Corpus,[0],[0]
"DocReader trained on the original SQuAD training set achieves 69.5% EM, 78.8% F-1 indicating that our dataset is more difficult and/or less natural than the crowd-sourced QA pairs of SQuAD.",6.3 The Generated Corpus,[0],[0]
We propose a new neural network model for better encoding coreference knowledge for paragraphlevel question generation.,7 Conclusion,[0],[0]
Evaluations with different metrics on the SQuAD machine reading dataset show that our model outperforms state-ofthe-art baselines.,7 Conclusion,[0],[0]
The ablation study shows the effectiveness of different components in our model.,7 Conclusion,[0],[0]
"Finally, we apply our question generation framework to produce a corpus of 1.26 million questionanswer pairs, which we hope will benefit the QA research community.",7 Conclusion,[0],[0]
It would also be interesting to apply our approach to incorporating coreference knowledge to other text generation tasks.,7 Conclusion,[0],[0]
We thank the anonymous reviewers and members of Cornell NLP group for helpful comments.,Acknowledgments,[0],[0]
We study the task of generating from Wikipedia articles question-answer pairs that cover content beyond a single sentence.,abstractText,[0],[0]
We propose a neural network approach that incorporates coreference knowledge via a novel gating mechanism.,abstractText,[0],[0]
"Compared to models that only take into account sentence-level information (Heilman and Smith, 2010; Du et al., 2017; Zhou et al., 2017), we find that the linguistic knowledge introduced by the coreference representation aids question generation significantly, producing models that outperform the current state-of-theart.",abstractText,[0],[0]
"We apply our system (composed of an answer span extraction system and the passage-level QG system) to the 10,000 top-ranking Wikipedia articles and create a corpus of over one million questionanswer pairs.",abstractText,[0],[0]
We also provide a qualitative analysis for this large-scale generated corpus from Wikipedia.,abstractText,[0],[0]
Harvesting Paragraph-Level Question-Answer Pairs from Wikipedia,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4791–4796 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4791",text,[0],[0]
"Neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015) has become the de-facto standard in machine translation, outperforming earlier phrasebased approaches in many data settings and shared translation tasks (Luong and Manning, 2015; Sennrich et al., 2016; Cromieres et al., 2016).",1 Introduction,[0],[0]
"Some recent results suggest that neural machine translation “approaches the accuracy achieved by average bilingual human translators [on some test sets]” (Wu et al., 2016), or even that its “translation quality is at human parity when compared to professional human translators” (Hassan et al., 2018).",1 Introduction,[0],[0]
"Claims of human parity in machine translation are certainly extraordinary, and require extraordinary evidence.1",1 Introduction,[0],[0]
"Laudably, Hassan et al. (2018) have
1The term “parity” may raise the expectation that there is evidence for equivalence, but the term is used in the definition of “there [being] no statistical significance between [two outputs] for a test set of candidate translations” by Hassan et al. (2018).",1 Introduction,[0],[0]
"Still, we consider this finding noteworthy given the strong evaluation setup.
released their data publicly to allow external validation of their claims.",1 Introduction,[0],[0]
"Their claims are further strengthened by the fact that they follow best practices in human machine translation evaluation, using evaluation protocols and tools that are also used at the yearly Conference on Machine Translation (WMT) (Bojar et al., 2017), and take great care in guarding against some confounds such as test set selection and rater inconsistency.
",1 Introduction,[0],[0]
"However, the implications of a statistical tie between two machine translation systems in a shared translation task are less severe than that of a statistical tie between a machine translation system and a professional human translator, so we consider the results worthy of further scrutiny.",1 Introduction,[0],[0]
We perform an independent evaluation of the professional translation and best machine translation system that were found to be of equal quality by Hassan et al. (2018).,1 Introduction,[0],[0]
"Our main interest lies in the evaluation protocol, and we empirically investigate if the lack of document-level context could explain the inability of human raters to find a quality difference between human and machine translations.",1 Introduction,[0],[0]
"We test the following hypothesis:
A professional translator who is asked to rank the quality of two candidate translations on the document level will prefer a professional human translation over a machine translation.
",1 Introduction,[0],[0]
"Note that our hypothesis is slightly different from that tested by Hassan et al. (2018), which could be phrased as follows:
A bilingual crowd worker who is asked to directly assess the quality of candidate translations on the sentence level will prefer a professional human translation over a machine translation.
",1 Introduction,[0],[0]
"As such, our evaluation is not a direct replication of that by Hassan et al. (2018), and a failure to reproduce their findings does not imply an error on either our or their part.",1 Introduction,[0],[0]
"Rather, we hope to indirectly assess the accuracy of different evaluation protocols.",1 Introduction,[0],[0]
"Our underlying assumption is that professional human translation is still superior to neural machine translation, but that the sensitivity of human raters to these quality differences depends on the evaluation protocol.",1 Introduction,[0],[0]
"Machine translation is typically evaluated by comparing system outputs to source texts, reference translations, other system outputs, or a combination thereof (for examples, see Bojar et al., 2016a).",2 Human Evaluation of Machine Translation,[0],[0]
"The scientific community concentrates on two aspects: adequacy, typically assessed by bilinguals; and target language fluency, typically assessed by monolinguals.",2 Human Evaluation of Machine Translation,[0],[0]
"Evaluation protocols have been subject to controversy for decades (e. g., Van Slype, 1979), and we identify three aspects with particular relevance to assessing human parity: granularity of measurement (ordinal vs. interval scales), raters (experts vs. crowd workers), and experimental unit (sentence vs. document).",2 Human Evaluation of Machine Translation,[0],[0]
Granularity of Measurement Callison-Burch et al. (2007) show that ranking (Which of these translations is better?) leads to better inter-rater agreement than absolute judgement on 5-point Likert scales (How good is this translation?) but gives no insight about how much a candidate translation differs from a (presumably perfect) reference.,2.1 Related Work,[0],[0]
"To this end, Graham et al. (2013) suggest the use of continuous scales for direct assessment of translation quality.",2.1 Related Work,[0],[0]
"Implemented as a slider between 0 (Not at all) and 100 (Perfectly), their method yields scores on a 100-point interval scale in practice (Bojar et al., 2016b, 2017), with each raters’ rating being standardised to increase homogeneity.",2.1 Related Work,[0],[0]
Hassan et al. (2018) use source-based direct assessment to avoid bias towards reference translations.,2.1 Related Work,[0],[0]
"In the shared task evaluation by Cettolo et al. (2017), raters are shown the source and a candidate text, and asked: How accurately does the above candidate text convey the semantics of the source text?",2.1 Related Work,[0],[0]
"In doing so, they have translations produced by humans and machines rated indepen-
dently, and parity is assumed if the mean score of the former does not significantly differ from the mean score of the latter.
",2.1 Related Work,[0],[0]
"Raters To optimise cost, machine translation quality is typically assessed by means of crowdsourcing.",2.1 Related Work,[0],[0]
"Combined ratings of bilingual crowd workers have been shown to be more reliable than automatic metrics and “very similar” to ratings produced by “experts”2 (Callison-Burch, 2009).",2.1 Related Work,[0],[0]
"Graham et al. (2017) compare crowdsourced to “expert” ratings on machine translations from WMT 2012, concluding that, with proper quality control, “machine translation systems can indeed be evaluated by the crowd alone.”",2.1 Related Work,[0],[0]
"However, it is unclear whether this finding carries over to translations produced by NMT systems where, due to increased fluency, errors are more difficult to identify (Castilho et al., 2017a), and concurrent work by Toral et al. (2018) highlights the importance of expert translators for MT evaluation.
",2.1 Related Work,[0],[0]
"Experimental Unit Machine translation evaluation is predominantly performed on single sentences, presented to raters in random order (e. g., Bojar et al., 2017; Cettolo et al., 2017).",2.1 Related Work,[0],[0]
There are two main reasons for this.,2.1 Related Work,[0],[0]
"The first is cost: if raters assess entire documents, obtaining the same number of data points in an evaluation campaign multiplies the cost by the average number of sentences per document.",2.1 Related Work,[0],[0]
The second is experimental validity.,2.1 Related Work,[0],[0]
"When comparing systems that produce sentences without considering documentlevel context, the perceived suprasentential cohesion of a system output is likely due to randomness and thus a confounding factor.",2.1 Related Work,[0],[0]
"While incorporating document-level context into machine translation systems is an active field of research (Webber et al., 2017), state-of-the-art systems still operate at the level of single sentences (Sennrich et al., 2017; Vaswani et al., 2017; Hassan et al., 2018).",2.1 Related Work,[0],[0]
"In contrast, human translators can and do take document-level context into account (Krings, 1986).",2.1 Related Work,[0],[0]
The same holds for raters in evaluation campaigns.,2.1 Related Work,[0],[0]
"In the discussion of their results, Wu et al. (2016) note that their raters “[did] not necessarily fully understand each randomly sampled sentence sufficiently” because it was provided with no context.",2.1 Related Work,[0],[0]
"In such setups, raters cannot reward textual cohesion and coherence.
2“Experts” here are computational linguists who develop MT systems, who may not be expert translators.",2.1 Related Work,[0],[0]
"We conduct a quality evaluation experiment with a 2× 2 mixed factorial design, testing the effect of source text availability (adequacy, fluency) and experimental unit (sentence, document) on ratings by professional translators.
",2.2 Our Evaluation Protocol,[0],[0]
Granularity of Measurement We elicit judgements by means of pairwise ranking.,2.2 Our Evaluation Protocol,[0],[0]
"Raters choose the better (with ties allowed) of two translations for each item: one produced by a professional translator (HUMAN), the other by machine translation (MT).",2.2 Our Evaluation Protocol,[0],[0]
"Since our evaluation includes that of human translation, it is reference-free.",2.2 Our Evaluation Protocol,[0],[0]
"We evaluate in two conditions: adequacy, where raters see source texts and translations (Which translation expresses the meaning of the source text more adequately?); and fluency, where raters only see translations (Which text is better English?).
",2.2 Our Evaluation Protocol,[0],[0]
"Raters We recruit professional translators, only considering individuals with at least three years of professional experience and positive client reviews.
",2.2 Our Evaluation Protocol,[0],[0]
"Experimental Unit To test the effect of context on perceived translation quality, raters evaluate entire documents as well as single sentences in random order (i. e., context is a within-subjects factor).",2.2 Our Evaluation Protocol,[0],[0]
"They are shown both translations (HUMAN and MT) for each unit; the source text is only shown in the adequacy condition.
",2.2 Our Evaluation Protocol,[0],[0]
"Quality Control To hedge against random ratings, we convert 5 documents and 16 sentences per set into spam items (Kittur et al., 2008): we render one of the two options nonsensical by shuffling its words randomly, except for 10 % at the beginning and end.
",2.2 Our Evaluation Protocol,[0],[0]
Statistical Analysis We test for statistically significant preference of HUMAN over MT or vice versa by means of two-sided Sign Tests.,2.2 Our Evaluation Protocol,[0],[0]
"Let a be the number of ratings in favour of MT, b the number of ratings in favour of HUMAN, and t the number of ties.",2.2 Our Evaluation Protocol,[0],[0]
"We report the number of successes x and the number of trials n for each test, such that x = b and n = a+ b.3
3Emerson and Simon (1979) suggest the inclusion of ties such that x = b+0.5t and n = a+ b+ t. This modification has no effect on the significance levels reported in this paper.",2.2 Our Evaluation Protocol,[0],[0]
We use the experimental protocol described in the previous section for a quality assessment of Chinese to English translations of news articles.,2.3 Data Collection,[0],[0]
"To this end, we randomly sampled 55 documents and 2×120 sentences from the WMT 2017 test set.",2.3 Data Collection,[0],[0]
"We only considered the 123 articles (documents) which are native Chinese,4 containing 8.13 sentences on average.",2.3 Data Collection,[0],[0]
"Human and machine translations (REFERENCE-HT as HUMAN, and COMBO6 as MT) were obtained from data released by Hassan et al. (2018).5
The sampled documents and sentences were rated by professional translators we recruited from ProZ:6 4 native in Chinese (2), English (1), or both (1) to rate adequacy, and 4 native in English to rate fluency.",2.3 Data Collection,[0],[0]
"On average, translators had 13.7 years of experience and 8.8 positive client reviews on ProZ, and received US$ 188.75 for rating 55 documents and 120 sentences.
",2.3 Data Collection,[0],[0]
"The averages reported above include an additional translator we recruited when one rater showed poor performance on document-level spam items in the fluency condition, whose judgements we exclude from analysis.",2.3 Data Collection,[0],[0]
"We also exclude sentence-level results from 4 raters because there was overlap with the documents they annotated, which means that we cannot rule out that the sentence-level decisions were informed by access to the full document.",2.3 Data Collection,[0],[0]
"To allow for external validation and further experimentation, we make all experimental data publicly available.7",2.3 Data Collection,[0],[0]
"In the adequacy condition, MT and HUMAN are not statistically significantly different on the sentence level (x=86, n=189, p= .244).",3 Results,[0],[0]
This is consistent with the results Hassan et al. (2018) obtained with an alternative evaluation protocol (crowdsourcing and direct assessment; see Section 2.1).,3 Results,[0],[0]
"However, when evaluating entire doc-
4While it is common practice in machine translation to use the same test set in both translation directions, we consider a direct comparison between human “translation” and machine translation hard to interpret if one is in fact the original English text, and the other an automatic translation into English of a human translation into Chinese.",3 Results,[0],[0]
"In concurrent work, Toral et al. (2018) expand on the confounding effect of evaluating text where the target side is actually the original document.
5 http://aka.ms/Translator-HumanParityData 6 https://www.proz.com 7 https://github.com/laeubli/parity
uments, raters show a statistically significant preference for HUMAN (x=104, n=178, p<.05).",3 Results,[0],[0]
"While the number of ties is similar in sentenceand document-level evaluation, preference for MT drops from 50 to 37 % in the latter (Figure 1a).
",3 Results,[0],[0]
"In the fluency condition, raters prefer HUMAN on both the sentence (x= 106, n=172, p<.01) and document level (x=99, n=143, p< .001).",3 Results,[0],[0]
"In contrast to adequacy, fluency ratings in favour of HUMAN are similar in sentence- and document-level evaluation, but raters find more ties with document-level context as preference for MT drops from 32 to 22 % (Figure 1b).
",3 Results,[0],[0]
We note that these large effect sizes lead to statistical significance despite modest sample size.,3 Results,[0],[0]
Inter-annotator agreement (Cohen’s κ) ranges from 0.13 to 0.32 (see Appendix for full results and discussion).,3 Results,[0],[0]
Our results emphasise the need for suprasentential context in human evaluation of machine translation.,4 Discussion,[0],[0]
"Starting with Hassan et al.’s (2018) finding of no statistically significant difference in translation quality between HUMAN and MT for their Chinese–English test set, we set out to test this result with an alternative evaluation protocol which we expected to strengthen the ability of raters to judge translation quality.",4 Discussion,[0],[0]
"We employed professional translators instead of crowd workers, and pairwise ranking instead of direct assessment, but in a sentence-level evaluation of adequacy, raters still found it hard to discriminate between HUMAN and MT: they did not show a statistically significant preference for either of them.
",4 Discussion,[0],[0]
"Conversely, we observe a tendency to rate HUMAN more favourably on the document level than on the sentence level, even within single raters.",4 Discussion,[0],[0]
Adequacy raters show a statistically significant preference for HUMAN when evaluating entire documents.,4 Discussion,[0],[0]
"We hypothesise that document-level evaluation unveils errors such as mistranslation of an ambiguous word, or errors related to textual cohesion and coherence, which remain hard or impossible to spot in a sentence-level evaluation.",4 Discussion,[0],[0]
"For a subset of articles, we elicited both sentence-level and document-level judgements, and inspected articles for which sentence-level judgements were mixed, but where HUMAN was strongly preferred in document-level evaluation.",4 Discussion,[0],[0]
"In these articles, we do indeed observe the hypothesised phenomena.",4 Discussion,[0],[0]
"We find an example of lexical coherence in a 6-sentence article about a new app “微信挪 车”, which HUMAN consistently translates into “WeChat Move the Car”.",4 Discussion,[0],[0]
"In MT, we find three different translations in the same article: “Twitter Move Car”, “WeChat mobile”, and “WeChat Move”.",4 Discussion,[0],[0]
"Other observations include the use of more appropriate discourse connectives in HUMAN, a more detailed investigation of which we leave to future work.
",4 Discussion,[0],[0]
"To our surprise, fluency raters show a stronger preference for HUMAN than adequacy raters (Figure 1).",4 Discussion,[0],[0]
"The main strength of neural machine translation in comparison to previous statistical approaches was found to be increased fluency, while adequacy improvements were less clear (Bojar et al., 2016b; Castilho et al., 2017b), and we expected a similar pattern in our evaluation.",4 Discussion,[0],[0]
"Does this indicate that adequacy is in fact a strength of
MT, not fluency?",4 Discussion,[0],[0]
We are wary to jump to this conclusion.,4 Discussion,[0],[0]
"An alternative interpretation is that MT, which tends to be more literal than HUMAN, is judged more favourably by raters in the bilingual condition, where the majority of raters are native speakers of the source language, because of L1 interference.",4 Discussion,[0],[0]
We note that the availability of document-level context still has a strong impact in the fluency condition (Section 3).,4 Discussion,[0],[0]
"In response to recent claims of parity between human and machine translation, we have empirically tested the impact of sentence and document level context on human assessment of machine translation.",5 Conclusions,[0],[0]
"Raters showed a markedly stronger preference for human translations when evaluating at the level of documents, as compared to an evaluation of single, isolated sentences.
",5 Conclusions,[0],[0]
We believe that our findings have several implications for machine translation research.,5 Conclusions,[0],[0]
"Most importantly, if we accept our interpretation that human translation is indeed of higher quality in the dataset we tested, this points to a failure of current best practices in machine translation evaluation.",5 Conclusions,[0],[0]
"As machine translation quality improves, translations will become harder to discriminate in terms of quality, and it may be time to shift towards document-level evaluation, which gives raters more context to understand the original text and its translation, and also exposes translation errors related to discourse phenomena which remain invisible in a sentence-level evaluation.
",5 Conclusions,[0],[0]
"Our evaluation protocol was designed with the aim of providing maximal validity, which is why we chose to use professional translators and pairwise ranking.",5 Conclusions,[0],[0]
"For future work, it would be of high practical relevance to test whether we can also elicit accurate quality judgements on the document-level via crowdsourcing and direct assessment, or via alternative evaluation protocols.",5 Conclusions,[0],[0]
"The data released by Hassan et al. (2018) could serve as a test bed to this end.
",5 Conclusions,[0],[0]
"One reason why document-level evaluation widens the quality gap between machine translation and human translation is that the machine translation system we tested still operates on the sentence level, ignoring wider context.",5 Conclusions,[0],[0]
It will be interesting to explore to what extent existing and future techniques for document-level machine translation can narrow this gap.,5 Conclusions,[0],[0]
"We ex-
pect that this will require further efforts in creating document-level training data, designing appropriate models, and supporting research with discourse-aware automatic metrics.",5 Conclusions,[0],[0]
We thank Xin Sennrich for her help with the analysis of translation errors.,Acknowledgements,[0],[0]
We also thank Antonio Toral and the anonymous reviewers for their helpful comments.,Acknowledgements,[0],[0]
Recent research suggests that neural machine translation achieves parity with professional human translation on the WMT Chinese– English news translation task.,abstractText,[0],[0]
"We empirically test this claim with alternative evaluation protocols, contrasting the evaluation of single sentences and entire documents.",abstractText,[0],[0]
"In a pairwise ranking experiment, human raters assessing adequacy and fluency show a stronger preference for human over machine translation when evaluating documents as compared to isolated sentences.",abstractText,[0],[0]
Our findings emphasise the need to shift towards document-level evaluation as machine translation improves to the degree that errors which are hard or impossible to spot at the sentence-level become decisive in discriminating quality of different translation outputs.,abstractText,[0],[0]
Has Machine Translation Achieved Human Parity? A Case for Document-level Evaluation,title,[0],[0]
"Sequential LSTMs have been extended to model tree structures, giving competitive results for a number of tasks. Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes. This is different from sequential LSTMs, which contain references to input words for each node. In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node. In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure. Experiments show that both extensions give better representations of tree structures. Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.",text,[0],[0]
Both sequence structured and tree structured neural models have been applied to NLP problems.,1 Introduction,[0],[0]
"Seminal work uses convolutional neural networks (Collobert and Weston, 2008), recurrent neural networks (Elman, 1990; Mikolov et al., 2010) and recursive neural networks (Socher et al., 2011) for sequence and tree modeling.",1 Introduction,[0],[0]
"Long short-term memory (LSTM) networks have significantly improved accuracies in a variety of sequence tasks (Sutskever et al., 2014; Bahdanau et al., 2015) compared to
vanilla recurrent neural networks.",1 Introduction,[0],[0]
"Addressing diminishing gradients effectively, they have been extended to tree structures, achieving promising results for tasks such as syntactic language modeling (Zhang et al., 2016), sentiment analysis (Li et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015; Tai et al., 2015; Teng et al., 2016) and relation extraction (Miwa and Bansal, 2016).
",1 Introduction,[0],[0]
"Depending on the node type, typical tree structures in NLP can be categorized to constituent trees and dependency trees.",1 Introduction,[0],[0]
A salient difference between the two types of tree structures is in the node.,1 Introduction,[0],[0]
"While dependency tree nodes are input words themselves, constituent tree nodes represent syntactic constituents.",1 Introduction,[0],[0]
Only leaf nodes in constituent trees correspond to words.,1 Introduction,[0],[0]
"Though LSTM structures have been developed for both types of trees above, we investigate constituent trees in this paper.",1 Introduction,[0],[0]
"There are three existing methods for constituent tree LSTMs (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which make essentially the same extension from sequence structured LSTMs.",1 Introduction,[0],[0]
"We take the method of Zhu et al. (2015) as our baseline.
",1 Introduction,[0],[0]
"Figure 1 shows the sequence structured LSTM of Hochreiter and Schmidhuber (1997) and the treestructured LSTM of Zhu et al. (2015), illustrating the input (x), cell (c) and hidden (h) nodes at a certain time step t. The most important difference between Figure 1(a) and Figure 1(b) is the branching factor.",1 Introduction,[0],[0]
"While a cell in the sequence structure LSTM depends on the single previous hidden node, a cell in the tree-structured LSTM depends on a left hidden node and a right hidden node.",1 Introduction,[0],[0]
"Such tree-structured extensions of the sequence structured LSTM assume
163
Transactions of the Association for Computational Linguistics, vol. 5, pp.",1 Introduction,[0],[0]
"163–177, 2017.",1 Introduction,[0],[0]
Action Editor: Scott Yih.,1 Introduction,[0],[0]
"Submission batch: 5/2016; Revision batch: 12/2016; Published 6/2017.
",1 Introduction,[0],[0]
c©2017 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
that the constituent tree is binarized, building hidden nodes from the input words in the bottom-up direction.",1 Introduction,[0],[0]
"The leaf node structure is shown in Figure 1(c).
",1 Introduction,[0],[0]
A second salient difference between the two types of LSTMs is the modeling of input words.,1 Introduction,[0],[0]
"While each cell in the sequence structure LSTM directly depends on its corresponding input word (Figure 1(a)), only leaf cells in the tree structure LSTM directly depend on corresponding input words (Figure 1(c)).",1 Introduction,[0],[0]
"This corresponds well to the constituent tree structure, where there is no direct association between non-leaf constituent nodes and input words.",1 Introduction,[0],[0]
"However, it leaves the tree structure a degraded version of a perfect binary-branching variation of the sequence-structure LSTM, with one important source of information (i.e. words) missing in forming a cell (Figure 1(b)).
",1 Introduction,[0],[0]
"We fill this gap by proposing an extension to the tree LSTM model, injecting lexical information into every node in the tree.",1 Introduction,[0],[0]
"Our method takes inspiration from work on head-lexicalization, which shows that each node in a constituent tree structure is governed by a head word.",1 Introduction,[0],[0]
"As shown in Figure 2, the head word for the verb phrase “visited Mary” is “visited”, and the head word of the adverb phrase “this afternoon” is “afternoon”.",1 Introduction,[0],[0]
"Research has shown that head word information can significantly improve the performance of syntactic parsing (Collins, 2003; Clark and Curran, 2004).",1 Introduction,[0],[0]
"Correspondingly, we use the head lexical information of each constituent word as the input node x for calculating the corresponding cell c in Figure 1(b).
",1 Introduction,[0],[0]
"Traditional head-lexicalization relies on specific rules (Collins, 2003; Zhang and Clark, 2009), typically extracting heads from constituent treebanks according to certain grammar formalisms.",1 Introduction,[0],[0]
"For better generalization, we use a neural attention mechanism to derive head lexical information automatically, rather than relying on linguistic head rules to find the head lexicon of each constituent, which is language- and formalism-dependent.
",1 Introduction,[0],[0]
"Based on such head lexicalization, we further make a bidirectional extension of the tree structured LSTM, propagating information in the top-down direction as well as the bottom-up direction.",1 Introduction,[0],[0]
"This is analogous to the bidirectional extension of sequence structured LSTMs, which are commonly used for NLP tasks such as speech recognition (Graves et al., 2013), sentiment analysis (Tai et al., 2015; Li et al., 2015) and machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) tasks.
",1 Introduction,[0],[0]
Results on a standard sentiment classification benchmark and a question type classification benchmark show that our tree LSTM structure gives significantly better accuracies compared with the method of Zhu et al. (2015).,1 Introduction,[0],[0]
We achieve the best reported results for sentiment classification.,1 Introduction,[0],[0]
"Interestingly, the head lexical information that is learned automatically from the sentiment treebank consists of both syntactic head information and key sentiment word information.",1 Introduction,[0],[0]
This shows the advantage of automatic head-finding as compared with rule-based head lexicalization.,1 Introduction,[0],[0]
We make our code available under GPL at https://github.com/ zeeeyang/lexicalized_bitreelstm.,1 Introduction,[0],[0]
"LSTM Recurrent neural network (RNN) (Elman, 1990; Mikolov et al., 2010) achieves success on
modeling linear structures due to its ability to preserve history over arbitrary length sequences.",2 Related Work,[0],[0]
"At each step, RNN decides its hidden state based on both the current input and the previous hidden state.",2 Related Work,[0],[0]
"In theory, it can carry over unbounded history.",2 Related Work,[0],[0]
"Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) is a special type of RNN that leverages multiple gate vectors and a memory cell vector to solve the vanishing and exploding gradient problems of training RNNs.",2 Related Work,[0],[0]
"It has been successfully applied to parsing (Vinyals et al., 2015a), sentiment classification (Tai et al., 2015; Li et al., 2015), speech recognition (Graves et al., 2013), machine translation (Sutskever et al., 2014; Bahdanau et al., 2015) and image captioning (Vinyals et al., 2015b).",2 Related Work,[0],[0]
"There are many variants of sequential LSTMs, such as simple Gated Recurrent Neural Networks (Cho et al., 2014).",2 Related Work,[0],[0]
Greff et al. (2017) compared various architectures of LSTM.,2 Related Work,[0],[0]
"In this paper, we take the standard LSTM with peephole connections (Gers and Schmidhuber, 2000) as a baseline.
",2 Related Work,[0],[0]
Structured LSTM There has been a line of research that extends the standard sequential LSTM in order to model more complex structures.,2 Related Work,[0],[0]
Kalchbrenner et al. (2016) proposed Grid LSTMs to process multi-dimensional data.,2 Related Work,[0],[0]
Theis and Bethge (2015) proposed Spatial LSTMs to handle image data.,2 Related Work,[0],[0]
Dyer et al. (2015) designed Stack LSTMs by adding a top pointer to sequential LSTMs to deal with push and pop sequences of a stack.,2 Related Work,[0],[0]
"Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015) extended sequential LSTMs to Tree-Structured LSTMs (Tree LSTMs) by adding branching factors.",2 Related Work,[0],[0]
"Experiments demonstrated that Tree LSTMs can outperform competitive LSTM baselines on several tasks, such as semantic relatedness prediction and sentiment classification.",2 Related Work,[0],[0]
Li et al. (2015) further investigated the effectiveness of Tree LSTMs on various tasks and discussed when Tree LSTMs are necessary.,2 Related Work,[0],[0]
"In addition, Li et al. (2016) employed graph gated units to model graph-based structures.
",2 Related Work,[0],[0]
Tree LSTM,2 Related Work,[0],[0]
"The idea of extending linear recurrent structures to tree recurrent structures is reminiscent of extending Recurrent Neural Network to Recursive Neural Network (ReNN) (Socher et al., 2013b; Le and Zuidema, 2014) to support information flow over trees.",2 Related Work,[0],[0]
"In addition to Tai et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), who
explicitly named their models as Tree LSTMs, Cho et al. (2014) designed gated recurrent units over tree structures, and Chen et al. (2015) introduced gate mechanisms to recursive neural networks.",2 Related Work,[0],[0]
"These can also be regarded as variants of Tree LSTMs.
",2 Related Work,[0],[0]
"Both Zhu et al. (2015) and Le and Zuidema (2015) proposed Binary Tree LSTM models, which can be applied to situations where there are exactly two children of each internal node in a tree.",2 Related Work,[0],[0]
"The difference between Zhu et al. (2015) and Le and Zuidema (2015) is that besides using two forget gates, Le and Zuidema (2015) also make use of two input gates to let a node know its sibling.",2 Related Work,[0],[0]
Tai et al. (2015) introduced Child-Sum Tree LSTM and Nary Tree LSTM.,2 Related Work,[0],[0]
"Child-Sum Tree LSTMs can support multiple children, while N-ary Tree LSTMs work for trees with a branching factor of at most N .",2 Related Work,[0],[0]
"In this perspective, Binary Tree LSTM is a special case of N-ary Tree LSTM with N = 2.
",2 Related Work,[0],[0]
"When a Child-Sum Tree LSTM is applied to a dependency tree, it is referred to as a Dependency Tree LSTM.",2 Related Work,[0],[0]
A Binary Tree LSTM is also referred to as a Constituent Tree LSTM.,2 Related Work,[0],[0]
"Based on Tai et al. (2015), Miwa and Bansal (2016) introduced a Tree LSTM model that can handle different types of children.",2 Related Work,[0],[0]
"A dependency tree naturally contains lexical information at every node, while only leaf nodes contain lexical information in a constituent tree.",2 Related Work,[0],[0]
"None of these methods (Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015) make direct use of lexical input for internal nodes when using constituent Tree LSTMs.
",2 Related Work,[0],[0]
"Bi-LSTM Another common extension to sequential LSTM is to include bidirectional information (Graves et al., 2013), which can model history both left-to-right and right-to-left.",2 Related Work,[0],[0]
"The aforementioned Tree LSTM models (Tai et al., 2015; Zhu et al., 2015; Le and Zuidema, 2015) propagate the history of children to their parent in the bottom-up direction only, while ignoring the top-down information flow from parents to children.",2 Related Work,[0],[0]
Zhang et al. (2016) proposed a top-down Tree LSTM to estimate the generation probability of a dependency tree.,2 Related Work,[0],[0]
"However, no corresponding bottom-up Tree LSTM is incorporated into their model.
",2 Related Work,[0],[0]
Paulus et al. (2014) leveraged bidirectional information over recursive binary trees by propagating global belief down from the tree root to leaf nodes.,2 Related Work,[0],[0]
"However, their model is based on recursive neural
network rather than LSTM.",2 Related Work,[0],[0]
Miwa and Bansal (2016) adopted a bidirectional Tree LSTM model to jointly extract named entities and relations under a dependency tree structure.,2 Related Work,[0],[0]
"For constituent tree structures, however, their model does not work due to lack of word inputs on non-leaf constituent nodes, and in particular the root node.",2 Related Work,[0],[0]
Our head lexicalization allows us to investigate the top-down constituent Tree LSTM.,2 Related Work,[0],[0]
"To our knowledge, we are the first to report a bidirectional constituent Tree LSTM.",2 Related Work,[0],[0]
"A sequence-structure LSTM estimates a sequence of hidden state vectors given a sequence of input vectors, through the calculation of a sequence of hidden cell vectors using a gate mechanism.",3 Baselines,[0],[0]
"For NLP, the input vectors are typically word embeddings (Mikolov et al., 2013), but can also include partof-speech (POS) embeddings, character embeddings or other types of information.",3 Baselines,[0],[0]
"For notational convenience, we refer to the input vectors as lexical vectors.
",3 Baselines,[0],[0]
"Formally, given an input vector sequence x1, x2, . . .",3 Baselines,[0],[0]
", xn, each state vector ht is estimated from the Hadamard product of a cell vector ct and a corresponding output gate vector ot
ht = ot ⊗ tanh(ct) (1)
Here the cell vector depends on both the previous cell vector ct, and a combination of the previous state vector ht−1; the current input vector xt:
ct = ft ⊗ ct−1 + it ⊗ gt gt = tanh(Wxgxt +Whght−1 + bg)
(2)
",3 Baselines,[0],[0]
"The combination of ct−1 and gt is controlled by the Hadamard product between a forget gate vector ft and an input gate vector it, respectively.",3 Baselines,[0],[0]
"The gates ot, ft and it are defined as follows
it = σ(Wxixt +Whiht−1 +Wcict−1 + bi)
ft = σ(Wxfxt",3 Baselines,[0],[0]
"+Whfht−1 +Wcfct−1 + bf )
",3 Baselines,[0],[0]
"ot = σ(Wxoxt +Whoht−1 +Wcoct + bo),
(3)
where σ is the sigmoid function.",3 Baselines,[0],[0]
"Wxg, Whg, bg, Wxi, Whi, Wci, bi, Wxf , Whf , Wcf , bf , Wxo, Who, Wco and bo are model parameters.
",3 Baselines,[0],[0]
"The bottom-up Tree LSTM of Zhu et al. (2015) extends the left-to-right sequence LSTM by splitting
the previous state vector ht−1 into a left child state vector hLt−1 and a right child state vector h R t−1, and the previous cell vector ct−1 into a left child cell vector cLt−1 and a right child cell vector c R t−1, calculating ct as
ct = f L t ⊗ cLt−1 + fRt ⊗ cRt−1 +",3 Baselines,[0],[0]
"it ⊗ gt, (4)
and the input/output gates it/ot as
it = σ",3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhih N",3 Baselines,[0],[0]
t−1 +W N ci c N t−1) +,3 Baselines,[0],[0]
"bi
)
",3 Baselines,[0],[0]
ot = σ,3 Baselines,[0],[0]
"( ∑
N∈{L,R} WNhoh N t−1 +Wcoct + bo
) (5)
",3 Baselines,[0],[0]
"The forget gate ft is split into fLt and f R t for regulating cLt−1 and c R t−1, respectively:
fLt = σ",3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhflh N t−1 +W N cfl cNt−1)",3 Baselines,[0],[0]
"+ bfl
)
",3 Baselines,[0],[0]
fRt = σ,3 Baselines,[0],[0]
"( ∑
N∈{L,R} (WNhfrh N t−1 +W N cfrc N t−1) + bfr
)
(6)
",3 Baselines,[0],[0]
"gt depends on both hLt−1 and h R t−1, but as shown in Figure 1 (b), it does not depend on xt
gt = tanh ( ∑
N∈{L,R} WNhgh N t−1 + bg
) (7)
Finally, the hidden state vector ht is calculated in the same way as in the sequential LSTM model shown in Equation 1.",3 Baselines,[0],[0]
"WLhi, W R hi, W L ci , W R ci , bi, W L ho, WRho, Wco, bo, W L hfl , WRhfl , W L cfl , WRcfl , bfl , W L hfr
, WRhfr , W L cfr , WRcfr , bfr , W L hg, W R hg",3 Baselines,[0],[0]
and bg are model parameters.,3 Baselines,[0],[0]
We introduce an input lexical vector xt to the calculation of each cell vector ct via a bottom-up head propagation mechanism.,4.1 Head Lexicalization,[0],[0]
"As shown in the shaded nodes in Figure 3 (b), the head propagation mechanism is parallel to the cell propagation mechanism.",4.1 Head Lexicalization,[0],[0]
"In contrast, the method of Zhu et al. (2015) in Figure 3 (a) does not have the input vector xt for non-leaf constituents.
",4.1 Head Lexicalization,[0],[0]
There are multiple ways to choose a head lexicon for a given binary-branching constituent.,4.1 Head Lexicalization,[0],[0]
"One
simple method is to choose the head lexicon of the left child as the head (left-headedness).",4.1 Head Lexicalization,[0],[0]
"Correspondingly, an alternative is to use the right child for head lexicon.",4.1 Head Lexicalization,[0],[0]
There is less consistency in the governing head lexicons across variations of the same type of constituents with slightly different typologies.,4.1 Head Lexicalization,[0],[0]
"Hence, simple baselines can be less effective compared to linguistically motivated head findings.
",4.1 Head Lexicalization,[0],[0]
"Rather than selecting head lexicons using manually-defined head-finding rules, which are language- and formalism-dependent (Collins, 2003), we cast head finding as a part of the neural network model, learning the head lexicon of each constituent by a gated combination of the head lexicons of its two children1.",4.1 Head Lexicalization,[0],[0]
"Formally,
xt",4.1 Head Lexicalization,[0],[0]
= zt ⊗ xLt−1 +,4.1 Head Lexicalization,[0],[0]
"(1− zt)⊗ xRt−1, (8)
where xt represents the head lexicon vector of the current constituent, xLt−1 represents the head lexicon of its left child constituent, and xRt−1 represents the head lexicon of its right child constituent.",4.1 Head Lexicalization,[0],[0]
"The gate zt is calculated based on xLt−1 and x R t−1,
zt = σ(W L zxx L t−1 +W R zxx R t−1 + bz) (9)
Here WLzx, W R zx and bz are model parameters.",4.1 Head Lexicalization,[0],[0]
"Given head lexicon vectors for nodes, the Tree LSTM of Zhu et al. (2015) can be extended by leveraging xt in calculating the corresponding ct.",4.2 Lexicalized Tree LSTM,[0],[0]
"In particular, xt is used to estimate the input (it), output
1In this paper, we work on binary trees only, which is a common form for CKY and shift-reduce parsing.",4.2 Lexicalized Tree LSTM,[0],[0]
"Typical binarization methods, such as head binarization (Klein and Manning, 2003) , also rely on specific head-finding rules.
",4.2 Lexicalized Tree LSTM,[0],[0]
(ot) and forget (fRt and f L t ),4.2 Lexicalized Tree LSTM,[0],[0]
"gates:
it = σ",4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxixt+
∑
N∈{L,R} (WNhih N",4.2 Lexicalized Tree LSTM,[0],[0]
t−1 +W N ci c N t−1) +,4.2 Lexicalized Tree LSTM,[0],[0]
"bi
)
",4.2 Lexicalized Tree LSTM,[0],[0]
fLt = σ,4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxfxt+
∑
N∈{L,R} (WNhflh N t−1 +W N cfl cNt−1)",4.2 Lexicalized Tree LSTM,[0],[0]
"+ bfl
)
",4.2 Lexicalized Tree LSTM,[0],[0]
fRt = σ,4.2 Lexicalized Tree LSTM,[0],[0]
"( Wxfxt+
∑
N∈{L,R} (WNhfrh N t−1 +W N cfrc N t−1) + bfr
)
",4.2 Lexicalized Tree LSTM,[0],[0]
"ot = σ ( Wxoxt+
∑
N∈{L,R} WNhoh N t−1 +Wcoct + bo
)
(10)
",4.2 Lexicalized Tree LSTM,[0],[0]
"In addition, xt is also used in computing gt,
gt = tanh ( Wxgxt + ∑
N∈{L,R} WNhgh N t−1 + bg
) (11)
",4.2 Lexicalized Tree LSTM,[0],[0]
"With the new definition of it, fRt , f L t and gt, the computing of ct remains the same as the baseline Tree LSTM model as shown in Equation 4.",4.2 Lexicalized Tree LSTM,[0],[0]
"Similarly, ht remains the Hadamard product of ct and the new ot as shown in Equation 1.
",4.2 Lexicalized Tree LSTM,[0],[0]
"In this model, Wxi, Wxf , Wxg and Wxo are newly-introduced model parameters.",4.2 Lexicalized Tree LSTM,[0],[0]
The use of xt in computing the gate and cell values are consistent with those in the baseline sequential LSTM.,4.2 Lexicalized Tree LSTM,[0],[0]
Given a sequence of input vectors,4.3 Bidirectional Extensions,[0],[0]
"[x1, x2, . . .",4.3 Bidirectional Extensions,[0],[0]
", xn], a bidirectional sequential LSTM (Graves et al., 2013) computes two sets of hidden state vectors, [h̃1, h̃2, . . .",4.3 Bidirectional Extensions,[0],[0]
", h̃n] and",4.3 Bidirectional Extensions,[0],[0]
"[h̃′n, h̃ ′",4.3 Bidirectional Extensions,[0],[0]
"n−1, . .",4.3 Bidirectional Extensions,[0],[0]
.,4.3 Bidirectional Extensions,[0],[0]
", h̃ ′",4.3 Bidirectional Extensions,[0],[0]
"1] in the left-to-right and the right-to-left directions, respectively.",4.3 Bidirectional Extensions,[0],[0]
"The final hidden state hi of the input xi is the concatenation of the corresponding state vectors in the two LSTMs,",4.3 Bidirectional Extensions,[0],[0]
hi = h̃i ⊕ h̃′n−i+1 (12),4.3 Bidirectional Extensions,[0],[0]
The two LSTMs can share the same model parameters or use different parameters.,4.3 Bidirectional Extensions,[0],[0]
"We choose the latter in our baseline experiments.
",4.3 Bidirectional Extensions,[0],[0]
"We make a bidirectional extension to the Lexicalized Tree LSTM in Section 4.2 by following the sequential LSTMs in Section 3, adding an additional
set of hidden state vectors in the top-down direction.",4.3 Bidirectional Extensions,[0],[0]
"Different from the bottom-up direction, each hidden state in the top-down LSTM has exactly one predecessor.",4.3 Bidirectional Extensions,[0],[0]
"In fact, the path from the root of a tree down to any node forms a sequential LSTM.
",4.3 Bidirectional Extensions,[0],[0]
"Note, however, that two different sets of model parameters are used when the current node is the left and the right child of its predecessor.",4.3 Bidirectional Extensions,[0],[0]
"Denoting the two sets of parameters as UL and UR, respectively, the hidden state vector h7 in Figure 4 is calculated from the hidden state vector h1 using the parameter set sequence [UL,UL,UR].",4.3 Bidirectional Extensions,[0],[0]
"Similarly, h8 is calculated from h1 using [UL,UR,UL].",4.3 Bidirectional Extensions,[0],[0]
"At each step t, the computing of ht follows the sequential LSTM model:
ht = ot ⊗ tanh(ct−1)",4.3 Bidirectional Extensions,[0],[0]
ct = ft ⊗ ct−1 + it ⊗ gt gt =,4.3 Bidirectional Extensions,[0],[0]
tanh(W N xg↓xt−1,4.3 Bidirectional Extensions,[0],[0]
"+W N hg↓ht−1 + b N g↓)
(13)
",4.3 Bidirectional Extensions,[0],[0]
"With the gate values being defined as:
it = σ(W N xi↓xt +W N hi↓ht−1 +W N ci↓ct−1 + b N i↓) ft = σ(W N xf↓xt",4.3 Bidirectional Extensions,[0],[0]
+W N,4.3 Bidirectional Extensions,[0],[0]
hf↓ht−1,4.3 Bidirectional Extensions,[0],[0]
+W N cf↓ct−1 + b N f↓),4.3 Bidirectional Extensions,[0],[0]
ot = σ(W N xo↓xt,4.3 Bidirectional Extensions,[0],[0]
"+W N ho↓ht−1 +W N co↓ct + b N o↓)
",4.3 Bidirectional Extensions,[0],[0]
"(14)
Here N ∈ {L,R} and UN = {WNxg↓,WNhg↓, bNg↓,W N xi↓,W N",4.3 Bidirectional Extensions,[0],[0]
"hi↓,W N ci↓, b N i↓ ,W N xf↓,W N hf↓,W N cf↓, b N f↓, WNxo↓,W N ho↓,W N",4.3 Bidirectional Extensions,[0],[0]
"co↓, b N o↓}.",4.3 Bidirectional Extensions,[0],[0]
UL and UR are model parameters in the top-down Tree LSTM.,4.3 Bidirectional Extensions,[0],[0]
"One final note is that the top-down Tree LSTM is enabled by the head propagation mechanism, which allows a head lexicon node to be made available for the root constituent node.",4.3 Bidirectional Extensions,[0],[0]
"Without such information, it would be difficult to build top-down LSTM for constituent trees.",4.3 Bidirectional Extensions,[0],[0]
"We apply the bidirectional Tree LSTM to classification tasks, where the input is a sentence with its binarized constituent tree, and the output is a discrete label.",5 Usage for Classification,[0],[0]
"We denote the bottom-up hidden state vector of the root as h̃ROOT↑, the top-down hidden state vector of the root as h̃ROOT↓ and the top-down hidden state vectors of the input words x1, x2, . . .",5 Usage for Classification,[0],[0]
", xn as h̃′1, h̃ ′ 2, . . .",5 Usage for Classification,[0],[0]
", h̃ ′",5 Usage for Classification,[0],[0]
n.,5 Usage for Classification,[0],[0]
"We take the concatenation of h̃ROOT↑, h̃ROOT↓ and the average of h̃′1, h̃ ′ 2, . . .",5 Usage for Classification,[0],[0]
", h̃ ′ n",5 Usage for Classification,[0],[0]
"as the final representation h of the sentence:
h = h̃ROOT↑ ⊕",5 Usage for Classification,[0],[0]
"h̃ROOT↓ ⊕ 1
n
n∑
i=1
h̃′i (15)
",5 Usage for Classification,[0],[0]
"A softmax classifier is used to predict the probability pj of sentiment label j from h by
hl = ReLU(Whlh+ bhl) P = softmax(Wlphl + blp)
pj = P",5 Usage for Classification,[0],[0]
"[j],
(16)
where Whl, bhl, Wlp and blp are model parameters, and ReLU is the rectifier function f(x) = max(0, x).",5 Usage for Classification,[0],[0]
"During prediction, the largest probability component of P will be taken as the answer.",5 Usage for Classification,[0],[0]
We train our classifier to maximize the conditional log-likelihood of gold labels of training samples.,6 Training,[0],[0]
"Formally, given a training set of size |D|, the training objective is defined by
L(Θ) =",6 Training,[0],[0]
"− |D|∑
i=1
log pyi + λ
2 ||Θ||2, (17)
where Θ is the set of model parameters, λ is a regularization parameter, yi is the gold label of the ith training sample and pyi is obtained according to Equation 16.",6 Training,[0],[0]
"For sequential LSTM models, we collect errors over each sequence.",6 Training,[0],[0]
"For Tree LSTMs, we sum up errors at every node.
",6 Training,[0],[0]
"The model parameters are optimized using ADAM (Kingma and Ba, 2015) without gradient clipping, with the default hyper-parameters of the AdamTrainer in the Dynet toolkits.2 We also use dropout (Srivastava et al., 2014) at lexical input
2https://github.com/clab/dynet
embeddings with a fixed probability pdrop to avoid overfitting.",6 Training,[0],[0]
"pdrop is set to 0.5 for all tasks.
",6 Training,[0],[0]
"Following Tai et al. (2015), Li et al. (2015), Zhu et al. (2015) and Le and Zuidema (2015), we use Glove-300d word embeddings3 to train our model.",6 Training,[0],[0]
The pretrained word embeddings are fine-tuned for all tasks.,6 Training,[0],[0]
Unknown words are handled in two steps.,6 Training,[0],[0]
"First, if a word is not contained in the pretrained word embeddings, but its lowercased form exists in the embedding table, we use the lowercase as a replacement.",6 Training,[0],[0]
"Second, if both the original word and its lowercased form cannot be found, we treat the word as unk.",6 Training,[0],[0]
"The embedding vector of the UNK token is initialized as the average of all embedding vectors.
",6 Training,[0],[0]
We use one hidden layer and the same dimensionality settings for both sequential and Tree LSTMs.,6 Training,[0],[0]
LSTM hidden states are of size 150.,6 Training,[0],[0]
"The output hidden size is 128 and 64 for the sentiment classification task and the question type classification task, respectively.",6 Training,[0],[0]
Each model is trained for 30 iterations.,6 Training,[0],[0]
"The same training procedure repeats five times using different random seeds, with parameters being evaluated at the end of every iteration on the development set.",6 Training,[0],[0]
The model that gives the best development result is used for final tests.,6 Training,[0],[0]
The effectiveness of our model is tested mainly on a sentiment classification task and a question type classification task.,7 Experiments,[0],[0]
Sentiment Classification.,7.1 Tasks,[0],[0]
"For sentiment classification, we use the same data settings as Zhu et al. (2015).",7.1 Tasks,[0],[0]
"Specifically, we use the Stanford Sentiment Treebank (Socher et al., 2013b).",7.1 Tasks,[0],[0]
Each sentence is annotated with a constituent tree.,7.1 Tasks,[0],[0]
Every internal node corresponds to a phrase.,7.1 Tasks,[0],[0]
"Each node is manually assigned an integer sentiment label from 0 to 4, that correspond to five sentiment classes: very negative, negative, neutral, positive and very positive, respectively.",7.1 Tasks,[0],[0]
"The root label represents the sentiment label of the whole sentence.
",7.1 Tasks,[0],[0]
We perform both binary classification and finegrained classification.,7.1 Tasks,[0],[0]
"Following previous work, we use labels of all phrases for training.",7.1 Tasks,[0],[0]
"Gold-standard
3http://nlp.stanford.edu/data/glove.840B.300d.zip
tree structures are used for training and testing (Le and Zuidema, 2015; Li et al., 2015; Zhu et al., 2015; Tai et al., 2015).",7.1 Tasks,[0],[0]
"Accuracies are evaluated for both the sentence root labels and phrase labels.
",7.1 Tasks,[0],[0]
Question Type Classification.,7.1 Tasks,[0],[0]
"For the question type classification task, we use the TREC data (Li and Roth, 2002).",7.1 Tasks,[0],[0]
Each training sample in this dataset contains a question sentence and its corresponding question type.,7.1 Tasks,[0],[0]
"We work on the sixway coarse classification task, where the six question types are ENTY, HUM, LOC, DESC, NUM and ABBR, corresponding to ENTITY, HUMAN, LOCATION, DESCRIPTION, NUMERIC VALUE and ABBREVIATION, respectively.",7.1 Tasks,[0],[0]
"For example, the type for the sentence “What year did the Titanic sink?” is NUM.",7.1 Tasks,[0],[0]
"The training set consists of 5,452 examples and the test set contains 500 examples.",7.1 Tasks,[0],[0]
"Since there is no development set, we follow Zhou et al. (2015), randomly extracting 500 examples from the training set as a development set.",7.1 Tasks,[0],[0]
"Unlike the sentiment treebank, there is no annotated tree for each sentence.",7.1 Tasks,[0],[0]
"Instead, we obtain an automatically parsed tree for each sentence using ZPar4 off-the-shelf (Zhang and Clark, 2011).",7.1 Tasks,[0],[0]
"Another difference between the TREC data and the sentiment treebank is that there is only one label, at the root node, rather than a label for each phrase.",7.1 Tasks,[0],[0]
We consider two models for our baselines.,7.2 Baselines,[0],[0]
"The first is bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997; Graves et al., 2013).",7.2 Baselines,[0],[0]
Our bidirectional constituency Tree LSTM (BiConTree) is compared against BiLSTM to investigate the effectiveness of tree structures.,7.2 Baselines,[0],[0]
"For the sentiment task, following Tai et al. (2015) and Li et al. (2015), we convert the treebank into sequences to allow the bidirectional LSTM model to make use of every phrase span as a training example.",7.2 Baselines,[0],[0]
The second baseline model is the bottom-up Tree LSTM model of Zhu et al. (2015).,7.2 Baselines,[0],[0]
"We compare this model with our lexicalized bidirectional models to show the effects of adding head lexicalization and top-down information flow.
4https://github.com/SUTDNLP/ZPar, version 7.5",7.2 Baselines,[0],[0]
"Table 1 shows the main results for the sentiment classification task, where RNTN is the recursive neural tensor model of Socher et al. (2013b), ConTree and DepTree denote constituency Tree LSTMs and dependency Tree LSTMs, respectively.",7.3 Main Results,[0],[0]
"Our reimplementations of sequential bidirectional LSTM and constituent Tree LSTM (Zhu et al., 2015) give comparable results to the original implementations.
",7.3 Main Results,[0],[0]
"After incorporating head lexicalization into our constituent Tree LSTM, the fine-grained sentiment classification accuracy increases from 51.2 to 52.8, and the binary sentiment classification accuracy increases from 88.5 to 89.2, which demonstrates the effectiveness of the head lexicalization mechanism.
",7.3 Main Results,[0],[0]
Table 1 also shows that a vanilla top-down ConTree LSTM by head-lexicalization (i.e. the topdown half of the final bidirectional model) alone obtains comparable accuracies to the bottom-up ConTree LSTM model.,7.3 Main Results,[0],[0]
"The BiConTree model can further improve the classification accuracies by 0.7 points (fine-grained) and 1.3 points (binary) compared to the unidirectional bottom-up lexicalized ConTree LSTM model, respectively.
",7.3 Main Results,[0],[0]
Table 1 includes 5 class accuracies for all nodes.,7.3 Main Results,[0],[0]
"There is no significant difference between different models, consistent with the observation of Li et al. (2015).",7.3 Main Results,[0],[0]
"To our knowledge, these are the best reported results for this sentiment classification task.
",7.3 Main Results,[0],[0]
Table 2 shows the question type classification results.,7.3 Main Results,[0],[0]
"Our final model gives better results compared
to the BiLSTM model and the bottom-up ConTree model, achieving comparable results to the state-ofthe-art SVM classifier with carefully designed features.",7.3 Main Results,[0],[0]
Introducing head lexicalization and bidirectional extension to the model increases the model complexity.,7.4 Training Time and Model Size,[0],[0]
"In this section, we analyze training time and model size with the fine-grained sentiment classification task.
",7.4 Training Time and Model Size,[0],[0]
We run all the models using an i7-4790 3.60GHz CPU with a single thread.,7.4 Training Time and Model Size,[0],[0]
Table 3 shows the average running time for different models over 30 iterations.,7.4 Training Time and Model Size,[0],[0]
The baseline ConTree model takes about 1.3 hours to finish the training procedure.,7.4 Training Time and Model Size,[0],[0]
"ConTree+Lex takes about 1.5 times longer than ConTree. BiConTree takes about 3.2 hours, which is about 2.5 times longer than that of ConTree.
Table 4 compares the model sizes.",7.4 Training Time and Model Size,[0],[0]
We did not count the number of parameters in the lookup table since these parameters are the same for all models.,7.4 Training Time and Model Size,[0],[0]
"Because the size of LSTM models mainly depends on the dimensionality of the state vector h, we change the size of h to study the effect of model size.",7.4 Training Time and Model Size,[0],[0]
"When |h| = 150, the model size of the baseline model ConTree is the smallest, which consists of about 538K parameters.",7.4 Training Time and Model Size,[0],[0]
The model size of ConTree+Lex is about 1.4 times as large as that of the baseline model.,7.4 Training Time and Model Size,[0],[0]
"The bidirectional model BiConTree is the largest, about 1.7 times as large as that of the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
"However, this parameter set is not very large compared to the modern memory capacity, even for a computer with 16GB RAM.",7.4 Training Time and Model Size,[0],[0]
"In conclusion, in terms of both time, number of parameters and accuracy, head lexicalization method is
a good choice.",7.4 Training Time and Model Size,[0],[0]
Table 4 also helps to clarify whether the gain of the BiConTree model over the ConTree+Lex model is from the top-down information flow or more parameters.,7.4 Training Time and Model Size,[0],[0]
"For the same model, increasing the model size can improve the performance to some extent.",7.4 Training Time and Model Size,[0],[0]
"For example, doubling the size of |h| (75 → 150) increases the performance from 51.5 to 52.8 for the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
"Similarly, we boost the performance of the BiConTree model when doubling the size of |h| from 75 to 150.",7.4 Training Time and Model Size,[0],[0]
"However, doubling the size of |h| from 150 to 300 empirically decreases the performance of the ConTree+Lex model.",7.4 Training Time and Model Size,[0],[0]
The size of the BiConTree model with |h| = 75 is much smaller than that of the ConTree+Lex model with |h| = 150.,7.4 Training Time and Model Size,[0],[0]
"However the performance of these two models is quite close, which indicates that top-down information is useful even for a small model.",7.4 Training Time and Model Size,[0],[0]
A ConTree+Lex model with |h| = 215 and a BiConTree model with |h| = 150 are of similar size.,7.4 Training Time and Model Size,[0],[0]
"The performance of the ConTree+Lex model is again worse than that of the BiConTree model (52.5 v.s. 53.5), which shows the effectiveness of top-down information.",7.4 Training Time and Model Size,[0],[0]
"In this experiment, we investigate the effect of our head lexicalization method over heuristic baselines.",7.5 Head Lexicalization Methods,[0],[0]
"We consider three baseline methods, namely left branching (L), right branching (R) and averaging (A).",7.5 Head Lexicalization Methods,[0],[0]
"For L, a parent node accepts lexical information of its left child while ignoring the right child.",7.5 Head Lexicalization Methods,[0],[0]
"Correspondingly, for R, a parent node accepts lexical information of its right child while ignoring the left child.",7.5 Head Lexicalization Methods,[0],[0]
"For A, a parent node takes the average of the lexical vectors of its children.
",7.5 Head Lexicalization Methods,[0],[0]
"Table 5 shows the accuracies on the test set, where G denotes our gated head lexicalization method de-
scribed in Section 4.1.",7.5 Head Lexicalization Methods,[0],[0]
R gives better results compared to L due to relatively more right-branching structures in this treebank.,7.5 Head Lexicalization Methods,[0],[0]
A simple average yields similar results compared with right branching.,7.5 Head Lexicalization Methods,[0],[0]
"In contrast, G outperforms A method by considering the relative weights of each branch according to treelevel contexts.
",7.5 Head Lexicalization Methods,[0],[0]
"We then investigate what lexical heads can be learned by G. Interestingly, the lexical heads contain both syntactic and sentiment information.",7.5 Head Lexicalization Methods,[0],[0]
"Some heads correspond well to syntactic rules (Collins, 2003), others are driven by subjective words.",7.5 Head Lexicalization Methods,[0],[0]
"Compared to Collins’ rules, our method found 30.68% and 25.72% overlapping heads on the development and test sets, respectively.
",7.5 Head Lexicalization Methods,[0],[0]
"Based on the cosine similarity between the head lexical vector and its children, we visualize the head of a node by choosing the head of the child that gives the largest similarity value.",7.5 Head Lexicalization Methods,[0],[0]
"Figure 5 shows some examples, where <> indicates head words, sentiment labels (e.g. 2, 3) are also included.",7.5 Head Lexicalization Methods,[0],[0]
"In Figure 5a, “Emerges” is the syntactic head word of the whole phrase, which is consistent with Collins-style head finding.",7.5 Head Lexicalization Methods,[0],[0]
"However, “rare” is the head word of the phrase “something rare”, which is different from the syntactic head.",7.5 Head Lexicalization Methods,[0],[0]
"Similar observations are found in Figure 5b, where “good” is the head word of the whole phrase, rather than the syntactic head “place”.",7.5 Head Lexicalization Methods,[0],[0]
The sentiment label of “good” and the sentiment label of the whole phrase are both 3.,7.5 Head Lexicalization Methods,[0],[0]
Figure 5c shows more complex interactions between syntax and sentiment for deciding the head word.,7.5 Head Lexicalization Methods,[0],[0]
"Table 6 shows some example sentences incorrectly predicted by the baseline bottom-up tree model, but correctly labeled by our final model.",7.6 Error Analysis,[0],[0]
"The head word of sentence #1 by our model is “Gloriously”, which is consistent with the sentiment of the whole sentence.",7.6 Error Analysis,[0],[0]
This shows how head lexicalization can affect sentiment classification results.,7.6 Error Analysis,[0],[0]
"Sentences #2 and #3 show the usefulness of top-down informa-
tion for complex semantic structures, where compositionality has subtle effects.",7.6 Error Analysis,[0],[0]
"Our final model improves the results for the ‘very negative’ and ‘very positive’ classes by 10% and 11%, respectively.",7.6 Error Analysis,[0],[0]
"It also boosts the accuracies for sentences with negation (e.g. “not”, “no”, and “none”) by 4.4%.
",7.6 Error Analysis,[0],[0]
"Figure 6 shows the accuracy distribution accord-
ing to the sentence length.",7.6 Error Analysis,[0],[0]
"We find that our model can improve the classification accuracy for longer sentences (>30 words) by 3.5 absolute points compared to the baseline ConTree LSTM of Zhu et al. (2015), which demonstrates the strength of our model for handling long range information.",7.6 Error Analysis,[0],[0]
"By considering bidirectional information over tree structures, our model is aware of more contexts for making better predictions.",7.6 Error Analysis,[0],[0]
"Our main results are obtained on semanticdriven sentence classification tasks, where the automatically-learned head words contain mixed syntactic and semantic information.",8 Applications,[0],[0]
"To further investigate the effectiveness of automatically learned head information on a pure syntactic task, we additionally conduct a simple parser reranking experiment.",8 Applications,[0],[0]
"Further, we discuss findings in language modeling by Kuncoro et al. (2017) on the model of recurrent neural network grammars (Dyer et al., 2016).",8 Applications,[0],[0]
"Finally, we show potential future work leveraging our idea for more tasks.",8 Applications,[0],[0]
We use our tree LSTM models to rerank the 10 best outputs of the Charniak (2000) parser.,8.1 Syntactic Parsing,[0],[0]
"Given a sentence x, suppose that Y (x) is a set of parse tree candidates generated by a baseline parser for x, the goal of a syntactic reranker is to choose the best parsing hypothesis ŷ",8.1 Syntactic Parsing,[0],[0]
"according to a score function f(x, y; Θ).",8.1 Syntactic Parsing,[0],[0]
"Formally,
ŷ = arg maxy∈Y (x){f(x, y; Θ)} (18)
",8.1 Syntactic Parsing,[0],[0]
"For each tree y of sentence x, we follow Socher et al. (2013a) and define the score f(x, y; Θ) as the sum of scores of each constituent node,
f(x, y; Θ) = ∑
r∈node(x,y) Score(r; Θ) (19)
",8.1 Syntactic Parsing,[0],[0]
"Without loss of generality, we take a binary node as an example.",8.1 Syntactic Parsing,[0],[0]
"Given a node A, suppose that its two children are B and C. Let the learned composition state vectors of A, B and C by our proposed TreeLSTM model be nA, nB and nC , respectively.",8.1 Syntactic Parsing,[0],[0]
"The head word vector of node A is hA. Score(A; Θ) is defined as:
oBCA = ReLU(W L s nB +W R s nC",8.1 Syntactic Parsing,[0],[0]
+W H s hA + b s) ScoreBCA,8.1 Syntactic Parsing,[0],[0]
= log(softmax(o BC A )),8.1 Syntactic Parsing,[0],[0]
"[A],
(20)
where WLs , W R s and b s are model parameters.",8.1 Syntactic Parsing,[0],[0]
Training.,8.1 Syntactic Parsing,[0],[0]
"Given a training instance 〈xi, Y (xi)〉 in the training set D, we use a max-margin loss function to train our reranking model.",8.1 Syntactic Parsing,[0],[0]
"Suppose that the oracle parse tree in Y (xi) is yi, the loss function L(Θ) is
L(Θ) = 1
|D|
|D|∑
i=1
ri(Θ) + λ
2 ||Θ||2 (21)
",8.1 Syntactic Parsing,[0],[0]
Here λ is a regularization parameter and ri(Θ) is the margin loss between yi and the highest score tree ŷi predicted by the reranking model.,8.1 Syntactic Parsing,[0],[0]
"ri(Θ) is given by
ri(Θ) = max",8.1 Syntactic Parsing,[0],[0]
"ŷi∈Y (xi) (0, f(xi, ŷi; Θ)+
∆(yi, ŷi)− f(xi, yi; Θ)), (22)
where ∆(yi, ŷi) is the structure loss between yi and ŷi by counting the number of incorrect nodes in the oracle tree:
∆(yi, ŷi) =",8.1 Syntactic Parsing,[0],[0]
"∑
node∈ŷi κ1{node /∈ yi}.",8.1 Syntactic Parsing,[0],[0]
"(23)
κ is a scalar.",8.1 Syntactic Parsing,[0],[0]
"With this loss function, we require the score of the oracle tree to be higher than the other candidates by a score margin.",8.1 Syntactic Parsing,[0],[0]
"Intuitively, the score of the yi will increase and the score of ŷi will decrease during training.
Results.",8.1 Syntactic Parsing,[0],[0]
"We experiment on the WSJ portion of the Penn Treebank, following the standard split (Collins, 2003).",8.1 Syntactic Parsing,[0],[0]
"Sections 2-21 are used for training, Section 24 and Section 23 are the development
set and test set, respectively.",8.1 Syntactic Parsing,[0],[0]
"The Charniak parser (Charniak, 2000; Charniak and Johnson, 2005) is adopted for our baseline by following the settings of Choe and Charniak (2016).
",8.1 Syntactic Parsing,[0],[0]
"To obtain N-best lists on the development set and test set, we first train a baseline parser on the training set.",8.1 Syntactic Parsing,[0],[0]
"To obtain N-best lists on the training data, we split the training data into 20 folds and trained 20 parsers.",8.1 Syntactic Parsing,[0],[0]
Each parser was trained on 19 folds data and used to produce the n-best list of the remaining fold.,8.1 Syntactic Parsing,[0],[0]
"For the neural reranking model, we use the pretrained word vectors from Collobert et al. (2011).",8.1 Syntactic Parsing,[0],[0]
The input dimension is 50.,8.1 Syntactic Parsing,[0],[0]
The dimension of state vectors in Tree-LSTM model is 60.,8.1 Syntactic Parsing,[0],[0]
"These parameters are trained with ADAM (Kingma and Ba, 2015) with a batch size of 20.",8.1 Syntactic Parsing,[0],[0]
We set κ = 0.1 for all experiments.,8.1 Syntactic Parsing,[0],[0]
"For practical reasons, we use the ConTree+Lex model to learn the node representations and define Y (xi) to be the 10-best parsing trees of xi.
Table 7 shows the reranking results on WSJ test set.",8.1 Syntactic Parsing,[0],[0]
The baseline F1 score is 89.7.,8.1 Syntactic Parsing,[0],[0]
Our ConTree improves the baseline model to 90.6.,8.1 Syntactic Parsing,[0],[0]
Using ConTree+Lex model can further improve the performance (90.6 → 90.9).,8.1 Syntactic Parsing,[0],[0]
This suggests that automatic heads can also be useful for a syntactic task.,8.1 Syntactic Parsing,[0],[0]
"Among neural rerankers, our model outperforms Socher et al. (2013a), but underperforms current state-of-theart models, including sequence-to-sequence based LSTM language models (Vinyals et al., 2015a; Choe and Charniak, 2016) and recurrent neural network grammars (Dyer et al., 2016).",8.1 Syntactic Parsing,[0],[0]
This is likely due to our simple reranking configurations and settings5.,8.1 Syntactic Parsing,[0],[0]
"Nevertheless, it serves our goal of contrasting the tree LSTM models.",8.1 Syntactic Parsing,[0],[0]
"Kuncoro et al. (2017) investigate composition functions in recurrent neural network grammars (RNNG) (Dyer et al., 2016), finding that syntactic head information can be automatically learned.",8.2 Language Modeling,[0],[0]
"Their observa-
5Dyer et al. (2016) employs 2-layerd LSTMs with input and hidden dimensions of size 256 and 128.",8.2 Language Modeling,[0],[0]
Choe and Charniak (2016) use 3-layered LSTMs with both the input and hidden dimensions of size 1500.,8.2 Language Modeling,[0],[0]
"In addition, we only use the tree LSTM for scoring candidate parses in order to isolate the effect of tree LSTMs.",8.2 Language Modeling,[0],[0]
"In contrast, the previous works use the complex feature combinations in order to achieve high accuracies, which is different from our goal.
tion is consistent with ours.",8.2 Language Modeling,[0],[0]
"Formally, an RNNG is a tuple 〈N,Σ, R, S,Θ〉, where N is the set of nonterminals, Σ is the set of terminals, R is a set of top-down transition-based rules, S is the start symbol and Θ is the set of model parameters.",8.2 Language Modeling,[0],[0]
"Given S, the derivation process resembles transition-based parsing, which is performed incrementally from left to right.",8.2 Language Modeling,[0],[0]
"Unlike surface language models, RNNGs model sentences with explicit grammar.",8.2 Language Modeling,[0],[0]
"Comparing naive sequence-to-sequence models of syntax (Vinyals et al., 2015a), RNNGs have the advantage of explicitly modeling syntactic composition between constituents, by combining the vector representation of child constituents into a single vector representation of their parent using a neural network.",8.2 Language Modeling,[0],[0]
"Kuncoro et al. (2017) show that such compositions are the key to the success, and further investigate several alternatives neural network structures.",8.2 Language Modeling,[0],[0]
"In particular, they compare vanilla LSTMs to attention networks when composing child constituents.",8.2 Language Modeling,[0],[0]
"Interestingly, the attention values represent syntactic heads among the child constituents to some extent.",8.2 Language Modeling,[0],[0]
"In addition, the vector constituent representation implicitly reflects constituent types.",8.2 Language Modeling,[0],[0]
Their finding is consistent with ours in that a neural network can learn pure syntactic head information from constituent vectors.,8.2 Language Modeling,[0],[0]
"Our head-lexicalized tree model can be used for all tasks that require representation learning for sentences, given their constituent syntax.",8.3 Relation Extraction,[0],[0]
One example of future work is relation extraction.,8.3 Relation Extraction,[0],[0]
"For example, given the sentence “John is from Google Inc.”, a relation ‘works in’ can be extracted between ‘John’ and ‘Google Inc.’.
",8.3 Relation Extraction,[0],[0]
"Miwa and Bansal (2016) solve this task by using the Child-Sum tree representation of Tai et al. (2015) to represent the input sentence, extracting features for the two entities according to their related nodes in the dependency tree, and then conducting rela-
tion classification based on these features.",8.3 Relation Extraction,[0],[0]
Headlexicalization and top-down information can potentially be useful for improving relation extraction in the framework of Miwa and Bansal (2016).,8.3 Relation Extraction,[0],[0]
We proposed lexicalized variants for constituent tree LSTMs.,9 Conclusion,[0],[0]
"Learning the heads of constituents automatically using a neural model, our lexicalized tree LSTM is applicable to arbitrary binary branching trees in CFG, and is formalism-independent.",9 Conclusion,[0],[0]
"In addition, lexical information on the root further allows a top-down extension to the model, resulting in a bidirectional constituent Tree LSTM.",9 Conclusion,[0],[0]
Experiments on two well-known datasets show that head-lexicalization improves the unidirectional Tree LSTM model.,9 Conclusion,[0],[0]
"In addition, the bidirectional Tree LSTM gives superior labeling results compared to both unidirectional Tree LSTMs and bidirectional sequential LSTMs.",9 Conclusion,[0],[0]
We thank the anonymous reviewers for their detailed and constructive comments.,Acknowledgments,[0],[0]
Yue Zhang is the corresponding author.,Acknowledgments,[0],[0]
"Sequential LSTMs have been extended to model tree structures, giving competitive results for a number of tasks.",abstractText,[0],[0]
"Existing methods model constituent trees by bottom-up combinations of constituent nodes, making direct use of input word information only for leaf nodes.",abstractText,[0],[0]
"This is different from sequential LSTMs, which contain references to input words for each node.",abstractText,[0],[0]
"In this paper, we propose a method for automatic head-lexicalization for tree-structure LSTMs, propagating head words from leaf nodes to every constituent node.",abstractText,[0],[0]
"In addition, enabled by head lexicalization, we build a tree LSTM in the top-down direction, which corresponds to bidirectional sequential LSTMs in structure.",abstractText,[0],[0]
Experiments show that both extensions give better representations of tree structures.,abstractText,[0],[0]
Our final model gives the best results on the Stanford Sentiment Treebank and highly competitive results on the TREC question type classification task.,abstractText,[0],[0]
Head-Lexicalized Bidirectional Tree LSTMs,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 358–363 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
358
tection may broadly be categorized according to two paradigms: pattern-based and distributional methods. In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets. Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.",text,[0],[0]
Hierarchical relationships play a central role in knowledge representation and reasoning.,1 Introduction,[0],[0]
"Hypernym detection, i.e., the modeling of word-level hierarchies, has long been an important task in natural language processing.",1 Introduction,[0],[0]
"Starting with Hearst (1992), pattern-based methods have been one of the most influential approaches to this problem.",1 Introduction,[0],[0]
Their key idea is to exploit certain lexico-syntactic patterns to detect is-a relations in text.,1 Introduction,[0],[0]
"For instance, patterns like “NPy such as NPx”, or “NPx and other NPy” often indicate hypernymy relations of the form x is-a y. Such patterns may be predefined, or they may be learned automatically (Snow et al., 2004; Shwartz et al., 2016).",1 Introduction,[0],[0]
"However, a well-known problem of Hearst-like patterns is their extreme sparsity: words must co-occur in exactly the right configuration, or else no relation can be detected.
",1 Introduction,[0],[0]
"To alleviate the sparsity issue, the focus in hypernymy detection has recently shifted to distributional representations, wherein words are represented as vectors based on their distribution across large corpora.",1 Introduction,[0],[0]
"Such methods offer rich representations of lexical meaning, alleviating the sparsity problem, but require specialized similarity mea-
sures to distinguish different lexical relationships.",1 Introduction,[0],[0]
"The most successful measures to date are generally inspired by the Distributional Inclusion Hypothesis (DIH) (Zhitomirsky-Geffet and Dagan, 2005), which states roughly that contexts in which a narrow term x may appear (“cat”) should be a subset of the contexts in which a broader term y (“animal”) may appear.",1 Introduction,[0],[0]
"Intuitively, the DIH states that we should be able to replace any occurrence of “cat” with “animal” and still have a valid utterance.",1 Introduction,[0],[0]
"An important insight from work on distributional methods is that the definition of context is often critical to the success of a system (Shwartz et al., 2017).",1 Introduction,[0],[0]
"Some distributional representations, like positional or dependency-based contexts, may even capture crude Hearst pattern-like features (Levy et al., 2015; Roller and Erk, 2016).
",1 Introduction,[0],[0]
"While both approaches for hypernym detection rely on co-occurrences within certain contexts, they differ in their context selection strategy: pattern-based methods use predefined manuallycurated patterns to generate high-precision extractions while DIH methods rely on unconstrained word co-occurrences in large corpora.
",1 Introduction,[0],[0]
"Here, we revisit the idea of using pattern-based methods for hypernym detection.",1 Introduction,[0],[0]
"We evaluate several pattern-based models on modern, large corpora and compare them to methods based on the DIH.",1 Introduction,[0],[0]
"We find that simple pattern-based methods consistently outperform specialized DIH methods on several difficult hypernymy tasks, including detection, direction prediction, and graded entailment ranking.",1 Introduction,[0],[0]
"Moreover, we find that taking low-rank embeddings of pattern-based models substantially improves performance by remedying the sparsity issue.",1 Introduction,[0],[0]
"Overall, our results show that Hearst patterns provide high-quality and robust predictions on large corpora by capturing important contextual constraints, which are not yet modeled in distributional methods.",1 Introduction,[0],[0]
"In the following, we discuss pattern-based and distributional methods to detect hypernymy relations.",2 Models,[0],[0]
We explicitly consider only relatively simple pattern-based approaches that allow us to directly compare their performance to DIH-based methods.,2 Models,[0],[0]
"First, let P = {(x, y)}ni=1 denote the set of hypernymy relations that have been extracted via Hearst patterns from a text corpus T .",2.1 Pattern-based Hypernym Detection,[0],[0]
"Furthermore let w(x, y) denote the count of how often (x, y) has been extracted and let W = ∑
(x,y)∈P w(x, y) denote the total number extractions.",2.1 Pattern-based Hypernym Detection,[0],[0]
"In the first, most direct application of Hearst patterns, we then simply use the counts w(x, y) or, equivalently, the extraction probability
p(x, y) = w(x, y)
W (1)
to predict hypernymy relations from T .",2.1 Pattern-based Hypernym Detection,[0],[0]
"However, simple extraction probabilities as in Equation (1) are skewed by the occurrence probabilities of their constituent words.",2.1 Pattern-based Hypernym Detection,[0],[0]
"For instance, it is more likely that we extract (France, country) over (France, republic), just because the word country is more likely to occur than republic.",2.1 Pattern-based Hypernym Detection,[0],[0]
This skew in word distributions is well-known for natural language and also translates to Hearst patterns (see also Figure 1).,2.1 Pattern-based Hypernym Detection,[0],[0]
"For this reason, we also consider predicting hypernymy relations based on the Pointwise Mutual Information of Hearst patterns: First, let p−(x) =",2.1 Pattern-based Hypernym Detection,[0],[0]
"∑
(x,y)∈P w(x, y)/W
and p+(x)",2.1 Pattern-based Hypernym Detection,[0],[0]
"= ∑
(y,x)∈P w(y, x)/W denote the probability that x occurs as a hyponym and hypernym, respectively.",2.1 Pattern-based Hypernym Detection,[0],[0]
"We then define the Positive Pointwise Mutual Information for (x, y) as
ppmi(x, y) = max
(
0, log p(x, y)
p−(x)p+(y)
)
.",2.1 Pattern-based Hypernym Detection,[0],[0]
"(2)
While Equation (2) can correct for different word occurrence probabilities, it cannot handle missing data.",2.1 Pattern-based Hypernym Detection,[0],[0]
"However, sparsity is one of the main issues when using Hearst patterns, as a necessarily incomplete set of extraction rules will lead inevitably to missing extractions.",2.1 Pattern-based Hypernym Detection,[0],[0]
"For this purpose, we also study low-rank embeddings of the PPMI matrix, which allow us to make predictions for unseen pairs.",2.1 Pattern-based Hypernym Detection,[0],[0]
"In particular, let m = |{x : (x, y) ∈ P ∨ (y, x) ∈ P}| denote the number of unique terms in P .",2.1 Pattern-based Hypernym Detection,[0],[0]
"Furthermore, let X ∈ Rm×m be the PPMI matrix with
entries Mxy = ppmi(x, y) and let M = UΣV ⊤ be its Singular Value Decomposition (SVD).",2.1 Pattern-based Hypernym Detection,[0],[0]
"We can then predict hypernymy relations based on the truncated SVD of M via
spmi(x, y) = u⊤xΣrvy (3)
where ux, vy denote the x-th and y-th row of U and V , respectively, and where Σr is the diagonal matrix of truncated singular values (in which all but the r largest singular values are set to zero).
",2.1 Pattern-based Hypernym Detection,[0],[0]
Equation (3) can be interpreted as a smoothed version of the observed PPMI matrix.,2.1 Pattern-based Hypernym Detection,[0],[0]
"Due to the truncation of singular values, Equation (3) computes a low-rank embedding of M where similar words (in terms of their Hearst patterns) have similar representations.",2.1 Pattern-based Hypernym Detection,[0],[0]
"Since Equation (3) is defined for all pairs (x, y), it allows us to make hypernymy predictions based on the similarity of words.",2.1 Pattern-based Hypernym Detection,[0],[0]
"We also consider factorizing a matrix that is constructed from occurrence probabilities as in Equation (1), denoted by sp(x, y).",2.1 Pattern-based Hypernym Detection,[0],[0]
"This approach is then closely related to the method of Cederberg and Widdows (2003), which has been proposed to improve precision and recall for hypernymy detection from Hearst patterns.",2.1 Pattern-based Hypernym Detection,[0],[0]
"Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis (Weeds et al., 2004; Kotlerman et al., 2010; Santus et al., 2014; Lenci and Benotto, 2012; Shwartz et al., 2017).",2.2 Distributional Hypernym Detection,[0],[0]
"Here, we compare to two methods with strong empirical results.",2.2 Distributional Hypernym Detection,[0],[0]
"As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces.",2.2 Distributional Hypernym Detection,[0],[0]
"First, we consider WeedsPrec (Weeds et al., 2004) which captures the features of
x which are included in the set of a broader term’s features, y:
WeedsPrec(x,y) = ∑n i=1",2.2 Distributional Hypernym Detection,[0],[0]
xi ∗,2.2 Distributional Hypernym Detection,[0],[0]
"✶yi>0 ∑n
i=1",2.2 Distributional Hypernym Detection,[0],[0]
"xi
Second, we consider invCL (Lenci and Benotto, 2012) which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term.",2.2 Distributional Hypernym Detection,[0],[0]
"In particular, let
CL(x,y) =",2.2 Distributional Hypernym Detection,[0],[0]
∑n i=1,2.2 Distributional Hypernym Detection,[0],[0]
"min(xi, yi)",2.2 Distributional Hypernym Detection,[0],[0]
"∑n
i=1",2.2 Distributional Hypernym Detection,[0],[0]
"xi
denote the degree of inclusion of x in y as proposed by Clarke (2009).",2.2 Distributional Hypernym Detection,[0],[0]
"To measure both the inclusion of x in y and the non-inclusion of y in x, invCL is then defined as
invCL(x,y) = √ CL(x,y) ∗ (1− CL(y,x))
",2.2 Distributional Hypernym Detection,[0],[0]
"Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis (Santus et al., 2014; Shwartz et al., 2017).",2.2 Distributional Hypernym Detection,[0],[0]
"Intuitively, the SLQS model presupposes that general words appear mostly in uninformative contexts, as measured by entropy.",2.2 Distributional Hypernym Detection,[0],[0]
"Specifically, SLQS depends on the median entropy of a term’s top N contexts, defined as
Ex = median N i=1",2.2 Distributional Hypernym Detection,[0],[0]
"[H(ci)] ,
where H(ci) is the Shannon entropy of context ci across all terms, and N is chosen in hyperparameter selection.",2.2 Distributional Hypernym Detection,[0],[0]
"Finally, SLQS is defined using the ratio between the two terms:
SLQS(x, y) = 1− Ex Ey .
",2.2 Distributional Hypernym Detection,[0],[0]
"Since the SLQS model only compares the relative generality of two terms, but does not make judgment about the terms’ relatedness, we report SLQS-cos, which multiplies the SLQS measure by cosine similarity of x and y (Santus et al., 2014).
",2.2 Distributional Hypernym Detection,[0],[0]
"For completeness, we also include cosine simi-
larity as a baseline in our evaluation.",2.2 Distributional Hypernym Detection,[0],[0]
"To evaluate the relative performance of patternbased and distributional models, we apply them to several challenging hypernymy tasks.",3 Evaluation,[0],[0]
Detection:,3.1 Tasks,[0],[0]
"In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation.",3.1 Tasks,[0],[0]
"For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of BLESS, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns.",3.1 Tasks,[0],[0]
"Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs.",3.1 Tasks,[0],[0]
"This version contains 14,542 total pairs with 1,337 positive examples.",3.1 Tasks,[0],[0]
"Second, we evaluate on LEDS (Baroni et al., 2012), which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs.",3.1 Tasks,[0],[0]
"We also consider EVAL (Santus et al., 2015), containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations.",3.1 Tasks,[0],[0]
EVAL is notable for its absence of random pairs.,3.1 Tasks,[0],[0]
"The largest dataset is SHWARTZ (Shwartz et al., 2016), which was collected from a mixture of WordNet, DBPedia, and other resources.",3.1 Tasks,[0],[0]
"We limit ourselves to a 52,578 pair subset excluding multiword expressions.",3.1 Tasks,[0],[0]
"Finally, we evaluate on WBLESS (Weeds et al., 2014), a 1,668 pair subset of BLESS, with negative pairs being selected from co-hyponymy, random, and hyponymy relations.",3.1 Tasks,[0],[0]
"Previous work has used different metrics for evaluating on BLESS (Lenci and Benotto, 2012; Levy et al., 2015; Roller and Erk, 2016).",3.1 Tasks,[0],[0]
We chose to evaluate the global ranking using Average Precision.,3.1 Tasks,[0],[0]
"This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in Shwartz et al. (2017).
",3.1 Tasks,[0],[0]
Direction:,3.1 Tasks,[0],[0]
"In direction prediction, the task is to identify which term is broader in a given pair
of words.",3.1 Tasks,[0],[0]
"For this task, we evaluate all models on three datasets described by Kiela et al. (2015):",3.1 Tasks,[0],[0]
"On BLESS, the task is to predict the direction for all 1337 positive pairs in the dataset.",3.1 Tasks,[0],[0]
"Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. score(x, y) >",3.1 Tasks,[0],[0]
"score(y, x).",3.1 Tasks,[0],[0]
"We reserve 10% of the data for validation, and test on the remaining 90%.",3.1 Tasks,[0],[0]
"On WBLESS, we follow prior work (Nguyen et al., 2017; Vulić and Mrkšić, 2017) and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data.",3.1 Tasks,[0],[0]
We report average accuracy across all iterations.,3.1 Tasks,[0],[0]
"Finally, we evaluate on BIBLESS (Kiela et al., 2015), a variant of WBLESS with hypernymy and hyponymy pairs explicitly annotated for their direction.",3.1 Tasks,[0],[0]
"Since this task requires three-way classification (hypernymy, hyponymy, and other), we perform two-stage classification.",3.1 Tasks,[0],[0]
"First, a threshold is tuned using 2% of the data, identifying whether a pair exhibits hypernymy in either direction.",3.1 Tasks,[0],[0]
"Second, the relative comparison of scores determines which direction is predicted.",3.1 Tasks,[0],[0]
"As with WBLESS, we report the average accuracy over 1000 iterations.
",3.1 Tasks,[0],[0]
Graded Entailment:,3.1 Tasks,[0],[0]
"In graded entailment, the task is to quantify the degree to which a hypernymy relation holds.",3.1 Tasks,[0],[0]
"For this task, we follow prior work (Nickel and Kiela, 2017; Vulić and Mrkšić, 2017) and use the noun part of HYPERLEX (Vulić et al., 2017), consisting of 2,163 noun pairs which are annotated to what degree x is-a y holds on a scale of [0, 6].",3.1 Tasks,[0],[0]
"For all models, we report Spearman’s rank correlation ρ.",3.1 Tasks,[0],[0]
We handle out-ofvocabulary (OOV) words by assigning the median of the scores (computed across the training set) to pairs with OOV words.,3.1 Tasks,[0],[0]
"Pattern-based models: We extract Hearst patterns from the concatenation of Gigaword and Wikipedia, and prepare our corpus by tokenizing, lemmatizing, and POS tagging using CoreNLP 3.8.0.",3.2 Experimental Setup,[0],[0]
The full set of Hearst patterns is provided in Table 1.,3.2 Experimental Setup,[0],[0]
"Our selected patterns match prototypical Hearst patterns, like “animals such as cats,” but also include broader patterns like “New Year is the most important holiday.”",3.2 Experimental Setup,[0],[0]
"Leading and following noun phrases are allowed to match limited modifiers (compound nouns, adjectives, etc.), in which case we also generate a hit for the head of the noun phrase.",3.2 Experimental Setup,[0],[0]
"Dur-
ing postprocessing, we remove pairs which were not extracted by at least two distinct patterns.",3.2 Experimental Setup,[0],[0]
"We also remove any pair (y, x) if p(y, x) < p(x, y).",3.2 Experimental Setup,[0],[0]
"The final corpus contains roughly 4.5M matched pairs, 431K unique pairs, and 243K unique terms.",3.2 Experimental Setup,[0],[0]
"For SVD-based models, we select the rank from r ∈ {5, 10, 15, 20, 25, 50, 100, 150, 200, 250, 300, 500, 1000} on the validation set.",3.2 Experimental Setup,[0],[0]
"The other pattern-based models do not have any hyperparameters.
",3.2 Experimental Setup,[0],[0]
"Distributional models: For the distributional baselines, we employ the large, sparse distributional space of Shwartz et al. (2017), which is computed from UkWaC and Wikipedia, and is known to have strong performance on several of the detection tasks.",3.2 Experimental Setup,[0],[0]
The corpus was POS tagged and dependency parsed.,3.2 Experimental Setup,[0],[0]
"Distributional contexts were constructed from adjacent words in dependency parses (Padó and Lapata, 2007; Levy and Goldberg, 2014).",3.2 Experimental Setup,[0],[0]
"Targets and contexts which appeared fewer than 100 times in the corpus were filtered, and the resulting co-occurrence matrix was PPMI transformed.1",3.2 Experimental Setup,[0],[0]
The resulting space contains representations for 218K words over 732K context dimensions.,3.2 Experimental Setup,[0],[0]
"For the SLQS model, we selected the number of contexts N from the same set of options as the SVD rank in pattern-based models.",3.2 Experimental Setup,[0],[0]
Table 2 shows the results from all three experimental settings.,3.3 Results,[0],[0]
"In nearly all cases, we find that patternbased approaches substantially outperform all three distributional models.",3.3 Results,[0],[0]
Particularly strong improvements can be observed on BLESS (0.76 average precision vs 0.19) and WBLESS (0.96 vs. 0.69) for the detection tasks and on all directionality tasks.,3.3 Results,[0],[0]
"For directionality prediction on BLESS, the SVD models surpass even the state-of-the-art supervised model of Vulić and Mrkšić (2017).",3.3 Results,[0],[0]
"Moreover, both SVD models perform generally better than their sparse counterparts on all tasks and datasets except on HYPERLEX.",3.3 Results,[0],[0]
"We performed a posthoc analysis of the validation sets comparing the ppmi and spmi models, and found that the truncated SVD improved recall via its matrix completion properties.",3.3 Results,[0],[0]
"We also found that the spmi model downweighted
1In addition, we also experimented with further distributional spaces and weighting schemes from Shwartz et al. (2017).",3.3 Results,[0],[0]
"We also experimented with distributional spaces using the same corpora and preprocessing as the Hearst patterns (i.e., Wikipedia and Gigaword).",3.3 Results,[0],[0]
"We found that the reported setting generally performed best, and omit others for brevity.
",3.3 Results,[0],[0]
"many high-scoring outlier pairs composed of rare terms.
",3.3 Results,[0],[0]
"When comparing the p(x, y) and ppmi models to distributional models, we observe mixed results.",3.3 Results,[0],[0]
The SHWARTZ dataset is difficult for sparse models due to its very long tail of low frequency words that are hard to cover using Hearst patterns.,3.3 Results,[0],[0]
"On EVAL, Hearst-pattern based methods get penalized by OOV words, due to the large number of verbs and adjectives in the dataset, which are not captured by our patterns.",3.3 Results,[0],[0]
"However, in 7 of the 9 datasets, at least one of the sparse models outperforms all distributional measures, showing that Hearst patterns can provide strong performance on large corpora.",3.3 Results,[0],[0]
We studied the relative performance of Hearst pattern-based methods and DIH-based methods for hypernym detection.,4 Conclusion,[0],[0]
Our results show that the pattern-based methods substantially outperform DIH-based methods on several challenging benchmarks.,4 Conclusion,[0],[0]
We find that embedding methods alleviate sparsity concerns of pattern-based approaches and substantially improve coverage.,4 Conclusion,[0],[0]
We conclude that Hearst patterns provide important contexts for the detection of hypernymy relations that are not yet captured in DIH models.,4 Conclusion,[0],[0]
Our code is available at https://github.com/ facebookresearch/hypernymysuite.,4 Conclusion,[0],[0]
We would like to thank the anonymous reviewers for their helpful suggestions.,Acknowledgments,[0],[0]
"We also thank Vered Shwartz, Enrico Santus, and Dominik Schlechtweg for providing us with their distributional spaces and baseline implementations.",Acknowledgments,[0],[0]
Methods for unsupervised hypernym detection may broadly be categorized according to two paradigms: pattern-based and distributional methods.,abstractText,[0],[0]
"In this paper, we study the performance of both approaches on several hypernymy tasks and find that simple pattern-based methods consistently outperform distributional methods on common benchmark datasets.",abstractText,[0],[0]
Our results show that pattern-based models provide important contextual constraints which are not yet captured in distributional methods.,abstractText,[0],[0]
Hearst Patterns Revisited: Automatic Hypernym Detection from Large Text Corpora,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 46–56 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Relation extraction is a fundamental task in information extraction. Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming. To overcome this drawback, we propose a novel framework, REHESSION, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics. These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance. Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion. Extensive experimental results demonstrate the superiority of REHESSION over the state-of-the-art.",text,[0],[0]
One of the most important tasks towards text understanding is to detect and categorize semantic relations between two entities in a given context.,1 Introduction,[0],[0]
"For example, in Fig. 1, with regard to the sentence of c1, relation between Jesse James and Missouri should be categorized as died in.",1 Introduction,[0],[0]
"With accurate identification, relation extraction systems can provide essential support for many applications.",1 Introduction,[0],[0]
"One
∗Equal contribution.
example is question answering, regarding a specific question, relation among entities can provide valuable information, which helps to seek better answers (Bao et al., 2014).",1 Introduction,[0],[0]
"Similarly, for medical science literature, relations like protein-protein interactions (Fundel et al., 2007) and gene disease associations (Chun et al., 2006) can be extracted and used in knowledge base population.",1 Introduction,[0],[0]
"Additionally, relation extractors can be used in ontology construction (Schutz and Buitelaar, 2005).
",1 Introduction,[0],[0]
"Typically, existing methods follow the supervised learning paradigm, and require extensive annotations from domain experts, which are costly and time-consuming.",1 Introduction,[0],[0]
"To alleviate such drawback, attempts have been made to build relation extractors with a small set of seed instances or humancrafted patterns (Nakashole et al., 2011; Carlson et al., 2010), based on which more patterns and instances will be iteratively generated by bootstrap learning.",1 Introduction,[0],[0]
"However, these methods often suffer from semantic drift (Mintz et al., 2009).",1 Introduction,[0],[0]
"Besides, knowledge bases like Freebase have been leveraged to automatically generate training data and provide distant supervision (Mintz et al., 2009).",1 Introduction,[0],[0]
"Nevertheless, for many domain-specific applications, distant supervision is either non-existent or insufficient (usually less than 25% of relation mentions are covered (Ren et al., 2015; Ling and Weld, 2012)).
",1 Introduction,[0],[0]
"Only recently have preliminary studies been developed to unite different supervisions, including knowledge bases and domain specific patterns, which are referred as heterogeneous supervision.",1 Introduction,[0],[0]
"As shown in Fig. 1, these supervisions often conflict with each other (Ratner et al., 2016).",1 Introduction,[0],[0]
"To address these conflicts, data programming (Ratner et al., 2016) employs a generative model, which encodes supervisions as labeling functions, and adopts the source consistency assumption: a source is likely to provide true information with
46
the same probability for all instances.",1 Introduction,[0],[0]
"This assumption is widely used in true label discovery literature (Li et al., 2016) to model reliabilities of information sources like crowdsourcing and infer the true label from noisy labels.",1 Introduction,[0],[0]
"Accordingly, most true label discovery methods would trust a human annotator on all instances to the same level.
",1 Introduction,[0],[0]
"However, labeling functions, unlike human annotators, do not make casual mistakes but follow certain “error routine”.",1 Introduction,[0],[0]
"Thus, the reliability of a labeling function is not consistent among different pieces of instances.",1 Introduction,[0],[0]
"In particular, a labeling function could be more reliable for a certain subset (Varma et al., 2016) (also known as its proficient subset) comparing to the rest.",1 Introduction,[0],[0]
"We identify these proficient subsets based on context information, only trust labeling functions on these subsets and avoid assuming global source consistency.
",1 Introduction,[0],[0]
"Meanwhile, embedding methods have demonstrated great potential in capturing semantic meanings, which also reduce the dimension of overwhelming text features.",1 Introduction,[0],[0]
"Here, we present REHESSION, a novel framework capturing context’s semantic meaning through representation learning, and conduct both relation extraction and true label discovery in a context-aware manner.",1 Introduction,[0],[0]
"Specifically, as depicted in Fig. 1, we embed relation mentions in a low-dimension vector space, where similar relation mentions tend to have similar relation types and annotations.",1 Introduction,[0],[0]
"‘True’ labels are further inferred based on reliabilities of labeling functions, which are calculated with their proficient subsets’ representations.",1 Introduction,[0],[0]
"Then, these inferred true labels would serve as supervision for all components, including context representation, true label discovery and relation extraction.",1 Introduction,[0],[0]
"Besides, the context representation bridges relation extraction with true label dis-
covery, and allows them to enhance each other.",1 Introduction,[0],[0]
"To the best of our knowledge, the framework proposed here is the first method that utilizes representation learning to provide heterogeneous supervision for relation extraction.",1 Introduction,[0],[0]
The high-quality context representations serve as the backbone of true label discovery and relation extraction.,1 Introduction,[0],[0]
"Extensive experiments on benchmark datasets demonstrate significant improvements over the state-ofthe-art.
",1 Introduction,[0],[0]
The remaining of this paper is organized as follows.,1 Introduction,[0],[0]
Section 2 gives the definition of relation extraction with heterogeneous supervision.,1 Introduction,[0],[0]
"We then present the REHESSION model and the learning algorithm in Section 3, and report our experimental evaluation in Section 4.",1 Introduction,[0],[0]
"Finally, we briefly survey related work in Section 5 and conclude this study in Section 6.",1 Introduction,[0],[0]
"In this section, we would formally define relation extraction and heterogeneous supervision, including the format of labeling functions.",2 Preliminaries,[0],[0]
"Here we conduct relation extraction in sentencelevel (Bao et al., 2014).",2.1 Relation Extraction,[0],[0]
"For a sentence d, an entity mention is a token span in d which represents an entity, and a relation mention is a triple (e1, e2, d) which consists of an ordered entity pair (e1, e2) and",2.1 Relation Extraction,[0],[0]
"d. And the relation extraction task is to categorize relation mentions into a given set of relation typesR, or Not-Target-Type (None) which means the type of the relation mention does not belong to R.",2.1 Relation Extraction,[0],[0]
"Similar to (Ratner et al., 2016), we employ labeling functions as basic units to encode supervision information and generate annotations.",2.2 Heterogeneous Supervision,[0],[0]
"Since different supervision information may have different proficient subsets, we require each labeling function to encode only one elementary supervision information.",2.2 Heterogeneous Supervision,[0],[0]
"Specifically, in the relation extraction scenario, we require each labeling function to only annotate one relation type based on one elementary piece of information, e.g., four examples are listed in Fig. 1.
Notice that knowledge-based labeling functions are also considered to be noisy because relation extraction is conducted in sentence-level, e.g. although president of (Obama, USA) exists in KB, it should not be assigned with “Obama was born in Honolulu, Hawaii, USA”, since president of is irrelevant to the context.",2.2 Heterogeneous Supervision,[0],[0]
"For a POS-tagged corpus D with detected entities, we refer its relation mentions as C = {ci = (ei,1, ei,2, d),∀d ∈ D}.",2.3 Problem Definition,[0],[0]
"Our goal is to annotate entity mentions with relation types of interest (R = {r1, . . .",2.3 Problem Definition,[0],[0]
", rK}) or None.",2.3 Problem Definition,[0],[0]
"We require users to provide heterogeneous supervision in the form of labeling function Λ = {λ1, . . .",2.3 Problem Definition,[0],[0]
", λM},",2.3 Problem Definition,[0],[0]
"and mark the annotations generated by Λ as O = {oc,i|λi generate annotation oc,i for c ∈ C}.",2.3 Problem Definition,[0],[0]
"We record relation mentions annotated by Λ as Cl, and refer relation mentions without annotation as Cu.",2.3 Problem Definition,[0],[0]
"Then, our task is to train a relation extractor based on Cl and categorize relation mentions in Cu.",2.3 Problem Definition,[0],[0]
"Here, we present REHESSION, a novel framework to infer true labels from automatically generated noisy labels, and categorize unlabeled instances
into a set of relation types.",3 The REHESSION Framework,[0],[0]
"Intuitively, errors of annotations (O) come from mismatch of contexts, e.g., in Fig. 1, λ1 annotates c1 and c2 with ’true’ labels but for mismatched contexts ‘killing’ and ’killed’.",3 The REHESSION Framework,[0],[0]
"Accordingly, we should only trust labeling functions on matched context, e.g., trust λ1 on c3 due to its context ‘was born in’, but not on c1 and c2.",3 The REHESSION Framework,[0],[0]
"On the other hand, relation extraction can be viewed as matching appropriate relation type to a certain context.",3 The REHESSION Framework,[0],[0]
"These two matching processes are closely related and can enhance each other, while context representation plays an important role in both of them.
",3 The REHESSION Framework,[0],[0]
Framework Overview.,3 The REHESSION Framework,[0],[0]
We propose a general framework to learn the relation extractor from automatically generated noisy labels.,3 The REHESSION Framework,[0],[0]
"As plotted in Fig. 1, distributed representation of context bridges relation extraction with true label discovery, and allows them to enhance each other.",3 The REHESSION Framework,[0],[0]
"Specifically, it follows the steps below:
1.",3 The REHESSION Framework,[0],[0]
"After being extracted from context, text features are embedded in a low dimension space by representation learning (see Fig. 2);
2.",3 The REHESSION Framework,[0],[0]
"Text feature embeddings are utilized to calculate relation mention embeddings (see Fig. 2);
3.",3 The REHESSION Framework,[0],[0]
"With relation mention embeddings, true labels are inferred by calculating labeling functions’ reliabilities in a context-aware manner (see Fig. 1);
4.",3 The REHESSION Framework,[0],[0]
"Inferred true labels would ‘supervise’ all components to learn model parameters (see Fig. 1).
",3 The REHESSION Framework,[0],[0]
We now proceed by introducing these components of the model in further details.,3 The REHESSION Framework,[0],[0]
"As shown in Table 2, we extract abundant lexical features (Ren et al., 2016; Mintz et al., 2009) to characterize relation mentions.",3.1 Modeling Relation Mention,[0],[0]
"However, this abundance also results in the gigantic dimension of original text features (∼ 107 in our case).",3.1 Modeling Relation Mention,[0],[0]
"In
order to achieve better generalization ability, we represent relation mentions with low dimensional (∼ 102) vectors.",3.1 Modeling Relation Mention,[0],[0]
"In Fig. 2, for example, relation mention c3 is first represented as bag-of-features.",3.1 Modeling Relation Mention,[0],[0]
"After learning text feature embeddings, we use the average of feature embedding vectors to derive the embedding vector for c3.
",3.1 Modeling Relation Mention,[0],[0]
Text Feature Representation.,3.1 Modeling Relation Mention,[0],[0]
"Similar to other principles of embedding learning, we assume text features occurring in the same contexts tend to have similar meanings (also known as distributional hypothesis(Harris, 1954)).",3.1 Modeling Relation Mention,[0],[0]
"Furthermore, we let each text feature’s embedding vector to predict other text features occurred in the same relation mentions or context.",3.1 Modeling Relation Mention,[0],[0]
"Thus, text features with similar meaning should have similar embedding vectors.",3.1 Modeling Relation Mention,[0],[0]
"Formally, we mark text features as F = {f1, · · · , f|F|}, record the feature set for ∀c ∈ C as fc, and represent the embedding vector for fi as vi ∈ Rnv , and we aim to maximize the following log likelihood: ∑ c∈Cl ∑ fi,fj∈fc log p(fi|fj), where
p(fi|fj) = exp(vTi v∗j )/",3.1 Modeling Relation Mention,[0],[0]
"∑
fk∈F exp(v T i v
∗ k).
",3.1 Modeling Relation Mention,[0],[0]
"However, the optimization of this likelihood is impractical because the calculation of ∇p(fi|fj) requires summation over all text features, whose size exceeds 107 in our case.",3.1 Modeling Relation Mention,[0],[0]
"In order to perform efficient optimization, we adopt the negative sampling technique (Mikolov et al., 2013) to avoid this summation.",3.1 Modeling Relation Mention,[0],[0]
"Accordingly, we replace the log likelihood with Eq. 1 as below:
JE = ∑ c∈Cl
fi,fj∈fc
(log σ(vTi v ∗ j )",3.1 Modeling Relation Mention,[0],[0]
− V∑,3.1 Modeling Relation Mention,[0],[0]
"k=1 Efk′∼P̂ [log σ(−v T i v ∗ k′)])
(1)
where P̂ is noise distribution used in (Mikolov et al., 2013), σ is the sigmoid function and V is number of negative samples.
",3.1 Modeling Relation Mention,[0],[0]
Relation Mention Representation.,3.1 Modeling Relation Mention,[0],[0]
"With text feature embeddings learned by Eq. 1, a naive method to
represent relation mentions is to concatenate or average its text feature embeddings.",3.1 Modeling Relation Mention,[0],[0]
"However, text features embedding may be in a different semantic space with relation types.",3.1 Modeling Relation Mention,[0],[0]
"Thus, we directly learn a mapping g from text feature representations to relation mention representations (Van Gysel et al., 2016a,b) instead of simple heuristic rules like concatenate or average (see Fig. 2):
zc = g(fc) =",3.1 Modeling Relation Mention,[0],[0]
"tanh(W · 1|fc| ∑
fi∈fc vi) (2)
where zc is the representation of c ∈ Cl, W is a nz × nv matrix, nz is the dimension of relation mention embeddings and tanh is the element-wise hyperbolic tangent function.
",3.1 Modeling Relation Mention,[0],[0]
"In other words, we represent bag of text features with their average embedding, then apply linear map and hyperbolic tangent to transform the embedding from text feature semantic space to relation mention semantic space.",3.1 Modeling Relation Mention,[0],[0]
"The non-linear tanh function allows non-linear class boundaries in other components, and also regularize relation mention representation to range [−1, 1] which avoids numerical instability issues.",3.1 Modeling Relation Mention,[0],[0]
"Because heterogeneous supervision generates labels in a discriminative way, we suppose its errors follow certain underlying principles, i.e., if a
labeling function annotates a instance correctly / wrongly, it would annotate other similar instances correctly / wrongly.",3.2 True Label Discovery,[0],[0]
"For example, λ1 in Fig. 1 generates wrong annotations for two similar instances c1, c2 and would make the same errors on other similar instances.",3.2 True Label Discovery,[0],[0]
"Since context representation captures the semantic meaning of relation mention and would be used to identify relation types, we also use it to identify the mismatch of context and labeling functions.",3.2 True Label Discovery,[0],[0]
"Thus, we suppose for each labeling function λi, there exists an proficient subset Si on Rnz , containing instances that λi can precisely annotate.",3.2 True Label Discovery,[0],[0]
"In Fig. 1, for instance, c3 is in the proficient subset of λ1, while c1 and c2 are not.",3.2 True Label Discovery,[0],[0]
"Moreover, the generation of annotations are not really random, and we propose a probabilistic model to describe the level of mismatch from labeling functions to real relation types instead of annotations’ generation.
",3.2 True Label Discovery,[0],[0]
"As shown in Fig. 3, we assume the indicator of whether c belongs to Si, sc,i = δ(c ∈ Si), would first be generated based on context representation
p(sc,i = 1|zc, li) =",3.2 True Label Discovery,[0],[0]
"p(c ∈ Si) = σ(zTc li) (3)
Then the correctness of annotation oc,i, ρc,i = δ(oc,i = o∗c), would be generated.",3.2 True Label Discovery,[0],[0]
"Furthermore, we assume p(ρc,i = 1|sc,i = 1) = ϕ1 and p(ρc,i = 1|sc,i = 0) = ϕ0 to be constant for all relation mentions and labeling functions.
",3.2 True Label Discovery,[0],[0]
"Because sc,i would not be used in other components of our framework, we integrate out sc,i and write the log likelihood as
JT = ∑
oc,i∈O",3.2 True Label Discovery,[0],[0]
"log(σ(zTc li)ϕ
δ(oc,i=o ∗ c ) 1",3.2 True Label Discovery,[0],[0]
"(1− ϕ1)δ(oc,i ̸=o ∗ c )
",3.2 True Label Discovery,[0],[0]
"+ (1− σ(zTc li))ϕδ(oc,i=o ∗ c ) 0",3.2 True Label Discovery,[0],[0]
"(1− ϕ0)δ(oc,i ̸=o ∗ c ))",3.2 True Label Discovery,[0],[0]
"(4)
Note that o∗c is a hidden variable but not a model parameter, and JT is the likelihood of ρc,i = δ(oc,i = o∗c).",3.2 True Label Discovery,[0],[0]
"Thus, we would first infer o∗c = argmaxo∗c JT , then train the true label discovery model by maximizing JT .",3.2 True Label Discovery,[0],[0]
We now discuss the model for identifying relation types based on context representation.,3.3 Modeling Relation Type,[0],[0]
"For each relation mention c, its representation zc implies its relation type, and the distribution of relation type can be described by the soft-max function:
p(ri|zc) =",3.3 Modeling Relation Type,[0],[0]
"exp(z T c ti)∑
rj∈R∪{None} exp(z T c tj)
(5)
where ti ∈ Rvz is the representation for relation type ri.",3.3 Modeling Relation Type,[0],[0]
"Moreover, with the inferred true label o∗c , the relation extraction model can be trained as a multi-class classifier.",3.3 Modeling Relation Type,[0],[0]
"Specifically, we use Eq. 5 to approach the distribution
p(ri|o∗c) =",3.3 Modeling Relation Type,[0],[0]
{ 1 ri = o ∗,3.3 Modeling Relation Type,[0],[0]
"c
0 ri ̸= o∗c (6)
",3.3 Modeling Relation Type,[0],[0]
"Moreover, we use KL-divergence to measure the dissimilarity between two distributions, and formulate model learning as maximizing JR:
JR = − ∑ c∈Cl KL(p(.|zc)||p(.|o∗c)) (7)
where KL(p(.|zc)||p(.|o∗c)) is the KL-divergence from p(ri|o∗c) to p(ri|zc), p(ri|zc) and p(ri|o∗c) has the form of Eq. 5 and Eq. 6.",3.3 Modeling Relation Type,[0],[0]
"Based on Eq. 1, Eq. 4 and Eq. 7, we form the joint optimization problem for model parameters as
min W,v,v∗,l,t,o∗ J = −JR − λ1JE − λ2JT s.t. ∀c",3.4 Model Learning,[0],[0]
"∈ Cl, o∗c = argmax
o∗c JT , zc = g(fc) (8)
Collectively optimizing Eq. 8 allows heterogeneous supervision guiding all three components, while these components would refine the context representation, and enhance each other.
",3.4 Model Learning,[0],[0]
"In order to solve the joint optimization problem in Eq. 8 efficiently, we adopt the stochastic gradient descent algorithm to update {W,v,v∗, l, t} iteratively, and oc∗ is estimated by maximizing JT after calculating zc.",3.4 Model Learning,[0],[0]
"Additionally, we apply the widely used dropout techniques (Srivastava et al., 2014) to prevent overfitting and improve generalization performance.
",3.4 Model Learning,[0],[0]
The learning process of REHESSION is summarized as below.,3.4 Model Learning,[0],[0]
"In each iteration, we would sample a relation mention c from Cl, then sample c’s text
features and conduct the text features’ representation learning.",3.4 Model Learning,[0],[0]
"After calculating the representation of c, we would infer its true label o∗c based on our true label discovery model, and finally update model parameters based on o∗c .",3.4 Model Learning,[0],[0]
We now discuss the strategy of performing type inference for Cu.,3.5 Relation Type Inference,[0],[0]
"As shown in Table 3, the proportion of None in Cu is usually much larger than in Cl.",3.5 Relation Type Inference,[0],[0]
"Additionally, not like other relation types in R, None does not have a coherent semantic meaning.",3.5 Relation Type Inference,[0],[0]
"Similar to (Ren et al., 2016), we introduce a heuristic rule: identifying a relation mention as None when (1) our relation extractor predict it as None, or (2) the entropy of p(.|zc) over R exceeds a pre-defined threshold η.",3.5 Relation Type Inference,[0],[0]
The entropy is calculated as H(p(.|zc)),3.5 Relation Type Inference,[0],[0]
= −∑ri∈R p(ri|zc)log(p(ri|zc)).,3.5 Relation Type Inference,[0],[0]
And the second situation means based on relation extractor this relation mention is not likely belonging to any relation types in R.,3.5 Relation Type Inference,[0],[0]
"In this section, we empirically validate our method by comparing to the state-of-the-art relation extraction methods on news and Wikipedia articles.",4 Experiments,[0],[0]
"In the experiments, we conduct investigations on two benchmark datasets from different domains:1 NYT (Riedel et al., 2010) is a news corpus sampled from∼ 294k 1989-2007 New York Times news articles.",4.1 Datasets and settings,[0],[0]
"It consists of 1.18M sentences, while 395 of them are annotated by authors of (Hoffmann et al., 2011) and used as test data; Wiki-KBP utilizes 1.5M sentences sampled from 780k Wikipedia articles (Ling and Weld, 2012) as training corpus, while test set consists of the 2k sentences manually annotated in 2013 KBP slot filling assessment results (Ellis et al., 2012).
",4.1 Datasets and settings,[0],[0]
"For both datasets, the training and test sets partitions are maintained in our experiments.",4.1 Datasets and settings,[0],[0]
"Furthermore, we create validation sets by randomly sampling 10% mentions from each test set and used the remaining part as evaluation sets.
",4.1 Datasets and settings,[0],[0]
Feature Generation.,4.1 Datasets and settings,[0],[0]
"As summarized in Table 2, we use a 6-word window to extract context features for each entity mention, apply the Stanford
1 Codes and datasets used in this paper can be downloaded at: https://github.com/LiyuanLucasLiu/ ReHession.
CoreNLP tool (Manning et al., 2014) to generate entity mentions and get POS tags for both datasets.",4.1 Datasets and settings,[0],[0]
"Brown clusters(Brown et al., 1992) are derived for each corpus using public implementation2.",4.1 Datasets and settings,[0],[0]
"All these features are shared with all compared methods in our experiments.
",4.1 Datasets and settings,[0],[0]
Labeling Functions.,4.1 Datasets and settings,[0],[0]
"In our experiments, labeling functions are employed to encode two kinds of supervision information.",4.1 Datasets and settings,[0],[0]
"One is knowledge base, the other is handcrafted domain-specific patterns.",4.1 Datasets and settings,[0],[0]
"For domain-specific patterns, we manually design a number of labeling functions3; for knowledge base, annotations are generated following the procedure in (Ren et al., 2016; Riedel et al., 2010).
",4.1 Datasets and settings,[0],[0]
"Regarding two kinds of supervision information, the statistics of the labeling functions are summarized in Table 4.",4.1 Datasets and settings,[0],[0]
"We can observe that heuristic patterns can identify more relation types for KBP datasets, while for NYT datasets, knowledge base can provide supervision for more relation types.",4.1 Datasets and settings,[0],[0]
"This observation aligns with our intuition that single kind of information might be insufficient while different kinds of information can complement each other.
",4.1 Datasets and settings,[0],[0]
We further summarize the statistics of annotations in Table 6.,4.1 Datasets and settings,[0],[0]
"It can be observed that a large portion of instances is only annotated as None, while lots of conflicts exist among other instances.",4.1 Datasets and settings,[0],[0]
This phenomenon justifies the motivation to employ true label discovery model to resolve the conflicts among supervision.,4.1 Datasets and settings,[0],[0]
"Also, we can observe most conflicts involve None type, accordingly, our proposed method should have more advantages over traditional true label discovery methods on the relation extraction task comparing to the relation classification task that excludes None type.",4.1 Datasets and settings,[0],[0]
"We compare REHESSION with below methods: FIGER (Ling and Weld, 2012) adopts multi-label
2https://github.com/percyliang/ brown-cluster
3pattern-based labeling functions can be accessed at: https://github.com/LiyuanLucasLiu/ ReHession
learning with Perceptron algorithm.",4.2 Compared Methods,[0],[0]
"BFK (Bunescu and Mooney, 2005) applies bag-offeature kernel to train a support vector machine; DSL (Mintz et al., 2009) trains a multi-class logistic classifier4 on the training data; MultiR (Hoffmann et al., 2011)",4.2 Compared Methods,[0],[0]
"models training label noise by multi-instance multi-label learning; FCM (Gormley et al., 2015) performs compositional embedding by neural language model.",4.2 Compared Methods,[0],[0]
"CoType-RM (Ren et al., 2016) adopts partial-label loss to handle label noise and train the extractor.
",4.2 Compared Methods,[0],[0]
"Moreover, two different strategies are adopted to feed heterogeneous supervision to these methods.",4.2 Compared Methods,[0],[0]
"The first is to keep all noisy labels, marked as ‘NL’.",4.2 Compared Methods,[0],[0]
"Alternatively, a true label discovery method, Investment (Pasternack and Roth, 2010), is applied to resolve conflicts, which is based on the source consistency assumption and iteratively updates inferred true labels and label functions’ reliabilities.",4.2 Compared Methods,[0],[0]
"Then, the second strategy is to only feed the inferred true labels, referred as ‘TD’.
",4.2 Compared Methods,[0],[0]
4We use liblinear package from https//github.,4.2 Compared Methods,[0],[0]
"com/cjlin1/liblinear
Universal Schemas (Riedel et al., 2013) is proposed to unify different information by calculating a low-rank approximation of the annotations O. It can serve as an alternative of the Investment method, i.e., selecting the relation type with highest score in the low-rank approximation as the true type.",4.2 Compared Methods,[0],[0]
But it doesnt explicitly model noise and not fit our scenario very well.,4.2 Compared Methods,[0],[0]
"Due to the constraint of space, we only compared our method to Investment in most experiments, and Universal Schemas is listed as a baseline in Sec. 4.4.",4.2 Compared Methods,[0],[0]
"Indeed, it performs similarly to the Investment method.
",4.2 Compared Methods,[0],[0]
Evaluation Metrics.,4.2 Compared Methods,[0],[0]
"For relation classification task, which excludes None type from training / testing, we use the classification accuracy (Acc) for evaluation, and for relation extraction task, precision (Prec), recall (Rec) and F1 score (Bunescu and Mooney, 2005; Bach and Badaskar, 2007) are employed.",4.2 Compared Methods,[0],[0]
"Notice that both relation extraction and relation classification are conducted and evaluated in sentence-level (Bao et al., 2014).
",4.2 Compared Methods,[0],[0]
Parameter Settings.,4.2 Compared Methods,[0],[0]
"Based on the semantic meaning of proficient subset, we set ϕ2 to 1/|R∪{None}|, i.e., the probability of generating right label with random guess.",4.2 Compared Methods,[0],[0]
"Then we set ϕ1 to 1 − ϕ2, λ1 = λ2 = 1, and the learning rate α = 0.025.",4.2 Compared Methods,[0],[0]
"As for other parameters, they are tuned on the validation sets for each dataset.",4.2 Compared Methods,[0],[0]
"Similarly, all parameters of compared methods are tuned on validation set, and the parameters achieving highest F1 score are chosen for relation extraction.",4.2 Compared Methods,[0],[0]
"Given the experimental setup described above, the averaged evaluation scores in 10 runs of relation classification and relation extraction on two datasets are summarized in Table 5.
",4.3 Performance Comparison,[0],[0]
"From the comparison, it shows that NL strategy yields better performance than TD strategy, since the true labels inferred by Investment are actually wrong for many instances.",4.3 Performance Comparison,[0],[0]
"On the other hand, as discussed in Sec. 4.4, our method introduces context-awareness to true label discovery, while the inferred true label guides the relation extractor achieving the best performance.",4.3 Performance Comparison,[0],[0]
"This observation justifies the motivation of avoiding the source consistency assumption and the effectiveness of proposed true label discovery model.
",4.3 Performance Comparison,[0],[0]
One could also observe the difference between REHESSION and the compared methods is more significant on the NYT dataset than on the WikiKBP dataset.,4.3 Performance Comparison,[0],[0]
"This observation accords with the fact that the NYT dataset contains more conflicts than KBP dataset (see Table 6), and the intuition is that our method would have more advantages on more conflicting labels.
",4.3 Performance Comparison,[0],[0]
"Among four tasks, the relation classification of Wiki-KBP dataset has highest label quality, i.e. conflicting label ratio, but with least number of training instances.",4.3 Performance Comparison,[0],[0]
And CoType-RM and DSL reach relatively better performance among all compared methods.,4.3 Performance Comparison,[0],[0]
"CoType-RM performs much better than DSL on Wiki-KBP relation classification task, while DSL gets better or similar performance with CoType-RM on other tasks.",4.3 Performance Comparison,[0],[0]
"This may be because the representation learning method is able to generalize better, thus performs better when the training set size is small.",4.3 Performance Comparison,[0],[0]
"However, it is rather vulnerable to the noisy labels compared to DSL.",4.3 Performance Comparison,[0],[0]
"Our method employs embedding techniques, and also integrates context-aware true label dis-
covery to de-noise labels, making the embedding method rather robust, thus achieves the best performance on all tasks.",4.3 Performance Comparison,[0],[0]
"Context Awareness of True Label Discovery.
",4.4 Case Study,[0],[0]
"Although Universal Schemas does not adopted the source consistency assumption, but it’s conducted in document-level, and is context-agnostic in our sentence-level setting.",4.4 Case Study,[0],[0]
"Similarly, most true label discovery methods adopt the source consistency assumption, which means if they trust a labeling function, they would trust it on all annotations.",4.4 Case Study,[0],[0]
"And our method infers true labels in a context-aware manner, which means we only trust labeling functions on matched contexts.
",4.4 Case Study,[0],[0]
"For example, Investment and Universal Schemas refer None as true type for all four instances in Table 7.",4.4 Case Study,[0],[0]
"And our method infers born-in as the true label for the first two relation mentions; after replacing the matched contexts (born) with other words (elected and examined), our method no longer trusts born-in since the modified contexts are no longer matched, then infers None as the true label.",4.4 Case Study,[0],[0]
"In other words, our proposed method infer the true label in a context aware manner.
",4.4 Case Study,[0],[0]
Effectiveness of True Label Discovery.,4.4 Case Study,[0],[0]
"We explore the effectiveness of the proposed contextaware true label discovery component by comparing REHESSION to its variants REHESSION-TD and REHESSION-US, which uses Investment or Universal Schemas to resolve conflicts.",4.4 Case Study,[0],[0]
The averaged evaluation scores are summarized in Table 8.,4.4 Case Study,[0],[0]
We can observe that REHESSION significantly outperforms its variants.,4.4 Case Study,[0],[0]
"Since the only difference between REHESSION and its variants is the model employed to resolve conflicts, this gap verifies the effectiveness of the proposed contextaware true label discovery method.",4.4 Case Study,[0],[0]
Relation extraction aims to detect and categorize semantic relations between a pair of entities.,5.1 Relation Extraction,[0],[0]
"To alleviate the dependency of annotations given by human experts, weak supervision (Bunescu and Mooney, 2007; Etzioni et al., 2004) and distant supervision (Ren et al., 2016) have been employed to automatically generate annotations based on knowledge base (or seed patterns/instances).",5.1 Relation Extraction,[0],[0]
"Universal Schemas (Riedel et al., 2013; Verga et al., 2015; Toutanova et al., 2015) has been proposed to unify patterns and knowledge base, but it’s designed for document-level relation extraction, i.e., not to categorize relation types based on a specific context, but based on the whole corpus.",5.1 Relation Extraction,[0],[0]
"Thus, it allows one relation mention to have multiple true relation types; and does not fit our scenario very well, which is sentence-level relation extraction and assumes one instance has only one relation type.",5.1 Relation Extraction,[0],[0]
"Here we propose a more general framework to consolidate heterogeneous information and further refine the true label from noisy labels, which gives the relation extractor potential to detect more types of relations in a more precise way.
",5.1 Relation Extraction,[0],[0]
"Word embedding has demonstrated great potential in capturing semantic meaning (Mikolov et al., 2013), and achieved great success in a wide range of NLP tasks like relation extraction (Zeng et al., 2014; Takase and Inui, 2016; Nguyen and Grishman, 2015).",5.1 Relation Extraction,[0],[0]
"In our model, we employed the embedding techniques to represent context information, and reduce the dimension of text features, which allows our model to generalize better.",5.1 Relation Extraction,[0],[0]
"True label discovery methods have been developed to resolve conflicts among multi-source information under the assumption of source consistency (Li et al., 2016; Zhi et al., 2015).",5.2 Truth Label Discovery,[0],[0]
"Specifically, in the spammer-hammer model (Karger et al., 2011), each source could either be a spammer, which annotates instances randomly; or a hammer, which annotates instances precisely.",5.2 Truth Label Discovery,[0],[0]
"In this paper, we assume each labeling function would be a hammer on its proficient subset, and would be a spammer otherwise, while the proficient subsets are identified in the embedding space.
",5.2 Truth Label Discovery,[0],[0]
"Besides data programming, socratic learning (Varma et al., 2016) has been developed to conduct binary classification under heterogeneous supervi-
sion.",5.2 Truth Label Discovery,[0],[0]
"Its true label discovery module supervises the discriminative module in label level, while the discriminative module influences the true label discovery module by selecting a feature subset.",5.2 Truth Label Discovery,[0],[0]
"Although delicately designed, it fails to make full use of the connection between these modules, i.e., not refine the context representation for classifier.",5.2 Truth Label Discovery,[0],[0]
"Thus, its discriminative module might suffer from the overwhelming size of text features.",5.2 Truth Label Discovery,[0],[0]
"In this paper, we propose REHESSION, an embedding framework to extract relation under heterogeneous supervision.",6 Conclusion and Future Work,[0],[0]
"When dealing with heterogeneous supervisions, one unique challenge is how to resolve conflicts generated by different labeling functions.",6 Conclusion and Future Work,[0],[0]
"Accordingly, we go beyond the “source consistency assumption” in prior works and leverage context-aware embeddings to induce proficient subsets.",6 Conclusion and Future Work,[0],[0]
"The resulting framework bridges true label discovery and relation extraction with context representation, and allows them to mutually enhance each other.",6 Conclusion and Future Work,[0],[0]
"Experimental evaluation justifies the necessity of involving contextawareness, the quality of inferred true label, and the effectiveness of the proposed framework on two real-world datasets.
",6 Conclusion and Future Work,[0],[0]
There exist several directions for future work.,6 Conclusion and Future Work,[0],[0]
One is to apply transfer learning techniques to handle label distributions’ difference between training set and test set.,6 Conclusion and Future Work,[0],[0]
Another is to incorporate OpenIE methods to automatically find domainspecific patterns and generate pattern-based labeling functions.,6 Conclusion and Future Work,[0],[0]
Research was sponsored in part by the U.S. Army Research Lab.,7 Acknowledgments,[0],[0]
under Cooperative Agreement No.,7 Acknowledgments,[0],[0]
"W911NF-09-2-0053 (NSCTA), National Science Foundation IIS-1320617, IIS 16-18481, and",7 Acknowledgments,[0],[0]
"NSF IIS 17-04532, and grant 1U54GM114838 awarded by NIGMS through funds provided by the transNIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).",7 Acknowledgments,[0],[0]
The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government.,7 Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon.,7 Acknowledgments,[0],[0]
Relation extraction is a fundamental task in information extraction.,abstractText,[0],[0]
"Most existing methods have heavy reliance on annotations labeled by human experts, which are costly and time-consuming.",abstractText,[0],[0]
"To overcome this drawback, we propose a novel framework, REHESSION, to conduct relation extractor learning using annotations from heterogeneous information source, e.g., knowledge base and domain heuristics.",abstractText,[0],[0]
"These annotations, referred as heterogeneous supervision, often conflict with each other, which brings a new challenge to the original relation extraction task: how to infer the true label from noisy labels for a given instance.",abstractText,[0],[0]
"Identifying context information as the backbone of both relation extraction and true label discovery, we adopt embedding techniques to learn the distributed representations of context, which bridges all components with mutual enhancement in an iterative fashion.",abstractText,[0],[0]
Extensive experimental results demonstrate the superiority of REHESSION over the state-of-the-art.,abstractText,[0],[0]
Heterogeneous Supervision for Relation Extraction: A Representation Learning Approach,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1723–1731 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1723",text,[0],[0]
Many idiomatic expressions may be interpreted both figuratively or literally.,1 Introduction,[0],[0]
Their intended usages depend on how they fit with their contexts.,1 Introduction,[0],[0]
"For example, the idiom ”spill the beans” is used figuratively in the first instance below, and literally in the second:
(1) [fig.]",1 Introduction,[0],[0]
The beans have been spilled.,1 Introduction,[0],[0]
From what I’ve read on Twitter I could probably fill out the forms and submit it to the FISA court.,1 Introduction,[0],[0]
"I don’t know what the big secret is.1
(2) [lit.]",1 Introduction,[0],[0]
"Spill the beans, flip the fruit, bust open a box of hot pockets.",1 Introduction,[0],[0]
"Make a general mess of the kitchen.2
This type of ambiguity is commonplace – prior work suggests that about half out of a sample of
1https://twitter.com/BTeboe/status/ 958792419302100993
2https://twitter.com/DukeRaccoon/ status/477530732173471744
60 idioms have a clear literal meaning as well as a figurative one (Fazly et al., 2009).",1 Introduction,[0],[0]
"Being able to distinguish the intended usage of an idiom in context has been shown to benefit many natural language processing (NLP) applications, e.g., machine translation and sentiment analysis (Salton et al., 2014; Williams et al., 2015).
",1 Introduction,[0],[0]
"While supervised models for idiom usage recognition have had some successes, they require appropriately annotated training examples (Peng et al., 2014; Byrne et al., 2013; Liu and Hwa, 2017).",1 Introduction,[0],[0]
"A more challenging problem is to recognize idiom usages without a dictionary or some annotated examples (Korkontzelos et al., 2013).",1 Introduction,[0],[0]
Some previous unsupervised models tried to exploit linguistic differences in usages.,1 Introduction,[0],[0]
"For example, Fazly et al.(2009) observed that an idiom appearing in its canonical form is usually used figuratively; Sporleder and Li(2009) relied on the break in lexical coherence between the idioms and the context to signal a figurative usage.",1 Introduction,[0],[0]
"These heuristics, however, are not always applicable because the distinctions they depend upon may not be present or obvious.",1 Introduction,[0],[0]
"To improve generalization across different idioms and usage contexts, we need a more reliable heuristic, and appropriately incorporate it into an unsupervised learning framework.
",1 Introduction,[0],[0]
"We propose a heuristic that differentiates usages based on distributional semantics (Harris, 1954; Turney and Pantel, 2010).",1 Introduction,[0],[0]
"Our key insight is that when an idiom is used literally, its relationship with its context is more predictable than when it is used figuratively.",1 Introduction,[0],[0]
"This is because the literal meaning of an idiom is compositional (Katz and Giesbrecht, 2006), and the constituent words that make up the idiom are also meant literally.",1 Introduction,[0],[0]
"For example, in instance (2), spill is meant literally and can take on objects other than beans; moreover, one of the context words, mess, can often be seen to co-
occur with spill in other text, even without beans.",1 Introduction,[0],[0]
Our strategy is to represent an idiom’s literal usage in terms of the word embeddings of the idiom’s constituent words and other words they frequently co-occur with.,1 Introduction,[0],[0]
"Then, for any instance in which the idiom’s usage is not known, we only need to determine the semantic similarity between that instance and the idiom’s literal representation.",1 Introduction,[0],[0]
"We define a literal usage metric that estimates the likelihood that an instance would be labeled ”literal”.
",1 Introduction,[0],[0]
"While the literal usage metric captures the distributional semantic information of the context, we find that some other linguistic cues are also significant for usage detection (such as whether the subject of the sentence is a person); therefore, we allow our model to further refine through unsupervised methods.",1 Introduction,[0],[0]
"Specifically, we treat the usage (figurative or literal) as a hidden variable in probabilistic latent variable models, and we define a set of features that are linguistically relevant for idiom usage detection as observables.",1 Introduction,[0],[0]
"We integrate our literal usage metric with the latent variable models by treating the metric outputs as soft labels to guide the latent variable models toward grouping by usages.
",1 Introduction,[0],[0]
"We hypothesize that unsupervised learning in a more linguistically motivated feature space, informed by soft labels from a semantically driven metric, will produce more robust classifiers.",1 Introduction,[0],[0]
We conduct experiments comparing our approach against other supervised and unsupervised baselines.,1 Introduction,[0],[0]
Results suggest that our approach achieves performances that are competitive to supervised models.,1 Introduction,[0],[0]
"Despite the common perception that idioms are mainly used figuratively, many can also be meant literally.",2 Related Work,[0],[0]
A number of models have been proposed in the literature to recognize an idiom’s usages under different context.,2 Related Work,[0],[0]
Many rely on specific linguistic property to draw a clear-cut decision boundary between literal and figurative usages.,2 Related Work,[0],[0]
"For example, Fazly et al. (2009) proposed a method that relies on the concept of canonical form.",2 Related Work,[0],[0]
"Based on the observation that while literal usages are less syntactically restricted, figurative usages tend to occur in a small number of canonical form(s).",2 Related Work,[0],[0]
"As shown in the examples above, however, this rule of thumb does not always hold.",2 Related Work,[0],[0]
"Sporleder and Li (2009) proposed a method by
building a cohesion graph to include all content words in the context; if removing the idiom improves cohesion, they assume the instance is figurative.",2 Related Work,[0],[0]
"Later, Li and Sporleder (2009) used their cohesion graph method to label a subset of the test data with high confidence.",2 Related Work,[0],[0]
"This subset is then passed on as training data to the supervised classifier, which then labels the remainder of the dataset.
",2 Related Work,[0],[0]
"When manually annotated examples are available, supervised classifiers are effective.",2 Related Work,[0],[0]
"Rajani et al. (2014) extracted all non-stop-words in the context and used them as ”bag of words” features to train a L2 regularized Logistic Regression (L2LR) classifier (Fan et al., 2008).",2 Related Work,[0],[0]
"As local context of an idiom holds clues for discriminating between its literal and figurative usages, Liu and Hwa (2017) find that context representation also plays a significant role in idiom usage recognition.",2 Related Work,[0],[0]
"They took an adaptive approach, applying supervised ensemble learning over three classifiers based on different context representations (Peng et al., 2014; Birke and Sarkar, 2006; Rajani et al., 2014).",2 Related Work,[0],[0]
"Given a target idiomatic expression and a collection of instances in which the idiom occurs, our proposed system (Figure 1) determines whether the idiom in each instance is meant figuratively or literally.",3 Our Approach,[0],[0]
We first build a Literal Usage Representation for each idiom by leveraging the distributional semantics of its constituents (Sec 3.1).,3 Our Approach,[0],[0]
"Given an instance of idiom, we can determine its usage by the semantic similarity between the context of the instance and the Literal Usage Representation.",3 Our Approach,[0],[0]
"We define a Literal Usage Metric to transform the semantic similarity score into soft label, i.e., an initial rough estimation of the instance’s usage (Sec 3.2).",3 Our Approach,[0],[0]
"Finally, we treat the soft labels as distant supervision for downstream probabilistic latent variable models, in which the usages are considered as the hidden variables and are represented over a set of features.",3 Our Approach,[0],[0]
An idiom co-occurs with different sets of words depending on whether it is meant literally or figuratively.,3.1 Literal Usage Representation,[0],[0]
"For example, when used literally, get wind is more likely to co-occur with words such as rain, storm or weather; in contrast, when used figuratively, it frequently co-occurs with rumor or
story, etc.",3.1 Literal Usage Representation,[0],[0]
"Comparing the two sets of words associated with the idiom, we see that the literal set of words also tend to co-occur with just wind, a constituent word within the idiom.",3.1 Literal Usage Representation,[0],[0]
"Therefore, even without annotated data or dictionary, we may still approximate a representation for the literal meaning of an idiom by the idiom’s constituent words and their semantic relationship to other words.",3.1 Literal Usage Representation,[0],[0]
"To do so, we begin by initializing a literal meaning set to just the idiom’s main constituent words3; we then grow the set by adding two types of semantically related words.",3.1 Literal Usage Representation,[0],[0]
"First, we look for co-occuring words in a large textual corpus (e.g., (David et al., 2005)): for each constituent word w, we randomly sample s sentences that containw from the corpus; we extract the top n most frequent words (excluding stop words) and add them to the literal meaning set.",3.1 Literal Usage Representation,[0],[0]
"Second, we look for words that are semantically close in a word embedding space: we train a continuous bag-of-words (CBOW) embedding model (Mikolov et al., 2013) and add additional t words that are the most related to w using cosine similarity.
",3.1 Literal Usage Representation,[0],[0]
"All together, the literal usage representation is a collection of vectors, i.e., the embeddings of the words in the final extended literal meaning set.",3.1 Literal Usage Representation,[0],[0]
"The size of the set depends on parameters s, n, and t; if the chosen values are too small, we do not end up with a word collection that is representative enough; if the numbers are too large, we would only be wasting computing resources chasing Zipfian tails.",3.1 Literal Usage Representation,[0],[0]
"Parameter setting choices are discussed further in the experiment section.
",3.1 Literal Usage Representation,[0],[0]
"3We observe that the nouns tend to be the most indicative of the idiom’s literal meaning, but if the idiom does not contain any noun, we back off to any constituent word that is not a stop word.",3.1 Literal Usage Representation,[0],[0]
"Among all the instances to be classified, we expect the context words of the literal cases to be more semantically close to the literal usage representation we just formed.",3.2 Literal Usage Metrics,[0],[0]
Let L denote the set of words in the literal usage representation for the target idiom.,3.2 Literal Usage Metrics,[0],[0]
"For each instance, letC be the set of non-stop context words in the instance.",3.2 Literal Usage Metrics,[0],[0]
"We calculate s, the semantic similarity score between the context of the instance and the literal usage representation as follows:
s = 1 |C| ∑ c∈C 1 |L| ∑ l∈L sim(c, l) (1)
where c denotes a word in C, l denotes a word in L and sim(c, l) refers to the cosine similarity between the word embeddings of c and l.
Let S = {s1, s2, ...sn} be the set of semantic similarity scores for all the instances we wish to classify.",3.2 Literal Usage Metrics,[0],[0]
Instances with higher scores are more likely to use the idiom literally.,3.2 Literal Usage Metrics,[0],[0]
A naive literal usage metrics is to choose a predefined threshold for all idioms and label all the instances with score above the threshold as literal usages.,3.2 Literal Usage Metrics,[0],[0]
This approach is unlikely to work well in practice.,3.2 Literal Usage Metrics,[0],[0]
"As noted by previous work, idioms have different levels of semantic analyzability (Gibbs et al., 1989; Cacciari and Levorato, 1998).",3.2 Literal Usage Metrics,[0],[0]
"When an idiom has a high degree of semantic analyzability, its contextual words will be more semantically close to the literal usage representation, thus a higher threshold is needed.
",3.2 Literal Usage Metrics,[0],[0]
"In this work, we select a different decision threshold for each idiom adaptively based on the similarity scores distribution.",3.2 Literal Usage Metrics,[0],[0]
"And most importantly, rather than generate a hard label, we transform these scores into a probabilistic metric,
where 0 means the usage in the instance is almost certainly figurative while 1.0 means it is literal.
",3.2 Literal Usage Metrics,[0],[0]
We propose a metric based on the principle of Minimum Variance (MinV).,3.2 Literal Usage Metrics,[0],[0]
"That is, we first sort the scores in S and choose the threshold (from these scores) that minimizes the sum of variances of the two resulting clusters.",3.2 Literal Usage Metrics,[0],[0]
"For each instance i, we then apply the following metric to estimate the probability that the idiom in instance i is meant literally based on its semantic similarity score si :
Pri = 1
1 + e−k∗(si−t) (2)
where k is a constant weighting factor and t indicates the learned threshold.",3.2 Literal Usage Metrics,[0],[0]
"The intuition is that the larger the difference between si and the threshold is, the more likely the instance i is literal; the probability of literal usage is not linearly correlated to the difference, we use the sigmoid function to account for this non-linearity.",3.2 Literal Usage Metrics,[0],[0]
We incorporate k to scale the value of the difference since it is generally very small (close to 0).,3.2 Literal Usage Metrics,[0],[0]
"Without k, all the Pr values gravitate toward 0.5, rendering the soft label being equivalent to random guess.",3.2 Literal Usage Metrics,[0],[0]
We set k to 5 for all the idioms based on a development set.,3.2 Literal Usage Metrics,[0],[0]
"The soft label, generated by MinV (the literal usage metric), captures the distributional semantic information of the context.",3.3 Heuristically Informed Usage Recognition,[0],[0]
"In practice, there are a variety of other linguistic features which are also informative of the intended usage of idiom.",3.3 Heuristically Informed Usage Recognition,[0],[0]
We explore probabilistic latent variable models over a collection of features that are linguistically relevant for idiom usage detection.,3.3 Heuristically Informed Usage Recognition,[0],[0]
The soft label is integrated into the unsupervised learning of hidden usages as a distant supervision.,3.3 Heuristically Informed Usage Recognition,[0],[0]
"In this section, we will describe the proposed features in the latent variable models and how we integrate the soft label into the learning process.",3.3 Heuristically Informed Usage Recognition,[0],[0]
"To predict an idiom’s usage in instances, we consider two representative probabilistic latent variable models:",3.3.1 Latent Variable Models,[0],[0]
"Latent Dirichlet Allocation (LDA) (Blei et al., 2003)4 and unsupervised Naive Bayes (NB).",3.3.1 Latent Variable Models,[0],[0]
"For both models, the latent variable is the idiom usage (figurative vs. literal); the observables
4Although originally conceived for modeling document content, LDA can be applied to any kind of discrete input
are linguistic features that can be extracted from the instances, described below:
",3.3.1 Latent Variable Models,[0],[0]
"Subordinate Clause We encode a binary feature indicating whether the target expression is followed by a subordinate clause (the Stanford Parser (Chen and Manning, 2014) is used).",3.3.1 Latent Variable Models,[0],[0]
This feature is useful for some idioms such as in the dark.,3.3.1 Latent Variable Models,[0],[0]
"It usually suggests a figurative usage as in You’ve kept us totally in the dark about what happened that night.
",3.3.1 Latent Variable Models,[0],[0]
"Selectional Preference Violation of selectional preference is normally a signal of figurative usage (e.g., having an abstract entity as the subject of play with fire).",3.3.1 Latent Variable Models,[0],[0]
We encode this feature if the head word of the idiom is a verb and focus on the subject of the verb.,3.3.1 Latent Variable Models,[0],[0]
"We apply Stanford Name Entity tagger (Finkel et al., 2005) with 3 classes (”Location”, ”Person”, ”Organization”) on the sentence containing the idiom.",3.3.1 Latent Variable Models,[0],[0]
"If the subject is labeled as an Entity, its class will be encoded in the feature vector.",3.3.1 Latent Variable Models,[0],[0]
Pronouns such as ”I” and ”he” also indicate the subject is a ”Person”.,3.3.1 Latent Variable Models,[0],[0]
"However, they are normally not tagged by Stanford Name Entity tagger.",3.3.1 Latent Variable Models,[0],[0]
"To overcome this issue, we add Part-of-Speech of the subject into the feature vector.
",3.3.1 Latent Variable Models,[0],[0]
Abstractness Abstract words refer to things which are hard to perceive directly with our senses.,3.3.1 Latent Variable Models,[0],[0]
"Abstractness has been shown to be useful in the detection of metaphor, another type of figurative language (Turney et al., 2011).",3.3.1 Latent Variable Models,[0],[0]
A figurative usage of an idiomatic phrase may have relatively more abstract contextual words.,3.3.1 Latent Variable Models,[0],[0]
"For example, in the sentence She has lived life in the fast lane, the word life is considered as an abstract word.",3.3.1 Latent Variable Models,[0],[0]
This is a useful indicator that in the fast lane is used figuratively.,3.3.1 Latent Variable Models,[0],[0]
"We use the MRC Psycholinguistic Database Machine Usable Dictionary (Coltheart, 1981) which contains a list of 4295 words with their abstractness measure between 100 and 700.",3.3.1 Latent Variable Models,[0],[0]
We calculate the average abstractness score for all the contextual words (with stop words being removed) in the sentence containing the idiom.,3.3.1 Latent Variable Models,[0],[0]
"The score is then transformed into categorical feature to overcome sparsity problem based on the following criteria: concrete (450 - 700), medium (350 - 450), abstract (100 - 350).
",3.3.1 Latent Variable Models,[0],[0]
Neighboring Words Words preceding and following the idiomatic expression can be very informative in terms of usage recognition.,3.3.1 Latent Variable Models,[0],[0]
"For example, words such as relax or shower before the idiom in hot water often signal a literal usage.
",3.3.1 Latent Variable Models,[0],[0]
Part-of-Speech of the Neighboring Words Class of neighboring words might be useful as well.,3.3.1 Latent Variable Models,[0],[0]
"For example, a pronoun preceding dog’s age generally indicates a literal usage, as in I think my dog’s age is starting to catch up.",3.3.1 Latent Variable Models,[0],[0]
"She sometimes needs help to jump on to my bed, while a determiner usually marks a figurative usage, as in It’s been a dog’s age since I’ve used Twitter.",3.3.1 Latent Variable Models,[0],[0]
"Given a collection of instances and their features, either LDA or NB can separate the instances into two groups (hopefully, by usages), but it does not associate the right label (i.e., ”figurative” or ”literal”) to the groups.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We do not want to rely on any manual annotations for this step.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Therefore, we integrate the automatically generated soft labels (based on MinV, our literal usage metric) into the unsupervised learning procedure as a weak form of supervision.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Formally, we want to estimate each instance’s posterior distribution over (literal/figurative) usages θdu and usage-feature distribution φuf .",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"For LDA, we derive a Gibbs sampling algorithm which incorporates the soft label into the learning procedure.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We refer it as informed Gibbs sampling (infGibbs).,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"For unsupervised naive Bayes model, we adapt the classical Expectation-Maximization algorithm to integrate the soft label.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"We refer it as informed ExpectationMaximization (infEM).
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Informed Gibbs Sampling The Gibbs sampling algorithm (Griffiths and Steyvers, 2004) used in traditional LDA initializes each word token a random hidden topic.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The system needs to interpret the learned topics post-hoc, e.g., by human annotation.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"In our case, for each feature f in each instance, an initial random usage biased by the instance’s soft label is assigned to f (i.e., a Bernoulli trial).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Since the soft label explicitly encodes an instance’s literal and figurative usage distribution, we do not need to interpret the learned usages at the end of the algorithm.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Based on these assignments, we build a feature-usage counting matrix CFU and instance-usage counting matrix CDU with dimensions |F | × 2 and |D|",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"× 2 respectively (|F | is the feature size and |D| is the number of instances): CFUi,j is the count of feature i assigned to usage j; CDUd,j is the count of features assigned to usage j in instance d.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Then for each feature f in each instance, we resample a new usage for f and matrices CFU and CDU will
be updated accordingly.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This step will be repeated for T times.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The resampling equation is:
p(ui = j|u−i, f) ∝",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"pj · C
fi −i,j+β
C (∗) −i,j+|F |β
· C di −i,j+α
C di −i,∗+|U |α
(3) where i indexes features in the instance d, j is an index into literal and figurative usages, ∗ indicates a summation over that dimension and − means excluding the corresponding instance.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The first factor pj is the soft label encoding prior usage distribution.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The second factor represents the probability of feature f under usage j (Cfi−i,j is the count of the feature f assigned to usage j, excluding the current usage assignment ui).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The third factor represents the probability of usage j in the current instance (Cdi−i,j is the count of linguistic features which are assigned to usage j in the current instance, excluding the current feature f ).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The value of |U | is 2, representing the number of usages (i.e., figurative and literal).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
α and β are the hyper-parameters from the Dirichlet priors (we set both of them to 1).,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The core idea of Equation 3 is to integrate both distribution semantic information (soft label, the first factor) and linguistically motivated features (the second and third factors) into the inference procedure.
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The matrices of CFU and CDU from the last 10% ∗ T iterations are averaged and then normalized to approximate the true usage-feature distribution φuf and instance-usage distribution θdu respectively.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The final result is determined by θdu, i.e., assigning each instance with the usage of probability higher than 0.5.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We do average to have a more stable result because an accidental bad sampling would affect our model negatively if we only use the CFU and CDU from the last iteration.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This procedure is important for some idioms if their feature space is sparse.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The iteration number T is set to 500 based on a development set.
",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Informed Expectation Maximization Combining a Naive Bayes classifier with the EM algorithm has been widely used in text classification and word sense disambiguation (Hristea, 2013; Nigam et al., 2000).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"In our case, we want to construct a model to recover the missing literal and figurative labels of the instances of the target idiom.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
This section describes two extensions to the basic EM algorithm for idiom usage recognition.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"The extensions help improve parameter estimation by taking the automatically learned soft labels into
consideration.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Our informed EM method extends a basic version for NB (Hristea, 2013), where the initial parameter values θdu and φuf are chosen randomly.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"At each iteration, the E-step of the algorithm estimates the expectations of the missing values (i.e. the literal and figurative usage) given the latest iteration of the model parameters; the M-step maximizes the likelihood of the model parameters using the previously-computed expectations of the missing values.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"As we’ve done with extending Gibbs sampling for LDA, we also perform two similar adaptations on conventional EM for NB to incorporate soft labels.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"First, we assign each instance an initial usage distribution θdu directly using the soft label, and then initialize the usagefeature distribution φuf using these assignments.",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
We refer it as informed initialization.,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"Second, in the E-step, we multiply the expectation result of the basic EM with the soft label as the new expected usage for each instance (i.e., updating θdu).",3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
The M-step is the same as basic EM to update the usage-feature distribution φuf .,3.3.2 Incorporating Soft Label into Usage Recognition,[0],[0]
"We conduct experiments to address three questions:
1.",4 Evaluation,[0],[0]
How effective is our overall approach?,4 Evaluation,[0],[0]
"How does it compare against previous work?
2.",4 Evaluation,[0],[0]
"How effective is our literal usage metric (i.e., MinV) compared to other heuristics?
3.",4 Evaluation,[0],[0]
How effective is our literal usage metric at informing downstream learning processes?,4 Evaluation,[0],[0]
Models Our main experiments will evaluate the two variants of the proposed fully unsupervised model as described in section 3: MinV+infGibbs and MinV+infEM.,4.1 Experimental Setup,[0],[0]
We report the average performance of our models over 5 runs.,4.1 Experimental Setup,[0],[0]
Performing multiple runs is necessary because we have a sampling process.,4.1 Experimental Setup,[0],[0]
"They are compared with three baseline unsupervised models: Sporleder and Li (2009), Li and Sporleder (2009)5 and Fazly et al. (2009); and two baseline supervised models: Rajani et al. (2014) and Liu and Hwa (2017) (using 5-fold cross validation).
",4.1 Experimental Setup,[0],[0]
"5We replace Normalized Google Distance (NGD) with word embeddings to measure the semantic relatedness between words due to the query frequency restriction on the API of NGD.
",4.1 Experimental Setup,[0],[0]
"Parameter setting Recall that in order to build the literal usage representation of an idiom, we need to sample s sentences that contain each constituent word w from an external corpus; extract from them the top n most frequently co-occurring words with w; then separately find t words that are semantically similar to w using word embeddings.",4.1 Experimental Setup,[0],[0]
"To set parameters with values in reasonable ranges, we evaluated MinV on a small development set.",4.1 Experimental Setup,[0],[0]
"We picked 10 idioms that are different from the evaluation set, scraped 50 instances from the web for each idiom, and labeled them ourselves.",4.1 Experimental Setup,[0],[0]
"We find that s >= 100, n=10, and t=5 yield good results.
",4.1 Experimental Setup,[0],[0]
"We use the gensim toolkit (Řehůřek and Sojka, 2010) and train our word embedding model using the continuous bag of word model on Text8 Corpus6.",4.1 Experimental Setup,[0],[0]
Negative sampling is applied as the training method; the min count is set to 2.,4.1 Experimental Setup,[0],[0]
"For the other parameters, we use the default settings in gensim.",4.1 Experimental Setup,[0],[0]
Evaluative Data,4.1 Experimental Setup,[0],[0]
"Our goal is to compare all the methods under two public available corpora: SemEval 2013 Task 5B corpus (Korkontzelos et al., 2013), which is used by prior supervised methods (Liu and Hwa, 2017; Rajani et al., 2014) and verb–noun combination (VNC) dataset (Cook et al., 2008), which is used by a prior unsupervised method (Fazly et al., 2009).",4.1 Experimental Setup,[0],[0]
"However, there are some methods-datasets conflicts that have to be resolved.",4.1 Experimental Setup,[0],[0]
"Because the idioms in the SemEval dataset are all in their canonical forms, and because the idioms are not restricted to the verb-noun combination, we cannot evaluate the method by Fazly et al. on this dataset (as their method is tailored to verbnoun combination).",4.1 Experimental Setup,[0],[0]
"Some idioms from the VNC dataset are almost always used figuratively (or literally), which presents a problem for supervised methods.",4.1 Experimental Setup,[0],[0]
"To facilitate full comparisons, we select the subset of idioms from the VNC corpus whose number of literal and figurative instances are both higher than 10.",4.1 Experimental Setup,[0],[0]
A summary of the two corpora is shown in Table 1.,4.1 Experimental Setup,[0],[0]
"Note that each instance in SemEval corpus has about 3∼5 sentences; for consistency, we use 3 sentences as the context: the sentence with the target idiom and two neighboring sentences.",4.1 Experimental Setup,[0],[0]
Evaluation metric,4.1 Experimental Setup,[0],[0]
"Following the convention in prior works, we report the F-score for the recognition of figurative usages and the overall accuracy.
6From http://mattmahoney.net/dc/text8.",4.1 Experimental Setup,[0],[0]
zip,4.1 Experimental Setup,[0],[0]
Table 2 shows the result of our models and the other comparative methods.,4.2 The Performance of Our Full Models,[0],[0]
"Our proposed models show consistent performance across the two corpora, outperforming the unsupervised baselines from Sporleder and Li (2009), Li and Sporleder (2009) and the supervised model from Rajani et al. (2014).",4.2 The Performance of Our Full Models,[0],[0]
"Moreover, there is no statistical significance in the F-score difference between the supervised ensemble model from Liu and Hwa (2017) and our models.
",4.2 The Performance of Our Full Models,[0],[0]
"On the VNC corpus, our models have comparable average scores as that of Fazly et al. (2009); our scores are more stable across different idioms.",4.2 The Performance of Our Full Models,[0],[0]
"While the method of Fazly et al. is nearly perfect for some idioms (0.98 on ”take heart”), it performs poorly for others (e.g., 0.33 on ”pull leg”).",4.2 The Performance of Our Full Models,[0],[0]
Their algorithm has trouble with idioms whose canonical and non-canonical forms can appear frequently both in literal and figurative usages.,4.2 The Performance of Our Full Models,[0],[0]
"The core of our approach is MinV, the literal usage metric we developed to generate soft labels to guide the unsupervised learning.",4.3 Effectiveness of MinV,[0],[0]
"This experiment examines its effectiveness by creating usage classifications directly from it (i.e., if MinV predicts a probability of >0.5, predict ”literal”).",4.3 Effectiveness of MinV,[0],[0]
"We compare MinV against two alternative heuristics.
",4.3 Effectiveness of MinV,[0],[0]
MinV is based on two core ideas.,4.3 Effectiveness of MinV,[0],[0]
"First, if an idiom is used figuratively, we expect to see a big difference (low similarity scores) between its context and the semantic representation of idiom’s literal usage.",4.3 Effectiveness of MinV,[0],[0]
"The idea is similar to that of Sporleder and Li (2009), but they relied on lexical chain instead of distributional semantics.",4.3 Effectiveness of MinV,[0],[0]
"Second, instead of choosing a predefined threshold to separate the raw semantic similarity scores, we select a different decision threshold for each idiom adaptively based on the distribution of the scores.",4.3 Effectiveness of MinV,[0],[0]
"So as an alternative, we compare MinV against a Fixed-Threshold heuristic that labels an instance as ”literal” if its raw score is higher than some
global threshold (set to 0.346 based on development data).
",4.3 Effectiveness of MinV,[0],[0]
"In Table 3, we observe that Minv outperforms both Sporleder and Li’s model as well as FixedThreshold, but using MinV by itself is not sufficient.",4.3 Effectiveness of MinV,[0],[0]
"It has great fluctuations, e.g., the F-Score for individual idioms varies from 0.43 to 0.88.",4.3 Effectiveness of MinV,[0],[0]
Recall that MinV +infGibbs has a smaller fluctuation across different idioms in Table 2.,4.3 Effectiveness of MinV,[0],[0]
"These results suggest that the subsequent learning process is effective.
",4.3 Effectiveness of MinV,[0],[0]
"Through error analysis, we find two major factors contributing to the performance fluctuation.",4.3 Effectiveness of MinV,[0],[0]
"First, the context itself could be misleading.",4.3 Effectiveness of MinV,[0],[0]
"An error case of play ball by MinV is:
All 10-year-old Minnie Cruttwell wants to do is play with the boys , but the Football Association are not playing ball.",4.3 Effectiveness of MinV,[0],[0]
"She is a member of a mixed team called Balham Blazers , but the FA say she must join a girls’ team when she is 12.
",4.3 Effectiveness of MinV,[0],[0]
The context words in bold (which are related to the word ”ball”) mislead MinV to predict a ”literal” usage when it is actually a ”figurative” usage (since an organization such as the Football Association cannot literally play ball).,4.3 Effectiveness of MinV,[0],[0]
"Second, not all content words in the context are relevant for distinguishing the idiom’s usage.",4.3 Effectiveness of MinV,[0],[0]
A future direction is to prune contextual words more intelligently.,4.3 Effectiveness of MinV,[0],[0]
We have argued that an advantage of using a metric with a probabilistic interpretation instead of a binary class heuristic is that its scores can be incorporated into subsequent learning models as soft labels.,4.4 Integration of MinV into Learning,[0],[0]
"In this set of experiments, we evaluate the impact of the metric on the learning methods.",4.4 Integration of MinV into Learning,[0],[0]
"First, we consider unsupervised learning without input from the literal usage metric.",4.4 Integration of MinV into Learning,[0],[0]
We cluster the instances with the original Gibbs sampling and EM algorithms and then label the two clusters with the majority usage within the clusters.,4.4 Integration of MinV into Learning,[0],[0]
"Second, we explore using the information from the literal usage metric as ”noisy gold standard” to perform supervised training on a nearest neighbors (NN) classifier.",4.4 Integration of MinV into Learning,[0],[0]
"Specifically, the literal and figurative instances labeled by MinV with high confidence (top 30%) are used as example set.",4.4 Integration of MinV into Learning,[0],[0]
"Then for each test instance, we calculate its cosine similarity (in feature space) to the literal and figurative example sets and assign the label of the closest set.",4.4 Integration of MinV into Learning,[0],[0]
"We refer this model as MinV +NN.
Table 4 shows the performances of the new models, which are all worse than our full models MinV +infGibbs and MinV +infEM.",4.4 Integration of MinV into Learning,[0],[0]
This highlights the advantage of integrating distributional semantic information and local features into one single learning procedure.,4.4 Integration of MinV into Learning,[0],[0]
"Without the informed prior (encoded by the soft labels), the Gibbs sampling and EM algorithms only seek to maximize the probability of the observed data, and may fail to learn the underlying usage structure.
",4.4 Integration of MinV into Learning,[0],[0]
The model MinV +NN is not as competitive as our full models.,4.4 Integration of MinV into Learning,[0],[0]
It is too sensitive to the selected instances.,4.4 Integration of MinV into Learning,[0],[0]
"Even though the training examples are instances that MinV is the most confident about, there are still mislabelled instances.",4.4 Integration of MinV into Learning,[0],[0]
These ”noisy training examples” would lead the NN classifier to make unreliable predictions.,4.4 Integration of MinV into Learning,[0],[0]
"In contrast, our unsupervised learning is less sensitive to the performance of MinV; it can achieve a decent performance for an idiom even when the quality of the soft labels is poor.",4.4 Integration of MinV into Learning,[0],[0]
"For example, when using MinV as a stand-alone model for break a leg, its figura-
tive F-score is only 0.43, but through further training, the full model MinV+infGibbs achieves 0.64.",4.4 Integration of MinV into Learning,[0],[0]
Fig. 2 shows the training curve.,4.4 Integration of MinV into Learning,[0],[0]
A possible reason for this phenomenon is that the soft label is integrated into the learning process by biasing the sampling procedure (see Equation 3).,4.4 Integration of MinV into Learning,[0],[0]
We only encourage our model to follow the distributional semantic evidence captured by soft label and do not force it.,4.4 Integration of MinV into Learning,[0],[0]
So if there are strong evidences encoded by the linguistically motivated features in the instances to overcome the soft label it still has the freedom to do so.,4.4 Integration of MinV into Learning,[0],[0]
We have presented an unsupervised method for idiom usage recognition built upon the heuristic that instances that use the idiom literally are semantically closer to constituent words of the idiom.,5 Conclusion,[0],[0]
Experimental results on two different corpora suggest that our models are competitive against supervised methods and prior unsupervised methods.,5 Conclusion,[0],[0]
"Depending on the surrounding context, an idiomatic expression may be interpreted figuratively or literally.",abstractText,[0],[0]
This paper proposes an unsupervised learning method for recognizing the intended usages of idioms.,abstractText,[0],[0]
We treat the possible usages as a latent variable in probabilistic models and train them in a linguistically motivated feature space.,abstractText,[0],[0]
"Crucially, we show that distributional semantics serves as a helpful heuristic for formulating a literal usage metric to estimate the likelihood that the idiom is intended literally.",abstractText,[0],[0]
This information can then guide the unsupervised training process for the probabilistic models.,abstractText,[0],[0]
Experiments show that our overall model performs competitively against supervised methods.,abstractText,[0],[0]
Heuristically Informed Unsupervised Idiom Usage Recognition,title,[0],[0]
"Hierarchical clustering (HC) is a widely used data analysis tool, ubiquitous in information retrieval, data mining, and machine learning (see a survey by Berkhin [2006]).",1 Introduction,[0],[0]
This clustering technique represents a given dataset as a binary tree; each leaf represents an individual data point and each internal node represents a cluster on the leaves of its descendants.,1 Introduction,[0],[0]
"HC has become the most popular method for gene expression data analysis Eisen et al. [1998], and also has been used in the analysis of social networks Leskovec et al. [2014], Mann et al. [2008], bioinformatics Diez et al. [2015], image and text classification Steinbach et al. [2000], and even in analysis of financial markets Tumminello et al. [2010].",1 Introduction,[0],[0]
"It is attractive because it provides richer information at all levels of granularity simultaneously, compared to more traditional flat clustering approaches like k-means or k-median.
",1 Introduction,[0],[0]
"Recently, Dasgupta [2016] formulated HC as a combinatorial optimization problem, giving a principled way to compare the performance of different HC algorithms.",1 Introduction,[0],[0]
"This optimization viewpoint has since received a lot of attention Roy and Pokutta [2016], Charikar and Chatziafratis [2017], Cohen-Addad et al. [2017], Moseley and Wang [2017], Cohen-Addad et al. [2018] that has led not only to the development of new algorithms but also to theoretical justifications for the observed success of popular HC algorithms (e.g. average-linkage).
",1 Introduction,[0],[0]
"However, in real applications of clustering, the user often has background knowledge about the data that may not be captured by the input to the clustering algorithm.",1 Introduction,[0],[0]
"There is a rich body of work on constrained (flat) clustering formulations that take into account such user input in the form of “cannot link” and “must link” constraints Wagstaff and Cardie [2000], Wagstaff et al. [2001],
ar X
iv :1
80 5.
09 47
6v 2
[ cs
.D S]
1 4
Ju",1 Introduction,[0],[0]
"l 2
Bilenko et al. [2004], Rangapuram and Hein [2012].",1 Introduction,[0],[0]
"Very recently, “semi-supervised” versions of HC that incorporate additional constraints have been studied Vikram and Dasgupta [2016], where the natural form of such constraints is triplet (or “must link before”) constraints ab|c1: these require that valid solutions contain a sub-cluster with a, b together and c previously separated from them.2 Such triplet constraints, as we formally show later, can encode more general structural constraints in the form of rooted subtrees.",1 Introduction,[0],[0]
"Surprisingly, such simple triplet constraints already pose significant challenges for bottom-up linkage methods.",1 Introduction,[0],[0]
"(Figure 1).
",1 Introduction,[0],[0]
Our work is motivated by applying the optimization lens to study the interaction of hierarchical clustering algorithms with structural constraints.,1 Introduction,[0],[0]
Constraints can be fairly naturally incorporated into top-down (i.e. divisive) algorithms for hierarchical clustering; but can we establish guarantees on the quality of the solution they produce?,1 Introduction,[0],[0]
Another issue is that incorporating constraints from multiple experts may lead to a conflicting set of constraints; can the optimization viewpoint of hierarchical clustering still help us obtain good solutions even in the presence of infeasible constraints?,1 Introduction,[0],[0]
"Finally, different objective functions for HC have been studied in the literature; do algorithms designed for these objectives behave similarly in the presence of constraints?",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to propose a unified approach for constrained HC through the lens of optimization and to give provable approximation guarantees for a collection of fast and simple top-down algorithms that have been used for unconstrained HC in practice (e.g. community detection in social networks Mann et al. [2008]).
",1 Introduction,[0],[0]
Background on Optimization View of HC.,1 Introduction,[0],[0]
Dasgupta [2016] introduced a natural optimization framework for HC.,1 Introduction,[0],[0]
"Given a weighted graph G(V,E,w) and pairwise similarities wij ≥ 0 between the n data points i, j ∈ V , the goal is to find a hierarchical tree T ∗ such that
T ∗ = arg min all trees T ∑ (i,j)∈E wij · |Tij",1 Introduction,[0],[0]
"| (1)
where Tij is the subtree rooted at the lowest common ancestor of i, j in T and |Tij",1 Introduction,[0],[0]
"| is the number 1Hierarchies on data imply that all datapoints are linked at the highest level and all are separated at the lowest level, hence “cannot link” and “must link” constraints are not directly meaningful.",1 Introduction,[0],[0]
2For,1 Introduction,[0],[0]
"a concrete example from taxonomy of species, a triplet constraint may look like (Tuna,Salmon|Lion).
of leaves it contains.3",1 Introduction,[0],[0]
We denote (1) as similarity-HC.,1 Introduction,[0],[0]
"For applications where the geometry of the data is given by dissimilarities, again denoted by {wij}(i,j)∈E , Cohen-Addad et al. [2018] proposed an analogous approach, where the goal is to find a hierarchical tree T ∗ such that
T ∗ = arg max all trees T ∑ (i,j)∈E wij · |Tij",1 Introduction,[0],[0]
"| (2)
We denote (2) as dissimilarity-HC.",1 Introduction,[0],[0]
"A comprehensive list of desirable properties of the aformentioned objectives can be found in Dasgupta [2016], Cohen-Addad et al. [2018].",1 Introduction,[0],[0]
"In particular, if there is an underlying ground-truth hierarchical structure in the data, then T ∗ can recover the ground-truth.",1 Introduction,[0],[0]
"Also, both objectives are NP-hard to optimize, so the focus is on approximation algorithms.
",1 Introduction,[0],[0]
Our Results.,1 Introduction,[0],[0]
i),1 Introduction,[0],[0]
"We design algorithms that take into account both the geometry of the data, in the form of similarities, and the structural constraints imposed by the users.",1 Introduction,[0],[0]
Our algorithms emerge as the natural extensions of Dasgupta’s original recursive sparsest cut algorithm and the recursive balanced cut suggested in Charikar and Chatziafratis [2017].,1 Introduction,[0],[0]
"We generalize previous analyses to handle constraints and we prove an O(kαn)-approximation guarantee4, thus surprisingly matching the best approximation guarantee of the unconstrained HC problem for constantly many constraints.
ii)",1 Introduction,[0],[0]
"In the case of infeasible constraints, we extend the similarity-HC optimization framework, and we measure the quality of a possible tree T by a constraint-based regularized objective.",1 Introduction,[0],[0]
The regularization naturally favors solutions with as few constraint violations as possible and as far down the tree as possible (similar to the motivation behind similarity-HC objective).,1 Introduction,[0],[0]
"For this problem, we provide a top-down O(kαn)-approximation algorithm by drawing an interesting connection to an instance of the hypergraph sparsest cut problem.
",1 Introduction,[0],[0]
iii) We then change gears and study the dissimilarity-HC objective.,1 Introduction,[0],[0]
"Surprisingly, we show that known top-down techniques do not cope well with constraints, drawing a contrast with the situation for similarity-HC.",1 Introduction,[0],[0]
"Specifically, the (locally) densest cut heuristic performs poorly even if there is only one triplet constraint, blowing up its approximation factor to O(n).",1 Introduction,[0],[0]
"Moreover, we improve upon the state-of-the-art in Cohen-Addad et al. [2018], by showing a simple randomized partitioning is a 23 -approximation algorithm.",1 Introduction,[0],[0]
We also give a deterministic local-search algorithm with the same worst-case guarantee.,1 Introduction,[0],[0]
"Furthermore, we show that our randomized algorithm is robust under constraints, mainly because of its “exploration” behavior.",1 Introduction,[0],[0]
"In fact, besides the number of constraints, we propose an inherent notion of dependency measure among constraints to capture this behavior quantitatively.",1 Introduction,[0],[0]
"This helps us not only to explain why “non-exploring” algorithms may perform poorly, but also gives tight guarantees for our randomized algorithm.
",1 Introduction,[0],[0]
Experimental Results.,1 Introduction,[0],[0]
"We run experiments on the Zoo dataset [Lichman, 2013] to demonstrate our approach and the performance of our algorithms for a taxonomy application.",1 Introduction,[0],[0]
We consider a setup where there is a ground-truth tree and extra information regarding this tree is provided for the algorithm in the form of triplet constraints.,1 Introduction,[0],[0]
"The upshot is we believe specific variations of our algorithms can exploit this information; In this practical application, our algorithms have around %9 imrpvements in the objective compared to the naive recursive sparsest cut proposed in Dasgupta [2016] that does not use this information.",1 Introduction,[0],[0]
"See Appendix B for more details on the setup and precise conclusions of our experiments.
",1 Introduction,[0],[0]
"3Observe that in HC, all edges get cut eventually.",1 Introduction,[0],[0]
"Therefore it is better to postpone cutting “heavy” edges to when the clusters become small, i.e .as far down the tree as possible.
",1 Introduction,[0],[0]
"4For n data points, αn =",1 Introduction,[0],[0]
"O( √ logn) is the best approximation factor for the sparsest cut and k is the number of
constraints.
",1 Introduction,[0],[0]
Constrained HC work-flow in Practice.,1 Introduction,[0],[0]
"Throughout this paper, we develop different tools to handle user-defined structural constraints for hierarchical clustering.",1 Introduction,[0],[0]
"Here we describe a recipe on how to use our framework in practice.
",1 Introduction,[0],[0]
(1) Preprocessing constraints to form triplets.,1 Introduction,[0],[0]
User-defined structural constraints as rooted binary subtrees are convenient for the user and hence for the usability of our algorithm.,1 Introduction,[0],[0]
"The following proposition (whose proof is in the supplement) allows us to focus on studying HC with just triplet constraints.
",1 Introduction,[0],[0]
Proposition 1.,1 Introduction,[0],[0]
"Given constraints as a rooted binary subtree T on k data points (k ≥ 3), there is linear time algorithm that returns an equivalent set of at most k triplet constraints.
",1 Introduction,[0],[0]
(2) Detecting feasibility.,1 Introduction,[0],[0]
"The next step is to see if the set of triplet constraints is consistent, i.e. whether there exists a HC satisfying all the constraints.",1 Introduction,[0],[0]
"For this, we use a simple linear time algorithm called BUILD Aho et al. [1981].
(3) Hard constraints vs. regularization.",1 Introduction,[0],[0]
"BUILD can create a hierarchical decomposition that satisfies triplet constraints, but ignores the geometry of the data, whereas our goal here is to consider both simultaneously.",1 Introduction,[0],[0]
"Moreover, in the case that the constraints are infeasible, we aim to output a clustering that minimizes the cost of violating constraints combined with the cost of the clustering itself.",1 Introduction,[0],[0]
"• Feasible instance: to output a feasible HC, we propose using Constrained Recursive Sparsest Cut (CRSC) or Constrained Recursive Balanced Cut (CRBC): two simple top-down algorithms which are natural adaptations of recursive sparsest cut [Mann et al., 2008, Dasgupta, 2016] or recursive balanced cut Charikar and Chatziafratis [2017] to respect constraints (Section 2).",1 Introduction,[0],[0]
"• Infeasible instance: in this case, we turn our attention to a regularized version of HC, where the cost of violating constraints is added to the tree cost.",1 Introduction,[0],[0]
"We then propose an adaptation of CRSC, namely Hypergraph Recursive Sparsest Cut (HRSC) for the regularized problem (Section 3).
",1 Introduction,[0],[0]
Real-world application example.,1 Introduction,[0],[0]
"In phylogenetics, which is the study of the evolutionary history and relationships among species, an end-user usually has access to whole genomes data of a group of organisms.",1 Introduction,[0],[0]
"There are established methods in phylogeny to infer similarity scores between pairs of datapoints, which give the user the similarity weights wij .",1 Introduction,[0],[0]
"Often the user also has access to rare structural footprints of a common ancestry tree (e.g. through gene rearrangement data, gene inversions/transpositions etc., see Patané et al. [2018]).",1 Introduction,[0],[0]
"These rare, yet informative, footprints play the role of the structural constraints.",1 Introduction,[0],[0]
"The user can follow our pre-processing step to get triplet constraints from the given rare footprints, and then use Aho’s BUILD algorithm to choose between regularized or hard version of the HC problem.",1 Introduction,[0],[0]
"The above illustrates how to use our workflow and why using our algorithms facilitates HC when expert domain knowledge is available.
",1 Introduction,[0],[0]
Further related work.,1 Introduction,[0],[0]
"Similar to Vikram and Dasgupta [2016], constraints in the form of triplet queries have been used in an (adaptive) active learning framework by Tamuz et al. [2011], Emamjomeh-Zadeh and Kempe [2018], showing that approximately O(n log n) triplet queries are enough to learn an underlying HC.",1 Introduction,[0],[0]
"Other forms of user interaction in order to improve the quality of the produced clusterings have been used in Balcan and Blum [2008], Awasthi et al. [2014] where they prove that interactive feedback in the form of cluster split/merge requests can lead to significant improvements.",1 Introduction,[0],[0]
Robust algorithms for HC in the presence of noise were studied in Balcan et al. [2014] and a variety of sufficient conditions on the similarity function that would allow linkage-style methods to produce good clusters was explored in Balcan et al. [2008].,1 Introduction,[0],[0]
"On a different setting, the notion of triplets has been used as a measure of distance between hierarchical decomposition trees
on the same data points Brodal et al. [2013].",1 Introduction,[0],[0]
More technically distant analogs of how to use relations among triplets points have recently been proposed in Kleindessner and von Luxburg,1 Introduction,[0],[0]
[2017] for defining kernel functions corresponding to high-dimensional embeddings.,1 Introduction,[0],[0]
"Given an instance of the constrained hierarchical clustering, our proposed CRSC algorithm uses a blackbox αn-approximation algorithm for the sparsest cut problem (the best-known approximation factor for this problem is O( √ log n) due to Arora et al. [2009]).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Moreover, it also maintains the feasibility of the solution in a top-down approach by recursive partitioning of what we call the supergraph G′. Informally speaking, the supergraph is a simple data structure to track the progress of the algorithm and the resolved constraints.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"More formally, for every constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"we merge the nodes a and b into a supernode {a, b} while maintaining the edges in G (now connecting to their corresponding supernodes).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Note that G′ may have parallel edges, but this can easily be handled by grouping edges together and replacing them with the sum of their weights.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We repeatedly continue this merging procedure until there are no more constraints.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Observe that any feasible solution needs to start splitting the original graph G by using a cut that is also present in G′. When cutting the graph G′ = (G1, G2), if a constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"is resolved,5 then we can safely unpack the supernode {a, b} into two nodes again (unless there is another constraint ab|c′ in which case we should keep the supernode).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"By continuing and recursively finding approximate sparsest cuts on the supergraph G1 and G2, we can find a feasible hierarchical decomposition of G respecting all triplet constraints.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Next, we show the approximation guarantees for our algorithm.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Algorithm 1 CRSC 1:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given G and the triplet constraints ab|c, run BUILD to create the supergraph G′. 2: Use a blackbox access to an αn-approximation oracle for the sparsest cut problem, i.e.
arg minS⊆V wG′ (S,S̄) |S|·|S̄| .
3:",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given the output cut (S, S̄), separate the graph G′ into two pieces G1(S,E1) and G2(V \S,E2).
4: Recursively compute a HC T1 for G1 using only G1’s active constraints.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Similarly compute T2 for G2. 5: Output T = (T1, T2).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Analysis of CRSC Algorithm.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The main result of this section is the following theorem:
Theorem 1.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"for a, b, c ∈ V , the CRSC algorithm outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for the HC-similarity objective as in (1).
Notations and Definitions.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We slightly abuse notation by having OPT denote the optimum hierarchical decomposition or its optimum value as measured by (1).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Similarly for CRSC.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
For t ∈,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[n], OPT(t) denotes the maximal clusters in OPT of size at most t. Note that OPT(t) induces a partitioning of V .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
We use OPT(t) to denote edges cut by OPT(t) (i.e. edges with endpoints in different clusters in OPT(t)) or their total weight; the meaning will be clear from context.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For convenience, we define
5A constraint ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"is resolved, if c gets separated from a, b.
OPT(0) =",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"∑
(i,j)∈E wij .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For a cluster A created by CRSC, a constraint ab|c is active if a, b, c ∈",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"A, otherwise ab|c is resolved and can be discarded.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Overview of the Analysis.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
There are three main ingredients:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The first is to view a HC of n datapoints as a collection of partitions, one for each level t = n",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"− 1, . . .",2 Constrained Sparsest (Balanced) Cut,[0],[0]
", 1, as in [Charikar and Chatziafratis, 2017].",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"For a level t, the partition consists of maximal clusters of size at most t. The total cost incurred by OPT is then a combination of costs incurred at each level of this partition.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
This is useful for comparing our CRSC cost with OPT.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
The second idea is in handling constraints and it is the main obstacle where previous analyses Charikar and Chatziafratis,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[2017], Cohen-Addad et al. [2018] break down: constraints inevitably limit the possible cuts that are feasible at any level, and since the set of active constraints6 differ for CRSC and OPT, a direct comparison between them is impossible.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"If we have no constraints, we can charge the cost of partitioning a cluster A to lower levels of the OPT decomposition.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"However, when we have triplet constraints, the partition induced by the lower levels of OPT in a cluster A will not be feasible in general (Figure 2).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The natural way to overcome this obstacle is merging pieces of this partition so as to respect constraints and using higher levels of OPT, but it still may be impossible to compare CRSC with OPT if all pieces are merged.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We overcome this difficulty by an indirect comparison between the CRSC cost and lower levels r6kA of OPT, where kA is the number of active constraints in A. Finally, after a cluster-by-cluster analysis bounding the CRSC cost for each cluster, we exploit disjointness of clusters of the same level in the CRSC partition allowing us to combine their costs.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Proof of Theorem 1.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We start by borrowing the following facts from [Charikar and Chatziafratis, 2017], modified slightly for the purpose of our analysis (proofs are provided in the supplementary materials).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Fact 1 (Decomposition of OPT).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The total cost paid by OPT can be decomposed into costs of the different levels in the OPT partition, i.e. OPT = ∑n t=0w(OPT(t)).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Fact 2 (OPT at scaled levels).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let k ≤,2 Constrained Sparsest (Balanced) Cut,[0],[0]
n6 be the number of constraints.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Then, OPT ≥ 1 6k ·∑n
t=0w(OPT(b t 6kc)).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"6All constraints are active in the beginning of CRSC.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given the above facts, we look at any cluster A of size r produced by the algorithm.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Here is the main technical lemma that allows us to bound the cost of CRSC for partitioning A.
Lemma 1.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Suppose CRSC partitions a cluster A (|A| = r) in two clusters (B1, B2) (w.l.o.g. |B1| = s, |B2| = r− s, s ≤ b r2c ≤ r− s).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let the size r ≥ 6k,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"and let l = 6kA, where kA denotes the number of active constraints for A. Then: r · w(B1, B2) ≤ 4αn · s · w(OPT(b rl c) ∩A).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Proof.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The cost incurred by CRSC for partitioning A is r · w(B1, B2).",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Now consider OPT(b rl c).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"This induces a partitioning of A into pieces {Ai}i∈[m], where by design |Ai| = γi|A|, γi ≤ 1l , ∀i ∈",2 Constrained Sparsest (Balanced) Cut,[0],[0]
[m].,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Now, consider the cuts {(Ai, A \ Ai)}.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Even though all m cuts are allowed for OPT, for CRSC some of them are forbidden: for example, in Figure 2, the constraints ab|c, de|f render 4 out of the 6 cuts infeasible.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
But how many of them can become infeasible with kA active constraints?,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Since every constraint is involved in at most 2 cuts, we may have at most 2kA infeasible cuts.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Let F ⊆,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"[m] denote the index set of feasible cuts, i.e. if i ∈ F , the cut (Ai, A \Ai) is feasible for CRSC.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"To cut A, we use an αn-approximation of sparsest cut, whose sparsity is upper bounded by any feasible cut:
w(B1, B2)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ≤ αn · SP.CUT(A),2 Constrained Sparsest (Balanced) Cut,[0],[0]
≤ αn,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"min i∈F w(Ai, A \Ai) |Ai||A",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"\Ai|
≤ αn ∑
i∈F w(Ai, A \Ai)∑ i∈F |Ai||A \Ai|
where for the last inequality we used the standard fact that mini µiνi ≤ ∑ i µi∑",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"i νi
for µi ≥ 0",2 Constrained Sparsest (Balanced) Cut,[0],[0]
and νi > 0.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"We also have the following series of inequalities:
αn ∑ i∈F w(Ai, A \Ai)∑ i∈F |Ai||A \Ai| ≤ αn 2w(OPT(b rl c) ∩A) r2 ∑ i∈F γi(1− γi) ≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
4αn w(OPT(b rl c) ∩A),2 Constrained Sparsest (Balanced) Cut,[0],[0]
"r2
where the first inequality holds because we double count some (potentially all) edges of OPT(b rl c)∩A (these are the edges cut by OPT(b rl c) that are also present in cluster A, i.e. they have both endpoints in A) and the second inequality holds because γi ≤ 16k",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"=⇒ 1− γi ≥
6k−1 6k and∑",2 Constrained Sparsest (Balanced) Cut,[0],[0]
i∈F γi(1− γi) ≥ m∑ i=1,2 Constrained Sparsest (Balanced) Cut,[0],[0]
γi(1−,2 Constrained Sparsest (Balanced) Cut,[0],[0]
γi)− 2 ∑ i∈[m]\F 1 6k ≥ 6k,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"− 1 6k m∑ i=1 γi − 2k 6k = 4k − 1 6k ≥ 1/2
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Finally, we are ready to prove the lemma by combining the above inequalities ( r−sr ≤ 1):
r · w(B1, B2) = r ·",2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"w(B1, B2)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"s(r − s)
≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
r ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
s(r − s) ·,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"4αn w(OPT(b rl c) ∩A)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"r2 ≤ 4αn · s · w(OPT(b rl c) ∩A).
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"It is clear that we exploited the charging to lower levels of OPT, since otherwise if all pieces in A were merged, the denominator with the |Ai|’s would become 0.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The next lemma lets us combine the costs incurred by CRSC for different clusters A (proof is in the supplementary materials)
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Lemma 2 (Combining the costs of clusters in CRSC).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"The total CRSC cost for partitioning all clusters A into (B1, B2) (with |A| = rA, |B1| = sA) is bounded by:
(1) ∑
A:|A|≥6k rA · w(B1, B2) ≤ O(αn) ·",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"n∑ t=0 w(OPT(b t6kc))
(2) ∑
A:|A|<6k
rAw(B1, B2) ≤",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"6k · OPT
Combining Fact 2 and Lemma 2 finishes the proof.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Remark 1.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"In the supplementary material, we prove how one can use balanced cut, i.e. finding a cut S such that
arg min S⊆V :|S|≥n/3,|S̄|≥n/3 wG′(S, S̄) (3)
instead of sparsest cut, and using approximation algorithms for this problem achieves the same approximation factor as in Theorem 1, but with better running time.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Remark 2.,2 Constrained Sparsest (Balanced) Cut,[0],[0]
Optimality of the CRSC algorithm:,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Note that complexity theoretic lower-bounds for the unconstrained version of HC from Charikar and Chatziafratis [2017] also apply to our setting; more specifically, they show that no constant factor approximation exists for HC assuming the Small-Set Expansion Hypothesis.
",2 Constrained Sparsest (Balanced) Cut,[0],[0]
Theorem 2 (The divisive algorithm using balanced cut).,2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"for a, b, c ∈ V , the constrained recursive balanced cut algorithm CRBC (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for Dasgupta’s HC objective.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Moreover, the running time is almost linear time.",2 Constrained Sparsest (Balanced) Cut,[0],[0]
"Previously, we assumed that constraints were feasible.",3 Constraints and Regularization,[0],[0]
"However, in many practical applications, users/experts may disagree, hence our algorithm may receive conflicting constraints as input.",3 Constraints and Regularization,[0],[0]
Here we want to explore how to still output a satisfying HC that is a good in terms of objective (1) (similarity-HC) and also respects the constraints as much as possible.,3 Constraints and Regularization,[0],[0]
"To this end, we propose a regularized version of Dasgupta’s objective, where the regularizer measures quantitatively the degree by which constraints get violated.
",3 Constraints and Regularization,[0],[0]
"Informally, the idea is to penalize a constraint more if it is violated at top levels of the decomposition compared to lower levels.",3 Constraints and Regularization,[0],[0]
We also allow having different violation weights for different constraints (potentially depending on the expertise of the users providing the constraints).,3 Constraints and Regularization,[0],[0]
"More concretely, inspired by the Dasgupta’s original objective function, we consider the following optimization problem:
min T∈T ( ∑ (i,j)∈E wij |Tij",3 Constraints and Regularization,[0],[0]
|+,3 Constraints and Regularization,[0],[0]
"λ · ∑ ab|c∈K cab|c|Tab| · 1{ab|c is violated} ) , (4)
where T is the set of all possible binary HC trees for the given data points, K is the set of the k triplet constraints, Tab is the size of the subtree rooted at the least common ancestor of a, b, and cab|c is defined as the base cost of violating triplet constraint ab|c.",3 Constraints and Regularization,[0],[0]
"Note that the regularization parameter λ ≥ 0 allows us to interpolate between satisfying the constraints or respecting the geometry of the data.
",3 Constraints and Regularization,[0],[0]
"Hypergraph Recursive Sparsest Cut In order to design approximation algorithms for the regularized objective, we draw an interesting connection to a different problem, which we call 3- Hypergraph Hierarchical Clustering (3HHC).",3 Constraints and Regularization,[0],[0]
"An instance of this problem consists of a hypergraph GH = (V,E,EH) with edges E, and hyperedges of size 3, EH, together with similarity weights for
edges, {wij}(i,j)∈E , and similarity weights for 3-hyperedges,7 {wij|k}(i,j,k)∈EH .",3 Constraints and Regularization,[0],[0]
"We now think of HC on the hypergraph GH, where for every binary tree T we define the cost to be the natural extension of Dasgupta’s objective: ∑
(i,j)∈E
wij |Tij",3 Constraints and Regularization,[0],[0]
|+,3 Constraints and Regularization,[0],[0]
"∑
(i,j,k)∈EH wTijk|Tijk| (5)
where wTijk is either equal to wij|k, wjk|i or wki|j , and Tijk is either the subtree rooted at LCA(i, j), 8 LCA(i, k) or LCA(k, j), all depending on how T cuts the 3-hyperedge {i, j, k}.",3 Constraints and Regularization,[0],[0]
"The goal is to find a hierarchical clustering of this hypergraph, so as to minimize the cost (5) of the tree.
",3 Constraints and Regularization,[0],[0]
Reduction from Regularization to 3HHC.,3 Constraints and Regularization,[0],[0]
"Given an instance of HC with constraints (with their costs of violations) and a parameter λ, we create a hypergraph GH so that the total cost of any binary clustering tree in the 3HHC problem (5) corresponds to the regularized objective of the same tree as in (4).",3 Constraints and Regularization,[0],[0]
"GH has exactly the same set of vertices, (normal) edges and (normal) edge weights as in the original instance of the HC problem.",3 Constraints and Regularization,[0],[0]
"Moreover, for every constraint ab|c",3 Constraints and Regularization,[0],[0]
"(with cost cab|c) it has a hyperedge {a, b, c}, to which we assign three weights wab|c = 0, wac|b = wbc|a = λ ·cab|c.",3 Constraints and Regularization,[0],[0]
"Therefore, we ensure that any divisive algorithm for the 3HHC problem avoids the cost |Tabc|·λ·cab|c only if it chops {a, b, c} into {a, b} and {c} at some level, which matches the regularized objective.
",3 Constraints and Regularization,[0],[0]
Reduction from 3HHC to Hypergraph Sparsest Cut.,3 Constraints and Regularization,[0],[0]
"A natural generalization of the sparsest cut problem for our hypergraphs, which we call Hyper Sparsest Cut (HSC), is the following problem:
arg min S⊆V
( w(S, S̄)",3 Constraints and Regularization,[0],[0]
"+ ∑ (i,j,k)∈EH w S ijk
|S||S̄|
) ,
where w(S, S̄) is the weight of the cut (S, S̄) and wSijk is either equal to wij|k, wjk|i or wki|j , depending on how (S, S̄) chops the hyperedge {i, j, k}.",3 Constraints and Regularization,[0],[0]
"Now, similar to Charikar and Chatziafratis [2017], Dasgupta [2016], we can recursively run a blackbox approximation algorithm for HSC to solve 3HHC.",3 Constraints and Regularization,[0],[0]
"The main result of this section is the following technical proposition, whose proof is analogous to that of Theorem 1 (provided in the supplementary materials).
",3 Constraints and Regularization,[0],[0]
Proposition 2.,3 Constraints and Regularization,[0],[0]
"Given the hypergraph GH with k hyperedges, and given access to an algorithm which is αn-approximation for HSC, the Recursive Hypergraph Sparsest Cut (R-HSC) algorithm achieves an O(kαn)-approximation.
Reduction from HSC back to Sparsest Cut.",3 Constraints and Regularization,[0],[0]
We now show how to get an αn-approximation oracle for our instance of the HSC problem by a general reduction to sparsest cut.,3 Constraints and Regularization,[0],[0]
"Our reduction is simple: given a hypergraph GH and all the weights, create an instance of sparsest cut with the same vertices, (normal) edges and (normal) edge weights.",3 Constraints and Regularization,[0],[0]
"Moreover, for every 3-hyperedge {a, b, c}
7We have 3 different weights corresponding to the 3 possible ways of partitioning {i, j, k} in two parts: wij|k, wjk|i and wki|j .
",3 Constraints and Regularization,[0],[0]
"8LCA(i, j) denotes the lowest common ancestor of i, j ∈ T .
consider adding a triangle to the graph, i.e. three weighted edges connecting {a, b, c}, where:
w′ab = wbc|a + wac|b",3 Constraints and Regularization,[0],[0]
"− wab|c
2 = λ · cab|c,
w′ac = wbc|a + wab|c",3 Constraints and Regularization,[0],[0]
"− wac|b
2 = 0,
w′bc = wac|b +",3 Constraints and Regularization,[0],[0]
wab|c,3 Constraints and Regularization,[0],[0]
"− wbc|a
2 = 0.
",3 Constraints and Regularization,[0],[0]
This construction can be seen in Figure 3.,3 Constraints and Regularization,[0],[0]
The important observation is that w′ab+w ′,3 Constraints and Regularization,[0],[0]
"ac = wbc|a, w′ab+ w′bc = wac|b and w ′",3 Constraints and Regularization,[0],[0]
bc +w ′,3 Constraints and Regularization,[0],[0]
"ac = wab|c, which are exactly the weights associated with the corresponding splits of the 3-hyperedge {a, b, c}.",3 Constraints and Regularization,[0],[0]
"So, correctness of the reduction9 follows as the weight of each cut is preserved between the hypergraph and the graph after adding the triangles.",3 Constraints and Regularization,[0],[0]
"For a discussion on extending this gadget more generally, see the supplement.
",3 Constraints and Regularization,[0],[0]
Remark 3.,3 Constraints and Regularization,[0],[0]
Reduction to hypergraphs: we would like to emphasize the necessity of the hypergraph version in order for the reduction to work.,3 Constraints and Regularization,[0],[0]
"One might think that just adding extra heavy edges would be sufficient, but there is a technical difficulty with this approach.",3 Constraints and Regularization,[0],[0]
"Consider a triplet constraint ab|c; once c is separated from a and b at some level, there is no extra tendency anymore to keep a and b together (i.e. only the similarity weight should play role after this point).",3 Constraints and Regularization,[0],[0]
This behavior cannot be captured by only adding heavy-weight edges.,3 Constraints and Regularization,[0],[0]
"Instead, one needs to add a heavy edge between a and b that disappears once c is separated, and this is exactly why we need the hyperedge gadget.",3 Constraints and Regularization,[0],[0]
"One can replace the reduction for a one-shot proof, but we believe it will be less modular and less transparent.",3 Constraints and Regularization,[0],[0]
"In this section we study dissimilarity-HC, and we look into the problem of designing approximation algorithms for both unconstrained and constrained hierarchical clustering.",4 Variations on a Theme,[0],[0]
"In Cohen-Addad et al. [2017], they show that average linkage is a 12 -approximation for this problem and they propose a top-
down approach based on locally densest cut achieving a (23− )-approximation",4 Variations on a Theme,[0],[0]
in time Õ ( n2(n+m) ),4 Variations on a Theme,[0],[0]
.,4 Variations on a Theme,[0],[0]
"Notably, when gets small the running time blows up.",4 Variations on a Theme,[0],[0]
"Here, we prove that the most natural randomized algorithm for this problem, i.e. recursive random cutting, is a 23 -approximation with expected running time O(n log n).",4 Variations on a Theme,[0],[0]
"We further derandomize this algorithm to get a simple deterministic local-search style 23 -approximation algorithm.
",4 Variations on a Theme,[0],[0]
"If we also have structural constraints for the dissimilarity-HC, we show that the existing approaches fail.",4 Variations on a Theme,[0],[0]
In fact we show that they lead to an Ω(n)-approximation factor due to the lack of “exploration” (e.g. recursive densest cut).,4 Variations on a Theme,[0],[0]
"We then show that recursive random cutting is robust to adding user constraints, and indeed it preserves a constant approximation factor when there are, roughly speaking, constantly many user constraints.
",4 Variations on a Theme,[0],[0]
"9Since all weights in the final graph are non-negative, standard techniques for Sparsest Cut can be used.
",4 Variations on a Theme,[0],[0]
Randomized 23-approximation.,4 Variations on a Theme,[0],[0]
"Consider the most natural randomized algorithm for hierarchical clustering, i.e. recursively partition each cluster into two, where each point in the current cluster independently flips an unbiased coin and based on the outcome, it is put in one of the two parts.
",4 Variations on a Theme,[0],[0]
Theorem 3.,4 Variations on a Theme,[0],[0]
"Recursive-Random-Cutting is a 23 -approximation for maximizing dissimilarity-HC objective.
",4 Variations on a Theme,[0],[0]
Proof sketch.,4 Variations on a Theme,[0],[0]
"An alternative view of Dasgupta’s objective is to divide the reward of the clustering tree between all possible triples {i, j, k}, where (i, j) ∈ E and k is another point (possibly equal to i or j).",4 Variations on a Theme,[0],[0]
"Now, in any hierarchical clustering tree, if at the moment right before i and j become separated the vertex k has still been in the same cluster as {i, j}, then this triple contributes wij to the objective function.",4 Variations on a Theme,[0],[0]
We claim this event happens with probability exactly 23 .,4 Variations on a Theme,[0],[0]
"To see this, consider an infinite independent sequence of coin flips for i, j, and k.",4 Variations on a Theme,[0],[0]
"Without loss of generality, condition on i’s sequence to be all heads.",4 Variations on a Theme,[0],[0]
The aforementioned event happens only if j’s first tales in its sequence happens no later than k’s first tales in its sequence.,4 Variations on a Theme,[0],[0]
"This happens with probability∑
i≥1 1 2( 1 4) i−1 = 23 .",4 Variations on a Theme,[0],[0]
"Therefore, the algorithm gets the total reward 2n 3 ∑ (i,j)∈E wij in expectation.
",4 Variations on a Theme,[0],[0]
"Moreover, the total reward of any hierarchical clustering is upper-bounded by n ∑
(i,j)∈E wij , which completes the proof of the 23 -approximation.
",4 Variations on a Theme,[0],[0]
Remark 4.,4 Variations on a Theme,[0],[0]
"This algorithm runs in time O(n log n) in expectation, due to the fact that the binary clustering tree has expected depth O(log n) (see for example Cormen et al. [2009]) and at each level we only perform n operations.
",4 Variations on a Theme,[0],[0]
We now derandomize the recursive random cutting algorithm using the method of conditional expectations.,4 Variations on a Theme,[0],[0]
"At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the “left” partition or “right” partition for the next recursion.",4 Variations on a Theme,[0],[0]
"Once we make a decision for a point, we fix that point and go to the next one.",4 Variations on a Theme,[0],[0]
"Roughly speaking, these local improvements can be done in polynomial time, which will result in a simple local-search style deterministic algorithm.
",4 Variations on a Theme,[0],[0]
Theorem 4.,4 Variations on a Theme,[0],[0]
"There is a deterministic local-search style 23 -approximation algorithm for maximizing dissimilarity-HC objective that runs in time O(n2(n+m)).
",4 Variations on a Theme,[0],[0]
"Maximizing the Objective with User Constraints From a practical point of view, one can think of many settings in which the output of the hierarchical clustering algorithm should satisfy user-defined hard constraints.",4 Variations on a Theme,[0],[0]
"Now, combining the new perspective of maximizing Dasgupta’s objective with this practical consideration raises a natural question: which algorithms are robust to adding user constraints, in the sense that a simple variation of these algorithms still achieve a decent approximation factor?
",4 Variations on a Theme,[0],[0]
• Failure of “Non-exploring” Approaches.,4 Variations on a Theme,[0],[0]
"Surprisingly enough, there are convincing reasons that adapting existing algorithms for maximizing Dasgupta’s objective (e.g. those proposed in Cohen-Addad et al. [2018]) to handle user constraints is either challenging or hopeless.",4 Variations on a Theme,[0],[0]
"First, bottom-up algorithms, e.g. average-linkage, fail to output a feasible outcome if they only consider each constraint separately and not all the constraints jointly (as we saw in Figure 1).",4 Variations on a Theme,[0],[0]
"Second, maybe more surprisingly, the natural extension of (locally) Recursive-Densest-Cut10 algorithm proposed in Cohen-Addad et al. [2018] to handle user constraints performs poorly in the worst-case, even
10While a locally densest cut can be found in poly-time, desnest cut is NP-hard, making our negative result stronger.
",4 Variations on a Theme,[0],[0]
when we have only one constraint.,4 Variations on a Theme,[0],[0]
"Recursive-Densest-Cut proceeds by repeatedly picking the cut that has maximum density, i.e. arg maxS⊆V w(S,S̄) |S|·|S̄| and making two clusters.",4 Variations on a Theme,[0],[0]
"To handle the user constraints, we run it recursively on the supergraph generated by the constraints, similar to the approach in Section 2.",4 Variations on a Theme,[0],[0]
"Note that once the algorithm resolves a triplet constraint, it also breaks its corresponding supernode.
",4 Variations on a Theme,[0],[0]
"Now consider the following example in Figure 4, in which there is just one triplet constraint ab|c.",4 Variations on a Theme,[0],[0]
The weight W should be thought of as large and as small.,4 Variations on a Theme,[0],[0]
"By choosing appropriate weights on the edges of the clique Kn, we can fool the algorithm into cutting the dense parts in the clique, without ever resolving the ab|c constraint until it is too late.",4 Variations on a Theme,[0],[0]
"The algorithm gets a gain of O(n3+W ) whereas OPT gets Ω(nW ) by starting with the removal of the edge (b, c) and then removing (a, b), thus enjoying a gain of ≈ nW .
",4 Variations on a Theme,[0],[0]
• Constrained Recursive Random Cutting.,4 Variations on a Theme,[0],[0]
"The example in Figure 4, although a bit pathological, suggests that a meaningful algorithm for this problem should explore cutting low-weight edges that might lead to resolving constraints, maybe randomly, with the hope of unlocking rewarding edges that were hidden before this exploration.
",4 Variations on a Theme,[0],[0]
"Formally, our approach is showing that the natural extension of recursive random cutting for the constrained problem, i.e. by running it on the supergraph generated by constraints and unpacking supernodes as we resolve the constraints (in a similar fashion to CSC), achieves a constant factor approximation when the constraints have bounded dependency.",4 Variations on a Theme,[0],[0]
"In the remaining of this section, we define an appropriate notion of dependency between the constraints, under the name of dependency measure and analyze the approximation factor of constrained recursive random cutting (Constrained-RRC ) based on this notion.
",4 Variations on a Theme,[0],[0]
"Suppose we are given an instance of hierarchical clustering with triplet constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}, where ci = xi|yizi,∀i ∈",4 Variations on a Theme,[0],[0]
[k].,4 Variations on a Theme,[0],[0]
"For any triplet constraint ci, lets call the pair {yi, zi} the base, and zi the key of the constraint.",4 Variations on a Theme,[0],[0]
"We first partition our constraints into equivalence classes C1, . . .",4 Variations on a Theme,[0],[0]
", CN , where Ci ⊆ {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}.",4 Variations on a Theme,[0],[0]
"For every i, j, the constraints ci and cj belong to the same class C if they share the same base (see Figure 5).
",4 Variations on a Theme,[0],[0]
Definition 1 (Dependency digraph).,4 Variations on a Theme,[0],[0]
"The Dependency digraph is a directed graph with vertex set {C1, . . .",4 Variations on a Theme,[0],[0]
", CL}.",4 Variations on a Theme,[0],[0]
"For every i, j, there is a directed edge Ci → Cj if ∃ c = x|yz, c′ = x′|y′z′, such that c ∈ Ci, c′ ∈ Cj , and either {x, z} = {y′, z′} or {x, y} = {y′, z′} (see Figure 6).
",4 Variations on a Theme,[0],[0]
The dependency digraph captures how groups of constraints impact each other.,4 Variations on a Theme,[0],[0]
"Formally, the existence of the edge Ci → Cj implies that all the constraints in Cj should be resolved before one can separate the two endpoints of the (common) base edge of the constraints in Ci.",4 Variations on a Theme,[0],[0]
Remark 5.,4 Variations on a Theme,[0],[0]
"If the constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck} are feasible, i.e. there exists a hierarchical clustering that can respect all the constraints, the dependency digraph is clearly acyclic.
",4 Variations on a Theme,[0],[0]
Definition 2 (Layered dependency subgraph).,4 Variations on a Theme,[0],[0]
"Given any class C, the layered dependency subgraph of C is the induced subgraph in the dependency digraph by all the classes that are reachable from C. Moreover, the vertex set of this subgraph can be partitioned into layers {I0, I1, . . .",4 Variations on a Theme,[0],[0]
", IL}, where L is the maximum length of any directed path leaving C and Il is a subset of classes where the length of the longest path from C to each of them is exactly equal to l (see Figure 7).
",4 Variations on a Theme,[0],[0]
We are now ready to define a crisp quantity for every dependency graph.,4 Variations on a Theme,[0],[0]
"This will later help us give a more meaningful and refined beyond-worst-case guarantee for the approximation factor of the Constrained-RRC algorithm.
",4 Variations on a Theme,[0],[0]
Definition 3 (Dependency measure).,4 Variations on a Theme,[0],[0]
"Given any class C, the dependency measure of C is defined as
DM(C) , L∏ l=0 (1 + ∑ C′∈Il |C′|),
where I0, . . .",4 Variations on a Theme,[0],[0]
", IL are the layers of the dependency subgraph of C, as in Definition 2.",4 Variations on a Theme,[0],[0]
"Moreover, the dependency measure of a set of constraints DMC({c1, . . .",4 Variations on a Theme,[0],[0]
", ck}) is defined as maxC DM(C), where the maximum is taken over all the classes generated by {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}.
Intuitively speaking, the notion of the dependency measure quantitatively expresses how “deeply” the base of a constraint is protected by the other constraints, i.e. how many constraints need to be resolved first before the base of a particular constraint is unpacked and the Constrained-RRC algorithm can enjoy its weight.",4 Variations on a Theme,[0],[0]
"This intuition is formalized through the following theorem, whose proof is deferred to the supplementary materials.
",4 Variations on a Theme,[0],[0]
Theorem 5.,4 Variations on a Theme,[0],[0]
"The constrained recursive random cutting (Constrained-RRC ) algorithm is an αapproximation algorithm for maximizing dissimilarity-HC objective objective given a set of feasible constraints {c1, . . .",4 Variations on a Theme,[0],[0]
", ck}, where
α = 2(1− k/n) 3 ·DMC({c1, . . .",4 Variations on a Theme,[0],[0]
", ck}) ≤ 2(1−",4 Variations on a Theme,[0],[0]
"k/n) 3 ·maxC DM(C)
Corollary 1.",4 Variations on a Theme,[0],[0]
"Constrained-RRC is an O(1)-approximation for maximizing dissimilarity-HC objective, given feasible constraints of constant dependency measure.",4 Variations on a Theme,[0],[0]
We studied the problem of hierarchical clustering when we have structural constraints on the feasible hierarchies.,5 Conclusion,[0],[0]
"We followed the optimization viewpoint that was recently developed in Dasgupta [2016], Cohen-Addad et al. [2018] and we analyzed two natural top-down algorithms giving provable approximation guarantees.",5 Conclusion,[0],[0]
"In the case where the constraints are infeasible, we proposed and analyzed a regularized version of the HC objective by using the hypergraph version of the sparsest cut problem.",5 Conclusion,[0],[0]
"Finally, we also explored a variation of Dasgupta’s objective and improved upon previous techniques, both in the unconstrained and in the constrained setting.",5 Conclusion,[0],[0]
Vaggos Chatziafratis was partially supported by ONR grant N00014-17-1-2562.,Acknowledgements,[0],[0]
Rad Niazadeh was supported by Stanford Motwani fellowship.,Acknowledgements,[0],[0]
Moses Charikar was supported by NSF grant CCF1617577 and a Simons Investigator Award.,Acknowledgements,[0],[0]
"We would also like to thank Leo Keselman, Aditi Raghunathan and Yang Yuan for providing comments on an earlier draft of the paper.",Acknowledgements,[0],[0]
We also thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgements,[0],[0]
"A.1 Missing proofs and discussion in Section 2
Proof of Proposition 1.",A Supplementary Materials,[0],[0]
"For nodes u, v ∈ T , let P (u) denote the parent of u in the tree and LCA(u, v) denote the lowest common ancestor of u, v.",A Supplementary Materials,[0],[0]
"For a leaf node li, i ∈",A Supplementary Materials,[0],[0]
"[k], we say that its label is li, whereas for an internal node of T , we say that its label is the label of any of its two children.",A Supplementary Materials,[0],[0]
"As long as there are any two nodes a, b that are siblings (i.e. P (a) ≡ P (b)), we create a constraint ab|c",A Supplementary Materials,[0],[0]
where c is the label of the second child of P (P (a)).,A Supplementary Materials,[0],[0]
"We delete leaves a, b from the tree and repeat until there are fewer than 3 leaves left.",A Supplementary Materials,[0],[0]
"To see why the above procedure will only create at most k constraints, notice that every time a new constraint is created, we delete two nodes of the given tree T .",A Supplementary Materials,[0],[0]
"Since T has k leaves and is binary, it can have at most 2k − 1 nodes in total.",A Supplementary Materials,[0],[0]
It follows that we create at most 2k−12 < k triplet constraints.,A Supplementary Materials,[0],[0]
"For the equivalence between the constraints imposed by T and the created triplet constraints, observe that all triplet constraints we create are explicitly imposed by the given tree (since we only create constraints for two leaves that are siblings) and that for any three datapoints a, b, c ∈ T with LCA(a, c)=LCA(b, c), our set of triplet constraints will indeed imply ab|c, because LCA(a, b) appears further down the tree than LCA(a, c) and hence a, b become siblings before a, c or b, c.
Proof of Fact 1 from Charikar and Chatziafratis [2017].",A Supplementary Materials,[0],[0]
"We will measure the contribution of an edge e = (u, v) ∈ E to the RHS and to the LHS.",A Supplementary Materials,[0],[0]
"Suppose that r denotes the size of the minimal cluster in OPT that contains both u and v. Then the contribution of the edge e = (u, v) to the LHS is by definition r · we.",A Supplementary Materials,[0],[0]
"On the other hand, (u, v) ∈",A Supplementary Materials,[0],[0]
"OPT(t),∀t ∈ {0, ..., r",A Supplementary Materials,[0],[0]
− 1}.,A Supplementary Materials,[0],[0]
"Hence the contribution to the RHS is also r · we.
",A Supplementary Materials,[0],[0]
Proof of Fact 2 from Charikar and Chatziafratis [2017].,A Supplementary Materials,[0],[0]
"We rewrite OPT using the fact that
w(OPT(t))",A Supplementary Materials,[0],[0]
"≥ 0
at every level t ∈",A Supplementary Materials,[0],[0]
"[n]:
6k · OPT = 6k n∑ t=0 w(OPT(t))
",A Supplementary Materials,[0],[0]
= 6k(w(OPT(0)),A Supplementary Materials,[0],[0]
+ · · ·+ w(OPT(n))),A Supplementary Materials,[0],[0]
≥ 6k(w(OPT(0)),A Supplementary Materials,[0],[0]
"+ · · ·+ w(OPT(b n6kc)))
",A Supplementary Materials,[0],[0]
"= n∑ t=0 w(OPT(b t6kc))
",A Supplementary Materials,[0],[0]
Proof of Lemma 2.,A Supplementary Materials,[0],[0]
"By using the previous lemma we have: CRSC = ∑ A rAw(B1, B2) ≤≤ O(αn) ∑ A sAw(OPT(b rA6kA c) ∩A)
",A Supplementary Materials,[0],[0]
"Observe that w(OPT(t)) is a decreasing function of t, since as t decreases, more and more edges are getting cut.",A Supplementary Materials,[0],[0]
"Hence we can write:
∑ A sA · w(OPT(b rA6k c) ∩A) ≤ ∑",A Supplementary Materials,[0],[0]
"A rA∑ t=rA−sA+1 w(OPT(b rA6kA c) ∩A)
",A Supplementary Materials,[0],[0]
"To conclude with the proof of the first part all that remains to be shown is that:
∑ A rA∑ t=rA−sA+1 w(OPT(b t6kA c) ∩A) ≤ n∑ t=0 w(OPT(b t6kc))
",A Supplementary Materials,[0],[0]
To see why this is true consider the clusters A with a contribution to the LHS.,A Supplementary Materials,[0],[0]
We have that rA−sA+1 ≤,A Supplementary Materials,[0],[0]
"t ≤ rA, hence |B2| < tmeaning thatA is aminimal cluster of size",A Supplementary Materials,[0],[0]
|A| ≥ t > |B2|,A Supplementary Materials,[0],[0]
"≥ |B1|, i.e. if both A’s children are of size less than t, then this cluster A contributes such a term.",A Supplementary Materials,[0],[0]
"The set of all such A form a disjoint partition of V because of the definition for minimality (in order for them to overlap in the hierarchical clustering, one of them needs to be ancestor of the other and this cannot happen because of minimality).",A Supplementary Materials,[0],[0]
"Since OPT(b t6kc)∩A for all such A forms a disjoint partition of OPT(b t6kc), the claim follows by summing up over all t.
Note that so far our analysis handles clusters A with size rA ≥ 6k.",A Supplementary Materials,[0],[0]
"However, for clusters with smaller size rA < 6k we can get away by using a crude bound for bounding the total cost and still not affecting the approximation guarantee that will be dominated by O(kαn):∑
|A|<6k rAw(B1, B2) < 6k · ∑ ij∈E wij = 6k · OPT(1) ≤ 6k · OPT
Theorem 6 (The divisive algorithm using balanced cut).",A Supplementary Materials,[0],[0]
"Given a weighted graph G(V,E,w) with k triplet constraints ab|c",A Supplementary Materials,[0],[0]
"for a, b, c ∈ V , the constrained recursive balanced cut algorithm (same as CRSC, but using balanced cut instead of sparsest cut) outputs a HC respecting all triplet constraints and achieves an O(kαn)-approximation for the HC objective (1).
",A Supplementary Materials,[0],[0]
Proof.,A Supplementary Materials,[0],[0]
"It is not hard to show that one can use access to balanced cut rather than sparsest cut and achieve the same approximation factor by the recursive balanced cut algorithm.
",A Supplementary Materials,[0],[0]
We will follow the same notation as in the sparsest cut analysis and we will use some of the facts and inequalities we previously proved about OPT(t).,A Supplementary Materials,[0],[0]
"Again, for a cluster A of size r, the important observation is that the partition A1, . . .",A Supplementary Materials,[0],[0]
", Al (at the end, we will again choose l = 6kA) induced inside the cluster A by OPT( rl ) can be separated into two groups, let’s say (C1, C2) such that r/3 ≤ |C1|, |C2| ≤ 2r/3.",A Supplementary Materials,[0],[0]
"In other words we can demonstrate a Balanced Cut with ratio 13 : 2 3 for the cluster A. Since we cut fewer edges when creating C1, C2 compared to the partitioning of OPT( rl ):
w(C1, C2) ≤ w(OPT(b rl c) ∩A)",A Supplementary Materials,[0],[0]
"By the fact we used an αn-approximation to balanced cut we can get the following inequality
(similarly to Lemma 1):
r · w(C1, C2) ≤ O(αn) · s · w(OPT(b rl c) ∩A)
",A Supplementary Materials,[0],[0]
"Finally, we have to sum up over all the clusters A (now in the summation we should write rA, sA instead of just r, s, since there is dependence in A) produced by the constrained recursive balanced cut algorithm for Hierarchical Clustering and we get that we can approximate the HC objective function up to O(kαn).
",A Supplementary Materials,[0],[0]
Remark 6.,A Supplementary Materials,[0],[0]
Using balanced-cut can be useful for two reasons.,A Supplementary Materials,[0],[0]
"First, the runtime of sparsest and balanced cut on a graph with n nodes and m edges are Õ(m+n1+ ).",A Supplementary Materials,[0],[0]
"When run recursively however as in our case, taking recursive sparsest cuts might be worse off by a factor of n (in case of unbalanced splits at every step) in the worst case.",A Supplementary Materials,[0],[0]
"However, recursive balanced cut is still Õ(m+n1+ ).",A Supplementary Materials,[0],[0]
"Second, it is known that an α-approximation for the sparsest cut yields an O(α)-approximation for balanced cut, but not the other way.",A Supplementary Materials,[0],[0]
"This gives more flexibility to the balanced cut algorithm, and there is a chance it can achieve a better approximation factor (although we don’t study it further in this paper).",A Supplementary Materials,[0],[0]
Proof sketch of Proposition 2.,A.2 Missing proofs in Section 3,[0],[0]
"Here the main obstacle is similar to the one we handled when proving Theorem (1): for a given cluster A created by the R-HSC algorithm, different constraints are, in general, active compared to the OPT decomposition for this cluster A.",A.2 Missing proofs in Section 3,[0],[0]
"Note of course, that OPT itself will not respect all constraints, but because we don’t know which constraints are active for OPT, we still need to use a charging argument to low levels of OPT.",A.2 Missing proofs in Section 3,[0],[0]
"Observe that here we are allowed to cut an edge ab even if we had the ab|c constraint (incurring the corresponding cost cab|c), however we cannot possibly hope to charge this to the OPT solution, as OPT, for all we know, may have respected this constraint.",A.2 Missing proofs in Section 3,[0],[0]
"In the analysis, we crucially use a merging procedure between sub-clusters of A having active constraints between them and this allows us to compare the cost of our R-HSC with the cost of OPT .
3-hyperedges to triangles for general weights .",A.2 Missing proofs in Section 3,[0],[0]
"Even though the general reduction presented in Section 3 (Figure 3) to transform a 3-hyperedge to a triangle is valid even for general instances of HSC with 3-hyperedges and arbitrary weights, the reduced sparsest cut problem may have negative weights, e.g. when wbc|a + wac|b < wab|c.",A.2 Missing proofs in Section 3,[0],[0]
"To the best of our knowledge, sparsest cut with negative weights has not been studied.",A.2 Missing proofs in Section 3,[0],[0]
"Notice however that if the original weights wbc|a, wac|b, wab|c satisfy the triangle inequality (or as a special case, if two of them are zero which is usually the case when we have a triplet constraints), then we can actually solve (approximately) the HSC instance, as the sparsest cut instance will only have non-negative weights.",A.2 Missing proofs in Section 3,[0],[0]
Proof of Theorem 3.,A.3 Missing proofs in Section 4,[0],[0]
"We start by looking at the objective value of any algorithm as the summation of contributions of different triples i, j and k to the objective, where (i, j) ∈ E and k is some other point (possibly equal to i or j).
",A.3 Missing proofs in Section 4,[0],[0]
"OBJ = ∑
(i,j)∈E
wij |Tij",A.3 Missing proofs in Section 4,[0],[0]
"| = ∑
(i,j)∈E,k∈V
wij1{k ∈ leaves(Tij)} = ∑
(i,j)∈E ∑ k∈V Yi,j,k,
where random variable Yi,j,k denotes the contribution of the edge (i, j) and vertex k to the objective value.",A.3 Missing proofs in Section 4,[0],[0]
"The vertex k is a leaf of Tij if and only if right before the time that i and j gets separated k is still in the same cluster as i and j. Therefore,
Yi,j,k = wij1{i separates from k no earlier than j }
We now show",A.3 Missing proofs in Section 4,[0],[0]
that E,A.3 Missing proofs in Section 4,[0],[0]
"[Yi,j,k] = 23wij .",A.3 Missing proofs in Section 4,[0],[0]
"Given this, the expected objective value of recursive random cutting algorithm will be at least 2n3 ∑ (i,j)∈E wij .",A.3 Missing proofs in Section 4,[0],[0]
"Moreover, the objective value of the optimal
hierarchical clustering, i.e. maximizer of the Dasgupta’s objective, is no more than n ∑
(i,j)∈E wij , and we conclude that recursive random cutting is a 23 -approximation.",A.3 Missing proofs in Section 4,[0],[0]
"To see why E [Yi,j,k] = 2 3wij , think of randomized cutting as flipping an independent unbiased coin for each vertex, and then deciding on which side of the cut this vertex belongs to based on the outcome of its coin.",A.3 Missing proofs in Section 4,[0],[0]
"Look at the sequence of the coin flips of i, j and k.",A.3 Missing proofs in Section 4,[0],[0]
"Our goal is to find the probability of the event that for the first time that i and j sequences are not matched, still i’s sequence and k’s sequence are matched up to this point, or still j’s sequence and k’s sequence are matched up to this.",A.3 Missing proofs in Section 4,[0],[0]
The probability of each of these events is equal to 13 .,A.3 Missing proofs in Section 4,[0],[0]
"To see this for the first event, suppose i’s sequence is all heads (H).",A.3 Missing proofs in Section 4,[0],[0]
"We then need the pair of coin flips of (j, k) to be a sequence of (H,H)’s ending with a (T,H), and this happens with probability ∑ i≥1( 1 4) i = 13 .",A.3 Missing proofs in Section 4,[0],[0]
The probability of the second event is similarly calculated.,A.3 Missing proofs in Section 4,[0],[0]
"Now, these events are disjoint.",A.3 Missing proofs in Section 4,[0],[0]
"Hence, the probability that i is separated from k no earlier than j is exactly 23 , as desired.
",A.3 Missing proofs in Section 4,[0],[0]
Proof of Theorem 4.,A.3 Missing proofs in Section 4,[0],[0]
We derandomize the recursive random cutting algorithm using the method of conditional expectations.,A.3 Missing proofs in Section 4,[0],[0]
"At every recursion, we go over the points in the current cluster one by one, and decide whether to put them in the “left” partition or “right” partition for the next recursion.",A.3 Missing proofs in Section 4,[0],[0]
"Once we make a decision for a point, we fix that point and go to the next one.",A.3 Missing proofs in Section 4,[0],[0]
"Now suppose for a cluster C we have already fixed points S ⊆ C, and now we want to make a decision for i ∈ C",A.3 Missing proofs in Section 4,[0],[0]
\ S.,A.3 Missing proofs in Section 4,[0],[0]
"The reward of assigning to left(right) partition is now defined as the expected value of recursive random cutting restricted to C, when the points in S are fixed (i.e. it is already decided which points in S are going to the left partition and which ones are going to the right partition), i goes to the left(right) partition and j ∈",A.3 Missing proofs in Section 4,[0],[0]
C \,A.3 Missing proofs in Section 4,[0],[0]
({i} ∪ S) are randomly assigned to either the left or right.,A.3 Missing proofs in Section 4,[0],[0]
"Note that these two rewards (or the difference of the two rewards) can be calculated exactly in polynomial time by considering all triples consisting of an edge and another vertex, and then calculating the probability that this triple contributes to the objective function (this is similar to the proof of Theorem 3, and we omit the details for brevity here).",A.3 Missing proofs in Section 4,[0],[0]
"Because we know the randomized assignment of i gives a 23 -approximation (Theorem 3), we conclude that assigning to the better of left or right partition for every vertex will remain to be at least a 23 -approximation.",A.3 Missing proofs in Section 4,[0],[0]
"For running time, we have at most n clusters to investigate.",A.3 Missing proofs in Section 4,[0],[0]
"Moreover, a careful counting argument shows that the total number of operations required to calculate the differences of the rewards of assigning to left and right partitions for all vertices is at most n(n+ 2m).",A.3 Missing proofs in Section 4,[0],[0]
"Hence, the running time is bounded by O(n2(n+m)).
",A.3 Missing proofs in Section 4,[0],[0]
Proof sketch of Theorem 5.,A.3 Missing proofs in Section 4,[0],[0]
"Before starting to prove the theorem, we prove the following simple lemma.
",A.3 Missing proofs in Section 4,[0],[0]
Lemma 3.,A.3 Missing proofs in Section 4,[0],[0]
"There is no edge between any two classes in the same layer Il.
Proof of Lemma 3.",A.3 Missing proofs in Section 4,[0],[0]
"If such an edge exists, then there is a path of length l + 1 from C to a class in Il, a contradiction.
",A.3 Missing proofs in Section 4,[0],[0]
"Now, similar to the proof of Theorem 3, we consider every triple {x, y, z}, where (x, y) ∈ E and z is another point , but this time we only consider z’s that are not involved in any triplet constraint (there are at least n− k such points).",A.3 Missing proofs in Section 4,[0],[0]
"We claim with probability at least 23·DMC({c1,...,ck}) the supernode containing z is still in the same cluster as supernodes containing x and y right before x and y gets separated.",A.3 Missing proofs in Section 4,[0],[0]
"By summing over all such triples, we show that the algorithm gets a gain of at least 2(n−k)3·DMC({c1,...,ck}) ∑ (x,y)∈E wxy, which proves the α-approximation as the optimal clustering
has a reward bounded by n ∑
(x,y)∈E wxy.",A.3 Missing proofs in Section 4,[0],[0]
"To prove the claim, if (x, y) is not the base of any triplet constraint then a similar argument as in the proof of Theorem 3 shows the desired probability is exactly 23 (with a slight adaptation, i.e. by looking at the coin sequences of supernodes containing x and y, which are going to be disjoint in this case at all iterations, and the coin sequence of z).",A.3 Missing proofs in Section 4,[0],[0]
"Now suppose (x, y) is the base of any constraint c and suppose c belongs to a class C. Consider the layered dependency subgraph of C as in Definition 2 and let the layers to be I0, . . .",A.3 Missing proofs in Section 4,[0],[0]
", IL.",A.3 Missing proofs in Section 4,[0],[0]
"In order for z to be in the same cluster as x and y when they get separated, a chain of L + 1 independent events needs to happen.",A.3 Missing proofs in Section 4,[0],[0]
"These events are defined inductively; for the first event, consider the coin sequence of z, coin sequence of (the supernode containing all the bases of) constraints in ∪Ll=0Il and coin sequences of all the keys of constraints in IL (there are ∑ C′∈IL |C
′| of them).",A.3 Missing proofs in Section 4,[0],[0]
"Without loss of generality, suppose the coin sequence of (the supernode containing) ∪Ll=0Il is all heads.",A.3 Missing proofs in Section 4,[0],[0]
Now the event happens only if at the time z flips its first tales all keys of IL have already flipped at least one tales.,A.3 Missing proofs in Section 4,[0],[0]
"Conditioned on this event happening, all the constraints in IL will be resolved and z remains in the same cluster as x and y.",A.3 Missing proofs in Section 4,[0],[0]
"Now, remove IL from the dependency subgraph and repeat the same process to define the events 2, . . .",A.3 Missing proofs in Section 4,[0],[0]
", L in a similar fashion.",A.3 Missing proofs in Section 4,[0],[0]
"For the lth event to happen, we need to look at 1 + ∑ C′∈IL |C
′| number of i.i.d.",A.3 Missing proofs in Section 4,[0],[0]
"symmetric geometric random variable, and calculate the probability that first of
them is no smaller than the rest.",A.3 Missing proofs in Section 4,[0],[0]
"This event happens with a probability at least ( 1 + ∑ C′∈IL |C ′| )−1
.",A.3 Missing proofs in Section 4,[0],[0]
"Moreover the events are independent, as there is no edge between any two classes in Il for l ∈",A.3 Missing proofs in Section 4,[0],[0]
"[L], and different classes have different keys.",A.3 Missing proofs in Section 4,[0],[0]
"After these L events, the final event that needs to happen is when all the constraints are unlocked, and z needs to remain in the same cluster as x and y at the time they get separated.",A.3 Missing proofs in Section 4,[0],[0]
This event happens with probability 23 .,A.3 Missing proofs in Section 4,[0],[0]
Multiplying all of these probabilities due to independence implies the desired approximation factor.,A.3 Missing proofs in Section 4,[0],[0]
The purpose of this section is to present the benefits of incorporating triplet constraints when performing Hierarchical Clustering.,B Experiments,[0],[0]
We will focus on real data using the Zoo dataset (Lichman [2013]) for a taxonomy application.,B Experiments,[0],[0]
"We demonstrate that using our approach, the performance of simple recursive spectral clustering algorithms can be improved by approximately 9% as measured by the Dasgupta’s Hierarchical Clustering cost function (1).",B Experiments,[0],[0]
"More specifically:
• The Zoo dataset : It contains 100 animals forming 7 different categories (e.g. mammals, amphibians etc.).",B Experiments,[0],[0]
"The features of each animal are provided by a 16-dimensional vector containing
information such as if the animal has hair or feathers etc.
",B Experiments,[0],[0]
"• Evaluation method : Given the feature vectors, we can create a similarity matrixM(·, ·) indexed by the labels of the animals.",B Experiments,[0],[0]
"We choose the widely used cosine similarity to create M .
•",B Experiments,[0],[0]
Algorithms: We use a simple implementation of spectral clustering based on the second eigenvector of the normalized Laplacian of M .,B Experiments,[0],[0]
"By applying the spectral clustering algorithm once, we can create two clusters; by applying it recursively we can create a complete hierarchical decomposition, which is ultimately the output of the HC algorithm.
",B Experiments,[0],[0]
"• Baseline comparison: Since triplet constraints are especially useful when there is noisy information (i.e. noisy features), we simulate this situation by hiding some of the features of our Zoo dataset.",B Experiments,[0],[0]
"Specifically, when we want to find the target HC tree T ∗, we use the full 16-dimensional feature vectors, but for the comparison between the unconstrained and the constrained HC algorithms we will use a noisy version of the feature vectors which consists of only the first 10 coordinates from every vector.
",B Experiments,[0],[0]
"In more detail, the first step in our experiments is to evaluate the cost of the target clustering T ∗.",B Experiments,[0],[0]
"For this, we use the full feature vectors and perform repeated spectral clustering to get a hierarchical decomposition (without incorporating any constraints).",B Experiments,[0],[0]
"We call this cost OPT.
",B Experiments,[0],[0]
"The second step is to perform unconstrained HC but with noisy information, i.e. to run the spectral clustering algorithm repeatedly on the 10-dimensional feature vectors (again without taking into account any triplet constraints).",B Experiments,[0],[0]
"This will output a hierarchical tree that has cost in terms of the Dasgupta’s HC cost Unconstrained_Noisy_Cost.11
The final step is to choose some structural constraints (that are valid in T ∗)12 and perform again HC with noisy information.",B Experiments,[0],[0]
We again use the 10-dimensional feature vectors but the spectral clustering algorithm is allowed only cuts that do not violate any of the given structural constraints.,B Experiments,[0],[0]
"Repeating until we get a decomposition gives us the final output which will have cost in terms of the Dasgupta’s HC cost Constrained_Noisy_Cost.
",B Experiments,[0],[0]
"The first main result of our experimental evaluation is that the Constrained_Noisy_Cost is surprisingly close to OPT, even though to get the Constrained_Noisy_Cost the features used were noisy and the second main result is that incorporating the structural constraints yields ≈ 9% improvement over the noisy unconstrained version of HC with cost Unconstrained_Noisy_Cost.",B Experiments,[0],[0]
"Now that we have presented the experimental set-up, we can proceed by describing our results and final observations in greater depth.",B Experiments,[0],[0]
"We ran our experiments for 20, 50, 80 and 100 animals from the Zoo dataset and for the evaluation of the % improvement in terms of the Dasgupta’s HC cost (1), we used the following formula:",B.1 Experimental Results,[0],[0]
"The improvements obtained due to the constrained version are presented in Table 1.
11The cost of the trees are always evaluated using the actual similarities obtained from the full feature vectors.",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
12Here we chose triplet constraints that will induce the same first cut as T ∗ and required no constraints after that.,Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
"This corresponds to a high-level separation of the animals, for example to those that are “land” animals versus those that are “water” animals.
",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
"#animals OPT Unconstrained_Noisy_Cost Constrained_Noisy_Cost % Improvement
20 1137 1286 1142 12.63 50 23088 25216 23443 7.68 80 89256 99211 90419 9.85 100 171290 190205 173499 9.75",Unconstrained_Noisy_Cost− Constrained_Noisy_Cost OPT,[0],[0]
Hierarchical clustering is a popular unsupervised data analysis method.,abstractText,[0],[0]
"For many real-world applications, we would like to exploit prior information about the data that imposes constraints on the clustering hierarchy, and is not captured by the set of features available to the algorithm.",abstractText,[0],[0]
This gives rise to the problem of hierarchical clustering with structural constraints.,abstractText,[0],[0]
"Structural constraints pose major challenges for bottom-up approaches like average/single linkage and even though they can be naturally incorporated into top-down divisive algorithms, no formal guarantees exist on the quality of their output.",abstractText,[0],[0]
"In this paper, we provide provable approximation guarantees for two simple top-down algorithms, using a recently introduced optimization viewpoint of hierarchical clustering with pairwise similarity information [Dasgupta, 2016].",abstractText,[0],[0]
"We show how to find good solutions even in the presence of conflicting prior information, by formulating a constraint-based regularization of the objective.",abstractText,[0],[0]
"We further explore a variation of this objective for dissimilarity information [Cohen-Addad et al., 2018] and improve upon current techniques.",abstractText,[0],[0]
"Finally, we demonstrate our approach on a real dataset for the taxonomy application.",abstractText,[0],[0]
Hierarchical Clustering with Structural Constraints,title,[0],[0]
"Multivariate time series (MTS) analysis (Hamilton, 1994; Reinsel, 2003) has attracted a lot of attention in machine learning, signal processing, and other related areas, due to its impact and usefulness in many real world applications such as healthcare, climate, and financial forecasting.",1. Introduction,[0],[0]
"Statespace models such as Kalman filters (Kalman et al., 1960) and hidden Markov models (Rabiner, 1989) have been developed to model MTS and have shown promising results on prediction tasks such as forecasting and interpolation.",1. Introduction,[0],[0]
"However, in many applications, the MTS observations usually come from multiple sources and are often characterized
*Equal contribution 1Department of Computer Science, University of Southern California, Los Angeles, California, United States.",1. Introduction,[0],[0]
"Correspondence to: Zhengping Che, Sanjay Purushotham, Guangyu Li, Bo Jiang, Yan Liu <{zche,spurusho,guangyul,boj,yanliu.cs}@usc.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
by various sampling rates.",1. Introduction,[0],[0]
"For example, in healthcare, vital signs such as heart rate are sampled frequently, while lab results such as pH are measured infrequently; in finance, the stock prices are sampled daily or even more frequently, while macro-economic data such as employment, GDP are sampled monthly or quarterly.",1. Introduction,[0],[0]
Such time series observations with either regular or irregular sampling rates are termed as Multi-Rate Multivariate Time Series (MR-MTS) data.,1. Introduction,[0],[0]
Modeling the MR-MTS using state-space models is challenging since MR-MTS naturally comes with multiple temporal dependencies and these dependencies may not have direct relationship to the sampling rates.,1. Introduction,[0],[0]
"That is, the long and short-term temporal dependencies may be associated with a few or all the time series data with different sampling rates.",1. Introduction,[0],[0]
"Capturing these temporal dependencies is important as they model the underlying data generation mechanism, and they impact the interpolation and forecasting tasks.",1. Introduction,[0],[0]
"Upsampling or downsampling MR-MTS to a single rate time series cannot address this challenge, since these simple techniques may artifically introduce or remove some naturally occurring dependencies present in MR-MTS.",1. Introduction,[0],[0]
"For example, forward/backward imputation will introduce long-term dependencies.",1. Introduction,[0],[0]
"Therefore, building models which can capture multiple temporal dependencies directly from the MR-MTS data is still an open problem in the time series analysis field.
",1. Introduction,[0],[0]
"Deep learning models such as recurrent neural networks (RNNs) (Hochreiter & Schmidhuber, 1997) have emerged as successful models for time series analysis (Graves et al., 2013; Mikolov et al., 2010) and sequence modeling applications (Socher et al., 2011; Xu et al., 2015).",1. Introduction,[0],[0]
"While deep discriminative models (Hermans & Schrauwen, 2013; Martens & Sutskever, 2011; Pascanu et al., 2013; Chung et al., 2016) have been shown to model complex non-linear temporal dependencies present in MTS, deep generative models (Gan et al., 2015; Rezende et al., 2014) have become more popular since they are intuitive, interpretable and are more powerful than their discriminative counterparts (Durbin & Koopman, 2012) and they capture the data generation process.",1. Introduction,[0],[0]
"Despite their success with single-rate time series data, the existing deep generative models are not suitable for modeling MRMTS as they are not designed to capture multiple temporal dependencies from different sampling rates.
",1. Introduction,[0],[0]
"Recently, latent hierarchical structure learning based on deep learning models have led to remarkable ad-
vances in capturing temporal dependencies from sequential data (El Hihi & Bengio, 1995; Chung et al., 2016; Koutnik et al., 2014).",1. Introduction,[0],[0]
"Motivated by these models, we propose a novel deep generative model termed as Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), which learns multiple temporal dependencies directly from MR-MTS by jointly modeling time series with different sampling rates.",1. Introduction,[0],[0]
MRHDMM learns the latent hierarchical structures along with learnable switches and captures the data generation process of MR-MTS.,1. Introduction,[0],[0]
It simultaneously learns a inference network and a generative model by leveraging a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution.,1. Introduction,[0],[0]
"The data generation process of MR-HDMM can automatically infer the hierarchical structures directly from data, which is extremely helpful for downstream tasks such as interpolation and forecasting.
",1. Introduction,[0],[0]
"In summary, we develop a first-of-a-kind novel deep generative model called MR-HDMM to systematically capture the multiple temporal dependencies present in MR-MTS by using hierarchical latent structures and learnable switches.",1. Introduction,[0],[0]
"In addition, we also propose a new structured inference network for MR-HDMM.",1. Introduction,[0],[0]
A comprehensive and systematic evaluation of the MR-HDMM model is conducted on two real-world datasets to demonstrate the state-of-the-art performance in forecasting and interpolation tasks.,1. Introduction,[0],[0]
"Finally, we interpret the learnt latent hierarchies from MR-HDMM to study the captured temporal dependencies.",1. Introduction,[0],[0]
"State-space models such as Kalman filters (KF) (Kalman et al., 1960), and hidden Markov models (HMMs) (Rabiner, 1989) have been widely used in various time series applications such as speech recognition (Rabiner, 1989), atmospheric monitoring (Houtekamer & Mitchell, 2001), and robotic control (Negenborn, 2003).",2. Related Work,[0],[0]
"These approaches successfully model regularly sampled (i.e. sampled at the same frequency/rate) time series data, however, they cannot be directly used for MR-MTS as they cannot simultaneously capture the multiple temporal dependencies present in MR-MTS.",2. Related Work,[0],[0]
"To handle MR-MTS with state-space models, researchers have extended KF models and proposed multirate Kalman filters (MR-KF) (Armesto et al., 2008; Safari et al., 2014).",2. Related Work,[0],[0]
MR-KF approaches either fuse the data with different sampling rates or fuse the estimates for KFs trained on each sampling rate.,2. Related Work,[0],[0]
Many of these MR-KF approaches aim to improve the estimates for the highest sampled rate data and do not focus on capturing the multiple temporal dependencies present in MR-MTS.,2. Related Work,[0],[0]
"Moreover, the linear transition and emission functionality of the MR-KF models limits their usability on complex real-world data.
",2. Related Work,[0],[0]
"Recently, researchers have resorted to deep learning models (Chung et al., 2016; Krishnan et al., 2015; Gan et al.,
2015) to model the non-linear temporal dynamics of realworld and sequential data.",2. Related Work,[0],[0]
"Discriminative models such as hierarchical recurrent neural network (El Hihi & Bengio, 1995), hierarchical multiscale recurrent neural network (HM-RNN) (Chung et al., 2016), and phased long short-term memory (PLSTM) (Neil et al., 2016) have been proposed to capture temporal dependencies of sequential data.",2. Related Work,[0],[0]
"However, these discriminative models do not capture the underlying data generation process and therefore are not suited for forecasting and interpolation tasks.",2. Related Work,[0],[0]
"Deep generative models (Rezende et al., 2014; Krishnan et al., 2015; Gan et al., 2015) have been developed to model the data generation process of the complex time series data.",2. Related Work,[0],[0]
"Krishnan et al. (2015) proposed deep Kalman filter, a nonlinear state-space model, by marrying the ideas of deep neural networks with Kalman filters.",2. Related Work,[0],[0]
Fraccaro et al. (2016) introduced stochastic recurrent neural network (SRNN) which glued a RNN with a state space model together to form a stochastic and sequential neural generative model.,2. Related Work,[0],[0]
"Even though these deep generative models are the state-of-the-art approaches to obtain the underlying data generation process, they are not designed to capture all the temporal dependencies of MR-MTS.",2. Related Work,[0],[0]
None of the existing deep learning models or state-space models can be directly used for modeling MR-MTS.,2. Related Work,[0],[0]
"Thus, in this work, we develop a deep generative model which leverages the properties of the above discriminative and generative models, to model the data generation process of MR-MTS while also capturing the multiple temporal dependencies using a latent hierarchical structure.",2. Related Work,[0],[0]
"In this section, we present our proposed Multi RateHierarchical Deep Markov Model (MR-HDMM).",3. Our Model,[0],[0]
"We first clarify the notations and definitions used in this paper.
",3. Our Model,[0],[0]
"Notations Given a MR-MTS of L different sampling rates and length T , we use a vectorxlt ∈ RDl to represent the time series observations of lth rate at time t. Here l = 1, . . .",3. Our Model,[0],[0]
", L, t = 1, . . .",3. Our Model,[0],[0]
", T , and Dl is the dimension of time series with lth rate.",3. Our Model,[0],[0]
"The L sampling rates are in descending order, i.e., l = 1 and l = L refer to the highest and lowest sampling rates.",3. Our Model,[0],[0]
"To make the notations succinct, we use xl:l ′
t:t′ to denote all observed time series of lth to l′th rates and from time t to t′.",3. Our Model,[0],[0]
We use θ(.) and φ(.) to denote the parameter sets for generation model pθ and inference network qφ respectively.,3. Our Model,[0],[0]
we use L layers of RNNs in the inference network to model MR-MTS of L different sampling rates.,3. Our Model,[0],[0]
"We use LHS , the number of hidden layers in both generation model and inference network, to control the depth of the learnt hierarchical structures.",3. Our Model,[0],[0]
"In the rest of this paper we take LHS = L for model simplicity, but in practice they are not tied.",3. Our Model,[0],[0]
"The latent states or variables are denoted by z, s and h. Their superscript and subscript respectively indicate the corresponding layer(s) and the time step(s) (e.g., z1:L1:T , s 2:L t , h l t).
",3. Our Model,[0],[0]
Figure 1 illustrates our MR-HDMM model which consists of the generation model and inference network.,3. Our Model,[0],[0]
"MR-HDMM captures the underlying data generation process by using the variational inference methods (Rezende et al., 2014; Kingma & Welling, 2013) and learns the latent hierarchical structures using learnable switches and auxiliary connections to adaptively encode the dependencies across the hierarchies and the timestamps.",3. Our Model,[0],[0]
"In particular, the switches use an update-and-reuse mechanism to control the updates of the latent states of a layer based on their previous states (i.e., utilizing temporal information) and the lower latent layers (i.e., utilizing the hierarchy).",3. Our Model,[0],[0]
"The switch triggers an update of the current states if it gets enough information from lower-level states, otherwise it reuses the previous states.",3. Our Model,[0],[0]
"Thus, the higher-level states act as summarized representations over the lower-level states and the switches help to propagate the temporal dependencies.",3. Our Model,[0],[0]
The auxiliary connections (dashed lines in Figure 1(a)) between MR-MTS of different sampling rates and different latent layers help the model effectively capture the short-term and long-term temporal dependencies.,3. Our Model,[0],[0]
"Without the auxiliary connections, the higher-rate time series may mask the multi-scale dependencies present in the lower-rate time series data while propagating dependencies through bottom-up connections.",3. Our Model,[0],[0]
"Note that, the auxiliary connections are not related to the sampling rate of MR-MTS, and the sampling rate of higherrate variable need not be a multiple of sampling rate of the lower-rate variable.",3. Our Model,[0],[0]
"Due to the flexibility of auxiliary connections, our MR-HDMM can also handle irregularly sampled time series data or missing data.",3. Our Model,[0],[0]
"We can a) zero-out the missing data points in the inference network and remove the corresponding auxiliary connections in the generation model during training, and b) interpolate missing values by adding auxiliary connections in the well-trained model.",3. Our Model,[0],[0]
Figure 1(a) shows the generation model of our MR-HDMM.,3.1. Generation Model,[0],[0]
"The generation process of our MR-HDMM follows the transition and emission framework, which is obtained by applying deep recurrent neural networks to non-linear continuous state space models.",3.1. Generation Model,[0],[0]
"The generation model is carefully designed to incorporate the switching mechanism and auxiliary connections in order to capture the multiple temporal dependencies present in MR-MTS.
",3.1. Generation Model,[0],[0]
Transition We design the transition process of the latent state z to capture the hierarchical structure for multiple temporal dependencies with learnable binary switches s.,3.1. Generation Model,[0],[0]
"For each non-bottom layer l > 1 and time step t ≥ 1, we use a binary switch state slt to control the updates of the corresponding latent states zlt, as shown in Figure 2.",3.1. Generation Model,[0],[0]
"slt is obtained based on the values of the previous latent states zlt−1 and the lower layer latent states z l−1 t by a de-
terministic mapping slt =",3.1. Generation Model,[0],[0]
"I ( gθs(z l t−1, z l−1 t ) ≥ 0 ) .",3.1. Generation Model,[0],[0]
"When the switch is on (i.e., update operation, slt = 1), z l t is updated based on zlt−1 and z l−1 t through a learnt transition distribution.",3.1. Generation Model,[0],[0]
"We use a multivariate Gaussian distribution N ( µlt,Σ l t|zlt−1, zl−1t ; θz ) with mean and covariance given by (µlt,Σ l t) = gθz",3.1. Generation Model,[0],[0]
"(z l t−1, z l−1 t )",3.1. Generation Model,[0],[0]
as the transition distribution.,3.1. Generation Model,[0],[0]
"When the switch is off (i.e., reuse operation, slt = 0), z l t will
be drawn from the same distribution as its previous states zlt−1, which is N ( µlt−1,Σ l t−1 ) .",3.1. Generation Model,[0],[0]
"Note, unlike Chung et al. (2016), we do not copy the previous state since our latent states are stochastic.",3.1. Generation Model,[0],[0]
The latent states of the first layer (z11:T ) are always updated at each time step.,3.1. Generation Model,[0],[0]
"In our model, gθs is parameterized by a multilayer perceptron (MLP), and gθz is parameterized by gated recurrent units (GRU) (Chung et al., 2014) to capture the temporal dependencies.",3.1. Generation Model,[0],[0]
"With this update-or-reuse transition mechanism, higher latent layers tend to capture longer-term temporal dependencies through the bottom-up connections in the latent layers.
",3.1. Generation Model,[0],[0]
Emission Multi-rate multivariate observation x needs to be generated from z in the emission process.,3.1. Generation Model,[0],[0]
"In order to embed the multiple temporal dependencies in the generated MR-MTS, we introduce auxiliary connections (denoted by the dashed lines in Figure 1(a)) from the higher latent layers to the lower rate time series.",3.1. Generation Model,[0],[0]
"That is, time series of lth rate at time t (i.e., xlt) is generated from all latent states up to lth layer z1:lt through emission distribution Π ( xlt|z1:lt ; θx ) .",3.1. Generation Model,[0],[0]
The choice of emission distribution Π is flexible and depends on the data type.,3.1. Generation Model,[0],[0]
"Multinomial distribution is used for categorical data, and Gaussian distribution is used for continuous data.",3.1. Generation Model,[0],[0]
"Since all the data in our tasks are continuous, we use Gaussian distribution where the mean µ(x) l
t
and covariance Σ(x)",3.1. Generation Model,[0],[0]
"l
t are determined by gθx(z 1:l t ), which
is parameterized by an MLP.
",3.1. Generation Model,[0],[0]
"To summarize, the overall generation process is described in Algorithm 1.",3.1. Generation Model,[0],[0]
"The parameter set of generation model is θ = {θx, θz, θs}.",3.1. Generation Model,[0],[0]
"Given this, the joint probability of MRMTS and the latent states/switches can be factorized by the following Equation (1).
pθ ( x1:L1:T ,z 1:L 1:T , s 2:L 1:T |z1:L0 ) =pθ ( x1:L1:T |z1:L1:T ) pθ ( z1:L1:T , s 2:L 1:",3.1. Generation Model,[0],[0]
"T |z1:L0
) =
T∏ t=1 pθ ( x1:Lt |z1:Lt ) · T∏ t=1 pθ ( z1:Lt , s 2:L t |z1:Lt−1 ) =
T∏ t=1",3.1. Generation Model,[0],[0]
L∏ l=1 pθx ( xlt|z1:lt ) · T∏ t=1 pθz ( z1t |z1t−1 ) · T∏ t=1,3.1. Generation Model,[0],[0]
L∏ l=2 pθs (,3.1. Generation Model,[0],[0]
"slt|zlt−1,zl−1t ) pθz ( zlt|zlt−1,zl−1t , slt ) (1)
",3.1. Generation Model,[0],[0]
"In order to obtain the parameters of MR-HDMM, we need to maximize the log marginal likelihood of all MR-MTS data points, which is the summation of the log marginal likelihood L(θ) = log pθ",3.1. Generation Model,[0],[0]
( x1:L1:T |z1:L0 ) of each MR-MTS data point x1:L1:T .,3.1. Generation Model,[0],[0]
The log marginal likelihood of one data point can be achieved by integrating out all possible z and s in Equation (1).,3.1. Generation Model,[0],[0]
"Since s are deterministic binary variables, integrating them out can be done straightforwardly by taking their values in the likelihood.",3.1. Generation Model,[0],[0]
"However, stochastic variable
Algorithm 1 Generation model of MR-HDMM 1: Initialize z1:L0 ∼ N (0, I) 2: for t = 1, . . .",3.1. Generation Model,[0],[0]
", T do 3: ( µ1t ,Σ 1 t ) = gθz",3.1. Generation Model,[0],[0]
"(z 1 t−1)
4: z1t ∼ N ( µ1t ,Σ 1 t ) {Transition of the first layer.}",3.1. Generation Model,[0],[0]
"5: for l = 2, · · · , L do 6: slt =",3.1. Generation Model,[0],[0]
I,3.1. Generation Model,[0],[0]
"( gθs(z l t−1,z l−1 t )",3.1. Generation Model,[0],[0]
"≥ 0
) 7: ( µlt,Σ l t )",3.1. Generation Model,[0],[0]
"= { gθz (z l t−1,z l−1 t )",3.1. Generation Model,[0],[0]
"if s l t = 1(
µlt−1,Σ l t−1 )
otherwise.",3.1. Generation Model,[0],[0]
"8: zlt ∼ N ( µlt,Σ l t ) {Transition of other layers.}
9: end for 10: for l = 1, · · · , L do 11: ( µ(x) l
t,Σ (x)l t ) =",3.1. Generation Model,[0],[0]
"gθx(z 1:l t )
12: xlt ∼ N ( µ(x) l t,Σ (x)l t ) {Emission.} 13: end for 14: end for
z cannot be analytically integrated out.",3.1. Generation Model,[0],[0]
"Thus, we resort to the well-known variational principle (Jordan, 1998) and introduce our inference network below.",3.1. Generation Model,[0],[0]
We design our inference network to mimic the structure of the generative model.,3.2. Inference Network,[0],[0]
The goal is to obtain an objective which can be optimized easily and which can make the model parameter learning amenable.,3.2. Inference Network,[0],[0]
"Instead of directly maximizing L(θ) w.r.t θ, we build an inference network with a tractable distribution qφ , and maximize the variational evidence lower bound (ELBO) F(θ, φ) ≤ L(θ) with respect to both θ and φ.",3.2. Inference Network,[0],[0]
"Note, φ is the parameter set of the inference network which will is formally defined at the end of this section.",3.2. Inference Network,[0],[0]
"The lower bound can be written as (please refer to the supplementary materials for full derivation):
F(θ, φ) =",3.2. Inference Network,[0],[0]
Eqφ [ log pθ ( x1:L1:T |z1:L0:T )],3.2. Inference Network,[0],[0]
"−DKL ( qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0
)∥∥∥pθ (z1:L1:T , s2:L1:T |z1:L0 ))",3.2. Inference Network,[0],[0]
"(2)
where the expectation of the first term is under qφ ( z1:L1:T |x1:L1:T , z1:L0 ) .",3.2. Inference Network,[0],[0]
"To get a tight bound and an accurate estimate from our MR-HDMM, we need to properly design a new inference network as using the existing inference networks from SRNN (Fraccaro et al., 2016) or DMM (Krishnan et al., 2015) is not applicable for MR-MTS.",3.2. Inference Network,[0],[0]
"In the following, we show how we design the inference network (Figure 1(b)) to obtain a good structured approximation to the posterior.",3.2. Inference Network,[0],[0]
"First, we maintain the Markov properties of z in the inference network, which leads to the factorization:
qφ ( z1:L1:T , s 2:L 1:T |x1:L1:T ,z1:L0 ) =",3.2. Inference Network,[0],[0]
T∏ t=1,3.2. Inference Network,[0],[0]
"qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) (3)
We then leverage the hierarchical structure and inherit the switches from the generation model into the
Table 1.",3.2. Inference Network,[0],[0]
"Comparison of structured inference networks.
",3.2. Inference Network,[0],[0]
Inference network Implemented with RNN output Captured in hlt Variational approximation for zlt filtering forward RNN hforward xl1:,3.2. Inference Network,[0],[0]
"t qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) smoothing backward RNN hbackward xlt:T qφ ( zlt|zlt−1,zl−1t , slt,x1:Lt:T
) bi-direction bi-directional RNN [ hforward,hbackward ]",3.2. Inference Network,[0],[0]
"xl1:T qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T
) inference network.",3.2. Inference Network,[0],[0]
"That is, the same gθs from the generation model is used in the inference network, i.e., qφ ( slt|zlt−1, zl−1t ,x1:L1:T ) =",3.2. Inference Network,[0],[0]
"qφs ( slt|zlt−1, zl−1t ) =
pθs ( slt|zlt−1, zl−1t ) .",3.2. Inference Network,[0],[0]
"Then, for each term in the righthand side of Equation (3) and for all t = 1, · · · , T , we have:
qφ ( z1:Lt , s 2:L t |z1:Lt−1,x1:L1:T ) =qφ ( z1t |z1t−1,x1:L1:T
) ·",3.2. Inference Network,[0],[0]
"L∏ l=2 qφ ( slt|zlt−1,zl−1t ,x1:L1:T ) qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T
) =qφ ( z1t |z1t−1,x1:L1:T
) ·",3.2. Inference Network,[0],[0]
"L∏ l=2 pθs ( slt|zlt−1,zl−1t )",3.2. Inference Network,[0],[0]
"qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) (4) Thus, the inference network can be factorized by Equation (3) and (4).",3.2. Inference Network,[0],[0]
"Note, we also can factorize generative model based on Equation (1).",3.2. Inference Network,[0],[0]
"Given these, we further factorize the ELBO in Equation (2) as a summation of expectations of conditional log likelihood and KL divergence terms over time steps and hierarchical layers:
F(θ, φ) = T∑ t=1 L∑",3.2. Inference Network,[0],[0]
l=1,3.2. Inference Network,[0],[0]
EQ∗(z1,3.2. Inference Network,[0],[0]
":lt ) log pθx ( xlt|z1:lt ) +
T∑ t=1",3.2. Inference Network,[0],[0]
"EQ∗(z1t−1)DKL ( qφ ( z1t |x1:L1:T ,z1t−1 )∥∥∥pθ (z1t |z1t−1))",3.2. Inference Network,[0],[0]
"+
T∑ t=1 L∑ l=2 EQ∗(z1t−1,zl−1t )
DKL ( qφ ( zlt|x1:L1:T ,zlt−1,zl−1t )∥∥∥pθ (z1t |z1t−1,zl−1t )) (5) where Q∗ (·) denotes the marginal distribution of (·) from qφ .",3.2. Inference Network,[0],[0]
"The details about the factorization and the marginalized distribution are provided in the supplementary materials.
",3.2. Inference Network,[0],[0]
"Parameterization of inference network We parameterize the inference network and construct the variational approximation qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) used in Equation 5 by deep learning models.",3.2. Inference Network,[0],[0]
"First, we use L RNNs to capture MR-MTS with L different sampling rates such that each rate is modeled by one RNN model separately.",3.2. Inference Network,[0],[0]
"Second, to obtain lth latent states zlt of the inference network at time step t, we not only use the previous latent states zlt−1 and the lower layer latent states z l−1 t but also take the lth RNN output denoted by hlt as an input.",3.2. Inference Network,[0],[0]
"Third, we reuse
the same latent state distribution and switch mechanism from the generation model to generate z of the inference network.",3.2. Inference Network,[0],[0]
"To be more specific, zlt is drawn from a multivariate normal distribution, where the mean and covariance are reused from those of zlt−1 if s l t = 1 and l > 1, otherwise the mean and covariance are modeled by gated recurrent units (GRU) with input [ hlt, z l t−1, z l−1 t ] .",3.2. Inference Network,[0],[0]
"The choice of the RNN models for hlt affects what and how the information at other time steps is considered in the approximation at time t, i.e. the form of qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) .",3.2. Inference Network,[0],[0]
"Inspired by Krishnan et al. (2016), we construct the variational approximation in three settings (filtering, smoothing, bi-direction) for forecasting and interpolation tasks.",3.2. Inference Network,[0],[0]
"In filtering setting, we only consider the information up to time t (i.e., x1:L1:t ) using forward RNNs.",3.2. Inference Network,[0],[0]
"By doing this, we have hlt = hlt forward = RNN forward ( hlt−1 forward ,xlt ) , and thus
qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:T ) =",3.2. Inference Network,[0],[0]
"qφ ( zlt|zlt−1,zl−1t , slt,x1:L1:t ) .",3.2. Inference Network,[0],[0]
"The filtering setting does not use future information, so it is suitable for forecasting task at future time step t′ > T .",3.2. Inference Network,[0],[0]
"For interpolation tasks, we can use backward RNNs to utilize the information after time t (i.e., x1:Lt:T ) with h l t = h",3.2. Inference Network,[0],[0]
"l t backward =
RNN backward ( hlt+1 backward ,xlt ) , or bi-directional RNNs to uti-
lize information across all time steps, which is x1:L1:T , at any time t with hlt =",3.2. Inference Network,[0],[0]
"[ hlt forward ,hlt backward ] .",3.2. Inference Network,[0],[0]
"These two models lead to smoothing and bidirection settings, respectively.",3.2. Inference Network,[0],[0]
We summarize the three inference networks in Table 1.,3.2. Inference Network,[0],[0]
"We use φh and φz to denote the parameter sets related to h and z respectively and use φ = {φh, φz, φs = θs} to represent the parameter set of the inference network.",3.2. Inference Network,[0],[0]
"We jointly learn the parameters (θ, φ) of the generative model pθ and the inference network qφ by maximizing the ELBO in Equation (5).",3.3. Learning the Parameters,[0],[0]
"The main challenge in the optimization is obtaining the gradients of all the terms under the correct expectation i.e, EQ∗ .",3.3. Learning the Parameters,[0],[0]
"We use stochastic backpropagation (Kingma & Welling, 2013) for estimating all these gradients and train the model by stochastic gradient descent (SGD) approaches.",3.3. Learning the Parameters,[0],[0]
We employ ancestral sampling techniques to obtain the samples z .,3.3. Learning the Parameters,[0],[0]
"That is, we draw all samples z in a sequential way from time 1 to T and from layer 1 to L. Given the samples from previous layer l − 1 or previous time t− 1, the new samples at time t and layer l will be distributed according to the marginal distribution Q∗. Notice
Algorithm 2 Learning MR-HDMM with stochastic backpropagation and SGD Require: X : a set of MR-MTS of L sampling rates; Initial (θ, φ)
1: while not converged do 2: Choose a random minibatch of MR-MTS X ′ ⊂ X 3: for each sample",3.3. Learning the Parameters,[0],[0]
x1:L1:T ∈ X ′,3.3. Learning the Parameters,[0],[0]
"do 4: Compute h1:L1:T by inference network φh on input x 1:L 1:T 5: Sample ẑ1:L0 ∼ N (0, I) 6: for t = 1, · · · , T do 7: Estimate µ1t (φ) ,Σ1t
(φ) by φz , and µ1t ,Σ1t by θz , given samples ẑ1t−1 and h 1 t
8:",3.3. Learning the Parameters,[0],[0]
"Based onµ1t (φ) ,Σ1t (φ) ,µ1t ,Σ 1 t , compute the gradient of DKL ( qφ ( z1t |· )∥∥∥pθ (z1t |·))",3.3. Learning the Parameters,[0],[0]
"9: Sample ẑ1t ∼ N ( µ1t (φ) ,Σ1t (φ) )
10: for l = 2, · · · , L do 11: Compute slt by θs from samples ẑlt−1 and ẑ l−1 t 12: Estimate µlt (φ) ,Σlt (φ) by φz , and µlt,Σlt by θz ,
given samples ẑlt−1, ẑ l−1 t , s l t, and hlt
13: Based on µlt (φ) ,Σlt (φ) ,µlt,Σ l t, compute the gradient of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·))
14: Sample ẑlt ∼ N ( µlt (φ) ,Σlt (φ) )",3.3. Learning the Parameters,[0],[0]
"15: end for 16: Compute the gradient of log pθx ( xlt|ẑ1:lt
) 17: end for 18: end for 19: Update (θ, φ) using all gradients 20: end while
that all terms of DKL ( qφ ( zlt|· )∥∥∥pθ (zlt|·))",3.3. Learning the Parameters,[0],[0]
"in Equation (5) are KL divergences between two multivariate Gaussian distributions, and pθx ( xlt|z1:lt ) is also a multivariate Gaussian distribution.",3.3. Learning the Parameters,[0],[0]
"Thus, all the required gradients can be estimated analytically from the samples drawn in our proposed way.",3.3. Learning the Parameters,[0],[0]
Algorithm 2 shows the overall learning procedure.,3.3. Learning the Parameters,[0],[0]
We conducted experiments on two real-world datasets - the MIMIC-III healthcare dataset and the USHCN climate dataset - and answer the following questions: (a) How does our proposed model perform when compared to the existing state-of-the-art approaches?,4. Experiments,[0],[0]
"(b) To what extent, are the proposed learnable hierarchical latent structure and auxiliary connections useful to model the data generation process?",4. Experiments,[0],[0]
(c) How do we interpret the hierarchy learned by the proposed model?,4. Experiments,[0],[0]
"In the remainder of this section, we will describe the datasets, methods, empirical results and interpretations to answer the above questions.",4. Experiments,[0],[0]
"MIMIC-III dataset MIMIC-III is a public de-identified dataset collected at Beth Israel Deaconess Medical Cen-
ter from 2001 to 2012 (Johnson et al., 2016).",4.1. Datasets and Experimental Design,[0],[0]
"It contains over 58,000 hospital admission records of 38,645 adults and 7,875 neonates.",4.1. Datasets and Experimental Design,[0],[0]
"For our experiments, we chose 10,709 adult admission records and extracted 62 temporal features from the first 72 hours.",4.1. Datasets and Experimental Design,[0],[0]
"These features had one of the three sampling rates of 1 hour, 4 hours and 12 hours.",4.1. Datasets and Experimental Design,[0],[0]
To fill-in any missing entries in our dataset we used forward or linear imputation similar to Che et al. (2016).,4.1. Datasets and Experimental Design,[0],[0]
"To ensure fair comparison, we only evaluate and compare all the models on the original time-series (i.e. non-imputed data).",4.1. Datasets and Experimental Design,[0],[0]
"Our main tasks on the MIMIC-III dataset are forecasting on time series with all rates, and interpolation of the low-rate time series values.
",4.1. Datasets and Experimental Design,[0],[0]
"USHCN climate dataset The U.S. Historical Climatology Network Monthly (USHCN) dataset (Menne et al., 2010) is publicly available and consists of daily meteorological data of 54 stations in California spanning from 1887 to 2009.",4.1. Datasets and Experimental Design,[0],[0]
"It has five climate variables for each station: a) daily maximum temperature, b) daily minimum temperature, c) whether it was a snowy day or not, d) total daily precipitation, and e) daily snow precipitation.",4.1. Datasets and Experimental Design,[0],[0]
We preprocessed this dataset to extract daily climate data for 100 consecutive years starting from 1909.,4.1. Datasets and Experimental Design,[0],[0]
"To get multi-rate time series data, we extract 208 features and split all features into 3 groups with sampling rates of 1 day, 5 days, and 10 days respectively.",4.1. Datasets and Experimental Design,[0],[0]
This public dataset has been carefully processed by National Oceanic and Atmospheric Administration (NOAA) to ensure quality control and it has no missing entries.,4.1. Datasets and Experimental Design,[0],[0]
"Our tasks on this dataset are climate forecasting on all features and interpolation on 5-day and 10-day sampled data.
",4.1. Datasets and Experimental Design,[0],[0]
Tasks We use the proposed MR-HDMM on two prediction tasks: multi-rate time series forecasting and low-rate time series interpolation.,4.1. Datasets and Experimental Design,[0],[0]
"Since both datasets have 3 different sampling rates, we use HSR/MSR/LSR to denote high/medium/low sampling rate respectively.
",4.1. Datasets and Experimental Design,[0],[0]
•,4.1. Datasets and Experimental Design,[0],[0]
Forecasting:,4.1. Datasets and Experimental Design,[0],[0]
Predict the future multivariate time series based on its history.,4.1. Datasets and Experimental Design,[0],[0]
"For MIMIC-III dataset, we predict the last 24 hrs time series based on the first (previous) 48 hours time series data.",4.1. Datasets and Experimental Design,[0],[0]
"In USHCN dataset, we forecast the climate for the next 30 days based on the observations of the previous year.",4.1. Datasets and Experimental Design,[0],[0]
•,4.1. Datasets and Experimental Design,[0],[0]
Interpolation: Fill-in the low rate time series based on co-evolving higher rate time series data.,4.1. Datasets and Experimental Design,[0],[0]
"For MIMIC-III dataset, we down-sampled 8 features from MSR to LSR and then performed interpolation task by up-sampling these 8 features back to MSR.",4.1. Datasets and Experimental Design,[0],[0]
"For USHCN dataset, the interpolation task involved up-sampling the MSR and LSR features to HSR features, i.e. up-sample 5-day and 10-day data to 1-day.",4.1. Datasets and Experimental Design,[0],[0]
"We demonstrate in-sample interpolation (i.e. interpolation within training dataset) and out-sample interpolation (i.e. interpolation in the testing dataset) on the MIMIC-III dataset and in-sample interpolation on the USHCN dataset.
",4.1. Datasets and Experimental Design,[0],[0]
Baselines We compare MR-HDMM with several strong baselines in these two tasks.,4.1. Datasets and Experimental Design,[0],[0]
"Additionally, to show the advantage of learnable hierarchical latent structure and auxiliary connections, we simplify MR-HDMM into two other models for comparison: (a) Multi-Rate Deep Markov Models (MR-DMM) which removes the hierarchical structure in latent space; (b) Hierarchical Deep Markov Models (HDMM) which drops the auxiliary connections between the lowerrate time series and higher level latent layers.",4.1. Datasets and Experimental Design,[0],[0]
"MR-DMM and HDMM are discussed in the supplementary materials.
",4.1. Datasets and Experimental Design,[0],[0]
"For forecasting tasks, we compare MR-HDMM with the following baseline models:
• Single-rate: Kalman Filters (KF), Vector AutoRegression (VAR), Long-Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), PhasedLSTM (PLSTM) (Neil et al., 2016), Deep Markov Models (DMM) (Krishnan et al., 2015) and Hierarchical Multiscale Recurrent Neural Networks (HM-RNN) (Chung et al., 2016).",4.1. Datasets and Experimental Design,[0],[0]
"• Multi-rate: Multiple Kalman Filters (MKF) (Drolet et al.,
2000), Multi-rate Kalman Filters (MR-KF) (Safari et al., 2014), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM).
",4.1. Datasets and Experimental Design,[0],[0]
"For interpolation task, we compare MR-HDMM with the following baseline models:
• Imputation methods: Mean imputation (Simple-Mean), Cubic Spline (CubicSpline) (De Boor et al., 1978), Multiple Imputations by Chained Equations (MICE) (White et al., 2011), MissForest (Stekhoven & Bühlmann, 2011), SoftImpute (Mazumder et al., 2010).",4.1. Datasets and Experimental Design,[0],[0]
"• Deep learning models: Deep Markov Models (DMM), Multi-Rate Deep Markov Models (MR-DMM) and Hierarchical Deep Markov Models (HDMM).",4.1. Datasets and Experimental Design,[0],[0]
We show the evaluation results of our MR-HDMM on the following: (a) Forecasting: we generate the next latent state using the learned transition distribution and then generate observations from these new latent states; (b) Interpolation: we use the mode of the approximated posterior in the generation model to generate the unseen data in low-rate time series.,4.2. Evaluation and Implementation Details,[0],[0]
"(c) Inference: we take multi-rate time series as the input to obtain the approximate posterior of latent states.
",4.2. Evaluation and Implementation Details,[0],[0]
"For generation model in MR-HDMM, we use multivariate Gaussian with diagonal covariance for both emission distribution and transition distribution.",4.2. Evaluation and Implementation Details,[0],[0]
"We parameterized the emission mapping gθx by a 3-layer MLP with ReLU activations, the transition mapping gθz by gated recurrent unit (GRU), and mapping gθs by a 3-layer MLP with ReLU activations on the hidden layers and linear activations on the output layer.",4.2. Evaluation and Implementation Details,[0],[0]
"For inference networks, we adopt filter-
ing setting for forecasting and bidirection setting for interpolation from Table 1 with 3-layer GRUs.",4.2. Evaluation and Implementation Details,[0],[0]
"To update θs, we replace the sign function with a sharp sigmoid function during training, and use the indicator function during validation.",4.2. Evaluation and Implementation Details,[0],[0]
"The single-rate baseline models cannot handle multi-rate data directly, and we up-sample all the lower rate data into higher rate data using linear interpolation.",4.2. Evaluation and Implementation Details,[0],[0]
"We use the stats-toolbox (Seabold & Perktold, 2010) in python for the VAR model implementation.",4.2. Evaluation and Implementation Details,[0],[0]
"We use pykalman (Duckworth, 2013) to implement all the KFbased models.",4.2. Evaluation and Implementation Details,[0],[0]
The implementation details of the KF-based methods are discussed in the supplementary materials.,4.2. Evaluation and Implementation Details,[0],[0]
"For LSTM and PLSTM model, we use one layer with 100 neurons to model the time-series, and then apply a soft-max regressor on top of the last hidden state to do regression.
",4.2. Evaluation and Implementation Details,[0],[0]
"To ensure a fair comparison, we use roughly the same amount of parameters for all models.",4.2. Evaluation and Implementation Details,[0],[0]
"For experiments on USHCN dataset, train/valid/test sets were split as 70/10/20.",4.2. Evaluation and Implementation Details,[0],[0]
"For experiments on MIMIC-III, we used 5-fold cross validation (train on 3 folds, validate on another fold and test on the remaining fold) and report the average Mean Squared Error (MSE) of 5 runs for both forecasting and interpolation tasks.",4.2. Evaluation and Implementation Details,[0],[0]
"Note that, we train all the deep learning models with the Adam optimization method (Kingma & Ba, 2014) and use validation set to find the best weights, and report the results on the held-out test set.",4.2. Evaluation and Implementation Details,[0],[0]
All the input variables are normalized to be of 0 mean and 1 standard deviation.,4.2. Evaluation and Implementation Details,[0],[0]
Forecasting Table 2 and 3 respectively show the forecasting results on MIMIC-III and USHCN datasets in terms of MSE.,4.3. Quantitative Results,[0],[0]
Our proposed MR-HDMM outperforms all the competing multi-rate latent space models by at least 5% and beats the single-rate models by at least 15% on both datasets with all features.,4.3. Quantitative Results,[0],[0]
"Our model also performs the best on single-rate HSR and MSR forecasting tasks, and performs well on the LSR forecasting task on MIMIC-III and USHCN datasets.
",4.3. Quantitative Results,[0],[0]
Interpolation Table 4 shows the interpolation results on the two datasets.,4.3. Quantitative Results,[0],[0]
"Since VAR and LSTM cannot be directly
used for the interpolation task, we focus on evaluating generative models and imputation methods.",4.3. Quantitative Results,[0],[0]
"From Table 4, we observe that our proposed model outperforms the baselines and the competing multi-rate latent space models by a large margin on all the interpolation tasks on these two datasets.
",4.3. Quantitative Results,[0],[0]
Inference We also compare the lower bound of loglikelihood of all generative models in Table 5.,4.3. Quantitative Results,[0],[0]
"The higher
lower bound value indicates a better fitted model given the training data.",4.3. Quantitative Results,[0],[0]
Our MR-HDMM model achieves the best performance on both datasets.,4.3. Quantitative Results,[0],[0]
"In all our experiments, MR-HDMM outperforms other generative models by a significant margin.",4.4. Discussion,[0],[0]
"Considering that all the deep generative models have the same amount of parameters, this improvement empirically demonstrates the effectiveness of our proposed learnable latent hierarchical structure and auxiliary connections.",4.4. Discussion,[0],[0]
"In Figure 3(a) and 3(b), we visualize the latent hierarchical structure of MR-HDMM learned from the first 48 hours of an admission in MIMICIII dataset and one-year climate observations in USHCN dataset.",4.4. Discussion,[0],[0]
"A color block indicates that the latent state zlt is updated from zlt−1 and z l−1 t (update), while the white block indicates zlt is generated from the same distribution of z l t−1 (reuse).",4.4. Discussion,[0],[0]
"As expected, the higher latent layers tend to update less frequently and capture the long-term temporal dependencies.",4.4. Discussion,[0],[0]
"To understand learned hierarchical structure more intuitively, we also show precipitation time series from USCHN dataset along with learned switches in Figure 3(b).",4.4. Discussion,[0],[0]
"We observe that the higher latent layer tends to update along with the precipitation, which is reasonable since precipitation makes significant changes to the underlying weather condition which is captured by the higher latent layer.",4.4. Discussion,[0],[0]
We proposed the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM) - a novel deep generative model for forecasting and interpolation tasks on multi-rate multivariate time series (MR-MTS) data.,5. Summary,[0],[0]
MR-HDMM models the data generation process by learning a latent hierarchical structure using auxiliary connections and learnable switches to capture the temporal dependencies.,5. Summary,[0],[0]
Empirically we showed that our proposed model outperforms the existing single-rate and multi-rate models on healthcare and climate datasets.,5. Summary,[0],[0]
"This work is supported in part by NSF Research Grant IIS-1254206 and IIS-1539608, and MURI grant W911NF-11-1-0332.",Acknowledgments,[0],[0]
"The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency, or the U.S. Government.",Acknowledgments,[0],[0]
Multi-Rate Multivariate Time Series (MR-MTS) are the multivariate time series observations which come with various sampling rates and encode multiple temporal dependencies.,abstractText,[0],[0]
State-space models such as Kalman filters and deep learning models such as deep Markov models are mainly designed for time series data with the same sampling rate and cannot capture all the dependencies present in the MR-MTS data.,abstractText,[0],[0]
"To address this challenge, we propose the Multi-Rate Hierarchical Deep Markov Model (MR-HDMM), a novel deep generative model which uses the latent hierarchical structure with a learnable switch mechanism to capture the temporal dependencies of MR-MTS.",abstractText,[0],[0]
Experimental results on two real-world datasets demonstrate that our MR-HDMM model outperforms the existing state-of-the-art deep learning and state-space models on forecasting and interpolation tasks.,abstractText,[0],[0]
"In addition, the latent hierarchies in our model provide a way to show and interpret the multiple temporal dependencies.",abstractText,[0],[0]
Hierarchical Deep Generative Models for Multi-Rate Multivariate Time Series,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 97–109 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
97",text,[0],[0]
"Identifying and understanding entities is a central component in knowledge base construction (Roth et al., 2015) and essential for enhancing downstream tasks such as relation extraction
*equal contribution Data and code for experiments: https://github.
com/MurtyShikhar/Hierarchical-Typing
(Yaghoobzadeh et al., 2017b), question answering (Das et al., 2017; Welbl et al., 2017) and search (Dalton et al., 2014).",1 Introduction,[0.9951821256205742],"['Identifying and understanding entities is a central component in knowledge base construction (Roth et al., 2015) and essential for enhancing downstream tasks such as relation extraction com/MurtyShikhar/Hierarchical-Typing (Yaghoobzadeh et al., 2017b), question answering (Das et al., 2017; Welbl et al., 2017) and search (Dalton et al., 2014).']"
"This has led to considerable research in automatically identifying entities in text, predicting their types, and linking them to existing structured knowledge sources.
",1 Introduction,[0],[0]
Current state-of-the-art models encode a textual mention with a neural network and classify the mention as being an instance of a fine grained type or entity in a knowledge base.,1 Introduction,[0],[0]
"Although in many cases the types and their entities are arranged in a hierarchical ontology, most approaches ignore this structure, and previous attempts to incorporate hierarchical information yielded little improvement in performance (Shimaoka et al., 2017).",1 Introduction,[0],[0]
"Additionally, existing benchmark entity typing datasets only consider small label sets arranged in very shallow hierarchies.",1 Introduction,[0],[0]
"For example, FIGER (Ling and Weld, 2012), the de facto standard fine grained entity type dataset, contains only 113 types in a hierarchy only two levels deep.
",1 Introduction,[0.9999999725134359],"['For example, FIGER (Ling and Weld, 2012), the de facto standard fine grained entity type dataset, contains only 113 types in a hierarchy only two levels deep.']"
"In this paper we investigate models that explicitly integrate hierarchical information into the embedding space of entities and types, using a hierarchy-aware loss on top of a deep neural network classifier over textual mentions.",1 Introduction,[0],[0]
"By using this additional information, we learn a richer, more robust representation, gaining statistical efficiency when predicting similar concepts and aiding the classification of rarer types.",1 Introduction,[1.0],"['By using this additional information, we learn a richer, more robust representation, gaining statistical efficiency when predicting similar concepts and aiding the classification of rarer types.']"
"We first validate our methods on the narrow, shallow type system of FIGER, out-performing state-of-the-art methods not incorporating hand-crafted features and matching those that do.
To evaluate on richer datasets and stimulate further research into hierarchical entity/typing prediction with larger and deeper ontologies, we introduce two new human annotated datasets.",1 Introduction,[0],[0]
"The first is MedMentions, a collection of PubMed ab-
stracts in which 246k concept mentions have been annotated with links to the Unified Medical Language System (UMLS) ontology (Bodenreider, 2004), an order of magnitude more annotations than comparable datasets.",1 Introduction,[0],[0]
UMLS contains over 3.5 million concepts in a hierarchy having average depth 14.4.,1 Introduction,[0],[0]
"Interestingly, UMLS does not distinguish between types and entities (an approach we heartily endorse), and the technical details of linking to such a massive ontology lead us to refer to our MedMentions experiments as entity linking.",1 Introduction,[0],[0]
"Second, we present TypeNet, a curated mapping from the Freebase type system into the WordNet hierarchy.",1 Introduction,[0],[0]
"TypeNet contains over 1900 types with an average depth of 7.8.
",1 Introduction,[0],[0]
"In experimental results, we show improvements with a hierarchically-aware training loss on each of the three datasets.",1 Introduction,[0],[0]
"In entity-linking MedMentions to UMLS, we observe a 6% relative increase in accuracy over the base model.",1 Introduction,[0],[0]
"In experiments on entity-typing from Wikipedia into TypeNet, we show that incorporating the hierarchy of types and including a hierarchical loss provides a dramatic 29% relative increase in MAP.",1 Introduction,[0],[0]
"Our models even provide benefits for shallow hierarchies allowing us to match the state-of-art results of Shimaoka et al. (2017) on the FIGER (GOLD) dataset without requiring hand-crafted features.
",1 Introduction,[0.999999986125929],['Our models even provide benefits for shallow hierarchies allowing us to match the state-of-art results of Shimaoka et al. (2017) on the FIGER (GOLD) dataset without requiring hand-crafted features.']
"We will publicly release the TypeNet and MedMentions datasets to the community to encourage further research in truly fine-grained, hierarchical entity-typing and linking.",1 Introduction,[0],[0]
"Over the years researchers have constructed many large knowledge bases in the biomedical domain (Apweiler et al., 2004; Davis et al., 2008; Chatraryamontri et al., 2017).",2.1 MedMentions,[0],[0]
"Many of these knowledge bases are specific to a particular sub-domain encompassing a few particular types such as genes and diseases (Piñero et al., 2017).
",2.1 MedMentions,[0],[0]
"UMLS (Bodenreider, 2004) is particularly comprehensive, containing over 3.5 million concepts (UMLS does not distinguish between entities and types) defining their relationships and a curated hierarchical ontology.",2.1 MedMentions,[0],[0]
For example LETM1 Protein IS-A Calcium Binding Protein IS-A Binding Protein IS-A Protein IS-A Genome Encoded Entity.,2.1 MedMentions,[0],[0]
"This fact makes UMLS particularly well suited for methods explicitly exploiting hierarchical struc-
ture.",2.1 MedMentions,[0],[0]
Accurately linking textual biological entity mentions to an existing knowledge base is extremely important but few richly annotated resources are available.,2.1 MedMentions,[0],[0]
"Even when resources do exist, they often contain no more than a few thousand annotated entity mentions which is insufficient for training state-of-the-art neural network entity linkers.",2.1 MedMentions,[0],[0]
"State-of-the-art methods must instead rely on string matching between entity mentions and canonical entity names (Leaman et al., 2013; Wei et al., 2015; Leaman and Lu, 2016).",2.1 MedMentions,[0],[0]
"To address this, we constructed MedMentions, a new, large dataset identifying and linking entity mentions in PubMed abstracts to specific UMLS concepts.",2.1 MedMentions,[0],[0]
"Professional annotators exhaustively annotated UMLS entity mentions from 3704 PubMed abstracts, resulting in 246,000 linked mention spans.",2.1 MedMentions,[0],[0]
"The average depth in the hierarchy of a concept from our annotated set is 14.4 and the maximum depth is 43.
",2.1 MedMentions,[0],[0]
"MedMentions contains an order of magnitude more annotations than similar biological entity linking PubMed datasets (Doğan et al., 2014; Wei et al., 2015; Li et al., 2016).",2.1 MedMentions,[0],[0]
"Additionally, these datasets contain annotations for only one or two entity types (genes or chemicals and disease etc.).",2.1 MedMentions,[0],[0]
MedMentions instead contains annotations for a wide diversity of entities linking to UMLS.,2.1 MedMentions,[0],[0]
Statistics for several other datasets are in Table 1 and further statistics are in 2.,2.1 MedMentions,[0],[0]
TypeNet is a new dataset of hierarchical entity types for extremely fine-grained entity typing.,2.2 TypeNet,[0],[0]
"TypeNet was created by manually aligning Freebase types (Bollacker et al., 2008) to noun synsets from the WordNet hierarchy (Fellbaum, 1998), naturally producing a hierarchical type set.
",2.2 TypeNet,[1.0000000457872067],"['TypeNet was created by manually aligning Freebase types (Bollacker et al., 2008) to noun synsets from the WordNet hierarchy (Fellbaum, 1998), naturally producing a hierarchical type set.']"
"To construct TypeNet, we first consider all Freebase types that were linked to more than 20 entities.",2.2 TypeNet,[0],[0]
This is done to eliminate types that are either very specific or very rare.,2.2 TypeNet,[1.0],['This is done to eliminate types that are either very specific or very rare.']
"We also remove all Freebase API types, e.g. the [/freebase, /dataworld, /schema, /atom, /scheme, and /topics] domains.
",2.2 TypeNet,[0],[0]
"For each remaining Freebase type, we generate a list of candidate WordNet synsets through a substring match.",2.2 TypeNet,[0],[0]
"An expert annotator then attempted to map the Freebase type to one or more synsets in the candidate list with a parent-of, child-of or equivalence link by comparing the definitions of each synset with example entities of the Freebase type.",2.2 TypeNet,[0],[0]
"If no match was found, the annotator manually formulated queries for the online WordNet API until an appropriate synset was found.",2.2 TypeNet,[1.0],"['If no match was found, the annotator manually formulated queries for the online WordNet API until an appropriate synset was found.']"
"See Table 9 for an example annotation.
",2.2 TypeNet,[0],[0]
Two expert annotators independently aligned each Freebase type before meeting to resolve any conflicts.,2.2 TypeNet,[0],[0]
The annotators were conservative with assigning equivalence links resulting in a greater number of child-of links.,2.2 TypeNet,[0],[0]
"The final dataset contained 13 parent-of, 727 child-of, and 380 equivalence links.",2.2 TypeNet,[0],[0]
"Note that some Freebase types have multiple child-of links to WordNet, making TypeNet, like WordNet, a directed acyclic graph.",2.2 TypeNet,[0],[0]
"We then took the union of each of our annotated Freebase types, the synset that they linked to, and any ancestors of that synset.
",2.2 TypeNet,[0],[0]
We also added an additional set of 614 FB → FB links 4.,2.2 TypeNet,[0],[0]
This was done by computing conditional probabilities of Freebase types given other Freebase types from a collection of 5 million randomly chosen Freebase entities.,2.2 TypeNet,[0],[0]
"The conditional probability P(t2 | t1) of a Freebase type t2 given another Freebase type t1 was calculated as #(t1,t2)#t1 .",2.2 TypeNet,[1.0],"['The conditional probability P(t2 | t1) of a Freebase type t2 given another Freebase type t1 was calculated as #(t1,t2)#t1 .']"
Links with a conditional probability less than or equal to 0.7 were discarded.,2.2 TypeNet,[0],[0]
"The remaining links were manually verified by an expert annotator and valid links were added to the final dataset, preserving acyclicity.",2.2 TypeNet,[0],[0]
We define a textual mention m as a sentence with an identified entity.,3.1 Background: Entity Typing and Linking,[0],[0]
The goal is then to classify m with one or more labels.,3.1 Background: Entity Typing and Linking,[0],[0]
"For example, we could take the sentence m = “Barack Obama is the President of the United States.”",3.1 Background: Entity Typing and Linking,[0],[0]
with the identified entity string Barack Obama.,3.1 Background: Entity Typing and Linking,[0],[0]
"In the task of entity linking, we want to map m to a specific entity in a knowledge base such as “m/02mjmr” in Freebase.",3.1 Background: Entity Typing and Linking,[0],[0]
"In mention-level typing, we label m with one or more types from our type system T such as tm = {president, leader, politician} (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017).",3.1 Background: Entity Typing and Linking,[0],[0]
"In entity-level typing, we instead consider a bag of mentions Be which are all linked to the same entity.",3.1 Background: Entity Typing and Linking,[0],[0]
"We label Be with te, the set of all types expressed in all m ∈ Be (Yao et al., 2013; Neelakantan and Chang, 2015; Verga et al., 2017; Yaghoobzadeh et al., 2017a).",3.1 Background: Entity Typing and Linking,[0],[0]
Our model converts each mention m to a d dimensional vector.,3.2 Mention Encoder,[0],[0]
This vector is used to classify the type or entity of the mention.,3.2 Mention Encoder,[0],[0]
The basic model depicted in Figure 1 concatenates the averaged word embeddings of the mention string with the output of a convolutional neural network (CNN).,3.2 Mention Encoder,[0],[0]
"The
word embeddings of the mention string capture global, context independent semantics while the CNN encodes a context dependent representation.",3.2 Mention Encoder,[0],[0]
Each sentence is made up of s tokens which are mapped to dw dimensional word embeddings.,3.2.1 Token Representation,[1.0],['Each sentence is made up of s tokens which are mapped to dw dimensional word embeddings.']
"Because sentences may contain mentions of more than one entity, we explicitly encode a distinguished mention in the text using position embeddings which have been shown to be useful in state of the art relation extraction models (dos Santos et al., 2015; Lin et al., 2016) and machine translation (Vaswani et al., 2017).",3.2.1 Token Representation,[0],[0]
Each word embedding is concatenated with a dp dimensional learned position embedding encoding the token’s relative distance to the target entity.,3.2.1 Token Representation,[0],[0]
"Each token within the distinguished mention span has position 0, tokens to the left have a negative distance from [−s, 0), and tokens to the right of the mention span have a positive distance from (0, s].",3.2.1 Token Representation,[0],[0]
We denote the final sequence of token representations as M .,3.2.1 Token Representation,[0],[0]
The embedded sequence M is then fed into our context encoder.,3.2.2 Sentence Representation,[0],[0]
"Our context encoder is a single layer CNN followed by a tanh non-linearity to produce C. The outputs are max pooled across
time to get a final context embedding, mCNN.
",3.2.2 Sentence Representation,[0],[0]
"ci = tanh(b+ w∑
j=0
W [j]M [i− bw 2 c+ j])
mCNN = max 0≤i≤n−w+1",3.2.2 Sentence Representation,[0],[0]
"ci
Each W [j] ∈",3.2.2 Sentence Representation,[0],[0]
"Rd×d is a CNN filter, the bias b ∈ Rd, M [i] ∈",3.2.2 Sentence Representation,[0],[0]
"Rd is a token representation, and the max is taken pointwise.",3.2.2 Sentence Representation,[0],[0]
"In all of our experiments we set w = 5.
",3.2.2 Sentence Representation,[0],[0]
"In addition to the contextually encoded mention, we create a global mention encoding, mG, by averaging the word embeddings of the tokens within the mention span.
",3.2.2 Sentence Representation,[0],[0]
"The final mention representation mF is constructed by concatenating mCNN and mG and applying a two layer feed-forward network with tanh non-linearity (see Figure 1):
mF = W2 tanh(W1",3.2.2 Sentence Representation,[0],[0]
[ mSFM mCNN ] + b1) + b2,3.2.2 Sentence Representation,[0],[0]
Mention level entity typing is treated as multilabel prediction.,4.1 Mention-Level Typing,[0],[0]
"Given the sentence vector mF, we compute a score for each type in typeset T as:
yj = tj >mF
where tj is the embedding for the jth type in T and yj is its corresponding score.",4.1 Mention-Level Typing,[0],[0]
"The mention is labeled with tm, a binary vector of all types where tmj = 1 if the j
th type is in the set of gold types for m and 0 otherwise.",4.1 Mention-Level Typing,[0],[0]
"We optimize a multi-label binary cross entropy objective:
Ltype(m) =",4.1 Mention-Level Typing,[0],[0]
− ∑ j tmj log yj + (1− tmj ) log(1− yj),4.1 Mention-Level Typing,[0],[0]
"In the absence of mention-level annotations, we instead must rely on distant supervision (Mintz et al., 2009) to noisily label all mentions of entity e with all types belonging to e. This procedure inevitably leads to noise as not all mentions of an entity express each of its known types.",4.2 Entity-Level Typing,[0],[0]
"To alleviate this noise, we use multi-instance multi-label learning (MIML) (Surdeanu et al., 2012) which operates over bags rather than mentions.",4.2 Entity-Level Typing,[0],[0]
"A bag of mentions Be = {m1,m2, . . .",4.2 Entity-Level Typing,[0],[0]
",mn} is the set of
all mentions belonging to entity e.",4.2 Entity-Level Typing,[0],[0]
"The bag is labeled with te, a binary vector of all types where tej = 1 if the j
th type is in the set of gold types for e and 0 otherwise.
",4.2 Entity-Level Typing,[0],[0]
"For every entity, we subsample k mentions from its bag of mentions.",4.2 Entity-Level Typing,[0],[0]
Each mention is then encoded independently using the model described in Section 3.2 resulting in a bag of vectors.,4.2 Entity-Level Typing,[0],[0]
"Each of the k sentence vectors miF is used to compute a score for each type in te:
yij = tj >miF
where tj is the embedding for the jth type in te and yi is a vector of logits corresponding to the ith mention.",4.2 Entity-Level Typing,[0],[0]
"The final bag predictions are obtained using element-wise LogSumExp pooling across the k logit vectors in the bag to produce entity level logits y:
y = log ∑ i exp(yi)
",4.2 Entity-Level Typing,[0],[0]
"We use these final bag level predictions to optimize a multi-label binary cross entropy objective:
Ltype(Be) =",4.2 Entity-Level Typing,[0],[0]
− ∑ j tej log yj + (1− tej) log(1− yj),4.2 Entity-Level Typing,[0],[0]
Entity linking is similar to mention-level entity typing with a single correct class per mention.,4.3 Entity Linking,[0],[0]
"Because the set of possible entities is in the millions, linking models typically integrate an alias table mapping entity mentions to a set of possible candidate entities.",4.3 Entity Linking,[0],[0]
"Given a large corpus of entity linked data, one can compute conditional probabilities from mention strings to entities (Spitkovsky and Chang, 2012).",4.3 Entity Linking,[1.0],"['Given a large corpus of entity linked data, one can compute conditional probabilities from mention strings to entities (Spitkovsky and Chang, 2012).']"
In many scenarios this data is unavailable.,4.3 Entity Linking,[0],[0]
"However, knowledge bases such as UMLS contain a canonical string name for each of its curated entities.",4.3 Entity Linking,[0],[0]
"State-of-the-art biological entity linking systems tend to operate on various string edit metrics between the entity mention string and the set of canonical entity strings in the existing structured knowledge base (Leaman et al., 2013; Wei et al., 2015).
",4.3 Entity Linking,[0],[0]
"For each mention in our dataset, we generate 100 candidate entities ec = (e1, e2, . . .",4.3 Entity Linking,[0],[0]
", e100) each with an associated string similarity score csim.",4.3 Entity Linking,[0],[0]
See Appendix A.5.1 for more details on candidate generation.,4.3 Entity Linking,[0],[0]
"We generate the sentence representation mF using our encoder and compute a similarity score between mF and the learned embedding
e of each of the candidate entities.",4.3 Entity Linking,[0],[0]
This score and string cosine similarity csim are combined via a learned linear combination to generate our final score.,4.3 Entity Linking,[0],[0]
"The final prediction at test time ê is the maximally similar entity to the mention.
",4.3 Entity Linking,[0],[0]
"φ(m, e)",4.3 Entity Linking,[0],[0]
= α e>mF + β,4.3 Entity Linking,[0],[0]
"csim(m, e)
ê = argmax e∈ec φ(m, e)
We optimize this model by multinomial cross entropy over the set of candidate entities and correct entity e.
Llink(m, ec) =",4.3 Entity Linking,[0],[0]
"− φ(m, e) + log ∑ e′∈ec expφ(m, e′)",4.3 Entity Linking,[0],[0]
Both entity typing and entity linking treat the label space as prediction into a flat set.,5 Encoding Hierarchies,[0],[0]
"To explicitly incorporate the structure between types/entities into our training, we add an additional loss.",5 Encoding Hierarchies,[0],[0]
"We consider two methods for modeling the hierarchy of the embedding space: real and complex bilinear maps, which are two of the state-of-the-art knowledge graph embedding models.",5 Encoding Hierarchies,[0],[0]
Bilinear:,5.1 Hierarchical Structure Models,[0],[0]
"Our standard bilinear model scores a hypernym link between (c1, c2) as:
s(c1, c2) = c1 >Ac2
where A ∈ Rd×d is a learned real-valued nondiagonal matrix and c1 is the child of c2 in the hierarchy.",5.1 Hierarchical Structure Models,[0],[0]
"This model is equivalent to RESCAL (Nickel et al., 2011) with a single IS-A relation type.",5.1 Hierarchical Structure Models,[1.0],"['This model is equivalent to RESCAL (Nickel et al., 2011) with a single IS-A relation type.']"
The type embeddings are the same whether used on the left or right side of the relation.,5.1 Hierarchical Structure Models,[0],[0]
We merge this with the base model by using the parameter A as an additional map before type/entity scoring.,5.1 Hierarchical Structure Models,[0],[0]
"Complex Bilinear: We also experiment with a complex bilinear map based on the ComplEx model (Trouillon et al., 2016), which was shown to have strong performance predicting the hypernym relation in WordNet, suggesting suitability for asymmetric, transitive relations such as those in our type hierarchy.",5.1 Hierarchical Structure Models,[0],[0]
"ComplEx uses complex valued vectors for types, and diagonal complex matrices for relations, using Hermitian inner products (taking the complex conjugate of the second argument, equivalent to treating the right-hand-side
type embedding to be the complex conjugate of the left hand side), and finally taking the real part of the score1.",5.1 Hierarchical Structure Models,[0],[0]
"The score of a hypernym link between (c1, c2) in the ComplEx model is defined as:
s(c1, c2) =",5.1 Hierarchical Structure Models,[0],[0]
"Re(< c1, rIS-A, c2 >) = Re( ∑ k c1krk c̄2k)
= 〈Re(c1),Re(rIS-A),Re(c2)〉 + 〈Re(c1), Im(rIS-A), Im(c2)〉 + 〈Im(c1),Re(rIS-A), Im(c2)〉 − 〈Im(c1), Im(rIS-A),Re(c2)〉
where c1, c2 and rIS-A are complex valued vectors representing c1, c2 and the IS-A relation respectively.",5.1 Hierarchical Structure Models,[0],[0]
Re(z) represents the real component of z and Im(z) is the imaginary component.,5.1 Hierarchical Structure Models,[0],[0]
"As noted in Trouillon et al. (2016), the above function is antisymmetric when rIS-A is purely imaginary.
",5.1 Hierarchical Structure Models,[0],[0]
"Since entity/type embeddings are complex vectors, in order to combine it with our base model, we also need to represent mentions with complex vectors for scoring.",5.1 Hierarchical Structure Models,[0],[0]
"To do this, we pass the output of the mention encoder through two different affine transformations to generate a real and imaginary component:
Re(mF) = WrealmF + breal Im(mF)",5.1 Hierarchical Structure Models,[0],[0]
"= WimgmF + bimg
where mF is the output of the mention encoder, and Wreal, Wimg ∈ Rd×d and breal, bimg ∈ Rd .",5.1 Hierarchical Structure Models,[0],[0]
Learning a hierarchy is analogous to learning embeddings for nodes of a knowledge graph with a single hypernym/IS-A relation.,5.2 Training with Hierarchies,[0],[0]
"To train these embeddings, we sample (c1, c2) pairs, where each pair is a positive link in our hierarchy.",5.2 Training with Hierarchies,[0],[0]
"For each positive link, we sample a set N of n negative links.",5.2 Training with Hierarchies,[0],[0]
"We encourage the model to output high scores for positive links, and low scores for negative links via a binary cross entropy (BCE) loss:
Lstruct = − log σ(s(c1i, c2i))",5.2 Training with Hierarchies,[0],[0]
+,5.2 Training with Hierarchies,[0],[0]
"∑ N log(1− σ(s(c1i, c′2i)))
",5.2 Training with Hierarchies,[0],[0]
L = Ltype/link + γLstruct,5.2 Training with Hierarchies,[0],[0]
"1This step makes the scoring function technically not bilinear, as it commutes with addition but not complex multiplication, but we term it bilinear for ease of exposition.
where s(c1, c2) is the score of a link (c1, c2), and σ(·) is the logistic sigmoid.",5.2 Training with Hierarchies,[0],[0]
"The weighting parameter γ is ∈ {0.1, 0.5, 0.8, 1, 2.0, 4.0}.",5.2 Training with Hierarchies,[0],[0]
The final loss function that we optimize is L.,5.2 Training with Hierarchies,[0],[0]
"We perform three sets of experiments: mentionlevel entity typing on the benchmark dataset FIGER, entity-level typing using Wikipedia and TypeNet, and entity linking using MedMentions.",6 Experiments,[1.0],"['We perform three sets of experiments: mentionlevel entity typing on the benchmark dataset FIGER, entity-level typing using Wikipedia and TypeNet, and entity linking using MedMentions.']"
CNN:,6.1 Models,[0],[0]
Each mention is encoded using the model described in Section 3.2.,6.1 Models,[0],[0]
The resulting embedding is used for classification into a flat set labels.,6.1 Models,[0],[0]
Specific implementation details can be found in Appendix A.2.,6.1 Models,[0],[0]
CNN+Complex:,6.1 Models,[0],[0]
The CNN+Complex model is equivalent to the CNN model but uses complex embeddings and Hermitian dot products.,6.1 Models,[0],[0]
Transitive:,6.1 Models,[0],[0]
This model does not add an additional hierarchical loss to the training objective (unless otherwise stated).,6.1 Models,[0],[0]
"We add additional labels to each entity corresponding to the transitive closure, or the union of all ancestors of its known types.",6.1 Models,[0],[0]
This provides a rich additional learning signal that greatly improves classification of specific types.,6.1 Models,[0],[0]
Hierarchy:,6.1 Models,[0],[0]
"These models add an explicit hierarchical loss to the training objective, as described in Section 5, using either complex or real-valued bilinear mappings, and the associated parameter sharing.",6.1 Models,[0],[0]
To evaluate the efficacy of our methods we first compare against the current state-of-art models of Shimaoka et al. (2017).,6.2 Mention-Level Typing in FIGER,[0],[0]
The most widely used type system for fine-grained entity typing is FIGER which consists of 113 types organized in a 2 level hierarchy.,6.2 Mention-Level Typing in FIGER,[1.0],['The most widely used type system for fine-grained entity typing is FIGER which consists of 113 types organized in a 2 level hierarchy.']
"For training, we use the publicly available W2M data (Ren et al., 2016) and optimize the mention typing loss function defined in Section4.1 with the additional hierarchical loss where specified.",6.2 Mention-Level Typing in FIGER,[0],[0]
"For evaluation, we use the manually annotated FIGER (GOLD) data by Ling and Weld (2012).",6.2 Mention-Level Typing in FIGER,[0],[0]
See Appendix A.2 and A.3 for specific implementation details.,6.2 Mention-Level Typing in FIGER,[0],[0]
"In Table 5 we see that our base CNN models (CNN and CNN+Complex) match LSTM models of Shimaoka et al. (2017) and Gupta et al. (2017), the
previous state-of-the-art for models without handcrafted features.",6.2.1 Results,[1.0000001001142027],"['In Table 5 we see that our base CNN models (CNN and CNN+Complex) match LSTM models of Shimaoka et al. (2017) and Gupta et al. (2017), the previous state-of-the-art for models without handcrafted features.']"
"When incorporating structure into our models, we gain 2.5 points of accuracy in our CNN+Complex model, matching the overall state of the art attentive LSTM that relied on handcrafted features from syntactic parses, topic models, and character n-grams.",6.2.1 Results,[1.0],"['When incorporating structure into our models, we gain 2.5 points of accuracy in our CNN+Complex model, matching the overall state of the art attentive LSTM that relied on handcrafted features from syntactic parses, topic models, and character n-grams.']"
The structure can help our model predict lower frequency types which is a similar role played by hand-crafted features.,6.2.1 Results,[0],[0]
Next we evaluate our models on entity-level typing in TypeNet using Wikipedia.,6.3 Entity-Level Typing in TypeNet,[0],[0]
"For each entity, we follow the procedure outlined in Section 4.2.",6.3 Entity-Level Typing in TypeNet,[0],[0]
We predict labels for each instance in the entity’s bag and aggregate them into entity-level predictions using LogSumExp pooling.,6.3 Entity-Level Typing in TypeNet,[0],[0]
Each type is assigned a predicted score by the model.,6.3 Entity-Level Typing in TypeNet,[0],[0]
"We then rank these scores and calculate average precision for each of the types in the test set, and use these scores to calculate mean average precision (MAP).",6.3 Entity-Level Typing in TypeNet,[0],[0]
"We evaluate using MAP instead of accuracy which is standard in large knowledge base link prediction tasks (Verga et al., 2017; Trouillon et al., 2016).",6.3 Entity-Level Typing in TypeNet,[0],[0]
"These scores are calculated only over Freebase types, which tend to be lower in the hierarchy.",6.3 Entity-Level Typing in TypeNet,[0],[0]
This is to avoid artificial score inflation caused by trivial predictions such as ‘entity.’,6.3 Entity-Level Typing in TypeNet,[0],[0]
See Appendix A.4 for more implementation details.,6.3 Entity-Level Typing in TypeNet,[0],[0]
Table 6 shows the results for entity level typing on our Wikipedia TypeNet dataset.,6.3.1 Results,[1.0],['Table 6 shows the results for entity level typing on our Wikipedia TypeNet dataset.']
We see that both the basic CNN and the CNN+Complex models perform similarly with the CNN+Complex model doing slightly better on the full data regime.,6.3.1 Results,[0],[0]
"We also see that both models get an improvement when adding an explicit hierarchy loss, even before adding in the transitive closure.",6.3.1 Results,[0],[0]
"The transitive closure itself gives an additional increase
in performance to both models.",6.3.1 Results,[0],[0]
"In both of these cases, the basic CNN model improves by a greater amount than CNN+Complex.",6.3.1 Results,[0],[0]
This could be a result of the complex embeddings being more difficult to optimize and therefore more susceptible to variations in hyperparameters.,6.3.1 Results,[0],[0]
"When adding in both the transitive closure and the explicit hierarchy loss, the performance improves further.",6.3.1 Results,[0],[0]
"We observe similar trends when training our models in a lower data regime with ~150,000 examples, or about 5% of the total data.
",6.3.1 Results,[0],[0]
"In all cases, we note that the baseline models that do not incorporate any hierarchical information (neither the transitive closure nor the hierarchy loss) perform ~9 MAP worse, demonstrating the benefits of incorporating structure information.",6.3.1 Results,[0],[0]
"In addition to entity typing, we evaluate our model’s performance on an entity linking task using MedMentions, our new PubMed / UMLS dataset described in Section 2.1.",6.4 MedMentions Entity Linking with UMLS,[0],[0]
Table 7 shows results for baselines and our proposed variant with additional hierarchical loss.,6.4.1 Results,[0],[0]
"None of these models incorporate transitive clo-
sure information, due to difficulty incorporating it in our candidate generation, which we leave to future work.",6.4.1 Results,[0],[0]
The Normalized metric considers performance only on mentions with an alias table hit; all models have 0 accuracy for mentions otherwise.,6.4.1 Results,[0],[0]
We also report the overall score for comparison in future work with improved candidate generation.,6.4.1 Results,[0],[0]
"We see that incorporating structure information results in a 1.1% reduction in absolute error, corresponding to a ~6% reduction in relative error on this large-scale dataset.
",6.4.1 Results,[0.9999999301043502],"['We see that incorporating structure information results in a 1.1% reduction in absolute error, corresponding to a ~6% reduction in relative error on this large-scale dataset.']"
Table 8 shows qualitative predictions for models with and without hierarchy information incorporated.,6.4.1 Results,[1.0],['Table 8 shows qualitative predictions for models with and without hierarchy information incorporated.']
"Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity.",6.4.1 Results,[1.0],"['Each example contains the sentence (with target entity in bold), predictions for the baseline and hierarchy aware models, and the ancestors of the predicted entity.']"
"In the first and second example, the baseline model becomes extremely dependent on TFIDF string similarities when the gold candidate is rare (≤ 10 occurrences).",6.4.1 Results,[0],[0]
This shows that modeling the structure of the entity hierarchy helps the model disambiguate rare entities.,6.4.1 Results,[0],[0]
"In the third example, structure helps the model understand the hierarchical nature of the labels and prevents it from predicting an entity that is overly specific (e.g predicting Interleukin-27 rather than the correct and more general entity IL2 Gene).
",6.4.1 Results,[0],[0]
"Note that, in contrast with the previous tasks, the complex hierarchical loss provides a significant boost, while the real-valued bilinear model does not.",6.4.1 Results,[0],[0]
"A possible explanation is that UMLS is a far larger/deeper ontology than even TypeNet, and the additional ability of complex embeddings to model intricate graph structure is key to realizing gains from hierarchical modeling.",6.4.1 Results,[1.0],"['A possible explanation is that UMLS is a far larger/deeper ontology than even TypeNet, and the additional ability of complex embeddings to model intricate graph structure is key to realizing gains from hierarchical modeling.']"
"By directly linking a large set of mentions and typing a large set of entities with respect to a new ontology and corpus, and our incorporation of structural learning between the many entities and types in our ontologies of interest, our work draws on many different but complementary threads of research in information extraction, knowledge base population, and completion.
",7 Related Work,[0],[0]
"Our structural, hierarchy-aware loss between types and entities draws on research in Knowledge Base Inference such as Jain et al. (2018), Trouillon et al. (2016) and Nickel et al. (2011).",7 Related Work,[0],[0]
"Combining KB completion with hierarchical structure in knowledge bases has been explored in (Dalvi et al., 2015; Xie et al., 2016).",7 Related Work,[0],[0]
"Recently, Wu et al. (2017) proposed a hierarchical loss for text classification.
",7 Related Work,[0],[0]
"Linking mentions to a flat set of entities, often in Freebase or Wikipedia, is a long-standing task in NLP (Bunescu and Pasca, 2006; Cucerzan, 2007; Durrett and Klein, 2014; Francis-Landau et al., 2016).",7 Related Work,[0],[0]
"Typing of mentions at varying levels of granularity, from CoNLL-style named entity recognition (Tjong Kim Sang and De Meulder, 2003), to the more fine-grained recent approaches (Ling and Weld, 2012; Gillick et al., 2014; Shimaoka et al., 2017), is also related to our task.",7 Related Work,[0],[0]
"A few prior attempts to incorporate a very shallow hierarchy into fine-grained entity typing have not lead to significant or consistent improvements (Gillick et al., 2014; Shimaoka et al., 2017).
",7 Related Work,[0],[0]
"The knowledge base Yago (Suchanek et al., 2007) includes integration with WordNet and type hierarchies have been derived from its type system (Yosef et al., 2012).",7 Related Work,[0],[0]
"Del Corro et al. (2015) use manually crafted rules and patterns (Hearst patterns (Hearst, 1992), appositives, etc) to automati-
cally match entity types to Wordnet synsets.",7 Related Work,[0],[0]
"Recent work has moved towards unifying these two highly related tasks by improving entity linking by simultaneously learning a fine grained entity type predictor (Gupta et al., 2017).",7 Related Work,[0],[0]
"Learning hierarchical structures or transitive relations between concepts has been the subject of much recent work (Vilnis and McCallum, 2015; Vendrov et al., 2016; Nickel and Kiela, 2017)
",7 Related Work,[0],[0]
"We draw inspiration from all of this prior work, and contribute datasets and models to address previous challenges in jointly modeling the structure of large-scale hierarchical ontologies and mapping textual mentions into an extremely fine-grained space of entities and types.",7 Related Work,[0],[0]
We demonstrate that explicitly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets.,8 Conclusion,[1.0],['We demonstrate that explicitly incorporating and modeling hierarchical information leads to increased performance in experiments on entity typing and linking across three challenging datasets.']
"Additionally, we introduce two new humanannotated datasets: MedMentions, a corpus of 246k mentions from PubMed abstracts linked to the UMLS knowledge base, and TypeNet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets.
",8 Conclusion,[1.0000000406006984],"['Additionally, we introduce two new humanannotated datasets: MedMentions, a corpus of 246k mentions from PubMed abstracts linked to the UMLS knowledge base, and TypeNet, a new hierarchical fine-grained entity typeset an order of magnitude larger and deeper than previous datasets.']"
"While this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as Box embeddings (Vilnis et al., 2018) and Poincaré embeddings (Nickel and Kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking.",8 Conclusion,[1.0],"['While this work already demonstrates considerable improvement over non-hierarchical modeling, future work will explore techniques such as Box embeddings (Vilnis et al., 2018) and Poincaré embeddings (Nickel and Kiela, 2017) to represent the hierarchical embedding space, as well as methods to improve recall in the candidate generation process for entity linking.']"
"Most of all, we are excited to see new techniques from the NLP community using the resources we have presented.",8 Conclusion,[0],[0]
"We thank Nicholas Monath, Haw-Shiuan Chang and Emma Strubell for helpful comments on early drafts of the paper.",9 Acknowledgements,[0],[0]
Creation of the MedMentions corpus is supported and managed by the Meta team at the Chan Zuckerberg Initiative.,9 Acknowledgements,[0],[0]
A pre-release of the dataset is available at http://github.com/chanzuckerberg/ MedMentions.,9 Acknowledgements,[0],[0]
"This work was supported in part by the Center for Intelligent Information Retrieval and the Center for Data Science, in part by the Chan Zuckerberg Initiative under the project
Scientific Knowledge Base Construction., and in part by the National Science Foundation under Grant No. IIS-1514053.",9 Acknowledgements,[0],[0]
"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",9 Acknowledgements,[0],[0]
"A.1 TypeNet Construction
A.2 Model Implementation Details For all of our experiments, we use pretrained 300 dimensional word vectors from Pennington et al. (2014).",A Supplementary Materials,[0],[0]
These embeddings are fixed during training.,A Supplementary Materials,[0],[0]
"The type vectors and entity vectors are all 300 dimensional vectors initialized using Glorot initialization (Glorot and Bengio, 2010).",A Supplementary Materials,[0],[0]
"The number of negative links for hierarchical training n ∈ {16, 32, 64, 128, 256}.
",A Supplementary Materials,[0],[0]
"For regularization, we use dropout (Srivastava et al., 2014) with p ∈ {0.5, 0.75, 0.8} on the sentence encoder output and L2 regularize all learned parameters with λ ∈ {1e-5, 5e-5, 1e-4}.",A Supplementary Materials,[0],[0]
"All our parameters are optimized using Adam (Kingma and Ba, 2014) with a learning rate of 0.001.",A Supplementary Materials,[0],[0]
"We tune our hyper-parameters via grid search and early stopping on the development set.
",A Supplementary Materials,[0],[0]
"A.3 FIGER Implementation Details To train our models, we use the mention typing loss function defined in Section-5.",A Supplementary Materials,[0],[0]
"For models with structure training, we additionally add in the hierarchical loss, along with a weight that is obtained by tuning on the dev set.",A Supplementary Materials,[0],[0]
We follow the same inference time procedure as Shimaoka et al. (2017),A Supplementary Materials,[0],[0]
"For each mention, we first assign the type with the largest probability according to the logits, and then assign additional types based on the condition that their corresponding probability be greater than 0.5.
A.4 Wikipedia Data and Implementation Details
At train time, each training example randomly samples an entity bag of 10 mentions.",A Supplementary Materials,[0],[0]
At test time we classify bags of 20 mentions of an entity.,A Supplementary Materials,[0],[0]
"The dataset contains a total of 344,246 entities mapped to the 1081 Freebase types from TypeNet.",A Supplementary Materials,[0],[0]
We consider all sentences in Wikipedia between 10 and 50 tokens long.,A Supplementary Materials,[0],[0]
"Tokenization and sentence splitting was performed using NLTK (Loper and Bird, 2002).",A Supplementary Materials,[0],[0]
"From these sentences, we considered all entities annotated with a cross-link in Wikipedia that we could link to Freebase and assign types in TypeNet.",A Supplementary Materials,[1.0],"['From these sentences, we considered all entities annotated with a cross-link in Wikipedia that we could link to Freebase and assign types in TypeNet.']"
"We then split the data by entities into a 90-5-5 train, dev, test split.
",A Supplementary Materials,[0],[0]
A.5 UMLS,A Supplementary Materials,[0],[0]
Implementation details,A Supplementary Materials,[0],[0]
We pre,A Supplementary Materials,[0],[0]
-process each string by lowercasing and removing stop words.,A Supplementary Materials,[0],[0]
"We consider ngrams from size 1 to 5 and keep the top 100,000 features and the final vectors are L2 normalized.",A Supplementary Materials,[0],[0]
"For each mention, In our experiments we consider the top 100 most similar entities as the candidate set.
A.5.1 Candidate Generation Details Each mention and each canonical entity string in UMLS are mapped to TFIDF character ngram vectors.",A Supplementary Materials,[0],[0]
We pre-process each string by lowercasing and removing stop words.,A Supplementary Materials,[1.0],['We pre-process each string by lowercasing and removing stop words.']
"We consider ngrams from size 1 to 5 and keep the top 100,000 features and the final vectors are L2 normalized.",A Supplementary Materials,[0],[0]
"For each mention, we calculate the cosine similarity, csim, between the mention string and each canonical entity string.",A Supplementary Materials,[0],[0]
In our experiments we consider the top 100 most similar entities as the candidate set.,A Supplementary Materials,[1.0],['In our experiments we consider the top 100 most similar entities as the candidate set.']
"Extraction from raw text to a knowledge base of entities and fine-grained types is often cast as prediction into a flat set of entity and type labels, neglecting the rich hierarchies over types and entities contained in curated ontologies.",abstractText,[0],[0]
Previous attempts to incorporate hierarchical structure have yielded little benefit and are restricted to shallow ontologies.,abstractText,[0],[0]
"This paper presents new methods using real and complex bilinear mappings for integrating hierarchical information, yielding substantial improvement over flat predictions in entity linking and fine-grained entity typing, and achieving new state-of-the-art results for end-to-end models on the benchmark FIGER dataset.",abstractText,[0],[0]
"We also present two new human-annotated datasets containing wide and deep hierarchies which we will release to the community to encourage further research in this direction: MedMentions, a collection of PubMed abstracts in which 246k mentions have been mapped to the massive UMLS ontology; and TypeNet, which aligns Freebase types with the WordNet hierarchy to obtain nearly 2k entity types.",abstractText,[0],[0]
In experiments on all three datasets we show substantial gains from hierarchy-aware training.,abstractText,[0],[0]
Hierarchical Losses and New Resources for Fine-grained Entity Typing and Linking,title,[0],[0]
