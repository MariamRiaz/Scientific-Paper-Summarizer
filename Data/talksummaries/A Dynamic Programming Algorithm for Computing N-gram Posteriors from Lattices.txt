3	46	Standard ASR and SMT techniques like discriminative training, rescoring with complex models and Minimum Bayes-Risk (MBR) decoding rely on lattices to represent intermediate system hypotheses that will be further processed to improve models or system output.
5	46	Most lattice-based techniques employed by speech and NLP systems make use of posterior quantities computed from probabilistic lattices.
6	32	In this paper, we are interested in two such posterior quantities: i) n-gram expected count, the expected number of occurrences of a particular n-gram in a lattice, and ii) n-gram posterior probability, the total probability of accepting paths that include a particular n-gram.
7	98	Expected counts have applications in the estimation of language model statistics from probabilistic input such as ASR lattices (Allauzen et al., 2003) and the estimation term frequencies from spoken corpora while posterior probabilities come up in MBR decoding of SMT lattices (Tromble et al., 2008), relevance ranking of spoken utterances and the estimation of document frequencies from spoken corpora (Karakos et al., 2011; Can and Narayanan, 2013).
8	34	The expected count c(x|A) of n-gram x given lattice A is defined as c(x|A) = ∑ y∈Σ∗ #y(x)p(y|A) (1) where #y(x) is the number of occurrences of n- 2388 gram x in hypothesis y and p(y|A) is the posterior probability of hypothesis y given lattice A.
11	22	There are efficient algorithms in literature (Allauzen et al., 2003; Allauzen et al., 2004) for computing n-gram expected counts from weighted automata that rely on weighted finite state transducer operations to reduce the computation to a sum over n-gram occurrences eliminating the need for an explicit sum over accepting paths.
12	34	The rather innocent looking difference between Equations 1 and 2, #y(x) vs. 1y(x), makes it hard to develop similar algorithms for computing n-gram posteriors from weighted automata since the summation of probabilities has to be carried out over paths rather than n-gram occurrences (Blackwood et al., 2010; de Gispert et al., 2013).
16	34	Computation of document frequency statistics from spoken corpora relies on estimating ngram posteriors from ASR lattices.
26	21	The key idea behind our algorithm is to limit the costly posterior computation to only those ngrams that can potentially repeat on some path of the input lattice.
28	27	The posteriors for the remaining n-grams are replaced with expected counts.
31	20	Once that is done, we com- pute the expected counts for all n-grams in the input lattice and represent them as a minimal deterministic weighted finite-state automaton, known as a factor automaton (Allauzen et al., 2004; Mohri et al., 2007), using the approach described in (Allauzen et al., 2004).
32	37	Finally we use general weighted automata algorithms to merge the weighted factor automaton representing expected counts with the weighted prefix tree representing posteriors to obtain a weighted factor automaton representing posteriors that can be used for efficient storage and retrieval.
33	55	This section introduces the definitions and notation related to weighted finite state automata and transducers (Mohri, 2009).
66	46	The inner loop starting at line 6 is essentially a custom forward procedure computing not only the standard forward probabilities α[q], the marginal probability of paths that lead to state q, α[q] = ⊕ π ∈Π(I,q) λ(s[π])⊗ w[π] (3) = ⊕ e∈E t[e] = q α[s[e]]⊗ w[e] (4) but also the label specific forward probabilities α̃[q][x], the marginal probability of paths that lead to state q and include label x. α̃[q][x] = ⊕ π ∈Π(I,q) ∃u,v ∈Σ∗: i[π] =uxv λ(s[π])⊗ w[π] (5) = ⊕ e∈E t[e] = q i[e] =x α[s[e]]⊗ w[e] ⊕ ⊕ e∈E t[e] = q i[e] 6=x α̃[s[e]][x]⊗ w[e] (6) Just like in the case of the standard forward algorithm, visiting states in topological order ensures that forward probabilities associated with a state has already been computed when that state is visited.
67	63	At each state s, the algorithm examines each arc e = (s, x, w, q) and updates the forward probabilities for state q in accordance with the recursions in Equations 4 and 6 by propagating the forward probabilities computed for s (lines 8-12).
69	19	In other words, if a label y repeats on some path π leading to state q, then π contributes to α̃[q][y] only once.
70	43	This is exactly what is required by the indicator function in Equation 2 when computing unigram posteriors.
71	39	Whenever a final state is processed, the posterior probability accumulator for each label observed on paths reaching that state is updated by multiplying the label specific forward probability and the final weight associated with that state Algorithm 1 Compute N-gram Posteriors 1 for n← 1, .
72	28	, N do 2 An←Min(Det(RmEps(ProjOut(A ◦ Φn)))) 3 α[q]← λn(q), ∀ state q ∈ Qn 4 α̃[q][x]← 0, ∀ state q ∈ Qn, ∀ label x ∈ Σn 5 p(x|A)← 0, ∀ label x ∈ Σn 6 for each state s ∈ Qn do .
73	34	In topological order 7 for each arc (s, x, w, q) ∈ En do 8 α[q]← α[q]⊕ α[s]⊗ w 9 α̃[q][x]← α̃[q][x]⊕ α[s]⊗ w 10 for each label y ∈ α̃[s] do 11 if y 66= x then 12 α̃[q][y]← α̃[q][y]⊕ α̃[s][y]⊗ w 13 if s ∈ Fn then 14 for each label x ∈ α̃[s] do 15 p(x|A)← p(x|A)⊕ α̃[s][x]⊗ ρn(s) 16 P ←Min(ConstructPrefixTree(p)) and adding the resulting value to the accumulator (lines 13-15).
76	39	The key idea behind our algorithm is to restrict the computation of posteriors to only those n-grams that may potentially repeat on some path of the input lattice and exploit the equivalence of expected counts and posterior probabilities for the remaining n-grams.
103	27	If it is not, then no n-gram x = gi can repeat on some path of A since that would require g to repeat as well.
117	23	Similarly, Figure 2 gives a scatter plot of the maximum memory used by the program (maximum resident set size) during the computation of posteriors vs. the number of lattice n-grams (up to 5-grams).
119	30	To better understand the runtime characteristics of Algorithms 1 and 2, we conducted a small experiment where we randomly selected 100 lattices (total size: #states + #arcs = 81K, disk size: 1.2MB) from our data set and analyzed the relation between the runtime and the maximum ngram length N .
120	19	Table 2 gives a runtime comparison between the baseline posterior computation algorithm described in (Tromble et al., 2008), Algorithm 1, Algorithm 2 and the expected count computation algorithm of (Allauzen et al., 2004).
132	40	Unweighted factor automata, on the other hand, are significantly more compact than their weighted counterparts even though they accept the same set of strings.
152	18	We have described an efficient algorithm for computing n-gram posteriors from an input lattice and constructing an efficient and compact data structure for storing and retrieving them.
153	34	The runtime and memory requirements of the proposed algorithm grow linearly with the length of the n-grams as opposed to the exponential growth observed with the original algorithm we are building upon.
154	79	This is achieved by limiting the posterior computation to only those n-grams that may repeat on some path of the input lattice and using the relatively cheaper expected count computation algorithm for the rest.
155	46	This filtering of n-grams introduces a slight bookkeeping overhead over the baseline algorithm but in return dramatically reduces the runtime and memory requirements for long n-grams.
