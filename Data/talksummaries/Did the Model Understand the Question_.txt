0	26	Recently, deep learning has been applied to a variety of question answering tasks.
1	40	For instance, to answer questions about images (e.g. (Kazemi and Elqursh, 2017)), tabular data (e.g. (Neelakantan et al., 2017)), and passages of text (e.g. (Yu et al., 2018)).
6	22	In this paper, we propose techniques to analyze the sensitivity of a deep learning model to question words.
9	40	Consider the question “how symmetrical are the white bricks on either side of the building?” (corresponding image in Figure 1).
40	20	NP determines the answer to a question by selecting a sequence of operations to apply on the accompanying table (akin to an SQL query; details in section 5).
48	15	We quantify this weakness by evaluating NP on the set of perturbed tables generated by Pasupat and Liang (2016) and find that its accuracy drops from 33.5% to 23%.
56	27	For instance, we find that attacks are 50% more likely to be successful when the added sentence includes top-attributed nouns in the question.
82	16	In contrast, our attacks are based on models’ over-reliance on few question words even when other words should matter.
86	37	[0, 1] represents a deep network, and an input x = (x1, .
87	21	An attribution of the prediction at input x relative to a baseline input x0 is a vector AF (x, x0) = (a1, .
93	17	In this paper, we use an empty question as the baseline, that is, a sequence of word embeddings corresponding to padding value.
97	24	At each point on this trajectory, one can use the gradient of the function F with respect to the input to attribute the change in probability back to the input variables.
112	22	We use these observations to craft attacks against the network by perturbing instances where generic words (e.g., “a”, “the”) receive high attribution or contentful words receive low attribution.
128	16	We verified that altering the low attribution words in the question does not change the network’s answer.
134	50	To determine the set of question words that the network finds most important, we isolate words that most frequently occur as top attributed words in questions.
135	18	We then drop all words except these and compute the accuracy.
137	24	We find that just one word is enough for the model to achieve more than 50% of its final accuracy.
147	19	Subject ablation attack In this attack, we replace the subject of a question with a specific noun that consistently receives low attribution across questions.
151	23	Prefix attack In this attack, we attach content-free phrases to questions.
153	19	Table 1 (top half) shows the resulting accuracy for three prefixes —“in not a lot of words”, “what is the answer to”, and “in not many words”.
155	38	The union of the three attacks drops the model’s accuracy from 61.1% to 19%.
158	15	The union of these three ineffective prefixes drops the accuracy from 61.1% to only 46.9%.
220	41	Stop word deletion attacks We find that sometimes an operator is selected based on stop words like: “a”, “at”, “the”, etc.
254	33	for “Where according to gross state product does Victoria rank in Australia?”, “Australia” receives high attribution.
264	62	Recall that the attack sentences were constructed by (a) generating a sentence that answers the question, (b) replacing all the adjectives and nouns with antonyms, and named entities by the nearest word in GloVe word vector space (Pennington et al., 2014) and (c) crowdsourcing to check that the new sentence is grammatically correct.
265	191	This suggests a use of attributions to improve the effectiveness of the attacks, namely ensuring that question words that the model thinks are important are left untouched in step (b) (we note that other changes in should be carried out).
269	25	We believe that a workflow that uses attributions can aid the developer in iterating on model quality more effectively.
271	17	Under-reliance on important question terms is not safe.
272	77	We also believe that other QA models may share these weaknesses.
276	15	com/pramodkaushik/acl18_results.
