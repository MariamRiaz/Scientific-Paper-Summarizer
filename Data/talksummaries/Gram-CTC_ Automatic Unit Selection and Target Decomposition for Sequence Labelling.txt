21	30	Just as sequence prediction with cross entropy training can be seen as special case of the CTC loss with a fixed alignment, CTC can be seen as a special case of Gram-CTC with a fixed decomposition of target sequences.
23	41	Extensive experiments on multiple scales of data validate that Gram-CTC can improve CTC in terms of both performance and efficiency, and that using Gram-CTC the models outperform state-of-the-arts on standard speech benchmarks.
40	19	Since there is no alignment information, CTC marginalizes over all possible alignments.
42	34	For example, if the size of input is 3, and the output is ‘hi’, whose length is 2, there are three possible alignments, ‘-hi’, ‘h-i’ and ‘hi-’, where ‘-’ represents blank.
43	35	For the details, please refer to the original paper (Graves et al., 2006).
47	40	To simplify the problem, we also assume C ⊆ G. 1 For an input sequence x of length T , let y = Nw(x) be the sequence of network outputs, and denote by ytk as the probability of the k-th gram at time t, where k is the index of grams in G′ = G ∪ {blank}, then we have p(π|x) = T∏ t=1 ytπt ,∀π ∈ G ′T (1) Just as in the case of CTC, here we refer to the elements of G′T as paths, and denote them by π, which represents a possible alignment between input and output.
53	13	For a target sequence l, B−1(l) represents all paths mapped to l. Then, we have p(l|x) = ∑ π∈B−1(l) p(π|x) (2) This equation allows for training sequence labeling models without any alignment information using CTC loss, because it marginalizes over all possible alignments during training.
54	19	Gram-CTC uses the same effect to enable the model to marginalize over not only alignments, but also decompositions of the target sequence.
56	46	This is because there are O(τ) times more valid states per time step, and each state may have a valid transition from O(τ) states in the previous time step.
58	7	While the quadratic increase in the complexity of the algorithm is non trivial, we assert that it is a trivial increase in the overall training time of typical neural networks, where the computation time is dominated by the neural networks themselves.
59	10	Additionally, the algorithm extends generally to any arbitrary G and need not have all possible n-grams up to length τ .
60	18	To efficiently compute p(l|x), we also adopt the dynamic programming algorithm.
62	11	In our case, the state must contain all the information required to identify all valid extensions of an incomplete path π such that the collapsing function will eventually collapse the complete π back to l. For Gram-CTC, this can be done by collapsing all but the last element of the path π.
63	25	Therefore, the state is a tuple (l1:i, j), where the first item is a collapsed path, representing a prefix of the target label sequence, and j ∈ {0, .
64	37	, τ} is the length of the last gram (li−j+1:i) used for making the prefix.
66	8	We denote the gram (li−j+1:i) by g j i (l), and the state (l1:i, j) as s j i (l).
67	11	For readability, we will further shorten sji (l) to s j i and g j i (l) to g j i .
68	9	For a state s, its corresponding gram is denoted by sg , and the positions of the first character and last character of sg are denoted by b(s) and e(s), respectively.
79	18	For the backpropagation, the most important formula is the partial derivative of loss with regard to the unnormalized output utk.
80	9	∂ ln p(l|x) ∂utk = ytk − 1 ytkZt ∑ s∈lab(l,k) αt(s)βt(s) (12) where Zt def = ∑ s∈S αt(s)βt(s) ytsg .
84	21	They look very similar, although the number of grams is greatly reduced after refinement, which makes training faster and potentially more robust due to less gram sparsity.
88	20	Here we describe additional techniques we found useful in practice to enable the Gram-CTC to work efficiently as well as effectively.
90	28	For example, in English, we have 26 characters, then the total number of bi-grams is 262 = 676, the total number of tri-grams are 263 = 17576, .
92	11	However, it is unnecessary to consider many grams, such as ‘aaaa’, which are obviously useless.
93	53	In our experiments, we first eliminate most of useless grams from the statistics of a huge corpus, that is, we count the frequency of each gram in the corpus and drop these grams with rare frequencies.
94	19	Then, we train a model with Gram-CTC on all the remaining grams.
95	103	By applying (decoding) the trained model on a large speech dataset, we get the real statistics of gram’s usage.
98	27	For details, please refer to Section 5.2.
99	40	Gram-CTC needs to solve both decomposition and alignment tasks, which is a harder task for a model to learn than CTC.
100	31	This is often manifested in unstable training curves, forcing us to lower the learning rate which in turn results in models converging to a worse optima.
