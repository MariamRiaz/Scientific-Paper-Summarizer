0	18	Support vector machines (SVM) is an established algorithm for classification with two categories (Vapnik, 1998; Smola and Schlkopf, 1998; Steinwart and Christmann, 2008; Friedman et al., 2009).
1	21	The method finds the maximum margin separating hyperplane; it finds the hyperplane dividing the input space (perhaps after mapping the data to a higher dimensional space) into two categories and maximizing the minimum distance from a point to the hyperplane.
2	26	SVM can also be adapted to allow for imperfect classification, in which case we speak of soft margin SVM.
3	19	Given the success of SVM at binary classification, many 1Harris School of Public Policy, University of Chicago, Chicago, IL, USA.
15	19	Likewise, standard methods for deriving Donsker theorems and limit distribution theory do not apply to such constrained vectors of random variables.2 In a separate strain of literature, Mroueh, Poggio, Rosasco and Slotine (2012) have proposed the simplex-cone SVM (SC-SVM), a multicategory classifier developped within the vector reproducing kernel Hilbert space set-up.
22	25	As a second contribution, we deliver a Donsker theorem for MSVM, as well as an asymptotic covariance formula with sample analog.
25	19	The fourth contribution is important because it provides analytical substance to a long-standing open question.
26	81	To be sure, the different attempts at developping a multicategory generalization of binary SVM can be understood as subscribing to one of two broad approaches.
28	65	For instance, the popular One-vs-Rest approach works as follows: to predict the category of a point in a test set3 (i.e. out of sample), run K binary SVMs where the first category is one of the original K categories, and the second category is the union of the remaining K 1 categories.
29	23	The predicted category is the one that was picked against all others with the greatest “confidence”.
30	26	In practice, the confidence criteria used is the distance of the test point to the separating hyperplane (we show in Subsection 3.1 that even this can be improved according to statistical considerations).
31	58	The second approach consists in generalizing the standard SVM to develop a single machine which implements multicategory classification solving a single, joint optimization problem.
32	39	Many such algorithms have been suggested (Weston and Watkins, 1999; Crammer and Singer, 2002; Lee et al., 2004).
35	28	This phenomenon is widely acknowledged (Rifkin and Klautau, 2004) but very little theory has been put forth to explain it.
53	54	For their multicategory SVM (MSVM), Lee et al. (2004) encode y i associated with category k 2 {1, ...,K} as a Ktuple with 1 in the kth entry and 1 K 1 in every other entry.
57	21	Importantly, the decision function is constrained to sum to zero, i.e. 1T k (Wx+ b) = 0, 8 x.
59	38	Mroueh et al. (2012) preconize an encoding that does away with the sum-to-zero constraint.
63	23	It encodes the responses as unit vectors in RK 1 having maximal equal angle with each other.
69	17	That is, both problems are exactly equivalent.
88	21	By Gantmacher (1959), G(C) = K X i=1 det 2 (C i·) , (8) where C i· is C with the ith row removed.
92	32	In Table 1, we display “clock-on-the-wall” computation times.
102	44	The representer theorem yields that f j (x) = b j + P n i=1 aijK(xi, x) for j = 1, ...,K with sum-to-zero constraint.
105	25	We then get, again, equality of the objective functions up to the tuning parameter.
109	23	To the best of my knowledge, if a practitioner wants to compute the asymptotic covariance matrix of SVM or MSVM –which is essential in order to know where extrapolation is reliable– this article is the only resource displaying worked out expressions with sample analogs.4 One readily obtains (Van der Vaart, 2008) a standard central limit theorem result of the form p n ⇣ ˆ ˜ ⇥ n ˜⇥⇤ ⌘ d!
116	45	i , b i ) 2 Rp+1, and categorizes a point by attributing it to the category in which it is the “deepest”.
117	37	However, studentized distances yield more sensible and reliable classifications by accounting for the comparative uncertainty of the hyperplanes when categorizing a given point.
118	53	Naturally, a point being ”deeper” with respect to a classifying hyperplane –in terms of the length of the line from the point to the hyperplane and normal to the hyperplane– should make one more confident in the classification if it occurs in a section of the space where the hyperplane has lower variance.
141	23	It is clear from the comparison of the loss functions (2) and (10), corresponding to SC-SVM and the simplex encoding of One-vs-Rest, respectively, that both penalize for an observation that falls within the half-space assigned to an “other” category, but only One-vs-Rest rewards for points falling within their true, “own” category.
145	54	Furthermore, in the special case of a separable data generating process (DGP), that is in the case in which 1{ã D c y , ˜f E 0} = 0 a.s., we get that ⌦Multi = ⌦1vsR and both procedures have the same target hyperplane.
157	82	We gave an analytical characterization of the surprisingly good performance of the One-vs-Rest procedure, comparatively to MSVM, using the asymptotic distribution of estimators.
