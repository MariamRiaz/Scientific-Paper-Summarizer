0	45	Document summarization is a task to generate a fluent, condensed summary for a document, and keep important information.
1	25	As a useful technique to alleviate the information overload people are facing today, document summarization has been extensively investigated.
4	15	They have the advantage of producing fluent sentences and preserving the meaning of original documents, but also inevitably face the drawbacks of information redundancy and incoherence between sentences.
11	24	Success has been witnessed on tasks like machine translation and image captioning, together with the abstractive sentence summarization (Rush et al., 2015).
12	17	Unfortunately, the extension of sentence abstractive methods to the document summarization task is not straightforward.
14	24	Recent abstractive document summarization models are yet not able to achieve convincing performance, with a considerable gap from extractive methods.
15	12	In this paper, we review the key factors of document summarization, i.e., the saliency, fluency, coherence, and novelty requirements of the generated summary.
16	30	Fluency is what neural generation models are naturally good at, but the other factors are less considered in previous neural abstractive models.
71	40	We also use an LSTM-based hierarchical decoder framework to generate the summary, because the summary typically comprises several sentences.
72	12	The sentence decoder decsent receives the document representation c as the initial state h ′ 0 = c, and predicts the sentence representations sequentially, by h ′ j = decsent(h ′ j−1,x ′ j−1), where x ′ j−1 is the encoded representation of the previously generated sentence s ′ j−1.
75	15	A word decoder stops when it generates the “<eos>” token and similarly the sentence decoder stops when it generates the “<eod>” token.
89	47	In graph-based extractive summarization, a graph G is constructed to rank the original sentences.
91	22	Let W ∈ Rn×n be the adjacent matrix.
93	39	, fn] ∈ Rn denotes the rank scores of the n sentences.
94	12	f(t) denotes the rank scores at the t-th iteration.
96	21	Assume we use hi as the representation of si, and W (i, j) = hTi Mhj , where M is a parameter matrix to be learned.
101	29	In our model we use the scores f j to compute the attention.
103	13	Inspired by the query-focused graph-based extractive summarization model (Wan et al., 2007), we realize this by applying the idea of topic-sensitive PageRank (Haveliwala, 2002), which is to rank the sentences with the concern of their relevance to the topic.
109	83	In- spired by (Chen et al., 2016) we adopt a distraction mechanism to compute the final attention value αji , which subtracts the rank scores of the previous step, to penalize the model from attending to previously attended sentences, and also help to normalize the ranked scores f j .
114	41	1, and the neural model using the graph-based attention can also be trained using traditional gradientbased methods.
119	26	We find there are several problems during the generation of summary, including out-of-vocabulary (OOV) words, information incorrectness, error accumulation and repetition.
121	24	In this work, we propose a hierarchical decoding algorithm with a reference mechanism to tackle these difficulties, which effectively improves the quality of generated summaries.
122	29	As OOV words frequently occur in name entities, we can first identify the entities of a document using NLP toolkit like Stanford CoreNLP1.
123	16	Then we prefix every entity with an “@entity” token and a number indicating how many words the entity has.
125	17	After decoding we recover the OOV words by matching entities in the original document according to the contexts.
127	19	A beam search strategy may help to alleviate the repetition in a sentence, but the repetition in the whole generated summary is remained a problem.
129	62	The reason is that the K-best sentences generated by a word decoder will mostly be similar to each other, which is also noticed by Li et al. (2016).
131	13	The hierarchical algorithm comprises K-best word-level beam search and N -best sentencelevel beam search.
133	11	s∗ is an original sentence to refer to.
135	24	The added term aims to favor the generated word yτ with improving the bigram overlap between current generated summary Yτ−1 and the target original sentence s∗.
