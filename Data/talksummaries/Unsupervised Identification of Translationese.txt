0	44	Human-translated texts (in any language) have distinct features that distinguish them from original, non-translated texts.
4	56	First, for training translation models, parallel texts that were translated in the direction of the SMT task are preferable to texts translated in the opposite direction; second, for training language models, monolingual corpora of translated texts are better than original texts.
6	35	Existing approaches, however, only employ supervised machine-learning; they therefore suffer from two main drawbacks: (i) they inherently depend on data annotated with the translation direction, and (ii) they may not be generalized to unseen (related or unrelated) domains.1 These shortcomings undermine the usability of supervised methods for translationese identification in a typical real-life scenario, where no labelled in-domain data are available.
7	45	In this work we explore unsupervised techniques for reliable discrimination of original and translated texts.
50	33	Our main dataset2 consists of texts originally written in English and texts translated to English from French.
53	25	Table 1 details some statistical data on the corpora (after tokenization).3 We now briefly describe each dataset.
56	26	We use a version of Europarl (Rabinovich and Wintner, Forthcoming) that aims to further increase the confidence in the direction of translation, through a comprehensive cross-lingual validation of the original language of the speakers.
61	43	The Literature corpus consists of literary classics written (and translated) in the 18th–20th centuries by English and French authors; the raw material is available from the Gutenberg project.
64	23	Our TED talks corpus consists of talks originally given in English and talks translated to English from French.
68	25	All datasets are first tokenized using the Stanford tools (Manning et al., 2014) and then partitioned into chunks of approximately 2000 tokens (ending on a sentence boundary).
98	46	We first reproduce the Europarl classification results with the best performing feature sets, as reported by Volansky et al. (2015), and present results for three additional sub-corpora: Hansard, Literature and TED.
103	35	Attempting to enrich the classifier’s training “experience” we conducted additional experiments, where we train on two sub-corpora out of Europarl, Hansard and Literature, and test on the remaining one.
107	22	Excellent indomain classification results on the one hand and poor cross-domain predictive performance on the other, imply that the model describing the relation in a certain domain is inapplicable to a different (even seemingly similar) domain due to significant differences in the distribution of the underlying data.
128	32	As is always the case with unsupervised methods, clustering can divide observations into classes but cannot label those classes.
132	39	We then compare language models to reveal similarities between the prototypical O and T samples and the chunk sets produced by clustering.
136	21	Similarly, Tm (T-markers) is a set of words with O-to-T frequency ratio below (1-δ).
139	37	Formally, for w ∈ V , p(w | Om) = tf (w) + |Om|+ × |V | p(w | Tm) = tf (w) + |Tm|+ × |V | We denote the resulting language models by PO and PT , respectively.
145	43	We construct prototypical O- and T-texts by selecting O- and T-markers from a random sample of Europarl and Hansard texts, using 600 chunks from each corpus.8 We then compare the language models induced by these samples to those of the generated clusters (tested on different chunks, of course) to determine the cluster labels; the predicted labels are then verified against the majority-driven labeling, based on ground truth annotations.
149	27	Since different feature sets have different predictions on our data, we hypothesize that consensus voting can improve the accuracy of clustering.
154	41	Both three judges and five judges yield a consistent increase in accuracy.
160	35	Naturally, clustering accuracy stabilizes when the number of chunks increases, since the effect of random noise diminishes with more data.
162	22	The accuracy of supervised classification deteriorates when the size of the underlying logical units (here, chunks) decreases (Kurokawa et al., 2009).
165	42	Poor cross-domain classification results, as described in Section 4, demonstrate that the in-domain discriminative features of translated texts cannot be easily generalized to other, even related, domains.
166	20	In this section we explore the tension between the discriminative power of domain- and translationeserelated properties, in the unsupervised scenario.
187	49	The two-phase method first clusters a mixture of texts into domains (e.g., using KMeans), and then separates each of the resulting (presumably, domain-coherent) clusters into two sub-clusters, presumably O and T. The flat approach applies KMeans, attempting to divide the dataset into 2 × k clusters; that is, we expect classification by domains and by translationese status, simultaneously.
201	116	In this work we advocate the use of unsupervised classification as an effective way to ad- dress this task.
204	21	We further highlight the dominance of domain-based characteristics of the texts over their translationese-related properties and propose a simple methodology for identification of translationese in a mixed-domain setup.
210	45	However, its effect on clustering accuracy was not uniform: the most prominent improvement (over 15 percent points) was obtained on the TED dataset, while a slight accuracy deterioration was observed in a few cases (e.g., 5 percent points on Europarl with FW).
214	104	In the future, we intend to explore the robustness of our approach even further, with more datasets in various language pairs.
216	37	We will also experiment with English-German, in both directions, and hopefully also with EnglishHebrew, a more challenging setup.
