0	55	Let the input space H be a separable Hilbert space with inner product denoted by 〈·, ·〉H , and the output space R. Let ρ be an unknown probability measure on H ×R.
1	55	In this paper, we study the following expected risk minimization, inf ω∈H Ẽ(ω), Ẽ(ω) = ∫ H×R (〈ω, x〉H − y)2dρ(x, y), (1) where the measure ρ is known only through a sample z = {zi = (xi, yi)}ni=1 of size n ∈ N, independently and identically distributed (i.i.d.)
2	54	The above regression setting covers nonparametric regression over a reproducing kernel Hilbert space (Cucker & Zhou, 2007; Steinwart & Christmann, 2008), and it is close 1 Laboratory for Information and Inference Systems, École Polytechnique Fédérale de Lausanne, Lausanne, Switzerland.
3	12	to functional regression (Ramsay, 2006) and linear inverse problems (Engl et al., 1996).
4	24	A basic algorithm for the problem is ridge regression, and its generalization, spectralregularized algorithm.
5	11	Such algorithms can be viewed as solving an empirical, linear equation with the empirical covariance operator replaced by a regularized one, see (Caponnetto & Yao, 2006; Bauer et al., 2007; Gerfo et al., 2008; Lin et al., 2018) and references therein.
6	54	Here, the regularization is used to control the complexity of the solution to against over-fitting and to achieve best generalization ability.
7	14	The function/estimator generated by classic regularized algorithm is in the subspace span{x} of H , where x = {x1, · · · , xn}.
8	31	More often, the search of an estimator for some specific algorithms is restricted to a different (and possibly smaller) subspace S, which leads to regularized algorithms with projection.
9	11	Such approaches have computational advantages in nonparametric regression with kernel methods (Williams & Seeger, 2000; Smola & Schölkopf, 2000).
11	13	The resulted algorithms are called Nyström regularized algorithm and sketched-regularized algorithm, respectively.
12	1	Our starting points of this paper are recent papers (Bach, 2013; Alaoui & Mahoney, 2015; Yang et al., 2015; Rudi et al., 2015; Myleiko et al., 2017) where convergence results on Nyström/sketched regularized algorithms for learning with kernel methods are given.
13	7	Particularly, within the fixed design setting, i.e., the input set x are deterministic while the output set y = {y1, · · · , yn} treated randomly, convergence results have been derived, in (Bach, 2013; Alaoui & Mahoney, 2015) for Nyström ridge regression and in (Yang et al., 2015) for sketched ridge regression.
15	44	The latter results were further generalized in (Myleiko et al., 2017) to a general Nyström regularized algorithm.
17	41	Besides, all the derived results, either for sketched or Nyström regularized algorithms, are only for the attainable case, i.e., the case that the expected risk minimization (1) has at least one solution in H .
18	10	Moreover, they saturate (Bauer et al., 2007) at a critical value, meaning that they can not lead to better convergence rates even with a smoother target function.
19	20	Motivated by these, in this paper, we study statistical results of projected-regularized algorithms for least-squares regression over a separable Hilbert space within the random design setting.
20	19	We first extend the analysis in (Lin et al., 2018) for classicregularized algorithms to projected-regularized algorithms, and prove statistical results with respect to a broader class of norms.
21	11	We then show that optimal rates can be retained for sketched-regularized algorithms, provided that the sketch dimension is proportional to the effective dimension (Zhang, 2005) up to a logarithmic factor.
22	14	As a byproduct, we obtain similar results for Nyström regularized algorithms.
23	41	Interestingly, our results are the first ones with optimal, distribution-dependent rates that do not have any saturation effect for sketched/Nyström regularized algorithms, considering both the attainable and non-attainable cases.
24	13	In our proof, we naturally integrate proof techniques from (Smale & Zhou, 2007; Caponnetto & De Vito, 2007; Rudi et al., 2015; Myleiko et al., 2017; Lin et al., 2018).
25	3	Our novelties lie in a new estimates on the projection error for sketched-regularized algorithms, a novel analysis to conquer the saturation effect, and a refined analysis for Nyström regularized algorithms, see Section 4 for details.
26	5	The rest of the paper is organized as follows.
30	15	In this section, we introduce some notations as well as auxiliary operators, and present projected-regularized algorithms.
31	4	Let Z = H ×R, ρX(·) the induced marginal measure on H of ρ, and ρ(·|x) the conditional probability measure on R with respect to x ∈ H and ρ.
33	12	(2) Define the hypothesis space Hρ = {f : H → R|∃ω ∈ H with f(x) = 〈ω, x〉H , ρX -almost surely}.
