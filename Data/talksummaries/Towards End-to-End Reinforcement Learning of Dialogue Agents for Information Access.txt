0	32	The design of intelligent assistants which interact with users in natural language ranks high on the agenda of current NLP research.
1	49	With an increasing focus on the use of statistical and machine learning based approaches (Young et al., 2013), the last few years have seen some truly remarkable conversational agents appear on the market (e.g. Apple Siri, Microsoft Cortana, Google Allo).
2	26	These agents can perform simple tasks, answer factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far behind a human assistant in terms of both the variety and complexity of tasks they can perform.
3	33	In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time.
4	73	Recently, Reinforcement Learning (RL) has been explored to leverage user interactions to adapt various dialogue agents designed, respectively, for task completion (Gašić et al., 2013), information access (Wen et al., 2016b), and chitchat (Li et al., 2016a).
6	56	Such agents must necessarily query databases in order to retrieve the requested information.
7	18	This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen et al. (2016b), Williams and Zweig (2016), and Li et al. (2017)’s work.
9	17	While natural, this approach has two drawbacks: (1) the retrieved results do not carry any information about uncertainty in semantic parsing, and (2) the retrieval operation is non differentiable, and hence the parser and dialog policy are trained separately.
11	75	In this work, we propose a probabilistic framework for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup.
45	16	A Knowledge Base consists of triples of the form (h, r, t), which denotes that relation r holds between the head h and tail t. We assume that the KB-InfoBot has access to a domain-specific entity-centric knowledge base (EC-KB) (Zwicklbauer et al., 2013) where all head entities are of a particular type (such as movies or persons), and the relations correspond to attributes of these head entities.
62	25	For Φj = 0, the user does not know the value of the slot, and from the prior: Pr(Gj = i|Φj = 0) = 1 N , 1 ≤ i ≤ N (2) For Φj = 1, the user knows the value of slot j, but this may be missing from T , and we again have two cases: Pr(Gj = i|Φj = 1) = { 1 N , i ∈Mj ptj(v) Nj(v) ( 1− |Mj | N ) , i 6∈Mj (3) Here, Nj(v) is the count of value v in slot j.
75	13	We describe two versions of the belief tracker.
76	36	Hand-Crafted Tracker: We first identify mentions of slot-names (such as “actor”) or slot-values (such as “Bill Murray”) from the user input ut, using token-level keyword search.
77	17	Let {w ∈ x} de- note the set of tokens in a string x3, then for each slot in 1 ≤ j ≤ M and each value v ∈ V j , we compute its matching score as follows: stj [v] = |{w ∈ ut} ∩ {w ∈ v}| |{w ∈ v}| (4) A similar score btj is computed for the slot-names.
80	14	Starting from an prior distribution p0j (based on the counts of the values in the KB), ptj [v] is updated as: ptj [v] ∝ pt−1j [v] + C ( stj [v] + b t j + 1(req t[j] = 1) ) (5) Here C is a tuning parameter, and the normalization is given by setting the sum over v to 1.
95	24	(8) Here, p0j is a prior distribution over the values of slot j, estimated using counts of each value in the KB.
102	12	The dialogue policy’s job is to select the next action based on the current summary state s̃t and the dialogue history.
126	12	We have described two belief trackers – (A) HandCrafted and (B) Neural, and two dialogue policies – (C) Hand-Crafted and (D) Neural.
135	34	Lastly, the E2E agent uses the neural belief tracker and the neural policy (B+D), with a Soft-KB lookup.
136	31	For the RL agents, we also append q̂tj and a one-hot encoding of the previous agent action to the policy network input.
140	17	In this work we adapt the publicly-available user simulator presented in Li et al. (2016b) to follow a simple agenda while interacting with the KB-InfoBot, as well as produce natural language utterances .
150	23	We compare each of the discussed versions along three metrics: the average rewards obtained (R), success rate (S) (where success is defined as providing the user target among top R results), and the average number of turns per dialogue (T).
153	32	For reference we also show the performance of an agent which receives perfect information about the user target without any errors, and selects actions based on the entropy of the slots (Max).
155	14	In each case the Soft-KB versions achieve the highest average reward, which is the metric all agents optimize.
159	14	Further, reinforcement learning helps discover better policies than the handcrafted rule-based agents, and we see a higher reward for RL agents compared to Rule ones.
168	65	We further evaluate the KB-InfoBot versions trained using the simulator against real subjects, recruited from the author’s affiliations.
171	25	Subjects were asked to initiate the conversation by specifying some of these values, and respond to the agent’s subsequent requests, all in natural language.
172	52	We test RL-Hard and the three Soft-KB agents in this study, and in each session one of the agents was picked at random for testing.
174	16	Figure 3 shows a comparison of these agents in terms of success rate and number of turns, and Figure 4 shows some sample dialogues from the user interactions with RL-Soft.
175	29	In comparing Hard-KB versus Soft-KB lookup methods we see that both Rule-Soft and RL-Soft agents achieve a higher success rate than RL-Hard, while E2E-Soft does comparably.
