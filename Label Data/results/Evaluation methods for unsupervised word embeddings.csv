0,1,label2,summary_sentences
"People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors.",1 Introduction,[0],[0]
"It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them.",1 Introduction,[0],[0]
"This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender.
",1 Introduction,[0],[0]
Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness.,1 Introduction,[0],[0]
"Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users.",1 Introduction,[0],[0]
"Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of each group.",1 Introduction,[0],[0]
"However, hand-designed user groups
may not reflect the true variability of the data, and may therefore inhibit the system’s ability to flexibly adapt to new users.
",1 Introduction,[0],[0]
"In this paper, we present a user adaptation model for NLG systems which induces user groups from training data in which these groups were not annotated.",1 Introduction,[0],[0]
"At training time, we probabilistically assign users to groups and learn the language preferences for each group.",1 Introduction,[0],[0]
"At evaluation time, we assume that our system has a chance to interact with each new user repeatedly – e.g., in the context of a dialogue system.",1 Introduction,[0],[0]
"It will then calculate an increasingly accurate estimate of the user’s group membership based on observable behavior, and use it to generate utterances that are suitable to the user’s true group.
",1 Introduction,[0],[0]
We evaluate our model on two tasks involving the generation of referring expressions (RE).,1 Introduction,[0],[0]
"First, we predict the use of spatial relations in humanlike REs in the GRE3D domain (Viethen and Dale, 2010) using a log-linear production model in the spirit of Ferreira and Paraboni (2014).",1 Introduction,[0],[0]
"Second, we predict the comprehension of generated REs, in a synthetic dataset based on data from the GIVE Challenge domain (Striegnitz et al., 2011) with the log-linear comprehension model of Engonopoulos et al. (2013).",1 Introduction,[0],[0]
"In both cases, we show that our model discovers user groups in the training data and infers the group of unseen users with high confidence after only a few interactions during testing.",1 Introduction,[0],[0]
"In the GRE3D domain, our system outperformed a strong baseline which used demographic information for the users.",1 Introduction,[0],[0]
Differences between individual users have a substantial impact on language comprehension.,2 Related Work,[0],[0]
"Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (Häuser et al., 2017); gender (Dräger and Koller,
ar X
iv :1
80 6.
",2 Related Work,[0],[0]
"05 94
7v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
5 Ju
n 20
18
2012); or language proficiency (Koller et al., 2010).
",2 Related Work,[0],[0]
Individual differences are also reflected in the way people produce language.,2 Related Work,[0],[0]
"Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does.",2 Related Work,[0],[0]
Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE.,2 Related Work,[0],[0]
"However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users.",2 Related Work,[0],[0]
"Ferreira and Paraboni (2017) also demonstrate that splitting speakers in predefined groups and training each group separately improves the human likeness of REs compared to training individual user models.
",2 Related Work,[0],[0]
"The ability to adapt to the comprehension and production preferences of a user is especially important in the context of a dialog system, where there are multiple chances of interacting with the same user.",2 Related Work,[0],[0]
Some methods adapt to dialog system users by explicitly modeling the users’ knowledge state.,2 Related Work,[0],[0]
"An early example is Paris (1988); she selects a discourse plan for a user, depending on their level of domain knowledge ranging between novice and expert, but provides no mechanism for inferring the group to which the user belongs.",2 Related Work,[0],[0]
"Rosenblum and Moore (1993) try to infer what knowledge a user possesses during dialogue, based on the questions they ask.",2 Related Work,[0],[0]
Janarthanam and Lemon (2014) adapt to unseen users by using reinforcement learning with simulated users to make a system able to adjust to the level of the user’s knowledge.,2 Related Work,[0],[0]
"They use five predefined groups from which they generate the simulated users’ behavior, but do not assign real users to these groups.",2 Related Work,[0],[0]
"Our system makes no assumptions about the user’s knowledge and does not need to train with simulated users, or use any kind of information-seeking moves; we instead rely on the groups that are discovered in training and dynamically assign new, unseen users, based only on their observable behavior in the dialog.
",2 Related Work,[0],[0]
"Another example of a user-adapting dialog component is SPaRKy (Walker et al., 2007), a trainable sentence planner that can tailor sentence plans to individual users’ preferences.",2 Related Work,[0],[0]
"This requires training on separate data for each user; in contrast to this, we leverage the similarities between users and can take advantage of the full training data.",2 Related Work,[0],[0]
We start with a basic model of the way in which people produce and comprehend language.,3 Log-linear models for NLG in dialog,[0],[0]
"In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s.",3 Log-linear models for NLG in dialog,[0],[0]
"The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is the meaning b which they assign to the utterance s they hear.
",3 Log-linear models for NLG in dialog,[0],[0]
"Given this terminology, we define a basic loglinear model (Berger et al., 1996) of language use as follows:
P (b|s; ρ) = exp(ρ · φ(b, s))∑ b′",3 Log-linear models for NLG in dialog,[0],[0]
"exp(ρ · φ(b′, s))
",3 Log-linear models for NLG in dialog,[0],[0]
"(1)
where ρ is a real-valued parameter vector of length n and φ(b, s) is a vector of real-valued feature functions f1, ..., fn over behaviors and stimuli.",3 Log-linear models for NLG in dialog,[0],[0]
"The parameters can be trained by maximum-likelihood estimation from a corpus of observations (b, s).",3 Log-linear models for NLG in dialog,[0],[0]
"In addition to maximum-likelihood training it is possible to include some prior probability distribution, which expresses our belief about the probability of any parameter vector and which is generally used for regularization.",3 Log-linear models for NLG in dialog,[0],[0]
"The latter case is referred to as a posteriori training, which selects the value of ρ that maximizes the product of the parameter probability and the probability of the data.
",3 Log-linear models for NLG in dialog,[0],[0]
"In this paper, we focus on the use of such models in the context of the NLG module of a dialogue system, and more specifically on the generation of referring expressions (REs).",3 Log-linear models for NLG in dialog,[0],[0]
"Using (1) as a comprehension model, Engonopoulos et al. (2013) developed an RE generation model in which the stimulus s = (r, c) consists of an RE r and a visual context c of the GIVE Challenge (Striegnitz et al., 2011), as illustrated in Fig. 1.",3 Log-linear models for NLG in dialog,[0],[0]
The behavior is the object b in the visual scene to which the user will resolve the RE.,3 Log-linear models for NLG in dialog,[0],[0]
"Thus for instance, when we consider the RE r =“the blue button” in the context of Fig. 1, the log-linear model may assign a higher probability to the button on the right than to the one in the background.",3 Log-linear models for NLG in dialog,[0],[0]
"Engonopoulos and Koller (2014) develop an algorithm for generating the RE r which maximizes P (b∗|s; ρ), where b∗ is the intended referent in this setting.
",3 Log-linear models for NLG in dialog,[0],[0]
"Conversely, log-linear models can also be used to directly capture how a human speaker would refer to an object in a given scene.",3 Log-linear models for NLG in dialog,[0],[0]
"In this case, the stimulus s = (a, c) consists of the target object a and
the visual context c, and the behavior b is the RE.",3 Log-linear models for NLG in dialog,[0],[0]
"We follow Ferreira and Paraboni (2014) in training individual models for the different attributes which can be used in the RE (e.g., that a is a button; that it is blue; that the RE contains a binary relation such as “to the right of”), such that we can simply represent b as a binary choice b ∈ {1,−1} between whether a particular attribute should be used in the RE or not.",3 Log-linear models for NLG in dialog,[0],[0]
"We can then implement an analog of Ferreira’s model in terms of (1) by using feature functions φ(b, a, c) = b · φ′(a, c), where φ′(a, c) corresponds to their context features, which do not capture any speaker-specific information.",3 Log-linear models for NLG in dialog,[0],[0]
"As discussed above, a user-agnostic model such as (1) does not do justice to the variability of language comprehension and production across different speakers and listeners.",4 Log-linear models with user groups,[0],[0]
We will therefore extend it to a model which distinguishes different user groups.,4 Log-linear models with user groups,[0],[0]
We will not try to model why1 users behave differently.,4 Log-linear models with user groups,[0],[0]
"Instead our model sorts users into groups simply based on the way in which they respond to stimuli, in the sense of Section 3, and implements this by giving each group g its own parameter vector ρ(g).",4 Log-linear models with user groups,[0],[0]
"As a theoretical example, Group 1 might contain users who reliably comprehend REs which use colors (“the green button”), whereas Group 2 might contain users who more easily understand relational REs (“the button next to the lamp”).",4 Log-linear models with user groups,[0],[0]
"These groups are then discovered at training time.
",4 Log-linear models with user groups,[0],[0]
"When our trained NLG system starts interacting with an unseen user u, it will infer the group to which u belongs based on u’s observed responses to previous stimuli.",4 Log-linear models with user groups,[0],[0]
"Thus as the dialogue with u unfolds, the system will have an increasingly pre-
1E.g., in the sense of explicitly modeling sociolects or the difference between novice system users vs. experts.
cise estimate of the group to which u belongs, and will thus be able to generate language which is increasingly well-tailored to this particular user.",4 Log-linear models with user groups,[0],[0]
"We assume training data D = {(bi, si, ui)}i which contains stimuli si together with the behaviors bi which the users ui exhibited in response to si.",4.1 Generative story,[0],[0]
"We write D(u) = {(bu1 , su1), . . .",4.1 Generative story,[0],[0]
"(buN , suN )} for the data points for each user",4.1 Generative story,[0],[0]
"u.
",4.1 Generative story,[0],[0]
"The generative story we use is illustrated in Fig. 2; observable variables are shaded gray, unobserved variables and parameters to be set in training are shaded white and externally set hyperparameters have no circle around them.",4.1 Generative story,[0],[0]
"Arrows indicate which variables and parameters influence the probability distribution of other variables.
",4.1 Generative story,[0],[0]
"We assume that each user belongs to a group g ∈ {1, . . .",4.1 Generative story,[0],[0]
",K}, where the number K of groups is fixed beforehand based on, e.g., held out data.",4.1 Generative story,[0],[0]
"A group g is assigned to u at random from the distribution
P (g|π) = exp(πg)∑K g′=1 exp(πg′)
(2)
",4.1 Generative story,[0],[0]
"Here π ∈ RK is a vector of weights, which defines how probable each group is a-priori.
",4.1 Generative story,[0],[0]
"We replace the single parameter vector ρ of (1) with group-specific parameters vectors ρ(g), thus obtaining a potentially different log-linear model P ( b|s; ρ(g) ) for each group.",4.1 Generative story,[0],[0]
"After assigning a group, our model generates responses bu1 , . . .",4.1 Generative story,[0],[0]
", b u N at
random from P ( b|s; ρ(g) ) , based on the group specific parameter vector and the stimuli su1 , . . .",4.1 Generative story,[0],[0]
", s u N .",4.1 Generative story,[0],[0]
This accounts for the generation of the data.,4.1 Generative story,[0],[0]
"We model the parameter vectors π ∈ RK , and ρ(g) ∈",4.1 Generative story,[0],[0]
"Rn for every 1 ≤ g ≤ K as drawn from
P (D; θ) = ∏ u∈U K∑ g=1",4.1 Generative story,[0],[0]
"P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ) · N (π|0, σ(π)) · K∏",4.1 Generative story,[0],[0]
g=1 N,4.1 Generative story,[0],[0]
"( ρ(g)|0, σ(ρ) )",4.1 Generative story,[0],[0]
"(3) L(θ) =
∑ u∈U log K∑",4.1 Generative story,[0],[0]
g=1,4.1 Generative story,[0],[0]
P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ),4.1 Generative story,[0],[0]
"(4)
AL(θ) = ∑ u∈U K∑ g=1
P (g|D(u); θ(i−1)) ·",4.1 Generative story,[0],[0]
logP (g|π) +,4.1 Generative story,[0],[0]
"∑
d∈Du
logP ( bd|sd; ρ(g) )",4.1 Generative story,[0],[0]
"(5)
normal distributions N (0, σ(π)), and N (0, σ(ρ)), which are centered at 0 with externally given variances and no covariance between parameters.",4.1 Generative story,[0],[0]
This has the effect of making parameter choices close to zero more probable.,4.1 Generative story,[0],[0]
"Consequently, our models are unlikely to contain large weights for features that only occurred a few times or which are only helpful for a few examples.",4.1 Generative story,[0],[0]
"This should reduce the risk of overfitting the training set.
",4.1 Generative story,[0],[0]
The equation for the full probability of the data and a specific parameter setting is given in (3).,4.1 Generative story,[0],[0]
"The left bracket contains the likelihood of the data, while the right bracket contains the prior probability of the parameters.",4.1 Generative story,[0],[0]
Once we have set values θ =,4.2 Predicting user behavior,[0],[0]
"(π, ρ(1), . . .",4.2 Predicting user behavior,[0],[0]
", ρ(K))",4.2 Predicting user behavior,[0],[0]
"for all the parameters, we want to predict what behavior b a user u will exhibit in response to a stimulus s. If we encounter a completely new user u, the prior user group distribution from (2) gives the probability that this user belongs to each group.",4.2 Predicting user behavior,[0],[0]
"We combine this with the group-specific log-linear behavior models to obtain the distribution:
P (b|s; θ) = K∑ g=1 P ( b|s; ρ(g) )",4.2 Predicting user behavior,[0],[0]
"· P (g|π) (6)
",4.2 Predicting user behavior,[0],[0]
"Thus, we have a group-aware replacement for (1).",4.2 Predicting user behavior,[0],[0]
"Furthermore, in the interactive setting of a dialogue system, we may have multiple opportunities to interact with the same user u. We can then develop a more precise estimate of u’s group based on their responses to previous stimuli.",4.2 Predicting user behavior,[0],[0]
Say that we have made the previous observations D(u) =,4.2 Predicting user behavior,[0],[0]
"{〈s1, b1〉, . . .",4.2 Predicting user behavior,[0],[0]
", 〈sN , bN 〉} for user u.",4.2 Predicting user behavior,[0],[0]
"Then we can use Bayes’ theorem to calculate a posterior estimate for u’s group membership:
P ( g|D(u); θ ) ∝",4.2 Predicting user behavior,[0],[0]
P ( D(u)|ρ(g) ) ·,4.2 Predicting user behavior,[0],[0]
"P (g|π) (7)
",4.2 Predicting user behavior,[0],[0]
This posterior balances whether a group is likely in general against whether members of that group behave as u does.,4.2 Predicting user behavior,[0],[0]
"We can use Pu(g) = P ( g|D(u); θ ) as our new estimate for the group membership probabilities for u and replace (6) with: P ( b|s,D(u); θ ) =
K∑ g=1 P ( b|s; ρ(g) ) ·",4.2 Predicting user behavior,[0],[0]
"Pu(g) (8)
for the next interaction with u. An NLG system can therefore adapt to each new user over time.",4.2 Predicting user behavior,[0],[0]
"Before the first interaction with u, it has no specific information about u and models u’s behavior based on (6).",4.2 Predicting user behavior,[0],[0]
"As the system interacts with u repeatedly, it collects observationsD(u) about u’s behavior.",4.2 Predicting user behavior,[0],[0]
"This allows it to calculate an increasingly accurate posterior Pu(g) = P ( g|D(u); θ ) of u’s group membership, and thus generate utterances which are more and more suitable to u using (8).",4.2 Predicting user behavior,[0],[0]
"So far we have not discussed how to find settings for the parameters θ = π, ρ(1), . . .",5 Training,[0],[0]
", ρ(K), which define our probability model.",5 Training,[0],[0]
"The key challenge for training is the fact that we want to be able to train while treating the assignment of users to groups as unobserved.
",5 Training,[0],[0]
"We will use a maximum a posteriori estimate for θ, i.e., the setting which maximizes (3) when D is our training set.",5 Training,[0],[0]
"We will first discuss how to pick parameters to maximize only the left part of (3), i.e., the data likelihood, since this is the part that involves unobserved variables.",5 Training,[0],[0]
We will then discuss handling the parameter prior in section 5.2.,5 Training,[0],[0]
"Gradient descent based methods (Nocedal and Wright, 2006) exist for finding the parameter settings which maximize the likelihood for log-linear
models, under the conditions that all relevant variables are observed in the training data.",5.1 Expectation Maximization,[0],[0]
"If group assignments were given, gradient computations, and therefore gradient based maximization, would be straightforward for our model.",5.1 Expectation Maximization,[0],[0]
"One algorithm specifically designed to solve maximization problems with unknown variables by reducing them to the case where all variables are observed, is the expectation maximization (EM) algorithm (Neal and Hinton, 1999).",5.1 Expectation Maximization,[0],[0]
"Instead of maximizing the data likelihood from (3) directly, EM equivalently maximizes the log-likelihood, given in (4).",5.1 Expectation Maximization,[0],[0]
"It helps us deal with unobserved variables by introducing “pseudo-observations” based on the expected frequency of the unobserved variables.
",5.1 Expectation Maximization,[0],[0]
"EM is an iterative algorithm which produces a sequence of parameter settings θ(1), . . .",5.1 Expectation Maximization,[0],[0]
", θ(n).",5.1 Expectation Maximization,[0],[0]
Each will achieve a larger value for (4).,5.1 Expectation Maximization,[0],[0]
Each new setting is generated in two steps: (1) an lower bound on the log-likelhood is generate and (2) the new parameter setting is found by optimizing this lower bound.,5.1 Expectation Maximization,[0],[0]
"To find the lower bound we compute the probability for every possible value the unobserved variables could have had, based on the observed variables and the parameter setting θ(i−1) from the last iteration step.",5.1 Expectation Maximization,[0],[0]
"Then the lower bound essentially assumes that each assignment was seen with a frequency equal to these probabilities - these are the “pseudo-observations”.
",5.1 Expectation Maximization,[0],[0]
In our model the unobserved variables are the assignments of users to groups.,5.1 Expectation Maximization,[0],[0]
"The probability of seeing each user u assigned to a group, given all the data D(u) and the model parameters from the last iteration θ(i−1), is simply the posterior group membership probability P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
The lower bound is then given by (5).,5.1 Expectation Maximization,[0],[0]
"This is the sum of the log probabilities of the data points under each group model, weighted by P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
We can now use gradient descent techniques to optimize this lower bound.,5.1 Expectation Maximization,[0],[0]
To fully implement EM we need a way to maximize (5).,5.1.1 Maximizing the Lower Bound,[0],[0]
"This can be achieved with gradient based methods such as L-BFGS (Nocedal and Wright, 2006).",5.1.1 Maximizing the Lower Bound,[0],[0]
Here the gradient refers to the vector of all partial derivatives of the function with respect to each dimension of θ.,5.1.1 Maximizing the Lower Bound,[0],[0]
"We therefore need to calculate these partial derivatives.
",5.1.1 Maximizing the Lower Bound,[0],[0]
There are existing implementations of the gradient computations our base model such as in Engonopoulos et al. (2013).,5.1.1 Maximizing the Lower Bound,[0],[0]
"The gradients of (5)
for each of the ρ(g) is simply the gradient for the base model on each datapoint d weighted by P ( g|D(u); θ(i−1) )",5.1.1 Maximizing the Lower Bound,[0],[0]
"if d ∈ Du, i.e., the probability that the user u from which the datapoint originates belongs to group g. We can therefore compute the gradients needed for each ρ(g) by using implementations developed for the base model.
",5.1.1 Maximizing the Lower Bound,[0],[0]
"We also need gradients for the parameters in π, which are only used in our extended model.",5.1.1 Maximizing the Lower Bound,[0],[0]
"We can use the rules for computing derivatives to find, for each dimension g:
∂UL(θ) ∂πg = ∑ u∈U Pu(g)− exp (πg)∑K g′=1 exp ( πg′ )
where Pu(g) = P ( g|D(u); θ(i−1) ) .",5.1.1 Maximizing the Lower Bound,[0],[0]
Using these gradients we can use L-BFGS to maximize the lower bound and implement the EM iteration.,5.1.1 Maximizing the Lower Bound,[0],[0]
So far we have discussed maximization only for the likelihood without accounting for the prior probabilities for every parameter.,5.2 Handling the Parameter Prior,[0],[0]
"To obtain our full training objective we add the log of the right hand side of (3):
log N (π|0, σ(π)) · K∏",5.2 Handling the Parameter Prior,[0],[0]
g=1 N,5.2 Handling the Parameter Prior,[0],[0]
"( ρ(g)|0, σ(ρ) )",5.2 Handling the Parameter Prior,[0],[0]
"i.e., the parameter prior, to (4) and (5).",5.2 Handling the Parameter Prior,[0],[0]
The gradient contribution from these priors can be computed with standard techniques.,5.2 Handling the Parameter Prior,[0],[0]
"We can now implement an EM loop, which maximizes (3) as follows: we randomly pick an initial value θ(0) for all parameters.",5.3 Training Iteration,[0],[0]
Then we repeatedly compute the P ( g|D(u); θ(i−1) ) values and maximize the lower bound using L-BFGS to find θ(i).,5.3 Training Iteration,[0],[0]
This EM iteration is guaranteed to eventually converge towards a local optimum of our objective function.,5.3 Training Iteration,[0],[0]
"Once change in the objective falls below a pre-defined threshold, we keep the final θ setting.
",5.3 Training Iteration,[0],[0]
"For our implementation we make a small improvement to the approach: L-BFGS is itself an iterative algorithm and instead of running it until convergence every time we need to find a new θ(i), we only let it take a few steps.",5.3 Training Iteration,[0],[0]
"Even if we just took a single L-BFGS step in each iteration, we would still obtain a correct algorithm (Neal and
Hinton, 1999) and this has the advantage that we do not spend time trying to find a θ(i) which is a good fit for the likely poor group assignments P ( g|D(u); θ(i−1) )",5.3 Training Iteration,[0],[0]
we obtain from early parameter estimates.,5.3 Training Iteration,[0],[0]
Our model can be used in any component of a dialog system for which a prediction of the user’s behavior is needed.,6 Evaluation,[0],[0]
"In this work, we evaluate it in two NLG-related prediction tasks: RE production and RE comprehension.",6 Evaluation,[0],[0]
In both cases we evaluate the ability of our model to predict the user’s behavior given a stimulus.,6 Evaluation,[0],[0]
"We expect our user-group model to gradually improve its prediction accuracy compared to a generic baseline without user groups as it sees more observations from a given user.
",6 Evaluation,[0],[0]
In all experiments described below we set the prior variances σγ = 1.0 and σπ = 0.3 after trying out values between 0.1 and 10 on the training data of the comprehension experiment.,6 Evaluation,[0],[0]
"Task The task of RE generation can be split in two steps: attribute selection, the selection of the visual attributes to be used in the RE such as color, size, relation to other objects and surface realization, the generation of a full natural language expression.",6.1 RE production,[0],[0]
"We focus here on attribute selection: given a visual scene and a target object, we want to predict the set of attributes of the target object that a human speaker would use in order to describe it.",6.1 RE production,[0],[0]
"Here we treat attribute selection in terms of individual classification decisions on whether to use each attribute, as described in Section 3.",6.1 RE production,[0],[0]
"More specifically, we focus on predicting whether the speaker will use a spatial relation to another object (“landmark”).",6.1 RE production,[0],[0]
"Our motivation for choosing this attribute stems from the fact that previous authors (Viethen and Dale, 2008; Ferreira and Paraboni, 2014) have found substantial variation between different users with respect to their preference towards using spatial relations.
",6.1 RE production,[0],[0]
"Data We use the GRE3D3 dataset of humanproduced REs (Viethen and Dale, 2010), which contains 630 descriptions for 10 scenes collected from 63 users, each describing the same target object in each scene.",6.1 RE production,[0],[0]
35% of the descriptions in this corpus use a spatial relation.,6.1 RE production,[0],[0]
"An example of such a scene can be seen in Fig. 3.
",6.1 RE production,[0],[0]
"Models We use two baselines for comparison:
Basic: The state-of-the-art model on this task with this dataset, under the assumption that users are seen in training, is presented in Ferreira and Paraboni (2014).",6.1 RE production,[0],[0]
"They define context features such as type of relation between the target object and its landmark, number of object of the same color or size, etc., then train an SVM classifier to predict the use of each attribute.",6.1 RE production,[0],[0]
"We recast their model in terms of a log-linear model with the same features, to make it fit with the setup of Section 3.
Basic++: Ferreira and Paraboni (2014) also take speaker features into account.",6.1 RE production,[0],[0]
"We do not use speaker identity and the speaker’s attribute frequency vector, because we only evaluate on unseen users.",6.1 RE production,[0],[0]
"We do use their other speaker features (age, gender), together with Basic’s context features; this gives us a strong baseline which is aware of manually annotated user group characteristics.
",6.1 RE production,[0],[0]
"We compare these baselines to our Group model for values of K between 1 and 10, using the exact same features as Basic.",6.1 RE production,[0],[0]
"We do not use the speaker features of Basic++, because we do not want to rely on manually annotated groups.",6.1 RE production,[0],[0]
"Note that our results are not directly comparable with those of Ferreira and Paraboni (2014), because of a different training-test split: their model requires having seen speakers in training, while we explicitly want to test our model’s ability to generalize to unseen users.
",6.1 RE production,[0],[0]
"Experimental setup We evaluate using crossvalidation, splitting the folds so that all speakers we see in testing are previously unseen in training.",6.1 RE production,[0],[0]
We use 9 folds in order to have folds of the same size (each containing 70 descriptions coming from 7 speakers).,6.1 RE production,[0],[0]
At each iteration we train on 8 folds and test on the 9th.,6.1 RE production,[0],[0]
"At test time, we process each test instance iteratively: we first predict for each instance whether the user uwould use a spatial relation or not and test our prediction; we then add the
actual observation from the corpus to the set D(u) of observations for this particular user, in order to update our estimate about their group membership.
",6.1 RE production,[0],[0]
"Results Figure 4 shows the test F1-score (microaveraged over all folds) as we increase the number of groups, compared to the baselines.",6.1 RE production,[0],[0]
"For our Group models, these are averaged over all interactions with the user.",6.1 RE production,[0],[0]
"Our model gets F1-scores between 0.69 and 0.76 for all values ofK > 1, outperforming both Basic (0.22) and Basic++ (0.23).
",6.1 RE production,[0],[0]
"In order to take a closer look at our model’s behavior, we also show the accuracy of our model as it observes more instances at test time.",6.1 RE production,[0],[0]
We compare the model with K = 3 groups against the two baselines.,6.1 RE production,[0],[0]
"Figure 5 shows that the group model’s F1-score increases dramatically after the first two observations and then stays high throughout the test phase, always outperforming both baselines by at least 0.37 F1-score points after the first observation.",6.1 RE production,[0],[0]
The baseline models of course are not expected to improve with time; fluctuations are due to differences between the visual scenes.,6.1 RE production,[0],[0]
"In the same figure, we plot the evolution of the entropy of the group model’s posterior distribution over the groups (see (7)).",6.1 RE production,[0],[0]
"As expected, the model is highly uncertain at the beginning of the test phase about which group the user belongs to, then gets more and more certain as the set D(u) of observations from that user grows.",6.1 RE production,[0],[0]
Task Our next task is to predict the referent to which a user will resolve an RE in the context of a visual scene.,6.2 RE comprehension,[0],[0]
"Our model is given a stimulus s = (r, c) consisting of an instruction containing an RE r and a visual context c and outputs a probability distribution over all possible referents b.",6.2 RE comprehension,[0],[0]
"Such a model can be used by a probabilistic RE generator to select an RE which is highly likely to be correctly understood by the user or predict potential
misunderstandings (see Section 3).
",6.2 RE comprehension,[0],[0]
Data We use the GIVE-2.5 corpus for training and the GIVE-2 corpus for testing our model (the same used by Engonopoulos et al. (2013)).,6.2 RE comprehension,[0],[0]
These contain recorded observations of dialog systems giving instructions to users who play a game in a 3D environment.,6.2 RE comprehension,[0],[0]
"Each instruction contains an RE r, which is recorded in the data together with the visual context c at the time the instruction was given.",6.2 RE comprehension,[0],[0]
The object b which the user understood as the referent of the RE is inferred by the immediately subsequent action of the user.,6.2 RE comprehension,[0],[0]
"In total, we extracted 2927 observations by 403 users from GIVE-25 and 5074 observations by 563 users from GIVE-2.
",6.2 RE comprehension,[0],[0]
Experimental setup We follow the training method described in Section 3.,6.2 RE comprehension,[0],[0]
"At test time, we present the observations from each user in the order they occur in the test data; for each stimulus, we ask our models to predict the referent a which the user understood to be the referent of the RE, and compare with the recorded observation.",6.2 RE comprehension,[0],[0]
"We subsequently add the recorded observation to the dataset for the user and continue.
",6.2 RE comprehension,[0],[0]
"Models As a baseline, we use the Basic model described in Section 3, with the features of the “semantic” model of Engonopoulos et al. (2013).",6.2 RE comprehension,[0],[0]
"Those features capture information about the objects in the visual scene (e.g. salience) and some basic semantic properties of the RE (e.g. color, position).",6.2 RE comprehension,[0],[0]
"We use those features for our Group model as well, and evaluate for K between 1 and 10.
Results on GIVE data Basic had a test accuracy of 72.70%, which was almost identical with the accuracy of our best Group model for K = 6 (72.78%).",6.2 RE comprehension,[0],[0]
This indicates that our group model does not differentiate between users.,6.2 RE comprehension,[0],[0]
"Indeed, after training, the 6-group model assigns 81% prior probabil-
ity to one of the groups, and effectively gets stuck with this assignment while testing; the mean entropy of the posterior group distribution only falls from an initial 1.1 to 0.7 after 10 observations.
",6.2 RE comprehension,[0],[0]
We speculate that the reason behind this is that the features we use are not sensitive enough to capture the differences between the users in this data.,6.2 RE comprehension,[0],[0]
"Since our model relies completely on observable behavior, it also relies on the ability of the features to make relevant distinctions between users.
",6.2 RE comprehension,[0],[0]
"Results on synthetic data In order to test this hypothesis, we made a synthetic dataset based on the GIVE datasets with 1000 instances from 100 users, in the following way: for each user, we randomly selected 10 scenes from GIVE-2, and replaced the target the user selected, so that half of the users always select the target with the highest visual salience, and the other half always select the one with the lowest.",6.2 RE comprehension,[0],[0]
"Our aim was to test whether our model is capable of identifying groups when they are clearly present in the data and exhibit differences which our features are able to capture.
",6.2 RE comprehension,[0],[0]
We evaluated the same models in a 2-fold crossvalidation.,6.2 RE comprehension,[0],[0]
Figure 6 shows the prediction accuracy for Basic and the Group models for K from 1 to 10.,6.2 RE comprehension,[0],[0]
"All models for K > 1 clearly outperform the baseline model: the 2-group model gets 62.3% vs 28.6% averaged over all test examples, while adding more than two groups does not further improve the accuracy.",6.2 RE comprehension,[0],[0]
We also show in Figure 7 the evolution of the accuracy asD(u) grows: the Group model with K = 2 reaches a 64% testing accuracy after seeing two observations from the same user.,6.2 RE comprehension,[0],[0]
"In the same figure, the entropy of the posterior distribution over groups (see production experiment) falls towards zero as D(u) grows.",6.2 RE comprehension,[0],[0]
"These results show that our model is capable of correctly assigning a user to the group they belong to, once the features are adequate for distinguishing between different user behaviors.",6.2 RE comprehension,[0],[0]
"Our model was shown to be successful in discovering groups of users with respect to their behavior, within datasets which present discernible user variation.",6.3 Discussion,[0],[0]
"In particular, if all listeners are influenced in a similar way by e.g. the visual salience of an object, then the group model cannot learn different weights for the visual salience feature; if this happens for all available features, there are effectively no groups for our model to discover.
",6.3 Discussion,[0],[0]
"Once the groups have been discovered, our model can then very quickly distinguish between them at test time.",6.3 Discussion,[0],[0]
This is reflected in the steep performance improvement even after the first user observation in both the real data experiment in 6.1 and the synthetic data experiment in 6.2.,6.3 Discussion,[0],[0]
"We have presented a probabilistic model for NLG which predicts the behavior of individual users of a dialog system by dynamically assigning them to user groups, which were discovered during training2.",7 Conclusion,[0],[0]
"We showed for two separate NLG-related tasks, RE production and RE comprehension, how our model, after being trained with data that is not annotated with user groups, can quickly adapt to unseen users as it gets more observations from them in the course of a dialog and makes increasingly accurate predictions about their behavior.
",7 Conclusion,[0],[0]
"Although in this work we apply our model to tasks related to NLG, nothing hinges on this choice; it can also be applied to any other dialog-related prediction task where user variation plays a role.",7 Conclusion,[0],[0]
"In the future, we will also try to apply the basic principles of our user group approach to more sophisticated underlying models, such as neural networks.
2Our code and data is available in https://bit.ly/ 2jIu1Vm",7 Conclusion,[0],[0]
We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups.,abstractText,[0],[0]
"In contrast to previous work, these user groups are not specified beforehand, but learned in training.",abstractText,[0],[0]
"We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.",abstractText,[0],[0]
Discovering User Groups for Natural Language Generation,title,[0],[0]
"As originally defined by Pearl (1988), Bayesian networks express joint distributions over finite sets of random variables as products of conditional distributions.",1. Introduction,[0],[0]
"Probabilistic programming languages (PPLs) (Koller et al., 1997; Milch et al., 2005a; Goodman et al., 2008; Wood et al., 2014b) apply the same idea to potentially infinite sets of variables with general dependency structures.",1. Introduction,[0],[0]
"Thanks to their expressive power, PPLs have been used to solve many real-world applications, including Captcha (Le et al., 2017), seismic monitoring (Arora et al., 2013), 3D pose estimation (Kulkarni et al., 2015), generating design suggestions (Ritchie et al., 2015), concept learning (Lake et al., 2015), and cognitive science applications (Stuhlmüller & Goodman, 2014).
",1. Introduction,[0],[0]
"In practical applications, we often have to deal with a mix-
1University of California, Berkeley 2Arizona State University 3Vicarious Inc. 4Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Yi Wu <jxwuyi@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
ture of continuous and discrete random variables.",1. Introduction,[0],[0]
"Existing PPLs support both discrete and continuous random variables, but not discrete-continuous mixtures, i.e., variables whose distributions combine discrete and continuous elements.",1. Introduction,[0],[0]
"Such variables are fairly common in practical applications: sensors that have thresholded limits, e.g. thermometers, weighing scales, speedometers, pressure gauges; or a hybrid sensor that can report a either real value or an error condition.",1. Introduction,[0],[0]
"The occurrence of such variables has been noted in many other applications from a wide range of scientific domains (Kharchenko et al., 2014; Pierson & Yau, 2015; Gao et al., 2017).
",1. Introduction,[0],[0]
"Many PPLs have a restricted syntax that forces the expressed random variables to be either discrete or continuous, including WebPPL (Goodman & Stuhlmüller, 2014), Edward (Tran et al., 2016), Figaro (Pfeffer, 2009) and Stan (Carpenter et al., 2016).",1. Introduction,[0],[0]
"Even for PPLs whose syntax allows for mixtures of discrete and continuous variables, such as BLOG (Milch et al., 2005a), Church (Goodman, 2013), Venture (Mansinghka et al., 2014) and Anglican (Wood et al., 2014a), the underlying semantics of these PPLs implicitly assumes the random variables are not mixtures.",1. Introduction,[0],[0]
"Moreover, the inference algorithms associated with the semantics inherit the same assumption and can produce incorrect results when discrete-continuous mixtures are used.
",1. Introduction,[0],[0]
"Consider the following GPA example: a two-variable Bayes net Nationality → GPA where the nationality follows a binary distribution
P (Nationality = USA) = P (Nationality = India)",1. Introduction,[0],[0]
"= 0.5
and the conditional probabilities are discrete-continuous mixtures
GPA|Nationality = USA ∼0.01 · 1 {GPA = 4}+ 0.99 · Unif(0, 4),
GPA|Nationality = India ∼0.01 · 1 {GPA = 10}+ 0.99 · Unif(0, 10).
",1. Introduction,[0],[0]
This is a typical scenario in practice because many top students have perfect GPAs.,1. Introduction,[0],[0]
Now suppose we observe a student with a GPA of 4.0.,1. Introduction,[0],[0]
Where do they come from?,1. Introduction,[0],[0]
"If the student is Indian, the probability of any singleton set {g}
where 0 <",1. Introduction,[0],[0]
"g < 10 is zero, as this range has a probability density.",1. Introduction,[0],[0]
"On the other hand if the student is American, the set {4} has the probability 0.01.",1. Introduction,[0],[0]
"Thus, by Bayes theorem, P (Nationality = USA|GPA = 4) = 1, which means the student must be from the USA.
",1. Introduction,[0],[0]
"However, if we run the default Bayesian inference algorithm for this problem in PPLs, e.g., the standard importance sampling algorithm (Milch et al., 2005b), a sample that picks India receives a density weight of 0.99/10.0 = 0.099, whereas one that picks USA receives a discrete-mass weight of 0.01.",1. Introduction,[0],[0]
"Since the algorithm does not distinguish probability density and mass, it will conclude that the student is very probably from India, which is far from the truth.
",1. Introduction,[0],[0]
"We can fix the GPA example by considering a density weight infinitely smaller than a discrete-mass weight (Nitti et al., 2016; Tolpin et al., 2016).",1. Introduction,[0],[0]
"However, the situation becomes more complicated when involving more than one evidence variable, e.g., GPAs over multiple semesters for students who may study in both countries.",1. Introduction,[0],[0]
Vector-valued variables also cause problems—does a point mass in three dimensions count more or less than a point mass in two dimensions?,1. Introduction,[0],[0]
"These practical issues motivate the following two tasks:
• Inherit all the existing properties of PPL semantics and extend it to handle random variables with mixed discrete and continuous distributions;
• Design provably correct inference algorithms for the extended semantics.
",1. Introduction,[0],[0]
"In this paper, we carry out all these two tasks and implement the extended semantics as well as the new algorithms in a widely used PPL, Bayesian Logic (BLOG) (Milch et al., 2005a).",1. Introduction,[0],[0]
Measure-Theoretical Bayesian Nets (MTBNs) Measure theory can be applied to handle discrete-continuous mixtures or even more abstract measures.,1.1. Main Contributions,[0],[0]
"In this paper, we define a generalization of Bayesian networks called measure-theoretic Bayesian networks (MTBNs) and prove that every MTBN represents a unique measure on the input space.",1.1. Main Contributions,[0],[0]
"We then show how MTBNs can provide a more general semantic foundation for PPLs.
More concretely, MTBNs support (1) random variables with infinitely (even uncountably) many parents, (2) random variables valued in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed), (3) establishment of conditional independencies implied by an infinite graph, and (4) open-universe semantics in terms of the possible worlds in the vocabulary of the model.
",1.1. Main Contributions,[0],[0]
Inference Algorithms,1.1. Main Contributions,[0],[0]
"We propose a provably correct inference algorithm, lexicographic likelihood weighting (LLW), for general MTBNs with discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
"In addition, we propose LPF, a particle-filtering variant of LLW for sequential Monte Carlo (SMC) inference on state-space models.
",1.1. Main Contributions,[0],[0]
"Incorporating MTBNs into an existing PPL We incorporate MTBNs into BLOG with simple modifications and then define the generalized BLOG language, measuretheoretic BLOG, which formally supports arbitrary distributions, including discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
We prove that every generalized BLOG model corresponds to a unique MTBN.,1.1. Main Contributions,[0],[0]
"Thus, all the desired theoretical properties of MTBNs can be carried to measure-theoretic BLOG.",1.1. Main Contributions,[0],[0]
We also implement the LLW and LPF algorithms in the backend of measure-theoretic BLOG and use three representative examples to show their effectiveness.,1.1. Main Contributions,[0],[0]
This paper is organized as follows.,1.2. Organization,[0],[0]
We first discuss related work in Section 2.,1.2. Organization,[0],[0]
"In Section 3, we formally define measure-theoretic Bayesian nets and study their theoretical properties.",1.2. Organization,[0],[0]
Section 4 describes the LLW and LPF inference algorithms for MTBNs with discrete-continuous mixtures and establishes their correctness.,1.2. Organization,[0],[0]
"In Section 5, we introduce the measure-theoretic extension of BLOG and study its theoretical foundations for defining probabilistic models.",1.2. Organization,[0],[0]
"In Section 6, we empirically validate the generalized BLOG system and the new inference algorithms on three representative examples.",1.2. Organization,[0],[0]
"The motivating GPA example has been also discussed as a special case under some other PPL systems (Tolpin et al., 2016; Nitti et al., 2016).",2. Related Work,[0],[0]
Tolpin et al. (2016) and Nitti et al. (2016) proposed different solutions specific to this example but did not address the general problems of representation and inference with random variables with mixtures of discrete and continuous distributions.,2. Related Work,[0],[0]
"In contrast, we present a general formulation with provably correct inference algorithms.
",2. Related Work,[0],[0]
"Our approach builds upon the foundations of the BLOG probabilistic programming language (Milch, 2006).",2. Related Work,[0],[0]
We use a measure theoretic formulation to generalize the syntax and semantics of BLOG to random variables that may have infinitely many parents and mixed continuous and discrete distributions.,2. Related Work,[0],[0]
"The BLP framework Kersting & De Raedt (2007) unifies logic programming with probability models, but requires each random variable to be influenced by a finite set of random variables in order to define the semantics.",2. Related Work,[0],[0]
"This amounts to requiring only finitely many ances-
tors of each random variable.",2. Related Work,[0],[0]
Choi et al. (2010) present an algorithm for carrying out lifted inference over models with purely continuous random variables.,2. Related Work,[0],[0]
"They also require parfactors to be functions over finitely many random variables, thus limiting the set of influencing variables for each node to be finite.",2. Related Work,[0],[0]
Gutmann et al. (2011a) also define densities over finite dimensional vectors.,2. Related Work,[0],[0]
"In a relatively more general formulation (Gutmann et al., 2011b) define the distribution of each random variable using a definite clause, which corresponds to the limitation that each random variable (either discrete or continuous) has finitely many parents.",2. Related Work,[0],[0]
Frameworks building on Markov networks also have similar restrictions.,2. Related Work,[0],[0]
"Wang & Domingos (2008) only consider networks of finitely many random variables, which can have either discrete or continuous distributions.",2. Related Work,[0],[0]
"Singla & Domingos (2007) extend Markov logic to infinite (non-hybrid) domains, provided that each random variable has only finitely many influencing random variables.
",2. Related Work,[0],[0]
"In contrast, our approach not only allows models with arbitrarily many random variables with mixed discrete and continuous distributions, but each random variable can also have arbitrarily many parents as long as all ancestor chains are finite (but unbounded).",2. Related Work,[0],[0]
"The presented work constitutes a rigorous framework for expressing probability models with the broadest range of cardinalities (uncountably infinite parent sets) and nature of random variables (discrete, mixed, and even arbitrary measure spaces), with clear semantics in terms of first-order possible worlds and the generalization of conditional independences on such models.
",2. Related Work,[0],[0]
"Lastly, there are also other works using measure-theoretic approaches to analyze the semantics properties of probabilistic programs but with different emphases, such as the commutativity (Staton, 2017), design choices for monad structures (Ramsey, 2016) and computing a disintegration (Shan & Ramsey, 2017).",2. Related Work,[0],[0]
"In this section, we introduce measure-theoretic Bayesian networks (MTBNs) and prove that an MTBN represents a unique measure with desired theoretical properties.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We assume familiarity with measure-theoretic approaches to probability theory.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Some background is included in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A.
We begin with some necessary definitions of graph theory.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.1.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A digraph G is a pair G = (V,E) of a set of vertices V , of any cardinality, and a set of directed edges E ⊆ V × V .",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The notation u→ v denotes (u, v) ∈ E, and u 7→ v denotes the existence of a path from u to v in G.
Definition 3.2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"A vertex v ∈ V is a root vertex if there are no incoming edges to it, i.e., there is no u ∈ V such that u → v. Let pa(v)",3. Measure-Theoretic Bayesian Networks,[0],[0]
"= {u ∈ V : u → v} denote the set of parents of a vertex v ∈ V , and nd(v) =",3. Measure-Theoretic Bayesian Networks,[0],[0]
"{u ∈ V : not v 7→
u} denote its set of non-descendants.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.3.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A well-founded digraph (V,E) is one with no countably infinite ancestor chain v0 ← v1 ← v2 ← . . .",3. Measure-Theoretic Bayesian Networks,[0],[0]
".
",3. Measure-Theoretic Bayesian Networks,[0],[0]
This is the natural generalization of a finite directed acyclic graph to the infinite case.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Now we are ready to give the key definition of this paper.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.4.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A measure-theoretic Bayesian network M = (V,E, {Xv}v∈V , {Kv}v∈V ) consists of (a) a wellfounded digraph (V,E) of any cardinality, (b) an arbitrary measurable space Xv for each v ∈ V , and (c) a probability kernel Kv from ∏ u∈pa(v) Xu to Xv for each v ∈ V .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"By definition, MTBNs allow us to define very general and abstract models with the following two major benefits:
1.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables with infinitely (even uncountably) many parents because MTBN is defined on a well-founded digraph.
2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Next, we related MTBN to a probability measure.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Fix an MTBN M = (V,E, {Xv}v∈V , {Kv}v∈V ).",3. Measure-Theoretic Bayesian Networks,[0],[0]
For U ⊆ V let XU = ∏ u∈U Xu be the product measurable space over variables u ∈ U .,3. Measure-Theoretic Bayesian Networks,[0],[0]
"With this notation, Kv is a kernel from Xpa(v) to Xv.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Whenever W ⊆ U let πUW : XU → XW denote the projection map.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Let XV be our base measurable space upon which we will consider different probability measures µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Let Xv for v ∈ V denote both the underlying set of Xv and the random variable given by the projection πV{v}, and XU for U ⊆ V the underlying space of XU and the random variable given by the projection πVU .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.5.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a measure µ on XV , if for all v ∈ V :
• Xv is conditionally independent of its non-descendants Xnd(v) given its parents Xpa(v).",3. Measure-Theoretic Bayesian Networks,[0],[0]
"• Kv(Xpa(v), A) = Pµ[Xv ∈ A|Xpa(v)] holds almost surely for any A ∈ Xv, i.e., Kv is a version of the conditional distribution of Xv given its parents.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Def. 3.5 captures the generalization of the local properties of Bayes networks – conditional independence and conditional distributions defined by parent-child relationships.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Here we assume the conditional probability exists and is unique.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"This is a mild condition because this holds as long as the probability space is regular (Kallenberg, 2002).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The next theorem shows that MTBNs are well-defined.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Theorem 3.6.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a unique measure µ on XV .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof of theorem 3.6 requires several intermediate results and is presented in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
B.,3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof proceeds by first defining a projective family of measures.,3. Measure-Theoretic Bayesian Networks,[0],[0]
This gives a way to recursively construct our measure µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
We then define a notion of consistency such that every consistent projective family constructs a measure that M represents.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Lastly, we give an explicit characterization of the unique consistent projective family, and thus of the unique measure M represents.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We introduce the lexicographic likelihood weighting (LLW) algorithm for provably correct inference on MTBNs.,4. Generalized Inference Algorithms,[0],[0]
We also present lexicographic particle filter (LPF) for statespace models by adapting LLW for the sequential Monte Carlo (SMC) framework.,4. Generalized Inference Algorithms,[0],[0]
"Suppose we have an MTBN with finitely many random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XN , and that, without loss of generality, we observe real-valued random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XM for M < N as evidence.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose the distribution of Xi given its parents Xpa(i) is a mixture between a density fi(xi|xpa(i)) with respect to the Lebesgue measure and a discrete distribution Fi(xi|xpa(i)), i.e., for any > 0, we have P (Xi ∈",4.1. Lexicographic likelihood weighting,[0],[0]
"[xi − , xi]|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"=∑ x∈[xi− ,xi] Fi(xi|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
+ ∫,4.1. Lexicographic likelihood weighting,[0],[0]
xi xi− fi(x|Xpa(i)),4.1. Lexicographic likelihood weighting,[0],[0]
dx.,4.1. Lexicographic likelihood weighting,[0],[0]
This implies that Fi(xi|xpa(i)) is nonzero for at most countably many values,4.1. Lexicographic likelihood weighting,[0],[0]
xi.,4.1. Lexicographic likelihood weighting,[0],[0]
"If Fi is nonzero for finitely many points, it can be represented by a list of those points and their values.
",4.1. Lexicographic likelihood weighting,[0],[0]
"Lexicographic Likelihood Weighting (LLW) extends the classical likelihood weighting (Milch et al., 2005b) to this setting.",4.1. Lexicographic likelihood weighting,[0],[0]
"It visits each node of the graph in topological order, sampling those variables that are not observed, and accumulating a weight for those that are observed.",4.1. Lexicographic likelihood weighting,[0],[0]
"In particular, at an evidence variable Xi we update a tuple (d,w) of the number of densities and a weight, initially (0, 1), by:
(d,w)← { (d,wFi(xi|xpa(i))) Fi(xi|xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"> 0, (d+ 1, wfi(xi|xpa(i)))",4.1. Lexicographic likelihood weighting,[0],[0]
"otherwise.
(1)
Finally, having K samples x(1), . .",4.1. Lexicographic likelihood weighting,[0],[0]
.,4.1. Lexicographic likelihood weighting,[0],[0]
", x(K) by this process and accordingly a tuple (d(i), w(i)) for each sample x(i), let d∗ = mini:w(i) 6=0 d
(i) and estimate E[f(X)|X1:M ] by∑ {i:d(i)=d∗} w
(i) f(x(i))∑",4.1. Lexicographic likelihood weighting,[0],[0]
{i:d(i)=d∗} w (i) .,4.1. Lexicographic likelihood weighting,[0],[0]
"(2)
The algorithm is summarised in Alg. 1",4.1. Lexicographic likelihood weighting,[0],[0]
The next theorem shows this procedure is consistent.,4.1. Lexicographic likelihood weighting,[0],[0]
Theorem 4.1.,4.1. Lexicographic likelihood weighting,[0],[0]
"LLW is consistent: (2) converges almost surely to E[f(X)|X1:M ].
",4.1. Lexicographic likelihood weighting,[0],[0]
"Algorithm 1 Lexicographic Likelihood Weighting Require: densities f , masses F , evidences E, and K.
for i = 1 . .",4.1. Lexicographic likelihood weighting,[0],[0]
.K,4.1. Lexicographic likelihood weighting,[0],[0]
"do sample all the ancestors of E from prior compute (d(i), w(i)) by Eq.",4.1. Lexicographic likelihood weighting,[0],[0]
(1) end for d?,4.1. Lexicographic likelihood weighting,[0],[0]
"← mini:w(i) 6=0 d(i)
",4.1. Lexicographic likelihood weighting,[0],[0]
"Return (∑
i:d(i)=d? w (i)f(x(i))
) /",4.1. Lexicographic likelihood weighting,[0],[0]
(∑ i:d(i)=d?,4.1. Lexicographic likelihood weighting,[0],[0]
"w (i) )
",4.1. Lexicographic likelihood weighting,[0],[0]
"In order to prove Theorem 4.1, the main technique we adopt is to use a more restricted algorithm, the Iterative Refinement Likelihood Weighting (IRLW) as a reference.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose we want to approximate the posterior distribution of an X -valued random variable X conditional on a Yvalued random variable Y , for arbitrary measure spaces X and Y .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In general, there is no notion of a probability density of Y given X for weighing samples.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If, however, we could make a discrete approximation Yt of Y then we could weight samples by the probability P",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
[Yt = yt|X].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If we increase the accuracy of the approximation with the number of samples, this should converge in the limit.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"We show this is possible, if we are careful about how we approximate:
Definition 4.2.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
An approximation scheme for a measurable space Y consists of a measurable spaceA and measurable approximation functions αi :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y → A for i = 1, 2, . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
and αji :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
A → A for i < j such that αj ◦ α,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"i = αi and y can be measurably recovered from the subsequence αt(y), αt+1(y), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"for any t > 0.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"When Y is a real-valued variable we will use the approximation scheme αn(y) = 2−nd2nye where dre denotes the ceiling of r, i.e., the smallest integer no smaller than it.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Observe in this case that P (αn(Y ) = αn(y)),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= P (αn(y)− 2−n < Y ≤ αn(y)) which we can compute from the CDF of Y .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Lemma 4.3.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"IfX,Y are real-valued random variables with E |X| <∞, then limi→∞ E[X|αi(Y )]",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= E[X|Y ].
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Proof.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Let Fi = σ(αi(Y )),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
be the sigma algebra generated by αi(Y ).,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Whenever i ≤,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
we have αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= (αj ◦αji )(Y ) and so Fi ⊆ Fj .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"This means E[X|αi(Y )] = E[X|Fi] is a martingale, so we can use martingale convergence results.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In particular, since E |X| <∞
E[X|Fi]→ E[X|F∞] a.s. and in L1, where F∞ =",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
⋃,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
i Fi is the sigma-algebra generated by {αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
": i ∈ N} (see Theorem 7.23 in (Kallenberg, 2002)).
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y is a measurable function of the sequence (α1(Y ), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"),",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
as limi→∞ αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= Y , and so σ(Y ) ⊆ F∞. By definition
the sequence is a measurable function of Y , and so F∞ ⊆ σ(Y ), and so E[X|F∞] = E[X|Y ] giving our result.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Iterative refinement likelihood weighting (IRLW) samples x(1), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
", x(K)",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
from the prior,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"and evaluates:
∑K i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i))f(x(i))∑K
i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i)) (3)
Using Lemma 4.3, G.12, and G.13, we can show IRLW is consistent.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Theorem 4.4.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
IRLW is consistent: (3) converges almost surely to E[f(X)|Y ].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Now we are ready to prove Theorem 4.1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
Proof of Theorem 4.1.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
We prove the theorem for evidence variables that are leaves It is straightforward to extend the proof when the evidence variables are non-leaf nodes.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Let x be a sample produced by the algorithm with number of densities and weight (d,w).",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
With In = ∏ i=1...,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"M (αn(xi)− 2−n, αn(xi)] a 2−n-cube around x1:M we have
lim n→∞ P (X1:M ∈ In|XM+1:N = xM+1:N )",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"w 2−dn = 1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Using In as an approximation scheme by Def. 4.2, the numerator in the above limit is the weight used by IRLW.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"But given the above limit, using w 2−dn as the weight will give the same result in the limit.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Then if we have K samples, in the limit of n→∞ only those samples x(i) with minimal d(i) will contribute to the estimation, and up to normalization they will contribute weight w(i) to the estimation.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"We now consider inference in a special class of highdimensional models known as state-space models, and show how LLW can be adapted to avoid the curse of dimensionality when used with such models.",4.2. Lexicographic particle filter,[0],[0]
A state-space model (SSM) consists of latent states {Xt}0≤t≤T and the observations {Yt}0≤t≤T with a special dependency structure where pa(Yt) =,4.2. Lexicographic particle filter,[0],[0]
"Xt and pa(Xt) = Xt−1 for 0 < t ≤ T .
SMC methods (Doucet et al., 2001), also knowns as particle filters, are a widely used class of methods for inference on SSMs.",4.2. Lexicographic particle filter,[0],[0]
"Given the observed variables {Yt}0≤t≤T , the posterior distribution P (Xt|Y0:t) is approximated by a set of K particles where each particle x(k)t represents a sample of {Xi}0≤i≤t.",4.2. Lexicographic particle filter,[0],[0]
"Particles are propagated forward through the transition model P (Xt|Xt−1) and resampled at each time step t according to the weight of each particle, which is defined by the likelihood of observation Yt.
",4.2. Lexicographic particle filter,[0],[0]
"Algorithm 2 Lexicographic Particle Filter (LPF) Require: densities f , masses F , evidences Y , and K
for t = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
", T do for k = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
",K do x
(k) t ← sample from transition
compute (d(k), w(k)) by Eq. 4 end for d?",4.2. Lexicographic particle filter,[0],[0]
← mink:w(k) 6=0 d(k) ∀k : d(k) >,4.2. Lexicographic particle filter,[0],[0]
"d?, w(k) ← 0 Output ( w(k)f(x (k) t ) ) / (∑ k w (k) ) resample particles according to w(k)
end for
In the MTBN setting, the distribution of Yt1 given its parent Xt can be a mixture of density ft(yt|xt) and a discrete distribution Ft(yt|xt).",4.2. Lexicographic particle filter,[0],[0]
"Hence, the resampling step in a particle filter should be accordingly modified: following the idea from LLW, when computing the weight of a particle, we enumerate all the observations yt,",4.2. Lexicographic particle filter,[0],[0]
"i at time step t and again update a tuple (d,w), initially (0,1), by
(d,w)← { (d,wFt(yt,i|xt)) Ft(yt,i|xt) > 0, (d+ 1, wft(yt,i|xt))",4.2. Lexicographic particle filter,[0],[0]
otherwise.,4.2. Lexicographic particle filter,[0],[0]
"(4)
We discard all those particles with a non-minimum d value and then perform the normal resampling step.",4.2. Lexicographic particle filter,[0],[0]
"We call this algorithm lexicographical particle filter (LPF), which is summarized in Alg. 2.
",4.2. Lexicographic particle filter,[0],[0]
The following theorem guarantees the correctness of LPF.,4.2. Lexicographic particle filter,[0],[0]
"Its Proof easily follows the analysis for LLW and the classical proof of particle filtering based on importance sampling.
",4.2. Lexicographic particle filter,[0],[0]
Theorem 4.5.,4.2. Lexicographic particle filter,[0],[0]
LPF is consistent: the outputs of Alg.,4.2. Lexicographic particle filter,[0],[0]
2 converges almost surely to {E[f(Xt)|Y0:t]}0≤t≤T .,4.2. Lexicographic particle filter,[0],[0]
In Section 3 and Section 4 we provided the theoretical foundation of MTBN and general inference algorithms.,5. Generalized Probabilistic Programming Languages,[0],[0]
This section describes how to incorporate MTBN into a practical PPL.,5. Generalized Probabilistic Programming Languages,[0],[0]
"We focus on a widely used open-universe PPL, BLOG (Milch, 2006).",5. Generalized Probabilistic Programming Languages,[0],[0]
"We define the generalized BLOG language, the measure-theoretic BLOG, and prove that every well-formed measure-theoretic BLOG model corresponds to a unique MTBN.",5. Generalized Probabilistic Programming Languages,[0],[0]
"Note that our approach also applies to other PPLs2.
",5. Generalized Probabilistic Programming Languages,[0],[0]
1There can be multiple variables observed.,5. Generalized Probabilistic Programming Languages,[0],[0]
"Here the notation Yt denotes {Yt,i}i for conciseness.
",5. Generalized Probabilistic Programming Languages,[0],[0]
"2It has been shown that BLOG has equivalent semantics to other PPLs (Wu et al., 2014; McAllester et al., 2008).",5. Generalized Probabilistic Programming Languages,[0],[0]
Figure 1.,16 query Nationality(David) = USA;,[0],[0]
"A BLOG code for the GPA example.
",16 query Nationality(David) = USA;,[0],[0]
"We begin with a brief description of the core syntax of BLOG, with particular emphasis on (1) number statements, which are critical for expressing open-universe models3, and (2) new syntax for expressing MTBNs, i.e., the Mix distribution.",16 query Nationality(David) = USA;,[0],[0]
Further description of BLOG’s syntax can be found in Li & Russell (2013).,16 query Nationality(David) = USA;,[0],[0]
Fig. 1 shows a BLOG model with measure-theoretic extensions for a multi-student GPA example.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 1 declares two types, Applicant and Country.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 2 defines 3 distinct countries with keyword distinct, New Zealand, India and USA.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Lines 3 to 5 define a number statement, which states that the number of US applicants follows a Poisson distribution with a higher mean than those from New Zealand or India.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 defines an origin function, which maps the object being generated to the arguments that were used in the number statement that was responsible for generating it.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Here Nationality maps applicants to their nationalities.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 and 13 define two random variables by keyword random.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 to 12 state that the GPA of an applicant is distributed as a mixture of weighted discrete and continuous distributions.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"For US applicants, the range of values 0",5.1. Syntax of measure-theoretic BLOG,[0],[0]
< GPA < 4 follows a truncated Gaussian with bounds 0 and 4 (line 9).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
The probability mass outside the range is attributed to the corresponding bounds: P (GPA = 0),5.1. Syntax of measure-theoretic BLOG,[0],[0]
= P,5.1. Syntax of measure-theoretic BLOG,[0],[0]
(,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA = 4) = 10−4 (line 10).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA distributions for other countries are specified similarly.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Line 13 defines a random applicant David.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 15 states that the David’s GPA is observed to be 4 and we query in line 16 whether David is from USA.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Number Statement (line 3 to 5) Fig. 2 shows the syntax of a number statement for Typei.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In this specification, gj are origin functions (discussed below); ȳj are tuples of arguments drawn from x̄ = x1, . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; ϕj are first-order formulas with free variables ȳj ; ēj are tuples of expressions
3The specialized syntax in BLOG to express models with infinite number of variables.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"over a subset of x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; and cj(ēj) specify kernels κj : Π{Xτe :e∈ēj}Xe → N where τe is the type of the expression e.
#Typei(g1 = x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", gk = xk) ∼",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"if ϕ1(ȳ1) then c1(ē1)
else if ϕ2(ȳ2) then c2(ē2)
.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
". .
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"else cm(ēm);
ments can be recovered using the origin functions gj , each of which is declared as:
origin Typej gj(Typei),
where Typej is the type of the argument xj in the number statement of Typei where gj was used.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"The value of the jth variable used in the number statement that generated u, an element of the universe, is given by gj(u).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 in Fig. 1 is an example of origin function.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Mixture Distribution (line 9 to 12),5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In measure-theoretic BLOG, we introduce a new distribution, the mixture distribution (e.g., lines 9-10 in Fig. 1).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"A mixture distribution is specified as:
Mix({c1(ē1)→ w1(ē′), . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", ck(ēk)→ wk(ē′)}), where ci are arbitrary distributions, and wi’s are arbitrary real valued functions that sum to 1 for every possible assignment to their arguments: ∀ē′ ∑ i wi(ē
′) =",5.1. Syntax of measure-theoretic BLOG,[0],[0]
1.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Note that in our implementation of measure-theoretical BLOG, we only allow a Mix distribution to express a mixture of densities and masses for simplifying the system design, although it still possible to express the same semantics without Mix.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
In this section we present the semantics of measure-theoretic BLOG and its theoretical properties.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Every BLOG model implicitly defines a first-order vocabulary consisting of the set of functions and types mentioned in the model.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"BLOG’s semantics are based on the standard, open-universe semantics of first-order logic.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We first define the set of all possible elements that may be generated for a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.1.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of possible elements UM for a BLOG model M with types {τ1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", τk} is ⋃ j∈N{Uj}, where
• U0 = 〈U01 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U0k 〉, U0j = {cj : cj is a distinct τi constant inM} • Ui+1",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= 〈U i+11 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U i+1 k 〉, where U i+1m = U",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"im ∪
{uν,ū,m : ν(x̄) is a number statement of type τm, ū is a tuple of elements of the type of x̄ from U i, m ∈ N}
Def.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"5.1 allows us to define the set of random variables corresponding to a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.2.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of basic random variables for a BLOG modelM, BRV (M), consists of:
• for each number statement ν(x̄), a number variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vν,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] over the standard measurable space N, where ū is of the type of x̄. • for each function f(x̄) and tuple ū from UM of the type of x̄, a function application variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vf [ū] with the measurable space XVf,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] = Xτf , where Xτf is the measurable space corresponding to τf , the return type of f .
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We now define the space of consistent assignments to random variables.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.3.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"An instantiation σ of the basic RVs defined by a BLOG modelM is consistent if and only if:
•",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"For every element uν,v̄,i used in an assignment of the form",5.2. Semantics of measure-theoretic BLOG,[0],[0]
σ(Vf [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
= w or σ(Vν [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= m > 0, σ(Vν",5.2. Semantics of measure-theoretic BLOG,[0],[0]
[v̄]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
≥,5.2. Semantics of measure-theoretic BLOG,[0],[0]
i;,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• For every fixed function symbol f with the interpretation f̃ , σ(Vf [ū]) = f̃(ū); and • For every element uν,ū=〈u1,...,um〉,i, generated by the number statement ν, with origin functions g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm, for every gj ∈ {g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm}, σ(Vgj [uν,ū,i]) = uj .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"That is, origin functions give correct inverse maps.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Lemma 5.4.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Every consistent assignment σ to the basic RVs forM defines a unique possible world in the vocabulary ofM.
The proof of Lemma 5.4 is in Appx.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"F. In the following definition, we use the notation e[ū/x̄] to denote a substitution of every occurrence of the variable xi with ui in the expression e. For any BLOG modelM, let V (M) = BRV (M); for each v ∈ V , Xv is the measurable space corresponding to v. Let E(M) consist of the following edges for every number statement or function application statement of the form s(x̄):
• The edge (Vg[w̄], Vs[ū])",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"if g is a function symbol in M such that g(ȳ) appears in s(x̄), and either g(w̄) = g(ȳ)[ū/x̄] or an occurrence of g(ȳ) in s(x̄) uses quantified variables z1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", zn, ū′ is a tuple of elements of the type of z̄ and g(w̄) = g(ȳ)[ū/x̄][ū′/z̄].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• The edge (Vν [v̄], Vs[ū]), for element uν,v̄,i ∈ ū.
Note that the first set of edges defined in E(M) above may include infinitely many parents for Vs[ū].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Let the dependency statement in the BLOG model M corresponding to a number or function variable Vs[f̄ ] be s. Let expr(s) be the set of expressions used in s.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Each such statement then defines in a straightforward manner, a kernel Ks(ū) : Xexpr(s(ū))",5.2. Semantics of measure-theoretic BLOG,[0],[0]
→ XVs[ū].,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"In order ensure consistent assignments, we include a special value null ∈",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Xτ for each τ,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Figure 3.,9 query hasFakeCoin;,[0],[0]
"BLOG code for the Scale example
inM, and require that Ks(ū)(σ(pa(Vs[ū])), {null}c) = 0",9 query hasFakeCoin;,[0],[0]
whenever σ violates the first condition of consistent assignments (Def. 5.3).,9 query hasFakeCoin;,[0],[0]
"In other words, all the local kernels ensure are locally consistent: variables involving an object uν,ū,i get a non-null assignment only if the assignment to its number statement represents the generation of at least i objects (σ(Vν(ū))",9 query hasFakeCoin;,[0],[0]
≥ i).,9 query hasFakeCoin;,[0],[0]
"Each kernel of the formKs(ū) can be transformed into a kernel Kpa(Vs[ū]) from its parent vertices (representing basic random variables) by composing the kernels determining the truth value of each expression e ∈ expr(v) in terms of the basic random variables, with the kernel KeVs[ū].",9 query hasFakeCoin;,[0],[0]
Let κ(M) = {Kpa(Vs[ū]) : Vs[ū] ∈ BRV (M)}.,9 query hasFakeCoin;,[0],[0]
Definition 5.5.,9 query hasFakeCoin;,[0],[0]
"The MTBN M for a BLOG model M is defined using V = V (M), E = E(M), the set of measurable spaces {Xv : v ∈ BRV (M)} and the kernels for each vertex given by κ(M).
",9 query hasFakeCoin;,[0],[0]
"By Thm. 3.6, we have the main result of this section, which provides the theoretical foundation for the generalized BLOG language:
Theorem 5.6.",9 query hasFakeCoin;,[0],[0]
"If the MTBNM for a BLOG model is a wellfounded digraph, thenM represents a unique measure µ on XBRV (M).",9 query hasFakeCoin;,[0],[0]
"We implemented the measure-theoretic extension of BLOG and evaluated our inference algorithms on three models where naive algorithms fail: (1) the GPA model (GPA); (2) the noisy scale model (Scale); and (3) a SSM, the aircraft tracking model (Aircraft-Tracking).",6. Experiment Results,[0],[0]
"The implementation is based on BLOG’s C++ compiler (Wu et al., 2016).
",6. Experiment Results,[0],[0]
GPA model: Fig. 1 presents the BLOG code for the GPA example as explained in Sec. 5.,6. Experiment Results,[0],[0]
"Since the GPA of David is exactly 4, Bayes rule implies that David must be from USA.",6. Experiment Results,[0],[0]
"We evaluate LLW and the naive LW on this model in Fig 4(a), where the naive LW converges to an incorrect posterior.
",6. Experiment Results,[0],[0]
Scale model:,6. Experiment Results,[0],[0]
"In the noisy scale example (Fig. 3), we have an even number of coins and there might be a fake coin among them (Line 4).",6. Experiment Results,[0],[0]
The fake coin will be slightly heavier than a normal coin (Line 2-3).,6. Experiment Results,[0],[0]
We divide the coins into two halves and place them onto a noisy scale.,6. Experiment Results,[0],[0]
"When there is no fake coin, the scale always balances (Line 7).
",6. Experiment Results,[0],[0]
"When there is a fake coin, the scale will noisily reflect the weight difference with standard deviation σ",6. Experiment Results,[0],[0]
(sigma in Line 6).,6. Experiment Results,[0],[0]
Now we observe that the scale is balanced (Line 8),6. Experiment Results,[0],[0]
and we would like to infer whether a fake coin exists.,6. Experiment Results,[0],[0]
We again compare LLW against the naive LW with different choices of the σ parameter in Fig. 4(b).,6. Experiment Results,[0],[0]
"Since the scale is precisely balanced, there must not be a fake coin.",6. Experiment Results,[0],[0]
"LLW always produces the correct answer but naive LW converges to different incorrect posteriors for different values of σ; as σ increases, naive LWs result approaches the true posterior.
",6. Experiment Results,[0],[0]
Aircraft-Tracking model: Fig. 5 shows a simplified BLOG model for the aircraft tracking example.,6. Experiment Results,[0],[0]
"In this state-space model, we have N = 6 radar points (Line 1) and a single aircraft to track.",6. Experiment Results,[0],[0]
Both the radars and the aircraft are considered as points on a 2D plane.,6. Experiment Results,[0],[0]
The prior of the aircraft movement is a Gaussian process (Line 3 to 6).,6. Experiment Results,[0],[0]
"Each radar r has an effective range radius(r): if the aircraft is within the range, the radar will noisily measure the distance from the aircraft to its own location (Line 13); if the aircraft is out of range, the radar will almost surely just output its radius (Line 10 to 11).",6. Experiment Results,[0],[0]
Now we observe the measurements from all the radar points for T time steps and we want to infer the location of the aircraft.,6. Experiment Results,[0],[0]
"With the measure-theoretic extension, a generalized BLOG program is more expressive for modeling truncated sensors: if a radar outputs exactly its radius, we can surely infer that the aircraft must be out of the effective range of this radar.",6. Experiment Results,[0],[0]
"However, this information cannot be captured by the original BLOG language.",6. Experiment Results,[0],[0]
"To illustrate this case, we manually generated a synthesis dataset of T = 8 time steps4 and evaluated LPF against the naive particle filter with different numbers of particles in Fig. 4(c).",6. Experiment Results,[0],[0]
We take the mean of the samples from all the particles as the predicted aircraft location.,6. Experiment Results,[0],[0]
"Since we know the ground truth, we measure the average mean square error between the true location and the prediction.",6. Experiment Results,[0],[0]
"LPF accurately predicts the
4The full BLOG programs with complete data are available at https://goo.gl/f7qLwy.",6. Experiment Results,[0],[0]
Figure 5.,18 query Y(t) for Timestep t;,[0],[0]
"BLOG code for the Aircraft-Tracking example
true locations while the naive PF converges to the incorrect results.",18 query Y(t) for Timestep t;,[0],[0]
"We presented a new formalization, measure-theoretic Bayesian networks, for generalizing the semantics of PPLs to include random variables with mixtures of discrete and continuous distributions.",7. Conclusion,[0],[0]
"We developed provably correct inference algorithms for such random variables and incorporated MTBNs into a widely used PPL, BLOG.",7. Conclusion,[0],[0]
"We believe that together with the foundational inference algorithms, our proposed rigorous framework will facilitate the development of powerful techniques for probabilistic reasoning in practical applications from a much wider range of scientific areas.",7. Conclusion,[0],[0]
"This work is supported by the DARPA PPAML program, contract FA8750-14-C-0011.",Acknowledgment,[0],[0]
"Simon S. Du is funded by NSF grant IIS1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.",Acknowledgment,[0],[0]
"Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements.",abstractText,[0],[0]
We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces.,abstractText,[0],[0]
"We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for statespace models.",abstractText,[0],[0]
"We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.",abstractText,[0],[0]
Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment).",1 Introduction,[0],[0]
"It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015).",1 Introduction,[0],[0]
"Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences.",1 Introduction,[0],[0]
"Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation.
",1 Introduction,[0],[0]
∗These authors contribute equally to this paper.,1 Introduction,[0],[0]
"†To whom correspondence should be addressed.
",1 Introduction,[0],[0]
"Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential.",1 Introduction,[0],[0]
"Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space.",1 Introduction,[0],[0]
Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations.,1 Introduction,[0],[0]
"Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013).
",1 Introduction,[0],[0]
Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs).,1 Introduction,[0],[0]
"CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees).",1 Introduction,[0],[0]
"RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree.",1 Introduction,[0],[0]
"However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009).",1 Introduction,[0],[0]
"(CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)
",1 Introduction,[0],[0]
"A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).
",1 Introduction,[0],[0]
"In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees.",1 Introduction,[0],[0]
"The model variants are denoted as c-TBCNN and d-TBCNN, respectively.",1 Introduction,[0],[0]
"The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire
1The model of tree-based convolution was firstly proposed to process program source code in our (unpublished) previous work (Mou et al., 2014).
2315
parse tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension.",1 Introduction,[0],[0]
"One merit of such architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.
",1 Introduction,[0],[0]
"TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments.",1 Introduction,[0],[0]
"To understand how TBCNNs work, we also visualize the network by plotting the convolution process.",1 Introduction,[0],[0]
We make our code and results available on our project website.2,1 Introduction,[0],[0]
"In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.",2 Background and Related Work,[0],[0]
"Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well.",2.1 Convolutional Neural Networks,[0],[0]
"Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008).",2.1 Convolutional Neural Networks,[0],[0]
"A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features.",2.1 Convolutional Neural Networks,[0],[0]
"Let t be the window size, and x1, · · · ,",2.1 Convolutional Neural Networks,[0],[0]
xt ∈ Rne be ne-dimensional word embeddings.,2.1 Convolutional Neural Networks,[0],[0]
"The output of convolution, evaluated at the current position, is
y = f (W ·",2.1 Convolutional Neural Networks,[0],[0]
[x1; · · · ; xt] + b) where y ∈ Rnc (nc is the number of feature detectors).,2.1 Convolutional Neural Networks,[0],[0]
"W ∈ Rnc×(t·ne) and b ∈ Rnc are parame-
2https://sites.google.com/site/tbcnnsentence/
ters; f is the activation function.",2.1 Convolutional Neural Networks,[0],[0]
Semicolons represent column vector concatenation.,2.1 Convolutional Neural Networks,[0],[0]
"After convolution, the extracted features are pooled to a fixedsize vector for classification.
",2.1 Convolutional Neural Networks,[0],[0]
Convolution can extract neighboring information effectively.,2.1 Convolutional Neural Networks,[0],[0]
"However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related.",2.1 Convolutional Neural Networks,[0],[0]
Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers.,2.1 Convolutional Neural Networks,[0],[0]
Similar CNNs include Kim (2014) and Hu et al. (2014).,2.1 Convolutional Neural Networks,[0],[0]
"All these models are “flat,” by which we mean no structural information is used explicitly.",2.1 Convolutional Neural Networks,[0],[0]
"Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees.",2.2 Recursive Neural Networks,[0],[0]
"In the original version, RNN is built upon a binarized constituency tree.",2.2 Recursive Neural Networks,[0],[0]
"Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings.",2.2 Recursive Neural Networks,[0],[0]
"Non-leaf nodes are sentence constituents, coded by child nodes recursively.",2.2 Recursive Neural Networks,[0],[0]
"Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2.",2.2 Recursive Neural Networks,[0],[0]
"The parent’s representation is composited by
p = f(W ·",2.2 Recursive Neural Networks,[0],[0]
[c1; c2] + b) (1) where W and b are parameters.,2.2 Recursive Neural Networks,[0],[0]
"This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b).
",2.2 Recursive Neural Networks,[0],[0]
"Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014).",2.2 Recursive Neural Networks,[0],[0]
Irsoy and Cardie (2014) build deep RNNs to enhance information interaction.,2.2 Recursive Neural Networks,[0],[0]
"Im-
provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013).",2.2 Recursive Neural Networks,[0],[0]
"They are more suitable for capturing logical information in sentences, such as negation and exclamation.
",2.2 Recursive Neural Networks,[0],[0]
One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss.,2.2 Recursive Neural Networks,[0],[0]
"Thus, RNNs bury illuminating information under a complicated neural architecture.",2.2 Recursive Neural Networks,[0],[0]
"Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009).",2.2 Recursive Neural Networks,[0],[0]
"Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).
",2.2 Recursive Neural Networks,[0],[0]
Recurrent networks.,2.2 Recursive Neural Networks,[0],[0]
"A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree.",2.2 Recursive Neural Networks,[0],[0]
"In such models, meaningful tree structures are also lost, similar to CNNs.",2.2 Recursive Neural Networks,[0],[0]
This section introduces the proposed tree-based convolutional neural networks (TBCNNs).,3 Tree-based Convolution,[0],[0]
"Figure 1c depicts the convolution process on a tree.
",3 Tree-based Convolution,[0],[0]
"First, a sentence is converted to a parse tree, either a constituency or dependency tree.",3 Tree-based Convolution,[0],[0]
The corresponding model variants are denoted as c-TBCNN and d-TBCNN.,3 Tree-based Convolution,[0],[0]
"Each node in the tree is represented as a distributed, real-valued vector.
",3 Tree-based Convolution,[0],[0]
"Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution window.",3 Tree-based Convolution,[0],[0]
"The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in Figure 1c.",3 Tree-based Convolution,[0],[0]
"Formally, let us assume we have t nodes in the convolution window, x1, · · · ,xt, each represented as an ne-dimensional vector.",3 Tree-based Convolution,[0],[0]
Let nc be the number of feature detectors.,3 Tree-based Convolution,[0],[0]
"The output of the tree-based convolution window, evaluated at the current subtree, is given by the following generic equation.
",3 Tree-based Convolution,[0],[0]
"y = f
( t∑
i=1
Wi ·xi + b )
(2)
where Wi ∈ Rnc×ne is the weight parameter associated with node xi; b ∈ Rnc is the bias term.
",3 Tree-based Convolution,[0],[0]
"Extracted features are thereafter packed into one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension is taken.",3 Tree-based Convolution,[0],[0]
"Finally, we add a fully connected hidden layer, and a softmax output layer.
",3 Tree-based Convolution,[0],[0]
"From the designed architecture (Figure 1c), we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree.",3 Tree-based Convolution,[0],[0]
"Therefore structural feature learning becomes effective.
",3 Tree-based Convolution,[0],[0]
Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees?,3 Tree-based Convolution,[0],[0]
"(2) How can we determine weights, Wi, for dependency trees, where nodes may have different numbers of children?",3 Tree-based Convolution,[0],[0]
"(3) How can we pool varying sized and shaped features to fixed-size vectors?
",3 Tree-based Convolution,[0],[0]
"In the rest of this section, we explain model variants in detail.",3 Tree-based Convolution,[0],[0]
"Particularly, Subsections 3.1 and 3.2 address the first and second problems; Subsection 3.3 deals with the third problem by introducing several pooling heuristics.",3 Tree-based Convolution,[0],[0]
Subsection 3.4 presents our training objective.,3 Tree-based Convolution,[0],[0]
"Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase.",3.1 c-TBCNN,[0],[0]
"Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity.
",3.1 c-TBCNN,[0],[0]
One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings.,3.1 c-TBCNN,[0],[0]
"Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b).",3.1 c-TBCNN,[0],[0]
"After pretraining, vector representations of nodes are fixed.
",3.1 c-TBCNN,[0],[0]
"We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr.",3.1 c-TBCNN,[0],[0]
"The convolution equation, specific for c-TBCNN, is
y = f ( W (c)p ·p +W (c)l ·cl +W (c)r ·cr + b(c) ) where W (c)p , W (c) l , and W (c) r are weights associated with the parent and its child nodes.",3.1 c-TBCNN,[0],[0]
Superscript (c) indicates that the weights are for cTBCNN.,3.1 c-TBCNN,[0],[0]
"For leaf nodes, which do not have children, we set cl and cr to be 0.
3http://nlp.stanford.edu/software/lex-parser.shtml
Tree-based convolution windows can be extended to arbitrary depths straightforwardly.",3.1 c-TBCNN,[0],[0]
"The complexity is exponential to the depth of the window, but linear to the number of nodes.",3.1 c-TBCNN,[0],[0]
"Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time.",3.1 c-TBCNN,[0],[0]
"In our experiments, we use convolution windows of depth 2.",3.1 c-TBCNN,[0],[0]
Dependency trees are another representation of sentence structures.,3.2 d-TBCNN,[0],[0]
The nature of dependency representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes.,3.2 d-TBCNN,[0],[0]
"This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., Collobert and Weston (2008) or c-TBCNN.
",3.2 d-TBCNN,[0],[0]
"To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions.",3.2 d-TBCNN,[0],[0]
"We believe this strategy makes much sense because dependency types (de Marneffe et al., 2006) reflect the relationship between a governing word and its child words.",3.2 d-TBCNN,[0],[0]
"To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes
y = f ( W (d)p ·p + n∑ i=1",3.2 d-TBCNN,[0],[0]
"W (d) r[ci] ·ci + b(d) )
where W (d)p is the weight parameter for the parent p (governing word); W (d)r[ci] is the weight for child ci, who has grammatical relationship r[ci]
to its parent,",3.2 d-TBCNN,[0],[0]
p. Superscript (d) indicates the parameters are for d-TBCNN.,3.2 d-TBCNN,[0],[0]
"Note that we keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix.
",3.2 d-TBCNN,[0],[0]
Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN.,3.2 d-TBCNN,[0],[0]
"As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape.",3.3 Pooling Heuristics,[0],[0]
"Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with
this problem.",3.3 Pooling Heuristics,[0],[0]
We propose several heuristics for pooling along a tree structure.,3.3 Pooling Heuristics,[0],[0]
Our generic design criteria for pooling include: (1) Nodes that are pooled to one slot should be “neighboring” from some viewpoint.,3.3 Pooling Heuristics,[0],[0]
"(2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it.",3.3 Pooling Heuristics,[0],[0]
"Thus, (approximately) equal amount of information is aggregated along different parts of the tree.",3.3 Pooling Heuristics,[0],[0]
"Following the above intuition, we propose pooling heuristics as follows.
",3.3 Pooling Heuristics,[0],[0]
•,3.3 Pooling Heuristics,[0],[0]
Global pooling.,3.3 Pooling Heuristics,[0],[0]
"All features are pooled to one vector, shown in Figure 3a.",3.3 Pooling Heuristics,[0],[0]
We take the maximum value in each dimension.,3.3 Pooling Heuristics,[0],[0]
"This simple heuristic is applicable to any structure, including c-TBCNN and d-TBCNN.",3.3 Pooling Heuristics,[0],[0]
• 3-slot pooling for c-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"To preserve
more information over different parts of constituency trees, we propose 3-slot pooling (Figure 3b).",3.3 Pooling Heuristics,[0],[0]
"If a tree has maximum depth d, we pool nodes of less than α · d layers to a TOP slot (α is set to 0.6); lower nodes are pooled to slots LOWER LEFT or LOWER",3.3 Pooling Heuristics,[0],[0]
RIGHT according to their relative position with respect to the root node.,3.3 Pooling Heuristics,[0],[0]
"For a constituency tree, it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time.",3.3 Pooling Heuristics,[0],[0]
"Therefore, we regard 3-slot pooling for c-TBCNN is a “hard mechanism” temporarily.",3.3 Pooling Heuristics,[0],[0]
Further improvement can be addressed in future work.,3.3 Pooling Heuristics,[0],[0]
• k-slot pooling for d-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"Different from
constituency trees, nodes in dependency trees are one-one corresponding to words in a sentence.",3.3 Pooling Heuristics,[0],[0]
"Thus, a total order on features (after convolution) can be defined according to their corresponding word orders.",3.3 Pooling Heuristics,[0],[0]
"For kslot pooling, we can adopt an “equal allocation” strategy, shown in Figure 3c.",3.3 Pooling Heuristics,[0],[0]
"Let i be the position of a word in a sentence (i = 1, 2, · · · , n).",3.3 Pooling Heuristics,[0],[0]
"Its extracted feature vector is pooled to the j-th slot, if
(j − 1) n",3.3 Pooling Heuristics,[0],[0]
k ≤,3.3 Pooling Heuristics,[0],[0]
"i ≤ j n k
We assess the efficacy of pooling quantitatively in Section 4.3.1.",3.3 Pooling Heuristics,[0],[0]
"As we shall see by the experimental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large.",3.3 Pooling Heuristics,[0],[0]
TBCNNs are not very sensitive to pooling methods.,3.3 Pooling Heuristics,[0],[0]
"After pooling, information is packed into one or more fixed-size vectors (slots).",3.4 Training Objective,[0],[0]
"We add a hidden layer, and then a softmax layer to predict the probability of each target label in a classification task.",3.4 Training Objective,[0],[0]
"The error function of a sample is the standard cross entropy loss, i.e., J = −∑ci=1 ti log yi, where t is the ground truth (one-hot represented), y the output by softmax, and c the number of classes.",3.4 Training Objective,[0],[0]
"To regularize our model, we apply both `2 penalty and dropout (Srivastava et al., 2014).",3.4 Training Objective,[0],[0]
Training details are further presented in Section 4.1 and 4.2.,3.4 Training Objective,[0],[0]
"In this section, we evaluate our models with two tasks, sentiment analysis and question classification.",4 Experimental Results,[0],[0]
We also conduct quantitative and qualitative model analysis in Subsection 4.3.,4 Experimental Results,[0],[0]
Sentiment analysis is a widely studied task for discriminative sentence modeling.,4.1.1 The Task and Dataset,[0],[0]
"The Stanford sentiment treebank4 consists of more than 10,000 movie reviews.",4.1.1 The Task and Dataset,[0],[0]
"Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative).",4.1.1 The Task and Dataset,[0],[0]
"Some examples are shown in
4http://nlp.stanford.edu/sentiment/
Table 1.",4.1.1 The Task and Dataset,[0],[0]
"We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction.",4.1.1 The Task and Dataset,[0],[0]
"Binary classification does not contain the neutral class.
",4.1.1 The Task and Dataset,[0],[0]
"In the dataset, phrases (sub-sentences) are also tagged with sentiment labels.",4.1.1 The Task and Dataset,[0],[0]
RNNs deal with them naturally during the recursive process.,4.1.1 The Task and Dataset,[0],[0]
"We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014).",4.1.1 The Task and Dataset,[0],[0]
"The training set therefore has more than 150,000 entries in total.",4.1.1 The Task and Dataset,[0],[0]
"For validating and testing, only whole sentences (root labels) are considered in our experiments.
",4.1.1 The Task and Dataset,[0],[0]
Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.,4.1.1 The Task and Dataset,[0],[0]
"This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation.",4.1.2 Training Details,[0],[0]
"c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters.",4.1.2 Training Details,[0],[0]
"c-TBCNN’s settings can be found on our website.
",4.1.2 Training Details,[0],[0]
"In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer.",4.1.2 Training Details,[0],[0]
"Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus.",4.1.2 Training Details,[0],[0]
2- slot pooling is applied for d-TBCNN.,4.1.2 Training Details,[0],[0]
"(c-TBCNN uses 3-slot pooling.)
",4.1.2 Training Details,[0],[0]
"To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200.",4.1.2 Training Details,[0],[0]
"We use ReLU (Nair and Hinton, 2010) as the activation function .
",4.1.2 Training Details,[0],[0]
"For regularization, we add `2 penalty for weights with a coefficient of 10−5.",4.1.2 Training Details,[0],[0]
"Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings.",4.1.2 Training Details,[0],[0]
"All hidden layers are dropped out by 50%, and embeddings 40%.",4.1.2 Training Details,[0],[0]
Table 2 compares our models to state-of-the-art results in the task of sentiment analysis.,4.1.3 Performance,[0],[0]
"For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015).",4.1.3 Performance,[0],[0]
c-TBCNN is slightly worse.,4.1.3 Performance,[0],[0]
"It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).
",4.1.3 Performance,[0],[0]
"Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes.",4.1.3 Performance,[0],[0]
(The neutral class is discarded as in other studies.),4.1.3 Performance,[0],[0]
"This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position.",4.1.3 Performance,[0],[0]
"Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list.
",4.1.3 Performance,[0],[0]
"In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%.",4.1.3 Performance,[0],[0]
"Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs.
",4.1.3 Performance,[0],[0]
We also observe d-TBCNN achieves higher performance than c-TBCNN.,4.1.3 Performance,[0],[0]
This suggests that compact tree expressiveness is more important than integrating global information in this task.,4.1.3 Performance,[0],[0]
We further evaluate TBCNN models on a question classification task.6,4.2 Question Classification,[0],[0]
The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10.,4.2 Question Classification,[0],[0]
"We also use the standard split, like Silva et al. (2011).",4.2 Question Classification,[0],[0]
"Target labels contain 6 classes, namely abbreviation, entity, description, human, location, and numeric.",4.2 Question Classification,[0],[0]
"Some examples are also shown in Table 1.
",4.2 Question Classification,[0],[0]
"We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs’ performance when applied to datasets of different sizes.",4.2 Question Classification,[0],[0]
"To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively.",4.2 Question Classification,[0],[0]
"We do not back-propagate gradient to embeddings in this
5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification.",4.2 Question Classification,[0],[0]
We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model.,4.2 Question Classification,[0],[0]
"Our website reviews some related work and provides more discussions.
",4.2 Question Classification,[0],[0]
"6http://cogcomp.cs.illinois.edu/Data/QA/QC/
task.",4.2 Question Classification,[0],[0]
"Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%.
",4.2 Question Classification,[0],[0]
Table 3 compares our models to various other methods.,4.2 Question Classification,[0],[0]
"The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011).",4.2 Question Classification,[0],[0]
Their method utilizes more than 10k features and 60 hand-coded rules.,4.2 Question Classification,[0],[0]
"On the contrary, our TBCNN models do not use a single human-engineered feature or rule.",4.2 Question Classification,[0],[0]
"Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%.",4.2 Question Classification,[0],[0]
"To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
",4.2 Question Classification,[0],[0]
"The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task.",4.2 Question Classification,[0],[0]
"In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs.",4.3 Model Analysis,[0],[0]
The extracted features by tree-based convolution have topologies varying in size and shape.,4.3.1 The Effect of Pooling,[0],[0]
We propose in Section 3.3 several heuristics for pooling.,4.3.1 The Effect of Pooling,[0],[0]
"This subsection aims to provide a fair comparison among these pooling methods.
",4.3.1 The Effect of Pooling,[0],[0]
One reasonable protocol for comparison is to tune all hyperparameters for each setting and compare the highest accuracy.,4.3.1 The Effect of Pooling,[0],[0]
"This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning.",4.3.1 The Effect of Pooling,[0],[0]
An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the same setting.,4.3.1 The Effect of Pooling,[0],[0]
"In this experiment, we chose the latter protocol, where hidden layers are all 300- dimensional; no `2 penalty is added.",4.3.1 The Effect of Pooling,[0],[0]
Each configuration was run five times with different random initializations.,4.3.1 The Effect of Pooling,[0],[0]
"We summarize the mean and standard deviation in Table 4.
",4.3.1 The Effect of Pooling,[0],[0]
"As the results imply, complicated pooling is better than global pooling to some degree for both model variants.",4.3.1 The Effect of Pooling,[0],[0]
"But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data.",4.3.1 The Effect of Pooling,[0],[0]
"In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.
",4.3.1 The Effect of Pooling,[0],[0]
"Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering.",4.3.1 The Effect of Pooling,[0],[0]
"Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification.",4.3.1 The Effect of Pooling,[0],[0]
Our TBCNNs are typically trained within 25 epochs.,4.3.1 The Effect of Pooling,[0],[0]
We analyze how sentence lengths affect our models.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"Sentences are split into 7 groups by length, with granularity 5.",4.3.2 The Effect of Sentence Lengths,[0],[0]
A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457.,4.3.2 The Effect of Sentence Lengths,[0],[0]
Figure 4 presents accuracies versus lengths in TBCNNs.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b).",4.3.2 The Effect of Sentence Lengths,[0],[0]
"Thus, we think our reimplementation is fair and that the comparison is sensible.
",4.3.2 The Effect of Sentence Lengths,[0],[0]
We observe that c-TBCNN and d-TBCNN yield very similar behaviors.,4.3.2 The Effect of Sentence Lengths,[0],[0]
They consistently outperform the RNN in all scenarios.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"By contrast, our models explore structural information more effectively with
tree-based convolution.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences.",4.3.2 The Effect of Sentence Lengths,[0],[0]
Visualization is important to understanding the mechanism of neural networks.,4.3.3 Visualization,[0],[0]
"For TBCNNs, we would like to see how the extracted features (after convolution) are further processed by the max pooling layer, and ultimately related to the supervised task.
",4.3.3 Visualization,[0],[0]
"To show this, we trace back where the max pooling layer’s features come from.",4.3.3 Visualization,[0],[0]
"For each dimension, the pooling layer chooses the maximum value from the nodes that are pooled to it.",4.3.3 Visualization,[0],[0]
"Thus, we can count the fraction in which a node’s features are gathered by pooling.",4.3.3 Visualization,[0],[0]
"Intuitively, if a node’s features are more related to the task, the fraction tends to be larger, and vice versa.
",4.3.3 Visualization,[0],[0]
"Figure 5 illustrates an example processed by dTBCNN in the task of sentiment analysis.7 Here, we applied global pooling because information tracing is more sensible with one pooling slot.",4.3.3 Visualization,[0],[0]
"As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest.",4.3.3 Visualization,[0],[0]
"The 2-layer windows corresponding to “visual will impress viewers,” “the stunning dreamlike visual,” say, are discriminative to the sentence’s sentiment.",4.3.3 Visualization,[0],[0]
"Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling.",4.3.3 Visualization,[0],[0]
"On the other hand, words like the, will, even are known as stop words (Fox, 1989).",4.3.3 Visualization,[0],[0]
"They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered.",4.3.3 Visualization,[0],[0]
"Such results are consistent with human intuition.
",4.3.3 Visualization,[0],[0]
We further observe that tree-based convolution does integrate information of different words in the window.,4.3.3 Visualization,[0],[0]
"For example, the word stunning appears in two windows: (a) the window “stunning” itself, and (b) the window of “the stunning dreamlike visual,” with root node visual, stunning acting as a child.",4.3.3 Visualization,[0],[0]
"We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment.",4.3.3 Visualization,[0],[0]
"In fact,
7We only have space to present one example in the paper.",4.3.3 Visualization,[0],[0]
This example was not chosen deliberately.,4.3.3 Visualization,[0],[0]
"Similar traits can be found through out the entire gallery, available on our website.",4.3.3 Visualization,[0],[0]
"Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization since we know the “meaning” of every node.
",4.3.3 Visualization,[0],[0]
"Window a has a larger fraction than the sum of its children’s (the windows of “the,” “stunning,” and “dreamlike”).",4.3.3 Visualization,[0],[0]
"In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing structures.",5 Conclusion,[0],[0]
"Our model can be built upon either constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).
",5 Conclusion,[0],[0]
Both variants have achieved high performance in sentiment analysis and question classification.,5 Conclusion,[0],[0]
"d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous stateof-the-art results in both tasks.",5 Conclusion,[0],[0]
"The results show that tree-based convolution can capture sentences’ structural information effectively, which is useful for sentence modeling.",5 Conclusion,[0],[0]
This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301.,Acknowledgments,[0],[0]
This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling.,abstractText,[0],[0]
Our model leverages either constituency trees or dependency trees of sentences.,abstractText,[0],[0]
"The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.",abstractText,[0],[0]
"Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.",abstractText,[0],[0]
We evaluate our models on two tasks: sentiment analysis and question classification.,abstractText,[0],[0]
"In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.",abstractText,[0],[0]
"We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.",abstractText,[0],[0]
Discriminative Neural Sentence Modeling by Tree-Based Convolution,title,[0],[0]
Representation learning remains an outstanding research problem in machine learning and computer vision.,1. Introduction,[0],[0]
"Recently there is a rising interest in disentangled representations, in which each component of learned features refers to a semantically meaningful concept.",1. Introduction,[0],[0]
"In the example of video sequence modelling, an ideal disentangled representation would be able to separate time-independent concepts (e.g. the identity of the object in the scene) from dynamical information (e.g. the time-varying position and the orientation or pose of that object).",1. Introduction,[0],[0]
"Such disentangled represen-
1University of Cambridge, UK 2Disney Research, Los Angeles, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Yingzhen Li<yl494@cam.ac.uk>, Stephan Mandt <stephan.mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tations would open new efficient ways of compression and style manipulation, among other applications.
",1. Introduction,[0],[0]
"Recent work has investigated disentangled representation learning for images within the framework of variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"Some of them, e.g. the β-VAE method (Higgins et al., 2016), proposed new objective functions/training techniques that encourage disentanglement.",1. Introduction,[0],[0]
"On the other hand, network architecture designs that directly enforce factored representations have also been explored by e.g. Siddharth et al. (2017); Bouchacourt et al. (2017).",1. Introduction,[0],[0]
"These two types of approaches are often mixed together, e.g. the infoGAN approach (Chen et al., 2016) partitioned the latent space and proposed adding a mutual information regularisation term to the vanilla GAN loss.",1. Introduction,[0],[0]
"Mathieu et al. (2016) also partitioned the encoding space into style and content components, and performed adversarial training to encourage the datapoints from the same class to have similar content representations, but diverse style features.
",1. Introduction,[0],[0]
Less research has been conducted for unsupervised learning of disentangled representations of sequences.,1. Introduction,[0],[0]
"For video sequence modelling, Villegas et al. (2017) and Denton & Birodkar (2017) utilised different networks to encode the content and dynamics information separately, and trained the auto-encoders with a combination of reconstruction loss and GAN loss.",1. Introduction,[0],[0]
"Structured (Johnson et al., 2016) and Factorised VAEs (Deng et al., 2017) used hierarchical priors to learn more interpretable latent variables.",1. Introduction,[0],[0]
Hsu et al. (2017) designed a structured VAE in the context of speech recognition.,1. Introduction,[0],[0]
Their VAE architecture is trained using a combination of the standard variational lower bound and a discriminative regulariser to further encourage disentanglement.,1. Introduction,[0],[0]
"More related work is discussed in Section 3.
",1. Introduction,[0],[0]
"In this paper, we propose a generative model for unsupervised structured sequence modelling, such as video or audio.",1. Introduction,[0],[0]
"We show that, in contrast to previous approaches, a disentangled representation can be achieved by a careful design of the probabilistic graphical model.",1. Introduction,[0],[0]
"In the proposed architecture, we explicitly use a latent variable to represent content, i.e., information that is invariant through the sequence, and a set of latent variables associated to each frame to represent dynamical information, such as pose and position.",1. Introduction,[0],[0]
"Com-
pared to the mentioned previous models that usually predict future frames conditioned on the observed sequences, we focus on learning the distribution of the video/audio content and dynamics to enable sequence generation without conditioning.",1. Introduction,[0],[0]
"Therefore our model can also generalise to unseen sequences, which is confirmed by our experiments.",1. Introduction,[0],[0]
"In more detail, our contributions are as follows:
• Controlled generation.",1. Introduction,[0],[0]
Our architecture allows us to approximately control for content and dynamics when generating videos.,1. Introduction,[0],[0]
"We can generate random dynamics for fixed content, and random content for fixed dynamics.",1. Introduction,[0],[0]
"This gives us a controlled way of manipulating a video/audio sequence, such as swapping the identity of moving objects or the voice of a speaker.
",1. Introduction,[0],[0]
• Efficient encoding.,1. Introduction,[0],[0]
Our representation is more data efficient than encoding a video frame by frame.,1. Introduction,[0],[0]
"By factoring out a separate variable that encodes content, our dynamical latent variables can have smaller dimensions.",1. Introduction,[0],[0]
"This may be promising when it comes to end-to-end neural video encoding methods.
",1. Introduction,[0],[0]
"• We design a new metric that allow us to verify disentanglement of the latent variables, by investigating the stability of an object classifier over time.
",1. Introduction,[0],[0]
"• We give empirical evidence, based on video data of a physics simulator, that for long sequences, a stochastic transition model generates more realistic dynamics.
",1. Introduction,[0],[0]
The paper is structured as follows.,1. Introduction,[0],[0]
Section 2 introduces the generative model and the problem setting.,1. Introduction,[0],[0]
Section 3 discusses related work.,1. Introduction,[0],[0]
Section 4 presents three experiments on video and speech data.,1. Introduction,[0],[0]
"Finally, Section 5 concludes the paper and discusses future research directions.",1. Introduction,[0],[0]
"Let x1:T = (x1,x2, ...,xT ) denote a high dimensional sequence, such as a video with T consecutive frames.",2. The model,[0],[0]
"Also, assume the data distribution of the training sequences is pdata(x1:T ).",2. The model,[0],[0]
"In this paper, we model the observed data with a latent variable model that separates the representation of time-invariant concepts (e.g. object identities) from those of time-varying concepts (e.g. pose information).
",2. The model,[0],[0]
Generative model.,2. The model,[0],[0]
"Consider the following probabilistic model, which is also visualised in Figure 1:
pθ(x1:T , z1:T ,f) = pθ(f) T∏ t=1 pθ(zt|z<t)pθ(xt|zt,f).
",2. The model,[0],[0]
(1) We use the convention that z0 = 0.,2. The model,[0],[0]
The generation of frame xt at time t depends on the corresponding latent variables zt and f .,2. The model,[0],[0]
"θ are model parameters.
",2. The model,[0],[0]
"Ideally, f will be capable of modelling global aspects of the whole sequence which are time-invariant, while zt will encode time-varying features.",2. The model,[0],[0]
"This separation may be achieved when choosing the dimensionality of zt to be small enough, thus reserving zt only for time-dependent features while compressing everything else into f .",2. The model,[0],[0]
"In the context of video encodings, zt would thus encode a “morphing transformation”, which encodes how a frame at time t is morphed into a frame at time t+ 1.
Inference models.",2. The model,[0],[0]
"We use variational inference to learn an approximate posterior over latent variables given data (Jordan et al., 1999).",2. The model,[0],[0]
"This involves an approximating distribution q. We train the generative model with the VAE algorithm (Kingma & Welling, 2013):
max θ,φ
EpD(x1:T )",2. The model,[0],[0]
"[ Eqφ [ log pθ(x1:T , z1:T ,f)
qφ(z1:T ,f |x1:T )
",2. The model,[0],[0]
]] .,2. The model,[0],[0]
"(2)
To quantify the effect of the architecture of q on the learned generative model, we test with two types of q factorisation structures as follows.
",2. The model,[0],[0]
"The first architecture constructs a factorised q distribution
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )",2. The model,[0],[0]
"T∏
t=1
qφ(zt|xt) (3)
",2. The model,[0],[0]
as the amortised variational distribution.,2. The model,[0],[0]
We refer to this as “factorised q” in the experiments section.,2. The model,[0],[0]
This factorization assumes that content features are approximately independent of motion features.,2. The model,[0],[0]
"Furthermore, note that the distribution over content features is conditioned on the entire time series, whereas the dynamical features are only conditioned on the individual frames.
",2. The model,[0],[0]
"The second encoder assumes that the variational posterior of z1:T depends on f , and the q distribution has the following architecture:
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )qφ(z1:T |f ,x1:T ), (4)
and the distribution q(z1:T |f ,x1:T ) is conditioned on the entire time series.",2. The model,[0],[0]
"It can be implemented by e.g. a bidirectional LSTM (Graves & Schmidhuber, 2005) conditioned on f , followed by an RNN taking the bi-LSTM hidden states as the inputs.",2. The model,[0],[0]
We provide a visualisation of the corresponding computation graph in the appendix.,2. The model,[0],[0]
This encoder is referred to as “full q”.,2. The model,[0],[0]
"The idea behind the structured approximation is that content may affect dynamics: in video, the shape of objects may be informative about their motion patterns, thus z1:T is conditionally dependent on f .",2. The model,[0],[0]
"The architectures of the generative model and both encoders are visualised in Figure 1.
",2. The model,[0],[0]
Unconditional generation.,2. The model,[0],[0]
"After training, one can use the generative model to synthesise video or audio sequences
by sampling the latent variables from the prior and decoding them.",2. The model,[0],[0]
"Furthermore, the proposed generative model allows generation of multiple sequences entailing the same global information (e.g. the same object in a video sequence), simply by fixing f ∼ p(f), sampling different zk1:T ∼ p(z1:T ), k = 1, ...,K, and generating the observations xkt ∼ p(xt|zkt ,f).",2. The model,[0],[0]
"Generating sequences with similar dynamics is done analogously, by fixing z1:T ∼ p(z1:T ) and sampling fk, k = 1, ...K from the prior.
",2. The model,[0],[0]
Conditional generation.,2. The model,[0],[0]
"Together with the encoder, the model also allows conditional generation of sequences.",2. The model,[0],[0]
"As an example, given a video sequence x1:T as reference, one can manipulate the latent variables and generate new sequences preserving either the object identity or the pose/movement information.",2. The model,[0],[0]
"This is done by conditioning on f ∼ q(f |x1:T ) for a given x1:T then randomising z1:T from the prior, or the other way around.
",2. The model,[0],[0]
Feature swapping.,2. The model,[0],[0]
One might also want to generate a new video sequence with the object identity and pose information encoded from different sequence.,2. The model,[0],[0]
"Given two sequences xa1:T and x b 1:T , the synthesis process first infers the latent variables fa ∼ q(f |xa1:T ) and zb1:T ∼ q(z1:T |xb1:T )1, then produces a new sequence by sampling xnewt ∼ p(xt|zbt ,fa).",2. The model,[0],[0]
"This allows us to control both the content and the dynamics of the generated sequence, which can be applied to e.g. conversion of voice of the speaker in a speech sequence.",2. The model,[0],[0]
Research on learning disentangled representation has mainly focused on two aspects: the training objective and the generative model architecture.,3. Related work,[0],[0]
"Regarding the loss function design for VAE models, Higgins et al. (2016) propose the β-VAE by scaling up the KL[q(z|x)||p(z)] term in the variational lower-bound with β > 1 to encourage learning of independent attributes (as the prior p(z) is usually factorised).",3. Related work,[0],[0]
"While the β-VAE has been shown effective in learning better representations for natural images and might be able to further improve the performance of our model, we do not
1For the full q encoder",3. Related work,[0],[0]
"it also requires f b ∼ q(f |xb1:T ).
",3. Related work,[0],[0]
"test this recipe here to demonstrate that disentanglement can be achieved by a careful model design.
",3. Related work,[0],[0]
"For sequence modelling, a number of prior publications have extended VAE to video and speech data (Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014; Chung et al., 2015).",3. Related work,[0],[0]
"These models, although being able to generate realistic sequences, do not explicitly disentangle the representation of time-invariant and time-dependent information.",3. Related work,[0],[0]
"Thus it is inconvenient for these models to perform tasks such as controlled generation and feature swapping.
",3. Related work,[0],[0]
"For GAN-like models, both Villegas et al. (2017) and Denton & Birodkar (2017) proposed an auto-encoder architecture for next frame prediction, with two separate encoders responsible for content and pose information at each time step.",3. Related work,[0],[0]
"While in Villegas et al. (2017), the pose information is extracted from the difference between two consecutive frames xt−1 and xt, Denton & Birodkar (2017) directly encoded xt for both pose and content, and further designed a training objective to encourage learning of disentangled representations.",3. Related work,[0],[0]
"On the other hand, Vondrick et al. (2016) used a spatio-temporal convolutional architecture to disentangle a video scene’s foreground from its background.",3. Related work,[0],[0]
"Although it has successfully achieved disentanglement, we note that the time-invariant information in this model is predefined to represent the background, rather than learned from the data automatically.",3. Related work,[0],[0]
"Also this architecture is suitable for video sequences only, unlike our model which can be applied to any type of sequential data.
",3. Related work,[0],[0]
"Very recent work (Hsu et al., 2017) introduced the factorised hierarchical variational auto-encoder (FHVAE) for unsupervised learning of disentangled representation of speech data.",3. Related work,[0],[0]
"Given a speech sequence that has been partitioned into segments {xn1:T }Nn=1, FHVAE models the joint distribution of {xn1:T }Nn=1 and latent variables as follows:
p({xn1:T , zn1 , zn2 },µ2) = p(µ2) N∏
n=1
p(xn1:T , z n 1 , z n 2 |µ2),
p(xn1:T , z n 1 , z n 2 |µ2) = p(zn1 )p(zn2",3. Related work,[0],[0]
"|µ2)p(xn1:T |zn1 , zn2 ).
",3. Related work,[0],[0]
"Here the zn2 variable has a hierarchical prior p(z n 2 |µ2) = N (µ2, σ2I), p(µ2) = N (0, λI).",3. Related work,[0],[0]
"The authors showed that by having different prior structures for zn1 and z n 2 , it allows the model to encode with zn2 speech sequence-level
attributes (e.g. pitch of a speaker), and other residual information with zn1 .",3. Related work,[0],[0]
"A discriminative training objective (see discussions in Section 4.2) is added to the variational lowerbound, which has been shown to further improve the quality of the disentangled representation.",3. Related work,[0],[0]
"Our model can also benefit from the usage of hierarchical prior distributions, e.g. fn ∼ p(f |µ2),µ2 ∼ p(µ2), and we leave the investigation to future work.",3. Related work,[0],[0]
We carried out experiments both on video data (Section 4.1) as well as speech data (Section 4.2).,4. Experiments,[0],[0]
"In both setups, we find strong evidence that our model learns an approximately disentangled representation that allows for conditional generation and feature swapping.",4. Experiments,[0],[0]
We further investigated the efficiency for encoding long sequences with a stochastic transition model in Section 4.3.,4. Experiments,[0],[0]
The detailed model architectures of the networks used in each experiment are reported in the appendix.,4. Experiments,[0],[0]
"We present an initial test of the proposed VAE architecture on a dataset of video game “sprites”, i.e. animated cartoon characters whose clothing, pose, hairstyle, and skin color we can fully control.",4.1. Video sequence: Sprites,[0],[0]
"This dataset comes from an open-source video game project called Liberated Pixel Cup2, and has been also considered in Reed et al. (2015); Mathieu et al. (2016) for image processing experiments.",4.1. Video sequence: Sprites,[0],[0]
"Our experiments show that static attributes such as hair color and clothing are well preserved over time for randomly generated videos.
",4.1. Video sequence: Sprites,[0],[0]
Data and preprocessing.,4.1. Video sequence: Sprites,[0],[0]
"We downloaded and selected the online available sprite sheets3, and organised them into 4 attribute categories (skin color, tops, pants and hairstyle) and 9 action categories (walking, casting spells and slashing, each with three viewing angles).",4.1. Video sequence: Sprites,[0],[0]
"In order to avoid a combinatorial explosion problem, each of the attribute categories contains 6 possible variants (see Figure 2), therefore it leads to 64 = 1296 unique characters in total.",4.1. Video sequence: Sprites,[0],[0]
We used 1000 of them for training/validation and the rest of them for testing.,4.1. Video sequence: Sprites,[0],[0]
The resulting dataset consists of sequences with T = 8 frames of dimension 64× 64.,4.1. Video sequence: Sprites,[0],[0]
Note here we did not use the labels for training the generative model.,4.1. Video sequence: Sprites,[0],[0]
"Instead these labels on the data frames are used to train a classifier that is later deployed to produce quantitative evaluations on the VAE, see below.
",4.1. Video sequence: Sprites,[0],[0]
Qualitative analysis.,4.1. Video sequence: Sprites,[0],[0]
We start with a qualitative evaluation of our VAE architecture.,4.1. Video sequence: Sprites,[0],[0]
"Figure 3 shows both re-
2http://lpc.opengameart.org/",4.1. Video sequence: Sprites,[0],[0]
"3https://github.com/jrconway3/
Universal-LPC-spritesheet
constructed as well as generated video sequences from our model.",4.1. Video sequence: Sprites,[0],[0]
Each panel shows three video sequences with time running from left to right.,4.1. Video sequence: Sprites,[0],[0]
"Panel (a) shows parts of the original data from the test set, and (b) shows its reconstruction.
",4.1. Video sequence: Sprites,[0],[0]
The sequences visualised in panel (c) are generated using zt ∼ q(zt|xt) but f ∼ p(f).,4.1. Video sequence: Sprites,[0],[0]
"Hence, the dynamics are imposed by the encoder, but the identity is sampled from the prior.",4.1. Video sequence: Sprites,[0],[0]
"We see that panel (c) reveals the same motion patterns as (a), but has different character identities.",4.1. Video sequence: Sprites,[0],[0]
"Conversely, in panel (d) we take the identity from the encoder, but sample the dynamics from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Panel (d) reveals the same characters as (a), but different motion patterns.
",4.1. Video sequence: Sprites,[0],[0]
Panels (e) and (f) focus on feature swapping.,4.1. Video sequence: Sprites,[0],[0]
"In (e), the frames are constructed by computing zt ∼ q(zt|xt) on one input sequence but f encoded on another input sequence.",4.1. Video sequence: Sprites,[0],[0]
"These panels demonstrate that the encoder and the decoder have learned a factored representation for content and pose.
",4.1. Video sequence: Sprites,[0],[0]
"Panels (g) and (h) focus on conditional generation, showing randomly generated sequences that share the same f or z1:T samples from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Thus, in panel (g) we see the same character performing different actions, and in (h) different characters performing the same motion.",4.1. Video sequence: Sprites,[0],[0]
"This again illustrates that the prior model disentangles the representation.
",4.1. Video sequence: Sprites,[0],[0]
Quantitative analysis.,4.1. Video sequence: Sprites,[0],[0]
"Next we perform quantitative evaluations of the generative model, using a classifier trained on the labelled frames.",4.1. Video sequence: Sprites,[0],[0]
"Empirically, we find that the fully factorized and structured inference networks produce almost identical results here, presumably because in this dataset the object identity and pose information are truly independent.",4.1. Video sequence: Sprites,[0],[0]
"Therefore we only report results on the fully factorised q distribution case.
",4.1. Video sequence: Sprites,[0],[0]
The first evaluation task considers reconstructing the test sequences with encoded f and randomly sampled zt (in the same way as to produce panel (d) in Figure 3).,4.1. Video sequence: Sprites,[0],[0]
Then we compare the classifier outputs on both the original frames and the reconstructed frames.,4.1. Video sequence: Sprites,[0],[0]
"If the character’s identity is preserved over time, the classifier should produce identical probability vectors on the data frames and the reconstructed frames (denoted as pdata and precon respectively).
",4.1. Video sequence: Sprites,[0],[0]
We evaluate the similarity between the original and reconstructed sequences both in terms of the disagreement of the predicted class labels maxi[precon(i)] 6= maxi[pdata(i)] and the KL-divergence KL[precon||pdata].,4.1. Video sequence: Sprites,[0],[0]
We also compute the two metrics on the action predictions using reconstructed sequences with randomised f and inferred zt.,4.1. Video sequence: Sprites,[0],[0]
The results in Table 1 indicate that the learned representation is indeed factorised.,4.1. Video sequence: Sprites,[0],[0]
"For example, in the fix-f generation test, only 4% out of 296× 9 data-reconstruction frame pairs contain characters whose generated skin color differs from the rest, where in the case of hairstyle preservation the disagreement rate is only 0.06%.",4.1. Video sequence: Sprites,[0],[0]
The KL metric is also much smaller than the KL-divergence KL[prandom||pdata],4.1. Video sequence: Sprites,[0],[0]
"where prandom = (1/Nclass, ..., 1/Nclass), indicating that our result is significant.
",4.1. Video sequence: Sprites,[0],[0]
"In the second evaluation, we test whether static attributes of generated sequences, such as clothing or hair style, are preserved over time.",4.1. Video sequence: Sprites,[0],[0]
"We sample 200 video sequences from the generator, using the same f but different latent dynamics z1:T .",4.1. Video sequence: Sprites,[0],[0]
We use the trained classifier to predict both the attributes and the action classes for each of the generated frames.,4.1. Video sequence: Sprites,[0],[0]
"Results are shown in Figure 4(a), where we plot the prediction of the classifiers for each frame over time.",4.1. Video sequence: Sprites,[0],[0]
"For example, the trajectory curve in the “skin color” panel in Figure 4(a) corresponds to the skin color attribute classification results for frames x1:T of a generated video sequence.",4.1. Video sequence: Sprites,[0],[0]
"We repeat this process 5 times with different f samples,
where each f corresponds to one color.
",4.1. Video sequence: Sprites,[0],[0]
"It becomes evident that those lines with the same color are clustered together, confirming that f mainly controls the generation of time-invariant attributes.",4.1. Video sequence: Sprites,[0],[0]
"Also, most character attributes are preserved over time, e.g. for the attribute “tops”, the trajectories are mostly straight lines.",4.1. Video sequence: Sprites,[0],[0]
"However, some of the trajectories for the attributes drift away from the majority class.",4.1. Video sequence: Sprites,[0],[0]
"We conjecture that this is due of the mass-covering behaviour of (approximate) maximum likelihood training, which makes the trained model generate characters that do not exist in the dataset.",4.1. Video sequence: Sprites,[0],[0]
"Indeed the middle row of panel (c) in Figure 3 contains a character with an unseen hairstyle, showing that our model is able to generalise beyond the training set.",4.1. Video sequence: Sprites,[0],[0]
"On the other hand, the sampling process returns sequences with diverse actions as depicted in the action panel, meaning that f contains little information regarding the video dynamics.
",4.1. Video sequence: Sprites,[0],[0]
"We performed similar tests on sequence generations with shared latent dynamics z1:T but different f , shown in Figure 4(b).",4.1. Video sequence: Sprites,[0],[0]
"The experiment is repeated 5 times as well, and again trajectories with the same color encoding correspond to videos generated with the same z1:T (but different f ).",4.1. Video sequence: Sprites,[0],[0]
Here we also observe diverse trajectories for the attribute categories.,4.1. Video sequence: Sprites,[0],[0]
"In contrast, the characters’ actions are mostly the same.",4.1. Video sequence: Sprites,[0],[0]
These two test results again indicate that the model has successfully learned disentangled representations of character identities and actions.,4.1. Video sequence: Sprites,[0],[0]
"Interestingly we observe multi-modalities in the action domain for the generated sequences, e.g. the trajectories in the action panel of Figure 4(b) are jumping between different levels.",4.1. Video sequence: Sprites,[0],[0]
We also visualise in Figure 5 generated sequences of the “turning” action that is not present in the dataset.,4.1. Video sequence: Sprites,[0],[0]
It again shows that the trained model generalises to unseen cases.,4.1. Video sequence: Sprites,[0],[0]
We also experiment on audio sequence data.,4.2. Speech data: TIMIT,[0],[0]
Our disentangled representation allows us to convert speaker identities into each other while conditioning on the content of the speech.,4.2. Speech data: TIMIT,[0],[0]
"We also show that our model gives rise to speaker verification, where we outperform a recent probabilistic baseline model.
(a) Trajectory plots on the generated sequences with shared f .
(b) Trajectory plots on the generated sequences with shared z1:T .
",4.2. Speech data: TIMIT,[0],[0]
Figure 4.,4.2. Speech data: TIMIT,[0],[0]
"Classification test on the generated video sequences with shared f (top) or shared z1:T (bottom), respectively.",4.2. Speech data: TIMIT,[0],[0]
The experiment is repeated 5 times and depicted by different color coding.,4.2. Speech data: TIMIT,[0],[0]
"The x and y axes are time and the class id of the attributes, respectively.
",4.2. Speech data: TIMIT,[0],[0]
Figure 5.,4.2. Speech data: TIMIT,[0],[0]
Visualising multi-modality in action space.,4.2. Speech data: TIMIT,[0],[0]
"In this case the characters turn from left to right, and this action sequence is not observed in data.
Data and preprocessing.",4.2. Speech data: TIMIT,[0],[0]
"The TIMIT data (Garofolo et al., 1993) contains broadband 16kHz recordings of phonetically-balanced read speech.",4.2. Speech data: TIMIT,[0],[0]
A total of 6300 utterances (5.4 hours) are presented with 10 sentences from each of the 630 speakers (70% male and 30% female).,4.2. Speech data: TIMIT,[0],[0]
"We follow Hsu et al. (2017) for data pre-processing: the raw speech waveforms are first split into sub-sequences of 200ms, and then preprocessed with sparse fast Fourier transform to obtain a 200 dimensional log-magnitude spectrum, computed every 10ms.",4.2. Speech data: TIMIT,[0],[0]
"This implies T = 20 for the observation x1:T .
",4.2. Speech data: TIMIT,[0],[0]
Qualitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We perform voice conversion experiments to demonstrate the disentanglement of the learned representation.,4.2. Speech data: TIMIT,[0],[0]
The goal here is to convert male voice to female voice (and vice versa) with the speech content being preserved.,4.2. Speech data: TIMIT,[0],[0]
"Assuming that f has learned the representation of speaker’s identity, the conversion can be done by first encoding two sequences xmale1:T and x female 1:T with q to obtain representations {fmale, zmale1:T } and {f female, zfemale1:T }, then construct the converted sequence by feeding f female and zmale1:T to the decoder p(xt|zt,f).",4.2. Speech data: TIMIT,[0],[0]
Figure 6 shows the reconstructed spectrogram after the swapping process of the f features.,4.2. Speech data: TIMIT,[0],[0]
"We also provide the reconstructed speech waveforms using the Griffin-Lim algorithm (Griffin & Lim, 1984) in the appendix.
",4.2. Speech data: TIMIT,[0],[0]
The experiments show that the harmonics of the converted speech sequences shifted to higher frequency in the “male to female” test and vice versa.,4.2. Speech data: TIMIT,[0],[0]
"Also the pitch (the red arrow in Figure 6 indicating the fundamental frequency, i.e. the first harmonic) of the converted sequence (b) is close to the pitch of (c), same as for the comparison between (d) and (a).",4.2. Speech data: TIMIT,[0],[0]
"By an informal listening test of the speech sequence pairs (a, d) and (b, c), we confirm that the speech content is preserved.",4.2. Speech data: TIMIT,[0],[0]
"These results show that our model is successfully applied to speech sequences for learning disentangled representations.
",4.2. Speech data: TIMIT,[0],[0]
Quantitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We further follow Hsu et al. (2017) to use speaker verification for quantitative evaluation.,4.2. Speech data: TIMIT,[0],[0]
"Speaker verification is the process of verifying the claimed identity of a speaker, usually by comparing the “features” wtest of the test utterance xtest1:",4.2. Speech data: TIMIT,[0],[0]
T1 with those of the target utterance xtarget1:T2 from the claimed identity.,4.2. Speech data: TIMIT,[0],[0]
"The claimed identity is confirmed if the cosine similarity cos(wtest,wtarget) is grater than a given threshold (Dehak et al., 2009).",4.2. Speech data: TIMIT,[0],[0]
By varying ∈,4.2. Speech data: TIMIT,[0],[0]
"[0, 1], we report the verification performance in terms of equal error rate (EER), where the false rejection rate equals the false acceptance rate.
",4.2. Speech data: TIMIT,[0],[0]
The extraction of the “features” is crucial for the performance of this speaker verification system.,4.2. Speech data: TIMIT,[0],[0]
"Given a speech sequence containing N segments {x(n)1:T }Nn=1, we constructed two types of “features”, one by computing µf as the mean
of q(f (n)|x(n)1:T ) across the segments, and the other by extracting the mean µzt of q(zt|x1:T ) and averaging them across both time T and segments.",4.2. Speech data: TIMIT,[0],[0]
"In formulas,
µf = 1
N N∑ n=1 µfn , µfn = Eq(fn|xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[f n],
µz = 1
TN T∑ t=1 N∑ n=1 µznt , µznt = Eq(znt |xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[z n t ].
We also include two baseline results from Hsu et al. (2017): one used the i-vector method (Dehak et al., 2011) for feature extraction, and the other one used µ1 and µ2 (analogous to µz and µf in our case) from a trained FHVAE model on Mel-scale filter bank (FBank) features.
",4.2. Speech data: TIMIT,[0],[0]
"The test data were created from the test set of TIMIT, containing 24 unique speakers and 18,336 pairs for verification.",4.2. Speech data: TIMIT,[0],[0]
Table 2 presents the EER results of the proposed model and baselines.4,4.2. Speech data: TIMIT,[0],[0]
"It is clear that the µf feature performs significantly better than the i-vector method, indicating that the f variable has learned to represent a speaker’s identity.",4.2. Speech data: TIMIT,[0],[0]
"On the other hand, using µz as the features returns considerably worse EER rates compared to the i-vector method and µf feature.",4.2. Speech data: TIMIT,[0],[0]
"This is good, as it indicates that the z variables contain less information about the speaker’s identity, again validating the success of disentangling time-variant and time-independent information.",4.2. Speech data: TIMIT,[0],[0]
"Note that the EER results for µz get worse when using the full q encoder, and in the 64 dimensional feature case the verification performance of µf improves slightly.",4.2. Speech data: TIMIT,[0],[0]
"This also shows that for real-world data it is useful to use a structured inference network to further improve the quality of disentangled representation.
",4.2. Speech data: TIMIT,[0],[0]
Our results are competitive with (or slightly better than) the FHVAE results (α = 0) reported in Hsu et al. (2017).,4.2. Speech data: TIMIT,[0],[0]
The better results for FHVAE (α = 10) is obtained by adding a discriminative training objective (scaled by α) to the variational lower-bound.,4.2. Speech data: TIMIT,[0],[0]
"In a nutshell, the timeinvariant information in FHVAE is encoded in a latent variable zn2 ∼ p(zn2 |µ2), and the discriminative objective encourages zn2 encoded from a segment of one sequence to be close to the corresponding µ2 while far away from µ2 of other sequences.",4.2. Speech data: TIMIT,[0],[0]
"However, we do not test this idea here because (1) our goal is to demonstrate that the proposed architecture is a minimalistic framework for learning disentangled representations of sequential data; (2) this discriminative objective is specifically designed for hierarchical VAE, and in general the assumption behind it might not always be true (consider encoding two speech sequences coming from the same speaker).",4.2. Speech data: TIMIT,[0],[0]
"Similar ideas for discriminative training have been considered in e.g. Mathieu et al. (2016), but that discriminative objective can only be applied
4 Hsu et al. (2017) did not provide the EER results for α = 0 and µ1 in the 16 dimension case.
to two sequences that are known to entail different timeinvariant information (e.g. two sequences with different labels), which implicitly uses supervisions.",4.2. Speech data: TIMIT,[0],[0]
"Nevertheless, a better design for the discriminative objective without supervision can further improve the disentanglement of the learned representations, and we leave it to future work.",4.2. Speech data: TIMIT,[0],[0]
"Lastly, although not a main focus of the paper, we show that the usage of a stochastic transition model for the prior leads to more realistic dynamics of the generated sequence.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we consider another class of models:
p(x1:T , z,f) = p(f)p(z) T∏ t=1 p(xt|z,f).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The parameters of p(xt|z,f) are defined by a neural network NN(ht,f), with ht computed by a deterministic RNN conditioned on",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"z. We experiment with two types of deterministic dynamics, with the graphical model visualised in appendix.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The first model uses an LSTM with z as the initial state: h0 = z, ht = LSTM(ht−1).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
In later experiments we refer this dynamics as LSTM-f as the latent variable z is forward propagated in a deterministic way.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The second one deploys an LSTM conditioned on z (i.e. h0 = 0,ht = LSTM(ht−1, z)), therefore we refer it as LSTM-c. This is identical to the transition dynamics used in the FHVAE model (Hsu et al., 2017).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we refer to our model as the ’stochastic’ model (Eq. 1).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The LSTM models encodes temporal information in a global latent variable z. Therefore, small differences/errors in z will accumulate over time, which may result in unrealistic long-time dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"In contrast, the stochastic model (Eq. 1) keeps track of the time-varying aspects of xt in zt
for every t, making the reconstruction to be time-local and therefore much easier.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Therefore, the stochastic model is better suited if the sequences are long and complex.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We give empirical evidence to support this claim.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Data preprocessing & hyper-parameters.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We follow Fraccaro et al. (2017) to simulate video sequences of a ball (or a square) bouncing inside an irregular polygon using Pymunk.5,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The irregular shape was chosen because it induces chaotic dynamics, meaning that small deviations from the initial position and velocity of the ball will create exponentially diverging trajectories at long times.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
This makes memorizing the dynamics of a prototypical sequence challenging.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We randomly sampled the initial position and velocity of the ball, but did not apply any force to the ball, except for the fully elastic collisions with the walls.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We generated 5,000 sequences in total (1000 for test), each of them containing T = 30 frames with a resolution of 32×32.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the deterministic LSTMs, we fix the dimensionality of zt to 64, and set ht and the LSTM internal states to be 512 dimensions.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The latent variable dimensionality of the stochastic dynamics is dim(zt) = 16.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Qualitative & quantitative analyses.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We consider both reconstruction and missing data imputation tasks for the learned generative models.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the latter and for T = 30, the models observe the first t < T frames of a sequence and predict the remaining T − t frames using the prior dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We visualise in Figure 7 the ground truth, recon-
5http://www.pymunk.org/en/latest/.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For simplicity we disabled rotation of the square when hitting the wall, by setting the inertia to infinity.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"structed, and predicted sequences (t = 20) from all models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We further consider average fraction of incorrectly reconstructed/predicted pixels as a quantitative metric, to evaluate how well the ground-truth dynamics is recovered given consecutive missing frames.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The result is reported in Figure 8.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The stochastic model outperforms the deterministic models both qualitatively and quantitatively.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The shape of the ball is better preserved over time, and the trajectories are more physical.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"This explains the lower errors of the stochastic model, and the advantage is significant when the number of missing frames is small.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Our experiments give evidence that the stochastic model is better suited to modelling long, complex sequences when compared to the deterministic dynamical models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We expect that a better design for the stochastic transition dynamics, e.g. by combining deep neural networks with well-studied linear dynamical systems (Krishnan et al., 2015; Fraccaro et al., 2016; Karl et al., 2016; Johnson et al., 2016; Krishnan et al., 2017; Fraccaro et al., 2017), can further enhance the quality of the learned representations.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We presented a minimalistic generative model for learning disentangled representations of high-dimensional time series.,5. Conclusions and outlook,[0],[0]
"Our model consists of a global latent variable for content features, and a stochastic RNN with time-local latent variables for dynamical features.",5. Conclusions and outlook,[0],[0]
The model is trained using standard amortized variational inference.,5. Conclusions and outlook,[0],[0]
We carried out experiments both on video and audio data.,5. Conclusions and outlook,[0],[0]
"Our approach allows us to perform full and conditional generation, as well as feature swapping, such as voice conversion and video content manipulation.",5. Conclusions and outlook,[0],[0]
"We also showed that a stochastic transition model generally outperforms a deterministic one.
",5. Conclusions and outlook,[0],[0]
Future work may investigate whether a similar model applies to more complex video and audio sequences.,5. Conclusions and outlook,[0],[0]
"Also, disentangling may further be improved by additional crossentropy terms, or discriminative training.",5. Conclusions and outlook,[0],[0]
A promising avenue of research is to explore the usage of this architecture for neural compression.,5. Conclusions and outlook,[0],[0]
"An advantage of the model is that it separates dynamical from static features, allowing the latent space for the dynamical part to be low-dimensional.",5. Conclusions and outlook,[0],[0]
"We thank Robert Bamler, Rich Turner, Jeremy Wong and Yu Wang for discussions and feedback on the manuscript.",Acknowledgements,[0],[0]
We also thank Wei-Ning Hsu for helping reproduce the FHVAE experiments.,Acknowledgements,[0],[0]
Yingzhen Li thanks Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio.",abstractText,[0],[0]
"Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content).",abstractText,[0],[0]
This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features.,abstractText,[0],[0]
"In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping.",abstractText,[0],[0]
"For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.",abstractText,[0],[0]
"Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.",abstractText,[0],[0]
Disentangled Sequential Autoencoder,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1",text,[0],[0]
"Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.",1 Introduction,[0],[0]
"There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by
1Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list
the following words.",1 Introduction,[0],[0]
"An example is shown below that includes edited words and filler words.
",1 Introduction,[0],[0]
"I want a flight to Boston︸ ︷︷ ︸ edited uh I mean︸ ︷︷ ︸ filler to Denver
Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection.
",1 Introduction,[0],[0]
Many models have been evaluated for this task.,1 Introduction,[0],[0]
Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.,1 Introduction,[0],[0]
They showed that CRFs significantly outperformed Maximum Entropy models and HMMs.,1 Introduction,[0],[0]
"Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001).",1 Introduction,[0],[0]
Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking.,1 Introduction,[0],[0]
They obtained the best reported F-score of 83.8% on the Switchboard corpus.,1 Introduction,[0],[0]
"Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.
",1 Introduction,[0],[0]
"From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006).",1 Introduction,[0],[0]
"Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.",1 Introduction,[0],[0]
"They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked learning approach for disfluency detection.",1 Introduction,[0],[0]
"In our method, we first perform filler word detection, then edited word detection.",1 Introduction,[0],[0]
"In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.",1 Introduction,[0],[0]
"We also include a new type of features, called inbetween features, and incorporate them into the last step.",1 Introduction,[0],[0]
"For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.",1 Introduction,[0],[0]
"On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection.",1 Introduction,[0],[0]
"Weighted M3Ns
We use a sequence labeling model for edit detection.",2 Balancing Precision and Recall Using,[0],[0]
"Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).",2 Balancing Precision and Recall Using,[0],[0]
"For example, the previous sentence is represented as:
I/",2 Balancing Precision and Recall Using,[0],[0]
O want/O a/O flight/,2 Balancing Precision and Recall Using,[0],[0]
O to/BE Boston/EE uh/,2 Balancing Precision and Recall Using,[0],[0]
O I/,2 Balancing Precision and Recall Using,[0],[0]
"O mean/O to/O Denver/O
We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2× P ×R
P + R
There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006).",2 Balancing Precision and Recall Using,[0],[0]
"Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.
",2 Balancing Precision and Recall Using,[0],[0]
"In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary
results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).
",2 Balancing Precision and Recall Using,[0],[0]
"The learning task for M3Ns can be represented as follows:
min α
1 2 C∥ ∑ x,y αx,y∆f(x, y)∥22 + ∑ x,y αx,yL(x, y)
s.t. ∑
y
αx,y = 1 ∀x
αx,y ≥ 0, ∀x, y
The above shows the dual form for training M3Ns, where x is the observation of a training sample, y ∈ Y is a label.",2 Balancing Precision and Recall Using,[0],[0]
"α is the parameter needed to be optimized, C > 0 is the regularization parameter.",2 Balancing Precision and Recall Using,[0],[0]
"∆f(x, y) is the residual feature vector: f(x, ỹ)",2 Balancing Precision and Recall Using,[0],[0]
"− f(x, y), where ỹ is the true label of x. L(x, y) is the loss function.",2 Balancing Precision and Recall Using,[0],[0]
"Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) = ∑ t δ(yt, ỹt), where δ(a, b) is the binary indicator function (it is 0 if a = b).",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we use the weighted hamming loss:
L(x, y) = ∑
t
v(yt, ỹt)δ(yt, ỹt)
where v(yt, ỹt) is the weighted loss for the error when ỹt is mislabeled as yt.",2 Balancing Precision and Recall Using,[0],[0]
Such a weighted loss function allows us to balance the model’s precision and recall rates.,2 Balancing Precision and Recall Using,[0],[0]
"For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we tune the weight matrix v using the development dataset.",2 Balancing Precision and Recall Using,[0],[0]
"Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005).",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Its idea is to use two
(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
It has the advantage of incorporating non-local features as well as nonlinear classifiers.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In our task, we do not just use the classifier’s output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
We use 10 fold cross validation to train the low level predictors.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
The following describes the three steps in our approach.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In the first step, we automatically detect filler words.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy.",3.1 Step 1: Filler Word Detection,[0],[0]
"For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.",3.1 Step 1: Filler Word Detection,[0],[0]
"Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.
",3.1 Step 1: Filler Word Detection,[0],[0]
"For filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O. We use un-weighted hamming loss to learn M3Ns for this task.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this.",3.1 Step 1: Filler Word Detection,[0],[0]
The features we used are listed in Table 1.,3.1 Step 1: Filler Word Detection,[0],[0]
All n-grams are extracted from the original text.,3.1 Step 1: Filler Word Detection,[0],[0]
"In the second step, edited words are detected using M3Ns with the weighted-hamming loss.",3.2 Step 2: Edited Word Detection,[0],[0]
The features we used are listed in Table 2.,3.2 Step 2: Edited Word Detection,[0],[0]
All n-grams in the first step are also used here.,3.2 Step 2: Edited Word Detection,[0],[0]
"Besides that, word n-grams, POS n-grams and logic",3.2 Step 2: Edited Word Detection,[0],[0]
n,3.2 Step 2: Edited Word Detection,[0],[0]
-grams extracted from filler word removed text are included.,3.2 Step 2: Edited Word Detection,[0],[0]
"Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.",3.2 Step 2: Edited Word Detection,[0],[0]
"In this step, we use n-gram features extracted from the text after removing edit disfluencies based on
the previous step.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"The following is an example:
• Ref: The new type is prettier than what their/SE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"• Sys: The new type is prettier than what/BE their/EE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Using the system’s prediction, we would have bigram than they, which is odd.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Usually, the pronoun following than is accusative case.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,
2011).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"They have the benefit of measuring the fluency of the cleaned text.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"For example,
• Coordinates: they ca n′t decide which are the good aspects and which are the bad aspects
• Rough Copies: it/",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"BE ′s/IE a/IE pleasure/IE to/EE it s good to get outside
To distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We extract all the pieces lying between identical word bigrams AB . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
AB.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
The observation is that coordinates are often longer than edited sequences.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
Hence we introduce the in-between features for each word.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We also used other patterns such as A . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
A and ABC . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"ABC, but they are too noisy or infrequent and do not yield much performance gain.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
Table 3 lists the feature templates used in this last step.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).",4.1 Experimental Setup,[0],[0]
"We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and
such kind of information is not available (Johnson and Charniak, 2004).
",4.1 Experimental Setup,[0],[0]
We tuned the weight matrix for hamming loss on the development dataset using simple grid search.,4.1 Experimental Setup,[0],[0]
"The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.",4.1 Experimental Setup,[0],[0]
The optimal weight matrix is shown in Table 4.,4.1 Experimental Setup,[0],[0]
"Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ·E will be mapped to the class of edited words.",4.1 Experimental Setup,[0],[0]
"We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.",4.2 Results,[0],[0]
"For each model, we tuned the parameters on the development data:",4.2 Results,[0],[0]
"Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1.",4.2 Results,[0],[0]
"For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.",4.2 Results,[0],[0]
"Regularization penalty is C = 0.1 and iteration number is 30.
",4.2 Results,[0],[0]
Table 5 shows the results using different models and features.,4.2 Results,[0],[0]
The baseline models use only the ngrams features extracted from the original text.,4.2 Results,[0],[0]
"We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models.",4.2 Results,[0],[0]
"Regarding the features, the gradually added n-gram features have consistent improvement for all models.",4.2 Results,[0],[0]
"Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words.",4.2 Results,[0],[0]
"In our analysis, we also noticed that the in-between fea-
tures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words).",4.2 Results,[0],[0]
"We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001).",4.2 Results,[0],[0]
"There are no significant differences among CRFs, AP and PA.",4.2 Results,[0],[0]
"Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value < 0.001).
",4.2 Results,[0],[0]
"We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6.",4.2 Results,[0],[0]
We achieved the best F-score.,4.2 Results,[0],[0]
"The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models.",4.2 Results,[0],[0]
"In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.",5 Conclusion,[0],[0]
The first level removes the filler words providing new ngrams for the second level to remove edited words.,5 Conclusion,[0],[0]
"The
third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.",5 Conclusion,[0],[0]
"To minimize the F-loss approximately, we modified the hamming loss in M3Ns.",5 Conclusion,[0],[0]
"Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.",5 Conclusion,[0],[0]
We thank three anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041.,Acknowledgments,[0],[0]
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.,Acknowledgments,[0],[0]
"In this paper, we propose a multi-step stacked learning model for disfluency detection.",abstractText,[0],[0]
Our method incorporates refined n-gram features step by step from different word sequences.,abstractText,[0],[0]
"First, we detect filler words.",abstractText,[0],[0]
"Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.",abstractText,[0],[0]
"In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.",abstractText,[0],[0]
We use Max-Margin Markov Networks (MNs) as the classifier with the weighted hamming loss to balance precision and recall.,abstractText,[0],[0]
Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and MNs with weighted hamming loss can significantly improve the performance.,abstractText,[0],[0]
Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1,abstractText,[0],[0]
Disfluency Detection Using Multi-step Stacked Learning,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1753–1762, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods.",text,[0],[0]
"In relation extraction, one challenge that is faced when building a machine learning system is the generation of training examples.",1 Introduction,[0],[0]
"One common technique for coping with this difficulty is distant supervision (Mintz et al., 2009) which assumes that if two entities have a relationship in a known knowledge base, then all sentences that mention these two entities will express that relationship in some way.",1 Introduction,[0],[0]
"Figure 1 shows an example of the auto-
matic labeling of data through distant supervision.",1 Introduction,[0],[0]
"In this example, Apple and Steve Jobs are two related entities in Freebase1.",1 Introduction,[0],[0]
All sentences that contain these two entities are selected as training instances.,1 Introduction,[0],[0]
The distant supervision strategy is an effective method of automatically labeling training data.,1 Introduction,[0],[0]
"However, it has two major shortcomings when used for relation extraction.
",1 Introduction,[0],[0]
"First, the distant supervision assumption is too strong and causes the wrong label problem.",1 Introduction,[0],[0]
A sentence that mentions two entities does not necessarily express their relation in a knowledge base.,1 Introduction,[0],[0]
It is possible that these two entities may simply share the same topic.,1 Introduction,[0],[0]
"For instance, the upper sentence indeed expresses the “company/founders” relation in Figure 1.",1 Introduction,[0],[0]
"The lower sentence, however, does not express this relation but is still selected as a training instance.",1 Introduction,[0],[0]
"This will hinder the performance of a model trained on such noisy data.
",1 Introduction,[0],[0]
"Second, previous methods (Mintz et al., 2009; Riedel et al., 2010; Hoffmann et al., 2011) have typically applied supervised models to elaborately designed features when obtained the labeled data through distant supervision.",1 Introduction,[0],[0]
These features are often derived from preexisting Natural Language Processing (NLP) tools.,1 Introduction,[0],[0]
"Since errors inevitably exist in NLP tools, the use of traditional features leads to error propagation or accumulation.",1 Introduction,[0],[0]
"Distant supervised relation extraction generally ad-
1http://www.freebase.com/
1753
dresses corpora from the Web, including many informal texts.",1 Introduction,[0],[0]
Figure 2 shows the sentence length distribution of a benchmark distant supervision dataset that was developed by Riedel et al. (2010).,1 Introduction,[0],[0]
Approximately half of the sentences are longer than 40 words.,1 Introduction,[0],[0]
McDonald and Nivre (2007) showed that the accuracy of syntactic parsing decreases significantly with increasing sentence length.,1 Introduction,[0],[0]
"Therefore, when using traditional features, the problem of error propagation or accumulation will not only exist, it will grow more serious.
",1 Introduction,[0],[0]
"In this paper, we propose a novel model dubbed Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address the two problems described above.",1 Introduction,[0],[0]
"To address the first problem, distant supervised relation extraction is treated as a multi-instance problem similar to previous studies (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012).",1 Introduction,[0],[0]
"In multi-instance problem, the training set consists of many bags, and each contains many instances.",1 Introduction,[0],[0]
"The labels of the bags are known; however, the labels of the instances in the bags are unknown.",1 Introduction,[0],[0]
We design an objective function at the bag level.,1 Introduction,[0],[0]
"In the learning process, the uncertainty of instance labels can be taken into account; this alleviates the wrong label problem.
",1 Introduction,[0],[0]
"To address the second problem, we adopt convolutional architecture to automatically learn relevant features without complicated NLP preprocessing inspired by Zeng et al. (2014).",1 Introduction,[0],[0]
"Our proposal is an extension of Zeng et al. (2014), in which a single max pooling operation is utilized to determine the most significant features.",1 Introduction,[0],[0]
"Although this operation has been shown to be effective for textual feature representation (Collobert et al., 2011; Kim, 2014), it reduces the size of the
hidden layers too rapidly and cannot capture the structural information between two entities (Graham, 2014).",1 Introduction,[0],[0]
"For example, to identify the relation between Steve Jobs and Apple in Figure 1, we need to specify the entities and extract the structural features between them.",1 Introduction,[0],[0]
Several approaches have employed manually crafted features that attempt to model such structural information.,1 Introduction,[0],[0]
These approaches usually consider both internal and external contexts.,1 Introduction,[0],[0]
A sentence is inherently divided into three segments according to the two given entities.,1 Introduction,[0],[0]
"The internal context includes the characters inside the two entities, and the external context involves the characters around the two entities (Zhang et al., 2006).",1 Introduction,[0],[0]
"Clearly, single max pooling is not sufficient to capture such structural information.",1 Introduction,[0],[0]
"To capture structural and other latent information, we divide the convolution results into three segments based on the positions of the two given entities and devise a piecewise max pooling layer instead of the single max pooling layer.",1 Introduction,[0],[0]
The piecewise max pooling procedure returns the maximum value in each segment instead of a single maximum value over the entire sentence.,1 Introduction,[0],[0]
"Thus, it is expected to exhibit superior performance compared with traditional methods.
",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows.
",1 Introduction,[0],[0]
• We explore the feasibility of performing distant supervised relation extraction without hand-designed features.,1 Introduction,[0],[0]
"PCNNS are proposed to automatically learn features without complicated NLP preprocessing.
",1 Introduction,[0],[0]
"• To address the wrong label problem, we develop innovative solutions that incorporate multi-instance learning into the PCNNS for distant supervised relation extraction.
",1 Introduction,[0],[0]
"• In the proposed network, we devise a piecewise max pooling layer, which aims to capture structural information between two entities.",1 Introduction,[0],[0]
Relation extraction is one of the most important topics in NLP.,2 Related Work,[0],[0]
"Many approaches to relation extraction have been developed, such as bootstrapping, unsupervised relation discovery and supervised classification.",2 Related Work,[0],[0]
"Supervised approaches are the most commonly used methods for relation
extraction and yield relatively high performance (Bunescu and Mooney, 2006; Zelenko et al., 2003; Zhou et al., 2005).",2 Related Work,[0],[0]
"In the supervised paradigm, relation extraction is considered to be a multi-class classification problem and may suffer from a lack of labeled data for training.",2 Related Work,[0],[0]
"To address this problem, Mintz et al. (2009) adopted Freebase to perform distant supervision.",2 Related Work,[0],[0]
"As described in Section 1, the algorithm for training data generation is sometimes faced with the wrong label problem.",2 Related Work,[0],[0]
"To address this shortcoming, (Riedel et al., 2010; Hoffmann et al., 2011; Surdeanu et al., 2012) developed the relaxed distant supervision assumption for multi-instance learning.",2 Related Work,[0],[0]
"The term ‘multiinstance learning was coined by (Dietterich et al., 1997) while investigating the problem of predicting drug activity.",2 Related Work,[0],[0]
"In multi-instance learning, the uncertainty of instance labels can be taken into account.",2 Related Work,[0],[0]
"The focus of multi-instance learning is to discriminate among the bags.
",2 Related Work,[0],[0]
These methods have been shown to be effective for relation extraction.,2 Related Work,[0],[0]
"However, their performance depends strongly on the quality of the designed features.",2 Related Work,[0],[0]
Most existing studies have concentrated on extracting features to identify the relations between two entities.,2 Related Work,[0],[0]
Previous methods can be generally categorized into two types: feature-based methods and kernel-based methods.,2 Related Work,[0],[0]
"In feature-based methods, a diverse set of strategies is exploited to convert classification clues (e.g., sequences, parse trees) into feature vectors (Kambhatla, 2004; Suchanek et al., 2006).",2 Related Work,[0],[0]
Feature-based methods suffer from the necessity of selecting a suitable feature set when converting structured representations into feature vectors.,2 Related Work,[0],[0]
"Kernel-based methods provide a natural alternative to exploit rich representations of input classification clues, such as syntactic parse trees.",2 Related Work,[0],[0]
Kernelbased methods enable the use of a large set of features without needing to extract them explicitly.,2 Related Work,[0],[0]
"Several kernels have been proposed, such as the convolution tree kernel (Qian et al., 2008), the subsequence kernel (Bunescu and Mooney, 2006) and the dependency tree kernel (Bunescu and Mooney, 2005).
",2 Related Work,[0],[0]
"Nevertheless, as mentioned in Section 1, it is difficult to design high-quality features using existing NLP tools.",2 Related Work,[0],[0]
"With the recent revival of interest in neural networks, many researchers have investigated the possibility of using neural networks to automatically learn features (Socher et
al., 2012; Zeng et al., 2014).",2 Related Work,[0],[0]
"Inspired by Zeng et al. (2014), we propose the use of PCNNs with multi-instance learning to automatically learn features for distant supervised relation extraction.",2 Related Work,[0],[0]
Dietterich et al. (1997) suggested that the design of multi-instance modifications for neural networks is a particularly interesting topic.,2 Related Work,[0],[0]
Zhang and Zhou (2006) successfully incorporated multiinstance learning into traditional Backpropagation (BP) and Radial Basis Function (RBF) networks and optimized these networks by minimizing a sum-of-squares error function.,2 Related Work,[0],[0]
"In contrast to their method, we define the objective function based on the cross-entropy principle.",2 Related Work,[0],[0]
Distant supervised relation extraction is formulated as multi-instance problem.,3 Methodology,[0],[0]
"In this section, we present innovative solutions that incorporate multi-instance learning into a convolutional neural network to fulfill this task.",3 Methodology,[0],[0]
PCNNs are proposed for the automatic learning of features without complicated NLP preprocessing.,3 Methodology,[0],[0]
Figure 3 shows our neural network architecture for distant supervised relation extraction.,3 Methodology,[0],[0]
It illustrates the procedure that handles one instance of a bag.,3 Methodology,[0],[0]
"This procedure includes four main parts: Vector Representation, Convolution, Piecewise Max Pooling and Softmax Output.",3 Methodology,[0],[0]
We describe these parts in detail below.,3 Methodology,[0],[0]
The inputs of our network are raw word tokens.,3.1 Vector Representation,[0],[0]
"When using neural networks, we typically transform word tokens into low-dimensional vectors.",3.1 Vector Representation,[0],[0]
"In our method, each input word token is transformed into a vector by looking up pre-trained word embeddings.",3.1 Vector Representation,[0],[0]
"Moreover, we use position features (PFs) to specify entity pairs, which are also transformed into vectors by looking up position embeddings.",3.1 Vector Representation,[0],[0]
Word embeddings are distributed representations of words that map each word in a text to a ‘k’dimensional real-valued vector.,3.1.1 Word Embeddings,[0],[0]
"They have recently been shown to capture both semantic and syntactic information about words very well, setting performance records in several word similarity tasks (Mikolov et al., 2013; Pennington et al., 2014).",3.1.1 Word Embeddings,[0],[0]
"Using word embeddings that have been trained a priori has become common practice for
enhancing many other NLP tasks (Parikh et al., 2014; Huang et al., 2014).
",3.1.1 Word Embeddings,[0],[0]
A common method of training a neural network is to randomly initialize all parameters and then optimize them using an optimization algorithm.,3.1.1 Word Embeddings,[0],[0]
"Recent research (Erhan et al., 2010) has shown that neural networks can converge to better local minima when they are initialized with word embeddings.",3.1.1 Word Embeddings,[0],[0]
Word embeddings are typically learned in an entirely unsupervised manner by exploiting the co-occurrence structure of words in unlabeled text.,3.1.1 Word Embeddings,[0],[0]
"Researchers have proposed several methods of training word embeddings (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013).",3.1.1 Word Embeddings,[0],[0]
"In this paper, we use the Skip-gram model (Mikolov et al., 2013) to train word embeddings.",3.1.1 Word Embeddings,[0],[0]
"In relation extraction, we focus on assigning labels to entity pairs.",3.1.2 Position Embeddings,[0],[0]
"Similar to Zeng et al. (2014), we use PFs to specify entity pairs.",3.1.2 Position Embeddings,[0],[0]
A PF is defined as the combination of the relative distances from the current word to e1 and e2.,3.1.2 Position Embeddings,[0],[0]
"For instance, in the following example, the relative distances from son to e1 (Kojo Annan) and e2 (Kofi Annan) are 3 and -2, respectively.
...",3.1.2 Position Embeddings,[0],[0]
"hired Kojo Annan , the son of Kofi Annan , in ...3 -2 Two position embedding matrixes (PF1 and PF2) are randomly initialized.",3.1.2 Position Embeddings,[0],[0]
We then transform the relative distances into real valued vectors by looking up the position embedding matrixes.,3.1.2 Position Embeddings,[0],[0]
"In the example shown in Figure 3, it is assumed that
the size of the word embedding is dw = 4 and that the size of the position embedding is dp = 1.",3.1.2 Position Embeddings,[0],[0]
"In combined word embeddings and position embeddings, the vector representation part transforms an instance into a matrix S ∈ Rs×d, where s is the sentence length and d = dw + dp ∗ 2.",3.1.2 Position Embeddings,[0],[0]
The matrix S is subsequently fed into the convolution part.,3.1.2 Position Embeddings,[0],[0]
"In relation extraction, an input sentence that is marked as containing the target entities corresponds only to a relation type; it does not predict labels for each word.",3.2 Convolution,[0],[0]
"Thus, it might be necessary to utilize all local features and perform this prediction globally.",3.2 Convolution,[0],[0]
"When using a neural network, the convolution approach is a natural means of merging all these features (Collobert et al., 2011).
",3.2 Convolution,[0],[0]
"Convolution is an operation between a vector of weights, w, and a vector of inputs that is treated as a sequence q.",3.2 Convolution,[0],[0]
The weights matrix w is regarded as the filter for the convolution.,3.2 Convolution,[0],[0]
"In the example shown in Figure 3, we assume that the length of the filter is w (w = 3);",3.2 Convolution,[0],[0]
"thus, w ∈ Rm (m = w∗d).",3.2 Convolution,[0],[0]
"We consider S to be a sequence {q1,q2, · · · ,qs}, where qi ∈ Rd.",3.2 Convolution,[0],[0]
"In general, let qi:j refer to the concatenation of qi to qj .",3.2 Convolution,[0],[0]
"The convolution operation involves taking the dot product of w with each w-gram in the sequence q to obtain another sequence c ∈ Rs+w−1:
cj = wqj−w+1:j (1)
where the index j ranges from 1 to s+w−1.",3.2 Convolution,[0],[0]
"Outof-range input values qi, where i < 1 or i > s, are
taken to be zero.",3.2 Convolution,[0],[0]
The ability to capture different features typically requires the use of multiple filters (or feature maps) in the convolution.,3.2 Convolution,[0],[0]
"Under the assumption that we use n filters (W = {w1,w2, · · · ,wn}), the convolution operation can be expressed as follows:
cij = wiqj−w+1:j 1 ≤ i ≤ n",3.2 Convolution,[0],[0]
(2),3.2 Convolution,[0],[0]
"The convolution result is a matrix C = {c1, c2, · · · , cn} ∈ Rn×(s+w−1).",3.2 Convolution,[0],[0]
Figure 3 shows an example in which we use 3 different filters in the convolution procedure.,3.2 Convolution,[0],[0]
The size of the convolution output matrix C ∈ Rn×(s+w−1) depends on the number of tokens s in the sentence that is fed into the network.,3.3 Piecewise Max Pooling,[0],[0]
"To apply subsequent layers, the features that are extracted by the convolution layer must be combined such that they are independent of the sentence length.",3.3 Piecewise Max Pooling,[0],[0]
"In traditional Convolution Neural Networks (CNNs), max pooling operations are often applied for this purpose (Collobert et al., 2011; Zeng et al., 2014).",3.3 Piecewise Max Pooling,[0],[0]
This type of pooling scheme naturally addresses variable sentence lengths.,3.3 Piecewise Max Pooling,[0],[0]
"The idea is to capture the most significant features (with the highest values) in each feature map.
",3.3 Piecewise Max Pooling,[0],[0]
"However, despite the widespread use of single max pooling, this approach is insufficient for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"As described in the first section, single max pooling reduces the size of the hidden layers too rapidly and is too coarse to capture finegrained features for relation extraction.",3.3 Piecewise Max Pooling,[0],[0]
"In addition, single max pooling is not sufficient to capture the structural information between two entities.",3.3 Piecewise Max Pooling,[0],[0]
"In relation extraction, an input sentence can be divided into three segments based on the two selected entities.",3.3 Piecewise Max Pooling,[0],[0]
"Therefore, we propose a piecewise max pooling procedure that returns the maximum value in each segment instead of a single maximum value.",3.3 Piecewise Max Pooling,[0],[0]
"As shown in Figure 3, the output of each convolutional filter ci is divided into three segments {ci1, ci2, ci3} by Kojo Annan and Kofi Annan.",3.3 Piecewise Max Pooling,[0],[0]
"The piecewise max pooling procedure can be expressed as follows:
pij = max(cij) 1 ≤ i ≤ n, 1 ≤ j ≤ 3 (3) For the output of each convolutional filter, we can obtain a 3-dimensional vector pi = {pi1, pi2, pi3}.",3.3 Piecewise Max Pooling,[0],[0]
"We then concatenate all vectors
p1:n and apply a non-linear function, such as the hyperbolic tangent.",3.3 Piecewise Max Pooling,[0],[0]
"Finally, the piecewise max pooling procedure outputs a vector:
g = tanh(p1:n) (4)
where g ∈ R3n.",3.3 Piecewise Max Pooling,[0],[0]
The size of g is fixed and is no longer related to the sentence length.,3.3 Piecewise Max Pooling,[0],[0]
"To compute the confidence of each relation, the feature vector g is fed into a softmax classifier.
",3.4 Softmax Output,[0],[0]
"o = W1g + b (5)
W1 ∈ Rn1×3n is the transformation matrix, and o ∈ Rn1 is the final output of the network, where n1 is equal to the number of possible relation types for the relation extraction system.
",3.4 Softmax Output,[0],[0]
"We employ dropout (Hinton et al., 2012) on the penultimate layer for regularization.",3.4 Softmax Output,[0],[0]
Dropout prevents the co-adaptation of hidden units by randomly dropping out a proportion p of the hidden units during forward computing.,3.4 Softmax Output,[0],[0]
"We first apply a “masking” operation (g◦r) on g, where r is a vector of Bernoulli random variables with probability p of being 1. Eq.(5)",3.4 Softmax Output,[0],[0]
"becomes:
o = W1(g ◦ r) + b (6)
",3.4 Softmax Output,[0],[0]
Each output can then be interpreted as the confidence score of the corresponding relation.,3.4 Softmax Output,[0],[0]
This score can be interpreted as a conditional probability by applying a softmax operation (see Section 3.5).,3.4 Softmax Output,[0],[0]
"In the test procedure, the learned weight vectors are scaled by p such that Ŵ1 = pW1 and are used (without dropout) to score unseen instances.",3.4 Softmax Output,[0],[0]
"In order to alleviate the wrong label problem, we use multi-instance learning for PCNNs.",3.5 Multi-instance Learning,[0],[0]
"The PCNNs-based relation extraction can be stated as a quintuple θ = (E,PF1,PF2,W,W1)2.",3.5 Multi-instance Learning,[0],[0]
The input to the network is a bag.,3.5 Multi-instance Learning,[0],[0]
"Suppose that there are T bags {M1, M2, · · · ,MT } and that the i-th bag contains qi instances Mi = {m1i ,m2i , · · · , mqii }.",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to predict the labels of the unseen bags.,3.5 Multi-instance Learning,[0],[0]
"In this paper, all instances in a bag are considered independently.",3.5 Multi-instance Learning,[0],[0]
"Given an input instance mji , the network with the parameter θ outputs a vector o, where the r-th component or corresponds to the score associated
2E represents the word embeddings.
",3.5 Multi-instance Learning,[0],[0]
Algorithm 1 Multi-instance learning 1: Initialize θ.,3.5 Multi-instance Learning,[0],[0]
"Partition the bags into mini-
batches of size bs. 2: Randomly choose a mini-batch, and feed the
bags into the network one by one.",3.5 Multi-instance Learning,[0],[0]
3: Find the j-th instance mji (1 ≤,3.5 Multi-instance Learning,[0],[0]
"i ≤ bs) in each
bag according to Eq. (9).",3.5 Multi-instance Learning,[0],[0]
"4: Update θ based on the gradients of mji (1 ≤
i ≤ bs) via Adadelta.",3.5 Multi-instance Learning,[0],[0]
"5: Repeat steps 2-4 until either convergence or
the maximum number of epochs is reached.
with relation r.",3.5 Multi-instance Learning,[0],[0]
"To obtain the conditional probability p(r|m, θ), we apply a softmax operation over all relation types:
p(r|mji ; θ) = eor n1∑ k=1 eok (7)
",3.5 Multi-instance Learning,[0],[0]
The objective of multi-instance learning is to discriminate bags rather than instances.,3.5 Multi-instance Learning,[0],[0]
"To do so, we must define the objective function on the bags.",3.5 Multi-instance Learning,[0],[0]
"Given all (T ) training bags (Mi, yi), we can define the objective function using cross-entropy at the bag level as follows:
J (θ) = T∑
i=1
log p(yi|mji ; θ) (8)
where j is constrained as follows:
j∗ = arg max j p(yi|mji ; θ) 1 ≤ j ≤ qi (9)
",3.5 Multi-instance Learning,[0],[0]
"Using this defined objective function, we maximize J(θ) through stochastic gradient descent over shuffled mini-batches with the Adadelta (Zeiler, 2012) update rule.",3.5 Multi-instance Learning,[0],[0]
"The entire training procedure is described in Algorithm 1.
",3.5 Multi-instance Learning,[0],[0]
"From the introduction presented above, we know that the traditional backpropagation algorithm modifies a network in accordance with all training instances, whereas backpropagation with multi-instance learning modifies a network based on bags.",3.5 Multi-instance Learning,[0],[0]
"Thus, our method captures the nature of distant supervised relation extraction, in which some training instances will inevitably be incorrectly labeled.",3.5 Multi-instance Learning,[0],[0]
"When a trained PCNN is used for prediction, a bag is positively labeled if and only if the output of the network on at least one of its instances is assigned a positive label.",3.5 Multi-instance Learning,[0],[0]
Our experiments are intended to provide evidence that supports the following hypothesis: automatically learning features using PCNNs with multiinstance learning can lead to an increase in performance.,4 Experiments,[0],[0]
"To this end, we first introduce the dataset and evaluation metrics used.",4 Experiments,[0],[0]
"Next, we test several variants via cross-validation to determine the parameters to be used in our experiments.",4 Experiments,[0],[0]
We then compare the performance of our method to those of several traditional methods.,4 Experiments,[0],[0]
"Finally, we evaluate the effects of piecewise max pooling and multiinstance learning3.",4 Experiments,[0],[0]
"We evaluate our method on a widely used dataset4 that was developed by (Riedel et al., 2010) and has also been used by (Hoffmann et al., 2011; Surdeanu et al., 2012).",4.1 Dataset and Evaluation Metrics,[0],[0]
"This dataset was generated by aligning Freebase relations with the NYT corpus, with sentences from the years 2005-2006 used as the training corpus and sentences from 2007 used as the testing corpus.
",4.1 Dataset and Evaluation Metrics,[0],[0]
"Following previous work (Mintz et al., 2009), we evaluate our method in two ways: the held-out evaluation and the manual evaluation.",4.1 Dataset and Evaluation Metrics,[0],[0]
The heldout evaluation only compares the extracted relation instances against Freebase relation data and reports the precision/recall curves of the experiments.,4.1 Dataset and Evaluation Metrics,[0],[0]
"In the manual evaluation, we manually check the newly discovered relation instances that are not in Freebase.",4.1 Dataset and Evaluation Metrics,[0],[0]
"In this paper, we use the Skip-gram model (word2vec)5 to train the word embeddings on the NYT corpus.",4.2.1 Pre-trained Word Embeddings,[0],[0]
Word2vec first constructs a vocabulary from the training text data and then learns vector representations of the words.,4.2.1 Pre-trained Word Embeddings,[0],[0]
"To obtain the embeddings of the entities, we concatenate the tokens of a entity using the ## operator when the entity has multiple word tokens.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Since a comparison of the word embeddings is beyond the scope
3With regard to the position feature, our experiments yield the same positive results described in Zeng et al. (2014).",4.2.1 Pre-trained Word Embeddings,[0],[0]
"Because the position feature is not the main contribution of this paper, we do not present the results without the position feature.
",4.2.1 Pre-trained Word Embeddings,[0],[0]
"4http://iesl.cs.umass.edu/riedel/ecml/ 5https://code.google.com/p/word2vec/
Window size Feature maps
Word dimension
Position dimension
Batch size
Adadelta parameter Dropout probability
of this paper, our experiments directly utilize 50- dimensional vectors.",4.2.1 Pre-trained Word Embeddings,[0],[0]
"In this section, we experimentally study the effects of two parameters on our models: the window size, w, and the number of feature maps, n. Following (Surdeanu et al., 2012), we tune all of the models using three-fold validation on the training set.",4.2.2 Parameter Settings,[0],[0]
"We use a grid search to determine the optimal parameters and manually specify subsets of the parameter spaces: w ∈ {1, 2, 3, · · · , 7} and n ∈ {50, 60, · · · , 300}.",4.2.2 Parameter Settings,[0],[0]
Table 1 shows all parameters used in the experiments.,4.2.2 Parameter Settings,[0],[0]
"Because the position dimension has little effect on the result, we heuristically choose dp = 5.",4.2.2 Parameter Settings,[0],[0]
The batch size is fixed to 50.,4.2.2 Parameter Settings,[0],[0]
"We use Adadelta (Zeiler, 2012) in the update procedure; it relies on two main parameters, ρ and ε, which do not significantly affect the performance (Zeiler, 2012).",4.2.2 Parameter Settings,[0],[0]
"Following (Zeiler, 2012), we choose 0.95 and 1e−6, respectively, as the values of these parameters.",4.2.2 Parameter Settings,[0],[0]
"In the dropout operation, we randomly set the hidden unit activities to zero with a probability of 0.5 during training.",4.2.2 Parameter Settings,[0],[0]
The held-out evaluation provides an approximate measure of precision without requiring costly human evaluation.,4.3.1 Held-out Evaluation,[0],[0]
Half of the Freebase relations are used for testing.,4.3.1 Held-out Evaluation,[0],[0]
"The relation instances discovered from the test articles are automatically compared with those in Freebase.
",4.3.1 Held-out Evaluation,[0],[0]
"To evaluate the proposed method, we select the following three traditional methods for comparison.",4.3.1 Held-out Evaluation,[0],[0]
"Mintz represents a traditional distantsupervision-based model that was proposed by (Mintz et al., 2009).",4.3.1 Held-out Evaluation,[0],[0]
"MultiR is a multi-instance learning method that was proposed by (Hoffmann et al., 2011).",4.3.1 Held-out Evaluation,[0],[0]
"MIML is a multi-instance multilabel model that was proposed by (Surdeanu et al., 2012).",4.3.1 Held-out Evaluation,[0],[0]
"Figure 4 shows the precision-recall curves for each method, where PCNNs+MIL denotes our method, and demonstrates that PCNNs+MIL achieves higher precision over the entire range of recall.",4.3.1 Held-out Evaluation,[0],[0]
"PCNNs+MIL enhances the recall to ap-
proximately 34% without any loss of precision.",4.3.1 Held-out Evaluation,[0],[0]
"In terms of both precision and recall, PCNNs+MIL outperforms all other evaluated approaches.",4.3.1 Held-out Evaluation,[0],[0]
"Notably, the results of the methods evaluated for comparison were obtained using manually crafted features.",4.3.1 Held-out Evaluation,[0],[0]
"By contrast, our result is obtained by automatically learning features from original words.",4.3.1 Held-out Evaluation,[0],[0]
The results demonstrate that the proposed method is an effective technique for distant supervised relation extraction.,4.3.1 Held-out Evaluation,[0],[0]
Automatically learning features via PCNNs can alleviate the error propagation that occurs in traditional feature extraction.,4.3.1 Held-out Evaluation,[0],[0]
Incorporating multi-instance learning into a convolutional neural network is an effective means of addressing the wrong label problem.,4.3.1 Held-out Evaluation,[0],[0]
It is worth emphasizing that there is a sharp decline in the held-out precision-recall curves of PCNNs+MIL at very low recall (Figure 4).,4.3.2 Manual Evaluation,[0],[0]
"A manual check of the misclassified examples that were produced with high confidence reveals that the ma-
jorities of these examples are false negatives and are actually true relation instances that were misclassified due to the incomplete nature of Freebase.
",4.3.2 Manual Evaluation,[0],[0]
"Thus, the held-out evaluation suffers from false negatives in Freebase.",4.3.2 Manual Evaluation,[0],[0]
We perform a manual evaluation to eliminate these problems.,4.3.2 Manual Evaluation,[0],[0]
"For the manual evaluation, we choose the entity pairs for which at least one participating entity is not present in Freebase as a candidate.",4.3.2 Manual Evaluation,[0],[0]
This means that there is no overlap between the held-out and manual candidates.,4.3.2 Manual Evaluation,[0],[0]
"Because the number of relation instances that are expressed in the test data is unknown, we cannot calculate the recall in this case.",4.3.2 Manual Evaluation,[0],[0]
"Instead, we calculate the precision of the top N extracted relation instances.",4.3.2 Manual Evaluation,[0],[0]
"Table 2 presents the manually evaluated precisions for the top 100, top 200, and top 500 extracted instances.",4.3.2 Manual Evaluation,[0],[0]
"The results show that PCNNs+MIL achieves the best performance; moreover, the precision is higher than in the held-out evaluation.",4.3.2 Manual Evaluation,[0],[0]
"This finding indicates that many of the false negatives that we predict are, in fact, true relational facts.",4.3.2 Manual Evaluation,[0],[0]
The sharp decline observed in the held-out precision-recall curves is therefore reasonable.,4.3.2 Manual Evaluation,[0],[0]
"In this paper, we develop a method of piecewise max pooling and incorporate multi-instance learning into convolutional neural networks for distant supervised relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"To demonstrate the effects of these two techniques, we empirically study the performance of systems in which these techniques are not implemented through held-out evaluations (Figure 5).",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
CNNs represents convolutional neural networks to which single max pooling is applied.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Figure 5 shows that when piecewise max pooling is used (PCNNs), better results are produced than those achieved using CNNs.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Moreover, compared with CNNs+MIL, PCNNs achieve slightly higher precision when the recall is greater than 0.08.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Since the parameters for all the model are determined by grid search, we can observe that CNNs cannot achieve competitive results compared to PCNNs when increasing the size of the hidden layer of convolutional neural networks.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
It means that we cannot capture more useful information by simply increasing the network parameter.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"These results demonstrate that the proposed piecewise max pooling technique is beneficial and
can effectively capture structural information for relation extraction.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
A similar phenomenon is also observed when multi-instance learning is added to the network.,4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"Both CNNs+MIL and PCNNs+MIL outperform their counterparts CNNs and PCNNs, respectively, thereby demonstrating that incorporation of multi-instance learning into our neural network was successful in solving the wrong label problem.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"As expected, PCNNs+MIL obtains the best results because the advantages of both techniques are achieved simultaneously.",4.4 Effect of Piecewise Max Pooling and Multi-instance Learning,[0],[0]
"In this paper, we exploit Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning for distant supervised relation extraction.",5 Conclusion,[0],[0]
"In our method, features are automatically learned without complicated NLP preprocessing.",5 Conclusion,[0],[0]
We also successfully devise a piecewise max pooling layer in the proposed network to capture structural information and incorporate multi-instance learning to address the wrong label problem.,5 Conclusion,[0],[0]
Experimental results show that the proposed approach offers significant improvements over comparable methods.,5 Conclusion,[0],[0]
This work was sponsored by the National Basic Research Program of China (no. 2014CB340503) and the National Natural Science Foundation of China (no. 61272332 and no. 61202329).,Acknowledgments,[0],[0]
We thank the anonymous reviewers for their insightful comments.,Acknowledgments,[0],[0]
Two problems arise when using distant supervision for relation extraction.,abstractText,[0],[0]
"First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data.",abstractText,[0],[0]
"However, the heuristic alignment can fail, resulting in wrong label problem.",abstractText,[0],[0]
"In addition, in previous approaches, statistical models have typically been applied to ad hoc features.",abstractText,[0],[0]
The noise that originates from the feature extraction process can cause poor performance.,abstractText,[0],[0]
"In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems.",abstractText,[0],[0]
"To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account.",abstractText,[0],[0]
"To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features.",abstractText,[0],[0]
Experiments show that our method is effective and outperforms several competitive baseline methods.,abstractText,[0],[0]
Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 614–620 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
614",text,[0],[0]
Low-resource languages lack manually annotated data to learn even the most basic models such as part-of-speech (POS) taggers.,1 Introduction,[0],[0]
"To compensate for the absence of direct supervision, work in crosslingual learning and distant supervision has discovered creative use for a number of alternative data sources to learn feasible models: – aligned parallel corpora to project POS annota-
tions to target languages (Yarowsky et al., 2001; Agić",1 Introduction,[0],[0]
"et al., 2015; Fang and Cohn, 2016), – noisy tag dictionaries for type-level approximation of full supervision (Li et al., 2012), – combination of projection and type constraints (Das and Petrov, 2011; Täckström",1 Introduction,[0],[0]
"et al., 2013), – rapid annotation of seed training data (Garrette and Baldridge, 2013; Garrette et al., 2013).",1 Introduction,[0],[0]
"However, only one or two compatible sources of distant supervision are typically employed.",1 Introduction,[0],[0]
In reality severely under-resourced languages may require a more pragmatic “take what you can get” viewpoint.,1 Introduction,[0],[0]
"Our results suggest that combining supervision sources is the way to go about creating viable low-resource taggers.
",1 Introduction,[0],[0]
We propose a method to strike a balance between model simplicity and the capacity to easily integrate heterogeneous learning signals.,1 Introduction,[0],[0]
"Our
system is a uniform neural model for POS tagging that learns from disparate sources of distant supervision (DSDS).",1 Introduction,[0],[0]
"We use it to combine: i) multi-source annotation projection, ii) instance selection, iii) noisy tag dictionaries, and iv) distributed word and sub-word representations.",1 Introduction,[0],[0]
"We examine how far we can get by exploiting only the wide-coverage resources that are currently readily available for more than 300 languages, which is the breadth of the parallel corpus we employ.
",1 Introduction,[0],[0]
DSDS yields a new state of the art by jointly leveraging disparate sources of distant supervision in an experiment with 25 languages.,1 Introduction,[0],[0]
"We demonstrate: i) substantial gains in carefully selecting high-quality instances in annotation projection, ii) the usefulness of lexicon features for neural tagging, and iii) the importance of word embeddings initialization for faster convergence.",1 Introduction,[0],[0]
DSDS is illustrated in Figure 1.,2 Method,[0],[0]
"The base model is a bidirectional long short-term memory network (bi-LSTM) (Graves and Schmidhuber, 2005; Hochreiter and Schmidhuber, 1997; Plank et al., 2016; Kiperwasser and Goldberg, 2016).",2 Method,[0],[0]
"Let x1:n
be a given sequence of input vectors.",2 Method,[0],[0]
"In our base model, the input sequence consists of word embeddings ~w and the two output states of a character-level bi-LSTM ~c.",2 Method,[0],[0]
"Given x1:n and a desired index i, the functionBiRNNθ(x1:n, i) (here instantiated as LSTM) reads the input sequence in forward and reverse order, respectively, and uses the concatenated (◦) output states as input for tag prediction at position i.1",2 Method,[0],[0]
"Our model differs from prior work on the type of input vectors x1:n and distant data sources, in particular, we extend the input with lexicon embeddings, all described next.
",2 Method,[0],[0]
Annotation projection.,2 Method,[0],[0]
"Ever since the seminal work of Yarowsky et al. (2001), projecting sequential labels from source to target languages has been one of the most prevalent approaches to crosslingual learning.",2 Method,[0],[0]
"Its only requirement is that parallel texts are available between the languages, and that the source side is annotated for POS.
",2 Method,[0],[0]
"We apply the approach by Agić et al. (2016), where labels are projected from multiple sources and then decoded through weighted majority voting with word alignment probabilities and source POS tagger confidences.",2 Method,[0],[0]
"We exploit their widecoverage Watchtower corpus (WTC), in contrast to the typically used Europarl data.",2 Method,[0],[0]
"Europarl covers 21 languages of the EU with 400k-2M sentence pairs, while WTC spans 300+ widely diverse languages with only 10-100k pairs, in effect sacrificing depth for breadth, and introducing a more radical domain shift.",2 Method,[0],[0]
"However, as our results show little projected data turns out to be the most beneficial, reinforcing breadth for depth.
",2 Method,[0],[0]
"While Agić et al. (2016) selected 20k projected sentences at random to train taggers, we propose a novel alternative: selection by coverage.",2 Method,[0],[0]
"We rank the target sentences by percentage of words covered by word alignment from 21 sources of Agić et al. (2016), and select the top k covered instances for training.",2 Method,[0],[0]
"In specific, we employ the mean coverage ranking of target sentences, whereby each target sentence is coupled with the arithmetic mean of the 21 individual word alignment coverages for each of the 21 source-language sentences.",2 Method,[0],[0]
"We show that this simple approach to instance selection offers substantial improvements: across all languages, we learn better taggers with significantly fewer training instances.
",2 Method,[0],[0]
"1CRF decoding did not consistently improve POS accuracy, as recently also independently found (Yang et al., 2018).
",2 Method,[0],[0]
Dictionaries.,2 Method,[0],[0]
"Dictionaries are a useful source for distant supervision (Li et al., 2012; Täckström",2 Method,[0],[0]
"et al., 2013).",2 Method,[0],[0]
"There are several ways to exploit such information: i) as type constraints during encoding (Täckström et al., 2013), ii) to guide unsupervised learning (Li et al., 2012), or iii) as additional signal at training.",2 Method,[0],[0]
"We focus on the latter and evaluate two ways to integrate lexical knowledge into neural models, while comparing to the former two: a) by representing lexicon properties as n-hot vector (e.g., if a word has two properties according to lexicon src, it results in a 2-hot vector, if the word is not present in src, a zero vector), with m the number of lexicon properties; b) by embedding the lexical features, i.e., ~esrc is a lexicon src embedded into an l-dimensional space.",2 Method,[0],[0]
"We represent ~esrc as concatenation of all embedded m properties of length l, and a zero vector otherwise.",2 Method,[0],[0]
"Tuning on the dev set, we found the second embedding approach to perform best, and simple concatenation outperformed mean vector representations.
",2 Method,[0],[0]
"We evaluate two dictionary sources, motivated by ease of accessibility to many languages: WIKTIONARY, a word type dictionary that maps tokens to one of the 12 Universal POS tags (Li et al., 2012; Petrov et al., 2012); and UNIMORPH, a morphological dictionary that provides inflectional paradigms across 350 languages (Kirov et al., 2016).",2 Method,[0],[0]
"For Wiktionary, we use the freely available dictionaries from Li et al. (2012) and Agić",2 Method,[0],[0]
et al. (2017).,2 Method,[0],[0]
"The size of the dictionaries ranges from a few thousands (e.g., Hindi and Bulgarian) to 2M (Finnish UniMorph).",2 Method,[0],[0]
"Sizes are provided in Table 1, first columns.",2 Method,[0],[0]
"UniMorph covers between 8-38 morphological properties (for English and Finnish, respectively).
",2 Method,[0],[0]
Word embeddings.,2 Method,[0],[0]
Embeddings are available for many languages.,2 Method,[0],[0]
Pre-initialization of ~w offers consistent and considerable performance improvements in our distant supervision setup (Section 4).,2 Method,[0],[0]
"We use off-the-shelf Polyglot embeddings (AlRfou et al., 2013), which performed consistently better than FastText (Bojanowski et al., 2016).",2 Method,[0],[0]
Baselines.,3 Experiments,[0],[0]
"We compare to the following weaklysupervised POS taggers: – AGIC: Multi-source annotation projection with
Bible parallel data by Agić",3 Experiments,[0],[0]
et al. (2015).,3 Experiments,[0],[0]
– DAS:,3 Experiments,[0],[0]
"The label propagation approach by Das
and Petrov (2011) over Europarl data.
– GARRETTE:",3 Experiments,[0],[0]
"The approach by Garrette and Baldridge (2013) that works with projections, dictionaries, and unlabeled target text. – LI: Wiktionary supervision (Li et al., 2012).
",3 Experiments,[0],[0]
Data.,3 Experiments,[0],[0]
Our set of 25 languages is motivated by accessibility to embeddings and dictionaries.,3 Experiments,[0],[0]
"In all experiments we work with the 12 Universal POS tags (Petrov et al., 2012).",3 Experiments,[0],[0]
"For development, we use 21 dev sets of the Universal Dependencies 2.1 (Nivre et al., 2017).",3 Experiments,[0],[0]
We employ UD test sets on additional languages as well as the test sets of Agić,3 Experiments,[0],[0]
et al. (2015) to facilitate comparisons.,3 Experiments,[0],[0]
"Their test sets are a mixture of CoNLL (Buchholz and Marsi, 2006; Nivre et al., 2007) and HamleDT test data (Zeman et al., 2014), and are more distant from the training and development data.
",3 Experiments,[0],[0]
Model and parameters.,3 Experiments,[0],[0]
We extend an off-theshelf state-of-the-art bi-LSTM tagger with lexicon information.,3 Experiments,[0],[0]
The code is available at: https:// github.com/bplank/bilstm-aux.,3 Experiments,[0],[0]
The parameter l=40 was set on dev data across all languages.,3 Experiments,[0],[0]
"Besides using 10 epochs, word dropout rate (p=.25) and 40-dimensional lexicon embeddings, we use the parameters from Plank et al. (2016).",3 Experiments,[0],[0]
"For all experiments, we average over 3 randomly seeded runs, and provide mean accuracy.",3 Experiments,[0],[0]
"For the learning curve, we average over 5 random samples with 3 runs each.",3 Experiments,[0],[0]
"Table 1 shows the tagging accuracy for individual languages, while the means over all languages are given in Figure 2.",4 Results,[0],[0]
"There are several take-aways.
",4 Results,[0],[0]
Data selection.,4 Results,[0],[0]
"The first take-away is that coverage-based instance selection yields substan-
tially better training data.",4 Results,[0],[0]
"Most prior work on annotation projection resorts to arbitrary selection; informed selection clearly helps in this noisy data setup, as shown in Figure 2 (a).",4 Results,[0],[0]
"Training on 5k instances results in a sweet spot; more data (10k) starts to decrease performance, at a cost of runtime.",4 Results,[0],[0]
Training on all WTC data (around 120k) is worse for most languages.,4 Results,[0],[0]
"From now on we consider the 5k model trained with Polyglot as our baseline (Table 1, column “5k”), obtaining a mean accuracy of 83.0 over 21 languages.
",4 Results,[0],[0]
Embeddings initialization.,4 Results,[0],[0]
"Polyglot initialization offers a large boost; on average +3.8% absolute improvement in accuracy for our 5k training scheme, as shown in Figure 2 (b).",4 Results,[0],[0]
"The big gap in low-resource setups further shows their effectiveness, with up to 10% absolute increase in accuracy when training on only 500 instances.
",4 Results,[0],[0]
Lexical information.,4 Results,[0],[0]
"The main take-away is that lexical information helps neural tagging, and embedding it proves the most helpful.",4 Results,[0],[0]
"Embedding Wiktionary tags reaches 83.7 accuracy on average, versus 83.4 for n-hot encoding, and 83.2 for type constraints.",4 Results,[0],[0]
Only on 4 out of 21 languages are type constraints better.,4 Results,[0],[0]
This is the case for only one language for n-hot encoding (French).,4 Results,[0],[0]
"The best approach is to embed both Wiktionary and Unimorph, boosting performance further to 84.0, and resulting in our final model.",4 Results,[0],[0]
"It helps the most on morphological rich languages such as Uralic.
",4 Results,[0],[0]
"On the test sets (Table 4, right) DSDS reaches 87.2 over 8 test languages intersecting Li et al. (2012) and Agić",4 Results,[0],[0]
et al. (2016).,4 Results,[0],[0]
"It reaches 86.2 over the more commonly used 8 languages of Das and Petrov (2011), compared to their 83.4.",4 Results,[0],[0]
"This shows that our novel “soft” inclusion of noisy dictionaries is superior to a hard decoding restriction, and including lexicons in neural taggers helps.",4 Results,[0],[0]
"We did not assume any gold data to further enrich the lexicons, nor fix possible tagset divergences.",4 Results,[0],[0]
Analysis.,5 Discussion,[0],[0]
The inclusion of lexicons results in higher coverage and is part of the explanation for the improvement of DSDS; see correlation in Figure 3 (a).,5 Discussion,[0],[0]
"What is more interesting is that our model benefits from the lexicon beyond its content: OOV accuracy for words not present in the lexicon overall improves, besides the expected improvement on known OOV, see Figure 3 (b).
More languages.",5 Discussion,[0],[0]
All data sources employed in our experiment are very high-coverage.,5 Discussion,[0],[0]
"However, for true low-resource languages, we cannot safely assume the availability of all disparate information sources.",5 Discussion,[0],[0]
Table 2 presents results for four additional languages where some supervision sources are missing.,5 Discussion,[0],[0]
"We observe that adding lexicon information always helps, even in cases where only 1k entries are available, and embedding it is usually the most beneficial way.",5 Discussion,[0],[0]
"For closely-related languages such as Serbian and Croatian, using resources for one aids tagging the other, and modern resources are a better fit.",5 Discussion,[0],[0]
"For example, using the Croatian WTC projections to train a model for Serbian is preferable over in-language Serbian Bible data where the OOV rate is much higher.
",5 Discussion,[0],[0]
How much gold data?,5 Discussion,[0],[0]
We assume not having access to any gold annotated data.,5 Discussion,[0],[0]
It is thus interesting to ask how much gold data is needed to reach our performance.,5 Discussion,[0],[0]
"This is a tricky question, as training within the same corpus naturally favors the same corpus data.",5 Discussion,[0],[0]
"We test both in-corpus (UD)
and out-of-corpus data (our test sets) and notice an important gap: while in-corpus only 50 sentences are sufficient, outside the corpus one would need over 200 sentences.",5 Discussion,[0],[0]
"This experiment was done for a subset of 18 languages with both in- and out-ofcorpus test data.
",5 Discussion,[0],[0]
Further comparison.,5 Discussion,[0],[0]
"In Table 1 we directly report the accuracies from the original contributions by DAS, LI, GARRETTE, and AGIC over the same test data.",5 Discussion,[0],[0]
We additionally attempted to reach the scores of LI by running their tagger over the Table 1 data setup.,5 Discussion,[0],[0]
The results are depicted in Figure 4 as mean accuracies over EM iterations until convergence.,5 Discussion,[0],[0]
"We show: i) LI peaks at 10 iterations for their test languages, and at 35 iterations for all the rest.",5 Discussion,[0],[0]
"This is in slight contrast to 50 iterations that Li et al. (2012) recommend, although selecting 50 does not dramatically hurt the scores; ii)",5 Discussion,[0],[0]
Our replication falls ∼5 points short of their 84.9 accuracy.,5 Discussion,[0],[0]
"There is a large 33-point accuracy gap between the scores of Li et al. (2012), where the dictionaries are large, and the other languages in Figure 4, with smaller dictionaries.
",5 Discussion,[0],[0]
"Compared to DAS, our tagger clearly benefits from pre-trained word embeddings, while theirs relies on label propagation through Europarl, a much cleaner corpus that lacks the coverage of the noisier WTC.",5 Discussion,[0],[0]
"Similar applies to Täckström et al. (2013), as they use 1-5M near-perfect parallel sentences.",5 Discussion,[0],[0]
"Even if we use much smaller and noisier data sources, DSDS is almost on par: 86.2 vs. 87.3 for the 8 languages from Das and Petrov (2011), and we even outperform theirs on four languages: Czech, French, Italian, and Spanish.",5 Discussion,[0],[0]
"Most successful work on low-resource POS tagging is based on projection (Yarowsky et al., 2001), tag dictionaries (Li et al., 2012), annotation of seed training data (Garrette and Baldridge, 2013) or even more recently some combination of these, e.g., via multi-task learning (Fang and
Cohn, 2016; Kann et al., 2018).",6 Related Work,[0],[0]
"Our paper contributes to this literature by leveraging a range of prior directions in a unified, neural test bed.
",6 Related Work,[0],[0]
Most prior work on neural sequence prediction follows the commonly perceived wisdom that hand-crafted features are unnecessary for deep learning methods.,6 Related Work,[0],[0]
They rely on end-to-end training without resorting to additional linguistic resources.,6 Related Work,[0],[0]
Our study shows that this is not the case.,6 Related Work,[0],[0]
"Only few prior studies investigate such sources, e.g., for MT (Sennrich and Haddow, 2016; Chen et al., 2017; Li et al., 2017; Passban et al., 2018) and Sagot and Martı́nez Alonso (2017) for POS tagging use lexicons, but only as n-hot features and without examining the cross-lingual aspect.",6 Related Work,[0],[0]
We show that our approach of distant supervision from disparate sources (DSDS) is simple yet surprisingly effective for low-resource POS tagging.,7 Conclusions,[0],[0]
"Only 5k instances of projected data paired with off-the-shelf embeddings and lexical information integrated into a neural tagger are sufficient to reach a new state of the art, and both data selection and embeddings are essential components to boost neural tagging performance.",7 Conclusions,[0],[0]
"We introduce DSDS: a cross-lingual neural part-of-speech tagger that learns from disparate sources of distant supervision, and realistically scales to hundreds of low-resource languages.",abstractText,[0],[0]
"The model exploits annotation projection, instance selection, tag dictionaries, morphological lexicons, and distributed representations, all in a uniform framework.",abstractText,[0],[0]
"The approach is simple, yet surprisingly effective, resulting in a new state of the art without access to any gold annotated data.",abstractText,[0],[0]
Distant Supervision from Disparate Sources for Low-Resource Part-of-Speech Tagging,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1393–1402 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1393",text,[0],[0]
"Search-based structured prediction models the generation of natural language structure (part-ofspeech tags, syntax tree, translations, semantic graphs, etc.)",1 Introduction,[0],[0]
"as a search problem (Collins and Roark, 2004; Liang et al., 2006; Zhang and Clark, 2008; Huang et al., 2012; Sutskever et al., 2014; Goodman et al., 2016).",1 Introduction,[0],[0]
It has drawn a lot of research attention in recent years thanks to its competitive performance on both accuracy and running time.,1 Introduction,[0],[0]
A stochastic policy that controls the whole search process is usually learned by imitating a reference policy.,1 Introduction,[0],[0]
"The imitation is usually addressed as training a classifier to predict the ref-
∗*",1 Introduction,[0],[0]
"Email corresponding.
erence policy’s search action on the encountered states when performing the reference policy.",1 Introduction,[0],[0]
Such imitation process can sometimes be problematic.,1 Introduction,[0],[0]
"One problem is the ambiguities of the reference policy, in which multiple actions lead to the optimal structure but usually, only one is chosen as training instance (Goldberg and Nivre, 2012).",1 Introduction,[0],[0]
"Another problem is the discrepancy between training and testing, in which during the test phase, the learned policy enters non-optimal states whose search action is never learned (Ross and Bagnell, 2010; Ross et al., 2011).",1 Introduction,[0],[0]
"All these problems harm the generalization ability of search-based structured prediction and lead to poor performance.
",1 Introduction,[0],[0]
Previous works tackle these problems from two directions.,1 Introduction,[0],[0]
"To overcome the ambiguities in data, techniques like ensemble are often adopted (Di-
etterich, 2000).",1 Introduction,[0],[0]
"To mitigate the discrepancy, exploration is encouraged during the training process (Ross and Bagnell, 2010; Ross et al., 2011; Goldberg and Nivre, 2012; Bengio et al., 2015; Goodman et al., 2016).",1 Introduction,[0],[0]
"In this paper, we propose to consider these two problems in an integrated knowledge distillation manner (Hinton et al., 2015).",1 Introduction,[0],[0]
We distill a single model from the ensemble of several baselines trained with different initialization by matching the ensemble’s output distribution on the reference states.,1 Introduction,[0],[0]
We also let the ensemble randomly explore the search space and learn the single model to mimic ensemble’s distribution on the encountered exploration states.,1 Introduction,[0],[0]
Combing the distillation from reference and exploration further improves our single model’s performance.,1 Introduction,[0],[0]
"The workflow of our method is shown in Figure 1.
",1 Introduction,[0],[0]
We conduct experiments on two typical searchbased structured prediction tasks: transition-based dependency parsing and neural machine translation.,1 Introduction,[0],[0]
The results of both these two experiments show the effectiveness of our knowledge distillation method by outperforming strong baselines.,1 Introduction,[0],[0]
"In the parsing experiments, an improvement of 1.32 in LAS is achieved and in the machine translation experiments, such improvement is 2.65 in BLEU.",1 Introduction,[0],[0]
"Our model also outperforms the greedy models in previous works.
",1 Introduction,[0],[0]
"Major contributions of this paper include:
• We study the knowledge distillation in search-based structured prediction and propose to distill the knowledge of an ensemble into a single model by learning to match its distribution on both the reference states (§3.2) and exploration states encountered when using the ensemble to explore the search space (§3.3).",1 Introduction,[0],[0]
"A further combination of these two methods is also proposed to improve the performance (§3.4).
",1 Introduction,[0],[0]
• We conduct experiments on two search-based structured prediction problems: transitionbased dependency parsing and neural machine translation.,1 Introduction,[0],[0]
"In both these two problems, the distilled model significantly improves over strong baselines and outperforms other greedy structured prediction (§4.2).",1 Introduction,[0],[0]
Comprehensive analysis empirically shows the feasibility of our distillation method (§4.3).,1 Introduction,[0],[0]
Structured prediction maps an input x =,2.1 Search-based Structured Prediction,[0],[0]
"(x1, x2, ..., xn) to its structural output y = (y1, y2, ..., ym), where each component of y has some internal dependencies.",2.1 Search-based Structured Prediction,[0],[0]
"Search-based structured prediction (Collins and Roark, 2004; Daumé III et al., 2005; Daumé III et al., 2009; Ross and Bagnell, 2010; Ross et al., 2011; Doppa et al., 2014; Vlachos and Clark, 2014; Chang et al., 2015) models the generation of the structure as a search problem and it can be formalized as a tuple (S,A, T (s, a),S0,ST ), in which S is a set of states, A is a set of actions, T is a function that maps S × A → S, S0 is a set of initial states, and ST is a set of terminal states.",2.1 Search-based Structured Prediction,[0],[0]
"Starting from an initial state s0 ∈ S0, the structured prediction model repeatably chooses an action at ∈ A by following a policy π(s) and applies at to st and enter a new state st+1 as st+1 ← T (st, at), until a final state sT ∈ ST is achieved.",2.1 Search-based Structured Prediction,[0],[0]
"Several natural language structured prediction problems can be modeled under the search-based framework including dependency parsing (Nivre, 2008) and neural machine translation (Liang et al., 2006; Sutskever et al., 2014).",2.1 Search-based Structured Prediction,[0],[0]
"Table 1 shows the search-based structured prediction view of these two problems.
",2.1 Search-based Structured Prediction,[0],[0]
"In the data-driven settings, π(s) controls the whole search process and is usually parameterized by a classifier p(a | s) which outputs the proba-
Algorithm 1: Generic learning algorithm for search-based structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y).
",2.1 Search-based Structured Prediction,[0],[0]
Output: classifier p(a|s).,2.1 Search-based Structured Prediction,[0],[0]
"1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n)); 5 while st /∈ ST do 6 at ← πR(st,y(n)); 7 D ← D ∪ {st}; 8 st+1 ← T (st, at); 9 t← t+ 1;
10 end 11 end 12 optimize LNLL;
bility of choosing an action a on the given state s.",2.1 Search-based Structured Prediction,[0],[0]
The commonly adopted greedy policy can be formalized as choosing the most probable action with π(s) = argmaxa,2.1 Search-based Structured Prediction,[0],[0]
p(a | s) at test stage.,2.1 Search-based Structured Prediction,[0],[0]
"To learn an optimal classifier, search-based structured prediction requires constructing a reference policy πR(s,y), which takes an input state s, gold structure y and outputs its reference action a, and training p(a | s) to imitate the reference policy.",2.1 Search-based Structured Prediction,[0],[0]
"Algorithm 1 shows the common practices in training p(a | s), which involves: first, using πR(s,y) to generate a sequence of reference states and actions on the training data (line 1 to line 11 in Algorithm 1); second, using the states and actions on the reference sequences as examples to train p(a | s) with negative log-likelihood (NLL) loss (line 12 in Algorithm 1),
LNLL = ∑ s∈D ∑ a −1{a = πR} · log p(a | s)
where D is a set of training data.",2.1 Search-based Structured Prediction,[0],[0]
"The reference policy is sometimes sub-optimal and ambiguous which means on one state, there can be more than one action that leads to the optimal prediction.",2.1 Search-based Structured Prediction,[0],[0]
"In transition-based dependency parsing, Goldberg and Nivre (2012) showed that one dependency tree can be reached by several search sequences using Nivre (2008)’s arcstandard algorithm.",2.1 Search-based Structured Prediction,[0],[0]
"In machine translation, the ambiguity problem also exists because one source language sentence usually has multiple semantically correct translations but only one reference
translation is presented.",2.1 Search-based Structured Prediction,[0],[0]
"Similar problems have also been observed in semantic parsing (Goodman et al., 2016).",2.1 Search-based Structured Prediction,[0],[0]
"According to Frénay and Verleysen (2014), the widely used NLL loss is vulnerable to ambiguous data which make it worse for searchbased structured prediction.
",2.1 Search-based Structured Prediction,[0],[0]
"Besides the ambiguity problem, training and testing discrepancy is another problem that lags the search-based structured prediction performance.",2.1 Search-based Structured Prediction,[0],[0]
"Since the training process imitates the reference policy, all the states in the training data are optimal which means they are guaranteed to reach the optimal structure.",2.1 Search-based Structured Prediction,[0],[0]
"But during the test phase, the model can predict non-optimal states whose search action is never learned.",2.1 Search-based Structured Prediction,[0],[0]
The greedy search which is prone to error propagation also worsens this problem.,2.1 Search-based Structured Prediction,[0],[0]
"A cumbersome model, which could be an ensemble of several models or a single model with larger number of parameters, usually provides better generalization ability.",2.2 Knowledge Distillation,[0],[0]
"Knowledge distillation (Buciluǎ et al., 2006; Ba and Caruana, 2014; Hinton et al., 2015) is a class of methods for transferring the generalization ability of the cumbersome teacher model into a small student model.",2.2 Knowledge Distillation,[0],[0]
"Instead of optimizing NLL loss, knowledge distillation uses the distribution q(y | x) outputted by the teacher model as “soft target” and optimizes the knowledge distillation loss,
LKD = ∑ x∈D ∑ y −q(y",2.2 Knowledge Distillation,[0],[0]
"| x) · log p(y | x).
",2.2 Knowledge Distillation,[0],[0]
"In search-based structured prediction scenario, x corresponds to the state s and y corresponds to the action a. Through optimizing the distillation loss, knowledge of the teacher model is learned by the student model p(y | x).",2.2 Knowledge Distillation,[0],[0]
"When correct label is presented, NLL loss can be combined with the distillation loss via simple interpolation as
L = αLKD + (1− α)LNLL (1)",2.2 Knowledge Distillation,[0],[0]
"As Hinton et al. (2015) pointed out, although the real objective of a machine learning algorithm is to generalize well to new data, models are usually trained to optimize the performance on training data, which bias the model to the training data.
",3.1 Ensemble,[0],[0]
"In search-based structured prediction, such biases can result from either the ambiguities in the training data or the discrepancy between training and testing.",3.1 Ensemble,[0],[0]
"It would be more problematic to train p(a | s) using the loss which is in-robust to ambiguities and only considering the optimal states.
",3.1 Ensemble,[0],[0]
The effect of ensemble on ambiguous data has been studied in Dietterich (2000).,3.1 Ensemble,[0],[0]
They empirically showed that ensemble can overcome the ambiguities in the training data.,3.1 Ensemble,[0],[0]
Daumé III et al. (2005) also use weighted ensemble of parameters from different iterations as their final structure prediction model.,3.1 Ensemble,[0],[0]
"In this paper, we consider to use ensemble technique to improve the generalization ability of our search-based structured prediction model following these works.",3.1 Ensemble,[0],[0]
"In practice, we train M search-based structured prediction models with different initialized weights and ensemble them by the average of their output distribution as q(a | s) = 1M ∑ m qm(a | s).",3.1 Ensemble,[0],[0]
"In Section 4.3.1, we empirically show that the ensemble has the ability to choose a good search action in the optimal-yetambiguous states and the non-optimal states.",3.1 Ensemble,[0],[0]
"As we can see in Section 4, ensemble indeed improves the performance of baseline models.",3.2 Distillation from Reference,[0],[0]
"However, real world deployment is usually constrained by computation and memory resources.",3.2 Distillation from Reference,[0],[0]
"Ensemble requires running the structured prediction models for multiple times, and that makes it less applicable in real-world problem.",3.2 Distillation from Reference,[0],[0]
"To take the advantage of the ensemble model while avoid running the models multiple times, we use the knowledge distillation technique to distill a single model from the ensemble.",3.2 Distillation from Reference,[0],[0]
We started from changing the NLL learning objective in Algorithm 1 into the distillation loss (Equation 1) as shown in Algorithm 2.,3.2 Distillation from Reference,[0],[0]
"Since such method learns the model on the states produced by the reference policy, we name it as distillation from reference.",3.2 Distillation from Reference,[0],[0]
Blocks connected by in dashed red lines in Figure 1 show the workflow of our distillation from reference.,3.2 Distillation from Reference,[0],[0]
"In the scenario of search-based structured prediction, transferring the teacher model’s generalization ability into a student model not only includes matching the teacher model’s soft targets on the reference search sequence, but also imitating the search decisions made by the teacher model.",3.3 Distillation from Exploration,[0],[0]
"One way to accomplish the imitation can be sampling
Algorithm 2: Knowledge distillation for search-based structured prediction.
",3.3 Distillation from Exploration,[0],[0]
"Input: training data: {x(n),y(n)}Nn=1; the reference policy: πR(s,y); the exploration policy: πE(s) which samples an action from the annealed ensemble q(a | s) 1 T
Output: classifier p(a | s).",3.3 Distillation from Exploration,[0],[0]
1 D ← ∅; 2 for n← 1...N do 3 t← 0; 4 st ← s0(x(n));,3.3 Distillation from Exploration,[0],[0]
"5 while st /∈ ST do 6 if distilling from reference then 7 at ← πR(st,y(n)); 8 else 9 at ← πE(st);
10 end 11 D ← D ∪ {st}; 12 st+1 ← T (st, at); 13 t← t+ 1; 14 end 15 end 16 if distilling from reference then 17 optimize αLKD +",3.3 Distillation from Exploration,[0],[0]
"(1− α)LNLL; 18 else 19 optimize LKD; 20 end
search sequence from the ensemble and learn from the soft target on the sampled states.",3.3 Distillation from Exploration,[0],[0]
"More concretely, we change πR(s,y) into a policy πE(s) which samples an action a from q(a | s) 1 T , where T is the temperature that controls the sharpness of the distribution (Hinton et al., 2015).",3.3 Distillation from Exploration,[0],[0]
The algorithm is shown in Algorithm 2.,3.3 Distillation from Exploration,[0],[0]
"Since such distillation generate training instances from exploration, we name it as distillation from exploration.",3.3 Distillation from Exploration,[0],[0]
"Blocks connected by in solid blue lines in Figure 1 show the workflow of our distillation from exploration.
",3.3 Distillation from Exploration,[0],[0]
"On the sampled states, reference decision from πR is usually non-trivial to achieve, which makes learning from NLL loss infeasible.",3.3 Distillation from Exploration,[0],[0]
"In Section 4, we empirically show that fully distilling from the soft target, i.e. setting α = 1 in Equation 1, achieves comparable performance with that both from distillation and NLL.",3.3 Distillation from Exploration,[0],[0]
Distillation from reference can encourage the model to predict the action made by the reference policy and distillation from exploration learns the model on arbitrary states.,3.4 Distillation from Both,[0],[0]
They transfer the generalization ability of the ensemble from different aspects.,3.4 Distillation from Both,[0],[0]
Hopefully combining them can further improve the performance.,3.4 Distillation from Both,[0],[0]
"In this paper, we combine distillation from reference and exploration with the following manner: we use πR and πE to generate a set of training states.",3.4 Distillation from Both,[0],[0]
"Then, we learn p(a | s) on the generated states.",3.4 Distillation from Both,[0],[0]
"If one state was generated by the reference policy, we minimize the interpretation of distillation and NLL loss.",3.4 Distillation from Both,[0],[0]
"Otherwise, we minimize the distillation loss only.",3.4 Distillation from Both,[0],[0]
We perform experiments on two tasks: transitionbased dependency parsing and neural machine translation.,4 Experiments,[0],[0]
"Both these two tasks are converted to search-based structured prediction as Section 2.1.
",4 Experiments,[0],[0]
"For the transition-based parsing, we use the stack-lstm parsing model proposed by Dyer et al. (2015) to parameterize the classifier.1 For the neural machine translation, we parameterize the classifier as an LSTM encoder-decoder model by following Luong et al. (2015).2 We encourage the reader of this paper to refer corresponding papers for more details.",4 Experiments,[0],[0]
"We perform experiments on Penn Treebank (PTB) dataset with standard data split (Section 2-21 for training, Section 22 for development, and Section 23 for testing).",4.1.1 Transition-based Dependency Parsing,[0],[0]
Stanford dependencies are converted from the original constituent trees using Stanford CoreNLP 3.3.03 by following Dyer et al. (2015).,4.1.1 Transition-based Dependency Parsing,[0],[0]
Automatic part-of-speech tags are assigned by 10-way jackknifing whose accuracy is 97.5%.,4.1.1 Transition-based Dependency Parsing,[0],[0]
Labeled attachment score (LAS) excluding punctuation are used in evaluation.,4.1.1 Transition-based Dependency Parsing,[0],[0]
"For the other hyper-parameters, we use the same settings as Dyer et al. (2015).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The best iteration and α is determined on the development set.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"1The code for parsing experiments is available at: https://github.com/Oneplus/twpipe.
",4.1.1 Transition-based Dependency Parsing,[0],[0]
"2We based our NMT experiments on OpenNMT (Klein et al., 2017).",4.1.1 Transition-based Dependency Parsing,[0],[0]
"The code for NMT experiments is available at: https://github.com/Oneplus/OpenNMT-py.
3stanfordnlp.github.io/CoreNLP/ history.html
Reimers and Gurevych (2017) and others have pointed out that neural network training is nondeterministic and depends on the seed for the random number generator.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"To control for this effect, they suggest to report the average of M differentlyseeded runs.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"In all our dependency parsing, we set n = 20.",4.1.1 Transition-based Dependency Parsing,[0],[0]
"We conduct our experiments on a small machine translation dataset, which is the Germanto-English portion of the IWSLT 2014 machine translation evaluation campaign.",4.1.2 Neural Machine Translation,[0],[0]
"The dataset contains around 153K training sentence pairs, 7K development sentence pairs, and 7K testing sentence pairs.",4.1.2 Neural Machine Translation,[0],[0]
"We use the same preprocessing as Ranzato et al. (2015), which leads to a German vocabulary of about 30K entries and an English vocabulary of 25K entries.",4.1.2 Neural Machine Translation,[0],[0]
One-layer LSTM for both encoder and decoder with 256 hidden units are used by following Wiseman and Rush (2016).,4.1.2 Neural Machine Translation,[0],[0]
"BLEU (Papineni et al., 2002) was used to evaluate the translator’s performance.4 Like in the dependency parsing experiments, we run M = 10 differentlyseeded runs and report the averaged score.
",4.1.2 Neural Machine Translation,[0],[0]
Optimizing the distillation loss in Equation 1 requires enumerating over the action space.,4.1.2 Neural Machine Translation,[0],[0]
It is expensive for machine translation since the size of the action space (vocabulary) is considerably large (25K in our experiments).,4.1.2 Neural Machine Translation,[0],[0]
"In this paper, we use the K-most probable actions (translations on target side) on one state to approximate the whole probability distribution of q(a | s) as ∑ a q(a |
s) · log p(a | s)",4.1.2 Neural Machine Translation,[0],[0]
"≈ ∑K
k q(âk",4.1.2 Neural Machine Translation,[0],[0]
"| s) · log p(âk | s), where âk is the k-th probable action.",4.1.2 Neural Machine Translation,[0],[0]
"We fix α to
4We use multi-bleu.perl to evaluate our model’s performance
1 and vary K and evaluate the distillation model’s performance.",4.1.2 Neural Machine Translation,[0],[0]
"These results are shown in Figure 2 where there is no significant difference between different Ks and in speed consideration, we set K to 1 in the following experiments.",4.1.2 Neural Machine Translation,[0],[0]
Table 2 shows our PTB experimental results.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"From this result, we can see that the ensemble model outperforms the baseline model by 1.90 in LAS.",4.2.1 Transition-based Dependency Parsing,[0],[0]
"For our distillation from reference, when setting α = 1.0, best performance on development set is achieved and the test LAS is 91.99.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We tune the temperature T during exploration and the results are shown in Figure 3.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Sharpen the distribution during the sampling process generally performs better on development set.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"Our distillation from exploration model gets almost the same performance as that from reference, but simply combing these two sets of data outperform both models by achieving an LAS of 92.14.
",4.2.1 Transition-based Dependency Parsing,[0],[0]
We also compare our parser with the other parsers in Table 2.,4.2.1 Transition-based Dependency Parsing,[0],[0]
The second group shows the greedy transition-based parsers in previous literatures.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Andor et al. (2016) presented an alternative state representation and explored both greedy and beam search decoding.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"(Ballesteros et al., 2016) explores training the greedy parser with dynamic oracle.",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser outperforms all these greedy counterparts.,4.2.1 Transition-based Dependency Parsing,[0],[0]
"The third group shows
parsers trained on different techniques including decoding with beam search (Buckman et al., 2016; Andor et al., 2016), training transitionbased parser with beam search (Andor et al., 2016), graph-based parsing (Dozat and Manning, 2016), distilling a graph-based parser from the output of 20 parsers (Kuncoro et al., 2016), and converting constituent parsing results to dependencies (Kuncoro et al., 2017).",4.2.1 Transition-based Dependency Parsing,[0],[0]
Our distillation parser still outperforms its transition-based counterparts but lags the others.,4.2.1 Transition-based Dependency Parsing,[0],[0]
We attribute the gap between our parser with the other parsers to the difference in parsing algorithms.,4.2.1 Transition-based Dependency Parsing,[0],[0]
Table 3 shows the experimental results on IWSLT 2014 dataset.,4.2.2 Neural Machine Translation,[0],[0]
"Similar to the PTB parsing results, the ensemble 10 translators outperforms the baseline translator by 3.47 in BLEU score.",4.2.2 Neural Machine Translation,[0],[0]
"Distilling from the ensemble by following the reference leads to a single translator of 24.76 BLEU score.
",4.2.2 Neural Machine Translation,[0],[0]
"Like in the parsing experiments, sharpen the distribution when exploring the search space is more helpful to the model’s performance but the differences when T ≤ 0.2 is not significant as shown in Figure 3.",4.2.2 Neural Machine Translation,[0],[0]
We set T = 0.1 in our distillation from exploration experiments since it achieves the best development score.,4.2.2 Neural Machine Translation,[0],[0]
Table 3 shows the exploration result of a BLEU score of 24.64 and it slightly lags the best reference model.,4.2.2 Neural Machine Translation,[0],[0]
"Distilling from both the reference and exploration improves the single model’s performance by a large margin and achieves a BLEU score of 25.44.
",4.2.2 Neural Machine Translation,[0],[0]
"We also compare our model with other translation models including the one trained with reinforcement learning (Ranzato et al., 2015) and that using beam search in training (Wiseman and Rush, 2016).",4.2.2 Neural Machine Translation,[0],[0]
"Our distillation translator outperforms these models.
",4.2.2 Neural Machine Translation,[0],[0]
Both the parsing and machine translation experiments confirm that it’s feasible to distill a reasonable search-based structured prediction model by just exploring the search space.,4.2.2 Neural Machine Translation,[0],[0]
Combining the reference and exploration further improves the model’s performance and outperforms its greedy structured prediction counterparts.,4.2.2 Neural Machine Translation,[0],[0]
"In Section 4.2, improvements from distilling the ensemble have been witnessed in both the transition-based dependency parsing and neural machine translation experiments.",4.3 Analysis,[0],[0]
"However, questions like “Why the ensemble works better? Is it feasible to fully learn from the distillation loss without NLL?",4.3 Analysis,[0],[0]
Is learning from distillation loss stable?” are yet to be answered.,4.3 Analysis,[0],[0]
"In this section, we first study the ensemble’s behavior on “problematic” states to show its generalization ability.",4.3 Analysis,[0],[0]
"Then, we empirically study the feasibility of fully learning from the distillation loss by studying the effect of α in the distillation from reference setting.",4.3 Analysis,[0],[0]
"Finally, we show that learning from distillation loss is less sensitive to initialization and achieves a more stable model.",4.3 Analysis,[0],[0]
"As mentioned in previous sections, “problematic” states which is either ambiguous or non-optimal harm structured prediciton’s performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Ensemble shows to improve the performance in Section 4.2, which indicates it does better on these states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"To empirically testify this, we use dependency parsing as a testbed and study the ensemble’s output distribution using the dynamic oracle.
",4.3.1 Ensemble on “Problematic” States,[0],[0]
"The dynamic oracle (Goldberg and Nivre, 2012; Goldberg et al., 2014) can be used to efficiently determine, given any state s, which transition action leads to the best achievable parse from s; if some errors may have already made, what is the best the parser can do, going forward?",4.3.1 Ensemble on “Problematic” States,[0],[0]
"This allows us to analyze the accuracy of each parser’s individual decisions, in the “problematic” states.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"In this paper, we evaluate the output distributions of the baseline and ensemble parser against the reference actions suggested by the dynamic oracle.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Since dynamic oracle yields more than one reference actions due to ambiguities and previous mistakes and the output distribution can be treated as their scoring, we evaluate them as a ranking problem.",4.3.1 Ensemble on “Problematic” States,[0],[0]
"Intuitively, when multiple reference actions exist, a good parser should push probability mass to these actions.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We draw problematic states by sampling from our baseline parser.,4.3.1 Ensemble on “Problematic” States,[0],[0]
The comparison in Table 4 shows that the ensemble model significantly outperforms the baseline on ambiguous and non-optimal states.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"This observation indicates the ensemble’s output distribution is more “informative”, thus generalizes well on problematic states and achieves better performance.",4.3.1 Ensemble on “Problematic” States,[0],[0]
We also observe that the distillation model perform better than both the baseline and ensemble.,4.3.1 Ensemble on “Problematic” States,[0],[0]
We attribute this to the fact that the distillation model is learned from exploration.,4.3.1 Ensemble on “Problematic” States,[0],[0]
"Over our distillation from reference model, we study the effect of α in Equation 1.",4.3.2 Effect of α,[0],[0]
We vary α from 0 to 1 by a step of 0.1 in both the transitionbased dependency parsing and neural machine translation experiments and plot the model’s performance on development sets in Figure 4.,4.3.2 Effect of α,[0],[0]
Similar trends are witnessed in both these two experiments that model that’s configured with larger α generally performs better than that with smaller α.,4.3.2 Effect of α,[0],[0]
"For the dependency parsing problem, the best development performance is achieved when we set α = 1, and for the machine translation, the best α is 0.8.",4.3.2 Effect of α,[0],[0]
There is only 0.2 point of difference between the best α model and the one with α equals to 1.,4.3.2 Effect of α,[0],[0]
Such observation indicates that when distilling from the reference policy paying more attention to the distillation loss rather than the NLL is more beneficial.,4.3.2 Effect of α,[0],[0]
It also indicates that fully learning from the distillation loss outputted by the ensemble is reasonable because models configured with α = 1 generally achieves good performance.,4.3.2 Effect of α,[0],[0]
"Besides the improved performance, knowledge distillation also leads to more stable learning.",4.3.3 Learning Stability,[0],[0]
The performance score distributions of differentlyseed runs are depicted as violin plot in Figure 5.,4.3.3 Learning Stability,[0],[0]
Table 5 also reveals the smaller standard derivations are achieved by our distillation methods.,4.3.3 Learning Stability,[0],[0]
"As Keskar et al. (2016) pointed out that the general-
ization gap is not due to overfit, but due to the network converge to sharp minimizer which generalizes worse, we attribute the more stable training from our distillation model as the distillation loss presents less sharp minimizers.",4.3.3 Learning Stability,[0],[0]
Several works have been proposed to applying knowledge distillation to NLP problems.,5 Related Work,[0],[0]
Kim and Rush (2016) presented a distillation model which focus on distilling the structured loss from a large model into a small one which works on sequencelevel.,5 Related Work,[0],[0]
"In contrast to their work, we pay more attention to action-level distillation and propose to do better action-level distillation by both from reference and exploration.
",5 Related Work,[0],[0]
Freitag et al. (2017) used an ensemble of 6- translators to generate training reference.,5 Related Work,[0],[0]
Exploration was tried in their work with beam-search.,5 Related Work,[0],[0]
"We differ their work by training the single model
to match the distribution of the ensemble.",5 Related Work,[0],[0]
"Using ensemble in exploration was also studied in reinforcement learning community (Osband et al., 2016).",5 Related Work,[0],[0]
"In addition to distilling the ensemble on the labeled training data, a line of semisupervised learning works show that it’s effective to transfer knowledge of cumbersome model into a simple one on the unlabeled data (Liang et al., 2008; Li et al., 2014).",5 Related Work,[0],[0]
"Their extensions to knowledge distillation call for further study.
",5 Related Work,[0],[0]
Kuncoro et al. (2016) proposed to compile the knowledge from an ensemble of 20 transitionbased parsers into a voting and distill the knowledge by introducing the voting results as a regularizer in learning a graph-based parser.,5 Related Work,[0],[0]
"Different from their work, we directly do the distillation on the classifier of the transition-based parser.
",5 Related Work,[0],[0]
"Besides the attempts for directly using the knowledge distillation technique, Stahlberg and Byrne (2017) propose to first build the ensemble of several machine translators into one network by unfolding and then use SVD to shrink its parameters, which can be treated as another kind of knowledge distillation.",5 Related Work,[0],[0]
"In this paper, we study knowledge distillation for search-based structured prediction and propose to distill an ensemble into a single model both from reference and exploration states.",6 Conclusion,[0],[0]
Experiments on transition-based dependency parsing and machine translation show that our distillation method significantly improves the single model’s performance.,6 Conclusion,[0],[0]
Comparison analysis gives empirically guarantee for our distillation method.,6 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments and suggestions.,Acknowledgments,[0],[0]
This work was supported by the National Key Basic Research Program of China via grant 2014CB340503 and the National Natural Science Foundation of China (NSFC) via grant 61632011 and 61772153.,Acknowledgments,[0],[0]
Many natural language processing tasks can be modeled into structured prediction and solved as a search problem.,abstractText,[0],[0]
"In this paper, we distill an ensemble of multiple models trained with different initialization into a single model.",abstractText,[0],[0]
"In addition to learning to match the ensemble’s probability output on the reference states, we also use the ensemble to explore the search space and learn from the encountered states in the exploration.",abstractText,[0],[0]
Experimental results on two typical search-based structured prediction tasks – transition-based dependency parsing and neural machine translation show that distillation can effectively improve the single model’s performance and the final model achieves improvements of 1.32 in LAS and 2.65 in BLEU score on these two tasks respectively over strong baselines and it outperforms the greedy structured prediction models in previous literatures.,abstractText,[0],[0]
Distilling Knowledge for Search-based Structured Prediction,title,[0],[0]
"Over the last several years, the world has witnessed the emergence of data sets of an unprecedented scale across different scientific disciplines.",1. Introduction,[0],[0]
"This development has created a need for scalable, distributed machine learning algorithms to deal with the increasing amount of data.",1. Introduction,[0],[0]
"In this paper, we consider large-scale clustering or, more specifically, the task of finding provably good seedings for kMeans in a massive data setting.
",1. Introduction,[0],[0]
Seeding — the task of finding initial cluster centers — is critical to finding good clusterings for k-Means.,1. Introduction,[0],[0]
"In fact, the seeding step of the state of the art algorithm k-means++ (Arthur & Vassilvitskii, 2007) provides the
1Department of Computer Science, ETH Zurich.",1. Introduction,[0],[0]
"Correspondence to: Olivier Bachem <olivier.bachem@inf.ethz.ch>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"theoretical guarantee on the solution quality while the subsequent refinement using Lloyd’s algorithm (Lloyd, 1982) only guarantees that the quality does not deteriorate.",1. Introduction,[0],[0]
"While the k-means++ seeding step guarantees a solution that is O(log k) competitive with the optimal solution in expectation, it also requires k inherently sequential passes through the data set.",1. Introduction,[0],[0]
"This makes it unsuitable for the massive data setting where the data set is distributed across machines and computation has to occur in parallel.
",1. Introduction,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the k-means‖ algorithm which produces seedings for kMeans with a reduced number of sequential iterations.",1. Introduction,[0],[0]
"Whereas k-means++ only samples a single cluster center in each of k rounds, k-means‖ samples in expectation ` points in each of t iterations.",1. Introduction,[0],[0]
"Provided t is small enough, this makes k-means‖ suitable for a distributed setting as the number of synchronizations is reduced.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"We provide a novel analysis of k-means‖ that bounds the expected solution quality for any number of rounds t and any oversampling factor ` ≥ k, the two parameters that need to be chosen in practice.",1. Introduction,[0],[0]
Our bound on the expected quantization error includes both a “traditional” multiplicative error term based on the optimal solution as well as a scale-invariant additive error term based on the variance of the data.,1. Introduction,[0],[0]
The key insight is that this additive error term vanishes at a rate of ( k e` )t if t or ` is increased.,1. Introduction,[0],[0]
"This shows that k-means‖ provides provably good clusterings even for a small, constant number of iterations and explains the commonly observed phenomenon that k-means‖ works very well even for small t.
We further provide a hard instance on which k-means‖ provably incurs an additive error based on the variance of the data and for which an exclusively multiplicative error guarantee cannot be achieved.",1. Introduction,[0],[0]
This implies that an additive error term such as the one in our analysis is in fact necessary if less than k − 1 rounds are employed.,1. Introduction,[0],[0]
k-Means clustering.,2. Background & related work,[0],[0]
Let X denote a set of points in Rd.,2. Background & related work,[0],[0]
"The k-Means clustering problem is to find a set C of k cluster centers in Rd that minimizes the quantization error
φX (C) = ∑ x∈X d(x,C)2 = ∑",2. Background & related work,[0],[0]
x∈X min,2. Background & related work,[0],[0]
"q∈C ‖x− q‖22.
Algorithm 1 k-means++ seeding Require: weighted data set (X , w), number of clusters k
1: C ← sample single x ∈ X with probability wx∑ x′∈X wx′ 2: for i = 2, . . .",2. Background & related work,[0],[0]
", k do 3: Sample x ∈ X with probability wx d(x,C)
2∑ x′∈X wx′ d(x",2. Background & related work,[0],[0]
"′,C)2
4: C ← C ∪ {x} 5: Return C
We denote the optimal quantization error by φOPT(X ) while the variance of the data is defined as Var(X ) = φX ({µ(X )}) where µ(X ) is the mean of X .
k-means++ seeding.",2. Background & related work,[0],[0]
"Given a data set X and any set of cluster centers C ⊂ X , the D2-sampling strategy selects a new center by sampling each point x ∈ X with probability
p(x) = d(x,C)2∑
x′∈X d(x ′, C ′)2
.
",2. Background & related work,[0],[0]
"The seeding step of k-means++ (Arthur & Vassilvitskii, 2007), detailed for potentially weighted data sets in Algorithm 1, selects an initial cluster center uniformly at random and then sequentially adds k − 1 cluster centers using D2 sampling whereby C is always the set of previously sampled centers.",2. Background & related work,[0],[0]
"Arthur & Vassilvitskii (2007) show that the solution quality φk-means++ of k-means++ seeding is bounded in expectation by
E[φk-means++] ≤ 8 (log2 k + 2)φOPT(X ).
",2. Background & related work,[0],[0]
The computational complexity of k-means++ seeding is O(nkd) where n is the number of data points and d the dimensionality.,2. Background & related work,[0],[0]
"Unfortunately, the iterations in k-means++ seeding are inherently sequential and, as a result, the algorithm requires k full passes through the data.",2. Background & related work,[0],[0]
"This makes the algorithm unsuitable for the distributed setting.
",2. Background & related work,[0],[0]
k-means‖ seeding.,2. Background & related work,[0],[0]
"As a remedy, Bahmani et al. (2012) propose the algorithm k-means‖ which aims to reduce the number of sequential iterations.",2. Background & related work,[0],[0]
"The key component of k-means‖ is detailed in Algorithm 2 in what we call k-means‖ overseeding: First, a data point is sampled as the first cluster center uniformly at random.",2. Background & related work,[0],[0]
"Then, in each of t sequential rounds, each data point x ∈ X is independently sampled with probability min ( 1, ` d(x,C) 2
φX (C)
) and
added to the set of sampled centers C at the end of the round.",2. Background & related work,[0],[0]
"The parameter ` ≥ 1 is called the oversampling factor and determines the expected number of sampled points in each iteration.
",2. Background & related work,[0],[0]
"At the end of Algorithm 2, one obtains an oversampled solution with t` cluster centers in expectation.",2. Background & related work,[0],[0]
"The full k-means‖ seeding algorithm as detailed in Algorithm 3 reduces such a solution to k centers as follows: First, each of the centers in the oversampled solution is weighted by the number of data points which are closer to it than the
Algorithm 2 k-means‖ overseeding Require: data set X , # rounds t, oversampling factor `
1: C ← sample a point uniformly at random from X 2: for i = 1, 2, . . .",2. Background & related work,[0],[0]
", t do 3: C ′",2. Background & related work,[0],[0]
← ∅ 4: for x ∈ X,2. Background & related work,[0],[0]
"do 5: Add x to C ′ with probability min ( 1, ` d(x,C) 2
φX (C) ) 6: C ← C ∪ C ′ 7: Return C
Algorithm 3 k-means‖ seeding Require: data set X , # rounds t, oversampling factor `
1: B ← Result of Algorithm 2 applied to (X , t, `) 2: for c ∈ B do 3:",2. Background & related work,[0],[0]
Xc ← points x ∈ X,2. Background & related work,[0],[0]
"whose closest center in B is c (ties broken arbitrarily but consistently) 4: wc ← |Xc| 5: C ← Result of Algorithm 1 applied to (B,w) 6:",2. Background & related work,[0],[0]
"Return C
other centers.",2. Background & related work,[0],[0]
"Then, k-means++ seeding is run on the weighted oversampled solution to produce a set of k final centers.",2. Background & related work,[0],[0]
"The total computational complexity of Algorithm 3 is O(nt`d) in expectation.
",2. Background & related work,[0],[0]
"The key intuition behind k-means‖ is that, if we choose a large oversampling factor `, the number of rounds t can be small — certainly much smaller than k, preferably even constant.",2. Background & related work,[0],[0]
The step in lines 4 and 5 in Algorithm 2 can be distributed over several machines and after each round the set C can be synchronized.,2. Background & related work,[0],[0]
"Due to the low number of synchronizations (i.e., rounds), Algorithm 2 can be efficiently run in a distributed setting.1
Other related work.",2. Background & related work,[0],[0]
Celebi et al. (2013) provide an overview over different seeding methods for k-Means.,2. Background & related work,[0],[0]
D2sampling and k-means++ style algorithms have been independently studied by both Ostrovsky et al. (2006) and Arthur & Vassilvitskii (2007).,2. Background & related work,[0],[0]
"This research direction has led to polynomial time approximation schemes based on D2-sampling (Jaiswal et al., 2014; 2015), constant factor approximations based on sampling more than k centers (Ailon et al., 2009; Aggarwal et al., 2009) and the analysis of hard instances (Arthur & Vassilvitskii, 2007; Brunsch & Röglin, 2011).",2. Background & related work,[0],[0]
"Recently, algorithms to approximate k-means++ seeding based on Markov Chain Monte Carlo have been proposed by Bachem et al. (2016b;a).",2. Background & related work,[0],[0]
"Finally, k-means++ has been used to construct coresets — small data set summaries — for k-Means clustering (Lucic et al., 2016; Bachem et al., 2015; Fichtenberger et al., 2013; Ackermann et al., 2012) and Gaussian mixture models (Lucic et al., 2017).
",2. Background & related work,[0],[0]
"1A popular choice is the MLLib library of Apache Spark (Meng et al., 2016) which uses k-means‖ by default.",2. Background & related work,[0],[0]
"In this section, we provide the intuition and the main results behind our novel analysis of k-means‖ and defer the formal statements and the formal proofs to Section 4.",3. Intuition and key results,[0],[0]
Solution quality of Algorithm 2.,3.1. Solution quality of k-means‖,[0],[0]
We first consider Algorithm 2 as it largely determines the final solution quality.,3.1. Solution quality of k-means‖,[0],[0]
"Algorithm 3 with its use of k-means++ to obtain the final k cluster centers, only adds an additional O(log k) factor as shown in Theorem 1.",3.1. Solution quality of k-means‖,[0],[0]
"Our key result is Lemma 4 (see Section 4) which guarantees that, for ` ≥ k, the expected error of solutions computed by Algorithm 2 is at most
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
"(1)
The first term may be regarded as a scale-invariant additive error: It is additive as it does not depend on the optimal quantization error φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
It is scale-invariant since both the variance and the quantization error are scaled by λ2 if we scale the data setX by λ > 0.,3.1. Solution quality of k-means‖,[0],[0]
"The second term is a “traditional” multiplicative error term based on the optimal quantization error.
",3.1. Solution quality of k-means‖,[0],[0]
"Given a fixed oversampling factor `, the additive error term decreases exponentially if the number of rounds t is increased.",3.1. Solution quality of k-means‖,[0],[0]
"Similarly, for a fixed number of rounds t, it decreases polynomially at a rate O ( 1 `t )",3.1. Solution quality of k-means‖,[0],[0]
if the over sampling factor ` is increased.,3.1. Solution quality of k-means‖,[0],[0]
This result implies that even for a constant number of rounds one may obtain good clusterings by increasing the oversampling factor `.,3.1. Solution quality of k-means‖,[0],[0]
"This explains the empirical observation that often even a low number of rounds t is sufficient and that increasing ` increases the solution quality (Bahmani et al., 2012).",3.1. Solution quality of k-means‖,[0],[0]
The practical implications of this result are non-trivial: Even for the choice of t = 5 and ` = 5k one retains at most 0.0004% of the variance as an additive error.,3.1. Solution quality of k-means‖,[0],[0]
"Furthermore, state of the art uniform deviation bounds for k-Means include a similar additive error term (Bachem et al., 2017).
",3.1. Solution quality of k-means‖,[0],[0]
Comparison to previous result.,3.1. Solution quality of k-means‖,[0],[0]
Bahmani et al. (2012) show the following result: Let C be the set returned by Algorithm 2 with t rounds.,3.1. Solution quality of k-means‖,[0],[0]
For α = exp ( −(1− e−`/(2k)) ),3.1. Solution quality of k-means‖,[0],[0]
"≈ e− `2k , Corollary 3 of Bahmani et al. (2012) bounds the expected quality of C by
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"( 1 + α
2
)t ψ + 16
1− α φOPT(X ), (2)
where ψ denotes the quantization error of X based on the first, uniformly sampled center in k-means‖.",3.1. Solution quality of k-means‖,[0],[0]
"The key difference compared to our result is as follows: First, even as we increase `, the factor α is always non-negative.",3.1. Solution quality of k-means‖,[0],[0]
"Hence, regardless of the choice of `, the additive ψ term is reduced
by at most 12 per round.",3.1. Solution quality of k-means‖,[0],[0]
2,3.1. Solution quality of k-means‖,[0],[0]
"This means that, given the analysis in Bahmani et al. (2012), one would always obtain a constant additive error for a constant number of rounds t, even as ` is increased.
",3.1. Solution quality of k-means‖,[0],[0]
Guarantee for Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
Our main result — Theorem 1 — bounds the expected quality of solutions produced by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"As in Bahmani et al. (2012), one loses another factor ofO(ln k) compared to (1) due to Algorithm 3.",3.1. Solution quality of k-means‖,[0],[0]
Theorem 1.,3.1. Solution quality of k-means‖,[0],[0]
"Let k ∈ N, t ∈ N",3.1. Solution quality of k-means‖,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the set returned by Algorithm 3.,3.1. Solution quality of k-means‖,[0],[0]
"Then,
E[φX (C)] ≤",3.1. Solution quality of k-means‖,[0],[0]
"O
(( k
e`
)t ln k ) Var(X )+O(ln k)φOPT(X ).",3.1. Solution quality of k-means‖,[0],[0]
We consider the case t < k−1 which captures the scenario where k-means‖ is useful in practice as for t ≥,3.2. A hard instance for k-means‖,[0],[0]
"k one may simply use k-means++ instead.
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2.,3.2. A hard instance for k-means‖,[0],[0]
"For any β > 0, k ∈ N, t < k",3.2. A hard instance for k-means‖,[0],[0]
"− 1 and ` ≥ 1, there exists a data set X of size 2(t+ 1) such that
E[φX (C)] ≥ 1
4 (4`t)
−t Var(X ),
where C is the output of Algorithm 2 or Algorithm 3 applied to X with t and `.",3.2. A hard instance for k-means‖,[0],[0]
"Furthermore,
Var(X )",3.2. A hard instance for k-means‖,[0],[0]
"> 0, φOPT(X ) = 0 and n∆2 ≤ β
where ∆ is the largest distance between any points in X .
",3.2. A hard instance for k-means‖,[0],[0]
Theorem 2 shows that there exists a data set on which k-means‖ provably incurs a non-negligible error even if the optimal quantization error is zero.,3.2. A hard instance for k-means‖,[0],[0]
This implies that k-means‖ with t < k,3.2. A hard instance for k-means‖,[0],[0]
− 1 cannot provide a multiplicative guarantee on the expected quantization error for general data sets.,3.2. A hard instance for k-means‖,[0],[0]
We thus argue that an additive error bound such as the one in Theorem 1 is required.,3.2. A hard instance for k-means‖,[0],[0]
"We note that the upper bound in (1) and the lower bound in Theorem 2 exhibit the same 1`t dependence on the oversampling factor ` for a given number of rounds t.
Furthermore, Theorem 2 implies that, for general data sets, k-means‖ cannot achieve the multiplicative error of O(log k) in expectation as claimed by Bahmani et al. (2012).3 In particular, if the optimal quantization error is
2Note that E[ψ] ≤ 2Var(X ) (Arthur & Vassilvitskii, 2007).",3.2. A hard instance for k-means‖,[0],[0]
"3To see this, let ψ = φX (c1) be the quantization error of the first sampled center in Algorithm 2 and choose β small enough such that the choice of t ∈ O(logψ) leads to t < k",3.2. A hard instance for k-means‖,[0],[0]
− 1.,3.2. A hard instance for k-means‖,[0],[0]
"For X in Theorem 2, φOPT(X ) = 0 which implies that the desired multiplicative guarantee would require E[φX (C)] = 0.",3.2. A hard instance for k-means‖,[0],[0]
"However, the non-negligible, additive error in Theorem 2 and Var(X )",3.2. A hard instance for k-means‖,[0],[0]
> 0 implies that E[φX (C)],3.2. A hard instance for k-means‖,[0],[0]
"> 0.
zero, then k-means‖ would need to return a solution with quantization error zero.",3.2. A hard instance for k-means‖,[0],[0]
"While we are guaranteed to remove a constant fraction of the error in each round, the number of required iterations may be unbounded.",3.2. A hard instance for k-means‖,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
"The proof is divided into four steps: First, we relate k-means‖-style oversampling to k-means++-style D2-sampling in Lemmas 1 and 2.",4. Theoretical analysis,[0],[0]
"Second, we analyze a single iteration of Algorithm 2 in Lemma 3.",4. Theoretical analysis,[0],[0]
"Third, we bound the expected solution quality of Algorithm 2 in Lemma 4.",4. Theoretical analysis,[0],[0]
"Finally, we use this to bound the expected solution quality of Algorithm 3 in Theorem 1.
Lemma 1.",4. Theoretical analysis,[0],[0]
Let A be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .
",4. Theoretical analysis,[0],[0]
"Let P be a probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa ∈",4. Theoretical analysis,[0],[0]
"[0, 1].",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability qa/ ∑ a∈A qa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
"To prove the claim, we first construct a series of sub-events of the events {Ea}a∈A and then use them to recursively bound EP [f(C)].
",4. Theoretical analysis,[0],[0]
Let m ∈ N.,4. Theoretical analysis,[0],[0]
"For each a ∈ A, let ia be an independent random variable drawn uniformly at random from {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}.",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, let Fai be an independent event that occurs with probability
P[Fai] = (
1− qa m
)i−1 .
",4. Theoretical analysis,[0],[0]
"For each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m, denote by Eai the event that occurs if i = ia and both Ea and Fai occur.",4. Theoretical analysis,[0],[0]
"By design, all these events are independent and thus
P[Eai] = P[Ea]P[Fai]P[ia = i] =",4. Theoretical analysis,[0],[0]
"qa m
( 1− qa
m
)i−1 (3)
for each a ∈ A and i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m. Furthermore, for any a, a′ ∈ A with a 6= a′ and any i, i′ ∈ {1, 2, . . .",4. Theoretical analysis,[0],[0]
",m}, the events Eai and Ea′i′ are independent.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m let Gi be the event that none of the events {Eai′}a∈A,i′≤i occur, i.e.,
Gi =",4. Theoretical analysis,[0],[0]
⋂ i′≤i ⋂ a∈A,4. Theoretical analysis,[0],[0]
"Eai′
where A denotes the complement of A. For convenience, let G0 be the event that occurs with probability one.
",4. Theoretical analysis,[0],[0]
"Let (a1, a2, . . .",4. Theoretical analysis,[0],[0]
", a|A|) be any enumeration of A.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|+1, define the event
Gi,j = Gi−1 ∩ ⋂
0<j′<j
Eaj′ i.
We note that by definitionGi,1 = Gi−1 andGi,|A|+1 = Gi for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m.
For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m and j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A|, we have E[f(C)|Gi,j ] =",4. Theoretical analysis,[0],[0]
"P [ Eaji | Gi,j ] E",4. Theoretical analysis,[0],[0]
[ f(C),4. Theoretical analysis,[0],[0]
"| Eaji ∩Gij ] + P [ Eaj′ i | Gij ] E[f(C) | Gi,j+1].
(4)
We now bound the individual terms.",4. Theoretical analysis,[0],[0]
"The event Gi,j implies that the events {Eaji′}i′<i did not occur.",4. Theoretical analysis,[0],[0]
"Furthermore, Eaji is independent of the events {Eaj′ i′}i′=1,2,...,m for j′ 6=",4. Theoretical analysis,[0],[0]
"j. Hence, we have
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = P [ Eaji | G0 ∩ ⋂ i′<i Eaji′ ]
= P",4. Theoretical analysis,[0],[0]
"[ Eaji ] P [ G0 ∩ ⋂ i′<iEaji′
] = P [ Eaji ] 1− P",4. Theoretical analysis,[0],[0]
"[⋃ i′<iEaji′
] = P [ Eaji ] 1− ∑ i′<i P [ Eaji′ ] ,
(5)
where the last equality follows since the events {Eaji′}i′<i are disjoint.",4. Theoretical analysis,[0],[0]
"Using (3), we observe that ∑ i′<i P [ Eaji′ ] is a sum of a finite geometric series and we have∑ i′<i P [ Eaji′ ] = ∑ i′<i qa m ( 1− qa m
)i′−1 =",4. Theoretical analysis,[0],[0]
qa m 1− ( 1− qam ),4. Theoretical analysis,[0],[0]
i−1 1−,4. Theoretical analysis,[0],[0]
"( 1− qam
)",4. Theoretical analysis,[0],[0]
"= 1− ( 1− qa
m
)",4. Theoretical analysis,[0],[0]
"i−1 .
",4. Theoretical analysis,[0],[0]
"Together with (3) and (5), this implies
P",4. Theoretical analysis,[0],[0]
"[ Eaji | Gi,j ] = qa m
( 1− qam )",4. Theoretical analysis,[0],[0]
"i−1( 1− qam
)i−1 = qam.",4. Theoretical analysis,[0],[0]
(6) The event Eaji implies that C contains aj .,4. Theoretical analysis,[0],[0]
"Hence, since f is monotonically decreasing, we have
E [ f(C) | Eaji ∩Gij ] ≤ f({aj}).
",4. Theoretical analysis,[0],[0]
"Using (4) and (6), this implies
E[f(C)|Gi,j ] ≤ qaj m f({aj})+
( 1−
qaj m
) E[f(C)",4. Theoretical analysis,[0],[0]
"| Gi,j+1].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively for j = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", |A| implies E[f(C)|Gi,1] = |A|∑ j=1 qaj m ∏",4. Theoretical analysis,[0],[0]
j′<j ( 1− qaj′ m ) f({aj}),4. Theoretical analysis,[0],[0]
"+
 |A|∏ j=1 ( 1− qaj m )E[f(C) | Gi,|A|+1].",4. Theoretical analysis,[0],[0]
Note that 0 ≤ 1,4. Theoretical analysis,[0],[0]
− qam ≤ 1 for all a ∈ A and that f is non-negative.,4. Theoretical analysis,[0],[0]
"This implies that for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi,1] ≤ ∑ a∈A qa m f({a}) + c · E [ f(C)|Gi,|A|+1 ] where
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
"( 1− qa m ) .
",4. Theoretical analysis,[0],[0]
"Since Gi,1 = Gi−1 and Gi,|A|+1 = Gi, we have for i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
",m
E[f(C)|Gi−1] ≤ ∑ a∈A qa m f({a}) + c ·",4. Theoretical analysis,[0],[0]
"E[f(C)|Gi].
",4. Theoretical analysis,[0],[0]
"Applying this result iteratively, we obtain
E[f(C)] ≤",4. Theoretical analysis,[0],[0]
( m∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 )∑ a∈A qa m f({a}) + cm · f(∅).
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ c ≤ 1, we have m∑ i=1",4. Theoretical analysis,[0],[0]
ci−1 ≤ ∞∑ i=1,4. Theoretical analysis,[0],[0]
"ci−1 = 1 1− c .
",4. Theoretical analysis,[0],[0]
For x ∈,4. Theoretical analysis,[0],[0]
"[−1, 0]",4. Theoretical analysis,[0],[0]
"it holds that log(1 + x) ≤ x and hence
cm = ∏ a∈A ( 1− qa m )m = exp ( m ∑ a∈A log ( 1− qa m ))
≤ exp ( −m
∑ a∈A qa m
) = e− ∑ a∈A qa .
",4. Theoretical analysis,[0],[0]
"This implies that
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 1− c ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m f({a})+e− ∑ a∈A qa ·f(∅).,4. Theoretical analysis,[0],[0]
"(7)
We show the main claim by contradiction.",4. Theoretical analysis,[0],[0]
Assume,4. Theoretical analysis,[0],[0]
"that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
",4. Theoretical analysis,[0],[0]
"If EQ[f({a})] = 0, the contradiction follows directly from (7).",4. Theoretical analysis,[0],[0]
"Otherwise, EQ[f({a})]",4. Theoretical analysis,[0],[0]
"> 0 implies that there exists an > 0 such that
EP [f(C)]",4. Theoretical analysis,[0],[0]
> (1 + )EQ[f({a})],4. Theoretical analysis,[0],[0]
+ e− ∑ a∈A qaf(∅).,4. Theoretical analysis,[0],[0]
"(8)
By definition, we have
c = ∏ a∈A",4. Theoretical analysis,[0],[0]
( 1− qa m ),4. Theoretical analysis,[0],[0]
= 1− ∑ a∈A,4. Theoretical analysis,[0],[0]
qa m + o,4. Theoretical analysis,[0],[0]
"( 1 m ) .
",4. Theoretical analysis,[0],[0]
"Thus, there exists a m ∈ N sufficiently large such that
c = 1− ∑ a∈A qa m + o",4. Theoretical analysis,[0],[0]
( 1 m ) ≤,4. Theoretical analysis,[0],[0]
"1− 1 1 + ∑ a∈A qa m .
",4. Theoretical analysis,[0],[0]
"Together with (7), this implies
E[f(C)]",4. Theoretical analysis,[0],[0]
≤ 1 + ∑ a∈A qa m ∑ a∈A qa m f({a}),4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qa · f(∅)
= (1 + )",4. Theoretical analysis,[0],[0]
EQ[f({a})],4. Theoretical analysis,[0],[0]
"+ e− ∑ a∈A qaf(∅).
which is a contradiction to (8) and thus proves the claim.
",4. Theoretical analysis,[0],[0]
"Lemma 2 extends Lemma 1 to k-means‖-style sampling probabilities of the form qa = min (1, `pa).
",4. Theoretical analysis,[0],[0]
Lemma 2.,4. Theoretical analysis,[0],[0]
Let ` ≥ 1.,4. Theoretical analysis,[0],[0]
LetA be a finite set and let,4. Theoretical analysis,[0],[0]
"f : 2A → R be a set function that is non-negative and monotonically decreasing, i.e., f(V )",4. Theoretical analysis,[0],[0]
"≥ f(U) ≥ 0, for all V ⊆ U .",4. Theoretical analysis,[0],[0]
"For each a ∈ A, let pa ≥ 0",4. Theoretical analysis,[0],[0]
"and ∑ a∈A pa ≤ 1.
",4. Theoretical analysis,[0],[0]
"Let P be the probability distribution where, for each a ∈ A, Ea denotes an independent event that occurs with probability qa = min (1, `pa).",4. Theoretical analysis,[0],[0]
"Let C be the set of elements a ∈ A for which the event Ea occurs.
",4. Theoretical analysis,[0],[0]
Let Q be the probability distribution on A where,4. Theoretical analysis,[0],[0]
"a single a ∈ A is sampled with probability pa/ ∑ a∈A pa.
",4. Theoretical analysis,[0],[0]
"Then, with ∅ denoting the empty set, we have that
EP [f(C)] ≤ 2EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
LetA1 be the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa ≤ 1 and A2 the set of elements a ∈,4. Theoretical analysis,[0],[0]
A such that `pa > 1.,4. Theoretical analysis,[0],[0]
"By definition, every element in A2 is sampled almost surely, i.e., A2 ⊆ C.",4. Theoretical analysis,[0],[0]
"This implies that almost surely
f(C) = f (A2 ∪ (C ∩A1)) .",4. Theoretical analysis,[0],[0]
"(9)
If |A1|= 0, the result follows trivially since
EP [f(C)] = f(A2) = EQ[f({a})].
",4. Theoretical analysis,[0],[0]
"Similarly, if |A2|= 0, the result follows directly from Lemma 1 with qa = `pa.",4. Theoretical analysis,[0],[0]
"For the remainder of the proof, we may thus assume that both A1 and A2 are non-empty.
",4. Theoretical analysis,[0],[0]
"For a ∈ A1, let qa = `pa and define the non-negative and monotonically decreasing function
g(C) = f (A2 ∪ C) .
",4. Theoretical analysis,[0],[0]
Let p1 = ∑ a∈A1 pa and p2 = ∑ a∈A2 pa.,4. Theoretical analysis,[0],[0]
"Lemma 1 applied to A1, qa and g implies that
EP [f(C)]",4. Theoretical analysis,[0],[0]
"= E[g(C)] ≤ ∑ a∈A1 pa p1 g({a}) + e−`p1g(∅).
",4. Theoretical analysis,[0],[0]
"(10)
Let d = ( 1− e−`p2 ) e−`p1
and define α =
p2 p1 + p2 − p1 p1 + p2 d.
By design, α ≤ 1.",4. Theoretical analysis,[0],[0]
"Furthermore
`p1 ≥ log `p1.
SinceA2 is nonempty and pa ≥ 1` for all a ∈ A2, it follows that p2 ≥ 1` .",4. Theoretical analysis,[0],[0]
"This implies
e`p1 ≥ `p1 ≥ p1 p2 .
",4. Theoretical analysis,[0],[0]
"Since 0 ≤ ( 1− e−`p2 ) ≤ 1, we have
p2 ≥ p1e−`p1 ≥",4. Theoretical analysis,[0],[0]
"p1 ( 1− e−`p2 ) e−`p1 = p1d.
",4. Theoretical analysis,[0],[0]
"Hence,
α = p2 p1 + p2 − p1",4. Theoretical analysis,[0],[0]
"p1 + p2
( 1− e−`p2 ) e−`p1 ≥ 0.
",4. Theoretical analysis,[0],[0]
Since α ∈,4. Theoretical analysis,[0],[0]
"[0, 1] and g({a}) ≤ g(∅) for any a ∈ A1, we may write (10), i.e., EP [f(C)] ≤ (1− α) ∑ a∈A1 pa p1 g({a})",4. Theoretical analysis,[0],[0]
+ ( α+ e−`p1 ),4. Theoretical analysis,[0],[0]
"g(∅).
(11)
",4. Theoretical analysis,[0],[0]
"By definition, we have
1− α = 1− p2 p1 + p2 + p1 p1 + p2 d = p1 p1 + p2 (1 + d).
",4. Theoretical analysis,[0],[0]
"Since g({a}) ≤ f({a}), we thus have
(1− α) ∑ a∈A1 pa p1 g({a}) ≤ (1 + d) ∑ a∈A1 pa p1 + p2 f({a}).
",4. Theoretical analysis,[0],[0]
"(12)
Similarly, we have
α+ e−`p1 = p2 p1 + p2 − p1 p1 + p2 d+ e−`p1
= p2
p1 + p2 + d p2 p1 + p2 − d+ e−`p1
= (1 + d)",4. Theoretical analysis,[0],[0]
"p2
p1 + p2 + e−`(p1+p2).
",4. Theoretical analysis,[0],[0]
"Since g(∅) ≤ f(∅), it follows that( α+ e−`p1 ) g(∅) ≤ (1+d) p2
p1 + p2 g(∅)+e−`(p1+p2)f(∅).
",4. Theoretical analysis,[0],[0]
(13) Since g(∅) = f (A2) and thus g(∅) ≤,4. Theoretical analysis,[0],[0]
"f({a}) for all a ∈ A2, we have
p2g(∅) = ∑ a∈A2 pag(∅) ≤ ∑",4. Theoretical analysis,[0],[0]
a∈A2 paf({a}).,4. Theoretical analysis,[0],[0]
"(14)
Combining (11), (12), (13), and (14) leads to
EP [f(C)] ≤ (1 + d)EQ[f({a})]",4. Theoretical analysis,[0],[0]
"+ e−` ∑ a∈A paf(∅).
",4. Theoretical analysis,[0],[0]
"Since p1 ≥ 0, we have 1 + d = 1 + ( 1− e−`p2 ) e−`p1 ≤ 2
which proves the main claim.
",4. Theoretical analysis,[0],[0]
"Lemma 3 bounds the solution quality after each iteration of Algorithm 2 based on the solution before the iteration.
",4. Theoretical analysis,[0],[0]
Lemma 3.,4. Theoretical analysis,[0],[0]
Let k ∈,4. Theoretical analysis,[0],[0]
N,4. Theoretical analysis,[0],[0]
and ` ≥ 1.,4. Theoretical analysis,[0],[0]
Let X be a data set in Rd and denote by φOPT(X ),4. Theoretical analysis,[0],[0]
the optimal k-Means clustering cost.,4. Theoretical analysis,[0],[0]
LetC denote the set of cluster centers at the beginning of an iteration in Algorithm 2 and C ′,4. Theoretical analysis,[0],[0]
the random set added in the iteration.,4. Theoretical analysis,[0],[0]
"Then, it holds that
E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The proof relies on applying Lemma 2 to each cluster of the optimal solution.,4. Theoretical analysis,[0],[0]
Let OPT denote any clustering achieving the minimal cost φOPT(X ) on X .,4. Theoretical analysis,[0],[0]
We assign all the points x ∈ X to their closest cluster center in OPT with ties broken arbitrarily but consistently.,4. Theoretical analysis,[0],[0]
"For c ∈ OPT we denote by Xc the subset of X assigned to c. For each c ∈ OPT, let
C ′c = C ′ ∩",4. Theoretical analysis,[0],[0]
"Xc.
",4. Theoretical analysis,[0],[0]
"By definition, a ∈ Xc is included in C ′c with probability
qa = min
( 1,
`d(a,C)2∑ a′∈X d(a ′, C)2
) .
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, we define the monotonically decreasing function fc : 2Xc → R≥0 to be
fc(C ′",4. Theoretical analysis,[0],[0]
"c) = φXc(C ∪ C ′c).
",4. Theoretical analysis,[0],[0]
"For each c ∈ OPT, Lemma 2 applied to Xc, C ′c and fc implies
E[fc(C ′c)] ≤2 ∑ a∈Xc d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a})
+ e",4. Theoretical analysis,[0],[0]
"−`
∑ a∈Xc d(a,C)
2∑ a′∈X d(a
′,C)2 fc(∅).
",4. Theoretical analysis,[0],[0]
"(15)
Since fc({a}) = φXc(C ∪ {a}), the first term is equivalent to sampling a single element from Xc using D2 sampling.",4. Theoretical analysis,[0],[0]
"Hence, by Lemma 3.3 of Arthur & Vassilvitskii (2007) we have for all c ∈ OPT∑
a∈Xc
d(a,C)2∑ a′∈Xc d(a ′, C)2 fc({a}).",4. Theoretical analysis,[0],[0]
≤ 8φOPT(Xc).,4. Theoretical analysis,[0],[0]
"(16)
For each c ∈ OPT, we further have
e −`
∑ a∈Xc d(a,C)
",4. Theoretical analysis,[0],[0]
"2∑ a′∈X d(a
′,C)2 fc(∅) =",4. Theoretical analysis,[0],[0]
"e−`ucucφX (C).
",4. Theoretical analysis,[0],[0]
"where
uc =
∑ a∈Xc d(a,C)
2∑ a′∈X d(a ′, C)2 = φXc(C) φX (C) .
",4. Theoretical analysis,[0],[0]
"We have that
log `uc ≤",4. Theoretical analysis,[0],[0]
"`uc − 1 ⇐⇒ `uc ≤ e`uc
e
which implies
e−`ucucφX (C) ≤ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(17)
Combining (15), (16) and (17), we obtain
E[fc(C ′c)] ≤16φOPT(Xc)",4. Theoretical analysis,[0],[0]
"+ 1
e` φX (C).",4. Theoretical analysis,[0],[0]
"(18)
",4. Theoretical analysis,[0],[0]
"Since E[φX (C ∪ C ′)] ≤ ∑ c∈OPT E[fc(C ′c)]
and φX (OPT) = ∑ c∈OPT φXc(OPT),
we thus have E[φX (C ∪ C ′)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
) φX (C) + 16φOPT(X )
which concludes the proof.
",4. Theoretical analysis,[0],[0]
"An iterated application of Lemma 3 allows us to bound the solution quality of Algorithm 2 in Lemma 4.
",4. Theoretical analysis,[0],[0]
Lemma 4.,4. Theoretical analysis,[0],[0]
"Let k ∈ N, t ∈ N",4. Theoretical analysis,[0],[0]
and ` ≥ k. Let X be a data set in Rd and C be the random set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"Then,
E[φX (C)] ≤ 2 ( k
e`
)t Var(X ) + 26φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof.,4. Theoretical analysis,[0],[0]
The algorithm starts with a uniformly sampled initial cluster center c1.,4. Theoretical analysis,[0],[0]
"We iteratively apply Lemma 3 for each of the t rounds to obtain
E[φX (C)] ≤",4. Theoretical analysis,[0],[0]
"( k
e`
)t E[φX ({c1})]",4. Theoretical analysis,[0],[0]
"+ 16stφOPT(X ) (19)
",4. Theoretical analysis,[0],[0]
"where
st = t∑ i=1",4. Theoretical analysis,[0],[0]
"( k e` )i−1 .
",4. Theoretical analysis,[0],[0]
"For ` ≥ k, we have 0 ≤ ke` ≤ 1/e and hence
st ≤",4. Theoretical analysis,[0],[0]
t∑ i=1,4. Theoretical analysis,[0],[0]
1,4. Theoretical analysis,[0],[0]
"ei−1 ≤ ∞∑ i=0 1 ei =
1
1− 1/e .",4. Theoretical analysis,[0],[0]
"(20)
By Lemma 3.2 of Arthur & Vassilvitskii (2007), we have that E[φX ({c1})] ≤ 2 Var(X ).",4. Theoretical analysis,[0],[0]
"Together with (19), (20) and 16/(1− 1/e) ≈ 25.31 < 26, this implies the required result.
",4. Theoretical analysis,[0],[0]
"With Lemma 4, we are further able to bound the solution quality of Algorithm 3 and prove Theorem 1.
",4. Theoretical analysis,[0],[0]
Proof of Theorem 1.,4. Theoretical analysis,[0],[0]
Let B be the set returned by Algorithm 2.,4. Theoretical analysis,[0],[0]
"For any x ∈ X , let bx denote its closest point in B with ties broken arbitrarily.",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2 + 2b2, for any x ∈ X
d(x,C)2 ≤ 2 d(x, bx)2 + 2 d(bx, C)2
and hence
E[φX (C)]",4. Theoretical analysis,[0],[0]
"= ∑ x∈X d(x,C)2
≤ 2 ∑ x∈X d(x, bx) 2 + 2 ∑ x∈X d(bx, C) 2
= 2φX (B) + 2 ∑ x∈B wx d(x,C) 2.
(21)
",4. Theoretical analysis,[0],[0]
"Let OPTX be the optimal k-Means clustering solution on X and OPT(B,w) the optimal solution on the weighted set (B,w).",4. Theoretical analysis,[0],[0]
"By Theorem 1.1 of Arthur & Vassilvitskii (2007),
k-means++ produces an α = 8(log2 k + 2) approximation to the optimal solution.",4. Theoretical analysis,[0],[0]
"This implies that∑
x∈B wx d(x,C) 2 ≤ α ∑ x∈B wx d",4. Theoretical analysis,[0],[0]
"( x,OPT(B,w) )",4. Theoretical analysis,[0],[0]
"2
≤ α ∑ x∈B wx d(x,OPTX ) 2
= α ∑ x∈X d(bx,OPTX ) 2.
(22)
",4. Theoretical analysis,[0],[0]
"By the triangle inequality and since (|a|+|b|)2 ≤ 2a2+2b2, it holds for any x ∈ X that
d(bx,OPTX ) 2 ≤ 2 d(x, bx)2 + 2 d(x,OPTX )2
and hence∑",4. Theoretical analysis,[0],[0]
x∈X,4. Theoretical analysis,[0],[0]
"d(bx,OPTX ) 2 ≤",4. Theoretical analysis,[0],[0]
2φX (B) + 2φOPT(X ).,4. Theoretical analysis,[0],[0]
"(23)
Combining (21), (22) and (23), we obtain
E[φX (C)] ≤ 2(1 + α)φX (B) + 2αφOPT(X ).
",4. Theoretical analysis,[0],[0]
"Finally, by Lemma 4, we have E[φX (C)]",4. Theoretical analysis,[0],[0]
"≤(32 log2 k + 68) ( k
e`
)t Var(X )
+ (432 log2 k + 916)φOPT(X ).
",4. Theoretical analysis,[0],[0]
Proof of Theorem 2.,4. Theoretical analysis,[0],[0]
"For this proof, we explicitly construct a data set: Let β′ > 0 and consider points in onedimensional Euclidean space.",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, set
xi =
√ β′(4`t) 1−i",4. Theoretical analysis,[0],[0]
"− β′(4`t)−i
as well as xt+1 = √ β′(4`t) −t .
",4. Theoretical analysis,[0],[0]
"Let the data setX consist of the t+1 points {xi}t=1,2,...,t+1 as well as t + 1 points at the origin.",4. Theoretical analysis,[0],[0]
Since t < k,4. Theoretical analysis,[0],[0]
"− 1, the optimal k-Means clustering solution consists of t + 2 points placed at each of the {xi}i=1,2,...t+1 and at 0.",4. Theoretical analysis,[0],[0]
"By design, this solution has a quantization error of zero and the variance is nonzero, i.e., φOPT(X )",4. Theoretical analysis,[0],[0]
= 0 and Var(X ),4. Theoretical analysis,[0],[0]
"> 0 as claimed.
",4. Theoretical analysis,[0],[0]
Choose β′ = β2(t+1) .,4. Theoretical analysis,[0],[0]
"The maximal distance ∆ between any two points in X is bounded by ∆ = d(0, x1)2 ≤ β′.",4. Theoretical analysis,[0],[0]
"Since n = 2(t+ 1), this implies ψ ≤",4. Theoretical analysis,[0],[0]
n∆2 ≤,4. Theoretical analysis,[0],[0]
"β as claimed.
",4. Theoretical analysis,[0],[0]
"For i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t, let Ci consist of 0 and all xj with j < i.",4. Theoretical analysis,[0],[0]
"By design, we have d(0, Ci)2 = 0 as well as d(xj , Ci)2 = 0 for j < i.",4. Theoretical analysis,[0],[0]
For j ≥,4. Theoretical analysis,[0],[0]
"i, we have d(xj , Ci)2 = d(xj , 0)2.",4. Theoretical analysis,[0],[0]
"For any i = 1, 2, . . .",4. Theoretical analysis,[0],[0]
", t+ 1, we thus have∑
j≥i
d(xj , 0) 2 = β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
Consider a single iteration of Algorithm 2 where C = Ci.,4. Theoretical analysis,[0],[0]
"In this case, all points in Xj with j <",4. Theoretical analysis,[0],[0]
i are added to C ′ with probability zero and for j >,4. Theoretical analysis,[0],[0]
i each point xj is added to C ′,4. Theoretical analysis,[0],[0]
"with probability
min ( 1, `d(xj , 0) 2∑
j′≥i d(xj′ , 0) 2
) =",4. Theoretical analysis,[0],[0]
"`d(xj , 0) 2
β′(4`t) 1−i .
",4. Theoretical analysis,[0],[0]
"By the union bound, the probability that any of the points in ⋃ j>i{xj} are sampled is bounded by∑
j>i
`d(xj , 0) 2 β′(4`t) 1−i = 1 4t .
",4. Theoretical analysis,[0],[0]
"The point xi is not sampled with probability at most
1−min ( 1, ` d(xi, 0) 2∑
j′≥i d(xj′ , 0) 2
) = 1−min ( 1, `− 1
4t ) ≤ 1
4t .
",4. Theoretical analysis,[0],[0]
"By the union bound, a single iteration of Algorithm 2 with C = Ci hence samples exactly the set C ′ = {xi} with probability at least ( 1− 12t ) .
",4. Theoretical analysis,[0],[0]
"In Algorithm 2, the first center is sampled uniformly at random from X .",4. Theoretical analysis,[0],[0]
"Since half of the elements in X are placed at 0, with probability at least 12 , the first center is at 0 or equivalently C = C1.",4. Theoretical analysis,[0],[0]
"With probability ( 1− 12t
)t ≥ 12 , we then sample exactly the points x1, x2, . . .",4. Theoretical analysis,[0],[0]
", xt in the t subsequent iterations.",4. Theoretical analysis,[0],[0]
"Hence, with probability at least 14 , the solution produced by Algorithm 2 consists of 0 and all xi except xt+1.",4. Theoretical analysis,[0],[0]
"Since xt+1 is closest to 0, this implies
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 d(xt+1, 0)
2 = 1
4 β′(4`t) −t .",4. Theoretical analysis,[0],[0]
"(24)
The variance of X is bounded by a single point at 0, i.e., Var(X ) ≤ φX ({0}) =",4. Theoretical analysis,[0],[0]
"∑ j≥1 d(xj , 0) 2 = β′.
Together with (24), we have that
E[φX (C)]",4. Theoretical analysis,[0],[0]
"≥ 1
4 (4`t)
−t Var(X ).
",4. Theoretical analysis,[0],[0]
The same result extends to the output of Algorithm 3 as it always picks a subset of the output of Algorithm 2.,4. Theoretical analysis,[0],[0]
"This research was partially supported by SNSF NRP 75, ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship.",Acknowledgements,[0],[0]
This work was done in part while Andreas Krause was visiting the Simons Institute for the Theory of Computing.,Acknowledgements,[0],[0]
The k-means++ algorithm is the state of the art algorithm to solve k-Means clustering problems as the computed clusterings are O(log k) competitive in expectation.,abstractText,[0],[0]
"However, its seeding step requires k inherently sequential passes through the full data set making it hard to scale to massive data sets.",abstractText,[0],[0]
The standard remedy is to use the k-means‖ algorithm which reduces the number of sequential rounds and is thus suitable for a distributed setting.,abstractText,[0],[0]
"In this paper, we provide a novel analysis of the k-means‖ algorithm that bounds the expected solution quality for any number of rounds and oversampling factors greater than k, the two parameters one needs to choose in practice.",abstractText,[0],[0]
"In particular, we show that k-means‖ provides provably good clusterings even for a small, constant number of iterations.",abstractText,[0],[0]
This theoretical finding explains the common observation that k-means‖ performs extremely well in practice even if the number of rounds is low.,abstractText,[0],[0]
We further provide a hard instance that shows that an additive error term as encountered in our analysis is inevitable if less than k−1 rounds are employed.,abstractText,[0],[0]
Distributed and Provably Good Seedings for k-Means in Constant Rounds,title,[0],[0]
"Bayesian optimization (BO) has recently gained considerable traction due to its capability of finding the global maximum of a highly complex (e.g., non-convex, no closedform expression nor derivative), noisy black-box objective
1Ludwig-Maximilians-Universität, Munich, Germany.",1. Introduction,[0],[0]
A substantial part of this research was performed during his student exchange program at the National University of Singapore under the supervision of Bryan Kian Hsiang Low and culminated in his Bachelor’s thesis.,1. Introduction,[0],[0]
"2Department of Computer Science, National University of Singapore, Republic of Singapore.",1. Introduction,[0],[0]
"Correspondence to: Bryan Kian Hsiang Low <lowkh@comp.nus.edu.sg>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"function with a limited budget of (often costly) function evaluations, consequently witnessing its use in an increasing diversity of application domains such as robotics, environmental sensing/monitoring, automatic machine learning, among others (Brochu et al., 2010; Shahriari et al., 2016).",1. Introduction,[0],[0]
"A number of acquisition functions (e.g., probability of improvement or expected improvement (EI) over the currently found maximum (Brochu et al., 2010), entropybased (Villemonteix et al., 2009; Hennig & Schuler, 2012; Hernández-Lobato et al., 2014), and upper confidence bound (UCB) (Srinivas et al., 2010)) have been devised to perform BO: They repeatedly select an input for evaluating/querying the black-box function (i.e., until the budget is depleted) that intuitively trades off between sampling where the maximum is likely to be given the current, possibly imprecise belief of the function modeled by a Gaussian process (GP) (i.e., exploitation) vs. improving the GP belief of the function over the entire input domain (i.e., exploration) to guarantee finding the global maximum.
",1. Introduction,[0],[0]
"The rapidly growing affordability and availability of hardware resources (e.g., computer clusters, sensor networks, robot teams/swarms) have motivated the recent development of BO algorithms that can repeatedly select a batch of inputs for querying the black-box function in parallel instead.",1. Introduction,[0],[0]
"Such batch/parallel BO algorithms can be classified into two types: On one extreme, batch BO algorithms like multi-points EI (q-EI) (Chevalier & Ginsbourger, 2013), parallel predictive entropy search (PPES)",1. Introduction,[0],[0]
"(Shah & Ghahramani, 2015), and the parallel knowledge gradient method (q-KG) (Wu & Frazier, 2016) jointly optimize the batch of inputs and hence scale poorly in the batch size.",1. Introduction,[0],[0]
"On the other extreme, greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016) boost the scalability by selecting the inputs of the batch one at a time.",1. Introduction,[0],[0]
"We argue that such a highly suboptimal approach to gain scalability is an overkill: In practice, each function evaluation is often much more computationally and/or economically costly (e.g., hyperparameter tuning for deep learning, drug testing on human subjects), which justifies dedicating more time to obtain better BO performance.",1. Introduction,[0],[0]
"In this paper, we show that it is in fact possible to jointly optimize the batch of inputs and still preserve scalability in the batch size by giving practitioners the flexibility to trade off BO performance for time efficiency.
",1. Introduction,[0],[0]
"To achieve this, we first observe that, interestingly, batch BO can be perceived as a cooperative multi-agent decision making problem whereby each agent optimizes a separate input of the batch while coordinating with the other agents doing likewise.",1. Introduction,[0],[0]
"To the best of our knowledge, this has not been considered in the BO literature.",1. Introduction,[0],[0]
"In particular, if batch BO can be framed as some known class of multi-agent decision making problems, then it can be solved efficiently and scalably by the latter’s state-of-the-art solvers.",1. Introduction,[0],[0]
"The key technical challenge would therefore be to investigate how batch BO can be cast as one of such to exploit its advantage of scalability in the number of agents (hence, batch size) while at the same time theoretically guaranteeing the resulting BO performance.
",1. Introduction,[0],[0]
"To tackle the above challenge, this paper presents a novel distributed batch BO algorithm (Section 3) that, in contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), can jointly optimize a batch of inputs and, unlike the batch BO algorithms (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",1. Introduction,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem (DCOP) in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",1. Introduction,[0],[0]
Our proposed distributed batch GPUCB (DB-GP-UCB) algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,1. Introduction,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DBGP-UCB algorithm via bounds on its cumulative regret.,1. Introduction,[0],[0]
We empirically evaluate the cumulative regret incurred by our DB-GP-UCB algorithm and its scalability in the batch size on synthetic benchmark objective functions and a realworld optimization problem (Section 4).,1. Introduction,[0],[0]
Consider the problem of sequentially optimizing an unknown objective function f : D → R where D ⊂ Rd denotes a domain of d-dimensional input feature vectors.,"2. Problem Statement, Background, and Notations",[0],[0]
"We consider the domain to be discrete as it is known how to generalize results to a continuous, compact domain via suitable discretizations (Srinivas et al., 2010).","2. Problem Statement, Background, and Notations",[0],[0]
"In each iteration t = 1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", T , a batch Dt ⊂ D of inputs is selected for evaluating/querying f to yield a corresponding column vector yDt , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt of noisy observed outputs yx , f(x)+ with i.i.d.","2. Problem Statement, Background, and Notations",[0],[0]
"Gaussian noise ∼ N (0, σ2n) and noise variance σ2n.
Regret.","2. Problem Statement, Background, and Notations",[0],[0]
"Supposing our goal is to get close to the global maximum f(x∗) as rapidly as possible where x∗ , arg maxx∈D f(x), this can be achieved by minimizing a standard batch BO objective such as the batch or full cumulative regret (Contal et al., 2013; Desautels et al., 2014):","2. Problem Statement, Background, and Notations",[0],[0]
The notion of regret intuitively refers to a loss in reward from not knowing x∗ beforehand.,"2. Problem Statement, Background, and Notations",[0],[0]
"Formally, the instantaneous regret incurred by selecting a single input x to evaluate its corresponding f is defined as rx , f(x∗)− f(x).","2. Problem Statement, Background, and Notations",[0],[0]
"Assuming a fixed cost of evaluating f for every possible batch Dt of the same size, the batch and full cumulative regrets are, respectively, defined as sums (over iteration t = 1, . .","2. Problem Statement, Background, and Notations",[0],[0]
.,"2. Problem Statement, Background, and Notations",[0],[0]
", T ) of the smallest instantaneous regret incurred by any input within every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., RT , ∑T t=1 minx∈Dt rx, and of the instantaneous regrets incurred by all inputs of every batch","2. Problem Statement, Background, and Notations",[0],[0]
"Dt, i.e., R′T ,∑T t=1 ∑ x∈Dt rx.","2. Problem Statement, Background, and Notations",[0],[0]
The convergence rate of a batch BO algorithm can then be assessed based on some upper bound on the average regret RT /T or R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
(Section 3) since the currently found maximum after T iterations is no further away from f(x∗) than RT /T,"2. Problem Statement, Background, and Notations",[0],[0]
or R′T /T .,"2. Problem Statement, Background, and Notations",[0],[0]
"It is desirable for a batch BO algorithm to asymptotically achieve no regret, i.e., limT→∞RT /T","2. Problem Statement, Background, and Notations",[0],[0]
=,"2. Problem Statement, Background, and Notations",[0],[0]
0 or limT→∞R′T /T,"2. Problem Statement, Background, and Notations",[0],[0]
"= 0, implying that it will eventually converge to the global maximum.
","2. Problem Statement, Background, and Notations",[0],[0]
Gaussian Processes (GPs).,"2. Problem Statement, Background, and Notations",[0],[0]
"To guarantee no regret (Section 3), the unknown objective function f is modeled as a sample of a GP.","2. Problem Statement, Background, and Notations",[0],[0]
"Let {f(x)}x∈D denote a GP, that is, every finite subset of {f(x)}x∈D follows a multivariate Gaussian distribution (Rasmussen & Williams, 2006).","2. Problem Statement, Background, and Notations",[0],[0]
"Then, the GP is fully specified by its prior mean mx , E[f(x)] and covariance kxx′ , cov[f(x), f(x′)] for all x,x′ ∈ D, which, for notational simplicity (and w.l.o.g.), are assumed to be zero, i.e., mx = 0, and bounded, i.e., kxx′ ≤ 1, respectively.","2. Problem Statement, Background, and Notations",[0],[0]
"Given a column vector yD1:t-1 , (yx) >","2. Problem Statement, Background, and Notations",[0],[0]
"x∈D1:t-1 of noisy observed outputs for some set D1:t−1 , D1 ∪ . . .","2. Problem Statement, Background, and Notations",[0],[0]
"∪ Dt−1 of inputs after t− 1 iterations, a GP model can perform probabilistic regression by providing a predictive distribution p(fDt |yD1:t-1)","2. Problem Statement, Background, and Notations",[0],[0]
"= N (µDt ,ΣDtDt) of the latent outputs fDt , (f(x))","2. Problem Statement, Background, and Notations",[0],[0]
>,"2. Problem Statement, Background, and Notations",[0],[0]
"x∈Dt for any set Dt ⊆ D of inputs selected in iteration t with the following posterior mean vector and covariance matrix:
µDt,KDtD1:t-1(KD1:t-1D1:t-1+σ 2 nI) −1yD1:t-1 , ΣDtDt,KDtDt−KDtD1:t-1(KD1:t-1D1:t-1+σ2nI)−1KD1:t-1Dt (1) where KBB′ , (kxx′)x∈B,x′∈B′ for all B,B′ ⊂ D. GP-UCB and its Greedy Batch Variants.","2. Problem Statement, Background, and Notations",[0],[0]
"Inspired by the UCB algorithm for the multi-armed bandit problem, the GP-UCB algorithm (Srinivas et al., 2010) selects, in each iteration, an input x ∈ D for evaluating/querying f that trades off between sampling close to an expected maximum (i.e., with large posterior mean µ{x}) given the current GP belief of f (i.e., exploitation) vs. that of high predictive un-
certainty (i.e., with large posterior variance Σ{x}{x}) to improve the GP belief of f over D (i.e., exploration), that is, maxx∈D µ{x} + β 1/2 t Σ 1/2
{x}{x} where the parameter βt","2. Problem Statement, Background, and Notations",[0],[0]
"> 0 is set to trade off between exploitation vs. exploration for bounding its cumulative regret.
","2. Problem Statement, Background, and Notations",[0],[0]
"Existing generalizations of GP-UCB such as GP batch UCB (GP-BUCB) (Desautels et al., 2014) and GP-UCB with pure exploration (GP-UCB-PE) (Contal et al., 2013) are greedy batch BO algorithms that select the inputs of the batch one at a time (Section 1).","2. Problem Statement, Background, and Notations",[0],[0]
"Specifically, to avoid selecting the same input multiple times within a batch (hence reducing to GP-UCB), they update the posterior variance (but not the posterior mean) after adding each input to the batch, which can be performed prior to evaluating its corresponding f since the posterior variance is independent of the observed outputs (1).","2. Problem Statement, Background, and Notations",[0],[0]
"They differ in that GPBUCB greedily adds each input to the batch using GP-UCB (without updating the posterior mean) while GP-UCB-PE selects the first input using GP-UCB and each remaining input of the batch by maximizing only the posterior variance (i.e., pure exploration).","2. Problem Statement, Background, and Notations",[0],[0]
"Similarly, a recently proposed UCB-DPP-SAMPLE algorithm (Kathuria et al., 2016) selects the first input using GP-UCB and the remaining inputs by sampling from a determinantal point process (DPP).","2. Problem Statement, Background, and Notations",[0],[0]
"Like GP-BUCB, GP-UCB-PE, and UCB-DPP-SAMPLE, we can theoretically guarantee the convergence rate of our DB-GP-UCB algorithm, which, from a theoretical point of view, signifies an advantage of GP-UCB-based batch BO algorithms over those (e.g., q-EI and PPES) inspired by other acquisition functions such as EI and PES.","2. Problem Statement, Background, and Notations",[0],[0]
"Unlike these greedy batch BO algorithms (Contal et al., 2013; Desautels et al., 2014), our DB-GP-UCB algorithm can jointly optimize the batch of inputs while still preserving scalability in batch size by casting as a DCOP to be described next.
","2. Problem Statement, Background, and Notations",[0],[0]
Distributed Constraint Optimization Problem (DCOP).,"2. Problem Statement, Background, and Notations",[0],[0]
"A DCOP can be defined as a tuple (X ,V,A, h,W) that comprises a set X of input random vectors, a set V of |X | corresponding finite domains (i.e., a separate domain for each random vector), a set A of agents, a function h : X → A assigning each input random vector to an agent responsible for optimizing it, and a setW , {wn}n=1,...,N of non-negative payoff functions such that each function wn defines a constraint over only a subset Xn ⊆ X of input random vectors and represents the joint payoff that the corresponding agents An , {h(x)|x ∈ Xn} ⊆ A achieve.","2. Problem Statement, Background, and Notations",[0],[0]
"Solving a DCOP involves finding the input values ofX that maximize the sum of all functions w1, . . .","2. Problem Statement, Background, and Notations",[0],[0]
", wn (i.e., social welfare maximization), that is, maxX ∑N n=1 wn(Xn).","2. Problem Statement, Background, and Notations",[0],[0]
"To achieve a truly decentralized solution, each agent can only optimize its local input random vector(s) based on the assignment function h but communicate with its neighboring agents: Two agents are considered neighbors if there is a function/constraint involving input random vectors that
the agents have been assigned to optimize.","2. Problem Statement, Background, and Notations",[0],[0]
"Complete and approximation algorithms exist for solving a DCOP; see (Chapman et al., 2011; Leite et al., 2014) for reviews of such algorithms.","2. Problem Statement, Background, and Notations",[0],[0]
"A straightforward generalization of GP-UCB (Srinivas et al., 2010) to jointly optimize a batch of inputs is to simply consider summing the GP-UCB acquisition function over all inputs of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, results in selecting the same input |Dt| times within a batch, hence reducing to GP-UCB, as explained earlier in Section 2.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To resolve this issue but not suffer from the suboptimal behavior of greedy batch BO algorithms such as GP-BUCB (Desautels et al., 2014) and GP-UCB-PE (Contal et al., 2013), we propose a batch variant of GP-UCB that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
maxDt⊂D 1 >µDt + α 1/2 t I[fD;yDt |yD1:t-1 ]1/2 (2)
where the parameter αt > 0, which performs a similar role to that of βt in GP-UCB, is set to trade off between exploitation vs. exploration for bounding its cumulative regret (Theorem 1) and the conditional mutual information1 I[fD;yDt |yD1:t-1 ] can be interpreted as the information gain on f over D (i.e., equivalent to fD , (f(x))>x∈D) by selecting the batch Dt of inputs for evaluating/querying f given the noisy observed outputs yD1:t-1 from the previous t − 1 iterations.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"So, in each iteration t, our proposed batch GP-UCB algorithm (2) selects a batch Dt ⊂ D of inputs for evaluating/querying f that trades off between sampling close to expected maxima (i.e., with a large sum of posterior means 1>µDt = ∑ x∈Dt µ{x}) given the current GP belief of f (i.e., exploitation) vs. that yielding a large information gain I[fD;yDt |yD1:t-1 ] on f over D to improve its GP belief (i.e., exploration).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be derived that I[fD;yDt |yD1:t-1 ] = 0.5 log |I+σ−2n ΣDtDt | (Appendix A), which implies that the exploration term in (2) can be maximized by spreading the batch Dt of inputs far apart to achieve large posterior variance individually and small magnitude of posterior covariance between them to encourage diversity.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Unfortunately, our proposed batch variant of GP-UCB (2) involves evaluating prohibitively many batches of inputs (i.e., exponential in the batch size), hence scaling poorly in the batch size.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"However, we will show in this section that our batch variant of GP-UCB is, interestingly, amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the
1In contrast to the BO algorithm of Contal et al. (2014) that also uses mutual information, our work here considers batch BO by exploiting the correlation information between inputs of a batch in our acquisition function in (2) to encourage diversity.
efficiency of its state-of-the-art solvers for achieving linear time in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Markov Approximation.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The key idea is to design the structure of a matrix ΨDtDt whose log-determinant can closely approximate that of ΨDtDt , I + σ −2 n ΣDtDt residing in the I[fD;yDt |yD1:t-1 ] term in (2) and at the same time be decomposed into a sum of log-determinant terms, each of which is defined by submatrices of ΨDtDt that all depend on only a subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Such a decomposition enables our resulting approximation of (2) to be formulated as a DCOP (Section 2).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"At first glance, our proposed idea may be naively implemented by constructing a sparse block-diagonal matrix ΨDtDt using, say, the N > 1 diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | can be decomposed into a sum of logdeterminants of its diagonal blocks2, each of which depends on only a disjoint subset of the batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This, however, entails an issue similar to that discussed at the beginning of this section of selecting the same |Dt|/N inputs N times within a batch due to the assumption of independence of outputs between different diagonal blocks of ΨDtDt .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"To address this issue, we significantly relax this assumption and show that it is in fact possible to construct a more refined, dense matrix approximation ΨDtDt by exploiting a Markov assumption, which consequently correlates the outputs between all its constituent blocks and is, perhaps surprisingly, still amenable to the decomposition to achieve scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Specifically, evenly partition the batch Dt of inputs into N ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", |Dt|} disjoint subsets Dt1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
",DtN and ΨDtDt (ΨDtDt ) into N ×N square blocks, i.e., ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N (ΨDtDt , [ΨDtnDtn′ ]n,n′=1,...,N ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our first result below derives a decomposition of the logdeterminant of any symmetric positive definite block matrix ΨDtDt into a sum of log-determinant terms, each of which is defined by a separate diagonal block of the Cholesky factor of Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt :
Proposition 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Consider the Cholesky factorization of a symmetric positive definite Ψ
−1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"DtDt , U >U where Cholesky factor U , [Unn′ ]n,n′=1,...,N (i.e., partitioned into N × N square blocks) is an upper triangular block matrix (i.e., Unn′ = 0 for n > n′).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, log |ΨDtDt | =∑N n=1 log |(U>nnUnn)−1|.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix B) utilizes properties of the determinant and that the determinant of an upper triangular block matrix is a product of determinants of its diagonal blocks (i.e., |U | = ∏Nn=1 |Unn|).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 1 reveals a subtle possibility of imposing some structure on the inverse of
2The determinant of a block-diagonal matrix is a product of determinants of its diagonal blocks.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
ΨDtDt such that each diagonal block Unn of its Cholesky factor (and hence each log |(U>nnUnn)−1| term) will depend on only a subset of the batch.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The following result presents one such possibility: Proposition 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"LetB ∈ {1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N−1} be given.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If Ψ−1DtDt is B-block-banded3, then
(U>nnUnn) −1",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= ΨDtnDtn −ΨDtnDBtnΨ −1 DBtnDBtnΨDBtnDtn
(3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N where η , min(n + B,N), DBtn , ⋃η n′=n+1Dtn′ , ΨDtnDBtn , [ΨDtnDtn′ ]n′=n+1,...,η , ΨDBtnDBtn , [ΨDtn′Dtn′′ ]n′,n′′=n+1,...,η, and ΨDBtnDtn , Ψ > DtnDBtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 indicates that if Ψ
−1 DtDt is B-block-banded (Fig. 1b), then
each log |(U>nnUnn)−1| term depends on only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Dt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our next result defines a structure of ΨDtDt in terms of the blocks within the B-block band of ΨDtDt to induce a B-block-banded inverse of ΨDtDt :
Proposition 3.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let
ΨDtnDtn′,  ΨDtnDtn′ if |∆| ≤ B, ΨDtnDBtnΨ −1 DBtnDBtn
ΨDBtnDtn′ if ∆ < −B, ΨDtnDBtn′ Ψ−1DB tn′D B tn′ ΨDB tn′Dtn′ if ∆ > B;
(4) where ∆ , n−n′ for n, n′ = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N (see Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, Ψ −1 DtDt is B-block-banded (see Fig. 1b).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof follows directly from a block-banded matrix result of (Asif & Moura, 2005) (i.e., Theorem 3).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"It can be observed from (4) and Fig. 1 that (a) though Ψ
−1 DtDt is a sparse
B-block-banded matrix, ΨDtDt is a dense matrix approximation for B = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− 1; (b) when B = N − 1 or N = 1, ΨDtDt = ΨDtDt ; and (c) the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) coincide with that of ΨDtDt while each block outside the Bblock band of ΨDtDt (i.e., |n − n′| > B) is fully specified by the blocks within the B-block band of ΨDtDt (i.e., |n − n′| ≤ B) due to its recursive series of |n − n′| − B reduced-rank approximations (Fig. 1a).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Note, however, that the log |(U>nnUnn)−1| terms (3) for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N depend on only the blocks within (and not outside) the B-block band of ΨDtDt (Fig. 1c).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 1.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 3 provides an attractive principled interpretation: Let εx , σ−1n (yx − µ{x}) denote a scaled
3A block matrix P , [Pnn′ ]n,n′=1,...,N (i.e., partitioned into N ×N square blocks) is B-block-banded if any block Pnn′ outside its B-block band (i.e., |n− n′| > B) is 0.
residual incurred by the GP predictive mean (1).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its covariance is then cov[εx, εx′ ] = Ψ{x}{x′}.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In the same spirit as a Gaussian Markov random process, imposing a B-th order Markov property on the residual process {εx}x∈Dt is equivalent to approximating ΨDtDt with ΨDtDt (4) whose inverse isB-block-banded.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In other words, if |n−n′| > B, then {εx}x∈Dtn and {εx}x∈Dtn′ are conditionally independent given {εx}x∈Dt\(Dtn∪Dtn′ ).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
This conditional independence assumption therefore becomes more relaxed with a larger batch Dt.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Proposition 2 demonstrates the importance of such a B-th order Markov assumption (or, equivalently, the sparsity ofB-block-banded Ψ
−1 DtDt ) to achieving
scalability in the batch size.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Regarding the approximation quality of ΨDtDt (4), the following result (see Appendix C for its proof) shows that the Kullback-Leibler (KL) distance of ΨDtDt from ΨDtDt measures an intuitive notion of the approximation error of ΨDtDt being the difference in information gain when relying on our Markov approximation, which can be bounded by some quantity νt:
Proposition 4.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let the KL distance DKL(Ψ, Ψ̃) , 0.5(tr(ΨΨ̃−1)",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"− log |ΨΨ̃−1| − |Dt|) between two symmetric positive definite |Dt| × |Dt| matrices Ψ and Ψ̃ measure the error of approximating Ψ with Ψ̃. Also, let Ĩ[fD;yDt |yD1:t-1 ] , 0.5 log |ΨDtDt | denote the approximated information gain, and C ≥ I[f{x};yDt |yD1:t-1 ] for all x ∈ D and t ∈",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
N.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for all t ∈ N,
DKL(ΨDtDt ,ΨDtDt) = Ĩ[fD;yDt |yD1:t-1 ]− I[fD;yDt |yD1:t-1 ] ≤ (exp(2C)− 1) I[fD;yDt |yD1:t-1 ] , νt .
Proposition 4 implies that the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] is never smaller than the exact information gain I[fD;yDt |yD1:t-1 ] since DKL(ΨDtDt ,ΨDtDt) ≥ 0 with equality when N = 1, in which case ΨDtDt = ΨDtDt (4).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, intuitively, our proposed Markov approximation hallucinates information into ΨDtDt to yield an optimistic estimate of the information gain (by selecting a particular batch), ultimately making our resulting algorithm overconfident in selecting a batch.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This overconfidence is information-theoretically quantified by the approximation error DKL(ΨDtDt ,ΨDtDt) ≤ νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Remark 3.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"The KL distanceDKL(ΨDtDt ,ΨDtDt) of ΨDtDt from ΨDtDt is also the least among all |Dt|×|Dt|matrices with a B-block-banded inverse, as proven in Appendix D.
DCOP Formulation.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"By exploiting the approximated information gain Ĩ[fD;yDt |yD1:t-1 ] (Proposition 4), Proposition 1, (3), and (4), our batch variant of GP-UCB (2) can be reformulated in an approximate sense4 to a distributed batch GP-UCB (DB-GP-UCB) algorithm5 that jointly optimizes a batch of inputs in each iteration t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T according to
Dt , arg max Dt⊂D N∑ n=1 wn(Dtn ∪ DBtn)
wn(Dtn ∪ DBtn) , 1>µDtn+(0.5αt log |ΨDtnDtn|DBtn |) 1/2
(5) with ΨDtnDtn|DBtn ,ΨDtnDtn−ΨDtnDBtnΨ −1 DBtnDBtn ΨDBtnDtn .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"4Note that our acquisition function (5) uses ∑N
n=1(log | · |) 1/2 instead of ( ∑N
n=1 log | · |) 1/2 to enable the decomposition.
5Pseudocode for DB-GP-UCB is provided in Appendix E.
Note that (5) is equivalent to our batch variant of GP-UCB (2) when N = 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
It can also be observed that (5) is naturally formulated as a multi-agent DCOP (Section 2) whereby every agent an ∈,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"A is responsible for optimizing a disjoint subset Dtn of the batch Dt for n = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", N and each function wn defines a constraint over only the subset Dtn ∪ DBtn = ⋃η n′=nDtn′ of the batch Dt and represents the joint payoff that the corresponding agents An , {an′}ηn′=n ⊆ A achieve.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, (5) can be efficiently and scalably solved by the state-of-the-art DCOP algorithms (Chapman et al., 2011; Leite et al., 2014).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For example, the time complexity of an iterative message-passing algorithm called max-sum (Farinelli et al., 2008) scales exponentially in only the largest arity maxn∈{1,...,N} |Dtn ∪ DBtn| =",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"(B+1)|Dt|/N of the functionsw1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", wN .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Given a limited time budget, a practitioner can set a maximum arity of ω for any function wn, after which the number N of functions is adjusted to d(B + 1)|Dt|/ωe so that the time incurred by max-sum to solve the DCOP in (5) is O(|D|ωω3B|Dt|)6 per iteration (i.e., linear in the batch size |Dt| by assuming ω and the Markov orderB to be constants).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In contrast, our batch variant of GP-UCB (2) incurs exponential time in the batch size |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
The max-sum algorithm is also amenable to a distributed implementation on a cluster of parallel machines to boost scalability further.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"If a solution quality guarantee is desired, then a variant of maxsum called bounded max-sum (Rogers et al., 2011) can be used7.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the Markov order B can be varied to trade off between the approximation quality of ΨDtDt (4) and the time efficiency of max-sum in solving the DCOP in (5).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Regret Bounds.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Our main result to follow derives probabilistic bounds on the cumulative regret of DB-GP-UCB:
Theorem 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Let δ ∈ (0, 1) be given, C1 , 4/ log(1 + σ−2n ), γT , maxD1:T⊂D I[fD;yD1:T ], αt , C1|Dt| exp(2C) log(|D|t2π2/(6δ)), and ν̄T , ∑T t=1 νt.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Then, for the batch and full cumulative regrets incurred by our DB-GP-UCB algorithm (5),
RT ≤ 2 ( T |DT |−2αTN(γT + ν̄T ) )1/2 and R′T ≤ 2 (TαTN(γT + ν̄T )) 1/2
hold with probability of at least 1− δ.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"6We assume the use of online sparse GP models (Csató &
Opper, 2002; Hensman et al., 2013; Hoang et al., 2015; 2017; Low et al., 2014b; Xu et al., 2014) that can update the GP predictive/posterior distribution (1) in constant time in each iteration.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"7Bounded max-sum is previously used in (Rogers et al., 2011) to solve a related maximum entropy sampling problem (Shewry & Wynn, 1987) formulated as a DCOP.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"But, the largest arity of any function wn in this DCOP is still the batch size |Dt| and, unlike the focus of our work here, no attempt is made in (Rogers et al., 2011) to reduce it, thus causing max-sum and bounded max-sum to incur exponential time in |Dt|.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our proposed Markov approximation can be applied to this problem to reduce the largest arity of any function wn in this DCOP to again (B + 1)|Dt|/N .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Its proof (Appendix F), when compared to that of GP-UCB (Srinivas et al., 2010) and its greedy batch variants (Contal et al., 2013; Desautels et al., 2014), requires tackling the additional technical challenges associated with jointly optimizing a batch Dt of inputs in each iteration t. Note that the uncertainty sampling based initialization strategy proposed by Desautels et al. (2014) can be employed to replace the √ exp(2C) term (i.e., growing linearly in |Dt|) appearing in our regret bounds by a kernel-dependent constant factor of C ′",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"that is independent of |Dt|; values of C ′ for the most commonly-used kernels are replicated in Table 2 in Appendix G (see section 4.5 in (Desautels et al., 2014) for a more detailed discussion on this issue).
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Table 1 in Appendix G compares the bounds on RT of DB-GP-UCB (5), GP-UCB-PE, GP-BUCB, GP-UCB, and UCB-DPP-SAMPLE.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Compared to the bounds on RT of GP-UCB-PE and UCB-DPP-SAMPLE, our bound includes the additional kernel-dependent factor of C ′, which is similar to GP-BUCB.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"In fact, our regret bound is of the same form as that of GP-BUCB except that our bound incorporates a parameter N of our Markov approximation and an upper bound ν̄T on the cumulative approximation error, both of which vanish for our batch variant of GP-UCB (2):
Corollary 1.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"For our batch variant of GP-UCB (2), the cumulative regrets reduce to RT ≤ 2 ( T |DT |−2αT γT
)1/2 and R′T ≤ 2 (TαT γT ) 1/2.
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Corollary 1 follows directly from Theorem 1 and by noting that for our batch variant (2), N = 1 (since ΨDtDt then trivially reduces to ΨDtDt ) and νt",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"= 0 for t = 1, . . .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
", T .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Finally, the convergence rate of our DB-GP-UCB algorithm is dominated by the growth behavior of γT + ν̄T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"While it is well-known that the bounds on the maximum mutual information γT established for the commonly-used linear, squared exponential, and Matérn kernels in (Srinivas et al., 2010; Kathuria et al., 2016) (i.e., replicated in Table 2 in Appendix G) only grow sublinearly in T , it is not immediately clear how the upper bound ν̄T on the cumulative approximation error behaves.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Our next result reveals that ν̄T in fact only grows sublinearly in T as well: Corollary 2.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"ν̄T ≤ (exp(2C)− 1)γT .
",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
Corollary 2 follows directly from the definitions of νt in Proposition 4 and ν̄T and γT in Theorem 1 and applying the chain rule for mutual information.,3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Since γT grows sublinearly in T for the above-mentioned kernels (Srinivas et al., 2010) and C can be chosen to be independent of T (e.g., C , γ|Dt|−1) (Desautels et al., 2014), it follows from Corollary 2 that ν̄T grows sublinearly in T .",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"As a result, Theorem 1 guarantees sublinear cumulative regrets for the above-mentioned kernels, which implies that our DBGP-UCB algorithm (5) asymptotically achieves no regret, regardless of the degree of our proposed Markov approxi-
mation (i.e., configuration of [N,B]).",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"Thus, our batch variant of GP-UCB (2) achieves no regret as well.",3. Distributed Batch GP-UCB (DB-GP-UCB),[0],[0]
"This section evaluates the cumulative regret incurred by our DB-GP-UCB algorithm (5) and its scalability in the batch size empirically on two synthetic benchmark objective functions such as Branin-Hoo (Lizotte, 2008) and gSobol (González et al., 2016)",4. Experiments and Discussion,[0],[0]
"(Table 3 in Appendix H) and a real-world pH field of Broom’s Barn farm (Webster & Oliver, 2007) (Fig. 3 in Appendix H) spatially distributed over a 1200 m by 680 m region discretized into a 31 × 18 grid of sampling locations.",4. Experiments and Discussion,[0],[0]
"These objective functions and the real-world pH field are each modeled as a sample of a GP whose prior covariance is defined by the widelyused squared exponential kernel kxx′ , σ2s exp(−0.5(x− x′)>Λ−2(x− x′)) where Λ , diag[`1, . . .",4. Experiments and Discussion,[0],[0]
", `d] and σ2s are its length-scale and signal variance hyperparameters, respectively.",4. Experiments and Discussion,[0],[0]
"These hyperparameters together with the noise variance σ2n are learned using maximum likelihood estimation (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"The performance of our DB-GP-UCB algorithm (5) is compared with the state-of-the-art batch BO algorithms such as GP-BUCB (Desautels et al., 2014), GP-UCB-PE (Contal et al., 2013), SM-UCB (Azimi et al., 2010), q-EI (Chevalier & Ginsbourger, 2013), and BBO-LP by plugging in GP-UCB (González et al., 2016), whose implementations8 are publicly available.",4. Experiments and Discussion,[0],[0]
"These batch BO algorithms are evaluated using a performance metric that measures the cumulative regret incurred by a tested algorithm:∑T t=1 f(x
∗)",4. Experiments and Discussion,[0],[0]
"− f(x̃t) where x̃t , arg maxxt∈D µ{xt} (1) is the recommendation of the tested algorithm after t batch evaluations.",4. Experiments and Discussion,[0],[0]
"For each experiment, 5 noisy observations are randomly selected and used for initialization.",4. Experiments and Discussion,[0],[0]
This is independently repeated 64 times and we report the resulting mean cumulative regret incurred by a tested algorithm.,4. Experiments and Discussion,[0],[0]
"All experiments are run on a Linux system with Intelr Xeonr E5-2670 at 2.6GHz with 96 GB memory.
",4. Experiments and Discussion,[0],[0]
"For our experiments, we use a fixed budget of T |DT | = 64 function evaluations and analyze the trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) on the performance of the tested algorithms.",4. Experiments and Discussion,[0],[0]
"This experimental setup represents a practical scenario of costly function evaluations: On one hand, when a function evaluation is computationally costly (i.e., time-consuming), it is more desirable to evaluate f for a larger batch (e.g., |DT | = 16) of inputs in parallel in each iteration t (i.e., if hardware resources permit) to reduce the total time needed (hence smaller T ).",4. Experiments and Discussion,[0],[0]
"On the other hand,
8Details on the used implementations are given in Table 4 in Appendix I. We implemented DB-GP-UCB in MATLAB to exploit the GPML toolbox (Rasmussen & Williams, 2006).
",4. Experiments and Discussion,[0],[0]
"when a function evaluation is economically costly, one may be willing to instead invest more time (hence larger T ) to evaluate f for a smaller batch (e.g., |DT | = 2) of inputs in each iteration t in return for a higher frequency of information and consequently a more adaptive BO to achieve potentially better performance.",4. Experiments and Discussion,[0],[0]
"In some settings, both factors may be equally important, that is, moderate values of |DT | and T are desired.",4. Experiments and Discussion,[0],[0]
"To the best of our knowledge, such a form of empirical analysis does not seem to be available in the batch BO literature.
",4. Experiments and Discussion,[0],[0]
"Fig. 2 shows results9 of the cumulative regret incurred by the tested algorithms to analyze their trade-off between batch size |DT | (i.e., 2, 4, 8, 16) vs. time horizon T (respectively, 32, 16, 8, 4) using a fixed budget of T |DT | = 64 function evaluations for the Branin-Hoo function (left column), gSobol function (middle column), and real-world pH field (right column).",4. Experiments and Discussion,[0],[0]
"Our DB-GP-UCB algorithm uses the configurations of [N,B] =",4. Experiments and Discussion,[0],[0]
"[4, 2], [8, 5], [16, 10] in the experiments with batch size |DT | = 4, 8, 16, respectively; in the case of |DT | = 2, we use our batch variant of GPUCB (2) which is equivalent to DB-GP-UCB whenN = 1.",4. Experiments and Discussion,[0],[0]
"It can be observed that DB-GP-UCB achieves lower cumulative regret than GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP in all experiments (with the only exception being the gSobol function for the smallest batch size of |DT | = 2 where BBO-LP performs slightly better) since DB-GPUCB can jointly optimize a batch of inputs while GPBUCB, GP-UCB-PE, SM-UCB, and BBO-LP are greedy batch algorithms that select the inputs of a batch one at time.",4. Experiments and Discussion,[0],[0]
"Note that as the real-world pH field is not as wellbehaved as the synthetic benchmark functions (see Fig. 3 in Appendix H), the estimate of the Lipschitz constant by BBO-LP is potentially worse, hence likely degrading its performance.",4. Experiments and Discussion,[0],[0]
"Furthermore, DB-GP-UCB can scale to a much larger batch size of 16 than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES (Shah & Ghahramani, 2015) and q-KG (Wu & Frazier, 2016): Results of q-EI are not available for |DT | ≥ 4 as they require a prohibitively huge computational effort to be obtained10 while PPES can only operate with a small batch size of up to 3 for the Branin-Hoo function and up to 4 for other functions, as reported in (Shah & Ghahramani, 2015), and q-KG can only operate with a small batch size of 4 for all tested functions (including the Branin-Hoo function and four others), as reported in (Wu & Frazier, 2016).",4. Experiments and Discussion,[0],[0]
"The scalability of DB-GP-UCB is attributed to our proposed Markov approximation of our
9Error bars are omitted in Fig. 2 to preserve the readability of the graphs.",4. Experiments and Discussion,[0],[0]
"A replication of the graphs in Fig. 2 including standard error bars is provided in Appendix K.
10In the experiments of González et al. (2016), q-EI can reach a batch size of up to 10 but performs much worse than GP-BUCB, which is likely due to a considerable downsampling of possible batches available for selection in each iteration.
batch variant of GP-UCB (2) (Section 3), which can then be naturally formulated as a multi-agent DCOP (5) in order to fully exploit the efficiency of one of its state-of-the-art solvers called max-sum (Farinelli et al., 2008).",4. Experiments and Discussion,[0],[0]
"In the experiments with the largest batch size of |DT | = 16, we have reduced the number of iterations in max-sum to less than 5 without waiting for convergence to preserve the efficiency of DB-GP-UCB, thus sacrificing its BO performance.",4. Experiments and Discussion,[0],[0]
"Nevertheless, DB-GP-UCB can still outperform the other tested
batch BO algorithms.
",4. Experiments and Discussion,[0],[0]
We have also investigated and analyzed the trade-off between approximation quality and time efficiency of our DPGP-UCB algorithm and reported the results in Appendix J due to lack of space.,4. Experiments and Discussion,[0],[0]
"To summarize, it can be observed from our results that the approximation quality improves near-linearly with an increasing Markov order B at the expense of higher computational cost (i.e., exponential in B).",4. Experiments and Discussion,[0],[0]
"This paper develops a novel distributed batch GP-UCB (DB-GP-UCB) algorithm for performing batch BO of highly complex, costly-to-evaluate, noisy black-box objective functions.",5. Conclusion,[0],[0]
"In contrast to greedy batch BO algorithms (Azimi et al., 2010; Contal et al., 2013; Desautels et al., 2014; González et al., 2016), our DB-GP-UCB algorithm can jointly optimize a batch of inputs and, unlike (Chevalier & Ginsbourger, 2013; Shah & Ghahramani, 2015; Wu & Frazier, 2016), still preserve scalability in the batch size.",5. Conclusion,[0],[0]
"To realize this, we generalize GP-UCB (Srinivas et al., 2010) to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent DCOP in order to fully exploit the efficiency of its state-of-the-art solvers such as max-sum (Farinelli et al., 2008; Rogers et al., 2011) for achieving linear time in the batch size.",5. Conclusion,[0],[0]
Our proposed DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,5. Conclusion,[0],[0]
We provide a theoretical guarantee for the convergence rate of our DB-GP-UCB algorithm via bounds on its cumulative regret.,5. Conclusion,[0],[0]
"Empirical evaluation on synthetic benchmark objective functions and a real-world pH field shows that our DB-GP-UCB algorithm can achieve lower cumulative regret than the greedy batch BO algorithms such as GP-BUCB, GP-UCB-PE, SM-UCB, and BBO-LP, and scale to larger batch sizes than the other batch BO algorithms that also jointly optimize the batch of inputs, which include q-EI, PPES, and q-KG.",5. Conclusion,[0],[0]
"For future work, we plan to generalize DB-GP-UCB (a) to the nonmyopic context by appealing to existing literature on nonmyopic BO (Ling et al., 2016) and active learning (Cao et al., 2013; Hoang et al., 2014a;b; Low et al., 2008; 2009; 2011; 2014a) as well as (b) to be performed by a multi-robot team to find hotspots in environmental sensing/monitoring by seeking inspiration from existing literature on multi-robot active sensing/learning (Chen et al., 2012; 2013b; 2015; Low et al., 2012; Ouyang et al., 2014).",5. Conclusion,[0],[0]
"For applications with a huge budget of function evaluations, we like to couple DB-GP-UCB with the use of parallel/distributed sparse GP models (Chen et al., 2013a; Hoang et al., 2016; Low et al., 2015) to represent the belief of the unknown objective function efficiently.",5. Conclusion,[0],[0]
"This research is supported by Singapore Ministry of Education Academic Research Fund Tier 2, MOE2016-T2-2-156.",Acknowledgements,[0],[0]
Erik A. Daxberger would like to thank Volker Tresp for his advice throughout this research project.,Acknowledgements,[0],[0]
"This paper presents a novel distributed batch Gaussian process upper confidence bound (DB-GP-UCB) algorithm for performing batch Bayesian optimization (BO) of highly complex, costly-to-evaluate black-box objective functions.",abstractText,[0],[0]
"In contrast to existing batch BO algorithms, DBGP-UCB can jointly optimize a batch of inputs (as opposed to selecting the inputs of a batch one at a time) while still preserving scalability in the batch size.",abstractText,[0],[0]
"To realize this, we generalize GP-UCB to a new batch variant amenable to a Markov approximation, which can then be naturally formulated as a multi-agent distributed constraint optimization problem in order to fully exploit the efficiency of its state-of-the-art solvers for achieving linear time in the batch size.",abstractText,[0],[0]
Our DB-GP-UCB algorithm offers practitioners the flexibility to trade off between the approximation quality and time efficiency by varying the Markov order.,abstractText,[0],[0]
We provide a theoretical guarantee for the convergence rate of DB-GP-UCB via bounds on its cumulative regret.,abstractText,[0],[0]
Empirical evaluation on synthetic benchmark objective functions and a real-world optimization problem shows that DB-GP-UCB outperforms the stateof-the-art batch BO algorithms.,abstractText,[0],[0]
Distributed Batch Gaussian Process Optimization,title,[0],[0]
"Clustering is a fundamental problem in the analysis and understanding of data, and is used widely in different areas of science.",1. Introduction,[0],[0]
The broad goal of clustering is to divide a (typically large) dataset into groups that such that data points within a group are “similar” to one another.,1. Introduction,[0],[0]
"In most applications, there is a measure of similarity between any two objects, which typically forms a metric.",1. Introduction,[0],[0]
"The problem can be formalized in many different ways, depending on the properties desired of the obtained clustering.",1. Introduction,[0],[0]
"While a “perfect” formulation may not exist (see (Kleinberg, 2002)),
*Equal contribution 1School of Computing, University of Utah.",1. Introduction,[0],[0]
Correspondence to: Aditya Bhaskara,1. Introduction,[0],[0]
<bhaskaraaditya@gmail.com,1. Introduction,[0],[0]
">, Maheshakya Wijewardena <pmaheshakya4@gmail.com",1. Introduction,[0],[0]
">.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"many formulations have been very successful in applications, including k-means, k-median, k-center, and various notions of hierarchical clustering (see (Hastie et al., 2009; Dasgupta, 2016) and references there-in).
",1. Introduction,[0],[0]
"In this paper, we focus on k-means clustering, in which the input is a set of n points in Euclidean space.",1. Introduction,[0],[0]
"Here the goal is to partition the points into k clusters, so as to minimize the sum of squared distances from the points to the respective cluster centers (see Section 2 for a formal definition).",1. Introduction,[0],[0]
k-means is one of the most well-studied clustering variants.,1. Introduction,[0],[0]
"Lloyd’s algorithm (Lloyd, 1982), developed over 35 years ago, has been extremely successful in practice (the success has been ‘explained’ in many recent works; see (Arthur et al., 2011; Awasthi & Sheffet, 2012; Kumar & Kannan, 2010) and references there-in).",1. Introduction,[0],[0]
"Despite the success, Lloyd’s algorithm can have an arbitrarily bad approximation ratio in the worst case.",1. Introduction,[0],[0]
"To address this, constant factor approximation algorithms have been developed, which are more involved but have worst case guarantees (see (Kanungo et al., 2004) and (Ahmadian et al., 2017)).",1. Introduction,[0],[0]
"In another direction, works by (Ostrovsky et al., 2006; Jaiswal et al., 2012; Arthur & Vassilvitskii, 2007) have shown how to obtain simple bicriteria approximation algorithms for k-means.",1. Introduction,[0],[0]
"(Arthur & Vassilvitskii, 2007) also proposed a variant of Lloyd’s algorithm, termed “k-means++”, which also comes with a theoretical approximation factor guarantee of O(log k) approximation.
",1. Introduction,[0],[0]
All the algorithms above assume that data fits in a single machine.,1. Introduction,[0],[0]
"However, with the ubiquity of large data sets, there has been a lot of interest in distributed algorithms where data is spread across several machines.",1. Introduction,[0],[0]
"The goal is to use available distributed models of computation to design algorithms that can (a) work with machines having access only to their local data set, (b) use small amount of memory and only a few “rounds” of communication, and (c) have approximation guarantees for the solution they output.
",1. Introduction,[0],[0]
"For k-means and related objectives, the paradigm of iterative ‘data reduction’ has been remarkably successful.",1. Introduction,[0],[0]
"The main idea is that in each round, a machine chooses a small subset of its input, and only this subset is carried to the next round.",1. Introduction,[0],[0]
"Thus the total number of points reduces by a significant factor in every round, and this results in a small number of rounds overall.",1. Introduction,[0],[0]
"Such an algorithm can be implemented
efficiently in the MapReduce framework, introduced by (Dean & Ghemawat, 2004), and formalized by (Karloff et al., 2010)).",1. Introduction,[0],[0]
"(Ene et al., 2011) gave one of the first such implementations (for the k-median problem), and showed theoretical guarantees.",1. Introduction,[0],[0]
"This line of work has subsequently been developed in (Kumar et al., 2013; Balcan et al., 2013a; Awasthi et al., 2017).",1. Introduction,[0],[0]
"The last work also gives a summary of the known results in this space.
",1. Introduction,[0],[0]
The high level ideas used in these works are similar to those used in streaming algorithms for clustering.,1. Introduction,[0],[0]
"The literature here is very rich; one of the earliest works is that of (Charikar et al., 1997), for the k-center problem.",1. Introduction,[0],[0]
"The work of (Guha et al., 2001) introduced many ideas crucial to the distributed algorithms mentioned above.",1. Introduction,[0],[0]
"Indeed, all of these algorithms can be viewed as implicitly constructing coresets (or summaries) for the underlying clustering problem.",1. Introduction,[0],[0]
"We refer to the works of (Agarwal et al., 2004; 2012; Balcan et al., 2013b; Indyk et al., 2014) for more on this connection.
",1. Introduction,[0],[0]
Motivation for our work.,1. Introduction,[0],[0]
"While iterative data reduction is powerful, it has a key bottleneck: in order to have approximation guarantees, machines always need to store > k data points.",1. Introduction,[0],[0]
"Indeed, all the algorithms we are aware of require a memory of kn if they are to useO(1/ ) rounds of MAPREDUCE computation.1 The high level reason for this is that if a machine sees k points that are all very far from one another, it needs to keep all of them, or else we might lose all the information about one of the clusters, and this could lead to a large objective value.",1. Introduction,[0],[0]
"This is also the reason each machine needs to communicate ≥ k points to the others (such a lower bound was proved formally in (Chen et al., 2016), as we will discuss later).",1. Introduction,[0],[0]
"The natural question is thus to ask: can we partition the data across machines so that different machines work in different “regions of space”, and thus focus on finding different clusters?",1. Introduction,[0],[0]
"This would result in a smaller space requirement per machine, and lesser communication between machines.",1. Introduction,[0],[0]
"Our main result is to show that this is possible, as long as we have a rough estimate of the optimum objective value (up to an arbitrary polynomial factor).",1. Introduction,[0],[0]
"We give an algorithm based on a variant of locality sensitive hashing, and prove that this yields a bi-criteria approximation guarantee.
",1. Introduction,[0],[0]
"Locality sensitive hashing was introduced in the seminal work of (Indyk & Motwani, 1998), which gave an efficient algorithm for nearest neighbor search in high dimensional space.",1. Introduction,[0],[0]
"The idea has found several applications in machine learning and data science, ranging from the early applications of similarity search to the speeding up of neural networks (Spring & Shrivastava, 2017).",1. Introduction,[0],[0]
"(Datar et al., 2004)
1All the algorithms mentioned above can be naturally implemented in the MAPREDUCE framework.
generalized the original result of (Indyk & Motwani, 1998) to the case of `p norms, and (Andoni & Indyk, 2006) gave an improved analysis.",1. Introduction,[0],[0]
"Extensions of LSH are still an active area of research, but a discussion is beyond the scope of this paper.",1. Introduction,[0],[0]
Our contribution here is to understand the behavior of clusters of points under LSH and its variants.,1. Introduction,[0],[0]
Our focus in the paper will be on the k-means objective (defined formally in Section 2).,1.1. Our results,[0],[0]
"The data set is assumed to be a collection of points in a Euclidean space Rd for some d, and distance refers to the `2 distance.
",1.1. Our results,[0],[0]
"Our first contribution is an analysis of “product LSH” (PLSH), a hash obtained by concatenating independent copies of an LSH.",1.1. Our results,[0],[0]
"For each LSH, we consider the implementation of (Andoni & Indyk, 2006).
",1.1. Our results,[0],[0]
Informal theorem 1 (See Lemmas 1 and 2).,1.1. Our results,[0],[0]
Let C be any cluster of points with diameter σ.,1.1. Our results,[0],[0]
"Then PLSH with appropriate parameters yields the same hash for all the points in C, with probability ≥ 3/4.",1.1. Our results,[0],[0]
"Furthermore, for any two points u, v such that ‖u",1.1. Our results,[0],[0]
"− v‖ ≥ α · σ, where α ≈ log n log log n, the probability that u and v have the same hash is < 1/n2.
",1.1. Our results,[0],[0]
"Thus, PLSH has a “cluster preserving” property.",1.1. Our results,[0],[0]
"We show the above by extending the analyses of (Indyk & Motwani, 1998) and (Andoni & Indyk, 2006).",1.1. Our results,[0],[0]
"Then, we use this observation to give a simple bi-criteria approximation algorithm for k-means clustering.",1.1. Our results,[0],[0]
(A bi-criteria algorithm is one that is allowed to output a slightly larger number of centers; see Section 2.),1.1. Our results,[0],[0]
"We assume knowledge of k, as well as a very rough estimate of the objective value.",1.1. Our results,[0],[0]
"The algorithm returns a polylogarithmically larger number of clusters, while obtaining a polylogarithmic factor approximation.",1.1. Our results,[0],[0]
We refer to Theorem 2 for the statement.,1.1. Our results,[0],[0]
"As we note below, if s > k polylog(()n), then we can avoid violating the bound on the number of clusters (and obtain a “true” guarantee as opposed to a bi-criteria one).
",1.1. Our results,[0],[0]
"The algorithm can be implemented in a distributed manner, specifically in the MAPREDUCE model, with dlogs ne+ 2 rounds, using machines of memory s (when we say memory s, we mean that each machine can store at most s of the points.",1.1. Our results,[0],[0]
"This will be roughly the same as measuring s in bytes, as we see in Section 2.",1.1. Our results,[0],[0]
The formal result is stated in Theorem 3.,1.1. Our results,[0],[0]
"We highlight that the distributed algorithm works for any s ≥ ω(log n), even s k (in which case the standard reduce-and-merge framework has no non-trivial guarantees).
",1.1. Our results,[0],[0]
"Finally, we prove that for any MapReduce algorithm that uses poly(n) machines of space s, the number of rounds necessary to obtain any non-trivial approximation to k-means is at least dlogs ne.",1.1. Our results,[0],[0]
"Thus the ‘round/memory tradeoff’ we
obtain is nearly optimal.",1.1. Our results,[0],[0]
"This is based on ideas from the recent remarkable result of (Roughgarden et al., 2016).",1.1. Our results,[0],[0]
(See Theorem 4.),1.1. Our results,[0],[0]
Going beyond communication lower bounds.,"1.2. Discussion, extensions and limitations",[0],[0]
"The recent result of (Chen et al., 2016) shows lower bounds on the total amount of communication necessary for distributed clustering.","1.2. Discussion, extensions and limitations",[0],[0]
"They show that for a worst-case partition of points across machines, Ω(Mk) bits are necessary, where M is the number of machines.","1.2. Discussion, extensions and limitations",[0],[0]
"Our result in Section 4.2 implies that if points have been partitioned across machines according to PLSH hashes, we can bypass this lower bound.
","1.2. Discussion, extensions and limitations",[0],[0]
Round lower bound.,"1.2. Discussion, extensions and limitations",[0],[0]
"In light of Theorem 4, one way to interpret our algorithmic result is as saying that as far as obtaining polylogarithmic bi-criteria approximations go, clustering is essentially as easy as “aggregation” (i.e., summing a collection of n numbers – which also has the same logs n upper and lower bounds).
","1.2. Discussion, extensions and limitations",[0],[0]
Precisely k clusters.,"1.2. Discussion, extensions and limitations",[0],[0]
"Theorem 3 gives only a bi-criteria guarantee, so it is natural to ask if we can obtain any guarantee when we desire precisely k centers.","1.2. Discussion, extensions and limitations",[0],[0]
"In the case when s ≥ k log2 n, we can apply known results to obtain this.","1.2. Discussion, extensions and limitations",[0],[0]
"(Guha et al., 2001) showed (in our notation) that:
Theorem 1.","1.2. Discussion, extensions and limitations",[0],[0]
"Let U be a set of points, and let S be a set of centers with the property that ∑ u∈U d(u, S)
2 ≤ γ · OPT, where OPT is the optimum k-means objective value on U .","1.2. Discussion, extensions and limitations",[0],[0]
"Let g : U 7→ S map every u ∈ U to its closest point in S, breaking ties arbitrarily.","1.2. Discussion, extensions and limitations",[0],[0]
"Now, consider a new weighted instance I of k-means where we have points in S, with weight of v ∈ S being |g−1(v)|.","1.2. Discussion, extensions and limitations",[0],[0]
"Then, any set of centers that ρ-approximate the optimum objective for I give a (4γ+ 2ρ) approximation to the original instance (given by U ).
","1.2. Discussion, extensions and limitations",[0],[0]
"Thus, if s ≥ k log2 n, we can aggregate the output of our bi-criteria algorithm onto one machine, and solve k-means approximately.","1.2. Discussion, extensions and limitations",[0],[0]
"In essence, we are using the output of our algorithm as a coreset for k-means.","1.2. Discussion, extensions and limitations",[0],[0]
"We demonstrate this in our experimental results.
","1.2. Discussion, extensions and limitations",[0],[0]
Balanced clustering.,"1.2. Discussion, extensions and limitations",[0],[0]
A common constraint for clustering algorithms is that of the clusters being balanced.,"1.2. Discussion, extensions and limitations",[0],[0]
This is often captured by requiring an upper bound on the size of a cluster.,"1.2. Discussion, extensions and limitations",[0],[0]
"(Bateni et al., 2014) showed that balanced clustering can also be solved in a distributed setting.","1.2. Discussion, extensions and limitations",[0],[0]
"Specifically, they showed that any bi-criteria algorithm for k-means can be used to solve the balanced clustering problem, via a result analogous to 1.","1.2. Discussion, extensions and limitations",[0],[0]
"In our context, this implies that if s > k log2 n, our method also gives a distributed algorithm for balanced clustering with a k-means objective.
Limitations and lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
"There are two key limita-
tions to our result.","1.2. Discussion, extensions and limitations",[0],[0]
"First, the polylogarithmic approximation factor in the approximation ratio seems difficult to avoid (although our experiments show that the guarantees are very pessimistic).","1.2. Discussion, extensions and limitations",[0],[0]
"In our argument, it arises as a by-product of being able to detect very small clusters.","1.2. Discussion, extensions and limitations",[0],[0]
"This is in contrast with single machine algorithms (e.g., (Kanungo et al., 2004; Ahmadian et al., 2017)) and the prior work in MapReduce algorithms, (Ene et al., 2011), which give constant factor approximations.","1.2. Discussion, extensions and limitations",[0],[0]
Another restriction is that our algorithms assume a Euclidean setting for the points.,"1.2. Discussion, extensions and limitations",[0],[0]
"The algorithms of (Ene et al., 2011) and related works can handle the case of arbitrary metric spaces.","1.2. Discussion, extensions and limitations",[0],[0]
The bottleneck here is the lack of locality sensitive hashing for such spaces.,"1.2. Discussion, extensions and limitations",[0],[0]
"A very interesting open problem is to develop new methods in this case, or prove stronger lower bounds.","1.2. Discussion, extensions and limitations",[0],[0]
We now introduce some notation and definitions that will be used for the rest of the paper.,2. Notation and Preliminaries,[0],[0]
We will denote by U the set of points in the input.,2. Notation and Preliminaries,[0],[0]
We denote n = |U |.,2. Notation and Preliminaries,[0],[0]
"All of our algorithms are for the Euclidean setting, where the points in U are in Rd, and the distance between x, y ∈ U is the `2 norm ‖x− y‖2 = √∑ i(xi",2. Notation and Preliminaries,[0],[0]
"− yi)2.
",2. Notation and Preliminaries,[0],[0]
"A k-clustering of the points U is a partition C of U into subsets C1, C2, . . .",2. Notation and Preliminaries,[0],[0]
", Ck.",2. Notation and Preliminaries,[0],[0]
The centroid of a cluster Ci is the point µi := 1|Ci| ∑ u∈Ci u.,2. Notation and Preliminaries,[0],[0]
The k-means objective for the clustering C is now defined as∑ i∈[k],2. Notation and Preliminaries,[0],[0]
∑ u∈Ci ‖u− µi‖22.,2. Notation and Preliminaries,[0],[0]
"(1)
The problem of k-means clustering is to find C that minimizes the objective defined above.",2. Notation and Preliminaries,[0],[0]
The minimum objective value will be denoted by OPT(U).,2. Notation and Preliminaries,[0],[0]
"(When the U is clear from context, we simply write OPT.)",2. Notation and Preliminaries,[0],[0]
A ρ-approximation algorithm for k-means clustering is a polynomial time algorithm that outputs a clustering C′ whose objective value is at most ρ ·OPT(U).,2. Notation and Preliminaries,[0],[0]
We will be interested in ρ being a constant or polylog(n).,2. Notation and Preliminaries,[0],[0]
"A (ρ, β) bi-criteria approximation (where β ≥ 1) is an efficient algorithm that outputs a clustering C′ that has at most βk clusters and has an objective value at most ρ ·OPT(U).",2. Notation and Preliminaries,[0],[0]
"Note that the optimum still has k clusters.
",2. Notation and Preliminaries,[0],[0]
Note on the dimension d.,2. Notation and Preliminaries,[0],[0]
We assume that d = O(log n).,2. Notation and Preliminaries,[0],[0]
"This is without loss of generality, because we may assume that we have pre-processed the data by applying JohnsonLindenstrauss transform.",2. Notation and Preliminaries,[0],[0]
"As the JL transform preserves all pairwise `2 distances (Johnson & Lindenstrauss, 1984; Indyk & Motwani, 1998), clustering in the transformed space gives a (1 + ) approximation to clustering in the original one.",2. Notation and Preliminaries,[0],[0]
"Furthermore, the transform can be applied in parallel, individually to each point.",2. Notation and Preliminaries,[0],[0]
"Thus we henceforth assume that the space required to store a point is O(log n).
MapReduce model.",2. Notation and Preliminaries,[0],[0]
"To illustrate our ideas, we use the well-studied MapReduce model (Dean & Ghemawat, 2004; Karloff et al., 2010).",2. Notation and Preliminaries,[0],[0]
The details of the map and reduce operations are not important for our purpose.,2. Notation and Preliminaries,[0],[0]
We will view it as a model of computation that proceeds in levels.,2. Notation and Preliminaries,[0],[0]
"At each level, we have M machines that can perform computation on their local data (the input is distributed arbitrarily among machines in the first layer).",2. Notation and Preliminaries,[0],[0]
"Once all the machines are done with computation, they send information to machines in the next layer.",2. Notation and Preliminaries,[0],[0]
The information received by a machine acts as its local data for the next round of computation.,2. Notation and Preliminaries,[0],[0]
"We assume that each machine has a memory of s.
Constants.",2. Notation and Preliminaries,[0],[0]
"For the sake of easier exposition, we do not attempt to optimize the constants in our bounds.",2. Notation and Preliminaries,[0],[0]
The key to our distributed algorithm is a two step hashing scheme.,3. Two Step Hashing,[0],[0]
"The first step is a ‘product’ of locality sensitive hashes (LSH), and the second is a random hash that maps the tuples obtained from the product-LSH to a bin with a label in the range 1, . . .",3. Two Step Hashing,[0],[0]
", Lk, for an appropriate constant L.",3. Two Step Hashing,[0],[0]
We begin with a short discussion of LSH.,3.1. Product LSH,[0],[0]
"We follow the presentation of (Andoni & Indyk, 2006).2
Suppose we have a collection of points U in Rd.
",3.1. Product LSH,[0],[0]
Locality sensitive hashing (LSH).,3.1. Product LSH,[0],[0]
"Let t, w be parameters.",3.1. Product LSH,[0],[0]
"LSHt,w is a procedure that takes a u ∈ U , and produces a (t + 1)-tuple of integers.",3.1. Product LSH,[0],[0]
"The hash uses as parameters a matrix A of dimensions t × d, whose entries are i.i.d. N",3.1. Product LSH,[0],[0]
"(0, 1) Gaussians, and a collection of shift vectors S = {s1, . . .",3.1. Product LSH,[0],[0]
", sU}, where si is picked uniformly from [0, 4w]t.",3.1. Product LSH,[0],[0]
"The shifts are used to generate a collection of shifted grids Gti := G
t + si, where Gt is the integer grid Zt, scaled by 4w.",3.1. Product LSH,[0],[0]
"Now to compute the hash of a point u, first its projection to Rt is computed by u′ = Au.",3.1. Product LSH,[0],[0]
"Next, one searches for the smallest index",3.1. Product LSH,[0],[0]
i ∈,3.1. Product LSH,[0],[0]
"[U ] for which the ball B(u′, w) contains a point of the shifted grid Gti.",3.1. Product LSH,[0],[0]
"(Alternately one could imagine radius-w balls around the points in the shifted grids, and we look for the smallest i for which the point u′ is contained in one such ball.)",3.1. Product LSH,[0],[0]
"The hash of the point is then (i, x1, . . .",3.1. Product LSH,[0],[0]
", xt), where i is the index as above, and (x1, . . .",3.1. Product LSH,[0],[0]
", xt) are the integer coordinates corresponding to the grid point in Gti that is at distance ≤ w from u′.
(Andoni & Indyk, 2006) show that to cover all of Rt (and thus to have a well-defined hash for every point), the number of shifts that suffice is 2O(t log t).",3.1. Product LSH,[0],[0]
"Consequently, this is also
2The earlier LSH schemes of (Indyk & Motwani, 1998) and (Datar et al., 2004) can also be used; however, they give a weaker approximation factor.
",3.1. Product LSH,[0],[0]
"the time required to compute hash for a point, as we may need to go through all the shifts.",3.1. Product LSH,[0],[0]
"In our setting, we will choose t = o(log n/ log log n), and thus the time needed to hash is no(1).
",3.1. Product LSH,[0],[0]
Product LSH (PLSH).,3.1. Product LSH,[0],[0]
"Given an integer `, the product LSH PLSHt,w,` is a hashing scheme that maps a point u to a concatenation of ` independent copies of LSHt,w; it thus outputs an `(t+ 1)-tuple of integers.
",3.1. Product LSH,[0],[0]
We show the following properties of PLSH.,3.1. Product LSH,[0],[0]
"In what follows, let σ be a parameter.",3.1. Product LSH,[0],[0]
"Let
w = 8σ(log n)3/2; t = log n
(log log n)2 ; ` = 32(log log n)2.
",3.1. Product LSH,[0],[0]
"(2)
Lemma 1. Suppose C ⊆ U has diameter ≤ σ.",3.1. Product LSH,[0],[0]
"Then with probability at least 3/4, PLSHt,w,` maps all the points in C to the same hash value.",3.1. Product LSH,[0],[0]
Lemma 2.,3.1. Product LSH,[0],[0]
"Let u, v be points that are at distance≥ 4w/",3.1. Product LSH,[0],[0]
√ t (= O(log n log log n) · σ).,3.1. Product LSH,[0],[0]
"Then the probability that they have the same hash value is < 1n4 .
",3.1. Product LSH,[0],[0]
Proof of Lemma 1.,3.1. Product LSH,[0],[0]
Let C ′ = {,3.1. Product LSH,[0],[0]
Ax : x ∈ C}.,3.1. Product LSH,[0],[0]
"First, we claim that the diameter ofC ′ is at most 4σ √ t+ log n, w.h.p.",3.1. Product LSH,[0],[0]
over the choice of A.,3.1. Product LSH,[0],[0]
"This is because for any x, y ∈ C, the quantity ‖A(x− y)‖22/‖x− y‖22 is distributed as a χ2 with t degrees of freedom.",3.1. Product LSH,[0],[0]
"It is known (e.g., (Laurent & Massart, 2000), Lemma 1) that the tail of a chi-square statistic Y with t degrees of freedom satisfies: for any z > 0, Pr[Y ≥ t + 2 √ tz + 2z] ≤ e−z .",3.1. Product LSH,[0],[0]
"Setting z = 4 log n, and using 2 √ tz ≤ t+ z, we get that Pr[Y ≥ 16(t+ log n)]",3.1. Product LSH,[0],[0]
< 1/n4.,3.1. Product LSH,[0],[0]
"Thus by taking union bound over all pairs of points x, y, we have that with probability ≥ 1− 1n2 , the diameter of C
′ is ≤ 4σ √ t+ log n.
Conditioned on this, let us calculate the probability that the points in C all have the same hash value.",3.1. Product LSH,[0],[0]
"(The conditioning does not introduce any dependencies, as the above argument depended only on the choice of A, while the next step will depend on the choice of the shifts.)",3.1. Product LSH,[0],[0]
"Now, consider a ball B∗ ⊆ Rt of radius r′ := 4σ √ t+ log n that contains C ′",3.1. Product LSH,[0],[0]
"(as C ′ is of small diameter, such a ball exists).
",3.1. Product LSH,[0],[0]
"Before analyzing the probability of interest, let us understand when a shifted grid Gti contains a point that has distance ≤",3.1. Product LSH,[0],[0]
w to a given point x.,3.1. Product LSH,[0],[0]
This is equivalent to saying that (x−si) is w-close to a lattice point inGt.,3.1. Product LSH,[0],[0]
"This happens iff si is in the ball B(x,w), where the ball has been reduced modulo",3.1. Product LSH,[0],[0]
"[0, 4w]t (see Figure 1).
",3.1. Product LSH,[0],[0]
"Now, we can see how it could happen that some x ∈ B∗ is w-close to a lattice point in Gti but the entirety of B
∗ does not have this property.",3.1. Product LSH,[0],[0]
"Geometrically, the bad choices of si are shown in Figure 1 (before reducing moulo",3.1. Product LSH,[0],[0]
"[0, 4w]t).",3.1. Product LSH,[0],[0]
"Thus, we have that the probability that all points in B∗ are w-close to a lattice point in Gti conditioned on there existing
a point x ∈ B∗ that is close to a lattice point is at least
p1 := (w − r′)t
(w + r′)t =
( 1− 2r ′
w + r′
)t .
",3.1. Product LSH,[0],[0]
"Thus p1 is a lower bound on a single LSH giving the same hash value for all the points inC. Repeating this ` times, and plugging in our choice of values for r′, w, t, `, the desired claim follows.
",3.1. Product LSH,[0],[0]
"Next, we get to the proof of Lemma 2.
",3.1. Product LSH,[0],[0]
Proof.,3.1. Product LSH,[0],[0]
"Let u, v be points as in the statement of the lemma.",3.1. Product LSH,[0],[0]
We show that the probability that ‖A(u− v)‖ ≤ 2w in all the ` hashes is < 1/n4.,3.1. Product LSH,[0],[0]
"This clearly implies what we need, because if in even one LSH we haveAu",3.1. Product LSH,[0],[0]
"andAv being> 2w away, they cannot have the same PLSH.3
Now, for a randomA, the quantity ‖A(u−v)‖22/‖u−v‖22 is distributed as a χ2 distribution with t degrees of freedom (as we saw earlier), and thus using the lower tail from (Laurent & Massart, 2000), we get that for any z > 0, for such a random variable Y , we have Pr[Y ≤ t",3.1. Product LSH,[0],[0]
− 2 √ tz] < e−z .,3.1. Product LSH,[0],[0]
Thus Pr[Y ≤ (1− 1√ 2 )t] ≤ e−t/8.,3.1. Product LSH,[0],[0]
"Now, for our choice of parameters, we have 4w2/‖u",3.1. Product LSH,[0],[0]
− v‖22 ≤ t/4 <,3.1. Product LSH,[0],[0]
"(1 − 1√2 )t, and thus the probability that u and v have the same PLSH is upper bounded by e−t`/8 = 1/n4, as desired.",3.1. Product LSH,[0],[0]
We have shown that the probability that a cluster of diameter ≤ σ hashes to precisely one tuple (for appropriate parameters) is ≥ 3/4.,3.2. Number of tuples for a cluster,[0],[0]
"We now show something slightly stronger (as we will need it later).
",3.2. Number of tuples for a cluster,[0],[0]
Lemma 3.,3.2. Number of tuples for a cluster,[0],[0]
"Let C be a cluster of diameter σ, and let t, w, ` be set as in Eq. (2).",3.2. Number of tuples for a cluster,[0],[0]
"The expected number of distinct tuples
3This reasoning allows us to get a bound slightly better than (Andoni & Indyk, 2006).
for points in C (produced by PLSHt,w,`) is O(1).",3.2. Number of tuples for a cluster,[0],[0]
The PLSH maps each point u ∈ U to an `(t + 1) tuple of integers.,3.3. Second step of hashing,[0],[0]
"The second step of hashing is very simple – we simply hash each tuple independently and uniformly to an integer in [Lk], for a prescribed parameter L.",3.3. Second step of hashing,[0],[0]
We start by describing our algorithm in a single machine setting.,4. Approximation Algorithm,[0],[0]
"Then in Section 4.2, we describe how it can be implemented in parallel, with a small number of machines, and a small memory per machine.",4. Approximation Algorithm,[0],[0]
"The high level idea in our algorithm is to perform the twolevel hashing above, and choose a random subset of points from each bin.
",4.1. Main algorithm,[0],[0]
"Now, in order to choose the w parameter in the hash, we need a rough scale of the optimum.",4.1. Main algorithm,[0],[0]
"To this end, we will assume that we know a D such that the optimum objective value is in the range (D/f,D), for some f = poly(n).",4.1. Main algorithm,[0],[0]
"Note that f can be something like n2, so this is a very mild assumption.",4.1. Main algorithm,[0],[0]
"With this assumption, we have that the average contribution of a point to the objective (i.e., its squared distance to its center) is≥ D/(n · f).",4.1. Main algorithm,[0],[0]
Let us denote r0 := √ D/(n · f).,4.1. Main algorithm,[0],[0]
"Also, observe that no point can have a contribution more than D to the objective (as it is an upper bound on the sum of the contributions).",4.1. Main algorithm,[0],[0]
"Thus, inuitively, all the clusters have a radius (formally defined below) ≤",4.1. Main algorithm,[0],[0]
"√ D. Let κ = dlog(nf)e, and let ri := 2i/2r0, for 1 ≤",4.1. Main algorithm,[0],[0]
i ≤,4.1. Main algorithm,[0],[0]
κ.,4.1. Main algorithm,[0],[0]
These will be the different radius “scales” for our clusters of interest.,4.1. Main algorithm,[0],[0]
"Note that κ = O(log n), as f = poly(n).
",4.1. Main algorithm,[0],[0]
The algorithm can now be described (see Algorithm 4.1).,4.1. Main algorithm,[0],[0]
"Algorithm 1 Find-Cover
Input: set of points U , rough estimate of optimum D. Output: a subset of points S. for i = 1 . . .",4.1. Main algorithm,[0],[0]
"κ do
- Hash every point in U to a bin (range [Lk]) using the two layer hash with params t, wi, `, Lk, where wi := 8ri(log n)
3/2.",4.1. Main algorithm,[0],[0]
"Let Uj be the points hashed to bin j. - Let Gj be the group of machines assigned for bin j. For each j, assign points in Uj uniformly at random to a machine in Gj . -",4.1. Main algorithm,[0],[0]
"For each j, select a uniformly random subset of Uj of size O(1) from Gj and add them to S. (If the number of points in the group is O(1), add all of them to S.)
end for
In the remainder of this section, we analyze this algorithm.",4.1. Main algorithm,[0],[0]
"We start with a definition.
",4.1. Main algorithm,[0],[0]
Definition 1 (Cluster radius).,4.1. Main algorithm,[0],[0]
"For a cluster C with centroid µ, we define the radius to be the quantity ρ :=√
1 |C| ∑ p∈C‖p− µ‖22, i.e., the “`22 average” radius.
",4.1. Main algorithm,[0],[0]
Observation 1.,4.1. Main algorithm,[0],[0]
"In any clusterC with centroid µ and radius ρ, the number of points p in C such that ‖p− µ‖2 > 2ρ is at most |C|/4.
",4.1. Main algorithm,[0],[0]
The proof follows by an averaging argument.,4.1. Main algorithm,[0],[0]
"Now, a candidate goal is to prove that for every optimum cluster Ci (center µi, radius ρi), the algorithm chooses at least one point at a distance ≤ αρi from the center µi with high probability, for some small α.
",4.1. Main algorithm,[0],[0]
Unfortunately this statement is false in general.,4.1. Main algorithm,[0],[0]
Suppose the instance has an optimal cluster with small ρi and a small |Ci| that is really far from the rest of the points (thus it is essential to “find” that cluster).,4.1. Main algorithm,[0],[0]
"In this case, for rj that is roughly ρi (which is the scale at which we hope to find a point close to this cluster), the bin containing Ci may contain many other points that are far away; thus a random sample is unlikely to choose any point close to Ci.
",4.1. Main algorithm,[0],[0]
The fix for this problem comes from the observation that small clusters (i.e. small |Ci|) can afford to pay more per point to the objective.,4.1. Main algorithm,[0],[0]
We thus define the notion of “adjusted radius” of a cluster.,4.1. Main algorithm,[0],[0]
"First, we define θ to be the real number satisfying OPT = nθ2, i.e., the typical distance of a point to its cluster center in the optimal",4.1. Main algorithm,[0],[0]
"clustering.4 Now, we have:
Definition 2 (Adjusted radius).",4.1. Main algorithm,[0],[0]
"For a cluster C with radius ρ, we define the adjusted radius to be the quantity ρ satisfying ρ2 = ρ2 + θ2 + nθ 2
k|C| .
",4.1. Main algorithm,[0],[0]
"Our main lemma about the algorithm is the following.
",4.1. Main algorithm,[0],[0]
Lemma 4.,4.1. Main algorithm,[0],[0]
Let C be a cluster in the optimal clustering with adjusted radius ρ.,4.1. Main algorithm,[0],[0]
"With probability ≥ 1/4, Algorithm 4.1 outputs a point that is at a distance ≤ α · ρ from the center of the cluster C, where α = O(log n log log n).
",4.1. Main algorithm,[0],[0]
"This is used to show the main result of this section.
",4.1. Main algorithm,[0],[0]
Theorem 2.,4.1. Main algorithm,[0],[0]
Let S′ be the union of the sets output by O(log k) independent runs of Algorithm 4.1.,4.1. Main algorithm,[0],[0]
"For α = O(log n log log n), S′ gives an (α2, O(log n log k))",4.1. Main algorithm,[0],[0]
"bicriteria approximation for k-means, w.p. at least 9/10.
",4.1. Main algorithm,[0],[0]
Proof of Theorem 2 assuming Lemma 4.,4.1. Main algorithm,[0],[0]
"First, let us analyze the number of points output.",4.1. Main algorithm,[0],[0]
"Note that in each run of the algorithm, we output O(Lk) = O(k) points for each radius range.",4.1. Main algorithm,[0],[0]
There areO(log n) radius ranges andO(log k) independent runs.,4.1. Main algorithm,[0],[0]
"Thus we have the desired bound.
",4.1. Main algorithm,[0],[0]
"Next, consider the approximation factor.",4.1. Main algorithm,[0],[0]
"As we take S′ to be the union of O(log k) independent runs of Algorithm 4.1,
4We note that θ is used solely for analysis purposes – the algorithm is not assumed to know it.
",4.1. Main algorithm,[0],[0]
"the success probability in Lemma 4 can be boosted to 1− 1 10k , and by a union bound, we have that the conclusion of the lemma holds for all clusters, with probability > 1/10.",4.1. Main algorithm,[0],[0]
"Thus for every optimal cluster Ci of adjusted radius ρi, Algorithm 4.1 outputs at least one point at a distance≤ α·ρi, for α as desired.",4.1. Main algorithm,[0],[0]
"Thus, assigning all the points in Ci to one such point would imply that the points of Ci contribute at most |Ci|ρ2i +α2|Ci|ρ2i to the objective.5 Thus the objective value is at most∑
i
|Ci|ρ2i + α2|Ci| ( ρ2i + θ 2 + nθ2
k|Ci| ) =",4.1. Main algorithm,[0],[0]
"(1 + α2)OPT + α2 · 2nθ2 ≤ 4α2OPT.
",4.1. Main algorithm,[0],[0]
This completes the proof.,4.1. Main algorithm,[0],[0]
"We now see how to implement algorithm from Theorem 2 in a distributed setting with a small number of rounds and machines, while also using memory ≤ s per machine.",4.2. Distributed implementation,[0],[0]
"Our final result is the following.
",4.2. Distributed implementation,[0],[0]
Theorem 3.,4.2. Distributed implementation,[0],[0]
"There is a distributed algorithm that performs dlogs ne + 2 rounds of MAPREDUCE computation, and outputs a bi-criteria approximation to k-means, with the same guarantee as Theorem 2.",4.2. Distributed implementation,[0],[0]
"The number of machines needed is Õ ( n
min{k,s} · k s
) , and the space per machine is
s.
Note.",4.2. Distributed implementation,[0],[0]
"Whenever s ≥ k, the bound on the number of machines is Õ(n/s), which is essentially optimal, because we need n/s machines to hold n points, if each machine has a memory of s.
While most parts of the algorithm from Theorem 2 can be immediately parallelized, sampling from Uj (which may need to be split across machines) is the tricky part and requires some work.",4.2. Distributed implementation,[0],[0]
The proof is deferred to Section 3 of the supplement.,4.2. Distributed implementation,[0],[0]
"We now show that even in the very simple setting of points on a line, we have a tradeoff between the number of rounds and memory.",5. Lower Bound,[0],[0]
"This matches the behavior of our algorithm, up to an additive constant.
Theorem 4.",5. Lower Bound,[0],[0]
Let α be any parameter that is poly(n).,5. Lower Bound,[0],[0]
"Then, any α factor approximation algorithm for k-means with k ≥ 2 that uses poly(n) machines of memory ≤ s requires at least logs n rounds of MAPREDUCE.",5. Lower Bound,[0],[0]
"The proof is by a simple reduction from Boolean-OR,6 a
5This follows from a “center-of-mass” theorem that is standard: for a set T of points with centroid µ, and any other point µ′,∑
u∈T ‖u− µ ′‖2 = ∑ u∈T ‖u− µ‖
2 + |T |‖µ− µ′‖2.",5. Lower Bound,[0],[0]
"6The input is the set of bits x1, . . .",5. Lower Bound,[0],[0]
", xn, and the desired output
problem for which a round-memory trade-off was established in (Roughgarden et al., 2016).
",5. Lower Bound,[0],[0]
Proof.,5. Lower Bound,[0],[0]
"Suppose we have inputs x1, . . .",5. Lower Bound,[0],[0]
", xn, the inputs for Boolean OR.",5. Lower Bound,[0],[0]
"We produce an instance of clustering with k + n points, all on the line.
",5. Lower Bound,[0],[0]
"First, we place points at 1, 2, . . .",5. Lower Bound,[0],[0]
", k. Additionally, for 1 ≤ i ≤ n, if xi = 1, we add a point at k+α+1.",5. Lower Bound,[0],[0]
"If xi = 0, add a point at 1.",5. Lower Bound,[0],[0]
"Now if the OR of the xi’s is TRUE, then the optimum solution places centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k−1, k+α+1.",5. Lower Bound,[0],[0]
This results in an objective value of 1.,5. Lower Bound,[0],[0]
"If the OR is FALSE, the optimum solution is to place centers at 1, 2, . . .",5. Lower Bound,[0],[0]
", k (0 cost).",5. Lower Bound,[0],[0]
"Thus an α factor approximation should be able to distinguish between the two cases (because in the NO case, it needs to have error 0, and in the YES case, this solution will be a factor",5. Lower Bound,[0],[0]
"> α off.
",5. Lower Bound,[0],[0]
Note.,5. Lower Bound,[0],[0]
The parameter α implicitly comes up in the reduction.,5. Lower Bound,[0],[0]
The number of bits necessary to write down the points xi is n logα.,5. Lower Bound,[0],[0]
"This is why we insist on α = poly(n).
",5. Lower Bound,[0],[0]
"The lower bound above can be extended in order to rule out both (a) the case in which we have a rough estimate of the optimum (as in our algorithm), and (b) bi-criteria approximations.",5. Lower Bound,[0],[0]
"To handle (a), we can perturb the NO case so as to make the objective 1/p(nα) for a large polynomial p(·).",5. Lower Bound,[0],[0]
"In order to handle (b), i.e., to rule out an (α, β) bicriteria approximation, we need to add a multiplicity of βk for each of the points in the proof above.",5. Lower Bound,[0],[0]
This leads to a slightly weaker lower bound of logs n kβ rounds.,5. Lower Bound,[0],[0]
"The details of these steps are straightforward, so we omit them.",5. Lower Bound,[0],[0]
"We evaluate our algorithmic ideas on two synthetic and two real datasets, of varying sizes.",6. Empirical Study,[0],[0]
"In the former case, we know the ground truth clustering, the “right k”, and the optimum objective value.",6. Empirical Study,[0],[0]
We use it to demonstrate how the quality of clustering depends on the parameter ` – the number of independent hashes we concatenate.,6. Empirical Study,[0],[0]
"In all the datasets, we compare the objective value obtained by our algorithm with the one obtained by k-means++ (part of scikit-learn (Pedregosa et al., 2011)).",6. Empirical Study,[0],[0]
"This will only be possible for small enough datasets, as k-means++ is a single machine algorithm that uses Ω(nk) memory.",6. Empirical Study,[0],[0]
Both the datasets we consider are mixtures of Gaussians.,6.1. Synthetic datasets,[0],[0]
"The first has n = 105 points in R50 and k = 100, while the second has n = 106 point in R50 and k = 1000.",6.1. Synthetic datasets,[0],[0]
For i ∈,6.1. Synthetic datasets,[0],[0]
"[k], the centers are chosen uniformly from a box of length 400 in each dimension, maintaining a distance of 20 from one
is simply the OR of the bits.
another.",6.1. Synthetic datasets,[0],[0]
"To form cluster Ci, a random number of points are chosen from the Gaussian N (µi, 1).
",6.1. Synthetic datasets,[0],[0]
"For the first dataset, we produce PLSH tuples using w = 15, t = 2, and vary `.We call the set of points that hash to a given tuple a bucket of points.",6.1. Synthetic datasets,[0],[0]
"We measure the following quantities: (a) the total number of non-empty buckets, (b) the “purity” of the buckets, i.e., the number of distinct clusters at intersect a non-empty bucket (on average), and (c) the “spread” of a cluster, i.e., the number of buckets that points of a cluster go to.",6.1. Synthetic datasets,[0],[0]
The plots for these quantities as ` varies are shown in Figure 2.,6.1. Synthetic datasets,[0],[0]
"Note that it makes sense for the spread to increase as ` increases, as even a difference in one of the ` independent hashes results in unequal hashes.
",6.1. Synthetic datasets,[0],[0]
"Next, we study the objective value.",6.1. Synthetic datasets,[0],[0]
For this we choose ` = 3.,6.1. Synthetic datasets,[0],[0]
This results in 257 non-empty buckets.,6.1. Synthetic datasets,[0],[0]
"Now, from each bucket, we output j points uniformly at random to form a set S (and do this for different j).",6.1. Synthetic datasets,[0],[0]
"Even for j = 1, the objective value is 41079, which is less than a factor 2 away from the optimum, 26820.",6.1. Synthetic datasets,[0],[0]
This is significantly better than the guarantee we get from Theorem 2.,6.1. Synthetic datasets,[0],[0]
"It is also significantly better than a random subset of 257 points, for which it turns out that the objective is 5897317.
",6.1. Synthetic datasets,[0],[0]
"Intuitively, a random sample will be bad for this instance, because there are many clusters of size n/k, and no points from such clusters will be chosen.",6.1. Synthetic datasets,[0],[0]
This motivates us to measure the cluster recall of our procedure – how many clusters contribute to the 257 size set we output?,6.1. Synthetic datasets,[0],[0]
"Interestingly, all 100 of the clusters do, for the above values of the parameters.",6.1. Synthetic datasets,[0],[0]
"These results are consistent with the theoretical observations that PLSH finds small-cardinality clusters while a random sample does not.
",6.1. Synthetic datasets,[0],[0]
"Next, consider the larger synthetic dataset.",6.1. Synthetic datasets,[0],[0]
"Modulo n, k, the data is generated as before.",6.1. Synthetic datasets,[0],[0]
"Here, we produce PLSH tuples using w = 15, t = 3, and ` = 4.",6.1. Synthetic datasets,[0],[0]
"For these choices of n and k, the single-machine k-means++ runs out of memory.",6.1. Synthetic datasets,[0],[0]
"However, as we know the µi, we can estimate the optimum
objective value, which is 251208.
",6.1. Synthetic datasets,[0],[0]
"In this dataset, we illustrate the use of our algorithm to generate a coreset, as discussed in Section 1.2.",6.1. Synthetic datasets,[0],[0]
"We obtain 5722 buckets, from each of which we sample j points to obtain a coreset S. We then run k-means on S with k = 1000, thus obtaining 1000 centers.",6.1. Synthetic datasets,[0],[0]
We evaluate the k-means objective with these centers.,6.1. Synthetic datasets,[0],[0]
Results for different j are shown in Figure 3.,6.1. Synthetic datasets,[0],[0]
"Note that even with j = 10, the objective is within a 1.1 factor of the optimum.",6.1. Synthetic datasets,[0],[0]
"We show our results on two datasets, both available via the UC Irvine dataset repository.
SUSY.",6.2. Real datasets,[0],[0]
"The first dataset is SUSY (see (P. Baldi, 2014)), which contains 5M points with 18 features.",6.2. Real datasets,[0],[0]
"In order to efficiently compare with k-means++, we use a sub-sample of 100000 points.",6.2. Real datasets,[0],[0]
"In this case we use the values of t, ` as in our theoretical results.",6.2. Real datasets,[0],[0]
"We also try different values for w. We start with a guess of w = σ(log n)3/2, where σ was obtained from k-means++ with k = 10 (which is very fast).",6.2. Real datasets,[0],[0]
We then scale σ from 2−4 to 22 in order to perform the hashing in different radius ranges.,6.2. Real datasets,[0],[0]
"After hashing and finding S, we use it as a coreset and compute k-means.",6.2. Real datasets,[0],[0]
"Figure 4 shows the results, and also compares against a fully random subset of points.",6.2. Real datasets,[0],[0]
"Unlike the synthetic examples, here a random set of points is not orders of magnitude worse, but is still considerably worse than the output of our algorithm.",6.2. Real datasets,[0],[0]
We also note that our values are within a factor 1.2 of k-means++ (which is sequential and significantly slower).,6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 600 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.03, 0.31, 1.21, 3.97, 7, 10.55.
",6.2. Real datasets,[0],[0]
"FMA: A Dataset For Music Analysis This dataset (see (Defferrard et al., 2017)) contains 518 features extracted from audio files available in the free music archive (FMA).",6.2. Real datasets,[0],[0]
It has 106574 points.,6.2. Real datasets,[0],[0]
We perform the same experiment we did for the SUSY dataset.,6.2. Real datasets,[0],[0]
"Figure 5 shows
the results, comparing the outputs with the output of kmeans++, as well as a random subset.",6.2. Real datasets,[0],[0]
"The number of buckets per cluster when k = 512 for ` = 1, . . .",6.2. Real datasets,[0],[0]
", 6 are 0.08, 0.68, 2.29, 5.27, 9.43, 14.09 respectively.",6.2. Real datasets,[0],[0]
"Given the importance of clustering in the analysis of large scale data, distributed algorithms for formulations such as k-means, k-median, etc. have been extensively studied.",abstractText,[0],[0]
"A successful approach here has been the “reduce and merge” paradigm, in which each machine reduces its input size to Õ(k), and this data reduction continues (possibly iteratively) until all the data fits on one machine, at which point the problem is solved locally.",abstractText,[0],[0]
"This approach has the intrinsic bottleneck that each machine must solve a problem of size ≥ k, and needs to communicate at least Ω(k) points to the other machines.",abstractText,[0],[0]
"We propose a novel data partitioning idea to overcome this bottleneck, and in effect, have different machines focus on “finding different clusters”.",abstractText,[0],[0]
"Under the assumption that we know the optimum value of the objective up to a poly(n) factor (arbitrary polynomial), we establish worst-case approximation guarantees for our method.",abstractText,[0],[0]
We see that our algorithm results in lower communication as well as a near-optimal number of ‘rounds’ of computation (in the popular MapReduce framework).,abstractText,[0],[0]
Distributed Clustering via LSH Based Data Partitioning,title,[0],[0]
Given n vectors Xn def=,1.1. Background,[0],[0]
"X 1 , X 2 . .",1.1. Background,[0],[0]
.,1.1. Background,[0],[0]
", Xn 2 Rd that reside on n clients, the goal of distributed mean estimation is to estimate the mean of the vectors:
¯X def =
1
n
nX
i=1
Xi.",1.1. Background,[0],[0]
"(1)
This basic estimation problem is used as a subroutine in several learning and optimization tasks where data is distributed across several clients.",1.1. Background,[0],[0]
"For example, in Lloyd’s algorithm (Lloyd, 1982) for k-means clustering, if data is distributed across several clients, the server needs to compute
1Google Research, New York, NY, USA 2Google Research, Seattle, WA, USA.",1.1. Background,[0],[0]
"Correspondence to: Ananda Theertha Suresh <theertha@google.com>.
",1.1. Background,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1.1. Background,[0],[0]
"Copyright 2017 by the author(s).
",1.1. Background,[0],[0]
the means of all clusters in each update step.,1.1. Background,[0],[0]
"Similarly, for PCA, if data samples are distributed across several clients, then for the power-iteration method, the server needs to average the output of all clients in each step.
",1.1. Background,[0],[0]
"Recently, algorithms involving distributed mean estimation have been used extensively in training large-scale neural networks and other statistical models (McDonald et al., 2010; Povey et al., 2014; Dean et al., 2012; McMahan et al., 2016; Alistarh et al., 2016).",1.1. Background,[0],[0]
"In a typical scenario of synchronized distributed learning, each client obtains a copy of a global model.",1.1. Background,[0],[0]
The clients then update the model independently based on their local data.,1.1. Background,[0],[0]
"The updates (usually in the form of gradients) are then sent to a server, where they are averaged and used to update the global model.",1.1. Background,[0],[0]
A critical step in all of the above algorithms is to estimate the mean of a set of vectors as in Eq.,1.1. Background,[0],[0]
"(1).
",1.1. Background,[0],[0]
One of the main bottlenecks in distributed algorithms is the communication cost.,1.1. Background,[0],[0]
"This has spurred a line of work focusing on communication cost in learning (Tsitsiklis & Luo, 1987; Balcan et al., 2012; Zhang et al., 2013; Arjevani & Shamir, 2015; Chen et al., 2016).",1.1. Background,[0],[0]
"The communication cost can be prohibitive for modern applications, where each client can be a low-power and low-bandwidth device such as a mobile phone (Konečnỳ et al., 2016).",1.1. Background,[0],[0]
"Given such a wide set of applications, we study the basic problem of achieving the optimal minimax rate in distributed mean estimation with limited communication.
",1.1. Background,[0],[0]
"We note that our model and results differ from previous works on mean estimation (Zhang et al., 2013; Garg et al., 2014; Braverman et al., 2016) in two ways: previous works assume that the data is generated i.i.d.",1.1. Background,[0],[0]
according to some distribution; we do not make any distribution assumptions on data.,1.1. Background,[0],[0]
"Secondly, the objective in prior works is to estimate the mean of the underlying statistical model; our goal is to estimate the empirical mean of the data.",1.1. Background,[0],[0]
"Our proposed communication algorithms are simultaneous and independent, i.e., the clients independently send data to the server and they can transmit at the same time.",1.2. Model,[0],[0]
"In any independent communication protocol, each client transmits a function of Xi (say f(Xi)), and a central server estimates the mean by some function of f(X
1 ), f(X 2 ), . . .",1.2. Model,[0],[0]
", f(Xn).
",1.2. Model,[0],[0]
"Let ⇡ be any such protocol and let Ci(⇡, Xi) be the expected number of transmitted bits by the i-th client during protocol ⇡, where throughout the paper, expectation is over the randomness in protocol ⇡.
",1.2. Model,[0],[0]
"The total number of bits transmitted by all clients with the protocol ⇡ is
C(⇡, Xn) def= nX
i=1
Ci(⇡, Xi).
",1.2. Model,[0],[0]
Let the estimated mean be ˆ¯X .,1.2. Model,[0],[0]
"For a protocol ⇡, the MSE of the estimate is
E(⇡, Xn) = E  ˆ¯X ¯X 2
2
.
",1.2. Model,[0],[0]
We allow the use of both private and public randomness.,1.2. Model,[0],[0]
"Private randomness refers to random values that are generated by each machine separately, and public randomness refers to a sequence of random values that are shared among all parties1.
",1.2. Model,[0],[0]
The proposed algorithms work for any Xn.,1.2. Model,[0],[0]
"To measure the minimax performance, without loss of generality, we restrict ourselves to the scenario where each Xi 2 Sd, the ball of radius 1 in Rd, i.e., X 2 Sd iff
||X|| 2  1, where ||X||
2
denotes the ` 2 norm of the vector X .",1.2. Model,[0],[0]
"For a protocol ⇡, the worst case error for all Xn 2 Sd is
E(⇡, Sd) def=",1.2. Model,[0],[0]
"max Xn:Xi2Sd 8i E(⇡, Xn).
",1.2. Model,[0],[0]
Let ⇧(c) denote the set of all protocols with communication cost at most c.,1.2. Model,[0],[0]
"The minimax MSE is
E(⇧(c), Sd) def=",1.2. Model,[0],[0]
"min ⇡2⇧(c) E(⇡, Sd).",1.2. Model,[0],[0]
"We first analyze the MSE E(⇡, Xn) for three algorithms, when C(⇡, Xn) = ⇥(nd), i.e., each client sends a constant number of bits per dimension.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic uniform quantization.,1.3.1. ALGORITHMS,[0],[0]
"In Section 2.1, as a warm-up we first show that a naive stochastic binary quantization algorithm (denoted by ⇡sb) achieves an MSE of
E(⇡sb, Xn) = ⇥",1.3.1. ALGORITHMS,[0],[0]
"d
n · 1 n
nX
i=1
||Xi||2 2
!",1.3.1. ALGORITHMS,[0],[0]
",
1In the absence of public randomness, the server can communicate a random seed that can be used by clients to emulate public randomness.
and C(⇡sb, Xn) = n · (d + ˜O(1))2, i.e., each client sends one bit per dimension.",1.3.1. ALGORITHMS,[0],[0]
We further show that this bound is tight.,1.3.1. ALGORITHMS,[0],[0]
"In many practical scenarios, d is much larger than n and the above error is prohibitive (Konečnỳ et al., 2016).
",1.3.1. ALGORITHMS,[0],[0]
A natural way to decease the error is to increase the number of levels of quantization.,1.3.1. ALGORITHMS,[0],[0]
"If we use k levels of quantization, in Theorem 2, we show that the error deceases as
E(⇡sk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
d
n(k 1)2 · 1 n
nX
i=1
||Xi||2 2
! .",1.3.1. ALGORITHMS,[0],[0]
"(2)
However, the communication cost would increase to C(⇡sk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)) bits, which can be expensive, if we would like the MSE to be o(d/n).
",1.3.1. ALGORITHMS,[0],[0]
"In order to reduce the communication cost, we propose two approaches.
",1.3.1. ALGORITHMS,[0],[0]
Stochastic rotated quantization: We show that preprocessing the data by a random rotation reduces the mean squared error.,1.3.1. ALGORITHMS,[0],[0]
"Specifically, in Theorem 3, we show that this new scheme (denoted by ⇡srk) achieves an MSE of
E(⇡srk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O
log d
n(k",1.3.1. ALGORITHMS,[0],[0]
"1)2 · 1 n
nX
i=1
||Xi||2 2
! , 3
and has a communication cost of C(⇡srk, Xn) = n ·",1.3.1. ALGORITHMS,[0],[0]
"(ddlog
2 ke + ˜O(1)).",1.3.1. ALGORITHMS,[0],[0]
"Note that the new scheme achieves much smaller MSE than naive stochastic quantization for the same communication cost.
",1.3.1. ALGORITHMS,[0],[0]
Variable length coding: Our second approach uses the same quantization as ⇡sk but encodes levels via variable length coding.,1.3.1. ALGORITHMS,[0],[0]
"Instead of using dlog
2 ke bits per dimension, we show that using variable length encoding such as arithmetic coding to compress the data reduces the communication cost significantly.",1.3.1. ALGORITHMS,[0],[0]
"In particular, in Theorem 4 we show that there is a scheme (denoted by ⇡svk) such that
C(⇡svk, Xn)",1.3.1. ALGORITHMS,[0],[0]
= O(nd(1 + log(k2/d+ 1)),1.3.1. ALGORITHMS,[0],[0]
"+ ˜O(n)), (3) and E(⇡svk, Xn) = E(⇡sk, Xn).",1.3.1. ALGORITHMS,[0],[0]
"Hence, setting k = p d in Eqs.",1.3.1. ALGORITHMS,[0],[0]
"2 and 3 yields
E(⇡svk, Xn) =",1.3.1. ALGORITHMS,[0],[0]
"O 1 n · 1 n
nX
i=1
||Xi||2 2
! ,
and with ⇥(nd) bits of communication i.e., constant number of bits per dimension per client.",1.3.1. ALGORITHMS,[0],[0]
"Of the three protocols, ⇡svk has the best MSE for a given communication cost.",1.3.1. ALGORITHMS,[0],[0]
Note that ⇡svk uses k quantization levels but still uses O(1) bits per dimension per client for all k  pd.,1.3.1. ALGORITHMS,[0],[0]
"Theoretically, while variable length coding has better guarantees, stochastic rotated quantization has several practical
2We use ˜O(1) to denote O(log(dn)).",1.3.1. ALGORITHMS,[0],[0]
"3All logarithms are to base e, unless stated.
advantages: it uses fixed length coding and hence can be combined with encryption schemes for privacy preserving secure aggregation (Bonawitz et al., 2016).",1.3.1. ALGORITHMS,[0],[0]
"It can also provide lower quantization error in some scenarios due to better constants (see Section 7 for details).
",1.3.1. ALGORITHMS,[0],[0]
"Concurrent to this work, Alistarh et al. (2016) showed that stochastic quantization and Elias coding can be used to obtain communication-optimal SGD.",1.3.1. ALGORITHMS,[0],[0]
"Recently, Konečnỳ & Richtárik (2016) showed that ⇡sb can be improved further by optimizing the choice of stochastic quantization boundaries.",1.3.1. ALGORITHMS,[0],[0]
"However, their results depend on the number of bits necessary to represent a float, whereas ours do not.",1.3.1. ALGORITHMS,[0],[0]
"In the above protocols, all of the clients transmit the data.",1.3.2. MINIMAX MSE,[0],[0]
"We augment these protocols with a sampling procedure, where only a random fraction of clients transmit data.",1.3.2. MINIMAX MSE,[0],[0]
"We show that a combination of k-level quantization, variable length coding, and sampling can be used to achieve information theoretically optimal MSE for a given communication cost.",1.3.2. MINIMAX MSE,[0],[0]
"In particular, combining Corollary 1 and Theorem 5 yields our minimax result: Theorem 1.",1.3.2. MINIMAX MSE,[0],[0]
"There exists a universal constant t < 1 such that for communication cost c  ndt and n 1/t,
E(⇧(c), Sd) = ⇥",1.3.2. MINIMAX MSE,[0],[0]
"✓ min ✓ 1, d
c
◆◆ .
",1.3.2. MINIMAX MSE,[0],[0]
"This result shows that the product of communication cost and MSE scales linearly in the number of dimensions.
",1.3.2. MINIMAX MSE,[0],[0]
The rest of the paper is organized as follows.,1.3.2. MINIMAX MSE,[0],[0]
We first analyze the stochastic uniform quantization technique in Section 2.,1.3.2. MINIMAX MSE,[0],[0]
"In Section 3, we propose the stochastic rotated quantization technique, and in Section 4 we analyze arithmetic coding.",1.3.2. MINIMAX MSE,[0],[0]
"In Section 5, we combine the above algorithm with a sampling technique and state the upper bound on the minimax risk, and in Section 6 we state the matching minimax lower bounds.",1.3.2. MINIMAX MSE,[0],[0]
"Finally, in Section 7 we discuss some practical considerations and apply these algorithms on distributed power iteration and Lloyd’s algorithm.",1.3.2. MINIMAX MSE,[0],[0]
"For a vector Xi, let Xmaxi = max1jd Xi(j) and similarly let Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In the stochastic binary quantization protocol ⇡sb, for each client i, the quantized value for each coordinate j is generated independently with private randomness as
Yi(j) =
( Xmaxi w.p.
Xi(j) Xmini Xmaxi Xmini ,
Xmini otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Observe EYi(j) = Xi(j).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sb = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We first bound the communication cost of the this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 1.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic binary quantization that uses d + ˜O(1) bits per client and hence C(⇡sb, Xn)  n · ⇣ d+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Instead of sending vectors Yi, clients transmit two real values Xmaxi and Xmini (to a desired error) and a bit vector Y 0i such that Y 0i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
(j) = 1 if Yi = Xmaxi and 0 otherwise.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Hence each client transmits d + 2r bits, where r is the number of bits to transmit the real value to a desired error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let B be the maximum norm of the underlying vectors.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"To bound r, observe that using r bits, one can represent a number between B and B to an error of B/2r 1.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Thus using 3 log
2 (dn) + 1 bits one can represent the minimum and maximum to an additive error of B/(nd)3.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
This error in transmitting minimum and maximum of the vector does not affect our calculations and we ignore it for simplicity.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We note that in practice, each dimension of Xi is often stored as a 32 bit or 64 bit float, and r should be set as either 32 or 64.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In this case, using an even larger r does not further reduce the error.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"We now compute the estimation error of this protocol.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn) = 1 n2
nX
i=1
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sb, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2 ,
where the last equality follows by observing that Yi Xi, 8i, are independent zero mean random variables.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by observing that for every i,
E ||Yi Xi||2 2 =
dX
j=1
E[(Yi(j) Xi(j))2]
=
dX
j=1
(Xmaxi Xi(j))(Xi(j) Xmini ).
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 2 implies the following upper bound.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 3.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For any set of vectors Xn,
E(⇡sb, Xn)  d 2n · 1 n
nX
i=1
||Xi||2 2 .
Proof.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof follows by Lemma 2 observing that 8j
(Xmaxi Xi(j))(Xi(j) Xmini )  (Xmaxi Xmini )2
4
,
and (Xmaxi Xmini )2  2 ||Xi||2
2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"(4)
We also show that the above bound is tight: Lemma 4.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists a set of vectors Xn such that
E(⇡sb, Xn) d 2 2n · 1 n
nX
i=1
||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Proof.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, let Xi be defined as follows.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Xi(1),2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, Xi(2)",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"= 1/ p 2, and for all j > 2, Xi(j) = 0.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every i, Xmaxi = 1p 2 and Xmini = 1p 2
.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Substituting these bounds in the conclusion of Lemma 2 (which is an equality) yields the theorem.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Therefore, the simple algorithm proposed in this section gives MSE ⇥(d/n).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Such an error is too large for realworld use.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For example, in the application of neural networks (Konečnỳ et al., 2016), d can be on the order of millions, yet n can be much smaller than that.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In such cases, the MSE is even larger than the norm of the vector.
2.2.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Stochastic k-level quantization
A natural generalization of binary quantization is k-level quantization.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Let k be a positive integer larger than 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We propose a k-level stochastic quantization scheme ⇡sk to quantize each coordinate.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Recall that for a vector Xi, Xmaxi = max1jd Xi(j) and Xmini = min1jd Xi(j).",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"For every integer r in the range [0, k), let
Bi(r) def = Xmini + rsi",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"k 1 ,
where si satisfies Xmini + si Xmaxi .",2.1. Warm-up: Stochastic binary quantization,[0],[0]
A natural choice for si would be Xmaxi Xmini .4,2.1. Warm-up: Stochastic binary quantization,[0],[0]
The algorithm quantizes each coordinate into one of Bi(r)s stochastically.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"In ⇡sk, for the i-th client and j-th coordinate, if Xi(j) 2 [Bi(r), Bi(r + 1)),
Yi(j) =
( Bi(r + 1) w.p.
Xi(j) Bi(r) Bi(r+1) Bi(r)
Bi(r) otherwise.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"4We will show in Section 4, however, a higher value of si and variable length coding has better guarantees.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡sk = 1
n
nX
i=1
Yi.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"As before, the communication complexity of this protocol is bounded.",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"The proof is similar to that of Lemma 1 and hence omitted.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
Lemma 5.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"There exists an implementation of stochastic klevel quantization that uses ddlog(k)e+ ˜O(1) bits per client and hence C(⇡sk, Xn)  n ·",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"⇣ ddlog
2
ke+ ˜O(1) ⌘ .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
The mean squared loss can be bounded as follows.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
Theorem 2.,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"If Xmaxi Xmini  si  p 2 ||Xi||
2 8i, then for any Xn, the ⇡sk protocol satisfies,
E(⇡sk, Xn)  ",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"d 2n(k 1)2 · 1 n
nX
i=1
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"||Xi||2 2 .
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"Proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
"E(⇡sk, Xn) = E ˆ¯X ¯X
2
2
=
1 n2 E
nX
i=1
(Yi Xi)
2
2
=
1
n2
nX
i=1
E ||Yi Xi||2 2  1 n2
nX
i=1
d s2i
4(k 1)2 , (5)
where the last equality follows by observing Yi(j) Xi(j) is an independent zero mean random variable with E(Yi(j) Xi(j))2  s 2 i",2.1. Warm-up: Stochastic binary quantization,[0],[0]
4(k 1)2 .,2.1. Warm-up: Stochastic binary quantization,[0],[0]
"si  p 2 ||Xi|| 2
completes the proof.
",2.1. Warm-up: Stochastic binary quantization,[0],[0]
We conclude this section by noting that si = Xmaxi Xmini satisfies the conditions for the above theorem by Eq. (4).,2.1. Warm-up: Stochastic binary quantization,[0],[0]
We show that the algorithm of the previous section can be significantly improved by a new protocol.,3. Stochastic rotated quantization,[0],[0]
"The motivation comes from the fact that the MSE of stochastic binary quantization and stochastic k-level quantization is O( dn (X max
i Xmini )2) (the proof of Lemma 3 and Theorem 2 with si = Xmaxi Xmini ).",3. Stochastic rotated quantization,[0],[0]
Therefore the MSE is smaller when Xmaxi and Xmaxi are close.,3. Stochastic rotated quantization,[0],[0]
"For example, when Xi is generated uniformly on the unit sphere, with
high probability, Xmaxi Xmini is O ✓q log d d ◆ (Dasgupta
& Gupta, 2003).",3. Stochastic rotated quantization,[0],[0]
"In such case, E(⇡sk, Xn) is O( log dn ) instead of O( dn ).",3. Stochastic rotated quantization,[0],[0]
"In this section, we show that even without any assumptions on the distribution of the data, we can “reduce” Xmaxi Xmini with a structured random rotation, yielding
an O( log dn ) error.",3. Stochastic rotated quantization,[0],[0]
"We call the method stochastic rotated quantization and denote it by ⇡srk.
",3. Stochastic rotated quantization,[0],[0]
"Using public randomness, all clients and the central server generate a random rotation matrix (random orthogonal matrix) R 2 Rd⇥d according to some known distribution.",3. Stochastic rotated quantization,[0],[0]
Let Zi = RXi and ¯Z = R ¯X .,3. Stochastic rotated quantization,[0],[0]
"In the stochastic rotated quantization protocol ⇡srk(R), clients quantize the vectors Zi instead of Xi and transmit them similar to ⇡srk.",3. Stochastic rotated quantization,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡srk = R 1 ˆ
¯Z, ˆ¯Z = 1
n
nX
i=1
Yi.
",3. Stochastic rotated quantization,[0],[0]
The communication cost is same as ⇡sk and is given by Lemma 5.,3. Stochastic rotated quantization,[0],[0]
"We now bound the MSE.
Lemma 6.",3. Stochastic rotated quantization,[0],[0]
"For any Xn, E(⇡srk(R), Xn) is at most
d 2n2(k 1)2 nX
i=1
ER h (Zmaxi ) 2 + Zmini 2 i ,
where Zi = RXi and for every i, let si = Zmaxi Zmini .
",3. Stochastic rotated quantization,[0],[0]
"Proof.
",3. Stochastic rotated quantization,[0],[0]
"E(⇡srk, Xn) = E⇡ ˆ¯X ¯X
2
= E⇡ R 1",3. Stochastic rotated quantization,[0],[0]
"ˆ¯Z R 1 ¯Z
2 (a) = E⇡ ˆ¯Z ¯Z 2
(b) = ERE⇡  ˆ¯Z ¯Z 2 |Zn 1
 d 4n2(k",3. Stochastic rotated quantization,[0],[0]
"1)2
nX
i=1
ER[(Zmaxi Zmini )2],
where the last inequality follows Eq. (5) and the value of si.",3. Stochastic rotated quantization,[0],[0]
"(a) follows from the fact that rotation does not change the norm of the vector, and (b) follows from the tower law of expectation.",3. Stochastic rotated quantization,[0],[0]
"The lemma follows from observing that
(Zmaxi Zmini )2  2(Zmaxi )2 + 2(Zmini )2.
To obtain strong bounds, we need to find an orthogonal matrix R that achieves low (Zmaxi )2 and (Zmini )2.",3. Stochastic rotated quantization,[0],[0]
"In addition, due to the fact that d can be huge in practice, we need a type of orthogonal matrix that permits fast matrix-vector products.",3. Stochastic rotated quantization,[0],[0]
Naive orthogonal matrices that support fast multiplication such as block-diagonal matrices often result in high values of (Zmaxi )2 and (Zmini )2.,3. Stochastic rotated quantization,[0],[0]
"Motivated by recent works of structured matrices (Ailon & Chazelle, 2006; Yu et al., 2016), we propose to use a special type of orthogonal matrix R = HD, where D is a random diagonal matrix with i.i.d.",3. Stochastic rotated quantization,[0],[0]
Rademacher entries (±1 with probability 0.5).,3. Stochastic rotated quantization,[0],[0]
"H is a Walsh-Hadamard matrix (Horadam, 2012).",3. Stochastic rotated quantization,[0],[0]
"The WalshHadamard matrix of dimension 2m for m 2 N is given by
the recursive formula,
H(21) =
 1 1
1 1 , H(2m) =  H(2m 1) H(2m 1) H(2m 1) H(2m 1) .
",3. Stochastic rotated quantization,[0],[0]
Both applying the rotation and inverse rotation take O(d log d) time and O(1) additional space (with an inplace algorithm).,3. Stochastic rotated quantization,[0],[0]
"The next lemma bounds E (Zmaxi )
2 and E Zmini 2 for this choice of R. The lemma is similar to that of Ailon & Chazelle (2006), and we give the proof in Appendix A for completeness.",3. Stochastic rotated quantization,[0],[0]
Lemma 7.,3. Stochastic rotated quantization,[0],[0]
"Let R = HD, where D is a diagonal matrix with independent Radamacher random variables.",3. Stochastic rotated quantization,[0],[0]
"For every i and every sequence Xn,
E ⇥",3. Stochastic rotated quantization,[0],[0]
(Zmini ) 2 ⇤ = E ⇥,3. Stochastic rotated quantization,[0],[0]
"(Zmaxi ) 2 ⇤  ||Xi|| 2 2 (2 log d+ 2)
d .
",3. Stochastic rotated quantization,[0],[0]
Combining the above two lemmas yields the main result.,3. Stochastic rotated quantization,[0],[0]
Theorem 3.,3. Stochastic rotated quantization,[0],[0]
"For any Xn, ⇡srk(HD) protocol satisfies,
E(⇡srk(HD), Xn)  2 log d+ 2 n(k 1)2 · 1 n
nX
i=1
||Xi||2 2 .",3. Stochastic rotated quantization,[0],[0]
"Instead of preprocessing the data via a rotation matrix as in ⇡srk, in this section we propose to use a variable length coding strategy to minimize the number of bits.
Consider the stochastic k-level quantization technique.",4. Variable length coding,[0],[0]
"A natural way of transmitting Yi is sending the bin number for each coordinate, thus the total number of bits the algorithm sends per transmitted coordinate would be ddlog
2 ke.",4. Variable length coding,[0],[0]
This naive implementation is sub-optimal.,4. Variable length coding,[0],[0]
"Instead, we propose to further encode the transmitted values using universal compression schemes (Krichevsky & Trofimov, 1981; Falahatgar et al., 2015).",4. Variable length coding,[0],[0]
"We first encode hr, the number of times each quantized value r has appeared, and then use arithmetic or Huffman coding corresponding to the distribution pr = hrd .",4. Variable length coding,[0],[0]
We denote this scheme by ⇡svk.,4. Variable length coding,[0],[0]
"Since we quantize vectors the same way in ⇡sk and ⇡svk, the MSE of ⇡svk is also given by Theorem 2.",4. Variable length coding,[0],[0]
We now bound the communication cost.,4. Variable length coding,[0],[0]
Theorem 4.,4. Variable length coding,[0],[0]
Let si = p 2 ||Xi||.,4. Variable length coding,[0],[0]
"There exists an implementation of ⇡svk such that C(⇡svk, Xn) is at most
n ✓ d ✓ 2 + log2 ✓ (k 1)2
2d +
5
4
◆◆ + k log2
(d+ k)e",4. Variable length coding,[0],[0]
"k +
˜O(1) ◆ .
",4. Variable length coding,[0],[0]
Proof.,4. Variable length coding,[0],[0]
"As in Lemma 1, ˜O(1) bits are used to transmit the si’s and Xmini .",4. Variable length coding,[0],[0]
"Recall that hr is the number of coordinates that are quantized into bin r, and r takes k possible values.",4. Variable length coding,[0],[0]
"Furthermore, P r hr = d.",4. Variable length coding,[0],[0]
"Thus the number of bits
necessary to represent the hr’s is ⇠ log
2 ✓ d+",4. Variable length coding,[0],[0]
"k 1 k 1 ◆⇡  k log 2 (d+ k)e k .
",4. Variable length coding,[0],[0]
"Once we have compressed the hr’s, we use arithmetic coding corresponding to the distribution pr = hr/d to compress and transmit bin values for each coordinate.",4. Variable length coding,[0],[0]
"The total number of bits arithmetic coding uses is (MacKay, 2003)
d k 1X
r=0
hr d log 2 d hr + 2.
",4. Variable length coding,[0],[0]
"Let pr = hr/d, a = (k 1)Xmini , b = si, and =Pk 1 r=0 1/((a+ br) 2 + ).",4. Variable length coding,[0],[0]
"Note that
X
r
pr log 2
1
pr =
X
r
pr log 2
1/(((a+ br)2 + ) )
",4. Variable length coding,[0],[0]
"pr
+
X
r
pr log 2 (((a+ br)2 + ) )
 ",4. Variable length coding,[0],[0]
"X
r
pr log 2 (((a+ br)2 + ) )
 log 2 (
X
r
pr(a+ br) 2
+ ) + log 2
,
where the first inequality follows from the positivity of KLdivergence.",4. Variable length coding,[0],[0]
"Choosing = s2i , yields  4/s2i and hence, X
r
pr log 2
1 pr  log 2 (
X
r
pr(a+br) 2 +s2i )+",4. Variable length coding,[0],[0]
"log2(4/s 2 i ).
",4. Variable length coding,[0],[0]
"Note that if Yi(j) belongs to bin r, (a + br)2 = (k 1)
2Y 2i (j).",4. Variable length coding,[0],[0]
Recall that hr is the number of coordinates quantized into bin r.,4. Variable length coding,[0],[0]
"Hence P r hr(a + br)
2 is the scaled norm-square of Yi, i.e.,
X
r
hr(a+ br) 2
=",4. Variable length coding,[0],[0]
"(k 1)2 dX
j=1
Y 2i (j)
=
dX
j=1
((Xi(j) + ↵(j))(k 1))2 ,
where the ↵(j) = Yi(j) Xi(j).",4. Variable length coding,[0],[0]
"Taking expectations on both sides and using the fact that the ↵(j) are independent zero mean random variables over a range of si/(k 1), we get
E X
r
hr(a+ br) 2 =
dX
j=1
E(Xi(j)2 + ↵(j)2)(k 1)2
 ||Xi||2 2
✓ (k 1)2 + d
2
◆ .
",4. Variable length coding,[0],[0]
"Using Jensen’s inequality yields the result.
",4. Variable length coding,[0],[0]
"Thus if k = p d + 1, the communication complexity is O(nd) and the MSE is O(1/n).",4. Variable length coding,[0],[0]
"In the above protocols, all the clients transmit and hence the communication cost scales linearly with n. Instead, we show that any of the above protocols can be combined by client sampling to obtain trade-offs between the MSE and the communication cost.",5. Communication MSE trade-off,[0],[0]
"Note that similar analysis also holds for sampling the coordinates.
",5. Communication MSE trade-off,[0],[0]
"Let ⇡ be a protocol where the mean estimate is of the form:
ˆ ¯X = R 1 1
n
nX
i=1
Yi.",5. Communication MSE trade-off,[0],[0]
"(6)
All three protocols we have discussed are of this form.",5. Communication MSE trade-off,[0],[0]
Let ⇡p be the protocol where each client participates independently with probability p.,5. Communication MSE trade-off,[0],[0]
"The server estimates ¯X by
ˆ ¯X⇡p = R 1 · 1
np
X i2S Yi,
where Yis are defined in the previous section and S is the set of clients that transmitted.",5. Communication MSE trade-off,[0],[0]
Lemma 8.,5. Communication MSE trade-off,[0],[0]
"For any set of vectors Xn and protocol ⇡ of the form Equation (6), its sampled version ⇡p satisfies
E(⇡p, Xn) = 1 p · E(⇡, Xn) + 1 p np
nX
i=1
||Xi||2 2 .
and C(⇡p, Xn) = p · C(⇡, Xn).
",5. Communication MSE trade-off,[0],[0]
Proof.,5. Communication MSE trade-off,[0],[0]
"The proof of communication cost follows from Lemma 5 and the fact that in expectation, np clients transmit.",5. Communication MSE trade-off,[0],[0]
We now bound the MSE.,5. Communication MSE trade-off,[0],[0]
Let S be the set of clients that transmit.,5. Communication MSE trade-off,[0],[0]
"The error E(⇡p, Xn) is
E  ˆ¯X ¯X 2
2
= E
2
4 1
np
X i2S R 1Yi ¯X 2
2
3
5
=E
2
4 1
np
X i2S Xi ¯X 2
2
+
1
n2p2
X i2S (R 1Yi Xi) 2
2
3
5 ,
where the last equality follows by observing that R 1Yi Xi are independent zero mean random variables and hence for any i, E[(R",5. Communication MSE trade-off,[0],[0]
1Yi Xi)T ( P i2S Xi ¯X)] = 0.,5. Communication MSE trade-off,[0],[0]
"The first term can be bounded as
E 1
np
X i2S Xi ¯X 2
2
=
1
n2
nX
i=1
E 1
p Xi i2S Xi
2
2
=
1
n2
nX
i=1
✓ p (1 p)2
p2 ||Xi||2 2 + (1 p) ||Xi||2 2
◆
= 1 p np · 1 n
nX
i=1
||Xi||2 2 .
",5. Communication MSE trade-off,[0],[0]
"Furthermore, the second term can be bounded as
E
2
4 1 n2p2
X i2S (R 1Yi Xi) 2
2
3
5
(a) = 1
n2p2
X i2S E h (R 1Yi Xi) 2 2 i
=
1
n2p2
nX
i=1
E h
(R 1Yi Xi) 2 2 i2S
i
=
1
n2p
nX
i=1
E h",5. Communication MSE trade-off,[0],[0]
"R 1Yi Xi
2 2
i
=
1 n2p E
2
4
nX
i=1
(R 1Yi Xi)
2
2
3
5 = 1 p E(⇡, Xn)
where the last equality follows from the assumption that ⇡’s mean estimate is of the form (6).",5. Communication MSE trade-off,[0],[0]
"(a) follows from the fact that R 1Yi Xi are independent zero mean random variables.
",5. Communication MSE trade-off,[0],[0]
"Combining the above lemma with Theorem 4, and choosing k = p d+ 1 results in the following.",5. Communication MSE trade-off,[0],[0]
Corollary 1.,5. Communication MSE trade-off,[0],[0]
"For every c  nd(2+log 2
(7/4)), there exists a protocol ⇡",5. Communication MSE trade-off,[0],[0]
"such that C(⇡, Sd)  c and
E(⇡, Sd) =",5. Communication MSE trade-off,[0],[0]
"O ✓ min ✓ 1, d
c
◆◆ .",5. Communication MSE trade-off,[0],[0]
The lower bound relies on the lower bounds on distributed statistical estimation due to Zhang et al. (2013).,6. Lower bounds,[0],[0]
"Lemma 9 ((Zhang et al., 2013) Proposition 2).",6. Lower bounds,[0],[0]
"There exists a set of distributions Pd supported on h 1p
d ,",6. Lower bounds,[0],[0]
"1p d
id such
that if any centralized server wishes to estimate the mean of the underlying unknown distribution, then for any independent protocol ⇡
max pd2Pd E  ✓(pd) ˆ✓⇡ 2 2
tmin ✓ 1, d C(⇡) ◆ ,
where C(⇡) is the communication cost of the protocol, ✓(pd) is the mean of pd, and t is a positive constant.",6. Lower bounds,[0],[0]
Theorem 5.,6. Lower bounds,[0],[0]
Let t be the constant in Lemma 9.,6. Lower bounds,[0],[0]
"For every c  ndt/4 and n 4/t,
E(⇧(c), Sd) t 4 min
✓ 1, d
c
◆ .
",6. Lower bounds,[0],[0]
Proof.,6. Lower bounds,[0],[0]
"Given n samples from the underlying distribution where each sample belongs to Sd, it is easy to see that
E ✓(pd) ˆ✓(pd)
2
2
 1 n ,
0 1 2 3 4 5 6 −25
−20
−15
−10
−5
Bits per dimension
lo g
(M S
E )
uniform rotation variable
Figure 1.",6. Lower bounds,[0],[0]
"Distributed mean estimation on data generated from a Gaussian distribution.
",6. Lower bounds,[0],[0]
where ˆ✓(pd) is the empirical mean of the observed samples.,6. Lower bounds,[0],[0]
Let Pd be the set of distributions in Lemma 9.,6. Lower bounds,[0],[0]
"Hence for any protocol ⇡ there exists a distribution pd such that
E ˆ✓(pd) ˆ✓⇡
2
2
(a) 1
2
E ✓(pd) ˆ✓⇡
2
2
E ✓(pd) ˆ✓(pd)
2
2
(b) t
2
min ✓ 1, d C(⇡) ◆ 1 n (c) t 4 min ✓ 1, d C(⇡) ◆ ,
(a) follows from the fact that 2(a b)2 + 2(b c)2 (a c)2.",6. Lower bounds,[0],[0]
"(b) follows from Lemma 9 and (c) follows from the fact that C(⇡, Sd)  ndt/4 and n 4/t. Corollary 1 and Theorem 5 yield Theorem 1.",6. Lower bounds,[0],[0]
We note that the above lower bound holds only for communication cost c < O(nd).,6. Lower bounds,[0],[0]
"Extending the results for larger values of c remains an open problem.
",6. Lower bounds,[0],[0]
"At a first glance it may appear that combining structured random matrix and variable length encoding may improve the result asymptotically, and therefore violates the lower bound.",6. Lower bounds,[0],[0]
"However, this is not true.
",6. Lower bounds,[0],[0]
Observe that variable length coding ⇡svk and stochastic rotated quantization ⇡srk use different aspects of the data: the variable length coding uses the fact that bins with large values of index r are less frequent.,6. Lower bounds,[0],[0]
"Hence, we can use fewer bits to encode frequent bins and thus improve communication.",6. Lower bounds,[0],[0]
In this scheme bin-width (si/(k 1)) is p 2||Xi||2/(k 1).,6. Lower bounds,[0],[0]
Rotated quantization uses the fact that rotation makes the min and max closer to each other and hence we can make bins with smaller width.,6. Lower bounds,[0],[0]
"In such a case, all the bins become more or less equally likely and hence variable length coding does not help.",6. Lower bounds,[0],[0]
"In this scheme bin-width (si/(k 1)) is (Zmaxi Zmini )/(k 1) ⇡ ||Xi||2(log d)/(kd), which is much smaller than bin-width for variable length coding.",6. Lower bounds,[0],[0]
Hence variable length coding and random rotation cannot be used simultaneously.,6. Lower bounds,[0],[0]
"Based on the theoretical analysis, the variable-length coding method provides the lowest quantization error asymptotically when using a constant number of bits.",7. Practical considerations and applications,[0],[0]
"However in practice, stochastic rotated quantization may be preferred due to (hidden) constant factors and the fact that it uses a fixed amount of bits per dimension.",7. Practical considerations and applications,[0],[0]
"For example, considering quantizing the vector [ 1, 1, 0, 0], stochastic rotated
quantization can use 1 bit per dimension and gives zero error, whereas the other two protocols do not.",7. Practical considerations and applications,[0],[0]
"To see this, observe that the naive quantization will quantize 0 to either 1 or 1 and variable length coding cannot achieve 0 error with 1 bit per dimension due to its constant factors.
",7. Practical considerations and applications,[0],[0]
"We further note that the rotated quantization is preferred when applied on “unbalanced” data, due to the fact that the rotation can correct the unbalancedness.",7. Practical considerations and applications,[0],[0]
We demonstrate this by generating a dataset where the value of the last feature dimension entry is much larger than others.,7. Practical considerations and applications,[0],[0]
We generate 1000 datapoints each with 256 dimensions.,7. Practical considerations and applications,[0],[0]
The first 255 dimensions are generated i.i.d.,7. Practical considerations and applications,[0],[0]
"from N(0, 1),",7. Practical considerations and applications,[0],[0]
"and the last dimension is generated from N(100, 1).",7. Practical considerations and applications,[0],[0]
"As shown in Figure 1, the rotated stochastic quantization has the best performance.",7. Practical considerations and applications,[0],[0]
"The improvement is especially significant for low bit rate cases.
",7. Practical considerations and applications,[0],[0]
We demonstrate two applications in the rest of this section.,7. Practical considerations and applications,[0],[0]
"The experiments are performed on the MNIST (d = 1024) and CIFAR (d = 512) datasets.
",7. Practical considerations and applications,[0],[0]
Distributed Lloyd’s algorithm.,7. Practical considerations and applications,[0],[0]
"In the distributed Lloyd’s (k-means) algorithm, each client has access to a subset of data points.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the cluster centers to all the clients.",7. Practical considerations and applications,[0],[0]
"Each client updates the centers based on its local data, and sends the centers back to the server.",7. Practical considerations and applications,[0],[0]
The server then updates the centers by computing the weighted average of the centers sent from all clients.,7. Practical considerations and applications,[0],[0]
"In
the quantized setting, the client compresses the new centers before sending to the server.",7. Practical considerations and applications,[0],[0]
"This saves the uplink communication cost, which is often the bottleneck of distributed learning5.",7. Practical considerations and applications,[0],[0]
We set both the number of centers and number of clients to 10.,7. Practical considerations and applications,[0],[0]
"Figure 2 shows the result.
",7. Practical considerations and applications,[0],[0]
Distributed power iteration.,7. Practical considerations and applications,[0],[0]
Power iteration is a widely used method to compute the top eigenvector of a matrix.,7. Practical considerations and applications,[0],[0]
"In the distributed setting, each client has access to a subset of data.",7. Practical considerations and applications,[0],[0]
"In each iteration, the server broadcasts the current estimate of the eigenvector to all clients.",7. Practical considerations and applications,[0],[0]
"Each client then updates the eigenvector based on one power iteration on its local data, and sends the updated eigenvector back to the server.",7. Practical considerations and applications,[0],[0]
The server updates the eigenvector by computing the weighted average of the eigenvectors sent by all clients.,7. Practical considerations and applications,[0],[0]
"Similar to the above distributed Lloyd’s algorithm, in the quantized setting, the client compresses the estimated eigenvector before sending to the server.",7. Practical considerations and applications,[0],[0]
Figure 3 shows the result.,7. Practical considerations and applications,[0],[0]
"The dataset is distributed over 100 clients.
",7. Practical considerations and applications,[0],[0]
"For both of these applications, variable-length coding achieves the lowest quantization error in most of the settings.",7. Practical considerations and applications,[0],[0]
"Furthermore, for low-bit rate, stochastic rotated quantization is competitive with variable-length coding.
",7. Practical considerations and applications,[0],[0]
"5In this setting, the downlink is a broadcast, and therefore its cost can be reduced by a factor of O(n/ log n) without quantization, where n is the number of clients.",7. Practical considerations and applications,[0],[0]
"We thank Jayadev Acharya, Keith Bonawitz, Dan Holtmann-Rice, Jakub Konecny, Tengyu Ma, and Xiang Wu for helpful comments and discussions.",Acknowledgments,[0],[0]
"Motivated by the need for distributed learning and optimization algorithms with low communication cost, we study communication efficient algorithms for distributed mean estimation.",abstractText,[0],[0]
"Unlike previous works, we make no probabilistic assumptions on the data.",abstractText,[0],[0]
"We first show that for d dimensional data with n clients, a naive stochastic rounding approach yields a mean squared error (MSE) of ⇥(d/n) and uses a constant number of bits per dimension per client.",abstractText,[0],[0]
We then extend this naive algorithm in two ways: we show that applying a structured random rotation before quantization reduces the error to O((log d)/n) and a better coding strategy further reduces the error to O(1/n).,abstractText,[0],[0]
"We also show that the latter coding strategy is optimal up to a constant in the minimax sense i.e., it achieves the best MSE for a given communication cost.",abstractText,[0],[0]
We finally demonstrate the practicality of our algorithms by applying them to distributed Lloyd’s algorithm for kmeans and power iteration for PCA.,abstractText,[0],[0]
Distributed Mean Estimation with Limited Communication,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 12–21, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound. Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",text,[0],[0]
"Distributional models induce vector-based semantic representations of words from their contextual distributions in corpora, exploiting the observation that words with related meanings tend to occur in similar linguistic contexts (Turney and Pantel, 2010; Erk, 2012).",1 Introduction,[0],[0]
"Since the approach only requires raw text as input, it can be used to harvest word representations on a very large scale.",1 Introduction,[0],[0]
"By encoding the rich knowledge that is present in text, these representations are able to capture many aspects of word meaning.",1 Introduction,[0],[0]
"Moreover, approximating semantic similarity by graded geometric distance in a vector space is an effective strategy to address the
many linguistic phenomena that are better characterized in gradient rather than discrete terms, such as synonymy, selectional preferences, and semantic priming (Baroni and Lenci, 2010; Erk et al., 2010; Padó and Lapata, 2007, among others).
",1 Introduction,[0],[0]
"However, not all aspects of human semantic knowledge are satisfactorily captured in terms of fuzzy relations and graded similarity.",1 Introduction,[0],[0]
"In particular, our knowledge of the meaning of words denoting specific entities involves a number of “hard facts” about the referents they denote that are best formalized as attribute-value pairs, of the sort that are stored in manually-curated knowledge bases, such as FreeBase or Wikidata.1 While distributional vectors can capture the useful fact that, say, Italy is in many ways more similar to Spain than to Germany, as humans we also know (or we can easily look up) a set of objective facts about Italy, such as what is its capital, its area, its official language and GDP, that are difficult to express in the language of vector algebra and geometry.
",1 Introduction,[0],[0]
"In this paper, we explore the hypothesis that distributional vectors implicitly encode such attributes of referential entities, which we will call referential attributes.",1 Introduction,[0],[0]
We show that a simple supervised algorithm applied to vectors can retrieve them so that they can be expressed in the explicit language of structured knowledge bases.,1 Introduction,[0],[0]
"Concretely, we train a logistic regression model to predict the values of both numeric and categorical FreeBase attributes of countries and cities from their distributional vectors.",1 Introduction,[0],[0]
"This model makes predictions that are significantly better than an informed baseline, in-between the latter and an upper-bound method.",1 Introduction,[0],[0]
"Qualitative analysis of the results points both to the inherent difficulty of correctly retrieving certain classes of attributes, and to some intriguing properties of the conceptual nature of the knowledge encoded in distributional data, that bias their predictions about certain objective attributes of geographic entities.
",1 Introduction,[0],[0]
"1www.freebase.com, www.wikidata.org.
12
",1 Introduction,[0],[0]
"We see our experiment as a first step towards integrating conceptual and referential aspects of meaning in distributional semantics, as we further discuss in the conclusion.",1 Introduction,[0],[0]
Mikolov et al.’s (2013) skip-gram model is a state-of-the-art “predictive” distributional semantic model which represents each word in a space of latent dimensions optimized to predict the contexts of the word’s occurrences.,2.1 Distributional Representations,[0],[0]
"For our study, we adopt the pre-trained 1,000-dimensional skipgram model for Named Entities that is available at https://code.google.com/p/word2vec/ and was produced from a 100-billion token news corpus.",2.1 Distributional Representations,[0],[0]
We refer to this model as WORD2VEC.,2.1 Distributional Representations,[0],[0]
"As our source of referential attributes, we use FreeBase (see footnote 1), a knowledge base of structured information on a wide range of entities of different semantic types (people, geographical entities, etc.).",2.2 Referential Representations,[0],[0]
"The information in FreeBase comes from various sources, including Wikipedia and domainspecific databases, plus user content generation and correction.",2.2 Referential Representations,[0],[0]
"FreeBase currently records at least 2 attributes for over 47 million entities, and it has been used fairly extensively in NLP before (Mintz et al., 2009; Socher et al., 2013a, among others).
",2.2 Referential Representations,[0],[0]
"For each entity, FreeBase contains a list of attribute-value tuples (where values can in turn be entities, allowing a graph view of the data that we do not exploit here).",2.2 Referential Representations,[0],[0]
Table 1 shows a sample of the attributes that FreeBase records for countries.,2.2 Referential Representations,[0],[0]
"Note that some attributes are simple (e.g., date founded), while other can be called complex, in the sense that they are attributes of attributes (e.g., geolocation::latitude).",2.2 Referential Representations,[0],[0]
We use a double-colon notation to refer to complex attributes.,2.2 Referential Representations,[0],[0]
The values of all attributes can be either numeric or categorical.,2.2 Referential Representations,[0],[0]
"The numeric attributes in particular are often strongly correlated, both within attributes types across years (e.g., fertility rate in different years) and across attributes within years (e.g., absolute GDP and GDP per capita in a given year).
",2.2 Referential Representations,[0],[0]
"We built two datasets for our experiments, one for countries and one for cities, with data automatically extracted from FreeBase.2 We consider two
2Both datasets are publicly available at http:
datasets in order to check that the mapping we seek can be established not just for one, possible handpicked, type of entities; we leave it to future work to study very different kinds of entities, such as people or institutions.
",2.2 Referential Representations,[0],[0]
The Countries dataset consists of the 260 countries for which we have a distributional vector.,2.2 Referential Representations,[0],[0]
"Some countries do not exist anymore, like Yugoslavia, but, since this does not impact our method, we keep them in the dataset.",2.2 Referential Representations,[0],[0]
"The dataset records all simple attributes as well as complex attributes of at most two hops in the FreeBase graph, without manual inspection.",2.2 Referential Representations,[0],[0]
We linearly rescale all numeric attributes to [0..1] and translate all categorical attributes into a binary representation by suffixing the original value to the original attribute name.,2.2 Referential Representations,[0],[0]
"For example, the attribute member-of::organization with the value world bank results in a binary attribute member-of::organization::world bank having value 1 for all and only those countries that are members of the World Bank, 0 for the others.3 Attributes that occur less than 15 times are discarded, since they are either not consistently recorded or rare.",2.2 Referential Representations,[0],[0]
This results in a total of 707 numeric and 247 binary attributes.,2.2 Referential Representations,[0],[0]
"Finally, we partition the data into training, validation, and test set, using a 60-20-20 percent split.
",2.2 Referential Representations,[0],[0]
"We apply the same process to the Cities dataset, which consists of 1645 cities from the intersection of the distributional and FreeBase city lists.",2.2 Referential Representations,[0],[0]
"In
//www.ims.uni-stuttgart.de/forschung/ ressourcen/korpora/CityCountry.html.
3We considered treating some categorical attributes as multi-valued, but decided against it since the cases in which alternative values are mutually exclusive are rare (e.g., the same country can be containedBy multiple entities, cf. Table 1).
",2.2 Referential Representations,[0],[0]
"this case, we have 211 numeric and 106 binary attributes – the numbers are smaller because countries have a richer representation in FreeBase than cities.",2.2 Referential Representations,[0],[0]
"We do zero-shot learning of full FreeBase attributebased country/city representations, based on distributional (WORD2VEC) representations.",2.3 Attribute Prediction,[0],[0]
It is zeroshot learning in the sense of Palatucci et al. (2009):,2.3 Attribute Prediction,[0],[0]
"We split the datasets at the entity, rather than attribute level, such that at test time our system must predict the full attribute set of countries and cities that were not seen during training at all.
",2.3 Attribute Prediction,[0],[0]
We use logistic regression.,2.3 Attribute Prediction,[0],[0]
"In effect, we predict each output variable (FreeBase attribute) with an independent logistic regression model based on a constant set of input features (WORD2VEC distributional dimensions).",2.3 Attribute Prediction,[0],[0]
We call this model DIST2REF.,2.3 Attribute Prediction,[0],[0]
"DIST2REF does not take advantage of the correlations between the output attributes mentioned in Section 2.2.
",2.3 Attribute Prediction,[0],[0]
"The dependent variables are binary as well as numeric FreeBase attributes, and our model does not distinguish between them.",2.3 Attribute Prediction,[0],[0]
"For binary attributes, we interpret the value returned by the model as the probability of “success” of a binary Bernoulli trial.",2.3 Attribute Prediction,[0],[0]
"In the numeric case, we view the probability returned by the model as directly representing normalized attribute values.",2.3 Attribute Prediction,[0],[0]
"We design the model using the Countries dataset, and apply it to Cities without further tuning to test its robustness.",2.4 Experimental Setup,[0],[0]
"We optimize the parameters with gradient descent, using the Cross Entropy error function.",2.4 Experimental Setup,[0],[0]
"We considered L2 regularization to address possible overfitting, but experiments on validation set showed that the model performs best without any regularization.
",2.4 Experimental Setup,[0],[0]
"As for baselines, for binary features we predict the majority class (0 or 1), and for numeric features we predict the mean value of the feature in the training set.",2.4 Experimental Setup,[0],[0]
"These are of course strong baselines to beat.
",2.4 Experimental Setup,[0],[0]
"As an upper bound, we train a model that uses the same architecture as described above but uses as input not distributional vectors but the FreeBase attributes themselves.",2.4 Experimental Setup,[0],[0]
"In other words, this model has to learn “only” an identity mapping.",2.4 Experimental Setup,[0],[0]
"This is not trivial, though, for example due to the presence of strong correlations among attributes, in particular
the time series attributes (cf. Section 2.2).",2.4 Experimental Setup,[0],[0]
We call this model REF2REF.,2.4 Experimental Setup,[0],[0]
"Since there is no appropriate unified evaluation measure that covers both numeric and binary attributes, we evaluate them separately.",2.5 Evaluation,[0],[0]
"For binary attributes, we report the attributes’ mean accuracy.
",2.5 Evaluation,[0],[0]
"For numeric attributes, we consider attribute prediction a ranking task.",2.5 Evaluation,[0],[0]
"As an example, take the population::2011::number attribute, and imagine that we only have three countries (Germany: 80M; Spain: 36M; and Netherlands: 17M).",2.5 Evaluation,[0],[0]
"If we predict 56M for Spain’s population, it is still (correctly) predicted as the second most populous country (rank difference of 0); a prediction of 16M, however, would push Spain to third place (rank difference of 1).
",2.5 Evaluation,[0],[0]
This suggests the use of rank correlation coefficients like Spearman’s ρ.,2.5 Evaluation,[0],[0]
"However, we want to measure not only how well the model can rank the countries in the test set, but also whether these predictions are consistent with the training set (which makes evaluation both more challenging and more realistic).",2.5 Evaluation,[0],[0]
"One way of achieving this goal would be to use ρ on the union of training and test instances, but this could lead to misleadingly high correlation coefficients since this method would include the labels of the training instances in the evaluation.
",2.5 Evaluation,[0],[0]
"Consequently, we define our own evaluation measure, following a rationale similar to Frome et al.’s (2013) evaluation of a zero-shot learning scenario.",2.5 Evaluation,[0],[0]
"What we evaluate, for each attribute, is the rank of the test countries in the whole country list.",2.5 Evaluation,[0],[0]
"Note that this makes our task harder, as there are more confounders: If we only evaluated on the test set, there would be shorter lists and therefore less chances of getting bad rankings.",2.5 Evaluation,[0],[0]
"So, concretely, we first define the prediction quality of each attribute, Q(a), as the median of the rank difference between the prediction and the gold standard in a list that includes both training and test countries (we use the median to give less weight to outlier countries).",2.5 Evaluation,[0],[0]
We also normalize the rank difference to obtain a number between zero and one.,2.5 Evaluation,[0],[0]
"In a second step, we define the quality of the complete model, the normalized rank score (NRS), as the mean of all attribute quality scores, in parallel to our evaluation on binary attributes.
",2.5 Evaluation,[0],[0]
Let the set of instances I be partitioned into training instances Tr and test instances Ts.,2.5 Evaluation,[0],[0]
Let a ∈,2.5 Evaluation,[0],[0]
"A
denote an attribute.",2.5 Evaluation,[0],[0]
We write pa(i) for the predicted value of attribute a for instance i and ga(i) for the gold standard value.,2.5 Evaluation,[0],[0]
"Finally, let r(v, S) denote the rank of value v in the list resulting when ordering the set S.",2.5 Evaluation,[0],[0]
"Now we can define:
Q(a) = 1 ||I||med{|r(pa(i), I)",2.5 Evaluation,[0],[0]
"− (1)
r(ga(i), I)| − 1 | i ∈ Ts} NRS =
1 ||A|| ∑ a∈A Q(a) (2)
",2.5 Evaluation,[0],[0]
"This measure can be interpreted similarly to Mean Reciprocal Rank (Manning et al., 2008):",2.5 Evaluation,[0],[0]
"It has range [0..1], with smaller numbers indicating better ranking: 0.1, for example, means that, on average, the prediction is 10% of the ranks off (e.g., by four countries in a forty-country list).4
Note that, when evaluating each instance i, we use gold-standard values for all other instances, so that there the baseline is not hampered by ties.",2.5 Evaluation,[0],[0]
Table 2 shows the results of our experiments on the two test sets.,3 Results,[0],[0]
"For accuracy 1 is best, but for NRS 0 is best.",3 Results,[0],[0]
"Recall from Section 2.2 that we perform model selection on the Countries dataset only.
",3 Results,[0],[0]
"The baseline is relatively high, in particular for the binary attributes, many of which are positive for a small subset of entities only.",3 Results,[0],[0]
"The amount of skew differs considerably between the two datasets, though.",3 Results,[0],[0]
"For Countries, the baseline yields an accuracy of 0.86, but it achieves 0.97 on Cities.",3 Results,[0],[0]
"The increase stems from very sparse categorical City features such as containedBy, which includes all
4Subtracting 1 in Equation (1) ensures that, when the predicted and gold value of an attribute are adjacent in the ranking, their rank difference is 0, capturing the intuition of rank difference as counting the number of falsely intervening items.
levels of administrative divisions – that is, for the US, all counties appear as values and are transformed into sparse binary features (cf. Section 2.2).",3 Results,[0],[0]
"Of course, the predictions of the baseline are useless, since it always predicts the absence of any features.",3 Results,[0],[0]
"On numeric features, where the baseline predicts the mean, its performance is 0.35 NRS on both datasets.",3 Results,[0],[0]
"In other words, its average prediction is off by about one third the length of the ranked list for each attribute.
",3 Results,[0],[0]
"Recall that the upper bound model, REF2REF, uses FreeBase attributes to predict FreeBase attributes.",3 Results,[0],[0]
All it has to learn is that there is one feature in the input that corresponds ideally to the output.,3 Results,[0],[0]
"This works almost perfectly for binary attributes, with accuracy values of 0.96 (Countries) and 1.00 (Cities).",3 Results,[0],[0]
"However, its performance on numeric features (with NRS at 0.14 and 0.21, respectively) is not quite perfect.",3 Results,[0],[0]
"We attribute this to the presence of correlations (cf. Section 2.2).
",3 Results,[0],[0]
"The model whose performance we are actually interested in, DIST2REF, in which we map from distributional information to FreeBase features, performs with remarkable consistency between these two extremes.",3 Results,[0],[0]
"In fact, we see a consistent error reduction of around 30% over the baseline, with a similar distance to the upper bound.",3 Results,[0],[0]
"A significance test with bootstrap resampling (Efron and Tibshirani, 1994) showed that all pairwise comparisons (Baseline vs. DIST2REF, DIST2REF vs. REF2REF) are statistically significant at p<0.001.
",3 Results,[0],[0]
"To rule out that we misinterpret our accuracybased evaluation for the binary features in the face of a highly skewed class distribution, we also computed precision, recall, and F-Score values.",3 Results,[0],[0]
"The relative patterns match those of the accuracybased evaluation well (Countries: baseline F=0.13, DIST2REF",3 Results,[0],[0]
"F=0.51, REF2REF F=0.77) and indicate that generally precision is higher than recall.
",3 Results,[0],[0]
"We think that these are overall promising results, given that the FreeBase attributes we predict are fairly fine-grained, and we only use generic distributional information as input.",3 Results,[0],[0]
We take the overall results just presented to suggest that we are able to learn referential attributes from distributional information to a large extent.,4 Analysis,[0],[0]
"In this section we take a closer look at what kind of information we are able to learn, what is beyond the scope of our model, and what are the differences between the entity representations in WORD2VEC and the ones our model produces.",4 Analysis,[0],[0]
All the data concerns the test sets only.,4 Analysis,[0],[0]
We start with a qualitative analysis of the Countries dataset.,4.1 Attribute Groups,[0],[0]
"Due to the large number of attributes, we sort all individual attributes into attribute groups by their base name (i.e. the leftmost component of their name, cf. Section 2.2), which offers an accessible level of granularity for inspection.",4.1 Attribute Groups,[0],[0]
"We obtain 34 numeric and 40 binary attribute groups with median sizes of 8.5 and 2 attributes per group, respectively.
",4.1 Attribute Groups,[0],[0]
Table 3 shows the attribute groups for both types sorted by quality.,4.1 Attribute Groups,[0],[0]
"For each group, we report average normalized rank score (NRS) and accuracy, respectively, for both DIST2REF and the baseline.
",4.1 Attribute Groups,[0],[0]
"The analysis suggests that there are two main factors that account for the results: (1) The degree to which an attribute is contextually supported, that is, to what extent its values can be identified on the basis of the contextual information that is captured in a distributional model, and (2) general properties of the data that affect Machine Learning, most notably data sparseness, possibly also feature value distributions.
",4.1 Attribute Groups,[0],[0]
"Attributes that are contextually supported include for instance those related to socioeconomic development (see below for details); people talk (and so write) about countries being more or less developed, rich, having one or another kind of laws, and this is captured in the abstractions over textual context that distributional models perform.",4.1 Attribute Groups,[0],[0]
"As an extreme example of an attribute that is not contextually supported, consider the numeric ISO code of a country (iso numeric), whose values are arbitrary: They do not correspond to facts about the world that are reflected in the way people use lan-
guage, and so can’t be picked up by the distributional model.",4.1 Attribute Groups,[0],[0]
"For this reason, DIST2REF does worse than the baseline.
",4.1 Attribute Groups,[0],[0]
"Note that, in a sufficiently large corpus, we might indeed encounter statements like The numeric ISO code for Spain is 724.",4.1 Attribute Groups,[0],[0]
"However, since distributional models represent words as aggregated distributions of their contexts, and compute semantic similarity from these context distributions, the contexts that they use need to be generic enough to yield meaningful overlap between concepts (e.g., words).",4.1 Attribute Groups,[0],[0]
"As a result, distributional models cannot easily represent knowledge of the form “the value for property Y of word/concept X is Z”.
",4.1 Attribute Groups,[0],[0]
"Fortunately, we find that many FreeBase attributes are contextually supported to a substantial degree, even some seemingly arbitrary ones.",4.1 Attribute Groups,[0],[0]
"An example is calling codes, which we predict very well.",4.1 Attribute Groups,[0],[0]
"They turn out to be correlated with geolocations: 2X calling codes are located in Africa, 3X calling codes in Southern and Eastern Europe and 4X calling codes in Western and Northern Europe (for comparison, ISO codes are assigned in a roughly alphabetical order).
",4.1 Attribute Groups,[0],[0]
Numeric Attributes.,4.1 Attribute Groups,[0],[0]
Our best numeric attributes belong to the geolocation group (latitude and longitude).,4.1 Attribute Groups,[0],[0]
We provide a more detailed analysis of these attributes below (Section 4.2).,4.1 Attribute Groups,[0],[0]
"As mentioned above, we also excel at many attributes related to a country’s economic and social development (broadly construed), such as GNI, GDP, CO2 emissions, internet usage (each per capita), or fertility rate.",4.1 Attribute Groups,[0],[0]
"These attributes can be expected to be contextually grounded – e.g., Luxembourg will occur with contexts like “broadband” or “rich” more than India.
",4.1 Attribute Groups,[0],[0]
"Note, however, that the information contained in the vectors is surprisingly subtle: For instance, the fertility rate is a function of both general development status (lower rates in more developed countries) and of specific social factors (higher rates in countries with more support for families, such as France and Finland compared countries with less support, such as Germany or Italy).
",4.1 Attribute Groups,[0],[0]
"Around the middle of the table, we find the absolute versions of the developmental cluster above (GNI in $, real and nominal GDP).",4.1 Attribute Groups,[0],[0]
"Evidently, the absolute versions of these attributes are substantially less contextually supported than the relative versions.",4.1 Attribute Groups,[0],[0]
"This is not surprising: While India and China have high absolute GDPs because they are
large countries, and for instance Luxembourg has a much smaller one, these numbers are not indicative of the actual conditions in these countries, and therefore also not so clearly correlated with what people write about them.",4.1 Attribute Groups,[0],[0]
This provides another interesting angle on the difference between distributional and formal knowledge representation.,4.1 Attribute Groups,[0],[0]
"In a formal system, absolute GDP, relative GDP, and population stand in a fixed linear relationship and knowing any two of the three uniquely determines the third – thus, all three attributes have equal status.",4.1 Attribute Groups,[0],[0]
"In our distributional space, their status is clearly
different, determined by the conceptual relevance of the different attributes.
",4.1 Attribute Groups,[0],[0]
"Towards the end of the table, we find more attributes related to socioeconomic development, such as government percent debt and minimum wage.",4.1 Attribute Groups,[0],[0]
"While these should be contextually supported, too, the problem here is factor (2) mentioned above, namely severe data sparsity (see column f(A) in Table 3, which lists the median number of datapoints that exhibit each attribute group).",4.1 Attribute Groups,[0],[0]
"The same goes for the remaining attribute groups, for instance casualties (describing the
total number of military casualties incurred in history), date founded and date dissolved,5 or climate avg rainfall.
",4.1 Attribute Groups,[0],[0]
Binary Attributes.,4.1 Attribute Groups,[0],[0]
"The binary attributes show a similar picture, albeit somewhat less sharp.",4.1 Attribute Groups,[0],[0]
"We again find contextually unsupported groups, many of them arising from our fully automatic attribute mining from FreeBase (cf. Section 2.2).",4.1 Attribute Groups,[0],[0]
"There are many categorical attributes that store metadata about numeric attributes (such as the currency in the gdp and gni groups) as well as meta-information of FreeBase: exceptions is a specific marker of potentially inconsistent entries about Ghana, and equivalent instances is a flag concerning links between FreeBase and OpenCyc.",4.1 Attribute Groups,[0],[0]
"Fortunately, almost all contextually unsupported groups are small, with only one or two attributes, and do not have a large impact on the overall performance.",4.1 Attribute Groups,[0],[0]
"We decided not to exclude them from evaluation for robustness’ sake, since there is no automatic way to identify contextually unsupported attributes in a new dataset.
",4.1 Attribute Groups,[0],[0]
"We obtain good results on meaningful attributes that are arguably strongly contextually grounded, such as geographical and geopolitical attributes (member of: membership in international organizations; location on a continent, etc.).",4.1 Attribute Groups,[0],[0]
"However, we fare relatively badly on government-related attributes (form of government, governing officials).",4.1 Attribute Groups,[0],[0]
"While this seems surprising at first glance, the form of government attribute in FreeBase makes very fine-grained distinctions: Its values include “unitary state”, “presidential system”, “parliamentary system” and “republic”, which are not mutually exclusive, and misses obvious alternatives like “authoritarian system”.",4.1 Attribute Groups,[0],[0]
It is not surprising that distributional models cannot make such subtle distinction between presidential and parliamentary systems.,4.1 Attribute Groups,[0],[0]
The attribute governing official presents a similar case.,4.1 Attribute Groups,[0],[0]
"Other bad attributes are very domain-specific, including athletes, encoding the athletic disciplines that countries participate in (such as swimming, judo, running, etc.), and the data sparsity issue is certainly worse for the binary attributes.
",4.1 Attribute Groups,[0],[0]
"5Note that date-based attributes can be contextually supported: We do better on national anthem since, for which we have more datapoints, 97.",4.1 Attribute Groups,[0],[0]
"To analyze the difference between the distributional representations and the output of our model, we focus on geolocation, our best attribute group.
",4.2 Geolocation,[0],[0]
"It has already been shown that geometric distance in distributional space captures, to a certain extent, physical distance between locations in the real world (Louwerse and Zwaan, 2009).",4.2 Geolocation,[0],[0]
Table 4 shows that DIST2REF extracts even more precise distance information from distributional vectors.,4.2 Geolocation,[0],[0]
The table reports the correlation between real and model-predicted distances for countries and cities.,4.2 Geolocation,[0],[0]
"Ground-truth great circle distances (Kern and Bland, 1948) between items are computed using the FreeBase longitude and latitude values; for DIST2REF we use its predicted latitude and longitude values; for WORD2VEC, the cosines between the corresponding distributional vectors.
",4.2 Geolocation,[0],[0]
"We obtain highly significant correlations in all cases (p<10−14), but much higher for DIST2REF.",4.2 Geolocation,[0],[0]
"For countries, as shown in Table 4, the correlation is -0.36 for WORD2VEC (negative, because cosine is a similarity measure), 0.49 for DIST2REF.",4.2 Geolocation,[0],[0]
"For cities, WORD2VEC reaches -0.45 correlation, and DIST2REF distances are at 0.88, showing that the method can estimate city positions to a perhaps unexpectedly high degree of accuracy.6
This result suggests that we manage to objectify the information in the distributional model, anchoring the entities more firmly in the external world.",4.2 Geolocation,[0],[0]
"Indeed, distributional models are known to be subject to conceptual or cultural effects in their distance estimations.",4.2 Geolocation,[0],[0]
"For instance, in WORD2VEC German and Spanish cities are much farther away than in the physical world, while cities within Spain and within Germany are predicted to be a bit closer than they actually are.",4.2 Geolocation,[0],[0]
"Note that these effects have
6The results are confirmed when the analysis is repeated using the Spearman correlation measure: The DIST2REF coefficients are stable, whereas those of WORD2VEC go down to 0.22 (countries) and 0.40 (cities), respectively.",4.2 Geolocation,[0],[0]
"The good results for Spearman, as a rank-based measure, indicate that our success is not dominated by outliers.
",4.2 Geolocation,[0],[0]
"an actual cognitive basis: Human intuitions about objective physical distance between countries and cities are biased by cognitive, cultural and socioeconomic factors, as explored for example in Friedman et al. (2002), who report that Texans locate Canadian cities closer to the US border relative to Mexican cities, despite their proximity to the latter, and that they place Southern US cities further south than they really are.
",4.2 Geolocation,[0],[0]
"Interestingly, DIST2REF does also show some cultural effects in its geolocation errors: For example, some Pacific island states with lesser-known identities (e.g., Nauru and French Polynesia) are placed in the Indian Ocean, where we find the perhaps prototypes of beautiful islands, like Seychelles and Mauritius; also, Central American countries (such as Panama, El Salvador, and Nicaragua) move towards their “cultural center of gravity”, South America.
",4.2 Geolocation,[0],[0]
"However, this kind of cultural bias is much more prominent in the original WORD2VEC distributional representation.",4.2 Geolocation,[0],[0]
The Spain/Germany effect discussed above is not found in the DIST2REF model at all.,4.2 Geolocation,[0],[0]
"And while both DIST2REF and WORD2VEC place Mexican and Spanish cities in our test set closer to each other than they actually are, WORD2VEC does so to a much larger extent.",4.2 Geolocation,[0],[0]
"In line with our goal to extract referential attributes, thus, we are satisfied to see that DIST2REF manages to minimize this bias and distill the referential part from the distributional representations.",4.2 Geolocation,[0],[0]
"There is a large literature on exploiting corpus evidence, sometimes through distributional semantic methods, in order to construct and populate structured knowledge bases (KBs) (e.g., Buitelaar and Cimiano (2008) and references therein).",5 Related Work,[0],[0]
"This line of work, however, does not attempt to connect entity representations extracted from corpora and from KBs, as we do.",5 Related Work,[0],[0]
"Moreover, it focuses on harvesting relations between entities or between entities and a limited number of discrete attributes, rather than predicting full-fledged KB representations of specific entities, like we do.",5 Related Work,[0],[0]
Freitas and Curry (2014) and Freitas et al. (2014) embed relational graphs from KBs in a distributional semantic space to support various forms of search and reasoning about the KB.,5 Related Work,[0],[0]
"The focus is again on relations between discrete entities, and on exploiting distributional semantics to navigate among them.
",5 Related Work,[0],[0]
Socher et al. (2013a) represent WordNet and FreeBase entities with corpus-based distributional vectors.,5 Related Work,[0],[0]
They train a tensor for each relation of interest to return high scores when combined with the vectors of two entities that hold the intended relation.,5 Related Work,[0],[0]
"At test time, the system is used to classify relational tuples as true or false, as well as to predict new entities that hold a certain relationship with a target entity.",5 Related Work,[0],[0]
"This is quite close in spirit to what we do, except that, given an entity1-relation-entity2 tuple, we treat relation-entity2 as a binary attribute of entity1, and we try to induce such attributes on a larger scale (Socher et al. consider seven relations in total).",5 Related Work,[0],[0]
"Moreover, we rely on the same architecture to learn discrete features denoting relations with entities and numerical features, to induce full attribute-based descriptions of entities.
",5 Related Work,[0],[0]
"Our proposal is only distantly related to methods to embed words tokens and KB entities and relationships in a vector space, e.g., for better relation extraction (see Weston et al. (2013) and references therein).",5 Related Work,[0],[0]
"This line of work does not use distributional semantics to induce word vectors, and ignores numerical attributes.
",5 Related Work,[0],[0]
The broader goal of getting at referential information with distributional semantics is shared with Herbelot (2015).,5 Related Work,[0],[0]
"However, the specific approach is different, as she constructs vectors for individual entities (literary characters) by contextualizing generic noun vectors with distributional properties of those entities.",5 Related Work,[0],[0]
"Finally, we share our methodology with work on mapping between corpus-based word representations and other representational spaces, such as subject-generated concept properties (Johns and Jones, 2012; Hill et al., 2014; Făgărăşan",5 Related Work,[0],[0]
"et al., 2015), visual features (Frome et al., 2013; Socher et al., 2013b; Lazaridou et al., 2014) or brain signals (Mitchell et al., 2008; Murphy et al., 2012).",5 Related Work,[0],[0]
"In all these settings, the focus is entirely on predicting numerical attributes, whereas we treat both numerical and binary attributes.",5 Related Work,[0],[0]
"Rubinstein et al. (2015) use distributional vectors to predict binary conceptual attributes of common nouns, as well as a continuous score measuring saliency of such attributes.",5 Related Work,[0],[0]
Our target features are conceptually very different from those of all these studies.,5 Related Work,[0],[0]
"We have shown that a simple model can learn to predict, to a reasonable degree of accuracy, ref-
erential attributes of an entity that are typically seen in a knowledge base from the corresponding corpus-based distributional representation.",6 Discussion and Conclusion,[0],[0]
"The results suggest that, while distributional semantic vectors can be used “as-is” to capture generic word similarity, with some supervision it is also possible to extract other kinds of information from them, including structured factual statements of the sort encoded in manually-curated knowledge bases.",6 Discussion and Conclusion,[0],[0]
"This makes distributional vectors very attractive as general-purpose word meaning representations.
",6 Discussion and Conclusion,[0],[0]
"We have also shown that some of the errors in the predictions can be explained on cultural grounds, but that these effects are more pronounced in the input of our model, a standard distributional semantic model, than in its output.",6 Discussion and Conclusion,[0],[0]
"In this sense, our model manages to objectify the information that it is provided with.",6 Discussion and Conclusion,[0],[0]
"Our analyses also suggest that the main limiting factor in learning referential attributes, apart from good old data sparseness, is the degree to which they are contextually supported, that is, to what extent they are expressed with consistent and specific linguistic means in the context of their target words.",6 Discussion and Conclusion,[0],[0]
"This determines whether they are actually represented in the distributional model in the first place.
",6 Discussion and Conclusion,[0],[0]
"More generally, we see our work as a small step towards the more general goal of bridging the concept-referent gap in distributional semantics.",6 Discussion and Conclusion,[0],[0]
"A common noun such as dog denotes a concept, based on a prototype with fuzzy boundaries, susceptible of metaphorical extensions, and bearing all the other hallmarks of generic conceptual knowledge (Carlson, 2009; Murphy, 2002).",6 Discussion and Conclusion,[0],[0]
These might be adequately captured by the properties of the dog vector in distributional semantic space.,6 Discussion and Conclusion,[0],[0]
"However, when used in a specific discourse, words and more complex linguistic expressions often denote specific referents with fixed, “hard” properties, such as this dog, or Amur, when used for my neighbor’s dog at 3.31pm on May 29th 2015 in Novosibirsk, a 61cm-tall black-and-tan foxhound.",6 Discussion and Conclusion,[0],[0]
Amur is more easily characterized by a set of precise attributevalue pairs than by a vector in a generic conceptual space.,6 Discussion and Conclusion,[0],[0]
Our experiment suggests that distributional vectors encode both generic conceptual knowledge and more precise attributes of specific referents.,6 Discussion and Conclusion,[0],[0]
"Of course, while we can use FreeBase and other knowledge bases to gather training data about public-domain entities, such as countries or cities, it is still not clear where we could gather
appropriate training data to learn about the specific properties of “private-discourse” referents such as Amur.",6 Discussion and Conclusion,[0],[0]
"Moreover, it remains to be seen whether the properties of common named entities, such as countries and cities, that are in a sense “hybrid” between the conceptual and referential domains, also transfer to entities of a more specific and private kind.",6 Discussion and Conclusion,[0],[0]
"Finally, it is still not clear how to extend the current approach beyond words and phrases directly denoting an entity (Amur) to other kinds of definite descriptions (this dog).
",6 Discussion and Conclusion,[0],[0]
"Acknowledgments: This project has received funding from the European Union’s Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No 655577 (LOVe); ERC 2011 Starting Independent Research Grant n. 283554 (COMPOSES); DFG (SFB 732, Project D10); and Spanish MINECO (grant FFI2013-41301-P).",6 Discussion and Conclusion,[0],[0]
"This paper reflects the authors’ view only, and the EU is not responsible for any use that may be made of the information it contains.",6 Discussion and Conclusion,[0],[0]
"Special thanks to Christian Scheible for help with the Machine Learning part, to the anonymous reviewers for insightful and constructive feedback, and to the FLOSS reading group for helping us shape our ideas on the topic of this paper.",6 Discussion and Conclusion,[0],[0]
"Distributional methods have proven to excel at capturing fuzzy, graded aspects of meaning (Italy is more similar to Spain than to Germany).",abstractText,[0],[0]
"In contrast, it is difficult to extract the values of more specific attributes of word referents from distributional representations, attributes of the kind typically found in structured knowledge bases (Italy has 60 million inhabitants).",abstractText,[0],[0]
"In this paper, we pursue the hypothesis that distributional vectors also implicitly encode referential attributes.",abstractText,[0],[0]
"We show that a standard supervised regression model is in fact sufficient to retrieve such attributes to a reasonable degree of accuracy: When evaluated on the prediction of both categorical and numeric attributes of countries and cities, the model consistently reduces baseline error by 30%, and is not far from the upper bound.",abstractText,[0],[0]
"Further analysis suggests that our model is able to “objectify” distributional representations for entities, anchoring them more firmly in the external world in measurable ways.",abstractText,[0],[0]
Distributional vectors encode referential attributes,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1063–1072 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1098
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion. On the other hand, query-based summarization highlights those points that are relevant in the context of a given query. The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc. But it suffers from the drawback of generation of repeated phrases. In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary. In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia. Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.",text,[0],[0]
"Over the past few years neural models based on the encode-attend-decode (Bahdanau et al.,
2014) paradigm have shown great success in various natural language generation (NLG) tasks such as machine translation (Bahdanau et al., 2014), abstractive summarization ((Rush et al., 2015),(Nallapati et al., 2016)) dialog (Li et al., 2016), etc.",1 Introduction,[0],[0]
One such NLG problem which has not received enough attention in the past is query based abstractive text summarization where the aim is to generate the summary of a document in the context of a query.,1 Introduction,[0],[0]
"In general, abstractive summarization, aims to cover all the salient points of a document in a compact and coherent fashion.",1 Introduction,[0],[0]
"On the other hand, query focused summarization highlights those points that are relevant in the context of the query.",1 Introduction,[0],[0]
"Thus given a document on “the super bowl”, the query “How was the half-time show?”, would result in a summary that would not cover the actual game itself.
",1 Introduction,[0],[0]
Note that there has been some work on query based extractive summarization in the past where the aim is to simply extract the most salient sentence(s) from a document and treat these as a summary.,1 Introduction,[0],[0]
There is no natural language generation involved.,1 Introduction,[0],[0]
"Since, we were interested in abstractive (as opposed to extractive) summarization we created a new dataset based on debatepedia.",1 Introduction,[0],[0]
"This dataset contains triplets of the form (query, document, summary).",1 Introduction,[0],[0]
"Further, each summary is abstractive and not extractive in the sense that the summary does not necessarily comprise of a sentence which is simply copied from the original document.
",1 Introduction,[0],[0]
"Using this dataset as a testbed, we focus on a recurring problem in models based on the encode-attend-decode paradigm.",1 Introduction,[0],[0]
"Specifically, it is observed that the summaries produced by such models contain repeated phrases.",1 Introduction,[0],[0]
"Table 1 shows a few such examples of summaries gener-
1063
ated by such a model when trained on this new dataset.",1 Introduction,[0],[0]
"This problem has also been reported by (Chen et al., 2016) in the context of summarization and by (Sankaran et al., 2016) in the context of machine translation.
",1 Introduction,[0],[0]
We first provide an intuitive explanation for this problem and then propose a solution for alleviating it.,1 Introduction,[0],[0]
A typical encode-attend-decode model first computes a vectorial representation for the document and the query and then produces a contextual summary one word at a time.,1 Introduction,[0],[0]
Each word is produced by feeding a new context vector to the decoder at each time step by attending to different parts of the document and query.,1 Introduction,[0],[0]
"If the decoder produces the same word or phrase repeatedly then it could mean that the context vectors fed to the decoder at these time steps are very similar.
",1 Introduction,[0],[0]
We propose a model which explicitly prevents this by ensuring that successive context vectors are orthogonal to each other.,1 Introduction,[0],[0]
"Specifically, we subtract out any component that the
current context vector has in the direction of the previous context vector.",1 Introduction,[0],[0]
"Notice that, we do not require the current context vector to be orthogonal to all previous context vectors but just its immediate predecessor.",1 Introduction,[0],[0]
This enables the model to attend to words repeatedly if required later in the process.,1 Introduction,[0],[0]
"To account for the complete history (or all previous context vectors) we also propose an extension of this idea where we pass the sequence of context vectors through a LSTM (Hochreiter and Schmidhuber, 1997) and ensure that the current state produced by the LSTM is orthogonal to the history.",1 Introduction,[0],[0]
"At each time step, the state of the LSTM is then fed to the decoder to produce one word in the summary.
",1 Introduction,[0],[0]
Our contributions can be summarized as follows: (i) We propose a new dataset for query based abstractive summarization and evaluate encode-attend-decode models on this dataset (ii) We study the problem of repeating phrases in NLG in the context of this dataset and propose two solutions for countering this problem.,1 Introduction,[0],[0]
We show that our method outperforms a vanilla encoder-decoder model with a gain of 28% (absolute) in ROUGE-L score (iii) We also demonstrate that our method clearly outperforms a recent state of the art method proposed for handling the problem of repeating phrases with a gain of 7% (absolute) in ROUGE-L scores (iv) We do a qualitative analysis of the results and show that our model indeed produces outputs with fewer repetitions.,1 Introduction,[0],[0]
"Summarization has been studied in the context of text ((Mani, 2001), (Das and Martins, 2007), (Nenkova and McKeown, 2012)) as well as speech ((Zhu and Penn, 2006), (Zhu et al., 2009)).",2 Related Work,[0],[0]
"A vast majority of this work has focused on extractive summarization where the idea is to construct a summary by selecting the most relevant sentences from the document ((Neto et al., 2002), (Erkan and Radev, 2004), (Filippova and Altun, 2013), (Colmenares et al., 2015), (Riedhammer et al., 2010), (Ribeiro et al., 2013)).",2 Related Work,[0],[0]
There has been some work on abstractive summarization in the context of DUC-2003 and DUC-2004 contests (Zajic et al.).,2 Related Work,[0],[0]
"We refer the reader to (Das and Martins, 2007) and (Nenkova and McKeown, 2012) for an excellent survey of
the field.",2 Related Work,[0],[0]
"Recent research in abstractive summarization has focused on data driven neural models based on the encode-attend-decode paradigm (Bahdanau et al., 2014).",2 Related Work,[0],[0]
"For example, (Rush et al., 2015), report state of the art results on the GigaWord and DUC corpus using such a model.",2 Related Work,[0],[0]
"Similarly, the work of Lopyrev (2015) uses neural networks to generate news headline from short news stories.",2 Related Work,[0],[0]
Chopra et al. (2016) extend the work of Rush et al. (2015) and report further improvements on the two datasets.,2 Related Work,[0],[0]
"Hu et al. (2015) introduced a dataset for Chinese short text summarization and evaluated a similar RNN encoder-decoder model on it.
",2 Related Work,[0],[0]
One recurring problem in encoder-decoder models for NLG is that they often repeat the same phrase/word multiple times in the summary (at the cost of both coherency and fluency).,2 Related Work,[0],[0]
Sankaran et al. (2016) study this problem in the context of MT and propose a temporal attention model which enforces the attention weights for successive time steps to be different from each other.,2 Related Work,[0],[0]
"Similarly, and more relevant to this work, Chen et al. (2016) propose a distraction based attention model which maintains a history of attention vectors and context vectors.",2 Related Work,[0],[0]
It then subtracts this history from the current attention and context vector.,2 Related Work,[0],[0]
When evaluated on our dataset their method performs poorly.,2 Related Work,[0],[0]
This could be because their method is very aggressive in dealing with the history (as explained later in the Experiments section).,2 Related Work,[0],[0]
"On the other hand, our method has a better way of handling history (by passing context vectors through an LSTM recurrent network) which gives us the flexibility to forget/retain some portions of the history and at the same time produce diverse context vectors at successive time steps.
",2 Related Work,[0],[0]
We evaluate our method in the context of query based abstractive summarization - a problem which has received almost no attention in the past due to unavailability of datasets.,2 Related Work,[0],[0]
We create a new dataset for this task and show that our method indeed produces better output by reducing the number of repeated phrases produced by encoder decoder models.,2 Related Work,[0],[0]
"As mentioned earlier, there are no existing datasets for query based abstractive summarization.",3 Dataset,[0],[0]
We create such a dataset from Debatepedia an encyclopedia of pro and con arguments and quotes on critical debate topics.,3 Dataset,[0],[0]
There are 663 debates in the corpus (we have considered only those debates which have at least one query with one document).,3 Dataset,[0],[0]
"These 663 debates belong to 53 overlapping categories such as Politics, Law, Crime, Environment, Health, Morality, Religion, etc.",3 Dataset,[0],[0]
A given topic can belong to more than one category.,3 Dataset,[0],[0]
"For example, the topic “Eye for an Eye philosophy” belongs to both “Law” as well as “Morality”.",3 Dataset,[0],[0]
The average number of queries per debate is 5 and the average number of documents per query is 4.,3 Dataset,[0],[0]
"Please refer to the dataset url1 for more details about number of debates per category.
",3 Dataset,[0],[0]
"For example, Figure 1 shows the queries associated with the topic “Algae Biofuel”.",3 Dataset,[0],[0]
It also lists the set of documents and an abstractive summary associated with each query.,3 Dataset,[0],[0]
"As is obvious from the example, the summary is an abstractive summary and not extracted directly from the document.",3 Dataset,[0],[0]
"We crawled 12695 such {query, document, summary} triples from debatepedia (these were all the triples that were available).",3 Dataset,[0],[0]
"Table 2 reports the average length of the query, summary and documents in this dataset.
",3 Dataset,[0],[0]
We used 10 fold cross validation for all our experiments.,3 Dataset,[0],[0]
"Each fold uses 80% of the documents for training, 10% for validation and 10% for testing.",3 Dataset,[0],[0]
"Given a query q = q1, q2, ..., qk containing k words, a document d = d1, d2, ..., dn containing n words, the task is to generate a contextual summary y = y1, y2, ..., ym containing
1http://www.cse.iitm.ac.in/˜miteshk/ datasets/qbas.html
Figure 1: Queries associated with the topic “algae biofuel”
Figure 2: Documents and summaries for a given query
m words.",4 Proposed model,[0],[0]
"This can be modeled as the problem of finding a y∗ that maximizes the probability p(y|q,d) which can be further decomposed as:
y∗ = argmax y
m∏
t=1
p(yt|y1, ..., yt−1,q,d) (1)
We now describe a way of modeling p(yt|y1, ..., yt−1,q,d) using the neural encoderattention-decoder paradigm.",4 Proposed model,[0],[0]
The proposed model contains the following components: (i) an encoder RNN for the query (ii) an encoder RNN for the document (iii) attention mechanism for the query (iv) attention mechanism for the document and (v) a decoder RNN.,4 Proposed model,[0],[0]
All the RNNs use a GRU cell.,4 Proposed model,[0],[0]
Encoder for the query: We use a recurrent neural network with Gated Recurrent Units (GRU) for encoding the query.,4 Proposed model,[0],[0]
"It reads the query q = q1, q2, ..., qk from left to right and computes a hidden representation for each time-step as:
hqi = GRUq(h q i−1, e(qi))",4 Proposed model,[0],[0]
"(2)
where e(qi) ∈ Rd is the d-dimensional embedding of the query word qi.",4 Proposed model,[0],[0]
Encoder for the document:,4 Proposed model,[0],[0]
"This is similar to the query encoder and reads the document d = d1, d2, ..., dn from left to right and computes a hidden representation for each time-step as:
hdi = GRUd(h d i−1, e(di)) (3)
where e(di) ∈ Rd is the d-dimensional embedding of the document word di.",4 Proposed model,[0],[0]
"Attention mechanism for the query : At each time step, the decoder produces an output word
by focusing on different portions of the query (document) with the help of a query (document) attention model.",4 Proposed model,[0],[0]
"We first describe the query attention model which assigns weights αqt,i to each word in the query at each decoder timestep using the following equations.
",4 Proposed model,[0],[0]
"aqt,i = v T q tanh(Wqst +",4 Proposed model,[0],[0]
"Uqh q i ) (4) αqt,i = exp(aqt,i)∑k j=1 exp(a q t,j) (5)
where st is the current state of the decoder at time step t (we will see an exact formula for this soon).",4 Proposed model,[0],[0]
"Wq ∈ Rl2×l1 , Uq ∈ Rl2×l2 , vq ∈ Rl2 , l1 is the size of the decoder’s hidden state, l2 is both the size of hqi and also the size of the final query representation at time step t, which is computed as:
qt = k∑
i=1
αqt,ih q i (6)
Attention mechanism for the document : We now describe the document attention model which assigns weights to each word in the document using the following equations.
",4 Proposed model,[0],[0]
"adt,i = v T d",4 Proposed model,[0],[0]
tanh(Wdst,4 Proposed model,[0],[0]
+,4 Proposed model,[0],[0]
"Udh d i + Zqt) (7) αdt,i = exp(adt,i)∑n j=1 exp(a d t,j)
where st is the current state of the decoder at time step t (we will see an exact formula for this
soon).",4 Proposed model,[0],[0]
"Wd ∈ Rl4×l1 , Ud ∈ Rl4×l4 , Z ∈",4 Proposed model,[0],[0]
"Rl4×l2 , vd ∈ Rl2 , l4 is the size of hdi and also the size of the final document representation dt which is passed to the decoder at time step t as:
dt = n∑
i=1
αdt,ih d i (8)
Note that dt now encodes the relevant information from the document as well as the query (see Equation (7)) at time step t. We refer to this as the context vector for the decoder.",4 Proposed model,[0],[0]
"Decoder: The hidden state of the decoder st at each time t is again computed using a GRU as follows:
st = GRUdec(st−1, [e(yt−1), dt−1]) (9)
where, yt−1 gives a distribution over the vocabulary words at timestep t − 1 and is computed as:
yt = softmax(Wof(Wdecst + Vdecdt)) (10)
where Wo ∈ RN×l1 , Wdec ∈ Rl1×l1 , Vdec ∈ Rl1×l4 , N is the vocabulary size, yt is the final output of the model which defines a probability distribution over the output vocabulary.",4 Proposed model,[0],[0]
"This is exactly the quantity defined in Equation (1) that we wanted to model (p(yt|y1, ..., yt−1,q,d)).",4 Proposed model,[0],[0]
"Further, note that, e(yt−1) is the d-dimensional embedding of the word which has the highest probability under the distribution yt−1.",4 Proposed model,[0],[0]
"Also [e(yt−1), dt−1] means a concatenation of the vectors e(yt−1), dt−1.",4 Proposed model,[0],[0]
"We chose f to be the identity function.
",4 Proposed model,[0],[0]
The model as described above is an instantiation of the encoder-attention-decoder idea applied to query based abstractive summarization.,4 Proposed model,[0],[0]
"As mentioned earlier (and demonstrated later through experiments), this model suffers from the problem of repeating the same phrase/word in the output.",4 Proposed model,[0],[0]
We now propose a new attention model which we refer to as diversity based attention model to address this problem.,4 Proposed model,[0],[0]
"As hypothesized earlier, if the decoder produces the same phrase/word multiple times then it is possible that the context vectors being fed to the decoder at consecutive time steps are
very similar.",4.1 Diversity based attention model,[0],[0]
"We propose four models (D1, D2, SD1, SD2) to directly address this problem.",4.1 Diversity based attention model,[0],[0]
D1:,4.1 Diversity based attention model,[0],[0]
"In this model, after computing dt as described in Equation (8), we make it orthogonal to the context vector at time t− 1:
d ′",4.1 Diversity based attention model,[0],[0]
"t = dt −
dTt",4.1 Diversity based attention model,[0],[0]
d ′,4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′ t−1 (11)
SD1: The above model imposes a hard orthogonality constraint on the context vector(d ′ t).",4.1 Diversity based attention model,[0],[0]
We also propose a relaxed version of the above model which uses a gating parameter.,4.1 Diversity based attention model,[0],[0]
"This gating parameter decides what fraction of the previous context vector should be subtracted from the current context vector using the following equations:
γt = Wgdt−1 + bg
d ′",4.1 Diversity based attention model,[0],[0]
t = dt,4.1 Diversity based attention model,[0],[0]
"− γt
dTt d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′T t−1d ′",4.1 Diversity based attention model,[0],[0]
"t−1
d ′",4.1 Diversity based attention model,[0],[0]
"t−1
where Wg ∈ Rl4×l4 , bg ∈ Rl4 , l4 is the dimension of dt as defined in equation (8).",4.1 Diversity based attention model,[0],[0]
D2:,4.1 Diversity based attention model,[0],[0]
The above model only ensures that the current context vector is diverse w.r.t the previous context vector.,4.1 Diversity based attention model,[0],[0]
It ignores all history before time step t − 1.,4.1 Diversity based attention model,[0],[0]
"To account for the history, we treat successive context vectors as a sequence and use
a modified LSTM cell to compute the new state at each time step.",4.1 Diversity based attention model,[0],[0]
"Specifically, we use the following set of equations to compute a diverse context at time t:
it = σ(Widt + Uiht−1 + bi)
ft = σ(Wfdt + Ufht−1 + bf )
ot = σ(Wodt + Uoht−1 + bo)
ĉt = tanh(Wcdt + Ucht−1 + bc)
ct = it ĉt + ft ct−1
cdiverset",4.1 Diversity based attention model,[0],[0]
=,4.1 Diversity based attention model,[0],[0]
ct,4.1 Diversity based attention model,[0],[0]
"− ct T ct−1 cTt−1ct−1 ct−1 (12)
ht = ot tanh(cdiverset ) d ′",4.1 Diversity based attention model,[0],[0]
t = ht,4.1 Diversity based attention model,[0],[0]
"(13)
where Wi,Wf ,Wo,Wc ∈ Rl5×l4 , Ui, Uf , Uo, Uc ∈ Rl5×l4 , dt is the l4dimensional output of Equation (8); l5 is number of hidden units in the LSTM cell.",4.1 Diversity based attention model,[0],[0]
This final d ′ t from Equation (13) is then used in Equation (9).,4.1 Diversity based attention model,[0],[0]
Note that Equation (12) ensures that state of the LSTM at time step t is orthogonal to the previous history.,4.1 Diversity based attention model,[0],[0]
Figure 3 shows a pictorial representation of the model with a diversity LSTM cell.,4.1 Diversity based attention model,[0],[0]
SD2: This model again uses a relaxed version of the orthogonality constraint used in D2.,4.1 Diversity based attention model,[0],[0]
"Specifically, we define a gating parameter gt and replace (12) above by (14) as define below:
gt = σ(Wgdt + Ught−1",4.1 Diversity based attention model,[0],[0]
"+ bo)
cdiverset = ct",4.1 Diversity based attention model,[0],[0]
"− gt ct T ct−1 cTt−1ct−1 ct−1 (14)
where Wg ∈ Rl5×l4 , Ug ∈ Rl5×l4",4.1 Diversity based attention model,[0],[0]
"We compare with two recently proposed baseline diversity methods (Chen et al., 2016) as described below.",5 Baseline Methods,[0],[0]
Note that these methods were proposed in the context of abstractive summarization (not query based abstractive summarization) and we adapt them for the task of query based abstractive summarization.,5 Baseline Methods,[0],[0]
Below we just highlight the key differences from our model in computing the context vector d ′,5 Baseline Methods,[0],[0]
t passed to the decoder.,5 Baseline Methods,[0],[0]
M1:,5 Baseline Methods,[0],[0]
"This model accumulates all the previous context vectors as ∑t−1 j=1 d ′ j and incorporates
this history while computing a diverse context vector:
d ′",5 Baseline Methods,[0],[0]
"t = tanh(Wcdt − Uc
t−1∑
j=1
d ′ j) (15)
where Wc, Uc ∈ Rl4×l4 are diagonal matrices.",5 Baseline Methods,[0],[0]
We then use this diversity driven context d ′,5 Baseline Methods,[0],[0]
t in Equation (9) and (10).,5 Baseline Methods,[0],[0]
M2:,5 Baseline Methods,[0],[0]
"In this model, in addition to computing a diverse context as described in Equation (15), the attention weights at each time step are also forced to be diverse from the attention weights at the previous time step.
",5 Baseline Methods,[0],[0]
α ′,5 Baseline Methods,[0],[0]
"t,i = v T a tanh(Was ′",5 Baseline Methods,[0],[0]
t +,5 Baseline Methods,[0],[0]
"Uadt − ba
t−1∑
j=1
α ′",5 Baseline Methods,[0],[0]
"j,i)
where Wa ∈ Rl1×l1 , Ua ∈ Rl1×l4 , ba, va ∈ Rl1 , l1 is the number of hidden units in the decoder GRU.",5 Baseline Methods,[0],[0]
"Once again, they maintain a history of attention weights and compute a diverse attention vector by subtracting the history from the current attention vector.",5 Baseline Methods,[0],[0]
We evaluate our models on the dataset described in section 3.,6 Experimental Setup,[0],[0]
Note that there are no prior baselines on query based abstractive summarization so we could only compare with different variations of the encoder decoder models as described above.,6 Experimental Setup,[0],[0]
"Further, we compare our diversity based attention models with existing models for diversity by suitably adapting them to this problem as described earlier.",6 Experimental Setup,[0],[0]
"Specifically, we compare the performance of the following models:
• Vanilla e-a-d:",6 Experimental Setup,[0],[0]
This is the vanilla encoderattention-decoder model adapted to the problem of abstractive summarization.,6 Experimental Setup,[0],[0]
It contains the following components (i) document encoder (ii) document attention model (iii) decoder.,6 Experimental Setup,[0],[0]
It does not contain an encoder or attention model for the query.,6 Experimental Setup,[0],[0]
"This helps us understand the importance of the query.
",6 Experimental Setup,[0],[0]
• Queryenc:,6 Experimental Setup,[0],[0]
This model contains the query encoder in addition to the three components used in the vanilla model above.,6 Experimental Setup,[0],[0]
"It does not contain any attention model for the query.
",6 Experimental Setup,[0],[0]
• Queryatt:,6 Experimental Setup,[0],[0]
"This model contains the query attention model in addition to all the components in Queryenc.
",6 Experimental Setup,[0],[0]
"• D1: The diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• D2: The LSTM based diversity attention model as described in Section 4.1.
",6 Experimental Setup,[0],[0]
"• SD1: The soft diversity attention model as described in Section 4.1
•",6 Experimental Setup,[0],[0]
"SD2: The soft LSTM based diversity attention model as described in Section 4.1
• B1: Diversity cell in Figure3 is replaced by the basic LSTM cell (i.e. cdiverset = ct instead of using Equation (12).",6 Experimental Setup,[0],[0]
"This helps us understand whether simply using an LSTM to track the history of context vectors (without imposing a diversity constraint) is sufficient.
",6 Experimental Setup,[0],[0]
"• M1: The baseline model which operates on the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"• M2: The baseline model which operates on the attention weights in addition to the context vector as described in Section 5.
",6 Experimental Setup,[0],[0]
"We used 80% of the data for training, 10% for validation and 10% for testing.",6 Experimental Setup,[0],[0]
"We create 10 such folds and report the average Rouge-1, Rouge-2, Rouge-L scores across the 10 folds.",6 Experimental Setup,[0],[0]
The hyperparameters (batch size and GRU cell sizes) of all the models are tuned on the validation set.,6 Experimental Setup,[0],[0]
"We tried the following batch sizes : 32, 64 and the following GRU cell sizes 200, 300, 400.",6 Experimental Setup,[0],[0]
"We used Adam (Kingma and Ba, 2014) as the optimization algorithm with the initial learning rate set to 0.0004, β1 = 0.9, β2 = 0.999.",6 Experimental Setup,[0],[0]
We used pre-trained publicly available Glove word embeddings2 and fine-tuned them during training.,6 Experimental Setup,[0],[0]
"The same word embeddings are used for the query words and the document words.
",6 Experimental Setup,[0],[0]
"Table 3 summarizes the results of our experiments.
",6 Experimental Setup,[0],[0]
2http://nlp.stanford.edu/projects/glove/,6 Experimental Setup,[0],[0]
"In this section, we discuss the results of the experiments reported in Table 3. 1.",7 Discussions,[0],[0]
Effect of Query:,7 Discussions,[0],[0]
Comparing rows 1 and 2 we observe that adding an encoder for the query and allowing it to influence the outputs of the decoder indeed improves the performance.,7 Discussions,[0],[0]
This is expected as the query contains some keywords which could help in sharpening the focus of the summary.,7 Discussions,[0],[0]
2.,7 Discussions,[0],[0]
Effect of Query attention model:,7 Discussions,[0],[0]
Comparing rows 2 and 3 we observe that using an attention model to dynamically compute the query representation at each time step improves the results.,7 Discussions,[0],[0]
This suggests that the attention model indeed learns to focus on relevant portions of the query at different time steps.,7 Discussions,[0],[0]
3.,7 Discussions,[0],[0]
"Effect of Diversity models: All the diversity models introduced in the paper (rows 7, 8, 9, 10) give significant improvement over the nondiversity models.",7 Discussions,[0],[0]
"In particular, the modified LSTM based diversity model gives the best results.",7 Discussions,[0],[0]
This is indeed very encouraging and Table 4 shows some sample summaries comparing the performance of different models.,7 Discussions,[0],[0]
4.,7 Discussions,[0],[0]
Comparison with baseline diversity models: The baseline diversity model M1 performs at par with our models D1 and SD1 but not as good as D2 and SD2.,7 Discussions,[0],[0]
"However, the model M2 performs very poorly.",7 Discussions,[0],[0]
We believe that simultaneously adding a constraint on the context vectors as well as attention weights (as is indeed the case with M2) is a bit too aggressive and leads to poor performance (although this needs further investigation).,7 Discussions,[0],[0]
5.,7 Discussions,[0],[0]
Quantitative Analysis:,7 Discussions,[0],[0]
"In addition to the qualitative analysis reported in Table 4 we also did a quantitative analysis by counting the num-
ber of sentences containing repeated words generated by different models.",7 Discussions,[0],[0]
Specifically for the 1268 test instances we counted the number of sentences containing repeated words as generated by different modes.,7 Discussions,[0],[0]
Table 5 summarizes this analysis.,7 Discussions,[0],[0]
In this work we proposed a query-based summarization method.,8 Conclusion,[0],[0]
"The unique feature of
the model is a novel diversification mechanism based on successive orthogonalization.",8 Conclusion,[0],[0]
This gives us the flexibility to: (i) provide diverse context vectors at successive time steps and (ii) pay attention to words repeatedly if need be later in the summary (as opposed to existing models which aggressively delete the history).,8 Conclusion,[0],[0]
We also introduced a new data set and empirically verified we perform significantly better (gain of 28% (absolute) in ROUGE-L score) than applying a plain encode-attend-decode mechanism to this problem.,8 Conclusion,[0],[0]
We observe that adding an attention mechanism on the query string gives significant improvements.,8 Conclusion,[0],[0]
We also compare with a state of the art diversity model and outperform it by a good margin (gain of 7% (absolute) in ROUGE-L score).,8 Conclusion,[0],[0]
The diversification model proposed is general enough to apply to other NLG tasks with suitable modifications and we are currently working on extending this to dialog systems and general summarization.,8 Conclusion,[0],[0]
Abstractive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.ive summarization aims to generate a shorter version of the document covering all the salient points in a compact and coherent fashion.,abstractText,[0],[0]
"On the other hand, query-based summarization highlights those points that are relevant in the context of a given query.",abstractText,[0],[0]
"The encodeattend-decode paradigm has achieved notable success in machine translation, extractive summarization, dialog systems, etc.",abstractText,[0],[0]
But it suffers from the drawback of generation of repeated phrases.,abstractText,[0],[0]
In this work we propose a model for the query-based summarization task based on the encode-attend-decode paradigm with two key additions (i) a query attention model (in addition to document attention model) which learns to focus on different portions of the query at different time steps (instead of using a static representation for the query) and (ii) a new diversity based attention model which aims to alleviate the problem of repeating phrases in the summary.,abstractText,[0],[0]
In order to enable the testing of this model we introduce a new query-based summarization dataset building on debatepedia.,abstractText,[0],[0]
Our experiments show that with these two additions the proposed model clearly outperforms vanilla encode-attend-decode models with a gain of 28% (absolute) in ROUGE-L scores.,abstractText,[0],[0]
Diversity driven attention model for query-based abstractive summarization,title,[0],[0]
"ar X
iv :1
70 9.
01 12
1v 2
[ cs
.C L
] 2
6 Fe
learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time. Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers. This paper aims to investigate what these latent tree learning models learn. We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",text,[0],[0]
"Tree-structured recursive neural networks (TreeRNNs; Socher et al., 2011)—which build a vector representation for a sentence by incrementally computing representations for each node in its parse tree—have been proven to be effective at sentence understanding tasks like sentiment analysis (Socher et al., 2013), textual entailment (Bowman et al., 2016), and translation (Eriguchi et al., 2016).",1 Introduction,[0],[0]
"Some variants of these models (Socher et al., 2011; Bowman et al., 2016) can also be trained to produce
∗Now at eBay, Inc.
parse trees that they then consume.",1 Introduction,[0],[0]
"Recent work on latent tree learning (Yogatama et al., 2017; Maillard et al., 2017; Choi et al., 2018) has led to the development of new training methods for TreeRNNs that allow them to learn to parse without ever being given an example of a correct parse tree, thereby replacing direct syntactic supervision with indirect supervision from a downstream task like sentence classification.",1 Introduction,[0],[0]
"These models are designed to learn grammars—strategies for assigning trees to sentences—that are suited to help solve the sentence understanding task at hand, rather than ones that approximate expert-designed grammars like that of the Penn Treebank (PTB; Marcus et al., 1999).
",1 Introduction,[0],[0]
"Latent tree learning models have shown striking success at sentence understanding, reliably performing better on sentiment analysis and textual entailment than do comparable TreeRNN models which use parses assigned by conventional parsers, and setting the state of the art among sentence-encoding models for textual entailment.",1 Introduction,[0],[0]
"However, none of the
work in latent tree learning to date has included any substantial evaluation of the quality of the trees induced by these models, leaving open an important question which this paper aims to answer: Do these models owe their success to consistent, principled latent grammars?",1 Introduction,[0],[0]
"If they do, these grammars may be worthy objects of study for researchers in syntax and semantics.",1 Introduction,[0],[0]
"If they do not, understanding why the models succeed without such syntax could lead to new insights into the use of TreeRNNs and into sentence understanding more broadly.
",1 Introduction,[0],[0]
"While there is still lively debate within linguistic syntax and semantics over the precise grammars that should be used for language understanding and generation, it has long been clear that understanding any natural language sentence requires implicitly or explicitly recognizing which substrings of the sentence form meaningful units, or constituents (Chomsky, 1965; Frege, 1892; Heim and Kratzer, 1998).",1 Introduction,[0],[0]
"This is well illustrated by structurally ambiguous sentences like the one below, repeated from Sag (1991)",1 Introduction,[0],[0]
"a.o.:
1. (a) I saw the [ man [ with the telescope ] ]
→֒I saw the man who had a telescope.",1 Introduction,[0],[0]
(b) I [ saw the man ],1 Introduction,[0],[0]
"[ with the telescope ]
→֒ I used the telescope to view the man.
",1 Introduction,[0],[0]
"Under the partial constituency parse shown in 1a, with a telescope forms a constituent with man, thereby providing additional information about the individual man describes.",1 Introduction,[0],[0]
"On the other hand, in 1b, with a telescope does not form a constituent with man, but instead provides additional information about the action described by saw a man.",1 Introduction,[0],[0]
"In this way, the same string of words can be assigned two different, yet equally valid constituency structures reflecting the different interpretations for the string.",1 Introduction,[0],[0]
"Constituency can be straightforwardly expressed using unlabeled parse trees like the ones used in TreeRNNs, and expressing constituency information is generally thought to be the primary motivation for using trees in TreeRNNs.
",1 Introduction,[0],[0]
"In this paper, we reimplement the latent tree learning models of Yogatama et al. (2017) and Choi et al. (2018) in a shared codebase, train both models (and several baselines) to perform textual entailment on the SNLI and MultiNLI corpora (Bowman et al., 2015; Williams et al., 2017), and evaluate the results quantitatively and qualitatively with a focus on four
issues: the degree to which latent tree learning improves task performance, the degree to which latent tree learning models learn similar grammars across random restarts, the degree to which their grammars match PTB grammar, and the degree to which their grammars appear to follow any recognizable grammatical principles.
",1 Introduction,[0],[0]
"We confirm that both types of model succeed at producing usable sentence representations, but find that only the stronger of the two models—that of Choi et al. (2018)—outperforms either a comparable TreeRNN baseline or a simple LSTM RNN.",1 Introduction,[0],[0]
"We find that the grammar of the Choi et al. model varies dramatically across random restarts, and tends to agree with PTB grammar with roughly chance accuracy.",1 Introduction,[0],[0]
"We do find, though, that the resulting grammar has some regularities, including a preference for shallow trees, a somewhat systematic treatment of negation, and a preference to treat pairs of adjacent words at the edges of a sentence as constituents.",1 Introduction,[0],[0]
"The work discussed in this paper is closely related to work on grammar induction, in which statistical learners attempt to solve the difficult problem of reconstructing the grammar that generated a corpus of text using only that corpus and, optionally, some heuristics about the nature of the expected grammar.",2 Background,[0],[0]
"Grammar induction in NLP has been widely studied since at least the mid-1990s (Chen, 1995; Cohen et al., 2011; Hsu et al., 2012), and builds on a earlier line of more general work in machine learning surveyed in Duda et al. (1973) and Fu (1977).",2 Background,[0],[0]
"One work in this area, Naseem and Barzilay (2011), additionally provides some semantic information to the learner, though only as a source of additional guidance, rather than as a primary objective as here.",2 Background,[0],[0]
"In related work, Gormley et al. (2014) present a method for jointly training a grammar induction model and a semantic role labeling (SRL) model.",2 Background,[0],[0]
"They find that the resulting SRL model is more effective than one built on a purely unsupervised grammar induction system, but that using a conventionally trained parser instead yields better SRL performance.
",2 Background,[0],[0]
"There is also a long history of work on artificial neural network models that build latent hierarchical structures without direct supervision when solving
algorithmic problems, including Das et al. (1992), Sun et al. (1993), and more recently, Joulin and Mikolov (2015) and Grefenstette et al. (2015).
",2 Background,[0],[0]
We are only aware of four prior works on latent tree learning for sentence understanding with neural networks.,2 Background,[0],[0]
"All four jointly train two model components—a parser based on distributed representations of words and phrases, and a TreeRNN of some kind that uses those parses—but differ in the parsing strategies, TreeRNN parameterizations, and training objective used.
",2 Background,[0],[0]
Socher et al. (2011) present the first neural network model that we are aware of that use the same learned representations to both parse a sentence and—using the resulting parse in a TreeRNN— perform sentence-level classification.,2 Background,[0],[0]
They use a plain TreeRNN and a simple parser that scores pairs of adjacent words and phrases and merges the highest-scoring pair.,2 Background,[0],[0]
"They train their model on a sentiment objective, but rather than training the parsing component on that objective as well, they use a combination of an auxiliary autoencoding objective and a nonparametric scoring function to parse.",2 Background,[0],[0]
"While this work shows good results on sentiment, it does not feature any evaluation of the induced trees, either through direct analysis nor through comparison with any sentiment baseline that uses trees from a conventionally-trained parser.",2 Background,[0],[0]
"Bowman et al. (2016) introduce an efficient, batchable architecture for doing this—the Shift-reduce ParserInterpreter Neural Network (SPINN; Figure 2)— which is adapted by Yogatama et al. (2017) for latent tree learning and used in this work.
",2 Background,[0],[0]
The remaining three models all use TreeLSTMs,2 Background,[0],[0]
"(Tai et al., 2015), and all train and evaluate both components on a shared semantic objective.",2 Background,[0],[0]
"All three use the task of recognizing textual entailment on the SNLI corpus (Bowman et al., 2015) as one such objective.",2 Background,[0],[0]
"The models differ from one another primarily in the ways in which they use this task objective to train their parsing components, and in the structures of those components.
",2 Background,[0],[0]
"Yogatama et al. (2017) present a model (which we call RL-SPINN) that is identical to SPINN at test time, but uses the REINFORCE algorithm (Williams, 1992) at training time to compute gradients for the transition classification function, which produces discrete decisions and does not otherwise
receive gradients through backpropagation.",2 Background,[0],[0]
"Surprisingly, and in contrast to Gormley et al., they find that a small 100D instance of this RL-SPINN model performs somewhat better on several text classification tasks than an otherwise-identical model which is explicitly trained to parse.
",2 Background,[0],[0]
"In unpublished work, Maillard et al. (2017) present a model which explicitly computes O(N2) possible tree nodes for N words, and uses a soft gating strategy to approximately select valid combinations of these nodes that form a tree.",2 Background,[0],[0]
"This model is trainable entirely using backpropagation, and a 100D instance of the model performs slightly better than RL-SPINN on SNLI.
",2 Background,[0],[0]
"Choi et al. (2018) present a model (which we call ST-Gumbel) that uses a similar data structure and gating strategy to Maillard et al., but which uses the Straight-Through Gumbel-Softmax estimator (Jang et al., 2016).",2 Background,[0],[0]
"This allows them to use a hard categorical gating function, so that their output sentence vector is computed according to a single tree, rather than a gated combination of partial trees as in Maillard et al.. They report substantial gains in both speed and accuracy over Maillard et al. and Yogatama et al. on SNLI.
",2 Background,[0],[0]
"Several models (Naradowsky et al., 2012; Kim et al., 2017; Liu and Lapata, 2017; Munkhdalai and Yu, 2017a) have also been proposed that can induce latent dependency trees over text using mechanisms like attention, but do not propagate information up the trees as in typical compositional models.",2 Background,[0],[0]
"Other models like that of Chung et al. (2017) induce and use latent trees or tree-like structures, but constrain these structures to be of a low fixed depth.
",2 Background,[0],[0]
Other past work has also investigated the degree to which neural network models for non-syntactic tasks implicitly learn syntactic information.,2 Background,[0],[0]
Recent highlights from this work include Linzen et al. (2016) on language modeling and Belinkov et al. (2017) on translation.,2 Background,[0],[0]
"Also on translation, Neubig et al. (2012) and DeNero and Uszkoreit (2011) present methods that use aligned sentences from bilingual parallel text to learn a binary constituency parser for use in word reordering.",2 Background,[0],[0]
"These two papers do not evaluate these parsers on typical parsing metrics, but find that the parsers support word reorderings that in turn yield improvements in translation quality, suggesting that they do capture some notion
of syntactic constituency.",2 Background,[0],[0]
This paper investigates the behavior of two models: RL-SPINN and ST-Gumbel.,3 Models and Methods,[0],[0]
"Both have been shown to outperform similar models based on supervised parsing, and the two represent substantially different approaches to latent tree learning.
",3 Models and Methods,[0],[0]
SPINN Variants Three of our of our baselines and one of the two latent tree learning models are based on the SPINN architecture of Bowman et al. (2016).,3 Models and Methods,[0],[0]
"Figure 2 shows and describes the architecture.
",3 Models and Methods,[0],[0]
"In the base SPINN model, all model components are used, and the transition classifier is trained on binarized Penn Treebank-style parses from the Stanford PCFG Parser (Klein and Manning, 2003), which are included with SNLI and MultiNLI.",3 Models and Methods,[0],[0]
"These binary-branching parse trees are converted to SHIFT/REDUCE sequences for use in the model through a simple reversible transformation.
",3 Models and Methods,[0],[0]
"RL-SPINN, based on the unsupervised syntax model of Yogatama et al. (2017), is architecturally equivalent to SPINN, but its transition classifier is optimized for MultiNLI classification accuracy, rather than any parsing-related loss.",3 Models and Methods,[0],[0]
"Because this component produces discrete decisions, the REINFORCE algorithm (with the standard exponential moving average baseline) is used to supply gradients for it.",3 Models and Methods,[0],[0]
We explored several alternative baseline strategies—including parametric value networks and strategies based on greedy decoding— as well as additional strategies for increasing exploration.,3 Models and Methods,[0],[0]
We also thoroughly tuned the relevant hyperparameter values for each alternative.,3 Models and Methods,[0],[0]
"In all of these experiments, we found that a standard implementation with the exponential moving average baseline produces accuracy no worse than any readily available alternative.
",3 Models and Methods,[0],[0]
We also evaluate two other variants of SPINN as baselines.,3 Models and Methods,[0],[0]
"In SPINN-NC (for No Connection from tracking to composition), the connection from the tracking LSTM to the composition function is severed.",3 Models and Methods,[0],[0]
"This weakens the model, but makes it exactly equivalent to a plain TreeLSTM—it will produce the exact same vector that a TreeLSTM with the same composition function would have produced for the tree that the transition classifier implicitly pro-
duces.",3 Models and Methods,[0],[0]
"This model serves as a maximally comparable baseline for the ST-Gumbel model, which also performs composition using a standard TreeLSTM in forward-propagation.
",3 Models and Methods,[0],[0]
"SPINN-PI-NT (for Parsed Input, No Tracking) removes the tracking LSTM, as well as the two components that depend on it: the tracking-composition connection and the transition decision function.",3 Models and Methods,[0],[0]
"As such, it cannot produce its own parse trees and must rely on trees from the input data.",3 Models and Methods,[0],[0]
"We include this in our comparison to understand the degree to which training a parser, rather than using a higher-quality off-the-shelf parser, impacts performance on our semantic task.
",3 Models and Methods,[0],[0]
ST-Gumbel The ST-Gumbel model was developed by Choi et al. (2018) and is shown in Figure 3.,3 Models and Methods,[0],[0]
The model takes a sequence of N − 1 steps to build a tree over N words.,3 Models and Methods,[0],[0]
"At every step, every possible pair of adjacent words or phrase vectors in the partial tree is given to a TreeLSTM composition function to produce a new candidate phrase vector.",3 Models and Methods,[0],[0]
"A simple learned scoring function then selects the best of these candidates, which forms a constituent node in the tree and replaces its two children in the list of nodes that are available to compose.",3 Models and Methods,[0],[0]
"This repeats until only two nodes remain, at which point they are composed and the tree is complete.",3 Models and Methods,[0],[0]
"This exhaustive search increases the computational complexity of the model over (RL-)SPINN, but also allows the model to perform a form of easy-first parsing, making it easier for the model to explore the space of possible parsing strategies.
",3 Models and Methods,[0],[0]
"Though the scoring function yields discrete decisions, the Jang et al. Straight-Through GumbelSoftmax estimator makes it possible to nonetheless efficiently compute an approximate gradient for the full model without the need for relatively brittle policy gradient techniques like REINFORCE.
",3 Models and Methods,[0],[0]
Other Baselines We also train three baselines that do not depend on a parser.,3 Models and Methods,[0],[0]
The first is a unidirectional LSTM RNN over the embedded tokens.,3 Models and Methods,[0],[0]
"The second is a version of SPINN-PI-NT that is supplied sequences of randomly sampled legal transitions (corresponding to random parse trees), rather than the output of a parser.",3 Models and Methods,[0],[0]
"The third is also a version of SPINN-PI-NT, and receives transition sequences corresponding to maximally-
shallow, approximately-balanced parse trees based on the “full binary” trees used in Munkhdalai and Yu (2017b).
",3 Models and Methods,[0],[0]
"Data To ensure that we are able to roughly reproduce the results reported by Yogatama et al. and Choi et al., we conduct an initial set of experiments on the Stanford NLI corpus of Bowman et al. (2015).",3 Models and Methods,[0],[0]
"Our primary experiments use the newer Multi-Genre Natural Language Inference Corpus (MultiNLI; Williams et al., 2017).",3 Models and Methods,[0],[0]
"MultiNLI is a 433k-example textual entailment dataset created in the style of SNLI, but with a more diverse range
of source texts and longer, more complex sentences, which we expect will encourage the models to produce more consistent and interpretable trees than they otherwise might.",3 Models and Methods,[0],[0]
"Following Williams et al., we train on the combination of MultiNLI and SNLI in these experiments (yielding just under 1M training examples) and evaluate on MultiNLI (using the matched development and test sets).
",3 Models and Methods,[0],[0]
"We also evaluate trained models on the full Wall Street Journal section of the Penn Treebank, a seminal corpus of manually-constructed constituency parses, which introduced the parsing standard used in this work.",3 Models and Methods,[0],[0]
"Because the models under study produce and consume binary-branching constituency trees without labels (and because such trees are already included with SNLI and MultiNLI), we use the Stanford Parser’s CollapseUnaryTransformer and TreeBinarizer tools to convert these Penn Treebank Trees to this form.
",3 Models and Methods,[0],[0]
"Sentence Pair Classification Because our textual entailment task requires a model to classify pairs of sentences, but the models under study produce vectors for single sentences, we concatenate the two sentence vectors, their difference, and their elementwise product (following Mou et al., 2016), and feed the result into a 1024D ReLU layer to produce a representation for the sentence pair.",3 Models and Methods,[0],[0]
"This representation is fed into a three-way softmax classifier that selects
one of the labels entailment, neutral, and contradiction for the pair.
",3 Models and Methods,[0],[0]
Additional Details We implement all models in PyTorch 0.2.,3 Models and Methods,[0],[0]
"We closely follow the original Theano code for SPINN in our implementation, and we incorporate source code provided by Choi et al. for the core parsing data structures and sampling mechanism of the ST-Gumbel model.",3 Models and Methods,[0],[0]
"Our code, saved models, and model output are available on GitHub.1
We use GloVe vectors to represent words (standard 300D, 840B word package, without fine tuning; Pennington et al., 2014), and feed them into a singlelayer 2",3 Models and Methods,[0],[0]
× 300D bi-directional GRU RNN (based on the leaf LSTM of Choi et al.),3 Models and Methods,[0],[0]
to give the models access to local context information when making parsing decisions.,3 Models and Methods,[0],[0]
"To understand the impact of this component, we follow Choi et al. in also training each model with the leaf GRU replaced with a simpler context-insensitive input encoder that simply multiplies each GloVe vector by a matrix.",3 Models and Methods,[0],[0]
"We find that these models perform best when the temperature of the ST-Gumbel distribution is a trained parameter, rather than fixed at 1.0 as in Choi et al..
We use L2 regularization and apply dropout (Srivastava et al., 2014) to the input of the 1024D sentence pair combination layer.",3 Models and Methods,[0],[0]
"We train all models using the Adam optimizer (Kingma and Ba, 2015).",3 Models and Methods,[0],[0]
"For hyperparameters for which no obvious default value exists—the L2 and dropout parameters, the relative weighting of the gradients from REINFORCE in RL-SPINN, the starting learning rate, and the size of the tracking LSTM state in SPINN—we heuristically select ranges in which usable values can be found (focusing on MultiNLI development set performance), and then randomly sample values from those ranges.",3 Models and Methods,[0],[0]
We train each model five times using different samples from those ranges and different random initializations for model parameters.,3 Models and Methods,[0],[0]
We use early stopping based on development set performance with all models.,3 Models and Methods,[0],[0]
"Table 1 shows the accuracy of all models on two test sets: SNLI (training on SNLI only, for comparison
1 https://github.com/nyu-mll/spinn/tree/is-it-syntax-release
with prior work), and MultiNLI (training on both datasets).",4 Does latent tree learning help sentence understanding?,[0],[0]
"Each figure represents the accuracy of the best run, selected using the development set, of five runs with different random initializations and hyperparameter values.",4 Does latent tree learning help sentence understanding?,[0],[0]
"Our LSTM baseline is strikingly effective, and matches or exceeds the performance of all of our PTB grammar-based tree-structured models on both SNLI and MultiNLI.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This contradicts the primary result of Bowman et al. (2016), and suggests that there is little value in using the correct syntactic structure for a sentence to guide neural network composition, at least in the context of the TreeLSTM composition function and the NLI task.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"We do, however, reproduce the key result of Choi et al. on both datasets.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Their ST-Gumbel model, which receives no syntactic information at training time, outperforms SPINN-NC, which performs composition in an identical manner but is explicitly trained to parse, and also outperforms the LSTM baseline.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"This suggests that the learned latent trees are helpful in the construction of semantic representations for sentences, whether or not they resemble conventional parse trees.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
Our results with RL-SPINN are more equivocal.,300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"That model matches, but does not beat, the performance of the full SPINN model, which is equivalent except that it is trained to parse.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"However, our implementation of RL-SPINN outperforms Yogatama et al.’s (lower-dimensional) implementation by a substantial margin.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"The impact of the leaf GRU is sometimes substantial, but the direction of its effect is not consistent.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Our results with SPINN-PI-NT are not substantially better than those with any other model, suggesting the relatively simple greedy parsing strategies used by the other models are not a major limiting factor in their performance.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Balanced trees consistently outperform randomly sampled transitions (albeit by a small margin), yet perform worse than ST-Gumbel even though ST-Gumbel uses very shallow trees as well.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Similarly, RL-SPINN depends on mostly left-branching binary parse trees, but is outperformed by a forward LSTM.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"Structure is important, but there are differences between the architectures of compositional models worth investigating in future work.
",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"None of our latent tree learning models reach the state of the art on either task, but all are comparable in both absolute and relative performance to other published results, suggesting that we have trained reasonable examples of latent tree learning models and can draw informative conclusions by studying the behaviors of these models.",300D SPINN 67.1 (1.0) 68.3 71.5,[0],[0]
"If it were the case that a latent tree learning model outperforms its baselines by identifying some specific grammar for English that is better than the one used in PTB and the Stanford Parser, then we would expect these models to identify roughly the same grammar across random restarts and minor configuration changes, and to use that grammar to produce consistent task performance.",5 Are these models consistent?,[0],[0]
"Table 2 shows two measures of consistency for the four models that produce parses, a simple random baseline that produces parses by randomly merging pairs of adjacent words and phrases, and (trivially) the deterministic strategy used in the Balanced Trees runs.
",5 Are these models consistent?,[0],[0]
We first show the variation in accuracy on the MultiNLI development set across runs.,5 Are these models consistent?,[0],[0]
"While one outlier distorts these numbers for ST-Gumbel without the leaf GRU, these figures are roughly equivalent between the latent tree learning models and the baselines, suggesting that these models are not substantially more brittle or more hyperparameter sensitive in their task performance.",5 Are these models consistent?,[0],[0]
"The second metric shows the self F1 for each model: the unlabeled binary F1 between the parses produced by two runs of the same model for the MultiNLI development
set, averaged out over all possible pairings of different runs.",5 Are these models consistent?,[0],[0]
"This measures the degree to which the models reliably converge on the same parses, and sheds some light on the behavior of the models.",5 Are these models consistent?,[0],[0]
"The baseline models show relatively high consistency, with self F1 above 65%.",5 Are these models consistent?,[0],[0]
"ST-Gumbel is substantially less consistent, with scores below 50% but above the 32.6% random baseline.",5 Are these models consistent?,[0],[0]
"RL-SPINN appears to be highly consistent, with the runs without the leaf GRU reaching 98.5% self F1, suggesting that it reliably converges to a specific grammar.",5 Are these models consistent?,[0],[0]
"However, as we will discuss in later sections, this grammar appears to be trivial.",5 Are these models consistent?,[0],[0]
"Given that our latent tree learning models are at least somewhat consistent in what they learn, it is reasonable to ask what it is that they learn.",6 Do these models learn PTB grammar?,[0],[0]
"We investigate this first quantitatively, then, in the next section, more qualitatively.
",6 Do these models learn PTB grammar?,[0],[0]
Table 3 shows parsing performance on the Wall Street Journal sections of PTB for models trained on SNLI and MultiNLI.,6 Do these models learn PTB grammar?,[0],[0]
"The baseline models perform fairly poorly in absolute terms, as they are neither well tuned for parse quality nor trained on news text, but the latent tree learning models perform dramatically worse.",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform at or slightly above chance (represented by the random trees results), while the RL-SPINN models per-
form consistently below chance.",6 Do these models learn PTB grammar?,[0],[0]
"These results suggest that these models do not learn grammars that in any way resemble PTB grammar.
",6 Do these models learn PTB grammar?,[0],[0]
"To confirm this, we also show results for individual Penn Treebank nonterminal node types.",6 Do these models learn PTB grammar?,[0],[0]
"On common intermediate node types such as ADJP, NP, and PP, the latent tree learning models do not perform substantially better than chance.",6 Do these models learn PTB grammar?,[0],[0]
"It is only on a two rare types that any latent tree learning model, or the balanced tree baseline, outperforms random trees by a significant margin: INTJ (interjection, as in Oh no, he ’s...) and the even rarer LST (list marker, as in 1 .",6 Do these models learn PTB grammar?,[0],[0]
"Determine if...), both of which are generally short and sentence-initial (discussed in more detail in Section 7).
",6 Do these models learn PTB grammar?,[0],[0]
"Next, we turn to the MultiNLI development set for further investigation.",6 Do these models learn PTB grammar?,[0],[0]
Table 4 shows results on MultiNLI for a wider range of measures.,6 Do these models learn PTB grammar?,[0],[0]
"The table shows F1 measured with respect to three different references: automatically generated trivial trees for the corpus that are either strictly left-branching or strictly right-branching, and the PTB-style trees produced by the Stanford Parser for the corpus.",6 Do these models learn PTB grammar?,[0],[0]
"We see that the baseline models perform about as well on MultiNLI as on PTB, with scores above 65%, and that these models produce trees that tend toward right branching rather than left branching.
",6 Do these models learn PTB grammar?,[0],[0]
"The ST-Gumbel models perform only at or slightly above chance on the parsed sentences, and
show a similar use of both right- and left-branching structures, with only a slight preference for the more linguistically common right-branching structures.",6 Do these models learn PTB grammar?,[0],[0]
"This suggests that they learn grammars that differ quite substantially from PTB grammars, but may share some minor properties.",6 Do these models learn PTB grammar?,[0],[0]
"Our implementation of an ST-Gumbel model has F1 scores with respect to left-branching and Stanford Parser trees that are much closer to the ones Yogatama et al. report than to the ones we find for our RL-SPINN implementation.
",6 Do these models learn PTB grammar?,[0],[0]
Our RL-SPINN results are unequivocally negative.,6 Do these models learn PTB grammar?,[0],[0]
"We find that our models could be tuned to produce trees that are qualitatively similar to those in Yogatama et al.. However, in our primary experiments, we tune model hyperparameters with the sole criterion of downstream task performance, and find that the trees from these experiments yield relactively trivial trees, with F1 scores that are much lower than theirs with respect to the Stanford Parser, and much higher with respect to left branching trees (see Table 4).",6 Do these models learn PTB grammar?,[0],[0]
"All runs perform below chance on the parsed sentences, and all have F1 scores over 92% with respect to the left-branching structures, suggesting that they primarily learn to produce strictly left-branching trees.",6 Do these models learn PTB grammar?,[0],[0]
"This trivial strategy, which
makes the model roughly equivalent to a sequential RNN, is very easy to learn.",6 Do these models learn PTB grammar?,[0],[0]
"In a shift–reduce model like SPINN, the model can simply learn to perform the REDUCE operation whenever it is possible to do so, regardless of the specific words and phrases being parsed.",6 Do these models learn PTB grammar?,[0],[0]
"This can be done by setting a high bias value for this choice in the transition classifier.
",6 Do these models learn PTB grammar?,[0],[0]
The rightmost column shows another measure of what is learned: the average depth—the length of the path from the root to any given word—of the induced trees.,6 Do these models learn PTB grammar?,[0],[0]
"For the baseline models, this value is slightly above the 5.7 value for the Stanford Parser trees.",6 Do these models learn PTB grammar?,[0],[0]
"For the RL-SPINN models, this number is predictably much higher, reflecting the very deep and narrow left-branching trees that those models tend to produce.",6 Do these models learn PTB grammar?,[0],[0]
"For the ST-Gumbel model, though, this metric is informative: the models consistently produce shallow trees with depth under 5—closer to the balanced trees baseline than to SPINN.",6 Do these models learn PTB grammar?,[0],[0]
"This hints at a possible interpretation: While shallower trees may be less informative about the structure of the sentence than real PTB trees, they reduce the number of layers that a word needs to pass through to reach the final classifier, potentially making it easier to learn an effective composition function that faithfully encodes the contents of a sentence.",6 Do these models learn PTB grammar?,[0],[0]
"This interpretation
is supported by the results of Munkhdalai and Yu (2017b), who show that it is possible to do well on SNLI using a TreeLSTM (with a leaf LSTM) over arbitrarily chosen balanced trees with low depths, and our balanced trees baseline, which approximates this result.
",6 Do these models learn PTB grammar?,[0],[0]
The ST-Gumbel models tend to implement their shallow parsing strategy with a good deal of randomness.,6 Do these models learn PTB grammar?,[0],[0]
"They tend to assign near-zero probability (< 10−10) to many possible compositions, generally those that would result in unnecessarily deep trees, and relatively smooth probabilities (generally > 0.01) to the remaining options.",6 Do these models learn PTB grammar?,[0],[0]
"The trainable temperature parameter for these models generally converged slowly to a value between 1 and 20, and did not fluctuate substantially during training.",6 Do these models learn PTB grammar?,[0],[0]
"In the previous three sections, we have shown that latent tree learning models are able to perform as well or better than models that have access to linguistically principled parse trees at training or test time, but that the grammars that they learn are neither consistent across runs, nor meaningfully similar to PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"In this section, we investigate the trees produced by these learned grammars directly to identify whether they capture any recognizable syntactic or semantic phenomena.
",7 Analyzing the Learned Trees,[0],[0]
The RL-SPINN models create overwhelmingly left-branching trees.,7 Analyzing the Learned Trees,[0],[0]
"We observe few deviations from this pattern, which occur almost exclusively on sentences with fewer than seven words.",7 Analyzing the Learned Trees,[0],[0]
"Given that the self-F1 scores for these models (92.7 and 98.5, Table 2) are similar to their F1 scores with respect to strictly left-branching trees (95.0 and 99.1, Table 4), there is little room for these models to have learned any consistent behavior beside left branching.
",7 Analyzing the Learned Trees,[0],[0]
"In some preliminary tuning runs not shown above, we saw models that deviated from this pattern more often, and one that fixated on right-branching structures instead, but we find no grammatically interesting patterns in any of these deviant structures.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models learned substantially more complex grammars, and we focus on these for the remainder of the section.",7 Analyzing the Learned Trees,[0],[0]
We discuss three model behaviors which yield linguistically implausible constituents.,7 Analyzing the Learned Trees,[0],[0]
"The first two highlight settings
where the ST-Gumbel model is consistent where it shouldn’t be, and the third highlights a setting in which it is worryingly inconsistent.",7 Analyzing the Learned Trees,[0],[0]
"The models’ treatment of these three phenomena and our observation of these models’ behavior more broadly suggest that the models do not produce trees that follow any recognizable semantic or syntactic principles.
",7 Analyzing the Learned Trees,[0],[0]
Initial and Final Two-Word Constituents The shallow trees produced by the ST-Gumbel models generally contain more constituents comprised of two words (rather than a single word combined with a phrase) than appear in the reference parses.,7 Analyzing the Learned Trees,[0],[0]
"This behavior is especially pronounced at the edges of sentences, where the models frequently treat the first two words and the last two words as constituents.",7 Analyzing the Learned Trees,[0],[0]
"Since this behavior does not correspond to any grammatical phenomenon known to these authors, it likely stems from some unknown bias within the model design.
",7 Analyzing the Learned Trees,[0],[0]
These models parse the first two words of a sentence into a constituent at rates well above both the 50% rate seen in random and balanced parses2 and the 27.7% rate seen with the SPINN models.,7 Analyzing the Learned Trees,[0],[0]
This strategy appeares in 77.4% of the model’s parses with the leaf GRU and 64.1% without.,7 Analyzing the Learned Trees,[0],[0]
"While it was consistently discovered across all of our runs of STGumbel models with the leaf GRU, it was discovered less frequently across restarts for runs without, which do not have direct access to linear position information.",7 Analyzing the Learned Trees,[0],[0]
"We observe that the models combine the final two words in each sentence at similar rates.
",7 Analyzing the Learned Trees,[0],[0]
"While merging the final two words of a sentence nearly always results in a meaningless constituent containing a period or punctuation mark, merging
2The balanced parses are right-aligned, following Munkhdalai and Yu; they parse the first two words as a constituent in about 50% of cases, but the final two words in all cases.
",7 Analyzing the Learned Trees,[0],[0]
the first two words can produce reasonable parses.,7 Analyzing the Learned Trees,[0],[0]
"This strategy is reasonable, for example, in sentences that begin with a determiner and a noun (Figure 4, top left).",7 Analyzing the Learned Trees,[0],[0]
"However, combining the first two words in sentences that start with adverbials, proper names, bare plurals, or noun phrases with multiple modifiers will generally result in meaningless constituents like Kings frequently (Figure 4, bottom).
",7 Analyzing the Learned Trees,[0],[0]
Combining the first two words of a sentence also often results in more subtly unorthodox trees—like the one in the top right of Figure 4—that combine a verb with its subject rather than its object.,7 Analyzing the Learned Trees,[0],[0]
"This contrasts with some mainstream syntactic theories (Adger 2003; Sportiche et al. 2013), which generally take the object and the verb of a sentence to form a constituent for three reasons: Taking the top right sentence in Figure 4 as an example, (i) we can replace it with a new constituent of the same type without changing the surrounding sentence structure, as in he did so, (ii) it can stand alone as an answer to a question like what did he do?, and (iii) it can be omitted in otherwise-repetitive sentences like He shot his gun, but Bill didn’t .
",7 Analyzing the Learned Trees,[0],[0]
"Negation The ST-Gumbel models also tend to learn a systematic and superficially reasonable strategy for negation: they pair any negation word (e.g., not, n’t, no, none) with the word that immediately follows it.",7 Analyzing the Learned Trees,[0],[0]
"Random parses only form these constituents in 34% of the sentences, and balanced parses only do so in 50%, but the ST-Gumbel models with the leaf GRU do so about 67% of the time and consistently across runs, while those without the leaf GRU do so less consistently, but over 90% of the time in some runs.
",7 Analyzing the Learned Trees,[0],[0]
"This strategy is effective when the negation word is meant to modify a single other word to its right, as in Figure 5, top left sub-figure, but this is frequently not the case.",7 Analyzing the Learned Trees,[0],[0]
"In Figure 5, bottom left, although the model creates the potentially reasonable constituent, not at all, it also combines not with the preposition at to form a constituent with no clear interpretation (or, at best, an incredibly bizarre one).",7 Analyzing the Learned Trees,[0],[0]
"Further, combining not with at goes contra the syntactic observation that prepositional phrases can generally move along with the following noun phrases as a constituent (as in semantically comparably sentences like, He is not sure at all.).
",7 Analyzing the Learned Trees,[0],[0]
Function Words and Modifiers,7 Analyzing the Learned Trees,[0],[0]
"Finally, the STGumbel models are not consistent in their treatment of function words, like determiners or prepositions, or in their treatment of modifiers like adverbs and adjectives.",7 Analyzing the Learned Trees,[0],[0]
This reflects quantitative results in Table 3 showing that ST-Gumbel parse trees correspond to PTB for PP and ADJP constituents at much lower rates than do SPINN-based models or models supplied with random trees.,7 Analyzing the Learned Trees,[0],[0]
"For example, the top left tree of Figure 6 (ST-Gumbel) associated the determiner the with the verb, when it should form a constituent with the noun phrase Nazi angle as in the top right tree (PTB).",7 Analyzing the Learned Trees,[0],[0]
"The resulting phrase, the Nazi angle, has a clear meaning—unlike discussed the—and it passes syntactic tests for constituency; for example, one can replace the noun phrase with the pronoun it without otherwise modifying the meaning of the sentence.
",7 Analyzing the Learned Trees,[0],[0]
"Similarly, prepositions are generally expected to form constituents with the noun phrases that follow them (Adger, 2003; Sportiche et al., 2013), as in the the bottom right tree (PTB) of Figure 6.",7 Analyzing the Learned Trees,[0],[0]
"One syntactic test that with horror forms a P-NP constituent comes from the fact that it can be a stand-alone answer to a question; for example, the question how did the students react?",7 Analyzing the Learned Trees,[0],[0]
can be answered simply with with horror.,7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel models often instead pair prepositions with the verb phrases that precede them, as in Figure 6, lower left, where this results in the constituent the students acted with, which cannot be a stand-alone answer to a question.",7 Analyzing the Learned Trees,[0],[0]
"From this perspective, constituents like discussed the and we briefly (Figure 6, top left) are also syntactically anomalous, and cannot be given coherent meanings.
",7 Analyzing the Learned Trees,[0],[0]
"The ST-Gumbel models outperform syntax-based models on MultiNLI and SNLI, and the trees that they assign to sentences do not generally resemble
those of PTB grammar.",7 Analyzing the Learned Trees,[0],[0]
"If we attempt to interpret these trees under the standard assumption that all the constituents in a sentence must be interpretable and must contribute to the meaning of the sentence, we force ourselves to interpret implausible constituents like we briefly, and reach implausible sentence-level interpretations, such as taking the sentence in Figure 6, top left, to mean that those of us who are brief discussed the Nazi angle.",7 Analyzing the Learned Trees,[0],[0]
"It is clear that these models do not use constituency in the same way as the widely accepted syntactic or semantic frameworks we cite do.
",7 Analyzing the Learned Trees,[0],[0]
"In sum, we find that RL-SPINN adopts a trivial, largely left-branching parse strategy, which is consistent across runs.",7 Analyzing the Learned Trees,[0],[0]
"ST-Gumbel, on the other hand, adopts the unexpected strategy to merge initial and final constituents at higher than average rates, and is also very inconsistent with its behavior on function words and modifiers.",7 Analyzing the Learned Trees,[0],[0]
"We weren’t able to qualitatively identify structure that matches PTB-style syntax in ST-Gumbel parses, but we do find that it utilizes a strategy for negation—merging it with the immediately following constituent—that can lead to unexpected constituents, but nevertheless, is somewhat promising.",7 Analyzing the Learned Trees,[0],[0]
The experiments and analysis presented in this paper show that the best available models for latent tree learning learn grammars that do not correspond to the structures of formal syntax and semantics in any recognizable way.,8 Conclusion,[0],[0]
"In spite of this, these models perform as well or better on sentence understanding— as measured by MultiNLI performance—as models with access to Penn Treebank-style parses.
",8 Conclusion,[0],[0]
"This result leaves us with an immediate puzzle: What do these models—especially those based on the ST-Gumbel technique—learn that allows them
to do so well?",8 Conclusion,[0],[0]
"We present some observations, but we are left without a fully satisfying explanation.",8 Conclusion,[0],[0]
"A thorough investigation of this problem will likely require a search of new architectures for sentence encoding that borrow various behaviors from the models trained in this work.
",8 Conclusion,[0],[0]
This result also opens farther-reaching questions about grammar and sentence understanding: Will the optimal grammars for sentence understanding problems like NLI—were we to explore the full space of grammars to find them—share any recognizable similarities with the structures seen in formal work on syntax and semantics?,8 Conclusion,[0],[0]
"A priori, we should expect that they should.",8 Conclusion,[0],[0]
"While it is unlikely that PTB grammar is strictly optimal for any task, the empirical motivations for many of its core constituent types—the noun phrase, the prepositional phrase, and so forth—are straightforward and compelling.",8 Conclusion,[0],[0]
"However, our best latent tree learning models do not seem able to discover these structures.
",8 Conclusion,[0],[0]
"If we accept that some form of principled constituent structure is necessary or desirable, then we are left with an engineering problem: How do we identify this structure?",8 Conclusion,[0],[0]
"Making progress in this direction will likely involve both improvements to the TreeRNN models at the heart of latent tree learning systems, to make sure that these models are able to perform composition effectively enough to be able to make full use of learned structures, and also improvements to the structure search methods that are used to explore possible grammars.",8 Conclusion,[0],[0]
"This project has benefited from financial support to SB by Google, Tencent Holdings, and Samsung Research and from a Titan X Pascal GPU donated by the NVIDIA Corporation to AD.",Acknowledgments,[0],[0]
"Jon Gauthier contributed to early discussions that motivated this work, and he, Nikita Nangia, Kelly Zhang, and Cipta Herwana provided help and advice.",Acknowledgments,[0],[0]
"Recent work on the problem of latent tree learning has made it possible to train neural networks that learn to both parse a sentence and use the resulting parse to interpret the sentence, all without exposure to groundtruth parse trees at training time.",abstractText,[0],[0]
"Surprisingly, these models often perform better at sentence understanding tasks than models that use parse trees from conventional parsers.",abstractText,[0],[0]
This paper aims to investigate what these latent tree learning models learn.,abstractText,[0],[0]
"We replicate two such models in a shared codebase and find that (i) only one of these models outperforms conventional tree-structured models on sentence classification, (ii) its parsing strategies are not especially consistent across random restarts, (iii) the parses it produces tend to be shallower than standard Penn Treebank (PTB) parses, and (iv) they do not resemble those of PTB or any other semantic or syntactic formalism that the authors are aware of.",abstractText,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1722–1732, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
We then test the performance of our model on part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness, controlling for embedding dimensionality. We find that multi-sense embeddings do improve performance on some tasks (part-of-speech tagging, semantic relation identification, semantic relatedness) but not on others (named entity recognition, various forms of sentiment analysis). We discuss how these differences may be caused by the different role of word sense information in each of the tasks. The results highlight the importance of testing embedding models in real applications.",text,[0],[0]
Enriching vector models of word meaning so they can represent multiple word senses per word type seems to offer the potential to improve many language understanding tasks.,1 Introduction,[0],[0]
"Most traditional embedding models associate each word
type with a single embedding (e.g., Bengio et al. (2006)).",1 Introduction,[0],[0]
Thus the embedding for homonymous words like bank (with senses including ‘sloping land’ and ‘financial institution’) is forced to represent some uneasy central tendency between the various meanings.,1 Introduction,[0],[0]
"More fine-grained embeddings that represent more natural regions in semantic space could thus improve language understanding.
",1 Introduction,[0],[0]
"Early research pointed out that embeddings could model aspects of word sense (Kintsch, 2001) and recent research has proposed a number of models that represent each word type by different senses, each sense associated with a sensespecific embedding (Kintsch, 2001; Reisinger and Mooney, 2010; Neelakantan et al., 2014; Huang et al., 2012; Chen et al., 2014; Pina and Johansson, 2014; Wu and Giles, 2015; Liu et al., 2015).",1 Introduction,[0],[0]
"Such sense-specific embeddings have shown improved performance on simple artificial tasks like matching human word similarity judgments— WS353 (Rubenstein and Goodenough, 1965) or MC30 (Huang et al., 2012).
",1 Introduction,[0],[0]
"Incorporating multisense word embeddings into general NLP tasks requires a pipelined architecture that addresses three major steps:
1.",1 Introduction,[0],[0]
"Sense-specific representation learning: learn word sense specific embeddings from a large corpus, either unsupervised or aided by external resources like WordNet.
2.",1 Introduction,[0],[0]
"Sense induction: given a text unit (a phrase, sentence, document, etc.), infer word senses for its tokens and associate them with corresponding sense-specific embeddings.
3.",1 Introduction,[0],[0]
"Representation acquisition for phrases or sentences: learn representations for text units given sense-specific embeddings and pass them to machine learning classifiers.
",1 Introduction,[0],[0]
"Most existing work on multi-sense embeddings emphasizes the first step by learning sense spe-
1722
cific embeddings, but does not explore the next two steps.",1 Introduction,[0],[0]
"These are important steps, however, since it isn’t clear how existing multi-sense embeddings can be incorporated into and benefit realworld NLU tasks.
",1 Introduction,[0],[0]
"We propose a pipelined architecture to address all three steps and apply it to a variety of NLP tasks: part-of-speech tagging, named entity recognition, sentiment analysis, semantic relation identification and semantic relatedness.",1 Introduction,[0],[0]
"We find:
• Multi-sense embeddings give improved performance in some tasks (e.g., semantic similarity for words and sentences, semantic relation identification part-of-speech tagging), but not others (e.g., sentiment analysis, named entity extraction).",1 Introduction,[0],[0]
"In our analysis we offer some suggested explanations for these differences.
",1 Introduction,[0],[0]
"• Some of the improvements for multi-sense embeddings are no longer visible when using more sophisticated neural models like LSTMs which have more flexibility in filtering away the informational chaff from the wheat.
",1 Introduction,[0],[0]
"• It is important to carefully compare against embeddings of the same dimensionality.
",1 Introduction,[0],[0]
"• When doing so, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",1 Introduction,[0],[0]
"After describing related work, we introduce the new unsupervised sense-learning model in section 3, give our sense-induction algorithm in section 4, and then in following sections evaluate its performance for word similarity, and then various NLP tasks.",1 Introduction,[0],[0]
"Neural embedding learning frameworks represent each token with a dense vector representation, optimized through predicting neighboring words or decomposing co-occurrence matrices (Bengio et al., 2006; Collobert and Weston, 2008; Mnih and Hinton, 2007; Mikolov et al., 2013; Mikolov et al., 2010; Pennington et al., 2014).",2 Related Work,[0],[0]
"Standard neural models represent each word with a single unique vector representation.
",2 Related Work,[0],[0]
"Recent work has begun to augment the neural paradigm to address the multi-sense problem
by associating each word with a series of sense specific embeddings.",2 Related Work,[0],[0]
"The central idea is to augment standard embedding learning models like skip-grams by disambiguating word senses based on local co-occurrence— e.g., the fruit “apple” tends to co-occur with the words “cider, tree, pear” while the homophonous IT company co-occurs with words like “iphone”, “Google” or “ipod”.
",2 Related Work,[0],[0]
"For example Reisinger and Mooney (2010) and Huang et al. (2012) propose ways to develop multiple embeddings per word type by pre-clustering the contexts of each token to create a fixed number of senses for each word, and then relabeling each word token with the clustered sense before learning embeddings.",2 Related Work,[0],[0]
Neelakantan et al. (2014) extend these models by relaxing the assumption that each word must have a fixed number of senses and using a non-parametric model setting a threshold to decide when a new sense cluster should be split off; Liu et al. (2015) learns sense/topic specific embeddings by combining neural frameworks with LDA topic models.,2 Related Work,[0],[0]
Wu and Giles (2015) disambiguate sense embeddings from Wikipedia by first clustering wiki documents.,2 Related Work,[0],[0]
"Chen et al. (2014) turn to external resources and used a predefined inventory of senses, building a distinct representation for every sense defined by the Wordnet dictionary.",2 Related Work,[0],[0]
"Other relevant work includes Qiu et al. (2014) who maintains separate representations for different part-ofspeech tags of the same word.
",2 Related Work,[0],[0]
Recent work is mostly evaluated on the relatively artificial task of matching human word similarity judgments.,2 Related Work,[0],[0]
"We propose to build on this previous literature, most specifically Huang et al. (2012) and Neelakantan et al. (2014), to develop an algorithm for learning multiple embeddings for each word type, each embedding corresponding to a distinct induced word sense.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such an algorithm should have the property that a word should be associated with a new sense vector just when evidence in the context (e.g., neighboring words, document-level co-occurrence statistics) suggests that it is sufficiently different from its early senses.",3 Learning Sense-Specific Embeddings,[0],[0]
"Such a line of thinking naturally points to Chinese Restaurant Processes (CRP) (Blei et al., 2004; Teh et al., 2006) which have been applied in the related field of word sense induction.",3 Learning Sense-Specific Embeddings,[0],[0]
"In the analogy of
CRP, the current word could either sit at one of the existing tables (belonging to one of the existing senses) or choose a new table (a new sense).",3 Learning Sense-Specific Embeddings,[0],[0]
The decision is made by measuring semantic relatedness (based on local context information and global document information) and the number of customers already sitting at that table (the popularity of word senses).,3 Learning Sense-Specific Embeddings,[0],[0]
We propose such a model and show that it improves over the state of the art on a standard word similarity task.,3 Learning Sense-Specific Embeddings,[0],[0]
"We offer a brief overview of Chinese Restaurant Processes in this section; readers interested in more details can consult the original papers (Blei et al., 2004; Teh et al., 2006; Pitman, 1995).",3.1 Chinese Restaurant Processes,[0],[0]
"CRP can be viewed as a practical interpretation of Dirichlet Processes (Ferguson, 1973) for nonparametric clustering.",3.1 Chinese Restaurant Processes,[0],[0]
"In the analogy, each data point is compared to a customer in a restaurant.",3.1 Chinese Restaurant Processes,[0],[0]
"The restaurant has a series of tables t, each of which serves a dish dt.",3.1 Chinese Restaurant Processes,[0],[0]
This dish can be viewed as the index of a cluster or a topic.,3.1 Chinese Restaurant Processes,[0],[0]
"The next customer w to enter would either choose an existing table, sharing the dish (cluster) already served or choosing a new cluster based on the following probability distribution:
Pr(tw = t) ∝",3.1 Chinese Restaurant Processes,[0],[0]
"{ NtP (w|dt) if t already exists γP (w|dnew) if t is new
(1) where Nt denotes the number of customers already sitting at table t and P (w|dt) denotes the probability of assigning the current data point to cluster dt.",3.1 Chinese Restaurant Processes,[0],[0]
"γ is the hyper parameter controlling the preference for sitting at a new table.
",3.1 Chinese Restaurant Processes,[0],[0]
CRPs exhibit a useful “rich get richer” property because they take into account the popularity of different word senses.,3.1 Chinese Restaurant Processes,[0],[0]
"They are also more flexible than a simple threshold strategy for setting up new clusters, due to the robustness introduced by adopting the relative ratio of P (w|dt) and P (w|dnew).",3.1 Chinese Restaurant Processes,[0],[0]
"We describe how we incorporate CRP into a standard distributed language model1.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"1We omit details about training standard distributed models; see Collobert and Weston (2008) and Mikolov et al. (2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As in the standard vector-space model, each token w is associated with a K dimensional global embedding ew.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Additionally, it is associated with a set of senses Zw = {z1w, z2w, ..., z|Zw|w } where |Zw| denotes the number of senses discovered for word w.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Each sense z is associated with a distinct sense-specific embedding ezw.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"When we encounter a new token w in the text, at the first stage, we maximize the probability of seeing the current token given its context as in standard language models using the global vector ew:
p(ew|eneigh) = F (ew, eneigh) (2)
F() can take different forms in different learning paradigms, e.g., F = ∏ w′∈neigh p(ew, ew′) for skip-gram or F = p(ew, g(ew)) for SENNA (Collobert and Weston, 2008) and CBOW, where g(eneigh) denotes a function that projects the concatenation of neighboring vectors to a vector with the same dimension as ew for SENNA and the bag-or-word averaging for CBOW (Mikolov et al., 2013).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Unlike traditional one-word-one-vector frameworks, eneigh includes sense information in addition to the global vectors for neighbors.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh can therefore be written as2.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"eneigh = {en−k, , ..., en−1, en+1, ..., en−k} (3)
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Next we would use CRP to decide which sense the current occurrence corresponds to, or construct a new sense if it is a new meaning that we have not encountered before.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Based on CRP, the probability that assigns the current occurrence to each of the discovered senses or a new sense is given by:
Pr(zw = z) ∝  ",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Nwz P (e z w|context) if z already exists
γP (w|znew) if z is new (4)
where Nwz denotes the number of times already assigned to sense z for token w. P (ezw|context) denotes the probability that current occurrence belonging to (or generated by) sense",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"z.
The algorithm for parameter update for the one token predicting procedure is illustrated in Figure
2For models that predict succeeding words, sense labels for preceding words have already been decided.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"For models that predict words using both left and right contexts, the labels for right-context words have not been decided yet.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"In such cases we just use its global word vector to fill up the position.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"01: Input : Token sequence {wn, wneigh}.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
02: Update parameters involved in Equ (3)(4) based on current word prediction.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
03:,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Sample sense label z from CRP.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
04: If a new sense label z is sampled: 05: - add z to Zwn 06: - ezwn = argmax p(wn|zm),3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"07: else: update parameters involved based on sampled sense label z.
Figure 1: Incorporating CRP into Neural Language Models.
1: Line 2 shows parameter updating through predicting the occurrence of current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Lines 4-6 illustrate the situation when a new word sense is detected, in which case we would add the newly detected sense z into Zwn .",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The vector representation ezw for the newly detected sense would be obtained by maximizing the function p(ezw|context).
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"As we can see, the model performs word-sense clustering and embedding learning jointly, each one affecting the other.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The prediction of the global vector of the current token (line2) is based on both the global and sense-specific embeddings of its neighbors, as will be updated through predicting the current token.",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"Similarly, once the sense label is decided (line7), the model will adjust the embeddings for neighboring words, both global word vectors and sense-specific vectors.
",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Training We train embeddings using Gigaword5 + Wikipedia2014.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"The training approach is implemented using skip-grams (SG) (Mikolov et al., 2013).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
"We induced senses for the top 200,000 most frequent words (and used a unified “unknown” token for other less-frequent tokens).",3.2 Incorporating CRP into Distributed Language Models,[0],[0]
The window size is set to 11.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
We iterate three times over the corpus.,3.2 Incorporating CRP into Distributed Language Models,[0],[0]
Next we describe how we decide sense labels for tokens in context.,4 Obtaining Word Representations for NLU tasks,[0],[0]
"The scenario is treated as a inference procedure for sense labels where all global word embeddings and sense-specific embeddings are kept fixed.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Given a document or a sentence, we have an objective function with respect to sense labels by multiplying Eq.2 over each containing token.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
"Computing the global optimum sense labeling— in which every word gets an optimal sense label— requires searching over the space of all senses for all words, which can be expensive.",4 Obtaining Word Representations for NLU tasks,[0],[0]
"We therefore chose two simplified heuristic approaches:
• Greedy Search: Assign each token the locally optimum sense label and represent the current token with the embedding associated with that sense.
",4 Obtaining Word Representations for NLU tasks,[0],[0]
• Expectation:,4 Obtaining Word Representations for NLU tasks,[0],[0]
"Compute the probability of each possible sense for the current word, and represent the word with the expectation vector:
~ew = ∑ z∈Zw p(w|z, context) ·",4 Obtaining Word Representations for NLU tasks,[0],[0]
ezw,4 Obtaining Word Representations for NLU tasks,[0],[0]
"We evaluate our embeddings by comparing with other multi-sense embeddings on the standard artificial task for matching human word similarity judgments.
",5 Word Similarity Evaluation,[0],[0]
"Early work used similarity datasets like WS353 (Finkelstein et al., 2001) or RG (Rubenstein and Goodenough, 1965), whose context-free nature makes them a poor evaluation.",5 Word Similarity Evaluation,[0],[0]
"We therefore adopt Stanford’s Contextual Word Similarities (SCWS) (Huang et al., 2012), in which human judgments are associated with pairs of words in context.",5 Word Similarity Evaluation,[0],[0]
"Thus for example “bank” in the context of “river bank” would have low relatedness with “deficit” in the context “financial deficit”.
",5 Word Similarity Evaluation,[0],[0]
We first use the Greedy or Expectation strategies to obtain word vectors for tokens given their context.,5 Word Similarity Evaluation,[0],[0]
"These vectors are then used as input to get the value of cosine similarity between two words.
",5 Word Similarity Evaluation,[0],[0]
Performances are reported in Table 1.,5 Word Similarity Evaluation,[0],[0]
"Consistent with earlier work (e.g.., Neelakantan et al. (2014)), we find that multi-sense embeddings result in better performance in the context-dependent SCWS task (SG+Greedy and SG+Expect are better than SG).",5 Word Similarity Evaluation,[0],[0]
"As expected, performance is not as high when global level information is ignored when choosing word senses (SG+Greedy) as when it is included (SG+Expect), as neighboring words don’t provide sufficient information for word sense disambiguation.
",5 Word Similarity Evaluation,[0],[0]
"To note, the proposed CRF models work a little better than earlier baselines, which gives some evidence that it is sufficiently strong to stand in for
this class of multi-sense models and serves as a promise for being extended to NLU tasks.
",5 Word Similarity Evaluation,[0],[0]
Visualization Table 2 shows examples of semantically related words given the local context.,5 Word Similarity Evaluation,[0],[0]
Word embeddings for tokens are obtained by using the inferred sense labels from the Greedy model and are then used to search for nearest neighbors in the vector space based on cosine similarity.,5 Word Similarity Evaluation,[0],[0]
"Like earlier models (e.g., Neelakantan et al. (2014))., the model can disambiguate different word senses (in examples like bank, rock and apple) based on their local context; although of course the model is also capable of dealing with polysemy—senses that are less distinct.",5 Word Similarity Evaluation,[0],[0]
"Having shown that multi-sense embeddings improve word similarity tasks, we turn to ask whether they improve real-world NLU tasks: POS tagging, NER tagging, sentiment analysis at the phrase and sentence level, semantic relationship identification and sentence-level semantic relatedness.",6 Experiments on NLP Tasks,[0],[0]
"For each task, we experimented on the following sets of embeddings, which are trained using the word2vec package on the same corpus:
• Standard one-word-one-vector embeddings from skip-gram (50d).
",6 Experiments on NLP Tasks,[0],[0]
"• Sense disambiguated embeddings from Section 3 and 4 using Greedy Search and Expectation (50d)
",6 Experiments on NLP Tasks,[0],[0]
"• The concatenation of global word embeddings and sense-specific embeddings (100d).
",6 Experiments on NLP Tasks,[0],[0]
"• Standard one-word-one-vector skip-gram embeddings with dimensionality doubled (100d) (100d is the correct corresponding
baseline since the concatenation above doubles the dimensionality of word vectors)
",6 Experiments on NLP Tasks,[0],[0]
"• Embeddings with very high dimensionality (300d).
",6 Experiments on NLP Tasks,[0],[0]
"As far as possible we try to perform an appleto-apple comparison on these tasks, and our goal is an analytic one—to investigate how well semantic information can be encoded in multi-sense embeddings and how they can improve NLU performances—rather than an attempt to create state-of-the-art results.",6 Experiments on NLP Tasks,[0],[0]
"Thus for example, in tagging tasks (e.g., NER, POS), we follow the protocols in (Collobert et al., 2011) using the concatenation of neighboring embeddings as input features rather than treating embeddings as auxiliary features which are fed into a CRF model along with other manually developed features as in Pennington et al. (2014).",6 Experiments on NLP Tasks,[0],[0]
"Or for experiments on sentiment and other tasks where sentence level embeddings are required we only employ standard recurrent or recursive models for sentence embedding rather than models with sophisticated state-of-theart methods (e.g., Tai et al. (2015; Irsoy and Cardie (2014)).
",6 Experiments on NLP Tasks,[0],[0]
"Significance testing for comparing models is done via the bootstrap test (Efron and Tibshirani, 1994).",6 Experiments on NLP Tasks,[0],[0]
"Unless otherwise noted, significant testing is performed on one-word-one-vector embedding (50d) versus multi-sense embedding using Expectation inference (50d) and one-vector embedding (100d) versus Expectation (100d).",6 Experiments on NLP Tasks,[0],[0]
"Named Entity Recognition We use the CoNLL-2003 English benchmark for training, and test on the CoNLL-2003 test data.",6.1 The Tasks,[0],[0]
"We follow the protocols in Collobert et al. (2011), using the concatenation of neighboring embeddings as input to a multi-layer neural model.",6.1 The Tasks,[0],[0]
"We employ a five-layer neural architecture, comprised of an input layer, three convolutional layers with rectifier linear activation function and a softmax output layer.",6.1 The Tasks,[0],[0]
Training is done by gradient descent with minibatches where each sentence is treated as one batch.,6.1 The Tasks,[0],[0]
"Learning rate, window size, number of hidden units of hidden layers, L2 regularizations and number of iterations are tuned on the development set.
",6.1 The Tasks,[0],[0]
"Part-of-Speech Tagging We use Sections 0–18 of the Wall Street Journal (WSJ) data for train-
ing, sections 19–21 for validation and sections 22–24 for testing.",6.1 The Tasks,[0],[0]
"Similar to NER, we trained 5- layer neural models which take the concatenation of neighboring embeddings as inputs.",6.1 The Tasks,[0],[0]
"We adopt a similar training and parameter tuning strategy as for POS tagging.
",6.1 The Tasks,[0],[0]
Sentence-level Sentiment Classification (Pang),6.1 The Tasks,[0],[0]
The sentiment dataset of Pang et al. (2002) consists of movie reviews with a sentiment label for each sentence.,6.1 The Tasks,[0],[0]
We divide the original dataset into training(8101)/dev(500)/testing(2000).,6.1 The Tasks,[0],[0]
Word embeddings are initialized using the aforementioned types of embeddings and kept fixed in the learning procedure.,6.1 The Tasks,[0],[0]
"Sentence level embeddings are achieved by using standard sequence recurrent neural models (Pearlmutter, 1989) (for details, please refer to Appendix section).",6.1 The Tasks,[0],[0]
"The ob-
tained embedding is then fed into a sigmoid classifier.",6.1 The Tasks,[0],[0]
"Convolutional matrices at the word level are randomized from [-0.1, 0.1] and learned from sequence models.",6.1 The Tasks,[0],[0]
"For training, we adopt AdaGrad with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"Due to space limitations, we omit details of recurrent models and training.
",6.1 The Tasks,[0],[0]
Sentiment Analysis–Stanford Treebank,6.1 The Tasks,[0],[0]
"The Stanford Sentiment Treebank (Socher et al., 2013) contains gold-standard labels for each constituent in the parse tree (phrase level), thus allowing us to investigate a sentiment task at a finer granularity than the dataset in Pang et al. (2002) where labels are only found at the top of each sentence, The sentences in the treebank were split into a training(8544)/development(1101)/testing(2210) dataset.
",6.1 The Tasks,[0],[0]
"Following Socher et al. (2013) we obtained embeddings for tree nodes by using a recursive neural network model, where the embedding for parent node is obtained in a bottom-up fashion based on its children.",6.1 The Tasks,[0],[0]
"The embeddings for each parse tree constituent are output to a softmax layer; see Socher et al. (2013).
",6.1 The Tasks,[0],[0]
We focus on the standard version of recursive neural models.,6.1 The Tasks,[0],[0]
Again we fixed word embeddings to each of the different embedding settings described above3.,6.1 The Tasks,[0],[0]
"Similarly, we adopted AdaGrad
3Note that this is different from the settings used in
with mini-batch.",6.1 The Tasks,[0],[0]
"Parameters (i.e., L2 penalty, learning rate and mini batch size) are tuned on the development set.",6.1 The Tasks,[0],[0]
"The number of iterations is treated as a variable to tune and parameters are harvested based on the best performance on the development set.
",6.1 The Tasks,[0],[0]
"Semantic Relationship Classification SemEval-2010 Task 8 (Hendrickx et al., 2009) is to find semantic relationships between pairs of nominals, e.g., in “My [apartment]e1 has a pretty large [kitchen]e2” classifying the relation between [apartment] and",6.1 The Tasks,[0],[0]
[kitchen] as component-whole.,6.1 The Tasks,[0],[0]
"The dataset contains 9 ordered relationships, so the task is formalized as a 19-class classification problem, with directed relations treated as separate labels; see Hendrickx et al. (2009) for details.
",6.1 The Tasks,[0],[0]
We follow the recursive implementations defined in Socher et al. (2012).,6.1 The Tasks,[0],[0]
"The path in the parse tree between the two nominals is retrieved, and the embedding is calculated based on recursive models and fed to a softmax classifier.",6.1 The Tasks,[0],[0]
"For pure comparison purpose, we only use embeddings as features and do not explore other combination of artificial features.",6.1 The Tasks,[0],[0]
"We adopt the same training strategy as for the sentiment task (e.g., Adagrad, minibatches, etc).
",6.1 The Tasks,[0],[0]
"(Socher et al., 2013) where word vectors were treated as parameters to optimize.
",6.1 The Tasks,[0],[0]
Sentence Semantic Relatedness,6.1 The Tasks,[0],[0]
"We use the Sentences Involving Compositional Knowledge (SICK) dataset (Marelli et al., 2014) consisting of 9927 sentence pairs, split into training(4500)/development(500)/Testing(4927).",6.1 The Tasks,[0],[0]
"Each sentence pair is associated with a gold-standard label ranging from 1 to 5, indicating how semantically related are the two sentences, from 1 (the two sentences are unrelated) to 5 (the two are very related).
",6.1 The Tasks,[0],[0]
"In our setting, the similarity between two sentences is measured based on sentence-level embeddings.",6.1 The Tasks,[0],[0]
Let s1 and s2 denote two sentences and es1 and es2 denote corresponding embeddings.,6.1 The Tasks,[0],[0]
es1 and es2 are achieved through recurrent or recursive models (as illustrated in Appendix section).,6.1 The Tasks,[0],[0]
"Again, word embeddings are obtained by simple table look up in one-word-one-vector settings and inferred using the Greedy or Expectation strategy in multi-sense settings.",6.1 The Tasks,[0],[0]
"We adopt two different recurrent models for acquiring sentencelevel embeddings, a standard recurrent model and an LSTM model (Hochreiter and Schmidhuber, 1997).
",6.1 The Tasks,[0],[0]
"The similarity score is predicted using a regression model built on the structure of a three layer convolutional model, with concatenation of es1 and es2 as input, and a regression score from 1- 5 as output.",6.1 The Tasks,[0],[0]
We adopted the same training strategy as described earlier.,6.1 The Tasks,[0],[0]
The trained model is then used to predict the relatedness score between two new sentences.,6.1 The Tasks,[0],[0]
Performance is measured using Pearson’s r between the predicted score and goldstandard labels.,6.1 The Tasks,[0],[0]
"Results for different tasks are represented in Tables 3-9.
",6.2 Discussions,[0],[0]
"At first glance it seems that multi-sense embeddings do indeed offer superior performance, since combining global vectors with sense-specific vectors introduces a consistent performance boost
for every task, when compared with the standard (50d) setting.",6.2 Discussions,[0],[0]
"But of course this is an unfair comparison; combining global vector with sensespecific vector doubles the dimensionality of vector to 100, making comparison with standard dimensionality (50d) unfair.",6.2 Discussions,[0],[0]
"When comparing with standard (100), the conclusions become more nuanced.
",6.2 Discussions,[0],[0]
"For every task, the +Expectation method has performances that often seem to be higher than the simple baseline (both for the 50d case or the 100d case).",6.2 Discussions,[0],[0]
"However, only some of these differences are significant.
",6.2 Discussions,[0],[0]
(1) Using multi-sense embeddings is significantly helpful for tasks like semantic relatedness (Tables 7-8).,6.2 Discussions,[0],[0]
"This is sensible since sentence meaning here is sensitive to the semantics of one particular word, which could vary with word sense and which would directly be reflected on the relatedness score.
",6.2 Discussions,[0],[0]
"(2) By contrast, for sentiment analysis (Tables 5-6), much of the task depends on correctly identifying a few sentiment words like “good” or “bad”, whose senses tend to have similar sentiment values, and hence for which multi-sense embeddings offer little help.",6.2 Discussions,[0],[0]
"Multi-sense embeddings might promise to help sentiment analysis for some cases, like disambiguating the word “sound” in “safe and sound” versus “movie sound”.",6.2 Discussions,[0],[0]
"But we suspect that such cases are not common, explaining the nonsignificance of the improvement.",6.2 Discussions,[0],[0]
"Furthermore, the advantages of neural models in sentiment analysis tasks presumably lie in their capability to capture local composition like negation, and it’s not clear how helpful multi-sense embeddings are for that aspect.
",6.2 Discussions,[0],[0]
"(3) Similarly, multi-sense embeddings help for POS tagging, but not for NER tagging (Table 3-4).",6.2 Discussions,[0],[0]
Word senses have long been known to be related to POS tags.,6.2 Discussions,[0],[0]
"But the largest proportion of NER tags consists of the negative not-a-NER (“O”) tag, each of which is likely correctly labelable regard-
less of whether senses are disambiguated or not (since presumably if a word is not a named entity, most of its senses are not named entities either).
(4) As we apply more sophisticated models like LSTM to semantic relatedness tasks (in Table 9), the advantages caused by multi-sense embeddings disappears.
",6.2 Discussions,[0],[0]
(5) Doubling the number of dimensions is sufficient to increase performance as much as using the complex multi-sense algorithm.,6.2 Discussions,[0],[0]
"(Of course increasing vector dimensionality (to 300) boosts performance even more, although at the significant cost of exponentially increasing time complexity.)",6.2 Discussions,[0],[0]
We do larger one-word-one-vector embeddings do so well?,6.2 Discussions,[0],[0]
"We suggest some hypotheses:
• though information about distinct senses is encoded in one-word-one-vector embeddings in a mixed and less structured way, we suspect that the compositional nature of neural models is able to separate the informational chaff from the wheat and choose what information to take up, bridging the gap between single vector and multi-sense paradigms.",6.2 Discussions,[0],[0]
"For models like LSTMs which are better at doing such a job by using gates to control information flow, the difference between two paradigms should thus be further narrowed, as indeed we found.
",6.2 Discussions,[0],[0]
•,6.2 Discussions,[0],[0]
"The pipeline model proposed in the work requires sense-label inference (i.e., step 2).",6.2 Discussions,[0],[0]
"We proposed two strategies: GREEDY and EXPECTATION, and found that GREEDY models perform worse than EXPECTATION, as we might expect4.",6.2 Discussions,[0],[0]
"But even EXPECTATION can be viewed as another form of one-wordone-vector models, just one where different senses are entangled but weighted to emphasize the important ones.",6.2 Discussions,[0],[0]
"Again, this suggests another cause for the strong relative performance of larger-dimensioned one-word-onevector models.",6.2 Discussions,[0],[0]
"In this paper, we expand ongoing research into multi-sense embeddings by first proposing a new version based on Chinese restaurant processes that achieves state of the art performance on simple
4GREEDY models work in a more aggressive way and likely make mistakes due to the non-global-optimum nature and limited context information
word similarity matching tasks.",7 Conclusion,[0],[0]
"We then introduce a pipeline system for incorporating multisense embeddings into NLP applications, and examine multiple NLP tasks to see whether and when multi-sense embeddings can introduce performance boosts.",7 Conclusion,[0],[0]
Our results suggest that simply increasing the dimensionality of baseline skip-gram embeddings is sometimes sufficient to achieve the same performance wins that come from using multi-sense embeddings.,7 Conclusion,[0],[0]
"That is, the most straightforward way to yield better performance on these tasks is just to increase embedding dimensionality.
",7 Conclusion,[0],[0]
Our results come with some caveats.,7 Conclusion,[0],[0]
"In particular, our conclusions are based on the pipelined system that we introduce, and other multi-sense embedding systems (e.g., a more advanced sense learning model or a better sense label model or a completely different pipeline system) may find stronger effects of multi-sense models.",7 Conclusion,[0],[0]
"Nonetheless we do consistently find improvements for multi-sense embeddings in some tasks (part-ofspeech tagging and semantic relation identification), suggesting the benefits of our multi-sense models and those of others.",7 Conclusion,[0],[0]
"Perhaps the most important implication of our results may be the evidence they provide for the importance of going beyond simple human-matching tasks, and testing embedding models by using them as components in real NLP applications.",7 Conclusion,[0],[0]
"In sentiment classification and sentence semantic relatedness tasks, classification models require embeddings that represent the input at a sentence or phrase level.",8 Appendix,[0],[0]
"We adopt recurrent networks (standard ones or LSTMs) and recursive networks in order to map a sequence of tokens with various length to a vector representation.
",8 Appendix,[0],[0]
"Recurrent Networks A recurrent network successively takes wordwt at step t, combines its vector representation et with the previously built hidden vector ht−1 from time t− 1, calculates the resulting current embedding ht, and passes it to the next step.",8 Appendix,[0],[0]
"The embedding ht for the current time t is thus:
ht = tanh(W · ht−1 + V · et) (5) whereW and V denote compositional matrices.",8 Appendix,[0],[0]
"If Ns denote the length of the sequence, hNs represents the whole sequence S.
Recursive Networks Standard recursive models work in a similar way by working on neighboring words by parse tree order rather than sequence order.",8 Appendix,[0],[0]
They compute the representation for each parent node based on its immediate children recursively in a bottom-up fashion until reaching the root of the tree.,8 Appendix,[0],[0]
"For a given node η in the tree and its left child ηleft (with representation eleft) and right child ηright (with representation eright), the standard recursive network calculates eη:
eη = tanh(W · eηleft + V · eηright) (6)
",8 Appendix,[0],[0]
"Long Short Term Memory (LSTM) LSTM models (Hochreiter and Schmidhuber, 1997) are defined as follows: given a sequence of inputs X = {x1, x2, ..., xnX}, an LSTM associates each timestep with an input, memory and output gate, respectively denoted as it, ft and ot.",8 Appendix,[0],[0]
"We notationally disambiguate e and h, where et denote the vector for an individual text unit (e.g., word or sentence) at time step t while ht denotes the vector computed by the LSTM model at time t by combining et and ht−1.",8 Appendix,[0],[0]
σ denotes the sigmoid function.,8 Appendix,[0],[0]
W ∈ R4K×2K .,8 Appendix,[0],[0]
"The vector representation ht for each time-step t is given by:
[ it ft ot lt ] =",8 Appendix,[0],[0]
"[ σ σ σ tanh ] W · [ ht−1 et ] (7)
ct = ft · ct−1 + it · lt (8) hst = ot · ct (9)",8 Appendix,[0],[0]
"We would like to thank Sam Bowman, Ignacio Cases, Kevin Gu, Gabor Angeli, Sida Wang, Percy Liang and other members of the Stanford NLP group, as well as anonymous reviewers for their helpful advice on various aspects of this work.",9 Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the NSF via award IIS-1514268, the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no.",9 Acknowledgments,[0],[0]
FA8750-13-2-0040.,9 Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of NSF, DARPA, AFRL, or the US government.",9 Acknowledgments,[0],[0]
Learning a distinct representation for each sense of an ambiguous word could lead to more powerful and fine-grained models of vector-space representations.,abstractText,[0],[0]
"Yet while ‘multi-sense’ methods have been proposed and tested on artificial wordsimilarity tasks, we don’t know if they improve real natural language understanding tasks.",abstractText,[0],[0]
"In this paper we introduce a multisense embedding model based on Chinese Restaurant Processes that achieves state of the art performance on matching human word similarity judgments, and propose a pipelined architecture for incorporating multi-sense embeddings into language un-",abstractText,[0],[0]
Do Multi-Sense Embeddings Improve Natural Language Understanding?,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 462–468 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
462",text,[0],[0]
"Neural network mappings are widely used to bridge modalities or spaces in cross-modal retrieval (Qiao et al., 2017; Wang et al., 2016; Zhang et al., 2016), zero-shot learning (Lazaridou et al., 2015b, 2014; Socher et al., 2013) in building multimodal representations (Collell et al., 2017) or in word translation (Lazaridou et al., 2015a), to name a few.",1 Introduction,[0],[0]
"Typically, a neural network is firstly trained
to predict the distributed vectors of one modality (or space) from the other.",1 Introduction,[0],[0]
"At test time, some operation such as retrieval or labeling is performed based on the nearest neighbors of the predicted (mapped) vectors.",1 Introduction,[0],[0]
"For instance, in zero-shot image classification, image features are mapped to the text space and the label of the nearest neighbor word is assigned.",1 Introduction,[0],[0]
"Thus, the success of such systems relies entirely on the ability of the map to make the predicted vectors similar to the target vectors in terms of semantic or neighborhood structure.1",1 Introduction,[0],[0]
"However, whether neural nets achieve this goal in general has not been investigated yet.",1 Introduction,[0],[0]
"In fact, recent work evidences that considerable information about the input modality propagates into the predicted modality (Collell et al., 2017; Lazaridou et al., 2015b; Frome et al., 2013).
",1 Introduction,[0],[0]
"To shed light on these questions, we first introduce the (to the best of our knowledge) first existing measure to quantify similarity between the neighborhood structures of two sets of vectors.",1 Introduction,[0],[0]
"Second, we perform extensive experiments in three benchmarks where we learn image-to-text and text-to-image neural net mappings using a rich variety of state-of-the-art text and image features and loss functions.",1 Introduction,[0],[0]
"Our results reveal that, contrary to expectation, the semantic structure of the mapped vectors consistently resembles more that of the input vectors than that of the target vectors of interest.",1 Introduction,[0],[0]
"In a second experiment, by using six concept similarity tasks we show that the semantic structure of the input vectors is preserved after mapping them with an untrained network, further evidencing that feed-forward nets naturally preserve semantic information about the input.",1 Introduction,[0],[0]
"Overall, we uncover and rise awareness of a largely
1We indistinctly use the terms semantic structure, neighborhood structure and similarity structure.",1 Introduction,[0],[0]
"They refer to all pairwise similarities of a set of N vectors, for some similarity measure (e.g., Euclidean or cosine).
",1 Introduction,[0],[0]
"ignored phenomenon relevant to a wide range of cross-modal / cross-space applications such as retrieval, zero-shot learning or image annotation.
",1 Introduction,[0],[0]
"Ultimately, this paper aims at: (1) Encouraging the development of better architectures to bridge modalities / spaces; (2) Advocating for the use of semantic-based criteria to evaluate the quality of predicted vectors such as the neighborhood-based measure proposed here, instead of purely geometric measures such as mean squared error (MSE).",1 Introduction,[0],[0]
Neural network and linear mappings are popular tools to bridge modalities in cross-modal retrieval systems.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) leverage a text-to-image linear mapping to retrieve images given text queries.,2 Related Work and Motivation,[0],[0]
Weston et al. (2011) map label and image features into a shared space with a linear mapping to perform image annotation.,2 Related Work and Motivation,[0],[0]
"Alternatively, Frome et al. (2013), Lazaridou et al. (2014) and Socher et al. (2013) perform zero-shot image classification with an image-to-text neural network mapping.",2 Related Work and Motivation,[0],[0]
"Instead of mapping to latent features, Collell et al. (2018) use a 2-layer feedforward network to map word embeddings directly to image pixels in order to visualize spatial arrangements of objects.",2 Related Work and Motivation,[0],[0]
Neural networks are also popular in other cross-space applications such as cross-lingual tasks.,2 Related Work and Motivation,[0],[0]
"Lazaridou et al. (2015a) learn a linear map from language A to language B and then translate new words by returning the nearest neighbor of the mapped vector in the B space.
",2 Related Work and Motivation,[0],[0]
"In the context of zero-shot learning, shortcomings of cross-space neural mappings have also been identified.",2 Related Work and Motivation,[0],[0]
"For instance, “hubness” (Radovanović et al., 2010) and “pollu-
tion” (Lazaridou et al., 2015a) relate to the highdimensionality of the feature spaces and to overfitting respectively.",2 Related Work and Motivation,[0],[0]
"Crucially, we do not assume that our cross-modal problem has any class labels, and we study the similarity between input and mapped vectors and between output and mapped vectors.
",2 Related Work and Motivation,[0],[0]
Recent work evidences that the predicted vectors of cross-modal neural net mappings are still largely informative about the input vectors.,2 Related Work and Motivation,[0],[0]
Lazaridou et al. (2015b) qualitatively observe that abstract textual concepts are grounded with the visual input modality.,2 Related Work and Motivation,[0],[0]
"Counterintuitively, Collell et al. (2017) find that the vectors “imagined” from a language-to-vision neural map, outperform the original visual vectors in concept similarity tasks.",2 Related Work and Motivation,[0],[0]
The paper argued that the reconstructed visual vectors become grounded with language because the map preserves topological properties of the input.,2 Related Work and Motivation,[0],[0]
"Here, we go one step further and show that the mapped vectors often resemble the input vectors more than the target vectors in semantic terms, which goes against the goal of a cross-modal map.
",2 Related Work and Motivation,[0],[0]
"Well-known theoretical work shows that networks with as few as one hidden layer are able to approximate any function (Hornik et al., 1989).",2 Related Work and Motivation,[0],[0]
"However, this result does not reveal much neither about test performance nor about the semantic structure of the mapped vectors.",2 Related Work and Motivation,[0],[0]
"Instead, the phenomenon described is more closely tied to other properties of neural networks.",2 Related Work and Motivation,[0],[0]
"In particular, continuity guarantees that topological properties of the input, such as connectedness, are preserved (Armstrong, 2013).",2 Related Work and Motivation,[0],[0]
"Furthermore, continuity in a topology induced by a metric also ensures that points that are close together are mapped close together.",2 Related Work and Motivation,[0],[0]
"As a toy example, Fig. 1 illustrates the distortion of a manifold after being mapped by a neural net.2
In a noiseless world with fully statistically dependent modalities, the vectors of one modality could be perfectly predicted from those of the other.",2 Related Work and Motivation,[0],[0]
"However, in real-world problems this is unrealistic given the noise of the features and the fact that modalities encode complementary information (Collell and Moens, 2016).",2 Related Work and Motivation,[0],[0]
"Such unpredictability combined with continuity and topology-preserving properties of neural nets propel the phenomenon identified, namely mapped vectors resembling more the input than the target vectors, in nearest neighbors terms.
2Parameters of these mappings were generated at random.",2 Related Work and Motivation,[0],[0]
"To bridge modalities X and Y , we consider two popular cross-modal mappings f :",3 Proposed Approach,[0],[0]
"X → Y .
",3 Proposed Approach,[0],[0]
"(i) Linear mapping (lin):
f(x) =W0x+ b0
with W0 ∈ Rdy×dx , b0 ∈",3 Proposed Approach,[0],[0]
"Rdy , where dx and dy are the input and output dimensions respectively.
(ii) Feed-forward neural network (nn):
f(x) =W1σ(W0x+ b0) + b1
with W1 ∈ Rdy×dh , W0 ∈",3 Proposed Approach,[0],[0]
"Rdh×dx , b0 ∈ Rdh ,",3 Proposed Approach,[0],[0]
b1 ∈,3 Proposed Approach,[0],[0]
"Rdy where dh is the number of hidden units and σ() the non-linearity (e.g., tanh or sigmoid).",3 Proposed Approach,[0],[0]
"Although single hidden layer networks are already universal approximators (Hornik et al., 1989), we explored whether deeper nets with 3 and 5 hidden layers could improve the fit (see Supplement).
",3 Proposed Approach,[0],[0]
Loss:,3 Proposed Approach,[0],[0]
Our primary choice is the MSE: 1 2‖f(x),3 Proposed Approach,[0],[0]
"− y‖
2, where y is the target vector.",3 Proposed Approach,[0],[0]
"We also tested other losses such as the cosine: 1 − cos(f(x), y) and the max-margin: max{0, γ + ‖f(x)",3 Proposed Approach,[0],[0]
"− y‖ − ‖f(x̃) − y‖}, where x̃ belongs to a different class than (x, y), and γ is the margin.",3 Proposed Approach,[0],[0]
"As in Lazaridou et al. (2015a) and Weston et al. (2011), we choose the first x̃ that violates the constraint.",3 Proposed Approach,[0],[0]
"Notice that losses that do not require class labels such as MSE are suitable for a wider, more general set of tasks than discriminative losses (e.g., cross-entropy).",3 Proposed Approach,[0],[0]
"In fact, cross-modal retrieval tasks often do not exhibit any class labels.",3 Proposed Approach,[0],[0]
"Additionally, our research question concerns the cross-space mapping problem in isolation (independently of class labels).
",3 Proposed Approach,[0],[0]
Let us denote a set of N input and output vectors by X ∈ RN×dx and Y ∈ RN×dy respectively.,3 Proposed Approach,[0],[0]
"Each input vector xi is paired to the output vector yi of the same index (i = 1, · · · , N ).",3 Proposed Approach,[0],[0]
Let us henceforth denote the mapped input vectors by f(X) ∈ RN×dy .,3 Proposed Approach,[0],[0]
"In order to explore the similarity between f(X) and X , and between f(X) and Y , we propose two ad hoc settings below.",3 Proposed Approach,[0],[0]
"To measure the similarity between the neighborhood structure of two sets of paired vectors V and
Z, we propose the mean nearest neighbor overlap measure (mNNOK(V,Z)).",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define the nearest neighbor overlap NNOK(vi, zi) as the number of K nearest neighbors that two paired vectors vi, zi share in their respective spaces.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"E.g., if the 3 (= K) nearest neighbors of vcat in V are {vdog, vtiger, vlion} and those of zcat in Z are {zmouse, ztiger, zlion}, the NNO3(vcat, zcat) is 2.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
Definition 1 Let V = {vi}Ni=1 and Z = {zi}Ni=1 be two sets of N paired vectors.,3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"We define:
mNNOK(V,Z) = 1
KN N∑ i=1",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"NNOK(vi, zi) (1)
with NNOK(vi, zi) = |NNK(vi) ∩ NNK(zi)|, where NNK(vi) and NNK(zi) are the indexes of the K nearest neighbors of vi and zi, respectively.
",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"The normalizing constant K simply scales mNNOK(V,Z) between 0 and 1, making it independent of the choice of K. Thus, a mNNOK(V,Z) = 0.7 means that the vectors in V and Z share, on average, 70% of their nearest neighbors.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"Notice that mNNO implicitly performs retrieval for some similarity measure (e.g., Euclidean or cosine), and quantifies how semantically similar two sets of paired vectors are.",3.1 Neighborhood Structure of Mapped Vectors (Experiment 1),[0],[0]
"To complement the setting above (Sect. 3.1), it is instructive to consider the limit case of an untrained network.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"Concept similarity tasks provide a suitable setting to study the semantic structure of distributed representations (Pennington et al., 2014).",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"That is, semantically similar concepts should ideally be close together.",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"In particular, our interest is in comparing X with its projection f(X) through a mapping with random parameters, to understand the extent to which the mapping may disrupt or preserve the semantic structure of X .",3.2 Mapping with Untrained Networks (Experiment 2),[0],[0]
"To test the generality of our claims, we select a rich diversity of cross-modal tasks involving texts at three levels: word level (ImageNet), sentence level (IAPR TC-12), and document level (Wiki).",4.1.1 Datasets,[0],[0]
"ImageNet (Russakovsky et al., 2015).",4.1.1 Datasets,[0],[0]
"Consists of ∼14M images, covering ∼22K WordNet synsets
(or meanings).",4.1.1 Datasets,[0],[0]
"Following Collell et al. (2017), we take the most relevant word for each synset and keep only synsets with more than 50 images.",4.1.1 Datasets,[0],[0]
"This yields 9,251 different words (or instances).",4.1.1 Datasets,[0],[0]
"IAPR TC-12 (Grubinger et al., 2006).",4.1.1 Datasets,[0],[0]
Contains 20K images (18K train / 2K test) annotated with 255 labels.,4.1.1 Datasets,[0],[0]
Each image is accompanied with a short description of one to three sentences.,4.1.1 Datasets,[0],[0]
"Wikipedia (Pereira et al., 2014).",4.1.1 Datasets,[0],[0]
"Has 2,866 samples (2,173 train / 693 test).",4.1.1 Datasets,[0],[0]
Each sample is a section of a Wikipedia article paired with one image.,4.1.1 Datasets,[0],[0]
See the Supplement (Sect. 1) for details.,4.1.2 Hyperparameters and Implementation,[0],[0]
"To ensure that results are independent of the choice of image and text features, we use 5 (2 image + 3 text) features of varied dimensionality (64- d, 128-d, 300-d, 2,048-d) and two directions, textto-image (T → I) and image-to-text (I → T ).",4.1.3 Image and Text Features,[0],[0]
We make our extracted features publicly available.3 Text.,4.1.3 Image and Text Features,[0],[0]
"In ImageNet we use 300-dimensional GloVe4 (Pennington et al., 2014) and 300-d word2vec (Mikolov et al., 2013) word embeddings.",4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, we employ stateof-the-art bidirectional gated recurrent unit (biGRU) features (Cho et al., 2014) that we learn with a classification task (see Sect.",4.1.3 Image and Text Features,[0],[0]
2 of Supplement).,4.1.3 Image and Text Features,[0],[0]
Image.,4.1.3 Image and Text Features,[0],[0]
"For ImageNet, we use the publicly available5 VGG-128 (Chatfield et al., 2014) and ResNet (He et al., 2015) visual features from Collell et al. (2017), where we obtained 128- dimensional VGG-128 and 2,048-d ResNet features from the last layer (before the softmax) of the forward pass of each image.",4.1.3 Image and Text Features,[0],[0]
The final representation for a word is the average feature vector (centroid) of all available images for this word.,4.1.3 Image and Text Features,[0],[0]
"In IAPR TC-12 and Wiki, features for individual images are obtained similarly from the last layer of a ResNet and a VGG-128 model.",4.1.3 Image and Text Features,[0],[0]
"We include six benchmarks, comprising three types of concept similarity: (i) Semantic similarity: SemSim (Silberer and Lapata, 2014), Simlex999 (Hill et al., 2015) and SimVerb-3500 (Gerz et al., 2016); (ii) Relatedness: MEN (Bruni et al.,
3http://liir.cs.kuleuven.be/software.html 4http://nlp.stanford.edu/projects/glove 5http://liir.cs.kuleuven.be/software.html
2014) and WordSim-353 (Finkelstein et al., 2001); (iii) Visual similarity: VisSim (Silberer and Lapata, 2014) which includes the same word pairs as SemSim, rated for visual similarity instead of semantic.",4.2.1 Datasets,[0],[0]
"All six test sets contain human ratings of similarity for word pairs, e.g., (‘cat’,‘dog’).",4.2.1 Datasets,[0],[0]
"The parameters in W0,W1 are drawn from a random uniform distribution [−1, 1] and b0, b1 are set to zero.",4.2.2 Hyperparameters and Implementation,[0],[0]
"We use a tanh activation σ().6 The output dimension dy is set to 2,048 for all embeddings.",4.2.2 Hyperparameters and Implementation,[0],[0]
Textual and visual features are the same as described in Sect.,4.2.3 Image and Text Features,[0],[0]
4.1.3 for the ImageNet dataset.,4.2.3 Image and Text Features,[0],[0]
"We compute the prediction of similarity between two vectors z1, z2 with both the cosine z1z2‖z1‖‖z2‖ and the Euclidean similarity 11+‖z1−z2‖ .",4.2.4 Similarity Predictions,[0],[0]
7,4.2.4 Similarity Predictions,[0],[0]
"As is common practice, we evaluate the predictions of similarity of the embeddings (Sect. 4.2.4) against the human similarity ratings with the Spearman correlation ρ.",4.2.5 Performance Metrics,[0],[0]
We report the average of 10 sets of randomly generated parameters.,4.2.5 Performance Metrics,[0],[0]
We test statistical significance with a two-sided Wilcoxon rank sum test adjusted with Bonferroni.,5 Results and Discussion,[0],[0]
The null hypothesis is that a compared pair is equal.,5 Results and Discussion,[0],[0]
"In Tab. 1, ∗ indicates that mNNO(X, f(X)) differs from mNNO(Y, f(X))",5 Results and Discussion,[0],[0]
"(p < 0.001) on the same mapping, embedding and direction.",5 Results and Discussion,[0],[0]
"In Tab. 2, ∗ indicates that performance of mapped and input vectors differs (p < 0.05) in the 10 runs.",5 Results and Discussion,[0],[0]
Results below are with cosine neighbors and K = 10.,5.1 Experiment 1,[0],[0]
Euclidean neighbors yield similar results and are thus left to the Supplement.,5.1 Experiment 1,[0],[0]
"Similarly, results in ImageNet with GloVe embeddings are shown below and word2vec results in the Supplement.",5.1 Experiment 1,[0],[0]
"The choice ofK = {5, 10, 30} had no visible effect on results.",5.1 Experiment 1,[0],[0]
Results with 3- and 5-layer nets did not show big differences with the results below (see Supplement).,5.1 Experiment 1,[0],[0]
"The cosine and max-margin losses
6We find that sigmoid and ReLu yield similar results.",5.1 Experiment 1,[0],[0]
"7Notice that papers generally use only cosine similarity
(Lazaridou et al., 2015b; Pennington et al., 2014).
performed slightly worse than MSE (see Supplement).",5.1 Experiment 1,[0],[0]
"Although Lazaridou et al. (2015a) and Weston et al. (2011) find that max-margin performs the best in their tasks, we do not find our result entirely surprising given that max-margin focuses on inter-class differences while we look also at intraclass neighbors (in fact, we do not require classes).
",5.1 Experiment 1,[0],[0]
"Tab. 1 shows our core finding, namely that the semantic structure of f(X) resembles more that of X than that of Y , for both lin and nn maps.
",5.1 Experiment 1,[0],[0]
Fig. 2 is particularly revealing.,5.1 Experiment 1,[0],[0]
"If we would only look at train performance (and allow train MSE to reach 0) then f(X) = Y and clearly train mNNO(f(X), Y )",5.1 Experiment 1,[0],[0]
"= 1 while mNNO(f(X), X) can only be smaller than 1.",5.1 Experiment 1,[0],[0]
"However, the interest is always on test samples, and (near-)perfect test prediction is unrealistic.",5.1 Experiment 1,[0],[0]
"Notice in fact in Fig. 2 that even if we look at train fit, MSE needs to be close to 0 for mNNO(f(X), Y ) to be
reasonably large.",5.1 Experiment 1,[0],[0]
"In all the combinations from Tab. 1, the test mNNO(f(X), Y ) never surpasses test mNNO(f(X), X) for any number of epochs, even with an oracle (not shown).",5.1 Experiment 1,[0],[0]
"Tab. 2 shows that untrained linear (flin) and neural net (fnn) mappings preserve the semantic structure of the input X , complementing thus the findings of Experiment 1.",5.2 Experiment 2,[0],[0]
"Experiment 1 concerns learning, while, by “ablating” the learning part and randomizing weights, Experiment 2 is revealing about the natural tendency of neural nets to preserve semantic information about the input, regardless of the choice of the target vectors and loss function.",5.2 Experiment 2,[0],[0]
"Overall, we uncovered a phenomenon neglected so far, namely that neural net cross-modal mappings can produce mapped vectors more akin to the input vectors than the target vectors, in terms of semantic structure.",6 Conclusions,[0],[0]
Such finding has been possible thanks to the proposed measure that explicitly quantifies similarity between the neighborhood structure of two sets of vectors.,6 Conclusions,[0],[0]
"While other measures such as mean squared error can be misleading, our measure provides a more realistic estimate of the semantic similarity between predicted and target vectors.",6 Conclusions,[0],[0]
"In fact, it is the semantic structure (or pairwise similarities) what ultimately matters in cross-modal applications.",6 Conclusions,[0],[0]
This work has been supported by the CHIST-ERA EU project MUSTER8 and by the KU Leuven grant RUN/15/005.,Acknowledgments,[0],[0]
"Feed-forward networks are widely used in cross-modal applications to bridge modalities by mapping distributed vectors of one modality to the other, or to a shared space.",abstractText,[0],[0]
"The predicted vectors are then used to perform e.g., retrieval or labeling.",abstractText,[0],[0]
"Thus, the success of the whole system relies on the ability of the mapping to make the neighborhood structure (i.e., the pairwise similarities) of the predicted vectors akin to that of the target vectors.",abstractText,[0],[0]
"However, whether this is achieved has not been investigated yet.",abstractText,[0],[0]
"Here, we propose a new similarity measure and two ad hoc experiments to shed light on this issue.",abstractText,[0],[0]
In three cross-modal benchmarks we learn a large number of language-to-vision and visionto-language neural network mappings (up to five layers) using a rich diversity of image and text features and loss functions.,abstractText,[0],[0]
"Our results reveal that, surprisingly, the neighborhood structure of the predicted vectors consistently resembles more that of the input vectors than that of the target vectors.",abstractText,[0],[0]
"In a second experiment, we further show that untrained nets do not significantly disrupt the neighborhood (i.e., semantic) structure of the input vectors.",abstractText,[0],[0]
Do Neural Network Cross-Modal Mappings Really Bridge Modalities?,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1175–1185, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Grapheme-to-phoneme (G2P) conversion is the problem of converting a string of letters into a string of phonetic symbols.,1 Introduction,[0],[0]
"Closely related to G2P are other string transduction problems in natural language processing (NLP) such as transliteration (Sherif and Kondrak, 2007),",1 Introduction,[0],[0]
"lemmatization (Dreyer et al., 2008), and spelling error correction (Brill and Moore, 2000).",1 Introduction,[0],[0]
"The classical learning paradigm in each of these settings is to train a model on pairs of strings {(x,y)} and then to evaluate model performance on test data.",1 Introduction,[0],[0]
"While there are exceptions (e.g., (Rao et al., 2015)), most state-of-the-art modelings (e.g., (Jiampojamarn et al., 2007; Bisani and Ney, 2008; Jiampojamarn et al., 2008; Jiampojamarn et al., 2010; Novak et al., 2012)) view string transduction as a two-stage process in which string pairs (x,y) in the training data are first aligned, and then a subsequent (e.g., sequence labeling) module is learned on the aligned data.
",1 Introduction,[0],[0]
"State-of-the-art alignments in G2P are characterized by the following properties:
(i) Alignments are monotone in that the ordering of characters in input and output sequences is preserved by the alignments.",1 Introduction,[0],[0]
"Furthermore, they are many-to-many in the sense that several x sequence characters may be matched up with several y sequence characters as illustrated in Table 1.
(ii)",1 Introduction,[0],[0]
"The alignment is a latent variable and learnt in an unsupervised manner from pairs of strings in the training data.
",1 Introduction,[0],[0]
"(iii) The unsupervised alignment models are unigram alignment models insofar as the overall score that the alignment model assigns an alignment is the same for all orderings of the matched-up subsequences (context independence).
",1 Introduction,[0],[0]
"To illustrate point (iii), consider, in the field of lemmatization, the case of aligning an inflected word form with the extended infinitive in German, such as absagt (‘rejects’) with abzusagen (‘to reject’).",1 Introduction,[0],[0]
"Critically, the insertion -zu- appears in infixal position and a plausible alignment might be as in Table 2.",1 Introduction,[0],[0]
"Then, correctly aligning certain
analogous forms such as zusagt (‘accepts’) with
1175
their corresponding extended infinitive zuzusagen (‘to accept’) is beyond the scope of a unigram alignment model since this cannot distinguish the linguistically correct alignment from the following linguistically incorrect alignment
z u s a g t zu z u s a g en
precisely because it has no notion of context.",1 Introduction,[0],[0]
"In this work, we firstly address bigram alignment models in G2P.",1 Introduction,[0],[0]
"We investigate whether there are phenomena in G2P that require bigram alignment models and, more generally, whether bigram alignment models produce better alignments — with respect to a human gold standard — than unigram alignment models within the G2P setting.",1 Introduction,[0],[0]
"We do so, secondly, in a supervised setting where the model learns from gold-standard alignments.",1 Introduction,[0],[0]
"While this may seem an odd scenario at first sight, modern alignment toolkits in the related field of machine translation typically include the possibility to learn both in a supervised and unsupervised manner (Liu et al., 2010; Liu and Sun, 2015).",1 Introduction,[0],[0]
"The rationale behind supervised learning models may be that they perform better than unsupervised models, and if alignment quality has a large impact upon subsequent string translation performance, then a supervised model may be a suitable alternative.",1 Introduction,[0],[0]
"Thirdly, we investigate how alignment quality affects overall G2P performance.",1 Introduction,[0],[0]
"This allows us to address whether it is worthwhile to work on better alignment models, which bigram and supervised alignment models promise to be.",1 Introduction,[0],[0]
"To our knowledge, all three outlined aspects of alignments — bigram models, supervised learning, and systematically estimating the relationship between alignment quality and overall string transduction performance — are novel in the G2P setting and its related fields as outlined; however, see also the related work section.
",1 Introduction,[0],[0]
This work is structured as follows.,1 Introduction,[0],[0]
Section 2 presents definitions and algorithms for uni- and bigram alignment models.,1 Introduction,[0],[0]
Section 3 surveys related work.,1 Introduction,[0],[0]
Section 4 presents our data and Section 5 our experiments.,1 Introduction,[0],[0]
We conclude in Section 6.,1 Introduction,[0],[0]
We first formally define the problem of aligning two strings x and y over arbitrary alphabets in a monotone and many-to-many manner.,2 Uni- and bigram alignment models,[0],[0]
Let `x = |x|,2 Uni- and bigram alignment models,[0],[0]
"and `y = |y| denote the lengths of x and y, respectively.",2 Uni- and bigram alignment models,[0],[0]
"Let N = {0, 1, 2, . . .}, and let S ⊆
N2\{(0, 0)} be a set defining the valid match-up operations between x characters and y characters.",2 Uni- and bigram alignment models,[0],[0]
"In other words, when (s, t) ∈ S, then this means we allow matches of subsequences of x of length s and subsequences of y of length t.1
It is convenient to define a monotone many-tomany alignment of x and y as a 2×k (for k ≥ 1 arbitrary) nonnegative integer matrix Ax,y ∈ N2×k
",2 Uni- and bigram alignment models,[0],[0]
"satisfying Ax,y1k = ( `x `y ) , i.e., the two rows of Ax,y sum up to the lengths of the respective strings,2 and where each column of Ax,y lies in S. For any such alignment, we let (x1, . . .",2 Uni- and bigram alignment models,[0],[0]
",xk) be the corresponding induced segmentation of x and (y1, . . .",2 Uni- and bigram alignment models,[0],[0]
",yk) be the corresponding induced segmentation of y.
Example.",2 Uni- and bigram alignment models,[0],[0]
"For any S ⊇ {(1, 1), (1, 2), (2, 1)}, the alignment of x = phoenix and y = finIks shown in Table 1 may be represented by the ma-
trix Ax,y = (
2 2 1 1 1 1 1 1 1 2
) .",2 Uni- and bigram alignment models,[0],[0]
"The correspond-
ing induced segmentations are (ph,oe,n,i,x) and (f,i,n,I,ks).
",2 Uni- and bigram alignment models,[0],[0]
"Let AS(x,y) denote the class of all alignments of x and y.",2 Uni- and bigram alignment models,[0],[0]
"We call a function f : AS(x,y)→ R an alignment model.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a unigram alignment model if f takes the form, for any Ax,y ∈ AS(x,y),
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim1(xi,yi) (1)
where sim1 is an arbitrary (real-valued) similarity function measuring similarity of two subsequences.",2 Uni- and bigram alignment models,[0],[0]
"We call an alignment model f a bigram alignment model if f takes the form
f(Ax,y) = k∑ i=1",2 Uni- and bigram alignment models,[0],[0]
"sim2 ( (xi,yi), (xi−1,yi−1) )",2 Uni- and bigram alignment models,[0],[0]
"(2)
where sim2 is an arbitrary (real-valued) similarity function measuring similarity of successive pairs of subsequences.
Example.",2 Uni- and bigram alignment models,[0],[0]
"Let sim1(u,v) be equal to |u| · |v| and let funi(Ax,y) be as in Eq. (1).",2 Uni- and bigram alignment models,[0],[0]
"Then, funi is a unigram alignment model that assigns the score
1This is sometimes denoted in the manner M -N (e.g., 3- 2, 1-0), indicating that M characters of one string may be matched up with N characters of the other string.",2 Uni- and bigram alignment models,[0],[0]
"Analogously, we could write here s-t rather than (s, t).
2Here, 1k denotes the unit vector of dimension k.",2 Uni- and bigram alignment models,[0],[0]
"given in Table 2.
Example.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let sim2 ( (u,v), (u′,v′) )",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
= (|u| · |v|)|v′| if |u| = |u′| − 1 or u = v and −2 otherwise.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Let fbi(Ax,y) be as in Eq.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
(2).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Then, fbi is a bigram alignment model assigning the score (1 · 1)0 + (1 · 1)1 + (0 · 2)1 + (1 · 1)2 + (1 · 1)1 + (1 · 1)1",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"− 2 = 3 to the alignment in Table 2.
",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"In statistical alignment modeling, the task is to find an optimal alignment (i.e., one with maximal score) given strings x and y and given the alignment model f .",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a unigram model, this can be solved efficiently via dynamic programming (DP).",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"When f is a bigram alignment model, then finding the optimal alignment can still be solved via DP, by introducing a variable Mijqw denoting the score of the best alignment of x(1 : i) and y(1 : j) that ends in the matchup of x(q : i) with y(w : j).3 The variable Mijqw satisfies a recurrence leading to a DP algorithm, shown in Algorithm 1.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
The actual alignment can be found by storing pointers to the maximizing steps taken.,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Running time of the algorithm is O(`2x`2y|S|).,1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Note also that the sketched algorithm is supervised insofar as it assumes that the similarity values sim2(·, ·) are known.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
"Typically, such alignment algorithms can be converted into unsupervised algorithms in which similarity measures sim are learnt iteratively, e.g., in an EM-like fashion (cf., e.g., Eger (2012), Eger (2013)); however, in this paper, we only investigate the supervised base version as indicated.",1 + 1 + 0 + 1 + 1 + 1 + 2 = 7 to the alignment,[0],[0]
Monotone alignments have a long tradition in NLP.,3 Related work,[0],[0]
"The classical Needleman-Wunsch algorithm (Needleman and Wunsch, 1970) computes the optimal alignment between two sequences when only single character matches, mismatches, and skips are allowed.",3 Related work,[0],[0]
"It is a special case of the unigram model (1) for which S = {(1, 0), (0, 1), (1, 1)} and sim1 takes on values from {0,−1}, depending on whether compared subsequences match or not.",3 Related work,[0],[0]
"As is well-known, this alignment specification is equivalent to the edit distance problem (Levenshtein, 1966) in which the minimal number of insertions, deletions and substitutions is sought that transforms one string
3We denote by x(a : b) the substring xaxa+1 · · ·xb of the string x1x2 · · ·xt.
into another.",3 Related work,[0],[0]
"Substring-to-substring edit operations — or equivalently, (monotone) many-tomany alignments — have appeared in the NLP context, e.g., in Deligne et al. (1995), Brill and Moore (2000), Jiampojamarn et al. (2007), Bisani and Ney (2008), Jiampojamarn et al. (2010), or, significantly earlier, in Ukkonen (1985), Véronis (1988).",3 Related work,[0],[0]
"Learning edit distance/monotone alignments in an unsupervised manner has been the topic of, e.g., Ristad and Yianilos (1998), Cotterell et al. (2014), besides the works already mentioned.",3 Related work,[0],[0]
"All of these approaches are special cases of our unigram model — i.e., they consider particular S (most prominently, S = {(1, 0), (0, 1), (1, 1)})",3 Related work,[0],[0]
"and sim1.4 Eger (2015b), Yao and Kondrak (2015), and Eger (2015a) generalize to alignments of multiple strings, but likewise only consider unigram alignment models in their experiments.
",3 Related work,[0],[0]
Probably the most closely related work to ours is Jiampojamarn and Kondrak (2010).,3 Related work,[0],[0]
"There, older and specialized alignment techniques such as ALINE (Kondrak, 2000) (as well as partly heuristic/semi-automatic alignment methods) are compared with variants of the M2M alignment algorithm, which we also survey.",3 Related work,[0],[0]
"This work does not consider supervised alignments or bigram alignments, as we do.",3 Related work,[0],[0]
"Moreover, Jiampojamarn and Kondrak (2010) also evaluate the impact of alignment quality on overall G2P system accuracy by running a few experiments, finding that better alignment quality does not always translate into better G2P accuracy, but that there is a “strong correlation” between the two.",3 Related work,[0],[0]
"We more thorougly investigate this question, using, arguably, more heterogeneous aligners, and many more experiments.",3 Related work,[0],[0]
"We also quantitatively estimate how alignment quality influences G2P system accuracy on two different languages via linear regression.
",3 Related work,[0],[0]
"Goldwater et al. (2006) study the effect of context in (unsupervised) word/sequence segmentation, which may be considered the onedimensional specialization of sequence alignment, using a Bayesian method.",3 Related work,[0],[0]
"They find that bigram models greatly outperform unigram models for their task.
",3 Related work,[0],[0]
"Of course, our study is also related to the field of machine translation and its studies on the rela-
4In Cotterell et al. (2014), context influences alignments, so that the approach goes beyond the unigram model sketched in (1) (but does not allow for many-to-many match-ups).",3 Related work,[0],[0]
"The contextual dependencies in this model are set up differently from the bigram dependencies in our paper.
",3 Related work,[0],[0]
Algorithm 1 1: procedure BIGRAM-ALIGN(x = x1 . . .,3 Related work,[0],[0]
"xn,y = y1 . . .",3 Related work,[0],[0]
"ym; S, sim2) 2:",3 Related work,[0],[0]
"Mijqw ← −∞ for all (i, j, q, w) ∈ Z4 3: M0000 ← 0 4: for i = 0 . . .",3 Related work,[0],[0]
n,3 Related work,[0],[0]
do 5: for j = 0 . .,3 Related work,[0],[0]
.m,3 Related work,[0],[0]
do 6: for q = 0 . . .,3 Related work,[0],[0]
i+ 1 do 7: for w = 0 . . .,3 Related work,[0],[0]
"j + 1 do 8: if (i, j, q, w) 6=",3 Related work,[0],[0]
"(0, 0, 0, 0) then 9: if (i− q + 1, j − w + 1) ∈ S then
10: Mijqw= max (a,b)∈S Mq−1,w−1,q−a,w−b+sim2
(( x(q:i),y(w:j) ) , ( x(q−a:q−1),y(w−b:w−1) ))
",3 Related work,[0],[0]
"tionship between alignment quality and translation performance (Ganchev et al., 2008).",3 Related work,[0],[0]
"In machine translation, the monotonicity assumption of string transduction does typically not hold, however, rendering alignment and translation techniques different and more heuristic in nature.",3 Related work,[0],[0]
"For English, we conduct experiments on the General American (GA) variant of the Combilex data set (Richmond et al., 2009).",4.1 Data,[0],[0]
This contains about 128 000 grapheme-phoneme pairs as exemplified in Table 3.,4.1 Data,[0],[0]
"Importantly, Combilex provides goldstandard alignments, which we will make use of for the supervised alignment models as well as for measuring alignment quality.",4.1 Data,[0],[0]
"For German, we ran-
domly extract 3 000 G2P string pairs from CELEX (Baayen et al., 1995).",4.1 Data,[0],[0]
"We had a native speaker manually align them so that gold standard alignments are available here, too.",4.1 Data,[0],[0]
"Both data sets contain quite complex match-ups of character subsequences such as (2,3) as in English s-oi-r-ee-s/swOA-r-P-z or (4,1) as in w-eigh-t/w-P-t but the majority of match-ups are of type (1,1), (2,1), and, to a lesser degree, (1,2) and (3,1).",4.1 Data,[0],[0]
"The M2M aligner (Jiampojamarn et al., 2007), which is based on EM maximum likelihood estimation of alignment parameters, is the classical unsupervised unigram many-to-many aligner in G2P.",4.2 Alignment toolkits/models,[0],[0]
"As has been pointed out (Kubo et al., 2011), M2M greatly overfits the data.5",4.2 Alignment toolkits/models,[0],[0]
"This means that when the M2M aligner is given the freedom to align two sequences without restrictions, it matches them up as a whole.",4.2 Alignment toolkits/models,[0],[0]
"The reason is that a (probabilistic) unigram alignment model adds log-probabilities of matched-up subsequences, which, if not appropriately corrected for, makes alignments with few match-ups a priori more likely than alignments with many matchups, when probabilities of individual match-ups are uniformly or randomly initialized (as is typically the case for EM maximum likelihood estimation in unsupervised models).",4.2 Alignment toolkits/models,[0],[0]
"To address this, M2M must artifically restrain, in our language, the set S to be {(1, 1), (1, 2), (2, 1)}.",4.2 Alignment toolkits/models,[0],[0]
"In contrast, the Mpaligner (Kubo et al., 2011) introduces a prior (or penalty) in the alignment model which favors ‘short’ matches (s, t) over ‘long’ ones.",4.2 Alignment toolkits/models,[0],[0]
"Finally, the Phonetisaurus aligner (Novak et al., 2012) modifies the M2M aligner by adding additional soft constraints.
",4.2 Alignment toolkits/models,[0],[0]
"Our own alignment model is, as indicated, supervised.",4.2 Alignment toolkits/models,[0],[0]
"We implement a unigram alignment model where we specify sim1(u,v) as
α · logp((u,v))",4.2 Alignment toolkits/models,[0],[0]
"+ β · logp((|u|, |v|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u) + δ · logp(v).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z) denotes the log-probability — estimated from the training data — of observing the
5See also the discussion in (Goldwater et al., 2006) for the related word segmentation problem.
object z, and α, β, γ and δ are parameters.",4.2 Alignment toolkits/models,[0],[0]
"This specification says that the subsequences u and v are similar insofar as (i) u and v have been paired frequently in the training data, (ii) the length of u and the length of v have been paired frequently, (iii)/(iv) u/v by itself is likely.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this unigram alignment model as uniα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
"We also implement a bigram alignment model where we specify sim2 ( (u,v), (u′,v′) )",4.2 Alignment toolkits/models,[0],[0]
"as
α · logp((u,v) | (u′,v′))",4.2 Alignment toolkits/models,[0],[0]
"+β · logp((|u|, |v|)",4.2 Alignment toolkits/models,[0],[0]
"| (|u′|, |v′|))",4.2 Alignment toolkits/models,[0],[0]
"+γ · logp(u|u′)+ δ · logp(v|v′).
",4.2 Alignment toolkits/models,[0],[0]
"Here, logp(z | z′) denotes the logarithm of the conditional probability of observing the object z following the object z′.",4.2 Alignment toolkits/models,[0],[0]
"We refer to this bigram alignment model as biα,β,γ,δ.",4.2 Alignment toolkits/models,[0],[0]
We use two string transduction systems for our experiments.,4.3 Transduction systems,[0],[0]
"The first one is DirecTL+ (Jiampojamarn et al., 2010), a discriminative string-tostring translation system incorporating joint ngram features.",4.3 Transduction systems,[0],[0]
DirecTL+ is an extension of the model presented in Jiampojamarn et al. (2008) which treats string transduction as a source sequence segmentation and subsequent sequence labeling task.,4.3 Transduction systems,[0],[0]
"In addition, we use Phonetisaurus (Novak et al., 2012), a weighted finite state-based joint n-gram model employing recurrent neural network language model N -best rescoring in decoding.",4.3 Transduction systems,[0],[0]
Both systems take aligned pairs of strings as input and from this construct a monotone translation model.6,4.3 Transduction systems,[0],[0]
We employ two measures of alignment quality.,4.4 Measuring alignment quality,[0],[0]
"First, we use word accuracy, defined as the fraction of correctly aligned sequence pairs in a test sample.",4.4 Measuring alignment quality,[0],[0]
This is a very strict measure that penalizes even tiny deviations from the gold standard.,4.4 Measuring alignment quality,[0],[0]
"Additionally, we measure the edit distance between the true alignment Ax,y and the predicted alignment Âx,y. To implement this, we view the two induced segmentations that constitute an alignment — e.g., (ph,oe,n,i,x) and (f,i,n,I,ks) — as strings including splitting signs.",4.4 Measuring alignment quality,[0],[0]
"Thus, we can compute the edit distance between the gold-standard segmented x
6We run both systems with parameters determined by some manual tuning, without trying to systematically optimize their individual performances, however.
string and the predicted segmentation, and analogously for the y sequence.",4.4 Measuring alignment quality,[0],[0]
"Then, we define the edit distance between Ax,y and Âx,y as the sum of these two string edit distances.",4.4 Measuring alignment quality,[0],[0]
"For a test sample, we indicate so-defined average edit distance, averaged over all pairs in the sample.",4.4 Measuring alignment quality,[0],[0]
"To measure alignment quality for the different systems, for English, we run experiments on sets of size x+5 000, where x = 1 000, 2 000, 5 000, 10 000, and 20 000.",5.1 Alignment quality,[0],[0]
"For the supervised models, we consider x as the training data and the 5 000 additional string pairs as test data.7 To quantify effects when training data is very little, we let x also range over 100 and 500 string pairs for the supervised models.",5.1 Alignment quality,[0],[0]
"For the unsupervised models, we simply take all x+5 000 string pairs as data to learn from (but evaluate performance only on the 5 000 string pairs, for comparability).
",5.1 Alignment quality,[0],[0]
"Results are shown in Tables 4, 5, and 6.",5.1 Alignment quality,[0],[0]
"We first note (Table 4) that the unsupervised models perform decently, obtaining accuracy rates of 80% and beyond under appropriate parametrizations.",5.1 Alignment quality,[0],[0]
"We also observe the M2M aligner’s deterioration in performance as we increase its degrees of freedom (allowing it to match subsequences of larger length), confirming our previous remarks.",5.1 Alignment quality,[0],[0]
The Mpaligner does not suffer from this problem as it penalizes large matches.,5.1 Alignment quality,[0],[0]
"Phonetisaurus suffers from the same problems as M2M, but to a lesser degree.",5.1 Alignment quality,[0],[0]
"Overall, we find that, under optimal parametrizations, Phonetisaurus produces best alignments, followed by Mpalign and M2M.",5.1 Alignment quality,[0],[0]
"However, peak performances of all three unsupervised aligners are close.",5.1 Alignment quality,[0],[0]
"Unsurprisingly, the supervised alignment models perform better than the unsupervised ones (Tables 5 and 6).",5.1 Alignment quality,[0],[0]
"Surprisingly, however, they do so with very little training data; fewer than 100 aligned string pairs suffice to outperform the unsupervised models under good calibrations.",5.1 Alignment quality,[0],[0]
"When there is sufficient training data, the supervised models perform splendidly, with a peak accuracy of 99.43% for the bigram alignment model that includes appropriate features (scoring lengths of aligned subsequences,
7For all our below experiments involving the supervised aligners, we set S to a (‘pessimistically’ large) value of {(a, b) | 1 ≤ a ≤ 6, 1 ≤ b ≤ 6}.",5.1 Alignment quality,[0],[0]
"Also, for the bigram models, we add special sequence boundary markers.
etc.).",5.1 Alignment quality,[0],[0]
"We also note that the bigram alignment model is almost consistently better than the unigram alignment model, with a surplus of about 1% point, depending on specific parametrizations.
",5.1 Alignment quality,[0],[0]
We performed an analogous analysis for the German data.,5.1 Alignment quality,[0],[0]
"Results are quite similar except that unigram and bigram alignment model have indistinguishable performance on the German data, indicating (the known fact) that G2P is a more complex task in English, apparently not requiring bigram alignment models.
",5.1 Alignment quality,[0],[0]
"Error analysis Concerning errors that the unigram model commits and the bigram model does not, the majority of errors (roughly 80%) involve match-ups of ed/d and d.",5.1 Alignment quality,[0],[0]
"For example, the unigram model aligns as in
t w",5.1 Alignment quality,[0],[0]
i n k le d t,5.1 Alignment quality,[0],[0]
w I N k @l,5.1 Alignment quality,[0],[0]
"d
while the gold-standard alignment is
t w i n k",5.1 Alignment quality,[0],[0]
l ed t w,5.1 Alignment quality,[0],[0]
I N k @l,5.1 Alignment quality,[0],[0]
"d
While all match-ups in both alignments are plausible, the bigram model assigns here higher probability to the correct ed/d match-up in terminal position (consistently favored in the data set), which
has a particular meaning there, namely, that of a suffix marker for past tense.8,9 In the German data, there is a single instance where the unigram and bigram alignment model disagree, namely, in the alignment of s-t-o-ff-f-l-a-sch-e/S-t-O-f-f-l-&S-@, which the unigram model falsely aligns as s-t-o-f-ff-l-a-sch-e/S-t-O-f-f-l-&-S-@; note that in the correct alignment f must follow ff, not vice versa, which depends on context information, e.g., that o/O signifies a short vowel which is followed by a double consonant, not a single consonant.
",5.1 Alignment quality,[0],[0]
"All remaining errors that the bigram alignment models commits are, for the best considered parametrization and training set size, typically due to match-up types not seen in the training data, and thus mostly concern foreign names or writings (e.g., Bh-u-tt-o/b-u-t-F, falsely aligned as B-hu-tto/b-u-t-F).",5.1 Alignment quality,[0],[0]
"A few other errors might be corrected when the feature coefficients α, β, γ, δ were optimized on a development set rather than set manually.",5.1 Alignment quality,[0],[0]
"We find no indication that our G2P data, either for English or German, would further benefit from n-gram alignment models of order n > 2.",5.1 Alignment quality,[0],[0]
"Next, we estimate the relationship between alignment quality and overall G2P performance (transcription accuracy).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To this end, for the English data, we use the 5 000 aligned string pairs from the previous experiment on alignment quality and feed them in — as training data — to either DirecTL+ or Phonetisaurus as outlined in Section 4.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We then evaluate G2P performance — in terms of word accuracy (fraction of correctly transcribed strings) — on a distinct test set of size 10 000.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
Figure 1 shows a plot of overall G2P accuracy vs. training set size for the aligner (ranging over the x values in the last section); and a second plot that sketches G2P accuracy as a function of corresponding alignment accuracy.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"We first note that, as the supervised aligner receives more training
8Similar cases are, e.g., alignments of the type f-ee-d-ba-ck/f-i-d-b-a-k, which the unigram model falsely aligns as f-e-ed-b-a-ck/f-i-d-b-a-k. Here, too, the unigram is unable to account for the almost exclusive terminal position of the ed/d match-up in the data.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"9Other errors involve ‘unusual/foreign’ spelling/pronunciation pairs such as Ph-oe-n-i-c-ia/f-@n-i-S-@ (wrongly aligned as Ph-o-en-i-c-ia/f-@-n-i-S-@ by the unigram model) or m-a-d-e-m-oi-s-e-ll-e-’s/m-a-d-@-mw@-z-E-l-0-z (m-a-d-e-m-o-i-s-e-ll-e-’s/m-a-d-@-m-w-@z-E-l-0-z), where the bigram alignment model has apparently gathered the more appropriate statistics.
data from which to align the 5 000 string pairs, the overall G2P accuracy of both DirecTL+ and Phonetisaurus increase substantially (and as a convex function of training set size).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Apparently, the better alignments produced by more training data for the particular supervised aligner considered directly translate into better overall G2P accuracy.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The other plot in the figure shows that, indeed, there seems to be a linear trend coupling alignment quality with overall G2P performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 7 pairs G2P accuracy with alignment accuracy of selected systems, all run in the x = 20 000 setting.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"While, in the table, better alignments do not necessarily imply better overall G2P performance, the two best alignments also lead to the two best overall G2P performances (although, in this case, the second best alignment is paired with the best overall G2P performance); conversely, the worst alignment quality is coupled with the worst overall G2P performance.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Overall, we ran 249 experiments (including the German data) in which we trained DirecTL+ or Phonetisaurus with alignments of specific quali-
ties obtained from particularly parametrized aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In each of these cases, we obtained an alignment quality score and a subsequent overall G2P system performance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
The English part of this data is sketched in Figure 2.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This figure seems to corroborate the linear relationship (apparently present in Figure 1) between alignment quality and overall G2P system accuracy, particularly, when alignment quality is measured in the more finegrained metric of edit distance.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To formally test
this, we regress overall G2P system performance (measured in word accuracy) on edit distance and other variables.10 This yielded the coefficients as given in Table 8; in each case, the goodness-offit of the linear model was quite large, with R2 values above 90% for the English data and about 84% for the German data.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Also, the coefficients on alignment quality were highly significantly different from zero.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"The table shows that the coefficients are on the order of about −3.80% to −4.70%, meaning that, all else being equal, increasing alignment quality by 1 edit distance to the gold-standard alignment increases overall G2P by about 3.80 to 4.70%.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"So far, we have estimated the effects of alignment quality on overall G2P system performance for a fixed size of training data, namely, 5 000 aligned string pairs.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"To see whether this relationship changes when we vary the amount of training data, we run several more experiments.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"In these, we align training sets of sizes 100, 500,
10These include binary dummy variables for the specific systems as well as alignment consistency and its square — measured in conditional entropy H(Y |X)",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"(Pervouchine et al., 2009) — in the regression.
",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"1 000, 2 000, 10 000, 20 000, 40 000 and 60 000 via our several alignment systems.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
Then we feed the aligned data to the Phonetisaurus system (we omit DirecTL+ here because of its long run times) and compute overall G2P accuracy on a disjoint test set of size 28 000 approximately.,5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This time, we only use the unsupervised aligners and the gold-standard alignments directly, omitting results for our various supervised aligners.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Note, however, that these aligners could, in principle, imitate the gold-standard alignments with a very high degree of precision, as previously seen.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Table 9
shows that training G2P systems from the human gold standard alignments in each case yields better overall G2P transcriptions than training them from either of the three unsupervised alignments considered here.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"However, we note that the surplus over the unsupervised alignments decreases as training set size increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"This may be due to the fact that the unsupervised aligners themselves create better alignments once they are boot-
strapped from larger data sets (cf. Table 4).",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Additionally, the effect of alignment quality on overall G2P system performance may simply vanish as training set sizes become large enough because the translation modules can better accomodate ‘noisy’ data as long as its size is sufficiently large.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
"Figure
3 sketches the decreasing influence of alignment system on overall G2P system performance as size of the aligned data increases.",5.2 Alignment quality vs. overall G2P performance,[0],[0]
We have investigated the need for bigram alignment models and the benefit of supervised alignment techniques in G2P. We have also quantitatively estimated the relationship between alignment quality and overall G2P system performance.,6 Conclusion,[0],[0]
"We have found that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task (we find almost no differences between unigram and bigram models for the German sample of G2P data we considered).",6 Conclusion,[0],[0]
"Moreover, we have found that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",6 Conclusion,[0],[0]
"Finally, we have estimated a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",6 Conclusion,[0],[0]
"At a particular training size, a linear regression model has estimated that improving alignment quality by 1 edit distance toward the
gold standard alignments leads to an 3.80-4.70% increase in G2P transcription accuracy.",6 Conclusion,[0],[0]
"However, we have also found that the importance of good alignments on G2P accuracy appears to dimish as data set size increases, possibly because the translation modules can accomodate more ‘noisy’ data in this scenario.
",6 Conclusion,[0],[0]
"As a ‘policy’ implication, we recommend the use of supervised alignment techniques particularly when the size of the G2P corpus is small or when high quality alignments, as an end in themselves, are required.",6 Conclusion,[0],[0]
"In this case, constructing a few dozen or few hundred alignments in an unsupervised manner and correcting them by hand (to serve as an input for a supervised technique) may be highly beneficial.
",6 Conclusion,[0],[0]
"In future work, it may be worthwhile to study the impact of alignment techniques on overall system performance in other string transduction problems such as transliteration, lemmatization, and spelling error correction.
",6 Conclusion,[0],[0]
Our supervised uni- and bigram aligners are available via https://github.com/ SteffenEger/.,6 Conclusion,[0],[0]
I thank three anonymous reviewers and Tim vor der Brück for valuable suggestions.,Acknowledgments,[0],[0]
We investigate the need for bigram alignment models and the benefit of supervised alignment techniques in graphemeto-phoneme (G2P) conversion.,abstractText,[0],[0]
"Moreover, we quantitatively estimate the relationship between alignment quality and overall G2P system performance.",abstractText,[0],[0]
"We find that, in English, bigram alignment models do perform better than unigram alignment models on the G2P task.",abstractText,[0],[0]
"Moreover, we find that supervised alignment techniques may perform considerably better than their unsupervised brethren and that few manually aligned training pairs suffice for them to do so.",abstractText,[0],[0]
"Finally, we estimate a highly significant impact of alignment quality on overall G2P transcription performance and that this relationship is linear in nature.",abstractText,[0],[0]
Do we need bigram alignment models? On the effect of alignment quality on transduction accuracy in G2P,title,[0],[0]
"Ambiguity is one of the defining characteristics of human languages, and language understanding crucially relies on the ability to obtain unambiguous representations of linguistic content.",1 Introduction,[0],[0]
"While some ambiguities can be resolved using intra-linguistic contextual cues, the disambiguation of many linguistic constructions requires integration of world knowledge and perceptual information obtained from other modalities.
",1 Introduction,[0],[0]
"In this work, we focus on the problem of grounding language in the visual modality, and introduce a novel task for language understanding which requires resolving linguistic ambiguities by utilizing the visual context in which the linguistic content is expressed.",1 Introduction,[0],[0]
"This type of inference is frequently called for in human communication that occurs in a visual environment, and is crucial for language acquisition, when much of the linguistic content refers to the visual surroundings of the child (Snow, 1972).
",1 Introduction,[0],[0]
"Our task is also fundamental to the problem of grounding vision in language, by focusing on phenomena of linguistic ambiguity, which are prevalent in language, but typically overlooked when using language as a medium for expressing understanding of visual content.",1 Introduction,[0],[0]
"Due to such ambiguities, a superficially appropriate description of a visual scene may in fact not be sufficient for demonstrating a correct understanding of the relevant visual content.",1 Introduction,[0],[0]
"Our task addresses this issue by introducing a deep validation protocol for visual understanding, requiring not only providing a surface description of a visual activity but also demonstrating structural understanding at the levels of syntax, semantics and discourse.
",1 Introduction,[0],[0]
"To enable the systematic study of visually grounded processing of ambiguous language, we create a new corpus, LAVA (Language and Vision Ambiguities).",1 Introduction,[0],[0]
This corpus contains sentences with linguistic ambiguities that can only be resolved using external information.,1 Introduction,[0],[0]
The sentences are paired with short videos that visualize different interpretations of each sentence.,1 Introduction,[0],[0]
"Our sentences encompass a wide range of syntactic, semantic and dis-
ar X
iv :1
60 3.
08 07
9v 1
[ cs
.C",1 Introduction,[0],[0]
"V
] 2
6 M
ar 2
course ambiguities, including ambiguous prepositional and verb phrase attachments, conjunctions, logical forms, anaphora and ellipsis.",1 Introduction,[0],[0]
"Overall, the corpus contains 237 sentences, with 2 to 3 interpretations per sentence, and an average of 3.37 videos that depict visual variations of each sentence interpretation, corresponding to a total of 1679 videos.
",1 Introduction,[0],[0]
"Using this corpus, we address the problem of selecting the interpretation of an ambiguous sentence that matches the content of a given video.",1 Introduction,[0],[0]
"Our approach for tackling this task extends the sentence tracker introduced in (Siddharth et al., 2014).",1 Introduction,[0],[0]
The sentence tracker produces a score which determines if a sentence is depicted by a video.,1 Introduction,[0],[0]
This earlier work had no concept of ambiguities; it assumed that every sentence had a single interpretation.,1 Introduction,[0],[0]
"We extend this approach to represent multiple interpretations of a sentence, enabling us to pick the interpretation that is most compatible with the video.
",1 Introduction,[0],[0]
"To summarize, the contributions of this paper are threefold.",1 Introduction,[0],[0]
"First, we introduce a new task for visually grounded language understanding, in which an ambiguous sentence has to be disambiguated using a visual depiction of the sentence’s content.",1 Introduction,[0],[0]
"Second, we release a multimodal corpus of sentences coupled with videos which covers a wide range of linguistic ambiguities, and enables a systematic study of linguistic ambiguities in visual contexts.",1 Introduction,[0],[0]
"Finally, we present a computational model which disambiguates the sentences in our corpus with an accuracy of 75.36%.",1 Introduction,[0],[0]
"Previous language and vision studies focused on the development of multimodal word and sentence representations (Bruni et al., 2012; Socher et al., 2013; Silberer and Lapata, 2014; Gong et al., 2014; Lazaridou et al., 2015), as well as methods for describing images and videos in natural language (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012; Socher et al., 2014; Thomason et al., 2014; Karpathy and Fei-Fei, 2014; Siddharth et al., 2014; Venugopalan et al., 2015; Vinyals et al., 2015).",2 Related Work,[0],[0]
"While these studies handle important challenges in multimodal processing of language and vision, they do not provide explicit modeling of linguistic ambiguities.
",2 Related Work,[0],[0]
"Previous work relating ambiguity in language to the visual modality addressed the problem of word
sense disambiguation (Barnard et al., 2003).",2 Related Work,[0],[0]
"However, this work is limited to context independent interpretation of individual words, and does not consider structure-related ambiguities.",2 Related Work,[0],[0]
"Discourse ambiguities were previously studied in work on multimodal coreference resolution (Ramanathan et al., 2014; Kong et al., 2014).",2 Related Work,[0],[0]
"Our work expands this line of research, and addresses further discourse ambiguities in the interpretation of ellipsis.",2 Related Work,[0],[0]
"More importantly, to the best of our knowledge our study is the first to present a systematic treatment of syntactic and semantic sentence level ambiguities in the context of language and vision.
",2 Related Work,[0],[0]
"The interactions between linguistic and visual information in human sentence processing have been extensively studied in psycholinguistics and cognitive psychology (Tanenhaus et al., 1995).",2 Related Work,[0],[0]
"A considerable fraction of this work focused on the processing of ambiguous language (Spivey et al., 2002; Coco and Keller, 2015), providing evidence for the importance of visual information for linguistic ambiguity resolution by humans.",2 Related Work,[0],[0]
"Such information is also vital during language acquisition, when much of the linguistic content perceived by the child refers to their immediate visual environment (Snow, 1972).",2 Related Work,[0],[0]
"Over time, children develop mechanisms for grounded disambiguation of language, manifested among others by the usage of iconic gestures when communicating ambiguous linguistic content (Kidd and Holler, 2009).",2 Related Work,[0],[0]
Our study leverages such insights to develop a complementary framework that enables addressing the challenge of visually grounded disambiguation of language in the realm of artificial intelligence.,2 Related Work,[0],[0]
In this work we provide a concrete framework for the study of language understanding with visual context by introducing the task of grounded language disambiguation.,3 Task,[0],[0]
This task requires to choose the correct linguistic representation of a sentence given a visual context depicted in a video.,3 Task,[0],[0]
"Specifically, provided with a sentence, n candidate interpretations of that sentence and a video that depicts the content of the sentence, one needs to choose the interpretation that corresponds to the content of the video.
",3 Task,[0],[0]
"To illustrate this task, consider the example in figure 1, where we are given the sentence “Sam approached the chair with a bag” along with two different linguistic interpretations.",3 Task,[0],[0]
"In the first in-
terpretation, which corresponds to parse 1(a), Sam has the bag.",3 Task,[0],[0]
"In the second interpretation associated with parse 1(b), the bag is on the chair rather than with Sam.",3 Task,[0],[0]
"Given the visual context from figure 1(c), the task is to choose which interpretation is most appropriate for the sentence.",3 Task,[0],[0]
"To address the grounded language disambiguation task, we use a compositional approach for determining if a specific interpretation of a sentence is depicted by a video.",4 Approach Overview,[0],[0]
"In this framework, described in detail in section 6, a sentence and an accompanying interpretation encoded in first order logic, give rise to a grounded model that matches a video against the provided sentence interpretation.
",4 Approach Overview,[0],[0]
"The model is comprised of Hidden Markov Models (HMMs) which encode the semantics of words, and trackers which locate objects in video frames.",4 Approach Overview,[0],[0]
"To represent an interpretation of a sentence, word models are combined with trackers through a cross-product which respects the semantic representation of the sentence to create a single model which recognizes that interpretation.
",4 Approach Overview,[0],[0]
"Given a sentence, we construct an HMM based representation for each interpretation of that sentence.",4 Approach Overview,[0],[0]
We then detect candidate locations for objects in every frame of the video.,4 Approach Overview,[0],[0]
"Together the re-
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
JJ
green
NP
NN chair CC and NN bag
(a) First interpretation
S
NP NNP Bill
VP
VBD held
NP
DT the
NP
NP
JJ
green
NN chair
CC and NN bag
(b) Second interpretation
forestation for the sentence and the candidate object locations are combined to form a model which can determine if a given interpretation is depicted by the video.",4 Approach Overview,[0],[0]
We test each interpretation and report the interpretation with highest likelihood.,4 Approach Overview,[0],[0]
"To enable a systematic study of linguistic ambiguities that are grounded in vision, we compiled a corpus with ambiguous sentences describing visual actions.",5 Corpus,[0],[0]
"The sentences are formulated such that the correct linguistic interpretation of each sentence can only be determined using external, non-linguistic, information about the depicted activity.",5 Corpus,[0],[0]
"For example, in the sentence “Bill held the green chair and bag”, the correct scope of “green” can only be determined by integrating additional information about the color of the bag.",5 Corpus,[0],[0]
"This information is provided in the accompanying videos, which visualize the possible interpretations of each sentence.",5 Corpus,[0],[0]
Figure 2 presents the syntactic parses for this example along with frames from the respective videos.,5 Corpus,[0],[0]
"Although our videos contain visual uncertainty, they are not ambiguous with respect to the linguistic interpretation they are presenting, and hence a video always corresponds to a single candidate representation of a sentence.
",5 Corpus,[0],[0]
"The corpus covers a wide range of well
known syntactic, semantic and discourse ambiguity classes.",5 Corpus,[0],[0]
"While the ambiguities are associated with various types, different sentence interpretations always represent distinct sentence meanings, and are hence encoded semantically using first order logic.",5 Corpus,[0],[0]
"For syntactic and discourse ambiguities we also provide an additional, ambiguity type specific encoding as described below.
",5 Corpus,[0],[0]
"• Syntax Syntactic ambiguities include Prepositional Phrase (PP) attachments, Verb Phrase (VP) attachments, and ambiguities in the interpretation of conjunctions.",5 Corpus,[0],[0]
"In addition to logical forms, sentences with syntactic ambiguities are also accompanied with Context Free Grammar (CFG) parses of the candidate interpretations, generated from a deterministic CFG parser.
",5 Corpus,[0],[0]
• Semantics,5 Corpus,[0],[0]
"The corpus addresses several classes of semantic quantification ambiguities, in which a syntactically unambiguous sentence may correspond to different logical forms.",5 Corpus,[0],[0]
"For each such sentence we provide the respective logical forms.
",5 Corpus,[0],[0]
• Discourse,5 Corpus,[0],[0]
"The corpus contains two types of discourse ambiguities, Pronoun Anaphora and Ellipsis, offering examples comprising two sentences.",5 Corpus,[0],[0]
"In anaphora ambiguity cases, an ambiguous pronoun in the second sentence is given its candidate antecedents in the first sentence, as well as a corresponding logical form for the meaning of the second sentence.",5 Corpus,[0],[0]
"In ellipsis cases, a part of the second sentence, which can constitute either the subject and the verb, or the verb and the object, is omitted.",5 Corpus,[0],[0]
"We provide both interpretations of the omission in the form of a single unambiguous sentence, and its logical form, which combines the meanings of the first and the second sentences.
",5 Corpus,[0],[0]
"Table 2 lists examples of the different ambiguity classes, along with the candidate interpretations of each example.
",5 Corpus,[0],[0]
The corpus is generated using Part of Speech (POS) tag sequence templates.,5 Corpus,[0],[0]
"For each template, the POS tags are replaced with lexical items from the corpus lexicon, described in table 3, using all the visually applicable assignments.",5 Corpus,[0],[0]
"This generation process yields an overall of 237 sentences,
of which 213 sentences have 2 candidate interpretations, and 24 sentences have 3 interpretations.",5 Corpus,[0],[0]
"Table 1 presents the corpus templates for each ambiguity class, along with the number of sentences generated from each template.
",5 Corpus,[0],[0]
The corpus videos are filmed in an indoor environment containing background objects and pedestrians.,5 Corpus,[0],[0]
"To account for the manner of performing actions, videos are shot twice with different actors.",5 Corpus,[0],[0]
"Whenever applicable, we also filmed the actions from two different directions (e.g. approach from the left, and approach from the right).",5 Corpus,[0],[0]
"Finally, all videos were shot with two cameras from two different view points.",5 Corpus,[0],[0]
"Taking these variations into account, the resulting video corpus contains 7.1 videos per sentence and 3.37 videos per sentence interpretation, corresponding to a total of 1679 videos.",5 Corpus,[0],[0]
"The average video length is 3.02 seconds (90.78 frames), with in an overall of 1.4 hours of footage (152434 frames).
",5 Corpus,[0],[0]
"A custom corpus is required for this task because no existing corpus, containing either videos or images, systematically covers multimodal ambiguities.",5 Corpus,[0],[0]
"Datasets such as UCF Sports (Rodriguez et al., 2008), YouTube (Liu et al., 2009), and HMDB (Kuehne et al., 2011) which come out of the activity recognition community are accompanied by action labels, not sentences, and do not control for the content of the videos aside from the principal action being performed.",5 Corpus,[0],[0]
"Datasets for image and video captioning, such as MSCOCO (Lin et al., 2014) and TACOS (Regneri et al., 2013),
aim to control for more aspects of the videos than just the main action being performed but they do not provide the range of ambiguities discussed here.",5 Corpus,[0],[0]
"The closest dataset is that of Siddharth et al. (2014) as it controls for object appearance, color, action, and direction of motion, making it more likely to be suitable for evaluating disambiguation tasks.",5 Corpus,[0],[0]
"Unfortunately, that dataset was designed to avoid ambiguities, and therefore is not suitable for evaluating the work described here.",5 Corpus,[0],[0]
"To perform the disambiguation task, we extend the sentence recognition model of Siddharth et al. (2014) which represents sentences as compositions of words.",6 Model,[0],[0]
"Given a sentence, its first order logic interpretation and a video, our model produces a score which determines if the sentence is depicted by the video.",6 Model,[0],[0]
It simultaneously tracks the participants in the events described by the sentence while recognizing the events themselves.,6 Model,[0],[0]
"This al-
lows it to be flexible in the presence of noise by integrating top-down information from the sentence with bottom-up information from object and property detectors.",6 Model,[0],[0]
"Each word in the query sentence is represented by an HMM (Baum et al., 1970), which recognizes tracks (i.e. paths of detections in a video for a specific object) that satisfy the semantics of the given word.",6 Model,[0],[0]
"In essence, this model can be described as having two layers, one in which object tracking occurs and one in which words observe tracks and filter tracks that do not satisfy the word constraints.
",6 Model,[0],[0]
"Given a sentence interpretation, we construct a sentence-specific model which recognizes if a video depicts the sentence as follows.",6 Model,[0],[0]
"Each predicate in the first order logic formula has a corresponding HMM, which can recognize if that predicate is true of a video given its arguments.",6 Model,[0],[0]
"Each variable has a corresponding tracker which attempts to physically locate the bounding box corresponding to that variable in each frame of a
video.",6 Model,[0],[0]
This creates a bipartite graph: HMMs that represent predicates are connected to trackers that represent variables.,6 Model,[0],[0]
"The trackers themselves are similar to the HMMs, in that they comprise a lattice of potential bounding boxes in every frame.",6 Model,[0],[0]
"To construct a joint model for a sentence interpretation, we take the cross product of HMMs and trackers, taking only those cross products dictated by the structure of the formula corresponding to the desired interpretation.",6 Model,[0],[0]
"Given a video, we employ an object detector to generate candidate detections in each frame, construct trackers which select one of these detections in each frame, and finally construct the overall model from HMMs and trackers.
",6 Model,[0],[0]
"Provided an interpretation and its corresponding formula composed of P predicates and V variables, along with a collection of object detections, bframedetection index, in each frame of a video of length T the model computes the score of the videosentence pair by finding the optimal detection for each participant in every frame.",6 Model,[0],[0]
"This is in essence the Viterbi algorithm (Viterbi, 1971), the MAP algorithm for HMMs, applied to finding optimal object detections jframevariable for each participant, and the optimal state kframepredicate for each predicate HMM, in every frame.",6 Model,[0],[0]
"Each detection is scored by its confidence from the object detector, f and each object track is scored by a motion coherence metric g which determines if the motion of the track agrees with the underlying optical flow.",6 Model,[0],[0]
"Each predicate,
p, is scored by the probability of observing a particular detection in a given state hp, and by the probability of transitioning between states ap.",6 Model,[0],[0]
"The structure of the formula and the fact that multiple predicates often refer to the same variables is recorded by θ, a mapping between predicates and their arguments.",6 Model,[0],[0]
"The model computes the MAP estimate as:
max j11 ,..., j T 1
...",6 Model,[0],[0]
"j1V ,..., j T V
max k11,..., k T 1
... k1P ,..., k T P
V∑ v=1 T∑ t=1",6 Model,[0],[0]
f(btjtv ),6 Model,[0],[0]
+ T∑ t=2 g(bt−1,6 Model,[0],[0]
"jt−1v , btjtv )+
P∑ p=1 T∑ t=1 hp(k t p, b t jt θ1p , btjt θ2p ) + T∑ t=2 ap(k t−1 p , k t p)
for sentences which have words that refer to at most two tracks (i.e. transitive verbs or binary predicates) but is trivially extended to arbitrary arities.",6 Model,[0],[0]
"Figure 3 provides a visual overview of the model as a cross-product of tracker models and word models.
",6 Model,[0],[0]
Our model extends the approach of Siddharth et al. (2014) in several ways.,6 Model,[0],[0]
"First, we depart from the dependency based representation used in that work, and recast the model to encode first order logic formulas.",6 Model,[0],[0]
Note that some complex first order logic formulas cannot be directly encoded in the model and require additional inference steps.,6 Model,[0],[0]
"This extension enables us to represent ambiguities in which a given sentence has multiple logical interpretations for the same syntactic parse.
",6 Model,[0],[0]
"Second, we introduce several model components which are not specific to disambiguation, but are required to encode linguistic constructions that are present in our corpus and could not be handled by the model of Siddharth et al. (2014).",6 Model,[0],[0]
"These new components are the predicate “not equal”, disjunction, and conjunction.",6 Model,[0],[0]
"The key addition among these components is support for the new predicate “not equal”, which enforces that two tracks, i.e. objects, are distinct from each other.",6 Model,[0],[0]
"For example, in the sentence “Claire and Bill moved a chair” one would want to ensure that the two movers are distinct entities.",6 Model,[0],[0]
"In earlier work, this was not required because the sentences tested in that work were designed to distinguish objects based on constraints rather than identity.",6 Model,[0],[0]
"In other words, there might have been two different people but they were distinguished in the sentence by their actions or appearance.",6 Model,[0],[0]
"To faithfully recognize that two actors are moving the chair in the earlier example, we must ensure that they are disjoint from each other.",6 Model,[0],[0]
"In order to do this we create a new HMM for this predicate, which assigns low probability to tracks that heavily overlap, forcing the model to fit two different actors in the previous example.",6 Model,[0],[0]
"By combining the new first order logic based semantic representation in lieu of a syntactic representation with a more expressive model, we can encode the sentence interpretations required to perform the disambiguation task.
",6 Model,[0],[0]
Figure 3(left) shows an example of two different interpretations of the above discussed sentence “Claire and Bill moved a chair”.,6 Model,[0],[0]
"Object trackers, which correspond to variables in the first order logic representation of the sentence interpretation, are shown in red.",6 Model,[0],[0]
"Predicates which constrain the possible bindings of the trackers, corresponding to predicates in the representation of the sentence, are shown in blue.",6 Model,[0],[0]
"Links represent the argument structure of the first order logic formula, and determine the cross products that are taken between the predicate HMMs and tracker lattices in order to form the joint model which recognizes the entire interpretation in a video.
",6 Model,[0],[0]
The resulting model provides a single unified formalism for representing all the ambiguities in table 2.,6 Model,[0],[0]
"Moreover, this approach can be tuned to different levels of specificity.",6 Model,[0],[0]
"We can create models that are specific to one interpretation of a sentence or that are generic, and accept multiple interpretations by eliding constraints that are not com-
mon between the different interpretations.",6 Model,[0],[0]
"This allows the model, like humans, to defer deciding on a particular interpretation or to infer that multiple interpretation of the sentence are plausible.",6 Model,[0],[0]
We tested the performance of the model described in the previous section on the LAVA dataset presented in section 5.,7 Experimental Results,[0],[0]
"Each video in the dataset was pre-processed with object detectors for humans, bags, chairs, and telescopes.",7 Experimental Results,[0],[0]
"We employed a mixture of CNN (Krizhevsky et al., 2012) and DPM (Felzenszwalb et al., 2010) detectors, trained on held out sections of our corpus.",7 Experimental Results,[0],[0]
"For each object class we generated proposals from both the CNN and the DPM detectors, and trained a scoring function to map both results into the same space.",7 Experimental Results,[0],[0]
The scoring function consisted of a sigmoid over the confidence of the detectors trained on the same held out portion of the training set.,7 Experimental Results,[0],[0]
"As none of the disambiguation examples discussed here rely on the specific identity of the actors, we did not detect their identity.",7 Experimental Results,[0],[0]
"Instead, any sentence which contains names was automatically converted to one which contains arbitrary “person” labels.
",7 Experimental Results,[0],[0]
The sentences in our corpus have either two or three interpretations.,7 Experimental Results,[0],[0]
"Each interpretation has one or more associated videos where the scene was shot from a different angle, carried out either by different actors, with different objects, or in different directions of motion.",7 Experimental Results,[0],[0]
"For each sentence-video pair, we performed a 1-out-of-2 or 1-out-of-3 classification task to determine which of the interpretations of the corresponding sentence best fits that video.",7 Experimental Results,[0],[0]
"Overall chance performance on our dataset is 49.04%, slightly lower than 50% due to the 1- out-of-3 classification examples.
",7 Experimental Results,[0],[0]
The model presented here achieved an accuracy of 75.36% over the entire corpus averaged across all error categories.,7 Experimental Results,[0],[0]
This demonstrates that the model is largely capable of capturing the underlying task and that similar compositional crossmodal models may do the same.,7 Experimental Results,[0],[0]
"For each of the 3 major ambiguity classes we had an accuracy of 84.26% for syntactic ambiguities, 72.28% for semantic ambiguities, and 64.44% for discourse ambiguities.
",7 Experimental Results,[0],[0]
The most significant source of model failures are poor object detections.,7 Experimental Results,[0],[0]
Objects are often rotated and presented at angles that are difficult to recognize.,7 Experimental Results,[0],[0]
"Certain object classes like the telescope
are much more difficult to recognize due to their small size and the fact that hands tend to largely occlude them.",7 Experimental Results,[0],[0]
"This accounts for the degraded performance of the semantic ambiguities relative to the syntactic ambiguities, as many more semantic ambiguities involved the telescope.",7 Experimental Results,[0],[0]
Object detector performance is similarly responsible for the lower performance of the discourse ambiguities which relied much more on the accuracy of the person detector as many sentences involve only people interacting with each other without any additional objects.,7 Experimental Results,[0],[0]
"This degrades performance by removing a helpful constraint for inference, according to which people tend to be close to the objects they are manipulating.",7 Experimental Results,[0],[0]
"In addition, these sentences introduced more visual uncertainty as they often involved three actors.
",7 Experimental Results,[0],[0]
The remaining errors are due to the event models.,7 Experimental Results,[0],[0]
"HMMs can fixate on short sequences of events which seem as if they are part of an action, but in fact are just noise or the prefix of another action.",7 Experimental Results,[0],[0]
"Ideally, one would want an event model which has a global view of the action, if an object went up from the beginning to the end of the video while a person was holding it, it’s likely that the object was being picked up.",7 Experimental Results,[0],[0]
"The event models used here cannot enforce this constraint, they merely assert that the object was moving up for some number of frames; an event which can happen due to noise in the object detectors.",7 Experimental Results,[0],[0]
Enforcing such local constraints instead of the global constraint of the motion of the object over the video makes joint tracking and event recognition tractable in the framework presented here but can lead to errors.,7 Experimental Results,[0],[0]
Finding models which strike a better balance between local information and global constraints while maintaining tractable inference remains an area of future work.,7 Experimental Results,[0],[0]
We present a novel framework for studying ambiguous utterances expressed in a visual context.,8 Conclusion,[0],[0]
"In particular, we formulate a new task for resolving structural ambiguities using visual signal.",8 Conclusion,[0],[0]
"This is a fundamental task for humans, involving complex cognitive processing, and is a key challenge for language acquisition during childhood.",8 Conclusion,[0],[0]
"We release a multimodal corpus that enables to address this task, as well as support further investigation of ambiguity related phenomena in visually grounded language processing.",8 Conclusion,[0],[0]
"Finally, we
present a unified approach for resolving ambiguous descriptions of videos, achieving good performance on our corpus.
While our current investigation focuses on structural inference, we intend to extend this line of work to learning scenarios, in which the agent has to deduce the meaning of words and sentences from structurally ambiguous input.",8 Conclusion,[0],[0]
"Furthermore, our framework can be beneficial for image and video retrieval applications in which the query is expressed in natural language.",8 Conclusion,[0],[0]
"Given an ambiguous query, our approach will enable matching and clustering the retrieved results according to the different query interpretations.",8 Conclusion,[0],[0]
"This material is based upon work supported by the Center for Brains, Minds, and Machines (CBMM), funded by NSF STC award CCF1231216.",Acknowledgments,[0],[0]
SU was also supported by ERC Advanced Grant 269627 Digital Baby.,Acknowledgments,[0],[0]
Understanding language goes hand in hand with the ability to integrate complex contextual information obtained via perception.,abstractText,[0],[0]
"In this work, we present a novel task for grounded language understanding: disambiguating a sentence given a visual scene which depicts one of the possible interpretations of that sentence.",abstractText,[0],[0]
"To this end, we introduce a new multimodal corpus containing ambiguous sentences, representing a wide range of syntactic, semantic and discourse ambiguities, coupled with videos that visualize the different interpretations for each sentence.",abstractText,[0],[0]
We address this task by extending a vision model which determines if a sentence is depicted by a video.,abstractText,[0],[0]
"We demonstrate how such a model can be adjusted to recognize different interpretations of the same underlying sentence, allowing to disambiguate sentences in a unified fashion across the different ambiguity types.",abstractText,[0],[0]
Do You See What I Mean? Visual Resolution of Linguistic Ambiguities,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1275–1284 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1275
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks. We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document. The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies. We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent. Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",text,[0],[0]
"Neural machine translation (NMT) has proven to be powerful (Sutskever et al., 2014; Bahdanau et al., 2015).",1 Introduction,[0],[0]
"It is on-par, and in some cases, even surpasses the traditional statistical MT (Luong et al., 2015) while enjoying more flexibility and significantly less manual effort for feature engineering.",1 Introduction,[0],[0]
"Despite their flexibility, most neural MT models translate sentences independently.",1 Introduction,[0],[0]
"Discourse phenomenon such as pronominal anaphora and lexical consistency, may depend on long-range dependency going farther than a
few previous sentences, are neglected in sentencebased translation (Bawden et al., 2017).
",1 Introduction,[0],[0]
There are only a handful of attempts to document-wide machine translation in statistical and neural MT camps.,1 Introduction,[0],[0]
Hardmeier and Federico (2010); Gong et al. (2011); Garcia et al. (2014) propose document translation models based on statistical MT but are restrictive in the way they incorporate the document-level information and fail to gain significant improvements.,1 Introduction,[0],[0]
"More recently, there have been a few attempts to incorporate source side context into neural MT (Jean et al., 2017; Wang et al., 2017; Bawden et al., 2017); however, these works only consider a very local context including a few previous source/target sentences, ignoring the global source and target documental contexts.",1 Introduction,[0],[0]
"The latter two report deteriorated performance when using the target-side context.
",1 Introduction,[0],[0]
"In this paper, we present a document-level machine translation model which combines sentencebased NMT (Bahdanau et al., 2015) with memory networks (Sukhbaatar et al., 2015).",1 Introduction,[0],[0]
"We capture the global source and target document context with two memory components, one each for the source and target side, and incorporate it into the sentence-based NMT by changing the decoder to condition on it as the sentence translation is generated.",1 Introduction,[0],[0]
"We conduct experiments on three language pairs: French-English, German-English and Estonian-English.",1 Introduction,[0],[0]
"The experimental results and analysis demonstrate that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",1 Introduction,[0],[0]
"Our document NMT model is grounded on sentence-based NMT model (Bahdanau et al.,
2015) which contains an encoder to read the source sentence as well as an attentional decoder to generate the target translation.
",2.1 Neural Machine Translation (NMT),[0],[0]
Encoder It is a bidirectional RNN consisting of two RNNs running in opposite directions over the source sentence:,2.1 Neural Machine Translation (NMT),[0],[0]
"−→ hi = −−→ RNN( −→ h i−1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi]), ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i = ←−− RNN( ←− h i+1,ES",2.1 Neural Machine Translation (NMT),[0],[0]
"[xi])
",2.1 Neural Machine Translation (NMT),[0],[0]
"where ES [xi] is embedding of the word xi from the embedding table ES of the source language, and −→ h i and ←−",2.1 Neural Machine Translation (NMT),[0],[0]
h,2.1 Neural Machine Translation (NMT),[0],[0]
"i are the hidden states of the forward and backward RNNs which can be based on the LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) units.",2.1 Neural Machine Translation (NMT),[0],[0]
"Each word in the source sentence is then represented by the concatenation of the corresponding bidirectional hidden states, hi =",2.1 Neural Machine Translation (NMT),[0],[0]
[ −→ h i; ←− h,2.1 Neural Machine Translation (NMT),[0],[0]
"i].
",2.1 Neural Machine Translation (NMT),[0],[0]
"Decoder The generation of each word yj is conditioned on all of the previously generated words y<j via the state of the RNN decoder sj , and the source sentence via a dynamic context vector cj :
yj ∼ softmax(Wy · rj + br) rj = tanh(sj +Wrc · cj",2.1 Neural Machine Translation (NMT),[0],[0]
+,2.1 Neural Machine Translation (NMT),[0],[0]
"Wrj ·ET [yj−1]) sj = tanh(Ws · sj−1 +Wsj ·ET [yj−1] +Wsc · cj)
where ET",2.1 Neural Machine Translation (NMT),[0],[0]
"[yj ] is embedding of the word yj from the embedding table ET of the target language, and W matrices and br vector are the parameters.",2.1 Neural Machine Translation (NMT),[0],[0]
"The dynamic context vector cj is computed via cj = ∑ i αjihi, where
αj = softmax(aj) aji = v · tanh(Wae ·",2.1 Neural Machine Translation (NMT),[0],[0]
"hi +Wat · sj−1)
",2.1 Neural Machine Translation (NMT),[0],[0]
This is known as the attention mechanism which dynamically attends to relevant parts of the source necessary for generating the next target word.,2.1 Neural Machine Translation (NMT),[0],[0]
"Memory Networks (Weston et al., 2015) are a class of neural models that use external memories to perform inference based on long-range dependencies.",2.2 Memory Networks (MemNets),[0],[0]
"A memory is a collection of vectors M = {m1, ..,mK} constituting the memory cells, where each cell mk may potentially correspond to a discrete object xk.",2.2 Memory Networks (MemNets),[0],[0]
The memory is equipped with a read and optionally a write operation.,2.2 Memory Networks (MemNets),[0],[0]
"Given a query vector q, the output vector generated by reading from the memory is ∑|M | i=1",2.2 Memory Networks (MemNets),[0],[0]
"pimi, where pi represents the relevance of the query to the i-th memory cell p =
softmax(qT ·M).",2.2 Memory Networks (MemNets),[0],[0]
"For the rest of the paper, we denote the read operation by MemNet(M , q).",2.2 Memory Networks (MemNets),[0],[0]
We formulate document-wide machine translation as a structured prediction problem.,3 Document NMT as Structured Prediction,[0],[0]
"Given a set of sentences {x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|} in a source document d, we are interested in generating the collection of their translations {y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d|} taking into account interdependencies among them imposed by the document.",3 Document NMT as Structured Prediction,[0],[0]
We achieve this by the factor graph in Figure 1 to model the probability of the target document given the source document.,3 Document NMT as Structured Prediction,[0],[0]
"Our model has two types of factors:
• fθ(yt;xt,x−t) to capture the interdependencies between the translation yt, the corresponding source sentence xt and all the other sentences in the source document x−t, and
• gθ(yt;y−t) to capture the interdependencies between the translation yt and all the other translations in the document y−t.
",3 Document NMT as Structured Prediction,[0],[0]
"Hence, the probability of a document translation given the source document is
P (y1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",y|d||x1, . . .",3 Document NMT as Structured Prediction,[0],[0]
",x|d|) ∝",3 Document NMT as Structured Prediction,[0],[0]
"exp (∑
t
fθ(yt;xt,x−t) + gθ(yt;y−t) ) .
",3 Document NMT as Structured Prediction,[0],[0]
"The factors fθ and gθ are realised by neural architectures whose parameters are collectively denoted by θ.
Training It is challenging to train the model parameters by maximising the (regularised) likelihood since computing the partition function is hard.",3 Document NMT as Structured Prediction,[0],[0]
"This is due to the enormity of factors
gθ(yt;y−t) over a large number of translation variables yt’s (i.e., the number of sentences in the document) as well as their unbounded domain (i.e., all sentences in the target language).",3 Document NMT as Structured Prediction,[0],[0]
"Thus, we resort to maximising the pseudo-likelihood (Besag, 1975) for training the parameters:
argmax θ ∏ d∈D |d|∏ t=1 Pθ(yt|xt,y−t,x−t) (1)
whereD is the set of bilingual training documents, and |d| denotes the number of (bilingual) sentences in the document d = {(xt,yt)}|d|t=1.",3 Document NMT as Structured Prediction,[0],[0]
"We directly model the document-conditioned NMT model Pθ(yt|xt,y−t,x−t) using a neural architecture which subsumes both the fθ and gθ factors (covered in the next section).
",3 Document NMT as Structured Prediction,[0],[0]
"Decoding To generate the best translation for a document according to our model, we need to solve the following optimisation problem:
arg max y1,...,y|d| |d|∏ t=1 Pθ(yt|xt,y−t,x−t)
which is hard (due to similar reasons as mentioned earlier).",3 Document NMT as Structured Prediction,[0],[0]
We hence resort to a block coordinate descent optimisation algorithm.,3 Document NMT as Structured Prediction,[0],[0]
"More specifically, we initialise the translation of each sentence using the base neural MT model P (yt|xt).",3 Document NMT as Structured Prediction,[0],[0]
"We then repeatedly visit each sentence in the document, and update its translation using our document-context dependent NMT model P (yt|xt,y−t,x−t) while the translations of other sentences are kept fixed.",3 Document NMT as Structured Prediction,[0],[0]
"We augment the sentence-level attentional NMT model by incorporating the document context (both source and target) using memory networks when generating the translation of a sentence, as shown in Figure 2.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model generates the target translation word-by-word from left to right, similar to the vanilla attentional neural translation model.",4 Context Dependent NMT with MemNets,[0],[0]
"However, it conditions the generation of a target word not only on the previously generated words and the current source sentence (as in the vanilla NMT model), but also on all the other source sentences of the document and their translations.",4 Context Dependent NMT with MemNets,[0],[0]
"That is, the
generation process is as follows:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt,y−t,x−t)
(2)
where yt,j is the j-th word of the t-th target sentence, yt,<j are the previously generated words, and x−t and y−t are as introduced previously.
",4 Context Dependent NMT with MemNets,[0],[0]
"Our model represents the source and target document contexts as external memories, and attends to relevant parts of these external memories when generating the translation of a sentence.",4 Context Dependent NMT with MemNets,[0],[0]
"Let M [x−t] and M [y−t] denote external memories representing the source and target document context, respectively.",4 Context Dependent NMT with MemNets,[0],[0]
These contain memory cells corresponding to all sentences in the document except the t-th sentence (described shortly).,4 Context Dependent NMT with MemNets,[0],[0]
"Let ht and st be representations of the t-th source sentence and its current translation, from the encoder and decoder respectively.",4 Context Dependent NMT with MemNets,[0],[0]
"We make use of ht as the query to get the relevant context from the source external memory:
csrct = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[x−t],ht)
Furthermore, for the t-th sentence, we get the relevant information from the target context:
ctrgt = MemNet(M",4 Context Dependent NMT with MemNets,[0],[0]
"[y−t], st +Wat · ht)
where the query consists of the representation of the translation st from the decoder endowed with that of the source sentence ht from the encoder to make the query robust to potential noises in the current translation and circumvent error propagation, and Wat projects the source representation into the hidden state space.
",4 Context Dependent NMT with MemNets,[0],[0]
"Now that we have representations of the relevant source and target document contexts, Eq. 2 can be re-written as:
Pθ(yt|xt,y−t,x−t)",4 Context Dependent NMT with MemNets,[0],[0]
"= |yt|∏ j=1 Pθ(yt,j |yt,<j ,xt, ctrgt , c src t )
(3)
More specifically, the memory contexts csrct and ctrgt are incorporated into the NMT decoder as:
• Memory-to-Context in which the memory contexts are incorporated when computing the next decoder hidden state:
st,j = tanh(Ws · st,j−1 +Wsj ·ET [yt,j ] + Wsc · ct,j +Wsm · csrct +Wst · c trg t )
",4 Context Dependent NMT with MemNets,[0],[0]
"• Memory-to-Output in which the memory contexts are incorporated in the output layer:
yt,j ∼ softmax(Wy · rt,j +Wym · csrct + Wyt · ctrgt + br)
",4 Context Dependent NMT with MemNets,[0],[0]
"where Wsm, Wst, Wym, and Wyt are the new parameter matrices.",4 Context Dependent NMT with MemNets,[0],[0]
"We use only the source, only the target, or both external memories as the additional conditioning contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"Furthermore, we use either the Memory-to-Context or Memory-toOutput architectures for incorporating the document contexts.",4 Context Dependent NMT with MemNets,[0],[0]
"In the experiments, we will explore these different options to investigate the most effective combination.",4 Context Dependent NMT with MemNets,[0],[0]
"We now turn our attention to the construction of the external memories for the source and target sides of a document.
",4 Context Dependent NMT with MemNets,[0],[0]
The Source Memory We make use of a hierarchical 2-level RNN architecture to construct the external memory of the source document.,4 Context Dependent NMT with MemNets,[0],[0]
"More specifically, we pass each sentence of the document through a sentence-level bidirectional RNN to get the representation of the sentence (by concatenating the last hidden states of the forward and backward RNNs).",4 Context Dependent NMT with MemNets,[0],[0]
We then pass the sentence representations through a document-level bidirectional RNN to propagate sentences’ information across the document.,4 Context Dependent NMT with MemNets,[0],[0]
"We take the hidden states
of the document-level bidirectional RNNs as the memory cells of the source external memory.
",4 Context Dependent NMT with MemNets,[0],[0]
"The source external memory is built once for each minibatch, and does not change throughout the document translation.",4 Context Dependent NMT with MemNets,[0],[0]
"To be able to fit the computational graph of the document NMT model within GPU memory limits, we pre-train the sentence-level bidirectional RNN using the language modelling training objective.",4 Context Dependent NMT with MemNets,[0],[0]
"However, the document-level bidirectional RNN is trained together with other parameters of the document NMT model by back-propagating the document translation training objective.
",4 Context Dependent NMT with MemNets,[0],[0]
The Target Memory,4 Context Dependent NMT with MemNets,[0],[0]
The memory cells of the target external memory represent the current translations of the document.,4 Context Dependent NMT with MemNets,[0],[0]
Recall from the previous section that we use coordinate descent iteratively to update these translations.,4 Context Dependent NMT with MemNets,[0],[0]
"Let {y1, . . .",4 Context Dependent NMT with MemNets,[0],[0]
",y|d|} be the current translations, and let {s|y1|, . . .",4 Context Dependent NMT with MemNets,[0],[0]
", s|y|d||} be the last states of the decoder when these translations were generated.",4 Context Dependent NMT with MemNets,[0],[0]
We use these last decoder states as the cells of the external target memory.,4 Context Dependent NMT with MemNets,[0],[0]
"We could make use of hierarchical sentencedocument RNNs to transform the document translations into memory cells (similar to what we do for the source memory); however, it would have been computationally expensive and may have resulted in error propagation.",4 Context Dependent NMT with MemNets,[0],[0]
We will show in the experiments that our efficient target memory construction is indeed effective.,4 Context Dependent NMT with MemNets,[0],[0]
Datasets.,5 Experiments and Analysis,[0],[0]
"We conducted experiments on three language pairs: French-English, German-English and Estonian-English.",5 Experiments and Analysis,[0],[0]
Table 1 shows the statistics of the datasets used in our experiments.,5 Experiments and Analysis,[0],[0]
"The French-English dataset is based on the TED Talks corpus1 (Cettolo et al., 2012) where each talk is considered a document.",5 Experiments and Analysis,[0],[0]
"The EstonianEnglish data comes from the Europarl v7 corpus2 (Koehn, 2005).",5 Experiments and Analysis,[0],[0]
"Following Smith et al. (2013), we split the speeches based on the SPEAKER tag and treat them as documents.",5 Experiments and Analysis,[0],[0]
The FrenchEnglish and Estonian-English corpora were randomly split into train/dev/test sets.,5 Experiments and Analysis,[0],[0]
"For GermanEnglish, we use the News Commentary v9 corpus3 for training, news-dev2009 for development, 1https://wit3.fbk.eu/ 2http://www.statmt.org/europarl/ 3http://statmt.org/wmt14/news-commentary-v9-bydocument.tgz
and news-test2011 and news-test2016 as the test sets.",5 Experiments and Analysis,[0],[0]
"The news-commentary corpus has document boundaries already provided.
",5 Experiments and Analysis,[0],[0]
We pre-processed all corpora to remove very short documents and those with missing translations.,5 Experiments and Analysis,[0],[0]
"Out-of-vocabulary and rare words (frequency less than 5) are replaced by the <UNK> token, following Cohn et al. (2016).4
Evaluation Measures We use BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007) scores to measure the quality of the generated translations.",5 Experiments and Analysis,[0],[0]
"We use bootstrap resampling (Clark et al., 2011) to measure statistical significance, p < 0.05, comparing to the baselines.
",5 Experiments and Analysis,[0],[0]
"Implementation and Hyperparameters We implement our document-level neural machine translation model in C++ using the DyNet library (Neubig et al., 2017), on top of the basic sentence-level NMT implementation in mantis (Cohn et al., 2016).",5 Experiments and Analysis,[0],[0]
"For the source memory, the sentence and document-level bidirectional RNNs use LSTM and GRU units, respectively.",5 Experiments and Analysis,[0],[0]
The translation model uses GRU units for the bidirectional RNN encoder and the 2-layer RNN decoder.,5 Experiments and Analysis,[0],[0]
GRUs are used instead of LSTMs to reduce the number of parameters in the main model.,5 Experiments and Analysis,[0],[0]
"The RNN hidden dimensions and word embedding sizes are set to 512 in the translation and memory components, and the alignment dimension is set to 256 in the translation model.
",5 Experiments and Analysis,[0],[0]
Training We use a stage-wise method to train the variants of our document context NMT model.,5 Experiments and Analysis,[0],[0]
"Firstly, we pre-train the Memory-toContext/Memory-to-Output models, setting their readings from the source and target memories to
4We do not split words into subwords using BPE (Sennrich et al., 2016) as that increases sentence lengths resulting in removing long documents due to GPU memory limitations, which would heavily reduce the amount of data that we have.
",5 Experiments and Analysis,[0],[0]
the zero vector.,5 Experiments and Analysis,[0],[0]
"This effectively learns parameters associated with the underlying sentence-based NMT model, which is then used as initialisation when training all parameters in the second stage (including the ones from the first stage).",5 Experiments and Analysis,[0],[0]
"For the first stage, we make use of stochastic gradient descent (SGD)5 with initial learning rate of 0.1 and a decay factor of 0.5 after the fourth epoch for a total of ten epochs.",5 Experiments and Analysis,[0],[0]
The convergence occurs in 6-8 epochs.,5 Experiments and Analysis,[0],[0]
"For the second stage, we use SGD with an initial learning rate of 0.08 and a decay factor of 0.9 after the first epoch for a total of 15 epochs6.",5 Experiments and Analysis,[0],[0]
The best model is picked based on the dev-set perplexity.,5 Experiments and Analysis,[0],[0]
"To avoid overfitting, we employ dropout with the rate 0.2 for the single memory model.",5 Experiments and Analysis,[0],[0]
"For the dual memory model, we set dropout for Document RNN to 0.2 and for the encoder and decoder to 0.5.",5 Experiments and Analysis,[0],[0]
Mini-batching is used in both stages to speed up training.,5 Experiments and Analysis,[0],[0]
"For the largest dataset, the document NMT model takes about 4.5 hours per epoch to train on a single P100 GPU, while the sentence-level model takes about 3 hours per epoch for the same settings.
",5 Experiments and Analysis,[0],[0]
"When training the document NMT model in the second stage, we need the target memory.",5 Experiments and Analysis,[0],[0]
One option would be to use the ground truth translations for building the memory.,5 Experiments and Analysis,[0],[0]
"However, this may result in inferior training, since at the test time, the decoder iteratively updates the translation of sentences based on the noisy translations of other sentences (accessed via the target memory).",5 Experiments and Analysis,[0],[0]
"Hence, while training the document NMT model, we construct the target memory from the translations generated by the pre-trained sentence-level model7.",5 Experiments and Analysis,[0],[0]
"This effectively exposes the model to its potential test-time mistakes during the training time, resulting in more robust learned parameters.",5 Experiments and Analysis,[0],[0]
"We have three variants of our model, using: (i) only the source memory (S-NMT+src mem), (ii) only the target memory (S-NMT+trg mem), or
5In our initial experiments, we found SGD to be more effective than Adam/Adagrad; an observation also made by Bahar et al. (2017).
",5.1 Main Results,[0],[0]
"6For the document NMT model training, we did some preliminary experiments using different learning rates and used the scheme which converged to the best perplexity in the least number of epochs while for sentence-level training we follow Cohn et al. (2016).
",5.1 Main Results,[0],[0]
"7We report results for two-pass decoding, i.e., we only update the translations once using the initial translations generated from the base model.",5.1 Main Results,[0],[0]
"We tried multiple passes of decoding at test-time but it was not helpful.
",5.1 Main Results,[0],[0]
(iii) both the source and target memories (SNMT+both mems).,5.1 Main Results,[0],[0]
We compare these variants against the standard sentence-level NMT model (S-NMT).,5.1 Main Results,[0],[0]
"We also compare the source memory variants of our model to the local context-NMT models8 of Jean et al. (2017) and Wang et al. (2017), which use a few previous source sentences as context, added to the decoder hidden state (similar to our Memory-to-Context model).
",5.1 Main Results,[0],[0]
Memory-to-Context We consistently observe +1.15/+1.13 BLEU/METEOR score improvements across the three language pairs upon comparing our best model to S-NMT (see Table 2).,5.1 Main Results,[0],[0]
"Overall, our document NMT model with both memories has been the most effective variant for all of the three language pairs.
",5.1 Main Results,[0],[0]
We further experiment to train the target memory variants using gold translations instead of the generated ones for German-English.,5.1 Main Results,[0],[0]
"This led to −0.16 and −0.25 decrease9 in the BLEU scores for the target-only and both-memory variants, which confirms the intuition of constructing the target memory by exposing the model to its noises during training time.
",5.1 Main Results,[0],[0]
"Memory-to-Output From Table 2, we consistently see +.95/+1.00",5.1 Main Results,[0],[0]
"BLEU/METEOR improvements between the best variants of our model and the sentence-level baseline across the three lan-
8We implemented and trained the baseline local context models using the same hyperparameters and training procedure that we used for training our memory models.
",5.1 Main Results,[0],[0]
9Latter is statistically significant decrease w.r.t.,5.1 Main Results,[0],[0]
"the both memory model trained on generated target translations.
guage pairs.",5.1 Main Results,[0],[0]
"For French→English, all variants of document NMT model show comparable performance when using BLEU; however, when evaluated using METEOR, the dual memory model is the best.",5.1 Main Results,[0],[0]
"For German→English, the target memory variants give comparable results, whereas for Estonian→English, the dual memory variant proves to be the best.",5.1 Main Results,[0],[0]
"Overall, the Memory-toContext model variants perform better than their Memory-to-Output counterparts.",5.1 Main Results,[0],[0]
"We attribute this to the large number of parameters in the latter architecture (Table 3) and limited amount of data.
",5.1 Main Results,[0],[0]
"We further experiment with more data for train-
BLEU-1 Fr→En De→En Et→En
NC-11NC-16
Jean et al. (2017) 52.8 30.6 39.2 51.9 Wang et al. (2017) 52.6 28.2 38.3 52.3 S-NMT 51.4 28.7 36.9 50.4
+src mem 53.0 30.5 39.1 52.6 +both mems 53.5 33.1 41.3 53.2
Table 5: Unigram BLEU for our Memory-to-Context Document NMT models vs. S-NMT and Source context NMT baselines.",5.1 Main Results,[0],[0]
bold:,5.1 Main Results,[0],[0]
"Best performance.
",5.1 Main Results,[0],[0]
ing the sentence-based NMT to investigate the extent to which document context is useful in this setting.,5.1 Main Results,[0],[0]
We randomly choose an additional 300K German-English sentence pairs from WMT’14 data to train the base NMT model in stage 1.,5.1 Main Results,[0],[0]
"In stage 2, we use the same document corpus as before to train the document-level models.",5.1 Main Results,[0],[0]
"As seen from Figure 3, the document MT variants still benefit from the document context even when the base model is trained on a larger bilingual corpus.",5.1 Main Results,[0],[0]
"For the Memory-to-Context model, we see massive improvements of +0.72 and +1.44 METEOR scores for the source memory and dual memory model respectively, when compared to the baseline.",5.1 Main Results,[0],[0]
"On the other hand, for the Memory-to-Output model, the target memory model’s METEOR score increases significantly by +1.09 compared to the baseline, slightly differing from the corresponding model using the smaller corpus (+1.2).
",5.1 Main Results,[0],[0]
"Local Source Context Models Table 4 shows comparison of our Memory-to-Context model variants to local source context-NMT models (Jean et al., 2017; Wang et al., 2017).",5.1 Main Results,[0],[0]
"For French→English, our source memory model is comparable to both baselines.",5.1 Main Results,[0],[0]
"For German→English, our S-NMT+src mem model is comparable to Jean et al. (2017) but outperforms Wang et al. (2017) for one test set according to BLEU, and for both test sets according to METEOR.",5.1 Main Results,[0],[0]
"For Estonian→English, our model outperforms Jean et al. (2017).",5.1 Main Results,[0],[0]
"Our global source context model has only surface-level sentence information, and is oblivious to the individual words in the context since we do an offline training to get the sentence representations (as previously mentioned).",5.1 Main Results,[0],[0]
"However, the other two context baselines have access to that information, yet our
model’s performance is either better or quite close to those models.",5.1 Main Results,[0],[0]
We also look into the unigram BLEU scores to see how much our global source memory variants lead to improvement at the word-level.,5.1 Main Results,[0],[0]
"From Table 5, it can be seen that our model’s performance is better than the baselines for majority of the cases.",5.1 Main Results,[0],[0]
"The S-NMT+both mems model gives the best results for all three language pairs, showing that leveraging both source and target document context is indeed beneficial for improving MT performance.",5.1 Main Results,[0],[0]
Using Global/Local Target Context We first investigate whether using a local target context would have been equally sufficient in comparison to our global target memory model for the three datasets.,5.2 Analysis,[0],[0]
We condition the decoder on the previous target sentence representation (obtained from the last hidden state of the decoder) by adding it as an additional input to all decoder states (PrevTrg) similar to our Memory-to-Context model.,5.2 Analysis,[0],[0]
"From Table 6, we observe that for French→English and Estonian→English, using all sentences in the target context or just the previous target sentence gives comparable results.",5.2 Analysis,[0],[0]
"We may attribute this to these specific datasets, that is documents from TED talks or European Parliament Proceedings may depend more on the local than on the global context.",5.2 Analysis,[0],[0]
"However, for German→English (NC-11), the target memory model performs the best show-
ing that for documents with richer context (e.g. news articles) we do need the global target document context to improve MT performance.
",5.2 Analysis,[0],[0]
"Output Analysis To better understand the dual memory model, we look at the first sentence example in Table 7.",5.2 Analysis,[0],[0]
It can be seen that the source sentence has the noun “Qimonda” but the sentencelevel NMT model fails to attend to it when generating the translation.,5.2 Analysis,[0],[0]
"On the other hand, the single memory models are better in delivering some, if not all, of the underlying information in the source sentence but the dual memory model’s translation quality surpasses them.",5.2 Analysis,[0],[0]
"This is because the word “Qimonda” was being repeated in this specific document, providing a strong contextual signal to our global document context model while the local context model by Wang et al. (2017) is still unable to correctly translate the noun even when it has access to the word-level information of previous sentences.
",5.2 Analysis,[0],[0]
We resort to manual evaluation as there is no standard metric which evaluates document-level discourse information like consistency or pronominal anaphora.,5.2 Analysis,[0],[0]
"By manual inspection, we observe that our models can identify nouns in the source sentence to resolve coreferent pronouns, as shown in the second example of Table 7.",5.2 Analysis,[0],[0]
"Here the topic of the sentence is “the country under the dictatorship of Lukashenko” and our target and dual memory models are able to generate the appropriate pronoun/determiner as well as accurately translate the word ‘diktatuur’, hence producing much better translation as compared to both baselines.",5.2 Analysis,[0],[0]
"Apart from these improvements, our models are better in improving the readability of sentences by generating more context appropriate grammatical structures such as verbs and adverbs.
",5.2 Analysis,[0],[0]
"Furthermore, to validate that our model improves the consistency of translations, we look at five documents (roughly 70 sentences) from the test set of Estonian-English, each of which had a word being repeated in the gold translation.",5.2 Analysis,[0],[0]
Our model is able to resolve the consistency in 22 out of 32 cases as compared to the sentencebased model which only accurately translates 16 of those.,5.2 Analysis,[0],[0]
"Following Wang et al. (2017), we also investigate the extent to which our model can correct errors made by the baseline system.",5.2 Analysis,[0],[0]
We randomly choose five documents from the test set.,5.2 Analysis,[0],[0]
"Out of the 20 words/phrases which were incorrectly translated by the sentence-based model, our
model corrects 85% of them while also generating 10% new errors.",5.2 Analysis,[0],[0]
"Document-level Statistical MT There have been a few SMT-based attempts to document MT, but they are either restrictive or do not lead to significant improvements.",6 Related Work,[0],[0]
Hardmeier and Federico (2010) identify links among words in the source document using a word-dependency model to improve translation of anaphoric pronouns.,6 Related Work,[0],[0]
Gong et al. (2011) make use of a cache-based system to save relevant information from the previously generated translations and use that to enhance document-level translation.,6 Related Work,[0],[0]
"Garcia et al. (2014) propose a two-pass approach to improve the translations already obtained by a sentencelevel model.
",6 Related Work,[0],[0]
"Docent is an SMT-based document-level decoder (Hardmeier et al., 2012, 2013), which tries to modify the initial translation generated by the Moses decoder (Koehn et al., 2007) through stochastic local search and hill-climbing.",6 Related Work,[0],[0]
Garcia et al. (2015) make use of neural-based continuous word representations to incorporate distributional semantics into Docent.,6 Related Work,[0],[0]
"In another work, Garcia et al. (2017) incorporate new word embedding features into Docent to improve the lexical consistency of translations.",6 Related Work,[0],[0]
"The proposed methods fail to yield improvements upon automatic evaluation.
",6 Related Work,[0],[0]
"Larger Context Neural MT Jean et al. (2017)
extend the vanilla attention-based neural MT model (Bahdanau et al., 2015) by conditioning the decoder on the previous sentence via attention over its words.",6 Related Work,[0],[0]
Extending their model to consider the global source document context would be challenging due to the large size of computation graph over all the words in the source document.,6 Related Work,[0],[0]
"Wang et al. (2017) employ a 2-level hierarichal RNN to summarise three previous source sentences, which is then used as an additional input to the decoder hidden state.",6 Related Work,[0],[0]
Bawden et al. (2017) use multi-encoder NMT models to exploit context from the previous source and target sentence.,6 Related Work,[0],[0]
They highlight the importance of targetside context but report deteriorated BLEU scores when using it.,6 Related Work,[0],[0]
All these works consider a very local source/target context and completely ignore the global source and target document contexts.,6 Related Work,[0],[0]
We have proposed a document-level neural MT model that captures global source and target document context.,7 Conclusion,[0],[0]
Our model augments the vanilla sentence-based NMT model with external memories to incorporate documental interdependencies on both source and target sides.,7 Conclusion,[0],[0]
We show statistically significant improvements of the translation quality on three language pairs.,7 Conclusion,[0],[0]
"For future work, we intend to investigate models which incorporate specific discourse-level phenomena.",7 Conclusion,[0],[0]
The authors are grateful to André,Acknowledgments,[0],[0]
Martins and the anonymous reviewers for their helpful comments and corrections.,Acknowledgments,[0],[0]
"This work was supported by the Multi-modal Australian ScienceS Imaging and Visualisation Environment (MASSIVE) (www. massive.org.au), and partially supported by a Google Faculty Award to GH and the Australian Research Council through DP160102686.",Acknowledgments,[0],[0]
We present a document-level neural machine translation model which takes both source and target document context into account using memory networks.,abstractText,[0],[0]
"We model the problem as a structured prediction problem with interdependencies among the observed and hidden variables, i.e., the source sentences and their unobserved target translations in the document.",abstractText,[0],[0]
"The resulting structured prediction problem is tackled with a neural translation model equipped with two memory components, one each for the source and target side, to capture the documental interdependencies.",abstractText,[0],[0]
"We train the model endto-end, and propose an iterative decoding algorithm based on block coordinate descent.",abstractText,[0],[0]
"Experimental results of English translations from French, German, and Estonian documents show that our model is effective in exploiting both source and target document context, and statistically significantly outperforms the previous work in terms of BLEU and METEOR.",abstractText,[0],[0]
Document Context Neural Machine Translation with Memory Networks,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 414–419 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
414",text,[0],[0]
Event Detection (ED) is an important subtask of event extraction.,1 Introduction,[0],[0]
It extracts event triggers from individual sentences and further identifies the type of the corresponding events.,1 Introduction,[0],[0]
"For instance, according to the ACE-2005 annotation guideline, in the sentence “Jane and John are married”, an ED system should be able to identify the word “married” as a trigger of the event “Marry”.",1 Introduction,[0],[0]
"However, it may be difficult to identify events from isolated sentences, because the same event trigger might represent different event types in different contexts.
",1 Introduction,[0],[0]
"Existing ED methods can mainly be categorized into two classes, namely, feature-based methods (e.g., (McClosky et al., 2011; Hong et al., 2011; Li et al., 2014)) and representation-based methods (e.g., (Nguyen and Grishman, 2015; Chen et al.,
2015; Liu et al., 2016a; Chen et al., 2017)).",1 Introduction,[0],[0]
"The former mainly rely on a set of hand-designed features, while the latter employ distributed representation to capture meaningful semantic information.",1 Introduction,[0],[0]
"In general, most of these existing methods mainly exploit sentence-level contextual information.",1 Introduction,[0],[0]
"However, document-level information is also important for ED, because the sentences in the same document, although they may contain different types of events, are often correlated with respect to the theme of the document.",1 Introduction,[0],[0]
"For example, there are the following sentences in ACE-2005:
...",1 Introduction,[0],[0]
I knew it was time to leave.,1 Introduction,[0],[0]
Isn’t that a great argument for term limits? ...,1 Introduction,[0],[0]
"If we only examine the first sentence, it is hard to determine whether the trigger “leave” indicates a “Transport” event meaning that he wants to leave the current place, or an “End-Position” event indicating that he will stop working for his current organization.",1 Introduction,[0],[0]
"However, if we can capture the contextual information of this sentence, it is more confident for us to label “leave” as the trigger of an “End-Position” event.",1 Introduction,[0],[0]
"Upon such observation, there have been some feature-based studies (Ji and Grishman, 2008; Liao and Grishman, 2010; Huang and Riloff, 2012) that construct rules to capture document-level information for improving sentence-level ED.",1 Introduction,[0],[0]
"However, they suffer from two major limitations.",1 Introduction,[0],[0]
"First, the features used therein often need to be manually designed and may involve error propagation due to natural language processing; Second, they discover inter-event information at document level by constructing inference rules, which is time-consuming and is hard to make the rule set as complete as possible.",1 Introduction,[0],[0]
"Besides, a representation-based study has been presented in (Duan et al., 2017), which employs the PV-DM model to train document embeddings and further uses it in a RNN-based event classifier.",1 Introduction,[0],[0]
"However, as being limited by the unsupervised training
process, the document-level representation cannot specifically capture event-related information.
",1 Introduction,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, for ED at sentence level.",1 Introduction,[0],[0]
"This model first learns ED oriented embeddings of documents through a hierarchical and supervised attention based bidirectional RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",1 Introduction,[0],[0]
It then uses the learned document embeddings to facilitate another bidirectional RNN model to identify event triggers and their types in individual sentences.,1 Introduction,[0],[0]
This learning process is guided by a general loss function where the loss corresponding to attention at both word and sentence levels and that of event type identification are integrated.,1 Introduction,[0],[0]
"It should be mentioned that although the attention mechanism has recently been applied effectively in various tasks, including machine translation (Zhang et al., 2017), question answering (Hao et al., 2017), document summarization (Tan et al., 2017), etc., this is the first study, to the best of our knowledge, which adopts a hierarchical and supervised attention mechanism to learn ED oriented embeddings of documents.
",1 Introduction,[0],[0]
"We evaluate the developed DEEB-RNN model on the benchmark dataset, ACE-2005, and systematically investigate the impacts of different supervised attention strategies on its performance.",1 Introduction,[0],[0]
"Experimental results show that the DEEBRNN model outperforms both feature-based and
representation-based state-of-the-art methods in terms of recall and F1-measure.",1 Introduction,[0],[0]
We formalize ED as a multi-class classification problem.,2 The Proposed Model,[0],[0]
"Given a sentence, we treat every word in it as a trigger candidate, and classify each candidate to a certain event type.",2 The Proposed Model,[0],[0]
"In the ACE-2005 dataset, there are 8 event types, further being divided into 33 subtypes, and a “Not Applicable (NA)” type.",2 The Proposed Model,[0],[0]
"Without loss of generality, in this paper we regard the 33 subtypes as 33 event types.",2 The Proposed Model,[0],[0]
"Figure 1 presents the schematic diagram of the proposed DEEB-RNN model, which contains two main modules:
1.",2 The Proposed Model,[0],[0]
"The ED Oriented Document Embedding Learning (EDODEL) module, which learns the distributed representations of documents from both word and sentence levels via the well-designed hierarchical and supervised attention mechanism.
2.",2 The Proposed Model,[0],[0]
"The Document-level Enhanced Event Detector (DEED) module, which tags each trigger candidate with an event type based on the learned embedding of documents.",2 The Proposed Model,[0],[0]
"To learn the ED oriented embedding of a document, we apply the hierarchical and supervised attention network presented in Figure 1, which consists of a word-level Bi-GRU (Schuster and Paliwal, 2002) encoder with attention on event triggers
and a sentence-level Bi-GRU encoder with attention on sentences with events.",2.1 The EDODEL Module,[0],[0]
"Given a document with L sentences, DEEB-RNN learns its embedding for detecting events in all sentences.
",2.1 The EDODEL Module,[0],[0]
"Word-level embeddings Given a sentence si (i = 1, 2, ..., L) consisting of words {wit|t = 1, 2, ..., T}.",2.1 The EDODEL Module,[0],[0]
"For each word wit, we first concatenate its embedding wit and its entity type embedding1 eit (Nguyen and Grishman, 2015) as the input git of a Bi-GRU and thus obtain the bidirectional hidden state hit:
hit = [ −−−−→ GRUw(git), ←−−−− GRUw(git)].",2.1 The EDODEL Module,[0],[0]
"(1)
We then feed hit to a perceptron with no bias to get uit = tanh(Wwhit) as a hidden representation of hit and also obtain an attention weight αit = u T itcw, which should be normalized through a softmax function.",2.1 The EDODEL Module,[0],[0]
"Here, similar to that in (Yang et al., 2016), cw is a vector representing the wordlevel context of wit, which is initialized at random.",2.1 The EDODEL Module,[0],[0]
"Finally, the embedding of the sentence si can be obtained by summing up hit with their weights:
si = T∑ t=1 αithit.",2.1 The EDODEL Module,[0],[0]
"(2)
To pay more attention to trigger words than other words, we construct the gold word-level attention signals α∗i for the sentence si, as illustrated in Figure 2a.",2.1 The EDODEL Module,[0],[0]
"We can then take the square error as the general loss of the attention at word level to supervise the learning process:
Ew(α ∗,α) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
T∑ t=1 (α∗it − αit)2.,2.1 The EDODEL Module,[0],[0]
"(3)
1The words in the ACE-2005 dataset are annotated with their entity types (annotated as “NA” if they are not an entity).
",2.1 The EDODEL Module,[0],[0]
"Sentence-level embeddings Given the sentence embeddings {si|i = 1, 2, ..., L}, we first get the hidden state qi via a Bi-GRU:
qi = [ −−−→ GRUs(si), ←−−− GRUs(si)].",2.1 The EDODEL Module,[0],[0]
"(4)
Then we feed qi to a perceptron with no bias to get the hidden representation ti = tanh(Wsqi) and also obtain an attention weight βi = tTi cs to be normalized via softmax.",2.1 The EDODEL Module,[0],[0]
"Similarly, cs represents the sentence-level context of si to be randomly initialized.",2.1 The EDODEL Module,[0],[0]
"We eventually obtain the document embedding d as:
d = L∑ i=1 βisi.",2.1 The EDODEL Module,[0],[0]
"(5)
We also think that the sentences containing event should obtain more attention than other ones.",2.1 The EDODEL Module,[0],[0]
"Therefore, similar to the case at word level, we construct the gold sentence-level attention signals β∗ for the document d, as illustrated in Figure 2b, and further take the square error as the general loss of the attention at sentence level to supervise the learning process:
Es(β ∗,β) = L∑ i=1",2.1 The EDODEL Module,[0],[0]
(β∗i − βi)2.,2.1 The EDODEL Module,[0],[0]
(6),2.1 The EDODEL Module,[0],[0]
"We employ another Bi-GRU encoder and a softmax output layer to model the ED task, which can handle event triggers with multiple words.",2.2 The DEED Module,[0],[0]
"Specifically, given a sentence sj (j = 1, 2, ..., L) in document d, for each of its word wjt (t = 1, 2, ..., T ), we concatenate its word embedding wjt and entity type embedding ejt with the corresponding document embedding d as the input rjt of the Bi-GRU and thus obtain the hidden state fjt:
fjt = [ −−−→ GRUe(rjt), ←−−− GRUe(rjt)].",2.2 The DEED Module,[0],[0]
"(7)
Finally, we get the probability vector ojt with K dimensions through a softmax layer for wjt, where the k-th element, o(k)jt , of ojt indicates the probability of classifying wjt to the k-th event type.",2.2 The DEED Module,[0],[0]
"The loss function, J(y,o), can thus be defined in terms of the cross-entropy error of the real event type yjt and the predicted probability o(k)jt as follows:
J(y,o) =",2.2 The DEED Module,[0],[0]
"− L∑
j=1 T∑ t=1 K∑ k=1",2.2 The DEED Module,[0],[0]
"I(yjt = k)log o (k) jt , (8)
where I(·) is the indicator function.",2.2 The DEED Module,[0],[0]
"In the DEEB-RNN model, the above two modules are jointly trained.",2.3 Joint Training of the DEEB-RNN model,[0],[0]
"For this purpose, we define the joint loss function in the training process upon the losses specified for different modules as follows:
J(θ)= ∑ ∀d∈ϕ (J(y,o)+λEw(α ∗,α)+µEs(β ∗,β)), (9) where θ denotes, as a whole, the parameters used in DEEB-RNN, ϕ is the training document set, and λ and µ are hyper-parameters for striking a balance among J(y,o), Ew(α∗,α) and Es(β∗,β).",2.3 Joint Training of the DEEB-RNN model,[0],[0]
We validate the proposed model through comparison with state-of-the-art methods on the ACE2005 dataset.,3.1 Datasets and Settings,[0],[0]
"In the experiments, the validation set has 30 documents from different genres, the test set has 40 documents and the training set contains the remaining 529 documents.",3.1 Datasets and Settings,[0],[0]
"All the data preprocessing and evaluation criteria follow those in (Ghaeini et al., 2016).
",3.1 Datasets and Settings,[0],[0]
Hyper-parameters are tuned on the validation set.,3.1 Datasets and Settings,[0],[0]
"We set the dimension of the hidden layers corresponding to GRUw, GRUs, and GRUe to 300, 200, and 300, respectively, the output size of Ww and Ws to 600 and 400, respectively, the dimension of entity type embeddings to 50, the batch size to 25, the dropout rate to 0.5.",3.1 Datasets and Settings,[0],[0]
"In addition, we utilize the pre-trained word embeddings with 300 dimensions from (Mikolov et al., 2013) for initialization.",3.1 Datasets and Settings,[0],[0]
"For entity types, their embeddings are randomly initialized.",3.1 Datasets and Settings,[0],[0]
"We train the model using Stochastic Gradient Descent (SGD) over shuffled mini-batches and using dropout (Krizhevsky et al., 2012) for regularization.",3.1 Datasets and Settings,[0],[0]
"In order to validate the proposed DEEB-RNN model through experimental comparison, we choose the following typical models as the baselines.
",3.2 Baseline Models,[0],[0]
"Sentence-level is a feature-based model proposed in (Hong et al., 2011), which regards entitytype consistency as a key feature to predict event mentions.
",3.2 Baseline Models,[0],[0]
"Joint Local is a feature-based model developed in (Li et al., 2013), which incorporates such features that explicitly capture the dependency among multiple triggers and arguments.
",3.2 Baseline Models,[0],[0]
"JRNN is a representation-based model proposed in (Nguyen et al., 2016), which exploits the inter-dependency between event triggers and argument roles via discrete structures.
",3.2 Baseline Models,[0],[0]
"Skip-CNN is a representation-based model presented in (Nguyen and Grishman, 2016), which proposes a novel convolution to exploit nonconsecutive k-grams for event detection.
",3.2 Baseline Models,[0],[0]
"ANN-S2 is a representation-based model developed in (Liu et al., 2017), which explicitly exploits argument information for event detection via supervised attention mechanisms.
",3.2 Baseline Models,[0],[0]
"Cross-event is a feature-based model proposed in (Liao and Grishman, 2010), which learns relations among event types from training corpus and futher helps predict the occurrence of events.
",3.2 Baseline Models,[0],[0]
"PSL is a feature-based model developed in (Liu et al., 2016b), which encods global information such as event-event association in the form of logic using the probabilistic soft logic model.
",3.2 Baseline Models,[0],[0]
"DLRNN is a representation-based model proposed in (Duan et al., 2017), which automatically extracts cross-sentence clues to improve sentencelevel event detection.",3.2 Baseline Models,[0],[0]
"In this section, we conduct experiments on the ACE-2005 dataset to demonstrate the effectiveness of different attention strategies.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"Bi-GRU is the basic ED model, which does not employ document-level embeddings.
",3.3 Impacts of Different Attention Strategies,[0],[0]
"DEEB-RNN uses the document embeddings and computes attentions without supervision, in which hyper-parameters λ and µ are set to 0.
DEEB-RNN1/2/3 means they uses the gold attention signals as supervision information.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Specifically, DEEB-RNN1 uses only the gold word-level attention signal (λ = 1 and µ = 0), DEEB-RNN2 uses only the gold sentence-level attention signal (λ = 0 and µ = 1), whilst DEEB-RNN3 employs the gold attention signals at both word and sen-
tence levels (λ = 1 and µ = 1).",3.3 Impacts of Different Attention Strategies,[0],[0]
"Table 1 compares these methods, where we can observe that the methods with document embeddings (i.e., the last four) significantly outperform the pure Bi-GRU method, which suggests that document-level information is very beneficial for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
"An interesting phenomenon is that, as compared to DEEB-RNN, DEEB-RNN2 changes the precision-recall balance.",3.3 Impacts of Different Attention Strategies,[0],[0]
This is because of the following reasons.,3.3 Impacts of Different Attention Strategies,[0],[0]
"On one hand, as compared to DEEB-RNN, DEEB-RNN2 uses the gold sentence-level attention signal, indicating that it pays special attention to the sentences containing events with event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
"In this way, the BiRNN model for learning document embeddings will filter out the sentences containing events but without explicit event triggers.",3.3 Impacts of Different Attention Strategies,[0],[0]
That means the events detected by DEEB-RNN2 are basically the ones with explicit event triggers.,3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, as compared to DEEB-RNN, the precision of DEEBRNN2 is improved; On the other hand, the above strategy may result in less learning of words, which are event triggers but do not appear in the training dataset.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Therefore, those sentences with such event triggers cannot be detected.",3.3 Impacts of Different Attention Strategies,[0],[0]
"The recall of DEEB-RNN2 is thus lowered, as compared to DEEB-RNN.",3.3 Impacts of Different Attention Strategies,[0],[0]
"Moreover, DEEB-RNN3 shows the best performance, indicating that the gold attention signals at both word and sentence levels are useful for ED.",3.3 Impacts of Different Attention Strategies,[0],[0]
Table 2 presents the overall performance of all methods on ACE-2005.,3.4 Performance Comparison,[0],[0]
"We can see that different versions of DEEB-RNN consistently out-
perform the existing state-of-the-art methods in terms of both recall and F1-measure, while their precision is comparable to that of others.",3.4 Performance Comparison,[0],[0]
"The better performance of DEEB-RNN can be explained by the following reasons: (1) Compared with feature-based methods, including Sentencelevel, Joint Local, and representation-based methods, including JRNN, Skip-CNN and ANN-S2, our method exploits document-level information (i.e., the ED oriented document embeddings) from both word and sentence levels in a document by the supervised attention mechanism, which enhance the ability of identifying trigger words; (2) Compared with feature-based methods using document-level information, such as Cross-event, PSL, our method can automatically capture event types in documents via a end-to-end Bi-RNN based model without manually designed rules; (3) Compared with representation-based methods using document-level information, such as DLRNN, our method can learn event detection oriented embeddings of documents through the hierarchical and supervised attention based Bi-RNN network.",3.4 Performance Comparison,[0],[0]
"In this study, we proposed a hierarchical and supervised attention based and document embedding enhanced Bi-RNN method, called DEEB-RNN, for event detection.",4 Conclusions and Future Work,[0],[0]
We explored different strategies to construct gold word- and sentence-level attentions to focus on event information.,4 Conclusions and Future Work,[0],[0]
Experiments on the ACE-2005 dataset demonstrate that DEEB-RNN achieves better performance as compared to the state-of-the-art methods in terms of both recall and F1-measure.,4 Conclusions and Future Work,[0],[0]
"In this paper, we can strike a balance between sentence and document embeddings by adjusting their dimensions.",4 Conclusions and Future Work,[0],[0]
"In the future, we may improve the DEEB-RNN model to automatically determine the weights of sentence and document embeddings.",4 Conclusions and Future Work,[0],[0]
"This work is supported by National Key Research and Development Program of China under grants 2016YFB1000902 and 2017YFC0820404, and National Natural Science Foundation of China under grants 61772501, 61572473, 61572469, and 91646120.",Acknowledgments,[0],[0]
"We are grateful to Dr. Liu Kang of the Institute of Automation, Chinese Academy of Sciences for very helpful discussion on event detection.",Acknowledgments,[0],[0]
Document-level information is very important for event detection even at sentence level.,abstractText,[0],[0]
"In this paper, we propose a novel Document Embedding Enhanced Bi-RNN model, called DEEB-RNN, to detect events in sentences.",abstractText,[0],[0]
"This model first learns event detection oriented embeddings of documents through a hierarchical and supervised attention based RNN, which pays word-level attention to event triggers and sentence-level attention to those sentences containing events.",abstractText,[0],[0]
It then uses the learned document embedding to enhance another bidirectional RNN model to identify event triggers and their types in sentences.,abstractText,[0],[0]
"Through experiments on the ACE-2005 dataset, we demonstrate the effectiveness and merits of the proposed DEEB-RNN model via comparison with state-of-the-art methods.",abstractText,[0],[0]
Document Embedding Enhanced Event Detection with Hierarchical and Supervised Attention,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2020–2030 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2020",text,[0],[0]
Recurrent neural networks have become one of the most widely used models in natural language processing (NLP).,1 Introduction,[0],[0]
"A number of variants of RNNs such as Long Short-Term Memory networks (LSTM; Hochreiter and Schmidhuber, 1997) and Gated Recurrent Unit networks (GRU; Cho et al., 2014) have been designed to model text capturing long-term dependencies in problems such as language modeling.",1 Introduction,[0],[0]
"However, document modeling, a key to many natural language
∗The first three authors made equal contributions to this paper.",1 Introduction,[0],[0]
"The work was done when the second author was visiting Edinburgh.
",1 Introduction,[0],[0]
"1Our TensorFlow code and datasets are publicly available at https://github.com/shashiongithub/ Document-Models-with-Ext-Information.
understanding tasks, is still an open challenge.",1 Introduction,[0],[0]
"Recently, some neural network architectures were proposed to capture large context for modeling text (Mikolov and Zweig, 2012; Ghosh et al., 2016; Ji et al., 2015; Wang and Cho, 2016).",1 Introduction,[0],[0]
"Lin et al. (2015) and Yang et al. (2016) proposed a hierarchical RNN network for document-level modeling as well as sentence-level modeling, at the cost of increased computational complexity.",1 Introduction,[0],[0]
"Tran et al. (2016) further proposed a contextual language model that considers information at interdocument level.
",1 Introduction,[0],[0]
"It is challenging to rely only on the document for its understanding, and as such it is not surprising that these models struggle on problems such as document summarization (Cheng and Lapata, 2016; Chen et al., 2016; Nallapati et al., 2017; See et al., 2017; Tan and Wan, 2017) and machine reading comprehension (Trischler et al., 2016; Miller et al., 2016; Weissenborn et al., 2017; Hu et al., 2017; Wang et al., 2017).",1 Introduction,[0],[0]
"In this paper, we formalize the use of external information to further guide document modeling for end goals.
",1 Introduction,[0],[0]
We present a simple yet effective document modeling framework for sentence extraction that allows machine reading with “external attention.”,1 Introduction,[0],[0]
Our model includes a neural hierarchical document encoder (or a machine reader) and a hierarchical attention-based sentence extractor.,1 Introduction,[0],[0]
Our hierarchical document encoder resembles the architectures proposed by Cheng and Lapata (2016) and Narayan et al. (2018) in that it derives the document meaning representation from its sentences and their constituent words.,1 Introduction,[0],[0]
"Our novel sentence extractor combines this document meaning representation with an attention mechanism (Bahdanau et al., 2015) over the external information to label sentences from the input document.",1 Introduction,[0],[0]
"Our model explicitly biases the extractor with external cues and
implicitly biases the encoder through training.
",1 Introduction,[0],[0]
We demonstrate the effectiveness of our model on two problems that can be naturally framed as sentence extraction with external information.,1 Introduction,[0],[0]
"These two problems, extractive document summarization and answer selection for machine reading comprehension, both require local and global contextual reasoning about a given document.",1 Introduction,[0],[0]
"Extractive document summarization systems aim at creating a summary by identifying (and subsequently concatenating) the most important sentences in a document, whereas answer selection systems select the candidate sentence in a document most likely to contain the answer to a query.",1 Introduction,[0],[0]
"For document summarization, we exploit the title and image captions which often appear with documents (specifically newswire articles) as external information.",1 Introduction,[0],[0]
"For answer selection, we use word overlap features, such as the inverse sentence frequency (ISF, Trischler et al., 2016) and the inverse document frequency (IDF) together with the query, all formulated as external cues.
",1 Introduction,[0],[0]
"Our main contributions are three-fold: First, our model ensures that sentence extraction is done in a larger (rich) context, i.e., the full document is read first before we start labeling its sentences for extraction, and each sentence labeling is done by implicitly estimating its local and global relevance to the document and by directly attending to some external information for importance cues.
",1 Introduction,[0],[0]
"Second, while external information has been shown to be useful for summarization systems using traditional hand-crafted features (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001), our model is the first to exploit such information in deep learning-based summarization.",1 Introduction,[0],[0]
"We evaluate our models automatically (in terms of ROUGE scores) on the CNN news highlights dataset (Hermann et al., 2015).",1 Introduction,[0],[0]
"Experimental results show that our summarizer, informed with title and image captions, consistently outperforms summarizers that do not use this information.",1 Introduction,[0],[0]
We also conduct a human evaluation to judge which type of summary participants prefer.,1 Introduction,[0],[0]
"Our results overwhelmingly show that human subjects find our summaries more informative and complete.
",1 Introduction,[0],[0]
"Lastly, with the machine reading capabilities of our model, we confirm that a full document needs to be “read” to produce high quality extracts allowing a rich contextual reasoning, in contrast to previous answer selection approaches that often
measure a score between each sentence in the document and the question and return the sentence with highest score in an isolated manner (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",1 Introduction,[0],[0]
Our model with ISF and IDF scores as external features achieves competitive results for answer selection.,1 Introduction,[0],[0]
"Our ensemble model combining scores from our model and word overlap scores using a logistic regression layer achieves state-ofthe-art results on the popular question answering datasets WikiQA (Yang et al., 2015) and NewsQA (Trischler et al., 2016), and it obtains comparable results to the state of the art for SQuAD (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"We also evaluate our approach on the MSMarco dataset (Nguyen et al., 2016) and elaborate on the behavior of our machine reader in a scenario where each candidate answer sentence is contextually independent of each other.",1 Introduction,[0],[0]
"Given a document D consisting of a sequence of n sentences (s1, s2, ..., sn) , we aim at labeling each sentence si in D with a label yi ∈ {0, 1} where yi = 1 indicates that si is extraction-worthy and 0 otherwise.",2 Document Modeling For Sentence Extraction,[0],[0]
"Our architecture resembles those previously proposed in the literature (Cheng and Lapata, 2016; Nallapati et al., 2017).",2 Document Modeling For Sentence Extraction,[0],[0]
"The main components include a sentence encoder, a document encoder, and a novel sentence extractor (see Figure 1) that we describe in more detail below.",2 Document Modeling For Sentence Extraction,[0],[0]
"The novel characteristics of our model are that each sentence is labeled by implicitly estimating its (local and global) relevance to the document and by directly attending to some external information for importance cues.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Sentence Encoder A core component of our model is a convolutional sentence encoder (Kim, 2014; Kim et al., 2016) which encodes sentences into continuous representations.",2 Document Modeling For Sentence Extraction,[0],[0]
We use temporal narrow convolution by applying a kernel filter K of width h to a window of h words in sentence s to produce a new feature.,2 Document Modeling For Sentence Extraction,[0],[0]
This filter is applied to each possible window of words in s to produce a feature map f ∈ Rk−h+1 where k is the sentence length.,2 Document Modeling For Sentence Extraction,[0],[0]
We then apply max-pooling over time over the feature map f and take the maximum value as the feature corresponding to this particular filter K. We use multiple kernels of various sizes and each kernel multiple times to construct the representation of a sentence.,2 Document Modeling For Sentence Extraction,[0],[0]
"In Figure 1, ker-
nels of size 2 (red) and 4 (blue) are applied three times each.",2 Document Modeling For Sentence Extraction,[0],[0]
The max-pooling over time operation yields two feature lists fK2 and fK4 ∈ R3.,2 Document Modeling For Sentence Extraction,[0],[0]
"The final sentence embeddings have six dimensions.
",2 Document Modeling For Sentence Extraction,[0],[0]
Document Encoder,2 Document Modeling For Sentence Extraction,[0],[0]
The document encoder composes a sequence of sentences to obtain a document representation.,2 Document Modeling For Sentence Extraction,[0],[0]
"We use a recurrent neural network with LSTM cells to avoid the vanishing gradient problem when training long sequences (Hochreiter and Schmidhuber, 1997).",2 Document Modeling For Sentence Extraction,[0],[0]
"Given a document D consisting of a sequence of sentences (s1, s2, . . .",2 Document Modeling For Sentence Extraction,[0],[0]
", sn), we follow common practice and feed the sentences in reverse order (Sutskever et al., 2014; Li et al., 2015; Filippova et al., 2015).
",2 Document Modeling For Sentence Extraction,[0],[0]
Sentence Extractor,2 Document Modeling For Sentence Extraction,[0],[0]
Our sentence extractor sequentially labels each sentence in a document with 1 or 0 by implicitly estimating its relevance in the document and by directly attending to the external information for importance cues.,2 Document Modeling For Sentence Extraction,[0],[0]
"It is implemented with another RNN with LSTM cells with an attention mechanism (Bahdanau et al., 2015) and a softmax layer.",2 Document Modeling For Sentence Extraction,[0],[0]
Our attention mechanism differs from the standard practice of attending intermediate states of the input (encoder).,2 Document Modeling For Sentence Extraction,[0],[0]
"Instead, our extractor attends to a sequence of p pieces of external information E : (e1, e2, ..., ep) relevant for the task (e.g., ei is a title or an image caption for summarization) for cues.",2 Document Modeling For Sentence Extraction,[0],[0]
"At time ti, it reads sentence si and makes a binary prediction, conditioned on the document representation (obtained from the document encoder), the previously labeled sentences and the external information.",2 Document Modeling For Sentence Extraction,[0],[0]
"This way, our labeler is able to identify locally and globally important sentences within the document which correlate well with the external information.
",2 Document Modeling For Sentence Extraction,[0],[0]
"Given sentence st at time step t, it returns a probability distribution over labels as:
p(yt|st, D,E) = softmax(g(ht, h′t))",2 Document Modeling For Sentence Extraction,[0],[0]
"(1) g(ht, h ′ t) = Uo(Vhht",2 Document Modeling For Sentence Extraction,[0],[0]
+W ′ hh ′,2 Document Modeling For Sentence Extraction,[0],[0]
"t) (2)
ht = LSTM(st, ht−1)
",2 Document Modeling For Sentence Extraction,[0],[0]
h′t = p∑ i=1,2 Document Modeling For Sentence Extraction,[0],[0]
"α(t,i)ei,
where α(t,i) = exp(htei)∑",2 Document Modeling For Sentence Extraction,[0],[0]
"j exp(htej)
where g(·) is a single-layer neural network with parameters Uo, Vh and W ′h.",2 Document Modeling For Sentence Extraction,[0],[0]
"ht is an intermedi-
ate RNN state at time step t. The dynamic context vector h′t is essentially the weighted sum of the external information (e1, e2, . .",2 Document Modeling For Sentence Extraction,[0],[0]
.,2 Document Modeling For Sentence Extraction,[0],[0]
", ep).",2 Document Modeling For Sentence Extraction,[0],[0]
Figure 1 summarizes our model.,2 Document Modeling For Sentence Extraction,[0],[0]
We validate our model on two sentence extraction problems: extractive document summarization and answer selection for machine reading comprehension.,3 Sentence Extraction Applications,[0],[0]
Both these tasks require local and global contextual reasoning about a given document.,3 Sentence Extraction Applications,[0],[0]
"As such, they test the ability of our model to facilitate document modeling using external information.
",3 Sentence Extraction Applications,[0],[0]
Extractive Summarization,3 Sentence Extraction Applications,[0],[0]
An extractive summarizer aims to produce a summary S by selecting m sentences from D (where m < n).,3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si should be included in the summary) by assigning score p(yi|si,D ,E , θ) quantifying the relevance of si to the summary.",3 Sentence Extraction Applications,[0],[0]
"We assemble a summary S by selecting m sentences with top p(yi = 1|si,D ,E , θ) scores.
",3 Sentence Extraction Applications,[0],[0]
We formulate external information E as the sequence of the title and the image captions associated with the document.,3 Sentence Extraction Applications,[0],[0]
"We use the convolutional sentence encoder to get their sentence-level representations.
",3 Sentence Extraction Applications,[0],[0]
"Answer Selection Given a question q and a document D , the goal of the task is to select one candidate sentence si ∈ D in which the answer exists.",3 Sentence Extraction Applications,[0],[0]
"In this setting, our sentence extractor sequentially predicts label yi ∈ {0, 1} (where 1 means that si contains the answer) and assign score p(yi|si,D ,E , θ) quantifying si’s relevance to the query.",3 Sentence Extraction Applications,[0],[0]
"We return as answer the sentence si with the highest p(yi = 1|si,D ,E , θ) score.
",3 Sentence Extraction Applications,[0],[0]
We treat the question q as external information and use the convolutional sentence encoder to get its sentence-level representation.,3 Sentence Extraction Applications,[0],[0]
This simplifies Eq.,3 Sentence Extraction Applications,[0],[0]
"(1) and (2) as follow:
p(yt|st, D, q) = softmax(g(ht, q)) (3) g(ht, q) = Uo(Vhht +Wqq),
where Vh and Wq are network parameters.",3 Sentence Extraction Applications,[0],[0]
"We exploit the simplicity of our model to further assimilate external features relevant for answer selection: the inverse sentence frequency (ISF, (Trischler et al., 2016)), the inverse document frequency (IDF) and a modified version of the ISF score which we call local ISF.",3 Sentence Extraction Applications,[0],[0]
"Trischler et al. (2016) have shown that a simple ISF baseline (i.e., a sentence with the highest ISF score as an answer) correlates well with the answers.",3 Sentence Extraction Applications,[0],[0]
"The ISF score αsi for the sentence si is computed as αsi =∑
w∈si∩q IDF(w), where IDF is the inverse document frequency score of word w, defined as: IDF(w) = log NNw , whereN is the total number of sentences in the training set and Nw is the number of sentences in which w appears.",3 Sentence Extraction Applications,[0],[0]
"Note that, si ∩ q
refers to the set of words that appear both in si and in q. Local ISF is calculated in the same manner as the ISF score, only with setting the total number of sentences (N ) to the number of sentences in the article that is being analyzed.
",3 Sentence Extraction Applications,[0],[0]
"More formally, this modifies Eq.",3 Sentence Extraction Applications,[0],[0]
"(3) as follows:
p(yt|st, D, q) = softmax(g(ht, q, αt, βt, γt)),(4)
where αt, βt and γt are the ISF, IDF and local ISF scores (real values) of sentence st respectively .",3 Sentence Extraction Applications,[0],[0]
"The function g is calculated as follows:
g(ht, q, αt, βt, γt) =Uo (Vhht+
Wqq +Wisf(αt · 1)+ Widf(βt · 1) +Wlisf(γt · 1) ) ,
where Wisf , Widf and Wlisf are new parameters added to the network and 1 is a vector of 1s of size equal to the sentence embedding size.",3 Sentence Extraction Applications,[0],[0]
"In Figure 1, these external feature vectors are represented as 6-dimensional gray vectors accompanied with dashed arrows.",3 Sentence Extraction Applications,[0],[0]
This section presents our experimental setup and results assessing our model in both the extractive summarization and answer selection setups.,4 Experiments and Results,[0],[0]
"In the rest of the paper, we refer to our model as XNET for its ability to exploit eXternal information to improve document representation.",4 Experiments and Results,[0],[0]
"Summarization Dataset We evaluated our models on the CNN news highlights dataset (Hermann et al., 2015).2",4.1 Extractive Document Summarization,[0],[0]
"We used the standard splits of Hermann et al. (2015) for training, validation, and testing (90,266/1,220/1,093 documents).",4.1 Extractive Document Summarization,[0],[0]
"We followed previous studies (Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017; Tan and Wan, 2017) in assuming that the
2Hermann et al. (2015) have also released the DailyMail dataset, but we do not report our results on this dataset.",4.1 Extractive Document Summarization,[0],[0]
We found that the script written by Hermann et al. to crawl DailyMail articles mistakenly extracts image captions as part of the main body of the document.,4.1 Extractive Document Summarization,[0],[0]
"As image captions often do not have sentence boundaries, they blend with the sentences of the document unnoticeably.",4.1 Extractive Document Summarization,[0],[0]
"This leads to the production of erroneous summaries.
",4.1 Extractive Document Summarization,[0],[0]
“story highlights” associated with each article are gold-standard abstractive summaries.,4.1 Extractive Document Summarization,[0],[0]
We trained our network on a named-entity-anonymized version of news articles.,4.1 Extractive Document Summarization,[0],[0]
"However, we generated deanonymized summaries and evaluated them against gold summaries to facilitate human evaluation and to make human evaluation comparable to automatic evaluation.
",4.1 Extractive Document Summarization,[0],[0]
"To train our model, we need documents annotated with sentence extraction information, i.e., each sentence in a document is labeled with 1 (summary-worthy) or 0 (not summary-worthy).",4.1 Extractive Document Summarization,[0],[0]
"We followed Nallapati et al. (2017) and automatically extracted ground truth labels such that all positively labeled sentences from an article collectively give the highest ROUGE (Lin and Hovy, 2003) score with respect to the gold summary.
",4.1 Extractive Document Summarization,[0],[0]
"We used a modified script of Hermann et al. (2015) to extract titles and image captions, and we associated them with the corresponding articles.",4.1 Extractive Document Summarization,[0],[0]
All articles get associated with their titles.,4.1 Extractive Document Summarization,[0],[0]
"The availability of image captions varies from 0 to 414 per article, with an average of 3 image captions.",4.1 Extractive Document Summarization,[0],[0]
"There are 40% CNN articles with at least one image caption.
",4.1 Extractive Document Summarization,[0],[0]
"All sentences, including titles and image captions, were padded with zeros to a sentence length of 100.",4.1 Extractive Document Summarization,[0],[0]
All input documents were padded with zeros to a maximum document length of 126.,4.1 Extractive Document Summarization,[0],[0]
"For each document, we consider a maximum of 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We experimented with various numbers (1, 3, 5, 10 and 20) of image captions on the validation set and found that our model performed best with 10 image captions.",4.1 Extractive Document Summarization,[0],[0]
"We refer the reader to the supplementary material for more implementation details to replicate our results.
",4.1 Extractive Document Summarization,[0],[0]
Comparison Systems We compared the output of our model against the standard baseline of simply selecting the first three sentences from each document as the summary.,4.1 Extractive Document Summarization,[0],[0]
"We refer to this baseline as LEAD in the rest of the paper.
",4.1 Extractive Document Summarization,[0],[0]
We also compared our system against the sentence extraction system of Cheng and Lapata (2016).,4.1 Extractive Document Summarization,[0],[0]
"We refer to this system as POINTERNET as the neural attention architecture in Cheng and Lapata (2016) resembles the one of Pointer Networks (Vinyals et al., 2015).3 It does not exploit any external information.4 Cheng and Lap-
3The architecture of POINTERNET is closely related to our model without external information.
",4.1 Extractive Document Summarization,[0],[0]
"4Adding external information to POINTERNET is an in-
ata (2016) report only on the DailyMail dataset.",4.1 Extractive Document Summarization,[0],[0]
"We used their code (https://github.com/ cheng6076/NeuralSum) to produce results on the CNN dataset.5
Automatic Evaluation To automatically assess the quality of our summaries, we used ROUGE (Lin and Hovy, 2003), a recall-oriented metric, to compare our model-generated summaries to manually-written highlights.6",4.1 Extractive Document Summarization,[0],[0]
"Previous work has reported ROUGE-1 (R1) and ROUGE-2 (R2) scores to access informativeness, and ROUGE-L (RL) to access fluency.",4.1 Extractive Document Summarization,[0],[0]
"In addition to R1, R2 and RL, we also report ROUGE-3 (R3) and ROUGE-4 (R4) capturing higher order n-grams overlap to assess informativeness and fluency simultaneously.
",4.1 Extractive Document Summarization,[0],[0]
teresting direction of research,4.1 Extractive Document Summarization,[0],[0]
but we do not pursue it here.,4.1 Extractive Document Summarization,[0],[0]
"It requires decoding with multiple types of attentions and this is not the focus of this paper.
",4.1 Extractive Document Summarization,[0],[0]
5We are unable to compare our results to the extractive system of Nallapati et al. (2017) because they report their results on the DailyMail dataset and their code is not available.,4.1 Extractive Document Summarization,[0],[0]
"The abstractive systems of Chen et al. (2016) and Tan and Wan (2017) report their results on the CNN dataset, however, their results are not comparable to ours as they report on the full-length F1 variants of ROUGE to evaluate their abstractive summaries.",4.1 Extractive Document Summarization,[0],[0]
"We report ROUGE recall scores which is more appropriate to evaluate our extractive summaries.
",4.1 Extractive Document Summarization,[0],[0]
"6We used pyrouge, a Python package, to compute all our ROUGE scores with parameters “-a -c 95 -m -n 4 -w 1.2.”
",4.1 Extractive Document Summarization,[0],[0]
We report our results on both full length (three sentences with the top scores as the summary) and fixed length (first 75 bytes and 275 bytes as the summary) summaries.,4.1 Extractive Document Summarization,[0],[0]
"For full length summaries, our decision of selecting three sentences is guided by the fact that there are 3.11 sentences on average in the gold highlights of the training set.",4.1 Extractive Document Summarization,[0],[0]
"We conduct our ablation study on the validation set with full length ROUGE scores, but we report both fixed and full length ROUGE scores for the test set.
",4.1 Extractive Document Summarization,[0],[0]
We experimented with two types of external information: title (TITLE) and image captions (CAPTION).,4.1 Extractive Document Summarization,[0],[0]
"In addition, we experimented with the first sentence (FS) of the document as external information.",4.1 Extractive Document Summarization,[0],[0]
"Note that the latter is not external information, it is a sentence in the document.",4.1 Extractive Document Summarization,[0],[0]
"However, we wanted to explore the idea that the first sentence of the document plays a crucial part in generating summaries (Rush et al., 2015; Nallapati et al., 2016).",4.1 Extractive Document Summarization,[0],[0]
"XNET with FS acts as a baseline for XNET with title and image captions.
",4.1 Extractive Document Summarization,[0],[0]
We report the performance of several variants of XNET on the validation set in Table 1.,4.1 Extractive Document Summarization,[0],[0]
We also compare them against the LEAD baseline and POINTERNET.,4.1 Extractive Document Summarization,[0],[0]
These two systems do not use any additional information.,4.1 Extractive Document Summarization,[0],[0]
"Interestingly, all the variants of XNET significantly outperform LEAD and POINTERNET.",4.1 Extractive Document Summarization,[0],[0]
"When the title (TITLE), image captions (CAPTION) and the first sentence (FS) are used separately as additional information, XNET performs best with TITLE as its external information.",4.1 Extractive Document Summarization,[0],[0]
"Our result demonstrates the importance of the title of the document in extractive summarization (Edmundson, 1969; Kupiec et al., 1995; Mani, 2001).",4.1 Extractive Document Summarization,[0],[0]
The performance with TITLE and CAPTION is better than that with FS.,4.1 Extractive Document Summarization,[0],[0]
"We also tried possible combinations of TITLE, CAPTION and FS.",4.1 Extractive Document Summarization,[0],[0]
All XNET models are superior to the ones without any external information.,4.1 Extractive Document Summarization,[0],[0]
"XNET performs best when TITLE and CAPTION are jointly used as external information (55.4%, 21.8%, 11.8%, 7.5%, and 49.2% for R1, R2, R3, R4, and RL respectively).",4.1 Extractive Document Summarization,[0],[0]
"It is better than the the LEAD baseline by 3.7 points on average and than POINTERNET by 1.8 points on average, indicating that external information is useful to identify the gist of the document.",4.1 Extractive Document Summarization,[0],[0]
"We use this model for testing purposes.
",4.1 Extractive Document Summarization,[0],[0]
Our final results on the test set are shown in Table 2.,4.1 Extractive Document Summarization,[0],[0]
"It turns out that for smaller summaries (75 bytes) LEAD and POINTERNET are superior
to XNET.",4.1 Extractive Document Summarization,[0],[0]
"This result could be because LEAD (always) and POINTERNET (often) include the first sentence in their summaries, whereas, XNET is better capable at selecting sentences from various document positions.",4.1 Extractive Document Summarization,[0],[0]
"This is not captured by smaller summaries of 75 bytes, but it becomes more evident with longer summaries (275 bytes and full length) where XNET performs best across all ROUGE scores.",4.1 Extractive Document Summarization,[0],[0]
"We note that POINTERNET outperforms LEAD for 75-byte summaries, then its performance drops behind LEAD for 275-byte summaries, but then it outperforms LEAD for full length summaries on the metrics R1, R2 and RL.",4.1 Extractive Document Summarization,[0],[0]
"It shows that POINTERNET with its attention over sentences in the document is capable of exploring more than first few sentences in the document, but it is still behind XNET which is better at identifying salient sentences in the document.",4.1 Extractive Document Summarization,[0],[0]
"XNET performs significantly better than POINTERNET by 0.8 points for 275-byte summaries and by 1.9 points for full length summaries, on average for all ROUGE scores.
",4.1 Extractive Document Summarization,[0],[0]
Human Evaluation We complement our automatic evaluation results with human evaluation.,4.1 Extractive Document Summarization,[0],[0]
"We randomly selected 20 articles from the test set.
",4.1 Extractive Document Summarization,[0],[0]
Annotators were presented with a news article and summaries from four different systems.,4.1 Extractive Document Summarization,[0],[0]
"These include the LEAD baseline, POINTERNET, XNET and the human authored highlights.",4.1 Extractive Document Summarization,[0],[0]
"We followed the guidelines in Cheng and Lapata (2016), and asked our participants to rank the summaries from best (1st) to worst (4th) in order of informativeness (does the summary capture important information in the article?) and fluency (is the summary written in well-formed English?).",4.1 Extractive Document Summarization,[0],[0]
We did not allow any ties and we only sampled articles with nonidentical summaries.,4.1 Extractive Document Summarization,[0],[0]
We assigned this task to five annotators who were proficient English speakers.,4.1 Extractive Document Summarization,[0],[0]
Each annotator was presented with all 20 articles.,4.1 Extractive Document Summarization,[0],[0]
The order of summaries to rank was randomized per article.,4.1 Extractive Document Summarization,[0],[0]
"An example of summaries our subjects ranked is provided in the supplementary material.
",4.1 Extractive Document Summarization,[0],[0]
The results of our human evaluation study are shown in Table 3.,4.1 Extractive Document Summarization,[0],[0]
"As one might imagine, HUMAN gets ranked 1st most of the time (41%).",4.1 Extractive Document Summarization,[0],[0]
"However, it is closely followed by XNET which ranked 1st 28% of the time.",4.1 Extractive Document Summarization,[0],[0]
"In comparison, POINTERNET and LEAD were mostly ranked at 3rd and 4th places.",4.1 Extractive Document Summarization,[0],[0]
We also carried out pairwise comparisons between all models in Table 3 for their statistical significance using a one-way ANOVA with post-hoc Tukey HSD tests with (p < 0.01).,4.1 Extractive Document Summarization,[0],[0]
"It showed that XNET is significantly better than LEAD and POINTERNET, and it does not differ significantly from HUMAN.",4.1 Extractive Document Summarization,[0],[0]
"On the other hand, POINTERNET does not differ significantly from LEAD and it differs significantly from both XNET and HUMAN.",4.1 Extractive Document Summarization,[0],[0]
The human evaluation results corroborates our empirical results in Table 1 and Table 2: XNET is better than LEAD and POINTERNET in producing informative and fluent summaries.,4.1 Extractive Document Summarization,[0],[0]
"Question Answering Datasets We run experiments on four datasets collected for open domain question-answering tasks: WikiQA (Yang et al., 2015), SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MSMarco (Nguyen et al., 2016).
",4.2 Answer Selection,[0],[0]
NewsQA was especially designed to present lexical and syntactic divergence between questions and answers.,4.2 Answer Selection,[0],[0]
"It contains 119,633 questions posed by crowdworkers on 12,744 CNN articles previously collected by Hermann et al. (2015).",4.2 Answer Selection,[0],[0]
"In a similar manner, SQuAD associates 100,000+
question with a Wikipedia article’s first paragraph, for 500+ previously chosen articles.",4.2 Answer Selection,[0],[0]
WikiQA was collected by mining web-searching query logs and then associating them with the summary section of the Wikipedia article presumed to be related to the topic of the query.,4.2 Answer Selection,[0],[0]
"A similar collection procedure was followed to create MSMarco with the difference that each candidate answer is a whole paragraph from a different browsed website associated with the query.
",4.2 Answer Selection,[0],[0]
"We follow the widely used setup of leaving out unanswered questions (Trischler et al., 2016; Yang et al., 2015) and adapt the format of each dataset to our task of answer sentence selection by labeling a candidate sentence with 1 if any answer span is contained in that sentence.",4.2 Answer Selection,[0],[0]
"In the case of MSMarco, each candidate paragraph comes associated with a label, hence we treat each one as a single long sentence.",4.2 Answer Selection,[0],[0]
"Since SQuAD keeps the official test dataset hidden and MSMarco does not provide labels for its released test set, we report results on their official validation sets.",4.2 Answer Selection,[0],[0]
"For validation, we set apart 10% of each official training set.
",4.2 Answer Selection,[0],[0]
"Our dataset splits consist of 92,525, 5,165 and 5,124 samples for NewsQA; 79,032, 8,567, and 10,570 for SQuAD; 873, 122, and 237 for WikiQA; and 79,704, 9,706, and 9,650 for MSMarco, for training, validation, and testing respectively.
",4.2 Answer Selection,[0],[0]
"Comparison Systems We compared the output of our model against the ISF (Trischler et al., 2016) and LOCALISF baselines.",4.2 Answer Selection,[0],[0]
"Given an article, the sentence with the highest ISF score is selected as an answer for the ISF baseline and the sentence with the highest local ISF score for the LOCALISF baseline.",4.2 Answer Selection,[0],[0]
"We also compare our model against a neural network (PAIRCNN) that encodes (question, candidate) in an isolated manner as in previous work (Yin et al., 2016; dos Santos et al., 2016; Wang et al., 2016).",4.2 Answer Selection,[0],[0]
The architecture uses the sentence encoder explained in earlier sections to learn the question and candidate representations.,4.2 Answer Selection,[0],[0]
"The distribution over labels is given by p(yt|q) = p(yt|st, q) = softmax(g(st, q))",4.2 Answer Selection,[0],[0]
"where g(st, q) = ReLU(Wsq ·",4.2 Answer Selection,[0],[0]
[st; q] + bsq).,4.2 Answer Selection,[0],[0]
"In addition, we also compare our model against APCNN (dos Santos et al., 2016), ABCNN (Yin et al., 2016), L.D.C (Wang and Jiang, 2017), KVMemNN (Miller et al., 2016), and COMPAGGR, a state-of-the-art system by Wang et al. (2017).
",4.2 Answer Selection,[0],[0]
We experiment with several variants of our model.,4.2 Answer Selection,[0],[0]
"XNET is the vanilla version of our sen-
tence extractor conditioned only on the query q as external information (Eq.",4.2 Answer Selection,[0],[0]
(3)).,4.2 Answer Selection,[0],[0]
"XNET+ is an extension of XNET which uses ISF, IDF and local ISF scores in addition to the query q as external information (Eqn. (4)).",4.2 Answer Selection,[0],[0]
"We also experimented with a baseline XNETTOPK where we choose the top k sentences with highest ISF score, and then among them choose the one with the highest probability according to XNET.",4.2 Answer Selection,[0],[0]
"In our experiments, we set k = 5.",4.2 Answer Selection,[0],[0]
"In the end, we experimented with an ensemble network LRXNET which combines the XNET score, the COMPAGGR score and other word-overlap-based scores (tweaked and optimized for each dataset separately) for each sentence using a logistic regression classifier.",4.2 Answer Selection,[0],[0]
"It uses ISF and LocalISF scores for NewsQA, IDF and ISF scores for SQuAD, sentence length, IDF and ISF scores for WikiQA, and word overlap and ISF score for MSMarco.",4.2 Answer Selection,[0],[0]
"We refer the reader to the supplementary material for more implementation and optimization details to replicate our results.
",4.2 Answer Selection,[0],[0]
Evaluation Metrics,4.2 Answer Selection,[0],[0]
"We consider metrics that evaluate systems that return a ranked list of candidate answers: mean average precision (MAP), mean reciprocal rank (MRR), and accuracy (ACC).
",4.2 Answer Selection,[0],[0]
"Results Table 4 gives the results for the test sets of NewsQA and WikiQA, and the original validation sets of SQuAD and MSMarco.",4.2 Answer Selection,[0],[0]
"Our first observation is that XNET outperforms PAIRCNN, supporting our claim that it is beneficial to read the whole document in order to make decisions,
instead of only observing each candidate in isolation.
",4.2 Answer Selection,[0],[0]
"Secondly, we can observe that ISF is indeed a strong baseline that outperforms XNET.",4.2 Answer Selection,[0],[0]
"This means that just “reading” the document using a vanilla version of XNET is not sufficient, and help is required through a coarse filtering.",4.2 Answer Selection,[0],[0]
"Indeed, we observe that XNET+ outperforms all baselines except for COMPAGGR.",4.2 Answer Selection,[0],[0]
"Our ensemble model LRXNET can ultimately surpass COMPAGGR on majority of the datasets.
",4.2 Answer Selection,[0],[0]
This consistent behavior validates the machine reading capabilities and the improved document representation with external features of our model for answer selection.,4.2 Answer Selection,[0],[0]
"Specifically, the combination of document reading and word overlap features is required to be done in a soft manner, using a classification technique.",4.2 Answer Selection,[0],[0]
"Using it as a hard constraint, with XNETTOPK, does not achieve the best result.",4.2 Answer Selection,[0],[0]
We believe that often the ISF score is a better indicator of answer presence in the vicinity of certain candidate instead of in the candidate itself.,4.2 Answer Selection,[0],[0]
"As such, XNET+ is capable of using this feature in datasets with richer context.
",4.2 Answer Selection,[0],[0]
It is worth noting that the improvement gained by LRXNET over the state-of-the-art follows a pattern.,4.2 Answer Selection,[0],[0]
"For the SQuAD dataset, the results are comparable (less than 1%).",4.2 Answer Selection,[0],[0]
"However, the improvement for WikiQA reaches ∼3% and then the gap shrinks again for NewsQA, with an improvement of ∼1%.",4.2 Answer Selection,[0],[0]
"This could be explained by the fact that each sample of the SQuAD is a paragraph, compared to an article summary for WikiQA, and
to an entire article for NewsQA.",4.2 Answer Selection,[0],[0]
"Hence, we further strengthen our hypothesis that a richer context is needed to achieve better results, in this case expressed as document length, but as the length of the context increases the limitation of sequential models to learn from long rich sequences arises.7
Interestingly, our model lags behind COMPAGGR on the MSMarco dataset.",4.2 Answer Selection,[0],[0]
"It turns out this is due to contextual independence between candidates in the MSMarco dataset, i.e., each candidate is a stand-alone paragraph in this dataset, in contrast to contextually dependent candidate sentences from a document in the NewsQA, SQuAD and WikiQA datasets.",4.2 Answer Selection,[0],[0]
"As a result, our models (XNET+ and LRXNET) with document reading abilities perform poorly.",4.2 Answer Selection,[0],[0]
This can be observed by the fact that XNET and PAIRCNN obtain comparable results.,4.2 Answer Selection,[0],[0]
COMPAGGR performs better because comparing each candidate independently is a better strategy.,4.2 Answer Selection,[0],[0]
We describe an approach to model documents while incorporating external information that informs the representations learned for the sentences in the document.,5 Conclusion,[0],[0]
"We implement our approach through an attention mechanism of a neural network architecture for modeling documents.
",5 Conclusion,[0],[0]
"Our experiments with extractive document summarization and answer selection tasks validates our model in two ways: first, we demonstrate that external information is important to guide document modeling for natural language understanding tasks.",5 Conclusion,[0],[0]
"Our model uses image captions and the title of the document for document summarization, and the query with word overlap features for answer selection and outperforms its counterparts that do not use this information.",5 Conclusion,[0],[0]
"Second, our external attention mechanism successfully guides the learning of the document representation for the relevant end goal.",5 Conclusion,[0],[0]
"For answer selection, we show that inserting the query with word overlap features using our external attention mechanism outperforms state-of-the-art systems that naturally also have access to this information.",5 Conclusion,[0],[0]
"We thank Jianpeng Cheng for providing us with the CNN dataset and the implementation of Point-
7See the supplementary material for an example supporting our hypothesis.
",Acknowledgments,[0],[0]
erNet.,Acknowledgments,[0],[0]
We also thank the members of the Edinburgh NLP group for participating in our human evaluation experiments.,Acknowledgments,[0],[0]
"This work greatly benefitted from discussions with Jianpeng Cheng, Annie Louis, Pedro Balage, Alfonso Mendes, Sebastião Miranda, and members of the Edinburgh NLP group.",Acknowledgments,[0],[0]
"We gratefully acknowledge the support of the European Research Council (Lapata; award number 681760), the European Union under the Horizon 2020 SUMMA project (Narayan, Cohen; grant agreement 688139), and Huawei Technologies (Cohen).",Acknowledgments,[0],[0]
Document modeling is essential to a variety of natural language understanding tasks.,abstractText,[0],[0]
We propose to use external information to improve document modeling for problems that can be framed as sentence extraction.,abstractText,[0],[0]
We develop a framework composed of a hierarchical document encoder and an attention-based extractor with attention over external information.,abstractText,[0],[0]
We evaluate our model on extractive document summarization (where the external information is image captions and the title of the document) and answer selection (where the external information is a question).,abstractText,[0],[0]
"We show that our model consistently outperforms strong baselines, in terms of both informativeness and fluency (for CNN document summarization) and achieves state-of-the-art results for answer selection on WikiQA and NewsQA.1",abstractText,[0],[0]
Document Modeling with External Attention for Sentence Extraction,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1422–1432, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Document level sentiment classification is a fundamental task in sentiment analysis, and is crucial to understand user generated content in social networks or product reviews (Manning and Schütze, 1999; Jurafsky and Martin, 2000; Pang and Lee, 2008; Liu, 2012).",1 Introduction,[0],[0]
"The task calls for identifying the overall sentiment polarity (e.g. thumbs up or thumbs down, 1-5 stars on review sites) of a document.",1 Introduction,[0],[0]
"In literature, dominant approaches follow (Pang et al., 2002) and exploit machine learn-
∗Corresponding author.",1 Introduction,[0],[0]
"1 Codes and datasets are publicly available at
http://ir.hit.edu.cn/˜dytang.
ing algorithm to build sentiment classifier.",1 Introduction,[0],[0]
"Many of them focus on designing hand-crafted features (Qu et al., 2010; Paltoglou and Thelwall, 2010) or learning discriminate features from data, since the performance of a machine learner is heavily dependent on the choice of data representation (Bengio et al., 2015).
",1 Introduction,[0],[0]
Document level sentiment classification remains a significant challenge: how to encode the intrinsic (semantic or syntactic) relations between sentences in the semantic meaning of document.,1 Introduction,[0],[0]
This is crucial for sentiment classification because relations like “contrast” and “cause” have great influences on determining the meaning and the overall polarity of a document.,1 Introduction,[0],[0]
"However, existing studies typically fail to effectively capture such information.",1 Introduction,[0],[0]
"For example, Pang et al. (2002) and Wang and Manning (2012) represent documents with bag-of-ngrams features and build SVM classifier upon that.",1 Introduction,[0],[0]
"Although such feature-driven SVM is an extremely strong performer and hardly to be transcended, its “sparse” and “discrete” characteristics make it clumsy in taking into account of side information like relations between sentences.",1 Introduction,[0],[0]
"Recently, Le and Mikolov (2014) exploit neural networks to learn continuous document representation from data.",1 Introduction,[0],[0]
"Essentially, they use local ngram information and do not capture semantic relations between sentences.",1 Introduction,[0],[0]
"Furthermore, a person asked to do this task will naturally carry it out in a sequential, bottom-up fashion, analyze the meanings of sentences before considering semantic relations between them.",1 Introduction,[0],[0]
"This motivates us to develop an end-to-end and bottom-up algorithm to effectively model document representation.
",1 Introduction,[0],[0]
"In this paper, we introduce a neural network approach to learn continuous document representation for sentiment classification.",1 Introduction,[0],[0]
"The method is on the basis of the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a docu-
1422
ment) depends on the meanings of its constituents.",1 Introduction,[0],[0]
"Specifically, the approach models document representation in two steps.",1 Introduction,[0],[0]
"In the first step, it uses convolutional neural network (CNN) or long short-term memory (LSTM) to produce sentence representations from word representations.",1 Introduction,[0],[0]
"Afterwards, gated recurrent neural network is exploited to adaptively encode semantics of sentences and their inherent relations in document representations.",1 Introduction,[0],[0]
These representations are naturally used as features to classify the sentiment label of each document.,1 Introduction,[0],[0]
"The entire model is trained end-to-end with stochastic gradient descent, where the loss function is the cross-entropy error of supervised sentiment classification2.
",1 Introduction,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB3 and Yelp Dataset Challenge4.,1 Introduction,[0],[0]
"We compare to neural network models such as paragraph vector (Le and Mikolov, 2014), convolutional neural network, and baselines such as feature-based SVM (Pang et al., 2002), recommendation algorithm JMARS (Diao et al., 2014).",1 Introduction,[0],[0]
"Experimental results show that: (1) the proposed neural model shows superior performances over all baseline algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural
2A similar work can be found at: http: //deeplearning.net/tutorial/lstm.html
3http://www.imdb.com/ 4http://www.yelp.com/dataset_challenge
network in document modeling.",1 Introduction,[0],[0]
The main contributions of this work are as follows: • We present a neural network approach to encode relations between sentences in document representation for sentiment classification.,1 Introduction,[0],[0]
"•We report empirical results on four large-scale datasets, and show that the approach outperforms state-of-the-art methods for document level sentiment classification.",1 Introduction,[0],[0]
"•We report empirical results that traditional recurrent neural network is weak in modeling document composition, while adding neural gates dramatically improves the classification performance.",1 Introduction,[0],[0]
"We introduce the proposed neural model in this section, which computes continuous vector representations for documents of variable length.",2 The Approach,[0],[0]
These representations are further used as features to classify the sentiment label of each document.,2 The Approach,[0],[0]
"An overview of the approach is displayed in Figure 1.
",2 The Approach,[0],[0]
"Our approach models document semantics based on the principle of compositionality (Frege, 1892), which states that the meaning of a longer expression (e.g. a sentence or a document) comes from the meanings of its constituents and the rules used to combine them.",2 The Approach,[0],[0]
"Since a document consists of a list of sentences and each sentence is made up of a list of words, the approach models document representation in two stages.",2 The Approach,[0],[0]
"It first produces continuous sentence vectors from word represen-
tations with sentence composition (Section 2.1).",2 The Approach,[0],[0]
"Afterwards, sentence vectors are treated as inputs of document composition to get document representation (Section 2.2).",2 The Approach,[0],[0]
Document representations are then used as features for document level sentiment classification (Section 2.3).,2 The Approach,[0],[0]
"We first describe word vector representation, before presenting a convolutional neural network with multiple filters for sentence composition.
",2.1 Sentence Composition,[0],[0]
"Each word is represented as a low dimensional, continuous and real-valued vector, also known as word embedding (Bengio et al., 2003).",2.1 Sentence Composition,[0],[0]
"All the word vectors are stacked in a word embedding matrix Lw ∈ Rd×|V |, where d is the dimension of word vector and |V",2.1 Sentence Composition,[0],[0]
| is vocabulary size.,2.1 Sentence Composition,[0],[0]
"These word vectors can be randomly initialized from a uniform distribution (Socher et al., 2013b), or be pre-trained from text corpus with embedding learning algorithms (Mikolov et al., 2013; Pennington et al., 2014; Tang et al., 2014).",2.1 Sentence Composition,[0],[0]
"We adopt the latter strategy to make better use of semantic and grammatical associations of words.
",2.1 Sentence Composition,[0],[0]
We use convolutional neural network (CNN) and long short-term memory (LSTM) to compute continuous representations of sentences with semantic composition.,2.1 Sentence Composition,[0],[0]
"CNN and LSTM are stateof-the-art semantic composition models for sentiment classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2015; Li et al., 2015a).",2.1 Sentence Composition,[0],[0]
"They learn fixed-length vectors for sentences of varying length, captures words order in a sentence and does not depend on external dependency or constituency parse results.",2.1 Sentence Composition,[0],[0]
"One could also use tree-based composition method such as Recursive Neural Tensor Network (Socher et al., 2013b) or Tree-Structured LSTM (Tai et al., 2015; Zhu et al., 2015) as alternatives.
",2.1 Sentence Composition,[0],[0]
"Specifically, we try CNN with multiple convolutional filters of different widths (Tang et al., 2015) to produce sentence representation.",2.1 Sentence Composition,[0],[0]
Figure 2 displays the method.,2.1 Sentence Composition,[0],[0]
"We use multiple convolutional filters in order to capture local semantics of n-grams of various granularities, which have been proven effective for sentiment classification.",2.1 Sentence Composition,[0],[0]
"For example, a convolutional filter with a width of 2 essentially captures the semantics of bigrams in a sentence.",2.1 Sentence Composition,[0],[0]
"In this work, we use three convolutional filters whose widths are 1, 2 and 3 to encode the semantics of unigrams, bigram-
s and trigrams in a sentence.",2.1 Sentence Composition,[0],[0]
Each filter consists of a list of linear layers with shared parameters.,2.1 Sentence Composition,[0],[0]
"Formally, let us denote a sentence consisting of n words as {w1, w2, ...wi, ...wn}, let lc be the width of a convolutional filter, and let Wc, bc be the shared parameters of linear layers in the filter.",2.1 Sentence Composition,[0],[0]
Each word wi is mapped to its embedding representation ei ∈ Rd.,2.1 Sentence Composition,[0],[0]
"The input of a linear layer is the concatenation of word embeddings in a fixed-length window size lc, which is denoted as Ic =",2.1 Sentence Composition,[0],[0]
[ei; ei+1; ...; ei+lc−1] ∈ Rd·lc .,2.1 Sentence Composition,[0],[0]
"The output of a linear layer is calculated as
Oc = Wc · Ic + bc (1)
where Wc ∈ Rloc×d·lc , bc ∈ Rloc , loc is the output length of linear layer.",2.1 Sentence Composition,[0],[0]
"To capture global semantics of a sentence, we feed the outputs of linear layers to an average pooling layer, resulting in an output vector with fixed-length.",2.1 Sentence Composition,[0],[0]
"We further add hyperbolic tangent (tanh) to incorporate pointwise nonlinearity, and average the outputs of multiple filters to get sentence representation.
",2.1 Sentence Composition,[0],[0]
"We also try lstm as the sentence level semantic calculator, the performance comparison between these two variations is given in Section 3.",2.1 Sentence Composition,[0],[0]
The obtained sentence vectors are fed to a document composition component to calculate the document representation.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We present a gated recurrent neural network approach for document composition in this part.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Given the vectors of sentences of variable length as input, document composition produces a fixed-length document vector as output.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To this end, a simple strategy is ignoring the order of sen-
tences and averaging sentence vectors as document vector.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Despite its computational efficiency, it fails to capture complex linguistic relations (e.g. “cause” and “contrast”) between sentences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Convolutional neural network (Denil et al., 2014) is an alternative for document composition, which models local sentence relations with shared parameters of linear layers.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Standard recurrent neural network (RNN) can map vectors of sentences of variable length to a fixed-length vector by recursively transforming current sentence vector st with the output vector of the previous step ht−1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The transition function is typically a linear layer followed by pointwise non-linearity layer such as tanh.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
ht = tanh(Wr ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (2) where Wr ∈ Rlh×(lh+loc), br ∈ Rlh , lh and loc are dimensions of hidden vector and sentence vector, respectively.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Unfortunately, standard RNN suffers the problem of gradient vanishing or exploding (Bengio et al., 1994; Hochreiter and Schmidhuber, 1997), where gradients may grow or decay exponentially over long sequences.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
This makes it difficult to model long-distance correlations in a sequence.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"To address this problem, we develop a gated recurrent neural network for document composition, which works in a sequential way and adaptively encodes sentence semantics in document representations.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The approach is analogous to the recently emerged LSTM (Graves et al., 2013; Zaremba and Sutskever, 2014; Sutskever et al., 2014; Xu et al., 2015) and gated neural network (Cho et al., 2014; Chung et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"Specifically, the transition function of the gated RNN used in this work is calculated as follows.
",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
it = sigmoid(Wi ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
[ht−1; st] + bi) (3) ft = sigmoid(Wf ·,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + bf ) (4)
gt = tanh(Wr ·",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"[ht−1; st] + br) (5) ht = tanh(it gt + ft ht−1) (6)
where stands for element-wise multiplication, Wi, Wf , bi, bf adaptively select and remove history vector and input vector for semantic composition.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The model can be viewed as a LSTM whose output gate is alway on, since we prefer not to discarding any part of the semantics of sentences to get a better document representation.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
Figure 3 (a) displays a standard sequential way where the last hidden vector is regarded as the document representation for sentiment classification.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can make further extensions such as averaging hidden vectors as document representation, which takes considerations of a hierarchy of historical semantics with different granularities.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"The method is illustrated in Figure 3 (b), which shares some characteristics with (Zhao et al., 2015).",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
"We can go one step further to use preceding histories and following evidences in the same way, and exploit bidirectional (Graves et al., 2013) gated RNN as the calculator.",2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The model is embedded in Figure 1.,2.2 Document Composition with Gated Recurrent Neural Network,[0],[0]
The composed document representations can be naturally regarded as features of documents for sentiment classification without feature engineering.,2.3 Sentiment Classification,[0],[0]
"Specifically, we first add a linear layer to transform document vector to real-valued vector whose length is class number C. Afterwards, we add a softmax layer to convert real values to conditional probabilities, which is calculated as follows.
",2.3 Sentiment Classification,[0],[0]
"Pi = exp(xi)∑C
i′=1 exp(xi′) (7)
We conduct experiments in a supervised learning setting, where each document in the training data is accompanied with its gold sentiment label.
",2.3 Sentiment Classification,[0],[0]
Corpus #docs #s/d #w/d |V,2.3 Sentiment Classification,[0],[0]
"| #class Class Distribution
For model training, we use the cross-entropy error between gold sentiment distribution P g(d) and predicted sentiment distribution P (d) as the loss function.
loss = − ∑ d∈T",2.3 Sentiment Classification,[0],[0]
C∑ i=1,2.3 Sentiment Classification,[0],[0]
P gi (d) · log(Pi(d)),2.3 Sentiment Classification,[0],[0]
"(8)
where T is the training data, C is the number of classes, d represents a document.",2.3 Sentiment Classification,[0],[0]
"P g(d) has a 1-of-K coding scheme, which has the same dimension as the number of classes, and only the dimension corresponding to the ground truth is 1, with all others being 0.",2.3 Sentiment Classification,[0],[0]
We take the derivative of loss function through back-propagation with respect to the whole set of parameters θ =,2.3 Sentiment Classification,[0],[0]
"[Wc; bc;Wi; bi;Wf ; bf ;Wr; br;Wsoftmax, bsoftmax], and update parameters with stochastic gradient descent.",2.3 Sentiment Classification,[0],[0]
"We set the widths of three convolutional filters as 1, 2 and 3, output length of convolutional filter as 50.",2.3 Sentiment Classification,[0],[0]
"We learn 200-dimensional word embeddings with SkipGram (Mikolov et al., 2013) on each dataset separately, randomly initialize other parameters from a uniform distribution U(−0.01, 0.01), and set learning rate as 0.03.",2.3 Sentiment Classification,[0],[0]
We conduct experiments to empirically evaluate our method by applying it to document level sentiment classification.,3 Experiment,[0],[0]
We describe experimental settings and report empirical results in this section.,3 Experiment,[0],[0]
We conduct experiments on large-scale datasets consisting of document reviews.,3.1 Experimental Setting,[0],[0]
"Specifically, we use one movie review dataset from IMDB (Diao et al., 2014) and three restaurant review datasets from Yelp Dataset Challenge in 2013, 2014 and 2015.",3.1 Experimental Setting,[0],[0]
"Human labeled review ratings are regarded as gold standard sentiment labels, so that we do not need to manually annotate sentiment labels of
documents.",3.1 Experimental Setting,[0],[0]
"We do not consider the cases that rating does not match with review texts (Zhang et al., 2014).
",3.1 Experimental Setting,[0],[0]
Statistical information of these datasets are given in Table 1.,3.1 Experimental Setting,[0],[0]
"We use the same dataset split as in (Diao et al., 2014) on IMDB dataset, and split Yelp datasets into training, development and testing sets with 80/10/10.",3.1 Experimental Setting,[0],[0]
We run tokenization and sentence splitting with Stanford CoreNLP,3.1 Experimental Setting,[0],[0]
"(Manning et al., 2014) on all these datasets.",3.1 Experimental Setting,[0],[0]
"We use accuracy (Manning and Schütze, 1999; Jurafsky and Martin, 2000) and MSE (Diao et al., 2014) as evaluation metrics, where accuracy is a standard metric to measure the overall sentiment classification performance.",3.1 Experimental Setting,[0],[0]
"We use MSE to measure the divergences between predicted sentiment labels and ground truth sentiment labels because review labels reflect sentiment strengths (e.g. one star means strong negative and five star means strong positive).
",3.1 Experimental Setting,[0],[0]
"MSE = ∑N
i (goldi − predictedi)2 N
(9)",3.1 Experimental Setting,[0],[0]
"We compare our methods (Conv-GRNN and LSTM-GRNN) with the following baseline methods for document level sentiment classification.
",3.2 Baseline Methods,[0],[0]
"(1) Majority is a heuristic baseline, which assigns the majority sentiment label in training set to each document in test set.
",3.2 Baseline Methods,[0],[0]
"(2) In SVM+Ngrams, we use bag-of-unigrams and bag-of-bigrams as features and train SVM classifier with LibLinear (Fan et al., 2008)5.
",3.2 Baseline Methods,[0],[0]
"(3) In TextFeatures, we implement sophisticated features (Kiritchenko et al., 2014) including word ngrams, character ngrams, sentiment lexicon features, cluster features, et al.
5We also try discretized regression (Pang and Lee, 2005) with fixed decision thresholds (e.g. 0.5, 1.5, 2.5, ...).",3.2 Baseline Methods,[0],[0]
"However, its performance is obviously worse than SVM classifier.
(4) In AverageSG, we learn 200-dimensional word vectors with word2vec6 (Mikolov et al., 2013), average word embeddings to get document representation, and train a SVM classifier.
(5) We learn sentiment-specific word embeddings (SSWE), and use max/min/average pooling (Tang et al., 2014) to get document representation.
",3.2 Baseline Methods,[0],[0]
"(6) We compare with a state-of-the-art recommendation algorithm JMARS (Diao et al., 2014), which utilizes user and aspects of a review with collaborative filtering and topic modeling.
",3.2 Baseline Methods,[0],[0]
"(7) We implement a convolutional neural network (CNN) baseline as it is a state-of-the-art semantic composition method for sentiment analysis (Kim, 2014; Denil et al., 2014).
",3.2 Baseline Methods,[0],[0]
"(8) We implement a state-of-the-art neural network baseline Paragraph Vector (Le and Mikolov, 2014) because its codes are not officially provided.",3.2 Baseline Methods,[0],[0]
Window size is tuned on the development set.,3.2 Baseline Methods,[0],[0]
Experimental results are given in Table 2.,3.3 Comparison to Other Methods,[0],[0]
"We evaluate each dataset with two metrics, namely accuracy (higher is better) and MSE (lower is better).",3.3 Comparison to Other Methods,[0],[0]
"The best method in each dataset and each evaluation metric is in bold.
",3.3 Comparison to Other Methods,[0],[0]
"From Table 2, we can see that majority is the worst method because it does not capture any textual semantics.",3.3 Comparison to Other Methods,[0],[0]
"SVM classifiers with unigram and bigram features (Pang et al., 2002) are extremely strong, which are almost the strongest performers
6We use Skipgram as it performs slightly better than CBOW in the experiment.",3.3 Comparison to Other Methods,[0],[0]
"We also try off-the-shell word embeddings from Glove, but its performance is slightly worse than tailored word embedding from each corpus.
among all baseline methods.",3.3 Comparison to Other Methods,[0],[0]
"Designing complex features are also effective for document level sentiment classification, however, it does not surpass the bag-of-ngram features significantly as on Twitter corpora (Kiritchenko et al., 2014).",3.3 Comparison to Other Methods,[0],[0]
"Furthermore, the aforementioned bag-of-features are discrete and sparse.",3.3 Comparison to Other Methods,[0],[0]
"For example, the feature dimension of bigrams and TextFeatures on Yelp 2015 dataset are 899K and 4.81M after we filter out low frequent features.",3.3 Comparison to Other Methods,[0],[0]
"Based on them, we try to concatenate several discourse-driven features, but the classification performances remain unchanged.
",3.3 Comparison to Other Methods,[0],[0]
AverageSG,3.3 Comparison to Other Methods,[0],[0]
is a straight forward way to compose document representation without feature engineering.,3.3 Comparison to Other Methods,[0],[0]
"Unfortunately, we can see that it does not work in this scenario, which appeals for powerful semantic composition models for document level sentiment classification.",3.3 Comparison to Other Methods,[0],[0]
"We try to make better use of the sentiment information to learn a better SSWE (Tang et al., 2014), e.g. setting a large window size.",3.3 Comparison to Other Methods,[0],[0]
"However, its performance is still worse than context-based word embedding.",3.3 Comparison to Other Methods,[0],[0]
"This stems from the fact that there are many sentiment shifters (e.g. negation or contrast words) in document level reviews, while Tang et al. (2014) learn SSWE by assigning sentiment label of a text to each phrase it contains.",3.3 Comparison to Other Methods,[0],[0]
"How to learn SSWE effectively with document level sentiment supervision remains as an interesting future work.
",3.3 Comparison to Other Methods,[0],[0]
"Since JMARS outputs real-valued outputs, we only evaluate it in terms ofMSE.",3.3 Comparison to Other Methods,[0],[0]
"We can see that sophisticated baseline methods such as JMARS, paragraph vector and convolutional NN obtain significant performance boosts over AverageSG by
capturing deeper semantics of texts.",3.3 Comparison to Other Methods,[0],[0]
"Comparing between CNN and AverageSG, we can conclude that deep semantic compositionality is crucial for understanding the semantics and the sentiment of documents.",3.3 Comparison to Other Methods,[0],[0]
"However, it is somewhat disappointing that these models do not significantly outperform discrete bag-of-ngrams and bag-of-features.",3.3 Comparison to Other Methods,[0],[0]
"The reason might lie in that semantic meanings of documents, e.g. relations between sentences, are not well captured.",3.3 Comparison to Other Methods,[0],[0]
We can see that the proposed method Conv-GRNN and LSTM-GRNN yield the best performance on all four datasets in two evaluation metrics.,3.3 Comparison to Other Methods,[0],[0]
"Compared with CNN, Conv-GRNN shows its superior power in document composition component, which encodes semantics of sentences and their relations in document representation with gated recurrent neural network.",3.3 Comparison to Other Methods,[0],[0]
We also find that LSTM (almost) consistently performs better than CNN in modeling the sentence representation.,3.3 Comparison to Other Methods,[0],[0]
"As discussed before, document composition contributes a lot to the superior performance of ConvGRNN and LSTM-GRNN.",3.4 Model Analysis,[0],[0]
"Therefore, we take Conv-GRNN as an example and compare different neural models for document composition in this part.",3.4 Model Analysis,[0],[0]
"Specifically, after obtaining sentence vectors with convolutional neural network as described in Section 2.1, we carry out experiments in following settings.
",3.4 Model Analysis,[0],[0]
(1) Average.,3.4 Model Analysis,[0],[0]
"Sentence vectors are averaged to get the document vector.
(2) Recurrent / GatedNN.",3.4 Model Analysis,[0],[0]
Sentence vectors are fed to standard (or gated) recurrent neural network in a sequential way from the beginning of the input document.,3.4 Model Analysis,[0],[0]
"The last hidden vector is regarded as document representation.
",3.4 Model Analysis,[0],[0]
(3) Recurrent Avg / GatedNN,3.4 Model Analysis,[0],[0]
Avg.,3.4 Model Analysis,[0],[0]
"We extend setting (2) by averaging hidden vectors of recurrent neural network as document vector.
(4) Bi Recurrent Avg / Bi GatedNN Avg.",3.4 Model Analysis,[0],[0]
We extend setting (3) by calculating hidden vectors from both preceding histories and following contexts.,3.4 Model Analysis,[0],[0]
"Bi-directional hidden vectors are averaged as document representation.
",3.4 Model Analysis,[0],[0]
Table 3 shows the experimental results.,3.4 Model Analysis,[0],[0]
"We can see that standard recurrent neural network (RNN) is the worst method, even worse than the simple vector average.",3.4 Model Analysis,[0],[0]
"This is because RNN suffers from the vanishing gradient problem, stating that the influence of a given input on the hidden layer decays exponentially over time on the network output.",3.4 Model Analysis,[0],[0]
"In this paper, it means that document representation encodes rare semantics of the beginning sentences.",3.4 Model Analysis,[0],[0]
This is further justified by the great improvement of Recurrent Avg over Recurrent.,3.4 Model Analysis,[0],[0]
"Bi Recurrent Avg and Recurrent Avg perform comparably, but disappointingly both of them fail to transcend Average.",3.4 Model Analysis,[0],[0]
"After adding neural gates, GatedNN obtains dramatic accuracy improvements over Recurrent and significantly outperforms previous settings.",3.4 Model Analysis,[0],[0]
"The results indicate that Gated RNN is capable of handling the vanishing gradient problem to some extend, and it is practical to adaptively model sentence semantics in document representation. GatedNN",3.4 Model Analysis,[0],[0]
Avg and Bi GatedNN,3.4 Model Analysis,[0],[0]
Avg obtains comparable performances with GatedNN.,3.4 Model Analysis,[0],[0]
"Document level sentiment classification is a fundamental problem in sentiment analysis (Pang and Lee, 2008; Liu, 2012), which aims at identifying the sentiment label of a document (Pang et al., 2002; Turney, 2002).",4 Related Work,[0],[0]
"Pang and Lee (2002; 2005)
cast this problem as a classification task, and use machine learning method in a supervised learning framework.",4 Related Work,[0],[0]
Turney (2002) introduces an unsupervised approach by using sentiment words/phrases extracted from syntactic patterns to determine the document polarity.,4 Related Work,[0],[0]
"Goldberg and Zhu (2006) place this task in a semi-supervised setting, and use unlabelled reviews with graph-based method.",4 Related Work,[0],[0]
Dominant studies in literature follow Pang et al. (2002) and work on designing effective features for building a powerful sentiment classifier.,4 Related Work,[0],[0]
"Representative features include word ngrams (Wang and Manning, 2012), text topic (Ganu et al., 2009), bag-of-opinions (Qu et al., 2010), syntactic relations (Xia and Zong, 2010), sentiment lexicon features (Kiritchenko et al., 2014).
",4 Related Work,[0],[0]
"Despite the effectiveness of feature engineering, it is labor intensive and unable to extract and organize the discriminative information from data (Bengio et al., 2015).",4 Related Work,[0],[0]
"Recently, neural network emerges as an effective way to learn continuous text representation for sentiment classification.",4 Related Work,[0],[0]
Existing studies in this direction can be divided into two groups.,4 Related Work,[0],[0]
One line of research focuses on learning continuous word embedding.,4 Related Work,[0],[0]
"Traditional embedding learning algorithms typically leverage contexts of words in a context-prediction way (Bengio et al., 2003; Mikolov et al., 2013; Baroni et al., 2014).",4 Related Work,[0],[0]
"Since these methods typically map words with similar contexts but opposite polarity (e.g. “good” and “bad”) to neighboring vectors, several studies (Maas et al., 2011; Labutov and Lipson, 2013; Tang et al., 2014) learn sentiment-specific word embeddings by taking sentiment of texts into account.",4 Related Work,[0],[0]
"Another line of research concentrates on semantic composition (Mitchell and Lapata, 2010).",4 Related Work,[0],[0]
Yessenalina and Cardie (2011) represent each word as a matrix and use iterated matrix multiplication as phrase-level composition function.,4 Related Work,[0],[0]
Socher et al. (2013b) introduce a family of recursive neural networks for sentence-level semantic composition.,4 Related Work,[0],[0]
"Recursive neural network is extended with global feedbackward (Paulus et al., 2014), feature weight tuning (Li, 2014), deep recursive layer (Irsoy and Cardie, 2014), adaptive composition functions (Dong et al., 2014), combined with Combinatory Categorial Grammar (Hermann and Blunsom, 2013), and used for opinion relation detection (Xu et al., 2014).",4 Related Work,[0],[0]
Glorot et al. (2011) use stacked denoising autoencoder.,4 Related Work,[0],[0]
"Convolutional neural networks are widely used for semantic compo-
sition (Kim, 2014; Kalchbrenner et al., 2014; Denil et al., 2014; Johnson and Zhang, 2015) by automatically capturing local and global semantics.",4 Related Work,[0],[0]
Le and Mikolov (2014) introduce Paragraph Vector to learn document representation from semantics of words.,4 Related Work,[0],[0]
"Sequential model like recurrent neural network or long short-term memory (LSTM) are also verified as strong approaches for semantic composition (Li et al., 2015a).
",4 Related Work,[0],[0]
"In this work, we represent document with convolutional-gated recurrent neural network, which adaptively encodes semantics of sentences and their relations.",4 Related Work,[0],[0]
"A recent work in (Li et al., 2015b) also investigate LSTM to model document meaning.",4 Related Work,[0],[0]
They verify the effectiveness of LSTM in text generation task.,4 Related Work,[0],[0]
We introduce neural network models (ConvGRNN and LSTM-GRNN) for document level sentiment classification.,5 Conclusion,[0],[0]
"The approach encodes semantics of sentences and their relations in document representation, and is effectively trained end-to-end with supervised sentiment classification objectives.",5 Conclusion,[0],[0]
We conduct extensive experiments on four review datasets with two evaluation metrics.,5 Conclusion,[0],[0]
Empirical results show that our approaches achieve state-of-the-art performances on all these datasets.,5 Conclusion,[0],[0]
"We also find that (1) traditional recurrent neural network is extremely weak in modeling document composition, while adding neural gates dramatically boosts the performance, (2) LSTM performs better than a multi-filtered CNN in modeling sentence representation.
",5 Conclusion,[0],[0]
We briefly discuss some future plans.,5 Conclusion,[0],[0]
How to effectively compose sentence meanings to document meaning is a central problem in natural language processing.,5 Conclusion,[0],[0]
"In this work, we develop neural models in a sequential way, and encode sentence semantics and their relations automatically without using external discourse analysis results.",5 Conclusion,[0],[0]
"From one perspective, one could carefully define a set of sentiment-sensitive discourse relations (Zhou et al., 2011), such as “contrast”, “condition”, “cause”, etc.",5 Conclusion,[0],[0]
"Afterwards, relation-specific gated RNN can be developed to explicitly model semantic composition rules for each relation (Socher et al., 2013a).",5 Conclusion,[0],[0]
"However, defining such a relation scheme is linguistic driven and time consuming, which we leave as future work.",5 Conclusion,[0],[0]
"From another perspective, one could compose document
representation over discourse tree structures rather than in a sequential way.",5 Conclusion,[0],[0]
"Accordingly, Recursive Neural Network (Socher et al., 2013b) and Structured LSTM (Tai et al., 2015; Zhu et al., 2015) can be used as composition algorithms.",5 Conclusion,[0],[0]
"However, existing discourse structure learning algorithms are difficult to scale to massive review texts on the web.",5 Conclusion,[0],[0]
How to simultaneously learn document structure and composition function is an interesting future work.,5 Conclusion,[0],[0]
The authors give great thanks to Yaming Sun and Jiwei Li for the fruitful discussions.,Acknowledgments,[0],[0]
We also would like to thank three anonymous reviewers for their valuable comments and suggestions.,Acknowledgments,[0],[0]
"This work was supported by the National High Technology Development 863 Program of China (No. 2015AA015407), National Natural Science Foundation of China (No. 61133012 and No. 61273321).",Acknowledgments,[0],[0]
Duyu Tang is supported by Baidu Fellowship and IBM Ph.D. Fellowship.,Acknowledgments,[0],[0]
Document level sentiment classification remains a challenge: encoding the intrinsic relations between sentences in the semantic meaning of a document.,abstractText,[0],[0]
"To address this, we introduce a neural network model to learn vector-based document representation in a unified, bottom-up fashion.",abstractText,[0],[0]
The model first learns sentence representation with convolutional neural network or long short-term memory.,abstractText,[0],[0]
"Afterwards, semantics of sentences and their relations are adaptively encoded in document representation with gated recurrent neural network.",abstractText,[0],[0]
We conduct document level sentiment classification on four large-scale review datasets from IMDB and Yelp Dataset Challenge.,abstractText,[0],[0]
Experimental results show that: (1) our neural model shows superior performances over several state-of-the-art algorithms; (2) gated recurrent neural network dramatically outperforms standard recurrent neural network in document modeling for sentiment classification.1,abstractText,[0],[0]
Document Modeling with Gated Recurrent Neural Network for Sentiment Classification,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2044–2054 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Document-level sentiment classification is one of the pragmatical sentiment analysis tasks (Pang and Lee, 2007; Liu, 2010).",1 Introduction,[0],[0]
"There are many Web sites having platforms for users to input reviews over products or services, such as TripAdvisor, Yelp, Amazon, etc.",1 Introduction,[0],[0]
Most of reviews are very comprehensive and thus long documents.,1 Introduction,[0],[0]
Analyzing these documents to predict ratings of products or services is an important complementary way for better customer relationship management.,1 Introduction,[0],[0]
"Recently, neural network based approaches have been developed and become state-of-the-arts for longdocument sentiment classification (Tang et al., 2015a,b; Yang et al., 2016).",1 Introduction,[0],[0]
"However, predicting an overall score for each long document is not enough, because the document can mention dif-
ferent aspects of the corresponding product or service.",1 Introduction,[0],[0]
"For example, in Figure 1, there could be different aspects for a review of hotel.",1 Introduction,[0],[0]
These aspects help customer service better understand what are the major pros and cons of the product or service.,1 Introduction,[0],[0]
"Compared to the overall rating, users are less motivated to give aspect ratings.",1 Introduction,[0],[0]
"Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.
",1 Introduction,[0],[0]
"One straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997).",1 Introduction,[0],[0]
"For neural networks, we can simply treat each aspect (e.g., rating from one to five) as a classification task, and let different tasks use softmax classifier to extract task-specific representations at the top layer while share the input and hidden layers to mutually enhance the prediction results (Collobert et al., 2011; Luong et al., 2016).",1 Introduction,[0],[0]
"However, such approach ignores the fact that the aspects themselves have semantic meanings.",1 Introduction,[0],[0]
"For example, as human beings, if we were asked to evaluate the aspect rating of a document, we simply read the review, and find aspect-related keywords, and see around comments.",1 Introduction,[0],[0]
"Then, we aggregate all the related snippets to make a decision.
",1 Introduction,[0],[0]
"In this paper, we propose a novel approach to treat document-level multi-aspect sentiment clas-
2044
sification as a machine comprehension (Kumar et al., 2016; Sordoni et al., 2016) problem.",1 Introduction,[0],[0]
"To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect.",1 Introduction,[0],[0]
"For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc.",1 Introduction,[0],[0]
Then we can ask pseudo questions: “How is the room?”,1 Introduction,[0],[0]
“How is the bed?”,1 Introduction,[0],[0]
“How is the view?” and provide an answer “Rating 5.”,1 Introduction,[0],[0]
"In this case, we can train a machine comprehension model to automatically attend corresponding text snippets in the review document to predict the aspect rating.",1 Introduction,[0],[0]
"Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations.",1 Introduction,[0],[0]
We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions.,1 Introduction,[0],[0]
"At each level, the model consists of input encoders and iterative attention modules.",1 Introduction,[0],[0]
The input encoder learns memories1 of documents and questions with Bi-directional LSTM (Bi-LSTM) model and non-linear mapping respectively.,1 Introduction,[0],[0]
"The iterative attention module takes into memories as input and attends them sequentially with a multiple hop mechanism, performing effective interactions between documents and aspect questions.
",1 Introduction,[0],[0]
"To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines.",1 Introduction,[0],[0]
"We also analyze the effects of num-
1Following the work (Weston et al., 2015; Sukhbaatar et al., 2015), we refer the memory to a set vectors which are stacked together and could be attended.
",1 Introduction,[0],[0]
bers of the hop and aspect words on performances.,1 Introduction,[0],[0]
"Moreover, a case study for attention results is performed at both word and sentence levels.
",1 Introduction,[0],[0]
The contributions of this paper are two-fold.,1 Introduction,[0],[0]
"First, we study the document-level multi-aspect sentiment classification as a machine comprehension problem and introduce a hierarchical iterative attention model for it.",1 Introduction,[0],[0]
"Second, we demonstrate the effectiveness of proposed model on two datasets, showing that our model outperforms classical baselines.",1 Introduction,[0],[0]
The code and data for this paper are available at https://github.com/ HKUST-KnowComp/DMSCMC.,1 Introduction,[0],[0]
"In this section, we introduce our proposed method.",2 Method,[0],[0]
We first briefly introduce the problem we work on.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Given a piece of review, our task is to predict the ratings of different aspects.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For example, in Figure 1, we predict the ratings of Cleanliness, Room, and Value.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"To achieve this, we assume that there are existing reviews with aspect ratings for machines to learn.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Formally, we denote the review document as d containing a set of Td sentences {s1, s2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
sTd}.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For the t-th sentence st, we use a set of words { w1, w2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"w|st| } to represent it, and use wi, wwi and w p",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"i as the one-hot encoding, word embedding, and phrase embedding for wi respectively.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"The phrase embedding encodes the semantics of phrases where the current word wi is the center (e.g., hidden vectors learned by Bi-LSTM shown in Section 2.2).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"For each qk of K aspects
{q1, q2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
", qK}, we use Nk aspect-related keywords, { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"qkNk } , to represent it.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Similarly, we use qki , q w ki
as the one-hot encoding and word embedding for qki respectively.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"There are several sophisticated methods for choosing aspect keywords (e.g., topic model).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Here, we consider a simple way where five seeds were first manually selected for each aspect and then more words were obtained based on their cosine similarities with seeds2
As shown in Figure 2 (left), our framework follows the idea of multi-task learning, which learns different aspects simultaneously.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"In this case, all these tasks share the representations of words and architecture of semantic model for the final classifiers.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Different from straightforward neural network based multi-task learning (Collobert et al., 2011), for each document d and an aspect qk, our model uses both the content of d and all the related keywords { qk1 , qk2 . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
qkNk } as input.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Since the keywords can cover most of the semantic meanings of the aspect, and we do not know which document mentions which semantic meaning, we build an attention model to automatically decide it (introduced in Section 2.3).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Assuming that the keywords have been decided, we use a hierarchical attention model to select useful information from the review documents.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"As shown in Figure 2 (right), the hierarchical attention of keywords is applied to both sentence level (to select meaningful words) and document level (to select meaningful sentence).",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Thus, our model builds aspectspecific representations in a bottom-up manner.
",2.1 Problem Definition and Hierarchical Framework,[0],[0]
"Specifically, we obtain sentence representations { sk1, sk2, . . .",2.1 Problem Definition and Hierarchical Framework,[0],[0]
skT } using the input encoder (Section 2.2) and iterative attention module (Section 2.3) at the word level.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification.,2.1 Problem Definition and Hierarchical Framework,[0],[0]
"As shown in Figure 2 (right), the attention model is applied twice at different levels of the representation.",2.1 Problem Definition and Hierarchical Framework,[0],[0]
The input module builds memory vectors for the iterative attention module and is performed both at word and sentence levels.,2.2 Input Encoder,[0],[0]
"For a document, it con-
2For example, the words “value,” “price,” “worth,” “cost,” and “$” are selected as seeds for aspect Price.",2.2 Input Encoder,[0],[0]
"The information for seeds can be found in our released resource.
",2.2 Input Encoder,[0],[0]
verts word sequence into word level memory Mdw and sentence sequence into sentence level memory Mds respectively.,2.2 Input Encoder,[0],[0]
"For an aspect question qk, it takes a set of aspect-specific words {qki}1≤i≤Nk as input and derives word level memory Mqw and sentence level memory Mqs.
To construct Mdw, we obtain word embeddings{ ww1 , ww2 , . . .",2.2 Input Encoder,[0],[0]
ww|st| } from an embedding matrix EA applied to all words shown in the corpus.,2.2 Input Encoder,[0],[0]
"Then, LSTM (Hochreiter and Schmidhuber, 1997) model is used as the encoder to produce hidden vectors of words based on the word embeddings.",2.2 Input Encoder,[0],[0]
"At each step, LSTM takes input wwt and derives a new hidden vector by ht = LSTM(wwt , ht−1).",2.2 Input Encoder,[0],[0]
"To preserve the subsequent context information for words, another LSTM is ran over word sequence in a reverse order simultaneously.",2.2 Input Encoder,[0],[0]
"Then the forward hidden vector −→ h t and backward hidden
vector ←− h t are concatenated as phrase embedding wpt .",2.2 Input Encoder,[0],[0]
We stack these phrase embeddings together as word level memory Mdw.,2.2 Input Encoder,[0],[0]
"Similarly, we feed sentence representations into another Bi-LSTM to derive the sentence level memory Mds .",2.2 Input Encoder,[0],[0]
"Note that, the sentence representations are obtained using the iterative attention module which is described as Eq. (5) in Section 2.3.
",2.2 Input Encoder,[0],[0]
"Since we have question keywords as input, to allow the interactions between questions and documents, we also build question memory in following way.",2.2 Input Encoder,[0],[0]
"We obtain Qk = { qwki }
1≤i≤Nk by looking up an embedding matrix 3 EB applied to all question keywords.",2.2 Input Encoder,[0],[0]
"Then a non-linear mapping is applied to obtain the question memory at word level:
Mqkw = tanh(QkW q w), (1)
where Wqw is the parameter matrix to adapt qk at word level.",2.2 Input Encoder,[0],[0]
"Similarly, we use another mapping to obtain the sentence level memory:
Mqks = tanh(QkW q s), (2)
where Wqs is the parameter matrix to adapt qk at sentence level.",2.2 Input Encoder,[0],[0]
"The iterative attention module (IAM) attends and reads memories of questions and documents alternatively with a multi-hop mechanism, deriving
3EA and EB are initialized by the same pre-trained embeddings but are different embedding matrices with different updates.
aspect-specific sentence and document representations.",2.3 Iterative Attention Module,[0],[0]
"As we discussed in the introduction, the set of selected question keywords may not best characterize the aspect for different documents.",2.3 Iterative Attention Module,[0],[0]
"Thus, the IAM module introduces a backward attention to use document information (word or sentence) to select useful keywords of each aspect as the document-specific question to build attention model.
",2.3 Iterative Attention Module,[0],[0]
The illustration of IAM is shown in Figure 3.,2.3 Iterative Attention Module,[0],[0]
"To obtain sentence representations, it takes Mdw and Mqw as the input and performs m iterations (hops).",2.3 Iterative Attention Module,[0],[0]
"For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vectors into a single vector q̂; (2) updates the selective vector by the previous one and q̂; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vector ĉ; (4) updates the selective vector by the previous one and ĉ.
We unify operations (1) and (3) by an attention function x̂ = A(p, M), where M could be Mdw or Mqw which corresponds x̂ = ĉ or x̂ = q̂.",2.3 Iterative Attention Module,[0],[0]
"The attention function A is decomposed as:
H = tanh(MWa (1p))",2.3 Iterative Attention Module,[0],[0]
"a = softmax(HvTa )
",2.3 Iterative Attention Module,[0],[0]
x̂,2.3 Iterative Attention Module,[0],[0]
"= ∑ aiMi,
(3)
where 1 is a vector with all elements are 1, which copies the selective vector to meet the dimension requirement.",2.3 Iterative Attention Module,[0],[0]
"The Wa and va are parameters, a is attention weights for memory vectors, and Mi
means i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively.",2.3 Iterative Attention Module,[0],[0]
We initialize p0 by a zero vector.,2.3 Iterative Attention Module,[0],[0]
"The update function U can be a recurrent neural network (Xiong et al., 2017) or other heuristic weighting functions.",2.3 Iterative Attention Module,[0],[0]
"In this paper, we introduce a simple strategy:
p2i−{l} = x̂, (4)
which ignores the previous selective vector but succeeds to obtain comparable results with other more complicated function in the initial experiments.
",2.3 Iterative Attention Module,[0],[0]
"Multi-hop mechanism attends different memory locations in different hops (Sukhbaatar et al., 2015), capturing different interactions between documents and questions.",2.3 Iterative Attention Module,[0],[0]
"In order to preserve the information of various kinds of interactions, we concatenate all ĉ’s in each hop as the final representations of sentences:
s = [ĉ1; ĉ2; · · · ĉm].",2.3 Iterative Attention Module,[0],[0]
"(5) After obtaining sentence representations, we feed them into the sentence-level input encoder, deriving the memories Mds and Mqs.",2.3 Iterative Attention Module,[0],[0]
"Then, the aspect-specific document representation dk is obtained by the sentence-level IAM in a similar way.",2.3 Iterative Attention Module,[0],[0]
"For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K .",2.4 Objective Function,[0],[0]
"All these representations are fed into classifiers, each of which includes a softmax layer.",2.4 Objective Function,[0],[0]
"The softmax layer outputs the probability distribution over |Y| categories for the distributed representation, which is defined as:
p′(d, k) = softmax(Wclassk dk), (6)
where Wclassk is the parameter matrix.",2.4 Objective Function,[0],[0]
"We define the cross-entropy objective function between gold sentiment distribution p(d, k) and predicted sentiment distribution p′(d, k) as the classification loss function:
− ∑ d∈D K∑ k=1",2.4 Objective Function,[0],[0]
|Y|∑ i=1,2.4 Objective Function,[0],[0]
"p(d, k)log(p′(d, k)), (7)
where p(d, k) is a one-hot vector, which has the same dimension as the number of classes, and only the dimension associated with the ground truth label is one, with others being zeros.",2.4 Objective Function,[0],[0]
"In this section, we show experimental results to demonstrate our proposed algorithm.",3 Experiment,[0],[0]
"We conduct our experiments on TripAdvisor (Wang et al., 2010) and BeerAdvocate (McAuley et al., 2012; Lei et al., 2016) datasets, which contain seven aspects (value, room, location, cleanliness, check in/front desk, service, and business service) and four aspects (feel, look, smell, and taste) respectively.",3.1 Datasets,[0],[0]
"We follow the processing step (Lei et al., 2016) by choosing the reviews with different aspect ratings and the new datasets are described in Table 1.",3.1 Datasets,[0],[0]
"We tokenize the datasets by Stanford corenlp4 and randomly split them into training, development, and testing sets with 80/10/10%.",3.1 Datasets,[0],[0]
"To demonstrate the effectiveness of the proposed method, we compare our model with following baselines:
Majority uses the majority sentiment label in development sets as the predicted label.
",3.2 Baseline Methods,[0],[0]
"SVM uses unigram and bigram as text features and uses Liblinear (Fan et al., 2008) for learning.
",3.2 Baseline Methods,[0],[0]
"SLDA refers to supervised latent Dirichlet allocation (Blei and Mcauliffe, 2010) which is a statistical model of labeled documents.
",3.2 Baseline Methods,[0],[0]
"NBoW is a neural bag-of-words model averaging embeddings of all words in a document and feeds the resulted embeddings into SVM classifier.
",3.2 Baseline Methods,[0],[0]
DAN is a deep averaging network model which consists of several fully connected layers with averaged word embeddings as input.,3.2 Baseline Methods,[0],[0]
"One novel word dropout strategy is employed to boost model performances (Iyyer et al., 2015).
",3.2 Baseline Methods,[0],[0]
"CNN continuously performs a convolution operation over a sentence to extract words neighboring features, then gets a fixed-sized representation by a pooling layer (Kim, 2014).
",3.2 Baseline Methods,[0],[0]
"4http://nlp.stanford.edu/software/corenlp.shtml
LSTM is one variant of recurrent neural network and has been proved to be one of state-ofthe-art models for document-level sentiment classification (Tang et al., 2015a).",3.2 Baseline Methods,[0],[0]
"We use LSTM to refer Bi-LSTM which captures both forward and backward semantic information.
",3.2 Baseline Methods,[0],[0]
"HAN means the hierarchical attention network which is proposed in (Yang et al., 2016) for document classification.",3.2 Baseline Methods,[0],[0]
"Note that, the original HAN depends GRU as the encoder.",3.2 Baseline Methods,[0],[0]
"In our experiments, LSTM-based HAN obtains slightly better results.",3.2 Baseline Methods,[0],[0]
"Thus, we report the results of HAN with LSTM as the encoder.
",3.2 Baseline Methods,[0],[0]
"We extend DAN, CNN, LSTM with the hierarchical architecture and multi-task framework, the corresponding models are MHDAN, MHCNN and MHLSTM respectively.",3.2 Baseline Methods,[0],[0]
"Besides, MHAN is also evaluated as one baseline, which is HAN with the multi-task learning.",3.2 Baseline Methods,[0],[0]
"We implement all neural models using Theano (Theano Development Team, 2016).",3.3 Implementation Details,[0],[0]
The model parameters are tuned based on the development sets.,3.3 Implementation Details,[0],[0]
"We learn 200-dimensional word embeddings with Skip-gram model (Mikolov et al., 2013) on in-domain corpus, which follows (Tang et al., 2015a).",3.3 Implementation Details,[0],[0]
The pre-trained word embeddings are used to initialize the embedding matrices EA and EB .,3.3 Implementation Details,[0],[0]
The dimensions of all hidden vectors are set to 200.,3.3 Implementation Details,[0],[0]
"For TripAdvisor dataset, the hop numbers of word-level and sentence-level iterative attention modules are set to 4 and 2 respectively.",3.3 Implementation Details,[0],[0]
"For BeerAdvocate dataset, the hop numbers are set to 6 and 2.",3.3 Implementation Details,[0],[0]
The number of selected keywords Nk = N is set to 20.,3.3 Implementation Details,[0],[0]
"To avoid model over-fitting, we use dropout and regularization as follows: (1) the regularization parameter is set to 1e-5; (2) the dropout rate is set to 0.3, which is applied to both sentence and document vectors.",3.3 Implementation Details,[0],[0]
"All parameters are trained by ADADELTA (Zeiler, 2012) without needing to set the initial learning rate.",3.3 Implementation Details,[0],[0]
"To ensure fair comparisons, we make baselines have same settings as the proposed model, such as word embeddings, dimensions of hidden vectors and optimization details and so on.",3.3 Implementation Details,[0],[0]
"We use accuracy and mean squared error (MSE) as the evaluation metrics and the results are shown in Table 2.
",3.4 Results and Analyses,[0],[0]
"Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets.",3.4 Results and Analyses,[0],[0]
All neural network models outperform NBoW.,3.4 Results and Analyses,[0],[0]
"It shows the advantages of neural networks in the document sentiment classification.
",3.4 Results and Analyses,[0],[0]
"From the results of neural networks, we can observe that DAN performs worse than LSTM and CNN, and LSTM achieves slightly higher results than CNN.",3.4 Results and Analyses,[0],[0]
"It can be explained that the simple composition method averaging embeddings of words in a document but ignoring word order, may not be as effective as other flexible composition models, such as LSTM and CNN, on aspect classification.",3.4 Results and Analyses,[0],[0]
"Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks.",3.4 Results and Analyses,[0],[0]
"Among all baselines, MHAN and MHLSTM achieve comparable results and outperform others.
",3.4 Results and Analyses,[0],[0]
"Compared with MHAN and MHLSTM, our method achieves improvements of 1.5% (3% relative improvement) and 1.0% (2.5% relative improvement) on TripAdvisor and BeerAdvocate re-
spectively, which shows that the incorporation of iterative attention mechanism helps the deep neural network based model build up more discriminative aspect-aware representation.",3.4 Results and Analyses,[0],[0]
Note that BeerAdvocate is relatively more difficult since the predicted ratings are from 1 to 10 while TripAdvisor is 1 to 5.,3.4 Results and Analyses,[0],[0]
"Moreover, t-test is conducted by randomly splitting datasets into train/dev/test sets and random initialization.",3.4 Results and Analyses,[0],[0]
The results on test sets are described in Table 3 which show performance of our model is stable.,3.4 Results and Analyses,[0],[0]
"In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study.",3.5 Case Study for Attention Results,[0],[0]
Both word-level and sentence-level attention visualizations are shown in Figure 4.,3.5 Case Study for Attention Results,[0],[0]
"We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.
",3.5 Case Study for Attention Results,[0],[0]
"From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect.",3.5 Case Study for Attention Results,[0],[0]
"For example, in the first sentence, the words comfortable and bed are assigned higher
weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness.",3.5 Case Study for Attention Results,[0],[0]
"In the second sentence, the word internet is assigned a high attention value for Business.",3.5 Case Study for Attention Results,[0],[0]
"Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable.",3.5 Case Study for Attention Results,[0],[0]
"Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops.",3.5 Case Study for Attention Results,[0],[0]
"In the second sentence, we observe that the value of word internet increases with the number of hop.",3.5 Case Study for Attention Results,[0],[0]
"Thus, we can see that the more sensible weights are obtained for words through the proposed iterative attention mechanism.",3.5 Case Study for Attention Results,[0],[0]
"Similarly, the figures (c) and (d) show that the conclusion from words is also suitable for sentences.",3.5 Case Study for Attention Results,[0],[0]
"For the first sentence, the sentence weight regarding the aspect Room is lower than Cleanliness in the first hop, but surpasses Cleanliness in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"For the second sentence, the weight for Business becomes higher in the second hop.",3.5 Case Study for Attention Results,[0],[0]
"In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"All the experiments are conducted
on the development set.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Due to lack of space, we only present the results of TripAdvisor and the results of BeerAdvocate have a similar behavior as TripAdvisor.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left).",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Moreover, we can see that the hop number of word level leads to larger variation than the hop number of sentence level.
",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"For the size of aspect keywords, we vary N from 0 to 35, incremented by 5.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
"Note that, we set a learnable vector to represent question memory when N = 0.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
The results are shown in Figure 5 (right).,3.6 Effects of Hop and Aspect Keywords,[0],[0]
"We observe that the performance increases when N ≤ 20, and has no improvement after N > 20.",3.6 Effects of Hop and Aspect Keywords,[0],[0]
This indicates that a small number of keywords can help the proposed model achieve competitive results.,3.6 Effects of Hop and Aspect Keywords,[0],[0]
Multi-Aspect Sentiment Classification.,4 Related Work,[0],[0]
Multiaspect sentiment classification has been studied extensively in literature.,4 Related Work,[0],[0]
"Lu et al. (2011) used support vector regression model based on hand-
crafted features to predict aspect ratings.",4 Related Work,[0],[0]
"To handle the correlation between aspects, McAuley et al. (2012) added a dependency term in final multi-class SVM objective.",4 Related Work,[0],[0]
"There were also some heuristic based methods and sophisticated topic models where multi-aspect sentiment classification is solved as a subproblem (Titov and McDonald, 2008; Wang et al., 2010; Diao et al., 2014; Pappas and Popescu-Belis, 2014).",4 Related Work,[0],[0]
"However, these approaches often rely on strict assumptions about words and sentences, for example, using the word syntax to determine if a word is an aspect or a sentiment word, or relating a sentence with an specific aspect.",4 Related Work,[0],[0]
"Another related problem is called aspect-based sentiment classification (Pontiki et al., 2014, 2016; Poria et al., 2016), which first extracts aspect expressions from sentences (Poria et al., 2014; Balahur and Montoyo, 2008; Chen et al., 2014, 2013), and then determines their sentiments.",4 Related Work,[0],[0]
"With the developments of neural networks and word embeddings in NLP, neural network based models have shown the state-of-the-art results with less feature engineering work.",4 Related Work,[0],[0]
Tang et al. (2016) employed a deep memory network for aspect-based sentiment classification given the aspect location and Lakkaraju et al. (2014) employed recurrent neural networks and its variants for the task of extraction of aspectsentiment pair.,4 Related Work,[0],[0]
"However, these tasks are sentencelevel.",4 Related Work,[0],[0]
Another related research field is documentlevel sentiment classification because we can treat single aspect sentiment classification as an individual document classification task.,4 Related Work,[0],[0]
"This line of research includes (Tang et al., 2015b; Chen et al., 2016; Tang et al., 2016; Yang et al., 2016) which are based on neural networks in a hierarchical structure.",4 Related Work,[0],[0]
"However, they did not work on multiple aspects.
",4 Related Work,[0],[0]
Machine Comprehension.,4 Related Work,[0],[0]
"Recently, neural network based machine comprehension (or reading) has been studied extensively in NLP, with the releases of large-scale evaluation datasets (Hermann et al., 2015; Hill et al., 2016; Rajpurkar et al., 2016).",4 Related Work,[0],[0]
"Most of the related studies focus on attention mechanism (Bahdanau et al., 2014) which is firstly proposed in machine translating and aims to solve the long-distance dependency between words.",4 Related Work,[0],[0]
"Hermann et al. (2015) used BiLSTM to encode document and query, and proposed Attentive Reader and Impatient Reader.",4 Related Work,[0],[0]
"The first one attends document based on the query representation, and the second one attends document by the representation of each token in query with an incremental manner.",4 Related Work,[0],[0]
"Memory Networks (Weston et al., 2015; Sukhbaatar et al., 2015) attend and reason document representation in a multihop fashion, enriching interactions between documents and questions.",4 Related Work,[0],[0]
"Dynamic Memory Network (Kumar et al., 2016) updates memories of documents by re-running GRU models based on derived attention weights.",4 Related Work,[0],[0]
"Meanwhile, the query representation is refined by another GRU model.",4 Related Work,[0],[0]
"Gated-Attention Reader (Dhingra et al., 2016) proposes a novel attention mechanism, which is based on multiplicative interactions between the query embeddings and the intermediate states of a recurrent neural network document reader.",4 Related Work,[0],[0]
"BiDirectional Attention Model (Xiong et al., 2017; Seo et al., 2017) fuses co-dependent representations of queries and documents in order to focus on relevant parts of both.",4 Related Work,[0],[0]
"Iterative Attention model (Sordoni et al., 2016) attends question and document sequentially, which is related to our model.",4 Related Work,[0],[0]
"Different from Iterative Attention model, our model focuses on the document-level multiaspect sentiment classification, which is proposed
in a hierarchical architecture and has different procedures in the iterative attention module.",4 Related Work,[0],[0]
Another related research problem is visual question answering which uses an image as question context rather than a set of keywords as question.,4 Related Work,[0],[0]
"Neural network based visual question answering (Lu et al., 2016; Xiong et al., 2016) is similar as the proposed models in text comprehension.",4 Related Work,[0],[0]
"In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model.",5 Conclusion,[0],[0]
Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture.,5 Conclusion,[0],[0]
This paper is partially supported by the National Natural Science Foundation of China (NSFC Grant Nos. 61472006 and 91646202) as well as the National Basic Research Program (973 Program No. 2014CB340405).,6 Acknowledgments,[0],[0]
"This work was also supported by NVIDIA Corporation with the donation of the Titan X GPU, Hong Kong CERG Project 26206717, China 973 Fundamental R&D Program (No.2014CB340304), and the LORELEI Contract HR0011-15-2-0025 with DARPA.",6 Acknowledgments,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the Department of Defense or the U.S. Government.,6 Acknowledgments,[0],[0]
We also thank the anonymous reviewers for their valuable comments and suggestions that help improve the quality of this manuscript.,6 Acknowledgments,[0],[0]
Document-level multi-aspect sentiment classification is an important task for customer relation management.,abstractText,[0],[0]
"In this paper, we model the task as a machine comprehension problem where pseudo questionanswer pairs are constructed by a small number of aspect-related keywords and aspect ratings.",abstractText,[0],[0]
A hierarchical iterative attention model is introduced to build aspectspecific representations by frequent and repeated interactions between documents and aspect questions.,abstractText,[0],[0]
"We adopt a hierarchical architecture to represent both word level and sentence level information, and use the attention operations for aspect questions and documents alternatively with the multiple hop mechanism.",abstractText,[0],[0]
Experimental results on the TripAdvisor and BeerAdvocate datasets show that our model outperforms classical baselines.,abstractText,[0],[0]
Document-Level Multi-Aspect Sentiment Classification as Machine Comprehension,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2911–2916 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"A key assumption made by classic supervised text classification (or learning) is that classes appeared in the test data must have appeared in training, called the closed-world assumption (Fei and Liu, 2016; Chen and Liu, 2016).",1 Introduction,[0],[0]
"Although this assumption holds in many applications, it is violated in many others, especially in dynamic or open environments.",1 Introduction,[0],[0]
"For example, in social media, a classifier built with past topics or classes may not be effective in classifying future data because new topics appear constantly in social media (Fei et al., 2016).",1 Introduction,[0],[0]
"This is clearly true in other domains too, e.g., self-driving cars, where new objects may appear in the scene all the time.
",1 Introduction,[0],[0]
"Ideally, in the text domain, the classifier should classify incoming documents to the right existing classes used in training and also detect those documents that don’t belong to any of the existing classes.",1 Introduction,[0],[0]
"This problem is called open world classification or open classification (Fei and Liu, 2016).",1 Introduction,[0],[0]
"Such a classifier is aware what it does and does
not know.",1 Introduction,[0],[0]
"This paper proposes a novel technique to solve this problem.
",1 Introduction,[0],[0]
Problem Definition:,1 Introduction,[0],[0]
"Given the training data D = {(x1, y1), (x2, y2), . . .",1 Introduction,[0],[0]
", (xn, yn)}, where xi is the i-th document, and yi ∈ {l1, l2, . . .",1 Introduction,[0],[0]
", lm} = Y is xi’s class label, we want to build a model f(x) that can classify each test instance x to one of them training or seen classes inY or reject it to indicate that it does not belong to any of them training or seen classes, i.e., unseen.",1 Introduction,[0],[0]
"In other words, we want to build a (m + 1)-class classifier f(x) with the classes C = {l1, l2, . . .",1 Introduction,[0],[0]
", lm, rejection}.
",1 Introduction,[0],[0]
There are some prior approaches for open classification.,1 Introduction,[0],[0]
"One-class SVM (Schölkopf et al., 2001; Tax and Duin, 2004) is the earliest approach.",1 Introduction,[0],[0]
"However, as no negative training data is used, oneclass classifiers work poorly.",1 Introduction,[0],[0]
"Fei and Liu (2016) proposed a Center-Based Similarity (CBS) space learning method (Fei and Liu, 2015).",1 Introduction,[0],[0]
This method first computes a center for each class and transforms each document to a vector of similarities to the center.,1 Introduction,[0],[0]
A binary classifier is then built using the transformed data for each class.,1 Introduction,[0],[0]
The decision surface is like a “ball” encircling each class.,1 Introduction,[0],[0]
Everything outside the ball is considered not belonging to the class.,1 Introduction,[0],[0]
Our proposed method outperforms this method greatly.,1 Introduction,[0],[0]
"Fei et al. (2016) further added the capability of incrementally or cumulatively learning new classes, which connects this work to lifelong learning (Chen and Liu, 2016) because without the ability to identify novel or new things and learn them, a system will never be able to learn by itself continually.
",1 Introduction,[0],[0]
"In computer vision, Scheirer et al. (2013) studied the problem of recognizing unseen images that the system was not trained for by reducing open space risk.",1 Introduction,[0],[0]
The basic idea is that a classifier should not cover too much open space where there are few or no training data.,1 Introduction,[0],[0]
"They proposed to reduce the half-space of a binary SVM classifier
2911
with a positive region bounded by two parallel hyperplanes.",1 Introduction,[0],[0]
Similar works were also done in a probability setting by Scheirer et al. (2014) and Jain et al. (2014).,1 Introduction,[0],[0]
"Both approaches use probability threshold, but choosing thresholds need prior knowledge, which is a weakness of the methods.",1 Introduction,[0],[0]
Dalvi et al. (2013) proposed a multi-class semisupervised method based on the EM algorithm.,1 Introduction,[0],[0]
"It has been shown that these methods are poorer than the method in (Fei and Liu, 2016).
",1 Introduction,[0],[0]
"The work closest to ours is that in (Bendale and Boult, 2016), which leverages an algorithm called OpenMax to add the rejection capability by utilizing the logits that are trained via closed-world softmax function.",1 Introduction,[0],[0]
"One weak assumption of OpenMax is that examples with equally likely logits are more likely from the unseen or rejection class, which can be examples that are hard to classify.",1 Introduction,[0],[0]
Another weakness is that it requires validation examples from the unseen/rejection class to tune the hyperparameters.,1 Introduction,[0],[0]
"Our method doesn’t make these weak assumptions and performs markedly better.
",1 Introduction,[0],[0]
"Our proposed method, called DOC (Deep Open Classification), uses deep learning (Goodfellow et al., 2016; Kim, 2014).",1 Introduction,[0],[0]
"Unlike traditional classifiers, DOC builds a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than softmax to reduce the open space risk.",1 Introduction,[0],[0]
It reduces the open space risk further for rejection by tightening the decision boundaries of sigmoid functions with Gaussian fitting.,1 Introduction,[0],[0]
Experimental results show that DOC dramatically outperforms state-of-the-art existing approaches from both text classification and image classification domains.,1 Introduction,[0],[0]
"DOC uses CNN (Collobert et al., 2011; Kim, 2014) as its base and augments it with a 1-vsrest final sigmoid layer and Gaussian fitting for
classification.",2 The Proposed DOC Architecture,[0],[0]
"Note: other existing deep models like RNN (Williams and Zipser, 1989; Schuster and Paliwal, 1997) and LSTM (Hochreiter and Schmidhuber, 1997; Gers et al., 2002) can also be adopted as the base.",2 The Proposed DOC Architecture,[0],[0]
"Similar to RNN, CNN also works on embedded sequential data (using 1D convolution on text instead of 2D convolution on images).",2 The Proposed DOC Architecture,[0],[0]
"We choose CNN because OpenMax uses CNN and CNN performs well on text (Kim, 2014), which enables a fairer comparison with OpenMax.",2 The Proposed DOC Architecture,[0],[0]
"The proposed DOC system (given in Fig. 1) is a variant of the CNN architecture (Collobert et al., 2011) for text classification (Kim, 2014)1.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
The first layer embeds words in document x into dense vectors.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
The second layer performs convolution over dense vectors using different filters of varied sizes (see Sec. 3.4).,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Next, the max-over-time pooling layer selects the maximum values from the results of the convolution layer to form a kdimension feature vector h.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Then we reduce h to a m-dimension vector d = d1:m (m is the number of training/seen classes) via 2 fully connected layers and one intermediate ReLU activation layer:
d = W ′(ReLU(Wh + b))",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"+ b′, (1)
where W ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Rr×k, b ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Rr, W ′ ∈ Rm×r, and b′ ∈",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
Rm are trainable weights; r is the output dimension of the first fully connected layer.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"The output layer of DOC is a 1-vs-rest layer applied to d1:m, which allows rejection.",2.1 CNN and Feed Forward Layers of DOC,[0],[0]
We describe it next.,2.1 CNN and Feed Forward Layers of DOC,[0],[0]
"Traditional multi-class classifiers (Goodfellow et al., 2016; Bendale and Boult, 2016) typically use softmax as the final output layer, which does not have the rejection capability since the probability of prediction for each class is normalized across all training/seen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Instead, we build a 1-vs-rest layer containing m sigmoid functions for m seen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"For the i-th sigmoid function corresponding to class li, DOC takes all examples with y = li as positive examples and all the rest examples y 6= li as negative examples.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"The model is trained with the objective of summation of all log loss of the m sigmoid functions
1https://github.com/alexander-rakhlin/ CNN-for-Sentence-Classification-in-Keras
on the training data D.
Loss = m∑
i=1",2.2 1-vs-Rest Layer of DOC,[0],[0]
"n∑ j=1 −I(yj = li) log p(yj = li)
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"−I(yj 6= li) log(1− p(yj = li)), (2)
where I is the indicator function and p(yj = li) = Sigmoid(dj,i) is the probability output from ith sigmoid function on the jth document’s ithdimension of d.
During testing, we reinterpret the prediction of m sigmoid functions to allow rejection, as shown in Eq. 3.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"For the i-th sigmoid function, we check if the predicted probability Sigmoid(di) is less than a threshold ti belonging to class li.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"If all predicted probabilities are less than their corresponding thresholds for an example, the example is rejected; otherwise, its predicted class is the one with the highest probability.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Formally, we have
ŷ = {
reject, if Sigmoid(di) <",2.2 1-vs-Rest Layer of DOC,[0],[0]
"ti,∀li ∈ Y; arg maxli∈Y Sigmoid(di), otherwise.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
"(3)
Note that although multi-label classification (Huang et al., 2013; Zhang and Zhou, 2006; Tsoumakas and Katakis, 2006) may also leverage multiple sigmoid functions, Eq. 3 forbids multiple predicted labels for the same example, which is allowed in multi-label classification.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"DOC is also related to multi-task learning (Huang et al., 2013; Caruana, 1998), where each label li is related to a 1-vs-rest binary classification task with shared representations from CNN and fully connected layers.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"However, Eq. 3 performs classification and rejection based on the outputs of these binary classification tasks.
",2.2 1-vs-Rest Layer of DOC,[0],[0]
Comparison with OpenMax:,2.2 1-vs-Rest Layer of DOC,[0],[0]
OpenMax builds on the traditional closed-world multi-class classifier (softmax layer).,2.2 1-vs-Rest Layer of DOC,[0],[0]
"It reduces the open space for each seen class, which is weak for rejecting unseen classes.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"DOC’s 1-vs-rest sigmoid layer provides a reasonable representation of all other classes (the rest of seen classes and unseen classes), and enables the 1 class forms a good boundary.",2.2 1-vs-Rest Layer of DOC,[0],[0]
Sec. 3.5 shows that this basic DOC is already much better than OpenMax.,2.2 1-vs-Rest Layer of DOC,[0],[0]
"Below, we improve DOC further by tightening the decision boundaries more.",2.2 1-vs-Rest Layer of DOC,[0],[0]
"Sigmoid function usually uses the default probability threshold of ti = 0.5 for classification of
each class i.",2.3 Reducing Open Space Risk Further,[0],[0]
But this threshold does not consider potential open space risks from unseen (rejection) class data.,2.3 Reducing Open Space Risk Further,[0],[0]
We can improve the boundary by increasing ti.,2.3 Reducing Open Space Risk Further,[0],[0]
We use Fig. 2 to illustrate.,2.3 Reducing Open Space Risk Further,[0],[0]
The x-axis represents di and y-axis is the predicted probability p(y = li|di).,2.3 Reducing Open Space Risk Further,[0],[0]
"The sigmoid function tries to push positive examples (belonging to the i-th class) and negative examples (belonging to the other seen classes) away from the y-axis via a high gain around di = 0, which serves as the default decision boundary for di with ti = 0.5.",2.3 Reducing Open Space Risk Further,[0],[0]
"As demonstrated by those 3 circles on the right-hand side of the y-axis, during testing, unseen class examples (circles) can easily fill in the gap between the y-axis and those dense positive (+) examples, which may reduce the recall of rejection and the precision of the i-th seen class prediction.",2.3 Reducing Open Space Risk Further,[0],[0]
"Obviously, a better decision boundary is at di = T , where the decision boundary more closely “wrap” those dense positive examples with the probability threshold ti 0.5 .
",2.3 Reducing Open Space Risk Further,[0],[0]
"To obtain a better ti for each seen class i-th, we use the idea of outlier detection in statistics:
1.",2.3 Reducing Open Space Risk Further,[0],[0]
"Assume the predicted probabilities p(y = li|xj , yj = li) of all training data of each class i follow one half of the Gaussian distribution (with mean µi = 1), e.g., the three positive points in Fig. 2 projected to the y-axis (we don’t need di).",2.3 Reducing Open Space Risk Further,[0],[0]
"We then artificially create the other half of the Gaussian distributed points (≥ 1): for each existing point p(y = li|xj , yj = li), we create a mirror point 1 + (1 − p(y = li|xj , yj = li) (not a probability) mirrored on the mean of 1.
2.",2.3 Reducing Open Space Risk Further,[0],[0]
"Estimate the standard deviation σi using both the existing points and the created points.
3.",2.3 Reducing Open Space Risk Further,[0],[0]
"In statistics, if a value/point is a certain number (α) of standard deviations away from the mean, it is considered an outlier.",2.3 Reducing Open Space Risk Further,[0],[0]
"We thus set the probability threshold ti = max(0.5, 1 − ασi).",2.3 Reducing Open Space Risk Further,[0],[0]
"The commonly used number for α is 3, which also works well in our experiments.
",2.3 Reducing Open Space Risk Further,[0],[0]
"Note that due to Gaussian fitting, different class li can have a different classification threshold ti.",2.3 Reducing Open Space Risk Further,[0],[0]
"We perform evaluation using two publicly available datasets, which are exactly the same datasets used in (Fei and Liu, 2016).
",3.1 Datasets,[0],[0]
"(1) 20 Newsgroups2 (Rennie, 2008): The 20 newsgroups data set contains 20 non-overlapping classes.",3.1 Datasets,[0],[0]
"Each class has about 1000 documents.
",3.1 Datasets,[0],[0]
"(2) 50-class reviews (Chen and Liu, 2014):",3.1 Datasets,[0],[0]
The dataset has Amazon reviews of 50 classes of products.,3.1 Datasets,[0],[0]
Each class has 1000 reviews.,3.1 Datasets,[0],[0]
"Although product reviews are used, we do not do sentiment classification.",3.1 Datasets,[0],[0]
We still perform topic-based classification.,3.1 Datasets,[0],[0]
"That is, given a review, the system decides what class of product the review is about.
",3.1 Datasets,[0],[0]
"For every dataset, we keep a 20000 frequent word vocabulary.",3.1 Datasets,[0],[0]
Each document is fixed to 2000- word length (cutting or padding when necessary).,3.1 Datasets,[0],[0]
"For a fair comparison, we use exactly the same settings as in (Fei and Liu, 2016).",3.2 Test Settings and Evaluation Metrics,[0],[0]
"For each class in each dataset, we randomly sampled 60% of documents for training, 10% for validation and 30% for testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
"Fei and Liu (2016) did not use a validation set, but the test data is the same 30%.",3.2 Test Settings and Evaluation Metrics,[0],[0]
We use the validation set to avoid overfitting.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"For openworld evaluation, we hold out some classes (as unseen) in training and mix them back during testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
"We vary the number of training classes and use 25%, 50%, 75%, or 100% classes for training and all classes for testing.",3.2 Test Settings and Evaluation Metrics,[0],[0]
Here using 100% classes for training is the same as the traditional closedworld classification.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"Taking 20 newsgroups as an example, for 25% classes, we use 5 classes (we randomly choose 5 classes from 20 classes for 10 times and average the results, as in (Fei and Liu, 2016)) for training and all 20 classes for testing (15 classes are unseen in training).",3.2 Test Settings and Evaluation Metrics,[0],[0]
"We use macro F1-score over 5 + 1 classes (1 for rejection) for
2http://qwone.com/˜jason/20Newsgroups/
evaluation.",3.2 Test Settings and Evaluation Metrics,[0],[0]
Please note that examples from unseen classes are dropped in the validation set.,3.2 Test Settings and Evaluation Metrics,[0],[0]
"We compare DOC with two state-of-the-art methods published in 2016 and one DOC variant.
cbsSVM:",3.3 Baselines,[0],[0]
"This is the latest method published in NLP (Fei and Liu, 2016).",3.3 Baselines,[0],[0]
It uses SVM to build 1-vs-rest CBS classifiers for multiclass text classification with rejection option.,3.3 Baselines,[0],[0]
"The results of this system are taken from (Fei and Liu, 2016).
",3.3 Baselines,[0],[0]
OpenMax:,3.3 Baselines,[0],[0]
"This is the latest method from computer vision (Bendale and Boult, 2016).",3.3 Baselines,[0],[0]
"Since it is a CNN-based method for image classification, we adapt it for text classification by using CNN with a softmax output layer, and adopt the OpenMax layer3 for open text classification.",3.3 Baselines,[0],[0]
"When all classes are seen (100%), the result from softmax is reported since OpenMax layer always performs rejection.",3.3 Baselines,[0],[0]
"We use default hyperparameter values of OpenMax (Weibull tail size is set to 20).
DOC(t = 0.5):",3.3 Baselines,[0],[0]
This is the basic DOC (t = 0.5).,3.3 Baselines,[0],[0]
"Gaussian fitting isn’t used to choose each ti.
",3.3 Baselines,[0],[0]
"Note that (Fei and Liu, 2016) compared with several other baselines.",3.3 Baselines,[0],[0]
We don’t compare with them as it was shown that cbsSVM was superior.,3.3 Baselines,[0],[0]
We use word vectors pre-trained from Google News4 (3 million words and 300 dimensions).,3.4 Hyperparameter Setting,[0],[0]
"For the CNN layers, 3 filter sizes are used [3, 4, 5].",3.4 Hyperparameter Setting,[0],[0]
"For each filter size, 150 filters are applied.",3.4 Hyperparameter Setting,[0],[0]
"The dimension r of the first fully connected layer is 250.
",3.4 Hyperparameter Setting,[0],[0]
"3https://github.com/abhijitbendale/ OSDN
4https://code.google.com/archive/p/ word2vec/",3.4 Hyperparameter Setting,[0],[0]
"The results of 20 newsgroups and 50-class reviews are given in Tables 1 and 2, respectively.",3.5 Result Analysis,[0],[0]
"From the tables, we can make the following observations:
1.",3.5 Result Analysis,[0],[0]
"DOC is markedly better than OpenMax and cbsSVM in macro-F1 scores for both datasets in the 25%, 50%, and 75% settings.",3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings (most test examples are from unseen classes), DOC is dramatically better.",3.5 Result Analysis,[0],[0]
"Even for 100% of traditional closed-world classification, it is consistently better too.",3.5 Result Analysis,[0],[0]
"DOC(t = 0.5) is better too.
",3.5 Result Analysis,[0],[0]
2.,3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings, DOC is also markedly better than DOC(t = 0.5), which shows that Gaussian fitting finds a better probability threshold than t = 0.5 when many unseen classes are present.",3.5 Result Analysis,[0],[0]
"In the 75% setting (most test examples are from seen classes), DOC(t = 0.5) is slightly better for 20 newsgroups but worse for 50-class reviews.",3.5 Result Analysis,[0],[0]
"DOC sacrifices some recall of seen class examples for better precision, while t = 0.5 sacrifices the precision of seen classes for better recall.",3.5 Result Analysis,[0],[0]
DOC(t = 0.5) is also worse than cbsSVM for 25% setting for 50-class reviews.,3.5 Result Analysis,[0],[0]
"It is thus not as robust as DOC.
3.",3.5 Result Analysis,[0],[0]
"For the 25% and 50% settings, cbsSVM is also markedly better than OpenMax.",3.5 Result Analysis,[0],[0]
"This paper proposed a novel deep learning based method, called DOC, for open text classification.",4 Conclusion,[0],[0]
"Using the same text datasets and experiment settings, we showed that DOC performs dramatically better than the state-of-the-art methods from both the text and image classification domains.",4 Conclusion,[0],[0]
"We also believe that DOC is applicable to images.
",4 Conclusion,[0],[0]
"In our future work, we plan to improve the cumulative or incremental learning method in (Fei et al., 2016) to learn new classes without training on all past and new classes of data from scratch.",4 Conclusion,[0],[0]
"This will enable the system to learn by self to achieve continual or lifelong learning (Chen and Liu, 2016).",4 Conclusion,[0],[0]
"We also plan to improve model performance during testing (Shu et al., 2017).",4 Conclusion,[0],[0]
This work was supported in part by grants from National Science Foundation (NSF) under grant no.,Acknowledgments,[0],[0]
IIS-1407927 and IIS-1650900.,Acknowledgments,[0],[0]
Traditional supervised learning makes the closed-world assumption that the classes appeared in the test data must have appeared in training.,abstractText,[0],[0]
This also applies to text learning or text classification.,abstractText,[0],[0]
"As learning is used increasingly in dynamic open environments where some new/test documents may not belong to any of the training classes, identifying these novel documents during classification presents an important problem.",abstractText,[0],[0]
This problem is called openworld classification or open classification.,abstractText,[0],[0]
This paper proposes a novel deep learning based approach.,abstractText,[0],[0]
It outperforms existing state-of-the-art techniques dramatically.,abstractText,[0],[0]
DOC: Deep Open Classification of Text Documents,title,[0],[0]
Supervised learning has been successful in many application fields.,1. Introduction,[0],[0]
"The vast majority of supervised learning research falls into the Empirical Risk Minimization (ERM) framework (Vapnik, 1998) that assumes a test distribution to be the same as a training distribution.",1. Introduction,[0],[0]
"However, such an assumption can be easily contradicted in real-world applications due to sample selection bias or non-stationarity of the environment (Quionero-Candela et al., 2009).",1. Introduction,[0],[0]
"Once the distribution shift occurs, the performance of the traditional machine learning techniques can be significantly degraded.",1. Introduction,[0],[0]
"This makes the traditional techniques unreliable for practitioners to use in the real world.
",1. Introduction,[0],[0]
"1University of Tokyo, Japan 2RIKEN, Tokyo, Japan.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
Weihua Hu,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"weihua916@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
Distributionally Robust Supervised Learning (DRSL) is a promising paradigm to tackle this problem by obtaining prediction functions explicitly robust to distribution shift.,1. Introduction,[0],[0]
"More specifically, DRSL considers a minimax game between a learner and an adversary: the adversary first shifts the test distribution from the training distribution within a pre-specified uncertainty set so as to maximize the expected loss on the test distribution.",1. Introduction,[0],[0]
"The learner then minimizes the adversarial expected loss.
",1. Introduction,[0],[0]
"DRSL with f -divergences (Bagnell, 2005; Ben-Tal et al., 2013; Duchi et al., 2016; Namkoong & Duchi, 2016; 2017) is particularly well-studied and lets the uncertainty set for test distributions be an f -divergence ball from a training distribution (see Section 2 for the detail).",1. Introduction,[0],[0]
This DRSL has been mainly studied under the assumption that the same continuous loss is used for training and testing.,1. Introduction,[0],[0]
"This is not the case in the classification scenario, in which we care about the 0-1 loss (i.e., the mis-classification rate) at test time, while at training time, we use a surrogate loss for optimization tractability.
",1. Introduction,[0],[0]
"In this paper, we revisit DRSL with f -divergences, providing novel insight for the classification scenario.",1. Introduction,[0],[0]
"In particular, we prove rather surprising results (Theorems 1–3), showing that when the DRSL is applied to classification, the obtained classifier ends up being optimal for the training distribution.",1. Introduction,[0],[0]
This is too pessimistic for DRSL given that DRSL is explicitly formulated for a distribution shift scenario and is naturally expected to give a classifier different from the one that exactly fits the given training distribution.,1. Introduction,[0],[0]
"Such pessimism comes from two sources: the particular losses used in classification and the over-flexibility of the uncertainty set used by DRSL with f -divergences.
",1. Introduction,[0],[0]
"Motivated by our analysis, we propose simple DRSL that overcomes the pessimism of the previous DRSL by incorporating structural assumptions on distribution shift (Section 4).",1. Introduction,[0],[0]
We establish convergence properties of our proposed DRSL (Theorem 4) and derive efficient optimization algorithms (Section 5).,1. Introduction,[0],[0]
"Finally, we demonstrate the effectiveness of our DRSL through experiments (Section 6).",1. Introduction,[0],[0]
"All the appendices of this paper are provided in the supplementary material.
",1. Introduction,[0],[0]
"Related work: Besides DRSL with f -divergences, different DRSL considers different classes of uncertainty sets for test distributions.",1. Introduction,[0],[0]
"DRSL by Globerson & Roweis
(2006) considered the uncertainty of features deletion, while DRSL by Liu & Ziebart (2014) considered the uncertainty of unknown properties of the conditional label distribution.",1. Introduction,[0],[0]
"DRSL by Esfahani & Kuhn (2015), Blanchet et al. (2016) and Sinha et al. (2017) lets the uncertainty set of test distributions be a Wasserstein ball from the training distribution.",1. Introduction,[0],[0]
"DRSL with the Wasserstein distance can make classifiers robust to adversarial examples (Sinha et al., 2017), while DRSL with f -divergences can make classifiers robust against adversarial reweighting of data points as shown in Section 2.",1. Introduction,[0],[0]
"Recently, in the context of fair machine learning, Hashimoto et al. (2018) applied DRSL with f - divergences in an attempt to achieve fairness without demographic information.",1. Introduction,[0],[0]
"In this section, we first review the ordinary ERM framework.",2. Review of ERM and DRSL,[0],[0]
"Then, we explain a general formulation of DRSL and review DRSL with f -divergences.
",2. Review of ERM and DRSL,[0],[0]
"Suppose training samples, {(x1, y1), . . .",2. Review of ERM and DRSL,[0],[0]
", (xN , yN )} ≡ D, are drawn i.i.d.",2. Review of ERM and DRSL,[0],[0]
"from an unknown training distribution over X × Y with density p(x, y), where X ⊂ Rd and Y is an output domain.",2. Review of ERM and DRSL,[0],[0]
"Let gθ be a prediction function with parameter θ, mapping x ∈ X into a real scaler or vector, and let ℓ(ŷ, y) be a loss between y and real-valued prediction ŷ.
ERM: The objective of the risk minimization (RM) is
min θ Ep(x,y)[ℓ(gθ(x), y)]︸ ︷︷ ︸ ≡ R(θ) , (1)
where R(θ) is called the risk.",2. Review of ERM and DRSL,[0],[0]
"In ERM, we approximate the expectation in Eq.",2. Review of ERM and DRSL,[0],[0]
"(1) by training data D:
min θ
1
N
N∑
i=1
ℓ(gθ(xi), yi)
︸ ︷︷ ︸ ≡ R̂(θ)
, (2)
where R̂(θ) is called the empirical risk.",2. Review of ERM and DRSL,[0],[0]
"To prevent overfitting, we can add regularization term Ω(θ) to Eq.",2. Review of ERM and DRSL,[0],[0]
"(2) and minimize R̂(θ)+λΩ(θ), where λ ≥ 0 is a trade-off hyperparameter.
",2. Review of ERM and DRSL,[0],[0]
"General formulation of DRSL: ERM implicitly assumes the test distribution to be the same as the training distribution, which does not hold in most real-world applications.",2. Review of ERM and DRSL,[0],[0]
"DRSL is explicitly formulated for a distribution shift scenario, where test density q(x, y) is different from training density p(x, y).",2. Review of ERM and DRSL,[0],[0]
Let Qp be an uncertainty set for test distributions.,2. Review of ERM and DRSL,[0],[0]
"In DRSL, the learning objective is
min θ sup",2. Review of ERM and DRSL,[0],[0]
"q∈Qp Eq(x,y)[ℓ(gθ(x), y)].",2. Review of ERM and DRSL,[0],[0]
"(3)
We see that Eq. (3) minimizes the risk w.r.t.",2. Review of ERM and DRSL,[0],[0]
"the worst-case test distribution within the uncertainty set Qp.
DRSL with f -divergences: Let q ≪ p denote that q is absolutely continuous w.r.t.",2. Review of ERM and DRSL,[0],[0]
"p, i.e., p(x, y) = 0 implies q(x, y) = 0.",2. Review of ERM and DRSL,[0],[0]
"Bagnell (2005) and Ben-Tal et al. (2013) considered the particular uncertainty set
Qp = {q ≪ p | Df [q∥p] ≤ δ}, (4)
where Df [·∥·] is an f -divergence defined as Df [q∥p] ≡",2. Review of ERM and DRSL,[0],[0]
"Ep [f (q/p)], and f(·) is convex with f(1) = 0.",2. Review of ERM and DRSL,[0],[0]
"The f - divergence (Ciszar, 1967) measures a discrepancy between probability distributions.",2. Review of ERM and DRSL,[0],[0]
"When f(x) = x log x, we have the well-known Kullback-Leibler divergence as an instance of it.",2. Review of ERM and DRSL,[0],[0]
Hyper-parameter δ > 0,2. Review of ERM and DRSL,[0],[0]
in Eq.,2. Review of ERM and DRSL,[0],[0]
(4) controls the degree of the distribution shift.,2. Review of ERM and DRSL,[0],[0]
"Define r(x, y) ≡ q(x, y)/p(x, y).",2. Review of ERM and DRSL,[0],[0]
"Through some calculations, the objective of DRSL with f - divergences can be rewritten as
min θ sup r∈Uf Ep(x,y)[r(x, y)ℓ(gθ(x), y)] ︸ ︷︷ ︸
≡ Radv(θ)
, (5)
",2. Review of ERM and DRSL,[0],[0]
"Uf ≡ {r(x, y) |",2. Review of ERM and DRSL,[0],[0]
"Ep(x,y)",2. Review of ERM and DRSL,[0],[0]
"[f (r(x, y))]",2. Review of ERM and DRSL,[0],[0]
"≤ δ,
Ep(x,y)[r(x, y)]",2. Review of ERM and DRSL,[0],[0]
"= 1,
r(x, y) ≥ 0, ∀(x, y) ∈ X × Y}.",2. Review of ERM and DRSL,[0],[0]
"(6)
We call Radv(θ) the adversarial risk and call the minimization problem of Eq.",2. Review of ERM and DRSL,[0],[0]
(5) the adversarial risk minimization (ARM).,2. Review of ERM and DRSL,[0],[0]
"In ARM, the density ratio, r(x, y), can be considered as the weight put by the adversary on the loss of data (x, y).",2. Review of ERM and DRSL,[0],[0]
"Then, Eq. (5) can be regarded as a minimax game between the learner (corresponding to minθ) and the adversary (corresponding to supr∈Uf ): the adversary first reweights the losses using r(·, ·) so as to maximize the expected loss; the learner then minimizes the reweighted expected loss, i.e., adversarial risk Radv(θ).",2. Review of ERM and DRSL,[0],[0]
"For notational convenience, we denote ℓ(gθ(xi), yi) by ℓi(θ).",2. Review of ERM and DRSL,[0],[0]
"Also, let r ≡ (r1, . . .",2. Review of ERM and DRSL,[0],[0]
", rN ) be a vector of density ratios evaluated at training data points, i.e., ri ≡ r(xi, yi) for 1 ≤ i ≤",2. Review of ERM and DRSL,[0],[0]
N .,2. Review of ERM and DRSL,[0],[0]
"Equations (5) and (6) can be empirically approximated as1
min θ sup r∈Ûf
1 N
N∑
i=1
riℓi(θ)
",2. Review of ERM and DRSL,[0],[0]
"︸ ︷︷ ︸ ≡ R̂adv(θ)
, (7)
Ûf = { r ∣∣∣∣∣",2. Review of ERM and DRSL,[0],[0]
"1 N N∑
i=1
f (ri) ≤ δ, 1 N
N∑
i=1
ri = 1, r ≥ 0 } , (8)
1The formulation in Eqs.",2. Review of ERM and DRSL,[0],[0]
"(7) and (8) is similar to Duchi et al. (2016), Namkoong & Duchi (2016) and Namkoong & Duchi (2017) except that they decay δ",2. Review of ERM and DRSL,[0],[0]
linearly w.r.t.,2. Review of ERM and DRSL,[0],[0]
the number of training data N .,2. Review of ERM and DRSL,[0],[0]
"Different from us, they assume δ = 0",2. Review of ERM and DRSL,[0],[0]
in Eq.,2. Review of ERM and DRSL,[0],[0]
"(4) (thus, their objective is the ordinary risk) and try to be robust to apparent distribution fluctuations due to the finiteness of training samples.",2. Review of ERM and DRSL,[0],[0]
"On the other hand, we consider using the same δ > 0",2. Review of ERM and DRSL,[0],[0]
for both Eqs.,2. Review of ERM and DRSL,[0],[0]
"(4) and (8) and try to be robust to the actual distribution change between training and test stages.
where the inequality constraint for a vector is applied in an element-wise fashion.",2. Review of ERM and DRSL,[0],[0]
We call R̂adv(θ) the adversarial empirical risk and call the minimization problem of Eq.,2. Review of ERM and DRSL,[0],[0]
(7) the adversarial empirical risk minimization (AERM).,2. Review of ERM and DRSL,[0],[0]
"In AERM, the adversary (corresponding to supr∈Ûf ) reweights data losses through r to maximize the empirical loss in Eq.",2. Review of ERM and DRSL,[0],[0]
(7).,2. Review of ERM and DRSL,[0],[0]
"To prevent overfitting, we can add regularization term Ω(θ) to Eq.",2. Review of ERM and DRSL,[0],[0]
(7).,2. Review of ERM and DRSL,[0],[0]
"At first glance, DRSL with f -divergences (which we call ARM and AERM in this paper) is reasonable to give a distributionally robust classifier in the sense that it explicitly minimizes the loss for the shifted worst-case test distribution.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"However, we show rather surprising results, suggesting that the DRSL, when applied to classification, still ends up giving a classifier optimal for a training distribution.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
This is too pessimistic for DRSL because it ends up behaving similarly to ordinary ERM-based supervised classification that does not explicitly consider distribution shift.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"To make a long story short, our results hold because of the particular losses used in classification (especially, the 0-1 loss at test time) and the overly flexible uncertainty sets used by ARM and AERM.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"We will detail these points after we state our main results.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Classification setting: Let us first briefly review classification settings to set up notations.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In binary classification, we have gθ(·) : x )",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"→ ŷ ∈ R, Y = {+1,−1} and ℓ(·, ·) :",3. Analysis of DRSL with f -divergences in classification,[0],[0]
R × Y → R≥0.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In K-class classification for K ≥ 2, we have gθ(·) : x )",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"→ ŷ ∈ RK , Y = {1, 2, . . .",3. Analysis of DRSL with f -divergences in classification,[0],[0]
",K} and ℓ(·, ·) : RK × Y → R≥0.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
The goal of classification is to learn the prediction function that minimizes the mis-classification rate on the test distribution.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"The misclassification rate corresponds to the use of the 0-1 loss, i.e., ℓ(ŷ, y) ≡ 1{sign(ŷ)",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"̸= y} for binary classification, and ℓ(ŷ, y) ≡",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"1{argmaxkŷk ̸= y} for multi-class classification, where 1{·} is the indicator function and ŷk is the k-th element of ŷ ∈ RK .",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"However, since the 0-1 loss is non-convex and non-continuous, learning with it is difficult in practice.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Therefore, at training time, we instead use surrogate losses that are easy to optimize, such as the logistic loss and the cross-entropy loss.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"In the following, we state our main results, analyzing ARM and AERM in the classification scenario by considering the use of the 0-1 loss and a surrogate loss.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"The 0-1 loss case: Theorem 1 establishes the non-trivial relationship between the adversarial risk and the ordinary risk when the 0-1 loss is used.
",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Theorem 1.,3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Let ℓ(ŷ, y) be the 0-1 loss.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
"Then, there is a monotonic relationship between Radv(θ) and R(θ) in the sense that for any pair of parameters θ1 and θ2, the followings hold.",3. Analysis of DRSL with f -divergences in classification,[0],[0]
Radv(θ1),"If Radv(θ1) < 1, then",[0],[0]
< Radv(θ2)⇐⇒ R(θ1),"If Radv(θ1) < 1, then",[0],[0]
< R(θ2).,"If Radv(θ1) < 1, then",[0],[0]
(9),"If Radv(θ1) < 1, then",[0],[0]
R(θ1) ≤,"If Radv(θ1) = 1, then",[0],[0]
R(θ2) =⇒ Radv(θ2),"If Radv(θ1) = 1, then",[0],[0]
"= 1. (10)
The same monotonic relationship also holds between their empirical approximations: R̂adv(θ) and R̂(θ).","If Radv(θ1) = 1, then",[0],[0]
See Appendix A for the proof.,"If Radv(θ1) = 1, then",[0],[0]
"Theorem 1 shows a surprising result that when the 0-1 loss is used, R(θ) and Radv(θ) are essentially equivalent objective functions in the sense that the minimization of one objective function results in the minimization of another objective function.","If Radv(θ1) = 1, then",[0],[0]
This readily implies that R(θ) and Radv(θ) have exactly the same set of global minima in the regime of Radv(θ) < 1.,"If Radv(θ1) = 1, then",[0],[0]
"An immediate practical implication is that if we select hyper-parameters such as λ for regularization according to the adversarial risk with the 0-1 loss, we will end up choosing hyper-parameters that attain the minimum misclassification rate on the training distribution.","If Radv(θ1) = 1, then",[0],[0]
"The surrogate loss case: We now turn our focus on the training stage of classification, where we use a surrogate loss instead of the 0-1 loss.","If Radv(θ1) = 1, then",[0],[0]
"In particular, for binary classification, we consider a class of classification calibrated losses (Bartlett et al., 2006) that are margin-based, i.e., ℓ(ŷ, y) is a function of product yŷ. For multi-class classification, we consider a class of classification calibrated losses (Tewari & Bartlett, 2007) that are invariant to class permutation, i.e., for any class permutation π : Y → Y , ℓ(ŷπ,π(y))","If Radv(θ1) = 1, then",[0],[0]
"= ℓ(ŷ, y) holds, where ŷπk = ŷπ(k) for 1 ≤ k ≤ K.","If Radv(θ1) = 1, then",[0],[0]
"Although we only consider the sub-class of general classification-calibrated losses (Bartlett et al., 2006; Tewari & Bartlett, 2007), we note that ours still includes some of the most widely used losses: the logistic, hinge, and exponential losses for binary classification and the softmax cross entropy loss for multi-class classification.
","If Radv(θ1) = 1, then",[0],[0]
We first review Proposition 1 by Bartlett et al. (2006) and Tewari & Bartlett (2007) that justifies the use of classification-calibrated losses in ERM for classification.,"If Radv(θ1) = 1, then",[0],[0]
We then show a surprising fact in Theorem 2 that the similar property also holds for ARM using the sub-class of classification-calibrated losses.,"If Radv(θ1) = 1, then",[0],[0]
Proposition 1 (Bartlett et al. (2006); Tewari & Bartlett (2007)).,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be a classification calibrated loss, and assume that the hypothesis class is equal to all measurable functions.","If Radv(θ1) = 1, then",[0],[0]
"Then, the risk minimization (RM) gives the Bayes optimal classifier2.","If Radv(θ1) = 1, then",[0],[0]
Theorem 2.,"If Radv(θ1) = 1, then",[0],[0]
"Let f(·) be differentiable, the hypothesis class be all measurable functions, and ℓ(ŷ, y) be a classificationcalibrated loss that is margin-based or invariant to class
2The classifier that minimizes the mis-classification rate for the training density p(x, y) (the 0-1 loss is considered), i.e., the classifier whose prediction on x is equal to argmaxy∈Y p(y|x).
permutation.","If Radv(θ1) = 1, then",[0],[0]
"Let g(adv) be any solution of ARM3 under the above setting, and define
r∗ ≡ argmax","If Radv(θ1) = 1, then",[0],[0]
"r∈Uf Ep(x,y)[r(x, y)ℓ(g(adv)(x), y)].","If Radv(θ1) = 1, then",[0],[0]
"(11)
Then, the prediction of g(adv) coincides with that of the Bayes optimal classifier almost surely over q∗(x) ≡∑
y∈Y r ∗(x, y)p(x, y).","If Radv(θ1) = 1, then",[0],[0]
"Furthermore, among the solutions of ARM, there exists g(adv) whose prediction coincides with that of the Bayes optimal classifier almost surely over p(x).
","If Radv(θ1) = 1, then",[0],[0]
See Appendix B for the proof.,"If Radv(θ1) = 1, then",[0],[0]
"Theorem 2 indicates that ARM, similarly to RM, ends up giving the optimal decision boundary for the training distribution, if the hypothesis class is all measurable functions and we have access to true density p(x, y).","If Radv(θ1) = 1, then",[0],[0]
"Even though the assumptions made are strong, Theorem 2 together with Proposition 1 highlight the non-trivial fact that when a certain surrogate loss is used, AERM and ERM demonstrate the similar asymptotic behavior in classification.
","If Radv(θ1) = 1, then",[0],[0]
"We proceed to consider a more practical scenario, where we only have a finite amount of training data and the hypothesis class is limited.","If Radv(θ1) = 1, then",[0],[0]
"In the rest of the section, we focus on a differentiable loss and a real-valued scalar output, i.e., ŷ ∈ R, which includes the scenario of binary classification.
","If Radv(θ1) = 1, then",[0],[0]
"We first define the notion of a steeper loss, which will play a central role in our result.
","If Radv(θ1) = 1, then",[0],[0]
Definition 1 (Steeper loss).,"If Radv(θ1) = 1, then",[0],[0]
"Loss function ℓsteep(ŷ, y) is said to be steeper than loss function ℓ(ŷ, y), if there exists a non-constant, non-decreasing and non-negative function h :","If Radv(θ1) = 1, then",[0],[0]
"R≥0 → R≥0 such that
∂ℓsteep(ŷ, y)
∂ŷ = h(ℓ(ŷ, y))
","If Radv(θ1) = 1, then",[0],[0]
"∂ℓ(ŷ, y)
∂ŷ .","If Radv(θ1) = 1, then",[0],[0]
"(12)
For example, following Definition 1, we can show that the exponential loss is steeper than the logistic loss.","If Radv(θ1) = 1, then",[0],[0]
"Intuitively, outlier-sensitive losses are steeper than more outlier-robust losses.","If Radv(θ1) = 1, then",[0],[0]
"Lemma 1 shows an important property of a steeper loss in a classification scenario.
","If Radv(θ1) = 1, then",[0],[0]
Lemma 1.,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be a margin-based convex classification-calibrated loss.","If Radv(θ1) = 1, then",[0],[0]
"Then, its steeper loss defined in Eq.","If Radv(θ1) = 1, then",[0],[0]
"(12) is also convex classification-calibrated if h(ℓ(0, y))","If Radv(θ1) = 1, then",[0],[0]
"> 0.
See Appendix C for the proof.
","If Radv(θ1) = 1, then",[0],[0]
Now we are ready to state our result in Theorem 3 that considers ŷ ∈ R. Theorem 3 holds for any hypothesis class that is parametrized by θ and sub-differentiable w.r.t.,"If Radv(θ1) = 1, then",[0],[0]
"θ, e.g., linear-in-parameter models and deep neural networks.
","If Radv(θ1) = 1, then",[0],[0]
"3There can be multiple solutions that achieve the same minimum adversarial risk.
","If Radv(θ1) = 1, then",[0],[0]
Theorem 3.,"If Radv(θ1) = 1, then",[0],[0]
Let θ∗ be a stationary point of AERM in Eq.,"If Radv(θ1) = 1, then",[0],[0]
"(7) using ℓ(ŷ, y).","If Radv(θ1) = 1, then",[0],[0]
"Then, there exists a steeper loss function, ℓDRSL(ŷ, y), such that θ∗ is also a stationary point of the following ERM.
min θ
1
N
N∑
i=1
ℓDRSL(gθ(xi), yi).","If Radv(θ1) = 1, then",[0],[0]
"(13)
See Appendix D for the proof.","If Radv(θ1) = 1, then",[0],[0]
Remark 1 (Conditions for convexity).,"If Radv(θ1) = 1, then",[0],[0]
"Let ℓ(ŷ, y) be convex in ŷ, gθ(x) be a linear-in-parameter model.","If Radv(θ1) = 1, then",[0],[0]
"Then, both AERM in Eq.","If Radv(θ1) = 1, then",[0],[0]
(7) and ERM in Eq.,"If Radv(θ1) = 1, then",[0],[0]
(13) become convex in θ.,"If Radv(θ1) = 1, then",[0],[0]
This implies that the stationary point θ∗ in Theorem 3 turns out to be the global optimum for both Eqs.,"If Radv(θ1) = 1, then",[0],[0]
(7) and (13) in this usual setting.,"If Radv(θ1) = 1, then",[0],[0]
"Note that Theorem 3 holds for general real-valued scalar prediction, i.e., ŷ ∈ R; thus, the result holds for ordinary regression (using the same loss for training and testing) as well as for binary classification.","If Radv(θ1) = 1, then",[0],[0]
"However, as we discuss in the following, the implication of Theorem 3 is drastically different for the two scenarios.
","If Radv(θ1) = 1, then",[0],[0]
"Implication for classification: Theorem 3 together with Lemma 1 indicate that under a mild condition,4 AERM using a convex classification-calibrated margin-based loss reduces to Eq.","If Radv(θ1) = 1, then",[0],[0]
"(13), which is ERM using a convex classification-calibrated loss.","If Radv(θ1) = 1, then",[0],[0]
"This implies that AERM, similarly to ordinary ERM using a classification-calibrated loss, will try to give a classifier optimal for the training distribution.
","If Radv(θ1) = 1, then",[0],[0]
Why does the use of the steeper surrogate loss fail to give meaningful robust classifiers?,"If Radv(θ1) = 1, then",[0],[0]
"This is because we are dealing with classification tasks, where we care about the performance in terms of the 0-1 loss at test time.","If Radv(θ1) = 1, then",[0],[0]
"The use of the steeper surrogate loss may make a classifier distributionally robust in terms of the surrogate loss,5 but not necessarily so in terms of the 0-1 loss.","If Radv(θ1) = 1, then",[0],[0]
"Moreover, even if we obtain a classifier that minimizes the adversarial risk in terms of the 0-1 loss, the obtained classifier ends up being optimal for the training distribution (see Theorem 1).","If Radv(θ1) = 1, then",[0],[0]
"In any case, the use of the steeper loss does not in general give classifiers that are robust to change from a training distribution.
","If Radv(θ1) = 1, then",[0],[0]
"In summary, in the classification scenario, the use of the steeper loss does more harm (making a classifier sensitive
4The condition that h(ℓ(0, y))","If Radv(θ1) = 1, then",[0],[0]
> 0,"If Radv(θ1) = 1, then",[0],[0]
in Lemma 1.,"If Radv(θ1) = 1, then",[0],[0]
"Whether the condition holds or not generally depends on the uncertainty set, the model, the loss function, and training data.","If Radv(θ1) = 1, then",[0],[0]
"Nonetheless, the condition is mild in practice; especially, the condition always holds when the Kullback-Leibler divergence is used.","If Radv(θ1) = 1, then",[0],[0]
"See Appendix C for detailed discussion.
","If Radv(θ1) = 1, then",[0],[0]
5For fixed δ (non-decaying w.r.t.,"If Radv(θ1) = 1, then",[0],[0]
"N ), whether AERM is consistent with ARM or not is an open problem.","If Radv(θ1) = 1, then",[0],[0]
"Nevertheless, we empirically confirm in Section 6 that AERM achieves lower adversarial risk than other baselines in terms of the surrogate loss.
to outliers due to the use of the steeper surrogate loss) than good (making a classifier robust to change from a training distribution).
","If Radv(θ1) = 1, then",[0],[0]
"Implication for ordinary regression: For comparison, let us rethink about the classical regression scenario, in which we use the same loss, e.g., the squared loss, during training and testing.","If Radv(θ1) = 1, then",[0],[0]
"In such a case, the use of the steeper loss may indeed make regressors distributionally robust in terms of the same loss.","If Radv(θ1) = 1, then",[0],[0]
"Nonetheless, learning can be extremely sensitive to outliers due to the use of the steeper loss.","If Radv(θ1) = 1, then",[0],[0]
"Hence, when applying DRSL with f -divergences to real-world regression tasks, we need to pay extra attention to ensure that there are no outliers in datasets.","If Radv(θ1) = 1, then",[0],[0]
"In this section, motivated by our theoretical analysis in Section 3, we propose simple yet practical DRSL that overcomes the over pessimism of ARM and AERM in the classification scenario.",4. DRSL with Latent Prior Probability Change,[0],[0]
"We then analyze its convergence property and discuss the practical use of our DRSL.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Theoretical motivation: What insight can we get from our theoretical analyses in Section 3?,4. DRSL with Latent Prior Probability Change,[0],[0]
"Our key insight from proving the theorems is that the adversary of ARM has too much (non-parametric) freedom to shift the test distribution, and as a result, the learner becomes overly pessimistic.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In fact, the proofs of all the theorems rely on the overflexibility of the uncertainty set Uf in Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(6), i.e., the values of r(·, ·) are not tied together for different (x, y) within Uf (see Eqs.",4. DRSL with Latent Prior Probability Change,[0],[0]
(5) and (6)).,4. DRSL with Latent Prior Probability Change,[0],[0]
"Consequently, the adversary of ARM simply assigns larger weight r(x, y) to data (x, y) with a larger loss.",4. DRSL with Latent Prior Probability Change,[0],[0]
"This fact, combined with the fact that we use the different losses during training and testing in classification (see discussion at the end of Section 3), led to the pessimistic results of Theorems 1–3.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Our theoretical insight suggests that in order to overcome the pessimism of ARM applied to classification, it is crucial to structurally constrain r(·, ·) in Uf , or equivalently, to impose structural assumptions on the distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To this end, in this section, we propose DRSL that overcomes the limitation of the DRSL by incorporating structural assumptions on distribution shift.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Practical structural assumptions: In practice, there can be a variety of ways to impose structural assumptions on distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Here, as one possible way, we adopt the latent prior probability change assumption (Storkey & Sugiyama, 2007) because this particular class of assumptions enjoys the following two practical advantages.
1.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Within the class, users of our DRSL can easily and intuitively model their assumptions on distribution shift (see the discussion at the end of this section).
",4. DRSL with Latent Prior Probability Change,[0],[0]
2.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Efficient learning algorithms can be derived (see Section 5).
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Let us introduce a latent variable z ∈ Z ≡ {1, . . .",4. DRSL with Latent Prior Probability Change,[0],[0]
", S}, which we call a latent category, where S is a constant.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The latent prior probability change assumes
p(x, y|z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"= q(x, y|z), q(z) ̸=",4. DRSL with Latent Prior Probability Change,[0],[0]
"p(z), (14)
where p and q are the training and test densities, respectively.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The intuition is that we assume a two-level hierarchical data-generation process: we first sample latent category z from the prior and then sample actual data (x, y) conditioned on z.",4. DRSL with Latent Prior Probability Change,[0],[0]
"We then assume that only the prior distribution over the latent categories changes, leaving the conditional distribution intact.
",4. DRSL with Latent Prior Probability Change,[0],[0]
We assume the structural assumption in Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(14) to be provided externally by users of our DRSL based on their knowledge of potential distribution shift, rather than something to be inferred from data.",4. DRSL with Latent Prior Probability Change,[0],[0]
"As we will see at the end of this section, specifying Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(14) amounts to grouping training data points according to their latent categories, which is quite intuitive to do in practice.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Objective function of our DRSL: With the latent prior probability change of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(14), the uncertainty set for test distributions in our DRSL becomes
Qp = {q ≪",4. DRSL with Latent Prior Probability Change,[0],[0]
"p | Df [q(x, y, z)||p(x, y, z)]",4. DRSL with Latent Prior Probability Change,[0],[0]
"≤ δ, q(x, y|z) = p(x, y|z)}.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(15)
Then, corresponding to Eq. (3), the objective of our DRSL can be written as
min θ sup w∈Wf Ep(x,y,z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"[w(z)ℓ(gθ(x), y)] ︸ ︷︷ ︸
≡ Rs-adv(θ)
, (16)
",4. DRSL with Latent Prior Probability Change,[0],[0]
Wf ≡ { w(z) ∣∣∣∣∣,4. DRSL with Latent Prior Probability Change,[0],[0]
"∑
z∈Z
p(z)f (w(z)) ≤ δ,
∑
z∈Z
p(z)w(z)",4. DRSL with Latent Prior Probability Change,[0],[0]
"= 1, w(z) ≥ 0, ∀z ∈ Z } , (17)
",4. DRSL with Latent Prior Probability Change,[0],[0]
"where w(z) ≡ q(z)/p(z) = q(x, y, z)/p(x, y, z) because of q(x, y|z) = p(x, y|z).",4. DRSL with Latent Prior Probability Change,[0],[0]
We call Rs-adv(θ) the structural adversarial risk and call the minimization problem of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(16) the structural adversarial risk minimization (structural ARM).,4. DRSL with Latent Prior Probability Change,[0],[0]
"Similarly to ARM, structural ARM is a minimax game between the learner and the adversary.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Differently from ARM, the adversary of structural ARM (corresponding to supw∈Wf ) uses w(·) to reweight data; hence, it has much less (only parametric) freedom to shift the test distribution compared to the adversary of ARM that uses non-parametric weight r(·, ·) (see Eq. (5)).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Because of this limited freedom for the adversary, we can show that Theorems 1–3 do not hold for structural ARM, and we can expect to learn meaningful classifiers that are robust to structurally constrained distribution shift.
Discussion and proposal of evaluation metric for distributional robustness:",4. DRSL with Latent Prior Probability Change,[0],[0]
"Recall from Theorem 1 that when the 0-1 loss is used, the adversarial risk ends up being equivalent to the ordinary risk as an evaluation metric, which is too pessimistic as a metric for the distributional robustness of a classifier.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In contrast, we can easily verify that our structural adversarial risk using the 0-1 loss does not suffer from the pessimism.",4. DRSL with Latent Prior Probability Change,[0],[0]
We argue that our structural adversarial risk can be an alternative metric in distributionally robust classification.,4. DRSL with Latent Prior Probability Change,[0],[0]
"To better understand its property, inspired by Namkoong & Duchi (2017), we decompose it as6
Rs-adv(θ) = R(θ)︸ ︷︷ ︸",4. DRSL with Latent Prior Probability Change,[0],[0]
"(a) ordinary risk
+ √ δ ·
√∑
z∈Z p(z)(Rz(θ)",4. DRSL with Latent Prior Probability Change,[0],[0]
"− R(θ))2 ︸ ︷︷ ︸ (b) sensitivity , (18)
where Rz(θ)(≡ Ep(x,y|z)[ℓ(gθ(x), y)]) is the risk of the classifier on latent category z ∈ Z .",4. DRSL with Latent Prior Probability Change,[0],[0]
We see that Rs-adv(θ) in Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(18) contains the risk variance term (b).,4. DRSL with Latent Prior Probability Change,[0],[0]
This variance term (b) can be large when the obtained classifier performs extremely poorly on a small number of latent categories.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Once a test density concentrates on those poorlyperformed latent categories, the test accuracy of the classifier can extremely deteriorate.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, the classifier with large (b) is sensitive to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In contrast, the small risk variance (b) indicates that the obtained classifier attains similar accuracy on all the latent categories.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In such a case, the test accuracy of the classifier is insensitive to latent category prior change.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, the classifier with small (b) is robust to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To sum up, the additional term (b) measures the sensitivity of the classifier to the specified structural distribution shift.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"On the basis of the above discussion, we see that Rs-adv(θ) in Eq.",4. DRSL with Latent Prior Probability Change,[0],[0]
"(18) simultaneously captures (a) the ordinary risk, i.e., the mis-classification rate when no distribution shift occurs, and (b) the sensitivity to distribution shift.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this sense, our structural adversarial risk is an intuitive and reasonable metric for distributional robustness of a classifier, and we will employ this metric in our experiments in Section 6.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Empirical approximation: We explain how to empirically approximate the objective functions in Eqs.,4. DRSL with Latent Prior Probability Change,[0],[0]
(16) and (17) using training data D′ ≡,4. DRSL with Latent Prior Probability Change,[0],[0]
"{(x1, y1, z1), .",4. DRSL with Latent Prior Probability Change,[0],[0]
. .,4. DRSL with Latent Prior Probability Change,[0],[0]
", (xN , yN , zN )} drawn independently from p(x, y, z).
",4. DRSL with Latent Prior Probability Change,[0],[0]
Define Gs ≡,4. DRSL with Latent Prior Probability Change,[0],[0]
{,4. DRSL with Latent Prior Probability Change,[0],[0]
"i | zi = s, 1 ≤",4. DRSL with Latent Prior Probability Change,[0],[0]
"i ≤ N} for 1 ≤ s ≤ S, which is a set of data indices belonging to latent category s. In our DRSL, users are responsible for specifying the groupings of training data points, i.e., {Gs}Ss=1.",4. DRSL with Latent Prior Probability Change,[0],[0]
"By specifying these groupings, the users incorporate their structural
6This particular decomposition holds when the Pearson (PE) divergence is used and δ is not so large.",4. DRSL with Latent Prior Probability Change,[0],[0]
Refer to Appendix E for the derivation.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Analogous decomposition can be also derived when other f -divergences are used.
assumptions on distribution shift into our DRSL.",4. DRSL with Latent Prior Probability Change,[0],[0]
We will discuss how this can be done in practice at the end of this section.,4. DRSL with Latent Prior Probability Change,[0],[0]
"For notational convenience, let ws ≡ w(s), 1 ≤ s ≤ S, and define w ≡",4. DRSL with Latent Prior Probability Change,[0],[0]
"(w1, . . .",4. DRSL with Latent Prior Probability Change,[0],[0]
", wS).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Equations (16) and (17) can be empirically approximated as follows using D′:
min θ sup w∈Ŵf
1 N
S∑
s=1
nswsR̂s(θ)
︸ ︷︷ ︸ ≡ R̂s-adv(θ)
(19)
Ŵf = { w ∈ RS ∣∣∣∣∣ 1 N S∑
s=1
nsf (ws) ≤ δ,
1 N
S∑
s=1
nsws = 1, w ≥ 0 } , (20)
where ns is the cardinality of Gs and R̂s(θ)(≡ 1 ns ∑ i∈Gs ℓi(θ)) is the average loss of all data points in Gs.",4. DRSL with Latent Prior Probability Change,[0],[0]
We call R̂s-adv(θ) the structural adversarial empirical risk and call the minimization problem of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(19) the structural adversarial empirical risk minimization (structural AERM).,4. DRSL with Latent Prior Probability Change,[0],[0]
We can add regularization term Ω(θ) to Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
"(19) to prevent overfitting.
",4. DRSL with Latent Prior Probability Change,[0],[0]
Convergence rate and estimation error: We establish the convergence rate of the model parameter and the order of the estimation error for structural AERM in terms of the number of training data points N .,4. DRSL with Latent Prior Probability Change,[0],[0]
"Due to the limited space, we only present an informal statement here.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The formal statement can be found in Appendix G and its proof can be found in Appendix H.
Theorem 4 (Convergence rate and estimation error, informal statement).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Let θ∗ be the solution of structural ARM, and θ̂N be the solution of regularized structural AERM given training data of size N .",4. DRSL with Latent Prior Probability Change,[0],[0]
"Assume gθ(x) is linear in θ, and regularization hyper-parameter λ decreases at a rate of O(N−1/2).",4. DRSL with Latent Prior Probability Change,[0],[0]
"Under mild conditions, as N → ∞, we have ∥θ̂N − θ∗∥2 = O(N−1/4) and consequently, |Rs-adv(θ̂N )−Rs-adv(θ∗)| = O(N−1/4).
",4. DRSL with Latent Prior Probability Change,[0],[0]
Notice that the convergence rate of θ̂N to θ∗ is not the optimal parametric rate O(N−1/2).,4. DRSL with Latent Prior Probability Change,[0],[0]
This is because the inner maximization of Eq.,4. DRSL with Latent Prior Probability Change,[0],[0]
(19) converges in O(N−1/4) that slows down the entire convergence rate.,4. DRSL with Latent Prior Probability Change,[0],[0]
"Theorem 4 applies to any f -divergence where f(t) is nonlinear in t, while knowing which f -divergence is used may improve the result to the optimal parametric rate.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Discussion on groupings: In our structural ARM and AERM, users need to incorporate their structural assumptions by grouping training data points.",4. DRSL with Latent Prior Probability Change,[0],[0]
"Here, we discuss how this can be done in practice.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Most straightforwardly, a user of our DRSL may assume
class prior change (Saerens et al., 2002) or sub-category7 prior change.",4. DRSL with Latent Prior Probability Change,[0],[0]
"To incorporate such assumptions into our DRSL, the user can simply group training data by class labels or a sub-categories, respectively.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"Alternatively, a user of our DRSL can group data by available meta-information of data such as time and places in which data are collected.",4. DRSL with Latent Prior Probability Change,[0],[0]
"The intuition is that data collected in the same situations (e.g., time and places) are likely to “share the same destiny” in the future distribution shift; hence, it is natural to assume that only the prior over the situations changes at test time while the conditionals remain the same.
",4. DRSL with Latent Prior Probability Change,[0],[0]
"In any case, it is crucial that the users provide structural assumptions on distribution shift so that we can overcome the pessimism of ARM and AERM for classification raised in Section 3.",4. DRSL with Latent Prior Probability Change,[0],[0]
"In this section, we derive efficient gradient-based learning algorithms for our structural AERM in Eq.",5. Efficient Learning Algorithms,[0],[0]
(19).,5. Efficient Learning Algorithms,[0],[0]
"Thanks to Danskin’s theorem (Danskin, 1966), we can obtain the gradient ∇θR̂s-adv(θ) as
∇θR̂s-adv(θ) = 1
N
S∑
s=1
nsw ∗ s∇θR̂s(θ), (21)
where w∗ = (w∗1 , . . .",5. Efficient Learning Algorithms,[0],[0]
", w∗S) is the solution of inner maximization of AERM in Eq.",5. Efficient Learning Algorithms,[0],[0]
"(19).
",5. Efficient Learning Algorithms,[0],[0]
"In the following, we show that w∗ can be obtained very efficiently for two well-known instances of f -divergences.
",5. Efficient Learning Algorithms,[0],[0]
"Kullback-Leibler (KL) divergence: For the KL divergence, f(x) = x log x, we have
w∗s = N
Z(γ) · exp ( R̂s(θ) γ ) , 1 ≤ s ≤ S, (22)
where γ is a scalar such that the first constraint of Ŵf in Eq.",5. Efficient Learning Algorithms,[0],[0]
"(20) holds with equality, and Z(γ) ≡∑S
s=1 nsexp ( R̂s(θ)/γ ) is a normalizing constant in or-
der to satisfy the second constraint of Ŵf .",5. Efficient Learning Algorithms,[0],[0]
"To compute γ, we can simply perform a binary search.
",5. Efficient Learning Algorithms,[0],[0]
"Pearson (PE) divergence: For the PE divergence, f(x) = (x − 1)2.",5. Efficient Learning Algorithms,[0],[0]
"For small δ, w ≥ 0 of Ŵf is often satisfied in practice.",5. Efficient Learning Algorithms,[0],[0]
"We drop the inequality for simplicity; then, the solution of the inner maximization of Eq.",5. Efficient Learning Algorithms,[0],[0]
"(19) becomes analytic and efficient to obtain:
w∗ =
√ Nδ
∑S s=1",5. Efficient Learning Algorithms,[0],[0]
"nsv 2 s v + 1S , (23)
",5. Efficient Learning Algorithms,[0],[0]
7A sub,5. Efficient Learning Algorithms,[0],[0]
"-category (Ristin et al., 2015) is a refined category of a class label, e.g., a “flu” label contains three sub-categories: types A, B, and C flu.
where 1S is the S-dimensional vector with all the elements equal to 1.",5. Efficient Learning Algorithms,[0],[0]
"v is the S-dimensional vector such that vs = R̂s(θ)− R̂(θ), 1 ≤ s ≤ S.
Computational complexity:",5. Efficient Learning Algorithms,[0],[0]
"The time complexity for obtaining w∗ is: O(mS) for the KL divergence and O(S) for the PE divergence, where m is the number of the binary search iterations to compute γ in Eq.",5. Efficient Learning Algorithms,[0],[0]
(22).,5. Efficient Learning Algorithms,[0],[0]
"Calculating the adversarial weights therefore adds negligible computational overheads to computing ∇ℓi(θ) and ℓi(θ) for 1 ≤ i ≤ N , which for example requires O(Nb)-time for a b-dimensional linear-in-parameter model.",5. Efficient Learning Algorithms,[0],[0]
"In this section, we experimentally analyze our DRSL (structural AERM) in classification by comparing it with ordinary ERM and DRSL with f -divergences (AERM).",6. Experiments,[0],[0]
"We empirically demonstrate (i) the undesirability of AERM in classification and (ii) the robustness of structural AERM against specified distribution shift.
",6. Experiments,[0],[0]
"Datasets: We obtained six classification datasets from the UCI repository (Blake & Merz, 1998), two of which are for multi-class classification.",6. Experiments,[0],[0]
"We also obtained MNIST (LeCun et al., 1998) and 20newsgroups (Lang, 1995).",6. Experiments,[0],[0]
"Refer to Appendix I for the details of the datasets.
",6. Experiments,[0],[0]
"Evaluation metrics: We evaluated the three methods (ordinary ERM, AERM and structural AERM) with three kinds of metrics: the ordinary risk, the adversarial risk, and the structural adversarial risk, where the 0-1 loss is used for all the metrics.8",6. Experiments,[0],[0]
"We did not explicitly report the adversarial risk in our experiments because of Theorem 1.
",6. Experiments,[0],[0]
Both the risk and structural adversarial risk are estimated using held-out test data.,6. Experiments,[0],[0]
"In particular, the structural adversarial risk can be estimated similarly to Eqs.",6. Experiments,[0],[0]
"(19) and (20), i.e., calculating the mis-classification rate on the held-out test data and structurally and adversarially reweight them.",6. Experiments,[0],[0]
See discussion of Eq.,6. Experiments,[0],[0]
"(18) for why the structural adversarial risk is a meaningful evaluation metric to measure distributional robustness of classifiers.
",6. Experiments,[0],[0]
"Experimental protocols: For our DRSL, we consider learning classifiers robust against (a) the class prior change and (b) the sub-category prior change.",6. Experiments,[0],[0]
"This corresponds to grouping training data by (a) class labels and (b) subcategory labels, respectively.",6. Experiments,[0],[0]
"In the benchmark datasets, the sub-category labels are not available.",6. Experiments,[0],[0]
"Hence, we manually created such labels as follows.",6. Experiments,[0],[0]
"First, we converted the original multi-class classification problems into classification problems with fewer classes by integrating some classes together.",6. Experiments,[0],[0]
"Then, the original class labels are regarded as the subcategories.",6. Experiments,[0],[0]
"In this way, we converted the satimage, letter and MNIST datasets into binary classification problems, and 20newsgroups into a 7-class classifica-
8To gain more insight on the methods, we also reported all the metrics in terms of the surrogate loss in Appendix K.
tion.",6. Experiments,[0],[0]
"Appendix J details how we grouped the class labels.
",6. Experiments,[0],[0]
"For all the methods, we used linear models with softmax output for the prediction function gθ(x).",6. Experiments,[0],[0]
The cross-entropy loss with ℓ2 regularization was adopted.,6. Experiments,[0],[0]
"The regularization hyper-parameter λ was selected from {1.0, 0.1, 0.01, 0.001, 0.0001} via 5-fold cross validation.
",6. Experiments,[0],[0]
We used the two f -divergences (the KL and PE divergences) and set δ = 0.5 for AERM and structural AERM.,6. Experiments,[0],[0]
The same δ and f -divergence were used for estimating the structural adversarial risk.,6. Experiments,[0],[0]
"At the end of this section, we discuss how we can choose δ in practice.",6. Experiments,[0],[0]
Results:,6. Experiments,[0],[0]
"In Table 1, we report experimental results on the classification tasks when the KL divergence is used.",6. Experiments,[0],[0]
"Refer to Appendix L for the results when the PE divergence is used, which showed similar tendency.
",6. Experiments,[0],[0]
We see from the left half of Table 1 that ordinary ERM achieved lower estimated risks as expected.,6. Experiments,[0],[0]
"On the other hand, we see from the entire Table 1 that AERM, which does not incorporate any structural assumptions on distribution shift, performed poorly in terms of both of two evaluation metrics; hence, it also performed poorly in terms of the adversarial risk (see Theorem 1).",6. Experiments,[0],[0]
This may be because AERM was excessively sensitive to outliers as implied by Theorem 3.,6. Experiments,[0],[0]
We see from the right half of Table 1 that structural AERM achieved significantly lower estimated structural adversarial risks.,6. Experiments,[0],[0]
"Although this was expected, our experiments confirmed that structural AERM indeed obtained classifiers robust against the structural distribution shift.9
9When we used the surrogate loss to evaluate the methods
Discussion: Here we provide an insight for users to determine δ in our DRSL (structural ARM and AERM).",6. Experiments,[0],[0]
We see from Eq.,6. Experiments,[0],[0]
"(18) that the structural adversarial risk can be decomposed into the sum of the ordinary risk and the sensitivity term, where δ acts as a trade-off hyper-parameter between the two terms.",6. Experiments,[0],[0]
"In practice, users of our DRSL may want to have good balance between the two terms, i.e., the learned classifier should achieve high accuracy on the training distribution while being robust to specified distribution shift.",6. Experiments,[0],[0]
"Since both terms in Eq. (18) can be estimated by cross validation, the users can adjust δ of AERM at training time to best trade-off the two terms for their purposes, e.g., increasing δ during training to decrease the sensitivity term at the expense of a slight increase of the risk term.",6. Experiments,[0],[0]
"In this paper, we theoretically analyzed DRSL with f - divergences applied to classification.",7. Conclusion,[0],[0]
"We showed that the DRSL ends up giving a classifier optimal for the training distribution, which is too pessimistic in terms of the original motivation of distributionally robust classification.",7. Conclusion,[0],[0]
"To rectify this, we presented simple DRSL that gives a robust classifier based on structural assumptions on distribution shift.",7. Conclusion,[0],[0]
We derived efficient optimization algorithms for our DRSL and empirically demonstrated its effectiveness.,7. Conclusion,[0],[0]
"(which is not the case in ordinary classification), we confirmed that the methods indeed achieved the best performance in terms of the metrics they optimized for, i.e., ERM, AERM, and structural AERM performed the best in terms of the ordinary risk, adversarial risk and structural adversarial risk, respectively.",7. Conclusion,[0],[0]
See Appendix K for the actual experimental results.,7. Conclusion,[0],[0]
We thank anonymous reviewers for their constructive feedback.,Acknowledgement,[0],[0]
WH was supported by JSPS KAKENHI 18J22289.,Acknowledgement,[0],[0]
MS was supported by CREST JPMJCR1403.,Acknowledgement,[0],[0]
Distributionally Robust Supervised Learning (DRSL) is necessary for building reliable machine learning systems.,abstractText,[0],[0]
"When machine learning is deployed in the real world, its performance can be significantly degraded because test data may follow a different distribution from training data.",abstractText,[0],[0]
DRSL with f -divergences explicitly considers the worst-case distribution shift by minimizing the adversarially reweighted training loss.,abstractText,[0],[0]
"In this paper, we analyze this DRSL, focusing on the classification scenario.",abstractText,[0],[0]
"Since the DRSL is explicitly formulated for a distribution shift scenario, we naturally expect it to give a robust classifier that can aggressively handle shifted distributions.",abstractText,[0],[0]
"However, surprisingly, we prove that the DRSL just ends up giving a classifier that exactly fits the given training distribution, which is too pessimistic.",abstractText,[0],[0]
This pessimism comes from two sources: the particular losses used in classification and the fact that the variety of distributions to which the DRSL tries to be robust is too wide.,abstractText,[0],[0]
"Motivated by our analysis, we propose simple DRSL that overcomes this pessimism and empirically demonstrate its effectiveness.",abstractText,[0],[0]
Does Distributionally Robust Supervised Learning Give Robust Classifiers?,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1077–1087 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1077",text,[0],[0]
"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (Imran et al., 2015).",1 Introduction,[0],[0]
"In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks.",1 Introduction,[0],[0]
"Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic dam-
age (Varga et al., 2013; Vieweg et al., 2014; Power et al., 2013).
",1 Introduction,[0],[0]
"In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time.",1 Introduction,[0],[0]
"Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining.",1 Introduction,[0],[0]
"However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).",1 Introduction,[0],[0]
"On the other hand, in most cases, we can have access to a good amount of labeled and abundant unlabeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event.",1 Introduction,[0],[0]
"In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to a new event (we refer to this as a target domain) without requiring any labeled data in the new event.",1 Introduction,[0],[0]
"In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.
",1 Introduction,[0],[0]
"Most recent approaches to semi-supervised learning (Yang et al., 2016) and domain adaptation (Ganin et al., 2016) use the automatic feature learning capability of DNN models.",1 Introduction,[0],[0]
"In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework.",1 Introduction,[0],[0]
"In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks – one for semi-supervised learning and the other for domain adaptation.",1 Introduction,[0],[0]
"The semisupervised component learns internal representa-
tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances.",1 Introduction,[0],[0]
"The domain adaptation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to distinguish the domains.",1 Introduction,[0],[0]
"The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.",1 Introduction,[0],[0]
"We propose a stochastic gradient descent (SGD) algorithm to train the components of our model simultaneously.
",1 Introduction,[0],[0]
The evaluation of our proposed model is conducted using two Twitter datasets on scenarios where there is only unlabeled data in the target domain.,1 Introduction,[0],[0]
"Our results demonstrate the following.
1.",1 Introduction,[0],[0]
"When the network combines the semisupervised component with the supervised component, depending on the amount of labeled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.
2.",1 Introduction,[0],[0]
"Domain adaptation with adversarial training improves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1.
3.",1 Introduction,[0],[0]
"When the network combines domain adversarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.
",1 Introduction,[0],[0]
"Our source code is available on Github1 and the data is available on CrisisNLP2.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph embedding learning.",1 Introduction,[0],[0]
"In Section 3, we present the experimental setup and baselines.",1 Introduction,[0],[0]
The results and analysis are presented in Section 4.,1 Introduction,[0],[0]
"In Section 5, we present the works relevant to this study.",1 Introduction,[0],[0]
"Finally, conclusions appear in Section 6.",1 Introduction,[0],[0]
We demonstrate our approach for domain adaptation with adversarial training and graph embedding on a tweet classification task to support crisis response efforts.,2 The Model,[0],[0]
"Let DlS = {ti, yi} Ls i=1 and DuS = {ti} Us i=1",2 The Model,[0],[0]
"be the set of labeled and unlabeled tweets for a source crisis event S (e.g., 1https://github.com/firojalam/ domain-adaptation 2http://crisisnlp.qcri.org
Nepal earthquake), where yi ∈ {1, . . .",2 The Model,[0],[0]
",K} is the class label for tweet ti, Ls and Us are the number of labeled and unlabeled tweets for the source event, respectively.",2 The Model,[0],[0]
"In addition, we have unlabeled tweets DuT = {ti} Ut i=1 for a target event T (e.g., Queensland flood) with Ut being the number of unlabeled tweets in the target domain.",2 The Model,[0],[0]
"Our ultimate goal is to train a cross-domain model p(y|t, θ) with parameters θ that can classify any tweet in the target event T without having any information about class labels in T .
",2 The Model,[0],[0]
Figure 1 shows the overall architecture of our neural model.,2 The Model,[0],[0]
The input to the network is a tweet t =,2 The Model,[0],[0]
"(w1, . . .",2 The Model,[0],[0]
", wn) containing words that come from a finite vocabulary V defined from the training set.",2 The Model,[0],[0]
The first layer of the network maps each of these words into a distributed representation Rd by looking up a shared embedding matrix E ∈ R|V |×d.,2 The Model,[0],[0]
We initialize the embedding matrix E in our network with word embeddings that are pretrained on a large crisis dataset (Subsection 2.5).,2 The Model,[0],[0]
"However, embedding matrix E can also be initialize randomly.",2 The Model,[0],[0]
"The output of the look-up layer is a matrix X ∈ Rn×d, which is passed through a number of convolution and pooling layers to learn higher-level feature representations.",2 The Model,[0],[0]
"A convolution operation applies a filter u ∈ Rk.d to a window of k vectors to produce a new feature ht as
ht = f(u.",2 The Model,[0],[0]
"Xt:t+k−1) (1)
where Xt:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU.",2 The Model,[0],[0]
"We apply this filter to each possible k-length windows in X with stride size of 1 to generate a feature map hj as:
hj =",2 The Model,[0],[0]
"[h1, . . .",2 The Model,[0],[0]
", hn+k−1] (2)
",2 The Model,[0],[0]
We repeat this process N times with N different filters to get N different feature maps.,2 The Model,[0],[0]
"We use a wide convolution (Kalchbrenner et al., 2014), which ensures that the filters reach the entire tweet, including the boundary words.",2 The Model,[0],[0]
"This is done by performing zero-padding, where out-ofrange (i.e., t<1 or t>n) vectors are assumed to be zero.",2 The Model,[0],[0]
"With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features.",2 The Model,[0],[0]
"After the convolution, we apply a max-pooling operation to each of the feature maps,
m = [µp(h 1), · · · , µp(hN )]",2 The Model,[0],[0]
"(3)
where µp(hj) refers to the max operation applied to each window of p features with stride size of 1 in the feature map hi.",2 The Model,[0],[0]
"Intuitively, the convolution operation composes local features into higherlevel representations in the feature maps, and maxpooling extracts the most important aspects of each feature map while reducing the output dimensionality.",2 The Model,[0],[0]
"Since each convolution-pooling operation is performed independently, the features extracted become invariant in order (i.e., where they occur in the tweet).",2 The Model,[0],[0]
"To incorporate order information between the pooled features, we include a fully-connected (dense) layer
z = f(Vm) (4)
where V is the weight matrix.",2 The Model,[0],[0]
"We choose a convolutional architecture for feature composition because it has shown impressive results on similar tasks in a supervised setting (Nguyen et al., 2017).
",2 The Model,[0],[0]
"The network at this point splits into three branches (shaded with three different colors in Figure 1) each of which serves a different purpose and contributes a separate loss to the overall loss of the model as defined below:
L(Λ,Φ,Ω,Ψ) =",2 The Model,[0],[0]
"LC(Λ,Φ) + λgLG(Λ,Ω) + λdLD(Λ,Ψ) (5)
where Λ = {U, V } are the convolutional filters and dense layer weights that are shared across the three branches.",2 The Model,[0],[0]
"The first componentLC(Λ,Φ) is a supervised classification loss based on the labeled data in the source event.",2 The Model,[0],[0]
"The second component LG(Λ,Ω) is a graph-based semi-supervised loss that utilizes both labeled and unlabeled data in the
source and target events to induce structural similarity between training instances.",2 The Model,[0],[0]
"The third component LD(Λ,Ω) is an adversary loss that again uses all available data in the source and target domains to induce domain invariance in the learned features.",2 The Model,[0],[0]
The tunable hyperparameters λg and λd control the relative strength of the components.,2 The Model,[0],[0]
"The supervised component induces label information (e.g., relevant vs. non-relevant) directly in the network through the classification loss LC(Λ,Φ), which is computed on the labeled instances in the source event, DlS .",2.1 Supervised Component,[0],[0]
"Specifically, this branch of the network, as shown at the top in Figure 1, takes the shared representations z as input and pass it through a task-specific dense layer
zc = f(Vcz) (6)
where Vc is the corresponding weight matrix.",2.1 Supervised Component,[0],[0]
The activations zc along with the activations from the semi-supervised branch zs are used for classification.,2.1 Supervised Component,[0],[0]
"More formally, the classification layer defines a Softmax
p(y = k|t, θ) = exp
( W Tk [zc; zs] )∑ k′ exp ( W Tk′",2.1 Supervised Component,[0],[0]
"[zc; zs]
) (7) where [.; .] denotes concatenation of two column vectors, Wk are the class weights, and θ = {U, V, Vc,W} defines the relevant parameters for this branch of the network with Λ = {U, V } being the shared parameters and Φ = {Vc,W} being the parameters specific to this branch.",2.1 Supervised Component,[0],[0]
"Once learned,
we use θ for prediction on test tweets.",2.1 Supervised Component,[0],[0]
"The classification loss LC(Λ,Φ) (or LC(θ)) is defined as
LC(Λ,Φ) =",2.1 Supervised Component,[0],[0]
"− 1
Ls Ls∑ i=1",2.1 Supervised Component,[0],[0]
"I(yi = k) log p(yi = k|ti,Λ,Φ) (8)
where I(.) is an indicator function that returns 1 when the argument is true, otherwise it returns 0.",2.1 Supervised Component,[0],[0]
The semi-supervised branch (shown at the middle in Figure 1) induces structural similarity between training instances (labeled or unlabeled) in the source and target events.,2.2 Semi-supervised Component,[0],[0]
"We adopt the recently proposed graph-based semi-supervised deep learning framework (Yang et al., 2016), which shows impressive gains over existing semisupervised methods on multiple datasets.",2.2 Semi-supervised Component,[0],[0]
"In this framework, a “similarity” graph G first encodes relations between training instances, which is then used by the network to learn internal representations (i.e., embeddings).",2.2 Semi-supervised Component,[0],[0]
The semi-supervised branch takes the shared representation z as input and learns internal representations by predicting a node in the graph context of the input tweet.,2.2.1 Learning Graph Embeddings,[0],[0]
"Following (Yang et al., 2016), we use negative sampling to compute the loss for predicting the context node, and we sample two types of contextual nodes: (i) one is based on the graph G to encode structural information, and (ii) the second is based on the labels in DlS to incorporate label information through this branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"The ratio of positive and negative samples is controlled by a random variable ρ1 ∈ (0, 1), and the proportion of the two context types is controlled by another random variable ρ2 ∈ (0, 1); see Algorithm 1 of (Yang et al., 2016) for details on the sampling procedure.
",2.2.1 Learning Graph Embeddings,[0],[0]
"Let (j, γ) is a tuple sampled from the distribution p(j, γ|i,DlS , G), where j is a context node of an input node i and γ ∈ {+1,−1} denotes whether it is a positive or a negative sample; γ = +1 if ti and tj are neighbors in the graph (for graph-based context) or they both have same labels (for label-based context), otherwise γ = −1.",2.2.1 Learning Graph Embeddings,[0],[0]
"The negative log loss for context prediction LG(Λ,Ω) can be written as
LG(Λ,Ω) =",2.2.1 Learning Graph Embeddings,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.2.1 Learning Graph Embeddings,[0],[0]
"E(j,γ) log σ",2.2.1 Learning Graph Embeddings,[0],[0]
( γCTj zg(i) ),2.2.1 Learning Graph Embeddings,[0],[0]
"(9)
where zg(i) = f(Vgz(i)) defines another dense layer (marked as Dense (zg) in Figure 1) having weights Vg, and Cj is the weight vector associated with the context node tj .",2.2.1 Learning Graph Embeddings,[0],[0]
"Note that here Λ = {U, V } defines the shared parameters and Ω = {Vg, C} defines the parameters specific to the semi-supervised branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"Typically graphs are constructed based on a relational knowledge source, e.g., citation links in (Lu and Getoor, 2003), or distance between instances (Zhu, 2005).",2.2.2 Graph Construction,[0],[0]
"However, we do not have access to such a relational knowledge in our setting.",2.2.2 Graph Construction,[0],[0]
"On the other hand, computing distance between n(n−1)/2 pairs of instances to construct the graph is also very expensive (Muja and Lowe, 2014).",2.2.2 Graph Construction,[0],[0]
"Therefore, we choose to use k-nearest neighborbased approach as it has been successfully used in other study (Steinbach et al., 2000).
",2.2.2 Graph Construction,[0],[0]
"The nearest neighbor graph consists of n vertices and for each vertex, there is an edge set consisting of a subset of n instances, i.e., tweets in our training set.",2.2.2 Graph Construction,[0],[0]
"The edge is defined by the distance measure d(i, j) between tweets ti and tj , where the value of d represents how similar the two tweets are.",2.2.2 Graph Construction,[0],[0]
"We used k-d tree data structure (Bentley, 1975) to efficiently find the nearest instances.",2.2.2 Graph Construction,[0],[0]
"To construct the graph, we first represent each tweet by averaging the word2vec vectors of its words, and then we measure d(i, j) by computing the Euclidean distance between the vectors.",2.2.2 Graph Construction,[0],[0]
The number of nearest neighbor k was set to 10.,2.2.2 Graph Construction,[0],[0]
The reason of averaging the word vectors is that it is computationally simpler and it captures the relevant semantic information for our task in hand.,2.2.2 Graph Construction,[0],[0]
"Likewise, we choose to use Euclidean distance instead of cosine for computational efficiency.",2.2.2 Graph Construction,[0],[0]
The network described so far can learn abstract features through convolutional and dense layers that are discriminative for the classification task (relevant vs. non-relevant).,2.3 Domain Adversarial Component,[0],[0]
"The supervised branch of the network uses labels in the source event to induce label information directly, whereas the semi-supervised branch induces similarity information between labeled and unlabeled instances.",2.3 Domain Adversarial Component,[0],[0]
"However, our goal is also to make these learned features invariant across domains or events (e.g., Nepal Earthquake vs. Queensland Flood).",2.3 Domain Adversarial Component,[0],[0]
"We achieve this by domain adversarial training of
neural networks (Ganin et al., 2016).",2.3 Domain Adversarial Component,[0],[0]
"We put a domain discriminator, another branch in the network (shown at the bottom in Figure 1) that takes the shared internal representation z as input, and tries to discriminate between the domains of the input — in our case, whether the input tweet is from DS or from DT .",2.3 Domain Adversarial Component,[0],[0]
"The domain discriminator is defined by a sigmoid function:
δ̂ = p(d = 1|t,Λ,Ψ) = sigm(wTd",2.3 Domain Adversarial Component,[0],[0]
"zd) (10)
where d ∈ {0, 1} denotes the domain of the input tweet t, wd are the final layer weights of the discriminator, and zd = f(Vdz) defines the hidden layer of the discriminator with layer weights Vd.",2.3 Domain Adversarial Component,[0],[0]
"Here Λ = {U, V } defines the shared parameters, and Ψ = {Vd,wd} defines the parameters specific to the domain discriminator.",2.3 Domain Adversarial Component,[0],[0]
"We use the negative log-probability as the discrimination loss:
Ji(Λ,Ψ) = −di log δ̂ − (1− di) log ( 1− δ̂ ) (11)
",2.3 Domain Adversarial Component,[0],[0]
"We can write the overall domain adversary loss over the source and target domains as
LD(Λ,Ψ) =",2.3 Domain Adversarial Component,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ)− 1 Ut Ut∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ) (12)
where Ls + Us and Ut are the number of training instances in the source and target domains, respectively.",2.3 Domain Adversarial Component,[0],[0]
"In adversarial training, we seek parameters (saddle point) such that
θ∗ = argmin Λ,Φ,Ω max Ψ L(Λ,Φ,Ω,Ψ) (13)
which involves a maximization with respect to Ψ and a minimization with respect to {Λ,Φ,Ω}.",2.3 Domain Adversarial Component,[0],[0]
"In other words, the updates of the shared parameters Λ = {U, V } for the discriminator work adversarially to the rest of the network, and vice versa.",2.3 Domain Adversarial Component,[0],[0]
"This is achieved by reversing the gradients of the discrimination loss LD(Λ,Ψ), when they are backpropagated to the shared layers (see Figure 1).",2.3 Domain Adversarial Component,[0],[0]
Algorithm 1 illustrates the training algorithm based on stochastic gradient descent (SGD).,2.4 Model Training,[0],[0]
We first initialize the model parameters.,2.4 Model Training,[0],[0]
"The word embedding matrixE is initialized with pre-trained word2vec vectors (see Subsection 2.5) and is kept fixed during training.3 Other parameters are initialized with small random numbers sampled from
3Tuning E on our task by backpropagation increased the training time immensely (3 days compared to 5 hours on a Tesla GPU) without any significant performance gain.
",2.4 Model Training,[0],[0]
"Algorithm 1: Model Training with SGD Input : data DlS , DuS , DuT ; graph G Output: learned parameters θ = {Λ,Φ} 1.",2.4 Model Training,[0],[0]
"Initialize model parameters {E,Λ,Φ,Ω,Ψ}; 2. repeat
//",2.4 Model Training,[0],[0]
"Semi-supervised for each batch sampled from p(j, γ|i,DlS , G) do a) Compute loss LG(Λ,Ω) b)",2.4 Model Training,[0],[0]
"Take a gradient step for LG(Λ,Ω); end //",2.4 Model Training,[0],[0]
"Supervised & domain adversary for each batch sampled from DlS do a) Compute LC(Λ,Φ) and LD(Λ,Ψ) b)",2.4 Model Training,[0],[0]
"Take gradient steps for LC(Λ,Φ) and LD(Λ,Ψ); end // Domain adversary for each batch sampled from DuS and DuT do
a) Compute LD(Λ,Ψ) b) Take a gradient step for LD(Λ,Ψ);
end until convergence;
a uniform distribution (Bengio and Glorot, 2010).",2.4 Model Training,[0],[0]
"We use AdaDelta (Zeiler, 2012) adaptive update to update the parameters.
",2.4 Model Training,[0],[0]
"In each iteration, we do three kinds of gradient updates to account for the three different loss components.",2.4 Model Training,[0],[0]
"First, we do an epoch over all the training instances updating the parameters for the semi-supervised loss, then we do an epoch over the labeled instances in the source domain, each time updating the parameters for the supervised and the domain adversary losses.",2.4 Model Training,[0],[0]
"Finally, we do an epoch over the unlabeled instances in the two domains to account for the domain adversary loss.
",2.4 Model Training,[0],[0]
The main challenge in adversarial training is to balance the competing components of the network.,2.4 Model Training,[0],[0]
"If one component becomes smarter than the other, its loss to the shared layer becomes useless, and the training fails to converge (Arjovsky et al., 2017).",2.4 Model Training,[0],[0]
"Equivalently, if one component becomes weaker, its loss overwhelms that of the other, causing the training to fail.",2.4 Model Training,[0],[0]
"In our experiments, we observed the domain discriminator is weaker than the rest of the network.",2.4 Model Training,[0],[0]
"This could be due to the noisy nature of tweets, which makes the job for the domain discriminator harder.",2.4 Model Training,[0],[0]
"To balance the components, we would want the error signals from the discriminator to be fairly weak, also we would want the supervised loss to have more impact than the semi-supervised loss.",2.4 Model Training,[0],[0]
"In our experiments, the weight of the domain adversary loss λd was fixed to 1e",2.4 Model Training,[0],[0]
"− 8, and the weight of the semi-supervised loss λg was fixed to 1e",2.4 Model Training,[0],[0]
− 2.,2.4 Model Training,[0],[0]
"Other sophisticated weighting schemes have been proposed recently
(Ganin et al., 2016; Arjovsky et al., 2017; Metz et al., 2016).",2.4 Model Training,[0],[0]
"It would be interesting to see how our model performs using these advanced tuning methods, which we leave as a future work.",2.4 Model Training,[0],[0]
"As mentioned, we used word embeddings that are pre-trained on a crisis dataset.",2.5 Crisis Word Embedding,[0],[0]
"To train the wordembedding model, we first pre-processed tweets collected using the AIDR system (Imran et al., 2014) during different events occurred between 2014 and 2016.",2.5 Crisis Word Embedding,[0],[0]
"In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol.",2.5 Crisis Word Embedding,[0],[0]
"After preprocessing, the resulting dataset contains about 364 million tweets and about 3 billion words.
",2.5 Crisis Word Embedding,[0],[0]
"There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec (Mikolov et al., 2013), and Glove (Pennington et al., 2014).",2.5 Crisis Word Embedding,[0],[0]
"For our work, we trained the CBOW model from word2vec.",2.5 Crisis Word Embedding,[0],[0]
"While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples.",2.5 Crisis Word Embedding,[0],[0]
The resulting embedding model contains about 2 million words with vector dimensions of 300.,2.5 Crisis Word Embedding,[0],[0]
"In this section, we describe our experimental settings – datasets used, settings of our models, compared baselines, and evaluation metrics.",3 Experimental Settings,[0],[0]
"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL).",3.1 Datasets,[0],[0]
"These datasets are comprised of millions of tweets collected through the Twitter streaming API4 using event-specific keywords/hashtags.
",3.1 Datasets,[0],[0]
To obtain the labeled examples for our task we employed paid workers from the Crowdflower5 – a crowdsourcing platform.,3.1 Datasets,[0],[0]
The annotation consists of two classes relevant and non-relevant.,3.1 Datasets,[0],[0]
"For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively.",3.1 Datasets,[0],[0]
"Given a
4https://dev.twitter.com/streaming/overview 5http://crowdflower.com
tweet, we asked crowdsourcing workers to assign the “relevant” label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise assign the “non-relevant” label.",3.1 Datasets,[0],[0]
"We split the labeled data into 60% as training, 30% as test and 10% as development.",3.1 Datasets,[0],[0]
Table 1 shows the resulting datasets with class-wise distributions.,3.1 Datasets,[0],[0]
Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5).,3.1 Datasets,[0],[0]
"In all the experiments, the classification task consists of two classes: relevant and non-relevant.",3.1 Datasets,[0],[0]
"In order to demonstrate the effectiveness of our joint learning approach, we performed a series of experiments.",3.2 Model Settings and Baselines,[0],[0]
"To understand the contribution of different network components, we performed an ablation study showing how the model performs as a semi-supervised model alone and as a domain adaptation model alone, and then we compare them with the combined model that incorporates all the components.",3.2 Model Settings and Baselines,[0],[0]
"As a baseline for the semi-supervised experiments, we used the self-training approach (Scudder, 1965).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For this purpose, we first trained a supervised model using the CNN architecture (i.e., shared components followed by the supervised part in Figure 1).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
The trained model was then used to automatically label the unlabeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Instances with a classifier confidence score ≥ 0.75 were then used to retrain a new model.
",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Next, we run experiments using our graphbased semi-supervised approach (i.e., shared components followed by the supervised and semisupervised parts in Figure 1), which exploits unlabeled data.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For reducing the computational cost, we randomly selected 50K unlabeled instances from the same domain.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For our semi-supervised setting, one of the main goals was to understand how much labeled data is sufficient to obtain a
reasonable result.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Therefore, we experimented our system by incrementally adding batches of instances, such as 100, 500, 2000, 5000, and all instances from the training set.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
Such an understanding can help us design the model at the onset of a crisis event with sufficient amount of labeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To demonstrate that the semi-supervised approach outperforms the supervised baseline, we run supervised experiments using the same number of labeled instances.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"In the supervised setting, only zc activations in Figure 1 are used for classification.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To set a baseline for the domain adaptation experiments, we train a CNN model (i.e., shared components followed by the supervised part in Figure 1) on one event (source) and test it on another event (target).",3.2.2 Settings for Domain Adaptation,[0],[0]
"We call this as transfer baseline.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"To assess the performance of our domain adaptation technique alone, we exclude the semisupervised component from the network.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We train and evaluate models with this network configuration using different source and target domains.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"Finally, we integrate all the components of the network as shown in Figure 1 and run domain adaptation experiments using different source and target domains.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In all our domain adaptation experiments, we only use unlabeled instances from the target domain.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In domain adaption literature, this is known as unsupervised adaptation.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pooling length of 2, 3, and 4, respectively.",3.2.3 Training Settings,[0],[0]
We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outperform the baselines or not.,3.2.3 Training Settings,[0],[0]
"Furthermore, we do not filter out any vocabulary item in any settings.
",3.2.3 Training Settings,[0],[0]
"As mentioned before in Subsection 2.4, we used AdaDelta (Zeiler, 2012) to update the model parameters in each SGD step.",3.2.3 Training Settings,[0],[0]
The learning rate was set to 0.1 when optimizing on the classification loss and to 0.001 when optimizing on the semisupervised loss.,3.2.3 Training Settings,[0],[0]
The learning rate for domain adversarial training was set to 1.0.,3.2.3 Training Settings,[0],[0]
"The maximum number of epochs was set to 200, and dropout rate of 0.02 was used to avoid overfitting (Srivastava et al., 2014).",3.2.3 Training Settings,[0],[0]
"We used validation-based early stopping using the F-measure with a patience of 25,
Experiments AUC P R F1
NEPAL EARTHQUAKE
Supervised 61.22 62.42 62.31 60.89
Semi-supervised (Self-training) 61.15 61.53 61.53 61.26
Semi-supervised (Graph-based) 64.81 64.58 64.63 65.11
QUEENSLAND FLOODS
i.e., we stop training if the score does not increase for 25 consecutive epochs.",3.2.3 Training Settings,[0],[0]
"To measure the performance of the trained models using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and machine learning communities.",3.2.4 Evaluation Metrics,[0],[0]
The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.,3.2.4 Evaluation Metrics,[0],[0]
"In this section, we present the experimental results and discuss our main findings.",4 Results and Discussion,[0],[0]
"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets.",4.1 Semi-supervised Learning,[0],[0]
It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines – supervised and self-training based semi-supervised.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, the graph-based approach shows 4% to 13% absolute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively.
To determine how the semi-supervised approach performs in the early hours of an event when only fewer labeled instances are available, we mimic a batch-wise (not to be confused with minibatch in SGD) learning setting.",4.1 Semi-supervised Learning,[0],[0]
"In Table 3, we present the results using different batch sizes – 100, 500, 1,000, 2,000, and all labels.
",4.1 Semi-supervised Learning,[0],[0]
"From the results, we observe that models’ performance improve as we include more labeled data
Exp.",4.1 Semi-supervised Learning,[0],[0]
100 500 1000 2000,4.1 Semi-supervised Learning,[0],[0]
"All L
NEPAL EARTHQUAKE
L 43.63 52.89 56.37 60.11 60.89
L+50kU 52.32 59.95 61.89 64.05 65.11
QUEENSLAND FLOOD
— from 43.63 to 60.89 for NEQ and from 48.97 to 80.16 for QFL in the case of labeled only (L).",4.1 Semi-supervised Learning,[0],[0]
"When we compare supervised vs. semi-supervised (L vs. L+U), we observe significant improvements in F1 scores for the semi-supervised model for all batches over the two datasets.",4.1 Semi-supervised Learning,[0],[0]
"As we include unlabeled instances with labeled instances from the same event, performance significantly improves in each experimental setting giving 5% to 26% absolute improvements over the supervised models.",4.1 Semi-supervised Learning,[0],[0]
These improvements demonstrate the effectiveness of our approach.,4.1 Semi-supervised Learning,[0],[0]
We also notice that our semi-supervised approach can perform above 90% depending on the event.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, major improvements are observed from batch size 100 to 1,000, however, after that the performance improvements are comparatively minor.",4.1 Semi-supervised Learning,[0],[0]
"The results obtained using batch sizes 500 and 1,000 are reasonably in the acceptable range when labeled and unlabeled instances are combined (i.e., L+50kU for Nepal and L+∼21kU for Queensland), which is also a reasonable number of training examples to obtain at the onset of an event.",4.1 Semi-supervised Learning,[0],[0]
"In Table 4, we present domain adaptation results.",4.2 Domain Adaptation,[0],[0]
"The first block shows event-specific (i.e., train and test on the same event)",4.2 Domain Adaptation,[0],[0]
results for the supervised CNN model.,4.2 Domain Adaptation,[0],[0]
These results set the upper bound for our domain adaptation methods.,4.2 Domain Adaptation,[0],[0]
"The transfer baselines are shown in the next block, where we train a CNN model in one domain and test it on a different domain.",4.2 Domain Adaptation,[0],[0]
"Then, the third block shows the results for the domain adversarial approach without the semi-supervised loss.",4.2 Domain Adaptation,[0],[0]
These results show the importance of domain adversarial component.,4.2 Domain Adaptation,[0],[0]
"After that, the fourth block presents the performance of the model trained with graph
Source Target AUC P R F1
",4.2 Domain Adaptation,[0],[0]
"IN-DOMAIN SUPERVISED MODEL
embedding without domain adaptation to show the importance of semi-supervised learning.",4.2 Domain Adaptation,[0],[0]
"The final block present the results for the complete model that includes all the loss components.
",4.2 Domain Adaptation,[0],[0]
The results with domain adversarial training show improvements across both events – from 1.8% to 4.1% absolute gains in F1.,4.2 Domain Adaptation,[0],[0]
"These results attest that adversarial training is an effective approach to induce domain invariant features in the internal representation as shown previously by Ganin et al. (2016).
",4.2 Domain Adaptation,[0],[0]
"Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.",4.2 Domain Adaptation,[0],[0]
"From these improvements, we can conclude that domain adaptation with adversarial training along with graphbased semi-supervised learning is an effective method to leverage unlabeled and labeled data from a different domain.
",4.2 Domain Adaptation,[0],[0]
"Note that for our domain adaptation methods, we only use unlabeled data from the target domain.",4.2 Domain Adaptation,[0],[0]
"Hence, we foresee future improvements of this approach by utilizing a small amount of target domain labeled data.",4.2 Domain Adaptation,[0],[0]
Two lines of research are directly related to our work: (i) semi-supervised learning and (ii) domain adaptation.,5 Related Work,[0],[0]
Several models have been proposed for semi-supervised learning.,5 Related Work,[0],[0]
"The earliest approach is self-training (Scudder, 1965), in
which a trained model is first used to label unlabeled data instances followed by the model retraining with the most confident predicted labeled instances.",5 Related Work,[0],[0]
"The co-training (Mitchell, 1999) approach assumes that features can be split into two sets and each subset is then used to train a classifier with an assumption that the two sets are conditionally independent.",5 Related Work,[0],[0]
"Then each classifier classifies the unlabeled data, and then most confident data instances are used to re-train the other classifier, this process repeats multiple times.
",5 Related Work,[0],[0]
"In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances and edge weights represent the similarity between them.",5 Related Work,[0],[0]
"The structural information encoded in the graph is then used to regularize a model (Zhu, 2005).",5 Related Work,[0],[0]
"There are two paradigms in semi-supervised learning: 1) inductive – learning a function with which predictions can be made on unobserved instances, 2) transductive – no explicit function is learned and predictions can only be made on observed instances.",5 Related Work,[0],[0]
"As mentioned before, inductive semi-supervised learning is preferable over the transductive approach since it avoids building the graph each time it needs to infer the labels for the unlabeled instances.
",5 Related Work,[0],[0]
"In our work, we use a graph-based inductive deep learning approach proposed by Yang et al. (2016) to learn features in a deep learning model by predicting contextual (i.e., neighboring) nodes in the graph.",5 Related Work,[0],[0]
"However, our approach is different from Yang et al. (2016) in several ways.",5 Related Work,[0],[0]
"First, we construct the graph by computing the distance between tweets based on word embeddings.",5 Related Work,[0],[0]
"Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed representation of the words in a tweet.",5 Related Work,[0],[0]
"Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph.",5 Related Work,[0],[0]
"Similar similarity-based graph has shown impressive results in learning sentence representations (Saha et al., 2017).
",5 Related Work,[0],[0]
"In the literature, the proposed approaches for domain adaptation include supervised, semisupervised and unsupervised.",5 Related Work,[0],[0]
"It also varies from linear kernelized approach (Blitzer et al., 2006) to non-linear deep neural network techniques (Glorot et al., 2011; Ganin et al., 2016).",5 Related Work,[0],[0]
"One direction of research is to focus on feature space distribution matching by reweighting the samples from
the source domain (Gong et al., 2013) to map source into target.",5 Related Work,[0],[0]
The overall idea is to learn a good feature representation that is invariant across domains.,5 Related Work,[0],[0]
"In the deep learning paradigm, Glorot et al. (Glorot et al., 2011) used Stacked Denoising Auto-Encoders (SDAs) for domain adaptation.",5 Related Work,[0],[0]
"SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise.",5 Related Work,[0],[0]
"Adversarial training of neural networks has shown big impact recently, especially in areas such as computer vision, where generative unsupervised models have proved capable of synthesizing new images (Goodfellow et al., 2014; Radford et al., 2015; Makhzani et al., 2015).",5 Related Work,[0],[0]
"Ganin et al. (2016) proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target.",5 Related Work,[0],[0]
"We extend this work by combining with semi-supervised graph embedding for unsupervised domain adaptation.
",5 Related Work,[0],[0]
"In a recent work, Kipf and Welling (2016) present CNN applied directly on graph-structured datasets - citation networks and on a knowledge graph dataset.",5 Related Work,[0],[0]
Their study demonstrate that graph convolution network for semi-supervised classification performs better compared to other graph based approaches.,5 Related Work,[0],[0]
"In this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events.",6 Conclusions,[0],[0]
"We use a convolutional neural network to compose high-level representation from the input, which is then passed to three components that perform supervised training, semisupervised learning and domain adversarial training.",6 Conclusions,[0],[0]
"For domain adaptation, we considered a scenario, where we have only unlabeled data in the target event.",6 Conclusions,[0],[0]
"Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines.",6 Conclusions,[0],[0]
We have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.,6 Conclusions,[0],[0]
The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data.,abstractText,[0],[0]
"However, obtaining labeled data is a big challenge in many real-world problems.",abstractText,[0],[0]
"In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains.",abstractText,[0],[0]
"In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake).",abstractText,[0],[0]
"For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event.",abstractText,[0],[0]
We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework.,abstractText,[0],[0]
Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.,abstractText,[0],[0]
Domain Adaptation with Adversarial Training and Graph Embeddings,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1077–1087 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
1077",text,[0],[0]
"The application that motivates our work is the time-critical analysis of social media (Twitter) data at the sudden-onset of an event like natural or man-made disasters (Imran et al., 2015).",1 Introduction,[0],[0]
"In such events, affected people post timely and useful information of various types such as reports of injured or dead people, infrastructure damage, urgent needs (e.g., food, shelter, medical assistance) on these social networks.",1 Introduction,[0],[0]
"Humanitarian organizations believe timely access to this important information from social networks can help significantly and reduce both human loss and economic dam-
age (Varga et al., 2013; Vieweg et al., 2014; Power et al., 2013).
",1 Introduction,[0],[0]
"In this paper, we consider the basic task of classifying each incoming tweet during a crisis event (e.g., Earthquake) into one of the predefined classes of interest (e.g., relevant vs. nonrelevant) in real-time.",1 Introduction,[0],[0]
"Recently, deep neural networks (DNNs) have shown great performance in classification tasks in NLP and data mining.",1 Introduction,[0],[0]
"However the success of DNNs on a task depends heavily on the availability of a large labeled dataset, which is not a feasible option in our setting (i.e., classifying tweets at the onset of an Earthquake).",1 Introduction,[0],[0]
"On the other hand, in most cases, we can have access to a good amount of labeled and abundant unlabeled data from past similar events (e.g., Floods) and possibly some unlabeled data for the current event.",1 Introduction,[0],[0]
"In such situations, we need methods that can leverage the labeled and unlabeled data in a past event (we refer to this as a source domain), and that can adapt to a new event (we refer to this as a target domain) without requiring any labeled data in the new event.",1 Introduction,[0],[0]
"In other words, we need models that can do domain adaptation to deal with the distribution drift between the domains and semi-supervised learning to leverage the unlabeled data in both domains.
",1 Introduction,[0],[0]
"Most recent approaches to semi-supervised learning (Yang et al., 2016) and domain adaptation (Ganin et al., 2016) use the automatic feature learning capability of DNN models.",1 Introduction,[0],[0]
"In this paper, we extend these methods by proposing a novel model that performs domain adaptation and semi-supervised learning within a single unified deep learning framework.",1 Introduction,[0],[0]
"In this framework, the basic task-solving network (a convolutional neural network in our case) is put together with two other networks – one for semi-supervised learning and the other for domain adaptation.",1 Introduction,[0],[0]
"The semisupervised component learns internal representa-
tions (features) by predicting contextual nodes in a graph that encodes similarity between labeled and unlabeled training instances.",1 Introduction,[0],[0]
"The domain adaptation is achieved by training the feature extractor (or encoder) in adversary with respect to a domain discriminator, a binary classifier that tries to distinguish the domains.",1 Introduction,[0],[0]
"The overall idea is to learn high-level abstract representation that is discriminative for the main classification task, but is invariant across the domains.",1 Introduction,[0],[0]
"We propose a stochastic gradient descent (SGD) algorithm to train the components of our model simultaneously.
",1 Introduction,[0],[0]
The evaluation of our proposed model is conducted using two Twitter datasets on scenarios where there is only unlabeled data in the target domain.,1 Introduction,[0],[0]
"Our results demonstrate the following.
1.",1 Introduction,[0],[0]
"When the network combines the semisupervised component with the supervised component, depending on the amount of labeled data used, it gives 5% to 26% absolute gains in F1 compared to when it uses only the supervised component.
2.",1 Introduction,[0],[0]
"Domain adaptation with adversarial training improves over the adaptation baseline (i.e., a transfer model) by 1.8% to 4.1% absolute F1.
3.",1 Introduction,[0],[0]
"When the network combines domain adversarial training with semi-supervised learning, we get further gains ranging from 5% to 7% absolute in F1 across events.
",1 Introduction,[0],[0]
"Our source code is available on Github1 and the data is available on CrisisNLP2.
",1 Introduction,[0],[0]
The rest of the paper is organized as follows.,1 Introduction,[0],[0]
"In Section 2, we present the proposed method, i.e., domain adaptation and semi-supervised graph embedding learning.",1 Introduction,[0],[0]
"In Section 3, we present the experimental setup and baselines.",1 Introduction,[0],[0]
The results and analysis are presented in Section 4.,1 Introduction,[0],[0]
"In Section 5, we present the works relevant to this study.",1 Introduction,[0],[0]
"Finally, conclusions appear in Section 6.",1 Introduction,[0],[0]
We demonstrate our approach for domain adaptation with adversarial training and graph embedding on a tweet classification task to support crisis response efforts.,2 The Model,[0],[0]
"Let DlS = {ti, yi} Ls i=1 and DuS = {ti} Us i=1",2 The Model,[0],[0]
"be the set of labeled and unlabeled tweets for a source crisis event S (e.g., 1https://github.com/firojalam/ domain-adaptation 2http://crisisnlp.qcri.org
Nepal earthquake), where yi ∈ {1, . . .",2 The Model,[0],[0]
",K} is the class label for tweet ti, Ls and Us are the number of labeled and unlabeled tweets for the source event, respectively.",2 The Model,[0],[0]
"In addition, we have unlabeled tweets DuT = {ti} Ut i=1 for a target event T (e.g., Queensland flood) with Ut being the number of unlabeled tweets in the target domain.",2 The Model,[0],[0]
"Our ultimate goal is to train a cross-domain model p(y|t, θ) with parameters θ that can classify any tweet in the target event T without having any information about class labels in T .
",2 The Model,[0],[0]
Figure 1 shows the overall architecture of our neural model.,2 The Model,[0],[0]
The input to the network is a tweet t =,2 The Model,[0],[0]
"(w1, . . .",2 The Model,[0],[0]
", wn) containing words that come from a finite vocabulary V defined from the training set.",2 The Model,[0],[0]
The first layer of the network maps each of these words into a distributed representation Rd by looking up a shared embedding matrix E ∈ R|V |×d.,2 The Model,[0],[0]
We initialize the embedding matrix E in our network with word embeddings that are pretrained on a large crisis dataset (Subsection 2.5).,2 The Model,[0],[0]
"However, embedding matrix E can also be initialize randomly.",2 The Model,[0],[0]
"The output of the look-up layer is a matrix X ∈ Rn×d, which is passed through a number of convolution and pooling layers to learn higher-level feature representations.",2 The Model,[0],[0]
"A convolution operation applies a filter u ∈ Rk.d to a window of k vectors to produce a new feature ht as
ht = f(u.",2 The Model,[0],[0]
"Xt:t+k−1) (1)
where Xt:t+k−1 is the concatenation of k look-up vectors, and f is a nonlinear activation; we use rectified linear units or ReLU.",2 The Model,[0],[0]
"We apply this filter to each possible k-length windows in X with stride size of 1 to generate a feature map hj as:
hj =",2 The Model,[0],[0]
"[h1, . . .",2 The Model,[0],[0]
", hn+k−1] (2)
",2 The Model,[0],[0]
We repeat this process N times with N different filters to get N different feature maps.,2 The Model,[0],[0]
"We use a wide convolution (Kalchbrenner et al., 2014), which ensures that the filters reach the entire tweet, including the boundary words.",2 The Model,[0],[0]
"This is done by performing zero-padding, where out-ofrange (i.e., t<1 or t>n) vectors are assumed to be zero.",2 The Model,[0],[0]
"With wide convolution, o zero-padding size and 1 stride size, each feature map contains (n + 2o − k + 1) convoluted features.",2 The Model,[0],[0]
"After the convolution, we apply a max-pooling operation to each of the feature maps,
m = [µp(h 1), · · · , µp(hN )]",2 The Model,[0],[0]
"(3)
where µp(hj) refers to the max operation applied to each window of p features with stride size of 1 in the feature map hi.",2 The Model,[0],[0]
"Intuitively, the convolution operation composes local features into higherlevel representations in the feature maps, and maxpooling extracts the most important aspects of each feature map while reducing the output dimensionality.",2 The Model,[0],[0]
"Since each convolution-pooling operation is performed independently, the features extracted become invariant in order (i.e., where they occur in the tweet).",2 The Model,[0],[0]
"To incorporate order information between the pooled features, we include a fully-connected (dense) layer
z = f(Vm) (4)
where V is the weight matrix.",2 The Model,[0],[0]
"We choose a convolutional architecture for feature composition because it has shown impressive results on similar tasks in a supervised setting (Nguyen et al., 2017).
",2 The Model,[0],[0]
"The network at this point splits into three branches (shaded with three different colors in Figure 1) each of which serves a different purpose and contributes a separate loss to the overall loss of the model as defined below:
L(Λ,Φ,Ω,Ψ) =",2 The Model,[0],[0]
"LC(Λ,Φ) + λgLG(Λ,Ω) + λdLD(Λ,Ψ) (5)
where Λ = {U, V } are the convolutional filters and dense layer weights that are shared across the three branches.",2 The Model,[0],[0]
"The first componentLC(Λ,Φ) is a supervised classification loss based on the labeled data in the source event.",2 The Model,[0],[0]
"The second component LG(Λ,Ω) is a graph-based semi-supervised loss that utilizes both labeled and unlabeled data in the
source and target events to induce structural similarity between training instances.",2 The Model,[0],[0]
"The third component LD(Λ,Ω) is an adversary loss that again uses all available data in the source and target domains to induce domain invariance in the learned features.",2 The Model,[0],[0]
The tunable hyperparameters λg and λd control the relative strength of the components.,2 The Model,[0],[0]
"The supervised component induces label information (e.g., relevant vs. non-relevant) directly in the network through the classification loss LC(Λ,Φ), which is computed on the labeled instances in the source event, DlS .",2.1 Supervised Component,[0],[0]
"Specifically, this branch of the network, as shown at the top in Figure 1, takes the shared representations z as input and pass it through a task-specific dense layer
zc = f(Vcz) (6)
where Vc is the corresponding weight matrix.",2.1 Supervised Component,[0],[0]
The activations zc along with the activations from the semi-supervised branch zs are used for classification.,2.1 Supervised Component,[0],[0]
"More formally, the classification layer defines a Softmax
p(y = k|t, θ) = exp
( W Tk [zc; zs] )∑ k′ exp ( W Tk′",2.1 Supervised Component,[0],[0]
"[zc; zs]
) (7) where [.; .] denotes concatenation of two column vectors, Wk are the class weights, and θ = {U, V, Vc,W} defines the relevant parameters for this branch of the network with Λ = {U, V } being the shared parameters and Φ = {Vc,W} being the parameters specific to this branch.",2.1 Supervised Component,[0],[0]
"Once learned,
we use θ for prediction on test tweets.",2.1 Supervised Component,[0],[0]
"The classification loss LC(Λ,Φ) (or LC(θ)) is defined as
LC(Λ,Φ) =",2.1 Supervised Component,[0],[0]
"− 1
Ls Ls∑ i=1",2.1 Supervised Component,[0],[0]
"I(yi = k) log p(yi = k|ti,Λ,Φ) (8)
where I(.) is an indicator function that returns 1 when the argument is true, otherwise it returns 0.",2.1 Supervised Component,[0],[0]
The semi-supervised branch (shown at the middle in Figure 1) induces structural similarity between training instances (labeled or unlabeled) in the source and target events.,2.2 Semi-supervised Component,[0],[0]
"We adopt the recently proposed graph-based semi-supervised deep learning framework (Yang et al., 2016), which shows impressive gains over existing semisupervised methods on multiple datasets.",2.2 Semi-supervised Component,[0],[0]
"In this framework, a “similarity” graph G first encodes relations between training instances, which is then used by the network to learn internal representations (i.e., embeddings).",2.2 Semi-supervised Component,[0],[0]
The semi-supervised branch takes the shared representation z as input and learns internal representations by predicting a node in the graph context of the input tweet.,2.2.1 Learning Graph Embeddings,[0],[0]
"Following (Yang et al., 2016), we use negative sampling to compute the loss for predicting the context node, and we sample two types of contextual nodes: (i) one is based on the graph G to encode structural information, and (ii) the second is based on the labels in DlS to incorporate label information through this branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"The ratio of positive and negative samples is controlled by a random variable ρ1 ∈ (0, 1), and the proportion of the two context types is controlled by another random variable ρ2 ∈ (0, 1); see Algorithm 1 of (Yang et al., 2016) for details on the sampling procedure.
",2.2.1 Learning Graph Embeddings,[0],[0]
"Let (j, γ) is a tuple sampled from the distribution p(j, γ|i,DlS , G), where j is a context node of an input node i and γ ∈ {+1,−1} denotes whether it is a positive or a negative sample; γ = +1 if ti and tj are neighbors in the graph (for graph-based context) or they both have same labels (for label-based context), otherwise γ = −1.",2.2.1 Learning Graph Embeddings,[0],[0]
"The negative log loss for context prediction LG(Λ,Ω) can be written as
LG(Λ,Ω) =",2.2.1 Learning Graph Embeddings,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.2.1 Learning Graph Embeddings,[0],[0]
"E(j,γ) log σ",2.2.1 Learning Graph Embeddings,[0],[0]
( γCTj zg(i) ),2.2.1 Learning Graph Embeddings,[0],[0]
"(9)
where zg(i) = f(Vgz(i)) defines another dense layer (marked as Dense (zg) in Figure 1) having weights Vg, and Cj is the weight vector associated with the context node tj .",2.2.1 Learning Graph Embeddings,[0],[0]
"Note that here Λ = {U, V } defines the shared parameters and Ω = {Vg, C} defines the parameters specific to the semi-supervised branch of the network.",2.2.1 Learning Graph Embeddings,[0],[0]
"Typically graphs are constructed based on a relational knowledge source, e.g., citation links in (Lu and Getoor, 2003), or distance between instances (Zhu, 2005).",2.2.2 Graph Construction,[0],[0]
"However, we do not have access to such a relational knowledge in our setting.",2.2.2 Graph Construction,[0],[0]
"On the other hand, computing distance between n(n−1)/2 pairs of instances to construct the graph is also very expensive (Muja and Lowe, 2014).",2.2.2 Graph Construction,[0],[0]
"Therefore, we choose to use k-nearest neighborbased approach as it has been successfully used in other study (Steinbach et al., 2000).
",2.2.2 Graph Construction,[0],[0]
"The nearest neighbor graph consists of n vertices and for each vertex, there is an edge set consisting of a subset of n instances, i.e., tweets in our training set.",2.2.2 Graph Construction,[0],[0]
"The edge is defined by the distance measure d(i, j) between tweets ti and tj , where the value of d represents how similar the two tweets are.",2.2.2 Graph Construction,[0],[0]
"We used k-d tree data structure (Bentley, 1975) to efficiently find the nearest instances.",2.2.2 Graph Construction,[0],[0]
"To construct the graph, we first represent each tweet by averaging the word2vec vectors of its words, and then we measure d(i, j) by computing the Euclidean distance between the vectors.",2.2.2 Graph Construction,[0],[0]
The number of nearest neighbor k was set to 10.,2.2.2 Graph Construction,[0],[0]
The reason of averaging the word vectors is that it is computationally simpler and it captures the relevant semantic information for our task in hand.,2.2.2 Graph Construction,[0],[0]
"Likewise, we choose to use Euclidean distance instead of cosine for computational efficiency.",2.2.2 Graph Construction,[0],[0]
The network described so far can learn abstract features through convolutional and dense layers that are discriminative for the classification task (relevant vs. non-relevant).,2.3 Domain Adversarial Component,[0],[0]
"The supervised branch of the network uses labels in the source event to induce label information directly, whereas the semi-supervised branch induces similarity information between labeled and unlabeled instances.",2.3 Domain Adversarial Component,[0],[0]
"However, our goal is also to make these learned features invariant across domains or events (e.g., Nepal Earthquake vs. Queensland Flood).",2.3 Domain Adversarial Component,[0],[0]
"We achieve this by domain adversarial training of
neural networks (Ganin et al., 2016).",2.3 Domain Adversarial Component,[0],[0]
"We put a domain discriminator, another branch in the network (shown at the bottom in Figure 1) that takes the shared internal representation z as input, and tries to discriminate between the domains of the input — in our case, whether the input tweet is from DS or from DT .",2.3 Domain Adversarial Component,[0],[0]
"The domain discriminator is defined by a sigmoid function:
δ̂ = p(d = 1|t,Λ,Ψ) = sigm(wTd",2.3 Domain Adversarial Component,[0],[0]
"zd) (10)
where d ∈ {0, 1} denotes the domain of the input tweet t, wd are the final layer weights of the discriminator, and zd = f(Vdz) defines the hidden layer of the discriminator with layer weights Vd.",2.3 Domain Adversarial Component,[0],[0]
"Here Λ = {U, V } defines the shared parameters, and Ψ = {Vd,wd} defines the parameters specific to the domain discriminator.",2.3 Domain Adversarial Component,[0],[0]
"We use the negative log-probability as the discrimination loss:
Ji(Λ,Ψ) = −di log δ̂ − (1− di) log ( 1− δ̂ ) (11)
",2.3 Domain Adversarial Component,[0],[0]
"We can write the overall domain adversary loss over the source and target domains as
LD(Λ,Ψ) =",2.3 Domain Adversarial Component,[0],[0]
"− 1
Ls + Us Ls+Us∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ)− 1 Ut Ut∑ i=1",2.3 Domain Adversarial Component,[0],[0]
"Ji(Λ,Ψ) (12)
where Ls + Us and Ut are the number of training instances in the source and target domains, respectively.",2.3 Domain Adversarial Component,[0],[0]
"In adversarial training, we seek parameters (saddle point) such that
θ∗ = argmin Λ,Φ,Ω max Ψ L(Λ,Φ,Ω,Ψ) (13)
which involves a maximization with respect to Ψ and a minimization with respect to {Λ,Φ,Ω}.",2.3 Domain Adversarial Component,[0],[0]
"In other words, the updates of the shared parameters Λ = {U, V } for the discriminator work adversarially to the rest of the network, and vice versa.",2.3 Domain Adversarial Component,[0],[0]
"This is achieved by reversing the gradients of the discrimination loss LD(Λ,Ψ), when they are backpropagated to the shared layers (see Figure 1).",2.3 Domain Adversarial Component,[0],[0]
Algorithm 1 illustrates the training algorithm based on stochastic gradient descent (SGD).,2.4 Model Training,[0],[0]
We first initialize the model parameters.,2.4 Model Training,[0],[0]
"The word embedding matrixE is initialized with pre-trained word2vec vectors (see Subsection 2.5) and is kept fixed during training.3 Other parameters are initialized with small random numbers sampled from
3Tuning E on our task by backpropagation increased the training time immensely (3 days compared to 5 hours on a Tesla GPU) without any significant performance gain.
",2.4 Model Training,[0],[0]
"Algorithm 1: Model Training with SGD Input : data DlS , DuS , DuT ; graph G Output: learned parameters θ = {Λ,Φ} 1.",2.4 Model Training,[0],[0]
"Initialize model parameters {E,Λ,Φ,Ω,Ψ}; 2. repeat
//",2.4 Model Training,[0],[0]
"Semi-supervised for each batch sampled from p(j, γ|i,DlS , G) do a) Compute loss LG(Λ,Ω) b)",2.4 Model Training,[0],[0]
"Take a gradient step for LG(Λ,Ω); end //",2.4 Model Training,[0],[0]
"Supervised & domain adversary for each batch sampled from DlS do a) Compute LC(Λ,Φ) and LD(Λ,Ψ) b)",2.4 Model Training,[0],[0]
"Take gradient steps for LC(Λ,Φ) and LD(Λ,Ψ); end // Domain adversary for each batch sampled from DuS and DuT do
a) Compute LD(Λ,Ψ) b) Take a gradient step for LD(Λ,Ψ);
end until convergence;
a uniform distribution (Bengio and Glorot, 2010).",2.4 Model Training,[0],[0]
"We use AdaDelta (Zeiler, 2012) adaptive update to update the parameters.
",2.4 Model Training,[0],[0]
"In each iteration, we do three kinds of gradient updates to account for the three different loss components.",2.4 Model Training,[0],[0]
"First, we do an epoch over all the training instances updating the parameters for the semi-supervised loss, then we do an epoch over the labeled instances in the source domain, each time updating the parameters for the supervised and the domain adversary losses.",2.4 Model Training,[0],[0]
"Finally, we do an epoch over the unlabeled instances in the two domains to account for the domain adversary loss.
",2.4 Model Training,[0],[0]
The main challenge in adversarial training is to balance the competing components of the network.,2.4 Model Training,[0],[0]
"If one component becomes smarter than the other, its loss to the shared layer becomes useless, and the training fails to converge (Arjovsky et al., 2017).",2.4 Model Training,[0],[0]
"Equivalently, if one component becomes weaker, its loss overwhelms that of the other, causing the training to fail.",2.4 Model Training,[0],[0]
"In our experiments, we observed the domain discriminator is weaker than the rest of the network.",2.4 Model Training,[0],[0]
"This could be due to the noisy nature of tweets, which makes the job for the domain discriminator harder.",2.4 Model Training,[0],[0]
"To balance the components, we would want the error signals from the discriminator to be fairly weak, also we would want the supervised loss to have more impact than the semi-supervised loss.",2.4 Model Training,[0],[0]
"In our experiments, the weight of the domain adversary loss λd was fixed to 1e",2.4 Model Training,[0],[0]
"− 8, and the weight of the semi-supervised loss λg was fixed to 1e",2.4 Model Training,[0],[0]
− 2.,2.4 Model Training,[0],[0]
"Other sophisticated weighting schemes have been proposed recently
(Ganin et al., 2016; Arjovsky et al., 2017; Metz et al., 2016).",2.4 Model Training,[0],[0]
"It would be interesting to see how our model performs using these advanced tuning methods, which we leave as a future work.",2.4 Model Training,[0],[0]
"As mentioned, we used word embeddings that are pre-trained on a crisis dataset.",2.5 Crisis Word Embedding,[0],[0]
"To train the wordembedding model, we first pre-processed tweets collected using the AIDR system (Imran et al., 2014) during different events occurred between 2014 and 2016.",2.5 Crisis Word Embedding,[0],[0]
"In the preprocessing step, we lowercased the tweets and removed URLs, digit, time patterns, special characters, single character, username started with the @ symbol.",2.5 Crisis Word Embedding,[0],[0]
"After preprocessing, the resulting dataset contains about 364 million tweets and about 3 billion words.
",2.5 Crisis Word Embedding,[0],[0]
"There are several approaches to train word embeddings such as continuous bag-of-words (CBOW) and skip-gram models of wrod2vec (Mikolov et al., 2013), and Glove (Pennington et al., 2014).",2.5 Crisis Word Embedding,[0],[0]
"For our work, we trained the CBOW model from word2vec.",2.5 Crisis Word Embedding,[0],[0]
"While training CBOW, we filtered out words with a frequency less than or equal to 5, and we used a context window size of 5 and k = 5 negative samples.",2.5 Crisis Word Embedding,[0],[0]
The resulting embedding model contains about 2 million words with vector dimensions of 300.,2.5 Crisis Word Embedding,[0],[0]
"In this section, we describe our experimental settings – datasets used, settings of our models, compared baselines, and evaluation metrics.",3 Experimental Settings,[0],[0]
"To conduct the experiment and evaluate our system, we used two real-world Twitter datasets collected during the 2015 Nepal earthquake (NEQ) and the 2013 Queensland floods (QFL).",3.1 Datasets,[0],[0]
"These datasets are comprised of millions of tweets collected through the Twitter streaming API4 using event-specific keywords/hashtags.
",3.1 Datasets,[0],[0]
To obtain the labeled examples for our task we employed paid workers from the Crowdflower5 – a crowdsourcing platform.,3.1 Datasets,[0],[0]
The annotation consists of two classes relevant and non-relevant.,3.1 Datasets,[0],[0]
"For the annotation, we randomly sampled 11,670 and 10,033 tweets from the Nepal earthquake and the Queensland floods datasets, respectively.",3.1 Datasets,[0],[0]
"Given a
4https://dev.twitter.com/streaming/overview 5http://crowdflower.com
tweet, we asked crowdsourcing workers to assign the “relevant” label if the tweet conveys/reports information useful for crisis response such as a report of injured or dead people, some kind of infrastructure damage, urgent needs of affected people, donations requests or offers, otherwise assign the “non-relevant” label.",3.1 Datasets,[0],[0]
"We split the labeled data into 60% as training, 30% as test and 10% as development.",3.1 Datasets,[0],[0]
Table 1 shows the resulting datasets with class-wise distributions.,3.1 Datasets,[0],[0]
Data preprocessing was performed by following the same steps used to train the word2vec model (Subsection 2.5).,3.1 Datasets,[0],[0]
"In all the experiments, the classification task consists of two classes: relevant and non-relevant.",3.1 Datasets,[0],[0]
"In order to demonstrate the effectiveness of our joint learning approach, we performed a series of experiments.",3.2 Model Settings and Baselines,[0],[0]
"To understand the contribution of different network components, we performed an ablation study showing how the model performs as a semi-supervised model alone and as a domain adaptation model alone, and then we compare them with the combined model that incorporates all the components.",3.2 Model Settings and Baselines,[0],[0]
"As a baseline for the semi-supervised experiments, we used the self-training approach (Scudder, 1965).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For this purpose, we first trained a supervised model using the CNN architecture (i.e., shared components followed by the supervised part in Figure 1).",3.2.1 Settings for Semi-supervised Learning,[0],[0]
The trained model was then used to automatically label the unlabeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Instances with a classifier confidence score ≥ 0.75 were then used to retrain a new model.
",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Next, we run experiments using our graphbased semi-supervised approach (i.e., shared components followed by the supervised and semisupervised parts in Figure 1), which exploits unlabeled data.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For reducing the computational cost, we randomly selected 50K unlabeled instances from the same domain.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"For our semi-supervised setting, one of the main goals was to understand how much labeled data is sufficient to obtain a
reasonable result.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"Therefore, we experimented our system by incrementally adding batches of instances, such as 100, 500, 2000, 5000, and all instances from the training set.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
Such an understanding can help us design the model at the onset of a crisis event with sufficient amount of labeled data.,3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To demonstrate that the semi-supervised approach outperforms the supervised baseline, we run supervised experiments using the same number of labeled instances.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"In the supervised setting, only zc activations in Figure 1 are used for classification.",3.2.1 Settings for Semi-supervised Learning,[0],[0]
"To set a baseline for the domain adaptation experiments, we train a CNN model (i.e., shared components followed by the supervised part in Figure 1) on one event (source) and test it on another event (target).",3.2.2 Settings for Domain Adaptation,[0],[0]
"We call this as transfer baseline.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"To assess the performance of our domain adaptation technique alone, we exclude the semisupervised component from the network.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We train and evaluate models with this network configuration using different source and target domains.
",3.2.2 Settings for Domain Adaptation,[0],[0]
"Finally, we integrate all the components of the network as shown in Figure 1 and run domain adaptation experiments using different source and target domains.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In all our domain adaptation experiments, we only use unlabeled instances from the target domain.",3.2.2 Settings for Domain Adaptation,[0],[0]
"In domain adaption literature, this is known as unsupervised adaptation.",3.2.2 Settings for Domain Adaptation,[0],[0]
"We use 100, 150, and 200 filters each having the window size of 2, 3, and 4, respectively, and pooling length of 2, 3, and 4, respectively.",3.2.3 Training Settings,[0],[0]
We do not tune these hyperparameters in any experimental setting since the goal was to have an end-to-end comparison with the same hyperparameter setting and understand whether our approach can outperform the baselines or not.,3.2.3 Training Settings,[0],[0]
"Furthermore, we do not filter out any vocabulary item in any settings.
",3.2.3 Training Settings,[0],[0]
"As mentioned before in Subsection 2.4, we used AdaDelta (Zeiler, 2012) to update the model parameters in each SGD step.",3.2.3 Training Settings,[0],[0]
The learning rate was set to 0.1 when optimizing on the classification loss and to 0.001 when optimizing on the semisupervised loss.,3.2.3 Training Settings,[0],[0]
The learning rate for domain adversarial training was set to 1.0.,3.2.3 Training Settings,[0],[0]
"The maximum number of epochs was set to 200, and dropout rate of 0.02 was used to avoid overfitting (Srivastava et al., 2014).",3.2.3 Training Settings,[0],[0]
"We used validation-based early stopping using the F-measure with a patience of 25,
Experiments AUC P R F1
NEPAL EARTHQUAKE
Supervised 61.22 62.42 62.31 60.89
Semi-supervised (Self-training) 61.15 61.53 61.53 61.26
Semi-supervised (Graph-based) 64.81 64.58 64.63 65.11
QUEENSLAND FLOODS
i.e., we stop training if the score does not increase for 25 consecutive epochs.",3.2.3 Training Settings,[0],[0]
"To measure the performance of the trained models using different approaches described above, we use weighted average precision, recall, F-measure, and Area Under ROC-Curve (AUC), which are standard evaluation measures in the NLP and machine learning communities.",3.2.4 Evaluation Metrics,[0],[0]
The rationale behind choosing the weighted metric is that it takes into account the class imbalance problem.,3.2.4 Evaluation Metrics,[0],[0]
"In this section, we present the experimental results and discuss our main findings.",4 Results and Discussion,[0],[0]
"In Table 2, we present the results obtained from the supervised, self-training based semi-supervised, and our graph-based semi-supervised experiments for the both datasets.",4.1 Semi-supervised Learning,[0],[0]
It can be clearly observed that the graph-based semi-supervised approach outperforms the two baselines – supervised and self-training based semi-supervised.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, the graph-based approach shows 4% to 13% absolute improvements in terms of F1 scores for the Nepal and Queensland datasets, respectively.
To determine how the semi-supervised approach performs in the early hours of an event when only fewer labeled instances are available, we mimic a batch-wise (not to be confused with minibatch in SGD) learning setting.",4.1 Semi-supervised Learning,[0],[0]
"In Table 3, we present the results using different batch sizes – 100, 500, 1,000, 2,000, and all labels.
",4.1 Semi-supervised Learning,[0],[0]
"From the results, we observe that models’ performance improve as we include more labeled data
Exp.",4.1 Semi-supervised Learning,[0],[0]
100 500 1000 2000,4.1 Semi-supervised Learning,[0],[0]
"All L
NEPAL EARTHQUAKE
L 43.63 52.89 56.37 60.11 60.89
L+50kU 52.32 59.95 61.89 64.05 65.11
QUEENSLAND FLOOD
— from 43.63 to 60.89 for NEQ and from 48.97 to 80.16 for QFL in the case of labeled only (L).",4.1 Semi-supervised Learning,[0],[0]
"When we compare supervised vs. semi-supervised (L vs. L+U), we observe significant improvements in F1 scores for the semi-supervised model for all batches over the two datasets.",4.1 Semi-supervised Learning,[0],[0]
"As we include unlabeled instances with labeled instances from the same event, performance significantly improves in each experimental setting giving 5% to 26% absolute improvements over the supervised models.",4.1 Semi-supervised Learning,[0],[0]
These improvements demonstrate the effectiveness of our approach.,4.1 Semi-supervised Learning,[0],[0]
We also notice that our semi-supervised approach can perform above 90% depending on the event.,4.1 Semi-supervised Learning,[0],[0]
"Specifically, major improvements are observed from batch size 100 to 1,000, however, after that the performance improvements are comparatively minor.",4.1 Semi-supervised Learning,[0],[0]
"The results obtained using batch sizes 500 and 1,000 are reasonably in the acceptable range when labeled and unlabeled instances are combined (i.e., L+50kU for Nepal and L+∼21kU for Queensland), which is also a reasonable number of training examples to obtain at the onset of an event.",4.1 Semi-supervised Learning,[0],[0]
"In Table 4, we present domain adaptation results.",4.2 Domain Adaptation,[0],[0]
"The first block shows event-specific (i.e., train and test on the same event)",4.2 Domain Adaptation,[0],[0]
results for the supervised CNN model.,4.2 Domain Adaptation,[0],[0]
These results set the upper bound for our domain adaptation methods.,4.2 Domain Adaptation,[0],[0]
"The transfer baselines are shown in the next block, where we train a CNN model in one domain and test it on a different domain.",4.2 Domain Adaptation,[0],[0]
"Then, the third block shows the results for the domain adversarial approach without the semi-supervised loss.",4.2 Domain Adaptation,[0],[0]
These results show the importance of domain adversarial component.,4.2 Domain Adaptation,[0],[0]
"After that, the fourth block presents the performance of the model trained with graph
Source Target AUC P R F1
",4.2 Domain Adaptation,[0],[0]
"IN-DOMAIN SUPERVISED MODEL
embedding without domain adaptation to show the importance of semi-supervised learning.",4.2 Domain Adaptation,[0],[0]
"The final block present the results for the complete model that includes all the loss components.
",4.2 Domain Adaptation,[0],[0]
The results with domain adversarial training show improvements across both events – from 1.8% to 4.1% absolute gains in F1.,4.2 Domain Adaptation,[0],[0]
"These results attest that adversarial training is an effective approach to induce domain invariant features in the internal representation as shown previously by Ganin et al. (2016).
",4.2 Domain Adaptation,[0],[0]
"Finally, when we do both semi-supervised learning and unsupervised domain adaptation, we get further improvements in F1 scores ranging from 5% to 7% absolute gains.",4.2 Domain Adaptation,[0],[0]
"From these improvements, we can conclude that domain adaptation with adversarial training along with graphbased semi-supervised learning is an effective method to leverage unlabeled and labeled data from a different domain.
",4.2 Domain Adaptation,[0],[0]
"Note that for our domain adaptation methods, we only use unlabeled data from the target domain.",4.2 Domain Adaptation,[0],[0]
"Hence, we foresee future improvements of this approach by utilizing a small amount of target domain labeled data.",4.2 Domain Adaptation,[0],[0]
Two lines of research are directly related to our work: (i) semi-supervised learning and (ii) domain adaptation.,5 Related Work,[0],[0]
Several models have been proposed for semi-supervised learning.,5 Related Work,[0],[0]
"The earliest approach is self-training (Scudder, 1965), in
which a trained model is first used to label unlabeled data instances followed by the model retraining with the most confident predicted labeled instances.",5 Related Work,[0],[0]
"The co-training (Mitchell, 1999) approach assumes that features can be split into two sets and each subset is then used to train a classifier with an assumption that the two sets are conditionally independent.",5 Related Work,[0],[0]
"Then each classifier classifies the unlabeled data, and then most confident data instances are used to re-train the other classifier, this process repeats multiple times.
",5 Related Work,[0],[0]
"In the graph-based semi-supervised approach, nodes in a graph represent labeled and unlabeled instances and edge weights represent the similarity between them.",5 Related Work,[0],[0]
"The structural information encoded in the graph is then used to regularize a model (Zhu, 2005).",5 Related Work,[0],[0]
"There are two paradigms in semi-supervised learning: 1) inductive – learning a function with which predictions can be made on unobserved instances, 2) transductive – no explicit function is learned and predictions can only be made on observed instances.",5 Related Work,[0],[0]
"As mentioned before, inductive semi-supervised learning is preferable over the transductive approach since it avoids building the graph each time it needs to infer the labels for the unlabeled instances.
",5 Related Work,[0],[0]
"In our work, we use a graph-based inductive deep learning approach proposed by Yang et al. (2016) to learn features in a deep learning model by predicting contextual (i.e., neighboring) nodes in the graph.",5 Related Work,[0],[0]
"However, our approach is different from Yang et al. (2016) in several ways.",5 Related Work,[0],[0]
"First, we construct the graph by computing the distance between tweets based on word embeddings.",5 Related Work,[0],[0]
"Second, instead of using count-based features, we use a convolutional neural network (CNN) to compose high-level features from the distributed representation of the words in a tweet.",5 Related Work,[0],[0]
"Finally, for context prediction, instead of performing a random walk, we select nodes based on their similarity in the graph.",5 Related Work,[0],[0]
"Similar similarity-based graph has shown impressive results in learning sentence representations (Saha et al., 2017).
",5 Related Work,[0],[0]
"In the literature, the proposed approaches for domain adaptation include supervised, semisupervised and unsupervised.",5 Related Work,[0],[0]
"It also varies from linear kernelized approach (Blitzer et al., 2006) to non-linear deep neural network techniques (Glorot et al., 2011; Ganin et al., 2016).",5 Related Work,[0],[0]
"One direction of research is to focus on feature space distribution matching by reweighting the samples from
the source domain (Gong et al., 2013) to map source into target.",5 Related Work,[0],[0]
The overall idea is to learn a good feature representation that is invariant across domains.,5 Related Work,[0],[0]
"In the deep learning paradigm, Glorot et al. (Glorot et al., 2011) used Stacked Denoising Auto-Encoders (SDAs) for domain adaptation.",5 Related Work,[0],[0]
"SDAs learn a robust feature representation, which is artificially corrupted with small Gaussian noise.",5 Related Work,[0],[0]
"Adversarial training of neural networks has shown big impact recently, especially in areas such as computer vision, where generative unsupervised models have proved capable of synthesizing new images (Goodfellow et al., 2014; Radford et al., 2015; Makhzani et al., 2015).",5 Related Work,[0],[0]
"Ganin et al. (2016) proposed domain adversarial neural networks (DANN) to learn discriminative but at the same time domain-invariant representations, with domain adaptation as a target.",5 Related Work,[0],[0]
"We extend this work by combining with semi-supervised graph embedding for unsupervised domain adaptation.
",5 Related Work,[0],[0]
"In a recent work, Kipf and Welling (2016) present CNN applied directly on graph-structured datasets - citation networks and on a knowledge graph dataset.",5 Related Work,[0],[0]
Their study demonstrate that graph convolution network for semi-supervised classification performs better compared to other graph based approaches.,5 Related Work,[0],[0]
"In this paper, we presented a deep learning framework that performs domain adaptation with adversarial training and graph-based semi-supervised learning to leverage labeled and unlabeled data from related events.",6 Conclusions,[0],[0]
"We use a convolutional neural network to compose high-level representation from the input, which is then passed to three components that perform supervised training, semisupervised learning and domain adversarial training.",6 Conclusions,[0],[0]
"For domain adaptation, we considered a scenario, where we have only unlabeled data in the target event.",6 Conclusions,[0],[0]
"Our evaluation on two crisis-related tweet datasets demonstrates that by combining domain adversarial training with semi-supervised learning, our model gives significant improvements over their respective baselines.",6 Conclusions,[0],[0]
We have also presented results of batch-wise incremental training of the graph-based semi-supervised approach and show approximation regarding the number of labeled examples required to get an acceptable performance at the onset of an event.,6 Conclusions,[0],[0]
The success of deep neural networks (DNNs) is heavily dependent on the availability of labeled data.,abstractText,[0],[0]
"However, obtaining labeled data is a big challenge in many real-world problems.",abstractText,[0],[0]
"In such scenarios, a DNN model can leverage labeled and unlabeled data from a related domain, but it has to deal with the shift in data distributions between the source and the target domains.",abstractText,[0],[0]
"In this paper, we study the problem of classifying social media posts during a crisis event (e.g., Earthquake).",abstractText,[0],[0]
"For that, we use labeled and unlabeled data from past similar events (e.g., Flood) and unlabeled data for the current event.",abstractText,[0],[0]
We propose a novel model that performs adversarial learning based domain adaptation to deal with distribution drifts and graph based semi-supervised learning to leverage unlabeled data within a single unified deep learning framework.,abstractText,[0],[0]
Our experiments with two real-world crisis datasets collected from Twitter demonstrate significant improvements over several baselines.,abstractText,[0],[0]
Domain Adaptation with Adversarial Training and Graph Embeddings,title,[0],[0]
√ ε rather than the eigengap.,text,[0],[0]
"The Generalized Eigenvector (GenEV) problem and the Canonical Correlation Analysis (CCA) are two fundamental problems in scientific computing, machine learning, operations research, and statistics.",1 Introduction,[0],[0]
"Algorithms solving these problems are often used to extract features to compare large-scale datasets, as well as used for problems in regression (Kakade & Foster, 2007), clustering (Chaudhuri et al., 2009), classification (Karampatziakis & Mineiro, 2014), word embeddings (Dhillon et al., 2011), and many others.
",1 Introduction,[0],[0]
GenEV.,1 Introduction,[0],[0]
"Given two symmetric matrices A,B ∈ Rd×d whereB is positive definite.",1 Introduction,[0],[0]
"The GenEV problem is to find generalized eigenvectors v1, . . .",1 Introduction,[0],[0]
", vd where each vi satisfies
vi ∈ arg max v∈Rd ∣∣v>Av∣∣ s.t. { v>Bv = 1 v>Bvj = 0 ∀j ∈",1 Introduction,[0],[0]
"[i− 1]
The values λi def = v>",1 Introduction,[0],[0]
"i Avi are known as the generalized eigenvalues, and it satisfies |λ1| ≥ · · · |λd|.",1 Introduction,[0],[0]
Following the *Equal contribution .,1 Introduction,[0],[0]
Future version of this paper shall be found at http://arxiv.org/abs/1607.06017.,1 Introduction,[0],[0]
1Microsoft Research 2Princeton University.,1 Introduction,[0],[0]
"Correspondence to: Zeyuan Allen-Zhu <zeyuan@csail.mit.edu>, Yuanzhi Li <yuanzhil@cs.princeton.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1 Introduction,[0],[0]
"Copyright 2017 by the author(s).
tradition of (Wang et al., 2016; Garber & Hazan, 2015), we
assume without loss of generality that λi ∈",1 Introduction,[0],[0]
"[−1, 1].
CCA.",1 Introduction,[0],[0]
"Given matrices X ∈ Rn×dx , Y ∈ Rn×dy and denoting by Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY
>Y , the CCA problem is to find canonical-correlation vectors {(φi, ψi)}ri=1",1 Introduction,[0],[0]
"where r = min{dx, dy} and each pair (φi, ψi) ∈ arg max
φ∈Rdx ,ψ∈Rdy
{ φ>Sxyψ } such that { φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈",1 Introduction,[0],[0]
[i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈,1 Introduction,[0],[0]
"[i− 1]
The values σi def = φ>i",1 Introduction,[0],[0]
"Sxyψi ≥ 0 are known as the canonical-correlation coefficients, and
1 ≥ σ1 ≥ · · · ≥ σr ≥ 0 is always satisfied.
",1 Introduction,[0],[0]
"It is a fact that solving CCA exactly can be reduced to solving GenEV exactly, if one defines B = diag{Sxx, Syy} ∈ Rd×d andA =",1 Introduction,[0],[0]
"[[0, Sxy]; [S>xy, 0]] ∈",1 Introduction,[0],[0]
Rd×d for d def = dx+dy; see Lemma 2.3.,1 Introduction,[0],[0]
"(This reduction does not always hold if the generalized eigenvectors are computed only approximately.)
",1 Introduction,[0],[0]
"Despite the fundamental importance and the frequent necessity in applications, there are few results on obtaining provably efficient algorithms for GenEV and CCA until very recently.",1 Introduction,[0],[0]
"In the breakthrough result of Ma, Lu and Foster (Ma et al., 2015), they proposed to study algorithms to find top k generalized eigenvectors (k-GenEV) or top k canonical-correlation vectors (k-CCA).",1 Introduction,[0],[0]
They designed an alternating minimization algorithm whose running time is only linear in the input matrix sparsity and nearly-linear in k.,1 Introduction,[0],[0]
"Such algorithms are very appealing because in real-life applications, it is often only relevant to obtain top correlation vectors, as opposed to the less meaningful vectors in the directions where the datasets do not correlate.",1 Introduction,[0],[0]
"Unfortunately, the method of Ma, Lu and Foster has a running time that linearly scales with κ and 1/gap, where
• κ ≥ 1 is the condition number of matrix B in GenEV, or of matrices X>X,Y >Y in CCA; and • gap ∈",1 Introduction,[0],[0]
"[0, 1) is the eigengap λk−λk+1λk in GenEV, or σk−σk+1
σk in CCA.
",1 Introduction,[0],[0]
"These parameters are usually not constants and scale with
the problem size.
",1 Introduction,[0],[0]
"Challenge 1: Acceleration For many easier scientific computing problems, we are able to design algorithms that have accelerated dependencies on κ and 1/gap.",1 Introduction,[0],[0]
"As two concrete examples, k-PCA can be solved with a running time linearly in 1/ √ gap as opposed to 1/gap (Golub & Van Loan, 2012); computing B−1w for a vector w can be solved in time linearly in √ κ as opposed to κ, where κ is the condition number of matrix B (Shewchuk, 1994; Axelsson, 1985; Nesterov, 1983).
",1 Introduction,[0],[0]
"Therefore, can we obtain doubly-accelerated methods for k-GenEV and k-CCA, meaning that the running times linearly scale with both √ κ and 1/ √ gap?",1 Introduction,[0],[0]
"Before this paper, for the general case k > 1, the method of Ge et al. (Ge et al., 2016) made acceleration possible for parameter κ, but not for parameter 1/gap (see Table 1).
",1 Introduction,[0],[0]
"Challenge 2: Gap-Freeness Since gap can be even zero in the extreme case, can we design algorithms that do not scale with 1/gap?",1 Introduction,[0],[0]
Recall that this is possible for the easier task of k-PCA.,1 Introduction,[0],[0]
"The block Krylov method (Musco & Musco, 2015) runs in time linear in 1/ √ ε as opposed to 1/ √ gap, where ε is the approximation ratio.",1 Introduction,[0],[0]
"There is no gap-free result previously known for k-GenEV or k-CCA even for k = 1.
",1 Introduction,[0],[0]
"Challenge 3: Stochasticity For matrix-related problems, one can usually obtain stochastic running times which requires some notations to describe.
",1 Introduction,[0],[0]
"Consider a simple task of computing B−1w for some vector w, where accelerated methods solve it in time linearly in √ κ for κ being the condition number of B. If B = 1 nX >X is given in the form of a covariance matrix where X ∈ Rn×d, then (accelerated) stochastic methods compute B−1w in a time linearly in (1 + √ κ′/n) instead of √ κ, where κ′ = maxi∈[n]{‖Xi‖ 2} λmin(B) ∈",1 Introduction,[0],[0]
"[ κ, nκ ] and Xi is the i-th
row of X .",1 Introduction,[0],[0]
(See Lemma 2.6.),1 Introduction,[0],[0]
"Since 1 + √ κ′/n ≤ O( √ κ), stochastic methods are no slower than non-stochastic ones.
",1 Introduction,[0],[0]
"So, can we obtain a similar but doubly-accelerated stochastic method for k-CCA?1 Note that, if the doublyaccelerated requirement is dropped, this task is easier and indeed possible, see Ge et al. (Ge et al., 2016).",1 Introduction,[0],[0]
"However, since their stochastic method is not doubly-accelerated, in certain parameter regimes, it runs even slower than nonstochastic ones (even for k = 1, see Table 2).
",1 Introduction,[0],[0]
Remark.,1 Introduction,[0],[0]
"In general, if designed properly, for worst case running time:
• Accelerated results are usually better because they are 1 Note that a similar problem can be also asked for k-GenEV when A and B are both given in their covariance matrix forms.",1 Introduction,[0],[0]
"We refrain from doing it in this paper for notational simplicity.
",1 Introduction,[0],[0]
"no slower than non-accelerated ones in the worst-case.
• Gap-free results are better because they imply gapdependent ones.2
• Stochastic results are usually better because they are no slower than non-stochastic ones in the worst-case.",1 Introduction,[0],[0]
"We provide algorithms LazyEV and LazyCCA that are doubly-accelerated, gap-free, and stochastic.3
For the general k-GenEV problem, our LazyEV can be implemented to run in time4
Õ (knnz(B)√κ
√ gap
+ knnz(A) + k2d",1.1 Our Main Results,[0],[0]
"√ gap
) or
Õ (knnz(B)√κ√
ε + knnz(A)",1.1 Our Main Results,[0],[0]
+,1.1 Our Main Results,[0],[0]
k2d√ ε ) in the gap-dependent and gap-free cases respectively.,1.1 Our Main Results,[0],[0]
Since our running time only linearly depends on √ κ and √ gap (resp.,1.1 Our Main Results,[0],[0]
√ ε),1.1 Our Main Results,[0],[0]
", our algorithm LazyEV is doubly-accelerated.
",1.1 Our Main Results,[0],[0]
"For the general k-CCA problem, our LazyCCA can be implemented to run in time
Õ (knnz(X,Y ) · (1 +√κ′/n)+ k2d
√ gap
) or
Õ (knnz(X,Y ) · (1 +√κ′/n)+ k2d
√ ε ) in the gap-dependent and gap-free cases respectively.",1.1 Our Main Results,[0],[0]
"Here, nnz(X,Y ) = nnz(X) + nnz(Y ) and κ′ = 2 maxi{‖Xi‖2,‖Yi‖2} λmin(diag{Sxx,Syy}) where Xi or Yi is the i-th row vector of X or Y .",1.1 Our Main Results,[0],[0]
"Therefore, our algorithm LazyCCA is doublyaccelerated and stochastic.
",1.1 Our Main Results,[0],[0]
"We fully compare our results with prior work in Table 2 (for k = 1) and Table 1 (for k ≥ 1), and summarize our main contributions:
• For k > 1, we outperform all relevant prior works (see Table 1).",1.1 Our Main Results,[0],[0]
"Moreover, no known method was doublyaccelerated even in the non-stochastic setting.
",1.1 Our Main Results,[0],[0]
"• For k ≥ 1, we obtain the first gap-free running time.",1.1 Our Main Results,[0],[0]
•,1.1 Our Main Results,[0],[0]
"Even for k = 1, we outperform most of the state-of-
the-arts (see Table 2).
",1.1 Our Main Results,[0],[0]
"Note that for CCA with k > 1, previous result CCALin only outputs the subspace spanned by the top k correlation vectors but does not identify which vector gives the highest correlation and so on.",1.1 Our Main Results,[0],[0]
"Our LazyCCA provides per-vector
2If a method depends on 1/ε then one can choose ε = gap and this translates to a gap-dependent running time.
",1.1 Our Main Results,[0],[0]
"3Recalling Footnote 1, for notational simplicity, we only state our k-GenEV result in non-stochastic running time.
",1.1 Our Main Results,[0],[0]
"4Throughout the paper, we use the Õ notation to hide polylogarithmic factors with respect to κ, 1/gap, 1/ε, d, n.",1.1 Our Main Results,[0],[0]
"We use nnz(M) to denote the time needed to multiply M to a vector.
",1.1 Our Main Results,[0],[0]
"λk
∈",1.1 Our Main Results,[0],[0]
"[0, 1] and κB = λmax(B)λmin(B) > 1.
",1.1 Our Main Results,[0],[0]
"In CCA, gap = σk−σk+1 σk ∈",1.1 Our Main Results,[0],[0]
"[0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈",1.1 Our Main Results,[0],[0]
"[κ, 2nκ], and σk ∈",1.1 Our Main Results,[0],[0]
"[0, 1].
",1.1 Our Main Results,[0],[0]
Remark 1.,1.1 Our Main Results,[0],[0]
Stochastic methods depend on a modified condition number κ′. The reason κ′ ∈,1.1 Our Main Results,[0],[0]
"[κ, 2nκ] is in Fact 2.5.",1.1 Our Main Results,[0],[0]
Remark 2.,1.1 Our Main Results,[0],[0]
"All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).
",1.1 Our Main Results,[0],[0]
Remark 3.,1.1 Our Main Results,[0],[0]
Doubly-stochastic methods are not necessarily interesting.,1.1 Our Main Results,[0],[0]
"We discuss them in Section 1.2.
",1.1 Our Main Results,[0],[0]
guarantees on all the top k correlation vectors.,1.1 Our Main Results,[0],[0]
"Recall that when considering acceleration, there are two parameters κ and 1/gap.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"One can also design stochastic methods with respect to both parameters κ and 1/gap, meaning that
with a running time proportional to 1 + √ κ′/nc √ gap
instead of 1+ √ κ′/n
√ gap (stochastic) or √ κ√ gap (non-stochastic).
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
The constant c is usually 1/2.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"We call such methods doubly-stochastic.
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Unfortunately, doubly-stochastic methods are usually slower than stochastic ones.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
Take 1-CCA as an example.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"The best stochastic running time (obtained exclusively by
us) for 1-CCA is nnz(X,Y ) ·",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
Õ ( 1+√κ′/n √ gap ) .,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"In contrast, if one uses a doubly-stochastic method —either (Wang et al., 2016) or our LazyCCA— the running time becomes nnz(X,Y ) ·",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Õ ( 1 +
√ κ′/n1/4√",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"gap·σ1
) .",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Therefore, for 1-CCA,
doubly-stochastic methods are faster than stochastic ones
only when κ ′
σ1 ≤ o(n1/2) .
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
The above condition is usually not satisfied.,1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"For instance,
• κ′ is usually around n for most interesting data-sets, cf.
the experiments of (Shalev-Shwartz & Zhang, 2014);
• κ′ is between n1/2 and 100n in all the CCA experiments of (Wang et al., 2016); and
• by Fact 2.5 it satisfies κ′ ≥ d so κ′ cannot be smaller than o(n1/2) unless d n1/2.5 Even worse, parameter σ1 ∈",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"[0, 1] is usually much smaller than 1.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Note that σ1 is scaling invariant: even if one scales X and Y up by the same factor, σ1 remains unchanged.
",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Nevertheless, to compare our LazyCCA with all relevant prior works, we obtain doubly-stochastic running times for k-CCA as well.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"Our running time matches that of (Wang et al., 2016) when k = 1, and no doubly-stochastic running time for k > 1 was known before our work.",1.2 Our Side Results on Doubly-Stochastic Methods,[0],[0]
"For the easier task of PCA and SVD, the first gap-free result was obtained by Musco and Musco (Musco & Musco, 2015), the first stochastic result was obtained by Shamir (Shamir, 2015), and the first accelerated stochastic result was obtained by Garber et al. (Garber & Hazan, 2015; Garber et al., 2016).",1.3 Other Related Works,[0],[0]
"The shift-and-invert preconditioning technique of Garber et al. is also used in this paper.
",1.3 Other Related Works,[0],[0]
"For another related problem PCR (principle compo-
5Note that item (3) κ′ ≥ d may not hold in the more general setting of CCA, see Remark A.1.
",1.3 Other Related Works,[0],[0]
"λ1
∈",1.3 Other Related Works,[0],[0]
"[0, 1] and κB = λmax(B)λmin(B) > 1.
",1.3 Other Related Works,[0],[0]
"In CCA, gap = σ1−σ2 σ1 ∈",1.3 Other Related Works,[0],[0]
"[0, 1], κ = λmax(diag{Sxx,Syy}) λmin(diag{Sxx,Syy}) > 1, κ′ = 2maxi{‖Xi‖ 2,‖Yi‖2} λmin(diag{Sxx,Syy}) ∈",1.3 Other Related Works,[0],[0]
"[κ, 2nκ], and σ1 ∈",1.3 Other Related Works,[0],[0]
"[0, 1].
",1.3 Other Related Works,[0],[0]
Remark 1.,1.3 Other Related Works,[0],[0]
Stochastic methods depend on modified condition number κ′; the reason κ′ ∈,1.3 Other Related Works,[0],[0]
"[κ, 2nκ] is in Def. 2.4.",1.3 Other Related Works,[0],[0]
Remark 2.,1.3 Other Related Works,[0],[0]
"All non-stochastic CCA methods in this table have been outperformed because 1 + √ κ′/n ≤ O(κ).
",1.3 Other Related Works,[0],[0]
Remark 3.,1.3 Other Related Works,[0],[0]
Doubly-stochastic methods are not necessarily interesting.,1.3 Other Related Works,[0],[0]
"We discuss them in Section 1.2.
",1.3 Other Related Works,[0],[0]
Remark 4.,1.3 Other Related Works,[0],[0]
Some CCA methods have a running time dependency on σ1 ∈,1.3 Other Related Works,[0],[0]
"[0, 1], and this is intrinsic and cannot be removed.",1.3 Other Related Works,[0],[0]
"In particular, if we scale the data matrix X and Y , the value σ1 stays the same.
",1.3 Other Related Works,[0],[0]
Remark 5.,1.3 Other Related Works,[0],[0]
"The only (non-doubly-stochastic) doubly-accelerated method before our work is SI (Wang et al., 2016) (for 1-CCA only).",1.3 Other Related Works,[0],[0]
Our LazyEV is faster than theirs by a factor Ω( √ nκ/κ′ × √ 1/σ1).,1.3 Other Related Works,[0],[0]
"Here, nκ/κ′ ≥ 1/2 and 1/σ1 ≥ 1 are two scaling-invariant quantities usually much greater than 1.
nent regression), we recently obtained an accelerated method (Allen-Zhu & Li, 2017) as opposed the previously non-accelerated one (Frostig et al., 2016); however, the acceleration techniques there are not relevant to this paper.
",1.3 Other Related Works,[0],[0]
"For GenEV and CCA, many scalable algorithms have been designed recently (Ma et al., 2015; Wang & Livescu, 2015; Michaeli et al., 2015; Witten et al., 2009; Lu & Foster, 2014).",1.3 Other Related Works,[0],[0]
"However, as summarized by the authors of CCALin, these cited methods are more or less heuristics and do not have provable guarantees.",1.3 Other Related Works,[0],[0]
"Furthermore, for k > 1, the AppGrad method (Ma et al., 2015) only provides local convergence guarantees and thus requires a warm-start whose
computational complexity is not discussed in their paper.
",1.3 Other Related Works,[0],[0]
"Finally, our algorithms on GenEV and CCA are based on finding vectors one-by-one, which is advantageous in practice because one does not need k to be known and can stop the algorithm whenever the eigenvalues (or correlation values) are too small.",1.3 Other Related Works,[0],[0]
"Known approaches for k > 1 cases (such as GenELin, CCALin, AppGrad) find all k vectors at once, therefore requiring k to be known beforehand.",1.3 Other Related Works,[0],[0]
"As a separate note, these known approaches do not need the user to know the desired accuracy a priori but our LazyEV and LazyCCA algorithms do.",1.3 Other Related Works,[0],[0]
We denote by ‖x‖ or ‖x‖2 the Euclidean norm of vector x.,2 Preliminaries,[0],[0]
"We denote by ‖A‖2, ‖A‖F , and ‖A‖Sq respectively the spectral, Frobenius, and Schatten q-norm of matrix A (for q ≥ 1).",2 Preliminaries,[0],[0]
"We write A B if A,B are symmetric and A−B is positive semi-definite (PSD), and write A B if A,B are symmetric but A − B is positive definite (PD).",2 Preliminaries,[0],[0]
"We denote by λmax(M) and λmin(M) the largest and smallest eigenvalue of a symmetric matrix M , and by κM",2 Preliminaries,[0],[0]
"the condition number λmax(M)/λmin(M) of a PSD matrix M .
",2 Preliminaries,[0],[0]
"Throughout this paper, we use nnz(M) to denote the time to multiply matrix M to any arbitrary vector.",2 Preliminaries,[0],[0]
"For two matricesX,Y , we denote by nnz(X,Y ) = nnz(X)+nnz(Y ), and by Xi or Yi the i-th row vector of X or Y .",2 Preliminaries,[0],[0]
"We also use poly(x1, x2, . . .",2 Preliminaries,[0],[0]
", xt) to represent a quantity that is asymptotically at most polynomial in terms of variables x1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", xt.",2 Preliminaries,[0],[0]
"Given a column orthonormal matrix U ∈ Rn×k, we denote by U⊥ ∈ Rn×(n−k)",2 Preliminaries,[0],[0]
"the column orthonormal matrix consisting of an arbitrary basis in the space orthogonal to the span of U ’s columns.
",2 Preliminaries,[0],[0]
"Given a PSD matrixB and a vector v, v>Bv is theB-seminorm of v. Two vectors v, w are B-orthogonal if v>Bw = 0.",2 Preliminaries,[0],[0]
"We denote by B−1 the Moore-Penrose pseudoinverse of B if B is not invertible, and by B1/2 the matrix square root of B (satisfying B1/2 0).",2 Preliminaries,[0],[0]
"All occurrences of B−1, B1/2 and B−1/2 are for analysis purpose only.",2 Preliminaries,[0],[0]
"Our final algorithms only require multiplications of B to vectors.
",2 Preliminaries,[0],[0]
Definition 2.1 (GenEV).,2 Preliminaries,[0],[0]
"Given symmetric matrices A,B ∈ Rd×d where B is positive definite.",2 Preliminaries,[0],[0]
"The generalized eigenvectors ofA with respect toB are v1, . . .",2 Preliminaries,[0],[0]
", vd, where each vi is
vi ∈ arg max v∈Rd {∣∣v>Av∣∣ s.t. v>",2 Preliminaries,[0],[0]
Bv = 1 v>Bvj = 0 ∀j ∈,2 Preliminaries,[0],[0]
"[i− 1] } The generalized eigenvalues λ1, . .",2 Preliminaries,[0],[0]
.,2 Preliminaries,[0],[0]
", λd satisfy λi = v",2 Preliminaries,[0],[0]
>,2 Preliminaries,[0],[0]
"i Avi which can be negative.
",2 Preliminaries,[0],[0]
"Following (Wang et al., 2016; Garber & Hazan, 2015), we assume without loss of generality that λi ∈",2 Preliminaries,[0],[0]
"[−1, 1].
",2 Preliminaries,[0],[0]
Definition 2.2 (CCA).,2 Preliminaries,[0],[0]
"Given X ∈ Rn×dx , Y ∈",2 Preliminaries,[0],[0]
"Rn×dy , letting Sxx = 1nX >X , Sxy = 1nX >Y , Syy = 1nY
>Y , the canonical-correlation vectors are {(φi, ψi)}ri=1",2 Preliminaries,[0],[0]
"where r = min{dx, dy} and for all i ∈",2 Preliminaries,[0],[0]
"[r]:
(φi, ψi) ∈ arg max φ∈Rdx ,ψ∈Rdy
{ φ>Sxyψ such that
{ φ>Sxxφ = 1 ∧ φ>Sxxφj = 0 ∀j ∈",2 Preliminaries,[0],[0]
[i− 1] ψ>Syyψ = 1 ∧ ψ>Syyψj = 0 ∀j ∈,2 Preliminaries,[0],[0]
"[i− 1] }} The corresponding canonical-correlation coefficients σ1, . . .",2 Preliminaries,[0],[0]
", σr satisfy σi = φ>i Sxyψi ∈",2 Preliminaries,[0],[0]
"[0, 1].
We emphasize that σi always lies in [0, 1] and is scalinginvariant.",2 Preliminaries,[0],[0]
"When dealing with a CCA problem, we also denote by d = dx + dy .
",2 Preliminaries,[0],[0]
Lemma 2.3 (CCA to GenEV).,2 Preliminaries,[0],[0]
"Given a CCA problem with matrices X ∈ Rn×dx , Y ∈ Rn×dy , let the canonicalcorrelation vectors and coefficients be {(φi, ψi, σi)}ri=1 where r = min{dx, dy}.",2 Preliminaries,[0],[0]
"Define A = ( 0 Sxy
S>xy 0
) and
B =",2 Preliminaries,[0],[0]
"( Sxx 0
0 Syy
) .",2 Preliminaries,[0],[0]
"Then, the GenEV problem of A with re-
spect to B has 2r eigenvalues {±σi}ri=1 and corresponding generalized eigenvectors {( φi ψi ) , ( −φi ψi )}n i=1 .",2 Preliminaries,[0],[0]
The remaining dx + dy,2 Preliminaries,[0],[0]
"− 2r eigenvalues are zeros.
",2 Preliminaries,[0],[0]
Definition 2.4.,2 Preliminaries,[0],[0]
"In CCA, let A and B be as defined in Lemma 2.3.",2 Preliminaries,[0],[0]
"We define condition numbers
κ def = κB = λmax(B) λmin(B) and κ′ def= 2 maxi{‖Xi‖ 2,‖Yi‖2} λmin(B) .",2 Preliminaries,[0],[0]
Fact 2.5.,2 Preliminaries,[0],[0]
κ′ ∈,2 Preliminaries,[0],[0]
"[κ, 2nκ] and κ′ ≥ d. (See full version.)
",2 Preliminaries,[0],[0]
Lemma 2.6.,2 Preliminaries,[0],[0]
"Given matrices X ∈ Rn×dx , Y ∈ Rn×dy , let A and B be as defined in Lemma 2.3.",2 Preliminaries,[0],[0]
"For every w ∈ Rd, the Katyusha method (Allen-Zhu, 2017) finds a vector w′ ∈",2 Preliminaries,[0],[0]
"Rd satisfying ‖w′ −B−1Aw‖ ≤ ε in time
O ( nnz(X,Y ) · ( 1 + √ κ′/n ) ·",2 Preliminaries,[0],[0]
"log κ‖w‖ 2
ε
) .",2 Preliminaries,[0],[0]
"We introduce AppxPCA±, the multiplicative approximation algorithm for computing the two-sided leading eigenvector of a symmetric matrix.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"AppxPCA± uses the shift-and-invert framework (Garber & Hazan, 2015; Garber et al., 2016), and shall become our building block for the LazyEV and LazyCCA algorithms in the subsequent sections.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Our pseudo-code Algorithm 1 is a modification of Algorithm 5 in (Garber & Hazan, 2015), and reduces the eigenvector problem to oracle calls to an arbitrary matrix inversion oracle A.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"The main differences between AppxPCA± and (Garber & Hazan, 2015) are two-fold.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"First, given a symmetric matrix M , AppxPCA± simultaneously considers an upper-bounding shift together with a lower-bounding shift, and try to perform power methods with respect to (λI − M)−1 and (λI + M)−1.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"This allows us to determine approximately how close λ is to the largest and the smallest eigenvalues of M , and decrease λ accordingly.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"In the end, AppxPCA± outputs an approximate eigenvector of M that corresponds to a negative eigenvalue if needed.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Second, we provide a multiplicative-error guarantee rather than additive as appeared in (Garber & Hazan, 2015).",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Without such guarantee, our final running time will depend on 1gap·λmax(M) rather than 1 gap .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"6
6This is why the SI method of (Wang et al., 2016) also uses
Algorithm 1 AppxPCA±(A,M, δ×, ε, p)
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Input: A, an approximate matrix inversion method; M ∈ Rd×d, a symmetric matrix satisfying −I M I; δ× ∈ (0, 0.5], a multiplicative error; ε ∈ (0, 1), a numerical accuracy parameter; and p ∈ (0, 1), the confidence parameter.
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"1: ŵ0 ← RanInit(d); s← 0; λ(0) ← 1 + δ×; ŵ0 is a random unit vector, see Def. 3.2 2: m1 ← ⌈ 4 log ( 288dθ p2 )⌉ , m2 ← ⌈ log ( 36dθ p2ε )⌉ ; θ is the parameter of RanInit, see Def. 3.2
3: ε̃1 ← 164m1 ( δ× 48 )m1 and ε̃2",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"← ε8m2 ( δ×48 )m2 4: repeat m1 = TPM(8, 1/32, p) and m2 = TPM(2, ε/4, p), see Lemma B.1 5:",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"s← s+ 1; 6: for t = 1 to m1 do 7: Apply A to find ŵt satisfying
∥∥ŵt − (λ(s−1)I −M)−1ŵt−1∥∥ ≤ ε̃1; 8: wa ← ŵm1/‖ŵm1‖; wa is roughly (λ(s−1)I −M)−m1 ŵ0 then normalized 9: Apply A to find va satisfying
∥∥va",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
− (λ(s−1)I −M)−1wa∥∥ ≤ ε̃1; 10: for t = 1 to m1 do 11:,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Apply A to find ŵt satisfying
∥∥ŵt − (λ(s−1)I +M)−1ŵt−1∥∥ ≤ ε̃1; 12: wb ← ŵm1/‖ŵm1‖; wb is roughly (λ(s−1)I +M)−m1 ŵ0 then normalized 13: Apply A to find vb satisfying
∥∥vb",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (λ(s−1)I +M)−1wb∥∥ ≤ ε̃1; 14: ∆(s) ← 12 ·
1 max{w>a va,w>b vb}−ε̃1
and λ(s)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
← λ(s−1),3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− ∆ (s)
2 ;
15: until ∆(s) ≤ δ×λ (s) 12 16: f ← s; 17: if the last w>a va ≥ w>b vb then 18: for t = 1 to m2 do 19: Apply A to find ŵt satisfying
∥∥ŵt − (λ(f)I −M)−1ŵt−1∥∥ ≤ ε̃2; 20: return (+, w) where w def= ŵm2/‖ŵm2‖. 21: else 22: for t = 1 to m2 do 23: Apply A to find ŵt satisfying
∥∥ŵt",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (λ(f)I +M)−1ŵt−1∥∥ ≤ ε̃2; 24: return (−, w) where w def= ŵm2/‖ŵm2‖. 25: end if
We prove in full version the following theorem:
Theorem 3.1 (AppxPCA±, informal).",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
Let M ∈ Rd×d be a symmetric matrix with eigenvalues 1 ≥ λ1 ≥ · · ·,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
≥ λd ≥ −1,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"and eigenvectors u1, . . .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
", ud.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Let λ∗ = max{λ1,−λd}.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"With probability at least 1− p, AppxPCA± produces a pair (sgn,w) satisfying
• if sgn = +, then w is an approx.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"positive eigenvector: w>Mw ≥ (
1− δ× 2
)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λ∗ ∧ ∑
i∈[d]",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λi≤(1−δ×/2)λ∗
(w>ui) 2 ≤ ε
• if sgn = −, then w is an approx.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
negative eigenvector: w>Mw ≤,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"− (
1−δ× 2
)",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λ∗ ∧ ∑
i∈[d]",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λi≥−(1−δ×/2)λ∗
(w>ui) 2 ≤ ε
The number of oracle calls toA is Õ(log(1/δ×)), and each time we call A it satisfies
shift-and-invert but depends on 1 gap·σ1 in Table 2.
•",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"λmax(λ (s)I−M) λmin(λ(s)I−M) , λmax(λ (s)I+M) λmin(λ(s)I+M) ∈",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"[1, 96δ× ] and
• 1 λmin(λ(s)I−M) , 1 λmin(λ(s)I+M) ≤ 48δ×λ∗ .
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"We remark here that, unlike the original shift-and-invert method which chooses a random (Gaussian) unit vector in Line 1 of AppxPCA±, we have allowed this initial vector to be generated from an arbitrary θ-conditioned random vector generator (for later use), defined as follows:
Definition 3.2.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"An algorithm RanInit(d) is a θconditioned random vector generator if w = RanInit(d) is a d-dimensional unit vector and, for every p ∈ (0, 1), every unit vector u ∈ Rd, with probability at least 1− p, it satisfies (u>w)2 ≤ p
2θ 9d .
",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
This modification is needed in order to obtain our efficient implementations of GenEV and CCA.,3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"One can construct a θ-conditioned random vector generator as follows:
Proposition 3.3.",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"Given a PSD matrix B ∈ Rd×d, if we set RanInit(d) def = B
1/2v (v>Bv)0.5
where v is a random Gaussian vector, then RanInit(d) is a θ-conditioned random vector
generator for θ = κB .",3 Leading Eigenvector via Two-Sided Shift-and-Invert,[0],[0]
"In this section, we construct an algorithm LazyEV that, given symmetric matrix M ∈ Rd×d, computes approximately the k leading eigenvectors ofM that have the largest absolute eigenvalues.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Then, for the original k-GenEV problem, we set M = B−1/2AB−1/2 and run LazyEV.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"This is our plan to find the top k leading generalized eigenvectors of A with respect to B.
Our algorithm LazyEV is formally stated in Algorithm 2.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The algorithm applies k times AppxPCA±, each time computing an approximate leading eigenvector of M with a multiplicative error δ×/2, and projects the matrix M into the orthogonal space with respect to the obtained leading eigenvector.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"We state our main approximation theorem below.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
Theorem 4.1 (informal).,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let M ∈ Rd×d be a symmetric matrix with eigenvalues λ1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", λd ∈",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[−1, 1] and corresponding eigenvectors u1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud, and |λ1| ≥ · · · ≥ |λd|.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"If εpca is sufficiently small,7 LazyEV outputs a (column) orthonormal matrix Vk = (v1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", vk) ∈ Rd×k which, with probability at least 1− p, satisfies: (a) ‖V >",4 LazyEV: Generalized Eigendecomposition,[0],[0]
k,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"U‖2 ≤ ε where U = (uj , . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud) and j is the
smallest index satisfying |λj | ≤ (1− δ×)λk.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
(b) For every i ∈,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[k], (1−δ×)|λi| ≤ |v>i Mvi| ≤ 11−δ× |λi|.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Above, property (a) ensures the k columns of Vk have negligible correlation with the eigenvectors of M whose absolute eigenvalues are ≤ (1− δ×)λk; property (b) ensures the Rayleigh quotients v>",4 LazyEV: Generalized Eigendecomposition,[0],[0]
i Mvi are all correct up to a 1±δ× error.,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"We in fact have shown two more useful properties in the full version that may be of independent interest.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The next theorem states that, if M = B−1/2AB−1/2, our LazyEV can be implemented without the necessity to compute B1/2 or B−1/2.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
Theorem 4.2 (running time).,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose M = B−1/2AB−1/2 and RanInit(d) is defined in Proposition 3.3 with respect to B. Then, the computation of V ← B−1/2LazyEV(A,M, k, δ×, εpca, p) can be implemented to run in time
• Õ ( knnz(B)+k2d+kΥ√
δ×
) where Υ is the time to multiply
B−1A to a vector, or • Õ ( k √ κBnnz(B)+knnz(A)+k
2d√ δ×
) if we use Conjugate gra-
dient to multiply B−1A to a vector.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
7Meaning εpca ≤,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"O ( poly(ε, δ×,
|λ1| |λk+1| , 1 d ) ) .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"The complete
specifications of εpca is included in the full version.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Since our final running time only depends on log(1/εpca), we have not attempted to improve the constants in this polynomial dependency.
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Choosing parameter δ× as either gap or ε, our two main theorems above immediately imply the following results for the k-GenEV problem: (proved in full version)
Theorem 4.3 (gap-dependent GenEV, informal).",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Let A,B ∈ Rd×d be two symmetric matrices satisfying B 0 and −B A B. Suppose the generalized eigenvalue and eigenvector pairs of A with respect to B are {(λi, ui)}di=1, and it satisfies 1 ≥ |λ1| ≥ · · · ≥ |λd|.",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Then, LazyEV outputs V k ∈ Rd×k satisfying
V > k",4 LazyEV: Generalized Eigendecomposition,[0],[0]
BV k = I and ‖V > k BW‖2 ≤,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"ε
in time Õ (k√κBnnz(B) + knnz(A)",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"+ k2d√
gap )",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Here, W = (uk+1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", ud) and gap =
|λk|−|λk+1| |λk| .
",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"Theorem 4.4 (gap-free GenEV, informal).",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"In the same setting as Theorem 4.3, our LazyEV outputs V k = (v1, . . .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
", vk) ∈ Rd×k satisfying",4 LazyEV: Generalized Eigendecomposition,[0],[0]
V > k BV,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"k = I and
∀s ∈",4 LazyEV: Generalized Eigendecomposition,[0],[0]
[k] : ∣∣v>s Avs∣∣ ∈,4 LazyEV: Generalized Eigendecomposition,[0],[0]
"[(1− ε)|λs|, |λs|1− ε]
in time Õ (k√κBnnz(B) + knnz(A) + k2d√
ε
) .",4 LazyEV: Generalized Eigendecomposition,[0],[0]
"In Section 5.1 we discuss how to ensure accuracy: that is, why does LazyEV guarantee to approximately find the top eigenvectors ofM .",5 Ideas Behind Theorems 4.1 and 4.2,[0],[0]
"In the full version of this paper, we also discuss how to implement LazyEV without compute B1/2 explicitly, thus proving Theorem 4.2.",5 Ideas Behind Theorems 4.1 and 4.2,[0],[0]
"Our approximation guarantee in Theorem 4.1 is a natural generalization of the recent work on fast iterative methods to find the top k eigenvectors of a PSD matrix M (Allen-Zhu & Li, 2016).",5.1 Ideas Behind Theorem 4.1,[0],[0]
"That method is called LazySVD and we summarize it as follows.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"At a high level, LazySVD finds the top k eigenvectors oneby-one and approximately.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Starting with M0 = M , in the s-th iteration where s ∈",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[k], LazySVD computes approximately the leading eigenvector of matrix Ms−1 and call it vs. Then, LazySVD projects Ms ← (I − vsv>s )",5.1 Ideas Behind Theorem 4.1,[0],[0]
Ms−1(I,5.1 Ideas Behind Theorem 4.1,[0],[0]
"− vsv > s ) and proceeds to the next iteration.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"While the algorithmic idea of LazySVD is simple, the analysis requires some careful linear algebraic lemmas.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Most notably, if vs is an approximate leading eigenvector of Ms−1, then one needs to prove that the small eigenvectors of Ms−1 somehow still “embed” into that of Ms after projection.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"This is achieved by a gap-free variant of the Wedin theorem plus a few other technical lemmas, and we recommend interested readers to see the high-level overview section of (Allen-Zhu & Li, 2016).
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Algorithm 2 LazyEV(A,M, k, δ×, εpca, p)
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"Input: A, an approximate matrix inversion method; M ∈ Rd×d, a matrix satisfying −I M I; k ∈",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[d], the desired rank; δ× ∈ (0, 1), a multiplicative error; εpca ∈ (0, 1), a numerical accuracy; and p ∈ (0, 1), a confidence parameter.
1: M0 ←M ; V0 =",5.1 Ideas Behind Theorem 4.1,[0],[0]
"[]; 2: for s = 1 to k do 3: (∼, v′s)← AppxPCA±(A,Ms−1, δ×/2, εpca, p/k); v′s is an approximate two-sided leading eigenvector of Ms−1 4: vs ← ( (I − Vs−1V >s−1)v′s ) / ∥∥(I",5.1 Ideas Behind Theorem 4.1,[0],[0]
"− Vs−1V >s−1)v′s∥∥; project v′s to V ⊥s−1 5: Vs ← [Vs−1, vs]; 6: Ms ← (I − vsv>s )Ms−1(I − vsv>s ) we also have Ms = (I − VsV >s )M(I",5.1 Ideas Behind Theorem 4.1,[0],[0]
"− VsV >s ) 7: end for 8: return Vk.
",5.1 Ideas Behind Theorem 4.1,[0],[0]
"In this paper, to relax the assumption thatM is PSD, and to find leading eigenvectors whose absolute eigenvalues are large, we have to make several non-trivial changes.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"On the algorithm side, LazyEV uses our two-sided shift-andinvert method in Section 3 to find the leading eigenvector of Ms−1 with largest absolute eigenvalue.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"On the analysis side, we have to make sure all lemmas properly deal with negative eigenvalues.",5.1 Ideas Behind Theorem 4.1,[0],[0]
"For instance:
• If we perform a projection M ′",5.1 Ideas Behind Theorem 4.1,[0],[0]
← (I − vv>)M(I,5.1 Ideas Behind Theorem 4.1,[0],[0]
"− vv>) where v correlates by at most ε with all eigenvectors ofM whose absolute eigenvalues are smaller than a threshold µ, then, after the projection, we need to prove that these eigenvectors can be approximately “embedded” into the eigenspace spanned by all eigenvectors of M ′",5.1 Ideas Behind Theorem 4.1,[0],[0]
whose absolute eigenvalues are smaller than µ+ τ .,5.1 Ideas Behind Theorem 4.1,[0],[0]
"The approximation of this embedding should depend on ε, µ and τ .
",5.1 Ideas Behind Theorem 4.1,[0],[0]
The full proof of Theorem 4.1 is in the arXiv version.,5.1 Ideas Behind Theorem 4.1,[0],[0]
It relies on a few matrix algebraic lemmas (including the aforementioned “embedding lemma”).,5.1 Ideas Behind Theorem 4.1,[0],[0]
In this paper we propose new iterative methods to solve the generalized eigenvector and the canonical correlation analysis problems.,6 Conclusion,[0],[0]
"Our methods find the most significant k eigenvectors or correlation vectors, and have running times that linearly scales with k.
Most importantly, our methods are doubly-accelerated: the running times have square-root dependencies both with respect to the condition number of the matrix (i.e., κ) and with respect to the eigengap (i.e., gap).",6 Conclusion,[0],[0]
They are the first doubly-accelerated iterative methods at least for k > 1.,6 Conclusion,[0],[0]
"They can also be made gap-free, and are the first gap-free iterative methods even for 1-GenEV or 1-CCA.
",6 Conclusion,[0],[0]
"Although this is a theory paper, we believe that if implemented carefully, our methods can outperform not only previous iterative methods (such as GenELin, AppGrad, CCALin), but also the commercial mathematics libraries for sparse matrices of dimension more than 10, 000.",6 Conclusion,[0],[0]
"We
leave it a future work for such careful comparisons.",6 Conclusion,[0],[0]
"We study k-GenEV, the problem of finding the top k generalized eigenvectors, and k-CCA, the problem of finding the top k vectors in canonicalcorrelation analysis.",abstractText,[0],[0]
We propose algorithms LazyEV and LazyCCA to solve the two problems with running times linearly dependent on the input size and on,abstractText,[0],[0]
"k. Furthermore, our algorithms are doubly-accelerated: our running times depend only on the square root of the matrix condition number, and on the square root of the eigengap.",abstractText,[0],[0]
This is the first such result for both kGenEV or k-CCA.,abstractText,[0],[0]
"We also provide the first gapfree results, which provide running times that depend on 1/ √ ε rather than the eigengap.",abstractText,[0],[0]
Doubly Accelerated Methods for Faster CCA  and Generalized Eigendecomposition,title,[0],[0]
Regularized empirical risk minimization with linear predictors is a key workhorse in machine learning.,1. Introduction,[0],[0]
"It has the following general form:
min x2Rd
( P (x) def = 1
n
nX
i=1
i
(a > i x) + g(x)
) (1)
where a i 2 Rd is one of the n data samples with d features.",1. Introduction,[0],[0]
"i
: R!",1. Introduction,[0],[0]
"R is a convex loss function of the linear predictor a
> i x, for i = 1, · · · , n, and g : Rd !",1. Introduction,[0],[0]
R is a convex regularization function for the coefficient vector x 2 Rd.,1. Introduction,[0],[0]
"The loss function
i assigns a cost to the difference between the linear predictor a>
i
x and the associated label b i .
",1. Introduction,[0],[0]
"With continuous and discrete b i , (1) captures regression and classification problems respectively.",1. Introduction,[0],[0]
"As a popular instance,
1Department of ICES, University of Texas, Austin 2Department of CS, Carnegie Mellon University, Pittsburgh 3Department of CS, University of Texas, Austin 4Amazon/A9, Palo Alto.",1. Introduction,[0],[0]
Correspondence to: Qi Lei,1. Introduction,[0],[0]
"<leiqi@ices.utexas.edu>, Ian E.H. Yen <eyan@cs.cmu.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"when i (z) = max{0, 1 b i z} and g(x) = µ/2kxk22, (1) reduces to the linear SVM (support vector machine) classification problem.",1. Introduction,[0],[0]
"While setting
i (z) = log(1+exp( b i z)), we obtain the logistic regression problem.
",1. Introduction,[0],[0]
We are interested in developing efficient algorithms for solving this general problem (1) for the setting where the coefficient vector x is assumed to be sparse.,1. Introduction,[0],[0]
"Applications where such a sparsity is natural include large-scale multiclass/multi-label classification, low-degree polynomial data mapping (Chang et al., 2010), n-gram feature mapping (Sonnenburg & Franc, 2010), and random feature kernel machines (Rahimi & Recht, 2007), specifically with a sparsity constraint on the random features (Yen et al., 2014).
",1. Introduction,[0],[0]
Our paper is organized as follows:,1. Introduction,[0],[0]
"In Section 2 we review existing algorithms to solve the primal, dual as well as primal-dual formulations of the problem (1).",1. Introduction,[0],[0]
"In Section 3, we present our Doubly Greedy Primal-Dual Coordinate Descent method for the convex-concave saddle point formulation of the problem (1).",1. Introduction,[0],[0]
We propose an alternative method that is more efficient in practice with the use of incrementally increased active sets in both primal and dual variables.,1. Introduction,[0],[0]
"In Section 4 we show linear convergence for our proposed algorithm, and demonstrate the advantages of greedy methods with sparse variables.",1. Introduction,[0],[0]
"Finally in Section 5 we compare the performance of our method with other state-of-the-art methods on some real-world datasets, both with respect to time and iterations.",1. Introduction,[0],[0]
"Notations: We use A to denote the data matrix, with rows A
i
= a
i corresponding to samples, and the column Aj corresponding to features.",2. Formulation and related work,[0],[0]
"We use [n] to compactly denote {1, 2, · · ·n}.",2. Formulation and related work,[0],[0]
"Throughout the paper, k · k denotes l2-norm unless otherwise specified.
Assumptions:",2. Formulation and related work,[0],[0]
"In order to establish equivalence of the primal, dual problem and the convex-concave saddle point formulation, we make the following assumptions.
",2. Formulation and related work,[0],[0]
"• g, the regularization for primal variable, is assumed to be µ-strongly convex, formally,
g(y) g(x) +",2. Formulation and related work,[0],[0]
"hrg(x),y xi+ µ 2 ky xk2, for any sub-gradient rg(x) 2 @g(x),x,y 2 Rd.",2. Formulation and related work,[0],[0]
"We
also assume that g has decomposable structure, i.e., g(x) =",2. Formulation and related work,[0],[0]
P,2. Formulation and related work,[0],[0]
i g,2. Formulation and related work,[0],[0]
"i (x i ).
",2. Formulation and related work,[0],[0]
"• i is 1 -smooth, for i 2",2. Formulation and related work,[0],[0]
[n]: i (y)  i (x)+ 0,2. Formulation and related work,[0],[0]
"i
(x)(y x)+ 2",2. Formulation and related work,[0],[0]
"(y x)2, x, y 2 R or equivalently, 0
i is Lipschitz continuous, i.e., | 0
i (x) 0",2. Formulation and related work,[0],[0]
i (y)|  1 |x y|.,2. Formulation and related work,[0],[0]
"Under the assumption of strongly convex regularization g and smooth loss function
i , minimizing (1) is equivalent to maximizing its dual formulation:
max y2Rn
( D(y) ⌘","2.1. Primal, dual and primal-dual formulations",[0],[0]
"g⇤( A > y
n )","2.1. Primal, dual and primal-dual formulations",[0],[0]
"1 n
nX
i=1
","2.1. Primal, dual and primal-dual formulations",[0],[0]
⇤ i,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"(y i )
) (2)
or the unique solution for the following convex-concave saddle point problem:
max y2Rn min x2Rd
( L(x,y) = g(x) + 1
n","2.1. Primal, dual and primal-dual formulations",[0],[0]
y >,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"Ax 1 n
nX
i=1
⇤","2.1. Primal, dual and primal-dual formulations",[0],[0]
"i (y i )
)
(3)
Note that i (a > i x) in (1) is also smooth with respect to x, since r
x","2.1. Primal, dual and primal-dual formulations",[0],[0]
"i
(a > i x) = 0","2.1. Primal, dual and primal-dual formulations",[0],[0]
i (a > i x)a,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"i , therefore i (a > i x)
is R2/ -smooth with respect to x, where R is defined as R = max
i ka i k2.","2.1. Primal, dual and primal-dual formulations",[0],[0]
"(Zhang & Xiao, 2014) thus defined the condition number for the primal-dual form as:
 def =
R2 µ .
","2.1. Primal, dual and primal-dual formulations",[0],[0]
We share this definition in this paper.,"2.1. Primal, dual and primal-dual formulations",[0],[0]
"The commonly used condition number for the gradient descent of the primal form is simply (R2/ + µ)/µ = 1 + , see (Nesterov, 2004).","2.1. Primal, dual and primal-dual formulations",[0],[0]
There has been a long line of work over the years to derive fast solvers for the generic optimization problem (1).,2.2. Related work,[0],[0]
"In Table 1, we review the time complexity to achieve ✏ error with respect to either primal, dual or primal-dual optimality for existing methods.
",2.2. Related work,[0],[0]
"Primal (accelerated) gradient descent (Nesterov, 2004; 2005) require O((1+)",2.2. Related work,[0],[0]
log(1/✏)) (or O((1+p) log(1/✏)),2.2. Related work,[0],[0]
if accelerated) iterations to achieve primal error less than ✏.,2.2. Related work,[0],[0]
Note that 1+ is the condition number of (1).,2.2. Related work,[0],[0]
"Since each iteration takes O(nd) operations, the overall time complexity is O(nd (1 + )",2.2. Related work,[0],[0]
log(1/✏)) (or O(nd (1 + p) log(1/✏)) if accelerated).,2.2. Related work,[0],[0]
"Due to the large per iteration cost for large n, stochastic variants that separately optimize each i
have proved more popular in big data settings.",2.2. Related work,[0],[0]
"Examples include SGD (Bottou, 2010), SAG (Schmidt et al., 2013), SVRG (Johnson & Zhang, 2013), SAGA (Defazio et al., 2014), MISO (Mairal, 2015) and their accel-
erated versions (Xiao & Zhang, 2014).",2.2. Related work,[0],[0]
"The stochastic scheme of optimizing individual
i is similar to updating each dual coordinate individually.",2.2. Related work,[0],[0]
"Their time complexity thus matches that of dual coordinate descent methods (Hsieh et al., 2008; Shalev-Shwartz & Zhang, 2013b; Yang, 2013; Qu et al., 2014), which enjoy a time complexity of O(nd (1+/n) log(1/✏)), and a further acceleration step (Shalev-Shwartz & Zhang, 2016; 2013a) will improve the complexity to O(nd (1 +p/n)",2.2. Related work,[0],[0]
log(1/✏)).,2.2. Related work,[0],[0]
"These stochastic methods have a lower per iteration cost of O(d), but each step optimizes terms that are much less well-conditioned, and consequently have a larger iteration complexity, for instance of O(n (1 + p /n)",2.2. Related work,[0],[0]
"log(1/✏)) in the accelerated case.
",2.2. Related work,[0],[0]
"With the primal-dual formulation, (Zhang & Xiao, 2014) introduce a novel stochastic primal-dual coordinate method (SPDC), which with acceleration achieves a time complexity of O(nd (1 +p/n)",2.2. Related work,[0],[0]
"log(1/✏)), matching that of accelerated stochastic dual coordinate descent methods.
",2.2. Related work,[0],[0]
"However, in practice, SPDC could lead to more expensive computations for sparse data matrices due to dense updates.",2.2. Related work,[0],[0]
"For some special choices of the model, (Zhang & Xiao, 2014) provided efficient implementation for sparse feature structures, but the average update time for each coordinate is still much longer than that of dual coordinate descent.",2.2. Related work,[0],[0]
"Moreover, they cannot exploit intermediate sparse iterates by methods such as shrinking technique (Hsieh et al., 2008).",2.2. Related work,[0],[0]
"We note moreover that acceleration is not always practically useful in many real-world settings, other than in extremely ill-conditioned situations.",2.2. Related work,[0],[0]
"In particular,  is typically of the order of p n or n as shown in (Bousquet & Elisseeff, 2002; Zhang & Xiao, 2014), and therefore the conditioning of O(1+p/n) is not necessarily much better than O(1+ /n).",2.2. Related work,[0],[0]
"Our experiments also corroborate this, showing that vanilla dual coordinate descent under reasonable precision or condition number is not improved upon by SDPC.
",2.2. Related work,[0],[0]
"Therefore we raise the following question: Does the primaldual formulation have other good properties that could be leveraged to improve optimization performance?
",2.2. Related work,[0],[0]
"For instance, some recent work with the primal-dual formulation updates stochastically sampled coordinates (Yu et al., 2015), which has a reduced cost per iteration, provided the data admits a low-rank factorization or when the proximal mapping for primal and dual variables are relatively computational expensive, which however may not hold in practice, so that the the noise caused by this preprocessing could hurt test performance.",2.2. Related work,[0],[0]
"Moreover, even when their assumptions hold, their low-rank matrix factorization step itself may dominate the total computation time.",2.2. Related work,[0],[0]
"In this paper, we try to address the key question above in the setting of empirical risk minimization problems with very large n and d, and where the set of primal (and/or dual) variables are assumed to be sparse.",2.3. Our Contribution,[0],[0]
"We then show that the primal-dual formulation of the problem allows for naturally leveraging available primal and/or dual sparsity.
2",2.3. Our Contribution,[0],[0]
"[
,].
",2.3. Our Contribution,[0],[0]
"In Table 1, we review the total time complexity to achieve ✏ accuracy.",2.3. Our Contribution,[0],[0]
"We can see that all algorithms that achieve a linear convergence rate require running time that has a factor nd, and in particular, none of their convergence rates involve the sparsity of the primal or dual variables.
",2.3. Our Contribution,[0],[0]
"There have been some attempts to modify existing primal or dual coordinate approaches in order to exploit sparsity of either primal or dual variables, but these do not perform very well in practice.",2.3. Our Contribution,[0],[0]
"One popular approach uses a shrinking heuristic in updating dual coordinates (Hsieh et al., 2008), which however still requires complexity linear to the number of coordinates d and does not guarantee rate of convergence.",2.3. Our Contribution,[0],[0]
"(Nutini et al., 2015) consider the idea of searching more important active coordinates to update in each iteration.",2.3. Our Contribution,[0],[0]
"Their greedy updates yield an iteration complexity linear in 1/µ1 instead of d/µ, where µ and µ1 are the parameters of strong convexity with respect to L2 and L1 norms respectively.",2.3. Our Contribution,[0],[0]
"However, with the commonly used L2 regularization term µk · k2 that is used to ensure µ-strong convexity, the term is exactly µ1 = µ
d l1-strongly convex.",2.3. Our Contribution,[0],[0]
"Moreover, in practice, searching active coordinates causes considerable overhead.",2.3. Our Contribution,[0],[0]
"While there have been some strategies proposed to address this such as (Dhillon et al., 2011) that leverages nearest neighbor search to reduce the searching time, these have further requirements on the data structure used to store the data.",2.3. Our Contribution,[0],[0]
"Overall, it thus remains to more carefully study the optimization problem in order to facilitate the use of greedy
approaches to exploit primal or dual sparsity.
",2.3. Our Contribution,[0],[0]
"In this paper, we propose a Doubly Greedy Primal-Dual (DGPD) Coordinate method that greedily selects and updates both primal and dual variables.",2.3. Our Contribution,[0],[0]
"This method enjoys an overall low time complexity under a sparsity assumption on the primal variables:
Theorem 2.1.",2.3. Our Contribution,[0],[0]
"Main result: (informal) For the empirical risk minimization problem (1) with l1 + l2 regularization, there exists an algorithm (DGPD) that achieves ✏ error in O(s(n+d)(1+ 
n
) log 1 ✏ )) time, where s is an upper bound of the sparsity of the primal variables.",2.3. Our Contribution,[0],[0]
"Coordinate-wise updates are most natural when g is separable, as is assumed for instance in the Stochastic Primal-Dual Coordinate method of (Zhang & Xiao, 2014).",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In this paper, to exploit sparsity in primal variables, we additionally focus on the case where g(x) =",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
µ2 kxk2 + kxk1.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"With respect to the loss function , it is assumed to be 1
-smooth and convex.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For instance, setting
i as the smooth hinge loss(Shalev-Shwartz & Zhang, 2013b):
i
(z) =
8 <
:
0",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"if b i z 1 1 2 biz if biz  0
( 1 2 biz)2 otherwise,
the smoothness parameter = 12 .",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For the logit function i (z) = log(1 + exp( b i
z), the smoothness parameter = 4.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"When iterates are sparse, it is more efficient to perform greedy coordinate descent.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
We will provide a brief theoretical vignette of this phenomenon in Section 4.1.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"With this motivation, our proposed method Doubly Greedy PrimalDual Coordinate Descent (DGPD) greedily selects and updates both the primal and dual variables, one coordinate a time.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Our overall method is detailed in Algorithm 1.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In Algorithm 1, we start from all zero vectors x(0), z(0) 2 Rn, and y(0),w(0) 2 Rd, where x(0), and y(0) are the iterates for primal and dual variables, and w(0) and z(0) are two auxiliary vectors, maintained as w ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Ax and z ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A>y to cache and reduce computations.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Primal Updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In each iteration, we first compute the optimal primal variable x̄(t 1) for the current y(t 1), i.e.,
¯ x (t 1) = argmin
x L(x,y(t 1)))",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Eqn.(4),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Then, we only update the coordinate j(t) that will decrease L(x,y) the most, i.e., j(t) = argmin
k2[d] L(x(t)+(x̄(t 1) k x(t) k )e k ,y(t 1)))",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Eqn.(5),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Both two processes cost O(d) operations.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Afterwards, we update the value of w with Eqn.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(6) such that w(t) = Ax(t)
Algorithm 1 Doubly Greedy Primal-Dual Coordinate method 1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
2: Initialize: x(0) 0,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
2,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Rd, y(0) 0 2",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Rn,w(0) ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Ax = 0 2 Rn, z(0) ⌘",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A>y = 0 2 Rd 3: for t = 1, 2, · · · , T do 4: Choose greedily the primal coordinate to update:
x̄
(t)
k
argmin ↵
1
n
z (t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k ↵+ g k (↵)
, 8k 2",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"[d] (4)
j (t) argmin k2[d]
1
n
z (t 1) k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
(x̄ (t) k x(t 1) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
+ g k (x̄ (t) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"g k (x (t 1) k )
(5)
x
(t)
k
(
x̄
(t)
k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"if k = j(t), x
(t 1) k otherwise.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"5: Update w to maintain the value of Ax:
w (t) w(t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
+ (x(t) j x(t 1) j ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A j (6) 6: Choose greedily the dual coordinate to update:
i (t) argmax",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k2[n]
|w(t 1) k
1 n",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(
⇤ k ) 0",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
(y (t 1) k ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"| (7)
y
(t)
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k
(
argmax
1
n
w
(t)
k
⇤ k",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"( ) 1 2⌘ ( y(t 1) k ) 2
if k = i(t)
y (t 1)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"k
otherwise.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(8)
7: Update z to maintain the value of A>y z
(t) z(t 1) + (y(t)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
i (t) y(t 1),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
i (t) ),3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"A i (t) (9) 8: end for 9: Output: x(T ),y(T )
in O(d) or O(nnz(Aj)) operations.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This greedy choice of j(t) and aggressive update induces a sufficient primal progress, as shown in Lemma A.1.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Dual Updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
We note that the updates are not exactly symmetric in the primal x and dual y variables.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"The updates for the dual variables y do follow along similar lines as x, except that we use the Gauss-Southwell rule to select variables, and introduce a step size ⌘.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This is motivated by our convergence analysis, which shows that each primal update step yields a large descent in the objective, while each dual update only ascends the dual objective modulo an error term.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
This required a subtle analysis to show that the error terms were canceled out in the end by the progress made in the primal updates.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"But to proceed with such an analysis required the use of a step size in the dual updates, to balance the progress made in the dual updates, and the error term it introduced.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Note moreover, that we are using the Gauss-Southwell rule to choose the variable to optimize in the dual variables y, while we simply use the coordinate that causes the most function descent in the primal variables x.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
This is because our choice of step size in the dual updates required computations that are shared with our current approach of selecting the optimal primal variable.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This does incur more overhead when compared to the Gauss Southwell rule however, so that we simply use the latter for optimizing y.
The most significant feature in our method is that we select
and update one coordinate in both the primal and dual coordinates greedily.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
With a simple trick that maintains the value of w ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
Ax and z ⌘,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
A>y,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"(Lei et al., 2016), we are able to select and update primal and dual coordinates in O(n) and O(d) operations respectively.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"This happens when computing the value of Ax and A>y, which are the bottleneck in computing the gradient or updating the variables.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
An extension to choose and update a batch of primal and dual coordinate is straightforward.,3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"We provide further discussions on the designing of Algorithm 1 in Section 4.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In this paper, we have not incorporated an extrapolation/acceleration scheme to our algorithm.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"As noted earlier, in practice the condition number  is usually comparable to n, thus adding an extrapolation term that reduces the conditioning from /n to p /n is not necessarily materially advantageous in real applications.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Meanwhile, an extrapolation step usually worsens the stability of the algorithm, and is not easily combined with incorporating greedy updates, which is crucial to the leveraging the primal or dual sparsity structure in this paper.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"We thus defer an accelerated extension of our algorithm incorporating extrapolation term to future work.
",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"For Algorithm 1, each iteration can be seen to have a cost of O(n + d), while in Section 4 we show that the iteration complexity for our method is O((1 + 
n )s log(1/✏)) assuming that the primal variables are s-sparse.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"Therefore the overall time complexity for our algorithm is O (1 + 
n
)",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"s(n + d) log(1/✏) , which is cheaper than the
time complexity of even the accelerated SPDC algorithm",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"O (1 +p
n
)nd log(1/✏) except for extremely ill conditioned cases.",3. The Doubly Greedy Primal-Dual (DGPD) Coordinate Descent method,[0],[0]
"In real application settings, Algorithm 1 has some drawbacks.",3.1. A Practical Extension of DGPD,[0],[0]
"When data is sparse, we still require O(n) and O(d) operations to update primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Even when the data is dense, to find the greedy coordinate and to update it requires comparable time complexity, which suggests we should find some ways to eliminate overhead in practice.
",3.1. A Practical Extension of DGPD,[0],[0]
"To resolve these issues, we introduce the Doubly Greedy Primal-Dual Coordinate method with Active Sets in Algorithm 2.",3.1. A Practical Extension of DGPD,[0],[0]
"We make use of what we call active sets, that contains the newly selected coordinates as well as the current non-zero variables.",3.1. A Practical Extension of DGPD,[0],[0]
"We construct these active sets A
x
and A y
for both primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Initially, they are set as empty sets.",3.1. A Practical Extension of DGPD,[0],[0]
"In each iteration, we recurrently select coordinates outside the active sets with the Gauss-Southwell rule, and add them to A
x and A y
.",3.1. A Practical Extension of DGPD,[0],[0]
We then optimize all the variables within the active sets.,3.1. A Practical Extension of DGPD,[0],[0]
"Once a primal/dual variable gets set to 0, we can drop it from the corresponding active sets.",3.1. A Practical Extension of DGPD,[0],[0]
"This practice keeps the active sets A
x and A y
as the support of primal and dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Notice g0
k
(x k ) is 0 when x
k is zero, so that the variable selection step for primal variables can be simplified as stated in (10).
",3.1. A Practical Extension of DGPD,[0],[0]
"Now the time complexity per iteration becomes |A x |n + |A
y |d.",3.1. A Practical Extension of DGPD,[0],[0]
The sparsity in primal variables is encouraged by the choice of `1 + `2 regularization.,3.1. A Practical Extension of DGPD,[0],[0]
"Meanwhile, as shown by (Yen et al., 2016), a sparse set of primal variables usually induces a sparse set of dual variables.",3.1. A Practical Extension of DGPD,[0],[0]
"Therefore |A
x |⌧ d and |A
y | ⌧ n in practice, and the cost per iteration is sub-linear to nd.",3.1. A Practical Extension of DGPD,[0],[0]
We present further details in Section 3.2.,3.1. A Practical Extension of DGPD,[0],[0]
"Suppose we are given a sparse data matrix A with number of non-zero elements of each column and each row bounded by nnz
y and nnz x respectively, one can further reduce the cost for computing (10) and (12) from O(d|A
y | + n|A x |) to O(nnz
x |A y | + nnz y |A x |) by storing both {A i }n i=1",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"and
{Aj}d j=1 as sparse vectors and computing A>y and Ax as
A>y = X
i2A y
A>",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
i y,3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"i
, Ax = X
j2A x
Ajx j .",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"(14)
In our implementation, whenever the active sets A y , A x are expanded, we further maintain a submatrix [A]A which contains only rows in A
y and columns in A x
, so the primal and dual updates (11), (13) only cost P i2A
y
nnz([A i ]",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
A x ).,3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"This results in each update costing less than the search steps, and therefore, in practice, one can conduct multiple rounds of updates (11), (13) before conducting the search (10), (12), which in our experiment speeds up convergence
significantly.",3.2. Efficient Implementation for Sparse Data Matrix,[0],[0]
"In this section, we introduce the primal gap
p
and dual gap
d and analyze the convergence rate in terms of their sum, which we call primal and dual sub-optimality =
p
+
d
.
Definition 4.1.",4. Convergence Analysis,[0],[0]
"For the following convex-concave function L(x,y) def= g(x) + 1
n
y >",4. Convergence Analysis,[0],[0]
"Ax 1 n
P n
i=1",4. Convergence Analysis,[0],[0]
"⇤ i (y i ), with
its primal form P (x) def=",4. Convergence Analysis,[0],[0]
"min y L(x,y), and dual form D(y) def = max
x L(x,y), we define the primal gap at iteration t as
(t) p
def = L(x(t+1),y(t))",4. Convergence Analysis,[0],[0]
"D(y(t))
",4. Convergence Analysis,[0],[0]
", the dual gap at iteration t as
(t) d def = D⇤ D(y(t))
and sub-optimality as
(t) def =
(t) p + (t) d .
Theorem 4.2.",4. Convergence Analysis,[0],[0]
"Suppose in (1), g is µ-strongly convex (`1 + `2) regularization, and i is 1
-smooth.",4. Convergence Analysis,[0],[0]
Let R = max i2[n] kaik2.,4. Convergence Analysis,[0],[0]
"Then DGPD achieves
(t+1)  2n 2n+ ⌘
(t), (15)
if step size ⌘(t) satisfies that
⌘(t)  2n 2µ
kx(t) ¯x(t)k0(5R2 + n µ) (16)
",4. Convergence Analysis,[0],[0]
"Suppose kx(t) ¯x(t)k0  s, if we choose step size ⌘ = 2n2µ (5R2+n µ)s , then it requires
O(s( n + 1) log 1 ✏ )
iterations for achieving ✏ primal and dual sub-optimality.1
Proof sketch:",4. Convergence Analysis,[0],[0]
The proof analysis is straightforward with the introduction of primal and dual sub-optimality .,4. Convergence Analysis,[0],[0]
"We divide the proof into primal-dual progress, primal progress, and dual progress.
",4. Convergence Analysis,[0],[0]
"• Primal-Dual Progress (Lemma A.2).
",4. Convergence Analysis,[0],[0]
(t) d + (t) p ( (t 1),4. Convergence Analysis,[0],[0]
"d + (t 1) p )
 L(x(t+1),yt) L(xt,yt) +⌘( 1
n hA",4. Convergence Analysis,[0],[0]
"i (t) ,x(t)",4. Convergence Analysis,[0],[0]
"¯x(t)i)2
⌘( 1 n hA",4. Convergence Analysis,[0],[0]
"i (t) , ¯x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ i (t)) 0,4. Convergence Analysis,[0],[0]
(y(t),4. Convergence Analysis,[0],[0]
i (t))),4. Convergence Analysis,[0],[0]
"2(17)
1This result can be easily connected to traditional convergence analysis in primal or dual form.",4. Convergence Analysis,[0],[0]
"Notice  ✏ is sufficient requirement that dual gap
d
= D ⇤ D(y)  ✏, therefore the dual variable y(t) converges to optimal y⇤ with the same convergence rate.
",4. Convergence Analysis,[0],[0]
"Algorithm 2 Doubly Greedy Primal-Dual Coordinate method with Active Sets
1: Input: Training data A 2 Rn⇥d, dual step size ⌘ > 0.",4. Convergence Analysis,[0],[0]
2: Initialize: x(0) 0,4. Convergence Analysis,[0],[0]
2,4. Convergence Analysis,[0],[0]
"Rd, y(0) 0 2 Rn, A(0)
",4. Convergence Analysis,[0],[0]
"x
?,A(0) y ?",4. Convergence Analysis,[0],[0]
"3: for t 1, 2, · · · , T do 4: Update the active set A(t)
x
greedily based on the optimal primal variable ¯x(t 1) and update x in its active set.
x̄(t)",4. Convergence Analysis,[0],[0]
"k
argmin ↵
1 n hAk,y(t 1)i↵+ g k (↵) , 8k 2",4. Convergence Analysis,[0],[0]
"[d]
j(t) argmax k2[d] |x̄(t 1) k",4. Convergence Analysis,[0],[0]
"| (10)
A(t) x
A(t 1) x",4. Convergence Analysis,[0],[0]
"[ {j(t)} x(t) j
(
x̄(t 1)",4. Convergence Analysis,[0],[0]
"j , if j 2 A(t) x x(t 1) j , if j /2 A(t) x
(11)
5: Update the active set A(t) y greedily based on the value of r y L(x(t),y(t 1)) and update y in its active set.",4. Convergence Analysis,[0],[0]
"i(t) argmax
k2[n] A(t 1) y
|hA",4. Convergence Analysis,[0],[0]
"k ,x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ k ) 0,4. Convergence Analysis,[0],[0]
(y(t 1) k ),4. Convergence Analysis,[0],[0]
|.,4. Convergence Analysis,[0],[0]
"(12)
A(t) y
A(t 1)",4. Convergence Analysis,[0],[0]
"y [ {i(t)}
y(t)",4. Convergence Analysis,[0],[0]
"i
(
argmax 1 n hA",4. Convergence Analysis,[0],[0]
"i ,x(t)i",4. Convergence Analysis,[0],[0]
1 n,4. Convergence Analysis,[0],[0]
"⇤ i ( ) 12⌘ ( y(t 1)k ) , if i 2 A(t) y y(t 1)",4. Convergence Analysis,[0],[0]
"i , if i /2 A(t) y
(13)
6: Kick out 0 variables from active sets.",4. Convergence Analysis,[0],[0]
"A(t)
y
A(t) y
[
i,y (t)",4. Convergence Analysis,[0],[0]
"i =0
{i}, A(t) x
A(t) x
[
j,x (t) j =0
{j}
7: end for 8: Output: x(T ),y(T )
",4. Convergence Analysis,[0],[0]
This lemma connects the descent in PD sub-optimality with primal progress and dual progress.,4. Convergence Analysis,[0],[0]
"The third term and the second terms respectively represent the potential dual progress if we used the optimal ¯x(t), and the irrelevant part generated from the difference between ¯x(t) and x(t).
",4. Convergence Analysis,[0],[0]
• Primal Progress (Lemma A.1).,4. Convergence Analysis,[0],[0]
"L(x(t),y(t))",4. Convergence Analysis,[0],[0]
"L(x(t+1),y(t)) 1kx(t) ¯x(t)k0 1 (t) p
(18)
",4. Convergence Analysis,[0],[0]
"This inequality simply demonstrates function loss from primal update is at least a ratio of primal gap.
",4. Convergence Analysis,[0],[0]
• Dual Progress (Lemma A.3).,4. Convergence Analysis,[0],[0]
"( 1
n hA",4. Convergence Analysis,[0],[0]
"i (t) ,x(t)",4. Convergence Analysis,[0],[0]
"¯x(t)i)2
( 1 n hA",4. Convergence Analysis,[0],[0]
"i (t) , ¯x(t)i 1 n",4. Convergence Analysis,[0],[0]
( ⇤ i (t)) 0,4. Convergence Analysis,[0],[0]
(y(t),4. Convergence Analysis,[0],[0]
i (t))),4. Convergence Analysis,[0],[0]
"2
 2n (t) d +
5R2 2n2 kx(t) ¯x(t)k2 (19)
",4. Convergence Analysis,[0],[0]
"Finally, we establish the relation between the dual progress in our algorithm with dual gap and difference between ¯x(t) and x(t).",4. Convergence Analysis,[0],[0]
"Now we can prove our main theorem 4.2.
",4. Convergence Analysis,[0],[0]
"For cleaner notation, write a = ⌘ 2n , b = 5⌘R2 2n2 .",4. Convergence Analysis,[0],[0]
k¯x(t) x,4. Convergence Analysis,[0],[0]
"(t)k0  s. By combining (18) and (19) to (17), we get:
(t)",4. Convergence Analysis,[0],[0]
"d
(t 1) d + (t) p
(t 1) p
 L(x(t+1),yt) L(xt,yt) a (t) d
+
2
bkx(t) ¯x(t)k2
 L(x(t+1),yt) L(xt,yt) a (t) d
+b L(x(t),y(t))",4. Convergence Analysis,[0],[0]
"L(¯x(t),y(t))
",4. Convergence Analysis,[0],[0]
"= (1 b) L(x(t+1),yt) L(xt,yt) a (t) d
+b L(x(t+1),yt) L(¯x(t),yt)
 ",4. Convergence Analysis,[0],[0]
1 b s 1 (t) p a (t),4. Convergence Analysis,[0],[0]
"d
+b L(x(t+1),yt) L(¯x(t),yt)
",4. Convergence Analysis,[0],[0]
"= 1 b s 1 b (t) p a (t) d
Here the second inequality comes from strong convexity of L(·,y(t)).",4. Convergence Analysis,[0],[0]
The fourth inequality comes from Lemma A.1.,4. Convergence Analysis,[0],[0]
"Therefore when a  1 b
s 1 b, or sufficiently a  (s(1 + 5/n)) 1, we get (t)  ",4. Convergence Analysis,[0],[0]
11+a (t 1).,4. Convergence Analysis,[0],[0]
"Since a < 1, (a + 1) 1/a  1/2, therefore (t)  (1 + a) t (0) 
2
at (0).",4. Convergence Analysis,[0],[0]
"Therefore when T O(s(1 + /n) log2 (0)
✏
),
(T )  ✏.",4. Convergence Analysis,[0],[0]
"In this section, we give a simple analysis of the greedy variable selection rule showing that when iterate and minimizer of a generic optimization problem are sparse, its convergence rate is faster than choosing random coordinates.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"We define the optimization problem in the space of Rn:
min x2Rn f(x)
, where f is µ-strongly convex L-smooth: |r
i f(x+ ↵e i )",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"r i f(x)|  L|↵|, 8x 2 Rn Under this setting, a random coordinate descent method with step size 1
L , achieves E[f(x+)",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤]  (1 µ nL ) f(x)
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤ , where x+ is the next iterate of x.
Under the assumption that the current iterate x and the optimal x⇤ are both k-sparse, we thereby conduct greedy coordinate descent rule, i.e., x+ = x + ⌘e
i ⇤ , where ⌘, i⇤ satisfies f(x+⌘e
i
⇤ ) =",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"min
i, f(x+ e i ).",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"With L-Lipchitz continuity, we have:
f(x+ ⌘",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
e,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"i ⇤ ) f(x)
 min ,i hrf(x), e",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"i i+ L 2 2
= min
,i
hrf(x),",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"e i i+ L 2 k e i k21
= min
x hrf(x), xi+ L 2 k xk21
 min x
f(x+ x) f(x) + L
2
k xk21
 min 0 1
f(x+ (x⇤ x)) f(x) +",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"L
2
2kx⇤ xk21
 min 0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"+ L
2
2kx⇤ xk21
The last two inequalities are obtained by constraining x to be of the form (x⇤ x) and by the convexity of f .",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"For the k-sparse x, and x⇤, x x⇤ is at most 2k-sparse, and for any 2k-sparse vector a, kak21  2kkak22.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"Hereby we obtain:
min
0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
+,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"L
2
2kx⇤ xk21
 min 0 1 (f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
+,4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"Lk 2kx⇤ xk22
 min 0 1
(f⇤ f(x))",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"2kL
µ 2(f⇤ f(x))
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"=
µ
8kL",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"(f⇤ f(x))
",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
Therefore f(x+),4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"f⇤  (1 µ8kL )(f(x) f⇤), and when k ⌧ n, this convergence rate could be much better than randomized coordinate descent.",4.1. Analysis on greedy methods for sparse iterates,[0],[0]
"In this section, we implement the Doubly-Greedy PrimalDual Coordinate Descent algorithm with Active Sets, and compare its performance with other state-of-the-art methods for `1+`2-regularized Empirical Risk minimization, including Primal Randomized Coordinate Descent (PrimalRCD) (Richtárik & Takác, 2014), Dual Randomized Coordinate Descent (DualRCD, i.e., SDCA)",5. Experiment,[0],[0]
"(Shalev-Shwartz & Zhang, 2013b) and the Stochastic Primal-Dual Coordinate Method (SPDC) (Zhang & Xiao, 2014).
",5. Experiment,[0],[0]
"We conduct experiments on large-scale multi-class data sets with linear and non-linear feature mappings, as shown in Table 2.",5. Experiment,[0],[0]
"For Mnist and Aloi we use Random Fourier (RF) and Random Binning (RB) feature proposed in (Rahimi & Recht, 2007) to approximate effect of RBF Gaussian kernel and Laplacian Kernel respectively.",5. Experiment,[0],[0]
"The features generated by Random Fourier are dense, while Random Binning gives highly sparse data.
",5. Experiment,[0],[0]
"We give results for 2 {0.1, 0.01} and µ 2 {1, 0.1, 0.01}, where Figure 1 shows results for = 0.1, µ = 0.01 and others can be found in Appendix B. In the above six figures, we compare the running time with objective function.",5. Experiment,[0],[0]
"While in the below figures, the x-axis is number of iterations.",5. Experiment,[0],[0]
"For the baseline methods, one iteration is one pass over all the variables, and for our method, it is several (5) passes over the active sets.",5. Experiment,[0],[0]
"From the figures, we can see that in all cases, DGPD has better performance than other methods.",5. Experiment,[0],[0]
"Notice for clear presentation purposes we use log-scale for Mnist-RB-time, Aloi-RB-time and RCV-time, where our algorithm achieves improvements over others of orders of magnitude.
",5. Experiment,[0],[0]
"The result shows that, by exploiting sparsity in both the primal and dual, DGPD has much less cost per iteration and thus is considerably faster in terms of training time, while by maintaining an active set it does not sacrifice much in terms of convergence rate.",5. Experiment,[0],[0]
"Note since in practice we perform multiple updates after each search, the convergence rate (measured in outer iterations) can be sometimes even better than DualRCD.",5. Experiment,[0],[0]
"I.D. acknowledges the support of NSF via CCF-1320746, IIS-1546452, and CCF-1564000.",6. Acknowledgements,[0],[0]
"P.R. acknowledges the support of ARO via W911NF-12-1-0390 and NSF via IIS1149803, IIS-1320894, IIS-1447574, and DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.",6. Acknowledgements,[0],[0]
We consider the popular problem of sparse empirical risk minimization with linear predictors and a large number of both features and observations.,abstractText,[0],[0]
"With a convex-concave saddle point objective reformulation, we propose a Doubly Greedy PrimalDual Coordinate Descent algorithm that is able to exploit sparsity in both primal and dual variables.",abstractText,[0],[0]
"It enjoys a low cost per iteration and our theoretical analysis shows that it converges linearly with a good iteration complexity, provided that the set of primal variables is sparse.",abstractText,[0],[0]
We then extend this algorithm further to leverage active sets.,abstractText,[0],[0]
"The resulting new algorithm is even faster, and experiments on large-scale Multi-class data sets show that our algorithm achieves up to 30 times speedup on several state-of-the-art optimization methods.",abstractText,[0],[0]
Doubly Greedy Primal-Dual Coordinate Descent for Sparse Empirical Risk Minimization,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1460–1469 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Natural Language Inference (NLI; a.k.a.,1 Introduction,[0],[0]
"Recognizing Textual Entailment, or RTE) is an important and challenging task for natural language understanding (MacCartney and Manning, 2008).",1 Introduction,[0],[0]
"The goal of NLI is to identify the logical relationship (entailment, neutral, or contradiction) between a premise and a corresponding hypothesis.",1 Introduction,[0],[0]
"Table 1 shows few example relationships from the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015).
",1 Introduction,[0],[0]
"Recently, NLI has received a lot of attention from the researchers, especially due to the avail-
∗ ArXiv version of this work can be found here (arxiv.org/pdf/1802.05577.pdf).
†",1 Introduction,[0],[0]
"This work was conducted as part of an internship program at Philips Research.
ability of large annotated datasets like SNLI (Bowman et al., 2015).",1 Introduction,[0],[0]
"Various deep learning models have been proposed that achieve successful results for this task (Gong et al., 2017; Wang et al., 2017; Chen et al., 2017; Yu and Munkhdalai, 2017a; Parikh et al., 2016; Zhao et al., 2016; Sha et al., 2016).",1 Introduction,[0],[0]
Most of these existing NLI models use attention mechanism to jointly interpret and align the premise and hypothesis.,1 Introduction,[0],[0]
Such models use simple reading mechanisms to encode the premise and hypothesis independently.,1 Introduction,[0],[0]
"However, such a complex task require explicit modeling of dependency relationships between the premise and the hypothesis during the encoding and inference processes to prevent the network from the loss of relevant, contextual information.",1 Introduction,[0],[0]
"In this paper, we refer to such strategies as dependent reading.
",1 Introduction,[0],[0]
"There are some alternative reading mechanisms available in the literature (Sha et al., 2016; Rocktäschel et al., 2015) that consider dependency aspects of the premise-hypothesis relationships.",1 Introduction,[0],[0]
"However, these mechanisms have two major limitations:
• So far, they have only explored dependency aspects during the encoding stage, while ignoring its benefit during inference.
",1 Introduction,[0],[0]
"• Such models only consider encoding a hy-
1460
pothesis depending on the premise, disregarding the dependency aspects in the opposite direction.
",1 Introduction,[0],[0]
We propose a dependent reading bidirectional LSTM (DR-BiLSTM) model to address these limitations.,1 Introduction,[0],[0]
"Given a premise u and a hypothesis v, our model first encodes them considering dependency on each other (u|v and v|u).",1 Introduction,[0],[0]
"Next, the model employs a soft attention mechanism to extract relevant information from these encodings.",1 Introduction,[0],[0]
"The augmented sentence representations are then passed to the inference stage, which uses a similar dependent reading strategy in both directions, i.e. u→ v and v →",1 Introduction,[0],[0]
"u. Finally, a decision is made through a multi-layer perceptron (MLP) based on the aggregated information.
",1 Introduction,[0],[0]
"Our experiments on the SNLI dataset show that DR-BiLSTM achieves the best single model and ensemble model performance obtaining improvements of a considerable margin of 0.4% and 0.3% over the previous state-of-the-art single and ensemble models, respectively.
",1 Introduction,[0],[0]
"Furthermore, we demonstrate the importance of a simple preprocessing step performed on the SNLI dataset.",1 Introduction,[0],[0]
Evaluation results show that such preprocessing allows our single model to achieve the same accuracy as the state-of-the-art ensemble model and improves our ensemble model to outperform the state-of-the-art ensemble model by a remarkable margin of 0.7%.,1 Introduction,[0],[0]
"Finally, we perform an extensive analysis to clarify the strengths and weaknesses of our models.",1 Introduction,[0],[0]
"Early studies use small datasets while leveraging lexical and syntactic features for NLI (MacCartney and Manning, 2008).",2 Related Work,[0],[0]
"The recent availability of large-scale annotated datasets (Bowman et al., 2015; Williams et al., 2017) has enabled researchers to develop various deep learning-based architectures for NLI.
Parikh et al. (2016) propose an attention-based model (Bahdanau et al., 2014) that decomposes the NLI task into sub-problems to solve them in parallel.",2 Related Work,[0],[0]
They further show the benefit of adding intra-sentence attention to input representations.,2 Related Work,[0],[0]
Chen et al. (2017) explore sequential inference models based on chain LSTMs with attentional input encoding and demonstrate the effectiveness of syntactic information.,2 Related Work,[0],[0]
We also use similar attention mechanisms.,2 Related Work,[0],[0]
"However, our model is distinct
from these models as they do not benefit from dependent reading strategies.
",2 Related Work,[0],[0]
Rocktäschel et al. (2015) use a word-by-word neural attention mechanism while Sha et al. (2016) propose re-read LSTM units by considering the dependency of a hypothesis on the information of its premise (v|u) to achieve promising results.,2 Related Work,[0],[0]
"However, these models suffer from weak inferencing methods by disregarding the dependency aspects from the opposite direction (u|v).",2 Related Work,[0],[0]
"Intuitively, when a human judges a premise-hypothesis relationship, s/he might consider back-and-forth reading of both sentences before coming to a conclusion.",2 Related Work,[0],[0]
"Therefore, it is essential to encode the premise-hypothesis dependency relations from both directions to optimize the understanding of their relationship.
",2 Related Work,[0],[0]
"Wang et al. (2017) propose a bilateral multiperspective matching (BiMPM) model, which resembles the concept of matching a premise and hypothesis from both directions.",2 Related Work,[0],[0]
Their matching strategy is essentially similar to our attention mechanism that utilizes relevant information from the other sentence for each word sequence.,2 Related Work,[0],[0]
"They use similar methods as Chen et al. (2017) for encoding and inference, without any dependent reading mechanism.
",2 Related Work,[0],[0]
"Although NLI is well studied in the literature, the potential of dependent reading and interaction between a premise and hypothesis is not rigorously explored.",2 Related Work,[0],[0]
"In this paper, we address this gap by proposing a novel deep learning model (DRBiLSTM).",2 Related Work,[0],[0]
Experimental results demonstrate the effectiveness of our model.,2 Related Work,[0],[0]
"Our proposed model (DR-BiLSTM) is composed of the following major components: input encoding, attention, inference, and classification.",3 Model,[0],[0]
"Figure 1 demonstrates a high-level view of our proposed NLI framework.
",3 Model,[0],[0]
Let u =,3 Model,[0],[0]
"[u1, · · · , un] and v =",3 Model,[0],[0]
"[v1, · · · , vm] be the given premise with length n and hypothesis with length m respectively, where ui, vj ∈",3 Model,[0],[0]
Rr is an word embedding of r-dimensional vector.,3 Model,[0],[0]
The task is to predict a label y that indicates the logical relationship between premise u and hypothesis v.,3 Model,[0],[0]
"RNNs are the natural solution for variable length sequence modeling, consequently, we utilize a
bidirectional LSTM (BiLSTM) (Hochreiter and Schmidhuber, 1997) for encoding the given sentences.",3.1 Input Encoding,[0],[0]
"For ease of presentation, we only describe how we encode u depending on v. The same procedure is utilized for the reverse direction (v|u).
",3.1 Input Encoding,[0],[0]
"To dependently encode u, we first process v using the BiLSTM.",3.1 Input Encoding,[0],[0]
Then we read u through the BiLSTM that is initialized with previous reading final states (memory cell and hidden state).,3.1 Input Encoding,[0],[0]
Here we represent a word (e.g. ui) and its context depending on the other sentence (e.g. v).,3.1 Input Encoding,[0],[0]
"Equations 1 and 2 formally represent this component.
",3.1 Input Encoding,[0],[0]
"v̄, sv = BiLSTM(v, 0) û,− = BiLSTM(u, sv) (1)
ū, su = BiLSTM(u, 0)
v̂,− = BiLSTM(v, su) (2)
where {ū ∈ Rn×2d, û ∈ Rn×2d, su} and {v̄ ∈ Rm×2d, v̂ ∈ Rm×2d, sv} are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of u and v respectively.",3.1 Input Encoding,[0],[0]
"Note that, “−” in these equations means that we do not care about the associated variable and its value.",3.1 Input Encoding,[0],[0]
BiLSTM inputs are the word embedding sequences and initial state vectors.,3.1 Input Encoding,[0],[0]
"û and v̂ are passed to the next layer as the output of the input encoding component.
",3.1 Input Encoding,[0],[0]
The proposed encoding mechanism yields a richer representation for both premise and hypothesis by taking the history of each other into account.,3.1 Input Encoding,[0],[0]
Using a max or average pooling over the independent and dependent readings does not further improve our model.,3.1 Input Encoding,[0],[0]
This was expected since dependent reading produces more promising and relevant encodings.,3.1 Input Encoding,[0],[0]
We employ a soft alignment method to associate the relevant sub-components between the given premise and hypothesis.,3.2 Attention,[0],[0]
"In deep learning models, such purpose is often achieved with a soft attention mechanism.",3.2 Attention,[0],[0]
"Here we compute the unnormalized attention weights as the similarity of hidden states of the premise and hypothesis with Equation 3 (energy function).
",3.2 Attention,[0],[0]
"eij = ûiv̂ T j , i ∈",3.2 Attention,[0],[0]
"[1, n], j ∈",3.2 Attention,[0],[0]
"[1,m] (3)
where ûi and v̂j are the dependent reading hidden representations of u and v respectively which are computed earlier in Equations 1 and 2.",3.2 Attention,[0],[0]
"Next, for each word in either premise or hypothesis, the relevant semantics in the other sentence is extracted and composed according to eij .",3.2 Attention,[0],[0]
"Equations 4 and 5 provide formal and specific details of this procedure.
",3.2 Attention,[0],[0]
"ũi = m∑
j=1
exp(eij)∑m",3.2 Attention,[0],[0]
"k=1 exp(eik) v̂j , i ∈",3.2 Attention,[0],[0]
"[1, n] (4)
ṽj =
n∑
i=1
exp(eij)∑n k=1 exp(ekj) ûi, j ∈",3.2 Attention,[0],[0]
"[1,m] (5)
where ũi represents the extracted relevant information of v̂ by attending to ûi while ṽj represents the extracted relevant information of û by attending to v̂j .
",3.2 Attention,[0],[0]
"To further enrich the collected attentional information, a trivial next step would be to pass the concatenation of the tuples (ûi, ũi) or (v̂j , ṽj) which provides a linear relationship between them.",3.2 Attention,[0],[0]
"However, the model would suffer from the absence of similarity and closeness measures.",3.2 Attention,[0],[0]
"Therefore, we calculate the difference and element-wise product for the tuples (ûi, ũi) and (v̂j , ṽj) that represent the similarity and closeness information respectively (Chen et al., 2017; Kumar et al., 2016).
",3.2 Attention,[0],[0]
"The difference and element-wise product are then concatenated with the computed vectors, (ûi, ũi) or (v̂j , ṽj), respectively.",3.2 Attention,[0],[0]
"Finally, a feedforward neural layer with ReLU activation function projects the concatenated vectors from 8ddimensional vector space into a d-dimensional vector space (Equations 6 and 7).",3.2 Attention,[0],[0]
"This helps the model to capture deeper dependencies between the sentences besides lowering the complexity of vector representations.
",3.2 Attention,[0],[0]
ai =,3.2 Attention,[0],[0]
"[ûi, ũi, ûi − ũi, ûi ũi] pi = ReLU(Wpai + bp)
(6)
bj =",3.2 Attention,[0],[0]
"[v̂j , ṽj , v̂j − ṽj , v̂j ṽj ] qj = ReLU(Wpbj + bp)
(7)
Here stands for element-wise product while Wp ∈ R8d×d and bp ∈ Rd are the trainable weights and biases of the projector layer respectively.",3.2 Attention,[0],[0]
"During this phase, we use another BiLSTM to aggregate the two sequences of computed matching vectors, p and q from the attention stage (Section 3.2).",3.3 Inference,[0],[0]
"This aggregation is performed in a sequential manner to avoid losing effect of latent variables that might rely on the sequence of matching vectors.
",3.3 Inference,[0],[0]
"Instead of aggregating the sequences of matching vectors individually, we propose a similar dependent reading approach for the inference stage.",3.3 Inference,[0],[0]
We employ a BiLSTM reading process (Equations 8 and 9) similar to the input encoding step discussed in Section 3.1.,3.3 Inference,[0],[0]
"But rather than passing just the dependent reading information to the next step, we feed both independent reading (p̄ and q̄) and dependent reading (p̂ and q̂) to a max pooling layer, which selects maximum values from
each sequence of independent and dependent readings (p̄i and p̂i) as shown in Equations 10 and 11.",3.3 Inference,[0],[0]
"The main intuition behind this architecture is to maximize the inferencing ability of the model by considering both independent and dependent readings.
",3.3 Inference,[0],[0]
"q̄, sq = BiLSTM(q, 0) p̂,− = BiLSTM(p, sq) (8)
p̄, sp = BiLSTM(p, 0) q̂,− = BiLSTM(q, sp) (9)
p̃ = MaxPooling(p̄, p̂) (10)
q̃ = MaxPooling(q̄, q̂) (11)
Here {p̄ ∈ Rn×2d, p̂ ∈ Rn×2d, sp} and {q̄ ∈ Rm×2d, q̂ ∈ Rm×2d, sq} are the independent reading sequences, dependent reading sequences, and BiLSTM final state of independent reading of p and q respectively.",3.3 Inference,[0],[0]
"BiLSTM inputs are the word embedding sequences and initial state vectors.
",3.3 Inference,[0],[0]
"Finally, we convert p̃ ∈ Rn×2d and q̃ ∈ Rm×2d to fixed-length vectors with pooling, U ∈ R4d and V ∈ R4d.",3.3 Inference,[0],[0]
"As shown in Equations 12 and 13, we employ both max and average pooling and describe the overall inference relationship with concatenation of their outputs.
",3.3 Inference,[0],[0]
U =,3.3 Inference,[0],[0]
"[MaxPooling(p̃),AvgPooling(p̃)]",3.3 Inference,[0],[0]
"(12)
V = [MaxPooling(q̃),AvgPooling(q̃)]",3.3 Inference,[0],[0]
(13),3.3 Inference,[0],[0]
"Here, we feed the concatenation of U and V ([U, V ]) into a multilayer perceptron (MLP) classifier that includes a hidden layer with tanh activation and softmax output layer.",3.4 Classification,[0],[0]
"The model is trained in an end-to-end manner.
",3.4 Classification,[0],[0]
"Output = MLP([U, V ]) (14)",3.4 Classification,[0],[0]
The Stanford Natural Language Inference (SNLI) dataset contains 570K human annotated sentence pairs.,4.1 Dataset,[0],[0]
"The premises are drawn from the Flickr30k (Plummer et al., 2015) corpus, and then the hypotheses are manually composed for each relationship class (entailment, neutral, contradiction, and -).",4.1 Dataset,[0],[0]
"The “-” class indicates that there is no consensus decision among the annotators, consequently, we remove them during the training and evaluation following the literature.",4.1 Dataset,[0],[0]
We use the same data split as provided in Bowman et al. (2015) to report comparable results with other models.,4.1 Dataset,[0],[0]
"We use pre-trained 300-D Glove 840B vectors (Pennington et al., 2014) to initialize our word embedding vectors.",4.2 Experimental Setup,[0],[0]
All hidden states of BiLSTMs during input encoding and inference have 450 dimensions (r = 300 and d = 450).,4.2 Experimental Setup,[0],[0]
"The weights are learned by minimizing the log-loss on the training data via the Adam optimizer (Kingma and Ba, 2014).",4.2 Experimental Setup,[0],[0]
The initial learning rate is 0.0004.,4.2 Experimental Setup,[0],[0]
"To avoid overfitting, we use dropout (Srivastava et al., 2014) with the rate of 0.4 for regularization, which is applied to all feedforward connections.",4.2 Experimental Setup,[0],[0]
"During training, the word embeddings are updated to learn effective representations for the NLI task.",4.2 Experimental Setup,[0],[0]
We use a fairly small batch size of 32 to provide more exploration power to the model.,4.2 Experimental Setup,[0],[0]
Our observation indicates that using larger batch sizes hurts the performance of our model.,4.2 Experimental Setup,[0],[0]
Ensemble methods use multiple models to obtain better predictive performance.,4.3 Ensemble Strategy,[0],[0]
"Previous works typically utilize trivial ensemble strategies by either using majority votes or averaging the probability distributions over the same model with different initialization seeds (Wang et al., 2017; Gong et al., 2017).
",4.3 Ensemble Strategy,[0],[0]
"By contrast, we use weighted averaging of the probability distributions where the weight of each model is learned through its performance on the SNLI development set.",4.3 Ensemble Strategy,[0],[0]
"Furthermore, the differences between our models in the ensemble originate from: 1) variations in the number of dependent readings (i.e. 1 and 3 rounds of dependent reading), 2) projection layer activation (tanh and
ReLU in Equations 6 and 7), and 3) different initialization seeds.
",4.3 Ensemble Strategy,[0],[0]
The main intuition behind this design is that the effectiveness of a model may depend on the complexity of a premise-hypothesis instance.,4.3 Ensemble Strategy,[0],[0]
"For a simple instance, a simple model could perform better than a complex one, while a complex instance may need further consideration toward disambiguation.",4.3 Ensemble Strategy,[0],[0]
"Consequently, using models with different rounds of dependent readings in the encoding stage should be beneficial.
",4.3 Ensemble Strategy,[0],[0]
Figure 2 demonstrates the observed performance of our ensemble method with different number of models.,4.3 Ensemble Strategy,[0],[0]
The performance of the models are reported based on the best obtained accuracy on the development set.,4.3 Ensemble Strategy,[0],[0]
"We also study the effectiveness of other ensemble strategies e.g. majority voting, and averaging the probability distributions.",4.3 Ensemble Strategy,[0],[0]
"But, our ensemble strategy performs the best among them (see Section 1 in the supplementary material for additional details).",4.3 Ensemble Strategy,[0],[0]
We perform a trivial preprocessing step on SNLI to recover some out-of-vocabulary words found in the development set and test set.,4.4 Preprocessing,[0],[0]
"Note that our vocabulary contains all words that are seen in the training set, so there is no out-of-vocabulary word in it.",4.4 Preprocessing,[0],[0]
"The SNLI dataset is not immune to human
errors, specifically, misspelled words.",4.4 Preprocessing,[0],[0]
We noticed that misspelling is the main reason for some of the observed out-of-vocabulary words.,4.4 Preprocessing,[0],[0]
"Consequently, we simply fix the unseen misspelled words using Microsoft spell-checker (other approaches like edit distance can also be used).",4.4 Preprocessing,[0],[0]
"Moreover, while dealing with an unseen word during evaluation, we try to: 1) replace it with its lower case, or 2) split the word when it contains a “-” (e.g. “marsh-like”) or starts with “un” (e.g. “unloading”).",4.4 Preprocessing,[0],[0]
"If we still could not find the word in our vocabulary, we consider it as an unknown word.",4.4 Preprocessing,[0],[0]
"In the next subsection, we demonstrate the importance and impact of such trivial preprocessing (see Section 2 in the supplementary material for additional details).",4.4 Preprocessing,[0],[0]
Table 2 shows the accuracy of the models on training and test sets of SNLI.,4.5 Results,[0],[0]
The first row represents a baseline classifier presented by Bowman et al. (2015) that utilizes handcrafted features.,4.5 Results,[0],[0]
All other listed models are deep-learning based.,4.5 Results,[0],[0]
The gap between the traditional model and deep learning models demonstrates the effectiveness of deep learning methods for this task.,4.5 Results,[0],[0]
"We also report the estimated human performance on the SNLI dataset, which is the average accuracy of five annotators in comparison to the gold labels (Gong et al., 2017).",4.5 Results,[0],[0]
"It is noteworthy that recent deep learning models surpass the human performance in the NLI task.
",4.5 Results,[0],[0]
"As shown in Table 2, previous deep learning models (rows 2-19) can be divided into three categories: 1) sentence encoding based models (rows 2-7), 2) single inter-sentence attention-based models (rows 8-16), and 3) ensemble inter-sentence attention-based models (rows 17-19).",4.5 Results,[0],[0]
"We can see that inter-sentence attention-based models perform better than sentence encoding based models, which supports our intuition.",4.5 Results,[0],[0]
Natural language inference requires a deep interaction between the premise and hypothesis.,4.5 Results,[0],[0]
"Inter-sentence attention-based approaches can provide such interaction while sentence encoding based models fail to do so.
",4.5 Results,[0],[0]
"To further enhance the modeling of interaction between the premise and hypothesis for efficient disambiguation of their relationship, we introduce the dependent reading strategy in our proposed DR-BiLSTM model.",4.5 Results,[0],[0]
The results demonstrate the effectiveness of our model.,4.5 Results,[0],[0]
"DR-BiLSTM (Single)
achieves 88.5% accuracy on the test set which is noticeably the best reported result among the existing single models for this task.",4.5 Results,[0],[0]
"Note that the difference between DR-BiLSTM and Chen et al. (2017) is statistically significant with a p-value of < 0.001 over the Chi-square test1.
To further improve the performance of NLI systems, researchers have built ensemble models.",4.5 Results,[0],[0]
"Previously, ensemble systems obtained the best performance on SNLI with a huge margin.",4.5 Results,[0],[0]
Table 2 shows that our proposed single model achieves competitive results compared to these reported ensemble models.,4.5 Results,[0],[0]
"Our ensemble model considerably outperforms the current state-of-the-art by obtaining 89.3% accuracy.
",4.5 Results,[0],[0]
"Up until this point, we discussed the performance of our models where we have not con-
1Chi-square test (χ2 test) is used to determine if there is a significant difference between two categorical variables (i.e. models’ outputs).
",4.5 Results,[0],[0]
sidered preprocessing for recovering the out-ofvocabulary words.,4.5 Results,[0],[0]
"In Table 2, “DR-BiLSTM (Single) + Process”, and “DR-BiLSTM (Ensem.)",4.5 Results,[0],[0]
+ Process” represent the performance of our models on the preprocessed dataset.,4.5 Results,[0],[0]
We can see that our preprocessing mechanism leads to further improvements of 0.4% and 0.3% on the SNLI test set for our single and ensemble models respectively.,4.5 Results,[0],[0]
"In fact, our single model (“DR-BiLSTM (Single) + Process”) obtains the state-of-the-art performance over both reported single and ensemble models by performing a simple preprocessing step.",4.5 Results,[0],[0]
"Furthermore, “DR-BiLSTM (Ensem.)",4.5 Results,[0],[0]
+,4.5 Results,[0],[0]
Process” outperforms the existing state-of-the-art remarkably (0.7% improvement).,4.5 Results,[0],[0]
"For more comparison and analyses, we use “DR-BiLSTM (Single)” and “DR-BiLSTM (Ensemble)” as our single and ensemble models in the rest of the paper.",4.5 Results,[0],[0]
We conducted an ablation study on our model to examine the importance and effect of each major component.,4.6 Ablation and Configuration Study,[0],[0]
"Then, we study the impact of BiLSTM dimensionality on the performance of the development set and training set of SNLI.",4.6 Ablation and Configuration Study,[0],[0]
"We investigate all settings on the development set of the SNLI dataset.
",4.6 Ablation and Configuration Study,[0],[0]
"Table 3 shows the ablation study results on the development set of SNLI along with the statistical significance test results in comparison to the proposed model, DR-BiLSTM.",4.6 Ablation and Configuration Study,[0],[0]
"We can see that all modifications lead to a new model and their differ-
ences are statistically significant with a p-value of < 0.001 over Chi square test.
",4.6 Ablation and Configuration Study,[0],[0]
Table 3 shows that removing any part from our model hurts the development set accuracy which indicates the effectiveness of these components.,4.6 Ablation and Configuration Study,[0],[0]
"Among all components, three of them have noticeable influences: max pooling, difference in the attention stage, and dependent reading.
",4.6 Ablation and Configuration Study,[0],[0]
"Most importantly, the last four study cases in Table 3 (rows 8-11) verify the main intuitions behind our proposed model.",4.6 Ablation and Configuration Study,[0],[0]
"They illustrate the importance of our proposed dependent reading strategy which leads to significant improvement, specifically in the encoding stage.",4.6 Ablation and Configuration Study,[0],[0]
"We are convinced that the importance of dependent reading in the encoding stage originates from its ability to focus on more important and relevant aspects of the sentences due to its prior knowledge of the other sentence during the encoding procedure.
",4.6 Ablation and Configuration Study,[0],[0]
Figure 3 shows the behavior of the proposed model accuracy on the training set and development set of SNLI.,4.6 Ablation and Configuration Study,[0],[0]
"Since the models are selected based on the best observed development set accuracy during the training procedure, the training accuracy curve (red, top) is not strictly increasing.",4.6 Ablation and Configuration Study,[0],[0]
Figure 3 demonstrates that we achieve the best performance with 450-dimensional BiLSTMs.,4.6 Ablation and Configuration Study,[0],[0]
"In other words, using BiLSTMs with lower dimensionality causes the model to suffer from the lack of space for capturing proper information and dependencies.",4.6 Ablation and Configuration Study,[0],[0]
"On the other hand, using higher dimensionality leads to overfitting which hurts the performance on the development set.",4.6 Ablation and Configuration Study,[0],[0]
"Hence, we use 450-dimensional BiLSTM in our proposed
model.",4.6 Ablation and Configuration Study,[0],[0]
We first investigate the performance of our models categorically.,4.7 Analysis,[0],[0]
"Then, we show a visualization of the energy function in the attention stage (Equation 3) for an instance from the SNLI test set.
",4.7 Analysis,[0],[0]
"To qualitatively evaluate the performance of our models, we design a set of annotation tags that can be extracted automatically.",4.7 Analysis,[0],[0]
This design is inspired by the reported annotation tags in Williams et al. (2017).,4.7 Analysis,[0],[0]
"The specifications of our annotation tags are as follows:
• High Overlap: premise and hypothesis sentences share more than 70% tokens.
",4.7 Analysis,[0],[0]
"• Regular Overlap: sentences share between 30% and 70% tokens.
",4.7 Analysis,[0],[0]
"• Low Overlap: sentences share less than 30% tokens.
",4.7 Analysis,[0],[0]
"• Long Sentence: either sentence is longer than 20 tokens.
",4.7 Analysis,[0],[0]
"• Regular Sentence: premise or hypothesis length is between 5 and 20 tokens.
",4.7 Analysis,[0],[0]
"• Short Sentence: either sentence is shorter than 5 tokens.
",4.7 Analysis,[0],[0]
"• Negation: negation is present in a sentence.
",4.7 Analysis,[0],[0]
"• Quantifier: either of the sentences contains one of the following quantifiers: much, enough, more, most, less, least, no, none, some, any, many, few, several, almost, nearly.
",4.7 Analysis,[0],[0]
"• Belief: either of the sentences contains one of the following belief verbs: know, believe, understand, doubt, think, suppose, recognize, forget, remember, imagine, mean, agree, disagree, deny, promise.
",4.7 Analysis,[0],[0]
"Table 4 shows the frequency of aforementioned annotation tags in the SNLI test set along with the performance (accuracy) of ESIM (Chen et al., 2017), DR-BiLSTM (Single), and DR-BiLSTM (Ensemble).",4.7 Analysis,[0],[0]
"Table 4 can be divided into four major categories: 1) gold label data, 2) word overlap, 3) sentence length, and 4) occurrence of special words.",4.7 Analysis,[0],[0]
We can see that DR-BiLSTM (Ensemble) performs the best in all categories which matches our expectation.,4.7 Analysis,[0],[0]
"Moreover, DR-BiLSTM (Single)
performs noticeably better than ESIM in most of the categories except “Entailment”, “High Overlap”, and “Long Sentence”, for which our model is not far behind (gaps of 0.2%, 0.5%, and 0.9%, respectively).",4.7 Analysis,[0],[0]
It is noteworthy that DR-BiLSTM (Single) performs better than ESIM in more frequent categories.,4.7 Analysis,[0],[0]
"Specifically, the performance of our model in “Neutral”, “Negation”, and “Quantifier” categories (improvements of 1.4%, 3.5%, and 1.9%, respectively) indicates the superiority of our model in understanding and disambiguating complex samples.",4.7 Analysis,[0],[0]
Our investigations indicate that ESIM generates somewhat uniform attention for most of the word pairs while our model could effectively attend to specific parts of the given sentences and provide more meaningful attention.,4.7 Analysis,[0],[0]
"In other words, the dependent reading strategy enables our model to achieve meaningful representations, which leads to better attention to obtain further gains on such categories like Negation and Quantifier sentences (see Section 3 in the supplementary material for additional details).
",4.7 Analysis,[0],[0]
"Finally, we show a visualization of the normalized attention weights (energy function, Equation 3) of our model in Figure 4.",4.7 Analysis,[0],[0]
"We show a sentence pair, where the premise is “Male in a blue jacket decides to lay the grass.”, and the hypothesis is “The guy in yellow is rolling on the grass.”, and its logical relationship is contradiction.",4.7 Analysis,[0],[0]
"Figure 4 indicates the model’s ability in attending to critical pairs of words like <Male, guy>, <decides, rolling>, and <lay, rolling>.",4.7 Analysis,[0],[0]
"Finally, high attention between {decides, lay} and
{rolling}, and {Male} and {guy} leads the model to correctly classify the sentence pair as contradiction (for more samples with attention visualizations, see Section 4 in the supplementary material).",4.7 Analysis,[0],[0]
We propose a novel natural language inference model (DR-BiLSTM) that benefits from a dependent reading strategy and achieves the state-of-theart results on the SNLI dataset.,5 Conclusion,[0],[0]
We also introduce a sophisticated ensemble strategy and illustrate its effectiveness through experimentation.,5 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of a simple preprocessing step on the performance of our proposed models.",5 Conclusion,[0],[0]
Evaluation results show that the preprocessing step allows our DR-BiLSTM (single) model to outperform all previous single and ensemble methods.,5 Conclusion,[0],[0]
Similar superior performance is also observed for our DR-BiLSTM (ensemble) model.,5 Conclusion,[0],[0]
We show that our ensemble model outperforms the existing state-of-the-art by a considerable margin of 0.7%.,5 Conclusion,[0],[0]
"Finally, we perform an extensive analysis to demonstrate the strength and weakness of the proposed model, which would pave the way for further improvements in this domain.",5 Conclusion,[0],[0]
We present a novel deep learning architecture to address the natural language inference (NLI) task.,abstractText,[0],[0]
Existing approaches mostly rely on simple reading mechanisms for independent encoding of the premise and hypothesis.,abstractText,[0],[0]
"Instead, we propose a novel dependent reading bidirectional LSTM network (DR-BiLSTM) to efficiently model the relationship between a premise and a hypothesis during encoding and inference.",abstractText,[0],[0]
"We also introduce a sophisticated ensemble strategy to combine our proposed models, which noticeably improves final predictions.",abstractText,[0],[0]
"Finally, we demonstrate how the results can be improved further with an additional preprocessing step.",abstractText,[0],[0]
Our evaluation shows that DR-BiLSTM obtains the best single model and ensemble model results achieving the new state-of-the-art scores on the Stanford NLI dataset.,abstractText,[0],[0]
DR-BiLSTM: Dependent Reading Bidirectional LSTM for Natural Language Inference,title,[0],[0]
"Deep learning models have been used to obtain state-ofthe-art results on many tasks (Krizhevsky et al., 2012; Szegedy et al., 2014; Sutskever et al., 2014; Sundermeyer et al., 2012; Mikolov et al., 2010; Kalchbrenner & Blunsom, 2013), and in many pipelines these models have replaced the more traditional Bayesian probabilistic models (Sennrich et al., 2016).",1. Introduction,[0],[0]
"But unlike deep learning models, Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing the models’ ignorance about the world, and able to convey their increased uncertainty on out-of-data examples.",1. Introduction,[0],[0]
"This
1University of Cambridge, UK 2The Alan Turing Institute, UK.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Yingzhen Li <yl494@cam.ac.uk>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
information can be used, for example, to identify when a vision model is given an adversarial image (studied below), or to tackle many problems in AI safety (Amodei et al., 2016).",1. Introduction,[0],[0]
"With model uncertainty at hand, applications as farreaching as safety in self-driving cars can be explored, using models which can propagate their uncertainty up the decision making pipeline (Gal, 2016).",1. Introduction,[0],[0]
"With deterministic deep learning models this invaluable uncertainty information is often lost.
",1. Introduction,[0],[0]
"Bayesian deep learning – an approach to combining Bayesian probability theory together with deep learning – allows us to use state-of-the-art models and at the same time obtain model uncertainty (Gal, 2016; Gal & Ghahramani, 2016a).",1. Introduction,[0],[0]
"Originating in the 90s (Neal, 1995; MacKay, 1992; Denker & LeCun, 1991), Bayesian neural networks (BNNs) in particular have started gaining in popularity again (Graves, 2011; Blundell et al., 2015; HernandezLobato & Adams, 2015).",1. Introduction,[0],[0]
BNNs are standard neural networks (NNs) with prior probability distributions placed over their weights.,1. Introduction,[0],[0]
"Given observed data, inference is then performed to find what are the more likely and less likely weights to explain the data.",1. Introduction,[0],[0]
"But as easy it is to formulate BNNs, is as difficult to perform inference in them.",1. Introduction,[0],[0]
"Many approximations have been proposed over the years (Denker & LeCun, 1991; Neal, 1995; Graves, 2011; Blundell et al., 2015; Hernandez-Lobato & Adams, 2015; Hernández-Lobato et al., 2016), some more practical and some less practical.",1. Introduction,[0],[0]
"A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) (Rumelhart et al., 1985; LeCun et al., 1989)).",1. Introduction,[0],[0]
"Much more importantly perhaps, it would be impractical to change existing model architectures that have been well studied, and it is often impractical to work with complex and cumbersome techniques which are difficult to explain to non-experts.",1. Introduction,[0],[0]
"Many existing approaches to obtain model confidence often do not scale to complex models or large amounts of data, and require us to develop new models for existing tasks for which we already have well performing tools (Gal, 2016).
",1. Introduction,[0],[0]
"One possible solution for practical inference in BNNs is variational inference (VI) (Jordan et al., 1999), a ubiquitous technique for approximate inference.",1. Introduction,[0],[0]
"Dropout variational distributions in particular (a mixture of two Gaussians with
small standard deviations, and with one component fixed at zero) can be used to obtain a practical inference technique (Gal & Ghahramani, 2016b).",1. Introduction,[0],[0]
"These have been used for machine vision and medical applications (Kendall & Cipolla, 2016; Kendall et al., 2015; Angermueller & Stegle, 2015; Yang et al., 2016).",1. Introduction,[0],[0]
"Dropout variational inference can be implemented by adding dropout layers (Hinton et al., 2012; Srivastava et al., 2014) before every weight layer in the NN model.",1. Introduction,[0],[0]
"Inference is then carried out by Monte Carlo (MC) integration over the variational distribution, in practice implemented by simulating stochastic forward passes through the model at test time (referred to as MC dropout).",1. Introduction,[0],[0]
"Although dropout VI is a practical technique for approximate inference, it also has some major limitations.",1. Introduction,[0],[0]
"Dropout VI can severely underestimate model uncertainty (Gal, 2016, Section 3.3.2) – a property many VI methods share (Turner & Sahani, 2011).",1. Introduction,[0],[0]
"This can lead to devastating results in applications that must rely on good uncertainty estimates such as AI safety applications.
",1. Introduction,[0],[0]
Alternative objectives to VI’s objective are therefore needed.,1. Introduction,[0],[0]
"Black-box α-divergence minimisation (Hernández-Lobato et al., 2016; Li & Turner, 2016; Minka, 2005) is a class of approximate inference methods extending on VI, approximating EP’s energy function (Minka, 2001) as well as the Hellinger distance (Hellinger, 1909).",1. Introduction,[0],[0]
These were proposed as a solution to some of the difficulties encountered with VI.,1. Introduction,[0],[0]
"However, the main difficulty with α-divergences is that the divergences are hard to use in practice.",1. Introduction,[0],[0]
"Existing inference techniques only use Gaussian approximating distributions, with the density over the approximation having to be evaluated explicitly many times.",1. Introduction,[0],[0]
"The objective offers a limited intuitive interpretation which is difficult to explain to non-experts, and of limited use for engineers (Gal, 2016, Section 2.2.2).",1. Introduction,[0],[0]
"Perhaps more importantly, current α-divergence inference techniques require existing models and code-bases to be changed radically to perform inference in the Bayesian counterpart to these models.",1. Introduction,[0],[0]
"To implement a complex CNN structure with the inference and code of (Hernández-Lobato et al., 2016), for example, one would be required to re-implement many already-implemented software tools.
",1. Introduction,[0],[0]
"In this paper we propose a re-parametrisation of the induced α-divergence objectives, and by relying on some mild assumptions (justified below), derive a simple approximate inference technique which can easily be implemented with existing models.",1. Introduction,[0],[0]
"Further, we rely on the dropout approximate variational distribution and demonstrate how inference can be done in a practical way – requiring us to only change the loss of the NN, L(θ), and to perform multiple stochastic forward passes at training time.",1. Introduction,[0],[0]
"Precisely, given l(·, ·) some standard NN loss such as cross entropy or the Euclidean loss, and {f ω̂k(xn)}Kk=1 a set of K stochastic dropout network outputs on input xn
with randomly masked weights ω̂k, our proposed objective is:
L(θ) =",1. Introduction,[0],[0]
"− 1 α ∑ n log-sum-exp [ −αl(yn, f ω̂k(xn)) ]",1. Introduction,[0],[0]
"+ L2(θ)
with α a real number, θ the set of network weights to be optimised, and an L2 regulariser over θ.",1. Introduction,[0],[0]
"By selecting α = 1 this objective directly optimises the per-point predictive log-likelihood, while picking α → 0 would focus on increasing the training accuracy, recovering VI.
",1. Introduction,[0],[0]
"Specific choices of αwill result in improved uncertainty estimates (and accuracy) compared to VI in dropout BNNs, without slowing convergence time.",1. Introduction,[0],[0]
"We demonstrate this through a myriad of applications, including an assessment of fully connected NNs in regression and classification, and an assessment of Bayesian CNNs.",1. Introduction,[0],[0]
"Finally, we study the uncertainty estimates resulting from our approximate inference technique.",1. Introduction,[0],[0]
"We show that our models’ uncertainty increases on adversarial images generated from the MNIST dataset, suggesting that these lie outside of the training data distribution.",1. Introduction,[0],[0]
This in practice allows us to tell-apart such adversarial images from non-adversarial images by examining epistemic model uncertainty.,1. Introduction,[0],[0]
We review background in Bayesian neural networks and approximate variational inference.,2. Background,[0],[0]
In the next section we discuss α-divergences.,2. Background,[0],[0]
"Given training inputs X = {x1, . . .",2.1. Bayesian Neural Networks,[0],[0]
",xN} and their corresponding outputs Y = {y1, . .",2.1. Bayesian Neural Networks,[0],[0]
.,2.1. Bayesian Neural Networks,[0],[0]
",yN}, in parametric Bayesian regression we would like to infer a distribution over parameters ω of a function y = fω(x) that could have generated the outputs.",2.1. Bayesian Neural Networks,[0],[0]
"Following the Bayesian approach, to find parameters that could have generated our data, we put some prior distribution over the space of parameters p0(ω).",2.1. Bayesian Neural Networks,[0],[0]
This distribution captures our prior belief as to which parameters are likely to have generated our outputs before observing any data.,2.1. Bayesian Neural Networks,[0],[0]
"We further need to define a probability distribution over the outputs given the inputs p(y|x, ω).",2.1. Bayesian Neural Networks,[0],[0]
"For classification tasks we assume a softmax likelihood,
p ( y|x, ω )",2.1. Bayesian Neural Networks,[0],[0]
"= Softmax (fω(x))
or a Gaussian likelihood for regression.",2.1. Bayesian Neural Networks,[0],[0]
"Given a dataset X,Y, we then look for the posterior distribution over the space of parameters: p(ω|X,Y).",2.1. Bayesian Neural Networks,[0],[0]
"This distribution captures how likely the function parameters are, given our observed data.",2.1. Bayesian Neural Networks,[0],[0]
"With it we can predict an output for a new input point x∗ by integrating
p(y∗|x∗,X,Y) = ∫ p(y∗|x∗, ω)p(ω|X,Y)dω.",2.1. Bayesian Neural Networks,[0],[0]
"(1)
One way to define a distribution over a parametric set of functions is to place a prior distribution over a neural network’s weights ω",2.1. Bayesian Neural Networks,[0],[0]
"= {Wi}Li=1, resulting in a Bayesian NN (MacKay, 1992; Neal, 1995).",2.1. Bayesian Neural Networks,[0],[0]
Given weight matrices Wi and bias vectors bi for layer,2.1. Bayesian Neural Networks,[0],[0]
"i, we often place standard matrix Gaussian prior distributions over the weight matrices, p0(Wi) = N",2.1. Bayesian Neural Networks,[0],[0]
"(Wi;0, I) and often assume a point estimate for the bias vectors for simplicity.",2.1. Bayesian Neural Networks,[0],[0]
"In approximate inference, we are interested in finding the distribution of weight matrices (parametrising our functions) that have generated our data.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"This is the posterior over the weights given our observables X,Y: p(ω|X,Y), which is not tractable in general.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
Existing approaches to approximate this posterior are through variational inference (as was done in Hinton & Van Camp (1993); Barber & Bishop (1998); Graves (2011); Blundell et al. (2015)).,2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"We need to define an approximating variational distribution qθ(ω) (parametrised by variational parameters θ), and then minimise w.r.t.",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"θ the KL divergence (Kullback & Leibler, 1951; Kullback, 1959) between the approximating distribution and the full posterior:
KL ( qθ(ω)||p(ω|X,Y) ) ∝",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"− ∫ qθ(ω) log p(Y|X, ω)dω + KL(qθ(ω)||p0(ω))",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
=,2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"− N∑ i=1 ∫ qθ(ω) log p(yi|fω(xi))dω + KL(qθ(ω)||p0(ω)),
where A ∝ B is slightly abused here to denote equality up to an additive constant (w.r.t. variational parameters θ).",2.2. Approximate Variational Inference in Bayesian Neural Networks,[0],[0]
"Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout (Hinton et al., 2012; Srivastava et al., 2014)) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure (Gal & Ghahramani, 2016b).",2.3. Dropout Approximate Inference,[0],[0]
This is because applying a stochastic regularisation technique is equivalent to multiplying the NN weight matrices Mi by some random noise,2.3. Dropout Approximate Inference,[0],[0]
i (with a new noise realisation for each data point).,2.3. Dropout Approximate Inference,[0],[0]
"The resulting stochastic weight matrices Wi = iMi can be seen as draws from the approximate posterior over the BNN weights, replacing the deterministic NN’s weight matrices Mi.",2.3. Dropout Approximate Inference,[0],[0]
Our set of variational parameters is then the set of matrices θ = {Mi}Li=1.,2.3. Dropout Approximate Inference,[0],[0]
"For example, dropout can be seen as an approximation to Bayesian NN inference with dropout approximating distributions, where the rows of the matrices Wi distribute according to a mixture of two Gaussians with small variances
and the mean of one of the Gaussians fixed at zero.",2.3. Dropout Approximate Inference,[0],[0]
"The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration:
p(y = c|x,X,Y) = ∫",2.3. Dropout Approximate Inference,[0],[0]
"p(y = c|x, ω)p(ω|X,Y)dω
≈ ∫ p(y = c|x, ω)qθ(ω)dω
≈ 1 K K∑ k=1 p(y = c|x, ω̂k)
with ω̂k ∼ qθ(ω), where qθ(ω) is the Dropout distribution (Gal, 2016).",2.3. Dropout Approximate Inference,[0],[0]
"Given its popularity, we concentrate on the dropout stochastic regularisation technique throughout the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise (Srivastava et al., 2014) or dropConnect (Wan et al., 2013)).
",2.3. Dropout Approximate Inference,[0],[0]
"Dropout VI is an example of practical approximate inference, but it also underestimates model uncertainty (Gal, 2016, Section 3.3.2).",2.3. Dropout Approximate Inference,[0],[0]
"This is because minimising the KL divergence between q(ω) and p(ω|X,Y) penalises q(ω) for placing probability mass where p(ω|X,Y) has no mass, but does not penalise q(ω) for not placing probability mass at locations where p(ω|X,Y) does have mass.",2.3. Dropout Approximate Inference,[0],[0]
We next discuss α-divergences as an alternative to the VI objective.,2.3. Dropout Approximate Inference,[0],[0]
"In this section we provide a brief review of the black box alpha (BB-α, Hernández-Lobato et al. (2016)) method upon which the main derivation in this paper is based.",3. Black-box α-divergence minimisation,[0],[0]
"Consider approximating the following distribution:
p(ω) = 1
Z p0(ω) ∏ n fn(ω).
",3. Black-box α-divergence minimisation,[0],[0]
"In Bayesian neural networks context, these factors fn(ω) represent the likelihood terms p(yn|xn, ω), Z = p(Y|X), and the approximation target p(ω) is the exact posterior p(ω|X,Y).",3. Black-box α-divergence minimisation,[0],[0]
"Popular methods of approximate inference include variational inference (VI) (Jordan et al., 1999) and expectation propagation (EP) (Minka, 2001), where these two algorithms are special cases of power EP (Minka, 2004) that minimises Amari’s α-divergence (Amari, 1985)",3. Black-box α-divergence minimisation,[0],[0]
"Dα[p||q] in a local way:
Dα[p||q] = 1
α(1− α)
( 1− ∫ p(ω)αq(ω)1−αdω ) .
",3. Black-box α-divergence minimisation,[0],[0]
"We provide details of α-divergences and local approximation methods in the appendix, and in the rest of the paper we consider three special cases in this rich family:
1.",3. Black-box α-divergence minimisation,[0],[0]
Exclusive KL divergence: D0[p||q] = KL[q||p] =,3. Black-box α-divergence minimisation,[0],[0]
"Eq [ log q(ω)
p(ω)
] ;
2.",3. Black-box α-divergence minimisation,[0],[0]
"Hellinger distance: D0.5[p||q] = 4Hel2[q||p] = 2 ∫ (√ p(ω)− √ q(ω) )2 dω;
3.",3. Black-box α-divergence minimisation,[0],[0]
Inclusive KL divergence: D1[p||q] = KL[p||q] =,3. Black-box α-divergence minimisation,[0],[0]
"Ep [ log p(ω)
q(ω)
] .
",3. Black-box α-divergence minimisation,[0],[0]
"Since α = 0 is used in VI and α = 1.0 is used in EP, in later sections we will also refer to these alpha settings as the VI value, Hellinger value, and EP value, respectively.
",3. Black-box α-divergence minimisation,[0],[0]
"Power-EP, though providing a generic variational framework, does not scale with big data.",3. Black-box α-divergence minimisation,[0],[0]
"It maintains approximating factors attached to every likelihood term fn(ω), resulting in space complexity O(N) for the posterior approximation which is clearly undesirable.",3. Black-box α-divergence minimisation,[0],[0]
"The recently proposed stochastic EP (Li et al., 2015) and BB-α (HernándezLobato et al., 2016) inference methods reduce this memory overhead to O(1) by sharing these approximating factors.",3. Black-box α-divergence minimisation,[0],[0]
"Moreover, optimisation in BB-α is done by descending the so called BB-α energy function, where Monte Carlo (MC) methods and automatic differentiation are also deployed to allow fast prototyping.
",3. Black-box α-divergence minimisation,[0],[0]
"BB-α has been successfully applied to Bayesian neural networks for regression, classification (Hernández-Lobato et al., 2016) and model-based reinforcement learning (Depeweg et al., 2016).",3. Black-box α-divergence minimisation,[0],[0]
They all found that using α 6= 0,3. Black-box α-divergence minimisation,[0],[0]
often returns better approximations than the VI case.,3. Black-box α-divergence minimisation,[0],[0]
The reasons for the worse results of VI are two fold.,3. Black-box α-divergence minimisation,[0],[0]
"From the perspective of inference, due to the zero-forcing behaviour of exclusive KL discussed before, VI often fits to a local mode of the exact posterior and is over-confident in prediction.",3. Black-box α-divergence minimisation,[0],[0]
"On hyper-parameter learning point of view, as the variational lower-bound is used as a (biased) approximation to the maximum likelihood objective, the learned model could be biased towards over-simplified cases (Turner & Sahani, 2011).",3. Black-box α-divergence minimisation,[0],[0]
These problems could potentially be addressed by using α-divergences.,3. Black-box α-divergence minimisation,[0],[0]
"For example, inclusive KL encourages the coverage of the support set (referred as mass-covering), and when used in local divergence minimisation (Minka, 2005), it can fit an approximation to a mode of p(ω) with better estimates of uncertainty.",3. Black-box α-divergence minimisation,[0],[0]
"Moreover the BB-α energy provides a better approximation to the marginal likelihood as well, meaning that the learned model will be less biased and thus fitting the data distribution better (Li & Turner, 2016).",3. Black-box α-divergence minimisation,[0],[0]
"Hellinger distance seems to provide a good balance between zero-forcing and masscovering, and empirically it has been found to achieve the best performance.
",3. Black-box α-divergence minimisation,[0],[0]
"Given the success of α-divergence methods, it is a natural idea to extend these algorithms to other classes of approximations such as dropout.",3. Black-box α-divergence minimisation,[0],[0]
However this task is non-trivial.,3. Black-box α-divergence minimisation,[0],[0]
"First, the original formulation of BB-α energy is an ad hoc adaptation of power-EP energy (see appendix), which applies to exponential family q distributions only.",3. Black-box α-divergence minimisation,[0],[0]
"Second, the energy function offers a limited intuitive interpretation to non-experts, thus of limited use for practitioners.",3. Black-box α-divergence minimisation,[0],[0]
"Third and most importantly, a naive implementation of BB-α using dropout would bring in a prohibitive computational burden.",3. Black-box α-divergence minimisation,[0],[0]
"To see this, we first review the BB-α energy function in the general case (Li & Turner, 2016) given α 6= 0:
Lα(q) =",3. Black-box α-divergence minimisation,[0],[0]
"− 1
α ∑ n logEq
[( fn(ω)p0(ω) 1 N
q(ω) 1 N
)α] .",3. Black-box α-divergence minimisation,[0],[0]
"(2)
One could verify that this is the same energy function as presented in (Hernández-Lobato et al., 2016) by considering q an exponential family distribution.",3. Black-box α-divergence minimisation,[0],[0]
"In practice (2) might be intractable, hence an MC approximation is introduced:
LMCα (q) =",3. Black-box α-divergence minimisation,[0],[0]
"− 1
α ∑ n log 1 K ∑ k
[( fn(ω̂k)p0(ω̂k) 1 N
q(ω̂k) 1 N
)α] (3)
with ω̂k ∼ q(ω).",3. Black-box α-divergence minimisation,[0],[0]
This is a biased approximation as the expectation in (2) is computed before taking the logarithm.,3. Black-box α-divergence minimisation,[0],[0]
"But empirically Hernández-Lobato et al. (2016) showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible.",3. Black-box α-divergence minimisation,[0],[0]
When α → 0,3. Black-box α-divergence minimisation,[0],[0]
"it returns the variational free energy (the VI objective)
L0(q) = LVFE(q) = KL[q||p0]− ∑ n",3. Black-box α-divergence minimisation,[0],[0]
"Eq [log fn(ω)] , (4)
and the corresponding MC approximation LMCVFE becomes an unbiased estimator of LVFE.",3. Black-box α-divergence minimisation,[0],[0]
"Also LMCα → LMCVFE as the number of samples K → 1.
",3. Black-box α-divergence minimisation,[0],[0]
"The original paper (Hernández-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (3) with samples ω̂k ∼ q(ω).",3. Black-box α-divergence minimisation,[0],[0]
"However as discussed before, dropout implicitly samples different masked weight matrices ω̂ ∼ q for different data points.",3. Black-box α-divergence minimisation,[0],[0]
"This indicates that the naive approach, when applied to dropout approximation, would gather all these samples for all M datapoints in a mini-batch (i.e. MK sets of neural network weight matrices in total), which brings prohibitive cost if the network is wide and deep.",3. Black-box α-divergence minimisation,[0],[0]
"Interestingly, the minimisation of the variational free energy (α = 0) with the dropout approximation can be computed very efficiently.",3. Black-box α-divergence minimisation,[0],[0]
The main reason for this success is due to the additive structure of the variational free energy: no evaluation of q density is required if the “regulariser” KL[q||p0] can be computed/approximated efficiently.,3. Black-box α-divergence minimisation,[0],[0]
"In the following section we
propose an improved version of BB-α energy to allow applications with dropout and other flexible approximation structures.",3. Black-box α-divergence minimisation,[0],[0]
We propose a reparamterisation of the BB-α energy to reduce the computational overhead.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"First we denote q̃(ω) as a free-form “cavity distribution” (see appendix), and write the approximate posterior q as
q(ω) = 1
Zq q̃(ω)
( q̃(ω)
p0(ω)
) α N−α
, (5)
where we assume Zq < +∞ is the normalising constant to ensure q a valid distribution.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"When α/N → 0, the unnormalised density in (5) converges to q̃(ω) for every ω, and Zq → 1 by the assumption of Zq < +∞ (Van Erven & Harremoës, 2014).",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Hence q → q̃ when α/N → 0, and this happens for example when we choose α → 0, or N →",4. A New Reparameterisation of BB-α Energy,[0],[0]
+∞ as well as when α grows sub-linearly to N .,4. A New Reparameterisation of BB-α Energy,[0],[0]
"Now we rewrite the BB-alpha energy in terms of q̃:
Lα(q) =",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− 1
α ∑ n log
∫ ( 1
Zq q̃(ω)
( q̃(ω)
p0(ω)
) α N−α )1− αN",4. A New Reparameterisation of BB-α Energy,[0],[0]
"p0(ω) α N fn(ω) αdω
=",4. A New Reparameterisation of BB-α Energy,[0],[0]
− 1 α ∑ n (∫ q̃(ω)fn(ω),4. A New Reparameterisation of BB-α Energy,[0],[0]
α − ( 1− α N ),4. A New Reparameterisation of BB-α Energy,[0],[0]
"logZq )
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"= N
α
( 1− α
N
) log ∫ q̃(ω) ( q̃(ω)
p0(ω)
)",4. A New Reparameterisation of BB-α Energy,[0],[0]
"α N−α
dω
− 1 α ∑ n logEq̃",4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α]
= Rβ [q̃||p0]− 1
α ∑ n",4. A New Reparameterisation of BB-α Energy,[0],[0]
logEq̃,4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α] , β = N N",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− α ,
where Rβ [q̃||p0] represents the Rényi divergence (Rényi (1961), see appendix) of order β.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Furthermore, provided Rβ [q̃||p0] <",4. A New Reparameterisation of BB-α Energy,[0],[0]
"+∞ (which holds when assuming Zq < +∞), we have Rβ [q̃||p0] → KL[q̃||p0] = KL[q||p0] as α N → 0.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"This means that for a constant α that scales sublinearly with N , in large data settings we can further approximate the BB-α energy as
Lα(q)",4. A New Reparameterisation of BB-α Energy,[0],[0]
≈ L̃α(q),4. A New Reparameterisation of BB-α Energy,[0],[0]
= KL[q||p0]−,4. A New Reparameterisation of BB-α Energy,[0],[0]
"1
α ∑ n logEq",4. A New Reparameterisation of BB-α Energy,[0],[0]
"[fn(ω)α] .
Note that here we also use the fact",4. A New Reparameterisation of BB-α Energy,[0],[0]
that now q ≈ q̃.,4. A New Reparameterisation of BB-α Energy,[0],[0]
Critically,4. A New Reparameterisation of BB-α Energy,[0],[0]
", the proposed reparameterisation is continuous in α, and by taking α → 0 the variational free-energy (4) is recovered.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Given a loss function l(·, ·), e.g. l2 loss in regression or cross entropy in classification, we can define the (unnormalised) likelihood term fn(ω) ∝",4. A New Reparameterisation of BB-α Energy,[0],[0]
"p(yn|xn, ω) ∝
exp[−l(yn, fω(xn))",4. A New Reparameterisation of BB-α Energy,[0],[0]
"], e.g. see (LeCun et al., 2006)1.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Swapping fn(ω) for this last expression, and approximating the expectation over q using Monte Carlo sampling, we obtain our proposed minimisation objective:
L̃MCα",4. A New Reparameterisation of BB-α Energy,[0],[0]
"(q) = KL[q||p0] + const (6)
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"− 1 α ∑ n log-sum-exp[−αl(yn, f ω̂k(xn))",4. A New Reparameterisation of BB-α Energy,[0],[0]
"]
with log-sum-exp being the log-sum-exp operator over K samples from the approximate posterior ω̂k ∼ q(ω).",4. A New Reparameterisation of BB-α Energy,[0],[0]
This objective function also approximates the marginal likelihood.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"Therefore, compared to the original formulation (2), the improved version (6) is considerably simpler (both to implement and to understand), has a similar form to standard objective functions used in deep learning research, yet remains an approximate Bayesian inference algorithm.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"To gain some intuitive understanding of this objective, we observe what it reduces to for different α and K settings.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"By selecting α = 1 the per-point predictive log-likelihood logEq[p(yn|xn, ω)] is directly optimised.",4. A New Reparameterisation of BB-α Energy,[0],[0]
"On the other hand, picking the VI value (α → 0) would focus on increasing the training accuracy Eq[log p(yn|xn, ω)].",4. A New Reparameterisation of BB-α Energy,[0],[0]
"The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable (HernándezLobato et al., 2016; Depeweg et al., 2016).",4. A New Reparameterisation of BB-α Energy,[0],[0]
"Lastly, for K = 1 the log-sum-exp disappears, the α’s cancel out, and the original (stochastic) VI objective is recovered.
",4. A New Reparameterisation of BB-α Energy,[0],[0]
"In summary, our proposal modifies the loss function by multiplying it by α and then performing log-sum-exp with a sum over multiple stochastic forward passes sampled from the BNN approximate posterior.",4. A New Reparameterisation of BB-α Energy,[0],[0]
The remaining KLdivergence term (between q and the prior p) can often be approximated.,4. A New Reparameterisation of BB-α Energy,[0],[0]
"It can be viewed as a regulariser added to the objective function, and reduces to L2-norm regulariser for certain popular q choices (Gal, 2016).",4. A New Reparameterisation of BB-α Energy,[0],[0]
We now provide a concrete example where the approximate distribution is defined by dropout.,4.1. Dropout BB-α,[0],[0]
"With dropout VI, MC samples are used to approximate the expectation w.r.t.",4.1. Dropout BB-α,[0],[0]
"q, which in practice is implemented as performing stochastic forward passes through the dropout network – i.e. given an input x, the input is fed through the network and a new dropout mask is sampled and applied at each dropout layer.",4.1. Dropout BB-α,[0],[0]
This gives a stochastic output – a sample from the dropout network on the input x.,4.1. Dropout BB-α,[0],[0]
"A similar approximation is used in our case as well, where to implement the MC sampling in eq.",4.1. Dropout BB-α,[0],[0]
"(6) we perform multiple stochastic forward passes
1fn(ω) does not need to be a normalised density of yn unless one would like to optimise the associated hyper parameters.
through the network.
",4.1. Dropout BB-α,[0],[0]
Recall the neural network fω(x) is parameterised by the variable ω.,4.1. Dropout BB-α,[0],[0]
"In classification, cross entropy is often used as the loss function l(y, fω(x))",4.1. Dropout BB-α,[0],[0]
= −yT,4.1. Dropout BB-α,[0],[0]
"logpω(x), where the label yn is a one-hot binary vector, and the network output pω(xn) = Softmax(fω(xn))",4.1. Dropout BB-α,[0],[0]
encodes the probability vector of class assignments.,4.1. Dropout BB-α,[0],[0]
"Applying the re-formulated BB-α energy (6) with a Bayesian equivalent of the network, we arrive at the objective function L̃MCα (q) =",4.1. Dropout BB-α,[0],[0]
∑ i pi||Mi||22,4.1. Dropout BB-α,[0],[0]
− 1 α ∑ n yTn log 1 K ∑ k (pω̂k(xn)),4.1. Dropout BB-α,[0],[0]
"α
= 1
α ∑ n",4.1. Dropout BB-α,[0],[0]
"l
( yn, 1
K ∑ k pω̂k(xn) α )",4.1. Dropout BB-α,[0],[0]
"+ ∑ i L2(Mi)
with {pω̂k(xn)}Kk=1 being K stochastic network outputs on input xn, pi equals to one minus the dropout rate of the ith layer, and the L2 regularization terms coming from an approximation to the KL-divergence (Gal, 2016).",4.1. Dropout BB-α,[0],[0]
I.e. we raise network probability outputs to the power α and average them as an input to the standard cross entropy loss.,4.1. Dropout BB-α,[0],[0]
"Taking α 6= 1 can be viewed as training the neural network with an adjusted “power” loss, regularized by an L2 norm.",4.1. Dropout BB-α,[0],[0]
"Implementing this induced loss with Keras (Chollet, 2015) is as simple as a few lines of Python.",4.1. Dropout BB-α,[0],[0]
"A code snippet is given in Figure 1, with more details in the appendix.
",4.1. Dropout BB-α,[0],[0]
"In regression problems, the loss function is defined as l(y, fω(x))",4.1. Dropout BB-α,[0],[0]
"= τ2 ||y− f
ω(x)||22 and the likelihood term can be interpreted as y ∼ N (y; fω(x), τ−1I).",4.1. Dropout BB-α,[0],[0]
"Plugging this into the energy function returns the following objective
L̃MCα (q) =",4.1. Dropout BB-α,[0],[0]
"− 1
α ∑ n log-sum-exp [ −ατ 2 ||yn − f ω̂k(xn)||22 ]",4.1. Dropout BB-α,[0],[0]
"+ ND
2 log τ + ∑ i pi||Mi||22, (7)
with {f ω̂k(xn)}Kk=1 being K stochastic forward passes on input xn.",4.1. Dropout BB-α,[0],[0]
"Again, this is reminiscent of the l2 objective in standard deep learning, and can be implemented by simply passing the input through the dropout network multiple times, collecting the stochastic outputs, and feeding the set of outputs through our new BB-alpha loss function.",4.1. Dropout BB-α,[0],[0]
We test the reparameterised BB-α on Bayesian NNs with the dropout approximation.,5. Experiments,[0],[0]
"We assess the proposed inference in regression and classification tasks on standard benchmarking datasets, comparing different values of α.",5. Experiments,[0],[0]
This last experiment leads us to propose a technique that could be used to identify adversarial image attacks.,5. Experiments,[0],[0]
In the appendix we further provide a study of run time trade-off.,5. Experiments,[0],[0]
The first experiment considers Bayesian neural network regression with approximate posterior induced by dropout.,5.1. Regression,[0],[0]
We use benchmark UCI datasets2 that have been tested in related literature.,5.1. Regression,[0],[0]
"The model is a single-layer neural network with 50 ReLU units for all datasets except for Protein and Year, which use 100 units.",5.1. Regression,[0],[0]
"We consider α ∈ {0.0, 0.5, 1.0} in order to examine the effect of masscovering/zero-forcing behaviour in dropout.",5.1. Regression,[0],[0]
MC approximation with K = 10 samples is also deployed to compute the energy function.,5.1. Regression,[0],[0]
"Other initialisation settings are largely taken from (Li & Turner, 2016).
",5.1. Regression,[0],[0]
"We summarise the test negative log-likelihood (LL) and RMSE with standard error (across different random splits, the lower the better) for selected datasets in Figure 2 and 3, respectively.",5.1. Regression,[0],[0]
The full results are provided in the appendix.,5.1. Regression,[0],[0]
"Although optimal α may vary for different datasets, using non-VI values has significantly improved the test-LL performances, while remaining comparable in test error metric.",5.1. Regression,[0],[0]
"In particular, α = 0.5 produced overall good results for both test LL and RMSE, which is consistent with previous findings.",5.1. Regression,[0],[0]
"We also compare with a BNN with a Gaussian approximation (VI-G) (Li & Turner, 2016), a BNN with HMC, and a sparse Gaussian process model with 50 inducing points (Bui et al., 2016).",5.1. Regression,[0],[0]
"In test-LL metric our best dropout model out-performs the Gaussian approximation method on almost all datasets, and for some datasets is on par with HMC which is the current gold standard for Bayesian neural works, and with the GP model that is known to be superior in regression.",5.1. Regression,[0],[0]
"We further experiment with a classification task, comparing the accuracy of the various α values on the MNIST benchmark (LeCun & Cortes, 1998).",5.2. Classification,[0],[0]
We assessed a fully connect NN with 2 hidden layers and 100 units in each layer.,5.2. Classification,[0],[0]
"We used dropout probability 0.5 and α ∈ {0, 0.5, 1}.",5.2. Classification,[0],[0]
"Again, we use K = 10 samples at training time for all α values,
2http://archive.ics.uci.edu/ml/datasets.",5.2. Classification,[0],[0]
"html
and Ktest = 100 samples at test time.",5.2. Classification,[0],[0]
"We use weight decay 10−6, which is equivalent to prior lengthscale l2 = 0.1",5.2. Classification,[0],[0]
"(Gal & Ghahramani, 2016b).",5.2. Classification,[0],[0]
We repeat each experiment three times and plot mean and standard error.,5.2. Classification,[0],[0]
Test RMSE as well as test log likelihood are given in Figure 4.,5.2. Classification,[0],[0]
"As can be seen, Hellinger value α = 0.5 gives best test RMSE, with test log likelihood matching that of the EP value α = 1.",5.2. Classification,[0],[0]
"The VI value α = 0 under-performs according to both metrics.
",5.2. Classification,[0],[0]
We next assess a convolutional neural network model (CNN).,5.2. Classification,[0],[0]
"For this experiment we use the standard CNN example given in (Chollet, 2015) with 32 convolution filters, 100 hidden units at the top layer, and dropout probability 0.5 before each fully-connected layer.",5.2. Classification,[0],[0]
Other settings are as before.,5.2. Classification,[0],[0]
Average test accuracy and test log likelihood are given in Figure 5.,5.2. Classification,[0],[0]
"In this case, VI value α = 0 seems to supersede the EP value α = 1, and performs similarly to the Hellinger value α = 0.5 according to both metrics.",5.2. Classification,[0],[0]
The third set of experiments considers adversarial attacks on dropout-trained Bayesian neural networks.,5.3. Detecting Adversarial Examples,[0],[0]
"We test the hypothesis that certain techniques for generating adversarial examples will give images that lie outside of the image
manifold, i.e. far from the data distribution (note though that there exist techniques that will guarantee the images staying near the data manifold, by minimising the perturbation used to construct the adversarial example).",5.3. Detecting Adversarial Examples,[0],[0]
"By assessing the BNN uncertainty, we should see increased uncertainty for adversarial images if they indeed lie outside of the training data distribution.",5.3. Detecting Adversarial Examples,[0],[0]
The tested models are fully connected networks with 3 hidden layers of 1000 units trained using dropout rate 0.5 and different alpha values.,5.3. Detecting Adversarial Examples,[0],[0]
These models are also compared to a benchmark MLP with the same architecture but trained by maximum likelihood.,5.3. Detecting Adversarial Examples,[0],[0]
"The adversarial examples are generated on MNIST test data that is normalised to be in the range [0, 1].",5.3. Detecting Adversarial Examples,[0],[0]
"For the dropout trained networks we perform MC dropout at test time with Ktest = 10 MC samples.
",5.3. Detecting Adversarial Examples,[0],[0]
"The first attack in consideration is the Fast Gradient Sign (FGS) method (Goodfellow et al., 2014).",5.3. Detecting Adversarial Examples,[0],[0]
"This is an untargeted attack, which attempts to reduces the maximum value of the predicted class label probability
xadv = x− η · sgn(∇x max y log p(y|x)).
",5.3. Detecting Adversarial Examples,[0],[0]
"We use the single gradient step FGS implemented in Cleverhans (Papernot et al., 2016) with the stepsize η varied
between 0.0 and 0.5.",5.3. Detecting Adversarial Examples,[0],[0]
"The left panel in Figure 6 demonstrates the classification accuracy on adversarial examples, which shows that the dropout networks, especially the one trained with α = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN.",5.3. Detecting Adversarial Examples,[0],[0]
"For example, for η = 0.1 the adversarial samples still visually close to the original class, and the BNN trained with α = 0.0 achieves an accuracy level almost 3 times higher than the MLP and around 20% higher than the VI-trained version.",5.3. Detecting Adversarial Examples,[0],[0]
"More interestingly, the test data examples and adversarial images can be told-apart by investigating the uncertainty representation of the dropout models.",5.3. Detecting Adversarial Examples,[0],[0]
"In the right panel of Figure 6 we depict the predictive entropy computed on the neural network output probability vector, and show example corresponding adversarial images below the axis for each corresponding stepsize.",5.3. Detecting Adversarial Examples,[0],[0]
"Clearly the deterministic NN model produces over-confident predictions on adversarial samples, e.g. it predicts the wrong label very confidently even when the input is still visually close to digit “7” (η = 0.2).",5.3. Detecting Adversarial Examples,[0],[0]
"While dropout models, though producing wrong labels, are very uncertain about their predictions.",5.3. Detecting Adversarial Examples,[0],[0]
This uncertainty keeps increasing as we move away from the data manifold.,5.3. Detecting Adversarial Examples,[0],[0]
"Hence the dropout networks are much more immunised from noise-corrupted inputs, as they can be detected using uncertainty estimates in this example.
",5.3. Detecting Adversarial Examples,[0],[0]
"The second attack we consider is a targeted version of FGS (Goodfellow et al., 2014; Carlini & Wagner, 2016), which maximises the predictive probability of a selected class instead.",5.3. Detecting Adversarial Examples,[0],[0]
"As an example, we fix class 0 as the target and apply the iterative gradient-base attack to all non-zero digits in test data.",5.3. Detecting Adversarial Examples,[0],[0]
"At step t, the adversarial output is computed as
xtadv = x t−1 adv + η · sgn(∇x log p(ytarget|x t−1 adv )),
where the stepsize η is fixed at 0.01 in this case.",5.3. Detecting Adversarial Examples,[0],[0]
"Results are presented in the left panel of Figure 7, and again dropout
trained models are more robust to this attack compared with the MLP.",5.3. Detecting Adversarial Examples,[0],[0]
"Similarly these adversarial examples could be detected by the Bayesian neural networks’ uncertainty, by examining the predictive entropy.",5.3. Detecting Adversarial Examples,[0],[0]
"By visually inspecting the generated adversarial examples in the right panel of Figure 7, it is clear that the MLP overconfidently classifies a digit 7 to class",5.3. Detecting Adversarial Examples,[0],[0]
0.,5.3. Detecting Adversarial Examples,[0],[0]
"On the other hand, the dropout models are still fairly uncertain about their predictions even after 40 gradient steps.",5.3. Detecting Adversarial Examples,[0],[0]
"More interestingly, running this iterative attack on dropout models produces a smooth interpolation between different digits, and when the model is confident on predicting the target class, the corresponding adversarial images are visually close to digit zero.
",5.3. Detecting Adversarial Examples,[0],[0]
These initial results suggest that assessing the epistemic uncertainty of classification models can be used as a viable technique to identify adversarial examples.,5.3. Detecting Adversarial Examples,[0],[0]
"We would note though that we used this experiment to demonstrate our techniques’ uncertainty estimates, and much more research is needed to solve the difficulties faced with adversarial inputs.",5.3. Detecting Adversarial Examples,[0],[0]
We presented a practical extension of the BB-alpha objective which allows us to use the technique with dropout approximating distributions.,6. Conclusions,[0],[0]
"The technique often supersedes existing approximate inference techniques (even sparse Gaussian processes), and is easy to implement.",6. Conclusions,[0],[0]
A code snippet for our induced loss is given in the appendix.,6. Conclusions,[0],[0]
"We thank Rich Turner, Nicolas Papernot, and the reviewers for comments.",Acknowledgements,[0],[0]
YL thanks the Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed.",abstractText,[0],[0]
"Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty.",abstractText,[0],[0]
"Alpha-divergences are alternative divergences to VI’s KL objective, which are able to avoid VI’s uncertainty underestimation.",abstractText,[0],[0]
"But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners.",abstractText,[0],[0]
"We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model.",abstractText,[0],[0]
We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks.,abstractText,[0],[0]
"We study our model’s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model’s uncertainty.",abstractText,[0],[0]
Dropout Inference in Bayesian Neural Networks with Alpha-divergences,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 168–178, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics",text,[0],[0]
"Topic models aid exploration of the main thematic elements of large text corpora by revealing latent structure and producing a high level semantic view (Blei et al., 2003).",1 Introduction,[0],[0]
"Topic models have been used for understanding the contents of a corpus and identifying interesting aspects of a collection for more indepth analysis (Talley et al., 2011; Mimno, 2011).",1 Introduction,[0],[0]
"While standard topic models assume a flat semantic structure, there are potentially many dimensions of a corpus that contribute to word choice,
such as sentiment, perspective and ideology (Mei et al., 2007; Paul and Girju, 2010; Eisenstein et al., 2011).",1 Introduction,[0],[0]
"Rather than studying these factors in isolation, multi-dimensional topic models can consider multiple factors jointly.
",1 Introduction,[0],[0]
"Paul and Dredze (2012b) introduced factorial LDA (f-LDA), a general framework for multidimensional text models that capture an arbitrary number of factors (explained in §3).",1 Introduction,[0],[0]
"While a standard topic model learns distributions over “topics” in documents, f-LDA learns distributions over combinations of multiple factors (e.g. topic, perspective) called tuples (e.g. (HEALTHCARE,LIBERAL)).",1 Introduction,[0],[0]
"While f-LDA can model factors without supervision, it has not been used in situations where the user has prior information about the factors.
",1 Introduction,[0],[0]
"In this paper we consider a setting where the user has prior knowledge about the end application: mining recreational drug trends from user forums, an important clinical research problem (§2).",1 Introduction,[0],[0]
"We show how to incorporate available information from these forums into f-LDA as a novel hierarchical prior over the model parameters, guiding the model toward the desired output (§3.1).
",1 Introduction,[0],[0]
"We then demonstrate the model’s utility in exploring a corpus in a targeted manner by using it to automatically extract interesting sentences from the text, a simple form of extractive multi-document summarization (Goldstein et al., 2000).",1 Introduction,[0],[0]
"In the same way that topic models can be used for aspectspecific summarization (Titov and McDonald, 2008; Haghighi and Vanderwende, 2009), we use f-LDA to extract snippets corresponding to fine-grained information patterns.",1 Introduction,[0],[0]
"Our results demonstrate that our multi-dimensional modeling approach targets more informative text than a simpler model (§4).
168",1 Introduction,[0],[0]
Recreational drug use imposes a significant burden on the health infrastructure of the United States and other countries.,2 Analyzing Drug Trends on the Web,[0],[0]
"Accurate information on drugs, usage profiles and side effects are necessary for supporting a range of healthcare activities, such as addiction treatment programs, toxin diagnosis, prevention and awareness campaigns, and public policy.",2 Analyzing Drug Trends on the Web,[0],[0]
"These activities rely on up-to-date information on drug trends, but it is increasingly difficult to keep up with current drug information, as distribution and information-sharing of novel drugs is easier than ever via the web (Wax, 2002).",2 Analyzing Drug Trends on the Web,[0],[0]
"For the third consecutive year, a record number of new drugs (49) were detected in Europe in 2011 (EMCDDA, 2012).",2 Analyzing Drug Trends on the Web,[0],[0]
"About two-thirds of these new drugs were synthetic cannabinoids (used as legal marijuana substitutes), which led to 11,000 hospitalizations in the U.S. in 2010 (SAMHSA, 2012).",2 Analyzing Drug Trends on the Web,[0],[0]
"Treatment is complicated by the fact that novel substances like these may have unknown side effects and other properties.
",2 Analyzing Drug Trends on the Web,[0],[0]
"Accurate information on drug trends can be obtained by speaking directly with users, e.g. focus groups and interviews (Reyes et al., 2012; Hout and Bingham, 2012), but such studies are slow and costly, and can fail to identify the emergence of new drug classes, such as mephedrone (Dunn et al., 2011).",2 Analyzing Drug Trends on the Web,[0],[0]
"More recently, researchers have begun to recognize clinical value in information obtained from the web (Corazza et al., 2011).",2 Analyzing Drug Trends on the Web,[0],[0]
"By (manually) analyzing YouTube videos, Drugs-Forum (discussed below), and other social media websites and online communities, researchers have uncovered details about the use, effects, and popularity of a variety of new and emerging drugs (Morgan et al., 2010; Corazza et al., 2012; Gallagher et al., 2012), and comprehensive drug reviews now include nonstandard sources such as web forums in addition to standard sources (Hill and Thomas, 2011).
",2 Analyzing Drug Trends on the Web,[0],[0]
Organizing and understanding forums requires significant effort.,2 Analyzing Drug Trends on the Web,[0],[0]
We propose automated tools to aid in the exploration and analysis of these data.,2 Analyzing Drug Trends on the Web,[0],[0]
"While topic models are a natural fit for corpus exploration (Eisenstein et al., 2012; Chaney and Blei, 2012), and have been used for similar public health applications (Paul and Dredze, 2011), online forums can be organized in many ways beyond topic.",2 Analyzing Drug Trends on the Web,[0],[0]
"Guided by do-
main experts, we seek to model forums as a combination of drug type, route of intake (oral, injection, etc.)",2 Analyzing Drug Trends on the Web,[0],[0]
"and aspect (cultural settings, drug chemistry, etc.)",2 Analyzing Drug Trends on the Web,[0],[0]
"A multi-dimensional topic model can jointly capture these factors, providing a more informative understanding of the data, and can be used to produce fine-grained information such as the effects of taking a particular drug orally.",2 Analyzing Drug Trends on the Web,[0],[0]
Our hope is that models such as f-LDA can lead to exploratory tools that aide researchers in learning about new drugs.,2 Analyzing Drug Trends on the Web,[0],[0]
"Our data set is taken from drugs-forum.com, a site active for more than 10 years with over 100,000 members and more than 1 million monthly readers.",2.1 Corpus: Drugs-Forum,[0],[0]
"The site is an information hub where people can freely discuss recreational drugs with psychoactive effects, ranging from coffee to heroin, hosting information and discussions on specific drugs, as well as drug-related politics, law, news, recovery and addiction.",2.1 Corpus: Drugs-Forum,[0],[0]
"With current information on a variety of drugs and an extensive archive, Drugs-Forum provides an ideal information source for public health researchers (Corazza et al., 2012).
",2.1 Corpus: Drugs-Forum,[0],[0]
"Discussion threads are organized into numerous forums, including drugs, the law, addiction, etc.",2.1 Corpus: Drugs-Forum,[0],[0]
"Since we are modeling drug use, we focus on the drug forums.",2.1 Corpus: Drugs-Forum,[0],[0]
"Each thread is assigned to a specific forum or subforum (drug) and each thread has a user specified tag, which can indicate categories like “Effects” as well as routes of administration like “Oral.”",2.1 Corpus: Drugs-Forum,[0],[0]
"We organized the tags and subforum categorizations into factors and components, as shown in Table 1.",2.1 Corpus: Drugs-Forum,[0],[0]
We make use of these tags in §3.1.,2.1 Corpus: Drugs-Forum,[0],[0]
"Clinical researchers are interested in specific information about drug usage, including drug type, route of administration, and other aspects of drug use (e.g. dosage, side effects).",3 Multi-Dimensional Text Models,[0],[0]
"Rather than considering these factors independently, we would like to model these in a way that can capture interesting interactions between all three factors, because the effects and other aspects of drugs can vary by route of administration.",3 Multi-Dimensional Text Models,[0],[0]
"Oral consumption of drugs often produces longer lasting but milder effects than injection or smoking, for example.",3 Multi-Dimensional Text Models,[0],[0]
"Many mephedrone users report nose bleeds and nasal pain as a health effect of snorting the drug: this could be modeled as the triple (MEPHEDRONE,SNORTING,HEALTH), a particular combination of all three factors.
",3 Multi-Dimensional Text Models,[0],[0]
"To this end, we utilize the multi-dimensional text model factorial LDA (f-LDA) (Paul and Dredze, 2012b), which jointly models multiple semantic factors or dimensions.",3 Multi-Dimensional Text Models,[0],[0]
"In this section we summarize fLDA, then we describe an extension which incorporates user-generated metadata into the model (§3.1).
",3 Multi-Dimensional Text Models,[0],[0]
"In a standard topic model such as LDA (Blei et al., 2003), each word token is associated with a latent “topic” variable.",3 Multi-Dimensional Text Models,[0],[0]
"f-LDA is conceptually similar to LDA except that rather than a single topic variable, each token is associated with a K-dimensional vector of latent variables.",3 Multi-Dimensional Text Models,[0],[0]
"In a three-dimensional fLDA model, each token has three latent variables— drug, route, and aspect in this case.
",3 Multi-Dimensional Text Models,[0],[0]
"In f-LDA, each document has a distribution over all possible K-tuples (rather than topics), and each K-tuple is associated with its own word distribution.",3 Multi-Dimensional Text Models,[0],[0]
"Under this model, words are generated by first sampling a tuple from the document’s tuple distribution, then sampling a word from that tuple’s word distribution.",3 Multi-Dimensional Text Models,[0],[0]
"In our threedimensional model, we will consider triples such as (CANNABIS,SMOKING,EFFECTS).
",3 Multi-Dimensional Text Models,[0],[0]
"Formally, each document has a distribution θ(d) over triples, and each token is associated with a latent vector ~z of sizeK=3.",3 Multi-Dimensional Text Models,[0],[0]
"(We’ll describe the model in terms of the three factors we are modeling in this paper, but f-LDA generalizes toK dimensions.)",3 Multi-Dimensional Text Models,[0],[0]
"The Cartesian product of the three factors forms a set of triples and the vector ~z references three discrete components to form a triple ~t = (t1, t2, t3).",3 Multi-Dimensional Text Models,[0],[0]
"The car-
dinality of each dimension (denoted Zk) is the number of drugs, routes, and aspects, as shown in Table 1.",3 Multi-Dimensional Text Models,[0],[0]
"Each triple has a corresponding word distribution φ~t. The graphical model is shown in Figure 1.
",3 Multi-Dimensional Text Models,[0],[0]
"One would expect that triples that have components in common should have similar word distributions: (CANNABIS,SMOKING,EFFECTS) is expected to have some commonalities with (CANNABIS,ORAL,EFFECTS).",3 Multi-Dimensional Text Models,[0],[0]
"f-LDA models this intuition by sharing parameters across priors for triples which share components: all triples with CANNABIS as the drug include cannabis-specific parameters in the prior, and all triples with SMOKING as the route have smoking-specific parameters.",3 Multi-Dimensional Text Models,[0],[0]
"Formally, φ~t (the word distribution for tuple ~t) has a Dirichlet(ω̂(~t))",3 Multi-Dimensional Text Models,[0],[0]
"prior, where for each word w in the vector, ω̂( ~t) w is a log-linear function:
ω̂( ~t )",3 Multi-Dimensional Text Models,[0],[0]
"w , exp ( ω(B)+ω(0)w +ω",3 Multi-Dimensional Text Models,[0],[0]
(drug),3 Multi-Dimensional Text Models,[0],[0]
t1w,3 Multi-Dimensional Text Models,[0],[0]
+ω (route) t2w,3 Multi-Dimensional Text Models,[0],[0]
+ω (aspect) t3w ),3 Multi-Dimensional Text Models,[0],[0]
"(1) where ω(B) is a corpus-wide precision scalar (the bias), ω(0)w is a corpus-specific bias for word w, and ω
(k) tkw
is a bias parameter for word w for component tk of the kth factor.",3 Multi-Dimensional Text Models,[0],[0]
"That is, each drug, route, and aspect has a weight vector over the vocabulary, and the prior for a particular triple is influenced by the weight vectors of each of the three factors.",3 Multi-Dimensional Text Models,[0],[0]
The ω parameters are all independent and normally distributed around 0,3 Multi-Dimensional Text Models,[0],[0]
"(effectively L2 regularization).
",3 Multi-Dimensional Text Models,[0],[0]
"The prior over each document’s distribution over triples has a similar log-linear prior, where weights for each factor are combined to influence the distribution.",3 Multi-Dimensional Text Models,[0],[0]
"Under our model, θ(d) is drawn from Dirichlet(B · α̂(d)), where · denotes an element-wise product between B (described below) and α̂(d), with
α̂ (d) ~t for each triple ~t defined as:
α̂ (d) ~t
, exp ( α(B)",3 Multi-Dimensional Text Models,[0],[0]
"+α
(D,drug) t1",3 Multi-Dimensional Text Models,[0],[0]
"+α (d,drug)",3 Multi-Dimensional Text Models,[0],[0]
"t1
+α (D,route)",3 Multi-Dimensional Text Models,[0],[0]
"t2 +α (d,route)",3 Multi-Dimensional Text Models,[0],[0]
"t2 +α (D,aspect) t3",3 Multi-Dimensional Text Models,[0],[0]
"+α (d,aspect) t3 )",3 Multi-Dimensional Text Models,[0],[0]
"(2)
Similar to the ω formulation, α(B) is a global bias parameter, while the αD vectors are corpuswide weight vectors and αd are document-specific weight vectors over the components of each factor.",3 Multi-Dimensional Text Models,[0],[0]
"Structuring the prior in this way models the intuition that if a triple with a particular component has high probability, other triples containing that component are likely to also have high probability.",3 Multi-Dimensional Text Models,[0],[0]
"For example, if a message discusses triples of the form (CANNABIS,*,EFFECTS), it is more likely to discuss (CANNABIS,*,HEALTH) than (COCAINE,*,HEALTH), because the message is about cannabis.
",3 Multi-Dimensional Text Models,[0],[0]
"Finally, B is a 3-dimensional array that encodes a sparsity pattern over the space of possible triples.",3 Multi-Dimensional Text Models,[0],[0]
This is used to accommodate triples that can be generated by the model but are not supported by the data.,3 Multi-Dimensional Text Models,[0],[0]
"For example, not all routes of administration may be applicable to certain drugs, or certain aspects of a drug may happen to not be discussed in the forum.",3 Multi-Dimensional Text Models,[0],[0]
"Each element b~t of the array is a real-valued scalar in (0, 1) which is multiplied with α̂(d)~t to adjust the prior for that triple.",3 Multi-Dimensional Text Models,[0],[0]
"If the b value is near 0 for a particular triple, then it will have very low prior probability.",3 Multi-Dimensional Text Models,[0],[0]
"The b values have Beta(γ0,γ1) priors (γ < 1) which encourage them to be near 0 or 1, so that they function as binary variables.
",3 Multi-Dimensional Text Models,[0],[0]
"Posterior inference and parameter estimation consist of a Monte Carlo EM algorithm that alternates between an iteration of collapsed Gibbs sampler on the ~z variables (E-step), and an iteration of gradient ascent on the α and ω hyperparameters (M-step).",3 Multi-Dimensional Text Models,[0],[0]
See Paul and Dredze (2012b) for more details.,3 Multi-Dimensional Text Models,[0],[0]
"In an unsupervised setting, there is no reason f-LDA would actually infer parameters corresponding to the three factors we have been describing.",3.1 Tags and Word Priors,[0],[0]
"However, the forums include metadata that can help guide the model: the messages are organized into forums corresponding to drug type (factor 1), and some threads
are tagged with labels corresponding to routes of administration and other aspects (factors 2 and 3).",3.1 Tags and Word Priors,[0],[0]
"Tags for aspects are manually grouped into components: e.g. USAGE (tags: Dose, Storing, Weight).",3.1 Tags and Word Priors,[0],[0]
"Table 1 shows the factors and components in our model.
",3.1 Tags and Word Priors,[0],[0]
One could simply use these tags as labels in a simple supervised model—this will be our experimental baseline (§4.1).,3.1 Tags and Word Priors,[0],[0]
"However, this approach has limitations in that most documents are missing labels (less than a third of our corpus contains one of the labels in Table 1) and many messages discuss several components, not just the one implied by the tag.",3.1 Tags and Word Priors,[0],[0]
"For example, a message tagged “Side effects” may talk about both side effects and dosage.",3.1 Tags and Word Priors,[0],[0]
"While a supervised classifier may attribute all words to a single tag, f-LDA learns per-token assignments.
",3.1 Tags and Word Priors,[0],[0]
We will instead use the tags to inform the priors over our f-LDA word distribution parameters.,3.1 Tags and Word Priors,[0],[0]
We do this with a two-stage approach.,3.1 Tags and Word Priors,[0],[0]
"First, we use the tags to train parameters of a related but simplified model.",3.1 Tags and Word Priors,[0],[0]
"We then use the learned parameters as priors over the corresponding f-LDA parameters.
",3.1 Tags and Word Priors,[0],[0]
"In particular, we will place priors on the ω vectors, the Dirichlet hyperparameters which influence the word distributions.",3.1 Tags and Word Priors,[0],[0]
"Suppose that we are given a vector η(0) which is believed to contain desirable values for ω(0), the weight vector over words in the corpus, and similarly we are given vectors η(f)i over the vocabulary for the ith component of factor f , which are believed to be good values for ω(f)i .",3.1 Tags and Word Priors,[0],[0]
"One option
is to fix ω as η, forcing the component weights to match the provided weights.",3.1 Tags and Word Priors,[0],[0]
"However, in our case η will only be an approximation of the optimal component parameters since it is estimated from incomplete data (only some messages have tags) and the η vectors are learned using an approximate model (see below).",3.1 Tags and Word Priors,[0],[0]
"Instead, these weight vectors will merely guide learning as prior knowledge over model parameters ω.",3.1 Tags and Word Priors,[0],[0]
"While f-LDA assumes each ω is drawn from a 0-mean Gaussian, we alter the means of the appropriate ω parameters to use η.
ω(0)w ∼ N (η(0)w , σ2);ω (k) iw ∼ N (η (k) iw , σ 2) (3) Recall that ω(0)w are corpus-wide bias parameters for each word and ω(k)iw are component-specific parameters for each word.",3.1 Tags and Word Priors,[0],[0]
"This yields a hierarchical prior in which η parameterizes the prior over ω, while ω parameterizes the prior over φ (the word distributions).",3.1 Tags and Word Priors,[0],[0]
The resulting ω parameters can vary from the provided priors to adapt to the data.,3.1 Tags and Word Priors,[0],[0]
"An example of learned parameters is shown in Figure 2, illustrating the hierarchical process behind this model.
",3.1 Tags and Word Priors,[0],[0]
"Learning the Priors In various applications, priors can come from many different sources, such as labeled data (Jagarlamudi et al., 2012).",3.1 Tags and Word Priors,[0],[0]
We learn the prior means η from tagged messages.,3.1 Tags and Word Priors,[0],[0]
"However, these parameters imply a latent division of responsibility for observed words: some are present because of the tag while others are general words in the corpus.",3.1 Tags and Word Priors,[0],[0]
"As a result, they must be estimated.
",3.1 Tags and Word Priors,[0],[0]
"We learn these parameters from the tagged messages using SAGE, which model words in a document as combinations of background and topic word distributions.",3.1 Tags and Word Priors,[0],[0]
"Eisenstein et al. (2011) present SAGE models for Naive Bayes (one class per document), admixture models (one class per token), and admixture models where tokens come from multiple factors.",3.1 Tags and Word Priors,[0],[0]
"We combine the first and third models, such that a document has multiple factors which are given as labels across the entire document—the drug type and the tag, which could correspond to a component of either the route or aspect factors.",3.1 Tags and Word Priors,[0],[0]
"We posit the following model of text generation per document:
P (word w|drug = i, factorf = j) (4)
= exp(η
(0) w",3.1 Tags and Word Priors,[0],[0]
"+ η (drug) iw + η (f) jw )∑
w′ exp(η (0) w′",3.1 Tags and Word Priors,[0],[0]
+ η (drug) iw′ + η,3.1 Tags and Word Priors,[0],[0]
"(f) jw′)
",3.1 Tags and Word Priors,[0],[0]
"This log-linear model has a similar form as Eq. 1, but with two factors instead of three, and it is a distribution rather than a Dirichlet vector.",3.1 Tags and Word Priors,[0],[0]
"As in SAGE, we fix η(0) to be the observed vector of corpus log-frequencies over the vocabulary, which acts as an “overall” weight vector, while parameter estimation yields η(f)i , the logit parameters for the ith component of factor",3.1 Tags and Word Priors,[0],[0]
f .1,3.1 Tags and Word Priors,[0],[0]
"These parameters are then used as the mean of the Gaussian priors over ω.
",3.1 Tags and Word Priors,[0],[0]
Standard optimization methods can be used to estimate these parameters.,3.1 Tags and Word Priors,[0],[0]
"The partial derivative of the likelihood with respect to the parameter η(drug)iw is:
∂
∂η (drug) iw = ∑ f ∑ j∈f c(i, j, w)− π(i, j, w)c(i, j, ∗)
(5) where c(i, j, w) is the number of times word w appears in documents labeled with i (drug) and j (tag), and π(i, j, w) denotes the probability given by (4).",3.1 Tags and Word Priors,[0],[0]
The partial derivative of each η(f)j is similar.,3.1 Tags and Word Priors,[0],[0]
Our corpus consists of messages from drugs-forum.com (§2.1).,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"The site categorizes threads into many forums and subforums, including some on specific drugs, which are categorized hierarchically.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We treated higher-level categories with pharmacologically similar drugs as a single drug type (e.g. OPIOIDS, AMPHETAMINES); for others we took the finest-granularity subforum as the drug type.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We selected 22 popular drugs and from these forums we crawled 410K messages.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We selected a subset of tags to form components for the route and aspect factors.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
(Some tags were too general or infrequent to be useful.),4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
A list of the tags and drugs used appears in Table 1.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We also included a GENERAL component in the latter two factors to model word usage which does not pertain to a particular route or aspect; the prior parameters η for these components were simply set to 0.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
We wish to demonstrate that our modified f-LDA model can be used to discover useful information in the text.,4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"One way to demonstrate this is by using the model to extract relevant snippets of text from the
1SAGE models sparsity on the weights via a Laplacian prior.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Such sparsity is not modeled in f-LDA, so we ignore this here.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"forums, which will form the basis of our evaluation experiments.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Our goal is not to build a complete summarization system, but rather to use the model to direct researchers to interesting messages.
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"While we model all 22 drugs, our summarization experiments will focus on five drugs which have been studied only relatively recently: mephedrone and MDPV (β-ketones), BromoDragonfly (synthetic phenethylamines), Spice/K2 (synthetic cannabinoids), and salvia divinorum.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We will consider these drugs in particular because these are the five drugs for which technical reports were created by the EU Psychonaut Project (Schifano et al., 2006), an online database of novel and emerging drugs, whose information is collected by reading drug websites, including Drugs-Forum.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Extensive technical reports were written about these five popular drugs, and we can use these reports to produce reference summaries for our experiments (§4.2).
",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Of these five drugs, only salvia has its own subforum; the others belong to subforums representing the broader categories shown in parentheses.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"We simply model the drug type as a proxy for the specific drug, as most of the drugs in each category have similar effects and properties.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"The first two drugs are both in the same subforum, so for the purpose of our model we treat mephedrone and MDPV as the single drug type, β-ketones.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"These two drugs are grouped together during summarization (§4.2), but the corresponding reference summaries incorporate excepts from the technical reports on both drugs.",4 Experiments with Topic Modeling for Extractive Summarization,[0],[0]
"Of the four drug types being considered for summarization, our data set contains 12K messages with one of the tags in Table 1 and 30K without.",4.1 Model Setup,[0],[0]
"Of those without tags, we set aside 5K as development data.",4.1 Model Setup,[0],[0]
There are also over 300K messages (140K tagged) from the remaining 18 drug types: some of these messages are utilized when training f-LDA.,4.1 Model Setup,[0],[0]
"Even though we only consider four drug types in our experiments, our intuition is that it can be beneficial to model other drugs as well, because this will help to learn parameters for the various aspects and routes of administration.",4.1 Model Setup,[0],[0]
"Our model of the effects of mephedrone can be informed by also modeling the effects of other stimulants such as cocaine.
",4.1 Model Setup,[0],[0]
"Each message was treated as a document, and we
only used documents with at least five word tokens after stop words, low-frequency words, and punctuation were removed.",4.1 Model Setup,[0],[0]
"The preprocessed data sets contained an average of 45 tokens per document.
",4.1 Model Setup,[0],[0]
"Below, we describe two f-LDA variants as well as the baseline used in our experiments.
",4.1 Model Setup,[0],[0]
Baseline,4.1 Model Setup,[0],[0]
Our baseline model is a unigram language model trained on the subset of messages which are tagged.,4.1 Model Setup,[0],[0]
"We treat the drug subforum as a label for the drug factor, and each message’s tag is used as a label for either the route or aspect factor.",4.1 Model Setup,[0],[0]
"For example, the word distribution for the pair (SALVIA,EFFECTS) is estimated as the empirical distribution from messages posted in the salvia forum and tagged with “Effects.”",4.1 Model Setup,[0],[0]
"We use add-λ smoothing where λ is chosen to optimize likelihood on the held-out development set.
",4.1 Model Setup,[0],[0]
"This is a two-dimensional model, since we explicitly model pairs such as (MEPHEDRONE,SNORTING) or (SALVIA,EFFECTS).",4.1 Model Setup,[0],[0]
"However, we also created word distributions for triples such as (SALVIA,ORAL,EFFECTS) by taking a mixture of the corresponding pairs: in this example, we estimate the unigram distribution from salvia documents tagged with either “Oral” or “Effects.”
Factorial LDA Because f-LDA does not rely on tagged data (the tags are only used to create priors), we can run inference on larger sets of data.",4.1 Model Setup,[0],[0]
"The drawback is that despite these priors, it is still mostly unsupervised and we want to be careful to ensure the model will learn the patterns we care about.",4.1 Model Setup,[0],[0]
"We thus add some reasonable constraints to the parameter space to guide the model further.
",4.1 Model Setup,[0],[0]
"First, we treat the drug type as an observed variable based on the subforum the message comes from, just as with the baseline.",4.1 Model Setup,[0],[0]
"For example, only tuples of the form (SALVIA,∗,∗) can be assigned to tokens in the salvia forum.",4.1 Model Setup,[0],[0]
"Second, we restrict the set of possible routes of administration that can be assigned to tokens in particular drug forums, since most drugs can be taken through only a subset of routes.",4.1 Model Setup,[0],[0]
"For example, marijuana is typically smoked or eaten orally, but rarely injected.",4.1 Model Setup,[0],[0]
We therefore restrict each drug’s allowable set of administration routes to those which are tagged (e.g. with “Oral” or “Snorting”) in at least 1% of that drug’s data.,4.1 Model Setup,[0],[0]
"Similar ideas are used in Labeled LDA (Ramage et al.,
2009), in which tags are used to restrict the space of allowed topics in a document.
",4.1 Model Setup,[0],[0]
"We use f-LDA as a three-dimensional model which explicitly models triples, but we also obtain distributions for pairs such as (SALVIA,EFFECTS) by marginalizing across all distributions of the form (SALVIA,∗,EFFECTS).",4.1 Model Setup,[0],[0]
"We trained f-LDA on two different data sets, yielding the following models:
• f-LDA-1: We use the 12K messages with tags and fill the set out with 13K messages with tags uniformly sampled from the 18 other drugs, for a total of 25K messages.
",4.1 Model Setup,[0],[0]
• f-LDA-2:,4.1 Model Setup,[0],[0]
"We use all 37K messages (many without tags) and fill the set out with 63K messages with tags uniformly sampled from the 18 other drugs, for a total of 100K messages.
",4.1 Model Setup,[0],[0]
All f-LDA instances are run with 5000 iterations alternating between a sweep of Gibbs sampling followed by a step of gradient ascent on the hyperparameters.,4.1 Model Setup,[0],[0]
"While we do not use the tags as strict labels during sampling, we initialize the Gibbs sampler so that each token in a document is assigned to its label given by the tag, when available.",4.1 Model Setup,[0],[0]
"In the absence of tags (in f-LDA-2), we initialize tokens
to the GENERAL components.",4.1 Model Setup,[0],[0]
"We initialized ω to its prior mean (Eq. 3), while the variance σ2 and the initialization of bias ω(B) are chosen to optimize likelihood on the held-out development set.
",4.1 Model Setup,[0],[0]
We optimized the hyperparameters and sparsity array using gradient descent after each Gibbs sweep.,4.1 Model Setup,[0],[0]
"We use a decreasing step size of a/(t+1000), where t is the current iteration and a=10 for α and 1 for ω and the sparsity values.",4.1 Model Setup,[0],[0]
"To learn priors η, we ran our version of SAGE for 100 iterations of gradient ascent (fixed step size of 0.1).",4.1 Model Setup,[0],[0]
See Paul and Dredze (2012a) for examples of parameters (the top words associated with various triples) learned by this model on this corpus.,4.1 Model Setup,[0],[0]
"We created twelve reference summaries by editing together excerpts from the five Psychonaut Project reports ((Psychonaut), 2009).",4.2 Summary Generation,[0],[0]
Each reference is matched to drug-specific pairs and triples.,4.2 Summary Generation,[0],[0]
"For example, a paragraph describing the differences in effects of salvia between smoking and oral routes was matched to distributions for (SALVIA,EFFECTS), (SALVIA,SMOKING,EFFECTS), (SALVIA,ORAL,EFFECTS).",4.2 Summary Generation,[0],[0]
"Descriptions of creating tinctures and blotters for oral consumption were matched to (SALVIA,ORAL,CHEMISTRY).",4.2 Summary Generation,[0],[0]
"We consider pairs in addition to triples because not all summaries correspond to particular routes or aspects.
",4.2 Summary Generation,[0],[0]
"For each tuple-specific word distribution (a pair or a triple), we create a “summary” by extracting a set of five text snippets which minimize KL-divergence to the target word distribution.",4.2 Summary Generation,[0],[0]
"We consider all overlapping text windows of widths {10,15,20} in the corpus as candidate snippets.",4.2 Summary Generation,[0],[0]
"Following Haghighi and Vanderwende (2009), we greedily add snippets one by one with the lowest KL-divergence at each step until we have added five.
",4.2 Summary Generation,[0],[0]
"We only considered candidate snippets within the subforum for the particular drug, and snippets are based on the preprocessed topic model input with no stop words.",4.2 Summary Generation,[0],[0]
"Before presenting snippets to users, we then map the snippets back to the raw text by taking all sentences which are at least partly spanned by the window of tokens.",4.2 Summary Generation,[0],[0]
"Because each reference may be matched to more than one tuple, there may be more than five snippets which correspond to a reference.",4.2 Summary Generation,[0],[0]
Recall that the reports used as reference summaries were themselves created by reading web forums.,4.3 Experimental Results,[0],[0]
Our hypothesis is that f-LDA could be used as an exploratory tool to expedite the creation of these reports.,4.3 Experimental Results,[0],[0]
Thus in our evaluation we want to measure how useful the extracted snippets would be in informing the writing of such reports.,4.3 Experimental Results,[0],[0]
We performed both human and automatic evaluation on the summaries generated by f-LDA (variants 1 and 2) as well as our baseline.,4.3 Experimental Results,[0],[0]
"We also included randomly selected snippets as a control (five per reference).
",4.3 Experimental Results,[0],[0]
Example output is shown in Figure 3.,4.3 Experimental Results,[0],[0]
Three annotators were presented snippets pooled from all four systems we are evaluating alongside the corresponding reference text.,4.3.1 Human Judgments of Quality,[0],[0]
"Within each set corresponding to a reference summary, the snippets were shown in a random order.",4.3.1 Human Judgments of Quality,[0],[0]
"Annotators were asked to judge each snippet independently on a 5- point Likert scale as to how useful each snippet would be in writing the reference text.
",4.3.1 Human Judgments of Quality,[0],[0]
The distribution of scores is shown in Figure 4 and summarized in Table 2.,4.3.1 Human Judgments of Quality,[0],[0]
Annotators generally agreed on the relative quality of snippets: the average correlation of scores between each pair of annotators was 0.49.,4.3.1 Human Judgments of Quality,[0],[0]
"Snippets produced by f-LDA were given more high scores and fewer low scores than the baseline, while the two f-LDA variants were rated comparably.",4.3.1 Human Judgments of Quality,[0],[0]
"The breakdown is more interesting when we compare scores for snippets that were matched
to word distributions for pairs versus word distributions for triples.",4.3.1 Human Judgments of Quality,[0],[0]
The gap in scores between fLDA and the baseline increases when we look at the scores for only triples: f-LDA beats the baseline by a margin of 0.45 for snippets matched to triples and 0.21 for pairs.,4.3.1 Human Judgments of Quality,[0],[0]
This suggests that we produce better triples by modeling them jointly.,4.3.1 Human Judgments of Quality,[0],[0]
"For triples, f-LDA2 (which uses more data) beats f-LDA-1 (which uses only tagged data), while the reverse is true for pairs.
",4.3.1 Human Judgments of Quality,[0],[0]
"While some of the randomly selected control snippets happened to be useful, the scores for these snippets were much lower than those extracted through model-based systems.",4.3.1 Human Judgments of Quality,[0],[0]
"This suggests that exploring the forums in a targeted way (e.g. through our topic model approach) would be more efficient than exploring the data in a non-targeted way (akin to the random approach).
",4.3.1 Human Judgments of Quality,[0],[0]
"Finally, we asked two expert annotators (faculty members in psychiatry and behavioral pharmacology, who have used drug forums in the past to study emerging drugs) to rate the snippets corresponding to mephedrone/MDPV.",4.3.1 Human Judgments of Quality,[0],[0]
The best f-LDA system had an average score of 2.57 compared to a baseline score of 2.45 and random score of 1.63.,4.3.1 Human Judgments of Quality,[0],[0]
"The human judgments effectively measured a form of precision, as the quality of snippets were judged by their correspondence to the reference text, without regard to how much of the reference text was covered by all snippets.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We also used the automatic evaluation metric ROUGE (Lin, 2004) as a rough estimate of summary recall: this metric computes the percentage of n-grams in the reference text that appeared in the generated summaries.
",4.3.2 Automatic Evaluation of Recall,[0],[0]
We computed ROUGE for both 1-grams and 2- grams.,4.3.2 Automatic Evaluation of Recall,[0],[0]
"When computing n-gram counts, we applied Porter’s stemmer to all tokens.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We excluded stop
words from 1-gram counts but included them in 2- gram counts where we care about longer phrases.2
Results are shown in Table 2.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"We find that f-LDA1 has the highest score for both 1- and 2-grams, suggesting that it is extracting a more diverse set of relevant snippets.",4.3.2 Automatic Evaluation of Recall,[0],[0]
"When performing a paired t-test across the 12 reference summaries, we find that fLDA is better than the baseline with p-values 0.14 and 0.10 for 1-gram and 2-gram recall, respectively.",4.3.2 Automatic Evaluation of Recall,[0],[0]
f-LDA’s recall advantage may come from the fact that it learns from a larger amount of data and it may learn more diverse word distributions by directly modeling triples.,4.3.2 Automatic Evaluation of Recall,[0],[0]
"f-LDA-1 had slightly better recall (under ROUGE), while f-LDA-2 was slightly better according to the human annotators.",4.3.2 Automatic Evaluation of Recall,[0],[0]
We have proposed exploratory tools for the analysis of online drug communities.,5 Conclusion,[0],[0]
"Such communities are an emerging source of drug research, but manually browsing through large corpora is impractical and important information could be missed.",5 Conclusion,[0],[0]
"We have demonstrated that topic models are capable of modeling informative portions of text, and in particular multi-dimensional topic models can target desired structures such as the combination of aspect and route of administration for each drug.",5 Conclusion,[0],[0]
We have presented an extension to factorial LDA tailored to a particular application and data set which was demonstrated to induce desired properties.,5 Conclusion,[0],[0]
"As a technical contribution, this study lays out practical guidelines for customizing and incorporating prior knowledge into multi-dimensional text models.",5 Conclusion,[0],[0]
"We are grateful to Dr. Margaret S. Chisolm and Dr. Ryan Vandrey from the Johns Hopkins School of Medicine for providing the mephedrone/MDPV annotations, and Alex Lamb and Hieu Tran for assisting with the full annotations.",Acknowledgments,[0],[0]
"We also thank Dr. Matthew W. Johnson for additional advice, and the anonymous reviewers for helpful feedback and suggestions.",Acknowledgments,[0],[0]
"This research was partly supported by an NSF Graduate Research Fellowship.
",Acknowledgments,[0],[0]
"2In both cases, ROUGE scores were higher when stop words were included.",Acknowledgments,[0],[0]
f-LDA beats the baseline by similar margins regardless of whether we include stop words.,Acknowledgments,[0],[0]
"Multi-dimensional latent text models, such as factorial LDA (f-LDA), capture multiple factors of corpora, creating structured output for researchers to better understand the contents of a corpus.",abstractText,[0],[0]
"We consider such models for clinical research of new recreational drugs and trends, an important application for mining current information for healthcare workers.",abstractText,[0],[0]
"We use a “three-dimensional” f-LDA variant to jointly model combinations of drug (marijuana, salvia, etc.), aspect (effects, chemistry, etc.) and route of administration (smoking, oral, etc.)",abstractText,[0],[0]
"Since a purely unsupervised topic model is unlikely to discover these specific factors of interest, we develop a novel method of incorporating prior knowledge by leveraging user generated tags as priors in our model.",abstractText,[0],[0]
We demonstrate that this model can be used as an exploratory tool for learning about these drugs from the Web by applying it to the task of extractive summarization.,abstractText,[0],[0]
"In addition to providing useful output for this important public health task, our prior-enriched model provides a framework for the application of fLDA to other tasks.",abstractText,[0],[0]
Drug Extraction from the Web: Summarizing Drug Experiences with Multi-Dimensional Topic Models,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 917–927, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.",text,[0],[0]
Graphical models allow expert modeling of complex relations and interactions between random variables.,1 Introduction,[0],[0]
"Since a graphical model with given parameters defines a probability distribution, it can be used to reconstruct values for unobserved variables.",1 Introduction,[0],[0]
The marginal inference problem is to compute the posterior marginal distributions of these variables.,1 Introduction,[0],[0]
"The MAP inference (or MPE) problem is to compute the single highest-probability joint assignment to all the unobserved variables.
",1 Introduction,[0],[0]
"Inference in general graphical models is NPhard even when the variables’ values are finite discrete values such as categories, tags or domains.",1 Introduction,[0],[0]
"In this paper, we address the more challenging setting
∗This material is based upon work supported by the National Science Foundation under Grant No. 1423276.
where the variables in the graphical models range over strings.",1 Introduction,[0],[0]
"Thus, the domain of the variables is an infinite space of discrete structures.
",1 Introduction,[0],[0]
"In NLP, such graphical models can deal with large, incompletely observed lexicons.",1 Introduction,[0],[0]
"They could be used to model diverse relationships among strings that represent spellings or pronunciations; morphemes, words, phrases (such as named entities and URLs), or utterances; standard or variant forms; clean or noisy forms; contemporary or historical forms; underlying or surface forms; source or target language forms.",1 Introduction,[0],[0]
"Such relationships arise in domains such as morphology, phonology, historical linguistics, translation between related languages, and social media text analysis.
",1 Introduction,[0],[0]
"In this paper, we assume a given graphical model, whose factors evaluate the relationships among observed and unobserved strings.1 We present a dual decomposition algorithm for MAP inference, which returns a certifiably optimal solution when it converges.",1 Introduction,[0],[0]
We demonstrate our method on a graphical model for phonology proposed by Cotterell et al. (2015).,1 Introduction,[0],[0]
"We show that the method generally converges and that it achieves better results than alternatives.
",1 Introduction,[0],[0]
"The rest of the paper is arranged as follows: We will review graphical models over strings in section 2, and briefly introduce our sample problem in section 3.",1 Introduction,[0],[0]
Section 4 develops dual decomposition inference for graphical models over strings.,1 Introduction,[0],[0]
"Then our experimental setup and results are presented in sections 5 and 6, with some discussion.",1 Introduction,[0],[0]
"To perform inference on a graphical model (directed or undirected), one first converts the model to a factor graph representation (Kschischang et al., 2001).",2.1 Factor Graphs and MAP Inference,[0],[0]
"A factor graph is a finite bipartite
1In some task settings, it is also necessary to discover the model topology along with the model parameters.",2.1 Factor Graphs and MAP Inference,[0],[0]
In this paper we do not treat that structure learning problem.,2.1 Factor Graphs and MAP Inference,[0],[0]
"However, both structure learning and parameter learning need to call inference—such as the method presented here—in order to evaluate proposed topologies or improve their parameters.
917
graph over a set X = {X1, X2, . . .",2.1 Factor Graphs and MAP Inference,[0],[0]
} of variables and a set F of factors.,2.1 Factor Graphs and MAP Inference,[0],[0]
An assignment to the variables is a vector of values x =,2.1 Factor Graphs and MAP Inference,[0],[0]
"(x1, x2, . . .).",2.1 Factor Graphs and MAP Inference,[0],[0]
"Each factor F ∈ F is a real-valued function of x, but it depends on a given xi only if F is connected to Xi in the graph.",2.1 Factor Graphs and MAP Inference,[0],[0]
"Thus, a degree d-factor scores some length-d subtuple of x.",2.1 Factor Graphs and MAP Inference,[0],[0]
"The score of the whole joint assignment simply sums over all factors:
score(x) def=",2.1 Factor Graphs and MAP Inference,[0],[0]
∑ F∈F F (x).,2.1 Factor Graphs and MAP Inference,[0],[0]
"(1)
We seek the x of maximum score that is consistent with our partial observation of x.",2.1 Factor Graphs and MAP Inference,[0],[0]
This is a generic constraint satisfaction problem with soft constraints.,2.1 Factor Graphs and MAP Inference,[0],[0]
"While our algorithm does not depend on a probabilistic interpretation of the factor graph,2 it can be regarded as peforming maximum a posteriori (MAP) inference of the unobserved variables, under the probability distribution p(x) def=",2.1 Factor Graphs and MAP Inference,[0],[0]
(1/Z) exp score(x).,2.1 Factor Graphs and MAP Inference,[0],[0]
Graphical models over strings have enjoyed some attention in the NLP community.,2.2 The String Case,[0],[0]
"Tree-shaped graphical models naturally model the evolutionary tree of word forms (Bouchard-Côté et al., 2007; Bouchard-Côté et al., 2008; Hall and Klein, 2010; Hall and Klein, 2011).",2.2 The String Case,[0],[0]
"Cyclic graphical
2E.g., it could be used for exactly computing the separation oracle when training a structural SVM (Tsochantaridis et al., 2005; Finley and Joachims, 2007).",2.2 The String Case,[0],[0]
"Another use is minimum Bayes risk decoding—computing the joint assignment having minimum expected loss—if the loss function does not decompose over the variables, but a factor graph can be constructed that evaluates the expected loss of any assignment.
models have been used to model morphological paradigms (Dreyer and Eisner, 2009; Dreyer and Eisner, 2011) and to reconstruct phonological underlying forms of words (Cotterell et al., 2015).
",2.2 The String Case,[0],[0]
The variables in such a model are strings of unbounded length: each variable Xi is permitted to range over Σ∗,2.2 The String Case,[0],[0]
"where Σ is some fixed, finite alphabet.",2.2 The String Case,[0],[0]
"As in previous work, we assume that a degree-d factor is a d-way rational relation, i.e., a function of d strings that can be computed by a d-tape weighted finite-state machine (WFSM) (Mohri et al., 2002; Kempe et al., 2004).",2.2 The String Case,[0],[0]
"Such a machine is called an acceptor (WFSA) if d = 1 or a transducer (WFST) if d = 2.3
Past work has shown how to approximately sample from the distribution over x defined by such a model (Bouchard-Côté et al., 2007), or approximately compute the distribution’s marginals using variants of sum-product belief propagation (BP) (Dreyer and Eisner, 2009) and expectation propagation (EP) (Cotterell and Eisner, 2015).",2.2 The String Case,[0],[0]
BP iteratively updates messages between factors and variables.,2.3 Finite-State Belief Propagation,[0],[0]
"Each message is a vector whose elements score the possible values of a variable.
",2.3 Finite-State Belief Propagation,[0],[0]
Murphy et al. (1999) discusses BP on cyclic (“loopy”) graphs.,2.3 Finite-State Belief Propagation,[0],[0]
"For pedagogical reasons, suppose momentarily that all factors have degree ≤ 2 (this loses no power).",2.3 Finite-State Belief Propagation,[0],[0]
"Then BP manipulates only vectors and matrices—whose dimensionality depends on the number of possible values of the vari-
3Finite-state software libraries often support only these cases.",2.3 Finite-State Belief Propagation,[0],[0]
"Accordingly, Cotterell and Eisner (2015, Appendix B.10) explain how to eliminate factors of degree d > 2.
ables.",2.3 Finite-State Belief Propagation,[0],[0]
"In the string case, they have infinitely many rows and columns, indexed by possible strings.
",2.3 Finite-State Belief Propagation,[0],[0]
"Dreyer and Eisner (2009) represented these infinite vectors and matrices by WFSAs and WFSTs, respectively.",2.3 Finite-State Belief Propagation,[0],[0]
They observed that the simple linear-algebra operations used by BP can be implemented by finite-state constructions.,2.3 Finite-State Belief Propagation,[0],[0]
The pointwise product of two vectors is the intersection of their WFSAs; the marginalization of a matrix is the projection of its WFST; a vector-matrix product is computed by composing the WFSA with the WFST and then projecting onto the output tape.,2.3 Finite-State Belief Propagation,[0],[0]
"For degree > 2, BP’s rank-d tensors become dtape WFSMs, and these constructions generalize.
",2.3 Finite-State Belief Propagation,[0],[0]
"Unfortunately, except in small acyclic models, the BP messages—which are WFSAs—usually become impractically large.",2.3 Finite-State Belief Propagation,[0],[0]
Each intersection or composition involves a cross-product construction.,2.3 Finite-State Belief Propagation,[0],[0]
"For example, when finding the marginal distribution at a degree-d variable, intersecting d WFSA messages having m states each may yield a WFSA with up to md states.",2.3 Finite-State Belief Propagation,[0],[0]
(Our models in section 6 include variables with d up to 156.),2.3 Finite-State Belief Propagation,[0],[0]
"Combining many cross products, as BP iteratively passes messages along a path in the factor graph, leads to blowup that is exponential in the length of the path—which in turn is unbounded if the graph has cycles (Dreyer and Eisner, 2009), as ours do.
",2.3 Finite-State Belief Propagation,[0],[0]
The usual solution is to prune or otherwise approximate the messages at each step.,2.3 Finite-State Belief Propagation,[0],[0]
"In particular, Cotterell and Eisner (2015) gave a principled way to approximate the messages using variablelength n-gram models, using an adaptive variant of Expectation Propagation (Minka, 2001).",2.3 Finite-State Belief Propagation,[0],[0]
"In section 4, we will present a dual decomposition (DD) method that decomposes the original complex problem into many small subproblems that are free of cycles and high degree nodes.",2.4 Dual Decomposition Inference,[0],[0]
"BP can solve each subproblem without approximation.4
The subproblems “communicate” through Lagrange multipliers that guide them towards agreement on a single global solution.",2.4 Dual Decomposition Inference,[0],[0]
This information is encoded in WFSAs that score possible values of a string variable.,2.4 Dual Decomposition Inference,[0],[0]
"DD incrementally adjusts the WFSAs so as to encourage values that agree with
4Such small BP problems commonly arise in NLP.",2.4 Dual Decomposition Inference,[0],[0]
"In particular, using finite-state methods to decode a composition of several finite-state noisy channels (Pereira and Riley, 1997; Knight and Graehl, 1998) can be regarded as BP on a graphical model over strings that has a linear-chain topology.
",2.4 Dual Decomposition Inference,[0],[0]
the variable’s average value across subproblems.,2.4 Dual Decomposition Inference,[0],[0]
"Unlike BP messages, the WFSAs in our DD method will be restricted to be variable-length n-gram models, similar to Cotterell and Eisner (2015).",2.4 Dual Decomposition Inference,[0],[0]
They may still grow over time; but DD often halts while the WFSAs are still small.,2.4 Dual Decomposition Inference,[0],[0]
"It halts when its strings agree exactly, rather than when it has converged up to a numerical tolerance, like BP.",2.4 Dual Decomposition Inference,[0],[0]
Our factors may be nondeterministic WFSMs.,2.5 Switching Between Semirings,[0],[0]
"So when F ∈ F scores a given d-tuple of string values, it may accept that d-tuple along multiple different WFSM paths with different scores, corresponding to different alignments of the strings.
",2.5 Switching Between Semirings,[0],[0]
"For purposes of MAP inference, we define F to return the maximum of these path scores.",2.5 Switching Between Semirings,[0],[0]
"That is, we take the WFSMs to be defined with weights in the (max,+) semiring (Mohri et al., 2002).",2.5 Switching Between Semirings,[0],[0]
"Equivalently, we are seeking the “best global solution” in the sense of choosing not only the strings xi but also the alignments of the d-tuples.5
To do so, we must solve each DD subproblem in the same sense.",2.5 Switching Between Semirings,[0],[0]
We use max-product BP.,2.5 Switching Between Semirings,[0],[0]
This still applies the Dreyer-Eisner method of section 2.3.,2.5 Switching Between Semirings,[0],[0]
"Since these WFSMs are defined in the (max,+) semiring, the method’s finite-state operations will combine weights using max and +.
MAP inference in our setting is in general computationally undecidable.6 However, if DD converges (as in our experiments), then its solution is guaranteed to be the true MAP assignment.
",2.5 Switching Between Semirings,[0],[0]
"In section 6, we will compare DD with (loopy) max-product BP and (loopy) sum-product BP.",2.5 Switching Between Semirings,[0],[0]
These respectively approximate MAP inference and marginal inference over the entire factor graph.,2.5 Switching Between Semirings,[0],[0]
Marginal inference computes marginal string probabilities that sum (rather than maximize) over the choices of other strings and the choices of paths.,2.5 Switching Between Semirings,[0],[0]
"Thus, for sum-product BP, we re-interpret the factor WFSMs as defined over the (logadd,+) semiring.",2.5 Switching Between Semirings,[0],[0]
"This means that the exponentiated score assigned by a WFSM is the sum of the exponentiated scores of the accepting paths.
",2.5 Switching Between Semirings,[0],[0]
5This problem is more specifically called MPE inference.,2.5 Switching Between Semirings,[0],[0]
6The trouble is that we cannot bound the length of the latent strings.,2.5 Switching Between Semirings,[0],[0]
"If we could, then we could encode them using a finite set of boolean variables, and solve as an ILP problem.",2.5 Switching Between Semirings,[0],[0]
But that would allow us to determine whether there exists a MAP assignment with score ≥ 0.,2.5 Switching Between Semirings,[0],[0]
"That is impossible in general, because it would solve Post’s Correspondence Problem as a simple special case (see Dreyer and Eisner (2009)).",2.5 Switching Between Semirings,[0],[0]
"Before giving the formal details of our DD method, we give a motivating example: a recently proposed graphical model for morphophonology.",3 A Sample Task: Generative Phonology,[0],[0]
Cotterell et al. (2015) defined a Bayesian network to describe the generative process of phonological words.,3 A Sample Task: Generative Phonology,[0],[0]
"Our Figure 1 shows a conversion of their model to a factor graph and explains what the variables and factors mean.
",3 A Sample Task: Generative Phonology,[0],[0]
Inference on this graph performs unsupervised discovery of latent strings.,3 A Sample Task: Generative Phonology,[0],[0]
"Given observed surface representations of words (SRs), inference aims to recover the underlying representations (URs) of the words and their shared constituent morphemes.",3 A Sample Task: Generative Phonology,[0],[0]
"The latter can then be used to predict held-out SRs.
",3 A Sample Task: Generative Phonology,[0],[0]
Notice that the 8 edges in the first layer of Figure 1 form a cycle; such cycles make BP inexact.,3 A Sample Task: Generative Phonology,[0],[0]
"Moreover, the figure shows only a schematic fragment of the graphical model.",3 A Sample Task: Generative Phonology,[0],[0]
"In the actual experiments, the graphical models have up to 829 variables, and the variables representing morpheme URs are connected to up to 156 factors (because many words share the same affix).
",3 A Sample Task: Generative Phonology,[0],[0]
"To handle the above challenges without approximation, we want to decompose the original problem into subproblems where each subproblem can be solved efficiently.",3 A Sample Task: Generative Phonology,[0],[0]
"In particular, we want the subproblems to be free of cycles and highdegree nodes.",3 A Sample Task: Generative Phonology,[0],[0]
"In our phonology example, each observed word along with its correspondent latent URs forms an ideal subproblem.",3 A Sample Task: Generative Phonology,[0],[0]
"This decomposition is shown in Figure 2.
",3 A Sample Task: Generative Phonology,[0],[0]
"While the subproblems can be solved efficiently in isolation, they may share variables, as shown by the dashed lines in Figure 2.",3 A Sample Task: Generative Phonology,[0],[0]
DD repeatedly modifies and re-solves the subproblems until they agree on their shared variables.,3 A Sample Task: Generative Phonology,[0],[0]
Dual decomposition is a general technique for solving constrained optimization problems.,4 Dual Decomposition,[0],[0]
"It has been widely used for MAP inference in graphical models (Komodakis et al., 2007; Komodakis and Paragios, 2009; Koo et al., 2010; Martins et al., 2011; Sontag et al., 2011; Rush and Collins, 2014).",4 Dual Decomposition,[0],[0]
"However, previous work has focused on variables Xi whose values are in R or a small finite set; we will consider the infinite set Σ∗.",4 Dual Decomposition,[0],[0]
"To apply dual decomposition, we must partition the original problem into a union of K subproblems, each of which can be solved exactly and efficiently (and in parallel).",4.1 Review of Dual Decomposition,[0],[0]
"For example, our experiments partition Figure 1 as shown in Figure 2.
",4.1 Review of Dual Decomposition,[0],[0]
"Specifically, we partition the factors into K sets F1, . . .",4.1 Review of Dual Decomposition,[0],[0]
",FK .",4.1 Review of Dual Decomposition,[0],[0]
Each factor F ∈ F appears in exactly one of these sets.,4.1 Review of Dual Decomposition,[0],[0]
This lets us rewrite the score (1) as ∑ k ∑ F∈Fk F (x).,4.1 Review of Dual Decomposition,[0],[0]
"Instead of simply seeking its maximizer x, we equivalently seek
argmax x1,...,xK K∑ k=1",4.1 Review of Dual Decomposition,[0],[0]
( ∑ F∈Fk F (xk) ) s.t.,4.1 Review of Dual Decomposition,[0],[0]
"x1 = · · · = xK (2)
",4.1 Review of Dual Decomposition,[0],[0]
"If we dropped the equality constraint, (2) could be solved by separately maximizing∑
F∈Fk F (x k) for each k.",4.1 Review of Dual Decomposition,[0],[0]
This “subproblem” is itself a MAP problem which considers only the factors,4.1 Review of Dual Decomposition,[0],[0]
Fk and the variables X k adjacent to them in the original factor graph.,4.1 Review of Dual Decomposition,[0],[0]
"The subproblem objective does not depend on the other variables.
",4.1 Review of Dual Decomposition,[0],[0]
"We now attempt to enforce the equality constraint indirectly, by adding Lagrange multipliers that encourage agreement among the subproblems.",4.1 Review of Dual Decomposition,[0],[0]
Assume for the moment that the variables in the factor graph are real-valued (each xki is in R).,4.1 Review of Dual Decomposition,[0],[0]
"Then consider the Lagrangian relaxation of (2),
max x1,...,xK K∑ k=1",4.1 Review of Dual Decomposition,[0],[0]
( ∑ F∈Fk F (xk) + ∑ i λki · xki ),4.1 Review of Dual Decomposition,[0],[0]
"(3)
This can still be solved by separate maximizations.",4.1 Review of Dual Decomposition,[0],[0]
"For any choices of λki ∈ R having (∀i) ∑ k λ k i = 0, it upper-bounds the objective of (2).",4.1 Review of Dual Decomposition,[0],[0]
Why?,4.1 Review of Dual Decomposition,[0],[0]
"The solution to (2) achieves the same value in (3), yet (3) may do even better by considering solutions that do not satisfy the constraint.",4.1 Review of Dual Decomposition,[0],[0]
Our goal is to find λki values that tighten this upper bound as much as possible.,4.1 Review of Dual Decomposition,[0],[0]
"If we can find λki values so that
the optimum of (3) satisfies the equality constraint, then we have a tight bound and a solution to (2).
",4.1 Review of Dual Decomposition,[0],[0]
"To improve the method, recall that subproblem k considers only variables",4.1 Review of Dual Decomposition,[0],[0]
X k.,4.1 Review of Dual Decomposition,[0],[0]
It is indifferent to the value ofXi ifXi /∈,4.1 Review of Dual Decomposition,[0],[0]
"X k, so we just leave xki undefined in the subproblem’s solution.",4.1 Review of Dual Decomposition,[0],[0]
We treat that as automatically satisfying the equality constraint; thus we do not need any Lagrange multiplier λki to force equality.,4.1 Review of Dual Decomposition,[0],[0]
"Our final solution x ignores undefined values, and sets xi to the value agreed on by the subproblems that did consider Xi.7",4.1 Review of Dual Decomposition,[0],[0]
But what do we do if the variables are strings?,4.2 Substring Count Features,[0],[0]
The Lagrangian term λki ·xki in (3) is now ill-typed.,4.2 Substring Count Features,[0],[0]
"We replace it with λki · γ(xki ), where γ(·) extracts a real-valued feature vector from a string, and λki is a vector of Lagrange multipliers.
",4.2 Substring Count Features,[0],[0]
This corresponds to changing the constraint in (2).,4.2 Substring Count Features,[0],[0]
"Instead of requiring x1i = · · · = xKi for each i, we are now requiring γ(x1i ) = · · · = γ(xKi ), i.e., these strings must agree in their features.
",4.2 Substring Count Features,[0],[0]
"We want each possible string to have a unique feature vector, so that matching features forces the actual strings to match.",4.2 Substring Count Features,[0],[0]
We follow Paul and Eisner (2012) and use a substring count feature for each w ∈,4.2 Substring Count Features,[0],[0]
"Σ∗. In other words, γ(x) is an infinitely long vector, which maps each w to the number of times that w appears in x as a substring.8
Computing λki · γ(xki ) in (3) remains possible because in practice, λki will have only finitely many nonzeros.",4.2 Substring Count Features,[0],[0]
"This is so because our feature vector γ(x) has only finitely many nonzeros for any string x, and the subgradient algorithm in section 4.3 below always updates λki by adding multiples of such γ(x) vectors.
",4.2 Substring Count Features,[0],[0]
We will use a further trick below to prevent rapid growth of this finite set of nonzeros.,4.2 Substring Count Features,[0],[0]
"Each variable Xi maintains an active set of features, Wi.",4.2 Substring Count Features,[0],[0]
Only these features may have nonzero Lagrange multipliers.,4.2 Substring Count Features,[0],[0]
"While the active set can grow over time, it will be finite at any given step.
",4.2 Substring Count Features,[0],[0]
"Given the Lagrange multipliers, subproblem k of (3) is simply MAP inference on the factor graph consisting of the variables X k and factors",4.2 Substring Count Features,[0],[0]
"Fk as well as an extra unary factor Gki at each Xi ∈ X k:
7Without this optimization, the Lagrangian term λki · xki would have driven xki to match that value anyway.
8More precisely, the number of times that w appears in BOS x EOS, where BOS, EOS are distinguished boundary symbols.",4.2 Substring Count Features,[0],[0]
"We allow w to start with BOS and/or end with EOS, which yields prefix and suffix indicator features.
",4.2 Substring Count Features,[0],[0]
Gki (x k) def=,4.2 Substring Count Features,[0],[0]
"λki · γ(xki ) (4)
",4.2 Substring Count Features,[0],[0]
These unary factors penalize strings according to the Lagrange multipliers.,4.2 Substring Count Features,[0],[0]
"They can be encoded as WFSAs (Allauzen et al., 2003; Cotterell and Eisner, 2015, Appendices B.1–B.5), allowing us to solve the subproblem by max-product BP as usual.",4.2 Substring Count Features,[0],[0]
"The topology of the WFSA for Gki depends only onWi, while its weights come from λki .",4.2 Substring Count Features,[0],[0]
We aim to adjust the collection λ of Lagrange multipliers to minimize the upper bound (3).,4.3 Projected Subgradient Method,[0],[0]
"Following Komodakis et al. (2007), we solve this convex dual problem using a projected subgradient method.",4.3 Projected Subgradient Method,[0],[0]
We initialize λ = 0 and compute (3) by solving the K subproblems.,4.3 Projected Subgradient Method,[0],[0]
"Then we take a step to adjust λ, and repeat in hopes of eventually satisfying the equality condition.
",4.3 Projected Subgradient Method,[0],[0]
"The projected subgradient step is
λki := λ k",4.3 Projected Subgradient Method,[0],[0]
i + η · ( µi − γ(xki ) ),4.3 Projected Subgradient Method,[0],[0]
"(5)
where η > 0 is the current step size, and µi is the mean of γ(xk
′",4.3 Projected Subgradient Method,[0],[0]
i ) over all subproblems k ′,4.3 Projected Subgradient Method,[0],[0]
that consider Xi.,4.3 Projected Subgradient Method,[0],[0]
"This update modifies (3) to encourage solutions xk such that γ(xki ) comes closer to µi.
",4.3 Projected Subgradient Method,[0],[0]
"For each i, we update all λki at once to preserve the property that (∀i)∑k λki = 0.",4.3 Projected Subgradient Method,[0],[0]
"However, we are only allowed to update components of the λki that correspond to features in the active setWi.",4.3 Projected Subgradient Method,[0],[0]
"To ensure that we continue to make progress even after we agree on these features, we first expandWi by adding the minimal strings (if any) on which the xki do not yet all agree.",4.3 Projected Subgradient Method,[0],[0]
"For example, we will add the abc feature only when the xki already agree on their counts of its substrings ab and bc.9
Algorithm 1 summarizes the whole method.",4.3 Projected Subgradient Method,[0],[0]
"Table 1 illustrates how one active setWi (section 4.3) evolves, in our experiments, as it tries to enforce agreement on a particular string xi.",4.3 Projected Subgradient Method,[0],[0]
Our DD algorithm is an extension of one that Paul and Eisner (2012) developed for the simpler implicit intersection problem.,4.4 Past Work: Implicit Intersection,[0],[0]
"Given many WFSAs F1, . . .",4.4 Past Work: Implicit Intersection,[0],[0]
", FK , they were able to find the string x with maximum total score ∑K k=1 Fk(x).",4.4 Past Work: Implicit Intersection,[0],[0]
"(They applied this to solve instances of the NP-hard Steiner 9In principle, we should check that they also (still) agree on a, b, and c, but we skip this check.",4.4 Past Work: Implicit Intersection,[0],[0]
"Our active set heuristic is almost identical to that of Paul and Eisner (2012).
",4.4 Past Work: Implicit Intersection,[0],[0]
Algorithm 1 DD for graphical models over strings 1: initialize the active setWi for each variable Xi ∈ X 2: initialize λki = 0 for each Xi and each subproblem k 3: for t,4.4 Past Work: Implicit Intersection,[0],[0]
= 1 to T do .,4.4 Past Work: Implicit Intersection,[0],[0]
max number of iterations 4: for k = 1 to K do .,4.4 Past Work: Implicit Intersection,[0],[0]
"solve all primal subproblems 5: if any of the λki have changed then 6: run max-product BP on the acyclic graph de-
fined by variablesX k and factorsFk andGki 7: extract MAP strings: ∀i with Xi ∈ X k, xki
is the label of the max-scoring accepting path in the WFSA that represents the belief at Xi
8: for each Xi ∈ X do .",4.4 Past Work: Implicit Intersection,[0],[0]
improve dual bound 9: if the defined strings xki are not all equal then 10: Expand active feature setWi .,4.4 Past Work: Implicit Intersection,[0],[0]
section 4.3 11: Update each λki .,4.4 Past Work: Implicit Intersection,[0],[0]
"equation (5) 12: Update each Gki from Θi,λ k i .",4.4 Past Work: Implicit Intersection,[0],[0]
"see (4) 13: if none of the Xi required updates then 14: return any defined xki (all are equal) for each i 15: return {x1i , . . .",4.4 Past Work: Implicit Intersection,[0],[0]
", xKi } for each i . failed to converge
string problem, i.e., finding the string x of minimum total edit distance to a collection ofK ≈ 100 given strings.)",4.4 Past Work: Implicit Intersection,[0],[0]
The naive solution to this problem would be to find the highest-weighted path in the intersection F1 ∩ · · · ∩ FK .,4.4 Past Work: Implicit Intersection,[0],[0]
"Unfortunately, the intersection of WFSAs takes the Cartesian product of their state sets.",4.4 Past Work: Implicit Intersection,[0],[0]
"Thus materializing this intersection would have taken time exponential in K.
To put this another way, inference is NP-hard even on a “trivial” factor graph: a single variable X1 attached to K factors.",4.4 Past Work: Implicit Intersection,[0],[0]
Recall from section 2.3 that BP would solve this via the expensive intersection above.,4.4 Past Work: Implicit Intersection,[0],[0]
Paul and Eisner (2012) instead applied DD with one subproblem per factor.,4.4 Past Work: Implicit Intersection,[0],[0]
"We generalize their method to handle arbitrary factor graphs, with multiple latent variables and cycles.",4.4 Past Work: Implicit Intersection,[0],[0]
We also explored a possible speedup for our algorithm.,4.5 Block Coordinate Update,[0],[0]
We used a block coordinate update variant of the algorithm when performing inference on the phonology problem and observed an empirical speedup.,4.5 Block Coordinate Update,[0],[0]
"Block coordinate updates are widely used in Lagrangian relaxation and have also been explored specifically for dual decomposition.
",4.5 Block Coordinate Update,[0],[0]
"In general, block algorithms minimize the objective by holding some variables fixed while updating others.",4.5 Block Coordinate Update,[0],[0]
Sontag et al. (2011) proposed a sophisticated block method called MPLP that considers all values of variable Xi instead of the ones obtained from the best assignments for the subproblems.,4.5 Block Coordinate Update,[0],[0]
"However, it is not clear how to apply their technique to string-valued variables.",4.5 Block Coordinate Update,[0],[0]
"Instead, the algorithm we propose here is much simpler—it
divides the primal variables into groups and updates each group’s associated dual variables in turn, using a single subgradient step (5).",4.5 Block Coordinate Update,[0],[0]
"Note that this way of partitioning the dual variables has the nice property that we can still use the projected subgradient update we gave in (5) and preserve the property that (∀i)∑k λki = 0.
",4.5 Block Coordinate Update,[0],[0]
"In the graphical model for generative phonology, there are two types of underlying morphemes in the first layer: word stems and word affixes.",4.5 Block Coordinate Update,[0],[0]
Our block coordinate update algorithm thus alternates between subgradient updates to the dual variables for the stems and the dual variables for the affixes.,4.5 Block Coordinate Update,[0],[0]
"Note that when performing block coordinate update on the dual variables, the primal variables are not held constant, but rather are chosen by optimizing the corresponding subproblem.",4.5 Block Coordinate Update,[0],[0]
"We compare DD to belief propagation, using the graphical model for generative phonology discussed in section 3.",5.1 Datasets,[0],[0]
Inference in this model aims to reconstruct underlying morphemes.,5.1 Datasets,[0],[0]
"Since our focus is inference, we will evaluate these reconstructions directly (whereas Cotterell et al. (2015) evaluated their ability to predict novel surface forms using the reconstructions).
",5.1 Datasets,[0],[0]
Our factor graphs have a similar topology to the pedagogical fragment shown in Figure 1.,5.1 Datasets,[0],[0]
"How-
ever, they are actually derived from datasets constructed by Cotterell et al. (2015), which are available with full descriptions at http://hubal.cs.",5.1 Datasets,[0],[0]
"jhu.edu/tacl2015/. Briefly:
EXERCISE Small datasets of Catalan, English, Maori, and Tangale, drawn from phonology textbooks.",5.1 Datasets,[0],[0]
"Each dataset contains 55 to 106 surface words, formed from a collection of 16 to 55 morphemes.",5.1 Datasets,[0],[0]
"CELEX Larger datasets of German, English, and Dutch, drawn from the CELEX database (Baayen et al., 1995).",5.1 Datasets,[0],[0]
"Each dataset contains 1000 surface words, formed from 341 to 381 underlying morphemes.",5.1 Datasets,[0],[0]
"We compared three types of inference:
DD Use DD to perform exact MAP inference.",5.2 Evaluation Scheme,[0],[0]
"SP Perform approximate marginal inference by
sum-product loopy BP with pruning (Cotterell et al., 2015).
",5.2 Evaluation Scheme,[0],[0]
MP Perform approximate MAP inference by max-product loopy BP with pruning.,5.2 Evaluation Scheme,[0],[0]
"DD and SP improve this baseline in different ways.
",5.2 Evaluation Scheme,[0],[0]
DD predicts a string value for each variable.,5.2 Evaluation Scheme,[0],[0]
"For SP and MP, we deem the prediction at a variable to be the string that is scored most highly by the belief at that variable.
",5.2 Evaluation Scheme,[0],[0]
"We report the fraction of predicted morpheme URs that exactly match the gold-standard URs proposed by a human (Cotterell et al., 2015).",5.2 Evaluation Scheme,[0],[0]
"We also compare these predicted URs to one another, to see how well the methods agree.",5.2 Evaluation Scheme,[0],[0]
The model of Cotterell et al. (2015) has two factor types whose parameters must be chosen.10 The first is a unary factor Mφ.,5.3 Parameterization,[0],[0]
"Each underlyingmorpheme variable (layer 1 of Figure 1) is connected to a copy of Mφ, which gives the prior distribution over its values.",5.3 Parameterization,[0],[0]
The second is a binary factor Sθ.,5.3 Parameterization,[0],[0]
"For each surface word (layer 3), a copy of Sθ gives its conditional distribution given the corresponding underlying word (layer 2).",5.3 Parameterization,[0],[0]
"Mφ and Sθ respectively model the lexicon and the phonology of the specific language; both are encoded as WFSMs.
",5.3 Parameterization,[0],[0]
"10The model also has a three-way factor, connecting layers 1 and 2 of Figure 1.",5.3 Parameterization,[0],[0]
"This represents deterministic concatenation (appropriate for these languages) and has no parameters.
",5.3 Parameterization,[0],[0]
"Mφ is a 0-gram generative model: at each step it emits a character chosen uniformly from the alphabet Σ with probability φ, or halts with probability 1−φ.",5.3 Parameterization,[0],[0]
"It favors shorter strings in general, but φ determines how weak this preference is.",5.3 Parameterization,[0],[0]
"Sθ is a sequential edit model that produces a word’s SR by stochastically copying, inserting, substituting, and deleting the phonemes of its UR.",5.3 Parameterization,[0],[0]
"We explore two ways of parameterizing it.
",5.3 Parameterization,[0],[0]
"Model 1 is a simple model in which θ is a scalar, specifying the probability of copying the next character of the underlying word as it is transduced to the surface word.",5.3 Parameterization,[0],[0]
"The remaining probability mass 1−θ is apportioned equally among insertion, substitution and deletion operations.11 This models phonology as “noisy concatenation”—the minimum necessary to account for the fact that surface words cannot quite be obtained as simple concatenations of their shared underlying morphemes.
",5.3 Parameterization,[0],[0]
"Model 2 is a replication of the much more complicated parametric model of Cotterell et al. (2015), which can handle linguistic phonology.",5.3 Parameterization,[0],[0]
"Here the factor Sθ is a contextual edit FST (Cotterell et al., 2014).",5.3 Parameterization,[0],[0]
The probabilities of competing edits in a given context are determined by a loglinear model with weight vector θ and features that are meant to pick up on phonological phenomena.,5.3 Parameterization,[0],[0]
"When evaluating an inference method from section 5.2, we use the same inference method both for prediction and within training.
",5.4 Training,[0],[0]
We train Model 1 by grid search.,5.4 Training,[0],[0]
"Specifically, we choose φ ∈",5.4 Training,[0],[0]
"[0.65, 1) and θ ∈ [0.25, 1) such that the predicted forms maximize the joint score (1) (always using the (max,+) semiring).
",5.4 Training,[0],[0]
"For Model 2, we compared two methods for training the φ and θ parameters (θ is a vector):
Model 2S Supervised training, which observes the “true” (hand-constructed) values of the URs.",5.4 Training,[0],[0]
This idealized setting uses the best possible parameters (trained on the test data).,5.4 Training,[0],[0]
"Model 2E Expectation maximization (EM), whose E step imputes the unobserved URs.
",5.4 Training,[0],[0]
"EM’s E step calls for exact marginal inference, which is intractable for our model.",5.4 Training,[0],[0]
"So we substitute the same inference method that we are test-
11That is, probability mass of (1− θ)/3 is divided equally among the |Σ| possible insertions; another (1 − θ)/3 is divided equally among the |Σ|−1 possible substitutions; and the final (1− θ)/3 is allocated to deletion.
",5.4 Training,[0],[0]
ing.,5.4 Training,[0],[0]
"This gives us three approximations to EM, based on DD, SP and MP.",5.4 Training,[0],[0]
"Note that DD specifically gives the Viterbi approximation to EM— which sometimes gets better results than true EM (Spitkovsky et al., 2010).",5.4 Training,[0],[0]
"For MP (but not SP), we extract only the 1-best predictions for the E step, since we study MP as an approximation to DD.
",5.4 Training,[0],[0]
"As initialization, our first E step uses the trained version of Model 1 for the same inference method.",5.4 Training,[0],[0]
We run SP and MP for 20 iterations (usually the predictions converge within 10 iterations).,5.5 Inference Details,[0],[0]
We run DD to convergence (usually< 600 iterations).,5.5 Inference Details,[0],[0]
"DD iterations are much faster since each variable considers d strings, not d distributions over strings.",5.5 Inference Details,[0],[0]
"Hence DD does not intersect distributions, and many parts of the graph settle down early because discrete values can converge in finite time.12
We follow Paul and Eisner (2012, section 5.1) fairly closely.",5.5 Inference Details,[0],[0]
"In particular: Our stepsize in (5) is η = α/(t + 500), where t is the iteration number; α = 1 for Model 2S and α = 10 otherwise.",5.5 Inference Details,[0],[0]
"We proactively include all 1-gram and 2-gram substring features in the active sets Wi at initialization, rather than adding them only as needed.",5.5 Inference Details,[0],[0]
"At iterations 200, 400, and 600, we proactively add all 3-, 4-, and 5-gram features (respectively) on which the counts still disagree; this accelerates convergence on the few variables that have not already converged.",5.5 Inference Details,[0],[0]
We handle negative-weight cycles as Paul and Eisner do.,5.5 Inference Details,[0],[0]
"If we had ever failed to converge within 2000 iterations, we would have used their heuristic to extract a prediction anyway.
",5.5 Inference Details,[0],[0]
Model 1 suffers from a symmetry-breaking problem.,5.5 Inference Details,[0],[0]
"Many edits have identical probability, and when we run inference, many assignments will tie for highest scoring configuration.",5.5 Inference Details,[0],[0]
This can prevent DD from converging and makes performance hard to measure.,5.5 Inference Details,[0],[0]
"To break these ties, we add “jitter” separately to each copy of Mφ in Figure 1.",5.5 Inference Details,[0],[0]
"Specifically, if Fi is the unary factor attached to Xi, we expand our 0-gram model Fi(x) = log((p/|Σ|)|x| · (1 − p)) to become Fi(x) = log( ∏ c∈Σ p |x|c c,i · (1 − p)), where |x|c denotes the count of character c in string x, and pc,",5.5 Inference Details,[0],[0]
i ∝,5.5 Inference Details,[0],[0]
(p/|Σ|) ·,5.5 Inference Details,[0],[0]
"exp εc,i where εc,i ∼ N(0, 0.01) and we preserve ∑ c∈Σ pc,i = p.
12A variable need not update λ if its strings agree; a subproblem is not re-solved if none of its variables updated λ.",5.5 Inference Details,[0],[0]
"As linguists know, reconstructing an underlying stem or suffix can be difficult.",6.1 Convergence and Speed of DD,[0],[0]
We may face insufficient evidence or linguistic irregularity—or regularity that goes unrecognized because the phonological model is impoverished (Model 1) or poorly trained (early EM iterations on Model 2).,6.1 Convergence and Speed of DD,[0],[0]
DD may then require extensive negotiation to resolve disagreements among subproblems.,6.1 Convergence and Speed of DD,[0],[0]
"Furthermore, DD must renegotiate as conditions change elsewhere in the factor graph (Table 1).
",6.1 Convergence and Speed of DD,[0],[0]
DD converged in all of our experiments.,6.1 Convergence and Speed of DD,[0],[0]
Note that DD (section 4.3) has converged when all the equality constraints in (2) are satisfied.,6.1 Convergence and Speed of DD,[0],[0]
"In this case, we have found the true MAP configuration.
",6.1 Convergence and Speed of DD,[0],[0]
"In section 4.5, we discussed a block coordinate update variation (BCDD) of our DD algorithm.",6.1 Convergence and Speed of DD,[0],[0]
Figure 3 shows the convergence behavior of BCDD against the naive projected subgradient algorithm (NVDD) on the four EXERCISE languages under Model 1.,6.1 Convergence and Speed of DD,[0],[0]
"The dual objective (3) always upper-bounds the primal score (i.e., the score (1) of an assignment derived heuristically from the current subproblem solutions).",6.1 Convergence and Speed of DD,[0],[0]
The dual decreases as the algorithm progresses.,6.1 Convergence and Speed of DD,[0],[0]
"When the two objectives meet, we have found an optimal solution to the primal problem.",6.1 Convergence and Speed of DD,[0],[0]
We can see in Figure 3 that our DD algorithm converges quickly on the four EXERCISE languages and BCDD converges consistently faster than NVDD.,6.1 Convergence and Speed of DD,[0],[0]
"We use BCDD in the remaining experiments.
",6.1 Convergence and Speed of DD,[0],[0]
"When DD runs fast, it is competitive with the
other methods.",6.1 Convergence and Speed of DD,[0],[0]
"It is typically faster on the EXERCISE data, and a few times slower on the CELEX data.",6.1 Convergence and Speed of DD,[0],[0]
"But we stop the other methods after 20 iterations, whereas DD runs until it gets an exact answer.",6.1 Convergence and Speed of DD,[0],[0]
We find that this runtime is unpredictable and sometimes quite long.,6.1 Convergence and Speed of DD,[0],[0]
"In the grid search for training Model 1, we observed that changes in the parameters (φ, θ) could cause the runtime of DD inference to vary by 2 orders of magnitude.",6.1 Convergence and Speed of DD,[0],[0]
"Similarly, on the CELEX data, the runtime on Model 1 (over 10 different N = 600 subsets of English) varied from about 1 hour to nearly 2 days.13",6.1 Convergence and Speed of DD,[0],[0]
"For each language, we constructed several different unsupervised prediction problems.",6.2 Comparison of Inference,[0],[0]
"In each problem, we observe some size-N subset of the words in our dataset, and we attempt to predict the URs of the morphemes in those words.",6.2 Comparison of Inference,[0],[0]
"For each CELEX language, we took N = 600, and used three of the size-N training sets from (Cotterell et al., 2015).",6.2 Comparison of Inference,[0],[0]
"For each EXERCISE language, we took N to be one less than the dataset size, and used all N + 1 subsets of size N , again similar to (Cotterell et al., 2015).",6.2 Comparison of Inference,[0],[0]
"We report the unweighted macro-average of all these accuracy numbers.
13Note that our implementation is not optimized; e.g., it uses Python (not Cython).
",6.2 Comparison of Inference,[0],[0]
"We compare DD, SP, and MP inference on each language under different settings.",6.2 Comparison of Inference,[0],[0]
"Table 2 shows aggregate results, as an unweighted average over multiple languages and training sets.",6.2 Comparison of Inference,[0],[0]
We present various additional results at http://cs.,6.2 Comparison of Inference,[0],[0]
"jhu.edu/˜npeng/emnlp2015/, including a perlanguage breakdown of the results, runtime numbers, and significance tests.
",6.2 Comparison of Inference,[0],[0]
The results for Model 1 are shown in Tables 2a and 2b.,6.2 Comparison of Inference,[0],[0]
"As we can see, in both datasets, dual decomposition performed the best at recovering the URs, while MP performed the worst.",6.2 Comparison of Inference,[0],[0]
"Both DD and MP are doing MAP inference, so the differences reflect the search error in MP.",6.2 Comparison of Inference,[0],[0]
"Interestingly, DD agrees more with SP than with MP, even though SP uses marginal inference.
",6.2 Comparison of Inference,[0],[0]
"Although the aggregate results on the EXERCISE dataset show a large improvement of DD over both of the BP algorithms, the gain all comes from the English language.",6.2 Comparison of Inference,[0],[0]
"SP actually does better than DD on Catalan and Maori, and MP also gets better results than DD on Maori, tying with SP.
",6.2 Comparison of Inference,[0],[0]
"For Model 2S, all inference methods achieved 100% accuracy on the EXERCISE dataset, so we do not show a table.",6.2 Comparison of Inference,[0],[0]
The results on the CELEX dataset are shown in Table 2c.,6.2 Comparison of Inference,[0],[0]
"Here both DD and MP performed equally well, and outperformed BP—a result like (Spitkovsky et al., 2010).",6.2 Comparison of Inference,[0],[0]
This trend is consistent over all three languages: DD and MP always achieve similar results and both outperform SP.,6.2 Comparison of Inference,[0],[0]
"Of course, one advantage of DD in the setting is that it actually finds the true MAP prediction of the model; the errors are known to be due to the model, not the search procedure.
",6.2 Comparison of Inference,[0],[0]
"For Model 2E, we show results on the EXERCISE dataset in Table 2d.",6.2 Comparison of Inference,[0],[0]
Here the results resemble the pattern of Model 1.,6.2 Comparison of Inference,[0],[0]
"We presented a general dual decomposition algorithm for MAP inference on graphical models over strings, and applied it to an unsupervised learning task in phonology.",7 Conclusion and Future Work,[0],[0]
"The experiments show that our DD algorithm converges and gets better results than both max-product and sum-product BP.
Techniques should be explored to speed up the DD method.",7 Conclusion and Future Work,[0],[0]
"Adapting the MPLP algorithm (Sontag et al., 2011) to the string-valued case would be a nontrivial extension.",7 Conclusion and Future Work,[0],[0]
"We could also explore other serial update schemes, which generally speed up message-passing algorithms over parallel update.",7 Conclusion and Future Work,[0],[0]
We investigate dual decomposition for joint MAP inference of many strings.,abstractText,[0],[0]
"Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming.",abstractText,[0],[0]
"We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.",abstractText,[0],[0]
"This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length.",abstractText,[0],[0]
"Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general).",abstractText,[0],[0]
"On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.",abstractText,[0],[0]
Dual Decomposition Inference for Graphical Models over Strings,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4725–4730 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4725
In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE. The main idea behind dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either small or large values for the single forgetting factor in the original FOFE. In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs. Our results on the challenging Google Billion Words corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs. Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",text,[0],[0]
"Language modelling is an essential task for many natural language processing (NLP) applications including speech recognition, machine translation and text summarization.",1 Introduction,[0],[0]
"The goal of language modelling is to learn the distribution over a sequence of characters or words; this distribution may be utilized for encoding the language structure (e.g. the grammatical structure) as well as extracting information from the corpora (Jozefowicz et al., 2016).",1 Introduction,[0],[0]
"In the recent years, the popularity of neural networks (NN) has been a significant driving force for language modelling (LM) research; the well-known NN-LM models includes the feedforward NN-LMs (FNN-LMs) (Bengio et al., 2001, 2003), recurrent NN-LMs (RNN-LMs) (Mikolov et al., 2010; Mikolov
∗Equal contribution.
",1 Introduction,[0],[0]
"and Zweig, 2012) and the long short-term memory (LSTM-LMs) (Hochreiter and Schmidhuber, 1997).",1 Introduction,[0],[0]
"Among all, FNN-LMs often have a simpler and more efficient learning process, but they tend to underperform the other NN-LMs due to the limited capability to memorize the long term dependency in natural languages (Zhang et al., 2015b).",1 Introduction,[0],[0]
However this drawback could be addressed by applying the fixed-size ordinally-forgetting encoding (FOFE) to FNN’s inputs.,1 Introduction,[0],[0]
"FOFE is an encoding method, which relies on the ordinally-forgetting mechanism to encode any word sequence based on the positions of words; this also allows the FOFE code to capture the long-term dependency (Zhang et al., 2015b).",1 Introduction,[0],[0]
"As shown in Zhang (2015b), FNNLMs with FOFE can easily yield comparable performance as other NN-LMs.",1 Introduction,[0],[0]
"The key parameter in the FOFE method is the forgetting factor, which is responsible for determining the degree of sensitivity of the encoding with respect to the past context.",1 Introduction,[0],[0]
"However, the choice of a good value for the forgetting factor could be tricky since both small and large forgetting factors are offering different benefits.
",1 Introduction,[0],[0]
"In this paper, we propose a simple alteration to FOFE method, which allows to incorporate two forgetting factors into the fixed-size encoding of the variable-length word sequences.",1 Introduction,[0],[0]
We name this approach as dual-FOFE.,1 Introduction,[0],[0]
"Our hypothesis is that by incorporating both the small and large forgetting factors in the FOFE encoding, the dual-FOFE is able to simultaneously optimize the abilities to capture the positional information as well as to model long term dependency.",1 Introduction,[0],[0]
"In our experiments, we have evaluated the proposed dual FOFE models on two large scale language modeling tasks, namely enwiki9 and Google Billion Words (GBW) corpora.",1 Introduction,[0],[0]
"Experimental results have shown that both FOFE models yield very competitive performance on these tasks, comparable with the state-
of-the-art systems but with significantly reduced learning complexity.",1 Introduction,[0],[0]
"Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",1 Introduction,[0],[0]
"In this section, we will briefly review the NN-LMs and the original FOFE method.",2 Related Work,[0],[0]
"The general idea behind NN-LM is to project the discrete words onto a continuous space, then learn to estimate the conditional probabilities of each known word within the projected space.",2 Related Work,[0],[0]
The training of NNLMs are often incredibly slow due to the inefficiency of softmax normalization when applied to the extremely large output layer.,2 Related Work,[0],[0]
"The solution currently used by many NN-LMs (including our models in this work) is to use noise contrastive estimation (NCE) (Gutmann and Hyvrinen, 2010).",2 Related Work,[0],[0]
"The basic idea of NCE is to reduce the probability estimation problem into a probabilistic binary classification problem (Mnih and Teh, 2012; Mnih and Kavukcuoglu, 2013).",2 Related Work,[0],[0]
"Fixed-size ordinally-forgetting Encoding (FOFE) is an encoding method which generates a fixedsize representation, namely the FOFE code, for any variable-length word sequence (Zhang et al., 2015b).",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"For a given word sequence S = {w1, w2, ..., wT }, let et denote the one-hot representation of the word wt, zt for the FOFE code of the partial word sequence up to word wt, zt is computed as follows:
zt = α · zt−1 + et (1 ≤ t ≤ T ) (1)
where α (0< α < 1) denotes the forgetting factor, a parameter responsible for determining the degree of influence each time step of the past context has on the FOFE code.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Obviously, FOFE can convert any variable-length sequence into a fixed-size code with length equal to the size of vocabulary.
",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"In regard to uniqueness of FOFE code, the code is said to be (almost) unique under the two theorems (proven in Zhang (2015a)):",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
Theorem 1,2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"If 0 < α ≤ 0.5, then FOFE code is guarantee uniqueness for any values of vocabulary’s size and sequence’s length.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
Theorem 2,2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"If 0.5 < α < 1, then FOFE code is guarantee almost uniqueness for any finite values
of vocabulary’s size and sequence’s length, except for a finite set of countable choices of α.
",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Furthermore, the chance of actually having any collisions for α between 0.5 and 1 is nearly impossible in practice, due to quantization errors in real computer systems.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"Hence in practice, it is safe to argue that FOFE is able to uniquely encodes any variable-length sequence into a fixed-size representation.",2.1 Fixed-Size Ordinally Forgetting Encoding,[0],[0]
"The idea of FOFE based NN-LMs is to use FOFE to encode the partial history sequence of past words in a sentence, then feed this fixed-size FOFE code to a feedforward neural network as an input to predict next word.",2.2 FOFE for FNN-LMs,[0],[0]
"As shown in Figure 1, the FOFE code could be efficiently computed via time-delayed recursive structure, where the symbol z−1 in the figure represents a unit time delay (or equivalently a memory unit) from zt to zt−1.
",2.2 FOFE for FNN-LMs,[0],[0]
The basic architecture for FOFE based FNNLMs (called 1st-order) is the standard FNN architecture with an additional layer for encoding the input into FOFE code.,2.2 FOFE for FNN-LMs,[0],[0]
"However, in this work, we use the 2nd-order and 3rd-order FOFE FNNLMs, which are shown to produce slightly better results (Zhang et al., 2015b).",2.2 FOFE for FNN-LMs,[0],[0]
"In a 2nd-order FOFE model, both the current partial sequences FOFE code (denoted as zt) and the previous partial sequences FOFE code (denoted as zt−1) are utilized to predict next word.",2.2 FOFE for FNN-LMs,[0],[0]
"In a 3rd-order FOFE model, all zt, zt−1 and zt−2 are used as inputs to neural networks.
",2.2 FOFE for FNN-LMs,[0],[0]
"More recently, the FOFE methods have been successfully applied to many NLP tasks, including word embedding (Sanu et al., 2017), named entity recognition (Xu et al., 2017a), entity discovery and linking (Xu et al., 2016, 2017b).",2.2 FOFE for FNN-LMs,[0],[0]
The main idea of dual-FOFE is to generate augmented FOFE encoding codes by concatenating two FOFE codes using two different forgetting factors.,3 Dual-FOFE,[0],[0]
Each of these FOFE codes is still computed in the same way as the mathematical formulation shown in Equation (1).,3 Dual-FOFE,[0],[0]
The difference between them is that we may select to use two different values for the forgetting factor (denoted as α) for additional modeling benefits.,3 Dual-FOFE,[0],[0]
"As mentioned in the subsection 2.1, the values in a FOFE code are used to encode both the content and the order information in a sequence.",3.1 Intuition behind Dual-FOFE,[0],[0]
This is achieved by a recursive encoding method where at each recursive step the code will be multiplied by the forgetting factor (α) whose value is bounded by 0 < α < 1.,3.1 Intuition behind Dual-FOFE,[0],[0]
"In a practical computer with finite precision, this has an impact on the FOFE’s abilities to precisely memorize the long-term dependency of past context as well as to properly represent the positional information.
",3.1 Intuition behind Dual-FOFE,[0],[0]
The FOFE’s ability to represent the positional information would improve with smaller forgetting factors.,3.1 Intuition behind Dual-FOFE,[0],[0]
"The reason is that that when α is small, the FOFE code (zt) for each word vastly differs from its neighbour in magnitude.",3.1 Intuition behind Dual-FOFE,[0],[0]
"If α is too large (close to 1), the contribution of a word may not change too much no matter where it is.",3.1 Intuition behind Dual-FOFE,[0],[0]
This may hamper the following neural networks to model the positional information.,3.1 Intuition behind Dual-FOFE,[0],[0]
"Conversely, the FOFE’s ability to model the long-term dependency of the older context would improve with larger forgetting factors.",3.1 Intuition behind Dual-FOFE,[0],[0]
"This is because when α is small, the contribution of a word from the older history may quickly underflow to become irrelevant (i.e. forgotten) when computing the current word.
",3.1 Intuition behind Dual-FOFE,[0],[0]
"In the original FOFE with just a single forgetting factor, we would have to determine the best trade-off between these two benefits.",3.1 Intuition behind Dual-FOFE,[0],[0]
"On the other hand, the dual-FOFE does not face such issues since it is composed of two FOFE codes: the half of the dual-FOFE code using a smaller forgetting factor is solely optimized and responsible for representing the positional information of all words in the sequence; meanwhile the other half of the dual-FOFE code using a larger forgetting factor is optimized and responsible for maintaining the long-term dependency of past context.",3.1 Intuition behind Dual-FOFE,[0],[0]
"As shown in Figure 2, the architecture of dualFOFE based FNN-LMs is very similar to the original FOFE FNN-LMs.1 In the Dual-FOFE FNNLMs, the input word sequence would have to pass through two branches of the FOFE layers (using two different forgetting factors) and each encoding branch will produce a FOFE code representing the input sequence.",3.2 Dual-FOFE based FNN-LM,[0],[0]
"These two FOFE codes are then joined to produce the dual-FOFE code, which would be fed to FNNs to predict the next word.
",3.2 Dual-FOFE based FNN-LM,[0],[0]
"It might also be worth noting that in our implementation we do not explicitly reset FOFE codes, i.e. zt value, at sentence boundaries.",3.2 Dual-FOFE based FNN-LM,[0],[0]
"However, faraway histories will be gradually forgotten by the recursive calculation in FOFE due to 0 <",3.2 Dual-FOFE based FNN-LM,[0],[0]
α < 1 and finite precision in computers.,3.2 Dual-FOFE based FNN-LM,[0],[0]
"As mentioned previously in the subsection 2.2, the higer order FOFE codes would utilize both the current and the previous sequence FOFE codes for prediction.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"Hence similar to dual-FOFE, the higher order FOFE could also maintain the sensitivity to both nearby and faraway context.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
Obviously a much higher order FOFE code may be required in order to achieve the same effect as dual-FOFE in terms of modelling long-term dependency.,3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"In this case, the higher order FOFE may also significantly increase the number of parameters in the input layer.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"At last, the dual FOFE
1The difference in the location of the projection layer between Figure 1 and 2 simply indicates two equivalent ways to do word projection.",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"Figure 1 was originally from Zhang (2015b), but they mentioned in text (without a figure) that it is more efficient to do projection as in Figure 2 and both methods are mathematically equivalent since both projection and FOFE steps are linear.
",3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
and the higher order FOFE are largely complementary since we have observed consistent performance gains when combining dual FOFE with either 2nd-order or 3rd-order FOFE in our experiments.,3.3 Dual-FOFE vs. Higher Order FOFE,[0],[0]
"In this work, we have evaluated the proposed dual-FOFE based FNN-LMs against various traditional neural language models on two corpora: i) enwik9 corpus: it consists of the first 1 billion bytes of English wikipedia dump, having total size of 170.8 million words; the corpus was divided into three parts: the training set (153M words), the test set (8.9M words), and the validation set (8.9M words); the vocabulary size is limited to 80k words (Zhang et al., 2015b).",4 Experiments,[0],[0]
"ii) Google Billion Words (GBW) corpus: it contains about 800 million words and the corpus is divided into two parts: the training set (792M words) and the test set (8M words); the vocabulary size for this corpus is limited to 800k words (Chelba et al., 2013).",4 Experiments,[0],[0]
"In the experiments on the enwiki9 corpus, we have trained three dual-FOFE FNN-LMs with different forgetting factor pairs, one FOFE FNN-LM, and one tri-FOFE FNN-LM.",4.1 Results on enwiki9,[0],[0]
"All five models adopt a 2nd-order FOFE structure, employing a word embeddings of 256 dimensions, three hidden layers of 400, 600, 600 neurons and an output layer of 80k words (reflecting the vocabulary).",4.1 Results on enwiki9,[0],[0]
2 Note that the dual-FOFE FNN-LMs have to double the size of input context windows since dual-FOFE essentially contain two FOFE codes.,4.1 Results on enwiki9,[0],[0]
"But this increase only accounts for a negligible faction of total model parameters.
",4.1 Results on enwiki9,[0],[0]
"As shown in Table 1, all three dual-FOFE FNNLMs, using three pairs of forgetting factors as (0.5, 0.7) and (0.7, 0.9) and (0.5, 0.9), can significantly outperform other traditional models previously reported on this corpus.",4.1 Results on enwiki9,[0],[0]
"We also note that it is beneficial to include a relatively large forgetting factor, such as 0.9, in the dual FOFE models since such a large alpha may help to memorize much longer context in the inputs.",4.1 Results on enwiki9,[0],[0]
"When compared with the original FOFE counterpart, the best dual-FOFE model using forgetting factors (0.5, 0.9) offers a relative gain of around 8% in test PPL.
2Comparing with Zhang (2015b), our single FOFE FNNLM baseline use a slightly larger model, which lead to slightly better perplexity.
",4.1 Results on enwiki9,[0],[0]
It is worth noting that our dual-FOFE models can be extended to incorporate more than two alpha values.,4.1 Results on enwiki9,[0],[0]
"In fact after we have obtained a strong result supporting our dual-FOFE hypothesis, we have performed additional experiments using three alpha values, the so-called tri-FOFE model.",4.1 Results on enwiki9,[0],[0]
The result on Table 1 has shown that the tri-FOFE FNN-LMs still slightly outperforms the dual-FOFE models.,4.1 Results on enwiki9,[0],[0]
"However, the gain is marginal.",4.1 Results on enwiki9,[0],[0]
This leads us to believing that further extension of more alpha values in FOFE would be of limited use.,4.1 Results on enwiki9,[0],[0]
"In the experiments on the GBW corpus, we have trained one dual-FOFE FNN-LM and one FOFE FNN-LM.",4.2 Results on Google Billion Words (GBW),[0],[0]
"Following the best dual-FOFE model configuration on the previous corpus, this dualFOFE FNN-LM uses the same pair of dual forgetting factors (0.5, 0.9).",4.2 Results on Google Billion Words (GBW),[0],[0]
"Both models adopt a 3rd-order structure, employing word embeddings of 256 dimensions, three hidden layers each of 4096 neurons, a compression layer with 720 neurons, and an output layer of 800k words (reflecting the vocabulary).",4.2 Results on Google Billion Words (GBW),[0],[0]
"Although dual-FOFE FNNLM has doubled the size of input context windows of FOFE FNN-LM, the total number of model parameters in both models are almost equal, roughly 0.82 billion parameters.
",4.2 Results on Google Billion Words (GBW),[0],[0]
"As shown in Table 2, the dual-FOFE FNN-LM is able to produce a very competitive performance, comparable with the best previously reported results on this task, such as GCNN-13 (Dauphin et al., 2016) and LSTM-LM (Jozefowicz et al., 2016).",4.2 Results on Google Billion Words (GBW),[0],[0]
The dual-FOFE FNN-LM are among the few single-model systems that are able to achieve test PPL below 40 on this task.,4.2 Results on Google Billion Words (GBW),[0],[0]
"Furthermore, our proposed dual FOFE model can significantly reduce the computational complexity, e.g., our model has a relatively smaller number of parameter (0.82B parameters) and it requires much less hardware resource to train (using only 1 GPU in our experiments).",4.2 Results on Google Billion Words (GBW),[0],[0]
"When compared with the original FOFE counterpart, the dual-FOFE FNN-LM is able to provide approximately 11% relative improvement in PPL.",4.2 Results on Google Billion Words (GBW),[0],[0]
"In this paper, we have proposed a new approach of utilizing the fixed-size ordinally-forgetting encoding (FOFE) method for neural network lan-
guage models (NN-LMs), known as dual-FOFE.",5 Conclusions,[0],[0]
"As the name implies, this approach involves to produce a new fixed-sized representation for any variable-length sequence from a concatenation of two FOFE codes.",5 Conclusions,[0],[0]
This would have allowed us to select two values for the forgetting factors.,5 Conclusions,[0],[0]
One FOFE code with a smaller forgetting factor is responsible for representing the positional information of all words in the sequence while the other using a larger forgetting factor is responsible for modelling the even longer term dependency in far away history.,5 Conclusions,[0],[0]
Our experiments on both enwiki9 and Google Billion Words (GBW) tasks have both demonstrated the effectiveness of the dual-FOFE modeling approach.,5 Conclusions,[0],[0]
"Experimental results on the challenging GBW corpus have shown that the dual-FOFE FNN-LM has achieved over 10% improvement in perplexity over the original FOFE FNN-LM, without any significant drawback in model and learning complexity.",5 Conclusions,[0],[0]
"When compared with other traditional neural language models, the dual-FOFE FNN-LM has achieved com-
petitive performance with significantly lower computational complexity.",5 Conclusions,[0],[0]
"This work is supported mainly by a research donation from iFLYTEK Co., Ltd., Hefei, China, and partially by a discovery grant from Natural Sciences and Engineering Research Council (NSERC) of Canada.",Acknowledgement,[0],[0]
"In this paper, we propose a new approach to employ the fixed-size ordinally-forgetting encoding (FOFE) (Zhang et al., 2015b) in neural languages modelling, called dual-FOFE.",abstractText,[0],[0]
The main idea behind dual-FOFE is that it allows to use two different forgetting factors so that it can avoid the trade-off in choosing either small or large values for the single forgetting factor in the original FOFE.,abstractText,[0],[0]
"In our experiments, we have compared the dual-FOFE based neural network language models (NNLM) against the original FOFE counterparts and various traditional NNLMs.",abstractText,[0],[0]
Our results on the challenging Google Billion Words corpus show that both FOFE and dual FOFE yield very strong performance while significantly reducing the computational complexity over other NNLMs.,abstractText,[0],[0]
"Furthermore, the proposed dual-FOFE method further gives over 10% relative improvement in perplexity over the original FOFE model.",abstractText,[0],[0]
Dual Fixed-Size Ordinally Forgetting Encoding (FOFE) for Competitive Neural Language Models,title,[0],[0]
"Sparse learning has emerged as an effective approach to alleviate model overfitting when feature dimension
1Department of CS, Rutgers University, Piscataway, NJ, 08854, USA.",1. Introduction,[0],[0]
"2B-DAT Lab, Nanjing University of Information Science & Technology, Nanjing, Jiangsu, 210044, China.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Bo Liu <lb507@cs.rutgers.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
outnumbers training sample.,1. Introduction,[0],[0]
"Given a set of training samples{(xi, yi)}Ni=1 in which xi ∈ Rd is the feature representation and yi ∈ R the corresponding label, the following sparsity-constrained `2-norm regularized loss minimization problem is often considered in high-dimensional analysis:
min ‖w‖0≤k
P (w) := 1
N N∑ i=1",1. Introduction,[0],[0]
"l(w>xi, yi) +",1. Introduction,[0],[0]
λ 2 ‖w‖2.,1. Introduction,[0],[0]
"(1)
Here l(·; ·) is a convex loss function, w ∈ Rd is the model parameter vector and λ controls the regularization strength.",1. Introduction,[0],[0]
"For example, the squared loss l(a, b) =",1. Introduction,[0],[0]
"(b − a)2 is used in linear regression and the hinge loss l(a, b) = max{0, 1− ab} in support vector machines.",1. Introduction,[0],[0]
"Due to the presence of cardinality constraint ‖w‖0 ≤ k, the problem (1) is simultaneously non-convex and NP-hard in general, and thus is challenging for optimization.",1. Introduction,[0],[0]
"A popular way to address this challenge is to use proper convex relaxation, e.g., `1 norm (Tibshirani, 1996) and k-support norm (Argyriou et al., 2012), as an alternative of the cardinality constraint.",1. Introduction,[0],[0]
"However, the convex relaxation based techniques tend to introduce bias for parameter estimation.
",1. Introduction,[0],[0]
"In this paper, we are interested in algorithms that directly minimize the non-convex formulation in (1).",1. Introduction,[0],[0]
"Early efforts mainly lie in compressed sensing for signal recovery, which is a special case of (1) with squared loss.",1. Introduction,[0],[0]
"Among others, a family of the so called Iterative Hard Thresholding (IHT) methods (Blumensath & Davies, 2009; Foucart, 2011) have gained significant interests and they have been witnessed to offer the fastest and most scalable solutions in many cases.",1. Introduction,[0],[0]
"More recently, IHT-style methods have been generalized to handle generic convex loss functions (Beck & Eldar, 2013; Yuan et al., 2014; Jain et al., 2014) as well as structured sparsity constraints (Jain et al., 2016).",1. Introduction,[0],[0]
"The common theme of these methods is to iterate between gradient descent and hard thresholding to maintain sparsity of solution while minimizing the objective value.
",1. Introduction,[0],[0]
"Although IHT-style methods have been extensively studied, the state-of-the-art is only designed for the primal formulation (1).",1. Introduction,[0],[0]
"It remains an open problem to investigate the feasibility of solving the original NP-hard/non-convex formulation in a dual space that might potentially further im-
prove computational efficiency.",1. Introduction,[0],[0]
"To fill this gap, inspired by the recent success of dual methods in regularized learning problems, we systematically build a sparse duality theory and propose an IHT-style algorithm along with its stochastic variant for dual optimization.
",1. Introduction,[0],[0]
Overview of our contribution.,1. Introduction,[0],[0]
The core contribution of this work is two-fold in theory and algorithm.,1. Introduction,[0],[0]
"As the theoretical contribution, we have established a novel sparse Lagrangian duality theory for the NP-hard/non-convex problem (1) which to the best of our knowledge has not been reported elsewhere in literature.",1. Introduction,[0],[0]
We provide in this part a set of sufficient and necessary conditions under which one can safely solve the original non-convex problem through maximizing its concave dual objective function.,1. Introduction,[0],[0]
"As the algorithmic contribution, we propose the dual IHT (DIHT) algorithm as a super-gradient method to maximize the nonsmooth dual objective.",1. Introduction,[0],[0]
"In high level description, DIHT iterates between dual gradient ascent and primal hard thresholding pursuit until convergence.",1. Introduction,[0],[0]
A stochastic variant of DIHT is proposed to handle large-scale learning problems.,1. Introduction,[0],[0]
"For both algorithms, we provide non-asymptotic convergence analysis on parameter estimation error, sparsity recovery, and primal-dual gap as well.",1. Introduction,[0],[0]
"In sharp contrast to the existing analysis for primal IHT-style algorithms, our analysis is not relying on Restricted Isometry Property (RIP) conditions and thus is less restrictive in real-life highdimensional statistical settings.",1. Introduction,[0],[0]
Numerical results on synthetic datasets and machine learning benchmark datasets demonstrate that dual IHT significantly outperforms the state-of-the-art primal IHT algorithms in accuracy and efficiency.,1. Introduction,[0],[0]
"The theoretical and algorithmic contributions of this paper are highlighted in below:
• Sparse Lagrangian duality theory: we established a sparse saddle point theorem (Theorem 1), a sparse mini-max theorem (Theorem 2) and a sparse strong duality theorem (Theorem 3).
",1. Introduction,[0],[0]
• Dual optimization: we proposed an IHT-style algorithm along with its stochastic extension for nonsmooth dual maximization.,1. Introduction,[0],[0]
"These algorithms have been shown to converge at the rate of 1 ln 1 in dual
parameter estimation error and 1 2 ln 1 2 in primal-dual gap (see Theorem 4 and Theorem 5).",1. Introduction,[0],[0]
"These guarantees are invariant to RIP conditions which are required by virtually all the primal IHT-style methods without using relaxed sparsity levels.
",1. Introduction,[0],[0]
Notation.,1. Introduction,[0],[0]
"Before continuing, we define some notations to be used.",1. Introduction,[0],[0]
Let x ∈ Rd be a vector and F be an index set.,1. Introduction,[0],[0]
We use HF (x) to denote the truncation operator that restricts x to the set F . Hk(x) is a truncation operator which preserves the top k (in magnitude) entries of x and sets the remaining to be zero.,1. Introduction,[0],[0]
"The notation supp(x) represents the index set
of nonzero entries of x. We conventionally define ‖x‖∞ = maxi |[x]i| and define xmin = mini∈supp(x) |[x]i|.",1. Introduction,[0],[0]
"For a matrixA, σmax(A) (σmin(A)) denotes its largest (smallest) singular value.
Organization.",1. Introduction,[0],[0]
The rest of this paper is organized as follows: In §2 we briefly review some relevant work.,1. Introduction,[0],[0]
In §3 we develop a Lagrangian duality theory for sparsityconstrained minimization problems.,1. Introduction,[0],[0]
The dual IHT-style algorithms along with convergence analysis are presented in §4.,1. Introduction,[0],[0]
The numerical evaluation results are reported in §5.1.,1. Introduction,[0],[0]
"Finally, the concluding remarks are made in §6.",1. Introduction,[0],[0]
All the technical proofs are deferred to the appendix sections.,1. Introduction,[0],[0]
"For generic convex objective beyond quadratic loss, the rate of convergence and parameter estimation error of IHT-style methods were established under proper RIP (or restricted strong condition number) bound conditions (Blumensath, 2013; Yuan et al., 2014; 2016).",2. Related Work,[0],[0]
"In (Jain et al., 2014), several relaxed variants of IHT-style algorithms were presented for which the estimation consistency can be established without requiring the RIP conditions.",2. Related Work,[0],[0]
"In (Bahmani et al., 2013), a gradient support pursuit algorithm is proposed and analyzed.",2. Related Work,[0],[0]
"In large-scale settings where a full gradient evaluation on all data becomes a bottleneck, stochastic and variance reduction techniques have been adopted to improve the computational efficiency of IHT (Nguyen et al., 2014; Li et al., 2016; Chen & Gu, 2016).
",2. Related Work,[0],[0]
"Dual optimization algorithms have been widely used in various learning tasks including SVMs (Hsieh et al., 2008) and multi-task learning (Lapin et al., 2014).",2. Related Work,[0],[0]
"In recent years, stochastic dual optimization methods have gained significant attention in large-scale machine learning (ShalevShwartz & Zhang, 2013a;b).",2. Related Work,[0],[0]
"To further improve computational efficiency, some primal-dual methods are developed to alternately minimize the primal objective and maximize the dual objective.",2. Related Work,[0],[0]
"The successful examples of primal-dual methods include learning total variation regularized model (Chambolle & Pock, 2011) and generalized Dantzig selector (Lee et al., 2016).",2. Related Work,[0],[0]
"More recently, a number of stochastic variants (Zhang & Xiao, 2015; Yu et al., 2015) and parallel variants (Zhu & Storkey, 2016) were developed to make the primal-dual algorithms more scalable and efficient.",2. Related Work,[0],[0]
"In this section, we establish weak and strong duality theory that guarantees the original non-convex and NP-hard problem in (1) can be equivalently solved in a dual space.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The results in this part build the theoretical foundation of developing dual IHT methods.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"From now on we abbreviate li(w>xi) = l(w>xi, yi).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The convexity of l(w>xi, yi) implies that li(u) is also convex.",3. A Sparse Lagrangian Duality Theory,[0],[0]
Let l∗i (αi) = maxu {αiu − li(u)} be the convex conjugate of li(u) and F ⊆ R be the feasible set of αi.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"According to the well-known expression of li(u) = maxαi∈F {αiu− l∗i (αi)}, the problem (1) can be reformulated into the following mini-max formulation:
min ‖w‖0≤k
1
N N∑ i=1",3. A Sparse Lagrangian Duality Theory,[0],[0]
max αi∈F {αiw>xi − l∗i (αi)}+ λ 2 ‖w‖2.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"(2)
The following Lagrangian form will be useful in analysis:
L(w,α) = 1
N N∑ i=1",3. A Sparse Lagrangian Duality Theory,[0],[0]
( αiw >xi − l∗i (αi) ),3. A Sparse Lagrangian Duality Theory,[0],[0]
"+ λ 2 ‖w‖2,
where α =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[α1, ..., αN ]",3. A Sparse Lagrangian Duality Theory,[0],[0]
∈ FN is the vector of dual variables.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"We now introduce the following concept of sparse saddle point which is a restriction of the conventional saddle point to the setting of sparse optimization.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Definition 1 (Sparse Saddle Point).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A pair (w̄, ᾱ) ∈",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Rd × FN is said to be a k-sparse saddle point for L if ‖w̄‖0 ≤ k and the following holds for all ‖w‖0 ≤ k, α ∈ FN :
L(w̄, α) ≤",3. A Sparse Lagrangian Duality Theory,[0],[0]
"L(w̄, ᾱ) ≤ L(w, ᾱ).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(3)
Different from the conventional definition of saddle point, the k-sparse saddle point only requires the inequality (3) holds for any arbitrary k-sparse vector w. The following result is a basic sparse saddle point theorem for L. Throughout the paper, we will use f ′(·) to denote a subgradient (or super-gradient) of a convex (or concave) function f(·), and use ∂f(·) to denote its sub-differential (or super-differential).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 1 (Sparse Saddle Point Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let w̄,3. A Sparse Lagrangian Duality Theory,[0],[0]
∈,3. A Sparse Lagrangian Duality Theory,[0],[0]
Rd be,3. A Sparse Lagrangian Duality Theory,[0],[0]
a k-sparse primal vector and ᾱ ∈ FN be a dual vector.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Then the pair (w̄, ᾱ) is a sparse saddle point for L if and only if the following conditions hold:
(a) w̄ solves the primal problem in (1);
(b) ᾱ ∈",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[∂l1(w̄>x1), ..., ∂lN (w̄>xN )]; (c) w̄",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= Hk ( − 1λN ∑N i=1 ᾱixi ) .
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.1.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Remark 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Theorem 1 shows that the conditions (a)∼(c) are sufficient and necessary to guarantee the existence of a sparse saddle point for the Lagrangian form L. This result is different from from the traditional saddle point theorem which requires the use of the Slater Constraint Qualification to guarantee the existence of saddle point.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Remark 2.,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let us consider P ′(w̄) = 1N ∑N i=1,3. A Sparse Lagrangian Duality Theory,[0],[0]
ᾱixi + λw̄ ∈ ∂P (w̄).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Denote F̄ = supp(w̄).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"It is easy to verify that the condition (c) in Theorem 1 is equivalent to
HF̄ (P ′(w̄))",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= 0, w̄min ≥
1 λ ‖P ′(w̄)‖∞.
The following sparse mini-max theorem guarantees that the min and max in (2) can be safely switched if and only if there exists a sparse saddle point for L(w,α).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 2 (Sparse Mini-Max Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The mini-max relationship
max α∈FN min ‖w‖0≤k L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k max α∈FN L(w,α) (4)
holds if and only if there exists a sparse saddle point (w̄, ᾱ) for L.
Proof.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.2.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The sparse mini-max result in Theorem 2 provides sufficient and necessary conditions under which one can safely exchange a min-max for a max-min, in the presence of sparsity constraint.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The following corollary is a direct consequence of applying Theorem 1 to Theorem 2.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Corollary 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The mini-max relationship
max α∈FN min ‖w‖0≤k L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k max α∈FN L(w,α)
holds if and only if there exist a k-sparse primal vector w̄ ∈ Rd and a dual vector ᾱ ∈ FN such that the conditions (a)∼(c) in Theorem 1 are satisfied.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
The mini-max result in Theorem 2 can be used as a basis for establishing sparse duality theory.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Indeed, we have already shown the following:
min ‖w‖0≤k max α∈FN L(w,α) =",3. A Sparse Lagrangian Duality Theory,[0],[0]
"min ‖w‖0≤k P (w).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
This is called the primal minimization problem and it is the min-max side of the sparse mini-max theorem.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The other side, the max-min problem, will be called as the dual maximization problem with dual objective function D(α) := min‖w‖0≤k L(w,α), i.e.,
max α∈FN D(α) = max α∈FN min",3. A Sparse Lagrangian Duality Theory,[0],[0]
‖w‖0≤k,3. A Sparse Lagrangian Duality Theory,[0],[0]
"L(w,α).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(5)
The following Lemma 1 shows that the dual objective function D(α) is concave and explicitly gives the expression of its super-differential.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Lemma 1.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"The dual objective function D(α) is given by
D(α) = 1
N N∑ i=1 −l∗i",3. A Sparse Lagrangian Duality Theory,[0],[0]
"(αi)− λ 2 ‖w(α)‖2,
where w(α) = Hk ( − 1Nλ ∑N i=1 αixi ) .",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Moreover, D(α) is concave and its super-differential is given by
∂D(α)",3. A Sparse Lagrangian Duality Theory,[0],[0]
"= 1
N",3. A Sparse Lagrangian Duality Theory,[0],[0]
"[w(α)>x1−∂l∗1(α1), ..., w(α)>xN−∂l∗N (αN )].
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Particularly, if w(α) is unique at α and {l∗i }i=1,...,N are differentiable, then ∂D(α) is unique and it is the supergradient of D(α).
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.3.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Based on Theorem 1 and Theorem 2, we are able to further establish a sparse strong duality theorem which gives the sufficient and necessary conditions under which the optimal values of the primal and dual problems coincide.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Theorem 3 (Sparse Strong Duality Theorem).,3. A Sparse Lagrangian Duality Theory,[0],[0]
Let w̄,3. A Sparse Lagrangian Duality Theory,[0],[0]
∈,3. A Sparse Lagrangian Duality Theory,[0],[0]
Rd is a k-sparse primal vector and ᾱ ∈ FN be a dual vector.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Then ᾱ solves the dual problem in (5), i.e., D(ᾱ) ≥ D(α), ∀α ∈ FN , and P (w̄) = D(ᾱ) if and only if the pair (w̄, ᾱ) satisfies the conditions (a)∼(c) in Theorem 1.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
Proof.,3. A Sparse Lagrangian Duality Theory,[0],[0]
"A proof of this result is given in Appendix A.4.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
"We define the sparse primal-dual gap PD(w,α) := P (w) − D(α).",3. A Sparse Lagrangian Duality Theory,[0],[0]
"The main message conveyed by Theorem 3 is that the sparse primal-dual gap reaches zero at the primal-dual pair (w̄, ᾱ) if and only if the conditions (a)∼(c) in Theorem 1 hold.
",3. A Sparse Lagrangian Duality Theory,[0],[0]
The sparse duality theory developed in this section suggests a natural way for finding the global minimum of the sparsity-constrained minimization problem in (1) via maximizing its dual problem in (5).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Once the dual maximizer ᾱ is estimated, the primal sparse minimizer w̄ can then be recovered from it according to the prima-dual connection w̄ =",3. A Sparse Lagrangian Duality Theory,[0],[0]
Hk ( − 1λN ∑N i=1 ᾱixi ) as given in the condition (c).,3. A Sparse Lagrangian Duality Theory,[0],[0]
"Since the dual objective function D(α) is shown to be concave, its global maximum can be estimated using any convex/concave optimization method.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"In the next section, we present a simple projected super-gradient method to solve the dual maximization problem.",3. A Sparse Lagrangian Duality Theory,[0],[0]
"Generally, D(α) is a non-smooth function since: 1) the conjugate function l∗i of an arbitrary convex loss li is generally non-smooth and 2) the term ‖w(α)‖2 is non-smooth with respect to α due to the truncation operation involved in computing w(α).",4. Dual Iterative Hard Thresholding,[0],[0]
"Therefore, smooth optimization methods are not directly applicable here and we resort to subgradient-type methods to solve the non-smooth dual maximization problem in (5).
",4. Dual Iterative Hard Thresholding,[0],[0]
"Algorithm 1 Dual Iterative Hard Thresholding (DIHT) Input : Training set {xi, yi}Ni=1.",4. Dual Iterative Hard Thresholding,[0],[0]
Regularization strength parameter λ.,4. Dual Iterative Hard Thresholding,[0],[0]
Cardinality constraint k. Step-size η.,4. Dual Iterative Hard Thresholding,[0],[0]
"Initialization w(0) = 0, α(0)1 = ...",4. Dual Iterative Hard Thresholding,[0],[0]
= α (0) N = 0.,4. Dual Iterative Hard Thresholding,[0],[0]
"for t = 1, 2, ..., T do (S1) Dual projected super-gradient ascent: ∀ i ∈ {1, 2, ..., N},
α (t) i = PF
( α
(t−1) i + η (t−1)g (t−1) i
) , (6)
where g(t−1)i = 1 N (x > i",4. Dual Iterative Hard Thresholding,[0],[0]
w (t−1) − l∗′i,4. Dual Iterative Hard Thresholding,[0],[0]
(α (t−1) i )) is the super-gradient and PF (·) is the Euclidian projection operator with respect to feasible set F .,4. Dual Iterative Hard Thresholding,[0],[0]
"(S2) Primal hard thresholding:
w(t) =",4. Dual Iterative Hard Thresholding,[0],[0]
Hk ( − 1 λN N∑ i=1,4. Dual Iterative Hard Thresholding,[0],[0]
α (t) i xi ) .,4. Dual Iterative Hard Thresholding,[0],[0]
"(7)
end Output: w(T ).",4. Dual Iterative Hard Thresholding,[0],[0]
"The Dual Iterative Hard Thresholding (DIHT) algorithm, as outlined in Algorithm 1, is essentially a projected super-gradient method for maximizing D(α).",4.1. Algorithm,[0],[0]
"The procedure generates a sequence of prima-dual pairs (w(0), α(0)), (w(1), α(1)), . . .",4.1. Algorithm,[0],[0]
from an initial pair w(0) = 0 and α(0) = 0.,4.1. Algorithm,[0],[0]
"At the t-th iteration, the dual update step S1 conducts the projected super-gradient ascent in (6) to update α(t) from α(t−1) and w(t−1).",4.1. Algorithm,[0],[0]
"Then in the primal update step S2, the primal variable w(t) is constructed from α(t) using a k-sparse truncation operation in (7).
",4.1. Algorithm,[0],[0]
"When a batch estimation of super-gradient D′(α) becomes expensive in large-scale applications, it is natural to consider the stochastic implementation of DIHT, namely SDIHT, as outlined in Algorithm 2.",4.1. Algorithm,[0],[0]
"Different from the batch computation in Algorithm 1, the dual update step S1 in Algorithm 2 randomly selects a block of samples (from a given block partition of samples) and update their corresponding dual variables according to (8).",4.1. Algorithm,[0],[0]
"Then in the primal update step S2.1, we incrementally update an intermediate accumulation vector w̃(t) which records− 1λN ∑N i=1",4.1. Algorithm,[0],[0]
α (t),4.1. Algorithm,[0],[0]
i xi as a weighted sum of samples.,4.1. Algorithm,[0],[0]
"In S2.2, the primal vector w(t) is updated by applying k-sparse truncation on w̃(t).",4.1. Algorithm,[0],[0]
The SDIHT is essentially a block-coordinate super-gradient method for the dual problem.,4.1. Algorithm,[0],[0]
"Particularly, in the extreme case m = 1, SDIHT reduces to the batch DIHT.",4.1. Algorithm,[0],[0]
"At the opposite extreme end with m = N , i.e., each block contains one sample, SDIHT becomes a stochastic coordinate-wise super-gradient method.
",4.1. Algorithm,[0],[0]
"Algorithm 2 Stochastic Dual Iterative Hard Thresholding (SDIHT) Input : Training set {xi, yi}Ni=1.",4.1. Algorithm,[0],[0]
"Regularization strength
parameter λ.",4.1. Algorithm,[0],[0]
Cardinality constraint k. Step-size η.,4.1. Algorithm,[0],[0]
"A block disjoint partition {B1, ..., Bm} of the sample index set [1, ..., N ].
",4.1. Algorithm,[0],[0]
Initialization w(0) = w̃(0),4.1. Algorithm,[0],[0]
"= 0, α(0)1 = ...",4.1. Algorithm,[0],[0]
= α (0) N = 0.,4.1. Algorithm,[0],[0]
"for t = 1, 2, ..., T do (S1) Dual projected super-gradient ascent: Uniformly randomly select an index block B(t)i ∈ {B1, ..., Bm}.",4.1. Algorithm,[0],[0]
"For all j ∈ B(t)i update α (t) j as
α (t) j = PF
( α
(t−1) j + η (t−1)g (t−1) j
) .",4.1. Algorithm,[0],[0]
"(8)
Set α(t)j = α (t−1) j , ∀j /∈",4.1. Algorithm,[0],[0]
B (t) i .,4.1. Algorithm,[0],[0]
"(S2) Primal hard thresholding: – (S2.1) Intermediate update:
w̃(t) = w̃(t−1) − 1 λN ∑ j∈B(t)i (α (t) j",4.1. Algorithm,[0],[0]
− α (t−1) j )xj .,4.1. Algorithm,[0],[0]
"(9)
– (S2.2) Hard thresholding: w(t) = Hk(w̃(t)).",4.1. Algorithm,[0],[0]
"end Output: w(T ).
",4.1. Algorithm,[0],[0]
The dual update (8) in SDIHT is much more efficient than DIHT as the former only needs to access a small subset of samples at a time.,4.1. Algorithm,[0],[0]
"If the hard thresholding operation in primal update becomes a bottleneck, e.g., in high-dimensional settings, we suggest to use SDIHT with relatively smaller number of blocks so that the hard thresholding operation in S2.2 can be less frequently called.",4.1. Algorithm,[0],[0]
We now analyze the non-asymptotic convergence behavior of DIHT and SDIHT.,4.2. Convergence analysis,[0],[0]
"In the following analysis, we will denote ᾱ = arg maxα∈FN D(α) and use the abbreviation (t)PD := PD(w
(t), α(t)).",4.2. Convergence analysis,[0],[0]
"Let r = maxa∈F |a| be the bound of the dual feasible set F and ρ = maxi,a∈F |l∗ ′
i (a)|.",4.2. Convergence analysis,[0],[0]
"For example, such quantities exist when li and l∗i are Lipschitz continuous (Shalev-Shwartz & Zhang, 2013b).",4.2. Convergence analysis,[0],[0]
We also assume without loss of generality that ‖xi‖ ≤ 1.,4.2. Convergence analysis,[0],[0]
Let X =,4.2. Convergence analysis,[0],[0]
"[x1, ..., xN ] ∈ Rd×N be the data matrix.",4.2. Convergence analysis,[0],[0]
"Given an index set F , we denote XF as the restriction of X with rows restricted to F .",4.2. Convergence analysis,[0],[0]
"The following quantities will be used in our analysis:
σ2max(X, s) = sup u∈Rn,F
{ u>X>FXFu | |F | ≤ s, ‖u‖ = 1 } ,
σ2min(X, s) = inf u∈Rn,F
{ u>X>FXFu | |F | ≤ s, ‖u‖ = 1 } .
",4.2. Convergence analysis,[0],[0]
"Particularly, σmax(X, d) = σmax(X) and σmin(X, d) = σmin(X).",4.2. Convergence analysis,[0],[0]
"We say a univariate differentiable function f(x) is γ-smooth if ∀x, y, f(y) ≤ f(x)+〈f ′(x), y−x〉+ γ2 |x− y|2.",4.2. Convergence analysis,[0],[0]
"The following is our main theorem on the dual parameter estimation error, support recovery and primal-dual gap of DIHT.
",4.2. Convergence analysis,[0],[0]
Theorem 4.,4.2. Convergence analysis,[0],[0]
Assume that li is 1/µ-smooth.,4.2. Convergence analysis,[0],[0]
Set η(t) = λN,4.2. Convergence analysis,[0],[0]
"2
(λNµ+σmin(X,k))(t+1) .",4.2. Convergence analysis,[0],[0]
"Define constants c1 =
N3(r+λρ)2
(λNµ+σmin(X,k))2 and c2 =",4.2. Convergence analysis,[0],[0]
"(r + λρ)2 ( 1 + σmax(X,k)µλN )2 .
(a) Parameter estimation error: The sequence {α(t)}t≥1 generated by Algorithm 1 satisfies the following estimation error inequality:
‖α(t)",4.2. Convergence analysis,[0],[0]
"− ᾱ‖2 ≤ c1 ( 1
t +
ln t
t
) ,
(b) Support recovery and primal-dual gap: Assume additionally that ̄",4.2. Convergence analysis,[0],[0]
:= w̄min,4.2. Convergence analysis,[0],[0]
"− 1λ‖P
′(w̄)‖∞ > 0.",4.2. Convergence analysis,[0],[0]
"Then, supp(w(t))",4.2. Convergence analysis,[0],[0]
"= supp(w̄) when
t ≥ t0 = ⌈ 12c1σ 2 max(X)
λ2N2̄2 ln
12c1σ 2 max(X)
λ2N2̄2
⌉ .
",4.2. Convergence analysis,[0],[0]
"Moreover, for any > 0, the primal-dual gap satisfies (t)PD ≤ when t ≥ max{t0, t1} where t1 =⌈
3c1c2 λ2N 2 ln 3c1c2 λ2N 2
⌉ .
",4.2. Convergence analysis,[0],[0]
Proof.,4.2. Convergence analysis,[0],[0]
"A proof of this result is given in Appendix A.5.
",4.2. Convergence analysis,[0],[0]
Remark 3.,4.2. Convergence analysis,[0],[0]
"The theorem allows µ = 0 when σmin(X, k) > 0.",4.2. Convergence analysis,[0],[0]
"If µ > 0, then σmin(X, k) is allowed to be zero and thus the step-size can be set as η(t) = Nµ(t+1) .
Consider primal sub-optimality (t)P := P (w (t))",4.2. Convergence analysis,[0],[0]
− P (w̄).,4.2. Convergence analysis,[0],[0]
"Since (t)P ≤ (t) PD always holds, the convergence rates in Theorem 4 are applicable to the primal sub-optimality as well.",4.2. Convergence analysis,[0],[0]
"An interesting observation is that these convergence results on (t)P are not relying on the Restricted Isometry Property (RIP) (or restricted strong condition number) which is required in most existing analysis of IHT-style algorithms (Blumensath & Davies, 2009; Yuan et al., 2014).",4.2. Convergence analysis,[0],[0]
"In (Jain et al., 2014), several relaxed variants of IHT-style algorithms are presented for which the estimation consistency can be established without requiring the RIP conditions.",4.2. Convergence analysis,[0],[0]
"In contrast to the RIP-free sparse recovery analysis in (Jain et al., 2014), our Theorem 4 does not require the sparsity level k to be relaxed.
",4.2. Convergence analysis,[0],[0]
"For SDIHT, we can establish similar non-asymptotic convergence results as summarized in the following theorem.
",4.2. Convergence analysis,[0],[0]
Theorem 5.,4.2. Convergence analysis,[0],[0]
Assume that li is 1/µ-smooth.,4.2. Convergence analysis,[0],[0]
"Set η(t) = λmN2 (λNµ+σmin(X,k))(t+1) .
",4.2. Convergence analysis,[0],[0]
"(a) Parameter estimation error: The sequence {α(t)}t≥1 generated by Algorithm 2 satisfies the following expected estimation error inequality:
E[‖α(t)",4.2. Convergence analysis,[0],[0]
"− ᾱ‖2] ≤ mc1 ( 1
t +
ln t
t
) ,
(b) Support recovery and primal-dual gap: Assume additionally that ̄",4.2. Convergence analysis,[0],[0]
:= w̄min,4.2. Convergence analysis,[0],[0]
"− 1λ‖P
′(w̄)‖∞ > 0.",4.2. Convergence analysis,[0],[0]
"Then, for any δ ∈ (0, 1), with probability at least 1 − δ, it holds that supp(w(t))",4.2. Convergence analysis,[0],[0]
"= supp(w̄) when
t ≥ t2 = ⌈ 12mc1σ 2 max(X)
λ2δ2N2̄2 ln
12mc1σ 2 max(X)
λ2δ2N2̄2
⌉ .
",4.2. Convergence analysis,[0],[0]
"Moreover, with probability at least 1 − δ, the primaldual gap satisfies (t)PD ≤ when t ≥ max{4t2, t3} where t3 = ⌈ 12mc1c2 λ2δ2N 2 ln 12mc1c2",4.2. Convergence analysis,[0],[0]
"λ2δ2N 2 ⌉ .
",4.2. Convergence analysis,[0],[0]
Proof.,4.2. Convergence analysis,[0],[0]
"A proof of this result is given in Appendix A.6.
Remark 4.",4.2. Convergence analysis,[0],[0]
"Theorem 5 shows that, up to scaling factors, the expected or high probability iteration complexity of SDIHT is almost identical to that of DIHT.",4.2. Convergence analysis,[0],[0]
The scaling factor m appeared in t2 and t3 reflects a trade-off between the decreased per-iteration cost and the increased iteration complexity.,4.2. Convergence analysis,[0],[0]
This section dedicates in demonstrating the accuracy and efficiency of the proposed algorithms.,5. Experiments,[0],[0]
We first show the model estimation performance of DIHT when applied to sparse ridge regression models on synthetic datasets.,5. Experiments,[0],[0]
Then we evaluate the efficiency of DIHT/SDIHT on sparse `2- regularized Huber loss and Hinge loss minimization tasks using real-world datasets.,5. Experiments,[0],[0]
A synthetic model is generated with sparse model parameter w̄ =,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"[1, 1, · · · , 1︸ ︷︷ ︸
k̄
, 0, 0, · · · , 0︸ ︷︷ ︸ d−k̄ ].",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"Each xi ∈ Rd of the
N training data examples {xi}Ni=1 is designed to have two components.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The first component is the top-k̄ feature dimensions drawn from multivariate Gaussian distribution N(µ1,Σ).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
Each entry in µ1 ∈ Rk̄ independently follows standard normal distribution.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The entries of covariance
Σij = { 1 i = j 0.25",5.1. Model parameter estimation accuracy evaluation,[0],[0]
i 6= j .,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The second component consists the left d − k̄ feature dimensions.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"It follows N(µ2, I) where each entry in µ2 ∈ Rd−k̄ is drawn from standard normal distribution.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We simulate two data parameter settings: (1) d = 500, k̄ = 100; (2) d = 300, k̄ = 100.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"In each data parameter setting 150 random data copies are
produced independently.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The task is to solve the following `2-regularized sparse linear regression problem:
min ‖w‖≤k
1
N N∑ i=1",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"lsq(yi, w >xi) + λ 2 ‖w‖2,
where lsq(yi, w>xi) =",5.1. Model parameter estimation accuracy evaluation,[0],[0]
(yi − w>xi)2.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The responses {yi}Ni=1 are produced by yi = w̄>xi,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"+ εi, where εi ∼ N(0, 1).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"The convex conjugate of lsq(yi, w>xi) is known as l∗sq(αi) = α2i 4 +yiαi (Shalev-Shwartz & Zhang, 2013b).",5.1. Model parameter estimation accuracy evaluation,[0],[0]
We consider solving the problem under the sparsity level k = k̄.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
Two measurements are calculated for evaluation.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The first is parameter estimation error ‖w,5.1. Model parameter estimation accuracy evaluation,[0],[0]
− w̄‖/‖w̄‖. Apart from it we calculate the percentage of successful support recovery (PSSR) as the second performance metric.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
A successful support recovery is obtained if supp(w̄) = supp(w).,5.1. Model parameter estimation accuracy evaluation,[0],[0]
The evaluation is conducted on the generated batch data copies to calculate the percentage of successful support recovery.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We use 50 data copies as validation set to select the parameter λ from {10−6, ..., 102} and the percentage of successful support recovery is evaluated on the other 100 data copies.
",5.1. Model parameter estimation accuracy evaluation,[0],[0]
"Iterative hard thresholding (IHT) (Blumensath & Davies, 2009) and hard thresholding pursuit (HTP) (Foucart, 2011) are used as the baseline primal algorithms.",5.1. Model parameter estimation accuracy evaluation,[0],[0]
The parameter estimation error and percentage of successful support recovery curves under varying training size are illustrated in Figure 1.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
We can observe from this group of curves that DIHT consistently achieves lower parameter estimation error and higher rate of successful support recovery than IHT and HTP.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
It is noteworthy that most significant performance gap between DIHT and the baselines occurs when the training sizeN is comparable to or slightly smaller than the sparsity level k̄.,5.1. Model parameter estimation accuracy evaluation,[0],[0]
"We now evaluate the considered algorithms on the following `2-regularized sparse Huber loss minimization problem:
min ‖w‖0≤k
1
N N∑ i=1",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
lHuber(yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i w),5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"+ λ 2 ‖w‖2, (10)
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"where
lHuber(yix > i w) =  0",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
w ≥ 1 1−,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
>,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i w,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
− γ 2,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
yix >,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
i,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
w < 1−,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"γ
1 2γ (1− yix > i w)
2 otherwise .
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"It is known that (Shalev-Shwartz & Zhang, 2013b)
l∗Huber(αi) =
{ yiαi + γ 2α 2 i if yiαi ∈",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"[−1, 0]
+∞ otherwise .
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"Two binary benchmark datasets from LibSVM data repository1, RCV1 (d = 47, 236) and News20 (d = 1, 355, 191), are used for algorithm efficiency evaluation and comparison.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We select 0.5 million samples from RCV1 dataset for
1https://www.csie.ntu.edu.tw/˜cjlin/ libsvmtools/datasets/binary.html
model training (N d).",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"For news20 dataset, all of the 19, 996 samples are used as training data (d N ).
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We evaluate the algorithm efficiency of DIHT and SDIHT by comparing their running time against three primal baseline algorithms: IHT, HTP and gradient hard thresholding with stochastic variance reduction (SVR-GHT) (Li et al., 2016).",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We first run IHT by setting its convergence criterion to be |P (w
(t))−P (w(t−1))| P (w(t)) ≤ 10−4 or maximum number of iteration is reached.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
After that we test the time cost spend by other algorithms to make the primal loss reach P (w(t)).,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
The parameter update step-size of all the considered algorithms is tuned by grid search.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
The parameter γ is set to be 0.25.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"For the two stochastic algorithms SDIHT and SVRGHT we randomly partition the training data into |B| = 10 mini-batches.
",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"Figure 2 shows the running time curves on both datasets under varying sparsity level k and regularization strength λ = 0.002, 0.0002.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"It is obvious that under all tested (k, λ) configurations on both datasets, DIHT and SDIHT need much less time than the primal baseline algorithms, IHT, HTP and SVR-GHT to reach the same primal suboptimality.",5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
Figure 3 shows the primal-dual gap convergence curves with respect to the number of epochs.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
This group of results support the theoretical prediction in Theorem 4 and 5 that PD converges non-asymptotically.,5.2.1. HUBER LOSS MODEL LEARNING,[0],[0]
"We further test the performance of our algorithms when applied to the following `2-regularized sparse hinge loss minimization problem:
min ‖w‖0≤k
1
N N∑ i=1",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
lHinge(yix >,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
i w),5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"+ λ 2 ‖w‖2,
where lHinge(yix>i w) = max(0, 1 − yiw>xi)",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"It is standard to know (Hsieh et al., 2008)
l∗Hinge(αi) = { yiαi if yiαi ∈",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"[−1, 0] +∞ otherwise .
We follow the same experiment protocol as in § 5.2.1 to compare the considered algorithms on the benchmark datasets.",5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
The time cost comparison is illustrated in Figure 4 and the prima-dual gap sub-optimality is illustrated in Figure 5.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
This group of results indicate that DIHT and SDIHT still exhibit remarkable efficiency advantage over the considered primal IHT algorithms even when the loss function is non-smooth.,5.2.2. HINGE LOSS MODEL LEARNING,[0],[0]
"In this paper, we systematically investigate duality theory and algorithms for solving the sparsity-constrained minimization problem which is NP-hard and non-convex in its
primal form.",6. Conclusion,[0],[0]
"As a theoretical contribution, we develop a sparse Lagrangian duality theory which guarantees strong duality in sparse settings, under mild sufficient and necessary conditions.",6. Conclusion,[0],[0]
This theory opens the gate to solve the original NP-hard/non-convex problem equivalently in a dual space.,6. Conclusion,[0],[0]
We then propose DIHT as a first-order method to maximize the non-smooth dual concave formulation.,6. Conclusion,[0],[0]
The algorithm is characterized by dual super-gradient ascent and primal hard thresholding.,6. Conclusion,[0],[0]
"To further improve iteration efficiency in large-scale settings, we propose SDIHT as a block stochastic variant of DIHT.",6. Conclusion,[0],[0]
"For both algorithms we have proved sub-linear primal-dual gap convergence rate when the primal loss is smooth, without assuming RIPstyle conditions.",6. Conclusion,[0],[0]
"Based on our theoretical findings and numerical results, we conclude that DIHT and SDIHT are theoretically sound and computationally attractive alternatives to the conventional primal IHT algorithms, especially when the sample size is smaller than feature dimensionality.",6. Conclusion,[0],[0]
"Xiao-Tong Yuan is supported in part by Natural Science Foundation of China (NSFC) under Grant 61402232, Grant 61522308, and in part by Natural Science Foundation of Jiangsu Province of China (NSFJPC) under Grant BK20141003.",Acknowledgements,[0],[0]
Qingshan Liu is supported in part by NSFC under Grant 61532009.,Acknowledgements,[0],[0]
"Iterative Hard Thresholding (IHT) is a class of projected gradient descent methods for optimizing sparsity-constrained minimization models, with the best known efficiency and scalability in practice.",abstractText,[0],[0]
"As far as we know, the existing IHT-style methods are designed for sparse minimization in primal form.",abstractText,[0],[0]
It remains open to explore duality theory and algorithms in such a non-convex and NP-hard problem setting.,abstractText,[0],[0]
"In this paper, we bridge this gap by establishing a duality theory for sparsity-constrained minimization with `2-regularized loss function and proposing an IHT-style algorithm for dual maximization.",abstractText,[0],[0]
Our sparse duality theory provides a set of sufficient and necessary conditions under which the original NP-hard/non-convex problem can be equivalently solved in a dual formulation.,abstractText,[0],[0]
The proposed dual IHT algorithm is a super-gradient method for maximizing the non-smooth dual objective.,abstractText,[0],[0]
"An interesting finding is that the sparse recovery performance of dual IHT is invariant to the Restricted Isometry Property (RIP), which is required by virtually all the existing primal IHT algorithms without sparsity relaxation.",abstractText,[0],[0]
"Moreover, a stochastic variant of dual IHT is proposed for large-scale stochastic optimization.",abstractText,[0],[0]
Numerical results demonstrate the superiority of dual IHT algorithms to the state-of-the-art primal IHT-style algorithms in model estimation accuracy and computational efficiency.,abstractText,[0],[0]
Dual Iterative Hard Thresholding: From Non-convex Sparse Minimization to Non-smooth Concave Maximization,title,[0],[0]
"Deep learning brings state-of-the-art results to many artificial intelligence tasks, such as neural machine translation (Wu et al., 2016), image classification (He et al., 2016b;c), image generation (van den Oord et al., 2016b;a), speech recognition (Graves et al., 2013; Amodei et al., 2016), and speech generation/synthesis (Oord et al., 2016).
",1. Introduction,[0],[0]
"Interestingly, we find that many of the aforementioned AI tasks are emerged in dual forms, i.e., the input and output of one task are exactly the output and input of the other task respectively.",1. Introduction,[0],[0]
"Examples include translation from language A to language B vs. translation from language B to A, image classification vs. image generation, and speech
1School of Information Science and Technology, University of Science and Technology of China, Hefei, Anhui, China 2Microsoft Research, Beijing, China.",1. Introduction,[0],[0]
Correspondence to: Tao Qin,1. Introduction,[0],[0]
"<taoqin@microsoft.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
recognition vs. speech synthesis.,1. Introduction,[0],[0]
"Even more interestingly (and somehow surprisingly), this natural duality is largely ignored in the current practice of machine learning.",1. Introduction,[0],[0]
"That is, despite the fact that two tasks are dual to each other, people usually train them independently and separately.",1. Introduction,[0],[0]
"Then a question arises: Can we exploit the duality between two tasks, so as to achieve better performance for both of them?",1. Introduction,[0],[0]
"In this work, we give a positive answer to the question.
",1. Introduction,[0],[0]
"To exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task and its dual task.",1. Introduction,[0],[0]
"The primal task takes a sample from space X as input and maps to space Y , and the dual task takes a sample from space Y as input and maps to space X .",1. Introduction,[0],[0]
"Using the language of probability, the primal task learns a conditional distribution P (y|x; θxy) parameterized by θxy , and the dual task learns a conditional distribution P (x|y; θyx) parameterized by θyx, where x ∈ X and y ∈ Y .",1. Introduction,[0],[0]
"In the new scheme, the two dual tasks are jointly learned and their structural relationship is exploited to improve the learning effectiveness.",1. Introduction,[0],[0]
"We name this new scheme as dual supervised learning (briefly, DSL).
",1. Introduction,[0],[0]
There could be many different ways of exploiting the duality in DSL.,1. Introduction,[0],[0]
"In this paper, we use it as a regularization term to govern the training process.",1. Introduction,[0],[0]
"Since the joint probability P (x, y) can be computed in two equivalent ways: P (x, y) = P (x)P (y|x) = P (y)P (x|y), for any x ∈ X , y ∈ Y , ideally the conditional distributions of the primal and dual tasks should satisfy the following equality:
P (x)P (y|x; θxy) = P (y)P (x|y; θyx).",1. Introduction,[0],[0]
"(1) However, if the two models (conditional distributions) are learned separately by minimizing their own loss functions (as in the current practice of machine learning), there is no guarantee that the above equation will hold.",1. Introduction,[0],[0]
The basic idea of DSL is to jointly learn the two models θxy and θyx by minimizing their loss functions subject to the constraint of Eqn.(1).,1. Introduction,[0],[0]
"By doing so, the intrinsic probabilistic connection between θyx and θxy are explicitly strengthened, which is supposed to push the learning process towards the right direction.",1. Introduction,[0],[0]
"To solve the constrained optimization problem of DSL, we convert the constraint Eqn.(1) to a penalty term by using the method of Lagrange multipliers (Boyd & Vandenberghe, 2004).",1. Introduction,[0],[0]
"Note that the penalty term could also be seen as a data-dependent regularization term.
",1. Introduction,[0],[0]
"ar X
iv :1
70 7.
00 41
5v 1
[ cs
.L",1. Introduction,[0],[0]
"G
] 3
J ul
2 01
7
To demonstrate the effectiveness of DSL, we apply it to three artificial intelligence applications 1:
(1) Neural Machine Translation (NMT)",1. Introduction,[0],[0]
"We first apply DSL to NMT, which formulates machine translation as a sequence-to-sequence learning problem, with the sentences in the source language as inputs and those in the target language as outputs.",1. Introduction,[0],[0]
"The input space and output space of NMT are symmetric, and there is almost no information loss while mapping from x to y or from y to x.",1. Introduction,[0],[0]
"Thus, symmetric tasks in NMT fits well into the scope of DSL.",1. Introduction,[0],[0]
"Experimental studies illustrate significant accuracy improvements by applying DSL to NMT: +2.07/0.86 points measured by BLEU scores for English↔French translation, +1.37/0.12 points for English↔Germen translation and +0.74/1.69 points on English↔Chinese.
",1. Introduction,[0],[0]
"(2) Image Processing We then apply DSL to image processing, in which the primal task is image classification and the dual task is image generation conditioned on category labels.",1. Introduction,[0],[0]
Both tasks are hot research topics in the deep learning community.,1. Introduction,[0],[0]
"We choose ResNet (He et al., 2016b) as our baseline for image classification, and PixelCNN++(Salimans et al., 2017) as our baseline for image generation.",1. Introduction,[0],[0]
"Experimental results show that on CIFAR-10, DSL could reduce the error rate of ResNet-110 from 6.43 to 5.40 and obtain a better image generation model with both clearer images and smaller bits per dimension.",1. Introduction,[0],[0]
Note that these primal and dual tasks do not yield a pair of completely symmetric input and output spaces since there is information loss while mapping from an image to its class label.,1. Introduction,[0],[0]
"Therefore, our experimental studies reveal that DSL can also work well for dual tasks with information loss.
(3) Sentiment Analysis Finally, we apply DSL to sentiment analysis, in which the primal task is sentiment classification (i.e., to predict the sentiment of a given sentence) and the dual one is sentence generation with given sentiment polarity.",1. Introduction,[0],[0]
"Experiments on the IMDB dataset show that DSL can improve the error rate of a widely-used sentiment classification model by 0.9 point, and can generate sentences with clearer/richer styles of sentiment expression.
",1. Introduction,[0],[0]
"All of above experiments on real artificial intelligence applications have demonstrated that DSL can improve practical performance of both tasks, simultaneously.",1. Introduction,[0],[0]
"In this section, we formulate the problem of dual supervised learning (DSL), describe an algorithm for DSL, and discuss its connections with existing learning schemes and
1In our experiments, we chose the most cited models with either open-source codes or enough implementation details, to ensure that we can reproduce the results reported in previous papers.",2. Framework,[0],[0]
"All of our experiments are done on a single Telsa K40m GPU.
",2. Framework,[0],[0]
its application scope.,2. Framework,[0],[0]
"To exploit the duality, we formulate a new learning scheme, which involves two tasks: a primal task that takes a sample from space X as input and maps to space Y , and a dual task takes a sample from space Y as input and maps to space X .
",2.1. Problem Formulation,[0],[0]
"Assume we have n training pairs {(xi, yi)}ni=1 i.i.d.",2.1. Problem Formulation,[0],[0]
sampled from the space X × Y according to some unknown distribution P .,2.1. Problem Formulation,[0],[0]
"Our goal is to reveal the bi-directional relationship between the two inputs x and y. To be specific, we perform the following two tasks: (1) the primal learning task aims at finding a function f : X 7→ Y such that the prediction of f for x is similar to its real counterpart y; (2) the dual learning task aims at finding a function g : Y 7→ X such that the prediction of g for y is similar to its real counterpart x.",2.1. Problem Formulation,[0],[0]
The dissimilarity is penalized by a loss function.,2.1. Problem Formulation,[0],[0]
"Given any (x, y), let `1(f(x), y) and `2(g(y), x) denote the loss functions for f and g respectively, both of which are mappings from X × Y to R.
A common practice to design (f, g) is the maximum likelihood estimation based on the parameterized conditional distributions P (·|x; θxy) and P (·|y; θyx):
f(x; θxy) , arg max y′∈Y
P (y′|x; θxy),
g(y; θyx) , arg max x′∈X
P (x′|y; θyx),
where θxy and θyx are the parameters to be learned.
",2.1. Problem Formulation,[0],[0]
"By standard supervised learning, the primal model f is learned by minimizing the empirical risk in space Y:
minθxy (1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`1(f(xi; θxy), yi);
and dual model g is learned by minimizing the empirical risk in space X :
minθyx(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`2(g(yi; θyx), xi).
",2.1. Problem Formulation,[0],[0]
"Given the duality of the primal and dual tasks, if the learned primal and dual models are perfect, we should have
P (x)P (y|x; θxy) = P (y)P (x|y; θyx) = P (x, y),∀x, y.
We call this property probabilistic duality, which serves as a necessary condition for the optimality of the learned two dual models.
",2.1. Problem Formulation,[0],[0]
"By the standard supervised learning scheme, probabilistic duality is not considered during the training, and the primal and the dual models are trained independently and separately.",2.1. Problem Formulation,[0],[0]
"Thus, there is no guarantee that the learned dual models can satisfy probabilistic duality.",2.1. Problem Formulation,[0],[0]
"To tackle this problem, we propose explicitly reinforcing the empirical probabilistic duality of the dual modes by solving the fol-
lowing multi-objective optimization problem instead:
objective 1: min θxy
(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`1(f(xi; θxy), yi),
objective 2: min θyx
(1/n)",2.1. Problem Formulation,[0],[0]
"∑n i=1`2(g(yi; θyx), xi),
s.t. P (x)P (y|x; θxy) = P (y)P (x|y; θyx),∀x, y,
(2)
where P (x) and P (y) are the marginal distributions.",2.1. Problem Formulation,[0],[0]
"We call this new learning scheme dual supervised learning (abbreviated as DSL).
",2.1. Problem Formulation,[0],[0]
We provide a simple theoretical analysis which shows that DSL has theoretical guarantees in terms of generalization bound.,2.1. Problem Formulation,[0],[0]
"Since the analysis is straightforward, we put it in Appendix A.",2.1. Problem Formulation,[0],[0]
"In practical artificial intelligence applications, the groundtruth marginal distributions are usually not available.",2.2. Algorithm Description,[0],[0]
"As an alternative, we use the empirical marginal distributions P̂ (x) and P̂ (y) to fulfill the constraint in Eqn.(2).
",2.2. Algorithm Description,[0],[0]
"To solve the DSL problem, following the common practice in constraint optimization, we introduce Lagrange multipliers and add the equality constraint of probabilistic duality into the objective functions.",2.2. Algorithm Description,[0],[0]
"First, we convert the probabilistic duality constraint into the following regularization term (with the empirical marginal distributions included):
`duality =(log P̂ (x) + logP (y|x; θxy) − log P̂ (y)− logP (x|y; θyx))2.
(3)
",2.2. Algorithm Description,[0],[0]
"Then, we learn the models of the two tasks by minimizing the weighted combination between the original loss functions and the above regularization term.",2.2. Algorithm Description,[0],[0]
"The algorithm is shown in Algorithm 1.
",2.2. Algorithm Description,[0],[0]
"In the algorithm, the choice of optimizers Opt1 and Opt2 is quite flexible.",2.2. Algorithm Description,[0],[0]
"One can choose different optimizers such as Adadelta (Zeiler, 2012), Adam (Kingma & Ba, 2014), or SGD for different tasks, depending on common practice in the specific task and personal preferences.",2.2. Algorithm Description,[0],[0]
"The duality between tasks has been used to enable learning from unlabeled data in (He et al., 2016a).",2.3. Discussions,[0],[0]
"As an early attempt to exploit the duality, this work actually uses the exterior connection between dual tasks, which helps to form a closed feedback loop and enables unsupervised learning.",2.3. Discussions,[0],[0]
"For example, in the application of machine translation, the primal task/model first translates an unlabeled English sentence x to a French sentence y′; then, the dual task/model translates y′ back to an English sentence x′; finally, both the primal and the dual models get optimized by minimiz-
Algorithm 1 Dual Supervise Learning Algorithm Input: Marginal distributions P̂ (xi) and P̂ (yi) for any i ∈",2.3. Discussions,[0],[0]
"[n]; Lagrange parameters λxy and λyx; optimizers Opt1 and Opt2; repeat
Get a minibatch of m pairs {(xj , yj)}mj=1; Calculate the gradients as follows:
",2.3. Discussions,[0],[0]
Gf = ∇θxy (1/m) ∑m j=1,2.3. Discussions,[0],[0]
"[ `1(f(xj ; θxy), yj)
+ λxy`duality(xj , yj ; θxy, θyx) ] ;
Gg = ∇θyx(1/m) ∑m j=1",2.3. Discussions,[0],[0]
"[ `2(g(yj ; θyx), xj)
+",2.3. Discussions,[0],[0]
"λyx`duality(xj , yj ; θxy, θyx) ] ;
(4)
Update the parameters of f and g: θxy ← Opt1(θxy, Gf ), θyx ← Opt2(θyx, Gg).
",2.3. Discussions,[0],[0]
"until models converged
ing the difference between x′ with x.",2.3. Discussions,[0],[0]
"In contrast, by making use of the intrinsic probabilistic connection between the primal and dual models, DSL takes an innovative attempt to extend the benefit of duality to supervised learning.
",2.3. Discussions,[0],[0]
"While `duality can be regarded as a regularization term, it is data dependent, which makes DSL different from Lasso (Tibshirani, 1996) or SVM (Hearst et al., 1998), where the regularization term is data-independent.",2.3. Discussions,[0],[0]
"More accurately speaking, in DSL, every training sample contributes to the regularization term, and each model contributes to the regularization of the other model.
",2.3. Discussions,[0],[0]
"DSL is different from the following three learning schemes: (1) Co-training focuses on single-task learning and assumes that different subsets of features can provide enough and complementary information about data, while DSL targets at learning two tasks with structural duality simultaneously and does not yield any prerequisite or assumptions on features.",2.3. Discussions,[0],[0]
(2) Multi-task learning requires that different tasks share the same input space and coherent feature representation while DSL does not.,2.3. Discussions,[0],[0]
"(3) Transfer Learning uses auxiliary tasks to boost the main task, while there is no difference between the roles of two tasks in DSL, and DSL enables them to boost the performance of each other simultaneously.
",2.3. Discussions,[0],[0]
We would like to point that there are several requirements to apply DSL to a certain scenario: (1) Duality should exist for the two tasks.,2.3. Discussions,[0],[0]
(2) Both the primal and dual models should be trainable.,2.3. Discussions,[0],[0]
(3) P̂ (X) and P̂ (Y ) in Eqn.,2.3. Discussions,[0],[0]
(3) should be available.,2.3. Discussions,[0],[0]
"If these conditions are not satisfied, DSL might not work very well.",2.3. Discussions,[0],[0]
"Fortunately, as we have discussed in the paper, many machine learning tasks related to image, speech, and text satisfy these conditions.",2.3. Discussions,[0],[0]
We first apply our dual supervised learning algorithm to machine translation and study whether it can improve the translation qualities by utilizing the probabilistic duality of dual translation tasks.,3. Application to Machine Translation,[0],[0]
"In the following of the section, we perform experiments on three pairs of dual tasks 2: English↔French (En↔Fr), English↔Germany (En↔De), and English↔Chinese (En↔Zh).",3. Application to Machine Translation,[0],[0]
"Datasets We employ the same datasets as used in (Jean et al., 2015) to conduct experiments on En↔Fr and En↔De.",3.1. Settings,[0],[0]
"As a part of WMT’14, the training data consists of 12M sentences pairs for En↔Fr and 4.5M for En↔De, respectively (WMT, 2014).",3.1. Settings,[0],[0]
We combine newstest2012 and newstest2013 together as the validation sets and use newstest2014 as the test sets.,3.1. Settings,[0],[0]
"For the dual tasks of En↔Zh, we use 10M sentence pairs obtained from a commercial company as training data.",3.1. Settings,[0],[0]
We leverage NIST2006 as the validation set and NIST2008 as well as NIST2012 as the test sets3.,3.1. Settings,[0],[0]
"Note that, during the training of all three pairs of dual tasks, we drop all sentences with more than 50 words.
",3.1. Settings,[0],[0]
Marginal Distributions P̂ (x) and P̂ (y),3.1. Settings,[0],[0]
"We use the LSTMbased language modeling approach (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as ∏Tx i=1",3.1. Settings,[0],[0]
"P (xi|x<i), where xi is the ith word in x, Tx denotes the number of words in x, and the index <",3.1. Settings,[0],[0]
"i indicates {1, 2, · · · , i − 1}.",3.1. Settings,[0],[0]
"More details about such language modeling approach can be referred to Appendix B.
Model We apply the GRU as the recurrent module to implement the sequence-to-sequence model, which is the same as (Bahdanau et al., 2015; Jean et al., 2015).",3.1. Settings,[0],[0]
The word embedding dimension is 620 and the number of hidden node is 1000.,3.1. Settings,[0],[0]
"Regarding the vocabulary size of the source and target language, we set it as 30k, 50k, and 30k for En↔Fr, En↔De, and En↔Zh, respectively.",3.1. Settings,[0],[0]
The out-of-vocabulary words are replaced by a special token UNK.,3.1. Settings,[0],[0]
"Following the common practice, we denote the baseline algorithm proposed in (Bahdanau et al., 2015; Jean et al., 2015) as RNNSearch.",3.1. Settings,[0],[0]
"We implement the whole NMT learning system based on an open source code4.
2Since both tasks in each pair are symmetric, they play the same role in the dual supervised learning framework.",3.1. Settings,[0],[0]
"Consequently, any one of the dual tasks can be viewed as the primal task while the other as the dual task.
3The three NIST datasets correspond to Zh→En translation task, in which each Chinese sentence has four English references.",3.1. Settings,[0],[0]
"To build the test set for En→Zh, we use the Chinese sentence with one randomly picked English sentence to form up a En→Zh validation/test pair.
",3.1. Settings,[0],[0]
"4https://github.com/nyu-dl/dl4mt-tutorial
Evaluation Metrics",3.1. Settings,[0],[0]
"The translation qualities are measured by tokenized case-sensitive BLEU (Papineni et al., 2002) scores, which is implemented by (multi bleu, 2015).",3.1. Settings,[0],[0]
"The larger the BLEU score is, the better the translation quality is.",3.1. Settings,[0],[0]
"During the evaluation process, we use beam search with beam width 12 to generate sentences.",3.1. Settings,[0],[0]
"Note that, following the common practice, the Zh→En is evaluated by case-insensitive BLEU score.
",3.1. Settings,[0],[0]
Training Procedure,3.1. Settings,[0],[0]
"We initialize the two models in DSL (i.e., the θxy and θyx) by using two warm-start models, which is generated by following the same process as (Jean et al., 2015).",3.1. Settings,[0],[0]
"Then, we use SGD with the minibatch size of 80 as the optimization method for dual training.",3.1. Settings,[0],[0]
"During the training process, we first set the initial learning rate η to 0.2 and then halve it if the BLEU score on the validation set cannot grow for a certain number of mini batches.",3.1. Settings,[0],[0]
"In order to stabilize parameters, we will freeze the embedding matrix once halving learning rates can no long improve the BLEU score on the validation set.",3.1. Settings,[0],[0]
"The gradient clip is set as 1.0, 5.0 and 1.0 during the training for En↔Fr, En↔De, and En↔Zh, respectively (Pascanu et al., 2013).",3.1. Settings,[0],[0]
The value of both λxy and λyx in Algorithm 1 are set as 0.01 according to empirical performance on the validation set.,3.1. Settings,[0],[0]
"Note that, during the optimization process, the LSTM-based language models will not be updated.",3.1. Settings,[0],[0]
Table 1 shows the BLEU scores on the dual tasks by the DSL method with that by the baseline RNNSearch method.,3.2. Results,[0],[0]
"Note that, in this table, we use (MT08) and (MT12) to denote results carried out on NIST2008 and NIST2012, respectively.",3.2. Results,[0],[0]
"From this table, we can find that, on all these three pairs of symmetric tasks, DSL can improve the performance of both dual tasks, simultaneously.
",3.2. Results,[0],[0]
"To better understand the effects of applying the probabilistic duality constraint as the regularization, we compute the `duality on the test set by DSL compared with RNNSearch.",3.2. Results,[0],[0]
"In particular, after applying DSL to En→Fr, the `duality decreases from 1545.68 to 1468.28, which also indicates that
the two models become more coherent in terms of probabilistic duality.
",3.2. Results,[0],[0]
"(Jean et al., 2015) proposed an effective post-process technique, which can achieve better translation performance by replacing the “UNK” with the corresponding word-level translations.",3.2. Results,[0],[0]
"After applying this technique into DSL, we report its results on En→Fr in Table 2, compared with several baselines with the same model structures as ours that also integrate the “UNK” post-processing technique.",3.2. Results,[0],[0]
"From this table, it is clear to see that DSL can achieve better performance than all baseline methods.
",3.2. Results,[0],[0]
"In the previous experiments, we use a warm-start approach in DSL using the models trained by RNNSearch.",3.2. Results,[0],[0]
"Actually, we can use stronger models for initialization to achieve even better accuracy.",3.2. Results,[0],[0]
We conduct a light experiment to verify this.,3.2. Results,[0],[0]
"We use the models trained by (He et al., 2016a) as the initializations in DSL on En↔Fr translation.",3.2. Results,[0],[0]
"We find that BLEU score can be improved from 34.83 to 35.95 for En→Fr translation, and from 32.94 to 33.40 for Fr→En translation.
",3.2. Results,[0],[0]
Effects of λ There are two hyperparameters λxy and λyx in our DSL algorithm.,3.2. Results,[0],[0]
We conduct some experiments to investigate their effects.,3.2. Results,[0],[0]
"Since the input and output space are symmetric, we set λxy = λyx = λ and plot the validation accuracy of different λ’s in Figure 1(a).",3.2. Results,[0],[0]
"From this figure, we can see that both En→Fr and Fr→En reach the best performance when λ = 10−2, and thus the results of DSL reported in Table 1 are obtained with λ = 10−2.",3.2. Results,[0],[0]
"Moreover, we find that, within a relatively large interval of λ, DSL outperforms standard supervised learning, i.e., the point with λ = 0.",3.2. Results,[0],[0]
"We also plot
the BLEU scores for λ = 10−2 on the validation and test sets in Figure 1(b) with respect to training iterations.",3.2. Results,[0],[0]
"We can see that, in the first couple of rounds, the test BLEU curves fluctuate with large variance.",3.2. Results,[0],[0]
"The reason is that two separately initialized models of dual tasks yield are not consistent with each other, i.e., Eqn. (1) does not hold, which causes the declination of the performance of both models as they play as the regularizer for each other.",3.2. Results,[0],[0]
"As the training goes on, two models become more consistent and finally boost the performance of each other.
",3.2. Results,[0],[0]
Case studies Table 3 shows a couple of translation examples produced by RNNSearch compared with DSL.,3.2. Results,[0],[0]
"From this table, we find that DSL demonstrates three major advantages over RNNSearch.",3.2. Results,[0],[0]
"First, by leveraging the structural duality of sentences, DSL can result in the improvement of mutual translation, e.g. “when it comes to” and “lorsqu qu’il s’agit de”, which better fit the semantics expressed in the sentences.",3.2. Results,[0],[0]
"Second, DSL can consider more contextual information in translation.",3.2. Results,[0],[0]
"For example, in Fr→En, une société is translated to company, however, in the baseline, it is translated to society.",3.2. Results,[0],[0]
"Although the word level translation is not bad, it should definitely be translated as “company” given the contextual semantics.",3.2. Results,[0],[0]
"Furthermore, DSL can better handle the plural form.",3.2. Results,[0],[0]
"For example, DSL can correctly translate “the French are the worst”, which are of plural form, while the baseline deals with it by singular form.",3.2. Results,[0],[0]
"In the domain of image processing, image classification (image→label) and image generation (label→image) are in
the dual form.",4. Application to Images Processing,[0],[0]
"In this section, we apply our dual supervised learning framework to these two tasks and conduct experimental studies based on a public dataset, CIFAR10 (Krizhevsky & Hinton, 2009), with 10 classes of images.",4. Application to Images Processing,[0],[0]
"In our experiments, we employ a popular method, ResNet5, for image classification and a most recent method, PixelCNN++6, for image generation.",4. Application to Images Processing,[0],[0]
Let X denote the image space and Y denote the category space related to CIFAR10.,4. Application to Images Processing,[0],[0]
"Marginal Distributions In our experiments, we simply use the uniform distribution to set the marginal distribution P̂ (y) of 10-class labels, which means the marginal distribution of each class equals 0.1.",4.1. Settings,[0],[0]
The image distribution P̂ (x) is usually defined as ∏m i=1,4.1. Settings,[0],[0]
P{xi|x<,4.1. Settings,[0],[0]
"i}, where all pixels of the image is serialized and xi is the value of the i-th pixel of an m-pixel image.",4.1. Settings,[0],[0]
Note that the model can predict xi only based on the previous pixels xj with index j < i.,4.1. Settings,[0],[0]
"We use the PixelCNN++, which is so far the best algorithm, to model the image distribution.
",4.1. Settings,[0],[0]
"Models For the task of image classification, we choose 32- layer ResNet (denoted as ResNet-32) and 110-layer ResNet (denoted as ResNet-110) as two baselines, respectively, in order to examine the power of DSL on both relatively simple and complex models.",4.1. Settings,[0],[0]
"For the task of image generation, we use PixelCNN++ again.",4.1. Settings,[0],[0]
"Compared to the PixelCNN++ used for modeling distribution, the difference lies in the training process: When used for image generation given a certain class, PixelCNN++ takes the class label as an additional input, i.e., it tries to characterize ∏m i=1",4.1. Settings,[0],[0]
P{xi|x<,4.1. Settings,[0],[0]
"i, y}, where y is the 1-hot label vector.
",4.1. Settings,[0],[0]
Evaluation Metrics,4.1. Settings,[0],[0]
We use the classification error rates to measure the performance of image classification.,4.1. Settings,[0],[0]
"We use bits per dimension (briefly, bpd) (Salimans et al., 2017), to assess the performance of image generation.",4.1. Settings,[0],[0]
"In particular, for an image x with label y, the bpd is defined as:
− (∑Nx i=1",4.1. Settings,[0],[0]
"logP (xi|x<i, y) ) /",4.1. Settings,[0],[0]
"( Nx log(2) ) , (5)
where Nx is the number of pixels in image x.",4.1. Settings,[0],[0]
"By using the dataset CIFAR-10, Nx is 3072 for any image x, and we will report the average bpd on the test set.
",4.1. Settings,[0],[0]
Training Procedure,4.1. Settings,[0],[0]
We first initialize both the primal and the dual models with the ResNet model and PixelCNN++ model pre-trained independently and separately.,4.1. Settings,[0],[0]
We obtain a 32-layer ResNet with error rate of 7.65 and a 110-layer ResNet with error rate of 6.54 as the pre-trained models for image classification.,4.1. Settings,[0],[0]
"The error rates of these two pretrained models are comparable to results reported in (He
5https://github.com/tensorflow/models/tree/master/resnet 6https://github.com/openai/pixel-cnn
et al., 2016b).",4.1. Settings,[0],[0]
"We generate a pre-trained conditional image generation model with the test bpd of 2.94, which is the same as reported in (Salimans et al., 2017).",4.1. Settings,[0],[0]
"For DSL training, we set the initial learning rate of image classification model as 0.1 and that of image generation model as 0.0005.",4.1. Settings,[0],[0]
"The learning rates follow the same decay rules as those in (He et al., 2016b) and (Salimans et al., 2017).",4.1. Settings,[0],[0]
The whole training process takes about two weeks before convergence.,4.1. Settings,[0],[0]
Note that experimental results below are based on the training with λxy = (30/3072)2 and λyx = (1.2/3072)2.,4.1. Settings,[0],[0]
"Table 4 compares the error rates of two image classification models, i.e., DSL vs. Baseline, on the test set.",4.2. Results on Image Classification,[0],[0]
"From this table, we find that, with using either ResNet-32 or ResNet-110, DSL achieves better accuracy than the baseline method.
",4.2. Results on Image Classification,[0],[0]
"Interestingly, we observe from Table 4 that, DSL leads to higher relative performance improvement on the ResNet110 over the ResNet-32.",4.2. Results on Image Classification,[0],[0]
"We hypothesize one possible reason is that, due to the limited training data, an appropriate regularization can benefit more to the 110-layer ResNet with higher model complexity, and the dualityoriented regularization `duality indeed plays this role and consequently gives rise to higher relative improvement.",4.2. Results on Image Classification,[0],[0]
"Our further experimental results show that, based on ResNet-110, DSL can decrease the test bpd from 2.94 (baseline) to 2.93 (DSL), which is a new state-of-the-art result on CIFAR-10.",4.3. Results on Image Generation,[0],[0]
"Indeed, it is quite difficult to improve bpd by 0.01 which though seems like a minor change.",4.3. Results on Image Generation,[0],[0]
"We also find that, there is no significant improvement on test bpd based on ResNet-32.",4.3. Results on Image Generation,[0],[0]
"An intuitive explanation is that, since ResNet-110 is stronger than ResNet-32 in modeling the conditional probability P (y|x), it can better help the task of image generation through the constraint/regularization of the probabilistic duality.
",4.3. Results on Image Generation,[0],[0]
"As pointed out in (Theis et al., 2015), bpd is not the only evaluation rule of image generation.",4.3. Results on Image Generation,[0],[0]
"Therefore, we further conduct a qualitative analysis by comparing images generated by dual supervised learning with those by the base-
line model for each of image categories, some examples of which are shown in Figure 2.
",4.3. Results on Image Generation,[0],[0]
"Each row in Figure 2 corresponds to one category in CIFAR-10, the five images in the left side are generated by the baseline model, and the five ones in the right side are generated by the model trained by DSL.",4.3. Results on Image Generation,[0],[0]
"From this figure, we find that DSL generally generates images with clearer and more distinguishable characteristics regarding the corresponding category.",4.3. Results on Image Generation,[0],[0]
"Specifically, those right five images in Row 3, 4, and 6 can illustrate more distinguishable characteristics of birds, cats and dogs respectively, which is mainly due to benefits of introducing the probabilistic duality into DSL.",4.3. Results on Image Generation,[0],[0]
"But, there are still some cases that neither the baseline model nor DSL can perform well, like deers it Row 5 and frogs in Row 7.",4.3. Results on Image Generation,[0],[0]
"One reason is that the bpd of images in the category of deer and frogs are 3.17 and 3.32, which are significant larger than the average 2.94.",4.3. Results on Image Generation,[0],[0]
This shows that the images of these two categories are harder to generate.,4.3. Results on Image Generation,[0],[0]
"Finally, we apply the dual supervised learning framework to the domain of sentiment analysis.",5. Application to Sentiment Analysis,[0],[0]
"In this domain, the primal task, sentiment classification (Maas et al., 2011; Dai & Le, 2015), is to predict the sentiment polarity label of a given sentence; and the dual task, though not quite apparent but really existed, is sentence generation based on a sentiment polarity.",5. Application to Sentiment Analysis,[0],[0]
"In this section, let X denote the sentences and Y denote the sentiment related to our task.",5. Application to Sentiment Analysis,[0],[0]
"Dataset Our experiments are performed based on the IMDB movie review dataset (IMDB, 2011), which consists of 25k training and 25k test sentences.",5.1. Experimental Setup,[0],[0]
Each sentence in this dataset is associated with either a positive or a negative sentiment label.,5.1. Experimental Setup,[0],[0]
"We randomly sample a subset of 3750 sentences from the training data as the validation set for hyperparameter tuning and use the remaining training data for model training.
",5.1. Experimental Setup,[0],[0]
"Marginal Distributions We simply use the uniform distribution to set the marginal distribution P̂ (y) of polarity labels, which means the marginal distribution of positive or negative class equals 0.5.",5.1. Experimental Setup,[0],[0]
"On the other side, we take advantage of the LSTM-based language modeling to model the marginal distribution P̂ (x) of a sentence x.",5.1. Experimental Setup,[0],[0]
"The test perplexities (Bengio et al., 2003) of the obtained language model is 58.74.
",5.1. Experimental Setup,[0],[0]
"Model Implementation We leverage the widely used LSTM (Dai & Le, 2015) modeling approach for sentiment classification7 model.",5.1. Experimental Setup,[0],[0]
We set the embedding dimension as 500 and the hidden layer size as 1024.,5.1. Experimental Setup,[0],[0]
"For sentence generation, we use another LSTM model with W ewEwxt−1 + W e sEsy as input, where xt−1 denotes the t−1’th word,Ew andEs represent the embedding matrices for word and sentiment label respectively, and W ’s represent the connections between embedding matrix and LSTM cells.",5.1. Experimental Setup,[0],[0]
"A sentence is generated word by word sequentially, and the probability that word xt is generated is proportional to exp(W dwEwxt−1",5.1. Experimental Setup,[0],[0]
"+W d s Esy +Whht−1), where ht−1 is the hidden state outputted by LSTM.",5.1. Experimental Setup,[0],[0]
Note the W ’s and the E’s are the parameters to learn in training.,5.1. Experimental Setup,[0],[0]
"In the following, we call the model for sentiment based sentence generation as contextual language model (briefly, CLM).
",5.1. Experimental Setup,[0],[0]
Evaluation Metrics,5.1. Experimental Setup,[0],[0]
"We measure the performance of sentiment classification by the error rate, and that of sentence generation, i.e., CLM, by test perplexity.
",5.1. Experimental Setup,[0],[0]
"Training Procedure To obtain baseline models, we use Adadelta as the optimization method to train both the sentiment classification and sentence generation model.",5.1. Experimental Setup,[0],[0]
"Then, we use them to initialization the two models for DSL.",5.1. Experimental Setup,[0],[0]
"At the beginning of DSL training, we use plain SGD with an initial learning rate of 0.2 and then decrease it to 0.02 for both models once there is no further improvement on the validation set.",5.1. Experimental Setup,[0],[0]
"For each (x, y) pair, we set λxy =",5.1. Experimental Setup,[0],[0]
"(5/lx)2 and λyx = (0.5/lx)2, where lx is the length of x.",5.1. Experimental Setup,[0],[0]
"The whole training process of DSL takes less than two days.
",5.1. Experimental Setup,[0],[0]
"7Both supervised and semi-supervised sentiment classification are studied in (Dai & Le, 2015).",5.1. Experimental Setup,[0],[0]
We focus on supervised learning here.,5.1. Experimental Setup,[0],[0]
"Therefore, we do not compare with the models trained with semi-supervised (labeled + unlabeled) data.",5.1. Experimental Setup,[0],[0]
Table 5 compares the performance of DSL with the baseline method in terms of both the error rates of sentiment classification and the perplexity of sentence generation.,5.2. Results,[0],[0]
"Note that the test error of the baseline classification model, which is 10.10 as shown in the table, is comparable to the recent results as reported in (Dai & Le, 2015).",5.2. Results,[0],[0]
We have two observations from the table.,5.2. Results,[0],[0]
"First, DSL can reduce the classification error by 0.90 without modifying the LSTMbased model structure.",5.2. Results,[0],[0]
"Second, DSL slightly improves the perplexity for sentence generation, but the improvement is not very significant.",5.2. Results,[0],[0]
"We hypothesize the reason is that the sentiment label can merely supply at most 1 bit information such that the perplexity difference between the language model (i.e., the marginal distribution P̂ (x)) and CLM (i.e., the conditional distribution P (x|y)) are not large, which limits the improvement brought by DSL.
",5.2. Results,[0],[0]
"Qualitative analysis on sentence generation
In addition to quantitative studies as shown above, we further conduct qualitative analysis on the performance of sentence generation.",5.2. Results,[0],[0]
Table 6 demonstrates some examples of generated sentences based on sentiment labels.,5.2. Results,[0],[0]
"From this table, we can find that both the baseline model and DSL succeed in generating sentences expressing the certain sentiment.",5.2. Results,[0],[0]
"The baseline model prefers to produce the sentence with those words yielding high-frequency in the training data, such as the “the plot is simple/predictable, the acting is great/bad”, etc.",5.2. Results,[0],[0]
"This is because the sentence generation model itself is essentially a language model based generator, which aims at catching the high-frequency words in the training data.",5.2. Results,[0],[0]
"Meanwhile, since the training of CLM in DSL can leverage the signals provided by the classifier, DSL makes it more possible to select those words, phrases, or textual patterns that can present more specific and more intense sentiment, such as “nothing but good, 10/10, don’t waste your time”, etc.",5.2. Results,[0],[0]
"As a result, the CLM in DSL can generate sentences with richer expressions for sentiments.",5.2. Results,[0],[0]
"In previous experiments, we start DSL training with welltrained primal and dual models.",5.3. Discussions,[0],[0]
We conduct some further experiments to verify whether warm start is a must for DSL.,5.3. Discussions,[0],[0]
(1) We train DSL from a warm-start sentence generator and a cold-start (randomly initialized) sentence classifier.,5.3. Discussions,[0],[0]
"In this case, DSL achieves a classification error of 9.44%, which is better than the baseline classifier in Ta-
ble 5.",5.3. Discussions,[0],[0]
(2) We train DSL from a warm-start classifier and a cold-start sentence generator.,5.3. Discussions,[0],[0]
"The perplexity of the generator after DSL training reach 58.79, which is better than the baseline generator.",5.3. Discussions,[0],[0]
(3) We train DSL from both cold-start models.,5.3. Discussions,[0],[0]
"The final classification error is 9.50% and the perplexity of the generator is 58.82, which are both better than the baselines.",5.3. Discussions,[0],[0]
"These results show that the success of DSL does not necessarily require warm-start models, although they can speed up the training of DSL.",5.3. Discussions,[0],[0]
"Observing the existence of structure duality among many AI tasks, we have proposed a new learning framework, dual supervised learning, which can greatly improve the performance for both the primal and the dual tasks, simultaneously.",6. Conclusions and Future Work,[0],[0]
We have introduced a probabilistic duality term to serve as a data-dependent regularizer to better guide the training.,6. Conclusions and Future Work,[0],[0]
"Empirical studies have validated the effectiveness of dual supervised learning.
",6. Conclusions and Future Work,[0],[0]
There are multiple directions to explore in the future.,6. Conclusions and Future Work,[0],[0]
"First, we will test dual supervised learning on more dual tasks, such as speech recognition and speech synthesis.",6. Conclusions and Future Work,[0],[0]
"Second, we will enrich theoretical study to better understand dual supervised learning.",6. Conclusions and Future Work,[0],[0]
"Third, it is interesting to combine dual supervised learning with unsupervised dual learning (He et al., 2016a) to leverage unlabeled data so as to further improve the two dual tasks.",6. Conclusions and Future Work,[0],[0]
"Fourth, we will combine dual supervised learning with dual inference (Xia et al., 2017) so as to leverage structural duality to enhance both the training and inference procedures.
",6. Conclusions and Future Work,[0],[0]
Appendix,6. Conclusions and Future Work,[0],[0]
"As we know, the final goal of the dual learning is to give correct predictions for the unseen test data.",A. Theoretical Analysis,[0],[0]
"That is to say, we want to minimize the (expected) risk of the dual models, which is defined as follows8:
R(f, g) =",A. Theoretical Analysis,[0],[0]
"E [ `1(f(x), y) + `2(g(y), x)
2
] ,∀f ∈ F , g ∈ G,
whereF = {f(x; θxy); θxy∈Θxy}, G = {g(x; θyx); θyx ∈ Θyx}, Θxy and Θyx are parameter spaces, and the E is taken over the underlying distribution P .",A. Theoretical Analysis,[0],[0]
"Besides, let D denote the product space of the two models satisfying probabilistic duality, i.e., the constraint in Eqn.(4).",A. Theoretical Analysis,[0],[0]
"For ease of reference, defineHdual as (F × G) ∩ D.
Define the empirical risk on the n sample as follows: for any f ∈ F , g ∈ G,
Rn(f, g) = 1
n ∑n i=1 `1(f(xi), yi) + `2(g(yi), xi) 2 .
",A. Theoretical Analysis,[0],[0]
"Following (Bartlett & Mendelson, 2002), we introduce Rademacher complexity for dual supervised learning, a measure for the complexity of the hypothesis.",A. Theoretical Analysis,[0],[0]
Definition 1.,A. Theoretical Analysis,[0],[0]
"Define the Rademacher complexity of DSL, RDSLn , as follows:
RDSLn = E z,σ
[ sup
(f,g)∈Hdual
∣∣ 1 n n∑ i=1",A. Theoretical Analysis,[0],[0]
σi,A. Theoretical Analysis,[0],[0]
"( `1(f(xi), yi)+`2(g(yi), xi) )∣∣",A. Theoretical Analysis,[0],[0]
"], where z = {z1, z2, · · · , zn} ∼ Pn, zi = (xi, yi) in which xi ∈ X and yi ∈ Y , σ = {σ1, · · · , σm} are i.i.d sampled with P (σi = 1) =",A. Theoretical Analysis,[0],[0]
"P (σi = −1) = 0.5.
",A. Theoretical Analysis,[0],[0]
"Based on RDSLn , we have the following theorem for dual supervised learning:
Theorem 1 ((Mohri et al., 2012)).",A. Theoretical Analysis,[0],[0]
"Let 12`1(f(x), y) + 1 2`2(g(y), x) be a mapping from X × Y to [0, 1].",A. Theoretical Analysis,[0],[0]
"Then, for any δ ∈ (0, 1), with probability at least 1−δ, the following inequality holds for any (f, g) ∈ Hdual,
R(f, g) ≤",A. Theoretical Analysis,[0],[0]
"Rn(f, g) + 2RDSLn",A. Theoretical Analysis,[0],[0]
"+ √ 1
2n ln(
1 δ ).",A. Theoretical Analysis,[0],[0]
"(6)
Similarly, we define the Rademacher complexity for the standard supervised learning RSLn under our framework by replacing theHdual in Definition 1 byF×G. With probability at least 1 − δ, the generation error bound of supervised learning is smaller than 2RSLn + √ 1 2n ln( 1 δ ).
",A. Theoretical Analysis,[0],[0]
"8The parameters θxy and θyx in the dual models will be omitted when the context is clear.
",A. Theoretical Analysis,[0],[0]
"Since Hdual ∈ F × G, by the definition of Rademacher complexity, we have RDSLn ≤ RSLn .",A. Theoretical Analysis,[0],[0]
"Therefore, DSL enjoys a smaller generation error bound than supervised learning.
",A. Theoretical Analysis,[0],[0]
"The approximation of dual supervised learning is defined as
R(f∗F , g ∗ F )−R∗ (7)
in which
R(f∗F , g ∗ F ) = inf R(f, g), s.t. (f, g) ∈ Hdual; R∗ = inf R(f, g).
",A. Theoretical Analysis,[0],[0]
"The approximation error for supervised learning is similarly defined.
",A. Theoretical Analysis,[0],[0]
"Define Py|x = {P (y|x; θxy)|θxy ∈ Θxy}, Px|y = {P (x|y; θyx)|θyx ∈ Θyx}.",A. Theoretical Analysis,[0],[0]
Let P ∗y|x and P ∗ x|y denote the two conditional probabilities derived from P .,A. Theoretical Analysis,[0],[0]
"We have the following theorem:
Theorem 2.",A. Theoretical Analysis,[0],[0]
If P ∗y|x ∈ Py|x and P ∗,A. Theoretical Analysis,[0],[0]
"x|y ∈ Px|y , then supervised learning and DSL has the same approximation error.
",A. Theoretical Analysis,[0],[0]
Proof.,A. Theoretical Analysis,[0],[0]
"By definition, we can verify both of the two approximation errors are zero.",A. Theoretical Analysis,[0],[0]
"We use the LSTM language models (Sundermeyer et al., 2012; Mikolov et al., 2010) to characterize the marginal distribution of a sentence x, defined as ∏Tx i=1",B. Details about the Language Models for Marginal Distributions,[0],[0]
"P (xi|x<i), where xi is the i-th word in x, Tx denotes the number of words in x, and the index <",B. Details about the Language Models for Marginal Distributions,[0],[0]
"i indicates {1, 2, · · · , i − 1}.",B. Details about the Language Models for Marginal Distributions,[0],[0]
The embedding dimension and hidden node are both 1024.,B. Details about the Language Models for Marginal Distributions,[0],[0]
We apply 0.5 dropout to the input embedding and the last hidden layer before softmax.,B. Details about the Language Models for Marginal Distributions,[0],[0]
"The validation perplexities of the language models are shown in Table 7, where the validation sets are the same.
",B. Details about the Language Models for Marginal Distributions,[0],[0]
"For the marginal distributions for sentences of sentiment classification, we choose the LSTM language model again like those for machine translation applications.",B. Details about the Language Models for Marginal Distributions,[0],[0]
The two differences are: (i) the vocabulary size is 10000; (ii) the word embedding dimension is 500.,B. Details about the Language Models for Marginal Distributions,[0],[0]
The perplexity of this language model is 58.74.,B. Details about the Language Models for Marginal Distributions,[0],[0]
"Many supervised learning tasks are emerged in dual forms, e.g., English-to-French translation vs. French-to-English translation, speech recognition vs. text to speech, and image classification vs. image generation.",abstractText,[0],[0]
Two dual tasks have intrinsic connections with each other due to the probabilistic correlation between their models.,abstractText,[0],[0]
"This connection is, however, not effectively utilized today, since people usually train the models of two dual tasks separately and independently.",abstractText,[0],[0]
"In this work, we propose training the models of two dual tasks simultaneously, and explicitly exploiting the probabilistic correlation between them to regularize the training process.",abstractText,[0],[0]
"For ease of reference, we call the proposed approach dual supervised learning.",abstractText,[0],[0]
"We demonstrate that dual supervised learning can improve the practical performances of both tasks, for various applications including machine translation, image processing, and sentiment analysis.",abstractText,[0],[0]
Dual Supervised Learning,title,[0],[0]
Recent years have seen rapid progress in generative modeling made possible by advances in deep learning and stochastic variational inference.,1. Introduction,[0],[0]
"The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014) has made stochastic variational inference efficient by providing lower-variance gradient estimates.",1. Introduction,[0],[0]
"However, reparameterization, as originally proposed, does not easily extend to semi-supervised learning, binary latent attribute models, topic modeling, variational memory addressing, hard attention models, or clustering, which require discrete latentvariables.
",1. Introduction,[0],[0]
"Continuous relaxations have been proposed for accommodating discrete variables in variational inference (Maddison et al., 2016; Jang et al., 2016; Rolfe, 2016).",1. Introduction,[0],[0]
"The Gumbel-
1Quadrant.ai, D-Wave Systems Inc., Burnaby, BC, Canada.",1. Introduction,[0],[0]
"Correspondence to: Arash Vahdat <arash@quadrant.ai>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Softmax technique (Maddison et al., 2016; Jang et al., 2016) defines a temperature-based continuous distribution that in the zero-temperature limit converges to a discrete distribution.",1. Introduction,[0],[0]
"However, it is limited to categorical distributions and does not scale to multivariate models such as Boltzmann machines (BM).",1. Introduction,[0],[0]
"The approach presented in (Rolfe, 2016) can train models with BM priors but requires careful handling of the gradients during training.
",1. Introduction,[0],[0]
We propose a new class of smoothing transformations for relaxing binary latent variables.,1. Introduction,[0],[0]
The method relies on two distributions with overlapping support that in the zero temperature limit converge to a Bernoulli distribution.,1. Introduction,[0],[0]
"We present two variants of smoothing transformations using a mixture of exponential and a mixture of logistic distributions.
",1. Introduction,[0],[0]
"We demonstrate that overlapping transformations can be used to train discrete directed latent models as in (Maddison et al., 2016; Jang et al., 2016), and models with BMs in their prior as in (Rolfe, 2016).",1. Introduction,[0],[0]
"In the case of BM priors, we show that the Kullback-Leibler (KL) contribution to the variational bound can be approximated using an analytic expression that can be optimized using automatic differentiation without requiring the special treatment of gradients in (Rolfe, 2016).
",1. Introduction,[0],[0]
"Using this analytic bound, we develop a new variational autoencoder (VAE) architecture called DVAE++, which uses a BM prior to model discontinuous latent factors such as object categories or scene configuration in images.",1. Introduction,[0],[0]
"DVAE++ is inspired by (Rolfe, 2016) and includes continuous local latent variables to model locally smooth features in the data.",1. Introduction,[0],[0]
DVAE++ achieves comparable results to the state-of-the-art techniques on several datasets and captures semantically meaningful discrete aspects of the data.,1. Introduction,[0],[0]
"We show that even when all continuous latent variables are removed, DVAE++ still attains near state-of-the-art generative likelihoods.",1. Introduction,[0],[0]
Training of models with discrete latent variables z requires low-variance estimates of gradients of the form ∇φEqφ(z)[f(z)].,1.1. Related Work,[0],[0]
"Only when z has a modest number of configurations (as in semi-supervised learning (Kingma et al., 2014) or semi-supervised generation (Maaløe et al., 2017))
can the gradient of the expectation be decomposed into a summation over configurations.
",1.1. Related Work,[0],[0]
"The REINFORCE technique (Williams, 1992) is a more scalable method that migrates the gradient inside the expectation: ∇φEqφ(z)f(z)",1.1. Related Work,[0],[0]
= Eqφ(z)[f(z)∇φ log qφ(z)].,1.1. Related Work,[0],[0]
"Although the REINFORCE estimate is unbiased, it suffers from high variance and carefully designed “control variates” are required to make it practical.",1.1. Related Work,[0],[0]
Several works use this technique and differ in their choices of the control variates.,1.1. Related Work,[0],[0]
"NVIL (Mnih & Gregor, 2014) uses a running average of the function, f(z), and an input-dependent baseline.",1.1. Related Work,[0],[0]
"VIMCO (Mnih & Rezende, 2016) is a multi-sample version of NVIL that has baselines tailored for each sample based on all the other samples.",1.1. Related Work,[0],[0]
"MuProp (Gu et al., 2015) and DARN (Gregor et al., 2013) are two other REINFORCE-based methods (with non-zero biases) that use a Taylor expansion of the function f(z) to create control variates.
",1.1. Related Work,[0],[0]
"To address the high variance of REINFORCE, other work strives to make discrete variables compatible with the reparametrization technique.",1.1. Related Work,[0],[0]
A primitive form arises from estimating the discrete variables by a continuous function during back-propagation.,1.1. Related Work,[0],[0]
"For instance, in the case of Bernoulli distribution, the latent variables can be approximated by their mean value.",1.1. Related Work,[0],[0]
"This approach is called the straight-through (ST) estimator (Bengio et al., 2013).",1.1. Related Work,[0],[0]
Another way to make discrete variables compatible with the reparametrization is to relax them into a continuous distribution.,1.1. Related Work,[0],[0]
"Concrete (Maddison et al., 2016) or Gumbel-Softmax (Jang et al., 2016) adopt this strategy by adding Gumbel noise to the logits of a softmax function with a temperature hyperparameter.",1.1. Related Work,[0],[0]
"A slope-annealed version of the ST estimator is proposed by (Chung et al., 2016) and is equivalent to the Gumbel-Softmax approach for binary variables.",1.1. Related Work,[0],[0]
"REBAR (Tucker et al., 2017) is a recent method that blends REINFORCE with Concrete to synthesize control variates.",1.1. Related Work,[0],[0]
"(Rolfe, 2016) pairs discrete variables with auxiliary continuous variables and marginalizes out the discrete variables.
",1.1. Related Work,[0],[0]
"Both overlapping transformations and Gumbel-based approaches offer smoothing through non-zero temperature; however, overlapping transformations offer additional freedom through the choice of the mixture distributions.",1.1. Related Work,[0],[0]
Let x represent observed random variables and z latent variables.,2. Background,[0],[0]
"The joint distribution over these variables is defined by the generative model p(x,z) = p(z)p(x|z), where p(z) is a prior distribution and p(x|z) is a probabilistic decoder.",2. Background,[0],[0]
"Given a dataset X = {x(1), . . .",2. Background,[0],[0]
",x(N)}, the parameters of the model are trained by maximizing the log-likelihood:
log p(X ) =
N�
i=1
log p(x(i)).
",2. Background,[0],[0]
"Typically, computing log p(x) requires an intractable marginalization over the latent variables z .",2. Background,[0],[0]
"To address this problem, the VAE (Kingma & Welling, 2014) introduces an inference model or probabilistic encoder q(z |x) that infers latent variables for each observation.",2. Background,[0],[0]
"In the VAE, instead of the maximizing the marginal log-likelihood, a variational lower bound (ELBO) is maximized:
log p(x) ≥ Eq(z|x) � log p(x|z) �",2. Background,[0],[0]
− KL � q(z |x)||p(z) � .,2. Background,[0],[0]
"(1)
The gradient of this objective is computed for the parameters of both the encoder and decoder using the reparameterization trick.",2. Background,[0],[0]
"With reparametrization, the expectation with respect to q(z |x) in Eq.",2. Background,[0],[0]
(1) is replaced with an expectation with respect to a known optimization-parameterindependent base distribution and a differentiable transformation from the base distribution to q(z |x).,2. Background,[0],[0]
"This transformation may be a scale-shift transformation, in the case of Gaussian base distributions, or rely on the inverse cumulative distribution function (CDF) in the general case.",2. Background,[0],[0]
"Following the law of the unconscious statistician, the gradient is then estimated using samples from the base distribution.
",2. Background,[0],[0]
"Unfortunately, the reparameterization trick cannot be applied directly to the discrete latent variables because there is no differentiable transformation that maps a base distribution to a discrete distribution.",2. Background,[0],[0]
"Current remedies address this difficulty using a continuous relaxation of the discrete latent variables (Maddison et al., 2016; Jang et al., 2016).",2. Background,[0],[0]
"The discrete variational autoencoder (DVAE) (Rolfe, 2016) develops a different approach which applies the reparameterization trick to a marginal distribution constructed by pairing each discrete variable with an auxiliary continuous random variable.
",2. Background,[0],[0]
"For example, let z ∈ {0, 1} represent a binary random variable with the probability mass function q(z|x).",2. Background,[0],[0]
"A smoothing transformation is defined using spike-and-exponential transformation r(ζ|z), where r(ζ|z = 0) = δ(ζ) is a Dirac δ distribution and r(ζ|z = 1) ∝",2. Background,[0],[0]
exp(βζ) is an exponential distribution defined for ζ ∈,2. Background,[0],[0]
"[0, 1] with inverse temperature β that controls the sharpness of the distribution.",2. Background,[0],[0]
"(Rolfe, 2016) notes that the autoencoding term can be defined as:
�
z
q(z|x) � dζ r(ζ|z) log p(x|ζ) = � dζ q(ζ|x) log p(x|ζ),
where the marginal
q(ζ|x) = �
z
q(z|x)r(ζ|z) (2)
is a mixture of two continuous distributions.",2. Background,[0],[0]
"By factoring the inference model so that x depends on ζ rather than z, the discrete variables can be explicitly eliminated from the ELBO and the reparameterization trick applied.
",2. Background,[0],[0]
"The smoothing transformations in (Rolfe, 2016) are limited to spike-and-X type of transformations (e.g., spike-and-exp and spike-and-Gaussian) where r(ζ|z = 0) is assumed to be a Dirac δ distribution.",2. Background,[0],[0]
This property is required for computing the gradient of the KL term in the variational lower bound.,2. Background,[0],[0]
"A symmetric smoothing transformation of binary variables can also be defined using two exponential distributions:
r(ζ|z = 0) =",3. Overlapping Transformations,[0],[0]
"e −βζ
Zβ and r(ζ|z = 1) = e
β(ζ−1)
",3. Overlapping Transformations,[0],[0]
"Zβ ,
for ζ ∈",3. Overlapping Transformations,[0],[0]
"[0, 1], where Zβ = (1−e−β)/β.",3. Overlapping Transformations,[0],[0]
"These conditionals, visualized in Fig. 1(a), define the mixture distribution q(ζ|x) of Eq.",3. Overlapping Transformations,[0],[0]
(2).,3. Overlapping Transformations,[0],[0]
"The scalar β acts as an inverse temperature as in the Gumbel softmax relaxation, and as β → ∞, q(ζ|x) approaches q(z = 0|x)δ(ζ) + q(z = 1|x)δ(ζ − 1).",3. Overlapping Transformations,[0],[0]
Application of the reparameterization trick for q(ζ|x) requires the inverse CDF of q(ζ|x).,3. Overlapping Transformations,[0],[0]
"In Appendix A of the supplementary material, we show that the inverse CDF is
F−1q(ζ|x)(ρ) =",3. Overlapping Transformations,[0],[0]
"− 1
β log −b+ √ b2 − 4c 2
(3)
where b =",3. Overlapping Transformations,[0],[0]
[ρ + e−β(q − ρ)]/(1 − q) − 1 and c = −[qe−β ]/(1 − q).,3. Overlapping Transformations,[0],[0]
"Eq. (3) is a differentiable function that converts a sample ρ from the uniform distribution U(0, 1) to a sample from q(ζ|x).",3. Overlapping Transformations,[0],[0]
As shown in Fig. 1(b),3. Overlapping Transformations,[0],[0]
the inverse CDF approaches a step function as β →,3. Overlapping Transformations,[0],[0]
"∞. However, to benefit from gradient information during training, β is set to a finite value.",3. Overlapping Transformations,[0],[0]
"Appendix C provides further visualizations comparing overlapping transformations to Concrete smoothing (Maddison et al., 2016; Jang et al., 2016).
",3. Overlapping Transformations,[0],[0]
"The overlapping exponential distributions defined here can
be generalized to any pair of smooth distributions converging to δ(ζ) and δ(ζ − 1).",3. Overlapping Transformations,[0],[0]
"In Appendix B, we provide analogous results for logistic smoothing distributions.
",3. Overlapping Transformations,[0],[0]
"Next, we apply overlapping transformations to the training of generative models with discrete latent variables.",3. Overlapping Transformations,[0],[0]
We consider both directed and undirected latent variable priors.,3. Overlapping Transformations,[0],[0]
"The simplest discrete prior is factorial; however, with conditioning, we can build complex dependencies.",4. Directed Prior,[0],[0]
"To simplify presentation, we illustrate a VAE prior with one or two groups of conditioning variables, but note that the approach straight-forwardly generalizes to many conditioning groups.
",4. Directed Prior,[0],[0]
"Our approach parallels the method developed in (Rolfe, 2016) for undirected graphical models.",4. Directed Prior,[0],[0]
Consider the generative model in Fig. 2(a) and its corresponding inference model in Fig. 2(b).,4. Directed Prior,[0],[0]
"To train this model using smoothing transformations, we introduce the continuous ζ in Figs. 2(c) and 2(d) in which dependencies on z are transferred to dependencies on ζ.",4. Directed Prior,[0],[0]
"In this way, binary latent variables influence other variables only through their continuous counterparts.",4. Directed Prior,[0],[0]
In Figs. 2(e) and 2(f) we show the same model but with z marginalized out.,4. Directed Prior,[0],[0]
"The joint (z,ζ ) model of Figs. 2(c) and 2(d) gives rise to a looser ELBO than the marginal ζ model of Figs. 2(e) and 2(f).",4. Directed Prior,[0],[0]
"Assuming that p(z1), p(z2|ζ1), q(z1|x), q(z2|x,ζ1), r(ζ1|z1), and r(ζ2|z2) are factorial in both the inference and generative models, then q(ζ1|x) and q(ζ2|ζ1,x) are also factorial with q(ζ1|x) =",4.1. Joint ELBO,[0],[0]
"� i q(ζ1,i|x)
where q(ζ1,i|x) = �
z1,i r(ζ1,i|z1,i)q(z1,i|x), and
q(ζ2|ζ1,x) = �
i q(ζ2,i|ζ1,x) where q(ζ2,i|ζ1,x) =� z2,i r(ζ2,i|z2,i)q(z2,i|ζ1,x).",4.1. Joint ELBO,[0],[0]
"In this case, the ELBO for
the model in Fig. 2(c) and 2(d) is Eq(ζ1|x)",4.1. Joint ELBO,[0],[0]
"� Eq(ζ2|ζ1,x) [log p(x|ζ1,ζ2)] �",4.1. Joint ELBO,[0],[0]
"− KL(q(z1|x)||p(z1))
",4.1. Joint ELBO,[0],[0]
− Eq(ζ1|x),4.1. Joint ELBO,[0],[0]
"[KL(q(z2|x,ζ1)||p(z2|ζ1))] .",4.1. Joint ELBO,[0],[0]
"(4)
The KL terms corresponding to the divergence between factorial Bernoulli distributions have a closed form.",4.1. Joint ELBO,[0],[0]
The expectation over ζ1 and ζ2 is reparameterized using the technique presented in Sec. 3.,4.1. Joint ELBO,[0],[0]
The ELBO for the marginal graphical model of Fig. 2(e) and Fig. 2(f) is Eq(ζ1|x),4.2. Marginal ELBO,[0],[0]
"� Eq(ζ2|x,ζ1)",4.2. Marginal ELBO,[0],[0]
"[log p(x|ζ1,ζ2)]",4.2. Marginal ELBO,[0],[0]
"� − KL(q(ζ1|x)||p(ζ1))
",4.2. Marginal ELBO,[0],[0]
− Eq(ζ1|x),4.2. Marginal ELBO,[0],[0]
"[KL(q(ζ2|x,ζ1)||p(ζ2|ζ1))]",4.2. Marginal ELBO,[0],[0]
"(5)
with p(ζ1) =",4.2. Marginal ELBO,[0],[0]
"�
i p(ζ1,i) where p(ζ1,i) =� zi r(ζ1,i|z1,i)p(z1,i) and p(ζ2|ζ1) = � i p(ζ2,i|ζ1)
where p(ζ2,i|ζ1) = � z2,i r(ζ2,i|z2,i)p(z2,i|ζ1).",4.2. Marginal ELBO,[0],[0]
The KL terms no longer have a closed form but can be estimated with the Monte Carlo method.,4.2. Marginal ELBO,[0],[0]
"In Appendix D, we show that Eq. (5) provides a tighter bound on log p(x) than does Eq. (4).",4.2. Marginal ELBO,[0],[0]
"(Rolfe, 2016) defined an expressive prior over binary latent variables by using a Boltzmann machine.",5. Boltzmann Machine Prior,[0],[0]
"We build upon that work and present a simpler objective that can still be trained with a low-variance gradient estimate.
",5. Boltzmann Machine Prior,[0],[0]
"To simplify notation, we assume that the prior distribution over the latent binary variables is a restricted Boltzmann machine (RBM), but these results can be extended to general BMs.",5. Boltzmann Machine Prior,[0],[0]
"An RBM defines a probability distribution over binary random variables arranged on a bipartite graph as p(z1, z2) = e−E(z1,z2)/Z where E(z1, z2) =",5. Boltzmann Machine Prior,[0],[0]
"−aT1 z1 − aT2 z2 − zT1Wz2 is an energy function with linear biases a1 and a2, and pairwise interactions W .",5. Boltzmann Machine Prior,[0],[0]
"Z is the partition function.
",5. Boltzmann Machine Prior,[0],[0]
Fig. 2(g) visualizes a generative model with a BM prior.,5. Boltzmann Machine Prior,[0],[0]
"As in Figs. 2(c) and 2(d), conditionals are formed on the auxiliary variables ζ instead of the binary variables z .",5. Boltzmann Machine Prior,[0],[0]
"The inference model in this case is identical to the model in Fig. 2(d) and it infers both z and ζ in a hierarchical structure.
",5. Boltzmann Machine Prior,[0],[0]
The autoencoding contribution to the ELBO with an RBM prior is again the first term in Eq.,5. Boltzmann Machine Prior,[0],[0]
(4) since both models share the same inference model structure.,5. Boltzmann Machine Prior,[0],[0]
"However, computing the KL term with the RBM prior is more challenging.",5. Boltzmann Machine Prior,[0],[0]
"Here, a novel formulation for the KL term is introduced.",5. Boltzmann Machine Prior,[0],[0]
"Our derivation can be used for training discrete variational autoencoders with a BM prior without any manual coding of gradients.
",5. Boltzmann Machine Prior,[0],[0]
"We use Eq(z,ζ |x)[f ] = Eq(ζ |x)",5. Boltzmann Machine Prior,[0],[0]
�,5. Boltzmann Machine Prior,[0],[0]
"Eq(z|x,ζ)[f ] � to compute the KL contribution to the ELBO: KL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2) �",5. Boltzmann Machine Prior,[0],[0]
"=
logZ − H � q(z1|x) �",5. Boltzmann Machine Prior,[0],[0]
"− Eq(ζ1|x) � H � q(z2|x,ζ1) �� + (6) + Eq(ζ1|x)",5. Boltzmann Machine Prior,[0],[0]
"� Eq(ζ2|x,ζ1) �",5. Boltzmann Machine Prior,[0],[0]
"Eq(z1|x,ζ1) �",5. Boltzmann Machine Prior,[0],[0]
"Eq(z2|x,ζ1,ζ2) �",5. Boltzmann Machine Prior,[0],[0]
"E(z1, z2) �� � �� �
cross-entropy
�� .
",5. Boltzmann Machine Prior,[0],[0]
"Here, H(q) is the entropy of the distribution q, which has a closed form when q is factorial Bernoulli.",5. Boltzmann Machine Prior,[0],[0]
"The conditionals q(z1|x,ζ1) and q(z2|x,ζ1,ζ2) are both factorial distributions that have analytic expressions.",5. Boltzmann Machine Prior,[0],[0]
"Denoting
µ1,i(x) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z1,i = 1|x), ν1,i(x,ζ1) ≡ q(z1,i = 1|x,ζ1), µ2,i(x,ζ1) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z2,i = 1|x,ζ1),
ν2,i(x,ζ1,ζ2) ≡",5. Boltzmann Machine Prior,[0],[0]
"q(z2,i = 1|x,ζ1,ζ2), it is straightforward to show that
ν1,i(x, ζ1) = q(z1,i = 1|x)r(ζ1,i|z1,i = 1)�
z1,i q(z1,i|x)r(ζ1,i|z1,i)
=
= σ � g(µ1,i(x))",5. Boltzmann Machine Prior,[0],[0]
"+ log �r(ζ1,i|z = 1) r(ζ1,i|z = 0) �� ,
where σ(x) = 1/(1 + e−x) is the logistic function, and g(µ) ≡ log � µ/ � 1 − µ �� is the logit function.",5. Boltzmann Machine Prior,[0],[0]
"A similar expression holds for ν2(x,ζ1,ζ2).",5. Boltzmann Machine Prior,[0],[0]
"The expectation marked as cross-entropy in Eq. (6) corresponds to the cross-entropy between a factorial distribution and an unnormalized Boltzmann machine which is
−aT1 ν1(x,ζ1)−aT2 ν2(x,ζ1,ζ2)−ν1(x,ζ1)TWν2(x,ζ1,ζ2).",5. Boltzmann Machine Prior,[0],[0]
"Finally, we use the equalities Eq(ζ1|x)[ν1(x,ζ1)] = µ1(x) and Eq(ζ2|x,ζ1)[ν2(x,ζ1,ζ2)]",5. Boltzmann Machine Prior,[0],[0]
"= µ2(x,ζ1) to simplify the cross-entropy term which defines the KL as
KL � q(z1, z2,ζ1,ζ2|x)�p(z1, z2,ζ1,ζ2)",5. Boltzmann Machine Prior,[0],[0]
"� = logZ
− H � q(z1|x) �",5. Boltzmann Machine Prior,[0],[0]
"− Eq(ζ1|x) � H � q(z2|x,ζ1) ��",5. Boltzmann Machine Prior,[0],[0]
−,5. Boltzmann Machine Prior,[0],[0]
"aT1 µ1(x)− Eq(ζ1|x) � aT2 µ2(x,ζ1)",5. Boltzmann Machine Prior,[0],[0]
�,5. Boltzmann Machine Prior,[0],[0]
− Eq(ζ1|x) �,5. Boltzmann Machine Prior,[0],[0]
"ν1(x,ζ1) TWµ2(x,ζ1) � .
",5. Boltzmann Machine Prior,[0],[0]
All terms contributing to the KL other than logZ can be computed analytically given samples from the hierarchical encoder.,5. Boltzmann Machine Prior,[0],[0]
Expectations with respect to q(ζ1|x) are reparameterized using the inverse CDF function.,5. Boltzmann Machine Prior,[0],[0]
Any automatic differentiation (AD) library can then back-propagate gradients through the network.,5. Boltzmann Machine Prior,[0],[0]
Only logZ requires special treatment.,5. Boltzmann Machine Prior,[0],[0]
"In Appendix E, we show how this term can also be included in the objective function so that its gradient is computed automatically.",5. Boltzmann Machine Prior,[0],[0]
"The ability of AD to calculate gradients stands in contrast to (Rolfe, 2016) where gradients must be manually coded.",5. Boltzmann Machine Prior,[0],[0]
"This pleasing property is a result of r(ζ|z) having the same support for both z = 0 and z = 1, and having a probabilistic q(z|x, ζ) which is not the case for the spike-and-X transformations of (Rolfe, 2016).",5. Boltzmann Machine Prior,[0],[0]
"In previous sections, we have illustrated with simple examples how overlapping transformations can be used to train discrete latent variable models with either directed or undirected priors.",6. DVAE++,[0],[0]
"Here, we develop a network architecture (DVAE++) that improves upon convolutional VAEs for generative image modeling.
",6. DVAE++,[0],[0]
"DVAE++ features both global discrete latent variables (to capture global properties such as scene or object type) and local continuous latent variables (to capture local properties such as object pose, orientation, or style).",6. DVAE++,[0],[0]
Both generative and inference networks rely on an autoregressive structure defined over groups of latent and observed variables.,6. DVAE++,[0],[0]
"As we are modeling images, conditional dependencies between groups of variables are captured with convolutional neural networks.",6. DVAE++,[0],[0]
"DVAE++ is similar to the convolutional VAEs used in (Kingma et al., 2016; Chen et al., 2016), but does not use normalizing flows.",6. DVAE++,[0],[0]
The DVAE++ graphical model is visualized in Fig. 3.,6.1. Graphical Model,[0],[0]
Global and local variables are indicated by z and h respectively.,6.1. Graphical Model,[0],[0]
Subscripts indicate different groups of random variables.,6.1. Graphical Model,[0],[0]
"The conditional distribution of each group is factorial – except for z1 and z2 in the prior, which is modeled with an RBM.",6.1. Graphical Model,[0],[0]
"Global latent variables are represented with boxes and local variables are represented with 3D volumes as they are convolutional.
",6.1. Graphical Model,[0],[0]
Groups of local continuous variables are factorial (independent).,6.1. Graphical Model,[0],[0]
This assumption limits the ability of the model to capture correlations at different spatial locations and different depths.,6.1. Graphical Model,[0],[0]
"While the autoregressive structure mitigates this defect, we rely mainly on the discrete global latent variables to capture long-range dependencies.",6.1. Graphical Model,[0],[0]
"The discrete nature of the global RBM prior allows DVAE++ to capture richlycorrelated discontinuous hidden factors that influence data generation.
",6.1. Graphical Model,[0],[0]
"Fig. 3(a) defines the generative model as
p(z,ζ ,h,x) = p(z) �
i r(ζ1,i|z1,i)r(ζ2,i|z2,i)× �
j
p(hj |h<j ,ζ )p(x|ζ ,h)
where p(z) is an RBM, ζ =",6.1. Graphical Model,[0],[0]
"[ζ1,ζ2], and r is the smoothing transformation that is applied elementwise to z .",6.1. Graphical Model,[0],[0]
"The conditional p(hj |h<j ,ζ ) is defined over the jth local variable group using a factorial normal distribution.",6.1. Graphical Model,[0],[0]
"Inspired by (Reed et al., 2017; Denton et al., 2015), the conditional on the data variable p(x|ζ ,h) is decomposed into several
factors defined on different scales of x:
p(x|ζ ,h) =",6.1. Graphical Model,[0],[0]
"p(x0|ζ ,h) �
i
p(xi|ζ ,h,x<i)
Here, x0 is of size 4 × 4 and it represents downsampled x in the lowest scale.",6.1. Graphical Model,[0],[0]
"Conditioned on x0, we generate x1 in the next scale, which is of the size 8 × 8.",6.1. Graphical Model,[0],[0]
This process is continued until the full-scale image is generated (see Appendix G.1 for more details).,6.1. Graphical Model,[0],[0]
"Here, each conditional is represented using a factorial distribution.",6.1. Graphical Model,[0],[0]
"For binary images, a factorial Bernoulli distribution is used; for colored images a factorial mixture of discretized logistic distributions is used (Salimans et al., 2017).
",6.1. Graphical Model,[0],[0]
The inference model of Fig.,6.1. Graphical Model,[0],[0]
"3(b) conditions over latent variables in a similar order as the generative model: q(z,ζ ,h|x) = q(z1|x)",6.1. Graphical Model,[0],[0]
"�
i
r(ζ1,i|z1,i)×
q(z2|x,ζ1)",6.1. Graphical Model,[0],[0]
"�
k
r(ζ2,k|z2,k) �
j
q(hj |ζ ,h<j).
",6.1. Graphical Model,[0],[0]
"The conditionals q(z1|x) and q(z2|x,ζ1) are each modeled with a factorial Bernoulli distribution, and q(hj |ζ ,h<j) represents the conditional on the jth group of local variables.
",6.1. Graphical Model,[0],[0]
"DVAE++ is related to VAEs with mixture priors (Makhzani et al., 2015; Tomczak & Welling, 2017).",6.1. Graphical Model,[0],[0]
The discrete variables z1 and z2 take exponentially many joint configurations where each configuration corresponds to a mixture component.,6.1. Graphical Model,[0],[0]
"These components are mixed by p(z1, z2) in the generative model.",6.1. Graphical Model,[0],[0]
"During training, the inference model maps each data point to a small subset of all the possible mixture components.",6.1. Graphical Model,[0],[0]
"Thus, the discrete prior learns to suppress the probability of configurations that are not used by the inference model.",6.1. Graphical Model,[0],[0]
"Training results in a multimodal p(z1, z2) that assigns similar images to a common discrete mode.",6.1. Graphical Model,[0],[0]
We use a novel neural network architecture to realize the conditional probabilities within the graphical model Fig. 3.,6.2. Neural Network Architecture,[0],[0]
"The network uses residual connections (He et al., 2016) with squeeze-and-excitation (SE) blocks (Hu et al., 2017) that have shown state-of-the-art image classification performance.",6.2. Neural Network Architecture,[0],[0]
"Our architecture is explained fully in Appendix G, and here we sketch the main components.",6.2. Neural Network Architecture,[0],[0]
"We refer to a SEResNet block as a residual block, and the network is created by combining either residual blocks, fully-connected layers, or convolutional layers.
",6.2. Neural Network Architecture,[0],[0]
The encoder uses a series of downsampling residual blocks to extract convolutional features from an input image.,6.2. Neural Network Architecture,[0],[0]
This residual network is considered as a pre-processing step that extracts convolutional feature maps at different scales.,6.2. Neural Network Architecture,[0],[0]
"The output of this network at the highest level is fed to fullyconnected networks that define q(z i|x,ζ<i) successively for
all the global latent variables.",6.2. Neural Network Architecture,[0],[0]
"The feature maps at an intermediate scale are fed to another set of residual networks that define q(hj |x,ζ ,h<j) successively for all the local latent variables.
",6.2. Neural Network Architecture,[0],[0]
The decoder uses an upsampling network to scale-up the global latent variables to the intermediate scale.,6.2. Neural Network Architecture,[0],[0]
"Then, the output of this network is fed to a set of residual networks that define p(hj |ζ ,h<j) one at a time at the same scale.",6.2. Neural Network Architecture,[0],[0]
"Finally, another set of residual networks progressively scales the samples from the latent variables up to the data space.",6.2. Neural Network Architecture,[0],[0]
"In the data space, a distribution on the smallest scale x0 is formed using a residual network.",6.2. Neural Network Architecture,[0],[0]
"Given samples at this scale, the distribution at the next scale is formed using another upsampling residual network.",6.2. Neural Network Architecture,[0],[0]
"This process is repeated until the image is generated at full scale.
",6.2. Neural Network Architecture,[0],[0]
"With many layers of latent variables, the VAE objective often turns off many of the latent variables by matching their distribution in the inference model to the prior.",6.2. Neural Network Architecture,[0],[0]
The latent units are usually removed differentially across different groups.,6.2. Neural Network Architecture,[0],[0]
Appendix H presents a technique that enables efficient use of latent variables across all groups.,6.2. Neural Network Architecture,[0],[0]
"To provide a comprehensive picture of overlapping transformations and DVAE++, we conduct three sets of experiments.",7. Experiments,[0],[0]
In Sec. 7.1 and Sec. 7.2 we train a VAE with several layers of latent variables with a feed-forward encoder and decoder.,7. Experiments,[0],[0]
This allows to compare overlapping transformations with previous work on discrete latent variables.,7. Experiments,[0],[0]
"In Sec. 7.3, we
then compare DVAE++ to several baselines.",7. Experiments,[0],[0]
"We compare overlapping transformations to NVIL (Mnih & Gregor, 2014), MuProp (Gu et al., 2015), REBAR (Tucker et al., 2017), and Concrete (Maddison et al., 2016) for training discrete single-layer latent variable models.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"We follow the structure used by (Tucker et al., 2017) in which the prior distribution and inference model are factorial Bernoulli with 200 stochastic variables.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In this setting, the inference and generative models are either linear or nonlinear functions.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In the latter case, two layers of deterministic hidden units of the size 200 with tanh activation are used.
",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"We use the settings in (Tucker et al., 2017) to initialize the parameters, define the model, and optimize the parameters for the same number of iterations.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"However, (Tucker et al., 2017) uses the Adam optimizer with β2 = 0.99999 in training.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
We used Adam with its default parameters except for � which is set to 10−3.,7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"The learning rate is selected from the set {1 · 10−4, 5 · 10−4}.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"The inverse temperature β for smoothing is annealed linearly during training with initial and final values chosen using cross validation from {5, 6, 7, 8} and {12, 14, 16, 18} respectively.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In Table 1, the performance of our model is compared with several stateof-the-art techniques proposed for training binary latent models on (statically) binarized MNIST (Salakhutdinov & Murray, 2008) and OMNIGLOT (Lake et al., 2015).",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"At test time, all models are evaluated in the binary limit (β = ∞).",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"Smoothing transformations slightly outperform previous
techniques in most cases.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"In the case of the nonlinear model on OMNIGLOT, the difference is about 2.8 nats.",7.1. Comparison with Previous Discrete Latent Variable Models,[0],[0]
"Techniques such as KL annealing (Sønderby et al., 2016), batch normalization (Ioffe & Szegedy, 2015), autoregressive inference/prior, and learning-rate decay can significantly improve the performance of a VAE beyond the results reported in Sec. 7.1.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In this second set of experiments, we evaluate overlapping transformations by comparing the training of a VAE with an RBM prior to the original DVAE (Rolfe, 2016), both of which include these improvements.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"For a fair comparison, we apply only those techniques that were also used in (Rolfe, 2016).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
We examine VAEs with one and two latent layers with feed-forward linear or nonlinear inference and generative models.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In the one-latent-layer case, the KL term in both our model and (Rolfe, 2016) reduces to the mean-field approximation.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"The only difference in this case lies in the overlapping transformations used here and the original smoothing method of (Rolfe, 2016).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"In the two-latent-layer case, our inference and generative model have the forms depicted in Fig. 2(d) and Fig. 2(g).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Again, all models are evaluated in the binary limit at the test time.
",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
Comparisons are reported in Table 2.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"For reference, we also provide the performance of the directed VAE models with the structures visualized in Fig. 2(c) to Fig. 2(f).",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
Implementation details are provided in Appendix F. Two observations can be made from Table 2.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"First, our smoothing transformation outperforms (Rolfe, 2016) in most cases.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
In some cases the difference is as large as 5.1 nats.,7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Second, the RBM prior performs better than a directed prior of the same size.",7.2. Comparison with Previous RBM Prior VAE,[0],[0]
"Lastly, we explore the performance of DVAE++ for density estimation on 2D images.",7.3. Experiments on DVAE++,[0],[0]
"In addition to statically binarized MNIST and OMNIGLOT, we test dynamically binarized MNIST (LeCun et al., 1998) and Caltech-101 silhouettes (Marlin et al., 2010).",7.3. Experiments on DVAE++,[0],[0]
All datasets have 28 × 28 binary pixel images.,7.3. Experiments on DVAE++,[0],[0]
"We use the same architecture for the MNIST and OMNIGLOT datasets, but because the Caltech101 silhouettes dataset is smaller, our model easily overfits.",7.3. Experiments on DVAE++,[0],[0]
"Consequently, we use a shallower architecture for Caltech101.",7.3. Experiments on DVAE++,[0],[0]
"We also evaluate DVAE++ on the CIFAR10 dataset, which consists of 32×32 pixel natural images.",7.3. Experiments on DVAE++,[0],[0]
"Appendix G lists the details of our architecture for different datasets.
",7.3. Experiments on DVAE++,[0],[0]
"Our goal is to determine whether we can use overlapping transformations to train a convolutional VAE with an RBM prior, and whether the RBM prior in DVAE++ captures global discrete hidden factors.",7.3. Experiments on DVAE++,[0],[0]
"In addition to DVAE++ (which uses binary global latent variables and continuous local latent variables), four different baselines are introduced by modifying the global and local distributions.",7.3. Experiments on DVAE++,[0],[0]
These baselines are listed in Table 3.,7.3. Experiments on DVAE++,[0],[0]
"For RBM (Rolfe), the spikeand-exp smoothing transformation is used and the ELBO is optimized using the derivation supplied in (Rolfe, 2016).",7.3. Experiments on DVAE++,[0],[0]
"For Bernoulli latent variables, we used the marginal distributions proposed in Sec. 4.2.",7.3. Experiments on DVAE++,[0],[0]
"For all the models, we used 16 layers of local latent variables each with 32 random variables at each spatial location.",7.3. Experiments on DVAE++,[0],[0]
"For the RBM global variables, we used 16 binary variables for all the binary datasets and 128 binary variables for CIFAR10.",7.3. Experiments on DVAE++,[0],[0]
"We cross-validated the number of the hierarchical layers in the inference model for the global variables from the set {1, 2, 4}.",7.3. Experiments on DVAE++,[0],[0]
"We used an unconditional decoder (i.e., factorial p(x|ζ ,h)) for the MNIST
datasets.",7.3. Experiments on DVAE++,[0],[0]
"We measure performance by estimating test set log-likelihood (again, according to the binary model) with 4000 importance weighted samples.",7.3. Experiments on DVAE++,[0],[0]
"Appendix I presents additional ablation experiments.
",7.3. Experiments on DVAE++,[0],[0]
"Table 3 groups the baselines into three categories: all continuous latent, discrete global and continuous local (mixed), and all discrete.",7.3. Experiments on DVAE++,[0],[0]
"Within the mixed group, DVAE++ with RBM prior generally outperforms the same model trained with (Rolfe, 2016)’s.",7.3. Experiments on DVAE++,[0],[0]
Replacing the continuous normal local variables with Bernoulli variables does not dramatically hurt the performance.,7.3. Experiments on DVAE++,[0],[0]
"For example, in the case of statically and dynamically binarized MNIST dataset, we achieve −79.72 and −79.55 respectively with unconditional decoder and 3.59 on CIFAR10 with conditional decoder.",7.3. Experiments on DVAE++,[0],[0]
To the best of our knowledge these are the best reported results on these datasets with binary latent variables.,7.3. Experiments on DVAE++,[0],[0]
Samples generated from DVAE++ are visualized in Fig. 4.,7.3. Experiments on DVAE++,[0],[0]
"As shown, the discrete global prior clearly captures discontinuous latent factors such as digit category or scene configuration.
",7.3. Experiments on DVAE++,[0],[0]
"DVAE++ results are comparable to current state-of-theart convolutional latent variable models such as VampPrior (Tomczak & Welling, 2017) and variational lossy autoencoder (VLAE)",7.3. Experiments on DVAE++,[0],[0]
"(Chen et al., 2016).",7.3. Experiments on DVAE++,[0],[0]
We note two features of these models that may offer room for further improvement for DVAE++.,7.3. Experiments on DVAE++,[0],[0]
"First, the conditional decoder used here
makes independence assumptions in each scale, whereas the state-of-the-art techniques are based on PixelCNN (Van Den Oord et al., 2016), which assumes full autoregressive dependencies.",7.3. Experiments on DVAE++,[0],[0]
"Second, methods such as VLAE use normalizing flows for flexible inference models that reduce the KL cost on the convolutional latent variables.",7.3. Experiments on DVAE++,[0],[0]
"Here, the independence assumption in each local group in DVAE++ can cause a significant KL penalty.",7.3. Experiments on DVAE++,[0],[0]
We have introduced a new family of smoothing transformations consisting of a mixture of two overlapping distributions and have demonstrated that these transformations can be used for training latent variable models with either directed or undirected priors.,8. Conclusions,[0],[0]
"Using variational bounds derived for both cases, we developed DVAE++ having a global RBM prior and local convolutional latent variables.",8. Conclusions,[0],[0]
"All experiments used exponential mixture components, but it would be interesting to explore the efficacy of other choices.",8. Conclusions,[0],[0]
Training of discrete latent variable models remains challenging because passing gradient information through discrete units is difficult.,abstractText,[0],[0]
"We propose a new class of smoothing transformations based on a mixture of two overlapping distributions, and show that the proposed transformation can be used for training binary latent models with either directed or undirected priors.",abstractText,[0],[0]
We derive a new variational bound to efficiently train with Boltzmann machine priors.,abstractText,[0],[0]
"Using this bound, we develop DVAE++, a generative model with a global discrete prior and a hierarchy of convolutional continuous variables.",abstractText,[0],[0]
"Experiments on several benchmarks show that overlapping transformations outperform other recent continuous relaxations of discrete latent variables including Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016), and discrete variational autoencoders (Rolfe, 2016).",abstractText,[0],[0]
DVAE++: Discrete Variational Autoencoders with Overlapping Transformations,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1303–1313 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1303",text,[0],[0]
"The shift-reduce transition-based framework was initially introduced, and successfully adapted from the dependency formalism, into constituent parsing by Sagae and Lavie (2005), significantly increasing phrase-structure parsing performance.
",1 Introduction,[0],[0]
"A shift-reduce algorithm uses a sequence of transitions to modify the content of two main data structures (a buffer and a stack) and create partial phrase-structure trees (or constituents) in the stack to finally produce a complete syntactic analysis for an input sentence, running in linear time.",1 Introduction,[0],[0]
"Initially, Sagae and Lavie (2005) suggested that those partial phrase-structure trees be built in a bottom-up manner: two adjacent nodes already in the stack are combined under a non-terminal to become a new constituent.",1 Introduction,[0],[0]
"This strategy was followed by many researchers (Zhang and Clark, 2009; Zhu et al., 2013; Watanabe and Sumita, 2015; Mi and Huang, 2015; Crabbé, 2015; Cross and Huang, 2016b; Coavoux and Crabbé, 2016; FernándezGonzález and Gómez-Rodrı́guez, 2018) who managed to improve the accuracy and speed of the original Sagae and Lavie’s bottom-up parser.",1 Introduction,[0],[0]
"With this, shift-reduce algorithms have become com-
petitive, and are the fastest alternative to perform phrase-structure parsing to date.
",1 Introduction,[0],[0]
"Some of these attempts (Cross and Huang, 2016b; Coavoux and Crabbé, 2016; FernándezGonzález and Gómez-Rodrı́guez, 2018) introduced dynamic oracles (Goldberg and Nivre, 2012), originally designed for transition-based dependency algorithms, to bottom-up constituent parsing.",1 Introduction,[0],[0]
They propose to use these dynamic oracles to train shift-reduce parsers instead of a traditional static oracle.,1 Introduction,[0],[0]
The latter follows the standard procedure that uses a gold sequence of transitions to train a model for parsing new sentences at test time.,1 Introduction,[0],[0]
"A shift-reduce parser trained with this approach tends to be prone to suffer from error propagation (i.e. errors made in previous states are propagated to subsequent states, causing further mistakes in the transition sequence).",1 Introduction,[0],[0]
"Dynamic oracles (Goldberg and Nivre, 2012) were developed to minimize the effect of error propagation by training parsers under closer conditions to those found at test time, where mistakes are inevitably made.",1 Introduction,[0],[0]
They are designed to guide the parser through any state it might reach during learning time.,1 Introduction,[0],[0]
"This makes it possible to introduce error exploration to force the parser to go through nonoptimal states, teaching it how to recover from mistakes and lose the minimum number of gold constituents.
",1 Introduction,[0],[0]
"Alternatively, some researchers decided to follow a different direction and explore non-bottomup strategies for producing phrase-structure syntactic analysis.
",1 Introduction,[0],[0]
"On the one hand, (Dyer et al., 2016; Kuncoro et al., 2017) proposed a top-down transition-based algorithm, which creates a phrase structure tree in the stack by first choosing the non-terminal on the top of the tree, and then considering which should be its child nodes.",1 Introduction,[0],[0]
"In contrast to the bottom-up approach, this top-down strategy adds a lookahead
guidance to the parsing process, while it loses rich local features from partially-built trees.
",1 Introduction,[0],[0]
"On the other hand, Liu and Zhang (2017a) recently developed a novel strategy that finds a compromise between the strengths of top-down and bottom-up approaches, resulting in state-of-the-art accuracy.",1 Introduction,[0],[0]
"Concretely, this parser builds the tree following an in-order traversal: instead of starting the tree from the top, it chooses the non-terminal of the resulting subtree after having the first child node in the stack.",1 Introduction,[0],[0]
"In that way each partial constituent tree is created in a bottom-up manner, but the non-terminal node is not chosen when all child nodes are in the stack (as a purely bottom-up parser does), but after the first child is considered.
",1 Introduction,[0],[0]
Liu and Zhang (2017a) report that the top-down approach is on par with the bottom-up strategy in terms of accuracy and the in-order parser yields the best accuracy to date on the WSJ.,1 Introduction,[0],[0]
"However, despite being two adequate alternatives to the traditional bottom-up strategy, no further work has been undertaken to improve their performance.1
We propose what, to our knowledge, are the first optimal dynamic oracles for both the topdown and in-order shift-reduce parsers, allowing us to train these algorithms with exploration.",1 Introduction,[0],[0]
"The resulting parsers outperform the existing versions trained with static oracles on the WSJ Penn Treebank (Marcus et al., 1993) and Chinese Treebank (CTB) benchmarks (Xue et al., 2005).",1 Introduction,[0],[0]
The version of the in-order parser trained with our dynamic oracle achieves the highest accuracy obtained so far by a single fully-supervised greedy shift-reduce system on the WSJ.,1 Introduction,[0],[0]
"The original transition system of Sagae and Lavie (2005) parses a sentence from left to right by reading (moving) words from a buffer to a stack, where partial subtrees are built.",2 Preliminaries,[0],[0]
"This process is per-
1In parallel to this work, Fried and Klein (2018) present a non-optimal dynamic oracle for training the top-down parser.
formed by a sequence of Shift (for reading) and Reduce (for building) transitions that will lead the parser through different states or parser configurations until a terminal one is reached.",2 Preliminaries,[0],[0]
"While in the bottom-up strategy the Reduce transition is in charge of labeling the partial subtree with a nonterminal at the same time the tree is built, Dyer et al. (2016) and Liu and Zhang (2017a) introduce a novel transition to choose the non-terminal on top, leaving the Reduce transition just to create the subtree under the previously decided nonterminal.",2 Preliminaries,[0],[0]
"We will now explain more in detail both the top-down and the in-order transition systems.
",2 Preliminaries,[0],[0]
"In both transition systems, parser configurations have the form c = 〈Σ, i, f, γ, α〉, where Σ is a stack of constituents, i is the position of the leftmost unprocessed word in the buffer (which is the next to be pushed onto the stack), f is a boolean variable used by the in-order transition system to mark if a configuration is terminal or not and with no value in top-down parser configurations, γ is the set of constituents that have already been built, and α is the set of non-terminal nodes that are currently in the stack.
",2 Preliminaries,[0],[0]
"Each constituent is represented as a tuple (X, l, r), where X is a non-terminal and l and r are integers defining its span.",2 Preliminaries,[0],[0]
"Constituents are composed of one or several words or constituents, and just one non-terminal node on top.",2 Preliminaries,[0],[0]
"Each word wi is represented as (w, i, i+ 1).",2 Preliminaries,[0],[0]
"To define our oracles, we will need to represent each non-terminal node of the tree as (X, j), where j has the value of i when X is included in the stack and is used to keep them in order.2
For instance, the phrase-structure tree in Figure 1 can be decomposed as the following set of gold constituents: {(S, 0, 6), (NP, 0, 2), (VP, 2, 5), (ADVP, 3, 4), (ADJP, 4, 5)}.",2 Preliminaries,[0],[0]
"In addition, the ordered set of gold non-terminal nodes added to the stack while following a top-down strategy will be {(S, 0), (NP, 0), (VP, 2), (ADVP, 3), (ADJP, 4)} and, according to an in-order approach, {(NP, 1), (S, 2), (VP, 3), (ADVP, 4), (ADJP, 5)}.",2 Preliminaries,[0],[0]
It is worth mentioning that the index of non-terminal nodes in the top-down method is the same as the leftmost span index of the constituent that it will produce.,2 Preliminaries,[0],[0]
"However, this does not hold in the in-order approach, as the leftmost child is fully processed before the node is added to the stack, so the index
2When two or more non-terminals share their labels within the tree, we use a secondary index to make them unique.
for the node will point to the leftmost span index of the second leftmost child.
",2 Preliminaries,[0],[0]
"Note that the information about the span of a constituent, the set of predicted constituents γ and the set α of predicted non-terminal nodes in the stack is not used by the original top-down and inorder parsers.",2 Preliminaries,[0],[0]
"However, we need to include it in parser configurations at learning time to allow an efficient implementation of the proposed dynamic oracles.
",2 Preliminaries,[0],[0]
"Given an input string w0 · · ·wn−1, the in-order parsing process starts at the initial configuration cs(w0 . . .",2 Preliminaries,[0],[0]
wn−1) = 〈,2 Preliminaries,[0],[0]
"[ ], 0, false, {}, {}〉 and, after applying a sequence of transitions, it ends in a terminal configuration 〈(S, 0, n), n, true, γ, α〉, where n is the number of words in the input sentence.",2 Preliminaries,[0],[0]
"The top-down transition system shares the same form for the initial and terminal configurations, except for the fact that variable f has no value in both cases.
",2 Preliminaries,[0],[0]
Figure 2 shows the available transitions in the top-down algorithm.,2 Preliminaries,[0],[0]
"In particular, the Shift transition moves the first (leftmost) word in the buffer to the stack; the Non-Terminal-X transition pushes onto the stack the non-terminal node X that should be on top of a coming constituent, and the Reduce transition pops the topmost stack nodes until the first non-terminal node appears (which is also popped) and combines them into a constituent with this non-terminal node as their parent, pushing this new constituent into the stack.",2 Preliminaries,[0],[0]
"Note that every reduction action will add a new constituent to γ and remove a non-terminal node from α, and every Non-Terminal transition will include a new non-terminal node in α.",2 Preliminaries,[0],[0]
"Figure 3 shows the top-down transition sequence that produces the phrase-structure tree in Figure 1.
",2 Preliminaries,[0],[0]
In Figure 4 we describe the available transitions in the in-order algorithm.,2 Preliminaries,[0],[0]
"The Shift, Non-Terminal-X and Reduce transitions have the same behavior as defined for the top-down transition system, except that the Reduce transition not only pops stack nodes until finding a non-terminal node (also removed from the stack), but also the node below this non-terminal node, and combines them into a constituent spanning all the popped nodes with the non-terminal node on top.",2 Preliminaries,[0],[0]
"And, finally, a Finish transition is also available to end the parsing process.",2 Preliminaries,[0],[0]
"Figure 5 shows the in-order transition sequence that outputs the constituent tree in Figure 1.
",2 Preliminaries,[0],[0]
"The standard procedure to train a greedy shiftreduce parser consists of training a classifier to approximate an oracle, which chooses optimal transitions with respect to gold parse trees.",2 Preliminaries,[0],[0]
"This classifier will greedily choose which transition sequence the parser should apply at test time.
",2 Preliminaries,[0],[0]
"Depending on the strategy used for training the parser, oracles can be static or dynamic.",2 Preliminaries,[0],[0]
"A static oracle trains the parser only on gold transition sequences, while a dynamic one can guide the parser through any possible transition path, allowing the exploration of non-optimal sequences.",2 Preliminaries,[0],[0]
"Previous work such as (Cross and Huang, 2016b; Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018) has introduced and successfully applied dynamic oracles for bottomup phrase-structure parsing.",3 Dynamic Oracles,[0],[0]
"We present dynamic oracles for training the top-down and in-order transition-based constituent parsers.
",3 Dynamic Oracles,[0],[0]
Goldberg and Nivre (2012) show that implementing a dynamic oracle reduces to defining a loss function on configurations to measure the distance from the best tree they can produce to the gold parse.,3 Dynamic Oracles,[0],[0]
This allows us to compute which transitions will lead the parser to configurations where the minimum number of mistakes are made.,3 Dynamic Oracles,[0],[0]
"According to Fernández-González and GómezRodrı́guez (2018), we can define a loss function in constituent parsing as follows: given a parser configuration c and a gold tree tG, a loss function `(c) is implemented as the minimum Hamming loss between t and tG, (L(t, tG)), where t is the already-built tree of a configuration c′ reachable from c (written as c t).",3.1 Loss function,[0],[0]
"This Hamming loss is computed as the size of the symmetric difference between the set of constituents γ and γG in the trees t and tG, respectively.",3.1 Loss function,[0],[0]
"Therefore, the loss function is defined as:
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |γG",3.1 Loss function,[0],[0]
\,3.1 Loss function,[0],[0]
γ|+,3.1 Loss function,[0],[0]
"|γ \ γG|
and, according to the authors, it can be efficiently computed for a non-binary bottom-up transition system by counting the individually unreachable arcs from configuration c (|U(c, γG)|) plus the erroneous constituents created so far (|γc \ γG|):
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |U(c, γG)|+ |γc \ γG|
We adapt the latter to efficiently implement a loss function for the top-down and in-order strategies.
",3.1 Loss function,[0],[0]
"While in bottom-up parsing constituents are created at once by a Reduce transition, in the other two approaches a Non-Terminal transition begins the process by naming the future constituent and a Reduce transition builds it by setting its span and children.",3.1 Loss function,[0],[0]
"Therefore, a Non-Terminal transition that deviates from the non-terminals expected in the gold tree will eventually produce a wrong constituent in future configurations, so it should be penalized.",3.1 Loss function,[0],[0]
"Additionally, a sequence of gold Non-Terminal transitions may also lead to a wrong final parse if they are applied in an incorrect order.",3.1 Loss function,[0],[0]
"Then, the computation of the Hamming loss in top-down and in-order phrase-structure parsing adds two more terms to the bottom-up loss expression: (1) the number of predicted non-terminal nodes that are currently in the stack (αc),3 but not included in the set of gold non-terminal nodes (αG), and (2) the number of gold non-terminal
3Note that we only consider predicted non-terminal nodes still in the stack, since wrong non-terminal nodes that have been already reduced are included in the loss as erroneous constituents.
nodes in the stack that are out of order with respect to the order needed in the gold tree:
`(c) =",3.1 Loss function,[0],[0]
min γ|c,3.1 Loss function,[0],[0]
"γ
L(γ, γG) = |U(c, γG)|+ |γc \ γG|
+|αc \ αG|+ out of order(αc, αG)
",3.1 Loss function,[0],[0]
"This loss function is used to implement a dynamic oracle that, when given any parser configuration, will return the set of transitions τ that do not increase the overall loss (i.e., `(τ(c))",3.1 Loss function,[0],[0]
"− `(c) = 0), leading the system through optimal configurations that minimize Hamming loss with respect to tG.
As suggested by (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018), constituent reachability can be used to efficiently compute the first term of the symmetric difference (|γG \ γ|), by simply counting the gold constituents that are individually unreachable from configuration c, as we describe in the next subsection.
",3.1 Loss function,[0],[0]
"The second and third terms of the loss (|γc \γG| and |αc \ αG|) can be trivially computed and are used to penalize false positives (extra erroneous constituents) so that final F-score is not harmed due to the decrease of precision, as pointed out by (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018).",3.1 Loss function,[0],[0]
"Note that it is crucial that the creation of non-gold Non-Terminal transitions is avoided, since these might not affect the creation of gold constituents, however, they will certainly lead the parser to the creation of extra erroneous constituents in future steps.
",3.1 Loss function,[0],[0]
"Finally, the function out of order of the last term can be implemented by computing the longest increasing subsequence of gold nonterminal nodes in the stack, where the order relation is given by the order of non-terminals (provided by their associated index) in the transition sequence that builds the gold tree (this order is unique, as none of our two parsers of interest have spurious ambiguity).",3.1 Loss function,[0],[0]
Obtaining the longest increasing subsequence is a well-known problem solvable in time O(n log n),3.1 Loss function,[0],[0]
"(Fredman, 1975), where n denotes the length of the input sequence.",3.1 Loss function,[0],[0]
"Once we have the largest possible sub-
sequence of gold non-terminal nodes in our configuration’s stack that is compatible with the gold order, the remaining ones give us the number of erroneous constituents that we will unavoidably generate, even in the best case, due to building them in an incorrect order.
",3.1 Loss function,[0],[0]
We will prove below that this loss formulation returns the exact loss and the resulting dynamic oracle is correct.,3.1 Loss function,[0],[0]
"We now show how the computation of the set of reachable constituents developed for bottomup parsing in (Coavoux and Crabbé, 2016; Fernández-González and Gómez-Rodrı́guez, 2018) can be extended to deal with the top-down and in-order strategies.
",3.2 Constituent reachability,[0],[0]
"Top-down transition system Let γG and αG be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input.",3.2 Constituent reachability,[0],[0]
"We say that a gold constituent (X, l, r) ∈ γG is reachable from a con-
figuration c = 〈Σ, j, /, γc, αc〉 with Σ =",3.2 Constituent reachability,[0],[0]
"[(Yp, ip, ip−1) · · · (Y2, i2, i1)|(Y1, i1, j)], and it is included in the set of individually reachable constituentsR(c, γG), iff it satisfies one of the following conditions:4
(i) (X, l, r) ∈ γc",3.2 Constituent reachability,[0],[0]
"(i.e. it has already been created and, therefore, it is reachable by definition).",3.2 Constituent reachability,[0],[0]
(ii) j ≤,3.2 Constituent reachability,[0],[0]
l < r,3.2 Constituent reachability,[0],[0]
"∧ (X, l) /∈",3.2 Constituent reachability,[0],[0]
αc,3.2 Constituent reachability,[0],[0]
"(i.e. the words dominated by the gold constituent are still in the buffer and the non-terminal node that begins its creation has not been added to the stack yet; therefore, it can be still created after pushing the correct non-terminal node and shifting the necessary words).",3.2 Constituent reachability,[0],[0]
(iii) l ∈ {ik | 1 ≤ k ≤ p} ∧,3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X, l) ∈ αc",3.2 Constituent reachability,[0],[0]
"(i.e. its span is partially or completely in the stack and the corresponding non-terminal node was already added to the stack, then, by shifting more words or/and reducing, the constituent can still be created).
",3.2 Constituent reachability,[0],[0]
"In-order transition system Let γG and αG be the set of gold constituents and the set of gold non-terminal nodes, respectively, for our current input.",3.2 Constituent reachability,[0],[0]
"We say that a gold constituent (X, l, r) ∈ γG is reachable from a configuration c = 〈Σ, j, false, γc, αc〉 with Σ =",3.2 Constituent reachability,[0],[0]
"[(Yp, ip, ip−1) · · · (Y2, i2, i1)|(Y1, i1, j)], and it is included in the set of individually reachable constituentsR(c, γG), iff it satisfies one of the following conditions:
(i) (X, l, r) ∈ γc",3.2 Constituent reachability,[0],[0]
(i.e. it has already been created).,3.2 Constituent reachability,[0],[0]
(ii) j ≤,3.2 Constituent reachability,[0],[0]
"l < r (i.e. the constituent is entirely in the buffer, then it can be still built).",3.2 Constituent reachability,[0],[0]
(iii) l ∈ {ik | 1 ≤ k ≤ p} ∧,3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X,m) /∈",3.2 Constituent reachability,[0],[0]
αc,3.2 Constituent reachability,[0],[0]
"(i.e. its first child is still a totally- or partiallybuilt constituent on top of the stack and the non-terminal node has not been created yet;
4Please note that elements from the stack can be an already-built constituent, a shifted word or a non-terminal node.",3.2 Constituent reachability,[0],[0]
"Therefore, (Yp, ip, ip−1), (Y2, i2, i1) and (Y1, i1, j) should be represented as (Yp, ip−1), (Y2, i1) and (Y1, j), respectively, when they are non-terminal nodes.",3.2 Constituent reachability,[0],[0]
"We omit this for simplicity.
",3.2 Constituent reachability,[0],[0]
"therefore, it has to wait till the first child is completed (if it is still pending) and, then, it can be still created by pushing onto the stack the correct non-terminal node and shifting more words if necessary).
(iv) l ∈ {ik | 1 ≤ k ≤ p} ∧",3.2 Constituent reachability,[0],[0]
"j ≤ r ∧ (X,m) ∈ αc ∧ ∃(Y, l,m) ∈ Σ",3.2 Constituent reachability,[0],[0]
"(i.e. its span is partially or completely in the stack, and its first child (which is an alredy-built constituent) and the non-terminal node assigned are adjacent, thus, by shifting more words or/and reducing, the constituent can still be built).",3.2 Constituent reachability,[0],[0]
"In both transition systems, the set of individually unreachable constituents U(c, γG) with respect to the set of gold constituents γG can be easily computed as γG \ R(c, γG) and will contain the gold constituents that can no longer be built.",3.2 Constituent reachability,[0],[0]
"We will now prove that the above expression of `(c) indeed provides the minimum possible Hamming loss to the gold tree among all the trees that are reachable from configuration c. This implies correctness (or optimality) of our oracle.
",3.3 Correctness,[0],[0]
"To do so, we first show that both algorithms are constituent-decomposable.",3.3 Correctness,[0],[0]
"This amounts to saying that if we take a set of m constituents that are tree-compatible (can appear together in a constituent tree, meaning that no pair of constituent spans overlap unless one is a subset of the other) and individually reachable from a configuration c, then the set is also reachable as a whole.
",3.3 Correctness,[0],[0]
We prove this by induction on m. The base case (m = 1) is trivial.,3.3 Correctness,[0],[0]
Let us suppose that constituent-decomposability holds for any set of m tree-compatible constituents.,3.3 Correctness,[0],[0]
"We will show that it also holds for any set T ofm+1 tree-compatible constituents.
",3.3 Correctness,[0],[0]
"Let (X, l, r) be one of the constituents in T such that r = min{r′",3.3 Correctness,[0],[0]
"| (X ′, l′, r′) ∈ T} and l = max{l′ | (X ′, l′, r) ∈ T}.",3.3 Correctness,[0],[0]
Let T ′,3.3 Correctness,[0],[0]
"= T \ {(X, l, r)}.",3.3 Correctness,[0],[0]
"Since T ′ has m constituents, by induction hypothesis, T ′ is a reachable set from configuration c.
Since (X, l, r) is individually reachable by hypothesis, it must satisfy at least one of the conditions for constituent reachability.",3.3 Correctness,[0],[0]
"As these conditions are different for each particular algorithm, we continue the proof separately for each:
Top-down constituent-decomposability In this case, we enumerated three constituent reachability
conditions, so we divide the proof into three cases: If the first condition holds, then the constituent (X, l, r) has already been created in c.",3.3 Correctness,[0],[0]
"Thus, it will still be present after applying any of the possible transition sequences that build T ′ starting from c. Hence, T = T ′ ∪ {(X, l, r)} is reachable from c.
",3.3 Correctness,[0],[0]
"If the second condition holds, then j ≤",3.3 Correctness,[0],[0]
"l < r and the constituent (X, l, r) can be created by l−j Shift transitions, followed by one Non-Terminal transition, r",3.3 Correctness,[0],[0]
− l Shift transitions and one Reduce transition.,3.3 Correctness,[0],[0]
"This will leave the parser in a configuration whose value of j is r, and where stack elements with left span index ≤",3.3 Correctness,[0],[0]
l (apart from those referencing the new non-terminal and its leftmost child) have not changed.,3.3 Correctness,[0],[0]
"Thus, constituents of T ′ are still individually reachable in this configuration, as their left span index is either ≥ r (and then they meet the second reachability condition)",3.3 Correctness,[0],[0]
"or≤ l (and then they meet the third), so T is reachable from c.
Finally, if the third condition holds, then we can create (X, l, r) by applying r − j Shift transitions followed by a sequence of Reduce transitions stopping when we obtain (X, l, r) on the stack (this will always happen after a finite number of such transitions, as the reachability condition guarantees that l is the left span index of some constituent already on the stack, and that (X, l) is on the stack).",3.3 Correctness,[0],[0]
"Following the same reasoning as in the previous case regarding the resulting parser configuration, we conclude that T is reachable from c.
With this we have shown the induction step, and thus constituent decomposability for the top-down parser.
",3.3 Correctness,[0],[0]
In-order constituent decomposability The inorder parser has four constituent reachability conditions.,3.3 Correctness,[0],[0]
"Analogously to the previous case, we prove the reachability of T by case analysis.
",3.3 Correctness,[0],[0]
"If the first condition holds, then we have a situation where the constituent (X, l, r) has already been created in c, so reachability of T follows from the same reasoning as for the first condition in the top-down case.
",3.3 Correctness,[0],[0]
"If the second condition holds, we have j ≤",3.3 Correctness,[0],[0]
"l < r and the constituent (X, l, r) can be created by l − j + 1 Shift transitions (where the last one shifts a word that will be assigned as left child of the new constituent), followed by the relevant Non-Terminal-X transition, r",3.3 Correctness,[0],[0]
− l,3.3 Correctness,[0],[0]
− 1 more Shift transitions and one Reduce transition.,3.3 Correctness,[0],[0]
"After this,
the parser will be in a configuration where j takes the value r, where we can use the same reasoning as in the second condition of the top-down parser to show that all constituents of T ′ are still reachable, proving reachability of T .
",3.3 Correctness,[0],[0]
"For the third condition, the proof is analogous but the combination of transitions that creates the non-terminal starts with a sequence composed of Reduce transitions (when there is a non-terminal at the top of the stack) or Non-Terminal-Y transitions for arbitrary Y (when the top of the stack is a constituent) until the top node on the stack is a constituent with left span index",3.3 Correctness,[0],[0]
"l (this ensures that the constituent at the top of the stack can serve as leftmost child for our desired constituent), followed by a Non-Terminal-X, r−j Shift transitions and one Reduce transition.
",3.3 Correctness,[0],[0]
"Finally, for the fourth condition, the reasoning is again analogous, but the computation leading to the non-terminal starts with as many Reduce transitions as non-terminal nodes located above (X,m) in the stack (if any).",3.3 Correctness,[0],[0]
"If we call j the index associated to the resulting transition, then it only remains to apply r − j Shift transitions followed by a Reduce transition.
",3.3 Correctness,[0],[0]
"Optimality With this, we have shown constituent decomposability for both parsing algorithms.",3.3 Correctness,[0],[0]
"This means that, for a configuration c, and a set of constituents that are individually reachable from c, there is always some computation that can build them all.",3.3 Correctness,[0],[0]
"This facilitates the proof that the loss function is correct.
",3.3 Correctness,[0],[0]
"To finish the proof, we observe the following: • Let c′ be a final configuration reachable from c.",3.3 Correctness,[0],[0]
"The set (γc′ \ γG), representing erroneous constituents that have been built, will always contain at least |γc \ γG|, as the algorithm never deletes constituents.",3.3 Correctness,[0],[0]
"• In addition, c′ will contain one erroneous
constituent for each element of (αc \ αG), as once a non-terminal node is on the stack, there is no way to reach a final configuration without using it to create an erroneous constituent.",3.3 Correctness,[0],[0]
"Note that these erroneous constituents do not overlap those arising from the previous item, as γc stores already-built constituents and αc non-terminals that have still not been used to build a constituent.",3.3 Correctness,[0],[0]
•,3.3 Correctness,[0],[0]
"Given a subset S of R(c, γG), the previously
shown constituent decomposability property implies that there exists at least one transition
sequence starting from c that generates the tree S∪(γc\γG)∪E, whereE is a set of erroneous constituents containing one such constituent per element of (αc \ αG).",3.3 Correctness,[0],[0]
This tree has loss |tG|−(|γc∪S|)+|γc\γG|+|αc\αG|.,3.3 Correctness,[0],[0]
"The term |tG| − (|γc ∪ S|) corresponds to missed constituents (gold constituents that have not been already created and are not created as part of S), the other two to erroneous constituents.",3.3 Correctness,[0],[0]
"• As we have shown that the erroneous con-
stituents arising from (γc′ \γG) and (αc\αG) are unavoidable, computations yielding a tree with minimum loss are those that maximize |γc ∪ S| in the previous term.",3.3 Correctness,[0],[0]
"In general, the largest possible |S| is for S = R(c, γG).",3.3 Correctness,[0],[0]
"In that case, we would correctly generate every reachable constituent and the loss would be
`(c) = |U(c, γG)|+ |γc \ γG|
+|αc \",3.3 Correctness,[0],[0]
"αG|
However, we additionally want to generate constituents in the correct order, and this may not be possible if we have already shifted some of them into the stack in a wrong order.",3.3 Correctness,[0],[0]
The function out of order gives us the number of reachable constituents that are lost for this cause in the best case.,3.3 Correctness,[0],[0]
"Thus, indeed, the expression
`(c) = |U(c, γG)|+ |γc \ γG|
+|αc \ αG|+ out of order(αc, αG)
provides the minimum loss from configuration c.",3.3 Correctness,[0],[0]
"We test the two proposed approaches on two widely-used benchmarks for constituent parsers: the Wall Street Journal (WSJ) sections of the English Penn Treebank5 (Marcus et al., 1993) and version 5.1 of the Penn Chinese Treebank (CTB)6 (Xue et al., 2005).",4.1 Data,[0],[0]
"We use the same predicted POS tags and pre-trained word embeddings as Dyer et al. (2016) and Liu and Zhang (2017a).
",4.1 Data,[0],[0]
"5Sections 2-21 are used as training data, Section 22 for development and Section 23 for testing
6Articles 001- 270 and 440-1151 are taken for training, articles 301-325 for system development, and articles 271- 300 for final testing",4.1 Data,[0],[0]
"To perform a fair comparison, we define the novel dynamic oracles on the original implementations of the top-down parser by Dyer et al. (2016) and in-order parser by Liu and Zhang (2017a), where parsers are trained with a traditional static oracle.",4.2 Neural Model,[0],[0]
"Both implementations follow a stack-LSTM approach to represent the stack and the buffer, as well as a vanilla LSTM to represent the action history.",4.2 Neural Model,[0],[0]
"In addition, they also use a bi-LSTM as a compositional function for representing constituents in the stack.",4.2 Neural Model,[0],[0]
"Concretely, this consists in computing the composition representation scomp as:
scomp = (LSTMfwd[ent, s0, ..., sm];
LSTMbwd[ent, sm, ..., s0])
where ent is the vector representation of a nonterminal, and si, i ∈",4.2 Neural Model,[0],[0]
"[0,m] is the ith child node.
",4.2 Neural Model,[0],[0]
"Finally, the exact same word representation strategy and hyper-parameter values as (Dyer et al., 2016) and (Liu and Zhang, 2017a) are used to conduct the experiments.",4.2 Neural Model,[0],[0]
"In order to benefit from training a parser by a dynamic oracle, errors should be made during the training process so that the parser can learn to avoid and recover from them.",4.3 Error exploration,[0],[0]
"Unlike more complex error-exploration strategies as those studied in (Ballesteros et al., 2016; Cross and Huang, 2016b; Fried and Klein, 2018), we decided to consider a simple one that follows a non-optimal transition when it is the highest-scoring one, but with a certain probability.",4.3 Error exploration,[0],[0]
"In that way, we easily simulate test time conditions, when the parser greedily chooses the highest-scoring transition, even when it is not an optimal one, placing the parser in an incorrect state.
",4.3 Error exploration,[0],[0]
"In particular, we run experiments on development sets for each benchmark/algorithm with three different error exploration probabilities and choose the one that achieves the best F-score.",4.3 Error exploration,[0],[0]
"Table 1 reports all results, including those obtained by the top-down and in-order parsers trained by a dynamic oracle without error exploration (equivalent to a traditional static oracle).",4.3 Error exploration,[0],[0]
Table 2 compares our system’s accuracy to other state-of-the-art shift-reduce constituent parsers on the WSJ and CTB benchmarks.,4.4 Results,[0],[0]
"For comparison,
we also include some recent state-of-the-art parsers with global chart decoding that achieve the highest accuracies to date on WSJ, but are much slower than shift-reduce algorithms.
",4.4 Results,[0],[0]
Top-down and in-order parsers benefit from being trained by these new dynamic oracles in both datasets.,4.4 Results,[0],[0]
"The top-down strategy achieves a gain of 0.5 and 0.7 points in F-score on WSJ and CTB benchmarks, respectively.",4.4 Results,[0],[0]
"The in-order parser obtains similar improvements on the CTB (0.5 points), but less notable accuracy gain on the WSJ (0.2 points).",4.4 Results,[0],[0]
"Although a case of diminishing returns might explain the latter, the in-order parser trained with the proposed dynamic oracle still achieves the highest accuracy to date in greedy transition-based constituent parsing on the WSJ.7
While this work was under review, Fried and Klein (2018) proposed to train the top-down and in-order parsers with a policy gradient method instead of custom designed dynamic oracles.",4.4 Results,[0],[0]
"They also present a non-optimal dynamic oracle for the top-down parser that, combined with more complex error-exploration strategies and size-10 beam search, significantly outperforms the policy gradient-trained version, confirming that even non-optimal dynamic oracles are a good option.8",4.4 Results,[0],[0]
"Dan Bikel’s randomized parsing evaluation comparator (Bikel, 2004) was used to perform significance tests on precision and recall metrics on WSJ §23 and CTB §271-300.",4.5 Analysis,[0],[0]
"The top-down parser trained with dynamic oracles achieves statistically significant improvements (p < 0.05) in precision
7Note that the proposed dynamic oracles are orthogonal to approaches like beam search, re-ranking or semi-supervision, that can boost accuracy but at a large cost to parsing speed.
",4.5 Analysis,[0],[0]
"8Unfortunately, we cannot directly compare our approach to theirs, since they use beam-search decoding with size 10 in all experiments, gaining up to 0.3 points in F-score, while penalizing speed with respect to greedy decoding.",4.5 Analysis,[0],[0]
"However, by extrapolating the results above, we hypothesize that our optimal dynamic oracles (especially the one designed for the in-order algorithm) with their same training and beam-search decoding setup might achieve the best scores to date in shiftreduce parsing.
",4.5 Analysis,[0],[0]
"both on the WSJ and CTB benchmarks, and in recall on WSJ.",4.5 Analysis,[0],[0]
"The in-order parser trained with the proposed technique obtains significant improvements (p < 0.05) in recall in both benchmarks, although not in precision.
",4.5 Analysis,[0],[0]
We also undertake an analysis to check if dynamic oracles are able to mitigate error propagation.,4.5 Analysis,[0],[0]
We report in Table 3 the F-score obtained in constituents with different number of children on WSJ §23 by the top-down and in-order algorithms trained with both static and dynamic oracles.,4.5 Analysis,[0],[0]
"Please note that creating a constituent with a great number of children is more prone to suffer from error propagation, since a larger number of transitions is required to build it.",4.5 Analysis,[0],[0]
"The results seem to confirm that, indeed, dynamic oracles manage to alleviate error propagation, since improvements in F-score are more notable for larger constituents.",4.5 Analysis,[0],[0]
We develop the first optimal dynamic oracles for training the top-down and the state-of-the-art inorder parsers.,5 Conclusion,[0],[0]
"Apart from improving the systems’ accuracies in both cases, we achieve the best result to date in greedy shift-reduce parsing on the WSJ.",5 Conclusion,[0],[0]
"In addition, these promising techniques could easily benefit from recent studies in error-exploration strategies and yield stateof-the-art accuracies in transition-based parsing in the near future.",5 Conclusion,[0],[0]
The parser’s source code is freely available at https://github.com/ danifg/Dynamic-InOrderParser.,5 Conclusion,[0],[0]
"This work has received funding from the European Research Council (ERC), under the European Union’s Horizon 2020 research and innovation programme (FASTPARSE, grant agreement No 714150), from MINECO (FFI2014-51978-C2-2R, TIN2017-85160-C2-1-R) and from Xunta de Galicia (ED431B 2017/01).",Acknowledgments,[0],[0]
We introduce novel dynamic oracles for training two of the most accurate known shiftreduce algorithms for constituent parsing: the top-down and in-order transition-based parsers.,abstractText,[0],[0]
"In both cases, the dynamic oracles manage to notably increase their accuracy, in comparison to that obtained by performing classic static training.",abstractText,[0],[0]
"In addition, by improving the performance of the state-of-the-art in-order shift-reduce parser, we achieve the best accuracy to date (92.0 F1) obtained by a fullysupervised single-model greedy shift-reduce constituent parser on the WSJ benchmark.",abstractText,[0],[0]
Dynamic Oracles for Top-Down and In-Order Shift-Reduce Constituent Parsing,title,[0],[0]
"Online convex optimization is a powerful paradigm for sequential decision making (Zinkevich, 2003).",1. Introduction,[0],[0]
"It can be viewed as a game between a learner and an adversary: In the t-th round, the learner selects a decision wt ∈ Ω, simultaneously the adversary chooses a function ft(·) : Ω",1. Introduction,[0],[0]
"7→ R, and then the learner suffers an instantaneous loss ft(wt).",1. Introduction,[0],[0]
"This study focuses on the full-information setting, where the learner can query the value and gradient of ft (Cesa-Bianchi & Lugosi, 2006).",1. Introduction,[0],[0]
The goal of the learner is to minimize the cumulative loss over T periods .,1. Introduction,[0],[0]
"The standard performance measure is regret, which is the difference between the loss
1National Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China 2Department of Computer Science, The University of Iowa, Iowa City, USA 3Alibaba Group, Seattle, USA.",1. Introduction,[0],[0]
"Correspondence to: Lijun Zhang <zhanglj@lamda.nju.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
incurred by the learner and that of the best fixed decision in hindsight, i.e.,
Regret(T ) =",1. Introduction,[0],[0]
"T∑ t=1 ft(wt)− min w∈Ω T∑ t=1 ft(w).
",1. Introduction,[0],[0]
The above regret is typically referred to as static regret in the sense that the comparator is time-invariant.,1. Introduction,[0],[0]
The rationale behind this evaluation metric is that one of the decision in Ω is reasonably good over the T rounds.,1. Introduction,[0],[0]
"However, when the underlying distribution of loss functions changes, the static regret may be too optimistic and fails to capture the hardness of the problem.
",1. Introduction,[0],[0]
"To address this limitation, new forms of performance measure, including adaptive regret (Hazan & Seshadhri, 2007; 2009) and dynamic regret (Zinkevich, 2003; Hall & Willett, 2013), were proposed and received significant interest recently.",1. Introduction,[0],[0]
"Following the terminology of Daniely et al. (2015), we define the strongly adaptive regret as the maximum static regret over intervals of length τ , i.e.,
SA-Regret(T, τ)
",1. Introduction,[0],[0]
"= max [s,s+τ−1]⊆[T ] ( s+τ−1∑ t=s ft(wt)− min w∈Ω s+τ−1∑ t=s ft(w) ) .
",1. Introduction,[0],[0]
"(1)
Minimizing the adaptive regret enforces the learner to have a small static regret over any interval of length τ .",1. Introduction,[0],[0]
"Since the best decision for different intervals could be different, the learner is essentially competing with a changing comparator.
",1. Introduction,[0],[0]
"A parallel line of research introduces the concept of dynamic regret, where the cumulative loss of the learner is compared against a comparator sequence u1, . . .",1. Introduction,[0],[0]
",uT ∈ Ω, i.e.,
D-Regret(u1, . . .",1. Introduction,[0],[0]
",uT ) =",1. Introduction,[0],[0]
T∑ t=1 ft(wt)− T∑ t=1 ft(ut).,1. Introduction,[0],[0]
"(2)
It is well-known that in the worst case, a sublinear dynamic regret is impossible unless we impose some regularities on the comparator sequence or the function sequence (Jadbabaie et al., 2015).",1. Introduction,[0],[0]
"A representative example is the functional variation defined below
VT = T∑ t=2 max w∈Ω |ft(w)− ft−1(w)|.",1. Introduction,[0],[0]
"(3)
Besbes et al. (2015) have proved that as long as VT is sublinear in T , there exists an algorithm that achieves a sublinear dynamic regret.",1. Introduction,[0],[0]
"Furthermore, a general restarting procedure is developed, and it enjoys O(T 2/3V 1/3T ) andO(log T √ TVT ) rates for convex functions and strongly convex functions, respectively.",1. Introduction,[0],[0]
"However, the restarting procedure can only be applied when an upper bound of VT is known beforehand, thus limiting its application in practice.
",1. Introduction,[0],[0]
"While both the adaptive and dynamic regrets aim at coping with changing environments, little is known about their relationship.",1. Introduction,[0],[0]
This paper makes a step towards understanding their connections.,1. Introduction,[0],[0]
"Specifically, we show that the strongly adaptive regret in (1), together with the functional variation, can be used to upper bound the dynamic regret in (2).",1. Introduction,[0],[0]
"Thus, an algorithm with a small strongly adaptive regret is automatically equipped with a tight dynamic regret.",1. Introduction,[0],[0]
"As a result, we obtain a series of algorithms for minimizing the dynamic regret that do not need any prior knowledge of the functional variation.",1. Introduction,[0],[0]
"The main contributions of this work are summarized below.
",1. Introduction,[0],[0]
• We provide a general theorem that upper bounds the dynamic regret in terms of the strongly adaptive regret and the functional variation.,1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For convex functions, we show that the strongly adaptive algorithm of Jun et al. (2017) has a dynamic regret of O(T 2/3V 1/3T log
1/3 T ), which matches the minimax rate (Besbes et al., 2015), up to a polylogarithmic factor.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For exponentially concave functions, we propose a strongly adaptive algorithm that allows us to control the tradeoff between the adaptive regret and the computational cost explicitly.",1. Introduction,[0],[0]
"Then, we demonstrate that its dynamic regret is O(d √ TVT log T ), where d is the
dimensionality.",1. Introduction,[0],[0]
"To the best of our knowledge, this is the first time that exponential concavity is utilized in the analysis of dynamic regret.",1. Introduction,[0],[0]
•,1. Introduction,[0],[0]
"For strongly convex functions, our proposed algorithm can also be applied and yields a dynamic regret of O( √ TVT log T ), which is also minimax optimal up to
a polylogarithmic factor.",1. Introduction,[0],[0]
"We give a brief introduction to previous work on static, adaptive, and dynamic regrets in the context of online convex optimization.",2. Related Work,[0],[0]
"The majority of studies in online learning are focused on static regret (Shalev-Shwartz & Singer, 2007; Langford et al., 2009; Shalev-Shwartz, 2011; Zhang et al., 2013).",2.1. Static Regret,[0],[0]
"For general convex functions, the classical online gradient
descent achieves O( √ T ) and O(log T ) regret bounds for convex and strongly convex functions, respectively (Zinkevich, 2003; Hazan et al., 2007; Shalev-Shwartz et al., 2007).",2.1. Static Regret,[0],[0]
"Both the O( √ T ) and O(log T ) rates are known to be minimax optimal (Abernethy et al., 2009).",2.1. Static Regret,[0],[0]
"When functions are exponentially concave, a different algorithm, named online Newton step, is developed and enjoys an O(d log T ) regret bound, where d is the dimensionality (Hazan et al., 2007).",2.1. Static Regret,[0],[0]
"The concept of adaptive regret is introduced by Hazan & Seshadhri (2007), and later strengthened by Daniely et al. (2015).",2.2. Adaptive Regret,[0],[0]
"Specifically, Hazan & Seshadhri (2007) introduce the weakly adaptive regret
WA-Regret(T )
",2.2. Adaptive Regret,[0],[0]
"= max [s,q]⊆[T ] ( q∑ t=s ft(wt)− min w∈Ω q∑ t=s ft(w) ) .
",2.2. Adaptive Regret,[0],[0]
"To minimize the adaptive regret, Hazan & Seshadhri (2007) have developed two meta-algorithms: an efficient algorithm with O(log T ) computational complexity per iteration and an inefficient one with O(T ) computational complexity per iteration.",2.2. Adaptive Regret,[0],[0]
"These meta-algorithms use an existing online method (that was possibly designed to have small static regret) as a subroutine.1 For convex functions, the efficient and inefficient meta-algorithms have O( √ T log3 T )
and O( √ T log T ) regret bounds, respectively.",2.2. Adaptive Regret,[0],[0]
"For exponentially concave functions, those rates are improved to O(d log2 T ) and O(d log T ), respectively.",2.2. Adaptive Regret,[0],[0]
"We can see that the price paid for the adaptivity is very small: The rates of weakly adaptive regret differ from those of static regret only by logarithmic factors.
",2.2. Adaptive Regret,[0],[0]
A major limitation of weakly adaptive regret is that it does not respect short intervals well.,2.2. Adaptive Regret,[0],[0]
"Taking convex functions as an example, the O( √ T log3 T ) and O( √ T log T ) bounds are meaningless for intervals of length O( √ T ).",2.2. Adaptive Regret,[0],[0]
"To overcome this limitation, Daniely et al. (2015) proposed the strongly adaptive regret SA-Regret(T, τ) which takes the length of the interval τ as a parameter, as indicated in (1).",2.2. Adaptive Regret,[0],[0]
"From the definitions, we have SA-Regret(T, τ) ≤ WA-Regret(T ), but it does not mean the notation of weakly adaptive regret is stronger, because an upper bound for WA-Regret(T ) could be very loose for SA-Regret(T, τ) when τ is small.
",2.2. Adaptive Regret,[0],[0]
"If the strongly adaptive regret is small for all τ < T , we can guarantee the learner has a small regret over any interval of
1For brevity, we ignored the factor of subroutine in the statements of computational complexities.",2.2. Adaptive Regret,[0],[0]
"The O(·) computational complexity should be interpreted as O(·) × s space complexity and O(·)× t time complexity, where s and t are space and time complexities of the subroutine per iteration, respectively.
any length.",2.2. Adaptive Regret,[0],[0]
"In particular, Daniely et al. (2015) introduced the following definition.
",2.2. Adaptive Regret,[0],[0]
Definition 1 Let R(τ) be the minimax static regret bound of the learning problem over τ periods.,2.2. Adaptive Regret,[0],[0]
"An algorithm is strongly adaptive, if
SA-Regret(T, τ) = O(poly(log T ) ·R(τ)), ∀τ.
",2.2. Adaptive Regret,[0],[0]
"It is easy to verify that the meta-algorithms of Hazan & Seshadhri (2007) are strongly adaptive for exponentially concave functions,2 but not for convex functions.",2.2. Adaptive Regret,[0],[0]
"Thus, Daniely et al. (2015) developed a new meta-algorithm that satisfies SA-Regret(T, τ) =",2.2. Adaptive Regret,[0],[0]
"O( √ τ log T ) for convex functions, and thus is strongly adaptive.",2.2. Adaptive Regret,[0],[0]
The algorithm is also efficient and the computational complexity per iteration is O(log T ).,2.2. Adaptive Regret,[0],[0]
"Later, the strongly adaptive regret of convex functions was improved to O( √ τ log T ) by Jun et al. (2017), and the computational complexity remains O(log T ) per iteration.",2.2. Adaptive Regret,[0],[0]
All the previously mentioned algorithms for minimizing adaptive regret need to query the gradient of the loss function at least O(log t) times in the t-th iteration.,2.2. Adaptive Regret,[0],[0]
"In a recent study, Wang et al. (2018) demonstrate that the number of gradient evaluations per iteration can be reduced to 1 by introducing the surrogate loss.",2.2. Adaptive Regret,[0],[0]
"In a seminal work, Zinkevich (2003) proposed to use the path-length defined as
P(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
T∑ t=2 ‖ut,2.3. Dynamic Regret,[0],[0]
"− ut−1‖2
to upper bound the dynamic regret, where u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ∈ Ω is a comparator sequence.",2.3. Dynamic Regret,[0],[0]
"Specifically, Zinkevich (2003) proved that for any sequence of convex functions, the dynamic regret of online gradient descent can be upper bounded by O( √ TP(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT )).",2.3. Dynamic Regret,[0],[0]
"Another regularity of the comparator sequence, which is similar to the path-length, is defined as
P ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
T∑ t=2 ‖ut,2.3. Dynamic Regret,[0],[0]
"− Φt(ut−1)‖2
where Φt(·) is a dynamic model that predicts a reference point for the t-th round.",2.3. Dynamic Regret,[0],[0]
"Hall & Willett (2013) developed a novel algorithm named dynamic mirror descent and proved that its dynamic regret is on the order of√ TP ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ).",2.3. Dynamic Regret,[0],[0]
"The advantage of P ′(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) is that when the comparator sequence follows the dynamical
2That is because (i) SA-Regret(T, τ) ≤ WA-Regret(T ), and (ii) there is a poly(log T ) factor in the definition of strong adaptivity.
",2.3. Dynamic Regret,[0],[0]
"model closely, it can be much smaller than the path-length P(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ).
",2.3. Dynamic Regret,[0],[0]
Let w∗t,2.3. Dynamic Regret,[0],[0]
∈ argminw∈Ω ft(w) be a minimizer of ft(·).,2.3. Dynamic Regret,[0],[0]
"For any sequence of u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ∈ Ω, we have
D-Regret(u1, . . .",2.3. Dynamic Regret,[0],[0]
",uT ) =",2.3. Dynamic Regret,[0],[0]
"T∑ t=1 ft(wt)− T∑ t=1 ft(ut)
",2.3. Dynamic Regret,[0],[0]
"≤D-Regret(w∗1, . .",2.3. Dynamic Regret,[0],[0]
.,2.3. Dynamic Regret,[0],[0]
",w∗T ) = T∑ t=1 ft(wt)− T∑ t=1",2.3. Dynamic Regret,[0],[0]
"min w∈Ω ft(w).
",2.3. Dynamic Regret,[0],[0]
"Thus, D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) can be treated as the worst case of the dynamic regret, and there are many works that were devoted to minimizing D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) (Jadbabaie et al., 2015; Mokhtari et al., 2016; Yang et al., 2016; Zhang et al., 2017).
",2.3. Dynamic Regret,[0],[0]
"When a prior knowledge of P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) is available, D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) can be upper bounded by
O( √ TP(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"(Yang et al., 2016).",2.3. Dynamic Regret,[0],[0]
"If all the functions are strongly convex and smooth, the upper bound can be improved to O(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"(Mokhtari et al., 2016).",2.3. Dynamic Regret,[0],[0]
"The O(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T )) rate is also achievable when all the functions are convex and smooth, and all the minimizers w∗t ’s lie in the interior of Ω (Yang et al., 2016).",2.3. Dynamic Regret,[0],[0]
"In a recent study, Zhang et al. (2017) introduced a new regularity—squared path-length
S(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) = T∑ t=2 ‖w∗t −w∗t−1‖22
which could be much smaller than the path-length P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ) when the difference between successive minimizers is small.",2.3. Dynamic Regret,[0],[0]
"Zhang et al. (2017) developed a novel algorithm named online multiple gradient descent, and proved that D-Regret(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w ∗ T ) is on the order of min(P(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ),S(w∗1, . . .",2.3. Dynamic Regret,[0],[0]
",w∗T ))",2.3. Dynamic Regret,[0],[0]
"for (semi-) strongly convex and smooth functions.
Discussions Although closely related, adaptive regret and dynamic regret are studied independently and there are few discussions of their relationships.",2.3. Dynamic Regret,[0],[0]
"In the literature, dynamic regret is also referred to as tracking regret or shifting regret (Littlestone & Warmuth, 1994; Herbster & Warmuth, 1998; 2001).",2.3. Dynamic Regret,[0],[0]
"In the setting of “prediction with expert advice”, Adamskiy et al. (2012) have shown that the tracking regret can be derived from the adaptive regret.",2.3. Dynamic Regret,[0],[0]
"In the setting of “online linear optimization in the simplex”, Cesa-bianchi et al. (2012) introduced a generalized notion of shifting regret which unifies adaptive regret and shifting regret.",2.3. Dynamic Regret,[0],[0]
"Different from previous work, this paper considers the setting of online convex optimization, and illustrates that the dynamic regret can be upper bounded by the adaptive regret and the functional variation.",2.3. Dynamic Regret,[0],[0]
"In this section, we introduce a unified approach for minimizing the adaptive regret of exponentially concave functions, as well as strongly convex functions.",3. A Unified Adaptive Algorithm,[0],[0]
We first provide the definition of exponentially concave (abbr.,3.1. Motivation,[0],[0]
"exp-concave) functions (Cesa-Bianchi & Lugosi, 2006).
",3.1. Motivation,[0],[0]
Definition 2,3.1. Motivation,[0],[0]
A function f(·) : Ω,3.1. Motivation,[0],[0]
"7→ R is α-exp-concave if exp(−αf(·)) is concave over Ω.
For exp-concave functions, Hazan & Seshadhri (2007) have developed two meta-algorithms that take the online Newton step as its subroutine, and proved the following properties.
",3.1. Motivation,[0],[0]
"• The inefficient one has O(T ) computational complexity per iteration, and its adaptive regret is O(d log T ).",3.1. Motivation,[0],[0]
•,3.1. Motivation,[0],[0]
"The efficient one hasO(log T ) computational complexity per iteration, and its adaptive regret is O(d log2 T ).
",3.1. Motivation,[0],[0]
"As can be seen, there is a tradeoff between the computational complexity and the adaptive regret: A lighter computation incurs a looser bound and a tighter bound requires a higher computation.",3.1. Motivation,[0],[0]
"Our goal is to develop a unified approach, that allows us to trade effectiveness for efficiency explicitly.",3.1. Motivation,[0],[0]
"Let E be an online learning algorithm that is designed to minimize the static regret of exp-concave functions or strongly convex functions, e.g., online Newton step (Hazan et al., 2007) or online gradient descent (Zinkevich, 2003).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Similar to the approach of following the leading history (FLH) (Hazan & Seshadhri, 2007), at any time t, we will instantiate an expert by applying the online learning algorithm E to the sequence of loss functions ft, ft+1, . . ., and utilize the strategy of learning from expert advice to combine solutions of different experts (Herbster & Warmuth, 1998).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Our method is named as improved following the leading history (IFLH), and is summarized in Algorithm 1.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let Et be the expert that starts to work at time t. To control the computational complexity, we will associate an ending time et for eachEt.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"The expertEt is alive during the period [t, et − 1].",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In each round t, we maintain a working set of experts St, which contains all the alive experts, and assign a probability pjt for each E
j ∈ St.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Steps 6 and 7, we remove all the experts whose ending times are no larger than t. Since the number of alive experts has changed, we need to update the probability assigned to them, which is performed in Steps 12 to 14.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Steps 15 and 16, we add a new expert Et to St, calculate its ending time according to Definition 3 introduced below, and set ptt = 1 t .",3.2. Improved Following the Leading History (IFLH),[0],[0]
"It is easy
Algorithm 1 Improved Following the Leading History (IFLH)
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"1: Input: An integer K 2: Initialize S0 = ∅. 3: for t = 1, . . .",3.2. Improved Following the Leading History (IFLH),[0],[0]
", T do 4: Set Zt = 0 {Remove some existing experts} 5: for Ej ∈ St−1 do 6: if ej ≤ t then 7: Update St−1 ← St−1 \ {Ej} 8: else 9: Set Zt = Zt + p̂ j t
10: end if 11: end for {Normalize the probability} 12:",3.2. Improved Following the Leading History (IFLH),[0],[0]
"for Ej ∈ St−1 do 13: Set pjt = p̂jt Zt ( 1− 1t ) 14: end for {Add a new expert Et} 15: Set St = St−1 ∪ {Et} 16: Compute the ending time et = EK(t) according to
Definition 3 and set ptt = 1 t
{Compute the final predicted model} 17: Submit the solution
wt = ∑ Ej∈St pjtw j t
and suffer loss ft(wt) {Update weights and expert}
18: Set Zt+1 = 0 19: for Ej ∈ St do 20: Compute pjt+1 = p j t exp(−αft(w j t )) and Zt+1 =
Zt+1 + p j t+1
21: Pass the function ft(·) to Ej 22: end for 23: for Ej ∈ St do 24: Set p̂jt+1 = pjt+1 Zt+1 25: end for 26: end for
to verify ∑ Ej∈St p j t = 1.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let w j t be the output of E
j at the t-th round, where t ≥ j. In Step 17, we submit the weighted average of wjt with coefficient p j t as the output wt, and suffer the loss ft(wt).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"From Steps 18 to 25, we use the exponential weighting scheme to update the weight for each expert Ej based on its loss ft(w j t ).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In Step 21, we pass the loss function to all the alive experts such that they can update their predictions for the next round.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
The difference between our IFLH and the original FLH is how to decide the ending time et of expert Et.,3.2. Improved Following the Leading History (IFLH),[0],[0]
"In this paper, we propose the following base-K ending time.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Definition 3 (Base-K Ending Time) Let K be an integer, and the representation of t in the base-K number system as
t = ∑ τ≥0 βτK τ
where 0 ≤ βτ < K, for all τ ≥ 0.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let k be the smallest integer such that βk > 0, i.e., k = min{τ : βτ > 0}.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Then, the base-K ending time of t is defined as
EK(t) =",3.2. Improved Following the Leading History (IFLH),[0],[0]
"∑
τ≥k+1
βτK τ +Kk+1.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"In other words, the ending time is the number represented by the new sequence obtained by setting the first nonzero element in the sequence β0, β1, . . .",3.2. Improved Following the Leading History (IFLH),[0],[0]
"to be 0 and adding 1 to the element after it.
",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Let’s take the decimal system as an example (i.e., K = 10).",3.2. Improved Following the Leading History (IFLH),[0],[0]
"Then,
E10(1) = E10(2) = · · · = E10(9) = 10, E10(11) = E10(12)",3.2. Improved Following the Leading History (IFLH),[0],[0]
= · · ·,3.2. Improved Following the Leading History (IFLH),[0],[0]
"= E10(19) = 20, E10(10) = E10(20) = · · · = E10(90) = 100.",3.2. Improved Following the Leading History (IFLH),[0],[0]
"When the base-K ending time is used in Algorithm 1, we have the following properties.
",3.3. Theoretical Guarantees,[0],[0]
"Lemma 1 Suppose we use the base-K ending time in Algorithm 1.
1.",3.3. Theoretical Guarantees,[0],[0]
"For any t ≥ 1, we have |St| ≤ (blogK tc+ 1) (K − 1) =",3.3. Theoretical Guarantees,[0],[0]
"O ( K log t
logK
) .
",3.3. Theoretical Guarantees,[0],[0]
2.,3.3. Theoretical Guarantees,[0],[0]
For any interval I =,3.3. Theoretical Guarantees,[0],[0]
"[r, s] ⊆",3.3. Theoretical Guarantees,[0],[0]
"[T ], we can always find m segments",3.3. Theoretical Guarantees,[0],[0]
Ij =,3.3. Theoretical Guarantees,[0],[0]
"[tj , etj",3.3. Theoretical Guarantees,[0],[0]
"− 1], j ∈",3.3. Theoretical Guarantees,[0],[0]
"[m] with m ≤ dlogK(s− r + 1)e+ 1, such that t1 = r, etj = tj+1, j ∈",3.3. Theoretical Guarantees,[0],[0]
"[m− 1], and",3.3. Theoretical Guarantees,[0],[0]
"etm > s.
The first part of Lemma 1 implies that the size of St is O(K log t/ logK).",3.3. Theoretical Guarantees,[0],[0]
"An example of St in the decimal system is given below.
",3.3. Theoretical Guarantees,[0],[0]
"S486 =  481, 482, . . .",3.3. Theoretical Guarantees,[0],[0]
", 486, 410, 420, . . .",3.3. Theoretical Guarantees,[0],[0]
", 480,
100, 200, . . .",3.3. Theoretical Guarantees,[0],[0]
", 400  .",3.3. Theoretical Guarantees,[0],[0]
The second part of Lemma 1 implies that for any interval I =,3.3. Theoretical Guarantees,[0],[0]
"[r, s], we can find O(log s/ logK) experts such that their survival periods cover I .",3.3. Theoretical Guarantees,[0],[0]
"Again, we present an example in the decimal system: The interval [111, 832] can be covered by
[111, 119],",3.3. Theoretical Guarantees,[0],[0]
"[120, 199], and [200, 999]
which are the survival periods of experts E111, E120, and E200, respectively.",3.3. Theoretical Guarantees,[0],[0]
"Recall that E10(111) = 120, E10(120) = 200, and E10(200) = 1000.
",3.3. Theoretical Guarantees,[0],[0]
We note that a similar strategy for deciding the ending time was proposed by György et al. (2012) in the study of “prediction with expert advice”.,3.3. Theoretical Guarantees,[0],[0]
"The main difference is that their strategy is built upon base-2 number system and introduces an additional parameter g to compromise between the computational complexity and the regret, in contrast our method relies on base-K number system and uses K to control the tradeoff.",3.3. Theoretical Guarantees,[0],[0]
"Lemma 2 of György et al. (2012) indicates an O(g log t) bound on the number of alive experts, which is worse than our O(K log t/ logK) bound by a logarithmic factor.
",3.3. Theoretical Guarantees,[0],[0]
"To present adaptive regret bounds, we introduce the following common assumption.
",3.3. Theoretical Guarantees,[0],[0]
"Assumption 1 Both the gradient and the domain are bounded.
",3.3. Theoretical Guarantees,[0],[0]
"• The gradients of all the online functions are bounded by G, i.e., maxw∈Ω ‖∇ft(w)‖ ≤ G for all ft. •",3.3. Theoretical Guarantees,[0],[0]
"The diameter of the domain Ω is bounded by B, i.e., maxw,w′∈Ω ‖w",3.3. Theoretical Guarantees,[0],[0]
−w′‖ ≤,3.3. Theoretical Guarantees,[0],[0]
"B.
Based on Lemma 1, we have the following theorem regarding the adaptive regret of exp-concave functions.
",3.3. Theoretical Guarantees,[0],[0]
"Theorem 1 Suppose Assumption 1 holds, Ω ⊂ Rd, and all the functions are α-exp-concave.",3.3. Theoretical Guarantees,[0],[0]
"If online Newton step is used as the subroutine in Algorithm 1, we have
s∑ t=r ft(wt)− min w∈Ω s∑ t=r ft(w)
≤",3.3. Theoretical Guarantees,[0],[0]
"( (5d+ 1)m+ 2
α + 5dmGB
) log T
where [r, s]",3.3. Theoretical Guarantees,[0],[0]
⊆,3.3. Theoretical Guarantees,[0],[0]
[T ] and m ≤ dlogK(s− r + 1)e+ 1.,3.3. Theoretical Guarantees,[0],[0]
"Thus,
SA-Regret(T, τ) ≤ ( (5d+ 1)m̄+ 2
α + 5dm̄GB
) log T =",3.3. Theoretical Guarantees,[0],[0]
"O ( d log2 T
logK ) where m̄ = dlogK τe+ 1.
",3.3. Theoretical Guarantees,[0],[0]
"From Lemma 1 and Theorem 1, we observe that the adaptive regret is a decreasing function of K, while the computational cost is an increasing function of K.",3.3. Theoretical Guarantees,[0],[0]
"Thus, we can control the tradeoff by tuning the value of K. Specifically, Lemma 1 indicates the proposed algorithm has
(blogK T c+ 1) (K − 1) =",3.3. Theoretical Guarantees,[0],[0]
"O ( K log T
logK ) computational complexity per iteration.",3.3. Theoretical Guarantees,[0],[0]
"On the other hand, Theorem 1 implies that for α-exp-concave functions that
satisfy Assumption 1, the strongly adaptive regret of Algorithm 1 is(
(5d+ 1)m̄+ 2
α + 5dm̄GB
) log T = O ( d log2 T
logK ) where d is the dimensionality and m̄ = dlogK(τ)e+ 1.
",3.3. Theoretical Guarantees,[0],[0]
"We list several choices of K and the resulting theoretical guarantees in Table 1, and have the following observations.
",3.3. Theoretical Guarantees,[0],[0]
"• When K = 2, we recover the guarantee of the efficient algorithm of Hazan & Seshadhri (2007), and when K = T , we obtain the inefficient one.",3.3. Theoretical Guarantees,[0],[0]
•,3.3. Theoretical Guarantees,[0],[0]
"By setting K = dT 1/γe where γ > 1 is a small constant, such as 10, the strongly adaptive regret can be viewed as O(d log T ), and at the same time, the computational complexity is also very low for a large range of T .
",3.3. Theoretical Guarantees,[0],[0]
"Next, we consider strongly convex functions.
",3.3. Theoretical Guarantees,[0],[0]
Definition 4,3.3. Theoretical Guarantees,[0],[0]
A function f(·) :,3.3. Theoretical Guarantees,[0],[0]
"Ω 7→ R is λ-strongly convex if
f(y) ≥ f(x)+ 〈∇f(x),y − x〉+ λ 2 ‖y−x‖22, ∀x,y ∈",3.3. Theoretical Guarantees,[0],[0]
"Ω.
It is easy to verify that strongly convex functions with bounded gradients are also exp-concave (Hazan et al., 2007).
",3.3. Theoretical Guarantees,[0],[0]
Lemma 2 Suppose f(·) :,3.3. Theoretical Guarantees,[0],[0]
Ω 7→ R is λ-strongly convex and ‖∇f(w)‖ ≤ G for all w ∈,3.3. Theoretical Guarantees,[0],[0]
"Ω. Then, f(·) is λG2 -expconcave.
",3.3. Theoretical Guarantees,[0],[0]
"According to the above lemma, we still use Algorithm 1 as the meta-algorithm, but choose online gradient descent as the subroutine.",3.3. Theoretical Guarantees,[0],[0]
"In this way, the adaptive regret does not depend on the dimensionality d.
Theorem 2 Suppose Assumption 1 holds, and all the functions are λ-strongly convex.",3.3. Theoretical Guarantees,[0],[0]
"If online gradient descent is used as the subroutine in Algorithm 1, we have s∑ t=r ft(wt)− min w∈Ω s∑ t=r ft(w)",3.3. Theoretical Guarantees,[0],[0]
"≤ G2 2λ ( m+ (3m+ 4) log T )
",3.3. Theoretical Guarantees,[0],[0]
"where [r, s] ⊆",3.3. Theoretical Guarantees,[0],[0]
[T ] and m ≤ dlogK(s− r + 1)e+ 1.,3.3. Theoretical Guarantees,[0],[0]
"Thus
SA-Regret(T, τ)
≤G 2
2λ
( m̄+ (3m̄+ 4) log T ) =",3.3. Theoretical Guarantees,[0],[0]
"O
( log2 T
logK ) where m̄ = dlogK τe+ 1.",3.3. Theoretical Guarantees,[0],[0]
"In this section, we first introduce a general theorem that bounds the dynamic regret by the adaptive regret, and then derive specific regret bounds for convex functions, exponentially concave functions, and strongly convex functions.",4. From Adaptive to Dynamic,[0],[0]
Let I1 =,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[s1, q1], I2 = [s2, q2], . . .",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
", Ik =",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[sk, qk] be a partition of [1, T ].",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"That is, they are successive intervals such that
s1 = 1, qi + 1 = si+1, i ∈",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[k − 1], and qk = T. (4)
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"Define the local functional variation of the i-th interval as
VT (i) =",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"qi∑ t=si+1 max w∈Ω |ft(w)− ft−1(w)|
and it is obvious that ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
VT (i) ≤ VT .3,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"Then, we have the following theorem for bounding the dynamic regret in terms of the strongly adaptive regret and the functional variation.
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
Theorem 3 Let w∗t,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
∈,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
argminw∈Ω ft(w).,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
For all integer k ∈,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"[T ], we have
D-Regret(w∗1, . . .",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
",w ∗ T )
≤ min I1,...,Ik k∑ i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"( SA-Regret(T, |Ii|) +",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"2|Ii| · VT (i) ) where the minimization is taken over any sequence of intervals that satisfy (4).
",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"The above theorem is analogous to Proposition 2 of Besbes et al. (2015), which provides an upper bound for a special choice of the interval sequence.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"The main difference is that there is a minimization operation in our bound, which allows us to get rid of the issue of parameter selection.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For a specific type of problems, we can plug in the corresponding
3Note that in certain cases, the sum of local functional variation ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
VT (i) can be much smaller than the total functional variation VT .,4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For example, when the sequence of functions only changes k times, we can construct the intervals based on the changing rounds such that ∑k i=1",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"VT (i) = 0.
upper bound of strongly adaptive regret, and then choose any sequence of intervals to obtain a concrete upper bound.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"In particular, the choice of the intervals may depend on the (possibly unknown) functional variation.",4.1. Adaptive-to-Dynamic Conversion,[0],[0]
"For convex functions, we choose the meta-algorithm of Jun et al. (2017) and take the online gradient descent as its subroutine.",4.2. Convex Functions,[0],[0]
"The following theorem regarding the adaptive regret can be obtained from that paper.
",4.2. Convex Functions,[0],[0]
Theorem 4,4.2. Convex Functions,[0],[0]
"Under Assumption 1, the meta-algorithm of Jun et al. (2017) is strongly adaptive with
SA-Regret(T, τ) ≤ (
12BG√ 2− 1
+ 8 √ 7 log T + 5 )√ τ =",4.2. Convex Functions,[0],[0]
"O( √ τ log T ).
",4.2. Convex Functions,[0],[0]
"From Theorems 3 and 4, we derive the following bound for the dynamic regret.
",4.2. Convex Functions,[0],[0]
"Corollary 5 Under Assumption 1, the meta-algorithm of Jun et al. (2017) satisfies
D-Regret(w∗1, . . .",4.2. Convex Functions,[0],[0]
",w ∗ T )
≤max  (c+ 9 √ 7 log T + 5) √ T (c+ 8 √ 5)T 2/3V 1/3 T
log1/6 T + 24T 2/3V 1/3 T log 1/3 T
=O ( max {√
T log T , T 2/3V 1/3 T log
1/3 T })
where c = 12BG/( √ 2− 1).
",4.2. Convex Functions,[0],[0]
"According to Theorem 2 of Besbes et al. (2015), we know that the minimax dynamic regret of convex functions is O(T 2/3V
1/3 T ).",4.2. Convex Functions,[0],[0]
"Thus, our upper bound is minimax optimal
up to a polylogarithmic factor.",4.2. Convex Functions,[0],[0]
"Although the restarted online gradient descent of Besbes et al. (2015) achieves a dynamic regret of O(T 2/3V 1/3T ), it requires to know an upper bound of the functional variation VT .",4.2. Convex Functions,[0],[0]
"In contrast, the metaalgorithm of Jun et al. (2017) does not need any prior knowledge of VT .",4.2. Convex Functions,[0],[0]
"We note that the meta-algorithm of Daniely et al. (2015) can also be used here, and its dynamic regret is on the order of max {√ T log T, T 2/3V
1/3 T log
2/3 T } .",4.2. Convex Functions,[0],[0]
"We proceed to consider exp-concave functions, defined in Definition 2.",4.3. Exponentially Concave Functions,[0],[0]
Exponential concavity is stronger than convexity but weaker than strong convexity.,4.3. Exponentially Concave Functions,[0],[0]
"It can be used to model many popular losses used in machine learning, such as the square loss in regression, logistic loss in classification and negative logarithm loss in portfolio management (Koren, 2013).
",4.3. Exponentially Concave Functions,[0],[0]
"For exp-concave functions, we choose Algorithm 1 in this paper, and take the online Newton step as its subroutine.",4.3. Exponentially Concave Functions,[0],[0]
"Based on Theorems 1 and 3, we derive the dynamic regret of the proposed algorithm.
",4.3. Exponentially Concave Functions,[0],[0]
"Corollary 6 Let K = dT 1/γe, where γ > 1 is a small constant.",4.3. Exponentially Concave Functions,[0],[0]
"Suppose Assumption 1 holds, Ω ⊂ Rd, and all the functions are α-exp-concave.",4.3. Exponentially Concave Functions,[0],[0]
"Algorithm 1, with online Newton step as its subroutine, is strongly adaptive with
SA-Regret(T, τ) ≤",4.3. Exponentially Concave Functions,[0],[0]
"( (5d+ 1)(γ + 1) + 2
α + 5d(γ + 1)GB
) log T
=O (γd log T ) =",4.3. Exponentially Concave Functions,[0],[0]
"O (d log T )
and its dynamic regret satisfies
D-Regret(w∗1, . . .",4.3. Exponentially Concave Functions,[0],[0]
",w ∗ T ) ≤",4.3. Exponentially Concave Functions,[0],[0]
"( (5d+ 1)(γ + 1) + 2
α + 5d(γ + 1)GB + 2 ) ·max { log T, √ TVT log T
} =O ( d ·max { log T, √ TVT log T }) .
",4.3. Exponentially Concave Functions,[0],[0]
"To the best of our knowledge, this is the first dynamic regret that exploits exponential concavity.",4.3. Exponentially Concave Functions,[0],[0]
"Furthermore, according to the minimax dynamic regret of strongly convex functions (Besbes et al., 2015), our upper bound is minimax optimal, up to a polylogarithmic factor.",4.3. Exponentially Concave Functions,[0],[0]
"Finally, we study strongly convex functions.",4.4. Strongly Convex Functions,[0],[0]
"According to Lemma 2, we know that strongly convex functions with bounded gradients are also exp-concave.",4.4. Strongly Convex Functions,[0],[0]
"Thus, Corollary 6 can be directly applied to strongly convex functions, and yields a dynamic regret of O(d √ TVT log T ).",4.4. Strongly Convex Functions,[0],[0]
"However, the upper bound depends on the dimensionality d. To address this limitation, we use online gradient descent as the subroutine in Algorithm 1.
",4.4. Strongly Convex Functions,[0],[0]
"From Theorems 2 and 3, we have the following theorem, in which both the adaptive and dynamic regrets are independent from d.
Corollary 7 Let K = dT 1/γe, where γ > 1 is a small constant.",4.4. Strongly Convex Functions,[0],[0]
"Suppose Assumption 1 holds, and all the functions are λ-strongly convex.",4.4. Strongly Convex Functions,[0],[0]
"Algorithm 1, with online gradient descent as its subroutine, is strongly adaptive with
SA-Regret(T, τ)
≤G",4.4. Strongly Convex Functions,[0],[0]
"2
2λ
( γ + 1 + (3γ + 7) log T )",4.4. Strongly Convex Functions,[0],[0]
=O (γ log T ) =,4.4. Strongly Convex Functions,[0],[0]
"O (log T )
and its dynamic regret satisfies
D-Regret(w∗1, . . .",4.4. Strongly Convex Functions,[0],[0]
",w ∗ T )
≤max  ",4.4. Strongly Convex Functions,[0],[0]
"γG2 λ + ( 5γG2 λ + 2 ) log T γG2
λ √ TVT log T +",4.4. Strongly Convex Functions,[0],[0]
(,4.4. Strongly Convex Functions,[0],[0]
5γG2 λ + 2 ),4.4. Strongly Convex Functions,[0],[0]
"√ TVT log T
=O ( max { log T, √ TVT log T }) .
",4.4. Strongly Convex Functions,[0],[0]
"According to Theorem 4 of Besbes et al. (2015), the minimax dynamic regret of strongly convex functions is O( √ TVT ), which implies our upper bound is almost minimax optimal.",4.4. Strongly Convex Functions,[0],[0]
"By comparison, the restarted online gradient descent of Besbes et al. (2015) has a dynamic regret of O(log T √ TVT ), but it requires to know an upper bound of VT .",4.4. Strongly Convex Functions,[0],[0]
We here present the proof of Theorem 3.,5. Analysis,[0],[0]
The omitted proofs are provided in the supplementary.,5. Analysis,[0],[0]
"First, we upper bound the dynamic regret in the following way
D-Regret(w∗1, . . .",5.1. Proof of Theorem 3,[0],[0]
",w ∗ T )
",5.1. Proof of Theorem 3,[0],[0]
= k∑ i=1,5.1. Proof of Theorem 3,[0],[0]
"( qi∑ t=si ft(wt)− qi∑ t=si min w∈Ω ft(w) )
",5.1. Proof of Theorem 3,[0],[0]
= k∑ i=1  ,5.1. Proof of Theorem 3,[0],[0]
qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si ft(wt)− min w∈Ω qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si
ft(w)︸ ︷︷ ︸ :=ai
+ min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si min w∈Ω
ft(w)︸ ︷︷ ︸ :=bi
 .
(5)
",5.1. Proof of Theorem 3,[0],[0]
"From the definition of strongly adaptive regret, we can upper bound ai by
qi∑ t=si ft(wt)− min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w) ≤,5.1. Proof of Theorem 3,[0],[0]
"SA-Regret(T, |Ii|).
",5.1. Proof of Theorem 3,[0],[0]
"To upper bound bi, we follow the analysis of Proposition 2
of Besbes et al. (2015):
min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si min w∈Ω ft(w)
",5.1. Proof of Theorem 3,[0],[0]
= min w∈Ω qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si ft(w ∗ t )
≤ qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w ∗ si)− qi∑,5.1. Proof of Theorem 3,[0],[0]
"t=si ft(w ∗ t )
≤|Ii| · max t∈[si,qi]
( ft(w ∗ si)− ft(w ∗ t ) ) .
",5.1. Proof of Theorem 3,[0],[0]
"(6)
Furthermore, for any t ∈",5.1. Proof of Theorem 3,[0],[0]
"[si, qi], we have
ft(w ∗ si)− ft(w ∗ t )
=ft(w ∗ si)− fsi(w ∗ si) + fsi(w ∗ si)− ft(w ∗ t ) ≤ft(w∗si)− fsi(w ∗ si) + fsi(w ∗ t )",5.1. Proof of Theorem 3,[0],[0]
− ft(w∗t ) ≤2VT,5.1. Proof of Theorem 3,[0],[0]
"(i).
(7)
Combining (6) with (7), we have
min w∈Ω qi∑",5.1. Proof of Theorem 3,[0],[0]
t=si ft(w)− qi∑,5.1. Proof of Theorem 3,[0],[0]
t=si min w∈Ω ft(w),5.1. Proof of Theorem 3,[0],[0]
"≤ 2|Ii| · VT (i).
",5.1. Proof of Theorem 3,[0],[0]
"Substituting the upper bounds of ai and bi into (5), we arrive at
D-Regret(w∗1, . . .",5.1. Proof of Theorem 3,[0],[0]
",w ∗ T )
≤ k∑ i=1",5.1. Proof of Theorem 3,[0],[0]
"(SA-Regret(T, |Ii|) + 2|Ii| · VT (i)) .
",5.1. Proof of Theorem 3,[0],[0]
"Since the above inequality holds for any partition of [1, T ], we can take minimization to get a tight bound.",5.1. Proof of Theorem 3,[0],[0]
"In this paper, we demonstrate that the dynamic regret can be upper bounded by the adaptive regret and the functional variation, which implies strongly adaptive algorithms are automatically equipped with tight dynamic regret bounds.",6. Conclusions and Future Work,[0],[0]
"As a result, we are able to derive dynamic regret bounds for convex functions, exp-concave functions, and strongly convex functions.",6. Conclusions and Future Work,[0],[0]
"Moreover, we provide a unified approach for minimizing the adaptive regret of exp-concave functions, as well as strongly convex functions.
",6. Conclusions and Future Work,[0],[0]
The adaptive-to-dynamic conversion leads to a series of dynamic regret bounds in terms of the functional variation.,6. Conclusions and Future Work,[0],[0]
"As we mentioned before, dynamic regret can also be upper bounded by other regularities such as the path-length.",6. Conclusions and Future Work,[0],[0]
It is interesting to investigate whether those kinds of upper bounds can also be established for strongly adaptive algorithms.,6. Conclusions and Future Work,[0],[0]
"This work was partially supported by the National Key R&D Program of China (2018YFB1004300), NSFC (61603177, 61333014), JiangsuSF (BK20160658), YESS (2017QNRC001), NSF (IIS-1545995), and the Collaborative Innovation Center of Novel Software Technology and Industrialization.",Acknowledgements,[0],[0]
"To cope with changing environments, recent developments in online learning have introduced the concepts of adaptive regret and dynamic regret independently.",abstractText,[0],[0]
"In this paper, we illustrate an intrinsic connection between these two concepts by showing that the dynamic regret can be expressed in terms of the adaptive regret and the functional variation.",abstractText,[0],[0]
This observation implies that strongly adaptive algorithms can be directly leveraged to minimize the dynamic regret.,abstractText,[0],[0]
"As a result, we present a series of strongly adaptive algorithms that have small dynamic regrets for convex functions, exponentially concave functions, and strongly convex functions, respectively.",abstractText,[0],[0]
"To the best of our knowledge, this is the first time that exponential concavity is utilized to upper bound the dynamic regret.",abstractText,[0],[0]
"Moreover, all of those adaptive algorithms do not need any prior knowledge of the functional variation, which is a significant advantage over previous specialized methods for minimizing dynamic regret.",abstractText,[0],[0]
Dynamic Regret of Strongly Adaptive Methods,title,[0],[0]
"Language evolves over time and words change their meaning due to cultural shifts, technological inventions, or political events.",1. Introduction,[0],[0]
We consider the problem of detecting shifts in the meaning and usage of words over a given time span based on text data.,1. Introduction,[0],[0]
"Capturing these semantic shifts requires a dynamic language model.
",1. Introduction,[0],[0]
"Word embeddings are a powerful tool for modeling semantic relations between individual words (Bengio et al., 2003; Mikolov et al., 2013a; Pennington et al., 2014; Mnih & Kavukcuoglu, 2013; Levy & Goldberg, 2014; Vilnis & McCallum, 2014; Rudolph et al., 2016).",1. Introduction,[0],[0]
"Word embed-
1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA.",1. Introduction,[0],[0]
"Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
dings model the distribution of words based on their surrounding words in a training corpus, and summarize these statistics in terms of low-dimensional vector representations.",1. Introduction,[0],[0]
"Geometric distances between word vectors reflect semantic similarity (Mikolov et al., 2013a) and difference vectors encode semantic and syntactic relations (Mikolov et al., 2013c), which shows that they are sensible representations of language.",1. Introduction,[0],[0]
"Pre-trained word embeddings are useful for various supervised tasks, including sentiment analysis (Socher et al., 2013b), semantic parsing (Socher et al., 2013a), and computer vision (Fu & Sigal, 2016).",1. Introduction,[0],[0]
"As unsupervised models, they have also been used for the exploration of word analogies and linguistics (Mikolov et al., 2013c).
",1. Introduction,[0],[0]
"Word embeddings are currently formulated as static models, which assumes that the meaning of any given word is the same across the entire text corpus.",1. Introduction,[0],[0]
"In this paper, we propose a generalization of word embeddings to sequential data, such as corpora of historic texts or streams of text in social media.
",1. Introduction,[0],[0]
"Current approaches to learning word embeddings in a dynamic context rely on grouping the data into time bins and training the embeddings separately on these bins (Kim et al., 2014; Kulkarni et al., 2015; Hamilton et al., 2016).",1. Introduction,[0],[0]
"This approach, however, raises three fundamental problems.",1. Introduction,[0],[0]
"First, since word embedding models are non-convex, training them twice on the same data will lead to different results.",1. Introduction,[0],[0]
"Thus, embedding vectors at successive times can only be approximately related to each other, and only if the embedding dimension is large (Hamilton et al., 2016).",1. Introduction,[0],[0]
"Second, dividing a corpus into separate time bins may lead to training sets that are too small to train a word embedding model.",1. Introduction,[0],[0]
"Hence, one runs the risk of overfitting to few data whenever the required temporal resolution is fine-grained, as we show in the experimental section.",1. Introduction,[0],[0]
"Third, due to the finite corpus size the learned word embedding vectors are subject to random noise.",1. Introduction,[0],[0]
"It is difficult to disambiguate this noise from systematic semantic drifts between subsequent times, in particular over short time spans, where we expect only minor semantic drift.
",1. Introduction,[0],[0]
"In this paper, we circumvent these problems by introducing a dynamic word embedding model.",1. Introduction,[0],[0]
"Our contributions are as follows:
ar X
iv :1
70 2.
",1. Introduction,[0],[0]
"08 35
9v 2
[ st
at .M
L ]
1 7
Ju l 2
01 7
•",1. Introduction,[0],[0]
We derive a probabilistic state space model where word and context embeddings evolve in time according to a diffusion process.,1. Introduction,[0],[0]
"It generalizes the skip-gram model (Mikolov et al., 2013b; Barkan, 2017) to a dynamic setup, which allows end-to-end training.",1. Introduction,[0],[0]
"This leads to continuous embedding trajectories, smoothes out noise in the word-context statistics, and allows us to share information across all times.
",1. Introduction,[0],[0]
"• We propose two scalable black-box variational inference algorithms (Ranganath et al., 2014; Rezende et al., 2014) for filtering and smoothing.",1. Introduction,[0],[0]
These algorithms find word embeddings that generalize better to held-out data.,1. Introduction,[0],[0]
"Our smoothing algorithm carries out efficient black-box variational inference for structured Gaussian variational distributions with tridiagonal precision matrices, and applies more broadly.
",1. Introduction,[0],[0]
• We analyze three massive text corpora that span over long periods of time.,1. Introduction,[0],[0]
Our approach allows us to automatically find the words whose meaning changes the most.,1. Introduction,[0],[0]
"It results in smooth word embedding trajectories and therefore allows us to measure and visualize the continuous dynamics of the entire embedding cloud as it deforms over time.
",1. Introduction,[0],[0]
Figure 1 exemplifies our method.,1. Introduction,[0],[0]
The plot shows a fit of our dynamic skip-gram model to Google books (we give details in section 5).,1. Introduction,[0],[0]
We show the ten words whose meaning changed most drastically in terms of cosine distance over the last 150 years.,1. Introduction,[0],[0]
"We thereby automatically discover words such as “computer” or “radio” whose meaning changed due to technological advances, but also words like
“peer” and “notably” whose semantic shift is less obvious.
",1. Introduction,[0],[0]
Our paper is structured as follows.,1. Introduction,[0],[0]
"In section 2 we discuss related work, and we introduce our model in section 3.",1. Introduction,[0],[0]
In section 4 we present two efficient variational inference algorithms for our dynamic model.,1. Introduction,[0],[0]
We show experimental results in section 5.,1. Introduction,[0],[0]
Section 6 summarizes our findings.,1. Introduction,[0],[0]
"Probabilistic models that have been extended to latent time series models are ubiquitous (Blei & Lafferty, 2006; Wang et al., 2008; Sahoo et al., 2012; Gultekin & Paisley, 2014; Charlin et al., 2015; Ranganath et al., 2015; Jerfel et al., 2017), but none of them relate to word embeddings.",2. Related Work,[0],[0]
"The closest of these models is the dynamic topic model (Blei & Lafferty, 2006; Wang et al., 2008), which learns the evolution of latent topics over time.",2. Related Work,[0],[0]
Topic models are based on bag-of-word representations and thus treat words as symbols without modelling their semantic relations.,2. Related Work,[0],[0]
"They therefore serve a different purpose.
",2. Related Work,[0],[0]
Mikolov et al. (2013a;b) proposed the skip-gram model with negative sampling (word2vec) as a scalable word embedding approach that relies on stochastic gradient descent.,2. Related Work,[0],[0]
"This approach has been formulated in a Bayesian setup (Barkan, 2017), which we discuss separately in section 3.1.",2. Related Work,[0],[0]
"These models, however, do not allow the word embedding vectors to change over time.
",2. Related Work,[0],[0]
"Several authors have analyzed different statistics of text data to analyze semantic changes of words over time (Mihalcea & Nastase, 2012; Sagi et al., 2011; Kim et al., 2014;
Kulkarni et al., 2015; Hamilton et al., 2016).",2. Related Work,[0],[0]
"None of them explicitly model a dynamical process; instead, they slice the data into different time bins, fit the model separately on each bin, and further analyze the embedding vectors in post-processing.",2. Related Work,[0],[0]
"By construction, these static models can therefore not share statistical strength across time.",2. Related Work,[0],[0]
"This limits the applicability of static models to very large corpora.
",2. Related Work,[0],[0]
Most related to our approach are methods based on word embeddings.,2. Related Work,[0],[0]
"Kim et al. (2014) fit word2vec separately on different time bins, where the word vectors obtained for the previous bin are used to initialize the algorithm for the next time bin.",2. Related Work,[0],[0]
"The bins have to be sufficiently large and the found trajectories are not as smooth as ours, as we demonstrate in this paper.",2. Related Work,[0],[0]
Hamilton et al. (2016) also trained word2vec separately on several large corpora from different decades.,2. Related Work,[0],[0]
"If the embedding dimension is large enough (and hence the optimization problem less non-convex), the authors argue that word embeddings at nearby times approximately differ by a global rotation in addition to a small semantic drift, and they approximately compute this rotation.",2. Related Work,[0],[0]
"As the latter does not exist in a strict sense, it is difficult to distinguish artifacts of the approximate rotation from a true semantic drift.",2. Related Work,[0],[0]
"As discussed in this paper, both variants result in trajectories which are noisier.1",2. Related Work,[0],[0]
"We propose the dynamic skip-gram model, a generalization of the skip-gram model (word2vec) (Mikolov et al., 2013b) to sequential text data.",3. Model,[0],[0]
"The model finds word embedding vectors that continuously drift over time, allowing to track changes in language and word usage over short and long periods of time.",3. Model,[0],[0]
"Dynamic skip-gram is a probabilistic model which combines a Bayesian version of the skip-gram model (Barkan, 2017) with a latent time series.",3. Model,[0],[0]
"It is jointly
1 Rudolph & Blei (2017) independently developed a similar model, using a different likelihood model.",3. Model,[0],[0]
"Their approach uses a non-Bayesian treatment of the latent embedding trajectories, which makes the approach less robust to noise when the data per time step is small.
",3. Model,[0],[0]
"trained end-to-end and scales to massive data by means of approximate Bayesian inference.
",3. Model,[0],[0]
"The observed data consist of sequences of words from a finite vocabulary of size L. In section 3.1, all sequences (sentences from books, articles, or tweets) are considered time-independent; in section 3.2 they will be associated with different time stamps.",3. Model,[0],[0]
The goal is to maximize the probability of every word that occurs in the data given its surrounding words within a so-called context window.,3. Model,[0],[0]
"As detailed below, the model learns two vectors ui, vi ∈ Rd for each word i in the vocabulary, where d is the embedding dimension.",3. Model,[0],[0]
We refer to ui as the word embedding vector and to vi as the context embedding vector.,3. Model,[0],[0]
"The Bayesian skip-gram model (Barkan, 2017) is a probabilistic version of word2vec (Mikolov et al., 2013b) and forms the basis of our approach.",3.1. Bayesian Skip-Gram Model,[0],[0]
The graphical model is shown in Figure 2a).,3.1. Bayesian Skip-Gram Model,[0],[0]
"For each pair of words i, j in the vocabulary, the model assigns probabilities that word i appears in the context of word j.",3.1. Bayesian Skip-Gram Model,[0],[0]
This probability is σ(u>i vj) with the sigmoid function σ(x) = 1/(1 + e−x).,3.1. Bayesian Skip-Gram Model,[0],[0]
"Let zij ∈ {0, 1} be an indicator variable that denotes a draw from that probability distribution, hence p(zij = 1) = σ(u>i vj).",3.1. Bayesian Skip-Gram Model,[0],[0]
"The generative model assumes that many word-word pairs (i, j) are uniformly drawn from the vocabulary and tested for being a word-context pair; hence a separate random indicator zij is associated with each drawn pair.
",3.1. Bayesian Skip-Gram Model,[0],[0]
"Focusing on words and their neighbors in a context window, we collect evidence of word-word pairs for which zij = 1.",3.1. Bayesian Skip-Gram Model,[0],[0]
These are called the positive examples.,3.1. Bayesian Skip-Gram Model,[0],[0]
"Denote n+ij the number of times that a word-context pair (i, j) is observed in the corpus.",3.1. Bayesian Skip-Gram Model,[0],[0]
"This is a sufficient statistic of the model, and its contribution to the likelihood is p(n+ij |ui, vj) = σ(u>i vj)n + ij .",3.1. Bayesian Skip-Gram Model,[0],[0]
"However, the generative process also assumes the possibility to reject word-word pairs if zij = 0.",3.1. Bayesian Skip-Gram Model,[0],[0]
"Thus, one needs to construct a fictitious second training set of rejected word-word pairs, called negative examples.",3.1. Bayesian Skip-Gram Model,[0],[0]
Let the corresponding counts be n−ij .,3.1. Bayesian Skip-Gram Model,[0],[0]
"The total likelihood of both positive and negative examples is then
p(n+, n−|U, V ) =",3.1. Bayesian Skip-Gram Model,[0],[0]
"L∏
i,j=1
σ(u>i vj) n+ijσ(−u>",3.1. Bayesian Skip-Gram Model,[0],[0]
i vj)n − ij .,3.1. Bayesian Skip-Gram Model,[0],[0]
"(1)
",3.1. Bayesian Skip-Gram Model,[0],[0]
Above we used the antisymmetry σ(−x) = 1 − σ(x).,3.1. Bayesian Skip-Gram Model,[0],[0]
"In our notation, dropping the subscript indices for n+ and n− denotes the entire L × L matrices, U = (u1, · · · , uL) ∈ Rd×L is the matrix of all word embedding vectors, and V is defined analogously for the context vectors.",3.1. Bayesian Skip-Gram Model,[0],[0]
"To construct negative examples, one typically chooses n−ij ∝ P (i)P (j)3/4 (Mikolov et al., 2013b), where P (i) is the
frequency of word i in the training corpus.",3.1. Bayesian Skip-Gram Model,[0],[0]
"Thus, n− is well-defined up to a constant factor which has to be tuned.
",3.1. Bayesian Skip-Gram Model,[0],[0]
"Defining n± = (n+, n−) the combination of both positive and negative examples, the resulting log likelihood is
log p(n±|U, V ) = L∑
i,j=1
( n+ij log σ(u",3.1. Bayesian Skip-Gram Model,[0],[0]
> i vj) + n,3.1. Bayesian Skip-Gram Model,[0],[0]
− ij log σ(−u>i vj) ) .,3.1. Bayesian Skip-Gram Model,[0],[0]
"(2)
This is exactly the objective of the (non-Bayesian) skipgram model, see (Mikolov et al., 2013b).",3.1. Bayesian Skip-Gram Model,[0],[0]
The count matrices n+,3.1. Bayesian Skip-Gram Model,[0],[0]
"and n− are either pre-computed for the entire corpus, or estimated based on stochastic subsamples from the data in a sequential way, as done by word2vec.",3.1. Bayesian Skip-Gram Model,[0],[0]
Barkan (2017) gives an approximate Bayesian treatment of the model with Gaussian priors on the embeddings.,3.1. Bayesian Skip-Gram Model,[0],[0]
"The key extension of our approach is to use a Kalman filter as a prior for the time-evolution of the latent embeddings (Welch & Bishop, 1995).",3.2. Dynamic Skip-Gram Model,[0],[0]
"This allows us to share information across all times while still allowing the embeddings to drift.
",3.2. Dynamic Skip-Gram Model,[0],[0]
Notation.,3.2. Dynamic Skip-Gram Model,[0],[0]
We consider a corpus of T documents which were written at time stamps τ1 < . . .,3.2. Dynamic Skip-Gram Model,[0],[0]
< τT .,3.2. Dynamic Skip-Gram Model,[0],[0]
"For each time step t ∈ {1, . . .",3.2. Dynamic Skip-Gram Model,[0],[0]
", T} the sufficient statistics of word-context pairs are encoded in the L×L matrices n+t , n−t of positive and negative counts with matrix elements n+ij,t and n − ij,t, respectively.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Denote Ut = (u1,t, · · · , uL,t) ∈",3.2. Dynamic Skip-Gram Model,[0],[0]
"Rd×L the matrix of word embeddings at time t, and define Vt correspondingly for the context vectors.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Let U, V ∈ RT×d×L denote the tensors of word and context embeddings across all times, respectively.
",3.2. Dynamic Skip-Gram Model,[0],[0]
Model.,3.2. Dynamic Skip-Gram Model,[0],[0]
The graphical model is shown in Figure 2b).,3.2. Dynamic Skip-Gram Model,[0],[0]
We consider a diffusion process of the embedding vectors over time.,3.2. Dynamic Skip-Gram Model,[0],[0]
"The variance σ2t of the transition kernel is
σ2t =",3.2. Dynamic Skip-Gram Model,[0],[0]
"D(τt+1 − τt), (3) whereD is a global diffusion constant and (τt+1−τt) is the time between subsequent observations (Welch & Bishop, 1995).",3.2. Dynamic Skip-Gram Model,[0],[0]
"At every time step t, we add an additional Gaussian prior with zero mean and variance σ20 which prevents the embedding vectors from growing very large, thus
p(Ut+1|Ut) ∝",3.2. Dynamic Skip-Gram Model,[0],[0]
"N (Ut, σ2t )N (0, σ20).",3.2. Dynamic Skip-Gram Model,[0],[0]
"(4) Computing the normalization, this results in
Ut+1|Ut ∼ N",3.2. Dynamic Skip-Gram Model,[0],[0]
"(
Ut 1 + σ2t /σ 2 0 , 1 σ−2t + σ −2 0",3.2. Dynamic Skip-Gram Model,[0],[0]
"I
) , (5)
Vt+1|Vt ∼ N (
Vt 1 + σ2t /σ 2 0 , 1 σ−2t + σ −2 0",3.2. Dynamic Skip-Gram Model,[0],[0]
"I
) .",3.2. Dynamic Skip-Gram Model,[0],[0]
"(6)
In practice, σ0 σt, so the damping to the origin is very weak.",3.2. Dynamic Skip-Gram Model,[0],[0]
"This is also called Ornstein-Uhlenbeck process (Uhlenbeck & Ornstein, 1930).",3.2. Dynamic Skip-Gram Model,[0],[0]
"We recover the Wiener process for σ0 → ∞, but σ0 < ∞ prevents the latent time series from diverging to infinity.",3.2. Dynamic Skip-Gram Model,[0],[0]
"At time index t = 1, we define p(U1|U0) ≡ N (0, σ20I) and do the same for V1.",3.2. Dynamic Skip-Gram Model,[0],[0]
"Our joint distribution factorizes as follows:
p(n±, U, V ) =",3.2. Dynamic Skip-Gram Model,[0],[0]
"T−1∏
t=0
p(Ut+1|Ut) p(Vt+1|Vt)
",3.2. Dynamic Skip-Gram Model,[0],[0]
"× T∏
t=1
L∏
i,j=1
p(n±ij,t|ui,t, vj,t) (7)
",3.2. Dynamic Skip-Gram Model,[0],[0]
The prior model enforces that the model learns embedding vectors which vary smoothly across time.,3.2. Dynamic Skip-Gram Model,[0],[0]
This allows to associate words unambiguously with each other and to detect semantic changes.,3.2. Dynamic Skip-Gram Model,[0],[0]
"The model efficiently shares information across the time domain, which allows to fit the model even in setups where the data at every given point in time are small, as long as the data in total are large.",3.2. Dynamic Skip-Gram Model,[0],[0]
We discuss two scalable approximate inference algorithms.,4. Inference,[0],[0]
Filtering uses only information from the past; it is required in streaming applications where the data are revealed to us sequentially.,4. Inference,[0],[0]
"Smoothing is the other inference method, which learns better embeddings but requires the full sequence of documents ahead of time.
",4. Inference,[0],[0]
"In Bayesian inference, we start by formulating a joint distribution (Eq. 7) over observations n± and parameters U and V , and we are interested in the posterior distribution over parameters conditioned on observations,
p(U, V |n±)",4. Inference,[0],[0]
"= p(n ±, U, V )∫
p(n±, U, V ) dUdV (8)
The problem is that the normalization is intractable.",4. Inference,[0],[0]
"In variational inference (VI) (Jordan et al., 1999; Blei et al., 2016) one sidesteps this problem and approximates the posterior with a simpler variational distribution qλ(U, V ) by minimizing the Kullback-Leibler (KL) divergence to the posterior.",4. Inference,[0],[0]
"Here, λ summarizes all parameters of the variational distribution, such as the means and variances of a Gaussian, see below.",4. Inference,[0],[0]
Minimizing the KL divergence is equivalent to optimizing the evidence lower bound (ELBO),4. Inference,[0],[0]
"(Blei et al., 2016),
L(λ) =",4. Inference,[0],[0]
"Eqλ [log p(n±, U, V )]−Eqλ",4. Inference,[0],[0]
"[log qλ(U, V )].",4. Inference,[0],[0]
"(9)
For a restricted class of models, the ELBO can be computed in closed-form (Hoffman et al., 2013).",4. Inference,[0],[0]
"Our model is
non-conjugate and requires instead black-box VI using the reparameterization trick (Rezende et al., 2014; Kingma & Welling, 2014).",4. Inference,[0],[0]
"In many applications such as streaming, the data arrive sequentially.",4.1. Skip-Gram Filtering,[0],[0]
"Thus, we can only condition our model on past and not on future observations.",4.1. Skip-Gram Filtering,[0],[0]
"We will first describe inference in such a (Kalman) filtering setup (Kalman et al., 1960; Welch & Bishop, 1995).
",4.1. Skip-Gram Filtering,[0],[0]
"In the filtering scenario, the inference algorithm iteratively updates the variational distribution q as evidence from each time step t becomes available.",4.1. Skip-Gram Filtering,[0],[0]
"We thereby use a variational distribution that factorizes across all times, q(U, V ) =∏T t=1 q(Ut, Vt) and we update the variational factor at a given time t based on the evidence at time t and the approximate posterior of the previous time step.",4.1. Skip-Gram Filtering,[0],[0]
"Furthermore, at every time t we use a fully-factorized distribution:
q(Ut, Vt) =
L∏
i=1
N (ui,t;µui,t,Σui,t)N (vi,t;µvi,t.Σvi,t),
The variational parameters are the means µui,t, µvi,t ∈ Rd and the covariance matrices Σui,t and Σvi,t, which we restrict to be diagonal (mean-field approximation).
",4.1. Skip-Gram Filtering,[0],[0]
"We now describe how we sequentially compute q(Ut, Vt) and use the result to proceed to the next time step.",4.1. Skip-Gram Filtering,[0],[0]
"As other Markovian dynamical systems, our model assumes the following recursion,
p(Ut, Vt|n±1:t) ∝ p(n±t |Ut, Vt) p(Ut, Vt|n±1:t−1).",4.1. Skip-Gram Filtering,[0],[0]
"(10)
Within our variational approximation, the ELBO (Eq. 9) therefore separates into a sum of T terms, L = ∑t Lt with
Lt = E[log p(n±t |Ut, Vt)]",4.1. Skip-Gram Filtering,[0],[0]
"+ E[log p(Ut, Vt|n±1:t−1)]",4.1. Skip-Gram Filtering,[0],[0]
"− E[log q(Ut, Vt)], (11)
where all expectations are taken under q(Ut, Vt).",4.1. Skip-Gram Filtering,[0],[0]
"We compute the entropy term −E[log q] in Eq. 11 analytically and estimate the gradient of the log likelihood by sampling from the variational distribution and using the reparameterization trick (Kingma & Welling, 2014; Salimans & Kingma, 2016).",4.1. Skip-Gram Filtering,[0],[0]
"However, the second term of Eq. 11, containing the prior at time t, is still intractable.",4.1. Skip-Gram Filtering,[0],[0]
"We approximate the prior as
p(Ut, Vt|n±1:t−1) ≡",4.1. Skip-Gram Filtering,[0],[0]
"Ep(Ut−1,Vt−1|n±1:t−1) [ p(Ut, Vt|Ut−1, Vt−1) ]
",4.1. Skip-Gram Filtering,[0],[0]
"≈ Eq(Ut−1,Vt−1) [ p(Ut, Vt|Ut−1, Vt−1) ] .",4.1. Skip-Gram Filtering,[0],[0]
"(12)
The remaining expectation involves only Gaussians and can be carried-out analytically.",4.1. Skip-Gram Filtering,[0],[0]
"The resulting approximate
prior is a fully factorized distribution p(Ut, Vt|n±1:t−1) ≈∏L i=1N (ui,t; µ̃ui,t, Σ̃ui,t)N (vi,t; µ̃vi,t, Σ̃vit) with
µ̃ui,t = Σ̃ui,t ( Σui,t−1 + σ 2 t",4.1. Skip-Gram Filtering,[0],[0]
"I )−1 µui,t−1; Σ̃ui,t =",4.1. Skip-Gram Filtering,[0],[0]
"[( Σui,t−1 + σ 2",4.1. Skip-Gram Filtering,[0],[0]
t,4.1. Skip-Gram Filtering,[0],[0]
I )−1,4.1. Skip-Gram Filtering,[0],[0]
+ (1/σ20)I ],4.1. Skip-Gram Filtering,[0],[0]
−1 .,4.1. Skip-Gram Filtering,[0],[0]
"(13)
",4.1. Skip-Gram Filtering,[0],[0]
"Analogous update equations hold for µ̃vi,t and Σ̃vi,t. Thus, the second contribution in Eq. 11 (the prior) yields a closedform expression.",4.1. Skip-Gram Filtering,[0],[0]
We can therefore compute its gradient.,4.1. Skip-Gram Filtering,[0],[0]
"In contrast to filtering, where inference is conditioned on past observations until a given time t, (Kalman) smoothing performs inference based on the entire sequence of observations n±1:T .",4.2. Skip-Gram Smoothing,[0],[0]
"This approach results in smoother trajectories and typically higher likelihoods than with filtering, because evidence is used from both future and past observations.
",4.2. Skip-Gram Smoothing,[0],[0]
"Besides the new inference scheme, we also use a different variational distribution.",4.2. Skip-Gram Smoothing,[0],[0]
"As the model is fitted jointly to all time steps, we are no longer restricted to a variational distribution that factorizes in time.",4.2. Skip-Gram Smoothing,[0],[0]
For simplicity we focus here on the variational distribution for the word embeddings U ; the context embeddings V are treated identically.,4.2. Skip-Gram Smoothing,[0],[0]
"We use a factorized distribution over both embedding space and vocabulary space,
q(U1:T ) =
L∏
i=1
d∏
k=1
q(uik,1:T ).",4.2. Skip-Gram Smoothing,[0],[0]
"(14)
In the time domain, our variational approximation is structured.",4.2. Skip-Gram Smoothing,[0],[0]
"To simplify the notation we now drop the indices for words i and embedding dimension k, hence we write q(u1:T ) for q(uik,1:T ) where we focus on a single factor.",4.2. Skip-Gram Smoothing,[0],[0]
"This factor is a multivariate Gaussian distribution in the time domain with tridiagonal precision matrix Λ,
q(u1:T )",4.2. Skip-Gram Smoothing,[0],[0]
"= N (µ,Λ−1) (15) Both the means µ = µ1:T and the entries of the tridiagonal precision matrix Λ ∈ RT×T are variational parameters.",4.2. Skip-Gram Smoothing,[0],[0]
"This gives our variational distribution the interpretation of a posterior of a Kalman filter (Blei & Lafferty, 2006), which captures correlations in time.
",4.2. Skip-Gram Smoothing,[0],[0]
"We fit the variational parameters by training the model jointly on all time steps, using black-box VI and the reparameterization trick.",4.2. Skip-Gram Smoothing,[0],[0]
"As the computational complexity of an update step scales as Θ(L2), we first pretrain the model by drawing minibatches of L′ <",4.2. Skip-Gram Smoothing,[0],[0]
"L random words and L′ random contexts from the vocabulary (Hoffman et al., 2013).",4.2. Skip-Gram Smoothing,[0],[0]
We then switch to the full batch to reduce the sampling noise.,4.2. Skip-Gram Smoothing,[0],[0]
"Since the variational distribution does not factorize in the time domain we always include all time steps {1, . . .",4.2. Skip-Gram Smoothing,[0],[0]
", T} in the minibatch.
",4.2. Skip-Gram Smoothing,[0],[0]
"We also derive an efficient algorithm that allows us to estimate the reparametrization gradient using Θ(T ) time and memory, while a naive implementation of black-box variational inference with our structured variational distribution would require Θ(T 2) of both resources.",4.2. Skip-Gram Smoothing,[0],[0]
"The main idea is to parametrize Λ = B>B in terms of its Cholesky decomposition B, which is bidiagonal (Kılıç & Stanica, 2013), and to express gradients of B−1 in terms of gradients of B. We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce positive definiteness of B. The algorithm is detailed in our supplementary material.",4.2. Skip-Gram Smoothing,[0],[0]
We evaluate our method on three time-stamped text corpora.,5. Experiments,[0],[0]
We demonstrate that our algorithms find smoother embedding trajectories than methods based on a static model.,5. Experiments,[0],[0]
This allows us to track semantic changes of individual words by following nearest-neighbor relations over time.,5. Experiments,[0],[0]
"In our quantitative analysis, we find higher predictive likelihoods on held-out data compared to our baselines.
",5. Experiments,[0],[0]
Algorithms and Baselines.,5. Experiments,[0],[0]
"We report results from our proposed algorithms from section 4 and compare against baselines from section 2:
• SGI denotes the non-Bayesian skip-gram model with independent random initializations of word vectors (Mikolov et al., 2013b).",5. Experiments,[0],[0]
We used our own implementation of the model by dropping the Kalman filtering prior and point-estimating embedding vectors.,5. Experiments,[0],[0]
"Word vectors at nearby times are made comparable by approximate orthogonal transformations, which corresponds to Hamilton et al. (2016).
",5. Experiments,[0],[0]
"• SGP denotes the same approach as above, but with word and context vectors being pre-initialized with the values from the previous year, as in Kim et al. (2014).
",5. Experiments,[0],[0]
• DSG-F: dynamic skip-gram filtering (proposed).,5. Experiments,[0],[0]
"• DSG-S: dynamic skip-gram smoothing (proposed).
",5. Experiments,[0],[0]
Data and preprocessing.,5. Experiments,[0],[0]
"Our three corpora exemplify opposite limits both in the covered time span and in the amount of text per time step.
1.",5. Experiments,[0],[0]
We used data from the Google books corpus2,5. Experiments,[0],[0]
"(Michel et al., 2011) from the last two centuries (T = 209).",5. Experiments,[0],[0]
This amounts to 5 million digitized books and approximately 1010 observed words.,5. Experiments,[0],[0]
"The corpus consists of n-gram tables with n ∈ {1, . . .",5. Experiments,[0],[0]
", 5}, annotated by year of publication.",5. Experiments,[0],[0]
"We considered the years from 1800 to
2http://storage.googleapis.com/books/ ngrams/books/datasetsv2.html
2008 (the latest available).",5. Experiments,[0],[0]
"In 1800, the size of the data is approximately∼ 7 ·107 words.",5. Experiments,[0],[0]
"We used the 5-gram counts, resulting in a context window size of 4.
2.",5. Experiments,[0],[0]
"We used the “State of the Union” (SoU) addresses of U.S. presidents, which spans more than two centuries, resulting in T = 230 different time steps and approximately 106 observed words.3",5. Experiments,[0],[0]
Some presidents gave both a written and an oral address; if these were less than a week apart we concatenated them and used the average date.,5. Experiments,[0],[0]
"We converted all words to lower case and constructed the positive sample counts n+ij using a context window size of 4.
3.",5. Experiments,[0],[0]
We used a Twitter corpus of news tweets for 21 randomly drawn dates from 2010 to 2016.,5. Experiments,[0],[0]
The median number of tweets per day is 722.,5. Experiments,[0],[0]
"We converted all tweets to lower case and used a context window size of 4, which we restricted to stay within single tweets.
Hyperparameters.",5. Experiments,[0],[0]
"The vocabulary for each corpus was constructed from the 10,000 most frequent words throughout the given time period.",5. Experiments,[0],[0]
"In the Google books corpus, the number of words per year grows by a factor of 200 from the year 1800 to 2008.",5. Experiments,[0],[0]
"To avoid that the vocabulary is dominated by modern words we normalized the word frequencies separately for each year before adding them up.
",5. Experiments,[0],[0]
"For the Google books corpus, we chose the embedding dimension d = 200, which was also used in Kim et al. (2014).",5. Experiments,[0],[0]
"We set d = 100 for SoU and Twitter, as d = 200 resulted in overfitting on these much smaller corpora.",5. Experiments,[0],[0]
The ratio η = ∑ ij n,5. Experiments,[0],[0]
"− ij,t/ ∑ ij n + ij,t of negative to positive wordcontext pairs was η = 1.",5. Experiments,[0],[0]
The precise construction of the matrices n±t is explained in the supplementary material.,5. Experiments,[0],[0]
"We used the global prior variance σ20 = 1 for all corpora and all algorithms, including the baselines.",5. Experiments,[0],[0]
The diffusion constant D controls the time scale on which information is shared between time steps.,5. Experiments,[0],[0]
The optimal value for D depends on the application.,5. Experiments,[0],[0]
"A single corpus may exhibit semantic shifts of words on different time scales, and the optimal choice for D depends on the time scale in which one is interested.",5. Experiments,[0],[0]
"We used D = 10−3 per year for Google books and SoU, and D = 1 per year for the Twitter corpus, which spans a much shorter time range.",5. Experiments,[0],[0]
"In the supplementary material, we provide details of the optimization procedure.
",5. Experiments,[0],[0]
Qualitative results.,5. Experiments,[0],[0]
We show that our approach results in smooth word embedding trajectories on all three corpora.,5. Experiments,[0],[0]
"We can automatically detect words that undergo significant semantic changes over time.
",5. Experiments,[0],[0]
"Figure 1 in the introduction shows a fit of the dynamic skip-gram filtering algorithm to the Google books corpus.
3http://www.presidency.ucsb.edu/sou.php
Here, we show the ten words whose word vectors change most drastically over the last 150 years in terms of cosine distance.",5. Experiments,[0],[0]
"Figure 3 visualizes word embedding clouds over four subsequent years of Google books, where we compare DSG-F against SGI.",5. Experiments,[0],[0]
"We mapped the normalized embedding vectors to two dimensions using dynamic t-SNE (Rauber et al., 2016) (see supplement for details).",5. Experiments,[0],[0]
Lines indicate shifts of word vectors relative to the preceding year.,5. Experiments,[0],[0]
"In our model only few words change their position in the embedding space rapidly, while embeddings using SGI show strong fluctuations, making the cloud’s motion hard to track.
",5. Experiments,[0],[0]
Figure 4 visualizes the smoothness of the trajectories directly in the embedding space (without the projection to two dimensions).,5. Experiments,[0],[0]
We consider differences between word vectors in the year 1998 and the subsequent 10 years.,5. Experiments,[0],[0]
"In more detail, we compute histograms of the Euclidean distances ||uit − ui,t+δ|| over the word indexes i, where δ = 1, . . .",5. Experiments,[0],[0]
", 10 (as discussed previously, SGI uses a global rotation to optimally align embeddings first).",5. Experiments,[0],[0]
"In our model, embedding vectors gradually move away from their original position as time progresses, indicating a directed motion.",5. Experiments,[0],[0]
"In contrast, both baseline models show only little directed motion after the first time step, suggesting that most temporal changes are due to finite-size fluctuations of n±ij,t. Initialization schemes alone, thus, seem to have a minor effect on smoothness.
",5. Experiments,[0],[0]
Our approach allows us to detect semantic shifts in the usage of specific words.,5. Experiments,[0],[0]
Figures 5 and 1 both show the cosine distance between a given word and its neighboring words (colored lines) as a function of time.,5. Experiments,[0],[0]
Figure 5 shows results on all three corpora and focuses on a comparison across methods.,5. Experiments,[0],[0]
"We see that DSG-S and DSG-F (both proposed)
result in trajectories which display less noise than the baselines SGP and SGI.",5. Experiments,[0],[0]
"The fact that the baselines predict zero cosine distance (no correlation) between the chosen word pairs on the SoU and Twitter corpora suggests that these corpora are too small to successfully fit these models, in contrast to our approach which shares information in the time domain.",5. Experiments,[0],[0]
"Note that as in dynamic topic models, skipgram smoothing (DSG-S) may diffuse information into the past (see ”presidential” to ”clinton-trump” in Fig. 5).
",5. Experiments,[0],[0]
Quantitative results.,5. Experiments,[0],[0]
We show that our approach generalizes better to unseen data.,5. Experiments,[0],[0]
"We thereby analyze held-out predictive likelihoods on word-context pairs at a given time t, where t is excluded from the training set,
1 |n±t | log p(n±t |Ũt, Ṽt).",5. Experiments,[0],[0]
"(16)
Above, |n±t | = ∑ i,j ( n+ij,t + n",5. Experiments,[0],[0]
"− ij,t ) denotes the total number of word-context pairs at time τt.",5. Experiments,[0],[0]
"Since inference is different in all approaches, the definitions of word and context embedding matrices Ũt and Ṽt in Eq. 16 have to be adjusted:
• For SGI and SGP, we did a chronological pass through the time sequence and used the embeddings Ũt = Ut−1 and Ṽt = Vt−1 from the previous time step to predict the statistics",5. Experiments,[0],[0]
"n±ij,t at time step t. • For DSG-F, we did the same pass to test n±ij,t.",5. Experiments,[0],[0]
"We thereby set Ũt and Ṽt to be the modes Ut−1, Vt−1 of the approximate posterior at the previous time step.
",5. Experiments,[0],[0]
"• For DSG-S, we held out 10%, 10% and 20% of the documents from the Google books, SoU, and Twitter corpora for testing, respectively.",5. Experiments,[0],[0]
"After training, we estimated the word (context) embeddings Ũt (Ṽt) in
Eq. 16 by linear interpolation between the values of Ut−1 (Vt−1) and Ut+1 (Vt+1) in the mode of the variational distribution, taking into account that the time stamps τt are in general not equally spaced.
",5. Experiments,[0],[0]
The predictive likelihoods as a function of time τt are shown in Figure 6.,5. Experiments,[0],[0]
"For the Google Books corpus (left panel in figure 6), the predictive log-likelihood grows over time with all four methods.",5. Experiments,[0],[0]
This must be an artifact of the corpus since SGI does not carry any information of the past.,5. Experiments,[0],[0]
A possible explanation is the growing number of words per year from 1800 to 2008 in the Google Books corpus.,5. Experiments,[0],[0]
"On all three corpora, differences between the two implementations of the static model (SGI and SGP) are small, which suggests that pre-initializing the embeddings with the previous result may improve their continuity but seems to have little impact on the predictive power.",5. Experiments,[0],[0]
"Log-likelihoods for the skip-gram filter (DSG-F) grow over the first few time steps as the filter sees more data, and then saturate.",5. Experiments,[0],[0]
"The improvement of our approach over the static model is particularly pronounced in the SoU and Twitter corpora, which are much smaller than the massive Google books corpus.",5. Experiments,[0],[0]
"There, sharing information between across time is crucial because there is little data at every time slice.",5. Experiments,[0],[0]
"Skip-gram smoothing outperforms skip-gram filtering as it shares in-
formation in both time directions and uses a more flexible variational distribution.",5. Experiments,[0],[0]
We presented the dynamic skip-gram model: a Bayesian probabilistic model that combines word2vec with a latent continuous time series.,6. Conclusions,[0],[0]
We showed experimentally that both dynamic skip-gram filtering (which conditions only on past observations) and dynamic skip-gram smoothing (which uses all data) lead to smoothly changing embedding vectors that are better at predicting word-context statistics at held-out time steps.,6. Conclusions,[0],[0]
"The benefits are most drastic when the data at individual time steps is small, such that fitting a static word embedding model is hard.",6. Conclusions,[0],[0]
"Our approach may be used as a data mining and anomaly detection tool when streaming text on social media, as well as a tool for historians and social scientists interested in the evolution of language.",6. Conclusions,[0],[0]
"We would like to thank Marius Kloft, Cheng Zhang, Andreas Lehrmann, Brian McWilliams, Romann Weber, Michael Clements, and Ari Pakman for valuable feedback.",Acknowledgements,[0],[0]
"To create the word-clouds in Figure 1 of the main text we mapped the fitted word embeddings from Rd to the twodimensional plane using dynamic t-SNE (Rauber et al., 2016).",1. Dimensionality Reduction in Figure 1,[0],[0]
Dynamic t-SNE is a non-parametric dimensionality reduction algorithm for sequential data.,1. Dimensionality Reduction in Figure 1,[0],[0]
The algorithm finds a projection to a lower dimension by solving a non-convex optimization problem that aims at preserving nearest-neighbor relations at each individual time step.,1. Dimensionality Reduction in Figure 1,[0],[0]
"In
1Disney Research, 4720 Forbes Avenue, Pittsburgh, PA 15213, USA.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Correspondence to: Robert Bamler <Robert.Bamler@disneyresearch.com>, Stephan Mandt <Stephan.Mandt@disneyresearch.com>.
",1. Dimensionality Reduction in Figure 1,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Copyright 2017 by the author(s).
",1. Dimensionality Reduction in Figure 1,[0],[0]
"addition, projections at neighboring time steps are aligned with each other by a quadratic penalty with prefactor λ ≥ 0 for sudden movements.
",1. Dimensionality Reduction in Figure 1,[0],[0]
"There is a trade-off between finding good local projections for each individual time step (λ → 0), and finding smooth projections (large λ).",1. Dimensionality Reduction in Figure 1,[0],[0]
"Since we want to analyze the smoothness of word embedding trajectories, we want to avoid bias towards smooth projections.",1. Dimensionality Reduction in Figure 1,[0],[0]
"Unfortunately, setting λ = 0 is not an option since, in this limit, the optimization problem is invariant under independent rotations at each time, rendering trajectories in the two-dimensional projection plane meaningless.",1. Dimensionality Reduction in Figure 1,[0],[0]
"To still avoid bias towards smooth projections, we anneal λ exponentially towards zero over the course of the optimization.",1. Dimensionality Reduction in Figure 1,[0],[0]
"We start the optimizer with λ = 0.01, and we reduce λ by 5% with each training step.",1. Dimensionality Reduction in Figure 1,[0],[0]
"We run 100 optimization steps in total, so that λ",1. Dimensionality Reduction in Figure 1,[0],[0]
≈ 6×10−6 at the end of the training procedure.,1. Dimensionality Reduction in Figure 1,[0],[0]
"We used the opensource implementation,1 set the target perplexities to 200, and used default values for all other parameters.",1. Dimensionality Reduction in Figure 1,[0],[0]
Table S1 lists the hyperparameters used in our experiments.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"For the Google books corpus, we used the same context window size cmax and embedding dimension d as in (Kim et al., 2014).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"We reduced d for the SoU and Twitter corpora to avoid overfitting to these much smaller data sets.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In constrast to word2vec, we construct our positive and negative count matrices",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"n±ij,t deterministically in a preprocessing step.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"As detailed below, this is done such that it resembles as closely as possible the stochastic approach in word2vec (Mikolov et al., 2013).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In every update step, word2vec stochastically samples a context window size uniformly in an interval [1, · · · , cmax], thus the context size fluctuates and nearby words appear more often in the same context than words that are far apart from each other in the sentence.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
We follow a deterministic scheme that results in similar statistics.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"For each pair of words (w1, w2) in a given sentence, we increase the counts n+iw1 jw2 by max (0, 1− k/cmax), where 0 ≤ k ≤ cmax is the number of words that appear between w1 and w2, and iw1 and jw2 are the words’ unique indices in the vocabulary.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"1https://github.com/paulorauber/thesne
ar X
iv :1
70 2.
08 35
9v 2
[ st
at .M
L ]
1 7
Ju l 2
01 7
Algorithm 1 Skip-gram filtering; see section 4 of the main text.
",2. Hyperparemeters and Construction of n±1:T,[0],[0]
Remark: All updates are analogous for word and context vectors; we drop their indices for simplicity.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Input: number of time steps T , time stamps τ1:T , positive and negative examples n±1:T , hyperparameters.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
Init.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"prior means µ̃ik,1 ← 0 and variances Σ̃i,1 =",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Id×d Init. variational means µik,1 ← 0 and var.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Σi,1",2. Hyperparemeters and Construction of n±1:T,[0],[0]
= Id×d for t,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"= 1 to T do
if t 6= 1",2. Hyperparemeters and Construction of n±1:T,[0],[0]
then Update approximate Gaussian prior with params.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"µ̃ik,t and Σ̃i,t using µik,t−1 and Σi,t−1, see Eq. 13.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
end if Compute entropy Eq[log q(·)] analytically.,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Compute expected log Gaussian prior with parameters µ̃ik,t and Σ̃k,t analytically.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Maximize Lt in Eq. 11, using black-box VI with the reparametrization trick.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Obtain µik,t and Σi,t as outcome of the optimization.
end for
We also used a deterministic variant of word2vec to construct the negative count matrices n−t .",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In word2vec, η negative samples (i, j) are drawn for each positive sample (i, j′) by drawing η independent values for j from a distribution P ′t (j) defined below.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"We define n − ij,t such that it matches the expectation value of the number of times that word2vec would sample the negative word-context pair (i, j).",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"Specifically, we define
Pt(i) =
∑L j=1 n
+ ij,t∑L
i′,j=1 n + i′j,t
, (S1)
P ′t (j) =
( Pt(j) )",2. Hyperparemeters and Construction of n±1:T,[0],[0]
γ ∑L j′=1 ( Pt(j′) ),2. Hyperparemeters and Construction of n±1:T,[0],[0]
"γ , (S2)
n−ij,t =
( L∑
i′,j′=1
n+i′j′,t ) ηPt(i)P ′",2. Hyperparemeters and Construction of n±1:T,[0],[0]
t (j).,2. Hyperparemeters and Construction of n±1:T,[0],[0]
"(S3)
We chose γ = 0.75 as proposed in (Mikolov et al., 2013), and we set η = 1.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
"In practice, it is not necessary to explicitly construct the full matrices n−t , and it is more efficient to keep only the distributions Pt(i) and P ′t (j) in memory.",2. Hyperparemeters and Construction of n±1:T,[0],[0]
The skip-gram filtering algorithm is described in section 4 of the main text.,3. Skip-gram Filtering Algorithm,[0],[0]
We provide a formulation in pseudocode in Algorithm 1.,3. Skip-gram Filtering Algorithm,[0],[0]
"In this section, we give details for the skip-gram smoothing algorithm, see section 4 of the main text.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A summary is
Algorithm 2 Skip-gram smoothing; see section 4.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We drop indices i, j, and k for word, context, end embedding dimension, respectively, when they are clear from context.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"Input: number of time steps T , time stamps τ1:T , wordcontext counts n+1",4. Skip-gram Smoothing Algorithm,[0],[0]
":T , hyperparameters in Table S1 Obtain n−t ∀t using Eqs.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S1–S3 Initialize µu,1:T , µv,1:T ← 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"Initialize νu,1:T , νv,1:T , ωu,1:T−1, and ωv,1:T−1 such
that B>u Bu = B > v",4. Skip-gram Smoothing Algorithm,[0],[0]
Bv = Π (see Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
S5 and S11) for step = 1 to N ′tr do Draw I ⊂,4. Skip-gram Smoothing Algorithm,[0],[0]
{,4. Skip-gram Smoothing Algorithm,[0],[0]
"1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", L′} with |I| = L′ uniformly Draw J ⊂ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", L′} with |J | = L′ uniformly for all i ∈",4. Skip-gram Smoothing Algorithm,[0],[0]
"I do
Draw",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s]ui,1:T ∼ N (0, I)",4. Skip-gram Smoothing Algorithm,[0],[0]
"Solve Bu,ixui,1:T = ui,1:T for xui,1:T
end for Obtain xvj,1:T by repeating last loop ∀j ∈ J Calculate gradient estimates of L for minibatch
(I,J ) using Eqs.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S10, S14, and S15 Obtain update steps",4. Skip-gram Smoothing Algorithm,[0],[0]
"d[·] for all variational parameters
using Adam optimizer with parameters in Table S1 Update µu,1:T ← µu,1:T +d[µu,1:T ], and analogously
for µv,1:T , ωu,1:T−1, and ωv,1:T−1 Update νu,1:T and νv,1:T according to Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S18
end for Repeat above loop for Ntr more steps, this time without
minibatch sampling (i.e., setting L′ = L)
provided in Algorithm 2.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Variational distribution.,4. Skip-gram Smoothing Algorithm,[0],[0]
"For now, we focus on the word embeddings, and we simplify the notation by dropping the indices for the vocabulary and embedding dimensions.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational distribution for a single embedding dimension of a single word embedding trajectory is
q(u1:T )",4. Skip-gram Smoothing Algorithm,[0],[0]
"= N (µu,1:T , (B>u Bu)−1).",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S4)
Here, µu,1:T is the vector of mean values, and Bu is the Cholesky decomposition of the precision matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We restrict the latter to be bidiagonal,
Bu =   νu,1 ωu,1 νu,2 ωu,2 . . . . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
"νu,T−1 ωu,T−1
νT
  (S5)
with νu,t > 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"for all t ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T}.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational parameters are µu,1:T , νu,1:T , and ω1:T−1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"The variational distribution of the context embedding trajectories v1:T has the same structure.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"With the above form of Bu, the variational distribution is a Gaussian with an arbitrary tridiagonal symmetric precision matrix B>u Bu.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We chose this variational distribution because it is the exact posterior of a hidden time-series model with a Kalman filtering prior and Gaussian noise (Blei & Lafferty, 2006).",4. Skip-gram Smoothing Algorithm,[0],[0]
"Note that our variational distribution is a generalization of a fully factorized (mean-field) distribution, which is obtained for ωu,t = 0 ∀t.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In the general case, ωu,t 6= 0, the variational distribution can capture correlations between all time steps, with a dense covariance matrix (B>u Bu) −1.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Gradient estimation.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The skip-gram smoothing algorithm uses stochastic gradient ascent to find the variational parameters that maximize the ELBO,
L = Eq [ log p(U1:T , V1:T , n ± 1:T ) ]",4. Skip-gram Smoothing Algorithm,[0],[0]
"− Eq [ log q(U1:T , V1:T ) ] .
(S6)
Here, the second term is the entropy, which can be evaluated analytically.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We obtain for each component in vocabulary and embedding space,
−Eq[log q(u1:T )]",4. Skip-gram Smoothing Algorithm,[0],[0]
"= − ∑
t
log(νu,t) + const.",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S7)
and analogously for −Eq[log q(v1:T )].",4. Skip-gram Smoothing Algorithm,[0],[0]
The first term on the right-hand side of Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
S6 cannot be evaluated analytically.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We approximate its gradient by sampling from q using the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014).",4. Skip-gram Smoothing Algorithm,[0],[0]
"A naive calculation would require Ω(T 2) computing time since the derivatives of L with respect to νu,t and ωu,t for each t depend on the count matrices n±t′ of all t
′. However, as we show next, there is a more efficient way to obtain all gradient estimates in Θ(T ) time.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"We focus again on a single dimension of a single word embedding trajectory u1:T , and we drop the indices",4. Skip-gram Smoothing Algorithm,[0],[0]
i and k.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We draw S independent samples u[s]1:T with s ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", S} from q(u1:T ) by parameterizing
u [s] 1:T = µu,1:T + x",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T (S8)
with
x",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T = B −1",4. Skip-gram Smoothing Algorithm,[0],[0]
u,4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T",4. Skip-gram Smoothing Algorithm,[0],[0]
"where [s] u,1:T ∼ N (0, I).",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S9)
We obtain x[s]u,1:T in Θ(T ) time by solving the bidiagonal linear system",4. Skip-gram Smoothing Algorithm,[0],[0]
"Bux [s] u,1:T =",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T .",4. Skip-gram Smoothing Algorithm,[0],[0]
Samples v,4. Skip-gram Smoothing Algorithm,[0],[0]
[s] 1:T for the context embedding trajectories are obtained analogously.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Our implementation uses S = 1, i.e., we draw only a single sample per training step.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Averaging over several samples is done implicitly since we calculate the update steps
using the Adam optimizer (Kingma & Ba, 2014), which effectively averages over several gradient estimates in its first moment estimate.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The derivatives of L with respect to µu,1:T can be obtained using Eq. S8 and the chain rule.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We find
∂L ∂µu,1:T ≈ 1 S
S∑
s=1
[ Γ [s] u,1:T −Πu [s] 1:T ] .",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S10)
Here, Π ∈ RT×T is the precision matrix of the prior u1:T ∼ N (0,Π−1).",4. Skip-gram Smoothing Algorithm,[0],[0]
It is tridiagonal and therefore the matrix-multiplication Πu[s]1:T can be carried out efficiently.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The non-zero matrix elements of Π are
Π11 = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
+ σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
"1
ΠTT = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
"+ σ −2 T−1
Πtt = σ −2 0",4. Skip-gram Smoothing Algorithm,[0],[0]
+ σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
t−1 + σ −2,4. Skip-gram Smoothing Algorithm,[0],[0]
"t ∀t ∈ {2, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T − 1}
Π1,t+1 = Πt+1,1 = −σ−2t .",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S11)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The term Γ[s]u,1:T on the right-hand side of Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
S10 comes from the expectation value of the log-likelihood under q.,4. Skip-gram Smoothing Algorithm,[0],[0]
"It is given by
Γ",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] ui,t =
L∑
j=1
[( n+ij,t + n",4. Skip-gram Smoothing Algorithm,[0],[0]
"− ij,t ) σ",4. Skip-gram Smoothing Algorithm,[0],[0]
"( −u[s]>i,t v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t ) − n−ij,t ] v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t
(S12)
where we temporarily restored the indices i and j for words and contexts, respectively.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In deriving Eq. S12, we used the relations ∂ log σ(x)/∂x = σ(−x) and σ(−x) = 1− σ(x).",4. Skip-gram Smoothing Algorithm,[0],[0]
"The derivatives of L with respect to νu,t and ωu,t are more intricate.",4. Skip-gram Smoothing Algorithm,[0],[0]
Using the parameterization in Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S8–S9, the derivatives are functions of ∂B−1u /∂νt and ∂B −1",4. Skip-gram Smoothing Algorithm,[0],[0]
"u /∂ωt, respectively, where B−1u is a dense (upper triangular) T × T matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
"An efficient way to obtain these derivatives is to use the relation
∂B−1u ∂νt = −B−1u ∂Bu",4. Skip-gram Smoothing Algorithm,[0],[0]
"∂νt B−1u (S13)
and similarly for ∂B−1u /∂ωt.",4. Skip-gram Smoothing Algorithm,[0],[0]
Using this relation and Eqs.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S8–S9, we obtain the gradient estimates
∂L ∂νu,t ≈ − 1 S
S∑
s=1
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,tx",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,t −
1
νu,t , (S14)
∂L ∂ωu,t ≈ − 1 S
S∑
s=1
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,tx",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,t+1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"(S15)
",4. Skip-gram Smoothing Algorithm,[0],[0]
The second term on the right-hand side of Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S14 is the derivative of the entropy, Eq. S7, and
y",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] u,1:T = (B > u ) −1",4. Skip-gram Smoothing Algorithm,[0],[0]
"[ Γ [s] u,1:T −Πu",4. Skip-gram Smoothing Algorithm,[0],[0]
[s] 1:T ] .,4. Skip-gram Smoothing Algorithm,[0],[0]
"(S16)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"The values y[s]u,1",4. Skip-gram Smoothing Algorithm,[0],[0]
":T can again be obtained in Θ(T ) time by bringing B>u to the left-hand side and solving the corresponding bidiagonal linear system of equations.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Sampling in vocabulary space.,4. Skip-gram Smoothing Algorithm,[0],[0]
"In the above paragraph, we described an efficient strategy to obtain gradient estimates in only Θ(T ) time.",4. Skip-gram Smoothing Algorithm,[0],[0]
"However, the gradient estimation scales quadratic in the vocabulary size L because all L2 elements of the positive count matrices n+t contribute to the gradients.",4. Skip-gram Smoothing Algorithm,[0],[0]
"In order speed up the optimization, we pretrain the model using a minibatch of size",4. Skip-gram Smoothing Algorithm,[0],[0]
L′ <,4. Skip-gram Smoothing Algorithm,[0],[0]
L in vocabulary space as explained below.,4. Skip-gram Smoothing Algorithm,[0],[0]
The computational complexity of a single training step in this setup scales as (L′)2 rather than L2.,4. Skip-gram Smoothing Algorithm,[0],[0]
"After N ′tr = 5000 training steps with minibatch size L′, we switch to the full batch size of L and train the model for another Ntr = 1000 steps.
",4. Skip-gram Smoothing Algorithm,[0],[0]
The subsampling in vocabulary space works as follows.,4. Skip-gram Smoothing Algorithm,[0],[0]
"In each training step, we independently draw a set I of L′ random distinct words and a set J of L′ random distinct contexts from a uniform probability over the vocabulary.",4. Skip-gram Smoothing Algorithm,[0],[0]
We then estimate the gradients of L with respect to only the variational parameters that correspond to words i ∈,4. Skip-gram Smoothing Algorithm,[0],[0]
I and contexts j ∈ J .,4. Skip-gram Smoothing Algorithm,[0],[0]
This is possible because both the prior of our dynamic skip-gram model and the variational distribution factorize in the vocabulary space.,4. Skip-gram Smoothing Algorithm,[0],[0]
"The likelihood of the model, however, does not factorize.",4. Skip-gram Smoothing Algorithm,[0],[0]
"This affects only the definition of Γ[s]uik,t in Eq. S12.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We replace Γ [s] uik,t by an estimate Γ[s]′uik,t based on only the contexts j ∈ J in the current minibatch,
Γ [s] ui,t =
L L′ ∑
j∈J
[ ( n+ij,t + n",4. Skip-gram Smoothing Algorithm,[0],[0]
"− ij,t ) σ",4. Skip-gram Smoothing Algorithm,[0],[0]
"( −u[s]>i,t v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t )
− n−ij,t ] v",4. Skip-gram Smoothing Algorithm,[0],[0]
"[s] j,t. (S17)
",4. Skip-gram Smoothing Algorithm,[0],[0]
"Here, the prefactor L/L′ restores the correct ratio between evidence and prior knowledge (Hoffman et al., 2013).
",4. Skip-gram Smoothing Algorithm,[0],[0]
Enforcing positive definiteness.,4. Skip-gram Smoothing Algorithm,[0],[0]
"We update the variational parameters using stochastic gradient ascent with the Adam optimizer (Kingma & Ba, 2014).",4. Skip-gram Smoothing Algorithm,[0],[0]
"The parameters νu,1:T are the eigenvalues of the matrix Bu, which is the Cholesky decomposition of the precision matrix of q. Therefore, νu,t has to be positive for all t ∈ {1, . . .",4. Skip-gram Smoothing Algorithm,[0],[0]
", T}.",4. Skip-gram Smoothing Algorithm,[0],[0]
"We use mirror ascent (Ben-Tal et al., 2001; Beck & Teboulle, 2003) to enforce νu,t > 0.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Specifically, we update νt to a new value ν′t defined by
ν′u,t = 1
2 νu,td[νu,t] +
√( 1
2 νu,td[νu,t]
)2 + ν2u,t (S18)
where d[νu,t] is the step size obtained from the Adam optimizer.",4. Skip-gram Smoothing Algorithm,[0],[0]
Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S18 can be derived from the general mirror ascent update rule Φ′(ν′u,t) =",4. Skip-gram Smoothing Algorithm,[0],[0]
"Φ ′(νu,t) +",4. Skip-gram Smoothing Algorithm,[0],[0]
"d[νu,t] with the
mirror map Φ : x 7→",4. Skip-gram Smoothing Algorithm,[0],[0]
"−c1 log(x)+c2x2/2, where we set the parameters to c1 = νu,t and c2 = 1/νu,t for dimensional reasons.",4. Skip-gram Smoothing Algorithm,[0],[0]
The update step in Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S18 increases (decreases) νu,t for positive (negative) d[νu,t], while always keeping its value positive.
",4. Skip-gram Smoothing Algorithm,[0],[0]
Natural basis.,4. Skip-gram Smoothing Algorithm,[0],[0]
"As a final remark, let us discuss an optional extension to the skip-gram smoothing algorithm that converges in less training steps.",4. Skip-gram Smoothing Algorithm,[0],[0]
This extension only increases the efficiency of the algorithm.,4. Skip-gram Smoothing Algorithm,[0],[0]
It does not change the underlying model or the choice of variational distribution.,4. Skip-gram Smoothing Algorithm,[0],[0]
Observe that the prior of the dynamic skip-gram model connects only neighboring time-steps with each other.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Therefore, the gradient of L with respect to µu,t depends only on the values of µu,t−1 and µu,t+1.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A naive implementation of gradient ascent would thus require T−1 update steps until a change of µu,1 affects updates of µu,T .
",4. Skip-gram Smoothing Algorithm,[0],[0]
"This problem can be avoided with a change of basis from µu,1:T to new parameters ρu,1:T ,
µu,1:T = Aρu,1:T (S19)
with an appropriately chosen invertible matrix A ∈ RT×T .",4. Skip-gram Smoothing Algorithm,[0],[0]
"Derivatives of L with respect to ρu,1:T are given by the chain rule, ∂L/∂ρu,1:T = (∂L/∂µu,1:T )A.",4. Skip-gram Smoothing Algorithm,[0],[0]
"A natural (but inefficient) choice for A is to stack the eigenvectors of the prior precision matrix Π, see Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S11, into a matrix.",4. Skip-gram Smoothing Algorithm,[0],[0]
The eigenvectors of Π are the Fourier modes of the Kalman filtering prior (with a regularization due to σ0).,4. Skip-gram Smoothing Algorithm,[0],[0]
"Therefore, there is a component ρu,t that corresponds to the zero-mode of Π, and this component learns an average word embedding over all times.",4. Skip-gram Smoothing Algorithm,[0],[0]
Higher modes correspond to changes of the embedding vector over time.,4. Skip-gram Smoothing Algorithm,[0],[0]
"A single update to the zero immediately affects all elements of µu,1:T , and therefore changes the word embeddings at all time steps.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Thus, information propagates quickly along the time dimension.",4. Skip-gram Smoothing Algorithm,[0],[0]
The downside of this choice for A is that the transformation in Eq.,4. Skip-gram Smoothing Algorithm,[0],[0]
"S19 has complexity Ω(T 2), which makes update steps slow.
",4. Skip-gram Smoothing Algorithm,[0],[0]
"As a compromise between efficiency and a natural basis, we propose to set A in Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
S19 to the Cholesky decomposition of the prior covariance matrix Π−1 ≡,4. Skip-gram Smoothing Algorithm,[0],[0]
AA>.,4. Skip-gram Smoothing Algorithm,[0],[0]
"Thus, A is still a dense (upper triangular) matrix, and, in our experiments, updates to the last component ρu,T affect all components of µu,1:T in an approximately equal amount.",4. Skip-gram Smoothing Algorithm,[0],[0]
"Since Π is tridiagonal, the inverse of A is bidiagonal, and Eq.",4. Skip-gram Smoothing Algorithm,[0],[0]
"S19 can be evaluated in Θ(T ) time by solving Aµu,1:T = ρu,1:T for µu,1:T .",4. Skip-gram Smoothing Algorithm,[0],[0]
This is the parameterization we used in our implementation of the skip-gram smoothing algorithm.,4. Skip-gram Smoothing Algorithm,[0],[0]
We present a probabilistic language model for time-stamped text data which tracks the semantic evolution of individual words over time.,abstractText,[0],[0]
The model represents words and contexts by latent trajectories in an embedding space.,abstractText,[0],[0]
"At each moment in time, the embedding vectors are inferred from a probabilistic version of word2vec (Mikolov et al., 2013b).",abstractText,[0],[0]
These embedding vectors are connected in time through a latent diffusion process.,abstractText,[0],[0]
We describe two scalable variational inference algorithms—skipgram smoothing and skip-gram filtering—that allow us to train the model jointly over all times; thus learning on all data while simultaneously allowing word and context vectors to drift.,abstractText,[0],[0]
Experimental results on three different corpora demonstrate that our dynamic model infers word embedding trajectories that are more interpretable and lead to higher predictive likelihoods than competing methods that are based on static models trained separately on time slices.,abstractText,[0],[0]
Dynamic Word Embeddings,title,[0],[0]
Deep convolutional neural networks (CNNs) have been crucial to the success of deep learning.,1. Introduction,[0],[0]
"Architectures based on CNNs have achieved unprecedented accuracy in domains ranging across computer vision (Krizhevsky et al., 2012), speech recognition (Hinton et al., 2012), natural language processing (Collobert et al., 2011; Kalchbrenner et al., 2014;
1Google Brain 2Work done as part of the Google AI Residency program (g.co/airesidency).",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Lechao Xiao <xlc@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"Kim, 2014), and recently even the board game Go (Silver et al., 2016; 2017).
",1. Introduction,[0],[0]
The performance of deep convolutional networks has improved as these networks have been made ever deeper.,1. Introduction,[0],[0]
"For example, some of the best-performing models on ImageNet (Deng et al., 2009) have employed hundreds or even a thousand layers (He et al., 2016a;b).",1. Introduction,[0],[0]
"However, these extremely deep architectures have been trainable only in conjunction with techniques like residual connections (He et al., 2016a) and batch normalization (Ioffe & Szegedy, 2015).",1. Introduction,[0],[0]
It is an open question whether these techniques qualitatively improve model performance or whether they are necessary crutches that solely make the networks easier to train.,1. Introduction,[0],[0]
"In this work, we study vanilla CNNs using a combination of theory and experiment to disentangle the notions of trainability and generalization performance.",1. Introduction,[0],[0]
"In doing so, we show that through a careful, theoretically-motivated initialization scheme, we can train vanilla CNNs with 10,000 layers using no architectural tricks.
",1. Introduction,[0],[0]
"Recent work has used mean field theory to build a theoretical understanding of neural networks with random parameters (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018).",1. Introduction,[0],[0]
"These studies revealed a maximum depth through which signals can propagate at initialization, and verified empirically that networks are trainable precisely when signals can travel all the way through them.",1. Introduction,[0],[0]
"In the fully-connected setting, the theory additionally predicts the existence of an order-to-chaos phase transition in the space of initialization hyperparameters.",1. Introduction,[0],[0]
"For networks initialized on the critical line separating these phases, signals can propagate indefinitely and arbitrarily deep networks can be trained.",1. Introduction,[0],[0]
While mean field theory captures the “average” dynamics of random neural networks it does not quantify the scale of gradient fluctuations that are crucial to the stability of gradient descent.,1. Introduction,[0],[0]
"A related body of work (Saxe et al., 2013; Pennington et al., 2017; 2018) has examined the input-output Jacobian and used random matrix theory to quantify the distribution of its singular values in terms of the activation function and the distribution from which the initial random weight matrices are drawn.",1. Introduction,[0],[0]
"These works concluded that networks can be trained most efficiently when the Jacobian is well-conditioned, a criterion that can be achieved with orthogonal, but not Gaussian, weight matrices.",1. Introduction,[0],[0]
"Together, these approaches have allowed researchers to efficiently train extremely deep network architectures, but so far they have been limited to neural networks composed of fully-connected layers.
",1. Introduction,[0],[0]
"In the present work, we continue this line of research and extend it to the convolutional setting.",1. Introduction,[0],[0]
"We show that a welldefined mean-field theory exists for convolutional networks in the limit that the number of channels is large, even when the size of the image is small.",1. Introduction,[0],[0]
"Moreover, convolutional networks have precisely the same order-to-chaos transition as fully-connected networks, with vanishing gradients in the ordered phase and exploding gradients in the chaotic phase.",1. Introduction,[0],[0]
"And just like fully-connected networks, very deep CNNs that are initialized on the critical line separating those two phases can be trained with relative ease.
",1. Introduction,[0],[0]
"Moving beyond mean field theory, we additionally show that the random matrix analysis of (Pennington et al., 2017; 2018) carries over to the convolutional setting.",1. Introduction,[0],[0]
"Furthermore, we identify an efficient construction from the wavelet literature that generates random orthogonal matrices with the block-circulant structure that corresponds to convolution operators.",1. Introduction,[0],[0]
This construction facilitates random orthogonal initialization for convolulational layers and enables good conditioning of the end-to-end Jacobian matrices of arbitrarily deep networks.,1. Introduction,[0],[0]
"We show empirically that networks with this initialization can train significantly more quickly than standard convolutional networks.
",1. Introduction,[0],[0]
"Finally, we emphasize that although the order-to-chaos phase boundaries of fully-connected and convolutional networks look identical, the underlying mean-field theories are in fact quite different.",1. Introduction,[0],[0]
"In particular, a novel aspect of the convolutional theory is the existence of multiple depth scales that control signal propagation at different spatial frequencies.",1. Introduction,[0],[0]
"In the large depth limit, signals can only propagate along modes with minimal spatial structure; all other modes end up deteriorating, even at criticality.",1. Introduction,[0],[0]
"We hypothesize that this type of signal degradation is harmful for generalization, and we develop a modified initialization scheme that allows for balanced propagation of signals among all frequencies.",1. Introduction,[0],[0]
"In this scheme, which we call Delta-Orthogonal initialization, the orthogonal kernel is drawn from a spatially non-uniform distribution, and it allows us to train vanilla CNNs of 10,000 layers or more with no degradation in performance.",1. Introduction,[0],[0]
"In this section, we first derive a mean field theory for signal propagation in random convolutional neural networks.",2. Theoretical results,[0],[0]
We will follow the general methodology established in Poole et al. (2016); Schoenholz et al. (2017); Yang & Schoenholz (2017).,2. Theoretical results,[0],[0]
We will then arrive at a theory for the singular value distribution of the Jacobian following Pennington et al. (2017; 2018).,2. Theoretical results,[0],[0]
"Together, this will allow us to derive theoretically motivated initialization schemes for convolutional neural networks that we call orthogonal kernels and Delta-Orthogonal kernels.",2. Theoretical results,[0],[0]
Later we will demonstrate experimentally that these kernels outperform existing initialization schemes for very deep vanilla convolutional networks.,2. Theoretical results,[0],[0]
"Consider an L-layer 1D1 CNN with periodic boundary conditions, filter width 2k + 1, number of channels c, spatial size n, per-layer weight tensors !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"l 2 R(2k+1)⇥c⇥c, and biases bl 2 Rc.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
Let :,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"R! R be the activation function and let hlj(↵) denote the pre-activation unit at layer l, channel j, and spatial location ↵ 2 sp, where we define the set of spatial locations sp = {1, ..., n}.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"The forward-propagation dynamics can be described by the recurrence relation,
hl+1j (↵) = X
i2chn 2ker
(hli(↵ + ))!",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l+1 ij ( ) +,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"b l+1 j , (2.1)
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"where ker = { 2 Z : | |  k} and chn = {1, . . .",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
", c}.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"At initialization, we take the weights !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
lij ( ) to be drawn i.i.d.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"from the Gaussian N (0, 2!/(c(2k + 1)))",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"and the biases blj
1For notational simplicity, we consider one-dimensional convolutions, but the d-dimensional case proceeds identically.
to be drawn i.i.d.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"from the Gaussian N (0, 2b ).",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
Note that hli(↵) =,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
h,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l i(↵ + n),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
=,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
h,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
l i(↵ n),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
since we assume periodic boundary conditions.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
We wish to understand how signals propagate through these networks.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"As in previous work in this vein, we will take the large network limit, which in this context corresponds to the number of channels c!1.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
This allows us to use powerful theoretical tools such as mean field theory and random matrix theory.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Moreover, this approximation has been shown to give results that agree well with experiments on finite-size networks.
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"In the limit of a large number of channels, the central limit theorem implies that the pre-activation vectors hlj are i.i.d.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Gaussian with mean zero and covariance matrix ⌃l↵,↵0 = E[hlj(↵)hlj(↵0)].",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Here, the expectation is taken over the weights and biases and it is independent of the channel index j.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"In this limit, the covariance matrix takes the form (see Supplemental Materials (SM)),
⌃ l+1 ↵,↵0 = 2 b + 2w 2k+1
X
2ker E ⇥",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(hlj(↵+ )),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(h l j(↵ 0 + )),2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"⇤ ,
(2.2) and is independent of j. A more compact representation of this equation can be given as,
⌃l+1 ⌘ A ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"C(⌃l) , (2.3)
where A = 12k+1I2k+1 and ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"denotes 2D circular crosscorrelation, i.e. for any matrix C, A ?",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"C is defined as,
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"[A ? C]↵,↵0 = 1
2k + 1
X 2ker C↵+ ,↵0+ .",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.4)
",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
The function C : PSDn !,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"PSDn is related to the C-map defined in Poole et al. (2016) (see also (Daniely et al., 2016)) and is given by,
[C(⌃)]↵,↵0 = 2 !",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Eh⇠N (0,⌃)",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
[ (h↵) (h↵0)],2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
+,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
2b .,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.5)
All but the two dimensions ↵ and ↵0 in eqn.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"(2.5) marginalize, so, as in (Poole et al., 2016), the C-map can be computed by a two-dimensional integral.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
"Unlike in (Poole et al., 2016), ↵ and ↵0 do not correspond to different examples but rather to different spatial positions and eqn.",2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
(2.5) characterizes how signals from a single input propagate through convolutional networks in the mean-field approximation2.,2.1.1. RECURSION RELATION FOR COVARIANCE,[0],[0]
We now seek to study the dynamics induced by eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.3).,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"Schematically, our approach will be to identify fixed points of eqn.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3) and then linearize the dynamics around these
2The multi-input analysis proceeds in precisely the same manner as we present here, but comes with increased notational complexity and features no qualitatively different behavior, so we focus our presentation on the single-input case.
fixed points.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"These linearized dynamics will dictate the stability and rate of decay towards the fixed points, which determines the depth scales over which signals in the network can propagate.
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"Schoenholz et al. (2017) found that for many activation functions (e.g. tanh) and any choice of w and b, the C-map has a fixed point ⌃⇤ (i.e. C(⌃⇤) = ⌃⇤) of the form,
⌃",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"⇤ ↵,↵0 = q ⇤ ( ↵,↵0 + (1 ↵,↵0)c ⇤ ) , (2.6)
where a,b is the Kronecker- , q⇤ is the fixed-point variance of a single input, and c⇤ is the fixed-point correlation between two inputs.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
It follows from the form of eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.4) that ⌃⇤ is also a fixed point of the layer-to-layer covariance map in the convolutional case (eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3)), i.e. ⌃⇤ = A ?",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"C(⌃⇤).
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"To analyze the dynamics of the iteration map (2.3) near the fixed point ⌃⇤, we define ✏l = ⌃⇤ ⌃l and expand eqn.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
(2.3) to lowest order in ✏.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"This expansion requires the Jacobian of the C-map evaluated at the fixed point, the properties of which we analyze in the SM.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"In brief, perturbations in q⇤ and c⇤ evolve independently and the Jacobian decomposes into a diagonal eigenspace Vd with eigenvalue q⇤ , and an off-diagonal eigenspace Vo.d. with eigenvalue c⇤ .",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"The eigenvalues are given by3,
c⇤ = 2 wEh⇠N (0,C⇤)[ 0(h1) 0(h2)] , h1 6= h2 , q⇤ = 2 wEh⇠N (0,C⇤)[ 00(h1) (h1) + 0(h1)2] , (2.7)
and the eigenspaces have bases,
Bd = {M ↵,↵ : M↵,↵↵̄,↵̄0 = ↵,↵̄ ↵,↵̄0 + ↵̄,↵ + ↵̄0,↵}↵2sp
Bo.d. =",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"{M ↵,↵0 : M↵,↵ 0 ↵̄,↵̄0 = ↵,↵̄ ↵0,↵̄0 + ↵,↵̄0 ↵0,↵̄}↵ 6=↵0 ,
(2.8)
i.e. Vd = span(Bd) and Vo.d = span(Bo.d.).",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
Note that q⇤ and c⇤ also were found in Schoenholz et al. (2017) to control signal propagation in the fully-connected case.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
The constant is given in Lemma B.2 of the SM but does not concern us here.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
This eigen-decomposition implies that the layer-wise deviations from the fixed point evolve under eqn.,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"(2.3) as,
✏l+1 = q⇤A ?",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
✏ l d + c⇤A ?,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
✏ l o.d. +,2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"O((✏ l ) 2 ) , (2.9)
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"where ✏d and ✏o.d. are decomposition of ✏ into the eigenspaces Vd and Vo.d..
Eqn. (2.9) defines the linear dynamics of random convolutional neural networks near their fixed points and is the basis for the in-depth analysis of the following subsections.
",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"3By the symmetry of ⌃⇤, these expectations are independent of spatial location and of the choice of h1 and h2.",2.1.2. DYNAMICS OF SIGNAL PROPAGATION,[0],[0]
"In the fully-connected setting, the dynamics of signal propagation near the fixed point are governed by scalar evolution equations.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"In contrast, the convolutional setting enjoys much richer dynamics, as eqn.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.9) describes a multi-dimensional system that we now analyze.
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
It follows from eqns.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.4) and (2.8) (see also the SM) that A does not mix the diagonal and off-diagonal eigenspaces, i.e. A ?",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
✏d 2 Vd and A ?,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"✏o.d. 2 Vo.d.. To see this, note that for M↵,↵ 0 2 Vo.d., the definition implies M↵,↵ 0
↵̄+ ,↵̄0+ =
M↵ ,↵ 0 ↵̄,↵̄0 .",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"This property ensures that A ? M ↵,↵0 can be expressed as a linear combination of matrices in Vo.d., which means it also belongs to Vo.d.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"The same argument applies to M↵,↵ 2 Vd.. As a result, these eigenspaces evolve entirely independently under the linearization of the covariance iteration map (2.3).
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
Let l0 denote the depth over which transient effects persist and after which eqn.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
(2.9) accurately describes the linearized dynamics.,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Therefore, at depths larger than l0, we have
✏l ⇡ A ? · · · A ?| {z } l l0",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
( l l0q⇤,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
✏ l0 d +,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
l l0 c⇤ ✏ l0 o.d.) .,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"(2.10)
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"This matrix-valued equation is still somewhat complicated owing to the nested applications of A. To further elucidate the dynamics, we can move to a Fourier basis, which diagonalizes the circular cross-correlation operator and decouples the modes of eqn.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
(2.10).,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"In particular, let F denote the 2D discrete Fourier transform and ✏̃↵,↵0 ⌘ F(✏)↵,↵0 denote a Fourier mode of ✏.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Then eqn. (2.10) becomes a simple scalar equation,
",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"✏̃l↵,↵0 ⇡ ( ↵,↵0 q⇤)",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
l l0,2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"[✏̃l0d ]↵,↵0+( ↵,↵0 c⇤) l l0",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"[✏̃l0o.d.]↵,↵0 , (2.11) with ↵,↵0 = F(A)⇤↵,↵0 .",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
"Thus, the linearized dynamics of convolutional neural networks decouple into independentlyevolving Fourier modes that evolve near the fixed point at frequency-dependent rates.",2.1.3. MULTI-DIMENSIONAL SIGNAL PROPAGATION,[0],[0]
The stability of the fixed point ⌃⇤ is determined by whether nearby points move closer or farther from ⌃⇤ under the dynamics described by eqn.,2.1.4. FIXED-POINT ANALYSIS,[0],[0]
(2.9).,2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Eqn. (2.11) shows that this condition depends on the whether the quantities ↵,↵0 q⇤ and ↵,↵0 c⇤ are less than or greater than one.
",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Since A is a diagonal matrix, the eigenvalues ↵,↵0 have a specific structure.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"In particular, the set of eigenvalues is comprised of n copies of the 1D discrete Fourier transform of the diagonal entries of A. Furthermore, since the diagonal entries of A are non-negative and sum to one, their Fourier coefficients have absolute value no larger than one and the zero-frequency coefficient is equal to one; see Figure 4
for the full distribution in the case of 2D convolutions.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"It follows that the fixed point ⌃⇤ will be stable if and only if q⇤ < 1 and c⇤ < 1.
",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"These stability conditions are precisely the ones found to govern fully-connected networks (Poole et al., 2016; Schoenholz et al., 2017).",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Moreover, the fixed point matrix ⌃
⇤ is also the same as in the fully-connected case.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"Together, these observations imply that the entire fixed-point structure of the convolutional case is identical to that of the fullyconnected case.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"In particular, based on the results of (Poole et al., 2016), we can immediately conclude that the ( w, b) hyperparameter plane is separated by the line 1 = 1 into an ordered phase with c⇤ = 1 in which all pixels approach the same value, and a chaotic phase with c⇤ < 1 in which the pixels become decorrelated with one another; see the SM for a review of this phase diagram analysis.",2.1.4. FIXED-POINT ANALYSIS,[0],[0]
"We now assume that the conditions for a stable fixed point are met, i.e. q⇤ < 1 and c⇤ < 1, and we consider the rate at which the fixed point is approached.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"As in (Schoenholz et al., 2017), it is convenient to additionally assume q",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
⇤ < c,2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
⇤ so that the dynamics in the diagonal subspace can be neglected.,2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"In this case, eqn.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"(2.11) can be rewritten as
✏̃l↵,↵0 ⇡ e (l l0)/⇠↵,↵0 [✏̃o.d.]",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"l0 ↵,↵0 , (2.12)
where ⇠↵,↵0 = 1/ log( c⇤ ↵,↵0) are depth scales governing the convergence of the different modes.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"In particular, we expect signals corresponding to a specific Fourier mode
f↵,↵0 to be able to travel a depth commensurate to ⇠↵,↵0 through the network.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Thus, unlike fully-connected networks which exhibit only a single depth scale, convolutional networks feature a hierarchy of depth scales.
",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Recalling that ↵,n ↵ = 1, it follows that ⇠c ⌘ ⇠↵,n ↵ = 1/ log c⇤ , which is identical to the depth scale governing signal propagation through fully-connected networks.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"It follows from (Schoenholz et al., 2017) that when 1 = 1, ⇠↵,n ↵ diverges and thus convolutional networks can propagate signals arbitrarily far through the f↵,n ↵ modes.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Since | ↵,↵0 | < 1 for ↵0 6= n",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"↵, these are the only modes through which signals can propagate without attenuation.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"Finally, we note that the f↵,n ↵ modes correspond to perturbations that are spatially uniform along the cyclic diagonals of the covariance matrix.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
"The fact that all signals with additional spatial structure attenuate for large depth suggests that deep critical convolutional networks behave quite similarly to fully-connected networks, which also cannot propagate spatially-structured signals.",2.1.5. DEPTH SCALES OF SIGNAL PROPAGATION,[0],[0]
The similarities between signal propagation in convolutional neural networks and fully-connected networks in the limit of large depth are surprising.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
A consequence may be that the performance of very deep convolutional networks degrades as the signal is forced to propagate along modes with minimal spatial structure.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"Indeed, Fig. 3 shows that the generalization performance decreases with depth, and that for very large depth it barely surpasses the performance of a
fully-connected network.
",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"If increased spatial uniformity is the problem, eqn.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
(2.12) holds the solution.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"In order for all modes to propagate without attenuation, it is necessary that ↵,↵0 = 1 for all ↵, ↵0.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"In fact, it is easy to show that the distribution of { ↵,↵0} can be modified by allowing for spatial non-uniformity in the variance of the weights within the kernel.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"To this end, we introduce a non-negative vector v = (v )",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"2ker chosen such that P v = 1, and initialize the weights of the net-
work according to wlij( ) ⇠ N (0, 2wv /c).",2.1.6. NON-UNIFORM KERNELS,[0],[0]
Each choice of v will induce a new dynamical equation analogous to eqn.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"(2.3) (see SM),
⌃l+1 = Av ? C(⌃ l ) , (2.13)
where Av = diag(v).",2.1.6. NON-UNIFORM KERNELS,[0],[0]
It follows directly from the previous analysis that the linearized dynamics of eqn.,2.1.6. NON-UNIFORM KERNELS,[0],[0]
"(2.13) will be identical to the dynamics of eqn (2.3), only now with ↵,↵0 = F(Av)⇤↵,↵0 .",2.1.6. NON-UNIFORM KERNELS,[0],[0]
"By the same argument presented in Section 2.1.3, the set of eigenvalues is now comprised of n copies of the 1D Fourier transform of v. As a result, it is possible to control the depth scales over which different modes of the signal can propagate through the network by changing the variance vector v. We will return to this point in section 2.4.",2.1.6. NON-UNIFORM KERNELS,[0],[0]
We now turn our attention to the back-propgation of error signals through a convolutional network.,2.2. Back-propagation of signal,[0],[0]
"Let E denote the loss and lj(↵) the back-propagated signal at layer l, channel j and spatial location ↵, i.e.,
lj(↵) = @E
@hlj(↵) .",2.2. Back-propagation of signal,[0],[0]
"(2.14)
",2.2. Back-propagation of signal,[0],[0]
"The recurrence relation is given by
lj(↵) = X
i2chn
X",2.2. Back-propagation of signal,[0],[0]
2ker l+1i (↵ )!,2.2. Back-propagation of signal,[0],[0]
l+1 ji ( ) 0,2.2. Back-propagation of signal,[0],[0]
"(hlj(↵)).
",2.2. Back-propagation of signal,[0],[0]
"As in (Schoenholz et al., 2017), we additionally make the assumption that the weights used during back-propagation are drawn independently from the weights used in forward propagation, in which case the random variables { lj}j2chn are independent for each l.",2.2. Back-propagation of signal,[0],[0]
The covariance matrices ⌃̃l ⌘ E ⇥,2.2. Back-propagation of signal,[0],[0]
"lj( l j) T ⇤ back-propagate according to, ⌃̃l↵,↵0 = X
2ker v ⌃̃
l+1 ↵ ,↵0 · 2 wEh⇠N (0,⌃l)[ 0(h↵) 0(h↵0)] .
(2.15)",2.2. Back-propagation of signal,[0],[0]
"We are primarily interested in the diagonal of ⌃̃l, which measures the variance of back-propagated signals.",2.2. Back-propagation of signal,[0],[0]
We will also assume l > l0 (see section 2.1.3) so that ⌃l is wellapproximated by ⌃⇤.,2.2. Back-propagation of signal,[0],[0]
"In this case,
⌃̃l↵,↵ ⇡ 1 X
2ker v ⌃̃
l+1 ↵ ,↵ , (2.16)
where we used eqn.",2.2. Back-propagation of signal,[0],[0]
(2.7).,2.2. Back-propagation of signal,[0],[0]
"Therefore we find that, ⌃̃l↵,↵ ⇠ L l1 ⌃̃ L ↵,↵, where L is the total depth of the network.",2.2. Back-propagation of signal,[0],[0]
"As in the fully-connected case, 1 = 1 is a necessary condition for gradient signals to neither explode nor vanish as they back-propagate through a convolutional network.",2.2. Back-propagation of signal,[0],[0]
"However, as discussed in (Pennington et al., 2017; 2018), this is not always a sufficient condition for trainability.",2.2. Back-propagation of signal,[0],[0]
"To further understand backward signal propagation, we need to push our analysis beyond mean field theory.",2.2. Back-propagation of signal,[0],[0]
"We have observed that the quantity 1 is crucial for determining signal propagation in CNNs, both in the forward and backward directions.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As discussed in (Poole et al., 2016), 1 equals the the mean squared singular value of the Jacobian J l of the layer-to-layer transition operator.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Beyond just the second moment, higher moments and indeed the whole distribution of singular values of the entire end-to-end Jacobian J = Q l J
l are important for ensuring trainability of very deep fully-connected networks (Pennington et al., 2017; 2018).",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Specifically, networks train well when their input-output Jacobians exhibit dynamical isometry, namely the property that the entire distribution of singular values is close to 1.
",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In fact, we can adopt the entire analysis of (Pennington et al., 2017; 2018) into the convolutional setting with essentially no modification.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"The reason stems from the fact that, because convolution is a linear operator, it has a matrix representation, W l, which appears in the end-to-end Jacobian in precisely the same manner as do the weight matrices in the fully-connected case.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In particular, J = QL l=1 D
lW",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"l, where Dl is the diagonal matrix whose diagonal elements contain the vectorized representation of derivatives of postactivation neurons in layer l. Roughly speaking, since this is the same expression as in (Pennington et al., 2017; 2018), the conclusions found in that work regarding dynamical isometry apply equally well in the convolutional setting.
",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
The analysis of Pennington et al. (2017; 2018) reveals that the singular values of J depends crucially on the distribution of singular values of W l and Dl.,2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In particular, to achieve dynamical isometry, all of these matrices should be close to orthogonal.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As in the fully-connected case, the singular values of Dl can be made arbitrarily close to 1 by choosing a small value for q⇤ and by using an activation function like tanh that is smooth and linear near the origin.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In the convolutional setting, the matrix representation of the convolution operator W l is a c ⇥ c block matrix with n ⇥ n circulant blocks.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Note that in the large c limit, n/c! 0 and the relative size of the blocks vanishes.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"Therefore, if the weights are i.i.d. random variables, we can invoke universality results from random matrix theory to conclude its singular value distribution converges to the Marcenko-Pastur distribution; see Fig. S3 in the SM.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"As such, we find that CNNs with i.i.d. weights cannot achieve dynamical isometry.",2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
We address this issue in the next section.,2.2.1. BEYOND MEAN FIELD THEORY,[0],[0]
"In (Pennington et al., 2017; 2018), it was observed that dynamical isometry can lead to dramatic improvements in training speed, and that achieving these favorable conditions requires orthogonal weight initializations.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"While the procedure to generate random orthogonal weight matrices in the fully-connected setting is well-known, it is less obvious how to do so in the convolutional setting, and at first sight it is not at all clear whether it is even possible.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"We resolve this question by invoking a result from the wavelet literature (Kautsky & Turcajov, 1994) and provide an explicit construction.",2.3. Orthogonal Initialization for CNNs,[0],[0]
"We will focus on the two-dimensional convolution here and begin with some notation.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Definition 2.1.,2.3. Orthogonal Initialization for CNNs,[0],[0]
We say K 2 Rk⇥k⇥cin⇥cout is an orthogonal kernel,2.3. Orthogonal Initialization for CNNs,[0],[0]
"if for all x 2 Rn⇥n⇥cin , kK ⇤ xk2 = kxk2.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Definition 2.2.,2.3. Orthogonal Initialization for CNNs,[0],[0]
"Consider the block matrices B = {Bi,j}0i,jp 1 2 Rpn⇥pn and C = {Ci,j}0i,jq 1 2 Rqn⇥qn, with constituent blocks Bi,j 2 Rn⇥n and Ci,j 2
Rn⇥n. Define the block-wise convolution operator ⇤ by,
[B⇤C]i,j = X
i0,j0
Bi0,j0Ci i0,j j0 , (2.17)
where the out-of-range matrices are taken to be zero.
",2.3. Orthogonal Initialization for CNNs,[0],[0]
Algorithm 1 shows how to construct orthogonal kernels for 2D convolutions of size k ⇥,2.3. Orthogonal Initialization for CNNs,[0],[0]
k ⇥ cin ⇥ cout with cin  cout.,2.3. Orthogonal Initialization for CNNs,[0],[0]
One can employ the same method to construct kernels of higher (or lower) dimensions.,2.3. Orthogonal Initialization for CNNs,[0],[0]
This new initialization method can dramatically boost the learning speed of deep CNNs; see Fig. 5 and Section 3.2.,2.3. Orthogonal Initialization for CNNs,[0],[0]
"In Section 2.1.5 it was observed that, in contrast to fullyconnected networks, CNNs have multiple depth scales controlling propagation of signals along different Fourier modes.",2.4. Delta-Orthogonal Initialization,[0],[0]
"Even at criticality, for generic variance-averaging vectors v, the majority of these depth scales are finite.",2.4. Delta-Orthogonal Initialization,[0],[0]
"However, there does exist one special averaging vector for which all of the depth scales are infinite: a one-hot vector, i.e. vi = k,i.",2.4. Delta-Orthogonal Initialization,[0],[0]
This kernel places all of its variance in the spatial center of the kernel and zero variance elsewhere.,2.4. Delta-Orthogonal Initialization,[0],[0]
"In this case, the eigenvalues ↵,↵0 are all equal to 1 and all depth scales diverge, implying that signals can propagate arbitrarily far along all Fourier modes.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"If we combine this special averaging vector with the orthogonal initialization of the previous section, we obtain a powerful new initialization scheme that we call Delta-Orthogonal Initialization.",2.4. Delta-Orthogonal Initialization,[0],[0]
"Matrices of this type can be generated from Algorithm 1 with k = 1 and padding with appropriate zeros or directly from Algorithm 2 in the SM.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"In the following sections, we demonstrate experimentally that extraordinarily deep convolutional networks can be trained with these initialization techniques.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"Algorithm 1 2D orthogonal kernels for CNNs, available in TensorFlow via the ConvolutionOrthogonal initializer.
",2.4. Delta-Orthogonal Initialization,[0],[0]
"Input: k kernel size, cin number of input channels, cout number of output channels.",2.4. Delta-Orthogonal Initialization,[0],[0]
Return: a k⇥ k⇥ cin ⇥ cout tensor K. Step 1.,2.4. Delta-Orthogonal Initialization,[0],[0]
"Let K be the 1⇥ 1⇥ cout⇥ cout tensor such that K[0, 0] = I , where I is the cout ⇥ cout identity matrix.",2.4. Delta-Orthogonal Initialization,[0],[0]
Step 2.,2.4. Delta-Orthogonal Initialization,[0],[0]
Repeat the following (k 1) times: Randomly generate two orthogonal projection matrices P and Q of size cout ⇥ cout and set (see eqn.,2.4. Delta-Orthogonal Initialization,[0],[0]
"(2.17))
",2.4. Delta-Orthogonal Initialization,[0],[0]
"K K⇤ 
PQ P (1 Q) (1 P )Q (1 P )(1 Q) .
",2.4. Delta-Orthogonal Initialization,[0],[0]
Step 3.,2.4. Delta-Orthogonal Initialization,[0],[0]
"Randomly generate a cin ⇥ cout matrix H with orthonormal rows and for i = 0, . . .",2.4. Delta-Orthogonal Initialization,[0],[0]
",",2.4. Delta-Orthogonal Initialization,[0],[0]
"k 1 and j = 0, . . .",2.4. Delta-Orthogonal Initialization,[0],[0]
", k 1, set K[i, j] HK[i, j].",2.4. Delta-Orthogonal Initialization,[0],[0]
Return K.,2.4. Delta-Orthogonal Initialization,[0],[0]
"To support the theoretical results built up in Section 2, we trained a large number of very deep CNNs on MNIST and CIFAR-10 with tanh as the activation function.",3. Experiments,[0],[0]
We use the following vanilla CNN architecture.,3. Experiments,[0],[0]
First we apply three 3 ⇥ 3 ⇥,3. Experiments,[0],[0]
"c convolutions with strides 1, 2 and 2 in order to increase the channel size to c and reduce the spatial dimension to 7 ⇥ 7 (or 8 ⇥ 8 for CIFAR-10), and then a block of d 3 ⇥ 3 ⇥",3. Experiments,[0],[0]
"c convolutions with d varying from 2 to 10, 000.",3. Experiments,[0],[0]
"Finally, an average pooling layer and a fullyconnected layer are applied.",3. Experiments,[0],[0]
Here c = 256 when d  256 and c = 128 otherwise.,3. Experiments,[0],[0]
"To maximally support our theories, we applied no common techniques (including learning rate decay).",3. Experiments,[0],[0]
"Note that the early downsampling is necessary from a computational perspective, but it does diminish the maximum achievable performance; e.g. our best achieved test accuracy with downsampling was 82% on CIFAR-10.",3. Experiments,[0],[0]
We performed an additional experiment training a 50 layers network without downsampling.,3. Experiments,[0],[0]
"This resulted in a test accuracy of 89.90%, which is comparable to the best performance on CIFAR-10 using a tanh architecture that we were able to find (89.82%, (Mishkin & Matas, 2015)).",3. Experiments,[0],[0]
The analysis in Section 2.1 gives a prediction for precisely which initialization hyperparameters a CNN will be trainable.,3.1. Trainability and Critical Initialization,[0],[0]
"In particular, we predict that the network ought to be trainable provided L .",3.1. Trainability and Critical Initialization,[0],[0]
"⇠c. To test this, we train a large number of convolutional neural networks on MNIST with depth varying between L = 10 and L = 600 and with weights initialized with 2w 2",3.1. Trainability and Critical Initialization,[0],[0]
"[0, 4].",3.1. Trainability and Critical Initialization,[0],[0]
In Fig. 2 we plot – using a heatmap – the training accuracy obtained by these networks after different numbers of steps.,3.1. Trainability and Critical Initialization,[0],[0]
"Additionally we
overlay the depth scale predicted by our theory, ⇠c. We find strikingly good agreement between our theory of random networks and the results of our experiments.",3.1. Trainability and Critical Initialization,[0],[0]
We argued in Section 2.2.1 that the input-output Jacobian of CNNs with i.i.d. weights will become increasingly illconditioned as the number of layers grows.,3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"On the other hand, orthogonal weight initializations can achieve dynamical isometry and dramatically boost the training speed.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"To verify this, we train a 4,000-layer CNN on MNIST using a critically-tuned Gaussian weight initialization and the orthogonal initialization scheme developed in Section 2.3.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"Fig. 5 shows that the network with Gaussian initialization learns slowly (test and training accuracy is below 60% after 90, 000 steps, about 60 epochs).",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
"In contrast, orthogonal initialization learns quickly with test accuracy above 60% after only 1 epoch, and achieves 95% after 10, 000 steps or about 7 epochs.",3.2. Orthogonal Initialization and Ultra-deep CNNs,[0],[0]
The analysis in Section 2.1.3 and Section 2.1.6 suggest that CNNs initialized with kernels with spatially uniform variance may suffer a degradation in generalization performance as the depth increases.,3.3. Multi-dimensional Signal Propagation,[0],[0]
Fig. 3 shows the learning curves of CNNs on CIFAR-10 with depth varying from 32 to 8192.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"Although the orthogonal initialization enables even the deepest model to reach 100% training accuracy, the test accuracy decays as the depth increases with the deepest mode generalizing only marginally better than a fully-connected network.
",3.3. Multi-dimensional Signal Propagation,[0],[0]
"To test whether this degradation in performance may be the result of attenuation of spatially non-uniform signals, we trained a variety of models on CIFAR-10 whose kernels were initialized with spatially non-uniform variance.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"According to the analysis in Section 2.1.6, changing the shape of this non-uniformity controls the depth scales over which different Fourier components of the signal can propagate through the network.",3.3. Multi-dimensional Signal Propagation,[0],[0]
We examined five different non-uniform critical Gaussian initialization methods.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"The variance vectors v were chosen in the following way: GS0 refers to the one-hot delta initialization for which the eigenvalues ↵,↵0 are all equal to 1.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"GS1, GS2 and GS3 are obtained by interpolating between GS0 and GS4, which is the uniform variance initialization.
",3.3. Multi-dimensional Signal Propagation,[0],[0]
"Each variance vector has exactly 8⇥ 8 singular values, plotted in Fig. 4(b) in descending order.",3.3. Multi-dimensional Signal Propagation,[0],[0]
"Note that from GS0 to GS4, the singular values become more poorly-conditioned (the distribution becomes more concentrated around 0).",3.3. Multi-dimensional Signal Propagation,[0],[0]
Fig. 4(a) shows that the relative fall-off of generalization performance with depth follows the same pattern: the more poorly-conditioned the singular values the worse the model generalizes.,3.3. Multi-dimensional Signal Propagation,[0],[0]
"These observations suggest that salient infor-
mation may be propagating along multiple Fourier modes.",3.3. Multi-dimensional Signal Propagation,[0],[0]
Our theory predicts that an ultra-deep CNNs can train faster and perform better if critically initialized using DeltaOrthogonal kernels.,"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"To test this theory, we train CNNs of 1,250, 2,500, 5,000 and 10,000 layers on both MNIST and CIFAR-10 (Fig. 1).","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"All these networks learn surprisingly quickly and, remarkably, the learning time measured in number of training epochs is independent of depth.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"Furthermore, our experimental results match well with the predicted benefits of this initialization: 99% test accuracy on MNIST for a 10,000-layer network, and 82% on CIFAR-10.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"To isolate the benefits of the Delta-Orthogonal init, we also train a 2048- layer CNN (Fig. 3) using the spatially-uniform orthogonal initialization proposed in Section 2.3; the testing accuracy is about 70%.","3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
Note that the test accuracy using (spatially uniform),"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
Gaussian (non-orthogonal) initialization is already below 70% when the depth is 259.,"3.4. Training 10,000-layers: Delta-Orthogonal Initialization.",[0],[0]
"In this work, we developed a theoretical framework based on mean field theory to study the propagation of signals in deep convolutional neural networks.",4. Discussion,[0],[0]
"By examining the necessary conditions for signals to flow both forward and backward through the network without attenuation, we derived an initialization scheme that facilitates training of vanilla CNNs of unprecedented depths.",4. Discussion,[0],[0]
"We presented an algorithm for the generation of random orthogonal convolutional kernels, an ingredient that is necessary to enable dynamical isometry, i.e. good conditioning of the network’s input-output Jacobian.",4. Discussion,[0],[0]
"In contrast to the fully-connected case, signal propagation in CNNs is intrinsically multi-dimensional – we showed how to decompose those signals into independent Fourier modes and how to promote uniform signal propagation across them.",4. Discussion,[0],[0]
"By leveraging these various theoretical insights, we demonstrated empirically that it is possible to train vanilla CNNs with 10,000 layers or more.
",4. Discussion,[0],[0]
Our results indicate that we have removed all the major fundamental obstacles to training arbitrarily deep vanilla convolutional networks.,4. Discussion,[0],[0]
"In doing so, we have layed the groundwork to begin addressing some outstanding questions in the deep learning community, such as whether depth alone can deliver enhanced generalization performance.",4. Discussion,[0],[0]
"Our initial results suggest that past a certain depth, on the order of tens or hundreds of layers, the test performance for vanilla convolutional architecture saturates.",4. Discussion,[0],[0]
"These observations suggest that architectural features such as residual connections and batch normalization are likely to play an important role in defining a good model class, rather than simply enabling efficient training.",4. Discussion,[0],[0]
"We thank Xinyang Geng, Justin Gilmer, Alex Kurakin, Jaehoon Lee, Hoang Trieu Trinh, and Greg Yang for useful discussions and feedback.",Acknowledgements,[0],[0]
"In recent years, state-of-the-art methods in computer vision have utilized increasingly deep convolutional neural network architectures (CNNs), with some of the most successful models employing hundreds or even thousands of layers.",abstractText,[0],[0]
A variety of pathologies such as vanishing/exploding gradients make training such deep networks challenging.,abstractText,[0],[0]
"While residual connections and batch normalization do enable training at these depths, it has remained unclear whether such specialized architecture designs are truly necessary to train deep CNNs.",abstractText,[0],[0]
"In this work, we demonstrate that it is possible to train vanilla CNNs with ten thousand layers or more simply by using an appropriate initialization scheme.",abstractText,[0],[0]
"We derive this initialization scheme theoretically by developing a mean field theory for signal propagation and by characterizing the conditions for dynamical isometry, the equilibration of singular values of the input-output Jacobian matrix.",abstractText,[0],[0]
These conditions require that the convolution operator be an orthogonal transformation in the sense that it is norm-preserving.,abstractText,[0],[0]
We present an algorithm for generating such random initial orthogonal convolution kernels and demonstrate empirically that they enable efficient training of extremely deep architectures.,abstractText,[0],[0]
"Dynamical Isometry and a Mean Field Theory of CNNs: How to Train 10,000-Layer Vanilla Convolutional Neural Networks",title,[0],[0]
"Recurrent Neural Networks (RNNs) (Rumelhart et al., 1986; Elman, 1990) have found widespread use across a variety of domains from language modeling (Mikolov et al., 2010; Kiros et al., 2015; Jozefowicz et al., 2016) and machine translation (Bahdanau et al., 2014) to speech recogni-
*",1. Introduction,[0],[0]
Equal contribution 1Google 2Google Brain.,1. Introduction,[0],[0]
"Correspondence to: Minmin Chen <minminc@google.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tion (Graves et al., 2013) and recommendation systems (Hidasi et al., 2015; Wu et al., 2017).",1. Introduction,[0],[0]
"However, RNNs as originally proposed are difficult to train and are rarely used in practice.",1. Introduction,[0],[0]
"Instead, variants of RNNs - such as Long ShortTerm Memory (LSTM) networks (Hochreiter & Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Chung et al., 2014) - that feature various forms of “gating” perform significantly better than their vanilla counterparts.",1. Introduction,[0],[0]
"Often, these models must be paired with techniques such as normalization layers (Ioffe & Szegedy, 2015b; Ba et al., 2016) and gradient clipping (Pascanu et al., 2013) to achieve good performance.
",1. Introduction,[0],[0]
"A rigorous explanation for the remarkable success of gated recurrent networks remains illusive (Jozefowicz et al., 2015; Greff et al., 2017).",1. Introduction,[0],[0]
"Recent work (Collins et al., 2016) provides empirical evidence that the benefits of gating are mostly rooted in improved trainability rather than increased capacity or expressivity.",1. Introduction,[0],[0]
The problem of disentangling trainability from expressivity is widespread in machine learning since state-of-the-art architectures are nearly always the result of sparse searches in high dimensional spaces of hyperparameters.,1. Introduction,[0],[0]
"As a result, we often mistake trainability for expressivity.",1. Introduction,[0],[0]
"Seminal early work (Glorot & Bengio; Bertschinger et al.) showed that a major hindrance to trainability was the vanishing and exploding of gradients.
",1. Introduction,[0],[0]
"Recently, progress has been made in the feed-forward setting (Schoenholz et al., 2017; Pennington et al., 2017; Yang & Schoenholz, 2017) by developing a theory of both the forward-propagation of signal and the backwardpropagation of gradients.",1. Introduction,[0],[0]
This theory is based on studying neural networks whose weights and biases are randomly distributed.,1. Introduction,[0],[0]
"This is equivalent to studying the behavior of neural networks after random initialization or, equivalently, to studying the prior over functions induced by a particular choice of hyperparameters (Lee et al., 2017).",1. Introduction,[0],[0]
"It was shown that randomly initialized neural networks are trainable if three conditions are satisfied: (1) the size of the output of the network is finite for finite inputs, (2) the output of the network is sensitive to changes in the input, and (3) gradients neither explode nor vanish.",1. Introduction,[0],[0]
"Moreover, neural networks achieving dynamical isometry, i.e. having input-output Jacobian matrices that are well-conditioned, were shown to ar X iv :1
80 6.
05 39
4v 2
[ st
at .M
L ]
1 5
A ug
2 01
8
train orders of magnitude faster than networks that do not.
",1. Introduction,[0],[0]
"In this work, we combine mean field theory and random matrix theory to extend these results to the recurrent setting.",1. Introduction,[0],[0]
We will be particularly focused on understanding the role that gating plays in trainability.,1. Introduction,[0],[0]
"As we will see, there are a number of subtleties that must be addressed for (gated) recurrent networks that were not present in the feed-forward setting.",1. Introduction,[0],[0]
"To clarify the discussion, we will therefore contrast vanilla RNNs with a gated RNN cell, that we call the minimalRNN, which is significantly simpler than LSTMs and GRUs but implements a similar form of gating.",1. Introduction,[0],[0]
"We expect the framework introduced here to be applicable to more complicated gated architectures.
",1. Introduction,[0],[0]
The first main contribution of this paper is the development of a mean field theory for forward propagation of signal through vanilla RNNs and minimalRNNs.,1. Introduction,[0],[0]
"In doing so, we identify a theory of the maximum timescale over which signal can propagate in each case.",1. Introduction,[0],[0]
"Next, we produce a random matrix theory for the end-to-end Jacobian of the minimalRNN.",1. Introduction,[0],[0]
"As in the feed-forward setting, we establish that the duality between the forward propagation of signal and the backward propagation of gradients persists in the recurrent setting.",1. Introduction,[0],[0]
We then show that our theory is indeed predictive of trainability in recurrent neural networks by comparing the maximum trainable number of steps of RNNs with the timescale predicted by the theory.,1. Introduction,[0],[0]
"Overall, we find remarkable alignment between theory and practice.",1. Introduction,[0],[0]
"Additionally, we develop a closed-form initialization procedure for both networks and show that on a variety of tasks RNNs initialized to be dynamically isometric are significantly easier to train than those lacking this property.
",1. Introduction,[0],[0]
"Corroborating the experimental findings of Collins et al. (2016), we show that both signal propagation and dynamical isometry in vanilla RNNs is far more precarious than in the case of the minimalRNN.",1. Introduction,[0],[0]
"Indeed the vanilla RNN achieves dynamical isometry only if the network is initialized with orthogonal weights at the boundary between order-and-chaos, a one-dimensional line in parameter space.",1. Introduction,[0],[0]
"Owing to its gating mechanism, the minimalRNN on the other hand enjoys a robust multi-dimensional subspace of good initializations which all enable dynamical isometry.",1. Introduction,[0],[0]
"Based on these insights, we conjecture that more complex gated recurrent neural networks also benefit from the similar effects.",1. Introduction,[0],[0]
"Identity and Orthogonal initialization schemes have been identified as a promising approach to improve trainability of deep neural networks (Le et al., 2015; Mishkin & Matas, 2015).",2. Related Work,[0],[0]
"Additionally, Arjovsky et al. (2016); Hyland & Rätsch (2017); Xie et al. (2017) advocate going beyond initialization to constrain the transition matrix to be orthog-
onal throughout the entire learning process either through re-parametrisation or by constraining the optimization to the Stiefel manifold (Wisdom et al., 2016).",2. Related Work,[0],[0]
"However, as was pointed out in Vorontsov et al. (2017), strictly enforcing orthogonality during training may hinder training speed and generalization performance.",2. Related Work,[0],[0]
"While these contributions are similar to our own, in the sense that they attempt to construct networks that feature dynamical isometry, it is worth noting that orthogonal weight matrices do not guarantee dynamical isometry.",2. Related Work,[0],[0]
This is due to the nonlinear nature of deep neural networks as shown in Pennington et al. (2017).,2. Related Work,[0],[0]
"In this paper we continue this trend and show that orthogonality has little impact on the conditioning of the Jacobian (and so trainability) in gated RNNs.
",2. Related Work,[0],[0]
The notion of “edge of chaos” initialization has been explored previously especially in the case of recurrent neural networks.,2. Related Work,[0],[0]
Bertschinger et al.; Glorot & Bengio propose edge-of-chaos initialization schemes that they show leads to improved performance.,2. Related Work,[0],[0]
"Additionally, architectural innovations such as batch normalization (Ioffe & Szegedy, 2015a), orthogonal matrix initialization (Saxe et al., 2013), random walk initialization (Sussillo & Abbott, 2014), composition kernels (Daniely et al., 2016), or residual network architectures (He et al., 2015) all share a common goal of stabilizing gradients and improving training dynamics.
",2. Related Work,[0],[0]
There is a long history of applying mean field-like approaches to understand the behavior of neural networks.,2. Related Work,[0],[0]
"Indeed several pieces of seminal work used statistical physics (Derrida & Pomeau; Sompolinsky et al., 1988) and Gaussian Processes (Neal, 2012) to show that neural networks exhibit remarkable regularity as the width of the network gets large.",2. Related Work,[0],[0]
"Mean field theory also has long been used to study Boltzmann machines (Ackley et al.) and sigmoid belief networks (Saul et al., 1996).",2. Related Work,[0],[0]
"More recently, there has been a revitalization of mean field theory to explore questions of trainability and expressivity in fully-connected networks and residual networks (Poole et al., 2016; Schoenholz et al., 2017; Yang & Schoenholz, 2017; Schoenholz et al., 2017; Karakida et al., 2018; Hayou et al., 2018; Hanin & Rolnick, 2018; Yang & Schoenholz, 2018).",2. Related Work,[0],[0]
Our approach will closely follow these later contributions and extend many of their techniques to the case of recurrent networks with gating.,2. Related Work,[0],[0]
"Beyond mean field theory, there have been several attempts in understanding signal propagation in RNNs, e.g., using Gers̆gorin circle theorem (Zilly et al., 2016) or time invariance (Tallec & Ollivier, 2018).",2. Related Work,[0],[0]
We begin by developing a mean field theory for vanilla RNNs and discuss the notion of dynamical isometry.,3. Theory and Critical Initialization,[0],[0]
"Afterwards, we move on to a simple gated architecture to explain the role of gating in facilitating signal propagation in RNNs.",3. Theory and Critical Initialization,[0],[0]
"Vanilla RNNs are described by the recurrence relation,
et = Wht−1 + V xt + b ht = φ(et).",3.1. Vanilla RNN,[0],[0]
"(1)
Here xt ∈ RM is the input, et ∈ RN is the pre-activation, and ht ∈ RN is the hidden state after applying an arbitrary activation function φ :",3.1. Vanilla RNN,[0],[0]
R → R. For the purposes of this discussion we set φ = tanh.,3.1. Vanilla RNN,[0],[0]
"Furthermore, W ∈ RN×N and V ∈ RN×M are weight matrices that multiply the hidden state and inputs respectively and b ∈ RN is a bias.
",3.1. Vanilla RNN,[0],[0]
"Next, we apply mean-field theory to vanilla RNNs following a similar strategy introduced in (Poole et al., 2016; Schoenholz et al., 2017).",3.1. Vanilla RNN,[0],[0]
"At the level of mean-field theory, vanilla RNNs will prove to be intimately related to feed-forward networks and so this discussion proceeds analogously.",3.1. Vanilla RNN,[0],[0]
"For a more detailed discussion, see these earlier studies.
",3.1. Vanilla RNN,[0],[0]
"Consider two sequences of inputs {xt1} and {xt2}, described by the covariance matrix Rt ∈ R2×2 with Rtab = 1 ME[xa · xb], a, b ∈ {1, 2}.",3.1. Vanilla RNN,[0],[0]
"To simplify notation, we assume the input sequences have been standardized so that Rt11 = R t 22 = R independent of time.",3.1. Vanilla RNN,[0],[0]
"This allows us to write Rt = RΣt, where Σt is a matrix whose diagonal terms are 1 and whose off-diagonal terms are the cosine similarity between the inputs at time t. These sequences are then passed into two identical copies of an RNN to produce two corresponding pre-activation sequences {et1} and {et2}.",3.1. Vanilla RNN,[0],[0]
"As in Poole et al. (2016) we let the weights and biases be Gaussian distributed so that Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/M), and bi ∼ N (µb, σ2b )1, and we consider the wide network limit, N →∞.",3.1. Vanilla RNN,[0],[0]
"As in the fully-connected setting, we would like to invoke the Central Limit Theorem (CLT) to conclude that the pre-activations of hidden states are jointly Gaussian distributed.",3.1. Vanilla RNN,[0],[0]
"Unfortunately, the CLT is violated in the recurrent setting as ht−1 is correlated with W due to weight sharing between steps of the RNN.
",3.1. Vanilla RNN,[0],[0]
"To make progress, we proceed by developing the theory of signal propagation for RNNs with untied weights.",3.1. Vanilla RNN,[0],[0]
"This allows for several simplifications, including the application of the CLT to conclude that etia are jointly Gaussian distributed,
[eti1, e t j2] T ∼ N (µb1, qtδij), i, j ∈ {1, · · · , N}
where the covariance matrix qt ∈ R2×2 is independent of neuron index, i. We explore the ramifications of this approximation by comparing simulations of RNNs with tied and untied weights.",3.1. Vanilla RNN,[0],[0]
"Overall, we will see that while ignoring weight tying leads to quantitative differences between theory and experiment, it does not change the qualitative picture that emerges.",3.1. Vanilla RNN,[0],[0]
See figs.,3.1. Vanilla RNN,[0],[0]
"1 and 2 for verification.
",3.1. Vanilla RNN,[0],[0]
"With this approximation in mind, we will now quantify how the pre-activation hidden states {et1} and {et2} evolve by
1in practice we will set µb = 0 for vanillaRNN.
deriving the recurrence relation of the covariance matrix qt from the recurrence on et in eq.",3.1. Vanilla RNN,[0],[0]
(1).,3.1. Vanilla RNN,[0],[0]
"Using identical arguments to Poole et al. (2016) one can show that,
qt = σ2w ∫ Dqt−1z φ(z)φ(z)",3.1. Vanilla RNN,[0],[0]
>,3.1. Vanilla RNN,[0],[0]
"+ σ2vRΣ t + σ2bI. (2)
",3.1. Vanilla RNN,[0],[0]
"where z = [z1, z2]>, and∫ Dqz = 1
2π √ |q|
∫ dze−(z−µb1) T q−1(z−µb1) (3)
is a Gaussian measure with covariance matrix q. By symmetry, our normalization allows us to define qt11 = q t 22 = q t to be the magnitude of the pre-activation hidden state and ct = qt12/q
t to be the cosine similarity between the hidden states.",3.1. Vanilla RNN,[0],[0]
"We will be particularly concerned with understanding the dynamics of the cosine similarity, ct.
",3.1. Vanilla RNN,[0],[0]
"In feed-forward networks, the inputs dictate the initial value of the cosine similarity, c0 and then the evolution of ct is determined solely by the network architecture.",3.1. Vanilla RNN,[0],[0]
"By contrast in recurrent networks, inputs perturb ct at each timestep.",3.1. Vanilla RNN,[0],[0]
"Analyzing the dynamics of ct for arbitrary Σt is therefore challenging, however significant insight can be gained by studying the off-diagonal entries of eq.",3.1. Vanilla RNN,[0],[0]
(2) for Σt = Σ independent of time.,3.1. Vanilla RNN,[0],[0]
"In the case of time-independent Σt, as t → ∞ both qt → q∗ and ct → c∗ where q∗ and c∗ are fixed points of the variance of the pre-activation hidden state and the cosine-similarity between pre-activation hidden states respectively.",3.1. Vanilla RNN,[0],[0]
"As was discussed previously (Poole et al., 2016; Schoenholz et al., 2017), the dynamics of qt are generally uninteresting provided q∗ is finite.",3.1. Vanilla RNN,[0],[0]
"We therefore choose to normalize the hidden state such that q0 = q∗ which implies that qt = q∗ independent of time.
",3.1. Vanilla RNN,[0],[0]
"In this setting it was shown in Schoenholz et al. (2017) that in the vicinity of a fixed point, the off-diagonal term in eq.",3.1. Vanilla RNN,[0],[0]
(2) can be expanded to lowest order in t = c∗,3.1. Vanilla RNN,[0],[0]
"− ct to give the linearized dynamics, t = χc∗ t−1 where
χc∗ = σ 2 w ∫ Dq∗zφ ′(z1)φ ′(z2).",3.1. Vanilla RNN,[0],[0]
"(4)
These dynamics have the solution t = χt−t0c∗ t0 where t0 is the time when ct is close enough to c∗ for the linear approximation to be valid.",3.1. Vanilla RNN,[0],[0]
If χc∗ < 1 it follows that ct approaches c∗ exponentially quickly over a timescale τ = −1/,3.1. Vanilla RNN,[0],[0]
logχc∗ and c∗ is called a stable fixed point.,3.1. Vanilla RNN,[0],[0]
"When ct gets too close to c∗ to be distinguished from it to within numerical precision, information about the initial inputs has been lost.",3.1. Vanilla RNN,[0],[0]
"Thus, τ sets the maximum timescale over which we expect the RNN to be able to remember information.",3.1. Vanilla RNN,[0],[0]
If χc∗ > 1 then ct gets exponentially farther from c∗ over time and c∗ is an unstable fixed point.,3.1. Vanilla RNN,[0],[0]
"In this case, for the activation function considered here, another fixed point that
is stable will emerge.",3.1. Vanilla RNN,[0],[0]
"Note that χc∗ is independent of Σ and so the dynamics of ct near c∗ do not depend on Σ.
In vanilla fully-connected networks c∗",3.1. Vanilla RNN,[0],[0]
"= 1 is always a fixed point of ct, but it is not always stable.",3.1. Vanilla RNN,[0],[0]
"Indeed, it was shown that these networks exhibit a phase transition where c∗ = 1 goes from being a stable fixed point to an unstable one as a function of the network’s hyperparameters.",3.1. Vanilla RNN,[0],[0]
This is known as the order-to-chaos transition and it occurs exactly when χ1 = 1.,3.1. Vanilla RNN,[0],[0]
"Since τ = −1/ log(χ1), signal can propagate infinitely far at the boundary between order and chaos.",3.1. Vanilla RNN,[0],[0]
Comparing the diagonal and off-diagonal entries of eq.,3.1. Vanilla RNN,[0],[0]
"(2), we see that in recurrent networks, c∗ = 1 is a fixed point only when Σ12",3.1. Vanilla RNN,[0],[0]
"= 1, and in this case the discussion is identical to the feed-forward setting.",3.1. Vanilla RNN,[0],[0]
"When Σ12 < 1, it is easy to see that c∗ < 1 since if ct = 1 at some time t then ct+1 = 1−σ2vR(1−Σ12)/q∗ < 1.",3.1. Vanilla RNN,[0],[0]
We see that in recurrent networks noise from the inputs destroys the ordered phase and there is no ordered-to-chaos critical point.,3.1. Vanilla RNN,[0],[0]
"As a result we should expect the maximum timescale over which memory may be stored in vanilla RNNs to be fundamentally limited by noise from the inputs.
",3.1. Vanilla RNN,[0],[0]
"The end-to-end Jacobian of a vanilla RNN with untied weights is in fact formally identical to the input-output Jacobian of a feedforward network, and thus the results from (Pennington et al., 2017) regarding conditions for dynamical isometry apply directly.",3.1. Vanilla RNN,[0],[0]
"In particular, dynamical isometry is achieved with orthogonal state-to-state transition matrices W , tanh non-linearities, and small values of q∗.",3.1. Vanilla RNN,[0],[0]
"Perhaps surprisingly, these conclusions continue to be valid if the assumption of untied weights is relaxed.",3.1. Vanilla RNN,[0],[0]
"To understand why this is the case, consider the example of a linear network.",3.1. Vanilla RNN,[0],[0]
"For untied weights, the end-to-end Jacobian is given by Ĵ = ∏T t=1",3.1. Vanilla RNN,[0],[0]
"Wt, while for tied weights the Jacobian is given by J = W T .",3.1. Vanilla RNN,[0],[0]
"It turns out that as N →∞ there is sufficient self-averaging to overcome the dependencies induced by weight tying and the asymptotic singular value distributions of Ĵ and J are actually identical (Haagerup & Larsen, 2000).",3.1. Vanilla RNN,[0],[0]
"To study the role of gating, we introduce the minimalRNN which is simpler than other gated RNN architectures but nonetheless features the same gating mechanism.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"A sequence of inputs xt ∈ RM , is first mapped to the hidden space through x̃t = Φ(xt)2.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"From here on, we refer to x̃t ∈ RN as the inputs to minimalRNN.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"The minimalRNN
2Φ(·) here can be any highly flexible functions such as a feed-forward network.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In our experiments, we take Φ(·) to be a fully connected layer with tanh activation, that is, Φ(xt) = tanh(Wxx t).
is then described by the recurrence relation,
et = Wht−1 + V x̃t + b ut = σ(et) (5)
ht = ut ht−1 + (1− ut) x̃t
where et ∈ RN is the pre-activation to the gating function,",3.2.1. MEAN-FIELD THEORY,[0],[0]
ut ∈ RN the update gate and ht ∈ RN the hidden state.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"The minimalRNN retains the most essential gate in LSTMs (Jozefowicz et al., 2015; Greff et al., 2017) and achieves competitive performance.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"The simplified update of this cell on the other hand, enables us to pinpoint the role of gating in a more controlled setting.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the previous case we consider two sequences of inputs to the network, {x̃t1} and {x̃t2}.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We take Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/N) and bi ∼ N (µb, σ2b ).",3.2.1. MEAN-FIELD THEORY,[0],[0]
"By analogy to the vanilla case, we can make the mean field approximation that the etia are jointly Gaussian distributed with covariance matrix qtδij ∈ R2×2.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Here,
qt = σ2wQ t−1 + σ2vR t + σ2bI (6)
where we have defined Qt as the second-moment matrix with Qtab = E[htiahtib].",3.2.1. MEAN-FIELD THEORY,[0],[0]
3,3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the vanilla case, Rt is the covariance between inputs so that Rtab = 1 NE[x̃a · x̃b].
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We note that Rt is fixed by the input, but it remains for us to work out Qt.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We find that (see SI section B),
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Qt = Qt−1 ∫ Dqtz σ(z)σ(z) > (7)
+ Rt ∫ Dqtz (1− σ(z))(1− σ(z))",3.2.1. MEAN-FIELD THEORY,[0],[0]
">
Here we assume that the expectation factorizes so that ht−1 and ut are approximately independent.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We believe this approximation becomes exact in the N →∞ limit.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
We choose to normalize the data in a similar manner to the vanilla case so that Rt11 = R t 22 = R independent of time.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"An immediate consequence of this normalization is that Qt11 = Q t 22 = Q t and qt11 = q t 22 = q
t.",3.2.1. MEAN-FIELD THEORY,[0],[0]
We then write Ct = Qt12/Q t and ct = qt12/q t as the cosine similarities between the hidden states and the pre-activations respectively.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"With this normalization, we can work out the mean-field recurrence relation characterizing the covariance matrix for the minimalRNN.",3.2.1. MEAN-FIELD THEORY,[0],[0]
This analysis can be done by deriving the recurrence relation for either Qt or qt.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"We will choose to study the dynamics of qt, however the two are trivially related by eq. (6).",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In SI section C, we analyze the dynamics of the diagonal term in the recurrence relation and prove that there is always a fixed point at some q∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In SI section D, we compute the depth scale over which qt approaches q∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"However, as in the case of the vanillaRNN, the dynamics of q∗ are generally uninteresting.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"3ht will be centered under mean field approximation if h0 is initialized with mean zero.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We now turn our attention to the dynamics of the cosine similarity between the pre-activations, ct.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the case of vanilla RNNs, we note that qt approaches q∗ quickly relative to the dynamics of ct.",3.2.1. MEAN-FIELD THEORY,[0],[0]
We therefore choose to normalize the hidden state of the RNN so that Q0 = Q∗ in which case both Qt = Q∗ and qt = q∗ independent of time.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"From eq. (6) and (7) it follows that the cosine similarity of the pre-activation evolves as,
ct =",3.2.1. MEAN-FIELD THEORY,[0],[0]
"[ ct−1 + (σ2w − σ2v)ρt−1 ] ∫ Dqt−1z σ(z1)σ(z2)
",3.2.1. MEAN-FIELD THEORY,[0],[0]
"− 2σ2wρt−1 ∫ Dqt−1z σ(z1) + σ 2 wρ t−1 + σ2vρ t (8)
where we have defined ρt = RΣt12/q ∗.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"As in the case of the vanilla RNN, we can study the behavior of ct in the vicinity of a fixed point, c∗. By expanding eq.",3.2.1. MEAN-FIELD THEORY,[0],[0]
(8) to lowest order in t = c∗,3.2.1. MEAN-FIELD THEORY,[0],[0]
"− ct we arrive at a linearized recurrence relation that has an exponential solution t+1 = χc∗ t where here,
χc∗ = ∫ Dq∗zσ(z1)σ(z2)",3.2.1. MEAN-FIELD THEORY,[0],[0]
"(9)
+ ( q∗c∗ + (σ2w − σ2v)RΣ12 ) ∫",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Dq∗zσ ′(z1)σ ′(z2).
",3.2.1. MEAN-FIELD THEORY,[0],[0]
The discussion above in the vanilla case carries over directly to the minimalRNN with the appropriate replacement of χc∗ .,3.2.1. MEAN-FIELD THEORY,[0],[0]
"Unlike in the case of the vanilla RNN, here we see that χc∗ itself depends on Σ12.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
Again c∗ = 1 is a fixed point of the dynamics only when Σ12 = 1.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"In this case, the minimalRNN experiences an order-to-chaos phase transition when χ1 = 1 at which point the maximum timescale over which signal can propagate goes to infinity.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Similar to the vanilla RNN, when Σ12 < 1, we expect that the phase transition will be destroyed and the maximum duration of signal propagation will be severely limited.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"However, in a significant departure from the vanilla case, when µb → ∞ we notice that σ(z + µb) → 1, and σ′(z+µb)→ 0 for all z. Considering eq. (9) we notice that in this regime χc∗ → 1 independent of Σ12.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In other words, gating allows for arbitrarily long term signal propagation in recurrent neural networks independent of Σ12.
",3.2.1. MEAN-FIELD THEORY,[0],[0]
We explore agreement between our theory and MC simulations of the minimalRNN in fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
1.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"In this set of experiments, we consider inputs such that Σt12 = 0 for t < 10 and Σt12 = 1 for t ≥ 10.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Fig. 1 (a,c,d) show excellent quantitative agreement between our theory and MC simulations.",3.2.1. MEAN-FIELD THEORY,[0],[0]
In fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"1 (a,b) we compare the MC simulations of the minimalRNN with and without weight tying.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"While we observe that for many choices of hyperparameters the untied weight approximation is quite good (particularly when c∗ ≈ 1), deeper into the chaotic phase the quantitative agreement between breaks down.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"Nonetheless, we observe that the untied approximation describes the qualitative behavior of the real
minimalRNN overall.",3.2.1. MEAN-FIELD THEORY,[0],[0]
In fig.,3.2.1. MEAN-FIELD THEORY,[0],[0]
"1 (e) we plot the timescale for signal propagation for Σ12 = 1, 0.99, and 0 for the minimalRNN with identical choices of hyperparameters.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"We see that while τ → ∞ as µb gets large independent of Σ12, a critical point at µb = 0 is only observed when Σ12 = 1.",3.2.1. MEAN-FIELD THEORY,[0],[0]
"In the previous subsection, we derived a quantity χ1 that defines the boundary between the ordered and the chaotic phases of forward propagation.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
Here we show that it also defines the boundary between exploding and vanishing gradients.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"To see this, consider the Jacobian of the state-to-state transition operator,
Jt = ∂ht+1
∂ht = Dut + Dσ′(et) (ht−1−zt)W , (10)
where Dx denotes a diagonal matrix with x along its diagonal.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"We can compute the expected norm-squared of back-propagated error signals, which measures the growth or shrinkage of gradients.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"It is equal to the mean-squared singular value of the Jacobian (Poole et al., 2016; Schoenholz et al., 2017) or the first moment of JtJTt ,
1
N E[tr(JtJTt )] = E[(ut1)2] + σ2wE[σ′(et1)2(h",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"t−1 1 − zt1)2],
(11) where we have used the fact that the elements of ut, ht and zt are i.i.d.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Since we assume convergence to the fixed point, these distributions are independent of t and it is easy to see that 1NE[tr(JtJ T t )]",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
= χ1.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"The variance of backpropagated error signals through T time steps is therefore 1 NE[tr(JJ
T )] = χT1 .",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"As such, the constraint χ1 = 1 defines the boundary between phases of exponentially exploding and exponentially vanishing gradient norm (variance).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Note that unlike in the case of forward signal propagation, in the case of backpropagation this is independent of Σ.
As argued in (Pennington et al., 2017; 2018), controlling the variance of back-propagated gradients is necessary but not sufficient to guarantee trainability, especially for very deep networks.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Beyond the first moment, the entire distribution of eigenvalues of JJT (or of singular values of J) is relevant.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Indeed, it was found in (Pennington et al., 2017; 2018) that enabling dynamical isometry, namely the condition that all singular values of J are close to unity, can drastically improve training speed for very deep feed-forward networks.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Following (Pennington et al., 2017; 2018), we use tools from free probability theory to compute the variance σ2JJT of the limiting spectral density of JJT ; however, unlike previous work, in our case the relevant matrices are not symmetric and therefore we must invoke tools from nonHermitian free probability, see (Cakmak, 2012) for a review.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"As in previous section, we make the simplifying assumption that the weights are untied, relying on the same motivations
given in section 3.1.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Using these tools, an un-illuminating calculation reveals that,
σ2JJT = χ 2T 1
( 1 + T
2(µ1 − s1)µ2 + σ21 + σ22 χ21
) , (12)
where,
χ1 = µ1 + µ2 (13)
µ1 =
∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ2( √ q∗z + µb)
σ21 = −µ21 + ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ4( √ q∗z + µb)
µ2 = σ 2 w(Q ∗ +R)
∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz [σ′( √ q∗z + µb)] 2
σ22 = −µ22 + σ4w((Q∗)2 +R2) ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz [σ′( √ q∗z + µb)] 4
and s1 is the first term in the Taylor expansion of the Stransform of the eigenvalue distribution of WWT (Pennington et al., 2018).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"For example, for Gaussian matrices, s1 = −1 and for orthogonal matrices s1 = 0.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
Some remarks are in order about eq.,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
(12).,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"First, we note the duality between the forward and backward signal propagation (eq. (9) and eq.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
(13)).,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"For critical initializations, χ1 = 1, so σ2JJT does not grow exponentially, but it still grows linearly with T .",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"This situation is entirely analogous to the feed-forward analysis of (Pennington et al., 2017; 2018).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"In the case of the vanilla RNN, the coefficient of the linear term is proportion to q∗, and can only be reduced by taking the weight and bias variances (σ2w, σ 2 b )→ (1, 0).",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"A crucial difference in the minimalRNN is that the coefficient of the linear term can be made arbitrarily small by simply adjusting the bias mean µb to be positive, which will send µ2 → 0 and µ1 → 1 independent of Σ. Therefore the conditions for dynamical isometry decouple from the weight
and bias variances, implying that trainability can occur for a higher-dimensional, more robust, slice of parameter space.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Moreover, the value of s1 has no effect on the capacity of the minimalRNN to achieve dynamical isometry.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"We believe these are fundamental reasons why gated cells such as the minimalRNN perform well in practice.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Algorithm 1 describes the procedure to find σ2w, σ 2 v and σ 2 b to achieve χ1 condition for minimalRNN.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Given σ2w, σ 2 v , σ 2 b , we then construct the weight matrices and biases accordingly.",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Q∗ is used to initialize the h0 to avoid transient phase.
",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Algorithm 1: Critical initialization for minimalRNNs Require: q∗, µb, R
1: E[u2]← ∫",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Dz σ2( √ q∗z + µb)
2: E[(1− u)2]← ∫ Dz (1− σ( √ q∗z + µb))",3.2.2. DYNAMICAL ISOMETRY,[0],[0]
2 3: Q∗ ← R · E[(1− u)2]/(1− E[u2]) (eq.(7)) 4: E[u′2]← ∫ Dz[σ′( √ q∗z + µb)] 2 5: σ2w ← (1− E[u2])/(Q∗ +R)/E[u′2] (eq.(13)) 6: σ2b ← 0 7: σ2v ← (q∗ −Q∗σ2w − σ2b )/R,3.2.2. DYNAMICAL ISOMETRY,[0],[0]
"Having established a theory for the behavior of random vanilla RNNs and minimalRNNs, we now discuss the connection between our theory and trainability in practice.",4. Experiments,[0],[0]
We begin by corroborating the claim that the maximum timescale over which memory can be stored in a RNN is controlled by the timescale τ identified in the previous section.,4. Experiments,[0],[0]
"We will then investigate the role of dynamical isometry in speeding up learning.
6⌧ 6⌧ 6⌧ 6⌧",4. Experiments,[0],[0]
Dataset.,4.1. Trainability,[0],[0]
"To verify the results of our theoretical calculation, we consider a task that is reflective of the theory above.",4.1. Trainability,[0],[0]
"To that end, we constructed a sequence dataset for training RNNs from MNIST (LeCun et al., 1998).",4.1. Trainability,[0],[0]
Each of the 28× 28 digit image is flattened into a vector of 784 pixels and sent as the first input to a RNN.,4.1. Trainability,[0],[0]
"We then send T random inputs xt ∼ N (0, σ2x), 0 < t < T into the RNN varying T between 10 and 1000 steps.",4.1. Trainability,[0],[0]
"As the only salient information about the digit is in the first layer, the network will need to propagate information through T layers to accurately identify the MNIST digit.",4.1. Trainability,[0],[0]
"The random inputs are drawn independently for each example and so this is a regime where Σt = 0 for all t > 0.
",4.1. Trainability,[0],[0]
We then performed a series of experiments on this task to make connection with our theory.,4.1. Trainability,[0],[0]
In each case we experimented with both tied and untied weights.,4.1. Trainability,[0],[0]
The result are shown in fig.,4.1. Trainability,[0],[0]
2.,4.1. Trainability,[0],[0]
"In the case of untied weights, we observe strong quantitative agreement between our theoretical prediction for τ and the maximum depth T where the network is still trainable.",4.1. Trainability,[0],[0]
"When the weights of the network are tied, we observe quantitative deviations between our thoery and experiments, but the overall qualitative picture remains.
",4.1. Trainability,[0],[0]
We train vanilla RNNs for 103 steps (around 10 epochs) varying σw ∈,4.1. Trainability,[0],[0]
"[0.5, 1.5] while fixing σv = 0.025.",4.1. Trainability,[0],[0]
The results of this experiment are shown in fig.,4.1. Trainability,[0],[0]
2 (a-b).,4.1. Trainability,[0],[0]
We train minimalRNNs for 102 steps (around 1 epoch) fixing σv = 1.39.,4.1. Trainability,[0],[0]
"We perform three different experiments here: 1)
varying µb ∈",4.1. Trainability,[0],[0]
"[−4, 8] with σw = 6.88 shown in fig.",4.1. Trainability,[0],[0]
"2 (c-d), 2) varying σw ∈",4.1. Trainability,[0],[0]
"[0.5, 10] with µb = 4 shown in fig.",4.1. Trainability,[0],[0]
"2 (e-f), 3) varying σw ∈",4.1. Trainability,[0],[0]
"[0.5, 10] with µb = 6 shown in fig.",4.1. Trainability,[0],[0]
2 (g-h).,4.1. Trainability,[0],[0]
Comparing fig.,4.1. Trainability,[0],[0]
"2(a,b) with fig.",4.1. Trainability,[0],[0]
"2(c,d, g,h), the minimalRNN with large depth T is trainable over a much wider range of hyperparameters than the vanillaRNN despite the fact that the network was trained for an order of magnitude less time.",4.1. Trainability,[0],[0]
Dataset.,4.2. Critical initialization,[0],[0]
"To study the impact of critical initialization on training speed, we constructed a more realistic sequence dataset from MNIST.",4.2. Critical initialization,[0],[0]
"We unroll the pixels into a sequence of T inputs, each containing 784/T pixels.",4.2. Critical initialization,[0],[0]
"We tested T = 196 and T = 784 to vary the difficulty of the tasks.
",4.2. Critical initialization,[0],[0]
Note that we are more interested in the training speed of these networks under different initialization conditions than the test accuracy.,4.2. Critical initialization,[0],[0]
"We compare the convergence speed of vanilla RNN and minimalRNN under four initialization conditions: 1) critical initialization with orthogonal weights (solid blue); 2) critical initialization with Gaussian distributed weights (sold red); 3) off-critical initialization with orthogonal weights (dotted green); 4) off-critical initialization with Gaussian distributed weights (dotted black).
",4.2. Critical initialization,[0],[0]
We fix σ2b to zero in all settings.,4.2. Critical initialization,[0],[0]
"Under critical initialization, σ2w and σ 2 v are carefully chosen to achieve χ1 = 1 as defined in eqn.(4) for vanilla RNN and eqn.(13) (detailed in algorithm 1) for minimalRNN respectively.",4.2. Critical initialization,[0],[0]
"When testing networks off criticality, we employ a common initialization
procedure in which, σ2w = 1.0 and σ 2 v = 1.0.
",4.2. Critical initialization,[0],[0]
Figure 3 summarizes our findings: there is a clear difference in training speed between models trained with critical initialization compared with models initialized far from criticality.,4.2. Critical initialization,[0],[0]
We observe two orders of magnitude difference in training speed between a critical and off-critical initialization for vanilla RNNs.,4.2. Critical initialization,[0],[0]
"While a critically initialized model reaches a test accuracy of 90% after 750 optimization steps, the offcritical nework takes over 16,000 updates.",4.2. Critical initialization,[0],[0]
A similar trend was observed for the minimalRNN.,4.2. Critical initialization,[0],[0]
This difference is even more pronounced in the case of the longer sequence with T = 784.,4.2. Critical initialization,[0],[0]
Both vanilla RNNs and minimalRNNs initialized off-criticality failed at task.,4.2. Critical initialization,[0],[0]
The well-conditioned minimalRNN trains a factor of three faster than the vanilla RNN.,4.2. Critical initialization,[0],[0]
"As predicted above, the difference in training speed between orthogonal and Gaussian initialization schemes is significant for vanilla RNNs but is insignificant for the minimalRNN.",4.2. Critical initialization,[0],[0]
This is corroborated in fig.,4.2. Critical initialization,[0],[0]
"3 (b,d) where the distribution of the weights has no impact on the training speed.",4.2. Critical initialization,[0],[0]
"We compare the minimalRNN against more complex gated RNNs such as LSTM and GRU on the Penn Tree-Bank corpus (Marcus et al., 1993).",5. Language modeling,[0],[0]
"Language modeling is a difficult task, and competitive performance is often achieved by more complicated RNN cells.",5. Language modeling,[0],[0]
"We show that the minimalRNN achieves competitive performance despite its simplicity.
",5. Language modeling,[0],[0]
"We follow the precise setup of (Mikolov et al., 2010; Zaremba et al., 2014), and train RNNs of two sizes: a small configuration with 5M parameters and a medium-sized configuration with 20M parameters 4.",5. Language modeling,[0],[0]
We report the perplexity on the validation and test sets.,5. Language modeling,[0],[0]
"We focus our comparison on single layer RNNs, however we also report perplexities for multi-layer RNNs from the literature for reference.",5. Language modeling,[0],[0]
"We
4The hidden layer size of these networks are adjusted accordingly to reach the target model size.
follow the learning schedule of Zaremba et al. (2014) and (Jozefowicz et al., 2015).",5. Language modeling,[0],[0]
"We review additional hyperparameter ranges in section F of the supplementary material.
",5. Language modeling,[0],[0]
Table 1 summarizes our results.,5. Language modeling,[0],[0]
We find that single layer RNNs perform on par with their multi-layer counterparts.,5. Language modeling,[0],[0]
"Despite being a significantly simpler model, the minimalRNN performs comparably to GRUs.",5. Language modeling,[0],[0]
"Given the closed-form critical initialization developed here that significantly boosts convergence speed, the minimalRNN might be a favorable alternative to GRUs.",5. Language modeling,[0],[0]
There is a gap in perplexity between the performance of LSTMs and minimalRNNs.,5. Language modeling,[0],[0]
We hypothesize that this is due to the removal of an independent gate on the input.,5. Language modeling,[0],[0]
The same strategy is employed in GRUs and may cause a conflict between keeping longer-range memory and updating new information as was originally pointed out by Hochreiter & Schmidhuber (1997).,5. Language modeling,[0],[0]
We have developed a theory of signal propagation for random vanilla RNNs and a simple gated RNNs.,6. Discussion,[0],[0]
We demonstrate rigorously that the theory predicts trainability of these networks and gating mechanisms allow for a significantly larger trainable region.,6. Discussion,[0],[0]
We are planning to extend the theory to more complicated RNN cells as well as RNNs with multiple layers.,6. Discussion,[0],[0]
We thank Jascha Sohl-Dickstein and Greg Yang for helpful discussions and Ashish Bora for many contributions to early stages of this project.,Acknowledgements,[0],[0]
Figure Supp.1.,A. MinimalRNN Architecture,[0],[0]
Model architecture of minimalRNN.,A. MinimalRNN Architecture,[0],[0]
Here we analyze the mean field dynamics of the minimalRNN.,B. Diagonal Recurrence Relation,[0],[0]
The minimalRNN features a hidden state ht ∈ RN and inputs xt.,B. Diagonal Recurrence Relation,[0],[0]
The inputs are transformed via a fully-connected network zt = Φ(xt) ∈ RM before being fed into the network.,B. Diagonal Recurrence Relation,[0],[0]
"The RNN cell is then described by the equations,
vti;a = ∑ j Wijh t−1 j;a + ∑ j Vijz t j;a + bi (14)
uti;a",B. Diagonal Recurrence Relation,[0],[0]
"= σ(v t i;a) (15)
hti;a = u t i;ah t−1 i;a + (1− u t i;a)z t i;a.",B. Diagonal Recurrence Relation,[0],[0]
"(16)
Here i denotes the (pre)-activation and a denotes an input to the network.",B. Diagonal Recurrence Relation,[0],[0]
"Thus, uti acts as a gate on the t’th step.",B. Diagonal Recurrence Relation,[0],[0]
"We take Wij ∼ N (0, σ2w/N), Vij ∼ N (0, σ2v/M) and bi ∼ N (µb, σ2b ).
",B. Diagonal Recurrence Relation,[0],[0]
"By the CTL we can make a mean field assumption that vti;a ∼ N (µb, qtab)",B. Diagonal Recurrence Relation,[0],[0]
"where,
qtab",B. Diagonal Recurrence Relation,[0],[0]
= σ,B. Diagonal Recurrence Relation,[0],[0]
2 wE[h t−1 i;a h t−1 i;b ],B. Diagonal Recurrence Relation,[0],[0]
"+ σ 2 vE[zti;azti;b] + σ2b = σ2wQ t−1 ab + σ 2 vR t ab + σ 2 b (17)
where we have defined Qtab =",B. Diagonal Recurrence Relation,[0],[0]
[h t i;ah t i;b] and R t ab = E[zti;azti;b].,B. Diagonal Recurrence Relation,[0],[0]
"We note that Rtab is fixed by the input, but it remains for us to work out Qtab.",B. Diagonal Recurrence Relation,[0],[0]
"We find that,
Qtab = E[hti;ahti;b] = E[uti;ah t−1 i;a u t i;bh t−1 i;b ]",B. Diagonal Recurrence Relation,[0],[0]
"+ E[u t i;ah t−1 i;a (1− u t i;b)z t i;b] (18)
+ E[(1− uti;a)zti;auti;bht−1i;b ] +",B. Diagonal Recurrence Relation,[0],[0]
"E[(1− u t i;a)z t i;a(1− uti;b)zti;b] (19)
",B. Diagonal Recurrence Relation,[0],[0]
≈ E[uti;auti;b]E[ht−1i;a h t−1 i;b ],B. Diagonal Recurrence Relation,[0],[0]
"+ E[(1− u t i;a)(1− uti;b)]E[zti;azti;b] (20)
where we have assumed that the expectation factorizes so that ht−1i;a and u t i;a are approximately independent.
",B. Diagonal Recurrence Relation,[0],[0]
We choose to normalize the data so that Rtaa = R t bb = R independent of time.,B. Diagonal Recurrence Relation,[0],[0]
An immediate consequence of this normalization is that Qtaa = Q t bb = Q t and qtaa = q t bb = q,B. Diagonal Recurrence Relation,[0],[0]
t.,B. Diagonal Recurrence Relation,[0],[0]
"We then write Rtab = RΣ t, Qtab = Q tCt and qtab = q tct where Σt, Ct, and ct are cosine similarities between the inputs, the hidden states, and the vta,b respectively.",B. Diagonal Recurrence Relation,[0],[0]
"With this normalization, we can work out the mean-field recurrence relation characterizing the covariance matrix for the minimalRNN.
",B. Diagonal Recurrence Relation,[0],[0]
We begin by considering the diagonal recurrence relations.,B. Diagonal Recurrence Relation,[0],[0]
"We find that the dynamics are described by the equation,",B. Diagonal Recurrence Relation,[0],[0]
Qt = Qt−1 ∫ Dzσ2( √ qtz + µb),B. Diagonal Recurrence Relation,[0],[0]
"+R ∫ Dz [ 1− σ( √ qtz + µb) ]2 (21)
",B. Diagonal Recurrence Relation,[0],[0]
"qt = σ2wQ t−1 + σ2vR+ σ 2 b (22)
",B. Diagonal Recurrence Relation,[0],[0]
"As expected, the first and second integrands determine how much of the update of the random network is controlled by the norm of the hidden state and how much is determined by the norm of the input.",B. Diagonal Recurrence Relation,[0],[0]
Since σ(z) = 1− σ(−z),B. Diagonal Recurrence Relation,[0],[0]
"it follows that when µb = 0 the first and second term will be equal and so,
",B. Diagonal Recurrence Relation,[0],[0]
Qt = (Qt−1 +R) ∫,B. Diagonal Recurrence Relation,[0],[0]
Dzσ2( √ qtz).,B. Diagonal Recurrence Relation,[0],[0]
"(23)
In general, µb will therefore control the degree to which the hidden state of the random minimalRNN is updated based on the previous hidden state or based on the inputs with µb = 0 implying parity between the two.",B. Diagonal Recurrence Relation,[0],[0]
This is reflected in eq. (23).,B. Diagonal Recurrence Relation,[0],[0]
"In the event that the norm of the inputs is time-independent, Rt = R for all t, then the minimalRNN will have a fixed point provided there exists a Q∗ that satisfies a transcendental equation,",C. Existence of a Q∗ Fixed Point,[0],[0]
"namely that
F(Q∗) ≡ ∫ Dq∗z",C. Existence of a Q∗ Fixed Point,[0],[0]
[1− σ (z)]2∫ Dq∗z,C. Existence of a Q∗ Fixed Point,[0],[0]
[1− σ2 (z)],C. Existence of a Q∗ Fixed Point,[0],[0]
− Q ∗ R = 0 .,C. Existence of a Q∗ Fixed Point,[0],[0]
"(24)
It is easy to see that such a solution always exists.",C. Existence of a Q∗ Fixed Point,[0],[0]
WhenQ∗,C. Existence of a Q∗ Fixed Point,[0],[0]
→∞,C. Existence of a Q∗ Fixed Point,[0],[0]
the first term of F(Q∗) approaches 1 while the magnitude of the second increases without bound and so F(Q∗) < 0.,C. Existence of a Q∗ Fixed Point,[0],[0]
"Conversely, when Q∗ → 0 the first term is positive while Q∗/R→ 0 and so F(Q∗) > 0.",C. Existence of a Q∗ Fixed Point,[0],[0]
The existence of a Q∗ satisfying the transcendental equation then follows directly from the intermediate value theorem.,C. Existence of a Q∗ Fixed Point,[0],[0]
We can now investigate the dynamics of the norm of the hidden state in the vicinity of Q∗. To do this suppose that Qt = Q∗ + t with 1.,D. Q∗ Dynamics,[0],[0]
"Our goal is then to expand eq.(21) about Q∗. First, we note that,
σ( √ qtz + µb) = σ( √ q∗ + σ2w",D. Q∗ Dynamics,[0],[0]
"tz + µb) (25)
",D. Q∗ Dynamics,[0],[0]
"≈ σ (√ q∗z + µb + 1
2 √ q∗ σ2w tz
) (26)
≈ σ (√ q∗z + µb )",D. Q∗ Dynamics,[0],[0]
"+ 1
2 √ q∗ σ2w
tzσ′ (√ q∗z + µb )",D. Q∗ Dynamics,[0],[0]
+O(( t)2).,D. Q∗ Dynamics,[0],[0]
"(27)
Letting ζ(z) = √ q∗z + µb this implies that,
Qt = Qt−1 ∫ Dzσ2( √ q∗ + σ2w",D. Q∗ Dynamics,[0],[0]
t−1z + µb),D. Q∗ Dynamics,[0],[0]
+R ∫ Dz [ 1− σ( √ q∗ + σ2w,D. Q∗ Dynamics,[0],[0]
"t−1z + µb) ]2
(28) Q∗ + t = Q∗ ∫ Dzσ2 (ζ(z))",D. Q∗ Dynamics,[0],[0]
+,D. Q∗ Dynamics,[0],[0]
R ∫,D. Q∗ Dynamics,[0],[0]
"Dz [1− σ(ζ(z))]2 + t−1 [ ∫ Dzσ2 (ζ(z)) (29)
+ Q∗√ q∗ σ2w
∫ Dzzσ(ζ(z))σ′(ζ(z))−R √ q∗σ2w ∫ Dzz(1− σ(ζ(z)))σ′(ζ(z)) ]",D. Q∗ Dynamics,[0],[0]
"(30)
t = t−1 [ ∫ Dzσ2 (ζ(z))",D. Q∗ Dynamics,[0],[0]
+,D. Q∗ Dynamics,[0],[0]
"Q
∗ √ q∗ σ2w
∫ Dzzσ(ζ(z))σ′(ζ(z))−R √ q∗σ2w ∫ Dzz(1− σ(ζ(z)))σ′(ζ(z)) ]",D. Q∗ Dynamics,[0],[0]
"(31)
= t−1 ∫",D. Q∗ Dynamics,[0],[0]
Dz [ σ2(ζ(z)),D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ′(ζ(z))
] (32)
= t−1 ∫ Dz [ σ2(ζ(z))",D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ(ζ(z))(1− σ(ζ(z)))
] (33)
= t−1 ∫ Dz [ σ2(ζ(z))",D. Q∗ Dynamics,[0],[0]
"+
σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R} zσ(ζ(z))(1− σ(ζ(z)))
] (34)
",D. Q∗ Dynamics,[0],[0]
"It follows that qt → q∗ as, |qt − q∗| ∼ e−t/ξQ (35)
with
ξ−1Q =",D. Q∗ Dynamics,[0],[0]
− log (∫ Dz [ σ2(ζ(z)),D. Q∗ Dynamics,[0],[0]
+ σ2w√ q∗ {(Q∗ +R)σ(ζ(z))−R}σ′(ζ(z)),D. Q∗ Dynamics,[0],[0]
"]) (36)
as expected.",D. Q∗ Dynamics,[0],[0]
We now turn our attention to the off-diagonal term.,E. Off-Diagonal Recurrence Relation,[0],[0]
"From eq. (7) it follows that,
QtCt = Qt−1Ct−1 ∫ Dz1Dz2σ(ut1)σ(ut2) +RΣt ∫ Dz1Dz2(1− σ(ut1))(1− σ(ut2))",E. Off-Diagonal Recurrence Relation,[0],[0]
"(37)
qtct = σ2wQ t−1Ct−1 + σ2vRΣ t + σ2b (38)
where ut1 = √ qtz1 + µb and ut2 = √ qt ( ctz1 + √ 1− (ct)2z2 ) + µb.",E. Off-Diagonal Recurrence Relation,[0],[0]
"(39)
",E. Off-Diagonal Recurrence Relation,[0],[0]
"By expanding eq (37) as ct = c∗ + t we find t+1 = χc∗ t where,
χc∗ = ∫",E. Off-Diagonal Recurrence Relation,[0],[0]
Dz1Dz2σ(u1)σ(u2),E. Off-Diagonal Recurrence Relation,[0],[0]
+ q∗(c∗ + J−) ∫,E. Off-Diagonal Recurrence Relation,[0],[0]
Dz1Dz2σ′(u1)σ′(u2).,E. Off-Diagonal Recurrence Relation,[0],[0]
"(40)
We note that when c∗ = 1 it follows that χc∗ = χ1.",E. Off-Diagonal Recurrence Relation,[0],[0]
"We tune the learning hyper-parameters in the following ranges for all the models:
• learning rate: {0.1, 0.2, 0.3, 0.5, 1, 2} • max-epoch: {4, 7, 11} • decay: {0.5, 0.65, 0.8} • dropout: {0.0, 0.2, 0.3, 0.5}",F. Additional Hyperparameter Ranges,[0],[0]
Recurrent neural networks have gained widespread use in modeling sequence data across various domains.,abstractText,[0],[0]
"While many successful recurrent architectures employ a notion of gating, the exact mechanism that enables such remarkable performance is not well understood.",abstractText,[0],[0]
We develop a theory for signal propagation in recurrent networks after random initialization using a combination of mean field theory and random matrix theory.,abstractText,[0],[0]
"To simplify our discussion, we introduce a new RNN cell with a simple gating mechanism that we call the minimalRNN and compare it with vanilla RNNs.",abstractText,[0],[0]
Our theory allows us to define a maximum timescale over which RNNs can remember an input.,abstractText,[0],[0]
We show that this theory predicts trainability for both recurrent architectures.,abstractText,[0],[0]
"We show that gated recurrent networks feature a much broader, more robust, trainable region than vanilla RNNs, which corroborates recent experimental findings.",abstractText,[0],[0]
"Finally, we develop a closed-form critical initialization scheme that achieves dynamical isometry in both vanilla RNNs and minimalRNNs.",abstractText,[0],[0]
We show that this results in significantly improved training dynamics.,abstractText,[0],[0]
"Finally, we demonstrate that the minimalRNN achieves comparable performance to its more complex counterparts, such as LSTMs or GRUs, on a language modeling task.",abstractText,[0],[0]
Dynamical Isometry and a Mean Field Theory of RNNs: Gating Enables Signal Propagation in Recurrent Neural Networks,title,[0],[0]
"Dependency-based syntactic representations of sentences are central to many language processing tasks (Kübler et al., 2009).",1 Introduction,[0],[0]
"Dependency parse-trees encode not only the syntactic structure of a sentence but also many aspects of its semantics.
",1 Introduction,[0],[0]
"A recent trend in NLP is concerned with encoding sentences as vectors (“sentence embeddings”), which can then be used for further prediction tasks.",1 Introduction,[0],[0]
"Recurrent neural networks (RNNs) (Elman, 1990), and in particular methods based on the LSTM architecture (Hochreiter and Schmidhuber, 1997), work very well for modeling sequences, and constantly obtain state-of-the-art results on both languagemodeling and prediction tasks (see, e.g. (Mikolov et al., 2010)).
",1 Introduction,[0],[0]
"Several works attempt to extend recurrent neural networks to work on trees (see Section 8 for a brief overview), giving rise to the so-called recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2010).",1 Introduction,[0],[0]
"However, recursive neural networks
do not cope well with trees with arbitrary branching factors – most work require the encoded trees to be binary-branching, or have a fixed maximum arity.",1 Introduction,[0],[0]
"Other attempts allow arbitrary branching factors, at the expense of ignoring the order of the modifiers.
",1 Introduction,[0],[0]
"In contrast, we propose a tree-encoding that naturally supports trees with arbitrary branching factors, making it particularly appealing for dependency trees.",1 Introduction,[0],[0]
"Our tree encoder uses recurrent neural networks as a building block: we model the left and right sequences of modifiers using RNNs, which are composed in a recursive manner to form a tree (Section 3).",1 Introduction,[0],[0]
"We use our tree representation for encoding the partially-built parse trees in a greedy, bottom-up dependency parser which is based on the easy-first transition-system of Goldberg and Elhadad (2010).
",1 Introduction,[0],[0]
"Using the Hierarchical Tree LSTM representation, and without using any external embeddings, our parser achieves parsing accuracies of 92.6 UAS and 90.2 LAS on the PTB (Stanford dependencies) and 86.1 UAS and 84.4 LAS on the Chinese treebank, while relying on greedy decoding.
",1 Introduction,[0],[0]
"To the best of our knowledge, this is the first work to demonstrate competitive parsing accuracies for full-scale parsing while relying solely on recursive, compositional tree representations, and without using a reranking framework.",1 Introduction,[0],[0]
"We discuss related work in Section 8.
",1 Introduction,[0],[0]
"While the parsing experiments demonstrate the suitability of our representation for capturing the structural elements in the parse tree that are useful for predicting parsing decisions, we are interested in exploring the use of the RNN-based compositional vector representation of parse trees also for seman-
445
Transactions of the Association for Computational Linguistics, vol. 4, pp.",1 Introduction,[0],[0]
"445–461, 2016.",1 Introduction,[0],[0]
Action Editor: Noah Smith.,1 Introduction,[0],[0]
"Submission batch: 11/2015; Revision batch: 2/2016; Published 8/2016.
",1 Introduction,[0],[0]
c©2016 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
tic tasks such as sentiment analysis (Socher et al., 2013b; Tai et al., 2015), sentence similarity judgements (Marelli et al., 2014) and textual entailment (Bowman et al., 2015).",1 Introduction,[0],[0]
A dependency-based syntactic representation is centered around syntactic modification relations between head words and modifier words.,2.1 Dependency-based Representation,[0],[0]
"The result are trees in which each node is a word in the sentence, and every node except for one designated root node has a parent node.",2.1 Dependency-based Representation,[0],[0]
"A dependency tree over a sentence with n words w1, . . .",2.1 Dependency-based Representation,[0],[0]
", wn can be represented as a list of n pairs of the form (h,m), where 0 ≤ h ≤ n and 1 ≤ m ≤ n.",2.1 Dependency-based Representation,[0],[0]
"Each such pair represents an edge in the tree in which h is the index of a head word (including the special ROOT node 0), and m is the index of a modifier word.",2.1 Dependency-based Representation,[0],[0]
"In order for the dependency trees to be useful for actual downstream language processing tasks, each edge is labeled with a syntactic relation.",2.1 Dependency-based Representation,[0],[0]
"The tree representation then becomes a list of triplets (h,m, `), where 1 ≤ ` ≤ L is the index of a dependency relation out of a designated set of L syntactic relations.
",2.1 Dependency-based Representation,[0],[0]
"Dependency trees tend to be relatively shallow, with some nodes having many children.",2.1 Dependency-based Representation,[0],[0]
"Looking at trees in the PTB training set we find that 94% of the trees have a height of at most 10, and 49% of the trees a height of at most 6.",2.1 Dependency-based Representation,[0],[0]
"In terms of width, 93% of the trees have at least one node with an arity of 4 or more, and 56% of the trees have at least one node with an arity of 6 or more.",2.1 Dependency-based Representation,[0],[0]
"Recurrent neural networks (RNNs), first proposed by Elman (1990) are statistical learners for modeling sequential data.",2.2 Recurrent Networks and LSTMs,[0],[0]
"In this work, we use the RNN abstraction as a building block, and recursively combine several RNNs to obtain our tree representation.",2.2 Recurrent Networks and LSTMs,[0],[0]
We briefly describe the RNN abstraction below.,2.2 Recurrent Networks and LSTMs,[0],[0]
"For further detail on RNNs, the reader is referred to sources such as (Goldberg, 2015; Bengio and Courville, 2016; Cho, 2015).
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The RNN abstraction is a function RNN that takes in a sequence of inputs vectors x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn (xi ∈ Rdin), and produces a sequence of state vec-
tors (also called output vectors) y1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", yn (yi ∈ Rdout).",2.2 Recurrent Networks and LSTMs,[0],[0]
"Each yi is conditioned on all the inputs x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xi preceding it.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Ignoring the intermediate outputs y1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", yn−1, the RNN can be thought of as encoding the sequence x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn into a final state yn.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Our notation in this paper follows this view.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The RNN is defined recursively using two functions:1
RNN(s0, x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xn) = yn = O(sn) si = N(si−1, xi)
",2.2 Recurrent Networks and LSTMs,[0],[0]
"Here, a function N takes as input a vector xi and a state vector si−1 and returns as output a new state si.",2.2 Recurrent Networks and LSTMs,[0],[0]
"One can then extract an output vector yi from si using the function O (the function O is usually the identity function, or a function that returns a subset of the elements in si).
",2.2 Recurrent Networks and LSTMs,[0],[0]
"Taking an algorithmic perspective, one can view the RNN as a state object with three operations: s = RNN.initial() returns a new initial state, s.advance(x) takes an input vector and returns a new state, and s.output() returns the output vector for the current state.",2.2 Recurrent Networks and LSTMs,[0],[0]
"When clear from the context, we abbreviate and use the state’s name (s) instead of s.output() to refer to the output vector at the state.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"The functions N and O defining the RNN are parameterized by parameters θ (matrices and vectors), which are trained from data.",2.2 Recurrent Networks and LSTMs,[0],[0]
"Specifically, one is usually interested in using some of the outputs yi for making predictions.",2.2 Recurrent Networks and LSTMs,[0],[0]
The RNN is trained such that the encoding yi is good for the prediction task.,2.2 Recurrent Networks and LSTMs,[0],[0]
"That is, the RNN learns which aspects of the sequence x1, . . .",2.2 Recurrent Networks and LSTMs,[0],[0]
", xi are informative for the prediction.
",2.2 Recurrent Networks and LSTMs,[0],[0]
"We use subscripts (i.e. RNNL, RNNR) to indicate different RNNs, that is, RNNs that have different sets of parameters.
",2.2 Recurrent Networks and LSTMs,[0],[0]
Specific instantiations of N and O yield different recurrent network mechanisms.,2.2 Recurrent Networks and LSTMs,[0],[0]
"In this work we use the Long Short Term Memory (LSTM) variant (Hochreiter and Schmidhuber, 1997) which is shown to be a very capable sequence learner.",2.2 Recurrent Networks and LSTMs,[0],[0]
"However, our algorithm and encoding method do not rely on any specific property of the LSTM architecture, and the
1We follow the notation of Goldberg (2015), with the exception of taking the output of the RNN to be a single vector rather than a sequence, and renaming R to N .
",2.2 Recurrent Networks and LSTMs,[0],[0]
LSTM can be transparently switched for any other RNN variant.,2.2 Recurrent Networks and LSTMs,[0],[0]
We now describe our method for representing a tree as a d-dimensional vector.,3 Tree Representation,[0],[0]
We assume trees in which the children are ordered and there are kl ≥ 0 children before the parent node (left children) and kr ≥ 0 children after it (right children).,3 Tree Representation,[0],[0]
Such trees correspond well to dependency tree structures.,3 Tree Representation,[0],[0]
"We refer to the parent node as a head, and to its children as modifiers.",3 Tree Representation,[0],[0]
"For a node t, we refer to its left modifiers as t.l1, t.l2, . . .",3 Tree Representation,[0],[0]
", t.lkl and its right modifiers as t.r1, t.r2, . . .",3 Tree Representation,[0],[0]
", t.rkr The indices of the modifier are always from the parent outward, that is t.l1 is the left modifier closest to the head t:
t
t.r1 t.r2 t.r3",3 Tree Representation,[0],[0]
"t.r4t.l1t.l2t.l3
The gist of the idea is to treat the modifiers of a node as a sequence, and encode this sequence using an RNN.",3 Tree Representation,[0],[0]
"We separate left-modifiers from rightmodifiers, and use two RNNs: the first RNN encodes the sequence of left-modifiers from the head outwards, and the second RNN the sequence of right-modifiers from the head outwards.",3 Tree Representation,[0],[0]
"The first input to each RNN is the vector representation of the head word, and the last input is the vector representation of the left-most or the right-most modifier.",3 Tree Representation,[0],[0]
The node’s representation is then a concatenation of the RNN encoding of the left-modifiers with the RNN encoding of the right-modifiers.,3 Tree Representation,[0],[0]
"The encoding is recursive: the representation for each of the modifier nodes is computed in a similar fashion.
",3 Tree Representation,[0],[0]
"t
R
L
t.r1
R
t.r2
R
t.r3
R
t.r4
R
t t.l1
L
t.l2
L
t.l3
L enc(t)
",3 Tree Representation,[0],[0]
"RNNR
RNNL concatenate
and compress
More formally, consider a node t. Let i(t) be the sentence index of the word corresponding to the head node t, and let vi be a vector corresponding to the ith word in the sentence (this vector captures information such as the word form and its part of speech tag, and will be discussed shortly).",3 Tree Representation,[0],[0]
"The vec-
tor encoding of a node enc(t) ∈ Rdenc is then defined as follows:
enc(t) =g(W e · (el(t) ◦ er(t))",3 Tree Representation,[0],[0]
"+ be) el(t) =RNNL(vi(t), enc(t.l1), . . .",3 Tree Representation,[0],[0]
", enc(t.lkl))
er(t) =RNNR(vi(t), enc(t.r1), . . .",3 Tree Representation,[0],[0]
", enc(t.rkr))
",3 Tree Representation,[0],[0]
"First, the sequences consisting of the head-vector vi(t) followed by left-modifiers and the head-vector followed by right-modifiers are encoded using two RNNs, RNNL andRNNR, resulting in RNN states el(t) ∈ Rdout and er(t) ∈ Rdout .",3 Tree Representation,[0],[0]
"Then, the RNN states are concatenated, resulting in a 2doutdimensional vector (el(t) ◦ er(t)), which is reduced back to d-dimensions using a linear transformation followed by a non-linear activation function",3 Tree Representation,[0],[0]
"g. The recursion stops at leaf nodes, for which:
enc(leaf) =g(W e · (el(leaf) ◦ er(leaf))",3 Tree Representation,[0],[0]
"+ be) el(leaf) =RNNL(vi(leaf))
er(leaf) =RNNR(vi(leaf))
",3 Tree Representation,[0],[0]
Figure 1 shows the network used for encoding the sentence “the black fox who really likes apples did not jump over a lazy dog yesterday”.,3 Tree Representation,[0],[0]
In the discussion above we assume a vector representation vi associated with the ith sentence word.,3.1 Representing words,[0],[0]
What does vi look like?,3.1 Representing words,[0],[0]
"A sensible approach would be to take vi to be a function of the word-form and the part-of-speech (POS) tag of the ith word, that is:
vi = g(W v · (wi ◦ pi) + bv)
where wi and pi are the embedded vectors of the word-form and POS-tag of the ith word.
",3.1 Representing words,[0],[0]
"This encodes each word in isolation, disregarding its context.",3.1 Representing words,[0],[0]
The context of a word can be very informative regarding its meaning.,3.1 Representing words,[0],[0]
"One way of incorporating context is the Bidirectional RNN (Schuster and Paliwal, 1997).",3.1 Representing words,[0],[0]
"Bidirectional RNNs are shown to be an effective representation for sequence tagging (Irsoy and Cardie, 2014).",3.1 Representing words,[0],[0]
"Bidirectional RNNs represent a word in the sentence using a concatenation of the end-states of two RNNs, one running
yesterday”.",3.1 Representing words,[0],[0]
"Top: the network structure: boxed nodes represent LSTM cells, where L are cells belonging to the leftmodifiers sequence model RNNL, and R to the right-modifiers sequence model RNNR.",3.1 Representing words,[0],[0]
Circle nodes represent a concatenation followed by a linear transformation and a non-linearity.,3.1 Representing words,[0],[0]
"Bottom: the dependency parse of the sentence.
",3.1 Representing words,[0],[0]
from the beginning of the sentence to the word and the other running from the end to the word.,3.1 Representing words,[0],[0]
"The result is a vector representation for each word which captures not only the word but also its context.
",3.1 Representing words,[0],[0]
"We adopt the Bidirectional LSTM scheme to enrich our node vector representation, and for an nwords sentence compute the vector representations vi as follows:
v′i =g(W v · (wi ◦ pi) + bv) fi =LSTMF (v ′ 1, v ′ 2, . . .",3.1 Representing words,[0],[0]
", v ′",3.1 Representing words,[0],[0]
"i)
bi =LSTMB(v ′ n, v ′",3.1 Representing words,[0],[0]
"n−1, . .",3.1 Representing words,[0],[0]
.,3.1 Representing words,[0],[0]
", v′i) vi =(fi ◦ bi)
",3.1 Representing words,[0],[0]
"We plug this word representation as word vectors, allowing each word vector vi to capture information regarding the word form and POS-tag, as well as the sentential context it appears in.",3.1 Representing words,[0],[0]
"The BILSTM encoder is trained jointly with the rest of the network towards the parsing objective, using backpropagation.
",3.1 Representing words,[0],[0]
Embedding vectors,3.1 Representing words,[0],[0]
The word and POS embeddings wi and pi are also trained together with the network.,3.1 Representing words,[0],[0]
"For the word embeddings, we experiment with random initialization, as well as with initialization using pre-trained word embeddings.",3.1 Representing words,[0],[0]
"Our main goal in this work is not to provide top parsing accuracies, but rather to evaluate the ability of the proposed compositional architecture to learn and capture the structural cues that are needed for accurate parsing.",3.1 Representing words,[0],[0]
"Thus, we are most interested in the random initialization setup: what can the network learn from the training corpus alone, without relying on external resources.
",3.1 Representing words,[0],[0]
"However, the ability to perform semi-supervised learning by initializing the word-embeddings with vectors that are pre-trained on large amount of unannotated data is an appealing property of the neuralnetwork approaches, and we evaluate our parser also in this semi-supervised setup.",3.1 Representing words,[0],[0]
"When using pretrained word embeddings, we follow (Dyer et al., 2015) and use embedding vectors which are trained using positional context (Ling et al., 2015), as these
were shown to work better than traditional skipgram vectors for syntactic tasks such as part-ofspeech tagging and parsing.",3.1 Representing words,[0],[0]
"Why did we choose to encode the children from the head outward, and not the other way around?",3.2 A note on the head-outward generation,[0],[0]
"The head outward generation order is needed to facilitate incremental tree construction and allow for efficient parsing, as we show in section 4 below.",3.2 A note on the head-outward generation,[0],[0]
"Besides the efficiency considerations, using the headoutward encoding puts more emphasis on the outermost dependants, which are known to be the most informative for predicting parse structure.2 We rely on the RNN capability of extracting information from arbitrary positions in the sequence to incorporate information about the head word itself, which appears in the beginning of the sequence.",3.2 A note on the head-outward generation,[0],[0]
"This seems to work well, which is expected considering that the average maximal number of siblings in one direction in the PTB is 4.1, and LSTMs were demonstrated to capture much longer-range interactions.",3.2 A note on the head-outward generation,[0],[0]
"Still, when using the tree encoding in a situation where the tree is fully specified in advance, i.e. for sentence classification, sentence similarity or translation tasks, using a head-inward generation order (or even a bidirectional RNN) may prove to work better.",3.2 A note on the head-outward generation,[0],[0]
"We leave this line of inquiry to future work.
",3.2 A note on the head-outward generation,[0],[0]
"The head-outward modifier generation approach has a long history in the parsing literature, and goes back to at least Eisner (1996) and Collins (1997).",3.2 A note on the head-outward generation,[0],[0]
"In contrast to previous work in which each modifier could condition only on a fixed small number of modifiers preceding it, and in which the left- and right- sequences of modifiers were treated as independent from one another for computational efficiency reasons, our approach allows the model to access information from the entirety of both the left and the right sequences jointly.
",3.2 A note on the head-outward generation,[0],[0]
"2Features in transition-based dependency parsers often look at the current left-most and right-most dependents of a given node, and almost never look further than the second left-most or second right-most dependents.",3.2 A note on the head-outward generation,[0],[0]
"Second-order graph based dependency parsers (McDonald, 2006; Eisner, 2000) also condition on the current outermost dependent when generating its sibling.",3.2 A note on the head-outward generation,[0],[0]
We now turn to explain how to parse using the tree encoder defined above.,4 Parsing Algorithm,[0],[0]
"We begin by describing our bottom-up parsing algorithm, and then show how the encoded vector representation can be built and maintained throughout the parsing process.",4 Parsing Algorithm,[0],[0]
"We follow a (projective) bottom-up parsing strategy, similar to the easy-first parsing algorithm of Goldberg and Elhadad (2010).
",4.1 Bottom-up Parsing,[0],[0]
The main data-structure in the parser is a list of partially-built parse trees we call pending.,4.1 Bottom-up Parsing,[0],[0]
"For a sentence with words w1, . . .",4.1 Bottom-up Parsing,[0],[0]
", wn, the pending list is initialized with n nodes, where pending[i] corresponds to word wi.",4.1 Bottom-up Parsing,[0],[0]
"The algorithm then chooses two neighbouring trees in the pending list pending[i] and pending[i + 1] and either attaches the root of pending[i+1] as the right-most modifier of the root of pending[i], or attaches the root of pending[i] as the left-most modifier of the root of pending[i+ 1].",4.1 Bottom-up Parsing,[0],[0]
"The tree which was treated as modifier is then removed from the pending list, shortening it by one.",4.1 Bottom-up Parsing,[0],[0]
"The process ends after n−1 steps, when a single tree remains in the pending list, which is taken to be the output parse tree.",4.1 Bottom-up Parsing,[0],[0]
"The parsing process is described in Algorithm 1.
",4.1 Bottom-up Parsing,[0],[0]
"Algorithm 1 Parsing 1: Input: Sentence w = w1, . . .",4.1 Bottom-up Parsing,[0],[0]
", wn 2: for i ∈ 1, . . .",4.1 Bottom-up Parsing,[0],[0]
", n do 3: pend[i].id←",4.1 Bottom-up Parsing,[0],[0]
i 4: arcs←,4.1 Bottom-up Parsing,[0],[0]
"[] 5: while |pend| > 1 do 6: A← {(i, d) | 1 ≤",4.1 Bottom-up Parsing,[0],[0]
"i < |pend|, d ∈",4.1 Bottom-up Parsing,[0],[0]
"{l, r}} 7: i, d← select(A) 8: if d = l then 9: m,h← pend[i], pend[i+ 1] 10: pend.remove(i) 11: else 12: h,m← pend[i], pend[i+ 1] 13: pend.remove(i+ 1) 14: arcs.append(h.id,m.id) 15: return arcs
This parsing algorithm is both sound and complete with respect to the class of projective depen-
dency trees (Goldberg and Elhadad, 2010).",4.1 Bottom-up Parsing,[0],[0]
The algorithm depends on non-deterministic choices of an index in the pending list and an attachment direction (line 7).,4.1 Bottom-up Parsing,[0],[0]
"When parsing in practice, the nondeterministic choice will be replaced by using a trained classifier to assign a score to each indexdirection pair, and selecting the highest scoring pair.",4.1 Bottom-up Parsing,[0],[0]
"We discuss the scoring function in Section 4.4, and the training algorithm in Section 5.",4.1 Bottom-up Parsing,[0],[0]
We would like the scoring function to condition on the vector encodings of the subtrees it aims to connect.,4.2 Bottom-up Tree-Encoding,[0],[0]
"Algorithm 2 shows how to maintain the vector encodings together with the parsing algorithm, so that at every stage in the parsing process each item pending[i] is associated with a vector encoding of the corresponding tree.
",4.2 Bottom-up Tree-Encoding,[0],[0]
"Algorithm 2 Parsing while maintaining tree representations
1: Input: Sentence w = w1, . . .",4.2 Bottom-up Tree-Encoding,[0],[0]
", wn 2: Input: Vectors vi corresponding to words wi 3: arcs←",4.2 Bottom-up Tree-Encoding,[0],[0]
"[] 4: for i ∈ 1, . .",4.2 Bottom-up Tree-Encoding,[0],[0]
.,4.2 Bottom-up Tree-Encoding,[0],[0]
", n do 5: pend[i].id← i 6: pend[i].el ← RNNL.init().append(vi) 7: pend[i].er ← RNNR.init().append(vi) 8: while |pend|",4.2 Bottom-up Tree-Encoding,[0],[0]
"> 1 do 9: A← {(i, d)",4.2 Bottom-up Tree-Encoding,[0],[0]
"| 1 ≤ i < |pend|, d ∈ {l, r}}
10: i, d← select(A) 11: if d = l then 12: m,h← pend[i], pend[i+ 1] 13: m.c = m.el ◦m.er 14: m.enc = g(W (m.c) + b) 15: h.el.append(m.enc) 16: pend.remove(i) 17: else 18: h,m← pend[i], pend[i+ 1] 19: m.c = m.el ◦m.er 20: m.enc = g(W (m.c) + b) 21: h.er.append(m.enc) 22: pend.remove(i+ 1) 23: arcs.add(h.id,m.id) 24: return arcs",4.2 Bottom-up Tree-Encoding,[0],[0]
The tree representation described above does not account for the relation labels ` the parsing algorithm assigns each edge.,4.3 Labeled Tree Representation,[0],[0]
"In cases the tree is fully specified in advance, the relation of each word to its head can be added to the word representations vi.",4.3 Labeled Tree Representation,[0],[0]
"However, in the context of parsing, the labels become known only when the modifier is attached to its parent.",4.3 Labeled Tree Representation,[0],[0]
We thus extend the tree representation by concatenating the node vector representation with a vector representation assigned to the label connecting the subtree to its parent.,4.3 Labeled Tree Representation,[0],[0]
"Formally, only the final enc(t) equation changes:
enc(t) = g(W e · (el ◦ er ◦ `) + be) where ` is a learned embedding vector associated with the given label.",4.3 Labeled Tree Representation,[0],[0]
The parsing algorithm relies on a function select(A) for choosing the action to take at each stage.,4.4 Scoring Function,[0],[0]
"We model this function as:
select(A) = argmax(i,d,`)∈AScore(pend, i, d, `)
where Score(.) is a learned function whose job is to assign scores to possible actions to reflect their quality.",4.4 Scoring Function,[0],[0]
"Ideally, it will not only score correct actions above incorrect ones, but also more confident (easier) actions above less confident ones, in order to minimize error propagation in the greedy parsing process.
",4.4 Scoring Function,[0],[0]
"When scoring a possible attachment between a head h and a modifier m with relation `, the scoring function should attempt to reflect the following pieces of information: • Are the head words of h and m compatible un-
der relation",4.4 Scoring Function,[0],[0]
l?,4.4 Scoring Function,[0],[0]
"• Is the modifier m compatible with the already
existing modifiers of h?",4.4 Scoring Function,[0],[0]
"In other words, is m a good subtree to connect as an outer-most modifier in the subtree h?",4.4 Scoring Function,[0],[0]
"• Is m complete, in the sense that it already ac-
quired all of its own modifiers?",4.4 Scoring Function,[0],[0]
"to this end, the scoring function looks at a window of k subtrees to each side of the head-modifier pair (pend[i− k], . . .",4.4 Scoring Function,[0],[0]
", pend[i+1+ k]) where the neighbouring subtrees are used for providing hints regarding possible additional modifiers ofm",4.4 Scoring Function,[0],[0]
"and h that are
yet to be acquired.",4.4 Scoring Function,[0],[0]
"We use k = 2 in our experiments, for a total of 6 subtrees in total.",4.4 Scoring Function,[0],[0]
"This window approach is also used in the Easy-First parser of Goldberg and Elhadad (Goldberg and Elhadad, 2010) and works that extend it (Tratz and Hovy, 2011; Ma et al., 2012; Ma et al., 2013).",4.4 Scoring Function,[0],[0]
"However, unlike the previous work, which made use of extensive feature engineering and rich feature functions aiming at extracting the many relevant linguistic sub-structures from the 6 subtrees and their interactions, we provide the scoring function solely with the vector-encoding of the 6 subtrees in the window.
",4.4 Scoring Function,[0],[0]
Modeling the labeled attachment score is more difficult than modeling the unlabeled score and is prone to more errors.,4.4 Scoring Function,[0],[0]
"Moreover, picking the label for an attachment will cause less cascading error in contrast to picking the wrong attachment, which will necessarily preclude the parser from reaching the correct tree structure.",4.4 Scoring Function,[0],[0]
"In order to partially overcome this issue, our scoring function is a sum of two auxiliary scoring function, one scoring unlabeled and the other scoring labeled attachments.",4.4 Scoring Function,[0],[0]
"The unlabeled attachment score term in the sum functions as a fallback which makes it easier for a parser to predict the attachment direction even when there is no sufficient certainty as to the label.
",4.4 Scoring Function,[0],[0]
"Score(pend, i, d, `) = ScoreU (pend, i, d)
+ ScoreL(pend, i, d, `)
",4.4 Scoring Function,[0],[0]
"Each of ScoreU and ScoreL are modeled as multilayer perceptrons:
ScoreU (pend, i, d) =MLPU (xi)[d]
ScoreL(pend, i, d, `) =MLPL(xi)[(d, `)]",4.4 Scoring Function,[0],[0]
"xi = pend[i− 2].c ◦ · · · ◦ pend[i+ 3].c
where MLPU and MLPL are standard multilayer perceptron classifiers with one hidden layer (MLPX(x) =",4.4 Scoring Function,[0],[0]
W 2g(W,4.4 Scoring Function,[0],[0]
1x+,4.4 Scoring Function,[0],[0]
b1)+ b2),4.4 Scoring Function,[0],[0]
"and have output layers with size 2 and 2L respectively,",4.4 Scoring Function,[0],[0]
"[.] is an indexing operation, and we assume the values of d and (d, `) are mapped to integer values.",4.4 Scoring Function,[0],[0]
"The Easy-First parsing algorithm works in O(n log n) time (Goldberg and Elhadad, 2010).
",4.5 Computational Complexity,[0],[0]
"The parser in this works differ by three aspects: running a BI-LSTM encoder prior to parsing (O(n)); maintaining the tree representation during parsing (lines 11–22 in Algorithm 2) which take a constant time at each parsing step; and local scoring using an MLP rather than a linear classifier (again, a constant-time operation).",4.5 Computational Complexity,[0],[0]
"Thus, the parser maintains the O(n log n) complexity of the Easy-First parser.",4.5 Computational Complexity,[0],[0]
"At each step of the parsing process we select the highest scoring action (i, d, `).",5.1 Loss and Parameter Updates,[0],[0]
The goal of training is to set the Score function such that correct actions are scored above incorrect ones.,5.1 Loss and Parameter Updates,[0],[0]
"We use a marginbased objective, aiming to maximize the margin between the highest scoring correct action and the set of incorrect actions.",5.1 Loss and Parameter Updates,[0],[0]
"Formally, we define a hinge loss for each parsing step as follows:
max{0, 1−max(i,d,`)∈GScore(pend, i, d, `)",5.1 Loss and Parameter Updates,[0],[0]
"+max(i′,d′,`′)∈A\GScore(pend, i, d, `)}
where A is the set of all possible actions and G is the set of correct actions at the current stage.
",5.1 Loss and Parameter Updates,[0],[0]
"As the scoring function depends on vectorencodings of all trees in the window, and each treeencoding depends on the network’s parameters, each parameter update will invalidate all the vector encodings, requiring a re-computation of the entire network.",5.1 Loss and Parameter Updates,[0],[0]
"We thus sum the local losses throughout the parsing process, and update the parameter with respect to the sum of the losses at sentence boundaries.",5.1 Loss and Parameter Updates,[0],[0]
Since we are using hinge loss the gradients will become sparser as the training progresses.,5.1 Loss and Parameter Updates,[0],[0]
Fewer non-zero gradients could translate to unreliable updates.,5.1 Loss and Parameter Updates,[0],[0]
"In order to increase gradient stability and training speed, we use a variation of mini-batch in which we update the parameters only after 50 errors were made.",5.1 Loss and Parameter Updates,[0],[0]
This assures us a sufficient number of gradients for every update thus minimizing the effect of gradient instability.,5.1 Loss and Parameter Updates,[0],[0]
The gradients of the entire network with respect to the sum of the losses are calculated using the backpropagation algorithm.,5.1 Loss and Parameter Updates,[0],[0]
Initial experiments with an SGD optimizer showed very instable results.,5.1 Loss and Parameter Updates,[0],[0]
"We settled instead on using the ADAM optimizer (Kingma and Ba, 2015) which
worked well without requiring fiddling with learning rates.",5.1 Loss and Parameter Updates,[0],[0]
"At each stage in the training process, the parser assigns scores to all the possible actions (i, d, `)",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
∈ A.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"It then selects an action, applies it, and moves to the next step.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Which action should be chosen?,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"A sensible option is to defineG as the set of actions that can lead to the gold tree, and following the highest scoring actions in this set.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"However, using training in this manner tends to suffer from error propagation at test time.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The parser sees only states that result from following correct actions.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The lack of examples containing errors in the training phase makes it hard for the parser to infer the best action given partly erroneous trees.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"In order to cope with this, we follow the error exploration training strategy, in which we let the parser follow the highest scoring action in A during training even if this action is incorrect, exposing it to states that result from erroneous decisions.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
This strategy requires defining the set G such that the correct actions to take are well-defined also for states that cannot lead to the gold tree.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Such a set G is called a dynamic oracle.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Error-exploration and dynamic-oracles were introduced by Goldberg and Nivre (2012).
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"The Dynamic Oracle A dynamic-oracle for the easy-first parsing system we use is presented in (Goldberg and Nivre, 2013).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Briefly, the dynamicoracle version of G defines the set of gold actions as the set of actions which does not increase the number of erroneous attachments more than the minimum possible (given previous erroneous actions).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
The number of erroneous attachments is increased in three cases: (1) connecting a modifier to its head prematurely.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Once the modifier is attached it is removed from the pending list and therefore can no longer acquire any of its own modifiers; (2) connecting a modifier to an erroneous head, when the correct head is still on the pending list; (3) connecting a modifier to a correct head, but an incorrect label.
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Dealing with cases (2) and (3) is trivial.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"To deal with (1), we consider as correct only actions in which the modifier is complete.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"To efficiently identify complete modifiers we hold a counter for each word which is initialized to the number of modifiers
the word has in the gold tree.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
When applying an attachment the counter of the modifier’s gold head word is decreased.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"When the counter reaches 0, the sub-tree rooted at that word has no pending modifiers, and is considered complete.
",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Aggressive Exploration We found that even when using error-exploration, after one iteration the model remembers the training set quite well, and does not make enough errors to make error-exploration effective.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"In order to expose the parser to more errors, we employ a cost augmentation scheme: we sometimes follow incorrect actions also if they score below correct actions.",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Specifically, when the score of the correct action is greater than that of the wrong action but the difference is smaller than the margin constant, we chose to follow the wrong action with probability paug (we use paug = 0.1 in our experiments).",5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
Pseudocode for the entire training algorithm is given in the supplementary material.,5.2 Error-Exploration and Dynamic Oracle Training,[0],[0]
"Due to the sparsity of natural language, we are likely to encounter at test time a substantial number of the words that did not appear in the training data (OOV words).",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
OOV words are likely even when pre-training the word representations on a large unannotated corpora.,5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"A common approach is to designate a special “unknown-word” symbol, whose associated vector will be used as the word representation whenever an OOV word is encountered at test time.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"In order to train the unknown-word vector, a possible approach is to replace all the words appearing in the training corpus less than a certain number of times with the unknown-word symbol.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"This approach gives a good vector representation for unknown words but at the expense of ignoring many of the words from the training corpus.
",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"We instead propose a variant of the word-dropout approach (Iyyer et al., 2015).",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"During training, we replace a word with the unknown-word symbol with probability that is inversely proportional to frequency of the word.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
"Formally, we replace a word w appearing #(w) times in the training corpus with the unknown symbol with a probability:
punk(w) = α
#(w) + α
Using this approach we learn a vector representation for unknown words with minimal impact on the training of sparse words.",5.3 Out-of-vocabulary items and word-dropout,[0],[0]
Our Python implementation will be made available at the first author’s website.,6 Implementation Details,[0],[0]
"We use the PyCNN wrapper of the CNN library3 for building the computation graph of the network, computing the gradients using automatic differentiation, and performing parameter updates.",6 Implementation Details,[0],[0]
"We noticed the error on the development set does not improve after 20 iterations over the training set, therefore, we ran the training for 20 iterations.",6 Implementation Details,[0],[0]
The sentences where shuffled between iterations.,6 Implementation Details,[0],[0]
Non-projective sentences were skipped during training.,6 Implementation Details,[0],[0]
"We use the default parameters initialization, step sizes and regularization values provided by the PyCNN toolkit.",6 Implementation Details,[0],[0]
"The hyperparameters of the final networks used for all the reported experiments are detailed in Table 1.
",6 Implementation Details,[0],[0]
Weiss et al (2015) stress the importance of careful hyperparameter tuning for achieving top accuracy in neural network based parser.,6 Implementation Details,[0],[0]
"We did not follow this advice and made very few attempts at hyper-parameter tuning, using manual hill climbing until something seemed to work with reasonable accuracy, and then sticking with it for the rest of the experiments.
",6 Implementation Details,[0],[0]
3https://github.com/clab/cnn/tree/ master/pycnn,6 Implementation Details,[0],[0]
We evaluated our parsing model to English and Chinese data.,7 Experiments and Results,[0],[0]
"For comparison purposes we followed the setup of (Dyer et al., 2015).
",7 Experiments and Results,[0],[0]
"Data For English, we used the Stanford Dependency (SD) (de Marneffe and Manning, 2008) conversion of the Penn Treebank (Marcus et al., 1993), using the standard train/dev/test splitswith the same predicted POS-tags as used in (Dyer et al., 2015; Chen and Manning, 2014).",7 Experiments and Results,[0],[0]
This dataset contains a few non-projective trees.,7 Experiments and Results,[0],[0]
"Punctuation symbols are excluded from the evaluation.
",7 Experiments and Results,[0],[0]
"For Chinese, we use the Penn Chinese Treebank 5.1 (CTB5), using the train/test/dev splits of (Zhang and Clark, 2008; Dyer et al., 2015) with gold partof-speech tags, also following (Dyer et al., 2015; Chen and Manning, 2014).
",7 Experiments and Results,[0],[0]
"When using external word embeddings, we also use the same data as (Dyer et al., 2015).4
Experimental configurations We evaluated the parser in several configurations BOTTOMUPPARSER is the baseline parser, not using the tree-encoding, and instead representing each item in pending solely by the vector-representation (word and POS) of its head word.",7 Experiments and Results,[0],[0]
BOTTOMUPPARSER+HTLSTM is using our Hierarchical Tree LSTM representation.,7 Experiments and Results,[0],[0]
BOTTOMUPPARSER+HTLSTM+BI-LSTM is the Hierarchical Tree LSTM where we additionally use a BI-LSTM encoding for the head words.,7 Experiments and Results,[0],[0]
"Finally, we added external, pre-trained word embeddings to the BOTTOMUPPARSER+HTLSTM+BI-LSTM setup.",7 Experiments and Results,[0],[0]
"We also evaluated the final parsers in a –POS setup, in which we did not feed the parser with any POS-tags.
",7 Experiments and Results,[0],[0]
Results Results for English and Chinese are presented in Tables 2 and 3 respectively.,7 Experiments and Results,[0],[0]
"For comparison, we also show the results of the Stack-LSTM transition-based parser model of Dyer et al (2015), which we consider to be a state-of-the-art greedy model which is also very competitive with searchbased models, with and without pre-trained embeddings, and with and without POS-tags.
",7 Experiments and Results,[0],[0]
"4We thank Dyer et al for sharing their data with us.
",7 Experiments and Results,[0],[0]
The trends are consistent across the two languages.,7 Experiments and Results,[0],[0]
The baseline Bottom-Up parser performs very poorly.,7 Experiments and Results,[0],[0]
"This is expected, as only the headword of each subtree is used for prediction.",7 Experiments and Results,[0],[0]
"When adding the tree-encoding, results jump to near stateof-the-art accuracy, suggesting that the composed vector representation is indeed successful in capturing predictive structural information.",7 Experiments and Results,[0],[0]
"Replacing the head-words with their BI-LSTM encodings results in another increase in accuracy for English, outperforming the Dyer et al (S-LSTM no external) models on the test-set.",7 Experiments and Results,[0],[0]
"Adding the external pre-trained embeddings further improves the results for both our parser and Dyer et al’s model, closing the gap between them.",7 Experiments and Results,[0],[0]
"When POS-tags are not provided as input, the numbers for both parsers drop.",7 Experiments and Results,[0],[0]
"The drop is small for English and large for Chinese, and our parser seem to suffer a little less than the Dyer et al model.
",7 Experiments and Results,[0],[0]
"Importance of the dynamic oracle We also evaluate the importance of using the dynamic oracle and error-exploration training, and find that they are indeed important for achieving high parsing accura-
cies with our model (Table 4).
",7 Experiments and Results,[0],[0]
"When training without error-exploration (that is, the parser follows only correct actions during training and not using the dynamic aspect of the oracle), accuracies of unseen sentences drop by between 0.4 and 0.8 accuracy points (average 0.58).",7 Experiments and Results,[0],[0]
"This is consistent with previous work on training with error-exploration and dynamic oracles (Goldberg and Nivre, 2013), showing that the technique is not restricted to models trained with sparse linear models.
",7 Experiments and Results,[0],[0]
"Comparison to other state-of-the-art parsers Our main point of comparison is the model of Dyer et al, which was chosen because it is (a) a very strong parsing model; and (b) is the closest to ours in the literature: a greedy parsing model making heavy use of LSTMs.",7 Experiments and Results,[0],[0]
"To this end, we tried to make the comparison to Dyer et al as controlled as possible, using the same dependency annotation schemes, as well as the same predicted POS-tags and the pre-trained embeddings (when applicable).
",7 Experiments and Results,[0],[0]
"It is also informative to position our results with respect to other state-of-the-art parsing results reported in the literature, as we do in Table 5.",7 Experiments and Results,[0],[0]
"Here, some of the comparisons are less direct: some of the results use different dependency annotation schemes5, as well as different predicted POS-tags, and different pre-trained word embeddings.",7 Experiments and Results,[0],[0]
"While the numbers are not directly comparable, they do give a good reference as to the expected range of
5Our English parsing experiments use the Stanford Dependencies scheme, while other work use less informative dependency relations which are based on the Penn2Malt converter, using the Yamada and Matsumoto head rules.",7 Experiments and Results,[0],[0]
"From our experience, this conversion is somewhat easier to parse, resulting in numbers which are about 0.3-0.4 points higher than Stanford Dependencies.
state-of-the-art parsing results.",7 Experiments and Results,[0],[0]
Our system’s English parsing results are in range of state-of-the-art and the Chinese parsing results surpass it.,7 Experiments and Results,[0],[0]
"These numbers are achieved while using a greedy, bottom up parsing method without any search, and while relying solely on the compositional tree representations.",7 Experiments and Results,[0],[0]
"We survey two lines of related work: methods for encoding trees as vectors, and methods for parsing with vector representations.
",8 Related Work,[0],[0]
"The popular approach for encoding trees as vectors is using recursive neural networks (Goller and Kuchler, 1996; Socher et al., 2010; Tai et al., 2015).",8 Related Work,[0],[0]
Recursive neural networks represent the vector of a parent node in a tree as a function of its children nodes.,8 Related Work,[0],[0]
"However, the functions are usually restricted to having a fixed maximum arity (usually two) (Socher et al., 2010; Tai et al., 2015; Socher, 2014).",8 Related Work,[0],[0]
"While trees can be binarized to cope with the arity restriction, doing so results in deep trees which in turn leads to the vanishing gradient problem when training.",8 Related Work,[0],[0]
"To cope with the vanishing gradients, (Tai et al., 2015) enrich the composition function with a gating mechanism similar to that of the LSTM, resulting in the so-called Tree-LSTM model.",8 Related Work,[0],[0]
"Another approach is to allow arbitrary arities but ignoring the sequential nature of the modifiers, e.g. by using a bag-of-modifiers representation or a convolutional layer (Tai et al., 2015; Zhu et al., 2015).",8 Related Work,[0],[0]
"In contrast, our tree encoding method naturally allows for arbitrary branching trees by relying on the well established LSTM sequence model, and using it as a black box.",8 Related Work,[0],[0]
"Very recently, Zhang et al. (2015) proposed an RNN-based tree encoding which is similar to ours in encoding the sequence of modifiers as an RNN.",8 Related Work,[0],[0]
"Unlike our bottom-up encoder, their method works top-down, and is therefore not readily applicable for parsing.",8 Related Work,[0],[0]
On the other hand the top-down approach is well suited for generation.,8 Related Work,[0],[0]
"In future work, it could be interesting to combine the bottomup and top-down approaches in an encoder-decoder framework (Sutskever et al., 2014; Kiros et al., 2015).",8 Related Work,[0],[0]
"Work by Dyer et al (2016), that was submitted in parallel to ours, introduces a similar LSTMbased representation of syntactic constituents in the context of phrase-grammar parsing.
",8 Related Work,[0],[0]
"In terms of parsing with vector representations, there are four dominant approaches: search based parsers that use local features that are fed to a neural-network classifier (Pei et al., 2015; Durrett and Klein, 2015); greedy transition based parsers that use local features that are fed into a neuralnetwork classifier (Chen and Manning, 2014; Weiss et al., 2015), sometimes coupled with a node composition function (Dyer et al., 2015; Watanabe and Sumita, 2015); bottom up parsers that rely solely on recursively combined vector encodings of subtrees (Socher et al., 2010; Stenetorp, 2013; Socher et al., 2013a); and parse-reranking approaches that first produce a k-best list of parses using a traditional parsing technique, and then score the trees based on a recursive vector encoding of each node (Le and Zuidema, 2014; Le and Zuidema, 2015; Zhu et al., 2015).
",8 Related Work,[0],[0]
"Our parser is a greedy, bottom up parser that relies on compositional vector encodings of subtrees as its sole set of features.",8 Related Work,[0],[0]
"Unlike the re-ranking approaches, we do not rely on an external parser to provide k-best lists.",8 Related Work,[0],[0]
"Unlike the bottom-up parser in (Socher et al., 2010) that only parses sentences of up to 15 words and the parser of (Stenetorp, 2013) that achieves very low parsing accuracies, we parse arbitrary sentences with near state-of-the-art accuracy.",8 Related Work,[0],[0]
"Unlike the bottom up parser in (Socher et al., 2013a)",8 Related Work,[0],[0]
we do not make use of a grammar.,8 Related Work,[0],[0]
"The parser of (Weiss et al., 2015) obtains exceptionally high results using local features and no composition function.",8 Related Work,[0],[0]
The greedy version of their parser uses extensive tuning of hyper-parameters and network depth in order to squeeze every possible bit of accuracy.,8 Related Work,[0],[0]
Adding beam search on top of that further improves results.,8 Related Work,[0],[0]
"Due to our much more limited resources, we did not perform a methodological search over hyper-parameters, and explored only a tiny space of the possible hyper-parameters, and our parser does not perform search.",8 Related Work,[0],[0]
"Finally, perhaps closest to our approach is the greedy, transition-based parser of (Dyer et al., 2015) that also works in a bottomup fashion, and incorporates an LSTM encoding of the input tokens and hierarchical vector composition into its scoring mechanism.",8 Related Work,[0],[0]
"Indeed, that parser obtains similar scores to ours, although we obtain somewhat better results when not using pre-trained embeddings.",8 Related Work,[0],[0]
"We differ from the parser of Dyer et
al by having a more elaborate vector-composition function, relying solely on the compositional representations, and performing fully bottom-up parsing without being guided by a stack-and-buffer control structure.",8 Related Work,[0],[0]
"We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders, and demonstrate its effectiveness by integrating it in a bottom-up easy-first parser.",9 Conclusions and Future Work,[0],[0]
"Future extensions in terms of parsing include the addition of beam search, handling of unknown-words using character-embeddings, and adapting the algorithm to constituency trees.",9 Conclusions and Future Work,[0],[0]
"We also plan to establish the effectiveness of our Hierarchical Tree-LSTM encoder by applying it to more semantic vector representation tasks, i.e. training tree representation for capturing sentiment (Socher et al., 2013b; Tai et al., 2015), semantic sentence similarity (Marelli et al., 2014) or textual inference (Bowman et al., 2015).
",9 Conclusions and Future Work,[0],[0]
Acknowledgements This research is supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI) and the Israeli Science Foundation (grant number 1555/15).,9 Conclusions and Future Work,[0],[0]
"Algorithm 3 Training on annotated corpus 1: Input: Sentences w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wm 2: Input: Tree annotations T 1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", Tm 3: Input: Number of epochs to train
4: V ← InitializeV ectors() 5:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← []
6: for epoch ∈ {1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", Epochs} do 7: for S, T ∈ {(w1, T 1), . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", (wm, Tm)} do 8:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← TrainSentence (S, V [w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wn], T, Loss)
9: if |Loss| > 50 then 10: SumLoss← sum(Loss) 11: Call ADAM to minimize SumLoss 12:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Loss← []
(See Algorithm 4, training of a single sentence, on next page.)
",Appendix: Training Algorithm Pseudocode,[0],[0]
"Algorithm 4 Training on a single sentence with dynamic oracle algorithm 1: function TRAINSENTENCE(w, v, T, Loss) 2:",Appendix: Training Algorithm Pseudocode,[0],[0]
"Input: Sentence w = w1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", wn 3: Input: Vectors vi corresponding to inputs wi 4: Input: Annotated tree T in the form of (h,m, rel) triplets 5: Input: List Loss to which loss expressions are added
6: for i ∈ 1, . . .",Appendix: Training Algorithm Pseudocode,[0],[0]
", n do 7: unassigned[i]← |Children(wi)| 8: pend[i].id← i 9: pend[i].el ← RNNL.init().append(vi)
10: pend[i].er ← RNNR.init().append(vi)
11: while |pend| > 1 do 12: G,W ← {} , {}
13: for (i, d, rel) ∈ {1 ≤",Appendix: Training Algorithm Pseudocode,[0],[0]
"i < |pend|, d ∈ {l, r},",Appendix: Training Algorithm Pseudocode,[0],[0]
rel ∈ Relations} do 14: if d =,Appendix: Training Algorithm Pseudocode,[0],[0]
"l then m,h← pend[i], pend[i+ 1] 15: else m,h← pend[i+ 1], pend[i]
16:",Appendix: Training Algorithm Pseudocode,[0],[0]
"if unassigned[m.id] 6= 0 ∨ ∃`6=rel(h,m, `) ∈ T then 17: W.append((h,m, rel)) 18: else G.append((h,m, rel))
19: hG,mG, relG ← argmax(i,d,`)∈GScore(pend, i, d, `) 20: hW ,mW , relW ← argmax(i,d,`)∈WScore(pend, i, d, `) 21: scoreG ← Score(hG,mG, relG) 22: scoreW ← Score(hW ,mW , relW )
23: if scoreG − scoreW < 0 then 24: h,m, rel, score← hW ,mW , relW , scoreW 25: else if scoreG − scoreW > 1 ∨ random() < paug then 26: h,m, rel, score← hG,mG, relG, scoreG 27: else 28: h,m, rel, score← hW ,mW , relW , scoreW 29: if scoreG − score < 1 then 30: Loss.append(1− scoreG + score)
31: m.c = m.el ◦m.er 32: m.enc = g(W (m.c ◦ rel) + b) 33: if h.id < m.id then h.el.append(m.enc) 34: else h.er.append(m.enc)
35: unassigned[TParent(m).id]← unassigned[TParent(m).id]− 1 36: pend.remove(m) 37: return Loss",Appendix: Training Algorithm Pseudocode,[0],[0]
We suggest a compositional vector representation of parse trees that relies on a recursive combination of recurrent-neural network encoders.,abstractText,[0],[0]
"To demonstrate its effectiveness, we use the representation as the backbone of a greedy, bottom-up dependency parser, achieving very strong accuracies for English and Chinese, without relying on external word embeddings.",abstractText,[0],[0]
The parser’s implementation is available for download at the first author’s webpage.,abstractText,[0],[0]
Easy-First Dependency Parsing with Hierarchical Tree LSTMs,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1412–1421, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Neural Machine Translation (NMT) achieved state-of-the-art performances in large-scale translation tasks such as from English to French (Luong et al., 2015) and English to German (Jean et al., 2015).",1 Introduction,[0],[0]
NMT is appealing since it requires minimal domain knowledge and is conceptually simple.,1 Introduction,[0],[0]
The model by Luong et al. (2015) reads through all the source words until the end-of-sentence symbol <eos> is reached.,1 Introduction,[0],[0]
"It then starts emitting one target word at a time, as illustrated in Figure 1.",1 Introduction,[0],[0]
"NMT
1All our code and models are publicly available at http: //nlp.stanford.edu/projects/nmt.
is often a large neural network that is trained in an end-to-end fashion and has the ability to generalize well to very long word sequences.",1 Introduction,[0],[0]
"This means the model does not have to explicitly store gigantic phrase tables and language models as in the case of standard MT; hence, NMT has a small memory footprint.",1 Introduction,[0],[0]
"Lastly, implementing NMT decoders is easy unlike the highly intricate decoders in standard MT (Koehn et al., 2003).
",1 Introduction,[0],[0]
"In parallel, the concept of “attention” has gained popularity recently in training neural networks, allowing models to learn alignments between different modalities, e.g., between image objects and agent actions in the dynamic control problem (Mnih et al., 2014), between speech frames and text in the speech recognition task (Chorowski et al., 2014), or between visual features of a picture and its text description in the image caption generation task (Xu et al., 2015).",1 Introduction,[0],[0]
"In the context of NMT, Bahdanau et al. (2015) has successfully applied such attentional mechanism to jointly translate and align words.",1 Introduction,[0],[0]
"To the best of our knowledge, there has not been any other work exploring the use of attention-based architectures for NMT.
",1 Introduction,[0],[0]
"In this work, we design, with simplicity and effectiveness in mind, two novel types of attention-
1412
based models: a global approach in which all source words are attended and a local one whereby only a subset of source words are considered at a time.",1 Introduction,[0],[0]
"The former approach resembles the model of (Bahdanau et al., 2015) but is simpler architecturally.",1 Introduction,[0],[0]
"The latter can be viewed as an interesting blend between the hard and soft attention models proposed in (Xu et al., 2015): it is computationally less expensive than the global model or the soft attention; at the same time, unlike the hard attention, the local attention is differentiable, making it easier to implement and train.2 Besides, we also examine various alignment functions for our attention-based models.
",1 Introduction,[0],[0]
"Experimentally, we demonstrate that both of our approaches are effective in the WMT translation tasks between English and German in both directions.",1 Introduction,[0],[0]
Our attentional models yield a boost of up to 5.0 BLEU over non-attentional systems which already incorporate known techniques such as dropout.,1 Introduction,[0],[0]
"For English to German translation, we achieve new state-of-the-art (SOTA) results for both WMT’14 and WMT’15, outperforming previous SOTA systems, backed by NMT models and n-gram LM rerankers, by more than 1.0 BLEU.",1 Introduction,[0],[0]
"We conduct extensive analysis to evaluate our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, alignment quality, and translation outputs.",1 Introduction,[0],[0]
"A neural machine translation system is a neural network that directly models the conditional probability p(y|x) of translating a source sentence, x1, . . .",2 Neural Machine Translation,[0],[0]
", xn, to a target sentence, y1, . . .",2 Neural Machine Translation,[0],[0]
", ym.3",2 Neural Machine Translation,[0],[0]
"A basic form of NMT consists of two components: (a) an encoder which computes a representation s for each source sentence and (b) a decoder which generates one target word at a time and hence decomposes the conditional probability as:
log p(y|x)",2 Neural Machine Translation,[0],[0]
"= ∑m
j=1 log p (yj|y<j , s) (1)
",2 Neural Machine Translation,[0],[0]
"A natural choice to model such a decomposition in the decoder is to use a recurrent neural network (RNN) architecture, which most of the re-
2There is a recent work by Gregor et al. (2015), which is very similar to our local attention and applied to the image generation task.",2 Neural Machine Translation,[0],[0]
"However, as we detail later, our model is much simpler and can achieve good performance for NMT.
",2 Neural Machine Translation,[0],[0]
"3All sentences are assumed to terminate with a special “end-of-sentence” token <eos>.
cent NMT work such as (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2015; Luong et al., 2015; Jean et al., 2015) have in common.",2 Neural Machine Translation,[0],[0]
"They, however, differ in terms of which RNN architectures are used for the decoder and how the encoder computes the source sentence representation s.
Kalchbrenner and Blunsom (2013) used an RNN with the standard hidden unit for the decoder and a convolutional neural network for encoding the source sentence representation.",2 Neural Machine Translation,[0],[0]
"On the other hand, both Sutskever et al. (2014) and Luong et al. (2015) stacked multiple layers of an RNN with a Long Short-Term Memory (LSTM) hidden unit for both the encoder and the decoder.",2 Neural Machine Translation,[0],[0]
"Cho et al. (2014), Bahdanau et al. (2015), and Jean et al. (2015) all adopted a different version of the RNN with an LSTM-inspired hidden unit, the gated recurrent unit (GRU), for both components.4
In more detail, one can parameterize the probability of decoding each word yj as: p (yj|y<j, s) = softmax (g (hj))",2 Neural Machine Translation,[0],[0]
"(2) with g being the transformation function that outputs a vocabulary-sized vector.5 Here, hj is the RNN hidden unit, abstractly computed as:
hj = f(hj−1, s), (3)
where f computes the current hidden state given the previous hidden state and can be either a vanilla RNN unit, a GRU, or an LSTM unit.",2 Neural Machine Translation,[0],[0]
"In (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014; Luong et al., 2015), the source representation s is only used once to initialize the decoder hidden state.",2 Neural Machine Translation,[0],[0]
"On the other hand, in (Bahdanau et al., 2015; Jean et al., 2015) and this work, s, in fact, implies a set of source hidden states which are consulted throughout the entire course of the translation process.",2 Neural Machine Translation,[0],[0]
"Such an approach is referred to as an attention mechanism, which we will discuss next.
",2 Neural Machine Translation,[0],[0]
"In this work, following (Sutskever et al., 2014; Luong et al., 2015), we use the stacking LSTM architecture for our NMT systems, as illustrated in Figure 1.",2 Neural Machine Translation,[0],[0]
"We use the LSTM unit defined in (Zaremba et al., 2015).",2 Neural Machine Translation,[0],[0]
"Our training objective is formulated as follows:
",2 Neural Machine Translation,[0],[0]
"Jt = ∑
(x,y)∈D",2 Neural Machine Translation,[0],[0]
− log p(y|x) (4) 4They all used a single RNN layer except for the latter two works which utilized a bidirectional RNN for the encoder.,2 Neural Machine Translation,[0],[0]
"5One can provide g with other inputs such as the currently predicted word yj as in (Bahdanau et al., 2015).
",2 Neural Machine Translation,[0],[0]
with D being our parallel training corpus.,2 Neural Machine Translation,[0],[0]
"Our various attention-based models are classifed into two broad categories, global and local.",3 Attention-based Models,[0],[0]
These classes differ in terms of whether the “attention” is placed on all source positions or on only a few source positions.,3 Attention-based Models,[0],[0]
"We illustrate these two model types in Figure 2 and 3 respectively.
",3 Attention-based Models,[0],[0]
"Common to these two types of models is the fact that at each time step t in the decoding phase, both approaches first take as input the hidden state ht at the top layer of a stacking LSTM.",3 Attention-based Models,[0],[0]
The goal is then to derive a context vector ct that captures relevant source-side information to help predict the current target word yt.,3 Attention-based Models,[0],[0]
"While these models differ in how the context vector ct is derived, they share the same subsequent steps.
",3 Attention-based Models,[0],[0]
"Specifically, given the target hidden state ht and the source-side context vector ct, we employ a simple concatenation layer to combine the information from both vectors to produce an attentional hidden state as follows:
h̃t = tanh(Wc[ct;ht]) (5)
The attentional vector h̃t is then fed through the softmax layer to produce the predictive distribution formulated as:
p(yt|y<t, x) = softmax(Wsh̃t) (6) We now detail how each model type computes
the source-side context vector ct.",3 Attention-based Models,[0],[0]
The idea of a global attentional model is to consider all the hidden states of the encoder when deriving the context vector ct.,3.1 Global Attention,[0],[0]
"In this model type, a variable-length alignment vector at, whose size equals the number of time steps on the source side, is derived by comparing the current target hidden state ht with each source hidden state h̄s:
at(s) = align(ht, h̄s) (7)
= exp
( score(ht, h̄s) )∑ s′ exp ( score(ht, h̄s′)
)",3.1 Global Attention,[0],[0]
"Here, score is referred as a content-based function for which we consider three different alternatives:
score(ht, h̄s) =  h⊤t h̄s dot h⊤t Wah̄s general Wa[ht; h̄s] concat (8)
Besides, in our early attempts to build attentionbased models, we use a location-based function in which the alignment scores are computed from solely the target hidden state ht as follows:
at = softmax(Waht) location (9)
",3.1 Global Attention,[0],[0]
"Given the alignment vector as weights, the context vector ct is computed as the weighted average over all the source hidden states.6
Comparison to (Bahdanau et al., 2015) – While our global attention approach is similar in spirit to the model proposed by Bahdanau et al. (2015), there are several key differences which reflect how we have both simplified and generalized from the original model.",3.1 Global Attention,[0],[0]
"First, we simply use hidden states at the top LSTM layers in both the encoder and decoder as illustrated in Figure 2.",3.1 Global Attention,[0],[0]
"Bahdanau et al. (2015), on the other hand, use the concatenation of the forward and backward source hidden states in the bi-directional encoder and target hidden states in their non-stacking uni-directional decoder.",3.1 Global Attention,[0],[0]
"Second, our computation path is simpler; we go from ht → at → ct → h̃t then make a prediction as detailed in Eq.",3.1 Global Attention,[0],[0]
"(5), Eq. (6), and Figure 2.",3.1 Global Attention,[0],[0]
"On the other hand, at any time t, Bahdanau et al. (2015) build from the previous hidden state ht−1 → at → ct → ht, which, in turn,
6Eq.",3.1 Global Attention,[0],[0]
(9) implies that all alignment vectors at are of the same length.,3.1 Global Attention,[0],[0]
"For short sentences, we only use the top part of at and for long sentences, we ignore words near the end.
",3.1 Global Attention,[0],[0]
"goes through a deep-output and a maxout layer before making predictions.7 Lastly, Bahdanau et al. (2015) only experimented with one alignment function, the concat product; whereas we show later that the other alternatives are better.",3.1 Global Attention,[0],[0]
"The global attention has a drawback that it has to attend to all words on the source side for each target word, which is expensive and can potentially render it impractical to translate longer sequences, e.g., paragraphs or documents.",3.2 Local Attention,[0],[0]
"To address this deficiency, we propose a local attentional mechanism that chooses to focus only on a small subset of the source positions per target word.
",3.2 Local Attention,[0],[0]
This model takes inspiration from the tradeoff between the soft and hard attentional models proposed by Xu et al. (2015) to tackle the image caption generation task.,3.2 Local Attention,[0],[0]
"In their work, soft attention refers to the global attention approach in which weights are placed “softly” over all patches in the source image.",3.2 Local Attention,[0],[0]
"The hard attention, on the other hand, selects one patch of the image to attend to at a time.",3.2 Local Attention,[0],[0]
"While less expensive at inference time, the hard attention model is non-differentiable and requires more complicated techniques such as variance reduction or reinforcement learning to train.
",3.2 Local Attention,[0],[0]
"7We will refer to this difference again in Section 3.3.
",3.2 Local Attention,[0],[0]
Our local attention mechanism selectively focuses on a small window of context and is differentiable.,3.2 Local Attention,[0],[0]
"This approach has an advantage of avoiding the expensive computation incurred in the soft attention and at the same time, is easier to train than the hard attention approach.",3.2 Local Attention,[0],[0]
"In concrete details, the model first generates an aligned position pt for each target word at time t. The context vector ct is then derived as a weighted average over the set of source hidden states within the window",3.2 Local Attention,[0],[0]
"[pt−D, pt+D]; D is empirically selected.8 Unlike the global approach, the local alignment vector at is now fixed-dimensional, i.e., ∈ R2D+1.",3.2 Local Attention,[0],[0]
"We consider two variants of the model as below.
",3.2 Local Attention,[0],[0]
Monotonic alignment (local-m) – we simply set pt = t assuming that source and target sequences are roughly monotonically aligned.,3.2 Local Attention,[0],[0]
The alignment vector at is defined according to Eq.,3.2 Local Attention,[0],[0]
"(7).9
Predictive alignment (local-p) – instead of assuming monotonic alignments, our model predicts an aligned position as follows:
pt = S · sigmoid(v⊤p tanh(Wpht)), (10) Wp and vp are the model parameters which will be learned to predict positions.",3.2 Local Attention,[0],[0]
S is the source sentence length.,3.2 Local Attention,[0],[0]
"As a result of sigmoid, pt ∈",3.2 Local Attention,[0],[0]
"[0, S].",3.2 Local Attention,[0],[0]
"To favor alignment points near pt, we place a Gaussian distribution centered around pt .",3.2 Local Attention,[0],[0]
"Specifically, our alignment weights are now defined as:
at(s) = align(ht, h̄s) exp ( −(s− pt) 2
2σ2
) (11)
",3.2 Local Attention,[0],[0]
We use the same align function as in Eq. (7) and the standard deviation is empirically set as σ= D2 .,3.2 Local Attention,[0],[0]
"It is important to note that pt is a real nummber; whereas s is an integer within the window centered at pt.10
Comparison to (Gregor et al., 2015) – have proposed a selective attention mechanism, very similar to our local attention, for the image generation task.",3.2 Local Attention,[0],[0]
Their approach allows the model to select an image patch of varying location and zoom.,3.2 Local Attention,[0],[0]
"We, instead, use the same “zoom” for all target positions, which greatly simplifies the formulation and still achieves good performance.
8If the window crosses the sentence boundaries, we simply ignore the outside part and consider words in the window.
",3.2 Local Attention,[0],[0]
"9local-m is the same as the global model except that the vector at is fixed-length and shorter.
",3.2 Local Attention,[0],[0]
"10local-p is similar to the local-m model except that we dynamically compute pt and use a Gaussian distribution to modify the original alignment weights align(ht, h̄s) as shown in Eq.",3.2 Local Attention,[0],[0]
(11).,3.2 Local Attention,[0],[0]
"By utilizing pt to derive at, we can compute backprop gradients for Wp and vp.",3.2 Local Attention,[0],[0]
"In our proposed global and local approaches, the attentional decisions are made independently, which is suboptimal.",3.3 Input-feeding Approach,[0],[0]
"Whereas, in standard MT, a coverage set is often maintained during the translation process to keep track of which source words have been translated.",3.3 Input-feeding Approach,[0],[0]
"Likewise, in attentional NMTs, alignment decisions should be made jointly taking into account past alignment information.",3.3 Input-feeding Approach,[0],[0]
"To address that, we propose an inputfeeding approach in which attentional vectors h̃t are concatenated with inputs at the next time steps as illustrated in Figure 4.11 The effects of having such connections are two-fold: (a) we hope to make the model fully aware of previous alignment choices and (b) we create a very deep network spanning both horizontally and vertically.
",3.3 Input-feeding Approach,[0],[0]
"Comparison to other work – Bahdanau et al. (2015) use context vectors, similar to our ct, in building subsequent hidden states, which can also achieve the “coverage” effect.",3.3 Input-feeding Approach,[0],[0]
"However, there has not been any analysis of whether such connections are useful as done in this work.",3.3 Input-feeding Approach,[0],[0]
"Also, our approach is more general; as illustrated in Figure 4, it can be applied to general stacking recurrent architectures, including non-attentional models.
",3.3 Input-feeding Approach,[0],[0]
Xu et al. (2015) propose a doubly attentional approach with an additional constraint added to the training objective to make sure the model pays equal attention to all parts of the image during the caption generation process.,3.3 Input-feeding Approach,[0],[0]
"Such a constraint can
11If n is the number of LSTM cells, the input size of the first LSTM layer is 2n; those of subsequent layers are n.
also be useful to capture the coverage set effect in NMT that we mentioned earlier.",3.3 Input-feeding Approach,[0],[0]
"However, we chose to use the input-feeding approach since it provides flexibility for the model to decide on any attentional constraints it deems suitable.",3.3 Input-feeding Approach,[0],[0]
We evaluate the effectiveness of our models on the WMT translation tasks between English and German in both directions.,4 Experiments,[0],[0]
newstest2013 (3000 sentences) is used as a development set to select our hyperparameters.,4 Experiments,[0],[0]
"Translation performances are reported in case-sensitive BLEU (Papineni et al., 2002) on newstest2014 (2737 sentences) and newstest2015 (2169 sentences).",4 Experiments,[0],[0]
"Following (Luong et al., 2015), we report translation quality using two types of BLEU: (a) tokenized12 BLEU to be comparable with existing NMT work and (b) NIST13",4 Experiments,[0],[0]
BLEU to be comparable with WMT results.,4 Experiments,[0],[0]
"All our models are trained on the WMT’14 training data consisting of 4.5M sentences pairs (116M English words, 110M German words).",4.1 Training Details,[0],[0]
"Similar to (Jean et al., 2015), we limit our vocabularies to be the top 50K most frequent words for both languages.",4.1 Training Details,[0],[0]
"Words not in these shortlisted vocabularies are converted into a universal token <unk>.
",4.1 Training Details,[0],[0]
"When training our NMT systems, following (Bahdanau et al., 2015; Jean et al., 2015), we filter out sentence pairs whose lengths exceed 50 words and shuffle mini-batches as we proceed.",4.1 Training Details,[0],[0]
"Our stacking LSTM models have 4 layers, each with 1000 cells, and 1000-dimensional embeddings.",4.1 Training Details,[0],[0]
"We follow (Sutskever et al., 2014; Luong et al., 2015) in training NMT with similar settings: (a) our parameters are uniformly initialized in [−0.1, 0.1], (b) we train for 10 epochs using plain SGD, (c) a simple learning rate schedule is employed – we start with a learning rate of 1; after 5 epochs, we begin to halve the learning rate every epoch, (d) our mini-batch size is 128, and (e) the normalized gradient is rescaled whenever its norm exceeds 5.",4.1 Training Details,[0],[0]
"Additionally, we also use dropout for our LSTMs as suggested by (Zaremba et al., 2015).",4.1 Training Details,[0],[0]
"For dropout models, we train for 12 epochs and start halving the learning rate after 8 epochs.
",4.1 Training Details,[0],[0]
Our code is implemented in MATLAB.,4.1 Training Details,[0],[0]
"When
12All texts are tokenized with tokenizer.perl and BLEU scores are computed with multi-bleu.perl.
13With the mteval-v13a script as per WMT guideline.
running on a single GPU device Tesla K40, we achieve a speed of 1K target words per second.",4.1 Training Details,[0],[0]
It takes 7–10 days to completely train a model.,4.1 Training Details,[0],[0]
We compare our NMT systems in the EnglishGerman task with various other systems.,4.2 English-German Results,[0],[0]
"These include the winning system in WMT’14 (Buck et al., 2014), a phrase-based system whose language models were trained on a huge monolingual text, the Common Crawl corpus.",4.2 English-German Results,[0],[0]
"For end-to-end neural machine translation systems, to the best of our knowledge, (Jean et al., 2015) is the only work experimenting with this language pair and currently the SOTA system.",4.2 English-German Results,[0],[0]
"We only present results for some of our attention models and will later analyze the rest in Section 5.
",4.2 English-German Results,[0],[0]
"As shown in Table 1, we achieve progressive improvements when (a) reversing the source sentence, +1.3 BLEU, as proposed in (Sutskever et al., 2014) and (b) using dropout, +1.4 BLEU.",4.2 English-German Results,[0],[0]
"On top of that, (c) the global attention approach gives a significant boost of +2.8 BLEU, making our model slightly better than the base attentional system of Bahdanau et al. (2015) (row RNNSearch).",4.2 English-German Results,[0],[0]
"When (d) using the input-feeding approach, we seize another notable gain of +1.3 BLEU and outperform their system.",4.2 English-German Results,[0],[0]
"The local attention model with predictive alignments (row local-p) proves to be even better, giving us a further improvement of +0.9 BLEU on top of the global attention
model.",4.2 English-German Results,[0],[0]
"It is interesting to observe the trend previously reported in (Luong et al., 2015) that perplexity strongly correlates with translation quality.",4.2 English-German Results,[0],[0]
"In total, we achieve a significant gain of 5.0 BLEU points over the non-attentional baseline, which already includes known techniques such as source reversing and dropout.
",4.2 English-German Results,[0],[0]
"The unknown replacement technique proposed in (Luong et al., 2015; Jean et al., 2015) yields another nice gain of +1.9 BLEU, demonstrating that our attentional models do learn useful alignments for unknown works.",4.2 English-German Results,[0],[0]
"Finally, by ensembling 8 different models of various settings, e.g., using different attention approaches, with and without dropout etc., we were able to achieve a new SOTA result of 23.0 BLEU, outperforming the existing best system (Jean et al., 2015) by +1.4 BLEU.
",4.2 English-German Results,[0],[0]
"Latest results in WMT’15 – despite the fact that our models were trained on WMT’14 with slightly less data, we test them on newstest2015 to demonstrate that they can generalize well to different test sets.",4.2 English-German Results,[0],[0]
"As shown in Table 2, our best system es-
tablishes a new SOTA performance of 25.9 BLEU, outperforming the existing best system backed by NMT and a 5-gram LM reranker by +1.0 BLEU.",4.2 English-German Results,[0],[0]
We carry out a similar set of experiments for the WMT’15 translation task from German to English.,4.3 German-English Results,[0],[0]
"While our systems have not yet matched the performance of the SOTA system, we nevertheless show the effectiveness of our approaches with large and progressive gains in terms of BLEU as illustrated in Table 3.",4.3 German-English Results,[0],[0]
"The attentional mechanism gives us +2.2 BLEU gain and on top of that, we obtain another boost of up to +1.0 BLEU from the input-feeding approach.",4.3 German-English Results,[0],[0]
"Using a better alignment function, the content-based dot product one, together with dropout yields another gain of +2.7 BLEU.",4.3 German-English Results,[0],[0]
"Lastly, when applying the unknown word replacement technique, we seize an additional +2.1 BLEU, demonstrating the usefulness of attention in aligning rare words.",4.3 German-English Results,[0],[0]
"We conduct extensive analysis to better understand our models in terms of learning, the ability to handle long sentences, choices of attentional architectures, and alignment quality.",5 Analysis,[0],[0]
All models considered here are English-German NMT systems tested on newstest2014.,5 Analysis,[0],[0]
We compare models built on top of one another as listed in Table 1.,5.1 Learning curves,[0],[0]
It is pleasant to observe in Figure 5 a clear separation between non-attentional and attentional models.,5.1 Learning curves,[0],[0]
"The input-feeding ap-
proach and the local attention model also demonstrate their abilities in driving the test costs lower.",5.1 Learning curves,[0],[0]
"The non-attentional model with dropout (the blue + curve) learns slower than other non-dropout models, but as time goes by, it becomes more robust in terms of minimizing test errors.",5.1 Learning curves,[0],[0]
"We follow (Bahdanau et al., 2015) to group sentences of similar lengths together and compute a BLEU score per group.",5.2 Effects of Translating Long Sentences,[0],[0]
"As demonstrated in Figure 6, our attentional models are more effective than the other non-attentional model in handling long sentences: the translation quality does not degrade as sentences become longer.",5.2 Effects of Translating Long Sentences,[0],[0]
Our best model (the blue + curve) outperforms all other systems in all length buckets.,5.2 Effects of Translating Long Sentences,[0],[0]
"We examine different attention models (global, local-m, local-p) and different alignment functions (location, dot, general, concat) as described in Section 3.",5.3 Choices of Attentional Architectures,[0],[0]
"Due to limited resources, we cannot run all the possible combinations.",5.3 Choices of Attentional Architectures,[0],[0]
"However,
results in Table 4 do give us some idea about different choices.",5.3 Choices of Attentional Architectures,[0],[0]
"The location-based function does not learn good alignments: the global (location) model can only obtain a small gain when performing unknown word replacement compared to using other alignment functions.14 For content-based functions, our implementation of concat does not yield good performances and more analysis should be done to understand the reason.15",5.3 Choices of Attentional Architectures,[0],[0]
It is interesting to observe that dot works well for the global attention and general is better for the local attention.,5.3 Choices of Attentional Architectures,[0],[0]
"Among the different models, the local attention model with predictive alignments (local-p) is best, both in terms of perplexities and BLEU.",5.3 Choices of Attentional Architectures,[0],[0]
A by-product of attentional models are word alignments.,5.4 Alignment Quality,[0],[0]
"While (Bahdanau et al., 2015) visualized alignments for some sample sentences and observed gains in translation quality as an indication of a working attention model, no work has assessed the alignments learned as a whole.",5.4 Alignment Quality,[0],[0]
"In contrast, we set out to evaluate the alignment quality using the alignment error rate (AER) metric.
",5.4 Alignment Quality,[0],[0]
"Given the gold alignment data provided by RWTH for 508 English-German Europarl sentences, we “force” decode our attentional models to produce translations that match the references.",5.4 Alignment Quality,[0],[0]
"We extract only one-to-one alignments by selecting the source word with the highest alignment
14There is a subtle difference in how we retrieve alignments for the different alignment functions.",5.4 Alignment Quality,[0],[0]
"At time step t in which we receive yt−1 as input and then compute ht,at, ct, and h̃t before predicting yt, the alignment vector at is used as alignment weights for (a) the predicted word yt in the location-based alignment functions and (b) the input word yt−1 in the content-based functions.
",5.4 Alignment Quality,[0],[0]
"15With concat, the perplexities achieved by different models are 6.7 (global), 7.1 (local-m), and 7.1 (local-p).
",5.4 Alignment Quality,[0],[0]
weight per target word.,5.4 Alignment Quality,[0],[0]
"Nevertheless, as shown in Table 6, we were able to achieve AER scores comparable to the one-to-many alignments obtained by the Berkeley aligner (Liang et al., 2006).16
",5.4 Alignment Quality,[0],[0]
We also found that the alignments produced by local attention models achieve lower AERs than those of the global one.,5.4 Alignment Quality,[0],[0]
"The AER obtained by the ensemble, while good, is not better than the local-m AER, suggesting the well-known observation that AER and translation scores are not well correlated (Fraser and Marcu, 2007).",5.4 Alignment Quality,[0],[0]
"Due to space constraint, we can only show alignment visualizations in the arXiv version of our paper.17",5.4 Alignment Quality,[0],[0]
We show in Table 5 sample translations in both directions.,5.5 Sample Translations,[0],[0]
It it appealing to observe the effect of attentional models in correctly translating names such as “Miranda Kerr” and “Roger Dow”.,5.5 Sample Translations,[0],[0]
"Non-attentional models, while producing sensible names from a language model perspective, lack the direct connections from the source side to make correct translations.
",5.5 Sample Translations,[0],[0]
"We also observed an interesting case in the second English-German example, which requires translating the doubly-negated phrase, “not incompatible”.",5.5 Sample Translations,[0],[0]
The attentional model correctly produces “nicht . . .,5.5 Sample Translations,[0],[0]
"unvereinbar”; whereas the non-attentional model generates “nicht vereinbar”, meaning “not compatible”.18",5.5 Sample Translations,[0],[0]
The attentional model also demonstrates its superiority in translating long sentences as in the last example.,5.5 Sample Translations,[0],[0]
"In this paper, we propose two simple and effective attentional mechanisms for neural machine
16We concatenate the 508 sentence pairs with 1M sentence pairs from WMT and run the Berkeley aligner.
",6 Conclusion,[0],[0]
"17http://arxiv.org/abs/1508.04025 18The reference uses a more fancy translation of “incompatible”, which is “im Widerspruch zu etwas stehen”.",6 Conclusion,[0],[0]
"Both models, however, failed to translate “passenger experience”.
",6 Conclusion,[0],[0]
translation: the global approach which always looks at all source positions and the local one that only attends to a subset of source positions at a time.,6 Conclusion,[0],[0]
We test the effectiveness of our models in the WMT translation tasks between English and German in both directions.,6 Conclusion,[0],[0]
Our local attention yields large gains of up to 5.0 BLEU over non-attentional models that already incorporate known techniques such as dropout.,6 Conclusion,[0],[0]
"For the English to German translation direction, our ensemble model has established new state-of-the-art results for both WMT’14 and WMT’15.
",6 Conclusion,[0],[0]
We have compared various alignment functions and shed light on which functions are best for which attentional models.,6 Conclusion,[0],[0]
"Our analysis shows that attention-based NMT models are superior to nonattentional ones in many cases, for example in
translating names and handling long sentences.",6 Conclusion,[0],[0]
We gratefully acknowledge support from a gift from Bloomberg L.P. and the support of NVIDIA Corporation with the donation of Tesla K40 GPUs.,Acknowledgment,[0],[0]
We thank Andrew Ng and his group as well as the Stanford Research Computing for letting us use their computing resources.,Acknowledgment,[0],[0]
We thank Russell Stewart for helpful discussions on the models.,Acknowledgment,[0],[0]
"Lastly, we thank Quoc Le, Ilya Sutskever, Oriol Vinyals, Richard Socher, Michael Kayser, Jiwei Li, Panupong Pasupat, Kelvin Gu, members of the Stanford NLP Group and the annonymous reviewers for their valuable comments and feedback.",Acknowledgment,[0],[0]
An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation.,abstractText,[0],[0]
"However, there has been little work exploring useful architectures for attention-based NMT.",abstractText,[0],[0]
This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time.,abstractText,[0],[0]
We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions.,abstractText,[0],[0]
"With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout.",abstractText,[0],[0]
"Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT’15 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker.1",abstractText,[0],[0]
Effective Approaches to Attention-based Neural Machine Translation,title,[0],[0]
How can the elements from two sets be paired one-to-one to have the largest sum of pairwise utilities?,1 Introduction,[0],[0]
This maximum weighted perfect bipartite matching problem is a classical combinatorial optimization problem in computer science.,1 Introduction,[0],[0]
"It can be formulated and efficiently solved in polynomial time as a linear program or using more specialized Hungarian algorithm techniques (Kuhn, 1955).",1 Introduction,[0],[0]
"This has made it an
*Equal contribution 1Department of Computer Science, University of Illinois at Chicago.",1 Introduction,[0],[0]
"Correspondence to: Rizal Fathony <rfatho2@uic.edu>, Sima Behpour <sbehpo2@uic.edu>.
",1 Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1 Introduction,[0],[0]
"Copyright 2018 by the author(s).
attractive formalism for posing a wide range of problems, including recognizing correspondences in similar images (Belongie et al., 2002; Liu et al., 2008; Zhu et al., 2008; Rui et al., 2007), finding word alignments in text (Chan & Ng, 2008), and providing ranked lists of items for information retrieval tasks (Amini et al., 2008).
",1 Introduction,[0],[0]
Machine learning methods seek to estimate the pairwise utilities of bipartite graphs so that the maximum weighted complete matching is most compatible with the (distribution of) ground truth matchings of training data.,1 Introduction,[0],[0]
"When these utilities are learned abstractly, they can be employed to make predictive matchings for test samples.",1 Introduction,[0],[0]
"Unfortunately, important measures of incompatibility (e.g., the Hamming loss) are often non-continuous with many local optima in the predictors’ parameter spaces, making direct minimization intractable.",1 Introduction,[0],[0]
"Given this difficulty, two natural desiderata for any predictor are:
• Efficiency: learning from training data and making predictions must be computed efficiently in (lowdegree) polynomial time; and • Consistency: the predictor’s training objectives must also minimize the underlying Hamming loss, at least under ideal learning conditions (given the true distribution and fully expressive model parameters).
",1 Introduction,[0],[0]
"Existing methods for learning bipartite matchings fail in one or the other of these desiderata; exponentiated potential fields models (Lafferty et al., 2001; Petterson et al., 2009) are intractable for large sets of items, while maximum margin methods based on the hinge loss surrogate (Taskar et al., 2005a; Tsochantaridis et al., 2005) lack Fisher consistency (Tewari & Bartlett, 2007; Liu, 2007).",1 Introduction,[0],[0]
"We discuss these limitations formally in Section 2.
",1 Introduction,[0],[0]
"Given the deficiencies of the existing methods, we contribute the first approach for learning bipartite matchings that is both computationally efficient and Fisher consistent.",1 Introduction,[0],[0]
"Our approach is based on an adversarial formulation for learning (Topsøe, 1979; Grünwald & Dawid, 2004; Asif et al., 2015) that poses prediction-making as a dataconstrained zero-sum game between a player seeking to minimize the expected loss and an adversarial data approximator seeking to maximize the expected loss.",1 Introduction,[0],[0]
"We present two approaches for solving the corresponding zero-sum game arising from our formulation: (1) using the double
oracle method of constraint generation to find a sparselysupported equilibrium for the zero-sum game; and (2) decomposing the game’s solution into marginal probabilities and optimizes these marginal probabilities directly to obtain an equilibrium saddle point for the game.",1 Introduction,[0],[0]
We then establish the computational efficiency and consistency of this approach and demonstrate its benefits experimentally.,1 Introduction,[0],[0]
"Given two sets of elements A and B of equal size (|A| = |B|), a maximum weighted bipartite matching π is the one-toone mapping (e.g., Figure 1) from each element in A to each element in B that maximizes the sum of potentials: maxπ∈Π ψ(π) = maxπ∈Π ∑ i ψi(πi).",2.1 Bipartite Matching Task,[0],[0]
"Here πi ∈
",2.1 Bipartite Matching Task,[0],[0]
"[n] := {1, 2, . . .",2.1 Bipartite Matching Task,[0],[0]
", n} is the entry in B that is matched with the i-th entry of A. The set of possible solutions Π is simply all permutation of [n].",2.1 Bipartite Matching Task,[0],[0]
"Many machine learning tasks pose prediction as the solution to this problem, including: word alignment for natural language processing tasks (Taskar et al., 2005b; Padó & Lapata, 2006; MacCartney et al., 2008); learning correspondences between images in computer vision applications (Belongie et al., 2002; Dellaert et al., 2003); protein structure analysis in computational biology (Taylor, 2002; Wang et al., 2004); and learning to rank a set of items for information retrieval tasks (Dwork et al., 2001; Le & Smola, 2007).",2.1 Bipartite Matching Task,[0],[0]
"Thus, learning appropriate weights ψi(·) for bipartite graph matchings is a key problem for many application areas.",2.1 Bipartite Matching Task,[0],[0]
"Given a predicted permutation, π′, and the “ground truth” permutation, π, the Hamming loss counts the number of mistaken pairings: lossHam(π, π′) =",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
∑n i=1,2.2 Performance Evaluation and Fisher Consistency,[0],[0]
1(π ′,2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"i 6= πi), where 1(·) = 1 if · is true and 0 otherwise.",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"When the “ground truth” is a distribution over permutations, P (π), rather than a single permutation, the (set of) Bayes optimal prediction(s) is: argminπ′ ∑ π P (π) lossHam(π, π
′).",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"For a predictor to be Fisher consistent, it must provide a Bayes optimal prediction for any possible distribution P (π) when trained from that exact distribution using the predictor’s most general possible parameterization (e.g., all measurable functions ψ for potential-based models).",2.2 Performance Evaluation and Fisher Consistency,[0],[0]
"A probabilistic approach to learning bipartite graphs uses an exponential family distribution over permutations,
Pψ(π) = e",2.3 Exponential Family Random Field Approach,[0],[0]
∑n i=1,2.3 Exponential Family Random Field Approach,[0],[0]
"ψi(πi)/Zψ , trained by maximizing training data likelihood.",2.3 Exponential Family Random Field Approach,[0],[0]
"This provides certain statistical consistency guarantees for its marginal probability estimates (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
"Specifically, if the potentials ψ are chosen from the space of all measurable functions to maximize the likelihood of the true distribution of permutations P (π), then Pψ(π) will match the marginal probabilities of the true distribution: ∀i, j, Pψ(πi = j) = P (πi = j).",2.3 Exponential Family Random Field Approach,[0],[0]
"This implies Fisher consistency because the MAP estimate under this distribution, which can be obtained as a maximum weighted bipartite matching, is Bayes optimal.
",2.3 Exponential Family Random Field Approach,[0],[0]
The key challenge with this approach is its computational complexity.,2.3 Exponential Family Random Field Approach,[0],[0]
"The normalization term, Zψ , is the permanent of a matrix defined in terms of exponentiated potential terms:",2.3 Exponential Family Random Field Approach,[0],[0]
"Zψ = ∑ π ∏n i=1 e
ψi(πi) = perm(M) where Mi,j = e
ψi(j).",2.3 Exponential Family Random Field Approach,[0],[0]
"For sets of small size (e.g., n = 5), enumerating the permutations is tractable and learning using the exponential random field model incurs a run-time cost that is acceptable in practice (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
"However, the matrix permanent computation is a #P-hard problem to compute exactly (Valiant, 1979).",2.3 Exponential Family Random Field Approach,[0],[0]
"Monte Carlo sampling approaches are used instead of permutation enumeration to maximize the data likelihood (Petterson et al., 2009; Volkovs & Zemel, 2012).",2.3 Exponential Family Random Field Approach,[0],[0]
"Though exact samples can be generated efficiently in polynomial time (Huber & Law, 2008), the number of samples needed for reliable likelihood or gradient estimates makes this approach infeasible for applications with even modestly-sized sets of n = 20 elements (Petterson et al., 2009).",2.3 Exponential Family Random Field Approach,[0],[0]
Maximum margin methods for structured prediction seek potentials ψ,2.4 Maximum Margin Approach,[0],[0]
"that minimize the training sample hinge loss:
min ψ",2.4 Maximum Margin Approach,[0],[0]
"Eπ∼P̃ [ max π′ {loss(π, π′) + ψ(π′)}",2.4 Maximum Margin Approach,[0],[0]
"− ψ(π) ] , (1)
where P̃ is the empirical distribution.",2.4 Maximum Margin Approach,[0],[0]
"Finding the optimal ψ is a convex optimization problem (Boyd & Vandenberghe, 2004) that can generally be tractably solved using constraint generation methods as long as the maximizing assignments can be found efficiently.",2.4 Maximum Margin Approach,[0],[0]
"In the case of permutation learning, finding the permutation π′ with highest hinge loss reduces to a maximum weighted bipartite matching problem and can therefore be solved efficiently.
",2.4 Maximum Margin Approach,[0],[0]
"Though computationally efficient, maximum margin approaches for learning to make perfect bipartite matches lack Fisher consistency, which requires the prediction π∗ = argmaxπ ψ(π) resulting from Equation (1) to minimize the expected risk,",2.4 Maximum Margin Approach,[0],[0]
Eπ∼P̃,2.4 Maximum Margin Approach,[0],[0]
"[loss(π, π′)], for all distributions P̃ .",2.4 Maximum Margin Approach,[0],[0]
"We consider a distribution over permutations that is an extension of a counterexample for multiclass classification consistency analysis with no majority label (Liu,
2007): P (π = [1 2 3])",2.4 Maximum Margin Approach,[0],[0]
= 0.4;P,2.4 Maximum Margin Approach,[0],[0]
(π = [2 3 1]) = 0.3; and P (π = [3 1 2]) = 0.3.,2.4 Maximum Margin Approach,[0],[0]
"The potential function ψi(j) = 1 if i = j and 0 otherwise, provides a Bayes optimal permutation prediction for this distribution and an expected hinge loss of 3.6 = 0.4(3 − 3) + 0.3(3 + 3) + 0.3(3 + 3).",2.4 Maximum Margin Approach,[0],[0]
"However, the expected hinge loss is optimally minimized with a value of 3 when ψi(j) = 0,∀i, j, which is indifferent between all permutations and is not Bayes optimal.",2.4 Maximum Margin Approach,[0],[0]
"Thus, Fisher consistency is not guaranteed.",2.4 Maximum Margin Approach,[0],[0]
"To overcome the computational inefficiency of exponential random field methods and the Fisher inconsistency of maximum margin methods, we formulate the task of learning for bipartite matching problems as an adversarial structured prediction task.",3 Approach,[0],[0]
We present two approaches for efficiently solving the resulting game over permutations.,3 Approach,[0],[0]
"The training data for bipartite matching consists of triplets (A,B, π) where A and B are two sets of nodes with equal size and π is the assignment.",3.1 Permutation Mixture Formulation,[0],[0]
"To simplify the notation, we denote x as the bipartite graph containing the nodes A and B. We also denote φ(x, π) as a vector that enumerates the joint feature representations based on the bipartite graph x and the matching assignment π.",3.1 Permutation Mixture Formulation,[0],[0]
"This joint feature is defined additively over each node assignment, i.e., φ(x, π) = ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, πi).
",3.1 Permutation Mixture Formulation,[0],[0]
Our approach seeks a predictor that robustly minimizes the Hamming loss against the worst-case permutation mixture probability that is consistent with the statistics of the training data.,3.1 Permutation Mixture Formulation,[0],[0]
"In this setting, a predictor makes a probabilistic prediction over the set of all possible assignments (denoted as P̂ ).",3.1 Permutation Mixture Formulation,[0],[0]
"Instead of evaluating the predictor with the empirical distribution, the predictor is pitted against an adversary that also makes a probabilistic prediction (denoted as P̌ ).",3.1 Permutation Mixture Formulation,[0],[0]
"The predictor’s objective is to minimize the expected loss function calculated from the predictor’s and adversary’s probabilistic predictions, while the adversary seeks to maximize the loss.",3.1 Permutation Mixture Formulation,[0],[0]
"The adversary (and only the adversary) is constrained to select a probabilistic prediction that matches the statistical summaries of the empirical training distribution (denoted as P̃ ) via moment matching constraints on joint features φ(x, π).",3.1 Permutation Mixture Formulation,[0],[0]
"Formally, we write our formulation as:
min P̂ (π̂|x) max P̌ (π̌|x) Ex∼P̃ ;π̂|x∼P̂ ;π̌|x∼P̌ [loss(π̂, π̌)] s.t. (2)
Ex∼P̃ ;π̌|x∼P̌ [ n∑ i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, π̌i)",3.1 Permutation Mixture Formulation,[0],[0]
"] = E(x,π)∼P̃ [ n∑ i=1",3.1 Permutation Mixture Formulation,[0],[0]
"φi(x, πi) ] .
",3.1 Permutation Mixture Formulation,[0],[0]
"This follows a recent line of work for adversarial classification under additive (Asif et al., 2015) and non-additive
(Wang et al., 2015) loss functions that has been employed for chain-structured prediction (Li et al., 2016), object detection (Behpour et al., 2017), and robust cut learning (Behpour et al., 2018).",3.1 Permutation Mixture Formulation,[0],[0]
"Using the method of Lagrangian multipliers and strong duality for convex-concave saddle point problems (Von Neumann & Morgenstern, 1945; Sion, 1958), The optimization in Eq. (2) can be equivalently solved in the dual formulation:
min θ Ex,π∼P̃ min P̂ (π̂|x) max P̌ (π̌|x) Eπ̂|x∼P̂ π̌|x∼P̌
[ loss(π̂, π̌)+ (3)
",3.1 Permutation Mixture Formulation,[0],[0]
θ · n∑ i=1,3.1 Permutation Mixture Formulation,[0],[0]
"(φi(x, π̌i)− φi(x, πi)) ] ,
where θ is the Lagrange dual variable for the moment matching constraints.",3.1 Permutation Mixture Formulation,[0],[0]
"We refer the reader to Appendix A in the supplementary materials for a more detailed explanation of this construction (i.e., the transformation from Eq.",3.1 Permutation Mixture Formulation,[0],[0]
(2) to Eq. (3)).,3.1 Permutation Mixture Formulation,[0],[0]
"In this paper, we use Hamming distance, loss(π̂, π̌) = ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"1(π̂i 6= π̌i), as the loss function.
",3.1 Permutation Mixture Formulation,[0],[0]
Table 1 shows the payoff matrix for the game of size n = 3 with 3!,3.1 Permutation Mixture Formulation,[0],[0]
"actions (permutations) for the predictor player π̂ and for the adversarial approximation player π̌. Here, we define the difference between the Lagrangian potential of the adversary’s action and the ground truth permutation as δπ̌ = ψ(π̌)− ψ(π) = θ · ∑n i=1",3.1 Permutation Mixture Formulation,[0],[0]
"(φi(x, π̌i)− φi(x, πi)) .
",3.1 Permutation Mixture Formulation,[0],[0]
"Unfortunately, the number of permutations, π, grows factorially (O(n!))",3.1 Permutation Mixture Formulation,[0],[0]
with the number of elements being matched (n).,3.1 Permutation Mixture Formulation,[0],[0]
This makes explicit construction of the Lagrangian minimax game intractable for modestly-sized problems.,3.1 Permutation Mixture Formulation,[0],[0]
"Our first approach for taming the factorial computational complexity of explicitly constructing games for matching tasks is a constraint-generation approach known as the double oracle method (McMahan et al., 2003).",3.2 Optimization by Constraint Generation,[0],[0]
It obtains the equilibrium solution to the adversarial prediction game without explicitly constructing the entire game matrix (Table 1).,3.2 Optimization by Constraint Generation,[0],[0]
"Based on the key observation that the equilibrium of the zero-sum game is typically supported by a relatively small number of permutations, it seeks to efficiently uncover this sparse set of permutations for each player.
",3.2 Optimization by Constraint Generation,[0],[0]
Algorithm 1 Double Oracle Algorithm for Adversarial Bipartite Matching Equilibria.,3.2 Optimization by Constraint Generation,[0],[0]
"Input: Lagrangian potentials Ψ(·); Initial label πinitial Output: The (sparse) Nash equilibrium (Š, Ŝ, P̂ , P̌ )
1: Š ← Ŝ ← {πinitial} 2: repeat 3: (P̂ , P̌ , V̌ )← solveGame(Ψ(Š), lossHam(Ŝ, Š))",3.2 Optimization by Constraint Generation,[0],[0]
"4: (π̌new,Vmax)←argmaxπ̌Eπ̂∼P̂ [lossHam(π̂, π̌)+Ψ(π̌)] 5: if (V̌ 6= Vmax) then Š ← Š ∪ π̌new 6: (P̂ , P̌ , V̂ )← solveGame(Ψ(Š), lossHam(Ŝ, Š)) 7: (π̂new, Vmin)← argminπ̂",3.2 Optimization by Constraint Generation,[0],[0]
"Eπ̌∼P̌ [lossHam(π̂, π̌)] 8: if (V̂ 6= Vmin) then Ŝ ← Ŝ ∪ π̂new 9: until V̌ = Vmax = V̂ = Vmin
10: return (Š, Ŝ, P̂ , P̌ )
Algorithm 1 produces this set of “active” permutations for each player, Ŝ and Š (subsets of rows and columns in Table 1), and the associated Nash equilibrium (P̂ , P̌ ).",3.2 Optimization by Constraint Generation,[0],[0]
"Starting from an initial permutation, πinitial (Line 1), it repeatedly obtains the Nash equilibrium solution (P̂ , P̌ ) with value V̂ or V̌ for the zero-sum game defined only by permutations in Ŝ and Š (Lines 3 and 6).",3.2 Optimization by Constraint Generation,[0],[0]
"This is efficiently accomplished using a linear program (Von Neumann & Morgenstern, 1945).",3.2 Optimization by Constraint Generation,[0],[0]
"The algorithm then obtains the other player’s best response to either P̂ or P̌ (Lines 4 and 7) with values Vmax and Vmin using the Kuhn-Munkres (Hungarian) algorithm in O(n3) time for sets of size n. These best responses, π̌new and π̂new, are added to the set of active permutations (i.e., new rows or columns in the game matrix) if they have better values than the previous equilibrium values (Lines 5 and 8).",3.2 Optimization by Constraint Generation,[0],[0]
"This is repeated until no game value improvement exists for either player (Line 9), at which point a Nash equilibrium for the full game has been obtained.
",3.2 Optimization by Constraint Generation,[0],[0]
We solve the convex optimization of Lagrange parameters θ in Eq.,3.2 Optimization by Constraint Generation,[0],[0]
(3) using the results of Algorithm 1.,3.2 Optimization by Constraint Generation,[0],[0]
"We employ AdaGrad (Duchi et al., 2011) with the gradient calculated as the difference between expected features under the adversary’s distribution and the empirical training data: Ex∼P̃ ;π̌|x∼P̌ [ ∑n i=1 φi(x, π̌i)]− Ex,π∼P̃ [ ∑n i=1",3.2 Optimization by Constraint Generation,[0],[0]
"φi(x, πi)].
",3.2 Optimization by Constraint Generation,[0],[0]
"In contrast with SSVM, which compute the hinge loss for each training instance using only a single run of the Hungarian algorithm, our double oracle method must solve this problem repeatedly to find the equilibrium.",3.2 Optimization by Constraint Generation,[0],[0]
Though in practice the total number of active permutations is much smaller than the n!,3.2 Optimization by Constraint Generation,[0],[0]
"possibilities, no formal polynomial bound is known—and, consequentially, the run time of the approach as a whole cannot be characterized as polynomial.",3.2 Optimization by Constraint Generation,[0],[0]
"Our second approach, which significantly improves the efficiency of solving the adversarial bipartite matching game,
leverages the key insight that all quantities of interest for evaluating the loss and satisfying the constraints depend only on marginal probabilities of the permutation’s value assignments.",3.3 Marginal Distribution Formulation,[0],[0]
"Based on this, we employ a marginal distribution decomposition of the game.
",3.3 Marginal Distribution Formulation,[0],[0]
"We begin this reformulation by first defining a matrix representation of permutation π as Y(π) ∈ Rn×n (or simply Y) where the value of its cell Yi,j is 1 when πi = j and 0 otherwise.",3.3 Marginal Distribution Formulation,[0],[0]
"To be a valid complete bipartite matching or permutation, each column and row of Y can only have one entry of 1.",3.3 Marginal Distribution Formulation,[0],[0]
"For each feature function φ(k)i (x, πi), we also denote its matrix representation as Xk whose (i, j)-th cell represents the k-th entry of φi(x, j).",3.3 Marginal Distribution Formulation,[0],[0]
"For a given distribution of permutations, P (π), we denote the marginal probabilities of matching i with j as pi,j , P (πi = j).",3.3 Marginal Distribution Formulation,[0],[0]
"We let P = ∑ π P (π)Y(π) be the predictor’s marginal probability matrix where its (i, j) cell represents P̂ (π̂i = j), and similarly let Q be the adversary’s marginal probability matrix (based on P̌ ), as shown in Table 2.
",3.3 Marginal Distribution Formulation,[0],[0]
"The Birkhoff–von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953) states that the convex hull of the set of n× n permutation matrices forms a convex polytope in Rn2 (known as the Birkhoff polytope Bn) in which points are doubly stochastic matrices, i.e., the n×nmatrices with non-negative elements where each row and column must sum to one.",3.3 Marginal Distribution Formulation,[0],[0]
This implies that both marginal probability matrices P and Q are doubly stochastic matrices.,3.3 Marginal Distribution Formulation,[0],[0]
"In contrast to the space of distributions over permutation of n objects, which grows factorially (O(n!) with n!",3.3 Marginal Distribution Formulation,[0],[0]
"− 1 free parameters), the size of this marginal matrices grows only quadratically (O(n2) with n2 − 2n free parameters).",3.3 Marginal Distribution Formulation,[0],[0]
"This provides a significant benefit in terms of the optimization.
",3.3 Marginal Distribution Formulation,[0],[0]
"Starting with the minimax over P̂ (π̂) and P̌ (π̌) in the permutation mixture formulation, and using the matrix notation above, we rewrite Eq.",3.3 Marginal Distribution Formulation,[0],[0]
"(3) as a minimax over marginal probability matrices P and Q with additional constraints that both P and Q are doubly-stochastic matrices, i.e., P ≥ 0 (elementwise), Q ≥ 0, P1 = P>1 = Q1 = Q>1 = 1 where 1 = (1, . .",3.3 Marginal Distribution Formulation,[0],[0]
.,3.3 Marginal Distribution Formulation,[0],[0]
", 1)>).",3.3 Marginal Distribution Formulation,[0],[0]
"That is:
min θ EX,Y∼P̃ minP≥0 maxQ≥0",3.3 Marginal Distribution Formulation,[0],[0]
"[n−〈P,Q〉+〈Q−Y, ∑ k θkXk〉]
s.t. :",3.3 Marginal Distribution Formulation,[0],[0]
"P1 = P>1 = Q1 = Q>1 = 1, (4)
where 〈·, ·〉 denotes the Frobenius inner product between two matrices, i.e., 〈A,B〉 = ∑ i,j Ai,jBi,j .",3.3 Marginal Distribution Formulation,[0],[0]
We reduce the computational costs of the optimization in Eq.,3.3.1 OPTIMIZATION,[0],[0]
"(4) by focusing on optimizing the adversary’s marginal probability Q. By strong duality, we then push the maximization over Q in the formulation above to the outermost level of Eq.",3.3.1 OPTIMIZATION,[0],[0]
(4).,3.3.1 OPTIMIZATION,[0],[0]
"Note that the objective above is a non-smooth function (i.e., piece-wise linear).",3.3.1 OPTIMIZATION,[0],[0]
"For the purpose of smoothing the objective, we add a small amount of strongly convex prox-functions to both P and Q. We also add a regularization penalty to the parameter θ to improve the generalizability of our model.",3.3.1 OPTIMIZATION,[0],[0]
We unfold Eq.,3.3.1 OPTIMIZATION,[0],[0]
"(4) by replacing the empirical expectation with an average over all training examples, resulting in the following optimization:
max Q≥0 min θ
1
m m∑ i=1",3.3.1 OPTIMIZATION,[0],[0]
"min Pi≥0 [ 〈Qi −Yi, ∑ k θkXi,k〉 − 〈Pi,Qi〉
+ µ2 ‖Pi‖ 2 F − µ 2 ‖Qi‖ 2 F ] +",3.3.1 OPTIMIZATION,[0],[0]
"λ2 ‖θ‖ 2 2
s.t. : Pi1 = P>i 1 = Qi1 =",3.3.1 OPTIMIZATION,[0],[0]
Q >,3.3.1 OPTIMIZATION,[0],[0]
"i 1 = 1, ∀i, (5)
where m is the number of bipartite matching problems in the training set, λ is the regularization penalty parameter, µ is the smoothing penalty parameter, and ‖A‖F denotes the Frobenius norm of matrixA. The subscript i in Pi,Qi,Xi, and Yi refers to the i-th example in the training set.
",3.3.1 OPTIMIZATION,[0],[0]
"In the formulation above, given a fixed Q, the inner minimization over θ and P can then be solved separately.",3.3.1 OPTIMIZATION,[0],[0]
"The optimal θ in the inner minimization admits a closed-form solution, in which the k-th element of θ∗ is:
θ∗k =",3.3.1 OPTIMIZATION,[0],[0]
"− 1
λm m∑ i=1",3.3.1 OPTIMIZATION,[0],[0]
"〈Qi −Yi,Xi,k〉 .",3.3.1 OPTIMIZATION,[0],[0]
"(6)
The inner minimization over P can be solved independently for each training example.",3.3.1 OPTIMIZATION,[0],[0]
"Given the adversary’s marginal probability matrix Qi for the i-th example, the optimal Pi can be formulated as:
P∗i = argmin {Pi≥0|Pi1=P>i 1=1} µ 2 ‖Pi‖ 2 F − 〈Pi,Qi〉 (7)
= argmin {Pi≥0|Pi1=P>i 1=1}
‖Pi − 1µQi‖ 2 F .",3.3.1 OPTIMIZATION,[0],[0]
"(8)
We can interpret this minimization as projecting the matrix 1 µQi to the set of doubly-stochastic matrices.",3.3.1 OPTIMIZATION,[0],[0]
"We will discuss our projection technique in the upcoming subsection.
",3.3.1 OPTIMIZATION,[0],[0]
"For solving the outer optimization over Q with the doublystochastic constraints, we employ a projected QuasiNewton algorithm (Schmidt et al., 2009).",3.3.1 OPTIMIZATION,[0],[0]
Each iteration of the algorithm optimizes the quadratic approximation of the objective function (using limited-memory Quasi-Newton) over the the convex set.,3.3.1 OPTIMIZATION,[0],[0]
"In each update step, a projection to the set of doubly-stochastic matrices is needed, akin to the inner minimization of P in Eq.",3.3.1 OPTIMIZATION,[0],[0]
"(8).
",3.3.1 OPTIMIZATION,[0],[0]
"The optimization above provides the adversary’s optimal marginal probability Q∗. To achieve our learning goal, we recover θ∗ using Eq.",3.3.1 OPTIMIZATION,[0],[0]
(6) computed over the optimal Q∗. We use the θ∗ that our model learns from this optimization to construct a weighted bipartite graph for making predictions for test examples.,3.3.1 OPTIMIZATION,[0],[0]
"The projection from an arbitrary matrix R to the set of doubly-stochastic matrices can be formulated as:
min P≥0 ‖P−R‖2F , s.t. : P1 = P>1 = 1.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(9)
We employ the alternating direction method of multipliers (ADMM) technique (Douglas & Rachford, 1956; Glowinski & Marroco, 1975; Boyd et al., 2011) to solve the optimization problem above.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"We divide the doubly-stochastic matrix constraint into two sets of constraints C1 : P1 = 1 and P ≥ 0, andC2 :",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
P>1 = 1 and P ≥ 0.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Using this construction, we convert the optimization above into ADMM form as follows:
min P,S
1 2‖P−R‖ 2 F + 1 2‖S−R‖ 2 F + IC1(P) + IC2(S)
s.t. :",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
P− S = 0.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(10)
The augmented Lagrangian for this optimization is:
Lρ(P,S,W) = 12‖P−R‖ 2 F + 1 2‖S−R‖ 2 F + IC1(P)
+ IC2(S)",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"+ ρ 2‖P− S + W‖ 2 F , (11)
where ρ is the ADMM penalty parameter and W is the scaled dual variable.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"From the augmented Lagrangian, we compute the update for P as:
Pt+1 = argmin P Lρ(P,St,Wt) (12)
= argmin {P≥0|P1=1}
1 2‖P−R‖ 2 F + ρ 2‖P− S t + Wt‖2F
= argmin {P≥0|P1=1}
‖P− 11+ρ ( R + ρ",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( St −Wt )),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"‖2F .
",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
The minimization above can be interpreted as a projection to the set {P ≥ 0|P1 = 1} which can be realized by projecting to the probability simplex independently for each row of the matrix,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
11+ρ (R + ρ,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(S
t −Wt)).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Similarly, the ADMM update for S can also be formulated as a columnwise probability simplex projection.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"The technique for projecting a point to the probability simplex has been studied previously, e.g., by Duchi et al. (2008).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"Therefore, our ADMM algorithm consists of the following updates:
Pt+1 = ProjC1 ( 1 1+ρ ( R + ρ",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( St −Wt ))),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(13)
St+1 =",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
ProjC2 ( 1 1+ρ ( R + ρ,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
( Pt+1 + Wt ))),3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(14)
Wt+1 = Wt + Pt+1",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
− St+1.,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"(15)
We run this series of updates until the stopping conditions are met.",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
Our stopping conditions are based on the primal and dual residual optimality as described in Boyd et al. (2011).,3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"In our overall algorithm, this ADMM projection algorithm is used both in the projected Quasi-Newton algorithm for optimizing Q (Eq. (5)) and in the inner optimization for minimizing Pi (Eq. (8)).",3.3.2 DOUBLY-STOCHASTIC MATRIX PROJECTION,[0],[0]
"The convergence rate of ADMM is O(log 1 ) thanks to the strong convexity of the objective (Deng & Yin, 2016).",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Each step inside ADMM is simply a projection to a simplex, hence costing Õ(n) computations (Duchi et al., 2008).
",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"In terms of optimization on Q, since no explicit rates of convergence are available for the projected Quasi-Newton algorithm (Schmidt et al., 2009) that finely characterize the dependency on the condition numbers, we simply illustrate the √ L/µ log 1 rate using Nesterov’s accelerated gradient algorithm (Nesterov, 2003), where L is the Lipschitz continuous constant of the gradient.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"In our case, L = 1m2λ ∑ k ∑m i=1",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"‖Xi,k‖ 2 F + 1/µ.
",3.3.3 CONVERGENCE PROPERTY,[0],[0]
Comparison with Structured SVM (SSVM),3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Conventional SSVMs for learning bipartite matchings have only O(1/ ) rates due to the lack of smoothness (Joachims et al., 2009; Teo et al., 2010).",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"If smoothing is added, then similar linear convergence rates can be achieved with similar condition numbers.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"However, it is noteworthy that at each iteration we need to apply ADMM to solve a projection problem to the doubly stochastic matrix set (Eq. (9)), while SSVMs (without smoothing) solves a matching problem with the Hungarian algorithm, incurring O(n3) time.",3.3.3 CONVERGENCE PROPERTY,[0],[0]
"Despite its apparent differences from standard empirical risk minimization (ERM), adversarial loss minimization (Eq. (3)) can be equivalently recast as an ERM:
min θ E x∼P π|x∼P̃
[ ALpermfθ (x, π) ] where ALpermfθ (x, π) ,
min P̂ (π̂|x) max P̌ (π̌|x) Eπ̂|x∼P̂ π̌|x∼P̌
[ loss(π̂, π̌) + fθ(x, π̌)− fθ(x, π) ]
and fθ(x, π) = θ · ∑n i=1",3.4 Consistency Analysis,[0],[0]
"φ(x, πi) is the Lagrangian potential function.",3.4 Consistency Analysis,[0],[0]
"Here we consider fθ as the linear discriminant function for a proposed permutation π, using parameter value θ.",3.4 Consistency Analysis,[0],[0]
"ALpermfθ (x, π) is then the surrogate loss for input x and permutation π.
",3.4 Consistency Analysis,[0],[0]
"As described in Section 2.2, Fisher consistency is an important property for a surrogate loss L. It requires that under the true distribution P (x, π), the hypothesis that minimizes L is Bayes optimal (Tewari & Bartlett, 2007; Liu,
2007).",3.4 Consistency Analysis,[0],[0]
"For the cases of multiclass classification and ordinal regression, Fisher consistency for adversarial surrogate loss has been established by Fathony et al. (2016; 2017).",3.4 Consistency Analysis,[0],[0]
"In our setting, the Fisher consistency ofALpermf can be written as:
f∗ ∈ F∗ , argmin f
Eπ|x∼P [ ALpermf (x, π) ] (16)
⇒ argmax π f∗(x, π) ⊆ Π , argmin π Eπ̄|x∼P",3.4 Consistency Analysis,[0],[0]
"[loss(π, π̄)].
",3.4 Consistency Analysis,[0],[0]
Note that in Eq.,3.4 Consistency Analysis,[0],[0]
"(16) we allow f to be optimized over the set of all measurable functions on the input space (x, π).",3.4 Consistency Analysis,[0],[0]
"In our formulation, we have restricted f to be additively decomposable over individual elements of permutation, f(x, π) = ∑ i gi(x, πi).",3.4 Consistency Analysis,[0],[0]
"In the sequel, we will show that the condition in Eq.",3.4 Consistency Analysis,[0],[0]
"(16) also holds for this restricted set provided that g is allowed to be optimized over the set of all measurable functions on the space of individual input (x, πi).",3.4 Consistency Analysis,[0],[0]
"We start by establishing Fisher consistency for the case of singleton loss minimizing sets Π in Theorem 1 and then for more general cases in Theorem 2.
Theorem 1.",3.4 Consistency Analysis,[0],[0]
"Suppose loss(π, π̄) =",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) (symmetry) and loss(π, π) <",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) for all π̄ 6= π.",3.4 Consistency Analysis,[0],[0]
"Then the adversarial permutation loss ALpermf is Fisher consistent if f is over all measurable functions and Π is a singleton.
",3.4 Consistency Analysis,[0],[0]
Theorem 2.,3.4 Consistency Analysis,[0],[0]
"Suppose loss(π, π̄) = loss(π̄, π) (symmetry) and loss(π, π) <",3.4 Consistency Analysis,[0],[0]
"loss(π̄, π) for all π̄ 6= π.",3.4 Consistency Analysis,[0],[0]
"Furthermore if f is over all measurable functions, then:
(a) there exists f∗ ∈ F∗ such that argmaxπ f∗(x, π) ⊆ Π (i.e., satisfies the Fisher consistency requirement).",3.4 Consistency Analysis,[0],[0]
"In fact, all elements in Π can be recovered by some f∗ ∈ F∗ .
",3.4 Consistency Analysis,[0],[0]
"(b) if argminπ ∑ π′∈Π απ′ loss(π
′, π) ⊆ Π for all α(·) ≥ 0; ∑ π′∈Π απ′ = 1, then argmaxπ f ∗(x, π) ⊆ Π for all f∗ ∈",3.4 Consistency Analysis,[0],[0]
F∗.,3.4 Consistency Analysis,[0],[0]
"In this case, all f∗ ∈ F∗ satisfy the Fisher consistency requirement.
",3.4 Consistency Analysis,[0],[0]
"These assumptions of loss functions in the theorems above are quite mild, requiring only that wrong predictions suffer higher loss than correct ones.",3.4 Consistency Analysis,[0],[0]
We refer the reader to Appendix B for the detailed proofs of theorems.,3.4 Consistency Analysis,[0],[0]
"The key to the proofs is the observation that for the optimal potential function f∗, f∗(x, π) + loss(π, π ) is invariant to π when Π = {π }.",3.4 Consistency Analysis,[0],[0]
We refer to this as the loss reflective property.,3.4 Consistency Analysis,[0],[0]
"Note that this generalizes the observation for the case of ordinal regression loss (Fathony et al., 2017) into matching loss functions, subject to the mild pre-conditions assumed by the theorem.
",3.4 Consistency Analysis,[0],[0]
Theorem 3.,3.4 Consistency Analysis,[0],[0]
"Suppose the loss is Hamming loss, and the potential function f(x, π) decomposes additively by∑ i gi(x, πi).",3.4 Consistency Analysis,[0],[0]
"Then, the adversarial permutation loss ALpermf is Fisher consistent provided that gi is allowed to
be optimized over the set of all measurable functions on the space of individual inputs (x, πi).
",3.4 Consistency Analysis,[0],[0]
Proof.,3.4 Consistency Analysis,[0],[0]
"Simply choose gi such that for each sample x in the population, gi(x, πi) = −(πi 6= π i ).",3.4 Consistency Analysis,[0],[0]
This renders the loss reflective property under the Hamming loss.,3.4 Consistency Analysis,[0],[0]
"To evaluate our approach, we apply our adversarial bipartite matching model to video tracking tasks using public benchmark datasets (Leal-Taixé et al., 2015).",4 Experimental Evaluation,[0],[0]
"In this problem, we are given a set of images (video frames) and a list of objects in each image.",4 Experimental Evaluation,[0],[0]
We are also given the correspondence matching between objects in frame t and objects in frame t + 1.,4 Experimental Evaluation,[0],[0]
Figure 2 shows an example of the problem setup.,4 Experimental Evaluation,[0],[0]
It is important to note that the number of objects are not the same in every frames.,4 Experimental Evaluation,[0],[0]
"Some of the objects may enter, leave, or remain in the consecutive frames.",4 Experimental Evaluation,[0],[0]
"To handle the this issue, we setup our experiment as follows.",4 Experimental Evaluation,[0],[0]
"Let kt be the number of objects in frame t and k∗ be the maximum number of objects a frame can have, i.e., k∗ = maxt∈T kt.",4 Experimental Evaluation,[0],[0]
"Starting from k∗ nodes to represent the objects, we add k∗ more nodes as “invisible” nodes to allow new objects to enter and existing objects to leave.",4 Experimental Evaluation,[0],[0]
"As a result, the total number of nodes in each frame doubles to n = 2k∗.",4 Experimental Evaluation,[0],[0]
"We define the features for pairs of bounding boxes (i.e., φi(x, j) for pairing bounding box i with bounding box j) in two consecutive video frames so that we can compute the associative feature vectors, φ(x, π) = ∑n i=1",4.1 Feature Representation,[0],[0]
"φi(x, πi),",4.1 Feature Representation,[0],[0]
for each possible matching π.,4.1 Feature Representation,[0],[0]
"To define the feature vector φi(·, ·), we follow the feature representation reported by Kim et al. (2012) using six different types of features:
• Intersection over union (IoU) overlap ratio between bounding boxes, area(BBti ∩ BBt+1j )/",4.1 Feature Representation,[0],[0]
"area(BBti ∪ BBt+1j ), where BB t i denotes the bounding
box of object i at time frame t; • Euclidean distance between object centers; • 21 color histogram distance features (RGB) from
the Bhattacharyaa distance, 14 ln ( 1 4 (σ2p σ2q + σ2q σ2p + 2 ))",4.1 Feature Representation,[0],[0]
"+
We explain this feature representation in more detail in Appendix C.",4.1 Feature Representation,[0],[0]
"We compare our approach with the Structured SVM (SSVM) model (Taskar et al., 2005a; Tsochantaridis et al., 2005) implemented based on Kim et al. (2012) using SVM-Struct (Joachims, 2008; Vedaldi, 2011).",4.2 Experimental Setup,[0],[0]
"We implement our marginal version of adversarial bipartite matching using minConf (Schmidt, 2008) for performing projected Quasi-Newton optimization.
",4.2 Experimental Setup,[0],[0]
We consider two different groups of datasets in our experiment: TUD datasets and ETH datasets.,4.2 Experimental Setup,[0],[0]
"Each dataset contains different numbers of elements (i.e., the number of pedestrian bounding box in the frame plus the number of extra nodes to indicate entering or leaving) and different numbers of examples (i.e., pairs of two consecutive frames that we want to match).",4.2 Experimental Setup,[0],[0]
"Table 3 contains the detailed information about the datasets.
",4.2 Experimental Setup,[0],[0]
"To avoid having test examples that are too similar with the training set, we train the models on one dataset and test the model on another dataset that has similar characteristics.",4.2 Experimental Setup,[0],[0]
"In particular, we perform evaluations for every pair of datasets in TUD and ETH collections.",4.2 Experimental Setup,[0],[0]
"This results in eight pairs of training/test datasets, as shown in Table 4.
",4.2 Experimental Setup,[0],[0]
"To tune the regularization parameter (λ in adversarial matching, and C in SSVM), we perform 5-fold cross validation based on the training dataset only.",4.2 Experimental Setup,[0],[0]
"The resulting best regularization parameter is used to train the model over all training examples to obtain parameters θ, which we then use to predict the matching for the testing data.",4.2 Experimental Setup,[0],[0]
"For SSVM and the marginal version of adversarial matching, the pre-
diction is done by finding the bipartite matching that maximizes the potential value, i.e., argmaxY 〈Y, ∑ k θkXk〉 which can be solved using the Hungarian algorithm.",4.2 Experimental Setup,[0],[0]
The double oracle version of adversarial matching makes predictions by finding the most likely permutation from the predictor’s strategy in the equilibrium.,4.2 Experimental Setup,[0],[0]
"We report the average accuracy, which in this case is defined as (1 − the average Hamming loss) over all examples in the testing dataset.",4.3 Results,[0],[0]
Table 4 shows the mean and the standard deviation of our metric across different dataset pairs.,4.3 Results,[0],[0]
We report the results for both the double-oracle (DO) and marginal (MARG) versions of the adversarial model.,4.3 Results,[0],[0]
Our experiment indicates that both methods result in very similar values of θ.,4.3 Results,[0],[0]
The slight advantage of the double-oracle version is caused by the difference in prediction techniques between the double-oracle (argmax over predictor’s equilibrium strategy) and marginal version (argmax over potentials).,4.3 Results,[0],[0]
We also observe that the double-oracle approach requires only a small number of augmenting permutations to converge as shown in the last column (the average number of permutations) of Table 4.,4.3 Results,[0],[0]
"This indicates the sparseness of the set of permutations that support the equilibrium.
",4.3 Results,[0],[0]
"To compare with SSVM, we highlight (using bold font) the cases in which our result is better with statistical significance (under paired t-test with α < 0.05) in Table 4.",4.3 Results,[0],[0]
"Compared with SSVM, our proposed adversarial matching outperforms SSVM in all pairs of datasets—with statistical
significance on all six pairs of the ETH datasets and slightly better than SSVM on the TUD datasets.",4.3 Results,[0],[0]
"This suggests that our adversarial bipartite matching model benefits from its Fisher consistency property.
",4.3 Results,[0],[0]
"In terms of the running time, Table 5 shows that the marginal version of adversarial method is relatively fast.",4.3 Results,[0],[0]
"It only takes a few seconds to train until convergence in the case of 50 examples, with the number of elements varied up to 34.",4.3 Results,[0],[0]
"The running time grows roughly quadratically in the number of elements, which is natural since the size of the marginal probability matrices P and Q also grow quadratically in the number of elements.",4.3 Results,[0],[0]
"This shows that our approach is much more efficient than the CRF approach, which has a running time that is impractical even for small problems with 20 elements.",4.3 Results,[0],[0]
"The training time of SSVM is faster than the adversarial methods due to two different factors: (1) the inner optimization of SSVM can be solved using a single execution of the Hungarian algorithm compared with the inner optimization of adversarial method which requires ADMM optimization for projection to doubly stochastic matrix set; (2) different tools for implementation, i.e., C++ for SSVM and MATLAB for our method, which benefits the running time of SSVM.",4.3 Results,[0],[0]
"In addition, though the game size is relatively small, as indicated by the final column in Table 4, the double oracle version of adversarial method takes much longer to train compared to the marginal version.",4.3 Results,[0],[0]
"In this paper, we have presented an adversarial approach for learning bipartite matchings that is not only computationally efficient to employ but also provides Fisher consistency guarantees.",5 Conclusions and Future Work,[0],[0]
We showed that these theoretical advantages translate into better empirical performance for our model compared with previous approaches.,5 Conclusions and Future Work,[0],[0]
Our future work will explore matching problems with different loss functions and other graphical structures.,5 Conclusions and Future Work,[0],[0]
We thank our anonymous reviewers for their useful feedback and suggestions.,Acknowledgements,[0],[0]
This research was supported in part by NSF Grants RI-#1526379 and CAREER-#1652530.,Acknowledgements,[0],[0]
"Many important structured prediction problems, including learning to rank items, correspondence-based natural language processing, and multi-object tracking, can be formulated as weighted bipartite matching optimizations.",abstractText,[0],[0]
Existing structured prediction approaches have significant drawbacks when applied under the constraints of perfect bipartite matchings.,abstractText,[0],[0]
"Exponential family probabilistic models, such as the conditional random field (CRF), provide statistical consistency guarantees, but suffer computationally from the need to compute the normalization term of its distribution over matchings, which is a #P-hard matrix permanent computation.",abstractText,[0],[0]
"In contrast, the structured support vector machine (SSVM) provides computational efficiency, but lacks Fisher consistency, meaning that there are distributions of data for which it cannot learn the optimal matching even under ideal learning conditions (i.e., given the true distribution and selecting from all measurable potential functions).",abstractText,[0],[0]
We propose adversarial bipartite matching to avoid both of these limitations.,abstractText,[0],[0]
"We develop this approach algorithmically, establish its computational efficiency and Fisher consistency properties, and apply it to matching problems that demonstrate its empirical benefits.",abstractText,[0],[0]
Efficient and Consistent Adversarial Bipartite Matching,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1488–1498, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Knowledge bases (KBs), such as Freebase (Bollacker et al., 2008), NELL (Mitchell et al., 2015), and DBPedia (Mendes et al., 2012) contain large collections of facts about things, people, and places in the world.",1 Introduction,[0],[0]
"These knowledge bases are useful for various tasks, including training relation extractors and semantic parsers (Hoffmann et al., 2011; Krishnamurthy and Mitchell, 2012), and question answering (Berant et al., 2013).",1 Introduction,[0],[0]
"While these knowledge bases may be very large, they are still quite incomplete, missing large percentages of facts about common or popular entities (West et al., 2014; Choi et al., 2015).",1 Introduction,[0],[0]
"The task of knowledge base completion—filling in missing facts by
examining the facts already in the KB, or by looking in a corpus—is one attempt to mitigate the problems of this knowledge sparsity.
",1 Introduction,[0],[0]
"In this work we examine one method for performing knowledge base completion that is currently in use: the Path Ranking Algorithm (PRA) (Lao et al., 2011; Dong et al., 2014).",1 Introduction,[0],[0]
PRA is a method for performing link prediction in a graph with labeled edges by computing feature matrices over node pairs in the graph.,1 Introduction,[0],[0]
"The method has a strong connection to logical inference (Gardner et al., 2015), as the feature space considered by PRA consists of a restricted class of horn clauses found in the graph.",1 Introduction,[0],[0]
"While PRA can be applied to any link prediction task in a graph, its primary use has been in KB completion (Lao et al., 2012; Gardner et al., 2013; Gardner et al., 2014).
",1 Introduction,[0],[0]
"PRA is a two-step process, where the first step finds potential path types between node pairs to use as features in a statistical model, and the second step computes random walk probabilities associated with each path type and node pair (these are the values in a feature matrix).",1 Introduction,[0],[0]
"This second step is very computationally intensive, requiring time proportional to the average out-degree of the graph to the power of the path length for each cell in the computed feature matrix.",1 Introduction,[0],[0]
"In this paper we consider whether this computational effort is wellspent, or whether we might more profitably spend computation in other ways.",1 Introduction,[0],[0]
"We propose a new way of generating feature matrices over node pairs in a graph that aims to improve both the efficiency and the expressivity of the model relative to PRA.
",1 Introduction,[0],[0]
"Our technique, which we call subgraph feature extraction (SFE), is similar to only doing the first step of PRA.",1 Introduction,[0],[0]
"Given a set of node pairs in a graph, we first do a local search to characterize the graph around each node.",1 Introduction,[0],[0]
We then run a set of feature extractors over these local subgraphs to obtain feature vectors for each node pair.,1 Introduction,[0],[0]
"In the simplest case, where the feature extractors only
1488
look for paths connecting the two nodes, the feature space is equivalent to PRA’s, and this is the same as running PRA and binarizing the resultant feature vectors.",1 Introduction,[0],[0]
"However, because we do not have to compute random walk probabilities associated with each path type in the feature matrix, we can extract much more expressive features, including features which are not representable as paths in the graph at all.",1 Introduction,[0],[0]
"In addition, we can do a more exhaustive search to characterize the local graph, using a breadth-first search instead of random walks.",1 Introduction,[0],[0]
SFE is a much simpler method than PRA for obtaining feature matrices over node pairs in a graph.,1 Introduction,[0],[0]
"Despite its simplicity, however, we show experimentally that it substantially outperforms PRA, both in terms of running time and prediction performance.",1 Introduction,[0],[0]
"SFE decreases running time over PRA by an order of magnitude, it improves mean average precision from .432 to .528 on the NELL KB, and it improves mean reciprocal rank from .850 to .933.
",1 Introduction,[0],[0]
"In the remainder of this paper, we first describe PRA in more detail.",1 Introduction,[0],[0]
"We then situate our methods in the context of related work, and provide additional experimental motivation for the improvements described in this paper.",1 Introduction,[0],[0]
"We then formally define SFE and the feature extractors we used, and finally we present an experimental comparison between PRA and SFE on the NELL KB.",1 Introduction,[0],[0]
The code and data used in this paper is available at http://rtw.ml.cmu.edu/emnlp2015 sfe/.,1 Introduction,[0],[0]
The path ranking algorithm was introduced by Lao and Cohen (2010).,2 The Path Ranking Algorithm,[0],[0]
It is a two-step process for generating a feature matrix over node pairs in a graph.,2 The Path Ranking Algorithm,[0],[0]
"The first step finds a set of potentially useful path types that connect the node pairs, which become the columns of the feature matrix.",2 The Path Ranking Algorithm,[0],[0]
The second step then computes the values in the feature matrix by finding random walk probabilities as described below.,2 The Path Ranking Algorithm,[0],[0]
"Once the feature matrix has been computed, it can be used with whatever classification model is desired (or even incorporated as one of many factors in a structured prediction model), though almost all prior work with PRA simply uses logistic regression.
",2 The Path Ranking Algorithm,[0],[0]
"More formally, consider a graph G with nodes N , edges E , and edge labels R, and a set of node pairs (sj , tj) ∈ D that are instances of some relationship of interest.",2 The Path Ranking Algorithm,[0],[0]
"PRA will generate a feature vector for each (sj , tj) pair, where each feature is
some sequence of edge labels -e1-e2-. .",2 The Path Ranking Algorithm,[0],[0]
.-el-.,2 The Path Ranking Algorithm,[0],[0]
"If the edge sequence, or path type, corresponding to the feature exists between the source and target nodes in the graph, the value of that feature in the feature vector will be non-zero.
",2 The Path Ranking Algorithm,[0],[0]
"Because the feature space considered by PRA is so large,1 and because computing the feature values is so computationally intensive, the first step PRA must perform is feature selection, which is done using random walks over the graph.",2 The Path Ranking Algorithm,[0],[0]
"In this step of PRA, we find path types π that are likely to be useful in predicting new instances of the relation represented by the input node pairs.",2 The Path Ranking Algorithm,[0],[0]
These path types are found by performing random walks on the graph G starting at the source and target nodes in D and recording which paths connect some source node with its target.2,2 The Path Ranking Algorithm,[0],[0]
"Note that these are two-sided, unconstrained random walks: the walks from sources and targets can be joined on intermediate nodes to get a larger set of paths that connect the source and target nodes.",2 The Path Ranking Algorithm,[0],[0]
"Once connectivity statistics have been computed in this way, k path types are selected as features.",2 The Path Ranking Algorithm,[0],[0]
"Lao et al. (2011) use measures of the precision and recall of each feature in this selection, while Gardner et al. (2014) simply pick those most frequently seen.
",2 The Path Ranking Algorithm,[0],[0]
"Once a set of path features has been selected, the second step of PRA is to compute values for each cell in the feature matrix.",2 The Path Ranking Algorithm,[0],[0]
"Recall that rows in this matrix correspond to node pairs, and the columns correspond to the path types found in the first step.",2 The Path Ranking Algorithm,[0],[0]
"The cell value assigned by PRA is the probability of arriving at the target node of a node pair, given that a random walk began at the source node and was constrained to follow the path type: p(t|s, π).",2 The Path Ranking Algorithm,[0],[0]
There are several ways of computing this probability.,2 The Path Ranking Algorithm,[0],[0]
"The most straightforward method is to use a path-constrained breadth-first search to exhaustively enumerate all possible targets given a source node and a path type, count how frequently each target is seen, and normalize the distribution.",2 The Path Ranking Algorithm,[0],[0]
"This calculates the desired probability exactly, but at the cost of doing a breadth-first search (with
1The feature space consists of the set of all possible edge label sequences, with cardinality ∑l i=1
|R|i, assuming a bound l on the maximum path length.
",2 The Path Ranking Algorithm,[0],[0]
"2A deterministic algorithm, such as a breadth-first search, could obviously be used here instead of random walks, and indeed Lao’s original work did use a more exhaustive search.",2 The Path Ranking Algorithm,[0],[0]
"However, when moving to the larger graphs corresponding to the NELL and Freebase KBs, Lao (2011) (and all future work) switched to using random walks, because the graph was too large.
complexity proportional to the average per-edgelabel out-degree to the power of the path length) per source node per path type.
",2 The Path Ranking Algorithm,[0],[0]
There are three methods that can potentially reduce the computational complexity of this probability calculation.,2 The Path Ranking Algorithm,[0],[0]
"The first is to use random walks to approximate the probability via rejection sampling: for each path type and source node, a number of random walks are performed, attempting to follow the edge sequence corresponding to the path type.",2 The Path Ranking Algorithm,[0],[0]
"If a node is reached where it is no longer possible to follow the path type, the random walk is restarted.",2 The Path Ranking Algorithm,[0],[0]
"This does not reduce the time necessary to get an arbitrarily good approximation, but it does allow us to decrease computation time, even getting a fixed complexity, at the cost of accepting some error in our probability estimates.",2 The Path Ranking Algorithm,[0],[0]
"Second, Lao (2012) showed that when the target node of a query is known, the exponent can be cut in half by using a two-sided BFS.",2 The Path Ranking Algorithm,[0],[0]
"In this method, some careful bookkeeping is done with dynamic programming such that the probability can be computed correctly when the two-sided search meets at an intermediate node.",2 The Path Ranking Algorithm,[0],[0]
"Lao’s dynamic programming technique is only applicable when the target node is known, however, and only cuts the exponent in half—this is still quite computationally intensive.",2 The Path Ranking Algorithm,[0],[0]
"Lastly, we could replace the BFS with a multiplication of adjacency matrices, which performs the same computation.",2 The Path Ranking Algorithm,[0],[0]
"The efficiency gain comes from the fact that we can just do the multiplication once per path type, instead of once per path type per source node.",2 The Path Ranking Algorithm,[0],[0]
"However, to correctly compute the probabilities for a (source, target) pair, we need to exclude from the graph the edge connecting that training instance.",2 The Path Ranking Algorithm,[0],[0]
"This means that the matrix computed for each path type should be different for each training instance, and so we either lose our efficiency gain or we accept incorrect probability estimates.",2 The Path Ranking Algorithm,[0],[0]
"In this work we use the rejection sampling technique.
",2 The Path Ranking Algorithm,[0],[0]
"As mentioned above, once the feature matrix has been computed in the second step of PRA, one can use any kind of classifier desired to learn a model and make predictions on test data.",2 The Path Ranking Algorithm,[0],[0]
"The task of knowledge base completion has seen a lot of attention in recent years, with entire workshops devoted to it (Suchanek et al., 2013).",3 Related Work,[0],[0]
"We will touch on three broad categories related to KB
completion: the task of relation extraction, embedding methods for KB completion, and graph methods for KB completion.
",3 Related Work,[0],[0]
Relation extraction.,3 Related Work,[0],[0]
Relation extraction and knowledge base completion have the same goal: to predict new instances of relations in a formal knowledge base such as Freebase or NELL.,3 Related Work,[0],[0]
"The difference is that relation extraction focuses on determining what relationship is expressed by a particular sentence, while knowledge base completion tries to predict which relationships hold between which entities.",3 Related Work,[0],[0]
"A relation extraction system can be used for knowledge base completion, but typical KB completion methods do not make predictions on single sentences.",3 Related Work,[0],[0]
"This is easily seen in the line of work known as distantly-supervised relation extraction (Mintz et al., 2009; Hoffmann et al., 2011; Surdeanu et al., 2012); these models use the relation instances in a knowledge base as their only supervision, performing some heuristic mapping of the entities in text to the knowledge base, then using that mapping to train extractors for each relation in the KB.",3 Related Work,[0],[0]
"The cost of using these methods is that is it generally difficult to incorporate richer features from the knowledge base when predicting whether a particular sentence expresses a relation, and so techniques that make fuller use of the KB can perform better on the KB completion task (Weston et al., 2013; Riedel et al., 2013).
",3 Related Work,[0],[0]
Embedding methods for KB completion.,3 Related Work,[0],[0]
There has been much recent work that attempts to perform KB completion by learning an embedded representation of entities and relations in the KB and then using these representations to infer missing relationships.,3 Related Work,[0],[0]
"Some of earliest work along these lines were the RESCAL model (Nickel et al., 2011) and Structured Embeddings (Bordes et al., 2011).",3 Related Work,[0],[0]
"These were soon followed by TransE (Bordes et al., 2013), Neural Tensor Networks (Socher et al., 2013), and many variants on all of these algorithms (Chang et al., 2014; Garcı́a-Durán et al., 2014; Wang et al., 2014).",3 Related Work,[0],[0]
"These methods perform well when there is structural redundancy in the knowledge base tensor, but when the tensor (or individual relations in the tensor) has high rank, learning good embeddings can be challenging.",3 Related Work,[0],[0]
"The ARE model (Nickel et al., 2014) attempted to address this by only making the embeddings capture the residual of the tensor that cannot be readily predicted from the graph-based techniques mentioned below.
",3 Related Work,[0],[0]
Graph-based methods for KB completion.,3 Related Work,[0],[0]
A separate line of research into KB completion can be broadly construed as performing some kind of inference over graphs in order to predict missing instances in a knowledge base.,3 Related Work,[0],[0]
"Markov logic networks (Richardson and Domingos, 2006) fall into this category, as does ProPPR (Wang et al., 2013) and many other logic-based systems.",3 Related Work,[0],[0]
"PRA, the main subject of this paper, also fits in this line of work.",3 Related Work,[0],[0]
"Work specifically with PRA has ranged from incorporating a parsed corpus as additional evidence when doing random walk inference (Lao et al., 2012), to introducing better representations of the text corpus (Gardner et al., 2013; Gardner et al., 2014), and using PRA in a broader context as part of Google’s Knowledge Vault (Dong et al., 2014).",3 Related Work,[0],[0]
"An interesting piece of work that combines embedding methods with graph-based methods is that of Neelakantan et al. (2015), which uses a recursive neural network to create embedded representations of PRA-style paths.",3 Related Work,[0],[0]
We motivate our modifications to PRA with three observations.,4 Motivation,[0],[0]
"First, it appears that binarizing the feature matrix produced by PRA, removing most of the information gained in PRA’s second step, has no significant impact on prediction performance in knowledge base completion tasks.",4 Motivation,[0],[0]
"We show this on the NELL KB and the Freebase KB in
3The NELL data and experimental protocol is described in Section 6.1.",4 Motivation,[0],[0]
"The Freebase data consists of 24 relations from the Freebase KB; we used the same data used by Gardner et al. (2014).
",4 Motivation,[0],[0]
"periments showing a substantial increase in performance from using a much larger set of features in a PRA-like model.4 All of their experiments used binary features, so this is not a direct comparison of random walk probabilities versus binarized features, but it shows that increasing the feature size beyond the point that is computationally feasible with random walk probabilities seems useful.",4 Motivation,[0],[0]
"Additionally, they showed that using path bigram features, where each sequential pair of edges types in each path was added as an additional feature to the model, gave a significant increase in performance.",4 Motivation,[0],[0]
"These kind of features are not representable in the traditional formulation of PRA.
",4 Motivation,[0],[0]
"Lastly, the method used to compute the random walk probabilities—rejection sampling—makes the inclusion of more expressive features problematic.",4 Motivation,[0],[0]
"Consider the path bigrams mentioned above; one could conceivably compute a probability for a path type that only specifies that the last edge type in the path must be r, but it would be incredibly inefficient with rejection sampling, as most of the samples would end up rejected (leaving aside the additional issues of an unspecified path length).",4 Motivation,[0],[0]
"In contrast, if the features simply signify whether a particular path type exists in the graph, without any associated probability, these kinds of features are very easy to compute.
",4 Motivation,[0],[0]
"Given this motivation, our work attempts to improve both the efficiency and the expressivity of PRA by removing the second step of the algorithm.",4 Motivation,[0],[0]
"Efficiency is improved because the second step is the most computationally expensive, and expressivity is improved by allowing features that cannot be reasonably computed with rejection sampling.",4 Motivation,[0],[0]
We show experimentally that the techniques we introduce do indeed improve performance quite substantially.,4 Motivation,[0],[0]
In this section we discuss how SFE constructs feature matrices over node pairs in a graph using just a single search over the graph for each node (which is comparable to only using the first step of PRA).,5 Subgraph Feature Extraction,[0],[0]
"As outlined in Section 2, the first step of PRA does a series of random walks from each source and target node (sj , tj) in a dataset D. In
4Note that while Neelakantan et al. called the baseline they were comparing to “PRA”, they only used the first step of the algorithm to produce path types, and thus did not really compare against PRA per se.",5 Subgraph Feature Extraction,[0],[0]
"It is their version of “PRA” that we formalize and expand as SFE in this work.
",5 Subgraph Feature Extraction,[0],[0]
"PRA these random walks are used to find a relatively small set of potentially useful path types for which more specific random walk probabilities are then computed, at great expense.",5 Subgraph Feature Extraction,[0],[0]
"In our method, subgraph feature extraction (SFE), we stop after this first set of random walks and instead construct a binary feature matrix.
",5 Subgraph Feature Extraction,[0],[0]
"More formally, for each node n in the data (where n could be either a source node or a target node), SFE constructs a subgraph centered around that node using k random walks.",5 Subgraph Feature Extraction,[0],[0]
"Each random walk that leaves n follows some path type π and ends at some intermediate node i. We keep all of these (π, i) pairs as the characterization of the subgraph around n, and we will refer to this subgraph as Gn.",5 Subgraph Feature Extraction,[0],[0]
"To construct a feature vector for a source-target pair (sj , tj), SFE takes the subgraphs Gsj and Gtj and merges them on the intermediate nodes i. That is, if an intermediate node i is present in both Gsj and Gtj , SFE takes the path types π corresponding to i and combines them (reversing the path type coming from the target node tj).",5 Subgraph Feature Extraction,[0],[0]
"If some intermediate node for the source sj happens to be tj , no combination of path types is necessary (and similarly if an intermediate node for the target tj is sj—the path only needs to be reversed in this case).",5 Subgraph Feature Extraction,[0],[0]
This creates a feature space that is exactly the same as that constructed by PRA: sequences of edge types that connect a source node to a target node.,5 Subgraph Feature Extraction,[0],[0]
"To construct the feature vector SFE just takes all of these combined path types as binary features for (sj , tj).",5 Subgraph Feature Extraction,[0],[0]
"Note, however, that we need not restrict ourselves to only using the same feature space as PRA; Section 5.1 will examine extracting more expressive features from these subgraphs.
",5 Subgraph Feature Extraction,[0],[0]
"This method for generating a feature matrix over node pairs in a graph is much simpler and less computationally expensive than PRA, and from looking at Table 1 we would expect that it would perform on par with PRA with drastically reduced computation costs.",5 Subgraph Feature Extraction,[0],[0]
Some experimentation shows that it is not that simple.,5 Subgraph Feature Extraction,[0],[0]
"Table 2 shows a comparison between PRA and SFE on 10 NELL relations.5 SFE has a higher mean average precision, but the difference is not statistically significant.",5 Subgraph Feature Extraction,[0],[0]
"There is a large variance in SFE’s performance, and on some relations PRA performs better.
",5 Subgraph Feature Extraction,[0],[0]
We examined the feature matrices computed 5The data and evaluation methods are described more fully in Section 6.1.,5 Subgraph Feature Extraction,[0],[0]
"These experiments were conducted on a different development split of the same data.
by these methods and discovered that the reason for the inconsistency of SFE’s improvement is because its random walks are all unconstrained.",5 Subgraph Feature Extraction,[0],[0]
"Consider the case of a node with a very high degree, say 1000.",5 Subgraph Feature Extraction,[0],[0]
"If we only do 200 random walks from this node, we cannot possibly get a complete characterization of the graph even one step away from the node.",5 Subgraph Feature Extraction,[0],[0]
"If a particularly informative path is <CITYINSTATE, STATEINCOUNTRY>, and both the city from which a random walk starts and the intermediate state node have very high degree, the probability of actually finding this path type using unconstrained random walks is quite low.",5 Subgraph Feature Extraction,[0],[0]
This is the benefit gained by the path-constrained random walks performed by PRA; PRA leverages training instances with relatively low degree and aggregation across a large number of instances to find path types that are potentially useful.,5 Subgraph Feature Extraction,[0],[0]
"Once they are found, significant computational effort goes into discovering whether each path type exists for all (s, t) pairs.",5 Subgraph Feature Extraction,[0],[0]
"It is this computational effort that allows the path type <CITYINSTATE, STATEINCOUNTRY> to have a non-zero value even for very highly connected nodes.
",5 Subgraph Feature Extraction,[0],[0]
"How do we mitigate this issue, so that SFE can consistently find these path types?",5 Subgraph Feature Extraction,[0],[0]
It seems the only option without resorting to a similar two-step process to what PRA uses is to do a more exhaustive search.,5 Subgraph Feature Extraction,[0],[0]
"PRA uses random walks to improve scalability on very large graphs, particularly because the second step of the algorithm is so expensive.",5 Subgraph Feature Extraction,[0],[0]
"However, if we are only doing a single search, and the graph fits in memory, a few steps of a breadth-first search (BFS) per node is not infeasible.",5 Subgraph Feature Extraction,[0],[0]
We can make the BFS more tractable by excluding edge types whose fan out is too high.,5 Subgraph Feature Extraction,[0],[0]
"For example, at a type node in Freebase, there could be thousands of edges of type /TYPE/OBJECT/TYPE; if there are a large number of edges of the same type leaving a node, we do not include those edges in the BFS.",5 Subgraph Feature Extraction,[0],[0]
"Note that because the type node will still be counted as an intermediate node in the subgraph, we can still find paths that go through that
node; we just do not continue searching if the outdegree of a particular edge type is too high.
",5 Subgraph Feature Extraction,[0],[0]
"When using a BFS instead of random walks to obtain the subgraphs Gsj and Gtj for each node pair, we saw a dramatic increase in the number of path type features found and a substantial increase in performance.6 These results are shown in Table 3; SFE-RW is our SFE implementation using random walks, and SFE-BFS uses a BFS.",5 Subgraph Feature Extraction,[0],[0]
The description above shows how to recreate the feature space used by PRA using our simpler subgraph feature extraction technique.,5.1 More expressive features,[0],[0]
"As we have mentioned, however, we need not restrict ourselves to merely recreating PRA’s feature space.",5.1 More expressive features,[0],[0]
"Eliminating random walk probabilities allows us to extract a much richer set of features from the subgraphs around each node, and here we present the feature extractors we have experimented with.",5.1 More expressive features,[0],[0]
"Figure 1 contains an example graph that we will refer to when describing these features.
",5.1 More expressive features,[0],[0]
PRA-style features.,5.1 More expressive features,[0],[0]
"We explained these features in Section 5, but we repeat them here for consistency, and use the example to make the feature extraction process more clear.",5.1 More expressive features,[0],[0]
"Relying on the notation introduced earlier, these features are generated by intersecting the subgraphs Gs and Gt on the intermediate nodes.",5.1 More expressive features,[0],[0]
"That is, when the subgraphs share an intermediate node, we combine the path types found from the source and target to that node.",5.1 More expressive features,[0],[0]
"In the example in Figure 1, there are two common intermediate nodes (“Barack Obama” and “Michelle Obama”), and combining the path types corresponding to those nodes gives the same path type: -ALIAS-“is married to”-ALIAS-1-.
",5.1 More expressive features,[0],[0]
Path bigram features.,5.1 More expressive features,[0],[0]
"In Section 4, we mentioned that Neelakantan et al. (2015) experimented with using path bigrams as features.",5.1 More expressive features,[0],[0]
"We
6One should not read too much into the decrease in running time between SFE-RW and SFE-BFS, however, as it was mostly an implementation detail.
include those features here as well.",5.1 More expressive features,[0],[0]
"For any path π between a source node s and a target node t, we create a feature for each relation bigram in the path type.",5.1 More expressive features,[0],[0]
"In the example in Figure 1, this would result in the features “BIGRAM:@START@-ALIAS”, “BIGRAM:ALIAS-is married to”, “BIGRAM:is married to-ALIAS”, and “BIGRAM:ALIAS@END@”.
",5.1 More expressive features,[0],[0]
One-sided features.,5.1 More expressive features,[0],[0]
"We use one-sided path to describe a sequence of edges that starts at a source or target node in the data, but does not necessarily terminate at a corresponding target or source node, as PRA features do.",5.1 More expressive features,[0],[0]
"Following the notation introduced in Section 5, we use as features each (π, i) pair in the subgraph characterizations Gs and Gt, along with whether the feature came from the source node or the target node.",5.1 More expressive features,[0],[0]
The motivation for these one-sided path types is to better model which sources and targets are good candidates for participating in a particular relation.,5.1 More expressive features,[0],[0]
"For example, not all cities participate in the relation CITYCAPITALOFCOUNTRY, even though the domain of the relation is all cities.",5.1 More expressive features,[0],[0]
"A city that has a large number of sports teams may be more likely to be a capital city, and these one-sided features could easily capture that kind of information.
",5.1 More expressive features,[0],[0]
"Example one-sided features from the example in Figure 1 would be “SOURCE:GENDER-:male”, “TARGET:-GENDER-:female”,
“SOURCE:-ALIAS-:Barack Obama”, and “SOURCE:-ALIAS-is married to-:Michelle Obama”.
",5.1 More expressive features,[0],[0]
One-sided feature comparisons.,5.1 More expressive features,[0],[0]
We can expand on the one-sided features introduced above by allowing for comparisons of these features in certain circumstances.,5.1 More expressive features,[0],[0]
"For example, if both the source and target nodes have an age or gender encoded in the graph, we might profitably use comparisons of these values to make better predictions.
",5.1 More expressive features,[0],[0]
"Drawing again on the notation from Section 5, we can formalize these features as analogous to the pairwise PRA features.",5.1 More expressive features,[0],[0]
"To get the PRA features, we intersect the intermediate nodes i from the subgraphs Gs and Gt, and combine the path types π when we find common intermediate nodes.",5.1 More expressive features,[0],[0]
"To get these comparison features, we instead intersect the subgraphs on the path types, and combine the intermediate nodes when there are common path types.",5.1 More expressive features,[0],[0]
"That is, if we see a common path type, such as -GENDER-, we will construct a feature representing a comparison between the intermediate node for the source and the target.",5.1 More expressive features,[0],[0]
"If the values are the same, this information can be captured with a PRA feature, but it cannot be easily captured by PRA when the values are different.
",5.1 More expressive features,[0],[0]
"In the example in Figure 1, there are two common path types: -ALIAS-, and -GENDER.",5.1 More expressive features,[0],[0]
"The feature generated from the path type - GENDER- would be “COMPARISON:-GENDER:/m/Male:/m/Female”.
",5.1 More expressive features,[0],[0]
Vector space similarity features.,5.1 More expressive features,[0],[0]
Gardner et al. (2014) introduced a modification of PRA’s random walks to incorporate vector space similarity between the relations in the graph.,5.1 More expressive features,[0],[0]
"On the data they were using, a graph that combined a formal knowledge base with textual relations extracted from text, they found that this technique gave a substantial performance improvement.",5.1 More expressive features,[0],[0]
"The vector space random walks only affected the second step of PRA, however, and we have removed that step in SFE.",5.1 More expressive features,[0],[0]
"While it is not as conceptually clean as the vector space random walks, we can obtain a similar effect with a simple feature transformation using the vectors for each relation.",5.1 More expressive features,[0],[0]
"We obtain vector representations of relations through factorization of the knowledge base tensor as did Gardner et al., and replace each edge type in a PRA-style path with edges that are similar to it in the vector space.",5.1 More expressive features,[0],[0]
"We also introduce a special “any edge” symbol, and say that all other edge types are simi-
lar to this edge type.",5.1 More expressive features,[0],[0]
"To reduce the combinatorial explosion of the feature space that this feature extractor creates, we only allow replacing one relation at a time with a similar relation.",5.1 More expressive features,[0],[0]
"In the example graph in Figure 1, and assuming that “spouse of” is found to be similar to “is married to”, some of the features extracted would be the following: “VECSIM:-ALIAS-is married toALIAS-”, “VECSIM:-ALIAS-spouse of-ALIAS-”, “VECSIM:-ALIAS-@ANY REL@-ALIAS-”, and “VECSIM:-@ANY REL@-is married to-ALIAS”.",5.1 More expressive features,[0],[0]
"Note that the first of those features, “VECSIM:ALIAS-is married to-ALIAS-”, is necessary even though it just duplicates the original PRA-style feature.",5.1 More expressive features,[0],[0]
"This allows path types with different but similar relations to generate the same features.
",5.1 More expressive features,[0],[0]
Any-Relation features.,5.1 More expressive features,[0],[0]
"It turns out that much of the benefit gained from Gardner et al.’s vector space similarity features came from allowing any path type that used a surface edge to match any other surface edge with non-zero probability.7 To test whether the vector space similarity features give us any benefit over just replacing relations with dummy symbols, we add a feature extractor that is identical to the one above, assuming an empty vector similarity mapping.",5.1 More expressive features,[0],[0]
"The features extracted from Figure 1 would thus be “ANYREL:-@ANY REL@is married to-ALIAS”, “ANYREL:-ALIAS@ANY REL@-ALIAS”, “ANYREL:-ALIAS-is married to-@ANY REL@”.",5.1 More expressive features,[0],[0]
"Here we present experimental results evaluating the feature extractors we presented, and a comparison between SFE and PRA.",6 Experiments,[0],[0]
"As we showed in Section 5 that using a breadth-first search to obtain subgraphs is superior to using random walks, all of the experiments presented here use the BFS implementation of SFE.",6 Experiments,[0],[0]
"To evaluate SFE and the feature extractors we introduced, we learned models for 10 relations in the NELL KB.",6.1 Data,[0],[0]
"We used the same data as Gardner et al. (2014), using both the formal KB relations and the surface relations extracted from text in our
7Replacing all surface edges with a single dummy relation gives performance close to vector space PRA.",6.1 Data,[0],[0]
"The vector space walks do statistically outperform this, but the extra gain is small.
graph.",6.1 Data,[0],[0]
We used logistic regression with elastic net (L1 and L2) regularization.,6.1 Data,[0],[0]
"We tuned the L1 and L2 parameters for each method on a random development split of the data, then used a new split of the data to run the final tests presented here.
",6.1 Data,[0],[0]
The evaluation metrics we use are mean average precision (MAP) and mean reciprocal rank (MRR).,6.1 Data,[0],[0]
"We judge statistical significance using a paired permutation test, where the average precision8 on each relation is used as paired data.",6.1 Data,[0],[0]
One important practical issue for most uses of PRA is the selection of negative examples for training a model.,6.2 On Obtaining Negative Evidence,[0],[0]
"Typically a knowledge base only contains positive examples of a relation, and it is not clear a priori what the best method is for obtaining negative evidence.",6.2 On Obtaining Negative Evidence,[0],[0]
"Prior work with PRA makes a closed world assumption, treating any (s, t) pair not seen in the knowledge base as a negative example.",6.2 On Obtaining Negative Evidence,[0],[0]
"Negative instances are selected when performing the second step of PRA—if a random walk from a source ends at a target that is not a known correct target for that source, that sourcetarget pair is used as a negative example.
",6.2 On Obtaining Negative Evidence,[0],[0]
"SFE only scores (source, target) pairs; it has no mechanism similar to PRA’s that will find potential targets given a source node.",6.2 On Obtaining Negative Evidence,[0],[0]
"We thus need a new way of finding negative examples, both at training time and at test time.",6.2 On Obtaining Negative Evidence,[0],[0]
"We used a simple technique to find negative examples from a graph given a set of positive examples, and we used this to obtain the training and testing data used in the experiments below.",6.2 On Obtaining Negative Evidence,[0],[0]
Our technique takes each source and target node in the given positive examples and finds other nodes in the same category that are close in terms of personalized page rank (PPR).,6.2 On Obtaining Negative Evidence,[0],[0]
"We then sample new (source, target) pairs from these lists of similar nodes, weighted by their PPR score (while also allowing the original source and target to be sampled).",6.2 On Obtaining Negative Evidence,[0],[0]
"These become our negative examples, both at training and at testing time.
",6.2 On Obtaining Negative Evidence,[0],[0]
"Because this is changing the negative evidence available to PRA at training time, we wanted to be sure we were not unfairly hindering PRA in our comparisons.",6.2 On Obtaining Negative Evidence,[0],[0]
"If it is in fact better to let PRA find its own negative examples at training time, instead of the ones sampled based on personalized page rank, then we should let PRA get its own nega-
8Average precision is equivalent to the area under a precision/recall curve.
tive evidence.",6.2 On Obtaining Negative Evidence,[0],[0]
We thus ran an experiment to see under which training regime PRA performs better.,6.2 On Obtaining Negative Evidence,[0],[0]
"We created a test set with both positive and negative examples as described in the paragraph above, and at training time we compared two techniques: (1) letting PRA find its own negative examples through its random walks, and (2) only using the negative examples selected by PPR.",6.2 On Obtaining Negative Evidence,[0],[0]
"As can be seen in Table 4, the difference between the two training conditions is very small, and it is not statistically significant.",6.2 On Obtaining Negative Evidence,[0],[0]
"Because there is no significant difference between the two conditions, in the experiments that follow we give both PRA and SFE the same training data, created through the PPR-based sampling technique described above.",6.2 On Obtaining Negative Evidence,[0],[0]
We first examine the effect of each of the feature types introduced in Section 5.1.,6.3 Results,[0],[0]
The results are shown in Table 5.,6.3 Results,[0],[0]
"We can see that, for this data, the comparisons and one-sided features did not improve performance (and the decreases are not statistically significant).",6.3 Results,[0],[0]
"Bigram features do appear to improve performance, though the improvement was not consistent enough across relations to achieve statistical significance.",6.3 Results,[0],[0]
"The vector similarity features do improve performance, with p-values hovering right at 0.05 when comparing against only PRA features and PRA + bigram features.",6.3 Results,[0],[0]
"The any rel features, however, do statistically improve over all other methods (p <= 0.01) except the PRA + vec sim result (p = .21).
",6.3 Results,[0],[0]
"Finally, we present a comparison between PRA, PRA with vector space random walks, and the best SFE result from the ablation study.",6.3 Results,[0],[0]
This is shown in Table 6.,6.3 Results,[0],[0]
"SFE significantly outperforms PRA, both with and without the vector space random walks presented by Gardner et al. (2014).",6.3 Results,[0],[0]
"When using only PRA-style features with SFE, the highest weighted features were almost always those of the form -ALIAS-[some textual relation]ALIAS-1-.",6.4 Discussion,[0],[0]
"For example, for the relation WRITER-
WROTEBOOK, the textual relations used in this feature might be “wrote”, “describes in”, “writes in”, and “expresses in”.",6.4 Discussion,[0],[0]
"These are the same feature types that PRA itself finds to have the highest weight, also, though SFE finds many more of them than PRA does, as PRA has to do aggressive feature selection.",6.4 Discussion,[0],[0]
"For this particular dataset, where the graph consists of edges from a formal KB mixed with edges from extracted textual relations, these kinds of features are by far the most useful, and most of the improvements seen by the additional feature types we used with SFE come from more compactly encoding these features.
",6.4 Discussion,[0],[0]
"For example, the path bigram features can encode the fact that there exists a path from the source to the target that begins or ends with an ALIAS edge.",6.4 Discussion,[0],[0]
"This captures in just two features all path types of the form -ALIAS-[some textual relation]-ALIAS-1-, and those two bigram features are almost always the highest weighted features in models where they are used.
",6.4 Discussion,[0],[0]
"However, the bigram features do not capture those path types exactly.",6.4 Discussion,[0],[0]
"The Any-Rel features were designed in part specifically for this path type, and they capture it exactly with a single feature.",6.4 Discussion,[0],[0]
"For all 10 relations, the feature “ANYREL:ALIAS-@ANY REL@-ALIAS-1” is the highest
weighted feature.",6.4 Discussion,[0],[0]
"This is because, for the relations we experimented with, knowing that some relationship is expressed in text between a particular pair of KB entities is a very strong indication of a single KB relation.",6.4 Discussion,[0],[0]
"There are only so many possible relationships between cities and countries, for instance.",6.4 Discussion,[0],[0]
"These features are much less informative between entity types where more than one relation is possible, such as between people.
",6.4 Discussion,[0],[0]
"While the bigram and any-rel features capture succintly whether textual relations are present between two entities, the one-sided features are more useful for determining whether an entity fits into the domain or range of a particular relation.",6.4 Discussion,[0],[0]
"We saw a few features that did this, capturing finegrained entity types.",6.4 Discussion,[0],[0]
"Most of the features, however, tended towards memorizing (and thus overfitting) the training data, as these features contained the names of the training entities.",6.4 Discussion,[0],[0]
"We believe this overfitting to be the main reason these features did not improve performance, along with the fact that the relations we tested do not need much domain or range modeling (as opposed to, e.g., SPOUSEOF or CITYCAPITALOFCOUNTRY).",6.4 Discussion,[0],[0]
We have explored several practical issues that arise when using the path ranking algorithm for knowledge base completion.,7 Conclusion,[0],[0]
"An analysis of several of these issues led us to propose a simpler algorithm, which we called subgraph feature extraction, which characterizes the subgraph around node pairs and extracts features from that subgraph.",7 Conclusion,[0],[0]
SFE is both significantly faster and performs better than PRA on this task.,7 Conclusion,[0],[0]
"We showed experimentally that we can reduce running time by an order of magnitude, while at the same time improving mean average precision from .432 to .528 and mean reciprocal rank from .850 to .933.",7 Conclusion,[0],[0]
This thus constitutes the best published results for knowledge base completion on NELL data.,7 Conclusion,[0],[0]
The code and data used in the experiments in this paper are available at http://rtw.ml.cmu.edu/emnlp2015 sfe/.,7 Conclusion,[0],[0]
"This work was supported in part by NSF grant IIS1247489, in part by support as a Yahoo! Fellow, and in part by DARPA contract FA87501320005.",Acknowledgements,[0],[0]
"We explore some of the practicalities of using random walk inference methods, such as the Path Ranking Algorithm (PRA), for the task of knowledge base completion.",abstractText,[0],[0]
"We show that the random walk probabilities computed (at great expense) by PRA provide no discernible benefit to performance on this task, so they can safely be dropped.",abstractText,[0],[0]
"This allows us to define a simpler algorithm for generating feature matrices from graphs, which we call subgraph feature extraction (SFE).",abstractText,[0],[0]
"In addition to being conceptually simpler than PRA, SFE is much more efficient, reducing computation by an order of magnitude, and more expressive, allowing for much richer features than paths between two nodes in a graph.",abstractText,[0],[0]
"We show experimentally that this technique gives substantially better performance than PRA and its variants, improving mean average precision from .432 to .528 on a knowledge base completion task using the NELL KB.",abstractText,[0],[0]
Efficient and Expressive Knowledge Base Completion Using Subgraph Feature Extraction,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1215–1225 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1215",text,[0],[0]
"Benefited from the recent advances in neural networks (NNs) and the access to nearly unlimited corpora, neural language models are able to achieve a good perplexity score and generate highquality sentences.",1 Introduction,[0],[0]
"These LMs automatically capture abundant linguistic information and patterns from large text corpora, and can be applied to facilitate a wide range of NLP applications (Rei, 2017; Liu et al., 2018; Peters et al., 2018).
",1 Introduction,[0],[0]
"Recently, efforts have been made on learning contextualized representations with pre-trained language models (LMs)",1 Introduction,[0],[0]
"(Peters et al., 2018).",1 Introduction,[0],[0]
"These pre-trained layers brought significant improvements to various NLP benchmarks, yielding
up to 30% relative error reductions.",1 Introduction,[0],[0]
"However, due to high variability of language, gigantic NNs (e.g., LSTMs with 8,192 hidden states) are preferred to construct informative LMs and extract multifarious linguistic information (Peters et al., 2017).",1 Introduction,[0],[0]
"Even though these models can be integrated without retraining (using their forward pass only), they still result in heavy computation workloads during inference stage, making them prohibitive for realworld applications.
",1 Introduction,[0],[0]
"In this paper, we aim to compress LMs for the end task in a plug-in-and-play manner.",1 Introduction,[0],[0]
"Typically, NN compression methods require the retraining of the whole model (Mellempudi et al., 2017).",1 Introduction,[0],[0]
"However, neural language models are usually composed of RNNs, and their backpropagations require significantly more RAM than their inference.",1 Introduction,[0],[0]
It would become even more cumbersome when the target task equips the coupled LMs to capture information in both directions.,1 Introduction,[0],[0]
"Therefore, these methods do not fit our scenario very
well.",1 Introduction,[0],[0]
"Accordingly, we try to compress LMs while avoiding costly retraining.
",1 Introduction,[0],[0]
"Intuitively, layers of different depths would capture linguistic information of different levels.",1 Introduction,[0],[0]
"Meanwhile, since LMs are trained in a taskagnostic manner, not all layers and their extracted information are relevant to the end task.",1 Introduction,[0],[0]
"Hence, we propose to compress the model by layer selection, which retains useful layers for the target task and prunes irrelevant ones.",1 Introduction,[0],[0]
"However, for the widely-used stacked-LSTM, directly pruning any layers will eliminate all subsequent ones.",1 Introduction,[0],[0]
"To overcome this challenge, we introduce the dense connectivity.",1 Introduction,[0],[0]
"As shown in Fig. 1, it allows us to detach any layers while keeping all remaining ones, thus creating the basis to avoid retraining.",1 Introduction,[0],[0]
"Moreover, such connectivity can stretch shallow and wide LMs to be deep and narrow (Huang et al., 2017), and enable a more fine-grained layer selection.
",1 Introduction,[0],[0]
"Furthermore, we try to retain the effectiveness of the pruned model.",1 Introduction,[0],[0]
"Specifically, we modify the L1 regularization for encouraging the selection weights to be not only sparse but binary, which protects the retained layer connections from shrinkage.",1 Introduction,[0],[0]
"Besides, we design a layer-wise dropout to make LMs more robust and better prepared for the layer selection.
",1 Introduction,[0],[0]
"We refer to our model as LD-Net, since the layer selection and the dense connectivity form the basis of our pruning methods.",1 Introduction,[0],[0]
"For evaluation, we apply LD-Net on two sequence labeling benchmark datasets, and demonstrated the effectiveness of the proposed method.",1 Introduction,[0],[0]
"In the CoNLL03 Named Entity Recognition (NER) task, the F1 score increases from 90.78±0.24% to 91.86±0.15% by integrating the unpruned LMs.",1 Introduction,[0],[0]
"Meanwhile, after pruning over 90% calculation workloads from the best performing model1 (92.03%), the resulting model still yields 91.84±0.14%.",1 Introduction,[0],[0]
Our implementations and pre-trained models would be released for futher study2.,1 Introduction,[0],[0]
"Given a input sequence of T word-level tokens, {x1, x2, · · · , xT }, we use xt to denote the embedding of xt.",2 LD-Net,[0],[0]
"For a L-layers NN, we mark the input and output of the lth layer at the tth time stamp as xl,t and hl,t.
1Based on their performance on the development sets 2 https://github.com/LiyuanLucasLiu/
LD-Net.",2 LD-Net,[0],[0]
"We represent one RNN layer as a function:
hl,t = Fl(xl,t,hl,t−1) (1)
where Fl is the recurrent unit of lth layer, it could be any RNNs variants, and the vanilla LSTMs is used in our experiments.
",2.1 RNN and Dense Connectivity,[0],[0]
"As deeper NNs usually have more representation power, RNN layers are often stacked together to form the final model by setting xl,t = hl−1,t.",2.1 RNN and Dense Connectivity,[0],[0]
"These vanilla stacked-RNN models, however, suffer from problems like the vanishing gradient, and it’s hard to train very deep models.
",2.1 RNN and Dense Connectivity,[0],[0]
"Recently, the dense connectivity and residual connectivity have been proposed to handle these problems (He et al., 2016; Huang et al., 2017).",2.1 RNN and Dense Connectivity,[0],[0]
"Specifically, dense connectivity refers to adding direct connections from any layer to all its subsequent layers.",2.1 RNN and Dense Connectivity,[0],[0]
"As illustrated in Figure 1, the input of lth layer is composed of the original input and the output of all preceding layers as follows.
",2.1 RNN and Dense Connectivity,[0],[0]
"xl,t =",2.1 RNN and Dense Connectivity,[0],[0]
"[xt,h1,t, · · · ,hl−1,t]
Similarly, the final output of the L-layer RNN is ht =",2.1 RNN and Dense Connectivity,[0],[0]
"[xt,h1,t, · · · ,hL,t].",2.1 RNN and Dense Connectivity,[0],[0]
"With dense connectivity, we can detach any single layer without eliminating its subsequent layers (as in Fig. 1).",2.1 RNN and Dense Connectivity,[0],[0]
"Also, existing practices in computer vision demonstrate that such connectivities can lead to deep and narrow NNs and distribute parameters into different layers.",2.1 RNN and Dense Connectivity,[0],[0]
"Moreover, different layers in LMs usually capture linguistic information of different levels.",2.1 RNN and Dense Connectivity,[0],[0]
"Hence, we can compress LMs for a specific task by pruning unrelated or unimportant layers.",2.1 RNN and Dense Connectivity,[0],[0]
Language modeling aims to describe the sequence generation.,2.2 Language Modeling,[0],[0]
"Normally, the generation probability of the sequence {x1, · · · , xT } is defined in a “forward” manner:
p(x1, · · · , xT )",2.2 Language Modeling,[0],[0]
"= T∏ t=1 p(xt|x1, · · · , xt−1) (2)
",2.2 Language Modeling,[0],[0]
"Where p(xt|x1, · · · , xt−1) is computed based on the output of RNN, ht.",2.2 Language Modeling,[0],[0]
"Due to the dense connectivity, ht is composed of outputs from different layers, which are designed to capture linguistic information of different levels.",2.2 Language Modeling,[0],[0]
"Similar to the bottleneck layers employed in the DenseNet (Huang et al., 2017), we add additional layers to unify such
information.",2.2 Language Modeling,[0],[0]
"Accordingly, we add an projection layer with the ReLU activation function:
h∗t = ReLU(Wproj · ht + bproj) (3)
",2.2 Language Modeling,[0],[0]
"Based on h∗t , it’s intuitive to calculate p(xt|x1, · · · , xt−1) by the softmax function, i.e., softmax(Wout · h∗t + b).
",2.2 Language Modeling,[0],[0]
"Since the training of language models needs nothing but the raw text, it has almost unlimited corpora.",2.2 Language Modeling,[0],[0]
"However, conducting training on extensive corpora results in a huge dictionary, and makes calculating the vanilla softmax intractable.",2.2 Language Modeling,[0],[0]
"Several techniques have been proposed to handle this problem, including adaptive softmax (Grave et al., 2017), slim word embedding (Li et al., 2018), the sampled softmax and the noise contrastive estimation (Józefowicz et al., 2016).",2.2 Language Modeling,[0],[0]
"Since the major focus of our paper does not lie in the language modeling task, we choose the adaptive softmax because of its practical efficiency when accelerated with GPUs.",2.2 Language Modeling,[0],[0]
"As pre-trained LMs can describe the text generation accurately, they can be utilized to extract information and construct features for other tasks.",2.3 Contextualized Representations,[0],[0]
"These features, referred as contextualized representations, have been demonstrated to be essentially useful (Peters et al., 2018).",2.3 Contextualized Representations,[0],[0]
"To capture information from both directions, we utilized not only forward LMs, but also backward LMs.",2.3 Contextualized Representations,[0],[0]
Backward LMs are based on Eqn. 4 instead of Eqn. 2.,2.3 Contextualized Representations,[0],[0]
"Similar to forward LMs, backward LMs approach p(xt|xt+1, · · · , xT ) with NNs.",2.3 Contextualized Representations,[0],[0]
"For reference, the output of the RNN in backward LMs for xt is recorded as hrt .
",2.3 Contextualized Representations,[0],[0]
"p(x1, · · · , xn) = T∏ t=1 p(xt|xt+1, · · · , xT ) (4)
Ideally, the final output of LMs (e.g., h∗t ) would be the same as the representation of the target word (e.g., xt+1); therefore, it may not contain much context information.",2.3 Contextualized Representations,[0],[0]
"Meanwhile, the output of the densely connected RNN (e.g., ht) includes outputs from every layer, thus summarizing all extracted features.",2.3 Contextualized Representations,[0],[0]
"Since the dimensions of ht could be too large for the end task, we add a non-linear transformation to calculate the contextualized representation (rt):
rt = ReLU(Wcr ·",2.3 Contextualized Representations,[0],[0]
"[ht,hrt ] + bcr) (5)
",2.3 Contextualized Representations,[0],[0]
"Our proposed method bears the same intuition as the ELMo (Peters et al., 2018).",2.3 Contextualized Representations,[0],[0]
"ELMo is designed for the vanilla stacked-RNN, and tries to calculate a weighted average of different layers’ outputs as the contextualized representation.",2.3 Contextualized Representations,[0],[0]
"Our method, benefited from the dense connectivity and its narrow structure, can directly combine the outputs of different layers by concatenation.",2.3 Contextualized Representations,[0],[0]
"It does not assume the outputs of different layers to be in the same vector space, thus having more potential for transferring the constructed token representations.",2.3 Contextualized Representations,[0],[0]
More discussions are available in Sec. 4.,2.3 Contextualized Representations,[0],[0]
Typical model compression methods require retraining or gradient calculation.,2.4 Layer Selection,[0],[0]
"For the coupled LMs, these methods require even more computation resources compared to the training of LMs, thus not fitting our scenario very well.
",2.4 Layer Selection,[0],[0]
"Benefited from the dense connectivity, we are able to train deep and narrow networks.",2.4 Layer Selection,[0],[0]
"Moreover, we can detach one of its layer without eliminating all subsequent layers (as in Fig. 1).",2.4 Layer Selection,[0],[0]
"Since different layers in NNs could capture different linguistic information, only a few of them would be relevant or useful for a specific task.",2.4 Layer Selection,[0],[0]
"As a result, we try to compress these models by the task-guided layer selection.",2.4 Layer Selection,[0],[0]
"For i-th layer, we introduce a binary mask zi ∈ {0, 1} and calculate hl,t with Eqn. 6 instead of Eqn. 1.
hl,t = zi ·",2.4 Layer Selection,[0],[0]
"Fl(xl,t,hl,t−1) (6)
",2.4 Layer Selection,[0],[0]
"With this setting, we can conduct a layer selection by optimizing the regularized empirical risk:
minL+ λ0 · R (7)
where L is the empirical risk for the sequence labeling task andR is the sparse regularization.
",2.4 Layer Selection,[0],[0]
"The ideal choice for R would be the L0 regularization of z, i.e., R0(z) = |z|0.",2.4 Layer Selection,[0],[0]
"However, it is not continuous and cannot be efficiently optimized.",2.4 Layer Selection,[0],[0]
"Hence, we relax zi from binary to a real value (i.e., 0 ≤ zi ≤ 1) and replaceR0 by:
R1 = |z|1
Despite the sparsity achieved by R1, it could hurt the performance by shifting all zi far away from 1.",2.4 Layer Selection,[0],[0]
"Such shrinkage introduces additional noise in hl,t and xl,t, which may result in ineffective pruned LMs.",2.4 Layer Selection,[0],[0]
"Since our goal is to conduct
pruning without retraining, we further modify the L1 regularization to achieve sparsity while alleviating its shrinkage effect.",2.4 Layer Selection,[0],[0]
"As the target of R is to make z sparse, it can be “turned-off” after achieving a satisfying sparsity.",2.4 Layer Selection,[0],[0]
"Therefore, we extendR1 to a margin-based regularization:
R2 = δ(|z|0",2.4 Layer Selection,[0],[0]
>,2.4 Layer Selection,[0],[0]
"λ1)|z|1
In addition, we also want to make up the relaxation made on z, i.e., relaxing its values from binary to [0, 1].",2.4 Layer Selection,[0],[0]
"Accordingly, we add the penalty |z(1 − z)|1 to encourage z to be binary (Murray and Ng, 2010) and modifyR2 intoR3:
R3 = δ(|z|0",2.4 Layer Selection,[0],[0]
>,2.4 Layer Selection,[0],[0]
"λ1)|z|1 + |z(1− z)|1
To compare R1, R2 and R3, we visualize their penalty values in Fig. 2.",2.4 Layer Selection,[0],[0]
"The visualization is generated for a 3-dimensional z while the targeted sparsity, λ1, is set to 2.",2.4 Layer Selection,[0],[0]
"Comparing to R1, we can observe that R2 enlarges the optimal point set from 0 to all z with a satisfying sparsity, thus avoiding the over-shrinkage.",2.4 Layer Selection,[0],[0]
"To better demonstrate the effect of R3, we further visualize its penalties after achieving a satisfying sparsity (w.l.o.g., assuming z3 = 0).",2.4 Layer Selection,[0],[0]
One can observe that it penalizes non-binary z and favors binary values.,2.4 Layer Selection,[0],[0]
"So far, we’ve customized the regularization term for the layer-wise pruning, which protects the retained connections among layers from shrinking.",2.5 Layer-wise Dropout,[0],[0]
"After that, we try to further retain the effectiveness of the compressed model.",2.5 Layer-wise Dropout,[0],[0]
"Specifically, we choose to prepare the LMs for the pruned inputs, thus making them more robust to pruning.
",2.5 Layer-wise Dropout,[0],[0]
"Accordingly, we conduct the training of LMs with a layer-wise dropout.",2.5 Layer-wise Dropout,[0],[0]
"As in Figure 3, a random part of layers in the LMs are randomly dropped during each batch.",2.5 Layer-wise Dropout,[0],[0]
"The outputs of the dropped layers will not be passed to their subsequent recurrent layers, but will be sent to the projection layer (Eqn. 3) for predicting the next word.
",2.5 Layer-wise Dropout,[0],[0]
"In other words, this dropout is only applied to the input of recurrent layers, which aims to imitate the pruned input without totally removing any layers.",2.5 Layer-wise Dropout,[0],[0]
"In this section, we will introduce our sequence labeling architecture, which is augmented with the contextualized representations.",3 Sequence Labeling,[0],[0]
"Following the recent studies (Liu et al., 2018; Kuru et al., 2016), we construct the neural architecture as in Fig. 4.",3.1 Neural Architecture,[0],[0]
"Given the input sequence {x1, x2, · · · , xT }, for tth token (xt), we assume its word embedding is wt, its label is yt, and its character-level input is {ci,1, ci,2, · · · , ci, }, where ci, is the space character following xt.
",3.1 Neural Architecture,[0],[0]
The character-level representations have become the required components for most of the state-of-the-art.,3.1 Neural Architecture,[0],[0]
"Following the recent study (Liu et al., 2018), we employ LSTMs to take the character-level input in a context-aware manner, and mark its output for xt as ct.",3.1 Neural Architecture,[0],[0]
"Similar to the contextualized representation, ct usually has more dimensions than wt.",3.1 Neural Architecture,[0],[0]
"To integrate them together, we set the output dimension of Eqn. 5 as the dimension of wt, and project ct to a new space with the same dimension number.",3.1 Neural Architecture,[0],[0]
"We mark the projected character-level representation as c∗t .
",3.1 Neural Architecture,[0],[0]
"After projections, these vectors are concatenated as vt =",3.1 Neural Architecture,[0],[0]
[c∗t ;,3.1 Neural Architecture,[0],[0]
"rt;wt],∀i ∈",3.1 Neural Architecture,[0],[0]
"[1, T ] and further fed into the word-level LSTMs.",3.1 Neural Architecture,[0],[0]
"We refer to their output as U = {u1, · · · ,uT }.",3.1 Neural Architecture,[0],[0]
"To ensure the model to predict valid label sequences, we append a first-order conditional random field (CRF) layer to the model (Lample et al., 2016).",3.1 Neural Architecture,[0],[0]
"Specifically, the model defines the generation probability of y = {y1, · · · , yT } as
p(y|U) = ∏T t=1 φ(yt−1, yt,ut)∑
ŷ∈Y(U) ∏T t=1 φ(ŷt−1, ŷt,ut) (8)
where ŷ = {ŷ1, . . .",3.1 Neural Architecture,[0],[0]
", ŷT } is a generic label sequence, Y(U) is the set of all generic label sequences for U and φ(yt−1, yt,ut) is the potential function.",3.1 Neural Architecture,[0],[0]
"In our model, φ(yt−1, yt,ut) is defined as exp(Wytut + byt−1,yt), where Wyt and byt−1,yt are the weight and bias parameters.",3.1 Neural Architecture,[0],[0]
"We use the following negative log-likelihood as the empirical risk.
",3.2 Model Training and Inference,[0],[0]
"L = − ∑ U log p(y|U) (9)
For testing or decoding, we want to find the optimal sequence y∗ that maximizes the likelihood.
",3.2 Model Training and Inference,[0],[0]
"y∗ = argmax y∈Y(U) p(y|U) (10)
",3.2 Model Training and Inference,[0],[0]
"Although the denominator of Eq. 8 is complicated, we can calculate Eqs.",3.2 Model Training and Inference,[0],[0]
"9 and 10 efficiently by the Viterbi algorithm.
",3.2 Model Training and Inference,[0],[0]
"For optimization, we decompose it into two steps, i.e., model training and model pruning.",3.2 Model Training and Inference,[0],[0]
Model training.,3.2 Model Training and Inference,[0],[0]
"We set λ0 to 0 and optimize the empirical risk without any regularization, i.e., minL.",3.2 Model Training and Inference,[0],[0]
"In this step, we conduct optimization with
the stochastic gradient descent with momentum.",3.2 Model Training and Inference,[0],[0]
"Following (Peters et al., 2018), dropout would be added to both the coupled LMs and the sequence labeling model.",3.2 Model Training and Inference,[0],[0]
Model pruning.,3.2 Model Training and Inference,[0],[0]
We conduct the pruning based on the checkpoint which has the best performance on the development set during the model training.,3.2 Model Training and Inference,[0],[0]
We set λ0 to non-zero values and optimize minL + λ0R3,3.2 Model Training and Inference,[0],[0]
by the projected gradient descent with momentum.,3.2 Model Training and Inference,[0],[0]
Any layer i with zi = 0 would be deleted in the final model to complete the pruning.,3.2 Model Training and Inference,[0],[0]
"To get a better stability, dropout is only added to the sequence labeling model.",3.2 Model Training and Inference,[0],[0]
"We will first discuss the capability of the LD-Net as language models, then explore the effectiveness of its contextualized representations.",4 Experiments,[0],[0]
"For comparison, we conducted experiments on the one billion word benchmark dataset (Chelba et al., 2013) with both LD-Net (with 1,600 dimensional projection) and the vanilla stacked-LSTM.",4.1 Language Modeling,[0],[0]
Both kinds of models use word embedding (random initialized) of 300 dimension as input and use the adaptive softmax (with default setting) as an approximation of the full softmax.,4.1 Language Modeling,[0],[0]
"Additionally, as preprocessing, we replace all tokens occurring equal or less than 3 times with as UNK, which shrinks the dictionary from 0.79M to 0.64M.
The optimization is performed by the Adam algorithm (Kingma and Ba, 2014), the gradient is clipped at 5.0 and the learning rate is set to start from 0.001.",4.1 Language Modeling,[0],[0]
"The layer-wise dropout ratio is set to 0.5, the RNNs are unrolled for 20 steps without resetting the LSTM states, and the batch size is set to 128.",4.1 Language Modeling,[0],[0]
"Their performances are summarized in Table 1, together with several LMs used in our sequence labeling baselines.",4.1 Language Modeling,[0],[0]
"For models without official reported parameter numbers, we estimate their values (marked with†) by assuming they adopted the vanilla LSTM.",4.1 Language Modeling,[0],[0]
"Note that, for models 3, 5, 6, 7, 8, and 9, PPL refers to the averaged perplexity of the forward and the backward LMs.
",4.1 Language Modeling,[0],[0]
"We can observe that, for those models taking word embedding as the input, embedding composes the vast majority of model parameters.",4.1 Language Modeling,[0],[0]
"However, embedding can be embodied as a “sparse” layer which is computationally efficient.",4.1 Language Modeling,[0],[0]
"Instead, the intense calculations are conducted in
RNN layers and softmax layer for language modeling, or RNN layers for contextualized representations.",4.1 Language Modeling,[0],[0]
"At the same time, comparing the model 8192-1024 and CNN-8192-1024, their only difference is the input method.",4.1 Language Modeling,[0],[0]
"Instead of taking word embedding as the input, CNN-8192-1024 utilizes CNN to compose word representation from the character-level input.",4.1 Language Modeling,[0],[0]
"Despite the greatly reduced parameter number, the perplexity of the resulting models remains almost unchanged.",4.1 Language Modeling,[0],[0]
"Since replacing embedding layer with CNN would make the training slower, we only conduct experiments with models taking word embedding as the input.
",4.1 Language Modeling,[0],[0]
"Comparing LD-Net with other baselines, we think it achieves satisfactory performance with regard to the size of hidden states.",4.1 Language Modeling,[0],[0]
It demonstrates the LD-Net’s capability of capturing the underlying structure of natural language.,4.1 Language Modeling,[0],[0]
"Meanwhile, we find that the layer-wise dropout makes it harder to train LD-Net and its resulting model achieves less competitive results.",4.1 Language Modeling,[0],[0]
"However, as would be discussed in the next section, layer-wise dropout allows the resulting model to generate better contextualized representations and be more robust to pruning, even with a higher perplexity.",4.1 Language Modeling,[0],[0]
"Following TagLM (Peters et al., 2017), we evaluate our methods in two benchmark datasets, the CoNLL03 NER task (Tjong Kim Sang and De Meulder, 2003) and the CoNLL00 Chunking task (Tjong Kim Sang and Buchholz, 2000).",4.2 Sequence Labeling,[0],[0]
CoNLL03,4.2 Sequence Labeling,[0],[0]
"NER has four entity types and includes
the standard training, development and test sets.",4.2 Sequence Labeling,[0],[0]
"CoNLL00 chunking defines eleven syntactic chunk types (e.g., NP and VP) in addition to Other.",4.2 Sequence Labeling,[0],[0]
"Since it only includes training and test sets, we sampled 1000 sentences from training set as a held-out development set (Peters et al., 2017).
",4.2 Sequence Labeling,[0],[0]
"In both cases, we use the BIOES labeling scheme (Ratinov and Roth, 2009) and use the micro-averaged F1 as the evaluation metric.",4.2 Sequence Labeling,[0],[0]
"Based on the analysis conducted in the development set, we set λ0 = 0.05 for the NER task, and λ0 = 0.5 for the Chunking task.",4.2 Sequence Labeling,[0],[0]
"As discussed before, we conduct optimization with the stochastic gradient descent with momentum.",4.2 Sequence Labeling,[0],[0]
"We set the batch size, the momentum, and the learning rate to 10, 0.9, and ηt = η01+ρt respectively.",4.2 Sequence Labeling,[0],[0]
"Here, η0 = 0.015 is the initial learning rate and ρ = 0.05 is the decay ratio.",4.2 Sequence Labeling,[0],[0]
"Dropout is applied in our model, and its ratio is set to 0.5.",4.2 Sequence Labeling,[0],[0]
"For a better stability, we use gradient clipping of 5.0.",4.2 Sequence Labeling,[0],[0]
"Furthermore, we employ the early stopping in the development set and report averaged score across five different runs.
",4.2 Sequence Labeling,[0],[0]
"Regarding the network structure, we use the 30-dimension character-level embedding.",4.2 Sequence Labeling,[0],[0]
Both character-level and word-level RNNs are set to one-layer LSTMs with 150-dimension hidden states in each direction.,4.2 Sequence Labeling,[0],[0]
"The GloVe 100-dimension pre-trained word embedding3 is used as the initialization of word embedding wt, and will be finetuned during the training.",4.2 Sequence Labeling,[0],[0]
"The layer selection variables zi are initialized as 1, remained unchanged
3 https://nlp.stanford.edu/projects/glove/
during the model training and only be updated during the model pruning.",4.2 Sequence Labeling,[0],[0]
"All other variables are randomly initialized (Glorot and Bengio, 2010).",4.2 Sequence Labeling,[0],[0]
Compared methods.,4.2 Sequence Labeling,[0],[0]
"The first baseline, referred as NoLM, is our sequence labeling model without the contextualized representations, i.e., calculating vt as [c∗t ;wt] instead of [c ∗ t ; rt;wt].",4.2 Sequence Labeling,[0],[0]
"Besides, ELMo (Peters et al., 2018) is the major baseline.",4.2 Sequence Labeling,[0],[0]
"To make comparison more fair, we implemented the ELMo model and use it to calculate the rt in Eqn. 5 instead of [ht,hrt ].",4.2 Sequence Labeling,[0],[0]
"Results of reimplemented models are referred with R-ELMo (λ is set to the recommended value, 0.1) and the results reported in its original paper are referred with O-ELMo.",4.2 Sequence Labeling,[0],[0]
"Additionally, since TagLM (Peters et al., 2017) with one-layer NNs can be viewed as a special case of ELMo, we also include its results.",4.2 Sequence Labeling,[0],[0]
Sequence labeling results.,4.2 Sequence Labeling,[0],[0]
Table 2 and 3 summarizes the results of LD-Net and baselines.,4.2 Sequence Labeling,[0],[0]
"Besides the F1 score and averaged perplexity, we also estimate FLOPs (i.e., the number of floatingpoint multiplication-adds) for the efficiency evaluation.",4.2 Sequence Labeling,[0],[0]
"Since our model takes both word-level and character-level inputs, we estimated the FLOPs value for a word-level input with 4.39 characterlevel inputs, while 4.39 is the averaged length of words in the CoNLL03 dataset.
",4.2 Sequence Labeling,[0],[0]
"Before the model pruning, LD-Net achieves a 96.05±0.08 F1 score in the CoNLL00 Chunking task, yielding nearly 30% error reductions over the NoLM baseline.",4.2 Sequence Labeling,[0],[0]
"Also, it scores 91.86±0.15 F1 in the CoNLL03 NER task with over 10% error reductions.",4.2 Sequence Labeling,[0],[0]
"Similar to the language modeling, we
observe that the most complicated models achieve the best perplexity and provide the most improvements in the target task.",4.2 Sequence Labeling,[0],[0]
"Still, considering the number of model parameters and the resulting perplexity, our model demonstrates its effectiveness in generating contextualized representations.",4.2 Sequence Labeling,[0],[0]
"For example, comparing to our methods, R-ELMo (7) leverages LMs with the similar perplexity and parameter number, but cannot get the same improvements with our method on both datasets.
",4.2 Sequence Labeling,[0],[0]
"Actually, contextualized representations have strong connections with the skip-thought vectors (Kiros et al., 2015).",4.2 Sequence Labeling,[0],[0]
Skip-thought models try to embed sentences and are trained by predicting the previous and afterwards sentences.,4.2 Sequence Labeling,[0],[0]
"Similarly, LMs encode a specific context as the hidden states of RNNs, and use them to predict future contexts.",4.2 Sequence Labeling,[0],[0]
"Specifically, we recognize the cell states of LSTMs are more like to be the sentence embedding (Radford et al., 2017), since they are only passed to the next time stamps.",4.2 Sequence Labeling,[0],[0]
"At the same time, because the hidden states would be passed to other layers, we think they are more like to be the token representations capturing necessary signals for predicting the next word or updating context representations4.",4.2 Sequence Labeling,[0],[0]
"Hence, LD-Net should be more
4We tried to combine the cell states with the hidden states to construct the contextualized representations by concatenation or weighted average, but failed to get better performance.
",4.2 Sequence Labeling,[0],[0]
"effective then ELMo, as concatenating could preserve all extracted signals while weighted average might cause information loss.
",4.2 Sequence Labeling,[0],[0]
"Although the layer-wise dropout makes the model harder to train, their resulting LMs generate better contextualized representations, even without the same perplexity.",4.2 Sequence Labeling,[0],[0]
"Also, as discussed in (Peters et al., 2018, 2017), the performance of the contextualized representation can be further improved by training larger models or using the CNN to represent words.
",4.2 Sequence Labeling,[0],[0]
"For the pruning, we started from the model with the best performance on the development set (referred with “origin”), and refer the performances of pruned models with “pruned” in Table 2 and 3.",4.2 Sequence Labeling,[0],[0]
"Essentially, we can observe the pruned models get rid of the vast majority of calculation while still retaining a significant improvement.",4.2 Sequence Labeling,[0],[0]
We will discuss more on the pruned models in Sec. 4.4.,4.2 Sequence Labeling,[0],[0]
"We use FLOPs for measuring the inference efficiency as it reflects the time complexity (Han et al., 2015), and thus is independent of specific implementations.",4.3 Speed Up Measurements,[0],[0]
"For models with the same structure, improvements in FLOPs would result in monotonically decreasing inference time.",4.3 Speed Up Measurements,[0],[0]
"However, it may not reflect the actual efficiency of models due to the model differences in parallelism.",4.3 Speed Up Measurements,[0],[0]
"Accordingly, we also tested wall-clock speeds of our implementations.
",4.3 Speed Up Measurements,[0],[0]
"Our implementations are based on the PyTorch 0.3.15, and all experiments are conducted on the CoNLL03 dataset with the Nvidia GTX 1080 GPU.",4.3 Speed Up Measurements,[0],[0]
"Specifically, due to the limited size of CoNLL03 test set, we measure such speeds on the training set.",4.3 Speed Up Measurements,[0],[0]
"As in Table 4, we can observe that, the pruned model achieved about 5 times speed up.",4.3 Speed Up Measurements,[0],[0]
"Although there is still a large margin between
We think it implies that ELMo works as token representations instead of sentence representations
5http://pytorch.org/
the actual speed-up and the FLOPs speed-up, we think the resulting decode speed (166K words/s) is sufficient for most real-world applications.",4.3 Speed Up Measurements,[0],[0]
Effect of the pruning ratio.,4.4 Case Studies,[0],[0]
"To explore the effect of the pruning ratio, we adjust λ1 and visualize the performance of pruned models v.s. their FLOPs # in Fig 5.",4.4 Case Studies,[0],[0]
"We can observe that LD-Net outperforms its variants and demonstrates its effectiveness.
",4.4 Case Studies,[0],[0]
"As the pruning ratio becoming larger, we can observe the performance of LD-Net first increases a little, then starts dropping.",4.4 Case Studies,[0],[0]
"Besides, in the CoNLL03 NER task, LMs can be pruned to a relatively small size without much loss of efficiency.",4.4 Case Studies,[0],[0]
"As in Table 3, we can observe that, after pruning over 90% calculations, the error of the resulting model only increases about 2%, yielding a competitive performance.",4.4 Case Studies,[0],[0]
"As for the CoNLL00 Chunking task, the performance of LD-Net decays in a faster rate than that in the NER task.",4.4 Case Studies,[0],[0]
"For example, after pruning over 80% calculations, the error of the resulting model increases about 13%.",4.4 Case Studies,[0],[0]
"Considering the fact that this corpus is only half the size of the CoNLL03 NER dataset, we can expect the resulting models have more dependencies on the LMs.",4.4 Case Studies,[0],[0]
"Still, the pruned model achieves a 25% error reduction over the NoLM baseline.
",4.4 Case Studies,[0],[0]
"Table 1 0 1 2 3 4 5 6 7 8 9 >10 4 3 3 1 1 0 0 1 1 2 4
1
Layer selection pattern.",4.4 Case Studies,[0],[0]
We further studied the layer selection patterns.,4.4 Case Studies,[0],[0]
"Specifically, we use the same setting of LD-Net (9) in Table 3, conduct model pruning using for 50 times, and summarize the statics in Figure 6.",4.4 Case Studies,[0],[0]
"We can observe that network layers formulate two clear clusters, one is likely to be preserved during the selection, and the other is likely to be pruned.",4.4 Case Studies,[0],[0]
"This is consistent with our intuition that some layers are more important than others and the layer selection algorithm would pick up layers meaningfully.
",4.4 Case Studies,[0],[0]
"However, there is some randomness in the selection result.",4.4 Case Studies,[0],[0]
"We conjugate that large networks trained with dropout can be viewed as a ensemble of small sub-networks (Hara et al., 2016), also there would be several sub-networks having the similar function.",4.4 Case Studies,[0],[0]
"Accordingly, we think the randomness mainly comes from such redundancy.",4.4 Case Studies,[0],[0]
Effectiveness of model pruning.,4.4 Case Studies,[0],[0]
Zhu and Gupta (2017) observed pruned large models consistently outperform small models on various tasks (including LM).,4.4 Case Studies,[0],[0]
These observations are consistent with our experiments.,4.4 Case Studies,[0],[0]
"For example, LD-Net achieves 91.84 after pruning on the CoNLL03 dataset.",4.4 Case Studies,[0],[0]
"It outperforms TagLM (4) and R-ELMo (7), whose performances are 91.62 and 91.54.",4.4 Case Studies,[0],[0]
"Besides, we trained small LMs of the same size as the pruned LMs (1-layer densely connected LSTMs).",4.4 Case Studies,[0],[0]
Its perplexity is 69 and its performance on the CoNLL03 dataset is 91.55± 0.19.,4.4 Case Studies,[0],[0]
Sequence labeling.,5 Related Work,[0],[0]
"Linguistic sequence labeling is one of the fundamental tasks in NLP, encompassing various applications including POS tagging, chunking, and NER.",5 Related Work,[0],[0]
"Many attempts have been made to conduct end-to-end learning and build reliable models without handcrafted features (Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016).
",5 Related Work,[0],[0]
Language modeling.,5 Related Work,[0],[0]
Language modeling is a core task in NLP.,5 Related Work,[0],[0]
"Many attempts have been paid to develop better neural language models (Zilly et al., 2017; Inan et al., 2016; Godin et al., 2017; Melis et al., 2017).",5 Related Work,[0],[0]
"Specifically, with extensive corpora, language models can be well trained to generate high-quality sentences from scratch (Józefowicz et al., 2016; Grave et al., 2017; Li et al., 2018; Shazeer et al., 2017).",5 Related Work,[0],[0]
"Meanwhile, initial attempts have been made to improve the performance of other tasks with these methods.",5 Related Work,[0],[0]
"Some methods treat the language modeling as an additional supervision, and conduct co-training for knowledge transfer (Dai and Le, 2015; Liu et al., 2018; Rei, 2017).",5 Related Work,[0],[0]
"Others, including this paper, aim to construct additional features (referred as contextualized representations) with the pre-trained language models (Peters et al., 2017, 2018).",5 Related Work,[0],[0]
Neural Network Acceleration.,5 Related Work,[0],[0]
"There are mainly three kinds of NN acceleration methods, i.e., prune network into smaller sizes (Han et al., 2015; Wen et al., 2016), converting float operation into customized low precision arithmetic (Hubara et al., 2018; Courbariaux et al., 2016), and using shallower networks to mimic the output of deeper ones (Hinton et al., 2015; Romero et al., 2014).",5 Related Work,[0],[0]
"However, most of them require costly retraining.",5 Related Work,[0],[0]
"Here, we proposed LD-Net, a novel framework for efficient contextualized representation.",6 Conclusion,[0],[0]
"As demonstrated on two benchmarks, it can conduct the layer-wise pruning for a specific task.",6 Conclusion,[0],[0]
"Moreover, it requires neither the gradient oracle of LMs nor the costly retraining.",6 Conclusion,[0],[0]
"In the future, we plan to apply LD-Net to other applications.",6 Conclusion,[0],[0]
We thank Junliang Guo and all reviewers for their constructive comments.,Acknowledgments,[0],[0]
Research was sponsored by the Army Research Laboratory and was accomplished under Cooperative Agreement Number W911NF-09-2-0053 (the ARL Network Science CTA).,Acknowledgments,[0],[0]
"The views and conclusions in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgments,[0],[0]
"Many efforts have been made to facilitate natural language processing tasks with pre-trained language models (LMs), and brought significant improvements to various applications.",abstractText,[0],[0]
"To fully leverage the nearly unlimited corpora and capture linguistic information of multifarious levels, large-size LMs are required; but for a specific task, only parts of these information are useful.",abstractText,[0],[0]
"Such large-sized LMs, even in the inference stage, may cause heavy computation workloads, making them too time-consuming for large-scale applications.",abstractText,[0],[0]
Here we propose to compress bulky LMs while preserving useful information with regard to a specific task.,abstractText,[0],[0]
"As different layers of the model keep different information, we develop a layer selection method for model pruning using sparsityinducing regularization.",abstractText,[0],[0]
"By introducing the dense connectivity, we can detach any layer without affecting others, and stretch shallow and wide LMs to be deep and narrow.",abstractText,[0],[0]
"In model training, LMs are learned with layerwise dropouts for better robustness.",abstractText,[0],[0]
Experiments on two benchmark datasets demonstrate the effectiveness of our method.,abstractText,[0],[0]
Efficient Contextualized Representation: Language Model Pruning for Sequence Labeling,title,[0],[0]
"We consider learning a sparse linear regressor β minimizing the population objective:
β arg min β
EX,Y D r`pY, xX,βyqs , (1)
where pX, Y q P X Y",1. Introduction,[0],[0]
"Rp Y are drawn from an unknown distribution D and `p , q is a convex loss function, based on N i.i.d.",1. Introduction,[0],[0]
"samples txi, yiuNi 1 drawn from D, and when the support S : supportpβ q tj P rps |",1. Introduction,[0],[0]
"β j 0u of β is small, |S| ¤ s.",1. Introduction,[0],[0]
"In a standard single-machine setting, a common empirical approach is to minimize the `1 regularized empirical loss (see, e.g., (2) below).",1. Introduction,[0],[0]
"Here we consider a setting where data are distributed across m machines, and, for simplicity, assume1 that N nm, so that each machine j has access to n i.i.d. observations (from the same source D) txji, yjiuni 1 (equivalently, that N nm samples are randomly partitioned across machines).
",1. Introduction,[0],[0]
The main contribution of the paper is a novel algorithm for estimating β in a distributed setting.,1. Introduction,[0],[0]
"Our estimator is
1University of Chicago, USA 2Toyota Technological Institute at Chicago, USA 3Tencent AI Lab, China.",1. Introduction,[0],[0]
"Correspondence to: Jialei Wang <jialei@uchicago.edu>, Mladen Kolar <mkolar@chicagobooth.edu>, Nathan Srebro <nati@ttic.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"1Results in the paper easily generalize to a setting where each machine has a different number of observations.
able to achieve the performance of a centralized procedure that has access to all data, while keeping computation and communication costs low.",1. Introduction,[0],[0]
"Compared to the existing oneshot estimation approach (Lee et al., 2015b), our method can achieve the same statistical performance without performing the expensive debiasing step.",1. Introduction,[0],[0]
"As the number of communication rounds increases, the estimation accuracy improves until matching the performance of a centralized procedure, which happens after the logarithm of the total number of machines rounds.",1. Introduction,[0],[0]
"Furthermore, our results can be achieved under weak assumptions on the data generating procedure.
",1. Introduction,[0],[0]
We assume that the communication occurs in rounds.,1. Introduction,[0],[0]
"In each round, machines exchange messages with the master machine.",1. Introduction,[0],[0]
"Between two rounds, each machine only computes based on its local information, which includes local data and previous messages (Zhang et al., 2013b; Shamir & Srebro, 2014; Arjevani & Shamir, 2015).",1. Introduction,[0],[0]
"In a nondistributed setting, efficient estimation procedures need to balance statistical efficiency with computation efficiency (runtime).",1. Introduction,[0],[0]
"In a distributed setting, the situation is more complicated and we need to balance two resources, local runtime and number of rounds of communication, with the statistical error.",1. Introduction,[0],[0]
The local runtime refers to the amount of work each machine needs to do.,1. Introduction,[0],[0]
The number of rounds of communication refers to how often do local machines need to exchange messages with the master machine.,1. Introduction,[0],[0]
"We compare our procedure to other algorithm using the aforementioned metrics.
",1. Introduction,[0],[0]
We consider the following two baseline estimators of β : the local estimator uses data available only on the master (first) machine and ignores data available on other machines.,1. Introduction,[0],[0]
"In particular, it computes
β̂local arg min β
1
n
ņ
i 1
`py1i, xx1i,βyq λ||β||1 (2)
using locally available data.",1. Introduction,[0],[0]
"The local procedure is efficient in both communication and computation, however, the resulting estimation error is large compared to an estimator that uses all of the available data.",1. Introduction,[0],[0]
"The other idealized baseline is the centralized estimator
β̂centralize arg min",1. Introduction,[0],[0]
"β
1
mn
m̧
j 1
ņ
i 1
`pyji, xxji,βyq λ||β||1.
",1. Introduction,[0],[0]
"Unfortunately, due to data being huge and communication expensive, we cannot compute the centralized estimator, even though it achieves the optimal statistical error.
",1. Introduction,[0],[0]
"In a related setting, Lee et al. (2015b) studied a one-shot approach to learning β , called Avg-Debias, that is based on averaging the debiased lasso estimators (Zhang & Zhang, 2013).",1. Introduction,[0],[0]
"Under strong assumptions on the data generating procedure, their approach matches the centralized error bound after one round of communication.",1. Introduction,[0],[0]
"While an encouraging result, there are limitations to this approach, that we list below.
",1. Introduction,[0],[0]
• The debiasing step in Avg-Debias is computationally heavy as it requires each local machine to estimate a p p matrix.,1. Introduction,[0],[0]
"For example, Javanmard (2014) (section 5.1) transforms the problem of estimating the debiasing matrix Θ into p generalized lasso problems.",1. Introduction,[0],[0]
"This is computationally prohibitive for high-dimensional problems (Zhang & Zhang, 2013; Javanmard & Montanari, 2014).",1. Introduction,[0],[0]
"In comparison, our procedure requires only solving one `1 penalized objective in each iteration, which has the same time complexity as computing β̂local in (2).",1. Introduction,[0],[0]
"See Section 2 for details.
",1. Introduction,[0],[0]
• Avg-Debias procedure only matches the statistical error rate of the centralized procedure when the sample size per machine satisfies n,1. Introduction,[0],[0]
Á ms2 log p.,1. Introduction,[0],[0]
Our approach improves this sample complexity to n,1. Introduction,[0],[0]
"Á s2 log p.
•",1. Introduction,[0],[0]
Avg-Debias procedure requires strong conditions on the data generating process.,1. Introduction,[0],[0]
"For example, the data matrix is required to satisfy the generalized coherence condition for debiasing to work2.",1. Introduction,[0],[0]
"As we show here, such a condition is not needed for consistent highdimensional estimation in a distributed setting.",1. Introduction,[0],[0]
"Instead, we only require standard restricted eigenvalue condition that are commonly assumed in the highdimensional estimation literature.
",1. Introduction,[0],[0]
"Our method (EDSL) addresses the aforementioned issues 2The generalized coherence states that there exists a matrix
Θ, such that ||Σ̂Θ Ip||8 À b
log p n , where Σ̂ is the empirical covariance matrix.
of Avg-Debias.",1. Introduction,[0],[0]
"Table 1 summarizes the resources required for the approaches discussed above to solve the distributed sparse linear regression problems.
",1. Introduction,[0],[0]
"Parallel Work In parallel work (publicly announced on arXiv simultaneously with the results in this contribution), Jordan et al. (2016) present a method which is equivalent to the first iteration of our method, and thus achieves the same computational advantage over Avg-Debias as depicted in the left column of Table 1 and discussed in the first and third bullet points above.",1. Introduction,[0],[0]
"Jordan et al. extend the idea in ways different and orthogonal to this submission, by considering also low-dimensional and Bayesian inference problems.",1. Introduction,[0],[0]
"Still, for high-dimensional problems, they only consider a one-shot procedure, and so do not achieve statistical optimality in the way our method does, and do not allow using n À ms2 log p samples per machine (see right half of Table 1).",1. Introduction,[0],[0]
"The improved one-shot approach is thus a parallel contribution, made concurrently by Jordan et al. and by us, while the multi-step approach and accompanied reduction in required number of samples (discusse in the second bullet point above) and improvement in statistical accuracy is a distinct contribution of this this submission.
",1. Introduction,[0],[0]
"Other Related Work A large body of literature exists on distributed optimization for modern massive data sets (Dekel et al., 2012; Duchi et al., 2012; 2014; Zhang et al., 2013b; Zinkevich et al., 2010; Boyd et al., 2011; Balcan et al., 2012; Yang, 2013; Jaggi et al., 2014; Ma et al., 2015; Shamir & Srebro, 2014; Zhang & Xiao, 2015; Lee et al., 2015a; Arjevani & Shamir, 2015).",1. Introduction,[0],[0]
"A popular approach to distributed estimation is averaging estimators formed locally by different machines (Mcdonald et al., 2009; Zinkevich et al., 2010; Zhang et al., 2012; Huang & Huo, 2015).",1. Introduction,[0],[0]
"Divide-and-conquer procedures also found applications in statistical inference (Zhao et al., 2014a; Cheng & Shang, 2015; Lu et al., 2016).",1. Introduction,[0],[0]
Shamir & Srebro (2014) and Rosenblatt & Nadler (2014) showed that averaging local estimators at the end will have bad dependence on either condition number or dimension of the problem.,1. Introduction,[0],[0]
"Yang (2013), Jaggi et al. (2014) and Smith et al. (2016) studied distributed optimization using stochastic (dual) coordinate descent, these approaches try to find a good balance between computation and communication, however, their communication com-
plexity depends badly on the condition number.",1. Introduction,[0],[0]
"As a result, they are not better than first-order approaches, such as (proximal) accelerated gradient descent (Nesterov, 1983), in terms of communication.",1. Introduction,[0],[0]
Shamir et al. (2014) and Zhang & Xiao (2015) proposed truly communication-efficient distributed optimization algorithms.,1. Introduction,[0],[0]
"They leveraged the local second-order information and, as a result, obtained milder dependence on the condition number compared to the firstorder approaches (Boyd et al., 2011; Shamir & Srebro, 2014; Ma et al., 2015).",1. Introduction,[0],[0]
"Lower bounds were studied in Zhang et al. (2013a), Braverman et al. (2015), and Arjevani & Shamir (2015).",1. Introduction,[0],[0]
"However, it is not clear how to extend these existing approaches to problems with non-smooth objectives, including the `1 regularized problems.
",1. Introduction,[0],[0]
Most of the above mentioned work is focused on estimators that are (asymptotically) linear.,1. Introduction,[0],[0]
"Averaging at the end reduces the variance of the these linear estimators, resulting in an estimator that matches the performance of a centralized procedure.",1. Introduction,[0],[0]
"Zhang et al. (2013c) studied averaging local estimators obtained by the penalized kernel ridge regression, with the `2 penalty was chosen smaller than usual to avoid the large bias problem.",1. Introduction,[0],[0]
"The situation in a high-dimensional setting is not so straightforward, since the sparsity inducing penalty introduces the bias in a nonlinear way.",1. Introduction,[0],[0]
Zhao et al. (2014b) illustrated how averaging debiased composite quantile regression estimators can be used for efficient inference in a high-dimensional setting.,1. Introduction,[0],[0]
"Averaging debiased high-dimensional estimators was subsequently used in Lee et al. (2015b) for distributed estimation, multi-task learning (Wang et al., 2015), and statistical inference (Battey et al., 2015).
",1. Introduction,[0],[0]
Notation.,1. Introduction,[0],[0]
"We use rns to denote the set t1, . . .",1. Introduction,[0],[0]
", nu.",1. Introduction,[0],[0]
"For a vector a P Rn, we let supportpaq tj : aj 0u be the support set, ||a||q , q P r1,8q, the `q-norm defined as ||a||q p ° iPrns |ai|qq1{q , and ||a||8 maxiPrns |ai|.",1. Introduction,[0],[0]
"For a matrix A P Rn1 n2 , we use the following element-wise `8 matrix norms ||A||8 maxiPrn1s,jPrn2s |aij",1. Introduction,[0],[0]
|.,1. Introduction,[0],[0]
Denote,1. Introduction,[0],[0]
In as n n identity matrix.,1. Introduction,[0],[0]
"For two sequences of numbers tanu8n 1 and tbnu8n 1, we use an Opbnq to denote that an ¤ Cbn for some finite positive constant C, and for all n large enough.",1. Introduction,[0],[0]
"If an Opbnq and bn Opanq, we use the notation an bn.",1. Introduction,[0],[0]
We also use an À bn for an Opbnq and an Á bn for bn Opanq.,1. Introduction,[0],[0]
Paper Organization.,1. Introduction,[0],[0]
"We describe our method in Section 2, and present the main results in the context of sparse linear regression in Section 3, and provide a generalized theory in Section 4.",1. Introduction,[0],[0]
"We demonstrate the effectiveness of the proposal via experiments in Section 5, and conclude the paper with discussions in Section 6.",1. Introduction,[0],[0]
"In Appendix, in Section A we illustrate some concrete examples of the general results in Section 4, and all proofs are deferred in Section B. More experimental results are presented in Section C.
Algorithm 1 Efficient Distributed Sparse Learning (EDSL).",1. Introduction,[0],[0]
"Input: Data txji, yjiujPrms,iPrns, loss function `p , q. Initialization: The master obtains β̂0 by minimizing (3), and broadcast β̂0 to every worker.",1. Introduction,[0],[0]
"for t 0, 1, . . .",1. Introduction,[0],[0]
"do
Workers: for j 2, 3, . . .",1. Introduction,[0],[0]
",m do
if Receive β̂t from the master then Calculate gradient ∇Ljpβ̂tq and send it to the master.
",1. Introduction,[0],[0]
end end,1. Introduction,[0],[0]
"Master: if Receive t∇Ljpβ̂tqumj 2 from all workers then
Obtain β̂t 1 by solving the shifted `1 regularized problem in (4).",1. Introduction,[0],[0]
"Broadcast β̂t 1 to every worker.
",1. Introduction,[0],[0]
end end,1. Introduction,[0],[0]
"In this section, we detail our procedure for estimating β in a distributed setting.",2. Methodology,[0],[0]
Algorithm 1 provides an outline of the steps executed by the master and worker nodes.,2. Methodology,[0],[0]
"Let
Ljpβq 1 n
ņ
i 1
`pyji, xxji,βyq, j P rms,
be the empirical loss at each machine.",2. Methodology,[0],[0]
Our method starts by solving a local `1 regularized M -estimation program.,2. Methodology,[0],[0]
"At iteration t 0, the master (first) machine obtains β̂0 as a minimizer of the following program
minL1pβq λ0||β||1.",2. Methodology,[0],[0]
"(3)
The vector β̂0 is broadcasted to all other machines, which use it to compute a gradient of the local loss at β̂0.",2. Methodology,[0],[0]
"In particular, each worker computes∇Ljpβ̂0q and communicates it back to the master.",2. Methodology,[0],[0]
This constitutes one round of communication.,2. Methodology,[0],[0]
"At the iteration t 1, the master solves the shifted `1 regularized problem
β̂t 1 arg min β L1pβq
C 1
m
m̧
j 1
∇Ljpβ̂tq ∇L1pβ̂tq,β G
λt 1||β||1.",2. Methodology,[0],[0]
"(4)
A minimizer β̂t 1 is communicated to other machines, which use it to compute the local gradient ∇Ljpβ̂t 1q as before.
",2. Methodology,[0],[0]
"Formulation (4) is inspired by the proposal in Shamir et al. (2014), where the authors studied distributed optimization
for smooth and strongly convex empirical objectives.",2. Methodology,[0],[0]
"Compared to Shamir et al. (2014), we do not use any averaging scheme, which would require additional rounds of communication and, moreover, we add an `1 regularization term to ensure consistent estimation in high-dimensions.",2. Methodology,[0],[0]
"Different from the distributed first-order optimization approaches, the refined objective (4) leverages both global first-order information and local higher-order information.",2. Methodology,[0],[0]
"To see this, suppose we set λt 1 0 and that Ljpβq is a quadratic objective with invertible Hessian.",2. Methodology,[0],[0]
"Then we have the following closed form solution for (4),
β̂t 1 β̂t ∇2L1pβ̂tq 1 m 1 ¸",2. Methodology,[0],[0]
"jPrms ∇Ljpβ̂tq ,
which is exactly a sub-sampled Newton updating rule.",2. Methodology,[0],[0]
"Unfortunately for high-dimensional problems, the Hessian is no longer invertible, and a `1 regularization is added to make the solution well behaved.",2. Methodology,[0],[0]
"The regularization parameter λt will be chosen in a way, so that it decreases with the iteration number t. As a result we will be able to show that the final estimator performs as well at the centralized solution.",2. Methodology,[0],[0]
We discuss in details how to choose λt in the following section.,2. Methodology,[0],[0]
"We illustrate our main theoretical results in the context of sparse linear regression model
yji xxji,β y ji, i P rns, j P rms, (5) where xji is a subgaussian p-dimensional vector of input variables and ji is i.i.d. mean zero subgaussian noise.",3. Main Result,[0],[0]
"The loss function considered is the usual the squared loss `py, ŷq 12 py ŷq2.",3. Main Result,[0],[0]
"With this notation, the centralized approach leads to the lasso estimator (Tibshirani, 1996)
β̂centralize arg min β
1
m
m̧
j 1
Ljpβq λ||β||1,
where the loss at worker j is
Ljpβq 1 2n ¸",3. Main Result,[0],[0]
"iPrns pyji xβ,xjiyq2.
",3. Main Result,[0],[0]
"Before stating the main result, we provide the definition of the subgaussian norm (Vershynin, 2012).
",3. Main Result,[0],[0]
Definition 1 (Subgaussian norm).,3. Main Result,[0],[0]
"The subgaussian norm ||X||ψ2 of a subgaussian p-dimensional random vector X , is defined as
||X||ψ2 sup xPSp 1 sup q¡1 q 1{2pE|xX,xy|qq1{q,
where Sp 1 is the p-dimensional unit sphere.
",3. Main Result,[0],[0]
"We also need an assumption on the restricted strong convexity constant (Negahban et al., 2012).",3. Main Result,[0],[0]
Assumption 2.,3. Main Result,[0],[0]
"We assume that there exists a κ ¡ 0, such that for any ∆ P CpS, 3q,
1
2n ||X1∆||22 ¥ κ||∆||22,
where
CpS, 3q t∆ P Rp | ||∆Sc ||1 ¤ 3||∆S ||1u is a restricted cone in Rp, and
X1 rxT11; xT12; . . .",3. Main Result,[0],[0]
"; xT1ns P Rn p
is the data matrix on the master machine.
",3. Main Result,[0],[0]
"When xji are randomly drawn from a subgaussian distribution, Assumption (2) is satisfied with high probability as long as n",3. Main Result,[0],[0]
"Á s log p (Rudelson & Zhou, 2013).",3. Main Result,[0],[0]
We are now ready to state the estimation error bound for β̂t 1 obtained using Algorithm 1.,3. Main Result,[0],[0]
Theorem 3.,3. Main Result,[0],[0]
Assume that data are generated from a sparse linear regression model in (5) with ||xji||ψ2 ¤ σX and ||,3. Main Result,[0],[0]
ji||ψ2 ¤ σ.,3. Main Result,[0],[0]
"Let
λt 1 2 mn ¸",3. Main Result,[0],[0]
"jPrms ¸ iPrns xji ji 8
2L max j,i
||xji||28 ||β̂t β ||1
c logp2p{δq
n (6)
",3. Main Result,[0],[0]
Then for t ¥ 0,3. Main Result,[0],[0]
"we have, with probability at least 1 2δ,
||β̂t 1 β ||1",3. Main Result,[0],[0]
¤1 a t 1 n 1 an 48sσσX,3. Main Result,[0],[0]
"κ c logpp{δq mn
at 1n sσσX κ
c logpnp{δq
n , (7)
||β̂t 1 β ||2 ¤1 a t 1 n 1 an 12 ?",3. Main Result,[0],[0]
sσσX κ,3. Main Result,[0],[0]
c,3. Main Result,[0],[0]
"logpp{δq mn
atnbn sσσX κ
c logpnp{δq
n , (8)
where
an 96sσσX κ
c logp2p{δq
n and
bn 24 ?",3. Main Result,[0],[0]
"sσσX κ
c logpnp{δq
n .
",3. Main Result,[0],[0]
"We can simplify the bound obtained in Theorem 3 by looking at the scaling with respect to n,m, s, and p, by treating κ, σ and σX as constants.",3. Main Result,[0],[0]
Suppose n,3. Main Result,[0],[0]
"Á s2 log p and set
λt c log p mn c log p n s c log p n t .
",3. Main Result,[0],[0]
The following error bounds hold for Algorithm 1: ||β̂t β ||1 ÀP s c log,3. Main Result,[0],[0]
"p
mn s
c log p
n
t 1 ,
||β̂t β ||2 ÀP c s log p mn c s log p n
s
c log p
n
t .
",3. Main Result,[0],[0]
"We can compare the above bounds to the performance of the local and centralized lasso (Wainwright, 2009; Meinshausen & Yu, 2009; Bickel et al., 2009).",3. Main Result,[0],[0]
"For β̂local, we have
||β̂local β ||1 ÀP s c log p
n
and
||β̂local β ||2 ÀP c s log p
n .
",3. Main Result,[0],[0]
"For β̂centralize, we have
||β̂centralize β ||1 ÀP s c log p
mn
and
||β̂centralize β ||2 ÀP c s log p
mn .
",3. Main Result,[0],[0]
"We see that after one round of communication, we have
||β̂1 β ||1 ÀP s c log p
mn s
2 log p
n
and
||β̂1 β ||2 ÀP c s log p
mn s
3{2 log p
n .
",3. Main Result,[0],[0]
These bounds match the results in Lee et al. (2015b) without expensive debiasing step.,3. Main Result,[0],[0]
"Furthermore, when m À
n s2 log p , they match the performance of the centralized lasso.",3. Main Result,[0],[0]
"Finally, as long as t Á logm and n Á s2 log p, it is easy to check that s b
log p n
t 1 À s b log p mn .",3. Main Result,[0],[0]
"There-
fore,
||β̂t 1 β ||1 ÀP s c log p
mn
and
||β̂t 1 β ||2 ÀP c s log p
mn ,
which matches the centralized lasso performance without additional error terms.",3. Main Result,[0],[0]
"That is, as long as n Á s2 log p, the rounds of communication to matches centralized procedure only increase logarithmically with the number of machines and independent of other parameters.",3. Main Result,[0],[0]
"Differently, for distributed learning methods studied in the literature for minimizing smooth objectives, the rounds of communication to match centralized procedure increase polynomially with m
(see table 1 in (Zhang & Xiao, 2015)).",3. Main Result,[0],[0]
"This is because here we exploit the underlying restricted strong convexity from empirical loss functions, while prior work on distributed minimization of smooth objectives (Shamir et al., 2014; Zhang & Xiao, 2015) only consider strong convexity explicitly from regularization.",3. Main Result,[0],[0]
"In order to establish Theorem 3, we prove an error bound on β̂",4. Generalized Theory and Proof Sketch,[0],[0]
"β for a general loss `p , q and β̂ obtained using Algorithm 1.",4. Generalized Theory and Proof Sketch,[0],[0]
"To simplify the presentation, we assume that the domain X is bounded and that the loss function `p , q is smooth.
",4. Generalized Theory and Proof Sketch,[0],[0]
Assumption 4.,4. Generalized Theory and Proof Sketch,[0],[0]
"The loss `p , q is L-smooth with respect to the second argument:
`1pa, bq `1pa, cq ¤ L|b c|, @a, b, c P R
Furthermore, |`3pa, bq| ¤M for all a, b P R.
Commonly used loss functions in statistical learning, including the squared loss for regression and logistic loss for classification, satisfy this assumption (Zhang et al., 2013b).
",4. Generalized Theory and Proof Sketch,[0],[0]
"Next, we state the restricted strong convexity condition for a general loss function (Negahban et al., 2012).
",4. Generalized Theory and Proof Sketch,[0],[0]
Assumption 5.,4. Generalized Theory and Proof Sketch,[0],[0]
There exists κ ¡ 0,4. Generalized Theory and Proof Sketch,[0],[0]
"such that for any ∆ P CpS, 3q
L1pβ ∆q L1pβ q x∇L1pβ q,∆y ¥ κ||∆||22,
with CpS, 3q t∆ P Rp|||∆Sc ||1 ¤ 3||∆S ||1u.
",4. Generalized Theory and Proof Sketch,[0],[0]
"The restricted strong convexity holds with high probability for a wide range of models and designs and it is commonly assumed for showing consistent estimation in highdimensions (see, for example, van de Geer & Bühlmann, 2009; Negahban et al., 2012;",4. Generalized Theory and Proof Sketch,[0],[0]
"Raskutti et al., 2010; Rudelson & Zhou, 2013, for details).
",4. Generalized Theory and Proof Sketch,[0],[0]
"Our main theoretical result establishes a recursive estimation error bound, which relates the estimation error ||β̂t 1 β",4. Generalized Theory and Proof Sketch,[0],[0]
|| to that of the previous iteration ||β̂t β ||1.,4. Generalized Theory and Proof Sketch,[0],[0]
Theorem 6.,4. Generalized Theory and Proof Sketch,[0],[0]
Suppose Assumption 4 and 5 holds.,4. Generalized Theory and Proof Sketch,[0],[0]
"Let
λt 1 2 1m ¸",4. Generalized Theory and Proof Sketch,[0],[0]
"jPrms ∇Ljpβ q 8
2L max j,i
||xji||28 ||β β̂t||1
c logp2p{δq
n
2M max j,i
||xji||38
||β̂t β ||21 .
",4. Generalized Theory and Proof Sketch,[0],[0]
"(9)
Then with probability at least 1 δ, we have
||β̂t 1 β ||1 ¤48s κ 1m ¸",4. Generalized Theory and Proof Sketch,[0],[0]
"jPrms ∇Ljpβ q 8
48sL κ max j,i ||xji||28 ||β β̂t||1
c logp2p{δq
n
48sM κ max j,i ||xji||38
||β̂t β ||21 ,
and ||β̂t 1 β ||2 ¤12 ?",4. Generalized Theory and Proof Sketch,[0],[0]
"s
κ
1m ¸",4. Generalized Theory and Proof Sketch,[0],[0]
"jPrms ∇Ljpβ q 8
12 ?",4. Generalized Theory and Proof Sketch,[0],[0]
"sL
κ
max j,i
||xji||28 ||β β̂t||1
c logp2p{δq
n
4 ?",4. Generalized Theory and Proof Sketch,[0],[0]
"sM
κ
max j,i
||xji||38
||β̂t β ||21 .
",4. Generalized Theory and Proof Sketch,[0],[0]
Theorem 6 upper bounds the estimation error ||β̂t 1 β ||1 as a function of ||β̂t β ||1.,4. Generalized Theory and Proof Sketch,[0],[0]
"Applying Theorem 6 iteratively, we immediately obtain the following estimation error bound which depends on the quality of local `1 regularized estimation ||β̂0 β ||1.",4. Generalized Theory and Proof Sketch,[0],[0]
Corollary 7.,4. Generalized Theory and Proof Sketch,[0],[0]
Suppose the conditions of Theorem 6 are satisfied.,4. Generalized Theory and Proof Sketch,[0],[0]
"Furthermore, suppose that for all t, we have
M max j,i
||xji||8 ||β̂t β ||1 ¤ L
c logp2p{δq
n .",4. Generalized Theory and Proof Sketch,[0],[0]
"(10)
Then with probability at least 1 δ, we have ||β̂t 1 β ||1 ¤ at 1n ||β̂0 β ||1 p1 anq 1p1 at 1n q 48s
κ 1m ¸",4. Generalized Theory and Proof Sketch,[0],[0]
"jPrms ∇Ljpβ q 8
and
||β̂t 1 β ||2 ¤ atnbn ||β̂0 β ||1",4. Generalized Theory and Proof Sketch,[0],[0]
p1 anq 1p1 at 1n q 12 ?,4. Generalized Theory and Proof Sketch,[0],[0]
"s
κ 1m ¸",4. Generalized Theory and Proof Sketch,[0],[0]
"jPrms ∇Ljpβ q 8 ,
where
an 96sL κ max j,i
||xji||28 c logp2p{δq",4. Generalized Theory and Proof Sketch,[0],[0]
"n
and
bn 24 ?",4. Generalized Theory and Proof Sketch,[0],[0]
"sL
κ
max j,i
||xji||28 c logp2p{δq",4. Generalized Theory and Proof Sketch,[0],[0]
"n .
",4. Generalized Theory and Proof Sketch,[0],[0]
For the quadratic loss we have that M 0 and the condition in (10) holds.,4. Generalized Theory and Proof Sketch,[0],[0]
"For other types of losses, condition in (10) will be true for t large enough when m Á s2, leading to local exponential rate of convergence until reaching statistical optimal region.",4. Generalized Theory and Proof Sketch,[0],[0]
We first analyze how the estimation error bound decreases after one round of communication.,4.1. Proof Sketch of Theorem 6,[0],[0]
"In particular, we bound ||β̂t 1 β",4.1. Proof Sketch of Theorem 6,[0],[0]
||,4.1. Proof Sketch of Theorem 6,[0],[0]
with ||β̂t β ||.,4.1. Proof Sketch of Theorem 6,[0],[0]
"Define
L̃1pβ, β̂tq L1pβq C 1
m ¸",4.1. Proof Sketch of Theorem 6,[0],[0]
"jPrms
∇Ljpβ̂tq ∇L1pβ̂tq,β G .
(11) Then
∇L̃1pβ, β̂tq ∇L1pβq 1 m ¸",4.1. Proof Sketch of Theorem 6,[0],[0]
"jPrms ∇Ljpβ̂tq ∇L1pβ̂tq.
",4.1. Proof Sketch of Theorem 6,[0],[0]
"The following lemma bounds the `8 norm of∇L̃1pβ, β̂tq.",4.1. Proof Sketch of Theorem 6,[0],[0]
Lemma 8.,4.1. Proof Sketch of Theorem 6,[0],[0]
"With probability at least 1 δ, we have ∇L̃1pβ , β̂tq
8
¤ 1m ¸",4.1. Proof Sketch of Theorem 6,[0],[0]
"jPrms ∇Ljpβ q 8
2L max j,i
||xji||28 ||β β̂t||1
c logp2p{δq
n
M max j,i
||xji||38
||β̂t β ||21 .
",4.1. Proof Sketch of Theorem 6,[0],[0]
The lemma bounds the magnitude of the gradient of the loss at optimum point β .,4.1. Proof Sketch of Theorem 6,[0],[0]
This will be used to guide our choice of the `1 regularization parameter λt 1 in (4).,4.1. Proof Sketch of Theorem 6,[0],[0]
"The following lemma shows that as long as λt 1 is large enough, it is guaranteed that β̂t 1 β is in a restricted cone.",4.1. Proof Sketch of Theorem 6,[0],[0]
Lemma 9.,4.1. Proof Sketch of Theorem 6,[0],[0]
"Suppose
λt 1{2 ¥ ∇L̃1pβ , β̂tq
8
.
",4.1. Proof Sketch of Theorem 6,[0],[0]
"Then with probability at least 1 δ, we have β̂t 1 β P CpS, 3q.",4.1. Proof Sketch of Theorem 6,[0],[0]
"Based on the conic condition and restricted strong convexity condition, we can obtain the recursive error bound stated in Theorem 6 following the proof strategy as in Negahban et al. (2012).
",4.1. Proof Sketch of Theorem 6,[0],[0]
"Applications Theorem 6 can be used to establish statistical guarantees for more general sparse learning problems, for example consider the logistic regression is a popular classification model where the binary label yji P t 1, 1u is drawn according to a Bernoulli distribution:
",4.1. Proof Sketch of Theorem 6,[0],[0]
Ppyji 1|xjiq,4.1. Proof Sketch of Theorem 6,[0],[0]
"exppyjixxji,β yq
exppyjixxji,β yq 1 , (12)
we can establish local exponential convergence when applying Algorithm 1 to estimate β in the high-dimensional logistic model.",4.1. Proof Sketch of Theorem 6,[0],[0]
"Section A in Appendix provide formal guarantees and more illustrative examples.
",4.1. Proof Sketch of Theorem 6,[0],[0]
m 5 m 10 m 20,4.1. Proof Sketch of Theorem 6,[0],[0]
In this section we present empirical comparisons between various approaches on both simulated and real world datasets 3.,5. Experiments,[0],[0]
"We run the algorithms for both distributed regression and classification problems, and compare with the following algorithms: i)",5. Experiments,[0],[0]
Local; ii) Centralize;,5. Experiments,[0],[0]
iii) Distributed proximal gradient descent (Prox GD);,5. Experiments,[0],[0]
"iv) AvgDebias (Lee et al., 2015b) with hard thresholding, and v) the proposed EDSL approach.",5. Experiments,[0],[0]
We first examine the algorithms on simulated data.,5.1. Simulations,[0],[0]
"We generate txjiujPrms,iPrns from a multivariate normal distribution with mean zero and covariance matrix",5.1. Simulations,[0],[0]
Σ.,5.1. Simulations,[0],[0]
The covariance Σ controls the condition number of the problem and we will varying it to see how the performance changes.,5.1. Simulations,[0],[0]
We set Σij 0.5|i j| for the well-conditioned setting and Σij 0.5|i j|{5 for the ill-conditioned setting.,5.1. Simulations,[0],[0]
"The response variable tyjiujPrms,iPrns are drawn from (5) and (12) for regression and classification problems, respectively.",5.1. Simulations,[0],[0]
"For regression, the noise ji is sampled from a standard normal distribution.",5.1. Simulations,[0],[0]
"The true model β is set to be s-sparse, where the first s-entries are sampled i.i.d.",5.1. Simulations,[0],[0]
"from a uniform distribution in r0, 1s,",5.1. Simulations,[0],[0]
"and the other entries are set
3Please refer to Section C in Appendix for full experimental results and more details
to zero.
",5.1. Simulations,[0],[0]
"We run experiments with various pn, p,m, sq settings4.",5.1. Simulations,[0],[0]
The estimation error ||β̂t β ||2 is shown versus rounds of communications for for Prox GD and the proposed EDSL algorithm.,5.1. Simulations,[0],[0]
"We also plot the estimation error of Local, AvgDebias, and Centralize as horizontal lines, since the communication cost is fixed for for these algorithms56.",5.1. Simulations,[0],[0]
"Figure 1 summarize the results, averaged across 10 independent trials.",5.1. Simulations,[0],[0]
"We have the following observations:
• The Avg-Debias approach obtained much better estimation error compared to Local after one round of communication and sometimes performed quite close to Centralize.",5.1. Simulations,[0],[0]
"However, in most cases, there is still a gap compared with Centralize, especially when the problem is not well-conditioned or m is large.
",5.1. Simulations,[0],[0]
"• ProxGD converges very slow when the condition number becomes bad (Σij 0.5|i j|{5 case).
",5.1. Simulations,[0],[0]
"• As theory suggests, EDSL obtained a solution that is 4n: sample size per machine, p: problem dimension, m: number of machines, s: true support size.",5.1. Simulations,[0],[0]
"5these algorithms have zero, one-shot and full communications, respectively.",5.1. Simulations,[0],[0]
"6To give some senses about computational cost, for a problem with n 200, p 1000, at each round EDSL takes about 0.048s, while Avg-Debias takes about 40.334s.
competitive with Avg-Debias after one round of communication.",5.1. Simulations,[0],[0]
"The estimation error decreases to match performance of Centralize within few rounds of communications; typically less than 5, even though the theory suggests EDSL will match the performance of centralize withinOplogmq rounds of communication.
",5.1. Simulations,[0],[0]
Above experiments illustrate our theoretical results in finite samples.,5.1. Simulations,[0],[0]
"As suggested by theory, when sample size per machine n is relatively small, one round of communication is not sufficient to make Avg-Debias matches the performance of centralized procedure.",5.1. Simulations,[0],[0]
"However, EDSL could match the performance of Avg-Debias with one round of communication and further improve the estimation quality by exponentially reducing the gap between centralized procedure with Avg-Debias, until matching the centralized performance.",5.1. Simulations,[0],[0]
"Thus, the proposed EDSL improves the AvgDebias approach both computationally and statistically.",5.1. Simulations,[0],[0]
"In this section, we compare the distributed sparse learning algorithms on several real world datasets.",5.2. Real-world Data Evaluation,[0],[0]
"For all data sets, we use 60% of data for training, 20% as held-out validation set for tuning the parameters, and the remaining 20% for testing.",5.2. Real-world Data Evaluation,[0],[0]
We randomly partition data 10 times and report the average performance on the test set.,5.2. Real-world Data Evaluation,[0],[0]
"For regression tasks, the evaluation metric is the normalized Mean Squared Error (normalized MSE), while for classification tasks we report the miss-classification error.",5.2. Real-world Data Evaluation,[0],[0]
We randomly partition the data on m 10 machines.,5.2. Real-world Data Evaluation,[0],[0]
"A subset of the results are plotted in Figure 2 where for some data sets the performance of Avg-Debias is significantly worse than others (mostly because the debiasing step fails), thus we omit these plots.
",5.2. Real-world Data Evaluation,[0],[0]
"Since there is no well-specified model on these datasets, the curves behave quite differently on different data sets.",5.2. Real-world Data Evaluation,[0],[0]
"However, a large gap between the local and centralized procedure is consistent as the later uses 10 times more data.",5.2. Real-world Data Evaluation,[0],[0]
"AvgDebias often fails on these real datasets and performs much worse than in the simulations, the main reason might be that
the assumptions, such as well-specified model or generalized coherence condition, fail, then Avg-Debias can totally fail and produce solution even much worse than the local.",5.2. Real-world Data Evaluation,[0],[0]
"Nevertheless, the proposed EDSL performs quite robust on real world data sets, and can often output a solution which is highly competitive with the centralized model within a few rounds of communications.",5.2. Real-world Data Evaluation,[0],[0]
We also observed a slight “zig-zag” behavior for EDSL approach on some data sets.,5.2. Real-world Data Evaluation,[0],[0]
"For example, on the mushrooms data set, the predictive performance of EDSL is not stable.",5.2. Real-world Data Evaluation,[0],[0]
"In sum, the experimental results on real world data sets verified that the proposed EDSL method is effective for distributed sparse learning problems.",5.2. Real-world Data Evaluation,[0],[0]
"We proposed a novel approach for distributed learning with sparsity, which is efficient in both computation and communication.",6. Conclusion and Discussion,[0],[0]
Our theoretical analysis showed that the proposed method works under weaker conditions than AvgDebias estimator while matches its error bound with oneround communication.,6. Conclusion and Discussion,[0],[0]
"Furthermore, the estimation error can be improved with a logarithmic more rounds of communication until matching the centralized procedure.",6. Conclusion and Discussion,[0],[0]
"Experiments on both simulated and real-world data demonstrate that the proposed method significantly improves the performance over one shot averaging approaches, and matches the centralized procedure with few iterations.
",6. Conclusion and Discussion,[0],[0]
There might be several ways to improve this work.,6. Conclusion and Discussion,[0],[0]
"As we see in real data experiments, the proposed approach can still perform slightly worse than the centralized approach on certain datasets.",6. Conclusion and Discussion,[0],[0]
It is interesting to explore how to make EDSL provably work under even weaker assumptions.,6. Conclusion and Discussion,[0],[0]
"For example, EDSL requires Ops2 log pq samples per machine to match the centralized method in Oplogmq rounds of communications, however, it is not clear whether the sample size requirement can be improved, while still maintaining low-communication cost.",6. Conclusion and Discussion,[0],[0]
"Last but not the least, it is interesting to explore presented ideas to improve the computational cost of communication-efficient distributed multitask learning with shared support (Wang et al., 2015).",6. Conclusion and Discussion,[0],[0]
"We propose a novel, efficient approach for distributed sparse learning with observations randomly partitioned across machines.",abstractText,[0],[0]
"In each round of the proposed method, worker machines compute the gradient of the loss on local data and the master machine solves a shifted `1 regularized loss minimization problem.",abstractText,[0],[0]
"After a number of communication rounds that scales only logarithmically with the number of machines, and independent of other parameters of the problem, the proposed approach provably matches the estimation error bound of centralized methods.",abstractText,[0],[0]
Efficient Distributed Learning with Sparsity,title,[0],[0]
Learning the representations that respect the pairwise relationships is one of the most important problems in machine learning and pattern recognition with vast applications.,1. Introduction,[0],[0]
"To this end, deep metric learning methods (Hadsell et al., 2006; Weinberger et al., 2006; Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017; Bell & Bala, 2015; Sener et al., 2016) aim to learn an embedding representation space such that similar data are close to each other
1Department of Computer Science and Engineering, Seoul National University, Seoul, Korea.",1. Introduction,[0],[0]
"Correspondence to: Hyun Oh Song <hyunoh@snu.ac.kr>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
and vice versa for dissimilar data.",1. Introduction,[0],[0]
"Some of these methods have shown significant advances in various applications in retrieval (Sohn, 2016; Schroff et al., 2015), clustering (Song et al., 2017), domain adaptation (Sener et al., 2016), video understanding (Wang & Gupta, 2015), etc.
",1. Introduction,[0],[0]
"Despite recent advances in deep metric learning methods, deploying the learned embedding representation in large scale applications poses great challenges in terms of the inference efficiency and scalability.",1. Introduction,[0],[0]
"To address this, practitioners in large scale retrieval and recommendation systems often resort to a separate post-processing step where the learned embedding representation is run through quantization pipelines such as sketches, hashing, and vector quantization in order to significantly reduce the number of data to compare during the inference while trading off the accuracy.
",1. Introduction,[0],[0]
"In this regard, we propose a novel end-to-end learning method for quantizable representations jointly optimizing for the quality of the network embedding representation and the performance of the corresponding binary hash code.",1. Introduction,[0],[0]
"In contrast to some of the recent methods (Cao et al., 2016; Liu & Lu, 2017), our proposed method avoids ever having to cluster the entire dataset, offers the modularity to accommodate any existing deep embedding learning techniques (Schroff et al., 2015; Sohn, 2016), and is efficiently trained in a mini-batch stochastic gradient descent setting.",1. Introduction,[0],[0]
We show that the discrete optimization problem of finding the optimal binary hash codes given the embedding representations can be computed efficiently and exactly by solving an equivalent minimum cost flow problem.,1. Introduction,[0],[0]
"The proposed method alternates between finding the optimal hash codes of the given embedding representations in the mini-batch and adjusting the embedding representations indexed at the activated hash code dimensions via deep metric learning methods.
",1. Introduction,[0],[0]
"Our end-to-end learning method outperforms state-of-theart deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) in retrieval and clustering tasks on the Cifar-100 (Krizhevsky et al., 2009) and the ImageNet (Russakovsky et al., 2015) datasets while providing up to several orders of magnitude speedup during inference.",1. Introduction,[0],[0]
"Our method utilizes efficient off-the-shelf implementations from OR-Tools (Google optimization tools for combinatorial optimization problems) (OR-tools, 2018), the deep metric learning library implementation in Tensorflow (Abadi et al., 2015),
ar X
iv :1
80 5.
05 80
9v 3
[ cs
.L G
] 1
2 Ju
n 20
18
and is efficient to train.",1. Introduction,[0],[0]
"The state of the art deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) use the class labels during training (for the hard negative mining procedure) and since our method utilizes the approaches as a component, we focus on the settings where the class labels are available during training.",1. Introduction,[0],[0]
Learning the embedding representation via neural networks dates back to over two decades ago.,2. Related works,[0],[0]
"Starting with Siamese networks (Bromley et al., 1994; Hadsell et al., 2006), the task is to learn embedding representation of the data using neural networks so that similar examples are close to each other and dissimilar examples are farther apart in the embedding space.",2. Related works,[0],[0]
"Despite the recent successes and near human performance reported in retrieval and verification tasks (Schroff et al., 2015), most of the related literature on learning efficient representation via deep learning focus on learning the binary hamming codes for finding nearest neighbors with linear search over entire dataset per each query.",2. Related works,[0],[0]
"Directly optimizing for the embedding representations in deep networks for quantization codes and constructing the hash tables for significant search reduction in the number of data is much less studied.
",2. Related works,[0],[0]
"(Xia et al., 2014) first precompute the hash code based on the labels and trains the embedding to be similar to the hash code.",2. Related works,[0],[0]
"(Zhao et al., 2015) apply element-wise sigmoid on the embedding and minimizes the triplet loss.",2. Related works,[0],[0]
"(Norouzi et al., 2012) optimize the upper bound on the triplet loss defined on hamming code vectors.",2. Related works,[0],[0]
"(Liong et al., 2015) minimize the difference between the original and the signed version of the embedding with orthogonality regularizers in a network.",2. Related works,[0],[0]
"(Li et al., 2017) employ discrete cyclic coordinate descent (Shen et al., 2015) on a discrete sub-problem optimizing one hash bit at a time but the algorithm has neither the convergence guarantees nor the bound on the number of iterations.",2. Related works,[0],[0]
"All of these methods focus on learning the binary codes for hamming distance ranking and perform an exhaustive linear search over the entire dataset which is not likely to be suitable for large scale problems.
",2. Related works,[0],[0]
"(Cao et al., 2016) minimize the difference between the similarity label and the cosine distance of network embedding.",2. Related works,[0],[0]
"(Liu & Lu, 2017) define a distance between a quantized data and continuous embedding, and back-propagates the metric loss error only with respect to the continuous embedding.",2. Related works,[0],[0]
Both of these methods require repeatedly running k-means clustering on the entire dataset while training the network at the same time.,2. Related works,[0],[0]
"This is unlikely to be practical for large scale problems because of the prohibitive computational complexity and having to store the cluster centroids for all classes in the memory as the number of classes becomes extremely large (Prabhu & Varma, 2014; Choromanska et al., 2013).
",2. Related works,[0],[0]
"In this paper, we propose an efficient end-to-end learning method for quantizable representations jointly optimizing the quality of the embedding representation and the performance of the corresponding hash codes in a scalable mini-batch stochastic gradient descent setting in a deep network and demonstrate state of the art search accuracy and quantitative search efficiency on multiple datasets.",2. Related works,[0],[0]
"Consider a hash function r(x) that maps an input data x ∈ X onto a d dimensional binary compound hash code h ∈ {0, 1}d with the constraint that k out of d total bits needs to be set.",3. Problem formulation,[0],[0]
"We parameterize the mapping as
r(x) = argmin h∈{0,1}d
−f(x;θ)ᵀh
subject to ||h||1 = k, (1)
where f(·,θ) : X → Rd is a transformation (i.e. neural network) differentiable with respect to the parameter θ and takes the input x and emits the d dimensional embedding vector.",3. Problem formulation,[0],[0]
"Given the hash function r(·), we define a hash table H which is composed of d buckets with each bucket indexed by a compound hash code h.",3. Problem formulation,[0],[0]
"Then, given a query xq , union of all the items in the buckets indexed by k active bits in r(xq) is retrieved as the candidates of the approximate nearest items of xq .",3. Problem formulation,[0],[0]
"Finally, this is followed by a reranking operation where the retrieved items are ranked according to the distances computed using the original embedding representation f(·;θ).
",3. Problem formulation,[0],[0]
"Note, in quantization based hashing (Wang et al., 2016; Cao et al., 2016), a set of prototypes or cluster centroids are first computed via dictionary learning or other clustering (i.e. k-means) algorithms.",3. Problem formulation,[0],[0]
"Then, the function f(x;θ) is represented by the indices of k-nearest prototypes or centroids.",3. Problem formulation,[0],[0]
"Concretely, if we replace f(x;θ) in Equation (1) with the negative distances of the input item x with respect to all d prototypes or centroids, [−||x− c1||2, . . .",3. Problem formulation,[0],[0]
",−||x− cd||2]ᵀ, then the corresponding hash function r(x) can be used to build the hash table.
",3. Problem formulation,[0],[0]
"In contrast to most of the recent methods that learn a hamming ranking in a neural network and perform exhaustive linear search over the entire dataset (Xia et al., 2014; Zhao et al., 2015; Norouzi et al., 2012; Li et al., 2017), quantization based methods, have guaranteed search inference speed up by only considering a subset of k out d buckets and thus avoid exhaustive linear search.",3. Problem formulation,[0],[0]
"We explicitly maintain the sparsity constraint on the hash code in Equation (1) throughout our optimization without continuous relaxations to inherit the efficiency aspect of quantization based hashing and this is one of the key attributes of the algorithm.
",3. Problem formulation,[0],[0]
"Although quantization based hashing is known to show high search accuracy and search efficiency (Wang et al., 2016),
running the quantization procedure on the entire dataset to compute the cluster centroids is computationally very costly and requires storing all of the cluster centroids in the memory.",3. Problem formulation,[0],[0]
"Our desiderata are to formulate an efficient end-to-end learning method for quantizable representations which (1) guarantees the search efficiency by avoiding linear search over the entire data, (2) can be efficiently trained in a minibatch stochastic gradient descent setting and avoid having to quantize the entire dataset or having to store the cluster centroids for all classes in the memory, and (3) offers the modularity to accommodate existing embedding representation learning methods which are known to show the state of the art performance on retrieval and clustering tasks.",3. Problem formulation,[0],[0]
We formalize our proposed method in Section 4.1 and discuss the subproblems in Section 4.2 and in Section 4.4.,4. Methods,[0],[0]
Finding the optimal set of embedding representations and the corresponding hash codes is a chicken and egg problem.,4.1. End-to-end learning for quantizable representations,[0],[0]
"Embedding representations are required to infer which k activation dimensions to set in the corresponding binary hash code, but the binary hash codes are needed to adjust the embedding representations indexed at the activated bits so that similar items get hashed to the same buckets and vice versa.",4.1. End-to-end learning for quantizable representations,[0],[0]
"We formalize this notion in Equation (2) below.
minimize θ
h1,...,hn `metric({f(xi;θ)}ni=1;h1, . . .",4.1. End-to-end learning for quantizable representations,[0],[0]
",hn)︸ ︷︷ ︸ embedding representation quality +
γ  n∑ i",4.1. End-to-end learning for quantizable representations,[0],[0]
"−f(xi;θ)ᵀhi + n∑ i ∑ j:yj 6=yi hᵀiPhj  ︸ ︷︷ ︸
hash code performance
subject to hi ∈ {0, 1}d, ||hi||1 = k, ∀i, (2)
where the matrix P encodes the pairwise cost for the hash code similarity between each negative pair and γ is the trade-off hyperparameter balancing the loss contribution between the embedding representation quality given the hash codes and the performance of the hash code with respect to the embedding representations.",4.1. End-to-end learning for quantizable representations,[0],[0]
"We solve this optimization problem via alternating minimization through iterating over solving for k-sparse binary hash codes h1, . . .",4.1. End-to-end learning for quantizable representations,[0],[0]
",hn and updating the parameters of the deep network θ for the continuous embedding representations per each mini-batch.",4.1. End-to-end learning for quantizable representations,[0],[0]
Following subsections discuss these two steps in detail.,4.1. End-to-end learning for quantizable representations,[0],[0]
"Given a set of continuous embedding representations {f(xi;θ)}ni=1, we seek to solve the following subproblem in Equation (3) where the task is to (unary) select k as large elements of the each embedding vector as possible, while (pairwise) selecting as orthogonal elements as pos-
sible across different classes.",4.2. Learning the compound hash code,[0],[0]
"The unary term mimics the hash function r(x) in Equation (1) and the pairwise term has the added benefit that it also provides robustness to the optimization especially during the early stages of the training when the embedding representation is not very accurate.
minimize h1,...,hn n∑ i −f(xi;θ)ᵀhi + n∑ i ∑ j:yj 6=yi
hᵀiPhj︸ ︷︷ ︸",4.2. Learning the compound hash code,[0],[0]
:,4.2. Learning the compound hash code,[0],[0]
"= g(h1,...,n;θ)
subject to hi ∈ {0, 1}d, ||hi||1 = k, ∀i, (3)
",4.2. Learning the compound hash code,[0],[0]
"However, solving for the optimal solution of the problem in Equation (3) is NP-hard in general even for the simple case where k = 1 and d > 2",4.2. Learning the compound hash code,[0],[0]
"(Boykov et al., 2001).",4.2. Learning the compound hash code,[0],[0]
"Thus, we construct a upper bound function ḡ(h1,...,n;θ) to the objective function g(h1,...,n;θ) which we argue that it can be exactly optimized by establishing the connection to a network flow algorithm.",4.2. Learning the compound hash code,[0],[0]
The upper bound function is a slightly reparameterized discrete objective where we optimize the hash codes over the average embedding vectors per each class instead.,4.2. Learning the compound hash code,[0],[0]
"We first rewrite1 the objective function by indexing over each class and then over each data per class and derive the upper bound function as shown below.
",4.2. Learning the compound hash code,[0],[0]
"g(h1,...,n;θ)",4.2. Learning the compound hash code,[0],[0]
=,4.2. Learning the compound hash code,[0],[0]
"nc∑ i ∑ k:yk=i −f(xk;θ)ᵀhk + nc∑ i ∑ k:yk=i, l:yl 6=i hᵀkPhl
≤ nc∑",4.2. Learning the compound hash code,[0],[0]
"i ∑ k:yk=i −cᵀihk + nc∑ i ∑ k:yk=i, l:yl 6=i hᵀkPhl
+ maximize h1,...,hn
hi∈{0,1}d,||hi||1=k
nc∑ i=1",4.2. Learning the compound hash code,[0],[0]
∑,4.2. Learning the compound hash code,[0],[0]
"k:yk=i (ci − f(xk;θ))ᵀhk
︸ ︷︷ ︸ :=M(θ)
= ḡ(h1,...,n;θ) (4)
where nc denotes the number of classes in the mini-batch, m = |{k : yk = i}|, and ci = 1m ∑ k:yk=i
f(xk;θ).",4.2. Learning the compound hash code,[0],[0]
"Here, w.l.o.g we assume each class has m number of data in the mini-batch (i.e. Npairs (Sohn, 2016) mini-batch construction).",4.2. Learning the compound hash code,[0],[0]
"The last term in upper bound, denoted as M(θ), is constant with respect to the hash codes and is non-negative.",4.2. Learning the compound hash code,[0],[0]
"Note, from the bound in Equation (4), the gap between the minimum value of g and the minimum value of ḡ is bounded above by M(θ).",4.2. Learning the compound hash code,[0],[0]
"Furthermore, since this value corresponds to the maximum deviation of an embedding vector from its class mean of the embedding, the bound gap decreases over iterations as we update the network parameter θ to attract similar pairs of data and vice versa for dissimilar pairs in the other embedding subproblem (more details in Section 4.4).
",4.2. Learning the compound hash code,[0],[0]
"Moreover, minimizing the upper bound over each hash codes {hi}ni=1 is equivalent to minimizing a reparameter-
1We also omit the dependence of the index",4.2. Learning the compound hash code,[0],[0]
"i for each hk and hl to avoid the notation clutter.
ization ĝ(z1,...,nc ;θ) over {zi} nc i=1 defined below because for a given class label i, each hk shares the same ci vector.
minimize h1,...,hn
hi∈{0,1}d,||hi||1=k
nc∑ i ∑ k:yk=i −cᵀihk +",4.2. Learning the compound hash code,[0],[0]
nc∑,4.2. Learning the compound hash code,[0],[0]
"i ∑ k:yk=i, l:yl 6=i hᵀkPhl
= minimize z1,...,znc
zi∈{0,1}d,||zi||1=k
m  nc∑",4.2. Learning the compound hash code,[0],[0]
i −cᵀi,4.2. Learning the compound hash code,[0],[0]
zi + nc∑,4.2. Learning the compound hash code,[0],[0]
"i ∑ j 6=i zᵀiP ′zj  ︸ ︷︷ ︸
:= ĝ(z1,...,nc ;θ)
,
where P ′ = mP .",4.2. Learning the compound hash code,[0],[0]
"Therefore, we formulate the following optimization problem below whose objective upper bounds the original objective in Equation (3) over all feasible hash codes {hi}ni=1.
minimize z1,...,znc nc∑ i −cᵀi",4.2. Learning the compound hash code,[0],[0]
zi + ∑,4.2. Learning the compound hash code,[0],[0]
"i,j 6=i zᵀiPzj
subject to zi ∈ {0, 1}d, ||zi||1 = k, ∀i (5)
",4.2. Learning the compound hash code,[0],[0]
"In the upper bound problem above, we consider the case where the pairwise cost matrix P is a diagonal matrix of non-negative values 2.",4.2. Learning the compound hash code,[0],[0]
Theorem 1 in the next subsection proves that finding the optimal solution of Equation (5) is equivalent to finding the minimum cost flow solution of the flow network G′ illustrated in Figure 2 which can be solved efficiently and exactly in polynomial time.,4.2. Learning the compound hash code,[0],[0]
"In practice, we use the efficient implementations from OR-Tools (Google Optimization Tools for combinatorial optimization problems) (OR-tools, 2018) to solve the minimum cost flow problem per each mini-batch.",4.2. Learning the compound hash code,[0],[0]
Theorem 1.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"The optimization problem in Equation (5) can be solved by finding the minimum cost flow solution on the flow network G’.
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
Proof.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Suppose we construct a complete bipartite graph G = (A ∪ B,E) and create a directed graph G′ =",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"(A ∪ B ∪ {s, t}, E′) from G by adding source s and sink t and directing all edges E in G from A to B. We also add edges from s to each vertex ap ∈ A.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"For each vertex bq ∈ B, we add nc number of edges to t. Edges incident to s have capacity u(s, ap) = k and cost v(s, ap) = 0.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
The edges between ap ∈,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
A,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"and bq ∈ B have capacity u(ap, bq) = 1 and cost v(ap, bq) = −cp[q].",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Each edge r ∈ {0, . . .",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
", nc",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"− 1} from bq ∈ B to t has capacity u((bq, t)r) = 1 and cost v((bq, t)r) = 2λqr.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Figure 2 illustrates the flow network G′. The amount of flow to be sent from s to t is nck.
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Then we define the flow {fz(e)}e∈E′ , indexed both by (1) a given configuration of z1, . . .",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
", znc where each zi ∈
2Note that we absorb the scaler factor m from the definition of P ′ and redefine P = diag(λ1, . . .",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
", λd).
{0, 1}d, ||zi||1 = k, ∀i, and by (2) the edges of G′, below:
(i) fz(s, ap) = k, (ii) fz(ap, bq) = zp[q],
(iii) fz((bq, t)r)",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"=
{ 1 for r < ∑nc p=1 zp[q]
0 otherwise (6)
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
We first show the flow fz defined above is feasible for G′.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"The capacity constraints are satisfied by construction in Equation (6), so we only need to check the flow conservation conditions.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"First, the amount of input flow at s is nck and the output flow from s is ∑ ap∈A fz(s, ap) = ∑ ap∈A k = nck which is equal.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
The amount of input flow to each vertex ap ∈,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"A is given as k and the output flow is∑
bq∈B fz(ap, bq) = ∑d q zp[q] = ||zp||1 = k.
Let us denote the amount of input flow at a vertex bq ∈ B as yq = ∑nc p zp[q].",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"The output
flow from the vertex bq is ∑nc−1
r=0 fz((bq, t)r) =∑yq−1 r=0",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"fz((bq, t)r)",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"+ ∑nc−1 r=yq
fz((bq, t)r) =",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
yq from Equation (6) (iii).,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"The last condition to check is that the amount of input flow at t is equal to the output flow at s.∑
bq∈B ∑nc−1 r=0 fz((bq, t)r) = ∑d q=1 yq = ∑ q,p zp[q] = nck.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"This shows the construction of the flow {fz(e)}e∈E′ in Equation (6) is valid in G′.
Now denote {fo(e)}e∈E′ as the minimum cost flow solution of the flow network G′ which minimizes the total cost ∑ e∈E′ v(e)fo(e).",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
Denote the optimal flow from a vertex ap ∈,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"A to a vertex bq ∈ B as z′p[q] := fo(ap, bq).",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"By the optimality of the flow {fo(e)}e∈E′ , we have that ∑ e∈E′ v(e)fo(e) ≤ ∑ e∈E′ v(e)fz(e).",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"By
Lemma 1, the lhs of the inequality is equal to∑ p−cᵀpz′p + ∑ p1 6=p2 z ′ᵀ p1Pz ′",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
p2 .,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Also, by Lemma 2,
the rhs is equal to ∑ p−cᵀpzp + ∑ p1 6=p2 z ᵀ p1Pzp2 .",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Finally, we have that ∑ p−cᵀpz′p + ∑ p1 6=p2 z ′ᵀ p1Pz ′",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"p2 ≤∑
p−cᵀpzp + ∑ p1 6=p2 z ᵀ p1Pzp2 ,∀{zp}.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Thus, we have proved that finding the minimum cost flow solution on the flow network G′ and translating the flows between each vertices between A and B as {z′p}, we can find the optimal solution to the optimization problem in Equation (5).
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
Lemma 1.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"For the minimum cost flow {fo(e)}e∈E′ of the network G′, we have that the total cost is∑
e∈E′ v(e)fo(e) = ∑ p−cᵀpz′p + ∑ p1 6=p2 z ′ᵀ p1Pz ′",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"p2 .
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
Proof.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"The total minimum cost ∑
e∈E′ v(e)fo(e) is broken down as
∑ e∈E′ v(e)fo(e) = ∑ ap∈A
v(s, ap)fo(s, ap)︸ ︷︷ ︸ flow from s to A +
∑ ap∈A ∑ bq∈B
v(ap, bq)fo(ap, bq)︸ ︷︷ ︸ flow from A to B
+ ∑ bq∈B nc−1∑ r=0
v((bq, t)r)fo((bq, t)r)︸ ︷︷ ︸ flow from B to t
Denote the amount of input flow at a vertex bq given the minimum cost flow {fo(e)}e∈E′ as y′q = ∑ p fo(ap, bq) =∑nc
p z ′ p[q].",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"From the cost definition at the edges between bq and t, v((bq, t)r) = 2λqr, and by the optimality of the minimum cost flow, we have that fo((bq, t)r) = 1 ∀r < y′q",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"and fo((bq, t)r) = 0 ∀r",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
≥,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
y′q .,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Thus, the total cost is∑ e∈E′ v(e)fo(e) = 0 + nc∑ p d∑ q",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
−cp[q]z′p[q] + ∑ bq∈B y′q−1∑ r=0,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"2λqr
= ∑ p −cᵀpz′p + ∑",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
q λqy ′,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
q(y ′ q,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"− 1)
= ∑ p −cᵀpz′p + ∑ q λqy ′",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
q 2,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
− ∑ p ∑ q λqz ′,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"p[q]
= ∑ p −cᵀpz′p + ∑ p z′p ᵀ P ∑ p z′p − ∑ p z′ᵀp",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Pz ′ p
= ∑ p −cᵀpz′p + ∑
p1 6=p2
z′ᵀp1Pz ′",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"p2 (7)
Lemma 2.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"For the {fz(e)}e∈E′ defined as Equation (6) of the network G′, we have that the total cost is∑
e∈E′ v(e)fz(e) = ∑ p−cᵀpzp + ∑ p1 6=p2 z ᵀ p1Pzp2 .
",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
Proof.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
The proof is similar to Lemma 1 except that we use the definition of the flow {fz(e)}e∈E′ in Equation (6) (iii) to reduce the cost of the flow from B to t to ∑y′q−1 r=0,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"2λqr.
Time complexity For λq nc, note that the worst case time complexity of finding the minimum cost flow (MCF) solution in the network G′
is O ( (nc + d) 2 ncd log (nc + d) )",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"(Goldberg & Tarjan,
1990).",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"In practice, however, it has been shown that implementation heuristics such as price updates, price refinement, push-look-ahead, (Goldberg, 1997) and set-relabel (Bünnagel et al., 1998) methods drastically improve the reallife performance.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"Also, we emphasize again that we solve the minimum cost flow problem only within the mini-batch not on the entire dataset.",4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
We benchmarked the wall clock running time of the method at varying sizes of nc and d and observed approximately linear time complexity in nc and d. Figure 1 shows the benchmark wall clock run time results.,4.3. Equivalence of problem 5 to minimum cost flow,[0],[0]
"As the hash codes become more and more sparse, it becomes increasingly likely for hamming distances defined on binary codes (Norouzi et al., 2012; Zhao et al., 2015) to become zero regardless of whether the input pair of data is similar or dissimilar.",4.4. Learning the embedding,[0],[0]
This phenomenon can be problematic when trained in a deep network because the back-propagation gradient would become zero and thus the embedding representations would not be updated at all.,4.4. Learning the embedding,[0],[0]
"In this regard, we propose a distance function based on gated residual as shown in Equation (8).",4.4. Learning the embedding,[0],[0]
This parameterization outputs zero distance only if the embedding representations of the two input data are identical at all the hash code activations.,4.4. Learning the embedding,[0],[0]
"Concretely, given a pair of embedding vectors f(xi;θ), f(xj ;θ) and the corresponding binary k-sparse hash codes hi,hj ,
we define the following distance function d hashij between the embedding vectors
d hashij =",4.4. Learning the embedding,[0],[0]
||,4.4. Learning the embedding,[0],[0]
"(hi ∨ hj) (f(xi;θ)− f(xj ;θ)) ||1, (8)
where ∨ denotes the logical or operation of the two binary hash codes and denotes the element-wise multiplication.",4.4. Learning the embedding,[0],[0]
"Then, using the distance function above, we can define the following subproblems using any existing deep metric learning methods (Weinberger et al., 2006; Schroff et al., 2015; Song et al., 2016; Sohn, 2016; Song et al., 2017).",4.4. Learning the embedding,[0],[0]
"Given a set of binary hash codes {hi}ni=1, we seek to solve the following subproblems where the task is to optimize the embedding representations so that similar pairs of data get hashed to the same buckets and dissimilar pairs of data get hashed to different buckets.",4.4. Learning the embedding,[0],[0]
"In other words, we need similar pairs of data to have similar embedding representations indexed at the activated hash code dimensions and vice versa.",4.4. Learning the embedding,[0],[0]
"In terms of the hash code optimization in Equation (4), updating the network weight has the effect of tightening the bound gap M(θ).
",4.4. Learning the embedding,[0],[0]
"Equation (9) and Equation (10) show the subproblems defined on the distance function above using Triplet (Schroff et al., 2015) and Npairs (Sohn, 2016) method respectively.",4.4. Learning the embedding,[0],[0]
"We optimize these embedding subproblems by updating the network parameter θ via stochastic gradient descent using the subgradients ∂`metric(θ;h1,...,n)∂θ given the hash codes per each mini-batch.
minimize θ
1 |T | ∑
(i,j,k)∈T [d hashij + α− d hashik ]",4.4. Learning the embedding,[0],[0]
"+︸ ︷︷ ︸ `triplet(θ; h1,...,n)
subject to ||f(x;θ)||2 = 1, (9)
",4.4. Learning the embedding,[0],[0]
"where T = {(xi,x+i ,x",4.4. Learning the embedding,[0],[0]
"− i )}i is the set of triplets (Schroff
et al., 2015), and the embedding vectors are normalized onto unit hypersphere ||f(x;θ)||2 = 1, ∀x ∈ X .",4.4. Learning the embedding,[0],[0]
"We also apply the semi-hard negative mining procedure (Schroff et al., 2015) where hard negatives farther than the distance between the anchor and positives are mined within the mini-batch.",4.4. Learning the embedding,[0],[0]
"In practice, since our method can be applied to any deep metric learning methods, we use existing deep metric learning implementations available in tf.contrib.losses.metric learning.",4.4. Learning the embedding,[0],[0]
"Similarly, we could also employ npairs (Sohn, 2016) method,
minimize θ −1 |P| ∑ (i,j)∈P log exp ( −d hashij ) exp ( −d hashij )",4.4. Learning the embedding,[0],[0]
+ ∑,4.4. Learning the embedding,[0],[0]
k:yk 6=yi exp ( −d hashik ) ︸,4.4. Learning the embedding,[0],[0]
"︷︷ ︸
`npairs(θ; h1,...,n)
+ λ
m ∑ i ||f(xi;θ)||22, (10)
where the npairs mini-batch B is constructed with positive pairs (x,x+) which are negative with respect to all
other pairs.",4.4. Learning the embedding,[0],[0]
"B = {(x1,x+1 ), . . .",4.4. Learning the embedding,[0],[0]
", (xn,x+n ))} and P denotes the set of all positive pairs within the mini-batch.",4.4. Learning the embedding,[0],[0]
We use the existing implementation of npairs loss in Tensorflow as well.,4.4. Learning the embedding,[0],[0]
"Note that even though the distance d hashij is defined after masking the embeddings with the union binary vector (hi ∨ hj), it’s important to normalize or regularize the embedding representation before the masking operations for the optimization stability due to the sparse nature of the hash codes.
",4.4. Learning the embedding,[0],[0]
"Algorithm 1 Learning algorithm input θembb (pretrained metric learning base model); θd ∈ Rd initialize θf = [θb,θd] 1: for t = 1, . . .",4.4. Learning the embedding,[0],[0]
", MAXITER do 2: Sample a minibatch {xj} 3: Update the flow networkG′ by recomputing the cost vectors
for all classes in the minibatch ci =
1 m ∑ k:yk=i f(xk;θf )
4: Compute the hash codes {hi} minimizing Equation (5) via finding the minimum cost flow on G′ 5: Update the network parameter given the hash codes θf ← θf − η(t)∂`metric(θf ; h1,...,nc)/∂θf 6: Update stepsize η(t) ← ADAM rule (Kingma & Ba, 2014) 7: end for
output θf (final estimate);",4.4. Learning the embedding,[0],[0]
"In this subsection, we examine the expectation and the variance of the query time speed up over linear search.",4.5. Query efficiency analysis,[0],[0]
"Recall the properties of the compound hash code defined in Section 3, h ∈ {0, 1}d and ||h||1 =",4.5. Query efficiency analysis,[0],[0]
k.,4.5. Query efficiency analysis,[0],[0]
"Given n such hash codes, we have that Pr(hᵀi hj = 0) =",4.5. Query efficiency analysis,[0],[0]
( d−k k ) /,4.5. Query efficiency analysis,[0],[0]
"( d k
) assuming the hash code uniformly distributes the items throughout different buckets.",4.5. Query efficiency analysis,[0],[0]
"For a given hash code hq, the number of retrieved data is Nq = ∑ i 6=q 1(h",4.5. Query efficiency analysis,[0],[0]
ᵀ,4.5. Query efficiency analysis,[0],[0]
i hq 6= 0).,4.5. Query efficiency analysis,[0],[0]
"Then, the expected number of retrieved data is E[Nq] = (n−1) ( 1− ( d−k k ) /",4.5. Query efficiency analysis,[0],[0]
( d k )) .,4.5. Query efficiency analysis,[0],[0]
"Thus, in contrast to linear search, the expected speedup factor (SUF) under perfectly uniform distribution of the hash code is
E[SUF] = ( 1− ( d−k k )( d k ) )−1",4.5. Query efficiency analysis,[0],[0]
(11),4.5. Query efficiency analysis,[0],[0]
"In the case where d k, the speedup factor approaches(
d k2 ) .",4.5. Query efficiency analysis,[0],[0]
"Similarly, we have that the variance is V [Nq] =
(n− 1) ( 1− ( d−k k ) /",4.5. Query efficiency analysis,[0],[0]
( d k ))( d−k k ) /,4.5. Query efficiency analysis,[0],[0]
( d k ) .,4.5. Query efficiency analysis,[0],[0]
"Network architecture In our experiments, we used the NIN (Lin et al., 2013) architecture (denote the parameters as θb) with leaky relu (Xu et al., 2015) with α = 5.5 as activation function and trained Triplet embedding network with semi-hard negative mining (Schroff et al., 2015) and
Npairs network (Sohn, 2016) from scratch as the base model.",5. Implementation details,[0],[0]
We snapshot the network weights (θembb ) of the learned base model.,5. Implementation details,[0],[0]
"Then we replace the last layer in (θembb ) with a randomly initialized d dimensional fully connected projection layer (θd) and finetune the hash network (denote the parameters as θf = [θb,θd]).",5. Implementation details,[0],[0]
"Algorithm 1 summarizes the training procedure in detail.
",5. Implementation details,[0],[0]
Hash table construction and query We use the learned hash network θf and apply Equation (1) to convert a hash data xi into the hash code h(xi;θf ) and use the base embedding network θembb to convert the data into the embedding representation f(xi;θembb ).,5. Implementation details,[0],[0]
"Then, the embedding representation is hashed to buckets corresponding to the k set bits in the hash code.",5. Implementation details,[0],[0]
We use the similar procedure and convert a query data xq into the hash code h(xq;θf ) and into the embedding representation f(xq;θembb ).,5. Implementation details,[0],[0]
"Once we retrieve the union of all bucket items indexed at the k set bits in the hash code, we apply a reranking procedure (Wang et al., 2016) based on the euclidean distance in the embedding representation space.
",5. Implementation details,[0],[0]
"Evaluation metrics We report our accuracy results using precision@k (Pr@k) and normalized mutual information (NMI) (Manning et al., 2008) metrics.",5. Implementation details,[0],[0]
Precision@k is computed based on the reranked ordering (described above) of the retrieved items from the hash table.,5. Implementation details,[0],[0]
"We evaluate NMI, when the code sparsity is set to k = 1, treating each bucket as individual clusters.",5. Implementation details,[0],[0]
"In this setting, NMI becomes perfect, if each bucket has perfect class purity (pathologically putting one item per each bucket is prevented by construction since d n).",5. Implementation details,[0],[0]
We report the speedup results by comparing the number of retrieved items versus the total number of data (exhaustive linear search) and denote this metric as SUF.,5. Implementation details,[0],[0]
"As the hash code becomes uniformly distributed, SUF metric approaches the theoretical expected speedup in Equation (11).",5. Implementation details,[0],[0]
Figure 3 shows that the measured SUF of our method closely follows the theoretical upper bound in contrast to other methods.,5. Implementation details,[0],[0]
"We report our results on Cifar-100 (Krizhevsky et al., 2009) and ImageNet (Russakovsky et al., 2015) datasets and compare the accuracy against several baseline methods.",6. Experiments,[0],[0]
"First baseline methods are the state of the art deep metric learning models (Schroff et al., 2015; Sohn, 2016) performing an exhaustive linear search over the whole dataset given a query data.",6. Experiments,[0],[0]
"Another baselines are the Binarization transform (Agrawal et al., 2014; Zhai et al., 2017) methods where the dimensions of the hash code corresponding to the top k dimensions of the embedding representation are set.",6. Experiments,[0],[0]
"We also perform vector quantization (Wang et al., 2016) on the learned embedding representation from the deep metric learning methods above on the entire dataset and compute the hash code based on the indices of the k nearest centroids.
",6. Experiments,[0],[0]
‘Triplet’ and ‘Npairs’ denotes the deep metric learning base models performing an exhaust linear search per each query.,6. Experiments,[0],[0]
"‘Th’ denotes the binarization transform baseline, ‘VQ’ denotes the vector quantization baseline.",6. Experiments,[0],[0]
"Cifar-100 (Krizhevsky et al., 2009) dataset has 100 classes.",6.1. Cifar-100,[0],[0]
Each class has 500 images for train and 100 images for test.,6.1. Cifar-100,[0],[0]
"Given a query image from test, we experiment the search performance both when the hash table is constructed from train and from test.",6.1. Cifar-100,[0],[0]
"We subtract the per-pixel mean of training images across all the images and augmented the dataset by zero-padding 4 pixels on each side, randomly cropping 32× 32, and applying random horizontal flipping.",6.1. Cifar-100,[0],[0]
The batch size is set to 128.,6.1. Cifar-100,[0],[0]
"The metric learning base model is trained for 175k iterations, and learning rate decays to 0.1 of previous learning rate after 100k iterations.",6.1. Cifar-100,[0],[0]
We finetune the base model for 70k iterations and decayed the learning rate to 0.1 of previous learning rate after 40k iterations.,6.1. Cifar-100,[0],[0]
Table 1 show results using the triplet network with d=256 and Table 2 show results using the npairs network with d= 64.,6.1. Cifar-100,[0],[0]
The results show that our method not only outperforms search accuracies of the state of the art deep metric learning base models but also provides up to 98× speed up over exhaustive search.,6.1. Cifar-100,[0],[0]
"ImageNet ILSVRC-2012 (Russakovsky et al., 2015) dataset has 1, 000 classes and comes with train (1, 281, 167 images) and val set (50, 000 images).",6.2. ImageNet,[0],[0]
"We use the first nine splits of train set to train our model, the last split of train set for validation, and use validation dataset to test the query performance.",6.2. ImageNet,[0],[0]
"We use the images downsampled to 32× 32 from (Chrabaszcz et al., 2017).",6.2. ImageNet,[0],[0]
Preprocessing step is identical with cifar-100,6.2. ImageNet,[0],[0]
and we used the pixel mean provided in the dataset.,6.2. ImageNet,[0],[0]
"The batch size for the metric learning base model is set to 512 and is trained for 450k iterations, and learning rate decays to 0.3 of previous learning rate after each 200k iterations.",6.2. ImageNet,[0],[0]
"When we finetune npairs base model for d=512, we set the batch size to 1024 and total iterations to 35k with decaying the learning rate to 0.3 of previous learning rate after each 15k iterations.",6.2. ImageNet,[0],[0]
"When we finetune the triplet base model for d=256, we set the batch size to 512 and total iterations to 70k with decaying the learning rate to 0.3 of previous learning rate after each 30k iterations.",6.2. ImageNet,[0],[0]
Our results in Table 3 and Table 4 show that our method outperforms the state of the art deep metric learning base models in search accuracy while providing up to 478× speed up over exhaustive linear search.,6.2. ImageNet,[0],[0]
Table 5 compares the NMI metric and shows that the hash table constructed from our representation yields buckets with significantly better class purity on both datasets and on both methods.,6.2. ImageNet,[0],[0]
We have presented a novel end-to-end optimization algorithm for jointly learning a quantizable embedding representation and the sparse binary hash code which then can be used to construct a hash table for efficient inference.,7. Conclusion,[0],[0]
We also show an interesting connection between finding the optimal sparse binary hash code and solving a minimum cost flow problem.,7. Conclusion,[0],[0]
"Our experiments show that the proposed algorithm not only achieves the state of the art search accuracy outperforming the previous state of the art deep metric learning approaches (Schroff et al., 2015; Sohn, 2016) but also provides up to 98× and 478× search speedup on Cifar-100 and ImageNet datasets respectively.",7. Conclusion,[0],[0]
We would like to thank Zhen Li at Google Research for helpful discussions and anonymous reviewers for their constructive comments.,Acknowledgements,[0],[0]
"This work was partially supported by Kakao, Kakao Brain and Basic Science Research Program through the National Research Foundation of Korea (NRF) (2017R1E1A1A01077431).",Acknowledgements,[0],[0]
Hyun,Acknowledgements,[0],[0]
Oh Song is the corresponding author.,Acknowledgements,[0],[0]
Embedding representation learning via neural networks is at the core foundation of modern similarity based search.,abstractText,[0],[0]
"While much effort has been put in developing algorithms for learning binary hamming code representations for search efficiency, this still requires a linear scan of the entire dataset per each query and trades off the search accuracy through binarization.",abstractText,[0],[0]
"To this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the state of the art search accuracy outperforming previous state of the art deep metric learning methods.",abstractText,[0],[0]
We also show that finding the optimal sparse binary hash code in a mini-batch can be computed exactly in polynomial time by solving a minimum cost flow problem.,abstractText,[0],[0]
Our results on Cifar-100 and on ImageNet datasets show the state of the art search accuracy in precision@k and NMI metrics while providing up to 98× and 478× search speedup respectively over exhaustive linear search.,abstractText,[0],[0]
The source code is available at https://github.com/maestrojeong/Deep-HashTable-ICML18.,abstractText,[0],[0]
Efficient end-to-end learning for quantizable representations,title,[0],[0]
We consider the problem of sampling or inference using a complex probability distribution p∗(x) = p̃∗(x)/Z for which we can evaluate p̃∗(x) but not the normalization constant Z = ∫ x p̃∗(x)dx.,1. Introduction,[0],[0]
This problem is ubiquitous in machine leaning.,1. Introduction,[0],[0]
"For example, in Bayesian Inference, p̃∗(x) corresponds to the product of prior and likelihood.
",1. Introduction,[0],[0]
"As we can not sample directly from distribution p∗(x) or
1Computational Learning for Autonomous Systems, TU Darmstadt, Darmstadt, Germany 2Machine Learning Lab, University of Lincoln, Lincoln, UK 3Lincoln Center for Autonomous Systems, University of Lincoln, Lincoln, UK.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Oleg Arenz <oleg@robot-learning.de>.
Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"use it for inference, a common approach is to use Variational Inference (VI) to approximate the target distribution p∗(x) with a tractable distribution p such as multi-variate Gaussians (Blei et al., 2017; Regier et al., 2017) or Gaussian Mixture Models (GMM) (Miller et al., 2017; Guo et al., 2016; Zobay, 2014).",1. Introduction,[0],[0]
"The optimization problem to obtain this approximation is commonly framed as minimizing the reverse Kullback-Leibler divergence (KL)
KL(p||p∗) = ∫ x p(x;θ) log ( p(x;θ) p∗(x) )",1. Introduction,[0],[0]
"dx (1)
with respect to the parameters θ of the approximation.",1. Introduction,[0],[0]
"This objective is typically evaluated on samples drawn from p(x;θ) and optimized by stochastic optimization algorithms (Fan et al., 2015; Gershman et al., 2012).",1. Introduction,[0],[0]
"However, in order to perform the KL minimization efficiently, p(x) is often restricted to belong to a simple family of models or is assumed to have non-correlating degrees of freedom (Blei et al., 2017; Peterson & Hartman, 1989), which is known as the mean field approximation.",1. Introduction,[0],[0]
"Unfortunately, such restrictions can introduce significant approximation errors.
",1. Introduction,[0],[0]
Our approach focuses on learning multivariate Gaussian Mixture Models (GMMs) with full covariance matrices for approximating the target distribution.,1. Introduction,[0],[0]
"GMMs are desirable for VI, because they are capable of representing any continuous probability density function arbitrarily well, while inference with GMMs is relatively cheap.",1. Introduction,[0],[0]
"Naturally, variational inference with GMM approximations has been considered in the past.",1. Introduction,[0],[0]
"However, in order to make the minimization of objective (1) feasible, previous work either assumed factorized (Jaakkola & Jordan, 1998; Bishop et al., 1998) or isotropic (Gershman et al., 2012) mixture components or applied boosting by successively adding and optimizing new components while keeping previously added components fixed (Miller et al., 2017; Guo et al., 2016).
",1. Introduction,[0],[0]
"As areas of high density p̃∗(x) are initially unknown, Variational Inference can essentially be seen as a search problem that is inflicted by the exploration-exploitation dilemma which is typical for reinforcement learning or policy search problems.",1. Introduction,[0],[0]
The algorithms need to explore the sample space in order to ensure that all relevant areas are covered while they also need to exploit the current approximation p(x;θ) in order to fine tune p(x;θ) in areas of high density.,1. Introduction,[0],[0]
"This exploration-exploitation based view is so far under-
developed in the Variational Inference community but essential to achieve good approximations with a small number of function evaluations.
",1. Introduction,[0],[0]
"Our method transfers information-geometric insights used in Policy Search (Deisenroth et al., 2013) and Reinforcement Learning (Sutton & Barto, 1998) on solving such exploration-exploitation dilemma efficiently.",1. Introduction,[0],[0]
We therefore call our algorithm Variational Inference by Policy Search (VIPS).,1. Introduction,[0],[0]
"We extend the stochastic search method MORE (Abdolmaleki et al., 2016) to the variational inference setup and show that this version of MORE can efficiently learn single multivariate Gaussian variational distributions.",1. Introduction,[0],[0]
We further extend the algorithm for training GMM distributions using a variational lower bound that enables us to train the mixture components and the coefficients individually.,1. Introduction,[0],[0]
"Our optimization starts from an initial mixture model (e.g. a Gaussian prior) and the number of components is adapted online by adding components in promising regions and by deleting components with negligible weight.
",1. Introduction,[0],[0]
"We compare our method to other state-of-the-art VI methods (Miller et al., 2017; Gershman et al., 2012) and show that our algorithm can find approximations of much higher quality with significantly less evaluations of p̃∗(x).",1. Introduction,[0],[0]
"We further compare to existing sampling methods (Murray et al., 2010; Neal, 2003; Calderhead, 2014; Liu & Wang, 2016) and show that we can achieve similar sample quality with order of magnitudes less function evaluations.",1. Introduction,[0],[0]
We will now formalize the problem statement and introduce relevant concepts from information-geometric policy search.,2. Preliminaries,[0],[0]
"As stated above, we want to minimize the KL divergence between the approximation p and the target distribution p∗.",2.1. Problem formulation,[0],[0]
"This direct minimization is infeasible as the normalization constant of p∗ is unknown, however, it can be easily shown that the objective can be rewritten as
KL(p||p∗) = ∫ x p(x;θ) log p(x;θ)
p̃∗(x)",2.1. Problem formulation,[0],[0]
"dx︸ ︷︷ ︸
−ELBO
+ logZ, (2)
where the term logZ can be ignored as it does not depend on θ.",2.1. Problem formulation,[0],[0]
"This objective is known as the negative value of the evidence lower bound (ELBO) used in many variational inference methods (Blei et al., 2017).",2.1. Problem formulation,[0],[0]
"As we want to use insights from policy search, we rewrite the ELBO as reward maximization problem with an additional entropy objective,
L(θ) = ∫ x p(x;θ)R(x)dx+H(p), (3)
where the reward is given by R(x) = log p̃∗(x) and H(p) =",2.1. Problem formulation,[0],[0]
− ∫ x p(x;θ) log p(x;θ)dx is the entropy of p.,2.1. Problem formulation,[0],[0]
Please note the swap in the sign due to the change from a minimization to a maximization problem.,2.1. Problem formulation,[0],[0]
"Hence, policy search algorithms can directly be applied to VI if they can incorporate an additional entropy objective.",2.1. Problem formulation,[0],[0]
The ELBO objective can typically not be evaluated in closed form but is estimated by samples drawn from p(x;θ).,2.1. Problem formulation,[0],[0]
"Stochastic optimization can be used to optimize this sample-based objective (Blei et al., 2017).",2.1. Problem formulation,[0],[0]
"Policy search algorithms must solve the explorationexploitation dilemma when updating the policy, i.e., they must control how much information from the current sample set is integrated in the policy versus how much we rely on keeping the current exploration strategy to generate more information.",2.2. Information Geometric Distribution Updates,[0],[0]
"Information-geometric trust regions can be used to effectively control this exploration-exploitation trade-off (Peters et al., 2010; Schulman et al., 2015; Abdolmaleki et al., 2017; 2016).",2.2. Information Geometric Distribution Updates,[0],[0]
"We will heavily rely on a recent stochastic search algorithm called MORE (Abdolmaleki et al., 2016), that for the first time allows for a closed form solution of information geometric trust regions for Gaussian distributions.",2.2. Information Geometric Distribution Updates,[0],[0]
"Stochastic search is a special case of policy search where the policy can be interpreted as search distribution p(x) in the parameter space x of a low-level policy.
",2.2. Information Geometric Distribution Updates,[0],[0]
The MORE algorithm solves a constraint optimization problem where the objective is given by maximizing the average reward.,2.2. Information Geometric Distribution Updates,[0],[0]
The information-geometric trust region is implemented by limiting the KL-divergence between the old and new search distribution which is equivalent to limiting the information gain of the policy update.,2.2. Information Geometric Distribution Updates,[0],[0]
"Moreover, a second constraint limits the entropy loss of the distribution update to avoid a collapse of the search distribution.",2.2. Information Geometric Distribution Updates,[0],[0]
"The resulting optimization program has the following form:
maximize p(x) ∫ x p(x)R(x)dx,
s. t. KL ( p||q ) ≤ , H ( q )",2.2. Information Geometric Distribution Updates,[0],[0]
−H ( p ) ≤,2.2. Information Geometric Distribution Updates,[0],[0]
"γ. (4)
Here, q(x) is the old search distribution, R(x) is the reward and H the entropy of the distribution.",2.2. Information Geometric Distribution Updates,[0],[0]
The optimization problem can be solved using Lagrangian multipliers.,2.2. Information Geometric Distribution Updates,[0],[0]
"The solution for p(x) is given by
p(x) ∝ q(x) η η+ω exp (R(x)) 1 η+ω , (5)
where η and ω are Lagrangian multipliers, which can be found by solving the convex dual optimization problem (Abdolmaleki et al., 2016).",2.2. Information Geometric Distribution Updates,[0],[0]
"In order to use Equation 5, Gaussianity needs to be enforced.",2.3. Fitting a Reward Surrogate,[0],[0]
"This can be either performed by fitting a Gaussian (Daniel et al., 2012; Kupcsik et al., 2017) on samples that are weighted by Equation 5, which is prone to overfitting (as we need to fit a full covariance matrix) or by fitting a surrogate R̃(x)",2.3. Fitting a Reward Surrogate,[0],[0]
"≈ R(x) that is compatible to the Gaussian distribution (Abdolmaleki et al., 2016).",2.3. Fitting a Reward Surrogate,[0],[0]
"As the Gaussian distribution is log linear in quadratic features of x, the surrogate needs to be a quadratic approximation of R(x), i.e.,
R̃(x) = −0.5xTAx+ aTx+ a0.
",2.3. Fitting a Reward Surrogate,[0],[0]
"The parameters A, a and a0 are learned from the current sample set using linear regression.",2.3. Fitting a Reward Surrogate,[0],[0]
The surrogate therefore only needs to be a local approximation of the reward function.,2.3. Fitting a Reward Surrogate,[0],[0]
Using R̃(x) for R(x) in Equation 5 yields a Gaussian distribution for p(x) where the mean µ and the full covariance matrix Σ can be evaluated in closed form.,2.3. Fitting a Reward Surrogate,[0],[0]
For the exact equations we refer to Abdolmaleki et al. (2016).,2.3. Fitting a Reward Surrogate,[0],[0]
"As VI uses samples to evaluate the ELBO, VI is essentially a search problem where we need to find samples in areas of high density of p∗ but also make sure that these samples are distributed with the correct entropy.",3. Variational Inference by Policy Search,[0],[0]
"Interpreting VI as search problem, the current approximation p(x;θ) is used to search in the space of x. We can obtain samples from our current search distribution p(x;θ) and use them to update p(x;θ) such that it becomes a better approximation of p∗(x).",3. Variational Inference by Policy Search,[0],[0]
"Hence, VI is inflicted by the explorationexploitation dilemma in a similar way as reinforcement learning and policy search algorithms.",3. Variational Inference by Policy Search,[0],[0]
"In this paper, we want to use information-geometric trust regions for controlling the exploration-exploitation trade-off in the VI objective.
",3. Variational Inference by Policy Search,[0],[0]
"Information-geometric trust regions have been shown to yield efficient closed form updates for Gaussian distributions (Abdolmaleki et al., 2016).",3. Variational Inference by Policy Search,[0],[0]
"However, in order to cope with more complex, multimodal distributions, we will extend the information-geometric updates to Gaussian Mixture Models.",3. Variational Inference by Policy Search,[0],[0]
"Hence, our variational distribution is given by a GMM
p(x) = ∑ o p(o)p(x|o),
where o is the index of the mixture component, p(o) are the mixture weights and p(x|o)",3. Variational Inference by Policy Search,[0],[0]
"= N (µo,Σo) is a multivariate normal distribution with meanµo and full covariance matrix Σo.",3. Variational Inference by Policy Search,[0],[0]
"To improve readability, we will omit the parameter vector θ when writing the distribution p(x) in most cases.",3. Variational Inference by Policy Search,[0],[0]
"However, as we are dealing with mixture models, it should be noted that θ consists of the mean vectors, covariance matrices and mixture weights of all components.
",3. Variational Inference by Policy Search,[0],[0]
We will first introduce our objective including the trust regions and subsequently introduce a variational lower bound to decompose the objective into tractable optimization problems for the individual mixture components and the mixture coefficients.,3. Variational Inference by Policy Search,[0],[0]
The number of components is automatically adapted by deleting components that have low weight and by creating new components in promising regions.,3. Variational Inference by Policy Search,[0],[0]
We approximate the target distribution p∗(x) by minimizing L(θ) given in Equation 3 on the current set of samples.,3.1. Objective,[0],[0]
"As we will use local approximations of the reward function around each component, we introduce individual trust regions for each component and for the coefficients.",3.1. Objective,[0],[0]
"The resulting optimization problem has the form
maximize p(x|o),p(o)
∫ x p(x) ( R(x)− log p(x) )",3.1. Objective,[0],[0]
"dx,
subject to ∀o : ∫ x p(x|o) log p(x|o) q(x|o)
≤",3.1. Objective,[0],[0]
"(o),∑ o p(o) log p(o) q(o) ≤",3.1. Objective,[0],[0]
"w,
where q(o) and q(x|o) are the old mixture weights and components, respectively, and w and (o) upper-bound the corresponding KL-divergences.",3.1. Objective,[0],[0]
"However, the occurrence of the log-density of the GMM, log p(x), prevents us from updating each component independently.",3.1. Objective,[0],[0]
"By introducing an auxiliary distribution p̃(o|x), the objective of the optimization problem can be decomposed into a lower bound U(θ, p̃) and an expected KL-term, namely
J(θ) = U(θ, p̃) + Ep(x)",3.2. Variational Lower Bound,[0],[0]
"[ KL ( p(o|x)||p̃(o|x) )] , (6)
with
U(θ, p̃) = ∫ x ∑ o p(x, o) ( R(x)− log p(x, o)
+ log p̃(o|x) )",3.2. Variational Lower Bound,[0],[0]
"dx
and
KL ( p(o|x)||p̃(o|x) )",3.2. Variational Lower Bound,[0],[0]
"= ∑ o p(o|x) log p(o|x) p̃(o|x) .
",3.2. Variational Lower Bound,[0],[0]
"We also used the identity p(x, o) = p(x|o)p(o) to keep the notation uncluttered.",3.2. Variational Lower Bound,[0],[0]
"Eq. 6 can be easily verified by using Bayes theorem, i.e., by setting log p(o|x) = log p(x, o)− log p(x), all introduced terms will vanish and only the original objective remains, see supplement.",3.2. Variational Lower Bound,[0],[0]
The second term in Eq. 6 is always greater or equal zero.,3.2. Variational Lower Bound,[0],[0]
"Hence,
U(θ, p̃) is a lower bound on the objective.",3.2. Variational Lower Bound,[0],[0]
"This decomposition is closely related to the decomposition used in the expectation maximization (EM) algorithm (Bishop, 2006).",3.2. Variational Lower Bound,[0],[0]
"However, as we want to minimize the reverse KL instead of the maximum likelihood (which corresponds to the forward KL) that is the objective in standard EM, the KL used for p̃(o|x) also needs to be reversed to obtain the lower bound.
",3.2. Variational Lower Bound,[0],[0]
"Following the standard EM procedure, we can maximize the objective function by iteratively maximizing the lower bound (M-step) and tightening the lower bound (Estep).",3.2. Variational Lower Bound,[0],[0]
The E-step can be computed by setting p̃(o|x) = p(x|o)p(o)/p(x) using the current GMM.,3.2. Variational Lower Bound,[0],[0]
"Hence, after the E-step, the lower bound is tight as the KL is set to 0.",3.2. Variational Lower Bound,[0],[0]
"Consequently, improving the lower bound also improves the original objective J(θ) at each EM iteration.",3.2. Variational Lower Bound,[0],[0]
It can be easily seen that the lower bound does not contain the term log p(x) anymore and decomposes into individual terms for the weight distribution and the single components such that the updates can be performed independently.,3.2. Variational Lower Bound,[0],[0]
"Moreover, an interesting observation is the term log p̃(o|x) acts as additional reward.",3.2. Variational Lower Bound,[0],[0]
"After each sampling process from p(x), we perform 10 EM-iterations to fine tune the mixture components with the newly obtained samples.",3.2. Variational Lower Bound,[0],[0]
"When updating a single component p(x|o), maximizing U is equivalent to maximizing
Uo(µo,Σo) = ∫ x p(x|o)",3.3. M-step for Component Updates,[0],[0]
( R(x) + log p̃(o|x) ),3.3. M-step for Component Updates,[0],[0]
"dx
+H ( p(x|o) )",3.3. M-step for Component Updates,[0],[0]
+ η(o),3.3. M-step for Component Updates,[0],[0]
"( (o)− ∫ x p(x|o) log p(x|o) q(x|o) dx ) ,
where we already added the Lagrangian multiplier η(o) of the KL constraint for component o.",3.3. M-step for Component Updates,[0],[0]
"This optimization problem is very similar to the optimization problem that is solved in MORE (Eq.4), which becomes evident when defining
ro(x) = R(x) + log p̃(o|x) (7)
as reward function.",3.3. M-step for Component Updates,[0],[0]
"In fact, the only difference compared to MORE is that the entropy of the component enters the optimization via a constant factor 1 rather than a Lagrangian multiplier.",3.3. M-step for Component Updates,[0],[0]
"Hence, we only need to optimize the Lagrangian multiplier of the KL constraint, η(o) as ω is set to 1 in the MORE equations, see Eq. 5.
",3.3. M-step for Component Updates,[0],[0]
"We refer to the supplement and to the original MORE paper (Abdolmaleki et al., 2016) for the closed form updates of this optimization problem based on quadratic reward surrogates r̃o(x)",3.3. M-step for Component Updates,[0],[0]
≈ ro(x).,3.3. M-step for Component Updates,[0],[0]
"Note that in difference to the original MORE paper, we use a standard linear regression to obtain the quadratic models.",3.3. M-step for Component Updates,[0],[0]
"The used dimensionality reduction
technique reported by Abdolmaleki et al. (2016) was not needed to achieve satisfactory results.",3.3. M-step for Component Updates,[0],[0]
"Maximizing U with respect to p(o) is equivalent to maximizing
Uw (p(o))",3.4. M-step for Weight Updates,[0],[0]
= ∑ o p(o)rw(o),3.4. M-step for Weight Updates,[0],[0]
+H ( p(o) ),3.4. M-step for Weight Updates,[0],[0]
+ ηw,3.4. M-step for Weight Updates,[0],[0]
"( w −
∑ o p(o) log p(o) q(o)
) ,
where we already added the Lagrangian multiplier for the KL constraint on the weights and
rw(o) = ∫ x p(x|o)ro(x)dx+H(p(x|o)).
",3.4. M-step for Weight Updates,[0],[0]
This optimization problem corresponds to optimizing a discrete distribution with an additional entropy objective.,3.4. M-step for Weight Updates,[0],[0]
"The solution for p(o) is obtained by
p(o) ∝",3.4. M-step for Weight Updates,[0],[0]
q(o) ηw ηw+1 exp (rw(o)) 1 ηw+1 .,3.4. M-step for Weight Updates,[0],[0]
"(8)
Please refer to the supplement for the dual function and gradient of this optimization problem.",3.4. M-step for Weight Updates,[0],[0]
"Samples are used for approximating ro(x) for the component updates as well as rw(o) for the weight updates.
",3.5. Sample Reuse,[0],[0]
"We fit a quadratic surrogate to approximate ro(x) using linear regression, where the samples relate to the independent variables.",3.5. Sample Reuse,[0],[0]
The surrogate should be most accurate in the vicinity of p(x|o) which can be achieved by using independent variables that are distributed according to p(x|o).,3.5. Sample Reuse,[0],[0]
"However, we also want to make use of samples from previous iterations or different components.",3.5. Sample Reuse,[0],[0]
"We therefore perform weighted least squares, where the weight of sample i is given by the self-normalizing importance weight
wi(o) = 1
Z p(xi|o) z(xi) , Z = ∑ i p(xi|o) z(xi) .
",3.5. Sample Reuse,[0],[0]
"The sampling distribution z(x) for the current set of Ns samples is a Gaussian Mixture Model given by
z(x) =",3.5. Sample Reuse,[0],[0]
Ns∑ i=1 1,3.5. Sample Reuse,[0],[0]
"Ns Ni(x),
whereNi(x) corresponds to the normal distribution that was used to obtain sample i. For better efficiency, we include several samples from each sampling componentNi(x) such that z(x) comprises less than Ns different component.
",3.5. Sample Reuse,[0],[0]
We approximate rw(o) for the weight updates by using an importance-weighted Monte Carlo estimate based on the same importance weights that are used for the component updates.,3.5. Sample Reuse,[0],[0]
"Hence, we replace rw(o) in Eq. 8 by
r̃w(o) = Ns∑ i=1 wi(o)ro(xi) +H(p(x|o)).
",3.5. Sample Reuse,[0],[0]
The set of active samples is replaced directly after new samples have been drawn and is held constant during the EM iterations.,3.5. Sample Reuse,[0],[0]
"For all our experiments we selected the 3 · Nnew most recent samples, where Nnew corresponds to the number of samples that have been drawn during the last round of sampling.",3.5. Sample Reuse,[0],[0]
"Each round of sampling consists of drawing 10 ·D samples from each mixture component, where D corresponds to the number of dimensions of x.",3.5. Sample Reuse,[0],[0]
"In order to adapt the complexity of the GMM to the complexity of the target distribution, we initialize our algorithm with only one mixture component with high variance and gradually increase the number of mixture components.",3.6. Adding and Deleting Mixture Components,[0],[0]
We consider the locations of all previous samples as candidates for new components.,3.6. Adding and Deleting Mixture Components,[0],[0]
"Every nadd iterations, we heuristically choose the most promising candidate and use its location as mean for a newly added component.",3.6. Adding and Deleting Mixture Components,[0],[0]
"The covariance matrix is initialized by interpolating the covariance matrices of neighboring components using the responsibilities.
",3.6. Adding and Deleting Mixture Components,[0],[0]
"As we want the new component to eventually achieve high weight, we want to add it at an area where its componentspecific reward ro(x) will become large.",3.6. Adding and Deleting Mixture Components,[0],[0]
"We therefore try to find an area that has high likelihood under the target distribution but also yields high log-responsibilities for the newly added component, see Equation 7.",3.6. Adding and Deleting Mixture Components,[0],[0]
"However, the responsibilities of a newly initialized component hardly relate to the responsibilities it will eventually achieve.",3.6. Adding and Deleting Mixture Components,[0],[0]
"Instead, we choose the candidate that maximizes the score ei = log p̃
∗(xi)",3.6. Adding and Deleting Mixture Components,[0],[0]
"− max(log p(xi),maxj log p(xj) − γ).",3.6. Adding and Deleting Mixture Components,[0],[0]
The second term prefers locations that are little covered by the current approximation and behaves similar to the log-responsibilities which also saturate for far away areas.,3.6. Adding and Deleting Mixture Components,[0],[0]
"Without such saturation, the second term might dominate the score when the target distribution has heavy tails.
",3.6. Adding and Deleting Mixture Components,[0],[0]
"In order to add components without impairing the stability of the optimization, we initialize them with very low weight such that their effect on the approximation is negligible.",3.6. Adding and Deleting Mixture Components,[0],[0]
"As we draw the same number of samples from each component irrespective of their weight, such low weight components can still improve and eventually contribute to the approximation.",3.6. Adding and Deleting Mixture Components,[0],[0]
"However, keeping unnecessary components is costly in terms of computational cost and sample efficiency.",3.6. Adding and Deleting Mixture Components,[0],[0]
"We therefore delete components that did not have mentionable
effect on the approximation during the last ndel iterations.
",3.6. Adding and Deleting Mixture Components,[0],[0]
The full algorithm is outlined in Algorithm 1.,3.6. Adding and Deleting Mixture Components,[0],[0]
"An opensource implementation is available online1.
",3.6. Adding and Deleting Mixture Components,[0],[0]
Algorithm 1 Variational Inference by Policy Search 1: Input: initial parameters θ0 2: for j = 1,3.6. Adding and Deleting Mixture Components,[0],[0]
"to maxIter do 3: Add and delete components according to heuristics.
",3.6. Adding and Deleting Mixture Components,[0],[0]
"Store new parameters in θj 4: Draw samples si from each component and store
them along with ri = log p̃∗(si) and the parameters of the responsible component.
5: (s⊂, r⊂, z⊂(x))← select active samples() 6: compute zj = z⊂(s⊂) 7: for k = 1 to maxIterEM do 8: recompute p = p(s⊂;θj), p̃ = p(o|s⊂;θj)",3.6. Adding and Deleting Mixture Components,[0],[0]
"9: θj ← update weights(p, zj , p̃, s⊂, r⊂,θj)
10: recompute p = p(s⊂;θj), p̃ = p(o|s⊂;θj) 11: for each component do 12: θj ← update component(p, zj , p̃, s⊂, r⊂,θj) 13: end for 14: end for 15: end for",3.6. Adding and Deleting Mixture Components,[0],[0]
"Although MCMC samplers can not directly be used for approximating distributions, they are for many applications the main alternative to VI.",4. Related Work,[0],[0]
"Prominent examples of gradientfree MCMC methods include (random walk) Metropolis Hasting (Hastings, 1970), Gibbs sampling (Geman & Geman, 1984), slice sampling (Neal, 2003) and elliptical slice sampling (Murray et al., 2010; Nishihara et al., 2014).",4. Related Work,[0],[0]
"If the gradient of the target distribution is available, Hamiltonian MCMC (Duane et al., 1987) and the Metropolis-adjusted Langevin algorithm (Roberts & Stramer, 2002) are also popular choices.",4. Related Work,[0],[0]
"The No-U-Turn sampler (NUTS) (Hoffman & Gelman, 2014) is a notable variant of Hamiltonian MCMC that is appealing for not requiring hyper-parameter tuning.",4. Related Work,[0],[0]
"While many of these MCMC methods have problems with multimodal distributions in terms of mixing time, other methods use multiple chains and can therefore better explore multimodal sample spaces (Neal, 1996; Nishihara et al., 2014; Calderhead, 2014).
",4. Related Work,[0],[0]
"Many VI methods are only applicable to Gaussian variational distributions (Fan et al., 2015; Regier et al., 2017).",4. Related Work,[0],[0]
The approach by Fan et al. (2015) can learn Gaussians with full covariance matrices using fast second order optimization.,4. Related Work,[0],[0]
This idea has been extended by Regier et al. (2017) to trust region optimization.,4. Related Work,[0],[0]
"However, in difference to our approach, an euclidean trust region is used in parameter space
1https://github.com/OlegArenz/VIPS
of the variational distribution.",4. Related Work,[0],[0]
Such approach requires the computation of the Hessian of the objective which is only tractable for mean-field approximations of single Gaussian distributions.,4. Related Work,[0],[0]
"In contrast, we use the trust regions directly on the change of the distributions instead of the change of the parameters of the distribution.",4. Related Work,[0],[0]
"The information geometric trust regions in this paper allow for efficient estimation of GMMs with full covariance matrices without requiring gradient information from p∗.
Black-box Variational Inference methods do not make strong assumptions on the model family (Salimans & Knowles, 2013; Ranganath et al., 2014) and can therefore also be used for learning GMM approximations.",4. Related Work,[0],[0]
Salimans & Knowles (2013) derive a fixed point update of the natural parameters of a distribution from the exponential family that corresponds to a Monte-Carlo estimate of the gradient of Eq.,4. Related Work,[0],[0]
(1) preconditioned by the inverse of their empirical covariance.,4. Related Work,[0],[0]
"By making structural assumptions on the target distribution, they extend their method to mixture models and show its applicability to bivariate GMMs.",4. Related Work,[0],[0]
Ranganath et al. (2014) also use Monte-Carlo gradient estimates of Eq.,4. Related Work,[0],[0]
(1) and apply Rao-Blackwellization and control variates to reduce its variance.,4. Related Work,[0],[0]
The work of Weber et al. (2015) already explored the use of Reinforcement Learning for VI but formalizing VI as sequential decision problem.,4. Related Work,[0],[0]
"However, only simple policy gradient methods have been proposed in this context which are unsuitable for learning GMMs.
",4. Related Work,[0],[0]
"Closely related to our work are two recent approaches for Variational Inference that concurrently explored the idea of applying boosting to make the training of GMM approximations tractable (Miller et al., 2017; Guo et al., 2016).",4. Related Work,[0],[0]
These methods start by minimizing the ELBO objective for a single component and then successively add and optimize new components and learn an optimal weighting between the previous mixture and the newly added component.,4. Related Work,[0],[0]
"However, they do not use information-geometric trust region to efficiently explore the sample space and therefore have problems finding all the modes as well as accurate estimates of the covariance matrices.",4. Related Work,[0],[0]
"Non-parametric variational inference (NPVI) (Gershman et al., 2012) learns GMMs with uniform weights using a second-order approximation of the
ELBO for efficient gradient updates.",4. Related Work,[0],[0]
"However, this method only allows for mean field approximations for the mixture components which is a severe limitation as shown in our comparisons.",4. Related Work,[0],[0]
GMMs are also used by Zobay (2014) where an approximation of the GMM entropy is used to make the optimization tractable.,4. Related Work,[0],[0]
The optimization is gradient-based and does not consider exploration of the sample space.,4. Related Work,[0],[0]
It is therefore limited to rather low dimensional problems.,4. Related Work,[0],[0]
We evaluate VIPS with respect to efficiency of the optimization as well as quality of the learned approximations.,5. Experiments,[0],[0]
"For assessing efficiency, we focus on the number of function evaluations, but also include a comparison with respect to the wall-clock time.",5. Experiments,[0],[0]
"As the ELBO objective is hard to use for comparisons as it depends on the current sample set, we assess the quality of the approximation by comparing samples drawn from the learned model with groundtruth samples based on their Maximum Mean Discrepancy (MMD) (Gretton et al., 2012).",5. Experiments,[0],[0]
Please refer to the supplement how the MMD and the ground-truth samples are computed.,5. Experiments,[0],[0]
"Please note that the computation of the ground-truth samples is based on generalized elliptical slice sampling (GESS) (Nishihara et al., 2014) which is in most cases computationally very expensive.",5. Experiments,[0],[0]
"Due to the huge computational demands, we do not consider GESS as competitor.
",5. Experiments,[0],[0]
"We compare our method to a variety of state-of-the-art MCMC and VI approaches on unimodal and multimodal problems, namely we compare to Variational Boosting (Miller et al., 2017), Non-Parametric Variational Inference (Gershman et al., 2012), Stein Variational Gradient Descent (Liu & Wang, 2016), Hamiltonian Monte Carlo (Duane et al., 1987), Slice Sampling (Neal, 2003), Elliptical Slice Sampling (Murray et al., 2010), Parallel Tempering MCMC (Calderhead, 2014) and—indirectly, since it is used for initializing variational boosting—black box variational inference (Salimans & Knowles, 2013).",5. Experiments,[0],[0]
"However, due to the high computational demands, we do not compare to every method on each experiment but rather select promising candidates based on the sampling problem or on the preliminary experiments that we had to conduct for hyper-parameter tun-
ing.",5. Experiments,[0],[0]
"For VIPS, we use the same hyper-parameters for all experiments.",5. Experiments,[0],[0]
"However, we do not add new components if it would increase the number of components above a certain threshold and evaluate different values for this threshold.
",5. Experiments,[0],[0]
We perform three experiments on (essentially) unimodal sampling problems taken from related work.,5. Experiments,[0],[0]
"We perform Bayesian logistic regression on the German Credit and Breast Cancer datasets (Lichman, 2013) as described in (Nishihara et al., 2014) and approximate the posterior of the hierarchical Poisson GLM described in (Miller et al., 2017).",5. Experiments,[0],[0]
"We designed two challenging toy tasks for evaluating our approach on multimodal problems: sampling from an unknown twenty-dimensional GMM with full covariance matrices and distant modes, and sampling the joint configurations of a ten-dimensional planar robot.
",5. Experiments,[0],[0]
We start the experimental evaluation of VIPS by visualizing the iterative improvements of the GMM approximation for the task of approximating an unknown two-dimensional GMM.,5. Experiments,[0],[0]
Figure 1 shows the log-probability densities of the learned approximation during the optimization and the target distribution (right).,5. Experiments,[0],[0]
We can see that VIPS gradually adds more components and improves the GMM.,5. Experiments,[0],[0]
The final fit with 20 components closely approximates the target distribution.,5. Experiments,[0],[0]
"We perform two experiments for binary classification on the German credit and breast cancer datasets (Lichman, 2013).",5.1. Bayesian Logistic Regression,[0],[0]
For the German credit dataset twenty-five parameters are learned whereas the breast cancer dataset is thirty-one dimensional.,5.1. Bayesian Logistic Regression,[0],[0]
"We standardize both datasets and perform linear logistic regression where we put zero-mean Gaussian priors with variance 100 on all parameters.
",5.1. Bayesian Logistic Regression,[0],[0]
"On the German credit dataset we compare against NPVI, ESS, SVGD, HMC and variational boosting.",5.1. Bayesian Logistic Regression,[0],[0]
"For variational boosting we examine rank-0, rank-5 and rank-10 approximations of the covariance matrices.",5.1. Bayesian Logistic Regression,[0],[0]
"We also performed
experiments with full rank approximation but the optimization always failed after few iterations.",5.1. Bayesian Logistic Regression,[0],[0]
"For our approach we examine two variants, VIPS1 which learns only a single component and VIPS40 which stops adding new components after forty components.",5.1. Bayesian Logistic Regression,[0],[0]
Figure 2a shows that the sample quality achieved by VIPS is unmatched by any variational inference method and ESS needs more than two orders of magnitude more function evaluation to achieve a similar MMD to VIPS1.,5.1. Bayesian Logistic Regression,[0],[0]
"However, we could not measure an advantage of using a GMM instead of single Gaussian distribution on this dataset.",5.1. Bayesian Logistic Regression,[0],[0]
"As VIPS1 is only learning a single Gaussian, it is also more data-efficient than VIPS40.",5.1. Bayesian Logistic Regression,[0],[0]
"This result also illustrates the importance of using Gaussian distributions with an accurate estimation of the full covariance as VIPS1 could already outperform competing methods.
",5.1. Bayesian Logistic Regression,[0],[0]
Figure 2b depicts the achieved MMDs on the breast cancer dataset.,5.1. Bayesian Logistic Regression,[0],[0]
We only compared to the most promising methods from the German credit dataset.,5.1. Bayesian Logistic Regression,[0],[0]
"Although the posterior distribution is unimodal, the GMM variant of our method learns a slightly better approximation than VIPS1, presumably by matching higher order moments of the posterior.",5.1. Bayesian Logistic Regression,[0],[0]
"We evaluate our method on the problem of learning the posterior of a hierarchical Poisson GLM on the 37-dimensional stop-and-frisk dataset, where we refer to (Miller et al., 2017) for the description of the hierarchical model.",5.2. Multi-level Poisson GLM,[0],[0]
"We compared VIPS1 and VIPS5 to HMC, SVGD, variational boosting and non-parametric variational inference.",5.2. Multi-level Poisson GLM,[0],[0]
The MMD with respect to the baseline samples is shown in Figure 3a.,5.2. Multi-level Poisson GLM,[0],[0]
We evaluate our approach on a planar robot with ten degrees of freedom.,5.3. Planar Robot,[0],[0]
We want to sample joint configurations such that the end-effector of the robot is close to a desired position at x = 7 and y = 0.,5.3. Planar Robot,[0],[0]
"The likelihood of a configu-
ration is a Gaussian distribution in Cartesian end-effector space with a variance of 1e−4 in both dimensions.",5.3. Planar Robot,[0],[0]
"We assume a zero mean Gaussian prior in configuration space, where the first joint has variance 1 and the remaining joints have variance 0.04.",5.3. Planar Robot,[0],[0]
"We compare VIPS40 with ESS, parallel tempering MCMC, SVGD, slice sampling, NPVI and VBOOST.",5.3. Planar Robot,[0],[0]
The MMD is shown in Figure 3b.,5.3. Planar Robot,[0],[0]
"Figure 4 visualizes the learned models of NPVI, VBOOST and VIPS and compares the ground-truth samples with the samples obtained by VIPS.",5.3. Planar Robot,[0],[0]
All other VI methods can not represent the complex structure of the modes and get attracted by single modes resulting in bad approximations.,5.3. Planar Robot,[0],[0]
We believe the reason for this behavior is a missing principled treatment of the exploration-exploitation trade-off.,5.3. Planar Robot,[0],[0]
VIPS achieves a high sample quality that is comparable to MCMC methods in order of magnitude less function evaluations than MCMC.,5.3. Planar Robot,[0],[0]
"We evaluate our method on the task of approximating unknown, randomly generated 20-dimensional GMMs comprising ten components.",5.4. Gaussian Mixture Model,[0],[0]
Each dimensions of the component means is drawn uniformly in the interval,5.4. Gaussian Mixture Model,[0],[0]
"[−50, 50].",5.4. Gaussian Mixture Model,[0],[0]
The covariance matrices are given by Σ = A>A + I20 where each entry of the 20 × 20-dimensional matrix A is sampled from a normal distribution with mean 0 and variance 20.,5.4. Gaussian Mixture Model,[0],[0]
"Note that each component of the target distribution can
have a highly correlated covariance matrix, which is even a problem for the tested MCMC methods.",5.4. Gaussian Mixture Model,[0],[0]
"Figure 5 shows the MMD of VIPS40, SVGD, ESS and parallel tempering MCMC plotted over the number of iterations as well as wallclock time.",5.4. Gaussian Mixture Model,[0],[0]
VIPS was the only method that could reliable be applied to this task.,5.4. Gaussian Mixture Model,[0],[0]
We also briefly evaluated the scaling of the performance with the number of cores.,5.4. Gaussian Mixture Model,[0],[0]
"Although parallel computing is not the focus of this paper, the possibility of performing independent updates of the mixture components suggests that the method can make good use of multi-threading.",5.4. Gaussian Mixture Model,[0],[0]
"Figure 5b shows that VIPS scales almost linearly with the number of cores at least if their number is small, showing the potential of parallelizing our algorithm.",5.4. Gaussian Mixture Model,[0],[0]
VIPS is motivated by the insight from stochastic search that information-geometric trust regions allow for controlled exploration and stable optimization.,6. Conclusion,[0],[0]
We transfered the stochastic search method MORE to the field of variational inference and demonstrated that it is significantly more efficient than state-of-the-art approaches of VI for learning mean and full covariance of a multi-variate normal distribution.,6. Conclusion,[0],[0]
"Based on our variant of MORE, we derived a novel method for learning GMM approximations and demonstrated that it is capable of learning high quality approximations of complex, multimodal distributions with a limited amount of function evaluations.",6. Conclusion,[0],[0]
"Our method makes little assumptions on the unnormalized density function of the desired distribution and is thereby applicable to non-differentiable problems.
",6. Conclusion,[0],[0]
"However, for higher-dimensional problems (e.g. more than 100 dimensions) fitting quadratic surrogates may require too many samples which could be alleviated by using gradient information for constraining the surrogate.",6. Conclusion,[0],[0]
"Furthermore, learning diagonal surrogates can be preferable for such problems due to better sample- and computational efficiency.",6. Conclusion,[0],[0]
"For exploration, we start with an approximation with high entropy and decrease it slowly during optimization which can miss modes that are not discovered in the beginning.",6. Conclusion,[0],[0]
"Actively sampling unexplored regions would result in an anytime algorithm, capable of further improving the approximation in the later stages of optimization.",6. Conclusion,[0],[0]
This project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No 645582 (RoMaNS).,Acknowledgements,[0],[0]
Calculations for this research were conducted on the Lichtenberg high performance computer of the TU Darmstadt.,Acknowledgements,[0],[0]
Inference from complex distributions is a common problem in machine learning needed for many Bayesian methods.,abstractText,[0],[0]
"We propose an efficient, gradient-free method for learning general GMM approximations of multimodal distributions based on recent insights from stochastic search methods.",abstractText,[0],[0]
"Our method establishes information-geometric trust regions to ensure efficient exploration of the sampling space and stability of the GMM updates, allowing for efficient estimation of multi-variate Gaussian variational distributions.",abstractText,[0],[0]
"For GMMs, we apply a variational lower bound to decompose the learning objective into sub-problems given by learning the individual mixture components and the coefficients.",abstractText,[0],[0]
The number of mixture components is adapted online in order to allow for arbitrary exact approximations.,abstractText,[0],[0]
We demonstrate on several domains that we can learn significantly better approximations than competing variational inference methods and that the quality of samples drawn from our approximations is on par with samples created by state-of-the-art MCMC samplers that require significantly more computational resources.,abstractText,[0],[0]
Efficient Gradient-Free Variational Inference using Policy Search,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2214–2224 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2214",text,[0],[0]
Intelligent personal digital assistants (IPDAs) are one of the most advanced and successful artificial intelligence applications that have spoken language understanding (SLU).,1 Introduction,[0],[0]
"Many IPDAs have recently emerged in industry including Amazon Alexa, Google Assistant, Apple Siri, and Microsoft Cortana (Sarikaya, 2017).",1 Introduction,[0],[0]
"IPDAs have traditionally supported only dozens of well-separated domains, each defined in terms of a specific ap-
plication or functionality such as calendar and local search (Tur and de Mori, 2011; Sarikaya et al., 2016).",1 Introduction,[0],[0]
"To rapidly increase domain coverage and extend capabilities, some IPDAs have released Software Development Toolkits (SDKs) to allow third-party developers to quickly build and integrate new domains, which we refer to as skills henceforth.",1 Introduction,[0],[0]
"Amazon’s Alexa Skills Kit (Kumar et al., 2017a), Google’s Actions and Microsoft’s Cortana Skills Kit are all examples of such SDKs.",1 Introduction,[0],[0]
"Alexa Skills Kit is the largest of these services with over 40,000 skills.
",1 Introduction,[0],[0]
"For IPDAs, finding the most relevant skill to handle an utterance is an open problem for three reasons.",1 Introduction,[0],[0]
"First, the sheer number of skills makes the task difficult.",1 Introduction,[0],[0]
"Unlike traditional systems that have on the order of 10-20 built-in domains, largescale IPDAs can have up to 40,000 skills.",1 Introduction,[0],[0]
"Second, the number of skills is rapidly expanding with 100+ new skills added per week.",1 Introduction,[0],[0]
Largescale IPDAs should be able to accommodate new skills efficiently without compromising performance.,1 Introduction,[0],[0]
"Third, unlike traditional built-in domains that are carefully designed to be disjoint by a central team, skills are built independently by different developers and can cover overlapping functionalities.",1 Introduction,[0],[0]
"For instance, there are over 50 recipe skills in Alexa that can handle recipe-related utterances.
",1 Introduction,[0],[0]
One simple solution to this problem has been to require the user to explicitly mention a skill name and follow a strict invocation pattern as in ”Ask {Uber} to {get me a ride}.”,1 Introduction,[0],[0]
"However, this significantly limits the natural interaction with IPDAs.",1 Introduction,[0],[0]
"Users have to remember skill names and invocation patterns, and it places a cognitive burden on users who tend to forget both.",1 Introduction,[0],[0]
"Skill discovery is difficult with a pure voice user interface, it is hard for users to know the capabilities of thousands of skills a priori, which may leads to limited user en-
gagement with skills and potentially with IPDAs.",1 Introduction,[0],[0]
"In this paper, we propose a solution that addresses all three practical challenges without requiring skill names or invocation patterns.",1 Introduction,[0],[0]
"Our approach is based on a scalable neural model architecture with a shared encoder, a skill attention mechanism and skill-specific classification networks that can efficiently perform large-scale skill classification in IPDAs using a weakly supervised training dataset.",1 Introduction,[0],[0]
We demonstrate that our model achieves a high accuracy on a manually transcribed test set after being trained with weak supervision.,1 Introduction,[0],[0]
"Moreover, our architecture is designed to efficiently integrate new skills that appear in-between full model retraining cycles into the model.",1 Introduction,[0],[0]
"Besides accuracy, we also keep practical constraints in mind and focus on minimizing memory footprint and runtime latency, while ensuring architecture is scalable to thousands of skills, all of which are important for real-time production systems.",1 Introduction,[0],[0]
"Furthermore, we investigate two different ways of incorporating user personalization information into the model, our naive baseline method adds the information as a 1-bit flag in the feature space of the skill-specific networks, the personalized attention technique computes a convex combination of skill embeddings for the user’s enabled skills and significantly outperforms the naive personalization baseline.",1 Introduction,[0],[0]
"We show the effectiveness of our approach with extensive experiments using 1,500 skills from a deployed IPDA system.",1 Introduction,[0],[0]
"Traditional multi-domain SLU/NLU systems are designed hierarchically, starting with domain classification to classify an incoming utterance into one of many possible domains, followed by further semantic analysis with domain-specific intent classification and slot tagging (Tur and de Mori, 2011).",2 Related Work,[0],[0]
"Traditional systems have typically been limited to a small number of domains, designed by specialists to be well-separable.",2 Related Work,[0],[0]
"Therefore, domain classification has been considered a less complex task compared to other semantic analysis such as intent and slot predictions.",2 Related Work,[0],[0]
Traditional domain classifiers are built using simple linear models such as Multinomial Logistic Regression or Support Vector Machines in a one-versusall setting for multi-class prediction.,2 Related Work,[0],[0]
"The models typically use word n-gram features and also those
based on static lexicon match, and there have been several recent studies applying deep learning techniques (Xu and Sarikaya, 2014).
",2 Related Work,[0],[0]
There is also a line of prior work on enhancing sequential text classification or tagging.,2 Related Work,[0],[0]
"Hierarchical character-to-word level LSTM (Hochreiter and Schmidhuber, 1997) architectures similar to our models have been explored for the Named Entity Recognition task by Lample et al. (2016).",2 Related Work,[0],[0]
Character-informed sequence models have also been explored for simple text classification with small sets of classes by Xiao and Cho (2016).,2 Related Work,[0],[0]
"Joulin et al. (2016) explored highly scalable text classification using a shared hierarchical encoder, but their hierarchical softmax-based output formulation is unsuitable for incremental model updates.",2 Related Work,[0],[0]
Work on zero-shot domain classifier expansion by Kumar et al. (2017b) struggled to rank incoming domains higher than training domains.,2 Related Work,[0],[0]
"The attention-based approach of Kim et al. (2017d) does not require retraining from scratch, but it requires keeping all models stored in memory which is computationally expensive.",2 Related Work,[0],[0]
"Multi-Task learning was used in the context of SLU by Tur (2006) and has been further explored using neural networks for phoneme recognition (Seltzer and Droppo, 2013) and semantic parsing (Fan et al., 2017; Bapna et al., 2017).",2 Related Work,[0],[0]
"There have been many other pieces of prior work on improving NLU systems with pre-training (Kim et al., 2015b; Celikyilmaz et al., 2016; Kim et al., 2017e), multi-task learning (Zhang and Wang, 2016; Liu and Lane, 2016; Kim et al., 2017b), transfer learning (ElKahky et al., 2014; Kim et al., 2015a,c; Chen et al., 2016a; Yang et al., 2017), domain adaptation (Kim et al., 2016; Jaech et al., 2016; Liu and Lane, 2017; Kim et al., 2017d,c) and contextual signals (Bhargava et al., 2013; Chen et al., 2016b; Hori et al., 2016; Kim et al., 2017a).",2 Related Work,[0],[0]
Our model addresses the domain classification task in SLU systems.,3 Weakly Supervised Training Data Generation,[0],[0]
"In traditional IPDA systems, these domains are hand-crafted by experts to be well separable and can easily be annotated by humans because they are small in number.",3 Weakly Supervised Training Data Generation,[0],[0]
The emergence of self-service SLU results in a large number of potentially mutually overlapping SLU domains.,3 Weakly Supervised Training Data Generation,[0],[0]
"This means that eliciting large volumes of high quality human annotations to train our model
is no longer feasible, and we cannot assume that domains are designed to be well separable.
",3 Weakly Supervised Training Data Generation,[0],[0]
"Instead we can generate training data by adopting the weak supervision paradigm introduced by (Hoffmann et al., 2011), which proposes using heuristic labeling functions generate large numbers of noisy data samples.",3 Weakly Supervised Training Data Generation,[0],[0]
"Clean data generation with weak supervision is a challenging problem, so we address it by decomposing it into two simpler problems, of candidate generation and noise suppression, however it remains important for our model to be noise robust.",3 Weakly Supervised Training Data Generation,[0],[0]
The key insight of the Data Programming approach is that O(1) simple labeling functions can be used to approximate O(n) human annotated data points with much less effort.,3.1 Data Programming,[0],[0]
"We adopt the formalism used by (Ratner et al., 2016) to treat each of instance data generation rule as a rich generative model, defined by a labeling function λ and describe different families of labeling functions.",3.1 Data Programming,[0],[0]
"Our data programming pipeline is analogous to the noisy channel model proposed for spelling correction by (Kernighan et al., 1990), and consists of a set of candidate generation and noise detection functions.
arg max µ P (µ|si) = arg max µ P (si|µ).",3.1 Data Programming,[0],[0]
"P (µ)
where µ and si represent utterances and the ith skill respectively.",3.1 Data Programming,[0],[0]
"P (si|µ) the probability of a skill
being valid for an utterance is approximated by simple functions that act as candidate data generators λg ∈",3.1 Data Programming,[0],[0]
Λg based on recognitions produced by a family of query patterns λq ∈,3.1 Data Programming,[0],[0]
Λq.,3.1 Data Programming,[0],[0]
"P (µ) is represented by a family of simple functions that act as noise detectors λn ∈ Λn, which mark utterances as likely being noise.
",3.1 Data Programming,[0],[0]
"We apply the technique to the query logs of a popular IPDA, which has support for personalized third party domains.",3.1 Data Programming,[0],[0]
"Looking at the structure of utterances that match query pattern λq, each utterance of form ”Ask {Uber} to {get me a car}” can be considered as being parametrized by the underlying latent command µz , that is ”Get me a car”, a target domain corresponding to service st, which in this case is Uber and the query recognition pattern λq, in this case ”Ask {st} to {µz}”.",3.1 Data Programming,[0],[0]
"Next we assume that the distribution of latent commands over domains are independent of the query pattern.
",3.1 Data Programming,[0],[0]
"P (µz, st) ≈ P (µ, st, λq)
Making this simple distributional approximation allows us to generate a large number of noisy training samples.",3.1 Data Programming,[0],[0]
The family of generator functions λg ∈,3.1 Data Programming,[0],[0]
"Λg is thus defined such that uz = λig(µ, λiq)",3.1 Data Programming,[0],[0]
The distribution defined above contains a large number of noisy positive samples.,3.2 Noise Reduction,[0],[0]
"Related to P (µ) in the noisy channel in the spell correction context, we defined a small family of heuristic noise detection functions λn ∈ Λn that discards
training data instances that are not likely to be well formed.",3.2 Noise Reduction,[0],[0]
"For instance,
• λ1n requires u to contain a minimum threshold of information by removing those with µz that has token length fewer than 3.",3.2 Noise Reduction,[0],[0]
"Utterances shorter than this mostly consist of nonactionable commands.
",3.2 Noise Reduction,[0],[0]
"• λ2n discards all data samples below a certain threshold of occurrences in live traffic, since utterances that are rarely observed are more likely to be ASR errors or unnatural.
",3.2 Noise Reduction,[0],[0]
"• λ3n discards the data samples for a domain if they come from an overly broad pattern with a catch-all behavior.
",3.2 Noise Reduction,[0],[0]
"• λ4n discards utterances that belong to shared intents provided by the SLU SDK.
",3.2 Noise Reduction,[0],[0]
The end result of this stage is to retain utterances such as ‘call me a cab’ from ‘Ask Uber to call me a cab’ but discard ‘Boston’ from ‘Ask Accuweather for Boston’.,3.2 Noise Reduction,[0],[0]
"One can easily imagine extending this framework with other high recall noise detectors, for example, using language models to discard candidates that are unlikely to be spoken sentences.",3.2 Noise Reduction,[0],[0]
"Our model consists of a shared encoder network consisting of an orthography-sensitive hierarchical LSTM encoder that feeds into a set of domain specific classification layers trained to make a binary decision for each output label.
",4 Model Architecture,[0],[0]
"Our main novel contribution is the extension of this architecture with a personalized attention mechanism which uses the attention mechanism of (Bahdanau et al., 2014) to attend to memory locations corresponding to the specific domains enabled by a user, and allows the system to learn semantic representations of each domain via domain embeddings.",4 Model Architecture,[0],[0]
"As we will show, incorporating personalization features is key to disambiguating between multiple overlapping domains1, and the personalized attention mechanism outperforms more naive forms of personalization.",4 Model Architecture,[0],[0]
"The personalized attention mechanism first computes an attention weight for each of enabled domains, performs a convex combination to compute a context
1We assume that users can customize their IPDA settings to enable certain domains.
vector and then concatenates this vector to the encoded utterance before the final domain classification.",4 Model Architecture,[0],[0]
"Figure 1 depicts the model in detail.
",4 Model Architecture,[0],[0]
"Our model can efficiently accommodate new domains not seen during initial training by keeping the shared encoder frozen, bootstrapping a domain embedding based on existing ones, then optimizing a small number of network parameters corresponding to domain-specific classifier, which is orders of magnitude faster and more data efficient than retraining the full classifier.
",4 Model Architecture,[0],[0]
We make design decisions to ensure that our model has a low memory and latency footprint.,4 Model Architecture,[0],[0]
"We avoid expensive large vocabulary matrix multiplications on both the input and output stages, and instead use a combination of character embeddings and word embeddings in the input",4 Model Architecture,[0],[0]
stage.2,4 Model Architecture,[0],[0]
The output matrix is lightweight because each domain-specific classifier is a matrix of only 201×2 parameters.,4 Model Architecture,[0],[0]
"The inference task can be trivially parallelized across cores since there’s no requirement to compute a partition function across a high-dimensional softmax layer, which is the slowest component of large label multiclass neural networks.",4 Model Architecture,[0],[0]
"Instead, we achieve comparability between the probability scores generated by individual models by using a customized loss formulation.3",4 Model Architecture,[0],[0]
"First we describe our shared hierarchical utterance encoder, which is marked by the almond colored box in Figure 1.",4.1 Shared Encoder,[0],[0]
"Our hierarchical character to word to utterance design is motivated by the need to make the model operate on an open vocabulary in terms of words and to make it robust to small changes in orthography resulting from fluctuations in the upstream ASR system, all while avoiding expensive large matrix multiplications associated with one-hot word encoding in large vocabulary systems.
",4.1 Shared Encoder,[0],[0]
We denote an LSTM simply as a mapping φ :,4.1 Shared Encoder,[0],[0]
Rd × Rd′,4.1 Shared Encoder,[0],[0]
"→ Rd′ that takes a d dimensional input vector x and a d′ dimensional state vector h to output a new d′ dimensional state vector h′ =
2Using a one-hot representation of word vocabulary size 60,000 and hidden dimension 100 would require learning a matrix of size 60000 x 100 - using 100-dim word embeddings requires only a O(1) lookup followed by a 100 x 100 matrix, thus allowing our model to be significantly smaller and faster despite having what is effectively an open vocabulary
3Current inference consumes 50MB memory and the p99 latency is 15ms.
φ(x, h).",4.1 Shared Encoder,[0],[0]
Let C denote the set of characters andW the set of words in a given utterance.,4.1 Shared Encoder,[0],[0]
Let⊕ denote the vector concatenation operation.,4.1 Shared Encoder,[0],[0]
"We encode an utterance using BiLSTMs, and the model parameters Θ associated with this BiLSTM layer are
• Char embeddings ec ∈ R25 for each c ∈ C",4.1 Shared Encoder,[0],[0]
"• Char LSTMs φCf , φCb : R25 × R25 → R25
•",4.1 Shared Encoder,[0],[0]
Word embeddings ew ∈ R50 for each w ∈,4.1 Shared Encoder,[0],[0]
W •,4.1 Shared Encoder,[0],[0]
"Word LSTMs φWf , φWb : R100 × R50 → R50
Let w1 . . .",4.1 Shared Encoder,[0],[0]
wn ∈,4.1 Shared Encoder,[0],[0]
"W denote a word sequence where wordwi has characterwi(j) ∈ C at position j. First, the model computes a character-sensitive word representation vi ∈ R100 as
fCj = φ C f ( ewi(j), f C j−1 )
∀j = 1 . . .",4.1 Shared Encoder,[0],[0]
|wi| bCj = φ C b,4.1 Shared Encoder,[0],[0]
"( ewi(j), b C j+1 ) ∀j = |wi| . .",4.1 Shared Encoder,[0],[0]
.,4.1 Shared Encoder,[0],[0]
"1 vi = f C |wi| ⊕ b C 1 ⊕ ewi
for each i = 1 . . .",4.1 Shared Encoder,[0],[0]
n.4,4.1 Shared Encoder,[0],[0]
"These word representation vectors are encoded by forward and backward LSTMs for word φWf , φ W b as
fWi = φ W f ( vi, f W i−1 )
∀i = 1 . . .",4.1 Shared Encoder,[0],[0]
n,4.1 Shared Encoder,[0],[0]
bWi = φ W b,4.1 Shared Encoder,[0],[0]
"( vi, b W i+1 ) ∀i = n . . .",4.1 Shared Encoder,[0],[0]
"1
and induces a character and context-sensitive word representation hi ∈ R100 as
hi = f W i ⊕ bWi
for each i = 1 . . .",4.1 Shared Encoder,[0],[0]
n.,4.1 Shared Encoder,[0],[0]
"For convenience, we write the entire operation as a mapping BiLSTMΘ:
(h1 . . .",4.1 Shared Encoder,[0],[0]
hn)←,4.1 Shared Encoder,[0],[0]
BiLSTMΘ(w1 . . .,4.1 Shared Encoder,[0],[0]
"wn)
h̄ = n∑ i=1",4.1 Shared Encoder,[0],[0]
hi (1),4.1 Shared Encoder,[0],[0]
"Our Multitask domain classification formulation is motivated by a desire to avoid computing the full partition function during test time, which tends to be the slowest component of a multiclass neural network classifer, as has been documented before by (Jozefowicz et al., 2016) and (Mikolov et al., 2013), amongst others.
",4.2 Domain Classification,[0],[0]
"4For simplicity, we assume some random initial state vectors such as fC0 and bC|wi|+1 when we describe LSTMs.
",4.2 Domain Classification,[0],[0]
"However, we also want access to reliable probability estimates instead of raw scores - we accomplish this by constructing a custom loss function.",4.2 Domain Classification,[0],[0]
"During training, each domain classifier receives in-domain (IND) and out-of-domain (OOD) utterances, and we adapt the one-sided selection mechanism of (Kubat et al., 1997) to prevent OOD utterances from overpowering IND utterances, thus an utterance in a domain d ∈ D is considered as an IND utterance in the viewpoint of domain d and OOD for all other domains.
",4.2 Domain Classification,[0],[0]
We first use the shared encoder to compute the utterance representation h̄ as previously described.,4.2 Domain Classification,[0],[0]
"Then we define the probability of domain d̃ for the utterance by mapping h̄ to a 2-dimensional output vector with a linear transformation for each domain d̃ as
zd̃ = σ(W d̃ · h̄+ bd̃)
p(d̃|h̄) ∝",4.2 Domain Classification,[0],[0]
"exp ( [zd̃]IND ) , if d̃ = d exp ( [zd̃]OOD ) , otherwise
where σ is scaled exponential linear unit (SeLU) for normalized activation outputs (Klambauer et al., 2017) and",4.2 Domain Classification,[0],[0]
"[zd̃]IND and [zd̃]OOD denote the values in the IND and OOD position of vector zd̃.
We define the joint domain classification loss LD as the summation of positive (LP ) and negative (LN ) class loss functions 5:
LP ( Θ,Θd̃ ) =",4.2 Domain Classification,[0],[0]
"− log p ( d̃|h̄ ) LN ( Θ,Θd̃ )
=",4.2 Domain Classification,[0],[0]
− 1 k,4.2 Domain Classification,[0],[0]
"− 1  ∑ d̄∈D,d̄ 6=d̃ log p ( d̄|h̄ )
LD ( Θ,Θd̃ )",4.2 Domain Classification,[0],[0]
"= LP ( Θ,Θd̃ ) +",4.2 Domain Classification,[0],[0]
"LN ( Θ,Θd̃ )
where k is the total number of domains.",4.2 Domain Classification,[0],[0]
We divide the second term by k,4.2 Domain Classification,[0],[0]
− 1 so that LP and LN are balanced in terms of the ratio of the training examples for a domain to those for other domains.,4.2 Domain Classification,[0],[0]
"While a softmax over the entire domains tends to highlight only the ground-truth domain while suppressing all the rest, the our joint domain classification with a softmax over two classes is designed to produce a more balanced confidence score per domain independent of other domains.
",4.2 Domain Classification,[0],[0]
5Θd̃ denotes the additional parameters in the classification layer for domain d̃.,4.2 Domain Classification,[0],[0]
We explore encoding a user’s domain preferences in two ways.,4.3 Personalized Attention,[0],[0]
Our baseline method is a 1-bit flag that is appended to the input features of each domain-specific classifier.,4.3 Personalized Attention,[0],[0]
Our novel personalized attention method induces domain embeddings by supervising an attention mechanism to attend to a user’s enabled domains with different weights depending on their relevance.,4.3 Personalized Attention,[0],[0]
The domain embedding matrix in Figure 1 represents the embeddings of a user’s enabled domains.,4.3 Personalized Attention,[0],[0]
"We hypothesize that attention enables the network learn richer representations of user preferences and domain co-occurrence features.
",4.3 Personalized Attention,[0],[0]
Let eD(d̃) ∈ R100 and h̄ ∈ R100 denote the domain embeddings for domain d̃ and the utterance representation calculated by Eq.,4.3 Personalized Attention,[0],[0]
"(1), respectively.",4.3 Personalized Attention,[0],[0]
The domain attention weights for a given user u who has a preferred domain list d(u) =(,4.3 Personalized Attention,[0],[0]
"d̃
(u) 1 , . . .",4.3 Personalized Attention,[0],[0]
", d̃ (u) k
) are calculated by the dot-product
operation, ai = h̄ · eD ( d̃ (u) i ) ∀i",4.3 Personalized Attention,[0],[0]
= 1 . .,4.3 Personalized Attention,[0],[0]
.,4.3 Personalized Attention,[0],[0]
"k
The final, normalized attention weights ā are obtained after normalization via a softmax layer,
āi =",4.3 Personalized Attention,[0],[0]
"exp(ai)∑k j=1 exp(aj)
∀i",4.3 Personalized Attention,[0],[0]
= 1 . .,4.3 Personalized Attention,[0],[0]
.,4.3 Personalized Attention,[0],[0]
"k
The weighted combination of domain embeddings is
S̄attended = k∑ i=1",4.3 Personalized Attention,[0],[0]
( āi · eD ( d̃ (u) i )),4.3 Personalized Attention,[0],[0]
"Finally the two representations of enabled domains, namely the attention model and 1-bit flag are then concatenated with the utterance representation and used to make per-domain predictions via domain-specific affine transformations:
z̄att =",4.3 Personalized Attention,[0],[0]
"h̄⊕ S̄attended
z̄1bit =",4.3 Personalized Attention,[0],[0]
"h̄⊕ I(d̃ ∈ enabled)
",4.3 Personalized Attention,[0],[0]
Here I(d̄ ∈ enabled) is a 1-bit indicator for whether the domain is enabled by the user or not.,4.3 Personalized Attention,[0],[0]
z̄att,4.3 Personalized Attention,[0],[0]
and z̄1bit represent the encoded hidden state of the Attention and 1-Bit Flag configurations of the model from the experiment section.,4.3 Personalized Attention,[0],[0]
"In our experiments we will compare these two ways of encoding personalization information, as well
as evaluate a variant that combines the two.",4.3 Personalized Attention,[0],[0]
In this way we can ascertain whether the two personalization signals are complementary via an ablation study on the full model.,4.3 Personalized Attention,[0],[0]
Our model separates the responsibilities for utterance representation and domain classification between the shared encoder and the domain-specific classifiers.,4.4 Domain Bootstrapping,[0],[0]
"That is, the shared encoder needs to be retrained only if it cannot encode an utterance well (e.g., a new domain introduces completely new words) and the existing domain classifiers need to be retrained only when the shared encoder changes.",4.4 Domain Bootstrapping,[0],[0]
"For adding new domains efficiently without full retraining, the only two components in the architecture need to be updated for each new domain d̃new, are the domain embeddings for the new domain and its domain-specific",4.4 Domain Bootstrapping,[0],[0]
"classifier.6 We keep the weights of the encoder network frozen and use the hidden state vector h̄, calculated by Eq. 1, as a feature vector to feed into the downstream classifiers.",4.4 Domain Bootstrapping,[0],[0]
"To initialize the m-dimensional domain embeddings ed̃new , we use the column-wise average of all utterance vectors in the training data h̄avg, and project it to the domain embedding space using a matrix U ∈ Rm×m.",4.4 Domain Bootstrapping,[0],[0]
"Thus,
ed̃new = U ∗ · h̄avg
The parameters of U∗ are learned using the column-wise average utterance vectors h̄avgj and learned domain vectors for all existing domains dj
U∗ = arg min U ||U · h̄avgj − edj || ∀dj",4.4 Domain Bootstrapping,[0],[0]
∈,4.4 Domain Bootstrapping,[0],[0]
"D
This is a write-to-memory operation that creates a new domain representation after attending to all existing domain representations.",4.4 Domain Bootstrapping,[0],[0]
We then train the parameters of the domain-specific classifier with the new domain’s data while keeping the encoder fixed.,4.4 Domain Bootstrapping,[0],[0]
This mechanism allows us to efficiently support new domains that appear in-between full model deployment cycles without compromising performance on existing domains.,4.4 Domain Bootstrapping,[0],[0]
"A full model refresh would require us to fully retrain with the domains that have appeared in the intermediate period.
",4.4 Domain Bootstrapping,[0],[0]
"6We have assumed that the shared encoder covers most of the vocabulary of new domains; otherwise, the entire network may need to be retrained.",4.4 Domain Bootstrapping,[0],[0]
"Based on our observation of live usage data, this assumption is reasonable since the shared encoder after initial training is still shown to cover 95% of the vocabulary of new domains added in the subsequent week.",4.4 Domain Bootstrapping,[0],[0]
In this section we aim to demonstrate the effectiveness of our model architecture in two settings.,5 Experiments,[0],[0]
"First, we will demonstrate that attention based personalization significantly outperforms the baseline approach.",5 Experiments,[0],[0]
"Secondly, we will show that our model new domain bootstrapping procedure results in accuracies comparable to full retraining while requiring less than 1% of the orignal training time.",5 Experiments,[0],[0]
Weak: This is a weakly supervised dataset was generated by preprocessing utterances with strict invocation patterns according to the setup mentioned in Section 3.,5.1 Experimental Setup,[0],[0]
"The dataset consists of 5.34M utterances from 637,975 users across 1,500 different skills.",5.1 Experimental Setup,[0],[0]
"Since we are interested in capturing the temporal effects of the dataset as well as personalization effects, we partitioned the data based both on user and time.",5.1 Experimental Setup,[0],[0]
"Our core training data for the experiments in this paper was drawn from one month of live usage, the validation data came from the next 15 days of usage, and the test data came from the subsequent 15 days.",5.1 Experimental Setup,[0],[0]
"The training, validation and test sets are user-independent, and each user belongs to only one of the three sets to ensure no leakage of information.
MTurk:",5.1 Experimental Setup,[0],[0]
"Since the Weak dataset is generated by weak supervision, we verified the performance of our approach with human generated utterances.",5.1 Experimental Setup,[0],[0]
"A random sample of 12,428 utterances from the test partition of users were presented to 300 human judges, who were asked to produce two natural ways to issue the same command.",5.1 Experimental Setup,[0],[0]
"This dataset is treated as a representative clean held out test set on which we can observe the generalization of our weakly supervised training and validation data to
natural language.
",5.1 Experimental Setup,[0],[0]
"New Skills: In order to simulate the scenario in which new skills appear within a week between model updates, we selected 250 new skills which do not overlap with the skills in the Weak dataset.",5.1 Experimental Setup,[0],[0]
"The vocabulary size of 1,500 skills is 200K words, and on average, 5% of the vocabulary for new skills is not covered.",5.1 Experimental Setup,[0],[0]
"We randomly sampled 4,000 unique utterances for each skill using the same weak supervision method, and split them into 3,000 utterances for training and 1,000 for testing.",5.1 Experimental Setup,[0],[0]
Generalization from Weakly Supervised to Natural Utterances We first show the progression of model performance as we add more components to show their individual contribution.,5.2 Results and Discussion,[0],[0]
"Secondly, we show that training our models on a weakly supervised dataset can generalize to natural speech by showing their test performance on the human-annotated test data.",5.2 Results and Discussion,[0],[0]
"Finally, we compare two personalization strategies.
",5.2 Results and Discussion,[0],[0]
"The full results are summarized in Table 1, which shows the top-N test results separately for the Weak dataset (weakly supervised) and MTurk dataset (human-annotated).",5.2 Results and Discussion,[0],[0]
We report top-N accuracy to show the potential for further re-ranking or disambiguation downstream.,5.2 Results and Discussion,[0],[0]
"For top-1 results on the Weak dataset, using a separate binary classifier for each domain (Binary) shows a prediction accuracy at 78.29% and using a softmax layer on top of the shared encoder (MultiClass) shows a comparable accuracy at 78.58%.",5.2 Results and Discussion,[0],[0]
"The performance shows a slight improvement when using the Multitask neural loss structure, but adding personalization signals to the Multitask structure showed a significant boost in performance.",5.2 Results and Discussion,[0],[0]
We noted the large difference between the 1-bit and attention architecture.,5.2 Results and Discussion,[0],[0]
"At 94.83% accuracy, attention resulted in 35.6% relative error reduction over the 1-bit baseline 91.97% on the Weak validation set and 23.25% relative on the MTurk test set.",5.2 Results and Discussion,[0],[0]
"We hypothesize that this may be because the attention mechanism allows the model to focus on complementary features in case of overlapping domains as well as learning domain co-occurrence statistics, both of which are not possible with the simple 1-bit flag.
",5.2 Results and Discussion,[0],[0]
"When both personalization representations were combined, the performance peaked at 95.19% for the Weak dataset and a more modest
89.65% for the MTurk dataset.",5.2 Results and Discussion,[0],[0]
The improvement trend is extremely consistent across all top-N results for both of the Weak and MTurk datasets across all experiments.,5.2 Results and Discussion,[0],[0]
"The disambiguation task is complex due to similar and overlapping skills, but the results suggest that incorporating personalization signals equip the models with much better discriminative power.",5.2 Results and Discussion,[0],[0]
The results also suggest that the two mechanisms for encoding personalization provide a small amount of complementary information since combining them together is better than using them individually.,5.2 Results and Discussion,[0],[0]
"Although the performance on the Weak dataset tends to be more optimistic, the best performance on the humanannotated test data is still close to 90% for top-1 accuracy, which suggests that training our model with the samples derived from the invocation patterns can generalize well to natural utterances.
",5.2 Results and Discussion,[0],[0]
Rapid Bootstrapping of New Skills We show the results when new domains are added to an IPDA and the model needs to efficiently accommodate them with a limited number of data samples.,5.2 Results and Discussion,[0],[0]
We show the classification performance on the skills in the New Skills dataset while assuming we have access to the WEAK dataset to pre-train our model for transfer learning.,5.2 Results and Discussion,[0],[0]
"In the Binary setting, a domain-specific binary classifier is trained for each domain.",5.2 Results and Discussion,[0],[0]
Expand describes the case in which we incrementally train on top of an existing model.,5.2 Results and Discussion,[0],[0]
"Refresh is the setting in which the model is fully retrained from scratch with the new data - which would be ideal in case there were no time constraints.
",5.2 Results and Discussion,[0],[0]
We record the average training time for each epoch and the performance is measured with top-1 classification accuracy over new skills.,5.2 Results and Discussion,[0],[0]
The experiment results can be found in Table 2.,5.2 Results and Discussion,[0],[0]
Adapting a new skill is two orders of magnitude faster (30.34 seconds) than retraining the model (5300.18 seconds) while achieving 94.03% accuracy which is comparable to 94.58% accuracy of full retraining.,5.2 Results and Discussion,[0],[0]
"The first two techniques can also be easily parallelized unlike the Refresh configuration.
",5.2 Results and Discussion,[0],[0]
Behavior of Attention Mechanism Our expectation is that the model is able to learn to attend the relevant skills during the inference process.,5.2 Results and Discussion,[0],[0]
"To study the behavior of the attention layer, we compute the top-N prediction accuracy based on the most relevant skills defined by the attention weights.",5.2 Results and Discussion,[0],[0]
"In this experiment, we considered the subset of users who had enabled more than 20 domains to exclude trivial cases7.",5.2 Results and Discussion,[0],[0]
The results are shown in Table 3.,5.2 Results and Discussion,[0],[0]
"When the model attends to the entire set of 1500 (Full), the top-5 prediction accuracy is 20.41%, which indicates that a large number of skills can process the utterance, and thus it is highly likely to miss the correct one in the top-5 predictions.",5.2 Results and Discussion,[0],[0]
"This ambiguity issue can be significantly improved by users’ enabled domain lists as proved by the accuracies (Enabled): 85.62% for top-1, 96.15% for top-3, and 98.06% for top-5.8",5.2 Results and Discussion,[0],[0]
"Thus the attention mechanism can thus be viewed as an initial soft selection which is then followed by a fine-grained selection at the classification stage.
",5.2 Results and Discussion,[0],[0]
End-to-End User Evaluation All intermediate metrics on this task are proxies to a human customer’s eventual evaluation.,5.2 Results and Discussion,[0],[0]
"In order to assess the user experience, we need to measure its end-toend performance.",5.2 Results and Discussion,[0],[0]
"For a brief end-to-end system evaluation, 983 utterances from 283 domains were randomly sampled from the test set in the largescale IPDA setting.",5.2 Results and Discussion,[0],[0]
"15 human judges (male=12, female=3) rated the system responses, 1 judge per utterance, on a 5-point Likert scale with 1 being Terrible and 5 being Perfect.",5.2 Results and Discussion,[0],[0]
The judgment score of 3 or above was taken as SUCCESS and 2 or below as DEFECT.,5.2 Results and Discussion,[0],[0]
"The end-to-end SUCCESS rate,
7Thus, the random prediction accuracy on enabled domains is less than 5% and across the Full domain list is 0.066%
8Visual inspection of the embeddings confirms that meaningful clusters are learned.",5.2 Results and Discussion,[0],[0]
"We see clusters related to home automation, commerce, cooking, trivia etc, we show some examples in Figure 2, 3 and 4.",5.2 Results and Discussion,[0],[0]
However there are still other clusters where the the relationships cannot be established as easily.,5.2 Results and Discussion,[0],[0]
An example of these is show in Figure 5.,5.2 Results and Discussion,[0],[0]
"The personalized attention mechanism is learned using the semantic content as well as personalization signals, so we hypothesize clusters like this may be capturing user tendencies to enable these domains in a correlated manner.
",5.2 Results and Discussion,[0],[0]
"thus user satisfaction, was shown to be 95.52%.",5.2 Results and Discussion,[0],[0]
"The discrepancy between this score and the score produced on MTurk dataset indicates that even in cases in which the model makes classification mistakes, some of these interpretations remain perceptually meaningful to humans.",5.2 Results and Discussion,[0],[0]
We have described a neural model architecture to address large-scale skill classification in an IPDA used by tens of millions of users every day.,6 Conclusions,[0],[0]
We have described how personalization features and an attention mechanism can be used for handling ambiguity between domains.,6 Conclusions,[0],[0]
"We have also shown that the model can be extended efficiently and incrementally for new domains, saving multiple orders of magnitude in terms of training time.",6 Conclusions,[0],[0]
"The model also addresses practical constraints of having a low memory footprint, low latency and being easily parallelized, all of which are important characteristics for real-time production systems.",6 Conclusions,[0],[0]
"In future work, we plan to incorporate various types of context (e.g. anaphora, device-specific capabilities) and dialogue history into a large-scale NLU system.",6 Conclusions,[0],[0]
"In this paper, we explore the task of mapping spoken language utterances to one of thousands of natural language understanding domains in intelligent personal digital assistants (IPDAs).",abstractText,[0],[0]
This scenario is observed in mainstream IPDAs in industry that allow third parties to develop thousands of new domains to augment builtin first party domains to rapidly increase domain coverage and overall IPDA capabilities.,abstractText,[0],[0]
"We propose a scalable neural model architecture with a shared encoder, a novel attention mechanism that incorporates personalization information and domain-specific classifiers that solves the problem efficiently.",abstractText,[0],[0]
Our architecture is designed to efficiently accommodate incremental domain additions achieving two orders of magnitude speed up compared to full model retraining.,abstractText,[0],[0]
"We consider the practical constraints of real-time production systems, and design to minimize memory footprint and runtime latency.",abstractText,[0],[0]
We demonstrate that incorporating personalization significantly improves domain classification accuracy in a setting with thousands of overlapping domains.,abstractText,[0],[0]
Efficient Large-Scale Neural Domain Classification with Personalized Attention,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2247–2256 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2247",text,[0],[0]
Multimodal research has shown great progress in a variety of tasks as an emerging research field of artificial intelligence.,1 Introduction,[0],[0]
"Tasks such as speech recognition (Yuhas et al., 1989), emotion recognition, (De Silva et al., 1997), (Chen et al., 1998), (Wöllmer et al., 2013), sentiment analysis, (Morency et al., 2011)
∗ equal contributions
as well as speaker trait analysis and media description (Park et al., 2014a) have seen a great boost in performance with developments in multimodal research.
",1 Introduction,[0],[0]
"However, a core research challenge yet to be solved in this domain is multimodal fusion.",1 Introduction,[0],[0]
The goal of fusion is to combine multiple modalities to leverage the complementarity of heterogeneous data and provide more robust predictions.,1 Introduction,[0],[0]
"In this regard, an important challenge has been on scaling up fusion to multiple modalities while maintaining reasonable model complexity.",1 Introduction,[0],[0]
"Some of the recent attempts (Fukui et al., 2016), (Zadeh et al., 2017) at multimodal fusion investigate the use of tensors for multimodal representation and show significant improvement in performance.",1 Introduction,[0],[0]
"Unfortunately, they are often constrained by the exponential increase of cost in computation and memory introduced by using tensor representations.",1 Introduction,[0],[0]
"This heavily restricts the applicability of these models, especially when we have more than two views of modalities in the dataset.
",1 Introduction,[0],[0]
"In this paper, we propose the Low-rank Multimodal Fusion, a method leveraging low-rank weight tensors to make multimodal fusion efficient without compromising on performance.",1 Introduction,[0],[0]
The overall architecture is shown in Figure 1.,1 Introduction,[0],[0]
We evaluated our approach with experiments on three multimodal tasks using public datasets and compare its performance with state-of-the-art models.,1 Introduction,[0],[0]
We also study how different low-rank settings impact the performance of our model and show that our model performs robustly within a wide range of rank settings.,1 Introduction,[0],[0]
"Finally, we perform an analysis of the impact of our method on the number of parameters and run-time with comparison to other fusion methods.",1 Introduction,[0],[0]
"Through theoretical analysis, we show that our model can scale linearly in the number of modalities, and our experiments also show a corresponding speedup in training when compared with
other tensor-based models.",1 Introduction,[0],[0]
"The main contributions of our paper are as follows:
• We propose the Low-rank Multimodal Fusion method for multimodal fusion that can scale linearly in the number of modalities.
",1 Introduction,[0],[0]
"• We show that our model compares to state-ofthe-art models in performance on three multimodal tasks evaluated on public datasets.
",1 Introduction,[0],[0]
• We show that our model is computationally efficient and has fewer parameters in comparison to previous tensor-based methods.,1 Introduction,[0],[0]
"Multimodal fusion enables us to leverage complementary information present in multimodal data, thus discovering the dependency of information on multiple modalities.",2 Related Work,[0],[0]
"Previous studies have shown that more effective fusion methods translate to better performance in models, and there’s been a wide range of fusion methods.
",2 Related Work,[0],[0]
Early fusion is a technique that uses feature concatenation as the method of fusion of different views.,2 Related Work,[0],[0]
"Several works that use this method of fusion (Poria et al., 2016) , (Wang et al., 2016) use input-level feature concatenation and use the
concatenated features as input, sometimes even removing the temporal dependency present in the modalities (Morency et al., 2011).",2 Related Work,[0],[0]
"The drawback of this class of method is that although it achieves fusion at an early stage, intra-modal interactions are potentially suppressed, thus losing out on the context and temporal dependencies within each modality.
",2 Related Work,[0],[0]
"On the other hand, late fusion builds separate models for each modality and then integrates the outputs together using a method such as majority voting or weighted averaging (Wortwein and Scherer, 2017), (Nojavanasghari et al., 2016).",2 Related Work,[0],[0]
"Since separate models are built for each modality, inter-modal interactions are usually not modeled effectively.
",2 Related Work,[0],[0]
"Given these shortcomings, more recent work focuses on intermediate approaches that model both intra- and inter-modal dynamics.",2 Related Work,[0],[0]
Fukui et al. (2016) proposes to use Compact Bilinear Pooling over the outer product of visual and linguistic representations to exploit the interactions between vision and language for visual question answering.,2 Related Work,[0],[0]
"Similar to the idea of exploiting interactions, Zadeh et al. (2017) proposes Tensor Fusion Network, which computes the outer product between unimodal representations from three different modalities to compute a tensor representation.",2 Related Work,[0],[0]
"These methods exploit tensor representations to model
inter-modality interactions and have shown a great success.",2 Related Work,[0],[0]
"However, such methods suffer from exponentially increasing computational complexity, as the outer product over multiple modalities results in extremely high dimensional tensor representations.
",2 Related Work,[0],[0]
"For unimodal data, the method of low-rank tensor approximation has been used in a variety of applications to implement more efficient tensor operations.",2 Related Work,[0],[0]
"Razenshteyn et al. (2016) proposes a modified weighted version of low-rank approximation, and Koch and Lubich (2010) applies the method towards temporally dependent data to obtain lowrank approximations.",2 Related Work,[0],[0]
"As for applications, Lei et al. (2014) proposes a low-rank tensor technique for dependency parsing while Wang and Ahuja (2008) uses the method of low-rank approximation applied directly on multidimensional image data (Datumas-is representation) to enhance computer vision applications.",2 Related Work,[0],[0]
Hu et al. (2017) proposes a low-rank tensor-based fusion framework to improve the face recognition performance using the fusion of facial attribute information.,2 Related Work,[0],[0]
"However, none of these previous work aims to apply low-rank tensor techniques for multimodal fusion.
",2 Related Work,[0],[0]
Our Low-rank Multimodal Fusion method provides a much more efficient method to compute tensor-based multimodal representations with much fewer parameters and computational complexity.,2 Related Work,[0],[0]
"The efficiency and performance of our approach are evaluated on different downstream tasks, namely sentiment analysis, speaker-trait recognition and emotion recognition.",2 Related Work,[0],[0]
"In this section, we start by formulating the problem of multimodal fusion and introducing fusion methods based on tensor representations.",3 Low-rank Multimodal Fusion,[0],[0]
Tensors are powerful in their expressiveness but do not scale well to a large number of modalities.,3 Low-rank Multimodal Fusion,[0],[0]
"Our proposed model decomposes the weights into low-rank factors, which reduces the number of parameters in the model.",3 Low-rank Multimodal Fusion,[0],[0]
This decomposition can be performed efficiently by exploiting the parallel decomposition of low-rank weight tensor and input tensor to compute tensor-based fusion.,3 Low-rank Multimodal Fusion,[0],[0]
Our method is able to scale linearly with the number of modalities.,3 Low-rank Multimodal Fusion,[0],[0]
"In this paper, we formulate multimodal fusion as a multilinear function f ∶ V1 × V2 × ... × VM →
H where V1, V2, ..., VM are the vector spaces of input modalities and H is the output vector space.",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"Given a set of vector representations, {zm}Mm=1 which are encoding unimodal information of the M different modalities, the goal of multimodal fusion is to integrate the unimodal representations into one compact multimodal representation for downstream tasks.
",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
Tensor representation is one successful approach for multimodal fusion.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
It first requires a transformation of the input representations into a highdimensional tensor and then mapping it back to a lower-dimensional output vector space.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"Previous works have shown that this method is more effective than simple concatenation or pooling in terms of capturing multimodal interactions (Zadeh et al., 2017), (Fukui et al., 2016).",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
Tensors are usually created by taking the outer product over the input modalities.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"In addition, in order to be able to model the interactions between any subset of modalities using one tensor, Zadeh et al. (2017) proposed a simple extension to append 1s to the unimodal representations before taking the outer product.",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"The input tensor Z formed by the unimodal representation is computed by:
Z = M
⊗ m=1",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"zm, zm",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"∈ Rdm (1)
where⊗Mm=1 denotes the tensor outer product over a set of vectors indexed by m, and zm is the input representation with appended 1s.
",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"The input tensor Z ∈ Rd1×d2×...dM is then passed through a linear layer g(⋅) to to produce a vector representation:
h = g(Z;W, b) =",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"W ⋅Z + b, h, b ∈",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
Rdy (2) where W is the weight of this layer and b is the bias.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"With Z being an order-M tensor (where M is the number of input modalities), the weight W will naturally be a tensor of order-(M + 1) in Rd1×d2×...×dM×dh .",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
The extra (M +1)-th dimension corresponds to the size of the output representation dh.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"In the tensor dot productW ⋅Z , the weight tensorW can be then viewed as dh order-M tensors.",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"In other words, the weight W can be partitioned into W̃k ∈ Rd1×...×dM , k = 1, ..., dh.",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"Each W̃k contributes to one dimension in the output vector h, i.e. hk = W̃k ⋅Z .",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"This interpretation of tensor fusion is illustrated in Figure 2 for the bi-modal case.
",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
One of the main drawbacks of tensor fusion is that we have to explicitly create the highdimensional tensor Z .,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"The dimensionality of Z
will increase exponentially with the number of modalities as∏Mm=1 dm.",3.1 Multimodal Fusion using Tensor Representations,[0],[0]
The number of parameters to learn in the weight tensorW will also increase exponentially.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
This not only introduces a lot of computation but also exposes the model to risks of overfitting.,3.1 Multimodal Fusion using Tensor Representations,[0],[0]
"As a solution to the problems of tensor-based fusion, we propose Low-rank Multimodal Fusion (LMF).",3.2 Low-rank Multimodal Fusion with Modality-Specific Factors,[0],[0]
"LMF parameterizes g(⋅) from Equation 2 with a set of modality-specific low-rank factors that can be used to recover a low-rank weight tensor, in contrast to the full tensorW .",3.2 Low-rank Multimodal Fusion with Modality-Specific Factors,[0],[0]
"Moreover, we show that by decomposing the weight into a set of low-rank factors, we can exploit the fact that the tensor Z actually decomposes into {zm}Mm=1, which allows us to directly compute the output h without explicitly tensorizing the unimodal representations.",3.2 Low-rank Multimodal Fusion with Modality-Specific Factors,[0],[0]
LMF reduces the number of parameters as well as the computation complexity involved in tensorization from being exponential in M to linear.,3.2 Low-rank Multimodal Fusion with Modality-Specific Factors,[0],[0]
The idea of LMF is to decompose the weight tensor W into M sets of modality-specific factors.,3.2.1 Low-rank Weight Decomposition,[0],[0]
"However, since W itself is an order-(M + 1) tensor, commonly used methods for decomposition will result in M + 1 parts.",3.2.1 Low-rank Weight Decomposition,[0],[0]
"Hence, we still adopt the view introduced in Section 3.1 thatW is formed by dh order-M tensors",3.2.1 Low-rank Weight Decomposition,[0],[0]
"W̃k ∈ Rd1×...×dM , k = 1, ..., dh stacked together.",3.2.1 Low-rank Weight Decomposition,[0],[0]
"We can then decompose each W̃k separately.
",3.2.1 Low-rank Weight Decomposition,[0],[0]
"For an order-M tensor W̃k ∈ Rd1×...×dM , there always exists an exact decomposition into vectors in the form of:
W̃k = R
∑ i=1
M
⊗ m=1
w (i) m,k, w (i) m,k ∈ R d m (3)
",3.2.1 Low-rank Weight Decomposition,[0],[0]
The minimal R that makes the decomposition valid is called the rank of the tensor.,3.2.1 Low-rank Weight Decomposition,[0],[0]
"The vector sets
{{w(i)m,k} M m=1}Ri=1 are called the rank R decomposition factors of the original tensor.",3.2.1 Low-rank Weight Decomposition,[0],[0]
"In LMF, we start with a fixed rank r, and parameterize the model with r decomposition factors {{w(i)m,k} M m=1}ri=1, k = 1, ..., dh that can be used to reconstruct a low-rank version of these W̃k.",3.2.1 Low-rank Weight Decomposition,[0],[0]
We can regroup and concatenate these vectors into M modality-specific low-rank factors.,3.2.1 Low-rank Weight Decomposition,[0],[0]
"Let w (i) m = [w(i)m,1,w (i) m,2, ...,w (i) m,dh
], then for modality m, {w(i)m }ri=1 is its corresponding low-rank factors.",3.2.1 Low-rank Weight Decomposition,[0],[0]
"And we can recover a low-rank weight tensor by:
W = r
∑ i=1
M
⊗ m=1",3.2.1 Low-rank Weight Decomposition,[0],[0]
"w(i)m (4)
",3.2.1 Low-rank Weight Decomposition,[0],[0]
"Hence equation 2 can be computed by
h =",3.2.1 Low-rank Weight Decomposition,[0],[0]
"( r
∑ i=1
M
⊗ m=1 w(i)m ) ⋅Z",3.2.1 Low-rank Weight Decomposition,[0],[0]
"(5)
Note that for all m, w(i)m ∈ Rdm×dh shares the same size for the second dimension.",3.2.1 Low-rank Weight Decomposition,[0],[0]
We define their outer product to be over only the dimensions that are not shared: w(i)m ⊗w(i)n ∈ Rdm×dn×dh .,3.2.1 Low-rank Weight Decomposition,[0],[0]
"A bimodal example of this procedure is illustrated in Figure 3.
",3.2.1 Low-rank Weight Decomposition,[0],[0]
"Nevertheless, by introducing the low-rank factors, we now have to compute the reconstruction ofW = ∑ri=1⊗Mm=1w(i)m",3.2.1 Low-rank Weight Decomposition,[0],[0]
for the forward computation.,3.2.1 Low-rank Weight Decomposition,[0],[0]
Yet this introduces even more computation.,3.2.1 Low-rank Weight Decomposition,[0],[0]
"In this section, we will introduce an efficient procedure for computing h, exploiting the fact that tensor Z naturally decomposes into the original input {zm}Mm=1, which is parallel to the modalityspecific low-rank factors.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"In fact, that is the main reason why we want to decompose the weight tensor into M modality-specific factors.
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Using the fact that Z =⊗Mm=1 zm, we can simplify equation 5:
h =",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"( r
∑ i=1
M
⊗ m=1 w(i)m ) ⋅Z
= r
∑ i=1
( M
⊗ m=1 w(i)m ⋅Z)
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"= r
∑ i=1
( M
⊗ m=1
w(i)m",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"⋅ M
⊗ m=1 zm)
= M
Λ m=1
[ r
∑ i=1
w(i)m ⋅ zm] (6)
where ΛMm=1 denotes the element-wise product over a sequence of tensors:",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Λ3t=1 xt = x1 ○ x2 ○ x3.
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
An illustration of the trimodal case of equation 6 is shown in Figure 1.,3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"We can also derive equation 6 for a bimodal case to clarify what it does:
h =",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"( r
∑ i=1
w(i)a ⊗w(i)v ) ⋅Z
= ( r
∑ i=1
w(i)a ⋅ za) ○",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"( r
∑ i=1
w(i)v ⋅ zv) (7)
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"An important aspect of this simplification is that it exploits the parallel decomposition of both Z andW , so that we can compute h without actually creating the tensor Z from the input representations zm.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"In addition, different modalities are decoupled in the simplified computation of h, which allows for easy generalization of our approach to an arbitrary number of modalities.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
Adding a new modality can be simply done by adding another set of modality-specific factors and extend Equation 7.,3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Last but not least, Equation 6 consists of fully differentiable operations, which enables the parameters {w(i)m }ri=1m = 1, ...,M to be learned end-to-end via back-propagation.
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Using Equation 6, we can compute h directly from input unimodal representations and their modal-specific decomposition factors, avoiding the weight-lifting of computing the large input tensor Z and W , as well as the r linear transformation.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Instead, the input tensor and subsequent linear projection are computed implicitly together in Equation 6, and this is far more efficient than the original method described in Section 3.1.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"Indeed, LMF reduces the computation complexity of tensorization and fusion from O(dy∏Mm=1 dm) to O(dy × r ×∑Mm=1 dm).
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"In practice, we use a slightly different form of Equation 6, where we concatenate the low-rank
factors into M order-3 tensors and swap the order in which we do the element-wise product and summation:
h = r
∑ i=1
",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"[ M
Λ m=1",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"[w(1)m ,w(2)m , ...,w(r)m ] ⋅ ẑm] i,∶ (8)
and now the summation is done along the first dimension of the bracketed matrix.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"[⋅]i,∶ indicates the i-th slice of a matrix.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"In this way, we can parameterize the model with M order-3 tensors, instead of parameterizing with sets of vectors.",3.2.2 Efficient Low-rank Fusion Exploiting Parallel Decomposition,[0],[0]
"We compare LMF with previous state-of-the-art baselines, and we use the Tensor Fusion Networks (TFN) (Zadeh et al., 2017) as a baseline for tensorbased approaches, which has the most similar structure with us except that it explicitly forms the large multi-dimensional tensor for fusion across different modalities.
",4 Experimental Methodology,[0],[0]
We design our experiments to better understand the characteristics of LMF.,4 Experimental Methodology,[0],[0]
Our goal is to answer the following four research questions: (1) Impact of Multimodal Low-rank Fusion: Direct comparison between our proposed LMF model and the previous TFN model.,4 Experimental Methodology,[0],[0]
(2) Comparison with the State-of-the-art: We evaluate the performance of LMF and state-of-theart baselines on three different tasks and datasets.,4 Experimental Methodology,[0],[0]
(3) Complexity Analysis: We study the modal complexity of LMF and compare it with the TFN model.,4 Experimental Methodology,[0],[0]
"(4) Rank Settings: We explore performance of LMF with different rank settings.
",4 Experimental Methodology,[0],[0]
The results of these experiments are presented in Section 5.,4 Experimental Methodology,[0],[0]
"We perform our experiments on the following multimodal datasets, CMU-MOSI (Zadeh et al., 2016a),
POM (Park et al., 2014b), and IEMOCAP (Busso et al., 2008) for sentiment analysis, speaker traits recognition, and emotion recognition task, where the goal is to identify speakers emotions based on the speakers’ verbal and nonverbal behaviors.",4.1 Datasets,[0],[0]
CMU-MOSI The CMU-MOSI dataset is a collection of 93 opinion videos from YouTube movie reviews.,4.1 Datasets,[0],[0]
Each video consists of multiple opinion segments and each segment is annotated with the sentiment in the range,4.1 Datasets,[0],[0]
"[-3,3], where -3 indicates highly negative and 3 indicates highly positive.",4.1 Datasets,[0],[0]
POM,4.1 Datasets,[0],[0]
The POM dataset is composed of 903 movie review videos.,4.1 Datasets,[0],[0]
"Each video is annotated with the following speaker traits: confident, passionate, voice pleasant, dominant, credible, vivid, expertise, entertaining, reserved, trusting, relaxed, outgoing, thorough, nervous, persuasive and humorous.",4.1 Datasets,[0],[0]
"IEMOCAP The IEMOCAP dataset is a collection of 151 videos of recorded dialogues, with 2 speakers per session for a total of 302 videos across the dataset.",4.1 Datasets,[0],[0]
"Each segment is annotated for the presence of 9 emotions (angry, excited, fear, sad, surprised, frustrated, happy, disappointed and neutral).
",4.1 Datasets,[0],[0]
"To evaluate model generalization, all datasets are split into training, validation, and test sets such that the splits are speaker independent, i.e., no identical speakers from the training set are present in the test sets.",4.1 Datasets,[0],[0]
Table 1 illustrates the data splits for all datasets in detail.,4.1 Datasets,[0],[0]
"Each dataset consists of three modalities, namely language, visual, and acoustic modalities.",4.2 Features,[0],[0]
"To reach the same time alignment across modalities, we perform word alignment using P2FA (Yuan and Liberman, 2008) which allows us to align the three modalities at the word granularity.",4.2 Features,[0],[0]
"We calculate the visual and acoustic features by taking the average of their feature values over the word time interval (Chen et al., 2017).",4.2 Features,[0],[0]
"Language We use pre-trained 300-dimensional Glove word embeddings (Pennington et al., 2014) to encode a sequence of transcribed words into a sequence of word vectors.
",4.2 Features,[0],[0]
"Visual The library Facet1 is used to extract a set of visual features for each frame (sampled at 30Hz) including 20 facial action units, 68 facial landmarks, head pose, gaze tracking and HOG features (Zhu et al., 2006).",4.2 Features,[0],[0]
"Acoustic We use COVAREP acoustic analysis framework (Degottex et al., 2014) to extract a set of low-level acoustic features, including 12 Mel frequency cepstral coefficients (MFCCs), pitch, voiced/unvoiced segmentation, glottal source, peak slope, and maxima dispersion quotient features.",4.2 Features,[0],[0]
"In order to compare our fusion method with previous work, we adopt a simple and straightforward model architecture 2 for extracting unimodal representations.",4.3 Model Architecture,[0],[0]
"Since we have three modalities for each dataset, we simply designed three unimodal sub-embedding networks, denoted as fa, fv, fl, to extract unimodal representations za, zv, zl from unimodal input features xa, xv, xl.",4.3 Model Architecture,[0],[0]
"For acoustic and visual modality, the sub-embedding network is a simple 2-layer feed-forward neural network, and for language modality, we used an LSTM (Hochreiter and Schmidhuber, 1997) to extract representations.",4.3 Model Architecture,[0],[0]
The model architecture is illustrated in Figure 1.,4.3 Model Architecture,[0],[0]
"We compare the performance of LMF to the following baselines and state-of-the-art models in multimodal sentiment analysis, speaker trait recognition, and emotion recognition.",4.4 Baseline Models,[0],[0]
"Support Vector Machines Support Vector Machines (SVM) (Cortes and Vapnik, 1995) is a widely used non-neural classifier.",4.4 Baseline Models,[0],[0]
"This baseline is trained on the concatenated multimodal features for classification or regression task (Pérez-Rosas et al., 2013), (Park et al., 2014a), (Zadeh et al., 2016b).",4.4 Baseline Models,[0],[0]
"Deep Fusion The Deep Fusion model (DF) (Nojavanasghari et al., 2016) trains one deep neural model for each modality and then combine the output of each modality network with a joint neural network.",4.4 Baseline Models,[0],[0]
Tensor Fusion Network,4.4 Baseline Models,[0],[0]
"The Tensor Fusion Network (TFN) (Zadeh et al., 2017) explicitly models view-specific and cross-view dynamics by creating a multi-dimensional tensor that captures uni-
1goo.gl/1rh1JN 2The source code of our model is available on Github at https://github.com/Justin1904/Low-rank-Multimodal-Fusion
modal, bimodal and trimodal interactions across three modalities.",4.4 Baseline Models,[0],[0]
"Memory Fusion Network The Memory Fusion Network (MFN) (Zadeh et al., 2018a) accounts for view-specific and cross-view interactions and continuously models them through time with a special attention mechanism and summarized through time with a Multi-view Gated Memory.",4.4 Baseline Models,[0],[0]
Bidirectional Contextual LSTM The Bidirectional Contextual LSTM (BC-LSTM),4.4 Baseline Models,[0],[0]
"(Zadeh et al., 2017), (Fukui et al., 2016) performs contextdependent fusion of multimodal data.",4.4 Baseline Models,[0],[0]
"Multi-View LSTM The Multi-View LSTM (MVLSTM) (Rajagopalan et al., 2016) aims to capture both modality-specific and cross-modality interactions from multiple modalities by partitioning the memory cell and the gates corresponding to multiple modalities.",4.4 Baseline Models,[0],[0]
"Multi-attention Recurrent Network The Multiattention Recurrent Network (MARN) (Zadeh et al., 2018b) explicitly models interactions between modalities through time using a neural component called the Multi-attention Block (MAB) and storing them in the hybrid memory called the Long-short Term Hybrid Memory (LSTHM).",4.4 Baseline Models,[0],[0]
Multiple evaluation tasks are performed during our evaluation: multi-class classification and regression.,4.5 Evaluation Metrics,[0],[0]
"The multi-class classification task is applied to all three multimodal datasets, and the regression task is applied to the CMU-MOSI and the POM dataset.",4.5 Evaluation Metrics,[0],[0]
"For binary classification and multiclass classification, we report F1 score and accuracy Acc−k where k denotes the number of classes.",4.5 Evaluation Metrics,[0],[0]
"Specifically, Acc−2 stands for the binary classification.",4.5 Evaluation Metrics,[0],[0]
"For regression, we report Mean Absolute Error (MAE) and Pearson correlation (Corr).",4.5 Evaluation Metrics,[0],[0]
Higher values denote better performance for all metrics except for MAE.,4.5 Evaluation Metrics,[0],[0]
"In this section, we present and discuss the results from the experiments designed to study the research questions introduced in section 4.",5 Results and Discussion,[0],[0]
"In this experiment, we compare our model directly with the TFN model since it has the most similar structure to our model, except that TFN explicitly forms the multimodal tensor fusion.",5.1 Impact of Low-rank Multimodal Fusion,[0],[0]
"The com-
parison reported in the last two rows of Table 2 demonstrates that our model significantly outperforms TFN across all datasets and metrics.",5.1 Impact of Low-rank Multimodal Fusion,[0],[0]
This competitive performance of LMF compared to TFN emphasizes the advantage of Low-rank Multimodal Fusion.,5.1 Impact of Low-rank Multimodal Fusion,[0],[0]
"We compare our model with the baselines and stateof-the-art models for sentiment analysis, speaker traits recognition and emotion recognition.",5.2 Comparison with the State-of-the-art,[0],[0]
Results are shown in Table 2.,5.2 Comparison with the State-of-the-art,[0],[0]
"LMF is able to achieve competitive and consistent results across all datasets.
",5.2 Comparison with the State-of-the-art,[0],[0]
"On the multimodal sentiment regression task, LMF outperforms the previous state-of-the-art model on MAE and Corr.",5.2 Comparison with the State-of-the-art,[0],[0]
"Note the multiclass accuracy is calculated by mapping the range of continuous sentiment values into a set of intervals that are used as discrete classes.
",5.2 Comparison with the State-of-the-art,[0],[0]
"On the multimodal speaker traits Recognition task, we report the average evaluation score over 16 speaker traits and shows that our model achieves the state-of-the-art performance over all three evaluation metrics on the POM dataset.
",5.2 Comparison with the State-of-the-art,[0],[0]
"On the multimodal emotion recognition task, our model achieves better results compared to the stateof-the-art models across all emotions on the F1 score.",5.2 Comparison with the State-of-the-art,[0],[0]
F1-emotion in the evaluation metrics indicates the F1 score for a certain emotion class.,5.2 Comparison with the State-of-the-art,[0],[0]
"Theoretically, the model complexity of our fusion method is O(dy × r × ∑Mm=1 dm) compared to O(dy∏Mm=1 dm) of TFN from Section 3.1.",5.3 Complexity Analysis,[0],[0]
"In practice, we calculate the total number of parameters used in each model, where we choose M = 3, d1 = 32, d2 = 32, d3 = 64, r = 4, dy = 1.",5.3 Complexity Analysis,[0],[0]
"Under this hyper-parameter setting, our model contains about 1.1e6 parameters while TFN contains about 12.5e6 parameters, which is nearly 11 times more.",5.3 Complexity Analysis,[0],[0]
"Note that, the number of parameters above counts not only the parameters in the multimodal fusion stage but also the parameters in the subnetworks.
",5.3 Complexity Analysis,[0],[0]
"Furthermore, we evaluate the computational complexity of LMF by measuring the training and testing speeds between LMF and TFN.",5.3 Complexity Analysis,[0],[0]
Table 3 illustrates the impact of Low-rank Multimodal Fusion on the training and testing speeds compared with TFN model.,5.3 Complexity Analysis,[0],[0]
"Here we set rank to be 4 since it can generally achieve fairly competent performance.
",5.3 Complexity Analysis,[0],[0]
"Dataset CMU-MOSI POM IEMOCAP Metric MAE Corr Acc-2 F1 Acc-7 MAE Corr Acc F1-Happy F1-Sad F1-Angry F1-Neutral SVM 1.864 0.057 50.2 50.1 17.5 0.887 0.104 33.9 81.5 78.8 82.4 64.9 DF 1.143 0.518 72.3 72.1 26.8 0.869 0.144 34.1 81.0 81.2 65.4 44.0 BC-LSTM 1.079 0.581 73.9 73.9 28.7 0.840 0.278 34.8 81.7 81.7 84.2 64.1 MV-LSTM 1.019 0.601 73.9 74.0 33.2 0.891 0.270 34.6 81.3 74.0 84.3 66.7 MARN 0.968 0.625 77.1 77.0 34.7 - - 39.4 83.6 81.2 84.2 65.9 MFN 0.965 0.632 77.4 77.3 34.1 0.805 0.349 41.7 84.0 82.1 83.7 69.2 TFN 0.970 0.633 73.9 73.4 32.1 0.886 0.093 31.6 83.6 82.8 84.2 65.4 LMF 0.912 0.668 76.4 75.7 32.8 0.796 0.396 42.8 85.8 85.9 89.0 71.7
Table 2: Results for sentiment analysis on CMU-MOSI, emotion recognition on IEMOCAP and personality trait recognition on POM.",5.3 Complexity Analysis,[0],[0]
"Best results are highlighted in bold.
",5.3 Complexity Analysis,[0],[0]
"Based on these results, performing a low-rank multimodal fusion with modality-specific low-rank factors significantly reduces the amount of time needed for training and testing the model.",5.3 Complexity Analysis,[0],[0]
"On an NVIDIA Quadro K4200 GPU, LMF trains with an average frequency of 1134.82 IPS (data point inferences per second) while the TFN model trains at an average of 340.74 IPS.",5.3 Complexity Analysis,[0],[0]
"To evaluate the impact of different rank settings for our LMF model, we measure the change in performance on the CMU-MOSI dataset while varying
the number of rank.",5.4 Rank Settings,[0],[0]
The results are presented in Figure 4.,5.4 Rank Settings,[0],[0]
"We observed that as the rank increases, the training results become more and more unstable and that using a very low rank is enough to achieve fairly competent performance.",5.4 Rank Settings,[0],[0]
"In this paper, we introduce a Low-rank Multimodal Fusion method that performs multimodal fusion with modality-specific low-rank factors.",6 Conclusion,[0],[0]
LMF scales linearly in the number of modalities.,6 Conclusion,[0],[0]
LMF achieves competitive results across different multimodal tasks.,6 Conclusion,[0],[0]
"Furthermore, LMF demonstrates a significant decrease in computational complexity from exponential to linear time.",6 Conclusion,[0],[0]
"In practice, LMF effectively improves the training and testing efficiency compared to TFN which performs multimodal fusion with tensor representations.
",6 Conclusion,[0],[0]
"Future work on similar topics could explore the applications of using low-rank tensors for attention models over tensor representations, as they can be even more memory and computationally intensive.",6 Conclusion,[0],[0]
This material is based upon work partially supported by the National Science Foundation (Award # 1833355) and Oculus VR.,Acknowledgements,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of National Science Foundation or Oculus VR, and no official endorsement should be inferred.",Acknowledgements,[0],[0]
"Multimodal research is an emerging field of artificial intelligence, and one of the main research problems in this field is multimodal fusion.",abstractText,[0],[0]
The fusion of multimodal data is the process of integrating multiple unimodal representations into one compact multimodal representation.,abstractText,[0],[0]
Previous research in this field has exploited the expressiveness of tensors for multimodal representation.,abstractText,[0],[0]
"However, these methods often suffer from exponential increase in dimensions and in computational complexity introduced by transformation of input into tensor.",abstractText,[0],[0]
"In this paper, we propose the Lowrank Multimodal Fusion method, which performs multimodal fusion using low-rank tensors to improve efficiency.",abstractText,[0],[0]
"We evaluate our model on three different tasks: multimodal sentiment analysis, speaker trait analysis, and emotion recognition.",abstractText,[0],[0]
Our model achieves competitive results on all these tasks while drastically reducing computational complexity.,abstractText,[0],[0]
"Additional experiments also show that our model can perform robustly for a wide range of low-rank settings, and is indeed much more efficient in both training and inference compared to other methods that utilize tensor representations.",abstractText,[0],[0]
Efficient Low-rank Multimodal Fusion with Modality-Specific Factors,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 308–317, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Topic models, such as Latent Dirichlet Allocation (Blei et al., 2003, LDA), have been successfully used for discovering hidden topics in text collections.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"LDA is an unsupervised model—it requires no annotation—and discovers, without any supervision, the thematic trends in a text collection.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"However, LDA’s lack of supervision can lead to disappointing results.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Often, the hidden topics learned by LDA fail to make sense to end users.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Part of the problem is that the objective function of topic models does not always correlate with human judgments of topic quality (Chang et al., 2009).",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Therefore, it’s often necessary to incorporate prior knowledge into topic models to
improve the model’s performance.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Recent work has also shown that by interactive human feedback can improve the quality and stability of topics (Hu and Boyd-Graber, 2012; Yang et al., 2015).",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Information about documents (Ramage et al., 2009) or words (Boyd-Graber et al., 2007) can improve LDA’s topics.
",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"In addition to its occasional inscrutability, scalability can also hamper LDA’s adoption.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
Conventional Gibbs sampling—the most widely used inference for LDA—scales linearly with the number of topics.,1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Moreover, accurate training usually takes many sampling passes over the dataset.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Therefore, for large datasets with millions or even billions of tokens, conventional Gibbs sampling takes too long to finish.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"For standard LDA, recently introduced fast sampling methods (Yao et al., 2009; Li et al., 2014; Yuan et al., 2015) enable industrial applications of topic modeling to search engines and online advertising, where capturing the “long tail” of infrequently used topics requires large topic spaces.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"For example, while typical LDA models in academic papers have up to 103 topics, industrial applications with 105–106 topics are common (Wang et al., 2014).",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Moreover, scaling topic models to many topics can also reveal the hierarchical structure of topics (Downey et al., 2015).
",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"Thus, there is a need for topic models that can both benefit from rich prior information and that can scale to large datasets.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"However, existing methods for improving scalability focus on topic models without prior information.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"To rectify this, we propose a factor graph model that encodes a potential function over the hidden topic variables, encouraging topics consistent with prior knowledge.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
The factor model representation admits an efficient sampling algorithm that takes advantage of the model’s sparsity.,1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"We show that our method achieves comparable performance but runs significantly faster than baseline methods, enabling mod-
308
els to discover models with many topics enriched by prior knowledge.",1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models,[0],[0]
"In this section, we introduce the factor model for incorporating prior knowledge and show how to efficiently use Gibbs sampling for inference.",2 Efficient Algorithm for Incorporating Knowledge into LDA,[0],[0]
"A statistical topic model represents words in documents in a collection D as mixtures of T topics, which are multinomials over a vocabulary of size V .",2.1 Background: LDA and SparseLDA,[0],[0]
"In LDA, each document d is associated with a multinomial distribution over topics, θd.",2.1 Background: LDA and SparseLDA,[0],[0]
The probability of a word type w given topic z is φw|z .,2.1 Background: LDA and SparseLDA,[0],[0]
The multinomial distributions θd and φz are drawn from Dirichlet distributions: α and β are the hyperparameters for θ and φ.,2.1 Background: LDA and SparseLDA,[0],[0]
"We represent the document collection D as a sequence of words w, and topic assignments as z. We use symmetric priors α and β in the model and experiment, but asymmetric priors are easily encoded in the models (Wallach et al., 2009).
",2.1 Background: LDA and SparseLDA,[0],[0]
Discovering the latent topic assignments z from observed words w requires inferring the the posterior distribution P (z|w).,2.1 Background: LDA and SparseLDA,[0],[0]
Griffiths and Steyvers (2004) propose using collapsed Gibbs sampling.,2.1 Background: LDA and SparseLDA,[0],[0]
"The probability of a topic assignment z = t in document d given an observed word type w and the other topic assignments z− is
P (z = t|z−, w) ∝",2.1 Background: LDA and SparseLDA,[0],[0]
"(nd,t + α)nw,t + β",2.1 Background: LDA and SparseLDA,[0],[0]
"nt + V β
(1)
where z− are the topic assignments of all other tokens.",2.1 Background: LDA and SparseLDA,[0],[0]
"This conditional probability is based on cumulative counts of topic assignments: nd,t is the number of times topic t is used in document d, nw,t is the number of times word type w is used in topic t, and nt is the marginal count of the number of tokens assigned to topic t.
Unfortunately, explicitly computing the conditional probability is quite for models with many topics.",2.1 Background: LDA and SparseLDA,[0],[0]
The time complexity of drawing a sample by Equation 1 is linear to the number of topics.,2.1 Background: LDA and SparseLDA,[0],[0]
"Yao et al. (2009) propose a clever factorization of Equation 1 so that the complexity is typically sublinear by breaking the conditional probability into
three “buckets”:∑ t P (z = t|z−, w) = ∑ t
αβ
nt + V β︸ ︷︷ ︸ s
(2)
+ ∑
t,nd,t>0
nd,tβ
nt + V β︸ ︷︷ ︸ r
+ ∑
t,nw,t>0 (nd,t + α)nw,t nt + V β︸ ︷︷ ︸ q .
",2.1 Background: LDA and SparseLDA,[0],[0]
The first term s is the “smoothing only” bucket—constant for all documents.,2.1 Background: LDA and SparseLDA,[0],[0]
The second term r is the “document only” bucket that is shared by a document’s tokens.,2.1 Background: LDA and SparseLDA,[0],[0]
Both s and r have simple constant time updates.,2.1 Background: LDA and SparseLDA,[0],[0]
"The last term q has to be computed specifically for each token, only for the few types with non-zero counts in a topic, due to the sparsity of word-topic count.",2.1 Background: LDA and SparseLDA,[0],[0]
"Since q often has the largest mass and few non-zero terms, we start the sampling from bucket q.",2.1 Background: LDA and SparseLDA,[0],[0]
"With SparseLDA, inferring LDA models over large topic spaces becomes tractable.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"However, existing methods for incorporating prior knowledge use conventional Gibbs sampling, which hinders inference.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"We address this limitation in this section by adding a factor graph to encode prior knowledge.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"LDA assumes that the hidden topic assignment of a word is independent from other hidden topics, given the document’s topic distribution θ.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"While this assumption facilitates computational efficiency, it loses the rich correlation between words.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"In many scenarios, users have external knowledge regarding word correlation, document labels, or document relations, which can reshape topic models and improve coherence.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
Prior knowledge can constrain what models discover.,2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"A correlation between two words v and w indicates that they have a similar topic distribution, i.e., p(z|v)",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"≈ p(z|w).1 Therefore, the posterior topic assignments v and w will be correlated.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"In contrast, if v and w are uncorrelated, nothing— other than the Dirichlet’s rich get richer effect— prevents the topics from diverging.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Similarly, if two documents share a label, then it is reasonable
1In (Andrzejewski et al., 2009)",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
two correlated words are taken to indicate that p(v|z),2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
≈ p(w|z),2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
.,2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"However, for word types that have very different frequencies, these two quantities would never be close, and thus p(z|v)",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"≈ p(z|w) is a more intuitive constraint.
to assume that they are more likely than two random documents to share topics.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
We denote the set of prior knowledge as M .,2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Each prior knowledge m ∈ M defines a potential function fm(z, w, d) of the hidden topic z of word type w in document d with which m is associated.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Therefore, the complete prior knowledge M defines a score on the current topic assignments z:
ψ(z,M) = ∏ z∈z exp fm(z, w, d) (3)
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"If m is knowledge about word type w, then fm(z, w, d) applies to all hidden topics of word w.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"If m is knowledge about document d, then fm(z, w, d) applies to all topics that are in document d.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
The potential function assigns large values to the topics that accord with prior knowledge but penalizes the topic assignments that disagree with the prior knowledge.,2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"In an extreme case, if a prior knowledge m says word type w in document d is Topic 3, then the potential function fm(z, w, d) is zero for all topics but",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Topic 3.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Since the potential function ψ is a function of z, and it is only a real-value score of current topic assignments, the potential can be factored out of the marginalized joint:
P (w, z|α, β,M) = P (w|z, β)P (z|α)ψ(z,M) (4) = ∫ θ ∫ φ p(w|z, φ)p(φ|β)p(z|θ)p(θ|α)ψ(z,M)dθdφ
= ψ(z,M) ∫ θ ∫ φ p(w|z, φ)p(φ|β)p(z|θ)p(θ|α)dθdφ.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Given the joint likelihood and observed data, the goal is evaluate the posterior P (z|w).",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Computing P (z|w) involves evaluating a probability distribution on a large discrete state space: P (z|w) = P (z,w)/∑z P (z,w).",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Griffiths and Steyvers (2004)—mirroring the original inspirations for Gibbs sampling (Geman and Geman, 1990)—draw an analogy to statistical physics, viewing standard LDA as a system that favors configurations z that compromise between having few topics per document and having few words per topic, with the terms of this compromise being set by the hyperparameters α and β.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Our factor model representation of prior knowledge adds a further constraint that asks the model to also consider ensembles of topic assignments z that are compatible with a standard LDA model and the given prior knowledge.
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"The collapsed Gibbs Sampling for inferring
topic assignment z of word w in document d is:
P (z = t|w, z−,M) (5) =",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"P (w, z−, z = t|α, β,M)
P (w, z−|α, β,M) = P (w, z−, z = t)
P (w, z−) ψ(z−, z = t,M) ψ(z−,M)
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"∝ {
(nd,t + α) nw,t + β",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"nt +Wβ
} ψ(z−, z = t,M)
ψ(z−,M) ∝",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"{
(nd,t + α) nw,t + β",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"nt +Wβ
} exp fm(z = t, w, d).
",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"The first term is identical to standard LDA, and admits efficient computation using SparseLDA.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"However, if the second term, exp fm(z, w, d), is dense, we still need to compute it explicitly T times (once for each topic) because we need the summation of P (z = t) for sampling.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"Therefore, the critical part of speeding up the sampler is finding a sparse representation of the second term.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"In the following sections, we show that natural, sparse prior knowledge representations are possible.",2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
We first present an efficient sparse representation of word correlation prior knowledge and then one for document-label knowledge.,2.2 A Factor Model for Incorporating Prior Knowledge,[0],[0]
"We now illustrate how we can encode word correlation knowledge as a set of sparse constraints fm(z, w, d) in our model.",2.3 Word Correlation Prior Knowledge,[0],[0]
"In previous work (Andrzejewski et al., 2009; Hu et al., 2011; Xie et al., 2015), word correlation prior knowledge is represented as word must-link constraints and cannotlink constraints.",2.3 Word Correlation Prior Knowledge,[0],[0]
"A must-link relation between two words indicates that the two words tend to be related to the same topics, i.e. their topic probabilities are correlated.",2.3 Word Correlation Prior Knowledge,[0],[0]
"In contrast, a cannot-link relation between two words indicates that these two words are not topically similar, and they should not both be prominent within the same topic.",2.3 Word Correlation Prior Knowledge,[0],[0]
"For example, “quarterback” and “fumble” are both related to American football, so they can share a must-link relation.",2.3 Word Correlation Prior Knowledge,[0],[0]
"But “fumble” and “bank” imply two different topics, so they share a cannotlink.
",2.3 Word Correlation Prior Knowledge,[0],[0]
Let us say word w is associated with a set of prior knowledge correlations Mw.,2.3 Word Correlation Prior Knowledge,[0],[0]
Each prior knowledge m ∈,2.3 Word Correlation Prior Knowledge,[0],[0]
"Mw is a word pair (w,w′), and it has “topic preference” of w given its correlation word w′.",2.3 Word Correlation Prior Knowledge,[0],[0]
"The must-link set of w is Mmw , and the cannot-link set of w is M cw, i.e., Mw = M cw ⋃ Mmw .",2.3 Word Correlation Prior Knowledge,[0],[0]
"In the example above, M m fumble =
{quarterback}, and M cfumble = {bank}, so Mfumble = {quarterback, bank}.",2.3 Word Correlation Prior Knowledge,[0],[0]
"The topic assignment of word “fumble” has higher conditional probability for the same topics as “quarterback” but lower probability for topics containing “bank”.
",2.3 Word Correlation Prior Knowledge,[0],[0]
"The potential score of sampling topic t for word type w—if Mw is not empty—is
fm(z, w, d) =",2.3 Word Correlation Prior Knowledge,[0],[0]
"∑ u∈Mmw log max(λ, nu,z)+
∑ v∈Mcw log 1 max(λ, nv,z) .
(6)
where λ is a hyperparameter, which we call the correlation strength.",2.3 Word Correlation Prior Knowledge,[0],[0]
"The intuitive explanation of Equation 6 is that the prior knowledge about the word type w will make an impact on the conditional probability of sampling the hidden topic z. Unlike standard LDA where every word’s hidden topic is independent of other words given θ, Equation 6 instead increases the probability that a word w will be drawn from the same topics as those of w’s must-link word set, and decreases its probability of being drawn from the same topics as those of w’s cannot-link word set.
",2.3 Word Correlation Prior Knowledge,[0],[0]
The hyperparameter λ controls the strength of each piece of prior knowledge.,2.3 Word Correlation Prior Knowledge,[0],[0]
"The smaller λ is, the stronger this correlation is.",2.3 Word Correlation Prior Knowledge,[0],[0]
"For large λ, the constraint is inactive for topics except those with the large counts.",2.3 Word Correlation Prior Knowledge,[0],[0]
"As λ decreases, the constraint becomes active for topics with lesser counts.",2.3 Word Correlation Prior Knowledge,[0],[0]
We can adjust the value of λ for each piece of prior knowledge based on our confidence.,2.3 Word Correlation Prior Knowledge,[0],[0]
"In our experiments, for simplicity, we use the same value λ for all knowledge and set λ = 1.
",2.3 Word Correlation Prior Knowledge,[0],[0]
"From Equation 6 and Equation 5, the conditional probability of a topic z in document d given an observed word type w is:
P (z = t|w, z−,M) ∝",2.3 Word Correlation Prior Knowledge,[0],[0]
"{ αβ
nt + V β +
nd,tβ
nt + V β + (nd,t + α)nw,t nt + V β } { ∏ u∈Mmw max(λ, nu,t) ∏ v∈Mcw 1 max(λ, nv,t) } (7)
As explained above, λ controls the “strength” of the prior knowledge term.",2.3 Word Correlation Prior Knowledge,[0],[0]
"If λ is large, the prior knowledge has little impact on the conditional probability of topic assignments.
",2.3 Word Correlation Prior Knowledge,[0],[0]
"Let’s return to the question whether Equation 6 is sparse, allowing efficient computation of Equation 7.",2.3 Word Correlation Prior Knowledge,[0],[0]
"Fortunately, nu,t and nv,t, which are the
topic counts for must-link word u and cannotlink word v, are often sparse.",2.3 Word Correlation Prior Knowledge,[0],[0]
"For example, in a 100-topic model trained on the NIPS dataset, 87.2% of word types have fewer than ten topics with nonzero counts.",2.3 Word Correlation Prior Knowledge,[0],[0]
"In a 500-topic model trained on a larger dataset like the New York Times News (Sandhaus, 2008), 81.9% of word types have fewer than 50 topics with nonzero counts.",2.3 Word Correlation Prior Knowledge,[0],[0]
"Moreover, the model becomes increasingly sparse with additional Gibbs iterations.",2.3 Word Correlation Prior Knowledge,[0],[0]
"Figure 1 shows the word frequency histogram of nonzero topic counts of NYT-News dataset.
",2.3 Word Correlation Prior Knowledge,[0],[0]
"Therefore, the computational cost of Equation 7 can be reduced.",2.3 Word Correlation Prior Knowledge,[0],[0]
"SparseLDA efficiently computes the s, r, q bins as in Equation 3.",2.3 Word Correlation Prior Knowledge,[0],[0]
"Then for words that are associated with prior knowledge, we update s, r, q with an additional potential term.",2.3 Word Correlation Prior Knowledge,[0],[0]
We only need to compute the potential term for the topics whose counts are greater than λ.,2.3 Word Correlation Prior Knowledge,[0],[0]
"The collapsed Gibbs sampling procedure is summarized in Algorithm 1.
",2.3 Word Correlation Prior Knowledge,[0],[0]
"Algorithm 1 Gibbs Sampling for word type w in document d, given w’s correlation set Mw
1: compute st, rt, qt with SparseLDA, (see Eq. 3) 2: for t← 0 to T do 3: update st, rt, qt. ∀u ∈Mw if nu,t > λ 4: end for 5: p(t)",2.3 Word Correlation Prior Knowledge,[0],[0]
= st + rt + qt 6: sample new topic assignment for w from p(t),2.3 Word Correlation Prior Knowledge,[0],[0]
"The factor model framework can also handle other types of prior knowledge, such as document labels, sentence labels, and document link relations.",2.4 Other Types of Prior Knowledge,[0],[0]
"We briefly describe document labels here.
",2.4 Other Types of Prior Knowledge,[0],[0]
"Ramage et al. (2009) propose Labeled-LDA, which improves LDA with document labels.",2.4 Other Types of Prior Knowledge,[0],[0]
"It assumes that there is a one-to-one mapping between topics and labels, and it restricts each document’s topics to be sampled only from those allowed by the documents label set.",2.4 Other Types of Prior Knowledge,[0],[0]
"Therefore, Labeled-LDA can be expressed in our model.",2.4 Other Types of Prior Knowledge,[0],[0]
"We define
fm(z, w, d) =",2.4 Other Types of Prior Knowledge,[0],[0]
"{ 1, if z ∈ md −∞, else (8)
where md specifies document d’s label set converted to corresponding topic labels.",2.4 Other Types of Prior Knowledge,[0],[0]
"Since fm(z, w, d) is sparse, we can speed up the training as well.",2.4 Other Types of Prior Knowledge,[0],[0]
"Sentence-level prior knowledge (e.g., for sentiment or aspect models (Paul and Girju, 2010)) can be defined in a similar way.
",2.4 Other Types of Prior Knowledge,[0],[0]
Documents can be associated with other useful metadata.,2.4 Other Types of Prior Knowledge,[0],[0]
"For example, a scientific paper and the prior work it cites might have similar topics (Dietz et al., 2007) or friends in a social network might talk about the same topics (Chang and Blei, 2009).",2.4 Other Types of Prior Knowledge,[0],[0]
"To model link relations, we can use Equation 6 and replace the word-topic counts nv,z with document-topic counts nd,z .",2.4 Other Types of Prior Knowledge,[0],[0]
"By doing so, we encourage related documents to have similar topic structures.",2.4 Other Types of Prior Knowledge,[0],[0]
"Moreover, the document-topic count is also sparse, which fits into the efficient learning framework.
",2.4 Other Types of Prior Knowledge,[0],[0]
"Therefore, for different types of prior knowledge, as long as we can define ψ(z,M) appropriately so that f(z, w, d) are sparse, we are able to speed up learning.",2.4 Other Types of Prior Knowledge,[0],[0]
"In this section, we demonstrate the effectiveness of our SC-LDA by comparing it with several baseline methods on three benchmark datasets.",3 Experiments,[0],[0]
We first evaluate the convergence rate of each method and then evaluate the learned model parameter φ—the topic-word distribution—in terms of topic coherence.,3 Experiments,[0],[0]
We show that SC-LDA can achieve results comparable to the baseline models but is significantly faster.,3 Experiments,[0],[0]
"We set up all experiments on a 8- Core 2.8GHz CPU, 16GB RAM machine.2
2Our implementation of SC-LDA is available at https://github.com/yya518/",3 Experiments,[0],[0]
"We use the NIPS and NYT-News datasets from the UCI bag of words data collections.3 These two datasets have no document labels, and we use them for word correlation experiments.",3.1 Dataset,[0],[0]
"We also use the 20Newsgroup (20NG) dataset,4 which has document labels, for document label experiments.",3.1 Dataset,[0],[0]
Table 1 shows the characteristics of each dataset.,3.1 Dataset,[0],[0]
"Since NIPS and NYT-News have already been preprocessed, to ensure repeatability, we use the data “as they are” from the sources.",3.1 Dataset,[0],[0]
"For 20NG, we perform tokenization and stopword removal using Mallet (McCallum, 2002) and remove words that appear fewer than 10 times.",3.1 Dataset,[0],[0]
Word Correlation Prior Knowledge Previous work proposes two methods to automatically generate prior word correlation knowledge from external sources.,3.2 Prior Knowledge Generation,[0],[0]
"Hu and Boyd-Graber (2012) use WordNet 3.0 to obtain synsets for word types, and then if a synset is also in the vocabulary, they add a must-link correlation between the word type and the synset.",3.2 Prior Knowledge Generation,[0],[0]
Xie et al. (2015) use a different method that takes advantage of an existing pretrained word embedding.,3.2 Prior Knowledge Generation,[0],[0]
Each word embedding is a real-valued vector capturing the word’s semantic meaning based on distributional similarity.,3.2 Prior Knowledge Generation,[0],[0]
"If the similarity between the embeddings of two word types in the vocabulary exceeds a threshold, they generate a must-link between the two words.
",3.2 Prior Knowledge Generation,[0],[0]
"In our experiments, we adopt a hybrid method that combines the above two methods.",3.2 Prior Knowledge Generation,[0],[0]
"For a noun word type, we first obtain its synsets from WordNet 3.0.",3.2 Prior Knowledge Generation,[0],[0]
"We also obtain the embeddings of each word from word2vec (Mikolov et al., 2013).",3.2 Prior Knowledge Generation,[0],[0]
"If the synset is also in the vocabulary, and the similarity between the synset and the word is higher than a threshold, which in our experiment is 0.2, we generate a must-link between thee words.",3.2 Prior Knowledge Generation,[0],[0]
"Empir-
",3.2 Prior Knowledge Generation,[0],[0]
sparse-constrained-lda.,3.2 Prior Knowledge Generation,[0],[0]
"3https://archive.ics.uci.edu/ml/datasets/Bag+of+Words 4http://qwone.com/ jason/20Newsgroups/
ically, this hybrid method is able to obtain high quality correlated words.",3.2 Prior Knowledge Generation,[0],[0]
"For example, for the NIPS dataset, the must-links we obtain for randomness are {noise, entropy, stochasticity}.",3.2 Prior Knowledge Generation,[0],[0]
Document Label,3.2 Prior Knowledge Generation,[0],[0]
"Prior Knowledge Since documents in the 20NG dataset are associated with labels, we use the labels directly as prior knowledge.",3.2 Prior Knowledge Generation,[0],[0]
"The baseline methods for incorporating word correlation prior knowledge in our experiments are as follows: DF-LDA: incorporates word must-links and cannot-links using a Dirichlet Forest prior in LDA (Andrzejewski et al., 2009).",3.3 Baselines,[0],[0]
Here we use Hu and Boyd-Graber (2012)’s efficient implementation FAST-RB-SDW for DF-LDA.,3.3 Baselines,[0],[0]
"Logic-LDA: encodes general domain knowledge as first-order logic and incorporates it in LDA (Andrzejewski et al., 2011).",3.3 Baselines,[0],[0]
Logic-LDA has been used for word correlations and document label knowledge.,3.3 Baselines,[0],[0]
"MRF-LDA: encodes word correlations in LDA as a Markov random field (Xie et al., 2015).
",3.3 Baselines,[0],[0]
We also use Mallet’s SparseLDA implementation for vanilla LDA in the topic coherence experiment.,3.3 Baselines,[0],[0]
We use a symmetric Dirichlet prior for all models.,3.3 Baselines,[0],[0]
"We set α = 1.0, β = 0.01.",3.3 Baselines,[0],[0]
"For DF-LDA, η = 100.",3.3 Baselines,[0],[0]
"For Logic-LDA, we use the default parameter setting in the package: a sample rate of 1.0 and step rate of 10.0.",3.3 Baselines,[0],[0]
"For MRF-LDA, we use the default setting with γ = 1.0.",3.3 Baselines,[0],[0]
(Parameter semantics can be found in the original papers.),3.3 Baselines,[0],[0]
The main advantage of our method over other existing methods is efficiency.,3.4 Convergence,[0],[0]
"In this experiment, we show the change of our model’s log likelihood over time.",3.4 Convergence,[0],[0]
"In topic models, the log likelihood change is a good indicator of whether a model has converged or not.",3.4 Convergence,[0],[0]
Figure 2 shows the log likelihood change over time for SC-LDA and three baseline methods on NIPS and NYT-News dataset.,3.4 Convergence,[0],[0]
"SC-LDA converges faster than all the other methods.
",3.4 Convergence,[0],[0]
We also conduct experiments on SC-LDA with varying numbers of word correlations.,3.4 Convergence,[0],[0]
"Table 2 shows the Gibbs sampling iteration time on the 1st, 50th, 100th and the 200th iteration.",3.4 Convergence,[0],[0]
"We also incorporate different numbers of word correlations
in SC-LDA.",3.4 Convergence,[0],[0]
"SC-LDA runs faster as sampling proceeds as the sparsity increases, but additional correlations slow the model.",3.4 Convergence,[0],[0]
"Topic models are often evaluated using perplexity on held-out test data, but this evaluation is of-
ten at odds with human evaluations (Chang et al., 2009).",3.5 Topic Coherence,[0],[0]
"Following Mimno et al. (2011), we employ Topic Coherence—a metric that is consistent with human judgment—to measure a topic model’s quality.",3.5 Topic Coherence,[0],[0]
"Topic t’s coherence is defined
as C(t : V (t))",3.5 Topic Coherence,[0],[0]
"= ∑M
m=2 ∑m−1",3.5 Topic Coherence,[0],[0]
"l=1 log F (v (t) m ,v (t) l )",3.5 Topic Coherence,[0],[0]
"+
F (v (t)",3.5 Topic Coherence,[0],[0]
"l )
,
where F (v) is the document frequency of word type v, F (v, v′) is the co-document frequency of word type v and v′, and V (t) =",3.5 Topic Coherence,[0],[0]
"(v(t)1 , ..., v (t) M ) is a list of the M most probable words in topic t.",3.5 Topic Coherence,[0],[0]
"In our experiments, we choose the ten words with highest probability in the topic to compute topic coherence, i.e., M = 10.",3.5 Topic Coherence,[0],[0]
"Mimno et al. (2011) use = 1, but Röder et al. (2015) show smaller (such as 10−12) improves coherence stability, so we set = 10−12.",3.5 Topic Coherence,[0],[0]
"Larger topic coherence scores imply more coherent topics.
",3.5 Topic Coherence,[0],[0]
We train a 500-topic model on the NIPS dataset with different methods and compare the average topic coherence score and the average of the top twenty topic coherence scores.,3.5 Topic Coherence,[0],[0]
"Since the topics learned by topic model often contain “bad” topics (Mimno et al., 2011) which do not make sense to end users, evaluating the top twenty topics reflects the model’s performance.",3.5 Topic Coherence,[0],[0]
We let each model train for one hour.,3.5 Topic Coherence,[0],[0]
Figure 3 shows the topic coherence of each method.,3.5 Topic Coherence,[0],[0]
SC-LDA has about the same average topic coherence with LDA but has higher coherence score (-36.6) for the top 20 topics than LDA (-39.1).,3.5 Topic Coherence,[0],[0]
"This is because incorporating word correlation knowledge encourages correlated words to have high probability under the same topic, thus improving the coherence score.",3.5 Topic Coherence,[0],[0]
"For the other methods, however, because they cannot converge within an hour, their topic coherence scores are much worse than SC-LDA and LDA.",3.5 Topic Coherence,[0],[0]
This again demonstrates the efficiency of SC-LDA over other baselines.,3.5 Topic Coherence,[0],[0]
SC-LDA can also handle other types of prior knowledge.,3.6 Document Label Prior Knowledge,[0],[0]
"We compare it with Labeled-LDA (Ramage et al., 2009).",3.6 Document Label Prior Knowledge,[0],[0]
"Labeled-LDA also uses Gibbs sampling for inference, allowing direct computation time comparisons.
",3.6 Document Label Prior Knowledge,[0],[0]
Table 3 shows the average running time per iteration for Labeled-LDA and SC-LDA.,3.6 Document Label Prior Knowledge,[0],[0]
"Because document labels apply sparsity to the documenttopic counts, the average running time per iteration decreases as the number of labeled document increases.",3.6 Document Label Prior Knowledge,[0],[0]
"SC-LDA exhibits greater speedup with
more topics; when T = 500,5 SC-LDA runs more than ten times faster than Labeled-LDA.",3.6 Document Label Prior Knowledge,[0],[0]
This works brings together two lines of research: incorporating rich knowledge into probabilistic models and efficient inference of probabilistic models on large datasets.,4 Related Work,[0],[0]
"Both are common areas of interest across many machine learning formalisms: probabilistic logic (Bach et al., 2015), graph algorithms (Low et al., 2012), and probabilistic grammars (Cohen et al., 2008).",4 Related Work,[0],[0]
"However, our focus in this paper is the intersection of these lines of research with topic models.
",4 Related Work,[0],[0]
"5For 20NG dataset, it may overfit the data with 500 topics, but here we use it to demonstrate the scalability.
",4 Related Work,[0],[0]
"Adding knowledge and metadata to topic models makes the models richer, more understandable, and more domain-specific.",4 Related Work,[0],[0]
"A common distinction is upstream (conditioning on metadata) vs. downstream models (conditioning on variables already present in a topic model to predict metadata) (Mimno et al., 2008).",4 Related Work,[0],[0]
"Downstream models are typically better at prediction tasks such as predicting sentiment (Blei and McAuliffe, 2007), ideology (Nguyen et al., 2014a), or links in a social network (Chang and Blei, 2009).",4 Related Work,[0],[0]
"In contrast, our approach—an upstream model—is often easier to implement and leads to more interpretable topics.",4 Related Work,[0],[0]
"Upstream models at the document level have been used to understand the labels in large document collections (Ramage et al., 2009; Nguyen et al., 2014b) and capture relationships in document networks using Markov random fields (Daumé III, 2009).",4 Related Work,[0],[0]
"At the word level, Xie et al. (2015) incorporate word correlation to LDA by building a Markov Random Field regularization, similar to Newman et al. (2011), who use regularization to improve topic coherence.",4 Related Work,[0],[0]
"However, despite these exciting applications, the experiments in the above work are typically on small datasets.
",4 Related Work,[0],[0]
"In contrast, there is a huge interest in improving the scalability of topic models to large numbers of documents, numbers of topics, and vocabularies.",4 Related Work,[0],[0]
Attempts to scale inference for topic models have started from both variational inference and Gibbs sampling—two popular learning inference techniques for topic modeling.,4 Related Work,[0],[0]
Gibbs sampling is a popular technique because of its simplicitly and low latency.,4 Related Work,[0],[0]
"However, for large numbers of topics, Gibbs sampling can become unwieldy.",4 Related Work,[0],[0]
"Porteous et al. (2008) address this issue by creating an upper bound approximation that produces accurate results, while SparseLDA (Yao et al., 2009) present an effective factorization that speeds inference without sacrificing accuracy.",4 Related Work,[0],[0]
"Just as our model builds on SparseLDA’s insights, SparseLDA has been incorporated into commercial deployments (Wang et al., 2014) and improved using alias tables (Li et al., 2014).",4 Related Work,[0],[0]
Yuan et al. (2015) also presents an efficient constant time sampling algorithm for building big topic models.,4 Related Work,[0],[0]
"Variational inference can easily be parallelized (Nallapati et al., 2007; Zhai et al., 2012), but has high latency, which has been addressed by performing online updates (Hoffman et al., 2010) and taking stochastic gradients estimated by
MCMC inference (Mimno et al., 2012).",4 Related Work,[0],[0]
"In this paper, we only focus on single-processor learning, but existing parallelization techniques (Newman et al., 2009) are applicable to our model.
",4 Related Work,[0],[0]
At the intersection lies models that improve the scalability of upstream topic model inference.,4 Related Work,[0],[0]
"In addition to our SC-LDA, Hu and Boyd-Graber (2012) speed Gibbs sampling in tree-based topic models using SparseLDA’s factorization strategy, and Hu et al. (2014) extend this approach by parallelizing global parameter updates using variational inference.",4 Related Work,[0],[0]
Our work is more general (also encompassing document-based constraints) and is faster.,4 Related Work,[0],[0]
"In contrast to these upstream models, Zhu et al. (2013) and Nguyen et al. (2015) improve inference of downstream models.",4 Related Work,[0],[0]
We present a factor graph framework for incorporating prior knowledge into topic models.,5 Conclusion,[0],[0]
"By expressing the prior knowledge as sparse constraints on the hidden topic variables, we are able to take advantage of the sparsity to speed up training.",5 Conclusion,[0],[0]
We demonstrate in experiments that our model runs significantly faster than the other alternative models and achieves comparable performance in terms of topic coherence.,5 Conclusion,[0],[0]
Efficient algorithms for incorporating prior knowledge with large topic models will benefit several downstream applications.,5 Conclusion,[0],[0]
"For example, interactive topic modeling becomes feasible because fast model updates reduce the user’s waiting time and thus improve the user experience.",5 Conclusion,[0],[0]
Personalized topic modeling is also an interesting future direction in which the model will generate a personalized topic structure based on the user’s preferences or interests.,5 Conclusion,[0],[0]
"For all these applications, an efficient learning algorithm is a crucial prerequisite.",5 Conclusion,[0],[0]
We thank the anonymous reviews for their helpful comments.,Acknowledgments,[0],[0]
This research was supported in part by NSF grant IIS-1351029 and DARPA contract D11AP00268.,Acknowledgments,[0],[0]
"Boyd-Graber is supported by NSF Grants CCF-1409287, IIS-1320538, and NCSE1422492.",Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
Latent Dirichlet allocation (LDA) is a popular topic modeling technique for exploring hidden topics in text corpora.,abstractText,[0],[0]
"Increasingly, topic modeling needs to scale to larger topic spaces and use richer forms of prior knowledge, such as word correlations or document labels.",abstractText,[0],[0]
"However, inference is cumbersome for LDA models with prior knowledge.",abstractText,[0],[0]
"As a result, LDA models that use prior knowledge only work in small-scale scenarios.",abstractText,[0],[0]
"In this work, we propose a factor graph framework, Sparse Constrained LDA (SC-LDA), for efficiently incorporating prior knowledge into LDA.",abstractText,[0],[0]
We evaluate SC-LDA’s ability to incorporate word correlation knowledge and document label knowledge on three benchmark datasets.,abstractText,[0],[0]
"Compared to several baseline methods, SC-LDA achieves comparable performance but is significantly faster.",abstractText,[0],[0]
"1 Challenge: Leveraging Prior Knowledge in Large-scale Topic Models Topic models, such as Latent Dirichlet Allocation (Blei et al., 2003, LDA), have been successfully used for discovering hidden topics in text collections.",abstractText,[0],[0]
"LDA is an unsupervised model—it requires no annotation—and discovers, without any supervision, the thematic trends in a text collection.",abstractText,[0],[0]
"However, LDA’s lack of supervision can lead to disappointing results.",abstractText,[0],[0]
"Often, the hidden topics learned by LDA fail to make sense to end users.",abstractText,[0],[0]
"Part of the problem is that the objective function of topic models does not always correlate with human judgments of topic quality (Chang et al., 2009).",abstractText,[0],[0]
"Therefore, it’s often necessary to incorporate prior knowledge into topic models to improve the model’s performance.",abstractText,[0],[0]
"Recent work has also shown that by interactive human feedback can improve the quality and stability of topics (Hu and Boyd-Graber, 2012; Yang et al., 2015).",abstractText,[0],[0]
"Information about documents (Ramage et al., 2009) or words (Boyd-Graber et al., 2007) can improve LDA’s topics.",abstractText,[0],[0]
"In addition to its occasional inscrutability, scalability can also hamper LDA’s adoption.",abstractText,[0],[0]
Conventional Gibbs sampling—the most widely used inference for LDA—scales linearly with the number of topics.,abstractText,[0],[0]
"Moreover, accurate training usually takes many sampling passes over the dataset.",abstractText,[0],[0]
"Therefore, for large datasets with millions or even billions of tokens, conventional Gibbs sampling takes too long to finish.",abstractText,[0],[0]
"For standard LDA, recently introduced fast sampling methods (Yao et al., 2009; Li et al., 2014; Yuan et al., 2015) enable industrial applications of topic modeling to search engines and online advertising, where capturing the “long tail” of infrequently used topics requires large topic spaces.",abstractText,[0],[0]
"For example, while typical LDA models in academic papers have up to 103 topics, industrial applications with 105–106 topics are common (Wang et al., 2014).",abstractText,[0],[0]
"Moreover, scaling topic models to many topics can also reveal the hierarchical structure of topics (Downey et al., 2015).",abstractText,[0],[0]
"Thus, there is a need for topic models that can both benefit from rich prior information and that can scale to large datasets.",abstractText,[0],[0]
"However, existing methods for improving scalability focus on topic models without prior information.",abstractText,[0],[0]
"To rectify this, we propose a factor graph model that encodes a potential function over the hidden topic variables, encouraging topics consistent with prior knowledge.",abstractText,[0],[0]
The factor model representation admits an efficient sampling algorithm that takes advantage of the model’s sparsity.,abstractText,[0],[0]
"We show that our method achieves comparable performance but runs significantly faster than baseline methods, enabling mod-",abstractText,[0],[0]
Efficient Methods for Incorporating Knowledge into Topic Models,title,[0],[0]
"In many real-world applications, the process of analyzing data incurs some cost; for example, obtaining a label may require a laborious experiment, a human action, or depletion of some other expensive resource.",1. Introduction,[0],[0]
"In these scenarios, carefully selecting data to label can often help us achieve our goals more efficiently than, e.g., random sampling.",1. Introduction,[0],[0]
"This is the motivation behind active learning.
",1. Introduction,[0],[0]
"Naturally, which data examples are the most useful to label might change according to our objective.",1. Introduction,[0],[0]
"Traditionally, much of the active learning literature has focused on train-
1Washington University in St. Louis, St. Louis, MO, USA 2Simpson College, Indianola, IA, USA 3University of South Carolina, Columbia, SC, USA.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Shali Jiang <jiang.s@wustl.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
ing a model to have high generalization performance with few training examples.,1. Introduction,[0],[0]
"Here, we consider a special and atypical realization of active learning: the active search problem (Garnett et al., 2012).",1. Introduction,[0],[0]
"In active search, we seek to sequentially inspect data so as to discover members of a rare, desired class.",1. Introduction,[0],[0]
The labels are not known a priori but can be revealed by querying a costly labeling oracle.,1. Introduction,[0],[0]
The goal is to design an policy to sequentially query points to find as many valuable points as possible under a labeling budget.,1. Introduction,[0],[0]
"Several real-world problems can be naturally posed in terms of active search; drug discovery, fraud detection, and product recommendation are a few examples.",1. Introduction,[0],[0]
"A successful active search policy faces the fundamental dilemma between exploration and exploitation; i.e., whether to search for new regions of valuable points (exploration) or take advantage of the currently most-promising regions (exploitation).
",1. Introduction,[0],[0]
"Previous work developed policies for active search by appealing to Bayesian decision theory (Garnett et al., 2011; 2012).",1. Introduction,[0],[0]
Garnett et al. (2012) derived the optimal policy in this framework with a natural utility function.,1. Introduction,[0],[0]
"Not surprisingly, realizing this policy in the general case requires exponential computation.",1. Introduction,[0],[0]
"To overcome this intractability, the authors of that work proposed using myopic lookahead policies in practice, which compute the optimal policy only up to a limited number of steps into the future.",1. Introduction,[0],[0]
"This defines a family of policies ranging in complexity from completely greedy one-step lookahead to the optimal policy, which looks ahead to the depletion of the entire budget.",1. Introduction,[0],[0]
"The authors demonstrated improved performance on active search over the greedy policy even when looking just two steps into the future, including in a drug-discovery setting (Garnett et al., 2015).",1. Introduction,[0],[0]
"The main limitation of these strategies is that they completely ignore what can happen beyond the chosen horizon, which for typical problems is necessarily limited to ` ≤ 3, even with aggressive pruning.
",1. Introduction,[0],[0]
The contributions of this paper are two-fold.,1. Introduction,[0],[0]
"First, we prove that no polynomial time policy for active search can have nontrivial approximation ratio with respect to the optimal policy in terms of expected utility.",1. Introduction,[0],[0]
This extends the result by Garnett et al. (2012) that myopic approximations to the optimal policy cannot approximate the optimal policy.,1. Introduction,[0],[0]
"The proof of this theorem is constructive, creating a family of explicitly difficult active search instances and showing that no polynomial time algorithm can perform well compared
to the optimal (exponential cost) policy on these.
",1. Introduction,[0],[0]
"Second, we introduce a novel nonmyopic policy for active search that considers not only the potential immediate contribution of each unlabeled point but also its potential impact on the remaining points that could be chosen afterwards.",1. Introduction,[0],[0]
Our policy automatically balances exploitation against exploration consistent with the labeling budget without requiring any parameters controlling this tradeoff.,1. Introduction,[0],[0]
We also develop an effective strategy for pruning unlabeled points by bounding their potential impact on the search problem.,1. Introduction,[0],[0]
"We compare our method with several baselines by conducting experiments on numerous real datasets spanning several domains including citation networks, materials science, and drug discovery.",1. Introduction,[0],[0]
Our results thoroughly demonstrate that our policy typically significantly outperforms previously proposed active search approaches.,1. Introduction,[0],[0]
"Suppose we are given a finite domain of elementsX , {xi}.",2. Active Search and the Optimal Policy,[0],[0]
"We know that there is a rare subset R ⊂ X , the members of which are considered valuable, but their identities are unknown a priori.",2. Active Search and the Optimal Policy,[0],[0]
We will call the elements of R targets or positive items.,2. Active Search and the Optimal Policy,[0],[0]
"Assume that there is an oracle that can determine whether a specified element x ∈ X is a target, producing the binary output y , 1{x ∈ R}.",2. Active Search and the Optimal Policy,[0],[0]
"The oracle, however, is assumed to be expensive and may only be queried t times.",2. Active Search and the Optimal Policy,[0],[0]
"We seek to design a policy to sequentially query elements to maximize the number of targets found.
",2. Active Search and the Optimal Policy,[0],[0]
"We will express our preference over different sets of observations D , { (xi, yi) } through a simple utility:
u(D) , ∑
yi∈D yi, (1)
which simply counts the number of targets in D. Then, the problem is to sequentially construct a set of t observed points D with the goal of maximizing u(D).",2. Active Search and the Optimal Policy,[0],[0]
"Throughout this work, we use a subscript to specify a set of observed data after i ≤ t queries, defining Di , { (xj , yj) }i j=1 .",2. Active Search and the Optimal Policy,[0],[0]
"Following previous work, we consider the active search problem in the standard Bayesian framework.",2.1. The Bayesian Optimal Policy,[0],[0]
"Assume we have a probabilistic classification model that provides the posterior probability of a point x belonging to R, given observed data D: Pr(y = 1 | x,D).
",2.1. The Bayesian Optimal Policy,[0],[0]
"Recall that we are allowed to perform t labeling queries, and suppose we are at some iteration i for i ≤ t; having already observed i− 1 examples, Di−1.",2.1. The Bayesian Optimal Policy,[0],[0]
We wish to submit the ith item to the oracle.,2.1. The Bayesian Optimal Policy,[0],[0]
"Bayesian decision theory compels us to select the item that if we evaluate next maximizes the
expected utility of the final observed dataset:
x∗i = argmax xi∈X\Di−1
E [ u(Dt) | xi,Di−1 ] .",2.1. The Bayesian Optimal Policy,[0],[0]
"(2)
In other words, we choose a point x∗i maximizing the expected number of targets found at termination.",2.1. The Bayesian Optimal Policy,[0],[0]
"Unfortunately, as we shall see later, computing E",2.1. The Bayesian Optimal Policy,[0],[0]
"[ u(Dt) | xi,Di−1 ] is computationally impractical.
",2.1. The Bayesian Optimal Policy,[0],[0]
"To better understand the optimal policy, consider the case i = t, so we already have t− 1 observations Dt−1 and there is only one more query left.",2.1. The Bayesian Optimal Policy,[0],[0]
"The expected utility is
E [ u(Dt) | xt,Dt−1 ] = ∑
yt u(Dt)",2.1. The Bayesian Optimal Policy,[0],[0]
"Pr(yt | xt,Dt−1)
= u(Dt−1) +",2.1. The Bayesian Optimal Policy,[0],[0]
"Pr(yt = 1 | xt,Dt−1).",2.1. The Bayesian Optimal Policy,[0],[0]
"(3)
Note u(Dt−1) is a constant, since Dt−1 was already observed.",2.1. The Bayesian Optimal Policy,[0],[0]
"Thus, when there is one query remaining, the optimal decision is to greedily choose the remaining point with maximum probability of being a target.
",2.1. The Bayesian Optimal Policy,[0],[0]
"When two or more queries are left, the optimal policy is not as trivial.",2.1. The Bayesian Optimal Policy,[0],[0]
"The challenge is that after the first choice, the probability model changes, affecting all future decisions.",2.1. The Bayesian Optimal Policy,[0],[0]
"Below, we show the expected utility for i = t− 1.
",2.1. The Bayesian Optimal Policy,[0],[0]
E [ u(Dt),2.1. The Bayesian Optimal Policy,[0],[0]
"| xt−1,Dt−2 ] = u(Dt−2)",2.1. The Bayesian Optimal Policy,[0],[0]
"+
Pr(yt−1 = 1 | xt−1,Dt−2)",2.1. The Bayesian Optimal Policy,[0],[0]
+,2.1. The Bayesian Optimal Policy,[0],[0]
Eyt−1,2.1. The Bayesian Optimal Policy,[0],[0]
[ max xt,2.1. The Bayesian Optimal Policy,[0],[0]
"Pr(yt = 1 | xt,Dt−1) ] .",2.1. The Bayesian Optimal Policy,[0],[0]
"(4)
This expression has an intuitive interpretation.",2.1. The Bayesian Optimal Policy,[0],[0]
"First, we have the reward for the data already observed, u(Dt−2).",2.1. The Bayesian Optimal Policy,[0],[0]
"The second term is the expected reward contribution from the point xt−1 under consideration, Pr(yt−1 = 1 | xt−1,Dt−2).",2.1. The Bayesian Optimal Policy,[0],[0]
"Finally, the last term is the expected future reward, which is the expected reward to be gathered on the next step; from our previous analysis, we know that this will be maximized by a greedy selection (3).",2.1. The Bayesian Optimal Policy,[0],[0]
"These latter two terms can be interpreted as encouraging exploitation and exploration, respectively, with the optimal second-to-last query.
",2.1. The Bayesian Optimal Policy,[0],[0]
"In general, we can compute expected utility (2) at time i ≤ t recursively as (Garnett et al., 2012):
E [ u(Dt) | xi,Di−1 ] = u(Di−1) +
Pr(yi = 1 | xi,Di−1)︸ ︷︷ ︸ exploitation, < 1 +
Eyi [ maxx′ E [ u(Dt \ Di) |",2.1. The Bayesian Optimal Policy,[0],[0]
"x′,Di",2.1. The Bayesian Optimal Policy,[0],[0]
"]] ︸ ︷︷ ︸
exploration, <t−i
.",2.1. The Bayesian Optimal Policy,[0],[0]
"(5)
It is easy to show that the time complexity for computing Eq.",2.1. The Bayesian Optimal Policy,[0],[0]
"(5) is O ( (2n)` ) , where ` = t− i+ 1 is the lookahead and n is the total number of unlabeled points.
",2.1. The Bayesian Optimal Policy,[0],[0]
"This exponential running time complexity makes the Bayeisan optimal policy infeasible to compute, even for small-scale applications.",2.1. The Bayesian Optimal Policy,[0],[0]
"A typical workaround is to pretend there are only a few steps left in the search problem at each iteration, and sequentially apply a myopic policy (e.g., (3) or (4)).",2.1. The Bayesian Optimal Policy,[0],[0]
"We will refer to these policies as the one-step and two-step myopic policies, respectively, and more generally to the `-step myopic policy, with ` < t− i+ 1.
",2.1. The Bayesian Optimal Policy,[0],[0]
"Since these myopic approaches cannot plan more than ` steps ahead, they can underestimate the potential benefit of exploration.",2.1. The Bayesian Optimal Policy,[0],[0]
"In particular, the potential magnitude of the exploration term in (5) depends linearly on the budget, whereas in an `-step myopic policy, the magnitude of the equivalent term can go no higher than a fixed upper bound of `.",2.1. The Bayesian Optimal Policy,[0],[0]
"In fact, Garnett et al. (2012) showed via an explicit construction that the expected performance of the `-step policy can be arbitrarily worse than any m-step policy with ` < m, exploiting this inability to “see past” the horizon.",2.1. The Bayesian Optimal Policy,[0],[0]
"When following this suggestion, we must thus trade off the potential benefits of nonmyopia and the rapidly increasing computational burden of lookahead when choosing a policy.",2.1. The Bayesian Optimal Policy,[0],[0]
"We extend the above hardness result to show that no polynomial-time active search policy can be a (constant factor) approximation algorithm with respect to the optimal policy, in terms of expected utility.",2.2. Hardness of Approximation,[0],[0]
"In particular, under the assumption that algorithms only have access to a unit cost conditional marginal probability Pr(y = 1",2.2. Hardness of Approximation,[0],[0]
"| x,D) for any x and D, where |D| is less than the budget,1 then:
Theorem 1.",2.2. Hardness of Approximation,[0],[0]
"There is no polynomial-time active search policy with a constant factor approximation ratio for optimizing the expected utility.
",2.2. Hardness of Approximation,[0],[0]
We prove this theorem in the appendix.,2.2. Hardness of Approximation,[0],[0]
The main idea is to construct a class of instances where a small “secret” set of elements encodes the locations of a large “treasure” of targets.,2.2. Hardness of Approximation,[0],[0]
"The probability of revealing the treasure is vanishingly small without discovering the secret set; however, it is extremely unlikely to observe any information about this secret set with polynomial-time effort.
",2.2. Hardness of Approximation,[0],[0]
"Despite the negative result of Theorem 1, we may still search for policies that are empirically effective on real problems.",2.2. Hardness of Approximation,[0],[0]
"In the next section, we propose a novel alternative approximation to the optimal policy (2) that is nonmyopic, computationally efficient, and shows impressive empirical performance.
",2.2. Hardness of Approximation,[0],[0]
1The optimal policy operates under these restrictions.,2.2. Hardness of Approximation,[0],[0]
We have seen above how to myopically approximate the Bayesian optimal policy using an `-step-lookahead approximate policy (5).,3. Efficient Nonmyopic Active Search,[0],[0]
"Such an approximation, however, effectively assumes that the search procedure will terminate after the next ` evaluations, which does not reward exploratory behavior that improves performance beyond that horizon.",3. Efficient Nonmyopic Active Search,[0],[0]
"We propose to continue to exactly compute the expected utility to some fixed horizon, but to approximate the remainder of the search differently.",3. Efficient Nonmyopic Active Search,[0],[0]
"We will approximate the expected utility from any remaining portion of the search by assuming that any remaining points, {xi+1, xi+2, . . .",3. Efficient Nonmyopic Active Search,[0],[0]
", xt}, in our budget will be selected simultaneously in one big batch.",3. Efficient Nonmyopic Active Search,[0],[0]
"One rationale is if we assume that after observing Di, the labels of all remaining unlabeled points are conditionally independent, then this approximation recovers the Bayesian optimal policy exactly.",3. Efficient Nonmyopic Active Search,[0],[0]
"By exploiting linearity of expectation, it is easy to work out the optimal policy for selecting such a simultaneous batch observation: we simply select the points with the highest probability of being valuable.",3. Efficient Nonmyopic Active Search,[0],[0]
"The resulting approximation is
max x′
E [ u(Dt\Di) | x′,Di",3. Efficient Nonmyopic Active Search,[0],[0]
],3. Efficient Nonmyopic Active Search,[0],[0]
"≈ ∑′
t−i Pr(y = 1 | x,Di), (6)
where the summation-with-prime symbol ∑′
k indicates that we only sum the largest k values.
",3. Efficient Nonmyopic Active Search,[0],[0]
"Our proposed policy selects points by maximizing the approximate final expected utility using:
E [ u(Dt) | xi,Di−1 ]",3. Efficient Nonmyopic Active Search,[0],[0]
≈ u(Di−1),3. Efficient Nonmyopic Active Search,[0],[0]
"+
Pr(yi = 1 | xi,Di−1) +",3. Efficient Nonmyopic Active Search,[0],[0]
"Eyi [∑′ t−i Pr ( y = 1 | x,Di )] ︸ ︷︷ ︸
exploration, <t−i
.",3. Efficient Nonmyopic Active Search,[0],[0]
"(7)
We will call this policy efficient nonmyopic search (ENS).",3. Efficient Nonmyopic Active Search,[0],[0]
"As in the optimal policy, we can interpret (7) naturally as rewarding both exploitation and exploration, where the exploration benefit is judged by a point’s capability to increase the top probabilities among currently unlabeled points.",3. Efficient Nonmyopic Active Search,[0],[0]
"We note further that in (7) the reward for exploration naturally decreases over time as the budget is depleted, exactly as in the optimal policy.",3. Efficient Nonmyopic Active Search,[0],[0]
"In particular, the very last point xt is chosen greedily by maximizing probability, agreeing with the true optimal policy.",3. Efficient Nonmyopic Active Search,[0],[0]
"The second-to-last point is also guaranteed to match the optimal policy.
",3. Efficient Nonmyopic Active Search,[0],[0]
"Note that we may also use the approximation in (6) as part of a finite-horizon lookahead with ` > 1, producing a family of increasingly expensive but higher-fidelity approximations to the optimal policy, all retaining the same budget consciousness.",3. Efficient Nonmyopic Active Search,[0],[0]
The approximation in (7) is equivalent to a one-step maximization of (6).,3. Efficient Nonmyopic Active Search,[0],[0]
"We will see in our experiments that this is often enough to show massive gains in performance, and
that even this policy shows clear awareness of the remaining budget throughout the search process, automatically and dynamically trading off exploration and exploitation.",3. Efficient Nonmyopic Active Search,[0],[0]
"To illustrate the nonmyopic behavior of our policy, we have adapted the toy example presented by Garnett et al. (2012).",3.1. Nonmyopic Behavior,[0],[0]
"Let I , [0, 1]2 be the unit square.",3.1. Nonmyopic Behavior,[0],[0]
We repeated the following experiment 100 times.,3.1. Nonmyopic Behavior,[0],[0]
We selected 500 points i.i.d. uniformly at random from I to form the input space X .,3.1. Nonmyopic Behavior,[0],[0]
We create an active search problem by defining the set of targets R ⊆ X to be all points within Euclidean distance 1/4 from either the center or any corner of I .,3.1. Nonmyopic Behavior,[0],[0]
We took the closest point to the center (always a target) as an initial training set.,3.1. Nonmyopic Behavior,[0],[0]
"We then applied ENS and the two-step-lookahead (4) policies to sequentially select 200 further points for labeling.
",3.1. Nonmyopic Behavior,[0],[0]
Figure 1 shows a kernel density estimate of the distribution of locations selected by both methods during two time intervals.,3.1. Nonmyopic Behavior,[0],[0]
Figures 1(a–b) correspond to our method; Figures 1(c–d) to two-step lookahead.,3.1. Nonmyopic Behavior,[0],[0]
"Figures 1(a, c) consider the distribution of the first 100 selected locations; Figures 1(b, d) consider the last 100.",3.1. Nonmyopic Behavior,[0],[0]
The qualitative difference between these strategies is clear.,3.1. Nonmyopic Behavior,[0],[0]
"The myopic policy focused on collecting all targets around the center (Figure 1(c)), whereas our policy explores the boundaries of the center clump with considerable intensity, as well as some of the corners (Figure 1(a)).",3.1. Nonmyopic Behavior,[0],[0]
"As a result, our policy is capable of finding some of targets in the corners, whereas two-step lookahead hardly ever can (Figure 1(d)).",3.1. Nonmyopic Behavior,[0],[0]
"We can also see that the highest probability mass in Figure 1(b) is the center, which shows that our policy typically saves many high-probability points until the end.",3.1. Nonmyopic Behavior,[0],[0]
"On average, the ENS policy found about 40 more targets at termination than the two-step-lookahead policy.",3.1. Nonmyopic Behavior,[0],[0]
The complexity of our policy (7) isO ( n ( 2(n+n log n) )),3.2. Implementation and Time Complexity,[0],[0]
"= O(n2 log n), for n = |X |, because we need to compute the approximate expected utility for all n points, evaluate an expectation over its label, conditioning the model and sorting the posterior probabilities in the expectation.",3.2. Implementation and Time Complexity,[0],[0]
"However, for some classification models Pr(y = 1 | x,D), observing one point will only affect the probabilities on a small portion of the other points (e.g., in a k-nn model).",3.2. Implementation and Time Complexity,[0],[0]
"We can exploit such structure to reduce the complexity of our method by avoiding unnecessary computation.
",3.2. Implementation and Time Complexity,[0],[0]
"Specifically, suppose that after observing a point we only need to update the probabilities of at-most m other points.",3.2. Implementation and Time Complexity,[0],[0]
We can avoid repeatedly sorting the probabilities of every unlabeled point when computing the score of each candidate point.,3.2. Implementation and Time Complexity,[0],[0]
"Once the current probabilities are sorted (O(n log n)), we only need to update m probabilities and sort these as well (O(m logm)); now we can merge both
lists to get the top t − i posterior probabilities in time O(t− i), where i is the index of current iteration.",3.2. Implementation and Time Complexity,[0],[0]
"In summary, these tricks can reduce the computational complexity to O ( n(log n + m logm + t) ) .",3.2. Implementation and Time Complexity,[0],[0]
"We can see the complexity is about the same as two-step lookahead, which is O ( n(log n+m) )",3.2. Implementation and Time Complexity,[0],[0]
when using the same tricks.,3.2. Implementation and Time Complexity,[0],[0]
"To further reduce the computational complexity, we can use a similar strategy as suggested by Garnett et al. (2012) to bound the score function (7) and prune points that cannot possibly maximize our score.",3.3. Pruning the Search Space,[0],[0]
We consider the same two assumptions proposed by these authors.,3.3. Pruning the Search Space,[0],[0]
"First, observing a new negative point will not raise the probability of any other point being a target.",3.3. Pruning the Search Space,[0],[0]
"Second, we are able to bound the maximum probability of the unlabeled points after conditioning on a given number of additional targets; that is, we assume there is a function p∗(n,D) such that
p∗(n,D) ≥ max x∈X\D
Pr(y = 1 | x,D∪D′, ∑
y′∈D′y ′ ≤ n).
",3.3. Pruning the Search Space,[0],[0]
"That is, the probability of any unlabeled point can become at most p∗(n,D) after further conditioning on n or fewer additional target points.
",3.3. Pruning the Search Space,[0],[0]
"Consider an unlabeled point x at time i, and define π(x) =",3.3. Pruning the Search Space,[0],[0]
"Pr(y = 1 | x,Di) for the remainder of this discussion.",3.3. Pruning the Search Space,[0],[0]
"The
score (7), denoted f(x) here for simplicity, can be upper bounded by
f(x) ≤",3.3. Pruning the Search Space,[0],[0]
"π · ( 1 + (t− i)p∗(1,Di) )",3.3. Pruning the Search Space,[0],[0]
"+
(1− π) · (∑′
t−i",3.3. Pruning the Search Space,[0],[0]
Pr(y ′,3.3. Pruning the Search Space,[0],[0]
"= 1 | x′,Di)
) , U(π).
",3.3. Pruning the Search Space,[0],[0]
Note this upper bound is only a function of the current probability π.,3.3. Pruning the Search Space,[0],[0]
Let x+ be the point with maximum probability.,3.3. Pruning the Search Space,[0],[0]
Then f(x+) is certainly a lower bound of maxx f(x).,3.3. Pruning the Search Space,[0],[0]
"Hence, those points satisfying U ( π(x) )",3.3. Pruning the Search Space,[0],[0]
< f(x+) can be safely removed from consideration.,3.3. Pruning the Search Space,[0],[0]
"Solving this inequality, we have
π(x) <",3.3. Pruning the Search Space,[0],[0]
"f(x+)−
∑′",3.3. Pruning the Search Space,[0],[0]
t−i,3.3. Pruning the Search Space,[0],[0]
"Pr(y
′ = 1 | x′,D) 1 + p∗(1,D)(t− i)− ∑′ t−i",3.3. Pruning the Search Space,[0],[0]
Pr(y ′,3.3. Pruning the Search Space,[0],[0]
"= 1 | x′,D) .
",3.3. Pruning the Search Space,[0],[0]
"(8) Then, all points with current probability lower than the RHS of (8) can be removed from consideration.",3.3. Pruning the Search Space,[0],[0]
We will show empirically that a large fraction of points can often be pruned on massive datasets.,3.3. Pruning the Search Space,[0],[0]
Our method falls into the broader framework of active learning.,4. Related Work,[0],[0]
"The particular setting of finding elements of a valuable class is rather unusual in active learning, which typically considers the goal of training a high-fidelity model (Lewis & Gale, 1994).",4. Related Work,[0],[0]
"For an exhaustive introduction to active learning, we refer the reader to Settles (2010).
",4. Related Work,[0],[0]
"The multi-armed bandit (MAB) problem shares some similarities with active search, where selecting an item can understood as “pulling an arm.”",4. Related Work,[0],[0]
"However, in active search the items are correlated, and, critically, they can never be played twice.",4. Related Work,[0],[0]
"Despite the difference, we note that our ENS policy is somewhat similar to the knowledge gradient policy introduced by Frazier et al. (2008).
",4. Related Work,[0],[0]
"Active search can be seen as a special case of Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) with binary observations and cumulative reward.",4. Related Work,[0],[0]
"Several nonmyopic policies have been proposed for Bayesian optimization in the regression setting (e.g., Ling et al. (2016)), and our method is spiritually similar to the recently propopsed GLASSES algorithm (González et al., 2016).
",4. Related Work,[0],[0]
"Vanchinathan et al. (2015) proposed a method called GPSELECT to solve a class of problems the authors call “adaptive valuable item discovery,” which generalizes active search to the regression setting.",4. Related Work,[0],[0]
"GP-SELECT employs a Gaussian process regression model in a manner inspired by the Gaussian process upper confidence bound (GP-UCB) algorithm (Srinivas et al., 2010).",4. Related Work,[0],[0]
"A parameter must be specified to balance exploration and exploitation, whereas our method automatically and dynamically trades off these quantities.",4. Related Work,[0],[0]
"The method is also critically tied to Gaussian process
regression as the underlying model, which is inappropriate for classification.",4. Related Work,[0],[0]
"Our decision-theoretic approach does not make any assumptions about the classification model.
",4. Related Work,[0],[0]
"Active search can also be seen as a special case of (partially observable) Markov decision processes ((PO)MDPs), for which there are known hardness results.",4. Related Work,[0],[0]
"Sabbadin et al. (2007), for example, defined the class of so-called “purely epistemic” MDPs (EMDPs), where the state does not evolve over time.",4. Related Work,[0],[0]
The authors showed that the optimal policy for these problems cannot admit polynomial-time constant approximations.,4. Related Work,[0],[0]
"Unfortunately, these hardness results, for the very rich class of EMDPs are not trivially transferred to the more-specific active search problem.
",4. Related Work,[0],[0]
"Our proposed approximation is similar in nature to the active search policy proposed by Wang et al. (2013), which only considered the effect of raising probabilities after observing a positive label, and did not consider the budget.",4. Related Work,[0],[0]
"Rather, the proposed score always encourages maximal exploration, in opposition to the optimal policy.
",4. Related Work,[0],[0]
"There has been some attention to active search in the graph setting where the input domain X is the nodes of a graph (Garnett et al., 2011; Wang et al., 2013; Pfeiffer III et al., 2014; Ma et al., 2015a).",4. Related Work,[0],[0]
Our method does not restrict the input space.,4. Related Work,[0],[0]
"Further, the classification models used in these settings are often difficult to scale to large datasets, e.g., requiring the pseudoinverse of the graph Laplacian.
Finally, variations on the active search problem have also been considered.",4. Related Work,[0],[0]
"Ma et al. (2014) proposed the active area search problem, wherein a continuous function is sampled to discover regions with large mean value, and Ma et al. (2015b) extended this idea to define the more-general active pointillistic pattern search problem.",4. Related Work,[0],[0]
These settings do not allow querying for labels directly and offer no insight to the core active search problem.,4. Related Work,[0],[0]
"We implemented our approximation to the Bayesian optimal policy with the MATLAB active learning toolbox,2 and have compared the performance of our proposed ENS policy with several baselines.",5. Experiments,[0],[0]
"First we compare with the myopic onestep (greedy) and two-step approximations to the Bayesian optimal policy, presented in (3–4).",5. Experiments,[0],[0]
"Note that Garnett et al. (2012) and Garnett et al. (2015) thoroughly compared the one- and two-step policies, with the finding that the lessmyopic two-step algorithm usually performs better in terms of targets found, as one would expect.",5. Experiments,[0],[0]
"In our experiments we will mainly focus on comparing our algorithm with myopic two-step approximate policy.
",5. Experiments,[0],[0]
"We also consider a simple baseline which we call RANDOM-
2https://github.com/rmgarnett/active_learning
GREEDY (RG).",5. Experiments,[0],[0]
"Here we randomly select points to query (exploration) during the first half of the budget, and select the remainder using greedy selection (exploitation).",5. Experiments,[0],[0]
"Although naïve, this policy adapts to the budget.
",5. Experiments,[0],[0]
"We further compare with the score function proposed by Wang et al. (2013), which we refer to as IMS:
IMS(x) =",5. Experiments,[0],[0]
"Pr(y = 1 | x,D) ( 1 + α IM(x) )",5. Experiments,[0],[0]
"; (9)
where IM(x) measures the “expected impact”, the sum of the raised probabilities x results in if it is positive.",5. Experiments,[0],[0]
Note that it is difficult to determine the tradeoff parameter α without (expensive) cross validation.,5. Experiments,[0],[0]
"The empirical results in (Wang et al., 2013) indicate that α = 10−4 performs well on average; we will fix this value in our experiments.
",5. Experiments,[0],[0]
"Finally, we have also considered the following UCB-style (Auer, 2002) score function: α(x,D) = π + γ √ π(1− π), where π = Pr(y = 1 | x,D) and γ is a tradeoff parameter.",5. Experiments,[0],[0]
"The UCB score function is very popular and is the essence of the methods in (Vanchinathan et al., 2015; Srinivas et al., 2010) developed for Gaussian processes, including GP-SELECT.",5. Experiments,[0],[0]
"We considered various γ values and our experiments show that it is no better than two-step lookahead, so we present these results in the appendix due to space.
",5. Experiments,[0],[0]
"The probability model Pr(y = 1 | x,D) we will adopt is the k-nearest-neighbor (k-NN) classifier as described in Section 7 of (Garnett et al., 2012).",5. Experiments,[0],[0]
"This model, while being rather simple, shows reasonable generalization error, is nonparameteric, and can be rapidly updated given new training data, an important property in the active setting we consider here.",5. Experiments,[0],[0]
We will also adopt the probability bound (8) for this model described in that work.,5. Experiments,[0],[0]
"Note IMS was proposed together (but orthogonally) with a graph model for the probability, which is computationally infeasible (O(n3)) for our datasets.",5. Experiments,[0],[0]
"So we also use k-NN model for IMS.
5.1.",5. Experiments,[0],[0]
"CiteSeerx Data
For our first real data experiment, we consider a subset of the CiteSeerx citation network, first described in (Garnett et al., 2012).",5. Experiments,[0],[0]
This dataset comprises 39 788 computer science papers published in the top-50 most-popular computer science venues.,5. Experiments,[0],[0]
We form an undirected citation network from these papers.,5. Experiments,[0],[0]
"The target class is papers published in the NIPS proceedings; there are 2 190 such papers, 5.5% of the whole dataset.",5. Experiments,[0],[0]
"Note that distinguishing NIPS papers in the citation network is not an easy task, because many other highly related venues such as ICML, AAAI, IJCAI, etc. are also among the most-popular venues.",5. Experiments,[0],[0]
"A feature vector for each paper is computed by performing graph principal component analysis (Fouss et al., 2007) on the citation network and retaining the first 20 principal components.
",5. Experiments,[0],[0]
"We select a single target (i.e., a NIPS paper) uniformly at
random to form an initial training set.",5. Experiments,[0],[0]
"The budget is set to t = 500, and we use k = 50 in the k-NN model.",5. Experiments,[0],[0]
"These parameters match the choices in (Garnett et al., 2012).",5. Experiments,[0],[0]
We use each policy to sequentially select t papers for labeling.,5. Experiments,[0],[0]
"The experiment was repeated 20 times, varying the initial seed target.",5. Experiments,[0],[0]
Figure 2 shows the average number of targets found for each method as a function of the number of queries.,5. Experiments,[0],[0]
"We first observe that the ranking of the performance is ENS, two-step, IMS, one-step, and RG, and our policy outperforms the two-step policy in this task by a large margin.",5. Experiments,[0],[0]
"The mean difference in number of targets found at termination vs. twostep is 34.6 (189 vs. 155), an improvement on average of 22%.",5. Experiments,[0],[0]
"A two-sided paired t-test testing the hypothesis that the average difference of targets found is zero returns a pvalue of p < 10−4, and a 95% confidence interval on the increase in number of targets found of [19.80, 49.30].
",5. Experiments,[0],[0]
"Another interesting observation is that during the initial∼80 queries, ENS actually performs worse on average than all baseline policies except RG, after which it quickly outperforms them.",5. Experiments,[0],[0]
This feature perfectly illustrates an automatic exploration–exploitation transition made by our policy.,5. Experiments,[0],[0]
"As we are always cognizant of our budget, we spend the initial stage thoroughly exploring the domain, without immediate reward.",5. Experiments,[0],[0]
"Once complete, we exploit what we learned for the remainder of the budget.",5. Experiments,[0],[0]
"This tradeoff happens automatically and without any need for an explicit two-stage approach or arbitrary tuning parameters.
",5. Experiments,[0],[0]
Varying the Budget.,5. Experiments,[0],[0]
"A distinguishing feature of our method is that it always takes the remaining budget into consideration when selecting a point, so we would expect different behavior with different budgets.",5. Experiments,[0],[0]
"We repeated the above experiment for budgets t ∈ {100, 300, 500, 700, 900}, and report in Table 1 the average number of targets found at these time points for each method.",5. Experiments,[0],[0]
We have the following observations from the table.,5. Experiments,[0],[0]
"First, ENS performs better than
all other baseline policies for every budget.",5. Experiments,[0],[0]
"Second, ENS is able to adapt to the specified budget.",5. Experiments,[0],[0]
"For example, when comparing performance after 100 queries, ENS–100 has located many more targets than the ENS methods with greater budgets, which at that time are still strongly rewarding exploration.",5. Experiments,[0],[0]
"A similar pattern holds when comparing other pairs of ENS variations, with one minor exception.",5. Experiments,[0],[0]
Our next dataset considers an application from materials science: discovering novel alloys forming bulk metallic glasses (BMGs).,5.2. Finding Bulk Metallic Glasses,[0],[0]
"BMGs have numerous desirable properties, including high toughness and good wear resistance compared to crystalline alloys.",5.2. Finding Bulk Metallic Glasses,[0],[0]
"We compiled a database of 118 678 known alloys from the materials literature (e.g., (Kawazoe et al., 1997; all)), an extension of the dataset from (Ward et al., 2016).",5.2. Finding Bulk Metallic Glasses,[0],[0]
"Of these, 4 746 (∼4%) are known to exhibit glass-forming ability, which we define to be targets.",5.2. Finding Bulk Metallic Glasses,[0],[0]
We conduct the same experiments described for the CiteSeerx data above and show the results in Table 1.,5.2. Finding Bulk Metallic Glasses,[0],[0]
"We can see the results again demonstrate our policy’s superior performance over all other methods, and its ability of adapting to the remaining budget.",5.2. Finding Bulk Metallic Glasses,[0],[0]
We further conduct experiments on a massive database of chemoinformatic data.,5.3. Virtual Drug Screening Data,[0],[0]
The basic setting is to screen a large database of compounds searching for those that show binding activity against some biological target.,5.3. Virtual Drug Screening Data,[0],[0]
This is a basic component of drug-discovery pipelines.,5.3. Virtual Drug Screening Data,[0],[0]
"The dataset comprises 120 activity classes of human biological importance selected from the Binding DB (Liu et al., 2007) database.",5.3. Virtual Drug Screening Data,[0],[0]
"For each activity class, there are a small number of compounds with significant binding activity; the number of targets varies
from 200 to 1 488 across the activity classes.",5.3. Virtual Drug Screening Data,[0],[0]
From these we define 120 different active search problems.,5.3. Virtual Drug Screening Data,[0],[0]
"There are also 100 000 presumed inactive compounds selected at random from the ZINC database (Sterling & Irwin, 2015); these are used as a shared negative class for each of these problems.",5.3. Virtual Drug Screening Data,[0],[0]
"For each compound, we consider two different feature representations, also known as chemoinformatic fingerprints, called ECFP4 and GpiDAPH3.",5.3. Virtual Drug Screening Data,[0],[0]
"These fingerprints are binary vectors encoding the relevant chemical characteristics of the compounds; see (Garnett et al., 2015) for more details.3 So in total we have 240 active search problems, each with more than 100 000 points, and with targets less than 1.5%.
",5.3. Virtual Drug Screening Data,[0],[0]
"As is standard in this setting, we compute fingerprint similarities via the Jaccard index (Jasial et al., 2016), which are used to define the weight matrix of the k-NN model from above, setting k = 100 for all the experiments.",5.3. Virtual Drug Screening Data,[0],[0]
"For active search policies, we again randomly select one positive as the initial training set, and sequentially query t = 500 further points.",5.3. Virtual Drug Screening Data,[0],[0]
"We also report the performance of a baseline where we randomly sample a stratified sample of size 5% of the database (∼5 000 points, more than 10 times the budget of the active search policies).",5.3. Virtual Drug Screening Data,[0],[0]
"From this sample, we train the same k-NN model, compute the active probability of the remaining points, and query the 500 points with the highest posterior activity probability.",5.3. Virtual Drug Screening Data,[0],[0]
"All experiments were repeated 20 times, varying the initial training point.",5.3. Virtual Drug Screening Data,[0],[0]
Note we did not test IMS on these data due to computational expense.,5.3. Virtual Drug Screening Data,[0],[0]
"Our policy nominally has higher time complexity, but our pruning strategy can reduce the computation significantly in practice, as we show in Section 5.4.
",5.3. Virtual Drug Screening Data,[0],[0]
Table 2 summarizes the results.,5.3. Virtual Drug Screening Data,[0],[0]
First we notice that all 3We did not conduct experiments on the MACCS fingerprint.,5.3. Virtual Drug Screening Data,[0],[0]
It was inferior in the findings of Garnett et al. (2015).,5.3. Virtual Drug Screening Data,[0],[0]
"A reviewer of (Jasial et al., 2016) noted that it is no longer used, due to clear underperformance compared to, e.g., ECFP4 and GpiDAPH3.
",5.3. Virtual Drug Screening Data,[0],[0]
"Table 2: Number of active compounds found by various active search policies at termination for each fingerprint, averaged over 120 active classes and 20 experiments.",5.3. Virtual Drug Screening Data,[0],[0]
"Also shown is the difference of performance between ENS and two-step lookahead and the results of the corresponding paired t-test.
policy t-test results
fingerprint 100-NN RG one-step two-step ENS difference p-value 95% CI
ECFP4 189 189 289 297 303 5.29 1.76× 10−3 2.01 8.56 GpiDAPH3 134 170 255 261 276 14.8 3.90× 10−13 11.2 18.4
active search policies perform much better than the recall of a simple classification algorithm, even though they observe less than one-tenth the data.",5.3. Virtual Drug Screening Data,[0],[0]
"Interestingly, even the naïve random-greedy (RG) policy performs much better than this baseline, albeit much worse than other active search policies.",5.3. Virtual Drug Screening Data,[0],[0]
"The two-step policy is again better than the greedy policy for both fingerprints, which is consistent with the results reported in (Garnett et al., 2015).",5.3. Virtual Drug Screening Data,[0],[0]
"The ENS policy performs significantly better than two-step lookahead; a two-sided paired t-test overwhelmingly rejects the hypothesis that the performance at termination is equal in both cases.
",5.3. Virtual Drug Screening Data,[0],[0]
Figure 3 shows the mean difference in cumulative targets found between ENS and the two-step policy for the ECFP4 fingerprint.,5.3. Virtual Drug Screening Data,[0],[0]
"Again, we very clearly observe the automatic trade-off between exploration and exploitation by our method.",5.3. Virtual Drug Screening Data,[0],[0]
"In the initial stage of the search, we explore the space without much initial reward, but around query 200, our algorithm switches automatically to exploitation, outperforming the myopic policy significantly at termination.",5.3. Virtual Drug Screening Data,[0],[0]
"The mean difference curves for the other fingerprint is similar, and can be found in the appendix, along with the individual learning curves of the first six activity classes of ECFP4.",5.3. Virtual Drug Screening Data,[0],[0]
"To investigate how pruning can improve the efficiency of computing the policy, we computed the average number of pruned points across all 120 × 20 × 500 = 3 000 000 iterations of active search, for each fingerprint.",5.4. Effect of Pruning,[0],[0]
"On average about 93% of the unlabeled points are pruned, dramatically improving the computational efficiency by approximately a corresponding linear factor.",5.4. Effect of Pruning,[0],[0]
The time for each experiment was effectively reduced from on the order of one day to that of one hour.,5.4. Effect of Pruning,[0],[0]
See the appendix for detailed results.,5.4. Effect of Pruning,[0],[0]
In this paper we proved the theoretical hardness of active search and proposed an well-motivated and empirically better-performing policy for solving this problem.,6. Conclusion,[0],[0]
"In particular, we proved that no polynomial-time algorithm can approximate the expected utility of the optimal policy within a constant approximation ratio.",6. Conclusion,[0],[0]
"We then proposed a novel method, efficient nonmyopic search (ENS), for the active search problem.",6. Conclusion,[0],[0]
"Our method approximates the Bayesian optimal policy by computing, conditioned on the location of the next point, how many targets are expected at termination, if the remaining budget is spent simultaneously.",6. Conclusion,[0],[0]
"By taking the remaining budget into consideration in each step, we are able to automatically balance exploration and exploitation.",6. Conclusion,[0],[0]
"Despite being nonmyopic, ENS is efficient to compute because future steps are flattened into a single batch, in contrast to the recursive simulation required when computing the true expected utility.",6. Conclusion,[0],[0]
"We also derived an effective pruning strategy that can reduce the number of candidate points we must consider at each step, which can further improve the efficiency dramatically in practice.",6. Conclusion,[0],[0]
"We conducted a massive empirical evaluation that clearly demonstrated superior overall performance on various domains, as well as our automatic balance between exploration and exploitation.
",6. Conclusion,[0],[0]
"Given the hardness result we proved, in general there is little point to require more of an algorithm than superior empirical performance.",6. Conclusion,[0],[0]
"However, one exciting future direction is to understand, under what conditions (e.g., some assumption about the structure of problem instances) we can find efficient algorithms with guarantees.",6. Conclusion,[0],[0]
We would like to thank Brendan Juba for insightful discussion.,Acknowledgments,[0],[0]
"SJ, GM, and RG were supported by the National Science Foundation (NSF) under award number IIA–1355406.",Acknowledgments,[0],[0]
GM was also supported by the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES).,Acknowledgments,[0],[0]
GC and AS were supported by NSF under award number CNS–1560191.,Acknowledgments,[0],[0]
"BM was supported by a Google Research Award, a Yahoo Research Award, and by NSF under award number CCF–1617724.",Acknowledgments,[0],[0]
Active search is an active learning setting with the goal of identifying as many members of a given class as possible under a labeling budget.,abstractText,[0],[0]
"In this work, we first establish a theoretical hardness of active search, proving that no polynomial-time policy can achieve a constant factor approximation ratio with respect to the expected utility of the optimal policy.",abstractText,[0],[0]
"We also propose a novel, computationally efficient active search policy achieving exceptional performance on several real-world tasks.",abstractText,[0],[0]
"Our policy is nonmyopic, always considering the entire remaining search budget.",abstractText,[0],[0]
"It also automatically and dynamically balances exploration and exploitation consistent with the remaining budget, without relying on a parameter to control this tradeoff.",abstractText,[0],[0]
"We conduct experiments on diverse datasets from several domains: drug discovery, materials science, and a citation network.",abstractText,[0],[0]
"Our efficient nonmyopic policy recovers significantly more valuable points with the same budget than several alternatives from the literature, including myopic approximations to the optimal policy.",abstractText,[0],[0]
Efficient Nonmyopic Active Search,title,[0],[0]
Recurrent Neural Networks (RNNs) have been successfully used in many applications involving time series.,1. Introduction,[0],[0]
This is because RNNs are well suited for sequential data as they process inputs one element at a time and store relevant information in their hidden state.,1. Introduction,[0],[0]
"In practice, however, training simple RNNs (sRNN) can be challenging due to the problem of exploding and vanishing gradients (Hochreiter et al., 2001).",1. Introduction,[0],[0]
"It has been shown that exploding gradients can occur when the transition matrix of an RNN has a spectral norm larger than one (Glorot & Bengio, 2010).",1. Introduction,[0],[0]
"This results
1The University of Melbourne, Parkville, Australia 2Data61, CSIRO, Australia.",1. Introduction,[0],[0]
"Correspondence to: Zakaria Mhammedi <zak.mhammedi@data61.csiro.au>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
in an error surface, associated with some objective function, having very steep walls (Pascanu et al., 2013).",1. Introduction,[0],[0]
"On the other hand, when the spectral norm of the transition matrix is less than one, the information at one time step tend to vanish quickly after a few time steps.",1. Introduction,[0],[0]
"This makes it challenging to learn long-term dependencies in sequential data.
",1. Introduction,[0],[0]
Different methods have been suggested to solve either the vanishing or exploding gradient problem.,1. Introduction,[0],[0]
"The LSTM has been specifically designed to help with the vanishing gradient (Hochreiter & Schmidhuber, 1997).",1. Introduction,[0],[0]
This is achieved by using gate vectors which allow a linear flow of information through the hidden state.,1. Introduction,[0],[0]
"However, the LSTM does not directly address the exploding gradient problem.",1. Introduction,[0],[0]
"One approach to solving this issue is to clip the gradients (Mikolov, 2012) when their norm exceeds some threshold value.",1. Introduction,[0],[0]
"However, this adds an extra hyperparameter to the model.",1. Introduction,[0],[0]
"Furthermore, if exploding gradients can occur within some parameter search space, the associated error surface will still have steep walls.",1. Introduction,[0],[0]
"This can make training challenging even with gradient clipping.
",1. Introduction,[0],[0]
"Another way to approach this problem is to improve the shape of the error surface directly by making it smoother, which can be achieved by constraining the spectral norm of the transition matrix to be less than or equal to one.",1. Introduction,[0],[0]
"However, a value of exactly one is best for the vanishing gradient problem.",1. Introduction,[0],[0]
A good choice of the activation function between hidden states is also crucial in this case.,1. Introduction,[0],[0]
These ideas have been investigated in recent works.,1. Introduction,[0],[0]
"In particular, the unitary RNN (Arjovsky et al., 2016) uses a special parametrisation to constrain the transition matrix to be unitary, and hence, of norm one.",1. Introduction,[0],[0]
"This parametrisation and other similar ones (Hyland & Rätsch, 2017; Wisdom et al., 2016) have some advantages and drawbacks which we will discuss in more details in the next section.
",1. Introduction,[0],[0]
"The main contributions of this work are as follows:
• We first show that constraining the search space of the transition matrix of an RNN to the set of unitary matrices U(n) is equivalent to limiting the search space to a subset of O(2n) (O(2n) is the set of 2n× 2n orthogonal matrices) of a new RNN with twice the hidden size.",1. Introduction,[0],[0]
"This suggests that it may not be necessary to
ar X
iv :1
61 2.
00 18
8v 5
[ cs
.L G
] 1
3 Ju
n 20
17
work with complex matrices.
",1. Introduction,[0],[0]
"• We present a simple way to parametrise orthogonal transition matrices of RNNs using Householder matrices, and we derive the expressions of the back-propagated gradients with respect to the new parametrisation.",1. Introduction,[0],[0]
"This new parametrisation can also be used in other deep architectures.
",1. Introduction,[0],[0]
• We develop an algorithm to compute the backpropagated gradients efficiently.,1. Introduction,[0],[0]
"Using this algorithm, we show that the worst case time complexity of one gradient step is of the same order as that of the sRNN.",1. Introduction,[0],[0]
"Throughout this work we will refer to elements of the following sRNN architecture.
h(t) = φ(Wh(t−1) + V x(t)), (1)
o(t)",2. Related Work,[0],[0]
"= Y h(t), (2)
where W , V and Y are the hidden-to-hidden, input-tohidden, and hidden-to-output weight matrices.",2. Related Work,[0],[0]
h(t−1) and h(t) are the hidden vectors at time steps t− 1 and t respectively.,2. Related Work,[0],[0]
"Finally, φ is a non-linear activation function.",2. Related Work,[0],[0]
"We have omitted the bias terms for simplicity.
",2. Related Work,[0],[0]
Recent research explored how the initialisation of the transition matrix W influences training and the ability to learn long-term dependencies.,2. Related Work,[0],[0]
"In particular, initialisation with the identity or an orthogonal matrix can greatly improve performance (Le et al., 2015).",2. Related Work,[0],[0]
"In addition to these initialisation methods, one study also considered removing the non-linearity between the hidden-to-hidden connections (Henaff et al., 2016), i.e. the term Wh(t−1) in Equation (1) is outside the activation function φ.",2. Related Work,[0],[0]
"This method showed good results when compared to the LSTM on pathological problems exhibiting long-term dependencies.
",2. Related Work,[0],[0]
"After training a model for a few iterations using gradient descent, nothing guarantees that the initial structures of the transition matrix will be held.",2. Related Work,[0],[0]
"In fact, its spectral norm can deviate from one, and exploding and vanishing gradients can be a problem again.",2. Related Work,[0],[0]
"It is possible to constrain the transition matrix to be orthogonal during training using special parametrisations (Arjovsky et al., 2016; Hyland & Rätsch, 2017), which ensure that its spectral norm is always equal to one.",2. Related Work,[0],[0]
"The unitary RNN (uRNN) (Arjovsky et al., 2016) is one example where the hidden matrix W ∈ Cn×n is the product of elementary matrices, consisting of reflection, diagonal, and Fourier transform matrices.",2. Related Work,[0],[0]
"When the size of hidden layer is equal to n, the transition matrix has a total of only 7n parameters.",2. Related Work,[0],[0]
"Another advantage of this parametrisation is computational efficiency - the matrix-vector product Wv, for some vector v, can be calculated in time complexity O(n log n).",2. Related Work,[0],[0]
"However, it has been shown that this
parametrisation does not allow the transition matrix to span the full unitary group (Wisdom et al., 2016) when the size of the hidden layer is greater than 7.",2. Related Work,[0],[0]
"This may limit the expressiveness of the model.
",2. Related Work,[0],[0]
"Another interesting parametrisation (Hyland & Rätsch, 2017) has been suggested which takes advantage of the algebraic properties of the unitary group U(n).",2. Related Work,[0],[0]
The idea is to use the corresponding matrix Lie algebra u(n) of skew hermitian matrices.,2. Related Work,[0],[0]
"In particular, the transition matrix can be written as W = exp [∑n2 i=1",2. Related Work,[0],[0]
"λiTi ] , where exp is the exponential matrix map and {Ti}n 2
i=1 are predefined n",2. Related Work,[0],[0]
× n matrices forming a bases of the Lie algebra u(n).,2. Related Work,[0],[0]
The learning parameters are the weights {λi}.,2. Related Work,[0],[0]
The fact that the matrix Lie algebra u(n) is closed and connected ensures that the exponential mapping from u(n) to U(n) is surjective.,2. Related Work,[0],[0]
"Therefore, with this parametrisation the search space of the transition matrix spans the whole unitary group.",2. Related Work,[0],[0]
"This is one advantage over the original unitary parametrisation (Arjovsky et al., 2016).",2. Related Work,[0],[0]
"However, the cost of computing the matrix exponential to get W is O(n3), where n is the dimension of the hidden state. .
",2. Related Work,[0],[0]
"Another method (Wisdom et al., 2016) performs optimisation directly of the Stiefel manifold using the Cayley transformation.",2. Related Work,[0],[0]
The corresponding model was called fullcapacity unitary RNN.,2. Related Work,[0],[0]
"Using this approach, the transition matrix can span the full set of unitary matrices.",2. Related Work,[0],[0]
"However, this method involves a matrix inverse as well as matrixmatrix products which have time complexity O(n3).",2. Related Work,[0],[0]
"This can be problematic for large neural networks when using stochastic gradient descent with a small mini-batch size.
",2. Related Work,[0],[0]
"A more recent study (Vorontsov et al., 2017) investigated the effect of soft versus hard orthogonal constraints on the performance of RNNs.",2. Related Work,[0],[0]
The soft constraint was applied by specifying an allowable range for the maximum singular value of the transition matrix.,2. Related Work,[0],[0]
"To this end, the transition matrix was factorised as W = USV ′, where U and V are orthogonal matrices and S is a diagonal matrix containing the singular values ofW .",2. Related Work,[0],[0]
"A soft orthogonal constraint consists of specifying small allowable intervals around 1 for the diagonal elements of S. Similarly to (Wisdom et al., 2016), the matrices U and V were updated at each training iteration using the Cayley transformation, which involves a matrix inverse, to ensure that they remain orthogonal.
",2. Related Work,[0],[0]
"All the methods discussed above, except for the original unitary RNN, involve a step that requires at least a O(n3) time complexity.",2. Related Work,[0],[0]
"All of them, except for one, require the use of complex matrices.",2. Related Work,[0],[0]
"Table 1 summarises the time complexities of various methods, including our approach, for one stochastic gradient step.",2. Related Work,[0],[0]
"In the next section, we show that imposing a unitary constraint on a transition matrix W ∈ Cn×n is equivalent to imposing a special orthog-
onal constraint on a new RNN with twice the hidden size.",2. Related Work,[0],[0]
"Furthermore, since the norm of orthogonal matrices is also always one, using the latter has the same theoretical benefits as using unitary matrices when it comes to the exploding gradient problem.",2. Related Work,[0],[0]
"We can show that when the transition matrixW ∈ Cn×n of an RNN is unitary, there exists an equivalent representation of this RNN involving an orthogonal matrix Ŵ ∈ R2n×2n.
",3. Complex unitary versus orthogonal,[0],[0]
"In fact, consider a complex unitary transition matrix W = A + iB ∈",3. Complex unitary versus orthogonal,[0],[0]
"Cn×n, where A and B are now real-valued matrices in Rn×n.",3. Complex unitary versus orthogonal,[0],[0]
"We also define the following new variables
∀t, ĥ(t) =",3. Complex unitary versus orthogonal,[0],[0]
"[ < ( h(t) ) = ( h(t) )] , V̂ = [< (V )= (V ) ] , Ŵ =",3. Complex unitary versus orthogonal,[0],[0]
[,3. Complex unitary versus orthogonal,[0],[0]
"A −B B A ] ,
where < and = denote the real and imaginary parts of a complex number.",3. Complex unitary versus orthogonal,[0],[0]
"Note that ĥ(t) ∈ R2n, Ŵ ∈ R2n×2n, and V̂ ∈ R2n×nx , where nx is the dimension of the input vector x(t) in Equation (1).
",3. Complex unitary versus orthogonal,[0],[0]
"Assuming that the activation function φ applies to the real and imaginary parts separately, it is easy to show that the update equation of the complex hidden state h(t) of the unitary RNN has the following real space representation
ĥ(t) = φ(Ŵ ĥ(t−1) + V̂ x(t)).",3. Complex unitary versus orthogonal,[0],[0]
"(3)
Even when the activation function φ does not apply to the real and imaginary parts separately, it is still possible to find an equivalent representation in the real space.",3. Complex unitary versus orthogonal,[0],[0]
"Consider the activation function proposed by (Arjovsky et al., 2016)
σmodRelU(z) = { (|z|+ b) z|z| , if |z|+ b > 0
0, otherwise (4)
where b is a bias vector.",3. Complex unitary versus orthogonal,[0],[0]
"For a hidden state ĥ ∈ R2n, the equivalent activation function in the real space representation is given by
[ φ̂(a) ]",3. Complex unitary versus orthogonal,[0],[0]
i =  √ a2i+a 2 ki +bk̃i√,3. Complex unitary versus orthogonal,[0],[0]
"a2i+a 2 ki ai, if √ a2i + a 2 ki + bk̃i > 0
0, otherwise
where ki =",3. Complex unitary versus orthogonal,[0],[0]
"((i+ n) mod 2n) and k̃i = (i mod n) for all i ∈ {1, . . .",3. Complex unitary versus orthogonal,[0],[0]
", 2n}.",3. Complex unitary versus orthogonal,[0],[0]
"The activation function φ̂ is no longer applied to hidden units independently.
",3. Complex unitary versus orthogonal,[0],[0]
Now we will show that the matrix Ŵ is orthogonal.,3. Complex unitary versus orthogonal,[0],[0]
"By definition of a unitary matrix, we have WW ∗ = I where the ∗ represents the conjugate transpose.",3. Complex unitary versus orthogonal,[0],[0]
This implies that AA′,3. Complex unitary versus orthogonal,[0],[0]
+BB′ = I and BA′ −AB′ = 0.,3. Complex unitary versus orthogonal,[0],[0]
"And since we have
ŴŴ ′ =",3. Complex unitary versus orthogonal,[0],[0]
[ AA′ +BB′ AB′ −BA′ BA′ −AB′,3. Complex unitary versus orthogonal,[0],[0]
"AA′ +BB′ ] , (5)
it follows that ŴŴ ′ = I .",3. Complex unitary versus orthogonal,[0],[0]
"Also note that Ŵ has a special structure - it is a block-matrix.
",3. Complex unitary versus orthogonal,[0],[0]
"The discussion above shows that using a complex, unitary transition matrix in Cn×n is equivalent to using an orthogonal matrix, belonging to a subset of O(2n), in a new RNN with twice the hidden size.",3. Complex unitary versus orthogonal,[0],[0]
This is why in this work we focus mainly on parametrising orthogonal matrices.,3. Complex unitary versus orthogonal,[0],[0]
"Before discussing the details of our parametrisation, we first introduce a few notations.",4. Parametrisation of the transition matrix,[0],[0]
"For n, k ∈ N and 2 ≤ k ≤ n, letHk :",4. Parametrisation of the transition matrix,[0],[0]
"Rk → Rn×n be defined as
Hk(u) =
[ In−k 0
0",4. Parametrisation of the transition matrix,[0],[0]
"Ik − 2 uu ′
‖u‖2
] , (6)
where Ik denotes the k-dimensional identity matrix.",4. Parametrisation of the transition matrix,[0],[0]
For u ∈,4. Parametrisation of the transition matrix,[0],[0]
"Rk, Hk(u) is the Householder Matrix in O(n) representing the reflection about the hyperplane orthogonal to the vector (0′n−k,u
′)′ ∈ Rn and passing through the origin, where 0n−k denotes the zero vector in Rn−k.
",4. Parametrisation of the transition matrix,[0],[0]
We also define the mappingH1 :,4. Parametrisation of the transition matrix,[0],[0]
R→ Rn×n as H1(u) =,4. Parametrisation of the transition matrix,[0],[0]
[ In−1 0 0 u ] .,4. Parametrisation of the transition matrix,[0],[0]
"(7)
Note that H1(u) is not necessarily a Householder reflection.",4. Parametrisation of the transition matrix,[0],[0]
"However, when u ∈ {1,−1},H1(u) is orthogonal.
",4. Parametrisation of the transition matrix,[0],[0]
"Finally, for n, k ∈ N and 1 ≤ k ≤ n, we define
Mk :",4. Parametrisation of the transition matrix,[0],[0]
Rk × · · ·,4. Parametrisation of the transition matrix,[0],[0]
"× Rn → Rn×n
(uk, . . .",4. Parametrisation of the transition matrix,[0],[0]
",un) 7→ Hn(un) . . .Hk(uk).
",4. Parametrisation of the transition matrix,[0],[0]
We propose to parametrise the transition matrix W of an RNN using the mappings {Mk}.,4. Parametrisation of the transition matrix,[0],[0]
"When using m reflection vectors {ui}, the parametrisation can be expressed as
W =Mn−m+1(un−m+1, . . .",4. Parametrisation of the transition matrix,[0],[0]
",un) = Hn(un) . .",4. Parametrisation of the transition matrix,[0],[0]
".Hn−m+1(un−m+1), (8)
where ui ∈ Ri for i ∈ {n−m+ 1, . . .",4. Parametrisation of the transition matrix,[0],[0]
", n}.
",4. Parametrisation of the transition matrix,[0],[0]
"For the particular case where m = n in the above parametrisation, we have the following result.",4. Parametrisation of the transition matrix,[0],[0]
Theorem 1.,4. Parametrisation of the transition matrix,[0],[0]
"The image ofM1 includes the set of all n×n orthogonal matrices, i.e. O(n) ⊂M1[R× · · ·",4. Parametrisation of the transition matrix,[0],[0]
"× Rn].
Note that Theorem 1 would not be valid if H1(·) was a standard Householder reflection.",4. Parametrisation of the transition matrix,[0],[0]
"In fact, in the twodimensional case, for instance, the matrix ( 1 0 0 −1 ) cannot be expressed as the product of exactly two standard Householder matrices.
",4. Parametrisation of the transition matrix,[0],[0]
"The parametrisation in (8) has the following advantages:
1.",4. Parametrisation of the transition matrix,[0],[0]
"The parametrisation is smooth*, which is convenient for training with gradient descent.",4. Parametrisation of the transition matrix,[0],[0]
"It is also flexible - a good trade-off between expressiveness and speed can be found by tuning the number of reflection vectors.
",4. Parametrisation of the transition matrix,[0],[0]
2.,4. Parametrisation of the transition matrix,[0],[0]
"The time and space complexities involved in one gradient calculation are, in the worst case, the same as that of the sRNN with the same number of hidden units.",4. Parametrisation of the transition matrix,[0],[0]
"This is discussed in the following subsections.
3.",4. Parametrisation of the transition matrix,[0],[0]
"When m < n, the matrix W is always orthogonal, as long as the reflection vectors are nonzero.",4. Parametrisation of the transition matrix,[0],[0]
"Form = n, the only additional requirement for W to be orthogonal is that u1 ∈ {−1, 1}.
4.",4. Parametrisation of the transition matrix,[0],[0]
"Whenm = n, the transition matrix can span the whole set of n×n orthogonal matrices.",4. Parametrisation of the transition matrix,[0],[0]
"In this case, the total number of parameters needed for W is n(n + 1)/2.",4. Parametrisation of the transition matrix,[0],[0]
This results in only n redundant parameters since the orthogonal set O(n) is a n(n− 1)/2 manifold.,4. Parametrisation of the transition matrix,[0],[0]
Let ui ∈ Ri.,4.1. Back-propagation algorithm,[0],[0]
Let U := (un| . . .,4.1. Back-propagation algorithm,[0],[0]
|un−m+1) ∈ Rn×m be the parameter matrix constructed from the reflection vectors {ui}.,4.1. Back-propagation algorithm,[0],[0]
"In particular, the j-th column of U can be expressed using the zero vector 0j−1 ∈ Rj−1 as
U∗,j =
[ 0j−1
un−j+1
] ∈",4.1. Back-propagation algorithm,[0],[0]
"Rn, 1 ≤ j ≤ m. (9)
",4.1. Back-propagation algorithm,[0],[0]
LetL be a scalar loss function andC(t),4.1. Back-propagation algorithm,[0],[0]
":=Wh(t−1), where W is constructed using the {ui} vectors following Equation (8).",4.1. Back-propagation algorithm,[0],[0]
"In order to back-propagate the gradients through time, we need to compute the following partial derivatives
∂L ∂U (t) :=
[ ∂C(t)
∂U ]′",4.1. Back-propagation algorithm,[0],[0]
"∂L ∂C(t) , (10)
∂L ∂h(t−1) =
[ ∂C(t)
∂h(t−1) ]′",4.1. Back-propagation algorithm,[0],[0]
"∂L ∂C(t) , (11)
*except on a subset of zero Lebesgue measure.
at each time step t. Note that in Equation (10) h(t−1) is taken as a constant with respect to U .",4.1. Back-propagation algorithm,[0],[0]
"Furthermore, we have ∂L∂U = ∑T t=1",4.1. Back-propagation algorithm,[0],[0]
"∂L ∂U(t)
, where T is the length of the input sequence.",4.1. Back-propagation algorithm,[0],[0]
"The gradient flow through the RNN at time step t is shown in Figure 1.
",4.1. Back-propagation algorithm,[0],[0]
"Before describing the algorithm to compute the backpropagated gradients ∂L
∂U(t)",4.1. Back-propagation algorithm,[0],[0]
"and ∂L ∂h(t−1) , we first derive their
expressions as a function of U , h(t−1) and ∂L ∂C(t)
using the compact WY representation (Joffrain et al., 2006) of the product of Householder reflections.",4.1. Back-propagation algorithm,[0],[0]
Proposition 1.,4.1. Back-propagation algorithm,[0],[0]
"Let n,m ∈ N s.t. m ≤ n",4.1. Back-propagation algorithm,[0],[0]
− 1.,4.1. Back-propagation algorithm,[0],[0]
Let ui ∈ Ri and U = (un| . . .,4.1. Back-propagation algorithm,[0],[0]
|un−m+1) be the matrix defined in Equation (9).,4.1. Back-propagation algorithm,[0],[0]
"We have
T := striu(U ′U) + 1
2 diag(U ′U), (12)
Hn(un) . .",4.1. Back-propagation algorithm,[0],[0]
.Hn−m+1(un−m+1),4.1. Back-propagation algorithm,[0],[0]
= I,4.1. Back-propagation algorithm,[0],[0]
"− UT−1U ′, (13)
where striu(U ′U), and diag(U ′U) represent the strictly upper part and the diagonal of the matrix U ′U , respectively.
",4.1. Back-propagation algorithm,[0],[0]
Equation (13) is the compact WY representation of the product of Householder reflections.,4.1. Back-propagation algorithm,[0],[0]
"For the particular case where m = n, the RHS of Equation (13) should be replaced by ( I − UT−1U ′ )",4.1. Back-propagation algorithm,[0],[0]
"H1(u1), where H1 is defined in (7) and U = (un| . . .",4.1. Back-propagation algorithm,[0],[0]
"|u2).
",4.1. Back-propagation algorithm,[0],[0]
"The following theorem gives the expressions of the gradients ∂L
∂U(t) and ∂L ∂h(t−1) when m ≤ n− 1 and h = h(t−1).
",4.1. Back-propagation algorithm,[0],[0]
"Algorithm 1 Local forward and backward propagations at time step t. For a matrix A, A∗,k denotes the k-th column.
",4.1. Back-propagation algorithm,[0],[0]
"1: Inputs: h(t−1), ∂L∂C , U = (un| . . .",4.1. Back-propagation algorithm,[0],[0]
|un−m+1),4.1. Back-propagation algorithm,[0],[0]
.,4.1. Back-propagation algorithm,[0],[0]
"2: Outputs: ∂L
∂U(t) , ∂L ∂h(t−1) , C(t) =Wh(t−1)
3: Require: G ∈ Rn×m, g ∈ Rn, H ∈ Rn×(m+1) 4: N ← (‖un‖2 , . . .",4.1. Back-propagation algorithm,[0],[0]
", ‖un−m+1‖2) 5: H∗,m+1 ← h(t−1) 6: g ← ∂L∂C 7: for k = m to 1 do {Local Forward Propagation} 8: h̃k ← 2NkU ′ ∗,kH∗,k+1
9: H∗,k ← H∗,k+1 − h̃kU∗,k 10: end for 11: for k = 1 to m do {Local Backward Propagation} 12: C̃k ← 2NkU ′",4.1. Back-propagation algorithm,[0],[0]
"∗,kg 13: g ← g − C̃kU∗,k 14: G∗,k ← −h̃kg − C̃kH∗,k+1 15: end for 16: C(t) ← H∗,1 17: ∂L
∂h(t−1)",4.1. Back-propagation algorithm,[0],[0]
"← g
18: ∂L ∂U(t)",4.1. Back-propagation algorithm,[0],[0]
"← G
Theorem 2.",4.1. Back-propagation algorithm,[0],[0]
"Let n,m ∈ N s.t. m ≤ n−1.",4.1. Back-propagation algorithm,[0],[0]
"Let U ∈ Rn×m, h ∈ Rn, and C = (I − UT−1U ′)h, where T is defined in Equation (12).",4.1. Back-propagation algorithm,[0],[0]
"If L is a scalar loss function which depends on C, then we have
∂L ∂U =U [(h̃C̃ ′) ◦",4.1. Back-propagation algorithm,[0],[0]
B′ + (C̃h̃′) ◦B]− ∂L ∂C h̃′,4.1. Back-propagation algorithm,[0],[0]
"− hC̃ ′, (14) ∂L ∂h = ∂L ∂C",4.1. Back-propagation algorithm,[0],[0]
"− UC̃, (15)
where h̃ = T−1U ′h, C̃ = (T ′)−1U ′",4.1. Back-propagation algorithm,[0],[0]
"∂L∂C , and B = striu(J)",4.1. Back-propagation algorithm,[0],[0]
"+ 12I , with J being the m ×m matrix of all ones and ◦ the Hadamard product.
",4.1. Back-propagation algorithm,[0],[0]
The proof of Equations (14) and (15) is provided in Appendix A.,4.1. Back-propagation algorithm,[0],[0]
"Based on Theorem 2, Algorithm 1 performs the one-step forward-propagation (FP) and back-propagation (BP) required to compute C(t), ∂L
∂U(t) , and ∂L ∂h(t−1) .",4.1. Back-propagation algorithm,[0],[0]
"See
Appendix B for more detail about how this algorithm is derived using Theorem 2.
",4.1. Back-propagation algorithm,[0],[0]
In the next section we analyse the time and space complexities of this algorithm.,4.1. Back-propagation algorithm,[0],[0]
"At each time step t, the flop count required by Algorithm 1 is (13n+ 2)m; 6nm for the one-step FP and (7n+ 2)m for the one-step BP.",4.2. Time and Space complexity,[0],[0]
Note that the vector N only needs to be calculated at one time step.,4.2. Time and Space complexity,[0],[0]
This reduces the flop count at the remaining time steps to (11n + 3)m.,4.2. Time and Space complexity,[0],[0]
"The fact that the matrix U has all zeros in its upper triangular part can be used to further reduce the total flop count to (11n− 3m+
5)m; (4n−m+2)m for the one-step FP and (7n− 2m+ 3)m for the one-step BP.",4.2. Time and Space complexity,[0],[0]
"See Appendix C for more details.
",4.2. Time and Space complexity,[0],[0]
"Note that if the values of the matrices H , defined in Algorithm 1, are first stored during a “global” FP (i.e. through all time steps), then used in the BP steps, the time complexity† for a global FP and BP using one input sequence of length T are, respectively, ≈ 3n2T and ≈ 5n2T , when m ≈ n and",4.2. Time and Space complexity,[0],[0]
n 1.,4.2. Time and Space complexity,[0],[0]
"In contrast with the sRNN case with n hidden units, the global FP and BP have time complexities ≈ 2n2T and ≈ 3n2T .",4.2. Time and Space complexity,[0],[0]
"Hence, when m ≈ n, the FP and BP steps using our parametrisation require only about twice more flops than the sRNN case with the same number of hidden units.
",4.2. Time and Space complexity,[0],[0]
"Note, however, that storing the values of the matrices H at all time steps requires the storage of mnT values for one sequence of length T , compared with nT when only the hidden states {h(t)}t=1 are stored.",4.2. Time and Space complexity,[0],[0]
When m 1 this may not be practical.,4.2. Time and Space complexity,[0],[0]
One solution to this problem is to generate the matrices H locally at each BP step using U and h(t−1).,4.2. Time and Space complexity,[0],[0]
This results in a global BP complexity of (11n − 3m + 5)mT .,4.2. Time and Space complexity,[0],[0]
Table 2 summarises the flop counts for the FP and BP steps.,4.2. Time and Space complexity,[0],[0]
Note that these flop counts are for the case when m ≤ n−1.,4.2. Time and Space complexity,[0],[0]
"When m = n, the complexity added due to the multiplication byH1(u1) is negligible.",4.2. Time and Space complexity,[0],[0]
"Although we decided to focus on the set of real-valued orthogonal matrices, for the reasons given in Section 3, our parametrisation can readily be modified to apply to the general unitary case.
",4.3. Extension to the Unitary case,[0],[0]
"Let Ĥk : Ck → Cn×n, 2 ≤ k ≤ n, be defined by Equation (6) where the transpose sign ′ is replaced by the conjugate transpose ∗. Furthermore, let Ĥ1 : Rn → Cn×n be defined as Ĥ1(θ)",4.3. Extension to the Unitary case,[0],[0]
"= diag(eiθ1 , . . .",4.3. Extension to the Unitary case,[0],[0]
", eiθn).",4.3. Extension to the Unitary case,[0],[0]
"With the new mappings {Ĥk}k=nk=1 , we have the following corollary.
",4.3. Extension to the Unitary case,[0],[0]
"†We considered only the time complexity due to computations through the hidden-to-hidden connections of the network.
",4.3. Extension to the Unitary case,[0],[0]
Corollary 1.,4.3. Extension to the Unitary case,[0],[0]
"Let M̂1 be the mapping defined as
M̂1 :",4.3. Extension to the Unitary case,[0],[0]
Rn × C2 × · · ·,4.3. Extension to the Unitary case,[0],[0]
"× Cn → Cn×n
(θ,u2, . . .",4.3. Extension to the Unitary case,[0],[0]
",un) 7→Ĥn(un) . . .",4.3. Extension to the Unitary case,[0],[0]
"Ĥ2(u2)Ĥ1(θ).
",4.3. Extension to the Unitary case,[0],[0]
The image of M̂1 spans the full set of unitary matrices U(n) and any point on its image is a unitary matrix.,4.3. Extension to the Unitary case,[0],[0]
"All RNN models were implemented using the python library theano (Theano Development Team, 2016).",5. Experiments,[0],[0]
"For efficiency, we implemented the one-step FP and BP algorithms described in Algorithm 1 using C code‡.",5. Experiments,[0],[0]
We tested the new parametrisation on five different datasets all having long-term dependencies.,5. Experiments,[0],[0]
We call our parametrised network oRNN (for orthogonal RNN).,5. Experiments,[0],[0]
"We set its activation function to the leaky_ReLU defined as φ(x) = max( x10 , x).",5. Experiments,[0],[0]
"To ensure that the transition matrix of the oRNN is always orthogonal, we set the scalar u1 to -1",5. Experiments,[0],[0]
if u1 ≤ 0 and 1 otherwise after each gradient update.,5. Experiments,[0],[0]
Note that the parameter matrixU in Equation (9) has all zeros in its upper triangular part.,5. Experiments,[0],[0]
"Therefore, after calculating the gradient of a loss with respect to U (i.e. ∂L∂U ), the values in the upper triangular part are set to zero.
",5. Experiments,[0],[0]
"For all experiments, we used the adam method for stochastic gradient descent (Kingma & Ba, 2014).",5. Experiments,[0],[0]
"We initialised all the parameters using uniform distributions similar to (Arjovsky et al., 2016).",5. Experiments,[0],[0]
"The biases of all models were set to zero, except for the forget bias of the LSTM, which we set to 5 to facilitate the learning of long-term dependencies (Koutnı́k et al., 2014).",5. Experiments,[0],[0]
"In this experiment, we followed a similar setting to (Koutnı́k et al., 2014) where we trained RNNs to encode song excerpts.",5.1. Sequence generation,[0],[0]
We used the track Manyrista from album Musica Deposita by Cuprum.,5.1. Sequence generation,[0],[0]
"We extracted five consecutive excerpts around the beginning of the song, each having 800 data points and corresponding to 18ms with a 44.1Hz sampling frequency.",5.1. Sequence generation,[0],[0]
"We trained an sRNN, LSTM, and oRNN for 5000 epochs on each of the pieces with five random seeds.",5.1. Sequence generation,[0],[0]
"For each run, the lowest Normalised Mean Squared Error (NMSE) during the 5000 epochs was recorded.",5.1. Sequence generation,[0],[0]
"For each model, we tested three different hidden sizes.",5.1. Sequence generation,[0],[0]
"The total number of parameters Np corresponding to these hidden sizes was approximately equal to 250, 500, and 1000.",5.1. Sequence generation,[0],[0]
"For the oRNN, we set the number of reflection vectors to the hidden size for each case, so that the transition matrix is allowed to span the full set of orthogonal
‡Our implementation can be found at https://github.",5.1. Sequence generation,[0],[0]
"com/zmhammedi/Orthogonal_RNN.
matrices.",5.1. Sequence generation,[0],[0]
The results are shown in Figures 2 and 3.,5.1. Sequence generation,[0],[0]
All the learning rates were set to 10−3.,5.1. Sequence generation,[0],[0]
The orthogonal parametrisation outperformed the sRNN and performed on average better than the LSTM.,5.1. Sequence generation,[0],[0]
"In this experiment, we followed a similar setting to (Arjovsky et al., 2016), where the goal of the RNN is to output the sum of two elements in the first dimension of a twodimensional sequence.",5.2. Addition Task,[0],[0]
The location of the two elements to be summed are specified by the entries in the second dimension of the input sequence.,5.2. Addition Task,[0],[0]
"In particular, the first dimension of every input sequence consists of random numbers between 0 and 1.",5.2. Addition Task,[0],[0]
The second dimension has all zeros except for two elements equal to 1.,5.2. Addition Task,[0],[0]
"The first unit entry
is located in the first half of the sequence, and the second one in the second half.",5.2. Addition Task,[0],[0]
"We tested two different sequence lengths T = 400, 800.",5.2. Addition Task,[0],[0]
All models were trained to minimise the Mean Squared Error (MSE).,5.2. Addition Task,[0],[0]
"The baseline MSE for this task is 0.167; for a model that always outputs one.
",5.2. Addition Task,[0],[0]
We trained an oRNN with n = 128 hidden units and m = 16 reflections.,5.2. Addition Task,[0],[0]
"We trained an LSTM and sRNN with hidden sizes 28 and 54, respectively, corresponding to a total number of parameters ≈ 3600 (i.e. same as the oRNN model).",5.2. Addition Task,[0],[0]
"We chose a batch size of 50, and after each iteration, a new set of sequences was generated randomly.",5.2. Addition Task,[0],[0]
The learning rate for the oRNN was set to 0.01.,5.2. Addition Task,[0],[0]
"Figure 4 displays the results for both lags.
",5.2. Addition Task,[0],[0]
The oRNN was able to beat the baseline MSE in less than 5000 iterations for both lags and for two different random initialisation seeds.,5.2. Addition Task,[0],[0]
"This is in line with the results of the unitary RNN (Arjovsky et al., 2016).",5.2. Addition Task,[0],[0]
"In this experiment, we used the MNIST image dataset.",5.3. Pixel MNIST,[0],[0]
"We split the dataset into training (55000 instances), validation (5000 instances), and test sets (10000 instances).",5.3. Pixel MNIST,[0],[0]
"We trained oRNNs with n ∈ {128, 256} andm ∈ {16, 32, 64}, where n and m are the number of hidden units and reflections vectors respectively, to minimise the cross-entropy error function.",5.3. Pixel MNIST,[0],[0]
"We experimented with (mini-batch size, learning rate) ∈ {(1, 10−4), (50, 10−3)}.
",5.3. Pixel MNIST,[0],[0]
Table 3 compares the test performance of our best model against results available in the literature for unitary/orthogonal RNNs.,5.3. Pixel MNIST,[0],[0]
"Despite having fewer total number of parameters, our model performed better than three out the four models selected for comparison (all having ≥ 16K parameters).",5.3. Pixel MNIST,[0],[0]
"Figure 5 shows the validation accuracy as a function of the number of epochs of our oRNN model in Table 3.
",5.3. Pixel MNIST,[0],[0]
Figure 6 shows the effect of varying the number of reflection vectors m on the performance.,5.3. Pixel MNIST,[0],[0]
"In this experiment, we tested the oRNN on the task of character level prediction using the Penn Tree Bank Corpus.",5.4. Penn Tree Bank,[0],[0]
"The data was split into training (5017K characters), validation (393K characters), and test sets (442K characters).",5.4. Penn Tree Bank,[0],[0]
The total number of unique characters in the corpus was 49.,5.4. Penn Tree Bank,[0],[0]
The vocabulary size was 10K and any other words were replaced by the special token <unk>.,5.4. Penn Tree Bank,[0],[0]
The number of characters per instance (i.e. char/line) in the training data ranged between 2 and 518 with an average of 118 char/line.,5.4. Penn Tree Bank,[0],[0]
"We trained an oRNN and LSTM with hidden units 512 and 183 respectively, corresponding to a total of ≈ 180K parameters, for 20 epochs.",5.4. Penn Tree Bank,[0],[0]
"We set the number of reflections to 510
for the oRNN.",5.4. Penn Tree Bank,[0],[0]
"The learning rate was set to 0.0001 for both models with a mini-batch size of 1.
",5.4. Penn Tree Bank,[0],[0]
"Similarly to (Pascanu et al., 2013) we considered two tasks: one where the model predicts one character ahead and the other where it predicts a character five steps ahead.",5.4. Penn Tree Bank,[0],[0]
It was suggested that solving the later task would require the learning of longer term correlations in the data rather than the shorter ones.,5.4. Penn Tree Bank,[0],[0]
Table 4 summarises the test results.,5.4. Penn Tree Bank,[0],[0]
The oRNN and LSTM performed similarly to each other on the one-step head prediction task.,5.4. Penn Tree Bank,[0],[0]
"Whereas on the five-step ahead prediction task, the LSTM was better.",5.4. Penn Tree Bank,[0],[0]
"The performance of both models on this task was close to the state of the art result for RNNs 3.74 bpc (Pascanu et al., 2013).
",5.4. Penn Tree Bank,[0],[0]
"Nevertheless, our oRNN still outperformed the results of (Vorontsov et al., 2017) which used both soft and hard orthogonality constraints on the transition matrix.",5.4. Penn Tree Bank,[0],[0]
Their RNN was trained on 99% of the data (sentences with≤ 300 characters) and had the same number of hidden units as the oRNN in our experiment.,5.4. Penn Tree Bank,[0],[0]
The lowest test cost achieved was 2.20(bpc) for the one-step-ahead prediction task.,5.4. Penn Tree Bank,[0],[0]
"We tested our model on the copy task described in details in (Gers et al., 2001; Arjovsky et al., 2016).",5.5. Copying task,[0],[0]
"Using an oRNN with the leaky_ReLU we were not able to reproduce the same performance as the uRNN (Arjovsky et al., 2016; Wisdom et al., 2016).",5.5. Copying task,[0],[0]
"However, we were able to achieve a comparable performance when using the OPLU activation function (Chernodub & Nowicki, 2016), which is a normpreserving activation function.",5.5. Copying task,[0],[0]
"In order to explore whether the poor performance of the oRNN was only due to the activation function, we tested the same activation as the uRNN
(i.e. the real representation of modReLU defined in Equation (4)) on the oRNN.",5.5. Copying task,[0],[0]
"This did not improve the performance compared to the leaky_ReLU case suggesting that the block structure of the uRNN transition matrix, when expressed in the real space (see Section 3), may confer special benefits in some cases.",5.5. Copying task,[0],[0]
"In this work, we presented a new parametrisation of the transition matrix of a recurrent neural network using Householder reflections.",6. Discussion,[0],[0]
This method allows an easy and computationally efficient way to enforce an orthogonal constraint on the transition matrix which then ensures that exploding gradients do not occur during training.,6. Discussion,[0],[0]
Our method could also be applied to other deep neural architectures to enforce orthogonality between hidden layers.,6. Discussion,[0],[0]
"Note that a “soft” orthogonal constraint could also be applied using our parametrisation by, for example, allowing u1 to vary continuously between -1 and 1.
",6. Discussion,[0],[0]
It is important to note that our method is particularly advantageous for stochastic gradient descent when the minibatch size is close to 1.,6. Discussion,[0],[0]
"In fact, if B is the mini-batch size and T is the average length of the input sequences, then a network with n hidden units trained using other methods (Vorontsov et al., 2017; Wisdom et al., 2016; Hyland & Rätsch, 2017) that enforce orthogonality (see Section 2), would have time complexityO(BTn2+n3).",6. Discussion,[0],[0]
"Clearly when BT n this becomes O(BTn2), which is the same time complexity as that of the sRNN and oRNN (with m = n).",6. Discussion,[0],[0]
"In contrast with the case of fully connected deep forward networks with no weight sharing between layers (# layer = L), the time complexity using our method is O(BLnm) whereas other methods discussed in this work (see Section 2) would have time complexity O(BLn2 + Ln3).",6. Discussion,[0],[0]
"The latter methods are less efficient in this case since B n is less likely to be the case compared with BT n when using SGD.
",6. Discussion,[0],[0]
"From a performance point of view, further experiments should be performed to better understand the difference between the unitary versus orthogonal constraint.",6. Discussion,[0],[0]
The authors would like to acknowledge Department of State Growth Tasmania for partially funding this work through SenseT. We would also like to thank Christfried Webers for his valuable feedback.,Acknowledgment,[0],[0]
Sketch of the proof for Theorem 1.,A. Proofs,[0],[0]
"We need to the show that for every Q̃ ∈ O(n), there exits a tuple of vectors (u1, . . .",A. Proofs,[0],[0]
",un) ∈ R × · · ·",A. Proofs,[0],[0]
"× Rn such that Q̃ = M1(u1, . . .",A. Proofs,[0],[0]
",un).",A. Proofs,[0],[0]
Algorithm 2 shows how a QR decomposition can be performed using the matrices {Hk(uk)}nk=1 while ensuring that the upper triangular matrix R has positive diagonal elements.,A. Proofs,[0],[0]
"If we apply this algorithm to an orthogonal matrix Q̃, we get a tuple (u1, . . .",A. Proofs,[0],[0]
",un) which satisfies
QR = Hn(un) . . .H1(u1)R",A. Proofs,[0],[0]
"= Q̃.
Note that the matrixRmust be orthogonal sinceR = Q′Q̃. Therefore, R = I , since the only upper triangular matrix with positive diagonal elements is the identity matrix.",A. Proofs,[0],[0]
"Hence, we have
M1(u1, . . .",A. Proofs,[0],[0]
",un) = Hn(un) . .",A. Proofs,[0],[0]
.H1(u1),A. Proofs,[0],[0]
"= Q̃.
",A. Proofs,[0],[0]
Algorithm 2 QR decomposition using the mappings {Hk}.,A. Proofs,[0],[0]
"For a matrix B ∈ Rn×n, {Bk,k}1≤k≤n denote its diagonal elements, and Bk..n,k = (Bk,k, . . .",A. Proofs,[0],[0]
", Bn,k)′ ∈ Rn−k+1.",A. Proofs,[0],[0]
Require: A ∈ Rn×n is a full-rank matrix.,A. Proofs,[0],[0]
Ensure: Q and R where Q = Hn(un) . .,A. Proofs,[0],[0]
.H1(u1),A. Proofs,[0],[0]
"and R
is upper triangular with positive diagonal elements such that A = QR R←",A. Proofs,[0],[0]
A Q←,A. Proofs,[0],[0]
"I {Initialise Q to the identity matrix} for k = 1 to n− 1 do
if Rk,k == ‖Rk..n,",A. Proofs,[0],[0]
"k‖ then un−k+1 = (0, . . .",A. Proofs,[0],[0]
", 0, 1)
′",A. Proofs,[0],[0]
"∈ Rn−k+1 else un−k+1 ← Rk..n,k − ‖Rk..n,k‖ (1, 0, . . .",A. Proofs,[0],[0]
", 0)′ un−k+1 ← un−k+1/ ‖un−k+1‖ end if R← Hn−k+1(un−k+1)R Q← QHn−k+1(un−k+1)
end for u1 = sgn(Rn,n) ∈ R R← H1(u1)R Q← QH1(u1)
",A. Proofs,[0],[0]
Lemma 1. (?),A. Proofs,[0],[0]
"Let A, B, and C be real or complex matrices, such that C = f(A,B) where f is some differentiable mapping.",A. Proofs,[0],[0]
"Let L be some scalar quantity which depends on C. Then we have the following identity
Tr(C ′ dC)",A. Proofs,[0],[0]
= Tr(A ′,A. Proofs,[0],[0]
dA),A. Proofs,[0],[0]
"+ Tr(B ′ dB),
where dA, dB, and dC represent infinitesimal perturbations and
C := ∂L ∂C , A :=
[ ∂C
∂A ]′",A. Proofs,[0],[0]
"∂L ∂C , B := [ ∂C ∂B ]′",A. Proofs,[0],[0]
"∂L ∂C .
",A. Proofs,[0],[0]
Proof of Theorem 2.,A. Proofs,[0],[0]
Let C = h,A. Proofs,[0],[0]
"− UT−1U ′h where (U, h) ∈ Rn×m×Rn and T = striu(U ′U) + 12diag(U
′U).",A. Proofs,[0],[0]
"Notice that the matrix T can be written using the Hadamard product as follows
T = B ◦ (U ′U), (16)
where B = striu(Jm) + 12Im and Jm is the m×m matrix of all ones.
",A. Proofs,[0],[0]
"Calculating the infinitesimal perturbations of C gives
dC =(I − UT−1U ′)dh − dUT−1U ′h− UT−1dU ′h + UT−1dTT−1U ′h.
",A. Proofs,[0],[0]
"Using Equation (16) we can write
dT = B ◦ (dU ′U + U ′dU).
",A. Proofs,[0],[0]
"By substituting this back into the expression of dC, multiplying the left and right-hand sides by C ′ , and applying the trace we get
Tr(C ′ dC) = Tr(C ′",A. Proofs,[0],[0]
"(I − UT−1U ′)dh)
",A. Proofs,[0],[0]
−,A. Proofs,[0],[0]
"Tr(C ′dUT−1U ′h)− Tr(C ′UT−1dU ′h)
",A. Proofs,[0],[0]
+,A. Proofs,[0],[0]
Tr(C ′,A. Proofs,[0],[0]
"UT−1(B ◦ (dU ′U + U ′dU))T−1U ′h).
",A. Proofs,[0],[0]
"Now using the identity Tr(AB) = Tr(BA), where the second dimension of A agrees with the first dimension of B, we can rearrange the expression of Tr(C ′ dC) as follows
Tr(C ′ dC)",A. Proofs,[0],[0]
= Tr(C ′,A. Proofs,[0],[0]
"(I − UT−1U ′)dh)
",A. Proofs,[0],[0]
"− Tr(T−1U ′hC ′dU)− Tr(hC ′UT−1dU ′)
+ Tr(T−1U ′hC ′",A. Proofs,[0],[0]
"UT−1(B ◦ (dU ′U + U ′dU))).
",A. Proofs,[0],[0]
"To simplify the expression, we will use the short notations
C̃ = (T ′)−1U ′C,
h̃ = T−1U ′h,
Tr(C ′ dC) becomes
Tr(C ′ dC) = Tr((C ′",A. Proofs,[0],[0]
"− C̃ ′U ′)dh)
",A. Proofs,[0],[0]
"− Tr(h̃C ′dU)− Tr(hC̃ ′dU ′) + Tr(h̃C̃ ′(B ◦ (dU ′U + U ′dU))).
",A. Proofs,[0],[0]
"Now using the two following identities of the trace
Tr(A′) = Tr(A), Tr(A(B ◦ C))",A. Proofs,[0],[0]
"= Tr((A ◦B′)C)),
we can rewrite Tr(C ′ dC)",A. Proofs,[0],[0]
"as follows
Tr(C ′ dC) =Tr((C ′",A. Proofs,[0],[0]
"− C̃ ′U ′)dh)
",A. Proofs,[0],[0]
"− Tr(h̃C ′dU)− Tr(hC̃ ′dU ′) + Tr((h̃C̃ ′ ◦B′)dU ′U) + Tr((h̃C̃ ′ ◦B′)U ′dU).
",A. Proofs,[0],[0]
"By rearranging and taking the transpose of the third and fourth term of the right-hand side we obtain
Tr(C ′ dC) =Tr((C ′",A. Proofs,[0],[0]
"− C̃ ′U ′)dh)
",A. Proofs,[0],[0]
"− Tr(h̃C ′dU)− Tr(C̃h′dU) + Tr(((C̃h̃′) ◦B)U ′dU) + Tr(((h̃C̃ ′) ◦B′)U ′dU).
",A. Proofs,[0],[0]
"Factorising by dU inside the Tr we get
Tr(C ′ dC) = Tr((C ′",A. Proofs,[0],[0]
"− C̃ ′U ′)dh)−
Tr((h̃C ′",A. Proofs,[0],[0]
+ C̃h′,A. Proofs,[0],[0]
− [ (C̃h̃′) ◦B + (h̃C̃ ′) ◦,A. Proofs,[0],[0]
B′ ],A. Proofs,[0],[0]
"U ′)dU).
",A. Proofs,[0],[0]
Using lemma 1 we conclude that U =U [ (h̃C̃ ′) ◦B′ + (C̃h̃′) ◦B ] − Ch̃′,A. Proofs,[0],[0]
"− hC̃ ′,
h =C − UC̃.
Sketch of the proof for Corollary 1.",A. Proofs,[0],[0]
"For any nonzero complex valued vector x ∈ Cn, if we chose u = x+ eiθ ‖x‖ e1 and H = −e−iθ(I − 2 uu ∗
‖u‖2 ), where θ ∈ R is such that x1 = e iθ|x1|, we have (?)
",A. Proofs,[0],[0]
"Hx = ||x||e1 (17)
Taking this fact into account, a similar argument to that used in the proof of Theorem 1 can be used here.",A. Proofs,[0],[0]
"Let U := (vi,j) 1≤j≤m 1≤i≤n .",B. Algorithm Explanation,[0],[0]
"Then the element of the matrix T := striu(U ′U) + 12diag(U ′U) can be expressed as
ti,j = Ji ≤ jK",B. Algorithm Explanation,[0],[0]
"∑n k=j vk,ivk,j
1 + δi,j ,
where δi,j is the Kronecker delta and J·K is the Iversion bracket (i.e. JpK = 1 if p is true and JpK",B. Algorithm Explanation,[0],[0]
= 0 otherwise).,B. Algorithm Explanation,[0],[0]
In order to compute the gradients in Equations (14) and (15).,B. Algorithm Explanation,[0],[0]
we first need to compute h̃ = T−1U ′h and C̃ = (T ′)−1U ′,B. Algorithm Explanation,[0],[0]
∂L∂C .,B. Algorithm Explanation,[0],[0]
This is equivalent to solving the triangular systems of equations T h̃ = U ′h and T ′C̃ = U ′,B. Algorithm Explanation,[0],[0]
"∂L∂C .
",B. Algorithm Explanation,[0],[0]
Solving the triangular system T h̃ = U ′h.,B. Algorithm Explanation,[0],[0]
"For 1 ≤ k ≤ m, we can express the k-th row of this system as
tk,kh̃k + m∑ j=k+1 tk,j h̃j = n∑ j=k vj,khj ,
= n∑ j=k vj,khj − m∑ j=k+1 n∑ r=j vr,kvr,j h̃j ,
= n∑ r=k vr,khr − n∑ r=k+1 vr,k r∑ j=k+1 vr,j h̃j , (18)
",B. Algorithm Explanation,[0],[0]
=,B. Algorithm Explanation,[0],[0]
"U ′∗,k(h−",B. Algorithm Explanation,[0],[0]
"m∑
j=k+1
U∗,j h̃j), (19)
",B. Algorithm Explanation,[0],[0]
"where the passage from Equation (18) to (19) is justified because vr,j = 0 for j",B. Algorithm Explanation,[0],[0]
> r.,B. Algorithm Explanation,[0],[0]
"Therefore, ∑r j=k+1 vr,j h̃j =∑m
j=k+1 vr,j h̃j .",B. Algorithm Explanation,[0],[0]
"By setting H∗,k+1 := h− ∑m j=k+1 U∗,j h̃j , and noting that tk,k = U ′∗,kU∗,k
2 , we get
h̃k = 2
U ′∗,kU∗,k U ′∗,kH∗,k+1, (20)
",B. Algorithm Explanation,[0],[0]
"H∗,k = H∗,k+1",B. Algorithm Explanation,[0],[0]
"− h̃kU∗,k. (21)
Equations (20) and (21) explain the lines 8 and 9 in Algorithm 1.",B. Algorithm Explanation,[0],[0]
"Note that H∗,1 = h",B. Algorithm Explanation,[0],[0]
"− ∑m j=1 U∗,j h̃j = h −∑m
j=1 U∗,j",B. Algorithm Explanation,[0],[0]
[T −1U ′h]j = h − UT−1U ′h = Wh.,B. Algorithm Explanation,[0],[0]
"Hence, when h = h(t−1), we have H∗,1 = C(t), which explains line 16 in Algorithm 1.
",B. Algorithm Explanation,[0],[0]
Solving the triangular system T ′C̃ = U ′ ∂L∂C .,B. Algorithm Explanation,[0],[0]
"Similarly to the previous case, we have for 1 ≤ k ≤ m
tk,kC̃k + k−1∑ j=1 tj,kC̃j = n∑ j=k vj,k",B. Algorithm Explanation,[0],[0]
"[ ∂L ∂C ] j ,
= n∑ j=1 vj,k",B. Algorithm Explanation,[0],[0]
[ ∂L ∂C ],B. Algorithm Explanation,[0],[0]
"j − k−1∑ j=1 n∑ r=k vr,jvr,kC̃j , (22)
= n∑ r=1",B. Algorithm Explanation,[0],[0]
"vr,k",B. Algorithm Explanation,[0],[0]
[ ∂L ∂C ],B. Algorithm Explanation,[0],[0]
"r − n∑ r=1 vr,k k−1∑ j=1 vr,jC̃j , (23)
",B. Algorithm Explanation,[0],[0]
"= U ′∗,k  ∂L ∂C",B. Algorithm Explanation,[0],[0]
"− k−1∑ j=1 U∗,jC̃j  ,
where the passage from Equation (22) to (23) is justified by the fact that ∑n r=k vr,jvr,kC̃j = ∑n r=1",B. Algorithm Explanation,[0],[0]
"vr,jvr,kC̃j (since vr,k = 0 for r < k).
",B. Algorithm Explanation,[0],[0]
By setting g := ∂L ∂C(t),B. Algorithm Explanation,[0],[0]
"− ∑k−1 j=1 U∗,jC̃j , we can write C̃k = 2
U ′∗,kU∗,k U ′∗,kg which explains the lines 12 and
13 in Algorithm 1.",B. Algorithm Explanation,[0],[0]
"Note also that after m-iterations in the backward propagation loop in Algorithm 1, we have g = ∂L ∂C(t)",B. Algorithm Explanation,[0],[0]
"− ∑m j=1 U∗,jC̃j = ∂L ∂C(t)",B. Algorithm Explanation,[0],[0]
"−UC̃ = ∂L ∂h(t−1)
.",B. Algorithm Explanation,[0],[0]
"This explains line 17 of Algorithm 1.
",B. Algorithm Explanation,[0],[0]
"Finally, note that from Equation (14), we have for 1 ≤ i ≤ n",B. Algorithm Explanation,[0],[0]
and 1 ≤ k ≤ m,B. Algorithm Explanation,[0],[0]
"[ ∂L ∂U ] i,k =− [ ∂L ∂C ]",B. Algorithm Explanation,[0],[0]
"i h̃k − hiC̃k+
m∑ j=1 vi,j ( ((h̃C̃ ′) ◦",B. Algorithm Explanation,[0],[0]
"B′)j,k + ((C̃h̃′) ◦B)j,k ) ,
=− [ ∂L ∂C ]",B. Algorithm Explanation,[0],[0]
"i h̃k − hiC̃k+
m∑ j=1 vi,j ( h̃jC̃k",B. Algorithm Explanation,[0],[0]
"Jk ≤ jK 1 + δj,k + C̃j h̃k",B. Algorithm Explanation,[0],[0]
"Jj ≤ kK 1 + δj,k ) ,
=− [ ∂L ∂C ]",B. Algorithm Explanation,[0],[0]
"i h̃k − hiC̃k+
m∑ j=1 vi,j ( h̃jC̃kJk < jK + C̃j h̃kJj ≤ kK ) ,
=C̃k  m∑ j=k+1 vi,j h̃j",B. Algorithm Explanation,[0],[0]
"− hi  + h̃k
 k∑ j=1 vi,jC̃j",B. Algorithm Explanation,[0],[0]
− [ ∂L ∂C ],B. Algorithm Explanation,[0],[0]
i  .,B. Algorithm Explanation,[0],[0]
"Therefore, when C = C(t) and h = h(t−1) we have[
∂L ∂U (t) ]",B. Algorithm Explanation,[0],[0]
"∗,k = −C̃kH∗,k+1 − h̃kg,
where g = ∂L ∂C(t)",B. Algorithm Explanation,[0],[0]
"− ∑k−1 j=1 C̃jU∗,j .",B. Algorithm Explanation,[0],[0]
This explains lines 14 and 18 of Algorithm 1.,B. Algorithm Explanation,[0],[0]
Table 5 shows the flop count for different operations in the local backward and forward propagation steps in Algorithm 1.,C. Time complexity,[0],[0]
"∂h(t−1) (variable g is the code), and
∂L ∂U(t)
(variable G is the code).",D. Matlab implementation of Algorithm 1,[0],[0]
"The required inputs for the FP and BP are, respectively, the tuples (U, h(t−1)) and (U,C(t), ∂L
∂C(t) ).
",D. Matlab implementation of Algorithm 1,[0],[0]
Note that ∂L ∂C(t) is variable BPg in the Matlab code.,D. Matlab implementation of Algorithm 1,[0],[0]
The problem of learning long-term dependencies in sequences using Recurrent Neural Networks (RNNs) is still a major challenge.,abstractText,[0],[0]
Recent methods have been suggested to solve this problem by constraining the transition matrix to be unitary during training which ensures that its norm is equal to one and prevents exploding gradients.,abstractText,[0],[0]
"These methods either have limited expressiveness or scale poorly with the size of the network when compared with the simple RNN case, especially when using stochastic gradient descent with a small mini-batch size.",abstractText,[0],[0]
Our contributions are as follows; we first show that constraining the transition matrix to be unitary is a special case of an orthogonal constraint.,abstractText,[0],[0]
Then we present a new parametrisation of the transition matrix which allows efficient training of an RNN while ensuring that the matrix is always orthogonal.,abstractText,[0],[0]
"Our results show that the orthogonal constraint on the transition matrix applied through our parametrisation gives similar benefits to the unitary constraint, without the time complexity limitations.",abstractText,[0],[0]
Efficient Orthogonal Parametrisation of Recurrent Neural Networks  Using Householder Reflections,title,[0],[0]
This paper considers strategies to learn parametric models for language modeling with very large vocabularies.,1. Introduction,[0],[0]
"This problem is key to natural language processing, with applications in machine translation (Schwenk et al., 2012; Sutskever et al., 2014; Vaswani et al., 2013) or automatic speech recognition (Graves et al., 2013; Hinton et al., 2012).",1. Introduction,[0],[0]
"In particular, Neural Network Language Models (NNLMs) have received a renewed interest in recent years, by achieving state of the art performance on standard benchmarks (Jozefowicz et al., 2016; Mikolov et al., 2010).",1. Introduction,[0],[0]
"These approaches are more computationally intensive but generalize better than traditional non-parametric models (Bahl et al., 1983; Kneser & Ney, 1995).
",1. Introduction,[0],[0]
"Statistical language models assign a probability to words given their history (Bahl et al., 1983).",1. Introduction,[0],[0]
"They are evaluated
1Facebook AI Research.",1. Introduction,[0],[0]
"Correspondence to: Édouard Grave <egrave@fb.com>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
by objective criteria such as perplexity (ppl), which directly measures the ability of the system to determine proper probabilities for all the words.",1. Introduction,[0],[0]
This potentially makes parametric models prohibitively slow to train on corpora with very large vocabulary.,1. Introduction,[0],[0]
"For instance, the vocabulary of the One Billion Word benchmark (Chelba et al., 2013) contains around 800K words.",1. Introduction,[0],[0]
"In standard NNLMs, such as feedforward networks (Bengio et al., 2003a) or recurrent networks (Mikolov et al., 2010), computing this probability over the whole vocabulary is the bottleneck.",1. Introduction,[0],[0]
"Many solutions have been proposed to reduce the complexity of this expensive step (Bengio et al., 2003b; Goodman, 2001a; Gutmann & Hyvärinen, 2010).",1. Introduction,[0],[0]
"We distinguish (i) the methods that consider the original distribution and aim at providing approximations of the probabilities, or of a subset of them (Bengio et al., 2003b; Ji et al., 2015), from (ii) the approaches that compute exact probabilities for an approximate model yielding a lower computational time, such as the popular hierarchical softmax (Goodman, 2001a; Mnih & Hinton, 2009; Morin & Bengio, 2005).
",1. Introduction,[0],[0]
"Our approach, called adaptive softmax, belongs to the second category.",1. Introduction,[0],[0]
"More specifically, it is inspired by the hierarchical softmax and its subsequent variants.",1. Introduction,[0],[0]
"In contrast to previous works and motivated by the trend that GPUs are comparatively more and more performant than CPUs, our design is oriented towards efficient processing on GPUs.",1. Introduction,[0],[0]
"In this context, our paper makes the following points:
• We define a strategy to produce an approximate hierarchical model.",1. Introduction,[0],[0]
"It departs from previous ones in that it explicitly takes into account the computation time of matrix-matrix multiplications on modern architectures, which is not trivially linear in the dimensions of the matrices.
",1. Introduction,[0],[0]
• We conduct an empirical analysis of this model on recent GPUs.,1. Introduction,[0],[0]
"This leads us to define a realistic computation time model that is incorporated in the proposed optimization;
• Our approach provides a significant acceleration factor compared to the regular softmax, i.e., 2× to 10× speed-ups.",1. Introduction,[0],[0]
Equivalently we improve the accuracy under computational constraints.,1. Introduction,[0],[0]
"Importantly, on the largest corpus, this higher efficiency empirically comes
ar X
iv :1
60 9.
04 30
9v 3
[ cs
.C",1. Introduction,[0],[0]
"L
] 1
9 Ju
n 20
17
at no cost in accuracy for a given amount of training data, in contrast to concurrent approaches improving the efficiency.
",1. Introduction,[0],[0]
This paper is organized as follows.,1. Introduction,[0],[0]
Section 2 briefly reviews the related work and Section 3 provides some background on the language modeling task that we consider.,1. Introduction,[0],[0]
"Section 4 describes our proposal, which is subsequently evaluated in Section 5 on typical benchmarks of the language modeling literature, including Text8, Europarl and One Billion Word datasets.",1. Introduction,[0],[0]
"Many methods have been proposed to approximate the softmax efficiently (Bengio et al., 2003b; Goodman, 2001a; Gutmann & Hyvärinen, 2010; Morin & Bengio, 2005).",2. Related work,[0],[0]
We briefly describe the most popular ones below and point the reader to Chen et al. (2015) for a comparative study.,2. Related work,[0],[0]
"For the sake of completeness, we refer the reader to other strategies that can speed-up the training of language models in complementary manners (Mikolov et al., 2011b).
",2. Related work,[0],[0]
Loss function approximation.,2. Related work,[0],[0]
The Hierarchical Softmax (HSM) is an approximation of the softmax function introduced by Goodman (2001a).,2. Related work,[0],[0]
"This approach is generally used with a two-level tree (Goodman, 2001a; Mikolov et al., 2011c) but has also been extended to deeper hierarchies (Morin & Bengio, 2005; Mnih & Hinton, 2009).",2. Related work,[0],[0]
"In general, the hierarchy structure is built on word similarities (Brown et al., 1992; Le et al., 2011; Mikolov et al., 2013) or frequency binning (Mikolov et al., 2011c).",2. Related work,[0],[0]
"In particular, Mikolov et al. (2013) proposes an optimal hierarchy by constructing a Huffman coding based on frequency.",2. Related work,[0],[0]
"However this coding scheme does not take into account the theoretical complexity reduction offered by matrix-matrix multiplication and distributed computation, in particular with modern GPUs.
",2. Related work,[0],[0]
"Similar to our work, Zweig & Makarychev (2013) constructs their hierarchy in order to explicitly reduce the computational complexity.",2. Related work,[0],[0]
They also solve the assignment problem with dynamic programming.,2. Related work,[0],[0]
"However, they only consider hierarchies where words are kept in the leaves of the tree, leading to a significant drop of performance (reported to be around 5 − 10%), forcing them to also optimize for word similarity.",2. Related work,[0],[0]
"In our case, allowing classes to be stored in the internal node of the tree leads to almost no drop of performance.",2. Related work,[0],[0]
"Also, they assume a linear computational time for the vector-matrix operation which significantly limits the use of their approach on distributed system such as GPU.
",2. Related work,[0],[0]
"The idea of keeping a short-list of the most frequent words has been explored before (Le et al., 2011; Schwenk, 2007).",2. Related work,[0],[0]
"In particular, Le et al. (2011) combines a short-list with
a hierachical softmax based on word representation.",2. Related work,[0],[0]
"In contrast, the word hierarchy that we introduce in Section 4 explicitly aims at reducing the complexity.
",2. Related work,[0],[0]
Our work also shares similarities with the d-softmax introduced by Chen et al. (2015).,2. Related work,[0],[0]
They assign capacity to words according to their frequency to speed up the training.,2. Related work,[0],[0]
Less frequent words have smaller classifiers than frequent ones.,2. Related work,[0],[0]
"Unlike our method, their formulation requires accessing the whole vocabulary to evaluate the probability of a word.
",2. Related work,[0],[0]
Sampling based approximation.,2. Related work,[0],[0]
"Sampling based approaches have been successfully applied to approximate the softmax function over large dictionaries in different domains, such as language modeling (Jozefowicz et al., 2016), machine translation (Jean et al., 2015) and computer vision (Joulin et al., 2015).",2. Related work,[0],[0]
"In particular, importance sampling (Bengio & Senécal, 2008; Bengio et al., 2003b) selects a subset of negative targets to approximate the softmax normalization.",2. Related work,[0],[0]
"Different schemes have been proposed for sampling, such as the unigram and bigram distribution (Bengio et al., 2003b) or more recently, a power-raised distribution of the unigram (Ji et al., 2015; Mikolov et al., 2013).",2. Related work,[0],[0]
"While this approach often leads to significant speed-up at train time, it still requires to evaluate the full softmax at test time.
",2. Related work,[0],[0]
Self-normalized approaches.,2. Related work,[0],[0]
"Self-normalized approaches aim at learning naturally normalized classifier, to avoid computing the softmax normalization.",2. Related work,[0],[0]
"Popular methods are Noise Contrastive Estimation (Gutmann & Hyvärinen, 2010; Mnih & Teh, 2012; Vaswani et al., 2013) or a penalization on the normalization function (Andreas & Klein, 2014; Devlin et al., 2014).",2. Related work,[0],[0]
"Noise Contrastive Estimation (Gutmann & Hyvärinen, 2010) replaces the softmax by a binary classifier distinguishing the original distribution form a noisy one.",2. Related work,[0],[0]
"While the original formulation still requires to compute the softmax normalization, Mnih & Teh (2012) shows that good performance can be achieved even without it.
",2. Related work,[0],[0]
"Finally, Vincent et al. (2015) have also proposed an efficient way to train model with high dimensional output space.",2. Related work,[0],[0]
"Their approach is exact and leads to a promising speed-up but it cannot be directly applied to the softmax function, limiting its potential application to language modeling.",2. Related work,[0],[0]
The goal of language modeling is to learn a probability distribution over a sequence of words from a given dictionary V .,3. Preliminaries on language modeling,[0],[0]
"The joint distribution is defined as a product of conditional distribution of tokens given their past (Bahl et al., 1983).",3. Preliminaries on language modeling,[0],[0]
"More precisely, the probability of a sequence of T
words w1, . . .",3. Preliminaries on language modeling,[0],[0]
", wT ∈ VT is given as
P (w1, . . .",3. Preliminaries on language modeling,[0],[0]
", wT ) = T∏ t=1",3. Preliminaries on language modeling,[0],[0]
"P (wt | wt−1, . .",3. Preliminaries on language modeling,[0],[0]
.,3. Preliminaries on language modeling,[0],[0]
", w1).",3. Preliminaries on language modeling,[0],[0]
"(1)
This problem is traditionally addressed with nonparameteric models based on counting statistics (Goodman, 2001b).",3. Preliminaries on language modeling,[0],[0]
"In particular, smoothed N-gram models (Bahl et al., 1983; Katz, 1987; Kneser & Ney, 1995) achieve good performance in practice (Mikolov et al., 2011a), especially when they are associated with cache models (Kuhn & De Mori, 1990).",3. Preliminaries on language modeling,[0],[0]
"More recently, parametric models based on neural networks have gained popularity for language modeling (Bengio et al., 2003a; Jozefowicz et al., 2016; Mikolov et al., 2010).",3. Preliminaries on language modeling,[0],[0]
"They are mostly either feedforward networks (Bengio et al., 2003a) or recurrent networks (Mikolov et al., 2010).",3. Preliminaries on language modeling,[0],[0]
"In a standard feedforward network for language modeling, we fix a window of length N and predict the next words according to the words appearing in this window.",3.1. Feedforward network.,[0],[0]
"In the simplest case, this probability is represented by a 2-layer neural network acting on an input xt ∈ VN , defined as the concatenation of the one-hot representation of the N previous words, wt−N+1, . . .",3.1. Feedforward network.,[0],[0]
", wt.",3.1. Feedforward network.,[0],[0]
"The state ht of the hidden layer and subsequently the vector of scores yt associated with the next token wt+1 are computed as
ht = σ(APxt), (2) yt = f(Bht), (3)
where σ is a non linearity, e.g., the pointwise sigmoid function σ(z) = 1/(1+exp(−z)), and f is the softmax function discussed in section 3.3.",3.1. Feedforward network.,[0],[0]
"This model is parameterized by the weight matrices P , A and B and is routinely learned with an optimization scheme such as stochastic gradient descent or Adagrad (Duchi et al., 2011).",3.1. Feedforward network.,[0],[0]
"A Recurrent network (Elman, 1990) extends a feedforward network in that the current state of the hidden layer also depends on its previous state.",3.2. Recurrent network.,[0],[0]
"The hidden state ht is updated according to the equation
ht = σ(Awt +Rht−1),
where R is a weight matrix and xt is the one-hot representation of the current word wt.",3.2. Recurrent network.,[0],[0]
"Computing the exact gradient for this model is challenging but it is possible to compute an efficient and stable approximation of it, using a truncated back-propagation through time (Werbos, 1990; Williams & Peng, 1990) and norm clipping (Mikolov et al., 2010).
",3.2. Recurrent network.,[0],[0]
"Since the model introduced by Elman (1990), many extensions have been proposed, such as Longer Short Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997), Gated recurrent units (Chung et al., 2014) or structurally constrained network (Mikolov et al., 2014).",3.2. Recurrent network.,[0],[0]
"These models have been successfully used in the context of language modeling (Jozefowicz et al., 2016; Mikolov et al., 2010; Mikolov & Zweig, 2012).",3.2. Recurrent network.,[0],[0]
"In this work, we focus on the standard word level LSTM architecture since it has obtained state of the art performance on the challenging One Billion Word Benchmark (Jozefowicz et al., 2016).",3.2. Recurrent network.,[0],[0]
"In neural language modeling, predicting the probability of the next word requires computing scores for every word in the vocabulary and to normalize them to form a probability distribution.",3.3. Class-based hierarchical softmax.,[0],[0]
"This is typically achieved by applying a softmax function to the unnormalized score zw associated with each word w, where the softmax function is defined as
f(zw) = exp(zw)∑
w′∈V exp(zw′) .",3.3. Class-based hierarchical softmax.,[0],[0]
"(4)
For a vocabulary comprising k = |V| words, this function requires O(k) operations once the scores are computed.",3.3. Class-based hierarchical softmax.,[0],[0]
"In the case of neural networks, the overall complexity isO(dk), where d is the size of the last hidden layer.",3.3. Class-based hierarchical softmax.,[0],[0]
"When the vocabulary is large, this step is computationally expensive and often dominates the computation of the whole model (Jozefowicz et al., 2016; Mikolov et al., 2014), as discussed in introduction and related work.",3.3. Class-based hierarchical softmax.,[0],[0]
"A simple approach (Goodman, 2001a) to reduce this computational cost is to assign each word w of the vocabulary to a unique class C(w) and to factorize the probability distribution over words as
p(wt | ht) = p1(C(wt)",3.3. Class-based hierarchical softmax.,[0],[0]
"| ht)× p2(wt | C(wt), ht),
where p1 and p2 are obtained using the softmax function (Eq. 4).",3.3. Class-based hierarchical softmax.,[0],[0]
"If each class contains √ k words, the computational cost is reduced from O(dk) to O(d √ k).",3.3. Class-based hierarchical softmax.,[0],[0]
"In this section, we propose the adaptive softmax, a simple speedup technique for the computation of probability distributions over words.",4. Our approach: the adaptive softmax,[0],[0]
"The adaptive softmax is inspired by the class-based hierarchical softmax, where the word classes are built to minimize the computation time.",4. Our approach: the adaptive softmax,[0],[0]
"Our method is designed to be efficient for GPUs, which are commonly used to train neural networks.",4. Our approach: the adaptive softmax,[0],[0]
"For the sake of clarity, we first present the intuition behind our method in the simple case where we simply split our dictionary in two distinct clusters, before analyzing a more general case.",4. Our approach: the adaptive softmax,[0],[0]
"The bottleneck of the model described in the previous section is the matrix multiplication between the matrix representing the hidden states (of size B × d, where B denotes the batch size), and the matrix of word representations, of size d × k.",4.1. Computation time model of matrix-multiplication,[0],[0]
"For a fixed size d of the hidden layer, we denote by g(k,B) the computation time of this multiplication (using an efficient implementation such as cuBLAS), and simplify the notation wherever some parameters are fixed.",4.1. Computation time model of matrix-multiplication,[0],[0]
"Figure 1 reports empirical timings as a function of k for typical parameters of B and d for two GPU models, namely K40 and M40.",4.1. Computation time model of matrix-multiplication,[0],[0]
"We observe that the computation time g(k) is constant for low values of k, until a certain inflection point k0 ≈ 50, and then becomes affine for values k > k0.",4.1. Computation time model of matrix-multiplication,[0],[0]
"This suggests a computational model of the form
g(k) = max(c+ λk0, c+ λk) (5) = cm +max [ 0, λ(k",4.1. Computation time model of matrix-multiplication,[0],[0]
− k0) ] .,4.1. Computation time model of matrix-multiplication,[0],[0]
"(6)
Empirically, cm = 0.40ms on a K40 and 0.22 ms on a M40.",4.1. Computation time model of matrix-multiplication,[0],[0]
"We observe the same behavior when measuring the timings as a function of the batch size B, i.e., it is inefficient to matrix-multiplication when one of the dimensions is small.",4.1. Computation time model of matrix-multiplication,[0],[0]
"This observation suggests that hierarchical organizations of words with a low number of children per node, such as binary Huffman codes, are highly suboptimal.",4.1. Computation time model of matrix-multiplication,[0],[0]
"Similarly, clusters comprising only rare words have a low probabilty p and a shrinking batch size of p B, which also lead to iniffient matrix-multiplication.",4.1. Computation time model of matrix-multiplication,[0],[0]
"In the following, we propose to use the following model of computation time for matrixmultiplication
g(k,B) =",4.1. Computation time model of matrix-multiplication,[0],[0]
"max(c+ λk0B0, c+ λkB).",4.1. Computation time model of matrix-multiplication,[0],[0]
"(7)
While this is a very crude model of computation, it allows to explain empirical observations well.",4.1. Computation time model of matrix-multiplication,[0],[0]
"In natural languages, the distribution of the words notoriously follows a Zipf law (Zipf, 1949).",4.2. Intuition: the two-clusters case,[0],[0]
"Most of the probability mass is covered by a small fraction of the dictionary, e.g., 87% of the document is covered by only 20% of the vocabulary in the Penn TreeBank.",4.2. Intuition: the two-clusters case,[0],[0]
"Similar to the frequency binning hierarchical softmax (Mikolov et al., 2011c), this information can be exploited to reduce the computation time.
",4.2. Intuition: the two-clusters case,[0],[0]
"A simple strategy to reduce the overall computation time is to partition the dictionary V into two clusters as Vh and Vt, where Vh denotes the head of the distribution consisting of the most frequent words, and where Vt is the tail associated with a large number of rare words.",4.2. Intuition: the two-clusters case,[0],[0]
"The classifier frequently accesses the head, which motivates the fact that it should be computed efficiently.",4.2. Intuition: the two-clusters case,[0],[0]
"In contrast, the tail occurs less frequently and the corresponding computation can be slower.",4.2. Intuition: the two-clusters case,[0],[0]
"This suggests defining clusters with unbalanced cardinalities |Vh| |Vt| and probabilities P (Vh) P (Vt), where P (A) = ∑ w∈A pi is the probability of a word to occur in the set Vi.",4.2. Intuition: the two-clusters case,[0],[0]
"For instance, one may define the head would only contain 20% of the vocabulary (covering for 87% on PennTree Bank).",4.2. Intuition: the two-clusters case,[0],[0]
"These two clusters can be organized in two different ways: either they are both leaves of a 2-level tree (Mikolov et al., 2011c), or the head cluster is kept as a short-list in the root node (Le et al., 2011).
",4.2. Intuition: the two-clusters case,[0],[0]
Compromising between efficiency and accuracy.,4.2. Intuition: the two-clusters case,[0],[0]
"We observe empirically that putting all the clusters in the leaves of the tree leads to a significant drop of performance (around 5− 10% performance drop, Mikolov et al., 2011c; Zweig & Makarychev, 2013).",4.2. Intuition: the two-clusters case,[0],[0]
"The reason is that the probability of every word w belonging to a cluster c is multiplied by the probability of its class, i.e., it is equal to P (c | h)P",4.2. Intuition: the two-clusters case,[0],[0]
"(w | c, h), while attaching a frequent word directly to the root associates it directly to the probability P (w | h) making its inference sharper.",4.2. Intuition: the two-clusters case,[0],[0]
"For this reason, unless there is a significant difference in computation time, we favor using a short-list, over the standard 2-level hierarchical softmax.
",4.2. Intuition: the two-clusters case,[0],[0]
Minimizing the computation time.,4.2. Intuition: the two-clusters case,[0],[0]
"Given a vocabulary of k words, we are looking for the number kh = |Vh| of words from the head of the distribution to be assigned to the first cluster.",4.2. Intuition: the two-clusters case,[0],[0]
These words will cover for ph of the distribution.,4.2. Intuition: the two-clusters case,[0],[0]
"The tail cluster will then contain the rest of the vocabulary, made of kt = k − kh words and covering for pt = 1 − ph of the overall distribution.",4.2. Intuition: the two-clusters case,[0],[0]
"The computation time corresponding to the matrix multiplication of the root is equal to g(kh +1, B), while the computation time for the tail of the distribution is equal to g(kt, ptB), where B is the
batch size.",4.2. Intuition: the two-clusters case,[0],[0]
"We thus obtain the overall computation time
C = g(kh + 1, B) + g(kt, ptB).
",4.2. Intuition: the two-clusters case,[0],[0]
"We can then find the size of the head cluster kh which minimizes the computation time C. We plot the value of C as a function of kh in Figure 2, for the word distribution of the Bulgarian Europarl dataset.",4.2. Intuition: the two-clusters case,[0],[0]
We observe that the optimal splitting between head and tail gives a 5× speedup over the full softmax.,4.2. Intuition: the two-clusters case,[0],[0]
"Another important observation is the fact that the optimal size of the head cluster does not correspond to two clusters with equal probability.
",4.2. Intuition: the two-clusters case,[0],[0]
Adapting the classifier capacity for each cluster.,4.2. Intuition: the two-clusters case,[0],[0]
"Each cluster is accessed independently of each other, they thus do not need to have the same capacity.",4.2. Intuition: the two-clusters case,[0],[0]
Frequent words need high capacity to be predicted correctly.,4.2. Intuition: the two-clusters case,[0],[0]
"In contrast, rare words cannot be learned very well, since we only see them a few times.",4.2. Intuition: the two-clusters case,[0],[0]
It would then be wasteful to associate them with high capacity.,4.2. Intuition: the two-clusters case,[0],[0]
"Like in Chen et al. (2015), we exploit this observation to further reduce the computational time of our classifier.",4.2. Intuition: the two-clusters case,[0],[0]
"Unlike Chen et al. (2015), we share the state of hidden layer across clusters and simply reduce the input size of the classifiers by applying a projection matrix.",4.2. Intuition: the two-clusters case,[0],[0]
"Typically, the projection matrix for the tail cluster reduces the size from d to dt = d/4.",4.2. Intuition: the two-clusters case,[0],[0]
Let us now consider the more general case where the dictionary is partitioned as V = Vh ∪ V1 . .,4.3. General case,[0],[0]
".VJ , Vi ∩",4.3. General case,[0],[0]
Vj = ∅ if i 6=,4.3. General case,[0],[0]
"j. We consider the hierarchical model depicted in Figure 3, where the sub-dictionary Vh is accessed at the first level, and the others in the second level.",4.3. General case,[0],[0]
"We now investigate
the computational cost C of the forward (equivalently, backward) pass of this approximate softmax layer.",4.3. General case,[0],[0]
"For the time being, we fix the batch size B and the dimensionality d of the hidden layer, in order to analyze the computation time as a function of the sub-dictionary sizes and probabilities.",4.3. General case,[0],[0]
We denote by pi = ∑ w∈Vi p(w),4.3. General case,[0],[0]
"the probability P (w ∈ Vi) and ki = |Vi| the cardinality of each cluster.
",4.3. General case,[0],[0]
"The expected computational cost C is decomposed as C = Ch + ∑ i Ci, where
Ch = g(J +",4.3. General case,[0],[0]
"kh, B)
and ∀i, Ci = g(ki, pi B),
leading to
C = g(J + kh, B) +",4.3. General case,[0],[0]
"∑ i g(ki, piB).",4.3. General case,[0],[0]
"(8)
We add the constraint kB ≥ k0B0 to ensure that there is no penalty induced by the constant part of the computational model of Equation 7, the previous equation simplifies as
C = c+ λ(J + kh)B + ∑ i (c+ λkipiB) (9)
",4.3. General case,[0],[0]
=,4.3. General case,[0],[0]
(J + 1)c+ λB,4.3. General case,[0],[0]
[ J + kh,4.3. General case,[0],[0]
+ ∑,4.3. General case,[0],[0]
i pi ki ] .,4.3. General case,[0],[0]
"(10)
Let us discuss this equation, by first considering that the cardinalities of the sub-vocabularies are fixed.",4.3. General case,[0],[0]
The right-most term is the only one that depends on the word probabilities.,4.3. General case,[0],[0]
"For two distinct clusters Vi and Vj , we can re-write pjkj as (pi+j − pi)kj , where pi+j = pi + pj , so that
piki + pjkj = pi(ki − kj) + pi+jkj .",4.3. General case,[0],[0]
"(11)
",4.3. General case,[0],[0]
"Without loss of generality, we assume that ki > kj .",4.3. General case,[0],[0]
"The quantities pi+j , ki and kj being fixed, the second term of the right-hand side of this equation is constant, and the best strategy is trivially to minimize the probability of the largest cluster Vi.",4.3. General case,[0],[0]
"In other terms, an optimal solution for Equation 10 requires that the most frequent words are assigned to the smallest cluster.",4.3. General case,[0],[0]
"This remark is true for any tuple (i, j), and we easily see that this point also holds for the head cluster.",4.3. General case,[0],[0]
"As a consequence, for a fixed number of clusters of given sizes, the best strategy is to assign the words by decreasing probabilities to clusters of increasing size.",4.3. General case,[0],[0]
"Note, this analysis remains valid as long as the g is monotonically increasing in k.
Determining ki with J fixed: dynamic programming.",4.3. General case,[0],[0]
We now assume that the number of clusters is fixed.,4.3. General case,[0],[0]
"Following our analysis above, the optimization solely depends on the cardinalities ki for all clusters, which perfectly determines how to split the list of words ordered by frequency.",4.3. General case,[0],[0]
"We solve this problem by dynamic programming.
",4.3. General case,[0],[0]
Finding the number of clusters.,4.3. General case,[0],[0]
"The only remaining free variable in our optimization is J , since the other parameters are then determined by the aforementioned optimizations.",4.3. General case,[0],[0]
"We plot in Figure 4 the optimal computation time, as a function of the number of clusters J , according to our model.",4.3. General case,[0],[0]
"We observe that a small number of clusters, between 10 and 15 gives the best computation time.",4.3. General case,[0],[0]
"Moreover, we observe that using more than 5 clusters does not lead to significant gains in computational time (a couple of milliseconds at best).",4.3. General case,[0],[0]
"In practice, we thus decide to use a small number of clusters (between 2 and 5), as it usually lead to slightly better perplexity, and we empirically determine the best speed/perplexity compromise on training data.",4.3. General case,[0],[0]
"As shown later by our experiments, using a small number of clusters allows to obtain comparable perplexity as the exact softmax on large corpora.",4.3. General case,[0],[0]
"This section provides a set of experiments aiming at analyzing the trade-off between actual computation time and effectiveness of several strategies, in particular the approach presented in the previous section.",5. Experiments,[0],[0]
"First we describe our evaluation protocol, then we evaluate some of the properties of our model and finally we compare it on standard benchmark against standard baselines.
Datasets.",5. Experiments,[0],[0]
"We evaluate our method on standard datasets, and use the perplexity (ppl) as an evaluation metric, as the function of the training time or of the number of training data (epochs).",5. Experiments,[0],[0]
"The datasets have varying vocabulary sizes, in different languages, which allows us to better understand the strengths and weaknesses of the different approaches.
",5. Experiments,[0],[0]
• Text81 is a standard compression dataset containing a pre-processed version of the first 100 million characters from Wikipedia in English.,5. Experiments,[0],[0]
"It has been recently used for language modeling (Mikolov et al., 2014) and has a vocabulary of 44k words.
",5. Experiments,[0],[0]
"• Europarl2 is a machine translation corpus, containing 20 languages (Koehn, 2005).",5. Experiments,[0],[0]
"For most languages, there are 10M–60M tokens and the vocabulary is in between 44k and 250k words.
",5. Experiments,[0],[0]
• One Billion Word 3 is a massive corpus introduced by Chelba et al. (2013).,5. Experiments,[0],[0]
"It contains 0.8B tokens and a vocabulary comprising almost 800k words.
",5. Experiments,[0],[0]
Implementation details.,5. Experiments,[0],[0]
We use an LSTM with one layer in all our experiments.,5. Experiments,[0],[0]
"On Text8 and Europarl, the models
1http://mattmahoney.net/dc/textdata 2http://www.statmt.org/europarl/ 3https://code.google.com/archive/p/1-billion-word-language-
modeling-benchmark/
have d = 512 hidden units and are regularized with weight decay (λ = 10−6).",5. Experiments,[0],[0]
"On the One Billion Word benchmark, we use d = 2048 hidden units and no regularization.",5. Experiments,[0],[0]
"The dimension of the input word embeddings is set to 256, so that large models fit in GPU memory.",5. Experiments,[0],[0]
"For the backpropagation through time, we unroll the models for 20 steps.",5. Experiments,[0],[0]
"We use Adagrad (Duchi et al., 2011), with a step size of 0.1 and 5 epochs, and we clip the norm of the gradients to 1.",5. Experiments,[0],[0]
"The batch size B is set to 128, except on the Finnish portion of Europarl where B=64 due to memory constraints.",5. Experiments,[0],[0]
"All the experiments were run on the same GPU with the Maxwell architecture.
",5. Experiments,[0],[0]
Baselines.,5. Experiments,[0],[0]
"Our method is compared to: (1) the full softmax, (2) the hierarchical softmax with frequency binning (HSM freq) and similarity-based binning (HSM sim), (3) importance sampling (Bengio et al., 2003b; Bengio & Senécal, 2008) and (4) the differentiated softmax (Chen et al., 2015).",5. Experiments,[0],[0]
"For HSM, we tried different strategies for the binning.",5. Experiments,[0],[0]
We observe that using the square root function on the count before computing the word bins is the most efficient for frequency binning.,5. Experiments,[0],[0]
"For the similarity-based binning, we used the Brown clustering algorithm (Brown et al., 1992) to determine the word classes.",5. Experiments,[0],[0]
"For the negative sampling method, we used a number of samples equal to 20% of the
size of the vocabulary (Chen et al., 2015).",5. Experiments,[0],[0]
"For the differentiated softmax (D-softmax), we used the same partitions for the vocabulary as for our approach.",5. Experiments,[0],[0]
We tried two version of the differentiated softmax.,5. Experiments,[0],[0]
"The first is the one described by Chen et al. (2015), where each word cluster uses a disjoint subset of the hidden representation.",5. Experiments,[0],[0]
"We also present an improved version, referred to as D-softmax [*], which uses our choice to have the whole hidden representation mapped to the different word clusters using projection matrices of different sizes.
",5. Experiments,[0],[0]
Comparison with the state of the art.,5. Experiments,[0],[0]
Table 1 reports the results that we achieve on Text8.,5. Experiments,[0],[0]
"On this small vocabulary, approximate methods are comparatively less interesting.",5. Experiments,[0],[0]
"Our approach is the only one to approach the result of the full soft-max (below by 3 points of perplexity), while being the fastest.",5. Experiments,[0],[0]
"Our improved variant D-softmax [*] of the work by Chen et al. (2015) obtains similar results but is slower by a factor ×1.8.
",5. Experiments,[0],[0]
"On Europarl, we first present the convergence properties of our approach compared to other approximate strategies in Figure 5 show the perplexity (ppl) as a function of training time.",5. Experiments,[0],[0]
Our approach significantly outperforms all competitors by a large margin.,5. Experiments,[0],[0]
"For reference, we also show the
performance (D-softmax [*]) obtained by improving the Dsoftmax, to make it more comparable to our method.",5. Experiments,[0],[0]
"Our method is 2× to 3× faster than this improved competitor, which demonstrates how critical is our optimization strategy.",5. Experiments,[0],[0]
"Similar conclusions are drawn from Table 3 for other languages from the Europal corpus.
",5. Experiments,[0],[0]
Table 2 gives the test perplexity on One Billion Word benchmark:,5. Experiments,[0],[0]
"Our method achieves a perplexity of 43.9 after five epochs, taking less than three days to train on a single GPU.",5. Experiments,[0],[0]
"In comparison, only Jozefowicz et al. (2016) achieves a lower perplexity, but with a model 8× bigger than ours and trained over 32 GPUs during 3 weeks.",5. Experiments,[0],[0]
"We also note that for models of similar size, we achieve similar perplexity than the method introduced by Jozefowicz et al. (2016).",5. Experiments,[0],[0]
"As far as we know, ours the first method to achieve a perplexity lower than 50 on a single GPU.",5. Experiments,[0],[0]
"In this paper, we have proposed a simple yet efficient approximation of the softmax classifier.",6. Conclusion,[0],[0]
"To our knowledge, it is the first speed optimizing approximation that obtains performance on par with the exact model.",6. Conclusion,[0],[0]
"This is achieved by explicitly taking into account the computation time of matrix-multiplication on parallel systems and combining it with a few important observations, namely keeping a shortlist of frequent words in the root node (Schwenk, 2007) and reducing the capacity of rare words (Chen et al., 2015).",6. Conclusion,[0],[0]
"In all our experiments on GPU, our method consistently maintains a low perplexity while enjoying a speed-up going from 2× to 10× compared to the exact model.",6. Conclusion,[0],[0]
"This type of speed-up allows to deal with extremely large corpora in
reasonable time and without the need of a large number of GPUs.",6. Conclusion,[0],[0]
"We believe our approach to be general enough to be applied to other parallel computing architectures and other losses, as well as to other domains where the distributions of the class are unbalanced.",6. Conclusion,[0],[0]
"The authors would like to thank Jeff Johnson for his help with GPU benchmarking as well as Tomas Mikolov, Rob Fergus and Jeff Johnson for insightful discussions.",Acknowledgements,[0],[0]
We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies.,abstractText,[0],[0]
"Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time.",abstractText,[0],[0]
"Our approach further reduces the computational time by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units.",abstractText,[0],[0]
"Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax.",abstractText,[0],[0]
The code of our method is available at https://github.com/ facebookresearch/adaptive-softmax.,abstractText,[0],[0]
Efficient softmax approximation for GPUs,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1844–1853 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Entity Linking (EL) is the task of mapping mentions of an entity in text to the corresponding entity in Knowledge Graph (KG) (Hoffart et al., 2011; Dong et al., 2014; Chisholm and Hachey, 2015).",1 Introduction,[0],[0]
"EL systems primarily exploit two types of information: (1) similarity of the mention to the candidate entity string, and (2) coherence between the candidate entity and other entities mentioned in the vicinity of the mention in text.",1 Introduction,[0],[0]
"Coherence essentially measures how well the candidate entity is connected, either directly or indirectly, with other KG entities mentioned in the vicinity (Milne and Witten, 2008; Globerson et al., 2016).",1 Introduction,[0],[0]
"In the state-
of-the-art EL system by (Yamada et al., 2016), coherence is measured as distance between embeddings of entities.",1 Introduction,[0],[0]
"This system performs well on entities which are densely-connected in KG, but not so well on sparsely-connected entities in the KG.
",1 Introduction,[0],[0]
We demonstrate this problem using the example sentence in Figure 1.,1 Introduction,[0],[0]
This sentence has two mentions: Andrei Broder and WWW.,1 Introduction,[0],[0]
"The figure also shows mention-entity linkages, i.e., mentions and their candidate entities in KG.",1 Introduction,[0],[0]
"Using a conventional EL system, the first mention Andrei Broder1 can be easily linked to Andrei Broder using string similarity between the mention and candidate entity strings.",1 Introduction,[0],[0]
String similarity works well in this case as this mention is unambiguous in the given setting.,1 Introduction,[0],[0]
"However, the second mention
1We use italics to denote textual mentions and typewriter to indicate an entity in KG.
1844
WWW has two candidates, World Wide Web and WWW conference, and hence is ambiguous.",1 Introduction,[0],[0]
"In such cases, coherence measure between the candidate entity and other unambiguously linked entity(ies) is used for disambiguation.
",1 Introduction,[0],[0]
State-of-the-art EL systems measure coherence as similarity between embeddings of entities.,1 Introduction,[0],[0]
The entity embeddings are trained based on the number of common edges in KG2.,1 Introduction,[0],[0]
"In our example, common edges are edges World Wide Web shares with Andrei Broder and edges WWW conference shares with Andrei Broder.",1 Introduction,[0],[0]
But WWW conference has less number of edges (it is a sparsely-connected entity) compared to World Wide Web.,1 Introduction,[0],[0]
"This leads to poor performance3 whereby WWW is erroneously linked to World Wide Web instead of linking to WWW conference.
",1 Introduction,[0],[0]
"In this paper, we propose ELDEN, an EL system which increases nodes and edges of the KG by using information available on the web about entities and pseudo entities.",1 Introduction,[0],[0]
"Pseudo Entities are words and phrases that frequently occur in Wikipedia, and co-occur with mentions of KG entities in the web corpus.",1 Introduction,[0],[0]
"Thus ELDEN uses a web corpus to find pseudo entities and refines the cooccurrences with Pointwise Mutual Information (PMI) (Church and Hanks, 1989) measure.",1 Introduction,[0],[0]
ELDEN then adds edges to the entity from pseudo entities.,1 Introduction,[0],[0]
"In Figure 1, pseudo entity Program Committee co-occurs with mentions of Andrei Broder and WWW conference in web corpus and has a positive PMI value with both.",1 Introduction,[0],[0]
"So ELDEN adds edges from Program Committee to Andrei Broder and WWW conference, densifying neighborhood of the entities.",1 Introduction,[0],[0]
"Coherence, now measured as similarity between entity embeddings where embeddings are trained on densified KG, leads to improved EL performance.
",1 Introduction,[0],[0]
Density (number of KG edges) of candidate entity affects EL performance.,1 Introduction,[0],[0]
"In our analysis of density and number of entities having that density in the Wikipedia KG, we find that entities with 500 edges or less make up more than 90%.",1 Introduction,[0],[0]
"Thus, creating an EL system that performs well on densely as well as sparsely-connected entities is a challenging, yet unavoidable problem.
",1 Introduction,[0],[0]
"2Wikipedia Link based Measure (WLM) (Milne and Witten, 2008) used in Yamada et al.’s system is based on number of common edges in KG.
3This paper focuses on mention disambiguation.",1 Introduction,[0],[0]
"We assume mention and candidate entities are detected already.
",1 Introduction,[0],[0]
"We make the following contributions:
• ELDEN presents a simple yet effective graph densification method which may be applied to improve EL involving any KG.
",1 Introduction,[0],[0]
•,1 Introduction,[0],[0]
"By using pseudo entities and unambiguous mentions of entity in a corpus, we demonstrate how non-entity-linked corpus can be used to improve EL performance.
",1 Introduction,[0],[0]
• We have made ELDEN’s code and data publicly available4.,1 Introduction,[0],[0]
"Entity linking: Most EL systems use coherence among entities (Cheng and Roth, 2013) to link mentions.",2 Related Work,[0],[0]
"We studied coherence measures and datasets used in six recent5 EL systems (He et al., 2013; Huang et al., 2015; Sun et al., 2015; Yamada et al., 2016; Globerson et al., 2016; Barrena et al., 2016).",2 Related Work,[0],[0]
"We see that the two popular datasets used for evaluating EL (Chisholm and Hachey, 2015) on documents are CoNLL (Hoffart et al., 2011) and TAC2010 (Ji et al., 2010), here after TAC.",2 Related Work,[0],[0]
"The popular coherence measures used are (1) WLM, (2) Entity Embedding Similarity and (3) Jaccard Similarity (Chisholm and Hachey, 2015; Guo et al., 2013).",2 Related Work,[0],[0]
"WLM is widely acknowledged as most popular (Hoffart et al., 2012) with almost all the six approaches analyzed above using WLM or its variants.",2 Related Work,[0],[0]
"Entity embedding similarity (Yamada et al., 2016) is reported to give highest EL6 performance and is the baseline of ELDEN.",2 Related Work,[0],[0]
"Enhancing entity disambiguation: Among methods proposed in literature to enhance entity disambiguation utilizing KG (Bhattacharya and Getoor) uses additional relational information between database references; (Han and Zhao, 2010) uses semantic relatedness between entities in other KGs; and (Shen et al., 2018) uses paths consisting of defined relations between entities in the KG (IMDB and DBLP).",2 Related Work,[0],[0]
"All these methods utilize structured information, while our method shows how unstructured data (web corpus about the entity to be linked) can be effectively used for entity disambiguation.",2 Related Work,[0],[0]
"Entity Embeddings: ELDEN presents a method to enhance embedding of entities and words in a
4https://github.com/ priyaradhakrishnan0/ELDEN
5 (Shen et al., 2015) presents a survey of EL systems.",2 Related Work,[0],[0]
"6Named Entity Disambiguation (NED) and EL are syn-
onymous terms in research (Hoffart et al., 2011)
common vector space.",2 Related Work,[0],[0]
"Word embedding methods like word2vec (Mikolov et al., 2013) and Glove (Pennington et al., 2014) have been extended to entities in EL (Yamada et al., 2016; Fang et al., 2016; Zwicklbauer et al., 2016; Huang et al., 2015).",2 Related Work,[0],[0]
These methods use data about entity-entity co-occurrences to improve the entity embeddings.,2 Related Work,[0],[0]
"In ELDEN, we improve it with web corpus cooccurrence statistics.",2 Related Work,[0],[0]
Ganea and Hofmann (2017) present a very interesting neural model for jointly learning entity embedding along with mentions and contexts.,2 Related Work,[0],[0]
KG densification with pseudo entities: KG densification using external corpus has been studied by Kotnis et al. (2015) and Hegde and Talukdar (2015).,2 Related Work,[0],[0]
"Densifying edge graph is also studied as ‘link prediction’ in literature (Martı́nez et al., 2016).",2 Related Work,[0],[0]
Kotnis et al. augment paths between KG nodes using ‘bridging entities’ which are noun phrases mined from an external corpus.,2 Related Work,[0],[0]
ELDEN has a similar approach as it proposes densifying the KG edges of entities by adding edges from pseudo entities.,2 Related Work,[0],[0]
"However, densification is used for relation inference in the former methods whereas it is used for entity coherence measurement in ELDEN.",2 Related Work,[0],[0]
"Word co-occurrence measures: Chaudhari et al. (2011) survey several co-occurrence measures for word association including PMI, Jaccard (Dice, 1945) and Co-occurrence Significance Ratio (CSR).",2 Related Work,[0],[0]
"Damani (2013) proves that considering corpus level significant co-occurrences, PMI is better than others.",2 Related Work,[0],[0]
"Budiu et al. (2007) compare Latent Semantic Analysis (LSA), PMI and Generalized Latent Semantic Analysis (GLSA) and conclude that for large corpora like web corpus, PMI works best on word similarity tests.",2 Related Work,[0],[0]
"Hence, we chose PMI to refine co-occurring mentions of entities in web corpus.",2 Related Work,[0],[0]
"In this section, we present a few definitions and formulate the EL problem.",3 Definitions and Problem Formulation,[0],[0]
"Knowledge Graph (KG): A Knowledge Graph is defined as G = (E,F ) with entities E as nodes and F as edges.",3 Definitions and Problem Formulation,[0],[0]
"In allegiance to EL literature and baselines (Milne and Witten, 2008; Globerson et al., 2016), we use the Wikipedia hyperlink graph as the KG in this paper, where nodes correspond to Wikipedia articles and edges are incoming links from one Wikipedia article to another.",3 Definitions and Problem Formulation,[0],[0]
"ELDEN ultimately uses a densified version of this
Wikipedia KG, as described in Section 4.",3 Definitions and Problem Formulation,[0],[0]
"Sparsely connected entities: Following Hoffart et al. (2012), we define an entity to be a sparsely connected entity, if the number of edges incident on the entity node in the KG is less than threshold η7.",3 Definitions and Problem Formulation,[0],[0]
"Otherwise, the entity is called a denselyconnected entity.",3 Definitions and Problem Formulation,[0],[0]
Entity Linking (EL):,3 Definitions and Problem Formulation,[0],[0]
"Given a set of mentions MD = {m1, ...,mn} in a document D, and a knowledge graph G = (E,F ), the problem of entity linking is to find the assignment Λ : MD → ED, where ED is the set of entities linked to mentions in document D such that ED ⊆ E.
For mention mi ∈ MD, let the set of possible entities it can link to (candidate entities) be Ci.",3 Definitions and Problem Formulation,[0],[0]
"Then, the solution to the EL problem is an assignment Λ where,
Λ(mi) = arg max",3 Definitions and Problem Formulation,[0],[0]
"e∈Ci [φ(mi, e) + β · ψ(e, ED)]",3 Definitions and Problem Formulation,[0],[0]
"(1) Here, φ(mi, e) ∈",3 Definitions and Problem Formulation,[0],[0]
"[0, 1] measures the contextual compatibility of mentionmi and entity e. φ(mi, e) is obtained by combining prior probability and context similarity.",3 Definitions and Problem Formulation,[0],[0]
"ψ(e, ED) measures the coherence of e with entities in ED.",3 Definitions and Problem Formulation,[0],[0]
"β is a variable controlling inclusion of ψ in the assignment Λ. Problem Formulation : (Yamada et al., 2016) is a recently proposed state-of-the-art EL system.",3 Definitions and Problem Formulation,[0],[0]
We consider it as a representative EL system and use it as the main baseline for the experiments in this paper.,3 Definitions and Problem Formulation,[0],[0]
"In this section, we briefly describe Yamada et al. (2016)’s two-step approach that solves the EL problem presented above.
",3 Definitions and Problem Formulation,[0],[0]
"Step 1: A mention mi ∈ MD is defined to be unambiguous if ∃e ∈ Ci such that φ(mi, e) ≥ γ.",3 Definitions and Problem Formulation,[0],[0]
Let M (u)D ⊆ MD be the set of such unambiguous mentions in document D. For all unambiguous mentions m ∈ M,3 Definitions and Problem Formulation,[0],[0]
"(u)D , Yamada et al. freeze the assignment by solving Equation 1 after setting β = 0.",3 Definitions and Problem Formulation,[0],[0]
"In other words, ψ(e, ED) is not used while assigning entities to unambiguous mentions.",3 Definitions and Problem Formulation,[0],[0]
"Assigning entities first to unambiguous mentions has also been found to be helpful in prior research (Milne and Witten, 2008; Guo and Barbosa, 2014).",3 Definitions and Problem Formulation,[0],[0]
Let AD be the set of entities linked to in this step.,3 Definitions and Problem Formulation,[0],[0]
"In Figure 1, mention Andrei Broder is unambiguous8.
7Like (Hoffart et al., 2012)",3 Definitions and Problem Formulation,[0],[0]
"we set η = 500 for the experiments in this paper.
8Between mention Andrei Broder and entity Andrei Broder, string similarity and prior probability are 1.0.",3 Definitions and Problem Formulation,[0],[0]
"In the experiments we use a γ value of 0.95.
",3 Definitions and Problem Formulation,[0],[0]
Step 2:,3 Definitions and Problem Formulation,[0],[0]
"In this step, Yamada et al. links all ambiguous mentions by solving Equation 2.
Λ(mi) =
arg max e∈Ci
 φ(mi, e) + 1
|AD| ∑
ej∈AD ve · vej
 
∀mi ∈MD \M (u)D (2)
where ve, vej ∈ Rd are d-dimensional embeddings of entities e and ej respectively.",3 Definitions and Problem Formulation,[0],[0]
"Please note that the equation above is a reformulation of Equation 1 with β = 1 and ψ(e, ED) = 1 |AD| ∑ ej∈AD ve · vej , where AD is derived from ED as described in Step 1.",3 Definitions and Problem Formulation,[0],[0]
"As coherence ψ(e, ED) is applied only in disambiguation of ambiguous mentions, we apply densification to only selective nodes of KG.
",3 Definitions and Problem Formulation,[0],[0]
Embeddings of entities are generated using word2vec model and trained using WLM (Details in Section 5).,3 Definitions and Problem Formulation,[0],[0]
"Given a graph G = (E,F ), the WLM coherence measure ψwlm(ei, ej) between two entities ei and ej is defined in Equation 3 where Ce is the set of entities with edge to entity e.
ψwlm(ei, ej) = 1− log(max(|Cei |, |Cej |))− log(|Cei ∩",3 Definitions and Problem Formulation,[0],[0]
"Cej )|)
log(|E|)− log(min(|Cei |, |Cej |))",3 Definitions and Problem Formulation,[0],[0]
(3),3 Definitions and Problem Formulation,[0],[0]
"In this section, we present ELDEN, our proposed approach.",4 Our Approach: ELDEN,[0],[0]
"ELDEN extends (Yamada et al., 2016), with one important difference: rather than working with the input KG directly, ELDEN works with the densified KG, created by selective densification of the KG with co-occurrence statistics extracted from a large corpus.",4 Our Approach: ELDEN,[0],[0]
"Even though this
is a simple change, this results in improved EL performance.",4 Our Approach: ELDEN,[0],[0]
The method performs well even for sparsely connected entities.,4 Our Approach: ELDEN,[0],[0]
Overview : Overview of the ELDEN system is shown in Figure 2.,4 Our Approach: ELDEN,[0],[0]
"ELDEN starts off with densification of the input KG, using statistics from web corpus.",4 Our Approach: ELDEN,[0],[0]
Embeddings of entities are then learned utilizing the densified KG in the next step.,4 Our Approach: ELDEN,[0],[0]
Embedding similarity estimated using the learned entity embeddings is used in calculating coherence measure in subsequent EL.,4 Our Approach: ELDEN,[0],[0]
Notation used is summarized in Table 1.,4 Our Approach: ELDEN,[0],[0]
(i)KG Densification Figure 2 depicts densification of KG in ‘Input KG’ and ‘Densified KG’.,4 Our Approach: ELDEN,[0],[0]
It shows two Wikipedia titles Andrei Broder and WWW conference from our running example (Figure 1).,4 Our Approach: ELDEN,[0],[0]
There are no edges common between Andrei Broder and WWW conference.,4 Our Approach: ELDEN,[0],[0]
"In a web corpus, mentions of Andrei Broder and WWW conference co-occur with Program committee and it has a positive PMI value with both the entities.",4 Our Approach: ELDEN,[0],[0]
So ELDEN adds an edge from Program committee to both the entities.,4 Our Approach: ELDEN,[0],[0]
Here Program committee is a pseudo entity.,4 Our Approach: ELDEN,[0],[0]
"Thus, ELDEN densifies the KG by adding edges from pseudo entities when the mentions of Wikipedia entity and pseudo entity co-occur in a web corpus and the pseudo entity has a positive PMI value with given entity.
",4 Our Approach: ELDEN,[0],[0]
"Taking a closer look, KG densification process starts from ‘input KG’ which is Wikipedia hyperlink graph G = (E,F ), where the nodes are Wikipedia titles (E) and edges are hyperlinks (F ).",4 Our Approach: ELDEN,[0],[0]
"ELDEN processes Wikipedia text corpus and identifies phrases (unigrams and bi-grams) that occur frequently, i.e. more than 10 times in it.",4 Our Approach: ELDEN,[0],[0]
We denote these phrases as pseudo entities (S) and add them as nodes to the KG.,4 Our Approach: ELDEN,[0],[0]
"Let E+ = E ∪ S be the resulting set of nodes.
ELDEN then adds edges connecting entities in E+ to entities in E. This is done by processing a web text corpus looking for mentions of entities in E+, and linking the mentions to entities in KG G ′ =",4 Our Approach: ELDEN,[0],[0]
"(E+, F ).",4 Our Approach: ELDEN,[0],[0]
"ELDEN uses Equation 1 with β = 0 for this entity linking, i.e. only mentionentity similarity φ(m, e) is used during this linking9.",4 Our Approach: ELDEN,[0],[0]
"Based on this entity linked corpus, a cooccurrence matrix M of size |E+| × |E+| is constructed.",4 Our Approach: ELDEN,[0],[0]
"Each cell Mi,j is set to the PMI between
9Since prior probabilities of pseudo entities are not available, only mention-entity similarity component of φ(m, s) is used while linking a mention m to a pseudo entity s ∈ S.
Input KG Densified KG
Entity Embeddings (V)
the entities e and e ′ .
",4 Our Approach: ELDEN,[0],[0]
"Me,e′ = PMI(e, e ′ ) = log
f(e, e ′ )",4 Our Approach: ELDEN,[0],[0]
"×N
f(e)× f(e′) where f(e) is the frequency of entity e in web corpus, f(e, e ′ ) is the sentence-constrained pair frequency of the entity pair (e, e ′ ) in web corpus, and N = ∑
e,e′∈E+ f(e, e ′ ).",4 Our Approach: ELDEN,[0],[0]
"Please note that PMI, and
there by M , are symmetric.",4 Our Approach: ELDEN,[0],[0]
"The expanded set of edges, F+, is now defined as
F+ = F∪{(e, e ′ ), (e ′ , e)",4 Our Approach: ELDEN,[0],[0]
| e′ ∈,4 Our Approach: ELDEN,[0],[0]
"E+, e ∈ E,Me,e′ > 0}
In other words, we augment the set of initial edges F with additional edges connecting entities in E+ with entities in E such that PMI between the entities is positive.
",4 Our Approach: ELDEN,[0],[0]
"ELDEN now constructs the KG Gdense = (E+, F+), which is a densified version of the input KG G = (E,F ).",4 Our Approach: ELDEN,[0],[0]
ELDEN uses this densified KG Gdense for subsequent processing and entity linking.,4 Our Approach: ELDEN,[0],[0]
"(ii)Learning Embeddings of Densified KG Entities ELDEN derives entity embeddings using the same setup, corpus and Word2vec skip-gram with negative sampling model as in Yamada et al.,",4 Our Approach: ELDEN,[0],[0]
"However, instead of training embeddings over the input KG, ELDEN trains embeddings of entities in the densified KG Gdense.",4 Our Approach: ELDEN,[0],[0]
Let V be the word2vec matrix containing embeddings of entities in E+ where V ∈ Rk∗d.,4 Our Approach: ELDEN,[0],[0]
"ve is the embedding of entity e in E+ with dimension 1 ∗ d.
In word2vec model, entities in context are used
to predict the target entity.",4 Our Approach: ELDEN,[0],[0]
"ELDEN maximizes the objective function (Goldberg and Levy, 2014) of word2vec skip-gram model with negative sampling, L = ∑ (t,c)∈P Lt,c where
Lt,c = log θ(vc · vt) + ∑
n∈N(t,c) log θ(−vn · vt)
",4 Our Approach: ELDEN,[0],[0]
Here vt and vc are the entity embeddings of target entity t and context entity c. P is the set of target-context entity pairs considered by the model.,4 Our Approach: ELDEN,[0],[0]
"N(t,c) is a set of randomly sampled entities used as negative samples with pair (t, c).",4 Our Approach: ELDEN,[0],[0]
"This objective is maximized with respect to variables vt’s and vc’s, where θ(x) = 11+e−x .",4 Our Approach: ELDEN,[0],[0]
P and N are derived usingGdense.,4 Our Approach: ELDEN,[0],[0]
"t and c are entities inE+ such that c shares a common edge with t. vn is randomly sampled from V, for entities that do not share a common edge with t. Entity embedding similarity measured using V trained this way on Gdense is ψELDEN.",4 Our Approach: ELDEN,[0],[0]
Embedding similarity is measured as cosine distance between ves.,4 Our Approach: ELDEN,[0],[0]
Embeddings of S are trained using positive and negative word contexts derived using context length.,4 Our Approach: ELDEN,[0],[0]
"(iii) Bringing it All Together: ELDEN
ELDEN is a supervised EL system which uses two sets of features: (1) contextual compatibility φ(m, e); and (2) coherence ψ(ei, ej).",4 Our Approach: ELDEN,[0],[0]
These features are summarized in Table 2.,4 Our Approach: ELDEN,[0],[0]
Similarity between entity embeddings is measured as cosine similarity between ves.,4 Our Approach: ELDEN,[0],[0]
"In this section, we evaluate the following:
• Is ELDEN’s corpus co-occurrence statisticsbased densification helpful in disambiguating entities better?",5 Experiments,[0],[0]
"(Sec. 6.1)
• Where does ELDEN’s selective densification of KG nodes link entities better?",5 Experiments,[0],[0]
"(Sec. 6.3)
",5 Experiments,[0],[0]
"Setup : ELDEN is implemented using Random Forest ensemble 10 (Breiman, 1998).",5 Experiments,[0],[0]
Parameter values were set using CoNLL development set.,5 Experiments,[0],[0]
Feature limit of 3 with number of estimators as 100 yielded best performance.,5 Experiments,[0],[0]
Knowledge Graph: Wikipedia,5 Experiments,[0],[0]
"Following prior EL literature, we use Wikipedia hypergraph as our KG (Milne and Witten, 2008; Globerson et al., 2016).",5 Experiments,[0],[0]
This KG is enhanced with pseudo entities as explained in Section 4.,5 Experiments,[0],[0]
"We process
10http://scikit-learn.org
the Wikipedia corpus following the same procedure as in Yamada et al. (2016).",5 Experiments,[0],[0]
We cleaned 11 Wikipedia dump dated Nov 2015.,5 Experiments,[0],[0]
We then parsed the Wikipedia article text to identify pseudo entities.,5 Experiments,[0],[0]
More details on KG and parameters used for training embeddings are in Table 3.,5 Experiments,[0],[0]
Training took 4 days on gpu with 2 cores.,5 Experiments,[0],[0]
Preprocessing: Web corpus and Densified KG,5 Experiments,[0],[0]
"For our experiments, we created a web corpus by querying Google12.",5 Experiments,[0],[0]
Candidate entities of all mentions in the dataset are queried in Google and top ten search results are considered for unigram and bigram frequencies.,5 Experiments,[0],[0]
This corpus occupied 6.8GB for candidate entities of TAC and CoNLL dataset mentions (54336 entities).,5 Experiments,[0],[0]
"Even for sparsely connected entities, an average corpus size of 670 lines or more13 was collected.",5 Experiments,[0],[0]
"We note that though some of the entities mentioned in this dataset are ten or more years old, we are able to collect, on an average more than 670 lines of web content.",5 Experiments,[0],[0]
"Thus corpus proves to be a good source of additional links for densification, for both common and rare entities.",5 Experiments,[0],[0]
"As Taneva and Weikum (2013) also note, it is not hard to find content about sparsely connected entities on the web.
",5 Experiments,[0],[0]
The web corpus is analyzed for mentions and pseudo entities.,5 Experiments,[0],[0]
Co-occurrence matrix M is created14 for mention and pseudo entities occurring within window of size 10 for PMI calculation15.,5 Experiments,[0],[0]
Edges are added from pseudo entities with positive PMI to mention of given entity.,5 Experiments,[0],[0]
"In experiments we add edges from top 10 pseudo entities ordered by
11by removing disambiguation, navigation, maintenance and discussion pages.
12https://www.google.com/ 13A detailed analysis of knowledge gained from crawling for common versus less common entities is present in Figure 1 of supplementary material.
",5 Experiments,[0],[0]
"14This co-occurrence matrix is downloadable with source code.
",5 Experiments,[0],[0]
"15We experimented with window sizes 10, 25 and 50.",5 Experiments,[0],[0]
"We chose 10 that gave best results
PMI values16.",5 Experiments,[0],[0]
Evaluation Dataset:,5 Experiments,[0],[0]
"In line with prior work on EL, we test the performance of ELDEN on CoNLL and TAC datasets.",5 Experiments,[0],[0]
"As this paper focuses on entity disambiguation, we tested ELDEN against datasets and baseline methods for disambiguation.",5 Experiments,[0],[0]
"We note that the entity disambiguation evaluation part of other recent datasets like ERD 2014 and TAC 2015 is exactly same as the TAC 2010 evaluation (Ellis et al., 2014) 17.",5 Experiments,[0],[0]
Training: ELDEN’s parameters were tuned using training (development) sets of CoNLL and TAC datasets.,5 Experiments,[0],[0]
"CoNLL and TAC datasets consist of documents where mentions are marked and entity to which the mention links to, is specified.",5 Experiments,[0],[0]
We use only mentions that link to a valid Wikipedia title (non NIL entities) and report performance on test set.,5 Experiments,[0],[0]
Some aspects of these datasets relevant to our experiments are provided below.,5 Experiments,[0],[0]
CoNLL:,5 Experiments,[0],[0]
"In CoNLL test set (5267 mentions), we report Precision of topmost candidate entity, aggregated over all mentions (P-micro) and aggregated over all documents (P-macro), i.e., if tp, fp and p are the individual true positives, false positives and precision for each document in a dataset of δ documents, then
Pmicro =
δ∑ i=1",5 Experiments,[0],[0]
"tpi
δ∑ i=1",5 Experiments,[0],[0]
tpi+ δ∑ i=1,5 Experiments,[0],[0]
"fpi
and Pmacro =
δ∑ i=1",5 Experiments,[0],[0]
"pi
δ .
",5 Experiments,[0],[0]
"For CoNLL candidate entities, we use (Pershina et al., 2015) dataset18.",5 Experiments,[0],[0]
TAC:,5 Experiments,[0],[0]
"In TAC dataset, we report P-micro of topranked candidate entity on 1,020 mentions.",5 Experiments,[0],[0]
Pmacro is not applicable to TAC as most documents have only one mention as query mention ( or ’mention to be linked’).,5 Experiments,[0],[0]
"For TAC candidate entities, we index the Wikipedia word tokens and titles using solr19.",5 Experiments,[0],[0]
"We index terms in (1) title of the entity, (2) title of another entity redirecting to the entity, and (3) names of anchors that point to the entity, in line with baselines.",5 Experiments,[0],[0]
We are making this TAC candidate set publicly available.,5 Experiments,[0],[0]
Baseline:,5 Experiments,[0],[0]
Yamada16 Our baseline is the Yamada et al. system explained in Section 3.,5 Experiments,[0],[0]
"Entity embedding distance measured using ve trained on the
16This is a tunable parameter.",5 Experiments,[0],[0]
"17These recent datasets consist of other evaluations, e.g., mention detection, multilinguality etc. which is beyond the scope of the paper and hence we didnt focus on them in the paper.
",5 Experiments,[0],[0]
"18https://github.com/masha-p/PPRforNED 19http://lucene.apache.org/solr/
input KG G is ψYamada.",5 Experiments,[0],[0]
"In Table 4, we compare ELDEN’s EL performance with results of other recently proposed state-ofthe-art EL methods that use coherence models.",6.1 Does ELDEN’s selective densification help in disambiguation in EL?,[0],[0]
We see that ELDEN results matches best results on CoNLL and outperforms state-of-the-art in TAC dataset.,6.1 Does ELDEN’s selective densification help in disambiguation in EL?,[0],[0]
"In the table, the last four rows uses the Pershina et al. (2015) candidate set and hence, we provide a comparison of their disambiguation performance.",6.1 Does ELDEN’s selective densification help in disambiguation in EL?,[0],[0]
Improved results of ELDEN over baseline is attributed to the improved disambiguation due to KG densification.,6.1 Does ELDEN’s selective densification help in disambiguation in EL?,[0],[0]
We conduct ablation analysis using various feature and feature combinations and present performance of ELDEN and baseline in Table 5.,6.2 Why does ELDEN’s selective densification work?,[0],[0]
"Starting with base features, we add various features to ELDEN incrementally and report their impact on performance.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"The results when using base feature group alone, and base and string similarity groups together (φ) are presented in first and second rows for each dataset.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"We compare ψELDEN to three coherence measures: ψwlm, ψYamada and ψdense, details of which are provided in Table 2.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
The performance improvement from each of the four coherence measures are in the next four rows.,6.2 Why does ELDEN’s selective densification work?,[0],[0]
"Performance of ELDEN from using all four coherence features is given in ψELDEN++ row.
",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"On CoNLL dataset, ψdense combined with φ,
gave an improvement of 2.0 and 1.9 (P-micro and P-macro) over Yamada16 results.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"We note that Yamada16 results are from our re-implementation of (Yamada et al., 2016) system 20 and we are able to almost reproduce the baseline results.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
We also present the results combining baseline’s ψYamada and ψwlm versus ELDEN’s ψELDEN and ψdense in next two rows.,6.2 Why does ELDEN’s selective densification work?,[0],[0]
"We find the ELDEN’s KG densification features perform better than baselines.
",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"On TAC dataset also, combined with φ, ψdense is found to do better than ψwlm and ψELDEN gives a significant P-micro improvement of 4.2 over ψYamada.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
The ψELDEN++ P-micro in TAC dataset is statistically significant21.,6.2 Why does ELDEN’s selective densification work?,[0],[0]
"In short, we find the KG densification features, ψdense and ψELDEN, as the features causing better performance of ELDEN on both datasets.",6.2 Why does ELDEN’s selective densification work?,[0],[0]
"While most EL systems give higher precision on CoNLL dataset than TAC dataset, ELDEN performs with high precision on TAC dataset too.
",6.3 Where does ELDEN’s selective densification work better?,[0],[0]
"20We have re-implemented the Yamada et al system using hyper-parameters specified in the paper and these are our best-effort results.
21We performed two tailed t-test, with 2-tail 95% value of 1.96.
",6.3 Where does ELDEN’s selective densification work better?,[0],[0]
This is explained by analyzing distribution of densely-connected and sparsely connected entities in TAC and CoNLL datasets as presented in Table 6.,6.3 Where does ELDEN’s selective densification work better?,[0],[0]
"We see that CoNLL test set has almost half as densely-connected and half as sparsely connected entities, whereas in TAC test set, 63.6% are sparsely connected entities.",6.3 Where does ELDEN’s selective densification work better?,[0],[0]
"This higher constitution of sparsely connected entities in TAC, explains ELDEN’s better results in TAC relative to CoNLL dataset.",6.3 Where does ELDEN’s selective densification work better?,[0],[0]
"As the number of sparsely connected entities is more than the number of denselyconnected entities in most KGs (Reinanda et al., 2016), our method is expected to be of significance for most KGs.",6.3 Where does ELDEN’s selective densification work better?,[0],[0]
We analyzed errors fixed by ELDEN on TAC dataset.,6.4 What type of EL errors are best fixed with ELDEN’s selective densification ?,[0],[0]
We categorize the errors into four classes in line with error classes of Ling et al. (2015).,6.4 What type of EL errors are best fixed with ELDEN’s selective densification ?,[0],[0]
"We
manually analyzed 240 wrong predictions of Yamada16 and compared it with that of ELDEN, and the results are presented in Figure 3.",6.4 What type of EL errors are best fixed with ELDEN’s selective densification ?,[0],[0]
We found errors to reduce with use of KG densification features and most of the errors eliminated were in “Specific label” class.,6.4 What type of EL errors are best fixed with ELDEN’s selective densification ?,[0],[0]
"Errors in this class called for better modeling of mention’s context and linkbased similarity (Ling et al., 2015).(More details of this analysis in the supplementary document.)",6.4 What type of EL errors are best fixed with ELDEN’s selective densification ?,[0],[0]
We started this study by analyzing the performance of a state-of-the-art Entity Linking (EL) system and found that its performance was low when linking entities sparsely-connected in the KG.,7 Conclusion,[0],[0]
We saw that this can be addressed by densifying the KG with respect to the given entity.,7 Conclusion,[0],[0]
"We proposed ELDEN, which densifies edge graph of entities using pseudo entities and mentions of entities in a large web corpus.",7 Conclusion,[0],[0]
"Through our experiments, we find that ELDEN outperforms state-ofthe-art baseline on benchmark datasets.
",7 Conclusion,[0],[0]
We believe that ELDENs combination of KG densification and entity embeddings is novel.,7 Conclusion,[0],[0]
Poor performance of EL systems on sparsely connected entities has been recognized as one of the open challenges by prior research.,7 Conclusion,[0],[0]
"ELDEN performs well on sparsely connected entities too, as a validation of our method of combining KG densification followed by embedding.",7 Conclusion,[0],[0]
"Our approach may be applied to any KG as the densification is performed with the help of unstructured data, and not any specific KG.",7 Conclusion,[0],[0]
"We hope the simple graph densification method utilized in ELDEN will be of much interest to the research community.
",7 Conclusion,[0],[0]
"Pseudo entities can be looked at as entity candidates for KG expansion, as also noted by Farid et al. (2016).",7 Conclusion,[0],[0]
"In future, we plan to enhance ELDEN using EL of pseudo entities to estimate entity prior of entities not present in KG.",7 Conclusion,[0],[0]
We also plan to explore entity embeddings obtained using other graph densifying methods.,7 Conclusion,[0],[0]
We thank the Microsoft Research India Travel Grants for generous travel funds to attend and present this paper at NAACL.,8 Acknowledgments,[0],[0]
Entity Linking (EL) systems aim to automatically map mentions of an entity in text to the corresponding entity in a Knowledge Graph (KG).,abstractText,[0],[0]
Degree of connectivity of an entity in the KG directly affects an EL system’s ability to correctly link mentions in text to the entity in KG.,abstractText,[0],[0]
"This causes many EL systems to perform well for entities well connected to other entities in KG, bringing into focus the role of KG density in EL.",abstractText,[0],[0]
"In this paper, we propose Entity Linking using Densified Knowledge Graphs (ELDEN).",abstractText,[0],[0]
"ELDEN is an EL system which first densifies the KG with co-occurrence statistics from a large text corpus, and then uses the densified KG to train entity embeddings.",abstractText,[0],[0]
Entity similarity measured using these trained entity embeddings result in improved EL.,abstractText,[0],[0]
ELDEN outperforms stateof-the-art EL system on benchmark datasets.,abstractText,[0],[0]
"Due to such densification, ELDEN performs well for sparsely connected entities in the KG too.",abstractText,[0],[0]
"ELDEN’s approach is simple, yet effective.",abstractText,[0],[0]
We have made ELDEN’s code and data publicly available.,abstractText,[0],[0]
ELDEN: Improved Entity Linking Using Densified Knowledge Graphs,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3208–3218 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3208",text,[0],[0]
"Knowledge bases (KB) are an essential part of many computational systems with applications in search, structured data management, recommendations, question answering, and information retrieval.",1 Introduction,[0],[0]
"However, KBs often suffer from incompleteness, noise in their entries, and inefficient inference under uncertainty.",1 Introduction,[0],[0]
"To address these issues, learning relational knowledge representations has
been a focus of active research (Bordes et al., 2011, 2013; Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016; Dettmers et al., 2018).",1 Introduction,[0],[0]
"These approaches represent relational triples, that consist of a subject entity, relation, and an object entity, by learning fixed, low-dimensional representations for each entity and relation from observations, encoding the uncertainty and inferring missing facts accurately and efficiently.",1 Introduction,[0],[0]
"The subject and the object entities come from a fixed, enumerable set of entities that appear in the knowledge base.
",1 Introduction,[0],[0]
"Knowledge bases in the real world, however, contain a wide variety of data types beyond these direct links.",1 Introduction,[0],[0]
"Apart from relations to a fixed set of entities, KBs often not only include numerical attributes (such as ages, dates, financial, and geoinformation), but also textual attributes (such as names, descriptions, and titles/designations) and images (profile photos, flags, posters, etc.).",1 Introduction,[0],[0]
These different types of data can play a crucial role as extra pieces of evidence for knowledge base completion.,1 Introduction,[0],[0]
"For example the textual descriptions and images might provide evidence for a person’s age, profession, and designation.",1 Introduction,[0],[0]
"In the multimodal KB shown in Figure 1 for example, the image can be helpful in predicting of Carles Puyol’s occupation, while the description contains his nationality.",1 Introduction,[0],[0]
"Incorporating this information into existing approaches as entities, unfortunately, is challenging as they assign each entity a distinct vector and predict missing links (or attributes) by enumerating over the possible values, both of which are only possible if the entities come from a small, enumerable set.",1 Introduction,[0],[0]
"There is thus a crucial need for relational modeling that goes beyond just the link-based view of KB completion, by not only utilizing multimodal information for better link prediction between existing entities, but also being able to generate missing multimodal values.
",1 Introduction,[0],[0]
"In this paper, we introduce multimodal knowl-
edge base embeddings (MKBE) for modeling knowledge bases that contain a variety of data types, such as links, text, images, numerical, and categorical values.",1 Introduction,[0],[0]
"We propose neural encoders and decoders to replace initial layers of any embeddingbased relational model; we apply them to DistMult (Yang et al., 2015) and ConvE (Dettmers et al., 2018) here.",1 Introduction,[0],[0]
"Specifically, instead of learning a distinct vector for each entity and using enumeration to predict links, MKBE includes the following extensions: (1) introduce additional neural encoders to embed multimodal evidence types that the relational model uses to predict links, and (2) introduce neural decoders that use an entity’s embedding to generate its multimodal attributes (like image and text).",1 Introduction,[0],[0]
"For example, when the object of a triple is an image, we encode it into a fixed-length vector using a CNN, while textual objects are encoded using RNN-based sequence encoders.",1 Introduction,[0],[0]
"The scoring module remains identical to the underlying relational model; given the vector representations of the subject, relation, and object of a triple, we produce a score indicating the probability that the triple is correct using DistMult or ConvE. After learning the KB representation, neural decoders use entity embeddings to generate missing multimodal attributes, for example, generating the description of a person from their structured information in the KB.",1 Introduction,[0],[0]
"This unified framework allows for flow of the information across the different relation types (multimodal or otherwise), providing a more accurate modeling of relational data.
",1 Introduction,[0],[0]
We provide an evaluation of our proposed approach on two relational KBs.,1 Introduction,[0],[0]
"Since we are introducing the multimodal KB completion setting, we provide two benchmarks, created by extending the
existing YAGO-10 and MovieLens-100k datasets to include additional relations such as textual descriptions, numerical attributes, and images of the entities.",1 Introduction,[0],[0]
"We demonstrate that MKBE utilizes the additional information effectively to provide gains in link-prediction accuracy, achieving state-of-theart results on these datasets for both the DistMult and the ConvE scoring functions.",1 Introduction,[0],[0]
"We evaluate the quality of multimodal attributes generated by the decoders via user studies that demonstrate their realism and information content, along with presenting examples of such generated text and images.",1 Introduction,[0],[0]
"As described earlier, KBs often contain different types of information about entities including links, textual descriptions, categorical attributes, numerical values, and images.",2 Multimodal KB Completion,[0],[0]
"In this section, we briefly introduce existing relational embedding approaches that focus on modeling the linked data using distinct, dense vectors.",2 Multimodal KB Completion,[0],[0]
"We then describe MKBE that extends these approaches to the multimodal setting, i.e., modeling the KB using all the different information to predict the missing links and impute the missing attributes.",2 Multimodal KB Completion,[0],[0]
"Factual statements in a knowledge base are represented using a triple of subject, relation, and object, 〈s, r, o〉, where s, o ∈ ξ, a set of entities, and r ∈",2.1 Background on Link Prediction,[0],[0]
"R, a set of relations.",2.1 Background on Link Prediction,[0],[0]
"Respectively, we consider two goals for relational modeling, (1) to train a machine learning model that can score the truth value of any factual statement, and (2) to predict missing links between the entities.",2.1 Background on Link Prediction,[0],[0]
"In existing approaches, a scoring function ψ : ξ ×R× ξ → R (or sometimes, [0, 1]) is learned to evaluate whether any given fact is true, as per the model.",2.1 Background on Link Prediction,[0],[0]
"For predicting links between the entities, since the set ξ is small enough to be enumerated, missing links of the form 〈s, r, ?〉 are identified by enumerating all the objects and scoring the triples using ψ",2.1 Background on Link Prediction,[0],[0]
(i.e. assume the resulting entity comes from a known set).,2.1 Background on Link Prediction,[0],[0]
"For example, in Figure 1, the goal is to predict that Carles Puyol plays for Barcelona.
",2.1 Background on Link Prediction,[0],[0]
"Many of the recent advances in link prediction use an embedding-based approach; each entity in ξ and relation inR are assigned distinct, dense vectors, which are then used by ψ to compute the score.",2.1 Background on Link Prediction,[0],[0]
"In DistMult (Yang et al., 2015), for example, each entity i is mapped to a d-dimensional dense vector
(ei ∈ Rd) and each relation r to a diagonal matrix",2.1 Background on Link Prediction,[0],[0]
"Rr ∈ Rd×d, and consequently, the score for any triple 〈s, r, o〉 is computed as ψ(s, r, o) = eTs Rreo.",2.1 Background on Link Prediction,[0],[0]
"Along similar lines, ConvE (Dettmers et al., 2018) uses vectors to represent the entities and the relations, es, eo, rr ∈ Rd×1, then, after applying a CNN layer on es and rr, combines it with eo to score a triplet, i.e. the scoring function ψ(s, r, o) is f(vec(f([ēs; r̄r ∗ w]))W )eo.",2.1 Background on Link Prediction,[0],[0]
"Other relational embedding approaches primarily vary in their design of the scoring function (Bordes et al., 2013; Yang et al., 2015; Nickel et al., 2016; Trouillon et al., 2016), but share the shortcoming of assigning distinct vectors to every entity, and assuming that the possible object entities can be enumerated.",2.1 Background on Link Prediction,[0],[0]
"In this work we focus on DistMult because of its simplicity, popularity, and high accuracy, and ConvE because of its state-of-the-art results.",2.1 Background on Link Prediction,[0],[0]
"When faced with additional triples in form of multimodal data, the setup of link prediction is slightly different.",2.2 Problem Setup,[0],[0]
"Consider a set of all potential multimodal objects,M, i.e. possible images, text, numerical, and categorical values, and multimodal evidence triples, 〈s, r, o〉, where s ∈ ξ, r ∈ R, and o ∈",2.2 Problem Setup,[0],[0]
M.,2.2 Problem Setup,[0],[0]
"Our goals with incorporating multimodal information into KB remain the same: we want to be able to score the truth of any triple 〈s, r, o〉, where o is from ξ (link data) or fromM (multimodal data), and to be able to predict missing value 〈s, r, ?",2.2 Problem Setup,[0],[0]
〉 that may be from ξ or M (depending on r).,2.2 Problem Setup,[0],[0]
"For the example in Figure 1, in addition to predicting that Carles Puyol plays for Barcelona from multimodal evidence, we are also interested in generating an image for Carles Puyol, if it is missing.
",2.2 Problem Setup,[0],[0]
"Existing approaches to this problem assume that the subjects and the objects are from a fixed set of entities ξ, and thus are treated as indices into that set, which fails for the multimodal setting primarily for two reasons.",2.2 Problem Setup,[0],[0]
"First, learning distinct vectors for each object entity does not apply to multimodal values as they will ignore the actual content of the multimodal attribute.",2.2 Problem Setup,[0],[0]
"For example, there will be no way to generalize vectors learned during training to unseen values that might appear in the test; this is not a problem for the standard setup due to the assumption that all entities have been observed during training.",2.2 Problem Setup,[0],[0]
"Second, in order to predict a missing multimodal value, 〈s, r, ?〉, enumeration is not possible as the search space is potentially infinite
(or at least intractable to search).",2.2 Problem Setup,[0],[0]
"To incorporate such multimodal objects into the existing relational models like DistMult and ConvE, we propose to learn embeddings for these types of data as well.",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"We utilize recent advances in deep learning to construct encoders for these objects to represent them, essentially providing an embedding eo for any object value.
",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"The overall goal remains the same: the model needs to utilize all the observed subjects, objects, and relations, across different data types, in order to estimate whether any fact 〈s, r, o〉 holds.",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
We present an example of an instantiation of MKBE for a knowledge base containing YAGO entities in Figure 2a.,2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"For any triple 〈s, r, o〉, we embed the subject (Carles Puyol) and the relation (such as playsFor, wasBornOn, or playsFor) using a direct lookup.",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"For the object, depending on the domain (indexed, string, numerical, or image, respectively), we use approrpiate encoders to compute its embedding eo.",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"As in DistMult and ConvE, these embeddings are used to compute the score of the triple.
",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
"Via these neural encoders, the model can use the information content of multimodal objects to predict missing links where the objects are from ξ, however, learning embeddings for objects inM is not sufficient to generate missing multimodal values, i.e. 〈s, r, ?〉 where the object is inM. Consequently, we introduce a set of neural decoders D : ξ × R → M that use entity embeddings to generate multimodal values.",2.3 Multimodal KB Embeddings (MKBE),[0],[0]
An outline of our model for imputing missing values is depicted in Figure 2b.,2.3 Multimodal KB Embeddings (MKBE),[0],[0]
We will describe these decoders in Section 2.5.,2.3 Multimodal KB Embeddings (MKBE),[0],[0]
Here we describe the encoders we use for multimodal objects.,2.4 Encoding Multimodal Data,[0],[0]
A simple example of MKBE is provided in Figure 2a.,2.4 Encoding Multimodal Data,[0],[0]
"As it shows, we use different encoder to embed each specific data type.
",2.4 Encoding Multimodal Data,[0],[0]
"Structured Knowledge Consider a triplet of information in the form of 〈s, r, o〉.",2.4 Encoding Multimodal Data,[0],[0]
"To represent the subject entity s and the relation r as independent embedding vectors (as in previous work), we pass their one-hot encoding through a dense layer.",2.4 Encoding Multimodal Data,[0],[0]
"Furthermore, for the case that the object entity is categorical, we embed it through a dense layer with a recently introduced selu activation (Klambauer et al., 2017), with the same number of nodes as the
embedding space dimension.
",2.4 Encoding Multimodal Data,[0],[0]
Numerical Objects in the form of real numbers can provide a useful source of information and are often quite readily available.,2.4 Encoding Multimodal Data,[0],[0]
"We use a feed forward layer, after standardizing the input, in order to embed the numbers (in fact, we are projecting them to a higher-dimensional space, from R → Rd).",2.4 Encoding Multimodal Data,[0],[0]
"It is worth noting that existing methods treat numbers as distinct entities, e.g., learn independent vectors for numbers 39 and 40, relying on data to learn that these values are similar to each other.
",2.4 Encoding Multimodal Data,[0],[0]
"Text Since text can be used to store a wide variety of different types of information, for example names versus paragraph-long descriptions, we create different encoders depending on the lengths of the strings involved.",2.4 Encoding Multimodal Data,[0],[0]
"For attributes that are fairly short, such as names and titles, we use characterbased stacked, bidirectional GRUs to encode them, similar to Verga et al. (2016), using the final output of the top layer as the representation of the string.",2.4 Encoding Multimodal Data,[0],[0]
"For strings that are much longer, such as detailed descriptions of entities consisting of multiple sentences, we treat them as a sequence of words, and use a CNN over the word embeddings, similar to Francis-Landau et al. (2016), in order to learn the embedding of such values.",2.4 Encoding Multimodal Data,[0],[0]
"These two encoders provide a fixed length encoding that has been shown to be an accurate semantic representation of strings for multiple tasks (Dos Santos and Gatti, 2014).
",2.4 Encoding Multimodal Data,[0],[0]
Images Images can also provide useful evidence for modeling entities.,2.4 Encoding Multimodal Data,[0],[0]
"For example, we can ex-
tract person’s details such as gender, age, job, etc., from image of the person (Levi and Hassner, 2015), or location information such as its approximate coordinates, neighboring locations, and size from map images (Weyand et al., 2016).",2.4 Encoding Multimodal Data,[0],[0]
"A variety of models have been used to compactly represent the semantic information in the images, and have been successfully applied to tasks such as image classification, captioning (Karpathy and Fei-Fei, 2015), and question-answering (Yang et al., 2016).",2.4 Encoding Multimodal Data,[0],[0]
"To embed images such that the encoding represents such semantic information, we use the last hidden layer of VGG pretrained network on Imagenet (Simonyan and Zisserman, 2015), followed by compact bilinear pooling (Gao et al., 2016), to obtain the embedding of the images.",2.4 Encoding Multimodal Data,[0],[0]
Training We follow the setup from Dettmers et al. (2018) that consists of binary cross-entropy loss without negative sampling for both ConvE and DisMult scoring.,2.4 Encoding Multimodal Data,[0],[0]
"In particular, for a given subjectrelation pair (s, r), we use a binary label vector ts,r over all entities, indicating whether 〈s, r, o〉 is observed during training.",2.4 Encoding Multimodal Data,[0],[0]
"Further, we denote the model’s probability of truth for any triple 〈s, r, o〉 by ps,ro , computed using a sigmoid over ψ(s, r, o).",2.4 Encoding Multimodal Data,[0],[0]
"The binary cross-entropy loss is thus defined as:∑ (s,r) ∑ o ts,ro",2.4 Encoding Multimodal Data,[0],[0]
"log(p s,r o )",2.4 Encoding Multimodal Data,[0],[0]
"+ (1− ts,ro )",2.4 Encoding Multimodal Data,[0],[0]
"log(1− ps,ro ).
",2.4 Encoding Multimodal Data,[0],[0]
"We use the same loss for multimodal triples as well, except that the summation is restricted to the objects of the same modality, i.e. for an entity s
and its text description, ts,r is a one-hot vector over all descriptions observed during training.",2.4 Encoding Multimodal Data,[0],[0]
Here we describe the decoders we use to generate multimodal values for entities from their embeddings.,2.5 Decoding Multimodal Data,[0],[0]
"The multimodal imputing model is shown in Figure 2b, which uses different neural decoders to generate missing attributes (more details are provided in supplementary materials).",2.5 Decoding Multimodal Data,[0],[0]
"Numerical and Categorical data To recover the missing numerical and categorical data such as dates, gender, and occupation, we use a simple feed-forward network on the entity embedding to predict the missing attributes.",2.5 Decoding Multimodal Data,[0],[0]
"In other words, we are asking the model, if the actual birth date of an entity is not in the KB, what will be the most likely date, given the rest of the relational information.",2.5 Decoding Multimodal Data,[0],[0]
"These decoders are trained with embeddings from Section 2.4, with appropriate losses (RMSE for numerical and cross-entropy for categories).",2.5 Decoding Multimodal Data,[0],[0]
"Text A number of methods have considered generative adversarial networks (GANs) to generate grammatical and linguistically coherent sentences (Yu et al., 2017; Rajeswar et al., 2017; Guo et al., 2017).",2.5 Decoding Multimodal Data,[0],[0]
"In this work, we use the adversarially regularized autoencoder (ARAE) (Zhao et al., 2017) to train generators that decodes text from continuous codes, however, instead of using the random noise vector z, we condition the generator on the entity embeddings.",2.5 Decoding Multimodal Data,[0],[0]
"Images Similar to text recovery, to find the missing images we use conditional GAN structure.",2.5 Decoding Multimodal Data,[0],[0]
"Specifically, we combine the BE-GAN (Berthelot et al., 2017) structure with pix2pix-GAN (Isola et al., 2017) model to generate high-quality images, conditioning the generator on the entity embeddings in the knowledge base representation.",2.5 Decoding Multimodal Data,[0],[0]
"There is a rich literature on modeling knowledge bases using low-dimensional representations, differing in the operator used to score the triples.",3 Related Work,[0],[0]
"In particular, they use matrix and tensor multiplication (Nickel et al., 2011; Yang et al., 2015; Socher et al., 2013), Euclidean distance (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015), circular correlation (Nickel et al., 2016), or the Hermitian dot product (Trouillon et al., 2016) as scoring function.",3 Related Work,[0],[0]
"However, the objects for all of these approaches are a fixed set of entities, i.e., they only embed the
structured links between the entities.",3 Related Work,[0],[0]
"Here, we use different types of information (text, numerical values, images, etc.) in the encoding component by treating them as relational triples.
",3 Related Work,[0],[0]
"A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values (Garcia-Duran and Niepert, 2017) (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images (Xie et al., 2017; Oñoro-Rubio et al., 2017) (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text (McAuley and Leskovec, 2013; Zhong et al., 2015; Toutanova et al., 2015, 2016; Xie et al., 2016; Tu et al., 2017), and a combination of text and image (Sergieh et al., 2018).",3 Related Work,[0],[0]
"Further, Verga et al. (2016) address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations (Riedel et al., 2013).",3 Related Work,[0],[0]
"In addition to treating the extra information as features, graph embedding approaches (Schlichtkrull et al., 2017; Kipf and Welling, 2016) consider observed attributes while encoding to achieve more accurate embeddings.
",3 Related Work,[0],[0]
"The difference between MKBE and these mentioned approaches is three-fold: (1) we are the first to use different types of information in a unified model, (2) we treat these different types of information (numerical, text, image) as relational triples of structured knowledge instead of predetermined features, i.e., first-class citizens of the KB, and not auxiliary features, and (3) our model represents uncertainty in them, supporting the missing values and facilitating recovery of missing values.",3 Related Work,[0],[0]
"To evaluate the performance of our multimodal relational embeddings approach, we provide two new benchmarks by extending existing datasets.",4 Evaluation Benchmarks,[0],[0]
"Table 1 provides the statistics of these datasets.
",4 Evaluation Benchmarks,[0],[0]
"MovieLens-100k dataset (Harper and Konstan, 2016) is a popular benchmark in recommendation systems to predict user ratings with contextual features, containing around 1000 users on 1700 movies.",4 Evaluation Benchmarks,[0],[0]
"MovieLens already contains rich relational data about occupation, gender, zip code, and age for users and genre, release date, and the titles for movies.",4 Evaluation Benchmarks,[0],[0]
We augment this data with movie posters collected from TMDB (https:// www.themoviedb.org/).,4 Evaluation Benchmarks,[0],[0]
"We treat the 5-point ratings as five different relations in KB triple format, i.e., 〈user, r = 5,movie〉, and evaluate the rating predictions as other relations are introduced.
",4 Evaluation Benchmarks,[0],[0]
"YAGO-10 Even though MovieLens has a variety of data types, it is still quite small, and is over a specialized domain.",4 Evaluation Benchmarks,[0],[0]
"We also consider a second dataset that is much more appropriate for knowledge graph completion and is popular for link prediction, the YAGO3-10 knowledge graph (Suchanek et al., 2007; Nickel et al., 2012).",4 Evaluation Benchmarks,[0],[0]
"This graph consists of around 120,000 entities, such as people, locations, and organizations, and 37 relations, such as kinship, employment, and residency, and thus much closer to the traditional information extraction goals.",4 Evaluation Benchmarks,[0],[0]
"We extend this dataset with the textual description (as an additional relation) and the images associated with each entity (for half of the entities), provided by DBpedia (Lehmann et al., 2015).",4 Evaluation Benchmarks,[0],[0]
We also include additional relations such as wasBornOnDate that have dates as values.,4 Evaluation Benchmarks,[0],[0]
"In this section, we first evaluate the ability of MKBE to utilize the multimodal information by comparing to DistMult and ConvE through a variety of tasks.",5 Experiment Results,[0],[0]
"Then, by considering the recovery of missing multimodal values (text, images, and numerical) as the motivation, we examine the capability of our models in generation.",5 Experiment Results,[0],[0]
"Details of the hyperparameters and model configurations is provided in the supplementary material, and the source code and the datasets to reproduce the results is available at https://github.com/pouyapez/mkbe.",5 Experiment Results,[0],[0]
"In this section, we evaluate the capability of MKBE in the link prediction task.",5.1 Link Prediction,[0],[0]
"The goal is to calculate MRR and Hits@ metric (ranking evaluations) of recovering the missing entities from triples in the test dataset, performed by ranking all the entities and computing the rank of the correct entity.",5.1 Link Prediction,[0],[0]
"Similar to previous work, here we focus on providing the results in a filtered setting, that is we only rank triples in the test data against the ones that never appear in either train or test datasets.
",5.1 Link Prediction,[0],[0]
MovieLens-100k We train the model using Rating as the relation between users and movies.,5.1 Link Prediction,[0],[0]
"We use a character-level GRU for the movie titles, a separate feed-forward network for age, zip code, and release date, and finally, we use a VGG network on the posters (for every other relation we use a dense layer).",5.1 Link Prediction,[0],[0]
"Table 2 shows the link (rating) prediction
evaluation on MovieLens when test data is consisting only of rating triples.",5.1 Link Prediction,[0],[0]
We calculate our metrics by ranking the five relations that represent ratings instead of object entities.,5.1 Link Prediction,[0],[0]
"We label models that use ratings as R, movie-attributes as M, user-attributes as U, movie titles as T, and posters as P. As shown, the model R+M+U+T outperforms others with a considerable gap demonstrating the importance of incorporating extra information.",5.1 Link Prediction,[0],[0]
"Hits@1 for the baseline is 40%, matching existing recommendation systems (Guimerà et al., 2012).",5.1 Link Prediction,[0],[0]
"From these results, we see that the models benefit more from titles as compared to the posters.
",5.1 Link Prediction,[0],[0]
YAGO-10 The result of link prediction on our YAGO dataset is provided in Table 3.,5.1 Link Prediction,[0],[0]
"We label models using structured information as S, entitydescription as D, numerical information as N, and entity-image as I. We see that the model that encodes all type of information consistently performs better than other models, indicating that the model is effective in utilizing the extra information.",5.1 Link Prediction,[0],[0]
"On the other hand, the model that uses only text performs the second best, suggesting the entity descriptions contain more information than others.",5.1 Link Prediction,[0],[0]
"It is notable that model S is outperformed by all other models, demonstrating the importance of using different data types for attaining higher accuracy.",5.1 Link Prediction,[0],[0]
"This observation is consistent across both DistMult and ConvE, and the results obtained on ConvE are the new state-of-art for this dataset (as compared to Dettmers et al. (2018)).",5.1 Link Prediction,[0],[0]
"Furthermore, we implement KBLN (Garcia-Duran and Niepert, 2017) and IKRL (Xie et al., 2017) to compare them with our S+N and S+I models.",5.1 Link Prediction,[0],[0]
"Our models outperform these approaches, in part because both of these methods require same multimodal attributes for both of the subject and object in each triple.
",5.1 Link Prediction,[0],[0]
Relation Breakdown We perform additional analysis on the YAGO dataset to gain a deeper understanding of the performance of our model using ConvE method.,5.1 Link Prediction,[0],[0]
"Table 4 compares our models on
some of the most frequent relations.",5.1 Link Prediction,[0],[0]
"As shown, the model that includes textual description significantly benefits isAffiliatedTo, and playsFor relations, as this information often appears in text.",5.1 Link Prediction,[0],[0]
"Moreover, images are useful for hasGender and isMarriedTo, while for the relation isConnectedTo, numerical (dates) are more effective than images.",5.1 Link Prediction,[0],[0]
"Here we present an evaluation on imputing multimodal attributes (text, image and numerical).
",5.2 Imputing Multimodal Attributes,[0],[0]
"Numerical and Categorical Table 5a shows performance of predicting missing numerical attributes in the data, evaluated via holding out 10% of the data.",5.2 Imputing Multimodal Attributes,[0],[0]
We only consider numerical values (dates) that are more recent than 1000AD to focus on more relevant entities.,5.2 Imputing Multimodal Attributes,[0],[0]
"In addition to the neural decoder, we train a search-based decoder as well by considering all 1017 choices in the interval [1000, 2017], and for each triple in the test data, finding the number that the model scores the highest; we use this value to compute the RMSE.",5.2 Imputing Multimodal Attributes,[0],[0]
"As we can see, all info outperform other methods on both datasets, demonstrating MKBE is able to utilize different multimodal values for modeling numerical information.",5.2 Imputing Multimodal Attributes,[0],[0]
"Further, the neural decoder performs better than the search-based one, showing the importance of proper decoder, even for finite, enumerable sets.",5.2 Imputing Multimodal Attributes,[0],[0]
"Along the same line, Table 5b shows genre prediction accuracy on 10% of held-
out MovieLens dataset.",5.2 Imputing Multimodal Attributes,[0],[0]
"Again, the model that uses all the information outperforms other methods.
",5.2 Imputing Multimodal Attributes,[0],[0]
"MovieLens Titles For generating movie titles, we randomly consider 200 of them as test, 100 as validation, and the remaining ones as training data.",5.2 Imputing Multimodal Attributes,[0],[0]
The goal here is to generate titles for movies in the test data using the previously mentioned GAN structure.,5.2 Imputing Multimodal Attributes,[0],[0]
"To evaluate our results we conduct a human experiment on Amazon Mechanical Turk (AMT) asking participant two questions: (1) whether they find the movie title real, and (2) which of the four genres is most appropriate for the given title.",5.2 Imputing Multimodal Attributes,[0],[0]
"We
consider 30 movies each as reference titles, fake titles generated from only ratings as conditional data, and fake titles conditioned on all the information.",5.2 Imputing Multimodal Attributes,[0],[0]
"Further, each question was asked for 3 participants, and the results computed over the majority choice are shown in Table 6.",5.2 Imputing Multimodal Attributes,[0],[0]
"Fake titles generated with all the information are more similar to reference movie titles, demonstrating that the embeddings that have access to more information effectively generate higher-quality titles.
",5.2 Imputing Multimodal Attributes,[0],[0]
YAGO Descriptions,5.2 Imputing Multimodal Attributes,[0],[0]
The goal here is to generate descriptive text for entities from their embeddings.,5.2 Imputing Multimodal Attributes,[0],[0]
"Since the original descriptions can be quite long, we consider first sentences that are less than 30 tokens, resulting in 96, 405 sentences.",5.2 Imputing Multimodal Attributes,[0],[0]
"We randomly consider 3000 of them as test, 3000 as validation, and the remaining as training data for the decoder.",5.2 Imputing Multimodal Attributes,[0],[0]
"To evaluate the quality of the generated descriptions, and whether they are appropriate for the entity, we conduct a user study asking participants if they can guess the realness of sentences and the occupation (entertainer, sportsman, or politician), gender, and age (above or below 35) of the subject entity from the description.",5.2 Imputing Multimodal Attributes,[0],[0]
We provide 30 examples for each model asking each question from 3 participants and calculate the accuracy of the majority vote.,5.2 Imputing Multimodal Attributes,[0],[0]
"The results presented in Table 7 show that the models are fairly competent in informing the users of the entity information, and further, descriptions generated from embeddings that had
access to more information outperforms the model with only structured data.",5.2 Imputing Multimodal Attributes,[0],[0]
"Examples of generated descriptions are provided in Table 8 (in addition to screenshots of user study, more examples of generated descriptions, and MovieLens titles are provided in supplementary materials).
",5.2 Imputing Multimodal Attributes,[0],[0]
"YAGO Images Here, we evaluate the quality of images generated from entity embeddings by humans (31, 520, split into train/text).",5.2 Imputing Multimodal Attributes,[0],[0]
"Similar to descriptions, we conduct a study asking users to guess the realness of images and the occupation, gender, and age of the subject.",5.2 Imputing Multimodal Attributes,[0],[0]
"We provide 30 examples for each model asking each question from 3 participants, and use the majority choice.
",5.2 Imputing Multimodal Attributes,[0],[0]
The results in Table 7 indicate that the images generated with embeddings based on all the information are more accurate for gender and occupation.,5.2 Imputing Multimodal Attributes,[0],[0]
"Guessing age from the images is difficult since the image on DBpedia may not correspond to the age of the person, i.e. some of the older celebrities had photos from their youth.",5.2 Imputing Multimodal Attributes,[0],[0]
Examples of generated images are shown in Table 9.,5.2 Imputing Multimodal Attributes,[0],[0]
An important concern regarding KB embedding approaches is their scalability.,6 Discussion and Limitations,[0],[0]
"While large KBs are a problem for all embedding-based link prediction techniques, MKBE is not significantly worse than existing ones because we treat multimodal information as additional triples.",6 Discussion and Limitations,[0],[0]
"Specifically, although multimodal encoders/decoders are more expensive to train than existing relational models, the cost is still additive as we are effectively increasing the size of the training dataset.
",6 Discussion and Limitations,[0],[0]
"In addition to scalability, there are few other challenges when working with multimodal attributes.",6 Discussion and Limitations,[0],[0]
"Although multimodal evidence provides more information, it is not at all obvious which parts of this additional data are informative for predicting the relational structure of the KB, and the models are prone to overfitting.",6 Discussion and Limitations,[0],[0]
"MKBE builds upon the design of neural encoders and decoders that have been effective for specific modalities, and the results demonstrate that it is able to utilize the information effectively.",6 Discussion and Limitations,[0],[0]
"However, there is still a need to further study models that capture multimodal attributes in a more efficient and accurate manner.
",6 Discussion and Limitations,[0],[0]
"Since our imputing multimodal attributes model is based on GAN structure and the embeddings learned from KB representation, the generated attributes are directly limited by the power of GAN
models and the amount of information in the embedding vectors.",6 Discussion and Limitations,[0],[0]
"Although our generated attributes convey several aspects of corresponding entities, their quality is far from ideal due to the size of our datasets (both of our image and text datasets are order of magnitude smaller than common datasets in the existing text/image genration literature) and the amount of information captured by embedding vectors (the knowledge graphs are sparse).",6 Discussion and Limitations,[0],[0]
"In future, we would like to (1) expand multimodal datasets to have more attributes (use many more entities from YAGO), and (2) instead of using learned embeddings to generate missing attributes, utilize the knowledge graph directly for generation.",6 Discussion and Limitations,[0],[0]
"Motivated by the need to utilize multiple sources of information, such as text and images, to achieve more accurate link prediction, we present a novel neural approach to multimodal relational learning.",7 Conclusion,[0],[0]
"We introduce MKBE, a link prediction model that consists of (1) a compositional encoding component to jointly learn the entity and multimodal embeddings to encode the information available for each entity, and (2) adversarially trained decoding component that use these entity embeddings to impute missing multimodal values.",7 Conclusion,[0],[0]
"We enrich two existing datasets, YAGO-10 and MovieLens-100k, with multimodal information to introduce benchmarks.",7 Conclusion,[0],[0]
"We show that MKBE, in comparison to existing link predictors DistMult and ConvE, can achieve higher accuracy on link prediction by utilizing the multimodal evidence.",7 Conclusion,[0],[0]
"Further, we show that MKBE effectively incorporates relational information to generate high-quality multimodal attributes like images and text.",7 Conclusion,[0],[0]
We have release the datasets and the open-source implementation of our models at https://github.com/pouyapez/mkbe.,7 Conclusion,[0],[0]
"We would like to thank Zhengli Zhao, Robert L. Logan IV, Dheeru Dua, Casey Graff, and the anonymous reviewers for their detailed feedback and suggestions.",Acknowledgements,[0],[0]
This work is supported in part by Allen Institute for Artificial Intelligence (AI2) and in part by NSF award #IIS-1817183.,Acknowledgements,[0],[0]
The views expressed are those of the authors and do not reflect the official policy or position of the funding agencies.,Acknowledgements,[0],[0]
Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data.,abstractText,[0],[0]
"Existing approaches, however, primarily focus on simple link structure between a finite set of entities, ignoring the variety of data types that are often used in knowledge bases, such as text, images, and numerical values.",abstractText,[0],[0]
"In this paper, we propose multimodal knowledge base embeddings (MKBE) that use different neural encoders for this variety of observed data, and combine them with existing relational models to learn embeddings of the entities and multimodal data.",abstractText,[0],[0]
"Further, using these learned embedings and different neural decoders, we introduce a novel multimodal imputation model to generate missing multimodal values, like text and images, from information in the knowledge base.",abstractText,[0],[0]
We enrich existing relational datasets to create two novel benchmarks that contain additional information such as textual descriptions and images of the original entities.,abstractText,[0],[0]
"We demonstrate that our models utilize this additional information effectively to provide more accurate link prediction, achieving state-of-the-art results with a considerable gap of 5-7% over existing methods.",abstractText,[0],[0]
"Further, we evaluate the quality of our generated multimodal values via a user study.",abstractText,[0],[0]
We have release the datasets and the opensource implementation of our models at https: //github.com/pouyapez/mkbe.,abstractText,[0],[0]
Embedding Multimodal Relational Data for Knowledge Base Completion,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2002–2006, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.
The conferences ACL (Association for Computational Linguistics) and EMNLP (Empirical Methods in Natural Language Processing) rank among the premier venues that track the research developments in Natural Language Processing and Computational Linguistics. In this paper, we present a study on the research papers of approximately two decades from these two NLP conferences. We apply keyphrase extraction and corpus analysis tools to the proceedings from these venues and propose probabilistic and vector-based representations to represent the topics published in a venue for a given year. Next, similarity metrics are studied over pairs of venue representations to capture the progress of the two venues with respect to each other and over time.",text,[0],[0]
"Scientific findings in a subject-area are typically published in conferences, journals, patents, and books in that domain.",1 Introduction,[0],[0]
These research documents constitute valuable resources from the perspective of data mining applications.,1 Introduction,[0],[0]
"For instance, the citation links among research documents are used in computing bibliometric quantities for authors (Alonso et al., 2009) whereas topic models on research corpora are used to distinguish between influential and impactful researchers (Kataria et al., 2011) and to capture temporal topic trends (He et al., 2009).
",1 Introduction,[0],[0]
"Despite several potential benefits mentioned above and the free availability of most research
proceedings in NLP through the ACL Anthology1, the topical and temporal aspects of this corpus are yet to be fully studied in current literature.",1 Introduction,[0],[0]
"In this paper, we present our study on research proceedings of approximately two decades from two leading NLP conferences, namely ACL and EMNLP, to complement a previous study on this topic by Hall et al (2008).",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to characterize the developments in the NLP domain using a comparative study of two of its leading publication venues.",1 Introduction,[0],[0]
"Our contributions are summarized below:
1.",1 Introduction,[0],[0]
"We represent the NLP research corpus from approximately two decades as a keyphrasedocument matrix and apply Latent Dirichlet Allocation (Blei et al., 2003) to extract coherent topics from it (Newman et al., 2010).
2.",1 Introduction,[0],[0]
We propose two novel representations for summarizing the venue proceedings in a given year.,1 Introduction,[0],[0]
"(1) The probabilistic representation expresses each venue as a probability distribution over topics, whereas (2) the TPICP representation captures topics that are the major focus in the venue for a particular year via Topic Proportion (TP) as well as topic importance as measured with inverse corpus proportion (ICP).
3.",1 Introduction,[0],[0]
We apply Jensen-Shannon divergence and cosine similarity on our proposed venue representations to analyze the venues over time.,1 Introduction,[0],[0]
"Specifically, we ask the following questions: What are the popular topics in ACL and EMNLP in a particular year?",1 Introduction,[0],[0]
Is the topical focus in EMNLP different from ACL?,1 Introduction,[0],[0]
"How
1https://aclweb.org/anthology/
2002
did the topical focus in each venue change over time?
",1 Introduction,[0],[0]
Organization: We describe our novel venue representations and the measures used to compare them in Section 2.,1 Introduction,[0],[0]
The details of our datasets and experiments are presented in Section 3 along with results and observations.,1 Introduction,[0],[0]
We summarize related research in Section 4 before concluding the paper in Section 5.,1 Introduction,[0],[0]
"Let Y = {y1, y2 . . .",2 Methods,[0],[0]
"yT } be the consecutive years in which the research proceedings are available from V , set of publication venues under consideration (V = {“ACL”, “EMNLP”} in this paper).",2 Methods,[0],[0]
"If D is the set of all documents over the years, each document d ∈ D is associated with {Kd, y, v} where Kd refers to the content of d whereas v and y refer to the venue and year in which d was published.",2 Methods,[0],[0]
"Let t1, t2 . . .",2.1 Venues as Probability Distributions,[0],[0]
"tk denote the topics capturing the content of documents in D. Using probabilistic topic modeling and dimension reduction tools such as Latent Dirichlet Allocation or pLSA (Hofmann, 1999; Blei et al., 2003), we extract for each d ∈ D, P (ti|d), i = 1 . . .",2.1 Venues as Probability Distributions,[0],[0]
"k, the multinomial distribution over the topics associated with d.
The venue-topic probability distribution P (ti|vy) for a given (venue, year) pair (v = l, y = m) can be computed using Dl,m, the set of documents published in venue l, in the year m. That is,
Pl,m(ti) = 1 |Dl,m| ∑
d∈Dl,m P (ti|d) (1)
Note that the above probabilistic representation facilitates a quantitative measure to compare two venues: the divergence between the probability distributions of the two venues.",2.1 Venues as Probability Distributions,[0],[0]
"The Kullback−Leibler divergence (KLD) between two (discrete) probability distributions P and Q is given by: DKL(P ||Q) =
∑ i P (i)log P (i)Q(i) .",2.1 Venues as Probability Distributions,[0],[0]
"Due
to the unsymmetric nature of KLD, we use the Jensen-Shannon divergence, a symmetric and finite measure (0 ≤ JSD(P ||Q)",2.1 Venues as Probability Distributions,[0],[0]
≤ 1) based on KLD.,2.1 Venues as Probability Distributions,[0],[0]
"Let M = 12 (P + Q),
JSD(P ||Q)",2.1 Venues as Probability Distributions,[0],[0]
= 1 2,2.1 Venues as Probability Distributions,[0],[0]
[DKL(P ||M) + DKL(Q||M)],2.1 Venues as Probability Distributions,[0],[0]
Discrete probability distributions are often represented in computations as normalized vectors.,2.2 Venues as TP-ICP Vectors,[0],[0]
"For instance, the P (ti|v) values comprise the components of a k-dimensional vector.",2.2 Venues as TP-ICP Vectors,[0],[0]
"This topic proportion (TP) vector is similar to the normalized term frequency vector commonly used in Information Retrieval (IR) (Manning et al., 2008).",2.2 Venues as TP-ICP Vectors,[0],[0]
TP values are fractions indicating the percentage of a given topic among all topics covered in a particular year.,2.2 Venues as TP-ICP Vectors,[0],[0]
"Thus these values are higher for topics that are the major focus in the venue for a particular year .
",2.2 Venues as TP-ICP Vectors,[0],[0]
"We also extend inverse document frequency, a popular concept that is used to weigh terms in IR (Manning et al., 2008) to describe Inverse Corpus Proportion or ICP.",2.2 Venues as TP-ICP Vectors,[0],[0]
Our objective in defining ICP is to capture the importance of a topic by diminishing the effect of topics that are common across all years.,2.2 Venues as TP-ICP Vectors,[0],[0]
"Let TPv,y(i) represents the proportion of topic ti in venue v for year y, then ICP (ti) = log ( |Y |∑ y=1 |V",2.2 Venues as TP-ICP Vectors,[0],[0]
"|∑ v=1 k∑ j=1 TPv,y(j)
|Y |∑ y=1 |V |∑ v=1 TPv,y(i)
) =",2.2 Venues as TP-ICP Vectors,[0],[0]
( |D| |Y |∑ y=1 |V,2.2 Venues as TP-ICP Vectors,[0],[0]
"|∑ v=1 TPv,y(i) )
",2.2 Venues as TP-ICP Vectors,[0],[0]
"since k∑
j=1 TP(j) = 1, TP being a probability dis-
tribution vector and |Y | × |V | = |D|.",2.2 Venues as TP-ICP Vectors,[0],[0]
The TPICP vector for a venue is defined as: [TP (1) ×,2.2 Venues as TP-ICP Vectors,[0],[0]
"ICP (1), . . .",2.2 Venues as TP-ICP Vectors,[0],[0]
TP (k) ×,2.2 Venues as TP-ICP Vectors,[0],[0]
ICP (k)] and captures in each component the weighted proportion of a topic in that venue for a year.,2.2 Venues as TP-ICP Vectors,[0],[0]
This novel representation can be considered the topic-level counterpart of the popular TF-IDF representation in IR.,2.2 Venues as TP-ICP Vectors,[0],[0]
Given two TP-ICP vectors P =,2.2 Venues as TP-ICP Vectors,[0],[0]
"[p1, p2, . . .",2.2 Venues as TP-ICP Vectors,[0],[0]
pk] and Q =,2.2 Venues as TP-ICP Vectors,[0],[0]
"[q1, q2, . . .",2.2 Venues as TP-ICP Vectors,[0],[0]
"qk], the similarity between them using the cosine measure is given by:
cosine(P,Q) =
k∑ i=1",2.2 Venues as TP-ICP Vectors,[0],[0]
"pi.qi
||P ||2.||Q||2",2.2 Venues as TP-ICP Vectors,[0],[0]
"Corpus analysis tools often use bag-of-words models and term frequencies for representing documents (Heinrich, 2005).",2.3 Keyphrases for representing documents,[0],[0]
"However, research documents are often well-structured, and contain various sections with author information, citations,
and content-related sections such as abstract, related work, and experiments.",2.3 Keyphrases for representing documents,[0],[0]
"To best represent the topical content of these documents, we harness the latest work on keyphrase extraction for research documents and represent documents using keyphrases (Hasan and Ng, 2014).
",2.3 Keyphrases for representing documents,[0],[0]
"We use the ExpandRank algorithm (Wan and Xiao, 2008) to extract top n-grams ∀d ∈",2.3 Keyphrases for representing documents,[0],[0]
"D. ExpandRank effectively combines PageRank values on word graphs with text similarity scores between documents to score n-grams for a document and was shown to outperform other unsupervised keyphrase extraction methods on research documents in absence of other information such as citations (Gollapalli and Caragea, 2014).",2.3 Keyphrases for representing documents,[0],[0]
"Datasets and setup: We crawled the ACLWeb for research papers from EMNLP and ACL from the year 1996 through 20142 using the Java-based crawler, Heritrix3.",3 Experiments,[0],[0]
"The text from the PDF documents was extracted using the PDFBox software4 after which simple rules similar to the ones used in CiteSeer (Li et al., 2006) were employed to extract the “body” of the research document5.",3 Experiments,[0],[0]
The numbers of papers for each year at the end of this process are listed in Table 2.,3 Experiments,[0],[0]
"From these numbers,
2 Since our goal is to compare the two venues, we start from 1996 when EMNLP branched off into a full conference from a workshop on Very Large Corpora although ACL proceedings are available from 1979.
",3 Experiments,[0],[0]
"3 https://webarchive.jira.com/wiki/display/Heritrix/Heritrix 4 https://pdfbox.apache.org/ 5 Processed data available upon request.
",3 Experiments,[0],[0]
"it appears that the paper “intake” in each conference has gone up overall during the last decade although occasionally the increase is due to colocation with related conferences such as IJCNLP and HLT6.
We construct the keyphrase-document matrix using top-100 keyphrases of each document extracted with ExpandRank.",3 Experiments,[0],[0]
"The LDA implementation provided in Mallet (McCallum, 2002) was used to extract topics from this matrix.",3 Experiments,[0],[0]
"The LDA algorithm was run along with hyperparameter optimization (Minka, 2003) for different numbers of topics between 10 . . .",3 Experiments,[0],[0]
100 in increments of 10.,3 Experiments,[0],[0]
"We use the average corpus likelihood over ten randomly-initialized runs to choose the optimal number of topics that best “explain” the corpus (Heinrich, 2005).",3 Experiments,[0],[0]
As indicated by the left plot in Figure 1 this optimum is obtained when the number of topics is 30.,3 Experiments,[0],[0]
The top phrases that reflect the “theme” captured by a topic are shown in Table 1.,3.1 Results and Observations,[0],[0]
"As indicated in this table, we are able to extract coherent topics from the corpus using LDA on a documentkeyphrase matrix (AlSumait et al., 2009; Newman et al., 2010).
",3.1 Results and Observations,[0],[0]
"Top research topics in NLP: We select five timepoints {1996, 2000, 2005, 2010, 2014} by splitting the 1996-2014 period into roughly-
6 ACL was co-located with related conferences in the years 1997, 1998, 2006,
2008, and 2009 and EMNLP in the years 2005, 2007, and 2012.
equal parts and examine the top topics for ACL and EMNLP at these time points.",3.1 Results and Observations,[0],[0]
We rank the topics in each conference by their TP-ICP values and list the top 3 topics in the right table of Figure 1.,3.1 Results and Observations,[0],[0]
"“Semantic relation extraction”, “sentiment analysis”, and “topic models” are the top research topics in NLP last year (2014) whereas in the year 1996, the topics “noun phrase extraction”, “summarization”, “corpus modeling”, and “speech recognition” dominated the NLP research arena.",3.1 Results and Observations,[0],[0]
"From the table, it can be seen that “information retrieval” (topicID: 18) ranks among the top topics in EMNLP for all three timepoints during 2000-2010 whereas “natural language generation” (topicID: 9) was consistently addressed during 1996-2005 in ACL.
",3.1 Results and Observations,[0],[0]
EMNLP versus ACL: We compare the venues using JSD and Cosine similarity measures in the middle plot of Figure 1.,3.1 Results and Observations,[0],[0]
The plot shows decreasing divergence between the topical distributions over the years and increasing cosine similarity between the TP-ICP vectors for the venues.,3.1 Results and Observations,[0],[0]
These trends indicate that over the two decades the two venues ACL and EMNLP seem to have “become like each other” although their topical focus was different during the initial years.,3.1 Results and Observations,[0],[0]
"Increasingly, both venues seem to publish papers on similar top-
ics.",3.1 Results and Observations,[0],[0]
"This behavior could be interpreted to mean that the NLP research field is more stable now with two of its leading conferences addressing problems on similar topics.
",3.1 Results and Observations,[0],[0]
"Changing topical focus over the years: In the first plot of Figure 2, we show the JensenShannon divergence between the topic distributions of a particular venue for a given year y",3.1 Results and Observations,[0],[0]
and (y − 1),3.1 Results and Observations,[0],[0]
", the year preceding it.",3.1 Results and Observations,[0],[0]
"The curve indicates that in the years between 1997-2008, the rate of change from year to year is higher than in the years following 2008.",3.1 Results and Observations,[0],[0]
"We split the time period 1996-2014 into five roughly-equal parts to form the set {1996, 2000, 2005, 2010, 2014}.",3.1 Results and Observations,[0],[0]
The JSD between the distribution in a particular year and the years preceding it in the above set is shown for ACL (middle plot) and EMNLP (right plot) in Figure 2.,3.1 Results and Observations,[0],[0]
"For example, the first cluster in the middle plot, shows the JSD values between the distributions for the years 2000, 2005, 2010, 2014 with the starting year 1996 for ACL.",3.1 Results and Observations,[0],[0]
"For both venues, the divergences of a given year are higher with the early starting years 1996 and 2000 than with the later starting years 2005 and 2010, indicating that the topics being addressed currently in NLP research are significantly different from those addressed a decade back.",3.1 Results and Observations,[0],[0]
Temporal analysis of corpora is an upcoming research topic in text mining groups.,4 Related Work,[0],[0]
"Topic models were particularly investigated for detecting activity patterns in corpora annotated with time information (Huynh et al., 2008; Shen et al., 2009).",4 Related Work,[0],[0]
"Evolution of topics and their trends were studied on research corpora from NIPS (Wang and McCallum, 2006) as well as CiteSeer (He et al., 2009).
",4 Related Work,[0],[0]
"In contrast with existing approaches that seek to model the detection of new topics and their evolution, we focus on representing different venues pertaining to a research field and examine their development over time by comparing them against each other.",4 Related Work,[0],[0]
"In a similar study, Hall et al. (2008) examined the emergence of topics in NLP literature.",4 Related Work,[0],[0]
They proposed “topic entropy” to measure the diversity in three conferences from the ACL Anthology during the years 1978-2006.,4 Related Work,[0],[0]
They also noted that all the venues seem to converge in the topics they cover over the years based on the JSD between their topic distributions.,4 Related Work,[0],[0]
We presented our study on research proceedings of approximately two decades from the leading NLP conference venues: EMNLP and ACL.,5 Conclusions,[0],[0]
We extracted coherent topics from this corpus by applying topic modeling on the corresponding keyphrase-document matrix.,5 Conclusions,[0],[0]
"Next, the extracted topics were used to characterize each venue through probabilistic and vector representations and compared against each other and over the years using various similarity measures.",5 Conclusions,[0],[0]
"To the best of our knowledge, we are the first to present insights related to the development of a research field by studying two leading conferences in the area using various techniques from NLP and IR.",5 Conclusions,[0],[0]
The conferences ACL (Association for Computational Linguistics) and EMNLP (Empirical Methods in Natural Language Processing) rank among the premier venues that track the research developments in Natural Language Processing and Computational Linguistics.,abstractText,[0],[0]
"In this paper, we present a study on the research papers of approximately two decades from these two NLP conferences.",abstractText,[0],[0]
We apply keyphrase extraction and corpus analysis tools to the proceedings from these venues and propose probabilistic and vector-based representations to represent the topics published in a venue for a given year.,abstractText,[0],[0]
"Next, similarity metrics are studied over pairs of venue representations to capture the progress of the two venues with respect to each other and over time.",abstractText,[0],[0]
EMNLP versus ACL: Analyzing NLP Research Over Time,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 718–728 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1067
Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives. However, progress on emotion detection has been hampered by the absence of large labeled datasets. In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it. We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%). We also extend the task beyond emotion types to model Robert Plutchik’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.",text,[0],[0]
"According to the Oxford English Dictionary, emotion is defined as “[a] strong feeling deriving from one’s circumstances, mood, or relationships with others.”",1 Introduction,[0],[0]
"1 This “standard” definition identifies emotions as constructs involving something innate that is often invoked in social interactions and that aids in communicating with others(Hwang and Matsumoto, 2016).",1 Introduction,[0],[0]
"It is no exaggeration that humans are emotional beings: Emotions are an integral part of human life, and affect our decision making as well as our mental and physical health.",1 Introduction,[0],[0]
"As such, developing emotion detection models is important; they have a wide array of applications, ranging from building nuanced virtual assistants that cater for the emotions of their users to detecting the emotions of social media users in order to understand their mental and/or physical health.
",1 Introduction,[0],[0]
"1https://en.oxforddictionaries.com/ definition/emotion.
",1 Introduction,[0],[0]
"However, emotion detection has remained a challenging task, partly due to the limited availability of labeled data and partly due the controversial nature of what emotions themselves are (Aaron C. Weidman and Tracy, 2017).
",1 Introduction,[0],[0]
"Recent advances in machine learning for natural language processing (NLP) suggest that, given enough labeled data, there should be an opportunity to build better emotion detection models.",1 Introduction,[0],[0]
"Manual labeling of data, however, is costly and so it is desirable to develop labeled emotion data without annotators.",1 Introduction,[0],[0]
"While the proliferation of social media has made it possible for us to acquire large datasets with implicit labels in the form of hashtags (Mohammad and Kiritchenko, 2015), such labels are noisy and reliable.
",1 Introduction,[0],[0]
"In this work, we seek to enable deep learning by creating a large dataset of fine-grained emotions using Twitter data.",1 Introduction,[0],[0]
"More specifically, we harness cues in Twitter data in the form of emotion hashtags as a way to build a labeled emotion dataset that we then exploit using distant supervision (Mintz et al., 2009) (the use of hashtags as a surrogate for annotator-generated emotion labels) to build emotion models grounded in psychology.",1 Introduction,[0],[0]
"We construct such a dataset and exploit it using powerful deep learning methods to build accurate, high coverage models for emotion prediction.",1 Introduction,[0],[0]
"Overall, we make the following contributions: 1) Grounded in psychological theory of emotions, we build a large-scale, high quality dataset of tweets labeled with emotions.",1 Introduction,[0],[0]
"Key to this are methods to ensure data quality, 2) we validate the data collection method using human annotations, 3) we develop powerful deep learning models using a gated recurrent network to exploit the data, yielding new state-of-the-art on 24 fine-grained types of emotions, and 4) we extend the task beyond these emotion types to model Plutick’s 8 primary emotion dimensions.
",1 Introduction,[0],[0]
"718
Our emotion modeling relies on distant supervision (Read, 2005; Mintz et al., 2009), the approach of using cues in data (e.g., hashtags or emoticons) as a proxy for “ground truth” labels as we explained above.",1 Introduction,[0],[0]
"Distant supervision has been investigated by a number of researchers for emotion detection (Tanaka et al., 2005; Mohammad, 2012; Purver and Battersby, 2012; Wang et al., 2012; Pak and Paroubek, 2010; Yang et al., 2007) and for other semantic tasks such as sentiment analysis (Read, 2005; Go et al., 2009) and sarcasm detection (González-Ibánez et al., 2011).",1 Introduction,[0],[0]
"In these works, authors successfully use emoticons and/or hashtags as marks to label data after performing varying degrees of data quality assurance.",1 Introduction,[0],[0]
"We take a similar approach, using a larger collection of tweets, richer emotion definitions, and stronger filtering for tweet quality.
",1 Introduction,[0],[0]
"The remainder of the paper is organized as follows: We first overview related literature in Section 2, describe our data collection in Section 3.1, and the annotation study we performed to validate our distant supervision method in Section 4.",1 Introduction,[0],[0]
"We then describe our methods in Section 5, provide results in Section 6, and conclude in Section 8.",1 Introduction,[0],[0]
"The SemEval-2007 Affective Text task (Strapparava and Mihalcea, 2007)",2.1 Computational Treatment of Emotion,[0],[0]
"[SEM07] focused on classification of emotion and valence (i.e., positive and negative texts) in news headlines.",2.1 Computational Treatment of Emotion,[0],[0]
"A total of 1,250 headlines were manually labeled with the 6 basic emotions of Ekman (Ekman, 1972) and made available to participants.",2.1 Computational Treatment of Emotion,[0],[0]
"Similarly, (Aman and Szpakowicz, 2007) describe an emotion annotation task of identifying emotion category, emotion intensity and the words/phrases that indicate emotion in blog post data of 4,090 sentences and a system exploiting the data.",2.1 Computational Treatment of Emotion,[0],[0]
"Our work differs from both that of SEM07 (Strapparava and Mihalcea, 2007) and (Aman and Szpakowicz, 2007) in that we focus on a different genre (i.e., Twitter) and investigate distant supervision as a way to acquire a significantly larger labeled dataset.
",2.1 Computational Treatment of Emotion,[0],[0]
"Our work is similar to (Mohammad, 2012; Mohammad and Kiritchenko, 2015), (Wang et al., 2012), and (Volkova and Bachrach, 2016) who use distant supervision to acquire Twitter data with emotion hashtags and report analyses and experiments to validate the utility of this approach.",2.1 Computational Treatment of Emotion,[0],[0]
"For
example, (Mohammad, 2012) shows that by using a simple domain adaptation method to train a classifier on their data they are able to improve both precision and recall on the SemEval-2007 (Strapparava and Mihalcea, 2007) dataset.",2.1 Computational Treatment of Emotion,[0],[0]
"As the author points out, this is another premise that the selflabeled hashtags acquired from Twitter are consistent, to some degree, with the emotion labels given by the trained human judges who labeled the SemEval-2007 data.",2.1 Computational Treatment of Emotion,[0],[0]
"As pointed out earlier, (Wang et al., 2012) randomly sample a set of 400 tweets from their data and human-label as relevant/irrelevant, as a way to verify the distant supervision approach with the quality assurance heuristics they employ.",2.1 Computational Treatment of Emotion,[0],[0]
"The authors found that the precision on a test set is 93.16%, thus confirming the utility of the heuristics.",2.1 Computational Treatment of Emotion,[0],[0]
"(Wang et al., 2012) provide a number of important observations, as conclusions based on their work.",2.1 Computational Treatment of Emotion,[0],[0]
"These include that since they are provided by the tweets’ writers, the emotion hashtags are more natural and reliable than the emotion labels traditionally assigned by annotators to data by a few annotators.",2.1 Computational Treatment of Emotion,[0],[0]
"This is the case since in the lab-condition method annotators need to infer the writers emotions from text, which may not be accurate.",2.1 Computational Treatment of Emotion,[0],[0]
"Additionally, (Volkova and Bachrach, 2016) follow the same distant supervision approach and find correlations of users’ emotional tone and the perceived demographics of these users’ social networks exploiting the emotion hashtag-labeled data.",2.1 Computational Treatment of Emotion,[0],[0]
"Our dataset is more than an order of magnitude larger than (Mohammad, 2012) and (Volkova and Bachrach, 2016) and the range of emotions we target is much more fine grained than (Mohammad, 2012; Wang et al., 2012; Volkova and Bachrach, 2016) since we model 24 emotion types, rather than focus on ≤ 7 basic emotions.
",2.1 Computational Treatment of Emotion,[0],[0]
"(Yan et al., 2016; Yan and Turtle, 2016a,b) develop a dataset of 15,553 tweets labeled with 28 emotion types and so target a fine-grained range as we do.",2.1 Computational Treatment of Emotion,[0],[0]
"The authors instruct human annotators under lab conditions to assign any emotion they feel is expressed in the data, allowing them to assign more than one emotion to a given tweet.",2.1 Computational Treatment of Emotion,[0],[0]
A set of 28 chosen emotions was then decided upon and further annotations were performed using Amazon Mechanical Turk (AMT).,2.1 Computational Treatment of Emotion,[0],[0]
"The authors cite an agreement of 0.50 Krippendorff’s alpha (α) between the lab/expert annotators, and an (α) of 0.28 between experts and AMT workers.",2.1 Computational Treatment of Emotion,[0],[0]
"EmoTweet-
28 is a useful resource.",2.1 Computational Treatment of Emotion,[0],[0]
"However, the agreement between annotators is not high and the set of assigned labels do not adhere to a specific theory of emotion.",2.1 Computational Treatment of Emotion,[0],[0]
We use a much larger dataset and report an accuracy of the hashtag approach at 90% based on human judgement as reported in Section 4.,2.1 Computational Treatment of Emotion,[0],[0]
A number of studies have also been performed to analyze and/or model mood in social media data.,2.2 Mood,[0],[0]
"(De Choudhury et al., 2012) identify more than 200 moods frequent on Twitter as extracted from psychological literature and filtered by AMT workers.",2.2 Mood,[0],[0]
They then collect tweets which have one of the moods in their mood lexicon in the form of a hashtag.,2.2 Mood,[0],[0]
"To verify the quality of the mood data, the authors run AMT studies where they ask workers whether a tweet displayed the respective mood hashtag or not and find that in 83% of the cases hashtagged moods at the end of posts did capture users’ moods, whereas for posts with mood hashtags anywhere in the tweet, only 58% of the cases capture the mood of users.",2.2 Mood,[0],[0]
"Although they did not build models for mood detection, the annotation studies (De Choudhury et al., 2012) perform further support our specific use of hashtags to label emotions.",2.2 Mood,[0],[0]
"(Mishne and De Rijke, 2006) collect user-labeled mood from blog post text on LiveJournal and exploit them for predicting the intensity of moods over a time span rather than at the post level.",2.2 Mood,[0],[0]
"Similarly, (Nguyen, 2010) builds models to infer patterns of moods in a large collection of LiveJournal posts.",2.2 Mood,[0],[0]
"Some of the moods in these LiveJournal studies (e.g., hungry, cold), as (De Choudhury et al., 2012) explain, would not fit any psychological theory.",2.2 Mood,[0],[0]
Our work is different in that it is situated in psychological theory of emotion.,2.2 Mood,[0],[0]
"In spite of the effectiveness of feature engineering for NLP, it is a labor intensive task that also needs domain expertise.",2.3 Deep Learning for NLP,[0],[0]
"More importantly, feature engineering falls short of extracting and organizing all the discriminative information from data (LeCun et al., 2015; Goodfellow et al., 2016).",2.3 Deep Learning for NLP,[0],[0]
"Neural networks (Goodfellow et al., 2016) have emerged as a successful class of methods that has the power of automatically discovering the representations needed for detection or classification and has been successfully applied to multiple NLP tasks.",2.3 Deep Learning for NLP,[0],[0]
"A line of studies in the literature (e.g., (Labutov and Lip-
son, 2013; Maas et al., 2011; Tang et al., 2014b,a) aim to learn sentiment-specific word embeddings (Bengio et al., 2003; Mikolov et al., 2013) from neighboring text.",2.3 Deep Learning for NLP,[0],[0]
"Another thread of research focuses on learning semantic composition (Mitchell and Lapata, 2010), including extensions to phrases and sentences with recursive neural networks (a class of syntax-tree models) (Socher et al., 2013; Irsoy and Cardie, 2014; Li et al., 2015) and to documents with distributed representations of sentences and paragraphs (Le and Mikolov, 2014; Tang et al., 2015) for modeling sentiment.
",2.3 Deep Learning for NLP,[0],[0]
"Long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Neural Nets (GRNNs) (Cho et al., 2014; Chung et al., 2015), variations of recurrent neural networks (RNNs), a type of networks suitable for handling time-series data like speech (Graves et al., 2013) or handwriting recognition (Graves, 2012; Graves and Schmidhuber, 2009), have also been used successfully for sentiment analysis (Ren et al., 2016; Liu et al., 2015; Tai et al., 2015; Tang et al., 2015; Zhang et al., 2016).",2.3 Deep Learning for NLP,[0],[0]
"Convolutional neural networks (CNNs) have also been quite successful in NLP, and have been applied to a range of sentence classification tasks, including sentiment analysis (Blunsom et al., 2014; Kim, 2014; Zhang et al., 2015).",2.3 Deep Learning for NLP,[0],[0]
"Other architectures have also been recently proposed (e.g., (Bradbury et al., 2016)).",2.3 Deep Learning for NLP,[0],[0]
"A review of neural network methods for NLP can be found in (Goldberg, 2016).",2.3 Deep Learning for NLP,[0],[0]
"To be able to use deep learning for modeling emotion, we needed a large dataset of labeled tweets.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"Since there is no such human-labeled dataset publicly available, we follow (Mohammad, 2012; Mintz et al., 2009; Purver and Battersby, 2012; González-Ibánez et al., 2011; Wang et al., 2012) in adopting distant supervision: We collect tweets with emotion-carrying hashtags as a surrogate for emotion labels.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"To be able to collect enough tweets to serve our need, we developed a list of hashtags representing each of the 24 emotions proposed by Robert Plutchick (Plutchik, 1980, 1985, 1994).",3.1 Collection of a Large-Scale Dataset,[0],[0]
"Plutchik (Plutchik, 2001) organizes emotions in a three-dimensional circumplex model analogous to the colors on a color wheel.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"The cone’s vertical dimension represents intensity, and the 3 circle represent degrees of similarity
among the various emotion types.",3.1 Collection of a Large-Scale Dataset,[0],[0]
The eight sectors are meant to capture that there are eight primary emotion dimensions arranged as four pairs of opposites.,3.1 Collection of a Large-Scale Dataset,[0],[0]
"Emotions in the blank spaces are the primary emotion dyads (i.e., emotions that are mixtures of two of the primary emotions).",3.1 Collection of a Large-Scale Dataset,[0],[0]
"For this work, we exclude the dyads in the exploded model from our treatment.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"For simplicity, we refer to the circles as plutchik-1: with the emotions {admiration, amazement, ecstasy, grief, loathing, rage, terror, vigilance}, plutchik-2: with the emotions {joy, trust, fear, surprise, sadness, disgust, anger, anticipation}, and plutchik-3: with the emotions {acceptance, annoyance, apprehension, boredom, distraction, interest, pensiveness, serenity}.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"The wheel is shown in Figure 1.
",3.1 Collection of a Large-Scale Dataset,[0],[0]
"For each emotion type, we prepared a seed set of hashtags representing the emotion.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"We used Google synonyms and other online dictionaries and thesauri (e.g., www.thesaurus. com) to expand the initial seed set of each emotion.",3.1 Collection of a Large-Scale Dataset,[0],[0]
We acquire a total of 665 emotion hashtags across the 24 emotion types.,3.1 Collection of a Large-Scale Dataset,[0],[0]
"For example, for the joy emotion, a subset of the seeds in our expanded set is {“happy”, “happiness”, “joy”, “joyful”, “joyfully”, “delighted”, “feelingsunny”, “blithe”, “beatific”, “exhilarated”, “blissful”, “walkingonair”, “jubilant”}.",3.1 Collection of a Large-Scale Dataset,[0],[0]
We then used the expanded set to extract tweets with hashtags from the set from a number of massive-scale in-house Twitter datasets.,3.1 Collection of a Large-Scale Dataset,[0],[0]
We also used Twitter API to crawl Twitter with hashtags from the expanded set.,3.1 Collection of a Large-Scale Dataset,[0],[0]
"Using this method, we were able to acquire a dataset of about 1/4 billion tweets covering an extended time span from July 2009 till January 2017.",3.1 Collection of a Large-Scale Dataset,[0],[0]
"Twitter data are very noisy, not only because of use of non-standard typography (which is less of a problem here) but due to the many duplicate tweets and the fact that tweets often have multiple emotion hashtags.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Since these reduce our ability to build accurate models, we need to clean the data and remove duplicates.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Starting with > 1/4 billion tweets, we employ a rigorous and strict pipeline.",3.2 Preprocessing and Quality Assurance,[0],[0]
"This results in a vastly smaller set of about 1.6 million dependable labeled tweets.
",3.2 Preprocessing and Quality Assurance,[0],[0]
"Since our goal is to create non-overlapping categories at the level of a tweet, we first removed all tweets with hashtags belonging to more than one emotion of the 24 emotion categories.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Since it was observed (e.g., (Mohammad, 2012; Wang et al., 2012)) and also confirmed by our annotation study as described in Section 4, that hashtags in tweets with URLs are less likely to correlate with a true emotion label, we remove all tweets with URLs from our data.",3.2 Preprocessing and Quality Assurance,[0],[0]
We filter out duplicates using a two-step procedure: 1) we remove all retweets (based on existence of the token “RT” regardless of case) and 2) we use the Python library pandas http://pandas.,3.2 Preprocessing and Quality Assurance,[0],[0]
pydata.org/ “drop duplicates” method to compare the tweet texts of all the tweets after normalizing character repetitions [all consecutive characters of > 2 to 2] and user mentions (as detected by a string starting with an “@” sign).,3.2 Preprocessing and Quality Assurance,[0],[0]
"We then performed a manual inspection of a random sample of 1,000 tweets from the data and found no evidence of any remaining tweet duplicates.
",3.2 Preprocessing and Quality Assurance,[0],[0]
"Next, even though the emotion hashtags themselves are exclusively in English, we observe the data do have tweets in languages other than English.",3.2 Preprocessing and Quality Assurance,[0],[0]
"This is due to code-switching, but also to the fact that our data dates back to 2009 and Twitter did not allow use of hashtags for several non-English languages until 2012.",3.2 Preprocessing and Quality Assurance,[0],[0]
"To filter out non-English, we use the langid (Lui and Baldwin, 2012) (https://github.com/ saffsd/langid.py) library to assign language tags to the tweets.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Since the common wisdom in the literature (e.g., (Mohammad, 2012; Wang et al., 2012)) is to restrict data to hashtags occurring in final position of a tweet, we investigate correlations between a tweet’s relevance and emotion hashtag location in Section 4 and test models exclusively on data with hashtags occurring in final position.",3.2 Preprocessing and Quality Assurance,[0],[0]
"We also only use tweets con-
taining at least 5 words.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Table 2 shows statistics of the data after applying our cleaning, filtering, language identification, and deduplication pipeline.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Since our focus is on English, we only show statistics for tweets tagged with an “en” (for “English”) label by langid.",3.2 Preprocessing and Quality Assurance,[0],[0]
"Table 2 provides three types of relevant statistics: 1) counts of all tweets, 2) counts of tweets with at least 5 words and the emotion hashtags occurring in the last quarter of the tweet text (based on character count), and 3) counts of tweets with at least 5 words and the emotion hashtags occurring as the final word in the tweet text.",3.2 Preprocessing and Quality Assurance,[0],[0]
"As the last column in Table 2 shows, employing our most strict criterion where an emotion hashtag must occur finally in a tweet of a minimal length 5 words, we acquire a total of 1,608,233 tweets: 205,125 tweets for plutchik-1, 790,059 for plutchik-2, and 613,049 for plutchik-3. 2",3.2 Preprocessing and Quality Assurance,[0],[0]
"In their work, (Wang et al., 2012) manually label a random sample of 400 tweets extracted with hash-
2The data can be acquired by emailing the first author.",4 Annotation Study,[0],[0]
"The distribution is in the form of tweet ids and labels, to adhere to Twitter conditions.
",4 Annotation Study,[0],[0]
tags in a similar way as we acquire our data and find that human annotators agree 93% of the time with the hashtag emotion type if the hashtag occurs as the last word in the tweet.,4 Annotation Study,[0],[0]
We wanted to validate our use of hashtags in a similar fashion and on a bigger random sample.,4 Annotation Study,[0],[0]
"We had human annotators label a random sample of 5,600 tweets that satisfy our preprocessing pipeline.",4 Annotation Study,[0],[0]
Manual inspection during annotation resulted in further removing a negligible 16 tweets that were found to have problems.,4 Annotation Study,[0],[0]
"For each of the remaining 5,584 tweets, the annotators assign a binary tag from the set {relevant, irrelevant} to indicate whether a tweet carries an emotion category as assigned using our distant supervision method or not.",4 Annotation Study,[0],[0]
"Annotators assigned 61.37% (n = 3, 427) “relevant” tags and 38.63% (n = 2, 157) “irrelevant” tags.",4 Annotation Study,[0],[0]
"Our analysis of this manually labeled dataset also supports the findings of (Wang et al., 2012): When we limit position of the emotion hashtag to the end of a tweet, we acquire 90.57% relevant data.",4 Annotation Study,[0],[0]
"We also find that if we relax the constraint on the hashtag position such that we allow the hashtag to occur in the last quarter of a tweet (based on a total tweet character count), we acquire 85.43% relevant tweets.",4 Annotation Study,[0],[0]
"We also find that only 23.20% (n = 795 out of 3, 427) of the emotion carrying tweets have the emotion hashtags occurring in final position, whereas 31.75% (n = 1, 088 out of 3, 427) of the tweets have the emotion hashtags in the last quarter of the tweet string.",4 Annotation Study,[0],[0]
This shows how enforcing a final hashtag location results in loss of a considerable number of emotion tweets.,4 Annotation Study,[0],[0]
"As shown in Table 2, only 1, 608, 233 tweets out of a total of 6, 851, 955 tweets (% = 23, 47) in our bigger dataset have emotion hashtags occurring in final position.",4 Annotation Study,[0],[0]
"Overall, we agree with (Mohammad, 2012; Wang et al., 2012) that the accuracy acquired by enforcing a strict pipeline and limiting to emotion hashtags to final position is a reasonable measure for warranting good-quality data for training supervised systems, an assumption we have also validated with our empirical findings here.
",4 Annotation Study,[0],[0]
"One advantage of using distant supervision under these conditions for labeling emotion data, as (Wang et al., 2012) also notes, is that the label is assigned by the writer of the tweet himself/herself rather than an annotator who could wrongly decide what category a tweet is.",4 Annotation Study,[0],[0]
"After all, emotion is a fuzzy concept and > 90% agreement as we
report here is higher than the human agreement usually acquired on many NLP tasks.",4 Annotation Study,[0],[0]
Another advantage of this method is obviously that it enables us to acquire a sufficiently large training set to use deep learning.,4 Annotation Study,[0],[0]
We now turn to describing our deep learning methods.,4 Annotation Study,[0],[0]
"For our core modeling, we use Gated Recurrent Neural Networks (GRNNs), a modern variation of recurrent neural networks (RNNs), which we now turn to introduce.",5 Methods,[0],[0]
"For notation, we denote scalars with italic lowercase (e.g., x), vectors with bold lowercase (e.g.,x), and matrices with bold uppercase (e.g.,W).
",5 Methods,[0],[0]
Recurrent Neural Network A recurrent neural network (RNN) is one type of neural network architecture that is particularly suited for modeling sequential information.,5 Methods,[0],[0]
"At each time step t, an RNN takes an input vector xt IRn and a hidden state vector h t−1 IRm and produces the next hidden state h t by applying the recursive operation:
ht = f",5 Methods,[0],[0]
"(Wxt + Uht−1 + b) (1)
Where the input to hidden matrix W IRmxn, the hidden to hidden matrix U IRmxm, and the bias vector b IRm are parameters of an affine transformation and f is an element-wise nonlinearity.",5 Methods,[0],[0]
"While an RNN can in theory summarize all historical information up to time step ht, in practice it runs into the problem of vanishing/exploding gradients (Bengio et al., 1994; Pascanu et al., 2013) while attempting to learn longrange dependencies.
",5 Methods,[0],[0]
"LSTM Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) addresses this exact problem of learning long-term dependencies by augmenting an RNN with a memory cell ct IRn at each time step.",5 Methods,[0],[0]
"As such, in addition to the input vector xt, the hiddent vector ht−1, an LSTM takes a cell state vector ct−1 and produces ht and ct via the following calculations:
it = σ",5 Methods,[0],[0]
"( Wixt + Uiht−1 + bi ) ft = σ ( Wfxt + Ufht−1 + bf )
",5 Methods,[0],[0]
ot = σ,5 Methods,[0],[0]
"(Woxt + Uoht−1 + bo) gt = tanh (W
gxt + Ught−1",5 Methods,[0],[0]
+ bg) ct = ft ct−1,5 Methods,[0],[0]
"+ it gt ht = ot tanh(ct)
(2)
",5 Methods,[0],[0]
"Where σ(·) and tanh(·) are the element-wise sigmoid and hyperbolic tangent functions, the element-wise multiplication operator, and it, ft, ot are the input, forget, and output gates.",5 Methods,[0],[0]
The gt is a new memory cell vector with candidates that could be added to the state.,5 Methods,[0],[0]
"The LSTM parameters Wj , Uj , and bj are for j {i, f, o, g}.
",5 Methods,[0],[0]
"GRNNs (Cho et al., 2014; Chung et al., 2015) propose a variation of LSTM with a reset gate rt, an update state zt, and a new simpler hidden unit ht, as follows:
rt = σ",5 Methods,[0],[0]
(Wrxt + Urht−1,5 Methods,[0],[0]
+ br) zt = σ,5 Methods,[0],[0]
(Wzxt + Uzht−1 + bz) h̃t = tanh,5 Methods,[0],[0]
"( Wxt + rt ∗ Uh̃ht−1 + bh̃ )
",5 Methods,[0],[0]
ht = zt ∗ ht−1,5 Methods,[0],[0]
"+ (1− zt) ∗ h̃t
(3)
",5 Methods,[0],[0]
"The GRNN parameters Wj , Uj , and bj are for j {r, z, h̃}.",5 Methods,[0],[0]
"In this set up, the hidden state is forced to ignore a previous hidden state when the reset gate is close to 0, thus enabling the network to forget or drop irrelevant information.",5 Methods,[0],[0]
"Additionally, the update gate controls how much information carries over from a previous hidden state to the current hidden state (similar to an LSTM memory cell).",5 Methods,[0],[0]
We use GRNNs as they are simpler and faster than LSTM.,5 Methods,[0],[0]
"For GRNNs, we use Theano (Theano Development Team, 2016).
",5 Methods,[0],[0]
"Online Classifiers We compare the performance of the GRNNs to four online classifiers that are capable of handling the data size: Stochastic Gradient Descent (SGD), Multinomial Naive Bayes (MNB), Perceptron, and the Passive Agressive Classifier (PAC).",5 Methods,[0],[0]
These classifiers learn online from mini-batches of data.,5 Methods,[0],[0]
"We use minibatches of 10,000 instances with all the four classifiers.",5 Methods,[0],[0]
"We use the scikit-learn implementation of these classifiers (http://scikit-learn. org).
",5 Methods,[0],[0]
Settings We aim to model Plutchik’s 24 finegrained emotions as well as his 8 primary emotion dimensions where each 3 related types of emotion (perceived as varying in intensity) are combined in one dimension.,5 Methods,[0],[0]
We now turn to describing our experiments experiments.,5 Methods,[0],[0]
"As explained earlier, Plutchik organizes the 24 emotion types in the 3 main circles that we will refer to as plutchik-1, plutchik-2, and plutchik-3.
",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We model the set of emotions belonging to each of the 3 circles independently, thus casting each as an 8-way classification task.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"Inspired by observations from the literature and our own annotation study, we limit our data to tweets of at least 5 words with an emotional hashtag occurring at the end.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We then split the data representing each of the 3 circles into 80% training (TRAIN), 10% development (DEV), and 10% testing (TEST).",6.1 Predicting Fine-Grained Emotions,[0],[0]
"As mentioned above, we run experiments with a range of online, out-of-core classifiers as well as the
GRNNs.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"To train the GRNNs, we optimize the hyper-parameters of the network on a development set as we describe below, choosing a vocabulary size of 80K words (a vocabulary size we also use for the out-of-core classifiers), a word embedding vector of size 300 dimensions learnt directly from the training data, an input maximum length of 30 words, 7 epochs, and the Adam (Kingma and Ba, 2014) optimizer with a learning rate of 0.001.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We use 3 dense layers each with 1, 000 units.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We use dropout (Hinton et al., 2012) for regularization, with a dropout rate of 0.5.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"For our loss function, we use categorical cross-entropy.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We use a minibatch (Cotter et al., 2011) size of 128.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We found this architecture to work best with almost all the settings and so we fix it across the board for all experiments with GRNNs.
",6.1 Predicting Fine-Grained Emotions,[0],[0]
Results with Traditional Classifiers Results with the online classifiers are presented in terms of F-score in Table 3.,6.1 Predicting Fine-Grained Emotions,[0],[0]
"As the table shows, among this group of classifiers, the Passive Agressive classifier (PAC) acquires the best performance.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"PAC achieves an overall F-score of 64.86% on plutchik-1, 53.30% on plutchik-2, and 68.14% on plutchik-3, two of which are higher than an arbitrary baseline3 of 60%.
Results with GRNNs Table 4 presents results with GRNNs, compared with the best results using the traditional classifiers as acquired with PAC.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"As the table shows, the GRNN models are very successful across all the 3 classification tasks.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"With GRNNs, we acquire an overall F-scores of: 91.21% on plutchik-1, 82.32% on plutchik-2, and 87.47% on plutchik-3.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"These results are 26.35%, 29.02%, and 25.37% higher than PAC, respectively.
",6.1 Predicting Fine-Grained Emotions,[0],[0]
"Negative Results We experiment with aug3The arbitrary baseline is higher than the majority class in
the training data in any of the 3 cases.
",6.1 Predicting Fine-Grained Emotions,[0],[0]
"menting training data reported here in two ways: 1) For each emotion type, we concatenate the training data with training data of tweets that are more (or less) intense from the same sector/dimension in the wheel, and 2) for each emotion type, we add tweets where emotion hashtags occur in the last quarter of a tweet (which were originally filtered out from TRAIN).",6.1 Predicting Fine-Grained Emotions,[0],[0]
"However, we gain no improvements based on either of these methods, thus reflecting the importance of using high-quality training data and the utility of our strict pipeline.",6.1 Predicting Fine-Grained Emotions,[0],[0]
"We now investigate the task of predicting each of the 8 primary emotion dimensions represented by the sectors of the wheel (where the three degrees of intensity of a given emotion are reduced to a single emotion dimension [e.g., {ecstasy, joy, serenity} are reduced to the joy dimension]).",6.2 Predicting 8 Primary Dimensions,[0],[0]
"We concatenate the 80% training data (TRAIN) from each of the 3 circles’ data into a single training set
(TRAIN-ALL), the 10% DEV to form DEV-ALL, and the 10% TEST to form TEST-ALL.",6.2 Predicting 8 Primary Dimensions,[0],[0]
We test a number of hyper-parameters on DEV and find the ones we have identified on the fine-grained prediction to work best and so we adopt them as is with the exception of limiting to only 2 epochs.,6.2 Predicting 8 Primary Dimensions,[0],[0]
"We believe that with a wider exploration of hyperparameters, improvements could be possible.",6.2 Predicting 8 Primary Dimensions,[0],[0]
"As Table 5 shows, we are able to model the 8 dimensions with an overall superior accuracy of 95.68%.",6.2 Predicting 8 Primary Dimensions,[0],[0]
"As far as we know, this is the first work on modeling these dimensions.",6.2 Predicting 8 Primary Dimensions,[0],[0]
We compare our results on the 8 basic emotions to the published literature.,7 Comparisons to Other Systems,[0],[0]
"As Table 6 shows, on this subset of emotions, our system is 4.53% (acc) higher than the best published results (Volkova and Bachrach, 2016), facilitated by the fact that we have an order of magnitude more training data.",7 Comparisons to Other Systems,[0],[0]
"As shown in Table 7, we also apply (Volkova and Bachrach, 2016)’s pre-trained model on our test set of the 6 emotions they predict (which belong to plutchik-2), and acquire an overall accuracy of 26.95%, which is significantly lower than our accuracy.",7 Comparisons to Other Systems,[0],[0]
"In this paper, we built a large, automatically curated dataset for emotion detection using distant supervision and then used GRNNs to model finegrained emotion, achieving a new state-of-the-art performance.",8 Conclusion,[0],[0]
We also extended the classification to 8 primary emotion dimensions situated in psychological theory of emotion.,8 Conclusion,[0],[0]
Accurate detection of emotion from natural language has applications ranging from building emotional chatbots to better understanding individuals and their lives.,abstractText,[0],[0]
"However, progress on emotion detection has been hampered by the absence of large labeled datasets.",abstractText,[0],[0]
"In this work, we build a very large dataset for fine-grained emotions and develop deep learning models on it.",abstractText,[0],[0]
We achieve a new state-of-the-art on 24 fine-grained types of emotions (with an average accuracy of 87.58%).,abstractText,[0],[0]
"We also extend the task beyond emotion types to model Robert Plutchik’s 8 primary emotion dimensions, acquiring a superior accuracy of 95.68%.",abstractText,[0],[0]
EmoNet: Fine-Grained Emotion Detection with Gated Recurrent Neural Networks,title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 128–136, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
Recent years have seen an increase in the use of dependency representations throughout various NLP applications.,1 Introduction,[0],[0]
"For the discourse analysis of texts, dependency graph representations have also been studied by many researchers (Prasad et al., 2008; Muller et al., 2012; Hirao et al., 2013; Li et al., 2014).",1 Introduction,[0],[0]
"In particular, Hirao et al. (2013) proposed a current state-of-the-art text summarization method based on trimming discourse dependency trees (DDTs).",1 Introduction,[0],[0]
"Dependency tree representation is the key to the formulation of the tree trimming method (Filippova and Strube, 2008), and dependency-based discourse syntax has further potential to improve the modeling of a wide range of text-based applications.
",1 Introduction,[0],[0]
"However, no large-scale corpus exists that is annotated with DDTs since it is expensive to manually construct such a corpus from scratch.",1 Introduction,[0],[0]
"Therefore, Hirao et al. (2013) and Li et al. (2014) proposed heuristic rules that automatically transform RST discourse trees (RST-DTs)1 into DDTs.",1 Introduction,[0],[0]
"However, even researchers, who cited these two works in their papers, have ignored their differences, probably because the authors described only abstracts of their conversion methods.",1 Introduction,[0],[0]
"To clarify their algorithmic differences, this paper provides pseudocodes where the two different methods can be described in a unified form, showing that they analyze multinuclear relations differently on RST-DTs.",1 Introduction,[0],[0]
"As we show by example in Section 4, such a slight difference can derive significantly different DDTs.
",1 Introduction,[0],[0]
The main purpose of this paper is to experimentally reveal the differences between dependency formats.,1 Introduction,[0],[0]
"By investigating the complexity of their structures from the dependency graph theoretic point of view (Kuhlmann and Nivre, 2006), we prove that the Hirao13 method, which keeps the semantic equivalence of multinuclear discourse units in the dependency structures, introduces much more complex DDTs than Li14, while a simple post-editing method greatly reduces the complexity of DDTs.
",1 Introduction,[0],[0]
This paper also compares the methods with both intrinsic and extrinsic evaluations: (1) Which dependency structures are analyzed more accurately by automatic parsers?,1 Introduction,[0],[0]
"and (2) Which structures
1Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents discourse as a (constituent-style) tree structure.",1 Introduction,[0],[0]
"RST was developed as the basis of annotated corpora for the automatic analysis of text syntax, most notably the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003).
128
are more suitable to text summarization?",1 Introduction,[0],[0]
"We show from experimental results that even though the Hirao13 DDT format reduces performance, as measured by intrinsic evaluations, it is more useful for text summarization.",1 Introduction,[0],[0]
"While researchers developing discourse syntactic parsing (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013; Li et al., 2014) have focused excessively on improving accuracies, our experimental results emphasize the importance of extrinsic evaluations since the more accurate parser does not always lead to better performance of textbased applications.",1 Introduction,[0],[0]
"Mann and Thompson (1988)’s Rhetorical Structure Theory (RST), which is one of the most influential text organization frameworks, represents a discourse structure as a constituent tree.",2 Related Work,[0],[0]
"The RST Discourse Treebank (RST-DTB) (Carlson et al., 2003) has played a critical role in automatic discourse analysis (Soricut and Marcu, 2003; Hernault et al., 2010; Feng and Hirst, 2012; Joty et al., 2013), mainly because trees are both easy to formalize and computationally tractable.",2 Related Work,[0],[0]
"RST discourse trees (RST-DTs) are also used for modeling many text-based applications, such as text summarization (Marcu, 2000) and anaphora resolution (Cristea et al., 1998).
",2 Related Work,[0],[0]
Hirao et al. (2013) and Li et al. (2014) introduced dependency conversion methods from RSTDTs into DDTs in which a full discourse structure is represented by head-dependent binary relations between elementary discourse units.,2 Related Work,[0],[0]
"Hirao et al. (2013) also showed that a text summarization method, based on trimming DDTs, achieves significant improvements against Marcu (2000)’s method using RST-DTs.
",2 Related Work,[0],[0]
"On the other hand, some researchers argue that trees are inadequate to account for a full discourse structure (Wolf and Gibson, 2005; Lee et al., 2006; Danlos and others, 2008; Venant et al., 2013).",2 Related Work,[0],[0]
"Segmented Discourse Representation Theory (SDRT) (Asher and Lascarides, 2003) represents discourse structures as logical form, and relations function like logical operators on the meaning of their arguments.",2 Related Work,[0],[0]
"The annotation in the ANNODIS corpus was conducted based on SDRT (Afantenos et al., 2012).",2 Related Work,[0],[0]
"For automatic discourse analysis using the corpus, Muller et al. (2012) adopted dependency tree representation to
simplify discourse parsing.",2 Related Work,[0],[0]
"They also presented a method to automatically derive DDTs from SDR structures.
",2 Related Work,[0],[0]
Wolf and Gibson (2005) used a chain-graph for representing discourse structures and annotated 135 articles from the AP Newswire and the Wall Street Journal.,2 Related Work,[0],[0]
The annotated corpus is called the Discourse Graphbank.,2 Related Work,[0],[0]
"The graph represents crossed dependency and multiple parentship discourse phenomena, which cannot be represented by tree structures, but whose graph structures become very complex (Egg and Redeker, 2010).
",2 Related Work,[0],[0]
"The Penn Discourse Treebank (PDTB) (Prasad et al., 2008) is a large-scale corpus of annotated discourse connectives and their arguments.",2 Related Work,[0],[0]
"Its connective-argument structure can also represent complex discourse phenomena like multiple parentship, but its objective is to annotate the discourse relations between individual discourse units, not full discourse structures.",2 Related Work,[0],[0]
"Unfortunately, to the best of our knowledge, neither the Discourse Graphbank nor the PDTB has been used for any specific NLP applications.",2 Related Work,[0],[0]
RST represents a discourse as a tree structure.,3 RST Discourse Tree,[0],[0]
The leaves of an RST discourse tree (RST-DT) correspond to Elementary Discourse Units (EDUs).,3 RST Discourse Tree,[0],[0]
"Adjacent EDUs are linked by rhetorical relations, forming larger discourse units that are also subject to this relation linking.",3 RST Discourse Tree,[0],[0]
"Figure 1 shows part of an RST-DT (wsj-0623), taken from RST-DTB, for this text fragment:{
[The fiscal 1989 budget deficit figure came out Friday .]e-1 }
1 ,
{",3 RST Discourse Tree,[0],[0]
"[It was down a little .]e-2 } 2 ,{
[The next time you hear a Member of Congress moan about the deficit ,]e-3, [consider what Congress did Friday .]e-4",3 RST Discourse Tree,[0],[0]
"} 3 , {",3 RST Discourse Tree,[0],[0]
"[The Senate , 84-6 , voted to increase to $ 124,000 the ceiling on insured mortgages from the FHA ,]e-5, [which lost $ 4.2 billion in loan defaults last year .]e-6 } 4 ,{
[Then , by voice vote , the Senate voted a porkbarrel bill ,]e-7, [approved Thursday by the House ,]e-8, [for domestic military construction .]e-9",3 RST Discourse Tree,[0],[0]
"} 5 ,{
[the Bush request to what the Senators gave themselves :]e-10 }
6 , . .",3 RST Discourse Tree,[0],[0]
".
",3 RST Discourse Tree,[0],[0]
where each subscript at the end of square brackets,3 RST Discourse Tree,[0],[0]
[] corresponds to a leaf unit (EDU) in the tree.,3 RST Discourse Tree,[0],[0]
"EDUs grouped by {} consist of a sentence that is labeled with its index in the text.
",3 RST Discourse Tree,[0],[0]
"Each discourse unit in the tree that forms a rhetorical relation is characterized by a rhetorical status: Nucleus (N), which represents the most essential piece of information in the relation, or Satellite (S), which indicates the supporting information.",3 RST Discourse Tree,[0],[0]
Rhetorical relations must be either mononuclear or multinuclear.,3 RST Discourse Tree,[0],[0]
"Mononuclear relations hold between two units with Nucleus and Satellite, whereas multinuclear relations hold among two or more units with Nucleus.",3 RST Discourse Tree,[0],[0]
Each unit in a multinuclear relation has similar semantic information as the other units.,3 RST Discourse Tree,[0],[0]
Rhetorical relations can be grouped into classes that share such rhetorical meaning as “Elaboration” and “Condition”.,3 RST Discourse Tree,[0],[0]
"In Figure 1, the Satellite unit (covering e-3) and its sibling Nucleus unit (covering e-4) are linked by a mononuclear relation with rhetorical label “Condition”, and two Nucleus units (covering e-5, e-6 and e-7, e-8, e-9) are linked by a multinuclear relation with rhetorical label “Temporal”.",3 RST Discourse Tree,[0],[0]
"Next, this paper discusses text-level dependency syntax, which represents grammatical structure by head-dependent binary relations between EDUs.",4 Conversions from RST-DTs to DDTs,[0],[0]
This section introduces two existing automatic conversion methods from RST-DTs to DDTs: the methods of Li et al. (2014) and Hirao et al. (2013).,4 Conversions from RST-DTs to DDTs,[0],[0]
"Additionally, this paper presents a simple postediting method to reduce the complexity of DDTs.",4 Conversions from RST-DTs to DDTs,[0],[0]
"The heart of these conversions closely resembles that of constituent-to-dependency conversions for English sentences (Yamada and Matsumoto, 2003; Johansson and Nugues, 2007; De Marneffe and Manning, 2008), since RST-DTs can be regarded
Algorithm 1 convert-rst-into-dep Require: RST discourse tree: rst-dt Ensure: discourse dependency tree: ddt 1: ddt← /0 2: for all EDU e-",4 Conversions from RST-DTs to DDTs,[0],[0]
"j in rst-dt do
3: P← {
find-My-Top-Node(e- j) //",4 Conversions from RST-DTs to DDTs,[0],[0]
Li14 find-Nearest-S-Ancestor(e- j) //,4 Conversions from RST-DTs to DDTs,[0],[0]
"Hirao13
4: if isRoot(P) = TRUE then 5: ℓ←",4 Conversions from RST-DTs to DDTs,[0],[0]
"Root 6: i← 0 7: else 8: ℓ← Label(P) 9: P′← Parent(P) 10: i← find-Head-EDU(P′) 11: end if 12: j← Index(e- j) 13: ddt← ddt ∪ (i, ℓ, j) 14: end for 15: Return ddt
Algorithm 2 find-My-Top-Node(e) Require: EDU:",4 Conversions from RST-DTs to DDTs,[0],[0]
"e Ensure: C 1: C← e 2: P← Parent(e) 3: while LeftmostNucleusChild(P) = C and
isRoot(P) = FALSE do 4: C← P 5: P← Parent(P) 6: end while 7: if isRoot(P) = TRUE then 8: C← P 9: end if
10:",4 Conversions from RST-DTs to DDTs,[0],[0]
"Return C
as Penn Treebank-style constituent trees because EDUs and discourse units respectively correspond to terminal and non-terminal nodes, and a rhetorical relation, like a CFG-rule, forms an edge in the tree.
",4 Conversions from RST-DTs to DDTs,[0],[0]
"4.1 Li et al. (2014)’s Method
Algorithm 3 find-Head-EDU(P)",4 Conversions from RST-DTs to DDTs,[0],[0]
Require: non-terminal node:,4 Conversions from RST-DTs to DDTs,[0],[0]
P Ensure: i 1: while isLeaf(P) = FALSE do 2: P← LeftmostNucleusChild(P) 3: end while 4: i← Index(P) 5:,4 Conversions from RST-DTs to DDTs,[0],[0]
"Return i
Li et al. (2014)’s dependency conversion method is based on the idea of assigning each discourse unit in an RST-DT a unique head selected among the unit’s children.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Traversing each nonterminal node in a bottom-up manner, the headassignment procedure determines the head from its children in the following manner: the head of the leftmost child node with the Nucleus is the head; if no child node is the Nucleus, the head of the leftmost child node is the head.
",4 Conversions from RST-DTs to DDTs,[0],[0]
"The procedure was originally introduced by Sagae (2009), and its core idea is identical as the head-assignment rules for Penn Treebankstyle constituent trees (Magerman, 1994; Collins, 1999).",4 Conversions from RST-DTs to DDTs,[0],[0]
"Li’s conversion method uses the procedure to assign a head to each non-terminal node of a right-branching binarized RST-DT (Hernault et al., 2010) and transforms the head-annotated binary tree into a DDT.
",4 Conversions from RST-DTs to DDTs,[0],[0]
Algorithms 1-3 show the dependency conversion method.,4 Conversions from RST-DTs to DDTs,[0],[0]
"For brevity, we describe it in a different form from Li’s original conversion process2 cited above.",4 Conversions from RST-DTs to DDTs,[0],[0]
"In Algorithm 1, the main routine iteratively processes every EDU in given RST-DT t to directly find its single head rather than transforming head-annotated trees into DDTs.",4 Conversions from RST-DTs to DDTs,[0],[0]
"The main process is largely separated into three steps:
1.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Algorithm 1 calls Algorithm 2 at line 3, which finds the highest non-terminal
2Unlike Li’s procedure, our algorithm can take not only binary but also n-ary RST-DTs as inputs.",4 Conversions from RST-DTs to DDTs,[0],[0]
"To derive the same DDTs as those produced by Li’s original method, experiments were performed on right-branching binary RST-DTs.
node in t to which current processed EDU e-",4 Conversions from RST-DTs to DDTs,[0],[0]
j must be assigned as the head in Sagae’s lexicalization manner.,4 Conversions from RST-DTs to DDTs,[0],[0]
"Parent(P) and LeftmostNucleusChild(P) are respectively operations that return the parent node of node P and the leftmost child node with the Nucleus of node P3.
2.",4 Conversions from RST-DTs to DDTs,[0],[0]
"After obtaining node P from Algorithm 2, Algorithm 1 seeks the head EDU that is assigned to the parent node of P. If P is the root node of t, we set ℓ to rhetorical label “Root” and i to a special index 0 of virtual EDU e-0 (lines 5-6 in Algorithm 1).",4 Conversions from RST-DTs to DDTs,[0],[0]
"Otherwise, we set ℓ← Label(P) and P′← Parent(P) (lines 8-9 in Algorithm 1), where Label(P) returns the rhetorical label attached to node P4.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Then Algorithm 1 at line 10 calls Algorithm 3, which iteratively seeks the leftmost child node with the Nucleus in a top-down manner, starting from P′, until it reaches terminal node e-i. Operation Index(P) returns the index of EDU P.
3.",4 Conversions from RST-DTs to DDTs,[0],[0]
We attach e- j to head e-i and assign rhetorical label ℓ to the dependency edge.,4 Conversions from RST-DTs to DDTs,[0],[0]
"We write (i,ℓ, j) to denote that a dependency edge exists with rhetorical label ℓ from head e-i to modifier e-",4 Conversions from RST-DTs to DDTs,[0],[0]
"j.
Assuming that e-",4 Conversions from RST-DTs to DDTs,[0],[0]
"j is the e-7 of the RST-DT in Figure 1, Algorithm 2 returns the ‘N:Temporal’ node (covering e-7, e-8, e-9) since its parent node ‘N’ has the other ‘N:Temporal’ node (covering e5, e-6) as its leftmost Nucleus child.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Starting from the parent node ‘N’, Algorithm 3 iteratively seeks the leftmost Nucleus child in the top-down manner until it reaches the terminal node e-5.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Finally, we obtain a dependency edge (5,Temporal,7).
",4 Conversions from RST-DTs to DDTs,[0],[0]
The DDT in Figure 2 is produced by this method for the RST-DT in Figure 1.,4 Conversions from RST-DTs to DDTs,[0],[0]
"To each EDU, we also assign ‘N’ or ‘S’ rhetorical status of its parent node.",4 Conversions from RST-DTs to DDTs,[0],[0]
"Li’s dependency format is always projective, i.e., when all the edges are drawn in the half-plane above the text, no two edges cross (Kübler et al., 2009).",4 Conversions from RST-DTs to DDTs,[0],[0]
"3If P has no Nucleus children, LeftmostNucleusChild(P) returns the leftmost child node.
",4.2 Hirao et al. (2013)’s Method,[0],[0]
"4If P does not have any rhetorical labels, Label(P) returns a special non-rhetorical label: “Span”.
",4.2 Hirao et al. (2013)’s Method,[0],[0]
Algorithm 4 find-Nearest-S-Ancestor(e),4.2 Hirao et al. (2013)’s Method,[0],[0]
Require: EDU:,4.2 Hirao et al. (2013)’s Method,[0],[0]
"e Ensure: P 1: P← Parent(e) 2: while isNucleus(P) = TRUE and
Hirao et al. (2013) also proposed a dependency conversion method for RST-DTs.",4.2 Hirao et al. (2013)’s Method,[0],[0]
The only difference between Li’s and Hirao’s methods is the process that finds the highest non-terminal node to which each EDU must be assigned as the head.,4.2 Hirao et al. (2013)’s Method,[0],[0]
"At line 3 of Algorithm 1, Hirao’s method calls Algorithm 4, which seeks the nearest Satellite to each EDU on the path from it to the root node of t. Note that this head-assignment manner was originally presented in the Veins Theory (Cristea et al., 1998).
",4.2 Hirao et al. (2013)’s Method,[0],[0]
Assuming that e-,4.2 Hirao et al. (2013)’s Method,[0],[0]
"j is the e-7 in Figure 1, Algorithm 4 returns the ‘S:Elaboration’ node (covering e-5, e-6, e-7, e-8, e-9, e-10, . . .",4.2 Hirao et al. (2013)’s Method,[0],[0]
"), which is the nearest Satellite on the path from e-7 to the root node.",4.2 Hirao et al. (2013)’s Method,[0],[0]
"Then, as well as in Li’s method, Algorithm 3 iteratively seeks the leftmost child node with the Nucleus, starting from the parent node of the Satellite, until it reaches terminal node e-4.",4.2 Hirao et al. (2013)’s Method,[0],[0]
"Finally, we obtain a dependency edge (4,Elaboration,7).
",4.2 Hirao et al. (2013)’s Method,[0],[0]
Figure 3 represents the DDT produced by Hirao’s method for the RST-DT in Figure 1.,4.2 Hirao et al. (2013)’s Method,[0],[0]
"Note that unlike Li’s method, Hirao’s dependency format is not always projective.",4.2 Hirao et al. (2013)’s Method,[0],[0]
"The dependency edges made from the mononuclear relations are the same as those in Figure 2, but the difference comes from the treatment of the multinuclear relations.",4.2 Hirao et al. (2013)’s Method,[0],[0]
"We take as an example the “Temporal” multinuclear relation in Figure 1 that links sentences 4 (e-5 and e-6) and 5 (e-7, e-8, and e-9).",4.2 Hirao et al. (2013)’s Method,[0],[0]
"The Li14 DDT format links them with a “parentchild” relation, while in the Hirao13 DDT format, they have a “sibling” relation.",4.2 Hirao et al. (2013)’s Method,[0],[0]
"Unlike Li’s method, the dependency structures produced by Hirao’s method often lose the singlerooted tree structure of a sentence since Algorithm 4 has no constraints that restrict the EDUs covered by multinuclear relations to find its head outside the sentence.",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"For example, in Figure 3, both EDUs e-7 and e-9 in sentence 5 have the same head e-4 outside the sentence.
",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"Most sentences form a single-rooted subtree in a full-text RST-DT (Joty et al., 2013), and previous studies on sentence-level discourse parsing were based on this insight (Soricut and Marcu, 2003; Sagae, 2009).",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"To reduce the complexity of DDTs, it is reasonable to restrict the tree structure of a sentence to be single-rooted in a full-text DDT.
",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"To revise a multi-rooted dependency tree structure of a sentence to a single-rooted one, we propose a simple post-editing method.",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"Let L = ⟨e-x1, . . .",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
",e-xn⟩ be a multi-root list consisting of more than two EDUs (n ≥ 2 and x1 < · · · < xn) in identical sentence s, each of which has a head outside s. Next we define the post-editing process of multi-root list L ; for each EDU e-x j (2≤ j ≤ n), let its head be e-y j with rhetorical label ℓ",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"j. Then the post-editing method replaces the dependency edge (y j, ℓ j,x j) by (x1,Label(P),x j), where P is a child node, which covers e-x j, of the highest node among those that cover only sentence s in the RSTDT.
",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"For the DDT in Figure 3, the post-editing process for multi-root list L = ⟨e-7,e-9⟩ replaces the edge (4,Temporal,9) by (7,Same-Unit,9).",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
This process makes the tree structure of sentence 5 single-rooted (Figure 4).,4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"Note that if an input dependency graph structure is a tree, even after postediting all the multi-root lists of the input tree, the result remains a tree structure.",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"This post-editing reduces the number of non-projective dependency
edges, even though the structure might continue to be non-projective.",4.3 Post-editing Algorithm for Multi-rooted Sentence Tree Structures,[0],[0]
"Our experiments are based on data from the RST Discourse Treebank (RST-DTB) (Carlson et al., 2003)5, which consists of 385 Wall Street Journal articles.",5.1.1 Dependency Label Distributions,[0],[0]
"Following previous studies on RST-DTB, we used 18 coarse rhetorical labels.",5.1.1 Dependency Label Distributions,[0],[0]
We converted all 385 RST-DTs to DDTs using the methods introduced in Section 4.,5.1.1 Dependency Label Distributions,[0],[0]
Table 1 compares three distributions of 18 rhetorical labels and 2 special nonrhetorical labels: “Span”6 and “Root”.,5.1.1 Dependency Label Distributions,[0],[0]
"M-Hirao13 denotes a modified version of the Hirao13 dependency format by post-editing.
",5.1.1 Dependency Label Distributions,[0],[0]
"Here, we focus on the three underlined labels.",5.1.1 Dependency Label Distributions,[0],[0]
"Even though the DDTs produced by the Hirao13 method contain more edges labeled as “Elaboration”, the number of “Joint” and “Same-Unit” labels, which are assigned to some multinuclear relations, decreases considerably.",5.1.1 Dependency Label Distributions,[0],[0]
"This is because for each EDU, Algorithm 4 in the Hirao13 method finds a Satellite covering the EDU through multin-
5https://catalog.ldc.upenn.edu/ LDC2002T07
6In RST theory, a “Span” label may not be assigned to any dependency edges.",5.1.1 Dependency Label Distributions,[0],[0]
"We suspect that the illegal “Span” label in Table 1 might have been caused by an annotation error in a subtree from e-7 to e-9 of the wsj-1189 file.
uclear relations and most Satellites have the “Elaboration” label.
",5.1.1 Dependency Label Distributions,[0],[0]
"In practice, we should refine such “Elaboration” labels by encoding in them the information of multinuclear relations that appear on the path from the EDU to the Satellite.",5.1.1 Dependency Label Distributions,[0],[0]
"However, this encoding scheme has a trade-off; increasing the amount of information encoded in an edge label reduces the accuracy of the label prediction by automatic parsers.",5.1.1 Dependency Label Distributions,[0],[0]
"In future work, we will investigate what label encoding scheme strikes the best balance in the trade-off.",5.1.1 Dependency Label Distributions,[0],[0]
This section investigates the complexity of the dependency structures produced by each conversion method.,5.1.2 Complexity of Dependency Structures,[0],[0]
Table 2 shows the average maximum path length from an artificial root to a leaf EDU and the number of nodes where depth x ∈ N.,5.1.2 Complexity of Dependency Structures,[0],[0]
"The results clearly show that Hirao13 produces more broad and shallow dependency tree structures than Li14.
Table 2 also displays how large a portion of the dependency structures is allowed under projectivity, gap degree, and well-nestedness constraints.",5.1.2 Complexity of Dependency Structures,[0],[0]
"In the dependency parsing community, it is wellknown that these three constraints create a good balance between expressivity and complexity in dependency analysis.",5.1.2 Complexity of Dependency Structures,[0],[0]
"These constraints were formally defined (Kuhlmann and Nivre, 2006)7, and refer to that work for details.
",5.1.2 Complexity of Dependency Structures,[0],[0]
All of the DDTs produced by the Li14 method are projective.,5.1.2 Complexity of Dependency Structures,[0],[0]
"Projectivity is the most popular constraint for sentence-level dependency pars-
7Unlike Kuhlmann and Nivre (2006), when calculating the statistics in Table 2, we add an edge (0,Root, i) for every real root EDU",5.1.2 Complexity of Dependency Structures,[0],[0]
"e-i (i≥ 1) of the DDT.
",5.1.2 Complexity of Dependency Structures,[0],[0]
"ing since it offers cubic-time dynamic programming algorithms for dependency parsing (Eisner, 1996; Eisner and Satta, 1999; Gómez-Rodrıguez et al., 2008).",5.1.2 Complexity of Dependency Structures,[0],[0]
A higher gap degree means that the dependency trees have more complex nonprojective structures.,5.1.2 Complexity of Dependency Structures,[0],[0]
"Both the Hirao13 and MHirao13 methods produce many non-projective dependency edges, but most of the DDTs have at most 1 gap degree and all are well-nested.",5.1.2 Complexity of Dependency Structures,[0],[0]
"The well-nested dependency structures of the low gap degree also allow efficient dynamic programming solutions with polynominal time complexity to dependency parsing (Gómez-Rodrıguez et al., 2009).",5.1.2 Complexity of Dependency Structures,[0],[0]
The conversion methods introduce different complexities in DDTs.,5.2 Impact on Automatic Parsing Accuracy,[0],[0]
This section investigates which formats are more accurately analyzed by automatic discourse parsers.,5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"For evaluation, we implemented a maximum spanning tree algorithm for discourse dependency parsing, which was recently proposed (Muller et al., 2012; Li et al., 2014; Yoshida et al., 2014).",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"To compare discourse dependency parsing with standard RST parsing, we also implemented the HILDA RST parser (Hernault et al., 2010), which achieved 82.6/66.6/54.2 points for a standard set of RST-style evaluation measures, i.e., Span, Nuclearity and Relation (Marcu, 2000).
",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"We used a standard split of DDTs automatically converted from RST-DTB: 347 DDTs as the training set and 38 as the test set.
",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
Table 3 shows the evaluation results of dependency parsing.,5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"The lower the complexity of the DDT format, the higher is the dependency unlabeled attachment score.",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
Post-editing the Hirao13 DDTs improves the dependency attachment scores because the intra-sentential discourse analysis is more accurate than the inter-sentential one.,5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"In all the DDT formats, the labeled attachment scores
are considerably worse that the unlabeled scores.",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
"Compared with the HILDA parser, the Hirao13 and M-Hirao13 DDTs by the MST parser are less accurate than those by the RST parser, probably because unlike word dependency parsing, the features defined over the EDUs are too sparse to describe complex non-projective dependency relations.",5.2 Impact on Automatic Parsing Accuracy,[0],[0]
Hirao et al. (2013) proposed a state-of-the-art single text summarization method based on trimming unlabeled DDTs.,5.3 Impact on Text Summarization,[0],[0]
"That can be formulated by the Tree Knapsack Problem (TKP), which they solved with integer linear programming.",5.3 Impact on Text Summarization,[0],[0]
"To examine which dependency structures produced by the three conversion schemes are more suitable to the task, we performed text summarization experiments with the TKP method.
",5.3 Impact on Text Summarization,[0],[0]
"The 30 Wall Street Journal articles have a human-made reference summary, which we used for our evaluations.",5.3 Impact on Text Summarization,[0],[0]
Table 4 shows the ROUGE scores for the 30 gold-standard and auto-parse DDTs.,5.3 Impact on Text Summarization,[0],[0]
"The auto-parse DDTs were obtained by the MST and HILDA parsers, which were trained with 325 articles and whose hyper parameters were tuned with 30 articles.
",5.3 Impact on Text Summarization,[0],[0]
"Hirao13 achieved the best results when we employed the gold DDTs, although the differences between Hirao13 and the other methods were not large.",5.3 Impact on Text Summarization,[0],[0]
"On the other hand, Hirao13 and M-Hirao13 obtained good results when we employed automatic parse trees.",5.3 Impact on Text Summarization,[0],[0]
The gains against Li14 are large.,5.3 Impact on Text Summarization,[0],[0]
It is remarkable that the performance with MST’s DDTs closely approached that of the gold DDTs.,5.3 Impact on Text Summarization,[0],[0]
"These results imply that the auto parse trees obtained from Hirao13 have broad and shallow hierarchies because important EDUs, which must be included in a summary, can be easily extracted by TKP.",5.3 Impact on Text Summarization,[0],[0]
"Thus, the DDTs converted by the Hirao13 rule have better tree structures for a single document summarization even though the structures are complex and difficult to parse.",5.3 Impact on Text Summarization,[0],[0]
This is a significant advantage over Li’s conversion rule.,5.3 Impact on Text Summarization,[0],[0]
We evaluated two different RST-DT-to-DDT conversion schemes from various perspectives.,6 Summary,[0],[0]
"Experimental results show that even though the Hirao13 DDT format produces more complex dependency structures, it is more useful for text summa-
rization.",6 Summary,[0],[0]
"While studies developing discourse parsing have focused on improving parser accuracies, our experimental results identified the importance of extrinsic evaluations over intrinsic evaluations.",6 Summary,[0],[0]
"In future work, we will further compare the methods by extrinsic evaluation metrics using discourse relation labels.",6 Summary,[0],[0]
The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve the quality of the paper.,Acknowledgments,[0],[0]
This work was supported in part by JSPS KAKENHI Grant Numbers JP26730126 and JP26280079.,Acknowledgments,[0],[0]
"Two heuristic rules that transform Rhetorical Structure Theory discourse trees into discourse dependency trees (DDTs) have recently been proposed (Hirao et al., 2013; Li et al., 2014), but these rules derive significantly different DDTs because their conversion schemes on multinuclear relations are not identical.",abstractText,[0],[0]
This paper reveals the difference among DDT formats with respect to the following questions: (1) How complex are the formats from a dependency graph theoretic point of view?,abstractText,[0],[0]
(2) Which formats are analyzed more accurately by automatic parsers?,abstractText,[0],[0]
(3) Which are more suitable for text summarization task?,abstractText,[0],[0]
"Experimental results showed that Hirao’s conversion rule produces DDTs that are more useful for text summarization, even though it derives more complex dependency structures.",abstractText,[0],[0]
Empirical comparison of dependency conversions for RST discourse trees,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2357–2368 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2357",text,[0],[0]
"Automatic question answering (QA) has made big strides with several open-domain and machine comprehension systems built using large-scale annotated datasets (Voorhees et al., 1999; Ferrucci et al., 2010; Rajpurkar et al., 2016; Joshi et al., 2017).",1 Introduction,[0],[0]
"However, in the clinical domain this problem remains relatively unexplored.",1 Introduction,[0],[0]
"Physicians frequently seek answers to questions from unstructured electronic medical records (EMRs) to support clinical decision-making (Demner-Fushman et al., 2009).",1 Introduction,[0],[0]
"But in a significant majority of cases, they are unable to unearth the information they want from EMRs (Tang et al., 1994).",1 Introduction,[0],[0]
"Moreover to date, there is no general system for answering natural language questions asked by physicians on a patient’s EMR (Figure 1) due to lack of largescale datasets (Raghavan and Patwardhan, 2016).
",1 Introduction,[0],[0]
"EMRs are a longitudinal record of a patient’s health information in the form of unstructured clinical notes (progress notes, discharge summaries etc.) and structured vocabularies.",1 Introduction,[0],[0]
"Physi-
∗This work was conducted during an internship at IBM",1 Introduction,[0],[0]
"§https://www.i2b2.org/NLP/DataSets/
cians wish to answer questions about medical entities and relations from the EMR, requiring a deeper understanding of clinical notes.",1 Introduction,[0],[0]
"While this may be likened to machine comprehension, the longitudinal nature of clinical discourse, little to no redundancy in facts, abundant use of domain-specific terminology, temporal narratives with multiple related diseases, symptoms, medications that go back and forth in time, and misspellings, make it complex and difficult to apply existing NLP tools (Demner-Fushman et al., 2009; Raghavan and Patwardhan, 2016).",1 Introduction,[0],[0]
"Moreover, answers may be implicit or explicit and may require domain-knowledge and reasoning across clinical notes.",1 Introduction,[0],[0]
"Thus, building a credible QA system for patient-specific EMR QA requires largescale question and answer annotations that sufficiently capture the challenging nature of clinical narratives in the EMR.",1 Introduction,[0],[0]
"However, serious privacy concerns about sharing personal health information (Devereaux, 2013; Krumholz et al., 2016), and the tedious nature of assimilating answer annotations from across longitudinal clinical notes, makes this task impractical and possibly erroneous to do manually (Lee et al., 2017).
",1 Introduction,[0],[0]
"In this work, we address the lack of any publicly available EMR QA corpus by creating a large-scale dataset, emrQA, using a novel gener-
ation framework that allows for minimal expert involvement and re-purposes existing annotations available for other clinical NLP tasks (i2b2 challenge datasets (Guo et al., 2006)).",1 Introduction,[0],[0]
"The annotations serve as a proxy-expert in generating questions, answers, and logical forms.",1 Introduction,[0],[0]
"Logical forms provide a human-comprehensible symbolic representation, linking questions to answers, and help build interpretable models, critical to the medical domain (Davis et al., 1977; Vellido et al., 2012).",1 Introduction,[0],[0]
"We analyze the emrQA dataset in terms of question complexity, relations, and the reasoning required to answer questions, and provide neural and heuristic baselines for learning to predict questionlogical forms and question-answers.
",1 Introduction,[0],[0]
"The main contributions of this work are as follows:
• A novel framework for systematic generation of domain-specific large-scale QA datasets that can be used in any domain where manual annotations are challenging to obtain but limited annotations may be available for other NLP tasks.
",1 Introduction,[0],[0]
"• The first accessible patient-specific EMR QA dataset, emrQA∗, consisting of 400,000 question-answer pairs and 1 million questionlogical form pairs.",1 Introduction,[0],[0]
"The logical forms will allow users to train and benchmark interpretable models that justify answers with corresponding logical forms.
",1 Introduction,[0],[0]
"• Two new reasoning challenges, namely arithmetic and temporal reasoning, that are absent in open-domain datasets like SQuAD (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
"∗https://github.com/panushri25/emrQA, scripts to generate emrQA from i2b2 data.",1 Introduction,[0],[0]
i2b2 data is accessible by everyone subject to a license agreement.,1 Introduction,[0],[0]
"Question Answering (QA) datasets are classified into two main categories: (1) machine comprehension (MC) using unstructured documents, and (2) QA using Knowledge Bases (KBs).
",2 Related Work,[0],[0]
MC systems aim to answer any question that could be posed against a reference text.,2 Related Work,[0],[0]
"Recent advances in crowd-sourcing and search engines have resulted in an explosion of large-scale (100K) MC datasets for factoid QA, having ample redundant evidence in text (Rajpurkar et al., 2016; Trischler et al., 2016; Joshi et al., 2017; Dhingra et al., 2017).",2 Related Work,[0],[0]
"On the other hand, complex domainspecific MC datasets such as MCTest (Richardson et al., 2013), biological process modeling (Berant et al., 2014), BioASQ (Tsatsaronis et al., 2015), InsuranceQA (Feng et al., 2015), etc have been limited in scale (500-10K) because of the complexity of the task or the need for expert annotations that cannot be crowd-sourced or gathered from the web.",2 Related Work,[0],[0]
"In contrast to the open-domain, EMR data cannot be released publicly due to privacy concerns (Šuster et al., 2017).",2 Related Work,[0],[0]
"Also, annotating unstructured EMRs requires a medical expert who can understand and interpret clinical text.",2 Related Work,[0],[0]
"Thus, very few datasets like i2b2, MIMIC (Johnson et al., 2016) (developed over several years in collaboration with large medical groups and hospitals), share small-scale annotated clinical notes.",2 Related Work,[0],[0]
"In this work, we take advantage of the limited expertly annotated resources to generate emrQA.
",2 Related Work,[0],[0]
"KB-based QA datasets, used for semantic parsing, are traditionally limited by the requirement of annotated question and logical form (LF) pairs for supervision where the LF are used to retrieve answers from a schema (Cai and Yates, 2013; Lopez et al., 2013; Bordes et al., 2015).",2 Related Work,[0],[0]
"Roberts and Demner-Fushman (2016) generated a corpus by
manually annotating LFs on 468 EMR questions (not released publicly), thus limiting its ability to create large scale datasets.",2 Related Work,[0],[0]
"In contrast, we only collect LFs for question templates from a domainexpert - the rest of our corpus is automatically generated.
",2 Related Work,[0],[0]
"Recent advances in QA combine logic-based and neural MC approaches to build hybrid models (Usbeck et al., 2015; Feng et al., 2016; Palangi et al., 2018).",2 Related Work,[0],[0]
"These models are driven to combine the accuracy of neural approaches (Hermann et al., 2015) and the interpretability of the symbolic representations in logic-based methods (Gao et al.; Chabierski et al., 2017).",2 Related Work,[0],[0]
"Building interpretable yet accurate models is extremely important in the medical domain (Shickel et al., 2017).",2 Related Work,[0],[0]
"We generate large-scale ground truth annotations (questions, logical forms, and answers) that can provide supervision to learn such hybrid models.",2 Related Work,[0],[0]
"Our approach to generating emrQA is in the same spirit as Su et al. (2016), who generate graph queries (logical forms) from a structured KB and use them to collect answers.",2 Related Work,[0],[0]
"In contrast, our framework can be applied to generate QA dataset in any domain with minimal expert input using annotations from other NLP tasks.",2 Related Work,[0],[0]
"Our general framework for generating a largescale QA corpus given certain resources consists of three steps: (1) collecting questions to capture domain-specific user needs, followed by normalizing the collected questions to templates by replacing entities (that may be related via binary or composite relations) in the question with placeholders.",3 QA Dataset Generation Framework,[0],[0]
"The entity types replaced in the question are grounded in an ontology like WordNet (Miller, 1995), UMLS (Bodenreider, 2004), or a usergenerated schema that defines and relates different entity types.",3 QA Dataset Generation Framework,[0],[0]
"(2) We associate question templates with expert-annotated logical form templates; logical forms are symbolic representations using relations from the ontology/schema to express the relations in the question, and associate the ques-
tion entity type with an answer entity type.",3 QA Dataset Generation Framework,[0],[0]
(3) We then proceed to the important step of re-purposing existing NLP annotations to populate questionlogical form templates and generate answers.,3 QA Dataset Generation Framework,[0],[0]
QA is a complex task that requires addressing several fundamental NLP problems before accurately answering a question.,3 QA Dataset Generation Framework,[0],[0]
"Hence, obtaining expert manual annotations in complex domains is infeasible as it is tedious to expert-annotate answers that may be found across long document collections (e.g., longitudinal EMR) (Lee et al., 2017).",3 QA Dataset Generation Framework,[0],[0]
"Thus, we reverse engineer the process where we reuse expert annotations available in NLP tasks such as entity recognition, coreference, and relation learning, based on the information captured in the logical forms to populate entity placeholders in templates and generate answers.",3 QA Dataset Generation Framework,[0],[0]
Reverse engineering serves as a proxy expert ensuring that the generated QA annotations are credible.,3 QA Dataset Generation Framework,[0],[0]
"The only manual effort is in annotating logical forms, thus significantly reducing expert labor.",3 QA Dataset Generation Framework,[0],[0]
"Moreover, in domain specific instances such as EMRs, manually annotated logical forms allow the experts to express information essential for natural language understanding such as domain knowledge, temporal relations, and negation (Gao et al.; Chabierski et al., 2017).",3 QA Dataset Generation Framework,[0],[0]
"This knowledge, once captured, can be used to generate QA pairs on new documents, making the framework scalable.",3 QA Dataset Generation Framework,[0],[0]
"We apply the proposed framework to generate the emrQA corpus consisting of questions posed by physicians against longitudinal EMRs of a patient, using annotations provided by i2b2",4 Generating the emrQA Dataset,[0],[0]
(Figure 2).,4 Generating the emrQA Dataset,[0],[0]
"We collect questions for EMR QA by, 1) polling physicians at the Veterans Administration for what they frequently want to know from the EMR (976 questions), 2) using an existing source of 5,696 questions generated by a team of medical experts from 71 patient records (Raghavan et al., 2017) and 3) using 15 prototypical questions from an ob-
servational study done by physicians (Tang et al., 1994).",4.1 Question Collection and Normalization,[0],[0]
"To obtain templates, the questions were automatically normalized by identifying medical entities (using MetaMap (Aronson, 2001)) in questions and replacing them with generic placeholders.",4.1 Question Collection and Normalization,[0],[0]
The resulting ∼2K noisy templates were expert reviewed and corrected (to account for any entity recognition errors by MetaMap).,4.1 Question Collection and Normalization,[0],[0]
"We align our entity types to those defined in the i2b2 concept extraction tasks (Uzuner et al., 2010a, 2011) - problem, test, treatment, mode and medication.",4.1 Question Collection and Normalization,[0],[0]
"E.g., The question What is the dosage of insulin?",4.1 Question Collection and Normalization,[0],[0]
from the collection gets converted to the template What is the dosage of |medication|?,4.1 Question Collection and Normalization,[0],[0]
as shown in Fig.2.,4.1 Question Collection and Normalization,[0],[0]
This process resulted in 680 question templates.,4.1 Question Collection and Normalization,[0],[0]
"We do not correct for the usage/spelling errors in these templates, such as usage of ""pt"" for ""patient"", or make the templates gender neutral in order to provide a true representation of physicians’ questions.",4.1 Question Collection and Normalization,[0],[0]
"Further, analyzing these templates shows that physicians most frequently ask about test results (11%), medications for problem (9%), and problem existence (8%).",4.1 Question Collection and Normalization,[0],[0]
"The long tail following this includes questions about medication dosage, response to treatment, medication duration, prescription date, etiology, etc.",4.1 Question Collection and Normalization,[0],[0]
"Temporal constraints were frequently imposed on questions related to tests, problem diagnosis and medication start/stop.",4.1 Question Collection and Normalization,[0],[0]
"The 680 question templates were annotated by a physician with their corresponding logical form (LF) templates, which resulted in 94 unique LF templates.",4.2 Associating Templates w/ Logical Forms,[0],[0]
More than one question template that map to the same LF are considered paraphrases of each other and correspond to a particular question type (Table 2).,4.2 Associating Templates w/ Logical Forms,[0],[0]
"Logical forms are defined based
on an ontology schema designed by medical experts (Figure 3).",4.2 Associating Templates w/ Logical Forms,[0],[0]
"This schema captures entities in unstructured clinical notes through medical events and their attributes, interconnected through relations.",4.2 Associating Templates w/ Logical Forms,[0],[0]
"We align the entity and relation types of i2b2 to this schema.
",4.2 Associating Templates w/ Logical Forms,[0],[0]
A formal representation of the LF grammar using this schema (Figure 3) is as follows.,4.2 Associating Templates w/ Logical Forms,[0],[0]
"Medical events are denoted as MEi (e.g LabEvent, ConditionEvent) and relations are denoted as REi (e.g conducted/reveals).",4.2 Associating Templates w/ Logical Forms,[0],[0]
"Now, ME[a1, .., aj , .., oper(an)] is a medical event where aj represents the attribute of the event (such as result in LabEvent).",4.2 Associating Templates w/ Logical Forms,[0],[0]
"An event may optionally include constraints on attributes captured by an operator (oper() ∈ sort, range, check for null values, compare).",4.2 Associating Templates w/ Logical Forms,[0],[0]
"These operators sometimes require values from external medical KB (indicated by ref, e.g. lab.ref low/lab.ref high to indicate range of reference standards considered healthy in lab results) indicating the need for medical knowledge to answer the question.",4.2 Associating Templates w/ Logical Forms,[0],[0]
"Using these constructs, a LF can be defined using the following rules, LF →MEi |M1 relation M2 M1 →MEi, M2 →MEj M1 →M1 relation M2, M2 →M1 relation M2 relation→",4.2 Associating Templates w/ Logical Forms,[0],[0]
OR |,4.2 Associating Templates w/ Logical Forms,[0],[0]
"AND | REi
Advantages of our LF representation include the ability to represent composite relations, define attributes for medical events and constrain the attributes to precisely capture the information need in the question.",4.2 Associating Templates w/ Logical Forms,[0],[0]
"While these can be achieved using different methods that combine lambda calculus and first order logic (Roberts and Demner-Fushman, 2016), our representation is more human comprehensible.",4.2 Associating Templates w/ Logical Forms,[0],[0]
This allows a physician to consider an ontology like Figure 3 and easily define a logical form.,4.2 Associating Templates w/ Logical Forms,[0],[0]
Some example question templates with their LF annotations are described in Table 3 using the above notation.,4.2 Associating Templates w/ Logical Forms,[0],[0]
The LF representation of the question in Figure 2 is MedicationEvent(|medication|),4.2 Associating Templates w/ Logical Forms,[0],[0]
[dosage=x].,4.2 Associating Templates w/ Logical Forms,[0],[0]
The entities seen in LF are the entities posed in the question and entity marked x indicates the answer entity type.,4.2 Associating Templates w/ Logical Forms,[0],[0]
"The next step in the process is to populate the question and logical form (QL) templates with existing annotations in the i2b2 clinical datasets and extract answer evidence for the questions.
",4.3 Template Filling and Answer Extraction,[0],[0]
"The i2b2 datasets are expert annotated with fine-grained annotations (Guo et al., 2006) that were developed for various shared NLP challenge tasks, including (1) smoking status classification (Uzuner et al., 2008), (2) diagnosis of obesity and its co-morbidities (Uzuner, 2009), extraction of (3) medication concepts (Uzuner et al., 2010a), (4) relations, concepts, assertions (Uzuner et al., 2010b, 2011) (5) co-reference resolution (Uzuner et al., 2012) and (6) heart disease risk factor identification (Stubbs and Uzuner, 2015).",4.3 Template Filling and Answer Extraction,[0],[0]
"In Figure 2, this would correspond to leveraging annotations from medications challenge between medications and their dosages, such as medication=Nitroglycerin, dosage=40mg, to populate |medication| and generate several instances of the question “What is the dosage of |medication|?"" and its corresponding logical form MedicationEvent(|medication|)[dosage=x].",4.3 Template Filling and Answer Extraction,[0],[0]
"The answer would be derived from the value of the dosage entity in the dataset.
",4.3 Template Filling and Answer Extraction,[0],[0]
Preprocessing: The i2b2 entities are preprocessed before using them with our templates to ensure syntactic correctness of the generated questions.,4.3 Template Filling and Answer Extraction,[0],[0]
"The pre-processing steps are designed based on the i2b2 annotations syntax guidelines (Guo et al., 2006).",4.3 Template Filling and Answer Extraction,[0],[0]
"To estimate grammatical correctness, we randomly sampled 500 generated questions and found that <5% had errors.",4.3 Template Filling and Answer Extraction,[0],[0]
"These errors include, among others, incorrect usage of article with the entity and incorrect entity phrasing.
",4.3 Template Filling and Answer Extraction,[0],[0]
Answer Extraction: The final step in the process is generating answer evidence corresponding to each question.,4.3 Template Filling and Answer Extraction,[0],[0]
The answers in emrQA are defined differently; instead of a single word or phrase we provide the entire i2b2 annotation line from the clinical note as the answer.,4.3 Template Filling and Answer Extraction,[0],[0]
"This is because the context in which the answer entity or phrase is mentioned is extremely important in clinical decision making (Demner-Fushman et al., 2009).
",4.3 Template Filling and Answer Extraction,[0],[0]
"Hence, we call them answer evidence instead of just answers.",4.3 Template Filling and Answer Extraction,[0],[0]
"For example, consider the question Is the patient’s hypertension controlled?.",4.3 Template Filling and Answer Extraction,[0],[0]
The answer to this question is not a simple yes/no since the status of the patient’s hypertension can change through the course of treatment.,4.3 Template Filling and Answer Extraction,[0],[0]
"The answer evidence to this question in emrQA are multiple lines across the longitudinal notes that reflect this potentially changing status of the patients condition, e.g. Hypertension-borderline today.",4.3 Template Filling and Answer Extraction,[0],[0]
"Additionally, for questions seeking specific answers we also provide the corresponding answer entities.
",4.3 Template Filling and Answer Extraction,[0],[0]
The overall process for answer evidence generation was vetted by a physician.,4.3 Template Filling and Answer Extraction,[0],[0]
Here is a brief overview of how the different i2b2,4.3 Template Filling and Answer Extraction,[0],[0]
datasets were used in generating answers.,4.3 Template Filling and Answer Extraction,[0],[0]
The relations challenge datasets have various event-relation annotations across single/multiple lines in a clinical note.,4.3 Template Filling and Answer Extraction,[0],[0]
"We used a combination of one or more of these, to generate answers for a question; in doing so we used the annotations provided by the i2b2 co-reference datasets.",4.3 Template Filling and Answer Extraction,[0],[0]
"Similarly, the medications challenge dataset has various event-attribute annotations but since this dataset is not provided with co-reference annotations, it is currently not possible to combine all valid answers.",4.3 Template Filling and Answer Extraction,[0],[0]
The heart disease challenge dataset has longitudinal notes (∼5 per patient) with record dates.,4.3 Template Filling and Answer Extraction,[0],[0]
The events in this dataset are also provided with time annotations and are rich in quantitative entities.,4.3 Template Filling and Answer Extraction,[0],[0]
This dataset was primarily used to answer questions that require temporal and arithmetic reasoning on events.,4.3 Template Filling and Answer Extraction,[0],[0]
The patient records in the smoking and obesity challenge datasets are categorized into classes with no entity annotations.,4.3 Template Filling and Answer Extraction,[0],[0]
"Thus, for questions generated on these datasets, the entire document acts as evidence and the annotated class information (7 classes) needs to be predicted as the answer.
",4.3 Template Filling and Answer Extraction,[0],[0]
"The total questions, LFs and answers gener-
ated using this framework are summarized in Table 1.",4.3 Template Filling and Answer Extraction,[0],[0]
Consider the question How much does the patient smoke?,4.3 Template Filling and Answer Extraction,[0],[0]
for which we do not have i2b2annotations to provide an answer.,4.3 Template Filling and Answer Extraction,[0],[0]
"In cases where the answer entity is empty, we only generate the question and LF, resulting in more question types being used for QL than QA pairs: only 53% of question types have answers.",4.3 Template Filling and Answer Extraction,[0],[0]
"We analyze the complexity of emrQA by considering the LFs for question characteristics, variations in paraphrases, and the type of reasoning required for answering questions (Table 2, 3, 4).",5 emrQA Dataset Analysis,[0],[0]
"A quantitative and qualitative analysis of emrQA question templates is shown in Table 3, where logical forms help formalize their characteristics (Su et al., 2016).",5.1 Question/Logical Form Characteristics,[0],[0]
"Questions may request specific finegrained information (attribute values like dosage) or may express a more coarse-grained need (event entities like medications etc), or a combination of both.",5.1 Question/Logical Form Characteristics,[0],[0]
25% of questions require complex operators (e.g compare(>)) and 12% of questions express the need for external medical knowledge (e.g. lab.refhigh).,5.1 Question/Logical Form Characteristics,[0],[0]
"The questions in emrQA are highly compositional, where 47% of question templates have at least one event relation.",5.1 Question/Logical Form Characteristics,[0],[0]
"Questions templates that map to the same LF are considered paraphrases (e.g, Table 2) and correspond to the same question type.",5.2 Paraphrase Complexity Analysis,[0],[0]
"In emrQA, an average of 7 paraphrase templates exist per question type.",5.2 Paraphrase Complexity Analysis,[0],[0]
This is representative of FAQ types that are perhaps more important to the physician.,5.2 Paraphrase Complexity Analysis,[0],[0]
"Good paraphrases are lexically dissimilar to each other (Chen and Dolan, 2011).",5.2 Paraphrase Complexity Analysis,[0],[0]
"In order to understand the lexical variation within our paraphrases, we randomly select a question from the list of paraphrases as a reference and evaluate the others with respect to the reference, and report the average BLEU (0.74 ± 0.06) and Jaccard Score (0.72 ± 0.19).",5.2 Paraphrase Complexity Analysis,[0],[0]
"The low BLEU and Jaccard score with large standard deviation indicates the lexical diversity captured by emrQA’s paraphrases (Papineni et al., 2002; Niwattanakul et al., 2013).",5.2 Paraphrase Complexity Analysis,[0],[0]
"33% of the questions in emrQA have more than one answer evidence, with the number ranging
from 2 to 61.",5.3 Answer Evidence Analysis,[0],[0]
"E.g., the question Medications Record? has all medications in the patient’s longitudinal record as answer evidence.",5.3 Answer Evidence Analysis,[0],[0]
"In order to analyze the reasoning required to answer emrQA questions, we sampled 35 clinical notes from the corpus and analyzed 3 random questions per note by manually labeling them with the categories described in Table 4.",5.3 Answer Evidence Analysis,[0],[0]
Categories are not mutually exclusive: a single example can fall into multiple categories.,5.3 Answer Evidence Analysis,[0],[0]
"We compare and contrast this analysis with SQuAD (Rajpurkar et al., 2016), a popular MC dataset generated through crowdsourcing, to show that the framework is capable of generating a corpus as representative and even more complex.",5.3 Answer Evidence Analysis,[0],[0]
"Compared to SQuAD, emrQA offers two new reasoning categories, temporal and arithmetic which make up 31% of the dataset.",5.3 Answer Evidence Analysis,[0],[0]
"Additionally, over two times as many questions in emrQA require reasoning over multiple sentences.",5.3 Answer Evidence Analysis,[0],[0]
"Long and noisy documents make the question answering task more difficult (Joshi et al., 2017).",5.3 Answer Evidence Analysis,[0],[0]
EMRs are inherently noisy and hence 29% have incomplete context and the document length is 27 times more than SQuAD which offers new challenges to existing QA models.,5.3 Answer Evidence Analysis,[0],[0]
"Owing to the domain specific nature of the task, 39% of the examples required some form of medical/world knowledge.
",5.3 Answer Evidence Analysis,[0],[0]
"As discussed in Section 4.3, 12% of the questions in emrQA corpus require a class category from i2b2 smoking and obesity datasets to be predicted.",5.3 Answer Evidence Analysis,[0],[0]
"We also found 6% of the questions had other possible answers that were not included by emrQA, this is because of the lack of co-reference annotations for the medications challenge.",5.3 Answer Evidence Analysis,[0],[0]
We implement baseline models using neural and heuristic methods for question to logical form (QL) and question to answer (Q-A) mapping.,6 Baseline Methods,[0],[0]
"Heuristic Models: We use a template-matching approach where we first split the data into train/test sets, and then normalize questions in the test set into templates by replacing entities with placeholders.",6.1 Q-L Mapping,[0],[0]
"The templates are then scored against the ground truth templates of the questions in the train set, to find the best match.",6.1 Q-L Mapping,[0],[0]
The placeholders in the LF template corresponding to the best matched question template is then filled with the normalized entities to obtain the predicted LF.,6.1 Q-L Mapping,[0],[0]
"To normalize the test questions we use CLiNER
(Boag et al., 2015) for emrQA and Jia and Liang (2016)’s work for ATIS and GeoQuery.",6.1 Q-L Mapping,[0],[0]
"Scoring and matching is done using two heuristics: (1) HM-1, which computes an identical match, and (2) HM-2, which generates a GloVe vector (Arora et al., 2016) representation of the templates using sentence2vec and then computes pairwise cosine similarity.
",6.1 Q-L Mapping,[0],[0]
"Neural Model: We train a sequence-tosequence (seq2seq) (Sutskever et al., 2014) with attention paradigm (Bahdanau et al., 2014; Luong et al., 2017)",6.1 Q-L Mapping,[0],[0]
"as our neural baseline (2 layers, each with 64 hidden units)",6.1 Q-L Mapping,[0],[0]
.,6.1 Q-L Mapping,[0],[0]
The same setting when used with Geoquery and ATIS gives poor results because the parameters are not appropriate for the nature of that dataset.,6.1 Q-L Mapping,[0],[0]
"Hence, for comparison with GeoQuery and ATIS, we use the results of seq2seq model with a single 200 hidden units layer (Jia and Liang, 2016).",6.1 Q-L Mapping,[0],[0]
"At test time we automatically balance missing right parentheses.
†results from Jia and Liang (2016)",6.1 Q-L Mapping,[0],[0]
We randomly partition the QL pairs in the dataset in train(80%) and test(20%) sets in two ways.,6.1.1 Experimental Setup,[0],[0]
"(1) In emrQL-1, we first split the paraphrase templates corresponding to a single LF template into train and test, and then generate the instances of QL pairs.",6.1.1 Experimental Setup,[0],[0]
"(2) In emrQL-2, we first generate the instances of QL pairs from the templates and then distribute them into train and test sets.",6.1.1 Experimental Setup,[0],[0]
"As a result, emrQL-1 has more lexical variation between train and test distribution compared to emrQL-2, resulting in increased paraphrase complexity.",6.1.1 Experimental Setup,[0],[0]
"We use accuracy i.e, the total number of logical forms predicted correctly as a metric to evaluate our model.",6.1.1 Experimental Setup,[0],[0]
The performance of the proposed models is summarized in Table 5.,6.1.2 Results,[0],[0]
emrQL results are not directly comparable with GeoQuery and ATIS because of the differences in the lexicon and tools available for the domains.,6.1.2 Results,[0],[0]
"However, it helps us establish that QL learning in emrQA is non-trivial and supports significant future work.
",6.1.2 Results,[0],[0]
Error analysis of heuristic models on emrQL-1 and emrQL-2 showed that 70% of the errors occurred because of incorrect question normalization.,6.1.2 Results,[0],[0]
"In fact, 30% of these questions had not been normalized at all.",6.1.2 Results,[0],[0]
"This shows that the entities
added to the templates are complex and diverse and make the inverse process of template generation non trivial.",6.1.2 Results,[0],[0]
"This makes a challenging QL corpus that cannot trivially be solved by template matching based approaches.
",6.1.2 Results,[0],[0]
"Errors made by the neural model on both emrQL-1 and emrQL-2 are due to long LFs (20%) and incorrectly identified entities (10%), which are harder for the attention-based model (Jia and Liang, 2016).",6.1.2 Results,[0],[0]
"The increased paraphrase complexity in emrQL-1 compared to emrQL-2 resulted in 20% more structural errors in emrQL-1, where the predicted event/grammar structure deviates significantly from the ground truth.",6.1.2 Results,[0],[0]
This shows that the model is not adequately capturing the semantics in the questions to generalize to new paraphrases.,6.1.2 Results,[0],[0]
"Therefore, emrQL-1 can be used to benchmark QL models robust to paraphrasing.",6.1.2 Results,[0],[0]
"Question-answering on emrQA consists of two different tasks, (1) extraction of answer line from the clinical note (machine comprehension (MC)) and (2) prediction of answer class based on the entire clinical note.",6.2 Q-A Mapping,[0],[0]
"We provide baseline models to illustrate the complexity in doing both these tasks.
",6.2 Q-A Mapping,[0],[0]
"Machine Comprehension: To do extractive QA on EMRs, we use DrQA’s (Chen et al., 2017) document reader which is a multi-layer RNN based MC model.",6.2 Q-A Mapping,[0],[0]
"We use their best performing settings trained for SQuAD data using Glove vectors (300 dim-840B).
",6.2 Q-A Mapping,[0],[0]
Class Prediction: We build a multi-class logistic regression model for predicting a class as an answer based on the patient’s clinical note.,6.2 Q-A Mapping,[0],[0]
Features input to the classifier are TF-IDF vectors of the question and the clinical notes taken from i2b2 smoking and obesity datasets.,6.2 Q-A Mapping,[0],[0]
We consider a 80-20 split of the data for train-test.,6.2.1 Experimental setup,[0],[0]
"In order to evaluate worst-case performance, we train on question-evidence pairs in a clinical note obtained by using only one random paraphrase for a question instead of all the paraphrases.",6.2.1 Experimental setup,[0],[0]
We use a slightly modified‡ version of the two popularly reported metrics in MC for evaluation since our evidence span is longer: Exact Match (EM) and F1.,6.2.1 Experimental setup,[0],[0]
"Wherever the answer entity in an evidence is explicitly known, EM checks if the answer entity is
‡using the original definitions, the evaluated values were far less than those obtained in Table 7
present within the evidence, otherwise it checks if the predicted evidence span lies within ±20 characters of the ground truth evidence.",6.2.1 Experimental setup,[0],[0]
For F1 we construct a bag of tokens for each evidence string and measure the F1 score of the overlap between the two bags of tokens.,6.2.1 Experimental setup,[0],[0]
"Since there may be multiple evidence for a given question, we consider only the top 10 predictions and report an average of EM and F1 over ground truth number of answers.",6.2.1 Experimental setup,[0],[0]
"In the class prediction setting, we report the subset accuracy.",6.2.1 Experimental setup,[0],[0]
The performance of the proposed models is summarized in Table 7.,6.2.2 Results,[0],[0]
DrQA is one of the best performing models on SQuAD with an F1 of 78.8 and EM of 69.5.,6.2.2 Results,[0],[0]
"The relatively low performance of the models on emrQA (60.6 F1 and 59.2 EM) shows that QA on EMRs is a complex task and offers new challenges to existing QA models.
",6.2.2 Results,[0],[0]
"To understand model performance, we macroaverage the EM across all the questions corresponding to a LF template.",6.2.2 Results,[0],[0]
We observe that LFs representing temporal and arithmetic§ needs had,6.2.2 Results,[0],[0]
< 16% EM. LFs expressing the need for medical KB§ performed poorly since we used general Glove embeddings.,6.2.2 Results,[0],[0]
"An analysis of LFs which had approximately equal number of QA pair representation in the test set revealed an interesting relation between the model performance and LF complexity, as summarized in Table 6.",6.2.2 Results,[0],[0]
"The trend shows that performance is worse on multiple relation questions as compared to single relation and attribute questions, showing that the LFs sufficiently capture the complexity of the questions and give us an ability to do a qualitative model analysis.
",6.2.2 Results,[0],[0]
"Error analysis on a random sample of 50 questions containing at least one answer entity in an evidence showed that: (1) 38% of the examples required multiple sentence reasoning of which 16% were due to a missing evidence in a multiple evidence question, (2) 14% were due to syntactic variation, (3) 10% required medical reasoning and (4) in 14%, DrQA predicted an incomplete evidence span missing the answer entity in it.
",6.2.2 Results,[0],[0]
§maximum representation of these templates comes from the i2b2 heart disease risk dataset,6.2.2 Results,[0],[0]
"In this section, we describe how our generation framework may also be applied to generate opendomain QA datasets given the availability of other NLP resources.",7 Discussion,[0],[0]
"We also discuss possible extensions of the framework to increase the complexity of the generated datasets.
",7 Discussion,[0],[0]
"Open domain QA dataset generation: Consider the popularly used SQuAD (Rajpurkar et al., 2016) reading comprehension dataset generated by crowdworkers, where the answer to every question is a segment of text from the corresponding passage in the Wikipedia article.",7 Discussion,[0],[0]
"This dataset can easily be generated or extended using our proposed framework with existing NLP annotations on Wikipedia (Auer et al., 2007; Nothman et al., 2008; Ghaddar and Langlais, 2017).
",7 Discussion,[0],[0]
"For instance, consider DBPedia (Auer et al., 2007), an existing dataset of entities and their relations extracted from Wikipedia.",7 Discussion,[0],[0]
It also has its own ontology which can serve as the semantic frames schema to define logical forms.,7 Discussion,[0],[0]
"Using these resources, our reverse engineering technique for QA dataset generation can be applied as follows.",7 Discussion,[0],[0]
(1) Question templates can be defined for each entity type and relation in DBPedia.,7 Discussion,[0],[0]
"For example¶, consider the relation [place, country] field in DBpedia.",7 Discussion,[0],[0]
For this we can define a question template In what country is |place| located?.,7 Discussion,[0],[0]
(2) Every such question template can be annotated with a logical form template using existing DBPedia ontology.,7 Discussion,[0],[0]
"(3) By considering the entity values of DBPedia fields such as [place=Normandy, dbo:country=France], we can automatically generate the question In what country is Normandy located?",7 Discussion,[0],[0]
and its corresponding logical form from the templates.,7 Discussion,[0],[0]
"The text span of country=France from the Wikipedia passage is then used as the answer (Daiber et al., 2013).",7 Discussion,[0],[0]
"Currently, this QA pair instance is a part of the SQuAD dev set.",7 Discussion,[0],[0]
Using our framework we can generate many more instances like this example from different Wikipedia passages - without crowdsourcing efforts.,7 Discussion,[0],[0]
"¶example reference: http://dbpedia.org/page/Normandy
Extensions to the framework: The complexity of the generated dataset can be further extended as follows.",7 Discussion,[0],[0]
(1) We can use a coreferred or a lexical variant of the original entity in the question-logical form generation.,7 Discussion,[0],[0]
This can allow for increased lexical variation between the question and answer line entities in the passage.,7 Discussion,[0],[0]
(2) It is possible to combine two or more question templates to make compositional questions with the answers to these questions similarly combined.,7 Discussion,[0],[0]
This can also result in more multiple sentence reasoning questions.,7 Discussion,[0],[0]
(3) We can generate questions with entities not related to the context in the passage.,7 Discussion,[0],[0]
"This can increase empty answer questions in the dataset, resulting in increased negative training examples.",7 Discussion,[0],[0]
We propose a novel framework that can generate a large-scale QA dataset using existing resources and minimal expert input.,8 Conclusions and Future Work,[0],[0]
"This has the potential to make a huge impact in domains like medicine, where obtaining manual QA annotations is tedious and infeasible.",8 Conclusions and Future Work,[0],[0]
"We apply this framework to generate a large scale EMR QA corpus (emrQA), consisting of 400,000 question-answers pairs and 1 million question-logical forms, and analyze the complexity of the dataset to show its non-trivial nature.",8 Conclusions and Future Work,[0],[0]
We show that the logical forms provide a symbolic representation that is very useful for corpus generation and for model analysis.,8 Conclusions and Future Work,[0],[0]
The logical forms also provide an opportunity to build interpretable systems by perhaps jointly (or latently) learning the logical form and answer for a question.,8 Conclusions and Future Work,[0],[0]
"In future, this framework may be applied to also re-purpose and integrate other NLP datasets such as MIMIC and generate a more diverse and representative EMR QA corpus (Johnson et al., 2016).",8 Conclusions and Future Work,[0],[0]
"This project is partially funded by Sloan Research Fellowship, PhRMA Foundation Award in Informatics, and NSF Career Award (1652815).",Acknowledgments,[0],[0]
The authors would like to thank Siddharth Patwardhan for his valuable feedback in formatting the paper.,Acknowledgments,[0],[0]
We propose a novel methodology to generate domain-specific large-scale question answering (QA) datasets by re-purposing existing annotations for other NLP tasks.,abstractText,[0],[0]
We demonstrate an instance of this methodology in generating a large-scale QA dataset for electronic medical records by leveraging existing expert annotations on clinical notes for various NLP tasks from the community shared i2b2 datasets§.,abstractText,[0],[0]
"The resulting corpus (emrQA) has 1 million questions-logical form and 400,000+ question-answer evidence pairs.",abstractText,[0],[0]
We characterize the dataset and explore its learning potential by training baseline models for question to logical form and question to answer mapping.,abstractText,[0],[0]
emrQA: A Large Corpus for Question Answering on Electronic Medical Records,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1506–1515 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Semantic role labeling (SRL) (Gildea and Jurafsky, 2002) can be informally described as the task of discovering who did what to whom.",1 Introduction,[0],[0]
"For example, consider an SRL dependency graph shown above the sentence in Figure 1.",1 Introduction,[0],[0]
"Formally, the task includes (1) detection of predicates (e.g., makes); (2) labeling the predicates with a sense from a sense inventory (e.g., make.01); (3) identifying and assigning arguments to semantic roles (e.g., Sequa is A0, i.e., an agent / ‘doer’ for the corresponding predicate, and engines is A1, i.e., a patient / ‘an affected entity’).",1 Introduction,[0],[0]
"SRL is often regarded
as an important step in the standard NLP pipeline, providing information to downstream tasks such as information extraction and question answering.
",1 Introduction,[0],[0]
"The semantic representations are closely related to syntactic ones, even though the syntaxsemantics interface is far from trivial (Levin, 1993).",1 Introduction,[0],[0]
"For example, one can observe that many arcs in the syntactic dependency graph (shown in black below the sentence in Figure 1) are mirrored in the semantic dependency graph.",1 Introduction,[0],[0]
"Given these similarities and also because of availability of accurate syntactic parsers for many languages, it seems natural to exploit syntactic information when predicting semantics.",1 Introduction,[0],[0]
"Though historically most SRL approaches did rely on syntax (Thompson et al., 2003; Pradhan et al., 2005; Punyakanok et al., 2008; Johansson and Nugues, 2008), the last generation of SRL models put syntax aside in favor of neural sequence models, namely LSTMs (Zhou and Xu, 2015; Marcheggiani et al., 2017), and outperformed syntactically-driven methods on standard benchmarks.",1 Introduction,[0],[0]
"We believe that one of the reasons for this radical choice is the lack of simple and effective methods for incorporating syntactic information into sequential neural networks (namely, at the level of words).",1 Introduction,[0],[0]
"In this paper we
1506
propose one way how to address this limitation.",1 Introduction,[0],[0]
"Specifically, we rely on graph convolutional networks (GCNs) (Duvenaud et al., 2015; Kipf and Welling, 2017; Kearnes et al., 2016), a recent class of multilayer neural networks operating on graphs.",1 Introduction,[0],[0]
"For every node in the graph (in our case a word in a sentence), GCN encodes relevant information about its neighborhood as a real-valued feature vector.",1 Introduction,[0],[0]
GCNs have been studied largely in the context of undirected unlabeled graphs.,1 Introduction,[0],[0]
"We introduce a version of GCNs for modeling syntactic dependency structures and generally applicable to labeled directed graphs.
",1 Introduction,[0],[0]
"One layer GCN encodes only information about immediate neighbors and K layers are needed to encode K-order neighborhoods (i.e., information about nodes at most K hops aways).",1 Introduction,[0],[0]
"This contrasts with recurrent and recursive neural networks (Elman, 1990; Socher et al., 2013) which, at least in theory, can capture statistical dependencies across unbounded paths in a trees or in a sequence.",1 Introduction,[0],[0]
"However, as we will further discuss in Section 3.3, this is not a serious limitation when GCNs are used in combination with encoders based on recurrent networks (LSTMs).",1 Introduction,[0],[0]
"When we stack GCNs on top of LSTM layers, we obtain a substantial improvement over an already state-of-the-art LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009), both for English and Chinese.1
Interestingly, again unlike recursive neural networks, GCNs do not constrain the graph to be a tree.",1 Introduction,[0],[0]
"We believe that there are many applications in NLP, where GCN-based encoders of sentences or even documents can be used to incorporate knowledge about linguistic structures (e.g., representations of syntax, semantics or discourse).",1 Introduction,[0],[0]
"For example, GCNs can take as input combined syntactic-semantic graphs (e.g., the entire graph from Figure 1) and be used within downstream tasks such as machine translation or question answering.",1 Introduction,[0],[0]
"However, we leave this for future work and here solely focus on SRL.
",1 Introduction,[0],[0]
"The contributions of this paper can be summarized as follows:
• we are the first to show that GCNs are effective for NLP;
• we propose a generalization of GCNs suited 1The code is available at https://github.com/
diegma/neural-dep-srl.
",1 Introduction,[0],[0]
"to encoding syntactic information at word level;
• we propose a GCN-based SRL model and obtain state-of-the-art results on English and Chinese portions of the CoNLL-2009 dataset;
• we show that bidirectional LSTMs and syntax-based GCNs have complementary modeling power.",1 Introduction,[0],[0]
In this section we describe GCNs of Kipf and Welling (2017).,2 Graph Convolutional Networks,[0],[0]
"Please refer to Gilmer et al. (2017) for a comprehensive overview of GCN versions.
",2 Graph Convolutional Networks,[0],[0]
"GCNs are neural networks operating on graphs and inducing features of nodes (i.e., real-valued vectors / embeddings) based on properties of their neighborhoods.",2 Graph Convolutional Networks,[0],[0]
"In Kipf and Welling (2017), they were shown to be very effective for the node classification task: the classifier was estimated jointly with a GCN, so that the induced node features were informative for the node classification problem.",2 Graph Convolutional Networks,[0],[0]
"Depending on how many layers of convolution are used, GCNs can capture information only about immediate neighbors (with one layer of convolution) or any nodes at most K hops aways (if K layers are stacked on top of each other).
",2 Graph Convolutional Networks,[0],[0]
"More formally, consider an undirected graph G = (V, E), where V (|V | = n) and E are sets of nodes and edges, respectively.",2 Graph Convolutional Networks,[0],[0]
"Kipf and Welling (2017) assume that edges contain all the self-loops, i.e., (v, v) 2 E for any v. We can define a matrix X 2 Rm⇥n with each its column xv 2 Rm (v 2 V) encoding node features.",2 Graph Convolutional Networks,[0],[0]
"The vectors can either encode genuine features (e.g., this vector can encode the title of a paper if citation graphs are considered) or be a one-hot vector.",2 Graph Convolutional Networks,[0],[0]
"The node representation, encoding information about its immediate neighbors, is computed as
hv = ReLU 0@",2 Graph Convolutional Networks,[0],[0]
"X u2N (v) (W xu + b) 1A , (1) where W 2 Rm⇥m and b 2 Rm are a weight matrix and a bias, respectively; N (v) are neighbors of v; ReLU is the rectifier linear unit activation function.2",2 Graph Convolutional Networks,[0],[0]
"Note that v 2 N (v) (because of selfloops), so the input feature representation of v (i.e. xv) affects its induced representation hv.
2We dropped normalization factors used in Kipf and Welling (2017), as they are not used in our syntactic GCNs.
",2 Graph Convolutional Networks,[0],[0]
"As in standard convolutional networks (LeCun et al., 2001), by stacking GCN layers one can incorporate higher degree neighborhoods:
h(k+1)v = ReLU 0@ X u2N (v) W (k)h(k)u + b (k) 1A where k denotes the layer number and h(1)v = xv.",2 Graph Convolutional Networks,[0],[0]
"As syntactic dependency trees are directed and labeled (we refer to the dependency labels as syntactic functions), we first need to modify the computation in order to incorporate label information (Section 3.1).",3 Syntactic GCNs,[0],[0]
"In the subsequent section, we incorporate gates in GCNs, so that the model can decide which edges are more relevant to the task in question.",3 Syntactic GCNs,[0],[0]
"Having gates is also important as we rely on automatically predicted syntactic representations, and the gates can detect and downweight potentially erroneous edges.",3 Syntactic GCNs,[0],[0]
"Now, we introduce a generalization of GCNs appropriate for syntactic dependency trees, and in
general, for directed labeled graphs.",3.1 Incorporating directions and labels,[0],[0]
"First note that there is no reason to assume that information flows only along the syntactic dependency arcs (e.g., from makes to Sequa), so we allow it to flow in the opposite direction as well (i.e., from dependents to heads).",3.1 Incorporating directions and labels,[0],[0]
"We use a graph G = (V, E), where the edge set contains all pairs of nodes (i.e., words) adjacent in the dependency tree.",3.1 Incorporating directions and labels,[0],[0]
"In our example, both (Sequa, makes) and (makes, Sequa) belong to the edge set.",3.1 Incorporating directions and labels,[0],[0]
"The graph is labeled, and the label L(u, v) for (u, v) 2 E contains both information about the syntactic function and indicates whether the edge is in the same or opposite direction as the syntactic dependency arc.",3.1 Incorporating directions and labels,[0],[0]
"For example, the label for (makes, Sequa) is subj, whereas the label for (Sequa, makes) is subj0, with the apostrophe indicating that the edge is in the direction opposite to the corresponding syntactic arc.",3.1 Incorporating directions and labels,[0],[0]
"Similarly, self-loops will have label self .",3.1 Incorporating directions and labels,[0],[0]
"Consequently, we can simply assume that the GCN parameters are label-specific, resulting in the following computation, also illustrated in Figure 2:
h(k+1)v = ReLU 0@ X u2N (v) W (k) L(u,v)h (k) u + b",3.1 Incorporating directions and labels,[0],[0]
"(k) L(u,v) 1A .",3.1 Incorporating directions and labels,[0],[0]
"This model is over-parameterized,3 especially given that SRL datasets are moderately sized, by deep learning standards.",3.1 Incorporating directions and labels,[0],[0]
"So instead of learning the GCN parameters directly, we define them as
W (k) L(u,v) = V (k) dir(u,v), (2)
where dir(u, v) indicates whether the edge (u, v) is directed (1) along, (2) in the opposite direction to the syntactic dependency arc, or (3) is a selfloop; V (k)dir(u,v) 2",3.1 Incorporating directions and labels,[0],[0]
"Rm⇥m. Our simplification captures the intuition that information should be propagated differently along edges depending whether this is a head-to-dependent or dependent-to-head edge (i.e., along or opposite the corresponding syntactic arc) and whether it is a self-loop.",3.1 Incorporating directions and labels,[0],[0]
So we do not share any parameters between these three very different edge types.,3.1 Incorporating directions and labels,[0],[0]
"Syntactic functions are important, but perhaps less crucial, so they are encoded only in the feature vectors bL(u,v).",3.1 Incorporating directions and labels,[0],[0]
"Uniformly accepting information from all neighboring nodes may not be appropriate for the SRL
3Chinese and English CoNLL-2009 datasets used 41 and 48 different syntactic functions, which would result in having 83 and 97 different matrices in every layer, respectively.
setting.",3.2 Edge-wise gating,[0],[0]
"For example, we see in Figure 1 that many semantic arcs just mirror their syntactic counterparts, so they may need to be up-weighted.",3.2 Edge-wise gating,[0],[0]
"Moreover, we rely on automatically predicted syntactic structures, and, even for English, syntactic parsers are far from being perfect, especially when used out-of-domain.",3.2 Edge-wise gating,[0],[0]
"It is risky for a downstream application to rely on a potentially wrong syntactic edge, so the corresponding message in the neural network may need to be down-weighted.
",3.2 Edge-wise gating,[0],[0]
"In order to address the above issues, inspired by recent literature (van den Oord et al., 2016; Dauphin et al., 2016), we calculate for each edge node pair a scalar gate of the form
g(k)u,v = ⇣ h(k)u · v̂(k)dir(u,v) + b̂ (k) L(u,v) ⌘ , (3)
where is the logistic sigmoid function, v̂
(k) dir(u,v) 2 Rm and b̂ (k) L(u,v) 2 R are weights and a bias for the gate.",3.2 Edge-wise gating,[0],[0]
"With this additional gating mechanism, the final syntactic GCN computation is formulated as
h(k+1)v =ReLU(X u2N (v) g(k)v,u(V (k) dir(u,v)h (k) u + b",3.2 Edge-wise gating,[0],[0]
"(k) L(u,v))).",3.2 Edge-wise gating,[0],[0]
(4),3.2 Edge-wise gating,[0],[0]
"The inability of GCNs to capture dependencies between nodes far away from each other in the graph may seem like a serious problem, especially in the context of SRL: paths between predicates and arguments often include many dependency arcs (Roth and Lapata, 2016).",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"However, when graph convolution is performed on top of LSTM states (i.e., LSTM states serve as input xv = h (1) v to GCN) rather than static word embeddings, GCN may not need to capture more than a couple of hops.
",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"To elaborate on this, let us speculate what role GCNs would play when used in combinations with LSTMs, given that LSTMs have already been shown very effective for SRL (Zhou and Xu, 2015; Marcheggiani et al., 2017).",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"Though LSTMs are capable of capturing at least some degree of syntax (Linzen et al., 2016) without explicit syntactic supervision, SRL datasets are moderately sized, so LSTM models may still struggle with harder cases.",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"Typically, harder cases for SRL involve arguments far away from their predicates.",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"In fact, 20% and 30% of arguments are more than 5 tokens away from their predicate, in our English and
Chinese collections, respectively.",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"However, if we imagine that we can ‘teleport’ even over a single (longest) syntactic dependency edge, the ’distance’ would shrink: only 9% and 13% arguments will now be more than 5 LSTM steps away (again for English and Chinese, respectively).",3.3 Complementarity of GCNs and LSTMs,[0],[0]
GCNs provide this ‘teleportation’ capability.,3.3 Complementarity of GCNs and LSTMs,[0],[0]
"These observations suggest that LSTMs and GCNs may be complementary, and we will see that empirical results support this intuition.",3.3 Complementarity of GCNs and LSTMs,[0],[0]
"In this work, we build our semantic role labeler on top of the syntax-agnostic LSTM-based SRL model of Marcheggiani et al. (2017), which already achieves state-of-the-art results on the CoNLL-2009 English dataset.",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"Following their approach we employ the same bidirectional (BiLSTM) encoder and enrich it with a syntactic GCN.
",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"The CoNLL-2009 benchmark assumes that predicate positions are already marked in the test set (e.g., we would know that makes, repairs and engines in Figure 1 are predicates), so no predicate identification is needed.",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"Also, as we focus here solely on identifying arguments and labeling them with semantic roles, for predicate disambiguation
(i.e., marking makes as make.01) we use of an offthe-shelf disambiguation model (Roth and Lapata, 2016; Björkelund et al., 2009).",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"As in Marcheggiani et al. (2017) and in most previous work, we process individual predicates in isolation, so for each predicate, our task reduces to a sequence labeling problem.",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"That is, given a predicate (e.g., disputed in Figure 3) one needs to identify and label all its arguments (e.g., label estimates as A1 and label those as ‘NULL’, indicating that those is not an argument of disputed).
",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"The semantic role labeler we propose is composed of four components (see Figure 3):
• look-ups of word embeddings; • a BiLSTM encoder that takes as input the
word representation of each word in a sentence;
• a syntax-based GCN encoder that re-encodes the BiLSTM representation based on the automatically predicted syntactic structure of the sentence;
• a role classifier that takes as input the GCN representation of the candidate argument and the representation of the predicate to predict the role associated with the candidate word.",4 Syntax-Aware Neural SRL Encoder,[0],[0]
"For each word wi in the considered sentence, we create a sentence-specific word representation xi.",4.1 Word representations,[0],[0]
"We represent each word w as the concatenation of four vectors:4 a randomly initialized word embedding xre 2 Rdw , a pre-trained word embedding xpe 2 Rdw estimated on an external text collection, a randomly initialized part-of-speech tag embedding xpos 2 Rdp and a randomly initialized lemma embedding xle 2",4.1 Word representations,[0],[0]
Rdl (active only if the word is a predicate).,4.1 Word representations,[0],[0]
"The randomly initialized embeddings xre, xpos, and xle are fine-tuned during training, while the pre-trained ones are kept fixed.",4.1 Word representations,[0],[0]
"The final word representation is given by x = xre xpe xpos xle, where represents the concatenation operator.",4.1 Word representations,[0],[0]
"One of the most popular and effective ways to represent sequences, such as sentences (Mikolov et al., 2010), is to use recurrent neural networks
4We drop the index i from the notation for the sake of brevity.
",4.2 Bidirectional LSTM layer,[0],[0]
"(RNN) (Elman, 1990).",4.2 Bidirectional LSTM layer,[0],[0]
"In particular their gated versions, Long Short-Term Memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) and Gated Recurrent Units (GRU) (Cho et al., 2014), have proven effective in modeling long sequences (Chiu and Nichols, 2016; Sutskever et al., 2014).
",4.2 Bidirectional LSTM layer,[0],[0]
"Formally, an LSTM can be defined as a function LSTM✓(x1:i) that takes as input the sequence x1:i and returns a hidden state hi 2 Rdh .",4.2 Bidirectional LSTM layer,[0],[0]
"This state can be regarded as a representation of the sentence from the start to the position i, or, in other words, it encodes the word at position i along with its left context.",4.2 Bidirectional LSTM layer,[0],[0]
"However, the right context is also important, so Bidirectional LSTMs (Graves, 2008) use two LSTMs: one for the forward pass, and another for the backward pass, LSTMF and LSTMB , respectively.",4.2 Bidirectional LSTM layer,[0],[0]
"By concatenating the states of both LSTMs, we create a complete context-aware representation of a word BiLSTM(x1:n, i) =",4.2 Bidirectional LSTM layer,[0],[0]
LSTMF (x1:i),4.2 Bidirectional LSTM layer,[0],[0]
LSTMB(xn:,4.2 Bidirectional LSTM layer,[0],[0]
i).,4.2 Bidirectional LSTM layer,[0],[0]
"We follow Marcheggiani et al. (2017) and stack J layers of bidirectional LSTMs, where each layer takes the lower layer as its input.",4.2 Bidirectional LSTM layer,[0],[0]
The representation calculated with the BiLSTM encoder is fed as input to a GCN of the form defined in Equation (4).,4.3 Graph convolutional layer,[0],[0]
"The neighboring nodes of a node v, namely N (v), and their relations to v are predicted by an external syntactic parser.",4.3 Graph convolutional layer,[0],[0]
"The classifier predicts semantic roles of words given the predicate while relying on word representations provided by GCN; we concatenate hidden states of the candidate argument word and the predicate word and use them as input to a classifier (Figure 3, top).",4.4 Semantic role classifier,[0],[0]
"The softmax classifier computes the probability of the role (including special ‘NULL’ role):
p(r|ti, tp, l) / exp(Wl,r(ti tp)), (5)
where ti and tp are representations produced by the graph convolutional encoder, l is the lemma of predicate p, and the symbol / signifies proportionality.5 As FitzGerald et al. (2015) and Marcheggiani et al. (2017), instead of using a fixed matrix Wl,r or simply assuming that Wl,r = Wr,
5We abuse the notation and refer as p both to the predicate word and to its position in the sentence.
",4.4 Semantic role classifier,[0],[0]
"we jointly embed the role r and predicate lemma l using a non-linear transformation:
Wl,r = ReLU(U(ql qr)), (6)
where U is a parameter matrix, whereas ql 2 Rd0l and qr 2 Rdr are randomly initialized embeddings of predicate lemmas and roles.",4.4 Semantic role classifier,[0],[0]
"In this way each role prediction is predicate-specific, and, at the same time, we expect to learn a good representation for roles associated with infrequent predicates.",4.4 Semantic role classifier,[0],[0]
As our training objective we use the categorical cross-entropy.,4.4 Semantic role classifier,[0],[0]
"We tested the proposed SRL model on the English and Chinese CoNLL-2009 dataset with standard splits into training, test and development sets.",5.1 Datasets and parameters,[0],[0]
The predicted POS tags for both languages were provided by the CoNLL-2009 shared-task organizers.,5.1 Datasets and parameters,[0],[0]
For the predicate disambiguator we used the ones from Roth and Lapata (2016) for English and from Björkelund et al. (2009) for Chinese.,5.1 Datasets and parameters,[0],[0]
"We parsed English sentences with the BIST Parser (Kiperwasser and Goldberg, 2016), whereas for Chinese we used automatically predicted parses provided by the CoNLL-2009 shared-task organizers.
",5.1 Datasets and parameters,[0],[0]
"For English, we used external embeddings of Dyer et al. (2015), learned using the structured skip n-gram approach of Ling et al. (2015).",5.1 Datasets and parameters,[0],[0]
For Chinese we used external embeddings produced with the neural language model of Bengio et al. (2003).,5.1 Datasets and parameters,[0],[0]
"We used edge dropout in GCN: when
computing h(k)v , we ignore each node v 2 N (v) with probability .",5.1 Datasets and parameters,[0],[0]
"Adam (Kingma and Ba, 2015) was used as an optimizer.",5.1 Datasets and parameters,[0],[0]
The hyperparameter tuning and all model selection were performed on the English development set; the chosen values are shown in Appendix.,5.1 Datasets and parameters,[0],[0]
"In order to show that GCN layers are effective, we first compare our model against its version which lacks GCN layers (i.e. essentially the model of Marcheggiani et al. (2017)).",5.2 Results and discussion,[0],[0]
"Importantly, to measure the genuine contribution of GCNs, we first tuned this syntax-agnostic model (e.g., the number of LSTM layers) to get best possible performance on the development set.6
We compare the syntax-agnostic model with 3 syntax-aware versions: one GCN layer over syntax (K = 1), one layer GCN without gates and two GCN layers (K = 2).",5.2 Results and discussion,[0],[0]
"As we rely on the same
6For example, if we would have used only one layer of LSTMs, gains from using GCNs would be even larger.
off-the-shelf disambiguator for all versions of the model, in Table 1 and 2 we report SRL-only scores (i.e., predicate disambiguation is not evaluated) on the English and Chinese development sets.",5.2 Results and discussion,[0],[0]
"For both datasets, the syntax-aware model with one GCN layers (K = 1) performs the best, outperforming the LSTM version by 1.9% and 0.6% for Chinese and English, respectively.",5.2 Results and discussion,[0],[0]
"The reasons why the improvements on Chinese are much larger are not entirely clear (e.g., both languages are relative fixed word order ones, and the syntactic parses for Chinese are considerably less accurate), this may be attributed to a higher proportion of longdistance dependencies between predicates and arguments in Chinese (see Section 3.3).",5.2 Results and discussion,[0],[0]
"Edge-wise gating (Section 3.2) also appears important: removing gates leads to a drop of 0.3% F1 for English and 0.6% F1 for Chinese.
",5.2 Results and discussion,[0],[0]
Stacking two GCN layers does not give any benefit.,5.2 Results and discussion,[0],[0]
"When BiLSTM layers are dropped altogether, stacking two layers (K = 2) of GCNs greatly improves the performance, resulting in a 3.8% jump in F1 for English and a 3.0% jump in F1 for Chi-
nese.",5.2 Results and discussion,[0],[0]
Adding a 3rd layer of GCN (K = 3) further improves the performance.7,5.2 Results and discussion,[0],[0]
"This suggests that extra GCN layers are effective but largely redundant with respect to what LSTMs already capture.
",5.2 Results and discussion,[0],[0]
"In Figure 4, we show the F1 scores results on the English development set as a function of the distance, in terms of tokens, between a candidate argument and its predicate.",5.2 Results and discussion,[0],[0]
"As expected, GCNs appear to be more beneficial for long distance dependencies, as shorter ones are already accurately captured by the LSTM encoder.
",5.2 Results and discussion,[0],[0]
We looked closer in contribution of specific dependency relations for Chinese.,5.2 Results and discussion,[0],[0]
"In order to assess this without retraining the model multiple times, we drop all dependencies of a given type at test time (one type at a time, only for types appearing over 300 times in the development set) and observe changes in performance.",5.2 Results and discussion,[0],[0]
"In Figure 5, we see that the most informative dependency is COMP (complement).",5.2 Results and discussion,[0],[0]
Relative clauses in Chinese are very frequent and typically marked with particle Ñ (de).,5.2 Results and discussion,[0],[0]
"The relative clause will syntactically depend on Ñ as COMP, so COMP encodes important information about predicate-argument structure.",5.2 Results and discussion,[0],[0]
These are often long-distance dependencies and may not be accurately captured by LSTMs.,5.2 Results and discussion,[0],[0]
"Although TMP (temporal) dependencies are not as frequent (⇠2% of all dependencies), they are also important: temporal information is mirrored in semantic roles.
",5.2 Results and discussion,[0],[0]
"In order to compare to previous work, in Table 3 we report test results on the English indomain (WSJ) evaluation data.",5.2 Results and discussion,[0],[0]
"Our model is local, as all the argument detection and labeling decisions are conditionally independent: their interaction is captured solely by the LSTM+GCN encoder.",5.2 Results and discussion,[0],[0]
"This makes our model fast and simple, though, as shown in previous work, global modeling of the structured output is beneficial.8 We leave this extension for future work.",5.2 Results and discussion,[0],[0]
"Interestingly,
7Note that GCN layers are computationally cheaper than LSTM ones, even in our non-optimized implementation.
",5.2 Results and discussion,[0],[0]
"8As seen in Table 3, labelers of FitzGerald et al. (2015) and Roth and Lapata (2016) gained 0.6-1.0%.
",5.2 Results and discussion,[0],[0]
"we outperform even the best global model and the best ensemble of global models, without using global modeling or ensembles.",5.2 Results and discussion,[0],[0]
"When we create an ensemble of 3 models with the product-of-expert combination rule, we improve by 1.2% over the best previous result, achieving 89.1% F1.9
For Chinese (Table 4), our best model outperforms the state-of-the-art model of Roth and Lapata (2016) by even larger margin of 3.1%.
",5.2 Results and discussion,[0],[0]
"For English, in the CoNLL shared task, systems are also evaluated on the out-of-domain dataset.",5.2 Results and discussion,[0],[0]
Statistical models are typically less accurate when they are applied to out-of-domain data.,5.2 Results and discussion,[0],[0]
"Consequently, the predicted syntax for the out-ofdomain test set is of lower quality, which negatively affects the quality of GCN embeddings.",5.2 Results and discussion,[0],[0]
"However, our model works surprisingly well on out-of-domain data (Table 5), substantially outperforming all the previous syntax-aware models.",5.2 Results and discussion,[0],[0]
This suggests that our model is fairly robust to mistakes in syntax.,5.2 Results and discussion,[0],[0]
"As expected though, our model does not outperform the syntax-agnostic model of Marcheggiani et al. (2017).",5.2 Results and discussion,[0],[0]
"Perhaps the earliest methods modeling syntaxsemantics interface with RNNs are due to (Henderson et al., 2008; Titov et al., 2009; Gesmundo et al., 2009), they used shift-reduce parsers for joint SRL and syntactic parsing, and relied on RNNs to model statistical dependencies across syntactic and semantic parsing actions.",6 Related Work,[0],[0]
"A more
9To compare to previous work, we report combined scores which also include predicate disambiguation.",6 Related Work,[0],[0]
"As we use disambiguators from previous work (see Section 5.1), actual gains in argument identification and labeling are even larger.
",6 Related Work,[0],[0]
"modern (e.g., based on LSTMs) and effective reincarnation of this line of research has been proposed in Swayamdipta et al. (2016).",6 Related Work,[0],[0]
Other recent work which considered incorporation of syntactic information in neural SRL models include: FitzGerald et al. (2015) who use standard syntactic features within an MLP calculating potentials of a CRF model; Roth and Lapata (2016) who enriched standard features for SRL with LSTM representations of syntactic paths between arguments and predicates; Lei et al. (2015) who relied on low-rank tensor factorizations for modeling syntax.,6 Related Work,[0],[0]
Also Foland and Martin (2015) used (nongraph) convolutional networks and provided syntactic features as input.,6 Related Work,[0],[0]
"A very different line of research, but with similar goals to ours (i.e. integrating syntax with minimal feature engineering), used tree kernels (Moschitti et al., 2008).
",6 Related Work,[0],[0]
"Beyond SRL, there have been many proposals on how to incorporate syntactic information in RNN models, for example, in the context of neural machine translation (Eriguchi et al., 2017; Sennrich and Haddow, 2016).",6 Related Work,[0],[0]
"One of the most popular and attractive approaches is to use treestructured recursive neural networks (Socher et al., 2013; Le and Zuidema, 2014; Dyer et al., 2015), including stacking them on top of a sequential BiLSTM (Miwa and Bansal, 2016).",6 Related Work,[0],[0]
"An approach of Mou et al. (2015) to sentiment analysis and question classification, introduced even before GCNs became popular in the machine learning community, is related to graph convolution.",6 Related Work,[0],[0]
"However, it is inherently single-layer and tree-specific, uses bottom-up computations, does not share parameters across syntactic functions and does not use gates.",6 Related Work,[0],[0]
"Gates have been previously used in GCNs (Li et al., 2016) but between GCN layers rather than for individual edges.
",6 Related Work,[0],[0]
Previous approaches to integrating syntactic information in neural models are mainly designed to induce representations of sentences or syntactic constituents.,6 Related Work,[0],[0]
"In contrast, the approach we presented incorporates syntactic information at word level.",6 Related Work,[0],[0]
"This may be attractive from the engineering perspective, as it can be used, as we have shown, instead or along with RNN models.",6 Related Work,[0],[0]
"We demonstrated how GCNs can be used to incorporate syntactic information in neural models and specifically to construct a syntax-aware SRL
model, resulting in state-of-the-art results for Chinese and English.",7 Conclusions and Future Work,[0],[0]
There are relatively straightforward steps which can further improve the SRL results.,7 Conclusions and Future Work,[0],[0]
"For example, we relied on labeling arguments independently, whereas using a joint model is likely to significantly improve the performance.",7 Conclusions and Future Work,[0],[0]
"Also, in this paper we consider the dependency version of the SRL task, however the model can be generalized to the span-based version of the task (i.e. labeling argument spans with roles rather that syntactic heads of arguments) in a relatively straightforward fashion.
",7 Conclusions and Future Work,[0],[0]
"More generally, given simplicity of GCNs and their applicability to general graph structures (not necessarily trees), we believe that there are many NLP tasks where GCNs can be used to incorporate linguistic structures (e.g., syntactic and semantic representations of sentences and discourse parses or co-reference graphs for documents).",7 Conclusions and Future Work,[0],[0]
"We would thank Anton Frolov, Michael Schlichtkrull, Thomas Kipf, Michael Roth, Max Welling, Yi Zhang, and Wilker Aziz for their suggestions and comments.",Acknowledgements,[0],[0]
"The project was supported by the European Research Council (ERC StG BroadSem 678254), the Dutch National Science Foundation (NWO VIDI 639.022.518) and an Amazon Web Services (AWS) grant.",Acknowledgements,[0],[0]
Semantic role labeling (SRL) is the task of identifying the predicate-argument structure of a sentence.,abstractText,[0],[0]
It is typically regarded as an important step in the standard NLP pipeline.,abstractText,[0],[0]
"As the semantic representations are closely related to syntactic ones, we exploit syntactic information in our model.",abstractText,[0],[0]
"We propose a version of graph convolutional networks (GCNs), a recent class of neural networks operating on graphs, suited to model syntactic dependency graphs.",abstractText,[0],[0]
"GCNs over syntactic dependency trees are used as sentence encoders, producing latent feature representations of words in a sentence.",abstractText,[0],[0]
"We observe that GCN layers are complementary to LSTM ones: when we stack both GCN and LSTM layers, we obtain a substantial improvement over an already state-of-theart LSTM SRL model, resulting in the best reported scores on the standard benchmark (CoNLL-2009) both for Chinese and English.",abstractText,[0],[0]
Encoding Sentences with Graph Convolutional Networks for Semantic Role Labeling,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2350–2354, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base. In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts. Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate. We empirically evaluate our approach in two tasks of link prediction and triple classification. Experimental results show that our method outperforms other baselines on the two tasks consistently.",text,[0],[0]
"Knowledge bases (KBs) such as Freebase (Bollacker et al., 2008) and YAGO (Fabian et al., 2007) play a pivotal role in many NLP related applications.",1 Introduction,[0],[0]
"KBs consist of facts in the form of triplets (ei, r, ej), indicating that head entity ei and tail entity ej is linked by relation r. Although KBs are large, they are far from complete.",1 Introduction,[0],[0]
"Link prediction is to predict relations between entities based on existing triplets, which can alleviate the incompleteness of current KBs.",1 Introduction,[0],[0]
"Recently a promising approach for this task called knowledge base embedding (Nickel et al., 2011; Bordes et al., 2011; Socher et al., 2013) aims to embed entities and relations into a continuous vector space while preserving certain information of the KB graph.",1 Introduction,[0],[0]
"TransE (Bordes et al., 2013) is a typical model considering relation vector as trans-
lating operations between head and tail vector, i.e., ei + r",1 Introduction,[0],[0]
"≈ ej when (ei, r, ej) holds.
",1 Introduction,[0],[0]
Most existing KB embedding methods solely learn from time-unknown facts but ignore the useful temporal information in the KB.,1 Introduction,[0],[0]
"In fact, there are many temporal facts (or events) in the KB, e.g., (Obama, wasBornIn, Hawaii) happened at August 4, 1961.",1 Introduction,[0],[0]
"(Obama, presidentOf, USA) is true since 2009.",1 Introduction,[0],[0]
Current KBs such as YAGO and Freebase store such temporal information either directly or indirectly.,1 Introduction,[0],[0]
The happening time of time-sensitive facts may indicate special temporal order of facts and time-sensitive relations.,1 Introduction,[0],[0]
"For example, (Einstein, wasBornIn, Ulm) happened in 1879, (Einstein, wonPrize, Nobel Prize) happened in 1922, (Einstein, diedIn, Princeton) occurred in 1955.",1 Introduction,[0],[0]
We can infer the temporal order of time-sensitive relations from all such kinds of facts: wasBornIn → wonPrize → diedIn.,1 Introduction,[0],[0]
"Traditional KB embedding models such as TransE often confuse relations such as wasBornIn and diedIn when predicting (person,?,location) because TransE learns only from time-unknown facts and cannot distinguish relations with similar semantic meaning.",1 Introduction,[0],[0]
"To make more accurate predictions, it is non-trivial for existing KB embedding methods to incorporate temporal order information.
",1 Introduction,[0],[0]
This paper mainly focuses on incorporating the temporal order information and proposes a timeaware link prediction model.,1 Introduction,[0],[0]
"A new temporal dimension is added to fact triples, denoted as a quadruple: (ei, r, ej , tr), indicating the fact happened at time tr1.",1 Introduction,[0],[0]
"To make the embedding space compati-
1tr is the absolute beginning time when the fact is true, e.g., ”1961-08-04” for (Obama, wasBornIn, Hawaii).
2350
ble with the observed triple in the fact dimension, relation vectors behave as translations between entity vectors similarly as TransE models.",1 Introduction,[0],[0]
"To incorporate temporal order information between pair-wise temporal facts, we assume that prior time-sensitive relation vector can evolve into a subsequent timesensitive relation vector through a temporal transition.",1 Introduction,[0],[0]
"For example, we have two temporal facts sharing the same head entity: (ei, ri, ej , t1) and (ei, rj , ek, t2) and the temporal order constraint t1< t2, i.e., ri happens before rj , then we propose the assumption that prior relation ri after temporal transition should lie close to subsequent relation rj , i.e., riM",1 Introduction,[0],[0]
"≈ rj , here matrix M captures the temporal order information between relations.",1 Introduction,[0],[0]
"In this way, both semantic and temporal information are embedded into a continuous vector space during learning.",1 Introduction,[0],[0]
"To the best of our knowledge, we are the first to consider such temporal information for KB embedding.
",1 Introduction,[0],[0]
We evaluate our approach on public available datasets and our method outperforms state-of-the-art methods in the time-aware link prediction and triple classification tasks.,1 Introduction,[0],[0]
Traditional KB embedding methods encode only observed fact triples but neglect temporal constraints between time-sensitive entities and facts.,2 Time-Aware KB Embedding,[0],[0]
"To deal with this limitation, we introduce Time-Aware KB Embedding which constrains the task by incorporating temporal constraints.
",2 Time-Aware KB Embedding,[0],[0]
"To consider the happening time of facts, we formulate a temporal order constraint as an optimization problem based on a manifold regularization term.",2 Time-Aware KB Embedding,[0],[0]
"Specially, temporal order of relations in timesensitive facts should affect KB representation.",2 Time-Aware KB Embedding,[0],[0]
"If ri and rj share the same head entity ei, and ri occurs before rj , then prior relation’s vector ri could evolve into subsequent relation’s vector rj in the temporal dimension.
",2 Time-Aware KB Embedding,[0],[0]
"To encode the transition between time-sensitive relations, we define a transition matrix M ∈ Rn×n between pair-wise temporal ordering relation pair (ri, rj).",2 Time-Aware KB Embedding,[0],[0]
"Our optimization requires that positive temporal ordering relation pairs should have lower scores (energies) than negative pairs, so we define a
temporal order score function as
g(ri, rj) = ‖riM− rj‖1, (1)
which is expected to be a low score when the relation pair is in chronological order, and high otherwise.
",2 Time-Aware KB Embedding,[0],[0]
"To make the embedding space compatible with the observed triples, we make use of the triple set ∆ and follow the same strategy adopted in previous methods such as TransE.
f(ei, r, ej) = ‖ei + r− ej‖1.",2 Time-Aware KB Embedding,[0],[0]
"(2)
For each candidate triple, it requires positive triples to have lower scores than negative triples.
",2 Time-Aware KB Embedding,[0],[0]
"The optimization is to minimize the joint score function,
L= ∑
x+∈∆
[ ∑
x−∈∆′",2 Time-Aware KB Embedding,[0],[0]
"[γ1 + f(x
+)",2 Time-Aware KB Embedding,[0],[0]
"− f(x−)]+
+λ ∑
y+∈Ωei ,y −∈Ω′ei
[γ2 + g(y +)− g(y−)]+
] (3)
where x+ = (ei, ri, ej) ∈ ∆ is the positive triple (quad), x−=(e′i, ri, e ′ j)∈∆′ is corresponding the negative triple.",2 Time-Aware KB Embedding,[0],[0]
y+,2 Time-Aware KB Embedding,[0],[0]
"= (ri, rj)∈Ωei is the positive relation ordering pair with respect to (ei, ri, ej , tx).",2 Time-Aware KB Embedding,[0],[0]
"It’s defined as
Ωei = {(ri, rj)|(ei, ri, ej , tx)∈∆τ , (ei, rj , ek, ty)∈∆τ , tx< ty}, (4)
where ri and rj share the same head entity ei, and y− =",2 Time-Aware KB Embedding,[0],[0]
"(rj , ri) ∈ Ω′ei are the corresponding negative relation order pairs by inverse.",2 Time-Aware KB Embedding,[0],[0]
"In experiments, we enforce constrains as ‖ei‖2 ≤ 1, ‖ri‖2 ≤ 1, ‖rj‖ ≤ 1 and ‖riM‖2 ≤ 1.
",2 Time-Aware KB Embedding,[0],[0]
"The first term in Equation (3) enforces the resultant embedding space compatible with all the observed triples, and the second term further requires the space to be temporally consistent and more accurate.",2 Time-Aware KB Embedding,[0],[0]
Hyperparameter λ makes a trade-off between the two cases.,2 Time-Aware KB Embedding,[0],[0]
Stochastic gradient descent (in minibatch mode) is adopted to solve the minimization problem.,2 Time-Aware KB Embedding,[0],[0]
"We adopt the same evaluation metrics for timeaware KB embedding in two tasks: link prediction (Bordes et al., 2013) and triple classification (Socher et al., 2013).",3 Experiments,[0],[0]
"We create two temporal datasets from YAGO2 (Hoffart et al., 2013), consisting of time-sensitive facts.",3.1 Datasets,[0],[0]
"In YAGO2, MetaFacts contains all happening time for facts.",3.1 Datasets,[0],[0]
DateFacts contains all creation time for entities.,3.1 Datasets,[0],[0]
"First, to make a pure time-sensitive dataset where all facts have time annotations, we selected the subset of entities that have at least 2 mentions in MetaFacts and DateFacts.",3.1 Datasets,[0],[0]
"This resulted in 15,914 triples (quadruples) which were randomly split with the ratio shown in Table 1.",3.1 Datasets,[0],[0]
This dataset is denoted YG15k.,3.1 Datasets,[0],[0]
"Second, to make a mixed dataset, we created YG36k where 50% facts have time annotations and 50% do not.",3.1 Datasets,[0],[0]
We will release the data upon request.,3.1 Datasets,[0],[0]
"Link prediction is to complete the triple (h, r, t) when h, r or t is missing.",3.2 Link Prediction,[0],[0]
Evaluation protocol.,3.2.1 Entity Prediction,[0],[0]
"For each test triple with missing head or tail entity, various methods are used to compute the scores for all candidate entities and rank them in descending order.",3.2.1 Entity Prediction,[0],[0]
"We use two metrics for our evaluation as in (Bordes et al., 2013): the mean of correct entity ranks (Mean Rank) and the proportion of valid entities ranked in top-10 (Hits@10).",3.2.1 Entity Prediction,[0],[0]
"As mentioned in (Bordes et al., 2013), the metrics are desirable but flawed when a corrupted triple exists in the KB.",3.2.1 Entity Prediction,[0],[0]
"As a countermeasure, we filter out all these valid triples in the KB before ranking.",3.2.1 Entity Prediction,[0],[0]
We name the first evaluation set as Raw and the second as Filter.,3.2.1 Entity Prediction,[0],[0]
Baseline methods.,3.2.1 Entity Prediction,[0],[0]
"For comparison, we select translating methods such as TransE (Bordes et al., 2013), TransH (Wang et al., 2014b) and TransR",3.2.1 Entity Prediction,[0],[0]
"(Lin et al., 2015b) as our baselines.",3.2.1 Entity Prediction,[0],[0]
We then use time-aware embedding based on these methods to obtain the corresponding time-aware embedding models.,3.2.1 Entity Prediction,[0],[0]
A model with time-aware embedding is denoted as ”tTransE” for example.,3.2.1 Entity Prediction,[0],[0]
Implementation details.,3.2.1 Entity Prediction,[0],[0]
"For all methods, we create 100 mini-batches on each data set.",3.2.1 Entity Prediction,[0],[0]
"The di-
mension of the embedding n is set in the range of {20,50,100}, the margin γ1 and γ2 are set in the range {1,2,4,10}.",3.2.1 Entity Prediction,[0],[0]
"The learning rate is set in the range {0.1, 0.01, 0.001}.",3.2.1 Entity Prediction,[0],[0]
"The regularization hyperparameter λ is tuned in {10−1,10−2,10−3,10−4}.",3.2.1 Entity Prediction,[0],[0]
The best configuration is determined according to the mean rank in validation set.,3.2.1 Entity Prediction,[0],[0]
"The optimal configurations are n=100,γ1=γ2=4,λ=10−2, learning rate is 0.001 and taking `1−norm.",3.2.1 Entity Prediction,[0],[0]
Results.,3.2.1 Entity Prediction,[0],[0]
Table 2 reports the results on the test set.,3.2.1 Entity Prediction,[0],[0]
"From the results, we can see that time-aware embedding methods outperform all the baselines on all the data sets and with all the metrics.",3.2.1 Entity Prediction,[0],[0]
The improvements are usually quite significant.,3.2.1 Entity Prediction,[0],[0]
"The Mean Rank drops by about 75%, and Hits@10 rises about 19% to 30%.",3.2.1 Entity Prediction,[0],[0]
This demonstrates the superiority and generality of our method.,3.2.1 Entity Prediction,[0],[0]
"When dealing with sparse data YG15k, all the temporal information is utilized to model temporal associations and make the embeddings more accurate, so it obtains better improvement than mixing the time-unknown triples in YG36k.",3.2.1 Entity Prediction,[0],[0]
Relation prediction aims to predict relations given two entities.,3.2.2 Relation Prediction,[0],[0]
"Evaluation results are shown in Table 3 on only YG15K due to limited space, where we report Hits@1 instead of Hits@10.",3.2.2 Relation Prediction,[0],[0]
Example prediction results for TransE and tTransE are compared in Table 4.,3.2.2 Relation Prediction,[0],[0]
"For example, when testing (Billy Hughes,?,London,1862), it’s easy for TransE to mix relations wasBornIn and diedIn because they act similarly for a person and a place.",3.2.2 Relation Prediction,[0],[0]
"But known that (Billy Hughes, isAffiliatedTo, National Labor Party) happened in 1916, and tTransE have learnt temporal order that wasBornIn→isAffiliatedTo→diedIn, so the regularization term |rbornT − raffiliated| is smaller than |rdiedT− raffiliated|, so correct answer wasBornIn ranks higher than diedIn.",3.2.2 Relation Prediction,[0],[0]
Triple classification aims to judge whether an unseen triple is correct or not.,3.3 Triple Classification,[0],[0]
Evaluation protocol.,3.3 Triple Classification,[0],[0]
We follow the same evaluation protocol used in Socher et al. (2013).,3.3 Triple Classification,[0],[0]
"To create labeled data for classification, for each triple in the test and validation sets, we construct a corresponding negative triple by randomly corrupting the entities.",3.3 Triple Classification,[0],[0]
"To corrupt a position (head or tail), only entities that have appeared in that position are allowed.",3.3 Triple Classification,[0],[0]
"During triple classification, a triple is predicted as positive if the score is below a relation-specific threshold δr; otherwise as negative.",3.3 Triple Classification,[0],[0]
We report averaged accuracy on the test sets.,3.3 Triple Classification,[0],[0]
Implementation details.,3.3 Triple Classification,[0],[0]
We use the same hyperparameter settings as in the link prediction task.,3.3 Triple Classification,[0],[0]
The relation-specific threshold δr is determined by maximizing averaged accuracy on the validation sets.,3.3 Triple Classification,[0],[0]
Results.,3.3 Triple Classification,[0],[0]
Table 5 reports the results on the test sets.,3.3 Triple Classification,[0],[0]
The results indicate that time-aware embedding outperforms all the baselines consistently.,3.3 Triple Classification,[0],[0]
Temporal order information may help to distinguish positive and negative triples as different head entities may have different temporally associated relations.,3.3 Triple Classification,[0],[0]
"If the temporal order is the same with most facts, the regularization term helps it get lower energies and vice versa.",3.3 Triple Classification,[0],[0]
"Many models have been proposed for KB embedding (Nickel et al., 2011; Bordes et al., 2013; Socher et al., 2013).",4 Related Work,[0],[0]
"External information is employed to improve KB embedding such as text (Riedel et al.,
2013; Wang et al., 2014a; Zhao et al., 2015), entity type and relationship domain (Guo et al., 2015; Chang et al., 2014), and relation path (Lin et al., 2015a; Gu et al., 2015).",4 Related Work,[0],[0]
"However, these methods solely rely on triple facts but neglect temporal order constraints between facts.",4 Related Work,[0],[0]
"Temporal information such as relation ordering in text has been explored (Talukdar et al., 2012; Chambers et al., 2014; Bethard, 2013; Cassidy et al., 2014; Chambers et al., 2007; Chambers and Jurafsky, 2008).",4 Related Work,[0],[0]
This paper proposes a time-aware embedding approach that employs temporal order constraints to improve KB embedding.,4 Related Work,[0],[0]
"In this paper, we propose a general time-aware KB embedding, which incorporates creation time of entities and imposes temporal order constraints on the geometric structure of the embedding space and enforce it to be temporally consistent and accurate.",5 Conclusion and Future Work,[0],[0]
As future work: (1) We will incorporate the valid time of facts.,5 Conclusion and Future Work,[0],[0]
"(2) Some time-sensitive facts lack temporal information in YAGO2, we will mine such temporal information from texts.",5 Conclusion and Future Work,[0],[0]
"This research is supported by National Key Basic Research Program of China (No.2014CB340504) and National Natural Science Foundation of China (No.61375074,61273318).",Acknowledgments,[0],[0]
The contact author for this paper is Baobao Chang and Zhifang Sui.,Acknowledgments,[0],[0]
Most existing knowledge base (KB) embedding methods solely learn from time-unknown fact triples but neglect the temporal information in the knowledge base.,abstractText,[0],[0]
"In this paper, we propose a novel time-aware KB embedding approach taking advantage of the happening time of facts.",abstractText,[0],[0]
"Specifically, we use temporal order constraints to model transformation between time-sensitive relations and enforce the embeddings to be temporally consistent and more accurate.",abstractText,[0],[0]
We empirically evaluate our approach in two tasks of link prediction and triple classification.,abstractText,[0],[0]
Experimental results show that our method outperforms other baselines on the two tasks consistently.,abstractText,[0],[0]
Encoding Temporal Information for Time-Aware Link Prediction,title,[0],[0]
"Object tracking has gained much attention in recent decades (Bertinetto et al., 2016a; Danelljan et al., 2017; Zhu et al., 2016; Cui et al., 2016).",1. Introduction,[0],[0]
The aim of object tracking is to localize an object in continuous video frames given an initial annotation in the first frame.,1. Introduction,[0],[0]
"Much of the existing work is, however, on the passive tracker, where it is presumed that the object of interest is always in the image scene so
*Equal contribution 1Tencent AI Lab 2Peking University.",1. Introduction,[0],[0]
"Correspondence to: Wenhan Luo <whluo.china@gmail.com>, Peng Sun",1. Introduction,[0],[0]
"<pengsun000@gmail.com>, Fangwei Zhong <zfw@pku.edu.cn>, Wei Liu <wl2223@columbia.edu>, Tong Zhang <tongzhang@tongzhang-ml.org>, Yizhou Wang <yizhou.Wang@pku.edu.cn>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
that there is no need to handle the camera control during tracking.",1. Introduction,[0],[0]
"This fashion is inapplicable to some use-cases, e.g., the tracking performed by a mobile robot with a camera mounted or by a drone.",1. Introduction,[0],[0]
"To this end, one should seek a solution of active tracking, which composes two sub-tasks, i.e., the object tracking and the camera control (Fig. 1, Right).
",1. Introduction,[0],[0]
"Unfortunately, it is hard to jointly tune the pipeline with the two separate sub-tasks.",1. Introduction,[0],[0]
The tracking task may also involve many human efforts for bounding box labeling.,1. Introduction,[0],[0]
"Moreover, the implementation of camera control is non-trivial and can incur many expensive trial-and-errors happening in realworld.",1. Introduction,[0],[0]
"To address these issues, we propose an end-to-end active tracking solution via deep reinforcement learning.",1. Introduction,[0],[0]
"To be specific, we adopt a ConvNet-LSTM network, taking as input raw video frames and outputting camera movement actions (e.g., move forward, turn left, etc.).
",1. Introduction,[0],[0]
"We leverage virtual environments to conveniently simulate active tracking, saving the expensive human labeling or realworld trial-and-error.",1. Introduction,[0],[0]
"In a virtual environment, an agent (i.e., the tracker) observes a state (a visual frame) from a first-person perspective and takes an action, and then the environment returns the updated state (next visual frame).",1. Introduction,[0],[0]
"We adopt the modern Reinforcement Learning (RL) algorithm A3C (Mnih et al., 2016) to train the agent, where a customized reward function is designed to encourage the
agent to be closely following the object.
",1. Introduction,[0],[0]
We also adopt an environment augmentation technique to boost the tracker’s generalization ability.,1. Introduction,[0],[0]
"For this purpose, much engineering is devoted to preparing various environments in different object appearances, different backgrounds, and different object trajectories.",1. Introduction,[0],[0]
We manage this by either using a simulator’s plug-in or developing specific APIs to communicate with a simulator engine.,1. Introduction,[0],[0]
"See Sec. 3.1.
",1. Introduction,[0],[0]
"To our slight surprise, the trained tracker shows good generalization capability.",1. Introduction,[0],[0]
"In testing, it performs robust active tracking in the case of unseen object movement path, unseen object appearance, unseen background, and distracting object.",1. Introduction,[0],[0]
"Additionally, the tracker can restore tracking when it occasionally loses the target due to, e.g., abrupt object movement.
",1. Introduction,[0],[0]
"In our experiments, the proposed tracking approach also outperforms a few representative conventional passive trackers which are equipped with a hand-tuned camera-control module.",1. Introduction,[0],[0]
"While we are not pursuing a state-of-the-art passive tracker in this work, the experimental results do show that a passive tracker is not indispensable in active tracking.",1. Introduction,[0],[0]
"Alternatively, a direct end-to-end solution can be effective.",1. Introduction,[0],[0]
"As far as we know, there has not yet been any attempt to deal with active tracking in an end-to-end manner.
",1. Introduction,[0],[0]
"Finally, we perform qualitative evaluation on some video clips taken from the VOT dataset (Kristan et al., 2016).",1. Introduction,[0],[0]
"The results show that the tracking ability, obtained purely from simulators, can potentially transfer to real-world scenarios.",1. Introduction,[0],[0]
Object Tracking.,2. Related Work,[0],[0]
"Roughly, object tracking (Wu et al., 2013) is conducted in both passive and active ways.",2. Related Work,[0],[0]
"As mentioned in Sec. 1, passive object tracking has gained more attention due to its relatively simpler problem settings.",2. Related Work,[0],[0]
"In recent decades, passive object tracking has achieved a great progress (Wu et al., 2013).",2. Related Work,[0],[0]
"Many approaches (Hu et al., 2012) have been proposed to overcome difficulties resulted from the issues such as occlusion and illumination variations.",2. Related Work,[0],[0]
"In (Ross et al., 2008) subspace learning was adopted to update the appearance model of an object and integrated into a particle filter framework for object tracking.",2. Related Work,[0],[0]
"Babenko et al. (Babenko et al., 2009) employed multiple instance learning to track an object.",2. Related Work,[0],[0]
"Correlation filter based object tracking (Valmadre et al., 2017; Choi et al., 2017b) has also achieved a success in real-time object tracking (Bolme et al., 2010; Henriques et al., 2015).",2. Related Work,[0],[0]
"In (Hare et al., 2016), structured output prediction was used to constrain object tracking, avoiding converting positions to labels of training samples.",2. Related Work,[0],[0]
"In (Kalal et al., 2012), Tracking, Learning and Detection (TLD) were integrated into one framework for long-term tracking, where a detection module
can re-initialize the tracker once a missing object reappears.",2. Related Work,[0],[0]
"Recent years have witnessed the success of deep learning in object tracking (Wang et al., 2016; Bertinetto et al., 2016b).",2. Related Work,[0],[0]
"For instance, a stacked autoencoder was trained to learn good representations for object tracking in (Wang & Yeung, 2013).",2. Related Work,[0],[0]
"Both low-level and high-level representations were adopted to gain both accuracy and robustness (Ma et al., 2015).
",2. Related Work,[0],[0]
Active object tracking additionally considers camera control compared with traditional object tracking.,2. Related Work,[0],[0]
There exists not much research attention in the area of active tracking.,2. Related Work,[0],[0]
"Conventional solutions dealt with object tracking and camera control in separate components (Denzler & Paulus, 1994; Murray & Basu, 1994; Kim et al., 2005), but these solutions are difficult to tune.",2. Related Work,[0],[0]
"Our proposal is completely different from them as it tackles object tracking and camera control simultaneously in an end-to-end manner.
",2. Related Work,[0],[0]
Reinforcement Learning.,2. Related Work,[0],[0]
"Reinforcement Learning (RL) (Sutton & Barto, 1998) intends for a principled approach to temporal decision making problems.",2. Related Work,[0],[0]
"In a typical RL framework, an agent learns from the environment a policy function that maps state to action at each discrete time step, where the objective is to maximize the accumulated rewards returned by the environment.",2. Related Work,[0],[0]
"Historically, RL has been successfully applied to inventory management, path planning, game playing, etc.
",2. Related Work,[0],[0]
"On the other hand, the past half decade has witnessed a breakthrough in deep learning applied to computer vision tasks, including image classification (Krizhevsky et al., 2012), segmentation (Long et al., 2015), object detection and localization (Girshick et al., 2014), and so on.",2. Related Work,[0],[0]
"In particular, researchers believe that deep Convolutional Neural Networks (ConvNets) can learn good features from raw image pixels, which is able to benefit higher-level tasks.
",2. Related Work,[0],[0]
"Equipped with deep ConvNets, RL also shows impressive successes on those tasks involving image (-like) raw states, e.g., playing board game GO (Silver et al., 2016) and video game (Mnih et al., 2015; Wu & Tian, 2017).",2. Related Work,[0],[0]
"Recently, in the computer vision community there are also preliminary attempts of applying deep RL to traditional tasks, e.g., object localization (Caicedo & Lazebnik, 2015) and region proposal (Jie et al., 2016).",2. Related Work,[0],[0]
"There are also methods of visual tracking relying on RL (Choi et al., 2017a; Huang et al., 2017; Supancic & Ramanan, 2017; Yun et al., 2017).",2. Related Work,[0],[0]
"However, they are distinct from our work, as they formulate passive tracking with RL but have nothing to do with camera control.",2. Related Work,[0],[0]
While our focus in this work is active tracking.,2. Related Work,[0],[0]
"In our approach, virtual tracking scenes are generated for both training and testing.",3. Our Approach,[0],[0]
"To train the tracker, we employ
a state-of-the-art reinforcement learning algorithm, A3C (Mnih et al., 2016).",3. Our Approach,[0],[0]
"For the sake of robust and effective training, we also propose data augmentation techniques and a customized reward function, which are elaborated later.
",3. Our Approach,[0],[0]
"Although various types of states are available, for a research purpose we let the state be only an RGB screen frame of the first-person perspective in this study.",3. Our Approach,[0],[0]
"To be more specific, the tracker observes the raw visual state and takes one action from the action set A = {turn-left, turn-right, turn-left-andmove-forward, turn-right-and-move-forward, move-forward, no-op}.",3. Our Approach,[0],[0]
"The action is processed by the environment, which returns to the agent the updated screen frame as well as the current reward.",3. Our Approach,[0],[0]
It is impossible to train the desired end-to-end active tracker in real-world scenarios.,3.1. Tracking Scenarios,[0],[0]
"Thus, we adopt two types of virtual environments for simulated training.
ViZDoom.",3.1. Tracking Scenarios,[0],[0]
"ViZDoom (Kempka et al., 2016; ViZ) is an RL research platform based on a 3D FPS video game called Doom.",3.1. Tracking Scenarios,[0],[0]
"In ViZDoom, the game engine corresponds to the environment, while the video game player corresponds to the agent.",3.1. Tracking Scenarios,[0],[0]
The agent receives from the environment a state and a reward at each time step.,3.1. Tracking Scenarios,[0],[0]
"In this study, we make customized ViZDoom maps (see Fig. 4) composed of an object (a monster) and background (ceiling, floor, and wall).",3.1. Tracking Scenarios,[0],[0]
"The monster walks along a pre-specified path programmed by the ACS script (Kempka et al., 2016), and our goal is to train the agent, i.e., the tracker, to follow closely the object.
",3.1. Tracking Scenarios,[0],[0]
Unreal Engine.,3.1. Tracking Scenarios,[0],[0]
"Though convenient for research, ViZDoom does not provide realistic scenarios.",3.1. Tracking Scenarios,[0],[0]
"To this end, we adopt Unreal Engine (UE) (unr) to construct nearly real-world environments.",3.1. Tracking Scenarios,[0],[0]
UE is a popular game engine and has a broad influence in the game industry.,3.1. Tracking Scenarios,[0],[0]
It provides realistic scenarios which can mimic real-world scenes (please see exemplar images in Fig. 5 and videos in our supplementary materials).,3.1. Tracking Scenarios,[0],[0]
We employ UnrealCV,3.1. Tracking Scenarios,[0],[0]
"(Qiu et al., 2017), which provides convenient APIs, along with a wrapper (Zhong et al., 2017) compatible with OpenAI Gym (Brockman et al., 2016), for interactions between RL algorithms and the environments constructed based on UE.",3.1. Tracking Scenarios,[0],[0]
"Following (Mnih et al., 2016), we adopt a popular RL algorithm called Actor-Critic.",3.2. A3C Algorithm,[0],[0]
"At time step t, we denote by st the observed state, which corresponds to a raw RGB frame.",3.2. A3C Algorithm,[0],[0]
The action set is denoted by A of size K = |A|.,3.2. A3C Algorithm,[0],[0]
"An action, at ∈ A, is drawn from a policy function distribution: at v π(·|st) ∈ RK , referred to as an Actor.",3.2. A3C Algorithm,[0],[0]
"The environment then returns a reward rt ∈ R according to a reward function rt = g(st), which will be characterized in Sec. 3.4.",3.2. A3C Algorithm,[0],[0]
"The updated state st+1 at next time step t+ 1 is subject to a certain but unknown state transition function st+1 = f(st, at), governed by the environment.",3.2. A3C Algorithm,[0],[0]
"In this way, we can observe a trace consisting of a sequence of tuplets τ = {. . .",3.2. A3C Algorithm,[0],[0]
", (st, at, rt) , (st+1, at+1, rt+1) , . . .}.",3.2. A3C Algorithm,[0],[0]
"Meanwhile, we denote by V (st) ∈ R the expected accumulated reward in the future given state st (referred to as Critic).
",3.2. A3C Algorithm,[0],[0]
"The policy function π (·) and the value function V (·) are then jointly modeled by a neural network, as will be discussed in Sec. 3.3.",3.2. A3C Algorithm,[0],[0]
"Rewriting them as π(·|st; θ) and V (st; θ
′) with parameters θ and θ′, respectively, we can learn θ and θ′ over the trace τ with simultaneous stochastic policy gradient and value function regression:
θ ← θ + α",3.2. A3C Algorithm,[0],[0]
( Rt − V (st) ),3.2. A3C Algorithm,[0],[0]
∇θ log π (at|st) + β∇θH,3.2. A3C Algorithm,[0],[0]
"( π (·|st) ) ,
(1)
θ′",3.2. A3C Algorithm,[0],[0]
← θ′,3.2. A3C Algorithm,[0],[0]
"− α∇θ′ 1
2
( Rt − V (st) )2 , (2)
whereRt = ∑t+T−1 t′=t",3.2. A3C Algorithm,[0],[0]
γ t′−trt′ is a discounted sum of future rewards up to T time steps with factor 0,3.2. A3C Algorithm,[0],[0]
<,3.2. A3C Algorithm,[0],[0]
"γ ≤ 1, α is the learning rate, H (·) is an entropy regularizer, and β is the regularizer factor.
",3.2. A3C Algorithm,[0],[0]
"During training, several threads are launched, each maintaining an independent environment-agent interaction.",3.2. A3C Algorithm,[0],[0]
"However, the network parameters are shared across the threads and updated every T time steps asynchronously in a lock-free manner using Eq.",3.2. A3C Algorithm,[0],[0]
(1) in each thread.,3.2. A3C Algorithm,[0],[0]
"This kind of manythread training is reported to be fast yet stable, leading to improved generalization (Mnih et al., 2016).",3.2. A3C Algorithm,[0],[0]
"Later in Sec. 3.5, we will introduce environment augmentation techniques to further improve the generalization ability.",3.2. A3C Algorithm,[0],[0]
"The tracker is a ConvNet-LSTM neural network as shown in Fig. 2, where the architecture specification is given in the following table.",3.3. Network Architecture,[0],[0]
"The FC6 and FC1 correspond to the 6-action policy π (·|st) and the value V (st), respectively.",3.3. Network Architecture,[0],[0]
"The screen is resized to 84 × 84 × 3 RGB image as the network input.
",3.3. Network Architecture,[0],[0]
"Layer# 1 2 3 4 5
Parameters C8×8-16S4 C4×4-32S2 FC256 LSTM256 FC6FC1",3.3. Network Architecture,[0],[0]
"To perform active tracking, it is a natural intuition that the reward function should encourage the agent to closely follow the object.",3.4. Reward Function,[0],[0]
"In this line of thought, firstly we define a twodimensional local coordinate system, denoted by S (see Fig. 3).",3.4. Reward Function,[0],[0]
"The x-axis points from the agent’s left shoulder to right shoulder, and the y-axis is perpendicular to the x-axis and points to the agent’s front.",3.4. Reward Function,[0],[0]
The origin is where the agent is.,3.4. Reward Function,[0],[0]
System S is parallel to the floor.,3.4. Reward Function,[0],[0]
"Secondly, we manage to obtain object’s local coordinate (x, y) and orientation a (in radius) with regard to system S.
With a slight abuse of notation, we can now write the reward function as
r = A−
(√ x2 +",3.4. Reward Function,[0],[0]
"(y − d)2
c + λ|a|
) , (3)
where A > 0, c > 0, d > 0, λ > 0 are tuning parameters.",3.4. Reward Function,[0],[0]
"In plain English, Eq. (3) says that the maximum reward A is achieved when the object stands perfectly in front of the agent with a distance d and exhibits no rotation (see Fig. 3).
",3.4. Reward Function,[0],[0]
In Eq.,3.4. Reward Function,[0],[0]
(3) we have omitted the time step subscript t without loss of clarity.,3.4. Reward Function,[0],[0]
Also note that the reward function defined in this way does not explicitly depend on the raw visual state.,3.4. Reward Function,[0],[0]
"Instead, it depends on certain internal states.",3.4. Reward Function,[0],[0]
"Thanks to the APIs provided by virtual environments, we are able to access the interested internal states and develop the desired
reward function.",3.4. Reward Function,[0],[0]
"To make the tracker generalize well, we propose simple yet effective techniques for environment augmentation during training.
",3.5. Environment Augmentation,[0],[0]
"For ViZDoom, recall the object’s local position and orientation (x, y, a) in system S described in Sec. 3.4.",3.5. Environment Augmentation,[0],[0]
"For a given environment (i.e., a ViZDoom map) with initial (x, y, a), we randomly perturb it N times by editing the map with the ACS script (Kempka et al., 2016), yielding a set of environments with varied initial positions and orientations {xi, yi, ai}Ni=1.",3.5. Environment Augmentation,[0],[0]
We further allow flipping left-right the screen frame (and accordingly the left-right action).,3.5. Environment Augmentation,[0],[0]
"As a result, we obtain 2N environments out of one environment.",3.5. Environment Augmentation,[0],[0]
"See Fig. 3 for an illustration of several possible initial positions and orientations in the local system S. During the A3C training, we uniformly randomly sample one of the 2N environments at the beginning of every episode.",3.5. Environment Augmentation,[0],[0]
"As will be seen in Sec. 4.2, this technique significantly improves the generalization ability of the tracker.
",3.5. Environment Augmentation,[0],[0]
"For UE, we construct an environment with a character/target walking following a fixed path.",3.5. Environment Augmentation,[0],[0]
"To augment the environment, we randomly choose some background objects (e.g., tree or building) in the environment and make them invisible.",3.5. Environment Augmentation,[0],[0]
"At the same time, every episode starts from the position, where the agent fails at the last episode.",3.5. Environment Augmentation,[0],[0]
"This makes the environment and the starting point different from episode to episode, so the variations of the environment during training are augmented.",3.5. Environment Augmentation,[0],[0]
The settings are described in Sec. 4.1.,4. Experimental Results,[0],[0]
"The experimental results are reported for the virtual environments ViZDoom
(Sec. 4.2) and UE (Sec. 4.3).",4. Experimental Results,[0],[0]
Qualitative evaluation is performed for real-world scenarios taken from the VOT dataset (Sec. 4.4).,4. Experimental Results,[0],[0]
"To investigate what the tracker has learned, we conduct ablation analysis using a saliency visualization technique (Simonyan et al., 2013) in Sec. 4.5.",4. Experimental Results,[0],[0]
Environment.,4.1. Settings,[0],[0]
A set of environments are produced for both training and testing.,4.1. Settings,[0],[0]
"For ViZDoom, we adopt a training map as in Fig. 4, left column.",4.1. Settings,[0],[0]
"This map is then augmented as described in Sec. 3.5 with N = 21, leading to 42 environments that we can sample from during training.",4.1. Settings,[0],[0]
"For testing, we make other 9 maps, some of which are shown in Fig. 4, middle and right columns.",4.1. Settings,[0],[0]
"In all maps, the path of the target is pre-specified, indicated by the blue lines.",4.1. Settings,[0],[0]
"However, it is worth noting that the object does not strictly follow the planned path.",4.1. Settings,[0],[0]
"Instead, it sometimes randomly moves in a “zig-zag” way during the course, which is a built-in game engine behavior.",4.1. Settings,[0],[0]
"This poses an additional difficulty to the tracking problem.
",4.1. Settings,[0],[0]
"For UE, we generate an environment named Square with random invisible background objects and a target named Stefani walking along a fixed path for training.",4.1. Settings,[0],[0]
"For testing, we make another four environments named as Square1StefaniPath1 (S1SP1), Square1MalcomPath1 (S1MP1), Square1StefaniPath2 (S1SP2), and Square2MalcomPath2 (S2MP2).",4.1. Settings,[0],[0]
"As shown in Fig. 5, Square1 and Square2 are two different maps, Stefani and Malcom are two characters/targets, and Path1 and Path2 are different paths.",4.1. Settings,[0],[0]
"Note that, the training environment Square is generated by hiding some background objects in Square1.
",4.1. Settings,[0],[0]
"For both ViZDoom and UE, we terminate an episdoe when either the accumulated reward drops below a threshold or the episode length reaches a maximum number.",4.1. Settings,[0],[0]
"In our experiments, we let the reward threshold be -450 and the maximum length be 3000, respectively.
",4.1. Settings,[0],[0]
Metric.,4.1. Settings,[0],[0]
Two metrics are employed for the experiments.,4.1. Settings,[0],[0]
"Specifically, Accumulated Reward (AR) and Episode Length (EL) of each episode are calculated for quantitative evaluation.",4.1. Settings,[0],[0]
"Note that, the immediate reward defined in Eq.",4.1. Settings,[0],[0]
"(3) measures the goodness of tracking at some time step, so the metric AR is conceptually much like Precision in the conventional tracking literature.",4.1. Settings,[0],[0]
Also note that too small AR means a failure of tracking and leads to a termination of the current episode.,4.1. Settings,[0],[0]
"As such, the metric EL roughly measures the duration of good tracking, which shares the same spirit of the metric Successfully Tracked Frames in conventional tracking applications.",4.1. Settings,[0],[0]
When letting A = 1.0 in Eq.,4.1. Settings,[0],[0]
"(3), we have that the theoretically maximum AR and EL are both 3000 due to our episode termination criterion.",4.1. Settings,[0],[0]
"In all the following experiments, 100 episodes are run to report the mean and standard deviation, unless specified otherwise.
",4.1. Settings,[0],[0]
Implementation details.,4.1. Settings,[0],[0]
We include the implementation details in the supplementary material due to the space constraint.,4.1. Settings,[0],[0]
"We firstly test the active tracker in a testing environment named Standard, showing the effectiveness of the proposed environment augmentation technique.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"The second part is contributed to the experiments in more challenging testing environments which vary from the Standard environment with regard to object appearance, background, path, and object distraction.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Comparison with a set of traditional trackers is conducted in the last part.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
Standard Testing Environment.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"In Tab. 1, we report the results in an independent testing environment named Standard (see supplementary materials for its detailed description), where we compare two training protocols: with (called RandomizedEnv) or without (called SingleEnv) the augmentation technique as in Sec. 3.5.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"As can be seen, RandomizedEnv performs significantly better than SingleEnv.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"We discover that the SingleEnv protocol quickly exhausts
#1360 #1361 #1372 #1376 #1395 #1410
Figure 6.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Recovering tracking when the target disappears in the SharpTurn environment.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
the training capability and obtains the best validation result at about 9 × 106 training iterations.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"On the contrary, the best validation result of RandomizedEnv protocol occurs at 48 × 106, showing that the capacity of the network is exploited better despite the longer training time.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"In the following experiments, we only report experimental results with the RandomizedEnv protocol.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
Various Testing Environments.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"To evaluate the generalization ability of our active tracker, we test it in 8 more challenging environments as in Tab. 2.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Comparing to the training environment, they present different target appearances, different backgrounds, more varying paths, and distracting targets.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"See supplementary materials for the detailed description.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
From the 4 categories in Tab. 2 we have findings below.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
1),4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"The tracker generalizes well in the case of target appearance changing (Zombie, Cacodemon).",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
2),4.2. Active Tracking in The ViZDoom Environment,[0],[0]
The tracker is insensitive to background variations such as changing the ceiling and floor (FloorCeiling) or placing additional walls in the map (Corridor).,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
3),4.2. Active Tracking in The ViZDoom Environment,[0],[0]
The tracker does not lose a target even when the target takes several sharp turns (SharpTurn).,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Note that in conventional tracking, the target is commonly assumed to move smoothly.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
We also observe that the tracker can recover tracking when it accidentally loses the target.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"As shown in Fig. 6, the target turns right suddenly and the tracker loses it (frame #1372).",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Although the target completely disappears in the image, the tracker takes a series of turn-right actions (frame #1376 to #1394).",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"It rediscovers the target (frame #1410), and continues to track steadily afterwards.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
We believe that this capability attributes to the LSTM unit which takes into account historical states when producing current outputs.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Our tracker performs well when the target walks counterclockwise (Counterclockwise), indicating that the tracker does not work by simply memorizing the turning pattern.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
4),4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"The tracker is insensitive to a distracting object (Noise1), even when the “bait” is very close to the path (Noise2).
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
The proposed tracker shows satisfactory generalization in various unseen environments.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Readers are encouraged to
watch more result videos provided in our supplementary materials.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
Comparison with Simulated Active Trackers.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
In a more extensive experiment we compare the proposed tracker with a few traditional trackers.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
These trackers are originally developed for passive tracking applications.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Particularly, the MIL (Babenko et al., 2009), Meanshift (Comaniciu et al., 2000), KCF (Henriques et al., 2015), and Correlation (Danelljan et al., 2014) trackers are employed for comparison.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"We implement them by directly invoking the interface from OpenCV (Ope) (MIL, KCF and Meanshift trackers) and Dlib (Dli) (Correlation tracker).
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"To make the comparison feasible, we add to the passive tracker an additional PID-like module for the camera control, enabling it to interact with the environment (see Fig. 1, Right).",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"In the first frame, a manual bounding box must be given to indicate the object to be tracked.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"For each subsequent frame, the passive tracker then predicts a bounding box, which is passed to the “Camera Control” module.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Finally, the action is produced by “pulling back” the target to its position in a previous frame (see supplementary materials for the details of the implementation).",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"For a fair comparison
with the proposed active tracker, we employ the same action set A as described in Sec. 3.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Armed with this camera-control module, the performance of traditional trackers is compared with the active tracker in Standard, SharpTurn and Cacodemon.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
The results in Tab. 3 show that the end-to-end active tracker beats the simulated “active” trackers by a significant gap.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
We investigate the tracking process of these trackers and find that they lose the target soon.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"The Meanshift tracker works well when there is no camera shift between continuous frames, while in the active tracking scenario it loses the target soon.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Both KCF and Correlation trackers seem not capable of handling such a large camera shift, so they do not work as well as the case in passive tracking.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"The MIL tracker works reasonably in the active case, while it easily drifts when the object turns suddenly.
",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
"Recalling Fig. 6, another reason of our tracker beating the traditional trackers is that our tracker can quickly discover the target again in the case that it is missed.",4.2. Active Tracking in The ViZDoom Environment,[0],[0]
While the simulated active trackers can hardly recover from failure cases.,4.2. Active Tracking in The ViZDoom Environment,[0],[0]
We firstly compare models trained with randomized environment and single environment.,4.3. Active Tracking in The UE Environment,[0],[0]
"Then we test our active tracker in four environments and also compare it against
traditional trackers.
RandomizedEnv versus SingleEnv.",4.3. Active Tracking in The UE Environment,[0],[0]
"Based on the Square environment, we train two models individually by the RandomizedEnv protocol (random number of invisible background objects and starting point) and SingleEnv protocol (fixed environment).",4.3. Active Tracking in The UE Environment,[0],[0]
"They are tested in the S2MP2 environment, where the map, target, and the path are unseen during training.",4.3. Active Tracking in The UE Environment,[0],[0]
"As shown in Tab. 4, similar results are obtained as those in Tab. 1.",4.3. Active Tracking in The UE Environment,[0],[0]
We believe that the improvement benefits from the environment randomness brought by the proposed environment augmentation techniques.,4.3. Active Tracking in The UE Environment,[0],[0]
"In the following, we only report the results of RandomizedEnv protocol.
",4.3. Active Tracking in The UE Environment,[0],[0]
Various Testing Environments.,4.3. Active Tracking in The UE Environment,[0],[0]
"To intensively investigate the generalization ability of the active tracker, we test it in four different environments and present the results in Tab. 5.",4.3. Active Tracking in The UE Environment,[0],[0]
"We compare it with the simulated active trackers described in Sec. 4.2, as well as one based on the long-term TLD tracker (Kalal et al., 2012).
",4.3. Active Tracking in The UE Environment,[0],[0]
"According to the results in Tab. 5 we conduct the following analysis: 1) Comparison between S1SP1 and S1MP1 shows that the tracker generalizes well even when the model is trained with target Stefani, revealing that it does not overfit to a specialized appearance.",4.3. Active Tracking in The UE Environment,[0],[0]
2),4.3. Active Tracking in The UE Environment,[0],[0]
"The active tracker performs well when changing the path (S1SP1 versus S1SP2), demonstrating that it does not act by memorizing specialized path.",4.3. Active Tracking in The UE Environment,[0],[0]
3),4.3. Active Tracking in The UE Environment,[0],[0]
"When we change the map, target, and path at the same time (S2MP2), though the tracker could not seize the target as accurately as in previous environments (the AR value drops), it can still track objects robustly (comparable EL value as in previous environments), proving its superior generalization potential.",4.3. Active Tracking in The UE Environment,[0],[0]
4),4.3. Active Tracking in The UE Environment,[0],[0]
"In most cases, the proposed tracker outperforms the simulated active tracker, or achieves compa-
rable results if it is not the best.",4.3. Active Tracking in The UE Environment,[0],[0]
"The results of the simulated active tracker also suggest that it is difficult to tune a unified camera-control module for them, even when a long term tracker is adopted (see the results of TLD).",4.3. Active Tracking in The UE Environment,[0],[0]
"However, our work exactly sidesteps this issue by training an end-to-end active tracker.",4.3. Active Tracking in The UE Environment,[0],[0]
"To evaluate how the active tracker performs in real-world scenarios, we take the network trained in a UE environment and test it on a few video clips from the VOT dataset (Kristan et al., 2016).",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"Obviously, we can by no means control the camera action for a recorded video.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"However, we can feed in the video frame sequentially and observe the output action predicted by the network, checking whether it is consistent with the actual situation.
",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"Fig. 7 shows the output actions for two video clips named Woman and Sphere, respectively.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"The horizontal axis indicates the position of the target in the image, with a positive (negative) value meaning that a target in the right (left) part.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"The vertical axis indicates the size of the target, i.e., the area of the ground truth bounding box.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"Green and red dots indicate turn-left/turn-left-and-move-forward and turn-right/turn-right-and-move-forward actions, respectively.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
Yellow dots represent No-op action.,4.4. Active Tracking in Real-world Scenarios,[0],[0]
"As the figure
show, 1) When the target resides in the right (left) side, the tracker tends to turn right (left), trying to move the camera to “pull” the target to the center.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"2) When the target size becomes bigger, which probably indicates that the tracker is too close to the target, the tracker outputs no-op actions more often, intending to stop and wait the target to move farther.
",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"We believe that the qualitative evaluation shows evidence that the active tracker, learned from purely the virtual environment, is able to output correct actions for camera control in real-world scenarios.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
"Due to the constraint of space, we include more results of the real-world scenarios in the supplementary materials.",4.4. Active Tracking in Real-world Scenarios,[0],[0]
We are curious about what the tracker has learned so that it leads to good performance.,4.5. Action Saliency Map,[0],[0]
"To this end, we follow the method in (Simonyan et al., 2013) to generate a saliency map of the input image with regard to a specific action.",4.5. Action Saliency Map,[0],[0]
"Making it more specific, an input frame si is fed into the tracker and forwarded to output the policy function.",4.5. Action Saliency Map,[0],[0]
An action ai will be sampled subsequently.,4.5. Action Saliency Map,[0],[0]
"Then the gradient of ai with regard to si is propagated backwards to the input layer, and a saliency map is generated.",4.5. Action Saliency Map,[0],[0]
"This process calculates exactly which part of the original input image influences the corresponding action with the greatest magnitude.
",4.5. Action Saliency Map,[0],[0]
"Note that the saliency map is image specific, i.e., for each input image a corresponding saliency map can be derived.",4.5. Action Saliency Map,[0],[0]
"Consequently, we can observe how the input images influence the tracker’s actions.",4.5. Action Saliency Map,[0],[0]
Fig. 8 shows a few pairs of input image and corresponding saliency map.,4.5. Action Saliency Map,[0],[0]
The saliency maps consistently show that the pixels corresponding to the object dominate the importance to actions of the tracker.,4.5. Action Saliency Map,[0],[0]
It indicates that the tracker indeed learns how to find the target.,4.5. Action Saliency Map,[0],[0]
We proposed an end-to-end active tracker via deep reinforcement learning.,5. Conclusion,[0],[0]
"Unlike conventional passive trackers, the proposed tracker is trained in simulators, saving the efforts of human labeling or trail-and-errors in real-world.",5. Conclusion,[0],[0]
It shows good generalization to unseen environments.,5. Conclusion,[0],[0]
The tracking ability can potentially transfer to real-world scenarios.,5. Conclusion,[0],[0]
We appreciate the anonymous ICML reviews that improve the quality of this paper.,ACKNOWLEDGEMENT,[0],[0]
Thank Jia Xu for his helpful discussion.,ACKNOWLEDGEMENT,[0],[0]
"Fangwei Zhong and Yizhou Wang were supported in part by the following grants 973-2015CB351800, NSFC61625201, NSFC-61527804.",ACKNOWLEDGEMENT,[0],[0]
"We study active object tracking, where a tracker takes as input the visual observation (i.e., frame sequence) and produces the camera control signal (e.g., move forward, turn left, etc.).",abstractText,[0],[0]
"Conventional methods tackle the tracking and the camera control separately, which is challenging to tune jointly.",abstractText,[0],[0]
It also incurs many human efforts for labeling and many expensive trial-and-errors in realworld.,abstractText,[0],[0]
"To address these issues, we propose, in this paper, an end-to-end solution via deep reinforcement learning, where a ConvNet-LSTM function approximator is adopted for the direct frame-toaction prediction.",abstractText,[0],[0]
"We further propose an environment augmentation technique and a customized reward function, which are crucial for a successful training.",abstractText,[0],[0]
"The tracker trained in simulators (ViZDoom, Unreal Engine) shows good generalization in the case of unseen object moving path, unseen object appearance, unseen background, and distracting object.",abstractText,[0],[0]
It can restore tracking when occasionally losing the target.,abstractText,[0],[0]
"With the experiments over the VOT dataset, we also find that the tracking ability, obtained solely from simulators, can potentially transfer to real-world scenarios.",abstractText,[0],[0]
End-to-end Active Object Tracking via Reinforcement Learning,title,[0],[0]
Learning a policy from scratch is often difficult.,1. Introduction,[0],[0]
"However, in many problems, there exists an expert policy that achieves satisfactory performance.",1. Introduction,[0],[0]
We are interested in the scenario of imitating an expert.,1. Introduction,[0],[0]
"Imitation is needed for several reasons: Automation (in case the expert is human), distillation (e.g., if the expert is too expensive to run in realtime (Rusu et al., 2015)), and initialization (using an expert policy as an initial solution).",1. Introduction,[0],[0]
"In our setting, we assume that trajectories {s0, a0, s1, ...}Ni=0 of an expert policy πE are given.",1. Introduction,[0],[0]
"Our goal is to train a new policy π which imitates πE without access to the original reward signal rE that was used by the expert.
",1. Introduction,[0],[0]
There are two main approaches to solve imitation problems.,1. Introduction,[0],[0]
"The first, known as Behavioral Cloning (BC), directly learns the conditional distribution of actions over states p(a|s) in a supervised learning fashion (Pomerleau,
1Technion Institute of Technology, Israel.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Nir Baram <nirb@campus.technion.ac.il>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
1991).",1. Introduction,[0],[0]
"By providing constant supervision (dense reward signal in Reinforcement Learning (RL) terminology), BC overcomes fundamental difficulties of RL such as the credit assignment problem (Sutton, 1984).",1. Introduction,[0],[0]
"However, BC has its downsides as well.",1. Introduction,[0],[0]
"Contrary to temporal difference methods (Sutton, 1988) that integrate information over time, BC methods are trained using single time-step state-action pairs {st, at}.",1. Introduction,[0],[0]
"Therefore, an agent trained using BC is unaware of how his choice of actions affects the future state distribution, which makes it susceptible to compounding errors (Ross & Bagnell, 2010; Ross et al., 2011).",1. Introduction,[0],[0]
"On top of that, the sample complexity of BC methods is usually high, requiring a significant amount of expert data that could be expensive to obtain.
",1. Introduction,[0],[0]
The second approach to imitation is comprised of two stages.,1. Introduction,[0],[0]
"First, recovering a reward signal under which the expert is uniquely optimal (often called inverse RL, for instance see Ng, Russell, et al.):
",1. Introduction,[0],[0]
"E [∑
t
γtr̂(st, at)|πE ]",1. Introduction,[0],[0]
≥ E,1. Introduction,[0],[0]
"[∑ t γtr̂(st, at)|π ] ∀π.
(1)",1. Introduction,[0],[0]
"After reconstructing a reward signal r̂, the second step is to train a policy that maximizes the discounted cumulative expected reward: EπR =",1. Introduction,[0],[0]
Eπ [∑T t=0 γ tr̂t ] .,1. Introduction,[0],[0]
"The problem with this approach stems from the fact that restoring an informative reward signal, solely based on state-action observations, is an ill-posed problem (Ziebart et al., 2008).",1. Introduction,[0],[0]
A different strategy could be to recover a sparser reward signal (a more well-defined problem) and enrich it by hand.,1. Introduction,[0],[0]
"However, this requires extensive domain knowledge (Dorigo & Colombetti, 1998).
",1. Introduction,[0],[0]
"Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) is a recent method for training generative models.",1. Introduction,[0],[0]
"It uses a second neural network (D) to guide the generative model (G) towards producing patterns similar to those of the expert (see illustration in Figure 1).
",1. Introduction,[0],[0]
"The elegance of GANs has made it popular among a variety of problems other than creating generative models, such as image captioning (Mirza & Osindero, 2014) and video prediction (Mathieu et al., 2015).",1. Introduction,[0],[0]
"More recently, a work named Generative Adversarial Imitation Learning (GAIL) (Ho & Ermon, 2016), has successfully applied the ideas of GANs to imitate an expert in a model-free setup.",1. Introduction,[0],[0]
"It showed
that this type of learning could alleviate problems such as sample complexity or covariate shifts (Kanamori & Shimodaira, 2003), traditionally coupled with imitation learning.
",1. Introduction,[0],[0]
The disadvantage of the model-free approach comes to light when training stochastic policies.,1. Introduction,[0],[0]
"The presence of stochastic elements breaks the flow of information (gradients) from one neural network to the other, thus prohibiting the use of backpropagation.",1. Introduction,[0],[0]
"In this situation, a standard solution is to use gradient estimation (Williams, 1992).",1. Introduction,[0],[0]
"This tends to suffer from high variance, resulting in a need for larger sample sizes as well as variance reduction methods.
",1. Introduction,[0],[0]
"In this work, we present a model-based imitation learning algorithm (MGAIL), in which information propagates fluently from the guiding neural network (D) to the generative model G, which in our case represents the policy π we wish to train.",1. Introduction,[0],[0]
"This is achieved by (a) learning a forward model that approximates the environment’s dynamics, and (b) building an end-to-end differentiable computation graph that spans over multiple time-steps.",1. Introduction,[0],[0]
"The gradient in such a graph carries information from future states to earlier time-steps, helping the policy to account for compounding errors.",1. Introduction,[0],[0]
This leads to better policies that require fewer expert samples and interactions with the environment.,1. Introduction,[0],[0]
"In this section, we review the mathematical formulation of Markov Decision Processes, as well as previous approaches
to imitation learning.",2. Background,[0],[0]
"Lastly, we present GANs in detail.",2. Background,[0],[0]
"Consider an infinite-horizon discounted Markov decision process (MDP), defined by the tuple (S,A, P, r, ρ0, γ), where S is a set of states, A is a set of actions, P : S ×",2.1. Markov Decision Process,[0],[0]
A × S →,2.1. Markov Decision Process,[0],[0]
"[0, 1] is the transition probability distribution, r : (S × A) → R is the reward function, ρ0 : S",2.1. Markov Decision Process,[0],[0]
"→ [0, 1] is the distribution over initial states, and γ ∈ (0, 1) is the discount factor.",2.1. Markov Decision Process,[0],[0]
Let π denote a stochastic policy π : S ×,2.1. Markov Decision Process,[0],[0]
"A → [0, 1], R(π) denote its expected discounted reward: EπR = Eπ [∑T t=0 γ tr̂t ] , and τ denote a trajectory of states and actions τ = {s0, a0, s1, a1, ...}.",2.1. Markov Decision Process,[0],[0]
"Learning control policies directly from expert demonstrations, has been proven very useful in practice, and has led to satisfying performance in a wide range of applications (Ross et al., 2011).",2.2. Imitation Learning,[0],[0]
"A common approach to imitation learning is to train a policy π to minimize some loss function l(s, π(s)), under the discounted state distribution encountered by the expert: dπ(s) = (1−γ) ∑∞",2.2. Imitation Learning,[0],[0]
"t=0 γ
tp(st).",2.2. Imitation Learning,[0],[0]
This is possible using any standard supervised learning algorithm: π = argminπ∈Π Es∼dπ,2.2. Imitation Learning,[0],[0]
"[l(s, π(s))], where Π denotes the class of all possible policies.",2.2. Imitation Learning,[0],[0]
"However, the policy’s prediction affects the future state distribution, which violates the i.i.d assumption made by most SL algorithms.",2.2. Imitation Learning,[0],[0]
"A slight deviation in the learner’s behavior may lead it to a different state distribution than the one encountered by the expert, resulting in compounding errors.
",2.2. Imitation Learning,[0],[0]
"To overcome this issue, Ross & Bagnell (2010) introduced the Forward Training (FT) algorithm that trains a nonstationary policy iteratively over time (one policy πt for each time-step).",2.2. Imitation Learning,[0],[0]
"At time t, πt is trained to mimic πE on the state distribution induced by the previously trained policies π0, π1, ...πt−1.",2.2. Imitation Learning,[0],[0]
"This way, πt is trained on the actual state distribution it will encounter at inference.",2.2. Imitation Learning,[0],[0]
"However, the FT algorithm is impractical when the time horizon T is large (or undefined), since it needs to train a policy at each time-step, and cannot be stopped before completion.",2.2. Imitation Learning,[0],[0]
"The Stochastic Mixing Iterative Learning (SMILe) algorithm, proposed by the same authors, solves this problem by training a stochastic stationary policy over several iterations.",2.2. Imitation Learning,[0],[0]
SMILe starts with an initial policy π0 that blindly follows the expert’s action choice.,2.2. Imitation Learning,[0],[0]
"At iteration t, a policy π̂t is trained to mimic the expert under the trajectory distribution induced by πt−1, and then updates:
πt = πt−1 + α(1− α)t−1(π̂t − π0).
",2.2. Imitation Learning,[0],[0]
"Overall, both the FT algorithm and SMILe gradually modify the policy from following the expert’s policy to the learned one.",2.2. Imitation Learning,[0],[0]
"GANs learn a generative model using a two-player zerosum game:
argmin G argmax D∈(0,1)",2.3. Generative Adversarial Networks,[0],[0]
ExvpE,2.3. Generative Adversarial Networks,[0],[0]
"[logD(x)]+
Ezvpz [ log ( 1−D(G(z)) )",2.3. Generative Adversarial Networks,[0],[0]
"] , (2)
where pz is some noise distribution.",2.3. Generative Adversarial Networks,[0],[0]
"In this game, player G produces patterns (denoted as x), and the second one (D) judges their authenticity.",2.3. Generative Adversarial Networks,[0],[0]
"It does so by solving a binary classification problem where G’s patterns are labeled as 0, and expert patterns are labeled as 1.",2.3. Generative Adversarial Networks,[0],[0]
"At the point when D (the judge) can no longer discriminate between the two distributions, the game ends since G has learned to mimic the expert.
",2.3. Generative Adversarial Networks,[0],[0]
"The two players are modeled by neural networks (with parameters θd, θg respectively), therefore, their combination creates an end-to-end differentiable computation graph.",2.3. Generative Adversarial Networks,[0],[0]
"For this reason, G can train by generating patterns, feeding it to D, and minimize the probability that D assigns to them:
l(z, θg) = log ( 1−D(Gθg (z)) ) ,
The benefit of GANs is that it relieves us from the need to define a loss function or to handle complex models such as RBM’s and DBN’s (Lee et al., 2009).",2.3. Generative Adversarial Networks,[0],[0]
"Instead, GANs rely on basic ideas (binary classification), and basic algorithms (backpropagation).",2.3. Generative Adversarial Networks,[0],[0]
"The judge D trains to solve a binary classification problem by ascending at the following gradient:
∇θd 1
m m∑ i=1",2.3. Generative Adversarial Networks,[0],[0]
"[ logDθd ( x(i) ) + log ( 1−Dθd ( G(z(i) ))] ,
interchangeably while G descends at the following direction:
∇θg 1
m m∑ i=1 log ( 1−D ( Gθg (z (i)) )) .
",2.3. Generative Adversarial Networks,[0],[0]
Ho & Ermon (2016) (GAIL) proposed to apply GANs to an expert policy imitation task in a model-free approach.,2.3. Generative Adversarial Networks,[0],[0]
"GAIL draws a similar objective function like GANs, except that here pE stands for the expert’s joint distribution over state-action tuples:
argmin π argmax D∈(0,1)
Eπ[logD(s, a)]+ EπE [log(1−D(s, a))]− λH(π), (3)
where H(λ) , Eπ[− log π(a|s)] is the entropy.
",2.3. Generative Adversarial Networks,[0],[0]
"The new game defined by Eq. 3 can no longer be solved using the standard tools mentioned above because playerG (i.e., the policy π) is now stochastic.",2.3. Generative Adversarial Networks,[0],[0]
"Following this modification, the exact form of the first term in Eq. 3 is given
by Es∼ρπ(s)Ea∼π(·|s)[logD(s, a)], instead of the following expression if π was deterministic: Es∼ρ[logD(s, π(s))].",2.3. Generative Adversarial Networks,[0],[0]
The resulting game depends on the stochastic properties of the policy.,2.3. Generative Adversarial Networks,[0],[0]
"So, assuming that π = πθ, it is no longer clear how to differentiate Eq. 3 w.r.t.",2.3. Generative Adversarial Networks,[0],[0]
θ.,2.3. Generative Adversarial Networks,[0],[0]
"A standard solution is to use score function methods (Fu, 2006), of which REINFORCE is a special case (Williams, 1992), to obtain an unbiased gradient estimation:
∇θEπ[logD(s, a)]",2.3. Generative Adversarial Networks,[0],[0]
∼= Êτi,2.3. Generative Adversarial Networks,[0],[0]
"[∇θ log πθ(a|s)Q(s, a)], (4)
where Q(ŝ, â) is the score function of the gradient:
Q(ŝ, â) = Êτi",2.3. Generative Adversarial Networks,[0],[0]
"[logD(s, a) | s0 = ŝ, a0 = â].",2.3. Generative Adversarial Networks,[0],[0]
"(5)
Although unbiased, REINFORCE gradients tend to suffer high variance, which makes it hard to work with even after applying variance reduction techniques (Ranganath et al., 2014; Mnih & Gregor, 2014).",2.3. Generative Adversarial Networks,[0],[0]
"In the case of GANs, the difference between using the exact gradient and REINFORCE can be explained in the following way: with REINFORCE, G asks D whether the pattern it generates are authentic or not.",2.3. Generative Adversarial Networks,[0],[0]
D in return provides a brief Yes/No answer.,2.3. Generative Adversarial Networks,[0],[0]
"On the other hand, using the exact gradient, G gets access to the internal decision making logic of D. Thus it is better able to understand the changes needed to foolD. Such information is present in the Jacobian of D.
In this work, we show how a forward model utilizes the Jacobian of D when training π, without resorting to highvariance gradient estimations.",2.3. Generative Adversarial Networks,[0],[0]
The challenge of this approach is that it requires learning a differentiable approximation to the environment’s dynamics.,2.3. Generative Adversarial Networks,[0],[0]
Errors in the forward model introduce a bias to the policy gradient which impairs the ability of π to learn robust and competent policies.,2.3. Generative Adversarial Networks,[0],[0]
"We share our insights regarding how to train forward models, and in subsection 3.5 present an architecture that was found empirically adequate in modeling complex dynamics.",2.3. Generative Adversarial Networks,[0],[0]
We start this section by analyzing the characteristics of the discriminator.,3. Algorithm,[0],[0]
"Then, we explain how a forward model can alleviate problems that arise when using GANs for policy imitation.",3. Algorithm,[0],[0]
"Afterward, we present our model-based adversarial imitation algorithm.",3. Algorithm,[0],[0]
We conclude this section by presenting a forward model architecture that was found empirically adequate.,3. Algorithm,[0],[0]
"The discriminator network is trained to predict the conditional distribution: D(s, a) = p(y|s, a) where y ∈ {πE , π}.",3.1. The discriminator network,[0],[0]
"Put in words, D(s, a) represents the likelihood ratio that the pair {s, a} is generated by π rather than by
πE .",3.1. The discriminator network,[0],[0]
"Using Bayes rule and the law of total probability we can write that:
D(s, a) = p(π|s, a) = p(s, a|π)p(π) p(s, a) =
p(s, a|π)p(π) p(s, a|π)p(π) + p(s, a|πE)p(πE) = p(s, a|π) p(s, a|π) + p(s, a|πE) .
(6)
The last equality is correct since the discriminator is trained on an even distribution of expert/generator examples, therefore: p(π) = p(πE) = 12 .",3.1. The discriminator network,[0],[0]
"Re-arranging and factoring the joint distribution we can rewrite D(s, a) as:
D(s, a) = 1
p(s,a|π)+p(s,a|πE) p(s,a|π)
=
1
1 + p(s,a|πE)p(s,a|π) =
1
1 + p(a|s,πE)p(a|s,π) · p(s|πE) p(s|π)
.
(7)
Next let us define ϕ(s, a), and ψ(s) to be:
ϕ(s, a) = p(a|s, πE) p(a|s, π) , ψ(s) = p(s|πE) p(s|π) ,
and attain the final expression for D(s, a):
D(s, a)",3.1. The discriminator network,[0],[0]
"= 1
1 + ϕ(s, a) · ψ(s) .",3.1. The discriminator network,[0],[0]
"(8)
Inspecting the derived expression we see that ϕ(s, a) represents a policy likelihood ratio, and ψ(s) represents a state distribution likelihood ratio.",3.1. The discriminator network,[0],[0]
This interpretation suggests that the discriminator makes its decisions by answering two questions.,3.1. The discriminator network,[0],[0]
The first relates to the state distribution: what is the likelihood of encountering state s under the distribution induced by πE vs. the one induced by π?,3.1. The discriminator network,[0],[0]
"And the second question relates to the behavior: given a state s, how likely is action a under πE vs. π?
We reach the conclusion that effective learning requires the learner to be mindful of two effects.",3.1. The discriminator network,[0],[0]
"First, how its choice of actions stands against the expert?",3.1. The discriminator network,[0],[0]
"And second, how it affects the future state distribution?",3.1. The discriminator network,[0],[0]
The desired change in states is given by ψs ≡ ∂ψ/∂s.,3.1. The discriminator network,[0],[0]
"A careful inspection of the partial derivatives of D reveals that this information is present in the Jacobian of the discriminator:
∇aD = − ϕa(s, a)ψ(s)
(1 + ϕ(s, a)ψ(s))2 ,
∇sD = − ϕs(s, a)ψ(s) + ϕ(s, a)ψs(s)
(1 + ϕ(s, a)ψ(s))2 ,
(9)
which increases the motivation to use it over other highvariance estimations.",3.1. The discriminator network,[0],[0]
"Next, we show how using a forward model, we can build a policy gradient directly from the Jacobian of the discriminator (i.e.,∇aD and ∇sD).",3.1. The discriminator network,[0],[0]
We are interested in training stochastic policies.,3.2. Backpropagating through stochastic units,[0],[0]
Stochasticity is important for Policy Gradient (PG) methods since it encourages exploration.,3.2. Backpropagating through stochastic units,[0],[0]
"However, it poses a challenge for policies modeled by neural networks, considering it is unclear how to backpropagate through the stochastic elements.",3.2. Backpropagating through stochastic units,[0],[0]
"This problem plays a major role in algorithms that build differentiable computation graphs where gradients flow from one component to another, as in the case of deep actor-critic methods (Lillicrap et al., 2015), and GANs.",3.2. Backpropagating through stochastic units,[0],[0]
"In the following, we show how to estimate the gradients of continuous stochastic elements (for continuous action domains), and categorical stochastic elements (for the discrete case).",3.2. Backpropagating through stochastic units,[0],[0]
"In the case of continuous action policies we use a mathematical tool known as ”re-parametrization” (Kingma &
Welling, 2013; Rezende et al., 2014), which enables computing the derivatives of stochastic models.",3.2.1. CONTINUOUS ACTION DISTRIBUTIONS,[0],[0]
"Assume a stochastic policy with a Gaussian distribution1, where the mean and variance are given by some deterministic functions µθ and σθ, respectively: πθ(a|s) ∼ N (µθ(s), σ2θ(s)).",3.2.1. CONTINUOUS ACTION DISTRIBUTIONS,[0],[0]
"It is possible to re-write π as πθ(a|s) = µθ(s) + ξσθ(s), where ξ ∼ N (0, 1).",3.2.1. CONTINUOUS ACTION DISTRIBUTIONS,[0],[0]
"In this way, we are able to get a MonteCarlo estimator of the derivative of the expected value of D(s, a) with respect to θ:
∇θEπ(a|s)D(s, a) =Eρ(ξ)∇aD(a, s)∇θπθ(a|s) ∼=
1
M M∑ i=1",3.2.1. CONTINUOUS ACTION DISTRIBUTIONS,[0],[0]
"∇aD(s, a)∇θπθ(a|s) ∣∣∣ ξ=ξi .
(10)",3.2.1. CONTINUOUS ACTION DISTRIBUTIONS,[0],[0]
"For the case of discrete action domains, we suggest to follow the idea of categorical re-parametrization with Gumbel-Softmax (Maddison et al., 2016; Jang et al., 2016).",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"This approach relies on the Gumbel-Max trick (Gumbel & Lieblein, 1954); a method to draw samples from a categorical distribution with class probabilities π(a1|s), π(a2|s), ...π(aN |s):
aargmax =",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
argmax,3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"i
[gi + log π(ai|s)],
where gi ∼ Gumbel(0, 1).",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"Gumbel-Softmax provides a differentiable approximation of the hard sampling procedure in the Gumbel-Max trick, by replacing the argmax operation with a softmax:
asoftmax = exp",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
[ 1 τ (gi + log π(ai|s)),3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"]∑k j=1 exp [ 1 τ (gj + log π(ai|s))
",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"] , where τ is a ”temperature” hyper-parameter that trades bias with variance.",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"When τ approaches zero, the softmax operator acts like argmax (asoftmax ≈ aargmax) resulting in low bias.",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"However, the variance of the gradient ∇θasoftmax increases.",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"Alternatively, when τ is set to a large value, the softmax operator creates a smoothing effect.",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"This leads to low variance gradients, but at the cost of a high bias (asoftmax 6= aargmax).
",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"We use asoftmax, that is not necessarily ”one-hot”, to interact with the environment, which expects a single (”pure”) action.",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
"We solve this by applying argmax over asoftmax, but use the continuous approximation in the backward pass by using the estimation: ∇θaargmax ≈ ∇θasoftmax.
",3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
1A general version of the re-parametrization trick for other distributions such as beta or gamma was recently proposed by Ruiz et al. (2016),3.2.2. CATEGORICAL ACTION DISTRIBUTIONS,[0],[0]
So far we showed the changes necessary to use the exact partial derivative ∇aD. Incorporating the use of ∇sD as well is a more involved and constitutes the crux of this work.,3.3. Backpropagating through a Forward model,[0],[0]
"To understand why, we can look at the block diagram of the model-free approach in Figure 2.",3.3. Backpropagating through a Forward model,[0],[0]
"There, s is treated as fixed (it is given as an input), therefore ∇sD is discarded.",3.3. Backpropagating through a Forward model,[0],[0]
"On the contrary, in the model-based approach, st can be written as a function of the previous state and action: st = f(st−1, at−1), where f is the forward model.",3.3. Backpropagating through a Forward model,[0],[0]
"This way, using the law of total derivatives, we get that:
∇θD(st, at) ∣∣∣∣∣",3.3. Backpropagating through a Forward model,[0],[0]
"s=st,a=at = ∂D ∂a ∂a",3.3. Backpropagating through a Forward model,[0],[0]
∂θ ∣∣∣∣∣,3.3. Backpropagating through a Forward model,[0],[0]
a=at + ∂D ∂s ∂s,3.3. Backpropagating through a Forward model,[0],[0]
∂θ,3.3. Backpropagating through a Forward model,[0],[0]
"∣∣∣∣∣ s=st =
∂D
∂a
∂a
∂θ ∣∣∣∣∣",3.3. Backpropagating through a Forward model,[0],[0]
a=at + ∂D ∂s,3.3. Backpropagating through a Forward model,[0],[0]
( ∂f ∂s ∂s,3.3. Backpropagating through a Forward model,[0],[0]
∂θ,3.3. Backpropagating through a Forward model,[0],[0]
∣∣∣∣∣ s=st−1 + ∂f,3.3. Backpropagating through a Forward model,[0],[0]
∂a ∂a,3.3. Backpropagating through a Forward model,[0],[0]
∂θ ∣∣∣∣∣,3.3. Backpropagating through a Forward model,[0],[0]
"a=at−1 ) .
(11)
",3.3. Backpropagating through a Forward model,[0],[0]
"Considering that at−1 is a function of θ, we understand that by creating a computation graph that spans over more than a single time-step, we can link between ∇sD and the policy.",3.3. Backpropagating through a Forward model,[0],[0]
"Put in words, during the backward pass, the error message regarding deviations of future states (ψs), propagates back in time and influences the actions of the policy in earlier times.",3.3. Backpropagating through a Forward model,[0],[0]
Figure 3 summarizes this idea.,3.3. Backpropagating through a Forward model,[0],[0]
"We showed that a good approach for imitation requires: (a) to use a model, and (b) to process multi-step transitions.",3.4. MGAIL Algorithm,[0],[0]
"This setup was previously suggested by ShalevShwartz et al. (2016) and Heess et al. (2015), who built a multi-step computation graph for describing the familiar policy gradient objective, which in our case is given by: J(θ) =",3.4. MGAIL Algorithm,[0],[0]
E,3.4. MGAIL Algorithm,[0],[0]
"[∑ t=0 γ tD(st, at) ∣∣θ].",3.4. MGAIL Algorithm,[0],[0]
"To show how to differentiate J(θ) over a trajectory of (s, a, s′) transitions, we rely on the results of Heess et al. (2015):
Js = Ep(a|s)Ep(s′|s,a) [",3.4. MGAIL Algorithm,[0],[0]
Ds +Daπs + γJ ′ s′(fs + faπs),3.4. MGAIL Algorithm,[0],[0]
"] ,
(12)
Jθ = Ep(a|s)Ep(s′|s,a) [ Daπθ + γ(J ′",3.4. MGAIL Algorithm,[0],[0]
s′faπθ + J ′ θ) ] .,3.4. MGAIL Algorithm,[0],[0]
"(13)
The final policy gradient ∇θJ is calculated by applying Eq. 12 and 13 recursively, starting from t = T all the way down to t = 0.",3.4. MGAIL Algorithm,[0],[0]
The full algorithm is presented in Algorithm 1.,3.4. MGAIL Algorithm,[0],[0]
The forward model prediction accuracy plays a crucial role in the stability of the learning process.,3.5. Forward Model Structure,[0],[0]
"However, learning
Algorithm 1 Model-based Generative Adversarial Imitation Learning
1: Input: Expert trajectories τE , experience buffer B, initial policy and discriminator parameters θg , θd 2: for trajectory = 0 to∞ do 3: for t = 0 to T do 4: Act on environment: a = π(s, ξ; θg) 5: Push (s, a, s′) into B 6: end for 7: train forward model f using B 8: train discriminator model Dθd using B 9: set: j′s = 0, j ′",3.5. Forward Model Structure,[0],[0]
"θg
= 0 10: for t = T down to 0 do 11: jθg =",3.5. Forward Model Structure,[0],[0]
[Daπθg + γ(j ′,3.5. Forward Model Structure,[0],[0]
"s′faπθg + j ′ θg )] ∣∣ ξ
12: js =",3.5. Forward Model Structure,[0],[0]
[Ds +Daπs + γj′s′(fs,3.5. Forward Model Structure,[0],[0]
+ faπθg )] ∣∣ ξ 13: end for 14:,3.5. Forward Model Structure,[0],[0]
"Apply gradient update using j0θg 15: end for
an accurate forward model is a challenging problem by itself.",3.5. Forward Model Structure,[0],[0]
We found that the performance of the forward model can be improved by considering the following two aspects of its functionality.,3.5. Forward Model Structure,[0],[0]
"First, the forward model should learn to use the action as an operator over the state space.",3.5. Forward Model Structure,[0],[0]
"Actions and states are sampled from entirely different distributions, so it would be preferable to first represent both in a shared space.",3.5. Forward Model Structure,[0],[0]
"Therefore, we first encode the state and action with two separate neural networks and then combine them to form a single vector.",3.5. Forward Model Structure,[0],[0]
We found empirically that using a Hadamard product to combine the encoded state and action achieves the best performance.,3.5. Forward Model Structure,[0],[0]
"Additionally, predicting the next state based on the current state alone requires the environment to be representable as a first order MDP.",3.5. Forward Model Structure,[0],[0]
"Instead, we can assume the environment to be representable as an n’th order MDP and use multiple previous states to predict the next state.",3.5. Forward Model Structure,[0],[0]
"To model the multi-step dependencies, we use a recurrent connection from the previous state by incorporating a GRU layer (Cho et al., 2014) as part of the state encoder.",3.5. Forward Model Structure,[0],[0]
"Introducing these two modifications (see Figure 4), we found the complete model to achieve better
and more stable results compared to using a vanilla feedforward neural network as the forward model, as seen in Figure 5.",3.5. Forward Model Structure,[0],[0]
"We evaluate the proposed algorithm on three discrete control tasks (Cartpole, Mountain-Car, Acrobot), and five continuous control tasks (Hopper, Walker, Half-Cheetah, Ant, and Humanoid) modeled by the MuJoCo physics simulator (Todorov et al., 2012).",4. Experiments,[0],[0]
These tasks involve complex second order dynamics and direct torque control.,4. Experiments,[0],[0]
"We use the Trust Region Policy Optimization (TRPO) algorithm (Schulman et al., 2015) to train expert policies.",4. Experiments,[0],[0]
"For each task, we produce datasets with a different number of trajectories, where each trajectory: τ = {s0, s1, ...sN , aN} is of length N = 1000.
",4. Experiments,[0],[0]
"The discriminator and policy neural networks are built from two hidden layers with Relu non-linearity and are trained using the ADAM optimizer (Kingma & Ba, 2014).",4. Experiments,[0],[0]
"Table 1 presents the total reward over a period of N steps, measured using three different algorithms: BC, GAIL, and MGAIL.",4. Experiments,[0],[0]
The results for BC and GAIL are as reported in Ho & Ermon (2016).,4. Experiments,[0],[0]
Our algorithm achieves the highest reward for most environments while exhibiting performance comparable to the expert over all of them (a Wilcoxon signed-rank test indicates superior performance with p-value < 0.05).,4. Experiments,[0],[0]
"We also compared the performance of MGAIL when using a basic forward model, versus using the more advanced model as described in Section 3.",4. Experiments,[0],[0]
"Fig-
ure 5 shows that better and more stable results are obtained when using the advanced forward model.",4. Experiments,[0],[0]
"In this work, we presented a model-based algorithm for imitation learning.",5. Discussion,[0],[0]
We showed how using a forward model enables to train policies using the exact gradient of the discriminator network.,5. Discussion,[0],[0]
"This way, the policy can imitate the expert’s behavior, but also account for undesired deviations in the distributions of future states.",5. Discussion,[0],[0]
The downside of this approach is the need to learn a forward model; a task that could prove difficult in some domains.,5. Discussion,[0],[0]
"An interesting line of future work would be to learn the system dynamics directly from raw images, as was done in Oh et al. (2015).
GANs algorithm violates a fundamental assumption made by all SL algorithms, which requires the data to be i.i.d.",5. Discussion,[0],[0]
The problem arises because the discriminator network trains on a dynamic data distribution.,5. Discussion,[0],[0]
"For the training to succeed, the discriminator must continually adapt to the changes in the policy.",5. Discussion,[0],[0]
"In our context, the problem is emphasized even more since both the discriminator and the forward models are trained in a SL fashion using data that is sampled from a replay buffer B (Lin, 1993).",5. Discussion,[0],[0]
"A possible remedy is to restart the learning multiple times along the training period by resetting the learning rate (Loshchilov & Hutter, 2016).",5. Discussion,[0],[0]
We tried this solution without significant success.,5. Discussion,[0],[0]
"However, we believe that further research in this direction is needed.
",5. Discussion,[0],[0]
This research was supported in part by the European Communitys Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL) and the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).,5. Discussion,[0],[0]
Generative Adversarial Networks (GANs) have been successfully applied to the problem of policy imitation in a model-free setup.,abstractText,[0],[0]
"However, the computation graph of GANs, that include a stochastic policy as the generative model, is no longer differentiable end-to-end, which requires the use of high-variance gradient estimation.",abstractText,[0],[0]
"In this paper, we introduce the Modelbased Generative Adversarial Imitation Learning (MGAIL) algorithm.",abstractText,[0],[0]
"We show how to use a forward model to make the computation fully differentiable, which enables training policies using the exact gradient of the discriminator.",abstractText,[0],[0]
The resulting algorithm trains competent policies using relatively fewer expert samples and interactions with the environment.,abstractText,[0],[0]
We test it on both discrete and continuous action domains and report results that surpass the state-of-the-art.,abstractText,[0],[0]
End-to-End Differentiable Adversarial Imitation Learning,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1181–1194 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
"Tree Adjoining Grammar (TAG, Joshi and Schabes (1997)) and Combinatory Categorial Grammar (CCG, Steedman and Baldridge (2011)) are both mildly context-sensitive grammar formalisms that are lexicalized: every elementary structure (elementary tree for TAG and category for CCG) is associated with exactly one lexical item, and every lexical item of the language is associated with a finite set of elementary structures in the grammar (Rambow and Joshi, 1994).",1 Introduction,[0],[0]
"In TAG and CCG, the task of parsing can be decomposed into two phases (e.g. TAG: Bangalore and Joshi (1999); CCG: Clark and Curran (2007)):",1 Introduction,[0],[0]
"supertagging, where elementary units or supertags are assigned to each lexical item and parsing where these supertags are combined together.",1 Introduction,[0],[0]
"The first phase of supertagging can be considered as “almost parsing” because supertags for a sentence almost always determine a unique parse (Bangalore and
Joshi, 1999).",1 Introduction,[0],[0]
"This near uniqueness of a parse given a gold sequence of supertags has been confirmed empirically (TAG: Bangalore et al. (2009); Chung et al. (2016); Kasai et al. (2017); CCG: Lewis et al. (2016)).
",1 Introduction,[0],[0]
We focus on TAG parsing in this work.,1 Introduction,[0],[0]
TAG differs from CCG in having a more varied set of supertags.,1 Introduction,[0],[0]
"Concretely, the TAG-annotated version of the WSJ Penn Treebank (Marcus et al., 1993) that we use (Chen et al., 2005) includes 4727 distinct supertags (2165 occur once) while the CCGannotated version (Hockenmaier and Steedman, 2007) only includes 1286 distinct supertags (439 occur once).",1 Introduction,[0],[0]
"This large set of supertags in TAG presents a severe challenge in supertagging and causes a large discrepancy in parsing performance with gold supertags and predicted supertags (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017).
",1 Introduction,[0],[0]
"In this work, we present a supertagger and a parser that substantially improve upon previously reported results.",1 Introduction,[0],[0]
We propose crucial modifications to the bidirectional LSTM (BiLSTM) supertagger in Kasai et al. (2017).,1 Introduction,[0],[0]
"First, we use character-level Convolutional Neural Networks (CNNs) for encoding morphological information instead of suffix embeddings.",1 Introduction,[0],[0]
"Secondly, we perform concatenation after each BiLSTM layer.",1 Introduction,[0],[0]
"Lastly, we explore the impact of adding additional BiLSTM layers and highway connections.",1 Introduction,[0],[0]
These techniques yield an increase of 1.3% in accuracy.,1 Introduction,[0],[0]
"For parsing, since the derivation tree in a lexicalized TAG is a type of dependency tree (Rambow and Joshi, 1994), we can directly apply dependency parsing models.",1 Introduction,[0],[0]
"In particular, we use the biaffine graph-based parser proposed by Dozat and Manning (2017) together with our novel techniques for supertagging.
",1 Introduction,[0],[0]
"In addition to these architectural extensions for supertagging and parsing, we also explore multitask learning approaches for TAG parsing.",1 Introduction,[0],[0]
"Specif-
1181
",1 Introduction,[0],[0]
"ically, we perform POS tagging, supertagging, and parsing using the same feature representations from the BiLSTMs.",1 Introduction,[0],[0]
"This joint modeling has the benefit of avoiding a time-consuming and complicated pipeline process, and instead produces a full syntactic analysis, consisting of supertags and the derivation that combines them, simultaneously.",1 Introduction,[0],[0]
"Moreover, this multi-task learning framework further improves performance in all three tasks.",1 Introduction,[0],[0]
"We hypothesize that our multi-task learning yields feature representations in the LSTM layers that are more linguistically relevant and that generalize better (Caruana, 1997).",1 Introduction,[0],[0]
"We provide support for this hypothesis by analyzing syntactic analogies across induced vector representations of supertags (Kasai et al., 2017; Friedman et al., 2017).",1 Introduction,[0],[0]
"The end-to-end TAG parser substantially outperforms the previously reported best results.
",1 Introduction,[0],[0]
"Finally, we apply our new parsers to the downstream tasks of Parsing Evaluation using Textual Entailements (PETE, Yuret et al. (2010)) and Unbounded Dependency Recovery (Rimell et al., 2009).",1 Introduction,[0],[0]
We demonstrate that our end-to-end parser outperforms the best results in both tasks.,1 Introduction,[0],[0]
These results illustrate that TAG is a viable formalism for tasks that benefit from the assignment of rich structural descriptions to sentences.,1 Introduction,[0],[0]
TAG parsing can be decomposed into supertagging and parsing.,2 Our Models,[0],[0]
"Supertagging assigns to words elementary trees (supertags) chosen from a finite set, and parsing determines how these elementary trees can be combined to form a derivation tree that yield the observed sentence.",2 Our Models,[0],[0]
"The combinatory operations consist of substitution, which inserts obligatory arguments, and adjunction, which is responsible for the introduction of modifiers, function words, as well as the derivation of sentences involving long-distance dependencies.",2 Our Models,[0],[0]
"In this section, we present our supertagging models, parsing models, and joint models.",2 Our Models,[0],[0]
"Recent work has explored neural network models for supertagging in TAG (Kasai et al., 2017) and CCG (Xu et al., 2015; Lewis et al., 2016; Vaswani et al., 2016; Xu, 2016), and has shown that such models substantially improve performance beyond non-neural models.",2.1 Supertagging Model,[0],[0]
"We extend previously proposed BiLSTM-based models (Lewis
et al., 2016; Kasai et al., 2017) in three ways: 1) we add character-level Convolutional Neural Networks (CNNs) to the input layer, 2) we perform concatenation of both directions of the LSTM not only after the final layer but also after each layer, and 3) we use a modified BiLSTM with highway connections.",2.1 Supertagging Model,[0],[0]
"The input for each word is represented via concatenation of a 100-dimensional embedding of the word, a 100-dimensional embedding of a predicted part of speech (POS) tag, and a 30- dimensional character-level representation from CNNs that have been found to capture morphological information (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016).",2.1.1 Input Representations,[0],[0]
The CNNs encode each character in a word by a 30 dimensional vector and 30 filters produce a 30 dimensional vector for the word.,2.1.1 Input Representations,[0],[0]
"We initialize the word embeddings to be the pre-trained GloVe vectors (Pennington et al., 2014); for words not in GloVe, we initialize their embedding to a zero vector.",2.1.1 Input Representations,[0],[0]
The other embeddings are randomly initialized.,2.1.1 Input Representations,[0],[0]
We obtain predicted POS tags from a BiLSTM POS tagger with the same configuration as in Ma and Hovy (2016).,2.1.1 Input Representations,[0],[0]
"The core of the supertagging model is a deep bidirectional Long Short-Term Memory network (Graves and Schmidhuber, 2005).",2.1.2 Deep Highway BiLSTM,[0],[0]
"We use the following formulas to compute the activation of a single LSTM cell at time step t:
it = σ",2.1.2 Deep Highway BiLSTM,[0],[0]
"(Wi[xt;ht−1] + bi) (1)
ft = σ (Wf [xt;ht−1] + bf ) (2)
c̃t = tanh (Wc[xt;ht−1] + bc) (3)
ot = σ (Wo[xt;ht−1] + bo) (4)
ct = f ct−1",2.1.2 Deep Highway BiLSTM,[0],[0]
"+ it c̃t (5) ht = o tanh (ct) (6)
Here a semicolon ; means concatenation, is element-wise multiplication, and σ is the sigmoid function.",2.1.2 Deep Highway BiLSTM,[0],[0]
"In the first BiLSTM layer, the input xt is the vector representation of word t. (The sequence is reversed for the backwards pass.)",2.1.2 Deep Highway BiLSTM,[0],[0]
"In all subsequent layers, xt is the corresponding output from the previous BiLSTM; the output of a BiLSTM at timestep t is equal to [hft ;h b t ], the concatenation of hidden state corresponding to input t in the forward and backward pass.",2.1.2 Deep Highway BiLSTM,[0],[0]
"This concatenation af-
ter each layer differs from Kasai et al. (2017) and Lewis et al. (2016), where concatenation happens only after the final BiLSTM layer.",2.1.2 Deep Highway BiLSTM,[0],[0]
"We will show in a later section that concatenation after each layer contributes to improvement in performance.
",2.1.2 Deep Highway BiLSTM,[0],[0]
We also extend the models in Kasai et al. (2017) and Lewis et al. (2016) by allowing highway connections between LSTM layers.,2.1.2 Deep Highway BiLSTM,[0],[0]
"A highway connection is a gating mechanism that combines the current and previous layer outputs, which can prevent the problem of vanishing/exploding gradients (Srivastava et al., 2015).",2.1.2 Deep Highway BiLSTM,[0],[0]
"Specifically, in networks with highway connections, we replace Eq. 6 by:
rt = σ (Wr[xt;ht−1] + br) ht = rt ot tanh (ct) +",2.1.2 Deep Highway BiLSTM,[0],[0]
"(1− rt) Whxt
Indeed, our experiments will show that highway connections play a crucial role as we add more BiLSTM layers.
",2.1.2 Deep Highway BiLSTM,[0],[0]
We generally follow the hyperparameters chosen in Lewis et al. (2016) and Kasai et al. (2017).,2.1.2 Deep Highway BiLSTM,[0],[0]
"Specifically, we use BiLSTMs layers with 512 units each.",2.1.2 Deep Highway BiLSTM,[0],[0]
"Input, layer-to-layer, and recurrent (Gal and Ghahramani, 2016) dropout rates are all 0.5.",2.1.2 Deep Highway BiLSTM,[0],[0]
"For the CNN character-level representation, we used the hyperparameters from Ma and Hovy (2016).
",2.1.2 Deep Highway BiLSTM,[0],[0]
"We train this network, including the embeddings, by optimizing the negative log-likelihood of the observed sequences of supertags in a minibatch stochastic fashion with the Adam optimization algorithm with batch size 100 and ` = 0.01 (Kingma and Ba, 2015).",2.1.2 Deep Highway BiLSTM,[0],[0]
"In order to obtain predicted POS tags and supertags of the training data for subsequent parser input, we also perform 10- fold jackknife training.",2.1.2 Deep Highway BiLSTM,[0],[0]
"After each training epoch, we test the supertagger on the dev set.",2.1.2 Deep Highway BiLSTM,[0],[0]
"When classification accuracy does not improve on five consecutive epochs, training ends.",2.1.2 Deep Highway BiLSTM,[0],[0]
"Until recently, TAG parsers have been grammar based, requiring as input a set of elemenetary trees (supertags).",2.2 Parsing Model,[0],[0]
"For example, Bangalore et al. (2009) proposes the MICA parser, an Earley parser that exploits a TAG grammar that has been transformed into a variant of a probabilistic CFG.",2.2 Parsing Model,[0],[0]
"One advantage of such a parser is that its parses are guaranteed to be well-formed according to the TAG grammar provided as input.
",2.2 Parsing Model,[0],[0]
"More recent work, however, has shown that data-driven transition-based parsing systems outperform such grammar-based parsers (Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017).",2.2 Parsing Model,[0],[0]
Kasai et al. (2017) and Friedman et al. (2017) achieved state-of-the-art TAG parsing performance using an unlexicalized shift-reduce parser with feed-forward neural networks that was trained on a version of the Penn Treebank that had been annotated with TAG derivations.,2.2 Parsing Model,[0],[0]
"Here, we pursue this data-driven approach, applying a graph-based parser with deep biaffine attention (Dozat and Manning, 2017) that allows for global training and inference.",2.2 Parsing Model,[0],[0]
"The input for each word is the concatenation of a 100-dimensional embedding of the word and a 30-dimensional character-level representation obtained from CNNs in the same fashion as in the supertagger.1 We also consider adding 100-dimensional embeddings for a predicted POS tag (Dozat and Manning, 2017) and a predicted supertag (Kasai et al., 2017; Friedman et al., 2017).",2.2.1 Input Representations,[0],[0]
The ablation experiments in Kiperwasser and Goldberg (2016) illustrated that adding predicted POS tags boosted performance in Stanford Dependencies.,2.2.1 Input Representations,[0],[0]
"In Universal Dependencies, Dozat et al. (2017) empirically showed that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger.",2.2.1 Input Representations,[0],[0]
"Indeed, Kasai et al. (2017) and Friedman et al. (2017) demonstrated that their unlexicalized neural network TAG parsers that only get as input predicted supertags can achieve state-of-theart performance, with lexical inputs providing no improvement in performance.",2.2.1 Input Representations,[0],[0]
We initialize word embeddings to be the pre-trained GloVe vectors as in the supertagger.,2.2.1 Input Representations,[0],[0]
The other embeddings are randomly initialized.,2.2.1 Input Representations,[0],[0]
We train our parser to predict edges between lexical items in an LTAG derivation tree.,2.2.2 Biaffine Parser,[0],[0]
"Edges are labeled by the operations together with the deep syntactic roles of substitution sites (0=underlying subject, 1=underlying direct object, 2=underlying indirect object, 3,4=oblique arguments, CO=cohead for prepositional/particle verbs, and adj=all adjuncts).",2.2.2 Biaffine Parser,[0],[0]
"Figure 1 shows our biaffine parsing ar-
1We fix the embedding of the ROOT token to be a 0- vector.
chitecture.",2.2.2 Biaffine Parser,[0],[0]
"Following Dozat and Manning (2017) and Kiperwasser and Goldberg (2016), we use BiLSTMs to obtain features for each word in a sentence.",2.2.2 Biaffine Parser,[0],[0]
"We add highway connections in the same fashion as our supertagging model.
",2.2.2 Biaffine Parser,[0],[0]
"We first perform unlabeled arc-factored scoring using the final output vectors from the BiLSTMs, and then label the resulting arcs.",2.2.2 Biaffine Parser,[0],[0]
"Specifically, suppose that we score edges coming into the ith word in a sentence i.e. assigning scores to the potential parents of the ith word.",2.2.2 Biaffine Parser,[0],[0]
Denote the final output vector from the BiLSTM for the kth word by hk and suppose that hk is d-dimensional.,2.2.2 Biaffine Parser,[0],[0]
"Then, we produce two vectors from two separate multilayer perceptrons (MLPs) with the ReLU activation:
h arc-dep k = MLP (arc-dep)(hk) harc-headk = MLP (arc-head)(hk)
where harc-depk and h arc-head k are darc-dimensional vectors that represent the kth word as a dependent and a head respectively.",2.2.2 Biaffine Parser,[0],[0]
"Now, suppose the kth row of matrix H (arc-head) is harc-headk .",2.2.2 Biaffine Parser,[0],[0]
"Then, the probability distribution si over the potential heads of the ith word is computed by
si = softmax(H",2.2.2 Biaffine Parser,[0],[0]
(arc-head)W,2.2.2 Biaffine Parser,[0],[0]
(,2.2.2 Biaffine Parser,[0],[0]
"arc)h arc-dep i
+H (arc-head)b(arc))",2.2.2 Biaffine Parser,[0],[0]
"(7)
where W (arc) ∈ Rdarc×darc and b(arc) ∈ Rdarc .",2.2.2 Biaffine Parser,[0],[0]
"In training, we simply take the greedy maximum
probability to predict the parent of each word.",2.2.2 Biaffine Parser,[0],[0]
"In the testing phase, we use the heuristics formulated by Dozat and Manning (2017) to ensure that the resulting parse is single-rooted and acyclic.
",2.2.2 Biaffine Parser,[0],[0]
"Given the head prediction of each word in the sentence, we assign labeling scores using vectors obtained from two additional MLP with ReLU.",2.2.2 Biaffine Parser,[0],[0]
"For the kth word, we obtain:
h rel-dep k = MLP (rel-dep)(hk) hrel-headk = MLP (rel-head)(hk)
where hrel-depk , h rel-head k ∈",2.2.2 Biaffine Parser,[0],[0]
Rdrel .,2.2.2 Biaffine Parser,[0],[0]
"Let pi be the index of the predicted head of the ith word, and r be the number of dependency relations in the dataset.",2.2.2 Biaffine Parser,[0],[0]
"Then, the probability distribution `i over the possible dependency relations of the arc pointing from the pith word to the ith word is calculated by:
`i = softmax(hT (rel-head)pi U (rel)h (rel-dep)",2.2.2 Biaffine Parser,[0],[0]
"i
+W (rel)(h(rel-head)i + h (rel-head) pi ) + b
(rel)) (8)
where U (rel) ∈ Rdrel×drel×r,W (rel) ∈ Rr×drel , and b(rel) ∈",2.2.2 Biaffine Parser,[0],[0]
"Rr.
We generally follow the hyperparameters chosen in Dozat and Manning (2017).",2.2.2 Biaffine Parser,[0],[0]
"Specifically, we use BiLSTMs layers with 400 units each.",2.2.2 Biaffine Parser,[0],[0]
"Input, layer-to-layer, and recurrent dropout rates are all 0.33.",2.2.2 Biaffine Parser,[0],[0]
"The depths of all MLPs are all 1, and the MLPs for unlabeled attachment and those for labeling contain 500 (darc) and 100 (drel) units respectively.",2.2.2 Biaffine Parser,[0],[0]
"For character-level CNNs, we use the hyperparameters from Ma and Hovy (2016).
",2.2.2 Biaffine Parser,[0],[0]
"We train this model with the Adam algorithm to minimize the sum of the cross-entropy losses from head predictions (si from Eq. 7) and label predictions (`i from Eq. 8) with ` = 0.01 and batch size 100 (Kingma and Ba, 2015).",2.2.2 Biaffine Parser,[0],[0]
"After each training epoch, we test the parser on the dev set.",2.2.2 Biaffine Parser,[0],[0]
"When labeled attachment score (LAS)2 does not improve on five consecutive epochs, training ends.",2.2.2 Biaffine Parser,[0],[0]
"The simple BiLSTM feature representations for parsing presented above are conducive to joint modeling of POS tagging and supertagging; rather than using POS tags and supertags to predict a derivation tree, we can instead use the BiLSTM hidden vectors derived from lexical inputs alone
2We disregard pure punctuation when evaluating LAS and UAS, following prior work (Bangalore et al., 2009; Chung et al., 2016; Kasai et al., 2017; Friedman et al., 2017).
to predict POS tags and supertags along with the TAG derivation tree.
",2.3 Joint Modeling,[0],[0]
"h pos k = MLP (pos)(hk) h stag k = MLP (stag)(hk)
where hposk ∈ Rdpos and h stag k ∈",2.3 Joint Modeling,[0],[0]
Rdstag .,2.3 Joint Modeling,[0],[0]
"We obtain probability distribution over the POS tags and supertags by:
softmax(W (pos)hposk + b (pos))",2.3 Joint Modeling,[0],[0]
(9) softmax(W,2.3 Joint Modeling,[0],[0]
(stag)hstagk + b (stag)),2.3 Joint Modeling,[0],[0]
"(10)
where W (pos), b(pos), W (stag), and b(stag) are in Rnpos×dpos , Rnpos , Rnstag×dstag , and Rnstag respectively, with npos and nstag the numbers of possible POS tags and supertags respectively.
",2.3 Joint Modeling,[0],[0]
We use the same hyperparameters as in the parser.,2.3 Joint Modeling,[0],[0]
The MLPs for POS tagging and supertagging both contain 500 units.,2.3 Joint Modeling,[0],[0]
"We again train this model with the Adam algorithm to minimize the sum of the cross-entropy losses from head predictions (si from Eq. 7), label predictions (`i from Eq. 8), POS predictions (Eq. 9), and supertag predictions (Eq. 10) with ` = 0.01 and batch size 100.",2.3 Joint Modeling,[0],[0]
"After each training epoch, we test the parser on the dev set and compute the percentage of each token that is assigned the correct parent, relation, supertag, and POS tag.",2.3 Joint Modeling,[0],[0]
"When the percentage does not improve on five consecutive epochs, training ends.
",2.3 Joint Modeling,[0],[0]
This joint modeling has several advantages.,2.3 Joint Modeling,[0],[0]
"First, the joint model yields a full syntactic analysis simultaneously without the need for training separate models or performing jackknife training.",2.3 Joint Modeling,[0],[0]
"Secondly, joint modeling introduces a bias on the hidden representations that could allow for better generalization in each task (Caruana, 1997).",2.3 Joint Modeling,[0],[0]
"Indeed, in experiments described in a later section, we show empirically that predicting POS tags and supertags does indeed benefit performance on parsing (as well as the tagging tasks).",2.3 Joint Modeling,[0],[0]
"We follow the protocol of Bangalore et al. (2009), Chung et al. (2016), Kasai et al. (2017), and Friedman et al. (2017); we use the grammar and the TAG-annotated WSJ Penn Tree Bank extracted by Chen et al. (2005).",3 Results and Discussion,[0],[0]
"Following that work, we use Sections 01-22 as the training set, Section 00 as the dev set, and Section 23 as the test set.",3 Results and Discussion,[0],[0]
"The training, dev, and test sets comprise 39832, 1921,
and 2415 sentences, respectively.",3 Results and Discussion,[0],[0]
"We implement all of our models in TensorFlow (Abadi et al., 2016).3",3 Results and Discussion,[0],[0]
"Our BiLSTM POS tagger yielded 97.37% and 97.53% tagging accuracy on the dev and test sets, performance on par with the state-of-the-art (Ling et al., 2015; Ma and Hovy, 2016).4 Seen in the middle section of Table 1 is supertagging performance obtained from various model configurations.",3.1 Supertaggers,[0],[0]
“Final concat” in the model name indicates that vectors from forward and backward pass are concatenated only after the final layer.,3.1 Supertaggers,[0],[0]
Concatenation happens after each layer otherwise.,3.1 Supertaggers,[0],[0]
Numbers immediately after BiLSTM indicate the numbers of layers.,3.1 Supertaggers,[0],[0]
"CNN, HW, and POS denote respectively character-level CNNs, highway connections, and pipeline POS input from our BiLSTM POS tagger.",3.1 Supertaggers,[0],[0]
"Firstly, the differences in performance between BiLSTM2 (final concat) and BiLSTM2 and between BiLSTM2 and BiLSTM2CNN suggest an advantage to performing concatenation after each layer and adding character-level CNNs.",3.1 Supertaggers,[0],[0]
Adding predicted POS to the input somewhat helps supertagging though the difference is small.,3.1 Supertaggers,[0],[0]
"Adding a third BiLSTM layer helps only if there are highway connections, presumably because deeper BiLSTMs are more vulnerable to the vanishing/exploding gradient problem.",3.1 Supertaggers,[0],[0]
"Our supertagging model (BiLSTM3-HW-CNN-POS) that performs best on the dev set achieves an accuracy of 90.81% on the test set, outperforming the previously best result by more than 1.3%.",3.1 Supertaggers,[0],[0]
Table 3 shows parsing results on the dev set.,3.2 Parsers,[0],[0]
Abbreviations for models are as before with one addition: Stag denotes pipeline supertag input from our best supertagger (BiLSTM3-HW-CNNPOS in Table 1).,3.2 Parsers,[0],[0]
"As with supertagging, we observe a gain from adding character-level CNNs.",3.2 Parsers,[0],[0]
"Interestingly, adding predicted POS tags or supertags deteriorates performance with BiLSTM3.",3.2 Parsers,[0],[0]
"These results suggest that morphological information and word information from character-level CNNs and word embeddings overwhelm the in-
3Our code is available online for easy replication of our results at https://github.com/jungokasai/ graph_parser.
4We cannot directly compare these results because the data split is different in the POS tagging literature.
formation from predicted POS tags and supertags.",3.2 Parsers,[0],[0]
"Again, highway connections become crucial as the number of layers increases.",3.2 Parsers,[0],[0]
We finally evaluate the parsing model with the best dev performance (BiLSTM4-HW-CNN) on the test set (Table 3).,3.2 Parsers,[0],[0]
"It achieves 91.37 LAS points and 92.77 UAS points, improvements of 1.8 and 1.7 points respectively from the state-of-the-art.",3.2 Parsers,[0],[0]
We provide joint modeling results for supertagging and parsing in Tables 2 and 3.,3.3 Joint Models,[0],[0]
"For these joint models, we employed the best parsing configuration (4 layers of BiLSTMs, character-level CNNs, and highway connections), with and without POS tagging added as an additional task.",3.3 Joint Models,[0],[0]
"We can observe that our full joint model that performs
POS tagging, supertagging, and parsing further improves performance in all of the three tasks, yielding the test result of 91.89 LAS and 93.26 UAS points, an improvement of more than 2.2 points each from the state-of-the-art.
",3.3 Joint Models,[0],[0]
"Figures 2 and 3 illustrate the relative performance of the feed-forward neural network shiftreduce TAG parser (Kasai et al., 2017) and our joint graph-based parser with respect to two of the measures explored by McDonald and Nivre (2011), namely dependency length and distance between a dependency and the root of a parse.",3.3 Joint Models,[0],[0]
The graph-based parser outperforms the shift-reduce parser across all conditions.,3.3 Joint Models,[0],[0]
Most interesting is the fact that the graph-based parser shows less of an effect of dependency length.,3.3 Joint Models,[0],[0]
"Since the shiftreduce parser builds a parse sequentially with one parsing action depending on those that come before it, we would expect to find a propogation of errors made in establishing shorter dependencies to the establishment of longer dependencies.
",3.3 Joint Models,[0],[0]
"Lastly, it is worth noting our joint parsing ar-
chitecture has a substantial advantage regarding parsing speed.",3.3 Joint Models,[0],[0]
"Since POS tagging, supertagging, and parsing decisions are made independently for each word in a sentence, our system can parallelize computation once the sentence is encoded in the BiLSTM layers.",3.3 Joint Models,[0],[0]
"Our current implementation processes 225 sentences per second on a single Tesla K80 GPU, an order of magnitude faster than the MICA system (Bangalore et al., 2009).5",3.3 Joint Models,[0],[0]
"Given the improvements we have derived from the joint models, we analyze the nature of inductive bias that results from multi-task training and attempt to provide an explanation as to why joint modeling improves performance.",4 Joint Modeling and Network Representations,[0],[0]
One might argue that joint modeling improves performance merely because it adds noise to each task and prevents over-fitting.,4.1 Noise vs. Inductive Bias,[0],[0]
"If the introduction of noise were the key, we would still expect to gain an improvement in parsing even if the target supertag were corrupted, say by shuffling the order of supertags for the entire training data (Caruana, 1997).",4.1 Noise vs. Inductive Bias,[0],[0]
"We performed this experiment, and the result is shown as “Joint (Shuffled Stag)” in Table 3.",4.1 Noise vs. Inductive Bias,[0],[0]
Parsing performance falls behind the best non-joint parser by 0.7 LAS points.,4.1 Noise vs. Inductive Bias,[0],[0]
"This suggests that inducing the parser to create representations to predict both supertags and a parse tree is beneficial for both tasks, beyond a mere introduction of noise.",4.1 Noise vs. Inductive Bias,[0],[0]
"We next analyze the induced vector representations in the output projection matrices of our supertagger and joint parsers using the syntactic analogy framework (Kasai et al., 2017).",4.2 Syntactic Analogies,[0],[0]
"Consider, for instance, the analogy that an elementary tree representing a clause headed by a transitive verb (t27) is to a clause headed by an intransitive verb (t81) as a subject relative clause headed by a transitive verb (t99) is to a subject relative headed by an intransitive verb (t109).",4.2 Syntactic Analogies,[0],[0]
"Following the ideas in Mikolov et al. (2013) for word analogies, we can express this structural analogy as t27 - t81 +
5While such computational resources were not available in 2009, our parser differs from the MICA chart parser in being able to better exploit parallel computation enabled by modern GPUs.
",4.2 Syntactic Analogies,[0],[0]
t109 = t99 and test it by cosine similarity.,4.2 Syntactic Analogies,[0],[0]
Table 4 shows the results of the analogy test with 246 equations involving structural analogies with only the 300 most frequent supertags in the training data.,4.2 Syntactic Analogies,[0],[0]
"While the embeddings (projection matrix) from the independently trained supertagger do not appear to reflect the syntax, those obtained from the joint models yield linguistic structure despite the fact that the supertag embeddings (projection matrix) is trained without any a priori syntactic knowledge about the elementary trees.
",4.2 Syntactic Analogies,[0],[0]
The best performance is obtained by the supertag representations obtained from the training of the transition-based parser Kasai et al. (2017) and Friedman et al. (2017).,4.2 Syntactic Analogies,[0],[0]
"For the transitionbased parser, it is beneficial to share statistics among the input supertags that differ only by a certain operation or property (Kasai et al., 2017) during the training phase, yielding the success in the analogy task.",4.2 Syntactic Analogies,[0],[0]
"For example, a transitive verb supertag whose object has been filled by substitution should be treated by the parser in the same way as an intransitive verb supertag.",4.2 Syntactic Analogies,[0],[0]
"In our graph-based parsing setting, we do not have a notion of parse history or partial derivations that directly connect intransitive and transitive verbs.",4.2 Syntactic Analogies,[0],[0]
"However, syntactic analogies still hold to a considerable degree in the vector representations of supertags induced by our joint models, with average rank of the correct answer nearly the same as that obtained in the transition-based parser.
",4.2 Syntactic Analogies,[0],[0]
This analysis bolsters our hypothesis that joint training biases representation learning toward linguistically sensible structure.,4.2 Syntactic Analogies,[0],[0]
The supertagger is just trained to predict linear sequences of supertags.,4.2 Syntactic Analogies,[0],[0]
"In this setting, many intervening supertags can occur, for instance, between a subject noun and its verb, and the supertagger might not be able to systematically link the presence of the two in the sequence.",4.2 Syntactic Analogies,[0],[0]
"In the joint models, on the other hand, parsing actions will explicitly guide the network to associate the two supertags.",4.2 Syntactic Analogies,[0],[0]
"Previous work has applied TAG parsing to the downstream tasks of syntactically-oriented textual entailment (Xu et al., 2017) and semantic role labeling (Chen and Rambow, 2003).",5 Downstream Tasks,[0],[0]
"In this work, we apply our parsers to the textual entailment and unbounded dependency recovery tasks and achieve state-of-the-art performance.",5 Downstream Tasks,[0],[0]
"These re-
sults bolster the significance of the improvements gained from our joint parser and the utility of TAG parsing for downstream tasks.",5 Downstream Tasks,[0],[0]
"Parser Evaluation using Textual Entailments (PETE) is a shared task from the SemEval-2010 Exercises on Semantic Evaluation (Yuret et al., 2010).",5.1 PETE,[0],[0]
"The task was intended to evaluate syntactic parsers across different formalisms, focusing on entailments that could be determined entirely on the basis of the syntactic representations of the sentences that are involved, without recourse to lexical semantics, logical reasoning, or world knowledge.",5.1 PETE,[0],[0]
"For example, syntactic knowledge alone tells us that the sentence John, who loves Mary, saw a squirrel entails John saw a squirrel and John loves Mary but not, for instance, that John knows Mary or John saw an animal.",5.1 PETE,[0],[0]
"Prior work found the best performance was achieved with parsers using grammatical frameworks that provided rich linguistic descriptions, including CCG (Rimell and Clark, 2010; Ng et al., 2010), Minimal Recursion Semantics (MRS) (Lien, 2014), and TAG (Xu et al., 2017).",5.1 PETE,[0],[0]
Xu et al. (2017) provided a set of linguisticallymotivated transformations to use TAG derivation trees to solve the PETE task.,5.1 PETE,[0],[0]
"We follow their procedures and evaluation for our new parsers.
",5.1 PETE,[0],[0]
We present test results from two configurations in Table 5.,5.1 PETE,[0],[0]
"One configuration is a pipeline approach that runs our BiLSTM POS tagger, supertagger, and parser.",5.1 PETE,[0],[0]
The other one is a joint approach that only uses our full joint parser.,5.1 PETE,[0],[0]
"The joint method yields 78.1% in accuracy and 76.4% in F1, improvements of 2.4 and 2.7 points over the previously reported best results.",5.1 PETE,[0],[0]
"The unbounded dependency corpus (Rimell et al., 2009) specifically evaluates parsers on unbounded dependencies, which involve a constituent moved from its original position, where an unlimited number of clause boundaries can intervene.",5.2 Unbounded Dependency Recovery,[0],[0]
"The corpus comprises 7 constructions: object extraction from a relative clause (ObRC), object extraction from a reduced relative clause (ObRed), subject extraction from a relative clause (SbRC), free relatives (Free), object wh-questions (ObQ), right node raising (RNR), and subject extraction from an embedded clause (SbEm).
",5.2 Unbounded Dependency Recovery,[0],[0]
"Because of variations across formalisms in their representational format for unbounded depdendencies, past work has conducted manual evaluation on this corpus (Rimell et al., 2009; Nivre et al., 2010).",5.2 Unbounded Dependency Recovery,[0],[0]
We instead conduct an automatic evaluation using a procedure that converts TAG parses to structures directly comparable to those specified in the unbounded dependency corpus.,5.2 Unbounded Dependency Recovery,[0],[0]
"To this end, we apply two types of structural transformation in addition to those used for the PETE task:6 1) a more extensive analysis of coordination, 2) resolution of differences in dependency representations in cases involving copula verbs and co-anchors (e.g., verbal particles).",5.2 Unbounded Dependency Recovery,[0],[0]
See Appendix A for details.,5.2 Unbounded Dependency Recovery,[0],[0]
"After the transformations, we simply check if the resulting dependency graphs contain target labeled arcs given in the dataset.
",5.2 Unbounded Dependency Recovery,[0],[0]
Table 6 shows the results.,5.2 Unbounded Dependency Recovery,[0],[0]
"Our joint parser outperforms the other parsers, including the neural network shift-reduce TAG parser (Kasai et al., 2017).",5.2 Unbounded Dependency Recovery,[0],[0]
Our data-driven parsers yield relatively low performance in the ObQ and RNR constructions.,5.2 Unbounded Dependency Recovery,[0],[0]
"Performance on ObQ is low, we expect, because of their rarity in the data on which the parser is
6One might argue that since the unbounded dependency evaluation is recall-based, we added too many edges by the transformations.",5.2 Unbounded Dependency Recovery,[0],[0]
"However, it turns out that applying all the transformations for the corpus even improves performance on PETE (77.6 F1 score), which considers precision and recall, verifying that our transformations are reasonable.
",5.2 Unbounded Dependency Recovery,[0],[0]
"trained.7 For RNR, rarity may be an issue as well as the limits of the TAG analysis of this construction.",5.2 Unbounded Dependency Recovery,[0],[0]
"Nonetheless, we see that the rich structural representations that a TAG parser provides enables substantial improvements in the extraction of unbounded dependencies.",5.2 Unbounded Dependency Recovery,[0],[0]
"In the future, we hope to evaluate state-of-the-art Stanford dependency parsers automatically.",5.2 Unbounded Dependency Recovery,[0],[0]
"The two major classes of data-driven methods for dependency parsing are often called transitionbased and graph-based parsing (Kübler et al., 2009).",6 Related Work,[0],[0]
"Transition-based parsers (e.g. MALT (Nivre, 2003)) learn to predict the next transition given the input and the parse history.",6 Related Work,[0],[0]
"Graph-based parsers (e.g. MST (McDonald et al., 2005)) are trained to directly assign scores to dependency graphs.
",6 Related Work,[0],[0]
"Empirical studies have shown that a transitionbased parser and a graph-based parser yield similar overall performance across languages (McDonald and Nivre, 2011), but the two strands of data-driven parsing methods manifest the fundamental trade-off of parsing algorithms.",6 Related Work,[0],[0]
"The former prefers rich feature representations with parsing history over global training and exhaustive search, and the latter allows for global training and inference at the expense of limited feature representations (Kübler et al., 2009).
",6 Related Work,[0],[0]
Recent neural network models for transitionbased and graph-based parsing can be viewed as remedies for the aforementioned limitations.,6 Related Work,[0],[0]
Andor et al. (2016) developed a transition-based parser using feed-forward neural networks that performs global training approximated by beam search.,6 Related Work,[0],[0]
"The globally normalized objective addresses the label bias problem and makes global
7The substantially better performance of the C&C parser is in fact the result of additions that were made to the training data.
",6 Related Work,[0],[0]
training effective in the transition-based parsing setting.,6 Related Work,[0],[0]
"Kiperwasser and Goldberg (2016) incorporated a dynamic oracle (Goldberg and Nivre, 2013) in a BiLSTM transition-based parser that remedies global error propagation.",6 Related Work,[0],[0]
"Kiperwasser and Goldberg (2016) and Dozat and Manning (2017) proposed graph-based parsers that have access to rich feature representations obtained from BiLSTMs.
",6 Related Work,[0],[0]
"Previous work integrated CCG supertagging and parsing using belief propagation and dual decomposition approaches (Auli and Lopez, 2011).",6 Related Work,[0],[0]
"Nguyen et al. (2017) incorporated a graph-based dependency parser (Kiperwasser and Goldberg, 2016) with POS tagging.",6 Related Work,[0],[0]
Our work followed these lines of effort and improved TAG parsing performance.,6 Related Work,[0],[0]
"In this work, we presented a state-of-the-art TAG supertagger, a parser, and a joint parser that performs POS tagging, supertagging, and parsing.",7 Conclusion and Future Work,[0],[0]
The joint parser has the benefit of giving a full syntactic analysis of a sentence simultaneously.,7 Conclusion and Future Work,[0],[0]
"Furthermore, the joint parser achieved the best performance, an improvement of over 2.2 LAS points from the previous state-of-the-art.",7 Conclusion and Future Work,[0],[0]
"We have also seen that the joint parser yields state-of-the-art in textual entailment and unbounded dependency recovery tasks, and raised the possibility that TAG can provide useful structural analysis of sentences for other NLP tasks.",7 Conclusion and Future Work,[0],[0]
We will explore more applications of our TAG parsers in future work.,7 Conclusion and Future Work,[0],[0]
"For automatic evaluation on the unbounded dependency recovery corpus (UDR, Rimell et al. (2009)), we run simple conversion of dependency labels in UDR to those in our TAG grammar (See Table 7) with a couple of exceptions.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
• Change arcs from verbs to wh-adverbs as in “where is the city located?”,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"to adjunction.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
• Reflect causative-inchoative alternation in the subject embedded construction.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Concretely, change the role of “door” in “hold the door shut” from the subject to the object of “shut.”
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
We then transform TAG dependency trees.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Finally, we simply check if the resulting dependency graphs contain target labeled arcs given in the dataset.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Below is a full description of transformations.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"This set of structural transformations is applied in the order in which we will present it, so that the output of previous transformations can feed subsequent ones.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In the following, we denote an arc pointing from node B to node A with label C as (A, B, C) where A and B are called the child (dependent) and the parent (head) in the relation.
A.1 Transformations from PETE",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"We apply three types of transformation from Xu et al. (2017) to interpret the TAG parses.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Relative Clauses When an elementary tree of a relative clause adjoins into a noun, we add a reverse arc with the label reflecting the type of the relative clause elementary tree.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For a subject relative, we add a 0-labeled arc, for an object relative, we add a 1-labeled arc, and so forth.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Sentential Complements Sentential complementation in TAG derivations can be analyzed via either adjoining the higher clause into the embedded clause (necessarily so in cases of longdistance extraction from the embedded clause) or substituting the embedded clause in the higher clause.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In order to normalize this divergence, for an adjunction arc involving a predicative auxiliary elementary tree (supertag), we add a reverse arc involving the 1 relation (sentential complements).
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
A.2 Coordination We roughly follow the method presented in Xu et al. (2017) with extensions.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Under the TAG analysis, VP coordination involves a VP-recursive auxiliary tree headed by the coordinator that includes a VP substitution node (for the second conjunct) with label 1.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In order to allow the first clauses subject argument (as well as modal verbs and negations) to be shared by the second verb, we add the relevant relations to the second verb.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In addition, we analyze sentential coordination cases.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Sentence coordination in our TAG grammar usually happens between two complete sentences and no modifiers or arguments are shared, and therefore it can be analyzed via substituting a sentence int the coordinator with label 1.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"However, when sentential coordination happens between two relative clause modifiers, our TAG grammar analyzes the second clause as a complete sentence, meaning that we need to recover the extracted argument by consulting the property of the first clause.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Furthermore, the deep syntactic role of the extracted argument can be different in the two relative clauses.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For instance, in the sentence, “... the same stump which had impaled the car of many a guest in the past thirty years and which he refused to have removed,” we need to recover an arc from removed to stump with label 1 whereas the arc from impaled to stump has label 0.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"To resolve this issue, when there is coordination of two relative clause modifiers, we add an edge from the head of the second clause to the modified noun with the same label as the label that under which the relative pronoun is attached to the head.
A.3 Resolving Differences in Dependency Representations
Small Clauses The UDR corpus has inconsistency with regards to small clauses.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"UDR gives an analysis that a small clause contains a subject and a complement as in (nsubj, guy, liar) in
“the guy who I call a liar.”",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
in the subject embedded constructions.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"However, in the object question and object free relative constructions, a small clause is analyzed as two arguments of the verb.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For instance, UDR specifies (what, adopted, dobj) in “we adopted what I would term pseudocapitalism.”",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"To solve this problem we add an arc from the head of the matrix clause to the subject in a small clause with label 1.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Co-,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"anchors In our TAG grammar, Co-anchor attachment represents the substitution into a node that is construed as a co-head of an elementary tree.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For instance, “for” is deemed as a co-anchor to “hope” in the sentence “that is exactly what I’m hoping for (Figure 4).",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In this case, UDR would pick the relation (what, hope, pobj).",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Therefore, when there is a co-anchor to a head tree, we add all arcs that involve the head tree to the co-anchor tree.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Wh-determiners and Wh-adverbs Our TAG grammar analyzes a wh-determiner via adjoining the noun into the wh-determiner (Figure 5).,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
This is also true for cases where a wh-adverb is followed by an adjective and a noun as in how many battles did she win?,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In contrast, UDR corpus gives an analysis that the noun is the head of the constituent.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In order to resolve this discrepancy, when a word adjoins into a wh-word,8 we pick all arcs with the wh-word as the child and add the arcs obtained from such arcs by replacing the wh-word child by the word adjoining into the wh-word.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Copulas A copula is usually treated as a dependent to the predicate both in our TAG grammar (adjunction) and UDR.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"However, we found two situations where they differ from each other.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"First, when wh-extraction happens on the complement, as in “obviously there has been no agreement on what American conservatism is, or rather, what it should be,” the TAG grammar analyzes it via substituting the wh-word (“what”) into the copula (“is”).",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"To reconcile this disagreement between the TAG grammar and UDR, when substitution happens into a be verb, we add the substitution into
8We considered imposing a more strict condition that the word adjoining into the wh-word is a noun, but we found cases that this method fails to cover; for example, UDR gives (dobj, get, much) for a sentence “opinion is mixed on how much of a boost the overall stock market would get even if dividend growth continues at double-digit levels.”
the copula.9 Second, UDR treats non-be copulas differently than be verbs.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"An example is the UDR relation (those, stayed, nsubj)",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"“in the other hemisphere it is growing colder and nymphs, those who stayed alive through the summer, are being brought into nests for quickening and more growing” where our parser yields (those, alive, 0).",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For this reason, when a lemma of a verb is a non-be copula,10 we add arcs involving the word to the copula adjoining into the copula.
PP attachment with multiple noun candidates We observed that PP attachment with multiple noun candidates is often at stake in UDR.11 For instance, UDR provides (part, had, nsubj) and (several, tried, nsubj) for the sentences “... there is no part of the earth that has not had them” and “there were several on the Council who tried to live like Christians” while the TAG parser outputs (earth, had, nsubj) and (Council, tried, nsubj) respectively.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"While we count these cases as “wrong” since they manifest certain disambiguation (though not purely unbounded dependency recovery), we ignore superficial (conventional) differences in head selection.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
In our TAG grammar “a lot of people” would be headed by “lot” whereas UDR would recognize “people” as the head.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Hence, when “lot/lots/kind/kinds/none of” occurs, we add all arcs with “lot/lots/kind/kinds/none” to the head of the phrase that is the object of “of.”
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Modals In the UDR corpus, a modal depends on an auxiliary verb following the modal, if there is one.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For example, “Rosie reinvented this man, who may or may not have known about his child” is given the relation (may, have, aux).",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"In the TAG grammar, both “may” and “have” adjoin into “known.”",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Therefore, when the head of a modal has another child with adjunction, we add an arc from the child to the modal.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
Existential there UDR gives the “cop” relation between an existential there and the be verb.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"For example, it gives (be, legislation, cop) in “... on how much social legislation there should be.”",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"On the other hand, our TAG grammar analyzes that
9We use the nltk lemmatizer (Bird et al., 2009) to identify be verbs.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"10We chose “ stay,” “become,” “seem,” and “remain.”",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
11This is indeed one of the problems with UDR.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Performance on UDR is not purely reflective of unbounded dependency recovery.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"“there” is attached to “be” with label 0.12 To resolve this issue, for arcs that point into an existential there with label 0, we add a reverse edge with label 0.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"Determiner modifying a sentence Finally, when a determiner followed by an adjective modifies a sentence via adjunction in our TAG as in “the more highly placed they are – that is, the more they know – the more concerned they have become,” we add an edge from the verb to the adjective with label 1.
",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"12Usually, “there” is attached to the noun, not the be verb, but in this case, extraction is happening on the noun, so the be verb becomes the head.",A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
See the discussion on copulas above.,A Transformations for Unbounded Dependency Recovery Corpus,[0],[0]
"We present a graph-based Tree Adjoining Grammar (TAG) parser that uses BiLSTMs, highway connections, and characterlevel CNNs.",abstractText,[0],[0]
"Our best end-to-end parser, which jointly performs supertagging, POS tagging, and parsing, outperforms the previously reported best results by more than 2.2 LAS and UAS points.",abstractText,[0],[0]
"The graph-based parsing architecture allows for global inference and rich feature representations for TAG parsing, alleviating the fundamental trade-off between transition-based and graph-based parsing systems.",abstractText,[0],[0]
We also demonstrate that the proposed parser achieves state-of-the-art performance in the downstream tasks of Parsing Evaluation using Textual Entailments (PETE) and Unbounded Dependency Recovery.,abstractText,[0],[0]
This provides further support for the claim that TAG is a viable formalism for problems that require rich structural analysis of sentences.,abstractText,[0],[0]
End-to-end Graph-based TAG Parsing with Neural Networks,title,[0],[0]
"Structured Prediction Energy Networks (SPENs) are a simple, yet expressive family of structured prediction models (Belanger & McCallum, 2016). An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization. This paper presents end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction. In our experience, the approach is substantially more accurate than the structured SVM method of Belanger & McCallum (2016), as it allows us to use more sophisticated non-convex energies. We provide a collection of techniques for improving the speed, accuracy, and memory requirements of end-to-end SPENs, and demonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005 semantic role labeling tasks. In both, inexact minimization of non-convex SPEN energies is superior to baseline methods that use simplistic energy functions that can be minimized exactly.",text,[0],[0]
"In a variety of application domains, given an input x we seek to predict a structured output y.",1. Introduction,[0],[0]
"For example, given a noisy image, we predict a clean version of it, or given a sentence we predict its semantic structure.",1. Introduction,[0],[0]
"Often, it is insufficient to employ a feed-forward predictor y = F (x), since this may have prohibitive sample complexity, fail to model global interactions among outputs, or fail to enforce hard output constraints.",1. Introduction,[0],[0]
"Instead, it can be advantageous to define the prediction function implicitly in terms of energy
1University of Massachusetts, Amherst 2Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: David Belanger <belanger@cs.umass.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"minimization (LeCun et al., 2006):
ŷ = argmin
y
E x (y), (1)
",1. Introduction,[0],[0]
"where E x (·) depends on x and learned parameters.
",1. Introduction,[0],[0]
"This approach includes factor graphs (Kschischang et al., 2001), e.g., conditional random fields (CRFs) (Lafferty et al., 2001), and many recurrent neural networks (Sec. 2.1).",1. Introduction,[0],[0]
Output constraints can be enforced using constrained optimization.,1. Introduction,[0],[0]
"Compared to feed-forward approaches, energy-based approaches often provide better opportunities to inject prior knowledge about likely outputs and often have more parsimonious models.",1. Introduction,[0],[0]
"On the other hand, energy-based prediction requires non-trivial search in the exponentially-large space of outputs, and search techniques often need to be designed on a case-by-case basis.
",1. Introduction,[0],[0]
"Structured prediction energy networks (SPENs) (Belanger & McCallum, 2016) help reduce these concerns.",1. Introduction,[0],[0]
They can capture high-arity interactions among components of y that would lead to intractable factor graphs and provide a mechanism for automatic structure learning.,1. Introduction,[0],[0]
This is accomplished by expressing the energy function in Eq.,1. Introduction,[0],[0]
"(1) as a deep architecture and forming predictions by approximately optimizing y using gradient descent.
",1. Introduction,[0],[0]
"While providing the expressivity and generality of deep networks, SPENs also maintain the useful semantics of energy functions: domain experts can design architectures to capture known properties of the data, energy functions can be combined additively, and we can perform constrained optimization over y. Most importantly, SPENs provide for black-box interaction with the energy, via forward and back-propagation.",1. Introduction,[0],[0]
"This allows practitioners to explore a wide variety of models without the need to hand-design corresponding prediction methods.
",1. Introduction,[0],[0]
"Belanger & McCallum (2016) train SPENs using a structured SVM (SSVM) loss (Taskar et al., 2004; Tsochantaridis et al., 2004) and achieve competitive performance on simple multi-label classification tasks.",1. Introduction,[0],[0]
"Unfortunately, we have found it difficult to extend their method to more complex domains.",1. Introduction,[0],[0]
"SSVMs are unreliable when exact energy minimization is intractable, as loss-augmented inference may fail to discover margin violations (Sec. 2.3).
",1. Introduction,[0],[0]
"In response, we present end-to-end training of SPENs,
where one directly back-propagates through a computation graph that unrolls gradient-based energy minimization.",1. Introduction,[0],[0]
"This does not assume that exact minimization is tractable, and instead directly optimizes the practical performance of a particular approximate minimization algorithm.",1. Introduction,[0],[0]
End-to-end training for gradient-based prediction was introduced in Domke (2012) and applied to deep energy models by Brakel et al. (2013).,1. Introduction,[0],[0]
"See Sec. 3 for details.
",1. Introduction,[0],[0]
"When applying end-to-end training to SPENs for problems with sophisticated output structure, we have encountered a variety of technical challenges.",1. Introduction,[0],[0]
The core contribution of this paper is a set of general-purpose solutions for overcoming these.,1. Introduction,[0],[0]
Sec. 4.1 alleviates the effect of vanishing gradients when training SPENs defined over the convex relaxation of discrete prediction problems.,1. Introduction,[0],[0]
Sec. 4.2 trains energies such that gradient-based minimization is fast.,1. Introduction,[0],[0]
Sec.,1. Introduction,[0],[0]
4.3 reduces SPENs’ computation and memory overhead.,1. Introduction,[0],[0]
"Finally, Sec. 5 provides practical recommendations for specific architectures, parameter tying schemes, and pretraining methods that reduce overfitting and improve efficiency.
",1. Introduction,[0],[0]
We demonstrate the effectiveness of our SPEN training methods on two diverse tasks.,1. Introduction,[0],[0]
"We first consider depth image denoising on the 7-Scenes dataset (Newcombe et al., 2011), where we employ deep convolutional networks as priors over images.",1. Introduction,[0],[0]
"This provides a significant performance improvement, from 36.3 to 40.4 PSNR, over the recent work of (Wang et al., 2016), which unrolls more sophisticated optimization than us, but uses a simpler image prior.",1. Introduction,[0],[0]
"After that, we apply SPENs to semantic role labeling (SRL) on the CoNLL-2005 dataset (Carreras & Màrquez, 2005).",1. Introduction,[0],[0]
"The task is challenging for SPENs because the output is discrete, sparse, and subject to rigid non-local constraints.",1. Introduction,[0],[0]
"We show how to formulate SRL as a SPEN problem and demonstrate performance improvements over strong baselines that use deep features, but sufficiently simple energy functions that the constraints can be enforced using dynamic programming.
",1. Introduction,[0],[0]
"Despite substantial differences between the two applications, learning and prediction for all models is performed using the same gradient-based prediction and end-to-end learning code.",1. Introduction,[0],[0]
This black-box interaction with the model provides many opportunities for further use of SPENs.,1. Introduction,[0],[0]
A SPEN is defined as an instance of Eq.,2. Structured Prediction Energy Networks,[0],[0]
"(1) where the energy is given by a deep neural network that provides a subroutine for efficiently evaluating ddyEx(y) (Belanger & McCallum, 2016).",2. Structured Prediction Energy Networks,[0],[0]
Differentiability necessitates that the energy is defined on continuous inputs.,2. Structured Prediction Energy Networks,[0],[0]
"Going forward, y will always be continuous.",2. Structured Prediction Energy Networks,[0],[0]
"Prediction is performed by gradient-based optimization with respect to y.
This section first motivates the SPENs employed in this paper, by contrasting them with alternative energy-based approaches to structured prediction.",2. Structured Prediction Energy Networks,[0],[0]
"Then, we present two families of methods for training energy-based structured prediction models that have been explored in prior work.",2. Structured Prediction Energy Networks,[0],[0]
The definition of SPENs above is extremely general and includes many existing modeling techniques.,2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"However, both this paper and Belanger & McCallum (2016) depart from most prior work by employing monolithic energy functions that only provide forward and back-propagation.
",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"This contrasts with the two principal families of energybased models in the literature, where the tractability of (approximate) energy minimization depends crucially on the factorization structure of the energy.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"First, factor graphs decompose the energy into a sum of functions defined over small sets of subcomponents of y (Kschischang et al., 2001).",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"This structure provides opportunities for energy minimization using message passing, MCMC, or combinatorial solvers.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"Second, autoregressive models, such as recurrent neural networks (RNNs) assume an ordering on the components of y such that the energy for component yi only depends on its predecessors.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
Approximate energy minimization can be performed using search in the space of prefixes of y using beam search or greedy search.,2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"See, for example, Sutskever et al. (2014).
",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"By not relying on any such factorization when choosing learning and prediction algorithms for SPENs, we can consider much broader families of deep energy functions.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"We do not specify the interaction structure in advance, but instead learn it automatically by fitting a deep network.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
This can capture sophisticated global interactions among components of y that are difficult to represent using a factorized energy.,2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"Of course, the downside of such SPENs is that they provide few guarantees, particularly when employing nonconvex energies.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"Furthermore, for problems with hard constraints on outputs, the ability to do effective constrained optimization may have depended crucially on certain factorization structure.",2.1. Black-Box vs. Factorized Energy Functions,[0],[0]
"One method for estimating the parameters of an energybased model E
x (y) is to maximize the conditional likelihood of y:
P(y|x) /",2.2. Learning as Conditional Density Estimation,[0],[0]
exp ( E x (y)) .,2.2. Learning as Conditional Density Estimation,[0],[0]
"(2)
Unfortunately, computing the likelihood requires the distribution’s normalizing constant, which is intractable for black-box energies with no available factorization structure.",2.2. Learning as Conditional Density Estimation,[0],[0]
"In contrastive backprop, this is circumvented by performing contrastive divergence training, with Hamiltonian
Monte Carlo sampling from the energy surface (Mnih & Hinton, 2005; Hinton et al., 2006; Ngiam et al., 2011).",2.2. Learning as Conditional Density Estimation,[0],[0]
"Recently, Zhai et al. (2016) trained energy-based density models for anomaly detection by exploiting the connections between denosing autoencoders, energy-based models, and score matching (Vincent, 2011).",2.2. Learning as Conditional Density Estimation,[0],[0]
"Let (ˆy,y⇤) be a non-negative task-specific cost function for comparing ˆy and the ground truth y⇤.",2.3. Learning with Exact Energy Minimization,[0],[0]
"Belanger & McCallum (2016) employ a structured SVM (SSVM) loss (Taskar et al., 2004; Tsochantaridis et al., 2004):
X
{xi,yi}
max
y
[ (y,yi) Exi(y)",2.3. Learning with Exact Energy Minimization,[0],[0]
+,2.3. Learning with Exact Energy Minimization,[0],[0]
"Exi(yi)]+ , (3)
where [·]+ = max(0, ·).",2.3. Learning with Exact Energy Minimization,[0],[0]
Each step of minimizing Eq.,2.3. Learning with Exact Energy Minimization,[0],[0]
"(3) by subgradient descent requires loss-augmented inference:
min
y
( (y,yi) + Exi(y)) .",2.3. Learning with Exact Energy Minimization,[0],[0]
"(4)
For differentiable (y,yi), a local optimum of Eq. (4) can obtained using first-order methods.
",2.3. Learning with Exact Energy Minimization,[0],[0]
Solving Eq.,2.3. Learning with Exact Energy Minimization,[0],[0]
(4) probes the model for margin violations.,2.3. Learning with Exact Energy Minimization,[0],[0]
"If none exist, the gradient of the loss with respect to the parameters is zero.",2.3. Learning with Exact Energy Minimization,[0],[0]
"Therefore, SSVM performance does not degrade gracefully with optimization errors in the inner prediction problem, since inexact energy minimization may fail to discover margin violations that exist.",2.3. Learning with Exact Energy Minimization,[0],[0]
Performance can be recovered if Eq.,2.3. Learning with Exact Energy Minimization,[0],[0]
"(4) returns a lower bound, eg. by solving an LP relaxation (Finley & Joachims, 2008).",2.3. Learning with Exact Energy Minimization,[0],[0]
"However, this is not possible in general.",2.3. Learning with Exact Energy Minimization,[0],[0]
In Sec. 6.1.3 we compare the image denoising performance of SSVM learning vs. this paper’s end-to-end method.,2.3. Learning with Exact Energy Minimization,[0],[0]
"Overall, we have found SSVM learning to be unstable and difficult to tune for non-convex energies in applications more complex than the multi-label classification experiments of Belanger & McCallum (2016).
",2.3. Learning with Exact Energy Minimization,[0],[0]
"The implicit function theorem offers an alternative framework for training energy-based predictors (Foo et al., 2008; Samuel & Tappen, 2009).",2.3. Learning with Exact Energy Minimization,[0],[0]
See Domke (2012) for an overview.,2.3. Learning with Exact Energy Minimization,[0],[0]
"While a naive implementation requires inverting Hessians, one can solve the product of an inverse Hessian and a vector using conjugate gradients, which can leverage the techniques discussed in Sec. 3 as a subroutine.",2.3. Learning with Exact Energy Minimization,[0],[0]
"To perform reliably, the method unfortunately requires exact energy minimization and many conjugate gradient iterations.
",2.3. Learning with Exact Energy Minimization,[0],[0]
"Overall, both of these learning algorithms only update the energy function in the neighborhoods of the ground truth and the predictions of the current model.",2.3. Learning with Exact Energy Minimization,[0],[0]
"On the other hand, it may be advantageous to shape the entire energy surface such that is exhibits certain properties, e.g., gradient descent converges quickly when initialized well (Sec. 4.2).
",2.3. Learning with Exact Energy Minimization,[0],[0]
"Therefore, these methods may be undesirable even for problems where exact energy minimization is tractable.
",2.3. Learning with Exact Energy Minimization,[0],[0]
"For non-convex E x (y), gradient-based prediction will only find a local optimum.",2.3. Learning with Exact Energy Minimization,[0],[0]
"Amos et al. (2017) present inputconvex neural networks (ICNNs), which employ an easyto-implement method for constraining the parameters of a SPEN such that the energy is convex with respect to y, but perhaps non-convex with respect to the parameters.",2.3. Learning with Exact Energy Minimization,[0],[0]
"One simply uses convex, non-decreasing non-linearities and only non-negative parameters in any part of the computation graph downstream from y. Here, prediction will return the global optimum, but convexity, especially when achieved this way, may impose a strong restriction on the expressivity of the energy.",2.3. Learning with Exact Energy Minimization,[0],[0]
"Their construction is a sufficient condition for achieving convexity, but there are convex energies that disobey this property.",2.3. Learning with Exact Energy Minimization,[0],[0]
Our experiments present results for instances of ICNNs.,2.3. Learning with Exact Energy Minimization,[0],[0]
"In general, nonconvex SPENS perform better.",2.3. Learning with Exact Energy Minimization,[0],[0]
The methods of Sec. 2.3 are unreliable with non-convex energies because we cannot simply use the output of inexact energy minimization as a drop-in replacement for the exact minimizer.,3. Learning with Unrolled Optimization,[0],[0]
"Instead, a collection of prior work has performed end-to-end learning of gradient-based predictors (Gregor & LeCun, 2010; Domke, 2012; Maclaurin et al., 2015; Andrychowicz et al., 2016; Wang et al., 2016; Metz et al., 2017; Greff et al., 2017).",3. Learning with Unrolled Optimization,[0],[0]
"Rather than reasoning about the energy minimum as an abstract quantity, the authors pose a specific gradient-based algorithm for approximate energy minimization and optimize its empirical performance using back-propagation.",3. Learning with Unrolled Optimization,[0],[0]
"This is a form of direct risk minimization (Tappen et al., 2007; Stoyanov et al., 2011; Domke, 2013).
",3. Learning with Unrolled Optimization,[0],[0]
"Consider simple gradient descent:
yT = y0 TX
t=1
⌘t d
dy E x (yt).",3. Learning with Unrolled Optimization,[0],[0]
"(5)
To learn the energy function end-to-end, we can backpropagate through the unrolled optimization Eq.",3. Learning with Unrolled Optimization,[0],[0]
(5) for fixed T .,3. Learning with Unrolled Optimization,[0],[0]
"With this, it can be rendered API-equivalent to a feed-forward network that takes x as input and returns a prediction for y, and can thus be trained using standard methods.",3. Learning with Unrolled Optimization,[0],[0]
"Furthermore, certain hyperparameters, such as the learning rates ⌘t, are trainable (Domke, 2012).
",3. Learning with Unrolled Optimization,[0],[0]
"This backpropagation requires non-standard interaction with a neural-network library because Eq. (5) computes gradients in the forward pass, and thus it must compute second order terms in the backwards pass.",3. Learning with Unrolled Optimization,[0],[0]
"We can save space and computation by avoiding instantiating Hessian terms and instead directly computing Hessian-vector prod-
ucts.",3. Learning with Unrolled Optimization,[0],[0]
These can be achieved three ways.,3. Learning with Unrolled Optimization,[0],[0]
"First, the method of Pearlmutter (1994) is exact, but requires non-trivial code modifications.",3. Learning with Unrolled Optimization,[0],[0]
"Second, some libraries construct computation graphs for gradients that are themselves differentiable.",3. Learning with Unrolled Optimization,[0],[0]
"Third, we can employ finite-differences (Domke, 2012).
",3. Learning with Unrolled Optimization,[0],[0]
"It is clear that Eq. (5) can be naturally extended to certain alternative optimization methods, such as gradient descent with momentum, or L-BFGS (Liu & Nocedal, 1989; Domke, 2012).",3. Learning with Unrolled Optimization,[0],[0]
These require an additional state vector ht that is evolved along with yt across iterations.,3. Learning with Unrolled Optimization,[0],[0]
"Andrychowicz et al. (2016) unroll gradient-descent, but employ a learned non-linear RNN to perform per-coordinate updates to y. End-to-end learning is also applicable to special-case energy minimization algorithms for graphical models, such as mean-field inference and belief propagation (Domke, 2013; Chen et al., 2015; Tompson et al., 2014; Li & Zemel, 2014; Hershey et al., 2014; Zheng et al., 2015).",3. Learning with Unrolled Optimization,[0],[0]
We now present details for applying the methods of the previous section to SPENs.,4. End-to-End Learning for SPENs,[0],[0]
We first describe considerations for learning SPENs defined for the convex relaxation of discrete labeling problems.,4. End-to-End Learning for SPENs,[0],[0]
"Then, we describe how to encourage our models to optimize quickly in practice.",4. End-to-End Learning for SPENs,[0],[0]
"Finally, we present methods for improving the speed and memory overhead of SPEN implementations.
",4. End-to-End Learning for SPENs,[0],[0]
Our experiments unroll either Eq.,4. End-to-End Learning for SPENs,[0],[0]
(5) or an analogous version implementing gradient descent with momentum.,4. End-to-End Learning for SPENs,[0],[0]
"We compute Hessian-vector products using the finitedifference method of (Domke, 2012), which allows blackbox interaction with the energy.
",4. End-to-End Learning for SPENs,[0],[0]
"We avoid the RNN-based approach of Andrychowicz et al. (2016) because it diminishes the semantics of the energy, as the interaction between the optimizer and gradients of the energy is complicated.",4. End-to-End Learning for SPENs,[0],[0]
"In recent work, Gygli et al. (2017) propose an alternative learning method that fits the energy function such that E
x (·) ⇡ (·,y⇤), where is defined as in Sec. 2.3.",4. End-to-End Learning for SPENs,[0],[0]
"This is an interesting direction for future research, as it allows for non-differentiable .",4. End-to-End Learning for SPENs,[0],[0]
"The advantage of end-to-end learning, however, is that it provides a energy function that is precisely tuned for a particular testtime energy minimization procedure.",4. End-to-End Learning for SPENs,[0],[0]
"To apply SPENs to a discrete structured prediction problem, we relax to a constrained continuous problem, apply SPEN prediction, and then round to a discrete output.",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"For example, for tagging each pixel of a h ⇥ w image with a binary label, we would relax from {0, 1}w⇥h to [0, 1]w⇥h, and if the pixels can take on one of D values, we would relax from y 2 {0, . . .",4.1. End-to-End Learning for Discrete Problems,[0],[0]
", D}w⇥h to w⇥hD , where D is the
probability simplex on D elements.
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"While this rounding introduces poorly-understood sources of error, it has worked well for non-convex energy-based prediction in multi-label classification (Belanger & McCallum, 2016), sequence tagging (Vilnis et al., 2015), and translation (Hoang et al., 2017).
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Both [0, 1]w⇥h and w⇥hD are Cartesian products of probability simplices, and it is easy to adopt existing methods for projected gradient optimization over the simplex.
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"First, it is natural to apply Euclidean projected gradient descent.",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Over [0, 1]w⇥h, we have:
yt+1 =",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Clip0,1",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"[yt ⌘trEx(yt)] , (6)
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"This is unusable for end-to-end learning, however, since back-propagation through the projection will yield 0 gradients whenever yt ⌘trEx(yt) /2",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"[0, 1].",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"This is similarly problematic for projection onto w⇥hD (Duchi et al., 2008).
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Alternatively, we can apply entropic mirror descent, ie. projected gradient with distance measured by KL divergence (Beck & Teboulle, 2003).",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"For y 2 w⇥hD , we have:
yt+1 = SoftMax (log(yt) ⌘trEx(yt))",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"(7)
This is suitable for end-to-end learning, but the updates are similar to an RNN with sigmoid non-linearities, which is vulnerable to vanishing gradients (Bengio et al., 1994).
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Instead, we have found it useful to avoid constrained optimization entirely, by optimizing un-normalized logits lt, with yt = SoftMax(lt):
lt+1 = lt ⌘trEx (SoftMax(lt)) .",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"(8)
Here, the updates to lt are additive, and thus will be less susceptible to vanishing gradients (Hochreiter & Schmidhuber, 1997; Srivastava et al., 2015; He et al., 2016).
",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Finally, Amos et al. (2017) present the bundle entropy method for convex optimization with simplex constraints, along with a method for differentiating the output of the optimizer.",4.1. End-to-End Learning for Discrete Problems,[0],[0]
End-to-end learning for Eq.,4.1. End-to-End Learning for Discrete Problems,[0],[0]
"(10) can be performed using generic learning software, since the unrolled optimization obeys the API of a feed-forward predictor, but unfortunately this is not true for their method.",4.1. End-to-End Learning for Discrete Problems,[0],[0]
"Future work should consider their method, however, as it performs very rapid energy minimization.",4.1. End-to-End Learning for Discrete Problems,[0],[0]
We next enumerate methods for learning a model such that gradient-based energy minimization converges to highquality y quickly.,4.2. Learning to Optimize Quickly,[0],[0]
"When using such methods, we have found it important to maintain the same optimization configuration, such as T , at both train and test time.
",4.2. Learning to Optimize Quickly,[0],[0]
"First, we can encourage rapid optimization by defining our loss function as a sum of losses on every iterate yt, rather than only on the final one.",4.2. Learning to Optimize Quickly,[0],[0]
"Let `(yt,y⇤) be a differentiable loss between an iterate and the ground truth.",4.2. Learning to Optimize Quickly,[0],[0]
"We employ
L = 1
T
TX
t=1
wt`(yt,y ⇤ ), (9)
where wt is a non-negative weight.",4.2. Learning to Optimize Quickly,[0],[0]
This encourages the model to achieve high-quality predictions early.,4.2. Learning to Optimize Quickly,[0],[0]
"It has the additional benefit that it reduces vanishing gradients, since a learning signal is introduced at every timestep.",4.2. Learning to Optimize Quickly,[0],[0]
"Our experiments use wt = 1T t+1 .
",4.2. Learning to Optimize Quickly,[0],[0]
"Second, for the simplex-constrained problems of Sec. 4.1, we smooth the energy with an entropy term P i H(yi).",4.2. Learning to Optimize Quickly,[0],[0]
"This introduces extra strong convexity, which helps improve convergence.",4.2. Learning to Optimize Quickly,[0],[0]
"It also strengthens the parallel between SPEN prediction and marginal inference in a Markov random field, where the inference objective is expected energy plus entropy (Koller & Friedman, 2009, p. 385).
",4.2. Learning to Optimize Quickly,[0],[0]
"Third, we can set T to a small value.",4.2. Learning to Optimize Quickly,[0],[0]
"Of course, this guarantees that optimization converges quickly on the train data.",4.2. Learning to Optimize Quickly,[0],[0]
"Here, we lose the contract that Eq. (10) is even performing energy minimization, since it hasn’t converged, but this may be acceptable if predictions are accurate.",4.2. Learning to Optimize Quickly,[0],[0]
"For example, some experiments achieve good performance with T = 3.
",4.2. Learning to Optimize Quickly,[0],[0]
"In future work, it may be fruitful to directly penalize convergence criteria, such as kyt yt 1k and k ddytEx(yt)k.",4.2. Learning to Optimize Quickly,[0],[0]
"Since we can explicitly encourage our model to converge quickly, it is important to exploit fast convergence at train time.",4.3. Efficient Implementation,[0],[0]
Eq. (10) is unrolled for a fixed T .,4.3. Efficient Implementation,[0],[0]
"However, if optimization converges at T0 < T , it suffices to start backpropagation at T0, since the updates to yt for t > T0 are the identity.",4.3. Efficient Implementation,[0],[0]
"Therefore, we unroll for a fixed number of iterations T , but iterate only until convergence is detected.
",4.3. Efficient Implementation,[0],[0]
"To support back-propagation, a naive implementation of Eq.",4.3. Efficient Implementation,[0],[0]
(10) would require T clones of the energy (with tied parameters).,4.3. Efficient Implementation,[0],[0]
"We reduce memory overhead by checkpointing the inputs and outputs of the energy, but discarding its internal state.",4.3. Efficient Implementation,[0],[0]
"This allows us to use a single copy of the energy, but requires recomputing forward evaluations at specific yt during the backwards pass.",4.3. Efficient Implementation,[0],[0]
"To save additional memory, we could have reconstructed the yt on-the-fly either by reversing the dynamics of the energy minimization method (Domke, 2013; Maclaurin et al., 2015) or by performing a small amount of extra forward-propagation (Geoffrey & Padmanabhan, 2000; Lewis, 2003).",4.3. Efficient Implementation,[0],[0]
"To train SPENs end-to-end, we write Eq.",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"(5) as:
yT = Init(F",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
(x)),5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"TX
t=1
⌘t d
dy E(yt ; F (x)).",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"(10)
Here, Init(·) is a differentiable procedure for predicting an initial iterate y0.",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"Following Belanger & McCallum (2016), we also employ E
x (y) = E(y ; F (x)), where the dependence of E
x (y) on x comes by way of a parametrized feature function F (x).",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"This is useful because test-time prediction can avoid back-propagation in F (x).
",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"We have found it useful in practice to employ an energy that splits into global and local terms:
E(y ;F (x))",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
=,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
Eg(y ;F (x)),5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
+,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"X
i
Eli(yi ;F (x)).",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"(11)
Here, i indexes the components of y and Eg(y ;F (x)) is an arbitrary global energy function.",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
The modeling benefits of the local terms are similar to the benefits of using local factors in popular factor graph models.,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"We also can use the local terms to provide an implementation of Init(·).
",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
We pretrain F (x) by training the feed-forward predictor Init(F (x)).,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"We also stabilize learning by first clamping the local terms for a few epochs while updating Eg(y ;F (x)).
",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
To back-propagate through Eq.,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"(10), the energy function must be at least twice differentiable with respect to y. Therefore, we can’t use non-linearities with discontinuous gradients.",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
"Instead of ReLUs, we use a SoftPlus with a reasonably high temperature.",5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
Note that F (x) and Init(·) can be arbitrary networks that are sub-differentiable with respect to their parameters.,5. Recommended SPEN Architectures for End-to-End Learning,[0],[0]
We evaluate SPENs on image denoising and semantic role labeling (SRL) tasks.,6. Experiments,[0],[0]
"Image denoising is an important benchmark for SPENs, since the task appears in many prior works employing end-to-end learning.",6. Experiments,[0],[0]
"SRL is useful for evaluating SPENs’ suitability for challenging combinatorial problems, since the outputs are subject to rigid, nonlocal constraints.",6. Experiments,[0],[0]
"For both, we provide controlled experiments that isolate the impact of various SPEN design decisions, such as the optimization method that is unrolled and the expressivity of the energy function.
",6. Experiments,[0],[0]
"In these applications, we employ specific architectures based on our prior knowledge about the problem domain.",6. Experiments,[0],[0]
This capability is crucial for introducing the necessary inductive bias to fbe able to fit SPENs on limited datasets.,6. Experiments,[0],[0]
"Overall, black-box prediction and learning methods for
SPENs are useful because we can select architectures based on their suitability for the data, not whether they support model-specific algorithms.",6. Experiments,[0],[0]
"Let x 2 [0, 1]w⇥h be an observed grayscale image.",6.1. Image Denoising,[0],[0]
We assume that it is a noisy realization of a latent clean image y 2,6.1. Image Denoising,[0],[0]
"[0, 1]w⇥h, which we estimate using MAP inference.",6.1. Image Denoising,[0],[0]
Consider a Gaussian noise model with variance 2 and a prior P(y).,6.1. Image Denoising,[0],[0]
"The associated energy function is:
ky xk22 2 2 logP(y).",6.1. Image Denoising,[0],[0]
"(12)
Here, the feature network is the identity.",6.1. Image Denoising,[0],[0]
"The first term is the local energy network and the second, which does not depend on x, is the global energy network.
",6.1. Image Denoising,[0],[0]
There are three general families for the prior.,6.1. Image Denoising,[0],[0]
"First, it can be hard-coded.",6.1. Image Denoising,[0],[0]
"Second, it can be learned by approximate density estimation.",6.1. Image Denoising,[0],[0]
"Third, given a collection of {x,y} pairs, we can perform supervised learning, where the prior’s parameters are discriminatively trained such that the output of a particular algorithm for minimizing Eq.",6.1. Image Denoising,[0],[0]
(12) is highquality.,6.1. Image Denoising,[0],[0]
"End-to-end learning has proven to be highly successful for the third approach (Tappen et al., 2007; Barbu, 2009; Schmidt et al., 2010; Sun & Tappen, 2011; Domke, 2012; Wang et al., 2016), and thus it is important to evaluate the methods of this paper on the task.",6.1. Image Denoising,[0],[0]
"Much of the existing work on end-to-end training for denoising considers some form of a field-of-experts (FOE) prior (Roth & Black, 2005).",6.1.1. IMAGE PRIORS,[0],[0]
"We consider an `1 version, which assigns high probability to images with sparse activations from K learned filters:
P(y) / exp X
k
k(fk ⇤ y)k1 ! .",6.1.1. IMAGE PRIORS,[0],[0]
"(13)
Wang et al. (2016) perform end-to-end learning for Eq.",6.1.1. IMAGE PRIORS,[0],[0]
"(13), by unrolling proximal gradient methods that analytically handle the non-differentiable `1 term.
",6.1.1. IMAGE PRIORS,[0],[0]
This paper assumes we only have black-box interaction with the energy.,6.1.1. IMAGE PRIORS,[0],[0]
"In response, we alter Eq.",6.1.1. IMAGE PRIORS,[0],[0]
"(13) such that it is twice differentiable, so that we can unroll generic firstorder optimization methods.",6.1.1. IMAGE PRIORS,[0],[0]
We approximate Eq.,6.1.1. IMAGE PRIORS,[0],[0]
"(13) by leveraging a SoftPlus with temperature 25, replacing |·| by:
SoftAbs(y) = 0.5 SoftPlus(y)+0.5 SoftPlus( y).",6.1.1. IMAGE PRIORS,[0],[0]
"(14)
The principal advantage of learning algorithms that are not hand-crafted to the problem structure is that they provide the opportunity to employ more expressive energies.",6.1.1. IMAGE PRIORS,[0],[0]
"In response, we also consider a deeper prior, given by:
P(y) / exp ( DNN(y)) .",6.1.1. IMAGE PRIORS,[0],[0]
"(15)
Here, DNN(y) is a general deep convolutional network that takes an image and returns a number.",6.1.1. IMAGE PRIORS,[0],[0]
"The architecture in our experiments consists of a 7 ⇥ 7 ⇥ 32 convolution, a SoftPlus, another 7 ⇥ 7 ⇥ 32 convolution, a SoftPlus, a 1⇥ 1⇥ 1 convolution, and finally spatial average pooling.",6.1.1. IMAGE PRIORS,[0],[0]
The method of Wang et al. (2016) cannot handle this prior.,6.1.1. IMAGE PRIORS,[0],[0]
"We evaluate on the 7-Scenes dataset (Newcombe et al., 2011), where we seek to denoise depth measurements from a Kinect sensor.",6.1.2. EXPERIMENTAL SETUP,[0],[0]
"Our data processing and hyperparameters are designed to replicate the setup of Wang et al. (2016), who demonstrate state-of-the art results for energyminimization-based denoising on the dataset.",6.1.2. EXPERIMENTAL SETUP,[0],[0]
We train using random 96 ⇥,6.1.2. EXPERIMENTAL SETUP,[0],[0]
128 crops from 200 images of the same scene and report PSNR (higher is better) for 5500 images from different scenes.,6.1.2. EXPERIMENTAL SETUP,[0],[0]
We treat 2 as a trainable parameter and minimize the mean-squared-error of y.,6.1.2. EXPERIMENTAL SETUP,[0],[0]
Example outputs are given in Figure 1 and Table 1 compares PSNR.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"BM3D is a widely-used non-parametric method (Dabov et al., 2007).",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"FilterForest (FF) adaptively selects denoising filters for each location (Fanello et al., 2014).",6.1.3. RESULTS AND DISCUSSION,[0],[0]
ProximalNet (PN) is the system of Wang et al. (2016).,6.1.3. RESULTS AND DISCUSSION,[0],[0]
FOE-20 is an attempt to replicate PN using end-toend SPEN learning.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
We unroll 20 steps of gradient descent with momentum 0.75 and use the modification in Eq.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
(14).,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Note it performs similarly to PN, which unrolls 5 iterations of sophisticated optimization.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Note that we can obtain 37.0 PSNR using a feed-forward convnet with a similar architecture to our DeepPrior, but without spatial pooling.
",6.1.3. RESULTS AND DISCUSSION,[0],[0]
The next set of results consider improved instances of the FOE model.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"First, FOE-20+ is identical to FOE-20, except that it employs the average loss Eq.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"(9), uses a momentum constant of 0.25, and treats the learning rates ⌘t as trainable parameters.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
We find that this results in both better performance and faster convergence.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Of course, we could achieve fast convergence by simply setting T to be small.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"In response, we consider FOE-3.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"This only unrolls for T = 3 iterations and obtains superior performance.
",6.1.3. RESULTS AND DISCUSSION,[0],[0]
The final three results are with the DNN prior Eq.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
(15).,6.1.3. RESULTS AND DISCUSSION,[0],[0]
DP20 unrolls 20 steps of gradient descent with a momentum constant of 0.25.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"The gain in performance is substantial, especially considering that a PSNR of 30 can be obtained with elementary signal processing.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Similar to FOE-3 vs. FOE-20+, we experience a modest performance gain using DP-3, which only unrolls for 3 gradient steps but is otherwise identical.
",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Finally, the FOE-SSVM and DP-SSVM configurations use SSVM training.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"We find that FOE-SSVM performs
competitively with the other FOE configurations.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"This is not surprising, since the FOE prior is convex.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"However, fitting the DeepPrior with an SSVM is inferior to using endto-end learning.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"The performance is very sensitive to the energy minimization hyperparameters.
",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"In these experiments, it is superior to only unroll for a few iterations for end-to-end learning.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
One possible reason is that a shallow unrolled architecture is easier to train.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Truncated optimization with respect to y may also provide an interesting prior over outputs (Duvenaud et al., 2016).",6.1.3. RESULTS AND DISCUSSION,[0],[0]
It is also observed in Wang et al. (2014) that better energy minimization for FOE models may not improve PSNR.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Often unrolling for 20 iterations results in over-smoothed outputs.
",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"We are unable achieve reasonable performance with an ICNN (Amos et al., 2017), which restricts all of the parameters of the convolutions to be positive.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Unfortunately, this hinders the ability of the filters in the prior to act as edge detectors or encourage local smoothness.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
Both of these are important for high-quality denoising.,6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Note that the `1 FOE is convex, even without the restrictive ICNN constraint.",6.1.3. RESULTS AND DISCUSSION,[0],[0]
"Semantic role labeling (SRL) predicts the semantic structure of predicates and arguments in sentences (Gildea & Jurafsky, 2002).",6.2. Semantic Role Labeling,[0],[0]
"For example, in the sentence “I want to buy a car,” the verbs “want” and “buy” are two predicates, and “I” is an argument that refers to the wanter and buyer, “to buy a car” is the thing wanted, and “a car” is the thing bought.",6.2. Semantic Role Labeling,[0],[0]
"Given predicates, we seek to identify arguments and their semantic roles in relation to each predicate.",6.2. Semantic Role Labeling,[0],[0]
"Formally, given a set of predicates p in a sentence x and a set of candidate argument spans a, we assign a discrete semantic role r to each pair of predicate and argument, where r can be either a pre-defined role label or an empty label.",6.2. Semantic Role Labeling,[0],[0]
"We evaluate SRL instead of, for example, noun-phrase chunking (Lacoste-Julien et al., 2012), since it is a more challenging task, where the outputs are subject to substantially more complex non-local constraints.
",6.2. Semantic Role Labeling,[0],[0]
"Existing work imposes hard constraints on r, such as excluding overlapping arguments and repeated core roles during prediction.",6.2. Semantic Role Labeling,[0],[0]
"The objective is to minimize the energy:
min
r
E(r ;x,p,a) s.t. r 2 Q(x,p,a), (16)
where Q(x,p,a) is set of feasible joint role assignments.",6.2. Semantic Role Labeling,[0],[0]
"This constrained optimization problem can be solved using integer linear programming (ILP) (Punyakanok et al., 2008) or its relaxations (Das et al., 2012).",6.2. Semantic Role Labeling,[0],[0]
These methods rely on the output of local classifiers that are unaware of structural constraints during training.,6.2. Semantic Role Labeling,[0],[0]
"More recently, Täckström",6.2. Semantic Role Labeling,[0],[0]
et al. (2015) account for the constraint structure using dynamic programming at train time.,6.2. Semantic Role Labeling,[0],[0]
FitzGerald et al. (2015) extend this using neural network features and show improved results.,6.2. Semantic Role Labeling,[0],[0]
"We consider the CoNLL 2005 shared task data (Carreras & Màrquez, 2005), with standard data splits and official evaluation scripts.",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
We apply similar preprocessing as Täckström et al. (2015).,6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"This includes part-of-speech tagging, dependency parsing, and using the parse to generate candidate arguments.
",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"Our baseline is an arc-factored model for the conditional probability of the predicate-argument arc labels:
P(r|x,p,a) = ⇧iP(ri|x,p,a).",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"(17)
where P(ri|x,p,a) / exp g(ri,x,p,a) .",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"Here, each conditional distribution is given by a multiclass logistic regression model.",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"See Appendix A.2.1 for details of the architecture and training procedure for our baseline.
",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
When using the negative log of Eq.,6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"(18) as an energy in Eq. (16), there are variety of methods for finding a nearoptimal r 2 Q(x,p,a).",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"First, we can employ simple
heuristics for locally resolving constraint violation.",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
The Local + H system uses Eq.,6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
(18) and these.,6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"We can instead use the AD3 message passing algorithm (Martins et al., 2011) to solve the LP relaxation of this constrained problem.",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
We use Local + AD3 to refer to this system.,6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
"Since the LP solution may not be integral, we post-process the AD3 output using the same heuristics as Local + H.",6.2.1. DATA AND PREPROCESSING AND BASELINES,[0],[0]
The SPEN performs continuous optimization over the relaxed set yi 2,6.2.2. SPEN MODEL,[0],[0]
"A for each discrete label ri, where A is the number of possible roles.",6.2.2. SPEN MODEL,[0],[0]
"The preprocessing generates sparse predicate-argument candidates, but we optimize over the complete bipartite graph between predicates and arguments to support vectorization.",6.2.2. SPEN MODEL,[0],[0]
"We have y 2 n⇥mA , where n and m are the max number of predicates and arguments.",6.2.2. SPEN MODEL,[0],[0]
"Invalid arcs are constrained to the empty label.
",6.2.2. SPEN MODEL,[0],[0]
We employ a pretrained version of Eq.,6.2.2. SPEN MODEL,[0],[0]
(18) to provide the local energy term of a SPEN.,6.2.2. SPEN MODEL,[0],[0]
This is augmented with global terms that couple the outputs together.,6.2.2. SPEN MODEL,[0],[0]
See Appendix A.2.2 for details of the architecture we use.,6.2.2. SPEN MODEL,[0],[0]
"It has terms, for example, that apply a deep network to the feature representations of all of the arcs selected for a given predicate.
",6.2.2. SPEN MODEL,[0],[0]
"As with Täckström et al. (2015), we seek to account for constraints Q(x,p,a) during both inference and learning, rather than only imposing them via post-processing.",6.2.2. SPEN MODEL,[0],[0]
"Therefore, we include additional energy terms that encode membership in Q(x,p,a) as twice-differentiable soft constraints that can be applied to y. All of the constraints in Q(x,p,a) express that certain arcs cannot co-occur.",6.2.2. SPEN MODEL,[0],[0]
"For example, two arguments cannot attach to the same predicate if the arguments correspond to spans of tokens that overlap.",6.2.2. SPEN MODEL,[0],[0]
"Consider general binary variables a and b with corresponding relaxations ā,¯b 2 [0, 1].",6.2.2. SPEN MODEL,[0],[0]
"We convert the constraint ¬(a^b) into an energy function ↵SoftPlus(ā+¯b 1), where ↵ is a learned parameter.
",6.2.2. SPEN MODEL,[0],[0]
"We consider the SPEN + H and SPEN + AD3 configurations, which employ heuristics or AD3 to enforce the output constraints.",6.2.2. SPEN MODEL,[0],[0]
Rather than applying these methods to the probabilities from Eq.,6.2.2. SPEN MODEL,[0],[0]
"(18), we use the soft prediction output by energy minimization.",6.2.2. SPEN MODEL,[0],[0]
Table 2 contains results on the CoNLL 2005 WSJ dev and test sets and the Brown test set.,6.2.3. RESULTS AND DISCUSSION,[0],[0]
"We compare the SPEN and Local systems with the best non-ensemble systems of Täckström et al. (2015) and FitzGerald et al. (2015), which have similar overall setups as us for feature extraction and for the parametrization of the local energy terms.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"For these, ‘Local’ fits Eq.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"(18) without regard for the output constraints, whereas ‘Structured’ explicitly considers
them during training.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
Note that Zhou & Xu (2015) obtain slightly better performance with alternative RNN methods.,6.2.3. RESULTS AND DISCUSSION,[0],[0]
"We were unable to outputerform the Local systems using a SPEN system trained with an SSVM loss.
",6.2.3. RESULTS AND DISCUSSION,[0],[0]
We select our SPEN configuration by maximizing performance of SPEN + AD3 on the dev data.,6.2.3. RESULTS AND DISCUSSION,[0],[0]
"Our best system unrolls for 10 iterations, trains per-iteration learning rates, uses no momentum, and unrolls Eq.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
(8).,6.2.3. RESULTS AND DISCUSSION,[0],[0]
"Overall, SPEN + AD3 performs the best of all systems on the WSJ test data.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
We expect our diminished performance on the Brown test set is due to overfitting.,6.2.3. RESULTS AND DISCUSSION,[0],[0]
"The Brown set is not from the same source as the train, dev, and test WSJ data.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"SPENs are more susceptible to overfitting because the expressive global term introduces many parameters.
",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"Note that SPEN + AD3 and SPEN + H performs identically, whereas LOCAL + AD3 and LOCAL + H do not.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"This is because our learned global energy encourages constraint satisfaction during gradient-based optimization of y. Using the method of Amos et al. (2017) for restricting the energy to be convex wrt y, we obtain 80.3 on the test set.",6.2.3. RESULTS AND DISCUSSION,[0],[0]
"SPENs are a flexible, expressive framework for structured prediction, but training them can be challenging.",7. Conclusion and Future Work,[0],[0]
This paper provides a new end-to-end training method that enables high performance on considerably more complex tasks than those of Belanger & McCallum (2016).,7. Conclusion and Future Work,[0],[0]
We unroll an approximate energy minimization algorithm into a differentiable computation graph that is trainable by gradient descent.,7. Conclusion and Future Work,[0],[0]
"The approach is user-friendly in practice because it returns not just an energy function but also a test-time prediction procedure that has been tailored for it.
",7. Conclusion and Future Work,[0],[0]
"In the future, it may be useful to employ more sophisticated unrolled optimizers, perhaps where the optimizer’s hyperparameters are a learned function of x, and to perform iterative optimization in a learned feature space, rather than output space.",7. Conclusion and Future Work,[0],[0]
"Finally, we could model gradient-based prediction as a sequential decision making problem and train the energy using value-based reinforcement learning.",7. Conclusion and Future Work,[0],[0]
"Many thanks to Justin Domke, Tim Vieiria, Luke Vilnis, and Shenlong Wang for helpful discussions.",Acknowledgments,[0],[0]
The first and third authors were supported in part by the Center for Intelligent Information Retrieval and in part by DARPA under agreement number FA8750-13-2-0020.,Acknowledgments,[0],[0]
The second author was supported in part by DARPA under contract number FA8750-13-2-0005.,Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.,Acknowledgments,[0],[0]
"Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect those of the sponsor.",Acknowledgments,[0],[0]
"Structured Prediction Energy Networks (SPENs) are a simple, yet expressive family of structured prediction models (Belanger & McCallum, 2016).",abstractText,[0],[0]
"An energy function over candidate structured outputs is given by a deep network, and predictions are formed by gradient-based optimization.",abstractText,[0],[0]
"This paper presents end-to-end learning for SPENs, where the energy function is discriminatively trained by back-propagating through gradient-based prediction.",abstractText,[0],[0]
"In our experience, the approach is substantially more accurate than the structured SVM method of Belanger & McCallum (2016), as it allows us to use more sophisticated non-convex energies.",abstractText,[0],[0]
"We provide a collection of techniques for improving the speed, accuracy, and memory requirements of end-to-end SPENs, and demonstrate the power of our method on 7-Scenes image denoising and CoNLL-2005 semantic role labeling tasks.",abstractText,[0],[0]
"In both, inexact minimization of non-convex SPEN energies is superior to baseline methods that use simplistic energy functions that can be minimized exactly.",abstractText,[0],[0]
End-to-End Learning for Structured Prediction Energy Networks,title,[0],[0]
Understanding multi-entity interactions is a central question in many real-world applications.,1. Introduction,[0],[0]
"For example, in computational sustainability (Gomes, 2009; MacKenzie et al., 2004), it is important to understand the spatial distribution of species and how species interact with each other and their environment, for developing conservation plans.",1. Introduction,[0],[0]
"In computer vision, the detections of multiple objects are often
1Computer Science Department, Cornell University, Ithaca, NY, US 14850.",1. Introduction,[0],[0]
"Correspondence to: Di Chen <di@cs.cornell.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
correlated because of the shared background and scenario (Wang et al., 2016).",1. Introduction,[0],[0]
"In natural language processing, a text often has several correlated labels in terms of its topic, emotion, and semantic meaning (Nam et al., 2014).",1. Introduction,[0],[0]
"The multivariate probit model (MVP) (Ashford & Sowden, 1970) is a popular classic model for studying interactions of multiple entities.",1. Introduction,[0],[0]
"Nevertheless, learning the multivariate probit model is challenging because it involves the integration of a multivariate normal distribution over a constrained space.
",1. Introduction,[0],[0]
"A classic approach for optimizing the MVP model is Bayesian Inference (Chib & Greenberg, 1998; Tabet, 2007), where the posterior distribution is simulated by Markov Chain Monte Carlo (MCMC) methods (Jeliazkov & Hee Lee, 2010) and the maximum likelihood estimates are obtained by a Monte Carlo version of the Expectation Maximization (EM) algorithm.",1. Introduction,[0],[0]
These approaches require the simulation of observations from a multivariate truncated normal distribution involving an arbitrary covariance matrix.,1. Introduction,[0],[0]
"Although observations from a multivariate truncated normal distribution can be sampled from a sequence of univariate truncated normal distributions (Genz, 1992), the computational effort is rather heavy for high dimensional problems.",1. Introduction,[0],[0]
"Extensions of the classic MVP in specific domains have been proposed under specific assumptions of the covariance matrix (see e.g, (Song & Lee, 2005; Young et al., 2009)).",1. Introduction,[0],[0]
"Recent approaches for computing the maximum likelihood of MVP have been proposed using the first-order gradients and the second-order information (Chen et al., 2017; Mandt et al., 2017).",1. Introduction,[0],[0]
"Those approaches integrate MCMC methods and the numerical estimation of the multivariate probit (Genz, 1992), which is based on an importance sampling using the truncated normal distribution.
",1. Introduction,[0],[0]
"The accessibility of massive contextual data, as well as the success of deep learning, provide additional opportunities and challenges for boosting MVP.",1. Introduction,[0],[0]
"On the one hand, massive contextual data, such as millions of high-resolution images, create the possibility of improving predictive performance, particularly when integrated with deep neural networks, which are remarkably powerful for extracting high-level features from raw data.",1. Introduction,[0],[0]
"On the other hand, a scalable learning scheme, which integrates well with parallelized infrastructure such as GPUs, is needed to take advantage of various deep neural networks as well as the massive contextual data.",1. Introduction,[0],[0]
"Unfortunately, the classical approaches such as Bayesian
inference or previous gradient-based methods, inevitably contain sequential inferences, such as MCMC simulations, which is typically not easy to implement on GPUs.",1. Introduction,[0],[0]
"A recent approach called Multi-Entity Dependency Learning via Conditional Variational Auto-encoder (MEDL CVAE) (Tang et al., 2017) is compatible with deep neural networks and exploits GPUs, with competitive wall-clock training time, but suffers from two key limitations.",1. Introduction,[0],[0]
"On one hand, MEDL CVAE learns the joint likelihood by optimizing the variational lower bound of the joint likelihood but has no guarantee concerning the gap between the lower bound and the true likelihood.",1. Introduction,[0],[0]
"A second limitation is that the empirical optimization of the variational lower bound of MEDL CVAE suffers from the KL-vanishment problem, which is a known problem in applications based on variational auto-encoder.",1. Introduction,[0],[0]
"As a result, when integrating it with powerful deep neural networks such as Convolutional Neural Networks, the KLterm decreases dramatically to zero, which causes serious overfitting problems that restrict its performance.
",1. Introduction,[0],[0]
"We propose a novel end-to-end learning scheme for the Deep Multivariate Probit Model (DMVP), which is scalable and flexible with various deep neural networks.",1. Introduction,[0],[0]
"Specifically: (1) We introduce the Deep Multivariate Probit Model (DMVP), a deep generalization of classic MVP, in which a flexible deep neural network is embedded to extract the high-level features from the raw data.",1. Introduction,[0],[0]
"(2) We propose an efficient parallel sampling process, which transforms the integration over a high-dimensional constrained space into an expectation over the residual multivariate normal distribution with a variance strictly lower than the rejection sampling, tightly integrates with various deep neural networks, and can be implemented end-to-end on GPUs.",1. Introduction,[0],[0]
(3) We provide both a theoretical and an empirical analysis of the convergence behavior of the sampling process embedded in DMVP.,1. Introduction,[0],[0]
"We provide theoretical convergence guarantees for DMVP as well as a numerical analysis of the convergence behavior based on a tighter bound, which is much closer to the empirical results.",1. Introduction,[0],[0]
Our theoretical bound also sheds light on the trade-offs between performance and convergence.,1. Introduction,[0],[0]
(4) We apply DMVP to three multi-entity modelling problems.,1. Introduction,[0],[0]
"In the first application, we use the crowdsourced eBird dataset combined with the National Land Cover Database for the U.S (NLCD) (Homer et al., 2015) and satellite images to study the interaction among multiple species.",1. Introduction,[0],[0]
"In the second application, we study the deforestation and human encroachment in Amazon rainforest with high-resolution satellite images.",1. Introduction,[0],[0]
"In the last application, we study the associated concepts of real-world web images using the NUS-WIDE-LITE web image dataset collected from Flickr (Chua et al., July 8-10, 2009).",1. Introduction,[0],[0]
"Preview of results: We show that our DMVP (a) trains significantly faster than classic MVP models using the endto-end learning scheme fully implemented on GPUs; (b)
captures correlations among multiple entities in all applications; and (c) outperforms the approaches that assume the independence among entities conditioned on contextual data, classic MVP models, recent gradient-based MVP methods, and the recent variational approach MEDL CVAE.",1. Introduction,[0],[0]
"We use φ(x;µ,Σ) and Φ(x;µ,Σ) to denote the density function and the cumulative distribution function of the multivariate normal distribution with mean µ ∈ Rl and covariance Σ ∈ Rl×l, i.e,
φ(x;µ,Σ) = 1
(2π)l/2|Σ|1/2 e− 1 2 (x−µ) T Σ−1(x−µ) (1)
Φ(x;µ,Σ) = ∫",2.1. Notations,[0],[0]
x1 −∞ ... ∫,2.1. Notations,[0],[0]
"xl −∞ φ(s;µ,Σ).ds1...dsl (2)",2.1. Notations,[0],[0]
"where | · | denotes the determinant of a matrix.
",2.1. Notations,[0],[0]
"For the sake of simplicity, we use Φ(x) to denote the CDF of one-dimensional standard normal distribution.
",2.1. Notations,[0],[0]
"For the comparison between vectors, we use ”4” to denote the ”element-wise less or equal to”, i.e,
a 4 b iff ∀i, ai ≤ bi (3)",2.1. Notations,[0],[0]
The multivariate probit model (MVP) is described in terms of a multivariate normal distribution of the underlying latent variables that are manifested as binary responses through a threshold specification.,2.2. Deep Multivariate Probit Model,[0],[0]
"More specifically, given the dataset D = {(xi, yi)|i = 1, ..., N}, where xi ∈ Rm is the m-dimensional contextual data and yi ∈ {0, 1}l is the ldimensional binary label, MVP maps the Bernoulli distribution of each binary label yi,j to a sequence of latent variables ri =",2.2. Deep Multivariate Probit Model,[0],[0]
"(ri,1, ..., ri,l) through the threshold 0, where ri is subject to a multivariate normal distribution, i.e,
Pr(yi,j = 1|xi) =",2.2. Deep Multivariate Probit Model,[0],[0]
"Pr(ri,j > 0) (4) Pr(yi,j = 0|xi) =",2.2. Deep Multivariate Probit Model,[0],[0]
"Pr(ri,j ≤ 0)",2.2. Deep Multivariate Probit Model,[0],[0]
"where ri ∼ N(µ(xi),Σ).
",2.2. Deep Multivariate Probit Model,[0],[0]
"More specifically, the marginal likelihood is,
Pr(yi,j = 1|xi)",2.2. Deep Multivariate Probit Model,[0],[0]
"= Φ ( µ(xi)j√
Σj,j
)
",2.2. Deep Multivariate Probit Model,[0],[0]
"Pr(yi,j = 0|xi)",2.2. Deep Multivariate Probit Model,[0],[0]
"= Φ ( −µ(xi)j√
Σj,j
) ,
and the joint likelihood is, Pr(yi|xi) = ∫ A1 ... ∫",2.2. Deep Multivariate Probit Model,[0],[0]
"Al φ(s;µ(xi),Σ)ds1...dsl
Here Aj = {
(−∞, 0] if yi,j = 0",2.2. Deep Multivariate Probit Model,[0],[0]
"[0,+∞) if yi,j = 1
(5)
",2.2. Deep Multivariate Probit Model,[0],[0]
"LetDi = diag(2yi−1) ∈ {−1, 0, 1}l×l which is a diagonal matrix using vector 2yi",2.2. Deep Multivariate Probit Model,[0],[0]
− 1 as its diagonal.,2.2. Deep Multivariate Probit Model,[0],[0]
"Then, we can further reduce formula (5) into the CDF of a multivariate normal distribution, using the affine transformation, i.e.,
Pr(yi|xi) = Φ(0;−µ′i,Σ′i), (6) where µ′i = D iµ(xi) and Σ′i = D iΣDi.
",2.2. Deep Multivariate Probit Model,[0],[0]
Learning the classic MVP model involves estimating the coefficient W of the linear function µ(xi) =,2.2. Deep Multivariate Probit Model,[0],[0]
"Wxi and the covariance matrix Σ. Usually, both the coefficient matrixW and the covariance matrix Σ are learnt from data, but in some cases the variance matrix Σ can be computed directly from data.",2.2. Deep Multivariate Probit Model,[0],[0]
"For example, the model in (Mandt et al., 2017) used linear kernel for the covariance matrix, where the covariance matrix is the sum of a linear kernel matrix and a diagonal noise matrix computed from the raw input data.
",2.2. Deep Multivariate Probit Model,[0],[0]
"Taking advantage of the successful development of deep learning, we introduce the Deep Multivariate Probit Model (DMVP), which is a deep generalization of the classic MVP, where µ(xi) changes from the linear function Wxi to a non-linear function θ(xi), learnt via a deep neural network, and the covariance matrix Σ is always learnt from the data.",2.2. Deep Multivariate Probit Model,[0],[0]
"In this way, the DMVP obtains the flexibility as well as the predictive power of various deep neural networks while modelling the correlations of multiple entities by fitting the correlations of the latent variables.",2.2. Deep Multivariate Probit Model,[0],[0]
The generic learning methods of MVP are maximum-aposteriori estimation and maximum likelihood estimation.,3. End-to-End Learning for DMVP,[0],[0]
"Because we have introduced the deep neural networks into the DMVP, we train DMVP by maximizing the loglikelihood, which is the most commonly used method for the training of neural networks, i.e.,
argmax θ,Σ
∑ i log Pr(yi|xi) =",3. End-to-End Learning for DMVP,[0],[0]
"argmax
θ,Σ
∑ i log Φ(0;−µ′i,Σ′i).
",3. End-to-End Learning for DMVP,[0],[0]
"The difficulties with respect to learning the DMVP are mainly due to the computation of equation (6) as well as its gradients, which are obtained by integrating over a high-dimensional constrained space of latent variables.",3. End-to-End Learning for DMVP,[0],[0]
"As pointed out by (Magid, 1994), there is no closed form solution for equation (6), and to date can only be estimated via sampling methods.",3. End-to-End Learning for DMVP,[0],[0]
"The vanilla rejection sampling estimates Φ(0;−µ′i,Σ′i) by counting the rate that a sample r from N(−µ′i,Σ′i) satisfies r 4 0.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"However, because the value of Φ(0;−µ′i,Σ′i) could be exponentially small, on average, it could take exponentially many trials to get merely one trial that satisfies
the condition.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"One straightforward solution for this estimation, which has been adopted in (Chen et al., 2017), is to use the MCMC approaches to estimate the distribution over the truncated high-dimensional space.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Another importance sampling method proposed by (Genz, 1992) uses Cholesky factorization to compute the equation (6).",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"This method transforms the sampling of a truncated multivariate normal distribution into the sampling of a sequence of univariate truncated normal distributions, where the truncation of each univariate normal distribution depends on the samples of all preceding random variables.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Because both the MCMC method and the importance sampling require a sequentially dependent sampling, they cannot easily integrate with parallelized deep learning infrastructure such as GPUs.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Therefore, we propose a novel parallel sampling method to address this approximation problem.
",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Though there is no closed form for computing the CDF of a general multivariate normal distribution, the onedimensional CDF Φ(x) has very accurate analytical estimation (Cody, 1969), which has been implemented in almost all machine learning tools.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Inspired by this fact, we decompose the covariance matrix Σ into V + Σr, where V is a diagonal positive definite matrix and Σr is the residual covariance matrix, so that a random variable r ∼ N(0,Σ) can be decomposed as the subtraction of two random variable z ∼ N(0, V ) and w ∼ N(0,Σr), i.e., r = z",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"− w. Thus, the estimation of Φ(0;−µ,Σ) in equation (6) can be transformed into the expectation of the product of l onedimensional CDF’s, conditioned on the residual multivariate normal distribution w, i.e.,
Φ(0;−µ,Σ) =",3.1. End-to-End Sampling Process for DMVP,[0],[0]
Pr(r,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"− µ 4 0) r ∼ N(0,Σ)",3.1. End-to-End Sampling Process for DMVP,[0],[0]
= Pr(z,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"− w 4 µ) z ∼ N(0, V ), w ∼ N(0,Σr) = Ew∼N(0,Σr)[Pr(z 4 (w + µ)|w)]",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"z ∼ N(0, V )
= Ew∼N(µ,Σr)
",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"[ l∏
j=1
Φ ( wj√",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Vj,j )]
= Ew∼N(V−1/2µ,V−1/2ΣrV−1/2)
",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"[ l∏
j=1
Φ (wj)
]
≈ 1 M M∑ k=1
( l∏
j=1
Φ ( w
(k) j
)) .",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"(7)
Knowing that the role of the parameter V is to rescale the sample wj , without loss of generality, we can assume that V is an identity matrix and directly learn the ”rescaled” residual multivariate normal distribution.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"That is, in the rest of the paper as well as the Figure (1), we use the identity matrix I to replace V .
",3.1. End-to-End Sampling Process for DMVP,[0],[0]
Main idea: The high-level idea of our end-to-end learning scheme for DMVP is based on the transformation shown in equation (7).,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"The intuition behind our transformation is similar to the Rao-Blackwell theorem (Blackwell, 1947), which improves an estimator by computing its expectation, conditioned on a sufficient statistic.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"In our case, instead of using a sufficient statistic, we use the residual distribution w, which
fully captures the correlations of the original multivariate distribution.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Conceptually, given input features xi and labels yi, DMVP first learns the mean and the covariance of the residual multivariate normal distribution via a deep neural network.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Then, DMVP samples batches of independent samples w(k) from the residual multivariate normal distribution and uses equation (7) to compute the estimation of the joint likelihood.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
This sampling process outperforms previous estimation methods in several aspects.,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"First, the process samples from an explicit distribution, which is significantly more efficient than MCMC-based methods, which need to burn a lot of intermediate samples to reach the implicit distribution.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
(We show the experimental results in the section 5.),3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Second, the variance of this sampling process is strictly smaller than the vanilla rejection sampling (See appendix), therefore DMVP requires fewer samples, since every sample from this process provides non-trivial information.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Third, this sampling process can be implemented in parallel on GPUs, which is not the case for MCMC.
",3.1. End-to-End Sampling Process for DMVP,[0],[0]
Figure (1) depicts the detailed learning framework implemented in DMVP.,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"The feature network, composed of multilayer convolutional networks or fully connected networks, extracts high-level features from the contextual data source to learn the µ for each data point.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"The choice of the feature network depends on the type of contextual data and the problem, but is flexible enough to be any structure that could be boosted by GPUs.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"In DMVP, the residual covariance matrix Σr is a global parameter, which is learned from random initialization and shared by all data points.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"To ensure that Σr is a semi-positive definite matrix, we actually form the residual covariance matrix by the product of one matrix and its transpose, i.e., Σr = Σ 1/2 r (Σ 1/2 r )T .",3.1. End-to-End Sampling Process for DMVP,[0],[0]
The random variable generator generates batches of the standard normal distributed random variable z(k) in parallel on GPUs.,3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Then, using Σ1/2r , µ, and the diagonal matrix Di corresponding to yi, DMVP computes batches of samples w(k) = Di(µ + Σ1/2r z(k)).",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"According to the affine transformation of the normal distribution, the samples {w(k)} are subject to the multivariate normal distribution N(Diµ,DiΣrDi), which is the desired residual multivariate normal distribution as derived in equation (6) and (7).",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Because there is no dependency among those samples, all the operations described above could be computed in parallel using tensor operations.",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"Therefore, we can integrate DMVP with various deep neural networks and implement it end-to-end on GPUs using popular machine learning packages (such as Tensorflow or Pytouch).",3.1. End-to-End Sampling Process for DMVP,[0],[0]
"In terms of the convergence behavior of this sampling process, we provide a theoretical analysis with respect to the estimation error.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Since the estimate ∏l j=1 Φ ( w (k) j ) is
bounded between 0 and 1, Hoeffding’s inequality guarantees exponentially fast convergence in M between the r.h.s of equation (7) and Pr(yi|xi), i.e.,
Pr ∣∣∣∣∣∣ 1M M∑ k=1 n∏ j=1 Φ(w (k)",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"i,j )",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"− Pr(yi|xi) ∣∣∣∣∣∣ ≥ Pr(yi|xi) 
≤ 2e−M 2 Pr2(yi|xi).",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"(8)
Though equation (8) converges exponentially fast, the value of Pr(yi|xi) could be the magnitude of 2−l.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"That is, we may need to sampleO(22l) many times to have a reasonable multiplicative error bound.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"To address this issue, another assertion can be proven for this sampling process using Chebyshev’s inequality:
Theorem 1 Let µ ∈ Rl and Σ ∈ Rl×l be the rescaled mean and rescaled residual covariance matrix of the random variable w(k) in equation (7), then we have
Pr ∣∣∣∣∣∣ 1M M∑ k=1",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"l∏ j=1 Φ(wki,j)− Pr(yi|xi)",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"∣∣∣∣∣∣ ≥ Pr(yi|xi) 
≤",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Φ
( 0; [ −µ −µ ] , [ Σ + I Σ Σ Σ + I ]) /Φ2(0;−µ,Σ",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
+,3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"I)− 1 M 2
(9)
≤
( Φ(0;−µ,2Σ+I) Φ(0;−µ,Σ+I) )",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
2 |2Σ + I|1/2,3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"− 1
M 2 (10) ≤ ∏l i=1",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"g(µi)
2|2Σ + I|1/2",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"− 1 M 2
(11)
",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"where g(µi) = maxx Φ( √
2x+µi) Φ(x+µi) .",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"See the Appendix for a more detailed proof.
",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"The function g(µi) in the theorem (1) does not have a closed form but it is a monotonous decreasing function, which converges to 1 as µi increases.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
Figure 2 is the visualization of function g(µi).,3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"As can be seen, the function g(µi) is very close to 1 when µi is positive.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Though g(µi) increases exponentially with an upper bound √ 2e 3−2 √ 2 2 µ 2 i when µi is a
very small negative number, the training method - maximum likelihood estimation - ensures that most µi are positive.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In theorem (1), the equation (9) is the upper bound derived by exact analysis of the second moment of the random variable∏l j=1",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
Φ(w k,3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"i,j).",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Knowing there is no general closed form for the CDF of multivariate normal distribution, we further derive the equation (11) to provide an analytical upper bound.
",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Though, in the worst case, the upper bounds could be exponentially large with respect to the dimensionality, this still sheds light on the convergence behavior of our sampling process.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"For example, if the distribution of entities is independent, then the rescaled residual covariance Σ is a zero matrix.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In that case, the variance of our sampling process is zero, so that we only need to sample once to get the exact likelihood.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In more general cases, if the rescaled residual covariance Σ is a low-rank matrix, most eigenvalues of the matrix 2Σ +",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"I are 1, which indicates a small |2Σ + I|.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"According to our experiments, most eigenvalues of the rescaled residual covariance matrix Σ are very close to 0, which supports the empirical convergence behavior of our DMVP.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In the experimental section, we provide more detailed analysis in terms of the performance as well as the convergence behavior of DMVP with a low-rank residual covariance matrix, showing that the DMVP’s performance only degrades significantly when the rank of the residual covariance matrix is extremely small.
",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Since our learning scheme is based on stochastic gradient descent, we also use the derivatives of equation (7) as the estimation of the true derivatives.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"The variance analysis of the derivatives of Pr(yi|xi), which has a similar convergence bound as theorem (1), could also be derived using the similar method.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Because of space limitations, see the appendix for a more detailed proof.",3.2. Theoretical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Multi-entity modelling problems are studied extensively under the names of multi-label classification, multi-entity embedding and structured prediction.",4. Other Related Work,[0],[0]
"The simplest approach is to model the distribution of each entity independently, given the contextual data, known as the binary relevance model.",4. Other Related Work,[0],[0]
This approach is quite popular in multi-label image classification because of its simplicity and flexibility.,4. Other Related Work,[0],[0]
(We chose it as a baseline for this problem.),4. Other Related Work,[0],[0]
"However, this could perform
poorly, particularly when certain labels are rare or some are highly correlated.",4. Other Related Work,[0],[0]
"Therefore, max-margin (Sarawagi & Gupta, 2008), ranking losses (Elisseeff & Weston, 2002) and embedding methods (Rudolph et al., 2016) have been used to address the correlations.",4. Other Related Work,[0],[0]
"Along this line of research, recent approaches (Belanger et al., 2017; Belanger & McCallum, 2016) use SSVM minimizer to optimize energybased structured models.",4. Other Related Work,[0],[0]
"Those approaches mainly focus on the classification problem, in which the correlation among entities is implicit and therefore it is hard to derive the structured probabilistic distribution of entities.",4. Other Related Work,[0],[0]
"Our applications of DMVP, on the other hand, focus more on probabilistic modeling rather than classification.",4. Other Related Work,[0],[0]
"Another classic approach related to MVP is the Conditional Random Field (CRF) (Lafferty et al., 2001), which offers a general framework for structured prediction based on undirected graphical models.",4. Other Related Work,[0],[0]
"Instead of using correlated latent variables, CRF models the correlation among entities directly, where the joint probability of multiple outcomes is proportional to an energy function.",4. Other Related Work,[0],[0]
"However, optimizing CRF models suffers from the computational intractability of the partition function.",4. Other Related Work,[0],[0]
"To remedy this issue, (Xu et al., 2011) applied ensemble methods and (Deng et al., 2014) proposed a special CRF for problems involving specific hierarchical relations.",4. Other Related Work,[0],[0]
"Nevertheless, optimizing CRF models still inevitably depends on gibbs sampling for approximate inference, and has the same problem as the MCMC-based MVP models.",4. Other Related Work,[0],[0]
"A newly proposed ecological model, the Deep Multi-Species Embedding model (DMSE) (Chen et al., 2017), introduces deep neural networks into the classic MVP.",4. Other Related Work,[0],[0]
"Nevertheless, the learning methods of DMSE are also based on sequential inference such as the MCMC simulations, so that they are not easily boosted using GPUs.
",4. Other Related Work,[0],[0]
"The mixed-logit model (McFadden & Train, 2000) is another statistical model for analyzing discrete outcomes, whose marginal likelihood is similar to the formula of the transformation step in DMVP.",4. Other Related Work,[0],[0]
"However, the mixed-logit model is a general way to inject random variables into the logistic regression while the transformation in DMVP uses the auxiliary residual covariance to estimate the likelihood.",4. Other Related Work,[0],[0]
"MultiEntity Dependency Learning via Conditional Variational Learning (MEDL CVAE) uses a conditional variational autoencoder to handle correlation between multiple entities, and is also compatible with parallelized deep structures.",4. Other Related Work,[0],[0]
"Despite its limitations, as discussed in the introduction, MEDL CVAE is a state-of-the-art multi-entity modelling method and is also closely related to our DMVP model.",4. Other Related Work,[0],[0]
"Therefore, we chose MEDL CVAE as the representative approach among those competitive multi-entity modelling methods and compare its performance to DMVP, in the experimental section.",4. Other Related Work,[0],[0]
We evaluate our DMVP1.,5.1. Datasets and Implementation Details,[0],[0]
"on three datasets of multi-entity modelling problems.
",5.1. Datasets and Implementation Details,[0],[0]
"eBird is a crowd-sourced bird observation dataset collected from the successful citizen science project eBird (Munson et al., 2012).",5.1. Datasets and Implementation Details,[0],[0]
One record in this dataset is referred to as a checklist in which the bird observer records all the species,5.1. Datasets and Implementation Details,[0],[0]
he/she detects as well as the time and the geographical location of the observational site.,5.1. Datasets and Implementation Details,[0],[0]
"Crossed with the National Land Cover Dataset for the U.S. (NLCD) (Homer et al., 2015), we obtain a 15-dimensional feature vector for each observational site which describes the landscape composition with respect to 15 different land types such as water, forest, etc.",5.1. Datasets and Implementation Details,[0],[0]
We also collect the satellite images for each observation site by matching the geographical location of the observational site to Google Earth2.,5.1. Datasets and Implementation Details,[0],[0]
Each satellite image covers an area of 12.3km2 near the observation site and has 256×256 pixels.,5.1. Datasets and Implementation Details,[0],[0]
"The dataset for this experiment is formed by picking all the observation checklists from the Bird Conservation Region (BCR) 13 (Committee et al., 2000) in the last two weeks of May from 2004 to 2014, which contains 50,949 observations.",5.1. Datasets and Implementation Details,[0],[0]
"We choose the top 100 most frequently observed birds as the target species which cover over 95% of the records in our dataset.
",5.1. Datasets and Implementation Details,[0],[0]
"Amazon is the Amazon rainforest landscape satellite image dataset collected for Amazon rainforest landscape analysis,3 in which raw images were derived from Planet’s fullframe analytic scene products using 4-band satellites in sun-synchronous orbit and International Space Station orbit.",5.1. Datasets and Implementation Details,[0],[0]
The organizers used Planet’s visual product processor to transform raw images into 3-band 256x256-pixel jpg format.,5.1. Datasets and Implementation Details,[0],[0]
"The Amazon contains a total of 34,431 samples and each sample in this dataset contains a satellite image chip covering an area of 0.9 km2 in Amazon rainforest.",5.1. Datasets and Implementation Details,[0],[0]
The chips were analyzed using the Crowd Flower4 platform to obtain a ground-truth composition of the landscape.,5.1. Datasets and Implementation Details,[0],[0]
"There are 17 different labels for each satellite image chip, which represent a reasonable subset of phenomena of interest in the Amazon basin such as atmospheric conditions, common land cover phenomena, and land use phenomena.
",5.1. Datasets and Implementation Details,[0],[0]
"NUS-WIDE-LITE is a light version of the NUS-WIDE datasets collected by the National University of Singapore
1Code to reproduce the experiments can be found at https://bitbucket.org/DiChen9412/icml2018 dmvp
2https://www.google.com/earth/. Google Earth has already conducted preprocessing including cloud removing on the satellite images.
3https://www.kaggle.com/c/planet-understanding-theamazon-from-space.
",5.1. Datasets and Implementation Details,[0],[0]
"4https://www.crowdflower.com/
(Chua et al., July 8-10, 2009), which contains 55,615 samples and each sample is the low-level features (such as wavelet texture, histogram, correlogram, etc) of the realworld web image associated with tags from Flicker.",5.1. Datasets and Implementation Details,[0],[0]
"The 81 tags represent 81 different concepts related to the web images, such as the concepts related to the objects in the image (dog, cat, building, etc) and the concepts of the background (clouds, sunset, etc).",5.1. Datasets and Implementation Details,[0],[0]
"For the ease of presentation, we use NUS to denote this dataset.
",5.1. Datasets and Implementation Details,[0],[0]
"We randomly split the datasets into three parts for training, validation, and testing.",5.1. Datasets and Implementation Details,[0],[0]
The details of the three datasets are listed in table 1.,5.1. Datasets and Implementation Details,[0],[0]
We compare the proposed DMVP with baseline models from three different groups.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"The first group, which we refer to as conditional independent model (CIM), assumes independence among entities, conditioned on the contextual data.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Within this group, we chose different models based on the type of the input features.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"For example, when the input features are images, we choose to use convolutional neural networks (CNN), while we use the multi-layer fully connected neural network (MLP) for one-dimensional feature inputs.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"For the sake of fairness, the structure of CIM as well as the feature networks in other baseline models are always the same as the feature network of DMVP.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"More specifically, for the data resources of low-level features, such as the NLCD features of eBird dataset and the NUS dataset, we use a 4-layer fully connected neural network with hidden units of size 128, 256, 256, l, where the activation function of the first 3 layers is ReLU (Nair & Hinton, 2010) and there is no activation function in the last layer.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"For the image data resources, we use a CNN similar to the Alexnet (Krizhevsky et al., 2012) with some minor adjustments.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"The second group is the previously proposed Multivariate Probit Model, which can also model correlations among entities, but uses different inference methods.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Within this group, we chose the Deep Multi-Species Embedding (DMSE) model (Chen et al., 2017), a gradient-based MVP model, which uses the numerical computing method proposed by (Genz, 1992) to estimate the likelihood and a MCMC-based method to estimate the gradients.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"This model represents a wide class of MCMC-based multivariate probit models while further improving the classic MVP by taking advantages of the flexibility of deep neural networks to
obtain useful feature extractions.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Nevertheless, its training process involves MCMC approaches as well as the sequential importance sampling, and therefore cannot be integrated on GPUs.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"For the last group, we chose the MEDL CVAE(Tang et al., 2017) model, which is a state-of-the-art multi-entity modelling approach proposed recently.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"This model uses conditional variational auto-encoder to handle correlation between multiple entities, in which it approximates the joint likelihood by its variational lower bound.
",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Because we study multi-entity modelling problems, in our experiments, we use Negative Joint Distribution Loglikelihood (Neg.JLL) as the indicator of a model’s perfor-
mance: − 1N N∑ i=1",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"log Pr(yi|xi), where N is the number of samples in the test set.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Based on the theorem (1) we obtain 1,000,000 samples from the residual multivariate normal distribution for testing DMVP’s performance, which is sufficient to guarantee the accuracy of the estimation.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"However, for the training, DMVP empirically converges well with only 100 samples.
",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"All the training and testing process of our DMVP and other baseline models, which are compatible with the GPUs, are performed on one NVIDIA Quadro P4000 GPU with 8GB memory.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
The training and testing process for the DMSE model is performed on Intel(R) Core(TM) i7-7700K CPU@4.20Gz with 8 cores.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"Since the bottleneck of the DMSE model is on the MCMC sampling, which could not be parallelized trivially, additional cores do not improve the wall-clock time significantly.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"The whole training process lasts 200 epochs, using the batch size of 128, Adam optimizer (Kingma & Ba, 2014) with learning rate of 10−4 and utilizing batch normalization (Ioffe & Szegedy, 2015), 0.5 dropout rate (Srivastava et al., 2014) and early stopping to accelerate the training process and to prevent overfitting for not only DMVP but all baseline models.
",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
Table 2 shows the average performance of DMVP as well as other baseline models on the 3 datasets (4 different type of input features) in terms of the negative joint loglikelihood (Neg.JLL) and the wall-clock time of training.5 There are multiple key results in Table 2: (1) By comparing the Neg.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"JLL of the conditional independent model (CIM) with other models, one can observe significant advantages of modelling the correlations among entities.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
(2) DMVP trains more than 100 times faster than the MCMC-based DMSE model in terms of the wall-clock time.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
This huge gap between DMVP and DMSE is due not only to the parallelization but also to the advantage of sampling from an explicit distribution.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"For DMVP, empirically we only need to sample 100 samples per data point to converge very well and every sample here is an unbiased esti-
5We thank the authors of (Tang et al., 2017) and (Chen et al., 2017) for sharing the codes.
",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
mation of the joint likelihood.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"However, in DMSE, we need to burn every 1000 intermediate samples to merely get one quasi-unbiased sample from the implicit distribution, which is not cost-efficient.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"What’s more, the high-resolution image data resources are way beyond the capacity of the MCMCbased method, where the DMSE model cannot reach a reasonable performance after 2-day-long training.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
(3) In terms of the Neg.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"JLL, DMVP outperforms all baseline models including the competitive MEDL CVAE model, which is compatible with deep neural networks and also models the correlations among entities.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
There are two reasons of why DMVP outperforms MEDL CVAE: (i) DMVP directly learns the joint likelihood while MEDL CVAE approximates the joint likelihood by optimizing its variational lower bound.,5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
"(2) there is a KL-vanishment issue, which is notorious in all applications based on variational autoencoder, in the training of variational lower bound that hampers the performance of the MEDL CVAE model.",5.2. Performance Analysis of the DMVP on Multi-Entity Modelling Problems,[0],[0]
In Section 3.2 we provide the theoretical upper bound of the DMVP’s convergence behavior.,5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Based on the theorem (1), one way to reduce the sampling variance is to assume the low-rank property of the residual covariance matrix.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Therefore, we conducted the empirical analysis of the DMVP’s performance as well as the convergence behavior on three datasets with residual covariance matrix of different rank.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Based on the theorem (1), we use the numerators of both equation (9) and equation (11) to indicate the convergence rate, where the former is a tighter bound without an analytic form and the latter is the theoretical upper bound.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Though
the tighter bound (equation (9)) does not have a analytic form, we could show the value estimated using the numerical method proposed in (Genz, 1992).",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"What’s more, because the value of the indicators derived from equation (11) and equation (9) vary across data points over time, we pick the median at the end of the training as the representative.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"We implement the constraint of rank(Σr) by restricting the dimensionality of Σ1/2r , i.e., Σ 1/2 r ∈ Rl×k, where k ≤ l. Figure (3) show the experimental results conducted on three datasets.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Because of the similarity, for the eBird dataset, we only show the analysis using NLCD features.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"One observation from Figure (3) is that the theoretical bound is way looser than the numerical estimation of the tighter bound, which is actually closer to the empirical results.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In our experiments, DMVP converges well in all datasets using only 100 samples.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Nevertheless, the theoretical bound still sheds light on the convergence behavior of DMVP.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"One can see, as we restrict the rank of the residual covariance matrix to be lower, both the theoretical bound and the numerical estimation of the convergence rate become better while the performance of DMVP only degrades significantly
when the rank of Σr is extremely small.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
The reason behind this phenomenon is that the rank of Σr actually describes the the resolution of how fine-grained DMVP models the residual covariance.,5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Therefore, it is possible to approximate a full-rank matrix by a low-rank matrix with minimal discrepancy.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"As an example, the Figure 4 is the heatmap of the residual covariance matrix on Amazon dataset with rank from full-rank to rank-1.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"(Because of the space limitation, we only show the covariance heatmap of Amazon datasets.)",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"One can see, the pattern of residual covariance does not change too much until the resolution is extremely low.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"These facts are consistent with the empirical results of learning DMVP with full-rank residual covariance matrix, where most eigenvalues of Σr are very close to zero.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"Based on these observations, we can naturally balance the computational complexity and the predictive performance of DMVP by tuning the resolution.",5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
This provides the potential benefits of using DMVP to analyze large scale multi-entity correlation with low-rank constraints.,5.3. Empirical Analysis of the DMVP’s Convergence Behavior,[0],[0]
"In this paper, we propose an end-to-end learning scheme for DMVP, in which we propose an efficient parallel sampling process to integrate DMVP with various GPU-boosted deep neural networks.",6. Conclusion,[0],[0]
"Tested on three real-world applications of multi-entity modelling, we show that DMVP trains 100x faster than previous MCMC-based methods, captures rich correlations among entities, and consistently performs better than previous models.",6. Conclusion,[0],[0]
"We further provide both theoretical and empirical analysis of DMVP’s convergence behavior, revealing the benefits of balancing the computational complexity and the predictive performance by restricting the rank of the residual covariance matrix.",6. Conclusion,[0],[0]
Future directions include exploring the potential of applying DMVP on large scale correlation analysis with the low-rank residual covariance constraint.,6. Conclusion,[0],[0]
"We are thankful to thousands of eBird participants, and the Cornell Lab of Ornithology.",Acknowledgement,[0],[0]
"This research was supported by National Science Foundation (Grant Number 0832782,1522054, 1059284, 1356308), and ARO grant W911-NF-14-1-0498.",Acknowledgement,[0],[0]
The multivariate probit model (MVP) is a popular classic model for studying binary responses of multiple entities.,abstractText,[0],[0]
"Nevertheless, the computational challenge of learning the MVP model, given that its likelihood involves integrating over a multidimensional constrained space of latent variables, significantly limits its application in practice.",abstractText,[0],[0]
"We propose a flexible deep generalization of the classic MVP, the Deep Multivariate Probit Model (DMVP), which is an end-to-end learning scheme that uses an efficient parallel sampling process of the multivariate probit model to exploit GPU-boosted deep neural networks.",abstractText,[0],[0]
We present both theoretical and empirical analysis of the convergence behavior of DMVP’s sampling process with respect to the resolution of the correlation structure.,abstractText,[0],[0]
We provide convergence guarantees for DMVP and our empirical analysis demonstrates the advantages of DMVP’s sampling compared with standard MCMC-based methods.,abstractText,[0],[0]
"We also show that when applied to multi-entity modelling problems, which are natural DMVP applications, DMVP trains faster than classical MVP, by at least an order of magnitude, captures rich correlations among entities, and further improves the joint likelihood of entities compared with several competitive models.",abstractText,[0],[0]
End-to-End Learning for the Deep Multivariate Probit Model ,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 188–197 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
We present the first state-of-the-art coreference resolution model that is learned end-to-end given only gold mention clusters.,1 Introduction,[0],[0]
"All recent coreference models, including neural approaches that achieved impressive performance gains (Wiseman et al., 2016; Clark and Manning, 2016b,a), rely on syntactic parsers, both for head-word features and as the input to carefully hand-engineered mention proposal algorithms.",1 Introduction,[0],[0]
"We demonstrate for the first time that these resources are not required, and in fact performance can be improved significantly without them, by training an end-to-end neural model that jointly learns which spans are entity mentions and how to best cluster them.
",1 Introduction,[0],[0]
Our model reasons over the space of all spans up to a maximum length and directly optimizes the marginal likelihood of antecedent spans from gold coreference clusters.,1 Introduction,[0],[0]
"It includes a span-ranking model that decides, for each span, which of the previous spans (if any) is a good antecedent.",1 Introduction,[0],[0]
"At the core of our model are vector embeddings representing spans of text in the document, which combine context-dependent boundary representations with a head-finding attention mechanism over the span.",1 Introduction,[0],[0]
"The attention component is inspired by parser-derived head-word matching features from previous systems (Durrett and Klein, 2013), but is less susceptible to cascading errors.",1 Introduction,[0],[0]
"In our analyses, we show empirically that these learned attention weights correlate strongly with traditional headedness definitions.
",1 Introduction,[0],[0]
"Scoring all span pairs in our end-to-end model is impractical, since the complexity would be quartic in the document length.",1 Introduction,[0],[0]
"Therefore we factor the model over unary mention scores and pairwise antecedent scores, both of which are simple functions of the learned span embedding.",1 Introduction,[0],[0]
"The unary mention scores are used to prune the space of spans and antecedents, to aggressively reduce the number of pairwise computations.
",1 Introduction,[0],[0]
Our final approach outperforms existing models by 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble.,1 Introduction,[0],[0]
"It is not only accurate, but also relatively interpretable.",1 Introduction,[0],[0]
"The model factors, for example, directly indicate whether an absent coreference link is due to low mention scores (for either span) or a low score from the mention ranking component.",1 Introduction,[0],[0]
The head-finding attention mechanism also reveals which mentioninternal words contribute most to coreference decisions.,1 Introduction,[0],[0]
"We leverage this overall interpretability to do detailed quantitative and qualitative analyses, providing insights into the strengths and weaknesses of the approach.
188",1 Introduction,[0],[0]
Machine learning methods have a long history in coreference resolution (see Ng (2010) for a detailed survey).,2 Related Work,[0],[0]
"However, the learning problem is challenging and, until very recently, handengineered systems built on top of automatically produced parse trees (Raghunathan et al., 2010) outperformed all learning approaches.",2 Related Work,[0],[0]
"Durrett and Klein (2013) showed that highly lexical learning approaches reverse this trend, and more recent neural models (Wiseman et al., 2016; Clark and Manning, 2016b,a) have achieved significant performance gains.",2 Related Work,[0],[0]
"However, all of these models still use parsers for head features and include highly engineered mention proposal algorithms.1 Such pipelined systems suffer from two major drawbacks: (1) parsing mistakes can introduce cascading errors and (2) many of the hand-engineered rules do not generalize to new languages or domains.",2 Related Work,[0],[0]
"We present the first non-pipelined results, while providing further performance gains.
",2 Related Work,[0],[0]
"More generally, a wide variety of approaches for learning coreference models have been proposed.",2 Related Work,[0],[0]
"They can typically be categorized as (1) mention-pair classifiers (Ng and Cardie, 2002; Bengtson and Roth, 2008), (2) entitylevel models (Haghighi and Klein, 2010; Clark and Manning, 2015, 2016b; Wiseman et al., 2016), (3) latent-tree models (Fernandes et al., 2012; Björkelund and Kuhn, 2014; Martschat and Strube, 2015), or (4) mention-ranking models (Durrett and Klein, 2013; Wiseman et al., 2015; Clark and Manning, 2016a).",2 Related Work,[0],[0]
"Our span-ranking approach is most similar to mention ranking, but we reason over a larger space by jointly detecting mentions and predicting coreference.",2 Related Work,[0],[0]
We formulate the task of end-to-end coreference resolution as a set of decisions for every possible span in the document.,3 Task,[0],[0]
"The input is a document D containing T words along with metadata such as speaker and genre information.
",3 Task,[0],[0]
"Let N = T (T+1)2 be the number of possible text spans in D. Denote the start and end indices of a span i in D respectively by START(i) and END(i), for 1 ≤ i ≤ N .",3 Task,[0],[0]
"We assume an ordering of the
1For example, Raghunathan et al. (2010) use rules to remove pleonastic mentions of it detected by 12 lexicalized regular expressions over English parse trees.
spans based on START(i); spans with the same start index are ordered by END(i).
",3 Task,[0],[0]
The task is to assign to each span i an antecedent yi.,3 Task,[0],[0]
"The set of possible assignments for each yi is Y(i) = { , 1, . . .",3 Task,[0],[0]
", i − 1}, a dummy antecedent and all preceding spans.",3 Task,[0],[0]
"True antecedents of span i, i.e. span j such that 1 ≤ j ≤ i− 1, represent coreference links between i and",3 Task,[0],[0]
j. The dummy antecedent represents two possible scenarios: (1) the span is not an entity mention or (2) the span is an entity mention but it is not coreferent with any previous span.,3 Task,[0],[0]
"These decisions implicitly define a final clustering, which can be recovered by grouping all spans that are connected by a set of antecedent predictions.",3 Task,[0],[0]
"We aim to learn a conditional probability distribution P (y1, . . .",4 Model,[0],[0]
", yN | D) whose most likely configuration produces the correct clustering.",4 Model,[0],[0]
"We use a product of multinomials for each span:
P (y1, . . .",4 Model,[0],[0]
", yN | D) = N∏ i=1",4 Model,[0],[0]
"P (yi | D)
= N∏ i=1 exp(s(i, yi))∑ y′∈Y(i) exp(s(i, y′))
where s(i, j) is a pairwise score for a coreference link between span i and span j in",4 Model,[0],[0]
documentD.,4 Model,[0],[0]
We omit the document D from the notation when the context is unambiguous.,4 Model,[0],[0]
"There are three factors for this pairwise coreference score: (1) whether span i is a mention, (2) whether span j is a mention, and (3) whether j is an antecedent of i:
s(i, j) = { 0 j = sm(i) + sm(j) + sa(i, j) j 6=
Here sm(i) is a unary score for span i being a mention, and sa(i, j) is pairwise score for span j being an antecedent of span i.
By fixing the score of the dummy antecedent to 0, the model predicts the best scoring antecedent if any non-dummy scores are positive, and it abstains if they are all negative.
",4 Model,[0],[0]
A challenging aspect of this model is that its size is O(T 4) in the document length.,4 Model,[0],[0]
"As we will see in Section 5, the above factoring enables aggressive pruning of spans that are unlikely to belong to a coreference cluster according the mention score sm(i).
",4 Model,[0],[0]
"Scoring Architecture We propose an end-toend neural architecture that computes the above scores given the document and its metadata.
",4 Model,[0],[0]
"At the core of the model are vector representations gi for each possible span i, which we describe in detail in the following section.",4 Model,[0],[0]
"Given these span representations, the scoring functions above are computed via standard feed-forward neural networks:
sm(i) = wm · FFNNm(gi) sa(i, j) = wa ·",4 Model,[0],[0]
"FFNNa([gi, gj , gi ◦ gj , φ(i, j)])
where · denotes the dot product, ◦ denotes element-wise multiplication, and FFNN denotes a feed-forward neural network that computes a nonlinear mapping from input to output vectors.
",4 Model,[0],[0]
"The antecedent scoring function sa(i, j) includes explicit element-wise similarity of each
span gi ◦ gj and a feature vector φ(i, j) encoding speaker and genre information from the metadata and the distance between the two spans.
",4 Model,[0],[0]
Span Representations Two types of information are crucial to accurately predicting coreference links: the context surrounding the mention span and the internal structure within the span.,4 Model,[0],[0]
"We use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to encode the lexical information of both the inside and outside of each span.",4 Model,[0],[0]
"We also include an attention mechanism over words in each span to model head words.
",4 Model,[0],[0]
"We assume vector representations of each word {x1, . . .",4 Model,[0],[0]
",xT }, which are composed of fixed pretrained word embeddings and 1-dimensional convolution neural networks (CNN) over characters (see Section 7.1 for details)
",4 Model,[0],[0]
"To compute vector representations of each span, we first use bidirectional LSTMs to encode every word in its context:
ft,δ = σ(Wf[xt,ht+δ,δ] + bi) ot,δ = σ(Wo[xt,ht+δ,δ] + bo) c̃t,δ = tanh(Wc[xt,ht+δ,δ] + bc) ct,δ = ft,δ ◦ c̃t,δ + (1− ft,δ) ◦",4 Model,[0],[0]
"ct+δ,δ ht,δ = ot,δ ◦ tanh(ct,δ) x∗t",4 Model,[0],[0]
"= [ht,1,ht,−1]
where δ ∈ {−1, 1} indicates the directionality of each LSTM, and x∗t is the concatenated output of the bidirectional LSTM.",4 Model,[0],[0]
"We use independent LSTMs for every sentence, since cross-sentence context was not helpful in our experiments.
",4 Model,[0],[0]
"Syntactic heads are typically included as features in previous systems (Durrett and Klein, 2013; Clark and Manning, 2016b,a).",4 Model,[0],[0]
"Instead of relying on syntactic parses, our model learns a taskspecific notion of headedness using an attention mechanism (Bahdanau et al., 2014) over words in each span:
αt = wα · FFNNα(x∗t ) ai,t =
exp(αt) END(i)∑
k=START(i)
exp(αk)
",4 Model,[0],[0]
"x̂i = END(i)∑
t=START(i)
ai,t · xt
where x̂i is a weighted sum of word vectors in span i.",4 Model,[0],[0]
"The weights ai,t are automatically learned and correlate strongly with traditional definitions of head words as we will see in Section 9.2.
",4 Model,[0],[0]
"The above span information is concatenated to produce the final representation gi of span i:
gi =",4 Model,[0],[0]
"[x∗START(i),x ∗ END(i), x̂i, φ(i)]
This generalizes the recurrent span representations recently proposed for question-answering (Lee et al., 2016), which only include the boundary representations x∗START(i) and x ∗ END(i).",4 Model,[0],[0]
We introduce the soft head word vector x̂i and a feature vector φ(i) encoding the size of span i.,4 Model,[0],[0]
The size of the full model described above is O(T 4) in the document length T .,5 Inference,[0],[0]
"To maintain computation efficiency, we prune the candidate spans greedily during both training and evaluation.
",5 Inference,[0],[0]
We only consider spans with up to L words and compute their unary mention scores sm(i) (as defined in Section 4).,5 Inference,[0],[0]
"To further reduce the number of spans to consider, we only keep up to λT spans with the highest mention scores and consider only up to K antecedents for each.",5 Inference,[0],[0]
We also enforce non-crossing bracketing structures with a simple suppression scheme.2,5 Inference,[0],[0]
"We accept spans in decreasing order of the mention scores, unless, when considering span i, there exists a previously accepted span j such that START(i) < START(j) ≤
2The official CoNLL-2012 evaluation only considers predictions without crossing mentions to be valid.",5 Inference,[0],[0]
"Enforcing this consistency is not inherently necessary in our model.
",5 Inference,[0],[0]
"END(i) < END(j) ∨ START(j) < START(i) ≤ END(j) < END(i).
",5 Inference,[0],[0]
"Despite these aggressive pruning strategies, we maintain a high recall of gold mentions in our experiments (over 92% when λ = 0.4).
",5 Inference,[0],[0]
"For the remaining mentions, the joint distribution of antecedents for each document is computed in a forward pass over a single computation graph.",5 Inference,[0],[0]
The final prediction is the clustering produced by the most likely configuration.,5 Inference,[0],[0]
"In the training data, only clustering information is observed.",6 Learning,[0],[0]
"Since the antecedents are latent, we optimize the marginal log-likelihood of all correct antecedents implied by the gold clustering:
log N∏ i=1",6 Learning,[0],[0]
"∑ ŷ∈Y(i)∩GOLD(i) P (ŷ)
where GOLD(i) is the set of spans in the gold cluster containing span i.",6 Learning,[0],[0]
"If span i does not belong to a gold cluster or all gold antecedents have been pruned, GOLD(i) = { }.
",6 Learning,[0],[0]
"By optimizing this objective, the model naturally learns to prune spans accurately.",6 Learning,[0],[0]
"While the initial pruning is completely random, only gold mentions receive positive updates.",6 Learning,[0],[0]
"The model can quickly leverage this learning signal for appropriate credit assignment to the different factors, such as the mention scores sm used for pruning.
",6 Learning,[0],[0]
Fixing score of the dummy antecedent to zero removes a spurious degree of freedom in the overall model with respect to mention detection.,6 Learning,[0],[0]
It also prevents the span pruning from introducing noise.,6 Learning,[0],[0]
"For example, consider the case where span i has a single gold antecedent that was pruned, so GOLD(i) = { }.",6 Learning,[0],[0]
"The learning objective will only correctly push the scores of non-gold antecedents lower, and it cannot incorrectly push the score of the dummy antecedent higher.
",6 Learning,[0],[0]
"This learning objective can be considered a span-level, cost-insensitive analog of the learning objective proposed by Durrett and Klein (2013).",6 Learning,[0],[0]
"We experimented with these cost-sensitive alternatives, including margin-based variants (Wiseman et al., 2015; Clark and Manning, 2016a), but a simple maximum-likelihood objective proved to be most effective.",6 Learning,[0],[0]
"We use the English coreference resolution data from the CoNLL-2012 shared task (Pradhan et al., 2012) in our experiments.",7 Experiments,[0],[0]
"This dataset contains 2802 training documents, 343 development documents, and 348 test documents.",7 Experiments,[0],[0]
The training documents contain on average 454 words and a maximum of 4009 words.,7 Experiments,[0],[0]
"Word representations The word embeddings are a fixed concatenation of 300-dimensional GloVe embeddings (Pennington et al., 2014) and 50-dimensional embeddings from Turian et al. (2010), both normalized to be unit vectors.",7.1 Hyperparameters,[0],[0]
Outof-vocabulary words are represented by a vector of zeros.,7.1 Hyperparameters,[0],[0]
"In the character CNN, characters are represented as learned 8-dimensional embeddings.",7.1 Hyperparameters,[0],[0]
"The convolutions have window sizes of 3, 4, and 5 characters, each consisting of 50 filters.
",7.1 Hyperparameters,[0],[0]
Hidden dimensions The hidden states in the LSTMs have 200 dimensions.,7.1 Hyperparameters,[0],[0]
"Each feed-forward neural network consists of two hidden layers with 150 dimensions and rectified linear units (Nair and Hinton, 2010).
",7.1 Hyperparameters,[0],[0]
Feature encoding We encode speaker information as a binary feature indicating whether a pair of spans are from the same speaker.,7.1 Hyperparameters,[0],[0]
"Following Clark and Manning (2016b), the distance features are binned into the following buckets [1, 2, 3, 4, 5- 7, 8-15, 16-31, 32-63, 64+].",7.1 Hyperparameters,[0],[0]
"All features (speaker,
genre, span distance, mention width) are represented as learned 20-dimensional embeddings.
",7.1 Hyperparameters,[0],[0]
"Pruning We prune the spans such that the maximum span width L = 10, the number of spans per word λ = 0.4, and the maximum number of antecedents K = 250.",7.1 Hyperparameters,[0],[0]
"During training, documents are randomly truncated to up to 50 sentences.
",7.1 Hyperparameters,[0],[0]
"Learning We use ADAM (Kingma and Ba, 2014) for learning with a minibatch size of 1.",7.1 Hyperparameters,[0],[0]
The LSTM weights are initialized with random orthonormal matrices as described in Saxe et al. (2013).,7.1 Hyperparameters,[0],[0]
We apply 0.5 dropout to the word embeddings and character CNN outputs.,7.1 Hyperparameters,[0],[0]
We apply 0.2 dropout to all hidden layers and feature embeddings.,7.1 Hyperparameters,[0],[0]
Dropout masks are shared across timesteps to preserve long-distance information as described in Gal and Ghahramani (2016).,7.1 Hyperparameters,[0],[0]
The learning rate is decayed by 0.1% every 100 steps.,7.1 Hyperparameters,[0],[0]
"The model is trained for up to 150 epochs, with early stopping based on the development set.
",7.1 Hyperparameters,[0],[0]
"All code is implemented in TensorFlow (Abadi et al., 2015) and is publicly available.",7.1 Hyperparameters,[0],[0]
3,7.1 Hyperparameters,[0],[0]
We also report ensemble experiments using five models trained with different random initializations.,7.2 Ensembling,[0],[0]
"Ensembling is performed for both the span pruning and antecedent decisions.
",7.2 Ensembling,[0],[0]
"At test time, we first average the mention scores sm(i) over each model before pruning the spans.
3https://github.com/kentonl/e2e-coref
",7.2 Ensembling,[0],[0]
"Given the same pruned spans, each model then computes the antecedent scores sa(i, j) separately, and they are averaged to produce the final scores.",7.2 Ensembling,[0],[0]
"We report the precision, recall, and F1 for the standard MUC, B3, and CEAFφ4metrics using the official CoNLL-2012 evaluation scripts.",8 Results,[0],[0]
The main evaluation is the average F1 of the three metrics.,8 Results,[0],[0]
Table 1 compares our model to several previous systems that have driven substantial improvements over the past several years on the OntoNotes benchmark.,8.1 Coreference Results,[0],[0]
We outperform previous systems in all metrics.,8.1 Coreference Results,[0],[0]
"In particular, our single model improves the state-of-the-art average F1 by 1.5, and our 5-model ensemble improves it by 3.1.
",8.1 Coreference Results,[0],[0]
"The most significant gains come from improvements in recall, which is likely due to our end-toend setup.",8.1 Coreference Results,[0],[0]
"During training, pipelined systems typically discard any mentions that the mention detector misses, which for Clark and Manning (2016a) consists of more than 9% of the labeled mentions in the training data.",8.1 Coreference Results,[0],[0]
"In contrast, we only discard mentions that exceed our maximum mention width of 10, which accounts for less than 2% of the training mentions.",8.1 Coreference Results,[0],[0]
The contribution of joint mention scoring is further discussed in Section 8.3,8.1 Coreference Results,[0],[0]
"To show the importance of each component in our proposed model, we ablate various parts of the architecture and report the average F1 on the development set of the data (see Figure 2).
",8.2 Ablations,[0],[0]
"Features The distance between spans and the width of spans are crucial signals for coreference resolution, consistent with previous findings from other coreference models.",8.2 Ablations,[0],[0]
"They contribute 3.8 F1 to the final result.
",8.2 Ablations,[0],[0]
"Word representations Since our word embeddings are fixed, having access to a variety of word embeddings allows for a more expressive model without overfitting.",8.2 Ablations,[0],[0]
We hypothesis that the different learning objectives of the GloVe and Turian embeddings provide orthogonal information (the former is word-order insensitive while the latter is word-order sensitive).,8.2 Ablations,[0],[0]
"Both embeddings contribute to some improvement in development F1.
",8.2 Ablations,[0],[0]
The character CNN provides morphological information and a way to backoff for out-ofvocabulary words.,8.2 Ablations,[0],[0]
"Since coreference decisions often involve rare named entities, we see a contribution of 0.9 F1 from character-level modeling.
",8.2 Ablations,[0],[0]
Metadata Speaker and genre indicators many not be available in downstream applications.,8.2 Ablations,[0],[0]
"We show that performance degrades by 1.4 F1 without them, but is still on par with previous state-of-theart systems that assume access to this metadata.
",8.2 Ablations,[0],[0]
Head-finding attention Ablations also show a 1.3 F1 degradation in performance without the attention mechanism for finding task-specific heads.,8.2 Ablations,[0],[0]
"As we will see in Section 9.4, the attention mechanism should not be viewed as simply an approximation of syntactic heads.",8.2 Ablations,[0],[0]
"In many cases, it is beneficial to pay attention to multiple words that are useful specifically for coreference but are not traditionally considered to be syntactic heads.",8.2 Ablations,[0],[0]
"To tease apart the contributions of improved mention scoring and improved coreference decisions, we compare the results of our model with alternate span pruning strategies.",8.3 Comparing Span Pruning Strategies,[0],[0]
"In these experiments, we use the alternate spans for both training and evaluation.",8.3 Comparing Span Pruning Strategies,[0],[0]
"As shown in Table 3, keeping mention candidates detected by the rule-based system over predicted parse trees (Raghunathan et al., 2010) degrades performance by 1 F1.",8.3 Comparing Span Pruning Strategies,[0],[0]
"We also provide oracle experiment results, where we keep exactly the mentions that are present in gold coreference clusters.",8.3 Comparing Span Pruning Strategies,[0],[0]
"With oracle mentions, we see an improvement of 17.5 F1, suggesting an enormous room for improvement if our model can produce better mention scores and anaphoricity decisions.",8.3 Comparing Span Pruning Strategies,[0],[0]
"To highlight the strengths and weaknesses of our model, we provide both quantitative and qualitative analyses.",9 Analysis,[0],[0]
"In the following discussion, we use predictions from the single model rather than the ensembled model.",9 Analysis,[0],[0]
"The training data only provides a weak signal for spans that correspond to entity mentions, since singleton clusters are not explicitly labeled.",9.1 Mention Recall,[0],[0]
"As a by product of optimizing marginal likelihood,
our model automatically learns a useful ranking of spans via the unary mention scores from Section 4.
",9.1 Mention Recall,[0],[0]
"The top spans, according to the mention scores, cover a large portion of the mentions in gold clusters, as shown in Figure 3.",9.1 Mention Recall,[0],[0]
"Given a similar number of spans kept, our recall is comparable to the rulebased mention detector (Raghunathan et al., 2010) that produces 0.26 spans per word with a recall of 89.2%.",9.1 Mention Recall,[0],[0]
"As we increase the number of spans per word (λ in Section 5), we observe higher recall but with diminishing returns.",9.1 Mention Recall,[0],[0]
"In our experiments, keeping 0.4 spans per word results in 92.7% recall in the development data.",9.1 Mention Recall,[0],[0]
"While the training data does not offer a direct measure of mention precision, we can use the gold syntactic structures provided in the data as a proxy.",9.2 Mention Precision,[0],[0]
"Spans with high mention scores should correspond to syntactic constituents.
",9.2 Mention Precision,[0],[0]
"In Figure 4, we show the precision of topscoring spans when keeping 0.4 spans per word.",9.2 Mention Precision,[0],[0]
"For spans with 2–5 words, 75–90% of the predictions are constituents, indicating that the vast majority of the mentions are syntactically plausible.",9.2 Mention Precision,[0],[0]
"Longer spans, which are all relatively rare, prove more difficult for the model, and precision drops to 46% for spans with 10 words.",9.2 Mention Precision,[0],[0]
We also investigate how well the learned head preferences correlate with syntactic heads.,9.3 Head Agreement,[0],[0]
"For each of the top-scoring spans in the development data that correspond to gold constituents, we compute the word with the highest attention weight.
",9.3 Head Agreement,[0],[0]
We plot in Figure 4 the proportion of these words that match syntactic heads.,9.3 Head Agreement,[0],[0]
"Agreement ranges between 68-93%, which is surprisingly high, since no explicit supervision of syntactic heads is provided.",9.3 Head Agreement,[0],[0]
The model simply learns from the clustering data that these head words are useful for making coreference decisions.,9.3 Head Agreement,[0],[0]
Our qualitative analysis in Table 4 highlights the strengths and weaknesses of our model.,9.4 Qualitative Analysis,[0],[0]
Each row is a visualization of a single coreference cluster predicted by the model.,9.4 Qualitative Analysis,[0],[0]
"Bolded spans in parentheses belong to the predicted cluster, and the redness of a word indicates its weight from the headfinding attention mechanism (ai,t in Section 4).
",9.4 Qualitative Analysis,[0],[0]
Strengths The effectiveness of the attention mechanism for making coreference decisions can be seen in Example 1.,9.4 Qualitative Analysis,[0],[0]
"The model pays attention to fire in the span A fire in a Bangladeshi garment factory, allowing it to successfully predict
the coreference link with the blaze.",9.4 Qualitative Analysis,[0],[0]
"For a subspan of that mention, a Bangladeshi garment factory, the model pays most attention instead to factory, allowing it successfully predict the coreference link with the four-story building.
",9.4 Qualitative Analysis,[0],[0]
The task-specific nature of the attention mechanism is also illustrated in Example 4.,9.4 Qualitative Analysis,[0],[0]
"The model generally pays attention to coordinators more than the content of the coordination, since coordinators, such as and, provide strong cues for plurality.
",9.4 Qualitative Analysis,[0],[0]
"The model is capable of detecting relatively long and complex noun phrases, such as a region of central Italy bordering the Adriatic Sea in Example 2.",9.4 Qualitative Analysis,[0],[0]
"It also appropriately pays attention to region, showing that the attention mechanism provides more than content-word classification.",9.4 Qualitative Analysis,[0],[0]
"The context encoding provided by the bidirectional LSTMs is critical to making informative head word decisions.
",9.4 Qualitative Analysis,[0],[0]
"Weaknesses A benefit of using neural models for coreference resolution is their ability to use word embeddings to capture similarity between words, a property that many traditional featurebased models lack.",9.4 Qualitative Analysis,[0],[0]
"While this can dramatically increase recall, as demonstrated in Example 1, it is also prone to predicting false positive links when the model conflates paraphrasing with relatedness or similarity.",9.4 Qualitative Analysis,[0],[0]
"In Example 3, the model mistakenly
predicts a link between The flight attendants and The pilots’.",9.4 Qualitative Analysis,[0],[0]
"The predicted head words attendants and pilots likely have nearby word embeddings, which is a signal used—and often overused—by the model.",9.4 Qualitative Analysis,[0],[0]
"The same type of error is made in Example 4, where the model predicts a coreference link between Prince Charles and his new wife Camilla and Charles and Diana, two noncoreferent mentions that are similar in many ways.",9.4 Qualitative Analysis,[0],[0]
"These mistakes suggest substantial room for improvement with word or span representations that can cleanly distinguish between equivalence, entailment, and alternation.
",9.4 Qualitative Analysis,[0],[0]
"Unsurprisingly, our model does little in the uphill battle of making coreference decisions requiring world knowledge.",9.4 Qualitative Analysis,[0],[0]
"In Example 5, the model incorrectly decides that them (in the context of let the rescuer locate them) is coreferent with some ships, likely due to plurality cues.",9.4 Qualitative Analysis,[0],[0]
"However, an ideal model that uses common-sense reasoning would instead correctly infer that a rescuer is more likely to look for the man overboard rather than the ship from which he fell.",9.4 Qualitative Analysis,[0],[0]
This type of reasoning would require either (1) models that integrate external sources of knowledge with more complex inference or (2) a vastly larger corpus of training data to overcome the sparsity of these patterns.,9.4 Qualitative Analysis,[0],[0]
We presented a state-of-the-art coreference resolution model that is trained end-to-end for the first time.,10 Conclusion,[0],[0]
Our final model ensemble improves performance on the OntoNotes benchmark by over 3 F1 without external preprocessing tools used by previous systems.,10 Conclusion,[0],[0]
We showed that our model implicitly learns to generate useful mention candidates from the space of all possible spans.,10 Conclusion,[0],[0]
"A novel headfinding attention mechanism also learns a taskspecific preference for head words, which we empirically showed correlate strongly with traditional head-word definitions.
",10 Conclusion,[0],[0]
"While our model substantially pushes the stateof-the-art performance, the improvements are potentially complementary to a large body of work on various strategies to improve coreference resolution, including entity-level inference and incorporating world knowledge, which are important avenues for future work.",10 Conclusion,[0],[0]
"The research was supported in part by DARPA under the DEFT program (FA8750-13-2-0019), the ARO (W911NF-16-1-0121), the NSF (IIS1252835, IIS-1562364), gifts from Google and Tencent, and an Allen Distinguished Investigator Award.",Acknowledgements,[0],[0]
We also thank the UW NLP group for helpful conversations and comments on the work.,Acknowledgements,[0],[0]
We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or handengineered mention detector.,abstractText,[0],[0]
The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each.,abstractText,[0],[0]
The model computes span embeddings that combine context-dependent boundary representations with a headfinding attention mechanism.,abstractText,[0],[0]
It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions.,abstractText,[0],[0]
"Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources.",abstractText,[0],[0]
End-to-end Neural Coreference Resolution,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 680–685 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
680",text,[0],[0]
"When drugs are concomitantly administered to a patient, the effects of the drugs may be enhanced or weakened, which may also cause side effects.",1 Introduction,[0],[0]
These kinds of interactions are called Drug-Drug Interactions (DDIs).,1 Introduction,[0],[0]
"Several drug databases have been maintained to summarize drug and DDI information such as DrugBank (Law et al., 2014), Therapeutic Target database (Yang et al., 2016), and PharmGKB (Thorn et al., 2013).",1 Introduction,[0],[0]
Automatic DDI extraction from texts is expected to support the maintenance of databases with high coverage and quick update to help medical experts.,1 Introduction,[0],[0]
"Deep neural network-based methods have recently drawn a considerable attention (Liu et al., 2016; Sahu and Anand, 2017; Zheng et al., 2017; Lim et al., 2018) since they show state-of-the-art performance without manual feature engineering.
",1 Introduction,[0],[0]
"In parallel to the progress in DDI extraction from texts, Graph Convolutional Networks (GCNs) have been proposed and applied to estimate physical and chemical properties of molec-
ular graphs such as solubility and toxicity (Duvenaud et al., 2015; Li et al., 2016; Gilmer et al., 2017).
",1 Introduction,[0],[0]
"In this study, we propose a novel method to utilize both textual and molecular information for DDI extraction from texts.",1 Introduction,[0],[0]
We illustrate the overview of the proposed model in Figure 1.,1 Introduction,[0],[0]
We obtain the representations of drug pairs in molecular graph structures using GCNs and concatenate the representations with the representations of the textual mention pairs obtained by convolutional neural networks (CNNs).,1 Introduction,[0],[0]
"We trained the molecule-based model using interacting pairs mentioned in the DrugBank database and then trained the entire model using the labeled pairs in the text data set of the DDIExtraction 2013 shared task (SemEval-2013 Task 9) (Segura Bedmar et al., 2013).",1 Introduction,[0],[0]
"In the experiment, we show GCNs can predict DDIs from molecular graphs in a high accuracy.",1 Introduction,[0],[0]
"We also show molecular information can enhance the performance of DDI extraction from texts in 2.39 percent points in F-score.
",1 Introduction,[0],[0]
"The contribution of this paper is three-fold: • We propose a novel neural method to extract
DDIs from texts with the related molecular structure information.",1 Introduction,[0],[0]
"• We apply GCNs to pairwise drug molecules
for the first time and show GCNs can predict DDIs between drug molecular structures in a high accuracy.",1 Introduction,[0],[0]
"• We show the molecular information is useful
in extracting DDIs from texts.",1 Introduction,[0],[0]
Our model for extracting DDIs from texts is based on the CNN model by Zeng et al. (2014).,2.1 Text-based DDI Extraction,[0],[0]
"When an input sentence S = (w1, w2, · · · , wN ) is given, We prepare word embedding wwi of wi and word
position embeddings wpi,1",2.1 Text-based DDI Extraction,[0],[0]
and,2.1 Text-based DDI Extraction,[0],[0]
"w p i,2 that correspond to the relative positions from the first and second target entities, respectively.",2.1 Text-based DDI Extraction,[0],[0]
"We concatenate these embeddings as in Equation (1), and we use the resulting vector as the input to the subsequent convolution layer:
wi =",2.1 Text-based DDI Extraction,[0],[0]
"[w w i ;w p i,1;w p i,2], (1)
where [; ] denotes the concatenation.",2.1 Text-based DDI Extraction,[0],[0]
"We calculate the expression for each filter j with the window size kl.
zi,l =",2.1 Text-based DDI Extraction,[0],[0]
"[wi−(kl−1)/2, · · · ,wi−(kl+1)/2], (2) mi,j,l = relu(W conv j zi,l + bconv), (3)
mj,l = max i mi,j,l, (4)
where L is the number of windows, W convj and bconv are the weight and bias of CNN, and max indicates max pooling (Boureau et al., 2010).
",2.1 Text-based DDI Extraction,[0],[0]
"We convert the output of the convolution layer into a fixed-size vector that represents a textual pair as follows:
ml =",2.1 Text-based DDI Extraction,[0],[0]
"[m1,l, · · · ,mJ,l], (5) ht =",2.1 Text-based DDI Extraction,[0],[0]
[m1; . . .,2.1 Text-based DDI Extraction,[0],[0]
";mL], (6)
where J is the number of filters.",2.1 Text-based DDI Extraction,[0],[0]
"We get a prediction ŷt by the following fully connected neural networks:
h (1) t = relu(W (1) t ht + b (1) t ), (7)
ŷt = softmax(W (2) t h (1) t + b (2) t ), (8)
where W (1)t and W (2) t are weights and b (1) t and b (2) t are bias terms.",2.1 Text-based DDI Extraction,[0],[0]
"We represent drug pairs in molecular graph structures using two GCN methods: CNNs for fingerprints (NFP) (Duvenaud et al., 2015) and Gated Graph Neural Networks (GGNN) (Li et al., 2016).",2.2 Molecular Structure-based DDI Classification,[0],[0]
"They both convert a drug molecule graph G into a fixed size vector hg by aggregating the representation hTv of an atom node v in G. We represent atoms as nodes and bonds as edges in the graph.
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"NFP first obtains the representation htv by the following equations (Duvenaud et al., 2015).
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"mt+1v = h t v + ∑ w∈N(v) htw, (9)
",2.2 Molecular Structure-based DDI Classification,[0],[0]
ht+1v = σ(H deg(v),2.2 Molecular Structure-based DDI Classification,[0],[0]
"t m t+1 v ), (10)
where htv is the representation of v in the t-th step, N(v) is the neighbors of v, and Hdeg(v)t is a weight parameter.",2.2 Molecular Structure-based DDI Classification,[0],[0]
h0v is initialized by the atom features of v. deg(v) is the degree of a node v and σ is a sigmoid function.,2.2 Molecular Structure-based DDI Classification,[0],[0]
"NFP then acquires the representation of the graph structure
hg = ∑ v,t softmax(W thtv), (11)
where W t is a weight matrix.",2.2 Molecular Structure-based DDI Classification,[0],[0]
"GGNN first obtains the representation htv by using Gated Recurrent Unit (GRU)-based recurrent neural networks (Li et al., 2016) as follows:
mt+1v = ∑
w∈N(v)
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"Aevwh t w (12)
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"ht+1v = GRU([h t v;m t+1 v ]), (13)
where Aevw is a weight for the bond type of each edge evw.",2.2 Molecular Structure-based DDI Classification,[0],[0]
"GGNN then acquires the representation of the graph structure.
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"hg = ∑ v σ(i([hTv ;h 0 v])) (j(hTv )), (14)
where i and j are linear layers and is the element-wise product.
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"We obtain the representation of a molecular pair by concatenating the molecular graph representations of drugs g1 and g2, i.e., hm =",2.2 Molecular Structure-based DDI Classification,[0],[0]
"[hg1 ;hg2 ].
",2.2 Molecular Structure-based DDI Classification,[0],[0]
"We get a prediction ŷm as follows:
h(1)m = relu(W (1) m hm + b (1) m ), (15)
ŷm = softmax(W (2) m h (1) m + b (2) m ), (16)
where W (1)m and W (2) m are weights and b (1) m and b (2) m are bias terms.",2.2 Molecular Structure-based DDI Classification,[0],[0]
We realize the simultaneous use of textual and molecular information by concatenating a textbased and molecule-based vectors: hall =,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
[ht;hm].,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
We normalize molecule-based vectors.,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
"We then use hall instead of ht in Equation 7.
",2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
"In training, we first train the molecular-based DDI classification model.",2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
The molecular-based classification is performed by minimizing the loss function,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
Lm = − ∑ ym log ŷm.,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
We then fix the parameters for GCNs and train text-based DDI extraction model by minimizing the loss function Lt = − ∑ yt log ŷt.,2.3 DDI Extraction from Texts Using Molecular Structures,[0],[0]
"In this section, we explain the textual and molecular data and task settings and training settings.",3 Experimental Settings,[0],[0]
"We followed the task setting of Task 9.2 in the DDIExtraction 2013 shared task (Segura Bedmar et al., 2013; Herrero-Zazo et al., 2013) for the evaluation.",3.1 Text Corpus and Task Setting,[0],[0]
"This data set is composed of documents annotated with drug mentions and their four types of interactions: Mechanism, Effect, Advice and Int.",3.1 Text Corpus and Task Setting,[0],[0]
"For the data statistics, please refer to the supplementary materials.
",3.1 Text Corpus and Task Setting,[0],[0]
"The task is a multi-class classification task, i.e., to classify a given pair of drugs into the four interaction types or no interaction.",3.1 Text Corpus and Task Setting,[0],[0]
"We evaluated the performance with micro-averaged precision (P),
recall (R), and F-score (F) on all the interaction types.",3.1 Text Corpus and Task Setting,[0],[0]
"We used the official evaluation script provided by the task organizers.
",3.1 Text Corpus and Task Setting,[0],[0]
"As preprocessing, we split sentences into words using the GENIA tagger (Tsuruoka et al., 2005).",3.1 Text Corpus and Task Setting,[0],[0]
We replaced the drug mentions of the target pair with DRUG1 and DRUG2 according to their order of appearance.,3.1 Text Corpus and Task Setting,[0],[0]
We also replaced other drug mentions with DRUGOTHER.,3.1 Text Corpus and Task Setting,[0],[0]
"We did not employ negative instance filtering unlike other existing methods, e.g., Liu et al. (2016), since our focus is to evaluate the effect of the molecular information on texts.
",3.1 Text Corpus and Task Setting,[0],[0]
We linked mentions in texts to DrugBank entries by string matching.,3.1 Text Corpus and Task Setting,[0],[0]
We lowercased the mentions and the names in the entries and chose the entries with the most overlaps.,3.1 Text Corpus and Task Setting,[0],[0]
"As a result, 92.15% and 93.09% of drug mentions in train and test data set matched the DrugBank entries.",3.1 Text Corpus and Task Setting,[0],[0]
"We extracted 255,229 interacting (positive) pairs from DrugBank.",3.2 Data and Task for Molecular Structures,[0],[0]
"We note that, unlike text-based interactions, DrugBank only contains the information of interacting pairs; there are no detailed labels and no information for non-interacting (negative) pairs.",3.2 Data and Task for Molecular Structures,[0],[0]
We thus generated the same number of pseudo negative pairs by randomly pairing drugs and removing those in positive pairs.,3.2 Data and Task for Molecular Structures,[0],[0]
"To avoid overestimation of the performance, we also deleted drug pairs mentioned in the test set of the text corpus.",3.2 Data and Task for Molecular Structures,[0],[0]
"We split positive and negative pairs into 4:1 for training and test data, and we evaluated the classification accuracy using only the molecular information.
",3.2 Data and Task for Molecular Structures,[0],[0]
"To obtain the graph of a drug molecule, we took
as input the SMILES (Weininger, 1988) string encoding of the molecule from DrugBank and then converted it into the graph using RDKit (Landrum, 2016) as illustrated in Figure 2.",3.2 Data and Task for Molecular Structures,[0],[0]
"For the atom features, we used randomly embedded vectors for each atoms (i.e., C, O, N, ...).",3.2 Data and Task for Molecular Structures,[0],[0]
"We also used 4 bond types: single, double, triple, or aromatic.",3.2 Data and Task for Molecular Structures,[0],[0]
"We employed mini-batch training using the Adam optimizer (Kingma and Ba, 2015).",3.3 Training Settings,[0],[0]
We used L2 regularization to avoid over-fitting.,3.3 Training Settings,[0],[0]
We tuned the bias term b(2)t for negative examples in the final softmax layer.,3.3 Training Settings,[0],[0]
"For the hyper-parameters, please refer to the supplementary materials.
",3.3 Training Settings,[0],[0]
"We employed pre-trained word embeddings trained by using the word2vec tool (Mikolov et al., 2013) on the 2014 MEDLINE/PubMed baseline distribution.",3.3 Training Settings,[0],[0]
"The vocabulary size was 215,840.",3.3 Training Settings,[0],[0]
"The embedding of the drugs, i.e., DRUG1 and DRUG2 were initialized with the pre-trained embedding of the word drug.",3.3 Training Settings,[0],[0]
The embeddings of training words that did not appear in the pretrained embeddings were initialized with the average of all pre-trained word embeddings.,3.3 Training Settings,[0],[0]
"Words that appeared only once in the training data were replaced with an UNK word during training, and the embedding of words in the test data set that did not appear in both training and pre-trained embeddings were set to the embedding of the UNK word.",3.3 Training Settings,[0],[0]
"Word position embeddings are initialized with random values drawn from a uniform distribution.
",3.3 Training Settings,[0],[0]
We set the molecule-based vectors of unmatched entities to zero vectors.,3.3 Training Settings,[0],[0]
Table 1 shows the performance of DDI extraction models.,4 Results,[0],[0]
We show the performance without negative instance filtering or ensemble for the fair comparison.,4 Results,[0],[0]
"We observe the increase of recall and F-score by using molecular information,
which results in the state-of-the-art performance with GGNN.
",4 Results,[0],[0]
"Both GCNs improvements were statistically significant (p < 0.05 for NFP and p < 0.005 for GGNN) with randomized shuffled test.
",4 Results,[0],[0]
Table 2 shows F-scores on individual DDI types.,4 Results,[0],[0]
"The molecular information improves Fscores especially on type Mechanism and Effect.
",4 Results,[0],[0]
We also evaluated the accuracy of binary classification on DrugBank pairs by using only the molecular information in Table 3.,4 Results,[0],[0]
"The performance is high, although the accuracy is evaluated on automatically generated negative instances.
",4 Results,[0],[0]
"Finally, we applied the molecular-based DDI classification model trained on DrugBank to the DDIExtraction 2013 task data set.",4 Results,[0],[0]
"Since the DrugBank has no detailed labels, we mapped all four types of interactions to positive interactions and evaluated the classification performance.",4 Results,[0],[0]
The results in Table 4 show that GCNs produce higher recall than precision and the overall performance is low considering the high performance on DrugBank pairs.,4 Results,[0],[0]
This might be because the interactions of drugs are not always mentioned in texts even if the drugs can interact with each other and because hedged DDI mentions are annotated as DDIs in the text data set.,4 Results,[0],[0]
"We also trained the DDI extraction model only with molecular information by replacing hall with hm, but the F-scores were quite low (< 5%).",4 Results,[0],[0]
These results show that we cannot predict textual relations only with molecular information.,4 Results,[0],[0]
"Various feature-based methods have been proposed during and after the DDIExtraction-2013 shared task (Segura Bedmar et al., 2013).",5 Related Work,[0],[0]
"Kim et al. (2015) proposed a two-phase SVM-based approach that employed a linear SVM with rich features that consist of word, word pair, dependency graph, parse tree, and noun phrase-based constrained coordination features.",5 Related Work,[0],[0]
Zheng et al. (2016) proposed a context vector graph kernel to exploit various types of contexts.,5 Related Work,[0],[0]
"Raihani and Laachfoubi (2017) also employed a two-phase SVM-based approach using non-linear kernels and they proposed five groups of features: word, drug, pair of drug, main verb and negative sentence features.",5 Related Work,[0],[0]
"Our model does not use any features or kernels.
",5 Related Work,[0],[0]
Various neural DDI extraction models have been recently proposed using CNNs and Recurrent Neural Networks (RNNs).,5 Related Work,[0],[0]
Liu et al. (2016) built a CNN-based model based on word and position embeddings.,5 Related Work,[0],[0]
"Zheng et al. (2017) proposed a Bidirectional Long Short-Term Memory RNN (Bi-LSTM)-based model with an input attention mechanism, which obtained target drug-specific word representations before the Bi-LSTM.",5 Related Work,[0],[0]
Lim et al. (2018) proposed Recursive neural networkbased model with a subtree containment feature and an ensemble method.,5 Related Work,[0],[0]
This model showed the state-of-the-art performance on the DDIExtraction 2013 shared task data set if systems do not use negative instance filtering.,5 Related Work,[0],[0]
"These approaches did not consider molecular information, and they can also be enhanced by the molecular information.
",5 Related Work,[0],[0]
"Vilar et al. (2017) focused on detecting DDIs from different sources such as pharmacovigilance sources, scientific biomedical literature and social media.",5 Related Work,[0],[0]
"They did not use deep neural networks and they did not consider molecular information.
",5 Related Work,[0],[0]
"Learning representations of graphs are widely studied in several tasks such as knowledge base completion, drug discovery, and material science (Wang et al., 2017; Gilmer et al., 2017).",5 Related Work,[0],[0]
"Several graph convolutional neural networks have been proposed such as NFP (Duvenaud et al., 2015), GGNN (Li et al., 2016), and Molecular Graph Convolutions (Kearnes et al., 2016), but they have not been applied to DDI extraction.",5 Related Work,[0],[0]
"We proposed a novel neural method for DDI extraction using both textual and molecular informa-
tion.",6 Conclusions,[0],[0]
"The results show that DDIs can be predicted with high accuracy from molecular structure information and that the molecular information can improve DDI extraction from texts by 2.39 percept points in F-score on the data set of the DDIExtraction 2013 shared task.
",6 Conclusions,[0],[0]
"As future work, we would like to seek the way to model the textual and molecular representations jointly with alleviating the differences in labels.",6 Conclusions,[0],[0]
We will also investigate the use of other information in DrugBank.,6 Conclusions,[0],[0]
This work was supported by JSPS KAKENHI Grant Number 17K12741.,Acknowledgments,[0],[0]
We propose a novel neural method to extract drug-drug interactions (DDIs) from texts using external drug molecular structure information.,abstractText,[0],[0]
"We encode textual drug pairs with convolutional neural networks and their molecular pairs with graph convolutional networks (GCNs), and then we concatenate the outputs of these two networks.",abstractText,[0],[0]
"In the experiments, we show that GCNs can predict DDIs from the molecular structures of drugs in high accuracy and the molecular information can enhance text-based DDI extraction by 2.39 percent points in the F-score on the DDIExtraction 2013 shared task data set.",abstractText,[0],[0]
Enhancing Drug-Drug Interaction Extraction from Texts by Molecular Structure Information,title,[0],[0]
"Learning continuous representations of words has a long history in natural language processing (Rumelhart et al., 1988).",1 Introduction,[0],[0]
"These representations are typically derived from large unlabeled corpora using co-occurrence statistics (Deerwester et al., 1990; Schütze, 1992; Lund and Burgess, 1996).",1 Introduction,[0],[0]
"A large body of work, known as distributional semantics, has studied the properties of these methods (Turney
∗The two first authors contributed equally.
",1 Introduction,[0],[0]
"et al., 2010; Baroni and Lenci, 2010).",1 Introduction,[0],[0]
"In the neural network community, Collobert and Weston (2008) proposed to learn word embeddings using a feedforward neural network, by predicting a word based on the two words on the left and two words on the right.",1 Introduction,[0],[0]
"More recently, Mikolov et al. (2013b) proposed simple log-bilinear models to learn continuous representations of words on very large corpora efficiently.
",1 Introduction,[0],[0]
"Most of these techniques represent each word of the vocabulary by a distinct vector, without parameter sharing.",1 Introduction,[0],[0]
"In particular, they ignore the internal structure of words, which is an important limitation for morphologically rich languages, such as Turkish or Finnish.",1 Introduction,[0],[0]
"For example, in French or Spanish, most verbs have more than forty different inflected forms, while the Finnish language has fifteen cases for nouns.",1 Introduction,[0],[0]
"These languages contain many word forms that occur rarely (or not at all) in the training corpus, making it difficult to learn good word representations.",1 Introduction,[0],[0]
"Because many word formations follow rules, it is possible to improve vector representations for morphologically rich languages by using character level information.
",1 Introduction,[0],[0]
"In this paper, we propose to learn representations for character n-grams, and to represent words as the sum of the n-gram vectors.",1 Introduction,[0],[0]
"Our main contribution is to introduce an extension of the continuous skipgram model (Mikolov et al., 2013b), which takes into account subword information.",1 Introduction,[0],[0]
"We evaluate this model on nine languages exhibiting different morphologies, showing the benefit of our approach.
",1 Introduction,[0],[0]
"ar X
iv :1
60 7.
04 60
6v 2
[ cs
.C",1 Introduction,[0],[0]
"L
] 1
9 Ju
n 20",1 Introduction,[0],[0]
Morphological word representations.,2 Related work,[0],[0]
"In recent years, many methods have been proposed to incorporate morphological information into word representations.",2 Related work,[0],[0]
"To model rare words better, Alexandrescu and Kirchhoff (2006) introduced factored neural language models, where words are represented as sets of features.",2 Related work,[0],[0]
"These features might include morphological information, and this technique was succesfully applied to morphologically rich languages, such as Turkish (Sak et al., 2010).",2 Related work,[0],[0]
"Recently, several works have proposed different composition functions to derive representations of words from morphemes (Lazaridou et al., 2013; Luong et al., 2013; Botha and Blunsom, 2014; Qiu et al., 2014).",2 Related work,[0],[0]
"These different approaches rely on a morphological decomposition of words, while ours does not.",2 Related work,[0],[0]
"Similarly, Chen et al. (2015) introduced a method to jointly learn embeddings for Chinese words and characters.",2 Related work,[0],[0]
Cui et al. (2015) proposed to constrain morphologically similar words to have similar representations.,2 Related work,[0],[0]
"Soricut and Och (2015) described a method to learn vector representations of morphological transformations, allowing to obtain representations for unseen words by applying these rules.",2 Related work,[0],[0]
Word representations trained on morphologically annotated data were introduced by Cotterell and Schütze (2015).,2 Related work,[0],[0]
"Closest to our approach, Schütze (1993) learned representations of character four-grams through singular value decomposition, and derived representations for words by summing the four-grams representations.",2 Related work,[0],[0]
"Very recently, Wieting et al. (2016) also proposed to represent words using character n-gram count vectors.",2 Related work,[0],[0]
"However, the objective function used to learn these representations is based on paraphrase pairs, while our model can be trained on any text corpus.
",2 Related work,[0],[0]
Character level features for NLP.,2 Related work,[0],[0]
Another area of research closely related to our work are characterlevel models for natural language processing.,2 Related work,[0],[0]
These models discard the segmentation into words and aim at learning language representations directly from characters.,2 Related work,[0],[0]
"A first class of such models are recurrent neural networks, applied to language modeling (Mikolov et al., 2012; Sutskever et al., 2011; Graves, 2013; Bojanowski et al., 2015), text normalization (Chrupała, 2014), part-of-speech tag-
ging (Ling et al., 2015) and parsing (Ballesteros et al., 2015).",2 Related work,[0],[0]
"Another family of models are convolutional neural networks trained on characters, which were applied to part-of-speech tagging (dos Santos and Zadrozny, 2014), sentiment analysis (dos Santos and Gatti, 2014), text classification (Zhang et al., 2015) and language modeling (Kim et al., 2016).",2 Related work,[0],[0]
"Sperr et al. (2013) introduced a language model based on restricted Boltzmann machines, in which words are encoded as a set of character ngrams.",2 Related work,[0],[0]
"Finally, recent works in machine translation have proposed using subword units to obtain representations of rare words (Sennrich et al., 2016; Luong and Manning, 2016).",2 Related work,[0],[0]
"In this section, we propose our model to learn word representations while taking into account morphology.",3 Model,[0],[0]
"We model morphology by considering subword units, and representing words by a sum of its character n-grams.",3 Model,[0],[0]
"We will begin by presenting the general framework that we use to train word vectors, then present our subword model and eventually describe how we handle the dictionary of character n-grams.",3 Model,[0],[0]
"We start by briefly reviewing the continuous skipgram model introduced by Mikolov et al. (2013b), from which our model is derived.",3.1 General model,[0],[0]
"Given a word vocabulary of size W , where a word is identified by its index w ∈ {1, ...,W}, the goal is to learn a vectorial representation for each word w. Inspired by the distributional hypothesis (Harris, 1954), word representations are trained to predict well words that appear in its context.",3.1 General model,[0],[0]
"More formally, given a large training corpus represented as a sequence of words w1, ..., wT , the objective of the skipgram model is to maximize the following log-likelihood:
T∑ t=1 ∑ c∈Ct log p(wc | wt),
where the context Ct is the set of indices of words surrounding word wt.",3.1 General model,[0],[0]
The probability of observing a context word wc given wt will be parameterized using the aforementioned word vectors.,3.1 General model,[0],[0]
"For now, let us consider that we are given a scoring function s which maps pairs of (word, context) to scores in R.
One possible choice to define the probability of a context word is the softmax:
p(wc | wt) = es(wt, wc)∑W j=1 e s(wt, j) .
",3.1 General model,[0],[0]
"However, such a model is not adapted to our case as it implies that, given a word wt, we only predict one context word wc.
",3.1 General model,[0],[0]
The problem of predicting context words can instead be framed as a set of independent binary classification tasks.,3.1 General model,[0],[0]
Then the goal is to independently predict the presence (or absence) of context words.,3.1 General model,[0],[0]
For the word at position t we consider all context words as positive examples and sample negatives at random from the dictionary.,3.1 General model,[0],[0]
"For a chosen context position c, using the binary logistic loss, we obtain the following negative log-likelihood:
log ( 1 + e−s(wt, wc) )",3.1 General model,[0],[0]
"+ ∑
n∈Nt,c
log ( 1 + es(wt, n) ) ,
where Nt,c is a set of negative examples sampled from the vocabulary.",3.1 General model,[0],[0]
"By denoting the logistic loss function ` : x 7→ log(1 + e−x), we can re-write the objective as:
T∑ t=1 ∑ c∈Ct `(s(wt, wc))",3.1 General model,[0],[0]
+,3.1 General model,[0],[0]
"∑ n∈Nt,c `(−s(wt, n))  .",3.1 General model,[0],[0]
A natural parameterization for the scoring function s between a word wt and a context word wc is to use word vectors.,3.1 General model,[0],[0]
Let us define for each word w in the vocabulary two vectors uw and vw in Rd.,3.1 General model,[0],[0]
These two vectors are sometimes referred to as input and output vectors in the literature.,3.1 General model,[0],[0]
"In particular, we have vectors uwt and vwc , corresponding, respectively, to words wt and wc.",3.1 General model,[0],[0]
"Then the score can be computed as the scalar product between word and context vectors as s(wt, wc) = u>wtvwc .",3.1 General model,[0],[0]
"The model described in this section is the skipgram model with negative sampling, introduced by Mikolov et al. (2013b).",3.1 General model,[0],[0]
"By using a distinct vector representation for each word, the skipgram model ignores the internal structure of words.",3.2 Subword model,[0],[0]
"In this section, we propose a different scoring function s, in order to take into account this information.
",3.2 Subword model,[0],[0]
Each word w is represented as a bag of character n-gram.,3.2 Subword model,[0],[0]
"We add special boundary symbols < and > at the beginning and end of words, allowing to distinguish prefixes and suffixes from other character sequences.",3.2 Subword model,[0],[0]
"We also include the word w itself in the set of its n-grams, to learn a representation for each word (in addition to character n-grams).",3.2 Subword model,[0],[0]
"Taking the word where and n = 3 as an example, it will be represented by the character n-grams:
<wh, whe, her, ere, re>
and the special sequence
<where>.
",3.2 Subword model,[0],[0]
"Note that the sequence <her>, corresponding to the word her is different from the tri-gram her from the word where.",3.2 Subword model,[0],[0]
"In practice, we extract all the n-grams for n greater or equal to 3 and smaller or equal to 6.",3.2 Subword model,[0],[0]
"This is a very simple approach, and different sets of n-grams could be considered, for example taking all prefixes and suffixes.
",3.2 Subword model,[0],[0]
Suppose that you are given a dictionary of ngrams of size,3.2 Subword model,[0],[0]
"G. Given a word w, let us denote by Gw ⊂ {1, . . .",3.2 Subword model,[0],[0]
", G} the set of n-grams appearing in w.",3.2 Subword model,[0],[0]
We associate a vector representation zg to each n-gram g. We represent a word by the sum of the vector representations of its n-grams.,3.2 Subword model,[0],[0]
"We thus obtain the scoring function:
s(w, c) =",3.2 Subword model,[0],[0]
"∑ g∈Gw z>g vc.
",3.2 Subword model,[0],[0]
"This simple model allows sharing the representations across words, thus allowing to learn reliable representation for rare words.
",3.2 Subword model,[0],[0]
"In order to bound the memory requirements of our model, we use a hashing function that maps n-grams to integers in 1 to K.",3.2 Subword model,[0],[0]
We hash character sequences using the Fowler-Noll-Vo hashing function (specifically the FNV-1a variant).1,3.2 Subword model,[0],[0]
We set K = 2.106 below.,3.2 Subword model,[0],[0]
"Ultimately, a word is represented by its index in the word dictionary and the set of hashed n-grams it contains.",3.2 Subword model,[0],[0]
"In most experiments (except in Sec. 5.3), we compare our model to the C implementation
1http://www.isthe.com/chongo/tech/comp/fnv
of the skipgram and cbow models from the word2vec2 package.",4.1 Baseline,[0],[0]
We solve our optimization problem by performing stochastic gradient descent on the negative log likelihood presented before.,4.2 Optimization,[0],[0]
"As in the baseline skipgram model, we use a linear decay of the step size.",4.2 Optimization,[0],[0]
"Given a training set containing T words and a number of passes over the data equal to P , the step size at time t is equal to γ0(1 − tTP ), where γ0 is a fixed parameter.",4.2 Optimization,[0],[0]
"We carry out the optimization in parallel, by resorting to Hogwild (Recht et al., 2011).",4.2 Optimization,[0],[0]
All threads share parameters and update vectors in an asynchronous manner.,4.2 Optimization,[0],[0]
"For both our model and the baseline experiments, we use the following parameters: the word vectors have dimension 300.",4.3 Implementation details,[0],[0]
"For each positive example, we sample 5 negatives at random, with probability proportional to the square root of the uni-gram frequency.",4.3 Implementation details,[0],[0]
"We use a context window of size c, and uniformly sample the size c between 1 and 5.",4.3 Implementation details,[0],[0]
"In order to subsample the most frequent words, we use a rejection threshold of 10−4 (for more details, see (Mikolov et al., 2013b)).",4.3 Implementation details,[0],[0]
"When building the word dictionary, we keep the words that appear at least 5 times in the training set.",4.3 Implementation details,[0],[0]
The step size γ0 is set to 0.025 for the skipgram baseline and to 0.05 for both our model and the cbow baseline.,4.3 Implementation details,[0],[0]
"These are the default values in the word2vec package and work well for our model too.
",4.3 Implementation details,[0],[0]
"Using this setting on English data, our model with character n-grams is approximately 1.5× slower to train than the skipgram baseline.",4.3 Implementation details,[0],[0]
"Indeed, we process 105k words/second/thread versus 145k words/second/thread for the baseline.",4.3 Implementation details,[0],[0]
"Our model is implemented in C++, and is publicly available.3",4.3 Implementation details,[0],[0]
"Except for the comparison to previous work (Sec. 5.3), we train our models on Wikipedia data.4",4.4 Datasets,[0],[0]
"We downloaded Wikipedia dumps in nine languages: Arabic, Czech, German, English,
2https://code.google.com/archive/p/word2vec 3https://github.com/facebookresearch/fastText 4https://dumps.wikimedia.org
Spanish, French, Italian, Romanian and Russian.",4.4 Datasets,[0],[0]
We normalize the raw Wikipedia data using Matt Mahoney’s pre-processing perl script.5,4.4 Datasets,[0],[0]
"All the datasets are shuffled, and we train our models by doing five passes over them.",4.4 Datasets,[0],[0]
"We evaluate our model in five experiments: an evaluation of word similarity and word analogies, a comparison to state-of-the-art methods, an analysis of the effect of the size of training data and of the size of character n-grams that we consider.",5 Results,[0],[0]
We will describe these experiments in detail in the following sections.,5 Results,[0],[0]
We first evaluate the quality of our representations on the task of word similarity / relatedness.,5.1 Human similarity judgement,[0],[0]
"We do so by computing Spearman’s rank correlation coefficient (Spearman, 1904) between human judgement and the cosine similarity between the vector representations.",5.1 Human similarity judgement,[0],[0]
"For German, we compare the different models on three datasets: GUR65, GUR350 and ZG222 (Gurevych, 2005; Zesch and Gurevych, 2006).",5.1 Human similarity judgement,[0],[0]
"For English, we use the WS353 dataset introduced by Finkelstein et al. (2001) and the rare word dataset (RW), introduced by Luong et al. (2013).",5.1 Human similarity judgement,[0],[0]
"We evaluate the French word vectors on the translated dataset RG65 (Joubarne and Inkpen, 2011).",5.1 Human similarity judgement,[0],[0]
"Spanish, Arabic and Romanian word vectors are evaluated using the datasets described in (Hassan and Mihalcea, 2009).",5.1 Human similarity judgement,[0],[0]
"Russian word vectors are evaluated using the HJ dataset introduced by Panchenko et al. (2016).
",5.1 Human similarity judgement,[0],[0]
We report results for our method and baselines for all datasets in Table 1.,5.1 Human similarity judgement,[0],[0]
"Some words from these datasets do not appear in our training data, and thus, we cannot obtain word representation for these words using the cbow and skipgram baselines.",5.1 Human similarity judgement,[0],[0]
"In order to provide comparable results, we propose by default to use null vectors for these words.",5.1 Human similarity judgement,[0],[0]
"Since our model exploits subword information, we can also compute valid representations for out-of-vocabulary words.",5.1 Human similarity judgement,[0],[0]
We do so by taking the sum of its n-gram vectors.,5.1 Human similarity judgement,[0],[0]
"When OOV words are represented using
5http://mattmahoney.net/dc/textdata
null vectors we refer to our method as sisg- and sisg otherwise (Subword Information Skip Gram).
",5.1 Human similarity judgement,[0],[0]
"First, by looking at Table 1, we notice that the proposed model (sisg), which uses subword information, outperforms the baselines on all datasets except the English WS353 dataset.",5.1 Human similarity judgement,[0],[0]
"Moreover, computing vectors for out-of-vocabulary words (sisg) is always at least as good as not doing so (sisg-).",5.1 Human similarity judgement,[0],[0]
"This proves the advantage of using subword information in the form of character n-grams.
",5.1 Human similarity judgement,[0],[0]
"Second, we observe that the effect of using character n-grams is more important for Arabic, German and Russian than for English, French or Spanish.",5.1 Human similarity judgement,[0],[0]
German and Russian exhibit grammatical declensions with four cases for German and six for Russian.,5.1 Human similarity judgement,[0],[0]
"Also, many German words are compound words; for instance the nominal phrase “table tennis” is written in a single word as “Tischtennis”.",5.1 Human similarity judgement,[0],[0]
"By exploiting the character-level similarities between “Tischtennis” and “Tennis”, our model does not represent the two words as completely different words.
",5.1 Human similarity judgement,[0],[0]
"Finally, we observe that on the English Rare Words dataset (RW), our approach outperforms the
baselines while it does not on the English WS353 dataset.",5.1 Human similarity judgement,[0],[0]
This is due to the fact that words in the English WS353 dataset are common words for which good vectors can be obtained without exploiting subword information.,5.1 Human similarity judgement,[0],[0]
"When evaluating on less frequent words, we see that using similarities at the character level between words can help learning good word vectors.",5.1 Human similarity judgement,[0],[0]
"We now evaluate our approach on word analogy questions, of the form A is to B as C is to D, where D must be predicted by the models.",5.2 Word analogy tasks,[0],[0]
"We use the datasets introduced by Mikolov et al. (2013a) for English, by Svoboda and Brychcin (2016) for Czech, by Köper et al. (2015) for German and by Berardi et al. (2015) for Italian.",5.2 Word analogy tasks,[0],[0]
"Some questions contain words that do not appear in our training corpus, and we thus excluded these questions from the evaluation.
",5.2 Word analogy tasks,[0],[0]
We report accuracy for the different models in Table 2.,5.2 Word analogy tasks,[0],[0]
We observe that morphological information significantly improves the syntactic tasks; our approach outperforms the baselines.,5.2 Word analogy tasks,[0],[0]
"In contrast, it does not help for semantic questions, and even degrades the performance for German and Italian.",5.2 Word analogy tasks,[0],[0]
Note that this is tightly related to the choice of the length of character n-grams that we consider.,5.2 Word analogy tasks,[0],[0]
"We show in Sec. 5.5 that when the size of the n-grams is chosen optimally, the semantic analogies degrade
less.",5.2 Word analogy tasks,[0],[0]
"Another interesting observation is that, as expected, the improvement over the baselines is more important for morphologically rich languages, such as Czech and German.",5.2 Word analogy tasks,[0],[0]
We also compare our approach to previous work on word vectors incorporating subword information on word similarity tasks.,5.3 Comparison with morphological representations,[0],[0]
"The methods used are: the recursive neural network of Luong et al. (2013), the morpheme cbow of Qiu et al. (2014) and the morphological transformations of Soricut and Och (2015).",5.3 Comparison with morphological representations,[0],[0]
"In order to make the results comparable, we trained our model on the same datasets as the methods we are comparing to: the English Wikipedia data released by Shaoul and Westbury (2010), and the news crawl data from the 2013 WMT shared task for German, Spanish and French.",5.3 Comparison with morphological representations,[0],[0]
"We also compare our approach to the log-bilinear language model introduced by Botha and Blunsom (2014), which was trained on the Europarl and news commentary corpora.",5.3 Comparison with morphological representations,[0],[0]
"Again, we trained our model on the same data to make the results comparable.",5.3 Comparison with morphological representations,[0],[0]
"Using our model, we obtain representations of out-ofvocabulary words by summing the representations of character n-grams.",5.3 Comparison with morphological representations,[0],[0]
We report results in Table 3.,5.3 Comparison with morphological representations,[0],[0]
We observe that our simple approach performs well relative to techniques based on subword information obtained from morphological segmentors.,5.3 Comparison with morphological representations,[0],[0]
"We also observe that our approach outperforms the Soricut
and Och (2015) method, which is based on prefix and suffix analysis.",5.3 Comparison with morphological representations,[0],[0]
"The large improvement for German is due to the fact that their approach does not model noun compounding, contrary to ours.",5.3 Comparison with morphological representations,[0],[0]
"Since we exploit character-level similarities between words, we are able to better model infrequent words.",5.4 Effect of the size of the training data,[0],[0]
"Therefore, we should also be more robust to the size of the training data that we use.",5.4 Effect of the size of the training data,[0],[0]
"In order to assess that, we propose to evaluate the performance of our word vectors on the similarity task as a function of the training data size.",5.4 Effect of the size of the training data,[0],[0]
"To this end, we train our model and the cbow baseline on portions of Wikipedia of increasing size.",5.4 Effect of the size of the training data,[0],[0]
"We use the Wikipedia corpus described above and isolate the first 1, 2, 5, 10, 20, and 50 percent of the data.",5.4 Effect of the size of the training data,[0],[0]
"Since we don’t reshuffle the dataset, they are all subsets of each other.",5.4 Effect of the size of the training data,[0],[0]
"We report results in Fig. 1.
",5.4 Effect of the size of the training data,[0],[0]
"As in the experiment presented in Sec. 5.1, not all words from the evaluation set are present in the Wikipedia data.",5.4 Effect of the size of the training data,[0],[0]
"Again, by default, we use a null vector for these words (sisg-) or compute a vector by summing the n-gram representations (sisg).",5.4 Effect of the size of the training data,[0],[0]
"The out-of-vocabulary rate is growing as the dataset shrinks, and therefore the performance of sisgand cbow necessarily degrades.",5.4 Effect of the size of the training data,[0],[0]
"However, the proposed model (sisg) assigns non-trivial vectors to previously unseen words.
",5.4 Effect of the size of the training data,[0],[0]
"First, we notice that for all datasets, and all sizes, the proposed approach (sisg) performs better than
the baseline.",5.4 Effect of the size of the training data,[0],[0]
"However, the performance of the baseline cbow model gets better as more and more data is available.",5.4 Effect of the size of the training data,[0],[0]
"Our model, on the other hand, seems to quickly saturate and adding more data does not always lead to improved results.
",5.4 Effect of the size of the training data,[0],[0]
"Second, and most importantly, we notice that the proposed approach provides very good word vectors even when using very small training datasets.",5.4 Effect of the size of the training data,[0],[0]
"For instance, on the German GUR350 dataset, our model (sisg) trained on 5% of the data achieves better performance (66) than the cbow baseline trained on the full dataset (62).",5.4 Effect of the size of the training data,[0],[0]
"On the other hand, on the English RW dataset, using 1% of the Wikipedia corpus we achieve a correlation coefficient of 45 which is better than the performance of cbow trained on the full dataset (43).",5.4 Effect of the size of the training data,[0],[0]
This has a very important practical implication: well performing word vectors can be computed on datasets of a restricted size and still work well on previously unseen words.,5.4 Effect of the size of the training data,[0],[0]
"In general, when using vectorial word representations in specific applications, it is recommended to retrain the model on textual data relevant for the application.",5.4 Effect of the size of the training data,[0],[0]
"However, this kind of relevant task-specific data is often very scarce and learning from a reduced amount of training data is a great advantage.
",5.4 Effect of the size of the training data,[0],[0]
"5.5 Effect of the size of n-grams
The proposed model relies on the use of character ngrams to represent words as vectors.",5.4 Effect of the size of the training data,[0],[0]
"As mentioned in Sec. 3.2, we decided to use n-grams ranging from 3 to 6 characters.",5.4 Effect of the size of the training data,[0],[0]
"This choice was arbitrary, moti-
vated by the fact that n-grams of these lengths will cover a wide range of information.",5.4 Effect of the size of the training data,[0],[0]
They would include short suffixes (corresponding to conjugations and declensions for instance) as well as longer roots.,5.4 Effect of the size of the training data,[0],[0]
"In this experiment, we empirically check for the influence of the range of n-grams that we use on performance.",5.4 Effect of the size of the training data,[0],[0]
"We report our results in Table 4 for English and German on word similarity and analogy datasets.
",5.4 Effect of the size of the training data,[0],[0]
"We observe that for both English and German, our arbitrary choice of 3-6 was a reasonable decision, as it provides satisfactory performance across languages.",5.4 Effect of the size of the training data,[0],[0]
The optimal choice of length ranges depends on the considered task and language and should be tuned appropriately.,5.4 Effect of the size of the training data,[0],[0]
"However, due to the scarcity of test data, we did not implement any proper validation procedure to automatically select the best parameters.",5.4 Effect of the size of the training data,[0],[0]
"Nonetheless, taking a large range such as 3 − 6 provides a reasonable amount of subword information.
",5.4 Effect of the size of the training data,[0],[0]
"This experiment also shows that it is important to include long n-grams, as columns corresponding to n ≤ 5 and n ≤ 6 work best.",5.4 Effect of the size of the training data,[0],[0]
"This is especially true for German, as many nouns are compounds made up from several units that can only be captured by longer character sequences.",5.4 Effect of the size of the training data,[0],[0]
"On analogy tasks, we observe that using larger n-grams helps for semantic analogies.",5.4 Effect of the size of the training data,[0],[0]
"However, results are always improved by taking n ≥ 3 rather than n ≥ 2, which shows that character 2-grams are not informative for that task.",5.4 Effect of the size of the training data,[0],[0]
"As described in Sec. 3.2, before computing
character n-grams, we prepend and append special positional characters to represent the beginning and end of word.",5.4 Effect of the size of the training data,[0],[0]
"Therefore, 2-grams will not be enough to properly capture suffixes that correspond to conjugations or declensions, since they are composed of a single proper character and a positional one.",5.4 Effect of the size of the training data,[0],[0]
"In this section, we describe an evaluation of the word vectors obtained with our method on a language modeling task.",5.6 Language modeling,[0],[0]
"We evaluate our language model on five languages (CS, DE, ES, FR, RU) using the datasets introduced by Botha and Blunsom (2014).",5.6 Language modeling,[0],[0]
"Each dataset contains roughly one million training tokens, and we use the same preprocessing and data splits as Botha and Blunsom (2014).
",5.6 Language modeling,[0],[0]
"Our model is a recurrent neural network with 650 LSTM units, regularized with dropout (with probability of 0.5) and weight decay (regularization parameter of 10−5).",5.6 Language modeling,[0],[0]
"We learn the parameters using the Adagrad algorithm with a learning rate of 0.1, clipping the gradients which have a norm larger than 1.0.",5.6 Language modeling,[0],[0]
We initialize the weight of the network in the range,5.6 Language modeling,[0],[0]
"[−0.05, 0.05], and use a batch size of 20.",5.6 Language modeling,[0],[0]
"Two baselines are considered: we compare our ap-
proach to the log-bilinear language model of Botha and Blunsom (2014) and the character aware language model of Kim et al. (2016).",5.6 Language modeling,[0],[0]
We trained word vectors with character n-grams on the training set of the language modeling task and use them to initialize the lookup table of our language model.,5.6 Language modeling,[0],[0]
"We report the test perplexity of our model without using pre-trained word vectors (LSTM), with word vectors pre-trained without subword information (sg) and with our vectors (sisg).",5.6 Language modeling,[0],[0]
"The results are presented in Table 5.
",5.6 Language modeling,[0],[0]
We observe that initializing the lookup table of the language model with pre-trained word representations improves the test perplexity over the baseline LSTM.,5.6 Language modeling,[0],[0]
The most important observation is that using word representations trained with subword information outperforms the plain skipgram model.,5.6 Language modeling,[0],[0]
We observe that this improvement is most significant for morphologically rich Slavic languages such as Czech (8% reduction of perplexity over sg) and Russian (13% reduction).,5.6 Language modeling,[0],[0]
The improvement is less significant for Roman languages such as Spanish (3% reduction) or French (2% reduction).,5.6 Language modeling,[0],[0]
"This shows the importance of subword information on the language modeling task and exhibits the usefulness
of the vectors that we propose for morphologically rich languages.",5.6 Language modeling,[0],[0]
We report sample qualitative results in Table 7.,6.1 Nearest neighbors.,[0],[0]
"For selected words, we show nearest neighbors according to cosine similarity for vectors trained using the proposed approach and for the skipgram baseline.",6.1 Nearest neighbors.,[0],[0]
"As expected, the nearest neighbors for complex, technical and infrequent words using our approach are better than the ones obtained using the baseline model.
",6.1 Nearest neighbors.,[0],[0]
6.2 Character n-grams and morphemes We want to qualitatively evaluate whether or not the most important n-grams in a word correspond to morphemes.,6.1 Nearest neighbors.,[0],[0]
"To this end, we take a word vector that we construct as the sum of n-grams.",6.1 Nearest neighbors.,[0],[0]
"As described in Sec. 3.2, each word w is represented as the sum of its n-grams: uw = ∑ g∈Gw zg.",6.1 Nearest neighbors.,[0],[0]
"For each n-gram g, we propose to compute the restricted representation uw\g obtained by omitting g:
uw\g = ∑
g′∈G−{g}
zg′ .
",6.1 Nearest neighbors.,[0],[0]
We then rank n-grams by increasing value of cosine between uw and uw\g.,6.1 Nearest neighbors.,[0],[0]
"We show ranked n-grams for selected words in three languages in Table 6.
",6.1 Nearest neighbors.,[0],[0]
"For German, which has a lot of compound nouns, we observe that the most important n-grams cor-
respond to valid morphemes.",6.1 Nearest neighbors.,[0],[0]
Good examples include Autofahrer (car driver) whose most important n-grams are Auto (car) and Fahrer (driver).,6.1 Nearest neighbors.,[0],[0]
"We also observe the separation of compound nouns into morphemes in English, with words such as lifetime or starfish.",6.1 Nearest neighbors.,[0],[0]
"However, for English, we also observe that n-grams can correspond to affixes in words such as kindness or unlucky.",6.1 Nearest neighbors.,[0],[0]
"Interestingly, for French we observe the inflections of verbs with endings such as ais>, ent> or ions>.",6.1 Nearest neighbors.,[0],[0]
"As described in Sec. 3.2, our model is capable of building word vectors for words that do not appear in the training set.",6.3 Word similarity for OOV words,[0],[0]
"For such words, we simply average the vector representation of its n-grams.",6.3 Word similarity for OOV words,[0],[0]
"In order to assess the quality of these representations, we analyze which of the n-grams match best for OOV words by selecting a few word pairs from the English RW similarity dataset.",6.3 Word similarity for OOV words,[0],[0]
We select pairs such that one of the two words is not in the training vocabulary and is hence only represented by its ngrams.,6.3 Word similarity for OOV words,[0],[0]
"For each pair of words, we display the cosine similarity between each pair of n-grams that appear
in the words.",6.3 Word similarity for OOV words,[0],[0]
"In order to simulate a setup with a larger number of OOV words, we use models trained on 1% of the Wikipedia data as in Sec. 5.4.",6.3 Word similarity for OOV words,[0],[0]
"The results are presented in Fig. 2.
",6.3 Word similarity for OOV words,[0],[0]
"We observe interesting patterns, showing that subwords match correctly.",6.3 Word similarity for OOV words,[0],[0]
"Indeed, for the word chip, we clearly see that there are two groups of n-grams in microcircuit that match well.",6.3 Word similarity for OOV words,[0],[0]
"These roughly correspond to micro and circuit, and n-grams in between don’t match well.",6.3 Word similarity for OOV words,[0],[0]
Another interesting example is the pair rarity and scarceness.,6.3 Word similarity for OOV words,[0],[0]
"Indeed, scarce roughly matches rarity while the suffix -ness matches -ity very well.",6.3 Word similarity for OOV words,[0],[0]
"Finally, the word preadolescent matches young well thanks to the -adolescsubword.",6.3 Word similarity for OOV words,[0],[0]
This shows that we build robust word representations where prefixes and suffixes can be ignored if the grammatical form is not found in the dictionary.,6.3 Word similarity for OOV words,[0],[0]
"In this paper, we investigate a simple method to learn word representations by taking into account subword information.",7 Conclusion,[0],[0]
"Our approach, which incorporates character n-grams into the skipgram model, is related to an idea that was introduced by Schütze (1993).",7 Conclusion,[0],[0]
"Because of its simplicity, our model trains fast and does not require any preprocessing or supervision.",7 Conclusion,[0],[0]
"We show that our model outperforms baselines that do not take into account subword information, as well as methods relying on morphological analysis.",7 Conclusion,[0],[0]
"We will open source the implementation of our model, in order to facilitate comparison of future work on learning subword representations.",7 Conclusion,[0],[0]
"We thank Marco Baroni, Hinrich Schütze and the anonymous reviewers for their insightful comments.",Acknowledgements,[0],[0]
"Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks.",abstractText,[0],[0]
"Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word.",abstractText,[0],[0]
"This is a limitation, especially for languages with large vocabularies and many rare words.",abstractText,[0],[0]
"In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams.",abstractText,[0],[0]
A vector representation is associated to each character n-gram; words being represented as the sum of these representations.,abstractText,[0],[0]
"Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data.",abstractText,[0],[0]
"We evaluate our word representations on nine different languages, both on word similarity and analogy tasks.",abstractText,[0],[0]
"By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.",abstractText,[0],[0]
Enriching Word Vectors with Subword Information,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 697–707 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Text summarization is a task to generate a shorter and concise version of a text while preserving the meaning of the original text.,1 Introduction,[0],[0]
The task can be divided into two subtask based on the approach: extractive and abstractive summarization.,1 Introduction,[0],[0]
Extractive summarization is a task to create summaries by pulling out snippets of text form the original text and combining them to form a summary.,1 Introduction,[0],[0]
"Abstractive summarization asks to generate summaries from scratch without the restriction to use
∗ Amplayo and Lim are co-first authors with equal contribution.",1 Introduction,[0],[0]
"Names are arranged alphabetically.
",1 Introduction,[0],[0]
the available words from the original text.,1 Introduction,[0],[0]
"Due to the limitations of extractive summarization on incoherent texts and unnatural methodology (Yao et al., 2017), the research trend has shifted towards abstractive summarization.
",1 Introduction,[0],[0]
"Sequence-to-sequence models (Sutskever et al., 2014) with attention mechanism (Bahdanau et al., 2014) have found great success in generating abstractive summaries, both from a single sentence (Chopra et al., 2016) and from a long document with multiple sentences (Chen et al., 2016).",1 Introduction,[0],[0]
"However, when generating summaries, it is necessary to determine the main topic and to sift out unnecessary information that can be omitted.",1 Introduction,[0],[0]
"Sequenceto-sequence models have the tendency to include all the information, relevant or not, that are found in the original text.",1 Introduction,[0],[0]
This may result to unconcise summaries that concentrates wrongly on irrelevant topics.,1 Introduction,[0],[0]
"The problem is especially severe when summarizing longer texts.
",1 Introduction,[0],[0]
"In this paper, we propose to use entities found in the original text to infer the summary topic, miti-
697
gating the aforementioned problem.",1 Introduction,[0],[0]
"Specifically, we leverage on linked entities extracted by employing a readily available entity linking system.",1 Introduction,[0],[0]
The importance of using linked entities in summarization is intuitive and can be explained by looking at Figure 1 as an example.,1 Introduction,[0],[0]
"First (O1 in the Figure), aside from auxiliary words to construct a sentence, a summary is mainly composed of linked entities extracted from the original text.",1 Introduction,[0],[0]
"Second (O2), we can depict the main topic of the summary as a probability distribution of relevant entities from the list of entities.",1 Introduction,[0],[0]
"Finally (O3), we can leverage on entity commonsense learned from a separate large knowledge base such as Wikipedia.
",1 Introduction,[0],[0]
"To this end, we present a method to effectively apply linked entities in sequence-tosequence models, called Entity2Topic (E2T).",1 Introduction,[0],[0]
E2T is a module that can be easily attached to any sequence-to-sequence based summarization model.,1 Introduction,[0],[0]
"The module encodes the entities extracted from the original text by an entity linking system (ELS), constructs a vector representing the topic of the summary to be generated, and informs the decoder about the constructed topic vector.",1 Introduction,[0],[0]
"Due to the imperfections of current ELS’s, the extracted linked entities may be too ambiguous and coarse to be considered relevant to the summary.",1 Introduction,[0],[0]
"We solve this issue by using entity encoders with selective disambiguation and by constructing topic vectors using firm attention.
",1 Introduction,[0],[0]
"We experiment on two datasets, Gigaword and CNN, with varying lengths.",1 Introduction,[0],[0]
We show that applying our module to a sequence-to-sequence model with attention mechanism significantly increases its performance on both datasets.,1 Introduction,[0],[0]
"Moreover, when compared with the state-of-the-art models for each dataset, the model obtains a comparable performance on the Gigaword dataset where the texts are short, and outperforms all competing models on the CNN dataset where the texts are longer.",1 Introduction,[0],[0]
"Furthermore, we provide analysis on how our model effectively uses the extracted linked entities to produce concise and better summaries.",1 Introduction,[0],[0]
"In the next subsections, we present detailed arguments with empirical and previously examined evidences on the observations and possible issues when using linked entities extracted by an entity linking system (ELS) for generating abstractive
summaries.",2 Usefulness of linked entities in summarization,[0],[0]
"For this purpose, we use the development sets of the Gigaword dataset provided in (Rush et al., 2015) and of the CNN dataset provided in (Hermann et al., 2015) as the experimental data for quantitative evidence and refer the readers to Figure 1 as the running example.",2 Usefulness of linked entities in summarization,[0],[0]
"As discussed in Section 1, we find three observations that show the usefulness of linked entities for abstractive summarization.
",2.1 Observations,[0],[0]
"First, summaries are mainly composed of linked entities extracted from the original text.",2.1 Observations,[0],[0]
"In the example, it can be seen that the summary contains four words that refer to different entities.",2.1 Observations,[0],[0]
"In fact, all noun phrases in the summary mention at least one linked entity.",2.1 Observations,[0],[0]
"In our experimental data, we extract linked entities from the original text and compare them to the noun phrases found in the summary.",2.1 Observations,[0],[0]
"We report that 77.1% and 75.1% of the noun phrases on the Gigaword and CNN datasets, respectively, contain at least one linked entity, which confirms our observation.
",2.1 Observations,[0],[0]
"Second, linked entities can be used to represent the topic of the summary, defined as a multinomial distribution over entities, as graphically shown in the example, where the probabilities refer to the relevance of the entities.",2.1 Observations,[0],[0]
"Entities have been previously used to represent topics (Newman et al., 2006), as they can be utilized as a controlled vocabulary of the main topics in a document (Hulpus et al., 2013).",2.1 Observations,[0],[0]
"In the example, we see that the entity “Jae Seo” is the most relevant because it is the subject of the summary, while the entity “South Korean” is less relevant because it is less important when constructing the summary.
",2.1 Observations,[0],[0]
"Third, we can make use of the entity commonsense that can be learned as a continuous vector representation from a separate larger corpus (Ni et al., 2016; Yamada et al., 2017).",2.1 Observations,[0],[0]
"In the example, if we know that the entities “Los Angeles Dodgers” and “New York Mets” are American baseball teams and “Jae Seo” is a baseball player associated with the teams, then we can use this information to generate more coherent summaries.",2.1 Observations,[0],[0]
"We find that 76.0% of the extracted linked entities are covered by the pre-trained vectors1 in our experimental data, proving our third observation.
",2.1 Observations,[0],[0]
1https://github.com/idio/wiki2vec,2.1 Observations,[0],[0]
"Despite its usefulness, linked entities extracted from ELS’s have issues because of low precision rates (Hasibi et al., 2016) and design challenges in training datasets (Ling et al., 2015).",2.2 Possible issues,[0],[0]
"These issues can be summarized into two parts: ambiguity and coarseness.
",2.2 Possible issues,[0],[0]
"First, the extracted entities may be ambiguous.",2.2 Possible issues,[0],[0]
"In the example, the entity “South Korean” is ambiguous because it can refer to both the South Korean person and the South Korean language, among others2.",2.2 Possible issues,[0],[0]
"In our experimental data, we extract (1) the top 100 entities based on frequency, and (2) the entities extracted from 100 randomly selected texts, and check whether they have disambiguation pages in Wikipedia or not.",2.2 Possible issues,[0],[0]
"We discover that 71.0% of the top 100 entities and 53.6% of the entities picked at random have disambiguation pages, which shows that most entities are prone to ambiguity problems.
",2.2 Possible issues,[0],[0]
"Second, the linked entities may also be too common to be considered an entity.",2.2 Possible issues,[0],[0]
This may introduce errors and irrelevance to the summary.,2.2 Possible issues,[0],[0]
"In the example, “Wednesday” is erroneous because it is wrongly linked to the entity “Wednesday Night Baseball”.",2.2 Possible issues,[0],[0]
"Also, “swap” is irrelevant because although it is linked correctly to the entity “Trade (Sports)”, it is too common and irrelevant when generating the summaries.",2.2 Possible issues,[0],[0]
"In our experimental data, we randomly select 100 data instances and tag the correctness and relevance of extracted entities into one of four labels: A: correct and relevant, B: correct and somewhat relevant, C: correct but irrelevant, and D: incorrect.",2.2 Possible issues,[0],[0]
"Results show that 29.4%, 13.7%, 30.0%, and 26.9% are tagged with A, B, C, and D, respectively, which shows that there is a large amount of incorrect and irrelevant entities.",2.2 Possible issues,[0],[0]
"To solve the issues described above, we present Entity2Topic (E2T), a module that can be easily attached to any sequence-to-sequence based abstractive summarization model.",3 Our model,[0],[0]
E2T encodes the linked entities extracted from the text and transforms them into a single topic vector.,3 Our model,[0],[0]
This vector is ultimately concatenated to the decoder hidden state vectors.,3 Our model,[0],[0]
"The module contains two submodules specifically for the issues presented by the en-
2https://en.wikipedia.org/wiki/South_",3 Our model,[0],[0]
"Korean
tity linking systems: the entity encoding submodule with selective disambiguation and the pooling submodule with firm attention.
",3 Our model,[0],[0]
"Overall, our full architecture can be illustrated as in Figure 2, which consists of an entity linking system (ELS), a sequence-to-sequence with attention mechanism model, and the E2T module.",3 Our model,[0],[0]
"We note that our proposed module can be easily attached to more sophisticated abstractive summarization models (Zhou et al., 2017; Tan et al., 2017) that are based on the traditional encoderdecoder framework and consequently can produce better results.",3 Our model,[0],[0]
The code of the base model and the E2T are available online3.,3 Our model,[0],[0]
"As our base model, we employ a basic encoderdecoder RNN used in most neural machine translation (Bahdanau et al., 2014) and text summarization (Nallapati et al., 2016) tasks.",3.1 Base model,[0],[0]
We employ a two-layer bidirectional GRU (BiGRU) as the recurrent unit of the encoder.,3.1 Base model,[0],[0]
"The BiGRU consists of a forward and backward GRU, which results to sequences of forward and backward hidden states ( −→ h 1, −→ h 2, ..., −→ h n) and ( ←− h 1, ←− h 2, ..., ←− h n), respectively:
−→ h i = GRU(xi, −→ h i−1) ←−",3.1 Base model,[0],[0]
"h i = GRU(xi, ←− h i+1)
",3.1 Base model,[0],[0]
The forward and backward hidden states are concatenated to get the hidden state vectors of the tokens (i.e. hi =,3.1 Base model,[0],[0]
[ −→ h i; ←− h,3.1 Base model,[0],[0]
i]).,3.1 Base model,[0],[0]
The final states of the forward and backward GRU are also concatenated to create the final text representation vector of the encoder s =,3.1 Base model,[0],[0]
[ −→ h n; ←− h 1].,3.1 Base model,[0],[0]
"These values are calculated per layer, where xt of the second layer is ht of the first layer.",3.1 Base model,[0],[0]
"The final text representation vectors are projected by a fully connected layer and are passed to the decoder as the initial hidden states s0 = s.
For the decoder, we use a two-layer unidirectional GRU with attention.",3.1 Base model,[0],[0]
"At each time step t, the previous token yt−1, the previous hidden state st−1, and the previous context vector ct−1 are passed to a GRU to calculate the new hidden state st, as shown in the equation below.
",3.1 Base model,[0],[0]
"st = GRU(wt−1, st−1, ct−1)
3https://github.com/rktamplayo/",3.1 Base model,[0],[0]
"Entity2Topic
The context vector ct is computed using the additive attention mechanism (Bahdanau et al., 2014), which matches the current decoder state st and each encoder state hi to get an importance score.",3.1 Base model,[0],[0]
The scores are then passed to a softmax and are used to pool the encoder states using weighted sum.,3.1 Base model,[0],[0]
"The final pooled vector is the context vector, as shown in the equations below.
",3.1 Base model,[0],[0]
"gt,i = v >",3.1 Base model,[0],[0]
"a tanh(Wast−1 + Uahi)
at,i = exp(gt,i)∑ i exp(gt,i)
",3.1 Base model,[0],[0]
"ct = ∑
i
at,ihi
Finally, the previous token yt−1, the current context vector ct, and the current decoder state st are used to generate the current word yt with a softmax layer over the decoder vocabulary, as shown below.
",3.1 Base model,[0],[0]
ot =Wwwt−1 +Wcct +Wsst p(yt|y<t) = softmax(Woot),3.1 Base model,[0],[0]
"After performing entity linking to the input text using the ELS, we receive a sequential list of linked entities, arranged based on their location in the text.",3.2 Entity encoding submodule,[0],[0]
"We embed these entities to d-dimensional vectors E = {e1, e2, ..., em} where ei ∈ Rd.",3.2 Entity encoding submodule,[0],[0]
"Since these entities may still contain ambiguity, it is necessary to resolve them before applying them to the base model.",3.2 Entity encoding submodule,[0],[0]
"Based on the idea that an ambiguous entity can be disambiguated using its neighboring entities, we introduce two kinds of disambiguating encoders below.
",3.2 Entity encoding submodule,[0],[0]
"Globally disambiguating encoder One way to disambiguate an entity is by using all the other entities, putting more importance to entities that are nearer.",3.2 Entity encoding submodule,[0],[0]
"For this purpose, we employ an RNNbased model to globally disambiguate the entities.",3.2 Entity encoding submodule,[0],[0]
"Specifically, we use BiGRU and concatenate the forward and backward hidden state vectors as the new entity vector:
−→ h i = GRU(ei, −→ h i−1) ←−",3.2 Entity encoding submodule,[0],[0]
h,3.2 Entity encoding submodule,[0],[0]
"i = GRU(ei, ←− h i+1)
e′i",3.2 Entity encoding submodule,[0],[0]
=,3.2 Entity encoding submodule,[0],[0]
[ −→ h i;,3.2 Entity encoding submodule,[0],[0]
←− h,3.2 Entity encoding submodule,[0],[0]
"i]
Locally disambiguating encoder Another way to disambiguate an entity is by using only the direct neighbors of the entity, putting no importance value to entities that are far.",3.2 Entity encoding submodule,[0],[0]
"To do this, we employ a CNN-based model to locally disambiguate the entities.",3.2 Entity encoding submodule,[0],[0]
"Specifically, we do the convolution operation using filter matrices Wf ∈ Rh×d with filter size h to a window of h words.",3.2 Entity encoding submodule,[0],[0]
"We do this for different sizes of h. This produces new feature vectors ci,h as shown below, where f(.) is a non-linear function:
ci,h = f([ei−(h−1)/2; ...; ei+h(+1)/2]",3.2 Entity encoding submodule,[0],[0]
">Wf + bf )
",3.2 Entity encoding submodule,[0],[0]
"The convolution operation reduces the number of entities differently depending on the filter size h. To prevent loss of information and to produce the same amount of feature vectors ci,h, we pad the entity list dynamically such that when the filter size is h, the number of paddings on each side is (h− 1)/2.",3.2 Entity encoding submodule,[0],[0]
The filter size h therefore refers to the number of entities used to disambiguate a middle entity.,3.2 Entity encoding submodule,[0],[0]
"Finally, we concatenate all feature vectors
of different h’s for each i as the new entity vector:
e′i",3.2 Entity encoding submodule,[0],[0]
=,3.2 Entity encoding submodule,[0],[0]
"[ci,h1 ; ci,h2 ; ...]
The question on which disambiguating encoder is better has been a debate; some argued that using only the local context is appropriate (Lau et al., 2013) while some claimed that additionally using global context also helps (Wang et al., 2015).",3.2 Entity encoding submodule,[0],[0]
"The RNN-based encoder is good as it smartly makes use of all entities, however it may perform bad when there are many entities as it introduces noise when using a far entity during disambiguation.",3.2 Entity encoding submodule,[0],[0]
"The CNN-based encoder is good as it minimizes the noise by totally ignoring far entities when disambiguating, however determining the appropriate filter sizes h needs engineering.",3.2 Entity encoding submodule,[0],[0]
"Overall, we argue that when the input text is short (e.g. a sentence), both encoders perform comparably, otherwise when the input text is long (e.g. a document), the CNN-based encoder performs better.
",3.2 Entity encoding submodule,[0],[0]
Selective disambiguation It is obvious that not all entities need to be disambiguated.,3.2 Entity encoding submodule,[0],[0]
"When a correctly linked and already adequately disambiguated entity is disambiguated again, it would make the entity very context-specific and might not be suitable for the summarization task.",3.2 Entity encoding submodule,[0],[0]
Our entity encoding submodule therefore uses a selective mechanism that decides whether to use the disambiguating encoder or not.,3.2 Entity encoding submodule,[0],[0]
"This is done by introducing a selective disambiguation gate d. The final entity vector ẽi is calculated as the linear transfor-
mation of ei and e′i:
e′i = encoder(ei)
d = σ(Wde ′",3.2 Entity encoding submodule,[0],[0]
"i + bd)
ẽi = d× f(Wxei",3.2 Entity encoding submodule,[0],[0]
+,3.2 Entity encoding submodule,[0],[0]
bx)+,3.2 Entity encoding submodule,[0],[0]
"(1− d)× f(Wye′i + by)
",3.2 Entity encoding submodule,[0],[0]
The full entity encoding submodule is illustrated in Figure 3.,3.2 Entity encoding submodule,[0],[0]
"Ultimately, the submodule outputs the disambiguated entity vectors Ẽ = {ẽ1, ẽ2, ..., ẽm}.",3.2 Entity encoding submodule,[0],[0]
The entity vectors Ẽ are pooled to create a single topic vector t that represents the topic of the summary.,3.3 Pooling submodule,[0],[0]
"One possible pooling technique is to use soft attention (Xu et al., 2015) on the vectors to determine the importance value of each vector, which can be done by matching each entity vector with the text vector s from the text encoder as the context vector.",3.3 Pooling submodule,[0],[0]
The entity vectors are then pooled using weighted sum.,3.3 Pooling submodule,[0],[0]
One problem with soft attention is that it considers all entity vectors when constructing the topic vector.,3.3 Pooling submodule,[0],[0]
"However, not all entities are important and necessary when generating summaries.",3.3 Pooling submodule,[0],[0]
"Moreover, a number of these entities may be erroneous and irrelevant, as reported in Section 2.2.",3.3 Pooling submodule,[0],[0]
"Soft attention gives non-negligible important scores to these entities, thus adds unnecessary noise to the construction of the topic vector.
",3.3 Pooling submodule,[0],[0]
Our pooling submodule instead uses firm attention mechanism to consider only top k entities when constructing the topic vector.,3.3 Pooling submodule,[0],[0]
"This is done in a differentiable way as follows:
G = v>a tanh(WaẼ + Uas)
",3.3 Pooling submodule,[0],[0]
"K = top k(G)
P = sparse vector(K, 0,−∞) g′i = gi + pi ai = exp(g′i)∑",3.3 Pooling submodule,[0],[0]
i exp(g ′,3.3 Pooling submodule,[0],[0]
"i)
",3.3 Pooling submodule,[0],[0]
"t = ∑
i
aiẽi
where the functions K = top k(G) gets the indices of the top k vectors in G and P = sparse vector(K, 0,−∞) creates a sparse vector where the values of K is 0 and −∞ otherwise4.",3.3 Pooling submodule,[0],[0]
"The sparse vector P is added to the original importance score vector G to create a new importance
4We use −109 to represent −∞.
score vector.",3.3 Pooling submodule,[0],[0]
"In this new vector, important scores of non-top k entities are−∞. When softmax is applied, this gives very small, negligible, and closeto-zero values to non-top k entities.",3.3 Pooling submodule,[0],[0]
The value k depends on the lengths of the input text and summary.,3.3 Pooling submodule,[0],[0]
"Moreover, when k increases towards infinity, firm attention becomes soft attention.",3.3 Pooling submodule,[0],[0]
We decide k empirically (see Section 5).,3.3 Pooling submodule,[0],[0]
Entity2Topic module extends the base model as follows.,3.4 Extending from the base model,[0],[0]
The final text representation vector s is used as a context vector when constructing the topic vector t in the pooling submodule.,3.4 Extending from the base model,[0],[0]
"The topic vector t is then concatenated to the decoder hidden state vectors si, i.e. s′i =",3.4 Extending from the base model,[0],[0]
[si; t].,3.4 Extending from the base model,[0],[0]
"The concatenated vector is finally used to create the output vector:
oi =Wwwi−1",3.4 Extending from the base model,[0],[0]
+Wcci +Wss′i,3.4 Extending from the base model,[0],[0]
"Due to its recent success, neural network models have been used with competitive results on abstractive summarization.",4 Related work,[0],[0]
"A neural attention model was first applied to the task, easily achieving stateof-the-art performance on multiple datasets (Rush et al., 2015).",4 Related work,[0],[0]
"The model has been extended to instead use recurrent neural network as decoder (Chopra et al., 2016).",4 Related work,[0],[0]
"The model was further extended to use a full RNN encoder-decoder framework and further enhancements through lexical and statistical features (Nallapati et al., 2016).",4 Related work,[0],[0]
"The current state-of-the-art performance is achieved by selectively encoding words as a process of distilling salient information (Zhou et al., 2017).
",4 Related work,[0],[0]
Neural abstractive summarization models have also been explored to summarize longer documents.,4 Related work,[0],[0]
"Word extraction models have been previously explored, performing worse than sentence extraction models (Cheng and Lapata, 2016).",4 Related work,[0],[0]
"Hierarchical attention-based recurrent neural networks have also been applied to the task, owing to the idea that there are multiple sentences in a document (Nallapati et al., 2016).",4 Related work,[0],[0]
"Finally, distractionbased models were proposed to enable models to traverse the text content and grasp the overall meaning (Chen et al., 2016).",4 Related work,[0],[0]
"The current state-ofthe-art performance is achieved by a graph-based attentional neural model, considering the key factors of document summarization such as saliency, fluency and novelty (Tan et al., 2017).
",4 Related work,[0],[0]
"Previous studies on the summarization tasks have only used entities in the preprocessing stage to anonymize the dataset (Nallapati et al., 2016) and to mitigate out-of-vocabulary problems (Tan et al., 2017).",4 Related work,[0],[0]
Linked entities for summarization are still not properly explored and we are the first to use linked entities to improve the performance of the summarizer.,4 Related work,[0],[0]
Datasets We use two widely used summarization datasets with different text lengths.,5 Experimental settings,[0],[0]
"First, we use the Annotated English Gigaword dataset as used in (Rush et al., 2015).",5 Experimental settings,[0],[0]
This dataset receives the first sentence of a news article as input and use the headline title as the gold standard summary.,5 Experimental settings,[0],[0]
"Since the development dataset is large, we randomly selected 2000 pairs as our development dataset.",5 Experimental settings,[0],[0]
"We use the same held-out test dataset used in (Rush et al., 2015) for comparison.",5 Experimental settings,[0],[0]
"Second, we use the CNN dataset released in (Hermann et al., 2015).",5 Experimental settings,[0],[0]
This dataset receives the full news article as input and use the human-generated multiple sentence highlight as the gold standard summary.,5 Experimental settings,[0],[0]
"The original dataset has been modified and preprocessed specifically for the document summarization task (Nallapati et al., 2016).",5 Experimental settings,[0],[0]
"In addition to the previously provided datasets, we extract linked entities using Dexter5 (Ceccarelli et al., 2013), an open source ELS that links text snippets found in a given text to entities contained in Wikipedia.",5 Experimental settings,[0],[0]
We use the default recommended parameters stated in the website.,5 Experimental settings,[0],[0]
"We summarize the statistics of both datasets in Table 1.
",5 Experimental settings,[0],[0]
"Implementation For both datasets, we further reduce the size of the input, output, and entity vocabularies to at most 50K as suggested in (See et al., 2017) and replace less frequent words to
5http://dexter.isti.cnr.it/
“<unk>”.",5 Experimental settings,[0],[0]
"We use 300D Glove6 (Pennington et al., 2014) and 1000D wiki2vec7 pre-trained vectors to initialize our word and entity vectors.",5 Experimental settings,[0],[0]
"For GRUs, we set the state size to 500.",5 Experimental settings,[0],[0]
"For CNN, we set h = 3, 4, 5 with 400, 300, 300 feature maps, respectively.",5 Experimental settings,[0],[0]
"For firm attention, k is tuned by calculating the perplexity of the model starting with smaller values (i.e. k = 1, 2, 5, 10, 20, ...) and stopping when the perplexity of the model becomes worse than the previous model.",5 Experimental settings,[0],[0]
Our preliminary tuning showed that k = 5 for Gigaword dataset and k = 10 for CNN dataset are the best choices.,5 Experimental settings,[0],[0]
"We use dropout (Srivastava et al., 2014) on all non-linear connections with a dropout rate of 0.5.",5 Experimental settings,[0],[0]
"We set the batch sizes of Gigaword and CNN datasets to 80 and 10, respectively.",5 Experimental settings,[0],[0]
"Training is done via stochastic gradient descent over shuffled mini-batches with the Adadelta update rule, with l2 constraint (Hinton et al., 2012) of 3.",5 Experimental settings,[0],[0]
We perform early stopping using a subset of the given development dataset.,5 Experimental settings,[0],[0]
"We use beam search of size 10 to generate the summary.
",5 Experimental settings,[0],[0]
"Baselines For the Gigaword dataset, we compare our models with the following abstractive baselines: ABS+ (Rush et al., 2015) is a fine tuned version of ABS which uses an attentive CNN encoder and an NNLM decoder, Feat2s (Nallapati et al., 2016) is an RNN sequence-to-sequence model with lexical and statistical features in the encoder, Luong-NMT (Luong et al., 2015) is a two-layer LSTM encoder-decoder model, RASElman (Chopra et al., 2016) uses an attentive CNN encoder and an Elman RNN decoder, and SEASS (Zhou et al., 2017) uses BiGRU encoders and GRU decoders with selective encoding.",5 Experimental settings,[0],[0]
"For the CNN dataset, we compare our models with the following extractive and abstractive baselines: Lead-3 is a strong baseline that extracts the first three sentences of the document as summary, LexRank extracts texts using LexRank (Erkan and Radev, 2004), Bi-GRU is a non-hierarchical one-layer sequence-to-sequence abstractive baseline, Distraction-M3 (Chen et al., 2016) uses a sequence-to-sequence abstractive model with distraction-based networks, and GBA (Tan et al., 2017) is a graph-based attentional neural abstractive model.",5 Experimental settings,[0],[0]
All baseline results used beam search and are gathered from previous papers.,5 Experimental settings,[0],[0]
"Also,
6https://nlp.stanford.edu/projects/ glove/
7https://github.com/idio/wiki2vec
we compare our final model BASE+E2T with the base model BASE and some variants of our model (without selective disambiguation, using soft attention).",5 Experimental settings,[0],[0]
"We report the ROUGE F1 scores for both datasets of all the competing models using ROUGE F1 scores (Lin, 2004).",6 Results,[0],[0]
"We report the results on the Gigaword and the CNN dataset in Table 2 and Table 3, respectively.",6 Results,[0],[0]
"In Gigaword dataset where the texts are short, our best model achieves a comparable performance with the current state-of-the-art.",6 Results,[0],[0]
"In CNN dataset where the texts are longer, our best model outperforms all the previous models.",6 Results,[0],[0]
"We emphasize that E2T module is easily attachable to better models, and we expect E2T to improve
their performance as well.",6 Results,[0],[0]
"Overall, E2T achieves a significant improvement over the baseline model BASE, with at least 2 ROUGE-1 points increase in the Gigaword dataset and 6 ROUGE-1 points increase in the CNN dataset.",6 Results,[0],[0]
"In fact, all variants of E2T gain improvements over the baseline, implying that leveraging on linked entities improves the performance of the summarizer.",6 Results,[0],[0]
"Among the model variants, the CNN-based encoder with selective disambiguation and firm attention performs the best.
",6 Results,[0],[0]
Automatic evaluation on the Gigaword dataset shows that the CNN and RNN variants of BASE+E2T have similar performance.,6 Results,[0],[0]
"To break the tie between both models, we also conduct human evaluation on the Gigaword dataset.",6 Results,[0],[0]
"We instruct two annotators to read the input sentence and rank the competing summaries from first to last according to their relevance and fluency: (a) the original summary GOLD, and from models (b) BASE, (c) BASE+E2Tcnn, and (d) BASE+E2Trnn.",6 Results,[0],[0]
We then compute (i) the proportion of every ranking of each model and (ii) the mean rank of each model.,6 Results,[0],[0]
The results are reported in Table 4.,6 Results,[0],[0]
"The model with the best mean rank is BASE+E2Tcnn, followed by GOLD, then by BASE+E2Trnn and BASE, respectively.",6 Results,[0],[0]
We also perform ANOVA and post-hoc Tukey tests to show that the CNN variant is significantly (p < 0.01) better than the RNN variant and the base model.,6 Results,[0],[0]
"The RNN variant does not perform as well as the CNN variant, contrary to the automatic ROUGE evaluation above.",6 Results,[0],[0]
"Interestingly, the CNN variant produces better (but with no significant difference) summaries than the gold summaries.",6 Results,[0],[0]
"We posit that this is due to the fact that the article title does not correspond to the summary of the first sentence.
",6 Results,[0],[0]
Selective disambiguation of entities We show the effectiveness of the selective disambiguation gate d in selecting which entities to disambiguate or not.,6 Results,[0],[0]
"Table 6 shows a total of four different examples of two entities with the highest/lowest d
values.",6 Results,[0],[0]
"In the first example, sentence E1.1 contains the entity “United States” and is linked with the country entity of the same name, however the correct linked entity should be “United States Davis Cup team”, and therefore is given a high d value.",6 Results,[0],[0]
"On the other hand, sentence E1.2 is linked correctly to the country “United States”, and thus is given a low d value..",6 Results,[0],[0]
"The second example provides a similar scenario, where sentence E2.1 is linked to the entity “Gold” but should be linked to the entity “Gold medal”.",6 Results,[0],[0]
Sentence E2.2 is linked correctly to the chemical element.,6 Results,[0],[0]
"Hence, the former case received a high value d while the latter case received a low d value.
",6 Results,[0],[0]
"Entities as summary topic Finally, we provide one sample for each dataset in Table 5 for case study, comparing our final model that uses firm attention (BASEcnn+sd), a variant that uses soft attention (BASEcnn+soft), and the baseline model (BASE).",6 Results,[0],[0]
"We also show the attention weights of the firm and soft models.
",6 Results,[0],[0]
"In the Gigaword example, we find three observations.",6 Results,[0],[0]
"First, the base model generated a less informative summary, not mentioning “mexico state” and “first edition”.",6 Results,[0],[0]
"Second, the soft model produced a factually wrong summary, saying that “guadalajara” is a mexican state, while actually it is a city.",6 Results,[0],[0]
"Third, the firm model is able to solve the problem by focusing only on the five most important entities, eliminating possible noise such as “Unk” and less crucial entities such as “Country club”.",6 Results,[0],[0]
"We can also see the effectiveness of the selective disambiguation in this example, where the entity “U.S. state” is corrected to mean the entity “Mexican state” which becomes relevant and is therefore selected.
",6 Results,[0],[0]
"In the CNN example, we also find that the baseline model generated a very erroneous summary.",6 Results,[0],[0]
We argue that this is because the length of the input text is long and the decoder is not guided as to which topics it should focus on.,6 Results,[0],[0]
"The soft model generated a much better summary, however it focuses on the wrong topics, specifically on “Iran’s nuclear program”, making the summary less general.",6 Results,[0],[0]
A quick read of the original article tells us that the main topic of the article is all about the two political parties arguing over the deal with Iran.,6 Results,[0],[0]
"However, the entity “nuclear” appeared a lot in the article, which makes the soft model wrongly focus on the “nuclear” entity.",6 Results,[0],[0]
"The firm model produced the more relevant summary, focusing on the po-
litical entities (e.g. “republicans”, “democrats”).",6 Results,[0],[0]
This is due to the fact that only the k = 10 most important elements are attended to create the summary topic vector.,6 Results,[0],[0]
We proposed to leverage on linked entities to improve the performance of sequence-to-sequence models on neural abstractive summarization task.,7 Conclusion,[0],[0]
Linked entities are used to guide the decoding process based on the summary topic and commonsense learned from a knowledge base.,7 Conclusion,[0],[0]
"We introduced Entity2Topic (E2T), a module that is easily attachable to any model using an encoder-decoder framework.",7 Conclusion,[0],[0]
E2T applies linked entities into the summarizer by encoding the entities with selective disambiguation and pooling them into one summary topic vector with firm attention mechanism.,7 Conclusion,[0],[0]
"We showed that by applying E2T to a basic
sequence-to-sequence model, we achieve significant improvements over the base model and consequently achieve a comparable performance with more complex summarization models.",7 Conclusion,[0],[0]
We would like to thank the three anonymous reviewers for their valuable feedback.,Acknowledgement,[0],[0]
"This work was supported by Microsoft Research, and Institute for Information communications Technology Promotion (IITP) grant funded by the Korea government (MSIT) (No.2017-0-01778 , Development of Explainable Humanlevel Deep Machine Learning Inference Framework).",Acknowledgement,[0],[0]
S. Hwang is a corresponding author.,Acknowledgement,[0],[0]
A major proportion of a text summary includes important entities found in the original text.,abstractText,[0],[0]
These entities build up the topic of the summary.,abstractText,[0],[0]
"Moreover, they hold commonsense information once they are linked to a knowledge base.",abstractText,[0],[0]
"Based on these observations, this paper investigates the usage of linked entities to guide the decoder of a neural text summarizer to generate concise and better summaries.",abstractText,[0],[0]
"To this end, we leverage on an off-the-shelf entity linking system (ELS) to extract linked entities and propose Entity2Topic (E2T), a module easily attachable to a sequence-to-sequence model that transforms a list of entities into a vector representation of the topic of the summary.",abstractText,[0],[0]
"Current available ELS’s are still not sufficiently effective, possibly introducing unresolved ambiguities and irrelevant entities.",abstractText,[0],[0]
"We resolve the imperfections of the ELS by (a) encoding entities with selective disambiguation, and (b) pooling entity vectors using firm attention.",abstractText,[0],[0]
"By applying E2T to a simple sequenceto-sequence model with attention mechanism as base model, we see significant improvements of the performance in the Gigaword (sentence to title) and CNN (long document to multi-sentence highlights) summarization datasets by at least 2 ROUGE points.",abstractText,[0],[0]
Entity Commonsense Representation for Neural Abstractive Summarization,title,[0],[0]
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 68–77 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Query understanding has been an important research area in information retrieval and natural language processing (Croft et al., 2010).",1 Introduction,[0],[0]
"A key part of this problem is entity linking, which aims to annotate the entities in the query and link them to a knowledge base such as Freebase and
∗Contribution during internship at Microsoft Research.
Wikipedia.",1 Introduction,[0],[0]
"This problem has been extensively studied over the recent years (Ling et al., 2015; Usbeck et al., 2015; Cornolti et al., 2016).
",1 Introduction,[0],[0]
"The mainstream methods of entity linking for queries can be summed up in three steps: mention detection, candidate generation, and entity disambiguation.",1 Introduction,[0],[0]
The first step is to recognize candidate mentions in the query.,1 Introduction,[0],[0]
"The most common method to detect mentions is to search a dictionary collected by the entity alias in a knowledge base and the human-maintained information in Wikipedia (such as anchors, titles and redirects) (Laclavik et al., 2014).",1 Introduction,[0],[0]
The second step is to generate candidates by mapping mentions to entities.,1 Introduction,[0],[0]
It usually uses all possible senses of detected mentions as candidates.,1 Introduction,[0],[0]
"Hereafter, we refer to these two steps of generating candidate entities as entity search.",1 Introduction,[0],[0]
"Finally, they disambiguate and prune candidate entities, which is usually implemented with a ranking framework.
",1 Introduction,[0],[0]
There are two main issues in entity search.,1 Introduction,[0],[0]
"First, a mention may be linked to many entities.",1 Introduction,[0],[0]
The methods using entity search usually leverage little context information in the query.,1 Introduction,[0],[0]
"Therefore it may generate many completely irrelevant entities for the query, which brings challenges to the ranking phase.",1 Introduction,[0],[0]
"For example, the mention “Austin” usually represents the capital of Texas in the United States.",1 Introduction,[0],[0]
"However, it can also be linked to “Austin, Western Australia”, “Austin, Quebec”, “Austin (name)”, “Austin College”, “Austin (song)” and 31 other entities in the Wikipedia page of “Austin (disambiguation)”.",1 Introduction,[0],[0]
"For the query “blake shelton austin lyrics”, Blake Shelton is a singer and made his debut with the song “Austin”.",1 Introduction,[0],[0]
The entity search method detects the mention “austin” using the dictionary.,1 Introduction,[0],[0]
"However, while “Austin (song)” is most related to the context “blake shelton” and “lyrics”, the mention “austin” may be linked to all the above entities as candidates.",1 Introduction,[0],[0]
"Therefore candidate gener-
68
ation with entity search generates too many candidates especially for a common anchor text with a large number of corresponding entities.",1 Introduction,[0],[0]
"Second, it is hard to recognize entities with common surface names.",1 Introduction,[0],[0]
The common methods usually define a feature called “link-probability” as the probability that a mention is annotated in all documents.,1 Introduction,[0],[0]
There is an issue with this probability being static whatever the query is.,1 Introduction,[0],[0]
We show an example with the query “her film”.,1 Introduction,[0],[0]
“Her (film)” is a film while its surface name is usually used as a possessive pronoun.,1 Introduction,[0],[0]
"Since the static link-probability of “her” from all Wikipedia articles is very low, “her” is usually not treated as a mention linked to the entity “Her (film)”.
",1 Introduction,[0],[0]
"In this paper, we propose a novel approach to generating candidates by searching sentences from Wikipedia articles and directly using the humanannotated entities as the candidates.",1 Introduction,[0],[0]
Our approach can greatly reduce the number of candidate entities and obtain the query sensitive prior probability.,1 Introduction,[0],[0]
We take the query “blake shelton austin lyrics” as an example.,1 Introduction,[0],[0]
"Below we show a sentence in the Wikipedia page of “Austin (song)”.
",1 Introduction,[0],[0]
"In the above sentence, the mentions “Austin” and “Blake Shelton” in square brackets are annotated to the entity “Austin (song)” and “Blake Shelton”, respectively.",1 Introduction,[0],[0]
We generate candidates by searching sentences and thus obtain “Blake Shelton” as well as “Austin (song)” from this example.,1 Introduction,[0],[0]
We reduce the number of candidates because many irrelevant entities linked by “austin” do not occur in returned sentences.,1 Introduction,[0],[0]
"In addition, as previous methods generate candidates by searching entities without the query information, “austin” can be linked to “Austin, Texas” with much higher static link-probability than all other senses of “austin”.",1 Introduction,[0],[0]
"However, the number of returned sentences that contain “Austin, Texas” is close to the number of sentences that contain “Austin (song)” in our system.",1 Introduction,[0],[0]
We show another example with the query “her film” in Table 2.,1 Introduction,[0],[0]
"In this sentence, “Her”, “romantic”, “science fiction”, “comedy-drama” and “Spike Jonze” are annotated to corresponding en-
tities.",1 Introduction,[0],[0]
"As “Her” is annotated to “Her (film)” by humans in this example, we have strong evidence to annotate it even if it is usually used as a possessive pronoun with very low static link-probability.
",1 Introduction,[0],[0]
We obtain the anchors as well as corresponding entities and map them to the query after searching similar sentences.,1 Introduction,[0],[0]
Then we build a regression based framework to rank the candidates.,1 Introduction,[0],[0]
"We use a rich set of features, such as link-probability, context-matching, word embeddings, and relatedness among candidate entities as well as their related entities.",1 Introduction,[0],[0]
We evaluate our method on the ERD14 and GERDAQ datasets.,1 Introduction,[0],[0]
Experimental results show that our method outperforms state-ofthe-art systems and yields 75.0% and 56.9% in terms of F1 metric on the ERD14 dataset and the GERDAQ dataset respectively.,1 Introduction,[0],[0]
Recognizing entity mentions in text and linking them to the corresponding entries helps to understand documents and queries.,2 Related Work,[0],[0]
"Most work uses the knowledge base including Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011) and Dbpedia (Olieman et al., 2014).",2 Related Work,[0],[0]
"Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages.",2 Related Work,[0],[0]
It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates.,2 Related Work,[0],[0]
They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates.,2 Related Work,[0],[0]
"Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages.",2 Related Work,[0],[0]
Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages.,2 Related Work,[0],[0]
"Milne and Witten (2008) generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold.",2 Related Work,[0],[0]
"Then they define commonness and relatedness on the hyper-link structure of Wikipedia to disambiguate candidates.
",2 Related Work,[0],[0]
"The work on linking entities in queries has been
extensively studied in recent years.",2 Related Work,[0],[0]
"TagME (Ferragina and Scaiella, 2010) is a very early work on entity linking in queries.",2 Related Work,[0],[0]
"It generates candidates by searching Wikipedia page titles, anchors and redirects.",2 Related Work,[0],[0]
"Then disambiguation exploits the structure of the Wikipedia graph, according to a voting scheme based on a relatedness measure inspired by Milne and Witten (2008).",2 Related Work,[0],[0]
"The improved version of TagME, named WAT (Piccinno and Ferragina, 2014), uses Jaccard-similarity between two pages’ in-links as a measure of relatedness and uses PageRank to rank the candidate entities.",2 Related Work,[0],[0]
"Moreover, Meij (2012) proposes a two step approach for linking tweets to Wikipedia articles.",2 Related Work,[0],[0]
"They first extract candidate concepts for each n-gram, and then use a supervised learning algorithm to classify relevant concepts.
",2 Related Work,[0],[0]
"Unlike the work which revolves around ranking entities for query spans, the Entity Recognition and Disambiguation (ERD) Challenge (Carmel et al., 2014) views entity linking in queries as the problem of finding multiple query interpretations.",2 Related Work,[0],[0]
"The SMAPH system (Cornolti et al., 2014) which wins the short-text track works in three phases: fetching, candidate-entity generation and pruning.",2 Related Work,[0],[0]
"First, they fetch the snippets returned by a commercial search engine.",2 Related Work,[0],[0]
"Next, snippets are parsed to identify candidate entities by looking at the boldfaced parts of the search snippets.",2 Related Work,[0],[0]
"Finally, they implement a binary classifier using a set of features such as the coherence and robustness of the annotation process and the ranking as well as composition of snippets.",2 Related Work,[0],[0]
"They further extend SMAPH-1 to SMAPH-2 (Cornolti et al., 2016).",2 Related Work,[0],[0]
"They use the annotator WAT to annotate the snippets of search results to generate candidates and joint the additionally link-back step as well as the pruning step in the ranking phase, which gets the state-of-theart results on the ERD14 dataset and their released dataset GERDAQ.",2 Related Work,[0],[0]
"There is another work closed to SMAPH that uses information of query logs and anchor texts (Blanco et al., 2015), which gives a ranked list of entities and is evaluated by means of typical ranking metrics.
",2 Related Work,[0],[0]
Our work is different from using search engines to generate candidates.,2 Related Work,[0],[0]
We firstly propose to search Wikipedia sentences and take advantage of human annotations to generate candidates.,2 Related Work,[0],[0]
"The previous work, such as SMAPH, employs search engine for candidate generation, which puts queries in a larger context in which it is easier to
make sense of them.",2 Related Work,[0],[0]
"However, it uses WAT, an entity search based tool, to pre-annotate the snippets for candidate generation, which falls back the issues of entity search.",2 Related Work,[0],[0]
"As shown in Figure 1, we introduce our approach with the query “blake shelton austin lyrics”.",3 Our Approach,[0],[0]
"Our approach consists of three main phases: sentence search, candidate generation, and candidate ranking.",3 Our Approach,[0],[0]
"First, we search the query in all Wikipedia articles to obtain the similar sentences.",3 Our Approach,[0],[0]
"Second, we extract human-annotated entities from these sentences.",3 Our Approach,[0],[0]
"We keep the entities whose corresponding anchor texts occur in the query as candidates, and treat others as related entities.",3 Our Approach,[0],[0]
"Specifically, we obtain three candidates in this example, namely “Blake Shelton”, “Austin, Texas”, and “Austin (song)”.",3 Our Approach,[0],[0]
"Finally, we use a regression based model to rank the candidate entities.",3 Our Approach,[0],[0]
We get the final annotations of “Blake Shelton” and “Austin (song)” whose scores are higher than the threshold selected on the development set.,3 Our Approach,[0],[0]
"In the following sections, we describe these three phases in detail.",3 Our Approach,[0],[0]
Sentences in Wikipedia articles usually contain anchors linking to entities.,3.1 Sentence Search,[0],[0]
We are therefore motivated to generate the candidate entities based on the sentence search instead of the common method using entity search.,3.1 Sentence Search,[0],[0]
There are some issues in the original annotations because of the annotation regulation.,3.1 Sentence Search,[0],[0]
"First, entities in their own pages are usually not annotated.",3.1 Sentence Search,[0],[0]
Thus we annotate these entities with matching between the text and the page title.,3.1 Sentence Search,[0],[0]
"Second, entities are usually annotated only in their first appearance.",3.1 Sentence Search,[0],[0]
We annotate these entities if they are annotated in previous sentences in the page.,3.1 Sentence Search,[0],[0]
"Moreover, pronouns are widely used in Wikipedia sentences and are usually not annotated.",3.1 Sentence Search,[0],[0]
"We use the Stanford CoreNLP toolkit (Manning et al., 2014) to do the coreference resolution.",3.1 Sentence Search,[0],[0]
"In addition, we use the content in the disambiguation page and the infobox.",3.1 Sentence Search,[0],[0]
"Although these two kinds of information may have incomplete grammatical structure, it contains enough context information for the sentence search in our task.
",3.1 Sentence Search,[0],[0]
"We use the Wikipedia snapshot of May 1, 2016, which contains 4.45 million pages and 120 million sentences.",3.1 Sentence Search,[0],[0]
"We extract sentences that contain at least one anchor in the Wikipedia articles, and
extract human-annotated anchors as well as corresponding entities in the sentences.",3.1 Sentence Search,[0],[0]
The original annotation contains 82.6 million anchors.,3.1 Sentence Search,[0],[0]
We obtain 110 million annotated anchors in 48.4 million sentences after the incrementally annotation.,3.1 Sentence Search,[0],[0]
All of above annotations are indexed by Lucene1 by building documents consisting of two fields: the first one contains the sentence and the second one contains all anchors with their corresponding entities.,3.1 Sentence Search,[0],[0]
"For each query, we search it with Lucene using its default ranker2 based on the vector space model and tf-idf to obtain the top K sentences (K is selected on the development set).",3.1 Sentence Search,[0],[0]
We extract all entities as the related entities and use these sentences as their support sentences.,3.1 Sentence Search,[0],[0]
We back-map anchors and corresponding entities extracted in sentences to generate candidates.,3.2 Candidate Generation,[0],[0]
"We use (a, e) to denote the pair of the anchor text and corresponding entity and use w(a, e) to denote the number of sentences containing the pair (a, e).",3.2 Candidate Generation,[0],[0]
"Then, we prune the candidate pairs according to following rules.
",3.2 Candidate Generation,[0],[0]
"First, we only keep the pair whose corresponding anchor text a occurs in the query as a candidate, which has been used in previous work (Ferragina and Scaiella, 2010).",3.2 Candidate Generation,[0],[0]
"Second, we follow the long-string match strategy.",3.2 Candidate Generation,[0],[0]
"If we have two pairs (a1, e1) and (a2, e2) while a1 is a substring of
1http://lucene.apache.org 2Details can be found in https://lucene.apache.
org/core/2_9_4/api/core/org/apache/ lucene/search/Similarity.html
a2, we drop (a1, e1) if w(a1, e1) <",3.2 Candidate Generation,[0],[0]
"w(a2, e2).",3.2 Candidate Generation,[0],[0]
This is because a2 is typically less ambiguous than a1.,3.2 Candidate Generation,[0],[0]
"For example, for the query “mesa community college football”, we can obtain the anchor “mesa”, “college”, “community college”, and “mesa community college”.",3.2 Candidate Generation,[0],[0]
We only keep “mesa community college” because it is longest and occurs most times in returned sentences.,3.2 Candidate Generation,[0],[0]
"However, if w(a1, e1) > w(a2, e2), we keep both candidate pairs because a1 is more common in the query.
",3.2 Candidate Generation,[0],[0]
"In addition, we keep the entity whose surface form is the same with the anchor text and prune others.",3.2 Candidate Generation,[0],[0]
"If we have two pairs (a, e1) and (a, e2) with the same anchor, and only e2 occurs in the query, we drop the pair (a, e1)",3.2 Candidate Generation,[0],[0]
"if w(a, e1) < w(a, e2).",3.2 Candidate Generation,[0],[0]
"For example, for the query “business day south africa”, the anchor “south africa” can be linked to “south africa”, “union of south africa”, and “south africa cricket team”.",3.2 Candidate Generation,[0],[0]
We only keep the entity “south africa”.,3.2 Candidate Generation,[0],[0]
We build a regression based framework to rank the candidate entities.,3.3 Candidate Ranking,[0],[0]
"In the training phase, we treat the candidates that are equal to the ground truth as the positive samples and the others as negative samples.",3.3 Candidate Ranking,[0],[0]
The regression object of the positive sample is set to the score 1.0.,3.3 Candidate Ranking,[0],[0]
The negative sample is set to the maximum score of overlapping ratio of tokens between its text and each gold answer.,3.3 Candidate Ranking,[0],[0]
The regression object of the negative sample is not simply set to 0 in order to give a small score if the candidate is very closed to the ground truth.,3.3 Candidate Ranking,[0],[0]
We find it benefits the final results.,3.3 Candidate Ranking,[0],[0]
"We use LIBLIN-
EAR (Fan et al., 2008) with L2-regularized L2loss support vector regression to train the regression model.",3.3 Candidate Ranking,[0],[0]
"The object function is to minimize
wT w/2+C ∑ max(0, |yi−wT xi|−eps)2 (1)
where xi is the feature set, yi is the object score and w is the parameter to be learned.",3.3 Candidate Ranking,[0],[0]
"We follow the default setting that C is set to 1 and eps is set to 0.1.
",3.3 Candidate Ranking,[0],[0]
"In the test phase, each candidate gets a score of wT xi and then we only output the candidate whose score is higher than the threshold selected on the development set.
",3.3 Candidate Ranking,[0],[0]
We employ four different feature sets to capture the quality of a candidate from different aspects.,3.3 Candidate Ranking,[0],[0]
"All features are shown in Table 3.
",3.3 Candidate Ranking,[0],[0]
"Context-Independent Features This feature set measures each annotation pair (a, e) without context information.",3.3 Candidate Ranking,[0],[0]
Feature 1-4 catch the syntactic properties of the candidate.,3.3 Candidate Ranking,[0],[0]
"Feature 5 is the number of returned sentences that contain (a, e).",3.3 Candidate Ranking,[0],[0]
Feature 6 is the maximum search score (returned by Lucene) in its support sentences.,3.3 Candidate Ranking,[0],[0]
"Moreover, inspired by TagME (Ferragina and Scaiella, 2010), we denote freq(a) as the number of times the text a occurs in Wikipedia.",3.3 Candidate Ranking,[0],[0]
We use link(a) to denote the number of times the text a occurs as an anchor.,3.3 Candidate Ranking,[0],[0]
We use lp(a) = link(a)/freq(a) to denote the static link-probability that an occurrence of a has been set as an anchor.,3.3 Candidate Ranking,[0],[0]
"We use freq(a, e) to denote the number of times that the anchor text a links to the entity e, and use pr(e|a) = freq(a, e)/link(a) to denote the static prior-probability that the anchor text a links to e. Features 7 and 8 are these two probabilities.
",3.3 Candidate Ranking,[0],[0]
Context-Matching Features We treat the other words except for the anchor text as the context.,3.3 Candidate Ranking,[0],[0]
This feature set measures the context matching to the query.,3.3 Candidate Ranking,[0],[0]
Feature 9 is the context matching score calculated by tokens.,3.3 Candidate Ranking,[0],[0]
We denote c as the set of context words.,3.3 Candidate Ranking,[0],[0]
"For each ci in c, the cm sc(ci) is the ratio of times that ci occurs in the support sentences, and cm sc(c) = 1N ∑ cm sc(ci).",3.3 Candidate Ranking,[0],[0]
"Features 10 and 11 are the ratio of context words occurring in the first sentence in the entity page and the description of entity’s disambiguation page (if existed), respectively.",3.3 Candidate Ranking,[0],[0]
"Moreover, we train a 300- dimensional word embeddings on all Wikipedia articles by word2vec (Mikolov et al., 2013) and use the average embedding of each word as the
sentence representation.",3.3 Candidate Ranking,[0],[0]
Feature 12 is the maximum cosine score between the query and each support sentence.,3.3 Candidate Ranking,[0],[0]
"Features 13 and 14 are calculated with the first sentence in the entity’s page and the description in the disambiguation page.
",3.3 Candidate Ranking,[0],[0]
Relatedness Features of Candidate Entities This set of features measures how much an entity is supported by other candidates.,3.3 Candidate Ranking,[0],[0]
Feature 15 is the number of other candidate entities occurring in the support sentences.,3.3 Candidate Ranking,[0],[0]
"Feature 16 is the number of candidate entities occurring in the same Wikipedia page with the current entity.
",3.3 Candidate Ranking,[0],[0]
Relatedness Features to Related Entities This set of features measures the relatedness between candidates and related entities outside of queries.,3.3 Candidate Ranking,[0],[0]
Related entities can provide useful signals for disambiguating the candidates.,3.3 Candidate Ranking,[0],[0]
"Features 17 and 18 are analogous features with features 15 and 16, which are calculated by the related entities.",3.3 Candidate Ranking,[0],[0]
We conduct experiments on the ERD14 and GERDAQ datasets.,4 Experiment,[0],[0]
"We compare with several baseline annotators and experimental results show that
our method outperforms the baseline on these two datasets.",4 Experiment,[0],[0]
We also report the parameter selection on each dataset and analyze the quality of the candidates using different methods.,4 Experiment,[0],[0]
"ERD143 is a benchmark dataset in the ERD Challenge (Carmel et al., 2014), which contains both long-text track and short-text track.",4.1 Dataset,[0],[0]
In this paper we only focus on the short-text track.,4.1 Dataset,[0],[0]
It contains 500 queries as the development set and 500 queries as the test set.,4.1 Dataset,[0],[0]
"Due to the lack of training set, we use the development set to do the model training and tuning.",4.1 Dataset,[0],[0]
This dataset can be evaluated by both Freebase and Wikipedia as the ERD Challenge Organizers provide the Freebase Wikipedia Mapping with one-to-one correspondence of entities between two knowledge bases.,4.1 Dataset,[0],[0]
"We use Wikipedia to evaluate our results.
",4.1 Dataset,[0],[0]
GERDAQ4 is a benchmark dataset to annotate entities to Wikipedia built by Cornolti et al. (2016).,4.1 Dataset,[0],[0]
"It contains 500 queries for training, 250 for development, and 250 for test.",4.1 Dataset,[0],[0]
The query in this dataset is sampled from the KDD-Cup 2005 and then annotated manually.,4.1 Dataset,[0],[0]
Both name entities and common concepts are annotated in this dataset.,4.1 Dataset,[0],[0]
"We use average F1 designed by ERD Challenge (Carmel et al., 2014) as the evaluation metrics.",4.2 Evaluation Metric,[0],[0]
"Specifically, given a query q, with labeled entities Â = {Ê1, . . .",4.2 Evaluation Metric,[0],[0]
", Ên}.",4.2 Evaluation Metric,[0],[0]
"We define the Fmeasure of a set of hypothesized interpretations A = {E1, . . .",4.2 Evaluation Metric,[0],[0]
", Em} as follows:
Precision = |Â ∩A| |A| , Recall = |Â ∩A| |Â| (2) F1 = 2× Precision×Recall
Precision + Recall (3)
",4.2 Evaluation Metric,[0],[0]
"The average F1 of the evaluation set is the average of the F1 for each query:
AverageF1 = 1 N N∑ i=1",4.2 Evaluation Metric,[0],[0]
"F1(qi) (4)
",4.2 Evaluation Metric,[0],[0]
"Following the evaluation guideline in ERD14 and GERDAQ, we define recall to be 1.0 if the gold binding of a query is empty and define precision to be 1.0 if the hypothesized interpretation is empty.
3http://web-ngram.research.microsoft.",4.2 Evaluation Metric,[0],[0]
"com/erd2014/Datasets.aspx
4http://acube.di.unipi.it/datasets",4.2 Evaluation Metric,[0],[0]
"We compare with several baselines and use the results reported by the ERD organizer and Cornolti et al. (2016).
",4.3 Baseline Methods,[0],[0]
"AIDA (Hoffart et al., 2011) searches the mention using Stanford NER Tagger based on YAGO2.",4.3 Baseline Methods,[0],[0]
We select AIDA as a representative system aiming to entity linking for documents following the work in Cornolti et al. (2016).,4.3 Baseline Methods,[0],[0]
"WAT (Piccinno and Ferragina, 2014) is the improved version of TagME (Ferragina and Scaiella, 2010).",4.3 Baseline Methods,[0],[0]
"Magnetic IISAS (Laclavik et al., 2014) retrieves the index extracted from Wikipedia, Freebase and Dbpedia.",4.3 Baseline Methods,[0],[0]
Then it exploits Wikipedia link graph to assess the similarity of candidate entities for disambiguation and filtering.,4.3 Baseline Methods,[0],[0]
"Seznam (Eckhardt et al., 2014) uses Wikipedia and DBpedia to generate candidates.",4.3 Baseline Methods,[0],[0]
The disambiguation step is based on PageRank over the graph.,4.3 Baseline Methods,[0],[0]
"NTUNLP (Chiu et al., 2014) searches the query to match Freebase surface forms.",4.3 Baseline Methods,[0],[0]
The disambiguation step is built on top of TagME and Wikipedia.,4.3 Baseline Methods,[0],[0]
"SMAPH-1 (Cornolti et al., 2014) is the winner in the short-text track in the ERD14 Challenge.",4.3 Baseline Methods,[0],[0]
"SMAPH-2 (Cornolti et al., 2016) is the improved version of SMAPH-1.",4.3 Baseline Methods,[0],[0]
It generates candidates from the snippets of search results returned by the Bing search engine.,4.3 Baseline Methods,[0],[0]
"We report results on the ERD datset and GERDAQ dataset in Table 4 and Table 5, respectively.",4.4 Result,[0],[0]
"On the ERD14 dataset, WAT is superior to AIDA but it is still up to 10% than SMAPH-1 that wins the ERD Challenge.",4.4 Result,[0],[0]
SMAPH-2 improves 2% than SMAPH-1.,4.4 Result,[0],[0]
Our system significantly outperforms the state-of-the-art annotator SMAPH-2 by 4.2%.,4.4 Result,[0],[0]
"On the GERDAQ dataset, our system is 2.5% superior to the state-of-the-art annotator SMAPH-2.",4.4 Result,[0],[0]
"The F1 score in this dataset is much lower than the ERD dataset because common concepts such as “Week” and “Game” that are not annotated in the ERD dataset are annotated in the GERDAQ dataset.
",4.4 Result,[0],[0]
"Spell checking has been widely used in the baseline annotators as it is not uncommon in queries (Laclavik et al., 2014).",4.4 Result,[0],[0]
"The SMAPH system that generates candidates by search results implicitly leverages the spell-checking embedded in
search engines.",4.4 Result,[0],[0]
"In our experiments, spell checking improves 1.0% on the ERD dataset and 7.6% on the GERDAQ dataset.",4.4 Result,[0],[0]
"Furthermore, only 6.9% of queries in the ERD14 dataset have spelling mistakes, whereas the number in the GERDAQ dataset is 23.0%.",4.4 Result,[0],[0]
"Thus spell-checking is more important in the GERDAQ dataset.
",4.4 Result,[0],[0]
The result decreases 0.6% on the ERD dataset and 1.1% on the GERDAQ dataset without the additional annotation.,4.4 Result,[0],[0]
"Furthermore, while the F1 score decreases 2.4% on the ERD dataset and 1.4% on the GERDAQ dataset without the context features, the score only decreases 0.5% on the ERD dataset and 0.2% on the GERDAQ dataset without the relatedness features.",4.4 Result,[0],[0]
"Unlike the work on entity linking for documents (Eckhardt et al., 2014; Witten and Milne, 2008) that features derived from entity relations get promising results, the context features play a more important role than the relatedness features on entity linking for
queries as search queries are short and contain fewer entities than documents.",4.4 Result,[0],[0]
"There are two parameters in our framework, namely the number of search sentences and the threshold for final output.",4.5 Parameter Selection,[0],[0]
We select these two parameters on the development set.,4.5 Parameter Selection,[0],[0]
We show the F1 score with different numbers of search sentences and thresholds in Figure 2 and Figure 3.,4.5 Parameter Selection,[0],[0]
"On the ERD development set, better results occur in the search number between 600 and 800 as well as the threshold 0.55 and 0.6.",4.5 Parameter Selection,[0],[0]
"On the GERDAQ development set, better results occur in the search number between 700 and 1000 as well as the threshold between 0.45 and 0.5.",4.5 Parameter Selection,[0],[0]
"In our experiment, we set the number of sentences to 700 and the threshold to 0.56 on the ERD dataset as well as 800 and 0.48 on the GERDAQ dataset according to the F1 scores on the development set.",4.5 Parameter Selection,[0],[0]
The main difference between our method and most previous work is that we generate candidates by searching Wikipedia sentences instead of searching entities.,4.6 Model Analysis,[0],[0]
"For generating candidates with entity search, we build a dictionary containing all anchors, titles, and redirects in Wikipedia.",4.6 Model Analysis,[0],[0]
Then we query the dictionary to get the mention and obtain corresponding entities as candidates.,4.6 Model Analysis,[0],[0]
"We use the
same pruning rules and ranking framework in our experiments, but exclude the features from support sentences because the entity search method does not contain the information.",4.6 Model Analysis,[0],[0]
The F1 score is shown in Table 6.,4.6 Model Analysis,[0],[0]
"We achieve similar results in our implementation of the method using entity search on the ERD dataset as Magnetic IISAS (Laclavik et al., 2014) which uses a similar method and ranks 4th with the F1 of 65.57 in the ERD14 Challenge.
",4.6 Model Analysis,[0],[0]
We compare the two candidate generation methods in several aspects.,4.6 Model Analysis,[0],[0]
"First, we show the overall results in Table 6.",4.6 Model Analysis,[0],[0]
The average number of candidates from our method is much smaller.,4.6 Model Analysis,[0],[0]
It is noted that the anchors from sentence search can also be found in entity search.,4.6 Model Analysis,[0],[0]
"However, we only extract the entities in the returned sentences while the methods by entity search use all entities linked by the anchors.",4.6 Model Analysis,[0],[0]
"In addition, features such as the number of sentences containing the entity from sentence search which provide query sensitive prior probability contribute to the ranking process.",4.6 Model Analysis,[0],[0]
It improves the F1 score from 73.81 to 75.01 for sentence search and from 66.46 to 69.00 for entity search.,4.6 Model Analysis,[0],[0]
"More important, the result of “ES+RF” is still significantly worse than the result of both small candidate set and Wikipedia related features that prunes irrelevant candidates at the beginning, which proves that the high-quality candidate set is very important since the larger candidate set brings in lots of noise in training a ranking model.",4.6 Model Analysis,[0],[0]
"Moreover, there are 102 queries (20.4%) without labeled entities in the ERD dataset.",4.6 Model Analysis,[0],[0]
"We only give
7 incorrect annotations in these queries while the number is 13 from entity search.",4.6 Model Analysis,[0],[0]
"Furthermore, as shown in Table 7, the coverage of our method is lower in queries with at least one entity, but we obtain better results on precision, recall and F1 in the final stage.
",4.6 Model Analysis,[0],[0]
Figure 4 illustrates the F1 score grouped by the number of candidates using entity search.,4.6 Model Analysis,[0],[0]
In almost all columns the F1 score of our method is better than the baseline.,4.6 Model Analysis,[0],[0]
"In left columns (the number of candidates is less than 10), both methods generate few candidates.",4.6 Model Analysis,[0],[0]
"The F1 score of our method is higher, which proves that we train a better ranking model because of our small but quality candidate set.",4.6 Model Analysis,[0],[0]
"Moreover, the right columns (the number of candidates is more than 10) show that the F1 score using entity search gradually decreases with the incremental candidates.",4.6 Model Analysis,[0],[0]
"However, our method based on sentence search takes advantage of context information to keep a small set of candidates, which keeps a consistent result and outperforms the baseline.",4.6 Model Analysis,[0],[0]
In this paper we address the problem of entity linking for open-domain queries.,5 Conclusion,[0],[0]
"We introduce a novel approach to generating candidate entities by searching sentences in the Wikipedia to the query, then we extract the human-annotated entities as the candidates.",5 Conclusion,[0],[0]
We implement a regression model to rank these candidates for the final output.,5 Conclusion,[0],[0]
Two experiments on the ERD dataset and the GERDAQ dataset show that our approach outperforms the baseline systems.,5 Conclusion,[0],[0]
"In this work we directly use the default ranker in Lucene for similar sentences, which can be improved in future work.",5 Conclusion,[0],[0]
We thank Ming-Wei Chang for sharing the ERD14 dataset.,Acknowledgments,[0],[0]
Chuanqi Tan and Weifeng Lv are supported by the National Natural Science Foundation of China (Grant No. 61421003).,Acknowledgments,[0],[0]
Weifeng Lv is the corresponding author of this paper.,Acknowledgments,[0],[0]
We present a simple yet effective approach for linking entities in queries.,abstractText,[0],[0]
The key idea is to search sentences similar to a query from Wikipedia articles and directly use the human-annotated entities in the similar sentences as candidate entities for the query.,abstractText,[0],[0]
"Then, we employ a rich set of features, such as link-probability, contextmatching, word embeddings, and relatedness among candidate entities as well as their related entities, to rank the candidates under a regression based framework.",abstractText,[0],[0]
"The advantages of our approach lie in two aspects, which contribute to the ranking process and final linking result.",abstractText,[0],[0]
"First, it can greatly reduce the number of candidate entities by filtering out irrelevant entities with the words in the query.",abstractText,[0],[0]
"Second, we can obtain the query sensitive prior probability in addition to the static linkprobability derived from all Wikipedia articles.",abstractText,[0],[0]
"We conduct experiments on two benchmark datasets on entity linking for queries, namely the ERD14 dataset and the GERDAQ dataset.",abstractText,[0],[0]
Experimental results show that our method outperforms state-of-the-art systems and yields 75.0% in F1 on the ERD14 dataset and 56.9% on the GERDAQ dataset.,abstractText,[0],[0]
Entity Linking for Queries by Searching Wikipedia Sentences,title,[0],[0]
Feature selection in machine learning classification is an extremely relevant and widely studied topic.,1. Introduction,[0],[0]
"Wrapper models for feature selection have shown superior performance in many contexts (Doak, 1992).",1. Introduction,[0],[0]
They explore the lattice of feature subsets.,1. Introduction,[0],[0]
"For a given subset, a classifier is built over the features in the subset and an optimality condition is tested.",1. Introduction,[0],[0]
"However, complete search of the lattice of feature subsets is know to be NP hard (Amaldi & Kann, 1998).",1. Introduction,[0],[0]
"For this reason, heuristics searches are typically adopted in practice.",1. Introduction,[0],[0]
"Nevertheless, complete strategies have not to be exhaustive in order to find an optimal subset.",1. Introduction,[0],[0]
"In particular, feature subsets that lead to duplicate decision trees can be pruned from the search space.",1. Introduction,[0],[0]
"Such a pruning would be useful not only for complete searches, but also in the case of heuristics searches.",1. Introduction,[0],[0]
"A naı̈ve approach that stores all distinct trees found during the search is, however, unfeasible, since there may be an exponential number of such trees.",1. Introduction,[0],[0]
Our contribution is a non-trivial enumeration algorithm of all distinct decision trees built using subsets of the available features.,1. Introduction,[0],[0]
The procedure requires the storage of a linear number of decision trees in the worst case.,1. Introduction,[0],[0]
The starting point is a recursive procedure for the visit of the lattice of all subsets of features.,1. Introduction,[0],[0]
"The key idea is that
1University of Pisa and ISTI-CNR, Pisa, Italy.",1. Introduction,[0],[0]
Correspondence to:,1. Introduction,[0],[0]
"Salvatore Ruggieri <ruggieri@di.unipi.it>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
"a subset of features is denoted by the union R ∪ S of two sets, where elements in R must necessarily be used as split attributes, and elements in S may be used or not.",1. Introduction,[0],[0]
"Pruning of the search space is driven by the observation that if a feature a ∈ S is not used as split attribute by a decision tree built on R ∪ S, then the feature subset R ∪ S \ {a} leads to the same decision tree.",1. Introduction,[0],[0]
"Duplicate decision trees that still pass such a (necessary but not sufficient) pruning condition can be identified through a test on whether they use all attributes in R. Coupled with the tremendous computational optimization of decision tree induction algorithms, our approach makes it possible to increase the limit of practical applicability of theoretically hard complete searches.",1. Introduction,[0],[0]
"It also allows to optimize the sequential backward elimination (SBE) search heuristics when specifically designed for decision tree learning, with a speedup of up to 100× compared to a black-box approach.",1. Introduction,[0],[0]
This paper is organized as follows.,1. Introduction,[0],[0]
"First, we recall related work in Section 2.",1. Introduction,[0],[0]
The visit of the lattice of feature subsets is based on a generalization of binary counting enumeration devised in Sect.,1. Introduction,[0],[0]
3.,1. Introduction,[0],[0]
"Next, Sect. 4 introduces a procedure for the enumeration of distinct decision trees as a pruning of the feature subset lattice.",1. Introduction,[0],[0]
A white-box optimization of SBE is described in Sect.,1. Introduction,[0],[0]
5.,1. Introduction,[0],[0]
Experimental results are shown in Sect.,1. Introduction,[0],[0]
5.,1. Introduction,[0],[0]
"Finally, we summarize the contribution of the paper.",1. Introduction,[0],[0]
"(Blum & Langley, 1997; Dash & Liu, 1997; Guyon & Elisseeff, 2003; Liu & Yu, 2005; Bolón-Canedo et al., 2013) provide a categorization of approaches of feature subset selection along the orthogonal axes of the evaluation criteria, the search strategies, and the machine learning tasks.",2. Related Work,[0],[0]
"Common evaluation criteria include filter models, embedded and wrappers approaches.",2. Related Work,[0],[0]
"Filters are pre-processing algorithms that select a subset of features by looking at the data distribution, independently from the induction algorithm (Cover, 1977).",2. Related Work,[0],[0]
Embedded approaches perform feature selection in the process of training and are specific to the learning algorithm.,2. Related Work,[0],[0]
"Wrappers approaches optimize induction algorithm performances as part of feature selection (Kohavi & John, 1997).",2. Related Work,[0],[0]
"In particular, training data is split into a building set and a search set, and the space of feature subsets is explored.",2. Related Work,[0],[0]
"For each feature subset considered, the building set is used to train a classifier, which is then
evaluated on the search set.",2. Related Work,[0],[0]
"For a dataset with n features, the search space consists of 2n possible subsets.",2. Related Work,[0],[0]
"Search space exploration strategies include (see (Doak, 1992)): hill-climbing search (forward selection, backward elimination, bidirectional selection, beam search, genetic search), random search (random start hill-climbing, simulated annealing, Las Vegas), and complete search.",2. Related Work,[0],[0]
The aim of complete search is to find an optimal feature subset according to an evaluation metric.,2. Related Work,[0],[0]
"Typical objectives include minimizing the size of the feature subset provided that the classifier built from it has a minimal accuracy (dimensionality reduction), or minimizing the misclassification error of the classifier (performance maximization).",2. Related Work,[0],[0]
"Finally, feature subset selection has been considered both for classification and clustering tasks.",2. Related Work,[0],[0]
"Machine learning models and algorithms can be either treated as black-boxes or, instead, feature selection methods can be specific of the model and/or algorithm at hand (white-box).",2. Related Work,[0],[0]
"White-box approaches are less general, but can exploit assumptions on the model or algorithm to direct and speed up the feature subset search.
",2. Related Work,[0],[0]
"This paper falls in the category of complete search using a white-box wrapper model, tailored to decision tree classifiers, for performance maximization.",2. Related Work,[0],[0]
A feature subset is optimal if it leads to a decision tree with minimal error on the search set.,2. Related Work,[0],[0]
"Only complete space exploration can provide the guarantee of finding optimal subsets, whilst heuristics approaches can lead to results arbitrarily worse than the optimal (Murthy, 1998).",2. Related Work,[0],[0]
"Complete search is know to be NP hard (Amaldi & Kann, 1998).",2. Related Work,[0],[0]
"However, complete strategies do not need to be exhaustive in order to find an optimal subset.",2. Related Work,[0],[0]
"For instance, filter models can rely on monotonic evaluation metrics to support Branch and Bound search (Liu et al., 1998).",2. Related Work,[0],[0]
"Regarding wrapper approaches, evaluation metrics such as misclassification error, lack of the monotonicity property that would allow for pruning the search space in a complete search.",2. Related Work,[0],[0]
"Approximate Monotonicity with Branch and Bound (AMB&B) (Foroutan & Sklansky, 1987) tries and tackles this limitation, but it provides no formal guarantee that an optimal feature subset is found.",2. Related Work,[0],[0]
"Another form of search space pruning in wrapper approaches for decision trees has been pointed out by (Caruana & Freitag, 1994), which examines five hillclimbing procedures.",2. Related Work,[0],[0]
They adopt a caching approach to prevent re-building duplicate decision trees.,2. Related Work,[0],[0]
The basic property they observe is reported in a generalized form in this paper as Remark 4.3.,2. Related Work,[0],[0]
"While caching improves efficiency of a limited search, in the case of a complete search, it requires an exponential number of decision trees to be stored in cache, while our approach requires a linear number of them.",2. Related Work,[0],[0]
"We will also observe that Remark 4.3 may still leave duplicate trees in the search space, i.e., it is a necessary but not sufficient condition for enumerating distinct decision trees, while we will provide an exact enumeration.",2. Related Work,[0],[0]
"Let S = {a1, . . .",3. Enumerating Subsets,[0],[0]
", an} be a set of n elements, with n ≥ 0.",3. Enumerating Subsets,[0],[0]
The powerset of S is the set of its subsets: Pow(S) = {S′ | S′ ⊆ S}.,3. Enumerating Subsets,[0],[0]
"There are 2n subsets of S, and, for 0 ≤ k ≤ n, there are ( n k ) subsets of size k. Fig. 1 (top) shows the lattice (w.r.t. set inclusion) of subsets for n = 3.",3. Enumerating Subsets,[0],[0]
"The order of visit of the lattice, or, equivalently, the order of enumeration of elements in Pow(S), can be of primary importance for problems that explore the lattice as search space.",3. Enumerating Subsets,[0],[0]
"Well-known algorithms for subset generation produce lexicographic ordering, Grey code ordering, and binary counting ordering (Skiena, 2008).",3. Enumerating Subsets,[0],[0]
"Binary counting maps each subset into a binary number with n bits by setting the ith bit to 1 iff ai belongs to the subset, and generating subsets by counting from 0 to 2n − 1.",3. Enumerating Subsets,[0],[0]
"Subsets for n = 3 are generated as {}, {a3}, {a2}, {a2, a3}, {a1}, {a1, a3}, {a1, a2}, {a1, a2, a3}.",3. Enumerating Subsets,[0],[0]
"In this section, we introduce a recursive algorithm for a generalization of reverse binary counting (namely, counting from 2n",3. Enumerating Subsets,[0],[0]
− 1 down to 0) that will be the building block for solving the problems of generating distinct decision trees.,3. Enumerating Subsets,[0],[0]
Let us start by introducing the notation R 1 P = ∪S∈P {R ∪ S} to denote sets obtained by the union of R with elements of P .,3. Enumerating Subsets,[0],[0]
In particular: R 1 Pow(S) = ∪S′⊆S{R ∪ S′} consists of the subsets of R ∪ S which necessarily include R. This generalization of powersets will be crucial later on when we have to distinguish predictive attributes that must be used in a decision tree from those that may be used.,3. Enumerating Subsets,[0],[0]
A key observation of binary counting is that subsets can be partitioned between those including the value a1 and those not including it.,3. Enumerating Subsets,[0],[0]
"For example, Pow({a1, a2, a3}) = ({a1} 1 Pow({a2, a3})) ∪ (∅ 1 Pow({a2, a3})).",3. Enumerating Subsets,[0],[0]
"We can iterate the observation for the leftmost occurrence of a2 and obtain:
Pow({a1, a2, a3}) =",3. Enumerating Subsets,[0],[0]
"({a1, a2} 1 Pow({a3}))",3. Enumerating Subsets,[0],[0]
∪ ({a1} 1 Pow({a3})),3. Enumerating Subsets,[0],[0]
"∪ (∅ 1 Pow({a2, a3}))
",3. Enumerating Subsets,[0],[0]
"By iterating again for the leftmost occurrence of a3, we conclude:
Pow({a1, a2, a3}) =",3. Enumerating Subsets,[0],[0]
"({a1, a2, a3} 1 Pow(∅)) ∪ ({a1, a2} 1 Pow(∅))",3. Enumerating Subsets,[0],[0]
∪ ({a1} 1 Pow({a3})),3. Enumerating Subsets,[0],[0]
"∪ (∅ 1 Pow({a2, a3}))
",3. Enumerating Subsets,[0],[0]
"Since R 1 Pow(∅) = {R}, the leftmost set in the above union is {{a1, a2, a3}}.",3. Enumerating Subsets,[0],[0]
"In general, the following recurrence relation holds.
",3. Enumerating Subsets,[0],[0]
"Lemma 3.1 Let S = {a1, . . .",3. Enumerating Subsets,[0],[0]
", an}.",3. Enumerating Subsets,[0],[0]
"We have: R 1 Pow(S) = {R ∪ S} ∪⋃ i=n,...,1 (R ∪ {a1, . . .",3. Enumerating Subsets,[0],[0]
", ai−1}) 1 Pow({ai+1, . . .",3. Enumerating Subsets,[0],[0]
", an})
",3. Enumerating Subsets,[0],[0]
"This result can be readily translated into a procedure subset(R, S) for the enumeration of elements in R 1 Pow(S).",3. Enumerating Subsets,[0],[0]
"In particular, since ∅ 1 Pow(S) = Pow(S), subset(∅, S) generates all subsets of S. The procedure is shown as Alg. 1.",3. Enumerating Subsets,[0],[0]
The search space of the procedure is the tree of the recursive calls of the procedure.,3. Enumerating Subsets,[0],[0]
The search space for n = 3 is reported in Fig. 1 (bottom).,3. Enumerating Subsets,[0],[0]
"According to line 1 of Alg. 1, the subset outputted at a node labelled as R 1 Pow(S) is R ∪ S. Hence, the output is the reverse counting ordering: {a1, a2, a3}, {a1, a2}, {a1, a3}, {a1}, {a2, a3}, {a2}, {a3}, {}.",3. Enumerating Subsets,[0],[0]
Two key properties of the recursive procedure Alg.,3. Enumerating Subsets,[0],[0]
"1 will be relevant for the rest of the paper.
",3. Enumerating Subsets,[0],[0]
Remark 3.2 A set S′′ generated at a non-root node of the search tree of Alg.,3. Enumerating Subsets,[0],[0]
"1 is obtained by removing an element from the set S′ generated at the father node, i.e., S′′ = S′",3. Enumerating Subsets,[0],[0]
"\ {v} for some v ∈ S′.
The invariant |R′ ∪ S′| = |R ∪ S| readily holds for the loop at lines 4–8 of Alg.",3. Enumerating Subsets,[0],[0]
1.,3. Enumerating Subsets,[0],[0]
"Before the recursive call at line 6, an element is removed from R′, hence the set R′ ∪ S′ outputted at a child node has one element less than the set R ∪ S outputted at its father node.
",3. Enumerating Subsets,[0],[0]
Remark 3.3,3. Enumerating Subsets,[0],[0]
The selection order of ai ∈ S at line 4 of Alg.,3. Enumerating Subsets,[0],[0]
"1 is irrelevant.
",3. Enumerating Subsets,[0],[0]
"The procedure does not rely on any specific order of selecting members of S, which is a form of don’t care nondeterminism in the visit of the lattice.",3. Enumerating Subsets,[0],[0]
Any choice generates all elements in R 1 Pow(S).,3. Enumerating Subsets,[0],[0]
"In case of an apriori positional order of attributes, namely line 4 is “for ai ∈ S order by i desc do”, Alg. 1 produces precisely the reversed binary counting order.",3. Enumerating Subsets,[0],[0]
"However, if the selection order varies from one recursive call to another, then the output is still an enumeration of subsets.
",3. Enumerating Subsets,[0],[0]
"Algorithm 1 subset(R, S) enumerates R 1 Pow(S) 1: output R ∪ S 2: R′ ← R ∪ S 3:",3. Enumerating Subsets,[0],[0]
"S′ ← ∅ 4: for ai ∈ S do 5: R′ ← R′ \ {ai} 6: subset(R′, S′) 7: S′ ← S′ ∪ {ai} 8: end for",3. Enumerating Subsets,[0],[0]
We build on the subset generation procedure to devise an algorithm for the enumeration of all distinct decision trees built on subsets of the predictive features.,4. Generating All Distinct Decision Trees,[0],[0]
Let us first introduce some notation and assumptions.,4.1. On Top-Down Decision Tree Induction,[0],[0]
"Let S = {a1, . . .",4.1. On Top-Down Decision Tree Induction,[0],[0]
", an} be the set of predictive features.",4.1. On Top-Down Decision Tree Induction,[0],[0]
We write T = DT (S) to denote the decision tree built from predictive features S on a fixed training set.,4.1. On Top-Down Decision Tree Induction,[0],[0]
"Throughout the paper, we make the following assumption on the node split criterion in top-down decision tree induction with univariate split conditions.
",4.1. On Top-Down Decision Tree Induction,[0],[0]
Assumption 4.1 Let T = DT (S).,4.1. On Top-Down Decision Tree Induction,[0],[0]
"A split attribute at a decision node of T is chosen as argmaxa∈Sf(a,C), where f() is a quality measure and C are the cases of the training set reaching the node.
",4.1. On Top-Down Decision Tree Induction,[0],[0]
Our results will hold for any quality measure f() as far as the split attribute is chosen as the one that maximizes f().,4.1. On Top-Down Decision Tree Induction,[0],[0]
"Examples of quality measures used in this way include Information Gain (IG), Gain Ratio1 (GR), and the Gini index, used in C4.5 (Quinlan, 1993) and CART algorithms (Breiman et al., 1984).",4.1. On Top-Down Decision Tree Induction,[0],[0]
A second assumption regards the stopping criterion in top-down decision tree construction.,4.1. On Top-Down Decision Tree Induction,[0],[0]
"Let stop(S,C) be the boolean result of the stopping criterion at a node with cases C and predictive features S.
Assumption 4.2 If stop(S,C) = true then stop(S′, C) = true for every S′",4.1. On Top-Down Decision Tree Induction,[0],[0]
"⊆ S.
The assumption states that either: (1) the stopping criterion
1Gain Ratio normalizes Information Gain over the Split Information (SI) of an attribute, i.e., GR = IG/SI.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"This definition does not work well for attributes which are (almost) constants over the cases C, i.e., when SI ≈ 0.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"(Quinlan, 1986) proposed the heuristics of restricting the evaluation of GR only to attributes with above average IG.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"The heuristics is implemented in the C4.5 system (Quinlan, 1993).",4.1. On Top-Down Decision Tree Induction,[0],[0]
"It clearly breaks Assumption 4.1, making the selection of the split attribute dependent on the set S. An heuristics that satisfies Assumption 4.1 consists of restricting the evaluation of GR only for attributes with IG higher than a minimum threshold.
",4.1. On Top-Down Decision Tree Induction,[0],[0]
"Algorithm 2 DTdistinct(R, S) enumerates distinct decision trees necessarily using R and possibly using S as split features
1: build tree T = DT (R ∪ S) 2:",4.1. On Top-Down Decision Tree Induction,[0],[0]
U ← unused features in T 3:,4.1. On Top-Down Decision Tree Induction,[0],[0]
"if R ∩ U = ∅ then 4: output T 5: end if 6: R′ ← R ∪ (S \ U) 7: S′ ← S ∩ U 8: for ai ∈ S \ U order by frontier(T, ai) do 9: R′ ← R′ \ {ai}
10: DTdistinct(R′, S′) 11:",4.1. On Top-Down Decision Tree Induction,[0],[0]
"S′ ← S′ ∪ {ai} 12: end for
does not depend on S; or, if it does, then (2) stopping is monotonic with regard to the set of predictive features.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"(1) is a fairly general assumption, since typical stopping criteria are based on the size of cases C at a node and/or on the purity of the class attribute in C. (2) applies to criteria which require minimum quality of features for splitting a node.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"E.g., the C4.5 criterion of stopping if IG of all features is below a minimum threshold satisfies the assumption.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"The following remark, which is part of the decision tree folklore (see e.g., (Caruana & Freitag, 1994)), states a useful consequence of Assumptions 4.1 and 4.2.
",4.1. On Top-Down Decision Tree Induction,[0],[0]
"Remark 4.3 Let T = DT (S), and Ŝ be the set of split features used in T .",4.1. On Top-Down Decision Tree Induction,[0],[0]
"For every S′ such that S ⊇ S′ ⊇ Ŝ, we have DT (S′) = T .
",4.1. On Top-Down Decision Tree Induction,[0],[0]
"If the decision tree T built from S uses only features from Ŝ, then argmaxa∈Sf(a,C) = argmax â∈Ŝf(â, C) at any decision node of T .",4.1. On Top-Down Decision Tree Induction,[0],[0]
"Hence, any unused attribute in S \ Ŝ will not change the result of maximizing the quality measure and then, by Assumption 4.1, the split attribute at a decision node.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"Moreover, by Assumption 4.2, a leaf node in T will remain a leaf node for any S′ ⊆ S.",4.1. On Top-Down Decision Tree Induction,[0],[0]
"Let Ŝ = {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ak} be the set of features used in split nodes of the decision tree T = DT (S) built from S, and S \ Ŝ = {ak+1, . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
.,4.2. Enumerating Distinct Decision Trees,[0],[0]
", an} the set of features never selected as split features.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"By Remark 4.3, the decision tree T is equal to the one built starting from features a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ak plus any subset of ak+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"In symbols, all the decision trees for attribute subsets in {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ak} 1 Pow({ak+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an}) do coincide with T .",4.2. Enumerating Distinct Decision Trees,[0],[0]
We will use this observation to remove from the recurrence relation of Lemma 3.1 those sets in R 1 Pow(S) which lead to duplicate decision trees.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Formally, when searching for feature subsets that lead to distinct decision trees, the recurrence
relation can be modified as:
R 1 Pow(S) = {R ∪ S} ∪⋃ i=k,...,1 (R ∪ {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ai−1}) 1 Pow({ai+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an})
since the missing union:⋃ i=n,...,k+1 (R ∪ {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ai−1}) 1 Pow({ai+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an})
contains sets of features leading to the same decision tree as DT (R∪S).",4.2. Enumerating Distinct Decision Trees,[0],[0]
The simplified recurrence relation prunes from the the search space features subsets that lead to duplicated decision trees.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"However, we will show in Ex. 4.4 that such a pruning alone is not sufficient to generate distinct decision trees only, i.e., duplicate trees may still exists.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Alg. 2 provides an enumeration of all and only the distinct decision trees.,4.2. Enumerating Distinct Decision Trees,[0],[0]
It builds on the generalized subset generation procedure.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Line 1 constructs a tree T from features R ∪ S. Feature in the set U of unused features in T are not iterated over in the loop at lines 8–12, since those iterations would yield the same tree as T .",4.2. Enumerating Distinct Decision Trees,[0],[0]
This is formally justified by the modified recurrence relation above.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"The tree T is outputted at line 4 only if R ∩ U = ∅, namely features required to be used (i.e., R) are actually used in decision splits.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"This prevents from outputting more than once a decision tree that can be obtained from multiple paths of the search tree.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Example 4.4 Let S = {a1, a2, a3}.",4.2. Enumerating Distinct Decision Trees,[0],[0]
Assume that a1 has no discriminatory power unless data has been split by a3.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"More formally, DT (S′) = DT (S′ \ {a1})",4.2. Enumerating Distinct Decision Trees,[0],[0]
if a3 6∈ S′.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"The visit of feature subsets of Fig. 1 (bottom) gives rise to the trees built by DTdistinct(∅, S) as shown in Fig. 2 (top).",4.2. Enumerating Distinct Decision Trees,[0],[0]
"For instance, the subset {a1, a2} visited at the node
labelled {a1, a2} 1 ∅ in Fig. 1 (bottom), produces the decision tree DT ({a1, a2}).",4.2. Enumerating Distinct Decision Trees,[0],[0]
"By assumption, such a tree is equal to DT ({a2}), which is a duplicate tree produced in another node – underlined in Fig. 2 (top) – corresponding to the feature set visited at the node labelled {a2} 1 ∅. Another example regarding DT ({a1}) = DT (∅) is shown in Fig. 2 (top), together with its underlined duplicate tree.",4.2. Enumerating Distinct Decision Trees,[0],[0]
Unique trees for two or more duplicates can be characterized by the fact that features appearing to the left of 1 must necessarily be used as split features by the constructed decision tree.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"In the two previous example cases, the node underlined will output their decision trees, while the other duplicates will not pass the test at line 3 of Alg.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"2.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Remark 3.3 states that the selection order in the recursive calls of subset() is not relevant.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Alg. 2 adopts a specific order that, while not affecting the result (any order would produce the enumeration of distinct decision trees), impacts on the effectiveness of pruning the search space.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"We define the frontier frontier(T, ai) of an attribute ai in a decision tree T as the sum of the number of cases of the training set that reach a node in T where ai is the split attribute.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The smaller the frontier is the smaller is the impact of removing sub-trees of T rooted at nodes with ai as split attribute.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Example 4.5 (Ctd.),4.2. Enumerating Distinct Decision Trees,[0],[0]
The order of selection of ai’s in the visit of Fig. 2 (top) is by descending i’s.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"This order does not take into account the fact that a3 has more discriminatory power than a1, i.e., its presence gives rise to more distinct decision trees.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"As a consequence, it would be better to have a3 removed in the rightmost child of a node, which has the largest search sub-space, and hence the best possibilities of pruning.",4.2. Enumerating Distinct Decision Trees,[0],[0]
The ordering based on ascending frontier estimates the discriminatory power of ai by the amount of cases in the training set discriminated by splits using ai.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"In our example, such an order would likely be a1, a2, and a3.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The search space of DTdistinct(∅, S) is then reported in Fig. 2 (bottom).",4.2. Enumerating Distinct Decision Trees,[0],[0]
Notice that there is no duplicate tree here.,4.2. Enumerating Distinct Decision Trees,[0],[0]
Also notice that the size of the search space is smaller than in the previous example.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"In fact, the node labelled as DT ({a1, a2}) = DT ({a2}) corresponds to the exploration of ∅ 1 {a1, a2}.",4.2. Enumerating Distinct Decision Trees,[0],[0]
The a1 attribute is unused and hence is pruned at line 8 of Alg.,4.2. Enumerating Distinct Decision Trees,[0],[0]
2.,4.2. Enumerating Distinct Decision Trees,[0],[0]
The sub,4.2. Enumerating Distinct Decision Trees,[0],[0]
"-space to be searched consists then of only the subset of {a1}, not all subsets of {a1, a2}.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The following non-trivial result holds.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Theorem 4.6 DTdistinct(R, S) outputs the distinct decision trees built on sets of features in R 1 Pow(S).
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Proof.,4.2. Enumerating Distinct Decision Trees,[0],[0]
The search space of DTdistinct() is a pruning of the search space of subset().,4.2. Enumerating Distinct Decision Trees,[0],[0]
Every tree built at a node and outputted is then constructed from a subset in R 1 Pow(S).,4.2. Enumerating Distinct Decision Trees,[0],[0]
"By Remark 3.3, the order of selection of ai ∈
S \U at line 8 is irrelevant, since any order will lead to the same space R 1 Pow(S).
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Let us first show that decision trees in output are all distinct.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"The key observation here is that, by line 4, all features in R are used as split features in the outputted decision tree.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The proof proceed by induction on the size of S. If |S| = 0, then there is at most one decision tree in output, hence the conclusion.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Assume now |S| > 0, and let S = {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an}.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"By Lemma 3.1, any two recursive calls at line 10 have parameters (R ∪ {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ai−1}, {ai+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an}) and (R ∪ {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", aj−1}, {aj+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an}), for some i < j. By inductive hypothesis, ai is missing as a predictive attribute in the trees in output from the first call, while it must be a split attribute in the trees in output by the second call.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Hence, the trees in output from recursive calls are all distinct among them.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Moreover, they are all different from T , if it is outputted at line 4.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"In fact, T has |R ∪ S \",4.2. Enumerating Distinct Decision Trees,[0],[0]
"U | split features, whilst recursive calls construct decision trees with at most |R ∪ S \",4.2. Enumerating Distinct Decision Trees,[0],[0]
"U | − 1 features.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Let us now show that trees pruned at line 7 or at line 4 are already outputted elsewhere, which implies that every distinct decision tree is outputted at least once.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"First, by Remark 4.3, the trees of the pruned iterations S ∩U at line 7 are the same of the tree of T at line 1.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Second, if the tree T is not outputted at line 4, because R∩U 6= ∅, we have that it is outputted at another node of the search tree.",4.2. Enumerating Distinct Decision Trees,[0],[0]
The proof is by induction on |R|.,4.2. Enumerating Distinct Decision Trees,[0],[0]
For |R| = 0 it is trivial.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Let R = {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an}, with n > 0, and let R′ = {a1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", ai−1} be such that ai ∈ U and R′∩U = ∅.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"There is a sibling node in the search tree corresponding to a call with parameters R′ and S′ = {ai+1, . . .",4.2. Enumerating Distinct Decision Trees,[0],[0]
", an} ∪ S.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"By inductive hypothesis on |R′| < |R|, the distinct decision trees with features in R′ 1 Pow(S′) are all outputted, including T because T has split features in R ∪ S \ {ai} which belongs to R′ 1 Pow(S′).",4.2. Enumerating Distinct Decision Trees,[0],[0]
"2
Let us now point out some properties of DTdistinct().
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Property 1: linear space complexity.,4.2. Enumerating Distinct Decision Trees,[0],[0]
Alg. 2 is computationally linear (per number of trees built) in space in the number of predictive features.,4.2. Enumerating Distinct Decision Trees,[0],[0]
An exhaustive search would instead keep in memory the distinct decision trees built in order to check whether a new decision trees is a duplicate.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Similarly will do approaches based on complete search with some forms of caching of duplicates (Caruana & Freitag, 1994).",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Those approaches would require exponential space, as shown in the next example.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Example 4.7 Let us consider the well-known Adult dataset2 (Lichman, 2013), consisting of 48842 cases, 14 predictive features, and a binary class attribute.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"Fig. 3
2See Sect.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"5 for the experimental settings.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
"(left) shows, for the IG split criterion, the distribution of distinct decision trees w.r.t.",4.2. Enumerating Distinct Decision Trees,[0],[0]
the size of attribute subset.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"The distributions are plotted for various values of the stopping parameter m which halts tree construction if the number of cases of the training set reaching the current node is lower than a minimum threshold m (formally, stop(S,C) is true iff |C| < m).
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Property 2: reduced overhead.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Our procedure may construct duplicate decision trees at line 1, which, however, are not outputted thanks to the test at line 3.",4.2. Enumerating Distinct Decision Trees,[0],[0]
We measure such an overhead of Alg.,4.2. Enumerating Distinct Decision Trees,[0],[0]
2 as the ratio of all decision trees constructed at line 1 over the number of distinct decision trees.,4.2. Enumerating Distinct Decision Trees,[0],[0]
An ideal ratio of 1 means that no duplicate decision tree is constructed at all.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"The overhead can be controlled by the attribute selection ordering at line 8.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Example 4.8 (Ctd.),4.2. Enumerating Distinct Decision Trees,[0],[0]
Fig. 3 (center) shows the overhead at the variation of m for three possible orderings of selection at line 8 of Alg.,4.2. Enumerating Distinct Decision Trees,[0],[0]
2.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"One is the the ordering stated by DTdistinct(), based on ascending frontier.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The second one is the reversed order, namely descending frontier.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The third one is based on assigning a fixed index i to features ai’s, and then ordering over i. The DTdistinct() ordering is impressively effective, with an overhead close to 1 – i.e., the search space is precisely the set of distinct decision trees.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Fig. 3 (center) also reports the ratio of the number of trees in an exhaustive search (2n for n features) over the number of distinct trees.,4.2. Enumerating Distinct Decision Trees,[0],[0]
Smaller m’s lead to a smaller ratio.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"Thus, for small m values, pruning duplicate trees does not guarantee alone an enumeration more efficient than exhaustive search.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"The next property will help.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Property 3: feature-incremental tree building.,4.2. Enumerating Distinct Decision Trees,[0],[0]
The construction of each single decision tree at line 1 of Alg.,4.2. Enumerating Distinct Decision Trees,[0],[0]
2 can be speed up by Remark 3.2.,4.2. Enumerating Distinct Decision Trees,[0],[0]
The decision tree T ′ at a child node of the search tree differs from the decision tree T built at the father node by one missing attribute.,4.2. Enumerating Distinct Decision Trees,[0],[0]
The construction of T ′ can then benefit from this observation.,4.2. Enumerating Distinct Decision Trees,[0],[0]
"We first recursively clone T and then re-build only sub-trees rooted at node where the split attribute is ai.
",4.2. Enumerating Distinct Decision Trees,[0],[0]
Example 4.9 (Ctd.),4.2. Enumerating Distinct Decision Trees,[0],[0]
Fig. 3,4.2. Enumerating Distinct Decision Trees,[0],[0]
(right) contrasts the elapsed times of exhaustive search and DTdistinct().,4.2. Enumerating Distinct Decision Trees,[0],[0]
"For smaller values of m, there is an exponential number of duplicated decision trees, but the running time of DTdistinct() is still much better than the exhaustive search due to the incremental building of decision trees.",4.2. Enumerating Distinct Decision Trees,[0],[0]
"In wrapper models, the training set is divided into a building set and a search set.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
A decision tree is built on the building set and its the accuracy is evaluated on the search set.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
"Our enumeration procedure DTdistinct() has a direct application, which consists of running a complete wrapper search looking for the feature subset that leads to the most accurate decision tree on the search set.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"On the practical side, however, using DTdistinct() to look for the optimal feature subset is computationally feasible only when the number of predictive features is moderate.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"Moreover, optimality on the search set may be obtained at the cost of overfitting (Doak, 1992; Reunanen, 2003) and instability (Nogueira & Brown, 2016).",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"The ideas underlying our approach, however, can impact also on the efficiency of heuristics searches.
",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"In particular, we consider here the widely used sequential backward elimination (SBE) heuristics.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
SBE starts building a decision tree T using the set S of all features.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
We call T the top tree.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
"For every ai ∈ S, a decision tree Ti is built using features in S \ {ai}.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"If no Ti’s has a smaller error on the search set than T , the algorithm stops returning S as the subset of selected features.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"Otherwise, the procedure is repeated removing ak from S, where Tk is the tree with the smallest error.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"In summary, features are eliminated one at a time in a greedy way.
",5. PSBE: A White-Box Optimization of SBE,[0],[0]
SBE is a black-box approach.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
"The procedure applies to any type of classifier, not only to decision trees.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
A white-box optimization can be devised for decision tree classifiers that satisfy the assumptions of Section 4.1.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
The optimization relies on Remark 4.3.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
Let U be the set of features not used in the current decision tree T .,5. PSBE: A White-Box Optimization of SBE,[0],[0]
"For ai ∈ U , it turns out that
Ti = T .",5. PSBE: A White-Box Optimization of SBE,[0],[0]
"Thus, only trees built from S \ {ai} for ai 6∈ U need to be considered for backward elimination.",5. PSBE: A White-Box Optimization of SBE,[0],[0]
This saves the construction of |S∩U | decision trees at each step of the procedure.,5. PSBE: A White-Box Optimization of SBE,[0],[0]
We call this optimization PSBE (Pruned SBE).,5. PSBE: A White-Box Optimization of SBE,[0],[0]
"Table 1 reports the number of instances and of features for small and large standard benchmarks datasets publicly available from (Lichman, 2013).",6.1. Datasets and Experimental Settings,[0],[0]
"Following (Reunanen, 2003), we adopt 5-repeated stratified 10-fold cross validation in experimenting with wrapper models.",6.1. Datasets and Experimental Settings,[0],[0]
"For each holdout fold, feature selection is performed by splitting the 9- fold training set into 70% building set and 30% search set using stratified random sampling.",6.1. Datasets and Experimental Settings,[0],[0]
Information Gain (IG) is used as quality measure in node splitting.,6.1. Datasets and Experimental Settings,[0],[0]
"No form of tree simplification (e.g., error-based pruning) is used.",6.1. Datasets and Experimental Settings,[0],[0]
The search error is the average misclassification error on the search set.,6.1. Datasets and Experimental Settings,[0],[0]
The cross-validation error is the average misclassification error on the hold-out folds for the tree built on the training set using the selected feature subset.,6.1. Datasets and Experimental Settings,[0],[0]
"Misclassification errors are computed using the C4.5’s distribution imputation method (Saar-Tsechansky & Provost, 2007).
",6.1. Datasets and Experimental Settings,[0],[0]
"All procedures described in this paper were implemented by extending the YaDT system (Ruggieri, 2002; 2004; Aldinucci et al., 2014).",6.1. Datasets and Experimental Settings,[0],[0]
It is a state-of-the-art main-memory C++ implementation of C4.5 with many algorithmic and data structure optimizations as well as with multi-core tree building.,6.1. Datasets and Experimental Settings,[0],[0]
The extended YaDT version is publicly downloadable from: http://pages.di.unipi.it/ruggieri.,6.1. Datasets and Experimental Settings,[0],[0]
"Test were performed on a commodity PC with Intel 4 cores i52410@2.30 GHz, 16 Gb RAM, and Windows 10 OS.",6.1. Datasets and Experimental Settings,[0],[0]
"Or, in other words, how much our pruning approach will make a complete search feasible in practice?",6.2. How Fast is DTdistinct()?,[0],[0]
"Fig. 4 shows the ratio of the number of built trees over the number of distinct trees (left) and the total elapsed time of DTdistinct() (center) for low to medium dimensionality datasets – actually, those for which DTdistinct() terminates within a timeout of 1h.",6.2. How Fast is DTdistinct()?,[0],[0]
"The ratio ranges from 1 to 1.35, which show that the selection order based on ascending frontier size (line 8 of Alg. 2) is effective in practice.",6.2. How Fast is DTdistinct()?,[0],[0]
"The total elapsed time of the enumeration procedure, shown in Fig. 4 (center), grows exponentially with the inverse of m, the stopping parameter.",6.2. How Fast is DTdistinct()?,[0],[0]
"This is intuitive, since lower m’s lead to higher numbers of distinct decision trees, and, as shown in Fig. 3 (left), such numbers approach 2n – where n is the number of features.",6.2. How Fast is DTdistinct()?,[0],[0]
"However, the total elapsed time of DTdistinct() remains within a practically admissible bound for datasets with a moderate number of features.",6.2. How Fast is DTdistinct()?,[0],[0]
"Consider for instance, the Anneal dataset.",6.2. How Fast is DTdistinct()?,[0],[0]
"An exhaustive enumeration would be impossible, since it consists of building 239 ≈ 550B trees.",6.2. How Fast is DTdistinct()?,[0],[0]
"DTdistinct() runs in less than 20 seconds for m = 16, and less than 270 seconds for m = 8.",6.2. How Fast is DTdistinct()?,[0],[0]
"This is the coupled result of three factors: the pruning approach of Alg. 2, the feature-incremental tree building optimization, and the tremendous efficiency of the state-of-the-art tree induction implementations.",6.2. How Fast is DTdistinct()?,[0],[0]
Table 1 reports elapsed times that allows for comparing the efficiency of PSBE vs SBE.,6.3. PSBE vs SBE,[0],[0]
The m parameter is set to the small value 2 for all datasets.,6.3. PSBE vs SBE,[0],[0]
The ratio of elapsed times shows a speedup of up to 100× of PSBE vs SBE.,6.3. PSBE vs SBE,[0],[0]
The improvement increases with the number of features.,6.3. PSBE vs SBE,[0],[0]
"For high-
dimensional datasets, the black-box SBE does not even terminate within a time-out of 1h.",6.3. PSBE vs SBE,[0],[0]
"The white-box PSBE, instead, runs in about 150 seconds for the highly dimensional dataset p53Mutants.",6.3. PSBE vs SBE,[0],[0]
"This is a relevant result for machine learning practitioners, extending the applicability of the SBE heuristics.",6.3. PSBE vs SBE,[0],[0]
"Fig. 4 (right) shows the average search error over the Adult dataset of the decision trees constructed on: (1) all features (top); (2) the features selected by SBE (same as PSBE); and (3) the features selected by DTdistinct(), namely those with the lowest error on the search set (hence, the name optimal).",6.4. Complete Search or Heuristics?,[0],[0]
"Obviously, SBE is better than top, and optimal is better than SBE.",6.4. Complete Search or Heuristics?,[0],[0]
"Interestingly, SBE is close to the optimal search error, in particular for small m parameter.",6.4. Complete Search or Heuristics?,[0],[0]
Does this generalize to unknown cases?,6.4. Complete Search or Heuristics?,[0],[0]
"Fig. 5 reports the crossvalidation errors over the Adult, Ionosphere and Anneal datasets.",6.4. Complete Search or Heuristics?,[0],[0]
"For Adult, optimal is better than SBE, which in turn is better than top.",6.4. Complete Search or Heuristics?,[0],[0]
"For Ionosphere, instead, the optimal tree has the worst performance, the top tree is the best for almost all m’s, and SBE is the best for small m’s.",6.4. Complete Search or Heuristics?,[0],[0]
"For Anneal, SBE is the worst, and the top tree is better than the optimal for large m’s.",6.4. Complete Search or Heuristics?,[0],[0]
Table 1 reports the cross-validation errors for m = 2 for all datasets.,6.4. Complete Search or Heuristics?,[0],[0]
"PSBE, or equivalently SBE as they select the same subset of features, wins over top in most cases.",6.4. Complete Search or Heuristics?,[0],[0]
But there is no clear evidence of the superiority of optimal over SBE.,6.4. Complete Search or Heuristics?,[0],[0]
"This is consistent with the
conclusions of (Doak, 1992; Reunanen, 2003) that simple sequential elimination exhibits better generalization performances than more exhaustive searches when considering error on an unseen set of instances.",6.4. Complete Search or Heuristics?,[0],[0]
We have introduced an original pruning algorithm of the search space of feature subsets which allows for enumerating all and only the distinct decision trees.,7. Conclusions,[0],[0]
"On the theoretical side, this makes it possible to run a complete wrapper procedure for moderate dimensionality datasets.",7. Conclusions,[0],[0]
"This will allow, for instance, for a deeper investigation of old and new search heuristics by comparing their performances with those of a complete search.",7. Conclusions,[0],[0]
"On the practical side, ideas and results of the paper have been applied to improve the computational efficiency of the SBE heuristics.
",7. Conclusions,[0],[0]
"As future work, we will investigate the extension of the proposed approach in presence of decision tree simplification and for ensembles of decision trees (bagging, random forests).",7. Conclusions,[0],[0]
"Moreover, we will consider the related problem of finding an optimal subset of features, which in the present paper is tackled by simply enumerating all distinct decision trees.",7. Conclusions,[0],[0]
"Actually, there is no need to explore a sub-space of (distinct) decision trees, if a lower bound for the accuracy of any tree in the sub-space can be computed and such a lower bound is higher than the best error found so far.",7. Conclusions,[0],[0]
This idea would build upon the enumeration procedure DTdistinct() as a further pruning condition of the search space.,7. Conclusions,[0],[0]
The search space for the feature selection problem in decision tree learning is the lattice of subsets of the available features.,abstractText,[0],[0]
We provide an exact enumeration procedure of the subsets that lead to all and only the distinct decision trees.,abstractText,[0],[0]
The procedure can be adopted to prune the search space of complete and heuristics search methods in wrapper models for feature selection.,abstractText,[0],[0]
"Based on this, we design a computational optimization of the sequential backward elimination heuristics with a performance improvement of up to 100×.",abstractText,[0],[0]
Enumerating Distinct Decision Trees,title,[0],[0]
"Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1088–1097, Austin, Texas, November 1-5, 2016. c©2016 Association for Computational Linguistics
Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems. This paper focuses on identifying and understanding mathematical relations described within a single sentence. We introduce the problem of Equation Parsing – given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence. We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations. Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases. In 60% of the time, it also identifies the correct noun phrase → variables mapping, significantly outperforming baselines. We also release a new annotated dataset for task evaluation.",text,[0],[0]
Understanding text often involves reasoning with respect to quantities mentioned in it.,1 Introduction,[0],[0]
"Understanding the news article statement in Example 1 requires identifying relevant entities and the mathematical relations expressed among them in text, and determining how to compose them.",1 Introduction,[0],[0]
"Similarly, solving a math word problem with a sentence like Example 2, requires realizing that it deals with a single number, knowing the meaning of “difference” and compos-
Example 1 Emanuel’s campaign contributions total three times those of his opponents put together.",1 Introduction,[0],[0]
Example 2,1 Introduction,[0],[0]
Twice a number equals 25 less than triple the same number.,1 Introduction,[0],[0]
Example 3,1 Introduction,[0],[0]
"Flying with the wind , a bird was able to make 150 kilometers per hour.",1 Introduction,[0],[0]
Example 4,1 Introduction,[0],[0]
The sum of two numbers is 80.,1 Introduction,[0],[0]
Example 5,1 Introduction,[0],[0]
"There are 54 5-dollar and 10- dollar notes.
",1 Introduction,[0],[0]
"ing the right equation – “25” needs to be subtracted from a number only after it is multiplied by 3.
",1 Introduction,[0],[0]
"As a first step towards understanding such relations, we introduce the Equation Parsing task - given a sentence expressing a mathematical relation, the goal is to generate an equation representing the relation, and to map the variables in the equation to their corresponding noun phrases.",1 Introduction,[0],[0]
"To keep the problem tractable, in this paper we restrict the final output equation form to have at most two (possibly coreferent) variables, and assume that each quantity mentioned in the sentence can be used at most once in the final equation.1 In example 1, the gold output of an equation parse should be V1 = 3 × V2, with V1 = “Emanuel’s campaign contributions” and V2 = “those of his opponents put together”.
",1 Introduction,[0],[0]
"The task can be seen as a form of semantic parsing (Goldwasser and Roth, 2011; Kwiatkowski et al., 2013) where instead of mapping a sentence to a logical form, we want to map it to an equation.",1 Introduction,[0],[0]
"However,
1We empirically found that around 97% of sentences describing a relation have this property.
",1 Introduction,[0],[0]
"1088
there are some key differences that make this problem very challenging in ways that differ from the “standard” semantic parsing.",1 Introduction,[0],[0]
"In Equation Parsing, not all the components of the sentence are mapped to the final equation.",1 Introduction,[0],[0]
There is a need to identify noun phrases that correspond to variables in the relations and determine that some are irrelevant and can be dropped.,1 Introduction,[0],[0]
"Moreover, in difference from semantic parsing into logical forms, in Equation Parsing multiple phrases in the text could correspond to the same variable, and identical phrases in the text could correspond to multiple variables.
",1 Introduction,[0],[0]
We call the problem of mapping noun phrases to variables the problem of grounding variables.,1 Introduction,[0],[0]
"Grounding is challenging for various reasons, key among them are that: (i)",1 Introduction,[0],[0]
"The text often does not mention “variables” explicitly, e.g., the sentence in example 3 describes a mathematical relation between the speed of bird and the speed of wind, without mentioning “speed” explicitly.",1 Introduction,[0],[0]
"(ii) Sometimes, multiple noun phrases could refer to the same variable.",1 Introduction,[0],[0]
"For instance, in example 2, both “a number” and “the same number” refer to the same variable.",1 Introduction,[0],[0]
"On the other hand, the same noun phrase might refer to multiple variables, as in example 4, where the noun phrase “two numbers” refer to two variables.
",1 Introduction,[0],[0]
"In addition, the task involves deciding which of the quantities identified in the sentence are relevant to the final equation generation.",1 Introduction,[0],[0]
"In example 5, both “5” and “10” are not relevant for the final equation “V1 + V2 = 54”.",1 Introduction,[0],[0]
"Finally, the equation needs to be constructed from a list of relevant quantities and grounded variables.",1 Introduction,[0],[0]
"Overall, the output space becomes exponential in the number of quantities mentioned in the sentence.
",1 Introduction,[0],[0]
Determining the final equation that corresponds to the text is an inference step over a very large space.,1 Introduction,[0],[0]
"To address this, we define the concept of “projectivity” - a condition where the final equation can be generated by combining adjacent numbers or variables, and show that most sentences expressing mathematical relations exhibit the projectivity property.",1 Introduction,[0],[0]
"Finally, we restrict our inference procedure to only search over equations which have this property.
",1 Introduction,[0],[0]
"Our approach builds on a pipeline of structured predictors that identify irrelevant quantities, recognize coreferent variables, and, finally, generate equations.",1 Introduction,[0],[0]
"We also leverage a high precision lexicon of
mathematical expressions and develop a greedy lexicon matching strategy to guide inference.",1 Introduction,[0],[0]
"We discuss and exemplify the advantages of this approach and, in particular, explain where the “standard” NLP pipeline fails to support equation parsing, and necessitates the new approach proposed here.",1 Introduction,[0],[0]
Another contribution of this work is the development of a new annotated data set for the task of equation parsing.,1 Introduction,[0],[0]
"We evaluate our method on this dataset and show that our method predicts the correct equation in 70% of the cases and that in 60% of the time we also ground all variables correctly.
",1 Introduction,[0],[0]
The next section presents a discussion of related work.,1 Introduction,[0],[0]
Next we formally describe the task of equation parsing.,1 Introduction,[0],[0]
"The following sections describe our equation representation and the concept of projectivity, followed by the description of our algorithm to generate the equations and variable groundings from text.",1 Introduction,[0],[0]
We conclude with experimental results.,1 Introduction,[0],[0]
"The work most related to this paper is (Madaan et al., 2016), which focuses on extracting relation triples where one of the arguments is a number.",2 Related Work,[0],[0]
"In contrast, our work deals with multiple variables and complex equations involving them.",2 Related Work,[0],[0]
"There has been a lot of recent work in automatic math word problem solving (Kushman et al., 2014; Roy et al., 2015; Hosseini et al., 2014; Roy and Roth, 2015).",2 Related Work,[0],[0]
These solvers cannot handle sentences individually.,2 Related Work,[0],[0]
"They require the input to be a complete math word problem, and even then, they only focus on retrieving a set of answer values without mentioning what each answer value corresponds to.",2 Related Work,[0],[0]
"Our work is also conceptually related to work on semantic parsing – mapping natural language text to a formal meaning representation (Wong and Mooney, 2007; Clarke et al., 2010; Cai and Yates, 2013; Kwiatkowski et al., 2013; Goldwasser and Roth, 2011).",2 Related Work,[0],[0]
"However, as mentioned earlier, there are some significant differences in the task definition that necessitate the development of a new approach.",2 Related Work,[0],[0]
"Equation parsing takes as input a sentence x describing a single mathematical equation, comprising one or two variables and other quantities mentioned in x.
Let N be the set of noun phrases in the sentence x.",3 The Equation Parsing Task,[0],[0]
"The output of the task is the mathematical equation described in x, along with a mapping of each variable in the equation to its corresponding noun phrase in N .",3 The Equation Parsing Task,[0],[0]
We refer to this mapping as the “grounding” of the variable; the noun phrase represents what the variable stands for in the equation.,3 The Equation Parsing Task,[0],[0]
Table 1 gives an example of an input and output for the equation parsing of the text in example 2.,3 The Equation Parsing Task,[0],[0]
"Since an equation can be written in various forms, we use the form which most agrees with text, as our target output.",3 The Equation Parsing Task,[0],[0]
"So, for example 1, we will choose V1 = 3× V2 and not V2 = V1 ÷ 3.",3 The Equation Parsing Task,[0],[0]
"In cases where several equation forms seem to be equally likely to be the target equation, we randomly choose one of them, and keep this choice consistent across the dataset.",3 The Equation Parsing Task,[0],[0]
"In this section, we introduce an equation parse for a sentence.",3.1 Equation Parse Representation,[0],[0]
"An equation parse of a sentence x is a pair (T,E), where T represents a set of triggers extracted from x, and E represents an equation tree formed with the set T as leaves.",3.1 Equation Parse Representation,[0],[0]
We now describe these terms in detail.,3.1 Equation Parse Representation,[0],[0]
"Trigger Given a sentence xmentioning a mathematical relation, a trigger can either be a quantity trigger expressed in x, or variable trigger which is a noun phrase in x corresponding to a variable.",3.1 Equation Parse Representation,[0],[0]
"A quantity trigger is a tuple (q, s), where q is the numeric value of the quantity mentioned in text, and s is the span of text from the sentence x which refers to the quantity.",3.1 Equation Parse Representation,[0],[0]
"A variable trigger is a tuple (l, s), where l represents the label of the variable, and s represents the noun phrase representing the variable.",3.1 Equation Parse Representation,[0],[0]
"For example, for the sentence in Fig 1, the spans “Twice”, “25”, and “triple” generate quantity triggers, whereas “a number” and “the same number” generate variable triggers, with label V1.",3.1 Equation Parse Representation,[0],[0]
Trigger List,3.1 Equation Parse Representation,[0],[0]
"The trigger list T for a sentence x contains one trigger for each variable mention and each numeric value used in the final equation expressed
by the sentence x.",3.1 Equation Parse Representation,[0],[0]
"The trigger list might consist of multiple triggers having the same label, or extracted from the same span of text.",3.1 Equation Parse Representation,[0],[0]
"In the example sentence in Fig 1, the trigger list comprises two triggers having the same label V1.",3.1 Equation Parse Representation,[0],[0]
"The final trigger list for the example in Fig 1 is {(2, “2”), (V1, “a number”), (25, “25”), (3, “triple”), (V1, “the same number”)}.",3.1 Equation Parse Representation,[0],[0]
Note that there can be multiple valid trigger lists.,3.1 Equation Parse Representation,[0],[0]
"In our example, we could have chosen both variable triggers to point to the same mention “a number”.",3.1 Equation Parse Representation,[0],[0]
"Quantity triggers in the trigger list form the quantity trigger list, and the variable triggers in trigger list form the variable trigger list.",3.1 Equation Parse Representation,[0],[0]
Equation Tree,3.1 Equation Parse Representation,[0],[0]
"An equation tree of a sentence x is a binary tree whose leaves constitute the trigger list of x, and internal nodes (except the root) are labeled with one of the following operations – addition, subtraction, multiplication, division.",3.1 Equation Parse Representation,[0],[0]
"In addition, for nodes which are labeled with subtraction or division, we maintain a separate variable to determine order of its children.",3.1 Equation Parse Representation,[0],[0]
"The root of the tree is always labeled with the operation equal.
",3.1 Equation Parse Representation,[0],[0]
An equation tree is a natural representation for an equation.,3.1 Equation Parse Representation,[0],[0]
"Each node n in an equation tree represents an expression EXPR(n), and the label of the parent node determines how the expressions of its children are to be composed to construct its own expression.",3.1 Equation Parse Representation,[0],[0]
"Let us denote the label for a non-leaf node
n to be (n), where (n) ∈ {+,−,×,÷,=} and the order of a node n’s children by ORDER(n) (defined only for subtraction and division nodes), which takes values lr (Left-Right) or rl (Right-Left).",3.1 Equation Parse Representation,[0],[0]
"For a leaf node n, the expression EXPR(n) represents the variable label, if n is a variable trigger, and the numeric value of the quantity, if it is a quantity trigger.",3.1 Equation Parse Representation,[0],[0]
"Finally, we use lc(n) and rc(n) to represent the left and right child of node n, respectively.",3.1 Equation Parse Representation,[0],[0]
The equation represented by the tree can be generated as follows.,3.1 Equation Parse Representation,[0],[0]
"For all non-leaf nodes n, we have
EXPR(n) =   EXPR(lc(n))",3.1 Equation Parse Representation,[0],[0]
(n) EXPR(rc(n)),3.1 Equation Parse Representation,[0],[0]
"if (n) ∈ {+,×,=} EXPR(lc(n))",3.1 Equation Parse Representation,[0],[0]
(n) EXPR(rc(n)),3.1 Equation Parse Representation,[0],[0]
"if (n) ∈ {−,÷} ∧ ORDER(n) = lr
EXPR(rc(n))",3.1 Equation Parse Representation,[0],[0]
(n) EXPR(lc(n)),3.1 Equation Parse Representation,[0],[0]
"if (n) ∈ {−,÷} ∧ ORDER(n) = rl
(1)
",3.1 Equation Parse Representation,[0],[0]
"Given an equation tree T of a sentence, the equation represented by it is the expression generated by the root of T (following Equation 1).",3.1 Equation Parse Representation,[0],[0]
"Referring to the equation tree in Fig 1, the node marked “−r” represents (3× V1)− 25, and the root represents the full equation 2× V1 = (3× V1)− 25.",3.1 Equation Parse Representation,[0],[0]
"For each leaf n of an equation tree T , we define a function Location(·), to indicate the position of the corresponding trigger in text.",4 Projectivity,[0],[0]
"We also define for each node n of equation tree T , functions Span-Start(n) and Span-End(n) to denote the minimum span of text containing the leaves of the subtree rooted at n. We define them as follows:
",4 Projectivity,[0],[0]
"Span-Start(n) =    Location(n) if n is a leaf min(Span-Start(lc(n)), Span-Start(rc(n)))
",4 Projectivity,[0],[0]
"otherwise
Span-End(n) =    Location(n) if n is a leaf max(Span-End(lc(n)), Span-End(rc(n)))
",4 Projectivity,[0],[0]
"otherwise
An equation tree T is called projective iff for every node n of T , either Span-End(lc(n))",4 Projectivity,[0],[0]
≤,4 Projectivity,[0],[0]
Span-Start(rc(n)) or Span-End(rc(n)),4 Projectivity,[0],[0]
≤,4 Projectivity,[0],[0]
Span-Start(lc(n)).,4 Projectivity,[0],[0]
"In other words, the span of the left child and the right child cannot intersect in a projective equation tree2.
",4 Projectivity,[0],[0]
"The key observation, as our corpus analysis indicates, is that for most sentences, there exists a trigger list, such that the equation tree representing the relation in the sentence is projective.",4 Projectivity,[0],[0]
However this might involve mapping two mentions of the same variable to different noun phrases.,4 Projectivity,[0],[0]
"Figure 1 shows an example of a projective equation tree, which requires different mentions of V1 to be mapped to different noun phrases.",4 Projectivity,[0],[0]
"If we had mapped both mentions of V1 to same noun phrase “a number”, the resulting equation tree would not have been projective.",4 Projectivity,[0],[0]
"We collected 385 sentences which represent an equation with one or two mentions of variables, and each number in the sentence used at most once in the equation.",4 Projectivity,[0],[0]
We found that only one sentence among these could not generate a projective equation tree.,4 Projectivity,[0],[0]
"(See Section 6.1 for details on dataset
2This is more general than the definition of projective trees used in dependency parsing (McDonald et al., 2005).
creation).",4 Projectivity,[0],[0]
"Therefore, we develop an algorithmic approach for predicting projective equation trees, and show empirically that it compares favourably with ones which do not make the projective assumption.",4 Projectivity,[0],[0]
"Equation parsing of a sentence involves predicting three components – Quantity Trigger List, Variable Trigger List and Equation Tree.",5 Predicting Equation Parse,[0],[0]
"We develop three structured prediction modules to predict each of the above components.
",5 Predicting Equation Parse,[0],[0]
"All our prediction modules take a similar form: given input x and output y, we learn a scoring function fw(x, y), which scores how likely is the output y given input x.",5 Predicting Equation Parse,[0],[0]
"The scoring function fw(x, y) is linear, fw(y) = wTφ(x, y), where φ(x, y) is a feature vector extracted from x and",5 Predicting Equation Parse,[0],[0]
y.,5 Predicting Equation Parse,[0],[0]
"The inference problem, that is, the prediction y∗ for an input x is then: y∗ = argmaxy∈Y fw(y), where Y is the set of all allowed values of y.",5 Predicting Equation Parse,[0],[0]
"Given input text and the quantities mentioned in it, the role of this step is to identify , for each quantity in the text, whether it should be part of the final equation.",5.1 Predicting Quantity Trigger List,[0],[0]
"For instance, in example 5 in Section 1, both “5” and “10” are not relevant for the final equation “V1 + V2 = 54”.",5.1 Predicting Quantity Trigger List,[0],[0]
"Similarly, in example 4, the number “two” is irrelevant for the equation “V1 + V2 = 80”.
",5.1 Predicting Quantity Trigger List,[0],[0]
"We define for each quantity q in the sentence, a boolean value Relevance(q), which is set to true if q is relevant for the final equation, and to false otherwise.",5.1 Predicting Quantity Trigger List,[0],[0]
"For the structured classification, the input x is the sentence along with a set of recognized quantities mentioned in it, and the output y is the relevance values for all quantities in the sentence.",5.1 Predicting Quantity Trigger List,[0],[0]
We empirically found that predicting all relevance values jointly performs better than having a binary classifier predict each one separately.,5.1 Predicting Quantity Trigger List,[0],[0]
"The feature function φ(x, y) used for the classification generates neighborhood features (from neighborhood of q) and quantity features (properties of the quantity mention).",5.1 Predicting Quantity Trigger List,[0],[0]
Details added to the appendix.,5.1 Predicting Quantity Trigger List,[0],[0]
The goal of this step is to predict the variable trigger list for the equation.,5.2 Predicting Variable Trigger List,[0],[0]
"Our structured classifier takes
as input the sentence x, and the output y is either one or two noun-phrases, representing variables in the final equation.",5.2 Predicting Variable Trigger List,[0],[0]
"As we pointed out earlier, multiple groundings might be valid for any given variable, hence there can be multiple valid variable trigger lists.",5.2 Predicting Variable Trigger List,[0],[0]
"For every sentence x, we construct a set Y of valid outputs.",5.2 Predicting Variable Trigger List,[0],[0]
Each element in Y corresponds to a valid variable trigger list.,5.2 Predicting Variable Trigger List,[0],[0]
"Finally, we aim to output only one of the elements of Y .
",5.2 Predicting Variable Trigger List,[0],[0]
We modified the standard structured prediction algorithm to consider “superset supervision” and take into account multiple gold structures for an input x.,5.2 Predicting Variable Trigger List,[0],[0]
"We assume access to N training examples of the form : (x1, Y1), (x2, Y2), . . .",5.2 Predicting Variable Trigger List,[0],[0]
", (xN , YN ), where each Yi is a set of valid outputs for the sentence xi.",5.2 Predicting Variable Trigger List,[0],[0]
"Since we want to output only one variable trigger list, we want to score at least one y from Yi higher than all other possible outputs, for each xi.",5.2 Predicting Variable Trigger List,[0],[0]
We use a modified latent structured SVM to learn the weight vector w.,5.2 Predicting Variable Trigger List,[0],[0]
The algorithm treats the best choice among all of Yi as a latent variable.,5.2 Predicting Variable Trigger List,[0],[0]
"At each iteration, for all xi, the algorithm chooses the best choice y∗i from the set Yi, according to the weight vector w. Then, w is updated by learning on all (xi, y∗i ) by a standard structured SVM algorithm.",5.2 Predicting Variable Trigger List,[0],[0]
The details of the algorithm are in Algorithm 1.,5.2 Predicting Variable Trigger List,[0],[0]
"The distinction from stan-
Algorithm 1 Structural SVM with Superset Supervision Input: Training data T = {(x1, Y1), (x2, Y2), . . .",5.2 Predicting Variable Trigger List,[0],[0]
", (xN , YN )} Output: Trained weight vector w 1: w ← w0 2: repeat 3: T ′",5.2 Predicting Variable Trigger List,[0],[0]
"← ∅ 4: for all (xi, Yi) ∈ T do 5: y∗i ← argmaxy∈Yi wTφ(xi, y) 6: T ′",5.2 Predicting Variable Trigger List,[0],[0]
← T ′,5.2 Predicting Variable Trigger List,[0],[0]
"∪ {(xi, y∗i )} 7: end for 8: Update w by running standard Structural
SVM algorithm on T ′
9: until convergence 10: return w
dard latent structural SVM is in line 5 of Algorithm 1.",5.2 Predicting Variable Trigger List,[0],[0]
"In order to get the best choice y∗i for input xi, we search only inside Yi, instead of all of Y .",5.2 Predicting Variable Trigger List,[0],[0]
"A similar formulation can be found in Björkelund and Kuhn
(2014).",5.2 Predicting Variable Trigger List,[0],[0]
"The features φ(x, y) used for variable trigger prediction include variable features (properties of noun phrase indicating variable) and neighborhood features (lexical features from neighborhood of variable mention).",5.2 Predicting Variable Trigger List,[0],[0]
"Details added to the appendix.
",5.2 Predicting Variable Trigger List,[0],[0]
"If the output of the classifier is a pair of noun phrases, we use a rule based variable coreference detector, to determine whether both noun phrases should have the same variable label or not.",5.2 Predicting Variable Trigger List,[0],[0]
"The rules for variable coreference are as follows :
1.",5.2 Predicting Variable Trigger List,[0],[0]
"If both noun phrases are the same, and they do not have the token “two” or “2”, they have the same label.
2.",5.2 Predicting Variable Trigger List,[0],[0]
"If the noun phrases are different, and the noun phrase appearing later in the sentence contains tokens “itself”, “the same number”, they have the same label.
3.",5.2 Predicting Variable Trigger List,[0],[0]
"In all other cases, they have different labels.
",5.2 Predicting Variable Trigger List,[0],[0]
"Finally, each noun phrase contributes one variable trigger to the variable trigger list.",5.2 Predicting Variable Trigger List,[0],[0]
It is natural to assume that the syntactic parse of the sentence could be very useful in addressing all the predictions we are making in the equation parsing tasks.,5.3 Predicting Equation Tree,[0],[0]
"However, it turns out that this is not the case – large portions of the syntactic parse will not be part of the equation parse, hence we need the aforementioned modules to address this.",5.3 Predicting Equation Tree,[0],[0]
"Nevertheless, in the next task of predicting the equation tree, we attempted to constraint the output space using guidance from the syntactic tree; we found, though, that even enforcing this weak level of output expectation is not productive.",5.3 Predicting Equation Tree,[0],[0]
"This was due to the poor performance of current syntactic parsers on the equation data (eg., in 32% of sentences, the Stanford parser made a mistake which does not allow recovering the correct equation).
",5.3 Predicting Equation Tree,[0],[0]
"The tree prediction module receives the trigger list predicted by the previous two modules, and the goal is to create an equation tree using the trigger list as the leaves of that tree.",5.3 Predicting Equation Tree,[0],[0]
"The input x is the sentence and the trigger list, and the output y is the equation tree representing the relation described in the sentence.",5.3 Predicting Equation Tree,[0],[0]
"We assume that the output will be a projective
equation tree.",5.3 Predicting Equation Tree,[0],[0]
"For features φ(x, y), we extract for each non-leaf node n of the equation tree y, neighborhood features (from neighborhood of node spans of n’s children), connecting text features (from text between the spans of n’s children) and number features (properties of number in case of leaf nodes).",5.3 Predicting Equation Tree,[0],[0]
"Details are included in the appendix.
",5.3 Predicting Equation Tree,[0],[0]
"The projectivity assumption implies that the final equation tree can be generated by combining only adjacent nodes, once the set of leaves is sorted based on Span-Start(·) values.",5.3 Predicting Equation Tree,[0],[0]
This allows us to use CKY algorithm for inference.,5.3 Predicting Equation Tree,[0],[0]
A natural approach to further reduce the output space is to conform to the projective structure of the syntactic parse of the sentence.,5.3 Predicting Equation Tree,[0],[0]
"However, we found this to adversely affect performance, due to the poor performance of syntactic parser on equation data.",5.3 Predicting Equation Tree,[0],[0]
"Lexicon To bootstrap the equation parsing process, we developed a high precision lexicon to translate mathematical expressions to operations and orders, like “sum of A and B” translates to “A+B”, “A minus B” translates to “A-B”, etc.",5.3 Predicting Equation Tree,[0],[0]
(where A and B denote placeholder numbers or expressions).,5.3 Predicting Equation Tree,[0],[0]
"At each step of CKY, while constructing a node n of the equation tree, we check for a lexicon text expression corresponding to node n. If found, we allow only the corresponding operation (and order) for node n, and do not explore other operations or orders.",5.3 Predicting Equation Tree,[0],[0]
We show empirically that reducing the space using this greedy lexicon matching help improve performance.,5.3 Predicting Equation Tree,[0],[0]
We found that using the lexicon rules as features instead of hard constraints do not help as much.,5.3 Predicting Equation Tree,[0],[0]
"Note that our lexicon comprises only generic math concepts, and around 50% of the sentences in our dataset do not contain any pattern from the lexicon.
",5.3 Predicting Equation Tree,[0],[0]
"Finally, given input sentence, we first predict the quantity trigger and the variable trigger lists.",5.3 Predicting Equation Tree,[0],[0]
"Given the complete trigger list, we predict the equation tree relating the components of the trigger list.",5.3 Predicting Equation Tree,[0],[0]
"A natural approach could be to jointly learn to predict all three components, to capture the dependencies among them.",5.4 Alternatives,[0],[0]
"To investigate this, we developed a structured SVM which predicts all components jointly, using the union of the features of each component.",5.4 Alternatives,[0],[0]
"We use approximate inference, first enumerating possible trigger lists, and then equation trees,
and find the best scoring structure.",5.4 Alternatives,[0],[0]
"However, this method did not outperform the pipeline method.",5.4 Alternatives,[0],[0]
"The worse performance of joint learning is due to: (1) search space being too large for the joint model to do well given our dataset size of 385, and (2) our independent classifiers being good enough, thus supporting better joint inference.",5.4 Alternatives,[0],[0]
"This tradeoff is strongly supported in the literature (Punyakanok et al., 2005; Sutton and McCallum, 2007).
",5.4 Alternatives,[0],[0]
"Another option is to enforce constraints between trigger list predictions, such as, variable triggers should not overlap with the quantity triggers.",5.4 Alternatives,[0],[0]
"However, we noticed that often noun phrases returned by the Stanford parser were noisy, and would include neighboring numbers within the extracted noun phrases.",5.4 Alternatives,[0],[0]
This prevented us from enforcing such constraints.,5.4 Alternatives,[0],[0]
"We now describe the data set, and the annotation procedure used.",6 Experimental Results,[0],[0]
"We then evaluate the system’s performance on predicting trigger list, equation tree, and the complete equation parse.",6 Experimental Results,[0],[0]
We created a new dataset consisting of 385 sentences extracted from algebra word problems and financial news headlines.,6.1 Dataset,[0],[0]
"For algebra word problems, we used the MIT dataset (Kushman et al., 2014), and two high school mathematics textbooks, Elementary Algebra (College of Redwoods) and Beginning and Intermediate Algebra (Tyler Wallace).",6.1 Dataset,[0],[0]
"Financial news headlines were extracted from The Latest News feed of MarketWatch, over the month of February, 2015.",6.1 Dataset,[0],[0]
"All sentences with information describing a mathematical relation among at most two (possibly coreferent) variables, were chosen.",6.1 Dataset,[0],[0]
"Next, we pruned sentences which require multiple uses of a number to create the equation.",6.1 Dataset,[0],[0]
"This only removed a few time related sentences like “In 10 years, John will be twice as old as his son.”.",6.1 Dataset,[0],[0]
"We empirically found that around 97% of sentences describing a relation fall under the scope of our dataset.
",6.1 Dataset,[0],[0]
The annotators were shown each sentence paired with the normalized equation representing the relation in the sentence.,6.1 Dataset,[0],[0]
"For each variable in the equation, the annotators were asked to mark spans of
text which best describe what the variable represents.",6.1 Dataset,[0],[0]
The annotation guidelines are provided in the appendix.,6.1 Dataset,[0],[0]
We wanted to consider only noun phrase constituents for variable grounding.,6.1 Dataset,[0],[0]
"Therefore, for each annotated span, we extracted the noun phrase with maximum overlap with the span, and used it to represent the variables.",6.1 Dataset,[0],[0]
"Finally, a tuple with each variable being mapped to one of the noun phrases representing it, forms a valid output grounding (variable trigger list).",6.1 Dataset,[0],[0]
We computed interannotator agreement on the final annotations where only noun phrases represent variables.,6.1 Dataset,[0],[0]
"The agreement (kappa) was 0.668, indicating good agreement.",6.1 Dataset,[0],[0]
The average number of mention annotations per sentence was 1.74.,6.1 Dataset,[0],[0]
"In this section, we evaluate the performance of the individual modules of the equation parsing process.",6.2 Equation Parsing Modules,[0],[0]
We report Accuracy - the fraction of correct predictions.,6.2 Equation Parsing Modules,[0],[0]
Table 3 shows the 5-fold cross validation accuracy of the various modules.,6.2 Equation Parsing Modules,[0],[0]
"In each case, we also report accuracy by removing each feature group, one at a time.",6.2 Equation Parsing Modules,[0],[0]
"In addition, for equation tree prediction, we also show the effect of lexicon, projectivity, conforming to syntactic parse constraints, and using lexicon as features instead of hard constraints.",6.2 Equation Parsing Modules,[0],[0]
"For all our experiments, we use the Stanford Parser (Socher et al., 2013), the Illinois POS tagger (Roth and Zelenko, 1998) and the Illinois-SL structured prediction package (Chang et al., 2015).",6.2 Equation Parsing Modules,[0],[0]
"In this section, we evaluate the performance of our system on the overall equation parsing task.",6.3 Equation Parsing Results,[0],[0]
"We report Equation Accuracy - the fraction of sentences for which the system got the equation correct, and Equation+Grounding Accuracy - the fraction of sentences for which the system got both the equation and the grounding of variables correct.",6.3 Equation Parsing Results,[0],[0]
"Table 4 shows the overall performance of our system, on a 5-fold cross validation.",6.3 Equation Parsing Results,[0],[0]
We compare against Joint Learning - a system which jointly learns to predict all relevant components of an equation parse (Section 5.4).,6.3 Equation Parsing Results,[0],[0]
"We also compare with SPF (Artzi and Zettlemoyer, 2013), a publicly available semantic parser, which can learn from sentence-logical form pairs.",6.3 Equation Parsing Results,[0],[0]
"We train SPF with sentence-equation pairs
and a seed lexicon for mathematical terms (similar to ours), and report equation accuracy.",6.3 Equation Parsing Results,[0],[0]
"Our structured predictors pipeline approach is shown to be superior to both Joint Learning and SPF.
",6.3 Equation Parsing Results,[0],[0]
SPF gets only a few sentences correct.,6.3 Equation Parsing Results,[0],[0]
"We attribute this to the inability of SPF to handle overlapping mentions (like in Example 4), as well as its approach of parsing the whole sentence to the final output form.",6.3 Equation Parsing Results,[0],[0]
The developers of SPF also confirmed 3 that it is not suitable for equation parsing and that these results are expected.,6.3 Equation Parsing Results,[0],[0]
"Since equation parsing is a more involved process, a slight adaptation of SPF does not seem possible, necessitating a more involved process , of the type we propose.",6.3 Equation Parsing Results,[0],[0]
"Our approach, in contrast to SPF, can handle overlapping mentions, selects triggers from text, and parses the trigger list to form equations.
",6.3 Equation Parsing Results,[0],[0]
3Private communication,6.3 Equation Parsing Results,[0],[0]
"For variable trigger list prediction, around 25% of the errors were due to the predictor choosing a span which is contained within the correct span, e.g., when the target noun phrase is “The cost of a child’s ticket”, our predictor chose only “child’s ticket”.",6.4 Error Analysis,[0],[0]
"Although this choice might be sufficient for downstream tasks, we consider it to be incorrect in our current evaluation.",6.4 Error Analysis,[0],[0]
Another 25% of the errors were due to selection of entities which do not participate in the relation.,6.4 Error Analysis,[0],[0]
"For example, in “A rancher raises 5 times as many cows as horses.”, our predictor chose “A rancher” and “cows” as variables, whereas the relation exists between “cows” and “horses”.",6.4 Error Analysis,[0],[0]
"For the prediction of the equation tree, we found that 35% of the errors were due to rare math concepts expressed in text.",6.4 Error Analysis,[0],[0]
"For example, “7 dollars short of the price” represents 7 dollars should be subtracted from the price.",6.4 Error Analysis,[0],[0]
These errors can be handled by carefully augmenting the lexicon.,6.4 Error Analysis,[0],[0]
"Another 15% of the errors were due to lack of world knowledge, requiring understanding of time, speed, and distance.",6.4 Error Analysis,[0],[0]
This paper investigates methods that identify and understand mathematical relations expressed in text.,7 Conclusion,[0],[0]
"We introduce the equation parsing task, which involves generating an equation from a sentence and identifying what the variables represent.",7 Conclusion,[0],[0]
"We define the notion of projectivity, and construct a high precision lexicon, and use these to reduce the equation search space.",7 Conclusion,[0],[0]
Our experimental results are quite satisfying and raise a few interesting issues.,7 Conclusion,[0],[0]
"In particular, it suggests that predicting equation parses using a pipeline of structured predictors performs better than jointly trained alternatives.",7 Conclusion,[0],[0]
"As discussed, it also points out the limitation of the current NLP tools in supporting these tasks.",7 Conclusion,[0],[0]
Our current formulation has one key limitation; we only deal with expressions that are described within a sentence.,7 Conclusion,[0],[0]
"Our future work will focus on lifting this restriction, in order to allow relations expressed across multiple sentences and multiple relations expressed in the same sentence.",7 Conclusion,[0],[0]
Code and dataset are available at http://cogcomp.cs.illinois.edu/ page/publication_view/800.,7 Conclusion,[0],[0]
"This work is funded by DARPA under agreement number FA8750-13-2-0008, and a grant from the Allen Institute for Artificial Intelligence (allenai.org).",Acknowledgements,[0],[0]
A.1 Quantity Trigger List Prediction,A Features,[0],[0]
"The feature function φ(x, y) used for the classification generates the following features :
1.",A Features,[0],[0]
"Neighborhood features : For each quantity q in the input sentence, we add unigrams and bigrams generated from a window around q, part of speech tags of neighborhood tokens of q. We conjoin these features with Relevance(q).
2.",A Features,[0],[0]
"Quantity Features : For each quantity q, we add unigrams and bigrams of the phrase representing the quantity.",A Features,[0],[0]
"Also, we add a feature indicating whether the number is associated with number one or two, and whether it is the only number present in the sentence.",A Features,[0],[0]
"These features are also conjoined with Relevance(q).
",A Features,[0],[0]
A.2 Variable Trigger List Prediction,A Features,[0],[0]
"The features φ(x, y) used for variable trigger prediction are as follows:
1.",A Features,[0],[0]
"Variable features : Unigrams and bigrams generated from the noun phrase representing variables, part of speech tags of tokens in noun phrase representing variables.
",A Features,[0],[0]
2.,A Features,[0],[0]
"Neighborhood Features : Unigrams and POS tags from neighborhood of variables.
",A Features,[0],[0]
"All the above features are conjoined with two labels, one denoting whether y has two variables or one, and the second denoting whether y has two variables represented by the same noun phrase.
",A Features,[0],[0]
"A.3 Equation Tree Prediction For features φ(x, y), we extract for each non-leaf node n of the equation tree y, the following:
1.",A Features,[0],[0]
"Neighborhood Features : Unigrams, bigrams and POS tags from neighborhood of Span-Start(lc(n)), Span-Start(rc(n)),
",A Features,[0],[0]
Span-End(lc(n)),A Features,[0],[0]
"and Span-End(rc(n)), conjoined with (n) and ORDER(n).
",A Features,[0],[0]
2.,A Features,[0],[0]
Connecting Text Features :,A Features,[0],[0]
"Unigrams, bigrams and part of speech tags between min(Span-End(lc(n)),Span-End(rc(n))) and max(Span-Start(lc(n)), Span-Start(rc(n))), conjoined with (n) and ORDER(n).
",A Features,[0],[0]
3.,A Features,[0],[0]
Number Features :,A Features,[0],[0]
"In case we are combining two leaf nodes representing quantity triggers, we add a feature signifying whether one number is larger than the other.",A Features,[0],[0]
The annotators were shown each sentence paired with the normalized equation representing the relation in the sentence.,B Annotation Guidelines,[0],[0]
"For each variable in the equation, the annotators were asked to mark spans of text which best describe what the variable represents.",B Annotation Guidelines,[0],[0]
They were asked to annotate associated entities if exact variable description was not present.,B Annotation Guidelines,[0],[0]
"For instance, in example 3 (Section 1), the relation holds between the speed of bird and the speed of wind.",B Annotation Guidelines,[0],[0]
"However, “speed” is not explicitly mentioned in the sentence.",B Annotation Guidelines,[0],[0]
"In such cases, the annotators were asked to annotate the associated entities “the wind” and “a bird” as representing variables.
",B Annotation Guidelines,[0],[0]
"The guidelines also directed annotators to choose the longest possible mention, in case they feel the mention boundary is ambiguous.",B Annotation Guidelines,[0],[0]
"As a result, in the sentence, “City Rentals rent an intermediate-size car for 18.95 dollars plus 0.21 per mile.”",B Annotation Guidelines,[0],[0]
", the phrase “City Rentals rent an intermediate-size car” was annotated as representing variable.",B Annotation Guidelines,[0],[0]
We allow multiple mentions to be annotated for the same variable.,B Annotation Guidelines,[0],[0]
"In example 2 (Section 1), both “a number” and “the same number” were annotated as representing the same variable.",B Annotation Guidelines,[0],[0]
"Identifying mathematical relations expressed in text is essential to understanding a broad range of natural language text from election reports, to financial news, to sport commentaries to mathematical word problems.",abstractText,[0],[0]
This paper focuses on identifying and understanding mathematical relations described within a single sentence.,abstractText,[0],[0]
"We introduce the problem of Equation Parsing – given a sentence, identify noun phrases which represent variables, and generate the mathematical equation expressing the relation described in the sentence.",abstractText,[0],[0]
We introduce the notion of projective equation parsing and provide an efficient algorithm to parse text to projective equations.,abstractText,[0],[0]
"Our system makes use of a high precision lexicon of mathematical expressions and a pipeline of structured predictors, and generates correct equations in 70% of the cases.",abstractText,[0],[0]
"In 60% of the time, it also identifies the correct noun phrase → variables mapping, significantly outperforming baselines.",abstractText,[0],[0]
We also release a new annotated dataset for task evaluation.,abstractText,[0],[0]
EQUATION PARSING : Mapping Sentences to Grounded Equations,title,[0],[0]
"Support vector machines (SVM) is an established algorithm for classification with two categories (Vapnik, 1998; Smola and Schlkopf, 1998; Steinwart and Christmann, 2008; Friedman et al., 2009).",1. Introduction,[0],[0]
The method finds the maximum margin separating hyperplane; it finds the hyperplane dividing the input space (perhaps after mapping the data to a higher dimensional space) into two categories and maximizing the minimum distance from a point to the hyperplane.,1. Introduction,[0],[0]
"SVM can also be adapted to allow for imperfect classification, in which case we speak of soft margin SVM.
",1. Introduction,[0],[0]
"Given the success of SVM at binary classification, many 1Harris School of Public Policy, University of Chicago, Chicago, IL, USA.",1. Introduction,[0],[0]
"Correspondence to: Guillaume A. Pouliot <guillaumepouliot@uchicago.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
attempts have been made at extending the methodology to accommodate classification with K > 2 categories (Sun et al., 2017; Dogan et al., 2016; Lopez et al., 2016; Kumar et al., 2017, survey available in Ma and Guo, 2014).",1. Introduction,[0],[0]
"Lee, Lin and Wahba (2004) propose what is arguably the natural multicategory generalization of binary SVM.",1. Introduction,[0],[0]
"For instance, their multicategory SVM (MSVM) is Fisher consistent (i.e., the classification rule it produces converges to the Bayes rule), which is a key property and motivation for the use standard SVM.",1. Introduction,[0],[0]
"Furthermore, it encompasses standard SVM as a special case.
",1. Introduction,[0],[0]
"However, the method has not been widely used in application, nor has it been studied from a statistical perspective, the way SVM has been.",1. Introduction,[0],[0]
"Amongst the machine learning community, MSVM has not gathered popularity commensurate to that of SVM.",1. Introduction,[0],[0]
"Likewise, three major publications (Jiang et al., 2008; Koo et al., 2008; Li et al., 2011) have established Donsker theorems for SVM, and none have done so for MSVM.
",1. Introduction,[0],[0]
"Interestingly, computation and statistical analysis of MSVM are hindered by the same obstable.",1. Introduction,[0],[0]
The optimization problem which MSVM consists of is done under a sum-to-zero constraint on the vector argument.,1. Introduction,[0],[0]
This makes both the numerical optimization task and the statistical asymptotic analysis of the estimator more challenging.,1. Introduction,[0],[0]
The numerical optimization is substantially slowed down by the equality constraint1 as detailed in Table 1.,1. Introduction,[0],[0]
"Likewise, standard methods for deriving Donsker theorems and limit distribution theory do not apply to such constrained vectors of random variables.2
In a separate strain of literature, Mroueh, Poggio, Rosasco and Slotine (2012) have proposed the simplex-cone SVM (SC-SVM), a multicategory classifier developped within the vector reproducing kernel Hilbert space set-up.",1. Introduction,[0],[0]
"The SCSVM optimization program is computationally tractable, and in particular does away with the equality constraint slowing down the primal and dual formulations of MSVM (Lee
1In fact, a “hack” sometimes used is to ignore the equality constraint in the primal or dual formulation.",1. Introduction,[0],[0]
"This can result in arbitrarily large distorsions of the optimal solution.
",1. Introduction,[0],[0]
"2For instance, the covariance matrix of a vector of random variables constrained to sum to zero is not positive definite.
",1. Introduction,[0],[0]
"et al., 2004).",1. Introduction,[0],[0]
"Nevertheless, its use has remained marginal, arguably due to more limited interpretability, e.g., the notion of distance is captured via angles and the nesting of binary SVM as a special case is not entirely straightforward.
",1. Introduction,[0],[0]
"As our main contribution, we show that MSVM and SCSVM are in fact exactly equivalent.",1. Introduction,[0],[0]
"As a direct consequence, we deliver faster computations for MSVM.",1. Introduction,[0],[0]
Simulations such as those presented in Table 1 display speed gains or an order of magnitude.,1. Introduction,[0],[0]
"Furthermore, the equivalence with an unconstrained estimator allows for statistical analysis of MSVM.",1. Introduction,[0],[0]
"As a second contribution, we deliver a Donsker theorem for MSVM, as well as an asymptotic covariance formula with sample analog.",1. Introduction,[0],[0]
"A third contribution is to use the asymptotic analysis result to propose a statistically efficient, inverse-variance weighted modification of Onevs-Rest.",1. Introduction,[0],[0]
"Finally, as a fourth contribution, we show that the asymptotic analysis allows us to partially answer, with analytic characterizations, the open question relating to the very competitive performance of the seemingly more naive One-vs-Rest method against MSVM.
",1. Introduction,[0],[0]
The fourth contribution is important because it provides analytical substance to a long-standing open question.,1. Introduction,[0],[0]
"To be sure, the different attempts at developping a multicategory generalization of binary SVM can be understood as subscribing to one of two broad approaches.",1. Introduction,[0],[0]
"The first approach consists in doing multicategory classification using the standard, binary SVM.",1. Introduction,[0],[0]
"For instance, the popular One-vs-Rest approach works as follows: to predict the category of a point in a test set3 (i.e. out of sample), run K binary SVMs where the first category is one of the original K categories, and the second category is the union of the remaining K 1 categories.",1. Introduction,[0],[0]
The predicted category is the one that was picked against all others with the greatest “confidence”.,1. Introduction,[0],[0]
"In practice, the confidence criteria used is the distance of the test point to the separating hyperplane (we show in Subsection 3.1 that even this can be improved according to statistical considerations).",1. Introduction,[0],[0]
"The second approach consists in generalizing the standard SVM to develop a single machine which implements multicategory classification solving a single, joint optimization problem.",1. Introduction,[0],[0]
"Many such algorithms have been suggested (Weston and Watkins, 1999; Crammer and Singer, 2002; Lee et al., 2004).",1. Introduction,[0],[0]
"Intuition would suggest that joint optimization makes for a more statistically efficient procedure, and for superior out-of-sample prediction performance.",1. Introduction,[0],[0]
"However, in a quite counterintuitive turn of events, it has been widely observed in practice that multicategory classification with binary machines offers a performance (for instance, in out-of-sample classification) which is competitive with, and sometimes superior to, that of single-machine multicategory SVM algorithms.",1. Introduction,[0],[0]
"This phenomenon is widely acknowledged (Rifkin and Klautau, 2004) but very little
3Or the fitted category of a point in the training set.
theory has been put forth to explain it.
",1. Introduction,[0],[0]
"We make some progress towards an analytical characterization of the comparative performance, and are able to suggest an explanation as to the competitive, and sometimes superior, empirical performance of One-vs-Rest compared to MSVM.",1. Introduction,[0],[0]
"We argue that, in some respect, One-vs-Rest makes a more efficient use of the information contained in the data.
",1. Introduction,[0],[0]
The remainder of the paper is organized as follows.,1. Introduction,[0],[0]
"Section 2 defines both MSVM and SC-SVM, and contains the proof of the equivalence between the two methods.",1. Introduction,[0],[0]
"Section 3 gives the Donsker theorem for MSVM, and describes how the asymptotic distribution may be used for more efficient classification.",1. Introduction,[0],[0]
Section 4 suggests an analytical explanation for the surprisingly competitive performance of One-vs-Rest classifiers versus MSVM.,1. Introduction,[0],[0]
Section 5 discusses and concludes.,1. Introduction,[0],[0]
The multicategory SVM (MSVM) of Lee et al. (2004) is arguably the more elegant and natural generalization of SVM to multicategory data.,2. Equivalence,[0],[0]
"However, its implementation, even for moderate size data sets, is complicated by the presence of a sum constraint on the vector argument.
",2. Equivalence,[0],[0]
The simplex encoding of Mroueh et al. (2012) is relieved of the linear constraint on the vector argument.,2. Equivalence,[0],[0]
"However, we believe the simplex encoding is not more widely used because it is not known what standard encoding it corresponds to, making it challenging for practitioners to carry out interpretable classification analysis.",2. Equivalence,[0],[0]
"The following result resolves both issues, making it of practical interest for analysts and researchers using multicategory classification methods.
",2. Equivalence,[0],[0]
"We define MSVM and SC-SVM, and establish their equivalence.",2. Equivalence,[0],[0]
The presentation is done with finite-dimensional kernels for ease of exposition.,2. Equivalence,[0],[0]
"Remark 3 details the generalization to infinite-dimensional kernels in reproducing kernel Hilbert spaces.
",2. Equivalence,[0],[0]
"With K categories, data is of the form (x i , y i ) 2",2. Equivalence,[0],[0]
Rp ⇥,2. Equivalence,[0],[0]
"{1, ...,K}, i = 1, ..., N .",2. Equivalence,[0],[0]
"When carrying out multicategory classification, different choices of encodings of the category variables y
i lead to optimization problems that are differently formulated and implemented.
",2. Equivalence,[0],[0]
"For their multicategory SVM (MSVM), Lee et al. (2004)",2. Equivalence,[0],[0]
"encode y
i associated with category k 2 {1, ...,K} as a Ktuple with 1 in the kth entry and 1
K 1 in every other entry.",2. Equivalence,[0],[0]
"For instance,
”yi in category 2” , yi = ✓
1 K 1 , 1, 1 K 1 , · · · , 1 K 1
◆
.
",2. Equivalence,[0],[0]
"The loss function they suggest is then based on the difference between the decision function and the encoded y
i
’s.
Specifically, in the case of finite-dimensional feature maps, they suggest minimizing
1
n
n
X
i=1
L(y i ) ·",2. Equivalence,[0],[0]
[Wx i + b,2. Equivalence,[0],[0]
y,2. Equivalence,[0],[0]
"i
]+ +
2
|||W |||, (1)
where |||W ||| = trace(WTW ), and L(y i ) = 1 K e yi is a vector that has 0 in the kth entry when y i
designates category k, and a 1 in every other entry.",2. Equivalence,[0],[0]
"Importantly, the decision function is constrained to sum to zero, i.e. 1T
k (Wx+ b) = 0, 8 x.",2. Equivalence,[0],[0]
"The function [·]+applies pointwise to its vector argument.
",2. Equivalence,[0],[0]
Mroueh et al. (2012) preconize an encoding that does away with the sum-to-zero constraint.,2. Equivalence,[0],[0]
"The loss function they suggest is based on the inner product between the decision function and their encoding of y
i ’s. Likewise in the finite-dimensional case, the penalized minimization problem entailed by their loss function is
1
n
n
X
i=1
X
y 0 6=y

1 K 1 + D c y 0 , ˜Wx",2. Equivalence,[0],[0]
"i + ˜b E
+
+
˜
2
||| ˜W |||,
(2)
where c y is a unit vector in RK 1 which encodes the response; it is a row of a simplex coding matrix, which is the key building block of their construction.
",2. Equivalence,[0],[0]
"A simplex coding matrix (Mroueh et al., 2012; Pires et al., 2013) is a matrix C 2 RK⇥(K 1) such that its rows c
k
satisfy (i) kc k k22 = 1; (ii) cTi cj = 1 K 1 for i 6= j ; and (iii) P K
k=1 ck",2. Equivalence,[0],[0]
=,2. Equivalence,[0],[0]
0K 1.,2. Equivalence,[0],[0]
It encodes the responses as unit vectors in RK 1 having maximal equal angle with each other.,2. Equivalence,[0],[0]
"Further note that, because its domain is a (K 1)- dimensional subspace of RK , any given C has a unique inverse operator ˜C defined on the image {x 2 RK : 1T
K x = 0}.
",2. Equivalence,[0],[0]
"For a given choice of simplex encoding defined by C, the operator C : RK 1 !",2. Equivalence,[0],[0]
"RK can be thought of as mapping decision functions and encoded y’s from the unrestricted simplex encoding space to the standard, restricted encoding space used by Lee et al. (2004).
",2. Equivalence,[0],[0]
A natural question is then: if f(x) = Wx+ b and ˜f(x) = ˜Wx,2. Equivalence,[0],[0]
"+ ˜b are optimal solutions to (1) and (2), respectively, are ˜C (Wx+ b) and C( ˜Wx+˜b) then optimal solutions to (2) and (1), respectively?",2. Equivalence,[0],[0]
We show that this is in fact the case.,2. Equivalence,[0],[0]
"That is, both problems are exactly equivalent.
",2. Equivalence,[0],[0]
We now show the problems are equivalent.,2. Equivalence,[0],[0]
"The equivalence
of the loss functions is straighforward.",2. Equivalence,[0],[0]
"Indeed,
P
y0 6=y
h
fy0(x) + 1
K 1
i
+ =
P
y0 6=yi

⇣
C
˜ f(x)
⌘
y0 +
1 K 1
+
=
P
y0 6=yi
hD
cy0 , ˜ f(x)
E
+ 1 K 1
i
+ ,
(3)
which is exactly the SC-SVM loss of Mroueh et al. (2004).",2. Equivalence,[0],[0]
"Writing out f and ˜f as linear functions, the identity becomes
P
y0 6=y
h
!",2. Equivalence,[0],[0]
"y0x+ by0 + 1
K 1
i
+
=
P
y0 6=yi
hD
cy0 , ˜ Wx+ ˜",2. Equivalence,[0],[0]
"b
E
+ 1 K 1
i
+
(4)
with f(x) = Wx + b and ˜f(x) = ˜Wx",2. Equivalence,[0],[0]
"+ ˜b, and !",2. Equivalence,[0],[0]
"y 0 is the (y0)th row of W .
",2. Equivalence,[0],[0]
"Equality (up to a change of tuning parameter) of the penalty relies on the key observation of this exercise, which is that CTC is the diagonal matrix K
K 1IK 1.",2. Equivalence,[0],[0]
"It then immediately follows that
K 1 K trace
⇣
˜
W
T ˜
W
⌘
= K 1 K trace W T C T CW
= trace W T W .
(5)
",2. Equivalence,[0],[0]
"In conclusion, we have
1
n
n X
i=1
L(yi) ·",2. Equivalence,[0],[0]
[Wxi + b yi]+,2. Equivalence,[0],[0]
"+
2
|||W |||
=
1
n
n X
i=1
X
y0 6=y

1 K 1 + D c 0 y, ˜ Wx+ ˜",2. Equivalence,[0],[0]
"b E
+
+ (K 1) 2K ||| ˜W |||,
(6) as desired.
",2. Equivalence,[0],[0]
We now prove the key linear algebra result.,2. Equivalence,[0],[0]
There are other ways (see remarks below) to prove this result.,2. Equivalence,[0],[0]
"However, it is desirable to establish the equivalence between the more practical encoding and the more interpretable one in an intuitive way.",2. Equivalence,[0],[0]
"The geometric proof given below accomplishes this by establishing the equivalence through a volume preservation argument.
",2. Equivalence,[0],[0]
"PROPOSITION
Let C 2 RK⇥(K 1) be a simplex coding matrix.",2. Equivalence,[0],[0]
Then its columns are orthogonal and have norm,2. Equivalence,[0],[0]
"q K
K 1 .
",2. Equivalence,[0],[0]
"Proof
The key observation (Gantmacher, 1959, vol. 1, p.251) is that
V = p G, (7)
where V = V (C) is the volume of the parallelipiped spanned by the columns of C, and G = G(C) is the Grammian of C. The Grammian (defined below) extends the notion of volume to objects determined by more vectors than the space they are embedded in has dimensions.
",2. Equivalence,[0],[0]
"Let C·i denote the ith column of C, and recall that |||C||| denotes the sum of the squared entries of C. Note that V  kC·1k · · · · · C·(K 1)
, which holds with equality if and only if all columns are mutually orthogonal.",2. Equivalence,[0],[0]
"Further note that
kC·1k·· · ·· C·(K 1) 
r
|||C||| K 1
!",2. Equivalence,[0],[0]
"K 1
=
✓
K
K 1
◆ K 1 2
which holds with equality if and only if kC·ik = q
K K 1 , i = 1, ...,K 1.",2. Equivalence,[0],[0]
"Hence, if G = ⇣ K K 1 ⌘ K 1 ,
it must be that the statement of the proposition is true.
",2. Equivalence,[0],[0]
We compute the Grammian.,2. Equivalence,[0],[0]
"By Gantmacher (1959),
G(C) = K X
i=1
det 2 (C i·) , (8)
where C i· is C with the ith row removed.",2. Equivalence,[0],[0]
"Noting that C i·CT i· is a circulant matrix and using the relevant determinant formula, we find that
P
K i=1 det C i·CT",2. Equivalence,[0],[0]
"i·
= K · det
0
B
@
1 1 K 1
. . .",2. Equivalence,[0],[0]
"1
K 1 1
1
C
A
= K · Q K 2 j=0
⇣
1 1 K 1 P K 2 m=1
⇣
e 2⇡ij K 1
⌘
m
⌘
= K · ⇣
1 K 2 K 1
⌘
· Q K 2 j=1
⇣
1 1 K 1 P K 2 m=1
⇣
e 2⇡ij K 1
⌘
m
⌘
= K · ⇣
1 K 1
⌘
· Q K 2 j=1
⇣
1 + 1 K 1
⌘
= K · ⇣
1 K 1
⌘ · ⇣ K
K 1
⌘ K 2
=
⇣
K
K 1
⌘ K 1 ,
which proves the claim.
",2. Equivalence,[0],[0]
"Note that we have used the orthogonality of the complex exponential basis,
n 1 X
m=0
e 2⇡ijm n =
⇢
n, j mod n = 0 0, o.w. .
",2. Equivalence,[0],[0]
⇤,2. Equivalence,[0],[0]
"The immediate implication of the above argument and proposition is that we may compute MSVM using the equivalent, unconstrained representation of SC-SVM.",2. Equivalence,[0],[0]
"In Table 1, we display “clock-on-the-wall” computation times.",2. Equivalence,[0],[0]
"Collected simulations suggest gains of an order of magnitude.
",2. Equivalence,[0],[0]
Remark 1,2. Equivalence,[0],[0]
"The result of the Proposition holds for a more general simplex matrix C 2 RK⇥D, 0 < D < K, having rows of equal norm and maximal equal angle between them.
",2. Equivalence,[0],[0]
"Remark 2 A different argument of a more algebraic geometry flavor can be given, which suggests the choice of a canonical C. Given K, there exists a simplex coding matrix C such that pairwise coordinate projections (i.e. projections on a plane spanned by two distinct standard basis vectors) yield equidistant points around a circle (“a pie with equal sized slices”).",2. Equivalence,[0],[0]
"This is trivial for K = 3, and geometrically obvious for K = 4.",2. Equivalence,[0],[0]
Call such a simplex coding matrix a canonical coding matrix.,2. Equivalence,[0],[0]
"From this geometric observation, the orthogonality of the columns readily follows: for any two disctinct columns of C, say C·i, C·j , i 6= j, we have that
hC·i, C·ji = K X
t=1
cos
✓
t⇡
K/2
◆
sin
✓
t⇡
K/2
◆
=
1
2
K
X
t=1
sin
✓
t⇡
K/4
◆
= 0.
",2. Equivalence,[0],[0]
The length of the columns can be established as in the proof of the Proposition.,2. Equivalence,[0],[0]
"Furthermore, and somewhat surprisingly, we can go the other way and construct C from the condition on its pairwise coordinate projections (Chan, 2013).
",2. Equivalence,[0],[0]
Remark 3,2. Equivalence,[0],[0]
The equivalence of MSVM and SC-SVM immediately generalizes to the infinite-dimensional kernel case.,2. Equivalence,[0],[0]
"The representer theorem yields that f
j (x) = b j +
P
n i=1",2. Equivalence,[0],[0]
"aijK(xi, x)",2. Equivalence,[0],[0]
"for j = 1, ...,K with sum-to-zero constraint.",2. Equivalence,[0],[0]
Then (3) holds in the same notation.,2. Equivalence,[0],[0]
"Letting A denote the matrix with (i, j) entry a
ij and K the matrix with (i, j) entry K(x
i , x j ), the penalty equivalence follows from observing that
trace(ATKA) =",2. Equivalence,[0],[0]
"trace(C ˜ATK ˜ACT )
",2. Equivalence,[0],[0]
"= trace(CTC ˜ATK ˜A) = K
K 1trace( ˜ATK ˜A).
",2. Equivalence,[0],[0]
"We then get, again, equality of the objective functions up to the tuning parameter.",2. Equivalence,[0],[0]
"By considering MSVM as penalized M -estimator, one can in principle work out its asymptotic distribution.",3. Donsker Theorem,[0],[0]
"In (2), under simplex encoding, MSVM is phrased as an unconstrained M -estimator, and the asymptotic distribution for the estimated parameters –and thus separating hyperplane– can be obtained using standard empirical process theory.",3. Donsker Theorem,[0],[0]
The expression for the covariance matrices presented below are novel and of practical use.,3. Donsker Theorem,[0],[0]
"To the best of my knowledge, if a practitioner wants to compute the asymptotic covariance matrix of SVM or MSVM –which is essential in order to know where extrapolation is reliable– this article is the only resource displaying worked out expressions with sample analogs.4
One readily obtains (Van der Vaart, 2008) a standard central limit theorem result of the form p n ⇣ ˆ ˜ ⇥
n
˜⇥",3. Donsker Theorem,[0],[0]
"⇤ ⌘
d! N(0, H 1Multi⌦MultiH 1 Multi), (9)
where ˜⇥ =",3. Donsker Theorem,[0],[0]
"(vec( ˜W )T ,˜b)T , the information matrix ⌦Multi is
E
0
@
X
y0 6=y
c T y01
nD
cy0 , ˜",3. Donsker Theorem,[0],[0]
"f
E
ã o
1
A
0
@
X
y0 6=y
cy01 nD",3. Donsker Theorem,[0],[0]
"cy0 , ˜",3. Donsker Theorem,[0],[0]
"f E
ã o
1
A
⌦ ⇣ (x T , 1) T (x T , 1) ⌘ ,
and the Hessian HMulti is
Ey
2
4
X
y0 6=y
⇣
c T y0cy0
⌘
p
⇣ D
cy0 , ˜",3. Donsker Theorem,[0],[0]
"b
E ã ⌘
⌦E h (x T , 1) T (x T , 1) D
cy0 , ˜",3. Donsker Theorem,[0],[0]
"f
E = ã, y ii .
",3. Donsker Theorem,[0],[0]
"Both are evaluated at ˜⇥⇤, ˜f = ˜f(x), and ã = 1 K 1 , and p = p hcy0 ,W̃x+b̃i|y is the density of D c y 0 ,",3. Donsker Theorem,[0],[0]
˜f,3. Donsker Theorem,[0],[0]
"E
conditional on y. Derivations are given in the online appendix.
4Koo et al. (2008) and Jiang et al. (2008) do not provide expressions with sample analogs.",3. Donsker Theorem,[0],[0]
SVM are most commonly used for classification and prediction tasks.,3.1. Efficient Classifiers,[0],[0]
"Accordingly, the most immediate practical use for an estimate of the variance of the separating hyperplane is the construction of a more accurate classifier.
",3.1. Efficient Classifiers,[0],[0]
"Consider the One-vs-Rest method, for instance.",3.1. Efficient Classifiers,[0],[0]
"The Onevs-Rest method fits K hyperplanes, which in the linear case are defined by (!
i , b i ) 2 Rp+1, and categorizes a point by attributing it to the category in which it is the “deepest”.",3.1. Efficient Classifiers,[0],[0]
"That is,
ŷnew = argmax k
n
!̂T k xnew +ˆbk o .
",3.1. Efficient Classifiers,[0],[0]
"However, studentized distances yield more sensible and reliable classifications by accounting for the comparative uncertainty of the hyperplanes when categorizing a given point.",3.1. Efficient Classifiers,[0],[0]
"Naturally, a point being ”deeper” with respect to a classifying hyperplane –in terms of the length of the line from the point to the hyperplane and normal to the hyperplane– should make one more confident in the classification if it occurs in a section of the space where the hyperplane has lower variance.",3.1. Efficient Classifiers,[0],[0]
"In sections with high variance, the distance could be much smaller in resamplings of the data.",3.1. Efficient Classifiers,[0],[0]
"Accordingly, we suggest the following efficient categorization rule
ŷ⇤new = argmax k
(
!̂T k xnew +ˆbk p
(xTnew, 1)⌃k(x T new, 1)
)
,
where ⌃ k is the asymptotic variance of (!̂",3.1. Efficient Classifiers,[0],[0]
"k ,ˆb k ), or a consistent estimate.",3.1. Efficient Classifiers,[0],[0]
An analog modification can be applied to make the MSVM procedure more efficient.,3.1. Efficient Classifiers,[0],[0]
"Explaining the surprisingly competitive performance of the naive One-vs-Rest approach, comparatively to the more sophisticated MSVM approach, is an important open question.",4. Efficiency of One-vs-Rest,[0],[0]
The phenomenon is detailed and documented empirically in Rifkin and Klautau (2004) and is well established in the machine learning folklore.,4. Efficiency of One-vs-Rest,[0],[0]
"However, there are practically no theoretical results in the way of an explanation.",4. Efficiency of One-vs-Rest,[0],[0]
"In this section, we consider this question from the asymptotic statistics perspective and argue that the competitive performance of One-vs-Rest may be explained by a more efficient use of information.
",4. Efficiency of One-vs-Rest,[0],[0]
The idea is to consider the full One-vs-Rest method as a single M -estimator and to artificially impose a sum-to-zero constraint on the decision function.,4. Efficiency of One-vs-Rest,[0],[0]
"I can use the simplex encoding and obtain the (joint) asymptotic variance of the K separating hyperplanes in the form H 11vsR⌦1vsRH 1 1vsR.
Note that I pick the geometric margin to be 1 K 1 , rather than 1 in the standard form for binary (and thus One-vs-
Rest) SVM.",4. Efficiency of One-vs-Rest,[0],[0]
"The loss function for One-vs-Rest in simplex encoding is
K
X
k=1
1{y = k} ·  1
K 1 D c k , ˜Wx+˜b E
+
(10)
+1{y 6= k} ·  1
K 1 + D c k , ˜Wx+˜b E
+
!
which is minimized in ˜W and ˜b.",4. Efficiency of One-vs-Rest,[0],[0]
The first summand penalizes classification for which the point x is not sufficiently far from the hyperplane within the true category.,4. Efficiency of One-vs-Rest,[0],[0]
This is where we speak of using the information from a point’s “own” category.,4. Efficiency of One-vs-Rest,[0],[0]
The second summand penalizes classifications for which the point x is not sufficiently far from the hyperplane away from the wrong category.,4. Efficiency of One-vs-Rest,[0],[0]
"This is where we speak of using the information from “other” categories.
",4. Efficiency of One-vs-Rest,[0],[0]
The sum-to-zero constraint is added for analytical reasons; we need it to make the covariance matrices comparable.,4. Efficiency of One-vs-Rest,[0],[0]
"It will be apparent that the analytical conclusion is robust to this modification.
",4. Efficiency of One-vs-Rest,[0],[0]
"The information matrix ⌦1vsR is
E
⇣
c T y 1{ã
D
cy, ˜",4. Efficiency of One-vs-Rest,[0],[0]
"f
E 0} ⌘⇣ cy1{ã D cy, ˜ f E 0} ⌘
⌦(xT , 1)T (xT , 1)
2E(cTy 1{ã D cy, ˜ f E 0})( X
y0 6=y
cy01{ã+ D ck0 , ˜ f E 0})
⌦(xT , 1)T (xT , 1)
+E(
X
y0 6=y
c T y01{ã+
D
ck, ˜",4. Efficiency of One-vs-Rest,[0],[0]
"f
E
0})( X
y0 6=y
cy01{ã+ D ck0 , ˜ f E 0})
⌦(xT , 1)T (xT , 1),
and the Hessian H1vsR is
Ey
h
(c T y cy)
⇣
p
⇣D
cy, ˜",4. Efficiency of One-vs-Rest,[0],[0]
"b
E ã ⌘⌘
⌦E h (x T , 1) T (x T , 1) D
ck, ˜",4. Efficiency of One-vs-Rest,[0],[0]
"f
E
= ã, y
ii
+Ey
2
4
X
y0 6=y
(c T y0cy0)
⇣
p
⇣ D
cy0 , ˜",4. Efficiency of One-vs-Rest,[0],[0]
"b
E ã ⌘⌘
⌦E h (x T , 1) T (x T , 1) D
cy0 , ˜",4. Efficiency of One-vs-Rest,[0],[0]
"f
E = ã, y ii .
",4. Efficiency of One-vs-Rest,[0],[0]
We get instructive comparisons.,4. Efficiency of One-vs-Rest,[0],[0]
"First of all, HMulti < H1vsR.",4. Efficiency of One-vs-Rest,[0],[0]
"That is, the one-vs-rest problem has more “curvature” than the MSVM.",4. Efficiency of One-vs-Rest,[0],[0]
"Indeed, it is clear from inspection5
5Note the addition of a y = y0 positive summand.
that this comes from the one-vs-rest procedure using information from the “own category”, while MSVM doesn’t as it only uses information with respect to “other” categories.
",4. Efficiency of One-vs-Rest,[0],[0]
"It is clear from the comparison of the loss functions (2) and (10), corresponding to SC-SVM and the simplex encoding of One-vs-Rest, respectively, that both penalize for an observation that falls within the half-space assigned to an “other” category, but only One-vs-Rest rewards for points falling within their true, “own” category.",4. Efficiency of One-vs-Rest,[0],[0]
"It was not clear, however, if the rewarding a point for being in its “own” category is still informative and not redundant when it is already penalized if it is in any “other” category.",4. Efficiency of One-vs-Rest,[0],[0]
"However, in spite of imposing an additional constraint on the solution space of the One-vs-Rest problem, we do find from inspection of the Hessian that the additional information from rewarding classification of points within their “own” category is informative and not redundant.",4. Efficiency of One-vs-Rest,[0],[0]
"Although this was not obvious a priori, it is revealed by the statistical asymptotic analysis.
",4. Efficiency of One-vs-Rest,[0],[0]
"Furthermore, in the special case of a separable data generating process (DGP), that is in the case in which 1{ã D
c y
, ˜f",4. Efficiency of One-vs-Rest,[0],[0]
"E
0} = 0 a.s., we get that ⌦Multi = ⌦1vsR and both procedures have the same target hyperplane.",4. Efficiency of One-vs-Rest,[0],[0]
"Therefore, One-vs-Rest is strictly more statistically efficient than multicategory when the DGP is separable.",4. Efficiency of One-vs-Rest,[0],[0]
"In this specific case, this translates into smaller expected prediction error.",4. Efficiency of One-vs-Rest,[0],[0]
"We have displayed a case, that of perfect seprarability, where One-vs-Rest (with simplex encoding) provably dominates MSVM.
",4. Efficiency of One-vs-Rest,[0],[0]
"In non-separable cases, this dominance may not hold.",4. Efficiency of One-vs-Rest,[0],[0]
"In fact, we expect MSVM to outperform One-vs-Rest in some cases due to the more efficient gathering, by joint optimization, of the information from the “other” categories.",4. Efficiency of One-vs-Rest,[0],[0]
"We established rigorously, and with a proof conveying geometric intuition, the equivalence of MSVM and SC-SVM.",5. Discussion and Conclusion,[0],[0]
This provides a formulation of the optimization problem for computing MSVM which is relieved of the sum-to-zero constraint that bogged down computations in the implementations as suggested in Lee et al. (2004).,5. Discussion and Conclusion,[0],[0]
Our hope is that availablity of faster computations for MSVM will encourage applied researchers and analysts to employ MSVM in multicategory classification tasks.,5. Discussion and Conclusion,[0],[0]
"We gave the first central limit theorem for MSVM, along with an asymptotic covariance formula having a sample analog, which is a new result even for binary SVM.",5. Discussion and Conclusion,[0],[0]
"The variance formula allows for the construction of studentized decision functions for One-vs-Rest procedures, improving their accuracy and statistical efficiency.",5. Discussion and Conclusion,[0],[0]
"These make for more reliable classification, especially for extrapolation.",5. Discussion and Conclusion,[0],[0]
"We gave an analytical characterization of the surprisingly good performance of the
One-vs-Rest procedure, comparatively to MSVM, using the asymptotic distribution of estimators.",5. Discussion and Conclusion,[0],[0]
We hope this line of study fosters further research.,5. Discussion and Conclusion,[0],[0]
I would like to thank Lorenzo Rosasco for introducing me to the material studied in this article and for his support throughout this project.,Acknowledgements,[0],[0]
Different angles for studying SVM methods as M -estimators arose in stimulating conversation with Isaiah Andrews.,Acknowledgements,[0],[0]
Jann Spiess read an early draft of the article and made helpful comments.,Acknowledgements,[0],[0]
Jules MarchandGagnon collaborated on a cousin project and contributed insights which this article bears the mark of.,Acknowledgements,[0],[0]
It is straightforward to build a function that takes K and outputs the simplex encoding matrix C. We give intuitive pseudocode for the general K case.,Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"A code file is available in the online appendix.
",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
The construction relies on mapping vectors in spherical coordinates to their representation in Cartesian coordinates.,Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
The function StoC : RK 2 !,Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"RK 1 does just that.
",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"StoC function( 1, 2, ..., K 2){ v1 = cos( 1)
v2 = sin( 1) cos( 2)
",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"v3 = sin( 1) sin( 2) cos( 3)
...",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"v K 2 = sin( 1) · · · sin( K 3) cos( K 2) v K 1 = sin( 1) · · · sin( K 3) sin( K 2)
v = (v1, ..., vK 1)
v
}
Using the StoC function, it is now easy to construct the simplex encoding matrix.",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"This can be done with the function
C : K 7!",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"RK⇥(K 1) , which we now describe.
C function(K){ C1,· = StoC(0, 0, ..., 0)
C2,· = StoC
✓
acos
✓
1 K 1
◆
, 0, ..., 0
◆
C3,· = StoC
✓
acos
✓
1 K 1
◆
, acos
✓
1 K 2
◆
, 0, ..., 0
◆
...
C K 1,· = StoC
✓
acos
✓
1 K 1
◆
, acos
✓
1 K 2
◆
,
..., acos
✓
1 3
◆
, acos
✓
1 2
◆◆
C K,· = StoC
✓
acos
✓
1 K 1
◆
, acos
✓
1 K 2
◆
,
..., acos
✓
1 3
◆ , 2 · acos ✓
1 2
◆◆
C
}",Appendix: Construction of the Simplex Encoding Matrix C,[0],[0]
"The multicategory SVM (MSVM) of Lee et al. (2004) is a natural generalization of the classical, binary support vector machines (SVM).",abstractText,[0],[0]
"However, its use has been limited by computational difficulties.",abstractText,[0],[0]
"The simplex-cone SVM (SCSVM) of Mroueh et al. (2012) is a computationally efficient multicategory classifier, but its use has been limited by a seemingly opaque interpretation.",abstractText,[0],[0]
"We show that MSVM and SCSVM are in fact exactly equivalent, and provide a bijection between their tuning parameters.",abstractText,[0],[0]
MSVM may then be entertained as both a natural and computationally efficient multicategory extension of SVM.,abstractText,[0],[0]
We further provide a Donsker theorem for finite-dimensional kernel MSVM and partially answer the open question pertaining to the very competitive performance of One-vs-Rest methods against MSVM.,abstractText,[0],[0]
"Furthermore, we use the derived asymptotic covariance formula to develop an inverse-variance weighted classification rule which improves on the One-vs-Rest approach.",abstractText,[0],[0]
Equivalence of Multicategory SVM and Simplex Cone SVM:  Fast Computations and Statistical Theory,title,[0],[0]
"Given enough training data, a multi-layer perceptron would eventually learn the domain invariances in a classification task. Nevertheless, success of convolutional and recurrent networks suggests that encoding the domain symmetries through shared parameters can significantly boost the generalization of deep neural networks. The same observation can be made in deep learning for semi-supervised and unsupervised learning in structured domains. This raises an important question that is addressed in this paper: What kind of priors on input/output structure can be encoded through parameter-sharing?
This work is an attempt at answering this question, when our priors are in the form discrete domain symmetries. To formalize this type of prior, a family of transformations of input and output to a neural layer are expressed as group “action” on the input and output. The resulting neural network is invariant to this action, if transformations of the input within that particular family, does not change the output (e.g., rotation-invariance). However, if the output is transformed, in a predictable way, as we transform the input, the neural layer is equivariant to the action of the group.
1School of Computer Science, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA, USA 15217. Correspondence to: Siamak Ravanbakhsh <mravanba@cs.cmu.edu>.
Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).
Our goal is to show that parameter-sharing can be used to achieve equivariance to any discrete group action.
Application of group theory in machine learning has been the topic of various works in the past (e.g., Kondor, 2008; Bartók et al., 2010). In particular, many probabilistic inference techniques have been extended to graphical models with known symmetry groups (Raedt et al., 2016; Kersting et al., 2009; Bui et al., 2012; Niepert, 2012). Deep and hierarchical models have used a variety of techniques to study or obtain representations that isolate transformations from the “content” (e.g., Hinton et al., 2011; Jayaraman & Grauman, 2015; Lenc & Vedaldi, 2015; Agrawal et al., 2015). The simplest method of achieving equivariance is through data-augmentation (Krizhevsky et al., 2012; Dieleman et al., 2015). Going beyond augmentation, several methods directly apply the group-action, in one way or another, by transforming the data or its encodings using group members (Jaderberg et al., 2015; Anselmi et al., 2013; Dieleman et al., 2016). An alternative path to invariance via harmonic analysis. In particular cascade of wavelet transforms is investigated in (Bruna & Mallat, 2013; Oyallon & Mallat, 2015; Sifre & Mallat, 2013). More recently (Cohen & Welling, 2016b) study steerable filters (e.g., Freeman et al., 1991; Hel-Or & Teo, 1998) as a general mean for achieving equivariance in deep networks. Invariance and equivariance through parameter-sharing is also discussed in several prior works (Cohen & Welling, 2016a; Gens & Domingos, 2014).
The desirability of using parameter-sharing for this purpose is mainly due to its simplicity and computational efficiency. However, it also suggests possible directions for discovering domain symmetries through regularization schemes. Following the previous work on the study of symmetry in deep networks, we rely on group theory and group-actions to formulate invariances and equivariances of a function. Due to discrete nature of parameter-sharing, our treatment here is limited to permutation groups. Action of a permutation group G can model discrete transformations of a set of variables, such as translation and 90○ rotation of pixels around any center in an image. If the output of a function transforms with a G-action as we transform its input with a different G-action, the function is equivariant with respect to action of G. For example, in a convolution layer, as we translate the input, the feature-maps are also translated. If ar X
iv :1
70 2.
08 38
9v 2
[ st
at .M
L ]
1 3
Ju n
20 17
the output does not transform at all, the function is invariant to the action of G. Therefore, invariance is a special equivariance. In this example, different translations correspond to the action of different members of G.
The novelty of this work is its focus on the “model symmetry” as a gateway to equivariance. This gives us new theoretical guarantees for a “strict” notion of equivariance in neural networks. The core idea is simple: consider a colored bipartite graph Ω representing a neural network layer. Edges of the same color represent tied parameters. This neural network layer as a function is equivariant to the actions of a given group G (and nothing more) iff the action of G is the symmetry group of Ω – i.e., there is a simple bijection between parameter symmetries and equivariences of the corresponding neural network.
The problem then boils down to designing colored bipartite graphs with given symmetries, which constitutes a major part of this paper. Fig. 1 demonstrates this idea.1
For the necessary background on group theory see the Appendix. In the following, Section 1 formalizes equivariance wrt discrete group action. Section 2 relates the model symmetries a neural layer to its equivariance. Section 3 then builds on this observation to introduce two procedures for parameter-sharing that achieves a desirable equivariance.
1Throughout this paper, since we deal with finite sets, we use circular shift and circular convolution instead of shift and convolution. The two can be made identical with zero-padding of the input.
Here, we also see how group and graph convolution as well as deep-sets become special instances in our parametersharing procedure, which provides new insight and improved design in the case of group convolution. Where input and output of the layer have a one-to-one mapping, we see that the design problem reduces a well-known problem in combinatorics.",text,[0],[0]
Let x =,1. Group Action and Equivariance,[0],[0]
"[x1, . . .",1. Group Action and Equivariance,[0],[0]
", xN ] ∈ XN denote a set of variables and G = {g} be a finite group.",1. Group Action and Equivariance,[0],[0]
"The discrete action of G on x is in the form of permutation of indices in N = {1, . . .",1. Group Action and Equivariance,[0],[0]
",N}.",1. Group Action and Equivariance,[0],[0]
This group is a subgroup of the symmetric group SN; the group of all N !,1. Group Action and Equivariance,[0],[0]
permutations of N objects.,1. Group Action and Equivariance,[0],[0]
We use Ð→ N =,1. Group Action and Equivariance,[0],[0]
"[1, . .",1. Group Action and Equivariance,[0],[0]
.,1. Group Action and Equivariance,[0],[0]
",N] to denote the ordered counterpart to N and the G-action on this vector g Ð→",1. Group Action and Equivariance,[0],[0]
N ≐,1. Group Action and Equivariance,[0],[0]
"[g1, . . .",1. Group Action and Equivariance,[0],[0]
",gN] is a simple permutation.",1. Group Action and Equivariance,[0],[0]
"Using xÐ→ N
to denote x, the discrete action of g ∈ G on x ∈ XN is given by gxÐ→
N ≐",1. Group Action and Equivariance,[0],[0]
"x g Ð→ N .
",1. Group Action and Equivariance,[0],[0]
G-action on N is a permutation group that is not necessarily isomorphic to G itself.,1. Group Action and Equivariance,[0],[0]
GN ≤ G captures the structure of G when it acts on N. We use gN to denote the image of g ∈ G in GN.,1. Group Action and Equivariance,[0],[0]
G-action is faithful iff two groups are isomorphic G ≅,1. Group Action and Equivariance,[0],[0]
GN – that is G-action preserves its structure.,1. Group Action and Equivariance,[0],[0]
"In this case, each g ∈ G maps to a distinct permutation g Ð→",1. Group Action and Equivariance,[0],[0]
N ≠,1. Group Action and Equivariance,[0],[0]
"g′Ð→N∀g,g′ ∈ G.",1. Group Action and Equivariance,[0],[0]
"Given any G-action on N we can efficiently obtain GN; see Appendix.
Example 1.1 (Cyclic Group) Consider the cyclic group G = Z6 and define its action on x ∈ R3 by defining it on the index set N = {1,2,3} as gn ≐",1. Group Action and Equivariance,[0],[0]
g + n mod 3∀g ∈ Z6.,1. Group Action and Equivariance,[0],[0]
This action is not faithful.,1. Group Action and Equivariance,[0],[0]
"For example, the action of g = 1 and g = 4 result in the same permutations of variables in x; i.e., single-step of circular shift to the right.",1. Group Action and Equivariance,[0],[0]
"With the above action, the resulting permutation group GN is isomorphic to Z3 < Z6.",1. Group Action and Equivariance,[0],[0]
Now consider the same group G = Z6 with a different action on N:,1. Group Action and Equivariance,[0],[0]
gn ≐,1. Group Action and Equivariance,[0],[0]
"g − n mod 3∀g ∈ Z6, where we replaced (+) with (−).",1. Group Action and Equivariance,[0],[0]
Let G̃N be the resulting permutation group.,1. Group Action and Equivariance,[0],[0]
Here again G̃N ≅ Z3.,1. Group Action and Equivariance,[0],[0]
"Although isomorphic, G̃N ≠ GN, as they are different permutation groups of N.
Consider the function φ ∶ XN → YM and let GN and GM be the action of G on input/output index sets N and M.
Definition 1.1",1. Group Action and Equivariance,[0],[0]
"The joint permutation group GN,M is a subdirect product (or pairing) of GN and GM
GN,M = GN ⊙GM ≐",1. Group Action and Equivariance,[0],[0]
"{(gN,gM) ∣ g ∈ G}.
",1. Group Action and Equivariance,[0],[0]
We are now ready to define equivariance and invariance.,1. Group Action and Equivariance,[0],[0]
φ(⋅),1. Group Action and Equivariance,[0],[0]
"is GN,M-equivariant iff
gNφ(x) = φ(gMx) ∀x",1. Group Action and Equivariance,[0],[0]
"∈ XN , (gN,gM)",1. Group Action and Equivariance,[0],[0]
"∈ GN,M (1)
",1. Group Action and Equivariance,[0],[0]
"Moreover, if GM = {e} is trivial, we have
gNφ(x) = φ(x) ∀x ∈ XN ,gN ∈ GN and φ(⋅) is GN-invariant.",1. Group Action and Equivariance,[0],[0]
gN and gM,1. Group Action and Equivariance,[0],[0]
"can also be represented using permutation matrices GN ∈ {0,1}N×N , and GM ∈ {0,1}M×M .",1. Group Action and Equivariance,[0],[0]
"Equivariance relation of (1) then becomes
GMφ(x) = φ(GNx)∀x ∈ XN ,",1. Group Action and Equivariance,[0],[0]
"(GN,GM) ∈ GN,M (2)
",1. Group Action and Equivariance,[0],[0]
"The following observation shows that the subgroup relationship affects equivariance and invariance.
",1. Group Action and Equivariance,[0],[0]
Observation 1.1,1. Group Action and Equivariance,[0],[0]
"If the function φ ∶ XN → YM is GN,M - equivariant, then it is also HN,M -equivariant for any permutation group HN,M < G.
Example 1.2 (Reverse Convolution) Consider the cyclic group G = Z6 and for g ∈ G, define the action on N = {1,2,3} to be gn ≐",1. Group Action and Equivariance,[0],[0]
g + n mod 3.,1. Group Action and Equivariance,[0],[0]
"Also let its action on M = {1, . . .",1. Group Action and Equivariance,[0],[0]
",6} be gm ≐",1. Group Action and Equivariance,[0],[0]
g,1. Group Action and Equivariance,[0],[0]
− n mod 6.,1. Group Action and Equivariance,[0],[0]
"In other words, G-action on N performs circular shift to the right and its action on M shifts variables to the left.",1. Group Action and Equivariance,[0],[0]
"Examples of the permutation matrix representation for
two members of GN and GM are
2N =",1. Group Action and Equivariance,[0],[0]
( 0 1 0 0 0 1 1 0 0 ) 2M = ⎛ ⎜ ⎝ 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0,1. Group Action and Equivariance,[0],[0]
"⎞ ⎟ ⎠
corresponding to right and left shift on vectors of different lengths.",1. Group Action and Equivariance,[0],[0]
Now consider the function φ,1. Group Action and Equivariance,[0],[0]
"∶ RN → RM
φW(x) =Wx WT = ( 0 a b 0 a b a b 0 a b 0 b 0 a b 0 a ) ∀a, b ∈ R
Using permutation matrices one could check the equivariance condition (2) for this function.",1. Group Action and Equivariance,[0],[0]
"We can show that φ is equivariant to GN,M. Consider 2 ∈ Z6 and its images 2N ∈ GN and 2M ∈ GM.",1. Group Action and Equivariance,[0],[0]
"L.h.s. of (2) is
2MφW(x) = ⎛ ⎜ ⎝ 0 0",1. Group Action and Equivariance,[0],[0]
"1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 ⎞ ⎟ ⎠ ⎛ ⎜ ⎝ 0 a b a b 0 b 0 a 0 a b a b 0 b 0 a ⎞ ⎟ ⎠ x = ⎛ ⎜ ⎝ b 0 a 0 a b a b 0 b 0 a 0 a b a b 0 ⎞ ⎟ ⎠ x
which is equal to its r.h.s.
φW(2Nx) = ⎛ ⎜ ⎝ 0 a b a b 0 b 0 a 0 a b a b 0 b 0 a ⎞ ⎟ ⎠ ( 0 1 00 0 1 1 0 0 )x",1. Group Action and Equivariance,[0],[0]
= ⎛ ⎜ ⎝ b 0 a 0 a b a b 0 b 0,1. Group Action and Equivariance,[0],[0]
"a 0 a b a b 0 ⎞ ⎟ ⎠ x
for any x. One could verify this equality for all g ∈ Z6.",1. Group Action and Equivariance,[0],[0]
"Now consider the group HN,M < GN,M, where HN = GN and members of HM = {0,2,4}, perform left circular shift of length 0,2 and 4.",1. Group Action and Equivariance,[0],[0]
"It is easy to see that HN,M ≅ Z3.",1. Group Action and Equivariance,[0],[0]
"Moreover since HN,M < GN,M, φ(⋅) above is HN,Mequivariant as well.",1. Group Action and Equivariance,[0],[0]
"However, one prefers to characterize the equivariance properties of φ using GN,M rather than HN,M.
The observation above suggests that GN,M-equivariance is not restrictive enough.",1. Group Action and Equivariance,[0],[0]
"As an extreme case, a constant function φ(x) = 1 is equivariant to any permutation group GN,M ≤ SN × SM.",1. Group Action and Equivariance,[0],[0]
"In this case equivariance of φ with respect to a particular GN,M is not very informative to us.",1. Group Action and Equivariance,[0],[0]
"To remedy this, we define a more strict notion of equivariance.
",1. Group Action and Equivariance,[0],[0]
Definition 1.2,1. Group Action and Equivariance,[0],[0]
we say a function φ ∶ XN → YM is uniquely G-equivariant iff it is G-equivariant and it is “not” H-equivariant for any H > G.,1. Group Action and Equivariance,[0],[0]
"Given a group G, and its discrete action through GN,M, we are interested in defining parameter-sharing schemes for a parametric class of functions that guarantees their unique GN,M-equivariance.",2. Symmetry Groups of a Network,[0],[0]
"We start by looking at a single neural layer and relate its unique GN,M-equivariance to the symmetries of a colored multi-edged bipartite graph that de-
fines parameter-sharing.",2. Symmetry Groups of a Network,[0],[0]
"We then show that the idea extends to multiple-layers.
",2. Symmetry Groups of a Network,[0],[0]
Definition 2.1 A colored multi-edged bipartite graph Ω =,2. Symmetry Groups of a Network,[0],[0]
"(N,M, α) is a triple, where N and M are its two sets of nodes, and α ∶ N × M → 2{1,...,C} is the edge function that assigns multiple edge-colors from the set {1, . .",2. Symmetry Groups of a Network,[0],[0]
.,2. Symmetry Groups of a Network,[0],[0]
",C} to each edge.",2. Symmetry Groups of a Network,[0],[0]
"Non-existing edges receive no color.
",2. Symmetry Groups of a Network,[0],[0]
We are interested in the symmetries of this structure.,2. Symmetry Groups of a Network,[0],[0]
"The set of permutations (πN, πM) ∈ SN × SM of nodes (within each part of the bipartite graph) that preserve all edgecolors define the Automorphism Group Aut(Ω) ≤ SN × SM – that is ∀(n,m) ∈ N ×M
(πN, πM) ∈Aut(Ω) ⇔ α(n,m) = α((πNn,πMm))",2. Symmetry Groups of a Network,[0],[0]
"(3)
Alternatively, to facilitate the notation, we define the same structure (colored multi-edged bipartite graph) as a set of binary relations between N and M – that is Ω = (N,M,{∆c}1≤c≤C) where each relation is associated with one color ∆c = {(n,m) ∣ c ∈ α(n,m)∀(n,m) ∈ N ×M}.",2. Symmetry Groups of a Network,[0],[0]
"This definition of structure, gives an alternative expression for Aut(Ω)
(πN, πM) ∈Aut(Ω) ⇔ (4)
((n,m) ∈",2. Symmetry Groups of a Network,[0],[0]
"∆c ⇔ (πNn,πMm) ∈",2. Symmetry Groups of a Network,[0],[0]
"∆c) ∀c, n,m
The significance of this structure is in that, it defines a parameter-sharing scheme in a neural layer, where the same edge-colors correspond to the same parameters.",2. Symmetry Groups of a Network,[0],[0]
Consider the function φ ≐,2. Symmetry Groups of a Network,[0],[0]
"[φ1, . .",2. Symmetry Groups of a Network,[0],[0]
.,2. Symmetry Groups of a Network,[0],[0]
", φM ]",2. Symmetry Groups of a Network,[0],[0]
"∶ RN → RM
φm(x;w,Ω) ≐ σ(∑ n ∑ c∈α(n,m) wcxn) ∀m (5)
where σ ∶ R → R is a strictly monotonic nonlinearity and w =",2. Symmetry Groups of a Network,[0],[0]
"[w1, . .",2. Symmetry Groups of a Network,[0],[0]
".wc, . .",2. Symmetry Groups of a Network,[0],[0]
.,2. Symmetry Groups of a Network,[0],[0]
",wC] is the parameter-vector for this layer.
",2. Symmetry Groups of a Network,[0],[0]
"The following key theorem relates the equivariances of φ(⋅;w,Ω) to the symmetries of Ω.
Theorem 2.1 For any w ∈ RC s.t., wc ≠ wc′∀c, c′, the function φ(⋅;w,Ω) is uniquely Aut(Ω)-equivariant.
",2. Symmetry Groups of a Network,[0],[0]
"Corollary 2.2 For any HN,M ≤ Aut(Ω), the function φ(⋅;w,Ω) is HN,M-equivariant.
",2. Symmetry Groups of a Network,[0],[0]
"The implication is that to achieve unique equivariance for a given group-action, we need to define the parametersharing using the structure Ω with symmetry group GN,M.
Example 2.1 (Reverse Convolution)",2. Symmetry Groups of a Network,[0],[0]
Revisiting Example 1.2 we can show that the condition of Theorem 2.1 holds.,2. Symmetry Groups of a Network,[0],[0]
"In this case σ(x) = x and the parameter-sharing of the matrix W is visualized below, where we used two different line styles for a, b ∈ R.
In this figure, the circular shift of variables at the output and input level to the left and right respectively, does not change the edge-colors.",2. Symmetry Groups of a Network,[0],[0]
"For example in both cases node 1 ’s connection to nodes 3 , 6 using dashed-lines is preserved.
",2. Symmetry Groups of a Network,[0],[0]
"Six repetitions of this action produces different permutations corresponding to six members of GN,M. Therefore GN,M ≤Aut(Ω) and according to Corollary 2.2, φ(⋅) is GN,M equivariant.",2. Symmetry Groups of a Network,[0],[0]
"Moreover, using Theorem 3.3 of the next section, we can prove that these six permutations are the “only” edge-color preserving ones for this structure, resulting in unique equivariance.
",2. Symmetry Groups of a Network,[0],[0]
Matrix Form.,2. Symmetry Groups of a Network,[0],[0]
"To write (5) in a matrix form, if there are multiple edges between two nodes n,m, we need to merge them.",2. Symmetry Groups of a Network,[0],[0]
"In general, by assigning on distinct color to any set in the range of α ∶ N ×M → 2{1,...,C} we can w.l.o.g. reduce multiple edges to a single edge.",2. Symmetry Groups of a Network,[0],[0]
"In other words we can rewrite φ using W ∈ RM×N
φ(x;w; Ω) = σ(Wx) Wm,n = ∑ c∈α(n,m) wc (6)
Using this notation, and due to strict monotonicity of the nonlinearity σ(⋅), Theorem 2.1 simply states that for all (gN,gM) ∈Aut(Ω), x ∈ RN and W given by (6)
GMWx =WGNx.",2. Symmetry Groups of a Network,[0],[0]
"(7)
Example 2.2 (Permutation-Equivariant Layer) Consider all permutations of indices N and M = N.
We want to define a neural layer such that all permutations of the input gN ∈ GN = SN result in the same
permutation of the output gM",2. Symmetry Groups of a Network,[0],[0]
"= gN. Consider the following colored bipartite graph, for a special case where N = M = 4.",2. Symmetry Groups of a Network,[0],[0]
"It is easy to show that color-preserving permutations of this structure are Aut(Ω) = SN ⊙ SN = {(g,g) ∣ g ∈ SN} ≅ SN: On one hand, for (πN, πM) ∈ SN × SM, having πN = πM clearly preserves the colors.",2. Symmetry Groups of a Network,[0],[0]
"On the other hand, if πN ≠ πM, there exists u ∈ N (also in M) such that πNu ≠ πMu.",2. Symmetry Groups of a Network,[0],[0]
"Therefore (πN, πM) does not preserve the relation ∆ =",2. Symmetry Groups of a Network,[0],[0]
"{(n,n) ∣ n ∈ N} corresponding to dashed edges, and therefore (πN, πM) ∉Aut(Ω).",2. Symmetry Groups of a Network,[0],[0]
This proves Aut(Ω) = SN ⊙ SN.,2. Symmetry Groups of a Network,[0],[0]
"The function (5) for this Ω is
φ(x;w =",2. Symmetry Groups of a Network,[0],[0]
"[w1,w2],Ω) =",2. Symmetry Groups of a Network,[0],[0]
"σ(w1Ix +w211Tx).
",2. Symmetry Groups of a Network,[0],[0]
"Ravanbakhsh et al. (2016); Zaheer et al. (2017) derive the same permutation equivariant layer, by proving the commutativity in (7), while here it follows from Corollary 2.2.
",2. Symmetry Groups of a Network,[0],[0]
Multiple Layers.,2. Symmetry Groups of a Network,[0],[0]
"For deep networks, the equivariance of the composition φ2 ○ φ1 to G-action follows from that of individual layer φ1 ∶ XN → YM and φ2 ∶ YM → ZO.",2. Symmetry Groups of a Network,[0],[0]
"Assuming φ1 is GN,M-equivariant and φ2 is GM,O-equivariant, where G-action on M is shared between the two layers, it follows that φ2 ○ φ1 is GN,O-equivariant, where GN,O = GN ⊙GO.",2. Symmetry Groups of a Network,[0],[0]
"This is because ∀g ∈ G and x ∈ XN
φ2(φ1(gNx))",2. Symmetry Groups of a Network,[0],[0]
= φ2(gMφ1(x)) = gOφ2(φ1(x)).,2. Symmetry Groups of a Network,[0],[0]
(8),2. Symmetry Groups of a Network,[0],[0]
"Consider the definition of neural layer (5) that employs parameter-sharing according to Ω. Given G-action on N and M, we are interested in designing structures Ω such that Aut(Ω) = GN,M.",3. Structure Design,[0],[0]
"According to the Theorem 2.1, it then follows that φ is uniquely GN,M-equivariant.",3. Structure Design,[0],[0]
"Here, we give the sufficient conditions and the design recipe to achieve this.
",3. Structure Design,[0],[0]
"For this we briefly review some group properties that are used in later developments.
transitivity We say that G-action on N is transitive iff ∀n1, n2 ∈ N, there exists at least one action g ∈ G (or gN ∈ GN) such that gn1 = n2.
regularity The group action is free or semi-regular iff ∀n1, n2 ∈ N, there is at most one g ∈ G such at gn1 = n2, and the action is regular iff it is both transitive and free – i.e., for any pair n1, n2 ∈ N, there is uniquely one g ∈ G such that gn1 = n2.",3. Structure Design,[0],[0]
Any free action is also faithful.,3. Structure Design,[0],[0]
We use a similar terminology for GN.,3. Structure Design,[0],[0]
"That is we call GN semi-regular iff ∀n1, n2 ∈ N at
most one gN ∈ GN moves n1 to n2 and GN is regular if this number is exactly one.
orbit",3. Structure Design,[0],[0]
"The orbit of n ∈ N is all the members to which it can be moved, Gn = {gn ∣ g ∈ G}.",3. Structure Design,[0],[0]
The orbits of n ∈ N form an equivalence relation2,3. Structure Design,[0],[0]
"This equivalence relation partitions N into orbits N = ⋃1≤p≤P Gnp, where np is an arbitrary representative of the partition Gnp ⊆ N. Note that the G-action on N is always transitive on its orbits – that is for any n,n′ ∈ Gnp, there is at least one g ∈ G such that n =",3. Structure Design,[0],[0]
"gn′. Therefore, for a semi-regular G-action, the action of G on the orbits Gnp∀1 ≤ p ≤ P is regular.
",3. Structure Design,[0],[0]
"Example 3.1 (Mirror Symmetry) Consider G = Z2 = {e = 0,1} (1 + 1 = 0) acting on N, where the only non-trivial action is defined as flipping the input: 1N[1, . . .",3. Structure Design,[0],[0]
",N] =",3. Structure Design,[0],[0]
"[N,N − 1, . . .",3. Structure Design,[0],[0]
",1].",3. Structure Design,[0],[0]
"G is faithful in its action on N, however GN is not transitive – e.g., N cannot be moved to N − 1.",3. Structure Design,[0],[0]
"If N is even, then G-action is semi-regular.",3. Structure Design,[0],[0]
"This is because otherwise the element in the middle n = ⌈N
2 ⌉ is moved to itself by
two different actions e,1 ∈",3. Structure Design,[0],[0]
"G. Furthermore, ifN is even, G-action has N
2 orbits and G2 acts on these orbits regu-
larly.",3. Structure Design,[0],[0]
"If N is odd, G-action has ⌈N 2 ⌉ orbits.",3. Structure Design,[0],[0]
"However, its action on the orbit of the middle element G ⌈N 2 ⌉ is not regular.
",3. Structure Design,[0],[0]
"In the following, Section 3.1 proposes a procedure for parameter-sharing in a fully connected layer.",3. Structure Design,[0],[0]
"Although simple, this design is dense and does not guarantee “unique” of equivariance.",3. Structure Design,[0],[0]
Section 3.2 proposes an alternative design with sparse connections that in some settings ensures unique equivariance.,3. Structure Design,[0],[0]
"Section 3.3 investigates the effect of having multiple input and output channels in the neural layer and Section 3.4 studies a special case of GN = GM, where input and output indices have a one-toone mapping.",3. Structure Design,[0],[0]
"Consider a complete bipartite graph with N and M as its two parts and edges (n,m) ∈ N ×M.",3.1. Dense Design,[0],[0]
"The action of GN,M partitions the edges into orbits {GN,M(np,mq)}np,mq , where (np,mq) is a representative edge from an orbit.",3.1. Dense Design,[0],[0]
"Painting each orbit with a different color gives
Ω = (N,M,{∆p,q = GN,M(np,mq)}).",3.1. Dense Design,[0],[0]
"(9)
Therefore two edges (n,m) and (n′,m′) have the same color iff an action in GN,M moves one edge to the other.
",3.1. Dense Design,[0],[0]
"2n ∼ n′⇔ ∃g s.t., n = gn′⇔ n ∈ Gn′⇔ n′",3.1. Dense Design,[0],[0]
"∈ Gn.
",3.1. Dense Design,[0],[0]
"Proposition 3.1 GN,M ≤ Ω for Ω of (9).
",3.1. Dense Design,[0],[0]
"Corollary 3.2 φ(⋅;w,Ω), for structure (9), is equivariant to GN,M.
Example 3.2 (Nested Subsets and Wreath Product)",3.1. Dense Design,[0],[0]
The permutation-equivariant layer that we saw in Example 2.2 is useful for defining neural layers for set structure.,3.1. Dense Design,[0],[0]
"If our data-structure is in the form of nested subsets, then we require equivariance to permutation of variables within each set as well as permutation of subsets.",3.1. Dense Design,[0],[0]
"Here, we show how to use our dense design for this purpose.
",3.1. Dense Design,[0],[0]
We use a special indexing for the input to better identify the exchangeability of variables.,3.1. Dense Design,[0],[0]
"We assume D subsets, each of which has d variables x =",3.1. Dense Design,[0],[0]
"[x1,1, . . .",3.1. Dense Design,[0],[0]
", x1,d, x2,1, . . .",3.1. Dense Design,[0],[0]
", xD,d].",3.1. Dense Design,[0],[0]
The group of our interest is the wreath product,3.1. Dense Design,[0],[0]
Sd ≀ SD.,3.1. Dense Design,[0],[0]
This type of group product can be used to build hierarchical and nested structures with different type of symmetries at each level.,3.1. Dense Design,[0],[0]
Nesting subsets corresponds to the most basic form of such hierarchical constructions.,3.1. Dense Design,[0],[0]
"We use (n,n′) to index input variables and (m,m′) for output variables.
",3.1. Dense Design,[0],[0]
"The following figure shows the resulting parametersharing for an example with D = 2, d = 3.
",3.1. Dense Design,[0],[0]
How did we arrive at this structure Ω?,3.1. Dense Design,[0],[0]
Recall Our objective is to define parameter-sharing so that φW,3.1. Dense Design,[0],[0]
"∶ RdD → RdD is equivariant to the action of G = Sd ≀ SD – i.e., permutations within sets at two levels.",3.1. Dense Design,[0],[0]
"This group-action identifies three partitions of edges (seen in the figure): I) ((n,n′), (n,n′))∀n,n′ connects each variable to its counterpart (dashed orange); II) ((n,n′), (n,m′))∀n,n′ ≠ m′ connects each variable to other variables within the same subset; III) ((n,n′), (m,m′))∀n ≠ m is the set of edges from one subset to another.",3.1. Dense Design,[0],[0]
"According to the Corollary 3.2 this parameter-sharing guarantees equivariance.
",3.1. Dense Design,[0],[0]
"This fully-connected design is useful when the group GN,M is large; for example when dealing with SN.",3.1. Dense Design,[0],[0]
"However, for
smaller groups it could be very inefficient in practice, as sometimes we can achieve equivariance through a sparse structure Ω. As an example, consider the 2D circular convolution layer.",3.1. Dense Design,[0],[0]
"It is easy to show that according to this design, the convolution filter will be the same size as the input image.",3.1. Dense Design,[0],[0]
"While this achieves the desirable equivariance, it is inefficient and does not generalize as well as a convolution layer with small filters.",3.1. Dense Design,[0],[0]
"Moreover, the dense design does not guarantee “unique” equivariance.",3.1. Dense Design,[0],[0]
"We next show under some conditions on GN,M the sparse design can produce this stronger guarantee.",3.1. Dense Design,[0],[0]
"Our sparse construction uses orbits and symmetric generating sets:
• Let us denote the orbits of G-action on M and N by {Gnp ∣ 1 ≤ p ≤ P} and {Gmq ∣ 1 ≤ q ≤ Q} respectively, where P and Q are the total number of orbits and np,mq are (arbitrary) representative members of orbits Gnp, Gmq respectively.",3.2. Sparse Design,[0],[0]
"Note that in contrast to previous section, here we are considering the orbit of variables rather than the edges.
",3.2. Sparse Design,[0],[0]
"• The set A ⊆ G is called the generating set of G (< A >= G), iff every member of G can be expressed as a combination of members of A.",3.2. Sparse Design,[0],[0]
"If the generating set is closed under inverse a ∈A ⇒ a−1 ∈A we call it a symmetric generating set.
",3.2. Sparse Design,[0],[0]
"Define the structure Ω as
Ω = (N,M,{∆p,q,a}1≤p≤P, 1≤q≤Q,a∈A) ∆p,q,a = {(gNanp,gNmq) ∣ (gN,gM)",3.2. Sparse Design,[0],[0]
"∈ GN,M}.",3.2. Sparse Design,[0],[0]
"(10)
In words, we have one color per each combination of orbits (p, q) and members of the generating set a ∈ A.",3.2. Sparse Design,[0],[0]
"The following theorem relates the symmetry group of this structure to G.
Theorem 3.3 GN,M ≤ Aut(Ω) for Ω of (10).",3.2. Sparse Design,[0],[0]
"Moreover if GN and GM are both semi-regular, then GN,M = Aut(Ω).
",3.2. Sparse Design,[0],[0]
"Note that this result holds for any choice of a symmetric generating set A in defining Ω. Therefore, in designing sparse layers, one seeks a minimal A.
Corollary 3.4 The function φ(⋅,w,Ω), using the structure (10) is GN,M-equivariant.",3.2. Sparse Design,[0],[0]
"If GN and GM are semi-regular, this function is “uniquely” GN,M-equivariant.
",3.2. Sparse Design,[0],[0]
"Now, assuming G-action is semi-regular on both N and M, using (arbitrarily chosen) representatives {np}1≤p≤P and {mq}1≤q≤Q for orbits in N and M, we can rewrite the expression (5) of the structured neural layer for the structure above.",3.2. Sparse Design,[0],[0]
"Here, components of φ =",3.2. Sparse Design,[0],[0]
"[φ1, . . .",3.2. Sparse Design,[0],[0]
", φM ] are enumerated for 1 ≤ q ≤ Q,gM ∈",3.2. Sparse Design,[0],[0]
"GM:
φgMmq(x;w) = σ( ∑ 1≤p≤P ∑ a∈A wq,p,axgNanp) (11)
where w ∈ RP×Q×∣A∣ is the set of unique parameters, and each element φgMmq depends on subset of parameters {wq,p,a}p,a identified by q and a subset of inputs {xa,gNnp}p,a identified by gN.
Example 3.3 (Dihedral Group of Fig. 1)",3.2. Sparse Design,[0],[0]
"In the example of Fig. 1, the number of orbits of G-action on N is P = 2 and for M this is Q = 1.",3.2. Sparse Design,[0],[0]
"The symmetric generating set is the generating set that is used in the Cayley diagram, with the addition of inverse shift (inverse of the blue arrow).",3.2. Sparse Design,[0],[0]
"We then used (10) to build the structure of Fig. 1 (right).
",3.2. Sparse Design,[0],[0]
Example 3.4 (Reverse Convolution),3.2. Sparse Design,[0],[0]
The parametersharing structure of reverse convolution in Examples 1.2 and 2.1 is produced using our sparse design.,3.2. Sparse Design,[0],[0]
"In these examples, both GN and GM are regular.",3.2. Sparse Design,[0],[0]
Therefore the proposed parameter-sharing provides unique equivariance.,3.2. Sparse Design,[0],[0]
"In this section, we extend our results to multiple input and output channels.",3.3. Multiple Channels,[0],[0]
"Up to this point, we considered a neural network layer φ ∶ RN → RM .",3.3. Multiple Channels,[0],[0]
"Here, we want to see how to achieve GN,M-equivariance for φ ∶ RN×K → RM×K ′
, where K and K ′ are the number of input and output channels.
",3.3. Multiple Channels,[0],[0]
"First, we extend the action of G on N and M to NK =",3.3. Multiple Channels,[0],[0]
"[N, . .",3.3. Multiple Channels,[0],[0]
.,3.3. Multiple Channels,[0],[0]
",N ´¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¶ Ktimes ] as well as MK′ , to accommodate multiple channels.",3.3. Multiple Channels,[0],[0]
"For this, simply repeat the G-action on each component.",3.3. Multiple Channels,[0],[0]
G-action on multiple input channels is equivalent to sub-direct product GN ⊙ . .,3.3. Multiple Channels,[0],[0]
".⊙GN
",3.3. Multiple Channels,[0],[0]
"´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ Ktimes
≅ GN.",3.3. Multiple Channels,[0],[0]
"The same applies
to GM.
",3.3. Multiple Channels,[0],[0]
"This repetition, multiplies the orbits of GN, one for each channel, so that instead of having P and Q orbits on the input N and output M sets, we have K × P and K ′",3.3. Multiple Channels,[0],[0]
"× Q orbits on the input NK and output MK ′
.",3.3. Multiple Channels,[0],[0]
"This increases the number of parameters by a factor of K ×K ′.
The important implication is that, orbits and multiple channels are treated identically by both dense and sparse designs.
",3.3. Multiple Channels,[0],[0]
Example 3.5 (Group Convolution),3.3. Multiple Channels,[0],[0]
"The idea of groupconvolution is studied by Cohen & Welling (2016a); see also (Olah, 2014).",3.3. Multiple Channels,[0],[0]
"The following claim relates the function of this type of layer to our sparse design.
",3.3. Multiple Channels,[0],[0]
"Claim 3.5 Under the following conditions the neural layer (5) using our sparse design (10) performs group convolution: I) there is a bijection between the output and G (i.e., M = G) and; II)",3.3. Multiple Channels,[0],[0]
"GN is transitive.
",3.3. Multiple Channels,[0],[0]
"This also identifies the limitations of group-convolution even in the setting where M = G: When GN is semiregular and not transitive (P > 1), group convolution is not guaranteed to be uniquely equivariant while the sparse parameter-sharing of (10) provides this guarantee.
",3.3. Multiple Channels,[0],[0]
"For demonstration consider the following example in equivariance to mirror symmetry.
",3.3. Multiple Channels,[0],[0]
"This figure shows the bipartite structure for G = Z2 = {0,1} and A = {1}.",3.3. Multiple Channels,[0],[0]
G-action is horizontal flip of the input and the output.,3.3. Multiple Channels,[0],[0]
"On the right, M = G while on the left GM-action has two orbits.",3.3. Multiple Channels,[0],[0]
Orbits are identified by line-style and color of the circles.,3.3. Multiple Channels,[0],[0]
"In a neural layer with this parameter-sharing, when we flip the input variables (around the mirror line) the output is also flipped.
",3.3. Multiple Channels,[0],[0]
The representatives in each orbit on N and M is identified with a star.,3.3. Multiple Channels,[0],[0]
"Note that each combination of orbits p and q has a parameter of its own, identified with different edge-styles.",3.3. Multiple Channels,[0],[0]
"While this construction guarantees “unique” G-equivariance, if instead we use the same parameters across orbits (as suggested by the original group convolution) we get the parameter-sharing of the figure below middle.
",3.3. Multiple Channels,[0],[0]
"In this case, the resulting neural layer has the desired equivariance (right).",3.3. Multiple Channels,[0],[0]
"However, it is equivariant to the action of a larger group GN,M ≅ Z2 ×Z2 > Z2, in which 1 in the second Z2 group exchanges variables across the orbits on N (left in figure above).",3.3. Multiple Channels,[0],[0]
"In semi-supervised and un-supervised applications, we often need to produce a single output yn for each input xn∀n ∈ N – that is N = M. We can ensure this by having a relation ∆c∗ = {(n,n) ∣ n ∈ N} in Ω that guarantees any (πN, πM) ∈Aut(Ω) applies the same permutation to N and M = N – i.e., πN = πM.",3.4. GN = GM,[0],[0]
"The resulting structure Ω = (N,N,{∆c}1≤c≤C ∪ {∆c∗}} can be also interpreted as a colored multi-edged directed graph (digraph).",3.4. GN = GM,[0],[0]
This is because we can collapse the two parts by identifying n ∈ N with n ∈M.,3.4. GN = GM,[0],[0]
"Therefore, the symmetry-group of the original bipartite structure, is isomorphic to symmetry group of a colored multi-edged digraph on N. Achieving unique Gequivariance then reduces to answering the following question: when could we express a permutation group G ≤ SN as the symmetry group Aut(Ω) of a colored multi-edged digraph with N nodes?
",3.4. GN = GM,[0],[0]
"This problem is well-studied under the class of concrete representation problems (Babai, 1994).",3.4. GN = GM,[0],[0]
"Permutation groups G that can be expressed in this way are called 2- closed groups (Wielandt, 1969).",3.4. GN = GM,[0],[0]
"The recipe for achieving GN ≤ Aut(Ω) is similar to our dense construction of Section 3.13 The 2-closure ḠN of a group GN is then, the greatest permutation group ḠN ≤ SN with the same orbit on N×N as GN.",3.4. GN = GM,[0],[0]
It is known that for example semi-regular permutation groups are 2-closed ḠN = GN.,3.4. GN = GM,[0],[0]
"This result also follows a corollary of our Theorem 3.3 for sparse design of (10).
",3.4. GN = GM,[0],[0]
"Example 3.6 (Equivariance to ×90○ Rotations) Figure below compares the digraph representation of Ω produced using (left) our sparse design, and (right) our dense design.
",3.4. GN = GM,[0],[0]
"3In a fully connected digraph, the edges that belong to the same orbit by G-action on N ×N, receive the same color.
",3.4. GN = GM,[0],[0]
"Multiples of ±90○ rotation is produced as the action of cyclic group Z4 on eight input output variables – that is N = M = {1, . . .",3.4. GN = GM,[0],[0]
",8}.",3.4. GN = GM,[0],[0]
Z4-action is semi-regular with two orbits; these orbits the two inner and outer set of four nodes.,3.4. GN = GM,[0],[0]
The representatives of each orbit in our sparse design is indicated using filled circles.,3.4. GN = GM,[0],[0]
"The generating set consists of A = {1,3}, rotation by 90○ and its inverse, rotation by 270○.",3.4. GN = GM,[0],[0]
"Each edge in each of these figures, has a corresponding edge in the opposite direction, within a different relation.",3.4. GN = GM,[0],[0]
"To avoid over-crowding the figure, we have dropped this edge from the drawing above, unless both edges belong to the same relation.
",3.4. GN = GM,[0],[0]
"Example 3.7 (Graph Convolution) Consider the setting where we use the (normalized) adjacency matrix B ∈ {0,1}N×N (or Laplacian) of a graph Λ, to identify parameter-sharing in a neural network layer.",3.4. GN = GM,[0],[0]
"For a single input/output channel, this is often in the form of Ax, where x ∈ RN and A = w1B+w2I has different parameters for diagonal and off-diagonal values (e.g., Kipf & Welling, 2016; Bruna et al., 2013; Henaff et al., 2015); for multiple channels see Section 3.3.",3.4. GN = GM,[0],[0]
"The following corollary of Theorem 2.1 identifies the equivariance of Ax.
Corollary 3.6 Given the digraph Λ and its binary adjacency matrix B ∈ {0,1}N×N , then (w1B + w2I)x is uniquely equivariant to the symmetry-group of Λ.
Since two graphs on N nodes can have identical symmetries, one implication of this corollary is that graphconvolution has identical equivariances for graphs with the same symmetry groups.",3.4. GN = GM,[0],[0]
This work is a step towards designing neural network layers with a given equivariance and invariance properties.,4. Conclusion,[0],[0]
"Our approach was to relate the equivariance properties of the neural layer to the symmetries of the parameter-matrix.
",4. Conclusion,[0],[0]
We then proposed two parameter-sharing scheme that achieves equivariance wrt any discrete group-action.,4. Conclusion,[0],[0]
"Moreover under some conditions, we guarantee sensitivity wrt other group actions.",4. Conclusion,[0],[0]
This is important because even a trivial constant function is invariant to all transformations.,4. Conclusion,[0],[0]
It is therefore essential to be able to draw the line between equivariance/invariance and sensitivity in a function.,4. Conclusion,[0],[0]
"To our knowledge, our work presents the first results of its kind on guarantees regarding both variance and equivariance with respect to group actions.",4. Conclusion,[0],[0]
This research is supported in part by DOE grant DESC0011114 and NSF grant IIS1563887.,Acknowledgment,[0],[0]
"Proof of Observation 1.1
gMφ(x) = φ(gNx)∀g ∈ G⇒ gMφ(x) = φ(gMx)∀g ∈H ⊂ G.
Proof of Theorem 2.1 For unique Aut(Ω)-equivariance we need proofs in two directions.",A. Proofs,[0],[0]
"First we show that
(πN, πM) ∈Aut(Ω)⇒ φ(x;w,Ω)",A. Proofs,[0],[0]
"= π−1M φ(πNx;w,Ω) (12)
which in turn shows that πMφ(x;w,Ω) = φ(πNx;w,Ω).",A. Proofs,[0],[0]
"Starting from π−1M φ(πNx;w,Ω) on the r.h.s.",A. Proofs,[0],[0]
of (12) and considering an index m in φ =,A. Proofs,[0],[0]
"[φ1, . . .",A. Proofs,[0],[0]
", φM ] we have
φπ−1 M m(πNx;w,Ω)",A. Proofs,[0],[0]
"= σ( ∑ n∈N,c∈α(n,π−1 M m) wcxπNn)
= σ( ∑ n∈πNN,c∈α(π−1N n,π −1 N m) wcxn)
= σ( ∑ n∈N,c∈α(n,m) wcxn) = φm(x;w,Ω)
(13)
",A. Proofs,[0],[0]
"where in arriving at (13) we used the fact that (πN, πM) ∈ Aut(Ω) ⇒ α(n,m) = α((π−1N n,π−1M m)).",A. Proofs,[0],[0]
"In the opposite direction we need to show that φ(x;w,Ω)",A. Proofs,[0],[0]
"= πMφ(π−1N x;w,Ω) ∀x",A. Proofs,[0],[0]
"∈ RN ,w ∈ RC only if (πN, πM) ∈Aut(Ω).
φ(x;w,Ω)",A. Proofs,[0],[0]
"= πMφ(π−1N x;w,Ω) ∀x",A. Proofs,[0],[0]
"∈ RN ,w ∈ RC ⇒
(14)
φm(x;w,Ω) = φπMm(π−1N x;w,Ω)",A. Proofs,[0],[0]
"∀m,x ∈ RN ,w ∈ RC ⇒ (15)
∀m,x ∈",A. Proofs,[0],[0]
"RN ,w ∈ RC (16) ∑
n∈N,c∈α(n,m) wcxn = ∑ n∈N,c∈α(n,πMm) wcxπ−1 N n ⇒ (17)
∑ n∈N,c∈α(n,m) wcxn = ∑ n∈N,c∈α(πNn,πMm) wcxn (18)
where (17) follows from monotonicity of σ ∶ R → R. We need to show that this final equality ∀m,x ∈ RN ,w ∈ RC implies that α(πNn,πMm) = α(n,m), which in turn, according to (3) means (πN, πM) ∈Aut(Ω).",A. Proofs,[0],[0]
"We prove α(ππNn,ππMm) = α(n,m) by contradiction: assume α(ππNn∗, ππMm∗) ≠ α(n∗,m∗) for some n∗,m∗. Since α(πNn∗, πMm∗) ≠ α(n∗,m∗),",A. Proofs,[0],[0]
"we can w.l.o.g. assume ∃c∗ ∈ α(n∗,m∗) s.t. c∗ ∉ α(πn∗, πm∗) (the reverse direction, where c∗ ∈ α(πNn∗, πMm∗) ∧ c∗ ∉
α(n∗,m∗) is similar).",A. Proofs,[0],[0]
We show that an assignment of x ∈ RN and w ∈ RC contradicts (18).,A. Proofs,[0],[0]
"For this, define x such that xn = δ(n,n∗), is non-zero only at index n∗. Moreover, assigning wc = δ(c, c∗) the r.h.s.",A. Proofs,[0],[0]
"of (18) is ∑n∈N,c∈α(πNn,πMm∗)wcxn = 0",A. Proofs,[0],[0]
while the l.h.s.,A. Proofs,[0],[0]
"is ∑n∈N,c∈α(m,n)wcxn = wc∗xn∗ ≠ 0.",A. Proofs,[0],[0]
"Therefore α(πNn,πMm) = α(n,m) ∀n,m, which by definition of Aut(Ω) means (πN, πM) ∈Aut(Ω).",A. Proofs,[0],[0]
"∎
",A. Proofs,[0],[0]
"Proof of Proposition 3.1 To prove GM,N ≤ Aut(Ω) we simply show that all (gN,gM) ∈ GN,M preserve the relations in Aut(Ω).",A. Proofs,[0],[0]
"From (4),
gN,M = (gN,gM) ∈Aut(Ω)⇐
((n,m) ∈ ∆p,q ⇔ (gNn,gMm) ∈ ∆p,q) ∀(p, q), n,m
The r.h.s holds for all (gN,gM)",A. Proofs,[0],[0]
"∈ GN,M because in constructing relations ∆p,q in the dense design, we used edge-orbits:
(n,m) ∈ ∆p,q⇔ (gNn,gMm) ∈ ∆p,q ∀(p, q), n,m.
Therefore gN,M ∈ GM,N ⇒ gN,M ∈Aut(Ω).",A. Proofs,[0],[0]
"∎
",A. Proofs,[0],[0]
"Proof of Theorem 3.3 We first show that any permutation (gN,gM)",A. Proofs,[0],[0]
"∈ GN,M is also in Aut(Ω).",A. Proofs,[0],[0]
"The major part of the proof is to show that when GN and GM are semi-regular, then ∣Aut(Ω)∣ ≤ ∣GN,M∣. Combination of these two proves Aut(Ω) = GN,M. I) to prove that (hN,hM) ∈ GN,M ⇒ (hN,hM) ∈ Aut(Ω), we simply apply (hN,hM) to an arbitrary edge (m,n) in a relation of Ω.",A. Proofs,[0],[0]
"According to (10)
∆p,q,a = {(gNanp,gMmq) ∣ (gN,gM) ∈",A. Proofs,[0],[0]
"GN,M}.
",A. Proofs,[0],[0]
"Application of (hN,hM) to (gNanp,gMmq) gives (hgNanp,hgMmq)",A. Proofs,[0],[0]
=,A. Proofs,[0],[0]
"(g′Nanp,g′Mmq) ∈ ∆p,q,a .",A. Proofs,[0],[0]
"From (4), it follows that GN,M ≤Aut(Ω).",A. Proofs,[0],[0]
II),A. Proofs,[0],[0]
"For this part, we use the orbit-stabilizer theorem.",A. Proofs,[0],[0]
"The orbit of each pair (n,m) ∈ ∆p,q,a wrt HN,M is defined as HN,M(n,m) = {(hNn,hMm) ∣",A. Proofs,[0],[0]
"hN,M ∈",A. Proofs,[0],[0]
"HN,M}.",A. Proofs,[0],[0]
"The stabilizer H(n,m)N,M of (n,m) ∈ ∆p,q,a is H (n,m) N,M = {hN,M ∈",A. Proofs,[0],[0]
"HN,M ∣ hN,M(n,m) = (n,m)}, the group of all actions that fix (n,m).",A. Proofs,[0],[0]
"The orbit-stabilizer theorem states that ∣HN,M∣ = ∣H(n,m)N,M ∣× ∣HN,M(n,m)∣. In our argument, we apply this theorem to bound ∣Aut(Ω)∣ using ∣Aut(Ω)(n,m)∣ and ∣Aut(Ω)(n,m)∣. The orbit-size, ∣Aut(Ω)(n,m)∣, for a pair (n,m) is bounded by the size of its relation ∣∆p,q,a ∣, for some p, q,a.",A. Proofs,[0],[0]
"This is because, according to (3),
π ∈Aut(Ω)⇒ ((n,m) ∈ ∆p,q,a ⇒ π(n,m) ∈ ∆p,q,a).
",A. Proofs,[0],[0]
"From (10), ∣∆p,q,a∣ = ∣GN,M∣, and therefore ∣Aut(Ω)(n,m)∣ < ∣GN,M∣.",A. Proofs,[0],[0]
"Now, it only remains to show that if GN and GM are regular orbits (or semi-regular), the stabilizer is trivial Aut(Ω)(n,m) = {e}.",A. Proofs,[0],[0]
"Because in this case the size of Aut(Ω) is bounded by the size of orbit ∣Aut(Ω)∣ = ∣Aut(Ω)(n,m)∣ ≤ ∣GN,M∣, which combined with the result of part (I) gives GN,M =Aut(Ω).",A. Proofs,[0],[0]
"Since, according to our assumption, GN acts regularly on GNnp ∀p, going back to definition of ∆p,q,a = {(gNanp,gMmq) ∣ gN,M ∈ GN,M}, this (see definition of regularity) implies that for each n ∈ GNnp, a ∈ A and mq , we can identify a single g′N ∈ GN such that for some (n,m) =",A. Proofs,[0],[0]
"(ag′Nnp,g′Mmq) ∈ ∆p,q,a .",A. Proofs,[0],[0]
This means that the edges (or pairs) adjacent to each node n ∈,A. Proofs,[0],[0]
GNnp,A. Proofs,[0],[0]
all have distinct colors.,A. Proofs,[0],[0]
The same argument using regularity of GMaction on GMmq ∀q shows that edges (or pairs) adjacent to m ∈ GMmq all have distinct colors.,A. Proofs,[0],[0]
"Therefore if we fix a pair (m,n), all their neighboring edges (adjacent on n or m) are unambiguously fixed.",A. Proofs,[0],[0]
The same goes for the neighbors of the newly fixed nodes and so on.,A. Proofs,[0],[0]
"If we can show that the bipartite graph representing Ω is connected then fixing a pair guarantees that all pairs in all relations of Ω are fixed and therefore (n,m) has a trivial stabilizer.
",A. Proofs,[0],[0]
"Two properties guarantee the connectedness of Ω:
• Since A = A−1 is a generating set of G, the bipartite subset consisting of subset of nodes GNnp and GMmq are connected.",A. Proofs,[0],[0]
"To show this, it is enough to show that we can reach any node nz starting from an arbitrary representative np and zigzagging through the bipartite structure.",A. Proofs,[0],[0]
"Since nz, np ∈",A. Proofs,[0],[0]
GNnp,A. Proofs,[0],[0]
⇒,A. Proofs,[0],[0]
∃gz ∈ GN s.t. nz = gznp.,A. Proofs,[0],[0]
"Since < A >= GN,M, we can write gz = a1 . .",A. Proofs,[0],[0]
.aL.,A. Proofs,[0],[0]
"The path that starts from np and takes the connections corresponding to ∆p,q,aL ,∆p,q,a−1L−1 ,∆p,q,aL−2 , . .",A. Proofs,[0],[0]
.,A. Proofs,[0],[0]
",∆p,q,a−11 takes us through a zigzag path from np to nz .
",A. Proofs,[0],[0]
"• Since we have a relation ∆p,q,a for all pairs p, q, all the induced bipartite subgraphs on GNnp-GMmq are connected.
",A. Proofs,[0],[0]
"This proves that the whole bipartite graph is connected and unambiguously fixed if we fix any pair (n,m).",A. Proofs,[0],[0]
"Therefore, (n,m) has a trivial stabilizer, proving that Aut(Ω) = GN,M. ∎
Proof of Corollary 3.4 Follows directly from Theorems 2.1 and 3.3. ∎
Proof of Claim 3.5 To see this, note that GM acts on M = G regularly, with
the natural (group) action gh.",A. Proofs,[0],[0]
Set the representative from the resulting single orbit as mq = e. Then (11) becomes φ =,A. Proofs,[0],[0]
"[φg]g∈G with components
φg(x;w) = σ( ∑ 1≤p≤P ∑ a∈A wa,pxgNanp) (19)
",A. Proofs,[0],[0]
"If we further tie the parameters across the orbits so that wa,p = wa,p′∀p, p′, the (19) above is equivalent to formulation of (Cohen & Welling, 2016a) for a single input/output channels (see Section 3.3 for multiple channels)",A. Proofs,[0],[0]
.,A. Proofs,[0],[0]
"∎
",A. Proofs,[0],[0]
Proof of Corollary 3.6,A. Proofs,[0],[0]
First we show this assuming a single channel K = 1.,A. Proofs,[0],[0]
"For multiple channels see Section 3.3.
",A. Proofs,[0],[0]
"Consider the bipartite structure constructed from Λ: Ω = (N,N,{{(n,n) ∣ n ∈ N},{(n,n′) ∣ (n,n′) ∈ E(Λ)}}).",A. Proofs,[0],[0]
"Applying the result of Theorem 2.1 using σ(x) = x tells us that the function Ax is uniquely Aut(Ω)-equivariant – that is π(Bx⋅,k) = B(πx⋅,k)∀π ∈Aut(Ω).",A. Proofs,[0],[0]
"Because of the relation ∆c∗ = {{(n,n) ∣ n ∈ N} in Ω, the same bipartite structure Ω, can be interpreted as a digraph; here with a single color, since Ω has only one relation in addition to ∆c∗ .",A. Proofs,[0],[0]
"Since this relation defines Λ, Aut(Ω) = Aut(Λ), which means Bx is uniquely Aut(Λ)-equivariant. ∎",A. Proofs,[0],[0]
Let x =,B. Background on Permutation Groups,[0],[0]
"[x1, . . .",B. Background on Permutation Groups,[0],[0]
", xN ] ∈ XN be a vector of N variables taking value in the same domain X. A group G is a set, equipped with a binary operation, with the following properties: I) G is closed under its binary operation; II) the group operation is associative –i.e., (g1g2)g3 = g1(g2g3)∀g1,g2,g3 ∈ G; III) there exists an identity e ∈ G such that ge = eg = g and ; IV) every element g ∈ G has an inverse g−1 ∈ G, such that gg−1 = g−1g = e. A subset H ⊆ G is a subgroup of G (G ≤ H) iff H equipped with the binary operation of G forms a group.",B. Background on Permutation Groups,[0],[0]
"Moreover, if H is a proper subset of G, H is a proper subgroup of G, H < G. Two groups are isomorphic G ≅ H if there exists a bijection β ∶ G → H, such that g1g2",B. Background on Permutation Groups,[0],[0]
= g3 ⇔ β(g1)β(g2),B. Background on Permutation Groups,[0],[0]
"= β(g3)∀g1,g2,g3.",B. Background on Permutation Groups,[0],[0]
"If this last relation holds for a surjective mapping (not necessarily one-to-one) then β is a homomorphic mapping and H is isomorphic to a subgroup of G.
Cayley Diagram.",B. Background on Permutation Groups,[0],[0]
"The set A ⊆ G is called the generating set of G (< A >= G), iff every member of G can be expressed as a combination of members of A.",B. Background on Permutation Groups,[0],[0]
If the generating set is closed under inverse a ∈A ⇒ a−1 ∈A we call it a symmetric generating set.,B. Background on Permutation Groups,[0],[0]
A is the minimal generating set if it has the least number of members among the generating sets of G. Note that the minimal generating sets are generally not unique.,B. Background on Permutation Groups,[0],[0]
"The size of the minimal generating set of a group G becomes important because, the number
of parameters in our parameter-sharing scheme grows linearly with ∣A∣. A group G is often visualized by its Cayley diagram; a colored digraph in which the node-set is G and directed edge (g,ag)∀g ∈ G,a ∈",B. Background on Permutation Groups,[0],[0]
A is colored by a ∈ A. Fig.,B. Background on Permutation Groups,[0],[0]
"1(lower-left) shows the Cayley diagram of G =D5.
B.1.",B. Background on Permutation Groups,[0],[0]
"Discrete Group Action
We are interested on the way a group “acts” on the input and output of a deep network.",B. Background on Permutation Groups,[0],[0]
"Function γ ∶ G ×X → X is the left action of group G on x iff I) γ(e,x) =",B. Background on Permutation Groups,[0],[0]
"x and; II) γ(g1, γ(g2,x))",B. Background on Permutation Groups,[0],[0]
"= γ(g1g2,x).4
For our purpose we limit this action to actions on the indices N = {1, . . .",B. Background on Permutation Groups,[0],[0]
",N} of x =",B. Background on Permutation Groups,[0],[0]
"[xn] – i.e., function γ ∶ G × N → N satisfies γ(e, n) = n and γ(g1, γ(g2, n))",B. Background on Permutation Groups,[0],[0]
"= γ(g1g2, n).",B. Background on Permutation Groups,[0],[0]
"We often use gn as a shorthand for γ(g, n), and also use gN to denote {gn ∣ n ∈ N}.",B. Background on Permutation Groups,[0],[0]
The action of g on a vector/sequence Ð→ N =,B. Background on Permutation Groups,[0],[0]
"[1, . .",B. Background on Permutation Groups,[0],[0]
.,B. Background on Permutation Groups,[0],[0]
",N] is defined similarly g Ð→ N ≐",B. Background on Permutation Groups,[0],[0]
"[g1, . . .",B. Background on Permutation Groups,[0],[0]
",gN].",B. Background on Permutation Groups,[0],[0]
"Considering this, the G-action on x =",B. Background on Permutation Groups,[0],[0]
"[x1, . . .",B. Background on Permutation Groups,[0],[0]
", xN ] is gx ≐",B. Background on Permutation Groups,[0],[0]
"[xg1, . .",B. Background on Permutation Groups,[0],[0]
.,B. Background on Permutation Groups,[0],[0]
xgN,B. Background on Permutation Groups,[0],[0]
].,B. Background on Permutation Groups,[0],[0]
"From the properties of group and its action it follows that γ(g, ⋅) ∶ N → N is a bijection with γ−1(g, n)",B. Background on Permutation Groups,[0],[0]
"= γ(g−1, n)∀n ∈ N,g ∈ G. Since N is a finite set, this bijection for each g ∈ G is a permutation of Ð→N – i.e., gÐ→N ≐",B. Background on Permutation Groups,[0],[0]
"[γ(g,1), . . .",B. Background on Permutation Groups,[0],[0]
", γ(g,N)] is a permutation of Ð→N .",B. Background on Permutation Groups,[0],[0]
"Let GN = {γ(g, ⋅) ∣ g ∈ G} with (function composition as the binary group operation) denote the group of permutations of Ð→ N induced by",B. Background on Permutation Groups,[0],[0]
g ∈ G.,B. Background on Permutation Groups,[0],[0]
This group is a subgroup of the symmetric group SN; the group of all N !,B. Background on Permutation Groups,[0],[0]
permutations of Ð→ N .,B. Background on Permutation Groups,[0],[0]
"GN captures the structure of G when it acts on the set N and it is indeed a homomorphic image of G. We use gN to denote γ(g, ⋅), the the image of g ∈ G in GN.
B.1.1.",B. Background on Permutation Groups,[0],[0]
"PROPERTIES OF GROUP ACTION
G-action is faithful iff two groups are isomorphic G ≅ GN.",B. Background on Permutation Groups,[0],[0]
In this case all actions of g ∈ G are distinct permutations – that is g Ð→ N ≠,B. Background on Permutation Groups,[0],[0]
"g′Ð→N∀g,g′ ∈ G.",B. Background on Permutation Groups,[0],[0]
Given any G-action on N we can obtain its faithful subgroup that is isomorphic to GN.,B. Background on Permutation Groups,[0],[0]
"The importance of faithfulness of G-action is because it preserves the structure of G, and if an action is not faithful, we might as well focus on GN-action.
",B. Background on Permutation Groups,[0],[0]
"Given any unfaithful G-action γ ∶ G × N → N, let Kγ be the normal subgroup of G that corresponds to identity permutation –i.e., Kγ = {g ∈ G ∣ γ(g, n) = n∀n ∈ N}.",B. Background on Permutation Groups,[0],[0]
One obtains the group GN that acts faithfully on N as the quotient group GN = G/Kγ .,B. Background on Permutation Groups,[0],[0]
"We now define some group properties that are important in guaranteeing the “strict” equivariance with respect to G-
4All the following definitions and results may be extended to the “right” group action by substituting g↔ g−1∀g ∈ G.
action.",B. Background on Permutation Groups,[0],[0]
"G-action on N is transitive iff ∀n1, n2 ∈ N, there exists at least one action g ∈ G such that gn1 = n2.",B. Background on Permutation Groups,[0],[0]
"The group action is free or semi-regular iff ∀n1, n2 ∈ N, there is at most one g ∈ G such at gn1 = n2, and the action is regular iff it is both transitive and free – i.e., for any pair n1, n2 ∈ N, there is uniquely one g ∈ G such that gn1 = n2.",B. Background on Permutation Groups,[0],[0]
"Any free action is also faithful.
",B. Background on Permutation Groups,[0],[0]
B.1.2.,B. Background on Permutation Groups,[0],[0]
"ORBITS
",B. Background on Permutation Groups,[0],[0]
"Given G-action on N, the orbit of n ∈ N is all the members to which it can be moved, Gn = {gn ∣ n ∈ N}.",B. Background on Permutation Groups,[0],[0]
"The orbits of n ∈ N form an equivalence relation, where n ∼ n′ ⇔ ∃g s.t., n = gn′ ⇔ n ∈ Gn′ ⇔",B. Background on Permutation Groups,[0],[0]
n′ ∈ Gn.,B. Background on Permutation Groups,[0],[0]
"This equivalence relation partitions N into orbits N = ⋃1≤p≤P Gnp, where np is an arbitrary representative of the partition Gnp ⊆ N. Note that the G-action on N is always transitive on its orbits – that is for any n,n′ ∈ Gnp, there is at least one g ∈ G such that n =",B. Background on Permutation Groups,[0],[0]
"gn′. Therefore, for a semi-regular G-action, the action of G on the orbits Gnp∀1 ≤ p ≤ P is regular.",B. Background on Permutation Groups,[0],[0]
"As we see the number of distinct parameters in our parametersharing scheme grows with the number of orbits.
",B. Background on Permutation Groups,[0],[0]
Cycle Notation.,B. Background on Permutation Groups,[0],[0]
"To explicitly show the action of g ∈ G on the set N, we sometimes use the cycle notation of a permutation.",B. Background on Permutation Groups,[0],[0]
Any permutation π ∈ SN is decomposable to product of disjoint cycles.,B. Background on Permutation Groups,[0],[0]
"A cycle of length d, (b1, . .",B. Background on Permutation Groups,[0],[0]
.,B. Background on Permutation Groups,[0],[0]
", bd) sends bi → bi+1 mod d.",B. Background on Permutation Groups,[0],[0]
"Here bi ∈ {1, . .",B. Background on Permutation Groups,[0],[0]
.,B. Background on Permutation Groups,[0],[0]
",N} and a cycle acts on a subset of N.",B. Background on Permutation Groups,[0],[0]
"For example, the action of (1,3,2) on [1, . .",B. Background on Permutation Groups,[0],[0]
.,B. Background on Permutation Groups,[0],[0]
",6] is [3,1,2,4,5,6].",B. Background on Permutation Groups,[0],[0]
"We can write the permutation g where g[1, . . .",B. Background on Permutation Groups,[0],[0]
",6] =",B. Background on Permutation Groups,[0],[0]
"[3,1,2,5,4,6] as the product of disjoint cycles {(1,3,2), (6), (4,5)} = {(1,3,2), (4,5)}.",B. Background on Permutation Groups,[0],[0]
We propose to study equivariance in deep neural networks through parameter symmetries.,abstractText,[0],[0]
"In particular, given a group G that acts discretely on the input and output of a standard neural network layer φW",abstractText,[0],[0]
"∶ R → R , we show that φW is equivariant with respect to G-action iff G explains the symmetries of the network parameters W. Inspired by this observation, we then propose two parameter-sharing schemes to induce the desirable symmetry on W. Our procedure for tying the parameters achieves G-equivariance and, under some conditions on the action of G, it guarantees sensitivity to all other permutation groups outside G.",abstractText,[0],[0]
"Given enough training data, a multi-layer perceptron would eventually learn the domain invariances in a classification task.",abstractText,[0],[0]
"Nevertheless, success of convolutional and recurrent networks suggests that encoding the domain symmetries through shared parameters can significantly boost the generalization of deep neural networks.",abstractText,[0],[0]
The same observation can be made in deep learning for semi-supervised and unsupervised learning in structured domains.,abstractText,[0],[0]
This raises an important question that is addressed in this paper: What kind of priors on input/output structure can be encoded through parameter-sharing?,abstractText,[0],[0]
"This work is an attempt at answering this question, when our priors are in the form discrete domain symmetries.",abstractText,[0],[0]
"To formalize this type of prior, a family of transformations of input and output to a neural layer are expressed as group “action” on the input and output.",abstractText,[0],[0]
"The resulting neural network is invariant to this action, if transformations of the input within that particular family, does not change the output (e.g., rotation-invariance).",abstractText,[0],[0]
"However, if the output is transformed, in a predictable way, as we transform the input, the neural layer is equivariant to the action of the group.",abstractText,[0],[0]
"School of Computer Science, Carnegie Mellon University, 5000 Forbes Ave., Pittsburgh, PA, USA 15217.",abstractText,[0],[0]
Correspondence to: Siamak Ravanbakhsh <mravanba@cs.cmu.edu>.,abstractText,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",abstractText,[0],[0]
Copyright 2017 by the author(s).,abstractText,[0],[0]
Our goal is to show that parameter-sharing can be used to achieve equivariance to any discrete group action.,abstractText,[0],[0]
"Application of group theory in machine learning has been the topic of various works in the past (e.g., Kondor, 2008; Bartók et al., 2010).",abstractText,[0],[0]
"In particular, many probabilistic inference techniques have been extended to graphical models with known symmetry groups (Raedt et al., 2016; Kersting et al., 2009; Bui et al., 2012; Niepert, 2012).",abstractText,[0],[0]
"Deep and hierarchical models have used a variety of techniques to study or obtain representations that isolate transformations from the “content” (e.g., Hinton et al., 2011; Jayaraman & Grauman, 2015; Lenc & Vedaldi, 2015; Agrawal et al., 2015).",abstractText,[0],[0]
"The simplest method of achieving equivariance is through data-augmentation (Krizhevsky et al., 2012; Dieleman et al., 2015).",abstractText,[0],[0]
"Going beyond augmentation, several methods directly apply the group-action, in one way or another, by transforming the data or its encodings using group members (Jaderberg et al., 2015; Anselmi et al., 2013; Dieleman et al., 2016).",abstractText,[0],[0]
An alternative path to invariance via harmonic analysis.,abstractText,[0],[0]
"In particular cascade of wavelet transforms is investigated in (Bruna & Mallat, 2013; Oyallon & Mallat, 2015; Sifre & Mallat, 2013).",abstractText,[0],[0]
"More recently (Cohen & Welling, 2016b) study steerable filters (e.g., Freeman et al., 1991; Hel-Or & Teo, 1998) as a general mean for achieving equivariance in deep networks.",abstractText,[0],[0]
"Invariance and equivariance through parameter-sharing is also discussed in several prior works (Cohen & Welling, 2016a; Gens & Domingos, 2014).",abstractText,[0],[0]
The desirability of using parameter-sharing for this purpose is mainly due to its simplicity and computational efficiency.,abstractText,[0],[0]
"However, it also suggests possible directions for discovering domain symmetries through regularization schemes.",abstractText,[0],[0]
"Following the previous work on the study of symmetry in deep networks, we rely on group theory and group-actions to formulate invariances and equivariances of a function.",abstractText,[0],[0]
"Due to discrete nature of parameter-sharing, our treatment here is limited to permutation groups.",abstractText,[0],[0]
"Action of a permutation group G can model discrete transformations of a set of variables, such as translation and 90○ rotation of pixels around any center in an image.",abstractText,[0],[0]
"If the output of a function transforms with a G-action as we transform its input with a different G-action, the function is equivariant with respect to action of G.",abstractText,[0],[0]
"For example, in a convolution layer, as we translate the input, the feature-maps are also translated.",abstractText,[0],[0]
If ar X iv :1 70 2. 08 38,abstractText,[0],[0]
9v 2,abstractText,[0],[0]
[ st at .M,abstractText,[0],[0]
L ] 1 3 Ju n 20 17 Equivariance Through Parameter-Sharing Figure 1.,abstractText,[0],[0]
"Summary: given a group action on input and output of a neural network layer, define a parameter-sharing for this layer that is equivariant to these actions.",abstractText,[0],[0]
"(left) G = D5 is a Dihedral group, acting on a 4 × 5 input image and an output vector of size 5.",abstractText,[0],[0]
"N and M denote the index set of input, and output variables respectively.",abstractText,[0],[0]
Here G is represented using its Cayley diagram.,abstractText,[0],[0]
(middle-left) G-action for g ∈ G is shown for an example input.,abstractText,[0],[0]
G-action on the input is a combination of circular shifts (blue arrows) and vertical flips (red arrows) of the 2D image.,abstractText,[0],[0]
G acts on the output indices M only through circular shift.,abstractText,[0],[0]
"A permutation group GN,M encodes the simultaneous “action” of G on input and output indices.",abstractText,[0],[0]
(middle-right),abstractText,[0],[0]
"The structure Ω designed using our procedure, such that its symmetries Aut(Ω) subsumes the permutation group GN,M. (right) the same structure Ω unfolded to a bipartite form to better show the resulting parameter-sharing in the neural layer.",abstractText,[0],[0]
"The layer is equivariant to G-action: shifting the input will shift the output of the resulting neural network function, while flipping the input does not change the output.",abstractText,[0],[0]
"the output does not transform at all, the function is invariant to the action of G. Therefore, invariance is a special equivariance.",abstractText,[0],[0]
"In this example, different translations correspond to the action of different members of G. The novelty of this work is its focus on the “model symmetry” as a gateway to equivariance.",abstractText,[0],[0]
This gives us new theoretical guarantees for a “strict” notion of equivariance in neural networks.,abstractText,[0],[0]
The core idea is simple: consider a colored bipartite graph Ω representing a neural network layer.,abstractText,[0],[0]
Edges of the same color represent tied parameters.,abstractText,[0],[0]
"This neural network layer as a function is equivariant to the actions of a given group G (and nothing more) iff the action of G is the symmetry group of Ω – i.e., there is a simple bijection between parameter symmetries and equivariences of the corresponding neural network.",abstractText,[0],[0]
"The problem then boils down to designing colored bipartite graphs with given symmetries, which constitutes a major part of this paper.",abstractText,[0],[0]
Fig. 1 demonstrates this idea.1 For the necessary background on group theory see the Appendix.,abstractText,[0],[0]
"In the following, Section 1 formalizes equivariance wrt discrete group action.",abstractText,[0],[0]
Section 2 relates the model symmetries a neural layer to its equivariance.,abstractText,[0],[0]
Section 3 then builds on this observation to introduce two procedures for parameter-sharing that achieves a desirable equivariance.,abstractText,[0],[0]
"Throughout this paper, since we deal with finite sets, we use circular shift and circular convolution instead of shift and convolution.",abstractText,[0],[0]
The two can be made identical with zero-padding of the input.,abstractText,[0],[0]
"Here, we also see how group and graph convolution as well as deep-sets become special instances in our parametersharing procedure, which provides new insight and improved design in the case of group convolution.",abstractText,[0],[0]
"Where input and output of the layer have a one-to-one mapping, we see that the design problem reduces a well-known problem in combinatorics.",abstractText,[0],[0]
1. Group Action and Equivariance Let x =,abstractText,[0],[0]
"[x1, . . .",abstractText,[0],[0]
", xN ] ∈ X denote a set of variables and G = {g} be a finite group.",abstractText,[0],[0]
"The discrete action of G on x is in the form of permutation of indices in N = {1, . . .",abstractText,[0],[0]
",N}.",abstractText,[0],[0]
This group is a subgroup of the symmetric group SN; the group of all N !,abstractText,[0],[0]
permutations of N objects.,abstractText,[0],[0]
We use Ð→ N =,abstractText,[0],[0]
"[1, . .",abstractText,[0],[0]
.,abstractText,[0],[0]
",N] to denote the ordered counterpart to N and the G-action on this vector g Ð→",abstractText,[0],[0]
N ≐,abstractText,[0],[0]
"[g1, . . .",abstractText,[0],[0]
",gN] is a simple permutation.",abstractText,[0],[0]
"Using xÐ→ N to denote x, the discrete action of g ∈ G on x ∈ X is given by gxÐ→ N ≐",abstractText,[0],[0]
x g Ð→ N .,abstractText,[0],[0]
G-action on N is a permutation group that is not necessarily isomorphic to G itself.,abstractText,[0],[0]
GN ≤ G captures the structure of G when it acts on N. We use gN to denote the image of g ∈ G in GN.,abstractText,[0],[0]
G-action is faithful iff two groups are isomorphic G ≅,abstractText,[0],[0]
GN – that is G-action preserves its structure.,abstractText,[0],[0]
"In this case, each g ∈ G maps to a distinct permutation g Ð→",abstractText,[0],[0]
N ≠,abstractText,[0],[0]
"g′Ð→ N∀g,g′ ∈ G.",abstractText,[0],[0]
Given any G-action on N we can efficiently obtain GN; see Appendix.,abstractText,[0],[0]
"Equivariance Through Parameter-Sharing Example 1.1 (Cyclic Group) Consider the cyclic group G = Z6 and define its action on x ∈ R by defining it on the index set N = {1,2,3} as gn ≐",abstractText,[0],[0]
g + n mod 3∀g ∈ Z6.,abstractText,[0],[0]
This action is not faithful.,abstractText,[0],[0]
"For example, the action of g = 1 and g = 4 result in the same permutations of variables in x; i.e., single-step of circular shift to the right.",abstractText,[0],[0]
"With the above action, the resulting permutation group GN is isomorphic to Z3 < Z6.",abstractText,[0],[0]
Now consider the same group G = Z6 with a different action on N:,abstractText,[0],[0]
gn ≐,abstractText,[0],[0]
"g − n mod 3∀g ∈ Z6, where we replaced (+) with (−).",abstractText,[0],[0]
Let G̃N be the resulting permutation group.,abstractText,[0],[0]
Here again G̃N ≅ Z3.,abstractText,[0],[0]
"Although isomorphic, G̃N ≠ GN, as they are different permutation groups of N. Consider the function φ ∶ X → Y and let GN and GM be the action of G on input/output index sets N and M. Definition 1.1 The joint permutation group GN,M is a subdirect product (or pairing) of GN and GM GN,M = GN ⊙GM ≐",abstractText,[0],[0]
"{(gN,gM) ∣ g ∈ G}.",abstractText,[0],[0]
We are now ready to define equivariance and invariance.,abstractText,[0],[0]
φ(⋅),abstractText,[0],[0]
"is GN,M-equivariant iff gNφ(x)",abstractText,[0],[0]
"= φ(gMx) ∀x ∈ X , (gN,gM)",abstractText,[0],[0]
"∈ GN,M (1)",abstractText,[0],[0]
"Moreover, if GM = {e} is trivial, we have gNφ(x) = φ(x) ∀x ∈ X ,gN ∈ GN and φ(⋅) is GN-invariant.",abstractText,[0],[0]
gN and gM,abstractText,[0],[0]
"can also be represented using permutation matrices GN ∈ {0,1}N×N , and GM ∈ {0,1}M×M .",abstractText,[0],[0]
"Equivariance relation of (1) then becomes GMφ(x) = φ(GNx)∀x ∈ X ,",abstractText,[0],[0]
"(GN,GM) ∈ GN,M (2)",abstractText,[0],[0]
The following observation shows that the subgroup relationship affects equivariance and invariance.,abstractText,[0],[0]
Observation 1.1,abstractText,[0],[0]
"If the function φ ∶ X → Y is GN,M equivariant, then it is also HN,M -equivariant for any permutation group HN,M < G. Example 1.2 (Reverse Convolution) Consider the cyclic group G = Z6 and for g ∈ G, define the action on N = {1,2,3} to be gn ≐",abstractText,[0],[0]
g + n mod 3.,abstractText,[0],[0]
"Also let its action on M = {1, . . .",abstractText,[0],[0]
",6} be gm ≐",abstractText,[0],[0]
g,abstractText,[0],[0]
− n mod 6.,abstractText,[0],[0]
"In other words, G-action on N performs circular shift to the right and its action on M shifts variables to the left.",abstractText,[0],[0]
Examples of the permutation matrix representation for two members of GN and GM are 2N =,abstractText,[0],[0]
( 0 1 0 0 0 1 1 0 0 ) 2M = ⎛ ⎜ ⎝ 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 ⎞ ⎟ ⎠ corresponding to right and left shift on vectors of different lengths.,abstractText,[0],[0]
Now consider the function φ ∶ R → R φW(x),abstractText,[0],[0]
=Wx W = ( 0 a b 0 a b a b 0 a b 0 b 0,abstractText,[0],[0]
"a b 0 a ) ∀a, b ∈ R",abstractText,[0],[0]
Using permutation matrices one could check the equivariance condition (2) for this function.,abstractText,[0],[0]
"We can show that φ is equivariant to GN,M. Consider 2 ∈ Z6 and its images 2N ∈ GN and 2M ∈ GM.",abstractText,[0],[0]
L.h.s. of (2) is 2MφW(x) = ⎛ ⎜ ⎝ 0 0,abstractText,[0],[0]
1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 1 0 0 ⎞ ⎟ ⎠ ⎛ ⎜ ⎝ 0 a b a b 0 b 0 a 0 a b a b 0 b 0 a ⎞ ⎟ ⎠ x = ⎛ ⎜ ⎝ b 0 a 0 a b a b 0 b 0 a 0 a b a b 0 ⎞ ⎟ ⎠ x which is equal to its r.h.s. φW(2Nx),abstractText,[0],[0]
= ⎛ ⎜ ⎝ 0 a b a b 0 b 0,abstractText,[0],[0]
a 0 a b a b 0 b 0,abstractText,[0],[0]
a ⎞ ⎟ ⎠ ( 0 1 0 0 0 1 1 0 0 )x,abstractText,[0],[0]
= ⎛ ⎜ ⎝ b 0 a 0 a b a b 0 b 0,abstractText,[0],[0]
a 0 a b a b 0 ⎞ ⎟ ⎠ x for,abstractText,[0],[0]
any x. One could verify this equality for all g ∈ Z6.,abstractText,[0],[0]
"Now consider the group HN,M < GN,M, where HN = GN and members of HM = {0,2,4}, perform left circular shift of length 0,2 and 4.",abstractText,[0],[0]
"It is easy to see that HN,M ≅ Z3.",abstractText,[0],[0]
"Moreover since HN,M < GN,M, φ(⋅) above is HN,Mequivariant as well.",abstractText,[0],[0]
"However, one prefers to characterize the equivariance properties of φ using GN,M rather than HN,M. The observation above suggests that GN,M-equivariance is not restrictive enough.",abstractText,[0],[0]
"As an extreme case, a constant function φ(x) = 1 is equivariant to any permutation group GN,M ≤ SN × SM.",abstractText,[0],[0]
"In this case equivariance of φ with respect to a particular GN,M is not very informative to us.",abstractText,[0],[0]
"To remedy this, we define a more strict notion of equivariance.",abstractText,[0],[0]
Definition 1.2 we say a function φ,abstractText,[0],[0]
∶,abstractText,[0],[0]
X,abstractText,[0],[0]
→ Y is uniquely G-equivariant iff it is G-equivariant and it is “not” H-equivariant for any H > G. 2.,abstractText,[0],[0]
"Symmetry Groups of a Network Given a group G, and its discrete action through GN,M, we are interested in defining parameter-sharing schemes for a parametric class of functions that guarantees their unique GN,M-equivariance.",abstractText,[0],[0]
"We start by looking at a single neural layer and relate its unique GN,M-equivariance to the symmetries of a colored multi-edged bipartite graph that deEquivariance Through Parameter-Sharing fines parameter-sharing.",abstractText,[0],[0]
We then show that the idea extends to multiple-layers.,abstractText,[0],[0]
"Definition 2.1 A colored multi-edged bipartite graph Ω = (N,M, α) is a triple, where N and M are its two sets of nodes, and α ∶ N × M → 2{1,...,C} is the edge function that assigns multiple edge-colors from the set {1, . .",abstractText,[0],[0]
.,abstractText,[0],[0]
",C} to each edge.",abstractText,[0],[0]
Non-existing edges receive no color.,abstractText,[0],[0]
We are interested in the symmetries of this structure.,abstractText,[0],[0]
"The set of permutations (πN, πM) ∈ SN × SM of nodes (within each part of the bipartite graph) that preserve all edgecolors define the Automorphism Group Aut(Ω) ≤ SN × SM – that is ∀(n,m) ∈ N ×M (πN, πM) ∈Aut(Ω) ⇔ α(n,m) = α((πNn,πMm))",abstractText,[0],[0]
"(3) Alternatively, to facilitate the notation, we define the same structure (colored multi-edged bipartite graph) as a set of binary relations between N and M – that is Ω = (N,M,{∆c}1≤c≤C) where each relation is associated with one color ∆c = {(n,m) ∣ c ∈ α(n,m)∀(n,m) ∈ N ×M}.",abstractText,[0],[0]
"This definition of structure, gives an alternative expression for Aut(Ω) (πN, πM) ∈Aut(Ω) ⇔ (4) ((n,m) ∈",abstractText,[0],[0]
"∆c ⇔ (πNn,πMm) ∈",abstractText,[0],[0]
"∆c) ∀c, n,m The significance of this structure is in that, it defines a parameter-sharing scheme in a neural layer, where the same edge-colors correspond to the same parameters.",abstractText,[0],[0]
Consider the function φ ≐,abstractText,[0],[0]
"[φ1, . .",abstractText,[0],[0]
.,abstractText,[0],[0]
", φM ] ∶ R → R",abstractText,[0],[0]
Equivariance Through Parameter-Sharing,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 189–195 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2030",text,[0],[0]
"Robustness has always been a desirable property for natural language parsers: humans generate a variety of noisy outputs, such as ungrammatical webpages, speech disfluencies, and the text in language learner’s essays.",1 Introduction,[0],[0]
"Such non-canonical text contains grammatical errors such as substitutions, insertions, and deletions.",1 Introduction,[0],[0]
"For example, a nonnative speaker of English might write “*I look in forward hear from you”, where in is inserted, to is deleted, and hearing is substituted incorrectly.
",1 Introduction,[0],[0]
We propose a novel dependency parsing scheme that jointly parses and repairs ungrammatical sentences with these sorts of errors.,1 Introduction,[0],[0]
"The parser is based on the non-directional easy-first (EF) parser introduced by Goldberg and Elhadad (2010) (GE herein), which iteratively adds the most probable arc until the parse tree is completed.",1 Introduction,[0],[0]
These actions are called ATTACHLEFT and ATTACHRIGHT depending on the direction of the arc.,1 Introduction,[0],[0]
We extend the EF parsing scheme to be robust for ungrammatical inputs by correcting grammatical er-,1 Introduction,[0],[0]
"rors with three new actions: SUBSTITUTE, INSERT, and DELETE.",I you,[0],[0]
These new actions do not add an arc between tokens but instead they edit a single token.,I you,[0],[0]
"As a result, the parser is able to jointly parse and correct grammatical errors in the input sentence.",I you,[0],[0]
We call this new scheme Error-Repair NonDirectional Easy-First parsing (EREF).,I you,[0],[0]
"Since the new actions may greatly increase the search space (e.g., infinite substitutions), we also introduce simple constraints to avoid such issues.
",I you,[0],[0]
We first describe the technical details of EREF (§2) and then evaluate our EREF parser with respect to dependency accuracy (robustness) and grammaticality improvements (§3).,I you,[0],[0]
"Finally, we
189
position this effort at the intersection of noisy text parsing and grammatical error correction (§4).",I you,[0],[0]
"Non-directional Easy-first Parsing Let us begin with a brief review of a non-directional easyfirst (EF) parsing scheme proposed by GE, which is the foundation of our proposed scheme described in the following sections.
",2 Model,[0],[0]
"The EF parser has a list of partial structures p1, ..., pk (called pending) initialized with sentence tokens w1, ..., wn, and it keeps updating pending through derivations.",2 Model,[0],[0]
"Unlike left-to-right (e.g., shift-reduce) parsing algorithms (Yamada and Matsumoto, 2003; Nivre, 2004), EF iteratively selects the best pair of adjoining tokens and chooses the direction of attachment: ATTACHLEFT or ATTACHRIGHT.",2 Model,[0],[0]
"Once the action is committed, the corresponding dependency arc is added and the child token is removed from pending.",2 Model,[0],[0]
The first two derivations in Figure 1 depict ATTACHRIGHT and ATTACHLEFT.,2 Model,[0],[0]
"Pseudocode is shown in Algorithm 1 (lines 1, 3-12).
",2 Model,[0],[0]
"The parser is trained using the structured perceptron (Collins, 2002) to choose actions to take given a set of features expanded from templates.",2 Model,[0],[0]
The cost of actions is computed at every step by checking the validity: whether a new arc is included in the gold parse and whether the child already has all its children.,2 Model,[0],[0]
See GE for further description of feature templates and structured perceptron training.,2 Model,[0],[0]
"Since it is possible that there are multiple valid sequence of actions and it is important to examine a large search space, the parser is allowed to explore (possibly incorrect) actions with a certain probability, termed learning with exploration by Goldberg and Nivre (2013).
",2 Model,[0],[0]
Error-repair variant of EF Error-repair nondirectional easy-first parsing scheme (EREF) is a variant of EF.,2 Model,[0],[0]
"We add three new actions: SUBSTITUTE, DELETE, INSERT as ActsER.",2 Model,[0],[0]
"We do not deal with a swapping action (Nivre, 2009) to deal with word reordering errors, since the errors are even less frequent than other error types (Leacock et al., 2014).",2 Model,[0],[0]
"SUBSTITUTE replaces a token to a grammatically more probable token, DELETE removes an unnecessary token, and INSERT inserts a new token at a designated index.",2 Model,[0],[0]
These actions are shown in Figure 1 and Algorithm 1 (lines 13-25).,2 Model,[0],[0]
"Because the length of pending decreases as an attachment occurs, the parser
Algorithm 1: Error-repair non-directional parsing Input: ungrammatical sentence= w1 ...",2 Model,[0],[0]
"wn Output: a set of dependency arcs (Arcs),
repaired sentence (ŵ1 ... ŵm) 1 Acts = { ATTACHLEFT, ATTACHRIGHT } 2 ActsER = { DELETE, INSERT, SUBSTITUTE } 3 Arcs = { } 4 pending = p1...",2 Model,[0],[0]
pn ← w1...,2 Model,[0],[0]
wn 5 repaired = ŵ1...ŵn ← w1...,2 Model,[0],[0]
"wn 6 while len (pending) > 1 do 7 best ← argmax
act∈Acts∪ActsER score (act (i))
8 s.t. 1 ≤ i ≤ len(pending) ∩ isLegal(act, pending) 9 if best ∈ Acts then
10 (parent, child)← edgeFor(best) 11 Arcs.add((parent, child)) 12 pending.remove(child) 13 else if best = SUBSTITUTE then 14 c = bestCandidate(best, repaired) 15 pending.replace(pi, c) 16 repaired.replace(ŵpi.idx, c) 17 else if best = DELETE then 18 pending.remove(pi) 19 repaired.remove(ŵpi.idx) 20",2 Model,[0],[0]
"Arcs.updateIndex() 21 else if best = INSERT then 22 c = bestCandidate(best, repaired) 23 pending.insert(i, c) 24 repaired.insert(pi.idx, c) 25 Arcs.updateIndex() 26 end 27 return Arcs, repaired
also keeps the token indices in repaired (line 5), which holds all tokens in a sentence throughout the parsing process.",2 Model,[0],[0]
"Furthermore, the parser updates token indices in pending and repaired when INSERT or DELETE occurs.",2 Model,[0],[0]
"Technically, when a token at i is deleted/inserted, the parser decrements/increments the indices that are k >= i (before executing the action) in pending, repaired, and parents and children in a (partial) dependency tree (Arcs).
",2 Model,[0],[0]
"To find the best candidate for SUBSTITUTE and INSERT efficiently, we restrict candidates to the same part-of-speech or pre-defined candidate list.",2 Model,[0],[0]
"We select the best candidate by comparing each n-gram language model score with the same surrounding context.
",2 Model,[0],[0]
"Similar to EF, while training the parser, the cost
Algorithm 2: Check validity during training 1 Function isValid(act, repaired, Gold) 2 d before = editDistance(repaired, Gold) 3 repaired + = repaired.apply(act) 4 d after = editDistance(repaired +, Gold) 5 if d before > d after then return true; 6 else return false; 7 end
for ActsER is based on validity.",2 Model,[0],[0]
The validity of the new actions is computed by taking the edit distance (d) between the Gold tokens (w∗1 ...,2 Model,[0],[0]
w ∗ r ) and the sentence state that the parser stores in repaired (ŵ1 ... ŵm).,2 Model,[0],[0]
"When the edit distance after taking an action (d after) is smaller than before (d before), we regard the action as valid (Algorithm 2).
",2 Model,[0],[0]
"One serious concern of EREF is that the new actions may cause an infinite loop during parsing (e.g., infinite SUBSTITUTE, or an alternative DELETE and INSERT sequence.).",2 Model,[0],[0]
"To avoid this, we introduce two constraints: (1) edit flag and (2) edit limit.",2 Model,[0],[0]
"Edit flag is assigned for each token as a property, and a parser is not allowed to execute ActsER on a token if its flag is on.",2 Model,[0],[0]
The flag is turned on when a parser executes ActsER on a token whose flag is off.,2 Model,[0],[0]
"In INSERT action, the flag of the inserted token is activated, while the subsequent token (which gave rise to the INSERT) is not.",2 Model,[0],[0]
"Edit limit is set to be the number of tokens in a sentence, and the parser is not allowed to perform ActsER when the total number of execution of ActsER exceeds the limit.",2 Model,[0],[0]
These two constraints prevent the parser from falling into an infinite loop as well as parsing in the same order of time complexity as GE.,2 Model,[0],[0]
We also add the following constraints to avoid unreasonable derivations: (i) a word with a dependent cannot be deleted and (ii) any child words cannot be substituted.,2 Model,[0],[0]
All the constraints are implemented in the isLegal() function in Algorithm 1 (line 8).,2 Model,[0],[0]
We note that these constraints not only prevent undesirable derivations but also leads to an efficiency in exploring the search space during training.,2 Model,[0],[0]
"Data and Evaluation We evaluate EREF with respect to dependency parsing accuracy (Exp1) and grammaticality improvement (Exp2).1
1Code for the experiments is available at http:// github.com/keisks/error-repair-parsing
In the first experiment, as in GE, we train and evaluate our parser on the English dataset from the Penn Treebank (Marcus et al., 1993) with the Penn2Malt conversion program (Sections 2-21 for training, 22 for tuning, and 23 for test).",3 Experiment,[0],[0]
"We use the PTB for the dependency experiment, since there are no ungrammatical text corpora that has dependency annotation on the corrected texts by human.
",3 Experiment,[0],[0]
"We choose the following most frequent error types that are used in CoNLL 2013 shared task (Ng et al., 2013):
1.",3 Experiment,[0],[0]
"Determiner (substitution, deletion, insertion) 2.",3 Experiment,[0],[0]
"Preposition (substitution, deletion, insertion) 3.",3 Experiment,[0],[0]
Noun number (singular vs. plural) 4.,3 Experiment,[0],[0]
Verb form (tense and aspect) 5.,3 Experiment,[0],[0]
"Subject verb agreement
Regarding the candidate sets for INSERT and SUBSTITUTE actions, following Rozovskaya and Roth (2014), we focus on the most common candidates for each error type, setting the determiner candidates to be {a, an, the, φ (as deletion)}, preposition candidates to be {on, about, from, for, of, to, at, in, with, by, φ}, and verb forms to be {VB(P|Z|G|D|N)}.",3 Experiment,[0],[0]
"We build a 5-gram language model on English Gigaword with the KenLM Toolkit (Heafield, 2011) for EREF to select the best candidate.
",3 Experiment,[0],[0]
"We manually inject grammatical errors into PTB with certain error-rates similarly to the GenERRate toolkit by Foster and Andersen (2009), which is designed to create synthetic errors into sentences to improve grammatical error detection.
",3 Experiment,[0],[0]
"We train and tune EREF models with different token-level error injection rates from 5% (E05) to 20% (E20), because language learner corpora have generally around 5% to 15% of token level errors depending on learners’ proficiency (Leacock et al., 2014).",3 Experiment,[0],[0]
"Since the error injection is stochastic, we train each model with 10 runs and take an average of parser performance on the test set.
",3 Experiment,[0],[0]
"As a baseline, we use the original parser as described by GE, which is equivalent to EREF with training on an error-free corpus (E00).",3 Experiment,[0],[0]
"Since the EF baseline does not allow error correction during parsing, we pre-process the test data with a grammatical error correction system similar to Rozovskaya and Roth (2014), where a combination of classifiers for each error type corrects grammatical errors.
",3 Experiment,[0],[0]
"For evaluation, we jointly parse and correct grammatical errors in the test set with different
error injection rates (from 0% to 20%).",3 Experiment,[0],[0]
It is important to note that the number of tokens between the parser output and the oracle may be different because of error injection into the test set and ActsER during parsing.,3 Experiment,[0],[0]
"To handle this mismatch, we evaluate the dependency accuracy with alignment (Favre et al., 2010) in the spirit of SParseval (Roark et al., 2006), where tokens between a hypothesis and oracle are aligned prior to calculating the dependency accuracy.
",3 Experiment,[0],[0]
"In the second experiment, we use the Treebank of Learner English (TLE) (Berzak et al., 2016) to see the grammaticality improvement in a real scenario.",3 Experiment,[0],[0]
"TLE contains 5,124 sentences and 2.69 (std 1.9) token errors per sentence.",3 Experiment,[0],[0]
The average sentence length is 19.06 (std 9.47).,3 Experiment,[0],[0]
TLE also provides dependency labels and POS tags on the raw (ungrammatical) sentences.,3 Experiment,[0],[0]
"It is important to note that TLE has dependency annotation only for the original ungrammatical sentences, and therefore we do not compute the accuracy of dependency parse in this experiment.",3 Experiment,[0],[0]
"Since the corpus size is small, we train EREF (E05 to E20) on 100k sentences from Annotated Gigaword (Napoles et al., 2012) and used TLE as a test set.",3 Experiment,[0],[0]
Spelling errors are ignored because EREF can use the POS information.,3 Experiment,[0],[0]
"Grammaticality is evaluated by a regression model (Heilman et al., 2014), which scores grammaticality on the ordinal scale (from 1 to 4).
",3 Experiment,[0],[0]
"Results Table 1 shows the result of unlabeled dependency accuracy (UAS).2 As previously pre-
2Technically, it is possible to train the model with learning labels simultaneously (LAS), but there is a trade-off between
Successful cases I ’m looking forward to [-see-] {+seeing+} you next summer I ’ve never [-approve-] {+approved+} his deal There is not {+a+} possibility to travel Failure cases I ’ve [-assisted-] {+assisting+} your new musical show I am writing to complain",3 Experiment,[0],[0]
[-about-] {+with+} somethings,3 Experiment,[0],[0]
"I hope you liked {+the+} everything I said
Table 3: Successful and failure examples by EREF.",3 Experiment,[0],[0]
The edits are represented by [-deletion-] and {+insertion+}.,3 Experiment,[0],[0]
"Adjacent pairs of deletion and insertion are considered as substitution.
",3 Experiment,[0],[0]
"sented (Foster, 2007; Cahill, 2015), our experiment also shows that parser performance deteriorated as the error rate in the test corpus increased.",3 Experiment,[0],[0]
"On the error-free test set (0%), the baseline (EF pipeline) outperforms other EREF models; the accuracy is lower when the parser is trained on noisier data.",3 Experiment,[0],[0]
The difference among the models becomes small when the test set has 10% error injection rate.,3 Experiment,[0],[0]
"As the rate increases further, the trend of parser accuracy reverses.",3 Experiment,[0],[0]
"When the test set has 15% or higher noise, the E20 is the most accurate parser.",3 Experiment,[0],[0]
This trend is presented by the slope of deterioration ∇ = accuracy20%−accuracy0%20 in Table 1; a parser trained on noisier training data shows smaller decline and more robustness.3,3 Experiment,[0],[0]
"This indicates that the EREF is more robust than the vanilla EF on ungrammatical texts by jointly parsing and correcting errors.
",3 Experiment,[0],[0]
"Table 2 demonstrates the result of grammaticality improvement (1-4 scale) on the TLE corpus, and Table 3 shows successful and failure corrections by EREF.",3 Experiment,[0],[0]
Minimally trained models (E05 and E10) show little improvement in grammaticality because the models are too conservative to make edits.,3 Experiment,[0],[0]
The models with higher error-injection rates (E15 and E20) achieve 0.1 to 0.3 improvements that are statistically significant.,3 Experiment,[0],[0]
There is still room to improve regarding the amount of corrections.,3 Experiment,[0],[0]
"This is probably because TLE contains a variety of errors (e.g., collocation, punctuation) in addition to the five error types we focus.",3 Experiment,[0],[0]
"To deal with other error types, we can extend EREF by adding more actions, although it increases the search space.
",3 Experiment,[0],[0]
"From a practical perspective, the level of ungrammaticality should be realized ahead of time.",3 Experiment,[0],[0]
This is an issue to be addressed in future research.,3 Experiment,[0],[0]
search space and training time.,3 Experiment,[0],[0]
"The primary goal of this experiment is to see if the EREF is able to detect and correct grammatical errors.
",3 Experiment,[0],[0]
3Baseline model without preprocessing always underperformed the preprocessed baseline.,3 Experiment,[0],[0]
"Our work lies at the intersection of parsing noncanonical texts and grammatical error correction.
",4 Related Work,[0],[0]
"Joint dependency parsing and disfluency detection has been pursued (Rasooli and Tetreault, 2013, 2014; Honnibal and Johnson, 2014; Wu et al., 2015; Yoshikawa et al., 2016), where a parser jointly parses and detects disfluency (e.g., reparandum and interregnum) for a given speech utterance.",4 Related Work,[0],[0]
"Our work could be considered an extension via adding SUBSTITUTE and INSERT actions, although we depend on easy-first non-directional parsing framework instead of a left-to-right strategy.",4 Related Work,[0],[0]
"Importantly, the DELETE action is easier to handle than the SUBSTITUTE and INSERT actions, because they bring us challenging issues about a process of candidate word generation and avoiding an infinite loop in derivation.",4 Related Work,[0],[0]
"We have addressed these issues as explained in §2.
",4 Related Work,[0],[0]
"In terms of the literature from grammatical error correction, this work is closely related to Dahlmeier and Ng (2012), where they show an error correction decoder with the easy-first strategy.",4 Related Work,[0],[0]
The decoder iteratively corrects the most probable ungrammatical token by applying different classifiers for each error type.,4 Related Work,[0],[0]
"The EREF parser also depends on the easy-first strategy to find ungrammatical index to be deleted, inserted, or substituted, but it parses and corrects errors jointly whereas the decoder is designed as a grammatical error correction framework rather than a parser.
",4 Related Work,[0],[0]
"There is a line of work for parsing ungrammatical sentences (e.g., web forum) by adapting an existing parsing scheme on domain specific annotations (Petrov and McDonald, 2012; Cahill, 2015; Berzak et al., 2016; Nagata and Sakaguchi, 2016).",4 Related Work,[0],[0]
"Although we share an interest with respect to dealing with ungrammatical sentences, EREF focuses on the parsing scheme for repairing grammatical errors instead of adapting a parser with a domain specific annotation scheme.
",4 Related Work,[0],[0]
"More broadly, our work can also be regarded as one of the joint parsing and text normalization tasks such as joint spelling correction and POS tagging (Sakaguchi et al., 2012), word segmentation and POS tagging (Kaji and Kitsuregawa, 2014; Qian et al., 2015).",4 Related Work,[0],[0]
We have presented an error-repair variant of the non-directional easy-first dependency parser.,5 Conclusions,[0],[0]
"We
have introduced three new actions, SUBSTITUTE, INSERT, and DELETE into the parser so that it jointly parses and corrects grammatical errors in a sentence.",5 Conclusions,[0],[0]
"To address the issue of parsing incompletion due to the new actions, we have proposed simple constraints that keep track of editing history for each token and the total number of edits during derivation.",5 Conclusions,[0],[0]
The experimental result has demonstrated robustness of EREF parsers against EF and grammaticality improvement.,5 Conclusions,[0],[0]
Our work is positioned at the intersection of noisy text parsing and grammatical error correction.,5 Conclusions,[0],[0]
The EREF is a flexible formalism not only for grammatical error correction but other tasks with jointly editing and parsing a given sentence.,5 Conclusions,[0],[0]
"This work was supported in part by the JHU Human Language Technology Center of Excellence (HLTCOE), and DARPA LORELEI.",Acknowledgments,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes.,Acknowledgments,[0],[0]
The views and conclusions contained in this publication are those of the authors and should not be interpreted as representing official policies or endorsements of DARPA or the U.S. Government.,Acknowledgments,[0],[0]
"We propose a new dependency parsing scheme which jointly parses a sentence and repairs grammatical errors by extending the non-directional transitionbased formalism of Goldberg and Elhadad (2010) with three additional actions: SUBSTITUTE, DELETE, INSERT.",abstractText,[0],[0]
"Because these actions may cause an infinite loop in derivation, we also introduce simple constraints that ensure the parser termination.",abstractText,[0],[0]
"We evaluate our model with respect to dependency accuracy and grammaticality improvements for ungrammatical sentences, demonstrating the robustness and applicability of our scheme.",abstractText,[0],[0]
Error-repair Dependency Parsing for Ungrammatical Texts,title,[0],[0]
"Neural networks have achieved remarkable success in practical applications such as object recognition (He et al., 2016; Huang et al., 2017), machine translation (Bahdanau et al., 2015; Vinyals & Le, 2015), speech recognition (Hinton et al., 2012; Graves et al., 2013; Xiong et al., 2017) etc.",1. Introduction,[0],[0]
"Theoretical insights on why neural networks can be trained successfully despite their high-dimensional and non-convex loss functions are few or based on strong assumptions such as the eigenvalues of the Hessian at critical points being random (Dauphin et al., 2014), linear activations (Choromanska et al., 2014; Kawaguchi, 2016) or wide hidden layers (Soudry & Carmon, 2016; Nguyen & Hein, 2017).
",1. Introduction,[0],[0]
"In the current literature, minima of the loss function are typically depicted as points at the bottom of a strictly convex valley of a certain width that reflects the generalisation of the network, with network parameters given by the location of the minimum (Keskar et al., 2016).",1. Introduction,[0],[0]
"This is also the picture obtained when the loss function of neural networks is visualised in low dimension (Li et al., 2017).
",1. Introduction,[0],[0]
"In this work, we conjecture that neural network loss minima are not isolated points in parameter space, but essentially
1Heidelberg Collaboratory for Image Processing (HCI), IWR, Heidelberg University, D-69120 Heidelberg, Germany 2Institut für Theoretische Physik, Heidelberg University, D-69120 Heidelberg, Germany.",1. Introduction,[0],[0]
"Correspondence to: Fred A. Hamprecht <fred.hamprecht@iwr.uni-heidelberg.de>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
form a connected manifold.,1. Introduction,[0],[0]
"More precisely, we argue that the part of the parameter space where the loss remains below a certain low threshold forms one single connected component.
",1. Introduction,[0],[0]
We support the above claim by studying the energy landscape of several ResNets and DenseNets on CIFAR10 and CIFAR100:,1. Introduction,[0],[0]
"For random pairs of minima, we construct continuous paths through parameter space for which the loss remains very close to the value found directly at the minima.",1. Introduction,[0],[0]
"An example for such a path is shown in Figure 1.
",1. Introduction,[0],[0]
"Our main contribution is the finding of paths
1.",1. Introduction,[0],[0]
"that connect minima trained from different initialisations which are not related to each other via known loss-conserving operations like rescaling,
2.",1. Introduction,[0],[0]
"along which the training loss remains essentially at the same value as at the minima,
3. along which the test loss remains essentially constant while the test error rate slightly increases.
",1. Introduction,[0],[0]
"The existence of such paths suggests that modern neural networks have enough parameters such that they can achieve
good predictions while a big part of the network undergoes structural changes.",1. Introduction,[0],[0]
"In closing, we offer qualitative justification of this behaviour that may offer a handle for future theoretical investigation.",1. Introduction,[0],[0]
"In discussions about why neural networks generalise despite the extremely large number of parameters, one often finds the argument that wide minima generalise better (Keskar et al., 2016).",2. Related Work,[0],[0]
"This picture is confirmed when visualising the parameter space on a random plane around a minimum (Li et al., 2017).",2. Related Work,[0],[0]
"We draw a completely different image of the loss landscape: Minima are not located in finite-width valleys, but there are paths through the parameter space along which the loss remains very close to the value at the minima.",2. Related Work,[0],[0]
"A similar view had previously been conjectured by (Sagun et al., 2017).",2. Related Work,[0],[0]
They find flat linear paths between minima that are close in parameter space by construction.,2. Related Work,[0],[0]
"We extend their work by constructing flat paths between arbitrary minima.
",2. Related Work,[0],[0]
"It has previously been shown that minima of networks with ReLU activations are degenerate (Dinh et al., 2017): One can scale all parameters in one layer by a constant α and in following layer by α−1 without changing the output of the network.",2. Related Work,[0],[0]
"Here, we provide evidence for a different kind of degeneracy: We construct paths between independent minima that are essentially flat.
",2. Related Work,[0],[0]
"(Freeman & Bruna, 2016) showed that local minima are connected without large barriers for a CNN on MNIST and an RNN on PTB next word prediction.",2. Related Work,[0],[0]
"On CIFAR10 however, they found significant barriers between minima for the CNN considered.",2. Related Work,[0],[0]
"We extend their work in two ways: First, we consider ResNets and DenseNets that outperform plain CNNs by a large margin.",2. Related Work,[0],[0]
"Second, we apply a state of the art method for connecting minima from molecular statistical mechanics:",2. Related Work,[0],[0]
"The Automated Nudged Elastic Band (AutoNEB) algorithm (Kolsbjerg et al., 2016) which in turn is based on the Nudged Elastic Band (NEB) algorithm (Jónsson et al., 1998).",2. Related Work,[0],[0]
We additionally systematically replace paths that contain relatively high loss barriers.,2. Related Work,[0],[0]
"Combining the above we find paths with essentially no energy barrier.
",2. Related Work,[0],[0]
"NEB has so far been applied to a multi-layer perceptron with a single hidden layer (Ballard et al., 2016).",2. Related Work,[0],[0]
"High energy barriers between the minima of network were found when using three hidden neurons, and disappeared upon adding more neurons to the hidden layer.",2. Related Work,[0],[0]
"In follow-up work, (Ballard et al., 2017) trained a multi-layer perceptron with a single hidden layer on MNIST.",2. Related Work,[0],[0]
"They found that with l2-regularisation, the landscape had no significant energy barriers.",2. Related Work,[0],[0]
"However, for their network they report an error rate of 14.8% which is higher than the 12% achieved even by a
linear classifier (LeCun et al., 1998) and the 0.35% achieved with a standard CNN (Ciresan et al., 2011).
",2. Related Work,[0],[0]
"In this work, we apply AutoNEB to a nontrivial network for the first time, and make the surprising observation that different minima of state of the art networks on CIFAR10 and CIFAR100 are connected through essentially flat paths.
",2. Related Work,[0],[0]
"After submission of this work to the International Machine Learning Conference (ICML) 2018, (Garipov et al., 2018) independently reported that they also constructed paths between neural network minima.",2. Related Work,[0],[0]
They study the loss landscape of several architectures on CIFAR10 and CIFAR100 and report the same surprising observation: minima are connected by paths with constantly low loss.,2. Related Work,[0],[0]
"In the following, we use the terms energy and loss interchangeably.",3. Method,[0],[0]
"A neural network loss function depends on the architecture, the training set and the network parameters θ.",3.1. Minimum Energy Path,[0],[0]
"Keeping the former two fixed, we simply write L(θ) and start with two parameter sets θ1 and θ2.",3.1. Minimum Energy Path,[0],[0]
"In our case, they are minima of the loss function, i.e. they result from training the networks to convergence.",3.1. Minimum Energy Path,[0],[0]
"The goal is to find the continuous path p∗ from θ1 to θ2 through parameter space with the lowest maximum loss:
p(θ1, θ2) ∗",3.1. Minimum Energy Path,[0],[0]
"= arg min
p from θ1 to θ2
{ max θ∈p L(θ) } .
",3.1. Minimum Energy Path,[0],[0]
"For this optimisation to be tractable, the loss function must be sufficiently smooth, i.e. contain no jumps along the path.",3.1. Minimum Energy Path,[0],[0]
"The output and loss of neural networks are continuous functions of the parameters (Montúfar et al., 2014); only the derivative is discontinuous for the case of ReLU activations.",3.1. Minimum Energy Path,[0],[0]
"However, we cannot give any bounds on how steep the loss function may be.",3.1. Minimum Energy Path,[0],[0]
"We address this problem by sampling all paths very densely.
",3.1. Minimum Energy Path,[0],[0]
"Such a lowest path p∗ is called the minimum energy path (MEP) (Jónsson et al., 1998).",3.1. Minimum Energy Path,[0],[0]
"We refer to the parameter set with the maximum loss on a path as the “saddle point” of the path because it is a true saddle point of the loss function.
",3.1. Minimum Energy Path,[0],[0]
"In low-dimensional spaces, it is easy to construct the exact minimum energy path between two minima, for example by using dynamic programming on a densely sampled grid.
",3.1. Minimum Energy Path,[0],[0]
This is not possible for present day’s neural networks with parameter spaces that have millions of dimensions.,3.1. Minimum Energy Path,[0],[0]
We thus must resort to methods that construct an approximation of the MEP between two points using some local heuristics.,3.1. Minimum Energy Path,[0],[0]
"In particular, we resort to the Automated Nudged Elastic
Band (AutoNEB) algorithm (Kolsbjerg et al., 2016).",3.1. Minimum Energy Path,[0],[0]
"This method is based on the Nudged Elastic Band (NEB) algorithm (Jónsson et al., 1998).
",3.1. Minimum Energy Path,[0],[0]
NEB bends a straight line segment by applying gradient forces until there are no more gradients perpendicular to the path.,3.1. Minimum Energy Path,[0],[0]
"Then, as for the MEP, the highest point of the resulting path is a critical point.",3.1. Minimum Energy Path,[0],[0]
"While this critical point is not necessarily the saddle point we were looking for, it gives an upper bound for the energy at the saddle point.
",3.1. Minimum Energy Path,[0],[0]
"In the following, we present the mechanical model behind and the details of NEB.",3.1. Minimum Energy Path,[0],[0]
"We then proceed to AutoNEB.
",3.1. Minimum Energy Path,[0],[0]
"Mechanical Model A chain of N + 2 pivots (parameter sets) pi for i = 0, . . .",3.1. Minimum Energy Path,[0],[0]
", N + 1 is connected via springs of stiffness k. The initial and the final pivots are fixed to the minima to connect, i.e. p0 = θ1 and pN+1 = θ2.",3.1. Minimum Energy Path,[0],[0]
"Using gradient descent, the path that minimises the following energy function is found:
E(p) = N∑ i=1",3.1. Minimum Energy Path,[0],[0]
L(pi) + N∑ i=0 1 2 k ‖pi+1,3.1. Minimum Energy Path,[0],[0]
"− pi‖2 (1)
The problem with this energy formulation lies in the choice of the spring constant: If, on the one hand, k is too small, the distances between the pivots become larger in areas with high energy.",3.1. Minimum Energy Path,[0],[0]
"However, identifying the highest point on the path and its energy is the very goal of the algorithm, so the sampling rate should be high in the high-energy regions.",3.1. Minimum Energy Path,[0],[0]
"If, on the other hand, k is chosen too large, it becomes energetically advantageous to shorten and hence straighten the path as the spring energy grows quadratically with the total length of the path.",3.1. Minimum Energy Path,[0],[0]
"This cuts into corners of the loss surface and the resulting path can miss the saddle point.
",3.1. Minimum Energy Path,[0],[0]
"Nudged Elastic Band Inspired by the above model, (Jónsson et al., 1998) presented the Nudged Elastic Band (NEB).",3.1. Minimum Energy Path,[0],[0]
"For brevity, we directly present the improved version by (Henkelman & Jónsson, 2000).",3.1. Minimum Energy Path,[0],[0]
"The force resulting from Equation (1) consists of a force derived from the loss and a force originating from the springs:
Fi = −∇piE(p) = FLi + FSi For NEB, the physical forces are modified, or nudged, so that the loss force only acts perpendicularly to the path and the spring force only parallelly to the path (see also Figure 2):
FNEBi = F L i ∣∣ ⊥ + F S i ∣∣ ‖.
The direction of the path is defined by the local tangent τ̂i to the path.",3.1. Minimum Energy Path,[0],[0]
"The two forces now read:
FLi ∣∣ ⊥ = −(∇L(pi)− (∇L(pi) · τ̂i)τ̂i)
FSi ∣∣ ‖ =",3.1. Minimum Energy Path,[0],[0]
"(F S i · τ̂i)τ̂i
(2)
where the spring force opposes unequal distances along the path:
FSi = −k(‖pi − pi−1‖",3.1. Minimum Energy Path,[0],[0]
− ‖pi+1,3.1. Minimum Energy Path,[0],[0]
"− pi‖) (3)
In this formulation, high energy pivots no longer “slide down” from the saddle point.",3.1. Minimum Energy Path,[0],[0]
"The spring force only redistributes pivots on the path, but does not straighten it.",3.1. Minimum Energy Path,[0],[0]
"Pivots can be spaced unequally by introducing target distances or unequal spring constants into Equation (3).
",3.1. Minimum Energy Path,[0],[0]
"The local tangent is chosen to point in the direction of one of the adjacent pivots (N normalises to length one):
τ̂i",3.1. Minimum Energy Path,[0],[0]
= N { pi+1 − pi if L(pi+1) > L(pi−1) pi,3.1. Minimum Energy Path,[0],[0]
"− pi−1 else.
",3.1. Minimum Energy Path,[0],[0]
"This particular choice of τ̂ prevents kinks in the path and ensures a good approximation near the saddle point (Henkelman & Jónsson, 2000).
",3.1. Minimum Energy Path,[0],[0]
"The above procedure requires the following hyperparameters: The spring stiffness k and number of pivots N .
",3.1. Minimum Energy Path,[0],[0]
"(Sheppard et al., 2008) claim that a wide range of k leads to the same result on a given loss surface.",3.1. Minimum Energy Path,[0],[0]
"However, if chosen too large, the optimisation can become unstable.",3.1. Minimum Energy Path,[0],[0]
"If it is too small, an excessive number of iterations are needed before the pivots become equally distributed.",3.1. Minimum Energy Path,[0],[0]
"We did not find a value for k that worked well across different loss surfaces
and number of pivots N .",3.1. Minimum Energy Path,[0],[0]
"Instead, we re-distribute the pivots in each iteration t and set the actual spring force to zero.",3.1. Minimum Energy Path,[0],[0]
The loss force is still restricted to act parallelly to the path.,3.1. Minimum Energy Path,[0],[0]
"In the literature, this is sometimes referred to as the string method (Sheppard et al., 2008).
",3.1. Minimum Energy Path,[0],[0]
Algorithm 1 shows how the initial path is iteratively updated using the above forces.,3.1. Minimum Energy Path,[0],[0]
"As a companion, Figure 2 visualises the forces in one update step for a two dimensional example.",3.1. Minimum Energy Path,[0],[0]
"In this formulation, we use gradient descent to update the path.",3.1. Minimum Energy Path,[0],[0]
Any other gradient based optimiser can be used.,3.1. Minimum Energy Path,[0],[0]
"It typically introduces additional hyperparameters, for example a learning rate γ.",3.1. Minimum Energy Path,[0],[0]
"The number of iterations T should be chosen large enough for the optimisation to converge.
",3.1. Minimum Energy Path,[0],[0]
"Algorithm 1 NEB
Input: initial path p(0) with N + 2 pivots, p (0) 0",3.1. Minimum Energy Path,[0],[0]
= θ1 and p (0) N+1,3.1. Minimum Energy Path,[0],[0]
= θ2.,3.1. Minimum Energy Path,[0],[0]
"for t = 1, . . .",3.1. Minimum Energy Path,[0],[0]
", T do Redistribute pivots on path p(t−1) and store as p. for i = 1, . . .",3.1. Minimum Energy Path,[0],[0]
", N do
Compute projected loss force Fi = FLi ∣∣ ⊥.
Store pivot",3.1. Minimum Energy Path,[0],[0]
p(t)i = pi + γFi.,3.1. Minimum Energy Path,[0],[0]
"end for
end for return final path p(T )
",3.1. Minimum Energy Path,[0],[0]
The evaluation time of Algorithm 1 rises linearly with the number of iterations and the number of pivots on the path.,3.1. Minimum Energy Path,[0],[0]
"Computing the NEB forces can trivially be parallelised over the pivots.
",3.1. Minimum Energy Path,[0],[0]
The number of pivots N trades off between computational effort on the one hand and subsampling artefacts on the other hand.,3.1. Minimum Energy Path,[0],[0]
"In neural networks, it is not known what sampling density is needed for traversing the parameter space.",3.1. Minimum Energy Path,[0],[0]
"We use an adaptive procedure that inserts more pivots where needed:
AutoNEB",3.1. Minimum Energy Path,[0],[0]
"The Automated Nudged Elastic Band (AutoNEB, Algorithm 2) wraps the above NEB algorithm (Kolsbjerg et al., 2016).",3.1. Minimum Energy Path,[0],[0]
"It runs NEB only for a small number of iterations T at a time, initially with a small number of pivots N .",3.1. Minimum Energy Path,[0],[0]
It is then checked if the current pivots accurately sample the path.,3.1. Minimum Energy Path,[0],[0]
"If sampling is not dense enough, new pivots are added at locations where it is estimated that the path requires more accuracy, see Appendix A. This procedure is repeated several times.",3.1. Minimum Energy Path,[0],[0]
AutoNEB is not guaranteed to find the true MEP.,3.2. Local minimum energy paths,[0],[0]
"Instead, it can get stuck in local minimum energy paths (local MEPs) with spuriously high saddle point losses.",3.2. Local minimum energy paths,[0],[0]
"The good news is
Algorithm",3.2. Local minimum energy paths,[0],[0]
"2 AutoNEB Input: Minima to connect θ1, θ2.",3.2. Local minimum energy paths,[0],[0]
"Initialise N pivots equally spaced on line segment (θ1, θ2).",3.2. Local minimum energy paths,[0],[0]
"for t′ = 1, . . .",3.2. Local minimum energy paths,[0],[0]
", T ′",3.2. Local minimum energy paths,[0],[0]
"do
Optimise path using NEB (see Algorithm 1).",3.2. Local minimum energy paths,[0],[0]
Evaluate loss along NEB.,3.2. Local minimum energy paths,[0],[0]
"Insert pivots where residuum is large.
",3.2. Local minimum energy paths,[0],[0]
"end for return path after final iteration.
",3.2. Local minimum energy paths,[0],[0]
that the graph of minima and local MEPs has an ultrametric property: Suppose some local MEPs from a minimum A to B and from B to C are known.,3.2. Local minimum energy paths,[0],[0]
We call them pAB and pBC .,3.2. Local minimum energy paths,[0],[0]
"The respective saddle point energies give an upper bound for the true saddle point energies (marked with an asterisk):
L∗AB ≤",3.2. Local minimum energy paths,[0],[0]
"LAB = max θ∈pAB L(θ) L∗BC ≤ LBC = max θ∈pBC L(θ)
The concatenation of the two paths yields an upper bound for the true saddle point energy between A and C (ultrametric triangle inequality):
L∗AC ≤ max{LAB , LBC}
Proof.",3.2. Local minimum energy paths,[0],[0]
"Concatenating the paths pAB and pBC gives a new path pAC connecting A to C. The saddle point is located at the maximum loss along a path and hence the saddle point energy of pAC is LAC = max{LAB , LBC}.
",3.2. Local minimum energy paths,[0],[0]
"This has three consequences:
1.",3.2. Local minimum energy paths,[0],[0]
"As soon as the minima and computed local MEPs form one connected graph, upper bounds for all saddle energies are available.",3.2. Local minimum energy paths,[0],[0]
"We can hence very quickly get upper bounds for all pairs of minima by connecting one minimum to all others.
",3.2. Local minimum energy paths,[0],[0]
2.,3.2. Local minimum energy paths,[0],[0]
"When AutoNEB finds a bad local MEP, this can be addressed by computing paths between other pairs of minima.",3.2. Local minimum energy paths,[0],[0]
"As soon as a lower path is found by concatenating other paths, the bad local MEP can be removed.",3.2. Local minimum energy paths,[0],[0]
"This means that the bad local paths can easily be corrected for.
3.",3.2. Local minimum energy paths,[0],[0]
"When we evaluate the saddle point energies of a set of computed local MEPs, we can ignore paths with higher energy than the concatenation of paths with a lower maximal energy.",3.2. Local minimum energy paths,[0],[0]
"These lowest local MEPs form a minimum spanning tree in the available graph (Gower & Ross, 1969).",3.2. Local minimum energy paths,[0],[0]
"A
Minimum Spanning Tree (MST) can be found efficiently, e.g. using Kruskal’s algorithm.
",3.2. Local minimum energy paths,[0],[0]
"We resort to a heuristic (Figure 3, Algorithm 3) to systematically sample edge costs from a latent graph to find or approximate its MST.",3.2. Local minimum energy paths,[0],[0]
"Since running AutoNEB is computationally expensive (comparable to training the corresponding network once), we stop the iteration when the lightened spanning tree found so far contains only similar saddle point energies.",3.2. Local minimum energy paths,[0],[0]
"We connect minima of different CNNs, ResNets (He et al., 2016) and DenseNets (Huang et al., 2017) on the image classification tasks CIFAR10 and CIFAR100 (Krizhevsky & Hinton, 2009) using AutoNEB.",4. Experiments,[0],[0]
"Per architecture, we consider ten minima.1
The minima are constructed from multiple random initialisations and are truly distinct: On the test data, the set of misclassified images differs between the minima.",4. Experiments,[0],[0]
"More precisely, on the ResNet and DenseNet architectures, we
1Source code is available at https://github.com/ fdraxler/PyTorch-AutoNEB.
",4. Experiments,[0],[0]
Algorithm 3 Energy Landscape Exploration Input: set of minima θi.,4. Experiments,[0],[0]
"Connect θ1 to all θi, i 6= 1, yielding a spanning tree.",4. Experiments,[0],[0]
"repeat
Remove edge po with highest loss from spanning tree.",4. Experiments,[0],[0]
"From each resulting tree, try to select one minimum,
so that no local MEP is known for the pair.",4. Experiments,[0],[0]
"if search failed then
Re-insert po and ignore it when searching for the highest edge in the future.
else Compute new path pn using AutoNEB.",4. Experiments,[0],[0]
"if Lpn < Lpo then
Add pn to the tree, making tree “lighter”.",4. Experiments,[0],[0]
"else
Re-insert po to the tree (no better path was found).",4. Experiments,[0],[0]
"end if
end if until one local MEP is known for each pair of minima
or computational budget is exceeded.",4. Experiments,[0],[0]
"return saddle points in minimum spanning tree.
",4. Experiments,[0],[0]
"observe a maximum 70% overlap of the samples that are misclassified at two minima, proving their distinctiveness.
",4. Experiments,[0],[0]
We report the average cross-entropy loss and misclassification rates over the full training and test data for the minima found.,4. Experiments,[0],[0]
"For the final evaluation, we reduce the saddle points to the minimum spanning tree with the saddle training loss as weight.",4. Experiments,[0],[0]
"The set-up is identical for all network architectures, except for the batch sizes which we note in each case.
",4.1. AutoNEB schedule,[0],[0]
The minimum pairs to connect are ordered by Algorithm 3.,4.1. AutoNEB schedule,[0],[0]
"For each minimum pair, AutoNEB (see Algorithm 2) is run for a total of 14 cycles of NEB.",4.1. AutoNEB schedule,[0],[0]
"The loss is evaluated for each pivot on a random batch.
",4.1. AutoNEB schedule,[0],[0]
"After each cycle, new pivots are inserted at positions where the loss exceeds the energy estimated by linear interpolation between pivots by at least 20% compared to the total energy difference along the path.",4.1. AutoNEB schedule,[0],[0]
Comparing to the total loss difference prioritises big errors which is beneficial as each additional pivot implies one more loss evaluations per iteration.,4.1. AutoNEB schedule,[0],[0]
"The energy is evaluated on nine points between each pair of neighbouring pivots.
",4.1. AutoNEB schedule,[0],[0]
"As optimiser, we use SGD with momentum 0.9 and l2regularisation with λ = 0.0001.
",4.1. AutoNEB schedule,[0],[0]
"The NEB cycles are configured with a learning rate decay:
1.",4.1. AutoNEB schedule,[0],[0]
"Four cycles of 1000 steps each with learning rate 0.1.
2.",4.1. AutoNEB schedule,[0],[0]
Two cycles with 2000 steps and learning rate 0.1.,4.1. AutoNEB schedule,[0],[0]
"The number of steps was increased as it did not prove necessary inserting new pivots after 1000 steps.
3.",4.1. AutoNEB schedule,[0],[0]
Four cycles of 1000 steps with learning rate 0.01.,4.1. AutoNEB schedule,[0],[0]
"The loss drops significantly in this phase.
4.",4.1. AutoNEB schedule,[0],[0]
"No big improvement was seen in the last four cycles of 1000 steps each with a learning rate of 0.001.
",4.1. AutoNEB schedule,[0],[0]
Figure 4 shows typical snapshots of the loss-along-path between the above cycles.,4.1. AutoNEB schedule,[0],[0]
"We consider a wide range of architectures, from shallow CNNs to recent deep networks with skip connections.
",4.2. Architectures,[0],[0]
Basic CNN We analyse CNNs without skip connections with a variety of depths and widths on both CIFAR10 and CIFAR100.,4.2. Architectures,[0],[0]
We name them “CNN-W×D” where W corresponds to the width of each layer (number of channels) and D to the number of convolutional layers.,4.2. Architectures,[0],[0]
"Each convolution is 5× 5, a max pooling layer of 2 is attached to each convolution, and a single hidden fully connected layer of width 256 and batch normalisation (Ioffe & Szegedy, 2015) are used.",4.2. Architectures,[0],[0]
"We consider the one-layer CNN-12×1, CNN-24×1, CNN-36×1, CNN-48×1 and CNN-96×1, and the multi-layer CNN-48×2 and CNN-48×3.
",4.2. Architectures,[0],[0]
"ResNet We train ResNets on both CIFAR10 and CIFAR100 (ResNet-20, -32, -44 and -56) following the training procedure in (He et al., 2016).",4.2. Architectures,[0],[0]
"For ResNet-20 and ResNet-32, the best local MEPs were found using a batch size of 512 training samples.",4.2. Architectures,[0],[0]
"For ResNet-44 and ResNet-56, this number was decreased to 256.
",4.2. Architectures,[0],[0]
"DenseNet We train a DenseNet-40-12 and a DenseNet100-12-BC on both CIFAR10 and CIFAR100 following the
training procedure in (Huang et al., 2017).",4.2. Architectures,[0],[0]
The AutoNEB batch size was set to 256.,4.2. Architectures,[0],[0]
"The saddle point losses for both training and test sets found by AutoNEB are shown in Figure 5, and listed in detail in Table B.1 in Appendix B. They are small for the shallow networks and almost negligible for the deep residual networks.
",4.3. Saddle point losses,[0],[0]
Compare the saddle point loss to the loss at the minima on the training and on the test set.,4.3. Saddle point losses,[0],[0]
"For the shallow CNNs on the one hand, the saddle loss is found quite close to the test loss.",4.3. Saddle point losses,[0],[0]
"On the other hand, the saddle loss of the ResNets and DenseNets lies very close to the training loss.
",4.3. Saddle point losses,[0],[0]
"Further, we measure how late during training the learning curve crosses the saddle loss, as visualised in Figure 6.",4.3. Saddle point losses,[0],[0]
The learning curve falls below the saddle point energy only after the first learning rate decay and an additional significant drop of the loss for all architectures.,4.3. Saddle point losses,[0],[0]
"For the wider CNNs on CIFAR10 and the majority of ResNets and DenseNets, the losses meet even after the second decay, i.e. in the final phase of learning.
",4.3. Saddle point losses,[0],[0]
"We observe the following trend: The deeper and wider an architecture, the lower are the saddles between minima until they essentially vanish for current-day deep architectures.",4.3. Saddle point losses,[0],[0]
"The more complex dataset CIFAR100 raises the barriers.
",4.3. Saddle point losses,[0],[0]
"At the same time, the test accuracy is preserved: The classification error only increases slightly by maximally 0.5% (2.2%) for all deep architectures on CIFAR10 (CIFAR100) compared to the minima.
",4.3. Saddle point losses,[0],[0]
We conclude that the saddle points have surprisingly low loss with respect to the metrics above.,4.3. Saddle point losses,[0],[0]
"In other words, there are essentially no loss barriers in current-day deep architectures.",4.3. Saddle point losses,[0],[0]
"The local MEPs between the minima not only have very low loss, they also follow simple trajectories.",4.4. Properties of obtained local MEPs,[0],[0]
Figure 7 shows some coordinates of two local MEPs in a parallel coordinate plot.,4.4. Properties of obtained local MEPs,[0],[0]
We find that each coordinate has a smooth path.,4.4. Properties of obtained local MEPs,[0],[0]
The largest deviations occur near the saddle point of the path.,4.4. Properties of obtained local MEPs,[0],[0]
The paths are between 50% to 2.5 times longer than the direct connection between the minima.,4.4. Properties of obtained local MEPs,[0],[0]
"We have pointed out an intriguing property of the loss surface of current-day deep networks, by upper-bounding the saddle points between the parameter sets that result from stochastic gradient descent, a.k.a. “minima”.",5. Discussion,[0],[0]
"These empiri-
cal upper bounds are astonishingly close to the loss at the minima themselves.",5. Discussion,[0],[0]
The experiments on the CNNs suggest that the disappearance of barriers emerges as the networks get wider and especially deeper.,5. Discussion,[0],[0]
"At this point, we cannot give a formal characterization of the regime in which this finding holds.",5. Discussion,[0],[0]
"A formal proof is also complicated by the fact that the loss surface is a function not only of the parameters and the architecture, but also of the training set; and the distribution of real-world structured data such as images or sentences does not lend itself to a compact mathematical representation.",5. Discussion,[0],[0]
"That said, we want to make two related arguments that may help explain why we observe no substantial barrier between minima.",5. Discussion,[0],[0]
"State of the art neural networks have dozens or hundreds of neurons / channels per layer, and skip connections between non-adjacent layers.",5.1. Resilience,[0],[0]
"Assume that by training, a parameter set with low loss has been identified.",5.1. Resilience,[0],[0]
"Now if we perturb a single parameter, say by adding a small constant, but leave the others free to adapt to this change to still minimise the loss, it may be argued that by adjusting somewhat, the myriad other parameters can “make up” for the change imposed on only one of them.",5.1. Resilience,[0],[0]
"After this relaxation, the procedure and argument can be repeated, though possibly with the perturbation of a different parameter.
",5.1. Resilience,[0],[0]
"This type of resilience is exploited and encouraged by procedures such as Dropout (Srivastava et al., 2014) or ensembling (Hansen & Salamon, 1990).",5.1. Resilience,[0],[0]
"It is also the reason why neural networks can be greatly condensed before a substantial increase in loss occurs (Liu et al., 2017).",5.1. Resilience,[0],[0]
"Ali ce
Bo b
Minimum A Ali
ce Bob
Ch arl
ie Ali
ce Bob
Minimum B
Ali ce
Bo b
Lowest saddle has 25% error.
",5.2. Redundancy,[0],[0]
"Extra neuron “unlocks“ flat path.
",5.2. Redundancy,[0],[0]
Figure 8.,5.2. Redundancy,[0],[0]
Network capacity for XOR dataset: The continuous transition from one minimum (left) to another minimum (right) is not possible without misclassifying at least one instance (upper middle).,5.2. Redundancy,[0],[0]
(Lower middle),5.2. Redundancy,[0],[0]
"Adding one helper neuron makes the transition possible while always predicting the right class for all data points, i.e. by turning off the outgoing weight of Bob.
Consider the textbook example of a two-layer perceptron that can fit the XOR problem.",5.2. Redundancy,[0],[0]
The two neurons traditionally used in the first hidden layer – let’s call them Alice and Bob – are shown in Figure 8 on the left.,5.2. Redundancy,[0],[0]
"We can obtain an equivalent network by exchanging Alice and Bob (and permuting the weights of the neuron in the second hidden layer, not shown).",5.2. Redundancy,[0],[0]
"This network, also corresponding to a minimum of the loss surface, is shown in Figure 8 on the right.",5.2. Redundancy,[0],[0]
"Now, any path between these two minima will entail parameter sets such as the one in the upper centre of Figure 8 that incur high loss.
",5.2. Redundancy,[0],[0]
"If, on the other hand, we introduce an auxiliary neuron, Charlie, we can play a small choreography:",5.2. Redundancy,[0],[0]
Enter Charlie.,5.2. Redundancy,[0],[0]
Charlie stands in for Bob.,5.2. Redundancy,[0],[0]
"Bob transitions to Alice’s role via Figure 8, lower centre.",5.2. Redundancy,[0],[0]
Alice takes over from Charlie.,5.2. Redundancy,[0],[0]
Exit Charlie.,5.2. Redundancy,[0],[0]
"If the neuron in the second hidden layer adjusts its weights so as to disregard the output from the neuron-intransition, the entire network incurs no higher loss than at the two original minima.
",5.2. Redundancy,[0],[0]
We have constructed a perfect minimum energy path through increasing the width.,5.2. Redundancy,[0],[0]
"Similarly, it is possible to construct a zero-loss path by adding a second two-neuron layer to the network, that is by increasing the depth of the network.",5.2. Redundancy,[0],[0]
We find that the loss surface of deep neural networks contains paths with constantly low loss.,6. Conclusion,[0],[0]
The paths connect the minima so that they form one single connected component in the loss landscape.,6. Conclusion,[0],[0]
"The barriers are especially low with increasing depth and width.
",6. Conclusion,[0],[0]
We put forth two closely related explanations in the above.,6. Conclusion,[0],[0]
"Both hold only if the network has some extra capacity, or degrees of freedom, to spare.",6. Conclusion,[0],[0]
"Empirically, this seems to be the case for modern-day architectures applied to standard problems.
",6. Conclusion,[0],[0]
"This has the profound implication that low Hessian eigenvalues exist apart from the eigenvectors with analytically zero eigenvalues due to scaling.
",6. Conclusion,[0],[0]
We introduce AutoNEB for the characterisation of currentday architectures for the first time.,6. Conclusion,[0],[0]
The method opens the door to further empirical research on the energy landscape of neural networks.,6. Conclusion,[0],[0]
"When the hyperparameters of AutoNEB are further refined, we expect to find even lower paths up to the level where the true saddle points are recovered.",6. Conclusion,[0],[0]
It is then interesting to see if certain minima have a higher barrier between them than others.,6. Conclusion,[0],[0]
"This makes it possible to recursively form clusters of minima, i.e. using single-linkage clustering.",6. Conclusion,[0],[0]
"In the traditional energy landscape literature, this kind of clustering is summarised in disconnectivity graphs (Wales et al., 1998) which can help visualise very high-dimensional surfaces.
",6. Conclusion,[0],[0]
"On the practical side, we envisage using the resulting paths as a large ensemble of neural networks (Garipov et al., 2018), especially given that we observe marginally lower test loss along the path.
",6. Conclusion,[0],[0]
"More importantly, we hope these observations will stimulate new theoretical work to better understand the nature of the loss surface, and why local optimisation on such surfaces results in networks that generalize so well.",6. Conclusion,[0],[0]
FAH gratefully acknowledges support by DFG under grant no.,Acknowledgements,[0],[0]
HA 4364/9-1.,Acknowledgements,[0],[0]
Training neural networks involves finding minima of a high-dimensional non-convex loss function.,abstractText,[0],[0]
"Relaxing from linear interpolations, we construct continuous paths between minima of recent neural network architectures on CIFAR10 and CIFAR100.",abstractText,[0],[0]
"Surprisingly, the paths are essentially flat in both the training and test landscapes.",abstractText,[0],[0]
"This implies that minima are perhaps best seen as points on a single connected manifold of low loss, rather than as the bottoms of distinct valleys.",abstractText,[0],[0]
Essentially No Barriers in Neural Network Energy Landscape,title,[0],[0]
Making predictions about causal effects of actions is a central problem in many domains.,1. Introduction,[0],[0]
"For example, a doctor deciding which medication will cause better outcomes for a patient; a government deciding who would benefit most from subsidized job training; or a teacher deciding which study program would most benefit a specific student.",1. Introduction,[0],[0]
In this paper we focus on the problem of making these predictions based on observational data.,1. Introduction,[0],[0]
"Observational data is
*Equal contribution 1CIMS, New York University, New York, NY 10003 2IMES, MIT, Cambridge, MA 02142 3CSAIL, MIT, Cambridge, MA 02139.",1. Introduction,[0],[0]
"Correspondence to: Uri Shalit <shalit@cs.nyu.edu>, Fredrik D. Johansson <fredrikj@mit.edu>, David Sontag <dsontag@csail.mit.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
data which contains past actions, their outcomes, and possibly more context, but without direct access to the mechanism which gave rise to the action.",1. Introduction,[0],[0]
"For example we might have access to records of patients (context), their medications (actions), and outcomes, but we do not have complete knowledge of why a specific action was applied to a patient.
",1. Introduction,[0],[0]
"The hallmark of learning from observational data is that the actions observed in the data depend on variables which might also affect the outcome, resulting in confounding: For example, richer patients might better afford certain medications, and job training might only be given to those motivated enough to seek it.",1. Introduction,[0],[0]
The challenge is how to untangle these confounding factors and make valid predictions.,1. Introduction,[0],[0]
"Specifically, we work under the common simplifying assumption of “no-hidden confounding”, assuming that all the factors determining which actions were taken are observed.",1. Introduction,[0],[0]
"In the examples above, it would mean that we have measured a patient’s wealth or an employee’s motivation.
",1. Introduction,[0],[0]
"As a learning problem, estimating causal effects from observational data is different from classic learning in that in our training data we never see the individual-level effect.",1. Introduction,[0],[0]
"For each unit, we only see their response to one of the possible actions - the one they had actually received.",1. Introduction,[0],[0]
"This is close to what is known in the machine learning literature as “learning from logged bandit feedback” (Strehl et al., 2010; Swaminathan & Joachims, 2015), with the distinction that we do not have access to the model generating the action.
",1. Introduction,[0],[0]
"Our work differs from much work in causal inference in that we focus on the individual-level causal effect (“cspecific treatment effects” Shpitser & Pearl (2006); Pearl (2015)), rather than the average or population level.",1. Introduction,[0],[0]
"Our main contribution is to give what is, to the best of our knowledge, the first generalization-error1 bound for estimating individual-level causal effect, where each individual is identified by its features x.",1. Introduction,[0],[0]
"The bound leads naturally to a new family of representation-learning based algorithms (Bengio et al., 2013), which we show to match or outperform state-of-the-art methods on several causal effect inference tasks.
",1. Introduction,[0],[0]
"1Our use of the term generalization is different from its use in the study of transportability, where the goal is to generalize causal conclusion across distributions (Bareinboim & Pearl, 2016).
",1. Introduction,[0],[0]
"We frame our results using the Neyman-Rubin potential outcomes framework (Rubin, 2011), as follows.",1. Introduction,[0],[0]
"We assume that for a unit with features x ∈ X , and an action (also known as treatment or intervention) t ∈ {0, 1}, there are two potential outcomes: Y0 and Y1.",1. Introduction,[0],[0]
"For each unit we only observe one of the potential outcomes, according to treatment assignment: if t = 0",1. Introduction,[0],[0]
"we observe y = Y0, if t = 1, we observe y = Y1; this is known as the consistency assumption.",1. Introduction,[0],[0]
"For example, x can denote the set of lab tests and demographic factors of a diabetic patient, t = 0 denote the standard medication for controlling blood sugar, t = 1 denotes a new medication, and Y0 and Y1 indicate the patient’s blood sugar level if they were to be given medications t = 0 and t = 1, respectively.
",1. Introduction,[0],[0]
We will denote m1(x) = E,1. Introduction,[0],[0]
"[Y1|x], m0(x)",1. Introduction,[0],[0]
= E,1. Introduction,[0],[0]
[Y0|x].,1. Introduction,[0],[0]
We are interested in learning the function τ(x),1. Introduction,[0],[0]
":= E [Y1 − Y0|x] = m1(x) − m0(x). τ(x) is the expected treatment effect of t = 1 relative to t = 0 on a unit with characteristics x, or the Individual Treatment Effect (ITE)2.",1. Introduction,[0],[0]
"Our goal is to find an estimate τ̂ of τ such that some loss function, e.g. E [ (τ̂ − τ)2 ] , is small.",1. Introduction,[0],[0]
"For example, for a
patient with features x, we attempt to predict which of two treatments will have a better outcome.",1. Introduction,[0],[0]
"The fundamental problem of causal inference is that for any x in our data we only observe Y1 or Y0, but never both.
",1. Introduction,[0],[0]
"As mentioned above, we make an important “no-hidden confounders” assumption, in order to make the conditional causal effect identifiable.",1. Introduction,[0],[0]
"We formalize this assumption by using the standard strong ignorability condition: (Y1, Y0) ⊥ t|x, and 0 < p(t = 1|x)",1. Introduction,[0],[0]
"< 1 for all x. Strong ignorability is a sufficient condition for the ITE function τ(x) to be identifiable (Imbens & Wooldridge, 2009; Pearl, 2015; Rolling, 2014): see proof in the supplement.",1. Introduction,[0],[0]
"The validity of strong ignorability cannot be assessed from data, and must be determined by domain knowledge and understanding of the causal relationships between the variables.
",1. Introduction,[0],[0]
"One approach to the problem of estimating the function τ(x) is by learning the two functions m0(x) and m1(x) using samples from p(Yt|x, t).",1. Introduction,[0],[0]
This is similar to a standard machine learning problem of learning from finite samples.,1. Introduction,[0],[0]
"However, there is an additional source of variance at work here: For example, if mostly rich patients received treatment t = 1, and mostly poor patients received treatment t = 0, we might have an unreliable estimation of m1(x) for poor patients.",1. Introduction,[0],[0]
"In this paper we upper bound this additional source of variance using an Integral Probability Metric (IPM) measure of distance between two distributions p(x|t = 0), and p(x|t = 1), also known as the control and treated distributions.",1. Introduction,[0],[0]
"In practice we use two specific IPMs: the Maximum Mean Discrepancy (Gretton
2Also known as Conditional Average Treatment Effect, CATE.
et al., 2012), and the Wasserstein distance (Villani, 2008; Cuturi & Doucet, 2014).",1. Introduction,[0],[0]
"We show that the expected error in learning the individual treatment effect function τ(x) is upper bounded by the error of learning Y1 and Y0, plus the IPM term.",1. Introduction,[0],[0]
"In the randomized controlled trial setting, where t ⊥ x, the IPM term is 0, and our bound naturally reduces to a standard learning problem of learning two functions.
",1. Introduction,[0],[0]
"The bound we derive points the way to a family of algorithms based on the idea of representation learning (Bengio et al., 2013): Jointly learn hypotheses for both treated and control on top of a representation which minimizes a weighted sum of the factual loss (the standard supervised machine learning objective), and the IPM distance between the control and treated distributions induced by the representation.",1. Introduction,[0],[0]
This can be viewed as learning the functions m0 and m1 under a constraint that encourages better generalization across the treated and control populations.,1. Introduction,[0],[0]
"In the Experiments section we apply algorithms based on neural nets as representations and hypotheses, along with MMD or Wasserstein distributional distances over the representation layer; see Figure 1 for the basic architecture.
",1. Introduction,[0],[0]
"In his foundational text on causality, Pearl (2009) writes: “Whereas in traditional learning tasks we attempt to generalize from one set of instances to another, the causal modeling task is to generalize from behavior under one set of conditions to [...] another set.",1. Introduction,[0],[0]
Causal models should therefore be chosen by a criterion that challenges their stability against changing conditions”,1. Introduction,[0],[0]
[emphasis ours].,1. Introduction,[0],[0]
"We believe our work points the way to one such stability criterion, for causal inference in the strongly ignorable case.",1. Introduction,[0],[0]
"Much recent work in machine learning for causal inference focuses on causal discovery, with the goal of discovering the underlying causal graph or causal direction from data (Hoyer et al., 2009; Maathuis et al., 2010; Triantafillou & Tsamardinos, 2015; Mooij et al., 2016).",2. Related work,[0],[0]
"We focus on the case when the causal setup is simple and known to be of the form (Y1, Y0) ⊥ x|t, with no hidden confounders.",2. Related work,[0],[0]
"Under the causal model we assume, the most common goal of causal effect inference as used in the ap-
plied sciences is to obtain the average treatment effect: ATE = Ex∼p(x)",2. Related work,[0],[0]
[τ(x)].,2. Related work,[0],[0]
We will briefly discuss how some standard statistical causal effect inference methods relate to our proposed method.,2. Related work,[0],[0]
"Note that most of these approaches assume some form of ignorability.
",2. Related work,[0],[0]
"One of the most widely used approaches to estimating ATE is covariate (or back-door) adjustment, also known as the G-computation formula (Robins, 1986; Pearl, 2009).",2. Related work,[0],[0]
"In their basic version, covariate adjustment methods aim to estimate the functions m1(x), m0(x), and are therefore natural candidates for estimating ITE as well as ATE, using the estimates of mt(x).",2. Related work,[0],[0]
"Most previous work on this subject focused on asymptotic consistency (Belloni et al., 2014; Athey et al., 2016; Chernozhukov et al., 2016), and so far there has not been much work on the generalizationerror of such a procedure.",2. Related work,[0],[0]
One view of our results is that we point out a previously unaccounted for source of variance when using covariate adjustment to estimate ITE.,2. Related work,[0],[0]
"We suggest a new type of regularization, by learning representations with reduced IPM distance between treated and control, enabling a new type of bias-variance trade-off.
",2. Related work,[0],[0]
Another widely used family of statistical methods used in causal effect inference are weighting methods.,2. Related work,[0],[0]
"Methods such as inverse propensity score weighting (Austin, 2011) re-weight the units in the observational data so as to make the treated and control populations more comparable, and have been used for estimating conditional effects as well (Cole et al., 2003).",2. Related work,[0],[0]
"The major challenge, especially in high-dimensional cases, is controlling the variance of the estimates (Swaminathan & Joachims, 2015).",2. Related work,[0],[0]
"Doubly robust methods go further and combine propensity score reweighting and covariate adjustment in clever ways to reduce model bias (Funk et al., 2011).
",2. Related work,[0],[0]
"Adapting machine learning methods for causal effect inference, and in particular for individual level treatment effect, has gained much interest recently.",2. Related work,[0],[0]
For example Wager & Athey (2015); Athey & Imbens (2016) discuss how treebased methods can be adapted to obtain a consistent estimator with semi-parametric asymptotic convergence rate.,2. Related work,[0],[0]
"Recent work has also looked into how machine learning methods can help detect heterogeneous treatment effects when some data from randomized experiments is available (Taddy et al., 2016; Peysakhovich & Lada, 2016).",2. Related work,[0],[0]
"Neural nets have also been used for this purpose, exemplified in early work by Beck et al. (2000), and more recently by Hartford et al. (2016)’s work on deep instrumental variables.",2. Related work,[0],[0]
"Our work differs from all the above by focusing on the generalization-error aspects of estimating individual treatment effect, as opposed to asymptotic consistency, and by focusing solely on the observational study case, with no randomized components or instrumental variables.
",2. Related work,[0],[0]
"Our work has strong connections with work on domain
adaptation.",2. Related work,[0],[0]
"In particular, estimating ITE requires prediction of outcomes over a different distribution from the observed one.",2. Related work,[0],[0]
Our ITE error upper bound has similarities with generalization bounds in domain adaptation given by BenDavid et al. (2007); Mansour et al. (2009); Ben-David et al. (2010); Cortes & Mohri (2014).,2. Related work,[0],[0]
"These bounds employ distribution distance metrics such as the A-distance or the discrepancy metric, which are related to the IPM distance we use.",2. Related work,[0],[0]
"Our algorithm is similar to a recent algorithm for domain adaptation by Ganin et al. (2016), and in principle other domain adaptation methods (e.g. Daumé III (2007); Pan et al. (2011); Sun et al. (2016)) could be adapted for use in ITE estimation as presented here.
",2. Related work,[0],[0]
"Finally, our paper builds on Johansson et al. (2016), where we showed a connection between covariate shift and the task of estimating counterfactuals.",2. Related work,[0],[0]
"We proposed learning a representation of the data that makes the treated and control distributions more similar, fitting a linear ridge-regression model on top of it.",2. Related work,[0],[0]
We bounded the relative error of fitting a ridge-regression using the distribution with reverse treatment assignment versus fitting a ridge-regression using the factual distribution.,2. Related work,[0],[0]
"Unfortunately, the relative error bound is not at all informative regarding the absolute quality of the representation.",2. Related work,[0],[0]
"In this paper we focus on a related but more substantive task: estimating the individual treatment effect, building on the counterfactual error term.",2. Related work,[0],[0]
We provide an informative bound on the absolute quality of the representation.,2. Related work,[0],[0]
"We also derive a much more flexible family of algorithms, including non-linear hypotheses and much more powerful distribution metrics in the form of IPMs such as the Wasserstein and MMD distances.",2. Related work,[0],[0]
"Finally, we conduct significantly more thorough experiments including a real-world dataset and out-of-sample performance, and show our methods outperform previously proposed ones.",2. Related work,[0],[0]
"In this section we prove a bound on the expected error in estimating the individual treatment effect for a given representation, and a hypothesis defined over that representation.",3. Estimating ITE: Error bounds,[0],[0]
"The bound is expressed in terms of (1) the expected loss of the model when learning the observed outcomes y as a function of x and t, denoted F , F standing for “Factual”; (2) an Integral Probability Metric (IPM) distance between the distribution of treated and control units.",3. Estimating ITE: Error bounds,[0],[0]
"The term F is the classic machine learning generalization-error, and in turn can be upper bounded using the empirical error and model complexity terms, applying standard machine learning theory (Shalev-Shwartz & Ben-David, 2014).",3. Estimating ITE: Error bounds,[0],[0]
We will employ the following assumptions and notations.,3.1. Problem setup,[0],[0]
"The most important notations are in the Notation box in the
supplement.",3.1. Problem setup,[0],[0]
The space of covariates is a bounded subset X ⊂ Rd.,3.1. Problem setup,[0],[0]
The outcome space is Y ⊂ R. Treatment t is a binary variable.,3.1. Problem setup,[0],[0]
"We assume there exists a joint distribution p(x, t, Y0, Y1), such that (Y1, Y0) ⊥ t|x and 0 < p(t = 1|x)",3.1. Problem setup,[0],[0]
< 1 for all x ∈ X (strong ignorability).,3.1. Problem setup,[0],[0]
"The treated and control distributions are the distribution of the features x conditioned on treatment: pt=1(x) := p(x|t = 1), and pt=0(x) := p(x|t = 0), respectively.
",3.1. Problem setup,[0],[0]
"Throughout this paper we will discuss representation functions of the form Φ : X → R, where R is the representation space.",3.1. Problem setup,[0],[0]
"We make the following assumption about Φ:
Assumption 1.",3.1. Problem setup,[0],[0]
"The representation Φ is a twicedifferentiable, one-to-one function.",3.1. Problem setup,[0],[0]
"Without loss of generality we will assume that R is the image of X under Φ. We then have Ψ : R → X as the inverse of Φ, such that Ψ(Φ(x))",3.1. Problem setup,[0],[0]
"= x for all x ∈ X .
",3.1. Problem setup,[0],[0]
"The representation Φ pushes forward the treated and control distributions into the new space R; we denote the induced distribution by pΦ.
Definition 1.",3.1. Problem setup,[0],[0]
"Define pt=1Φ (r) := pΦ(r|t = 1), pt=0Φ (r) := pΦ(r|t = 0), to be the treated and control distributions induced over R. For a one-to-one Φ, the distributions pt=1Φ (r) and p t=0 Φ (r) can be obtained by the standard change of variables formula, using the determinant of the Jacobian of Ψ(r).
",3.1. Problem setup,[0],[0]
"Let Φ : X → R be a representation function, and h : R × {0, 1} → Y be an hypothesis defined over the representation space R. Let L : Y × Y → R+ be a loss function.",3.1. Problem setup,[0],[0]
"We define two complimentary loss functions: one is the standard machine learning loss, which we will call the factual loss F , as it relates to observable quantities.",3.1. Problem setup,[0],[0]
"The other is the expected loss with respect to the distribution where the treatment assignment is flipped, which we call the counterfactual loss, CF .
",3.1. Problem setup,[0],[0]
Definition 2.,3.1. Problem setup,[0],[0]
"The expected loss for the unit and treatment pair (x, t) is: `h,Φ(x, t) =∫ Y L(Yt, h(Φ(x), t))p(Yt|x)dYt.",3.1. Problem setup,[0],[0]
"The expected factual and counterfactual losses of h and Φ are:
F (h,Φ) = ∫ X×{0,1} `h,Φ(x, t) p(x, t) dxdt,
CF (h,Φ) = ∫ X×{0,1} `h,Φ(x, t) p(x, 1− t) dxdt.
",3.1. Problem setup,[0],[0]
"If x denotes patients’ features, t a treatment, and Yt a potential outcome such as mortality, we think of F as measuring how well do h and Φ predict mortality for the patients and doctors’ actions sampled from the same distribution as our data sample.",3.1. Problem setup,[0],[0]
"CF measures how well our prediction with h and Φ would do in a “topsy-turvy” world where the patients are the same but the doctors are inclined to prescribe
exactly the opposite treatment than the one the real-world doctors would prescribe.
",3.1. Problem setup,[0],[0]
Definition 3.,3.1. Problem setup,[0],[0]
"The expected factual treated and control losses are:
t=1F (h,Φ) = ∫ X `h,Φ(x, 1) p t=1(x) dx,
t=0F (h,Φ) = ∫ X `h,Φ(x, 0) p t=0(x) dx.
",3.1. Problem setup,[0],[0]
"For u := p(t = 1), it is immediate to show that F (h,Φ) = u t=1F (h,Φ) +",3.1. Problem setup,[0],[0]
"(1− u) t=0F (h,Φ).",3.1. Problem setup,[0],[0]
Definition 4.,3.1. Problem setup,[0],[0]
"The treatment effect (ITE) for unit x is:
τ(x) := E",3.1. Problem setup,[0],[0]
"[Y1 − Y0|x] .
",3.1. Problem setup,[0],[0]
Let f :,3.1. Problem setup,[0],[0]
"X × {0, 1} → Y by an hypothesis.",3.1. Problem setup,[0],[0]
"For example, we could have that f(x, t) = h(Φ(x), t).
",3.1. Problem setup,[0],[0]
Definition 5.,3.1. Problem setup,[0],[0]
"The treatment effect estimate of the hypothesis f for unit x is:
τ̂f (x) = f(x, 1)− f(x, 0).
",3.1. Problem setup,[0],[0]
Definition 6.,3.1. Problem setup,[0],[0]
"The expected Precision in Estimation of Heterogeneous Effect (PEHE, Hill (2011)) loss of f is:
PEHE(f) = ∫ X (τ̂f (x)− τ(x))2 p(x) dx, (1)
",3.1. Problem setup,[0],[0]
"When f(x, t) = h(Φ(x), t), we will also use the notation PEHE(h,Φ) = PEHE(f).
",3.1. Problem setup,[0],[0]
"Our proof relies on the notion of an Integral Probability Metric (IPM), which is a class of metrics between probability distributions (Sriperumbudur et al., 2012; Müller, 1997).",3.1. Problem setup,[0],[0]
"For two probability density functions p, q defined over S ⊆ Rd, and for a function family G of functions g : S → R, we have that
IPMG(p, q) := sup g∈G ∣∣∣∣∫",3.1. Problem setup,[0],[0]
S g(s)(p(s)− q(s)),3.1. Problem setup,[0],[0]
ds ∣∣∣∣ .,3.1. Problem setup,[0],[0]
"Integral probability metrics are always symmetric and obey the triangle inequality, and trivially satisfy IPMG(p, p) = 0.",3.1. Problem setup,[0],[0]
"For rich enough function families G, we also have that IPMG(p, q) = 0 =⇒ p = q, and then IPMG is a true metric.",3.1. Problem setup,[0],[0]
"Examples of function families G for which IPMG is a true metric are the family of bounded continuous functions, the family of 1-Lipschitz functions (Sriperumbudur et al., 2012), and the unit-ball of functions in a universal reproducing kernel Hilbert space (Gretton et al., 2012).",3.1. Problem setup,[0],[0]
"We first state a Lemma bounding the counterfactual loss, a key step in obtaining the bound on the error in estimating
individual treatment effect.",3.2. Bounds,[0],[0]
We then give the main Theorem.,3.2. Bounds,[0],[0]
"The proofs and details are in the supplement.
",3.2. Bounds,[0],[0]
Let u := p(t = 1) be the marginal probability of treatment.,3.2. Bounds,[0],[0]
"By the strong ignorability assumption, 0 < u < 1.
",3.2. Bounds,[0],[0]
Lemma 1.,3.2. Bounds,[0],[0]
"Let Φ : X → R be a one-to-one representation function, with inverse Ψ. Let h : R × {0, 1} → Y be an hypothesis.",3.2. Bounds,[0],[0]
Let G be a family of functions g : R → Y .,3.2. Bounds,[0],[0]
"Assume there exists a constantBΦ > 0, such that for fixed t ∈ {0, 1}, the per-unit expected loss functions `h,Φ(Ψ(r), t) (Definition 2) obey 1BΦ · `h,Φ(Ψ(r), t) ∈",3.2. Bounds,[0],[0]
"G. We have:
CF (h,Φ) ≤ (1− u) t=1F (h,Φ) + u t=0F (h,Φ) +BΦ · IPMG ( pt=1Φ , p t=0 Φ ) ,
where CF , t=0F and t=1 F are as in Definitions 2 and 3.
Theorem 1.",3.2. Bounds,[0],[0]
"Under the conditions of Lemma 1, and assuming the loss L used to define `h,Φ in Definitions 2 and 3 is the squared loss, we have:
PEHE(h,Φ) ≤ 2 ( CF (h,Φ) + F (h,Φ)− 2σ2Y ) ≤ (2)
2 ( t=0F (h,Φ)+ t=1",3.2. Bounds,[0],[0]
"F (h,Φ)+BΦIPMG",3.2. Bounds,[0],[0]
"( pt=1Φ , p t=0 Φ ) −2σ2Y ) ,
where F and CF are defined w.r.t.",3.2. Bounds,[0],[0]
"the squared loss, and σ2Y is the variance of the outcomes",3.2. Bounds,[0],[0]
"Yt (see Definition A11 in Appendix for detailed definition).
",3.2. Bounds,[0],[0]
The main idea of the proof is showing that PEHE is upper bounded by the sum of the expected factual loss F and expected counterfactual loss CF .,3.2. Bounds,[0],[0]
"However, we cannot estimate CF , since we only have samples relevant to F .",3.2. Bounds,[0],[0]
"We therefore bound the difference CF − F using an IPM.
Choosing a small function family G makes the bound tighter.",3.2. Bounds,[0],[0]
"However, choosing too small a family could result in an incomputable bound.",3.2. Bounds,[0],[0]
"For example, for the minimal choice G = {`h,Φ(x, 0), `h,Φ(x, 1)}, we will have to evaluate an expectation term of Y1 over pt=0Φ , and of Y0 over pt=1Φ .",3.2. Bounds,[0],[0]
"We cannot in general evaluate these expectations, since by assumption when t = 0",3.2. Bounds,[0],[0]
"we only observe Y0, and the same for t = 1 and Y1.",3.2. Bounds,[0],[0]
"In addition, for some function families there is no known way to efficiently compute the IPM distance or its gradients.",3.2. Bounds,[0],[0]
"Here, we use two function families for which there are available optimization tools.",3.2. Bounds,[0],[0]
"The first is the family of 1-Lipschitz functions, which leads to IPM being the Wasserstein distance (Villani, 2008), denoted Wass(p, q).",3.2. Bounds,[0],[0]
"The second is the family of norm-1 reproducing kernel Hilbert space (RKHS) functions, leading to the MMD metric (Gretton et al., 2012), denoted MMD(p, q).",3.2. Bounds,[0],[0]
"Both the Wasserstein and MMD metrics have consistent estimators which can be efficiently computed for finite samples (Sriperumbudur et al., 2012), and
have been used for various machine learning tasks in recent years (Gretton et al., 2009; 2012; Cuturi & Doucet, 2014).
",3.2. Bounds,[0],[0]
"In order to explicitly evaluate the constant BΦ in Theorem 1, we have to make some assumptions about the elements of the problem.",3.2. Bounds,[0],[0]
"For the Wasserstein case these are the loss L, the Lipschitz constants of p(Yt|x) and h, and the condition number of the Jacobian of Φ. For the MMD case, we make assumptions about the RKHS representability and RKHS norms of h , Φ, and the standard deviation of Yt|x.",3.2. Bounds,[0],[0]
"The full details are given in the supplement, with the major results stated in Theorems 2 and 3.",3.2. Bounds,[0],[0]
"In all cases we obtain that making Φ smaller increases the constant BΦ precluding trivial solutions such as making Φ arbitrarily small.
",3.2. Bounds,[0],[0]
"For an empirical sample, and a family of representations and hypotheses, we can further upper bound t=0F and t=1 F by their respective empirical losses and a model complexity term using standard arguments (Shalev-Shwartz & BenDavid, 2014).",3.2. Bounds,[0],[0]
"The IPMs we use can be consistently estimated from finite samples (Sriperumbudur et al., 2012).",3.2. Bounds,[0],[0]
"The negative variance term σ2Y arises from the fact that, following Hill (2011); Athey & Imbens (2016), we define the error PEHE in terms of the conditional mean functions mt(x), as opposed to fitting the random variables Yt.
",3.2. Bounds,[0],[0]
Our results hold for any given h and Φ obeying the Theorem conditions.,3.2. Bounds,[0],[0]
This immediately suggest an algorithm in which we minimize the upper bound in Eq.,3.2. Bounds,[0],[0]
"(2) with respect to Φ and h and either the Wasserstein or MMD IPM, in order to minimize the error in estimating the individual treatment effect.",3.2. Bounds,[0],[0]
This leads us to Algorithm 1 below.,3.2. Bounds,[0],[0]
We propose a general framework called CFR (for Counterfactual Regression) for ITE estimation based on the theoretical results above.,4. Algorithm for estimating ITE,[0],[0]
"Our algorithm is an end-to-end, regularized minimization procedure which fits both a balanced representation of the data and a hypothesis for the outcome.",4. Algorithm for estimating ITE,[0],[0]
"CFR draws on the same intuition as our previous work (Johansson et al., 2016), but overcomes the following limitations: a) Our previous theory requires a two-step optimization procedure and is specific to linear hypotheses (it does not support e.g. deep neural networks), b)",4. Algorithm for estimating ITE,[0],[0]
"The treatment indicator might be washed out in the old model, if the learned representation is high-dimensional (see discussion below).
",4. Algorithm for estimating ITE,[0],[0]
"We assume there exists a distribution p(x, t, Y0, Y1) over X × {0, 1} × Y × Y , such that strong ignorability holds.",4. Algorithm for estimating ITE,[0],[0]
"We further assume we have a sample from that distribution (x1, t1, y1), . . .",4. Algorithm for estimating ITE,[0],[0]
"(xn, tn, yn), where yi ∼ p(Y1|xi) if ti = 1, yi ∼ p(Y0|xi) if ti = 0.",4. Algorithm for estimating ITE,[0],[0]
This standard assumption means that the treatment assignment determines which potential outcome we see.,4. Algorithm for estimating ITE,[0],[0]
Our goal is to find a representation Φ : X → R and hypothesis h :,4. Algorithm for estimating ITE,[0],[0]
"X × {0, 1} → Y that will
minimize PEHE(f) for f(x, t) := h(Φ(x), t).
",4. Algorithm for estimating ITE,[0],[0]
"We parameterize Φ(x) and h(Φ, t) by deep neural networks trained jointly, see Figure 1.",4. Algorithm for estimating ITE,[0],[0]
This allows for learning complex non-linear representations and hypotheses with large flexibility.,4. Algorithm for estimating ITE,[0],[0]
"In Johansson et al. (2016), we parameterized h(Φ, t) with a single network, concatenating Φ and t as input.",4. Algorithm for estimating ITE,[0],[0]
"In this case, if Φ is high-dimensional, the influence of t on hmight be lost during training.",4. Algorithm for estimating ITE,[0],[0]
"To combat this, we parameterize h1(Φ) = h(Φ, 1) and h0(Φ)",4. Algorithm for estimating ITE,[0],[0]
"= h(Φ, 0) as two separate “heads” of the joint network, the former used to estimate the outcome under treatment, and the latter under control.",4. Algorithm for estimating ITE,[0],[0]
"This way, statistical power is shared in representation layers, while the effect of treatment is preserved in the separate heads.",4. Algorithm for estimating ITE,[0],[0]
"Note that each sample is used to update only the head corresponding to the observed treatment.
",4. Algorithm for estimating ITE,[0],[0]
Our second contribution is to explicitly adjust for the bias induced by treatment group imbalance.,4. Algorithm for estimating ITE,[0],[0]
"To this end, we seek a representation Φ and hypothesis h that minimizes a trade-off between predictive accuracy and imbalance in the representation space, using the following objective:
min h,Φ ‖Φ‖=1
1 n",4. Algorithm for estimating ITE,[0],[0]
"∑n i=1 wi · L (h(Φ(xi), ti) , yi) + λ ·R(h)
+α · IPMG ({Φ(xi)}i:ti=0, {Φ(xi)}i:ti=1) , with wi = ti2u + 1−ti 2(1−u) , where u = 1 n",4. Algorithm for estimating ITE,[0],[0]
∑n i=1,4. Algorithm for estimating ITE,[0],[0]
"ti,
and R is a model complexity term.",4. Algorithm for estimating ITE,[0],[0]
"(3)
Note that u = p(t = 1) is simply the proportion of treated units in the population.",4. Algorithm for estimating ITE,[0],[0]
"The weights wi compensate for the difference in treatment group size in our sample, see Theorem 1.",4. Algorithm for estimating ITE,[0],[0]
"IPMG(·, ·) is the (empirical) integral probability metric w.r.t.",4. Algorithm for estimating ITE,[0],[0]
G.,4. Algorithm for estimating ITE,[0],[0]
"For most IPMs, we cannot compute the factor Bφ in (2), but treat it as part of the hyperparameter α.",4. Algorithm for estimating ITE,[0],[0]
"This makes our objective sensitive to the scaling of Φ, even for a constant α.",4. Algorithm for estimating ITE,[0],[0]
"We therefore normalize Φ through either projection or batch-normalization with fixed scale.
",4. Algorithm for estimating ITE,[0],[0]
We refer to the model minimizing (3) with α > 0,4. Algorithm for estimating ITE,[0],[0]
as Counterfactual Regression (CFR) and the variant without balance regularization (α = 0) as Treatment-Agnostic Representation Network (TARNet).,4. Algorithm for estimating ITE,[0],[0]
"Both models are trained by minimizing (3) using stochastic gradient descent, as described in Algorithm 1.",4. Algorithm for estimating ITE,[0],[0]
"Both the prediction loss and the penalty term IPMG(·, ·) are computed for one mini-batch at a time.",4. Algorithm for estimating ITE,[0],[0]
Details of how to obtain the gradient g1 with respect to the empirical IPMs are in the supplement.,4. Algorithm for estimating ITE,[0],[0]
"Evaluating causal inference algorithms is more difficult than many machine learning tasks, since we rarely have access to the ground truth treatment effect.",5. Experiments,[0],[0]
Existing literature mostly deals with this in two ways.,5. Experiments,[0],[0]
"One is by using (semi-)
Algorithm 1 CFR: Counterfactual regression with integral probability metrics
1: Input: Factual sample (x1, t1, y1), . . .",5. Experiments,[0],[0]
", (xn, tn, yn), scaling parameter α > 0, loss function L (·, ·), representation network ΦW with initial weights W, outcome network hV with initial weights V, function family G for IPM.
2: Compute u =",5. Experiments,[0],[0]
1n,5. Experiments,[0],[0]
∑n i=1,5. Experiments,[0],[0]
"ti 3: Compute wi = ti2u + 1−ti
2(1−u) for i = 1 . . .",5. Experiments,[0],[0]
n,5. Experiments,[0],[0]
"4: while not converged do 5: Sample mini-batch {i1, i2, . .",5. Experiments,[0],[0]
.,5. Experiments,[0],[0]
", im} ⊂ {1, 2, . . .",5. Experiments,[0],[0]
",",5. Experiments,[0],[0]
"n} 6: Calculate the gradient of the IPM term: g1 =∇W IPMG({ΦW(xij )}tij=0, {ΦW(xik )}tij=1) 7: Calculate the gradients of the empirical loss:
g2 = ∇V 1m ∑ j wij · L ( hV(ΦW(xij ), tij ), yij )",5. Experiments,[0],[0]
"g3 = ∇W 1m ∑ j wij · L ( hV(ΦW(xij ), tij ), yij
) 8:",5. Experiments,[0],[0]
"Obtain step size scalar or matrix η with standard neural net methods e.g. Adam (Kingma & Ba, 2015) 9",5. Experiments,[0],[0]
:,5. Experiments,[0],[0]
"[W,V]←",5. Experiments,[0],[0]
[W − η(αg1,5. Experiments,[0],[0]
+,5. Experiments,[0],[0]
"g3),V",5. Experiments,[0],[0]
"− η(g2 + 2λV)]
10: Check convergence criterion 11: end while
synthetic datasets, where the outcome or treatment assignment are fully known; we use the semi-synthetic IHDP dataset from Hill (2011).",5. Experiments,[0],[0]
The other is using real-world data from randomized controlled trials (RCT).,5. Experiments,[0],[0]
"The problem with using data from RCTs is that there is no imbalance between treatment groups, making our method redundant.",5. Experiments,[0],[0]
"We partially overcome this problem by using the Jobs dataset from LaLonde (1986), which includes both a randomized and a non-randomized component.",5. Experiments,[0],[0]
"We use both components for training, but only use the randomized component for evaluation.",5. Experiments,[0],[0]
"This alleviates, but does not solve, the issue of a completely randomized and balanced dataset being unsuited for our method.
",5. Experiments,[0],[0]
"We evaluate our framework CFR, and its variant without balancing regularization (TARNet), in the task of estimating ITE and ATE.",5. Experiments,[0],[0]
"Both versions are implemented3 as feed-forward neural networks with 3 fully-connected exponential-linear layers (Clevert et al., 2016) for the representation and 3 for the hypothesis.",5. Experiments,[0],[0]
Layer sizes were 200 for all layers used for Jobs and 200 and 100 for the representation and hypothesis used for IHDP.,5. Experiments,[0],[0]
"The model is trained using Adam (Kingma & Ba, 2015).",5. Experiments,[0],[0]
The hypothesis parameters are regularized with a small `2 weight decay.,5. Experiments,[0],[0]
"For continuous data we use mean squared loss and for binary data, we use log-loss.",5. Experiments,[0],[0]
"While our theory does not immediately apply to log-loss, we were curious to see how our model performs with it.",5. Experiments,[0],[0]
"We use the Wasserstein (CFRWASS) and the squared linear MMD (CFRMMD) distances to penalize
3https://github.com/clinicalml/cfrnet
imbalance.
",5. Experiments,[0],[0]
"We compare our method to Ordinary Least Squares with treatment as a feature (OLS1), OLS with separate regressors for each treatment (OLS2), k-nearest neighbor (k-NN),",5. Experiments,[0],[0]
"Targeted Maximum Likelihood (TMLE), which is a doubly robust method (Gruber & van der Laan, 2011), Bayesian Additive Regression Trees (BART) (Chipman et al., 2010; Chipman & McCulloch, 2016), Random Forests (R. For.)",5. Experiments,[0],[0]
"(Breiman, 2001), Causal Forests (C. For.)",5. Experiments,[0],[0]
"(Wager & Athey, 2015) as well as the Balancing Linear Regression (BLR) and Balancing Neural Network (BNN) from Johansson et al. (2016).",5. Experiments,[0],[0]
For classification tasks we substitute Logistic Regression (LR) for OLS.,5. Experiments,[0],[0]
"Choosing hyperparameters for estimating PEHE is nontrivial; we detail our general procedure using a validation set, in subsection C.1 of the supplement.
",5. Experiments,[0],[0]
We consider two different estimation tasks.,5. Experiments,[0],[0]
"One is withinsample, where the task is to estimate ITE for all units in a sample for which the (factual) outcome of one treatment is observed.",5. Experiments,[0],[0]
This corresponds to the common scenario in which a cohort is selected once and not changed.,5. Experiments,[0],[0]
"This task is non-trivial, as we never observe the ITE for any unit.",5. Experiments,[0],[0]
"The other is out-of-sample, where the goal is to estimate ITE for units with no observed outcomes.",5. Experiments,[0],[0]
This corresponds to the problem of selecting the best treatment for a new patient.,5. Experiments,[0],[0]
"Within-sample error is computed over both the training and validation sets, out-of-sample error over the test set.",5. Experiments,[0],[0]
"Hill (2011) compiled a dataset for causal effect estimation based on the Infant Health and Development Program (IHDP), in which the covariates come from a randomized experiment studying the effects of specialist home visits on cognitive test scores.",5.1. Simulated outcome: IHDP,[0],[0]
The treatment groups have been made imbalanced by removing a biased subset of the treated population.,5.1. Simulated outcome: IHDP,[0],[0]
"The dataset comprises 747 units (139 treated, 608 control) and 25 covariates measuring aspects of children and their mothers.",5.1. Simulated outcome: IHDP,[0],[0]
"We use the simulated outcome implemented as setting “A” in the NPCI package (Dorie, 2016).",5.1. Simulated outcome: IHDP,[0],[0]
"Following Hill (2011), we use the noiseless outcome to compute the true effect.",5.1. Simulated outcome: IHDP,[0],[0]
"We report the estimated (finitesample) PEHE loss PEHE (Eq. 1), and the absolute error in average treatment effect ATE = | 1n",5.1. Simulated outcome: IHDP,[0],[0]
"∑n i=1(f(xi, 1)−
f(xi, 0))",5.1. Simulated outcome: IHDP,[0],[0]
− 1n ∑n i=1(m1(xi) − m0(xi))|.,5.1. Simulated outcome: IHDP,[0],[0]
The results of the experiments on IHDP are presented in Table 1 (left).,5.1. Simulated outcome: IHDP,[0],[0]
"We average over 1000 realizations of the outcomes with 63/27/10 train/validation/test splits.
",5.1. Simulated outcome: IHDP,[0],[0]
We also investigate the effects of increasing imbalance between the original treatment groups by constructing biased subsamples of the IHDP dataset.,5.1. Simulated outcome: IHDP,[0],[0]
A logistic-regression propensity score model is fit to form estimates p̂(t = 1|x) of the conditional treatment probability.,5.1. Simulated outcome: IHDP,[0],[0]
"Then, repeatedly,
with probability q we remove the remaining control observation x that has p̂(t = 1|x) closest to 1, and with probability 1− q, we remove a random control observation.",5.1. Simulated outcome: IHDP,[0],[0]
"The higher q, the more imbalance.",5.1. Simulated outcome: IHDP,[0],[0]
"For each value of q, we remove 347 observations from each set, leaving 400.",5.1. Simulated outcome: IHDP,[0],[0]
"The study by LaLonde (1986) is a widely used benchmark in the causal inference community, where the treatment is job training and the outcomes are income and employment status after training.",5.2. Real-world outcome: Jobs,[0],[0]
"This dataset combines a randomized study based on the National Supported Work program with observational data to form a larger dataset (Smith & Todd, 2005).",5.2. Real-world outcome: Jobs,[0],[0]
The presence of the randomized subgroup gives a way to estimate the “ground truth” causal effect.,5.2. Real-world outcome: Jobs,[0],[0]
"The study includes 8 covariates such as age and education, as well as previous earnings.",5.2. Real-world outcome: Jobs,[0],[0]
"We construct a binary classification task, called Jobs, where the goal is to predict unemployment, using the feature set of Dehejia & Wahba (2002).",5.2. Real-world outcome: Jobs,[0],[0]
"Following Smith & Todd (2005), we use the LaLonde experimental sample (297 treated, 425 control) and the PSID comparison group (2490 control).",5.2. Real-world outcome: Jobs,[0],[0]
There were 482 (15%) subjects unemployed by the end of the study.,5.2. Real-world outcome: Jobs,[0],[0]
"We average
over 10 train/validation/test splits with ratios 56/24/20.
",5.2. Real-world outcome: Jobs,[0],[0]
"Because all the treated subjects T were part of the original randomized sample E, we can compute the true average treatment effect on the treated by ATT = |T |−1 ∑ i∈T yi−
|C ∩ E|−1 ∑ i∈C∩E yi, where C is the control group.",5.2. Real-world outcome: Jobs,[0],[0]
"We
report the error ATT = |ATT − 1|T | ∑ i∈T (f(xi, 1)",5.2. Real-world outcome: Jobs,[0],[0]
"− f(xi, 0))|.",5.2. Real-world outcome: Jobs,[0],[0]
"We cannot evaluate PEHE on this dataset, since there is no ground truth for the ITE.",5.2. Real-world outcome: Jobs,[0],[0]
"Instead, in order to evaluate the quality of ITE estimation, we use a measure we call policy risk.",5.2. Real-world outcome: Jobs,[0],[0]
The policy risk is defined as the average loss in value when treating according to the policy implied by an ITE estimator.,5.2. Real-world outcome: Jobs,[0],[0]
"In our case, for a model f , we let the policy be to treat, πf (x) = 1, if f(x, 1)",5.2. Real-world outcome: Jobs,[0],[0]
"− f(x, 0) > λ, and to not treat, πf (x) = 0 otherwise.",5.2. Real-world outcome: Jobs,[0],[0]
The policy risk is RPol(πf ),5.2. Real-world outcome: Jobs,[0],[0]
"= 1 − (E[Y1|πf (x) = 1] · p(πf = 1) + E[Y0|πf (x) = 0] · p(πf = 0)) which we can estimate for the randomized trial subset of Jobs by R̂Pol(πf = 1 − (E[Y1|πf (x) = 1, t = 1] · p(πf = 1) + E[Y0|πf (x) = 0, t = 0] · p(πf = 0)).",5.2. Real-world outcome: Jobs,[0],[0]
"See figure 3 for risk as a function of treatment threshold λ, aligned by proportion of treated, and Table 1 for the risk when λ = 0.",5.2. Real-world outcome: Jobs,[0],[0]
"We note that indeed imbalance confers an advantage to using the IPM regularization term, as our theoretical results indicate, see e.g. the results for CFRWASS and TARNet on IHDP in Table 1.",5.3. Results,[0],[0]
"We also see in Figure 2 that even for the harder case of increased imbalance (q > 0) between treated and control, the relative gain from using our method remains significant.",5.3. Results,[0],[0]
"On Jobs, our proposed methods are better than or competitive with state-of-the-art, but we don’t see a significant gain from using IPM penalties.",5.3. Results,[0],[0]
This might be because we evaluate the predictions only on a randomized subset with treatment groups distributed identically.,5.3. Results,[0],[0]
Non-linear estimators perform significantly better than linear ones in terms of individual effect ( PEHE).,5.3. Results,[0],[0]
"On the Jobs dataset, straightforward logistic regression does
remarkably well in estimating the ATT.",5.3. Results,[0],[0]
"However, being a linear model, LR can only ascribe a uniform policy - in this case, “treat everyone”.",5.3. Results,[0],[0]
The more nuanced policies offered by non-linear methods achieve lower policy risk in the case of Causal Forests and CFR.,5.3. Results,[0],[0]
This emphasizes the fact that estimating average effect and individual effect can require different models.,5.3. Results,[0],[0]
"Specifically, while smoothing over many units may yield a good ATE estimate, this might significantly hurt ITE estimation.",5.3. Results,[0],[0]
"k-nearest neighbors has very good within-sample results on Jobs, because evaluation is performed over the randomized component, but suffers heavily in generalizing out of sample, as expected.",5.3. Results,[0],[0]
In this paper we give a meaningful and intuitive errorbound for estimating individual treatment effect.,6. Conclusion,[0],[0]
"Our bound relates ITE estimation to the classic machine learning problem of learning from samples, along with methods for measuring distributional distances from samples.",6. Conclusion,[0],[0]
The bound lends itself naturally to the creation of learning algorithms; we focus on using neural nets as representations and hypotheses.,6. Conclusion,[0],[0]
"We apply our theory-guided approach to both synthetic and real-world tasks, showing that in every case our method matches or outperforms the state-of-theart.",6. Conclusion,[0],[0]
"Important open questions are theoretical considerations in choosing the IPM weight α, how to best derive confidence intervals for our model’s predictions, and integrating our work with more complicated causal models such as those with hidden confounding or instrumental variables.",6. Conclusion,[0],[0]
"We thank Aahlad Puli for his assistance with the experiments; Sanjog Misra and Günter J. Hitsch for suggesting the policy risk evaluation; Jennifer Hill, Marco Cuturi and Esteban Tabak for fruitful conversations; and Stefan Wager for his help with the code for Causal Forests.",ACKNOWLEDGMENTS,[0],[0]
DS and US were supported by NSF CAREER award #1350965.,ACKNOWLEDGMENTS,[0],[0]
"There is intense interest in applying machine learning to problems of causal inference in fields such as healthcare, economics and education.",abstractText,[0],[0]
"In particular, individual-level causal inference has important applications such as precision medicine.",abstractText,[0],[0]
"We give a new theoretical analysis and family of algorithms for predicting individual treatment effect (ITE) from observational data, under the assumption known as strong ignorability.",abstractText,[0],[0]
"The algorithms learn a “balanced” representation such that the induced treated and control distributions look similar, and we give a novel and intuitive generalization-error bound showing the expected ITE estimation error of a representation is bounded by a sum of the standard generalization-error of that representation and the distance between the treated and control distributions induced by the representation.",abstractText,[0],[0]
"We use Integral Probability Metrics to measure distances between distributions, deriving explicit bounds for the Wasserstein and Maximum Mean Discrepancy (MMD) distances.",abstractText,[0],[0]
Experiments on real and simulated data show the new algorithms match or outperform the state-of-the-art.,abstractText,[0],[0]
Estimating individual treatment effect: generalization bounds and algorithms,title,[0],[0]
"Given samples from a distribution, many settings in machine learning and statistics involves estimating properties of the unseen portion of the distribution, i.e. elements in the support of the distribution that are not observed in the samples collected so far.",1. Introduction,[0],[0]
One important example of estimating the unseen is the problem of predicting the number of distinct new elements in additional samples collected.,1. Introduction,[0],[0]
This question is famously illustrated by the case of Corbet’s butterflies.,1. Introduction,[0],[0]
"Alexander Corbet was a British naturalist who
1Stanford University, Stanford, CA 2Chan Zuckerberg Biohub, San Francisco, CA.",1. Introduction,[0],[0]
"Correspondence to: Aditi Raghunathan <aditir@stanford.edu>, Gregory Valiant <valiant@stanford.edu>, James Zou <jamesz@stanford.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
spent two years in Malaya trapping butterflies.,1. Introduction,[0],[0]
"He found 118 rare species of butterflies for which he found only one specimen, another 74 species with two specimens, 44 with three specimens, etc.",1. Introduction,[0],[0]
Corbet was naturally interested in the butterflies that are heretofore unseen.,1. Introduction,[0],[0]
"In particular, he wanted to estimate how many distinct new species of butterflies he can expect to discover if he were to conduct a new expedition to Malaya—such an estimate could help determine whether a new experiment is warranted.",1. Introduction,[0],[0]
"GoodToulmin, extending earlier work of Ronald Fisher, came up with the remarkable estimate that the number of new species Corbet can expect to find is simply the alternating sum 118 - 74 + 44 - ...",1. Introduction,[0],[0]
The Good-Toulmin estimator sparked the investigation into how to estimate the discovery rate of new elements and this remains an active area of research.,1. Introduction,[0],[0]
Estimating the discovery rate has many important applications beyond the original species collection setting.,1. Introduction,[0],[0]
"In genomics, for example, an important question is: given the genetic variation already identified in the genomes of individuals from some population (say, East Asia), how many additional mutations do we expect to find by sequencing the genomes of additional individuals from East Asia.",1. Introduction,[0],[0]
"An accurate answer to this question can improve the cohort design of new population sequencing experiments.
",1. Introduction,[0],[0]
Predicting the number of new elements is a particular instance of estimating the unseen.,1. Introduction,[0],[0]
"In other applications, one may want to estimate different statistics that also depend on the currently unobserved elements.",1. Introduction,[0],[0]
"For example, one may want to predict how many new elements will be observed at least twice (for reproducibility) or at most three times (if the focus is on rare elements).",1. Introduction,[0],[0]
"More generally, one may want to estimate the histogram of the underlying distribution, which summarizes the frequency distribution of all the elements (see Sec. 2 for precise definition) and from which these other statistics can be derived.
",1. Introduction,[0],[0]
The unseen estimation literature has focused on the setting where there is a single distribution which generate current samples as well as any future samples.,1. Introduction,[0],[0]
"In practice, we often have multiple distinct distributions and we observe varying number of samples from each distribution.",1. Introduction,[0],[0]
"In the genomics example above, in addition to sequencing data from East Asians, we also have genome sequences of individuals from Europe, Africa, etc.",1. Introduction,[0],[0]
"The relevant question is: given we currently have the genomes of ni individuals
ar X
iv :1
70 7.
03 85
4v 1
[ cs
.L G
] 1
2 Ju
l 2 01
7
from population",1. Introduction,[0],[0]
"i, i ∈ {1, ...,m}, and we have identified all the genetic variants in this group, how many total new mutations do we expect to find if we sequence additional bi individuals from population i. Moreover, given a finite budget Nnew of new genomes that we can sequence, how should we allocate this budget across the different populations to maximize the expected number of new mutations oberved?",1. Introduction,[0],[0]
"Similarly, suppose Corbet had also collected butterflies in Brunei and Indonesia, in addition to Malaya.",1. Introduction,[0],[0]
"Then he might want to know how many totally new species he can expect to find if he was to spend, say, another six month in Malaya and one year in Brunei.",1. Introduction,[0],[0]
"He might also be interested in estimating the joint frequency distribution of butterflies across all three regions.
",1. Introduction,[0],[0]
Our contributions.,1. Introduction,[0],[0]
"In this paper, we address the general problem of estimating the unseen when we have samples from multiple populations, each corresponding to a potentially distinct distribution.",1. Introduction,[0],[0]
"Despite being very natural, this multi-population problem has not been systematically studied to the best of our knowledge.",1. Introduction,[0],[0]
We derive a multi-population generalization of the Good-Toulmin estimator for the expected number of new elements.,1. Introduction,[0],[0]
"Surprisingly, we prove that the accuracy of our extrapolation estimator is independent of the number of populations.",1. Introduction,[0],[0]
"Moreover, it achieves the optimal super-linear extrapolation rate.",1. Introduction,[0],[0]
"Next, we develop an efficient optimization method to estimate the more general multi-population joint frequency distribution.",1. Introduction,[0],[0]
"This complements our extrapolation estimator, and outperforms the generalized Good-Toulmin estimator in most settings.",1. Introduction,[0],[0]
This more general approach also enables predictions for other statistics of interest.,1. Introduction,[0],[0]
We systematically validate these two algorithms on synthetic data as well as real datasets from population genetics and from English books.,1. Introduction,[0],[0]
"Moreover, we illustrate that by estimating the joint frequency distribution, we can significantly improve the discovery power under a budget constraint.",1. Introduction,[0],[0]
"The problem of estimating the properties of the unobserved portion of a distribution, given n samples, and the related problem of estimating the number of new domain elements that are likely to be observed if an additional cn samples are collected, dates back to works of I.J. Good and A. Turing (Good, 1953), and R.A. Fisher (Fisher et al., 1943).",2. Related works,[0],[0]
"This was quickly followed by (Good & Toulmin, 1956), which introduced the Good-Toulmin estimator.",2. Related works,[0],[0]
"While the Good-Toulmin estimator is always unbiased, the variance increases rapidly for c ≥ 1.",2. Related works,[0],[0]
"Subsequent works, including (Efron & Thisted, 1976) have suggested “smoothing” approaches that tradeoff the bias and variance for this type of approach.",2. Related works,[0],[0]
"The recent work of Orlitsky et al. (2016) describes a clever variant that achieves good performance for
c = O(log n).",2. Related works,[0],[0]
"This ability to accurately estimate the number of domain elements seen in a second sample of size up to O(n log n), where n denotes the size of the original sample, was concurrently shown via a different approach in (Valiant & Valiant, 2016).",2. Related works,[0],[0]
"This logarithmic factor extrapolation matches the lower bounds of (Valiant & Valiant, 2011), to constant factors.",2. Related works,[0],[0]
"The linear estimators that we propose in Section 4 for the multiple population setting, and their analysis, are extensions of the smoothed GoodToulmin estimators of (Orlitsky et al., 2016).
",2. Related works,[0],[0]
"A different approach to this problem was proposed by Efron & Thisted (1976), who considered a linearprogramming approach to estimating this property by implicitly finding a label-less representation of the underlying distribution that was consistent with the observed frequency counts, then returning the support size of this distribution.",2. Related works,[0],[0]
"This approach was adapted and rigorously analyzed in (Valiant & Valiant, 2011; 2013), who showed that it provably yields an accurate representation of the frequency distribution of the underlying distribution, which can subsequently be leveraged to yield estimates of distributional properties, including entropy, distance metrics between distributions, and approximations for the number of new elements that would be observed in larger samples.",2. Related works,[0],[0]
"Recent works (Valiant & Valiant, 2016; Zou et al., 2016) also established that this approach can accurately estimate the number of new elements that will be observed in samples of size up to O(n log n).",2. Related works,[0],[0]
"Our optimization-based algorithm, described in Section 5, generalizes this approach.",2. Related works,[0],[0]
"Let Ω denote the domain, and D1, ..., Dm denote m probability distributions over Ω. Di represents the frequency of elements in population i.",3. Definitions and examples,[0],[0]
Note that it is not restrictive to assume that the populations share the same domain Ω since different Di’s may have distinct supports.,3. Definitions and examples,[0],[0]
We model the multi-population unseen estimation as a two stage process.,3. Definitions and examples,[0],[0]
"In the first period, we observe nj independent samples from the j-th population, {Xji } j=1,...,m i=1,...,nj
.",3. Definitions and examples,[0],[0]
This is the seen data.,3. Definitions and examples,[0],[0]
"In period two, which is in the future, we will sample additional tjnj samples from the j-th population, {Y ji } j=1,...,m i=1,...,tjnj .",3. Definitions and examples,[0],[0]
"The period two samples are unseen and we would like to estimate some statistic U({Y ji }, {X j i }).",3. Definitions and examples,[0],[0]
We can think of tj ≥ 0 as the extrapolation factors.,3. Definitions and examples,[0],[0]
"If tj is large, then we will obtain many more samples from population j in the second period compared to what we have, and the problem of estimating U could be more challenging.",3. Definitions and examples,[0],[0]
We can take tj as given for the purpose of estimating U .,3. Definitions and examples,[0],[0]
We later discuss how we to leverage our estimator of U to optimize the tj’s in order to maximize the number of new discoveries.,3. Definitions and examples,[0],[0]
"Note that in general, the nj’s and tj’s can differ arbitrarily across the populations.
",3. Definitions and examples,[0],[0]
A particularly important statistic is U = the total number of new elements in {Y ji } that are not observed in the period one samples {Xji }.,3. Definitions and examples,[0],[0]
A good estimator for this U quantifies the expected information gain of the second period.,3. Definitions and examples,[0],[0]
"In the one population setting, this statistic is the focus of Good-Toulmin and a large number of papers.",3. Definitions and examples,[0],[0]
"Other useful choices of U could be the number of distinct new elements that are observed at least twice in {Y ji }, which could be relevant if we want some reproducibility.
",3. Definitions and examples,[0],[0]
"Beyond estimating these single parameters, we could also hope to use the samples {Xji } to estimate the histogram of D1, ..., Dm.",3. Definitions and examples,[0],[0]
"The multi-population histogram, defined below, captures all of the information about the populations, other than the labels of the domain.",3. Definitions and examples,[0],[0]
Definition 3.1 (Multi-population histogram).,3. Definitions and examples,[0],[0]
"Given a collection of m distributions D1, . . .",3. Definitions and examples,[0],[0]
", Dm over a common domain Ω, the corresponding multi-population histogram H is a mapping from [0, 1]m \ 0m 7→ N ∪ {0}.",3. Definitions and examples,[0],[0]
"For each α = (α1, α2, . . .",3. Definitions and examples,[0],[0]
αm) ∈,3. Definitions and examples,[0],[0]
"[0, 1]m \ 0m, H(α) =",3. Definitions and examples,[0],[0]
|{y ∈ Ω,3. Definitions and examples,[0],[0]
"| Dj(y) = αj , 1 ≤ j ≤ m}|, where Di(y) is the probability mass of domain element y. in the ith distribution Di.
",3. Definitions and examples,[0],[0]
Any symmetric multi-population statistic—one that is invariant to permuting the labels of the domain—is a function of only the histogram.,3. Definitions and examples,[0],[0]
"Such statistics include distance metrics between the distributions/populations, measures of the entropy of the populations, and the number of new elements that one is likely to observe in a second batch of samples.",3. Definitions and examples,[0],[0]
"The multi-population histogram is also of intrinsic interest; in population genetics, H is exactly the joint frequency distribution of mutations, and reveals information about demographic history (e.g. historical variations in population size) and selective pressures.",3. Definitions and examples,[0],[0]
"One benefit of focusing on the histogram is that, while it does not contain as much information as the actual labeled distributions, it can often be accurately recovered even when given too few samples to learn the (labeled) distributions to any significant accuracy (Valiant & Valiant, 2011).
",3. Definitions and examples,[0],[0]
"Both for directly predicting U and estimating H , we rely on a label-less representation of the samples, termed the fingerprint of {Xji }.",3. Definitions and examples,[0],[0]
"The fingerprint of the samples is the analog of the histogram of the distributions, and captures all the information of {Xji } that is relevant for estimating symmetric statistics.",3. Definitions and examples,[0],[0]
Definition 3.2 (Multi-population fingerprint).,3. Definitions and examples,[0],[0]
"Given the samples {Xji }, its fingerprint is an m-dimensional tensor Φ whose i1...im-th entry, φi1...im , is the number of distinct elements observed exactly ij times in the samples from population j. Here each ij can range from 0 to nj .",3. Definitions and examples,[0],[0]
Example 3.1.,3. Definitions and examples,[0],[0]
"Suppose we have five samples from Population 1, (A,B,C,E, F ), and seven from Population 2, (A,B,D,E,E, F, F ).",3. Definitions and examples,[0],[0]
"The corresponding 2-dimensional fingerprint of this data is given by the following matrix:
0 1 2 0 · 1 0 1 1 2 2
The (1, 1) entry is 2 because A,B are observed once in each set of samples; the (1, 0) entry is 1 because exactly one element, C, is observed once in the samples from Population 1 and zero times in the samples from Population 2.",3. Definitions and examples,[0],[0]
"By convention, we omit the (0, 0) element.",3. Definitions and examples,[0],[0]
Unbiased estimator.,4. A linear estimator,[0],[0]
"Given the empirical fingerprints Φ and the extrapolation factors tj , j = 1, ...,m, we define the following estimator
Û = − ∑
i1,...,im: ∑ ij>0  m∏ j=1 (−tj)ij φi1...im .",4. A linear estimator,[0],[0]
"(1)
Û is a weighted alternating sum of the empirical fingerprints where the weights are determined by the extrapolation factors tj .
",4. A linear estimator,[0],[0]
Proposition 4.1.,4. A linear estimator,[0],[0]
"For any number of populations m, and any extrapolation factors tj ≥ 0, j = 1, ...m, Û is an unbiased estimator of U .
",4. A linear estimator,[0],[0]
"Proof of the proposition appears in Appendix 7.
",4. A linear estimator,[0],[0]
Û is linear in the fingerprint entries.,4. A linear estimator,[0],[0]
"Its computational cost is linear in the total number of period one samples, n = ∑ j nj , since there can be at most n non-zero fingerprint entries.",4. A linear estimator,[0],[0]
"To build more intuition for Û , we illustrate its application in two simple settings.
",4. A linear estimator,[0],[0]
Example 4.2.,4. A linear estimator,[0],[0]
"Consider the setting where all m distribution are identical, i.e. all the samples are drawn from the same discrete distribution D. Let tj = 1,∀j for simplicity.",4. A linear estimator,[0],[0]
"After rearranging terms, Û can be written as
Û = ∑ k=1
(−1)k+1  ∑
(i1,...,im): ∑ ij=k φi1...im  .",4. A linear estimator,[0],[0]
"Because the populations are identical, the sum in the parenthesis is just the number of elements that are observed k times from all the samples so far.",4. A linear estimator,[0],[0]
"Hence the general estimator Û reduces to the one dimensional Good-Toulmin estimator when all m populations are identical.
",4. A linear estimator,[0],[0]
Example 4.3.,4. A linear estimator,[0],[0]
Suppose the supports of the distributionsDi are disjoint.,4. A linear estimator,[0],[0]
Then the only possible non-zero fingerprint entries are φi1...im where exactly one of the ij is great than 0 and all the other ij’s are zero.,4. A linear estimator,[0],[0]
"For simplicity, assume tj = 1 for all j. Then Û = ∑k j=1 ∑ i(−1)k+1φki , where φki is the marginal fingerprint entry of the number of elements that are observed i times in population k. Hence
when the populations are disjoint, the expected number of new elements is the sum of the expected number of new elements in each population.",4. A linear estimator,[0],[0]
"When the populations have overlapping support, we have the nontrivial interaction terms due to the cross-population fingerprint entries.
",4. A linear estimator,[0],[0]
General weighted linear estimator.,4. A linear estimator,[0],[0]
"While Û is unbiased, its variance could be large if some of the extrapolation factors tj’s are greater than 1.",4. A linear estimator,[0],[0]
This is because the powers of tj appear in Eqn. 1.,4. A linear estimator,[0],[0]
"To address this issue, we introduce a general class of multi-population weighted linear estimators.",4. A linear estimator,[0],[0]
"ÛW = − ∑
ij : ∑ ij>0  m∏ j=1 (−tj)ij φi1,...,imW (i1, . . .",4. A linear estimator,[0],[0]
", im).
",4. A linear estimator,[0],[0]
"We focus on a particular weighting scheme, which is an extension of that introduced in (Orlitsky et al., 2016): W (i1, i2, . . .",4. A linear estimator,[0],[0]
im) =,4. A linear estimator,[0],[0]
P ( L ≥ ∑ j∈A ij ) where L ∼ Poi(r) and A = {j : tj > 1} are the populations that we would like to extrapolate beyond the original sample size.,4. A linear estimator,[0],[0]
"If tj ≤ 1 ∀j, then W = 1 and ÛW is just the unbiased estimator Û .",4. A linear estimator,[0],[0]
The Poisson rate r is a tuning parameter that determines the bias/variance tradeoff of ÛW .,4. A linear estimator,[0],[0]
"As r increases, all the weights approaches 1 and ÛW approaches the unbiased estimator Û .",4. A linear estimator,[0],[0]
"As r decreases, the fingerprint entries φi1...im with some large ij’s—which are also the terms with high variance—are weighted by a factor that is close to 0.",4. A linear estimator,[0],[0]
This reduces the total variance of ÛW at the cost of introducing bias.,4. A linear estimator,[0],[0]
We will see how to set r as a function of the nj’s and tj’s in order to minimize the overall estimation error.,4. A linear estimator,[0],[0]
"In the rest of the paper, unless otherwise specified, we will use ÛW to denote the multi-population linear estimator with Poisson weights.
",4. A linear estimator,[0],[0]
Performance guarantee of the weighted estimator.,4. A linear estimator,[0],[0]
"We use relative mean squared error, E",4. A linear estimator,[0],[0]
"[(
ÛW−U∑ njtj
)2] , to quan-
tify the performance of ÛW .",4. A linear estimator,[0],[0]
"This is a natural error metric, because ∑ njtj is the number of samples in period two and we care about how the error in the predicted number of new elements scales with the number of samples.",4. A linear estimator,[0],[0]
"Without loss of generality, we can relabel the populations so that t1 = maxjtj .",4. A linear estimator,[0],[0]
We are especially interested in the setting when t1 ≥ 1 (i.e. large extrapolation).,4. A linear estimator,[0],[0]
Proposition 4.4.,4. A linear estimator,[0],[0]
"Suppose t1 = maxj tj ≥ 1 and the Poisson rate is r = log( ∑ j nj(tj+1))
2t1 , then
E ( ÛW − U∑ njtj )2 ≤ (n1t1 +∑j nj n1t1 ) n −1/t1 1 .",4. A linear estimator,[0],[0]
"(2)
Remark 4.5 (log extrapolation factor).",4. A linear estimator,[0],[0]
"Suppose the ratio n1∑
j nj is bounded, then Prop. 4.4 guarantees that for
any > 0, we can achieve E",4. A linear estimator,[0],[0]
"[(
ÛW−U∑ njtj
)2] ≤ with
t1 = O(log n1/ log(1/ )).",4. A linear estimator,[0],[0]
This means that ÛW has low relative error even when the largest extrapolation factor t1 is logarithmic in its initial sample size n1.,4. A linear estimator,[0],[0]
Remark 4.6 (no dependence on m).,4. A linear estimator,[0],[0]
Note that the relative error in Eqn. 2 does not depend on the number of populations m.,4. A linear estimator,[0],[0]
This is somewhat surprising since the number of terms in ÛW potentially grows exponentially with m and the variance of each fingerprint entry φi1...im also increases as the number of population increases.,4. A linear estimator,[0],[0]
This population agnostic property of ÛW guarantees its accuracy even when m is arbitrarily large.,4. A linear estimator,[0],[0]
Remark 4.7 (lower bound).,4. A linear estimator,[0],[0]
Here we have focused on a specific form of the estimator ÛW where the weights W of the fingerprint entries correspond to the tail probability of Poisson distributions.,4. A linear estimator,[0],[0]
A natural question is whether there exists a different form of the weights or a different estimator altogether that can consistently be more accurate than our current ÛW .,4. A linear estimator,[0],[0]
"The answer is essentially no due to the following lower bound for one population extrapolation (Orlitsky et al., 2016; Valiant & Valiant, 2011): There exists universal constants c, c′ such that for all estimators Û , if the extrapolation factor t > c, then ∃ distribution such
that E",4. A linear estimator,[0],[0]
"[(
Û−U nt )2] >",4. A linear estimator,[0],[0]
∼ n−c ′/t.,4. A linear estimator,[0],[0]
"Here n is the number of sam-
ples drawn from this distribution in period one.",4. A linear estimator,[0],[0]
"This lower bound implies that in order to guarantee that the relative error is less than in general, the extrapolation factor can be at most O(log n/",4. A linear estimator,[0],[0]
"log(1/ )), matching Prop. 4.4.
",4. A linear estimator,[0],[0]
Outline of the proof of Prop. 4.4 (detailed analysis is in the Appendix).,4. A linear estimator,[0],[0]
"To analyze the relative error, we separately quantify the bias and variance of ÛW in terms of nj , tj , r. Lemma 4.8 (Bias).",4. A linear estimator,[0],[0]
"Let r denote the rate of the Poisson weights, then
∣∣∣E[ÛW − U ]∣∣∣ ≤ ∑ j∈A nj(tj + 1)  e−r Lemma 4.9 (Variance).",4. A linear estimator,[0],[0]
"Without loss of generality, let t1 = maxj tj and suppose t1 ≥ 1 then
Var(ÛW − U) ≤ ∑ nje 2r(t1−1) + ∑ j njtj .
To obtain the optimal r given in the statement of Prop. 4.4, we set r to balance the squared bias and variance.",4. A linear estimator,[0],[0]
"While we have a linear estimator for the number of unseen elements in a new sample, it is challenging to con-
struct good estimators of other statistics (e.g. number of new elements observed ≥ 2) directly from the fingerprints.",5. Estimating the multi-population frequency distribution,[0],[0]
"As discussed in Sec. 3, we can also take the less direct approach of first trying to estimating the true underlying multi-population histogram.",5. Estimating the multi-population frequency distribution,[0],[0]
"Given an accurate reconstruction of this underlying histogram, we can then estimate any symmetric statistic of the future samples.",5. Estimating the multi-population frequency distribution,[0],[0]
"We discuss some of the uses of such a representation in Section 5.
",5. Estimating the multi-population frequency distribution,[0],[0]
"Recovering the frequency distribution The core of our algorithm to recover the multi-population histogram is a natural extension of the single population algorithm presented in Valiant & Valiant (2011; 2013).
",5. Estimating the multi-population frequency distribution,[0],[0]
Estimating the multi-population histogram: Core Approach.,5. Estimating the multi-population frequency distribution,[0],[0]
"Input: Multi-population fingerprint Φ of samples, Output: Two estimates, Ĥcounts and Ĥll of histogram corresponding to the distributions underlying fingerprint Φ.
• Compute Ĥcounts and Ĥll minimizing the following expressions:
Ĥcounts = arg min H ∑ i 1√ 1 +",5. Estimating the multi-population frequency distribution,[0],[0]
"Φi |Φi − Φ̂(H)i|.
Ĥll = arg max H ∑ i log poi(Φi, Φ̂(H)i),
Where [Φ̂(H)]i = ∑ α H(α) m∏ j=1",5. Estimating the multi-population frequency distribution,[0],[0]
"bino(αj , nj , ij).
",5. Estimating the multi-population frequency distribution,[0],[0]
The intuition behind these two optimization problems is the following.,5. Estimating the multi-population frequency distribution,[0],[0]
"The histogram corresponding to a set of distributions is an unlabeled representation of the underlying distributions, hence it makes intuitive sense to try to recover the histogram that maximizes the likelihood of the unlabeled representation of the samples, namely the fingerprint Φ. Recent work (Acharya et al., 2016) provided rigorous support for this intuition.",5. Estimating the multi-population frequency distribution,[0],[0]
"In general, however, this likelihood might be difficulty to compute.",5. Estimating the multi-population frequency distribution,[0],[0]
"Nevertheless, an efficiently computable proxy for this likelihood can be obtained by treating the distribution of the fingerprint, corresponding to a histogram H , as a product distribution, with Φi1,...,im distributed according to the Poisson distribution with appropriate expectation EH",5. Estimating the multi-population frequency distribution,[0],[0]
"[Φi1,...,im ].",5. Estimating the multi-population frequency distribution,[0],[0]
"The recent central limit theorem for “Poisson Multinomials” from (Valiant & Valiant, 2011) provides at least some corroboration for the reasonableness of having a proxy for the log-likelihood that decomposes linearly across the different elements of Φ. The motivation for the 1√
1+Φi scaling
on the first proxy likelihood function is that this expression penalizes discrepancies between the observed and expected
fingerprint entries according to a rough approximation of the standard deviation of that fingerprint entry, as the variance of a Poisson random variable is equal to its expectation, and the observed fingerprint entry is an approximation for the expected fingerprint entry given the true underlying histogram.
",5. Estimating the multi-population frequency distribution,[0],[0]
"The work (Valiant & Valiant, 2013) focused on recovering Ĥcounts, as this optimization problem can be formulated as a linear program, whose variables correspond to a fine discretization of the potential support of the histogram.",5. Estimating the multi-population frequency distribution,[0],[0]
"Unfortunately, in the present multi-distribution setting, the number of variables required by this linear programming approach would scale exponentially with the number of distributions in question.",5. Estimating the multi-population frequency distribution,[0],[0]
"Even for fingerprints derived from modest-sized samples from two distributions, the resulting linear program becomes impractical.
",5. Estimating the multi-population frequency distribution,[0],[0]
"Instead of pursuing the linear programming based approach, we instead propose a black-box optimization approach to finding a histogram that optimizes either of the two proxy likelihood functions.",5. Estimating the multi-population frequency distribution,[0],[0]
"In this optimization approach, the dimensionality of the optimization problem is specified by the user, and corresponds to the number of (i1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", im) tuples for which the returned histogram Ĥ is nonzero.",5. Estimating the multi-population frequency distribution,[0],[0]
"Denoting this quantity by s, the resulting optimization problem can be regarded as the problem of specifying s vectors (h1, α1,1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", α1,m), . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", (hs, αs,1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", αs,m).",5. Estimating the multi-population frequency distribution,[0],[0]
"These s vectors are then interpreted as a histogram H with H(αj) = hj for all j ∈ {1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", s}, and H(α) = 0 for all other vectors α.
",5. Estimating the multi-population frequency distribution,[0],[0]
"The one additional modification that leads to a substantial improvement in runtime is to only evaluate the proxy likelihood expressions for fingerprint entries Φi1,...,im ≥ 2.",5. Estimating the multi-population frequency distribution,[0],[0]
The intuition for this is two-fold.,5. Estimating the multi-population frequency distribution,[0],[0]
"First, the number of vectors (i1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", im) for which Φi1,...,im = 0 will scale exponentially with m, as opposed to scaling as some parameter of the sample sizes; this is clearly undesirable.",5. Estimating the multi-population frequency distribution,[0],[0]
"Second, given that we wish to avoid evaluating the contribution to the proxy likelihood from fingerprint entries that are zero, we must now be careful in dealing with fingerprint entries that are equal to 1.",5. Estimating the multi-population frequency distribution,[0],[0]
"Suppose we have 1 element with true probability in and suppose we observe that fingerprint entry Φi = 1, and the other fingerprints near i are 0.",5. Estimating the multi-population frequency distribution,[0],[0]
"Since we are maximizing the likelihood that Φi = 1 (without taking into account the nearby 0 entries), we would assign roughly √ i elements to probability in which is undesirable.",5. Estimating the multi-population frequency distribution,[0],[0]
Removing the ones largely resolves this issue.,5. Estimating the multi-population frequency distribution,[0],[0]
"Note that the Φj = 2 entries do not cause as much of an issue, as such collisions are unlikely to occur in regions of the fingerprint in which there is not a significant number of domain elements.
",5. Estimating the multi-population frequency distribution,[0],[0]
"In this one-distribution example, a constraint on the total probability mass being 1 would resolve this issue, though
analogs of this issue in the multiple distribution setting cannot be resolved in this way.",5. Estimating the multi-population frequency distribution,[0],[0]
"Hence, we adopt the crude, but effective approach of viewing all the empirical fingerprint entries that are equal to 1 as being reflective of an element in the underlying set of distributions whose probability is close to the empirical probability of the corresponding element.",5. Estimating the multi-population frequency distribution,[0],[0]
"We summarize the complete algorithm below:
Estimating the multi-population histogram: Full Algorithm.",5. Estimating the multi-population frequency distribution,[0],[0]
"Input: Multi-population fingerprint Φ derived from samples from m distributions of respective sizes n1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", nm.",5. Estimating the multi-population frequency distribution,[0],[0]
"Output: Two estimates, Ĥcounts and Ĥll of histogram corresponding to the distributions underlying fingerprint Φ.
• Remove fingerprint entries that are 1, and add to empirical portion of histogram:
1.",5. Estimating the multi-population frequency distribution,[0],[0]
"Initialize m-distribution histogram Ĥemp to be identically zero.
",5. Estimating the multi-population frequency distribution,[0],[0]
2.,5. Estimating the multi-population frequency distribution,[0],[0]
"For each vector i = (i1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", im) such that Φi = 1, set Ĥemp( i1n1 , . .",5. Estimating the multi-population frequency distribution,[0],[0]
.,5. Estimating the multi-population frequency distribution,[0],[0]
", im nm ) = 1.
",5. Estimating the multi-population frequency distribution,[0],[0]
"• Compute Ĥcounts and Ĥll minimizing the following expressions:
Ĥcounts = arg min H ∑",5. Estimating the multi-population frequency distribution,[0],[0]
i:Φ(i)≥2 1√ 1 +,5. Estimating the multi-population frequency distribution,[0],[0]
"Φi |Φi − Φ̂(H)i|.
Ĥll = arg max H ∑",5. Estimating the multi-population frequency distribution,[0],[0]
"i:Φ(i)≥2 log poi(Φi, Φ̂(H)i.
",5. Estimating the multi-population frequency distribution,[0],[0]
"Where Φ̂(H)i = ∑ α H(α) m∏ j=1 bino(αj , nj , ij).
",5. Estimating the multi-population frequency distribution,[0],[0]
"Subject to the constraint that, together with Ĥemp, the total mass in all the distributions is 1.",5. Estimating the multi-population frequency distribution,[0],[0]
"Namely for all i ∈ {1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
",m},∑
α αiĤll(α) + ∑ α αiĤ∗(α) = 1.
• Return the concatenation of the empirical portion of the histogram and the portion returned by the optimization:",5. Estimating the multi-population frequency distribution,[0],[0]
"Ĥcount := Ĥcount + Ĥemp, and Ĥll := Ĥll + Ĥemp
Leveraging Ĥ for approximating the value of additional data.",5. Estimating the multi-population frequency distribution,[0],[0]
An accurate representation of the histogram corresponding to the multi-population distribution underlying a given set of observations can be leveraged to estimate a number of useful properties.,5. Estimating the multi-population frequency distribution,[0],[0]
"These properties include estimating the number of new domain elements that would
likely be seen given additional samples from the populations.",5. Estimating the multi-population frequency distribution,[0],[0]
"Specifically, given a histogram Ĥ , corresponding to m populations, we can estimate the expected number of distinct elements that will be observed in samples from the m populations of respective sizes n1, . . .",5. Estimating the multi-population frequency distribution,[0],[0]
", nm via the simple formula: E[num observed] = ∑ α Ĥ(α) ( 1− m∏ i=1",5. Estimating the multi-population frequency distribution,[0],[0]
(1− αi)ni ) .,5. Estimating the multi-population frequency distribution,[0],[0]
"(3)
An accurate approximation to the histogram can also be leveraged to answer many other questions about the populations that can not be readily addressed via the linear estimators of Section 4.",5. Estimating the multi-population frequency distribution,[0],[0]
"These include tasks such as estimating the amount of data that must be collected to capture, say, 99% of the mass of the distributions in question.",5. Estimating the multi-population frequency distribution,[0],[0]
Evaluating the weighted linear estimator for large m. We empirically evaluated the performance of the weighted linear estimator ÛW .,6. Experiments,[0],[0]
"The experiments were conducted for three types of distributions—Uniform, Dirichlet and Geometric—that are commonly used to evaluate extrapolation algorithms.",6. Experiments,[0],[0]
Each experiment contains m = 100 populations.,6. Experiments,[0],[0]
We have a total of 3000 distinct elements.,6. Experiments,[0],[0]
"In the Uniform setting, each population has support on 100 elements that are randomly sampled from the 3000.",6. Experiments,[0],[0]
"For Dirichlet, each population also has support on 100 random elements (from the 3000), and the weights on these 100 elements are sampled from a Dirichlet prior.",6. Experiments,[0],[0]
"For the Geometric experiments, each population corresponds to a random ordering of the 3000 elements and the k-th element is assigned probability ∝",6. Experiments,[0],[0]
(1− p)kp.,6. Experiments,[0],[0]
"In period one, ten samples are observed in each of the 100 populations.",6. Experiments,[0],[0]
"In period two, 95 randomly chosen populations have extrapolation factor t ∈",6. Experiments,[0],[0]
"[0, 1] and five populations have extrapolation factor 10t. This simulates the setting where we can obtain substantially more samples from a subset of the populations.
",6. Experiments,[0],[0]
"Figure 1(a, b, c) shows the results of the experiments for Uniform, Dirichlet(1) and Geometric with p = 0.05 respectively.",6. Experiments,[0],[0]
The results for other parameter settings are qualitatively similar.,6. Experiments,[0],[0]
The black curves indicate the true number of distinct new elements we expect to observe in period two by sampling from the true underlying distributions.,6. Experiments,[0],[0]
The red curves are the predictions of the weighted linear estimator (shaded regions indicate one standard deviation across 100 experiments).,6. Experiments,[0],[0]
"In all three settings, ÛW provides accurate estimate with low variance when the maximum extrapolation factor is relatively small (≤ 3).",6. Experiments,[0],[0]
"For Uniform and Geometric distributions, the accuracy is high up to 10 fold extrapolation.",6. Experiments,[0],[0]
"For Zipf, the bias is low but variance becomes large for the maximum extrapolation factor around 10.",6. Experiments,[0],[0]
"The downward bias in the predictions
is due to the weighting scheme.",6. Experiments,[0],[0]
"The relative error of the weighted estimator, ( ÛW−U∑ njtj )2 , is 0.09, 0.08 and 0.08 for the Uniform, Dirichlet and Geometric distributions when
the maximum extrapolation factor is 10.",6. Experiments,[0],[0]
"This confirms the theoretical results of Prop. 4.4 on the accuracy of the weighted linear estimator.
",6. Experiments,[0],[0]
Evaluating the histogram estimators.,6. Experiments,[0],[0]
We first validated the performance of Ĥcount and Ĥll on a three population setting with synthetic data.,6. Experiments,[0],[0]
"The true population consists of three uniform distributions over 200k elements, whose supports have 100k elements in common, and 100k elements unique to each distribution.",6. Experiments,[0],[0]
"In Figure 1(d), the x-axis corresponds to the number of samples we observe from each population, and the y-axis indicates the earthmover distance (EMD) between Ĥcount, Ĥll and the true histogram.",6. Experiments,[0],[0]
"As a baseline, we also compute the EMD between the empirical histogram of the observed samples and the true histogram.",6. Experiments,[0],[0]
Ĥcount and Ĥll performed roughly equally well and both are substantially better than the empirical estimator especially when the number of observed samples is small.,6. Experiments,[0],[0]
Figure 1(e) illustrates the extrapolation accuracy of our histogram estimators.,6. Experiments,[0],[0]
"We estimated Ĥcount and Ĥll using 16K from each population, and then used Eqn. 3 to estimate the number of unseen elements in additional samples.",6. Experiments,[0],[0]
"We tested two different settings: 1) when the additional samples are equally drawn from the three populations, and 2) a skewed mixture where 5/6 of the new samples are from population 1 and 1/12 each are drawn from population 2 and 3.",6. Experiments,[0],[0]
Ĥcount and Ĥll gave extremely accurate predictions.,6. Experiments,[0],[0]
"In comparison, the weighted linear estimator ÛW was accurate for the initial extrapolations but has downward bias when the extrapolation increases, consistent with Fig. 1(ac).
",6. Experiments,[0],[0]
"Additionally, we evaluate the performance of Ĥcount on a real dataset, in which we sampled words from three books– Hamlet (32K total words), Treasure Island (40K) and The Sun Also Rises (72K).",6. Experiments,[0],[0]
We used the true word frequencies (over the entire text) as the true histogram.,6. Experiments,[0],[0]
We sampled a small number of words (equal in all books) either randomly or from a contiguous block of text and used Ĥcounts to predict the total number of distinct words in total in all three books.,6. Experiments,[0],[0]
"In Figure 2(a), the red line is the true value, and blue and green lines are predictions based on Ĥcount derived from samples of either random words, or words occurring in a random contiguous block of text, respectively.",6. Experiments,[0],[0]
We obtain accurate estimates using a fraction of words (10K from each book).,6. Experiments,[0],[0]
"The estimates based on independent samples of words is more accurate than that based on contiguous blocks of text—likely due to correlation in words that occur near each other.
",6. Experiments,[0],[0]
Optimizing discovery rate.,6. Experiments,[0],[0]
"Given the estimated histogram Ĥcount or Ĥll, we can optimize the allocation of new samples across the populations to maximize the number of unseen elements we can expect to discover given a bound on ∑ j tjnj .",6. Experiments,[0],[0]
"To illustrate, we obtained genome sequencing data of 45K individuals from the Exome Aggregation Consortium (Lek et al., 2016).",6. Experiments,[0],[0]
"The individuals come from four ancestries: Europeans, Africans, East Asians and Latinos.",6. Experiments,[0],[0]
"We used all the observed mutations from the 45K
samples to construct a four population frequency distribution.",6. Experiments,[0],[0]
"For the experiment, we treat this as the ground truth and sampled 105 mutations from each population to obtain “seen” data.",6. Experiments,[0],[0]
"Suppose we have budget to sample 3 × 106 variants (10 fold extrapolation from current sample size), how should we allocate these new samples across the four populations in order to maximize the number of new variants discovered?",6. Experiments,[0],[0]
We use Ĥcount to predict the extrapolation curves for three scenarios: 1.,6. Experiments,[0],[0]
all the samples are allocated to Europeans (current genomic studies are heavily enriched of Europeans); 2.,6. Experiments,[0],[0]
the samples are evenly allocated across the four populations; 3) we explicitly optimize the factors tj using Ĥcount.,6. Experiments,[0],[0]
"The dotted curves in Fig. 2(b) correspond to the predictions, and the solid curves are the actual numbers using the true distribution, showing good agreement.",6. Experiments,[0],[0]
Optimization using Ĥcount led to 10 % increase in the number of new variants discovered.,6. Experiments,[0],[0]
This is a simplistic example (there are many other factors in the design of real cohorts) but it illustrate the potential power in having multi-population histogram estimates.,6. Experiments,[0],[0]
"In Appendix Fig. 3, we also show that Ĥcount gives accurate predictions for a different statistic—the number of new variants we expect to find at least twice in the new samples.",6. Experiments,[0],[0]
We introduce and formalize the problem of multipopulation unseen estimation.,7. Discussion,[0],[0]
We provide a weighted linear estimator for the number of new elements and a general optimization algorithm to estimate the multi-population histogram.,7. Discussion,[0],[0]
These two approaches have complementary strength.,7. Discussion,[0],[0]
The weighted linear estimator ÛW specifically estimates the number of unseen elements.,7. Discussion,[0],[0]
"It’s accuracy is independent of the number of populations, m, and it is worst-case optimal.",7. Discussion,[0],[0]
This can be a good method especially when m is large and the extrapolation factor is small compared to log of the number of observed samples.,7. Discussion,[0],[0]
"When the extrapolation is larger, however, ÛW is consistently downward biased due to its variance-reducing weights.",7. Discussion,[0],[0]
"For relatively small number of populations (m = 2, 3, 4) and larger extrapolation factors, the unseen predictions of our histogram estimators, Ĥcount and Ĥll are significantly more accurate than ÛW .",7. Discussion,[0],[0]
"While both likely have comparable worst-case performance, the linear estimator nearly always incurs this worst-case loss and is largely incapable of extrapolating beyond this worst-case logarithmic factor.",7. Discussion,[0],[0]
"In contrast, the histogram-based estimators seem to perform well for much larger extrapolation factors on all of the distributions that we considered.",7. Discussion,[0],[0]
"Ĥcount and Ĥll are computationally more expensive than ÛW , but are still tractable for many applications—each run of our experiments took less than 20 minutes on a single laptop.",7. Discussion,[0],[0]
Gregory Valiant’s contributions were supported by NSF CAREER CCF-1351108 and a Sloan Research Fellowship.,Acknowledgments,[0],[0]
James Zou is a Chan Zuckerberg Biohub investigator and is supported by NSF CISE-1657155.,Acknowledgments,[0],[0]
Proof of Prop. 4.1.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"For each element x ∈ X , let λx,j = njpx,j , where px,j is the probability of x in population j. We have
E[U ]",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"= ∑ x e− ∑ j λx,j ( 1− e− ∑ j tjλx,j ) .
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
The first term in the sum is the probability that x is not observed in period one and the second term is the probability that x is observed at least once in period two.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Taylor expand the second term followed by Binomial expansion gives
E[U ]",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"= ∑ x e− ∑ j λx,j ∞∑ i=1",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"(−1)i+1 ( ∑ j tjλx,j) i i!
=",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− ∑ x e− ∑ j λx,j ∑ i1,...,im: ∑ ij>0 m∏ j=1 (−tjλx,j)ij ij !
=",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− ∑
i1,...,im: ∑ ij>0 ∑ x e− ∑ j λx,j m∏ j=1 (−tjλx,j)ij ij !
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"= − ∑
i1,...,im: ∑ ij>0  m∏ j=1 (−tj)ij E[φi1...im ].
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"It’s easy to see that Û is an unbiased estimator of the last expression.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Weighting the fingerprints reduces the variance of the estimator at the cost of introducing bias.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
We analyze the bias and variance of ÛW separately.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"The proof follows the strategy of the analysis for the one population setting in (Orlitsky et al., 2016).
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Lemma A.1 (Lemma 4.8 restated).,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Let n = ∑m j=1 nj denote the total number of samples in period one and r denote the
rate of the Poisson weights, then ∣∣∣E[ÛW − U ]∣∣∣ ≤ ∑ j∈A nj(tj + 1)  e−r Proof.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"For each element x, its contribution to E[ÛW ] can be written as
−e− ∑m j=1",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"λx,j  ∑ i1,...,im m∏ j=1 (−tj)ij λ ij x,j ij !",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
P L ≥∑ j∈A ij,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− 1 
= −e− ∑m",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
j=1,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"λx,j  ∑ ij :j 6∈A ∏",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"j 6∈A (−tj)ij λ ij x,j ij !  ∑",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"ij :j∈A ∏ j∈A (−tj)ij λ ij x,j ij !",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
P L ≥∑ j∈A ij,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− 1 
= −e− ∑m",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
j=1,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"λx,j e−∑j 6∈A tjλx,j ∞∑ i=0 P(L ≥",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
i),A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
i!,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"−∑ j∈A tjλx,j i",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− 1 
We use the following two facts from (Orlitsky et al., 2016).
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Fact 1 For all y > 0,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"and for any random variable L,∣∣∣∣∣− ∞∑ i=0 P(L ≥ i)",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
i!,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
(−y)i + e−y ∣∣∣∣∣ ≤,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
maxs≤y ∣∣∣∣E,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
[ (−s)LL! ],A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"∣∣∣∣ (1− e−y) .
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Fact 2,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"If L ∼ Poi(r), then ∣∣∣∣E [ (−s)LL!",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"]∣∣∣∣ ≤ e−r.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Therefore, the contribution of x to E[ÛW",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− U ] is
e− ∑m j=1 λx,j− ∑ j 6∈A tjλx,j − ∞∑ i=0 P(L ≥ i)",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
i!,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"−∑ j∈A tjλx,j i + e−∑j∈A tjλx,j  ≤ (1− e−∑j∈A tjλx,j) e−r
where we used Facts 1 and 2 with y = ∑ j∈A tjλx,j and the fact that e − ∑m j=1 λx,j− ∑ j 6∈A tjλx,j ≤ 1.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Now summing over x ∈ X , we have∣∣∣E[ÛW",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"− U ]∣∣∣ ≤ ∑ x ( 1− e− ∑ j∈A tjλx,j ) e−r
≤ ∑ x ( 1− e− ∑ j∈A(tj+1)λx,j )",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"e−r
= ∑ x",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"[ e− ∑ j∈A λx,j ( 1− e− ∑ j∈A tjλx,j ) + 1− e− ∑ j∈A λx,j ] e−r
= ( E[UA] + E[ΦA+] )",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"e−r
≤ ∑ j∈A nj(tj + 1)  e−r where ΦA+ is the total number of distinct elements observed in period one for subpopulations j ∈",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"A and UA is the number of new elements observed in period two for j ∈ A.
Lemma 4.8 quantifies the bias of the weighted estimator.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Next we quantify its variance.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Lemma A.2 (Lemma 4.9 restated).,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Without loss of generality, let t1 = maxj tj and suppose t1 ≥ 1 then
Var(ÛW − U) ≤ ne2r(t1−1) + ∑ j njtj .
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
Proof.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Let Nx,j be the random variable corresponding to the number of times x is found in population j during period one.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Let N ′x,j be the random variable corresponding to the number of times x is found in population j during period two.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Define h(i1, ..., im) =",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
− ∏m j=1(−tij )ijP,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"( L ≥ ∑ j∈A ij ) .
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"For every element x, its contribution to Var(ÛW − U) is
Var  ∑ i1,...,im ∏ j 1Nx,j=ij h(i1, ..., im)− ∏ j 1Nx,j=0",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"1−∏ j 1N ′x,j=0  ≤",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"E
 ∑ i1,...,im ∏ j 1Nx,j=ij h(i1, ..., im)− ∏ j 1Nx,j=0",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"1−∏ j 1N ′x,j=0",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"2
= E  ∑ i1,...,im ∏ j 1Nx,j=ij h(i1, ..., im)2 + ∏",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"j 1Nx,j=0",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"1−∏ j 1N ′x,j=0  .
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
The last equality follows because the cross-term vanishes since the events,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Nx,j = 0,∀j and Nx,j = ij , ∑ j ij > 0 are disjoint.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Summing over all x gives
Var(ÛW − U) ≤",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
E[Φ+],A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"sup i1,...,im h(i1, ..., im) 2 + E[U ]",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"(4)
≤ n sup i1,...,im h(i1, ..., im) 2 + ∑ j njtj .",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"(5)
Moreover we have
|h(i1, ..., im)| = ∏ j t ij j P L ≥∑ j∈A ij  ≤",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"t ∑ j∈A ij
1 P L ≥∑ j ij  ≤ er(t1−1)
where we have used the following fact:
Fact 3",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"If L ∼ Poi(r) and t ≥ 1, then for all i > 0
tiP(L ≥ i) ≤ er(t−1).
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Note that only tmax ≥ 1 is assumed here; the other tj’s could be less than 1.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Putting the last two lemmas together, we have
Lemma A.3.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Let t1 = maxj tj ≥ 1, then
E",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
[ (ÛW − U)2 ] ≤ ne2r(t1−1),A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"+ ∑ j njtj + ∑ j∈A nj(tj + 1) 2 e−2r
Because t1 ≥ 1, Lemma A.3 implies that
E [ (ÛW − U)2 ] ≤ (n+ ∑ j njtj)e 2r(t1−1) +",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"(n+ ∑ j njtj) 2e−2r.
",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"The two terms on the RHS are equal when r = log(n+
∑ j njtj)
2tmax .",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"Using this value of r, we have
E ( ÛW − U∑ njtj )2 ≤ (n+ t1n̄ tn̄ )2 (n+ t1n̄) −1/t1
≤ ( n+ t1n̄
t1n̄
)2 n̄−1/t1
≤ ( n+ t1n1 t1n1 )",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"2 n −1/t1 1
where n̄ ≡ ∑ j njtj/t1.",A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
This completes the proof of Prop. 4.4.,A. Proof of Prop. 4.1 and Prop. 4.4,[0],[0]
"We define a natural distance metric on multi-population histograms, which is a measure of the extent to which the corresponding distributions are similar, up to a relabeling of the elements:
Definition B.1.",B. Multi-population Earth Mover’s Distance,[0],[0]
"Given two m-population histograms, H,H ′, the multi-population earthmover distance dW (H,H ′) is defined as the minimum over all schemes of moving the histogram elements in H to yield H ′, where the cost of moving c histogram elements from α ∈",B. Multi-population Earth Mover’s Distance,[0],[0]
"[0, 1]m to α′ is c 12m‖α−",B. Multi-population Earth Mover’s Distance,[0],[0]
α ′‖1 = ∑m i=1,B. Multi-population Earth Mover’s Distance,[0],[0]
|αi,B. Multi-population Earth Mover’s Distance,[0],[0]
− α′i|.,B. Multi-population Earth Mover’s Distance,[0],[0]
"To ensure that such a scheme exists, we regard there being an infinitude of elements that occur with probability zero in all populations, H(0) = H ′(0)",B. Multi-population Earth Mover’s Distance,[0],[0]
"=∞.
Note that for all pairs of histograms H,H ′",B. Multi-population Earth Mover’s Distance,[0],[0]
"it holds that dW (H,H ′) ∈",B. Multi-population Earth Mover’s Distance,[0],[0]
"[0, 1], with dW (H,H ′) = 0",B. Multi-population Earth Mover’s Distance,[0],[0]
"if and only if the distributions corresponding to H and H ′ are identical, up to relabeling the domain elements.",B. Multi-population Earth Mover’s Distance,[0],[0]
"The following example illustrates the above definition:
Example B.1.",B. Multi-population Earth Mover’s Distance,[0],[0]
"Consider a 3 population distribution corresponding to a three uniform distributions over 2n elements, where n of the elements are common to all 3 populations, and the other elements are unique.",B. Multi-population Earth Mover’s Distance,[0],[0]
"This corresponds to histogram H defined by H(1/2n, 1/2n, 1/2n) = n, H(1/2n, 0, 0) = n, H(0, 1/2n, 0) = n, H(0, 0, 1/2n) =",B. Multi-population Earth Mover’s Distance,[0],[0]
n. Consider a second histogram H ′,B. Multi-population Earth Mover’s Distance,[0],[0]
"corresponding to three uniform distributions over a common set of n elements, H ′(1/n, 1/n, 1/n) = n, and H ′(α) = 0 for all α 6= (1/n, 1/n, 1/n).",B. Multi-population Earth Mover’s Distance,[0],[0]
"The EMD
dW (H,H ′) =
1
2 · 3
( n 3
2n + 3n
1
2n
)",B. Multi-population Earth Mover’s Distance,[0],[0]
"= 1
2 ,
Since we can make H ′ from H by moving n histogram elements from (1/2n, 1/2n, 1/2n) to (1/n, 1/n, 1/n) at a per-unitcost of ‖( 12n , 1 2n , 1 2n )",B. Multi-population Earth Mover’s Distance,[0],[0]
"− ( 1 n , 1 n , 1 n )‖1 = 3 2n , and then moving the remaining 3n elements of H to (0, 0, 0)",B. Multi-population Earth Mover’s Distance,[0],[0]
at a per-unit-cost of 1/2n.,B. Multi-population Earth Mover’s Distance,[0],[0]
"We tested the prediction accuracy of Ĥcount on a different statistic: the number of elements we expect to find at least twice in the new samples, see Fig. 3.",C. Additional experiments,[0],[0]
"Given samples from a distribution, how many new elements should we expect to find if we continue sampling this distribution?",abstractText,[0],[0]
"This is an important and actively studied problem, with many applications ranging from unseen species estimation to genomics.",abstractText,[0],[0]
"We generalize this extrapolation and related unseen estimation problems to the multiple population setting, where population j has an unknown distributionDj from which we observe nj samples.",abstractText,[0],[0]
We derive an optimal estimator for the total number of elements we expect to find among new samples across the populations.,abstractText,[0],[0]
"Surprisingly, we prove that our estimator’s accuracy is independent of the number of populations.",abstractText,[0],[0]
We also develop an efficient optimization algorithm to solve the more general problem of estimating multi-population frequency distributions.,abstractText,[0],[0]
We validate our methods and theory through extensive experiments.,abstractText,[0],[0]
"Finally, on a real dataset of human genomes across multiple ancestries, we demonstrate how our approach for unseen estimation can enable cohort designs that can discover interesting mutations with greater efficiency.",abstractText,[0],[0]
Estimating the unseen from multiple populations,title,[0],[0]
Probabilistic modeling is a flexible approach to analyzing structured data.,1. Introduction,[0],[0]
Three steps define the approach.,1. Introduction,[0],[0]
"First, we specify a model; this captures our structural assumptions about the data.",1. Introduction,[0],[0]
"Then, we infer the hidden structure; this means computing (or approximating) the posterior.",1. Introduction,[0],[0]
"Last, we evaluate the model; this helps build better models down the road (Blei, 2014).
",1. Introduction,[0],[0]
How do we evaluate models?,1. Introduction,[0],[0]
"Decades of reflection have led to deep and varied forays into model checking, comparison, and criticism (Gelman et al., 2013).",1. Introduction,[0],[0]
"But a common theme permeates all approaches to model evaluation: the desire to generalize well.
",1. Introduction,[0],[0]
"In machine learning, we traditionally use two complementary tools: predictive accuracy and cross-validation.",1. Introduction,[0],[0]
"Predictive accuracy is the target evaluation metric. Cross-
1Columbia University, New York City, USA.",1. Introduction,[0],[0]
"Correspondence to: Alp Kucukelbir <alp@cs.columbia.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 34th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
validation captures a notion of generalization and justifies holding out data.",1. Introduction,[0],[0]
"This simple combination has fueled the development of myriad probabilistic models (Bishop, 2006; Murphy, 2012).
",1. Introduction,[0],[0]
Does predictive accuracy tell the whole story?,1. Introduction,[0],[0]
The predictive accuracy of an observation is its per-datapoint likelihood averaged over the posterior.,1. Introduction,[0],[0]
"In this sense, predictive accuracy reports a mean value for each datapoint; it ignores how each per-datapoint likelihood varies with respect to the posterior.
",1. Introduction,[0],[0]
Main idea.,1. Introduction,[0],[0]
"We propose to evaluate probabilistic models through the idea of posterior dispersion, analyzing how each datapoint fares in relation to posterior uncertainty around the hidden structure.",1. Introduction,[0],[0]
"To capture this, we propose a family of posterior dispersion indices (PDI).",1. Introduction,[0],[0]
"These are per-datapoint quantities, each a variance to mean ratio of the datapoint’s likelihood with respect to the posterior.",1. Introduction,[0],[0]
"A PDI highlights observations whose likelihoods exhibit the most uncertainty under the posterior.
",1. Introduction,[0],[0]
Consider a model p.x; / and the likelihood of a datapoint p.xn j /.,1. Introduction,[0],[0]
It depends on some hidden structure that we seek to infer.,1. Introduction,[0],[0]
"Since is random, we can view the likelihood of each datapoint as a random variable.",1. Introduction,[0],[0]
Predictive accuracy reports the average likelihood of each xn with respect to the posterior p. jx/.,1. Introduction,[0],[0]
But this ignores how the likelihood changes under the posterior.,1. Introduction,[0],[0]
"How can we capture this uncertainty and compare datapoints to each other?
To answer this, we appeal to various forms of dispersion, such as the variance of the likelihood under the posterior.",1. Introduction,[0],[0]
"We propose a family of dispersion indices in Section 2.2; they have the following form:
PDI D variance of likelihood under posterior
mean of likelihood under posterior
D V jxŒp.xn j /
E jxŒp.xn j / :
PDIs compliment predictive accuracy.",1. Introduction,[0],[0]
Here is a mental picture.,1. Introduction,[0],[0]
Consider a nuclear power plant where we monitor the temperature of a pool of water.,1. Introduction,[0],[0]
"We train a probabilistic model; the posterior represents our uncertainty around some safe temperature, say 80 degrees.",1. Introduction,[0],[0]
Suppose we receive a high measurement thigh (Figure 1).,1. Introduction,[0],[0]
"Its likelihood varies rapidly
across plausible posterior values for t .",1. Introduction,[0],[0]
"This datapoint is reasonably well modeled, but is sensitive to the posterior.",1. Introduction,[0],[0]
Now imagine the thermostat breaks and we receive a zero measurement tzero.,1. Introduction,[0],[0]
"This zero datapoint is poorly modeled.
",1. Introduction,[0],[0]
Both datapoints may have similar predictive accuracy values under the model.,1. Introduction,[0],[0]
(See Section 2.3 for how this can occur.),1. Introduction,[0],[0]
But the high measurement is different than the zero measurement.,1. Introduction,[0],[0]
"A PDI differentiates these measurements by considering not only their predictive accuracy scores, but also how their per-datapoint likelihoods vary with respect to the posterior.
",1. Introduction,[0],[0]
"Section 3 presents an empirical study of model mismatch in three real-world examples: voting preferences, supermarket shopping, and population genetics.",1. Introduction,[0],[0]
"In each case, a PDI provides insight beyond predictive accuracy and highlights potential directions for improvement.
",1. Introduction,[0],[0]
Related work.,1. Introduction,[0],[0]
"This paper relates to a constellation of ideas from statistical model criticism.
",1. Introduction,[0],[0]
"The first connection is to analysis of variance: PDI bears similarity to ANOVA, which is a frequentist approach to evaluating explanatory variables in linear regression (Davison, 2003).",1. Introduction,[0],[0]
Gelman et al. (1996) cemented the idea of studying predictive accuracy of probabilistic models at the data level; Vehtari et al. (2012) and Betancourt (2015) give up-to-date overviews of these ideas.,1. Introduction,[0],[0]
"PDIs add to this body of work by considering the variance of each datapoint in context of its predictive accuracy.
",1. Introduction,[0],[0]
The second connection is to model comparison.,1. Introduction,[0],[0]
"Recent research, such as Gelman et al. (2014), Vehtari et al. (2014) and Piironen & Vehtari (2015), explore the relationship between cross validation and information criteria, such as the widely applicable information criterion (WAIC)",1. Introduction,[0],[0]
"(Watanabe, 2010; Vehtari et al., 2016).",1. Introduction,[0],[0]
"WAIC offers an intuitive connection to cross validation (Vehtari & Lampinen, 2002; Watanabe, 2015); we draw inspiration from it in this paper too.",1. Introduction,[0],[0]
"However our focus is on evaluation at the datapoint level, not at the dataset level.",1. Introduction,[0],[0]
"In this sense, PDIs and information criteria are complimentary tools.
",1. Introduction,[0],[0]
"The third connection is to a body of work from the ma-
chine learning community.",1. Introduction,[0],[0]
Gretton et al. (2007) and Chwialkowski et al. (2016) developed effective kernel-based methods for independence and goodness-of-fit tests.,1. Introduction,[0],[0]
"Recently, Lloyd & Ghahramani (2015) visualized smooth regions of data space that the model fails to explain.",1. Introduction,[0],[0]
"In contrast, we focus directly on the datapoints, which can live in high-dimensional spaces that may be difficult to visualize.
",1. Introduction,[0],[0]
A final connection is to scoring rules.,1. Introduction,[0],[0]
"While the literature on scoring rules originally focused on probability forecasting (Winkler, 1996; Dawid, 2006), recent advances draw new connections to decision theory, divergence measures, and information theory (Dawid & Musio, 2014).",1. Introduction,[0],[0]
We discuss how PDIs fit into this picture in the conclusion.,1. Introduction,[0],[0]
A posterior dispersion index (PDI) highlights datapoints that exhibit the most uncertainty with respect to the hidden structure of a model.,2. Posterior Dispersion Indices,[0],[0]
Here is the road map for this section.,2. Posterior Dispersion Indices,[0],[0]
A small case study illustrates how a PDI gives more insight beyond predictive accuracy.,2. Posterior Dispersion Indices,[0],[0]
"Definitions, theory, and another small analysis give further insight; a straightforward algorithm leads into the empirical study.",2. Posterior Dispersion Indices,[0],[0]
Hayden (2005) considers the number of days each U.S. president stayed in office; he submits that 44% of presidents may be outliers.,2.1. 44% outliers?,[0],[0]
Figure 2 plots the data.,2.1. 44% outliers?,[0],[0]
One-term presidents stay in office for around 1460 days; two-term presidents approximately double that.,2.1. 44% outliers?,[0],[0]
"Yet many presidents deviate from this “two bump” trend.
",2.1. 44% outliers?,[0],[0]
A reasonable model for such data is a mixture of negative binomial distributions.1 Consider the parameterization of the negative binomial with mean and variance C 2= .,2.1. 44% outliers?,[0],[0]
Posit gamma priors on the (non-negative) latent variables.,2.1. 44% outliers?,[0],[0]
"Set the prior on to match the mean and variance of the data (Robbins, 1964).",2.1. 44% outliers?,[0],[0]
Choose an uninformative prior on .,2.1. 44% outliers?,[0],[0]
"Three mixtures make sense: two for the typical trends and one for the rest.
",2.1. 44% outliers?,[0],[0]
"1 A Poisson likelihood is too underdispersed.
",2.1. 44% outliers?,[0],[0]
"The complete model is
p. / D Dirichlet.",2.1. 44% outliers?,[0],[0]
"I ˛ D .1; 1; 1//
p. / D 3Y kD1 Gam.",2.1. 44% outliers?,[0],[0]
k,2.1. 44% outliers?,[0],[0]
"I mean and variance
matched to that of data/
p. /",2.1. 44% outliers?,[0],[0]
D 3Y kD1 Gam. k,2.1. 44% outliers?,[0],[0]
"I a D 1; ˇ D 0:01/
p.xn j ; ; / D 3X kD1 kNB.xn",2.1. 44% outliers?,[0],[0]
"I k ; k/:
Fitting this model gives posterior mean estimates b D .1461; 2896; 1578/ with correspondingb D .470; 509; 1.3/. The first two clusters describe the two typical term durations, while the third (highlighted in bold red) is a dispersed negative binomial that attempts to describe the rest of the data.
",2.1. 44% outliers?,[0],[0]
"We compute a PDI (defined in Section 2.2) and the posterior predictive density for each president p.xn jx/. Figure 3 compares both metrics and sorts the presidents according to the PDI.
",2.1. 44% outliers?,[0],[0]
"Some presidents are clear outliers: Harrison [31: natural death], Roosevelt [4452: four terms], and Garfield [199: assassinated].",2.1. 44% outliers?,[0],[0]
"However, there are three presidents with worse predictive accuracy than Harrison: Coolidge, Nixon, and Johnson.",2.1. 44% outliers?,[0],[0]
"A PDI differentiates Harrison from these three because his likelihood is varying rapidly with respect to the dispersed negative binomial cluster.
",2.1. 44% outliers?,[0],[0]
This PDI also calls attention to McKinley [1655: assassinated] and Arthur,2.1. 44% outliers?,[0],[0]
"[1260: succeeded Garfield], because they are close to the sharp negative binomial cluster at 1460 but not close enough to have good predictive accuracy.",2.1. 44% outliers?,[0],[0]
"They are datapoints whose likelihoods are rapidly changing with respect to a peaked posterior, like
the high measurement in the nuclear plant example in the introduction.
",2.1. 44% outliers?,[0],[0]
This case study suggests that predictive probability does not tell the entire story.,2.1. 44% outliers?,[0],[0]
Datapoints can exhibit low predictive accuracy in different ways.,2.1. 44% outliers?,[0],[0]
We now turn to a formal definition of PDIs.,2.1. 44% outliers?,[0],[0]
Let x D fxngN1 be a dataset with N observations.,2.2. Definitions,[0],[0]
A probabilistic model has two parts.,2.2. Definitions,[0],[0]
"The first is the likelihood, p.xn j /.",2.2. Definitions,[0],[0]
It relates an observation xn to hidden patterns described by a set latent random variables .,2.2. Definitions,[0],[0]
"If the observations are independent and identically distributed, the likelihood of the dataset factorizes as p.x j / D Q n p.xn j /.
",2.2. Definitions,[0],[0]
"The second is the prior density, p. /.",2.2. Definitions,[0],[0]
It captures the structure we expect from the hidden patterns.,2.2. Definitions,[0],[0]
Combining the likelihood and the prior gives the joint density p.x; / D p.x j /p. /.,2.2. Definitions,[0],[0]
"Conditioning on observed data gives the posterior density, p. jx/.
Treat the likelihood of each datapoint as a function of .",2.2. Definitions,[0],[0]
"To evaluate the model, we analyze how each datapoint fares in relation to the posterior density.",2.2. Definitions,[0],[0]
"Consider these expectations and variances with respect to the posterior,
.n/",2.2. Definitions,[0],[0]
"D E jxŒp.xn j /
log.n/ D E jxŒlogp.xn j /
2.n/ D V jxŒp.xn j /
2log.n/ D V jxŒlogp.xn j / :
(1)
",2.2. Definitions,[0],[0]
Each includes the likelihood in a slightly different fashion.,2.2. Definitions,[0],[0]
"The first expectation is a familiar object: .n/ is the posterior predictive density.
",2.2. Definitions,[0],[0]
A PDI is a ratio of these variances to expectations.,2.2. Definitions,[0],[0]
Taking the ratio calibrates this quantity for each datapoint.,2.2. Definitions,[0],[0]
Recall the mental picture from the introduction.,2.2. Definitions,[0],[0]
"The variance of
the likelihood under the posterior highlights potential model mismatch; dividing by the mean calibrates this spread to its predictive accuracy.
",2.2. Definitions,[0],[0]
Calibration puts all datapoints on a common scale.,2.2. Definitions,[0],[0]
"Imagine a binary classification problem where each datapoint yn lives in f0; 1g. The variances of the zero measurements may be numerically quite different than the one measurements; considering the mean renders these values comparable.
",2.2. Definitions,[0],[0]
"Related ratios also appear in classical statistics under a variety of forms, such as indices of dispersion (Hoel, 1943), coefficients of variation (Koopmans et al., 1964), or the Fano factor (Fano, 1947).",2.2. Definitions,[0],[0]
They all quantify dispersion of samples from a random process.,2.2. Definitions,[0],[0]
"PDIs extend these ideas by connecting to the posterior density of a probabilistic model.
",2.2. Definitions,[0],[0]
"In this paper, we study a particular PDI, called the widely applicable posterior dispersion index (WAPDI),
WAPDI.n/ D 2log.n/
log .n/",2.2. Definitions,[0],[0]
":
Its form and name comes from the widely applicable information criterion WAIC D 1
N
P n log .n/",2.2. Definitions,[0],[0]
C 2,2.2. Definitions,[0],[0]
"log.n/:
WAIC measures generalization error; it asymptotically equates to leave-one-one cross validation (Watanabe, 2010;
2015).",2.2. Definitions,[0],[0]
WAPDI has two advantages; both are practically motivated.,2.2. Definitions,[0],[0]
"First, we hope the reader is computing an estimate of generalization error.",2.2. Definitions,[0],[0]
"Gelman et al. (2014) suggests WAIC because it is easy to compute and designed for common machine learning models (Watanabe, 2010).",2.2. Definitions,[0],[0]
Computing WAIC gives WAPDI for free.,2.2. Definitions,[0],[0]
"Second, the variance is a second-order moment calculation; using the log likelihood gives numerical stability to the computation.",2.2. Definitions,[0],[0]
"(More on computation in Section 2.4.)
",2.2. Definitions,[0],[0]
WAPDI compares the variance of the log likelihood to the log posterior predictive.,2.2. Definitions,[0],[0]
This gives insight into how the likelihood of a datapoint fares under the posterior distribution of the hidden patterns.,2.2. Definitions,[0],[0]
We now study this in more detail.,2.2. Definitions,[0],[0]
"The posterior predictive density is an expectation, E jxŒp.xnew j / D R p.xnew j /p.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
jx/ d,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
:,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Expectations are integrals: areas under a curve.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"Different likelihood and posterior combinations can lead to similar integrals.
",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
A toy model illustrates this.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"Consider a gamma likelihood with fixed shape, and place a gamma prior on the rate.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"The
model is
p.ˇ/ D",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Gam.ˇ,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"I a0 D 1; b0 D 1/;
p.x jˇ/ D NY nD1 Gam.xn I a D 5; ˇ/;
which gives the posterior p.ˇ jx/ D",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Gam.ˇ,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"I a D a0 C 5N; b D b0 C P n xn/.
Now simulate a dataset of size N D 10 with ˇ D 1; the data have mean a=ˇ",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
D 5.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Now consider an outlier at 15.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"We can find another x value with essentially the same predictive accuracy
logp.x1",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
D 0:727 jx/,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"D 5:633433; logp.x2 D 15 jx/ D 5:633428:
Yet their WAPDI values differ by an order of magnitude
WAPDI.x1 D 0:727/ D 0:067;
WAPDI.x2 D 15/ D 0:229:
In this case, WAPDI highlights x2 D 15 as a more severe outlier than x1 D 0:727, even though they have the same predictive accuracy.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
What does that mean?,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"Figure 4 depicts the difference.
",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
The following lemma explains how WAPDI measures this effect.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"(Proof in Appendix B.)
Lemma 1",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"If logp.xn j / is at least twice differentiable and the posterior p. jx/ has finite first and second moments, then a second-order Taylor approximation gives
WAPDI.n/
logp0.xn jE jxŒ / 2 V jxŒ
log E jxŒp.xn j / : (2)
Corollary 1 WAPDI highlights datapoints whose likelihood is rapidly changing at the posterior mean estimate of the latent variables.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"(V jxŒ is constant across n.)
",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"Looking back at Figure 4, the likelihood p.x2 D 15 jˇ/ indeed changes rapidly under the posterior.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
WAPDI reports the ratio of this rate-of-change to the area under the curve.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"In this specific example, only the numerator matters, since the denominator is effectively the same for both datapoints.
",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Corollary 2 Equation (2) is zero if and only if the posterior mean coincides with the maximum likelihood estimate of for datapoint xn.,2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"(V jxŒ is positive for finite N .)
",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"For most interesting models, we do not expect such a coincidence.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"However, in practice, we find WAPDI to be close to zero for datapoints that match the model well.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
"With that, we now turn to computation.",2.3. Intuition: not all predictive probabilities are created equal,[0],[0]
Calculating WAPDI is straightforward.,2.4. Computation,[0],[0]
The only requirement are samples from the posterior.,2.4. Computation,[0],[0]
This is precisely the output of an Markov chain Monte Carlo (MCMC) sampling algorithm.,2.4. Computation,[0],[0]
"(We used the no-U-turn sampler (Hoffman & Gelman, 2014) for the analyses above.)",2.4. Computation,[0],[0]
"Other inference procedures, such as variational inference, give an analytic approximation to the posterior (Jordan et al., 1999; Blei et al., 2016).",2.4. Computation,[0],[0]
Drawing samples from an approximate posterior also works.,2.4. Computation,[0],[0]
"(We use this approach for the empirical study in Section 3.)
",2.4. Computation,[0],[0]
"Equipped with S samples from the posterior, Monte Carlo integration (Robert & Casella, 1999) gives unbiased estimates of the quantities in Equation (1).",2.4. Computation,[0],[0]
"The variance of these estimates decreases as O.1=S/; we assume S is sufficiently large to cover the posterior (Gelman et al., 2014).",2.4. Computation,[0],[0]
We default to S D 1000 in our experiments.,2.4. Computation,[0],[0]
"Algorithm 1 summarizes these steps.
",2.4. Computation,[0],[0]
"Algorithm 1: Calculating WAPDI.
",2.4. Computation,[0],[0]
"Input: Data x D fxngN1 , model p.x; /.
",2.4. Computation,[0],[0]
"Output: WAPDI for each datapoint xn.
",2.4. Computation,[0],[0]
Draw S samples f gS1 from posterior (approximation) p.,2.4. Computation,[0],[0]
"jx/.
for n in 1, 2, . .",2.4. Computation,[0],[0]
.,2.4. Computation,[0],[0]
", N do
Estimate log .n/; 2log.n/ from samples",2.4. Computation,[0],[0]
"f g S 1 .
",2.4. Computation,[0],[0]
"Store and return
WAPDI.n/",2.4. Computation,[0],[0]
"D 2log.n/
log .n/",2.4. Computation,[0],[0]
":
end",2.4. Computation,[0],[0]
"We now explore three real data examples using modern machine learning models: voting preferences, supermarket shopping, and population genetics.",3. Experimental Study,[0],[0]
"In 1988, CBS conducted a U.S. nation-wide survey of voting preferences.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Citizens indicated their preference towards the Democratic or Republican presidential candidate.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"Each individual also declared their gender, age, race, education level, and the state they live in; 11 566 individuals participated.
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Gelman & Hill (2006) study this data through a hierarchical logistic regression model.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"They begin by modeling gender, race, and state; the state variable has a hierarchical prior.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"This model is easy to fit using automatic differentiation variational inference (ADVI) within Stan (Kucukelbir et al., 2015; Carpenter et al., 2015).",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"(Model and inference details in Appendix C.)
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"VOTE R R R R R R R R R R SEX F F F F F F F F F F
RACE B B B B B B B B B B STATE WA WA NY WI NY NY NY NY MA MA
RACE W W W W W W W W W W STATE WY WY WY WY WY WY WY DC DC NV
Table 2.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"Worst WAPDI values.
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Tables 1 and 2 show the individuals with the lowest predictive accuracy and WAPDI.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
The nation-wide trend predicts that females (F) who identify as black (B) have a strong preference to vote democratic (D); predictive accuracy identifies the few individuals who defy this trend.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"However, there is not much to do with this information; the model identifies a nation-wide trend that correctly describes most female black voters.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"In contrast, WAPDI points to parts of the dataset that the model fails to describe; these are datapoints that we might try to explain better with a revised model.
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"Most of the individuals with poor WAPDI live in Wyoming, the District of Columbia, and Nevada.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
We focus on Wyoming and Nevada.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
The average WAPDI for Wyoming and Nevada are 0:057 and 0:041; these are baselines that we seek to improve.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"(The closer to zero, the better.)
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Consider expanding the model by modeling age.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Introducing age into the model with a hierarchical prior reveals that older voters tend to vote Republican.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
This helps explain Wyoming voters; their average WAPDI improves from 0:057 to 0:04; however Nevada’s average WAPDI remains unchanged.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
This means that Nevada’s voters may not follow the national age-dependent trend.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
Now consider removing age and introducing education in a similar way.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"Education helps explain voters from both states; the average WAPDI for Wyoming and Nevada improve to 0:041 and 0:029.
",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
WAPDI thus captures interesting datapoints beyond what predictive accuracy reports.,3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"As expected, predictive accuracy still highlights the same female black voters in both expanded models; WAPDI illustrates a deeper way to evaluate this model.",3.1. Voting preferences: a hierarchical logistic regression model,[0],[0]
"Market research firm IRi hosts an anonymized dataset of customer shopping behavior at U.S. supermarkets (Bronnenberg et al., 2008).",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
The dataset tracks 136 584 “checkout” sessions; each session contains a basket of purchased items.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"An inventory of 7 903 items range across categories such as carbonated beverages, toiletries, and yogurt.
",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
What items do customers tend to purchase together?,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"To study this, consider a hierarchical Poisson factorization (HPF) model (Gopalan et al., 2015).",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
HPF models the quantities of items purchased in each session with a Poisson likelihood; its rate is an inner product between a session’s preferences s and the item attributes ˇ.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"Hierarchical priors on and ˇ simultaneously promote sparsity, while accounting for variation in session size and item popularity.",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Some sessions contain only a few items; others are large purchases.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"(Model and inference details in Appendix D.)
A 20-dimensional HPF model discovers intuitive trends.",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"A
few stand out.",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Snack-craving customers like to buy Doritos tortilla chips along with Lay’s potato chips.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Morning birds typically pair Cheerios cereal with 2% skim milk.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Yoplait fans tend to purchase many different flavors at the same time.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"Tables 3 and 4 show the top five items in two of these twenty trends.
Sessions where a customer purchases many items from different categories have low predictive accuracy.",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"This makes sense as these customers do not exhibit a trend; mathematically, there is no combination of item attributes ˇ",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
that explain buying items from disparate categories.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"For example, the session with the lowest predictive accuracy contains 117 items ranging from coffee to hot dogs.
",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
WAPDI highlights a different aspect of the HPF model.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Sessions with poor WAPDI contain similar items but exhibit many purchases of a single item.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"Table 5 shows an example of a session where a customer purchased 14 blackberry Yoplait yogurts, but only a few of the other flavors.
",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
This indicates that the Poisson likelihood assumption may not be flexible enough to model customer purchasing behavior.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Perhaps a negative binomial likelihood could model this kind of spiked activity better.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Another option might be to keep the Poisson likelihood but increase the hierarchy of the probabilistic model; this approach may identify item attributes that explain such purchases.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
"In either case,
WAPDI identifies a valuable aspect of the data that the HPF struggles to capture: sessions with spiked activity.",3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
This is a concrete direction for model revision.,3.2. Supermarket shopping: a hierarchical Poisson factorization model,[0],[0]
Do all people who live nearby have similar genomes?,3.3. Population genetics: a mixed membership model,[0],[0]
Not necessarily.,3.3. Population genetics: a mixed membership model,[0],[0]
Population genetics considers how individuals exhibit ancestral patterns of mutations.,3.3. Population genetics: a mixed membership model,[0],[0]
Begin with N individuals and L locations on the genome.,3.3. Population genetics: a mixed membership model,[0],[0]
"For each location, report whether each individual reveals a mutation.",3.3. Population genetics: a mixed membership model,[0],[0]
"This gives an (N L) dataset x where xnl 2 f0; 1; 2; 3g. (We assume two specific forms of mutation; 3 encodes a missing observation.)
",3.3. Population genetics: a mixed membership model,[0],[0]
"Mixed membership models offer a way to study this (Pritchard et al., 2000).",3.3. Population genetics: a mixed membership model,[0],[0]
Assume K ancestral populations ; these are the mutation probabilities of each location.,3.3. Population genetics: a mixed membership model,[0],[0]
Each individual mixes these populations with weights ; these are the mixing proportions.,3.3. Population genetics: a mixed membership model,[0],[0]
"Place a beta prior on the mutation probabilities and a Dirichlet prior on the mixing proportions.
",3.3. Population genetics: a mixed membership model,[0],[0]
We study a dataset of N D 324 individuals from four geographic locations and focus on L D 13 928 locations on the genome.,3.3. Population genetics: a mixed membership model,[0],[0]
Figure 5 shows how these individuals mix K D 3 ancestral populations.,3.3. Population genetics: a mixed membership model,[0],[0]
"(Data, model, and inference details in Appendix E.)
WAPDI reveals three interesting patterns of mismatch here.",3.3. Population genetics: a mixed membership model,[0],[0]
"First, individuals with poor WAPDI values have many missing observations; the worst 10% of WAPDI have 1 344 missing values, in contrast to 563 for the lowest 10% of predictive scores.",3.3. Population genetics: a mixed membership model,[0],[0]
"We may consider directly modeling these missing observations.
",3.3. Population genetics: a mixed membership model,[0],[0]
"Second, ASW has two individuals with poor WAPDI; their mutation patterns are outliers within the group.",3.3. Population genetics: a mixed membership model,[0],[0]
"While the average individual reveals 272 mutations away from the median genome, these individuals show 410 and 383 mutations.",3.3. Population genetics: a mixed membership model,[0],[0]
"This points to potential mishaps while gathering or pre-processing the data.
",3.3. Population genetics: a mixed membership model,[0],[0]
"Last, MEX exhibits good predictive accuracy, yet poor WAPDI values compared to other groups.",3.3. Population genetics: a mixed membership model,[0],[0]
"Based on predictive accuracy, we may happily accept these patterns.",3.3. Population genetics: a mixed membership model,[0],[0]
Yet WAPDI highlights a serious issue with the inferred populations.,3.3. Population genetics: a mixed membership model,[0],[0]
The blue and red populations are almost twice as correlated across genes (0:58) as the other possible combinations (0:24 and 0:2).,3.3. Population genetics: a mixed membership model,[0],[0]
"In other words, the blue and red populations represent similar patterns of mutations at the same locations.",3.3. Population genetics: a mixed membership model,[0],[0]
"These populations, as they stand, are not necessarily interpretable.",3.3. Population genetics: a mixed membership model,[0],[0]
Revising the model to penalize correlation may be a direction worth pursuing.,3.3. Population genetics: a mixed membership model,[0],[0]
A posterior dispersion index (PDI) identifies informative forms of model mismatch that compliments predictive accuracy.,4. Discussion,[0],[0]
"By highlighting which datapoints exhibit the most uncertainty under the posterior, a PDI offers a new perspective into evaluating probabilistic models.",4. Discussion,[0],[0]
"Here, we show how one particular PDI, the widely applicable posterior dispersion index (WAPDI), reveals promising directions for model improvement across a range of models and applications.
",4. Discussion,[0],[0]
The choice of WAPDI is practically motivated; it comes for free as part of the calculation of WAIC.,4. Discussion,[0],[0]
"This highlights how PDIs are complimentary to tools such as predictive accuracy, cross validation, and information criteria.",4. Discussion,[0],[0]
"While PDIs and predictive accuracy assess model mismatch at the datapoint level, cross validation and information criteria indicate model mismatch at the dataset level.
PDIs provide a relative comparison of datapoints with respect to a model.",4. Discussion,[0],[0]
Can PDIs be thresholded to identify “problematic” datapoints?,4. Discussion,[0],[0]
"One approach in this direction draws inspiration from posterior predictive checks (PPCs) (Rubin, 1984; Gelman et al., 1996).",4. Discussion,[0],[0]
A PPC works by hallucinating data from the posterior predictive and comparing properties of the hallucinated data to the observed dataset.,4. Discussion,[0],[0]
"Comparing PDI values in this way could lead to a meaningful way of thresholding PDIs.
",4. Discussion,[0],[0]
There are several research directions.,4. Discussion,[0],[0]
"One is to extend the
notion of a PDI to non-exchangeable data.",4. Discussion,[0],[0]
Another is to leverage the bootstrap to extend this idea beyond probabilistic models.,4. Discussion,[0],[0]
"Computationally, ideas from importance sampling could reduce the variance of PDI computations for very high dimensional models.
",4. Discussion,[0],[0]
"A promising direction is to study PDIs under the viewpoint of scoring rules (Dawid & Musio, 2014).",4. Discussion,[0],[0]
"Understanding the decision theoretic properties of a PDI as a loss function could lead to alternative objectives for inference.
",4. Discussion,[0],[0]
"Finally, we end on a reminder that PDIs are simply another tool in the statistician’s toolbox.",4. Discussion,[0],[0]
"The design and criticism of probabilistic models is still a careful, manual craft.",4. Discussion,[0],[0]
"While good tools can help, an overarching obstacle remains to pursue their adoption by practitioners.",4. Discussion,[0],[0]
"To this end, making these tools easier to use and more automatic can only help.",4. Discussion,[0],[0]
"We thank Dustin Tran, Maja Rudolph, David Mimno, Aki Vehtari, Josh Vogelstein, and Rajesh Ranganath for their insightful comments.",Acknowledgments,[0],[0]
"This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C4032, and the Alfred P. Sloan Foundation.",Acknowledgments,[0],[0]
"Probabilistic modeling is cyclical: we specify a model, infer its posterior, and evaluate its performance.",abstractText,[0],[0]
"Evaluation drives the cycle, as we revise our model based on how it performs.",abstractText,[0],[0]
This requires a metric.,abstractText,[0],[0]
"Traditionally, predictive accuracy prevails.",abstractText,[0],[0]
"Yet, predictive accuracy does not tell the whole story.",abstractText,[0],[0]
We propose to evaluate a model through posterior dispersion.,abstractText,[0],[0]
The idea is to analyze how each datapoint fares in relation to posterior uncertainty around the hidden structure.,abstractText,[0],[0]
This highlights datapoints the model struggles to explain and provides complimentary insight to datapoints with low predictive accuracy.,abstractText,[0],[0]
We present a family of posterior dispersion indices (PDI) that capture this idea.,abstractText,[0],[0]
"We show how a PDI identifies patterns of model mismatch in three real data examples: voting preferences, supermarket shopping, and population genetics.",abstractText,[0],[0]
Evaluating Bayesian Models with Posterior Dispersion Indices,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 58–63 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-2010",text,[0],[0]
"Closed compounding, i.e., the formation of a oneword unit composing several lexemes, is a common linguistic phenomenon in several languages such as German, Dutch, Greek, and Finnish.",1 Introduction,[0],[0]
The goal of compound splitting is to obtain the constituents of a compound to increase its semantic transparency.,1 Introduction,[0],[0]
"For example, for the German compound Apfelsaft ‘apple1 juice2’ the desired output of a compound splitter is Apfel1 Saft2.
",1 Introduction,[0],[0]
"Intrinsic evaluation of compound splitting measures the correctness of the determined split point (Riedl and Biemann, 2016) and the resulting lemmas by means of precision, recall, F1-score and accuracy (e.g., Koehn and Knight (2003)).",1 Introduction,[0],[0]
"In extrinsic evaluation setups, compound splitting is applied to the input data of an external natural language processing (NLP) task that benefits from split compounds.",1 Introduction,[0],[0]
"As closed compounding introduces semantic opaqueness and vastly increases the vocabulary size of a language, many NLP tasks
benefit from compound splitters.",1 Introduction,[0],[0]
"Still, previous work that evaluates compound splitting with extrinsic evaluation methods mostly focuses on statistical machine translation (SMT) (e.g., Nießen and Ney (2000), Koehn and Knight (2003)).",1 Introduction,[0],[0]
"Some other external tasks such as information retrieval (Kraaij and Pohlmann, 1998) or speech recognition (Larson et al., 2000) have been shown to benefit from prior compound splitting, yet these works have not compared the extrinsic performance of different compound splitting methods.
",1 Introduction,[0],[0]
"Interestingly, the performance found in intrinsic evaluations does not automatically propagate to performance in downstream evaluations as shown in (Fritzinger and Fraser, 2010) for SMT, where oversplit compounds are simply learned as phrases (Dyer, 2009; Weller et al., 2014).",1 Introduction,[0],[0]
"Oversplitting is an example of a feature that might not be measured in intrinsic evaluations, because some available gold standards contain positive examples only (Ziering and van der Plas, 2016).",1 Introduction,[0],[0]
"It is highly relevant to increase the number of extrinsic tasks for the evaluation of compound splitting to be able to evaluate features that intrinsic evaluations and known extrinsic evaluations ignore.
",1 Introduction,[0],[0]
"In this paper we investigate the suitability of Recognizing Textual Entailment (RTE) for the task of compound splitting, inspired by the fact that previous work in RTE underlined the potential benefits of compound splitting for this task (Zeller, 2016).",1 Introduction,[0],[0]
"Textual Entailment (TE) is a directional relationship between an entailing text fragment T and an entailed hypothesis, H, saying that the meaning of T entails (or implies) the meaning of H. This relation holds if ‘typically, a human, reading T, would infer that H is most likely true’ (Dagan et al., 2006).",1 Introduction,[0],[0]
"The following is an example of an entailing T-H pair:
T: Yoko Ono unveiled a bronze statue of her late husband, John Lennon.",1 Introduction,[0],[0]
"H: Yoko Ono is John Lennon’s widow.
",1 Introduction,[0],[0]
"58
We opted for exploring the use of RTE as an extrinsic evaluation for compound splitting for three main reasons: first, in contrast to SMT systems, most RTE systems are less complex.",1 Introduction,[0],[0]
"In fact, we deliberately chose an RTE system that reaches good performance with a method that is transparent, i.e., a method that allows for exploring the effect of compounding.1 It is not our goal to reach state-of-the-art performance for the RTE task.",1 Introduction,[0],[0]
We aim to find a suitable alternative extrinsic evaluation for compound splitting.,1 Introduction,[0],[0]
"Second, human agreement on the binary RTE decisions is very high, e.g., on the dataset used in our experiments, an average agreement rate of 87.8% with a κ level of 0.75 was reported (Giampiccolo et al., 2007).",1 Introduction,[0],[0]
"Third, the potential benefits for RTE are large.",1 Introduction,[0],[0]
"According to Zeller (2016, p. 182) the number of T/H pairs in their phenomenon-specific RTE dataset would rise by about 16 percentage points by compound splitting.",1 Introduction,[0],[0]
"In the dataset we use in our experiments, about three-quarters of the T-H pairs contain at least one closed compound.",1 Introduction,[0],[0]
"The approach to RTE taken in this paper follows the Lexical Overlap Hypothesis (LOH), which states that the higher the number of lexical matches between T and H, the more likely the T-H pair is entailing rather than non-entailing (Zeller, 2016).",2 Relevance of Compound Splitting for RTE,[0],[0]
"In other words, H is more likely to be entailed by T if most of its lexical content also occurs in T. While this hypothesis is a simplification of the TE problem, it has been shown to perform reasonably well for some datasets (Noh et al., 2015).",2 Relevance of Compound Splitting for RTE,[0],[0]
"We argue that the brittleness of the chosen LOH-based RTE system may actually be a strength in terms of evaluation, since it will penalize oversplitting more severely than, e.g., an RNNbased RTE system or a phrase-based MT method that can recover from systematic oversplitting by chunking the splits.
",2 Relevance of Compound Splitting for RTE,[0],[0]
"Under the LOH, the problem caused by the opacity of closed compounds becomes evident.",2 Relevance of Compound Splitting for RTE,[0],[0]
"As shown in the example below, missing information on the constituents of closed compounds hinders the matching of words from T in H1.",2 Relevance of Compound Splitting for RTE,[0],[0]
"Conversely, compound splitting also helps to detect
1We did not opt for neural RTE systems (Bowman et al., 2015), albeit state-of-the-art, in this first study because of the opacity of the models and the inclusion of phrase-level information, which will make interpretation of the effect harder.
",2 Relevance of Compound Splitting for RTE,[0],[0]
non-entailing T-H pairs.,2 Relevance of Compound Splitting for RTE,[0],[0]
"By compound splitting, we increase the number of uncovered tokens in H2, which makes a non-entailment decision more likely2.
",2 Relevance of Compound Splitting for RTE,[0],[0]
T:,2 Relevance of Compound Splitting for RTE,[0],[0]
Kinder lieben Fruchtsäfte1 aus Äpfeln2 ‘Children love fruit juices1 made of apples2’ H1: Peters Sohn liebt Apfel3saft4 ‘Peter’s son loves apple3 juice4’ H2:,2 Relevance of Compound Splitting for RTE,[0],[0]
Peters Sohn liebt Apfel5kuchen6stücke7,2 Relevance of Compound Splitting for RTE,[0],[0]
‘Peter’s son loves pieces7 of apple5 pie6’,2 Relevance of Compound Splitting for RTE,[0],[0]
In this section we explain the splitters and the RTE framework used in our experiments.,3 Materials and Methods,[0],[0]
Our proposed extrinsic evaluation approach for compound splitting is language-independent as we do not use any language-specific parameters.,3.1 Inspected Compound Splitters,[0],[0]
"However, in the present work we test it on the most prominent closed-compounding language, German (Ziering and van der Plas, 2014).",3.1 Inspected Compound Splitters,[0],[0]
"We inspect the impact of three different types of automatic compound splitting3 methods that follow a generate-and-rank principle, where the candidate splits are ranked according to the geometric mean of the constituents’ frequencies in a given training corpus (Koehn and Knight, 2003).
",3.1 Inspected Compound Splitters,[0],[0]
FF2010,3.1 Inspected Compound Splitters,[0],[0]
"The compound splitter by Fritzinger and Fraser (2010) relies on the output of the German morphological analyzer SMOR (Schmid et al., 2004) to generate several plausible compound splits (e.g., due to word sense ambiguity).
",3.1 Inspected Compound Splitters,[0],[0]
WH2012,3.1 Inspected Compound Splitters,[0],[0]
"As an alternative method, we use the statistical approach presented in Weller and Heid (2012) for German compound splitting.",3.1 Inspected Compound Splitters,[0],[0]
"Instead of using the knowledge-rich SMOR, it includes an extensive list of hand-crafted transformation rules that allows to map constituents to corpus lemmas (e.g., by truncating linking morphemes) to generate all possible splits with up to four constituents per compound.",3.1 Inspected Compound Splitters,[0],[0]
"Moreover, misleading lemmas are removed from the training corpus using hand-crafted filters.
",3.1 Inspected Compound Splitters,[0],[0]
"2Note that we need to apply lemmatization prior to determining the lexical matches between T and H.
3The compound splitters are designed to split compounds with any content word as head, i.e., noun compounds (Hunde|hütte ‘doghouse’), verb compounds (eis|laufen ‘to ice-skate’) and adjective compounds (hunde|müde ‘dogtired’) and disregard constructions with a functional modifier (as in the particle verb auf|stehen ‘to stand up’).
",3.1 Inspected Compound Splitters,[0],[0]
"ZvdP2016 Finally, the method using least language-specific knowledge was proposed by Ziering and van der Plas (2016).",3.1 Inspected Compound Splitters,[0],[0]
"Instead of using a morphological analyzer or manually compiling a hand-crafted list of rules, they recursively generate all possible binary splits by learning constituent transformations from regular inflection derived from a monolingual lemmatized corpus, e.g., the s-suffix in the case of a genitive marker is often used as linking morpheme.",3.1 Inspected Compound Splitters,[0],[0]
"The recursion stops if a non-splitting (atomic) analysis is ranked highest.
",3.1 Inspected Compound Splitters,[0],[0]
"Additionally, to provide an upper bound, we manually split development and test data.",3.1 Inspected Compound Splitters,[0],[0]
"We conduct our RTE experiments using the opensource Excitement Open Platform (EOP) (Padó et al., 2015; Magnini et al., 2014), which provides comprehensive implementations of algorithms and lexical resources for textual inference.",3.2 RTE Framework,[0],[0]
"We use the alignment-based algorithm P1EDA (Noh et al., 2015) in all our experiments as it has been shown to be simple and transparent while yielding relatively good results.",3.2 RTE Framework,[0],[0]
P1EDA is based on the LOH for RTE explained in Section 2.,3.2 RTE Framework,[0],[0]
"The algorithm works in three steps: First, it extracts all possible alignments between sequences of identical lemmas in T and H. Then, it extracts various features4 from the alignments.",3.2 RTE Framework,[0],[0]
"Finally, these features are given as input to a multinomial logistic regression classifier which is trained on annotated data.",3.2 RTE Framework,[0],[0]
"For the sake of simplicity, for now we only use one basic aligner which aligns (sequences of) words in T and H that consist of identical lemmas.",3.2 RTE Framework,[0],[0]
"We will investigate the impact of prior compound splitting given additional lexical resources (such as a derivational morphology lex-
4We use a similar feature set as Noh et al. (2015), namely the ratio of aligned vs. unaligned words in H with respect to all words, content words, and named entities.
icon (Zeller et al., 2013)) in future work.",3.2 RTE Framework,[0],[0]
"We use TreeTagger (Schmid, 1995) as integrated in EOP to provide tokenization, lemmatization and Partof-Speech tagging as linguistic preprocessing.
",3.2 RTE Framework,[0],[0]
"We train and evaluate all models on the German translation of the RTE-3 dataset (Dagan et al., 2006; Magnini et al., 2014).",3.2 RTE Framework,[0],[0]
The training and test dataset contain 800 T-H pairs each.,3.2 RTE Framework,[0],[0]
"In both sets, entailing and non-entailing T-H pairs are equally distributed (chance baseline of 50% accuracy).
",3.2 RTE Framework,[0],[0]
We apply a compound splitter on the RTE training and test dataset before we input the data to the EOP pipeline.,3.2 RTE Framework,[0],[0]
"We replace all compounds by their constituents, separated by white-space.",3.2 RTE Framework,[0],[0]
"Thus, they are subsequently treated as individual words by EOP and the lexical aligner can benefit from the increased transparency of the compounds.",3.2 RTE Framework,[0],[0]
"Table 1 shows accuracy, precision, recall and F1score for the entailment and non-entailment class on the RTE-3 dataset.",4 Results and Discussion,[0],[0]
"As reflected in the results, reducing the opacity of compounds via the application of a compound splitter improves the subsequent RTE performance.",4 Results and Discussion,[0],[0]
This holds for all compound splitters that we used in our experiments.,4 Results and Discussion,[0],[0]
"It is also noticeable that the different compound splitters yield different results in the downstream task, with FF2010 being the most beneficial and significantly5 outperforming the initial RTE setup without prior compound splitting (INIT) by up to four percentage points in accuracy and F1-score.
",4 Results and Discussion,[0],[0]
"As expected, manual splitting performs best overall.",4 Results and Discussion,[0],[0]
The performance difference with FF2010 is however not statistically significant.,4 Results and Discussion,[0],[0]
"This is not surprising because FF2010 reaches an accuracy of around 90% in intrinsic evaluations (Ziering and van der Plas, 2016) and the small underperfor-
5McNemar test (McNemar, 1947), p < 0.05
mance is leveled out by the small size of the test set.",4 Results and Discussion,[0],[0]
"Moreover, manual inspections revealed that FF2010 has a higher recall than manual splitting in the non-entailment class due to its undersplitting which results in less lexical overlap between T and H, pointing to the non-entailment class.
",4 Results and Discussion,[0],[0]
"When we compare these results from the extrinsic evaluation with intrinsic evaluation results (in terms of splitting accuracy) reported in Ziering and van der Plas (2016), we see the same performance ordering with respect to the three compound splitters, while the current extrinsic evaluation on RTE differentiates between the best system (FF2010) and the two others in that only the former reached statistically significant improvements over the INIT baseline.
",4 Results and Discussion,[0],[0]
To analyze the possible causes of difference in performance between the systems and to see the benefits of using RTE for compound splitting evaluation we performed a manual error analysis.,4 Results and Discussion,[0],[0]
"First, we examined all entailment classifications that were correct using FF2010 and incorrect when using the INIT baseline.",4 Results and Discussion,[0],[0]
"Using FF2010, the classifier was able to correctly classify an additional 36 entailing and 25 non-entailing T-H pairs.",4 Results and Discussion,[0],[0]
"As expected, most of the hypotheses in these pairs contained correctly split compounds where the RTE system could benefit from the increased transparency.",4 Results and Discussion,[0],[0]
"Conversely, we also examined the 28 T-H pairs that the classifier missed to identify as entailing while they were correct in INIT.",4 Results and Discussion,[0],[0]
"Most of the examples were cases in which there was almost no lexical overlap between T and H even with compound splitting.
",4 Results and Discussion,[0],[0]
"Furthermore, we compared the correct entailment classifications of FF2010 with the other two splitters.",4 Results and Discussion,[0],[0]
"For ZvdP2016, most errors can be attributed to oversplitting.",4 Results and Discussion,[0],[0]
"Precisely, 25 out of its 37 (67.5%) misclassifications compared to FF2010 can be attributed to this problem.",4 Results and Discussion,[0],[0]
"For example, ZvdP2016 oversplit the name Landowska into Line Dow Ska6 that appeared in both T and H in an non-entailing pair, which artificially increased the coverage ratio of words in H and therefore pointed to the incorrect entailment classification.",4 Results and Discussion,[0],[0]
"For WH2012, oversplitting is also a major contributor of RTE errors, however it appeares not as predominant as for ZvdP2016.",4 Results and Discussion,[0],[0]
"10 out of its 29 (34.5%) misclassifications compared to FF2010
6Misleading knowledge about verbal inflection automatically derived from a lemmatized corpus is responsible for the oversplitting by ZvdP2016.
can be attributed to oversplitting, while 4 (13.8%) missclassifications are due to undersplitting.",4 Results and Discussion,[0],[0]
"For example, in an entailing T-H pair WH2012 correctly split Amazonas-Regenwald ‘Amazon rainforest’ in H into Amazonas Regen Wald, however it oversplit Amazonas in T into Amazon As ‘Amazon ace’ and thus, Amazonas in H remained unaligned.",4 Results and Discussion,[0],[0]
"To the contrary, FF2010 did not split Amazonas in T, which lead to a higher token coverage ratio in H. Again, in the H Die EU senkt die Fangquoten ‘The EU lowers the fishing quota’ of another entailing T-H pair, WH2010 correctly split Fang1quoten2 ‘fishing quota’ in H into fangen1 Quote2 but failed to split EU-Quote in T, failing to cover both EU and Quote in H.
Our closer inspections also showed that compound splitting does not always suffice to reveal a lexical match between T and H as shown in the following example:
T: Ben fährt1 einen Mercedes2 ‘Ben drives1 a Mercedes2’
H: Ben ist",4 Results and Discussion,[0],[0]
"Auto3fahrer4 ‘Ben is a car3 driver4’ Given a correct splitting of Autofahrer to Auto Fahrer, a derivational morphology resource (Zeller et al., 2013) would be required to discover the relationship between fahren and Fahrer and a synonym database to find that Mercedes is a hyponym of Auto.",4 Results and Discussion,[0],[0]
This does not weaken the claim that RTE is useful for evaluating compound splitters.,4 Results and Discussion,[0],[0]
"It just shows that deeper, semantic compound analysis could improve RTE further.
",4 Results and Discussion,[0],[0]
"Besides, the error analysis shed some light on the treatment of compound heads and modifiers.",4 Results and Discussion,[0],[0]
"It seems advisable to weight the compound head and modifiers differently when computing the ratio of aligned tokens in H. As illustrated by the following example, coverage of the head should be more important for the entailment decision than of the modifiers.",4 Results and Discussion,[0],[0]
"Given a correct split of Kinder1buch2 into Kind1 Buch2, H1 and H2 have the same token coverage ratio while only H1 is entailed by T.
T: Yuki kauft ein Kinder1buch2 ‘Yuki buys a children’s1 book2’
H1: Yuki kauft ein Buch ‘Yuki buys a book’ H2: Yuki ist ein Kind ‘Yuki is a child’
",4 Results and Discussion,[0],[0]
It should be noted that the transparency gain using compound splitting is limited to closed compounds that are compositional with respect to at least one constituent.,4 Results and Discussion,[0],[0]
"Splitting compounds in
H that are fairly non-compositional with respect to all constituents (e.g., Maulwurf ‘mole’ (lit.",4 Results and Discussion,[0],[0]
‘mouth throw’)) is counterproductive.,4 Results and Discussion,[0],[0]
"However, since most compounds (in particular ad-hoc productions) are compositional, this is only a side issue.",4 Results and Discussion,[0],[0]
"In fact, we did not observe any cases of noncompositional compounds in the course of our error analysis.
",4 Results and Discussion,[0],[0]
"In summary, compound splitting is a complex task that comprises many subtasks.",4 Results and Discussion,[0],[0]
"The multiple evaluation methods available, both intrinsic and extrinsic, vary in their suitability to evaluate them.",4 Results and Discussion,[0],[0]
"One of these subtasks concerns the ability of compound splitters to determine whether to split or not, which is an integral part of compound analysis.",4 Results and Discussion,[0],[0]
"While aspects such as oversplitting were not consistently evaluated in previous intrinsic evaluations, or compensated for by task-internal mechanisms in SMT, RTE proved more strict in this respect.",4 Results and Discussion,[0],[0]
"Moreover, the transparency of the models made it possible to better estimate the impact of splitting.",4 Results and Discussion,[0],[0]
"Despite the small size of the dataset, we were able to show significant differences, partly due to the clear definition of this binary classification task.
",4 Results and Discussion,[0],[0]
"On a side note, to the best of our knowledge, the result we obtained using the FF2010 compound splitter is the best result on the German RTE-3 dataset that has been reported using EOP.",4 Results and Discussion,[0],[0]
"Notably, we obtain an accuracy which is almost three percentage points higher than the results of Noh et al. (2015), although they include further (language-specific) linguistic knowledge.",4 Results and Discussion,[0],[0]
"Inspired by the potential benefits of compound splitting from the RTE literature and supported by the transparency of the models and the clear definition of this binary classification task, we set out to explore whether RTE is a suitable method to extrinsically evaluate the performance of compound splitting.",5 Conclusion,[0],[0]
We compared several compound splitters on a German textual entailment dataset and found that compound splitting is helpful for RTE across the board.,5 Conclusion,[0],[0]
"More importantly, we found that certain aspects of compound splitters, neglected in previous evaluations, such as oversplitting, had a large impact on this task and nicely differentiated the systems tested.",5 Conclusion,[0],[0]
"We conclude that RTE represents a suitable alternative to SMT for the extrinsic evaluation of compound splitters.
",5 Conclusion,[0],[0]
"In future work, we would like to investigate the interaction between additional lexical resources (such as GermaNet (Hamp and Feldweg, 1997; Henrich and Hinrichs, 2010) or DErivBase (Zeller et al., 2013)) and compound splitting, and the impact on the RTE performance.",5 Conclusion,[0],[0]
"We thank Sebastian Padó and Britta Zeller for their inspiring ideas about using compound splitting for improving RTE, and for their support in using the RTE framework.",Acknowledgments,[0],[0]
We are also grateful to the anonymous reviewers for their helpful feedback.,Acknowledgments,[0],[0]
"This research was funded by the German Research Foundation (Collaborative Research Centre 732, Project D11).",Acknowledgments,[0],[0]
"Traditionally, compound splitters are evaluated intrinsically on gold-standard data or extrinsically on the task of statistical machine translation.",abstractText,[0],[0]
"We explore a novel way for the extrinsic evaluation of compound splitters, namely recognizing textual entailment.",abstractText,[0],[0]
Compound splitting has great potential for this novel task that is both transparent and well-defined.,abstractText,[0],[0]
"Moreover, we show that it addresses certain aspects that are either ignored in intrinsic evaluations or compensated for by taskinternal mechanisms in statistical machine translation.",abstractText,[0],[0]
We show significant improvements using different compound splitting methods on a German textual entailment dataset.,abstractText,[0],[0]
Evaluating Compound Splitters Extrinsically with Textual Entailment,title,[0],[0]
"Proceedings of the SIGDIAL 2017 Conference, pages 174–185, Saarbrücken, Germany, 15-17 August 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Long before the terms conversational interface or chatbot were coined, Turing (1950) described them as the ultimate test for artificial intelligence.",1 Introduction,[0],[0]
"Despite their long history, there is a recent hype about chatbots in both, the scientific community (cf.",1 Introduction,[0],[0]
"e.g. Ferrara et al. (2016)) and industry (Gartner, 2016).",1 Introduction,[0],[0]
"While there are many related reasons for this development, we think that three key changes were particularly important:
• Rise of universal chat platforms (like Telegram, Facebook Messenger, Slack, etc.)
",1 Introduction,[0],[0]
"• Advances in machine learning (ML) • Natural Language Understanding (NLU) as a
service
In this paper, we focus on the latter.",1 Introduction,[0],[0]
"As we will show in Section 2, NLU services are already used by a number of researchers for building conversational interfaces.",1 Introduction,[0],[0]
"However, due to the lack of a systematic evaluation of theses services, the decision why one services was prefered over another, is usually not well justified.",1 Introduction,[0],[0]
"With this paper, we want to bridge this gap and enable both, researchers and companies, to make more educated decisions about which service they should use.",1 Introduction,[0],[0]
We describe the functioning of NLU services and their role within the general architecture of chatbots.,1 Introduction,[0],[0]
"We explain, how NLU services can be evaluated and conduct an evaluation, based on two different corpora consisting of nearly 500 annotated questions, of the most popular services.",1 Introduction,[0],[0]
"Recent publications have discussed the usage of NLU services in different domains and for different purposes, e.g. question answering for localized search (McTear et al., 2016), form-driven dialogue systems (Stoyanchev et al., 2016), dialogue management (Schnelle-Walka et al., 2016), and the internet of things (Kar and Haldar, 2016).
",2 Related Work,[0],[0]
"However, none of these publications explicitly discuss, why they choose one particular NLU service over another and how this decision may have influenced the performance of their system and hence their results.",2 Related Work,[0],[0]
"Moreover, to the best of our knowledge, so far there exists no systematic evaluation of a particular NLU service, let alone a comparison of multiple services.
",2 Related Work,[0],[0]
"Dale (2015) lists five NLP cloud services and describes their capabilities, but without conducting an evaluation.",2 Related Work,[0],[0]
"In the domain of spoken dialog
174
systems, similar evaluations have been conducted for automatic speech recognizer services, e.g. by Twiefel et al. (2014) and Morbini et al. (2013).
",2 Related Work,[0],[0]
"Speaking about chatbots in general, Shawar and Atwell (2007) present an approach to conduct endto-end evaluations, however, they do not take into account the single elements of a system.",2 Related Work,[0],[0]
Resnik and Lin (2010) provide a good overview and evaluation of Natural Language Processing (NLP) systems in general.,2 Related Work,[0],[0]
Many of the principals they apply for their evaluation (e.g. inter-annotator agreement and partitioning of data) play an important role in our evaluation too.,2 Related Work,[0],[0]
A comprehensive and extensive survey of question answering technologies was presented by Kolomiyets and Moens (2011).,2 Related Work,[0],[0]
"However, there has been a lot of progress since 2011, including the here presented NLU services.
",2 Related Work,[0],[0]
"One of our two corpora was labelled using Amazon Mechanical Turk (AMT, cf.",2 Related Work,[0],[0]
"Section 5.2), while there have been long discussions about whether or not AMT can replace the work of experts for labelling linguistic data, the recent consensus is that, given enough annotators, crowdsourced labels from AMT are as reliable as expert data.",2 Related Work,[0],[0]
"(Snow et al., 2008; Munro et al., 2010; Callison-Burch, 2009)",2 Related Work,[0],[0]
"In order to understand the role of NLU services for chatbots, one first has to look at the general architecture of chatbots.",3 Chatbot Architecture,[0],[0]
"While there exist different documented chatbot architectures for concrete use cases, no universal model of how a chatbot should be designed has emerged yet.",3 Chatbot Architecture,[0],[0]
Our proposal for a universal chatbot architecture is shown in Figure 1.,3 Chatbot Architecture,[0],[0]
"It consists of three main parts: Request Interpretation, Response Retrieval and Message Generation.",3 Chatbot Architecture,[0],[0]
The Message Generation follows the classical Natural Language Generation (NLG) pipeline described by Reiter and Dale (2000).,3 Chatbot Architecture,[0],[0]
"In the context of Request Interpretation, a “request” is not necessarily a question, but can also be any user input like “My name is John”.",3 Chatbot Architecture,[0],[0]
"Equally, a “response” to this input could e.g. be “What a nice name”.",3 Chatbot Architecture,[0],[0]
"The general goal of NLU services is the extraction of structured, semantic information from unstructured natural language input, e.g. chat messages.",4 NLU Services,[0],[0]
"They mainly do this by attaching user-defined la-
bels to messages or parts of messages.",4 NLU Services,[0],[0]
"At the time of writing, among the most popular NLU services are:
• LUIS1
• Watson Conversation2
• API.ai3
• wit.ai4
• Amazon Lex5
Moreover, there is a popular open source alternative which is called RASA6.",4 NLU Services,[0],[0]
"RASA offers the same functionality, while lacking the advantages of cloud-based solutions (managed hosting, scalability, etc).",4 NLU Services,[0],[0]
"On the other hand, it offers the typical advantages of self-hosted open source software (adaptability, data control, etc).
",4 NLU Services,[0],[0]
Table 1 shows a comparison of the basic functionality offered by the different services.,4 NLU Services,[0],[0]
"All of them, except for Amazon Lex, share the same basic concept: Based on example data, the user can train a classifier to classify so-called intents (which represent the intent of the whole message and are not bound to a certain position within the message) and entities (which can consist of a single or multiple characters).
",4 NLU Services,[0],[0]
Figure 2 shows a labelled sentence in the LUIS web interface.,4 NLU Services,[0],[0]
"The intent of this sentence was classified as FindConnection, with a confidence of 97%.",4 NLU Services,[0],[0]
"The labelled entities are: (next, Criterion), (train, Vehicle), (Universität, StationStart), (MaxWeber-Platz, StationDest).",4 NLU Services,[0],[0]
"Amazon Lex shares
1https://www.luis.ai 2https://www.ibm.com/watson/
developercloud/conversation.html 3https://www.api.ai 4https://www.wit.ai 5https://aws.amazon.com/lex 6https://www.rasa.ai
the concept of intents with the other services, but instead of entities, Lex is using so-called slots, which are not trained by concrete examples, but example patterns like “When is the {Criterion} {Vehicle} to {StationDest}”.",4 NLU Services,[0],[0]
"Moreover, all services, except for Amazon Lex, also offer an export and import functionality which uses a json-format to export and import the training data.",4 NLU Services,[0],[0]
"While wit.ai offers this functionality, as of today, it only works reliably for creating backups and restoring them, but not importing new data7.
",4 NLU Services,[0],[0]
"When it comes to the core of the services, the machine learning algorithms and the data on which they are initially trained, all services are very secretive.",4 NLU Services,[0],[0]
"None of them gives specific information about the used technologies and datasets.
7cf.",4 NLU Services,[0],[0]
"e.g. https://github.com/wit-ai/wit/ issues?q=is%3Aopen+is%3Aissue+label% 3Aimport
The exception in this case is, of course, RASA, which can either use MITIE (Geyer et al., 2016) or spaCy (Choi et al., 2015) as ML backend.",4 NLU Services,[0],[0]
Our evaluation is based on two very different data corpora.,5 Data Corpus,[0],[0]
"The Chatbot Corpus (cf. Section 5.1) is based on questions gathered by a Telegram chatbot in production use, answering questions about public transport connections.",5 Data Corpus,[0],[0]
The StackExchange Corpus (cf. Section 5.2) is based on data from two StackExchange8 platforms: ask ubuntu9 and Web Applications10.,5 Data Corpus,[0],[0]
Both corpora are available on GitHub under the Creative Commons CC BY-SA 3.0 license11: https://github.com/sebischair/ NLU-Evaluation-Corpora.,5 Data Corpus,[0],[0]
"The Chatbot Corpus consists of 206 questions, which were manually labelled by the authors.",5.1 Chatbot Corpus,[0],[0]
"There are two different intents (Departure Time,
8https://www.stackexchange.com 9https://www.askubuntu.com
",5.1 Chatbot Corpus,[0],[0]
"10https://webapps.stackexchange.com 11https://creativecommons.org/licenses/
by-sa/3.0/
Find Connection) in the corpus and five different entity types (StationStart, StationDest, Criterion, Vehicle, Line).",5.1 Chatbot Corpus,[0],[0]
"The general language of the questions was English, however, mixed with German street and station names.",5.1 Chatbot Corpus,[0],[0]
Example entries from the corpus can be found in Appendix A.1.,5.1 Chatbot Corpus,[0],[0]
"For the evaluation, the corpus was split into a training dataset with 100 entries and a test dataset with 106 entries.
",5.1 Chatbot Corpus,[0],[0]
43% of the questions in the training dataset belong to the intent Departure Time and 57% to Find Connection.,5.1 Chatbot Corpus,[0],[0]
The distribution for the test dataset is 33% (Departure Time) and 67% (Find Connection).,5.1 Chatbot Corpus,[0],[0]
Table 2 shows how the different entity types are distributed among the two datasets.,5.1 Chatbot Corpus,[0],[0]
"While some entity types occur very often, like StationStart, some occur very rarely, especially Line.",5.1 Chatbot Corpus,[0],[0]
"We do this differentiation to evaluate, if some services handle very common, or very rare, entity types better than others.
",5.1 Chatbot Corpus,[0],[0]
"While in this corpus, there are more tagged entities in the training dataset than in the test dataset, it is the other way round in the other corpus, which will be introduced in the next section.",5.1 Chatbot Corpus,[0],[0]
"Although one might expect that this leads to better results, the evaluation in Section 7 shows that this is not necessarily the case.",5.1 Chatbot Corpus,[0],[0]
"For the generation of the StackExchange corpus, we used the StackExchange Data Explorer12.",5.2 StackExchange Corpus,[0],[0]
"We choose the most popular questions (i.e. questions with the highest scores and most views), from the two StackExchange platforms ask ubuntu and Web Applications, because they are likely to have a better linguistic quality and a higher relevance, compared to less popular questions.",5.2 StackExchange Corpus,[0],[0]
"Additionally, we used only questions with an accepted, i.e. correct, answer.",5.2 StackExchange Corpus,[0],[0]
"Although we did not use the answers in
12https://data.stackexchange.com
our evaluation, we included them in our corpus, in order to create a corpus that is not only useful for this particular evaluation, but also for research on question answering in general.",5.2 StackExchange Corpus,[0],[0]
"In this way, we gathered 290 questions and answers in total, 100 from Web Applications and 190 from ask ubuntu.
",5.2 StackExchange Corpus,[0],[0]
The corpus was labelled with intents and entities using Amazon Mechanical Turk (AMT).,5.2 StackExchange Corpus,[0],[0]
"Each question was labelled by five different workers, summing up to nearly 1,500 datapoints.
",5.2 StackExchange Corpus,[0],[0]
"For each platform, we created a list of candidates for intents, which were extracted from the labels (i.e. tags) assigned to the questions by StackExchange users.",5.2 StackExchange Corpus,[0],[0]
"For each question, the AMT workers were asked to chose one of these intents or “None”, if they think no candidate is fitting.
",5.2 StackExchange Corpus,[0],[0]
"For ask ubuntu, the possible intents were: “Make Update”, “Setup Printer”, “Shutdown Computer”, and “Software Recommendation”.
",5.2 StackExchange Corpus,[0],[0]
"For Web Applications, the candidates were: “Change Password”, “Delete Account”, “Download Video”, “Export Data”, “Filter Spam”, “Find Alternative”, and “Sync Accounts”.
",5.2 StackExchange Corpus,[0],[0]
"Similarly, a set of entity type candidates were given.",5.2 StackExchange Corpus,[0],[0]
"By marking parts of the questions with the mouse, workers could assign these entity types to words (or characters) within the question.",5.2 StackExchange Corpus,[0],[0]
"For Web Applications the possible entity types were: “WebService”, “OperatingSystem” and “Browser”.",5.2 StackExchange Corpus,[0],[0]
"For ask ubuntu, they were: “SoftwareName”, “Printer”, and “UbuntuVersion”.
",5.2 StackExchange Corpus,[0],[0]
"Moreover, workers were asked to state how confident they are in their assessment: very confident, somewhat confident, undecided, somewhat unconfident, or very unconfident.
",5.2 StackExchange Corpus,[0],[0]
"For the generation of the annotated, final corpus, only submissions with a confidence level of “undecided” or higher were taken into account.",5.2 StackExchange Corpus,[0],[0]
"A label, no matter if intent or entity, was only added to the corpus if the inter-annotator agreement among those confident annotators was 60% or higher.",5.2 StackExchange Corpus,[0],[0]
"If no intent could be found for a question, satisfying these criteria, this question was not added to the corpus.",5.2 StackExchange Corpus,[0],[0]
"The final corpus was also checked for false positives by two experts, but non were found.",5.2 StackExchange Corpus,[0],[0]
"Therefore the final corpus consists of 251 entries, 162 from ask ubuntu and 89 from Web Applications.",5.2 StackExchange Corpus,[0],[0]
"Example entries from the corpus are shown in Appendix A.2.
",5.2 StackExchange Corpus,[0],[0]
"For the evaluation, we also split this corpus.
Four datasets were separated, one for training and one for testing, for each platform.",5.2 StackExchange Corpus,[0],[0]
"The distribution of intents among these datasets is shown in Table 3, the distribution of entity types is shown in Table 4.",5.2 StackExchange Corpus,[0],[0]
"Again, we do this differentiation to compare the classification results for frequently and rarely occurring intents and entity types.",5.2 StackExchange Corpus,[0],[0]
"In order to compare the performance of the different NLU services, we used the corpora described in Section 5.",6 Experimental Design,[0],[0]
"We used the respective training
datasets to train the NLU services LUIS, Watson Conversation, API.ai, and RASA.",6 Experimental Design,[0],[0]
"Amazon Lex was not included in this comparison because, as mentioned in Section 4, it does not offer a batch import functionality, which is crucial in order to effectively train all services with the exact same data.",6 Experimental Design,[0],[0]
"For the same reason, wit.ai was also excluded from the experiment.",6 Experimental Design,[0],[0]
"While it does offer an import option, currently, it only works reliable for data which was created through the wit.ai webinterface and not altered, or even created, manually.
",6 Experimental Design,[0],[0]
"Afterwards, the test datasets were sent to the NLU services and the labels created by the services were compared against our human created gold standard.",6 Experimental Design,[0],[0]
"For training, we used the batch import interfaces, offered by all compared services, in this way it was not only possible to train all different services relatively fast, despite many hundred individual labels, it also guaranteed, that all services are fed with exactly the same data.",6 Experimental Design,[0],[0]
"Since the data format differs from service to service, we used a Python script to automatically convert the training datasets from the format shown in the Appendix to the respective data format of the services.",6 Experimental Design,[0],[0]
"For retrieving the results for the test datasets from the NLU services, their respective RESTAPIs were used.
",6 Experimental Design,[0],[0]
"In order to evaluate the results, we calculated true positives, false positives, and false negatives, based on exact matches.",6 Experimental Design,[0],[0]
"Based on this data, we computed precision and recall as well as F-score for single intents, entity types, and corpora, as well as overall results.",6 Experimental Design,[0],[0]
We will say one service is better than another if it has a higher F-score.,6 Experimental Design,[0],[0]
"Before the conduction of the experiment, we had three main hypotheses:
1.",6.1 Hypotheses,[0],[0]
"The performance varies between services: Although it might sound obvious, it is worth mentioning that one of the reasons for this evaluation is the fact that we think, there is a difference between the compared NLU services.",6.1 Hypotheses,[0],[0]
"Despite their very similar concepts and “look and feel”, we expect differences when it comes to annotation quality (i.e. F-scores), which should be taken into account when deciding for one or another service.
2.",6.1 Hypotheses,[0],[0]
"The commercial products will (overall) perform better:
The initial language model of RASA, which comes with MITIE, is about 300 MB of data.",6.1 Hypotheses,[0],[0]
"The commercial services, on the other hand, are fed with data by hundreds, if not thousands, of users every day.",6.1 Hypotheses,[0],[0]
"We, therefore, assume, that the commercial products will perform better in the evaluation, especially when the training data is sparse.
3.",6.1 Hypotheses,[0],[0]
"The quality of the labels is influenced by the domain: We assume that, depending on the used algorithms and models, individual services will perform differently in different domains.",6.1 Hypotheses,[0],[0]
"Therefore, we think it is not unlikely that a service which performs well on the more technical corpus from StackExchange will perform considerably worse on the chatbot corpus, which has a focus on spatial and time data, and vice versa.",6.1 Hypotheses,[0],[0]
One important limitation of this evaluation is the fact that the results will not be representative for other domains.,6.2 Limitations,[0],[0]
"On the opposite, as already mentioned in Hypothesis 3, we do believe that there are important differences in performance between different domains.",6.2 Limitations,[0],[0]
"Therefore our final conclusion can not be that one service is absolutely better than the others, but rather that on the given corpus, one service performed better than the others.",6.2 Limitations,[0],[0]
"However, we believe that the here presented approach will help developers to conduct evaluations of NLU services for their domain and thus empower them to make better-informed decisions.
",6.2 Limitations,[0],[0]
"With regard to the used corpora, we made an effort to make them as naturally as possible by using only real data from real users.",6.2 Limitations,[0],[0]
"However, when analysing the results, one should keep in mind that the Chatbot Corpus consists of questions which were asked by users, which were aware of communicating with a chatbot.",6.2 Limitations,[0],[0]
"It is, therefore, conceivable that they formulated their questions in a way which they expect to be more understandable for a chatbot.
",6.2 Limitations,[0],[0]
"Finally, NLU services, like all other services, can change over time (and hopefully improve).",6.2 Limitations,[0],[0]
"While it is easy to track these changes for locally installed software, changes on cloud-based services may happen without any notice to the user.",6.2 Limitations,[0],[0]
"Conducting the very same experiment, described in this paper, in six months time, might, therefore,
lead to different results.",6.2 Limitations,[0],[0]
This evaluation can therefore only be a snapshot of the current state of the compared services.,6.2 Limitations,[0],[0]
"While this might decrease the reproducibility of our experiment, it is also a good argument for a formalized, repeatable evaluation process, as we describe it in this paper.",6.2 Limitations,[0],[0]
"The detailed results of the evaluation, broken down on single intents, entity types, corpora, and overall, are shown in Table 5 to 8.",7 Evaluation,[0],[0]
Each table shows the result from a different NLU service.,7 Evaluation,[0],[0]
"Within the tables, each row represents one particular entity type or intent.
",7 Evaluation,[0],[0]
"For each row, the corpus, type (intent/entity), and true positives, false negatives, and false positives are given.",7 Evaluation,[0],[0]
"From these values, precision, recall, and F-score have been calculated.",7 Evaluation,[0],[0]
The entity types and intents are also sorted by the corpus they appear in.,7 Evaluation,[0],[0]
"For each corpus, there is a summary row, which shows precision, recall, and Fscore for the whole corpus.",7 Evaluation,[0],[0]
"At the bottom of each table, there is also an overall summary.
",7 Evaluation,[0],[0]
"From a high-level perspective, LUIS performed best with an F-score of 0.916, followed by RASA (0.821), Watson Conversation (0.752), and API.ai (0.687).",7 Evaluation,[0],[0]
"LUIS also performed best on each individual dataset: chatbot, web apps, and ask ubuntu.",7 Evaluation,[0],[0]
"Similarly, API.ai performed worst on every dataset, while the second place changes between RASA and Watson Conversation (cf.",7 Evaluation,[0],[0]
"Figure 3).
",7 Evaluation,[0],[0]
"Based on this data, the second hypothesis can be rejected.",7 Evaluation,[0],[0]
"Although the best performance was indeed shown by a commercial product, RASA easily competes with the other commercial products.
",7 Evaluation,[0],[0]
The first hypothesis is supported by our findings.,7 Evaluation,[0],[0]
"We can see a difference between the services, with the F-score of LUIS being nearly 0.3 higher than the F-score of API.ai.",7 Evaluation,[0],[0]
"However, a conducted two-way ANOVA analysis with the Fscore as dependent variable and the NLU service and the entity type/intent as fixed factors does not show a significance at the level of p < 0.05 (p = 0.234, df = 3).",7 Evaluation,[0],[0]
"An even larger corpus might be necessary to get quantitatively more robust results.
",7 Evaluation,[0],[0]
"With regard to the third hypothesis, the picture is less clear.",7 Evaluation,[0],[0]
"Although we can see a clear influence of the domain on the F-score within each service, the ranking between different services is not
much influenced.",7 Evaluation,[0],[0]
"LUIS always performs best, independent from the domain, API.ai always worst, also independent from the domain, merely the second and third place changes.",7 Evaluation,[0],[0]
"Therefore, although the domain influences the results, it is not clear whether or not it should also influence the decision which service should be used.
",7 Evaluation,[0],[0]
"On a more detailed level, we also see differences between entities and intents.",7 Evaluation,[0],[0]
Especially API.ai seems to have big troubles identifying entities.,7 Evaluation,[0],[0]
"On the web apps corpus, for example, API.ai did not identify a single occurrence of the entity type WebService, which occurred 64 times in the dataset.",7 Evaluation,[0],[0]
"If we calculate the F-score for this dataset only based on the intents, it would increase from 0.519 to 0.803.",7 Evaluation,[0],[0]
"The overall results of API.ai were therefore heavily influenced by its shortcomings regarding entity detection.
",7 Evaluation,[0],[0]
"If we look at intents and entity types with sparse training data, like Line, ChangePassword, and ExportData, other than we expected, we do not see a significantly better performance of commercial services.",7 Evaluation,[0],[0]
"The evaluation of the NLU services LUIS, Watson Conversation, API.ai, and RASA, based on the two corpora we presented in Section 5, has shown that the quality of the annotations differs between the different services.",8 Conclusion,[0],[0]
"Before using an NLU service, no matter if for commercial or scientific purposes, one should therefore compare the different services with domain specific data.
",8 Conclusion,[0],[0]
"For our two corpora, LUIS showed the best results, however, the open source alternative RASA could achieve similar results.",8 Conclusion,[0],[0]
"Given the advantages of open source solutions (mainly adaptability), it might well be possible to achieve an even better results with RASA, after some customization.
",8 Conclusion,[0],[0]
"With regard to absolute numbers, it is difficult to decide whether an F-score of 0.916 or 0.821 is satisfactory for productive use within a conversational question answering system.",8 Conclusion,[0],[0]
This decision also depends strongly on the concrete use case.,8 Conclusion,[0],[0]
"We, therefore, focused on relative comparisons in our evaluation and leave this decision to future users.",8 Conclusion,[0],[0]
"A.1 Examples Chatbot Corpus
{ ""text"": ""what is the cheapest
↪→ connection between ↪→ quiddestraße and ↪→ hauptbahnhof?"",
""intent"": ""FindConnection"", ""entities"":",A Supplemental Material,[0],[0]
"[
{ ""entity"": ""Criterion"", ""start"": 3, ""stop"": 3
}, {
""entity"": ""StationStart"", ""start"": 6, ""stop"": 6
}, {
""entity"": ""StationDest"", ""start"": 8, ""stop"": 8
} ]
}, {
""text"": ""when is the next u6 ↪→ leaving from garching?"",
""intent"": ""DepartureTime"", ""entities"": [
{ ""entity"": ""Line"", ""start"": 4, ""stop"": 4
}, {
""entity"": ""StationStart"", ""start"": 7, ""stop"": 7
} ]
}
A.2 Examples StackExchange Corpus
A.2.1 Web Applications Dataset
{ ""text"": ""How can I delete my
↪→ Twitter account?"",
""url"": ""http:// ↪→ webapps.stackexchange.com ↪→ /questions/57/how-can-i↪→ delete-my-twitter-account ↪→ "",
""author"": ""Jared Harley"", ""answer"": {
""text"": """,A Supplemental Material,[0],[0]
"[...]"", ""author"": ""Ken Pespisa""
}, ""intent"": ""Delete Account"", ""entities"": [
{ ""text"": ""Twitter"", ""stop"": 5, ""start"": 5, ""entity"": ""WebService""
} ]
}, {
""text"": ""Is it possible to ↪→ export my data from ↪→ Trello to back it up?"",
""url"": ""http:// ↪→ webapps.stackexchange.com ↪→ /questions/18975/is-it↪→ possible-to-export-my↪→ data-from-trello-to-back↪→ it-up"",
""author"": ""Clare Macrae"", ""answer"": {
""text"": ""[...]"", ""author"": ""Daniel LeCheminant
↪→ "" }, ""intent"": ""Export Data"", ""entities"": [
{ ""text"": ""Trello"", ""stop"": 8, ""start"": 8, ""entity"": ""WebService""
} ]
}
A.2.2 Ask Ubuntu Dataset
{ ""text"": ""How do I install the
↪→ HP F4280 printer?"", ""url"": ""http://askubuntu.com/
↪→ questions/24073/how-do-i↪→ install-the-hp-f4280↪→ printer"",
""author"": ""ok comp"", ""answer"": {
""text"": """,A Supplemental Material,[0],[0]
"[...]"", ""author"": ""nejode""
}, ""intent"": ""Setup Printer"", ""entities"": [
{ ""text"": ""HP F4280"", ""stop"": 6, ""start"": 5, ""entity"": ""Printer""
} ]
}, {
""text"": ""What is a good MongoDB ↪→ GUI client?"",
""url"": ""http://askubuntu.com/ ↪→ questions/196136/what-is↪→ a-good-mongodb-gui-client ↪→ "",
""author"": ""Eyal"", ""answer"": {
""text"": ""[...]"", ""author"": ""Eyal""
}, ""intent"": ""Software
↪→ Recommendation"", ""entities"":",A Supplemental Material,[0],[0]
"[
{ ""text"": ""MongoDB"", ""stop"": 4, ""start"": 4, ""entity"": ""SoftwareName""
} ]
}",A Supplemental Material,[0],[0]
Conversational interfaces recently gained a lot of attention.,abstractText,[0],[0]
"One of the reasons for the current hype is the fact that chatbots (one particularly popular form of conversational interfaces) nowadays can be created without any programming knowledge, thanks to different toolkits and socalled Natural Language Understanding (NLU) services.",abstractText,[0],[0]
"While these NLU services are already widely used in both, industry and science, so far, they have not been analysed systematically.",abstractText,[0],[0]
"In this paper, we present a method to evaluate the classification performance of NLU services.",abstractText,[0],[0]
"Moreover, we present two new corpora, one consisting of annotated questions and one consisting of annotated questions with the corresponding answers.",abstractText,[0],[0]
"Based on these corpora, we conduct an evaluation of some of the most popular NLU services.",abstractText,[0],[0]
"Thereby we want to enable both, researchers and companies to make more educated decisions about which service they should use.",abstractText,[0],[0]
Evaluating Natural Language Understanding Services for Conversational Question Answering Systems,title,[0],[0]
"Proceedings of the SIGDIAL 2015 Conference, pages 199–208, Prague, Czech Republic, 2-4 September 2015. c©2015 Association for Computational Linguistics",text,[0],[0]
"Time-offset interaction allows real-time synchronous conversational interaction with a person who is not only physically absent, but also not engaged in the conversation at the same time.",1 Introduction,[0],[0]
"The basic premise of time-offset interaction is that when the topic of conversation is known, the participants’ utterances are predictable to a large extent (Gandhe and Traum, 2010).",1 Introduction,[0],[0]
"Knowing what an interlocutor is likely to say, a speaker can record statements in advance; during conversation, a computer program selects recorded statements that are appropriate reactions to the interlocutor’s utterances.",1 Introduction,[0],[0]
"The selection of statements can be done in a similar fashion to existing interactive systems with synthetic characters (Leuski and Traum, 2011).
",1 Introduction,[0],[0]
"In Artstein et al. (2014) we presented a proof of concept of time-offset interaction, which showed that given sufficiently interesting content, a reasonable interactive conversation could be demonstrated.",1 Introduction,[0],[0]
"However that system had a very small
amount of content, and would only really work if someone asked questions about a very limited set of topics.",1 Introduction,[0],[0]
There is a big gap from this proof of concept to evidence that the technique can work more generally.,1 Introduction,[0],[0]
One of the biggest questions is how much material needs to be recorded in order to support free-flowing conversation with naive interactors who don’t know specifically what they can ask.,1 Introduction,[0],[0]
"This question was addressed, at least for one specific case, in Artstein et al. (2015).",1 Introduction,[0],[0]
"There we showed that an iterative development process involving two separated recording sessions, with Wizard of Oz testing in the middle, resulted in a body of material of around 2000 responses that could be used to answer over 95% of questions from the desired target audience.",1 Introduction,[0],[0]
"In contrast, the 1400 responses from the first recording session alone was sufficient to answer less than 70% of users’ questions.",1 Introduction,[0],[0]
Another question is whether current language processing technology is adequate to pick enough appropriate responses to carry on interesting and extended dialogues with a wide variety of interested interactors.,1 Introduction,[0],[0]
"The proof of concept worked extremely well, even when people phrased questions very differently from the training data.",1 Introduction,[0],[0]
"However, that system had very low perplexity, with fewer than 20 responses, rather than something two orders of magnitude bigger.
",1 Introduction,[0],[0]
"In this paper, we address the second question, of whether time-offset interaction can be automatically supported at a scale that can support interaction with people who know only the general topic of discussion, not what specific content is available.",1 Introduction,[0],[0]
"In the next section, we review related work that is similar in spirit to time-offset interaction.",1 Introduction,[0],[0]
"In Section 3 we review our materials, including the domain of interaction, the system architecture, dialogue policy, and collected training and test data.",1 Introduction,[0],[0]
"In Section 4, we describe our evaluation methodology, including evaluation of speech recognition and classifier.",1 Introduction,[0],[0]
"In Section 5, we present our results,
199
showing that over 70% of user utterances can be given a direct answer, and an even higher percentage can reach task success through a clarification process.",1 Introduction,[0],[0]
We conclude with a discussion and future work in Section 6.,1 Introduction,[0],[0]
The idea for time-offset interaction is not new.,2 Related Work,[0],[0]
We see examples of this in science fiction and fantasy.,2 Related Work,[0],[0]
"For example, in the Hollywood movie “I, Robot”, Detective Spooner (Will Smith) interviews a computer-driven hologram of a recently deceased Dr. Lanning (James Cromwell).
",2 Related Work,[0],[0]
"The first computer-based dialogue system that we are aware of, that enabled a form of time-offset interactions with real people was installed at the Nixon Presidential Library in late 1980s (Chabot, 1990).",2 Related Work,[0],[0]
"The visitors were able to select one of over 280 predefined questions on a computer screen and observe a video of Nixon answering that question, taken from television interviews or filmed specifically for the project.",2 Related Work,[0],[0]
"This system did not allow Natural language input.
",2 Related Work,[0],[0]
"In the late 1990s Marinelli and Stevens came up with the idea of a “Synthetic Interview”, where users can interact with a historical persona that was composed using clips of an actor playing that historical character and answering questions from the user (Marinelli and Stevens, 1998).",2 Related Work,[0],[0]
"“Ben Franklin’s Ghost” is a system built on those ideas and was deployed in Philadelphia from 2005– 2007 (Sloss and Watzman, 2005).",2 Related Work,[0],[0]
"This system had a book in which users could select questions, but, again, did not use unrestricted natural language input.
",2 Related Work,[0],[0]
"What we believe is novel with our New Dimensions in Testimony prototype is the ability to interact with a real person, not an actor playing a historical person, and also the evaluation of its ability to interact naturally, face to face, using speech.",2 Related Work,[0],[0]
Our initial domain for time-offset interaction is the experiences of a Holocaust survivor.,3.1 Domain,[0],[0]
"Currently, an important aspect of Holocaust education in museums and classrooms is the opportunity to meet a survivor, hear their story firsthand, and interact with them.",3.1 Domain,[0],[0]
"This direct contact and ability to ask questions literally brings the topic to life and motivates many toward further historical study and ap-
preciation and determination of tolerance for others.",3.1 Domain,[0],[0]
"Unfortunately, due to the age of survivors, this opportunity will not be available far into the future.",3.1 Domain,[0],[0]
"The New Dimensions in Testimony project (Maio et al., 2012) is an effort to preserve as much as possible of this kind of interaction.
",3.1 Domain,[0],[0]
"The pilot subject is Pinchas Gutter, who has previously told his life story many times to diverse audiences.",3.1 Domain,[0],[0]
"The most obvious topic of conversation is Pinchas’ experiences during World War II, including the Nazi invasion of Poland, his time in the Warsaw Ghetto, his experiences in the concentration camps, and his liberation.",3.1 Domain,[0],[0]
"But there are many other topics that people bring up with Pinchas, including his pre- and post-war life and family, his outlook on life, and his favorite songs and pastimes.",3.1 Domain,[0],[0]
"The automatic system is built on top of the components from the USC ICT Virtual Human Toolkit, which is publicly available.1 Specifically, we use the AcquireSpeech tool for capturing the user’s speech, CMU PocketSphinx2 and Google Chrome ASR3 tools for converting the audio into text, NPCEditor (Leuski and Traum, 2011) for classifying the utterance text and selecting the appropriate response, and a video player to deliver the selected video response.",3.2 System architecture,[0],[0]
The individual components run as separate applications on the user’s machine and are linked together by ActiveMQ messaging4:,3.2 System architecture,[0],[0]
"An instance of ActiveMQ broker runs on the machine, each component connects to the server and sends and receives messages to other components via the broker.",3.2 System architecture,[0],[0]
"The system setup also includes the JLogger component for recording the messages, and the Launcher tool that controls starting and stopping of individual tools.",3.2 System architecture,[0],[0]
"For example, the user can select between PocketSphinx and Google ASR engines by checking the appropriate buttons in the Launcher interface.",3.2 System architecture,[0],[0]
Figure 1 shows the overall system architecture.,3.2 System architecture,[0],[0]
We show the data flow through the system as black lines.,3.2 System architecture,[0],[0]
Gray arrows indicate the control messages from the Launcher interface.,3.2 System architecture,[0],[0]
"Solid arrows represent messages passed via ActiveMQ and dotted lines represent data going over TCP/IP.
",3.2 System architecture,[0],[0]
"While most of the system components already
1http://vhtoolkit.ict.usc.edu 2http://cmusphinx.sourceforge.net 3https://www.google.com/intl/en/chrome/demos/speech.html 4http://activemq.apache.org
existed before the start of this project, the Google Chrome ASR Client and VideoPlayer tools were developed in the course of this project.",3.2 System architecture,[0],[0]
Google Chrome ASR client is a web application that takes advantage of the Google Speech API available in the Chrome browser.,3.2 System architecture,[0],[0]
"The tool provides push-totalk interface control for acquiring user’s speech; it uses the API to send audio to Google ASR servers, collect the recognition result, and broadcast it over the ActiveMQ messaging.",3.2 System architecture,[0],[0]
We developed the VideoPlayer tool so that we can control the response playback via the same ActiveMQ messaging.,3.2 System architecture,[0],[0]
VideoPlayer also implements custom transition between clips.,3.2 System architecture,[0],[0]
"It has video adjustment controls so that we can modify the scale and position of the video image, and it automatically displays a loop of idle video clips while the system is in resting or listening states.
",3.2 System architecture,[0],[0]
"While the system was developed to be crossplatform so that it can run both on OS X and Windows, we conducted all our testing and experiments on OS X. The system is packaged as a single OS X application that starts the Launcher interface and the rest of the system.",3.2 System architecture,[0],[0]
This significantly simplifies distribution and installation of the system on different computers.,3.2 System architecture,[0],[0]
"Currently the system can work with two speech recognition engines, CMU PocketSphinx and Google Chrome ASR.",3.3 Speech recognition,[0],[0]
"But for our experiments we also considered Apple Dictation.5
One major decision when selecting a speech recognizer is whether it allows for training domain-specific language models (LMs) or not.6
5https://support.apple.com/en-us/HT202584 6While the acoustic models of a speech recognizer recognize individual sounds, the LM provides information about
Purely domain-specific LMs cannot recognize outof-domain words or utterances.",3.3 Speech recognition,[0],[0]
"On the other hand, general-purpose LMs do not perform well with domain-specific words or utterances.",3.3 Speech recognition,[0],[0]
"Unlike PocketSphinx, which supports trainable LMs, both Google Chrome ASR and Apple Dictation come with their own out-of-the-box LMs that cannot be modified.
",3.3 Speech recognition,[0],[0]
Table 1 shows example outputs of all three recognizers (PocketSphinx examples were obtained with a preliminary LM).,3.3 Speech recognition,[0],[0]
"As we can see, Google Chrome ASR and Apple Dictation with their general-purpose LMs perform well for utterances that are not domain-specific.",3.3 Speech recognition,[0],[0]
"On the other hand, PocketSphinx clearly is much better at recognizing domain-specific words, e.g., “Pinchas”, “Majdanek”, etc. but fails to recognize general-purpose utterances if they are not included in its LM.",3.3 Speech recognition,[0],[0]
"For example, the user input “what’s your favorite restaurant” is misrecognized as “what’s your favorite rest shot” because the word “restaurant” or the sequence “favorite restaurant” was not part of the LM’s training data.",3.3 Speech recognition,[0],[0]
"Similarly, the user input “did you serve in the army” is misrecognized as “did you certain the army” because the word “serve” or the sequence “serve in the army” was not included in the LM’s training data.
",3.3 Speech recognition,[0],[0]
"For training LMs for PocketSphinx we used the CMU Statistical Language Modeling toolkit (Clarkson and Rosenfeld, 1997) with back-off 3- grams.",3.3 Speech recognition,[0],[0]
The CMU pronouncing dictionary v0.7a,3.3 Speech recognition,[0],[0]
"(Weide, 2008) was used as the main dictionary with the addition of domain-dependent words, such as names.",3.3 Speech recognition,[0],[0]
We used the standard US English acoustic models that are included in PocketSphinx.,3.3 Speech recognition,[0],[0]
"As mentioned in section 3.2, NPCEditor combines the functions of Natural Language Understanding (NLU) and Dialogue Management – understanding the utterance text and selecting an appropriate response.",3.4 Dialogue policy,[0],[0]
"The NLU functionality is a classifier trained on linked question-response pairs, which identifies the most appropriate response to new (unseen) user input.",3.4 Dialogue policy,[0],[0]
The dialogue management logic is designed to deal with instances where the classifier cannot identify a good direct response.,3.4 Dialogue policy,[0],[0]
"During training, NPCEditor calculates a response
what the recognizer should expect to listen to and recognize.",3.4 Dialogue policy,[0],[0]
"If a word or a sequence of words is not included in the LM, they will never be recognized.
",3.4 Dialogue policy,[0],[0]
threshold based on the classifier’s confidence in the appropriateness of selected responses: this threshold finds an optimal balance between false positives (inappropriate responses above threshold) and false negatives (appropriate responses below threshold) in the training data.,3.4 Dialogue policy,[0],[0]
"At runtime, if the confidence for a selected response falls below the predetermined threshold, that response is replaced with an “off-topic” utterance that asks the user to repeat the question or takes initiative and changes the topic (Leuski et al., 2006); such failure to return a direct response, also called non-understanding (Bohus and Rudnicky, 2005), is usually preferred over returning an inappropriate one (misunderstanding).
",3.4 Dialogue policy,[0],[0]
The current system uses a five-stage off-topic selection algorithm which is an extension of that presented in Artstein et al. (2009).,3.4 Dialogue policy,[0],[0]
"The first time Pinchas fails to understand an utterance, he will assume this is a speech recognition error and ask the user to repeat it.",3.4 Dialogue policy,[0],[0]
"If the misunderstanding persists, Pinchas will say that he doesn’t know (without asking for repetition), and the third time he will state that he cannot answer the user’s utterance.",3.4 Dialogue policy,[0],[0]
"In a severe misunderstanding that persists beyond three exchanges, Pinchas will suggest a new topic in the fourth turn, and if even this fails to bring the user to ask a question that Pinchas can understand, then in the fifth turn Pinchas will give a quick segue and launch into a story of his choice.",3.4 Dialogue policy,[0],[0]
"If at any point Pinchas hears an utterance that he can understand (that is, if the classifier finds a response above threshold), Pinchas will answer this directly, and the off-topic state will reset to zero.
",3.4 Dialogue policy,[0],[0]
A separate component of the dialogue policy is designed to avoid repetition.,3.4 Dialogue policy,[0],[0]
"Normally, Pinchas responds with the top-ranked response if it is above the threshold.",3.4 Dialogue policy,[0],[0]
"However, if the topranked response has been recently used (within a 4-turn window) and a lower ranked response
is also above the threshold, Pinchas will respond with the lower ranked response.",3.4 Dialogue policy,[0],[0]
"If the only responses above threshold are among the recently used then Pinchas will choose one of them, since repetition is considered preferable to responding with an off-topic or inappropriate statement.",3.4 Dialogue policy,[0],[0]
"The development process consisted of several stages: preliminary planning and question gathering, initial recording of survivor statements, Wizard of Oz studies using the recorded statements to identify gaps in the content, a second recording of survivor statements to address the gaps, assembly of an automated dialogue system, and continued testing with the automated system.",3.5 Data collection,[0],[0]
"The development process has been described in detail in Artstein et al. (2015); here we describe the data collected at the various stages of development, which constitute the training and test data for the automated system.
",3.5 Data collection,[0],[0]
"In the preliminary planning stages, potential user questions were collected from various sources, but these were not used directly as system training data.",3.5 Data collection,[0],[0]
"Instead, these questions formed the basis for an interview script that was used for eliciting the survivor statements during the recording sessions.",3.5 Data collection,[0],[0]
The first training data include the actual utterances used during these elicitation interviews.,3.5 Data collection,[0],[0]
"The interviewer utterances were manually linked to the survivor responses; in the typical case, an utterance is linked to the response it elicited during the recording sessions, but the links were manually adjusted to remove instances when the response was not appropriate, and to add links to additional appropriate responses.
",3.5 Data collection,[0],[0]
"Additional training data were collected in the various stages of user testing – the Wizard of Oz testing between the first and second recording sessions, and fully automated system testing
following the second recording.",3.5 Data collection,[0],[0]
"Wizard of Oz testing took place in June and July 2014; participants sat in front of a screen that showed roughcut video segments of Mr. Gutter’s statements, selected by human operators in response to user utterances in real time.",3.5 Data collection,[0],[0]
"Since the Wizard of Oz testing took place prior to the second recording, wizards were only able to choose statements from the first recording.",3.5 Data collection,[0],[0]
"The user utterances were recorded, transcribed, and analyzed to form the basis for the elicitation script for the second recording.",3.5 Data collection,[0],[0]
"Subsequent to the second recording, these utterances were reannotated to identify appropriate responses from all of the recorded statements, and these reannotated question-response links form the Wizard of Oz portion of the training data.
",3.5 Data collection,[0],[0]
"Testing with the automated system was carried out starting in October 2014, following the second recording of survivor statements.",3.5 Data collection,[0],[0]
"Users spoke to the automated system, and their utterances were recorded, transcribed, and annotated with appropriate responses.",3.5 Data collection,[0],[0]
"These data are partitioned into two – the testing that took place in late 2014 was mostly internal, with team members, other institute staff, and visitors, while the testing from early 2015 was mostly external, conducted over 3 days at a local museum.",3.5 Data collection,[0],[0]
"We thus have 4 portions of training data, summarized in Table 2.
",3.5 Data collection,[0],[0]
Test data for evaluating the classifier performance were taken from the system testing in late 2014.,3.5 Data collection,[0],[0]
"We picked a set of 400 user utterances, collected during the last day of testing, which was conducted off-site and therefore consisted primarily of external test participants (these utterances are not counted in Table 2 above).",3.5 Data collection,[0],[0]
We only included in-domain utterances for which an appropriate on-topic response was available.,3.5 Data collection,[0],[0]
"The evaluation therefore measures the ability of the system to identify an appropriate response when one is available, not its ability to identify instances where an on-topic response is unavailable.",3.5 Data collection,[0],[0]
"There
is some overlap in the test questions, so the 400 instances contain only 341 unique question types, with the most frequent question (What is your name?) occurring 5 times.",3.5 Data collection,[0],[0]
"We believe it is fair to include such overlap in the test set, since it gives higher weight to the more frequent questions.",3.5 Data collection,[0],[0]
"Also, while the text of overlapping questions is identical, each instance is associated with a unique audio file; these utterances may therefore yield different speech recognizer outputs, resulting in different outcomes.
",3.5 Data collection,[0],[0]
The test set was specially annotated to serve as a test key.,3.5 Data collection,[0],[0]
"There is substantial overlap in content between the recorded survivor statements, so many user utterances can be addressed appropriately by more than one response.",3.5 Data collection,[0],[0]
"For training purposes it is sufficient to link each user utterance to some appropriate responses, but the test key must link each utterance to all appropriate responses.",3.5 Data collection,[0],[0]
"It is impractical to check each of the 400 test utterances against all 1726 possible responses, so instead we used the following procedure to identify responses that are likely to come up in response to specific test questions: we trained the system under different partitions of the training data and different training parameters, ran the test questions through each of the system versions, and from each system run we collected the responses that the system considered appropriate (that is, above threshold) for each question.",3.5 Data collection,[0],[0]
"This resulted in a set of 3737 utterance-response pairs, ranging from 3 to 19 responses per utterance, which represent likely system outputs for future training configurations.",3.5 Data collection,[0],[0]
All the responses retrieved by the system were rated for coherence on a scale of 1–4 (Table 3).,3.5 Data collection,[0],[0]
"The responses rated 3 or 4 were deemed appropriate for inclusion in the test key, a total of 1838 utteranceresponse pairs, ranging from 1 to 10 responses per utterance.",3.5 Data collection,[0],[0]
"As mentioned above, neither Google nor Apple ASRs allow for trainable LMs.",4.1 Speech recognition,[0],[0]
"But for PocketSphinx we experimented with different domainspecific LMs and below we report results on PocketSphinx performance with two different domainspecific LMs: one trained on Wizard of Oz and system testing data (approx. 5000 utterances) collected until December 2014 (LM-ds), and another one trained on additional data (approx. 6500 utterances) collected until January 2015 (LM-ds-add).",4.1 Speech recognition,[0],[0]
The test set was the 400 utterances mentioned above.,4.1 Speech recognition,[0],[0]
"There was no overlap between the training and test data sets.
",4.1 Speech recognition,[0],[0]
"In order to evaluate the performance of the speech recognizers we use the standard word error rate (WER) metric:
WER = Substitutions+Deletions+ Insertions
Length of transcription string",4.1 Speech recognition,[0],[0]
"Evaluation of the classifier is difficult, because it has to take into account the dialogue policy: the classifier typically returns the top-ranked response, but may return a lower-ranked response if it is above threshold and the higher-ranked responses were used recently.",4.2 Classifier evaluation,[0],[0]
"So while the classifier ranks all the available responses, anything below the top few will never be selected by the dialogue manager, rendering measures such as precision and recall quite irrelevant.",4.2 Classifier evaluation,[0],[0]
"An ideal evaluation should give highest weight to the correctness of the top-ranked response, with rapidly decreasing weight to the next several responses, but it is difficult to determine what weights are appropriate.",4.2 Classifier evaluation,[0],[0]
"We therefore focus on the top answer, since in most cases the top answer is what will get served to the user.
",4.2 Classifier evaluation,[0],[0]
"The top answer can be one of three outcomes: it can be appropriate (good), inappropriate (bad), or below threshold, in which case an off-topic response is served.",4.2 Classifier evaluation,[0],[0]
"A good response is better than an off-topic, which is in turn better than a bad response.",4.2 Classifier evaluation,[0],[0]
"This makes it difficult to compare systems with different off-topic rates: how do two systems compare if one gives more good and bad responses than the other, but fewer off-topics?",4.2 Classifier evaluation,[0],[0]
"We therefore compare systems using error return plots, which show the error rate across all possible return rates (Artstein, 2011): for each system we calculate the
number of errors at each return rate, and then plot the number of errors against the number of offtopics.
",4.2 Classifier evaluation,[0],[0]
We used 6 combinations of the training data described in section 3.5.,4.2 Classifier evaluation,[0],[0]
"The baseline is trained with only the elicitation questions, and represents the performance we might expect if we were to build a dialogue system based on the recording sessions alone, without collecting user question data (except to the extent that user questions influenced the second recording session).",4.2 Classifier evaluation,[0],[0]
"To this baseline we successively added training data from the Wizard of Oz testing, system testing 2014, and system testing 2015.",4.2 Classifier evaluation,[0],[0]
"Our final training sets include the elicitation questions and system testing 2014 (without Wizard of Oz data), and the same with the system testing 2015 added.
",4.2 Classifier evaluation,[0],[0]
"All of the classifiers were trained in NPCEditor using the same options: text unigrams for the question language models, text unigrams plus IDs for the response language models, and F-score as the classifier scoring function during training.",4.2 Classifier evaluation,[0],[0]
"We used 3 versions of the test utterances: the transcribed text, the output of Google ASR, and the output of PocketSphinx, and ran each version through each of the 6 classifiers – a total of 18 configurations.",4.2 Classifier evaluation,[0],[0]
"For each testing configuration, we retrieved the top-ranked response for each utterance, together with the classifier confidence and a true/false indication of whether the response matched the answer key.",4.2 Classifier evaluation,[0],[0]
"The responses were ranked by the classifier confidence, and for each possible cutoff point (from returning zero offtopic responses to returning off-topic responses for all 400 utterances), we calculated the number of errors among the on-topic responses and plotted that against the number of off-topics.",4.2 Classifier evaluation,[0],[0]
Each plot represents the error-return tradeoff for a particular testing configuration (see section 5.2).,4.2 Classifier evaluation,[0],[0]
"Table 4 shows the WERs for the three different speech recognizers and the two different LMs.
",5.1 Speech recognition evaluation,[0],[0]
Note that we also experimented with interpolating domain-specific with background LMs available from http://keithv.com/software.,5.1 Speech recognition evaluation,[0],[0]
Interpolation did not help but this is still an issue under investigation.,5.1 Speech recognition evaluation,[0],[0]
"Interpolation helped with speakers who had low WERs (smooth easy to recognize speech) but hurt in cases of speakers with high
Speech Recognizer
Language Model
General LM-ds LM-ds-add
Google 5.07% — — Apple 7.76% — — PocketSphinx — 22.04% 19.39%
Table 4: Speech recognition results (WER).",5.1 Speech recognition evaluation,[0],[0]
"General LM stands for general-purpose LM, LM-ds stands for domain-specific LM trained with data collected until December 2014, and LM-ds-add stands for domain-specific LM trained with additional data collected until January 2015.
WERs.",5.1 Speech recognition evaluation,[0],[0]
"In the latter cases, having a background model meant that there were more choices for the speech recognizer to choose from, which instead of helping caused confusion.
",5.1 Speech recognition evaluation,[0],[0]
"We also noticed that PocketSphinx was less tolerant of environmental noises, which most of the time resulted in insertions and substitutions.",5.1 Speech recognition evaluation,[0],[0]
"For example, as we can see in Table 1, the user input “have you ever lived in israel” was misrecognized by PocketSphinx as “are you ever live in a israel”.",5.1 Speech recognition evaluation,[0],[0]
"These misrecognitions do not necessarily confuse the classifier, but of course they often do.",5.1 Speech recognition evaluation,[0],[0]
"Classifier performance is best when training on all the data, and testing on transcriptions rather than speech recognizer output.",5.2 Classifier evaluation,[0],[0]
Figure 2 shows the effect of the amount of training data on classifier performance when tested on transcribed text (a similar effect is observed when testing on speech recognizer output).,5.2 Classifier evaluation,[0],[0]
Lower curves represent better performance.,5.2 Classifier evaluation,[0],[0]
"As expected, performance improves with additional training data – training on the full set of data cuts error rates by about a third compared to training on the elicitation questions alone.",5.2 Classifier evaluation,[0],[0]
"Additional training data (both new questions and question-response links) are likely to improve performance even further.
",5.2 Classifier evaluation,[0],[0]
The effect of speech recognition on classifier performance is shown in Figure 3.,5.2 Classifier evaluation,[0],[0]
"Automatic speech recognition does impose a performance penalty compared to testing on transcriptions, but the penalty is not very large: classifier errors when testing with Google ASR are between 1 and 3 percentage points higher than with transcriptions, while PocketSphinx fares somewhat worse, with classifier errors about 5 to 8 percentage points
Test set: Transcriptions
Training on all the data
higher than with transcriptions.",5.2 Classifier evaluation,[0],[0]
"At a 20% off-topic rate, the response error rates are 14% for transcriptions and 16% for Google ASR, meaning that almost two thirds of user utterances receive a direct appropriate response.",5.2 Classifier evaluation,[0],[0]
"At 30% off-topics, errors drop to 10–11%, and direct appropriate responses drop to just shy of 60%.",5.2 Classifier evaluation,[0],[0]
Informal impressions from current testing at a museum (section 6) suggests that these numbers are sufficient to enable a reasonable conversation flow.,5.2 Classifier evaluation,[0],[0]
This paper has demonstrated that time-offset interaction with a real person is achievable with present day spoken language processing technology.,6 Discussion,[0],[0]
"Not only are we able to collect a sufficiently large and varied set of statements to address user utterances (Artstein et al., 2015), we are also able to use speech recognition and language understanding technology to identify appropriate responses frequently enough to enable a natural interaction flow.",6 Discussion,[0],[0]
"Future work is needed in three areas: investigating the interaction quality of the dialogue system, improving the language processing, and generalizing the process to additional situations.
",6 Discussion,[0],[0]
"To investigate the interaction quality, we need to look at dialogues in context rather than as isolated utterances, and to collect user feedback.",6 Discussion,[0],[0]
"We are presently engaged in a joint testing, demonstration, and data collection effort that is intended to address these issues.",6 Discussion,[0],[0]
"The time-offset interaction system has been temporarily installed at the Illinois Holocaust Museum and Education Center in Skokie, Illinois, where visitors interact with the system as part of their museum experience (Isaacs, 2015).",6 Discussion,[0],[0]
"The system is set up in an auditorium and users talk to Pinchas in groups, in a setting that is similar to in-person encounters with Holocaust survivors which also take place at the museum.",6 Discussion,[0],[0]
"Due to physical limitations of the exhibit space, interaction is mediated by museum docents: each user question is relayed by the docent into the microphone, and Pinchas responds to the docent’s speech.",6 Discussion,[0],[0]
An excerpt of museum interaction is in the Appendix.,6 Discussion,[0],[0]
"Data and feedback from the museum installation will be used to evaluate the interaction quality, including user feedback as to the naturalness of the interaction and user satisfaction.
",6 Discussion,[0],[0]
The ongoing testing also serves the purpose of data collection for improving system performance:,6 Discussion,[0],[0]
"Figure 2 shows that errors diminish with additional training data, and it appears that we have not yet reached the point of diminishing returns with about 7000 training utterances.",6 Discussion,[0],[0]
"We hope to collect an average of 10 training utterances per response, that is about 17000 user utterances.",6 Discussion,[0],[0]
"Annotation is also incomplete: the test key has an average of 4.6 links per utterance, as opposed to an average of around 1.4 links per utterance in the training data.",6 Discussion,[0],[0]
"While complete linking is not necessary for classifier operation, improving the links will probably improve performance.
",6 Discussion,[0],[0]
"In addition to improving performance through improved data, there are also algorithmic improvements that can be made to the language processing components.",6 Discussion,[0],[0]
"One goal is to leverage the relative strengths of the general purpose and domainspecific ASRs, e.g., through the classifier: past work has shown that language understanding can be improved by allowing NLU to select from among several hypotheses provided by a single speech recognizer (Morbini et al., 2012), and we propose to try a similar method to utilize the outputs of separate speech recognizers.",6 Discussion,[0],[0]
Another idea is to combine/align the outputs of the speech recognizers (before they are forwarded to the classifier) taking into account information from the recognition confidence scores and lattices.,6 Discussion,[0],[0]
"This will potentially help in cases where different recognizers succeed in correctly recognizing different parts of the utterance.
",6 Discussion,[0],[0]
"Time-offset interaction has a large potential impact on preservation and education – people in the future will be able to not only see and listen to historical figures, but also to interact with them in conversation.",6 Discussion,[0],[0]
"Future research into time-offset interaction will need to generalize the development process, in order to enable efficient use of resources by identifying common user questions that are specific to the person, ones that are specific to the dialogue context or conversation topic, and ones that are of more general application.",6 Discussion,[0],[0]
This work was made possible by generous donations from private foundations and individuals.,Acknowledgments,[0],[0]
"We are extremely grateful to The Pears Foundation, Louis F. Smith, and two anonymous donors for their support.",Acknowledgments,[0],[0]
"The work was supported in part by the U.S. Army; statements and opinions expressed do not necessarily reflect the position or the policy of the United States Government, and no official endorsement should be inferred.",Acknowledgments,[0],[0]
Heather Maio and Alesia Gainer spent long hours on data collection and system testing.,Acknowledgments,[0],[0]
"The Los Angeles Museum of the Holocaust, the Museum of Tolerance, and New Roads School in Santa Monica offered their facilities for data collection.",Acknowledgments,[0],[0]
"The USC Shoah Foundation provided financial and administrative support, and facilities.",Acknowledgments,[0],[0]
"Finally, we owe special thanks to Pinchas Gutter for sharing his story, and for his tireless efforts to educate the world about the Holocaust.",Acknowledgments,[0],[0]
The following dialogue excerpt is taken from the beta testing in Illinois (see section 6).,A Dialogue sample,[0],[0]
"Speech recognition output is shown in italics when it departs substantially from the spoken text.
",A Dialogue sample,[0],[0]
User,A Dialogue sample,[0],[0]
"Hello Pinchas, how are you?",A Dialogue sample,[0],[0]
"Las Vegas how are you
Pinchas Can you just repeat that?
",A Dialogue sample,[0],[0]
"User Can you hear me OK?
",A Dialogue sample,[0],[0]
"Pinchas I can see you and I can hear you.
",A Dialogue sample,[0],[0]
User,A Dialogue sample,[0],[0]
"Hello Pinchas, can you hear me?",A Dialogue sample,[0],[0]
"how thick is can you hear me
Pinchas I can hear you, yeah.
",A Dialogue sample,[0],[0]
"User Pinchas, can you tell me how old you are?",A Dialogue sample,[0],[0]
"Vegas can you tell me how old you are
Pinchas I was born in nineteen thirty two and we are now in two thousand and fourteen and you can make your own reckoning.
",A Dialogue sample,[0],[0]
"User Can you tell me a little bit about your childhood?
",A Dialogue sample,[0],[0]
"Pinchas My life before the war, I was born in Lodz in Poland, and I was eight years old when the war started.",A Dialogue sample,[0],[0]
"So the first eight years I lived in a home, a very religious one, a Hasidic home.",A Dialogue sample,[0],[0]
"I had a twin sister, my. . .
",A Dialogue sample,[0],[0]
User Have you ever met Hitler?,A Dialogue sample,[0],[0]
mermaid,A Dialogue sample,[0],[0]
"Hitler
Pinchas Please repeat that.
",A Dialogue sample,[0],[0]
"User Did you ever meet Hitler?
",A Dialogue sample,[0],[0]
Pinchas Did I ever meet Hitler?,A Dialogue sample,[0],[0]
"Well, Hitler was a Führer of Germany",A Dialogue sample,[0],[0]
"and I was born in Poland and I was a little Jewish boy, so the chances of somebody like me meeting Hitler was impossible, because Hitler was. . .
",A Dialogue sample,[0],[0]
User,A Dialogue sample,[0],[0]
"What would you do if you met Hitler?
",A Dialogue sample,[0],[0]
"Pinchas Geyn drerd arein, that’s Yiddish and what it means go and bury yourself.",A Dialogue sample,[0],[0]
"This paper presents the first evaluation of a full automated prototype system for time-offset interaction, that is, conversation between a live person and recordings of someone who is not temporally copresent.",abstractText,[0],[0]
"Speech recognition reaches word error rates as low as 5% with generalpurpose language models and 19% with domain-specific models, and language understanding can identify appropriate direct responses to 60–66% of user utterances while keeping errors to 10–16% (the remainder being indirect, or off-topic responses).",abstractText,[0],[0]
"This is sufficient to enable a natural flow and relatively open-ended conversations, with a collection of under 2000 recorded statements.",abstractText,[0],[0]
Evaluating Spoken Dialogue Processing for Time-Offset Interaction,title,[0],[0]
Word embeddings are a popular technique in natural language processing (NLP) in which the words in a vocabulary are mapped to low-dimensional vectors.,1 Introduction,[0],[0]
"Embedding models are easily trained—several implementations are publicly available—and relationships between the embedding vectors, often measured via cosine similarity, can be used to reveal latent semantic relationships between pairs of words.",1 Introduction,[0],[0]
"Word embeddings are increasingly being used by researchers in unexpected ways and have become popular in fields such as digital humanities and computational social science (Hamilton et al., 2016; Heuser, 2016; Phillips et al., 2017).
",1 Introduction,[0],[0]
"Embedding-based analyses of semantic similarity can be a robust and valuable tool, but we find that
standard methods dramatically under-represent the variability of these measurements.",1 Introduction,[0],[0]
"Embedding algorithms are much more sensitive than they appear to factors such as the presence of specific documents, the size of the documents, the size of the corpus, and even seeds for random number generators.",1 Introduction,[0],[0]
"If users do not account for this variability, their conclusions are likely to be invalid.",1 Introduction,[0],[0]
"Fortunately, we also find that simply averaging over multiple bootstrap samples is sufficient to produce stable, reliable results in all cases tested.
",1 Introduction,[0],[0]
"NLP research in word embeddings has so far focused on a downstream-centered use case, where the end goal is not the embeddings themselves but performance on a more complicated task.",1 Introduction,[0],[0]
"For example, word embeddings are often used as the bottom layer in neural network architectures for NLP (Bengio et al., 2003; Goldberg, 2017).",1 Introduction,[0],[0]
"The embeddings’ training corpus, which is selected to be as large as possible, is only of interest insofar as it generalizes to the downstream training corpus.
",1 Introduction,[0],[0]
"In contrast, other researchers take a corpuscentered approach and use relationships between embeddings as direct evidence about the language and culture of the authors of a training corpus (Bolukbasi et al., 2016; Hamilton et al., 2016; Heuser, 2016).",1 Introduction,[0],[0]
Embeddings are used as if they were simulations of a survey asking subjects to free-associate words from query terms.,1 Introduction,[0],[0]
"Unlike the downstream-centered approach, the corpus-centered approach is based on direct human analysis of nearest neighbors to embedding vectors, and the training corpus is not simply an off-the-shelf convenience but rather the central object of study.
107
Transactions of the Association for Computational Linguistics, vol. 6, pp.",1 Introduction,[0],[0]
"107–119, 2018.",1 Introduction,[0],[0]
Action Editor: Ivan Titov.,1 Introduction,[0],[0]
"Submission batch: 6/2017; Revision batch: 9/2017; Published 2/2018.
",1 Introduction,[0],[0]
c©2018 Association for Computational Linguistics.,1 Introduction,[0],[0]
"Distributed under a CC-BY 4.0 license.
",1 Introduction,[0],[0]
"While word embeddings may appear to measure properties of language, they in fact only measure properties of a curated corpus, which could suffer from several problems.",1 Introduction,[0],[0]
"The training corpus is merely a sample of the authors’ language model (Shazeer et al., 2016).",1 Introduction,[0],[0]
"Sources could be missing or over-represented, typos and other lexical variations could be present, and, as noted by Goodfellow et al. (2016), “Many datasets are most naturally arranged in a way where successive examples are highly correlated.”",1 Introduction,[0],[0]
"Furthermore, embeddings can vary considerably across random initializations, making lists of “most similar words” unstable.
",1 Introduction,[0],[0]
We hypothesize that training on small and potentially idiosyncratic corpora can exacerbate these problems and lead to highly variable estimates of word similarity.,1 Introduction,[0],[0]
"Such small corpora are common in digital humanities and computational social science, and it is often impossible to mitigate these problems simply by expanding the corpus.",1 Introduction,[0],[0]
"For example, we cannot create more 18th Century English books or change their topical focus.
",1 Introduction,[0],[0]
"We explore causes of this variability, which range from the fundamental stochastic nature of certain algorithms to more troubling sensitivities to properties of the corpus, such as the presence or absence of specific documents.",1 Introduction,[0],[0]
"We focus on the training corpus as a source of variation, viewing it as a fragile artifact curated by often arbitrary decisions.",1 Introduction,[0],[0]
"We examine four different algorithms and six datasets, and we manipulate the corpus by shuffling the order of the documents and taking bootstrap samples of the documents.",1 Introduction,[0],[0]
"Finally, we examine the effects of these manipulations on the cosine similarities between embeddings.
",1 Introduction,[0],[0]
"We find that there is considerable variability in embeddings that may not be obvious to users of these
methods.",1 Introduction,[0],[0]
"Rankings of most similar words are not reliable, and both ordering and membership in such lists are liable to change significantly.",1 Introduction,[0],[0]
"Some uncertainty is expected, and there is no clear criterion for “acceptable” levels of variance, but we argue that the amount of variation we observe is sufficient to call the whole method into question.",1 Introduction,[0],[0]
"For example, we find cases in which there is zero set overlap in “top 10” lists for the same query word across bootstrap samples.",1 Introduction,[0],[0]
Smaller corpora and larger document sizes increase this variation.,1 Introduction,[0],[0]
"Our goal is to provide methods to quantify this variability, and to account for this variability, we recommend that as the size of a corpus gets smaller, cosine similarities should be averaged over many bootstrap samples.",1 Introduction,[0],[0]
"Word embeddings are mappings of words to points in a K-dimensional continuous space, where K is much smaller than the size of the vocabulary.",2 Related Work,[0],[0]
"Reducing the number of dimensions has two benefits: first, large, sparse vectors are transformed into small, dense vectors; and second, the conflation of features uncovers latent semantic relationships between the words.",2 Related Work,[0],[0]
"These semantic relationships are usually measured via cosine similarity, though other metrics such as Euclidean distance and the Dice coefficient are possible (Turney and Pantel, 2010).",2 Related Work,[0],[0]
We focus on four of the most popular training algorithms:,2 Related Work,[0],[0]
"Latent Semantic Analysis (LSA) (Deerwester et al., 1990), Skip-Gram with Negative Sampling (SGNS) (Mikolov et al., 2013), Global Vectors for Word Representation (GloVe) (Pennington et al., 2014), and Positive Pointwise Mutual Information (PPMI) (Levy and Goldberg, 2014) (see Section 5 for more detailed descriptions of these algorithms).
",2 Related Work,[0],[0]
"In NLP, word embeddings are often used as features for downstream tasks.",2 Related Work,[0],[0]
"Dependency parsing (Chen and Manning, 2014), named entity recognition (Turian et al., 2010; Cherry and Guo, 2015), and bilingual lexicon induction (Vulic and Moens, 2015) are just a few examples where the use of embeddings as features has increased performance in recent years.
",2 Related Work,[0],[0]
"Increasingly, word embeddings have been used as evidence in studies of language and culture.",2 Related Work,[0],[0]
"For example, Hamilton et al. (2016) train separate embeddings on temporal segments of a corpus and then
analyze changes in the similarity of words to measure semantic shifts, and Heuser (2016) uses embeddings to characterize discourse about virtues in 18th Century English text.",2 Related Work,[0],[0]
"Other studies use cosine similarities between embeddings to measure the variation of language across geographical areas (Kulkarni et al., 2016; Phillips et al., 2017) and time (Kim et al., 2014).",2 Related Work,[0],[0]
"Each of these studies seeks to reconstruct the mental model of authors based on documents.
",2 Related Work,[0],[0]
An example that highlights the contrast between the downstream-centered and corpus-centered perspectives is the exploration of implicit bias in word embeddings.,2 Related Work,[0],[0]
"Researchers have observed that embedding-based word similarities reflect cultural stereotypes, such as associations between occupations and genders (Bolukbasi et al., 2016).",2 Related Work,[0],[0]
"From a downstream-centered perspective, these stereotypical associations represent bias that should be filtered out before using the embeddings as features.",2 Related Work,[0],[0]
"In contrast, from a corpus-centered perspective, implicit bias in embeddings is not a problem that must be fixed but rather a means of measurement, providing quantitative evidence of bias in the training corpus.
",2 Related Work,[0],[0]
"Embeddings are usually evaluated on direct use cases, such as word similarity and analogy tasks via cosine similarities (Mikolov et al., 2013; Pennington et al., 2014; Levy et al., 2015; Shazeer et al., 2016).",2 Related Work,[0],[0]
"Intrinsic evaluations like word similarities measure the interpretability of the embeddings rather than their downstream task performance (Gladkova and Drozd, 2016), but while some research does evaluate embedding vectors on their downstream task performance (Pennington et al., 2014; Faruqui et al., 2015), the standard benchmarks remain intrinsic.
",2 Related Work,[0],[0]
There has been some recent work in evaluating the stability of word embeddings.,2 Related Work,[0],[0]
"Levy et al. (2015) focus on the hyperparameter settings for each algorithm and show that hyperparameters such as the size of the context window, the number of negative samples, and the level of context distribution smoothing can affect the performance of embeddings on similarity and analogy tasks.",2 Related Work,[0],[0]
"Hellrich and Hahn (2016) examine the effects of word frequency, word ambiguity, and the number of training epochs on the reliability of embeddings produced by the SGNS and skip-gram hierarchical softmax (SGHS) (a variant of SGNS), striving for reproducibility and recommending against sampling the corpus in order to preserve
stability.",2 Related Work,[0],[0]
"Likewise, Tian et al. (2016) explore the robustness of SGNS and GloVe embeddings trained on large, generic corpora (Wikipedia and news data) and propose methods to align these embeddings across different iterations.
",2 Related Work,[0],[0]
"In contrast, our goal is not to produce artificially stable embeddings but to identify the factors that create instability and measure our statistical confidence in the cosine similarities between embeddings trained on small, specific corpora.",2 Related Work,[0],[0]
"We focus on the corpus as a fragile artifact and source of variation, considering the corpus itself as merely a sample of possible documents produced by the authors.",2 Related Work,[0],[0]
"We examine whether the embeddings accurately model those authors, using bootstrap sampling to measure the effects of adding or removing documents from the training corpus.",2 Related Work,[0],[0]
"We collected two sub-corpora from each of three datasets (see Table 2) to explore how word embeddings are affected by size, vocabulary, and other parameters of the training corpus.",3 Corpora,[0],[0]
"In order to better model realistic examples of corpus-centered research, these corpora are deliberately chosen to be publicly available, suggestive of social research questions, varied in corpus parameters (e.g. topic, size, vocabulary), and much smaller than the standard corpora typically used in training word embeddings (e.g. Wikipedia, Gigaword).",3 Corpora,[0],[0]
"Each dataset was created organically, over specific time periods, in specific social settings, by specific authors.",3 Corpora,[0],[0]
"Thus, it is impossible to expand these datasets without compromising this specificity.
",3 Corpora,[0],[0]
"We process each corpus by lowercasing all text, removing words that appear fewer than 20 times in the corpus, and removing all numbers and punctuation.",3 Corpora,[0],[0]
"Because our methods rely on bootstrap sampling (see Section 6), which operates by removing or multiplying the presence of documents, we also remove duplicate documents from each corpus.
U.S. Federal Courts of Appeals The U.S. Federal courts of appeals are regional courts that decide appeals from the district courts within their federal judicial circuit.",3 Corpora,[0],[0]
"We examine the embeddings of the most recent five years of the 4th and 9th circuits.1
1https://www.courtlistener.com/
The 4th circuit contains Washington D.C. and surrounding states, while the 9th circuit contains the entirety of the west coast.",3 Corpora,[0],[0]
"Social science research questions might involve measuring a widely held belief that certain courts have distinct ideological tendencies (Broscheid, 2011).",3 Corpora,[0],[0]
"Such bias may result in measurable differences in word association due to framing effects (Card et al., 2015), which could be observable by comparing the words associated with a given query term.",3 Corpora,[0],[0]
"We treat each opinion as a single document.
",3 Corpora,[0],[0]
New York Times The New York Times (NYT),3 Corpora,[0],[0]
"Annotated Corpus (Sandhaus, 2008) contains newspaper articles tagged with additional metadata reflecting their content and publication context.",3 Corpora,[0],[0]
"To constrain the size of the corpora and to enhance their specificity, we extract data only for the year 2000 and focus on only two sections of the NYT dataset: sports and music.",3 Corpora,[0],[0]
"In the resulting corpora, the sports section is substantially larger than the music section (see Table 2).",3 Corpora,[0],[0]
"We treat an article as a single document.
",3 Corpora,[0],[0]
Reddit Reddit2 is a social website containing thousands of forums (subreddits) organized by topic.,3 Corpora,[0],[0]
We use a dataset containing all posts for the years 2007- 2014 from two subreddits: /r/AskScience and /r/AskHistorians.,3 Corpora,[0],[0]
"These two subreddits allow users to post any question in the topics of history and science, respectively.",3 Corpora,[0],[0]
"AskScience is more than five times larger than AskHistorians, though the doc-
2https://www.reddit.com/
ument length is generally longer for AskHistorians (see Table 2).",3 Corpora,[0],[0]
"Reddit is a popular data source for computational social science research; for example, subreddits can be used to explore the distinctiveness and dynamicity of communities (Zhang et al., 2017).",3 Corpora,[0],[0]
We treat an original post as a single document.,3 Corpora,[0],[0]
"Order and presence of documents We use three different methods to sample the corpus: FIXED, SHUFFLED, and BOOTSTRAP.",4 Corpus Parameters,[0],[0]
"The FIXED setting includes each document exactly once, and the documents appear in a constant, chronological order across all models.",4 Corpus Parameters,[0],[0]
"The purpose of this setting is to measure the baseline variability of an algorithm, independent of any change in input data.",4 Corpus Parameters,[0],[0]
"Algorithmic variability may arise from random initializations of learned parameters, random negative sampling, or randomized subsampling of tokens within documents.",4 Corpus Parameters,[0],[0]
"The SHUFFLED setting includes each document exactly once, but the order of the documents is randomized for each model.",4 Corpus Parameters,[0],[0]
The purpose of this setting is to evaluate the impact of variation on how we present examples to each algorithm.,4 Corpus Parameters,[0],[0]
The order of documents could be an important factor for algorithms that use online training such as SGNS.,4 Corpus Parameters,[0],[0]
"The BOOTSTRAP setting samples N documents randomly with replacement, where N is equal to the number of documents in the FIXED setting.",4 Corpus Parameters,[0],[0]
"The purpose of this setting is to measure how much variability is due to the presence or absence of specific sequences of
tokens in the corpus.",4 Corpus Parameters,[0],[0]
"See Table 3 for a comparison of these three settings.
",4 Corpus Parameters,[0],[0]
Size of corpus We expect the stability of embedding-based word similarities to be influenced by the size of the training corpus.,4 Corpus Parameters,[0],[0]
"As we add more documents, the impact of any specific document should be less significant.",4 Corpus Parameters,[0],[0]
"At the same time, larger corpora may also tend to be more broad in scope and variable in style and topic, leading to less idiosyncratic patterns in word co-occurrence.",4 Corpus Parameters,[0],[0]
"Therefore, for each corpus, we curate a smaller sub-corpus that contains 20% of the total corpus documents.",4 Corpus Parameters,[0],[0]
"These samples are selected using contiguous sequences of documents at the beginning of each training (this ensures that the FIXED setting remains constant).
",4 Corpus Parameters,[0],[0]
Length of documents We use two document segmentation strategies.,4 Corpus Parameters,[0],[0]
"In the first setting, each training instance is a single document (i.e. an article for the NYT corpus, an opinion from the Courts corpus, and a post from the Reddit corpus).",4 Corpus Parameters,[0],[0]
"In the second setting, each training instance is a single sentence.",4 Corpus Parameters,[0],[0]
We expect this choice of segmentation to have the largest impact on the BOOTSTRAP setting.,4 Corpus Parameters,[0],[0]
"Documents are often characterized by “bursty” words that are locally frequent but globally rare (Madsen et al., 2005), such as the name of a defendant in a court case.",4 Corpus Parameters,[0],[0]
Sampling whole documents with replacement should magnify the effect of bursty words: a rare but locally frequent word will either occur in a Bootstrap corpus or not occur.,4 Corpus Parameters,[0],[0]
"Sampling sentences with replacement should have less effect on bursty words, since the chance that an entire document will be removed from the corpus is much smaller.",4 Corpus Parameters,[0],[0]
"Evaluating all current embedding algorithms and implementations is beyond the scope of this work, so we select four categories of algorithms that represent distinct optimization strategies.",5 Algorithms,[0],[0]
"Recall that our goal is to examine how algorithms respond to variation in the corpus, not to maximize performance in the accuracy or effectiveness of the embeddings.
",5 Algorithms,[0],[0]
"The first category is online stochastic updates, in which the algorithm updates model parameters using stochastic gradients as it proceeds through the training corpus.",5 Algorithms,[0],[0]
"All methods implemented in the
word2vec and fastText packages follow this format, including skip-gram, CBOW, negative sampling, and hierarchical softmax (Mikolov et al., 2013).",5 Algorithms,[0],[0]
We focus on SGNS as a popular and representative example.,5 Algorithms,[0],[0]
"The second category is batch stochastic updates, in which the algorithm first collects a matrix of summary statistics derived from a pass through the training data that takes place before any parameters are set, and then updates model parameters using stochastic optimization.",5 Algorithms,[0],[0]
"We select the GloVe algorithm (Pennington et al., 2014) as a representative example.",5 Algorithms,[0],[0]
"The third category is matrix factorization, in which the algorithm makes deterministic updates to model parameters based on a matrix of summary statistics.",5 Algorithms,[0],[0]
"As a representative example we include PPMI (Levy and Goldberg, 2014).",5 Algorithms,[0],[0]
"Finally, to test whether word order is a significant factor we include a document-based embedding method that uses matrix factorization, LSA (Deerwester et al., 1990; Landauer and Dumais, 1997).
",5 Algorithms,[0],[0]
"These algorithms each include several hyperparameters, which are known to have measurable effects on the resulting embeddings (Levy et al., 2015).",5 Algorithms,[0],[0]
"We have attempted to choose settings of these parameters that are commonly used and comparable across algorithms, but we emphasize that a full evaluation of the effect of each algorithmic parameter would be beyond the scope of this work.",5 Algorithms,[0],[0]
"For each of the following algorithms, we set the context window size to 5 and the embeddings size to 100.",5 Algorithms,[0],[0]
"Since we remove words that occur fewer than 20 times during preprocessing of the corpus, we set the frequency threshold for the following algorithms to 0.
",5 Algorithms,[0],[0]
"For all other hyperparameters, we follow the default or most popular settings for each algorithm, as described in the following sections.",5 Algorithms,[0],[0]
"Latent semantic analysis (LSA) factorizes a sparse term-document matrix X (Deerwester et al., 1990; Landauer and Dumais, 1997).",5.1 LSA,[0],[0]
"X is factored using singular value decomposition (SVD), retaining K singular values such that
X ≈ XK = UKΣKV TK .
",5.1 LSA,[0],[0]
"The elements of the term-document matrix are weighted, often with TF-IDF, which measures the
importance of a word to a document in a corpus.",5.1 LSA,[0],[0]
"The dense, low-rank approximation of the term-document matrix, XK , can be used to measure the relatedness of terms by calculating the cosine similarity of the relevant rows of the reduced matrix.
",5.1 LSA,[0],[0]
We use the sci-kit learn3 package to train our LSA embeddings.,5.1 LSA,[0],[0]
"We create a term-document matrix with TF-IDF weighting, using the default settings except that we add L2 normalization and sublinear TF scaling, which scales the importance of terms with high frequency within a document.",5.1 LSA,[0],[0]
"We perform dimensionality reduction via a randomized solver (Halko et al., September 2009).
",5.1 LSA,[0],[0]
The construction of the term-count matrix and the TF-IDF weighting should introduce no variation to the final word embeddings.,5.1 LSA,[0],[0]
"However, we expect variation due to the randomized SVD solver, even when all other parameters (training document order, presence, size, etc.) are constant.",5.1 LSA,[0],[0]
"The skip-gram with negative sampling (SGNS) algorithm (Mikolov et al., 2013) is an online algorithm that uses randomized updates to predict words based on their context.",5.2 SGNS,[0],[0]
"In each iteration, the algorithm proceeds through the original documents and, at each word token, updates model parameters based on gradients calculated from the current model parameters.",5.2 SGNS,[0],[0]
"This process maximizes the likelihood of observed word-context pairs and minimizes the likelihood of negative samples.
",5.2 SGNS,[0],[0]
"We use an implementation of the SGNS algorithm included in the Python library gensim4 (Řehůřek and Sojka, 2010).",5.2 SGNS,[0],[0]
"We use the default settings provided with gensim except as described above.
",5.2 SGNS,[0],[0]
We predict that multiple runs of SGNS on the same corpus will not produce the same results.,5.2 SGNS,[0],[0]
"SGNS randomly initializes all the embeddings before training begins, and it relies on negative samples created by randomly selecting word and context pairs (Mikolov et al., 2013; Levy et al., 2015).",5.2 SGNS,[0],[0]
"We also expect SGNS to be sensitive to the order of documents, as it relies on stochastic gradient descent which can be biased to be more influenced by initial documents (Bottou, 2012).
3http://scikit-learn.org/ 4https://radimrehurek.com/gensim/models/
word2vec.html",5.2 SGNS,[0],[0]
"Global Vectors for Word Representation (GloVe) uses stochastic gradient updates but operates on a “global” representation of word co-occurrence that is calculated once at the beginning of the algorithm (Pennington et al., 2014).",5.3 GloVe,[0],[0]
"Words and contexts are associated with bias parameters, bw and bc, where w is a word and c is a context, learned by minimizing the cost function:
L = ∑
w,c
f(xwc)~w · ~c + bw + bc − log(xwc).
",5.3 GloVe,[0],[0]
We use the GloVe implementation provided by Pennington et al. (2014)5.,5.3 GloVe,[0],[0]
"We use the default settings provided with GloVe except as described above.
",5.3 GloVe,[0],[0]
"Unlike SGNS, the algorithm does not perform model updates while examining the original documents.",5.3 GloVe,[0],[0]
"As a result, we expect GloVe to be sensitive to random initializations but not sensitive to the order of documents.",5.3 GloVe,[0],[0]
"The positive pointwise mutual information (PPMI) matrix, whose cells represent the PPMI of each pair of words and contexts, is factored using singular value decomposition (SVD) and results in lowdimensional embeddings that perform similarly to GloVe and SGNS (Levy and Goldberg, 2014).
",5.4 PPMI,[0],[0]
"PMI(w, c) = log P (w, c)
P (w)P (c) ;
PPMI(w, c) = max(PMI(w, c), 0).
",5.4 PPMI,[0],[0]
"To train our PPMI word embeddings, we use hyperwords,6 an implementation provided as part of Levy et al. (2015).7 We follow the authors’ recommendations and set the context distributional smoothing (cds) parameter to 0.75, the eigenvalue matrix (eig) to 0.5, the subsampling threshold (sub) to 10-5, and the context window (win) to 5.
5http://nlp.stanford.edu/projects/glove/ 6https://bitbucket.org/omerlevy/ hyperwords/src 7We altered the PPMI code to remove a fixed random seed in order to introduce variability given a fixed corpus; no other change was made.
",5.4 PPMI,[0],[0]
"Like GloVe and unlike SGNS, PPMI operates on a pre-computed representation of word co-occurrence, so we do not expect results to vary based on the order of documents.",5.4 PPMI,[0],[0]
"Unlike both GloVe and SGNS, PPMI uses a stable, non-stochastic SVD algorithm that should produce the same result given the same input, regardless of initialization.",5.4 PPMI,[0],[0]
"However, we expect variation due to PPMI’s random subsampling of frequent tokens.",5.4 PPMI,[0],[0]
"To establish statistical significance bounds for our observations, we train 50 LSA models, 50 SGNS models, 50 GloVe models, and 50 PPMI models for each of the three settings (FIXED, SHUFFLED, and BOOTSTRAP), for each document segmentation size, for each corpus.
",6 Methods,[0],[0]
"For each corpus, we select a set of 20 relevant query words from high probability words from an LDA topic model (Blei et al., 2003) trained on that corpus with 200 topics.",6 Methods,[0],[0]
"We calculate the cosine similarity of each query word to the other words in the vocabulary, creating a similarity ranking of all the words in the vocabulary.",6 Methods,[0],[0]
"We calculate the mean and standard deviation of the cosine similarities for each pair of query word and vocabulary word across each set of 50 models.
",6 Methods,[0],[0]
"From the lists of queries and cosine similarities, we select the 20 words most closely related to the set of query words and compare the mean and standard deviation of those pairs across settings.",6 Methods,[0],[0]
"We calculate the Jaccard similarity between top-N lists to compare membership change in the lists of most closely related words, and we find average changes in rank within those lists.",6 Methods,[0],[0]
We examine these metrics across different algorithms and corpus parameters.,6 Methods,[0],[0]
We begin with a case study of the framing around the query term marijuana.,7 Results,[0],[0]
"One might hypothesize that the authors of various corpora (e.g. judges of the 4th Circuit, journalists at the NYT, and users on Reddit) have different perceptions of this drug and that their language might reflect those differences.",7 Results,[0],[0]
"Indeed, after qualitatively examining the lists of most similar terms (see Table 4), we might come to the conclusion that the allegedly conservative 4th Circuit
Standard Deviation in the 9th Circuit Corpus
judges view marijuana as similar to illegal drugs such as heroin and cocaine, while Reddit users view marijuana as closer to legal substances such as nicotine and alcohol.
",7 Results,[0],[0]
"However, we observe patterns that cause us to lower our confidence in such conclusions.",7 Results,[0],[0]
Table 4 shows that the cosine similarities can vary significantly.,7 Results,[0],[0]
"We see that the top ranked words (chosen according to their mean cosine similarity across runs of the FIXED setting) can have widely different mean similarities and standard deviations depending on the algorithm and the three training settings, FIXED, SHUFFLED, and BOOTSTRAP.
",7 Results,[0],[0]
"As expected, each algorithm has a small variation in the FIXED setting.",7 Results,[0],[0]
"For example, we can see the effect of the random SVD solver for LSA and the effect of random subsampling for PPMI.",7 Results,[0],[0]
"We do not observe a consistent effect for document order in the SHUFFLED setting.
",7 Results,[0],[0]
"Most importantly, these figures reveal that the
BOOTSTRAP setting causes large increases in variation across all algorithms (with a weaker effect for PPMI) and corpora, with large standard deviations across word rankings.",7 Results,[0],[0]
This indicates that the presence of specific documents in the corpus can significantly affect the cosine similarities between embedding vectors.,7 Results,[0],[0]
"GloVe produced very similar embeddings in both the FIXED and SHUFFLED settings, with similar means and small standard deviations, which indicates that GloVe is not sensitive to document order.",7 Results,[0],[0]
"However, the BOOTSTRAP setting caused a reduction in the mean and widened the standard deviation, indicating that GloVe is sensitive to the presence of specific documents.
",7 Results,[0],[0]
"These patterns of larger or smaller variations are generalized in Figure 1, which shows the mean standard deviation for different algorithms and settings.",7 Results,[0],[0]
"We calculated the standard deviation across the 50 runs for each query word in each corpus, and then we averaged over these standard deviations.",7 Results,[0],[0]
The results show the average levels of variation for each algorithm and corpus.,7 Results,[0],[0]
"We observe that the FIXED and SHUFFLED settings for GloVe and LSA produce the least variable cosine similarities, while PPMI produces the most variable cosine similarities for all settings.",7 Results,[0],[0]
"The presence of specific documents has a significant effect on all four algorithms (lesser for PPMI), consistently increasing the standard deviations.
",7 Results,[0],[0]
We turn to the question of how this variation in standard deviation affects the lists of most similar words.,7 Results,[0],[0]
"Are the top-N words simply re-ordered, or do the words present in the list substantially change?",7 Results,[0],[0]
Table 5 shows an example of the top-N word lists for the query word pregnancy in the 9th Circuit corpus.,7 Results,[0],[0]
"Observing Run 1, we might believe that
judges of the 9th Circuit associate pregnancy most with questions of viability and abortion, while observing Run 5, we might believe that pregnancy is most associated with questions of prisons and family visits.",7 Results,[0],[0]
"Although the lists in this table are all produced from the same corpus and document size, the membership of the lists changes substantially between runs of the BOOTSTRAP setting.
",7 Results,[0],[0]
"As another example, Table 6 shows results for the query evolution for the GloVe model and the AskScience corpus.",7 Results,[0],[0]
"Although this query shows less variation between runs, we still find cause for concern.",7 Results,[0],[0]
"For example, Run 3 ranks the words human and humans highly, while Run 1 includes neither of those words in the top 10.
",7 Results,[0],[0]
These changes in top-N rank are shown in Figure 2.,7 Results,[0],[0]
"For each query word for the AskHistorians corpus, we find the N most similar words using SGNS.",7 Results,[0],[0]
"We generate new top-N lists for each of the 50 models trained in the BOOTSTRAP setting, and we use Jaccard similarity to compare the 50 lists.",7 Results,[0],[0]
"We observe similar patterns to the changes in standard deviation
Variation in Top 2 Words
in Figure 2; PPMI displays the lowest Jaccard similarity across settings, while the other algorithms have higher similarities in the FIXED and SHUFFLED settings but much lower similarities in the BOOTSTRAP setting.",7 Results,[0],[0]
"We display results for both N = 2 and N = 10, emphasizing that even very highly ranked words often drop out of the top-N list.
",7 Results,[0],[0]
"Even when words do not drop out of the top-N list, they often change in rank, as we observe in Figure 3.",7 Results,[0],[0]
We show both a specific example for the query term men and an aggregate of all the terms whose average rank is within the top-10 across runs of the BOOTSTRAP setting.,7 Results,[0],[0]
"In order to highlight the average changes in rank, we do not show outliers in this figure, but we note that outliers (large falls and jumps in rank) are common.",7 Results,[0],[0]
"The variability across samples from the BOOTSTRAP setting indicates that the presence of specific documents can significantly affect the top-N rankings.
",7 Results,[0],[0]
"We also find that document segmentation size af-
fects the cosine similarities.",7 Results,[0],[0]
Figure 4 shows that documents segmented at a more fine-grained level produce embeddings with less variability across runs of the BOOTSTRAP setting.,7 Results,[0],[0]
"Documents segmented at the sentence level have standard deviations clustering closer to the median, while larger documents have standard deviations that are spread more widely.",7 Results,[0],[0]
"This effect is most significant for the 4th Circuit and 9th Circuit corpora, as these have much larger “documents” than the other corpora.",7 Results,[0],[0]
We observe a similar effect for corpus size in Figure 5.,7 Results,[0],[0]
"The smaller corpus shows a larger spread in standard deviation than the larger corpus, indicating greater variability.
",7 Results,[0],[0]
"Finally, we find that the variance usually stabilizes at about 25 runs of the BOOTSTRAP setting.",7 Results,[0],[0]
Figure 6 shows that variability initially increases with the number of models trained.,7 Results,[0],[0]
"We observe this pattern across corpora, algorithms, and settings.",7 Results,[0],[0]
"The most obvious result of our experiments is to emphasize that embeddings are not even a single objective view of a corpus, much less an objective view of language.",8 Discussion,[0],[0]
"The corpus is itself only a sample, and we have shown that the curation of this sample (its size, document length, and inclusion of specific documents) can cause significant variability in the embeddings.",8 Discussion,[0],[0]
"Happily, this variability can be quantified by averaging results over multiple bootstrap samples.
",8 Discussion,[0],[0]
We can make several specific observations about algorithm sensitivities.,8 Discussion,[0],[0]
"In general, LSA, GloVe, SGNS, and PPMI are not sensitive to document order in the collections we evaluated.",8 Discussion,[0],[0]
"This is surprising, as we
had expected SGNS to be sensitive to document order and anecdotally, we had observed cases where the embeddings were affected by groups of documents (e.g. in a different language) at the beginning of training.",8 Discussion,[0],[0]
"However, all four algorithms are sensitive to the presence of specific documents, though this effect is weaker for PPMI.
",8 Discussion,[0],[0]
"Although PPMI appears deterministic (due to its pre-computed word-context matrix), we find that this algorithm produced results under the FIXED ordering whose variability was closest to the BOOTSTRAP setting.",8 Discussion,[0],[0]
We attribute this intrinsic variability to the use of token-level subsampling.,8 Discussion,[0],[0]
This sampling method introduces variation into the source corpus that appears to be comparable to a bootstrap resampling method.,8 Discussion,[0],[0]
"Sampling in PPMI is inspired by a similar method in the word2vec implementation of SGNS (Levy et al., 2015).",8 Discussion,[0],[0]
"It is therefore surprising that SGNS shows noticeable differentiation between the BOOTSTRAP setting on the one hand and the FIXED and SHUFFLED settings on the other.
",8 Discussion,[0],[0]
The use of embeddings as sources of evidence needs to be tempered with the understanding that finegrained distinctions between cosine similarities are not reliable and that smaller corpora and longer documents are more susceptible to variation in the cosine similarities between embeddings.,8 Discussion,[0],[0]
"When studying the top-N most similar words to a query, it is important to account for variation in these lists, as both rank and membership can significantly change across runs.",8 Discussion,[0],[0]
"Therefore, we emphasize that with smaller corpora comes greater variability, and we recommend that practitioners use bootstrap sampling to generate an
ensemble of word embeddings for each sub-corpus and present both the mean and variability of any summary statistics such as ordered word similarities.
",8 Discussion,[0],[0]
We leave for future work a full hyperparameter sweep for the three algorithms.,8 Discussion,[0],[0]
"While these hyperparameters can substantially impact performance, our goal with this work was not to achieve high performance but to examine how the algorithms respond to changes in the corpus.",8 Discussion,[0],[0]
We make no claim that one algorithm is better than another.,8 Discussion,[0],[0]
We find that there are several sources of variability in cosine similarities between word embeddings vectors.,9 Conclusion,[0],[0]
"The size of the corpus, the length of individual documents, and the presence or absence of specific documents can all affect the resulting embeddings.",9 Conclusion,[0],[0]
"While differences in word association are measurable and are often significant, small differences in cosine similarity are not reliable, especially for small corpora.",9 Conclusion,[0],[0]
"If the intention of a study is to learn about a specific corpus, we recommend that practitioners test the statistical confidence of similarities based on word embeddings by training on multiple bootstrap samples.",9 Conclusion,[0],[0]
"This work was supported by NSF #1526155, #1652536, and the Alfred P. Sloan Foundation.",10 Acknowledgements,[0],[0]
"We would like to thank Alexandra Schofield, Laure Thompson, our Action Editor Ivan Titov, and our anonymous reviewers for their helpful comments.",10 Acknowledgements,[0],[0]
Word embeddings are increasingly being used as a tool to study word associations in specific corpora.,abstractText,[0],[0]
"However, it is unclear whether such embeddings reflect enduring properties of language or if they are sensitive to inconsequential variations in the source documents.",abstractText,[0],[0]
We find that nearest-neighbor distances are highly sensitive to small changes in the training corpus for a variety of algorithms.,abstractText,[0],[0]
"For all methods, including specific documents in the training set can result in substantial variations.",abstractText,[0],[0]
We show that these effects are more prominent for smaller training corpora.,abstractText,[0],[0]
"We recommend that users never rely on single embedding models for distance calculations, but rather average over multiple bootstrap samples, especially for small corpora.",abstractText,[0],[0]
Evaluating the Stability of Embedding-based Word Similarities,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2850–2856 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2850",text,[0],[0]
"Deep neural networks have been proven to be a powerful framework for natural language processing, and have demonstrated strong performance on a number of challenging tasks, ranging from machine translation (Cho et al., 2014b,a), to text categorisation (Zhang et al., 2015; Joulin et al., 2017; Liu et al., 2018b).",1 Introduction,[0],[0]
"Not only do such deep models outperform traditional machine learning methods, they also come with the benefit of not requiring difficult feature engineering.",1 Introduction,[0],[0]
"For instance, both Lample et al. (2016) and Ma and Hovy (2016) propose end-to-end models for sequence labelling task and achieve state-of-the-art results.
∗https://github.com/minghao-wu/CRF-AE †Work carried out at The University of Melbourne
Orthogonal to the advances in deep learning is the effort spent on feature engineering.",1 Introduction,[0],[0]
"A representative example is the task of named entity recognition (NER), one that requires both lexical and syntactic knowledge, where, until recently, most models heavily rely on statistical sequential labelling models taking in manually engineered features (Florian et al., 2003; Chieu and Ng, 2002; Ando and Zhang, 2005).",1 Introduction,[0],[0]
"Typical features include POS and chunk tags, prefixes and suffixes, and external gazetteers, all of which represent years of accumulated knowledge in the field of computational linguistics.
",1 Introduction,[0],[0]
"The work of Collobert et al. (2011) started the trend of feature engineering-free modelling by learning internal representations of compositional components of text (e.g., word embeddings).",1 Introduction,[0],[0]
"Subsequent work has shown impressive progress through capturing syntactic and semantic knowledge with dense real-valued vectors trained on large unannotated corpora (Mikolov et al., 2013a,b; Pennington et al., 2014).",1 Introduction,[0],[0]
"Enabled by the powerful representational capacity of such embeddings and neural networks, feature engineering has largely been replaced with taking off-the-shelf pre-trained word embeddings as input, thereby making models fully end-to-end and the research focus has shifted to neural network architecture engineering.
",1 Introduction,[0],[0]
"More recently, there has been increasing recognition of the utility of linguistic features (Li et al., 2017; Chen et al., 2017; Wu et al., 2017; Liu et al., 2018a) where such features are integrated to improve model performance.",1 Introduction,[0],[0]
"Inspired by this, taking NER as a case study, we investigate the utility of hand-crafted features in deep learning models, challenging conventional wisdom in an attempt to refute the utility of manually-engineered features.",1 Introduction,[0],[0]
"Of particular interest to this paper is the work by Ma and Hovy (2016) where they
introduce a strong end-to-end model combining a bi-directional Long Short-Term Memory (BiLSTM) network with Convolutional Neural Network (CNN) character encoding in a Conditional Random Field (CRF).",1 Introduction,[0],[0]
Their model is highly capable of capturing not only word- but also characterlevel features.,1 Introduction,[0],[0]
"We extend this model by integrating an auto-encoder loss, allowing the model to take hand-crafted features as input and re-construct them as output, and show that, even with such a highly competitive model, incorporating linguistic features is still beneficial.",1 Introduction,[0],[0]
"Perhaps the closest to this study is the works by Ammar et al. (2014) and Zhang et al. (2017), who show how CRFs can be framed as auto-encoders in unsupervised or semisupervised settings.
",1 Introduction,[0],[0]
"With our proposed model, we achieve strong performance on the CoNLL 2003 English NER shared task with an F1 of 91.89, significantly outperforming an array of competitive baselines.",1 Introduction,[0],[0]
We conduct an ablation study to better understand the impacts of each manually-crafted feature.,1 Introduction,[0],[0]
"Finally, we further provide an in-depth analysis of model performance when trained with varying amount of data and show that the proposed model is highly competent with only 60% of the training set.",1 Introduction,[0],[0]
"In this section, we first outline the model architecture, then the manually crafted features, and finally how they are incorporated into the model.",2 Methodology,[0],[0]
"We build on a highly competitive sequence labelling model, namely Bi-LSTM-CNN-CRF, first
introduced by Ma and Hovy (2016).",2.1 Model Architecture,[0],[0]
"Given an input sequence of x = {x1, x2, . . .",2.1 Model Architecture,[0],[0]
", xT } of length T , the model is capable of tagging each input with a predicted label ŷ, resulting in a sequence of ŷ = {ŷ1, ŷ2, . . .",2.1 Model Architecture,[0],[0]
", ŷT } closely matching the gold label sequence y = {y1, y2, . . .",2.1 Model Architecture,[0],[0]
", yT }.",2.1 Model Architecture,[0],[0]
"Here, we extend the model by incorporating an auto-encoder loss taking hand-crafted features as in/output, thereby forcing the model to preserve crucial information stored in such features and allowing us to evaluate the impacts of each feature on model performance.",2.1 Model Architecture,[0],[0]
"Specifically, our model, referred to as Neural-CRF+AE, consists of four major components: (1) a character-level CNN (char-CNN); (2) a word-level bi-directional LSTM (Bi-LSTM); (3) a conditional random field (CRF); and (4) an auto-encoder auxiliary loss.",2.1 Model Architecture,[0],[0]
"An illustration of the model architecture is presented in Figure 1.
",2.1 Model Architecture,[0],[0]
Char-CNN.,2.1 Model Architecture,[0],[0]
"Previous studies (Santos and Zadrozny, 2014; Chiu and Nichols, 2016; Ma and Hovy, 2016) have demonstrated that CNNs are highly capable of capturing character-level features.",2.1 Model Architecture,[0],[0]
"Here, our character-level CNN is similar to that used in Ma and Hovy (2016) but differs in that we use a ReLU activation (Nair and Hinton, 2010).1
Bi-LSTM.",2.1 Model Architecture,[0],[0]
We use a Bi-LSTM to learn contextual information of a sequence of words.,2.1 Model Architecture,[0],[0]
"As inputs to the Bi-LSTM, we first concatenate the pre-trained embedding of each word wi with its character-level representation cwi (the output of the char-CNN) and a vector of manually crafted features fi (described in Section 2.2):
−→ h i = −−−−→",2.1 Model Architecture,[0],[0]
"LSTM( −→ h i−1, [wi; cwi ;fi]) (1) ←−",2.1 Model Architecture,[0],[0]
h,2.1 Model Architecture,[0],[0]
"i = ←−−−− LSTM( ←− h i+1, [wi; cwi ;fi]) , (2)
",2.1 Model Architecture,[0],[0]
where [; ] denotes concatenation.,2.1 Model Architecture,[0],[0]
The outputs of the forward and backward pass of the Bi-LSTM is then concatenated hi =,2.1 Model Architecture,[0],[0]
[ −→ h i;,2.1 Model Architecture,[0],[0]
"←− h i] to form the output of the Bi-LSTM, where dropout is also applied.
",2.1 Model Architecture,[0],[0]
CRF.,2.1 Model Architecture,[0],[0]
"For sequence labelling tasks, it is intuitive and beneficial to utilise information carried between neighbouring labels to predict the best sequence of labels for a given sentence.",2.1 Model Architecture,[0],[0]
"Therefore,
1While the hyperbolic tangent activation function results in comparable performance, the choice of ReLU is mainly due to faster convergence.
",2.1 Model Architecture,[0],[0]
"we employ a conditional random field layer (Lafferty et al., 2001) taking as input the output of the Bi-LSTM hi.",2.1 Model Architecture,[0],[0]
"Training is carried out by maximising the log probability of the gold sequence: LCRF = log p(y|x) while decoding can be efficiently performed with the Viterbi algorithm.
",2.1 Model Architecture,[0],[0]
Auto-encoder loss.,2.1 Model Architecture,[0],[0]
"Alongside sequence labelling as the primary task, we also deploy, as auxiliary tasks, three auto-encoders for reconstructing the hand-engineered feature vectors.",2.1 Model Architecture,[0],[0]
"To this end, we add multiple independent fully-connected dense layers, all taking as input the Bi-LSTM output hi with each responsible for reconstructing a particular type of feature: f̂ ti = σ(W
thi) where σ is the sigmoid activation function, t denotes the type of feature, and W t is a trainable parameter matrix.",2.1 Model Architecture,[0],[0]
"More formally, we define the auto-encoder loss as:
LtAE = T∑ i=0",2.1 Model Architecture,[0],[0]
"XEntropy(f ti , f̂ t i ) .",2.1 Model Architecture,[0],[0]
"(3)
Model training.",2.1 Model Architecture,[0],[0]
"Training is carried out by optimising the joint loss:
L = LCRF + ∑ t λtLtAE , (4)
where, in addition to LCRF , we also add the autoencoder loss, weighted by λt.",2.1 Model Architecture,[0],[0]
"In all our experiments, we set λt to 1 for all ts.",2.1 Model Architecture,[0],[0]
We consider three categories of widely used features: (1) POS tags; (2) word shape; and (3) gazetteers and present an example in Table 1.,2.2 Hand-crafted Features,[0],[0]
"While POS tags carry syntactic information regarding sentence structure, the word shape feature focuses on a more fine-grained level, encoding character-level knowledge to complement the loss of information caused by embedding lookup, such as capitalisation.",2.2 Hand-crafted Features,[0],[0]
"Both features are based on the implementation of spaCy.2 For the gazetteer fea-
2https://spacy.io/
ture, we focus on PERSON and LOCATION and compile a list for each.",2.2 Hand-crafted Features,[0],[0]
"The PERSON gazetteer is collected from U.S. census 2000, U.S. census 2010 and DBpedia whereas GeoNames is the main source for LOCATION, taking in both official and alternative names.",2.2 Hand-crafted Features,[0],[0]
All the tokens on both lists are then filtered to exclude frequently occurring,2.2 Hand-crafted Features,[0],[0]
common words.3,2.2 Hand-crafted Features,[0],[0]
Each category is converted into a one-hot sparse feature vector f ti and then concatenated to form a multi-hot vector fi =,2.2 Hand-crafted Features,[0],[0]
[fPOSi ;f shape i ;f gazetteer i ] for the i-th word.,2.2 Hand-crafted Features,[0],[0]
"In addition, we also experimented with including the label of the incoming dependency edge to each word as a feature, but observed performance deterioration on the development set.",2.2 Hand-crafted Features,[0],[0]
"While we still study and analyse the impacts of this feature in Table 3 and Section 3.2, it is excluded from our model configuration (not considered as part of fi unless indicated otherwise).",2.2 Hand-crafted Features,[0],[0]
"In this section, we present our experimental setup and results for name entity recognition over the CoNLL 2003 English NER shared task dataset (Tjong Kim Sang and De Meulder, 2003).",3 Experiments,[0],[0]
Dataset.,3.1 Experimental Setup,[0],[0]
"We use the CoNLL 2003 NER shared task dataset, consisting of 14,041/3,250/3,453 sentences in the training/development/test set respectively, all extracted from Reuters news articles during the period from 1996 to 1997.",3.1 Experimental Setup,[0],[0]
"The dataset is annotated with four categories of name entities: PERSON, LOCATION, ORGANIZATION and MISC.",3.1 Experimental Setup,[0],[0]
"We use the IOBES tagging scheme, as previous study have shown that this scheme provides a modest improvement to the model performance (Ratinov and Roth, 2009; Chiu and Nichols, 2016; Lample et al., 2016; Ma and Hovy, 2016).
",3.1 Experimental Setup,[0],[0]
"3Gazetteer data is included in the code release.
",3.1 Experimental Setup,[0],[0]
Model configuration.,3.1 Experimental Setup,[0],[0]
"Following the work of Ma and Hovy (2016), we initialise word embeddings with GloVe (Pennington et al., 2014) (300- dimensional, trained on a 6B-token corpus).",3.1 Experimental Setup,[0],[0]
"Character embeddings are 30-dimensional and randomly initialised with a uniform distribution in
the range",3.1 Experimental Setup,[0],[0]
"[− √
3 dim ,+
√ 3
dim ].",3.1 Experimental Setup,[0],[0]
Parameters are optimised with stochastic gradient descent (SGD) with an initial learning rate of η = 0.015 and momentum of 0.9.,3.1 Experimental Setup,[0],[0]
Exponential learning rate decay is applied every 5 epochs with a factor of 0.8.,3.1 Experimental Setup,[0],[0]
"To reduce the impact of exploding gradients, we employ gradient clipping at 5.0 (Pascanu et al., 2013).
",3.1 Experimental Setup,[0],[0]
We train our models on a single GeForce GTX TITAN X GPU.,3.1 Experimental Setup,[0],[0]
"With the above hyper-parameter setting, training takes approximately 8 hours for a full run of 40 epochs.
Evaluation.",3.1 Experimental Setup,[0],[0]
We measure model performance with the official CoNLL evaluation script and report span-level named entity F-score on the test set using early stopping based on the performance on the validation set.,3.1 Experimental Setup,[0],[0]
"We report average F-scores and standard deviation over 5 runs for our model.
Baseline.",3.1 Experimental Setup,[0],[0]
"In addition to reporting a number of prior results of competitive baseline models, as listed in Table 2, we also re-implement the BiLSTM-CNN-CRF model by Ma and Hovy (2016) (referred to as Neural-CRF in Table 2) and report its average performance.",3.1 Experimental Setup,[0],[0]
The experimental results are presented in Table 2.,3.2 Results,[0],[0]
"Observe that Neural-CRF+AE, trained either on the training set only or with the addition of the development set, achieves substantial improvements in F-score in both settings, superior to all but one of the benchmark models, highlighting the utility of hand-crafted features incorporated with the proposed auto-encoder loss.",3.2 Results,[0],[0]
"Compared against the Neural-CRF, a very strong model in itself, our model significantly improves performance, showing the positive impact of our technique for exploiting manually-engineered features.",3.2 Results,[0],[0]
"Although Peters et al. (2018) report a higher F-score using their ELMo embedding technique, our approach here is orthogonal, and accordingly we would expect a performance increase if we were to incorporate their ELMo representations into our model.
",3.2 Results,[0],[0]
"Ablation Study To gain a better understanding of the impacts of each feature, we perform an ab-
lation study and present the results in Table 3.",3.2 Results,[0],[0]
"We observe performance degradation when eliminating POS, word shape and gazetteer features, showing that each feature contributes to NER performance beyond what is learned through deep learning alone.",3.2 Results,[0],[0]
"Interestingly, the contribution of gazetteers is much less than that of the other features, which is likely due to the noise introduced in the matching process, with many incorrectly identified false positives.
",3.2 Results,[0],[0]
Including features based on dependency tags into our model decreases the performance slightly.,3.2 Results,[0],[0]
"This might be a result of our simple implementation (as illustrated in Table 1), which does not include dependency direction, nor parent-child relationships.
",3.2 Results,[0],[0]
"Next, we investigate the impact of different means of incorporating manually-engineered features into the model.",3.2 Results,[0],[0]
"To this end, we experiment with three configurations with features as: (1) input only; (2) output only (equivalent to multi-task learning); and (3) both input and output (Neural-CRF+AE) and present the results in Table 4.",3.2 Results,[0],[0]
"Simply using features as either input or output only improves model performance slightly, but insignificantly so.",3.2 Results,[0],[0]
"It is only when features are incorporated with the proposed auto-encoder loss do we observe a significant performance boost.
",3.2 Results,[0],[0]
Training Requirements Neural systems typically require a large amount of annotated data.,3.2 Results,[0],[0]
"Here we measure the impact of training with varying amount of annotated data, as shown in Figure 2.",3.2 Results,[0],[0]
"Wtih the proposed model architecture, the amount of labelled training data can be drastically reduced: our model, achieves comparable performance against the baseline Neural-CRF, with as little as 60% of the training data.",3.2 Results,[0],[0]
"Moreover, as we increase the amount of training text, the performance of Neural-CRF+AE continues to improve.
",3.2 Results,[0],[0]
"Hyperparameters Three extra hyperparameters are introduced into our model, controlling the weight of the autoencoder loss relative to the CRF loss, for each feature type.",3.2 Results,[0],[0]
Figure 3 shows the effect of each hyperparameter on test performance.,3.2 Results,[0],[0]
"Observe that setting λi = 1 gives strong performance, and that the impact of the gazetteer is less marked than the other two feature types.",3.2 Results,[0],[0]
"While increasing λ is mostly beneficial, performance drops if the λs are overly large, that is, the auto-encoder loss overwhelms the main prediction task.",3.2 Results,[0],[0]
"In this paper, we set out to investigate the utility of hand-crafted features.",4 Conclusion,[0],[0]
"To this end, we have presented a hybrid neural architecture to validate this hypothesis extending a Bi-LSTM-CNN-CRF by incorporating an auto-encoder loss to take manual features as input and then reconstruct them.",4 Conclusion,[0],[0]
"On the task of named entity recognition, we show significant improvements over a collection of competitive baselines, verifying the value of such features.",4 Conclusion,[0],[0]
"Lastly, the method presented in this work can also be easily applied to other tasks and models, where hand-engineered features provide key insights about the data.",4 Conclusion,[0],[0]
"Conventional wisdom is that hand-crafted features are redundant for deep learning models, as they already learn adequate representations of text automatically from corpora.",abstractText,[0],[0]
"In this work, we test this claim by proposing a new method for exploiting handcrafted features as part of a novel hybrid learning approach, incorporating a feature auto-encoder loss component.",abstractText,[0],[0]
"We evaluate on the task of named entity recognition (NER), where we show that including manual features for partof-speech, word shapes and gazetteers can improve the performance of a neural CRF model.",abstractText,[0],[0]
"We obtain a F1 of 91.89 for the CoNLL-2003 English shared task, which significantly outperforms a collection of highly competitive baseline models.",abstractText,[0],[0]
"We also present an ablation study showing the importance of autoencoding, over using features as either inputs or outputs alone, and moreover, show including the autoencoder components reduces training requirements to 60%, while retaining the same predictive accuracy.",abstractText,[0],[0]
Evaluating the Utility of Hand-crafted Features in Sequence Labelling,title,[0],[0]
"The success of deep learning owes much to efficient gradient computation using backpropagation (Rumelhart et al., 1986).",1. Introduction,[0],[0]
"When the model of interest includes internal stochasticities, the objective function is often written as a stochastic computational graph (Schulman et al., 2015).",1. Introduction,[0],[0]
"In this case, the exact gradient computation is intractable in general, and an approximate estimation is required.",1. Introduction,[0],[0]
"The variance introduced by the approximation often degrades the optimization performance for deep models, and there-
1Preferred Networks, Tokyo, Japan 2The University of Tokyo, Tokyo, Japan 3RIKEN, Tokyo, Japan.",1. Introduction,[0],[0]
"Correspondence to: Seiya Tokui <tokui@preferred.jp>, Issei Sato <sato@k.utokyo.ac.jp>.
",1. Introduction,[0],[0]
"Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017.",1. Introduction,[0],[0]
"Copyright 2017 by the author(s).
",1. Introduction,[0],[0]
fore variance reduction is crucial for practical learning.,1. Introduction,[0],[0]
"However, few things are known about its theoretical aspects, and we often struggle with model-specific heuristics whose degree of optimality is difficult to know.
",1. Introduction,[0],[0]
"The likelihood-ratio method (Glynn, 1990; Williams, 1992) and the reparameterization trick (Williams, 1992; Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzarogredilla, 2014) are widely used for the gradient estimation.",1. Introduction,[0],[0]
"The likelihood-ratio method only requires the computation of density functions and their derivatives, and therefore it is applicable to a wide range of models including those with discrete variables.",1. Introduction,[0],[0]
It requires variance reduction techniques in practice.,1. Introduction,[0],[0]
"The most common technique is the use of a baseline value (Paisley et al., 2012; Bengio et al., 2013; Ranganath et al., 2014; Mnih & Gregor, 2014; Gu et al., 2016a) which is subtracted from a sampled objective value.",1. Introduction,[0],[0]
"The optimal baseline is difficult to compute in general, and we often use alternatives that are efficiently computed, some of which are based on model-specific heuristics.",1. Introduction,[0],[0]
"The reparameterization trick, on the other hand, has a small estimation variance in practice and is only applicable to models with certain continuous variables.",1. Introduction,[0],[0]
"Various models with continuous variables have been proposed using it, whereas less progress on the research of deep discrete variable models has been made because of the inapplicability of this method.
",1. Introduction,[0],[0]
"In this paper, we give a novel framework to formulate gradient estimators.",1. Introduction,[0],[0]
"It is derived by the reparameterization and the local marginalization analogous to the local expectation gradient (Titsias & Lázaro-Gredilla, 2015).",1. Introduction,[0],[0]
"The likelihood-ratio method and the reparameterization trick can be formalized under this framework, and therefore it bridges these two families of estimators.",1. Introduction,[0],[0]
"We can derive the optimal estimator, which gives a lower bound of the variance of all estimators covered by the framework.",1. Introduction,[0],[0]
"Since the estimator is derived by applying local marginalization to the reparameterized gradient, we named it the reparameterization and marginalization (RAM) estimator.",1. Introduction,[0],[0]
"This estimator is an instance of the likelihood-ratio estimator with the optimal baseline, and therefore it can be used to evaluate the variance of existing baseline techniques.
",1. Introduction,[0],[0]
"When the variable of interest follows a Bernoulli distribution, we can derive a tighter connection of a wider range of
estimators to the framework.",1. Introduction,[0],[0]
"For example, the local expectation gradient (Titsias & Lázaro-Gredilla, 2015) becomes covered by our framework, and the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Raiko et al., 2015) approximates the optimal estimator where the finite difference of the objective function is replaced by the infinitesimal first-order approximation.",1. Introduction,[0],[0]
"Furthermore, the optimal estimator is reduced to a likelihood-ratio estimator with an input-dependent baseline, which implies that a practical baseline technique might achieve a near-optimal variance.
",1. Introduction,[0],[0]
The rest of this paper is organized as follows.,1. Introduction,[0],[0]
We overview the related work in Sec.2 and formulate the gradient estimation problem in Sec.3.,1. Introduction,[0],[0]
We introduce our framework in Sec.4 and derive important estimators with it in Sec.5.,1. Introduction,[0],[0]
We also introduce a wider range of estimators for Bernoulli variables in Sec.6.,1. Introduction,[0],[0]
We then show experimental results in Sec.7 and give a conclusion in Sec.8.,1. Introduction,[0],[0]
"The gradient estimation problem was being studied in the field of simulation around 1990, which is well summarized in L’Ecuyer (1991).",2. Related Work,[0],[0]
"The likelihood-ratio method (Glynn, 1989) is a general approach for solving the problem, in which the parametric density qφ(z) is replaced by qφ(z) q0(z) q0(z) where q0 := qφ is fixed against φ on differentiation.",2. Related Work,[0],[0]
"The ratio qφ(z)q0(z) is called the likelihood ratio, hence the name of this method.",2. Related Work,[0],[0]
"It can be seen as an importance sampling method that uses a proposal q0 (Jie & Abbeel, 2010), with which there is a study on reducing the variance by using a proposal better than q0 (Ruiz et al., 2016a).",2. Related Work,[0],[0]
"Another approach is the finite-difference method, in which the use of common random numbers, i.e., using the same random numbers to run two perturbed simulations, is effective in reducing the variance.",2. Related Work,[0],[0]
"The common random numbers naturally appear in the formulation of the optimal estimator of our framework.
",2. Related Work,[0],[0]
"The likelihood-ratio method has been combined with baselines and was introduced to the policy gradient methods for reinforcement learning, which is called the REINFORCE algorithm (Williams, 1992).",2. Related Work,[0],[0]
The baseline technique is used for reducing the variance.,2. Related Work,[0],[0]
"A simple estimation of the average reward is commonly used as the baseline, and the optimal constant baseline that minimizes the variance is also derived (Weaver & Tao, 2001).",2. Related Work,[0],[0]
"The likelihood-ratio estimator has also been used for black-box variational inference (Ranganath et al., 2014).",2. Related Work,[0],[0]
The likelihood-ratio estimator is used to derive the gradient estimation without depending on the specific form of the distributions.,2. Related Work,[0],[0]
"From a statistical point of view, the baseline can be seen as a special form of control variates, for which the optimal one can be derived again.",2. Related Work,[0],[0]
"The baseline technique has been
further made sophisticated by involving the variable-wise baselines and those depending on the varaible of interest (Mnih & Gregor, 2014; Gu et al., 2016a).",2. Related Work,[0],[0]
"Some of them are also exported to policy-gradient methods (Gu et al., 2016b).",2. Related Work,[0],[0]
"Taking the local expectation of the likelihood-ratio estimator (Titsias & Lázaro-Gredilla, 2015) is another approach of variance reduction.
",2. Related Work,[0],[0]
"For variational inference of models with continuous variables, the reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzaro-gredilla, 2014) is widely used.",2. Related Work,[0],[0]
It is easy to implement with modern frameworks of automatic differentiation.,2. Related Work,[0],[0]
"It also has low variance in practice, although the superiority to the likelihood-ratio estimator is not guaranteed in general (Gal, 2017).",2. Related Work,[0],[0]
"This method is also applied to the continuous relaxation of discrete variables (Jang et al., 2016; Maddison et al., 2016).
",2. Related Work,[0],[0]
"On the one hand, the connection between the likelihoodratio method and the reparameterization trick is studied in some literature, especially on continuous variables for which a tractable reparameterization is not available (Ruiz et al., 2016b).",2. Related Work,[0],[0]
"On the other hand, there are fewer studies for discrete variables.",2. Related Work,[0],[0]
This paper provides a bridge between these estimators for discrete variables.,2. Related Work,[0],[0]
Our task is to optimize an expectation over a parameterized distribution.,3. Problem Formulation,[0],[0]
"The objective function is given as F (φ;x) = Eqφ(z|x)f(x, z), where f is a feasible function, qφ(z|x) = ∏M i=1 qφi(zi|pai) is a directed graphical model of M variables z = (z1, . . .",3. Problem Formulation,[0],[0]
", zM )",3. Problem Formulation,[0],[0]
"conditioned on an input to the system x, pai are the parent nodes of zi, and φ are the model parameters.",3. Problem Formulation,[0],[0]
Each conditional qφi(zi|pai) is continuously differentiable w.r.t.,3. Problem Formulation,[0],[0]
φi and is typically a simple distribution such as a Bernoulli or Gaussian whose parameters are computed by a neural network with weights φi.,3. Problem Formulation,[0],[0]
"For simplicity, we will assume that φi and φi′ for i 6= i′ do not share any parameters; however, this assumption can be easily removed.",3. Problem Formulation,[0],[0]
"We want to optimize F by stochastic gradient methods, which require an unbiased estimation of its gradient∇φF .
",3. Problem Formulation,[0],[0]
"A motivating example is variational learning of a generative model pθ(x, z) with an approximate posterior qφ(z|x).",3. Problem Formulation,[0],[0]
"In this case, the objective function is the expectation of f(x, z) = log pθ(x, z)",3. Problem Formulation,[0],[0]
"− log qφ(z|x), which gives a lower bound of the log likelihood log pθ(x).",3. Problem Formulation,[0],[0]
"On the one hand, the gradient w.r.t.",3. Problem Formulation,[0],[0]
the generative parameter θ is easily estimated by a Monte Carlo simulation.,3. Problem Formulation,[0],[0]
"On the other hand, estimating the gradient w.r.t.",3. Problem Formulation,[0],[0]
"φ is not trivial, which falls into the above general setting.",3. Problem Formulation,[0],[0]
"Note that we omit the gradient incurred from the dependency of the second term − log qφ(z|x) on φ from our discussions since this gradi-
ent term is easy to estimate with low variance.",3. Problem Formulation,[0],[0]
Here we give a general formulation of our framework of gradient estimation.,4. Proposed Framework,[0],[0]
"The framework is based on the reparameterization of variables, which we also review.
",4. Proposed Framework,[0],[0]
"Suppose that each sample drawn from a conditional is reparameterized as follows.
zi ∼ qφi(zi|pai)⇔ zi = gφi(pai, i), i ∼ p( i).
",4. Proposed Framework,[0],[0]
Here i is a noise variable.,4. Proposed Framework,[0],[0]
"We will give concrete examples of reparameterization later, and here we only emphasize that gφi might be a non-continuous function.",4. Proposed Framework,[0],[0]
"We write the whole reparameterization as z = gφ(x, ).
",4. Proposed Framework,[0],[0]
"Using this reparameterization, we derive the general form of gradient estimation.",4. Proposed Framework,[0],[0]
"Let \i = { 1, . . .",4. Proposed Framework,[0],[0]
", i−1, i+1, . . .",4. Proposed Framework,[0],[0]
", M}.",4. Proposed Framework,[0],[0]
"We partially exchange the differentiation and integration as follows.
∇φiF (φ;x) = ∇φiE f(x, gφ(x, ))",4. Proposed Framework,[0],[0]
"= E \i∇φiE if(x, gφ(x, )).",4. Proposed Framework,[0],[0]
"(1)
Unlike the reparameterization trick, this equation holds even if the function gφ is not continuous because the local expectation E if(x, gφ(x, )) is differentiable.",4. Proposed Framework,[0],[0]
The technique of separating variables in leave-one-out manner is similar to Eq.,4. Proposed Framework,[0],[0]
"(8) of Titsias & Lázaro-Gredilla (2015), whereas it is applied to reparameterized, mutuallyindependent noise variables in our case.
",4. Proposed Framework,[0],[0]
Equation (1) gives our framework of gradient estimation.,4. Proposed Framework,[0],[0]
"Given a way to estimate the local gradient ∇φiE if(x, gφ(x, )), we can estimate ∇φiF (φ;x) by sampling \i and estimating the local gradient.",4. Proposed Framework,[0],[0]
"Many existing estimators are derived by specifying a method of local gradient estimation, which we review in the next section.
",4. Proposed Framework,[0],[0]
Examples of Reparameterization: We introduce typical ways of reparameterizing popular distributions.,4. Proposed Framework,[0],[0]
"When qφi(zi|pai) = N (zi|µi, σ2i ) is a Gaussian distribution, zi can be reparameterized as zi = µi + iσi, i ∼ N ( i; 0, 1) (Kingma & Welling, 2014).",4. Proposed Framework,[0],[0]
"In this case, the change-ofvariable formula gφi(pai, i) = µi(pai;φi)+ iσi(pai;φi) is differentiable w.r.t.",4. Proposed Framework,[0],[0]
φi.,4. Proposed Framework,[0],[0]
We can derive a reparameterization for Bernoulli variables as well.,4. Proposed Framework,[0],[0]
"Suppose zi ∈ {0, 1} is a binary variable following a Bernoulli distribution qφi(zi|pai)",4. Proposed Framework,[0],[0]
= µ zi i (1− µi)1−zi .,4. Proposed Framework,[0],[0]
"It can be reparameterized using a uniform noise i ∼ U(0, 1) as zi = H(µi− i),
where H(x) =
{ 1 (x > 0)
0",4. Proposed Framework,[0],[0]
"(x ≤ 0) is the Heaviside step func-
tion.",4. Proposed Framework,[0],[0]
"In this case, the change-of-variable formula zi =
H(µi(pai;φi)− i) is not continuous in general.",4. Proposed Framework,[0],[0]
"For categorical variables, we can use the Gumbel-Max trick (Gumbel, 1954; Jang et al., 2016; Maddison et al., 2016) for the reparameterization in a similar way.",4. Proposed Framework,[0],[0]
We derive existing estimators on the basis of our general framework (1).,5. Derivation of Gradient Estimators,[0],[0]
We also derive the estimator that is optimal in terms of the estimation variance.,5. Derivation of Gradient Estimators,[0],[0]
The likelihood-ratio estimator is derived by using the logderivative trick for the local gradient estimation.,5.1. Likelihood-Ratio Estimator,[0],[0]
"Let bi(x, ) be a baseline for zi, and \i ∼ p( \i).",5.1. Likelihood-Ratio Estimator,[0],[0]
"We use z = gφ(x, ) and omit the dependency of z on .",5.1. Likelihood-Ratio Estimator,[0],[0]
"The likelihood-ratio estimator with baseline bi is a Monte Carlo estimate of the following expectation.
",5.1. Likelihood-Ratio Estimator,[0],[0]
"∇φiE if(x, gφ(x, ))",5.1. Likelihood-Ratio Estimator,[0],[0]
"= E i(f(x, z)− bi(x, ))∇φi log qφi(zi|pai) +",5.1. Likelihood-Ratio Estimator,[0],[0]
"Ci(x, \i).
",5.1. Likelihood-Ratio Estimator,[0],[0]
"(2)
Here Ci(x, \i) = E ibi(x, )∇φi log qφi(zi|pai), which has to be analytically computed.
",5.1. Likelihood-Ratio Estimator,[0],[0]
There are many baseline techniques for variance reduction.,5.1. Likelihood-Ratio Estimator,[0],[0]
"We classify them into four categories as follows.
",5.1. Likelihood-Ratio Estimator,[0],[0]
"• Constant baseline is a constant of all variables {x, }.",5.1. Likelihood-Ratio Estimator,[0],[0]
It is a common choice for the baseline.,5.1. Likelihood-Ratio Estimator,[0],[0]
"In this case, it holds that Ci = 0.",5.1. Likelihood-Ratio Estimator,[0],[0]
"An exponential moving average of the simulated function f is often used.
",5.1. Likelihood-Ratio Estimator,[0],[0]
• Independent baseline is a baseline that is constant against i.,5.1. Likelihood-Ratio Estimator,[0],[0]
"It can depend on other variables, {x, \i}.",5.1. Likelihood-Ratio Estimator,[0],[0]
"In this case, it again holds that Ci = 0.",5.1. Likelihood-Ratio Estimator,[0],[0]
Two techniques proposed by Mnih & Gregor (2014) can be seen as examples of baselines in this class.,5.1. Likelihood-Ratio Estimator,[0],[0]
"One is the input-dependent baseline, which is a neural network that predicts the sampled objective value f(x, z) from x and pai.",5.1. Likelihood-Ratio Estimator,[0],[0]
"The other one is the use of local signals, where the terms of f that are not descendants of zi in the stochastic computational graphs are omitted.",5.1. Likelihood-Ratio Estimator,[0],[0]
"It can be seen as a baseline that includes all these terms.
",5.1. Likelihood-Ratio Estimator,[0],[0]
"• Linear baseline is a baseline that is a linear function of zi, i.e., bi =",5.1. Likelihood-Ratio Estimator,[0],[0]
"z>i ui(x, \i) + vi(x, \i)
1, where ui and vi are arbitrary functions.",5.1. Likelihood-Ratio Estimator,[0],[0]
"In this case, we can write Ci =",5.1. Likelihood-Ratio Estimator,[0],[0]
"(∇φiµi)>ui(x, \i), where µi = Eqφi (zi|pai)zi is the mean of zi.",5.1. Likelihood-Ratio Estimator,[0],[0]
"The MuProp estimator (Gu et al.,
1When zi is a binary or continuous scalar, the transposition is not needed.",5.1. Likelihood-Ratio Estimator,[0],[0]
"When zi is a categorical variable, we represent it by a one-hot vector, for which z>i ui is the innerproduct of two vectors.
2016a) is an example of estimators with linear baselines, where the baseline is given as the first-order approximation of the mean-field network of f at µi.
• Fully-informed baseline is a baseline that depends on all of x and , possibly in a nonlinear way.",5.1. Likelihood-Ratio Estimator,[0],[0]
"This is the most general class of baselines.
",5.1. Likelihood-Ratio Estimator,[0],[0]
It is easily expected that the fully-informed baseline can achieve the lowest variance.,5.1. Likelihood-Ratio Estimator,[0],[0]
We will show that the optimal estimator under the framework (1) falls into this category.,5.1. Likelihood-Ratio Estimator,[0],[0]
"The reparameterization trick (Kingma & Welling, 2014; Rezende et al., 2014; Titsias & Lzaro-gredilla, 2014) is a common way to estimate the gradient for models with continuous variables.",5.2. Reparameterization Trick Estimator,[0],[0]
"It is derived by exchanging the differentiation and integration of the local gradient as follows.
∇φiE",5.2. Reparameterization Trick Estimator,[0],[0]
"if(x, gφ(x, ))",5.2. Reparameterization Trick Estimator,[0],[0]
"= E i∇φif(x, gφ(x, )).",5.2. Reparameterization Trick Estimator,[0],[0]
"(3)
Note that this equation holds only if the function gφ(x, ) is differentiable, and therefore the reparameterization trick is only applicable to continuous variables.
",5.2. Reparameterization Trick Estimator,[0],[0]
"The reparameterization trick often gives a better estimation of the gradient compared with the likelihood-ratio estimator, although there is no theoretical guarantee.",5.2. Reparameterization Trick Estimator,[0],[0]
"Indeed, we can construct an example for which the likelihood-ratio estimator gives a better estimation (Gal, 2017).",5.2. Reparameterization Trick Estimator,[0],[0]
"The optimal estimator is obtained by analytically computing the local gradient ∇φiE if(x, gφ(x, )).",5.3. Optimal Estimator,[0],[0]
Let z\i,5.3. Optimal Estimator,[0],[0]
":= {z1, . . .",5.3. Optimal Estimator,[0],[0]
", zi−1, zi+1, . . .",5.3. Optimal Estimator,[0],[0]
", zM}.",5.3. Optimal Estimator,[0],[0]
"When we fix \i and modify the value of zi, the descendant variables of zi might be changed because they are functions of zi and noise variables.",5.3. Optimal Estimator,[0],[0]
"We denote the resulting values of z\i by z\i = hφ\i(x, zi, \i).",5.3. Optimal Estimator,[0],[0]
"The function hφ\i(x, zi, \i) represents the ancestral sampling procedure of z\i with given \i and clamped zi.",5.3. Optimal Estimator,[0],[0]
"Using the reparameterization again, we obtain E if(x, gφ(x, ))",5.3. Optimal Estimator,[0],[0]
"= Eqφi (zi|pai)f(x, z).",5.3. Optimal Estimator,[0],[0]
"The local gradient is then computed as follows.
∇φiE",5.3. Optimal Estimator,[0],[0]
"if(x, gφ(x, ))",5.3. Optimal Estimator,[0],[0]
"= ∑ zi f(x, z)∇φiqφi(zi|pai) ∣∣∣ z\i=hφ\i",5.3. Optimal Estimator,[0],[0]
"(x,zi, \i) .",5.3. Optimal Estimator,[0],[0]
"(4)
If zi is continuous, the summation is replaced by an integral, which is approximated numerically.",5.3. Optimal Estimator,[0],[0]
"The resulting algorithm is shown in Alg. 1, which we name the reparameterization and marginalization (RAM) estimator.",5.3. Optimal Estimator,[0],[0]
"It requires M times evaluations of h, and therefore it scales
Algorithm 1 Algorithm for RAM estimator (4) for discrete zi’s.",5.3. Optimal Estimator,[0],[0]
"If zi is continuous, the loop over all the configurations of zi is replaced by a loop over integration points.",5.3. Optimal Estimator,[0],[0]
"Require: a set of parameters φ and an input variable x.
1: Sample ∼ p( ).",5.3. Optimal Estimator,[0],[0]
"2: for i = 1, . . .",5.3. Optimal Estimator,[0],[0]
",M do 3: for all configurations of zi do 4: z\i := hφ\i(x, zi, \i).",5.3. Optimal Estimator,[0],[0]
"5: fzi := f(x, z)∇φiqφi(zi|pai).",5.3. Optimal Estimator,[0],[0]
6: end for 7: ∆i,5.3. Optimal Estimator,[0],[0]
:= ∑ zi fzi .,5.3. Optimal Estimator,[0],[0]
"8: end for 9: return (∆1, . . .",5.3. Optimal Estimator,[0],[0]
",∆M )",5.3. Optimal Estimator,[0],[0]
"as an estimation of∇F (φ;x).
worse than other estimators 2.",5.3. Optimal Estimator,[0],[0]
"However, these evaluations are easily parallelized, and it runs fast enough for models of moderate size.
",5.3. Optimal Estimator,[0],[0]
The optimality of this estimator is stated in the following theorem.,5.3. Optimal Estimator,[0],[0]
"Let φij be the j-th element of the vector of parameters φi, and let ∂ij = ∂/∂φij for notational simplicity.
",5.3. Optimal Estimator,[0],[0]
Theorem 1.,5.3. Optimal Estimator,[0],[0]
"Suppose an unbiased estimator δij of the local derivative ∂ijE if(x, gφ(x, )), i.e., δij is a random variable whose expectation matches the local derivative.",5.3. Optimal Estimator,[0],[0]
Let Vij be the variance of the estimator δij and V ?,5.3. Optimal Estimator,[0],[0]
ij be the variance of the RAM estimator.,5.3. Optimal Estimator,[0],[0]
"Then, it holds that V ?ij ≤",5.3. Optimal Estimator,[0],[0]
"Vij .
",5.3. Optimal Estimator,[0],[0]
Proof.,5.3. Optimal Estimator,[0],[0]
"This follows from the standard RaoBlackwellization argument.
",5.3. Optimal Estimator,[0],[0]
"We briefly review the relationships between the RAM estimator and existing ones.
",5.3. Optimal Estimator,[0],[0]
Relationship to the Likelihood-Ratio Estimator:,5.3. Optimal Estimator,[0],[0]
The RAM estimator can be seen as an example of the likelihood-ratio estimators with fully-informed baselines.,5.3. Optimal Estimator,[0],[0]
"Let bi(x, ) = f(x, gφ(x, )).",5.3. Optimal Estimator,[0],[0]
"Then, the log-derivative term of Eq.",5.3. Optimal Estimator,[0],[0]
"(2) is canceled, and only the residual Ci(x, \i) = ∇φiE",5.3. Optimal Estimator,[0],[0]
"if(x, gφ(x, )) remains.",5.3. Optimal Estimator,[0],[0]
The analytically-computed residual is equivalent to the RAM estimator.,5.3. Optimal Estimator,[0],[0]
"Since this estimator gives the minimum variance, our likelihood-ratio formulation (2) contains the optimal estimator.
",5.3. Optimal Estimator,[0],[0]
"While the fully-informed baseline is too general in practice to be efficiently computed, much more restrictive independent baselines can achieve the optimal estimator when zi follows a Bernoulli distribution.",5.3. Optimal Estimator,[0],[0]
"Let V LRij (bi) be the variance of the likelihood-ratio estimator with baseline bi.
",5.3. Optimal Estimator,[0],[0]
Theorem 2.,5.3. Optimal Estimator,[0],[0]
Suppose that qφi(zi|pai) is a Bernoulli distribution.,5.3. Optimal Estimator,[0],[0]
"Then, there is one and only one baseline b?i such
2 The difference in computational cost against the local expectation gradient (Titsias & Lázaro-Gredilla, 2015) comes from the inapplicability of pivot samples.
",5.3. Optimal Estimator,[0],[0]
that b?i is constant against i and V ?,5.3. Optimal Estimator,[0],[0]
ij = V LR ij,5.3. Optimal Estimator,[0],[0]
(b ?,5.3. Optimal Estimator,[0],[0]
"i ).
",5.3. Optimal Estimator,[0],[0]
The proof is given in Sec.6.,5.3. Optimal Estimator,[0],[0]
"This result implies that, for Bernoulli variables, the optimal variance might be obtained by a practical class of baseline techniques.",5.3. Optimal Estimator,[0],[0]
"Note that the optimal baseline b?i might depend on the noise variables corresponding to the descendants of zi, which are not used by existing baseline techniques.
",5.3. Optimal Estimator,[0],[0]
Relationship to the Reparameterization Trick: Theorem 1 also states that the RAM estimator gives a variance not larger than that of the reparameterization trick.,5.3. Optimal Estimator,[0],[0]
"Indeed, the RAM estimator is based on an analytical computation of the integral (3), and therefore gives the lower or equal variance.",5.3. Optimal Estimator,[0],[0]
"In practice, it is infeasible to compute the local gradient analytically, and numerical approximation is required.",5.3. Optimal Estimator,[0],[0]
"We can approximate the integral with high precision in practice, because zi is usually a scalar variable and therefore we only need to evaluate the function f(x, gφ(x, ))",5.3. Optimal Estimator,[0],[0]
"at a few integration points of zi.
Relationship to the Local Expectation Gradient: The local expectation gradient (Titsias & Lázaro-Gredilla, 2015) is an application of local marginalization to the likelihood-ratio estimator.",5.3. Optimal Estimator,[0],[0]
Let mbi be the Markov blanket of zi.,5.3. Optimal Estimator,[0],[0]
"This estimator is then derived as follows.
",5.3. Optimal Estimator,[0],[0]
"∇φiF (φ;x) = Eqφ(z|x)f(x, z)∇φi log qφi(zi|pai) =",5.3. Optimal Estimator,[0],[0]
"Eqφ(z\i|x)Eqφ(zi|mbi)f(x, z)∇φi log qφi(zi|pai)
= Eqφ(z\i|x) ∑ zi qφ(zi|mbi) qφi(zi|pai) f(x, z)∇φiqφi(zi|pai).",5.3. Optimal Estimator,[0],[0]
"(5)
For the Monte Carlo simulation of z\i, z is first sampled from qφ(z|x), and then zi is discarded.",5.3. Optimal Estimator,[0],[0]
It corresponds to sampling and computing z\i by using it in the reparameterized notation.,5.3. Optimal Estimator,[0],[0]
"If the latent variables z1, . . .",5.3. Optimal Estimator,[0],[0]
", zM are mutually independent given x, the density ratio factor qφ(zi|mbi)/qφi(zi|pai) equals 1, and therefore this estimator is equivalent to the RAM estimator (4).",5.3. Optimal Estimator,[0],[0]
"Otherwise, the density ratio factor remains, and these estimators do not match in general.",5.3. Optimal Estimator,[0],[0]
"The inference distribution qφ(zi|mbi) can be computed as
qφ(zi|mbi) = qφ(zi,mbi",5.3. Optimal Estimator,[0],[0]
"\ pai|pai)∑ zi qφ(zi,mbi \ pai|pai) .
",5.3. Optimal Estimator,[0],[0]
"Therefore, the density ratio is proportional to qφ(zi,mbi \ pai|pai).",5.3. Optimal Estimator,[0],[0]
"It tends to concentrate on zi used in the sampling of mbi \ pai, in which case the estimator degenerates to the plain likelihood-ratio estimator.",5.3. Optimal Estimator,[0],[0]
"Therefore, it cannot be guaranteed to have a lower variance than the likelihoodratio estimator with baselines in general.
",5.3. Optimal Estimator,[0],[0]
The RAM estimator can be seen as an application of the same technique to the reparameterized expectation (3).,5.3. Optimal Estimator,[0],[0]
"Thanks to the reparameterization, there is no need to solve the inference problem qφ(zi|mbi), and therefore the problematic density ratio factor does not appear.",5.3. Optimal Estimator,[0],[0]
"The evaluation of f(x, z) with fixed z\i does not reflect the full influence of the choice of zi, whereas the reparameterized counterpart f(x, z)|z\i=hφ\i",5.3. Optimal Estimator,[0],[0]
"(x,zi, \i) does reflect it.",5.3. Optimal Estimator,[0],[0]
"A Bernoulli variable is the most fundamental example of a discrete variable, and some estimators are dedicated for it.",6. Analyzing Estimators for Binary Variables,[0],[0]
It is beneficial to study the applications of any estimators to Bernoulli variables because they facilitate understanding and still contain most of the essential characteristics of discrete distributions.,6. Analyzing Estimators for Binary Variables,[0],[0]
"In some cases, an estimator has a connection to other estimators only when applied to Bernoulli variables.",6. Analyzing Estimators for Binary Variables,[0],[0]
Here we focus on Bernoulli variables and introduce how each estimator can be formalized and related to others.,6. Analyzing Estimators for Binary Variables,[0],[0]
"The derivations are given in the supplementary material 3
Suppose that qφi(zi|pai) is a Bernoulli distribution of the mean parameter µi = µi(pai, φi).",6. Analyzing Estimators for Binary Variables,[0],[0]
"Let fk = f(x, zi = k, z\i = hφ\i(x, zi, \i)) for k ∈ {0, 1}, i.e., fk is the reparameterized objective value for zi = k.",6. Analyzing Estimators for Binary Variables,[0],[0]
All estimators we introduce here can be written as an estimation of the gradient w.r.t.,6. Analyzing Estimators for Binary Variables,[0],[0]
"µi multiplied by ∇φiµi, and therefore we only focus on the gradient w.r.t.",6. Analyzing Estimators for Binary Variables,[0],[0]
µi denoted by ∆i.,6. Analyzing Estimators for Binary Variables,[0],[0]
"The likelihood-ratio estimator for a Bernoulli variable with an independent baseline bi is written as follows.
",6.1. Likelihood-Ratio Estimator,[0],[0]
"∆LRi = { (f1 − bi)/µi w.p. µi, −(f0",6.1. Likelihood-Ratio Estimator,[0],[0]
− bi)/(1− µi) w.p. 1− µi.,6.1. Likelihood-Ratio Estimator,[0],[0]
"(6)
It can be interpreted as an importance sampling estimation of the sum of f1−bi and−(f0−bi).",6.1. Likelihood-Ratio Estimator,[0],[0]
"Indeed, the likelihoodratio estimator for the general class of distributions can be seen as an importance sampling estimation of the expectation.",6.1. Likelihood-Ratio Estimator,[0],[0]
"When the distribution has low entropy (i.e., µi is close to 0 or 1), the variance of ∆LRi becomes large.",6.1. Likelihood-Ratio Estimator,[0],[0]
"However, it does not always mean that the variance of the gradient w.r.t.",6.1. Likelihood-Ratio Estimator,[0],[0]
"φi becomes huge, because in this case the sigmoid activation that outputs µi is in a flat regime so that its small derivative somehow alleviates the large variance.",6.1. Likelihood-Ratio Estimator,[0],[0]
"Neither does it mean that the variance is always small enough to optimize complex models.
",6.1. Likelihood-Ratio Estimator,[0],[0]
3The supplementary material is attached to the arXiv version of this paper.,6.1. Likelihood-Ratio Estimator,[0],[0]
The RAM estimator of the gradient w.r.t.,6.2. Optimal Estimator,[0],[0]
"µi is simply written as the difference of the f value at zi = 1 and zi = 0.
",6.2. Optimal Estimator,[0],[0]
∆?i = f1 − f0.,6.2. Optimal Estimator,[0],[0]
"(7)
Using this formulation, we can prove Theorem 2.
",6.2. Optimal Estimator,[0],[0]
Proof of Theorem 2.,6.2. Optimal Estimator,[0],[0]
Let bi = (1−µi)f1 +µif0.,6.2. Optimal Estimator,[0],[0]
"Then, the likelihood-ratio estimator (6) with baseline bi is equivalent to the RAM estimator (7).",6.2. Optimal Estimator,[0],[0]
"In this case, both cases of (6) are equal to ∆?i .",6.2. Optimal Estimator,[0],[0]
"This baseline does not depend on i, and therefore we conclude the proof by letting b?i = bi.
",6.2. Optimal Estimator,[0],[0]
"Interestingly, the optimal baseline is an expectation of fk with the mean of k being 1− µ instead of µ.",6.2. Optimal Estimator,[0],[0]
"This is different from the mean objective value f̄ = µif1 + (1− µi)f0, which a constant mean baseline approximates.",6.2. Optimal Estimator,[0],[0]
"The difference becomes large when µi is close to 0 or 1.
",6.2. Optimal Estimator,[0],[0]
The optimal estimator still has a positive variance since the noise variables \i are not integrated out.,6.2. Optimal Estimator,[0],[0]
In Eq.,6.2. Optimal Estimator,[0],[0]
"(7), f1 and f0 are evaluated with the same configurations of these noise variables.",6.2. Optimal Estimator,[0],[0]
The likelihood-ratio estimator can also be seen as an estimator that separately samples f1 and f0 at different iterations in which they use the separate samples of \i.,6.2. Optimal Estimator,[0],[0]
"When the number of variables is large, the influence of one variable zi on the objective value f(x, z) is small, and we can expect that f1 and f0 have a positive covariance.",6.2. Optimal Estimator,[0],[0]
"In general, the estimation variance of the difference of two random variables X,Y is reduced by estimating them with a positive covariance since V[X − Y ] = VX + VY",6.2. Optimal Estimator,[0],[0]
"− 2Cov(X,Y ), and therefore our estimator effectively reduces the variance by using the same configuration of the noise variables.",6.2. Optimal Estimator,[0],[0]
"This technique is known as common random numbers, which is also used to reduce the variance of the gradient estimations with the finite difference method for stochastic systems (L’Ecuyer, 1991).",6.2. Optimal Estimator,[0],[0]
The local expectation gradient (5) has a special view as a likelihood-ratio estimator when zi is a Bernoulli variable.,6.3. Local Expectation Gradient,[0],[0]
Let πi = qφ(zi = 1|mbi).,6.3. Local Expectation Gradient,[0],[0]
"Let f ′k = f(x, zi = k, z\i = hφ\i(x, zi = 1",6.3. Local Expectation Gradient,[0],[0]
"− k, \i)), i.e., f ′1−k is the objective value with fixed z\i and flipped zi.",6.3. Local Expectation Gradient,[0],[0]
Note that f ′k can be different from fk when some variables in z\i depend on zi.,6.3. Local Expectation Gradient,[0],[0]
"Then, the local expectation gradient is written as follows.
∆LEGi =
{ πi µi f1",6.3. Local Expectation Gradient,[0],[0]
"− 1−πi1−µi f ′ 0 w.p. µi,
πi µi f ′1 − 1−πi1−µi f0 w.p. 1− µi.
",6.3. Local Expectation Gradient,[0],[0]
"It is further rewritten as follows.
∆LEGi =  f1− 1−πi 1−µi ((1−µi)f1+µif ′0) µi w.p. µi,
− f0−",6.3. Local Expectation Gradient,[0],[0]
"πi µi ((1−µi)f ′1+µif0) 1−µi w.p. 1− µi.
Thus, it can be seen as a likelihood-ratio estimator with baseline
bLEGi = 1− qφ(zi|mbi) 1− qφi(zi|pai)",6.3. Local Expectation Gradient,[0],[0]
"b′ik
where b′ik =",6.3. Local Expectation Gradient,[0],[0]
(1 − qφi(zi|pai))fk + qφi(zi|pai)f ′1−k.,6.3. Local Expectation Gradient,[0],[0]
"The unweighted value b′ik has a similar form as the optimal baseline b?i , where f1−k is replaced by f ′ 1−k.",6.3. Local Expectation Gradient,[0],[0]
The final baseline bLEGi is given by multiplying b ′ ik by the density ratio.,6.3. Local Expectation Gradient,[0],[0]
"We have seen that it tends to be close to 0, in which case the baseline is also close to 0 so that the estimator degenerates to the plain likelihood-ratio estimator.",6.3. Local Expectation Gradient,[0],[0]
"Even if the weight does not vanish, Theorem 2 shows that the local expectation gradient estimator for Bernoulli variables has higher variance than the optimal one unless all latent variables are mutually independent given x.",6.3. Local Expectation Gradient,[0],[0]
"We give one example of an estimator dedicated for Bernoulli variables, the straight-through estimator (Hinton, 2012; Bengio et al., 2013; Raiko et al., 2015).",6.4. Straight-Through Estimator,[0],[0]
It is a biased estimator that leverages the gradient of f so that we can obtain the high-dimensional information of the direction towards which the objective would be decreasing.,6.4. Straight-Through Estimator,[0],[0]
"The estimator is written as follows.
",6.4. Straight-Through Estimator,[0],[0]
"∆STi =  ∂f ∂zi ∣∣∣ zi=1
w.p. µi, ∂f ∂zi ∣∣∣ zi=0 w.p. 1− µi. (8)
Observing that Eq.",6.4. Straight-Through Estimator,[0],[0]
"(7) gives the finite difference of f between zi = 1 and zi = 0, we can see that the straightthrough estimator is its infinitesimal counterpart at the sampled zi.",6.4. Straight-Through Estimator,[0],[0]
"Thus, this estimator is equivalent to our estimator (and is therefore unbiased) when f is a linear function of zi.",6.4. Straight-Through Estimator,[0],[0]
The difference between these estimators becomes large when the nonlinearity of f increases.,6.4. Straight-Through Estimator,[0],[0]
"If we consider a general class of the evaluation function f , we can construct an adversarial function f such that the derivative at zi ∈ {0, 1} has the opposite sign against the finite difference (7).",6.4. Straight-Through Estimator,[0],[0]
"For example, when f(x, z) = ∑ i zi, this estimator is equivalent to the optimal one.",6.4. Straight-Through Estimator,[0],[0]
"However, if we modify it to f̃(x, z) = ∑",6.4. Straight-Through Estimator,[0],[0]
i zi−sin,6.4. Straight-Through Estimator,[0],[0]
"(2π ∑ i zi), the straight-through estimator always gives a gradient opposite to the steepest direction of the expectation, which does not change as a result of the modification, i.e., Ef̃(x, z) = Ef(x, z).",6.4. Straight-Through Estimator,[0],[0]
"We conduct experiments to empirically verify Theorem 1 and to demonstrate a procedure to analyze the optimal de-
gree of a given estimator covered by our framework.",7. Experiments,[0],[0]
"All the methods are implemented with Chainer (Tokui et al., 2015).",7. Experiments,[0],[0]
"The task is variational learning of sigmoid belief networks (SBN) (Neal, 1992), which is a directed graphical model with layers of Bernoulli variables.",7.1. Experimental Settings,[0],[0]
"Let x ∈ {0, 1}d be a binary vector of input data and Z` = (z`,1, . . .",7.1. Experimental Settings,[0],[0]
", z`,N`) ∈ {0, 1}N` be a binary vector of the latent variables at the `- th layer.",7.1. Experimental Settings,[0],[0]
Denote the input layer as Z0 = x for notational simplicity.,7.1. Experimental Settings,[0],[0]
Let L be the number of latent layers.,7.1. Experimental Settings,[0],[0]
The generative model is specified by a conditional of each layer pθ(Z`|Z`+1) and the prior of the deepest layer pθ(ZL).,7.1. Experimental Settings,[0],[0]
"In our experiments, the prior of each variable zL,i ∈ ZL is independently parameterized by its logit.",7.1. Experimental Settings,[0],[0]
The conditional pθ(Z`|Z`+1) is modeled by an affine transformation of Z`+1 that outputs the logit of Z`.,7.1. Experimental Settings,[0],[0]
The parameters of the affine transformation are optimized through the learning.,7.1. Experimental Settings,[0],[0]
"We use two models with L = 2 and L = 4, respectively.",7.1. Experimental Settings,[0],[0]
"Each layer consists of N` = 200 Bernoulli variables.
",7.1. Experimental Settings,[0],[0]
We model the approximate posterior qφ(z|x) by a reversedirectional SBN.,7.1. Experimental Settings,[0],[0]
"In this case, the prior q(x) is not modeled, and each conditional qφ(Z`+1|Z`) is specified by its logit as an affine transformation of Z`.
",7.1. Experimental Settings,[0],[0]
"Both the generative parameter θ and the variational parameter φ are optimized simultaneously to maximize the following variational lower bound.
",7.1. Experimental Settings,[0],[0]
"log pθ(x) = logEqφ(z|x) pθ(x, z)
qφ(z|x)
≥ Eqφ(z|x) log pθ(x, z)
qφ(z|x) =",7.1. Experimental Settings,[0],[0]
"L.
The second line follows Jensen’s inequality with the concavity of log.",7.1. Experimental Settings,[0],[0]
The gradient w.r.t.,7.1. Experimental Settings,[0],[0]
"θ is estimated by a Monte Carlo simulation of ∇θL = Eqφ(z|x)∇θ log pθ(x, z).",7.1. Experimental Settings,[0],[0]
"We use gradient estimators for approximating the gradient w.r.t. φ.
",7.1. Experimental Settings,[0],[0]
"The plain likelihood-ratio estimator is denoted by LR, whereas the constant baseline using the moving average of f(x, z) = log pθ(x, z)",7.1. Experimental Settings,[0],[0]
"− log qφ(z|x) and the inputdependent baseline of Mnih & Gregor (2014) are expressed by the postfixes +C and +IDB, respectively.",7.1. Experimental Settings,[0],[0]
We also run experiments for MuProp and the Local Expectation Gradient (LEG).,7.1. Experimental Settings,[0],[0]
"Algorithm 1 is used to obtain the results for the optimal estimator.
",7.1. Experimental Settings,[0],[0]
"We use MNIST (Lecun et al., 1998) and Omniglot (Lake et al., 2015) for our experiments.",7.1. Experimental Settings,[0],[0]
These are sets of 28x28 pixel gray-scale images of hand-written digits and handwritten characters from various languages.,7.1. Experimental Settings,[0],[0]
"We binarize each pixel by sampling from a Bernoulli distribution with
the mean equal to the pixel intensity (Salakhutdinov & Murray, 2008).",7.1. Experimental Settings,[0],[0]
"The binarization is done in an online manner, i.e., we sample binarized vectors at each iteration.",7.1. Experimental Settings,[0],[0]
"For the MNIST dataset, we use the standard split of 60,000 training images and 10,000 test images.",7.1. Experimental Settings,[0],[0]
"The training images are further split into 50,000 images and 10,000 images, the latter of which are used for validation.",7.1. Experimental Settings,[0],[0]
"For the Omniglot dataset, we use the standard split of 24,345 training images and 8,070 test images used in the official implementation of Burda et al. (2015) 4.",7.1. Experimental Settings,[0],[0]
"The training images are further split into 20,288 images and 4,057 images, the latter of which are used for validation.
",7.1. Experimental Settings,[0],[0]
"We used RMSprop (Tieleman & Hinton, 2012) with a minibatch size of 100 to optimize the variational lower bound.",7.1. Experimental Settings,[0],[0]
We apply a weight decay of the coefficient 0.001 for all parameters.,7.1. Experimental Settings,[0],[0]
All the weights are initialized with the method of Glorot & Bengio (2010).,7.1. Experimental Settings,[0],[0]
"The learning rate is chosen from {3 × 10−4, 10−3, 3 × 10−3}.",7.1. Experimental Settings,[0],[0]
"We evaluate the model on the validation set during training, and choose the learning rate with which the best validation performance with earlystopping beats the others.",7.1. Experimental Settings,[0],[0]
"After each evaluation, we also measure the variance of the gradient estimations of variational parameters for the training set with the same minibatch size.
",7.1. Experimental Settings,[0],[0]
"Each experiment is done on an Intel(R) Xeon(R) CPU E52623 v3 at 3.00 GHz and an NVIDIA GeForce Titan X. Thanks to the parallel computation using the GPU, the computational time of the RAM estimator is only two times larger than the plain likelihood-ratio estimator.",7.1. Experimental Settings,[0],[0]
"The results for the two-layer SBN and four-layer SBN are shown in Fig. 1 and Fig. 2, respectively.",7.2. Results,[0],[0]
The results for these models have almost the same trends.,7.2. Results,[0],[0]
"As is predicted by Theorem 1, the optimal estimator gives the lower bound of the estimation variance.",7.2. Results,[0],[0]
"The plots imply that the modern baseline techniques effectively reduce the estimation variance, which is approaching the optimal value.",7.2. Results,[0],[0]
"However, the gap between these practical methods and the optimal one is not negligible, and there is still room for improvements.",7.2. Results,[0],[0]
"The local expectation gradient actually does not degenerate to the plain likelihood-ratio estimator, whereas the variance reduction effect is limited so that its variance stays at a similar level to that of the likelihood-ratio estimator with a constant baseline.",7.2. Results,[0],[0]
"The validation score almost agrees with the variance level, although there are some exceptions caused by the differences in selected learning rates.
",7.2. Results,[0],[0]
"Note that we do not align the computational cost by sampling multiple values in the experiments because the purpose of these experiments is evaluating the optimal degree
4https://github.com/yburda/iwae
of each method.",7.2. Results,[0],[0]
We can infer the performance with aligned computational budget by comparing the variance and the computational cost.,7.2. Results,[0],[0]
We introduced a novel framework of gradient estimation for stochastic computations using reparameterization.,8. Conclusion,[0],[0]
The framework serves as a bridge between the likelihood-ratio method and the reparameterization trick.,8. Conclusion,[0],[0]
The optimal estimator is naturally derived under the framework.,8. Conclusion,[0],[0]
"It provides the minimum variance attainable by the likelihood-ratio estimators with the general class of baselines, and therefore can be used to evaluate the optimal degree of each practical baseline technique.",8. Conclusion,[0],[0]
"We actually evaluated the common baseline techniques against the optimal estimator for variational learning of sigmoid belief networks and showed that the modern techniques achieve a variance level close to the lower bound.
",8. Conclusion,[0],[0]
"Comparison between continuous variable models and discrete variable models is needed for the further development of deep probabilistic modeling, which should consider the adequacy of the use of these variables in each task and the efficiency of gradient estimators available for these models.",8. Conclusion,[0],[0]
"While this study does not provide a way to compare such models in general, it bridges the gradient estimators of them through the optimal case, and therefore provides some insights on their relationships.",8. Conclusion,[0],[0]
"Observing the experimental results, the modern estimators for Bernoulli variables achieve variance close to the optimal one, and therefore we can expect that the modern estimators for Bernoulli variables are maturing and could be applied to much larger models capturing discrete phenomena.",8. Conclusion,[0],[0]
"We thank members of Preferred Networks, especially Daisuke Okanohara, for the helpful discussions.",ACKNOWLEDGMENTS,[0],[0]
"The likelihood-ratio method is often used to estimate gradients of stochastic computations, for which baselines are required to reduce the estimation variance.",abstractText,[0],[0]
"Many types of baselines have been proposed, although their degree of optimality is not well understood.",abstractText,[0],[0]
"In this study, we establish a novel framework of gradient estimation that includes most of the common gradient estimators as special cases.",abstractText,[0],[0]
The framework gives a natural derivation of the optimal estimator that can be interpreted as a special case of the likelihood-ratio method so that we can evaluate the optimal degree of practical techniques with it.,abstractText,[0],[0]
It bridges the likelihood-ratio method and the reparameterization trick while still supporting discrete variables.,abstractText,[0],[0]
It is derived from the exchange property of the differentiation and integration.,abstractText,[0],[0]
"To be more specific, it is derived by the reparameterization trick and local marginalization analogous to the local expectation gradient.",abstractText,[0],[0]
We evaluate various baselines and the optimal estimator for variational learning and show that the performance of the modern estimators is close to the optimal estimator.,abstractText,[0],[0]
Evaluating the Variance of Likelihood-Ratio Gradient Estimators,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2392–2400 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
2392",text,[0],[0]
"Possessing a capacity similar to human reasoning has been argued to be necessary for the success of artificial intelligence systems (e.g., Levesque et al., 2011).",1 Reasoning About Beliefs,[0],[0]
"One well-studied domain that requires reasoning is question answering, where simply memorizing and looking up information is often not enough to correctly answer a question.",1 Reasoning About Beliefs,[0],[0]
"For example, given the very simple scenario in Table 1, searching for the word “Mary” and returning a nearby word is not a correct strategy; instead, a model needs to recognize that Mary is currently at the second location (office and not the bathroom).
",1 Reasoning About Beliefs,[0],[0]
"Recent research has focused on developing neural models that succeed in such scenarios (Sukhbaatar et al., 2015; Henaff et al., 2017).",1 Reasoning About Beliefs,[0],[0]
"As a benchmark to evaluate these models, Weston et al. (2016) released a dataset – Facebook bAbi – that provides a set of toy tasks, each examining a specific type of reasoning.",1 Reasoning About Beliefs,[0],[0]
"For example, the scenario in Table 1 evaluates the capacity to reason using a single supporting fact.",1 Reasoning About Beliefs,[0],[0]
"However, the bAbi tasks are already too simple for the current models.",1 Reasoning About Beliefs,[0],[0]
"Only a few years after their release, existing
1 Code to generate dataset and replicate results is available at github.com/kayburns/tom-qa-dataset.
",1 Reasoning About Beliefs,[0],[0]
"models fail at only one or two (out of 20) tasks (Rae et al., 2016; Santoro et al., 2017).",1 Reasoning About Beliefs,[0],[0]
"Moreover, all except two of the reasoning tasks in this dataset only require transitive inference (Lee et al., 2016).
",1 Reasoning About Beliefs,[0],[0]
People reason not just about their own observations and beliefs but also about others’ mental states (such as beliefs and intentions).,1 Reasoning About Beliefs,[0],[0]
"The capacity to recognize that others can have mental states different than one’s own – theory of mind – marks an important milestone in the development of children and has been extensively studied by psychologists (for a review, see Flavell, 2004).",1 Reasoning About Beliefs,[0],[0]
"Artificial intelligence (AI) systems will also require a similar reasoning capacity about mental states as they are expected to be able to interact with people (e.g., Chandrasekaran et al., 2017; Grant et al., 2017; Rabinowitz et al., 2018).
",1 Reasoning About Beliefs,[0],[0]
"However, the bAbi dataset does not include tasks that evaluate a model’s ability to reason about beliefs.",1 Reasoning About Beliefs,[0],[0]
"Grant et al. (2017) created a bAbistyle dataset inspired by an influential experiment on the theory of mind called the Sally-Anne task (e.g. Baron-Cohen et al., 1985).",1 Reasoning About Beliefs,[0],[0]
"Their goal was to examine whether the end-to-end memory network (Sukhbaatar et al., 2015) can answer questions such as “where does Sally think the milk is?” in situations that Sally’s belief about the location of milk does not match the reality.",1 Reasoning About Beliefs,[0],[0]
"For example, Sally thinks that the milk is in the fridge but the milk is actually on the table.
",1 Reasoning About Beliefs,[0],[0]
"The dataset of Grant et al. (2017) provides a first step in designing benchmarks to evaluate the mental-state reasoning capacity of questionanswering models, but it is still limited in the types of reasoning it probes.",1 Reasoning About Beliefs,[0],[0]
"For example, it
only considered first-order beliefs (e.g., Sally’s belief about the location of milk).",1 Reasoning About Beliefs,[0],[0]
"People also reason about second-order (and higher-order) beliefs (e.g., Anne’s belief about Sally’s belief about the location of the milk).",1 Reasoning About Beliefs,[0],[0]
"More importantly, similarly to the bAbi dataset, success in each task is defined as correctly answering one question.",1 Reasoning About Beliefs,[0],[0]
"This does not guarantee that a model has an understanding of the state of the world; in fact, even in developmental theory-of-mind experiments, children are asked a few questions (e.g., “where is milk really?”) to ensure that their correct answer reflects their understanding and is not simply due to chance.
",1 Reasoning About Beliefs,[0],[0]
"In this paper, we address these shortcomings by designing a new dataset that enables us to evaluate a model’s capacity to reason about different types of beliefs as well as whether it maintains a correct understanding of the world.",1 Reasoning About Beliefs,[0],[0]
"To this end, we evaluate a number of different models that perform well on the bAbi tasks: the end-to-end memory network (Sukhbaatar et al., 2015), the multiple observer model (Grant et al., 2017), the recurrent entity network (Henaff et al., 2017), and RelationNetwork (Santoro et al., 2017).",1 Reasoning About Beliefs,[0],[0]
"We find that none of these models succeed at our tasks, suggesting that they are not able to keep track of inconsistent states of the world, in particular when someone’s belief does not match the history or reality of a situation.",1 Reasoning About Beliefs,[0],[0]
"Behavioral research shows that children gradually develop a theory of mind (for a review, see Gopnik and Astington, 1988).",2 Theory of Mind Experiments,[0],[0]
"At the age of two, most children have an understanding of others’ desires and perceptions – if someone wants something, they will try to get it and if something is in their sight, they can see it.",2 Theory of Mind Experiments,[0],[0]
"Children begin to understand others’ beliefs around the age of three, but this understanding is still limited.",2 Theory of Mind Experiments,[0],[0]
"For example, they might not be able to reason that someone’s actions are a result of their beliefs.",2 Theory of Mind Experiments,[0],[0]
"By the age of five, most children have a unified theory of mind and are able to represent and reason about others’ desires, perceptions, and beliefs.",2 Theory of Mind Experiments,[0],[0]
Developmental psychologists have designed various experimental paradigms to examine to what extent children are able to reason about others’ mental states.,2 Theory of Mind Experiments,[0],[0]
We use these experiments as guidelines for designing tasks to evaluate the reasoning capacity of question-answering models.,2 Theory of Mind Experiments,[0],[0]
We first explain these experiments.,2 Theory of Mind Experiments,[0],[0]
"The Sally-Anne false-belief experiment, proposed by Baron-Cohen et al. (1985), examines children’s ability to reason about others’ false beliefs, i.e., when someone’s belief does not match the reality.",2.1 The Sally-Anne Experiment,[0],[0]
"In this experiment, the participants observe two agents, Sally and Anne, with their containers, a basket and a box.",2.1 The Sally-Anne Experiment,[0],[0]
"After putting a marble in her basket, Sally leaves the room (and is not able to observe the events anymore).",2.1 The Sally-Anne Experiment,[0],[0]
"After Sally’s departure, Anne moves the marble to her box.",2.1 The Sally-Anne Experiment,[0],[0]
"Then, Sally returns to the room (see Figure 1).",2.1 The Sally-Anne Experiment,[0],[0]
"The participants are asked the following questions:
• “Where will Sally look for her marble?”",2.1 The Sally-Anne Experiment,[0],[0]
"(belief question)
• “Where is the marble really?”",2.1 The Sally-Anne Experiment,[0],[0]
"(reality question)
• “Where was the marble in the beginning?”",2.1 The Sally-Anne Experiment,[0],[0]
"(memory question)
",2.1 The Sally-Anne Experiment,[0],[0]
The first question tests the participants’ ability to reason about Sally’s belief about the location of her marble.,2.1 The Sally-Anne Experiment,[0],[0]
"Interestingly, most children before the age of 3 answer this question incorrectly and say that Sally will look at the box (where the marble really is) instead of the basket (where Sally thinks the marble is).",2.1 The Sally-Anne Experiment,[0],[0]
These children are not able to reason about Sally’s belief which is different from the reality of the world.,2.1 The Sally-Anne Experiment,[0],[0]
The reality and memory questions are used to confirm that children’s correct answer to the belief question is not due to chance; but because they have a correct understanding of the state of world and others’ beliefs.,2.1 The Sally-Anne Experiment,[0],[0]
The Sally-Anne experiment examines the ability to reason about another person’s belief or a firstorder belief.,2.2 The Icecream Van Experiment,[0],[0]
"People are also able to reason about
beliefs about beliefs, for example, Anne thinks that Sally believes that the marble is in basket.",2.2 The Icecream Van Experiment,[0],[0]
Perner and Wimmer (1985) performed a set of experiments to examine children’s reasoning capacity about such higher-order beliefs.,2.2 The Icecream Van Experiment,[0],[0]
"In their set-up Mary and John together see an ice cream van in the park, and the icecream man tells them that he will be in the park until later in the afternoon.",2.2 The Icecream Van Experiment,[0],[0]
Mary leaves the park and goes home.,2.2 The Icecream Van Experiment,[0],[0]
"A bit after she leaves, the icecream man decides to leave the park and tells John that he is going to the church.",2.2 The Icecream Van Experiment,[0],[0]
On his way to the church he runs into Mary and informs her that he will be selling icecreams close to the church all afternoon.,2.2 The Icecream Van Experiment,[0],[0]
The participants are then asked the following secondorder question: “Where does John think Mary goes to get icecream?”,2.2 The Icecream Van Experiment,[0],[0]
Note that John does not know that Mary has been told about the new location of the icecream van; he has a second-order false belief about Mary’s belief.,2.2 The Icecream Van Experiment,[0],[0]
"The participants are also asked a few control questions (e.g., “does Mary know that the van is in the church?”) to ensure that they do not correctly answer the secondorder question by chance.",2.2 The Icecream Van Experiment,[0],[0]
"Perner and Wimmer (1985) found that 6- and 7-year old children are able to answer the second-order questions, suggesting that reasoning about higher-order beliefs (as compared to a first-order belief) is a harder cognitive task.",2.2 The Icecream Van Experiment,[0],[0]
"Inspired by the theory-of-mind experiments explained in Section 2 and building on the work of Grant et al. (2017), we created a dataset based on three tasks designed to capture increasingly complex theory-of-mind reasoning: true-, false-, and second-order false-belief tasks.",3 The Theory of Mind Task Dataset,[0],[0]
Examples of each task type are given in Figure 2.,3 The Theory of Mind Task Dataset,[0],[0]
"In the true-belief task, Sally observes the world and as a result she has a first-order true belief about the location of the milk – her belief matches reality.",3 The Theory of Mind Task Dataset,[0],[0]
"In the falsebelief task, Sally’s first-order belief differs from reality (i.e., she has a false belief ) because she was absent when the state of the world changed.",3 The Theory of Mind Task Dataset,[0],[0]
"In the second-order false-belief task, Sally observes the new location of the milk; thus, she has a true belief about the milk’s location.",3 The Theory of Mind Task Dataset,[0],[0]
"However, Anne’s belief about Sally’s mental state does not match reality because Anne does not know that Sally has observed the change in the environment.",3 The Theory of Mind Task Dataset,[0],[0]
"As a result, Anne has a false belief about Sally’s beliefs.
",3 The Theory of Mind Task Dataset,[0],[0]
"These tasks are more challenging than the bAbI scenarios, because a model needs to learn whether each agent has a true or false belief about a given world state to succeed, where the world state now includes the mental states of each agent.
",3 The Theory of Mind Task Dataset,[0],[0]
"Note that we assume all containers are transparent in the underlying world; whenever an agent enters a location, they become aware of the objects true location.",3 The Theory of Mind Task Dataset,[0],[0]
We made this decision to keep the tasks as structurally similar as possible.,3 The Theory of Mind Task Dataset,[0],[0]
This prevents models from simply learning to produce a specific answer for a task type when a sentence like “Sally looks inside the pantry” is present in the story.,3 The Theory of Mind Task Dataset,[0],[0]
"The container-transparency property is consistent throughout all task-question pairs.
",3 The Theory of Mind Task Dataset,[0],[0]
Question types.,3 The Theory of Mind Task Dataset,[0],[0]
"To examine the reasoning capacity of each model about beliefs and secondorder beliefs, we employ four question types inspired by theory-of-mind experiments discussed in Section 2; see Table 2 for examples of these question types.",3 The Theory of Mind Task Dataset,[0],[0]
"These questions enable us to test whether a model can reason about first-order and second-order beliefs, and at the same time, knows the initial and current correct location of an object; thus, we can distinguish between when a model answers a question by chance and when it actually understands the entire state of the world.
",3 The Theory of Mind Task Dataset,[0],[0]
Table 3 gives the answers for the 12 combinations of task type and question.,3 The Theory of Mind Task Dataset,[0],[0]
"Given a true-belief or false-belief task, the answers to the first-order and second-order questions are the same (e.g., “pantry” in the true-belief condition and “fridge” in the false-belief condition for the tasks in Figure 2).",3 The Theory of Mind Task Dataset,[0],[0]
"However, they are different in the secondorder false belief task because Anne has a false belief about Sally’s belief.
",3 The Theory of Mind Task Dataset,[0],[0]
Dataset variants.,3 The Theory of Mind Task Dataset,[0],[0]
We use these tasks to generate two datasets: ToM and ToM-easy.,3 The Theory of Mind Task Dataset,[0],[0]
"The primary difference between these two datasets is that, in ToM-easy, each story has only one task, while ToM can have multiple tasks within a single story.",3 The Theory of Mind Task Dataset,[0],[0]
"Each dataset contains a training set with 10 000 examples with each of the 12 combinations of task and question types.
",3 The Theory of Mind Task Dataset,[0],[0]
"In ToM, the tasks are randomly grouped into sets of 5 to form stories, which is the same number used in the bAbI dataset.",3 The Theory of Mind Task Dataset,[0],[0]
"In the test set for ToM, each story contains 4 tasks, but there is only one question present at the end.",3 The Theory of Mind Task Dataset,[0],[0]
"Because questions that come closer to the beginning of a story have
fewer distracting sentences (i.e., potential answer words) that may confound a model, they are easier to answer.",3 The Theory of Mind Task Dataset,[0],[0]
"We found that this testing procedure gave us a more precise understanding of the performance of the model by separating the difficulty of a question due to its position in a story from the inherent difficulty of the question itself.
",3 The Theory of Mind Task Dataset,[0],[0]
Generating the data.,3 The Theory of Mind Task Dataset,[0],[0]
Each reasoning task in Weston et al. (2016) can be formalized with a grammar.,3 The Theory of Mind Task Dataset,[0],[0]
The training and test data are then the derivations of this grammar.,3 The Theory of Mind Task Dataset,[0],[0]
"We refer to each derivation as a story (e.g., Figure 2).",3 The Theory of Mind Task Dataset,[0],[0]
We follow Grant et al. (2017) in writing grammars for our new tasks.,3 The Theory of Mind Task Dataset,[0],[0]
"In particular, all the task grammars consist of a set of entities (people and objects in the stories) and predicates that take entities as subject or object.",3 The Theory of Mind Task Dataset,[0],[0]
The grammars also specify the properties of entities – which predicates take them as subjects or objects.,3 The Theory of Mind Task Dataset,[0],[0]
"A predicate can include actions that are ways an agent interact with the world (e.g., place, move, enter, exit) and beliefs that are mental state terms (e.g., believe, think).",3 The Theory of Mind Task Dataset,[0],[0]
"As an example, Sally with the property is agent can perform the action displace on apple with the property is object.",3 The Theory of Mind Task Dataset,[0],[0]
"Similar to the previous work, we use a restricted set of action and belief predicates.",3 The Theory of Mind Task Dataset,[0],[0]
We briefly describe the models that we evaluate in this paper.,4 The Models,[0],[0]
We chose these models based on the novelty in their architecture or their near stateof-the-art results in the bAbi tasks.,4 The Models,[0],[0]
"More specifically, given 10k examples and joint-training on the bAbi tasks, the best end-to-end memory network (Sukhbaatar et al., 2015) and relation network (Santoro et al., 2017) fail at 6 and 2 tasks, respectively.",4 The Models,[0],[0]
"Given the same training dataset and per-task training, the recurrent entity network succeed at all tasks (but the authors do not report the results of joint-training).",4 The Models,[0],[0]
"Recall that the bAbi tasks are structured as a set of sentences followed by a question about them (e.g., a story from Figure 2 followed by a question from Table 2).",4 The Models,[0],[0]
The End-to-End Memory Network.,4 The Models,[0],[0]
"Sukhbaatar et al. (2015) proposed a neural memoryaugmented model, the end-to-end memory network (MemN2N), that extends the memory network architecture (Weston et al., 2014).",4 The Models,[0],[0]
"Similarly to its predecessor, MemN2N has a memory component in which sentences are embedded and an attention function that weights the embedded sentences based on their similarity to a given question.",4 The Models,[0],[0]
The MemN2N model introduces multiple layers of memory (hops) by stacking memory components such that the question embedding at layer k+1 is the sum of the output and question embedding of layer k.,4 The Models,[0],[0]
The Multiple Observer Model.,4 The Models,[0],[0]
"To perform well on the false-belief and second-order falsebelief conditions, a model needs to identify that agents have experienced different events, and, as a result, have differing knowledge about the state of the world.",4 The Models,[0],[0]
"Although having multiple layers of memory in the MemN2N model enables it to combine attention to different memory slots (i.e., embedded sentences) at each layer, the model does not have access to each agent’s unique perspective.",4 The Models,[0],[0]
"For example, the model is not explicitly told that Sally does not observe the change of the lo-
cation of the milk in the false-belief condition.",4 The Models,[0],[0]
"To address this, Grant et al. (2017) propose the Multiple Observer model that integrates MemN2N with individual memory modules for each agent in the story.",4 The Models,[0],[0]
An agent’s memory only receives the sentences for which the agent is present and observes the world.,4 The Models,[0],[0]
Their model has an additional attention function that weighs the memory modules based on their relevance to the question.,4 The Models,[0],[0]
The model is expected to learn to attend to Sally’s memory module if the question is about her belief about a state of the world.,4 The Models,[0],[0]
The Recurrent Entity Network.,4 The Models,[0],[0]
"Henaff et al. (2017) propose a memory-augmented architecture, EntNet, with two interesting properties; first, their model is a recurrent neural network and thus can capture the sequential nature of the events in a story.",4 The Models,[0],[0]
"Second, instead of keeping a whole sentence embedding in a memory slot, their model can learn the important entities of a story (e.g., a person) and their properties (e.g., location) through a set of gated recurrent units and two weight matrices.",4 The Models,[0],[0]
The Relation Network.,4 The Models,[0],[0]
Santoro et al. (2017) propose a neural model for relational reasoning.,4 The Models,[0],[0]
Their model consider the possibility of a relation among each two possible pairs of objects.,4 The Models,[0],[0]
"To model the bAbi tasks, they consider each pair of sentences together with the question as inputs to their relation network.",4 The Models,[0],[0]
"Experiment set-up We train all models jointly over all task types without noise, but evaluate them independently on different task and question pairs.",5 Experimental Results,[0],[0]
We choose the best-performing models by selecting hyperparameters on the validation set.,5 Experimental Results,[0],[0]
"Similarly to Sukhbaatar et al. (2015), we consider a model successful only when its accuracy exceeds 95% across the entire task suite.
",5 Experimental Results,[0],[0]
MemN2N and Multiple Observer Models.,5 Experimental Results,[0],[0]
We first examine how each model performs across a range of parameter and initialization values.,5 Experimental Results,[0],[0]
"MemN2N models are very sensitive to the network initialization and for each set of parameters, the best result out of 10 runs is reported (Sukhbaatar et al., 2015).",5 Experimental Results,[0],[0]
We first visualize the accuracy of all runs as a box plot to identify how sensitive each model is to random initialization (of parameters and internal states) and thus difficult to train.,5 Experimental Results,[0],[0]
We also report the results for the best run in each experiment.,5 Experimental Results,[0],[0]
"We use a memory size of 50, the same
as experiments of Sukhbaatar et al. (2015), to ensure that the memory contains all sentences of a given story.
",5 Experimental Results,[0],[0]
EntNet.,5 Experimental Results,[0],[0]
We report results averaged over 3 initializations because we observed little randomness due to initialization.,5 Experimental Results,[0],[0]
"We selected the learning rate on a held out validation set separately for ToMeasy and ToM; all otehr the same hyperparameters as Henaff et al. (2017): 20 memory slots and an embedding size of 100 We trained until the training error hit zero, which occurred around 50 epochs for both datasets. .
",5 Experimental Results,[0],[0]
RelNet.,5 Experimental Results,[0],[0]
"We report results using a single seed because we saw little randomness due to initialization; this is in accordance with the authors’ findings (Santoro et al., 2017).",5 Experimental Results,[0],[0]
"We selected model hyperparameters on a held-out validation set separately for each of the ToM and ToM-easy datasets.
",5 Experimental Results,[0],[0]
"5.1 Overall Performance on ToM-easy We expect the models perform well on this dataset, given that there is only one task in the memory during both training and test and as a result unrelated events do not interfere with a model’s reasoning in this condition.
",5 Experimental Results,[0],[0]
"Despite the large variance in accuracy across runs, the MemN2N models often succeed at the memory, reality, and second-order questions (Figure 3a).",5 Experimental Results,[0],[0]
Note that the median accuracy (the dark blue line in box) is close to 1 for these questions.,5 Experimental Results,[0],[0]
"However, the model often fails (median accuracy around 0.5) given the first-order question (“where will Sally look for the milk”) and the false-belief and second-order false-belief tasks.",5 Experimental Results,[0],[0]
This pattern is different from the empirical findings on people; the second-order question is harder than the first order one for people.,5 Experimental Results,[0],[0]
We observe that the performance of the Multiple Observer model is similar to the MemNet for most question-task pairs (expect one) but there is less variation in the accuracy values.,5 Experimental Results,[0],[0]
"Interestingly, the median accuracy is close to one for the first-order question and the secondorder false-belief task but the Multiple Observer model still performs poorly for this question on the false-belief task.
",5 Experimental Results,[0],[0]
Why is the first-order question harder for the MemN2N model?,5 Experimental Results,[0],[0]
"To investigate this, we look more closely at our task-question pairs.",5 Experimental Results,[0],[0]
"As shown in Table 3, the answers to the first-order question are different for the false-belief and secondorder false-belief tasks but are the same for the
second-order one.",5 Experimental Results,[0],[0]
"We suspect that it is harder for the MemN2N model to learn two distinct answers for the same question given the the simi-
larities of the two false-belief tasks.",5 Experimental Results,[0],[0]
"To test this hypothesis, we altered the data such that the answers to first-order question are (incorrectly) the
same for both false-belief and second-order falsebelief tasks (and also the second-order question).",5 Experimental Results,[0],[0]
"We observe that the median accuracy is close to 1 for all conditions suggesting that the model can learn the distinction between two of the tasks but not all three.
",5 Experimental Results,[0],[0]
"We observe that EntNet and RelNet models are not too sensitive to the initialization value, and thus just report the result on best-performing models.",5 Experimental Results,[0],[0]
"Both EntNet and RelNet best models succeed at the ToM-easy tasks; their mean error is 0.
",5 Experimental Results,[0],[0]
"5.2 Overall Performance on ToM
This dataset is more similar to the bAbi dataset in that, during both training and test, the memory contains a story with multiple tasks; as a result, it is harder for the model to identify the entities relevant to a given question.
",5 Experimental Results,[0],[0]
"As shown in Figure 3c, the MemN2N performs worse on all the questions with the exception of the reality question, where performance is slightly worse but has lower variance.",5 Experimental Results,[0],[0]
(cf. the ToM-easy dataset).,5 Experimental Results,[0],[0]
"The first- and second-order questions are, overall, harder for the model.",5 Experimental Results,[0],[0]
"The performance of the Multiple Observer model is better for most of the questions (see Figure 3d), especially the second-order, but it is slightly worse for the memory question.",5 Experimental Results,[0],[0]
"This suggests that adding agentspecific memory modules is not enough to succeed on all the ToM tasks, and that the increased complexity of inference with multiple memory modules harms performance on another task.
",5 Experimental Results,[0],[0]
We also look at the best-performing MemN2N and Multiple Observer models over all questiontask pairs.,5 Experimental Results,[0],[0]
These models are selected based on their performance on the validation dataset.,5 Experimental Results,[0],[0]
"We observe that none of these models succeed on the ToM tasks, but, interestingly, the Multiple Observer model performs better overall as compared to the original MemN2N model (see Table 4).
ToM and bAbi.",5 Experimental Results,[0],[0]
How are ToM and bAbi tasks similar?,5 Experimental Results,[0],[0]
The combination of our true-belief task and the reality question is very similar to bAbi task 1 (single supporting fact).,5 Experimental Results,[0],[0]
"To correctly answer the reality question (“where is the milk really?”, a model need to use a single fact from the story (“Anne moved the milk to the pantry.”).",5 Experimental Results,[0],[0]
The MemN2N model succeeds at both bAbi task 1 and the reality question given the true-belief task.,5 Experimental Results,[0],[0]
"However, the correct answer to the memory question (“where was the milk at the beginning?”) for
the true-belief task also requires a single fact (“the milk is in the fridge.”).",5 Experimental Results,[0],[0]
"Interestingly, the error of MemN2N on the memory question is much higher than the reality question.",5 Experimental Results,[0],[0]
The model (unlike people) cannot learn two representations (initial and current location) for an object.,5 Experimental Results,[0],[0]
"This result demonstrates the importance of representing alternative states of the world, whether it be past states of reality (i.e., where the “milk” used to be) or mental states about the world (i.e., where an agent believes the “milk” to be).
",5 Experimental Results,[0],[0]
EntNet and RelNet.,5 Experimental Results,[0],[0]
"We also report results of two relevant memory-augmented neural network models on our tasks, EntNet (Henaff et al., 2017) and RelNet (Santoro et al., 2017), in Table 4.",5 Experimental Results,[0],[0]
"Again, because we did not observe sensitivity to initialization for these models, only average performance of their best-performing model is reported.",5 Experimental Results,[0],[0]
"We see that even though these models succeed on the ToM-easy dataset, they fail on the ToM tasks, suggesting that these models cannot simultaneously deal with inconsistent (i.e., past, present, and mental) states of the world.
",5 Experimental Results,[0],[0]
We further investigate which questions are the hardest for each best model; see Table 5.,5 Experimental Results,[0],[0]
"We observe that each of the MemN2N, Multiple Observer and RelNet models perform poorly on some combination of the first- and second-order questions, but are successful at answering the reality question.",5 Experimental Results,[0],[0]
We hypothesize that this phenomenon occurs because the reality question is the most similar to the bAbi tasks.,5 Experimental Results,[0],[0]
"In addition, all models fail the memory question for each task type.",5 Experimental Results,[0],[0]
"While this is to be expected for EntNet due to its recurrent nature and therefore bias towards recency, it is surprising that the other models, which exhibit only a small positional bias, cannot correctly represent a past state of the world in order to answer the memory question correctly.",5 Experimental Results,[0],[0]
We examine to what extent each model’s architecture is sensitive to the position of sentences in each story.,5.3 Experimenting with Noise,[0],[0]
We do so by adding a novel sentence at random locations in each story at test time.,5.3 Experimenting with Noise,[0],[0]
"For any setting of the noise, p, there is a p% probability of a noise sentence occurring before each sentence in the story.",5.3 Experimenting with Noise,[0],[0]
Noise sentences cannot follow other noise sentences in the story.,5.3 Experimenting with Noise,[0],[0]
"In this paper, we report results with p = .1.",5.3 Experimenting with Noise,[0],[0]
"We observe that the accuracy of all best models decreases notably in the
presence of noise (see Table 4).",5.3 Experimenting with Noise,[0],[0]
"This result is particularly interesting as it shows that none of the models are able to use the semantics of the sentences in a story in their reasoning – they are all sensitive to the presence of distractor sentences.
",5.3 Experimenting with Noise,[0],[0]
"Interestingly, the RelNet model is the best performer amongst the models we considered on the ToM dataset, yet it is also the most sensitive to noise.",5.3 Experimenting with Noise,[0],[0]
"Moreover, the Multiple Observer model – with explicit memories for each agent – is the most robust to noise; it has the minimum decrease in accuracy between each dataset and its noised version.",5.3 Experimenting with Noise,[0],[0]
"In the experiments of Sukhbaatar et al. (2015) the memory size is fixed to 50, which is necessary to capture the entire story in memory (e.g. the answer to the memory question in ToM may rely on information at the beginning of a story).",5.4 Experimenting with Memory,[0],[0]
We observed that smaller memory sizes artificially improved the performance of the MemN2N and Multiple Observer model on ToM tasks.,5.4 Experimenting with Memory,[0],[0]
"For example, using a memory size of 10, our best MemN2N model performance boosts on the hardest task of ToM (FB task with first order belief question) from 5.1% to 97.5% and on the easiest task from 98.3% to 100.0% (SOFB task with reality question).",5.4 Experimenting with Memory,[0],[0]
"This result is not surprising because given a small memory size, ToM and ToM-easy are very similar tasks; the memory size of 10 allows for at most two full tasks in memory.",5.4 Experimenting with Memory,[0],[0]
Recent research has emphasized the importance of modeling and understanding people’s mental states for AI systems.,6 Related Work,[0],[0]
"Eysenbach et al. (2016) created a dataset of scene-description pairs where
each scene is a set of visual frames and some frames include people with mistaken beliefs.2 The authors build a regression model for identifying a mistaken belief and the person who has such a belief in a given frame.",6 Related Work,[0],[0]
"Our work differs with theirs in that we are interested in understanding whether a model can reason about people’s true and false beliefs to correctly answer questions as opposed to identifying mistaken beliefs.
",6 Related Work,[0],[0]
Grant et al. (2017) studied whether the end-toend memory network of Sukhbaatar et al. (2015) can pass a false-belief test – correctly answer where Sally would search for an object in falseand true-belief situations.,6 Related Work,[0],[0]
They created a dataset inspired by the bAbi dataset to examine whether the model can reason about interaction of beliefs and actions in these situations – how actions cause beliefs and vice versa.,6 Related Work,[0],[0]
"They show that MemN2N fails at the false-belief test, and their extension of that model with separate memories for each agent and an observer outperforms MemN2N.
Rabinowitz et al. (2018) formulate the capacity to reason about others’ beliefs as a meta-learning problem.",6 Related Work,[0],[0]
"They propose a neural network, ToMnet, that learns to predict the behavior of different agents given their past and current trajectory.",6 Related Work,[0],[0]
"Similarly to Grant et al. (2017), in addition to individual agents, they model an “observer” that has access to states and actions of all agents (though this information can be noisy and partial).",6 Related Work,[0],[0]
"Interestingly, their model successfully predicts an agent’s behavior in a false-belief situation – the agent’s behavior reflects its false-belief as opposed to the reality of the world.
",6 Related Work,[0],[0]
"Finally, Chandrasekaran et al. (2017) take a dif-
2For example, a scene where a person gets sick eating mushrooms is paired with the sentence “the couple mistakenly thinks it’s ok to eat the mushrooms”.
ferent approach by studying whether people can understand the “beliefs” of a visual-question answering system.",6 Related Work,[0],[0]
"More specifically, they examine whether the participants can predict when the model would fail in answering a question as well as if they can predict the model’s answer.",6 Related Work,[0],[0]
"They find that even with a few examples, people get better at answering these questions.",6 Related Work,[0],[0]
We propose a dataset for evaluating questionanswering models.,7 Discussion,[0],[0]
Our dataset – inspired by seminal theory-of-mind experiments in children – measures to what extent recently introduced neural models can reason about beliefs and states of the world that are potentially mututally inconsistent.,7 Discussion,[0],[0]
We evaluate three of the recent neural question answering models (and an extension of one) on our tasks.,7 Discussion,[0],[0]
We find that none of the models are able to succeed fully on a suite of tasks that requires keeping track of inconsistent beliefs or states of the world.,7 Discussion,[0],[0]
"These inconsistencies arise from differences between the past and the present, as well as the mental states of agents who may have false beliefs about the world or about the mental states of other agents.
",7 Discussion,[0],[0]
"The purpose of the dataset introduced in this work is not to test advanced language fluency; instead, consistency in the linguistic structure of the tasks allows us to isolate the performance of the models’ reasoning capabilities.",7 Discussion,[0],[0]
"Even though the language is simple, the models struggle to achieve good performance.",7 Discussion,[0],[0]
"Furthermore, we note that the proposed dataset should be treated as a diagnostic tool and that good performance on similar toy tasks is not sufficient for reasoning capabilities.",7 Discussion,[0],[0]
We propose a new dataset for evaluating question answering models with respect to their capacity to reason about beliefs.,abstractText,[0],[0]
"Our tasks are inspired by theory-of-mind experiments that examine whether children are able to reason about the beliefs of others, in particular when those beliefs differ from reality.",abstractText,[0],[0]
We evaluate a number of recent neural models with memory augmentation.,abstractText,[0],[0]
"We find that all fail on our tasks, which require keeping track of inconsistent states of the world; moreover, the models’ accuracy decreases notably when random sentences are introduced to the tasks at test.1 1 Reasoning About Beliefs Possessing a capacity similar to human reasoning has been argued to be necessary for the success of artificial intelligence systems (e.g., Levesque et al., 2011).",abstractText,[0],[0]
"One well-studied domain that requires reasoning is question answering, where simply memorizing and looking up information is often not enough to correctly answer a question.",abstractText,[0],[0]
"For example, given the very simple scenario in Table 1, searching for the word “Mary” and returning a nearby word is not a correct strategy; instead, a model needs to recognize that Mary is currently at the second location (office and not the bathroom).",abstractText,[0],[0]
"Recent research has focused on developing neural models that succeed in such scenarios (Sukhbaatar et al., 2015; Henaff et al., 2017).",abstractText,[0],[0]
"As a benchmark to evaluate these models, Weston et al. (2016) released a dataset – Facebook bAbi – that provides a set of toy tasks, each examining a specific type of reasoning.",abstractText,[0],[0]
"For example, the scenario in Table 1 evaluates the capacity to reason using a single supporting fact.",abstractText,[0],[0]
"However, the bAbi tasks are already too simple for the current models.",abstractText,[0],[0]
"Only a few years after their release, existing 1 Code to generate dataset and replicate results is available at github.com/kayburns/tom-qa-dataset.",abstractText,[0],[0]
"models fail at only one or two (out of 20) tasks (Rae et al., 2016; Santoro et al., 2017).",abstractText,[0],[0]
"Moreover, all except two of the reasoning tasks in this dataset only require transitive inference (Lee et al., 2016).",abstractText,[0],[0]
Mary went to the bathroom.,abstractText,[0],[0]
John moved to the hallway.,abstractText,[0],[0]
Mary travelled to the office.,abstractText,[0],[0]
Where is Mary?,abstractText,[0],[0]
"A: office Table 1: A task from the bAbi dataset (Weston et al., 2016).",abstractText,[0],[0]
People reason not just about their own observations and beliefs but also about others’ mental states (such as beliefs and intentions).,abstractText,[0],[0]
"The capacity to recognize that others can have mental states different than one’s own – theory of mind – marks an important milestone in the development of children and has been extensively studied by psychologists (for a review, see Flavell, 2004).",abstractText,[0],[0]
"Artificial intelligence (AI) systems will also require a similar reasoning capacity about mental states as they are expected to be able to interact with people (e.g., Chandrasekaran et al., 2017; Grant et al., 2017; Rabinowitz et al., 2018).",abstractText,[0],[0]
"However, the bAbi dataset does not include tasks that evaluate a model’s ability to reason about beliefs.",abstractText,[0],[0]
"Grant et al. (2017) created a bAbistyle dataset inspired by an influential experiment on the theory of mind called the Sally-Anne task (e.g. Baron-Cohen et al., 1985).",abstractText,[0],[0]
"Their goal was to examine whether the end-to-end memory network (Sukhbaatar et al., 2015) can answer questions such as “where does Sally think the milk is?” in situations that Sally’s belief about the location of milk does not match the reality.",abstractText,[0],[0]
"For example, Sally thinks that the milk is in the fridge but the milk is actually on the table.",abstractText,[0],[0]
"The dataset of Grant et al. (2017) provides a first step in designing benchmarks to evaluate the mental-state reasoning capacity of questionanswering models, but it is still limited in the types of reasoning it probes.",abstractText,[0],[0]
"For example, it",abstractText,[0],[0]
Evaluating Theory of Mind in Question Answering,title,[0],[0]
"A central challenge of the “big data” era is to help users make sense of large text collections (Hotho et al., 2005).",1 Comprehensible Topic Models Needed,[0],[0]
"A common approach to summarizing the main themes in a corpus is to use topic models (Blei, 2012), which are data-driven statistical models that
identify words that appear together in similar documents.",1 Comprehensible Topic Models Needed,[0],[0]
These sets of words or “topics” evince internal coherence,1 Comprehensible Topic Models Needed,[0],[0]
and can help guide users to relevant documents.,1 Comprehensible Topic Models Needed,[0],[0]
"For instance, an FBI investigator sifting through the released Hillary Clinton e-mails may see a topic with the words “Benghazi”, “Libya”, “Blumenthal”, and “success”, spurring the investigator to dig deeper to find further evidence of inappropriate communication with longtime friend Sidney Blumenthal regarding Benghazi.
",1 Comprehensible Topic Models Needed,[0],[0]
"A key challenge for topic modeling, however, is how to promote end-user understanding of individual topics and the overall model.",1 Comprehensible Topic Models Needed,[0],[0]
"Most existing topic presentations use simple word lists (Chaney and Blei, 2012; Eisenstein et al., 2012).",1 Comprehensible Topic Models Needed,[0],[0]
"Although a variety of alternative topic visualization techniques exist (Sievert and Shirley, 2014; Yi et al., 2005), there has been no systematic assessment to compare them.",1 Comprehensible Topic Models Needed,[0],[0]
"Beyond exploring different visualization techniques, another means of making topics easier for users to understand is to provide descriptive labels to complement a topic’s set of words (Aletras et al., 2014).",1 Comprehensible Topic Models Needed,[0],[0]
"Unfortunately, manual labeling is slow and, while automatic labeling approaches exist (Lau et al., 2010; Mei et al., 2007; Lau et al., 2011), their effectiveness is not guaranteed for all tasks.
",1 Comprehensible Topic Models Needed,[0],[0]
"To better understand these problems, we use labeling to evaluate topic model visualizations.",1 Comprehensible Topic Models Needed,[0],[0]
"Our study compares the impact of four commonly used topic visualization techniques on the labels that users create when interpreting a topic (Figure 1): word lists, word lists with bars, word clouds, and network graphs.",1 Comprehensible Topic Models Needed,[0],[0]
"On Amazon Mechanical Turk, one set of users viewed a series of individual topic vi-
1
Transactions of the Association for Computational Linguistics, vol. 5, pp. 1–16, 2017.",1 Comprehensible Topic Models Needed,[0],[0]
Action Editor: Timothy Baldwin.,1 Comprehensible Topic Models Needed,[0],[0]
"Submission batch: 2/2016; Revision batch: 6/2016; Published 1/2017.
",1 Comprehensible Topic Models Needed,[0],[0]
c©2017 Association for Computational Linguistics.,1 Comprehensible Topic Models Needed,[0],[0]
"Distributed under a CC-BY 4.0 license.
sualizations and provided a label to describe each topic, while a second set of users assessed the quality of those labels alongside automatically generated ones.1 Better labels imply that the topic visualization provide users a more accurate interpretation (labeling) of the topic.
",1 Comprehensible Topic Models Needed,[0],[0]
The four visualization techniques have inherent trade-offs.,1 Comprehensible Topic Models Needed,[0],[0]
"Perhaps unsurprisingly, there is no meaningful difference in the quality of the labels produced from the four visualization techniques.",1 Comprehensible Topic Models Needed,[0],[0]
"However, simple visualizations (word list and word cloud) support a quick, first-glance understanding of topics, while more complex visualizations (network graph) take longer but reveal relationships between words.",1 Comprehensible Topic Models Needed,[0],[0]
"Also, user-created labels are better received than algorithmically-generated labels, but more detailed analysis uncovers features specific to high-quality labels (e.g., tendency towards abstraction, inclusion of phrases) and the types of topics for which automatic labeling works.",1 Comprehensible Topic Models Needed,[0],[0]
These findings motivate future automatic labeling algorithms.,1 Comprehensible Topic Models Needed,[0],[0]
Presenting the full text of a document corpus is often impractical.,2 Background,[0],[0]
"For truly large and complex text corpora, abstractions, such as topic models, are necessary.",2 Background,[0],[0]
Here we review probabilistic topic modeling and topic model interfaces.,2 Background,[0],[0]
"Topic modeling algorithms produce statistical models that discover key themes in documents (Blei, 2012).",2.1 Probabilistic Topic Modeling,[0],[0]
"Many specific algorithms exist; in this work we use Latent Dirichlet Allocation (Blei et al., 2003, LDA) as it is commonly employed.",2.1 Probabilistic Topic Modeling,[0],[0]
"LDA is an unsupervised statistical topic modeling algorithm that considers each document to be a “bag of words” and can scale to large corpora (Zhai et al., 2012; Hoffman et al., 2013; Smola and Narayanamurthy, 2010).",2.1 Probabilistic Topic Modeling,[0],[0]
"Assuming that each document is an admixture of topics, inference discovers each topic’s distribution over words and each document’s distribution over topics that best explain the corpus.",2.1 Probabilistic Topic Modeling,[0],[0]
"The set of topics provide a high-level overview of the cor-
1Data available at https://github.com/ alisonmsmith/Papers/tree/master/ TopicRepresentations.
pus, and individual topics can link back to the original documents to support directed exploration.",2.1 Probabilistic Topic Modeling,[0],[0]
"The topic distributions can also be used to present other documents related to a given document.
",2.1 Probabilistic Topic Modeling,[0],[0]
"Clustering is hard because there are multiple reasonable objectives that are impossible to satisfy simultaneously (Kleinberg, 2003).",2.1 Probabilistic Topic Modeling,[0],[0]
"Topic modeling evaluation has focused on perplexity, which measures how well a model can predict words in unseen documents (Wallach et al., 2009b; Jelinek et al., 1977).",2.1 Probabilistic Topic Modeling,[0],[0]
"However, Chang et al. (2009) argue that evaluations optimizing for perplexity encourage complexity at the cost of human interpretability.",2.1 Probabilistic Topic Modeling,[0],[0]
"Newman et al. (2010a) build on this insight, noting that “one indicator of usefulness is the ease by which one could think of a short label to describe the topic.”",2.1 Probabilistic Topic Modeling,[0],[0]
"Unlike previous interpretability studies, here we examine the connection between a topic’s visual representation (not just its content) and its interpretability.
",2.1 Probabilistic Topic Modeling,[0],[0]
Recent work has focused on automatic generation of labels for topics.,2.1 Probabilistic Topic Modeling,[0],[0]
Lau et al. (2011) use Wikipedia articles to automatically label topics.,2.1 Probabilistic Topic Modeling,[0],[0]
The assumption is that for each topic there will be a Wikipedia article title that offers a good representation of the topic.,2.1 Probabilistic Topic Modeling,[0],[0]
Aletras et al. (2014) use a graph-based approach to better rank candidate labels.,2.1 Probabilistic Topic Modeling,[0],[0]
They generate a graph from the words in candidate articles and use PageRank to find a representative label.,2.1 Probabilistic Topic Modeling,[0],[0]
In Section 3 we use an adapted version of the method presented by Lau et.,2.1 Probabilistic Topic Modeling,[0],[0]
al. (2011) as a representative automatic labeling algorithm.,2.1 Probabilistic Topic Modeling,[0],[0]
"The topic visualization techniques in our study— word list, word list with bars, word cloud, and network graph—commonly appear in topic modeling tools.",2.2 Topic Model Visualizations,[0],[0]
"Here, we provide an overview of tools that display an entire topic model or models to the user, while more detail on the individual topic visualization techniques can be found in Section 3.2.
",2.2 Topic Model Visualizations,[0],[0]
"Topical Guide (Gardner et al., 2010),",2.2 Topic Model Visualizations,[0],[0]
"Topic Viz (Eisenstein et al., 2012), and the Topic Model Visualization Engine (Chaney and Blei, 2012) are tools that support corpus understanding and directed browsing through topic models.",2.2 Topic Model Visualizations,[0],[0]
They display the model overview as an aggregate of underlying topic visualizations.,2.2 Topic Model Visualizations,[0],[0]
"For example, Topical Guide uses hor-
izontal word lists when displaying an overview of an entire topic model but uses a word cloud of the top 100 words for a topic when displaying only a single topic.",2.2 Topic Model Visualizations,[0],[0]
"Topic Viz and the Topic Model Visualization Engine both represent topics with vertical word lists; the latter also uses set notation.
",2.2 Topic Model Visualizations,[0],[0]
"Other tools provide additional information within topic model overviews, such as the relationship between topics or temporal changes in the model.",2.2 Topic Model Visualizations,[0],[0]
"However, they still require the user to understand individual topics.",2.2 Topic Model Visualizations,[0],[0]
"LDAVis (Sievert and Shirley, 2014) includes information about the relationship between topics in the model.",2.2 Topic Model Visualizations,[0],[0]
Multi-dimensional scaling projects the model’s topics as circles onto a two-dimensional plane based on their inter-topic distances; the circles are sized by their overall prevalence.,2.2 Topic Model Visualizations,[0],[0]
"The individual topics, however, are then visualized on demand using a word list with bars.",2.2 Topic Model Visualizations,[0],[0]
"Smith et al. (2014) visualize a topic model using a nested network graph layout called group-in-abox (Rodrigues et al., 2011, GIB).",2.2 Topic Model Visualizations,[0],[0]
"The individual
topics are displayed using a network graph visualization, and related topics are displayed within a treemap (Shneiderman, 1992) layout.",2.2 Topic Model Visualizations,[0],[0]
"The result is a visualization where related words cluster within topics and related topics cluster in the overall layout.
",2.2 Topic Model Visualizations,[0],[0]
"TopicFlow (Smith et al., 2015) visualizes how a model changes over time using a Sankey diagram (Riehmann et al., 2005).",2.2 Topic Model Visualizations,[0],[0]
The individual topics are represented both as word lists in the model overview and as word list with bars when viewing a single topic or comparing between two topics.,2.2 Topic Model Visualizations,[0],[0]
"Argviz (Nguyen et al., 2013) captures temporal shifts in topics during a debate or a conversation.",2.2 Topic Model Visualizations,[0],[0]
The individual topics are presented as word lists in the model overview and using word list with bars for the selected topics.,2.2 Topic Model Visualizations,[0],[0]
"Klein et al. (2015) use a dustand-magnet visualization (Yi et al., 2005) to visualize the force of topics on newspaper issues.",2.2 Topic Model Visualizations,[0],[0]
The temporal trajectories of several newspapers are displayed as dust trails in the visualization.,2.2 Topic Model Visualizations,[0],[0]
"The individual topics are displayed as word clouds.
",2.2 Topic Model Visualizations,[0],[0]
"In contrast to these visualizations which support viewing the underlying topics on demand, Termite (Chuang et al., 2012) uses a tabular layout of words and topics to provide an overview of the model to compare across topics.",2.2 Topic Model Visualizations,[0],[0]
It organizes the model into clusters of related topics based on word overlap.,2.2 Topic Model Visualizations,[0],[0]
"This clustered representation is both spaceefficient and speeds corpus understanding.
",2.2 Topic Model Visualizations,[0],[0]
"Despite the breadth of topic model visualizations, a small set of individual topic representations are ubiquitous: word list, word list with bars, word cloud, and network graph.",2.2 Topic Model Visualizations,[0],[0]
"In the following sections, we compare these topic visualization techniques.",2.2 Topic Model Visualizations,[0],[0]
"We conduct a controlled online study to compare the four commonly used visualization techniques identified in Section 2: word list, word list with bars, word cloud, and network graph.",3 Method: Comparing Visualizations,[0],[0]
"We also compare effectiveness with the number of topic words shown, that is, the cardinality of the visualization: five, ten or twenty topic words.",3 Method: Comparing Visualizations,[0],[0]
"We select a corpus that does not assume domain expertise: 7,156 New York Times articles from January 2007 (Sandhaus, 2008).",3.1 Dataset,[0],[0]
"We model the corpus using an LDA (Blei et al., 2003) implementation in Mallet (Yao et al., 2009) with domain-specific stopwords and standard hyperparameter settings.2 Our simple setup is by design: our goal is to emulate the “off the shelf” behavior of conventional topic modeling tools used by novice users.",3.1 Dataset,[0],[0]
"Instead of improving the quality of the model using asymmetric priors (Wallach et al., 2009a) or bigrams (Boyd-Graber et al., 2014), our topic model has topics of variable quality, allowing us to explore the relationship between topic quality and our task measures.
",3.1 Dataset,[0],[0]
Automatic labels are generated from representative Wikipedia article titles using a technique similar to Lau et al. (2011).,3.1 Dataset,[0],[0]
"We first index Wikipedia using Apache Lucene.3 To label a topic, we query Wikipedia with the top twenty topic words to retrieve fifty articles.",3.1 Dataset,[0],[0]
These articles’ titles comprise our candidate set of labels.,3.1 Dataset,[0],[0]
"We then represent each
2n=50, α=0.1, β=0.01 3http://lucene.apache.org/
article using its TF-IDF vector and calculate the centroid (average TF-IDF) of the retrieved articles.",3.1 Dataset,[0],[0]
"To rank and choose the most representative of the set, we calculate the cosine similarity between the centroid TF-IDF vector and the TF-IDF vector of each of the articles.",3.1 Dataset,[0],[0]
We choose the title of the article with the maximum cosine similarity to the centroid.,3.1 Dataset,[0],[0]
"Unlike Lau et al. (2011), we do not include the topic words or Wikipedia title n-grams derived from our label set, as these labels are typically not the best candidates.",3.1 Dataset,[0],[0]
"Although other automatic labeling techniques exist, we choose this one as it is representative of general techniques.",3.1 Dataset,[0],[0]
"As discussed in Section 2, our study compares four of the most common topic visualization techniques.",3.2 Visualizations,[0],[0]
"To produce a meaningful comparison, the space given to each visualization is held constant: 400 × 250 pixels.",3.2 Visualizations,[0],[0]
"Figure 1 shows each visualization for the three cardinalities (or number of words displayed) for the same topic.
",3.2 Visualizations,[0],[0]
"Word List The most straightforward topic representation is a list of the top n words in the topic, ranked by their probability.",3.2 Visualizations,[0],[0]
"In practice, topic word lists have many variations.",3.2 Visualizations,[0],[0]
"They can be represented horizontally (Gardner et al., 2010; Smith et al., 2015) or vertically (Eisenstein et al., 2012; Chaney and Blei, 2012), with or without commas separating the individual words, or using set notation (Chaney and Blei, 2012).",3.2 Visualizations,[0],[0]
"Nguyen et al. (2013) add the weights to the word list by sizing the words based on their probability for the topic, which blurs the boundary with word clouds; however, this approach is not common.",3.2 Visualizations,[0],[0]
"We use a horizontal list of equally sized words ordered by the probability p(w |z) for the word w in the topic z. For space efficiency, we organize our word list in two columns and add item numbers to make the ordering explicit.
",3.2 Visualizations,[0],[0]
Word List with Bars Combining bar graphs with word lists yields a visual representation that not only conveys the ordering but also the absolute value of the weights associated with the words.,3.2 Visualizations,[0],[0]
"We use a similar implementation to Smith et al. (2015) to add horizontal bars to the word list for a topic z where the length of each bar represents the probability p(w |z) for each word w.
Word Cloud The word cloud (or tag cloud) is one of the most popular and well-known text visualization techniques and is a common visualization for topics.",3.2 Visualizations,[0],[0]
"Many options exist for word cloud layout, color scheme, and font size (Mueller, 2012).",3.2 Visualizations,[0],[0]
"Existing work on layouts is split between those that size words by their frequency or probability for the topic (Ramage et al., 2010) and those that size by the rank order of the word (Barth et al., 2014).",3.2 Visualizations,[0],[0]
We use a combination of these techniques where the word’s font size is initially set proportional to its probability in a topic p(w |z).,3.2 Visualizations,[0],[0]
"However, when the word is too large to fit in the canvas, the size is gradually decreased (Barth et al., 2014).",3.2 Visualizations,[0],[0]
"We use a gray scale to visually distinguish words and display all words horizontally to improve readability.
",3.2 Visualizations,[0],[0]
Network Graph Our most complex topic visualization is a network graph.,3.2 Visualizations,[0],[0]
"We use a similar network graph implementation to Smith et al. (2014), which represents each topic as a node-link diagram, where words are circular nodes with edges drawn between commonly co-occurring words.",3.2 Visualizations,[0],[0]
Each word’s radius is scaled by the probability p(w |z) for the word w in a topic z.,3.2 Visualizations,[0],[0]
"While Smith et al. (2014) draw edges based on document-level co-occurrence, we instead use edges to pull together phrases, so they are drawn between words w1 and w2 based on bigram count,
specifically if log(count(w1,w2))",3.2 Visualizations,[0],[0]
>,3.2 Visualizations,[0],[0]
"k, with k = 0.1.4 Edge width and color are applied uniformly to further reduce complexity in the graph.",3.2 Visualizations,[0],[0]
"The network graph is displayed using a force-directed graph layout algorithm (Fruchterman and Reingold, 1991) where all nodes repel each other but links attract connected nodes.",3.2 Visualizations,[0],[0]
"Although every word has some probability for every topic, p(w |z), visualizations typically display only the top n words.",3.3 Cardinality,[0],[0]
"The cardinality may interact with the effectiveness of the different visualization techniques (e.g., more complicated visualizations may degrade with more words).",3.3 Cardinality,[0],[0]
"We use n ∈ {5,10,20}.",3.3 Cardinality,[0],[0]
The study includes two phases with different users.,3.4 Task and Procedure,[0],[0]
"In Labeling (Phase I), users describe a topic given a specific visualization, and we measure speed and self-reported confidence in completing the task.",3.4 Task and Procedure,[0],[0]
"In Validation (Phase II), users select the best and worst among a set of Phase I descriptions and an automatically generated description for how well they represent the original topics’ documents.
",3.4 Task and Procedure,[0],[0]
Phase I:,3.4 Task and Procedure,[0],[0]
"Labeling For each labeling task, users see a topic visualization, provide a short label (up
4From k∈{0.01,0.05,0.1,0.5}, we chose k = 0.1 as the best trade-off between complexity and provided information.
to three words), then give a longer sentence to describe the topic, and finally use a five-point Likert scale to rate their confidence that the label and sentence represent the topic well.",3.4 Task and Procedure,[0],[0]
We also track the time to perform the task.,3.4 Task and Procedure,[0],[0]
"Figure 2 shows an example of a labeling task using the network graph visualization technique with ten words.
",3.4 Task and Procedure,[0],[0]
"Labeling tasks are randomly grouped into human intelligence tasks (HIT) on Mechanical Turk5 such that each HIT includes five tasks from the same visualization technique.6
Phase II:",3.4 Task and Procedure,[0],[0]
"Validation In the validation phase, a new set of users assesses the quality of the labels and sentences created in Phase I by evaluating them against documents associated with the given topic.",3.4 Task and Procedure,[0],[0]
"It is important to evaluate the topic labels in context; a label that superficially looks good is useless if it is not representative of the underlying documents
5All users are in the US or Canada, have more than fifty previously approved HITs, and have an approval rating greater than 90%.
",3.4 Task and Procedure,[0],[0]
"6We did not restrict users from performing multiple HITs, which may have exposed them to multiple visualization techniques.",3.4 Task and Procedure,[0],[0]
"Users completed on average 1.5 HITs.
in the corpus.",3.4 Task and Procedure,[0],[0]
Algorithmically generated labels (not sentences) are also included.,3.4 Task and Procedure,[0],[0]
"Figure 3 shows an example of the validation task.
",3.4 Task and Procedure,[0],[0]
The user-generated labels and sentences are evaluated separately.,3.4 Task and Procedure,[0],[0]
"For each task, the user sees the titles of the top ten documents associated with a topic and a randomized set of labels or sentences, one elicited from each of the four visualization techniques within a given cardinality.",3.4 Task and Procedure,[0],[0]
The set of labels also includes an algorithmically generated label.,3.4 Task and Procedure,[0],[0]
We ask the user to select the “best” and “worst” of the labels or sentences based on how well they describe the documents.,3.4 Task and Procedure,[0],[0]
"Documents are associated to topics based on the probability of the topic, z, given the document, d, p(z |d).",3.4 Task and Procedure,[0],[0]
"Only the title of each document is initially shown to the user with an option to “show article” (or view the first 400 characters of the document).
",3.4 Task and Procedure,[0],[0]
All labels are lowercased to enforce uniformity.,3.4 Task and Procedure,[0],[0]
We merge identical labels so users do not see duplicates.,3.4 Task and Procedure,[0],[0]
"If a merged label receives a “best” or “worst” vote, the vote is split equally across all of the original instances (i.e., across multiple visualization techniques with that label).",3.4 Task and Procedure,[0],[0]
"Finally, we track task com-
pletion time.",3.4 Task and Procedure,[0],[0]
"Each user completes four randomly selected validation tasks as part of a HIT, with the constraint that each task must be from a different topic.",3.4 Task and Procedure,[0],[0]
We also use ground truth seeding for quality control: each HIT includes one additional test task that has a purposefully bad label generated by concatenating three random dictionary words.,3.4 Task and Procedure,[0],[0]
"If the user does not pick the bad label as the “worst”, we discard all data in that HIT.",3.4 Task and Procedure,[0],[0]
"For Phase I, we use a factorial design with factors of Visualization (levels: word list, word list with bars, word cloud, and network graph) and Cardinality (levels: 5, 10, and 20), yielding twelve conditions.",3.5 Study Design and Data Collection,[0],[0]
"For each of the fifty topics in the model and each of the twelve conditions, at least five users perform the labeling task, describing the topic with a label and sentence, resulting in a minimum of 3,000 label and sentence pairs.",3.5 Study Design and Data Collection,[0],[0]
"Each HIT includes five of these labeling tasks, for a minimum of 600 HITs.",3.5 Study Design and Data Collection,[0],[0]
"The users are paid $0.30 per HIT.
",3.5 Study Design and Data Collection,[0],[0]
"For Phase II, we compare descriptions across the four visualization techniques (and automatically generated labels), but only within a given cardinality level rather than across cardinalities.",3.5 Study Design and Data Collection,[0],[0]
"We collected 3,212 label and sentence pairs from 589 users during Phase I. For validation in Phase II, we use the first five labels and sentences collected for each condition for a total of 3.000 labels and sentences.",3.5 Study Design and Data Collection,[0],[0]
"These are shown in sets of four (labels or sentences) during Phase II, yielding a total of 1,500 (3,000/4 + 3,000/4) tasks.",3.5 Study Design and Data Collection,[0],[0]
"Each HIT contains four validation tasks and one ground truth seeding task, for a total of 375 HITs.",3.5 Study Design and Data Collection,[0],[0]
"To increase robustness, we validate twice for a total of 750 HITs, without allowing any two labels or sentences to be compared twice.",3.5 Study Design and Data Collection,[0],[0]
The users get $0.50 per HIT.,3.5 Study Design and Data Collection,[0],[0]
We analyze labeling time and self-reported confidence for the labeling task (Phase I) before reporting on the label quality assessments (Phase II).,4 Results,[0],[0]
"We then analyze linguistic qualities of the labels, which should motivate future work in automatic label generation.
",4 Results,[0],[0]
"We first provide an example of user-generated labels and sentences: the user labels for the topic shown in Figure 1 include government, iraq war, politics, bush administration, and war on terror.",4 Results,[0],[0]
"Examples of sentences include “President Bush’s military plan in Iraq” and “World news involving the US president and Iraq”.7
To interpret the results, it is useful to also understand the quality of the generated topics, which varies throughout the model and may impact a user’s ability to generate good labels.",4 Results,[0],[0]
"We measure topic quality using topic coherence, an automatic measure that correlates with how much sense a topic makes to a user (Lau et al., 2014).8",4 Results,[0],[0]
The average topic coherence for the model is 0.09 (SD = 0.05).,4 Results,[0],[0]
Figure 4 shows the three best (top) and three worst topics (bottom) according to their observed coherence: the coherence metric distinguishes obvious topics from inscrutable ones.,4 Results,[0],[0]
"Section 4.3 shows that users cre-
7The complete set of labels and sentences are available at https://github.com/alisonmsmith/Papers/ tree/master/TopicRepresentations.
8We use a reference corpus of 23 million Wikipedia articles for computing normalized pointwise mutual information needed for computing the observed coherence.
",4 Results,[0],[0]
ated lower quality labels for low coherence topics.,4 Results,[0],[0]
More complex visualization techniques take longer to label (Table 1 and Figure 5).,4.1 Labeling Time,[0],[0]
"The labeling tasks took on average 57.9 seconds (SD = 58.5) to complete and a two-way ANOVA (visualization technique × cardinality) reveals significant main effects for both the visualization technique9 and the cardinality,10 as well as a significant interaction effect.11
For lower cardinality, the labeling time across visualization techniques is similar, but there are notable differences for higher cardinality.",4.1 Labeling Time,[0],[0]
"Posthoc pairwise comparisons based on the interaction effect (with Bonferroni adjustment) found no significant differences between visualizations with five words and only one significant difference for ten words (word list with bars was slower than word cloud, p< .05).",4.1 Labeling Time,[0],[0]
"For twenty words, however, the network graph was significantly slower at an average of 77.9s (SD = 72.0) than the other three visualiza-
9F(3,3199) = 10.58, p < .001, η2p = .01 10F(2,3199)",4.1 Labeling Time,[0],[0]
"= 14.60, p < .001, η2p = .01 11F(6,3199) = 4.59, p < .001, η2p = .01
tions (p < .05).",4.1 Labeling Time,[0],[0]
"This effect is likely due to the network graph becoming increasingly dense with more nodes (Figure 1, bottom right).",4.1 Labeling Time,[0],[0]
"In contrast, the relatively simple word list visualization was significantly faster with twenty words than the three other visualizations (p < .05), taking only 52.1s on average (SD= 53.4).",4.1 Labeling Time,[0],[0]
"Word list with bars and word cloud were not significantly different from each other.
",4.1 Labeling Time,[0],[0]
"As a secondary analysis, we examine the relationship between elapsed time and the observed coherence for each topic.",4.1 Labeling Time,[0],[0]
"Topics with high coherence scores, for example, may be faster to label, because they are easier to interpret.",4.1 Labeling Time,[0],[0]
"However, the small negative correlation between time and coherence (Figure 6, top) was not significant (r48 =−.13, p = .364).",4.1 Labeling Time,[0],[0]
"For each labeling task, users rate their confidence that their labels and sentences describe the topic well on a scale from 1 (least confident) to 5 (most confident).",4.2 Self-Reported Labeling Confidence,[0],[0]
The average confidence across all conditions was 3.6 (SD= 0.9).,4.2 Self-Reported Labeling Confidence,[0],[0]
"Kruskal-Wallis tests show a significant impact of visualization technique on confidence with five and ten words, but not twenty.12 While average confidence ratings across all conditions only range from 3.4 to 3.7, perceived confidence with network graph suffers when the visualization has too few words (Table 1).
",4.2 Self-Reported Labeling Confidence,[0],[0]
"As a secondary analysis, we compare the selfreported confidence with observed coherence for each topic (Figure 6, bottom).",4.2 Self-Reported Labeling Confidence,[0],[0]
"Increased user confidence with more coherent topics is supported by a moderate positive correlation between topic coher-
12Five words: χ23 = 12.62, p = .006.",4.2 Self-Reported Labeling Confidence,[0],[0]
"Ten words: χ 2 3 = 7.94, p = .047.",4.2 Self-Reported Labeling Confidence,[0],[0]
"We used nonparametric tests because the data is ordinal and we cannot guarantee that all differences between points on the scale are equal.
ence and confidence (r48 = .32, p = .026).",4.2 Self-Reported Labeling Confidence,[0],[0]
This result provides further evidence that topic coherence is an effective measurement of topic interpretability.,4.2 Self-Reported Labeling Confidence,[0],[0]
Other users’ perceived quality of topic labels is the best real-world measure of quality (as described in Section 3.4).,4.3 Other Users’ Rating of Label Quality,[0],[0]
"Overall, the visualization techniques had similar quality labels, but automatically generated labels do not fare well.",4.3 Other Users’ Rating of Label Quality,[0],[0]
Automatic labels get far fewer “best” votes and far more “worst” votes than user-generated labels produced from any of the four visualization techniques (Figure 7).,4.3 Other Users’ Rating of Label Quality,[0],[0]
"Chi-square tests on the distribution of “best” votes for labels for each cardinality show that the visualization matters.13 Posthoc analysis using pairwise Chi-square
13Five words: χ24,N=500 = 16.47, p = .002.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"Ten words: χ24,N=500 = 14.62, p = .006.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"Twenty words: χ 2 4,N=500 = 22.83, p < .001.
tests with Bonferroni correction show that automatic labels were significantly worse than user-generated labels from each of the visualization techniques (all comparisons p < .05).",4.3 Other Users’ Rating of Label Quality,[0],[0]
"No other pairwise comparisons were significant.
",4.3 Other Users’ Rating of Label Quality,[0],[0]
"For sentences, no visualization technique emerged as better than the others.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"Additionally, there is no existing automatic approach to compare against.",4.3 Other Users’ Rating of Label Quality,[0],[0]
The distribution of “best” counts here was relatively uniform.,4.3 Other Users’ Rating of Label Quality,[0],[0]
"Separate Kruskal-Wallis tests for each cardinality to examine the impact of the visualization techniques on “best” counts did not reveal any significant results.
",4.3 Other Users’ Rating of Label Quality,[0],[0]
"As a secondary qualitative analysis, we examine the relationship between topic coherence and the assessed quality of the labels.",4.3 Other Users’ Rating of Label Quality,[0],[0]
The automatic algorithm tended to produce better labels for the coherent topics than for the incoherent topics.,4.3 Other Users’ Rating of Label Quality,[0],[0]
"For example, Topic 26 (Figure 4, b)—{music, band, songs}—and Topic 31 (Figure 4, c)—{food, restaurant, wine}— are two of the most coherent topics.",4.3 Other Users’ Rating of Label Quality,[0],[0]
The automatic algorithm labeled Topic 26 as music and Topic 31 as food.,4.3 Other Users’ Rating of Label Quality,[0],[0]
"For both of these coherent topics, the labels generated by the automatic algorithm secured the most “best” votes and no “worst” votes.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"In contrast, Topic 16 (Figure 4, e)—{years, home, work}—and Topic 23 (Figure 4, f)—{death, family, board}— are two of the least coherent topics.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"The automatic labels refusal of work and death of michael jackson yielded the most “worst” votes and fewest “best” votes.
",4.3 Other Users’ Rating of Label Quality,[0],[0]
"To further demonstrate this relationship, we extracted from the 50 topics the top and bottom quartiles of 13 topics each14 based on their observed coherence scores.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"Figure 8 shows a comparison of the “best” and “worst” votes for the topic labels for these quartiles, including user-generated and automatically generated labels.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"For the top quartile, the number of “best” votes per technique ranged from 61 for automatic labels to 96 for the network graph visualization.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"The range for the bottom quartile was larger, from only 45 “best” votes for automatic labels to 99 for word list with bars.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"The automatic labels, in particular, received a large relative increase in “best” votes when comparing the bottom quartile
14We could not get exact quartiles, because we have 50 topics, so we rounded up to include 13 topics in each quartile.
to the top quartile (increase of 37%).",4.3 Other Users’ Rating of Label Quality,[0],[0]
"Additionally, the word list, word cloud, and network graph visualizations all lead to labels with similar “best” and “worst” votes for both the top and bottom quartiles.",4.3 Other Users’ Rating of Label Quality,[0],[0]
"However, the word list with bars representation shows both a large relative increase for the best votes (increase of 19%) and relative decrease for the “worst” votes (decrease of 23%) when comparing the top to the bottom quartile.",4.3 Other Users’ Rating of Label Quality,[0],[0]
These results suggest that adding numeric word probability information highlighted by the bars may help users understand poor quality topics.,4.3 Other Users’ Rating of Label Quality,[0],[0]
The results of Phase I provide a large manually generated label set.,4.4 Label Analysis,[0],[0]
Exploratory analysis of these labels reveals linguistic features users tend to incorporate when labeling topics.,4.4 Label Analysis,[0],[0]
We discuss implications for automatic labeling in Section 5.,4.4 Label Analysis,[0],[0]
"In particular, users prefer shorter labels, labels that include topic words and phrases, and abstraction in topic labeling.
",4.4 Label Analysis,[0],[0]
Length,4.4 Label Analysis,[0],[0]
"The manually generated labels use 2.01 words (SD = 0.95), and the algorithmically generated labels use 3.16 words (SD = 2.05).",4.4 Label Analysis,[0],[0]
"Interestingly, the labels voted as “best” were shorter on average than those voted “worst”, regardless of whether algorithmically generated labels are included in the analysis.",4.4 Label Analysis,[0],[0]
"With algorithmically generated labels in-
cluded, the average lengths are 2.04 (SD = 1.16) words for “best” labels and 2.83 (SD = 1.79) words for “worst” labels,15",4.4 Label Analysis,[0],[0]
"but even without the algorithmically generated labels, the “best” labels are
15The “best” label set includes all labels voted at least once as “best”, and similarly the “worst” label set includes all labels voted at least once as “worst”.
",4.4 Label Analysis,[0],[0]
"shorter (M = 1.96, SD = .87) than the “worst” labels (M = 2.09, SD = 1.01).
",4.4 Label Analysis,[0],[0]
"Shared Topic Words Of the 3,212 labels, 2,278, or 71%, contain at least one word taken directly from the topic words—that is, the five, ten, or twenty words shown in the visualization; however, there are no notable differences between the visualization techniques.",4.4 Label Analysis,[0],[0]
"Additionally, the number of topic words included on average was similar across all three cardinalities, suggesting that users often use the same number of topic words regardless of how many were shown in the visualization.
",4.4 Label Analysis,[0],[0]
We further examine the relationship between a topic word’s rank and whether the word was selected for inclusion in the labels.,4.4 Label Analysis,[0],[0]
Figure 9 shows the average probability of a topic word being used in a label by the topic word’s rank.,4.4 Label Analysis,[0],[0]
More highly ranked words were included more frequently in labels.,4.4 Label Analysis,[0],[0]
"As cardinality increased, the highest ranked words were also less likely to be employed, as users had more words available to them.
",4.4 Label Analysis,[0],[0]
"Phrases Although LDA makes a “bag of words” assumption when generating topics, users can reconstruct relevant phrases from the unique words.",4.4 Label Analysis,[0],[0]
"For Topic 26, for example, all visualizations include the same topic terms.",4.4 Label Analysis,[0],[0]
"However, the network graph visualization highlights the phrases “jazz singer” and “rock band” by linking their words as commonly cooccurring terms in the corpus.",4.4 Label Analysis,[0],[0]
These phrases are not as easily discernible in the word cloud visualization (Figure 10).,4.4 Label Analysis,[0],[0]
"We compute a set of common
phrases by taking all bigrams and trigrams that occur more than fifty and twenty times, respectively, in the NYT corpus.",4.4 Label Analysis,[0],[0]
"Of the 3212 labels, 575 contain one of these common phrases, but those generated by users with the network graph visualization contain the most phrases.",4.4 Label Analysis,[0],[0]
"Labels generated in the word list (22% of the labels), word list with bars (25%), and word cloud (24%) conditions contain fewer phrases than the labels generated in the network graph condition (29%).",4.4 Label Analysis,[0],[0]
"Although it is not surprising that the network graph visualization better communicates common phrases in the corpus as edges are drawn between these phrases, this suggests other approaches to drawing edges.",4.4 Label Analysis,[0],[0]
"Edges drawn based on sentence or document-based co-occurrence, for example, could instead uncover longer-distance dependencies between words, potentially identifying distinct subtopics with a topic.
",4.4 Label Analysis,[0],[0]
"Hyponymy Users often prefer more general terms for labels than the words in the topic (Newman et al., 2010b).",4.4 Label Analysis,[0],[0]
"To measure this, we look for the set of unique hyponyms and hypernyms of the topic words, or those that are not themselves a topic word, that appear in the manually generated labels.",4.4 Label Analysis,[0],[0]
"We use the super-subordinate relation, which represents hypernymy and hyponymy, from WordNet (Miller, 1995).",4.4 Label Analysis,[0],[0]
"Of the 3,212 labels, 235 include a unique hypernym and 152 include a unique hyponym of the associated topic words found using WordNet, confirming that users are significantly more likely to produce a more generic description of the topic (χ21,N=387 = 17.38, p < .001).",4.4 Label Analysis,[0],[0]
"For the 235 more generic labels, fewer of these came from word list (22%) and more from the network graph (30%) than the other visualization techniques—word list with bars (24%) and word cloud (24%).",4.4 Label Analysis,[0],[0]
"This may mean
that the network graph helps users to better understand the topic words as a group and therefore label them using a hypernym.",4.4 Label Analysis,[0],[0]
We also compared hypernym inclusion for “best” and “worst” labels: 63 (5%) of the “best” labels included a hypernym while only 44 (3%) of the “worst” labels included a hypernym.,4.4 Label Analysis,[0],[0]
Each of the visualization techniques led to approximately the same percentage of the 152 total more specific labels.,4.4 Label Analysis,[0],[0]
"Although the four visualization techniques yield similar quality labels, our crowdsourced study highlights the strengths and weaknesses of the techniques.",5 Discussion,[0],[0]
"It also reveals some preferred linguistic features of user-generated labels and how these differ from automatically generated labels.
",5 Discussion,[0],[0]
The trade-offs among the visualization techniques show that context matters.,5 Discussion,[0],[0]
"If efficiency is paramount, then word lists—both simple and fast— are likely best.",5 Discussion,[0],[0]
"For a cardinality of twenty words, for example, users presented with the simple word list are significantly faster at labeling than those shown the network graph visualization.",5 Discussion,[0],[0]
"At the same time, more complex visualizations expose users to multi-word expressions that the simpler visualization techniques may obscure (Section 4.4).",5 Discussion,[0],[0]
Future work should investigate for what types of user tasks this information is most useful.,5 Discussion,[0],[0]
There is also potential for misinterpretation of topic meaning when cardinality is low.,5 Discussion,[0],[0]
"Users can misunderstand the topic based on the small set of words, or adjacent words can inadvertently appear to form a meaningful phrase, which may be particularly an issue for the word cloud.
",5 Discussion,[0],[0]
Our crowdsourced study identified the “best” and “worst” labels for the topic’s documents.,5 Discussion,[0],[0]
"An additional qualitative coding phase could evaluate each “worst” label to determine why, whether due to misinterpretation, spelling or grammatical errors, length, or something else.
",5 Discussion,[0],[0]
"Surprisingly, we found no relationship between topic coherence and labeling time (Section 4.1).",5 Discussion,[0],[0]
"This is perhaps because not only are users quick to label topics they understand, but they also quickly give up when they have no idea what a topic is about.",5 Discussion,[0],[0]
"We do, however, find a relationship between coher-
ence and confidence (Section 4.2).",5 Discussion,[0],[0]
"This positive correlation supports topic coherence as an effective measure for human interpretability.
",5 Discussion,[0],[0]
"Automatically generated labels are consistently chosen as the “worst” labels, although they are competitive with the user-generated labels for highly coherent topics (Section 4.3).",5 Discussion,[0],[0]
Future automatic labeling algorithms should still be robust to poor topics.,5 Discussion,[0],[0]
Algorithmically generated labels were longer and more specific than the user-generated labels.,5 Discussion,[0],[0]
It is unsurprising that these automatic labels were consistently deemed the worst.,5 Discussion,[0],[0]
"Users prefer shorter labels with more general words (e.g., hypernyms, Section 4.4).",5 Discussion,[0],[0]
We show specific examples of this phenomenon from Topic 14 and Topic 48.,5 Discussion,[0],[0]
"For Topic 14—{health, drug, medical, research, conditions}—the algorithm generated the label health care in the united states, but users preferred the less specific labels health and medical research.",5 Discussion,[0],[0]
"Similarly, for Topic 48—{league, team, baseball, players, contract}—the algorithm generated the label major league baseball on fox; users preferred simpler labels, such as baseball.",5 Discussion,[0],[0]
"Automatic labeling algorithms thus can be improved to focus on general, shorter labels.",5 Discussion,[0],[0]
"Interestingly, simple textual labels have been shown to be more efficient but less effective than topic keywords (i.e., word lists) for an automatic document retrieval task (Aletras and Stevenson, 2014), highlighting the extra information present in the word lists.",5 Discussion,[0],[0]
"Our findings show that users are also able to effectively interpret the word list information, as that visualization was both efficient and effective for the task of topic labeling compared to the other more complex visualizations.
",5 Discussion,[0],[0]
"Although we use WordNet to verify that users prefer more general labels, this is not a panacea, because WordNet does not capture all of the generalization users want in labels.",5 Discussion,[0],[0]
"In many cases, users use terms that synthesize relationships beyond trivial WordNet relationships, such as locations or entities.",5 Discussion,[0],[0]
"For example, Topic 18—{san, los, angeles, terms, francisco}—was consistently labeled as the location California, and Topic 38—{open, second, final, won, williams}—which almost all users labeled as tennis, required a knowledge of the entities Serena Williams and the U.S. Open.",5 Discussion,[0],[0]
"In addition to WordNet, an automatic labeling algorithm could
use a gazetteer for determining locations from topic words and a knowledge base such as TAP (Guha and McCool, 2003), which provides a broad range of information about popular culture for matching topic words to entities.",5 Discussion,[0],[0]
"We present a crowdsourced user study to compare four topic visualization techniques—a simple ranked word list, a ranked word list with bars representing word probability, a word cloud, and a network graph—based on how they impact the user’s understanding of a topic.",6 Conclusion and Future Work,[0],[0]
The four visualization techniques lead to similar quality labels as rated by end users.,6 Conclusion and Future Work,[0],[0]
"However, users label more quickly with the simple word list, yet tend to incorporate phrases and more generic terminology when using the more complex network graph.",6 Conclusion and Future Work,[0],[0]
"Additionally, users feel more confident labeling coherent topics, and manual labels far outperform the automatically generated labels against which they were evaluated.
",6 Conclusion and Future Work,[0],[0]
Automatic labeling can benefit from this research in two ways: by suggesting when to apply automatic labeling and by providing training data for improving automatic labeling.,6 Conclusion and Future Work,[0],[0]
"While automatic labels falter compared to human labels in general, they do quite well when the underlying topics are of high quality.",6 Conclusion and Future Work,[0],[0]
"Thus, one reasonable strategy would be to use automatic labels for a portion of topics, but to use human validation to either first improve the remainder of the topics (Hu et al., 2014) or to provide labels (as in this study) for lower quality topics.",6 Conclusion and Future Work,[0],[0]
"Moreover, our labels provide training data that may be useful for automatic labeling techniques using featurebased models (Charniak, 2000)—combining information from Wikipedia, WordNet, syntax, and the underlying topics—to reproduce the types of labels and sentences created (and favored) by users.
",6 Conclusion and Future Work,[0],[0]
"Finally, our study focuses on comparing individual topic visualization techniques.",6 Conclusion and Future Work,[0],[0]
An open question that we do not address is whether this generalizes to understanding entire topic models.,6 Conclusion and Future Work,[0],[0]
"In other words, simple word list visualizations are useful for quick and high-quality topic summarization, but does this mean that a collection of word lists— one per topic—will also be optimal when displaying the entire model?",6 Conclusion and Future Work,[0],[0]
"Future work should look at com-
paring visualization techniques for full topic model understanding.",6 Conclusion and Future Work,[0],[0]
"We would like to thank the anonymous reviewers as well as the TACL editors, Timothy Baldwin and Lillian Lee, for helpful comments on an earlier draft of this paper.",Acknowledgments,[0],[0]
This work was funded by NSF grant IIS1409287.,Acknowledgments,[0],[0]
"Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.",Acknowledgments,[0],[0]
"Probabilistic topic models are important tools for indexing, summarizing, and analyzing large document collections by their themes.",abstractText,[0],[0]
"However, promoting end-user understanding of topics remains an open research problem.",abstractText,[0],[0]
"We compare labels generated by users given four topic visualization techniques— word lists, word lists with bars, word clouds, and network graphs—against each other and against automatically generated labels.",abstractText,[0],[0]
Our basis of comparison is participant ratings of how well labels describe documents from the topic.,abstractText,[0],[0]
Our study has two phases: a labeling phase where participants label visualized topics and a validation phase where different participants select which labels best describe the topics’ documents.,abstractText,[0],[0]
"Although all visualizations produce similar quality labels, simple visualizations such as word lists allow participants to quickly understand topics, while complex visualizations take longer but expose multi-word expressions that simpler visualizations obscure.",abstractText,[0],[0]
"Automatic labels lag behind user-created labels, but our dataset of manually labeled topics highlights linguistic patterns (e.g., hypernyms, phrases) that can be used to improve automatic topic labeling algorithms.",abstractText,[0],[0]
"1 Comprehensible Topic Models Needed A central challenge of the “big data” era is to help users make sense of large text collections (Hotho et al., 2005).",abstractText,[0],[0]
"A common approach to summarizing the main themes in a corpus is to use topic models (Blei, 2012), which are data-driven statistical models that identify words that appear together in similar documents.",abstractText,[0],[0]
These sets of words or “topics” evince internal coherence,abstractText,[0],[0]
and can help guide users to relevant documents.,abstractText,[0],[0]
"For instance, an FBI investigator sifting through the released Hillary Clinton e-mails may see a topic with the words “Benghazi”, “Libya”, “Blumenthal”, and “success”, spurring the investigator to dig deeper to find further evidence of inappropriate communication with longtime friend Sidney Blumenthal regarding Benghazi.",abstractText,[0],[0]
"A key challenge for topic modeling, however, is how to promote end-user understanding of individual topics and the overall model.",abstractText,[0],[0]
"Most existing topic presentations use simple word lists (Chaney and Blei, 2012; Eisenstein et al., 2012).",abstractText,[0],[0]
"Although a variety of alternative topic visualization techniques exist (Sievert and Shirley, 2014; Yi et al., 2005), there has been no systematic assessment to compare them.",abstractText,[0],[0]
"Beyond exploring different visualization techniques, another means of making topics easier for users to understand is to provide descriptive labels to complement a topic’s set of words (Aletras et al., 2014).",abstractText,[0],[0]
"Unfortunately, manual labeling is slow and, while automatic labeling approaches exist (Lau et al., 2010; Mei et al., 2007; Lau et al., 2011), their effectiveness is not guaranteed for all tasks.",abstractText,[0],[0]
"To better understand these problems, we use labeling to evaluate topic model visualizations.",abstractText,[0],[0]
"Our study compares the impact of four commonly used topic visualization techniques on the labels that users create when interpreting a topic (Figure 1): word lists, word lists with bars, word clouds, and network graphs.",abstractText,[0],[0]
"On Amazon Mechanical Turk, one set of users viewed a series of individual topic vi-",abstractText,[0],[0]
Evaluating Visual Representations for Topic Understanding and Their Effects on Manually Generated Topic Labels,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 298–307, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Neural word embeddings represent meaning via geometry.,1 Introduction,[0],[0]
A good embedding provides vector representations of words such that the relationship between two vectors mirrors the linguistic relationship between the two words.,1 Introduction,[0],[0]
"Despite the growing interest in vector representations of semantic information, there has been relatively little work on direct evaluations of these models.",1 Introduction,[0],[0]
"In this work, we explore several approaches to measuring the quality of neural word embeddings.",1 Introduction,[0],[0]
"In particular, we perform a comprehensive analysis of evaluation methods and introduce novel methods that can be implemented through crowdsourcing, providing better insights into the relative strengths of different embeddings.
",1 Introduction,[0],[0]
Existing schemes fall into two major categories: extrinsic and intrinsic evaluation.,1 Introduction,[0],[0]
"In extrinsic evaluation, we use word embeddings as input features to a downstream task and measure changes in performance metrics specific to that task.",1 Introduction,[0],[0]
"Examples include part-of-speech tagging and named-entity recognition (Pennington et al., 2014).",1 Introduction,[0],[0]
"Extrinsic
evaluation only provides one way to specify the goodness of an embedding, and it is not clear how it connects to other measures.
",1 Introduction,[0],[0]
"Intrinsic evaluations directly test for syntactic or semantic relationships between words (Mikolov et al., 2013a; Baroni et al., 2014).",1 Introduction,[0],[0]
"These tasks typically involve a pre-selected set of query terms and semantically related target words, which we refer to as a query inventory.",1 Introduction,[0],[0]
"Methods are evaluated by compiling an aggregate score for each method such as a correlation coefficient, which then serves as an absolute measure of quality.",1 Introduction,[0],[0]
"Query inventories have so far been collected opportunistically from prior work in psycholinguistics, information retrieval (Finkelstein et al., 2002), and image analysis (Bruni et al., 2014).",1 Introduction,[0],[0]
"Because these inventories were not constructed for word embedding evaluation, they are often idiosyncratic, dominated by specific types of queries, and poorly calibrated to corpus statistics.
",1 Introduction,[0],[0]
"To remedy these problems, this paper makes the following contributions.",1 Introduction,[0],[0]
"First, this is the first paper to conduct a comprehensive study covering a wide range of evaluation criteria and popular embedding techniques.",1 Introduction,[0],[0]
"In particular, we study how outcomes from three different evaluation criteria are connected: word relatedness, coherence, downstream performance.",1 Introduction,[0],[0]
We show that using different criteria results in different relative orderings of embeddings.,1 Introduction,[0],[0]
"These results indicate that embedding methods should be compared in the context of a specific task, e.g., linguistic insight or good downstream performance.
",1 Introduction,[0],[0]
"Second, we study the connections between direct evaluation with real users and pre-collected offline data.",1 Introduction,[0],[0]
We propose a new approach to evaluation that focuses on direct comparison of embeddings with respect to individual queries rather than overall summary scores.,1 Introduction,[0],[0]
"Because we phrase all tasks as choice problems rather than ordinal relevance tasks, we can ease the burden of the an-
298
notators.",1 Introduction,[0],[0]
We show that these evaluations can be gathered efficiently from crowdsourcing.,1 Introduction,[0],[0]
Our results also indicate that there is in fact strong correlation between the results of automated similarity evaluation and direct human evaluation.,1 Introduction,[0],[0]
"This result justifies the use of offline data, at least for the similarity task.
",1 Introduction,[0],[0]
"Third, we propose a model- and data-driven approach to constructing query inventories.",1 Introduction,[0],[0]
"Rather than picking words in an ad hoc fashion, we select query words to be diverse with respect to their frequency, parts-of-speech and abstractness.",1 Introduction,[0],[0]
"To facilitate systematic evaluation and comparison of new embedding models, we release a new frequency-calibrated query inventory along with all user judgments at http://www.cs.",1 Introduction,[0],[0]
"cornell.edu/˜schnabts/eval/.
Finally, we observe that word embeddings encode a surprising degree of information about word frequency.",1 Introduction,[0],[0]
We found this was true even in models that explicitly reserve parameters to compensate for frequency effects.,1 Introduction,[0],[0]
This finding may explain some of the variability across embeddings and across evaluation methods.,1 Introduction,[0],[0]
"It also casts doubt on the common practice of using the vanilla cosine similarity as a similarity measure in the embedding space.
",1 Introduction,[0],[0]
It is important to note that this work is a survey of evaluation methods not a survey of embedding methods.,1 Introduction,[0],[0]
"The specific example embeddings presented here were chosen as representative samples only, and may not be optimal.",1 Introduction,[0],[0]
"We refer to a word embedding as a mapping V → RD : w 7→ ~w that maps a word w from a vocabulary V to a real-valued vector ~w in an embedding space of dimensionality D.
Following previous work (Collobert et al., 2011; Mikolov et al., 2013a)",2 Word embeddings,[0],[0]
"we use the commonly employed cosine similarity, defined as similarity(w1, w2) = ~w1·~w2‖~w1‖‖~w2‖ , for all similarity computations in the embedding space.",2 Word embeddings,[0],[0]
The list of nearest neighbors of a word w are all words v ∈ V,2 Word embeddings,[0],[0]
"\ {w}, sorted in descending order by similarity(w, v).",2 Word embeddings,[0],[0]
"We will denote w as the query word in the remainder of this paper.
",2 Word embeddings,[0],[0]
All experiments in this paper are carried out on six popular unsupervised embedding methods.,2 Word embeddings,[0],[0]
"These embeddings form a representative but incomplete subset; and since we are study-
ing evaluation methods and not embeddings themselves, no attempt has been made to optimize these embeddings.",2 Word embeddings,[0],[0]
"The first two embedding models, the CBOW model of word2vec (Mikolov et al., 2013a) and C&W embeddings (Collobert et al., 2011) both are motivated by a probabilistic prediction approach.",2 Word embeddings,[0],[0]
"Given a number of context words around a target word w, these models formulate the embedding task as that of finding a representation that is good at predicting w from the context representations.
",2 Word embeddings,[0],[0]
"The second group of models, Hellinger PCA (Lebret and Collobert, 2014), GloVe (Pennington et al., 2014), TSCCA (Dhillon et al., 2012) and Sparse Random Projections (Li et al., 2006) follow a reconstruction approach: word embeddings should be able to capture as much relevant information from the original co-occurrence matrix as possible.
",2 Word embeddings,[0],[0]
Training corpus.,2 Word embeddings,[0],[0]
We tried to make the comparison as fair as possible.,2 Word embeddings,[0],[0]
"As the C&W embeddings were only available pretrained on a November 2007 snapshot of Wikipedia, we chose the closest available Wikipedia dump (2008-03-01) for training the other models.",2 Word embeddings,[0],[0]
"We tokenized the data using the Stanford tokenizer (Manning et al., 2014).",2 Word embeddings,[0],[0]
"Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros.
Details.",2 Word embeddings,[0.9941667181189389],"['Like Collobert et al. (2011), we lower-cased all words and replaced digits with zeros.']"
All models embedded words into a 50- dimensional space (D = 50).,2 Word embeddings,[0],[0]
"As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.",2 Word embeddings,[1.0],"['As implemented, each method uses a different vocabulary, so we computed the intersection of the six vocabularies and used the resulting set of 103,647 words for all nearest-neighbor experiments.']"
We begin with intrinsic evaluation of relatedness using both pre-collected human evaluations and a novel online user study.,3 Relatedness,[0],[0]
Section 3.1 introduces the list of datasets that is commonly used as a benchmark for embedding methods.,3 Relatedness,[0],[0]
"There, embeddings are evaluated individually and only their final scores are compared, hence we refer to this scenario as absolute intrinsic evaluation.",3 Relatedness,[0],[0]
"We present a new scenario, comparative intrinsic evaluation, in which we ask people directly for their preferences among different embeddings.",3 Relatedness,[0],[0]
"We demonstrate that we can achieve the same results as offline, absolute metrics using online, comparative metrics.",3 Relatedness,[0],[0]
"For the absolute intrinsic evaluation, we used the same datasets and tasks as Baroni et al. (2014).",3.1 Absolute intrinsic evaluation,[0],[0]
"While we present results on all tasks for completeness, we will mainly focus on relatedness in this section.",3.1 Absolute intrinsic evaluation,[0],[0]
"There are four broad categories:
• Relatedness: These datasets contain relatedness scores for pairs of words; the cosine similarity of the embeddings for two words should have high correlation (Spearman or Pearson) with human relatedness scores.
",3.1 Absolute intrinsic evaluation,[0],[0]
• Analogy:,3.1 Absolute intrinsic evaluation,[0],[0]
This task was popularized by Mikolov et al. (2013a).,3.1 Absolute intrinsic evaluation,[0],[0]
The goal is to find a term x for a given term y,3.1 Absolute intrinsic evaluation,[0],[0]
"so that x : y best resembles a sample relationship a : b.
•",3.1 Absolute intrinsic evaluation,[0],[0]
"Categorization: Here, the goal is to recover a clustering of words into different categories.",3.1 Absolute intrinsic evaluation,[0],[0]
"To do this, the corresponding word vectors of all words in a dataset are clustered and the purity of the returned clusters is computed with respect to the labeled dataset.
",3.1 Absolute intrinsic evaluation,[0],[0]
"• Selectional preference: The goal is to determine how typical a noun is for a verb either as a subject or as an object (e.g., people eat, but we rarely eat people).",3.1 Absolute intrinsic evaluation,[0],[0]
"We follow the procedure that is outlined in Baroni et al. (2014).
",3.1 Absolute intrinsic evaluation,[0],[0]
Several important design questions come up when designing reusable datasets for evaluating relatedness.,3.1 Absolute intrinsic evaluation,[0],[0]
"While we focus mainly on challenges that arise in the relatedness evaluation task, many of the questions discussed also apply to other scenarios.
",3.1 Absolute intrinsic evaluation,[0],[0]
Query inventory.,3.1 Absolute intrinsic evaluation,[0],[0]
How we pick the word pairs to evaluate affects the results of the evaluation.,3.1 Absolute intrinsic evaluation,[0],[0]
"The commonly-used WordSim-353 dataset (Finkelstein et al., 2002), for example, only tries to have word pairs with a diverse set of similarity scores.",3.1 Absolute intrinsic evaluation,[0],[0]
"The more recent MEN dataset (Bruni et al., 2014) follows a similar strategy, but restricts queries to words that occur as annotations in an image dataset.",3.1 Absolute intrinsic evaluation,[0],[0]
"However, there are more important criteria that should be considered in order to create a diverse dataset: (i) the frequency of the words in the English language (ii) the parts of speech of the words and (iii) abstractness vs. concreteness of the terms.",3.1 Absolute intrinsic evaluation,[0],[0]
"Not only is frequency important because we want to test the quality of embeddings on rare words, but also because it is related with
distance in the embedding space as we show later and should be explicitly considered.
",3.1 Absolute intrinsic evaluation,[0],[0]
Metric aggregation.,3.1 Absolute intrinsic evaluation,[0],[0]
The main conceptual shortcoming of using correlation-based metrics is that they aggregate scores of different pairs — even though these scores can vary greatly in the embedding space.,3.1 Absolute intrinsic evaluation,[0],[0]
"We can view the relatedness task as the task of evaluating a set of rankings, similar to ranking evaluation in Information Retrieval.",3.1 Absolute intrinsic evaluation,[0],[0]
"More specifically, we have one query for each unique query word w and rank all remaining words v in the vocabulary accordingly.",3.1 Absolute intrinsic evaluation,[0],[0]
"The problem now is that we usually cannot directly compare scores from different rankings (Aslam and Montague, 2001) as their scores are not guaranteed to have the same ranges.",3.1 Absolute intrinsic evaluation,[0],[0]
An even worse case is the following scenario.,3.1 Absolute intrinsic evaluation,[0],[0]
Assume we use rank correlation as our metric.,3.1 Absolute intrinsic evaluation,[0],[0]
"As a consequence, we need our gold ranking to define an order on all the word pairs.",3.1 Absolute intrinsic evaluation,[0],[0]
"However, this also means that we somehow need to order completely unrelated word pairs; for example, we have to decide whether (dog, cat) is more similar than (banana, apple).",3.1 Absolute intrinsic evaluation,[0],[0]
Table 1 presents the results on 14 different datasets for the six embedding models.,3.2 Absolute results,[0],[0]
We excluded examples from datasets that contained words not in our vocabulary.,3.2 Absolute results,[0],[0]
"For the relatedness and selective preference tasks, the numbers in the table indicate the correlation coefficient of human scores and the cosine similarity times 100.",3.2 Absolute results,[0],[0]
The numbers for the categorization tasks reflect the purities of the resulting clusters.,3.2 Absolute results,[0],[0]
"For the analogy task, we report accuracy.
",3.2 Absolute results,[0],[0]
CBOW outperforms other embeddings on 10 of 14 datasets.,3.2 Absolute results,[0],[0]
"CBOW especially excels at the relatedness and analogy tasks, but fails to surpass other models on the selective preferences tasks.",3.2 Absolute results,[0],[0]
"Random projection performs worst in 13 out of the 14 tasks, being followed by Hellinger PCA.",3.2 Absolute results,[0],[0]
"C&W and TSCCA are similar on average, but differ across datasets.",3.2 Absolute results,[0],[0]
"Moreover, although TSCCA and GloVe perform similarly on most tasks, TSCCA suffers disproportionally on the analogy tasks.",3.2 Absolute results,[0],[0]
"In comparative evaluation, users give direct feedback on the embeddings themselves, so we do not have to define a metric that compares scored word pairs.",3.3 Comparative intrinsic evaluation,[1.0],"['In comparative evaluation, users give direct feedback on the embeddings themselves, so we do not have to define a metric that compares scored word pairs.']"
"Rather than defining both query and target words, we need only choose query words since the
embeddings themselves will be used to define the comparable target words.
",3.3 Comparative intrinsic evaluation,[0],[0]
Query inventory.,3.3 Comparative intrinsic evaluation,[0],[0]
"We compiled a diverse inventory of 100 query words that balance frequency, part of speech (POS), and concreteness.",3.3 Comparative intrinsic evaluation,[1.0],"['We compiled a diverse inventory of 100 query words that balance frequency, part of speech (POS), and concreteness.']"
"First, we selected 10 out of 45 broad categories from WordNet (Miller, 1995).",3.3 Comparative intrinsic evaluation,[0],[0]
We then chose an equal number of categories that mostly contained abstract concepts and categories that referred to concrete concepts.,3.3 Comparative intrinsic evaluation,[0],[0]
"Among those categories, we had one for adjectives and adverbs each, and four for nouns and verbs each.",3.3 Comparative intrinsic evaluation,[0],[0]
"From each category, we drew ten random words with the restriction that there be exactly three rare words (i.e., occurring fewer than 2500 times in the training corpus) among the ten.
Details.",3.3 Comparative intrinsic evaluation,[0],[0]
"Our experiments were performed with users from Amazon Mechanical Turk (MTurk) that were native speakers of English with sufficient experience and positive feedback on the Amazon Mechanical Turk framework.
",3.3 Comparative intrinsic evaluation,[0.9999999625715],['Our experiments were performed with users from Amazon Mechanical Turk (MTurk) that were native speakers of English with sufficient experience and positive feedback on the Amazon Mechanical Turk framework.']
"For each of the 100 query words in the dataset, the nearest neighbors at ranks k ∈",3.3 Comparative intrinsic evaluation,[0],[0]
"{1, 5, 50} for the six embeddings were retrieved.",3.3 Comparative intrinsic evaluation,[0],[0]
"For each query word and k, we presented the six words along with the query word to the users.",3.3 Comparative intrinsic evaluation,[0],[0]
"Each Turker was requested to evaluate between 25 and 50 items per task, where an item corresponds to the query word and the set of 6 retrieved neighbor words from each of the 6 embeddings.",3.3 Comparative intrinsic evaluation,[1.0],"['Each Turker was requested to evaluate between 25 and 50 items per task, where an item corresponds to the query word and the set of 6 retrieved neighbor words from each of the 6 embeddings.']"
The payment was between $0.01 and $0.02 per item.,3.3 Comparative intrinsic evaluation,[0],[0]
The users were then asked to pick the word that is most similar according to their perception (the instructions were almost identical to the WordSim-353 dataset instructions).,3.3 Comparative intrinsic evaluation,[0],[0]
"Duplicate words were consolidated, and a click was counted for all embeddings that returned that word.",3.3 Comparative intrinsic evaluation,[0],[0]
"An option “I don’t know the meaning of one (or several) of the words” was also
provided as an alternative.",3.3 Comparative intrinsic evaluation,[0],[0]
"Table 2 shows an example instance that was given to the Turkers.
",3.3 Comparative intrinsic evaluation,[0],[0]
"The combination of 100 query words and 3 ranks yielded 300 items on which we solicited judgements by a median of 7 Turkers (min=5, max=14).",3.3 Comparative intrinsic evaluation,[0],[0]
"We compare embeddings by average win ratio, where the win ratio was how many times raters chose embedding e divided by the number of total ratings for item i.",3.3 Comparative intrinsic evaluation,[1.0],"['We compare embeddings by average win ratio, where the win ratio was how many times raters chose embedding e divided by the number of total ratings for item i.']"
Overall comparative results replicate previous results.,3.4 Comparative results,[0],[0]
Figure 1(a) shows normalized win ratio scores for each embedding across 3 conditions corresponding to the frequency of the query word in the training corpus.,3.4 Comparative results,[1.0],['Figure 1(a) shows normalized win ratio scores for each embedding across 3 conditions corresponding to the frequency of the query word in the training corpus.']
The scores were normalized to sum to one in each condition to emphasize relative differences.,3.4 Comparative results,[0],[0]
CBOW in general performed the best and random projection the worst (p-value < 0.05 for all pairs except H-PCA and C&W in comparing un-normalized score differences for the ALL-FREQ condition with a randomized permutation test).,3.4 Comparative results,[1.0],['CBOW in general performed the best and random projection the worst (p-value < 0.05 for all pairs except H-PCA and C&W in comparing un-normalized score differences for the ALL-FREQ condition with a randomized permutation test).']
"The novel comparative evaluations correspond both in rank and in relative margins to those shown in Table 1.
",3.4 Comparative results,[1.0000000885848197],['The novel comparative evaluations correspond both in rank and in relative margins to those shown in Table 1.']
"Unlike previous results, we can now show differences beyond the nearest neighbors.",3.4 Comparative results,[0],[0]
"Figure 1(b) presents the same results, but this time
broken up by the rank k of the neighbors that were compared.",3.4 Comparative results,[0],[0]
CBOW has its strengths especially at rank k = 1.,3.4 Comparative results,[0],[0]
"For neighbors that appear after that, CBOW does not necessarily produce better embeddings.",3.4 Comparative results,[1.0],"['For neighbors that appear after that, CBOW does not necessarily produce better embeddings.']"
"In fact, it even does worse for k = 50 than GloVe.",3.4 Comparative results,[0],[0]
"It is important to note, however, that we cannot make absolute statements about how performance behaves across different values of k since each assessment is always relative to the quality of all other embeddings.
",3.4 Comparative results,[0],[0]
We balanced our query inventory also with respect to parts of speech and abstractness vs. concreteness.,3.4 Comparative results,[0],[0]
"Figure 1(c) shows the relative performances of all embeddings for the four POS classes (adjectives, adverbs, nouns and verbs).",3.4 Comparative results,[0],[0]
"While most embeddings show relatively homogeneous behaviour across the four classes, GloVe suffers disproportionally on adverbs.",3.4 Comparative results,[0],[0]
"Moving on to Figure 1(d), we can see a similar behavior for TSCCA:",3.4 Comparative results,[0],[0]
Its performance is much lower on concrete words than on abstract ones.,3.4 Comparative results,[0],[0]
"This difference may be important, as recent related work finds that simply differentiating between general and specific terms explains much of the observed
variation between embedding methods in hierarchical classification tasks (Levy et al., 2015b).",3.4 Comparative results,[0],[0]
"We take the two observations above as evidence that a more fine-grained analysis is necessary in discerning different embedding methods.
",3.4 Comparative results,[0],[0]
"As a by-product, we observed that there was no embedding method that consistently performed best on all of the four different absolute evaluation tasks.",3.4 Comparative results,[0],[0]
"However, we would like to reiterate that our goal is not to identify one best method, but rather point out that different evaluations (e.g., changing the rank k of the nearest neighbors in the comparison task) result in different outcomes.",3.4 Comparative results,[0],[0]
In the relatedness task we measure whether a pair of semantically similar words are near each other in the embedding space.,4 Coherence,[1.0],['In the relatedness task we measure whether a pair of semantically similar words are near each other in the embedding space.']
In this novel coherence task we assess whether groups of words in a small neighborhood in the embedding space are mutually related.,4 Coherence,[1.0],['In this novel coherence task we assess whether groups of words in a small neighborhood in the embedding space are mutually related.']
"Previous work has used this property for qualitative evaluation using visualizations of 2D projections (Turian et al., 2010), but we are not aware of any work using local neighborhoods for
quantitative evaluation.",4 Coherence,[0],[0]
"Good embeddings should have coherent neighborhoods for each word, so inserting a word not belonging to this neighborhood should be easy to spot.",4 Coherence,[1.0],"['Good embeddings should have coherent neighborhoods for each word, so inserting a word not belonging to this neighborhood should be easy to spot.']"
"Similar to Chang et al. (2009), we presented Turkers with four words, three of which are close neighbors and one of which is an “intruder.”",4 Coherence,[0],[0]
"For each of the 100 words in our query set of Section 3.3, we retrieved the two nearest neighbors.",4 Coherence,[0],[0]
These words along with the query word defined the set of (supposedly) good options.,4 Coherence,[0],[0]
"Table 3 shows an example instance that was given to the Turkers.
",4 Coherence,[0],[0]
"To normalize for frequency-based effects, we computed the average frequency avg of the three words in this set and chose the intruder word to be the first word that had a frequency of avg ± 500 starting at rank 100 of the list of nearest neighbors.
Results.",4 Coherence,[0],[0]
"In total, we solicited judgments on 600 items (100 query words for each of the 6 embeddings) from a median of 7 Turkers (min=4, max=11) per item, where each Turker evaluated between 25 and 50 items per task.",4 Coherence,[0],[0]
Figure 2 shows the results of the intrusion experiment.,4 Coherence,[0],[0]
"The evaluation measure is micro-averaged precision for an embedding across 100 query words, where peritem precision is defined as the number of raters that discovered the intruder divided the total number of raters of item i. Random guessing would achieve an average precision of 0.25.
",4 Coherence,[0],[0]
"All embeddings perform better than guessing, indicating that there is at least some coherent structure captured in all of them.",4 Coherence,[0],[0]
"However, the best performing embeddings at this task are TSCCA, CBOW and GloVe (the precision mean differences were not significant under a random permutation test), while TSCCA attains greater precision (p < 0.05) in relation to C&W, H-PCA and random projection embeddings.",4 Coherence,[1.0],"['However, the best performing embeddings at this task are TSCCA, CBOW and GloVe (the precision mean differences were not significant under a random permutation test), while TSCCA attains greater precision (p < 0.05) in relation to C&W, H-PCA and random projection embeddings.']"
"These results are in contrast to the direct comparison study, where the performance of TSCCA was found to be significantly worse than that of CBOW.",4 Coherence,[0],[0]
"However, the order of the last three embeddings remains unchanged, implying that performance on the intrusion task and performance on the direct compari-
son task are correlated.",4 Coherence,[0],[0]
"CBOW and C&W seem to do equally well on rare and frequent words, whereas the other models’ performance suffers on rare words.
",4 Coherence,[0],[0]
Discussion.,4 Coherence,[0],[0]
Evaluation of set-based properties of embeddings may produce different results from item-based evaluation: rankings we got from the intrusion task did not match the rankings we obtained from the relatedness task.,4 Coherence,[1.0],['Evaluation of set-based properties of embeddings may produce different results from item-based evaluation: rankings we got from the intrusion task did not match the rankings we obtained from the relatedness task.']
"Pairwise similarities seem to be only part of the information that is encoded in word embeddings, so looking at more global measures is necessary for a better understanding of differences between embeddings.
",4 Coherence,[0],[0]
"We choose intruder words based on similar but lower-ranked words, so an embedding could score well on this task by doing an unusually bad job at returning less-closely related words.",4 Coherence,[0],[0]
"However, the results in Figure 1(b) suggest that there is little differences at higher ranks (rank 50) between embeddings.",4 Coherence,[1.0],"['However, the results in Figure 1(b) suggest that there is little differences at higher ranks (rank 50) between embeddings.']"
Extrinsic evaluations measure the contribution of a word embedding model to a specific task.,5 Extrinsic Tasks,[1.0],['Extrinsic evaluations measure the contribution of a word embedding model to a specific task.']
"There is an implicit assumption in the use of such evaluations that there is a consistent, global ranking of word embedding quality, and that higher quality embeddings will necessarily improve results on any downstream task.",5 Extrinsic Tasks,[0],[0]
We find that this assumption does not hold: different tasks favor different embeddings.,5 Extrinsic Tasks,[0],[0]
"Although these evaluations are useful in characterizing the relative strengths of different models, we do not recommend that they be used as a proxy for a general notion of embedding quality.
",5 Extrinsic Tasks,[0],[0]
Noun phrase chunking.,5 Extrinsic Tasks,[0],[0]
First we use a noun phrase chunking task similar to that used by Turian et al. (2010).,5 Extrinsic Tasks,[0],[0]
"The only difference is that we normalize all word vectors to unit length, rather than scaling them with some custom factor, before giving them to the conditional random field (CRF) model as input.",5 Extrinsic Tasks,[0],[0]
"We expect that this task will be more sensitive to syntactic information than to semantic information.
Sentiment classification.",5 Extrinsic Tasks,[0],[0]
Second we use a recently released dataset for binary sentiment classification by Maas et al. (2011).,5 Extrinsic Tasks,[0],[0]
The dataset contains 50K movie reviews with a balanced distribution of binary polarity labels.,5 Extrinsic Tasks,[0],[0]
We evaluate the relative performance of word embeddings at this task as follows: we generate embedding-only features for each review by computing a linear combination of word embeddings weighted by the number of times that the word appeared in the review (using the same bag-of-words features as Maas et al. (2011)).,5 Extrinsic Tasks,[0],[0]
"A LIBLINEAR logistic regression model (Fan et al., 2008) with the default parameters is trained and evaluated using 10 fold crossvalidation.",5 Extrinsic Tasks,[0],[0]
"A vanilla bag of words feature set is
the baseline (denoted as BOW here).",5 Extrinsic Tasks,[0],[0]
"We expect that this task will be more sensitive to semantic information than syntactic information.
",5 Extrinsic Tasks,[0],[0]
Results.,5 Extrinsic Tasks,[0],[0]
Table 4 shows the average F1-scores for the chunking task.,5 Extrinsic Tasks,[0],[0]
"The p-values were computed using randomization (Yeh, 2000) on the sentence level.",5 Extrinsic Tasks,[0],[0]
"First, we can observe that adding word vectors as features results in performance lifts with all embeddings when compared to the baseline.",5 Extrinsic Tasks,[1.0],"['First, we can observe that adding word vectors as features results in performance lifts with all embeddings when compared to the baseline.']"
"The performance of C&W and TSCCA is statistically not significant, and C&W does better than all the remaining methods at the p = 0.05 level.",5 Extrinsic Tasks,[0],[0]
"Surprisingly, although the performance of Random Projections is still last, the gap to GloVe and CBOW is now very small.",5 Extrinsic Tasks,[0],[0]
Table 5 shows results on the sentiment analysis task.,5 Extrinsic Tasks,[0],[0]
"We recover a similar order of embeddings as in the absolute intrinsic evaluation, however, the order of TSCCA and GloVe is now reversed.
Discussion.",5 Extrinsic Tasks,[0],[0]
"Performance on downstream tasks is not consistent across tasks, and may not be consistent with intrinsic evaluations.",5 Extrinsic Tasks,[1.0],"['Performance on downstream tasks is not consistent across tasks, and may not be consistent with intrinsic evaluations.']"
"Comparing performance across tasks may provide insight into the information encoded by an embedding, but we should not expect any specific task to act as a proxy for abstract quality.",5 Extrinsic Tasks,[0],[0]
"Furthermore, if good downstream performance is really the goal of an embedding, we recommend that embeddings be trained specifically to optimize a specific objective (Lebret and Collobert, 2014).",5 Extrinsic Tasks,[0],[0]
"We find consistent differences between word embeddings, despite the fact that they are operating on the same input data and optimizing arguably very similar objective functions (Pennington et al., 2014; Levy and Goldberg, 2014).",6 Discussion,[0],[0]
"Recent work suggests that many apparent performance differences on specific tasks are due to a lack of hyperparameter optimization (Levy et al., 2015a).",6 Discussion,[0],[0]
"Different algorithms are, in fact, encoding surprisingly different information that may or may not align with our desired use cases.",6 Discussion,[0],[0]
"For example, we find that embeddings encode differing degrees of information about word frequency, even after length normalization.",6 Discussion,[0],[0]
This result is surprising for two reasons.,6 Discussion,[0],[0]
"First, many algorithms reserve distinct “intercept” parameters to absorb frequencybased effects.",6 Discussion,[0],[0]
"Second, we expect that the geometry of the embedding space will be primar-
ily driven by semantics: the relatively small number of frequent words should be evenly distributed through the space, while large numbers of rare, specific words should cluster around related, but more frequent, words.
",6 Discussion,[0],[0]
We trained a logistic regression model to predict word frequency categories based on word vectors.,6 Discussion,[0],[0]
"The linear classifier was trained to put words either in a frequent or rare category, with thresholds varying from 100 to 50,000.",6 Discussion,[0],[0]
"At each threshold frequency, we sampled the training sets to ensure a consistent balance of the label distribution across all frequencies.",6 Discussion,[0],[0]
"We used length-normalized embeddings, as rare words might have shorter vectors resulting from fewer updates during training (Turian et al., 2010).",6 Discussion,[0],[0]
"We report the mean accuracy and standard deviation (1σ) using five-fold crossvalidation at each threshold frequency in Figure 3.
",6 Discussion,[0],[0]
"All word embeddings do better than random, suggesting that they contain some frequency information.",6 Discussion,[0],[0]
GloVe and TSCCA achieve nearly 100% accuracy on thresholds up to 1000.,6 Discussion,[0],[0]
"Unlike all other embeddings, accuracy for C&W embeddings increases for larger threshold values.",6 Discussion,[0],[0]
"Further investigation revealed that the weight vector direction changes gradually with the threshold frequency — indicating that frequency seems to be encoded in a smooth way in the embedding space.
",6 Discussion,[0],[0]
"Although GloVe and CBOW are the two best performing embeddings on the intrinsic tasks, they differ vastly in the amount of frequency information they encode.",6 Discussion,[0],[0]
"As a consequence, we can conclude that most of the differences in frequency prediction are not due to intrinsic properties of natural language: it is not the case that frequent words naturally have only frequent neighbors.
",6 Discussion,[0],[0]
Word frequency information in the embedding space also affects cosine similarity.,6 Discussion,[0],[0]
"For each of the words in the WordSim-353 dataset, we queried for the k = 1000 nearest neighbors.",6 Discussion,[0],[0]
We then looked up their frequency ranks in the training corpus and averaged those ranks over all the query words.,6 Discussion,[0],[0]
We found a strong correlation between the frequency of a word and its position in the ranking of nearest neighbors in our experiments.,6 Discussion,[1.0],['We found a strong correlation between the frequency of a word and its position in the ranking of nearest neighbors in our experiments.']
Figure 4 shows a power law relationship for C&W embeddings between a word’s nearest neighbor rank (w.r.t.,6 Discussion,[0],[0]
a query) and the word’s frequency rank in the training corpus (nn-rank ∼ 1000 · corpus-rank0.17).,6 Discussion,[0],[0]
"This is a concern: the frequency of a word in the language plays a critical role in word processing of humans as well (Cattell, 1886).",6 Discussion,[0],[0]
"As a consequence, we need to explicitly consider word frequency as a factor in the experiment design.",6 Discussion,[1.0],"['As a consequence, we need to explicitly consider word frequency as a factor in the experiment design.']"
"Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects.",6 Discussion,[1.0],"['Also, the above results mean that the commonly-used cosine similarity in the embedding space for the intrinsic tasks gets polluted by frequency-based effects.']"
"We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric.",6 Discussion,[1.0],"['We believe that further research should address how to better measure linguistic relationships between words in the embedding space, e.g., by learning a custom metric.']"
Mikolov et al. (2013b) demonstrate that certain linguistic regularities exist in the embedding space.,7 Related work,[0],[0]
"The authors show that by doing simple vector arithmetic in the embedding space, one can solve various syntactic and semantic analogy tasks.",7 Related work,[0],[0]
"This is different to previous work, which phrased the analogy task as a classification problem (Turney, 2008).",7 Related work,[0],[0]
"Surprisingly, word embed-
dings seem to capture even more complex linguistic properties.",7 Related work,[0],[0]
"Chen et al. (2013) show that word embeddings even contain information about regional spellings (UK vs. US), noun gender and sentiment polarity.
",7 Related work,[0],[0]
Previous work in evaluation for word embeddings can be divided into intrinsic and extrinsic evaluations.,7 Related work,[0],[0]
"Intrinsic evaluations measure the quality of word vectors by directly measuring correlation between semantic relatedness and geometric relatedness, usually through inventories of query terms.",7 Related work,[0],[0]
"Focusing on intrinsic measures, Baroni et al. (2014) compare word embeddings against distributional word vectors on a variety of query inventories and tasks.",7 Related work,[0],[0]
Faruqui and Dyer (2014) provide a website that allows the automatic evaluation of embeddings on a number of query inventories.,7 Related work,[0],[0]
Gao et al. (2014) publish an improved query inventory for the analogical reasoning task.,7 Related work,[0],[0]
"Finally, Tsvetkov et al. (2015) propose a new intrinsic measure that better correlates with extrinsic performance.",7 Related work,[0],[0]
"However, all these evaluations are done on precollected inventories and mostly limited to local metrics like relatedness.
",7 Related work,[0],[0]
"Extrinsic evaluations use embeddings as features in models for other tasks, such as semantic role labeling or part-of-speech tagging (Collobert et al., 2011), and improve the performance of existing systems (Turian et al., 2010).",7 Related work,[0],[0]
"However, they have been less successful at other tasks such as parsing (Andreas and Klein, 2014).
",7 Related work,[0],[0]
More work has been done in unsupervised semantic modeling in the context of topic models.,7 Related work,[0],[0]
"One example is the word intrusion task (Chang et al., 2009), in which annotators are asked to identify a random word inserted into the set of high probability words for a given topic.",7 Related work,[0],[0]
"Word embeddings do not produce interpretable dimensions, so we cannot directly use this method, but we present a related task based on nearest neighbors.",7 Related work,[0],[0]
"Manual evaluation is expensive and time-consuming, but other work establishes that automated evaluations can closely model human intuitions (Newman et al., 2010).",7 Related work,[0],[0]
There are many factors that affect word embedding quality.,8 Conclusions,[1.0],['There are many factors that affect word embedding quality.']
"Standard aggregate evaluations, while useful, do not present a complete or consistent picture.",8 Conclusions,[0],[0]
"Factors such as word frequency play a significant and previously unacknowledged
role.",8 Conclusions,[1.000000026055283],['Factors such as word frequency play a significant and previously unacknowledged role.']
Word frequency also interferes with the commonly-used cosine similarity measure.,8 Conclusions,[0],[0]
"We present a novel evaluation framework based on direct comparisons between embeddings that provides more fine-grained analysis and supports simple, crowdsourced relevance judgments.",8 Conclusions,[1.0],"['We present a novel evaluation framework based on direct comparisons between embeddings that provides more fine-grained analysis and supports simple, crowdsourced relevance judgments.']"
We also present a novel Coherence task that measures our intuition that neighborhoods in the embedding space should be semantically or syntactically related.,8 Conclusions,[1.0],['We also present a novel Coherence task that measures our intuition that neighborhoods in the embedding space should be semantically or syntactically related.']
"We find that extrinsic evaluations, although useful for highlighting specific aspects of embedding performance, should not be used as a proxy for generic quality.",8 Conclusions,[1.0],"['We find that extrinsic evaluations, although useful for highlighting specific aspects of embedding performance, should not be used as a proxy for generic quality.']"
This research was funded in part through NSF Award IIS-1513692.,Acknowledgments,[0],[0]
"We would like to thank Alexandra Schofield, Adith Swaminathan and all other members of the NLP seminar for their helpful feedback.",Acknowledgments,[0],[0]
We present a comprehensive study of evaluation methods for unsupervised embedding techniques that obtain meaningful representations of words from text.,abstractText,[0],[0]
"Different evaluations result in different orderings of embedding methods, calling into question the common assumption that there is one single optimal vector representation.",abstractText,[0],[0]
We present new evaluation techniques that directly compare embeddings with respect to specific queries.,abstractText,[0],[0]
"These methods reduce bias, provide greater insight, and allow us to solicit data-driven relevance judgments rapidly and accurately through crowdsourcing.",abstractText,[0],[0]
Evaluation methods for unsupervised word embeddings,title,[0],[0]
