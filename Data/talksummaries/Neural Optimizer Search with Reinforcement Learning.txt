0	13	The choice of the right optimization method plays a major role in the success of training deep learning models.
2	19	Designing optimization methods for deep learning, however, is very challenging due to the non-convex nature of the optimization problems.
3	14	In this paper, we consider an approach to automate the process of designing update rules for optimization methods, especially for deep learning architectures.
4	18	The key insight is to use a controller in the form of a recurrent network to generate an update equation for the optimizer.
5	23	The recurrent network controller is trained with reinforcement learning to maximize the accuracy of a particular model architecture, being trained for a fixed number of epochs by the update rule, on a held-out validation set.
33	46	In our framework, the controller generates strings corresponding to update rules, which are then applied to a neural network to estimate the update rule’s performance; this performance is then used to update the controller so that the controller can generate improved update rules over time.
34	30	To map strings sampled by the controller to an update rule, we design a domain specific language that relies on a parenthesis-free notation (in contrast to the classic infix notation).
36	28	We therefore express each update rule with a string describing 1) the first operand to select, 2) the second operand to select, 3) the unary function to apply on the first operand, 4) the unary function to apply on the second operand and 5) the binary function to apply to combine the outputs of the unary functions.
37	15	The output of the binary function is then either temporarily stored in our operand bank (so that it can be selected as an operand in subsequent parts of the string) or used as the final weight update as follows: ∆w = λ ∗ b(u1(op1), u2(op2)) where op1, op2, u1(.
66	11	These techniques allow us to run experiments more quickly and efficiently compared to Zoph & Le (2017), with our controller experiments typically converging in less than a day using 100 CPUs, compared to 800 GPUs over several weeks.
71	16	The operands, unary functions and binary functions that are accessible to our controller are as follows (details below): • Operands: g, g2, g3, m̂, v̂, γ̂, sign(g), sign(m̂), 1, small constant noise, 10−4w, 10−3w, 10−2w, 10−1w, ADAM and RMSProp.
72	20	• Unary functions which map input x to: x, −x, ex, log |x|, clip(x, 10−5), clip(x, 10−4), clip(x, 10−3), drop(x, 0.1), drop(x, 0.3), drop(x, 0.5) and sign(x).
73	18	• Binary functions which map (x, y) to x+y (addition), x − y (subtraction), x ∗ y (multiplication), xy+ (division) or x (keep left).
76	15	In our experiments, we use binary trees with depths of 1, 2 and 3 which correspond to strings of length 5, 10 and 15 respectively.
77	10	The above list of operands, unary functions and binary function is quite large, so to address this issue, we find it helpful to only work with subsets of the operands and functions presented above.
79	10	We also experiment with several constraints when sampling an update rule, such as forcing the left and right operands to be different at each iteration, and not using addition as the final binary function.
80	15	An additional constraint added is to force the controller to reuse one of the previously computed operands in the subsequent iterations.
81	36	The constraints are implemented by manually setting the logits corresponding to the forbidden operands or functions to −∞.
83	66	The controller is a single-layer LSTM with hidden state size 150 and weights are initialized uniformly at random between -0.08 and 0.08.
86	32	The child network architecture that all sampled optimizers are run on is a small two layer 3x3 ConvNet.
90	13	Once a worker receives an optimizer to run from the controller, it performs a basic hyperparameter sweep on the learning rate: 10i with i ranging from -5 to 1, with each learning rate being tried for 1 epoch of 10,000 CIFAR-10 training examples.
91	43	The best learning rate after 1 epoch is then used to train our child network for 5 epochs and the final validation accuracy is reported as a reward to the controller.
97	14	Preliminary experiments indicate that the update rules are robust to slight changes in the hyperparameters they were searched over.
98	19	Our results show that the controller discovers many different updates that perform well during training and the maximum accuracy also increases over time.
99	16	In Figure 4, we show the learning curve of the controller as more optimizers are sampled.
107	16	In the following experiments, we will exercise some of the update equations found in the previous experiment on different network architectures and tasks.
108	21	The controller is not trained again, and the update rules are simply reused.
109	126	We first test one of the optimizers we found in the previous experiment on the famous Rosenbrock function against the commonly used deep learning optimizers in TensorFlow (Abadi et al., 2016): Adam, SGD, RMSProp and Momentum, and tune the value of in Adam in a log scale between 10−3 and 10−8.
114	113	Our controller finds many optimizers that perform well when run for 5 epochs on the small ConvNet.
115	15	To filter optimizers that do well when run for many more epochs, we run dozens of our top optimizers for 300 epochs and aggressively stop optimizers that show less promise.
