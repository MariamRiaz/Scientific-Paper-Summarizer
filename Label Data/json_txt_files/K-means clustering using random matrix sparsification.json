{"sections": [{"heading": "1. Introduction", "text": "K-means clustering is a classical clustering problems and has been studied for several decades. The goal of K-Means clustering is to find a set of k cluster centers for a dataset such that the sum of squared distances of each point to its closest cluster center is minimized. While it is known that k-means clustering is an NP hard optimization problem even for k = 2 (Dasgupta, 2008), in practice a local search heuristic due to Lloyd (Lloyd, 1982) is widely used for solving K-means clustering problem. Lloyd\u2019s iterative\n1Department of Electrical Engineering & Computer Science, Wichita State University, KS, USA. Correspondence to: Kaushik Sinha <kaushik.sinha@wichita.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nalgorithm begins with k arbitrary \u201ccluster centers\u201d, and in each iteration, each point is assigned to the nearest cluster center, and each cluster center is recomputed as the center of mass of all points assigned to it. These last two steps are repeated until the process stabilizes. Lloyd\u2019s algorithm for k-means clustering is known to be one of the top ten data mining tools of the last fifty years (Wu, 2008).\nK-means clustering is typically performed on a data matrix A \u2208 Rn\u00d7d, consisting of n data points each having d attributes/features and per iteration computational cost of Lloyd\u2019s algorithm is O(nkd). In recent years there has been a series of work towards reducing this computational cost and speeding up k-means clustering computation. Most of these works can broadly be classified into three categories. In the first category, K-means clustering algorithm is accelerated by avoiding unnecessary distance calculations by applying various forms of triangular inequality and by keeping track of lower and upper bounds for distances between points and cluster centers (Elkan, 2003; Hamerly, 2010; Drake & Hamerly, 2012; Ding et al., 2015; Newling & Fleuret, 2016; Bottesch et al., 2016). All these algorithms ensure the exact same clustering result that would have been obtained had one applied Lloyd\u2019s heuristic from the same set of initial cluster centers without applying any distance inequality bounds. In the second category, various dimension reduction techniques are applied to data matrix A to reduce data dimensionality from d to d\u2032 (d\u2032 d), where d\u2032 is independent of n and d, so that optimal k-means clustering solution of dimensionality reduced dataset A\u2032 \u2208 Rn\u00d7d\u2032 ensures an approximately optimal k-means clustering objective function of A. Most prominent among these is the random projection based dimensionality reduction technique that reduces data dimensionality from d to \u2126(k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al., 2010; 2015; Cohen et al., 2015) and also from d to O(log(k)/ 2) resulting in (9 + ) approximation of the optimal k-means objective function (Cohen et al., 2015). Recently, (Liu et al., 2017) demonstrated the the random projection step can be performed by multiplying with a sparse matrix that yields the same (1 + ) approximation guarantee. Additionally, random feature selection method reduces data dimensionality from d to \u2126(k log k/ 2) resulting in (1 + ) approximation of the optimal k-means objective function (Boutsidis et al.,\n2015; Cohen et al., 2015). In the third category, a smaller subset of n data points called coresets, are constructed so that optimal weighted k-means clustering objective function performed on this coreset is (1 + ) approximation of the optimal k-means objective function performed on the original dataset (Feldman & Langberg, 2011; Feldman et al., 2013). In k-means clustering using Lloyd\u2019s heuristic, a major computational bottleneck arises from Euclidean distance computation between each data point and k cluster centers in every iteration. For a data point a \u2208 Rd and a cluster center \u00b5 \u2208 Rd, the Euclidean distance can be represented as, \u2016a \u2212 \u00b5\u20162 = \u2016a\u20162 + \u2016\u00b5\u20162 \u2212 2a>\u00b5. Note that \u2016a\u20162 needs be computed for each data point only once over all iterations of Lloyd\u2019s heuristics (and can be done off-line), \u2016\u00b5\u20162 needs be computed once for each cluster center in each iteration, while the dot product needs to be computed for every possible data point, cluster center pair in every single iteration. In fact, the dot product between a cluster center \u00b5 and all n data points can be computed by a simple matrix-vector multiplication: A\u00b5. If the data matrix A is significantly sparse (i.e., number of non-zero entries is reasonable small) the above matrix vector multiplication can be performed reasonably fast. A key question that is not addressed in the literature is, To what extent the data matrix A can be made sparse without significantly affecting optimal k-means clustering objective? In this paper we show that under mild conditions, we can randomly sparsify data matrix A to obtain a sparse data matrix A\u0303 such that optimal k-means clustering solution of A\u0303 yields an approximately optimal k-means clustering objective of A with high probability. Note that such a sparsification scheme can be extremely useful in practice. If the original data matrix is reasonably dense, then such sparsification results in fast matrix-vector multiplication, thereby speeding up k-means clustering. However, it may seem at first that for many real world high dimensional datasets that are very sparse to begin with, such as text datasets represented in \u201cbag of word\u201d format, such sparsification scheme may not be useful. But, note that instead of directly working with high dimensional data, typically a random projection step is often applied first to reduce data dimensionality since it is known that optimal k-means clustering solution of this randomly projected dataset results in approximately optimal k-means clustering objective of the original high dimensional dataset (Boutsidis et al., 2010; 2015; Cohen et al., 2015; Liu et al., 2017). Unfortunately, such a random projection step results in a dense projected data matrix. Interestingly, our sparsification method can now be applied on this projected dense data matrix to reap further computational benefit in addition to the computational benefit already achieved by random projection step (see Figure 1).\nTo quantify the approximation factor as well as the level of sparsity of our proposed method, we use ideas from\n(Achlioptas & Mcsherry, 2007) which establishes that random matrix sparsification approximately preserves low rank matrix structure with high probability and also ideas from (Cohen et al., 2015) which establishes to what extent an approximately optimal low rank matrix serves as a projection cost preserving sketch. To the best of our knowledge, this is the first result that quantifies how random matrix sparsification affects k-means clustering. In particular, we make the following contributions in this paper.\n\u2022 We show that for any \u2208 (0, 1), a dense data matrix A \u2208 Rn\u00d7d can be randomly sparsified to yield a data matrix A\u0303 \u2208 Rn\u00d7d, such that, A\u0303 contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and optimal k-means clustering solution of A\u0303 results in (1 + ) approximation of optimal k-means objective of A with high probability.\n\u2022 We show that for any \u2208 (0, 1), a dense data matrix A \u2208 Rn\u00d7d can be randomly sparsified to yield a data matrix A\u0303 \u2208 Rn\u00d7d, such that, A\u0303 contains O(nk/ 9 + d log4 n) non-zero entries in expectation, and any approximately optimal k-means clustering solution of A\u0303, having (1 + ) approximation of optimal k-means objective of A\u0303, results in (1 +O( )) approximation of optimal k-means objective of A with high probability.\n\u2022 We present experimental results on three real world datasets to demonstrate effect of our proposed random sparsification scheme on k-means clustering solution.\nThe rest of the paper is organized as follows. In section 2 we present k-means clustering problem in matrix notation and introduce uniform and non-uniform sampling strategies for random matrix sparsification. We propose an algorithm for k-means clustering using random matrix sparsification in section 3 and present its analysis in section 4. Empirical evaluations are presented in section 5. Finally, we conclude and point out a few open questions in section 6."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Notation and linear algebra basics", "text": "We use bold lower case letters to denote vectors and bold upper case letters to denote matrices. For any n and d, consider a matrix A \u2208 Rn\u00d7d with rank r = rank(A). Using singular value decomposition A can be written as A = U\u03a3V>, where U \u2208 Rn\u00d7r contains r left singular vectors u1,u2, . . . ,ur \u2208 Rn, V contains r right singular vectors v1,v2, . . . ,vr \u2208 Rd, and \u03a3 \u2208 Rr\u00d7r is a positive diagonal matrix containing the singular values of A : \u03c31(A) \u2265 \u03c32(A) \u2265 \u00b7 \u00b7 \u00b7 \u2265 \u03c3r(A). A can also be written as A = \u2211r i=1 \u03c3i(A)uiv > i . For any k \u2264 r,\nAk = \u2211k i=1 \u03c3i(A)uiv > i is the best rank k approximation to A for any unitarily invariant norm, including Frobenious\nand spectral norm (Mirsky, 1960). Note that A = Ak + Ar\u2212k where Ar\u2212k = \u2211r i=k+1 \u03c3i(A)uiv > i . Therefore, Ar\u2212k = A\u2212Ak. Square Frobenious norm of A is given by \u2016A\u20162F = \u2211 i,jA(i, j) 2 = trace(AA>) = \u2211 i \u03c3 2 i (A). The spectral norm of A is given by \u2016A\u20162 = \u03c31(A). Ak satisfies \u2016A \u2212Ak\u2016F = minB,rank(B)=k \u2016A \u2212 B\u2016F and \u2016A\u2212Ak\u20162 = minB,rank(B)=k \u2016A\u2212B\u20162."}, {"heading": "2.2. K-means clustering", "text": "The objective of k-means clustering is to partition n data points in Rd, {a1, . . . ,an}, into k non-overlapping clusters C = {C1, . . . , Ck} such that points that are close to each other belong to the same cluster and points that are far from each other belong to to different clusters. Let \u00b5i be the centroid of cluster Ci and for any data point ai, let C(ai) be the index of the cluster to which ai is assigned to. The goal of k-means clustering is to minimize the objective function\nk\u2211 i=1 \u2211 aj\u2208Ci \u2016aj \u2212\u00b5i\u201622 = n\u2211 j=1 \u2016aj \u2212\u00b5C(aj)\u2016 2 2 (1)\nLet A \u2208 Rn\u00d7d be a data matrix containing the n data points {a1, . . . ,an} as rows and for any clustering C, let XC \u2208 Rn\u00d7k be the cluster indicator matrix, with XC(i, j) = 1/ \u221a |Cj | if ai is assigned to Cj and XC(i, j) = 0 otherwise. The k-means objective function given in equation 1 can now be represented in the matrix notation as,\n\u2016A\u2212XCX>CA\u20162F = n\u2211 j=1 \u2016aj \u2212\u00b5C(aj)\u2016 2 2 (2)\nBy construction, the columns of XC have disjoint support and are orthonormal vectors and XCX>C is an orthogonal projection matrix of rank k. Let S be the set of all possible rank k cluster projection matrices of the form XCX>C . The objective of k-means clustering is to find an optimal clustering of A that minimizes the objective function in equation\n2, that is, to find XCopt such that,\nXCopt = argmin XCX>C \u2208S\n\u2016A\u2212XCX>CA\u20162F\nAs mentioned earlier, finding XCopt is an NP-hard problem. Any cluster indicator matrix X\u03b3 is called an \u03b3approximation for the k-means clustering problem (\u03b3 \u2265 1) for data matrix A if it satisfies,\n\u2016A\u2212X\u03b3X>\u03b3A\u20162F \u2264 \u03b3 min XCX>C \u2208S \u2016A\u2212XCX>CA\u20162F\n= \u03b3\u2016A\u2212XCoptX>CoptA\u2016 2 F"}, {"heading": "2.3. Random matrix sparsification", "text": "Given a data matrix A \u2208 Rn\u00d7d, the basic idea of random matrix sparsification is to randomly sparsify the entries of A to get a matrix A\u0303 \u2208 Rn\u00d7d such that A\u0303 contains fewer nonzero entries compared to A. Such a sparse matrix A\u0303 speeds up matrix-vector multiplication by decreasing the number of arithmetic operations. Let us write A\u0303 = A+N, where N \u2208 Rn\u00d7d. A fundamental result of random matrix theory is that, as long as N is a random matrix whose entries are zero mean, independent random variables with bounded variance, no low dimensional subspace accommodates N well, i.e., \u2016Nm\u20162 and \u2016Nm\u2016F are small for small m. In fact, optimal rank m approximation to A\u0303 approximates A nearly as well as Am as long as \u2016Am\u2016 \u2016Nm\u2016 (for both Frobenious and spectral norm) and the quantity \u2016Nm\u2016 bounds the influence that N may exert on the optimal rank m approximation to A\u0303. Next we describe a simple random uniform sampling scheme as well as a simple non-uniform sampling scheme for generating sparse A\u0303 that were proposed in (Achlioptas & Mcsherry, 2007) along with bounds on \u2016Nm\u2016. In the following, we set b to be b = max(i,j) |A(i, j)|."}, {"heading": "2.3.1. UNIFORM SAMPLING SCHEME", "text": "In random matrix sparsificaion using uniform sampling scheme, p fraction of entries of A are set to zero to obtain a sparse A\u0303. In particular,\nA\u0303(i, j) = { A(i, j)/p with probability p 0 otherwise\n(3)\nIt was shown in (Achlioptas & Mcsherry, 2007) that as long as p is bounded from below, with high probability, \u2016Nm\u2016 is bounded as shown below (a simplified version of a result from (Achlioptas & Mcsherry, 2007)).\nTheorem 1. [Theorem 2 of (Achlioptas & Mcsherry, 2007)] For p \u2265 (8 log n)4/n, let A\u0303 be the random sparse matrix obtained by applying uniform sampling scheme (equation 3). Then with probability at least (1 \u2212 1/n19 log3 n), for any m \u2264 min{n, d}, N satisfies \u2016Nm\u20162 \u2264 4b \u221a n/p and\n\u2016Nm\u2016F \u2264 4b \u221a mn/p."}, {"heading": "2.3.2. NON-UNIFORM SAMPLING SCHEME", "text": "Random sparsification using uniform sampling can be improved by retaining entries with probability that depends on their magnitude. For any p > 0 define \u03c4ij = p(A(i, j)/b)2\nand let pij = max { \u03c4ij , \u221a \u03c4ij \u00d7 (8 log n)4/n } . Then a\nsparse A\u0303 can be obtained from A using the following nonuniform sampling scheme.\nA\u0303(i, j) = { A(i, j)/pij with probability pij 0 otherwise (4)\nSuch non-uniform sampling scheme yields greater sparsification when entry magnitudes vary, without increasing error bound of Theorem 1.\nTheorem 2. [Theorem 3 of (Achlioptas & Mcsherry, 2007)] Let A\u0303 be the random sparse matrix obtained by applying non-uniform sampling scheme (equation 4). Then with probability at least (1\u2212 1/n19 log3 n), for any m \u2264 min{n, d}, N satisfies \u2016Nm\u20162 \u2264 4b \u221a n/p and \u2016Nm\u2016F \u2264 4b \u221a mn/p."}, {"heading": "In addition, expected number of non-zero entries in A\u0303 is at", "text": "most p(\u2016A\u2016F /b)2 + d(8 log n)4.\nFor any s > 0, setting p = s(b/\u2016A\u2016F )2 in the above Theorem ensures that expected number of non-zero entries in A\u0303 is at most s+ d(8 log n)4."}, {"heading": "3. An algorithm for k-means clustering using random matrix sparsification", "text": "While the goal of k-means clustering is to well approximate each row of A with its cluster center, as can be seen from equation 1, an equivalent formulation in equation 2 shows that the problem actually amounts to finding an optimal rank\nAlgorithm 1 K-means clustering using random sparsification Input : Data matrix A \u2208 Rn\u00d7d, number of clusters k, a positive scalar p and a \u03b3-approximation k-means algorithm. Output : Cluster indicator matrix X\u03b3\u0303 determining a k partition of the rows of A.\n1: Compute A\u0303 using non-uniform sampling scheme (equation 4). 2: Run the \u03b3-approximation k-means algorithm on A\u0303 to obtain X\u03b3\u0303 . 3: Return X\u03b3\u0303 .\nk subspace for approximating the columns of A. Moreover, the choice of subspace is constrained because it must be spanned by the columns of a cluster indicator matrix. The random sampling schemes presented in the previous section yields a sparse A\u0303 whose optimal rank m approximation A\u0303m approximates Am reasonably well for small m. For appropriate choice of m, if such Am approximates optimal rank k subspace for approximating the columns of A well, then a reasonable strategy for k-means clustering that will reduce number of arithmetic operations is to perform kmeans clustering on A\u0303, instead of A, and hope that optimal k-means clustering solution of A\u0303 will be close to optimal kmeans clustering solution of A. We propose such a strategy in Algorithm 1. In the next section we present an analysis of this algorithm and prove that an optimal k-means clustering solution of A\u0303 indeed results in an approximately optimal k-means objective of A."}, {"heading": "4. Analysis of algorithm", "text": "In this section we present an analysis of Algorithm 1. For all our results we have assumed that n \u2265 d. The main intuition for the technical part of the proof is that even though A and A\u0303 look very different because of the enforced sparse structure, if their appropriate low-rank structures are similar, that is enough to argue that optimal k-means solution of A\u0303 is close to optimal k-means solution of A. We use the notion of projection cost preserving sketch1 as a useful mathematical object for our proof. If B is a rank k projection-cost preserving sketch for A with error 1, then it implies (can be easily shown) that optimal k-means solution of B is (1 + 1) optimal k-means solution of A (Cohen et al., 2015). It turns out that for appropriate choice of m = m(k, 1), the best rankm approximation of A, namely Am, constructed by the m largest SVD structure form a rank k projection-cost preserving sketch for A with error 1.\n1B \u2208 Rn\u00d7d \u2032\nis a rank k projection-cost preserving sketch of A \u2208 Rn\u00d7d, with error 0 \u2264 \u2264 1 if, for all rank k orthogonal projection matrices P \u2208 Rn\u00d7n it holds that (1\u2212 )\u2016A\u2212PA\u20162F \u2264 \u2016B \u2212 PB\u20162F + c \u2264 (1 + )\u2016A \u2212 PA\u20162F for some fixed nonnegative constant c that may depend on A and B but is independent of P.\nSimilarly, A\u0303m is a rank k projection-cost preserving sketch for A\u0303. We show that optimal k-means solution of A\u0303 is close to optimal k-mean solution of A in two steps.\n1. First, we show the reverse direction of the implication of rank k projection-cost preserving sketch also holds. In particular, we show that an optimal k-means solution of A\u0303 is also (1 +O( 1)) optimal for A\u0303m.\n2. Then we show that A\u0303m is also rank k projection-cost preserving sketch for A with a different error 2. (this is where we quantify amount of sparsity to k-means error)\nCombining these two facts and properly choosing 1 and 2, we conclude that optimal k-means solution of A\u0303 is (1 + ) optimal k-means solution of A. We lay out the necessary details in the following subsections."}, {"heading": "4.1. Relation between clustering objective functions of", "text": "A\u0303 and A\u0303m\nWe first show that close to optimal cluster indicator matrix obtained by solving k-means clustering problem on A\u0303 yields a close to optimal k-means objective of A\u0303m.\nLemma 1. For any 0 < \u2264 1/2, let m = dk/ e. For any A\u0303 \u2208 Rn\u00d7d with rank r \u2265 dk/ e + k, let A\u0303m be its best rank m approximation. For any set S of rank k cluster projection matrices, let P\u0303\u2217 = argminP\u2208S \u2016A\u0303 \u2212 PA\u0303\u20162F and P\u0303\u2217m = argminP\u2208S \u2016A\u0303m\u2212PA\u0303m\u20162F . For any \u03b3 \u2265 1, if \u2016A\u0303\u2212 P\u0302A\u0303\u20162F \u2264 \u03b3\u2016A\u0303\u2212 P\u0303\u2217A\u0303\u20162F , then, \u2016A\u0303m\u2212 P\u0302A\u0303m\u20162F \u2264 \u03b3\u2016A\u0303m \u2212 P\u0303\u2217mA\u0303m\u20162F + (\u03b3 \u2212 1)\u2016A\u0303 \u2212 A\u0303m\u20162F + \u2016P\u0302(A\u0303 \u2212 A\u0303m)\u20162F . In particular, the following holds.\n(i) If \u03b3 = 1, then, \u2016A\u0303m \u2212 P\u0303\u2217A\u0303m\u20162F \u2264 (1 + 2 )\u2016A\u0303m \u2212 P\u0303\u2217mA\u0303m\u20162. (ii) If \u03b3 = 1 + 1, for any 0 < 1 < 1 satisfying 1 \u2211r i=m+1 \u03c3 2 i (A\u0303) \u2264 \u2211m+k i=m+1 \u03c3 2 i (A\u0303), then, \u2016A\u0303m \u2212 P\u0302A\u0303m\u20162F \u2264 (1 + 1 + 4 )\u2016A\u0303m \u2212 P\u0303\u2217mA\u0303m\u20162.\nProof of the above lemma, which follows from repeated applications of linear algebra basics, choice of m, definition of P\u0302 and optimality of P\u0303\u2217, is long and technical and is differed to the supplementary material for better readability. Note that on the left hand side of the first inequality above (for \u03b3 = 1), we have used P\u0303\u2217 instead of P\u0302 since P\u0302 and P\u0303\u2217 are identical for \u03b3 = 1."}, {"heading": "4.2. Relation between clustering objective functions of A\u0303m and A", "text": "Now we show that optimal cluster indicator matrix obtained by solving k-means clustering problem on A\u0303m results in close to optimal k-means objective of A. Let P\u2217 =\nargminP\u2208S \u2016A \u2212 PA\u20162F and P\u0303\u2217m = argminP\u2208S \u2016A\u0303m \u2212 PA\u0303m\u20162F . Our goal is to show that \u2016A\u2212P\u0303\u2217mA\u20162F is close to \u2016A\u2212P\u2217A\u20162F . In fact, we prove a stronger result showing that A\u0303m is a rank k projection cost preserving sketch2 of A. We do this in multiple steps. First we show that for small k, A\u0303m is approximately best rank m subspace of A. Next, we show that such an A\u0303m is a rank k projection cost preserving sketch for A, which in turn ensures the required guarantee.\nWe start with the following lemma which is a consequence of Theorem 2. Lemma 2. Fix any m \u2265 1 and let 0 < 2 < 1/ \u221a m. Let A\u0303 be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = 16nb 2\n22\u2016A\u20162F . Then A\u0303 contains O ( n 22 + d(log n)4 ) non-zero entries in expectation and with probability at least (1\u2212 1/n19 log3 n),\n\u2016A\u2212 A\u0303m\u2016F \u2264 \u2016A\u2212Am\u2016F + 3 \u221a 2m 1/4\u2016A\u2016F\nWe tailor the above result to show that under mild conditions A\u0303m approximates Am reasonably well.\nLemma 3. Fix any 3, where 0 < 3 < 1. Let rank of A be \u03c1. Fix any k that satisfies \u2211k/ 3 i=1 \u03c3 2 i (A) \u2264 12 \u2211\u03c1 i=1 \u03c3 2 i (A), and let m = dk/ 3e. Let A\u0303 be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n93\u2016A\u20162F\n) . Then A\u0303 contains\nO ( nk 93 + d(log n)4 ) non-zero entries in expectation and with probability at least (1\u2212 1/n19 log3 n),\n\u2016A\u2212 A\u0303m\u2016F \u2264 (1 + 23)\u2016A\u2212Am\u2016F\nNext, we use a result from (Cohen et al., 2015) to show that if A\u0303m is close to best rank approximation of A, then A\u0303m is rank k projection cost preserving sketch for A.\nTheorem 3. [Theorem 9 of (Cohen et al., 2015)] Let m = dk/ 3e. For any A \u2208 Rn\u00d7d, 0 \u2264 4 \u2264 1 and any B \u2208 Rn\u00d7d with rank(B) = m satisfying \u2016A \u2212 B\u20162F \u2264 (1 + 24)\u2016A\u2212Am\u20162F , the sketch B is a projection cost preserving sketch for A. Specifically, for all rank k orthogonal projections P,\n(1\u2212 2 4)\u2016A\u2212PA\u20162F \u2264 \u2016B\u2212PB\u20162F + c \u2264 (1 + 2 3 + 5 4)\u2016A\u2212PA\u20162F\nwhere c is a non-negative scalar.\nFrom Lemma 3 we see that with high probability, \u2016A \u2212 A\u0303m\u20162F \u2264 (1 + 2 23 + 43)\u2016A \u2212Am\u20162F = (1 + 3 23)\u2016A \u2212\n2If B is a rank k projection cost preserving sketch of A, then optimal k-means clustering solution of B results in approximately optimal k-means clustering objective of A (Cohen et al., 2015).\nAm\u20162F . Setting 4 = \u221a\n3 3 and B = A\u0303m, it follows from Theorem 3 that A\u0303m is a rank k projection cost preserving sketch of A."}, {"heading": "4.3. Main result", "text": "We combine these results from previous two subsections to present the main result of this paper. Theorem 4. Fix any , where 0 < < 1/4. Let rank of A be \u03c1. For any k that satisfies \u2211k/ i=1 \u03c3 2 i (A) \u2264 1 2 \u2211\u03c1 i=1 \u03c3 2 i (A), and let m = dk/ e. Let A\u0303 be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n9\u2016A\u20162F\n) . For any\nset S of rank k cluster projection matrices, let P\u2217 = argminP\u2208S \u2016A\u2212PA\u20162F , P\u0303\u2217 = argminP\u2208S \u2016A\u0303\u2212PA\u0303\u20162F and P\u0303\u2217m = argminP\u2208S \u2016A\u0303m \u2212 PA\u0303m\u20162F . For any \u03b3 \u2265 1, if \u2016A\u0303 \u2212 P\u0302A\u0303\u20162F \u2264 \u03b3\u2016A\u0303 \u2212 P\u0303\u2217A\u0303\u20162F , then A\u0303 contains O ( nk 9 + d(log n) 4 )\nnon-zero entries in expectation and with probability at least (1 \u2212 1/n19 log3 n) the following holds,\n(i) If \u03b3 = 1, then \u2016A\u2212P\u0302A\u20162F \u2264 (1+2 )(1+11 ) 1\u22124 \u2016A\u2212P \u2217A\u20162\n(ii) If \u03b3 = 1 + 1, for any 0 < 1 < 1 satisfying 1 \u2211r i=m+1 \u03c3 2 i (A\u0303) \u2264 \u2211m+k i=m+1 \u03c3 2 i (A\u0303), then \u2016A \u2212 P\u0302A\u20162F \u2264 (1+ 1+4 )(1+11 ) 1\u22124 \u2016A\u2212P \u2217A\u20162\nProof. Set 3 = . Then from Lemma 3 we get, \u2016A \u2212 A\u0303m\u20162F \u2264 (1 + 3 2)\u2016A \u2212Am\u20162F . Now setting B = A\u0303m and 4 = \u221a 3 in Theorem 3, for any rank k orthogonal projection P we get, (1\u2212 2 \u221a\n3 )\u2016A\u2212PA\u20162F \u2264 \u2016A\u0303m \u2212PA\u0303m\u20162F + c \u2264 (1 + 2 + 5 \u221a 3 )\u2016A\u2212PA\u20162F\nor simplifying,\n(1\u2212 4 )\u2016A\u2212PA\u20162F \u2264 \u2016A\u0303m \u2212PA\u0303m\u20162F + c \u2264 (1 + 11 )\u2016A\u2212PA\u20162F (5)\nLet \u03b31 = (1 + 2 ) if \u03b3 = 1 and \u03b31 = (1 + 1 + 4 ) if \u03b3 \u2265 1. Then from lemma 1 we get \u2016A\u0303m \u2212 P\u0302A\u0303m\u20162F \u2264 \u03b31\u2016A\u0303m \u2212 P\u0303\u2217mA\u0303m\u20162. Using this result and repeated application of Equation 5 we get,\n\u2016A\u2212 P\u0302A\u20162F\n\u2264 1 1\u2212 4\n{ \u2016A\u0303m \u2212 P\u0302A\u0303m\u20162F + c } \u2264 1\n1\u2212 4\n{ \u03b31\u2016A\u0303m \u2212 P\u0303\u2217mA\u0303m\u20162F + c } \u2264 1\n1\u2212 4\n{ \u03b31\u2016A\u0303m \u2212P\u2217A\u0303m\u20162F + c } \u2264 1\n1\u2212 4 { \u03b31 [ (1 + 11 )\u2016A\u2212P\u2217A\u20162F \u2212 c ] + c }\n\u2264 \u03b31(1 + 11 ) 1\u2212 4 \u2016A\u2212P\u2217A\u20162F\nSubstituting appropriate value of \u03b31 yields the result.\nThe above result (Theorem 4) is obtained by stitching together many intermediate results. To make sure that everything works at the end, we have different ranges for in Lemma 1 and Theorem 4.\nA simple consequence of the above theorem is the following result which ensures (1 + \u2032) approximation for any 0 < \u2032 < 1.\nCorollary 1. Fix any \u2032, where 0 < \u2032 < 1. Let rank of A be \u03c1. Let m = O(k/ \u2032) and fix any k that satisfies\u2211m i=1 \u03c3 2 i (A) \u2264 12 \u2211\u03c1 i=1 \u03c3 2 i (A). Let A\u0303 be a sparse matrix obtained using non-uniform random sampling scheme as in equation 4 with p = O ( nb2k\n( \u2032)9\u2016A\u20162F\n) . For any set S of rank\nk cluster projection matrices, let P\u2217 = argminP\u2208S \u2016A\u2212 PA\u20162F and P\u0303\u2217 = argminP\u2208S \u2016A\u0303 \u2212 PA\u0303\u20162F . For any 1 \u2264 \u03b3 \u2264 2 satisfying (\u03b3 \u2212 1) \u2211r i=m+1 \u03c3\n2 i (A\u0303) \u2264\u2211m+k\ni=m+1 \u03c3 2 i (A\u0303), if \u2016A\u0303\u2212 P\u0302A\u0303\u20162F \u2264 \u03b3\u2016A\u0303\u2212 P\u0303\u2217A\u0303\u20162F , then A\u0303 contains O (\nnk ( \u2032)9 + d(log n)\n4 )\nnon-zero entries in ex-\npectation and with probability at least (1\u22121/n19 log3 n) the following holds,\n\u2016A\u2212 P\u0302A\u20162F \u2264 \u03b3(1 + \u2032)\u2016A\u2212P\u2217A\u20162"}, {"heading": "5. Empirical evaluations", "text": "In this section we present empirical evaluation of our proposed algorithm on three real world datasets: USPS, RCV1 and TDT2. The USPS dataset (Hull, 1994) contains 9298 handwritten digit images, where each 16 \u00d7 16 image is represented by a feature vector of length 256. We seek to find k = 10 clusters, one for each of the ten digits. The RCV1 dataset (Lewis et al., 2004) is an archive of over 800, 000 manually categorized news articles recently made available by Reuters. We use a smaller subset of this dataset available from LIBSVM webpage (LIB) containing 15, 564 news articles from 53 categories. Each such news article is represented by a feature vector of length 47, 236. We seek to find k = 53 clusters, one for each news article category. The TDT2 dataset (Cieri et al., 1999) consists of 11201 text documents which are classified into 96 semantic categories. We use a smaller subset of this dataset available from Deng Cai\u2019s webpage3 where those documents appearing in two or more categories are removed, and only the largest 30 categories are kept, resulting in 9, 394 documents in total. Each such document is represented by a feature vector of length 36, 771. We seek to find k = 30 clusters, one for each news article category. For USPS dataset, all (100%) data matrix entries are non-zero. However, for TDT2 dataset, only 0.35% entries of the 9394 \u00d7 36771 data matrix are\n3http://www.cad.zju.edu.cn/home/dengcai/Data/TextData.html\nnon-zero, while for RCV1 dataset, only 0.14% entries of the 15564 \u00d7 47236 data matrix are non-zero. Since these two later datasets are already very sparse we reduce data dimensionality to 1000 using random projection in both cases by multiplying original data matrices with a random projection matrix (of appropriate size) whose entries are standard i.i.d. normals4. After this random projection step, resulting projected data matrices become dense matrices, each containing 100% non-zero entries. As we will demonstrate next, for these two dense projected matrices our proposed sparsification method finds k-means clustering solution without severely affecting cluster quality.\nTo apply Lloyd\u2019s heuristic for k-means clustering we use Matlab\u2019s kmeans function which, by default, uses kmeans++ algorithm (Arthur & Vassilvitskii, 2007) for cluster center initialization. We repeat this clustering 30 times, each time initializing cluster center using k-means++ and selecting the final clustering as the one with lowest k-means objective. We demonstrate the effect of random sparsification obtained by uniform and non-uniform sampling on k-means clustering by reporting the following quantities, (a) the ratio h1(q) = \u2016A\u2212XqX>q A\u20162F /\u2016A\u2212XX>A\u20162F , (b) cluster quality h2(q), measured by normalized mutual information (with respect to ground truth cluster labels of A) of a sparse data matrix A\u0303 whose q fracation of entries are non-zero, and (c) normalized objective function h3(q) = \u2016A \u2212 XqX>q A\u20162F /\u2016A\u20162F , as we vary q. In the above description, Xq is the cluster indicator matrix obtained by running k-means on sparse data matrix A\u0303 whose q fraction of entries are non-zero and X is the cluster indicator matrix obtained by running k-means on A.\nFor uniform sampling, p simply indicates that p fraction\n4It has been shown (Cohen et al., 2015) that such dimensionality reduction introduces (1 + ) relative error to optimal k-means objective. We chose projected dimension to be 1000 since increasing it further did not increase normalized mutual information significantly.\nof entries of A\u0303 are non-zero (in expectation, when A is dense matrix). For non-uniform sampling, number of nonzero entries in A\u0303 can only be guaranteed by Theorem 2, which typically holds for large n. In our empirical evaluation we use a slightly different strategy for non-uniform sampling than what is presented in section 2.3.2. However, this modified strategy, in principle, is still similar to what is presented in section 2.3.2. For any fixed value of p, note that \u03c4ij = p(A(i, j)/b)\n2. Now, instead of using pij in terms of \u03c4ij as given in section 2.3.2, we use,\npij = { \u03c4ij if \u03c4ij \u2265 p\u00d7 f\u221a \u03c4ij \u00d7 p\u00d7 f otherwise\nwhere, f > 1, is to be chosen later. Therefore, for \u03c4ij < p\u00d7f , pij = \u221a \u03c4ij \u00d7 p\u00d7 f = p\u00d7(|A(i, j)|/b)\u00d7 \u221a f . The basic idea is still same as before, i.e., when \u03c4ij is small, instead of setting pij \u221d (A(i, j))2, we set pij \u221d |A(i, j)|. Now consider the case when \u03c4ij < p\u00d7 f , for all i, j. The expected number of non-zero entries is \u2211 i,j p \u00d7 \u221a f \u00d7 (|A(i, j)|/b) = pnd\u00d7 \u221a f\u00d7Avg(|A(i, j)|/b). Therefore, if we choose5 f = 1/(Avg(|A(i, j)|/b))2, expected number of non-zero entries in A\u0303 is pnd. In general, when the condition \u03c4ij < p\u00d7 f does not hold for all i, j, the expected fraction will be even less since pij < p, for \u03c4ij \u2265 p\u00d7 f . In our experimental setting, we set f = 1/(Avg(|A(i, j)|/b))2. This ensures for any p, non-uniform sampling results in at most p fraction of non-zero entries in A\u0303. In our experiments, we vary p from 0.01 to 1.0 in steps of 0.01 and for each value of p, the number of of non-zero entries in A\u0303 obtained due to non-uniform sampling is denoted by q and is plotted in Figure 2 for all three datasets. Observe that for all values of p, q = p for uniform sampling and q \u2264 p for non-uniform sampling.\nNext, in Figure 3 we show how random sparsification affects k-means clustering quality with increasing q. As can be seen from Figure 3, with increasing q, h1(q) and h3(q) decrease, while h2(q) increases as one would expect. In fact, h1(q) decreases quickly towards its optimal value 1 and corresponding h2(q) value quickly increases towards optimal k-means normalized mutual information of A. The normalized k-means objective h3(q) also shows steady decrease with increasing q. As can be seen from Figure 3, for all three datasets, non-uniform sampling yields better k-means clustering performance compared to uniform sampling. This makes perfect sense since non-uniform sampling, unlike uniform sampling, enforces sparsity by retaining entries with probability that depends on their magnitude. In fact, for TDT2 and RCV1 datasets, non-uniform sampling results in significant improvement in k-means clustering performance compared to uniform sampling.\n5Avg(|A(i, j)|) represents average over all entries |A(i, j)|.\nUSPS\nTDT2\nRCV1"}, {"heading": "6. Conclusion", "text": "In this paper we proposed a simple algorithm for k-means clustering using random matrix sparsification and presented its analysis. We proved that under mild condition, for any \u2208 (0, 1), a dense data matrix A \u2208 Rn\u00d7d can be randomly sparsified to yield a data matrix A\u0303 \u2208 Rn\u00d7d containing O(nk/ 9 + d log4 n) non-zero entries in expectation, such that an (1+ )-approximate k-mean clustering solution of A\u0303 results in (1 +O( ))-approximate clustering solution of A with high probability. Empirical results on three real world datasets demonstrated that k-means clustering solution of A\u0303 was indeed very close to k-means clustering solution of A. Moreover, sparsification obtained by non-uniform sampling resulted in better cluster quality compared to uniform sampling. Empirical results also seem to suggest that the O(1/ 9) dependence on the number of non-zero entries in A\u0303 is possibly a bit loose. We conclude this paper with two possible open questions: (a) Is it possible to analytically provide a better estimate (by improving dependence on 1/ ) of the number of non-zero entries in the sparse matrix A\u0303 that ensures (1 + ) approximation guarantee? and, (b) Using different proof technique, is it possible to show that \u03b3-approximate clustering solution of A\u0303 will result in \u03b3(1 + )-approximate solution of A as shown in Corollary 1, even for \u03b3 > 2? In other words, is the restriction on \u03b3\nin Corollary 1 a limitation of the proof technique or does it indicate computational hardness of the problem?"}], "year": 2018, "references": [{"title": "Fast computation of low-rank matrix approximations", "authors": ["D. Achlioptas", "F. Mcsherry"], "venue": "J. ACM,", "year": 2007}, {"title": "K-means++: The advantages of careful seeding", "authors": ["D. Arthur", "S. Vassilvitskii"], "venue": "In 18th Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2007}, {"title": "Speeding up kmeans by approximating euclidean distances via block vectors", "authors": ["T. Bottesch", "T. Buhler", "M. Kachele"], "venue": "In 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Random projections for k-means clustering", "authors": ["C. Boutsidis", "A. Zouzias", "P. Drineas"], "venue": "In 24th Annual Conference on Neural Information Processing Systems,", "year": 2010}, {"title": "Randomized dimensionality reduction for k-means clustering", "authors": ["C. Boutsidis", "A. Zouzias", "M.W. Mahoney", "P. Drineas"], "venue": "IEEE Transactions on Information Theory,", "year": 2015}, {"title": "The TDT-2 text and speech corpus", "authors": ["C. Cieri", "D. Graff", "M. Liberman", "N. Martey", "S. Strassel"], "venue": "In DARPA Broadcast News Workshop,", "year": 1999}, {"title": "Dimensionality reduction for k-means clustering and low rank approximation", "authors": ["M.B. Cohen", "S. Elder", "C. Musco", "M. Persu"], "venue": "In 47th Annual Symposium on Theory of Computing,", "year": 2015}, {"title": "The Hardness of K-means Clustering", "authors": ["S. Dasgupta"], "venue": "Technical report (University of California, San Diego. Department of Computer Science and Engineering). Department of Computer Science and Engineering,", "year": 2008}, {"title": "Yinyanh k-means: A drop-in replacement of the classic k-means with consistent speedup", "authors": ["Y. Ding", "Y. Zhao", "X. Shen", "M. Musuvathi", "T. Mytkowicz"], "venue": "In 32nd International Conference on Machine Learning,", "year": 2015}, {"title": "Accelerated k-means with adaptive distance bounds", "authors": ["J. Drake", "G. Hamerly"], "venue": "In 5th NIPS Workshop on Optimization for Machine Learning,", "year": 2012}, {"title": "Using triangle inequality to accelerate k-means", "authors": ["C. Elkan"], "venue": "In 20th International Conference on Machine Learning,", "year": 2003}, {"title": "A unified framework for approximating and clustering data", "authors": ["D. Feldman", "M. Langberg"], "venue": "In 43rd Annual ACM Symposium on Theory of Computing,", "year": 2011}, {"title": "Turning big data into tiny data: Constant-size coresets for k-means, pca and projective clustering", "authors": ["D. Feldman", "M. Schmidt", "C. Sohler"], "venue": "In 24th Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2013}, {"title": "Making k-means even faster", "authors": ["G. Hamerly"], "venue": "In SIAM International Conference on Data Mining,", "year": 2010}, {"title": "A database for hand written text recognition research", "authors": ["J.J. Hull"], "venue": "IEEE Transactions on pattern Analysis and machine Intelligence,", "year": 1994}, {"title": "RCV1: A new benchmark collection for text categorization research", "authors": ["D.D. Lewis", "Y. Yang", "T.G. Rose", "F. Li"], "venue": "Journal of Machine Learning Research,", "year": 2004}, {"title": "Sparse embedded kmeans clustering", "authors": ["W. Liu", "X. Shen", "I.W. Tsang"], "venue": "In 31st Annual Conference on Neural Information Processing Systems,", "year": 2017}, {"title": "Least square quantization in PCM", "authors": ["S. Lloyd"], "venue": "IEEE Transactions on Information Theory,", "year": 1982}, {"title": "Symmetric gauge functions and unitarily invariant norms", "authors": ["L. Mirsky"], "venue": "The Quarterly Journal of Mathematics,", "year": 1960}, {"title": "Fast k-means with accurate bounds", "authors": ["J. Newling", "F. Fleuret"], "venue": "In 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Top 10 algorithms in data mining. Knowledge and Information Syaytems", "authors": ["X. Wu"], "year": 2008}], "id": "SP:a85bb2f8fe74b6ab7f4c8a749997c5887503f51e", "authors": [{"name": "Kaushik Sinha", "affiliations": []}], "abstractText": "K-means clustering algorithm using Lloyd\u2019s heuristic is one of the most commonly used tools in data mining and machine learning that shows promising performance. However, it suffers from a high computational cost resulting from pairwise Euclidean distance computations between data points and cluster centers in each iteration of Lloyd\u2019s heuristic. Main contributing factor of this computational bottle neck is a matrix-vector multiplication step, where the matrix contains all the data points and the vector is a cluster center. In this paper we show that we can randomly sparsify the original data matrix resulting in a sparse data matrix which can significantly speed up the above mentioned matrix vector multiplication step without significantly affecting cluster quality. In particular, we show that optimal k-means clustering solution of the sparse data matrix, obtained by applying random matrix sparsification, results in an approximately optimal k-means clustering objective of the original data matrix. Our empirical studies on three real world datasets corroborate our theoretical findings and demonstrate that our proposed sparsification method can indeed achieve satisfactory clustering performance.", "title": "K-means clustering using random matrix sparsification"}