17	81	Our ensemble tuning method over multiple metrics produced superior translations than single metric tuning as measured by a post-editing task.
18	44	HTER (Snover et al., 2006) scores in our human evaluation confirm that multi-metric optimization can lead to better MT output.
41	25	(2) (Razmara et al., 2012) propose several mixture operations, such as log-wsum (simple linear mixture), wsum (log-linear mixture) and max (choose lo- cally best model) among others.
45	17	,Mk(H)] ) (3) where H = N (f ;w) where N (f ;w) is the decoding function generating a set of candidate hypotheses H based on the model parameters w, for the source sentences f .
52	25	We introduce four methods based on the above formulation and each method uses a different type of g(·) function for combining different metrics and we compare experimentally with existing methods.
68	55	The PMO ensemble approach is graphically illustrated in Figure 1; we will also refer to this figure while discussing other methods.2 The orig- settings for metrics (M1, M2), viz.
70	99	They combine the metric weights qi with the sentence-level metric scores Mi as ` = (∑ k qkMk ) /k where ` is the target value for negative examples (the else line in Alg 1) in the optimization step.
74	26	It uses a secondary hard EM objective to move away, when the primary soft EM objective gets stuck in a local optima.
80	16	In terms of Figure 1, lateen MMO corresponds to alternately maximizing the metrics along two dimensions as depicted by the solid arrows.
85	32	At each iteration lateen MMO excludes all but one metric for optimization.
113	21	In the first step it optimizes each of the k metrics independently (lines 6-7) along its respective dimension in Algorithm 2 Ensemble Tuning Algorithm 1: Input: Tuning set f , Metrics M1, .
120	19	could be any function that combines multiple metrics, we use the PMO-PRO algorithm (Alg.
136	28	We used the default parameter settings for different MT tuning metrics.
137	37	For METEOR, we tried both METEOR-tune and METEOR-hter settings and found the latter to perform better in BLEU and TER scores, even though the former was marginally better in METEOR3 and RIBES scores.
138	31	We observed the margin of loss in BLEU and TER to outweigh the gains in METEOR and RIBES and we chose METEOR-hter setting for both optimization and evaluation of all our experiments.
142	33	Similar to Duh et al. (2012), we use five different BLEU:RIBES weight settings, viz.
145	64	Figure 2(a) shows the Pareto frontier of L and P baselines using BLEU and RIBES as two metrics.
146	77	The frontier of the P dominates that of L for most part showing that the PMO approach benefits from picking Pareto points during the optimization.
147	136	We use the PMO-ensemble approach to combine the optimized weights from the 5 tuning runs and re-decode the devset employing ensemble decoding.
148	85	This yields the points LEns and PEns in the plot, which obtain better scores than most of the individual runs of L and P. This ensemble approach of combining the final weights also generalizes to the unseen test set as we show later.
151	32	In both datasets, our ensemble tuning approach dominates the curves of the (L and P) baselines.
154	127	This section contains multi-metric optimization results on the unseen test sets, one test set has multiple references and the other has a single-reference.
155	20	We plot BLEU scores against other metrics (RIBES, METEOR and TER) and this allows us to compare the performance of each metric relative to the defacto standard BLEU metric.
156	42	Baseline points are identified by single letters B for BLEU, T for TER, etc.
157	36	and the baseline (singlemetric optimized) score for each metric is indicated by a dashed line on the corresponding axis.
158	35	MMO points use a series of single letters referring to the metrics used, e.g. BT for BLEU-TER.
159	21	The union of metrics method is identified with the suffix ’J’ and lateen method with suffix ’L’ (thus BT-L refers to the lateen tuning with BLEU-TER).
160	24	MMO points without any suffix use the ensemble tuning approach.
162	72	We see noticeable and some statistically significant improvements in BLEU and RIBES (see Table 2 for BLEU improvements).
167	56	We again see in these figures that the MMO approaches can improve the BLEU-only tuning by 0.3 BLEU points, without much drop in other metrics.
