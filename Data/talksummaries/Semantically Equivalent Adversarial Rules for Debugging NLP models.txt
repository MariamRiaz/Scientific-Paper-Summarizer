0	67	With increasing complexity of models for tasks like classification (Joulin et al., 2016), machine comprehension (Rajpurkar et al., 2016; Seo et al., 2017), and visual question answering (Zhu et al., 2016), models are becoming increasingly challenging to debug, and to determine whether they are ready for deployment.
7	62	For text, however, a single word addition can change semantics (e.g. adding “not”), or have no semantic impact for the task at hand.
8	129	Inspired by adversarial examples for images, we introduce semantically equivalent adversaries (SEAs) – text inputs that are perturbed in semantics-preserving ways, but induce changes in a black box model’s predictions (example in Figure 1).
9	54	Producing such adversarial examples systematically can significantly aid in debugging ML models, as it allows users to detect problems that happen in the real world, instead of oversensitivity only to malicious attacks such as intentionally scrambling, misspelling, or removing words (Bansal et al., 2014; Ebrahimi et al., 2018; Li et al., 2016).
11	55	We represent these via simple replacement rules that induce SEAs on multiple predictions, such as in Figure 2, where a simple contraction of “is”after Wh pronouns (what, who, whom) (2b) makes 70 (1%) of the previously correct predictions of the model “flip” (i.e. become incorrect).
23	61	Given an indicator function SemEq(x, x′) that is 1 if x is semantically equivalent to x′ and 0 otherwise, we define a semantically equivalent adversary (SEA) as a semantically equivalent instance that changes the model prediction in Eq (1).
26	88	We turn instead to paraphrasing based on neural machine translation (Lapata et al., 2017), where P (x′|x) (the probability of a paraphrase x′ given original sentence x) is proportional to translating x into multiple pivot languages and then taking the score of back-translating the translations into the original language.
31	29	In order to generate adversaries, we generate a set of paraphrases Πx around x via beam search and get predictions on Πx using the black box model until an adversary is found, or until S(x, x′) < τ .
33	28	We illustrate this process in Figure 3, where we generate SEAs for a VQA model by generating paraphrases around the question, and checking when the model prediction changes.
36	53	In this section, we address the problem of generalizing local adversaries into Semantically Equivalent Adversarial Rules for Text (SEARs), search and replace rules that produce semantic adversaries with little or no change in semantics, when applied to a corpus of sentences.
37	46	Assuming that humans have limited time, and are thus willing to look at B rules, we propose a method for selecting such a set of rules given a reference dataset X .
38	55	A rule takes the form r = (a→c), where the first instance of the antecedent a is replaced by the consequent c for every instance that includes a, as we previously illustrated in Figure 2a.
65	95	For each rule, we display two example questions with the corresponding SEA, the prediction (with corresponding change) and the percentage of “flips” - instances previously predicted correctly on the validation data, but predicted incorrectly after the application of the rule.
66	78	The rule (What VBZ→What’s) generalizes the SEA on Figure 1, and shows that the model is fragile with respect to contractions (flips 2% of all correctly predicted instances on the validation data).
67	30	The second rule uncovers a bug with respect to simple question rephrasing, while the third and fourth rules show that the model is not robust to a more conversational style of asking questions.
72	44	Sentiment Analysis: Finally, in Table 3 we display SEARs for a fastText (Joulin et al., 2016) model for sentiment analysis trained on movie reviews.
95	40	The third condition (HSEA) is a collaboration between our method and humans: we take the top 5 adversaries ranked by S(x, x′), and ask workers to pick the one closest to the original instance, rather than asking them to generate the adversaries.
102	52	On both datasets, the automated method or humans were able to generate adversaries at the exclusion of the other roughly one third of the time, which indicates that they do not generate the same adversaries.
104	65	This is illustrated by examples in Table 5 - in Table 5a we see examples where very compact changes generate adversaries (humans were not able to find these changes though).
129	54	We compare expert-generated rules with accepted SEARs (each subject’s rules are compared to the SEARs they accepted) in terms of the percentage of the correct predictions that “flip” when the rules are applied.
130	58	This is what we asked the subjects to maximize, and all the rules were ones deemed to be semantic equivalent by the subjects themselves.
131	29	We also consider the union of expertgenerated rules and accepted SEARs.
136	42	Making good use of POS tags is also a challenge: only 50% of subjects attempt rules with POS tags for VQA, 36% for sentiment analysis.
138	74	Similar to the previous experiment, errors made by the semantic scorer lead to rules that are not semantically equivalent (e.g. Table 7).
139	27	With minimal human intervention, however, SEARs vastly outperform human experts in finding impactful bugs.
140	180	Once such bugs are discovered, it is natural to want to fix them.
143	29	We take the rules that are accepted by ≥ 20 subjects as accepted bugs, a total of 4 rules (in Table 2) for VQA, and 16 rules for sentiment (including ones in Table 3).
144	54	We then augment the training data by applying these rules to it, and retrain the models.
149	38	These results show that SEARs are useful not only for discovering bugs, but are also actionable through a simple augmentation technique for any model.
173	66	Other paraphrase limitations: Paraphrase models based on neural machine translation are biased towards maintaining the sentence structure, and thus do not produce certain adversaries (e.g. Table 5b), which recent work on paraphrasing (Iyyer et al., 2018) or generation using GANs (Zhao et al., 2018) may address.
