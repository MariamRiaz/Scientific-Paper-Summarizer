0	42	An important aspect of dialogue response generation systems, which are trained to produce a reasonable utterance given a conversational context, is how to evaluate the quality of the generated response.
1	89	Typically, evaluation is done using human-generated supervised signals, such as a task completion test or a user satisfaction score (Walker et al., 1997; Möller et al., 2006; Kamm, 1995), which are relevant when the dialogue is task-focused.
2	55	We call models optimized for such supervised objectives supervised dialogue models, while those that do not are unsupervised dialogue models.
3	116	This paper focuses on unsupervised dialogue response generation models, such as chatbots.
5	35	This avoids the need to collect supervised labels on a large scale, which can be prohibitively expensive.
7	65	Automatic evaluation metrics would help accelerate the deployment of unsupervised response generation systems.
11	42	However these metrics assume that valid responses have significant word overlap with the ground truth responses.
14	96	In this paper, we investigate the correlation between the scores from several automatic evaluation metrics and human judgements of dialogue response quality, for a variety of response generation models.
16	216	We find that all metrics show either weak or no correlation with human judgements, despite the fact that word overlap metrics have been used extensively in the literature for evaluating dialogue response models (see above, and Lasguido et al. (2014)).
18	124	For the word embedding metrics, we show that this is true even though all metrics are able to significantly distinguish between baseline and state-of-the-art models across multiple datasets.
19	27	We further highlight the shortcomings of these metrics using: a) a statistical analysis of our survey’s results; b) a qualitative analysis of examples from our data; and c) an exploration of the sensitivity of the metrics.
20	166	Our results indicate that a shift must be made in the research community away from these metrics, and highlight the need for a new metric that correlates more strongly with human judgement.
39	177	We focus on metrics that compare it to the ground truth response of the conversation.
42	29	We examine the BLEU and METEOR scores that have been used for machine translation, and the ROUGE score that has been used for automatic summarization.
43	127	While these metrics have been shown to correlate with human judgements in their target domains (Papineni et al., 2002a; Lin, 2004), they have not been thoroughly investigated for dialogue systems.2 We denote the ground truth response as r (thus we assume that there is a single candidate ground truth response), and the proposed response as r̂.
44	48	The j’th token in the ground truth response r is denoted by wj , with ŵj denoting the j’th token in the proposed response r̂.
58	26	An alternative to using word-overlap based metrics is to consider the meaning of each word as defined by a word embedding, which assigns a vector to each word.
67	23	The embedding average, ē, is defined as the mean of the word embeddings of each token in a sentence r: ēr = ∑ w∈r ew |∑w′∈r ew′ | .
68	51	To compare a ground truth response r and retrieved response r̂, we compute the cosine similarity between their respective sentence level embeddings: EA := cos(ēr, ēr̂).
76	71	In order to determine the correlation between automatic metrics and human judgements of response quality, we obtain response from a diverse range of response generation models in the recent literature, including both retrieval and generative models.
78	23	Such systems can be evaluated using recall or precision metrics.
80	26	Thus, in the results presented below we remove one occurrence of the ground-truth response from the corpus and ask the model to retrieve the most appropriate response from the remaining utterances.
82	18	We then evaluate each model by comparing the retrieved response to the ground truth response of the conversation.
86	67	In order to apply TF-IDF as a retrieval model for dialogue, we first compute the TF-IDF vectors for each context and response in the corpus.
90	20	The model then calculates the probability that the given response is the ground truth response given the context, by taking a weighted dot product: p(r is correct|c, r,M) = σ(cTMr + b) where M is a matrix of learned parameters and b is a bias.
92	40	To our knowledge, our application of neural network models to large-scale retrieval in dialogue systems is novel.
101	43	When evaluation metrics are not explicitly correlated to human judgement, it is possible to draw misleading conclusions by examining how the metrics rate different models.
102	19	To illustrate this point, we compare the performance of selected models according to the embedding metrics on two different domains: the Ubuntu Dialogue Corpus (Lowe et al., 2015), which contains technical vocabulary and where conversations are often oriented towards solv- ing a particular problem, and a non-technical Twitter corpus collected following the procedure of Ritter et al. (2010).
110	37	We conducted a human survey to determine the correlation between human judgements on the quality of responses, and the score assigned by each metric.
112	21	25 volunteers from the Computer Science department at the author’s institution were given a context and one proposed response, and were asked to judge the response quality on a scale of 1 to 5.5; a 1 indicates that the response is not appropriate or sensible given the context, and a 5 indicates that the response is very reasonable.
