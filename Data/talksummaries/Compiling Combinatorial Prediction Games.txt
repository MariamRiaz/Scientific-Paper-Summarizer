5	16	Conceptually, an online combinatorial optimization problem can be cast as a repeated prediction game between a learning algorithm and its environment (Audibert et al., 2011; 2014).
6	13	During each trial t, the learner chooses a feasible solution st from its decision set S and, simultaneously, the environment selects a loss vector `t ∈ [0, 1]d. Then, the learner incurs the loss 〈`t, st〉 = ∑d i=1 `t(i)st(i) and, in light of the feedback provided by its environment, updates its strategy in order to improve the chance of selecting better solutions on subsequent trials.
11	13	The first, called regret, measures the difference in cumulative loss between the algorithm and the best solution in hindsight.
13	10	In such non-oblivious or adversarial environments, the learner is generally allowed to make decisions in a randomized way, and its predictive performance is measured by the expected regret: RT = E [ T∑ t=1 〈`t, st〉 ] −min s∈S T∑ t=1 〈`t, s〉 The second metric is computational complexity, i.e. the amount of resources required to compute st at each round t, given the sequence of feedbacks observed so far.
14	15	In the literature of combinatorial prediction games, three main strategies have been proposed to attain an expected regret that is sublinear in the game horizon T and polynomial in the input dimension d. The first, and arguably simplest strategy, is to Follow the Perturbed Leader (FPL): on each trial t, the learner draws at random a perturbation vector zt ∈ Rd, and then selects in S a minimizer of ηLt + zt, where η ∈ (0, 1] is a step-size parameter, andLt is the cumulative lossLt = `1 + · · ·+`t−1.
22	9	Next, pt is decomposed as a convex composition of feasible solutions in S, and then, a decision st is picked at random according to the resulting distribution.
27	34	However, we get a different picture if computational considerations are taken into account: all aforementioned algorithms rely on powerful oracles for making decisions in spaces S represented by combinatorial constraints.
33	7	Based on this technique, tractable forms of EH can be derived (Takimoto & Warmuth, 2003; Rahmanian & Warmuth, 2017).
36	7	These theoretical results naturally motivate the following question: can we compile a set of constraints representing a combinatorial space S into a compact and Boolean circuit for which both solution sampling and linear optimization are tractable?
40	16	Although the size of C may grow exponentially in the treewidth of the input formula, it is usually much smaller in practice; existing compilers are able to compress combinatorial spaces defined over thousands of variables and constraints.
41	21	With these compilation tools in hand, our contributions are threefold: (i) we show that for dDNNF circuits, the sampling oracle in EH and the linear optimization oracle in FPL, run in linear time using a simple variant of the weight- pushing technique; (ii) for the SGD and CH strategies, we develop a Bregman projection-decomposition method that uses O(d2 ln(dT )) calls to the linear optimization oracle; (iii) we experimentally show on online configuration and planning tasks that EH and FPL are fast, but our variants of SGD and CH are more efficient to minimize the empirical regret.
60	20	Inference tasks on an NNF circuit C are defined using a commutative semiring Q = (R,⊕,⊗,⊥,>) and an input vector w ∈ Rd.
93	16	The class of Free Binary Decision Diagrams (FBDD) is the subset of dDNNF in which every or-node is a decision node, and at least one child of any and-node is a literal (Wegener, 2000).
100	17	Notably, using the fact that ‖s‖1 = d/2, the regret bounds for EH and FPL can easily be derived from (Audibert et al., 2011) and (Hutter & Poland, 2005), respectively.
104	15	By Proposition 1, the FPL strategy also runs inO(|C|) time per round, using a dDNNF encoding C of S, and the fact that |C| is in Ω(d).
106	14	The overall idea of Online Mirror Descent (OMD) is to “follow the regularized leader” through a primal-dual approach (Nemirovski & Yudin, 1983; Beck & Teboulle, 2003).
109	12	The projection step is captured by the Bregman divergence of F , which is a function BF : K × int(K)→ R given by: BF (p, q) = F (p)− F (q)− 〈∇F (q),p− q〉 In the stochastic variant of OMD, introduced by Audibert et.
114	21	For a convex set K, a differentiable function f : K → R is called α-strongly convex with respect to a norm ‖ · ‖ if f(p′)− f(p) ≥ 〈∇f(p),p′ − p〉+ α 2 ‖p′ − p‖2 Furthermore, f is called β-smooth1 with respect to ‖ · ‖ if f(p′)− f(p) ≤ 〈∇f(p),p′ − p〉+ β 2 ‖p′ − p‖2 Based on these notions, we say that a Bregman divergence BF has the condition number β/α if BF is both α-strongly convex and β-smooth with respect to the Euclidean norm ‖ · ‖2 in its first argument.
115	8	For such regularizers, the next result states that the projection-decomposition step can be approximated in low polynomial time, by exploiting the Pairwise Conditional Gradient (PCG) method, a variant of the Frank-Wolfe convex optimization algorithm, whose convergence rate has been analyzed in (Lacoste-Julien & Jaggi, 2015; Garber & Meshi, 2016; Bashiri & Zhang, 2017).
128	15	For the second term in (2), we get from the Cauchy-Schwarz inequality that E〈`t, st − p∗t 〉 ≤ ‖`t‖2‖pt − p∗t ‖2≤ √ d‖pt − p∗t ‖2 Moreover, by applying the Generalized Pythagorean Theorem (Cesa-Bianchi & Lugosi, 2006), we know that BF (p, qt) ≥ BF (p,p∗t ) + BF (p∗t , qt), for any p ∈ conv(S).
133	10	In this simple framework, the primal and dual spaces coincide with Rd, and hence, F ∗(u) = u, ∇F (p) = p, and ∇F ∗(u) = u.
142	7	It is easy to show that ∂Fδ(p) ∂p(i) = ln(p(i) + δ) ∂F ∗δ (u) ∂u(i) = eu(i) − δ BFδ(p, q) = BF (p+δ, q+δ) B ∗ Fδ (u,v) = B∗F (u,v) where B∗F (u,v) = ∑d i=1 e v(i)(ev(i)−u(i) +v(i)−u(i)−1).
151	27	Using the notation p̃1 = p∗1 + δ and r = d(1/2 + δ), we get that Fδ(s ∗ 1)− Fδ(p∗1) ≤ d∑ i=1 p̃1(i) ln 1 p̃1(i) ≤ r ln d r which is bounded by r. For the third term of (1), observe that Fδ is 1(1+δ)d -strongly convex with respect to the norm ‖ · ‖1, since ‖p − p′‖21≤ d‖p − p′‖22.
154	9	In order to evaluate the performance of the different online combinatorial optimization strategies examined in Section 3, we have considered 16 instances of the SAT Library,2 described in Table 3.
156	27	In the first four columns of the table are reported the name of the SAT instance, the number of attributes (d/2), the number of constraints (|SAT|), and the number |S| of feasible solutions.
157	26	We have used the recent D4 compiler 3 (Lagniez & Marquis, 2017) for transforming SAT instances into dDNNF circuits.
160	23	Suppose that the set X = {x1, · · · , xd} of literals is sorted in a lexicographic way, so that for each odd integer i, the pair (xi, xi+1) encodes both configurations of the same binary attribute.
161	36	First, we construct a vector µ0 of d/2 independent Bernoulli variables.
167	7	Finally, the horizon T was set to 103, and a timeout of one day was fixed for learning.
168	23	In our experiments, the regret is measured by the difference in cumulative loss between the algorithm and the best feasible solution in hindsight, which is obtained using the linear optimization oracle at horizon T .
