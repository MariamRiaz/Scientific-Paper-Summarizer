0,1,label2,summary_sentences
"People vary widely both in their linguistic preferences when producing language and in their ability to understand specific natural-language expressions, depending on what they know about the domain, their age and cognitive capacity, and many other factors.",1 Introduction,[0],[0]
"It has long been recognized that effective NLG systems should therefore adapt to the current user, in order to generate language which works well for them.",1 Introduction,[0],[0]
"This adaptation needs to address all levels of the NLG pipeline, including discourse planning (Paris, 1988), sentence planning (Walker et al., 2007), and RE generation (Janarthanam and Lemon, 2014), and depends on many features of the user, including level of expertise and language proficiency, age, and gender.
",1 Introduction,[0],[0]
Existing techniques for adapting the output of an NLG system have shortcomings which limit their practical usefulness.,1 Introduction,[0],[0]
"Some systems need user-specific information in training (Ferreira and Paraboni, 2014) and therefore cannot generalize to unseen users.",1 Introduction,[0],[0]
"Other systems assume that each user in the training data is annotated with their group, which allows them to learn a model from the data of each group.",1 Introduction,[0],[0]
"However, hand-designed user groups
may not reflect the true variability of the data, and may therefore inhibit the system’s ability to flexibly adapt to new users.
",1 Introduction,[0],[0]
"In this paper, we present a user adaptation model for NLG systems which induces user groups from training data in which these groups were not annotated.",1 Introduction,[0],[0]
"At training time, we probabilistically assign users to groups and learn the language preferences for each group.",1 Introduction,[0],[0]
"At evaluation time, we assume that our system has a chance to interact with each new user repeatedly – e.g., in the context of a dialogue system.",1 Introduction,[0],[0]
"It will then calculate an increasingly accurate estimate of the user’s group membership based on observable behavior, and use it to generate utterances that are suitable to the user’s true group.
",1 Introduction,[0],[0]
We evaluate our model on two tasks involving the generation of referring expressions (RE).,1 Introduction,[0],[0]
"First, we predict the use of spatial relations in humanlike REs in the GRE3D domain (Viethen and Dale, 2010) using a log-linear production model in the spirit of Ferreira and Paraboni (2014).",1 Introduction,[0],[0]
"Second, we predict the comprehension of generated REs, in a synthetic dataset based on data from the GIVE Challenge domain (Striegnitz et al., 2011) with the log-linear comprehension model of Engonopoulos et al. (2013).",1 Introduction,[0],[0]
"In both cases, we show that our model discovers user groups in the training data and infers the group of unseen users with high confidence after only a few interactions during testing.",1 Introduction,[0],[0]
"In the GRE3D domain, our system outperformed a strong baseline which used demographic information for the users.",1 Introduction,[0],[0]
Differences between individual users have a substantial impact on language comprehension.,2 Related Work,[0],[0]
"Factors that play a role include level of expertise and spatial ability (Benyon and Murray, 1993); age (Häuser et al., 2017); gender (Dräger and Koller,
ar X
iv :1
80 6.
",2 Related Work,[0],[0]
"05 94
7v 1
[ cs
.C",2 Related Work,[0],[0]
"L
] 1
5 Ju
n 20
18
2012); or language proficiency (Koller et al., 2010).
",2 Related Work,[0],[0]
Individual differences are also reflected in the way people produce language.,2 Related Work,[0],[0]
"Viethen and Dale (2008) present a corpus study of human-produced REs (GRE3D3) for simple visual scenes, where they note two clearly distinguishable groups of speakers, one that always uses a spatial relation and one that never does.",2 Related Work,[0],[0]
Ferreira and Paraboni (2014) show that a model using speaker-specific information outperforms a generic model in predicting the attributes used by a speaker when producing an RE.,2 Related Work,[0],[0]
"However, their system needs to have seen the particular speaker in training, while our system can dynamically adapt to unseen users.",2 Related Work,[0],[0]
"Ferreira and Paraboni (2017) also demonstrate that splitting speakers in predefined groups and training each group separately improves the human likeness of REs compared to training individual user models.
",2 Related Work,[0],[0]
"The ability to adapt to the comprehension and production preferences of a user is especially important in the context of a dialog system, where there are multiple chances of interacting with the same user.",2 Related Work,[0],[0]
Some methods adapt to dialog system users by explicitly modeling the users’ knowledge state.,2 Related Work,[0],[0]
"An early example is Paris (1988); she selects a discourse plan for a user, depending on their level of domain knowledge ranging between novice and expert, but provides no mechanism for inferring the group to which the user belongs.",2 Related Work,[0],[0]
"Rosenblum and Moore (1993) try to infer what knowledge a user possesses during dialogue, based on the questions they ask.",2 Related Work,[0],[0]
Janarthanam and Lemon (2014) adapt to unseen users by using reinforcement learning with simulated users to make a system able to adjust to the level of the user’s knowledge.,2 Related Work,[0],[0]
"They use five predefined groups from which they generate the simulated users’ behavior, but do not assign real users to these groups.",2 Related Work,[0],[0]
"Our system makes no assumptions about the user’s knowledge and does not need to train with simulated users, or use any kind of information-seeking moves; we instead rely on the groups that are discovered in training and dynamically assign new, unseen users, based only on their observable behavior in the dialog.
",2 Related Work,[0],[0]
"Another example of a user-adapting dialog component is SPaRKy (Walker et al., 2007), a trainable sentence planner that can tailor sentence plans to individual users’ preferences.",2 Related Work,[0],[0]
"This requires training on separate data for each user; in contrast to this, we leverage the similarities between users and can take advantage of the full training data.",2 Related Work,[0],[0]
We start with a basic model of the way in which people produce and comprehend language.,3 Log-linear models for NLG in dialog,[0],[0]
"In order to generalize over production and comprehension, we will simply say that a human language user exhibits a certain behavior b among a range of possible behaviors, in response to a stimulus s.",3 Log-linear models for NLG in dialog,[0],[0]
"The behavior of a speaker is the utterance b they produce in order to achieve a communicative goal s; the behavior of a listener is the meaning b which they assign to the utterance s they hear.
",3 Log-linear models for NLG in dialog,[0],[0]
"Given this terminology, we define a basic loglinear model (Berger et al., 1996) of language use as follows:
P (b|s; ρ) = exp(ρ · φ(b, s))∑ b′",3 Log-linear models for NLG in dialog,[0],[0]
"exp(ρ · φ(b′, s))
",3 Log-linear models for NLG in dialog,[0],[0]
"(1)
where ρ is a real-valued parameter vector of length n and φ(b, s) is a vector of real-valued feature functions f1, ..., fn over behaviors and stimuli.",3 Log-linear models for NLG in dialog,[0],[0]
"The parameters can be trained by maximum-likelihood estimation from a corpus of observations (b, s).",3 Log-linear models for NLG in dialog,[0],[0]
"In addition to maximum-likelihood training it is possible to include some prior probability distribution, which expresses our belief about the probability of any parameter vector and which is generally used for regularization.",3 Log-linear models for NLG in dialog,[0],[0]
"The latter case is referred to as a posteriori training, which selects the value of ρ that maximizes the product of the parameter probability and the probability of the data.
",3 Log-linear models for NLG in dialog,[0],[0]
"In this paper, we focus on the use of such models in the context of the NLG module of a dialogue system, and more specifically on the generation of referring expressions (REs).",3 Log-linear models for NLG in dialog,[0],[0]
"Using (1) as a comprehension model, Engonopoulos et al. (2013) developed an RE generation model in which the stimulus s = (r, c) consists of an RE r and a visual context c of the GIVE Challenge (Striegnitz et al., 2011), as illustrated in Fig. 1.",3 Log-linear models for NLG in dialog,[0],[0]
The behavior is the object b in the visual scene to which the user will resolve the RE.,3 Log-linear models for NLG in dialog,[0],[0]
"Thus for instance, when we consider the RE r =“the blue button” in the context of Fig. 1, the log-linear model may assign a higher probability to the button on the right than to the one in the background.",3 Log-linear models for NLG in dialog,[0],[0]
"Engonopoulos and Koller (2014) develop an algorithm for generating the RE r which maximizes P (b∗|s; ρ), where b∗ is the intended referent in this setting.
",3 Log-linear models for NLG in dialog,[0],[0]
"Conversely, log-linear models can also be used to directly capture how a human speaker would refer to an object in a given scene.",3 Log-linear models for NLG in dialog,[0],[0]
"In this case, the stimulus s = (a, c) consists of the target object a and
the visual context c, and the behavior b is the RE.",3 Log-linear models for NLG in dialog,[0],[0]
"We follow Ferreira and Paraboni (2014) in training individual models for the different attributes which can be used in the RE (e.g., that a is a button; that it is blue; that the RE contains a binary relation such as “to the right of”), such that we can simply represent b as a binary choice b ∈ {1,−1} between whether a particular attribute should be used in the RE or not.",3 Log-linear models for NLG in dialog,[0],[0]
"We can then implement an analog of Ferreira’s model in terms of (1) by using feature functions φ(b, a, c) = b · φ′(a, c), where φ′(a, c) corresponds to their context features, which do not capture any speaker-specific information.",3 Log-linear models for NLG in dialog,[0],[0]
"As discussed above, a user-agnostic model such as (1) does not do justice to the variability of language comprehension and production across different speakers and listeners.",4 Log-linear models with user groups,[0],[0]
We will therefore extend it to a model which distinguishes different user groups.,4 Log-linear models with user groups,[0],[0]
We will not try to model why1 users behave differently.,4 Log-linear models with user groups,[0],[0]
"Instead our model sorts users into groups simply based on the way in which they respond to stimuli, in the sense of Section 3, and implements this by giving each group g its own parameter vector ρ(g).",4 Log-linear models with user groups,[0],[0]
"As a theoretical example, Group 1 might contain users who reliably comprehend REs which use colors (“the green button”), whereas Group 2 might contain users who more easily understand relational REs (“the button next to the lamp”).",4 Log-linear models with user groups,[0],[0]
"These groups are then discovered at training time.
",4 Log-linear models with user groups,[0],[0]
"When our trained NLG system starts interacting with an unseen user u, it will infer the group to which u belongs based on u’s observed responses to previous stimuli.",4 Log-linear models with user groups,[0],[0]
"Thus as the dialogue with u unfolds, the system will have an increasingly pre-
1E.g., in the sense of explicitly modeling sociolects or the difference between novice system users vs. experts.
cise estimate of the group to which u belongs, and will thus be able to generate language which is increasingly well-tailored to this particular user.",4 Log-linear models with user groups,[0],[0]
"We assume training data D = {(bi, si, ui)}i which contains stimuli si together with the behaviors bi which the users ui exhibited in response to si.",4.1 Generative story,[0],[0]
"We write D(u) = {(bu1 , su1), . . .",4.1 Generative story,[0],[0]
"(buN , suN )} for the data points for each user",4.1 Generative story,[0],[0]
"u.
",4.1 Generative story,[0],[0]
"The generative story we use is illustrated in Fig. 2; observable variables are shaded gray, unobserved variables and parameters to be set in training are shaded white and externally set hyperparameters have no circle around them.",4.1 Generative story,[0],[0]
"Arrows indicate which variables and parameters influence the probability distribution of other variables.
",4.1 Generative story,[0],[0]
"We assume that each user belongs to a group g ∈ {1, . . .",4.1 Generative story,[0],[0]
",K}, where the number K of groups is fixed beforehand based on, e.g., held out data.",4.1 Generative story,[0],[0]
"A group g is assigned to u at random from the distribution
P (g|π) = exp(πg)∑K g′=1 exp(πg′)
(2)
",4.1 Generative story,[0],[0]
"Here π ∈ RK is a vector of weights, which defines how probable each group is a-priori.
",4.1 Generative story,[0],[0]
"We replace the single parameter vector ρ of (1) with group-specific parameters vectors ρ(g), thus obtaining a potentially different log-linear model P ( b|s; ρ(g) ) for each group.",4.1 Generative story,[0],[0]
"After assigning a group, our model generates responses bu1 , . . .",4.1 Generative story,[0],[0]
", b u N at
random from P ( b|s; ρ(g) ) , based on the group specific parameter vector and the stimuli su1 , . . .",4.1 Generative story,[0],[0]
", s u N .",4.1 Generative story,[0],[0]
This accounts for the generation of the data.,4.1 Generative story,[0],[0]
"We model the parameter vectors π ∈ RK , and ρ(g) ∈",4.1 Generative story,[0],[0]
"Rn for every 1 ≤ g ≤ K as drawn from
P (D; θ) = ∏ u∈U K∑ g=1",4.1 Generative story,[0],[0]
"P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ) · N (π|0, σ(π)) · K∏",4.1 Generative story,[0],[0]
g=1 N,4.1 Generative story,[0],[0]
"( ρ(g)|0, σ(ρ) )",4.1 Generative story,[0],[0]
"(3) L(θ) =
∑ u∈U log K∑",4.1 Generative story,[0],[0]
g=1,4.1 Generative story,[0],[0]
P (g|π) · ∏ d∈D(u) P ( bd|sd; ρ(g) ),4.1 Generative story,[0],[0]
"(4)
AL(θ) = ∑ u∈U K∑ g=1
P (g|D(u); θ(i−1)) ·",4.1 Generative story,[0],[0]
logP (g|π) +,4.1 Generative story,[0],[0]
"∑
d∈Du
logP ( bd|sd; ρ(g) )",4.1 Generative story,[0],[0]
"(5)
normal distributions N (0, σ(π)), and N (0, σ(ρ)), which are centered at 0 with externally given variances and no covariance between parameters.",4.1 Generative story,[0],[0]
This has the effect of making parameter choices close to zero more probable.,4.1 Generative story,[0],[0]
"Consequently, our models are unlikely to contain large weights for features that only occurred a few times or which are only helpful for a few examples.",4.1 Generative story,[0],[0]
"This should reduce the risk of overfitting the training set.
",4.1 Generative story,[0],[0]
The equation for the full probability of the data and a specific parameter setting is given in (3).,4.1 Generative story,[0],[0]
"The left bracket contains the likelihood of the data, while the right bracket contains the prior probability of the parameters.",4.1 Generative story,[0],[0]
Once we have set values θ =,4.2 Predicting user behavior,[0],[0]
"(π, ρ(1), . . .",4.2 Predicting user behavior,[0],[0]
", ρ(K))",4.2 Predicting user behavior,[0],[0]
"for all the parameters, we want to predict what behavior b a user u will exhibit in response to a stimulus s. If we encounter a completely new user u, the prior user group distribution from (2) gives the probability that this user belongs to each group.",4.2 Predicting user behavior,[0],[0]
"We combine this with the group-specific log-linear behavior models to obtain the distribution:
P (b|s; θ) = K∑ g=1 P ( b|s; ρ(g) )",4.2 Predicting user behavior,[0],[0]
"· P (g|π) (6)
",4.2 Predicting user behavior,[0],[0]
"Thus, we have a group-aware replacement for (1).",4.2 Predicting user behavior,[0],[0]
"Furthermore, in the interactive setting of a dialogue system, we may have multiple opportunities to interact with the same user u. We can then develop a more precise estimate of u’s group based on their responses to previous stimuli.",4.2 Predicting user behavior,[0],[0]
Say that we have made the previous observations D(u) =,4.2 Predicting user behavior,[0],[0]
"{〈s1, b1〉, . . .",4.2 Predicting user behavior,[0],[0]
", 〈sN , bN 〉} for user u.",4.2 Predicting user behavior,[0],[0]
"Then we can use Bayes’ theorem to calculate a posterior estimate for u’s group membership:
P ( g|D(u); θ ) ∝",4.2 Predicting user behavior,[0],[0]
P ( D(u)|ρ(g) ) ·,4.2 Predicting user behavior,[0],[0]
"P (g|π) (7)
",4.2 Predicting user behavior,[0],[0]
This posterior balances whether a group is likely in general against whether members of that group behave as u does.,4.2 Predicting user behavior,[0],[0]
"We can use Pu(g) = P ( g|D(u); θ ) as our new estimate for the group membership probabilities for u and replace (6) with: P ( b|s,D(u); θ ) =
K∑ g=1 P ( b|s; ρ(g) ) ·",4.2 Predicting user behavior,[0],[0]
"Pu(g) (8)
for the next interaction with u. An NLG system can therefore adapt to each new user over time.",4.2 Predicting user behavior,[0],[0]
"Before the first interaction with u, it has no specific information about u and models u’s behavior based on (6).",4.2 Predicting user behavior,[0],[0]
"As the system interacts with u repeatedly, it collects observationsD(u) about u’s behavior.",4.2 Predicting user behavior,[0],[0]
"This allows it to calculate an increasingly accurate posterior Pu(g) = P ( g|D(u); θ ) of u’s group membership, and thus generate utterances which are more and more suitable to u using (8).",4.2 Predicting user behavior,[0],[0]
"So far we have not discussed how to find settings for the parameters θ = π, ρ(1), . . .",5 Training,[0],[0]
", ρ(K), which define our probability model.",5 Training,[0],[0]
"The key challenge for training is the fact that we want to be able to train while treating the assignment of users to groups as unobserved.
",5 Training,[0],[0]
"We will use a maximum a posteriori estimate for θ, i.e., the setting which maximizes (3) when D is our training set.",5 Training,[0],[0]
"We will first discuss how to pick parameters to maximize only the left part of (3), i.e., the data likelihood, since this is the part that involves unobserved variables.",5 Training,[0],[0]
We will then discuss handling the parameter prior in section 5.2.,5 Training,[0],[0]
"Gradient descent based methods (Nocedal and Wright, 2006) exist for finding the parameter settings which maximize the likelihood for log-linear
models, under the conditions that all relevant variables are observed in the training data.",5.1 Expectation Maximization,[0],[0]
"If group assignments were given, gradient computations, and therefore gradient based maximization, would be straightforward for our model.",5.1 Expectation Maximization,[0],[0]
"One algorithm specifically designed to solve maximization problems with unknown variables by reducing them to the case where all variables are observed, is the expectation maximization (EM) algorithm (Neal and Hinton, 1999).",5.1 Expectation Maximization,[0],[0]
"Instead of maximizing the data likelihood from (3) directly, EM equivalently maximizes the log-likelihood, given in (4).",5.1 Expectation Maximization,[0],[0]
"It helps us deal with unobserved variables by introducing “pseudo-observations” based on the expected frequency of the unobserved variables.
",5.1 Expectation Maximization,[0],[0]
"EM is an iterative algorithm which produces a sequence of parameter settings θ(1), . . .",5.1 Expectation Maximization,[0],[0]
", θ(n).",5.1 Expectation Maximization,[0],[0]
Each will achieve a larger value for (4).,5.1 Expectation Maximization,[0],[0]
Each new setting is generated in two steps: (1) an lower bound on the log-likelhood is generate and (2) the new parameter setting is found by optimizing this lower bound.,5.1 Expectation Maximization,[0],[0]
"To find the lower bound we compute the probability for every possible value the unobserved variables could have had, based on the observed variables and the parameter setting θ(i−1) from the last iteration step.",5.1 Expectation Maximization,[0],[0]
"Then the lower bound essentially assumes that each assignment was seen with a frequency equal to these probabilities - these are the “pseudo-observations”.
",5.1 Expectation Maximization,[0],[0]
In our model the unobserved variables are the assignments of users to groups.,5.1 Expectation Maximization,[0],[0]
"The probability of seeing each user u assigned to a group, given all the data D(u) and the model parameters from the last iteration θ(i−1), is simply the posterior group membership probability P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
The lower bound is then given by (5).,5.1 Expectation Maximization,[0],[0]
"This is the sum of the log probabilities of the data points under each group model, weighted by P ( g|D(u); θ(i−1) ) .",5.1 Expectation Maximization,[0],[0]
We can now use gradient descent techniques to optimize this lower bound.,5.1 Expectation Maximization,[0],[0]
To fully implement EM we need a way to maximize (5).,5.1.1 Maximizing the Lower Bound,[0],[0]
"This can be achieved with gradient based methods such as L-BFGS (Nocedal and Wright, 2006).",5.1.1 Maximizing the Lower Bound,[0],[0]
Here the gradient refers to the vector of all partial derivatives of the function with respect to each dimension of θ.,5.1.1 Maximizing the Lower Bound,[0],[0]
"We therefore need to calculate these partial derivatives.
",5.1.1 Maximizing the Lower Bound,[0],[0]
There are existing implementations of the gradient computations our base model such as in Engonopoulos et al. (2013).,5.1.1 Maximizing the Lower Bound,[0],[0]
"The gradients of (5)
for each of the ρ(g) is simply the gradient for the base model on each datapoint d weighted by P ( g|D(u); θ(i−1) )",5.1.1 Maximizing the Lower Bound,[0],[0]
"if d ∈ Du, i.e., the probability that the user u from which the datapoint originates belongs to group g. We can therefore compute the gradients needed for each ρ(g) by using implementations developed for the base model.
",5.1.1 Maximizing the Lower Bound,[0],[0]
"We also need gradients for the parameters in π, which are only used in our extended model.",5.1.1 Maximizing the Lower Bound,[0],[0]
"We can use the rules for computing derivatives to find, for each dimension g:
∂UL(θ) ∂πg = ∑ u∈U Pu(g)− exp (πg)∑K g′=1 exp ( πg′ )
where Pu(g) = P ( g|D(u); θ(i−1) ) .",5.1.1 Maximizing the Lower Bound,[0],[0]
Using these gradients we can use L-BFGS to maximize the lower bound and implement the EM iteration.,5.1.1 Maximizing the Lower Bound,[0],[0]
So far we have discussed maximization only for the likelihood without accounting for the prior probabilities for every parameter.,5.2 Handling the Parameter Prior,[0],[0]
"To obtain our full training objective we add the log of the right hand side of (3):
log N (π|0, σ(π)) · K∏",5.2 Handling the Parameter Prior,[0],[0]
g=1 N,5.2 Handling the Parameter Prior,[0],[0]
"( ρ(g)|0, σ(ρ) )",5.2 Handling the Parameter Prior,[0],[0]
"i.e., the parameter prior, to (4) and (5).",5.2 Handling the Parameter Prior,[0],[0]
The gradient contribution from these priors can be computed with standard techniques.,5.2 Handling the Parameter Prior,[0],[0]
"We can now implement an EM loop, which maximizes (3) as follows: we randomly pick an initial value θ(0) for all parameters.",5.3 Training Iteration,[0],[0]
Then we repeatedly compute the P ( g|D(u); θ(i−1) ) values and maximize the lower bound using L-BFGS to find θ(i).,5.3 Training Iteration,[0],[0]
This EM iteration is guaranteed to eventually converge towards a local optimum of our objective function.,5.3 Training Iteration,[0],[0]
"Once change in the objective falls below a pre-defined threshold, we keep the final θ setting.
",5.3 Training Iteration,[0],[0]
"For our implementation we make a small improvement to the approach: L-BFGS is itself an iterative algorithm and instead of running it until convergence every time we need to find a new θ(i), we only let it take a few steps.",5.3 Training Iteration,[0],[0]
"Even if we just took a single L-BFGS step in each iteration, we would still obtain a correct algorithm (Neal and
Hinton, 1999) and this has the advantage that we do not spend time trying to find a θ(i) which is a good fit for the likely poor group assignments P ( g|D(u); θ(i−1) )",5.3 Training Iteration,[0],[0]
we obtain from early parameter estimates.,5.3 Training Iteration,[0],[0]
Our model can be used in any component of a dialog system for which a prediction of the user’s behavior is needed.,6 Evaluation,[0],[0]
"In this work, we evaluate it in two NLG-related prediction tasks: RE production and RE comprehension.",6 Evaluation,[0],[0]
In both cases we evaluate the ability of our model to predict the user’s behavior given a stimulus.,6 Evaluation,[0],[0]
"We expect our user-group model to gradually improve its prediction accuracy compared to a generic baseline without user groups as it sees more observations from a given user.
",6 Evaluation,[0],[0]
In all experiments described below we set the prior variances σγ = 1.0 and σπ = 0.3 after trying out values between 0.1 and 10 on the training data of the comprehension experiment.,6 Evaluation,[0],[0]
"Task The task of RE generation can be split in two steps: attribute selection, the selection of the visual attributes to be used in the RE such as color, size, relation to other objects and surface realization, the generation of a full natural language expression.",6.1 RE production,[0],[0]
"We focus here on attribute selection: given a visual scene and a target object, we want to predict the set of attributes of the target object that a human speaker would use in order to describe it.",6.1 RE production,[0],[0]
"Here we treat attribute selection in terms of individual classification decisions on whether to use each attribute, as described in Section 3.",6.1 RE production,[0],[0]
"More specifically, we focus on predicting whether the speaker will use a spatial relation to another object (“landmark”).",6.1 RE production,[0],[0]
"Our motivation for choosing this attribute stems from the fact that previous authors (Viethen and Dale, 2008; Ferreira and Paraboni, 2014) have found substantial variation between different users with respect to their preference towards using spatial relations.
",6.1 RE production,[0],[0]
"Data We use the GRE3D3 dataset of humanproduced REs (Viethen and Dale, 2010), which contains 630 descriptions for 10 scenes collected from 63 users, each describing the same target object in each scene.",6.1 RE production,[0],[0]
35% of the descriptions in this corpus use a spatial relation.,6.1 RE production,[0],[0]
"An example of such a scene can be seen in Fig. 3.
",6.1 RE production,[0],[0]
"Models We use two baselines for comparison:
Basic: The state-of-the-art model on this task with this dataset, under the assumption that users are seen in training, is presented in Ferreira and Paraboni (2014).",6.1 RE production,[0],[0]
"They define context features such as type of relation between the target object and its landmark, number of object of the same color or size, etc., then train an SVM classifier to predict the use of each attribute.",6.1 RE production,[0],[0]
"We recast their model in terms of a log-linear model with the same features, to make it fit with the setup of Section 3.
Basic++: Ferreira and Paraboni (2014) also take speaker features into account.",6.1 RE production,[0],[0]
"We do not use speaker identity and the speaker’s attribute frequency vector, because we only evaluate on unseen users.",6.1 RE production,[0],[0]
"We do use their other speaker features (age, gender), together with Basic’s context features; this gives us a strong baseline which is aware of manually annotated user group characteristics.
",6.1 RE production,[0],[0]
"We compare these baselines to our Group model for values of K between 1 and 10, using the exact same features as Basic.",6.1 RE production,[0],[0]
"We do not use the speaker features of Basic++, because we do not want to rely on manually annotated groups.",6.1 RE production,[0],[0]
"Note that our results are not directly comparable with those of Ferreira and Paraboni (2014), because of a different training-test split: their model requires having seen speakers in training, while we explicitly want to test our model’s ability to generalize to unseen users.
",6.1 RE production,[0],[0]
"Experimental setup We evaluate using crossvalidation, splitting the folds so that all speakers we see in testing are previously unseen in training.",6.1 RE production,[0],[0]
We use 9 folds in order to have folds of the same size (each containing 70 descriptions coming from 7 speakers).,6.1 RE production,[0],[0]
At each iteration we train on 8 folds and test on the 9th.,6.1 RE production,[0],[0]
"At test time, we process each test instance iteratively: we first predict for each instance whether the user uwould use a spatial relation or not and test our prediction; we then add the
actual observation from the corpus to the set D(u) of observations for this particular user, in order to update our estimate about their group membership.
",6.1 RE production,[0],[0]
"Results Figure 4 shows the test F1-score (microaveraged over all folds) as we increase the number of groups, compared to the baselines.",6.1 RE production,[0],[0]
"For our Group models, these are averaged over all interactions with the user.",6.1 RE production,[0],[0]
"Our model gets F1-scores between 0.69 and 0.76 for all values ofK > 1, outperforming both Basic (0.22) and Basic++ (0.23).
",6.1 RE production,[0],[0]
"In order to take a closer look at our model’s behavior, we also show the accuracy of our model as it observes more instances at test time.",6.1 RE production,[0],[0]
We compare the model with K = 3 groups against the two baselines.,6.1 RE production,[0],[0]
"Figure 5 shows that the group model’s F1-score increases dramatically after the first two observations and then stays high throughout the test phase, always outperforming both baselines by at least 0.37 F1-score points after the first observation.",6.1 RE production,[0],[0]
The baseline models of course are not expected to improve with time; fluctuations are due to differences between the visual scenes.,6.1 RE production,[0],[0]
"In the same figure, we plot the evolution of the entropy of the group model’s posterior distribution over the groups (see (7)).",6.1 RE production,[0],[0]
"As expected, the model is highly uncertain at the beginning of the test phase about which group the user belongs to, then gets more and more certain as the set D(u) of observations from that user grows.",6.1 RE production,[0],[0]
Task Our next task is to predict the referent to which a user will resolve an RE in the context of a visual scene.,6.2 RE comprehension,[0],[0]
"Our model is given a stimulus s = (r, c) consisting of an instruction containing an RE r and a visual context c and outputs a probability distribution over all possible referents b.",6.2 RE comprehension,[0],[0]
"Such a model can be used by a probabilistic RE generator to select an RE which is highly likely to be correctly understood by the user or predict potential
misunderstandings (see Section 3).
",6.2 RE comprehension,[0],[0]
Data We use the GIVE-2.5 corpus for training and the GIVE-2 corpus for testing our model (the same used by Engonopoulos et al. (2013)).,6.2 RE comprehension,[0],[0]
These contain recorded observations of dialog systems giving instructions to users who play a game in a 3D environment.,6.2 RE comprehension,[0],[0]
"Each instruction contains an RE r, which is recorded in the data together with the visual context c at the time the instruction was given.",6.2 RE comprehension,[0],[0]
The object b which the user understood as the referent of the RE is inferred by the immediately subsequent action of the user.,6.2 RE comprehension,[0],[0]
"In total, we extracted 2927 observations by 403 users from GIVE-25 and 5074 observations by 563 users from GIVE-2.
",6.2 RE comprehension,[0],[0]
Experimental setup We follow the training method described in Section 3.,6.2 RE comprehension,[0],[0]
"At test time, we present the observations from each user in the order they occur in the test data; for each stimulus, we ask our models to predict the referent a which the user understood to be the referent of the RE, and compare with the recorded observation.",6.2 RE comprehension,[0],[0]
"We subsequently add the recorded observation to the dataset for the user and continue.
",6.2 RE comprehension,[0],[0]
"Models As a baseline, we use the Basic model described in Section 3, with the features of the “semantic” model of Engonopoulos et al. (2013).",6.2 RE comprehension,[0],[0]
"Those features capture information about the objects in the visual scene (e.g. salience) and some basic semantic properties of the RE (e.g. color, position).",6.2 RE comprehension,[0],[0]
"We use those features for our Group model as well, and evaluate for K between 1 and 10.
Results on GIVE data Basic had a test accuracy of 72.70%, which was almost identical with the accuracy of our best Group model for K = 6 (72.78%).",6.2 RE comprehension,[0],[0]
This indicates that our group model does not differentiate between users.,6.2 RE comprehension,[0],[0]
"Indeed, after training, the 6-group model assigns 81% prior probabil-
ity to one of the groups, and effectively gets stuck with this assignment while testing; the mean entropy of the posterior group distribution only falls from an initial 1.1 to 0.7 after 10 observations.
",6.2 RE comprehension,[0],[0]
We speculate that the reason behind this is that the features we use are not sensitive enough to capture the differences between the users in this data.,6.2 RE comprehension,[0],[0]
"Since our model relies completely on observable behavior, it also relies on the ability of the features to make relevant distinctions between users.
",6.2 RE comprehension,[0],[0]
"Results on synthetic data In order to test this hypothesis, we made a synthetic dataset based on the GIVE datasets with 1000 instances from 100 users, in the following way: for each user, we randomly selected 10 scenes from GIVE-2, and replaced the target the user selected, so that half of the users always select the target with the highest visual salience, and the other half always select the one with the lowest.",6.2 RE comprehension,[0],[0]
"Our aim was to test whether our model is capable of identifying groups when they are clearly present in the data and exhibit differences which our features are able to capture.
",6.2 RE comprehension,[0],[0]
We evaluated the same models in a 2-fold crossvalidation.,6.2 RE comprehension,[0],[0]
Figure 6 shows the prediction accuracy for Basic and the Group models for K from 1 to 10.,6.2 RE comprehension,[0],[0]
"All models for K > 1 clearly outperform the baseline model: the 2-group model gets 62.3% vs 28.6% averaged over all test examples, while adding more than two groups does not further improve the accuracy.",6.2 RE comprehension,[0],[0]
We also show in Figure 7 the evolution of the accuracy asD(u) grows: the Group model with K = 2 reaches a 64% testing accuracy after seeing two observations from the same user.,6.2 RE comprehension,[0],[0]
"In the same figure, the entropy of the posterior distribution over groups (see production experiment) falls towards zero as D(u) grows.",6.2 RE comprehension,[0],[0]
"These results show that our model is capable of correctly assigning a user to the group they belong to, once the features are adequate for distinguishing between different user behaviors.",6.2 RE comprehension,[0],[0]
"Our model was shown to be successful in discovering groups of users with respect to their behavior, within datasets which present discernible user variation.",6.3 Discussion,[0],[0]
"In particular, if all listeners are influenced in a similar way by e.g. the visual salience of an object, then the group model cannot learn different weights for the visual salience feature; if this happens for all available features, there are effectively no groups for our model to discover.
",6.3 Discussion,[0],[0]
"Once the groups have been discovered, our model can then very quickly distinguish between them at test time.",6.3 Discussion,[0],[0]
This is reflected in the steep performance improvement even after the first user observation in both the real data experiment in 6.1 and the synthetic data experiment in 6.2.,6.3 Discussion,[0],[0]
"We have presented a probabilistic model for NLG which predicts the behavior of individual users of a dialog system by dynamically assigning them to user groups, which were discovered during training2.",7 Conclusion,[0],[0]
"We showed for two separate NLG-related tasks, RE production and RE comprehension, how our model, after being trained with data that is not annotated with user groups, can quickly adapt to unseen users as it gets more observations from them in the course of a dialog and makes increasingly accurate predictions about their behavior.
",7 Conclusion,[0],[0]
"Although in this work we apply our model to tasks related to NLG, nothing hinges on this choice; it can also be applied to any other dialog-related prediction task where user variation plays a role.",7 Conclusion,[0],[0]
"In the future, we will also try to apply the basic principles of our user group approach to more sophisticated underlying models, such as neural networks.
2Our code and data is available in https://bit.ly/ 2jIu1Vm",7 Conclusion,[0],[0]
We present a model which predicts how individual users of a dialog system understand and produce utterances based on user groups.,abstractText,[0],[0]
"In contrast to previous work, these user groups are not specified beforehand, but learned in training.",abstractText,[0],[0]
"We evaluate on two referring expression (RE) generation tasks; our experiments show that our model can identify user groups and learn how to most effectively talk to them, and can dynamically assign unseen users to the correct groups as they interact with the system.",abstractText,[0],[0]
Discovering User Groups for Natural Language Generation,title,[0],[0]
"As originally defined by Pearl (1988), Bayesian networks express joint distributions over finite sets of random variables as products of conditional distributions.",1. Introduction,[0],[0]
"Probabilistic programming languages (PPLs) (Koller et al., 1997; Milch et al., 2005a; Goodman et al., 2008; Wood et al., 2014b) apply the same idea to potentially infinite sets of variables with general dependency structures.",1. Introduction,[0],[0]
"Thanks to their expressive power, PPLs have been used to solve many real-world applications, including Captcha (Le et al., 2017), seismic monitoring (Arora et al., 2013), 3D pose estimation (Kulkarni et al., 2015), generating design suggestions (Ritchie et al., 2015), concept learning (Lake et al., 2015), and cognitive science applications (Stuhlmüller & Goodman, 2014).
",1. Introduction,[0],[0]
"In practical applications, we often have to deal with a mix-
1University of California, Berkeley 2Arizona State University 3Vicarious Inc. 4Carnegie Mellon University.",1. Introduction,[0],[0]
"Correspondence to: Yi Wu <jxwuyi@gmail.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
ture of continuous and discrete random variables.",1. Introduction,[0],[0]
"Existing PPLs support both discrete and continuous random variables, but not discrete-continuous mixtures, i.e., variables whose distributions combine discrete and continuous elements.",1. Introduction,[0],[0]
"Such variables are fairly common in practical applications: sensors that have thresholded limits, e.g. thermometers, weighing scales, speedometers, pressure gauges; or a hybrid sensor that can report a either real value or an error condition.",1. Introduction,[0],[0]
"The occurrence of such variables has been noted in many other applications from a wide range of scientific domains (Kharchenko et al., 2014; Pierson & Yau, 2015; Gao et al., 2017).
",1. Introduction,[0],[0]
"Many PPLs have a restricted syntax that forces the expressed random variables to be either discrete or continuous, including WebPPL (Goodman & Stuhlmüller, 2014), Edward (Tran et al., 2016), Figaro (Pfeffer, 2009) and Stan (Carpenter et al., 2016).",1. Introduction,[0],[0]
"Even for PPLs whose syntax allows for mixtures of discrete and continuous variables, such as BLOG (Milch et al., 2005a), Church (Goodman, 2013), Venture (Mansinghka et al., 2014) and Anglican (Wood et al., 2014a), the underlying semantics of these PPLs implicitly assumes the random variables are not mixtures.",1. Introduction,[0],[0]
"Moreover, the inference algorithms associated with the semantics inherit the same assumption and can produce incorrect results when discrete-continuous mixtures are used.
",1. Introduction,[0],[0]
"Consider the following GPA example: a two-variable Bayes net Nationality → GPA where the nationality follows a binary distribution
P (Nationality = USA) = P (Nationality = India)",1. Introduction,[0],[0]
"= 0.5
and the conditional probabilities are discrete-continuous mixtures
GPA|Nationality = USA ∼0.01 · 1 {GPA = 4}+ 0.99 · Unif(0, 4),
GPA|Nationality = India ∼0.01 · 1 {GPA = 10}+ 0.99 · Unif(0, 10).
",1. Introduction,[0],[0]
This is a typical scenario in practice because many top students have perfect GPAs.,1. Introduction,[0],[0]
Now suppose we observe a student with a GPA of 4.0.,1. Introduction,[0],[0]
Where do they come from?,1. Introduction,[0],[0]
"If the student is Indian, the probability of any singleton set {g}
where 0 <",1. Introduction,[0],[0]
"g < 10 is zero, as this range has a probability density.",1. Introduction,[0],[0]
"On the other hand if the student is American, the set {4} has the probability 0.01.",1. Introduction,[0],[0]
"Thus, by Bayes theorem, P (Nationality = USA|GPA = 4) = 1, which means the student must be from the USA.
",1. Introduction,[0],[0]
"However, if we run the default Bayesian inference algorithm for this problem in PPLs, e.g., the standard importance sampling algorithm (Milch et al., 2005b), a sample that picks India receives a density weight of 0.99/10.0 = 0.099, whereas one that picks USA receives a discrete-mass weight of 0.01.",1. Introduction,[0],[0]
"Since the algorithm does not distinguish probability density and mass, it will conclude that the student is very probably from India, which is far from the truth.
",1. Introduction,[0],[0]
"We can fix the GPA example by considering a density weight infinitely smaller than a discrete-mass weight (Nitti et al., 2016; Tolpin et al., 2016).",1. Introduction,[0],[0]
"However, the situation becomes more complicated when involving more than one evidence variable, e.g., GPAs over multiple semesters for students who may study in both countries.",1. Introduction,[0],[0]
Vector-valued variables also cause problems—does a point mass in three dimensions count more or less than a point mass in two dimensions?,1. Introduction,[0],[0]
"These practical issues motivate the following two tasks:
• Inherit all the existing properties of PPL semantics and extend it to handle random variables with mixed discrete and continuous distributions;
• Design provably correct inference algorithms for the extended semantics.
",1. Introduction,[0],[0]
"In this paper, we carry out all these two tasks and implement the extended semantics as well as the new algorithms in a widely used PPL, Bayesian Logic (BLOG) (Milch et al., 2005a).",1. Introduction,[0],[0]
Measure-Theoretical Bayesian Nets (MTBNs) Measure theory can be applied to handle discrete-continuous mixtures or even more abstract measures.,1.1. Main Contributions,[0],[0]
"In this paper, we define a generalization of Bayesian networks called measure-theoretic Bayesian networks (MTBNs) and prove that every MTBN represents a unique measure on the input space.",1.1. Main Contributions,[0],[0]
"We then show how MTBNs can provide a more general semantic foundation for PPLs.
More concretely, MTBNs support (1) random variables with infinitely (even uncountably) many parents, (2) random variables valued in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed), (3) establishment of conditional independencies implied by an infinite graph, and (4) open-universe semantics in terms of the possible worlds in the vocabulary of the model.
",1.1. Main Contributions,[0],[0]
Inference Algorithms,1.1. Main Contributions,[0],[0]
"We propose a provably correct inference algorithm, lexicographic likelihood weighting (LLW), for general MTBNs with discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
"In addition, we propose LPF, a particle-filtering variant of LLW for sequential Monte Carlo (SMC) inference on state-space models.
",1.1. Main Contributions,[0],[0]
"Incorporating MTBNs into an existing PPL We incorporate MTBNs into BLOG with simple modifications and then define the generalized BLOG language, measuretheoretic BLOG, which formally supports arbitrary distributions, including discrete-continuous mixtures.",1.1. Main Contributions,[0],[0]
We prove that every generalized BLOG model corresponds to a unique MTBN.,1.1. Main Contributions,[0],[0]
"Thus, all the desired theoretical properties of MTBNs can be carried to measure-theoretic BLOG.",1.1. Main Contributions,[0],[0]
We also implement the LLW and LPF algorithms in the backend of measure-theoretic BLOG and use three representative examples to show their effectiveness.,1.1. Main Contributions,[0],[0]
This paper is organized as follows.,1.2. Organization,[0],[0]
We first discuss related work in Section 2.,1.2. Organization,[0],[0]
"In Section 3, we formally define measure-theoretic Bayesian nets and study their theoretical properties.",1.2. Organization,[0],[0]
Section 4 describes the LLW and LPF inference algorithms for MTBNs with discrete-continuous mixtures and establishes their correctness.,1.2. Organization,[0],[0]
"In Section 5, we introduce the measure-theoretic extension of BLOG and study its theoretical foundations for defining probabilistic models.",1.2. Organization,[0],[0]
"In Section 6, we empirically validate the generalized BLOG system and the new inference algorithms on three representative examples.",1.2. Organization,[0],[0]
"The motivating GPA example has been also discussed as a special case under some other PPL systems (Tolpin et al., 2016; Nitti et al., 2016).",2. Related Work,[0],[0]
Tolpin et al. (2016) and Nitti et al. (2016) proposed different solutions specific to this example but did not address the general problems of representation and inference with random variables with mixtures of discrete and continuous distributions.,2. Related Work,[0],[0]
"In contrast, we present a general formulation with provably correct inference algorithms.
",2. Related Work,[0],[0]
"Our approach builds upon the foundations of the BLOG probabilistic programming language (Milch, 2006).",2. Related Work,[0],[0]
We use a measure theoretic formulation to generalize the syntax and semantics of BLOG to random variables that may have infinitely many parents and mixed continuous and discrete distributions.,2. Related Work,[0],[0]
"The BLP framework Kersting & De Raedt (2007) unifies logic programming with probability models, but requires each random variable to be influenced by a finite set of random variables in order to define the semantics.",2. Related Work,[0],[0]
"This amounts to requiring only finitely many ances-
tors of each random variable.",2. Related Work,[0],[0]
Choi et al. (2010) present an algorithm for carrying out lifted inference over models with purely continuous random variables.,2. Related Work,[0],[0]
"They also require parfactors to be functions over finitely many random variables, thus limiting the set of influencing variables for each node to be finite.",2. Related Work,[0],[0]
Gutmann et al. (2011a) also define densities over finite dimensional vectors.,2. Related Work,[0],[0]
"In a relatively more general formulation (Gutmann et al., 2011b) define the distribution of each random variable using a definite clause, which corresponds to the limitation that each random variable (either discrete or continuous) has finitely many parents.",2. Related Work,[0],[0]
Frameworks building on Markov networks also have similar restrictions.,2. Related Work,[0],[0]
"Wang & Domingos (2008) only consider networks of finitely many random variables, which can have either discrete or continuous distributions.",2. Related Work,[0],[0]
"Singla & Domingos (2007) extend Markov logic to infinite (non-hybrid) domains, provided that each random variable has only finitely many influencing random variables.
",2. Related Work,[0],[0]
"In contrast, our approach not only allows models with arbitrarily many random variables with mixed discrete and continuous distributions, but each random variable can also have arbitrarily many parents as long as all ancestor chains are finite (but unbounded).",2. Related Work,[0],[0]
"The presented work constitutes a rigorous framework for expressing probability models with the broadest range of cardinalities (uncountably infinite parent sets) and nature of random variables (discrete, mixed, and even arbitrary measure spaces), with clear semantics in terms of first-order possible worlds and the generalization of conditional independences on such models.
",2. Related Work,[0],[0]
"Lastly, there are also other works using measure-theoretic approaches to analyze the semantics properties of probabilistic programs but with different emphases, such as the commutativity (Staton, 2017), design choices for monad structures (Ramsey, 2016) and computing a disintegration (Shan & Ramsey, 2017).",2. Related Work,[0],[0]
"In this section, we introduce measure-theoretic Bayesian networks (MTBNs) and prove that an MTBN represents a unique measure with desired theoretical properties.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We assume familiarity with measure-theoretic approaches to probability theory.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Some background is included in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A.
We begin with some necessary definitions of graph theory.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.1.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A digraph G is a pair G = (V,E) of a set of vertices V , of any cardinality, and a set of directed edges E ⊆ V × V .",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The notation u→ v denotes (u, v) ∈ E, and u 7→ v denotes the existence of a path from u to v in G.
Definition 3.2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"A vertex v ∈ V is a root vertex if there are no incoming edges to it, i.e., there is no u ∈ V such that u → v. Let pa(v)",3. Measure-Theoretic Bayesian Networks,[0],[0]
"= {u ∈ V : u → v} denote the set of parents of a vertex v ∈ V , and nd(v) =",3. Measure-Theoretic Bayesian Networks,[0],[0]
"{u ∈ V : not v 7→
u} denote its set of non-descendants.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.3.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A well-founded digraph (V,E) is one with no countably infinite ancestor chain v0 ← v1 ← v2 ← . . .",3. Measure-Theoretic Bayesian Networks,[0],[0]
".
",3. Measure-Theoretic Bayesian Networks,[0],[0]
This is the natural generalization of a finite directed acyclic graph to the infinite case.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Now we are ready to give the key definition of this paper.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.4.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"A measure-theoretic Bayesian network M = (V,E, {Xv}v∈V , {Kv}v∈V ) consists of (a) a wellfounded digraph (V,E) of any cardinality, (b) an arbitrary measurable space Xv for each v ∈ V , and (c) a probability kernel Kv from ∏ u∈pa(v) Xu to Xv for each v ∈ V .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"By definition, MTBNs allow us to define very general and abstract models with the following two major benefits:
1.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables with infinitely (even uncountably) many parents because MTBN is defined on a well-founded digraph.
2.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"We can define random variables in arbitrary measure spaces (with RN as one case) distributed according to any measure (including discrete, continuous and mixed).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Next, we related MTBN to a probability measure.",3. Measure-Theoretic Bayesian Networks,[0],[0]
"Fix an MTBN M = (V,E, {Xv}v∈V , {Kv}v∈V ).",3. Measure-Theoretic Bayesian Networks,[0],[0]
For U ⊆ V let XU = ∏ u∈U Xu be the product measurable space over variables u ∈ U .,3. Measure-Theoretic Bayesian Networks,[0],[0]
"With this notation, Kv is a kernel from Xpa(v) to Xv.",3. Measure-Theoretic Bayesian Networks,[0],[0]
Whenever W ⊆ U let πUW : XU → XW denote the projection map.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Let XV be our base measurable space upon which we will consider different probability measures µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Let Xv for v ∈ V denote both the underlying set of Xv and the random variable given by the projection πV{v}, and XU for U ⊆ V the underlying space of XU and the random variable given by the projection πVU .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Definition 3.5.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a measure µ on XV , if for all v ∈ V :
• Xv is conditionally independent of its non-descendants Xnd(v) given its parents Xpa(v).",3. Measure-Theoretic Bayesian Networks,[0],[0]
"• Kv(Xpa(v), A) = Pµ[Xv ∈ A|Xpa(v)] holds almost surely for any A ∈ Xv, i.e., Kv is a version of the conditional distribution of Xv given its parents.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Def. 3.5 captures the generalization of the local properties of Bayes networks – conditional independence and conditional distributions defined by parent-child relationships.,3. Measure-Theoretic Bayesian Networks,[0],[0]
Here we assume the conditional probability exists and is unique.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"This is a mild condition because this holds as long as the probability space is regular (Kallenberg, 2002).
",3. Measure-Theoretic Bayesian Networks,[0],[0]
"The next theorem shows that MTBNs are well-defined.
",3. Measure-Theoretic Bayesian Networks,[0],[0]
Theorem 3.6.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"An MTBN M represents a unique measure µ on XV .
",3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof of theorem 3.6 requires several intermediate results and is presented in Appx.,3. Measure-Theoretic Bayesian Networks,[0],[0]
B.,3. Measure-Theoretic Bayesian Networks,[0],[0]
The proof proceeds by first defining a projective family of measures.,3. Measure-Theoretic Bayesian Networks,[0],[0]
This gives a way to recursively construct our measure µ.,3. Measure-Theoretic Bayesian Networks,[0],[0]
We then define a notion of consistency such that every consistent projective family constructs a measure that M represents.,3. Measure-Theoretic Bayesian Networks,[0],[0]
"Lastly, we give an explicit characterization of the unique consistent projective family, and thus of the unique measure M represents.",3. Measure-Theoretic Bayesian Networks,[0],[0]
We introduce the lexicographic likelihood weighting (LLW) algorithm for provably correct inference on MTBNs.,4. Generalized Inference Algorithms,[0],[0]
We also present lexicographic particle filter (LPF) for statespace models by adapting LLW for the sequential Monte Carlo (SMC) framework.,4. Generalized Inference Algorithms,[0],[0]
"Suppose we have an MTBN with finitely many random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XN , and that, without loss of generality, we observe real-valued random variables X1, . . .",4.1. Lexicographic likelihood weighting,[0],[0]
", XM for M < N as evidence.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose the distribution of Xi given its parents Xpa(i) is a mixture between a density fi(xi|xpa(i)) with respect to the Lebesgue measure and a discrete distribution Fi(xi|xpa(i)), i.e., for any > 0, we have P (Xi ∈",4.1. Lexicographic likelihood weighting,[0],[0]
"[xi − , xi]|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"=∑ x∈[xi− ,xi] Fi(xi|Xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
+ ∫,4.1. Lexicographic likelihood weighting,[0],[0]
xi xi− fi(x|Xpa(i)),4.1. Lexicographic likelihood weighting,[0],[0]
dx.,4.1. Lexicographic likelihood weighting,[0],[0]
This implies that Fi(xi|xpa(i)) is nonzero for at most countably many values,4.1. Lexicographic likelihood weighting,[0],[0]
xi.,4.1. Lexicographic likelihood weighting,[0],[0]
"If Fi is nonzero for finitely many points, it can be represented by a list of those points and their values.
",4.1. Lexicographic likelihood weighting,[0],[0]
"Lexicographic Likelihood Weighting (LLW) extends the classical likelihood weighting (Milch et al., 2005b) to this setting.",4.1. Lexicographic likelihood weighting,[0],[0]
"It visits each node of the graph in topological order, sampling those variables that are not observed, and accumulating a weight for those that are observed.",4.1. Lexicographic likelihood weighting,[0],[0]
"In particular, at an evidence variable Xi we update a tuple (d,w) of the number of densities and a weight, initially (0, 1), by:
(d,w)← { (d,wFi(xi|xpa(i))) Fi(xi|xpa(i))",4.1. Lexicographic likelihood weighting,[0],[0]
"> 0, (d+ 1, wfi(xi|xpa(i)))",4.1. Lexicographic likelihood weighting,[0],[0]
"otherwise.
(1)
Finally, having K samples x(1), . .",4.1. Lexicographic likelihood weighting,[0],[0]
.,4.1. Lexicographic likelihood weighting,[0],[0]
", x(K) by this process and accordingly a tuple (d(i), w(i)) for each sample x(i), let d∗ = mini:w(i) 6=0 d
(i) and estimate E[f(X)|X1:M ] by∑ {i:d(i)=d∗} w
(i) f(x(i))∑",4.1. Lexicographic likelihood weighting,[0],[0]
{i:d(i)=d∗} w (i) .,4.1. Lexicographic likelihood weighting,[0],[0]
"(2)
The algorithm is summarised in Alg. 1",4.1. Lexicographic likelihood weighting,[0],[0]
The next theorem shows this procedure is consistent.,4.1. Lexicographic likelihood weighting,[0],[0]
Theorem 4.1.,4.1. Lexicographic likelihood weighting,[0],[0]
"LLW is consistent: (2) converges almost surely to E[f(X)|X1:M ].
",4.1. Lexicographic likelihood weighting,[0],[0]
"Algorithm 1 Lexicographic Likelihood Weighting Require: densities f , masses F , evidences E, and K.
for i = 1 . .",4.1. Lexicographic likelihood weighting,[0],[0]
.K,4.1. Lexicographic likelihood weighting,[0],[0]
"do sample all the ancestors of E from prior compute (d(i), w(i)) by Eq.",4.1. Lexicographic likelihood weighting,[0],[0]
(1) end for d?,4.1. Lexicographic likelihood weighting,[0],[0]
"← mini:w(i) 6=0 d(i)
",4.1. Lexicographic likelihood weighting,[0],[0]
"Return (∑
i:d(i)=d? w (i)f(x(i))
) /",4.1. Lexicographic likelihood weighting,[0],[0]
(∑ i:d(i)=d?,4.1. Lexicographic likelihood weighting,[0],[0]
"w (i) )
",4.1. Lexicographic likelihood weighting,[0],[0]
"In order to prove Theorem 4.1, the main technique we adopt is to use a more restricted algorithm, the Iterative Refinement Likelihood Weighting (IRLW) as a reference.",4.1. Lexicographic likelihood weighting,[0],[0]
"Suppose we want to approximate the posterior distribution of an X -valued random variable X conditional on a Yvalued random variable Y , for arbitrary measure spaces X and Y .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In general, there is no notion of a probability density of Y given X for weighing samples.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If, however, we could make a discrete approximation Yt of Y then we could weight samples by the probability P",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
[Yt = yt|X].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"If we increase the accuracy of the approximation with the number of samples, this should converge in the limit.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"We show this is possible, if we are careful about how we approximate:
Definition 4.2.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
An approximation scheme for a measurable space Y consists of a measurable spaceA and measurable approximation functions αi :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y → A for i = 1, 2, . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
and αji :,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
A → A for i < j such that αj ◦ α,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"i = αi and y can be measurably recovered from the subsequence αt(y), αt+1(y), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"for any t > 0.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"When Y is a real-valued variable we will use the approximation scheme αn(y) = 2−nd2nye where dre denotes the ceiling of r, i.e., the smallest integer no smaller than it.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Observe in this case that P (αn(Y ) = αn(y)),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= P (αn(y)− 2−n < Y ≤ αn(y)) which we can compute from the CDF of Y .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Lemma 4.3.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"IfX,Y are real-valued random variables with E |X| <∞, then limi→∞ E[X|αi(Y )]",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= E[X|Y ].
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Proof.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Let Fi = σ(αi(Y )),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
be the sigma algebra generated by αi(Y ).,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Whenever i ≤,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
j,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
we have αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
= (αj ◦αji )(Y ) and so Fi ⊆ Fj .,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"This means E[X|αi(Y )] = E[X|Fi] is a martingale, so we can use martingale convergence results.",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"In particular, since E |X| <∞
E[X|Fi]→ E[X|F∞] a.s. and in L1, where F∞ =",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
⋃,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
i Fi is the sigma-algebra generated by {αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
": i ∈ N} (see Theorem 7.23 in (Kallenberg, 2002)).
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Y is a measurable function of the sequence (α1(Y ), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"),",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
as limi→∞ αi(Y ),4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"= Y , and so σ(Y ) ⊆ F∞. By definition
the sequence is a measurable function of Y , and so F∞ ⊆ σ(Y ), and so E[X|F∞] = E[X|Y ] giving our result.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Iterative refinement likelihood weighting (IRLW) samples x(1), . . .",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
", x(K)",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
from the prior,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"and evaluates:
∑K i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i))f(x(i))∑K
i=1",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"P (αn(Y )|X = x(i)) (3)
Using Lemma 4.3, G.12, and G.13, we can show IRLW is consistent.
",4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
Theorem 4.4.,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
IRLW is consistent: (3) converges almost surely to E[f(X)|Y ].,4.1.1. ITERATIVE REFINEMENT LIKELIHOOD WEIGHTING,[0],[0]
"Now we are ready to prove Theorem 4.1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
Proof of Theorem 4.1.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
We prove the theorem for evidence variables that are leaves It is straightforward to extend the proof when the evidence variables are non-leaf nodes.,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Let x be a sample produced by the algorithm with number of densities and weight (d,w).",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
With In = ∏ i=1...,4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"M (αn(xi)− 2−n, αn(xi)] a 2−n-cube around x1:M we have
lim n→∞ P (X1:M ∈ In|XM+1:N = xM+1:N )",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"w 2−dn = 1.
",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Using In as an approximation scheme by Def. 4.2, the numerator in the above limit is the weight used by IRLW.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"But given the above limit, using w 2−dn as the weight will give the same result in the limit.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"Then if we have K samples, in the limit of n→∞ only those samples x(i) with minimal d(i) will contribute to the estimation, and up to normalization they will contribute weight w(i) to the estimation.",4.1.2. PROOF OF THEOREM 4.1,[0],[0]
"We now consider inference in a special class of highdimensional models known as state-space models, and show how LLW can be adapted to avoid the curse of dimensionality when used with such models.",4.2. Lexicographic particle filter,[0],[0]
A state-space model (SSM) consists of latent states {Xt}0≤t≤T and the observations {Yt}0≤t≤T with a special dependency structure where pa(Yt) =,4.2. Lexicographic particle filter,[0],[0]
"Xt and pa(Xt) = Xt−1 for 0 < t ≤ T .
SMC methods (Doucet et al., 2001), also knowns as particle filters, are a widely used class of methods for inference on SSMs.",4.2. Lexicographic particle filter,[0],[0]
"Given the observed variables {Yt}0≤t≤T , the posterior distribution P (Xt|Y0:t) is approximated by a set of K particles where each particle x(k)t represents a sample of {Xi}0≤i≤t.",4.2. Lexicographic particle filter,[0],[0]
"Particles are propagated forward through the transition model P (Xt|Xt−1) and resampled at each time step t according to the weight of each particle, which is defined by the likelihood of observation Yt.
",4.2. Lexicographic particle filter,[0],[0]
"Algorithm 2 Lexicographic Particle Filter (LPF) Require: densities f , masses F , evidences Y , and K
for t = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
", T do for k = 0, . . .",4.2. Lexicographic particle filter,[0],[0]
",K do x
(k) t ← sample from transition
compute (d(k), w(k)) by Eq. 4 end for d?",4.2. Lexicographic particle filter,[0],[0]
← mink:w(k) 6=0 d(k) ∀k : d(k) >,4.2. Lexicographic particle filter,[0],[0]
"d?, w(k) ← 0 Output ( w(k)f(x (k) t ) ) / (∑ k w (k) ) resample particles according to w(k)
end for
In the MTBN setting, the distribution of Yt1 given its parent Xt can be a mixture of density ft(yt|xt) and a discrete distribution Ft(yt|xt).",4.2. Lexicographic particle filter,[0],[0]
"Hence, the resampling step in a particle filter should be accordingly modified: following the idea from LLW, when computing the weight of a particle, we enumerate all the observations yt,",4.2. Lexicographic particle filter,[0],[0]
"i at time step t and again update a tuple (d,w), initially (0,1), by
(d,w)← { (d,wFt(yt,i|xt)) Ft(yt,i|xt) > 0, (d+ 1, wft(yt,i|xt))",4.2. Lexicographic particle filter,[0],[0]
otherwise.,4.2. Lexicographic particle filter,[0],[0]
"(4)
We discard all those particles with a non-minimum d value and then perform the normal resampling step.",4.2. Lexicographic particle filter,[0],[0]
"We call this algorithm lexicographical particle filter (LPF), which is summarized in Alg. 2.
",4.2. Lexicographic particle filter,[0],[0]
The following theorem guarantees the correctness of LPF.,4.2. Lexicographic particle filter,[0],[0]
"Its Proof easily follows the analysis for LLW and the classical proof of particle filtering based on importance sampling.
",4.2. Lexicographic particle filter,[0],[0]
Theorem 4.5.,4.2. Lexicographic particle filter,[0],[0]
LPF is consistent: the outputs of Alg.,4.2. Lexicographic particle filter,[0],[0]
2 converges almost surely to {E[f(Xt)|Y0:t]}0≤t≤T .,4.2. Lexicographic particle filter,[0],[0]
In Section 3 and Section 4 we provided the theoretical foundation of MTBN and general inference algorithms.,5. Generalized Probabilistic Programming Languages,[0],[0]
This section describes how to incorporate MTBN into a practical PPL.,5. Generalized Probabilistic Programming Languages,[0],[0]
"We focus on a widely used open-universe PPL, BLOG (Milch, 2006).",5. Generalized Probabilistic Programming Languages,[0],[0]
"We define the generalized BLOG language, the measure-theoretic BLOG, and prove that every well-formed measure-theoretic BLOG model corresponds to a unique MTBN.",5. Generalized Probabilistic Programming Languages,[0],[0]
"Note that our approach also applies to other PPLs2.
",5. Generalized Probabilistic Programming Languages,[0],[0]
1There can be multiple variables observed.,5. Generalized Probabilistic Programming Languages,[0],[0]
"Here the notation Yt denotes {Yt,i}i for conciseness.
",5. Generalized Probabilistic Programming Languages,[0],[0]
"2It has been shown that BLOG has equivalent semantics to other PPLs (Wu et al., 2014; McAllester et al., 2008).",5. Generalized Probabilistic Programming Languages,[0],[0]
Figure 1.,16 query Nationality(David) = USA;,[0],[0]
"A BLOG code for the GPA example.
",16 query Nationality(David) = USA;,[0],[0]
"We begin with a brief description of the core syntax of BLOG, with particular emphasis on (1) number statements, which are critical for expressing open-universe models3, and (2) new syntax for expressing MTBNs, i.e., the Mix distribution.",16 query Nationality(David) = USA;,[0],[0]
Further description of BLOG’s syntax can be found in Li & Russell (2013).,16 query Nationality(David) = USA;,[0],[0]
Fig. 1 shows a BLOG model with measure-theoretic extensions for a multi-student GPA example.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 1 declares two types, Applicant and Country.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 2 defines 3 distinct countries with keyword distinct, New Zealand, India and USA.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Lines 3 to 5 define a number statement, which states that the number of US applicants follows a Poisson distribution with a higher mean than those from New Zealand or India.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 defines an origin function, which maps the object being generated to the arguments that were used in the number statement that was responsible for generating it.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Here Nationality maps applicants to their nationalities.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 and 13 define two random variables by keyword random.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Lines 7 to 12 state that the GPA of an applicant is distributed as a mixture of weighted discrete and continuous distributions.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"For US applicants, the range of values 0",5.1. Syntax of measure-theoretic BLOG,[0],[0]
< GPA < 4 follows a truncated Gaussian with bounds 0 and 4 (line 9).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
The probability mass outside the range is attributed to the corresponding bounds: P (GPA = 0),5.1. Syntax of measure-theoretic BLOG,[0],[0]
= P,5.1. Syntax of measure-theoretic BLOG,[0],[0]
(,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA = 4) = 10−4 (line 10).,5.1. Syntax of measure-theoretic BLOG,[0],[0]
GPA distributions for other countries are specified similarly.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
Line 13 defines a random applicant David.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 15 states that the David’s GPA is observed to be 4 and we query in line 16 whether David is from USA.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Number Statement (line 3 to 5) Fig. 2 shows the syntax of a number statement for Typei.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In this specification, gj are origin functions (discussed below); ȳj are tuples of arguments drawn from x̄ = x1, . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; ϕj are first-order formulas with free variables ȳj ; ēj are tuples of expressions
3The specialized syntax in BLOG to express models with infinite number of variables.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"over a subset of x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", xk; and cj(ēj) specify kernels κj : Π{Xτe :e∈ēj}Xe → N where τe is the type of the expression e.
#Typei(g1 = x1, . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", gk = xk) ∼",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"if ϕ1(ȳ1) then c1(ē1)
else if ϕ2(ȳ2) then c2(ē2)
.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
". .
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"else cm(ēm);
ments can be recovered using the origin functions gj , each of which is declared as:
origin Typej gj(Typei),
where Typej is the type of the argument xj in the number statement of Typei where gj was used.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"The value of the jth variable used in the number statement that generated u, an element of the universe, is given by gj(u).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Line 6 in Fig. 1 is an example of origin function.
",5.1. Syntax of measure-theoretic BLOG,[0],[0]
Mixture Distribution (line 9 to 12),5.1. Syntax of measure-theoretic BLOG,[0],[0]
"In measure-theoretic BLOG, we introduce a new distribution, the mixture distribution (e.g., lines 9-10 in Fig. 1).",5.1. Syntax of measure-theoretic BLOG,[0],[0]
"A mixture distribution is specified as:
Mix({c1(ē1)→ w1(ē′), . . .",5.1. Syntax of measure-theoretic BLOG,[0],[0]
", ck(ēk)→ wk(ē′)}), where ci are arbitrary distributions, and wi’s are arbitrary real valued functions that sum to 1 for every possible assignment to their arguments: ∀ē′ ∑ i wi(ē
′) =",5.1. Syntax of measure-theoretic BLOG,[0],[0]
1.,5.1. Syntax of measure-theoretic BLOG,[0],[0]
"Note that in our implementation of measure-theoretical BLOG, we only allow a Mix distribution to express a mixture of densities and masses for simplifying the system design, although it still possible to express the same semantics without Mix.",5.1. Syntax of measure-theoretic BLOG,[0],[0]
In this section we present the semantics of measure-theoretic BLOG and its theoretical properties.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Every BLOG model implicitly defines a first-order vocabulary consisting of the set of functions and types mentioned in the model.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"BLOG’s semantics are based on the standard, open-universe semantics of first-order logic.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We first define the set of all possible elements that may be generated for a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.1.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of possible elements UM for a BLOG model M with types {τ1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", τk} is ⋃ j∈N{Uj}, where
• U0 = 〈U01 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U0k 〉, U0j = {cj : cj is a distinct τi constant inM} • Ui+1",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= 〈U i+11 , . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", U i+1 k 〉, where U i+1m = U",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"im ∪
{uν,ū,m : ν(x̄) is a number statement of type τm, ū is a tuple of elements of the type of x̄ from U i, m ∈ N}
Def.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"5.1 allows us to define the set of random variables corresponding to a BLOG model.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.2.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"The set of basic random variables for a BLOG modelM, BRV (M), consists of:
• for each number statement ν(x̄), a number variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vν,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] over the standard measurable space N, where ū is of the type of x̄. • for each function f(x̄) and tuple ū from UM of the type of x̄, a function application variable",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Vf [ū] with the measurable space XVf,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"[ū] = Xτf , where Xτf is the measurable space corresponding to τf , the return type of f .
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"We now define the space of consistent assignments to random variables.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Definition 5.3.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"An instantiation σ of the basic RVs defined by a BLOG modelM is consistent if and only if:
•",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"For every element uν,v̄,i used in an assignment of the form",5.2. Semantics of measure-theoretic BLOG,[0],[0]
σ(Vf [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
= w or σ(Vν [ū]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
"= m > 0, σ(Vν",5.2. Semantics of measure-theoretic BLOG,[0],[0]
[v̄]),5.2. Semantics of measure-theoretic BLOG,[0],[0]
≥,5.2. Semantics of measure-theoretic BLOG,[0],[0]
i;,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• For every fixed function symbol f with the interpretation f̃ , σ(Vf [ū]) = f̃(ū); and • For every element uν,ū=〈u1,...,um〉,i, generated by the number statement ν, with origin functions g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm, for every gj ∈ {g1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", gm}, σ(Vgj [uν,ū,i]) = uj .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"That is, origin functions give correct inverse maps.
",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Lemma 5.4.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Every consistent assignment σ to the basic RVs forM defines a unique possible world in the vocabulary ofM.
The proof of Lemma 5.4 is in Appx.",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"F. In the following definition, we use the notation e[ū/x̄] to denote a substitution of every occurrence of the variable xi with ui in the expression e. For any BLOG modelM, let V (M) = BRV (M); for each v ∈ V , Xv is the measurable space corresponding to v. Let E(M) consist of the following edges for every number statement or function application statement of the form s(x̄):
• The edge (Vg[w̄], Vs[ū])",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"if g is a function symbol in M such that g(ȳ) appears in s(x̄), and either g(w̄) = g(ȳ)[ū/x̄] or an occurrence of g(ȳ) in s(x̄) uses quantified variables z1, . . .",5.2. Semantics of measure-theoretic BLOG,[0],[0]
", zn, ū′ is a tuple of elements of the type of z̄ and g(w̄) = g(ȳ)[ū/x̄][ū′/z̄].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
"• The edge (Vν [v̄], Vs[ū]), for element uν,v̄,i ∈ ū.
Note that the first set of edges defined in E(M) above may include infinitely many parents for Vs[ū].",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Let the dependency statement in the BLOG model M corresponding to a number or function variable Vs[f̄ ] be s. Let expr(s) be the set of expressions used in s.,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"Each such statement then defines in a straightforward manner, a kernel Ks(ū) : Xexpr(s(ū))",5.2. Semantics of measure-theoretic BLOG,[0],[0]
→ XVs[ū].,5.2. Semantics of measure-theoretic BLOG,[0],[0]
"In order ensure consistent assignments, we include a special value null ∈",5.2. Semantics of measure-theoretic BLOG,[0],[0]
Xτ for each τ,5.2. Semantics of measure-theoretic BLOG,[0],[0]
Figure 3.,9 query hasFakeCoin;,[0],[0]
"BLOG code for the Scale example
inM, and require that Ks(ū)(σ(pa(Vs[ū])), {null}c) = 0",9 query hasFakeCoin;,[0],[0]
whenever σ violates the first condition of consistent assignments (Def. 5.3).,9 query hasFakeCoin;,[0],[0]
"In other words, all the local kernels ensure are locally consistent: variables involving an object uν,ū,i get a non-null assignment only if the assignment to its number statement represents the generation of at least i objects (σ(Vν(ū))",9 query hasFakeCoin;,[0],[0]
≥ i).,9 query hasFakeCoin;,[0],[0]
"Each kernel of the formKs(ū) can be transformed into a kernel Kpa(Vs[ū]) from its parent vertices (representing basic random variables) by composing the kernels determining the truth value of each expression e ∈ expr(v) in terms of the basic random variables, with the kernel KeVs[ū].",9 query hasFakeCoin;,[0],[0]
Let κ(M) = {Kpa(Vs[ū]) : Vs[ū] ∈ BRV (M)}.,9 query hasFakeCoin;,[0],[0]
Definition 5.5.,9 query hasFakeCoin;,[0],[0]
"The MTBN M for a BLOG model M is defined using V = V (M), E = E(M), the set of measurable spaces {Xv : v ∈ BRV (M)} and the kernels for each vertex given by κ(M).
",9 query hasFakeCoin;,[0],[0]
"By Thm. 3.6, we have the main result of this section, which provides the theoretical foundation for the generalized BLOG language:
Theorem 5.6.",9 query hasFakeCoin;,[0],[0]
"If the MTBNM for a BLOG model is a wellfounded digraph, thenM represents a unique measure µ on XBRV (M).",9 query hasFakeCoin;,[0],[0]
"We implemented the measure-theoretic extension of BLOG and evaluated our inference algorithms on three models where naive algorithms fail: (1) the GPA model (GPA); (2) the noisy scale model (Scale); and (3) a SSM, the aircraft tracking model (Aircraft-Tracking).",6. Experiment Results,[0],[0]
"The implementation is based on BLOG’s C++ compiler (Wu et al., 2016).
",6. Experiment Results,[0],[0]
GPA model: Fig. 1 presents the BLOG code for the GPA example as explained in Sec. 5.,6. Experiment Results,[0],[0]
"Since the GPA of David is exactly 4, Bayes rule implies that David must be from USA.",6. Experiment Results,[0],[0]
"We evaluate LLW and the naive LW on this model in Fig 4(a), where the naive LW converges to an incorrect posterior.
",6. Experiment Results,[0],[0]
Scale model:,6. Experiment Results,[0],[0]
"In the noisy scale example (Fig. 3), we have an even number of coins and there might be a fake coin among them (Line 4).",6. Experiment Results,[0],[0]
The fake coin will be slightly heavier than a normal coin (Line 2-3).,6. Experiment Results,[0],[0]
We divide the coins into two halves and place them onto a noisy scale.,6. Experiment Results,[0],[0]
"When there is no fake coin, the scale always balances (Line 7).
",6. Experiment Results,[0],[0]
"When there is a fake coin, the scale will noisily reflect the weight difference with standard deviation σ",6. Experiment Results,[0],[0]
(sigma in Line 6).,6. Experiment Results,[0],[0]
Now we observe that the scale is balanced (Line 8),6. Experiment Results,[0],[0]
and we would like to infer whether a fake coin exists.,6. Experiment Results,[0],[0]
We again compare LLW against the naive LW with different choices of the σ parameter in Fig. 4(b).,6. Experiment Results,[0],[0]
"Since the scale is precisely balanced, there must not be a fake coin.",6. Experiment Results,[0],[0]
"LLW always produces the correct answer but naive LW converges to different incorrect posteriors for different values of σ; as σ increases, naive LWs result approaches the true posterior.
",6. Experiment Results,[0],[0]
Aircraft-Tracking model: Fig. 5 shows a simplified BLOG model for the aircraft tracking example.,6. Experiment Results,[0],[0]
"In this state-space model, we have N = 6 radar points (Line 1) and a single aircraft to track.",6. Experiment Results,[0],[0]
Both the radars and the aircraft are considered as points on a 2D plane.,6. Experiment Results,[0],[0]
The prior of the aircraft movement is a Gaussian process (Line 3 to 6).,6. Experiment Results,[0],[0]
"Each radar r has an effective range radius(r): if the aircraft is within the range, the radar will noisily measure the distance from the aircraft to its own location (Line 13); if the aircraft is out of range, the radar will almost surely just output its radius (Line 10 to 11).",6. Experiment Results,[0],[0]
Now we observe the measurements from all the radar points for T time steps and we want to infer the location of the aircraft.,6. Experiment Results,[0],[0]
"With the measure-theoretic extension, a generalized BLOG program is more expressive for modeling truncated sensors: if a radar outputs exactly its radius, we can surely infer that the aircraft must be out of the effective range of this radar.",6. Experiment Results,[0],[0]
"However, this information cannot be captured by the original BLOG language.",6. Experiment Results,[0],[0]
"To illustrate this case, we manually generated a synthesis dataset of T = 8 time steps4 and evaluated LPF against the naive particle filter with different numbers of particles in Fig. 4(c).",6. Experiment Results,[0],[0]
We take the mean of the samples from all the particles as the predicted aircraft location.,6. Experiment Results,[0],[0]
"Since we know the ground truth, we measure the average mean square error between the true location and the prediction.",6. Experiment Results,[0],[0]
"LPF accurately predicts the
4The full BLOG programs with complete data are available at https://goo.gl/f7qLwy.",6. Experiment Results,[0],[0]
Figure 5.,18 query Y(t) for Timestep t;,[0],[0]
"BLOG code for the Aircraft-Tracking example
true locations while the naive PF converges to the incorrect results.",18 query Y(t) for Timestep t;,[0],[0]
"We presented a new formalization, measure-theoretic Bayesian networks, for generalizing the semantics of PPLs to include random variables with mixtures of discrete and continuous distributions.",7. Conclusion,[0],[0]
"We developed provably correct inference algorithms for such random variables and incorporated MTBNs into a widely used PPL, BLOG.",7. Conclusion,[0],[0]
"We believe that together with the foundational inference algorithms, our proposed rigorous framework will facilitate the development of powerful techniques for probabilistic reasoning in practical applications from a much wider range of scientific areas.",7. Conclusion,[0],[0]
"This work is supported by the DARPA PPAML program, contract FA8750-14-C-0011.",Acknowledgment,[0],[0]
"Simon S. Du is funded by NSF grant IIS1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.",Acknowledgment,[0],[0]
"Despite the recent successes of probabilistic programming languages (PPLs) in AI applications, PPLs offer only limited support for random variables whose distributions combine discrete and continuous elements.",abstractText,[0],[0]
We develop the notion of measure-theoretic Bayesian networks (MTBNs) and use it to provide more general semantics for PPLs with arbitrarily many random variables defined over arbitrary measure spaces.,abstractText,[0],[0]
"We develop two new general sampling algorithms that are provably correct under the MTBN framework: the lexicographic likelihood weighting (LLW) for general MTBNs and the lexicographic particle filter (LPF), a specialized algorithm for statespace models.",abstractText,[0],[0]
"We further integrate MTBNs into a widely used PPL system, BLOG, and verify the effectiveness of the new inference algorithms through representative examples.",abstractText,[0],[0]
Discrete-Continuous Mixtures in Probabilistic Programming: Generalized Semantics and Inference Algorithms,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 2315–2325, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
"Discriminative sentence modeling aims to capture sentence meanings, and classify sentences according to certain criteria (e.g., sentiment).",1 Introduction,[0],[0]
"It is related to various tasks of interest, and has attracted much attention in the NLP community (Allan et al., 2003; Su and Markert, 2008; Zhao et al., 2015).",1 Introduction,[0],[0]
"Feature engineering—for example, n-gram features (Cui et al., 2006), dependency subtree features (Nakagawa et al., 2010), or more dedicated ones (Silva et al., 2011)—can play an important role in modeling sentences.",1 Introduction,[0],[0]
"Kernel machines, e.g., SVM, are exploited in Moschitti (2006) and Reichartz et al. (2010) by specifying a certain measure of similarity between sentences, without explicit feature representation.
",1 Introduction,[0],[0]
∗These authors contribute equally to this paper.,1 Introduction,[0],[0]
"†To whom correspondence should be addressed.
",1 Introduction,[0],[0]
"Recent advances of neural networks bring new techniques in understanding natural languages, and have exhibited considerable potential.",1 Introduction,[0],[0]
"Bengio et al. (2003) and Mikolov et al. (2013) propose unsupervised approaches to learn word embeddings, mapping discrete words to real-valued vectors in a meaning space.",1 Introduction,[0],[0]
Le and Mikolov (2014) extend such approaches to learn sentences’ and paragraphs’ representations.,1 Introduction,[0],[0]
"Compared with human engineering, neural networks serve as a way of automatic feature learning (Bengio et al., 2013).
",1 Introduction,[0],[0]
Two widely used neural sentence models are convolutional neural networks (CNNs) and recursive neural networks (RNNs).,1 Introduction,[0],[0]
"CNNs can extract words’ neighboring features effectively with short propagation paths, but they do not capture inherent sentence structures (e.g., parse trees).",1 Introduction,[0],[0]
"RNNs encode, to some extent, structural information by recursive semantic composition along a parse tree.",1 Introduction,[0],[0]
"However, they may have difficulties in learning deep dependencies because of long propagation paths (Erhan et al., 2009).",1 Introduction,[0],[0]
"(CNNs/RNNs and a variant, recurrent networks, will be reviewed in Section 2.)
",1 Introduction,[0],[0]
"A curious question is whether we can combine the advantages of CNNs and RNNs, i.e., whether we can exploit sentence structures (like RNNs) effectively with short propagation paths (like CNNs).
",1 Introduction,[0],[0]
"In this paper, we propose a novel neural architecture for discriminative sentence modeling, called the Tree-Based Convolutional Neural Network (TBCNN).1 Our models can leverage different sentence parse trees, e.g., constituency trees and dependency trees.",1 Introduction,[0],[0]
"The model variants are denoted as c-TBCNN and d-TBCNN, respectively.",1 Introduction,[0],[0]
"The idea of tree-based convolution is to apply a set of subtree feature detectors, sliding over the entire
1The model of tree-based convolution was firstly proposed to process program source code in our (unpublished) previous work (Mou et al., 2014).
2315
parse tree of a sentence; then pooling aggregates these extracted feature vectors by taking the maximum value in each dimension.",1 Introduction,[0],[0]
"One merit of such architecture is that all features, along the tree, have short propagation paths to the output layer, and hence structural information can be learned effectively.
",1 Introduction,[0],[0]
"TBCNNs are evaluated on two tasks, sentiment analysis and question classification; our models have outperformed previous state-of-the-art results in both experiments.",1 Introduction,[0],[0]
"To understand how TBCNNs work, we also visualize the network by plotting the convolution process.",1 Introduction,[0],[0]
We make our code and results available on our project website.2,1 Introduction,[0],[0]
"In this section, we present the background and related work regarding two prevailing neural architectures for discriminative sentence modeling.",2 Background and Related Work,[0],[0]
"Convolutional neural networks (CNNs), early used for image processing (LeCun, 1995), turn out to be effective with natural languages as well.",2.1 Convolutional Neural Networks,[0],[0]
"Figure 1a depicts a classic convolution process on a sentence (Collobert and Weston, 2008).",2.1 Convolutional Neural Networks,[0],[0]
"A set of fixed-width-window feature detectors slide over the sentence, and output the extracted features.",2.1 Convolutional Neural Networks,[0],[0]
"Let t be the window size, and x1, · · · ,",2.1 Convolutional Neural Networks,[0],[0]
xt ∈ Rne be ne-dimensional word embeddings.,2.1 Convolutional Neural Networks,[0],[0]
"The output of convolution, evaluated at the current position, is
y = f (W ·",2.1 Convolutional Neural Networks,[0],[0]
[x1; · · · ; xt] + b) where y ∈ Rnc (nc is the number of feature detectors).,2.1 Convolutional Neural Networks,[0],[0]
"W ∈ Rnc×(t·ne) and b ∈ Rnc are parame-
2https://sites.google.com/site/tbcnnsentence/
ters; f is the activation function.",2.1 Convolutional Neural Networks,[0],[0]
Semicolons represent column vector concatenation.,2.1 Convolutional Neural Networks,[0],[0]
"After convolution, the extracted features are pooled to a fixedsize vector for classification.
",2.1 Convolutional Neural Networks,[0],[0]
Convolution can extract neighboring information effectively.,2.1 Convolutional Neural Networks,[0],[0]
"However, the features are “local”—words that are not in a same convolution window do not interact with each other, even though they may be semantically related.",2.1 Convolutional Neural Networks,[0],[0]
Blunsom et al. (2014) build deep convolutional networks so that local features can mix at high-level layers.,2.1 Convolutional Neural Networks,[0],[0]
Similar CNNs include Kim (2014) and Hu et al. (2014).,2.1 Convolutional Neural Networks,[0],[0]
"All these models are “flat,” by which we mean no structural information is used explicitly.",2.1 Convolutional Neural Networks,[0],[0]
"Recursive neural networks (RNNs), proposed in Socher et al. (2011b), utilize sentence parse trees.",2.2 Recursive Neural Networks,[0],[0]
"In the original version, RNN is built upon a binarized constituency tree.",2.2 Recursive Neural Networks,[0],[0]
"Leaf nodes correspond to words in a sentence, represented by nedimensional embeddings.",2.2 Recursive Neural Networks,[0],[0]
"Non-leaf nodes are sentence constituents, coded by child nodes recursively.",2.2 Recursive Neural Networks,[0],[0]
"Let node p be the parent of c1 and c2, vector representations denoted as p, c1, and c2.",2.2 Recursive Neural Networks,[0],[0]
"The parent’s representation is composited by
p = f(W ·",2.2 Recursive Neural Networks,[0],[0]
[c1; c2] + b) (1) where W and b are parameters.,2.2 Recursive Neural Networks,[0],[0]
"This process is done recursively along the tree; the root vector is then used for supervised classification (Figure 1b).
",2.2 Recursive Neural Networks,[0],[0]
"Dependency parse and the combinatory categorical grammar can also be exploited as RNNs’ skeletons (Hermann and Blunsom, 2013; Iyyer et al., 2014).",2.2 Recursive Neural Networks,[0],[0]
Irsoy and Cardie (2014) build deep RNNs to enhance information interaction.,2.2 Recursive Neural Networks,[0],[0]
"Im-
provements for semantic compositionality include matrix-vector interaction (Socher et al., 2012), tensor interaction (Socher et al., 2013).",2.2 Recursive Neural Networks,[0],[0]
"They are more suitable for capturing logical information in sentences, such as negation and exclamation.
",2.2 Recursive Neural Networks,[0],[0]
One potential problem of RNNs is that the long propagation paths—through which leaf nodes are connected to the output layer—may lead to information loss.,2.2 Recursive Neural Networks,[0],[0]
"Thus, RNNs bury illuminating information under a complicated neural architecture.",2.2 Recursive Neural Networks,[0],[0]
"Further, during back-propagation over a long path, gradients tend to vanish (or blow up), which makes training difficult (Erhan et al., 2009).",2.2 Recursive Neural Networks,[0],[0]
"Long short term memory (LSTM), first proposed for modeling time-series data (Hochreiter and Schmidhuber, 1997), is integrated to RNNs to alleviate this problem (Tai et al., 2015; Le and Zuidema, 2015; Zhu et al., 2015).
",2.2 Recursive Neural Networks,[0],[0]
Recurrent networks.,2.2 Recursive Neural Networks,[0],[0]
"A variant class of RNNs is the recurrent neural network (Bengio et al., 1994; Shang et al., 2015), whose architecture is a rightmost tree.",2.2 Recursive Neural Networks,[0],[0]
"In such models, meaningful tree structures are also lost, similar to CNNs.",2.2 Recursive Neural Networks,[0],[0]
This section introduces the proposed tree-based convolutional neural networks (TBCNNs).,3 Tree-based Convolution,[0],[0]
"Figure 1c depicts the convolution process on a tree.
",3 Tree-based Convolution,[0],[0]
"First, a sentence is converted to a parse tree, either a constituency or dependency tree.",3 Tree-based Convolution,[0],[0]
The corresponding model variants are denoted as c-TBCNN and d-TBCNN.,3 Tree-based Convolution,[0],[0]
"Each node in the tree is represented as a distributed, real-valued vector.
",3 Tree-based Convolution,[0],[0]
"Then, we design a set of fixed-depth subtree feature detectors, called the tree-based convolution window.",3 Tree-based Convolution,[0],[0]
"The window slides over the entire tree to extract structural information of the sentence, illustrated by a dashed triangle in Figure 1c.",3 Tree-based Convolution,[0],[0]
"Formally, let us assume we have t nodes in the convolution window, x1, · · · ,xt, each represented as an ne-dimensional vector.",3 Tree-based Convolution,[0],[0]
Let nc be the number of feature detectors.,3 Tree-based Convolution,[0],[0]
"The output of the tree-based convolution window, evaluated at the current subtree, is given by the following generic equation.
",3 Tree-based Convolution,[0],[0]
"y = f
( t∑
i=1
Wi ·xi + b )
(2)
where Wi ∈ Rnc×ne is the weight parameter associated with node xi; b ∈ Rnc is the bias term.
",3 Tree-based Convolution,[0],[0]
"Extracted features are thereafter packed into one or more fixed-size vectors by max pooling,
that is, the maximum value in each dimension is taken.",3 Tree-based Convolution,[0],[0]
"Finally, we add a fully connected hidden layer, and a softmax output layer.
",3 Tree-based Convolution,[0],[0]
"From the designed architecture (Figure 1c), we see that our TBCNN models allow short propagation paths between the output layer and any position in the tree.",3 Tree-based Convolution,[0],[0]
"Therefore structural feature learning becomes effective.
",3 Tree-based Convolution,[0],[0]
Several main technical points in tree-based convolution include: (1) How can we represent hidden nodes as vectors in constituency trees?,3 Tree-based Convolution,[0],[0]
"(2) How can we determine weights, Wi, for dependency trees, where nodes may have different numbers of children?",3 Tree-based Convolution,[0],[0]
"(3) How can we pool varying sized and shaped features to fixed-size vectors?
",3 Tree-based Convolution,[0],[0]
"In the rest of this section, we explain model variants in detail.",3 Tree-based Convolution,[0],[0]
"Particularly, Subsections 3.1 and 3.2 address the first and second problems; Subsection 3.3 deals with the third problem by introducing several pooling heuristics.",3 Tree-based Convolution,[0],[0]
Subsection 3.4 presents our training objective.,3 Tree-based Convolution,[0],[0]
"Figure 2a illustrates an example of the constituency tree, where leaf nodes are words in the sentence, and non-leaf nodes represent a grammatical constituent, e.g., a noun phrase.",3.1 c-TBCNN,[0],[0]
"Sentences are parsed by the Stanford parser;3 further, constituency trees are binarized for simplicity.
",3.1 c-TBCNN,[0],[0]
One problem of constituency trees is that nonleaf nodes do not have such vector representations as word embeddings.,3.1 c-TBCNN,[0],[0]
"Our strategy is to pretrain the constituency tree with an RNN by Equation 1 (Socher et al., 2011b).",3.1 c-TBCNN,[0],[0]
"After pretraining, vector representations of nodes are fixed.
",3.1 c-TBCNN,[0],[0]
"We now consider the tree-based convolution process in c-TBCNN with a two-layer-subtree convolution window, which operates on a parent node p and its direct children cl and cr, their vector representations denoted as p, cl, and cr.",3.1 c-TBCNN,[0],[0]
"The convolution equation, specific for c-TBCNN, is
y = f ( W (c)p ·p +W (c)l ·cl +W (c)r ·cr + b(c) ) where W (c)p , W (c) l , and W (c) r are weights associated with the parent and its child nodes.",3.1 c-TBCNN,[0],[0]
Superscript (c) indicates that the weights are for cTBCNN.,3.1 c-TBCNN,[0],[0]
"For leaf nodes, which do not have children, we set cl and cr to be 0.
3http://nlp.stanford.edu/software/lex-parser.shtml
Tree-based convolution windows can be extended to arbitrary depths straightforwardly.",3.1 c-TBCNN,[0],[0]
"The complexity is exponential to the depth of the window, but linear to the number of nodes.",3.1 c-TBCNN,[0],[0]
"Hence, tree-based convolution, compared with “flat” CNNs, does not add to computational cost, provided the same amount of information to process at a time.",3.1 c-TBCNN,[0],[0]
"In our experiments, we use convolution windows of depth 2.",3.1 c-TBCNN,[0],[0]
Dependency trees are another representation of sentence structures.,3.2 d-TBCNN,[0],[0]
The nature of dependency representation leads to d-TBCNN’s major difference from traditional convolution: there exist nodes with different numbers of child nodes.,3.2 d-TBCNN,[0],[0]
"This causes trouble if we associate weight parameters according to positions in the window, which is standard for traditional convolution, e.g., Collobert and Weston (2008) or c-TBCNN.
",3.2 d-TBCNN,[0],[0]
"To overcome the problem, we extend the notion of convolution by assigning weights according to dependency types (e.g, nsubj) rather than positions.",3.2 d-TBCNN,[0],[0]
"We believe this strategy makes much sense because dependency types (de Marneffe et al., 2006) reflect the relationship between a governing word and its child words.",3.2 d-TBCNN,[0],[0]
"To be concrete, the generic convolution formula (Equation 2) for d-TBCNN becomes
y = f ( W (d)p ·p + n∑ i=1",3.2 d-TBCNN,[0],[0]
"W (d) r[ci] ·ci + b(d) )
where W (d)p is the weight parameter for the parent p (governing word); W (d)r[ci] is the weight for child ci, who has grammatical relationship r[ci]
to its parent,",3.2 d-TBCNN,[0],[0]
p. Superscript (d) indicates the parameters are for d-TBCNN.,3.2 d-TBCNN,[0],[0]
"Note that we keep 15 most frequently occurred dependency types; others appearing rarely in the corpus are mapped to one shared weight matrix.
",3.2 d-TBCNN,[0],[0]
Both c-TBCNN and d-TBCNN have their own advantages: d-TBCNN exploits structural features more efficiently because of the compact expressiveness of dependency trees; c-TBCNN may be more effective in integrating global features due to the underneath pretrained RNN.,3.2 d-TBCNN,[0],[0]
"As different sentences may have different lengths and tree structures, the extracted features by treebased convolution also have topologies varying in size and shape.",3.3 Pooling Heuristics,[0],[0]
"Dynamic pooling (Socher et al., 2011a) is a common technique for dealing with
this problem.",3.3 Pooling Heuristics,[0],[0]
We propose several heuristics for pooling along a tree structure.,3.3 Pooling Heuristics,[0],[0]
Our generic design criteria for pooling include: (1) Nodes that are pooled to one slot should be “neighboring” from some viewpoint.,3.3 Pooling Heuristics,[0],[0]
"(2) Each slot should have similar numbers of nodes, in expectation, that are pooled to it.",3.3 Pooling Heuristics,[0],[0]
"Thus, (approximately) equal amount of information is aggregated along different parts of the tree.",3.3 Pooling Heuristics,[0],[0]
"Following the above intuition, we propose pooling heuristics as follows.
",3.3 Pooling Heuristics,[0],[0]
•,3.3 Pooling Heuristics,[0],[0]
Global pooling.,3.3 Pooling Heuristics,[0],[0]
"All features are pooled to one vector, shown in Figure 3a.",3.3 Pooling Heuristics,[0],[0]
We take the maximum value in each dimension.,3.3 Pooling Heuristics,[0],[0]
"This simple heuristic is applicable to any structure, including c-TBCNN and d-TBCNN.",3.3 Pooling Heuristics,[0],[0]
• 3-slot pooling for c-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"To preserve
more information over different parts of constituency trees, we propose 3-slot pooling (Figure 3b).",3.3 Pooling Heuristics,[0],[0]
"If a tree has maximum depth d, we pool nodes of less than α · d layers to a TOP slot (α is set to 0.6); lower nodes are pooled to slots LOWER LEFT or LOWER",3.3 Pooling Heuristics,[0],[0]
RIGHT according to their relative position with respect to the root node.,3.3 Pooling Heuristics,[0],[0]
"For a constituency tree, it is not completely obvious how to pool features to more than 3 slots and comply with the aforementioned criteria at the same time.",3.3 Pooling Heuristics,[0],[0]
"Therefore, we regard 3-slot pooling for c-TBCNN is a “hard mechanism” temporarily.",3.3 Pooling Heuristics,[0],[0]
Further improvement can be addressed in future work.,3.3 Pooling Heuristics,[0],[0]
• k-slot pooling for d-TBCNN.,3.3 Pooling Heuristics,[0],[0]
"Different from
constituency trees, nodes in dependency trees are one-one corresponding to words in a sentence.",3.3 Pooling Heuristics,[0],[0]
"Thus, a total order on features (after convolution) can be defined according to their corresponding word orders.",3.3 Pooling Heuristics,[0],[0]
"For kslot pooling, we can adopt an “equal allocation” strategy, shown in Figure 3c.",3.3 Pooling Heuristics,[0],[0]
"Let i be the position of a word in a sentence (i = 1, 2, · · · , n).",3.3 Pooling Heuristics,[0],[0]
"Its extracted feature vector is pooled to the j-th slot, if
(j − 1) n",3.3 Pooling Heuristics,[0],[0]
k ≤,3.3 Pooling Heuristics,[0],[0]
"i ≤ j n k
We assess the efficacy of pooling quantitatively in Section 4.3.1.",3.3 Pooling Heuristics,[0],[0]
"As we shall see by the experimental results, complicated pooling methods do preserve more information along tree structures to some extent, but the effect is not large.",3.3 Pooling Heuristics,[0],[0]
TBCNNs are not very sensitive to pooling methods.,3.3 Pooling Heuristics,[0],[0]
"After pooling, information is packed into one or more fixed-size vectors (slots).",3.4 Training Objective,[0],[0]
"We add a hidden layer, and then a softmax layer to predict the probability of each target label in a classification task.",3.4 Training Objective,[0],[0]
"The error function of a sample is the standard cross entropy loss, i.e., J = −∑ci=1 ti log yi, where t is the ground truth (one-hot represented), y the output by softmax, and c the number of classes.",3.4 Training Objective,[0],[0]
"To regularize our model, we apply both `2 penalty and dropout (Srivastava et al., 2014).",3.4 Training Objective,[0],[0]
Training details are further presented in Section 4.1 and 4.2.,3.4 Training Objective,[0],[0]
"In this section, we evaluate our models with two tasks, sentiment analysis and question classification.",4 Experimental Results,[0],[0]
We also conduct quantitative and qualitative model analysis in Subsection 4.3.,4 Experimental Results,[0],[0]
Sentiment analysis is a widely studied task for discriminative sentence modeling.,4.1.1 The Task and Dataset,[0],[0]
"The Stanford sentiment treebank4 consists of more than 10,000 movie reviews.",4.1.1 The Task and Dataset,[0],[0]
"Two settings are considered for sentiment prediction: (1) fine-grained classification with 5 labels (strongly positive, positive, neutral, negative, and strongly negative), and (2) coarse-gained polarity classification with 2 labels (positive versus negative).",4.1.1 The Task and Dataset,[0],[0]
"Some examples are shown in
4http://nlp.stanford.edu/sentiment/
Table 1.",4.1.1 The Task and Dataset,[0],[0]
"We use the standard split for training, validating, and testing, containing 8544/1101/2210 sentences for 5-class prediction.",4.1.1 The Task and Dataset,[0],[0]
"Binary classification does not contain the neutral class.
",4.1.1 The Task and Dataset,[0],[0]
"In the dataset, phrases (sub-sentences) are also tagged with sentiment labels.",4.1.1 The Task and Dataset,[0],[0]
RNNs deal with them naturally during the recursive process.,4.1.1 The Task and Dataset,[0],[0]
"We regard sub-sentences as individual samples during training, like Blunsom et al. (2014) and Le and Mikolov (2014).",4.1.1 The Task and Dataset,[0],[0]
"The training set therefore has more than 150,000 entries in total.",4.1.1 The Task and Dataset,[0],[0]
"For validating and testing, only whole sentences (root labels) are considered in our experiments.
",4.1.1 The Task and Dataset,[0],[0]
Both c-TBCNN and d-TBCNN use the Stanford parser for data preprocessing.,4.1.1 The Task and Dataset,[0],[0]
"This subsection describes training details for dTBCNN, where hyperparameters are chosen by validation.",4.1.2 Training Details,[0],[0]
"c-TBCNN is mostly tuned synchronously (e.g., optimization algorithm, activation function) with some changes in hyperparameters.",4.1.2 Training Details,[0],[0]
"c-TBCNN’s settings can be found on our website.
",4.1.2 Training Details,[0],[0]
"In our d-TBCNN model, the number of units is 300 for convolution and 200 for the last hidden layer.",4.1.2 Training Details,[0],[0]
"Word embeddings are 300 dimensional, pretrained ourselves using word2vec (Mikolov et al., 2013) on the English Wikipedia corpus.",4.1.2 Training Details,[0],[0]
2- slot pooling is applied for d-TBCNN.,4.1.2 Training Details,[0],[0]
"(c-TBCNN uses 3-slot pooling.)
",4.1.2 Training Details,[0],[0]
"To train our model, we compute gradient by back-propagation and apply stochastic gradient descent with mini-batch 200.",4.1.2 Training Details,[0],[0]
"We use ReLU (Nair and Hinton, 2010) as the activation function .
",4.1.2 Training Details,[0],[0]
"For regularization, we add `2 penalty for weights with a coefficient of 10−5.",4.1.2 Training Details,[0],[0]
"Dropout (Srivastava et al., 2014) is further applied to both weights and embeddings.",4.1.2 Training Details,[0],[0]
"All hidden layers are dropped out by 50%, and embeddings 40%.",4.1.2 Training Details,[0],[0]
Table 2 compares our models to state-of-the-art results in the task of sentiment analysis.,4.1.3 Performance,[0],[0]
"For 5- class prediction, d-TBCNN yields 51.4% accuracy, outperforming the previous state-of-the-art result, achieved by the RNN based on long-short term memory (Tai et al., 2015).",4.1.3 Performance,[0],[0]
c-TBCNN is slightly worse.,4.1.3 Performance,[0],[0]
"It achieves 50.4% accuracy, ranking third in the state-of-the-art list (including our d-TBCNN model).
",4.1.3 Performance,[0],[0]
"Regarding 2-class prediction, we adopted a simple strategy in Irsoy and Cardie (2014),5 where the 5-class network is “transferred” directly for binary classification, with estimated target probabilities (by 5-way softmax) reinterpreted for 2 classes.",4.1.3 Performance,[0],[0]
(The neutral class is discarded as in other studies.),4.1.3 Performance,[0],[0]
"This strategy enables us to take a glance at the stability of our TBCNN models, but places itself in a difficult position.",4.1.3 Performance,[0],[0]
"Nonetheless, our d-TBCNN model achieves 87.9% accuracy, ranking forth in the list.
",4.1.3 Performance,[0],[0]
"In a more controlled comparison—with shallow architectures and the basic interaction (linearly transformed and non-linearly squashed)— TBCNNs, of both variants, consistently outperform RNNs (Socher et al., 2011b) to a large extent (50.4–51.4% versus 43.2%); they also consistently outperform “flat” CNNs by more than 10%.",4.1.3 Performance,[0],[0]
"Such results show that structures are important when modeling sentences; tree-based convolution can capture these structural information more effectively than RNNs.
",4.1.3 Performance,[0],[0]
We also observe d-TBCNN achieves higher performance than c-TBCNN.,4.1.3 Performance,[0],[0]
This suggests that compact tree expressiveness is more important than integrating global information in this task.,4.1.3 Performance,[0],[0]
We further evaluate TBCNN models on a question classification task.6,4.2 Question Classification,[0],[0]
The dataset contains 5452 annotated sentences plus 500 test samples in TREC 10.,4.2 Question Classification,[0],[0]
"We also use the standard split, like Silva et al. (2011).",4.2 Question Classification,[0],[0]
"Target labels contain 6 classes, namely abbreviation, entity, description, human, location, and numeric.",4.2 Question Classification,[0],[0]
"Some examples are also shown in Table 1.
",4.2 Question Classification,[0],[0]
"We chose this task to evaluate our models because the number of training samples is rather small, so that we can know TBCNNs’ performance when applied to datasets of different sizes.",4.2 Question Classification,[0],[0]
"To alleviate the problem of data sparseness, we set the dimensions of convolutional layer and the last hidden layer to 30 and 25, respectively.",4.2 Question Classification,[0],[0]
"We do not back-propagate gradient to embeddings in this
5Richard Socher, who first applies neural networks to this task, thinks direct transfer is fine for binary classification.",4.2 Question Classification,[0],[0]
We followed this strategy for simplicity as it is non-trivial to deal with the neutral sub-sentences in the training set if we train a separate model.,4.2 Question Classification,[0],[0]
"Our website reviews some related work and provides more discussions.
",4.2 Question Classification,[0],[0]
"6http://cogcomp.cs.illinois.edu/Data/QA/QC/
task.",4.2 Question Classification,[0],[0]
"Dropout rate for embeddings is 30%; hidden layers are dropped out by 5%.
",4.2 Question Classification,[0],[0]
Table 3 compares our models to various other methods.,4.2 Question Classification,[0],[0]
"The first entry presents the previous state-of-the-art result, achieved by traditional feature/rule engineering (Silva et al., 2011).",4.2 Question Classification,[0],[0]
Their method utilizes more than 10k features and 60 hand-coded rules.,4.2 Question Classification,[0],[0]
"On the contrary, our TBCNN models do not use a single human-engineered feature or rule.",4.2 Question Classification,[0],[0]
"Despite this, c-TBCNN achieves similar accuracy compared with feature engineering; d-TBCNN pushes the state-of-the-art result to 96%.",4.2 Question Classification,[0],[0]
"To the best of our knowledge, this is the first time that neural networks beat dedicated human engineering in this question classification task.
",4.2 Question Classification,[0],[0]
"The result also shows that both c-TBCNN and d-TBCNN reduce the error rate to a large extent, compared with other neural architectures in this task.",4.2 Question Classification,[0],[0]
"In this part, we analyze our models quantitatively and qualitatively in several aspects, shedding some light on the mechanism of TBCNNs.",4.3 Model Analysis,[0],[0]
The extracted features by tree-based convolution have topologies varying in size and shape.,4.3.1 The Effect of Pooling,[0],[0]
We propose in Section 3.3 several heuristics for pooling.,4.3.1 The Effect of Pooling,[0],[0]
"This subsection aims to provide a fair comparison among these pooling methods.
",4.3.1 The Effect of Pooling,[0],[0]
One reasonable protocol for comparison is to tune all hyperparameters for each setting and compare the highest accuracy.,4.3.1 The Effect of Pooling,[0],[0]
"This methodology, however, is too time-consuming, and depends largely on the quality of hyperparameter tuning.",4.3.1 The Effect of Pooling,[0],[0]
An alternative is to predefine a set of sensible hyperparameters and report the accuracy under the same setting.,4.3.1 The Effect of Pooling,[0],[0]
"In this experiment, we chose the latter protocol, where hidden layers are all 300- dimensional; no `2 penalty is added.",4.3.1 The Effect of Pooling,[0],[0]
Each configuration was run five times with different random initializations.,4.3.1 The Effect of Pooling,[0],[0]
"We summarize the mean and standard deviation in Table 4.
",4.3.1 The Effect of Pooling,[0],[0]
"As the results imply, complicated pooling is better than global pooling to some degree for both model variants.",4.3.1 The Effect of Pooling,[0],[0]
"But the effect is not strong; our models are not that sensitive to pooling methods, which mainly serve as a necessity for dealing with varying-structure data.",4.3.1 The Effect of Pooling,[0],[0]
"In our experiments, we apply 3-slot pooling for c-TBCNN and 2-slot pooling for d-TBCNN.
",4.3.1 The Effect of Pooling,[0],[0]
"Comparing with other studies in the literature, we also notice that pooling is very effective and efficient in information gathering.",4.3.1 The Effect of Pooling,[0],[0]
"Irsoy and Cardie (2014) report 200 epochs for training a deep RNN, which achieves 49.8% accuracy in the 5-class sentiment classification.",4.3.1 The Effect of Pooling,[0],[0]
Our TBCNNs are typically trained within 25 epochs.,4.3.1 The Effect of Pooling,[0],[0]
We analyze how sentence lengths affect our models.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"Sentences are split into 7 groups by length, with granularity 5.",4.3.2 The Effect of Sentence Lengths,[0],[0]
A few too long or too short sentences are grouped together for smoothing; the numbers of sentences in each group vary from 126 to 457.,4.3.2 The Effect of Sentence Lengths,[0],[0]
Figure 4 presents accuracies versus lengths in TBCNNs.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"For comparison, we also reimplemented RNN, achieving 42.7% overall accuracy, slightly worse than 43.2% reported in Socher et al. (2011b).",4.3.2 The Effect of Sentence Lengths,[0],[0]
"Thus, we think our reimplementation is fair and that the comparison is sensible.
",4.3.2 The Effect of Sentence Lengths,[0],[0]
We observe that c-TBCNN and d-TBCNN yield very similar behaviors.,4.3.2 The Effect of Sentence Lengths,[0],[0]
They consistently outperform the RNN in all scenarios.,4.3.2 The Effect of Sentence Lengths,[0],[0]
"We also notice the gap, between TBCNNs and RNN, increases when sentences contain more than 20 words.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"This result confirms our theoretical analysis in Section 2—for long sentences, the propagation paths in RNNs are deep, causing RNNs’ difficulty in information processing.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"By contrast, our models explore structural information more effectively with
tree-based convolution.",4.3.2 The Effect of Sentence Lengths,[0],[0]
"As information from any part of the tree can propagate to the output layer with short paths, TBCNNs are more capable for sentence modeling, especially for long sentences.",4.3.2 The Effect of Sentence Lengths,[0],[0]
Visualization is important to understanding the mechanism of neural networks.,4.3.3 Visualization,[0],[0]
"For TBCNNs, we would like to see how the extracted features (after convolution) are further processed by the max pooling layer, and ultimately related to the supervised task.
",4.3.3 Visualization,[0],[0]
"To show this, we trace back where the max pooling layer’s features come from.",4.3.3 Visualization,[0],[0]
"For each dimension, the pooling layer chooses the maximum value from the nodes that are pooled to it.",4.3.3 Visualization,[0],[0]
"Thus, we can count the fraction in which a node’s features are gathered by pooling.",4.3.3 Visualization,[0],[0]
"Intuitively, if a node’s features are more related to the task, the fraction tends to be larger, and vice versa.
",4.3.3 Visualization,[0],[0]
"Figure 5 illustrates an example processed by dTBCNN in the task of sentiment analysis.7 Here, we applied global pooling because information tracing is more sensible with one pooling slot.",4.3.3 Visualization,[0],[0]
"As shown in the figure, tree-based convolution can effectively extract information relevant to the task of interest.",4.3.3 Visualization,[0],[0]
"The 2-layer windows corresponding to “visual will impress viewers,” “the stunning dreamlike visual,” say, are discriminative to the sentence’s sentiment.",4.3.3 Visualization,[0],[0]
"Hence, large fractions (0.24 and 0.19) of their features, after convolution, are gathered by pooling.",4.3.3 Visualization,[0],[0]
"On the other hand, words like the, will, even are known as stop words (Fox, 1989).",4.3.3 Visualization,[0],[0]
"They are mostly noninformative for sentiment; hence, no (or minimal) features are gathered.",4.3.3 Visualization,[0],[0]
"Such results are consistent with human intuition.
",4.3.3 Visualization,[0],[0]
We further observe that tree-based convolution does integrate information of different words in the window.,4.3.3 Visualization,[0],[0]
"For example, the word stunning appears in two windows: (a) the window “stunning” itself, and (b) the window of “the stunning dreamlike visual,” with root node visual, stunning acting as a child.",4.3.3 Visualization,[0],[0]
"We see that Window b is more relevant to the ultimate sentiment than Window a, with fractions 0.19 versus 0.07, even though the root visual itself is neutral in sentiment.",4.3.3 Visualization,[0],[0]
"In fact,
7We only have space to present one example in the paper.",4.3.3 Visualization,[0],[0]
This example was not chosen deliberately.,4.3.3 Visualization,[0],[0]
"Similar traits can be found through out the entire gallery, available on our website.",4.3.3 Visualization,[0],[0]
"Also, we only present d-TBCNN, noticing that dependency trees are intrinsically more suitable for visualization since we know the “meaning” of every node.
",4.3.3 Visualization,[0],[0]
"Window a has a larger fraction than the sum of its children’s (the windows of “the,” “stunning,” and “dreamlike”).",4.3.3 Visualization,[0],[0]
"In this paper, we proposed a novel neural discriminative sentence model based on sentence parsing structures.",5 Conclusion,[0],[0]
"Our model can be built upon either constituency trees (denoted as c-TBCNN) or dependency trees (d-TBCNN).
",5 Conclusion,[0],[0]
Both variants have achieved high performance in sentiment analysis and question classification.,5 Conclusion,[0],[0]
"d-TBCNN is slightly better than c-TBCNN in our experiments, and has outperformed previous stateof-the-art results in both tasks.",5 Conclusion,[0],[0]
"The results show that tree-based convolution can capture sentences’ structural information effectively, which is useful for sentence modeling.",5 Conclusion,[0],[0]
This research is supported by the National Basic Research Program of China (the 973 Program) under Grant No. 2015CB352201 and the National Natural Science Foundation of China under Grant Nos. 61232015 and 91318301.,Acknowledgments,[0],[0]
This paper proposes a tree-based convolutional neural network (TBCNN) for discriminative sentence modeling.,abstractText,[0],[0]
Our model leverages either constituency trees or dependency trees of sentences.,abstractText,[0],[0]
"The tree-based convolution process extracts sentences structural features, which are then aggregated by max pooling.",abstractText,[0],[0]
"Such architecture allows short propagation paths between the output layer and underlying feature detectors, enabling effective structural feature learning and extraction.",abstractText,[0],[0]
We evaluate our models on two tasks: sentiment analysis and question classification.,abstractText,[0],[0]
"In both experiments, TBCNN outperforms previous state-of-the-art results, including existing neural networks and dedicated feature/rule engineering.",abstractText,[0],[0]
"We also make efforts to visualize the tree-based convolution process, shedding light on how our models work.",abstractText,[0],[0]
Discriminative Neural Sentence Modeling by Tree-Based Convolution,title,[0],[0]
Representation learning remains an outstanding research problem in machine learning and computer vision.,1. Introduction,[0],[0]
"Recently there is a rising interest in disentangled representations, in which each component of learned features refers to a semantically meaningful concept.",1. Introduction,[0],[0]
"In the example of video sequence modelling, an ideal disentangled representation would be able to separate time-independent concepts (e.g. the identity of the object in the scene) from dynamical information (e.g. the time-varying position and the orientation or pose of that object).",1. Introduction,[0],[0]
"Such disentangled represen-
1University of Cambridge, UK 2Disney Research, Los Angeles, CA, USA.",1. Introduction,[0],[0]
"Correspondence to: Yingzhen Li<yl494@cam.ac.uk>, Stephan Mandt <stephan.mandt@disneyresearch.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
tations would open new efficient ways of compression and style manipulation, among other applications.
",1. Introduction,[0],[0]
"Recent work has investigated disentangled representation learning for images within the framework of variational auto-encoders (VAEs) (Kingma & Welling, 2013; Rezende et al., 2014) and generative adversarial networks (GANs) (Goodfellow et al., 2014).",1. Introduction,[0],[0]
"Some of them, e.g. the β-VAE method (Higgins et al., 2016), proposed new objective functions/training techniques that encourage disentanglement.",1. Introduction,[0],[0]
"On the other hand, network architecture designs that directly enforce factored representations have also been explored by e.g. Siddharth et al. (2017); Bouchacourt et al. (2017).",1. Introduction,[0],[0]
"These two types of approaches are often mixed together, e.g. the infoGAN approach (Chen et al., 2016) partitioned the latent space and proposed adding a mutual information regularisation term to the vanilla GAN loss.",1. Introduction,[0],[0]
"Mathieu et al. (2016) also partitioned the encoding space into style and content components, and performed adversarial training to encourage the datapoints from the same class to have similar content representations, but diverse style features.
",1. Introduction,[0],[0]
Less research has been conducted for unsupervised learning of disentangled representations of sequences.,1. Introduction,[0],[0]
"For video sequence modelling, Villegas et al. (2017) and Denton & Birodkar (2017) utilised different networks to encode the content and dynamics information separately, and trained the auto-encoders with a combination of reconstruction loss and GAN loss.",1. Introduction,[0],[0]
"Structured (Johnson et al., 2016) and Factorised VAEs (Deng et al., 2017) used hierarchical priors to learn more interpretable latent variables.",1. Introduction,[0],[0]
Hsu et al. (2017) designed a structured VAE in the context of speech recognition.,1. Introduction,[0],[0]
Their VAE architecture is trained using a combination of the standard variational lower bound and a discriminative regulariser to further encourage disentanglement.,1. Introduction,[0],[0]
"More related work is discussed in Section 3.
",1. Introduction,[0],[0]
"In this paper, we propose a generative model for unsupervised structured sequence modelling, such as video or audio.",1. Introduction,[0],[0]
"We show that, in contrast to previous approaches, a disentangled representation can be achieved by a careful design of the probabilistic graphical model.",1. Introduction,[0],[0]
"In the proposed architecture, we explicitly use a latent variable to represent content, i.e., information that is invariant through the sequence, and a set of latent variables associated to each frame to represent dynamical information, such as pose and position.",1. Introduction,[0],[0]
"Com-
pared to the mentioned previous models that usually predict future frames conditioned on the observed sequences, we focus on learning the distribution of the video/audio content and dynamics to enable sequence generation without conditioning.",1. Introduction,[0],[0]
"Therefore our model can also generalise to unseen sequences, which is confirmed by our experiments.",1. Introduction,[0],[0]
"In more detail, our contributions are as follows:
• Controlled generation.",1. Introduction,[0],[0]
Our architecture allows us to approximately control for content and dynamics when generating videos.,1. Introduction,[0],[0]
"We can generate random dynamics for fixed content, and random content for fixed dynamics.",1. Introduction,[0],[0]
"This gives us a controlled way of manipulating a video/audio sequence, such as swapping the identity of moving objects or the voice of a speaker.
",1. Introduction,[0],[0]
• Efficient encoding.,1. Introduction,[0],[0]
Our representation is more data efficient than encoding a video frame by frame.,1. Introduction,[0],[0]
"By factoring out a separate variable that encodes content, our dynamical latent variables can have smaller dimensions.",1. Introduction,[0],[0]
"This may be promising when it comes to end-to-end neural video encoding methods.
",1. Introduction,[0],[0]
"• We design a new metric that allow us to verify disentanglement of the latent variables, by investigating the stability of an object classifier over time.
",1. Introduction,[0],[0]
"• We give empirical evidence, based on video data of a physics simulator, that for long sequences, a stochastic transition model generates more realistic dynamics.
",1. Introduction,[0],[0]
The paper is structured as follows.,1. Introduction,[0],[0]
Section 2 introduces the generative model and the problem setting.,1. Introduction,[0],[0]
Section 3 discusses related work.,1. Introduction,[0],[0]
Section 4 presents three experiments on video and speech data.,1. Introduction,[0],[0]
"Finally, Section 5 concludes the paper and discusses future research directions.",1. Introduction,[0],[0]
"Let x1:T = (x1,x2, ...,xT ) denote a high dimensional sequence, such as a video with T consecutive frames.",2. The model,[0],[0]
"Also, assume the data distribution of the training sequences is pdata(x1:T ).",2. The model,[0],[0]
"In this paper, we model the observed data with a latent variable model that separates the representation of time-invariant concepts (e.g. object identities) from those of time-varying concepts (e.g. pose information).
",2. The model,[0],[0]
Generative model.,2. The model,[0],[0]
"Consider the following probabilistic model, which is also visualised in Figure 1:
pθ(x1:T , z1:T ,f) = pθ(f) T∏ t=1 pθ(zt|z<t)pθ(xt|zt,f).
",2. The model,[0],[0]
(1) We use the convention that z0 = 0.,2. The model,[0],[0]
The generation of frame xt at time t depends on the corresponding latent variables zt and f .,2. The model,[0],[0]
"θ are model parameters.
",2. The model,[0],[0]
"Ideally, f will be capable of modelling global aspects of the whole sequence which are time-invariant, while zt will encode time-varying features.",2. The model,[0],[0]
"This separation may be achieved when choosing the dimensionality of zt to be small enough, thus reserving zt only for time-dependent features while compressing everything else into f .",2. The model,[0],[0]
"In the context of video encodings, zt would thus encode a “morphing transformation”, which encodes how a frame at time t is morphed into a frame at time t+ 1.
Inference models.",2. The model,[0],[0]
"We use variational inference to learn an approximate posterior over latent variables given data (Jordan et al., 1999).",2. The model,[0],[0]
"This involves an approximating distribution q. We train the generative model with the VAE algorithm (Kingma & Welling, 2013):
max θ,φ
EpD(x1:T )",2. The model,[0],[0]
"[ Eqφ [ log pθ(x1:T , z1:T ,f)
qφ(z1:T ,f |x1:T )
",2. The model,[0],[0]
]] .,2. The model,[0],[0]
"(2)
To quantify the effect of the architecture of q on the learned generative model, we test with two types of q factorisation structures as follows.
",2. The model,[0],[0]
"The first architecture constructs a factorised q distribution
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )",2. The model,[0],[0]
"T∏
t=1
qφ(zt|xt) (3)
",2. The model,[0],[0]
as the amortised variational distribution.,2. The model,[0],[0]
We refer to this as “factorised q” in the experiments section.,2. The model,[0],[0]
This factorization assumes that content features are approximately independent of motion features.,2. The model,[0],[0]
"Furthermore, note that the distribution over content features is conditioned on the entire time series, whereas the dynamical features are only conditioned on the individual frames.
",2. The model,[0],[0]
"The second encoder assumes that the variational posterior of z1:T depends on f , and the q distribution has the following architecture:
qφ(z1:T ,f |x1:T ) = qφ(f |x1:T )qφ(z1:T |f ,x1:T ), (4)
and the distribution q(z1:T |f ,x1:T ) is conditioned on the entire time series.",2. The model,[0],[0]
"It can be implemented by e.g. a bidirectional LSTM (Graves & Schmidhuber, 2005) conditioned on f , followed by an RNN taking the bi-LSTM hidden states as the inputs.",2. The model,[0],[0]
We provide a visualisation of the corresponding computation graph in the appendix.,2. The model,[0],[0]
This encoder is referred to as “full q”.,2. The model,[0],[0]
"The idea behind the structured approximation is that content may affect dynamics: in video, the shape of objects may be informative about their motion patterns, thus z1:T is conditionally dependent on f .",2. The model,[0],[0]
"The architectures of the generative model and both encoders are visualised in Figure 1.
",2. The model,[0],[0]
Unconditional generation.,2. The model,[0],[0]
"After training, one can use the generative model to synthesise video or audio sequences
by sampling the latent variables from the prior and decoding them.",2. The model,[0],[0]
"Furthermore, the proposed generative model allows generation of multiple sequences entailing the same global information (e.g. the same object in a video sequence), simply by fixing f ∼ p(f), sampling different zk1:T ∼ p(z1:T ), k = 1, ...,K, and generating the observations xkt ∼ p(xt|zkt ,f).",2. The model,[0],[0]
"Generating sequences with similar dynamics is done analogously, by fixing z1:T ∼ p(z1:T ) and sampling fk, k = 1, ...K from the prior.
",2. The model,[0],[0]
Conditional generation.,2. The model,[0],[0]
"Together with the encoder, the model also allows conditional generation of sequences.",2. The model,[0],[0]
"As an example, given a video sequence x1:T as reference, one can manipulate the latent variables and generate new sequences preserving either the object identity or the pose/movement information.",2. The model,[0],[0]
"This is done by conditioning on f ∼ q(f |x1:T ) for a given x1:T then randomising z1:T from the prior, or the other way around.
",2. The model,[0],[0]
Feature swapping.,2. The model,[0],[0]
One might also want to generate a new video sequence with the object identity and pose information encoded from different sequence.,2. The model,[0],[0]
"Given two sequences xa1:T and x b 1:T , the synthesis process first infers the latent variables fa ∼ q(f |xa1:T ) and zb1:T ∼ q(z1:T |xb1:T )1, then produces a new sequence by sampling xnewt ∼ p(xt|zbt ,fa).",2. The model,[0],[0]
"This allows us to control both the content and the dynamics of the generated sequence, which can be applied to e.g. conversion of voice of the speaker in a speech sequence.",2. The model,[0],[0]
Research on learning disentangled representation has mainly focused on two aspects: the training objective and the generative model architecture.,3. Related work,[0],[0]
"Regarding the loss function design for VAE models, Higgins et al. (2016) propose the β-VAE by scaling up the KL[q(z|x)||p(z)] term in the variational lower-bound with β > 1 to encourage learning of independent attributes (as the prior p(z) is usually factorised).",3. Related work,[0],[0]
"While the β-VAE has been shown effective in learning better representations for natural images and might be able to further improve the performance of our model, we do not
1For the full q encoder",3. Related work,[0],[0]
"it also requires f b ∼ q(f |xb1:T ).
",3. Related work,[0],[0]
"test this recipe here to demonstrate that disentanglement can be achieved by a careful model design.
",3. Related work,[0],[0]
"For sequence modelling, a number of prior publications have extended VAE to video and speech data (Fabius & van Amersfoort, 2014; Bayer & Osendorfer, 2014; Chung et al., 2015).",3. Related work,[0],[0]
"These models, although being able to generate realistic sequences, do not explicitly disentangle the representation of time-invariant and time-dependent information.",3. Related work,[0],[0]
"Thus it is inconvenient for these models to perform tasks such as controlled generation and feature swapping.
",3. Related work,[0],[0]
"For GAN-like models, both Villegas et al. (2017) and Denton & Birodkar (2017) proposed an auto-encoder architecture for next frame prediction, with two separate encoders responsible for content and pose information at each time step.",3. Related work,[0],[0]
"While in Villegas et al. (2017), the pose information is extracted from the difference between two consecutive frames xt−1 and xt, Denton & Birodkar (2017) directly encoded xt for both pose and content, and further designed a training objective to encourage learning of disentangled representations.",3. Related work,[0],[0]
"On the other hand, Vondrick et al. (2016) used a spatio-temporal convolutional architecture to disentangle a video scene’s foreground from its background.",3. Related work,[0],[0]
"Although it has successfully achieved disentanglement, we note that the time-invariant information in this model is predefined to represent the background, rather than learned from the data automatically.",3. Related work,[0],[0]
"Also this architecture is suitable for video sequences only, unlike our model which can be applied to any type of sequential data.
",3. Related work,[0],[0]
"Very recent work (Hsu et al., 2017) introduced the factorised hierarchical variational auto-encoder (FHVAE) for unsupervised learning of disentangled representation of speech data.",3. Related work,[0],[0]
"Given a speech sequence that has been partitioned into segments {xn1:T }Nn=1, FHVAE models the joint distribution of {xn1:T }Nn=1 and latent variables as follows:
p({xn1:T , zn1 , zn2 },µ2) = p(µ2) N∏
n=1
p(xn1:T , z n 1 , z n 2 |µ2),
p(xn1:T , z n 1 , z n 2 |µ2) = p(zn1 )p(zn2",3. Related work,[0],[0]
"|µ2)p(xn1:T |zn1 , zn2 ).
",3. Related work,[0],[0]
"Here the zn2 variable has a hierarchical prior p(z n 2 |µ2) = N (µ2, σ2I), p(µ2) = N (0, λI).",3. Related work,[0],[0]
"The authors showed that by having different prior structures for zn1 and z n 2 , it allows the model to encode with zn2 speech sequence-level
attributes (e.g. pitch of a speaker), and other residual information with zn1 .",3. Related work,[0],[0]
"A discriminative training objective (see discussions in Section 4.2) is added to the variational lowerbound, which has been shown to further improve the quality of the disentangled representation.",3. Related work,[0],[0]
"Our model can also benefit from the usage of hierarchical prior distributions, e.g. fn ∼ p(f |µ2),µ2 ∼ p(µ2), and we leave the investigation to future work.",3. Related work,[0],[0]
We carried out experiments both on video data (Section 4.1) as well as speech data (Section 4.2).,4. Experiments,[0],[0]
"In both setups, we find strong evidence that our model learns an approximately disentangled representation that allows for conditional generation and feature swapping.",4. Experiments,[0],[0]
We further investigated the efficiency for encoding long sequences with a stochastic transition model in Section 4.3.,4. Experiments,[0],[0]
The detailed model architectures of the networks used in each experiment are reported in the appendix.,4. Experiments,[0],[0]
"We present an initial test of the proposed VAE architecture on a dataset of video game “sprites”, i.e. animated cartoon characters whose clothing, pose, hairstyle, and skin color we can fully control.",4.1. Video sequence: Sprites,[0],[0]
"This dataset comes from an open-source video game project called Liberated Pixel Cup2, and has been also considered in Reed et al. (2015); Mathieu et al. (2016) for image processing experiments.",4.1. Video sequence: Sprites,[0],[0]
"Our experiments show that static attributes such as hair color and clothing are well preserved over time for randomly generated videos.
",4.1. Video sequence: Sprites,[0],[0]
Data and preprocessing.,4.1. Video sequence: Sprites,[0],[0]
"We downloaded and selected the online available sprite sheets3, and organised them into 4 attribute categories (skin color, tops, pants and hairstyle) and 9 action categories (walking, casting spells and slashing, each with three viewing angles).",4.1. Video sequence: Sprites,[0],[0]
"In order to avoid a combinatorial explosion problem, each of the attribute categories contains 6 possible variants (see Figure 2), therefore it leads to 64 = 1296 unique characters in total.",4.1. Video sequence: Sprites,[0],[0]
We used 1000 of them for training/validation and the rest of them for testing.,4.1. Video sequence: Sprites,[0],[0]
The resulting dataset consists of sequences with T = 8 frames of dimension 64× 64.,4.1. Video sequence: Sprites,[0],[0]
Note here we did not use the labels for training the generative model.,4.1. Video sequence: Sprites,[0],[0]
"Instead these labels on the data frames are used to train a classifier that is later deployed to produce quantitative evaluations on the VAE, see below.
",4.1. Video sequence: Sprites,[0],[0]
Qualitative analysis.,4.1. Video sequence: Sprites,[0],[0]
We start with a qualitative evaluation of our VAE architecture.,4.1. Video sequence: Sprites,[0],[0]
"Figure 3 shows both re-
2http://lpc.opengameart.org/",4.1. Video sequence: Sprites,[0],[0]
"3https://github.com/jrconway3/
Universal-LPC-spritesheet
constructed as well as generated video sequences from our model.",4.1. Video sequence: Sprites,[0],[0]
Each panel shows three video sequences with time running from left to right.,4.1. Video sequence: Sprites,[0],[0]
"Panel (a) shows parts of the original data from the test set, and (b) shows its reconstruction.
",4.1. Video sequence: Sprites,[0],[0]
The sequences visualised in panel (c) are generated using zt ∼ q(zt|xt) but f ∼ p(f).,4.1. Video sequence: Sprites,[0],[0]
"Hence, the dynamics are imposed by the encoder, but the identity is sampled from the prior.",4.1. Video sequence: Sprites,[0],[0]
"We see that panel (c) reveals the same motion patterns as (a), but has different character identities.",4.1. Video sequence: Sprites,[0],[0]
"Conversely, in panel (d) we take the identity from the encoder, but sample the dynamics from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Panel (d) reveals the same characters as (a), but different motion patterns.
",4.1. Video sequence: Sprites,[0],[0]
Panels (e) and (f) focus on feature swapping.,4.1. Video sequence: Sprites,[0],[0]
"In (e), the frames are constructed by computing zt ∼ q(zt|xt) on one input sequence but f encoded on another input sequence.",4.1. Video sequence: Sprites,[0],[0]
"These panels demonstrate that the encoder and the decoder have learned a factored representation for content and pose.
",4.1. Video sequence: Sprites,[0],[0]
"Panels (g) and (h) focus on conditional generation, showing randomly generated sequences that share the same f or z1:T samples from the prior.",4.1. Video sequence: Sprites,[0],[0]
"Thus, in panel (g) we see the same character performing different actions, and in (h) different characters performing the same motion.",4.1. Video sequence: Sprites,[0],[0]
"This again illustrates that the prior model disentangles the representation.
",4.1. Video sequence: Sprites,[0],[0]
Quantitative analysis.,4.1. Video sequence: Sprites,[0],[0]
"Next we perform quantitative evaluations of the generative model, using a classifier trained on the labelled frames.",4.1. Video sequence: Sprites,[0],[0]
"Empirically, we find that the fully factorized and structured inference networks produce almost identical results here, presumably because in this dataset the object identity and pose information are truly independent.",4.1. Video sequence: Sprites,[0],[0]
"Therefore we only report results on the fully factorised q distribution case.
",4.1. Video sequence: Sprites,[0],[0]
The first evaluation task considers reconstructing the test sequences with encoded f and randomly sampled zt (in the same way as to produce panel (d) in Figure 3).,4.1. Video sequence: Sprites,[0],[0]
Then we compare the classifier outputs on both the original frames and the reconstructed frames.,4.1. Video sequence: Sprites,[0],[0]
"If the character’s identity is preserved over time, the classifier should produce identical probability vectors on the data frames and the reconstructed frames (denoted as pdata and precon respectively).
",4.1. Video sequence: Sprites,[0],[0]
We evaluate the similarity between the original and reconstructed sequences both in terms of the disagreement of the predicted class labels maxi[precon(i)] 6= maxi[pdata(i)] and the KL-divergence KL[precon||pdata].,4.1. Video sequence: Sprites,[0],[0]
We also compute the two metrics on the action predictions using reconstructed sequences with randomised f and inferred zt.,4.1. Video sequence: Sprites,[0],[0]
The results in Table 1 indicate that the learned representation is indeed factorised.,4.1. Video sequence: Sprites,[0],[0]
"For example, in the fix-f generation test, only 4% out of 296× 9 data-reconstruction frame pairs contain characters whose generated skin color differs from the rest, where in the case of hairstyle preservation the disagreement rate is only 0.06%.",4.1. Video sequence: Sprites,[0],[0]
The KL metric is also much smaller than the KL-divergence KL[prandom||pdata],4.1. Video sequence: Sprites,[0],[0]
"where prandom = (1/Nclass, ..., 1/Nclass), indicating that our result is significant.
",4.1. Video sequence: Sprites,[0],[0]
"In the second evaluation, we test whether static attributes of generated sequences, such as clothing or hair style, are preserved over time.",4.1. Video sequence: Sprites,[0],[0]
"We sample 200 video sequences from the generator, using the same f but different latent dynamics z1:T .",4.1. Video sequence: Sprites,[0],[0]
We use the trained classifier to predict both the attributes and the action classes for each of the generated frames.,4.1. Video sequence: Sprites,[0],[0]
"Results are shown in Figure 4(a), where we plot the prediction of the classifiers for each frame over time.",4.1. Video sequence: Sprites,[0],[0]
"For example, the trajectory curve in the “skin color” panel in Figure 4(a) corresponds to the skin color attribute classification results for frames x1:T of a generated video sequence.",4.1. Video sequence: Sprites,[0],[0]
"We repeat this process 5 times with different f samples,
where each f corresponds to one color.
",4.1. Video sequence: Sprites,[0],[0]
"It becomes evident that those lines with the same color are clustered together, confirming that f mainly controls the generation of time-invariant attributes.",4.1. Video sequence: Sprites,[0],[0]
"Also, most character attributes are preserved over time, e.g. for the attribute “tops”, the trajectories are mostly straight lines.",4.1. Video sequence: Sprites,[0],[0]
"However, some of the trajectories for the attributes drift away from the majority class.",4.1. Video sequence: Sprites,[0],[0]
"We conjecture that this is due of the mass-covering behaviour of (approximate) maximum likelihood training, which makes the trained model generate characters that do not exist in the dataset.",4.1. Video sequence: Sprites,[0],[0]
"Indeed the middle row of panel (c) in Figure 3 contains a character with an unseen hairstyle, showing that our model is able to generalise beyond the training set.",4.1. Video sequence: Sprites,[0],[0]
"On the other hand, the sampling process returns sequences with diverse actions as depicted in the action panel, meaning that f contains little information regarding the video dynamics.
",4.1. Video sequence: Sprites,[0],[0]
"We performed similar tests on sequence generations with shared latent dynamics z1:T but different f , shown in Figure 4(b).",4.1. Video sequence: Sprites,[0],[0]
"The experiment is repeated 5 times as well, and again trajectories with the same color encoding correspond to videos generated with the same z1:T (but different f ).",4.1. Video sequence: Sprites,[0],[0]
Here we also observe diverse trajectories for the attribute categories.,4.1. Video sequence: Sprites,[0],[0]
"In contrast, the characters’ actions are mostly the same.",4.1. Video sequence: Sprites,[0],[0]
These two test results again indicate that the model has successfully learned disentangled representations of character identities and actions.,4.1. Video sequence: Sprites,[0],[0]
"Interestingly we observe multi-modalities in the action domain for the generated sequences, e.g. the trajectories in the action panel of Figure 4(b) are jumping between different levels.",4.1. Video sequence: Sprites,[0],[0]
We also visualise in Figure 5 generated sequences of the “turning” action that is not present in the dataset.,4.1. Video sequence: Sprites,[0],[0]
It again shows that the trained model generalises to unseen cases.,4.1. Video sequence: Sprites,[0],[0]
We also experiment on audio sequence data.,4.2. Speech data: TIMIT,[0],[0]
Our disentangled representation allows us to convert speaker identities into each other while conditioning on the content of the speech.,4.2. Speech data: TIMIT,[0],[0]
"We also show that our model gives rise to speaker verification, where we outperform a recent probabilistic baseline model.
(a) Trajectory plots on the generated sequences with shared f .
(b) Trajectory plots on the generated sequences with shared z1:T .
",4.2. Speech data: TIMIT,[0],[0]
Figure 4.,4.2. Speech data: TIMIT,[0],[0]
"Classification test on the generated video sequences with shared f (top) or shared z1:T (bottom), respectively.",4.2. Speech data: TIMIT,[0],[0]
The experiment is repeated 5 times and depicted by different color coding.,4.2. Speech data: TIMIT,[0],[0]
"The x and y axes are time and the class id of the attributes, respectively.
",4.2. Speech data: TIMIT,[0],[0]
Figure 5.,4.2. Speech data: TIMIT,[0],[0]
Visualising multi-modality in action space.,4.2. Speech data: TIMIT,[0],[0]
"In this case the characters turn from left to right, and this action sequence is not observed in data.
Data and preprocessing.",4.2. Speech data: TIMIT,[0],[0]
"The TIMIT data (Garofolo et al., 1993) contains broadband 16kHz recordings of phonetically-balanced read speech.",4.2. Speech data: TIMIT,[0],[0]
A total of 6300 utterances (5.4 hours) are presented with 10 sentences from each of the 630 speakers (70% male and 30% female).,4.2. Speech data: TIMIT,[0],[0]
"We follow Hsu et al. (2017) for data pre-processing: the raw speech waveforms are first split into sub-sequences of 200ms, and then preprocessed with sparse fast Fourier transform to obtain a 200 dimensional log-magnitude spectrum, computed every 10ms.",4.2. Speech data: TIMIT,[0],[0]
"This implies T = 20 for the observation x1:T .
",4.2. Speech data: TIMIT,[0],[0]
Qualitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We perform voice conversion experiments to demonstrate the disentanglement of the learned representation.,4.2. Speech data: TIMIT,[0],[0]
The goal here is to convert male voice to female voice (and vice versa) with the speech content being preserved.,4.2. Speech data: TIMIT,[0],[0]
"Assuming that f has learned the representation of speaker’s identity, the conversion can be done by first encoding two sequences xmale1:T and x female 1:T with q to obtain representations {fmale, zmale1:T } and {f female, zfemale1:T }, then construct the converted sequence by feeding f female and zmale1:T to the decoder p(xt|zt,f).",4.2. Speech data: TIMIT,[0],[0]
Figure 6 shows the reconstructed spectrogram after the swapping process of the f features.,4.2. Speech data: TIMIT,[0],[0]
"We also provide the reconstructed speech waveforms using the Griffin-Lim algorithm (Griffin & Lim, 1984) in the appendix.
",4.2. Speech data: TIMIT,[0],[0]
The experiments show that the harmonics of the converted speech sequences shifted to higher frequency in the “male to female” test and vice versa.,4.2. Speech data: TIMIT,[0],[0]
"Also the pitch (the red arrow in Figure 6 indicating the fundamental frequency, i.e. the first harmonic) of the converted sequence (b) is close to the pitch of (c), same as for the comparison between (d) and (a).",4.2. Speech data: TIMIT,[0],[0]
"By an informal listening test of the speech sequence pairs (a, d) and (b, c), we confirm that the speech content is preserved.",4.2. Speech data: TIMIT,[0],[0]
"These results show that our model is successfully applied to speech sequences for learning disentangled representations.
",4.2. Speech data: TIMIT,[0],[0]
Quantitative analysis.,4.2. Speech data: TIMIT,[0],[0]
We further follow Hsu et al. (2017) to use speaker verification for quantitative evaluation.,4.2. Speech data: TIMIT,[0],[0]
"Speaker verification is the process of verifying the claimed identity of a speaker, usually by comparing the “features” wtest of the test utterance xtest1:",4.2. Speech data: TIMIT,[0],[0]
T1 with those of the target utterance xtarget1:T2 from the claimed identity.,4.2. Speech data: TIMIT,[0],[0]
"The claimed identity is confirmed if the cosine similarity cos(wtest,wtarget) is grater than a given threshold (Dehak et al., 2009).",4.2. Speech data: TIMIT,[0],[0]
By varying ∈,4.2. Speech data: TIMIT,[0],[0]
"[0, 1], we report the verification performance in terms of equal error rate (EER), where the false rejection rate equals the false acceptance rate.
",4.2. Speech data: TIMIT,[0],[0]
The extraction of the “features” is crucial for the performance of this speaker verification system.,4.2. Speech data: TIMIT,[0],[0]
"Given a speech sequence containing N segments {x(n)1:T }Nn=1, we constructed two types of “features”, one by computing µf as the mean
of q(f (n)|x(n)1:T ) across the segments, and the other by extracting the mean µzt of q(zt|x1:T ) and averaging them across both time T and segments.",4.2. Speech data: TIMIT,[0],[0]
"In formulas,
µf = 1
N N∑ n=1 µfn , µfn = Eq(fn|xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[f n],
µz = 1
TN T∑ t=1 N∑ n=1 µznt , µznt = Eq(znt |xn1:T )",4.2. Speech data: TIMIT,[0],[0]
"[z n t ].
We also include two baseline results from Hsu et al. (2017): one used the i-vector method (Dehak et al., 2011) for feature extraction, and the other one used µ1 and µ2 (analogous to µz and µf in our case) from a trained FHVAE model on Mel-scale filter bank (FBank) features.
",4.2. Speech data: TIMIT,[0],[0]
"The test data were created from the test set of TIMIT, containing 24 unique speakers and 18,336 pairs for verification.",4.2. Speech data: TIMIT,[0],[0]
Table 2 presents the EER results of the proposed model and baselines.4,4.2. Speech data: TIMIT,[0],[0]
"It is clear that the µf feature performs significantly better than the i-vector method, indicating that the f variable has learned to represent a speaker’s identity.",4.2. Speech data: TIMIT,[0],[0]
"On the other hand, using µz as the features returns considerably worse EER rates compared to the i-vector method and µf feature.",4.2. Speech data: TIMIT,[0],[0]
"This is good, as it indicates that the z variables contain less information about the speaker’s identity, again validating the success of disentangling time-variant and time-independent information.",4.2. Speech data: TIMIT,[0],[0]
"Note that the EER results for µz get worse when using the full q encoder, and in the 64 dimensional feature case the verification performance of µf improves slightly.",4.2. Speech data: TIMIT,[0],[0]
"This also shows that for real-world data it is useful to use a structured inference network to further improve the quality of disentangled representation.
",4.2. Speech data: TIMIT,[0],[0]
Our results are competitive with (or slightly better than) the FHVAE results (α = 0) reported in Hsu et al. (2017).,4.2. Speech data: TIMIT,[0],[0]
The better results for FHVAE (α = 10) is obtained by adding a discriminative training objective (scaled by α) to the variational lower-bound.,4.2. Speech data: TIMIT,[0],[0]
"In a nutshell, the timeinvariant information in FHVAE is encoded in a latent variable zn2 ∼ p(zn2 |µ2), and the discriminative objective encourages zn2 encoded from a segment of one sequence to be close to the corresponding µ2 while far away from µ2 of other sequences.",4.2. Speech data: TIMIT,[0],[0]
"However, we do not test this idea here because (1) our goal is to demonstrate that the proposed architecture is a minimalistic framework for learning disentangled representations of sequential data; (2) this discriminative objective is specifically designed for hierarchical VAE, and in general the assumption behind it might not always be true (consider encoding two speech sequences coming from the same speaker).",4.2. Speech data: TIMIT,[0],[0]
"Similar ideas for discriminative training have been considered in e.g. Mathieu et al. (2016), but that discriminative objective can only be applied
4 Hsu et al. (2017) did not provide the EER results for α = 0 and µ1 in the 16 dimension case.
to two sequences that are known to entail different timeinvariant information (e.g. two sequences with different labels), which implicitly uses supervisions.",4.2. Speech data: TIMIT,[0],[0]
"Nevertheless, a better design for the discriminative objective without supervision can further improve the disentanglement of the learned representations, and we leave it to future work.",4.2. Speech data: TIMIT,[0],[0]
"Lastly, although not a main focus of the paper, we show that the usage of a stochastic transition model for the prior leads to more realistic dynamics of the generated sequence.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we consider another class of models:
p(x1:T , z,f) = p(f)p(z) T∏ t=1 p(xt|z,f).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The parameters of p(xt|z,f) are defined by a neural network NN(ht,f), with ht computed by a deterministic RNN conditioned on",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"z. We experiment with two types of deterministic dynamics, with the graphical model visualised in appendix.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The first model uses an LSTM with z as the initial state: h0 = z, ht = LSTM(ht−1).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
In later experiments we refer this dynamics as LSTM-f as the latent variable z is forward propagated in a deterministic way.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The second one deploys an LSTM conditioned on z (i.e. h0 = 0,ht = LSTM(ht−1, z)), therefore we refer it as LSTM-c. This is identical to the transition dynamics used in the FHVAE model (Hsu et al., 2017).",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For comparison, we refer to our model as the ’stochastic’ model (Eq. 1).
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The LSTM models encodes temporal information in a global latent variable z. Therefore, small differences/errors in z will accumulate over time, which may result in unrealistic long-time dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"In contrast, the stochastic model (Eq. 1) keeps track of the time-varying aspects of xt in zt
for every t, making the reconstruction to be time-local and therefore much easier.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Therefore, the stochastic model is better suited if the sequences are long and complex.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We give empirical evidence to support this claim.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Data preprocessing & hyper-parameters.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We follow Fraccaro et al. (2017) to simulate video sequences of a ball (or a square) bouncing inside an irregular polygon using Pymunk.5,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The irregular shape was chosen because it induces chaotic dynamics, meaning that small deviations from the initial position and velocity of the ball will create exponentially diverging trajectories at long times.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
This makes memorizing the dynamics of a prototypical sequence challenging.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We randomly sampled the initial position and velocity of the ball, but did not apply any force to the ball, except for the fully elastic collisions with the walls.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We generated 5,000 sequences in total (1000 for test), each of them containing T = 30 frames with a resolution of 32×32.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the deterministic LSTMs, we fix the dimensionality of zt to 64, and set ht and the LSTM internal states to be 512 dimensions.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The latent variable dimensionality of the stochastic dynamics is dim(zt) = 16.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
Qualitative & quantitative analyses.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We consider both reconstruction and missing data imputation tasks for the learned generative models.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For the latter and for T = 30, the models observe the first t < T frames of a sequence and predict the remaining T − t frames using the prior dynamics.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We visualise in Figure 7 the ground truth, recon-
5http://www.pymunk.org/en/latest/.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"For simplicity we disabled rotation of the square when hitting the wall, by setting the inertia to infinity.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"structed, and predicted sequences (t = 20) from all models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We further consider average fraction of incorrectly reconstructed/predicted pixels as a quantitative metric, to evaluate how well the ground-truth dynamics is recovered given consecutive missing frames.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The result is reported in Figure 8.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
The stochastic model outperforms the deterministic models both qualitatively and quantitatively.,4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"The shape of the ball is better preserved over time, and the trajectories are more physical.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"This explains the lower errors of the stochastic model, and the advantage is significant when the number of missing frames is small.
",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"Our experiments give evidence that the stochastic model is better suited to modelling long, complex sequences when compared to the deterministic dynamical models.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
"We expect that a better design for the stochastic transition dynamics, e.g. by combining deep neural networks with well-studied linear dynamical systems (Krishnan et al., 2015; Fraccaro et al., 2016; Karl et al., 2016; Johnson et al., 2016; Krishnan et al., 2017; Fraccaro et al., 2017), can further enhance the quality of the learned representations.",4.3. Comparing stochastic & deterministic dynamics,[0],[0]
We presented a minimalistic generative model for learning disentangled representations of high-dimensional time series.,5. Conclusions and outlook,[0],[0]
"Our model consists of a global latent variable for content features, and a stochastic RNN with time-local latent variables for dynamical features.",5. Conclusions and outlook,[0],[0]
The model is trained using standard amortized variational inference.,5. Conclusions and outlook,[0],[0]
We carried out experiments both on video and audio data.,5. Conclusions and outlook,[0],[0]
"Our approach allows us to perform full and conditional generation, as well as feature swapping, such as voice conversion and video content manipulation.",5. Conclusions and outlook,[0],[0]
"We also showed that a stochastic transition model generally outperforms a deterministic one.
",5. Conclusions and outlook,[0],[0]
Future work may investigate whether a similar model applies to more complex video and audio sequences.,5. Conclusions and outlook,[0],[0]
"Also, disentangling may further be improved by additional crossentropy terms, or discriminative training.",5. Conclusions and outlook,[0],[0]
A promising avenue of research is to explore the usage of this architecture for neural compression.,5. Conclusions and outlook,[0],[0]
"An advantage of the model is that it separates dynamical from static features, allowing the latent space for the dynamical part to be low-dimensional.",5. Conclusions and outlook,[0],[0]
"We thank Robert Bamler, Rich Turner, Jeremy Wong and Yu Wang for discussions and feedback on the manuscript.",Acknowledgements,[0],[0]
We also thank Wei-Ning Hsu for helping reproduce the FHVAE experiments.,Acknowledgements,[0],[0]
Yingzhen Li thanks Schlumberger Foundation FFTF fellowship for supporting her PhD study.,Acknowledgements,[0],[0]
"We present a VAE architecture for encoding and generating high dimensional sequential data, such as video or audio.",abstractText,[0],[0]
"Our deep generative model learns a latent representation of the data which is split into a static and dynamic part, allowing us to approximately disentangle latent time-dependent features (dynamics) from features which are preserved over time (content).",abstractText,[0],[0]
This architecture gives us partial control over generating content and dynamics by conditioning on either one of these sets of features.,abstractText,[0],[0]
"In our experiments on artificially generated cartoon video clips and voice recordings, we show that we can convert the content of a given sequence into another one by such content swapping.",abstractText,[0],[0]
"For audio, this allows us to convert a male speaker into a female speaker and vice versa, while for video we can separately manipulate shapes and dynamics.",abstractText,[0],[0]
"Furthermore, we give empirical evidence for the hypothesis that stochastic RNNs as latent state models are more efficient at compressing and generating long sequences than deterministic ones, which may be relevant for applications in video compression.",abstractText,[0],[0]
Disentangled Sequential Autoencoder,title,[0],[0]
"Proceedings of NAACL-HLT 2013, pages 820–825, Atlanta, Georgia, 9–14 June 2013. c©2013 Association for Computational Linguistics
In this paper, we propose a multi-step stacked learning model for disfluency detection. Our method incorporates refined n-gram features step by step from different word sequences. First, we detect filler words. Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text. In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection. We use Max-Margin Markov Networks (M3Ns) as the classifier with the weighted hamming loss to balance precision and recall. Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and M3Ns with weighted hamming loss can significantly improve the performance. Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1",text,[0],[0]
"Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.",1 Introduction,[1.0],"['Detecting disfluencies in spontaneous speech can be used to clean up speech transcripts, which helps improve readability of the transcripts and make it easy for downstream language processing modules.']"
"There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by
1Our source code is available at http://code.google.com/p/disfluency-detection/downloads/list
the following words.",1 Introduction,[0.9929618021126532],"['There are two types of disfluencies: filler words including filled pauses (e.g., ‘uh’, ‘um’) and discourse markers (e.g., ‘I mean’, ‘you know’), and edited words that are repeated, discarded, or corrected by the following words.']"
"An example is shown below that includes edited words and filler words.
",1 Introduction,[0],[0]
"I want a flight to Boston︸ ︷︷ ︸ edited uh I mean︸ ︷︷ ︸ filler to Denver
Automatic filler word detection is much more accurate than edit detection as they are often fixed phrases (e.g., “uh”, “you know”, “I mean”), hence our work focuses on edited word detection.
",1 Introduction,[0],[0]
Many models have been evaluated for this task.,1 Introduction,[0],[0]
Liu et al. (2006) used Conditional Random Fields (CRFs) for sentence boundary and edited word detection.,1 Introduction,[0],[0]
They showed that CRFs significantly outperformed Maximum Entropy models and HMMs.,1 Introduction,[0],[0]
"Johnson and Charniak (2004) proposed a TAGbased noisy channel model which showed great improvement over boosting based classifier (Charniak and Johnson, 2001).",1 Introduction,[0],[0]
Zwarts and Johnson (2011) extended this model using minimal expected F-loss oriented n-best reranking.,1 Introduction,[0],[0]
They obtained the best reported F-score of 83.8% on the Switchboard corpus.,1 Introduction,[0],[0]
"Georgila (2009) presented a post-processing method during testing based on Integer Linear Programming (ILP) to incorporate local and global constraints.
",1 Introduction,[0],[0]
"From the view of features, in addition to textual information, prosodic features extracted from speech have been incorporated to detect edited words in some previous work (Kahn et al., 2005; Zhang et al., 2006; Liu et al., 2006).",1 Introduction,[0],[0]
"Zwarts and Johnson (2011) trained an extra language model on additional corpora, and used output log probabilities of language models as features in the reranking stage.",1 Introduction,[0],[0]
"They reported that the language model gained about absolute 3% F-score for edited word detection on the Switchboard development dataset.
820
In this paper, we propose a multi-step stacked learning approach for disfluency detection.",1 Introduction,[0],[0]
"In our method, we first perform filler word detection, then edited word detection.",1 Introduction,[0],[0]
"In every step, we generate new refined n-gram features based on the processed text (remove the detected filler or edited words from the previous step), and use these in the next step.",1 Introduction,[0],[0]
"We also include a new type of features, called inbetween features, and incorporate them into the last step.",1 Introduction,[0],[0]
"For edited word detection, we use Max-Margin Markov Networks (M3Ns) with weighted hamming loss as the classifier, as it can well balance the precision and recall to achieve high performance.",1 Introduction,[0],[0]
"On the commonly used Switchboard corpus, we demonstrate that our proposed method outperforms other state-of-the-art systems for edit disfluency detection.",1 Introduction,[0],[0]
"Weighted M3Ns
We use a sequence labeling model for edit detection.",2 Balancing Precision and Recall Using,[1.0000000357193894],['Weighted M3Ns We use a sequence labeling model for edit detection.']
"Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).",2 Balancing Precision and Recall Using,[1.0],"['Each word is assigned one of the five labels: BE (beginning of the multi-word edited region), IE (in the edited region), EE (end of the edited region), SE (single word edited region), O (other).']"
"For example, the previous sentence is represented as:
I/",2 Balancing Precision and Recall Using,[0],[0]
O want/O a/O flight/,2 Balancing Precision and Recall Using,[0],[0]
O to/BE Boston/EE uh/,2 Balancing Precision and Recall Using,[0],[0]
O I/,2 Balancing Precision and Recall Using,[0],[0]
"O mean/O to/O Denver/O
We use the F-score as the evaluation metrics (Zwarts and Johnson, 2011; Johnson and Charniak, 2004), which is defined as the harmonic mean of the precision and recall of the edited words:
P = #correctly predicted edited words
#predicted edited words
R = #correctly predicted edited words
#gold standard edited words
F = 2× P ×R
P + R
There are many methods to train the sequence model, such as CRFs (Lafferty et al., 2001), averaged structured perceptrons (Collins, 2002), structured SVM (Altun et al., 2003), online passive aggressive learning (Crammer et al., 2006).",2 Balancing Precision and Recall Using,[0],[0]
"Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.
",2 Balancing Precision and Recall Using,[0.9999999853012553],"['Previous work has shown that minimizing F-loss is more effective than minimizing log-loss (Zwarts and Johnson, 2011), because edited words are much fewer than normal words.']"
"In this paper, we use Max-margin Markov Networks (Taskar et al., 2004) because our preliminary
results showed that they outperform other classifiers, and using weighted hamming loss is simple in this approach (whereas for perceptron or CRFs, the modification of the objective function is not straightforward).
",2 Balancing Precision and Recall Using,[0],[0]
"The learning task for M3Ns can be represented as follows:
min α
1 2 C∥ ∑ x,y αx,y∆f(x, y)∥22 + ∑ x,y αx,yL(x, y)
s.t. ∑
y
αx,y = 1 ∀x
αx,y ≥ 0, ∀x, y
The above shows the dual form for training M3Ns, where x is the observation of a training sample, y ∈ Y is a label.",2 Balancing Precision and Recall Using,[0],[0]
"α is the parameter needed to be optimized, C > 0 is the regularization parameter.",2 Balancing Precision and Recall Using,[0],[0]
"∆f(x, y) is the residual feature vector: f(x, ỹ)",2 Balancing Precision and Recall Using,[0],[0]
"− f(x, y), where ỹ is the true label of x. L(x, y) is the loss function.",2 Balancing Precision and Recall Using,[0],[0]
"Taskar et al. (2004) used un-weighted hamming loss, which is the number of incorrect components: L(x, y) = ∑ t δ(yt, ỹt), where δ(a, b) is the binary indicator function (it is 0 if a = b).",2 Balancing Precision and Recall Using,[0],[0]
"In our work, we use the weighted hamming loss:
L(x, y) = ∑
t
v(yt, ỹt)δ(yt, ỹt)
where v(yt, ỹt) is the weighted loss for the error when ỹt is mislabeled as yt.",2 Balancing Precision and Recall Using,[0],[0]
Such a weighted loss function allows us to balance the model’s precision and recall rates.,2 Balancing Precision and Recall Using,[1.0],['Such a weighted loss function allows us to balance the model’s precision and recall rates.']
"For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.",2 Balancing Precision and Recall Using,[1.0],"['For example, if we assign a large value to v(O, ·E) (·E denotes SE, BE, IE, EE), then the classifier is more sensitive to false negative errors (edited word misclassified as non-edited word), thus we can improve the recall rate.']"
"In our work, we tune the weight matrix v using the development dataset.",2 Balancing Precision and Recall Using,[0],[0]
"Rather than just using the above M3Ns with some features, in this paper we propose to use stacked learning to incorporate gradually refined n-gram features.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Stacked learning is a meta-learning approach (Cohen and de Carvalho, 2005).",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"Its idea is to use two
(or more) levels of predictors, where the outputs of the low level predictors are incorporated as features into the next level predictors.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
It has the advantage of incorporating non-local features as well as nonlinear classifiers.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[1.0],['It has the advantage of incorporating non-local features as well as nonlinear classifiers.']
"In our task, we do not just use the classifier’s output (a word is an edited word or not) as a feature, rather we use such output to remove the disfluencies and extract new n-gram features for the subsequent stacked classifiers.",3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
We use 10 fold cross validation to train the low level predictors.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
The following describes the three steps in our approach.,3 Multi-step Stacked Learning for Edit Disfluency Detection,[0],[0]
"In the first step, we automatically detect filler words.",3.1 Step 1: Filler Word Detection,[1.0],"['In the first step, we automatically detect filler words.']"
"Since filler words often occur immediately after edited words (before the corrected words), we expect that removing them will make rough copy detection easy.",3.1 Step 1: Filler Word Detection,[0],[0]
"For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.",3.1 Step 1: Filler Word Detection,[1.0],"['For example, in the previous example shown in Section 1, if “uh I mean” is removed, then the reparandum “to Boston” and repair “to Denver” will be adjacent and we can use word/POS based ngram features to detect that disfluency.']"
"Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.
",3.1 Step 1: Filler Word Detection,[0.9999999017026853],"['Otherwise, the classifier needs to skip possible filler words to find the rough copy of the reparandum.']"
"For filler word detection, similar to edited word detection, we define 5 labels: BP , IP , EP , SP , O. We use un-weighted hamming loss to learn M3Ns for this task.",3.1 Step 1: Filler Word Detection,[0],[0]
"Since for filler word detection, our performance metric is not F-measure, but just the overall accuracy in order to generate cleaned text for subsequent n-gram features, we did not use the weighted hamming hoss for this.",3.1 Step 1: Filler Word Detection,[0],[0]
The features we used are listed in Table 1.,3.1 Step 1: Filler Word Detection,[0],[0]
All n-grams are extracted from the original text.,3.1 Step 1: Filler Word Detection,[0],[0]
"In the second step, edited words are detected using M3Ns with the weighted-hamming loss.",3.2 Step 2: Edited Word Detection,[0],[0]
The features we used are listed in Table 2.,3.2 Step 2: Edited Word Detection,[0],[0]
All n-grams in the first step are also used here.,3.2 Step 2: Edited Word Detection,[0],[0]
"Besides that, word n-grams, POS n-grams and logic",3.2 Step 2: Edited Word Detection,[0],[0]
n,3.2 Step 2: Edited Word Detection,[0],[0]
-grams extracted from filler word removed text are included.,3.2 Step 2: Edited Word Detection,[0],[0]
"Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.",3.2 Step 2: Edited Word Detection,[1.0],"['Feature templates I(w0, w′i) is to generate features detecting rough copies separated by filler words.']"
"In this step, we use n-gram features extracted from the text after removing edit disfluencies based on
the previous step.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"According to our analysis of the errors produced by step 2, we observed that many errors occurred at the boundaries of the disfluencies, and the word bigrams after removing the edited words are unnatural.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"The following is an example:
• Ref: The new type is prettier than what their/SE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"• Sys: The new type is prettier than what/BE their/EE they used to look like.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Using the system’s prediction, we would have bigram than they, which is odd.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"Usually, the pronoun following than is accusative case.",3.3 Step 3: Refined Edited Word Detection,[1.0],"['Usually, the pronoun following than is accusative case.']"
We expect adding n-gram features derived from the cleaned-up sentences would allow the new classifier to fix such hypothesis.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"This kind of n-gram features is similar to the language models used in (Zwarts and Johnson,
2011).",3.3 Step 3: Refined Edited Word Detection,[0.9999999495919666],"['This kind of n-gram features is similar to the language models used in (Zwarts and Johnson, 2011).']"
"They have the benefit of measuring the fluency of the cleaned text.
",3.3 Step 3: Refined Edited Word Detection,[1.00000001811111],['They have the benefit of measuring the fluency of the cleaned text.']
"Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.",3.3 Step 3: Refined Edited Word Detection,[1.0],"['Another common error we noticed is caused by the ambiguities of coordinates, because the coordinates have similar patterns as rough copies.']"
"For example,
• Coordinates: they ca n′t decide which are the good aspects and which are the bad aspects
• Rough Copies: it/",3.3 Step 3: Refined Edited Word Detection,[0],[0]
"BE ′s/IE a/IE pleasure/IE to/EE it s good to get outside
To distinguish the rough copies and the coordinate examples shown above, we analyze the training data statistically.",3.3 Step 3: Refined Edited Word Detection,[0],[0]
We extract all the pieces lying between identical word bigrams AB . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
AB.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
The observation is that coordinates are often longer than edited sequences.,3.3 Step 3: Refined Edited Word Detection,[1.0],['The observation is that coordinates are often longer than edited sequences.']
Hence we introduce the in-between features for each word.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).",3.3 Step 3: Refined Edited Word Detection,[1.0],"['If a word lies between identical word bigrams, then its in-between feature is the log length of the subsequence lying between the two bigrams; otherwise, it is zero (we use log length to avoid sparsity).']"
We also used other patterns such as A . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
A and ABC . . .,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"ABC, but they are too noisy or infrequent and do not yield much performance gain.
",3.3 Step 3: Refined Edited Word Detection,[0],[0]
Table 3 lists the feature templates used in this last step.,3.3 Step 3: Refined Edited Word Detection,[0],[0]
"We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).",4.1 Experimental Setup,[1.0],"['We use the Switchboard corpus in our experiment, with the same train/develop/test split as the previous work (Johnson and Charniak, 2004).']"
"We also remove the partial words and punctuation from the training and test data for the reason to simulate the situation when speech recognizers are used and
such kind of information is not available (Johnson and Charniak, 2004).
",4.1 Experimental Setup,[0],[0]
We tuned the weight matrix for hamming loss on the development dataset using simple grid search.,4.1 Experimental Setup,[0],[0]
"The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.",4.1 Experimental Setup,[1.0],"['The diagonal elements are fixed at 0; for false positive errors, O → ·E (non-edited word mis-labeled as edited word), their weights are fixed at 1; for false negative errors, ·E → O, we tried the weight from 1 to 3, and increased the weight 0.5 each time.']"
The optimal weight matrix is shown in Table 4.,4.1 Experimental Setup,[1.0],['The optimal weight matrix is shown in Table 4.']
"Note that we use five labels in the sequence labeling task; however, for edited word detection evaluation, it is only a binary task, that is, all of the words labeled with ·E will be mapped to the class of edited words.",4.1 Experimental Setup,[0],[0]
"We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.",4.2 Results,[1.0],"['We compare several sequence labeling models: CRFs, structured averaged perceptron (AP), M3Ns with un-weighted/weighted loss, and online passiveaggressive (PA) learning.']"
"For each model, we tuned the parameters on the development data:",4.2 Results,[0],[0]
"Gaussian prior for CRFs is 1.0, iteration number for AP is 10, iteration number and regularization penalty for PA are 10 and 1.",4.2 Results,[0],[0]
"For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.",4.2 Results,[1.0],"['For M3Ns, we use Structured Sequential Minimal Optimization (Taskar, 2004) for model training.']"
"Regularization penalty is C = 0.1 and iteration number is 30.
",4.2 Results,[1.0000000260488962],['Regularization penalty is C = 0.1 and iteration number is 30.']
Table 5 shows the results using different models and features.,4.2 Results,[0],[0]
The baseline models use only the ngrams features extracted from the original text.,4.2 Results,[1.0],['The baseline models use only the ngrams features extracted from the original text.']
"We can see that M3Ns with the weighted hamming loss achieve the best performance, outperforming all the other models.",4.2 Results,[0],[0]
"Regarding the features, the gradually added n-gram features have consistent improvement for all models.",4.2 Results,[0],[0]
"Using the weighted hamming loss in M3Ns, we observe a gain of 2.2% after deleting filler words, and 1.8% after deleting edited words.",4.2 Results,[0],[0]
"In our analysis, we also noticed that the in-between fea-
tures yield about 1% improvement in F-score for all models (the gain of step 3 over step 2 is because of the in-between features and the new n-gram features extracted from the text after removing previously detected edited words).",4.2 Results,[0],[0]
"We performed McNemar’s test to evaluate the significance of the difference among various methods, and found that when using the same features, weighted M3Ns significantly outperforms all the other models (p value < 0.001).",4.2 Results,[0],[0]
"There are no significant differences among CRFs, AP and PA.",4.2 Results,[0],[0]
"Using recovered n-gram features and inbetween features significantly improves all sequence labeling models (p value < 0.001).
",4.2 Results,[0],[0]
"We also list the state-of-the-art systems evaluated on the same dataset, as shown in Table 6.",4.2 Results,[0],[0]
We achieved the best F-score.,4.2 Results,[0],[0]
"The most competitive system is (Zwarts and Johnson, 2011), which uses extra resources to train language models.",4.2 Results,[0],[0]
"In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.",5 Conclusion,[1.0],"['In this paper, we proposed multi-step stacked learning to extract n-gram features step by step.']"
The first level removes the filler words providing new ngrams for the second level to remove edited words.,5 Conclusion,[0],[0]
"The
third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.",5 Conclusion,[1.0000000264392377],['The third level uses the n-grams from the original text and the cleaned text generated by the previous two steps for accurate edit detection.']
"To minimize the F-loss approximately, we modified the hamming loss in M3Ns.",5 Conclusion,[1.0],"['To minimize the F-loss approximately, we modified the hamming loss in M3Ns.']"
"Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.",5 Conclusion,[1.0],"['Experimental results show that our method is effective, and achieved the best reported performance on the Switchboard corpus without the use of any additional resources.']"
We thank three anonymous reviewers for their valuable comments.,Acknowledgments,[0],[0]
This work is partly supported by DARPA under Contract No. HR0011-12-C-0016 and FA8750-13-2-0041.,Acknowledgments,[0],[0]
Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of DARPA.,Acknowledgments,[0],[0]
"In this paper, we propose a multi-step stacked learning model for disfluency detection.",abstractText,[0],[0]
Our method incorporates refined n-gram features step by step from different word sequences.,abstractText,[0],[0]
"First, we detect filler words.",abstractText,[0],[0]
"Second, edited words are detected using n-gram features extracted from both the original text and filler filtered text.",abstractText,[0],[0]
"In the third step, additional n-gram features are extracted from edit removed texts together with our newly induced in-between features to improve edited word detection.",abstractText,[0],[0]
We use Max-Margin Markov Networks (MNs) as the classifier with the weighted hamming loss to balance precision and recall.,abstractText,[0],[0]
Experiments on the Switchboard corpus show that the refined n-gram features from multiple steps and MNs with weighted hamming loss can significantly improve the performance.,abstractText,[0],[0]
Our method for disfluency detection achieves the best reported F-score 0.841 without the use of additional resources.1,abstractText,[0],[0]
Disfluency Detection Using Multi-step Stacked Learning,title,[0],[0]
