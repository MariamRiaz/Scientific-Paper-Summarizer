4	24	This paper addresses the problem by building a joint model for Entity Recognition and Disambiguation (ERD).
9	47	Existing ERD systems typically run a NER to extract entity mentions first, then run an entity linking model to link mentions to a knowledge base.
14	29	Example 1 is the first sentence from the Wikipedia article about “The New York Times”.
15	53	It is reasonable but incorrect for NER to identify “New York Times” without “The” as a named entity, while entity linking has no trouble connecting “The New York Times” to the correct entity.
16	25	879 Example 2 is a news title where our NER classifies “WASHINGTON” as a location, since a location followed by a date is a frequent pattern in news articles it learned, while the entity linking prefers linking this mention to the U.S. president “George Washington” since another president’s name “Clinton” is mentioned in the context.
18	38	Modeling such mutual dependency is helpful in resolving inconsistency and improving performance for both NER and linking.
28	21	The difference between our model and theirs is that our model jointly models NER and linking tasks from the training phrase, while their model is a combined one which depends on an existing state-of-art NER system.
32	110	However, we believe that joint optimization is a promising direction for improving performance for NLP tasks since it is closer to how human beings process text information.
33	28	Experiment result indicates that our joint model does a better job at both NER and linking tasks than separate models with the same features, and outperforms state-of-art systems on a widely used data set.
34	22	We found improvements of 0.4% absolute F1 for NER on CoNLL’03 and 0.36% absolute precision@1 for linking on AIDA.
37	61	We identify the mutual dependency between NER and linking tasks, and argue that NER and linking should be conducted together to improve the end to end performance.
41	44	The remainder of this paper is organized as follows: the next section discusses related works on NER, entity linking, and joint optimization; section 3 presents our Joint Entity Recognition and Linking model in detail; section 4 describes experiments, results, and analysis; and section 5 concludes.
62	23	Named entity recognition is usually formalized as a sequence labeling task, in which each word is classified to not-an-entity or entity labels.
63	33	Conditional Random Fields (CRFs) is one of the popular models used.
64	54	Most features used in NER are word-level (e.g. a word sequence appears at position i or whether a word contains exactly four digits).
66	21	Entity linking is typically formalized as a ranking task.
67	39	Features used for entity linking are at entitylevel inherently (such as entity prior probability; whether there are any related entity names or discriminative keywords occurring in the context).
68	86	The main challenges of joint optimization between NER and linking are: how to combine a sequence labeling model and a ranking model; and how to incorporate word-level and entity-level features.
69	24	In a linear chain CRF model, each word’s label is assumed to depend on the observations and the label of its previous word.
70	64	Semi-CRF carefully relaxes the Markov assumption between words in CRF, and models the distribution of segmentation boundaries directly.
71	120	We further extend Semi-CRF to model entity distribution and mutual dependency over segmentations, and name it Joint Entity Recognition and Linking (JERL).
72	79	Let x = {xi} be a word sequence containing |x| words.
73	36	Let s = {sj} be a segmentation assignment over x, where segment sj = (uj , vj) consist of a start position uj and an end position vj .
77	27	Each entity ej,k is associated with a label yej,k ∈ {0, 1}.
80	20	Based on the preliminaries and notations, Figure 1 shows the factor graph (Kschischang et al., 2001) of JERL.
88	64	Actually, every local features used in NER can be formulated in this way, and thus can be included in JERL.
90	40	Features defined on x, sj , yej,k are written as gel(x, sj , yej,k) and are called “linking features”.
91	154	These features model joint probabilities of word sequence xsj and linking decisions y k j,k = 1(0 ≤ k ≤ |Esj |) given context x. JERL incorporates all linking features in this way.
93	48	These features model “mutual dependency” between NER and linking’s outputs.
99	43	From the mention generation perspective, JERL actually considers every possible assignment and is able to find the optimal a.
