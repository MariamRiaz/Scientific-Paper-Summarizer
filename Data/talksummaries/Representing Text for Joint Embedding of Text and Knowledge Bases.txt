0	20	Representing information about real-world entities and their relations in structured knowledge base (KB) form enables numerous applications.
2	39	This has motivated research in automatically deriving new facts to extend a manually built knowledge base, by using information from the existing knowledge base, textual mentions of entities, and semi-structured data such as tables and web forms (Nickel et al., 2015).
3	67	In this paper we build upon the work of Riedel et al. (2013), which jointly learns continuous representations for knowledge base and textual relations.
8	17	Following prior work in latent feature models for knowledge base completion, every textual relation receives its own continuous representation, learned from the pattern of its co-occurrences in the knowledge graph.
9	26	However, largely synonymous textual relations often share common sub-structure, and are composed of similar words and dependency arcs.
13	33	The knowledge base is paired with textual mentions for all entity pairs derived from ClueWeb121 with Freebase entity mention annotations (Gabrilovich et al., 2013).
29	65	Note that these vector representations are based on the co-occurrence patterns for the textual relations and not on their compositional structure.
30	22	Cooccurrence based textual relation representations were also learned in (Neelakantan et al., 2015).
37	24	Additionally, we evaluate the approach on a dataset that contains rich prior information from the training knowledge base, as well as a wealth of textual information from a large document collection.
42	21	We begin by introducing notation to define the task, largely following the terminology in Nickel et al. (2015).
68	42	It aims to capture the compatibility be- tween entities and the subject and object positions of relations.
69	25	For each relation type r, the model learns two latent feature vectors v(rs) and v(ro) of dimension K. For each entity (node) ei, the model also learns a latent feature vector of the same dimensionality.
74	44	In this model, each entity ei and each relation r is assigned a latent feature vector of dimensionK.
89	17	We build on work using similar intuitions for other tasks and learn compositional representations of textual relations based on their internal structure, so that the derived representations are accurate for the task of predicting knowledge base relations.
90	19	We use a convolutional neural network applied to the lexicalized dependency paths treated as a sequence of words and dependency arcs with direction.
92	24	In the first layer, each word or directed labeled arc is mapped to a continuous representation using an embedding matrix V. In the hidden layer, every window of three elements is mapped to a hidden vector using position-specific maps W, a bias vector b, and a tanh activation function.
94	17	The CONV representation of textual relations can be used to augment any of the three basic models.
104	18	e f(es,r,e′;Θ) Conditional probabilities for subject entities p(es|eo, r; Θ) are defined analogously.
115	24	We use the FB15k-237 4 dataset, which is a subset of FB15k (Bordes et al., 2013) that excludes redundant relations and direct training links for held-out triples, with the goal of making the task more realistic (Toutanova and Chen, 2015).
116	31	The FB15k dataset has been used in multiple studies on knowledge base completion (Wang et al., 2014b; Yang et al., 2015).
117	21	Textual relations for 1504 FB15k-237 are extracted from 200 million sentences in the ClueWeb12 corpus coupled with Freebase mention annotations (Gabrilovich et al., 2013), and include textual links of all co-occurring entities from the KB set.
121	16	The two rows list statistics for the KB and text portions of the data separately.
130	43	We report the mean reciprocal rank (MRR) of the correct entity, as well as HITS@10 — the percentage of test triples for which the correct entity is ranked in the top 10.
159	17	Unlike model F, it is able to share parameters among entity pairs with common subject or object entities, and, unlike model E, it captures some dependencies between the subject and object entities of a relation.
160	16	The combination of models E+DISTMULT improves performance, but combining model F with the other two is not helpful.
164	18	We show the performance of each individual model and its corresponding variant with a CONV parameterization.
168	38	This gives it an advantage on test triples with textual mentions, but model F still does relatively very poorly overall when taking into account the much more numerous test triples without textual mentions.
169	147	The CONV parameterization performs slightly worse in MRR, but slightly better in HITS@10, compared to the atomic parameterization.
170	126	For model E and its CONV variant, we see that text does not help as its performance using text is the same as that when not using text and the optimal weight of the text is zero.
173	39	The basic model benefits from text slightly and the model with compositional representations of textual patterns CONVE+CONV-DISTMULT, improves the performance further, by 2.4 MRR overall, and by 5 MRR on triples with textual mentions.
