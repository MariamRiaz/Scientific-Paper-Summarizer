0	43	The central idea of model-based reinforcement learning is to decompose the RL problem into two subproblems: learning a model of the environment, and then planning with this model.
7	13	In this paper we introduce a new architecture, which we call the predictron, that integrates learning and planning into one end-to-end training procedure.
11	71	If our model makes accurate predictions, then an optimal plan with respect to our model will also be optimal for the underlying game – even if the model uses a different state space (e.g., abstract representations of enemy positions, ignoring their shapes and colours), action space (e.g., highlevel actions to move away from an enemy), rewards (e.g., a single abstract step could have a higher value than any real reward), or even time-step (e.g., a single abstract step could “jump” the agent to the end of a corridor).
12	26	All we require is that trajectories through the abstract model produce scores that are consistent with trajectories through the real environment.
18	13	In this case, the predictron can be implemented as a deep neural network with an MRP as a recurrent core.
19	16	The predictron unrolls this core multiple steps and accumulates rewards into an overall estimate of value.
23	14	The MRP is defined by a function, s′, r, γ = p(s, α), where s′ is the next state, r is the reward, and γ is the discount factor, which can for instance represent the non-termination probability for this transition.
28	11	The value function of an MRP p is the expected return from state s, vp(s) = Ep [gt | st = s].
35	95	First, a state representation s = f(s) that encodes raw input s (this could be a history of observations, in partially observed settings, for example when f is a recurrent network) into an internal (abstract, hidden) state s. Second, a model s′, r, γ = m(s, β) that maps from internal state s to subsequent internal state s′, internal rewards r, and internal discounts γ .
39	99	Finally, these internal rewards, discounts and values are combined together by an accumulator into an overall estimate of value g. The whole predictron, from input state s to output, may be viewed as a value function approximator for external targets (i.e., the returns in the real environment).
50	21	0 in a single “forward” pass of the network (see Figure 1b).
52	46	The individual λk weights may depend on the corresponding abstract state sk and can differ per prediction.
54	30	We first consider updates that optimise the joint parameters θ of the state representation, model, and value function.
79	11	In the first task, the input was a 13× 13 maze and a random initial position and the goal is to predict a trajectory generated by a simple fixed deterministic policy.
82	23	Two locations in a maze are considered connected if they are both empty and we can reach one from the other by moving horizontally or vertically through adjacent empty cells.
83	22	In both cases some predictions would seem to be easier if we could learn a simple algorithm, such as some form of search or flood fill; our hypothesis is that an internal model can learn to emulate such algorithms, where naive approximation may struggle.
87	12	We generate sequences of RGB frames starting from a random arrangement of balls on the table.
90	32	Each of these 14 × 4 events provides a binary pseudo-reward that we combine with 5 different discount factors {0, 0.5, 0.9, 0.98, 1} and predict their cumulative discounted sum over various time spans.
95	41	Additional domain details are provided in the appendix.
96	43	In the first experiment we trained a predictron to predict trajectories generated by a simple deterministic policy in 13×13 random mazes with random starting positions.
97	26	Figure 3 shows the weighted preturns wkgk and the resulting prediction gλ = ∑ kw kgk for six example inputs and targets.
98	36	The predictions are almost perfect—the training error was very close to zero.
99	105	The full prediction is composed from weighted preturns which decompose the trajectory piece by piece, starting at the start position in the first step k = 1, and where often multiple policy steps are added per planning step.
100	23	The predictron was not informed about the sequential build up of the targets—it never sees a policy walking through the maze, only the resulting trajectories— and yet sequential plans emerged spontaneously.
101	35	Notice also that the easier trajectory on the right was predicted in only two steps, while more thinking steps are used for more complex trajectories.
106	31	In the MRP case internal rewards and discounts are both learned.
120	12	Usage weighting further improved performance.
121	12	Our third set of experiments compares the predictron to feedforward and recurrent deep learning architectures, with and without skip connections.
122	14	We compare the corners of a new cube, as depicted on the left in Figure 5, based on three different binary dimensions.
123	13	The first dimension of this second cube is whether we use a predictron, or a (non-λ, non-(r, γ)) deep network that does not have an internal model and does not output or learn from intermediate predictions.
