0	37	Online forums are now one of the primary venues for public dialogue on current social and political issues.
6	46	Consider for example the sample posts and responses in Fig.
7	26	Argument segments that are good targets for argument extraction are indicated, in their dialogic context, in bold.
9	24	This paper addresses only the argument extraction task, as an important first step towards producing argument summaries that reflect the range and type of arguments being made, 217 on a topic, over time, by citizens in public forums.
15	35	Argument extraction resembles the sentence extraction phase of multi-document summarization.
17	26	Work on multidocument summarization also uses a similar module to merge redundant content from extracted candidate sentences (Barzilay, 2003; Gurevych and Strube, 2004; Misra et al., 2015).
24	60	We created a large corpus consisting of 109,074 posts on the topics gay marriage (GM, 22425 posts), gun control (GC, 38102 posts), death penalty (DP, 5283 posts) and evolution (EV, 43624), by combining the Internet Argument Corpus (IAC) (Walker et al., 2012), with dialogues from http://www.createdebate.com/.
25	129	Our aim is to develop a method that can extract high quality arguments from a large corpus of argumentative dialogues, in a topic and domain- independent way.
34	86	The Discourse Relation hypothesis suggests that the Arg1 and Arg2 of explicit SPECIFICATION, CONTRAST, CONCESSION and CONTINGENCY markers are more likely to contain good argumentative segments (Prasad et al., 2008).
41	22	We began by extracting the Arg2â€™s for the connectives most strongly associated with these discourse relations over the whole corpus, and then once we saw what the most frequent connectives were in our corpus, we refined this selection to include only but, if, so, and first.
51	53	For the topics gun control and gay marriage, we filtered sentences less than 4 words long, which removed about 8-9% of the sentences.
52	26	After collecting the argument quality annotations for these two topics and examining the distribution of scores (see Sec.
56	59	We determined this threshold by examining the values in gun control and gay marriage, such that at least 2/3 of the filtered sentences were in the bottom third of the argument quality score.
57	51	The PMI filter eliminates 39% of the sentences from death penalty (40% combined with the length filter) and 85% of the sentences from evolution (87% combined with the length filter).
59	24	Overall our experiments are based on 5,374 sampled sentences, with roughly equal numbers over each topic, and equal numbers representing each of our hypotheses and their interactions.
64	26	Finally, there are cases where the user is not making an argument or the argument cannot be reconstructed without significantly more context, e.g. S21 in Table 8.
73	48	We found that annotators could not distinguish between phrases that did not express an argument and hard sentences.
81	29	We can now briefly validate some of the IMPLICIT MARKUP hypothesis using an ANOVA testing the effect of a connective and its position in post on argument quality.
87	26	In addition to the presence of a connective, the dialogue structural position of being an initial sentence in a response post did not predict argument quality as we expected.
140	25	Meta Features: The 3 meta feature sets are: (1) all features except lexical n-grams (!LNG); (2) all features that use specific lexical or categorical information (SPFC); and (3) aggregate statistics (AGG) obtained from our feature extraction process.
148	83	Table 3 shows the 10 features most correlated with the annotated quality value in the training data for the topics gun control and gay marriage.
150	102	Sentence length has the highest correlation with the target value in both topics, as does the node:root feature, inversely correlated with length.
151	43	Therefore, in order to shift the quality distribution of the sample that we put out on MTurk for the death penalty or evolution topics, we applied a filter that removed all sentences shorter than 4 words.
152	36	For these topics, domain specific features such as lexical n-grams are better predictors of argument quality.
154	64	We first tested the performance of 3 regression algorithms using the training and testing data within each topic using 3 standard evaluation measures: R2, Root Mean Squared Error (RMSE) and Root Relative Squared Error (RRSE).
160	39	Table 4 shows that SVMs and OK perform the best, with better than baseline results for all topics.
170	61	Since the length and domain specific words are important features in the trained models, it seems likely that the filtering process made it harder to learn a good function.
183	25	To investigate whether learned models generalize across domains we also evaluate the performance of training with data from one domain and testing on another.
185	35	Although cross domain training does not perform as well as in-domain training, we are able to achieve much better than baseline results between gun control and gay marriage for many of the feature sets and some other minor transferability for the other domains.
192	29	These results show that sometimes the best quality predictors can be trained in a domain-independent way.
