1	76	In contrast to more rigid Statistical Machine Translation (SMT) architectures (Koehn et al., 2003), NMT models are trained end-to-end, exploit continuous representations that mitigate the sparsity problem, and overcome the locality problem by making use of unconstrained contexts.
7	13	In this paper, we explore whether the rigid and modular nature of SMT is more suitable for these unsupervised settings, and propose a novel unsupervised SMT system that can be trained on monolingual corpora alone.
23	18	The phrase table is a collection of source language n-grams and a list of their possible translations in the target language along with different scores for each of them.
31	16	The word and phrase penalties assign a fixed score to every generated word and phrase, and are useful to control the length of the output text and the preference for shorter or longer phrases.
32	18	Having trained all these different models, a tuning process is applied to optimize their weights in the resulting log-linear model, which typically maximizes some evaluation metric in a separate validation parallel corpus.
34	20	Section 3.1 presents our proposed extension of skip-gram to learn n-gram embeddings, while Section 3.2 describes how we map them to a shared space to obtain cross-lingual n-gram embeddings.
35	42	Negative sampling skip-gram takes word-context pairs (w, c), and uses logistic regression to predict whether the pair comes from the true distribution as sampled from the training corpus, or it is one of the k draws from a noise distribution (Mikolov et al., 2013b): log σ (w · c) + k∑ i=1 EcN∼PD [log σ (−w · cN )] In its basic formulation, both w and c correspond to words that co-occur within a certain window in the training corpus.
39	19	One option would be to merge all n-grams regardless of their score, but this is not straightforward given their overlapping nature, which is further accentuated when considering n-grams of different lengths.
42	32	As an alternative approach, we propose a generalization of skip-gram that learns n-gram embeddings on-the-fly, and has the desirable property of unigram invariance: our proposed model learns the exact same embeddings as the original skipgram for unigrams, while simultaneously learning additional embeddings for longer n-grams.
45	34	In order to enforce unigram invariance, the context c and negative samples cN , which always correspond to unigrams, are not updated for (p, c).
49	60	Cross-lingual mapping methods take independently trained word embeddings in two languages, and learn a linear transformation to map them to a shared cross-lingual space (Mikolov et al., 2013a; Artetxe et al., 2018a).
51	49	In our case, we use the method of Artetxe et al. (2018b) to map the n-gram embeddings to a shared cross-lingual space using their open source implementation VecMap1.
55	27	As discussed in Section 2, phrase-based SMT follows a modular architecture that combines several scoring functions through a log-linear model.
57	40	From the remaining models, typically trained on parallel corpora, we decide to leave the lexical reordering out, as the distortion model already accounts for word reordering.
63	41	So as to keep the size of the phrase table within a reasonable limit, we train cross-lingual phrase embeddings as described in Section 3, and limit the translation candidates for each source phrase to its 100 nearest neighbors in the target language.
64	13	In order to estimate their corresponding phrase translation probabilities, we apply the softmax function over the cosine similarities of their respective embeddings.
68	31	In order to compute the lexical weightings, we align each word in the target phrase with the one in the source phrase most likely generating it, and take the product of their respective translation probabilities: lex(f̄ |ē) = ∏ i max ( ,max j w(f̄i|ēj) ) The constant guarantees that each target language word will get a minimum probability mass, which is useful to model NULL alignments.
71	22	As discussed in Section 2, standard SMT uses MERT over a small parallel corpus to tune the weights of the different scoring functions combined through its log-linear model.
74	20	The procedure described in Section 4 suffices to train an SMT system from monolingual corpora which, as shown by our experiments in Section 6, already outperforms previous unsupervised systems.
76	22	Algorithm 2 Iterative refinement Input: cs (source language corpus) Input: ct (target language corpus) Input/Output: mt→s (target-to-source models) Input/Output: wt→s (target-to-source weights) Output: ms→t (source-to-target models) Output: ws→t (source-to-target weights) 1: trains, vals ← SPLIT(cs) 2: traint, valt ← SPLIT(ct) 3: repeat 4: btts ← TRANSLATE(mt→s,wt→s, traint) 5: btvs ← TRANSLATE(mt→s, wt→s, valt) 6: ms→t ← TRAIN(btts, traint) 7: ws→t ← MERT(ms→t, btvs, valt) 8: bttt ← TRANSLATE(ms→t,ws→t, trains) 9: btvt ← TRANSLATE(ms→t, ws→t, vals) 10: mt→s ← TRAIN(bttt, trains) 11: wt→s ← MERT(mt→s, btvt, vals) 12: until convergence In order to overcome these limitations, we propose an iterative refinement procedure based on backtranslation (Sennrich et al., 2016).
77	55	More concretely, we generate a synthetic parallel corpus by translating the monolingual corpus in one of the languages with the initial system, and train and tune a standard SMT system over it in the opposite direction.
78	14	Note that this new system does not have any of the initial restrictions: the phrase table is built and scored using standard word alignment with an unconstrained vocabulary, and a lexical reordering model is also learned.
79	14	Having done that, we use the resulting system to translate the monolingual corpus in the other language, and train another SMT system over it in the other direction.
86	13	As discussed throughout the paper, our system is trained on monolingual corpora alone, so we take the concatenation of all News Crawl monolingual corpora from 2007 to 2013 as our training data, which we tokenize and truecase using standard Moses tools.
89	13	In addition to that, we also report results in German-English newstest2016 (from WMT 2016), as this was used by some previous work in unsupervised NMT (Lample et al., 2018; Yang et al., 2018)5.
111	18	Note that both systems use the exact same language model trained on News Crawl, making them fully comparable in terms of the monolingual corpora they have access to.
113	163	As such, we think that our results are very encouraging, as they show that our fully unsupervised system is already quite close to this competitive baseline.
141	106	Our experiments on standard WMT FrenchEnglish and German-English datasets confirm the effectiveness of our proposal, where we obtain improvements above 10 and 7 BLEU points over previous NMT-based approaches, respectively, closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 points.
142	97	In the future, we would like to extend our approach to semi-supervised scenarios with small parallel corpora, which we expect to be particularly helpful for tuning purposes.
143	59	Moreover, we would like to try a hybrid approach with NMT, using our unsupervised SMT system to generate a synthetic parallel corpus and training an NMT system over it through iterative backtranslation.
