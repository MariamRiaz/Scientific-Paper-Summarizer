1	32	However, deep neural network (DNN) solutions remain poorly understood, leaving many to think of these models as black boxes, and to question whether they can be understood at all (Bornstein, 2016; Lipton, 2016).
5	21	Altogether, this line of research developed a set of promising tools for understanding DNNs, each paper producing a glimmer of insight.
6	37	Here, we propose another tool for the kit, leveraging methods inspired not by neuroscience, but instead by psychology.
7	22	Cognitive psychologists have long wrestled with the problem of understanding another opaque intelligent system: the human mind.
8	46	We contend that the search for a better understanding of DNNs may profit from the rich heritage of problem descriptions, theories, and experimental tools developed in cognitive psychology.
39	21	Psychologists have developed a repertoire of such hypotheses and experiments in their effort to understand the human mind.
40	35	Here we explore the application of one of these theory-experiment pairs to state of the art one-shot learning models.
42	34	Discussions of one-shot word learning in the psychological literature inevitably begin with the philosopher W.V.O.
47	29	Quine points out that the linguist is faced with an abundance of possible inferences, including that “gavagai” refers to rabbits, animals, white things, that specific rabbit, or “undetached parts of rabbits”.
49	23	Contrary to Quine’s intentions, when this example was introduced to the developmental psychology community by Macnamara (1972), it spurred them not to give up on the idea of internal meaning, but instead to posit and test for cognitive biases that enable children to eliminate broad swaths of the hypothesis space (Bloom, 2000).
50	51	A variety of hypothesis-eliminating biases were then proposed including the whole object bias, by which children assume that a word refers to an entire object and not its components (Markman, 1990); the taxonomic bias, by which children assume a word refers to the basic level category an object belongs to (Markman & Hutchinson, 1984); the mutual exclusivity bias, by which children assume that a word only refers to one object category (Markman & Wachtel, 1988); the shape bias, with which we are concerned here (Landau et al., 1988); and a variety of others (Bloom, 2000).
51	60	These biases were tested empirically in experiments wherein children or adults were given an object (or picture of an object) along with a novel name, then were asked whether the name should apply to various other objects.
52	38	Taken as a whole, this work yielded a computational level (Marr, 1982) account of word learning whereby people make use of biases to eliminate unlikely hypotheses when inferring the meaning of new words.
70	39	The function h is parameterised by Inception – one of the best performing ImageNet classification models (Szegedy et al., 2015a).
71	39	Specifically, h returns features from the last layer (the softmax input) of a pre-trained Inception classifier, where the Inception classifier is trained using rms-prop, as described in Szegedy et al. (2015b), section 8.
72	26	With these features as input and cosine distance as the distance function, the classifier in equation 2 achieves 87.6% accuracy on one-shot classification on the ImageNet dataset (Vinyals et al., 2016).
76	32	MNs are trained to assign label ŷ to probe image x̂ according to equation 1 using an attention mechanism a acting on image embeddings stored in the support set S: a(x̂, xi) = ed(f(x̂,S),g(xi,S))∑ j e d(f(x̂,S),g(xj ,S)) , (3) where d is a cosine distance and where f and g provide context-dependent embeddings of x̂ and xi (with context S).
83	24	Parameters are updated using stochastic gradient descent with a learning rate of 0.1; (3) After each update, the labels {yi : i ∈ [1, k]} in the training set are randomly re-assigned to new image classes (the label indices are randomly permuted, but the image labels are not changed).
91	22	The images are photographs of stimuli used previously in shape bias experiments conducted in the Cognitive Development Lab at Indiana University.
105	148	First, we measured the shape bias in IB: we used a pretrained Inception classifier (with 94% top-5 accuracy) to provide features for our nearest-neighbour one-shot classifier, and probed the model using the CogPsyc dataset.
109	20	To estimate the shape bias Bs, we calculated the proportion of shape labels assigned to the probe: Bs = E(δ(ŷ − ys)), (5) where E is an expectation across probe images and δ is the Dirac delta function.
131	32	For the CogPsyc dataset, the average shape bias was Bs = 0.628 with standard deviation σBs = 0.049 at the end of training and for the real-world dataset the average shape bias was Bs = 0.958 with σBs = 0.037.
153	36	The second surprising finding was the large variability in shape bias, both within models during training and across models, depending on the randomly chosen initialisation of our model.
155	84	This is an important result because it shows that not all models are created equally - some models will have a stronger preference for shape than others, even though they are architecturally identical and have almost identical classification accuracy.
156	37	Our third finding – that MNs retain the shape bias statistics of the downstream Inception network – demonstrates the possibility for biases to propagate across model components.
167	51	In our case, the DNN mechanism whereby model parameters and input images interact to give rise to a shape bias have not been elucidated, nor did we expect this to happen.
169	66	For example, in future work it would be interesting to use gradient-based visualization or neuron ablation techniques to augment the current results by identifying the mechanisms underlying the shape bias.
174	33	Another feature of our results supports this contention: in our model the shape bias increases dramatically early in training (Fig.
175	27	2a); similarly, humans show the shape bias much more strongly as adults than as children, and older children show the bias more strongly than younger children (Landau et al., 1988).
180	83	Probing a model in this way is much faster than running human behavioural experiments, so a wider range of hypotheses for human word learning may be rapidly tested.
