{"sections": [{"text": "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 26\u201331 Vancouver, Canada, July 30 - August 4, 2017. c\u00a92017 Association for Computational Linguistics\nhttps://doi.org/10.18653/v1/P17-2005"}, {"heading": "1 Introduction", "text": "The task of extractive summarization (ES) can naturally be cast as a discrete optimization problem where the text source is considered as a set of sentences and the summary is created by selecting an optimal subset of the sentences under a length constraint (McDonald, 2007; Lin and Bilmes, 2011).\nIn this work, we go one step further and mathematically prove that ES is equivalent to the problem of choosing (i) an objective function \u03b8 for scoring system summaries, and (ii) an optimizer O. We use (\u03b8, O) to denote the resulting decomposition of any extractive summarizer. Our proposed decomposition enables a principled analysis and evaluation of existing summarizers, and addresses a major issue in the current evaluation of ES.\nThis issue concerns the traditional \u201cintrinsic\u201d evaluation comparing system summaries against human reference summaries. This kind of evaluation is actually an end-to-end evaluation of summarization systems which is performed after \u03b8 has been optimized by O. This is highly problematic\nfrom an evaluation point of view, because first, \u03b8 is typically not optimized exactly, and second, there might be side-effects caused by the particular optimization technique O, e.g., a sentence extracted to maximize \u03b8 might be suitable because of other properties not included in \u03b8. Moreover, the commonly used evaluation metric ROUGE yields a noisy surrogate evaluation (despite its good correlation with human judgments) compared to the much more meaningful evaluation based on human judgments. As a result, the current end-toend evaluation does not provide any insights into the task of automatic summarization.\nThe (\u03b8,O) decomposition we propose addresses this issue: it enables a well-defined and principled evaluation of extractive summarizers on the level of their components \u03b8 and O. In this work, we focus on the analysis and evaluation of \u03b8, because \u03b8 is a model of the quality indicators of a summary, and thus crucial in order to understand the properties of \u201cgood\u201d summaries. Specifically, we compare \u03b8 functions of different summarizers by measuring the correlation of their \u03b8 functions with human judgments.\nOur goal is to provide an evaluation framework which the research community could build upon in future research to identify the best possible \u03b8 and use it in optimization-based systems. We believe that the identification of such a \u03b8 is the central question of summarization, because this optimal \u03b8 would represent an optimal definition of summary quality both from an algorithmic point of view and from the human perspective.\nIn summary, our contribution is twofold: (i) We present a novel and principled evaluation framework for ES which allows evaluating the objective function and the optimization technique separately and independently. (ii) We compare wellknown summarization systems regarding their implicit choices of \u03b8 by measuring the correlation\n26\nof their \u03b8 functions with human judgments on two datasets from the Text Analysis Conference (TAC). Our comparative evaluation yields surprising results and shows that extractive summarization is not solved yet.\nThe code used in our experiments, including a general evaluation tool is available at github.com/UKPLab/acl2017-theta_ evaluation_summarization."}, {"heading": "2 Evaluation Framework", "text": ""}, {"heading": "2.1 (\u03b8,O) decomposition", "text": "Let D = {si} be a document collection considered as a set of sentences. A summary S is then a subset of D, or we can say that S is an element of P(D), the power set of D. Objective function We define an objective function to be a function that takes a summary of the document collection D and outputs a score:\n\u03b8 : P(D) \u2192 R S 7\u2192 \u03b8D(S) (1)\nOptimizer Then the task of ES is to select the set of sentences S\u2217 with maximal \u03b8(S\u2217) under a length constraint:\nS\u2217 = argmax S \u03b8(S)\nlen(S) = \u2211\ns\u2208S len(s) \u2264 c (2)\nWe use O to denote the technique which solves this optimization problem. O is an operator which takes an objective function \u03b8 from the set of all objective functions \u0398 and a document collection D from the set of all document collections D, and outputs a summary S\u2217:\nO : \u0398\u00d7D \u2192 S (\u03b8,D) 7\u2192 S\u2217 (3)\nDecomposition Theorem Now we show that the problem of ES is equivalent to the problem of choosing a decomposition (\u03b8, O).\nWe formalize an extractive summarizer \u03c3 as a set function which takes a document collection D \u2208 D and outputs a summary SD,\u03c3 \u2208 P(D). With this formalism, it is clear that every (\u03b8,O) tuple forms a summarizer because O(\u03b8, \u00b7) produces a summary from a document collection.\nBut the other direction is also true: for every extractive summarizer there exists at least one tuple (\u03b8, O) which perfectly describes the summarizer:\nTheorem 1 \u2200\u03c3, \u2203(\u03b8,O) such that: \u2200D \u2208 D, \u03c3(D) = O(\u03b8,D)\nThis theorem is quite intuitive, especially since it is common to use a similar decomposition in optimization-based summarization systems. In the next section we illustrate the theorem by way of several examples, and provide a rigorous proof of the existence in the supplemental material."}, {"heading": "2.2 Examples of \u03b8", "text": "We analyze a range of different summarizers regarding their (mostly implicit) \u03b8. ICSI (Gillick and Favre, 2009) is a global linear optimization that extracts a summary by solving a maximum coverage problem considering the most frequent bigrams in the source documents. ICSI has been among the best systems in a classical ROUGE evaluation (Hong et al., 2014). For ICSI, the identification of \u03b8 is trivial because it was formulated as an optimization task. If ci is the i-th bigram selected in the summary and wi its weight computed from D, then:\n\u03b8ICSI(S) = \u2211\nci\u2208S ci \u2217 wi (4)\nLexRank (Erkan and Radev, 2004) is a wellknown graph-based approach. A similarity graph G(V,E) is constructed where V is the set of sentences and an edge eij is drawn between sentences vi and vj if and only if the cosine similarity between them is above a given threshold. Sentences are scored according to their PageRank score inG. We observe that \u03b8LexRank is given by:\n\u03b8LexRank(S) = \u2211\ns\u2208S PRG(s) (5)\nwhere PR is the PageRank score of sentence s. KL-Greedy (Haghighi and Vanderwende, 2009) minimizes the Kullback Leibler (KL) divergence between the word distributions in the summary and D (i.e \u03b8KL = \u2212KL). Recently, Peyrard and Eckle-Kohler (2016) optimized KL and Jensen Shannon (JS) divergence with a genetic algorithm. In this work, we use KL and JS for both unigram and bigram distributions. LSA (Steinberger and Jezek, 2004) is an approach involving a dimensionality reduction of the termdocument matrix via Singular Value Decomposition (SVD). The sentences extracted should cover the most important latent topics:\n\u03b8LSA = \u2211\nt\u2208S \u03bbt (6)\nwhere t is a latent topic identified by SVD on the term-document matrix and \u03bbt the associated singular value. Edmundson (Edmundson, 1969) is an older heuristic method which scores sentences according to cue-phrases, overlap with title, term frequency and sentence position. \u03b8Edmundson is simply a weighted sum of these heuristics. TF?IDF (Luhn, 1958) scores sentences with the TF*IDF of their terms. The best sentences are then greedily extracted. We use both the unigram and bigram versions in our experiments."}, {"heading": "3 Experiments", "text": "Now we compare the summarizers analyzed above by measuring the correlation of their \u03b8 functions with human judgments.\nDatasets We use two multi-document summarization datasets from the Text Analysis Conference (TAC) shared task: TAC-2008 and TAC2009.1 TAC-2008 and TAC-2009 contain 48 and 44 topics, respectively. Each topic consists of 10 news articles to be summarized in a maximum of 100 words. We use only the so-called initial summaries (A summaries), but not the update part.\nFor each topic, there are 4 human reference summaries along with a manually created Pyramid set. In both editions, all system summaries and the 4 reference summaries were manually evaluated by NIST assessors for readability, content selection (with Pyramid) and overall responsiveness. At the time of the shared tasks, 57 systems were submitted to TAC-2008 and 55 to TAC-2009. For our experiments, we use the Pyramid and the responsiveness annotations.\nSystem Comparison For each \u03b8, we compute the scores of all system and all manual summaries for any given topic. These scores are compared with the human scores. We include the manual summaries in our computation because this yields a more diverse set of summaries with a wider range of scores. Since an ideal summarizer would create summaries as well as humans, an ideal \u03b8 would also be able to correctly score human summaries with high scores.\nFor comparison, we also report the correlation between pyramid and responsiveness.\nCorrelations are measured with 3 metrics: Pear1http://tac.nist.gov/2009/ Summarization/, http://tac.nist.gov/2008/ Summarization/\nson\u2019s r, Spearman\u2019s \u03c1 and Normalized Discounted Cumulative Gain (Ndcg). Pearson\u2019s r is a value correlation metric which depicts linear relationships between the scores produced by \u03b8 and the human judgments. Spearman\u2019s \u03c1 is a rank correlation metric which compares the ordering of systems induced by \u03b8 and the ordering of systems induced by human judgments. Ndcg is a metric that compares ranked lists and puts more emphasis on the top elements by logarithmic decay weighting. Intuitively, it captures how well \u03b8 can recognize the best summaries. The optimization scenario benefits from high Ndcg scores because only summaries with high \u03b8 scores are extracted.\nPrevious work on correlation analysis averaged scores over topics for each system and then computed the correlation between averaged scores (Louis and Nenkova, 2013; Nenkova et al., 2007). An alternative and more natural option which we use here is to compute the correlation for each topic and average these correlations over topics (CORRELATION-AVERAGE). Since we want to estimate how well \u03b8 functions measure the quality of summaries, we find the summary level averaging more meaningful.\nAnalysis The results of our correlation analysis are presented in Table 1.\nIn our (\u03b8,O) formulation, the end-to-end approach maps a set of documents to exactly one summary selected by the system. We call the (classical and well known) evaluation of this single summary end-to-end evaluation because it measures the end product of the system. This is in contrast to our proposed evaluation of the assumption made by individual summarizers shown in Table 1. A system summary was extracted by a given system because it was high scoring using its \u03b8, but we ask the question whether optimizing this \u03b8 made sense in the first place.\nWe first observe that scores are relatively low. Summarization is not a solved problem and the systems we investigated can not identify correctly what makes a good summary. This is in contrast to the picture in the classical end-to-end evaluation with ROUGE where state-of-the-art systems score relatively high. Some Ndcg scores are higher (for TAC-2008) which explains why these systems can extract relatively good summaries in the end-toend evaluation. In this classical evaluation, only the single best summary is evaluated, which means that a system does not need to be able to rank all\npossible summaries correctly. We see that systems with high end-to-end ROUGE scores (according to Hong et al. (2014)) do not necessarily have a good model of summary quality. Indeed, the best performing \u03b8 functions are not part of the systems performing best with ROUGE. For example, ICSI is the best system according to ROUGE, but it is not clear that it has the best model of summary quality. In TAC-2009, LexRank, LSA and the heuristic Edmundson have better correlations with human judgments. The difference with end-to-end evaluation might stem from the fact that ICSI solves the optimization problem exactly, while LexRank and Edmundson use greedy optimizers. There might also be some side-effects from which ICSI profits: extracting sentences to improve \u03b8 might lead to accidentally selecting suitable sentences, because \u03b8 can merely correlate well with properties of good summaries, while not modeling these properties itself.\nIt is worth noting that systems perform differently on TAC2009 and TAC2008. There are several differences between TAC2008 and TAC2009 like redundancy level or guidelines for annotations; for example, responsiveness is scored out of 5 in 2008 and out of 10 in 2009. The LSA summarizer ranks among the best systems in TAC2009 with pearson\u2019s r but is closer to the worst systems in TAC2008. While this is difficult to explain we hypothesize that the model of summary quality from LSA is sensitive to the slight variations and therefore not robust. In general, any system which claims to have a better \u03b8 than previous works should indeed report results on several datasets to ensure robustness and generality.\nInterestingly, we observe that the correlation between Pyramid and responsiveness is better than in\nany system, but still not particularly high. Responsiveness is an overall annotation while Pyramid is a manual measure of content only. These results confirm the intuition that humans take into account much more aspects when evaluating summaries."}, {"heading": "4 Related Work and Discussion", "text": "While correlation analyses on human judgment data have been performed in the context of validating automatic summary evaluation metrics (Louis and Nenkova, 2013; Nenkova et al., 2007; Lin, 2004), there is no prior work which uses these data for a principled comparison of summarizers.\nMuch previous work focused on efficient optimizers O, such as ILP, which impose constraints on the \u03b8 function. Linear (Gillick and Favre, 2009) and submodular (Lin and Bilmes, 2011) \u03b8 functions are widespread in the summarization community because they can be optimized efficiently and effectively via ILP (Schrijver, 1986) and the greedy algorithm for submodularity (Fujishige, 2005). A greedy approach is often used when \u03b8 does not have convenient properties that can be leveraged by a classical optimizer (Haghighi and Vanderwende, 2009).\nSuch interdependencies of O and \u03b8 limit the expressiveness of \u03b8. However, realistic \u03b8 functions are unlikely to be linear or submodular, and in the well-studied field of optimization there exist a range of different techniques developed to tackle difficult combinatorial problems (Schrijver, 2003; Blum and Roli, 2003).\nA recent example of such a technique adapted to extractive summarization are meta-heuristics used to optimize non-linear, non-submodular objective functions (Peyrard and Eckle-Kohler, 2016).\nOther methods like Markov Chain Monte Carlo (Metropolis et al., 1953) or Monte-Carlo Tree Search (Suttner and Ertel, 1991; Silver et al., 2016) could also be adapted to summarization and thus become realistic choices for O. General purpose optimization techniques are especially appealing, because they offer a decoupling of \u03b8 and O and allow investigating complex \u03b8 functions without making any assumption on their mathematical properties. In particular, this supports future work on identifying an \u201coptimal\u201d \u03b8 as a model of relevant quality aspects of a summary."}, {"heading": "5 Conclusion", "text": "We presented a novel evaluation framework for ES which is based on the proof that ES is equivalent to the problem of choosing an objective function \u03b8 and an optimizer O. This principled and welldefined framework allows evaluating \u03b8 and O of any extractive summarizer \u2013 separately and independently. We believe that our framework can serve as a basis for future work on identifying an \u201coptimal\u201d \u03b8 function, which would provide an answer to the central question of what are the properties of a \u201cgood\u201d summary."}, {"heading": "Acknowledgments", "text": "This work has been supported by the German Research Foundation (DFG) as part of the Research Training Group \u201cAdaptive Preparation of Information from Heterogeneous Sources\u201d (AIPHES) under grant No. GRK 1994/1, and via the GermanIsraeli Project Cooperation (DIP, grant No. GU 798/17-1)."}], "year": 2017, "references": [{"title": "Metaheuristics in Combinatorial Optimization: Overview and Conceptual Comparison", "authors": ["Christian Blum", "Andrea Roli."], "venue": "ACM Computing Surveys 35(3):268\u2013308.", "year": 2003}, {"title": "New Methods in Automatic Extracting", "authors": ["H.P. Edmundson."], "venue": "Journal of the Association for Computing Machinery 16(2):264\u2013285.", "year": 1969}, {"title": "LexRank: Graph-based Lexical Centrality As Salience in Text Summarization", "authors": ["G\u00fcnes Erkan", "Dragomir R. Radev."], "venue": "Journal of Artificial Intelligence Research pages 457\u2013479.", "year": 2004}, {"title": "Submodular functions and optimization", "authors": ["Satoru Fujishige."], "venue": "Annals of discrete mathematics. Elsevier, Amsterdam, Boston, Paris.", "year": 2005}, {"title": "A Scalable Global Model for Summarization", "authors": ["Dan Gillick", "Benoit Favre."], "venue": "Proceedings of the Workshop on Integer Linear Programming for Natural Language Processing. Association for Computational Linguistics, Boulder, Colorado, pages 10\u201318.", "year": 2009}, {"title": "Exploring Content Models for Multi-document Summarization", "authors": ["Aria Haghighi", "Lucy Vanderwende."], "venue": "Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Compu-", "year": 2009}, {"title": "A Repository of State of the Art and Competitive Baseline Summaries for Generic News Summarization", "authors": ["Kai Hong", "John Conroy", "benoit Favre", "Alex Kulesza", "Hui Lin", "Ani Nenkova"], "venue": "In Proceedings of the Ninth International Con-", "year": 2014}, {"title": "ROUGE: A Package for Automatic Evaluation of Summaries", "authors": ["Chin-Yew Lin."], "venue": "Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Computational Linguistics, Barcelona, Spain, pages 74\u201381.", "year": 2004}, {"title": "A Class of Submodular Functions for Document Summarization", "authors": ["Hui Lin", "Jeff A. Bilmes."], "venue": "Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies. Association for Computa-", "year": 2011}, {"title": "Automatically Assessing Machine Summary Content Without a Gold Standard", "authors": ["Annie Louis", "Ani Nenkova."], "venue": "Computational Linguistics 39(2):267\u2013300.", "year": 2013}, {"title": "The Automatic Creation of Literature Abstracts", "authors": ["Hans Peter Luhn."], "venue": "IBM Journal of Research Development 2:159\u2013165.", "year": 1958}, {"title": "A Study of Global Inference Algorithms in Multi-document Summarization", "authors": ["Ryan McDonald."], "venue": "Proceedings of the 29th European Conference on IR Research. Springer-Verlag, Rome, Italy, pages 557\u2013 564.", "year": 2007}, {"title": "Equation of State Calculations by Fast Computing Machines", "authors": ["Nicholas Metropolis", "Arianna Rosenbluth", "Marshall Rosenbluth", "Augusta Teller", "Edward Teller."], "venue": "Journal of Chemical Physics 21:1087 \u2013 1092.", "year": 1953}, {"title": "The Pyramid Method: Incorporating Human Content Selection Variation in Summarization Evaluation", "authors": ["Ani Nenkova", "Rebecca Passonneau", "Kathleen McKeown."], "venue": "ACM Transactions on Speech and Language Processing (TSLP) 4(2).", "year": 2007}, {"title": "A General Optimization Framework for MultiDocument Summarization Using Genetic Algorithms and Swarm Intelligence", "authors": ["Maxime Peyrard", "Judith Eckle-Kohler."], "venue": "Proceedings of the 26th International Conference on Computational", "year": 2016}, {"title": "Theory of Linear and Integer Programming", "authors": ["Alexander Schrijver."], "venue": "John Wiley & Sons, Inc., New York, NY, USA.", "year": 1986}, {"title": "Combinatorial Optimization - Polyhedra and Efficiency", "authors": ["Alexander Schrijver."], "venue": "Springer, New York.", "year": 2003}, {"title": "Mastering the game of Go with deep neural networks and tree search", "authors": ["Sutskever", "Timothy Lillicrap", "Madeleine Leach", "Koray Kavukcuoglu", "Thore Graepel", "Demis Hassabis."], "venue": "Nature 529(7587):484\u2013489.", "year": 2016}, {"title": "Using latent semantic analysis in text summarization and summary evaluation", "authors": ["Josef Steinberger", "Karel Jezek."], "venue": "Proceedings of the 7th International Conference on Information Systems Implementation and Modelling (ISIM \u201904). Ro\u017enov pod", "year": 2004}, {"title": "Using Back-Propagation Networks for Guiding the Search of a Theorem Prover", "authors": ["Christian Suttner", "Wolfgang Ertel."], "venue": "International Journal of Neural Networks Research & Applications 2(1):3\u201316. 31", "year": 1991}], "id": "SP:81bdee6f6599ead99da4d46226cd50bcf12e0844", "authors": [{"name": "Maxime Peyrard", "affiliations": []}, {"name": "Judith Eckle-Kohler", "affiliations": []}], "abstractText": "We present a new framework for evaluating extractive summarizers, which is based on a principled representation as optimization problem. We prove that every extractive summarizer can be decomposed into an objective function and an optimization technique. We perform a comparative analysis and evaluation of several objective functions embedded in wellknown summarizers regarding their correlation with human judgments. Our comparison of these correlations across two datasets yields surprising insights into the role and performance of objective functions in the different summarizers.", "title": "A Principled Framework for Evaluating Summarizers: Comparing Models of Summary Quality against Human Judgments"}