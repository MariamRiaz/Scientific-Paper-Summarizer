0	16	Deep neural networks have shown tremendous success in recent years, achieving near-human performances on tasks such as visual recognition (Krizhevsky et al., 2012; Szegedy et al., 2015; He et al., 2016).
1	12	One of the key factors in this success of deep network is its expressive power, which is made possible by multiple layers of non-linear transformations.
3	71	Due to large number of parameters, deep networks require large amount of memory and computation power to train.
7	42	How can we then obtain a compact deep network without sacrificing the prediction accuracy?
12	15	Our idea is to enforce network weights at each layer to fit to different sets of input features as much as possible.
13	20	This exclusive feature learning is implemented by the exclusive sparsity regularization based on (1, 2)-norm (Zhou et al., 2010; Kong et al., 2014), which basically promotes network weights at each layer to compete for few meaningful features from the lower layer.
15	17	For example, if the lowerlayer feature is a wheel, and the upper layer weights are features describing car and bicycle respectively, then the two upper layer weights should share the common feature that describes the wheel.
16	73	Thus, we also allow for sharing of some important features, by introducing an additional group sparsity regularizer based on (2, 1)-norm and combine the two regularization terms, balancing their effect at each layer of the network to adjust the degree of feature sharing and competition.
52	15	The element-wise sparsity can be helpful when most of the features are irrelevant to the learning objective, as in the data-driven approaches.
72	16	Figure 2(c) illustrates the feature groups and effect of exclusive sparsity on the convolutional filters.
81	53	Our combined group and exclusive lasso regularizer is given as follows: Ω(W (l)) = ∑ g ( (1− µl)‖W (l)g ‖2 + µl 2 ‖W (l)g ‖21 ) (5) where λ is the parameter that decides the entire regularization effect, W l is the weight matrix for lth layer, and µl is the parameter for balancing the sharing and competition term at each layer.
82	13	Then how should we set the balancing term µl at each layer?
88	12	The proximal gradient algorithm for regularized objective first obtains the intermediate solution Wt+ 12 by taking a gradient step using the gradient computed on the loss only, and then optimize for the regularization term while performing Euclidean projection of it to the solution space, as in the following formulation: min Wt+1 Ω(Wt+1) + 1 2λs ‖Wt+1 −Wt+ 12 ‖ 2 2 (6) where Wt+1 is the variable to obtain after the current iteration, λ is the regularization parameter, and s is the step size.
102	25	The network regularized with `2,1-norm on the weights, which groups each convolutional filter as a group at convolutional layers.
104	19	The network that uses the same `2,1-regularization as in 3), but with each group defined as the same feature at different filters.
109	36	Datasets and base networks We validate our method on four public datasets for classification, with four different convolutional networks.
126	22	The exclusive sparsity improves the performance over the base `2- regularization model in low-sparsity range which is especially well shown in CIFAR-10 result, but degenerates performance as the sparsity increases.
129	15	Fig 3(a) shows the results on the MNIST dataset, on which our CGES obtains no accuracy reduction, using 36.48% less number of parameters and 14.46% less computation.
132	39	On ImageNet (Table 1), CGES obtains similar or slightly worse performance to the full network while using 60% − 68% of its parameters, while `1 shows noticeable performance degeneration at the same sparsity level.
133	19	Iterative pruning Iterative pruning (Han et al., 2015) is another effective method for obtaining a sparse network while maintaining high accuracy.
134	13	As iterative pruning is orthogonal to our method, we can couple the two methods to obtain even better performance per number of parameters used; specifically, we replace the usual weight decay regularizer used in (Han et al., 2015) with our CGES regularizer.
139	15	This faster convergence agrees with the observations in (Saxe et al., 2014), where networks whose weights are initialized as random orthgonal matrices con- verged faster than networks with random Gaussian initialization.
140	23	Convolutional vs. fully connected layers To see how much effect our combined regularizer has on different types of layers, we experiment applying the model only to the fully connected layer, or convolutional layers, while applying usual `2-regularizer to other layers.
144	24	However, conv layers obtained the best accuracy at low-sparsity range, since strict enforcement of exclusivity hurts the representational power of the features, whereas FC layers obtained improvements even on high-sparsity range; this may be because loss of expressiveness could be compensated by better discriminativity of the features at high level.
153	33	Each row is the softmax parameter for each class.
155	15	The group sparsity regularizer results in the total elimination of certain features that are not shared across multiple classes.
156	27	The exclusive sparsity regularizer, when used on its own, results in disjoint feature selection for each class.
162	17	In this work, we proposed a novel regularizer for generic deep neural networks that effectively utilizes the capacity of the network, by exploiting the sharing and competing relationships among different network weights.
163	21	Specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm on the network weights, along with group sparsity regularization using (2, 1)-norm, such that exclusive sparsity enforces the network weights to use input neurons that are as different as possible from the other weights, while the group sparsity allows for some degree of sharing among them, as it is impossible to make the network weights to fit to completely disjoint set of features.
164	69	We validate our method on some public datasets for both the accuracy and efficiency against other sparsity-inducing regularizers, and the results show that our combined regularizer helps obtain even better performance than the original full network, while significantly reducing the memory and computation requirements.
