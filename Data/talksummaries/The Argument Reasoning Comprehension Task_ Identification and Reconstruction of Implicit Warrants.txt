0	110	Russia has the opposite objectives of the US.
1	41	There is much innovation in 3-d printing and it is sustainable.
3	41	They were never uttered but solely presupposed in arguments made by the participants of online discussions.
4	17	Presuppositions are a fundamental pragmatic instrument of natural language argumentation in which parts of arguments are left unstated.
5	16	This phenomenon is also referred to as common knowledge (Macagno and Walton, 2014, p. 218), enthymemes (Walton, 2007b, p. 12), tacit major premises (Amossy, 2009, p. 319), or implicit warrants (Newman and Marshall, 1991, p. 8).
8	29	Although any incomplete argument can be completed in different ways (Plumer, 2016), it is assumed that certain knowledge is shared between the arguing parties (Macagno and Walton, 2014, p. 180).
9	35	Filling the gap between the claim and premises (aka reasons) of a natural language argument empirically remains an open issue, due to the inherent difficulty of reconstructing the world knowledge and reasoning patterns in arguments.
11	57	In an indirect fashion, implicit warrants correspond to major premises in argumentation schemes; a concept heavily referenced in argumentation theory (Walton, 2012).
12	17	However, mapping schemes to realworld arguments has turned out difficult even for the author himself.
13	75	Our main hypothesis is that, even if there is no limit to the tacit length of the reasoning chain between claims and premises, it is possible to systematically reconstruct a meaningful warrant, depending only on what we take as granted and what needs to be explicit.
14	90	As warrants encode our current presupposed world knowledge and connect the reason with the claim in a given argument, we expect that other warrants can be found which connect the reason with a different claim.
15	34	In the ex- 1930 Title: Is Marijuana a Gateway Drug?
76	20	For example, in a discussion about whether declawing a cat should be illegal, an author takes the following position (which is her claim C): ‘It should be illegal to declaw your cat’.
79	24	Now, the question is how to find the warrant W for a given reason R and claim C. Our key hypothesis in the definition of the argument reasoning comprehension task is the existence of an alternative warrant AW that justifies the use of R as support for the opposite ¬C of the claim C (regardless of the question of how strong this justification is).
84	15	However, if both W and AW are available, they usually capture the core of a reason’s relevance and reveal the implicit presuppositions (examples follow further below).
128	13	Given a number of noisy workers, MACE outputs best estimates, outperforming simple majority votes.
129	45	At least five workers are recommended for a crowdsourcing task, but how reliable is the output really?
131	63	We then considered each group as an independent crowdsourcing experiment and estimated gold labels using MACE for each group, thus yielding two ‘experts from the crowd.’ Having two independent ‘experts’ from the crowd allowed us to compute standard agreement scores.
134	32	Figure 3 shows the Cohen’s κ agreement for stance annotation with respect to the crowd size computed by our method.
137	16	For example, reason span annotation is a harder task; however, the results for six workers are comparable to those for the expert annotations of Habernal and Gurevych (2017).9 Table 1 lists statistics of the entire crowdsourcing process carried out for our dataset, including tasks for which we created data as a by-product.
139	40	For brevity, we omit the debate title and description here.
158	44	To evaluate human upper bounds for the task, we sampled 100 random questions (such as those presented in Section 4.4) from the test set and distributed them among 173 participants of an AMT survey.
160	30	We also asked the participants about their highest completed education (six categories) and the amount of formal training they have in reasoning, logic, or argumentation (no training, some, or extensive).
161	13	In addition, they specified for each question how familiar they were with the topic (3- point scale).
162	114	How Hard is the Task for Humans?
167	17	For all participants, the mean was 79.8%.
174	27	As another baseline, we used a 4-gram Modified Kneser-Ney language model trained on 500M tokens (100k vocabulary) from the C4Corpus (Habernal et al., 2016).
176	28	We computed log-likelihood of the candidate warrants and picked the one with lower score.10 To specifically appoach the given task, we implemented two neural models based on a bidirectional LSTM.
178	23	Our more elaborated version used intra-warrant attention, as shown in Figure 5.
179	99	Both versions were also extended with the debate title and description added as context to the attention layer (w/ context).
