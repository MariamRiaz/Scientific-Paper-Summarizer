11	32	Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park and Glass, 2008; Jansen et al., 2010; Jansen and Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n2) complexity, since they do an exhaustive comparison of the data against itself.
12	23	Our method leverages correlated information from a second modality - the visual domain - to guide the discovery of words and phrases.
13	21	This enables our method to run in O(n) time, and we demonstrate it scalability by discovering acoustic patterns in over 522 hours of audio.
14	54	A sub-field within speech processing that has garnered much attention recently is unsupervised 506 speech pattern discovery.
25	165	In this work, we go further by automatically segmenting and clustering the spoken captions into individual word-like units, as well as the images into object-like categories.
26	17	We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.
27	24	The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.
33	15	The model is trained to map entire image frames and entire spoken captions into a shared embedding space; however, as we will show, the trained network can then be used to localize patterns corresponding to words and phrases within the spectrogram, as well as visual objects within the image by applying it to small sub-regions of the image and spectrogram.
34	18	The model is comprised of two branches, one which takes as input images, and the other which takes as input spectrograms.
37	23	The second branch of our network analyzes speech spectrograms as if they were black and white images.
43	16	Additionally, both the images and spectrograms are mean normalized before training.
51	24	Although we have trained our multimodal network to compute embeddings at the granularity of entire images and entire caption spectrograms, we can easily apply it in a more localized fashion.
57	27	We use a constrained brute force ranking scheme to evaluate all possible groundings (with a restricted granularity) between an image and its caption.
58	17	Specifically, we divide the image into a grid, and extract all of the image crops whose boundaries sit on the grid lines.
61	19	Second, we define minimum and maximum aspect ratios as 2:3 and 3:2 so as not to introduce too much distortion and also to reduce the number of proposal boxes.
66	16	The number of proposal segments will vary depending on the caption length, and typically number in the several thousands.
70	52	A naive approach would be to simply keep the top N groundings from this list, but in practice we ran into two problems with this strategy.
79	19	Figure 1 displays a pictorial example of our grounding procedure.
80	100	Once we have completed the grounding procedure, we are left with a small set of regions of interest in each image and caption spectrogram.
94	17	We performed the grounding and pattern clustering steps on the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.
95	27	For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.
97	19	The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer Table 3: Top 50 clusters with k = 500 sorted by increasing variance.
100	16	A dash (-) indicates a cluster whose majority label is silence.
101	31	- 1059 3480 0.70 0.26 - snow 4331 3480 0.85 0.26 0.45 desert 1936 2896 0.82 0.27 0.67 kitchen 3200 2990 0.88 0.28 0.76 restaurant 1921 2536 0.89 0.29 0.71 mountain 4571 2768 0.86 0.30 0.38 black 4369 2387 0.64 0.30 0.17 skyscraper 843 3205 0.84 0.30 0.84 bridge 1654 2025 0.84 0.30 0.25 tree 5303 3758 0.90 0.30 0.16 castle 1298 2887 0.72 0.31 0.74 bridge 2779 2025 0.81 0.32 0.41 - 2349 2165 0.31 0.33 - ocean 2913 3505 0.87 0.33 0.71 table 3765 2165 0.94 0.33 0.23 windmill 1458 3752 0.71 0.33 0.76 window 1890 2795 0.85 0.34 0.21 river 2643 3204 0.76 0.35 0.62 water 5868 3204 0.90 0.35 0.27 beach 1897 2964 0.79 0.35 0.64 flower 3906 2587 0.92 0.35 0.67 wall 3158 3636 0.84 0.35 0.23 sky 4306 6055 0.76 0.36 0.34 street 2602 2385 0.86 0.36 0.49 golf course 1678 3864 0.44 0.36 0.63 field 3896 3261 0.74 0.36 0.37 tree 4098 3758 0.89 0.36 0.13 lighthouse 1254 1518 0.61 0.36 0.83 forest 1752 3431 0.80 0.37 0.56 church 2503 3140 0.86 0.37 0.72 people 3624 2275 0.91 0.37 0.14 baseball 2777 1929 0.66 0.37 0.86 field 2603 3922 0.74 0.37 0.25 car 3442 2118 0.79 0.38 0.27 people 4074 2286 0.92 0.38 0.17 shower 1271 2206 0.74 0.38 0.82 people walking 918 2224 0.63 0.38 0.25 wooden 3095 2723 0.63 0.38 0.28 mountain 3464 3239 0.88 0.38 0.29 tree 3676 2393 0.89 0.39 0.11 - 1976 3158 0.28 0.39 - snow 2521 3480 0.79 0.39 0.24 water 3102 2948 0.90 0.39 0.14 rock 2897 2967 0.76 0.39 0.26 - 2918 3459 0.08 0.39 - night 3027 3185 0.44 0.39 0.59 station 2063 2083 0.85 0.39 0.62 chair 2589 2288 0.89 0.39 0.22 building 6791 3450 0.89 0.40 0.21 city 2951 3190 0.67 0.40 0.50 Figure 2: Scatter plot of audio cluster purity weighted by log cluster size vs variance for k = 500 (least-squares line superimposed).
107	22	A breakdown of the segments captured by two clusters is shown in Table 2.
128	25	The cluster centroids for “lake,” “river,” “body,” “water,” “waterfall,” “pond,” and “pool” all form a tight meta-cluster, as do “restaurant,” “store,” “shop,” and “shelves,” as well as “children,” “girl,” “woman,” and “man.” Many other semantic meta-clusters can be seen in Figure 4, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.
151	120	The same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.
152	35	Additionally, by collecting a second dataset of captions for our images in a different language, such as Spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.
153	156	This paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations.
154	54	beach cliff pool desert field chair table staircase statue stone church forest mountain skyscraper trees waterfall windmills window city bridge flowers man wall archway baseball boat shelves cockpit girl children building rock kitchen plant hallway
