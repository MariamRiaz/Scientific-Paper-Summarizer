{"sections": [{"heading": "1. Introduction", "text": "Deep linear networks (DLN) are neural networks that have multiple hidden layers but have no nonlinearities between layers. That is, for given data points {x(i)}Ni=1 the outputs of such networks are computed via a series\ny\u0302(i) = WLWL\u22121 \u00b7 \u00b7 \u00b7W1x(i)\nof matrix multiplications. Given a target y(i) for the ith data point and a pairwise loss function `(y\u0302(i),y(i)), forming the usual summation\nL(W1, . . . ,WL) = 1\nN N\u2211 i=1 `(y\u0302(i),y(i)) (1)\nthen yields the total loss.\nSuch networks have few direct applications, but they frequently appear as a class of toy models used to understand the loss surfaces of deep neural networks (Saxe et al., 2014; Kawaguchi, 2016; Lu & Kawaguchi, 2017; Hardt & Ma, 2017). For example, numerical experiments indicate that DLNs exhibit some behavior that resembles the behavior of\n*Equal contribution 1Department of Mathematics, Loyola Marymount University, Los Angeles, CA 90045, USA 2Department of Mathematics and Statistics, California State University, Long Beach, Long Beach, CA 90840, USA. Correspondence to: Thomas Laurent <tlaurent@lmu.edu>, James H. von Brecht <james.vonbrecht@csulb.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\ndeep nonlinear networks during training (Saxe et al., 2014). Results of this sort provide a small piece of evidence that DLNs can provide a decent simplified model of more realistic networks with nonlinearities.\nFrom an analytical point-of-view, the simplicity of DLNs allows for a rigorous, in-depth study of their loss surfaces. These models typically employ a convex loss function `(y\u0302,y), and so with one layer (i.e. L = 1) the loss L(W1) is convex and the resulting optimization problem (1) has no sub-optimal local minimizers. With multiple layers (i.e. L \u2265 2) the loss L(W1, . . . ,WL) is not longer convex, and so the question of paramount interest concerns whether this addition of depth and the subsequent loss of convexity creates sub-optimal local minimizers. Indeed, most analytical treatments of DLNs focus on this question.\nWe resolve this question in full for arbitrary convex differentiable loss functions. Specifically, we consider deep linear networks satisfying the two following hypotheses:\n(i) The loss function y\u0302 7\u2192 `(y, y\u0302) is convex and differentiable.\n(ii) The thinnest layer is either the input layer or the output layer.\nMany networks of interest satisfy both of these hypotheses. The first hypothesis (i) holds for nearly all network criteria, such as the mean squared error loss, the logistic loss or the cross entropy loss, that appear in applications. In a classification scenario, the second hypothesis (ii) holds whenever each hidden layer has more neurons than the number of classes. Thus both hypotheses are often satisfied when using a deep linear network (1) to model its nonlinear counterpart. In any such situation we resolve the deep linear problem in its entirety. Theorem 1. If hypotheses (i) and (ii) hold then (1) has no sub-optimal minimizers, i.e. any local minimum is global.\nWe provide a short, transparent proof of this result. It is easily accessible to any reader with a basic understanding of the singular value decomposition, and in particular, it does not rely on any sophisticated machinery from either optimization or linear algebra. Moreover, this theorem is the strongest possible in the following sense \u2014 Theorem 2. There exists a convex, Lipschitz but not differentiable function y\u0302 7\u2192 `(y, y\u0302) for which (1) has sub-optimal\nlocal minimizers.\nIn other words, we have a (perhaps surprising) hard limit on how far \u201clocal equals global\u201d results can reach; differentiability of the loss is essential.\nMany prior analytical treatments of DLNs focus on similar questions. For instance, both (Baldi & Hornik, 1989) and (Baldi & Lu, 2012) consider \u201cdeep\u201d linear networks with two layers (i.e. L = 2) and a mean squared error loss criterion. They provide a \u201clocal equals global\u201d result under some relatively mild assumptions on the data and targets. More recently, (Kawaguchi, 2016) proved that deep linear networks with arbitrary number of layers and with mean squared error loss do not have sub-optimal local minima under certain structural assumptions on the data and targets. The follow-up (Lu & Kawaguchi, 2017) futher simplifies the proof of this result and weakens the structural assumptions. Specifically, this result shows that the loss (1) associated with a deep linear network has no sub-optimal local minima provided all of assumptions\n(i) The loss `(y\u0302(i),y(i)) = \u2016y(i) \u2212 y\u0302(i)\u20162 is the mean squared error loss criterion;\n(ii) The data matrixX = [x(1), . . . ,x(N)] has full row rank;\n(iii) The target matrix Y = [y(1), . . . ,y(N)] has full row rank;\nare satisfied. Compared to our result, (Lu & Kawaguchi, 2017) therefore allows for the hidden layers of the network to be thinner than the input and output layers. However, our result applies to network equipped with any differentiable convex loss (in fact any differentiable loss L for which firstorder optimality implies global optimality) and we do not require any assumption on the data and targets. Our proof is also shorter and much more elementary by comparison."}, {"heading": "2. Proof of Theorem 1", "text": "Theorem 1 follows as a simple consequence of a more general theorem concerning real-valued functions that take as input a product of matrices. That is, we view the deep linear problem as a specific instance of the following more general problem. Let Mm\u00d7n denote the space of m \u00d7 n real matrices, and let f : MdL\u00d7d0 \u2192 R denote any differentiable function that takes dL \u00d7 d0 matrices as input. For any such function we may then consider both the single-layer optimization\n(P1) { Minimize f(A) over all A in MdL\u00d7d0\nas well as the analogous problem\n(P2)  Minimize f(WLWL\u22121 \u00b7 \u00b7 \u00b7W1) over all L-tuples (W1, . . . ,WL) in Md1\u00d7d0 \u00d7 \u00b7 \u00b7 \u00b7 \u00d7MdL\u00d7dL\u22121\nthat corresponds to a multi-layer deep linear optimization. In other words, in (P2) we consider the task of optimizing f over those matrices A \u2208MdL\u00d7d0 that can be realized by an L-fold product\nA = WLWL\u22121 \u00b7 \u00b7 \u00b7W1 W` \u2208Md`\u00d7d`\u22121 (2)\nof matrices. We may then ask how the parametrization (2) of A as a product of matrices affects the minimization of f, or in other words, whether the problems (P1) and (P2) have similar structure. At heart, the use of DLNs to model nonlinear neural networks centers around this question.\nAny notion of structural similarity between (P1) and (P2) should require that their global minima coincide. As a matrix of the form (2) has rank at most min{d0, . . . , dL}, we must impose the structural requirement\nmin{d1, . . . , dL\u22121} \u2265 min{dL, d0} (3)\nin order to guarantee that (2) does, in fact, generate the full space of dL \u00d7 d0 matrices. Under this assumption we shall prove the following quite general theorem.\nTheorem 3. Assume that f(A) is any differentiable function and that the structural condition (3) holds. Then at any local minimizer ( W\u03021, . . . , W\u0302L ) of (P2) the optimality condition\n\u2207f ( A\u0302 ) = 0 A\u0302 := W\u0302LW\u0302L\u22121 \u00b7 \u00b7 \u00b7 W\u03021\nis satisfied.\nTheorem 1 follows as a simple consequence of this theorem. The first hypothesis (i) of theorem 1 shows that the total loss (1) takes the form\nL(W1, . . . ,WL) = f(WL \u00b7 \u00b7 \u00b7W1)\nfor f(A) some convex and differentiable function. The structural hypothesis (3) is equivalent to the second hypothesis (ii) of theorem 1, and so we can directly apply theorem 3 to conclude that a local minimum ( W\u03021, . . . , W\u0302L ) of L corresponds to a critical point A\u0302 = W\u0302L \u00b7 \u00b7 \u00b7 W\u03021 of f(A), and since f(A) is convex, this critical point is necessarily a global minimum.\nBefore turning to the proof of theorem 3 we recall a bit of notation and provide a calculus lemma. Let\n\u3008A,B\u3009fro := Tr(ATB) = \u2211 i \u2211 j AijBij and\n\u2016A\u20162fro := \u3008A,A\u3009fro\ndenote the Frobenius dot product and the Frobenius norm, respectively. Also, recall that for a differentiable function \u03c6 : Mm\u00d7n 7\u2192 R its gradient\u2207\u03c6(A) \u2208Mm\u00d7n is the unique matrix so that the equality\n\u03c6(A+H) = \u03c6(A) + \u3008\u2207\u03c6(A), H\u3009fro + o (\u2016H\u2016fro) (4)\nholds. If F (W1, . . . ,WL) := f(WL \u00b7 \u00b7 \u00b7W1) denotes the objective of interest in (P2) the following lemma gives the partial derivatives of F as a function of its arguments.\nLemma 1. The partial derivatives of F are given by\n\u2207W1F (W1, . . . ,WL) = WT2,+\u2207f ( A ) ,\n\u2207WkF (W1, . . . ,WL) = WTk+1,+\u2207f(A)WTk\u22121,\u2212, \u2207WLF (W1, . . . ,WL) = \u2207f ( A ) WTL\u22121,\u2212,\nwhere A stands for the full product A := WL \u00b7 \u00b7 \u00b7W1 and Wk,+,Wk,\u2212 are the truncated products\nWk,+ := WL \u00b7 \u00b7 \u00b7Wk, Wk,\u2212 := Wk \u00b7 \u00b7 \u00b7W1. (5)\nProof. The definition (4) implies\nF (W1, . . . ,Wk\u22121,Wk +H,Wk+1, . . . ,WL) = f ( A+Wk+1,+HWk\u22121,\u2212 ) = f(A) + \u3008\u2207f(A),Wk+1,+HWk\u22121,\u2212\u3009fro + o ( \u2016H\u2016fro ) .\nUsing the cyclic property Tr(ABC) = Tr(CAB) of the trace then gives\n\u3008\u2207f(A) , Wk+1,+HWk\u22121,\u2212 \u3009fro = Tr ( \u2207f(A)TWk+1,+HWk\u22121,\u2212 ) = Tr ( Wk\u22121,\u2212\u2207f(A)TWk+1,+H\n) = \u3008WTk+1,+\u2207f(A)WTk\u22121,\u2212 , H \u3009fro\nwhich, in light of (4), gives the desired formula for\u2207WkF . The formulas for\u2207W1F and\u2207WLF are obtained similarly.\nProof of Theorem 3: To prove theorem 3 it suffices to assume that dL \u2265 d0 without loss of generality. This follows from the simple observation that\ng ( A ) := f ( AT )\ndefines a differentiable function of d0 \u00d7 dL matrices for f(A) any differentiable function of dL \u00d7 d0 matrices. As a point ( W1, . . . ,WL ) defines a local minimum of\nf ( WLWL\u22121 \u00b7 \u00b7 \u00b7W1 ) if and only if ( WT1 , . . . ,W T L ) defines\na minimum of g ( V1 \u00b7 \u00b7 \u00b7VL\u22121VL ) , the theorem for the case dL < d0 follows by appealing to its dL \u2265 d0 instance. It\ntherefore suffices to assume that dL \u2265 d0, and by the structural assumption that dk \u2265 d0, throughout the remainder of the proof.\nConsider any local minimizer ( W\u03021, . . . , W\u0302L ) of F and denote by A\u0302, W\u0302k,+ and W\u0302k,\u2212 the corresponding full and truncated products (c.f. (5)). By definition of a local minimizer there exists some \u03b50 > 0 so that\nF (W1, . . . ,WL) \u2265 F (W\u03021, . . . , W\u0302L) = f ( A\u0302 ) (6)\nwhenever the family of inequalities\n\u2016W` \u2212 W\u0302`\u2016fro \u2264 \u03b50 for all 1 \u2264 ` \u2264 L\nall hold. Moreover, lemma 1 yields (i) 0 = W\u0302T2,+\u2207f ( A\u0302 ) ,\n(ii) 0 = W\u0302Tk+1,+\u2207f ( A\u0302 ) W\u0302Tk\u22121,\u2212 \u2200 2 \u2264 k \u2264 L\u2212 1,\n(iii) 0 = \u2207f ( A\u0302 ) W\u0302TL\u22121,\u2212. (7)\nsince all partial derivatives must vanish at a local minimum. If W\u0302L\u22121,\u2212 has a trivial kernel, i.e. ker(W\u0302L\u22121,\u2212) = {0}, then the theorem follows easily. The critical point condition (7) part (iii) implies\nW\u0302L\u22121,\u2212\u2207f ( A\u0302 )T = 0,\nand since W\u0302L\u22121,\u2212 has a trivial kernel this implies \u2207f ( A\u0302 ) = \u2207f ( W\u0302LW\u0302L\u22121 \u00b7 \u00b7 \u00b7 W\u03021 ) = 0 as desired.\nThe remainder of the proof concerns the case that W\u0302L\u22121,\u2212 has a nontrivial kernel. The main idea is to use this nontrivial kernel to construct a family of infinitesimal perturbations of the local minimizer ( W\u03021, . . . , W\u0302L ) that leaves the overall product W\u0302L \u00b7 \u00b7 \u00b7 W\u03021 unchanged. In other words, the family of perturbations ( W\u03031, . . . , W\u0303L ) satisfy\n\u2016W\u0303` \u2212 W\u0302`\u2016fro \u2264 0/2 \u2200` = 1, . . . , L, (8) W\u0303LW\u0303L\u22121 \u00b7 \u00b7 \u00b7 W\u03031 = W\u0302LW\u0302L\u22121 \u00b7 \u00b7 \u00b7 W\u03021. (9)\nAny such perturbation also defines a local minimizer. Claim 1. Any tuple of matrices ( W\u03031, . . . , W\u0303L ) satisfying (8) and (9) is necessarily a local minimizer F .\nProof. For any matrixW` satisfying \u2016W`\u2212W\u0303`\u2016fro \u2264 \u03b50/2, inequality (8) implies that\n\u2016W` \u2212 W\u0302`\u2016fro \u2264 \u2016W` \u2212 W\u0303`\u2016fro + \u2016W\u0303` \u2212 W\u0302`\u2016fro \u2264 \u03b50\nEquality (9) combined to (6) then leads to\nF ( W1, . . . ,WL ) \u2265 f ( A\u0302 )\n= f(W\u0302L \u00b7 \u00b7 \u00b7 W\u03021) = f(W\u0303L \u00b7 \u00b7 \u00b7 W\u03031) = F ( W\u03031, . . . , W\u0303L ) for any W` with \u2016W` \u2212 W\u0303`\u2016fro \u2264 \u03b50/2 and so the point (W\u03031, . . . , W\u0303L) defines a local minimum.\nThe construction of such perturbations requires a preliminary observation and then an appeal to the singular value decomposition. Due to the definition of W\u0302k,\u2212 it follows that ker(W\u0302k+1,\u2212) = ker(W\u0302k+1W\u0302k,\u2212) \u2287 ker(W\u0302k,\u2212), and so the chain of inclusions\nker(W\u03021,\u2212) \u2286 ker(W\u03022,\u2212) \u2286 \u00b7 \u00b7 \u00b7 \u2286 ker(W\u0302L\u22121,\u2212) (10)\nholds. Since W\u0302L\u22121,\u2212 has a nontrivial kernel, the chain of inclusions (10) implies that there exists k\u2217 \u2208 {1, . . . , L\u22121} such that\nker(W\u0302k,\u2212) = {0} if k < k\u2217 (11) ker(W\u0302k,\u2212) 6= {0} if k \u2265 k\u2217 (12)\nIn other words, W\u0302k\u2217,\u2212 is the first matrix appearing in (10) that has a nontrivial kernel.\nThe structural requirement (3) and the assumption that dL \u2265 d0 imply that dk \u2265 d0 for all k, and so the matrix W\u0302k,\u2212 \u2208 Mdk\u00d7d0 has more rows than columns. As a consequence its full singular value decomposition\nW\u0302k,\u2212 = U\u0302k\u03a3\u0302kV\u0302 T k (13)\nhas the shape depicted in figure 1. The matrices U\u0302k \u2208 Mdk\u00d7dk and V\u0302k \u2208 Md0\u00d7d0 are orthogonal, whereas \u03a3\u0302k \u2208 Mdk\u00d7d0 is a diagonal matrix containing the singular values of W\u0302k,\u2212 in descending order. From (12) W\u0302k,\u2212 has a nontrivial kernel for all k \u2265 k\u2217, and so in particular its least singular value is zero. In particular, the (d0, d0) entry of \u03a3\u0302k vanishes if k \u2265 k\u2217. Let u\u0302k denote the corresponding dth0 column of U\u0302k, which exists since dk \u2265 d0.\nClaim 2. Let wk\u2217+1, . . . ,wL denote any collection of vectors and \u03b4k\u2217+1, . . . , \u03b4L any collection of scalars satisfying\nwk \u2208 Rdk , \u2016wk\u20162 = 1 and (14) 0 \u2264 \u03b4k \u2264 0/2 (15)\nfor all k\u2217 + 1 \u2264 k \u2264 L. Then the tuple of matrices (W\u03031, . . . , W\u0303L) defined by\nW\u0303k := { W\u0302k if 1 \u2264 k \u2264 k\u2217 W\u0302k + \u03b4kwku\u0302 T k\u22121 else,\n(16)\nsatisfies (8) and (9).\nProof. Inequality (8) follows from the fact that\n\u2016W\u0303k \u2212 W\u0302k\u2016fro = \u03b4k\u2016wku\u0302Tk\u22121\u2016fro = \u03b4k\u2016wk\u20162\u2016u\u0302k\u22121\u20162\nfor all k > k\u2217, together with the fact that u\u0302k\u22121 and wk are unit vectors and that 0 \u2264 \u03b4k \u2264 0/2.\nTo prove (9) let W\u0303k,\u2212 = W\u0303k \u00b7 \u00b7 \u00b7 W\u03031 and W\u0302k,\u2212 = W\u0302k \u00b7 \u00b7 \u00b7 W\u03021 denote the truncated products of the matrices\nW\u0303k and W\u0302k. The equality W\u0303k\u2217,\u2212 = W\u0302k\u2217,\u2212 is immediate from the definition (16). The equality (9) will then follow from showing that\nW\u0303k,\u2212 = W\u0302k,\u2212 for all k\u2217 \u2264 k \u2264 L. (17)\nProceeding by induction, assume that W\u0303k,\u2212 = W\u0302k,\u2212 for a given k \u2265 k\u2217. Then\nW\u0303k+1,\u2212 = W\u0303k+1W\u0303k,\u2212\n= W\u0303k+1W\u0302k,\u2212 (induction hypothesis) = ( W\u0302k+1 + \u03b4k+1wk+1u\u0302 T k ) W\u0302k,\u2212\n= W\u0302k+1,\u2212 + \u03b4k+1wk+1u T k W\u0302k,\u2212\nThe second term of the last line vanishes, since\nuTk W\u0302k,\u2212 = u T k U\u0302k\u03a3\u0302kV\u0302 T k = e T d0\u03a3\u0302kV\u0302 T k = 0\nwith ed0 \u2208 Rdk the dth0 standard basis vector. The second equality comes from the fact that the columns of U\u0302k are orthonormal, and the last equality comes from the fact that eTd0\u03a3k\u2217 = 0 since the d th 0 row of \u03a3\u0302k\u2217 vanishes. Thus (17) holds, and so (9) holds as well.\nClaims 1 and claim 2 show that the perturbation( W\u03031, . . . , W\u0303L ) defined by (16) is a local minimizer of F . The critical point conditions\n(i) 0 = W\u0303T2,+\u2207f ( A\u0303 ) ,\n(ii) 0 = W\u0303Tk+1,+\u2207f ( A\u0303 ) W\u0303Tk\u22121,\u2212 \u2200 2 \u2264 k \u2264 L\u2212 1,\n(iii) 0 = \u2207f ( A\u0303 ) W\u0303TL\u22121,\u2212\ntherefore hold as well for all choices of wk\u2217+1, . . . ,wL and \u03b4k\u2217+1, . . . , \u03b4L satisfying (14) and (15).\nThe proof concludes by appealing to this family of critical point relations. If k\u2217 > 1 the transpose of condition (ii) gives\nW\u0302k\u2217\u22121,\u2212\u2207f ( A\u0302 )T W\u0303k\u2217+1,+ = 0 (18)\nsince the equalities W\u0303k\u2217\u22121,\u2212 = W\u0302k\u2217\u22121,\u2212 (c.f. (16)) and A\u0303 = W\u0303L \u00b7 \u00b7 \u00b7 W\u03031 = W\u0302L \u00b7 \u00b7 \u00b7 W\u03021 = A\u0302 (c.f. (9)) both hold. But ker(W\u0302k\u2217\u22121,\u2212) = {0} by definition of k\u2217 (c.f. (11)), and so\n\u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+1 = 0. (19)\nmust hold as well. If k\u2217 = 1 then (19) follows trivially from the critical point condition (i). Thus (19) holds for all choices of wk\u2217+1, . . . ,wL and \u03b4k\u2217+1, . . . , \u03b4L satisfying (14) and (15). First choose \u03b4k\u2217+1 = 0 so that W\u0303k\u2217+1 = W\u0302k\u2217+1 and apply (19) to find\n\u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+2W\u0302k\u2217+1 = 0. (20)\nThen take any \u03b4k\u2217+1 > 0 and substract (20) from (19) to get\n1 \u03b4k\u2217+1 \u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+2 ( W\u0303k\u2217+1 \u2212 W\u0302k\u2217+1 ) = \u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+2 ( wk\u2217+1u\u0302 T k\u2217 ) = 0\nfor wk\u2217+1 an arbitrary vector with unit length. Right multiplying the last equality by u\u0302k\u2217 and using the fact that (wk\u2217+1u\u0302 T k\u2217 )u\u0302k\u2217 = wk\u2217+1u\u0302 T k\u2217 u\u0302k\u2217 = wk\u2217+1 shows\n\u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+2wk\u2217+1 = 0 (21)\nfor all choices of wk\u2217+1 with unit length. Thus (21) implies\n\u2207f ( A\u0302 )T W\u0303L \u00b7 \u00b7 \u00b7 W\u0303k\u2217+2 = 0\nfor all choices of wk\u2217+2, . . . ,wL and \u03b4k\u2217+2, . . . , \u03b4L satisfying (14) and (15). The claim\n\u2207f ( A\u0302 ) = 0\nthen follows by induction."}, {"heading": "3. Concluding Remarks", "text": "Theorem 3 provides the mathematical basis for our analysis of deep linear problems. We therefore conclude by discussing its limits.\nFirst, theorem 3 fails if we refer to critical points rather than local minimizers. To see this, it suffices to observe that the critical point conditions for problem (P2),\n(i) 0 = W\u0302T2,+\u2207f ( A\u0302 ) ,\n(ii) 0 = W\u0302Tk+1,+\u2207f ( A\u0302 ) W\u0302Tk\u22121,\u2212 \u2200 2 \u2264 k \u2264 L\u2212 1,\n(iii) 0 = \u2207f ( A\u0302 ) W\u0302TL\u22121,\u2212\nwhere W\u0302k,+ := W\u0302L \u00b7 \u00b7 \u00b7 W\u0302k+1 and W\u0302k,\u2212 := W\u0302k\u22121 \u00b7 \u00b7 \u00b7 W\u03021, clearly hold if L \u2265 3 and all of the W\u0302` vanish. In other words, the collection of zero matrices always defines a critical point for (P2) but clearly \u2207f ( 0 ) need not vanish. To\nput it otherwise, if L \u2265 3 the problem (P2) always has saddle-points even though all local optima are global.\nSecond, the assumption that f(A) is differentiable is necessary as well. More specifically, a function of the form\nF (W1, . . . ,WL) := f(WL \u00b7 \u00b7 \u00b7W1)\ncan have sub-optimal local minima if f(A) is convex and globally Lipschitz but is not differentiable. A simple example demonstrates this, and therefore proves theorem 2. For instance, consider the bi-variate convex function\nf(x, y) := |x|+(1\u2212y)+\u22121, (y)+ := max{y, 0}, (22)\nwhich is clearly globally Lipschitz but not differentiable. The set\narg min f := {(x, y) \u2208 R2 : x = 0, y \u2265 1}\nfurnishes its global minimizers while fopt = \u22121 gives the optimal value. For this function even a two layer deep linear problem\nF ( W1,W2) := f(W2W1) W2 \u2208 R2, W1 \u2208 R\nhas sub-optimal local minimizers; the point\n(W\u03021, W\u03022) =\n( 0, [ 1 0 ]) (23)\nprovides an example of a sub-optimal solution. The set of all possible points in R2\nN (W\u03021, W\u03022) :={ W2W1 : \u2016W2 \u2212 W\u03022\u2016 \u2264 1\n4 , \u2016W1 \u2212 W\u03021\u2016 \u2264\n1\n4 } generated by a 1/4-neighborhood of the optimum (23) lies in the two-sided, truncated cone\nN (W\u03021, W\u03022) \u2282 {\n(x, y) \u2208 R2 : |x| \u2264 1 2 , |y| \u2264 1 2 |x| } ,\nand so if we let x \u2208 R denote the first component of the product W2W1 then the inequality\nf(W2W1) \u2265 1\n2 |x| \u2265 0 = f(W\u03022W\u03021)\nholds on N (W\u03021, W\u03022) and so (W\u03021, W\u03022) is a sub-optimal local minimizer. Moreover, the minimizer (W\u03021, W\u03022) is a strict local minimizer in the only sense in which strict optimality can hold for a deep linear problem. Specifically, the strict inequality\nf(W2W1) > f(W\u03022W\u03021) (24)\nholds on N (W\u03021, W\u03022) unless W2W1 = W\u03022W\u03021 = 0; in the latter case (W1,W2) and (W\u03021, W\u03022) parametrize the same\npoint and so their objectives must coincide. We may identify the underlying issue easily. The proof of theorem 3 requires a single-valued derivative \u2207f(A\u0302) at a local optimum, but with f(x, y) as in (22) its subdifferential\n\u2202f(0) = {(x, y) \u2208 R2 : \u22121 \u2264 x \u2264 1, y = 0}\nis multi-valued at the sub-optimal local minimum (23). In other words, if a globally convex function f(A) induces sub-optimal local minima in the corresponding deep linear problem then \u2207f(A\u0302) cannot exist at any such sub-optimal solution (assuming the structural condition, of course).\nThird, the structural hypothesis\nd` \u2265 min{dL, d0} for all ` \u2208 {1, . . . , L}\nis necessary for theorem 3 to hold as well. If d` < min{d0, dL} for some ` the parametrization\nA = WL \u00b7 \u00b7 \u00b7W1\ncannot recover full rank matrices. Let f(A) denote any function where\u2207f vanishes only at full rank matrices. Then\n\u2207f ( WL \u00b7 \u00b7 \u00b7W1 ) 6= 0\nat all critical points of (P2), and so theorem 3 fails.\nFinally, if we do not require convexity of f(A) then it is not true, in general, that local minima of (P2) correspond to minima of the original problem. The functions\nf(x, y) = x2 \u2212 y2 F (W1,W2) = f(W2W1)\nand the minimizer (23) illustrate this point. While the origin is clearly a saddle point of the one layer problem, the argument leading to (24) shows that (23) is a local minimizer for the deep linear problem. So in the absence of additional structural assumptions on f(A), we may infer that a minimizer of the deep linear problem satisfies first-order optimality for the original problem, but nothing more."}], "year": 2018, "references": [{"title": "Neural networks and principal component analysis: Learning from examples without local minima", "authors": ["P. Baldi", "K. Hornik"], "venue": "Neural networks,", "year": 1989}, {"title": "Complex-valued autoencoders", "authors": ["P. Baldi", "Z. Lu"], "venue": "Neural Networks,", "year": 2012}, {"title": "Identity matters in deep learning", "authors": ["M. Hardt", "T. Ma"], "venue": "In International Conference on Learning Representations,", "year": 2017}, {"title": "Deep learning without poor local minima", "authors": ["K. Kawaguchi"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Depth creates no bad local minima", "authors": ["H. Lu", "K. Kawaguchi"], "venue": "arXiv preprint arXiv:1702.08580,", "year": 2017}, {"title": "Exact solutions to the nonlinear dynamics of learning in deep linear neural networks", "authors": ["A.M. Saxe", "J.L. McClelland", "S. Ganguli"], "venue": "In International Conference on Learning Representations,", "year": 2014}], "id": "SP:c6f24674052cfaa1458ef7039fb44e006e2fc5ec", "authors": [{"name": "Thomas Laurent", "affiliations": []}, {"name": "James H. von Brecht", "affiliations": []}], "abstractText": "We consider deep linear networks with arbitrary convex differentiable loss. We provide a short and elementary proof of the fact that all local minima are global minima if the hidden layers are either 1) at least as wide as the input layer, or 2) at least as wide as the output layer. This result is the strongest possible in the following sense: If the loss is convex and Lipschitz but not differentiable then deep linear networks can have sub-optimal local minima.", "title": "Deep Linear Networks with Arbitrary Loss: All Local Minima Are Global"}