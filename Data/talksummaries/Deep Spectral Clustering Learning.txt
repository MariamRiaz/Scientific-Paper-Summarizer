0	61	Clustering is a widely used technique with applications in machine learning, statistics, speech processing, computer vision.
9	30	In the semi-supervised setting (Xing et al., 2002; Chopra et al., 2005), the training data is given as a small set of example pairs that are expected to be in the same or different clusters.
13	24	In particular, supervised clustering is a specific classification problem where the model is learned so that the representations of training examples are closer to the representative vector of their category than to the representative vector of any other category.
18	29	For instance, many approaches have proposed hard negative mining strategies to limit the number of training constraints.
19	20	A deep supervised clustering approach was proposed in (Song et al., 2017): to compute its gradient, the approach uses an iterative greedy algorithm whose algorithmic complexity is high.
20	32	Contributions: In this paper, we propose a metric learning framework that optimizes an embedding function so that the learned representations of similar examples are grouped into the same cluster, and dissimilar examples are in different clusters.
22	15	In particular, the gradient can be expressed in closed-form, and the algorithmic complexity to compute it is linear in the size of the training mini-batch and quadratic in the representation dimensionality, which is better than the complexity of existing iterative methods.
34	30	Clustering: Partitioning the n observations in F = [f1, · · · , fn]> ∈ Fn into k clusters is equivalent to determining an assignment matrix Ŷ ∈ {0, 1}n×k such that Ŷic = 1 if fi is assigned to cluster c and 0 otherwise.
35	26	In this paper, we assume that each example is assigned to one and only one cluster (i.e. the sum of each row of Ŷ is 1) and that there is no empty cluster (which corresponds to adding the constraint rank(Ŷ ) = k).
38	13	The problem of partitioning the n examples in F ∈ Fn with dφ can then be formulated as minimizing the energy function: min Ŷ ∈Yn×k,Z∈Fk n∑ i=1 k∑ c=1 Ŷic · dφ(fi, zc) (1) = min Ŷ ∈Yn×k,Z∈Fk 1>Φ(F )−1>Φ(Ŷ Z)−〈F−Ŷ Z,∇Φ(Ŷ Z)〉 where we have used the definition of Bregman divergences and we note: ∀A = [a1, · · · ,an]> ∈ Fn, Φ(A) = [φ(a1), · · · , φ(an)]> ∈ Rn is the concatenation into a single vector of the different φ(ai) ∈ R, and ∇Φ(A) = [∇φ(a1), · · · ,∇φ(an)]> ∈ Rn×d is the concatenation into a single matrix of the different gradients∇φ(ai) ∈ Rd.
39	20	Banerjee et al. (2005) demonstrated that, for any value of Ŷ ∈ Yn×k, the minimizer of zc in Eq.
43	18	(1) as a minimization problem w.r.t.
51	20	(2) is a NP-hard problem (Aloise et al., 2009), we then approximate it.
56	18	(2) with the constraint Ĉ ∈ Cn,k where the set Cn,k includes P and is defined as the set of n × n rank-k orthogonal projection matrices Cn,k := {Ĉ ∈ Rn×n : Ĉ2 = Ĉ, Ĉ> = Ĉ, rank(Ĉ) = k}.
57	18	The set of solutions of the resulting relaxed version of Eq.
67	20	In other words, letC = Y Y † ∈ P be the ground truth clustering matrix of F , we would like to learn the embedding function ϕθ so that the matrix predicted with the (relaxed) clustering problem f(F ) ⊆ Cn,k in Eq.
114	25	(7), the (kernel) similarity matrix is K = FF † = UU> where U ∈ Rn×s is a matrix whose columns are the left-singular vectors of F corresponding to its nonzero singular values and where s = rank(F ).
118	22	In this ideal case, we have arg maxĈ∈P〈Ĉ, UU>〉 = {C}, which corresponds to the solution of Eq.
119	34	(3) when replacing F by U ; in other words, partitioning the rows of U with kmeans will return the desired clustering matrix C. It is then clear that comparing the similarity between the rows of U (i.e. using spectral clustering) is at least as relevant as comparing the rows of F to partition the dataset.
156	48	Partitioning a test dataset Xt = [x1, · · · ,xnt ]> ∈ Xnt (where xi ∈ X is an image in these experiments, and nt is the number of test examples) is done by first computing the test representation matrix Ft = [ϕθ(x1), · · · , ϕθ(xnt)]> ∈ Fnt where ϕθ is the learned embedding function, and then applying a partition algorithm.
159	14	To test our method without spectral clustering, we `2- normalize the output representations as done in (Song et al., 2017) and then apply the usual kmeans algorithm with the squared Euclidean distance.
160	15	We compare our method to current state-of-the-art metric learning approaches in Tables 1 to 3 by evaluating the Normalized Mutual Information (NMI) and Recall@K performances.
175	38	We also note that when our model updates only the last layer during fine-tuning, it obtains state-of-the-art performance on Birds and Cars, but not as good as our fully learned model on the Products dataset.
176	34	The strong results on the former two is likely due to their small size (fewer than 10k training images); which makes learning all the layers prone to overfitting.
178	27	We observe that most baselines (Schroff et al., 2015; Sohn, 2016; Song et al., 2017) and our spectral method obtain comparable results on the Products dataset.
179	12	There is then not a clear way to learn deep models in contexts with small categories.
181	18	The largest performance gap is observed on the Cars dataset which contains the largest number of images per category (more than 80).
184	13	Training time: Once the matrix representation of the minibatch F ∈ Fn has been computed, computing the gradient G takes 1 second (with n = 1280 and d = 100).
222	12	Our method is simple to implement and scalable thanks to its low algorithmic complexity.
223	11	It also obtains state-of-the-art recall@K performance on different standard fine-grained datasets.
