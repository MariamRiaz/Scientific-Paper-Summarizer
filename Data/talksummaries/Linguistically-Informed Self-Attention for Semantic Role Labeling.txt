12	23	In experiments on the CoNLL-2005 and CoNLL-2012 datasets we show that our linguistically-informed models out-perform the syntax-free state-of-the-art.
14	59	On the challenging out-of-domain Brown test set, our model improves substantially over the previous state-of-the-art by more than 3.5 F1, a nearly 10% reduction in error.
15	35	On CoNLL-2012, our model gains more than 2.5 F1 absolute over the previous state-of-the-art.
17	36	Our goal is to design an efficient neural network model which makes use of linguistic information as effectively as possible in order to perform endto-end SRL.
18	50	LISA achieves this by combining: (1) A new technique of supervising neural attention to predict syntactic dependencies with (2) multi-task learning across four related tasks.
20	19	The basis for our model is the Transformer encoder introduced by Vaswani et al. (2017): we transform word embeddings into contextually-encoded token representations using stacked multi-head self-attention and feedforward layers (§2.1).
21	83	To incorporate syntax, one self-attention head is trained to attend to each token’s syntactic parent, allowing the model to use this attention head as an oracle for syntactic dependencies.
22	31	We introduce this syntactically-informed self-attention (Figure 2) in more detail in §2.2.
25	75	We simplify optimization and benefit from shared statistical strength derived from highly correlated POS and predicates by treating tagging and predicate detection as a single task, performing multi-class classification into the joint Cartesian product space of POS and predicate labels.
26	35	Though typical models, which re-encode the sentence for each predicate, can simplify SRL to token-wise tagging, our joint model requires a different approach to classify roles with respect to each predicate.
29	23	The model is trained end-to-end by maximum likelihood using stochastic gradient descent (§2.5).
30	101	The basis for our model is a multi-head selfattention token encoder, recently shown to achieve state-of-the-art performance on SRL (Tan et al., 2018), and which provides a natural mechanism for incorporating syntax, as described in §2.2.
34	27	For experiments with gold predicates, we concatenate a predicate indicator embedding pt following previous work (He et al., 2017).
37	37	We feed this token representation as input to a series of J residual multi-head self-attention layers with feed-forward connections.
40	38	The multi-head self attention consists of H attention heads, each of which learns a distinct attention function to attend to all of the tokens in the sequence.
45	41	Following Vaswani et al. (2017) we perform scaled dot-product attention: We scale the weights by the inverse square root of their embedding dimension and normalize with the softmax function to produce a distinct distribution for each token over all the tokens in the sentence: A (j) h = softmax(d 0.5 k Q (j) h K (j) h T ) (2) These attention weights are then multiplied by V (j) h for each token to obtain the self-attended to- ken representations M (j)h : M (j) h = A (j) h V (j) h (3) Row t of M (j)h , the self-attended representation for token t at layer j, is thus the weighted sum with respect to t (with weights given by A(j)h ) over the token representations in V (j)h .
69	31	Since POS is a strong predictor of predicates4 and the complexity of training a multi-task model increases with the number of tasks, we combine POS tagging and predicate detection into a joint label space: For each POS tag TAG which is observed co-occurring with a predicate, we add a label of the form TAG:PREDICATE.
75	74	So, the role label scores sft for the token at index t with respect to the predicate at index f (i.e. token t and frame f ) are given by: sft = (s pred f ) T Us role t (6) which can be computed in parallel across all semantic frames in an entire minibatch.
104	60	We present results on the CoNLL-2005 shared task (Carreras and Màrquez, 2005) and the CoNLL-2012 English subset of OntoNotes 5.0 (Pradhan et al., 2006), achieving state-of-the-art results for a single model with predicted predicates on both corpora.
105	20	We experiment with both standard pre-trained GloVe word embeddings (Pennington et al., 2014) and pre-trained ELMo representations with fine-tuned task-specific parameters (Peters et al., 2018) in order to best compare to prior work.
109	39	We compare our LISA models to four strong baselines: For experiments using predicted predicates, we compare to He et al. (2018) and the ensemble model (PoE) from He et al. (2017), as well as a version of our own self-attention model which does not incorporate syntactic information (SA).
114	24	We also evaluate our model using the gold syntactic parse at test time (+Gold), to provide an upper bound for the benefit that syntax could have for SRL using LISA.
118	25	For models using GloVe embeddings, our syntax-free SA model already achieves a new state-of-the-art by jointly predicting predicates, POS and SRL.
119	26	LISA with its own parses performs comparably to SA, but when supplied with D&M parses LISA out-performs the previous state-of-the-art by 2.5 F1 points.
122	88	The gap in SRL F1 between models using LISA and D&M parses is smaller due to LISA’s improved parsing accuracy (see §4.2), but LISA with D&M parses still achieves the highest F1: nearly 1.0 absolute F1 higher than the previous state-of-the art on WSJ, and more than 2.0 F1 higher on Brown.
147	71	Following He et al. (2017), we next apply a series of corrections to model predictions in order to understand which error types the gold parse resolves: e.g. Fix Labels fixes labels on spans matching gold boundaries, and Merge Spans merges adjacent predicted spans into a gold span.6 In Figure 3 we see that much of the performance gap between the gold and predicted parses is due to span boundary errors (Merge Spans, Split Spans and Fix Span Boundary), which supports the hypothesis proposed by He et al. (2017) that incorporating syntax could be particularly helpful for resolving these errors.
151	138	50%) even after providing the correct PP attachment to the model, indicating that PP span boundary mistakes are a fundamental difficulty for SRL.
152	35	We present linguistically-informed self-attention: a multi-task neural network model that effectively incorporates rich linguistic information for semantic role labeling.
153	20	LISA out-performs the state-ofthe-art on two benchmark SRL datasets, including out-of-domain.
154	167	Future work will explore improving LISA’s parsing accuracy, developing better training techniques and adapting to more tasks.
