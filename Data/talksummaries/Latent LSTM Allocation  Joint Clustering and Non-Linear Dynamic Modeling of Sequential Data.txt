2	40	Accurate user activity modeling is very important for serving relevant, personalized, and useful contents to the user.
3	41	A good model of sequential data should be accurate, sparse, and interpretable.
5	22	The state-of-the-art for modeling sequential data is to employ recurrent neural networks (RNN) (Lipton et al., 2015), such as LSTMs (Long-Short Term Memory) (Hochreiter & Schmidhuber, 1997).
14	63	LLA borrows graph- ical model techniques to infer topics (groups of related word or user activities) by sharing statistical strength across users/documents and recurrent deep networks to model the dynamics of topic evolution inside each sequence (document or user activities) rather than at user action/word level (Sec.
24	22	Imposing a bag of words assumption - as used in LDA - leads to ignoring the sequence information and yields ∑n i=1 log p(wi|model).
35	23	In such cases, input is appropriately transformed and desired output is produced at each time step from the state st: yt = g(st), where g is an arbitrary differentiable function.
36	14	For example, in a regular recurrent language model (RRLM) (Mikolov et al., 2010) a document is treated as a sequence and an LSTM is trained to predict directly the next word conditioned on the sequence of words before it, i.e. maximize p(wt|wt−1|, wt−2, ..., w0; model).
39	22	The output transformation is the projection of st into a vector of size of the vocabulary V followed by a softmax.
47	15	In this section, we provide a detailed description of the proposed LLA model and its variant for performing joint clustering and modeling non-linear dynamics.
58	21	Choose a topic zd,t ∼ Categorical(θ) iv.
60	58	Here p(zd,t|zd,1:t−1; LSTM) is the probability of generating topic for the next word in the document given topics of previous words and p(wd,t|zd,t;φ) is the probability of generating word given the topic, illustrating the simple modification of LSTM and LDA based language models.
63	21	This is because, unlike RRLM we operate at topic level instead of words directly and the number of topics is much smaller than the vocabulary size, i.e., K << V .
77	16	This acts as an unbiased sample estimate of the expectation with respect to the conditional distribution of z required in traditional EM.
84	19	To elaborate, let us define nwk as the number of times word w currently assigned to topic k and nk as the number of tokens assigned to topic k. Explicitly writing down first term: p(wd,t = w|zd,t = k,φ) = φwk = nwk + β nk + V β = nwk nk + V β︸ ︷︷ ︸ sparse + β nk + V β︸ ︷︷ ︸ dense (3) There is an inherent sparsity present in nwk, as a given word would be typically about only a handful of topics, Kw K. The second term represents the global count of tokens in the topic and will be dense, regardless of the number of documents, words or topics.
108	26	The corresponding graphical model is shown in Figure 2b and the joint likelihood is:∑ d log p(wd|LSTM, φ) = ∑ d ∑ t log ∑ zd,t p(wd,t|zd,t;φ)p(zd,t|wd,1:t−1; LSTM) A SEM inference strategy can be devised similar to that of topic LLA as presented in section 3.2.
110	14	This problem can be mitigated by using char-level LSTM (char LSTM for short) to construct the input transformation.
121	56	The different variants of LLA and LSTM as language model can be unified and thought of having different input and out transformations over LSTM for capturing the dynamics.
132	17	(a) Wikipedia (b) User search click history Figure 4.
151	34	Accuracy vs model size Figure 4(a) compares model size in terms of number of parameters with model accuracy in terms of test perplexity.
162	26	Moreover, compared to fast LDA samplers (Zaheer et al., 2017), our LLA variants introduce only a modest increase in training time.
163	23	The figure also shows that character based models (char-LSTM and char-LLA) are slightly slower to train compared to word level variants due to their nested nature and the need to propagate gradients over both word and character level LSTMs.
164	21	Ablation study Since both LDA and LLA result in interpretable models, we want to explore if LDA can achieve a perplexity similar to a given LLA model by just increasing the number of topics in LDA.
165	25	Figure 5 shows the performance of LDA and variants of LLA for different number of topics.
166	37	As can be seen from the figure, even with 250 topics, all LLA based models achieve much lower perplexity than LDA with 1000 topics.
167	19	In other words, LLA improves over LDA not because it uses a slightly larger model, but rather because it models the sequential order in the data.
172	39	LDA includes spurious words like Iowa in the topic just because it co-occurs in the same documents.
175	42	For example, strikes among mine workers are common, so the two words will co-occur heavily but it does not imply that strikes and mining should be in the same topic.
181	23	To understand this, we built a topic hierarchy using the embeddings which uncovers interesting facts about the data.
199	28	LLA, on the other hand, is capable of capturing this intricate dynamics by modeling the evolution of user topics via an LSTM – an extremely powerful dynamic model.
200	18	This point is more evident if we consider the following hand-crafted2 user click trace in context of the topics depicted in Figure 7: theknot.com zola.com weddingwire.com www.bridalguide.com pinterest.com food.com doityourself.com .....
