0	110	Online platforms have revolutionized the way individuals collect and share information (O’Connor et al., 2010; Lee and Ma, 2012; Bakshy et al., 2015), but the vast bulk of online content is irrelevant or unpalatable to any given individual.
1	12	A user interested in political discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015).
3	29	We approach this problem from a microblog conversation recommendation framework.
4	80	Where prior work has focused on the content of individual posts for recommendation (Chen et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new 375 and repeated entry into conversations based on a combination of topical and discourse features.
6	17	User U1 participates in both conversations.
7	27	The first conversation is centered around Clinton, and U1, who is more typically involved with conversations about candidate Sanders, does not return.
8	58	In the second conversation, however, U1 is involved in a heated back-and-forth debate, and thus is drawn back to a conversation that they may otherwise have abandoned but for their enjoyment of adversarial discourse.
34	22	Our proposed microblog conversation recommendation framework is based on collaborative filtering and a novel probabilistic graphical model.
36	15	First, L models user reply preference in a similar fashion to collaborative filtering (CF) (Hu et al., 2008; Pan et al., 2008).
46	18	3.1 Reply Preference (L) Our user reply preference modeling is built on the success of collaborative filtering (CF) for product ratings.
50	45	Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c.
63	15	Parameter a is an offset parameter, bu and bc are user and conversation biases, and λ ∈ [0, 1] serves as the weight for trading offs of topic and discourse factors in reply preference modeling.
92	10	However, it is nontrivial to link the topic-related parameters of γCc to the conversation topic distributions of θc, since the former takes real values from −∞ to +∞ while the latter is a stochastic vector.
93	22	Therefore, we follow the strategy from McAuley and Leskovec (2013) to apply a softmax function over γCc : θc,k = exp(κT γCc,k)∑K k′=1 exp(κ T γCc,k′) (6) We further assume that the discourse mode preference by users, δUu , can also be informed by the discourse mode distribution captured by πu, i.e., a user who enjoys arguments may be willing to participate another.
102	15	7 – Draw topic assignment zc,m ∼Multi(θc) – Draw discourse mode dc,m ∼Multi(πuc,m) – For word index n = 1 to Nc,m: ∗ Draw word type xc,m,n ∼Multi(τd) ∗ if xc,m,n == BACK: Draw word wc,m,n ∼Multi(φB) ∗ if xc,m,n == DISC: Draw word wc,m,n ∼Multi(φDdc,m) ∗ if xc,m,n == TOPIC: Draw word wc,m,n ∼Multi(φTzc,m) Parameter Learning.
111	18	We collected two microblog conversation datasets from Twitter for experiments3: one contains discussions about the U.S. presidential election (henceforth US Election), the other gathers conversations of diverse topics based on the tweets released by TREC 2011 microblog track (henceforth TREC)4.
118	21	For model training and testing, we divide conversations into three ordered segments, corresponding to training, development, and test sets at 75%, 12.5%, and 12.5%.8 Preprocessing and Hyperparameter Tuning.
131	19	For comparison, we first consider three baselines: 1) ranking conversations randomly (RANDOM); 2) longer conversations (i.e., more words) ranked higher (LENGTH); 3) conversations with more distinct users ranked higher (POPULARITY).
133	11	• RSVM: ranking SVM (Joachims, 2002), which ranks conversations for each user with the content and Twitter features as in Duan et al. (2010).
134	38	• CTR: messages in one conversation are aggregated into one post and a state-of-the art Collaborative Filtering-based post recommendation model is applied (Chen et al., 2012).
143	11	We find that the baselines that rank conversations with simple features (e.g., length or popularity) perform poorly.
145	18	Although some non-baseline systems capture content in one way or another, only ADAPTED HFT and our model exploit latent topic models to better represent content in tweets, and outperform other methods.
149	65	To test the model performance based different levels of user engagement history, we further experiment with varying the length of conversations for training.
165	15	Table 3 shows the predicted scores for the two conversations from OCCF, ADAPTED HFT, and our model (as in Eq.
181	11	We thus compare each recommendation system for first time replies.
183	16	Table 7 shows that, unsurprisingly, all systems perform poorly on this task, though our model performs slightly better.
184	63	This suggests that other features, e.g., network structures or other discussion thread features, could usefully be included in future studies that target new conversations.
185	15	This paper has presented a framework for microblog conversation recommendation via jointly modeling topics and discourse modes.
186	20	Experimental results show that our method can outperform competitive approaches that omit user discourse behaviors.
187	87	Qualitative analysis shows that our joint model yields meaningful topics and discourse representations.
