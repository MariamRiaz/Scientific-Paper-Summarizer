4	7	Machine learning methods seek to estimate the pairwise utilities of bipartite graphs so that the maximum weighted complete matching is most compatible with the (distribution of) ground truth matchings of training data.
7	34	Given this difficulty, two natural desiderata for any predictor are: • Efficiency: learning from training data and making predictions must be computed efficiently in (lowdegree) polynomial time; and • Consistency: the predictor’s training objectives must also minimize the underlying Hamming loss, at least under ideal learning conditions (given the true distribution and fully expressive model parameters).
10	20	Given the deficiencies of the existing methods, we contribute the first approach for learning bipartite matchings that is both computationally efficient and Fisher consistent.
14	37	Given two sets of elements A and B of equal size (|A| = |B|), a maximum weighted bipartite matching π is the one-toone mapping (e.g., Figure 1) from each element in A to each element in B that maximizes the sum of potentials: maxπ∈Π ψ(π) = maxπ∈Π ∑ i ψi(πi).
17	20	Many machine learning tasks pose prediction as the solution to this problem, including: word alignment for natural language processing tasks (Taskar et al., 2005b; Padó & Lapata, 2006; MacCartney et al., 2008); learning correspondences between images in computer vision applications (Belongie et al., 2002; Dellaert et al., 2003); protein structure analysis in computational biology (Taylor, 2002; Wang et al., 2004); and learning to rank a set of items for information retrieval tasks (Dwork et al., 2001; Le & Smola, 2007).
18	13	Thus, learning appropriate weights ψi(·) for bipartite graph matchings is a key problem for many application areas.
25	10	This implies Fisher consistency because the MAP estimate under this distribution, which can be obtained as a maximum weighted bipartite matching, is Bayes optimal.
27	9	The normalization term, Zψ , is the permanent of a matrix defined in terms of exponentiated potential terms: Zψ = ∑ π ∏n i=1 e ψi(πi) = perm(M) where Mi,j = e ψi(j).
32	11	Maximum margin methods for structured prediction seek potentials ψ that minimize the training sample hinge loss: min ψ Eπ∼P̃ [ max π′ {loss(π, π′) + ψ(π′)} − ψ(π) ] , (1) where P̃ is the empirical distribution.
33	7	Finding the optimal ψ is a convex optimization problem (Boyd & Vandenberghe, 2004) that can generally be tractably solved using constraint generation methods as long as the maximizing assignments can be found efficiently.
46	14	Our approach seeks a predictor that robustly minimizes the Hamming loss against the worst-case permutation mixture probability that is consistent with the statistics of the training data.
48	12	Instead of evaluating the predictor with the empirical distribution, the predictor is pitted against an adversary that also makes a probabilistic prediction (denoted as P̌ ).
50	12	The adversary (and only the adversary) is constrained to select a probabilistic prediction that matches the statistical summaries of the empirical training distribution (denoted as P̃ ) via moment matching constraints on joint features φ(x, π).
62	16	This makes explicit construction of the Lagrangian minimax game intractable for modestly-sized problems.
63	9	Our first approach for taming the factorial computational complexity of explicitly constructing games for matching tasks is a constraint-generation approach known as the double oracle method (McMahan et al., 2003).
65	16	Based on the key observation that the equilibrium of the zero-sum game is typically supported by a relatively small number of permutations, it seeks to efficiently uncover this sparse set of permutations for each player.
70	9	The algorithm then obtains the other player’s best response to either P̂ or P̌ (Lines 4 and 7) with values Vmax and Vmin using the Kuhn-Munkres (Hungarian) algorithm in O(n3) time for sets of size n. These best responses, π̌new and π̂new, are added to the set of active permutations (i.e., new rows or columns in the game matrix) if they have better values than the previous equilibrium values (Lines 5 and 8).
71	14	This is repeated until no game value improvement exists for either player (Line 9), at which point a Nash equilibrium for the full game has been obtained.
83	10	For a given distribution of permutations, P (π), we denote the marginal probabilities of matching i with j as pi,j , P (πi = j).
84	20	We let P = ∑ π P (π)Y(π) be the predictor’s marginal probability matrix where its (i, j) cell represents P̂ (π̂i = j), and similarly let Q be the adversary’s marginal probability matrix (based on P̌ ), as shown in Table 2.
85	9	The Birkhoff–von Neumann theorem (Birkhoff, 1946; Von Neumann, 1953) states that the convex hull of the set of n× n permutation matrices forms a convex polytope in Rn2 (known as the Birkhoff polytope Bn) in which points are doubly stochastic matrices, i.e., the n×nmatrices with non-negative elements where each row and column must sum to one.
140	7	For the cases of multiclass classification and ordinal regression, Fisher consistency for adversarial surrogate loss has been established by Fathony et al. (2016; 2017).
196	7	In terms of the running time, Table 5 shows that the marginal version of adversarial method is relatively fast.
198	8	The running time grows roughly quadratically in the number of elements, which is natural since the size of the marginal probability matrices P and Q also grow quadratically in the number of elements.
199	9	This shows that our approach is much more efficient than the CRF approach, which has a running time that is impractical even for small problems with 20 elements.
200	15	The training time of SSVM is faster than the adversarial methods due to two different factors: (1) the inner optimization of SSVM can be solved using a single execution of the Hungarian algorithm compared with the inner optimization of adversarial method which requires ADMM optimization for projection to doubly stochastic matrix set; (2) different tools for implementation, i.e., C++ for SSVM and MATLAB for our method, which benefits the running time of SSVM.
201	10	In addition, though the game size is relatively small, as indicated by the final column in Table 4, the double oracle version of adversarial method takes much longer to train compared to the marginal version.
202	18	In this paper, we have presented an adversarial approach for learning bipartite matchings that is not only computationally efficient to employ but also provides Fisher consistency guarantees.
203	38	We showed that these theoretical advantages translate into better empirical performance for our model compared with previous approaches.
204	35	Our future work will explore matching problems with different loss functions and other graphical structures.
