0	25	We consider extractive text summarization, the task of condensing a textual source, e.g., a set of source documents in multi-document summarization (MDS), into a short summary text.
4	13	To this end, optimization-based systems commonly use an objective function which encodes exactly those quality indicators which are measured by the particular evaluation metric being used.
7	27	ROUGE computes the n-gram overlap between a system summary and a pool of reference summaries.
9	15	However, ROUGE has been widely criticized for being too simplistic and not suitable for capturing important quality aspects we are interested in.
12	23	A well-known example of such a human evaluation method is the so-called Pyramid method (Nenkova et al., 2007): it evaluates the particular quality aspect of content selection and is based on a manual comparison of Summary Content Units (SCUs) in reference summaries against SCUs in system summaries.
19	19	In our work, we are the first to explore this new direction and to systematically investigate the use of AP in optimization-based extractive summarization.
21	21	• We develop a new extractive MDS system specifically optimizing for an approximation of AP.
27	15	Pyramid The Pyramid method (Nenkova et al., 2007) is a manual evaluation method which determines to what extent a system summary covers the content expressed in a set of reference summaries.
33	24	Each SCU has a weight corresponding to the number of reference summaries in which the SCU appears.
36	27	The Pyramid score of a system summary is then calculated as the sum of the SCU weights for all Pyramid set SCUs being aligned to annotated system summary phrases.
41	34	As for the Pyramid sets, we can assume that these have already been created, either via PEAK or by humans (e.g., using the TAC 2009 data1).
45	25	For scoring, PEAK processes a system summary with clausIE, converting it from a list of sentences to a list of propositions {si}.
49	30	Since each system summary unit can be aligned to at most one SCU, the alignment of the summary propositions {si} and the pyramid propositions {pj} is equivalent to finding a maximum weight matching, which PEAK solves using the Munkres-Kuhn bipartite graph algorithm.
75	29	Then the Munkres-Kuhn algorithm is applied to pS and ppyr in order to find matching propositions, and finally the scores of their corresponding SCUs are used to evaluate the fitness of the summary.
80	20	With a population of 100 summaries in the GA, the algorithm converges in less than a minute to high scoring summaries, which we can expect to be close to the real upper-bound.
87	14	fθ represents the notion of importance learned in the context of AP, while gγ contains notions of coherence and redundancy by scoring sentence intersections.
108	31	As training examples we take the population of scored summaries created by the same GA we use for computing upper-bound summaries.
112	17	Optimization-based Summary Extraction Since the function π is constrained to be linear, we can extract the best scoring summary by solving an ILP.
158	13	While the system is not designed with ROUGE in mind, it still performs reasonably well in the ROUGE evaluation, even though it does not outperform previous works.
161	16	When we compare the system performances to the upper-bound scores reported in Table 1, we see that there is still a large room for improvements.
166	24	Pearson’s r is a value correlation metric which depicts linear relationship between the scores produced by two ranking lists.
171	19	For comparison, we report how well our baselines correlate with π∗.
172	19	For this, we consider the scoring function for summaries which is part of all our baselines, and which they explicitly or implicitly optimize: TF*IDF greedily maximizes fTF∗IDF , the sum of the frequency of the words in the summary.
177	24	This explains why optimizing π with ILP outperforms the baseline systems in the end-to-end evaluation (Table 2).
191	50	We want to investigate this further and understand to what extent ROUGE and AP disagree.
192	62	To that end, we use the summaries automatically generated by the genetic algorithm during the upperbound computation.
193	34	Remember that for each topic of TAC-2009 it produces 100 summaries with a wide range of AP scores.
213	43	We observed that the summaries extracted with our framework achieve significantly better AP scores than several strong baselines, but compared to the upper-bound for AP, there is still a large room for improvement.
214	33	We show that AP and ROUGE catch different aspects of summary quality, but further work would be needed in order to substantiate the claim that AP is indeed better than ROUGE.
215	52	One way of doing so would be to perform a human evaluation of high-scoring summaries according to ROUGE and AP.
