12	31	In particular, we investigate to what extent gender prediction can rely on generic non-lexical features (RQ1), and how predictive such models are when transferred to other languages (RQ2).
13	25	We also glean insights from human judgments, and investigate how well people can perform cross-lingual gender prediction (RQ3).
17	16	We investigate this question by building a series of predictive models to infer the gender of a Twitter user, in absence of additional user-specific metadata.
19	33	To represent utterances in a more language agnostic way, we propose to simply transform the text into alternative textual representations, which deviate from the lexical form to allow for abstraction.
20	24	We propose the following transformations, exemplified in Table 1.
21	46	They are mostly motivated by intuition and inspired by prior work, like the use of shape features from NER and parsing (Petrov and Klein, 2007; Schnabel and Schütze, 2014; Plank et al., 2016; Limsopatham and Collier, 2016): • Frequency Each word is presented as its binned frequency in the training data; bins are sized by orders of magnitude.
22	23	• Length Number of characters (prefixed by 0 to avoid collision with the next transformation).
23	45	• PunctC Merges all consecutive alphanumeric characters to one ‘W’ and leaves all other characters as they are (C for conservative).
24	45	• PunctA Generalization of PunctC (A for aggressive), converting different types of punctuation to classes: emoticons1 to ‘E’ and emojis2 to ‘J’, other punctuation to ‘P’.
25	21	• Shape Transforms uppercase characters to ‘U’, lowercase characters to ‘L’, digits to ‘D’ and all other characters to ‘X’.
27	18	• Vowel-Consonant To approximate vowels, while being able to generalize over (IndoEuropean) languages, we convert any of the ‘aeiou’ characters to ‘V’, other alphabetic character to ‘C’, and all other characters to ‘O’.
33	37	If humans can predict gender cross-lingually, they are likely to rely on aspects beyond lexical information.
34	48	Data We obtain data from the TWISTY corpus (Verhoeven et al., 2016), a multi-lingual collection of Twitter users, for the languages with 500+ users, namely Dutch, French, Portuguese, and Spanish.
39	34	We use 200 tweets per user, as done by previous work (Verhoeven et al., 2016).
42	20	We use the scikit-learn (Pedregosa et al., 2011) implementation of a linear SVM with default parameters (e.g., L2 regularization).
45	62	For the lexicalized experiments, we adopt the features from the best performing system at the latest PAN evaluation campaign3 (Basile et al., 2017) (word 1-2 grams and character 3-6 grams).
46	29	For the multilingual embeddings model we use the mean embedding representation from the system of (Plank, 2017) and add max, std and coverage features.
60	28	If we go across language, the lexical approaches break down (overall to 53.7% for LEX AVG/56.3% for ALL), except for Portuguese and Spanish, thanks to their similarities (see Table 3 for pair-wise results).
67	15	The performance is on average 6% higher across all languages (57.9% for AVG, 63.9% for ALL) in comparison to their lexicalized counterparts, where ABS ALL results in the overall best model.
68	18	For Spanish, the multilingual embedding model clearly outperforms ABS.
69	39	However, the approach requires large Twitterspecific embeddings.4 For our ABS model, if we investigate predictive features over all languages, cf.
70	58	Table 4, we can see that the use of an emoji (like ) and shape-based features are predictive of female users.
74	15	In the other experiment, we asked speakers of French to identify the gender of the writer when reading Dutch tweets.
83	18	For each of the three experiments we had six judges, balanced for gender, and obtained three annotations per target user.
85	32	Table 5 shows accuracy against the gold labels, comparing humans (average accuracy over three annotators) to lexical and bleached models on the exact same subset of 200 users.
86	44	Systems were tested under two different conditions regarding the number of tweets per user for the target language: machine and human saw the exact same twenty tweets, or the full set of tweets (200) per user, as done during training (Section 3.1).
96	16	This seems to indicate that if humans cannot rely on the lexicon, they might be exploiting some other signal when guessing the gender of a user who tweets in a language unknown to them.
108	57	We are well aware that we are testing our crosslanguage bleached models in the context of closely related languages.
111	33	In our novel study on human proficiency for cross-lingual gender prediction, we discovered that people are also abstracting away from the lexicon.
112	36	Indeed, we observe that they are able to detect gender by looking at tweets in a language they do not know (RQ3) with an accuracy of 60% on average.
