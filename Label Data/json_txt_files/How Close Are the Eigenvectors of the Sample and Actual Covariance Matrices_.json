{"sections": [{"heading": "1 Introduction", "text": "The covariance matrix C of an n-dimensional distribution is an integral part of data analysis, with numerous occurrences in machine learning and signal processing. It is therefore crucial to understand how close is the sample covariance, i.e., the matrix C\u0303 estimated from a finite number of samples m, to the actual covariance matrix. Following developments in the tools for the concentration of measure, (Vershynin, 2012) showed that a sample size of m = O(n) is up to iterated logarithmic factors sufficient for all distributions with finite fourth moment supported in a centered Euclidean ball of radius O( \u221a n). Similar results hold for sub-exponential (Adamczak et al., 2010) and finite second moment distributions (Rudelson, 1999).\nWe take an alternative standpoint and ask if we can do\n1E\u0301cole Polytechnique Fe\u0301de\u0301rale de Lausanne, Switzerland. Correspondence to: Andreas Loukas<andreas.loukas@epfl.ch>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nbetter when only a subset of the spectrum is of interest. Concretely, our objective is to characterize how many samples are sufficient to guarantee that an eigenvector and/or eigenvalue of the sample and actual covariance matrices are, respectively, sufficiently close. Our approach is motivated by the observation that methods that utilize the covariance commonly prioritize the estimation of principal eigenspaces. For instance, in (local) principal component analysis we are usually interested in the direction of the first few eigenvectors (Berkmann & Caelli, 1994; Kambhatla & Leen, 1997), where in linear dimensionality reduction one projects the data to the span of the first few eigenvectors (Jolliffe, 2002; Frostig et al., 2016). In the nonasymptotic regime, an analysis of these methods hinges on characterizing how close are the principal eigenvectors and eigenspaces of the sample and actual covariance matrices.\nOur finding is that the \u201cspectral leaking\u201d occurring in the eigenvector estimation is strongly concentrated along the eigenvalue axis. In other words, the eigenvector u\u0303i of the sample covariance is far less likely to lie in the span of an eigenvector uj of the actual covariance when the eigenvalue distance |\u03bbi \u2212 \u03bbj | is large, and the concentration of the distribution in the direction of uj is small. This agrees with the intuition that principal components are easier to estimate, exactly because they are more likely to appear in the samples of the distribution.\nWe provide a mathematical argument confirming this phenomenon. Under fairly general conditions, we prove that\nm = O\n( k2j\n(\u03bbi \u2212 \u03bbj)2\n) and m = O ( k2i \u03bb2i ) (1)\nsamples are asymptotically almost surely (a.a.s.) sufficient to guarantee that |\u3008u\u0303i, uj\u3009| and |\u03bb\u0303i\u2212\u03bbi|/\u03bbi, respectively, is small for all distributions with finite second moment. Here, k2j is a measure of the concentration of the distribution in the direction of uj . We also attain a high probability bound for sub-gaussian distributions supported in a centered Euclidean ball. Interestingly, our results lead to sample estimates for linear dimensionality reduction, and suggest that linear reduction is feasible even from few samples.\nTo the best of our knowledge, these are the first nonasymptotic results concerning the eigenvectors of the sam-\nple covariance of non-Normal distributions. Previous studies have intensively investigated the limiting distribution of the eigenvalues of a sample covariance matrix (Silverstein & Bai, 1995; Bai, 1999), such as the smallest and largest eigenvalues (Bai & Yin, 1993) and the eigenvalue support (Bai & Silverstein, 1998). Eigenvectors and eigenprojections have attracted less attention; the main research thrust entails using tools from the theory of large-dimensional matrices to characterize limiting distributions (Anderson, 1963; Girko, 1996; Schott, 1997; Bai et al., 2007) and it has limited applicability in the nonasymptotic setting where the sample size m is small and n cannot be arbitrary large.\nDifferently, we use techniques from perturbation analysis and non-asymptotic concentration of measure. However, in contrast to arguments commonly used to reason about eigenspaces (Davis & Kahan, 1970; Yu et al., 2015; Huang et al., 2009; Hunter & Strohmer, 2010), our bounds can characterize weighted linear combinations of \u3008u\u0303i, uj\u30092 over i and j, and do not depend on the minimal eigenvalue gap separating two eigenspaces but rather on all eigenvalue differences. The latter renders them useful to many real datasets, where the eigenvalue gap is not significant but the eigenvalue magnitudes decrease sufficiently fast.\nWe also note two recent works targeting the nonassymptotic regime of Normal distributions. Shaghaghi and Vorobyov recently characterized the first two moments of the subspace projection error, a result which implies sample estimates (Shaghaghi & Vorobyov, 2015), but is restricted to specific projectors. A refined concentration analysis for spectral projectors of Normal distributions was also presented in (Koltchinskii & Lounici, 2015). Finally, we remark that there exist alternative estimators for the spectrum of the covariance with better asymptotic properties (Ahmed, 1998; Mestre, 2008). Instead, we here focus on the standard estimates, i.e., the eigenvalues and eigenvectors of the sample covariance."}, {"heading": "2 Problem Statement and Main Results", "text": "Let x \u2208 Cn be a sample of a multivariate distribution and denote by x1, x2, . . . , xm the m independent samples used\nto form the sample covariance, defined as\nC\u0303 = m\u2211 p=1 (xp \u2212 x\u0304)(xp \u2212 x\u0304)\u2217 m , (2)\nwhere x\u0304 is the sample mean. Denote by ui the eigenvector of C associated with eigenvalue \u03bbi, and correspondingly for the eigenvectors u\u0303i and eigenvalues \u03bb\u0303i of C\u0303, such that \u03bb1 \u2265 \u03bb2 \u2265 . . . \u2265 \u03bbn. We ask:\nProblem 1. How many samples are sufficient to guarantee that the inner product |\u3008u\u0303i, uj\u3009| = |u\u0303\u2217i uj | and the eigenvalue gap |\u03b4\u03bbi| = |\u03bb\u0303i \u2212 \u03bbi| is smaller than some constant t with probability larger than ?\nClearly, when asking that all eigenvectors and eigenvalues of the sample and actual covariance matrices are close, we will require at least as many samples as needed to ensure that \u2016C\u0303 \u2212 C\u20162 \u2264 t. However, we might do better when only a subset of the spectrum is of interest. The reason is that inner products |\u3008u\u0303i, uj\u3009| are strongly concentrated along the eigenvalue axis. To illustrate this phenomenon, let us consider the distribution constructed by the n = 784 pixel values of digit \u20181\u2019 in the MNIST database. Figure 1, compares the eigenvectors uj of the covariance computed from all 6742 images, to the eigenvectors u\u0303i of the sample covariance matrices C\u0303 computed from a random subset of m = 10, 100, 500, and 1000 samples. For each i = 1, 4, 20, 100, we depict at \u03bbj the average of |\u3008u\u0303i, uj\u3009| over 100 sampling draws. We observe that: (i) The magnitude of \u3008u\u0303i, uj\u3009 is inversely proportional to their eigenvalue gap |\u03bbi\u2212\u03bbj |. (ii) Eigenvector u\u0303j mostly lies in the span of eigenvectors uj over which the distribution is concentrated.\nWe formalize these statements in two steps."}, {"heading": "2.1 Perturbation arguments", "text": "First, we work in the setting of Hermitian matrices and notice the following inequality:\nTheorem 3.2. For Hermitian matricesC and C\u0303 = \u03b4C+C, with eigenvectors uj and u\u0303i respectively, the inequality\n|\u3008u\u0303i, uj\u3009| \u2264 2 \u2016\u03b4Cuj\u20162 |\u03bbi \u2212 \u03bbj | ,\nholds for sgn(\u03bbi \u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj) and \u03bbi 6= \u03bbj .\nThe above stands out from standard eigenspace perturbation results, such as the sin(\u0398) Theorem (Davis & Kahan, 1970) and its variants (Huang et al., 2009; Hunter & Strohmer, 2010; Yu et al., 2015) for three main reasons:\nFirst, Theorem 3.2 characterizes the angle between any pair of eigenvectors allowing us to jointly bound any linear combination of inner-products. Though this often proves handy (c.f. Section 5), it is infeasible using sin(\u0398)-type arguments. Second, classical bounds are not appropriate for a probabilistic analysis as they feature ratios of dependent random variables (corresponding to perturbation terms). In the analysis of spectral clustering, this complication was dealt with by assuming that |\u03bbi \u2212 \u03bbj | \u2264 |\u03bb\u0303i \u2212 \u03bbj | (Hunter & Strohmer, 2010). We weaken this condition at a cost of a multiplicative factor. In contrast to previous work, we also prove that the condition is met a.a.s. Third, previous bounds are expressed in terms of the minimal eigenvalue gap between eigenvectors lying in the interior and exterior of the subspace of interest. This is a limiting factor in practice as it renders the results only amenable to situations where there is a very large eigenvalue gap separating the subspaces. The proposed result improves upon this by considering every eigenvalue difference."}, {"heading": "2.2 Concentration of measure", "text": "The second part of our analysis focuses on the covariance and has a statistical flavor. In particular, we give an answer to Problem 1 for various families of distributions.\nIn the context of distributions with finite second moment, we prove in Section 4.1 that:\nTheorem 4.1. For any two eigenvectors u\u0303i and uj of the sample and actual covariance respectively, and for any real number t > 0:\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 1\nm ( 2kj t |\u03bbi \u2212 \u03bbj | )2 , (3)\nsubject to the same conditions as Theorem 3.2.\nFor eigenvalues, we have the following corollary:\nCorollary 2.1. For any eigenvalues \u03bbi and \u03bb\u0303i of C and C\u0303, respectively, and for any t > 0, we have\nP\n( |\u03bb\u0303i \u2212 \u03bbi|\n\u03bbi \u2265 t ) \u2264 1 m ( ki \u03bbi t )2 .\nTerm kj = (E [ \u2016xx\u2217uj\u201622 ] \u2212 \u03bb2j )1/2 captures the tendency of the distribution to fall in the span of uj : the smaller the tail in the direction of uj the less likely we are going to confuse u\u0303i with uj .\nFor normal distributions, we have that k2j = \u03bb 2 j + \u03bbj tr(C) and the number of samples needed for |\u3008u\u0303i, uj\u3009| to be small is m = O(tr(C)/\u03bb2i ) when \u03bbj = O(1) and m = O(\u03bb \u22122 i ) when \u03bbj = O(tr(C)\u22121). Thus for normal distributions, principal components ui and uj with min{\u03bbi/\u03bbj , \u03bbi} = \u2126(tr(C)1/2) can be distinguished given a constant number of samples. On the other hand, estimating \u03bbi with small relative error requires m = O(tr(C)/\u03bbi) samples and can thus be achieved from very few samples when \u03bbi is large1.\nIn Section 4.2, we also give a sharp bound for the family of distributions supported within a ball (i.e., \u2016x\u2016 \u2264 r a.s.).\nTheorem 4.2. For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c s.t. for any real number t > 0,\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 exp ( 1\u2212 cm\u03a6ij(t) 2\n\u03bbj \u2016x\u20162\u03a82\n) , (4)\nwhere \u03a6ij(t) = |\u03bbi\u2212\u03bbj | t\u22122\u03bbj 2 (r2/\u03bbj\u22121)1/2\n\u22122 \u2016x\u2016\u03a82 and subject to the same conditions as Theorem 3.2.\nAbove, \u2016x\u2016\u03a82 is the sub-gaussian norm, for which we usually have \u2016x\u2016\u03a82 = O(1) (Vershynin, 2010). As such, the theorem implies that, whenever \u03bbi \u03bbj = O(1), the sample requirement is with high probability m = O(r2/\u03bb2i ).\nThese theorems solidify our experimental findings shown in Figure 1 and provide a concrete characterization of the relation between the spectrum of the sample and actual covariance matrix as a function of the number of samples, the eigenvalue gap, and the distribution properties. As exemplified in Section 5 for linear dimensionality reduction, we believe that our results carry strong implications for the non-asymptotic analysis of PCA-based methods."}, {"heading": "3 Perturbation Arguments", "text": "Before focusing on the sample covariance matrix, it helps to study \u3008u\u0303i, uj\u3009 in the setting of Hermitian matrices. The presentation of the results is split in three parts. Section 3.1 starts by studying some basic properties of inner products of the form \u3008u\u0303i, uj\u3009, for any i and j. The results are used in Section 3.2 to provide a first bound on the angle between two eigenvectors, and refined in Section 3.3."}, {"heading": "3.1 Basic observations", "text": "We start by noticing an exact relation between the angle of a perturbed eigenvector and the actual eigenvectors of C.\nLemma 3.1. For every i and j in 1, 2, . . . , n, the relation (\u03bb\u0303i \u2212 \u03bbj) (u\u0303\u2217i uj) = \u2211n `=1(u\u0303 \u2217 i u`) (u \u2217 j\u03b4Cu`) holds .\n1Though the same cannot be stated about the absolute error |\u03b4\u03bbi|, that is smaller for small \u03bbi.\nProof. The proof follows from a modification of a standard argument in perturbation theory. We start from the definition C\u0303 u\u0303i = \u03bb\u0303i u\u0303i and write\n(C + \u03b4C) (ui + \u03b4ui) = (\u03bbi + \u03b4\u03bbi) (ui + \u03b4ui). (5)\nExpanded, the above expression becomes\nC\u03b4ui + \u03b4Cui + \u03b4C\u03b4ui\n= \u03bbi\u03b4ui + \u03b4\u03bbiui + \u03b4\u03bbi\u03b4ui, (6)\nwhere we used the fact that Cui = \u03bbiui to eliminate two terms. To proceed, we substitute \u03b4ui = \u2211n j=1 \u03b2ijuj , where \u03b2ij = \u03b4u\u2217i uj , into (6) and multiply from the left by u\u2217j , resulting to:\nn\u2211 `=1 \u03b2iju \u2217 jCu` + u \u2217 j\u03b4Cui + n\u2211 `=1 \u03b2iju \u2217 j\u03b4Cu`\n= \u03bbi n\u2211 `=1 \u03b2iju \u2217 ju` + \u03b4\u03bbiu \u2217 jui + \u03b4\u03bbi n\u2211 `=1 \u03b2iju \u2217 ju` (7)\nCancelling the unnecessary terms and rearranging, we have\n\u03b4\u03bbiu \u2217 jui + (\u03bbi + \u03b4\u03bbi \u2212 \u03bbj)\u03b2ij\n= u\u2217j\u03b4Cui + n\u2211 `=1 \u03b2iju \u2217 j\u03b4Cu`. (8)\nAt this point, we note that (\u03bbi + \u03b4\u03bbi \u2212 \u03bbj) = \u03bb\u0303i \u2212 \u03bbj and furthermore that \u03b2ij = u\u0303\u2217i uj \u2212 u\u2217i uj . With this in place, equation (8) becomes\n\u03b4\u03bbiu \u2217 jui + (\u03bb\u0303i \u2212 \u03bbj) (u\u0303\u2217i uj \u2212 u\u2217i uj)\n= u\u2217j\u03b4Cui + n\u2211 `=1 (u\u0303\u2217i u`)u \u2217 j\u03b4Cu` \u2212 u\u2217j\u03b4Cui. (9)\nThe proof completes by noticing that, in the left hand side, all terms other than (\u03bb\u0303i \u2212 \u03bbj) u\u0303\u2217i uj fall-off, either due to u\u2217i uj = 0, when i 6= j, or because \u03b4\u03bbi = \u03bb\u0303i\u2212\u03bbj , o.w.\nAs the expression reveals, \u3008u\u0303i, uj\u3009 depends on the orientation of u\u0303i with respect to all other u`. Moreover, the angles between eigenvectors depend not only on the minimal gap between the subspace of interest and its complement space (as in the sin(\u0398) theorem), but on every difference \u03bb\u0303i\u2212\u03bbj . This is a crucial ingredient to a tight bound, that will be retained throughout our analysis."}, {"heading": "3.2 Bounding arbitrary angles", "text": "We proceed to decouple the inner products.\nTheorem 3.1. For any Hermitian matrices C and C\u0303 = \u03b4C +C, with eigenvectors uj and u\u0303i respectively, we have that |\u03bb\u0303i \u2212 \u03bbj | |\u3008u\u0303i, uj\u3009| \u2264 \u2016\u03b4C uj\u20162.\nProof. We rewrite Lemma 3.1 as\n(\u03bb\u0303i \u2212 \u03bbj)2(u\u0303\u2217i uj)2 = ( n\u2211 `=1 (u\u0303\u2217i u`) (u \u2217 j\u03b4Cu`) )2 . (10)\nWe now use the Cauchy-Schwartz inequality\n(\u03bb\u0303i \u2212 \u03bbj)2(u\u0303\u2217i uj)2 \u2264 n\u2211 `=1 (u\u0303\u2217i u`) 2 n\u2211 `=1 (u\u2217j\u03b4Cu`) 2\n= n\u2211 `=1 (u\u2217j\u03b4Cu`) 2 = \u2016\u03b4C uj\u201622, (11)\nwhere in the last step we exploited Lemma 3.2. The proof concludes by taking a square root at both sides of the inequality. Lemma 3.2. n\u2211\u0300 =1 (u\u2217j\u03b4Cu`) 2 = \u2016\u03b4C uj\u201622. Proof. We first notice that u\u2217j\u03b4Cu` is a scalar and equal to its transpose. Moreover, \u03b4C is Hermitian as the difference of two Hermitian matrices. We therefore have that n\u2211 `=1 (u\u2217j\u03b4Cu`) 2 = n\u2211 `=1 u\u2217j\u03b4Cu`u \u2217 `\u03b4Cuj\n= u\u2217j\u03b4C n\u2211 `=1 (u`u \u2217 ` )\u03b4Cuj = u \u2217 j\u03b4C\u03b4Cuj = \u2016\u03b4Cuj\u201622,\nmatching our claim."}, {"heading": "3.3 Refinement", "text": "As a last step, we move all perturbation terms to the numerator, at the expense of a multiplicative constant factor.\nTheorem 3.2. For Hermitian matricesC and C\u0303 = \u03b4C+C, with eigenvectors uj and u\u0303i respectively, the inequality\n|\u3008u\u0303i, uj\u3009| \u2264 2 \u2016\u03b4Cuj\u20162 |\u03bbi \u2212 \u03bbj | ,\nholds for sgn(\u03bbi \u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj) and \u03bbi 6= \u03bbj .\nProof. Adding and subtracting \u03bbi from the left side of the expression in Lemma 3.1 and from definition we have\n(\u03b4\u03bbi + \u03bbi \u2212 \u03bbj) (u\u0303\u2217i uj) = n\u2211 `=1 (u\u0303\u2217i u`) (u \u2217 j\u03b4Cu`). (12)\nFor \u03bbi 6= \u03bbj , the above expression can be re-written as\n|u\u0303\u2217i uj | =\n\u2223\u2223\u2223\u2223 n\u2211\u0300 =1 (u\u0303\u2217i u`) (u \u2217 j\u03b4Cu`)\u2212 \u03b4\u03bbi (u\u0303\u2217i uj) \u2223\u2223\u2223\u2223 |\u03bbi \u2212 \u03bbj |\n\u2264 2 max  \u2223\u2223\u2223\u2223 n\u2211\u0300 =1 (u\u0303\u2217i u`) (u \u2217 j\u03b4Cu`) \u2223\u2223\u2223\u2223 |\u03bbi \u2212 \u03bbj | , |\u03b4\u03bbi| |u\u0303\u2217i uj | |\u03bbi \u2212 \u03bbj |  . (13)\nLet us examine the right-hand side inequality carefully. Obviously, when the condition |\u03bbi \u2212 \u03bbj | \u2264 2 |\u03b4\u03bbi| is not met, the right clause of (13) is irrelevant. Therefore, for |\u03b4\u03bbi| < |\u03bbi \u2212 \u03bbj | /2 the bound simplifies to\n|u\u0303\u2217i uj | \u2264 2\n\u2223\u2223\u2223\u2223 n\u2211\u0300 =1 (u\u0303\u2217i u`) (u \u2217 j\u03b4Cu`) \u2223\u2223\u2223\u2223 |\u03bbi \u2212 \u03bbj | . (14)\nSimilar to the proof of Theorem 3.1, applying the CauchySchwartz inequality we have that\n|u\u0303\u2217i uj | \u2264 2\n\u221a n\u2211\u0300 =1 (u\u0303\u2217i u`) 2 n\u2211\u0300 =1 (u\u2217j\u03b4Cu`) 2\n|\u03bbi \u2212 \u03bbj | = 2 \u2016\u03b4Cuj\u20162 |\u03bbi \u2212 \u03bbj | ,\n(15)\nwhere in the last step we used Lemma 3.2. To finish the proof we notice that, due to Theorem 3.2, whenever |\u03bbi \u2212 \u03bbj | \u2264 |\u03bb\u0303i \u2212 \u03bbj |, one has\n|u\u0303\u2217i uj | \u2264 \u2016\u03b4C uj\u20162 |\u03bb\u0303i \u2212 \u03bbj | \u2264 \u2016\u03b4C uj\u20162 |\u03bbi \u2212 \u03bbj | < 2 \u2016\u03b4Cuj\u20162 |\u03bbi \u2212 \u03bbj | . (16)\nOur bound therefore holds for the union of intervals |\u03b4\u03bbi| < |\u03bbi \u2212 \u03bbj | /2 and |\u03bbi\u2212 \u03bbj | \u2264 |\u03bb\u0303i\u2212 \u03bbj |, i.e., for \u03bb\u0303i > (\u03bbi + \u03bbj)/2 when \u03bbi > \u03bbj and for \u03bb\u0303i < (\u03bbi + \u03bbj)/2 when \u03bbi < \u03bbj ."}, {"heading": "4 Concentration of Measure", "text": "This section builds on the perturbation results of Section 3 to characterize how far any inner product \u3008u\u0303i, uj\u3009 and eigenvalue \u03bb\u0303i are from the ideal estimates.\nBefore proceeding, we remark on some simplifications employed in the following. W.l.o.g., we will assume that the mean E[x] is zero. In addition, we will assume the perspective of Theorem 3.2, for which the inequality sgn(\u03bbi\u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi\u2212\u03bbj)(\u03bbi+\u03bbj) holds. This event is shown to occur a.a.s. when the gap and the sample size are sufficiently large, but it is convenient to assume that it happens almost surely. In fact, removing this assumption is possible (see Section 4.1.2), but it is largely not pursued here as it leads to less elegant and sharp estimates."}, {"heading": "4.1 Distributions with finite second moment", "text": "Our first flavor of results is based on a variant of the Tchebichef inequality and holds for any distribution with finite second moment, though only with moderate probability estimates."}, {"heading": "4.1.1 CONCENTRATION OF EIGENVECTOR ANGLES", "text": "We start with the concentration of inner-products |\u3008u\u0303i, uj\u3009|.\nTheorem 4.1. For any two eigenvectors u\u0303i and uj of the sample and actual covariance respectively, with \u03bbi 6= \u03bbj , and for any real number t > 0, we have\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 1\nm ( 2 kj t |\u03bbi \u2212 \u03bbj | )2 for sgn(\u03bbi \u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj) and kj =( E [ \u2016xx\u2217uj\u201622 ] \u2212 \u03bb2j )1/2 .\nProof. According to a variant of Tchebichef\u2019s inequality (Sarwate, 2013), for any random variable X and for any real numbers t > 0 and \u03b1:\nP(|X \u2212 \u03b1| \u2265 t) \u2264 Var[X] + (E[X]\u2212 \u03b1) 2\nt2 . (17)\nSetting X = \u3008u\u0303i, uj\u3009 and \u03b1 = 0, we have\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 Var[\u3008u\u0303i, uj\u3009] + E[\u3008u\u0303i, uj\u3009]2\nt2 = E [ \u3008u\u0303i, uj\u30092 ] t2 \u2264 4E [ \u2016\u03b4Cuj\u201622 ] t2(\u03bbi \u2212 \u03bbj)2 , (18)\nwhere the last inequality follows from Theorem 3.2. We continue by expanding \u03b4C using the definition of the eigenvalue decomposition and substituting the expectation.\nE [ \u2016\u03b4Cuj\u201622 ] = E [ \u2016C\u0303uj \u2212 \u03bbjuj\u201622 ] = E [ u\u2217j (C\u0303 \u2212 \u03bbj)(C\u0303 \u2212 \u03bbj)uj\n] = E [ u\u2217j C\u0303 2uj ] + \u03bb2j \u2212 2\u03bbju\u2217jE [ C\u0303 ] uj\n= E [ u\u2217j C\u0303 2uj ] \u2212 \u03bb2j . (19)\nIn addition, E [ u\u2217j C\u0303 2uj ] = m\u2211 p,q=1 u\u2217j E [ (xpx \u2217 p)(xqx \u2217 q) ] m2 uj\n= \u2211 p 6=q u\u2217j E [ xpx \u2217 p ] E [ xqx \u2217 q ] m2 uj + m\u2211 p=1 u\u2217j E [ xpx \u2217 pxpx \u2217 p ] m2 uj\n= m(m\u2212 1)\nm2 \u03bb2j +\n1 m u\u2217jE[xx \u2217xx\u2217]uj\n= (1\u2212 1 m )\u03bb2j + 1 m u\u2217jE[xx \u2217xx\u2217]uj (20)\nand therefore\nE [ \u2016\u03b4Cuj\u201622 ] = (1\u2212 1\nm )\u03bb2j +\n1 m u\u2217jE[xx \u2217xx\u2217]uj \u2212 \u03bb2j\n= u\u2217jE[xx \u2217xx\u2217]uj \u2212 \u03bb2j m\n= E [ \u2016xx\u2217uj\u201622 ] \u2212 \u03bb2j\nm .\nPutting everything together, the claim follows.\nThe following corollary will be very useful when applying our results.\nCorollary 4.1. For any weights wij and real t > 0:\nP \u2211 i 6=j wij\u3008u\u0303i, uj\u30092 > t  \u2264\u2211 i6=j\n4wij k 2 j\nmt (\u03bbi \u2212 \u03bbj)2 ,\nwhere kj = ( E [ \u2016xx\u2217uj\u201622 ] \u2212 \u03bb2j )1/2 and wij 6= 0 when\n\u03bbi 6= \u03bbj and sgn(\u03bbi \u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj).\nProof. We proceed as in the proof of Theorem 4.1:\nP (\u2211 i 6=j wij\u3008u\u0303i, uj\u30092 ) 1 2 > t  \u2264 E [\u2211 i6=j wij\u3008u\u0303i, uj\u30092 ] t2\n\u2264 4 t2 \u2211 i 6=j wij E [ \u2016\u03b4Cuj\u201622 ] (\u03bbi \u2212 \u03bbj)2\n(21)\nThe claim follows by computing E [ \u2016\u03b4Cuj\u201622 ] (as before) and squaring both terms within the probability."}, {"heading": "4.1.2 EIGENVALUE CONCENTRATION", "text": "Though perhaps less sharp than what is currently known (e.g., see (Silverstein & Bai, 1995; Bai & Silverstein, 1998) for the asymptotic setting), it might be interesting to observe that a slight modification of the same argument can be used to characterize the eigenvalue relative difference, and as a consequence the main condition of Theorem 4.1.\nCorollary 4.2. For any eigenvalues \u03bbi and \u03bb\u0303i of C and C\u0303, respectively, and for any t > 0, we have\nP\n( |\u03bb\u0303i \u2212 \u03bbi|\n\u03bbi \u2265 t ) \u2264 1 m ( ki \u03bbi t )2 ,\nwhere ki = (E [ \u2016xx\u2217ui\u201622 ] \u2212 \u03bbi)1/2.\nProof. Directly from the Bauer-Fike theorem (Bauer & Fike, 1960) one sees that\n|\u03b4\u03bbi| \u2264 \u2016C\u0303ui \u2212 \u03bbiui\u20162 = \u2016\u03b4Cui\u20162. (22)\nThe proof is then identical to that of Theorem 4.1.\nUsing this, we find that the eventE = {sgn(\u03bbi\u2212\u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj)} occurs with probability at least\nP(E) \u2265 P ( |\u03bb\u0303i \u2212 \u03bbi| <\n|\u03bbi \u2212 \u03bbj | 2\n) > 1\u2212 2k 2 i\nm|\u03bbi \u2212 \u03bbj | .\nTherefore, one eliminates the condition from Theorem 4.1\u2019s statement by relaxing the bound to\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 P(|\u3008u\u0303i, uj\u3009| \u2265 t |E) + (1\u2212P(E))\n< 2\nm|\u03bbi \u2212 \u03bbj | ( 2k2j t2|\u03bbi \u2212 \u03bbj | + k2i ) . (23)"}, {"heading": "4.1.3 THE INFLUENCE OF THE DISTRIBUTION", "text": "As seen by the straightforward inequality E [ \u2016xx\u2217uj\u201622 ] \u2264\nE [ \u2016x\u201642 ] , kj connects to the kurtosis of the distribution. However, it also captures the tendency of the distribution to fall in the span of uj .\nTo see this, we will work with the whitened random vectors \u03b5 = C+1/2x, where C+ denotes the Moore\u2013Penrose pseudoinverse of C. In particular,\nk2j = E [ u\u2217jC 1/2\u03b5\u03b5\u2217C\u03b5\u03b5\u2217C1/2uj ] \u2212 \u03bb2j\n= \u03bbj(E [ \u2016\u039b1/2U\u2217\u03b5\u03b5\u2217uj\u201622 ] \u2212 \u03bbj)\n= \u03bbj ( n\u2211 `=1 \u03bb`E [ \u03b5\u0302(`)2\u03b5\u0302(j)2 ] \u2212 \u03bbj ) , (24)\nwhere \u03b5\u0302 = U\u2217\u03b5. It is therefore easier to untangle the spaces spanned by u\u0303i and uj when the variance of the distribution along the latter space is small (the expression is trivially minimized when \u03bbj \u2192 0) or when the variance is entirely contained along that space (the expression is also small when \u03bbi = 0 for all i 6= j). In addition, it can be seen that distributions with fast decaying tails allow for better principal component identification (E [ \u03b5\u0302(j)4 ] is a measure of kurtosis over the direction of uj).\nFor the particular case of a Normal distribution, we provide a closed-form expression.\nCorollary 4.3. For a Normal distribution, we have k2j = \u03bbj (\u03bbj + tr(C)).\nProof. For a centered and normal distribution with identity covariance, the choice of basis is arbitrary and the vector \u03b5\u0302 = U\u2217\u03b5 is also zero mean with identity covariance. Moreover, for every ` 6= j we can write E [ \u03b5\u0302(`)2\u03b5\u0302(j)2 ] =\nE [ \u03b5\u0302(`)2 ] E [ \u03b5\u0302(j)2 ] = 1. This implies that\nE [ \u2016xx\u2217uj\u201622 ] = \u03bb2j E [ \u03b5\u0302(j)4 ] + \u03bbj n\u2211 ` 6=j \u03bb`\n= \u03bb2j (3\u2212 1) + \u03bbj tr(C) = 2\u03bb2j + \u03bbj tr(C) (25)\nand, accordingly, k2j = \u03bbj (\u03bbj + tr(C))."}, {"heading": "4.2 Distributions supported in a Euclidean ball", "text": "Our last result provides a sharper probability estimate for the family of sub-gaussian distributions supported in a centered Euclidean ball of radius r, with their \u03a82-norm\n\u2016x\u2016\u03a82 = sup y\u2208Sn\u22121 \u2016\u3008x, y\u3009\u2016\u03c82 , (26)\nwhere Sn\u22121 is the unit sphere and with the \u03c82-norm of a random variable X defined as\n\u2016X\u2016\u03c82 = sup p\u22651\np\u22121/2E[|X|p]1/p . (27)\nOur setting is therefore similar to the one used to study covariance estimation (Vershynin, 2012). Due to space constraints, we refer the reader to the excellent review article (Vershynin, 2010) for an introduction to sub-gaussian distributions as a tool for non-asymptotic analysis of random matrices.\nTheorem 4.2. For sub-gaussian distributions supported within a centered Euclidean ball of radius r, there exists an absolute constant c, independent of the sample size, such that for any real number t > 0,\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 exp ( 1\u2212 cm\u03a6ij(t) 2\n\u03bbj \u2016x\u20162\u03a82\n) , (28)\nwhere \u03a6ij(t) = |\u03bbi\u2212\u03bbj | t\u22122\u03bbj 2 (r2/\u03bbj\u22121)1/2\n\u2212 2 \u2016x\u2016\u03a82 , \u03bbi 6= \u03bbj and sgn(\u03bbi \u2212 \u03bbj) 2\u03bb\u0303i > sgn(\u03bbi \u2212 \u03bbj)(\u03bbi + \u03bbj).\nProof. We start from the simple observation that, for every upper bound B of |\u3008u\u0303i, uj\u3009| the relation P(|\u3008u\u0303i, uj\u3009| > t) \u2264 P(B > t) holds. To proceed therefore we will construct a bound with a known tail. As we saw in Sections 3.3 and 4.1,\n|\u3008u\u0303i, uj\u3009| \u2264 2 \u2016\u03b4Cuj\u20162 |\u03bbi \u2212 \u03bbj |\n= 2 \u2225\u2225\u2225(1/m)\u2211mp=1(xpx\u2217puj \u2212 \u03bbjuj)\u2225\u2225\u2225 2\n|\u03bbi \u2212 \u03bbj | \u2264 2 \u2211m p=1 \u2225\u2225xpx\u2217puj \u2212 \u03bbjuj\u2225\u22252 m |\u03bbi \u2212 \u03bbj |\n= 2 \u2211m p=1 \u221a (u\u2217jxp)\n2(x\u2217pxp)\u2212 2\u03bbj(u\u2217jxp)2 + \u03bb2j m |\u03bbi \u2212 \u03bbj |\n= 2 \u2211m p=1 \u221a (u\u2217jxp)\n2(\u2016xp\u201622 \u2212 \u03bbj) + \u03bb2j m |\u03bbi \u2212 \u03bbj |\n(29)\nAssuming further that \u2016x\u20162 \u2264 r, and since the numerator is minimized when \u2016xp\u201622 approaches \u03bbj , we can write for every sample x = C1/2\u03b5:\u221a\n(u\u2217jx) 2(\u2016x\u201622 \u2212 \u03bbj) + \u03bb2j \u2264 \u221a (u\u2217jx) 2(r2 \u2212 \u03bbj) + \u03bb2j\n= \u221a \u03bbj(u\u2217j\u03b5) 2(r2 \u2212 \u03bbj) + \u03bb2j\n\u2264 |u\u2217j\u03b5| \u221a \u03bbjr2 \u2212 \u03bb2j + \u03bbj , (30)\nwhich is a shifted and scaled version of the random variable |\u03b5\u0302(j)| = |u\u2217j\u03b5|. Setting a = (\u03bbjr2 \u2212 \u03bb2j )1/2, we have\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 P ( 2 \u2211m p=1(|\u03b5\u0302p(j)| a+ \u03bbj) m |\u03bbi \u2212 \u03bbj | \u2265 t )\n= P ( m\u2211 p=1 (|\u03b5\u0302p(j)| a+ \u03bbj) \u2265 0.5mt |\u03bbi \u2212 \u03bbj | )\n= P ( m\u2211 p=1 |\u03b5\u0302p(j)| \u2265 m (0.5 t |\u03bbi \u2212 \u03bbj | \u2212 \u03bbj) a ) . (31)\nBy Lemma 4.1 however, the left hand side is a sum of independent sub-gaussian variables. Since the summands are not centered, we expand each |\u03b5\u0302p(j)| = zp + E[|\u03b5\u0302p(j)|] in terms of a centered sub-gaussian zp with the same \u03c82norm. Furthermore, by Jensen\u2019s inequality and Lemma 4.1\nE[|\u03b5\u0302p(j)|] \u2264 E [ \u03b5\u0302p(j) 2 ]1/2 \u2264 2\n\u03bbj \u2016x\u2016\u03a82 . (32)\nTherefore, if we set \u03a6ij(t) = (0.5 |\u03bbi\u2212\u03bbj | t\u2212\u03bbj)\n(r2/\u03bbj\u22121)1/2 \u2212 2 \u2016x\u2016\u03a82\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 P ( m\u2211 p=1 zp \u2265 m\u03a6ij(t) \u03bbj ) . (33)\nMoreover, by the rotation invariance principle, the left hand side of the last inequality is a sub-gaussian with \u03c82-norm smaller than (c1 \u2211m p=1 \u2016zp\u2016 2 \u03c82 )1/2 = (c1m) 1/2 \u2016z\u2016\u03c82 \u2264 (c1m/\u03bbj) 1/2 \u2016x\u2016\u03a82 , for some absolute constant c1. As a consequence, there exists an absolute constant c2, such that for each \u03b8 > 0:\nP (\u2223\u2223\u2223\u2223\u2223 m\u2211 p=1 zp \u2223\u2223\u2223\u2223\u2223 \u2265 \u03b8 ) \u2264 exp ( 1\u2212 c2 \u03b8 2\u03bbj m \u2016x\u20162\u03a82 ) . (34)\nSubstituting \u03b8 = m\u03a6ij(t)/\u03bbj , we have\nP(|\u3008u\u0303i, uj\u3009| \u2265 t) \u2264 exp ( 1\u2212 c2m 2 \u03a6ij(t) 2\u03bbj\nm\u03bb2j \u2016x\u2016 2 \u03a82\n)\n= exp ( 1\u2212 c2m\u03a6ij(t) 2\n\u03bbj \u2016x\u20162\u03a82\n) , (35)\nwhich is the desired bound.\nLemma 4.1. If x is a sub-gaussian random vector and \u03b5 = C+1/2x, then for every i, the random variable \u03b5\u0302(i) = u\u2217i \u03b5 is also sub-gaussian, with \u2016\u03b5\u0302(i)\u2016\u03c82 \u2264 \u2016x\u2016\u03a82 / \u221a \u03bbi.\nProof. Notice that\n\u2016x\u2016\u03a82 = sup y\u2208Sn\u22121 \u2016\u3008x, y\u3009\u2016\u03c82= sup y\u2208Sn\u22121 \u2225\u2225\u2225\u2225\u2225\u2225 n\u2211 j=1 \u03bb 1/2 j (u \u2217 jy)(u \u2217 j\u03b5) \u2225\u2225\u2225\u2225\u2225\u2225 \u03c82\n\u2265 \u2225\u2225\u2225\u2225\u2225\u2225 n\u2211 j=1 \u03bb 1/2 j (u \u2217 jui)\u03b5\u0302(j) \u2225\u2225\u2225\u2225\u2225\u2225 \u03c82 = \u03bb 1/2 i \u2016\u03b5\u0302(i)\u2016\u03c82 , (36)\nwhere, for the last inequality, we set y = ui."}, {"heading": "5 Application to Dimensionality Reduction", "text": "To emphasize the utility of our results, in the following we consider the practical example of linear dimensionality reduction. We show that a direct application of our bounds leads to upper estimates on the sample requirement.\nIn terms of mean squared error, the optimal way to reduce the dimension of a sample x of a distribution is by projecting it over the subspace of the covariance with maximum variance. Denote by Ik the diagonal matrix with the first k diagonal entries equal to one and the rest zero. When the actual covariance is known, the expected energy loss induced by the Pkx = IkU\u2217x projection is\nloss(Pk) = E [ \u2016x\u201622 \u2212 \u2016Pkx\u201622 ] E[\u2016x\u201622] = \u2211 i>k \u03bbi tr(C) . (37)\nHowever, when the projector P\u0303k = IkU\u0303\u2217 is constructed from the sample covariance, we have\nloss(P\u0303k) = E [ \u2016x\u201622 \u2212 \u2016P\u0303kx\u201622 ] E[\u2016x\u201622]\n=\n\u2211n i=1 \u03bbi \u2212 tr(IkU\u0303\u2217U\u039bU\u2217U\u0303)\ntr(C)\n=\n\u2211n i=1 \u03bbi \u2212 \u2211 i\u2264k,j(u\u0303 \u2217 i uj) 2\u03bbj\ntr(C) (38)\nwith the expectation taken over the to-be-projected vectors x, but not the samples used to estimate the covariance. After slight manipulation, one finds that\nloss(P\u0303k) = loss(Pk) +\n\u2211 i\u2264k,j 6=i (u\u0303\u2217i uj) 2(\u03bbi \u2212 \u03bbj)\ntr(C) . (39)\nThe loss difference has an intuitive interpretation: when reducing the dimension with P\u0303k one looses either by discarding useful energy (terms j > k), or by displacing kept components within the permissible eigenspace (terms j \u2264 k). Note also that all terms with j < i are negative and can be excluded from the sum if we are satisfied we an upper estimate2.\nIt is an implication of (39) and Corollary 4.1 that, when its conditions hold, for any distribution and t > 0\nP ( loss(P\u0303k) > loss(Pk) + t\ntr(C) ) \u2264 \u2211 i\u2264k j>i 4 k2j mt |\u03bbi \u2212 \u03bbj | .\nObserve that the loss difference becomes particularly small whenever k is small: (i) the terms in the sum are fewer and (ii) the magnitude of each term decreases (due to |\u03bbi\u2212\u03bbj |).\n2A similar approach could also be utilized to derive a lower bound of the quantity loss(P\u0303k)\u2212 loss(Pk).\nThis phenomenon is also numerically verified in Figure 2 for the distribution of the images featuring digit \u20183\u2019 in MNIST (total 6131 images with n = 784 pixels each). The figure depicts for different k how many samples are required such that the loss difference is smaller than a tolerance threshold, here 0.02, 0.05, and 0.1. Each point in the figure corresponds to an average over 10 sampling draws. The trends featured in these numerical results agree with our theoretical intuition. Moreover they illustrate that for modest k the sample requirement is far smaller than n.\nIt is also interesting to observe that for covariance matrices that are (approximately) low-rank, we obtain estimates reminiscent of compressed sensing (Cande\u0300s et al., 2011), in the sense that the sample requirement becomes a function of the non-zero eigenvalues. Though intuitive, with the exception of (Koltchinskii et al., 2016), this dependency of the estimation accuracy on the rank was not transparent in known results for covariance estimation (Rudelson, 1999; Adamczak et al., 2010; Vershynin, 2012)."}, {"heading": "6 Conclusions", "text": "The main contribution of this paper was the derivation of non-asymptotic bounds for the concentration of innerproducts |\u3008u\u0303i, uj\u3009| involving eigenvectors of the sample and actual covariance matrices. We also showed how these results can be extended to reason about eigenvalues and we applied them to the non-asymptotic analysis of linear dimensionality reduction.\nWe have identified two interesting directions for further research. The first has to do with obtaining tighter estimates. Especially with regards to our perturbation arguments, we believe that our current bounds on inner products could be sharpened by at least a constant multiplicative factor. The second direction involves using our results for the analysis of methods that utilize the eigenvectors of the covariance, such that principal component projection and regression (Jolliffe, 1982; Frostig et al., 2016)."}], "year": 2017, "references": [{"title": "Quantitative estimates of the convergence of the empirical covariance matrix in log-concave ensembles", "authors": ["Adamczak", "Rados\u0142aw", "Litvak", "Alexander", "Pajor", "Alain", "Tomczak-Jaegermann", "Nicole"], "venue": "Journal of the American Mathematical Society,", "year": 2010}, {"title": "Large-sample estimation strategies for eigenvalues of a wishart matrix. Metrika", "authors": ["Ahmed", "SE"], "year": 1998}, {"title": "Asymptotic theory for principal component analysis", "authors": ["Anderson", "Theodore Wilbur"], "venue": "The Annals of Mathematical Statistics,", "year": 1963}, {"title": "Methodologies in spectral analysis of large dimensional random matrices, a review", "authors": ["Bai", "ZD"], "venue": "Statistica Sinica, pp", "year": 1999}, {"title": "Limit of the smallest eigenvalue of a large dimensional sample covariance matrix", "authors": ["ZD Bai", "Yin", "YQ"], "venue": "The annals of Probability,", "year": 1993}, {"title": "On asymptotics of eigenvectors of large sample covariance matrix", "authors": ["ZD Bai", "BQ Miao", "GM Pan"], "venue": "The Annals of Probability,", "year": 2007}, {"title": "No eigenvalues outside the support of the limiting spectral distribution of large-dimensional sample covariance matrices", "authors": ["Bai", "Zhi-Dong", "Silverstein", "Jack W"], "venue": "Annals of probability,", "year": 1998}, {"title": "Norms and exclusion theorems", "authors": ["Bauer", "Friedrich L", "Fike", "Charles T"], "venue": "Numerische Mathematik,", "year": 1960}, {"title": "Computation of surface geometry and segmentation using covariance techniques", "authors": ["Berkmann", "Jens", "Caelli", "Terry"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 1994}, {"title": "Robust principal component analysis", "authors": ["Cand\u00e8s", "Emmanuel J", "Li", "Xiaodong", "Ma", "Yi", "Wright", "John"], "venue": "Journal of the ACM,", "year": 2011}, {"title": "The rotation of eigenvectors by a perturbation", "authors": ["Davis", "Chandler", "Kahan", "William Morton"], "venue": "III. SIAM Journal on Numerical Analysis,", "year": 1970}, {"title": "Principal component projection without principal component analysis", "authors": ["Frostig", "Roy", "Musco", "Cameron", "Christopher", "Sidford", "Aaron"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Strong law for the eigenvalues and eigenvectors of empirical covariance matrices", "authors": ["V. Girko"], "year": 1996}, {"title": "Spectral clustering with perturbed data", "authors": ["Huang", "Ling", "Yan", "Donghui", "Taft", "Nina", "Jordan", "Michael I"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2009}, {"title": "Performance analysis of spectral clustering on compressed, incomplete and inaccurate measurements", "authors": ["Hunter", "Blake", "Strohmer", "Thomas"], "venue": "arXiv preprint arXiv:1011.0997,", "year": 2010}, {"title": "Principal component analysis", "authors": ["Jolliffe", "Ian"], "venue": "Wiley Online Library,", "year": 2002}, {"title": "A note on the use of principal components in regression", "authors": ["Jolliffe", "Ian T"], "venue": "Applied Statistics, pp", "year": 1982}, {"title": "Dimension reduction by local principal component analysis", "authors": ["Kambhatla", "Nandakishore", "Leen", "Todd K"], "venue": "Neural computation,", "year": 1997}, {"title": "Normal approximation and concentration of spectral projectors of sample covariance", "authors": ["Koltchinskii", "Vladimir", "Lounici", "Karim"], "venue": "arXiv preprint arXiv:1504.07333,", "year": 2015}, {"title": "Asymptotics and concentration bounds for bilinear forms of spectral projectors of sample covariance", "authors": ["Koltchinskii", "Vladimir", "Lounici", "Karim"], "venue": "In Annales de l\u2019Institut Henri Poincare\u0301, Probabilite\u0301s et Statistiques,", "year": 2016}, {"title": "Improved estimation of eigenvalues and eigenvectors of covariance matrices using their sample estimates", "authors": ["Mestre", "Xavier"], "venue": "IEEE Transactions on Information Theory,", "year": 2008}, {"title": "Random vectors in the isotropic position", "authors": ["Rudelson", "Mark"], "venue": "Journal of Functional Analysis,", "year": 1999}, {"title": "Asymptotics of eigenprojections of correlation matrices with some applications in principal components analysis", "authors": ["Schott", "James R"], "year": 1997}, {"title": "Subspace leakage analysis of sample data covariance matrix", "authors": ["Shaghaghi", "Mahdi", "Vorobyov", "Sergiy A"], "venue": "In ICASSP,", "year": 2015}, {"title": "On the empirical distribution of eigenvalues of a class of large dimensional random matrices", "authors": ["Silverstein", "Jack W", "Bai", "ZD"], "venue": "Journal of Multivariate analysis,", "year": 1995}, {"title": "Introduction to the non-asymptotic analysis of random matrices", "authors": ["Vershynin", "Roman"], "year": 2010}, {"title": "How close is the sample covariance matrix to the actual covariance matrix", "authors": ["Vershynin", "Roman"], "venue": "Journal of Theoretical Probability,", "year": 2012}, {"title": "A useful variant of the davis\u2013kahan theorem for statisticians", "authors": ["Yu", "Yi", "Wang", "Tengyao", "Samworth", "Richard J"], "year": 2015}], "id": "SP:cfbb096d2439a5de9e8bfa2809ced2a78fd783db", "authors": [{"name": "Andreas Loukas", "affiliations": []}], "abstractText": "How many samples are sufficient to guarantee that the eigenvectors of the sample covariance matrix are close to those of the actual covariance matrix? For a wide family of distributions, including distributions with finite second moment and sub-gaussian distributions supported in a centered Euclidean ball, we prove that the inner product between eigenvectors of the sample and actual covariance matrices decreases proportionally to the respective eigenvalue distance and the number of samples. Our findings imply non-asymptotic concentration bounds for eigenvectors and eigenvalues and carry strong consequences for the non-asymptotic analysis of PCA and its applications. For instance, they provide conditions for separating components estimated from O(1) samples and show that even few samples can be sufficient to perform dimensionality reduction, especially for low-rank covariances.", "title": "How Close Are the Eigenvectors of the Sample and Actual Covariance Matrices?"}