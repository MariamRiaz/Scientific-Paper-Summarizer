0	43	Current successful methods for automated knowledge base construction tasks heavily rely on learned distributed vector representations (Nickel et al., 2012; Riedel et al., 2013; Socher et al., 2013; Chang et al., 2014; Neelakantan et al., 2015; Toutanova et al., 2015; Nickel et al., 2015; Verga et al., 2016; Verga and McCallum, 2016).
2	109	Such knowledge is rarely explicitly stated in texts but can be found in resources like PPDB (Ganitkevitch et al., 2013) or WordNet (Miller, 1995).
3	72	Combining neural methods with symbolic commonsense knowledge, for instance in the form of implication rules, is in the focus of current research (Rocktäschel et al., 2014; Wang et al., 2014; Bowman et al., 2015; Wang et al., 2015; Vendrov et al., 2016; Hu et al., 2016; Rocktäschel and Riedel, 2016; Cohen, 2016).
5	25	To this end, every first-order rule is propositionalized based on observed entity-tuples, and a differentiable loss term is added for every propositional rule.
7	42	For example, propositionalizing the rule ∀x : isMan(x)⇒ isMortal(x) would result in a very large number of loss terms on a large database.
8	103	In this paper, we present a method to incorporate simple rules while maintaining the computational efficiency of only modeling training facts.
10	74	It only involves representations of the relations that are mentioned in rules, as well as a general rule-independent constraint on the entity-tuple embedding space.
11	84	In the example given above, if we require that every component of the 1389 vector representation of isMan is smaller than the corresponding component of relation isMortal, then we can show that the rule holds for any nonnegative representation of an entity-tuple.
15	68	This allows for imposing large numbers of rules while learning distributed representations of relations and entity-tuples.
16	99	Besides drastically lower computation time, an important advantage of our method over Rocktäschel et al. (2015) is that when these constraints are satisfied, the injected rules always hold, even for unseen but inferred facts.
19	31	In this section we revisit the matrix factorization relation extraction model by Riedel et al. (2013) and introduce the notation used throughout the paper.
25	23	Model F by Riedel et al. (2013) measures the compatibility between a relation r and an entitytuple t using the dot product r>t of their respective vector representations.
39	33	The implication rule can be imposed by requiring that every tuple t ∈ T is at least as compatible with relation rp as with rq.
41	32	(4) therefore becomes ∀t ∈ T : r>p t ≤ r>q t (5) If 〈rp, t〉 is a true fact with a high score r>p t, and the fact 〈rq, t〉 has an even higher score, it must also be true, but not vice versa.
42	31	We can therefore inject an implication rule by minimizing a loss term with a separate contribution from every t ∈ T , adding up to the total loss if the corresponding inequality is not satisfied.
47	32	Moreover, with that simplification there is no guarantee that the implication between both relations would generalize towards inferred facts not seen during training.
58	51	Note that by minimizing LUI , the model is encouraged to satisfy the constraint rp ≤ rq on the relation embeddings, where ≤ denotes the component-wise comparison.
63	34	We have chosen to restrict the tuple space even more than required, namely to the hypercube t ∈ [0, 1]k, as approximately Boolean embeddings (Kruszewski et al., 2015).
64	26	The tuple embeddings are constructed from real-valued vectors e, using the component-wise sigmoid function t = σ(e), e ∈ Rk.
70	28	With the above choice of a non-negative tupleembedding space we can now state the full lifted rule injection model (FSL): LFSL = LFS + β̃ ∑ I∈I LUI (14) LUI denotes a lifted loss term for every rule in a set I of implication rules that we want to inject.
132	46	Inspecting the top relations for a sampled dimension in the embedding space reveals that the relation space of model FS more closely resembles clusters than that of model F (Table 2).
135	25	We inject the same hand-picked relations as used by Rocktäschel et al. (2015), after removing all Freebase training facts.
140	36	It effictively measures how well the proposed models, matrix factorization (F), propositionalized rule injection (R15-Joint), and our model (FSL), can make use of the provided rules and correlations between textual surface form pat- terns and increased fractions of Freebase training facts.
143	23	We attribute this to the Bayesian personalized ranking loss instead of the logistic loss used in Rocktäschel et al. (2015).
164	45	We measure on average 6.33s per epoch without rules (model FS), against 6.76s and 6.97s when injecting the 36 high-quality WordNet rules and the unfiltered 427 rules (model FSL), respectively.
180	26	For example, purely based on the training data, it appears to be more likely that the parent relation implies the father relation, than vice versa.
182	66	We presented a novel, fast approach for incorporating first-order implication rules into distributed representations of relations.
184	100	By construction, these rules are satisfied for any observed or unobserved fact.
190	94	In future work, we want to extend the proposed ideas beyond implications towards general firstorder logic rules.
191	47	We believe that supporting conjunctions, disjunctions and negations would enable to debug and improve representation learning based knowledge base completion.
