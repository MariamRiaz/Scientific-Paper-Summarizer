2	21	These relationships are often represented by an undirected graphical model also known as a Markov Random Field (MRF).
4	15	GGMs are used in structure-discovery settings for rich data such as neuroimaging, genetics, or finance (Friedman et al., 2008; Ryali et al, 2012; Mohan et al., 2012; Belilovsky et al., 2016).
5	4	Although multivariate Gaussian distributions are well-behaved, determining likely structures from few examples is a difficult task when the data is high dimensional.
6	37	It requires strong priors, typically a sparsity assumption, or other restrictions on the structure of the graph, which now make the distribution difficult to express analytically and use.
7	47	A standard approach to estimating structure with GGMs in high dimensions is based on the classic result that the zeros of a precision matrix correspond to zero partial correlation, a necessary and sufficient condition for conditional independence (Lauritzen, 1996).
9	77	Many popular approaches to learning GGMs can be seen as leveraging the `1-norm to create convex surrogates to this problem.
10	24	Meinshausen & Bühlmann (2006) use nodewise `1 penalized regressions, while other estimators penalize the precision matrix directly (Cai et al., 2011; Friedman et al., 2008; Ravikumar et al., 2011), the most popular being the graphical lasso fgl(Σ̂) = arg min Θ 0 − log |Θ|+Tr (Σ̂Θ) + λ‖Θ‖1 (1) which can be seen as a penalized maximum-likelihood estimator.
11	47	Here Θ and Σ̂ are the precision and sample covariance matrices, respectively.
12	6	A large variety of alternative penalties extend the priors of the graphical lasso (Danaher et al., 2014; Ryali et al, 2012; Varoquaux et al., 2010).
14	44	Constructing novel surrogates for structured-sparsity assumptions on MRF structures is difficult, as priors need to be formulated and incorporated into a penalized maximum likelihood objective which then calls for the development of an efficient optimization algorithm, often within a separate research effort.
17	9	Rather than manually de- signing a specific graph-estimation procedure, we frame this estimator-engineering problem as a learning problem, selecting a function from a large flexible function class by risk minimization.
18	56	This allows us to construct a loss function that explicitly aims to recover the edge structure.
22	7	For particular cases we show that the problem of interest can be solved with a polynomial function, which is learnable with a neural network (Andoni et al., 2014).
23	82	Motivated by this fact, as well as theoretical and empricial results on learning smooth functions approximating solutions to combinatorial problems (Cohen et al., 2016; Vinyals et al., 2015), we propose to use a particular convolutional neural network as the function class.
24	61	We train it by sampling small datasets, generated from graphs with the prescribed properties, with a primary focus on sparse graphical models.
25	79	We estimate from this data small-sample covariance matrices (n < p), where n is the number of samples and p is the dimensionality of the data.
26	52	Then we use them as training data for the neural network (Figure 2) where target labels are indicators of present and absent edges in the underlying GGM.
27	4	The learned network can then be employed in various real-world structure discovery problems.
30	23	Section 2.3 describes and motivates the deep-learning architecture we chose to use for the sparse GGM problem in this work.
32	5	We then evaluate its properties extensively on simulation data.
33	27	Finally, we show that this edge estimator trained only on synthetic data can obtain state of the art performance at inference time on real neuroimaging and genetics problems, while being much faster to execute than other methods.
48	13	We consider MRF edge estimation as a learnable function.
50	11	samples x ∼ P (x) of dimension p. LetG = (V,E) be an undirected and unweighted graph associated with the set of variables in x.
51	16	Let L = {0, 1} and Ne = p(p−1)2 the maximum possible edges in E. Let Y ∈ LNe indicate the presence or absence of edges in the edge set E of G, namely Y ij = { 0 xi ⊥ xj |xV \i,j 1 xi 6⊥ xj |xV \i,j .
60	9	For graphical model selection the 0/1 loss function is the natural error metric to consider (Wang et al., 2010).
62	15	(1) achieves the information theoretic optimal recovery rate up to a constant for certain P corresponding to uniformly sparse graphs with a maximum degree, but only when the optimal λ is used and the non-zero precision matrix values are bounded away from zero (Wang et al., 2010; Ravikumar et al., 2011).
67	26	Desired structural assumptions on samples from P on the underlying graph, such as sparsity, may imply that the distribution is not tractable for analytic solutions.
71	20	We now need to select a sufficiently rich function class for fw and a method to produce appropriate (Y, Σ̂) which model our desired data priors.
74	24	A typical assumption in many modalities is that the number of edges is sparse.
76	11	Additionally, the precision matrix and partial correlation matrix have the same sparsity pattern, while the partial correlation matrix has normalized entries.
