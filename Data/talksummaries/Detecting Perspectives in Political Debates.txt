3	51	Perspectives, that state people’s ideas or the facts known to one, can be contrastive, i.e. to be in favour of or against something (e.g. Brexit vs Bremain), or non-contrastive, i.e. independent discussions that share a common topic (e.g. unemployment and migration in the context of economy).
11	21	Similarly, if ‘Brexit’ is being discussed in terms of leaving or remaining, we want to cluster arguments into those two viewpoints.
15	25	It is also different from cross-perspective topic models which assume the perspectives are observed (Fang et al., 2012).
36	22	Lin and He (2009) introduced a joint sentiment topic (JST) model, which simultaneously extracts topics and topic-associated sentiments from text.
42	19	Das and Lavoie (2014) observed the editions and interactions of a user in Wikipedia pages to infer topics and points of view at the same time.
48	20	Depending on the type of word, we follow a different generative process.
50	44	The argument indicator here is a latent categorical variable.
52	27	More generally, it could also take a value from multiple stance or perspective categories.
61	138	We use Collapsed Gibbs Sampling (Casella and George, 1992) to infer the model parameters and the latent assignments of topics and arguments, given the observed data.
64	75	Letting the index t = (d, n) denote the nth word in document d and the subscript −t denote a quantity that excludes data from the nth word position in document d, Λ = {α, βb, βz, βa, γ, δ}, the conditional posterior for xt is: P (xt = r|x−t, z,a,w,Λ) ∝ {N rd}−t + γ {Nd}−t + 3γ · {N rwt}−t + βr∑ w′{N rw′}−t +Wβr , (1) where r denotes different word types, either background word, topic word or argument word.
65	38	N rd denotes the number of words in document d assigned to the word type r, Nd is the total number of words in the document d, N rwt is the number of times word wt is sampled from the distribution for the word type r, W is the vocabulary size.
66	90	For topic words, the conditional posterior for zt is: P (zt = k|z−t,w,Λ) ∝ N−td,k + αk N−td + ∑ k αk · N −t k,wt + βz N−tk +Wβz , (2) where Nd,k is the number of times topic k was assigned to some word tokens in document d, Nd is the total number of words in document d, Nk,wt is the number of times word wt appeared in topic k. For argument words, the conditional posterior for zt and at is: P (zt = k, at = j|z−t,a−t,w,Λ) ∝ N−td,k + αk N−td + ∑ k αk · N −t d,k,j + δk,j N−td,k + ∑ j δk,j ·N −t k,j,wt + βv N−tk,j +Wβv , (3) where Nd,k,j is the number of times a word from document d has been associated with topic k and argument j, Nk,j,wt is the number of times word wt appeared in topic k and with argument j, and Nk,j is the number of words assigned to topic k and argument j.
76	42	For the first variant, we adopt the similary strategy as in (Fang et al., 2012) that nouns (NOUN) are topic words; adjectives (ADJ), adverbs (ADV) and verbs (VERB) are argument words; words with other POS tags are background words.
84	27	Debates from the UK parliament are archived and available for consulting.2 A custom web-crawler was developed to obtain the records of every day that The House of Commons was in session between 2009 and 2016.
94	21	As topic models suffer from lack of robustness if large outliers are present, we also removed very frequent (above 99%) and rare words (below percentile 65%), assuming that word occurrences of the collection follow a Zip’s law distribution.5 Similar strategy was carried out for texts, in order to just consider texts of similar length.
97	19	The models for comparison are listed below: • LDA.
101	19	Joint Topic-Viewpoint Model (Trabelsi and Zaıane, 2014) is essentially the reparameterized version of the Joint SentimentTopic (JST) model (Lin and He, 2009) called REVERSE-JST (Lin et al., 2012) in which sentiment label (or viewpoint) generation is dependent on topics.
104	20	LAM with topic, argument or background words separated by POS tags.
109	38	Figure 2 plots the CV results8 versus the number of topics on HCD for various models.
126	18	In terms of a quantitative evaluation, we are interested in knowing how strongly the perspectives are related to their topic: it might be the case that the separate CV coherence for the topic and viewpoints is high, but there is no actual relation between them, which would be an undesirable behaviour.
127	21	To determine whether this is happening or not in the studied models, for each perspective we compute a mixed topic-perspective CV value, by extracting the top 5 perspective words, concatenating them with the top 5 words of the corresponding topic and running Palmetto as in the previous section.9 We then average the computed mixed topic-perspective CV values by T ×A.
143	21	Two external annotators were then asked to answer (‘yes’ or ‘no’) for each topic if they could differentiate two perspectives.
145	19	Table 4 shows the results of the evaluation and it is clear that LAM LEX outperforms CPT.
160	56	This shows that LAM LEX can infer perspectives from raw data, but we have little control on guiding the model on what perspectives to extract.
161	46	We have presented LAM, a model able to provide a glimpse of what is going on in political debates, without relying on any labelled data and assuming the perspectives of a topic to be latent.
162	19	It is implemented through a hierarchical Bayesian model considering that words can be separated as topic, argument or background words and follow different generative routes.
165	25	Although LAM can extract perspectives under a certain topic, there is little control in what kind of information to extract (e.g. we might want only contrastive or non-contrastive arguments).
166	25	In future work, we plan to improve the model through complex priors or semantic similarity strategies.
167	29	Also, adding a ‘Bill’ level could be beneficial as speeches about the same Bill should share the same high-level topic.
169	56	Including a ‘speaker’ level to know which parliamentarians discuss which topics is another interesting path to follow.
