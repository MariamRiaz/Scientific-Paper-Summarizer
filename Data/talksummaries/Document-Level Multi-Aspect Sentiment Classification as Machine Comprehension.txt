9	30	Therefore, it is more practically useful to perform document-level multi-aspect sentiment classification task, predicting different ratings for each aspect rather than an overall rating.
10	36	One straightforward approach for documentlevel multi-aspect sentiment classification is multi-task learning (Caruana, 1997).
16	50	To mimic human’s evaluation of aspect classification, we create a list of keywords for each aspect.
17	35	For example, when we work on the Room aspect, we generate some keywords such as “room,” “bed,” “view,” etc.
18	98	Then we can ask pseudo questions: “How is the room?” “How is the bed?” “How is the view?” and provide an answer “Rating 5.” In this case, we can train a machine comprehension model to automatically attend corresponding text snippets in the review document to predict the aspect rating.
19	28	Specifically, we introduce a hierarchical and iterative attention model to construct aspect-specific representations.
20	29	We use a hierarchical architecture to build up different representations at both word and sentence levels interacting with aspect questions.
24	16	To evaluate the effectiveness of the proposed model, we conduct extensive experiments on the TripAdvisor and BeerAdvocate datasets and the results show that our model outperforms typical baselines.
26	25	Moreover, a case study for attention results is performed at both word and sentence levels.
53	20	Then we take sentence representations and k-th aspect as input and apply the sentence-level input encoder and attention model to generate the document representation dk for final classification.
71	35	The iterative attention module (IAM) attends and reads memories of questions and documents alternatively with a multi-hop mechanism, deriving aspect-specific sentence and document representations.
76	37	For each iteration, IAM conducts four operations: (1) attends the question memory by the selective vector p and summarizes question memory vectors into a single vector q̂; (2) updates the selective vector by the previous one and q̂; (3) attends document (content) memory based on the updated selective vector and summarizes memory vectors in to a single vector ĉ; (4) updates the selective vector by the previous one and ĉ.
79	16	The Wa and va are parameters, a is attention weights for memory vectors, and Mi means i-th row in M. Operations (2) and (4) are formulated as an update function p2i−{l} = U(x̂, p2i−{l}−1), where i is the hop index, l can be 0 or 1 which corresponds to x̂ = ĉ or x̂ = q̂ respectively.
87	13	For each aspect, we obtain aspect-specific document representations {dk}1≤k≤K .
122	21	Compared to SVM and SLDA, NBoW achieves higher accuracy by 3% in both datasets, which shows that embedding features are more effective than traditional ngram features on these two datasets.
127	25	Additionally, we observe that the multi-task learning and hierarchical architecture are beneficial for neural networks.
133	23	In this section, we sample two sentences from TripAdvisor to show the visualization of attention results for case study.
134	14	Both word-level and sentence-level attention visualizations are shown in Figure 4.
135	120	We normalize the word weight by the sentence weight to make sure that only important words in a document are highlighted.
136	35	From the top figures in (a) and (b), we observe that our model assigns different attention weights for each aspect.
137	63	For example, in the first sentence, the words comfortable and bed are assigned higher weights in the aspect Room, and the word clean are highlighted by the aspect Cleaniness.
139	49	Moreover, the bottom figures in (a) and (b) show that (1) word weights of different hops are various; (2) attention values in higher hop are more reasonable.
140	19	Specifically, in the first sentence, the weight of word clean is higher than the word comfortable in first hop, while comfortable surpasses clean in higher hops.
146	34	In this experiment, we investigate the effects of hop number m and size of aspect keywords N on performances.
149	26	For the hop number, we vary m from 1 to 7 and the results are shown in Figure 5 (left).
150	55	We can see that: (1) at the word level, the performance increases when m ≤ 4, but shows no improvement after m > 4; (2) at the sentence level, model performs best when m = 2.
153	39	Note that, we set a learnable vector to represent question memory when N = 0.
154	19	The results are shown in Figure 5 (right).
183	21	In this paper, we model the document-level multiaspect sentiment classification as a text comprehension problem and propose a novel hierarchical iterative attention model in which documents and pseudo aspect-questions are interleaved at both word and sentence-level to learn aspect-aware document representation in a unified model.
184	22	Extensive experiments show that our model outperforms the other neural models with multi-task framework and hierarchical architecture.
