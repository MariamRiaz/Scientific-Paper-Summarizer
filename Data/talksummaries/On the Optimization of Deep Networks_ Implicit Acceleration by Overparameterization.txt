22	1	From an optimization perspective, overparameterizing using wide or narrow networks has the same effect – it is only the depth that matters.
79	1	To be able to derive, in our general setting, an explicit update rule for the end-to-end weight matrix We (Equation 5), we introduce an assumption by which the learning rate is small, i.e. η2 ≈ 0.
82	1	The use of differential equations, for both theoretical analysis and algorithm design, has a long and rich history in optimization research (see Helmke & Moore (2012) for an overview).
85	1	With the continuous formulation in place, we turn to express the dynamics of the end-to-end matrix We: Theorem 1.
97	1	It is customary in deep learning for both learning rate and weight initializations to be small, but nonetheless above assumptions are only met to a certain extent.
116	1	A-priori, such a preference may seem peculiar – why should an optimization algorithm be sensitive to its location in parameter space?
118	1	However, if one takes into account the common practice in deep learning of initializing weights near zero, the location in parameter space can also be regarded as the overall movement made by the algorithm.
120	1	This intuitive interpretation will become more concrete in the subsection that follows.
125	1	This corresponds, for example, to a binary (two-class) classification problem, or to the prediction of a numeric scalar property (regression).
126	1	It admits a particularly simple form for the end-to-end update rule of Equation 10: Claim 2.
127	1	Then, the end-toend update rule in Equation 10 can be written as follows: W (t+1)e ← [ (1− ηλN) ·W (t)e (12) −η‖W (t)e ‖ 2− 2N 2 · ( dL1 dW (W (t) e )+ (N − 1) · Pr W (t) e { dL1 dW (W (t) e ) }) where ‖·‖2− 2 N 2 stands for Euclidean norm raised to the power of 2− 2N , and PrW {·}, W ∈ R 1,d, is defined to be the projection operator onto the direction of W : PrW : R1,d → R1,d (13) PrW {V } := { W ‖W‖2 V > · W‖W‖2 , W 6= 0 0 , W = 0 Proof.
134	1	For example, AdaGrad was originally invented to compete with the best regularizer from a particular family.
142	1	Overparametrization changes gradient descent’s behavior: instead of following the original gradient dL 1 dW , it follows some other direction F (·) (see Equations 12 and 14) that 1 For the result to hold with N = 2, additional assumptions on L1(·) are required; otherwise any non-zero linear function L1(W ) = WU> serves as a counter-example – it leads to a vector field F (·) that is the gradient of W 7→ ‖W‖2 ·WU >.
147	1	We show that F (·) contradicts the fundamental theorem for line integrals.
166	1	In Appendix B we consider a special case and formalize this intuition, deriving a concrete bound for the acceleration of overparameterization.
176	1	As training objectives, we tried both `2 and `4 losses.
179	1	Alongside the validity of the end-to-end update rule, Figure 2 also demonstrates the negligible effect of network width on convergence, in accordance with our analysis (see Section 5).
182	1	The computational toll associated with overparameterization will thus be virtually non-existent.
183	1	As a final observation on Figure 2, notice that it exhibits faster convergence with a deeper network.
187	1	As can be seen, convergence of deeper networks is (slightly) slower in the case of `2 loss.
192	1	Figure 4-left shows convergence of a depth-3 network (optimized with gradient descent) against that of a single layer model optimized with AdaGrad (Duchi et al., 2011) and AdaDelta (Zeiler, 2012).
202	1	A possible approach for alleviating this issue is to initialize weights to be larger, yet small enough such that the end-to-end matrix does not “explode”.
203	1	The choice of identity (or near identity) initialization leads to what is known as linear residual networks (Hardt & Ma, 2016), akin to the successful residual networks architecture (He et al., 2015) commonly employed in deep learning.
204	1	Notice that identity initialization satisfies the condition in Equation 7, rendering the end-to-end update rule (Equation 10) applicable.
210	1	We introduced overparameterization by simply placing two matrices in succession instead of the matrix in each dense layer.
219	1	Our analysis of linear neural networks, the subject of various recent studies, yielded a new result: for these models, overparameterization by depth can be understood as a preconditioning scheme with a closed form description (Theorem 1 and the claims thereafter).
221	1	Given that it depends on network depth but not on width, acceleration by overparameterization can be attained at a minimal computational price, as we demonstrate empirically in Section 8.
223	24	Empirically however, we showed that the trivial idea of replacing an internal weight matrix by a product of two can significantly accelerate optimization, with absolutely no effect on expressiveness (Figure 5-right).
224	111	The fact that gradient descent over classic convex problems such as linear regression with `p loss, p > 2, can accelerate from transitioning to a non-convex overparameterized objective, does not coincide with conventional wisdom, and provides food for thought.
225	110	Can this effect be rigorously quantified, similarly to analyses of explicit acceleration methods such as momentum or adaptive regularization (AdaGrad)?
