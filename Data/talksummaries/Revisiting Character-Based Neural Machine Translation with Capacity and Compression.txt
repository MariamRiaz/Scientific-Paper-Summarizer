1	23	However, NMT systems still typically rely on preand post-processing operations such as tokenization and word fragmentation through byte-pair encoding (BPE; Sennrich et al., 2016).
3	18	Even when properly tuned, the representation of the corpus generated by pipelined external processing is likely to be sub-optimal.
5	15	NMT systems are generally robust to such infelicities—and can be made more robust through subword regularization (Kudo, 2018)—but their effect on performance has not been carefully studied.
6	19	The problem of finding optimal segmentations becomes more complex when an NMT system must handle multiple source and target languages, as in multilingual translation or zero-shot approaches (Johnson et al., 2017).
9	38	Longer sequences incur linear per-layer cost and quadratic attention cost, and require information to be retained over longer temporal spans.
11	40	Perhaps most significantly, since the meaning of a word is not a compositional function of its characters, the system must learn to memorize many character sequences, a different task from the (mostly) compositional operations it performs at higher levels of linguistic abstraction.
12	105	In this paper, we show that a standard LSTM sequence-to-sequence model works very well for characters, and given sufficient depth, consistently outperforms identical models operating over word fragments.
14	14	One approach to this problem is temporal compression: reducing the number of state vectors required to represent input or output sequences.
15	71	We evaluate various approaches for performing temporal compression, both according to a fixed schedule; and, more ambitiously, learning compression decisions with a Hierarchical Multiscale architecture (Chung et al., 2017).
16	38	Following recent work by Lee et al. (2017), we focus on compressing the encoder.
17	139	Our contributions are as follows: • The first large-scale empirical investigation of the translation quality of standard LSTM sequence-to-sequence architectures operating at the character level, demonstrating improvements in translation quality over word fragments, and quantifying the effect of corpus size and model capacity.
18	34	• A comparison of techniques to compress character sequences, assessing their ability to trade translation quality for increased speed.
19	28	• A first attempt to learn how to compress the source sequence during NMT training by using the Hierarchical Multiscale LSTM to dynamically shorten the source sequence as it passes through the encoder.
34	30	We adopt a simplified version of the LSTM architecture of Chen et al. (2018) that achieves state-ofthe-art performance on the competitive WMT14 English-French and English-German benchmarks.
35	75	This incorporates bidirectional LSTM (BiLSTM) layers in the encoder, concatenating the output from forward and backward directions before feeding the next layer.
36	46	Output from the top encoder layer is projected down to the decoder dimension and used in an additive attention mechanism computed over the bottom decoder layer.
38	12	For both encoder and decoder we use layer normalization (Ba et al., 2016) and residual connections beginning at the third layer.
42	45	Our baseline character models and BPE models both use this architecture, differing only in whether the source and target languages are tokenized into sequences of characters or BPE word fragments.
52	16	These solutions are characterized by pooling the contents of two or more contiguous timesteps to create a single vector that summarizes them, and will replace them to shorten the sequence in the next layer.
71	45	However, z`t ’s activation is a binary step function in the forward pass, to enable discrete decisions, and a hard sigmoid in the backward pass, to allow gradients to flow through the decision point.2 The z`t decision affects both the layer above, and the next timestep of the current layer: • z`t = 1, flow up: the node above (t, `+1) performs a normal LSTM update; the node to the right (t+1, `) performs a modified update called a flush, which ignores the LSTM internal cell at (t, `), and redirects the incoming LSTM hidden state from (t, `) to (t, `+ 1).
89	62	To discourage this, we added a compression loss component similar to that of Ke et al. (2018) to penalize z activation rates outside a specified range α1, α2: Lc = ∑ l max(0, Z l − α1T, α2T − Z l), where T is source sequence length and Z l = ∑T t=1 z l t. To incorporate the HM into our NMT encoder, we replace the lowest BiLSTM layer with unidirectional HM layers.3 We adapt any remaining BiLSTM layers to copy or update according to the z-values calculated by the top HM layer.
90	20	We adopt the corpora used by Lee et al (2017), with the exception of WMT15 Russian-English.4 To measure performance on an “easy” language pair, and to calibrate our results against recent benchmarks, we also included WMT14 EnglishFrench.
100	27	Gradient norm is clipped to 5.0.
106	25	Our main evaluation metric is Moses-tokenized case-sensitive BLEU score.
123	16	Figure 1 tests this hypothesis by measuring the impact of three model sizes on test BLEU score.
137	41	The performance advantage of working with characters comes at a significant computational cost.
142	44	To make a qualitative comparison between word fragments (BPE) and characters for NMT, we examined 100 randomly selected sentence pairs from the DeEn test set.
143	138	One author examined the sentences, using a display that showed the source7 and the reference, along with the output of BPE and character models.
150	25	The latter is surprising, and appears to be a side-effect of a general tendency of the character models to be more faithful to the source, verging on being overly literal.
155	110	The second error follows another common pattern, where both systems mishandle the German compound (Chemiestunden / chemistry lessons), but the character system fails in a more useful way.
