{"sections": [{"heading": "1. Introduction", "text": "Deep neural networks have recently received much limelight for their enormous success in a variety of applications across many different areas of artificial intelligence, computer vision, speech recognition, and natural language processing (LeCun et al., 2015; Hinton et al., 2012; Krizhevsky et al., 2012; Bahdanau et al., 2014; Kalchbrenner & Blunsom, 2013). Nevertheless, it is also well-known that our theoretical understanding of their efficacy remains incomplete.\nThere have been several attempts to analyze deep neural networks from different perspectives. Notably, earlier studies have suggested that a deep architecture could use parameters more efficiently and requires exponentially fewer parameters to express certain families of functions than a shallow architecture (Delalleau & Bengio, 2011; Bengio & Delal-\n1Department of Computer Science, University of Chicago, Chicago, IL 2Department of Statistics, University of Chicago, Chicago, IL 3Computational and Applied Mathematics Initiative, University of Chicago, Chicago, IL. Correspondence to: Lek-Heng Lim <lekheng@galton.uchicago.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nleau, 2011; Montufar et al., 2014; Eldan & Shamir, 2016; Poole et al., 2016; Arora et al., 2018). Recent work (Zhang et al., 2016) showed that several successful neural networks possess a high representation power and can easily shatter random data. However, they also generalize well to data unseen during training stage, suggesting that such networks may have some implicit regularization. Traditional measures of complexity such as VC-dimension and Rademacher complexity fail to explain this phenomenon. Understanding this implicit regularization that begets the generalization power of deep neural networks remains a challenge.\nThe goal of our work is to establish connections between neural network and tropical geometry in the hope that they will shed light on the workings of deep neural networks. Tropical geometry is a new area in algebraic geometry that has seen an explosive growth in the recent decade but remains relatively obscure outside pure mathematics. We will focus on feedforward neural networks with rectified linear units (ReLU) and show that they are analogues of rational functions, i.e., ratios of two multivariate polynomials f, g in variables x1, . . . , xd,\nfpx1, . . . , xdq gpx1, . . . , xdq ,\nin tropical algebra. For standard and trigonometric polynomials, it is known that rational approximation \u2014 approximating a target function by a ratio of two polynomials instead of a single polynomial \u2014 vastly improves the quality of approximation without increasing the degree. This gives our analogue: An ReLU neural network is the tropical ratio of two tropical polynomials, i.e., a tropical rational function. More precisely, if we view a neural network as a function \u03bd : Rd \u00d1 Rp, x \u201c px1, . . . , xdq \u00de\u00d1 p\u03bd1pxq, . . . , \u03bdppxqq, then each \u03bd is a tropical rational map, i.e., each \u03bdi is a tropical rational function. In fact, we will show that:\nthe family of functions represented by feedforward neural networks with rectified linear units and integer weights is exactly the family of tropical rational maps.\nIt immediately follows that there is a semifield structure on this family of functions. More importantly, this establishes a\nar X\niv :1\n80 5.\n07 09\n1v 1\n[ cs\n.L G\n] 1\n8 M\nay 2\n01 8\nbridge between neural networks1 and tropical geometry that allows us to view neural networks as well-studied tropical geometric objects. This insight allows us to closely relate boundaries between linear regions of a neural network to tropical hypersurfaces and thereby facilitate studies of decision boundaries of neural networks in classification problems as tropical hypersurfaces. Furthermore, the number of linear regions, which captures the complexity of a neural network (Montufar et al., 2014; Raghu et al., 2017; Arora et al., 2018), can be bounded by the number of vertices of the polytopes associated with the neural network\u2019s tropical rational representation. Lastly, a neural network with one hidden layer can be completely characterized by zonotopes, which serve as building blocks for deeper networks.\nIn Sections 2 and 3 we introduce basic tropical algebra and tropical algebraic geometry of relevance to us. We state our assumptions precisely in Section 4 and establish the connection between tropical geometry and multilayer neural networks in Section 5. We analyze neural networks with tropical tools in Section 6, proving that a deeper neural network is exponentially more expressive than a shallow network \u2014 though our objective is not so much to perform state-of-the-art analysis but to demonstrate that tropical algebraic geometry can provide useful insights. All proofs are deferred to Section D of the supplement."}, {"heading": "2. Tropical algebra", "text": "Roughly speaking, tropical algebraic geometry is an analogue of classical algebraic geometry over C, the field of complex numbers, but where one replaces C by a semifield2 called the tropical semiring, to be defined below. We give a brief review of tropical algebra and introduce some relevant notations. See (Itenberg et al., 2009; Maclagan & Sturmfels, 2015) for an in-depth treatment.\nThe most fundamental component of tropical algebraic geometry is the tropical semiring T :\u201c ` R Y t\u00b48u,\u2018,d \u02d8\n. The two operations \u2018 and d, called tropical addition and tropical multiplication respectively, are defined as follows.\nDefinition 2.1. For x, y P R, their tropical sum is x\u2018 y :\u201c maxtx, yu; their tropical product is x d y :\u201c x ` y; the tropical quotient of x over y is xm y :\u201c x\u00b4 y.\nFor any x P R, we have \u00b48 \u2018 x \u201c 0 d x \u201c x and \u00b48d x \u201c \u00b48. Thus \u00b48 is the tropical additive identity and 0 is the tropical multiplicative identity. Furthermore, these operations satisfy the usual laws of arithmetic: associativity, commutativity, and distributivity. The set RY t\u00b48u is therefore a semiring under the operations\u2018 andd. While it is not a ring (lacks additive inverse), one may nonetheless\n1Henceforth a \u201cneural network\u201d will always mean a feedforward neural network with ReLU activation.\n2A semifield is a field sans the existence of additive inverses.\ngeneralize many algebraic objects (e.g., matrices, polynomials, tensors, etc) and notions (e.g., rank, determinant, degree, etc) over the tropical semiring \u2014 the study of these, in a nutshell, constitutes the subject of tropical algebra.\nLet N \u201c tn P Z : n \u011b 0u. For an integer a P N, raising x P R to the ath power is the same as multiplying x to itself a times. When standard multiplication is replaced by tropical multiplication, this gives us tropical power:\nxda :\u201c xd \u00a8 \u00a8 \u00a8 d x \u201c a \u00a8 x,\nwhere the last \u00a8 denotes standard product of real numbers; it is extended to RY t\u00b48u by defining, for any a P N,\n\u00b48da :\u201c #\n\u00b48 if a \u0105 0, 0 if a \u201c 0.\nA tropical semiring, while not a field, possesses one quality of a field: Every x P R has a tropical multiplicative inverse given by its standard additive inverse, i.e., xdp\u00b41q :\u201c \u00b4x. Though not reflected in its name, T is in fact a semifield.\nOne may therefore also raise x P R to a negative power a P Z by raising its tropical multiplicative inverse \u00b4x to the positive power \u00b4a, i.e., xda \u201c p\u00b4xqdp\u00b4aq. As is the case in standard real arithmetic, the tropical additive inverse \u00b48 does not have a tropical multiplicative inverse and \u00b48da is undefined for a \u0103 0. For notational simplicity, we will henceforth write xa instead of xda for tropical power when there is no cause for confusion. Other algebraic rules of tropical power may be derived from definition; see Section B in the supplement.\nWe are now in a position to define tropical polynomials and tropical rational functions. In the following, x and xi will denote variables (i.e., indeterminates).\nDefinition 2.2. A tropical monomial in d variables x1, . . . , xd is an expression of the form\ncd xa11 d x a2 2 d \u00a8 \u00a8 \u00a8 d x ad d\nwhere c P R Y t\u00b48u and a1, . . . , ad P N. As a convenient shorthand, we will also write a tropical monomial in multiindex notation as cx\u03b1 where \u03b1 \u201c pa1, . . . , adq P Nd and x \u201c px1, . . . , xdq. Note that x\u03b1 \u201c 0 d x\u03b1 as 0 is the tropical multiplicative identity.\nDefinition 2.3. Following notations above, a tropical polynomial fpxq \u201c fpx1, . . . , xdq is a finite tropical sum of tropical monomials\nfpxq \u201c c1x\u03b11 \u2018 \u00a8 \u00a8 \u00a8 \u2018 crx\u03b1r ,\nwhere \u03b1i \u201c pai1, . . . , aidq P Nd and ci P R Y t\u00b48u, i \u201c 1, . . . , r. We will assume that a monomial of a given multiindex appears at most once in the sum, i.e., \u03b1i \u2030 \u03b1j for any i \u2030 j.\nDefinition 2.4. Following notations above, a tropical rational function is a standard difference, or, equivalently, a tropical quotient of two tropical polynomials fpxq and gpxq: fpxq \u00b4 gpxq \u201c fpxq m gpxq. We will denote a tropical rational function by f m g, where f and g are understood to be tropical polynomial functions.\nIt is routine to verify that the set of tropical polynomials Trx1, . . . , xds forms a semiring under the standard extension of \u2018 and d to tropical polynomials, and likewise the set of tropical rational functions Tpx1, . . . , xdq forms a semifield. We regard a tropical polynomial f \u201c f m 0 as a special case of a tropical rational function and thus Trx1, . . . , xds \u010e Tpx1, . . . , xdq. Henceforth any result stated for a tropical rational function would implicitly also hold for a tropical polynomial.\nA d-variate tropical polynomial fpxq defines a function f : Rd \u00d1 R that is a convex function in the usual sense as taking max and sum of convex functions preserve convexity (Boyd & Vandenberghe, 2004). As such, a tropical rational function f m g : Rd \u00d1 R is a DC function or differenceconvex function (Hartman, 1959; Tao & Hoai An, 2005).\nWe will need a notion of vector-valued tropical polynomials and tropical rational functions. Definition 2.5. F : Rd \u00d1 Rp, x \u201c px1, . . . , xdq \u00de\u00d1 pf1pxq, . . . , fppxqq, is called a tropical polynomial map if each fi : Rd \u00d1 R is a tropical polynomial, i \u201c 1, . . . , p, and a tropical rational map if f1, . . . , fp are tropical rational functions. We will denote the set of tropical polynomial maps by Polpd, pq and the set of tropical rational maps by Ratpd, pq. So Polpd, 1q \u201c Trx1, . . . , xds and Ratpd, 1q \u201c Tpx1, . . . , xdq."}, {"heading": "3. Tropical hypersurfaces", "text": "There are tropical analogues of many notions in classical algebraic geometry (Itenberg et al., 2009; Maclagan & Sturmfels, 2015), among which are tropical hypersurfaces, tropical analogues of algebraic curves in classical algebraic geometry. Tropical hypersurfaces are a principal object of interest in tropical geometry and will prove very useful in our approach towards neural networks. Intuitively, the tropical hypersurface of a tropical polynomial f is the set of points x where f is not linear at x. Definition 3.1. The tropical hypersurface of a tropical polynomial fpxq \u201c c1x\u03b11 \u2018 \u00a8 \u00a8 \u00a8 \u2018 crx\u03b1r is\nT pfq :\u201c x P Rd : cix\u03b1i \u201c cjx\u03b1j \u201c fpxq for some \u03b1i \u2030 \u03b1j ( .\ni.e., the set of points x at which the value of f at x is attained by two or more monomials in f .\nA tropical hypersurface divides the domain of f into convex cells on each of which f is linear. These cells are convex polyhedrons, i.e., defined by linear inequalities with integer coefficients: tx P Rd : Ax \u010f bu for A P Zm\u02c6d and b P Rm. For example, the cell where a tropical monomial cjx\n\u03b1j attains its maximum is tx P Rd : cj ` \u03b1Tjx \u011b ci ` \u03b1Tix for all i \u2030 ju. Tropical hypersurfaces of polynomials in two variables (i.e., in R2) are called tropical curves.\nJust like standard multivariate polynomials, every tropical polynomial comes with an associated Newton polygon.\nDefinition 3.2. The Newton polygon of a tropical polynomial fpxq \u201c c1x\u03b11 \u2018 \u00a8 \u00a8 \u00a8 \u2018 crx\u03b1r is the convex hull of \u03b11, . . . , \u03b1r P Nd, regarded as points in Rd,\n\u2206pfq :\u201c Conv \u03b1i P Rd : ci \u2030 \u00b48, i \u201c 1, . . . , r ( .\nA tropical polynomial f determines a dual subdivision of \u2206pfq, constructed as follows. First, lift each \u03b1i from Rd into Rd`1 by appending ci as the last coordinate. Denote the convex hull of the lifted \u03b11, . . . , \u03b1r as\nPpfq :\u201c Convtp\u03b1i, ciq P Rd \u02c6 R : i \u201c 1, . . . , ru. (1)\nNext let UF ` Ppfq \u02d8\ndenote the collection of upper faces in Ppfq and \u03c0 : Rd \u02c6 R \u00d1 Rd be the projection that drops the last coordinate. The dual subdivision determined by f is then\n\u03b4pfq :\u201c \u03c0ppq \u0102 Rd : p P UF ` Ppfq \u02d8( .\n\u03b4pfq forms a polyhedral complex with support \u2206pfq. By (Maclagan & Sturmfels, 2015, Proposition 3.1.6), the tropical hypersurface T pfq is the pd\u00b4 1q-skeleton of the polyhedral complex dual to \u03b4pfq. This means that each vertex in \u03b4pfq corresponds to one \u201ccell\u201d in Rd where the function f is linear. Thus, the number of vertices in Ppfq provides an upper bound on the number of linear regions of f .\nFigure 1 shows the Newton polygon and dual subdivision for the tropical polynomial fpx1, x2q \u201c 1d x21 \u2018 1d x22 \u2018 2d x1x2 \u2018 2d x1 \u2018 2d x2 \u2018 2. Figure 2 shows how we\nmay find the dual subdivision for this tropical polynomial by following the aforementioned procedures; with step-by-step details given in Section C.1.\nTropical polynomials and tropical rational functions are clearly piecewise linear functions. As such a tropical rational map is a piecewise linear map and the notion of linear region applies.\nDefinition 3.3. A linear region of F P Ratpd,mq is a maximal connected subset of the domain on which F is linear. The number of linear regions of F is denoted N pF q.\nNote that a tropical polynomial map F P Polpd,mq has convex linear regions but a tropical rational map F P Ratpd, nq generally has nonconvex linear regions. In Section 6.3, we will use N pF q as a measure of complexity for an F P Ratpd, nq given by a neural network."}, {"heading": "3.1. Transformations of tropical polynomials", "text": "Our analysis of neural networks will require figuring out how the polytope Ppfq transforms under tropical power, sum, and product. The first is straightforward.\nProposition 3.1. Let f be a tropical polynomial and let a P N. Then\nPpfaq \u201c aPpfq.\naPpfq \u201c tax : x P Ppfqu \u010e Rd`1 is a scaled version of Ppfq with the same shape but different volume.\nTo describe the effect of tropical sum and product, we need a few notions from convex geometry. The Minkowski sum of two sets P1 and P2 in Rd is the set\nP1 ` P2 :\u201c x1 ` x2 P Rd : x1 P P1, x2 P P2 ( ;\nand for \u03bb1, \u03bb2 \u011b 0, their weighted Minkowski sum is\n\u03bb1P1 ` \u03bb2P2 :\u201c \u03bb1x1 ` \u03bb2x2 P Rd : x1 P P1, x2 P P2 ( .\nWeighted Minkowski sum is clearly commutative and associative and generalizes to more than two sets. In particular, the Minkowski sum of line segments is called a zonotope.\nLet VpP q denote the set of vertices of a polytope P . Clearly, the Minkowski sum of two polytopes is given by the convex hull of the Minkowski sum of their vertex sets, i.e., P1 ` P2 \u201c Conv ` VpP1q ` VpP2q \u02d8\n. With this observation, the following is immediate.\nProposition 3.2. Let f, g P Polpd, 1q \u201c Trx1, . . . , xds be tropical polynomials. Then\nPpf d gq \u201c Ppfq ` Ppgq, Ppf \u2018 gq \u201c Conv ` VpPpfqq Y VpPpgqq \u02d8 .\nWe reproduce below part of (Gritzmann & Sturmfels, 1993, Theorem 2.1.10) and derive a corollary for bounding the number of verticies on the upper faces of a zonotope.\nTheorem 3.3 (Gritzmann\u2013Sturmfels). Let P1, . . . , Pk be polytopes in Rd and let m denote the total number of nonparallel edges of P1, . . . , Pk. Then the number of vertices of P1 ` \u00a8 \u00a8 \u00a8 ` Pk does not exceed\n2 d\u00b41 \u00ff j\u201c0 pm\u00b4 1j q .\nThe upper bound is attained if all Pi\u2019s are zonotopes and all their generating line segments are in general positions.\nCorollary 3.4. Let P \u010e Rd`1 be a zonotope generated by m line segments P1, . . . , Pm. Let \u03c0 : Rd \u02c6 R\u00d1 Rd be the projection. Suppose P satisfies:\n(i) the generating line segments are in general positions;\n(ii) the set of projected vertices t\u03c0pvq : v P VpP qu \u010e Rd are in general positions.\nThen P has d \u00ff\nj\u201c0 pmj q\nvertices on its upper faces. If either (i) or (ii) is violated, then this becomes an upper bound.\nAs we mentioned, linear regions of a tropical polynomial f correspond to vertices on UF ` Ppfq \u02d8\nand the corollary will be useful for bounding the number of linear regions."}, {"heading": "4. Neural networks", "text": "While we expect our readership to be familiar with feedforward neural networks, we will nevertheless use this short\nsection to define them, primarily for the purpose of fixing notations and specifying the assumptions that we retain throughout this article. We restrict our attention to fully connected feedforward neural networks.\nViewed abstractly, an L-layer feedforward neural network is a map \u03bd : Rd \u00d1 Rp given by a composition of functions\n\u03bd \u201c \u03c3pLq \u02dd \u03c1pLq \u02dd \u03c3pL\u00b41q \u02dd \u03c1pL\u00b41q \u00a8 \u00a8 \u00a8 \u02dd \u03c3p1q \u02dd \u03c1p1q.\nThe preactivation functions \u03c1p1q, . . . , \u03c1pLq are affine transformations to be determined and the activation functions \u03c3p1q, . . . , \u03c3pLq are chosen and fixed in advanced.\nWe denote the width, i.e., the number of nodes, of the lth layer by nl, l \u201c 1, \u00a8 \u00a8 \u00a8 , L\u00b4 1. We set n0 :\u201c d and nL :\u201c p, respectively the dimensions of the input and output of the network. The output from the lth layer will be denoted by\n\u03bdplq :\u201c \u03c3plq \u02dd \u03c1plq \u02dd \u03c3pl\u00b41q \u02dd \u03c1pl\u00b41q \u00a8 \u00a8 \u00a8 \u02dd \u03c3p1q \u02dd \u03c1p1q,\ni.e., it is a map \u03bdplq : Rd \u00d1 Rnl . For convenience, we assume \u03bdp0qpxq :\u201c x.\nThe affine function \u03c1plq : Rnl\u00b41 \u00d1 Rnl is given by a weight matrix Aplq P Znl\u02c6nl\u00b41 and a bias vector bplq P Rnl :\n\u03c1plqp\u03bdpl\u00b41qq :\u201c Aplq\u03bdpl\u00b41q ` bplq.\nThe pi, jqth coordinate of Aplq will be denoted aplqij and the ith coordinate of bplq by bplqi . Collectively they form the parameters of the lth layer.\nFor a vector input x P Rnl , \u03c3plqpxq is understood to be in coordinatewise sense; so \u03c3 : Rnl \u00d1 Rnl . We assume the final output of a neural network \u03bdpxq is fed into a score function s : Rp \u00d1 Rm that is application specific. When used as an m-category classifier, s may be chosen, for example, to be a soft-max or sigmoidal function. The score function is quite often regarded as the last layer of a neural network but this is purely a matter of convenience and we will not assume this. We will make the following mild assumptions on the architecture of our feedforward neural networks and explain next why they are indeed mild:\n(a) the weight matrices Ap1q, . . . , ApLq are integer-valued;\n(b) the bias vectors bp1q, . . . , bpLq are real-valued;\n(c) the activation functions \u03c3p1q, . . . , \u03c3pLq take the form\n\u03c3plqpxq :\u201c maxtx, tplqu,\nwhere tplq P pRYt\u00b48uqnl is called a threshold vector.\nHenceforth all neural networks in our subsequent discussions will be assumed to satisfy (a)\u2013(c).\n(b) is completely general but there is also no loss of generality in (a), i.e., in restricting the weights Ap1q, . . . , ApLq from real matrices to integer matrices, as:\n\u2022 real weights can be approximated arbitrarily closely by rational weights;\n\u2022 one may then \u2018clear denominators\u2019 in these rational weights by multiplying them by the least common multiple of their denominators to obtain integer weights;\n\u2022 keeping in mind that scaling all weights and biases by the same positive constant has no bearing on the workings of a neural network.\nThe activation function in (c) includes both ReLU activation (tplq \u201c 0) and identity map (tplq \u201c \u00b48) as special cases. Aside from ReLU, our tropical framework will apply to piecewise linear activations such as leaky ReLU and absolute value, and with some extra effort, may be extended to max pooling, maxout nets, etc. But it does not, for example, apply to activations such as hyperbolic tangent and sigmoid.\nIn this work, we view an ReLU network as the simplest and most canonical model of a neural network, from which other variants that are more effective at specific tasks may be derived. Given that we seek general theoretical insights and not specific practical efficacy, it makes sense to limit ourselves to this simplest case. Moreover, ReLU networks already embody some of the most important elements (and mysteries) common to a wider range of neural networks (e.g., universal approximation, exponential expressiveness); they work well in practice and are often the go-to choice for feedforward networks. We are also not alone in limiting our discussions to ReLU networks (Montufar et al., 2014; Arora et al., 2018)."}, {"heading": "5. Tropical algebra of neural networks", "text": "We now describe our tropical formulation of a multilayer feedforward neural network satisfying (a)\u2013(c).\nA multilayer feedforward neural network is generally nonconvex, whereas a tropical polynomial is always convex. Since most nonconvex functions are a difference of two convex functions (Hartman, 1959), a reasonable guess is that a feedforward neural network is the difference of two tropical polynomials, i.e., a tropical rational function. This is indeed the case, as we will see from the following.\nConsider the output from the first layer in neural network\n\u03bdpxq \u201c maxtAx` b, tu,\nwhere A P Zp\u02c6d, b P Rp, and t P pR Y t\u00b48uqp. We will decompose A as a difference of two nonnegative integervalued matrices, A \u201c A`\u00b4A\u00b4 with A`, A\u00b4 P Np\u02c6d; e.g., in the standard way with entries\na`ij :\u201c maxtaij , 0u, a \u00b4 ij :\u201c maxt\u00b4aij , 0u\nrespectively. Since\nmaxtAx` b, tu \u201c maxtA`x` b, A\u00b4x` tu \u00b4A\u00b4x,\nwe see that every coordinate of one-layer neural network is a difference of two tropical polynomials. For networks with more layers, we apply this decomposition recursively to obtain the following result.\nProposition 5.1. LetA P Zm\u02c6n, b P Rm be the parameters of the pl ` 1qth layer, and let t P pR Y t\u00b48uqm be the threshold vector in the pl` 1qth layer. If the nodes of the lth layer are given by tropical rational functions,\n\u03bdplqpxq \u201c F plqpxq mGplqpxq \u201c F plqpxq \u00b4Gplqpxq,\ni.e., each coordinate of F plq and Gplq is a tropical polynomial in x, then the outputs of the preactivation and of the pl ` 1qth layer are given by tropical rational functions\n\u03c1pl`1q \u02dd \u03bdplqpxq \u201c Hpl`1qpxq \u00b4Gpl`1qpxq, \u03bdpl`1qpxq \u201c \u03c3 \u02dd \u03c1pl`1q \u02dd \u03bdplqpxq \u201c F pl`1qpxq \u00b4Gpl`1qpxq\nrespectively, where\nF pl`1qpxq \u201c max Hpl`1qpxq, Gpl`1qpxq ` t ( ,\nGpl`1qpxq \u201c A`Gplqpxq `A\u00b4F plqpxq, Hpl`1qpxq \u201c A`F plqpxq `A\u00b4Gplqpxq ` b.\nWe will write f plqi , g plq i and h plq i for the ith coordinate of F plq, Gplq and Hplq respectively. In tropical arithmetic, the recurrence above takes the form\nf pl`1q i \u201c h pl`1q i \u2018 pg pl`1q i d tiq,\ng pl`1q i \u201c\n\u201e n \u00e4\nj\u201c1 pf plqj q a\u00b4ij\n d \u201e n \u00e4\nj\u201c1 pgplqj q a`ij\n\n,\nh pl`1q i \u201c\n\u201e n \u00e4\nj\u201c1 pf plqj q a`ij\n d \u201e n \u00e4\nj\u201c1 pgplqj q a\u00b4ij\n\nd bi.\n(2)\nRepeated applications of Proposition 5.1 yield the following.\nTheorem 5.2 (Tropical characterization of neural networks). A feedforward neural network under assumptions (a)\u2013(c) is a function \u03bd : Rd \u00d1 Rp whose coordinates are tropical rational functions of the input, i.e.,\n\u03bdpxq \u201c F pxq mGpxq \u201c F pxq \u00b4Gpxq\nwhere F and G are tropical polynomial maps. Thus \u03bd is a tropical rational map.\nNote that the tropical rational functions above have real coefficients, not integer coefficients. The integer weights Aplq P Znl\u02c6nl\u00b41 have gone into the powers of tropical monomials in f and g, which is why we require our weights to be integer-valued, although as we have explained, this requirement imposes little loss of generality.\nBy setting tp1q \u201c \u00a8 \u00a8 \u00a8 \u201c tpL\u00b41q \u201c 0 and tpLq \u201c \u00b48, we obtain the following corollary.\nCorollary 5.3. Let \u03bd : Rd \u00d1 R be an ReLU activated feedforward neural network with integer weights and linear output. Then \u03bd is a tropical rational function.\nA more remarkable fact is the converse of Corollary 5.3.\nTheorem 5.4 (Equivalence of neural networks and tropical rational functions).\n(i) Let \u03bd : Rd \u00d1 R. Then \u03bd is a tropical rational function if and only if \u03bd is a feedforward neural network satisfying assumptions (a)\u2013(c).\n(ii) A tropical rational function f m g can be represented as an L-layer neural network, with\nL \u010f maxtrlog2 rf s, rlog2 rgsu ` 2,\nwhere rf and rg are the number of monomials in the tropical polynomials f and g respectively.\nWe would like to acknowledge the precedence of (Arora et al., 2018, Theorem 2.1), which demonstrates the equivalence between ReLU-activatedL-layer neural networks with real weights and d-variate continuous piecewise functions with real coefficients, where L \u010f rlog2pd` 1qs` 1.\nBy construction, a tropical rational function is a continuous piecewise linear function. The continuity of a piecewise linear function automatically implies that each of the pieces on which it is linear is a polyhedral region. As we saw in Section 3, a tropical polynomial f : Rd \u00d1 R gives a tropical hypersurface that divides Rd into convex polyhedral regions defined by linear inequalities with integer coefficients: tx P Rd : Ax \u010f bu with A P Zm\u02c6d and b P Rm. A tropical rational function f m g : Rd \u00d1 R must also be a continuous piecewise linear function and divide Rd into polyhedral regions on each of which f m g is linear, although these regions are nonconvex in general. We will show the converse \u2014 any continuous piecewise linear function with integer coefficients is a tropical rational function.\nProposition 5.5. Let \u03bd : Rd \u00d1 R. Then \u03bd is a continuous piecewise linear function with integer coefficients if and only if \u03bd is a tropical rational function.\nCorollary 5.3, Theorem 5.4, and Proposition 5.5 collectively imply the equivalence of\n(i) tropical rational functions,\n(ii) continuous piecewise linear functions with integer coefficients,\n(iii) neural networks satisfying assumptions (a)\u2013(c).\nAn immediate advantage of this characterization is that the set of tropical rational functions Tpx1, . . . , xdq has a semifield structure as we pointed out in Section 2, a fact that we have implicitly used in the proof of Proposition 5.5. However, what is more important is not the algebra but the\nalgebraic geometry that arises from our tropical characterization. We will use tropical algebraic geometry to illuminate our understanding of neural networks in the next section.\nThe need to stay within tropical algebraic geometry is the reason we did not go for a simpler and more general characterization (that does not require the integer coefficients assumption). A tropical signomial takes the form\n\u03d5pxq \u201c m \u00e0\ni\u201c1 bi\nn \u00e4 j\u201c1 x aij j ,\nwhere aij P R and bi P R Y t\u00b48u. Note that aij is not required to be integer-valued nor nonnegative. A tropical rational signomial is a tropical quotient\u03d5m\u03c8 of two tropical signomials \u03d5,\u03c8. A tropical rational signomial map is a function \u03bd \u201c p\u03bd1, . . . , \u03bdpq : Rd \u00d1 Rp where each \u03bdi : Rd \u00d1 R is a tropical rational signomial \u03bdi \u201c \u03d5i m \u03c8i. The same argument we used to establish Theorem 5.2 gives us the following. Proposition 5.6. Every feedforward neural network with ReLU activation is a tropical rational signomial map.\nNevertheless tropical signomials fall outside the realm of tropical algebraic geometry and we do not use Proposition 5.6 in the rest of this article."}, {"heading": "6. Tropical geometry of neural networks", "text": "Section 5 defines neural networks via tropical algebra, a perspective that allows us to study them via tropical algebraic geometry. We will show that the decision boundary of a neural network is a subset of a tropical hypersurface of a corresponding tropical polynomial (Section 6.1). We will see that, in an appropriate sense, zonotopes form the geometric building blocks for neural networks (Section 6.2). We then prove that the geometry of the function represented by a neural network grows vastly more complex as its number of layers increases (Section 6.3)."}, {"heading": "6.1. Decision boundaries of a neural network", "text": "We will use tropical geometry and insights from Section 5 to study decision boundaries of neural networks, focusing on the case of two-category classification for clarity. As explained in Section 4, a neural network \u03bd : Rd \u00d1 Rp together with a choice of score function s : Rp \u00d1 R give us a classifier. If the output value sp\u03bdpxqq exceeds some decision threshold c, then the neural network predicts x is from one class (e.g., x is a CAT image), and otherwise x is from the other category (e.g., a DOG image). The input space is thereby partitioned into two disjoint subsets by the decision boundary B :\u201c tx P Rd : \u03bdpxq \u201c s\u00b41pcqu. Connected regions with value above the threshold and connected regions with value below the threshold will be called the positive regions and negative regions respectively.\nWe provide bounds on the number of positive and negative regions and show that there is a tropical polynomial whose tropical hypersurface contains the decision boundary.\nProposition 6.1 (Tropical geometry of decision boundary). Let \u03bd : Rd \u00d1 R be an L-layer neural network satisfying assumptions (a)\u2013(c) with tpLq \u201c \u00b48. Let the score function s : R\u00d1 R be injective with decision threshold c in its range. If \u03bd \u201c f m g where f and g are tropical polynomials, then\n(i) its decision boundary B \u201c tx P Rd : \u03bdpxq \u201c s\u00b41pcqu divides Rd into at most N pfq connected positive regions and at most N pgq connected negative regions;\n(ii) its decision boundary is contained in the tropical hypersurface of the tropical polynomial s\u00b41pcq d gpxq \u2018 fpxq \u201c maxtfpxq, gpxq ` s\u00b41pcqu, i.e.,\nB \u010e T ps\u00b41pcq d g \u2018 fq. (3)\nThe function s\u00b41pcqdg\u2018f is not necessarily linear on every positive or negative region and so its tropical hypersurface T ps\u00b41pcqdg\u2018fqmay further divide a positive or negative region derived from B into multiple linear regions. Hence the \u201c\u010e\u201d in (3) cannot in general be replaced by \u201c\u201c\u201d."}, {"heading": "6.2. Zonotopes as geometric building blocks of neural networks", "text": "From Section 3, we know that the number of regions a tropical hypersurface T pfq divides the space into equals the number of vertices in the dual subdivision of the Newton polygon associated with the tropical polynomial f . This allows us to bound the number of linear regions of a neural network by bounding the number of vertices in the dual subdivision of the Newton polygon.\nWe start by examining how geometry changes from one layer to the next in a neural network, more precisely:\nQuestion. How are the tropical hypersurfaces of the tropical polynomials in the pl ` 1qth layer of a neural network related to those in the lth layer?\nThe recurrent relation (2) describes how the tropical polynomials occurring in the pl ` 1qth layer are obtained from those in the lth layer, namely, via three operations: tropical sum, tropical product, and tropical powers. Recall that a tropical hypersurface of a tropical polynomial is dual to the dual subdivision of the Newton polytope of the tropical polynomial, which is given by the projection of the upper faces on the polytopes defined by (1). Hence the question boils down to how these three operations transform the polytopes, which is addressed in Propositions 3.1 and 3.2. We follow notations in Proposition 5.1 for the next result.\nLemma 6.2. Let f plqi , g plq i , h plq i be the tropical polynomials produced by the ith node in the lth layer of a neural network,\ni.e., they are defined by (2). Then P ` f plq i \u02d8 , P ` g plq i \u02d8 , P ` h plq i \u02d8 are subsets of Rd`1 given as follows:\n(i) P ` g p1q i \u02d8 and P ` h p1q i \u02d8 are points.\n(ii) P ` f p1q i \u02d8 is a line segment.\n(iii) P ` g p2q i \u02d8 and P ` h p2q i \u02d8 are zonotopes. (iv) For l \u011b 1,\nP ` f plq i \u02d8 \u201c Conv \u201c P ` g plq i d t plq i \u02d8 Y P ` h plq i \u02d8\u2030\nif tplqi P R, and P ` f plq i \u02d8 \u201c P ` h plq i \u02d8 if tplqi \u201c \u00b48.\n(v) For l \u011b 1, P ` g pl`1q i \u02d8 and P ` h pl`1q i \u02d8\nare weighted Minkowski sums,\nP ` g pl`1q i \u02d8\n\u201c nl \u00ff\nj\u201c1 a\u00b4ijP ` f plq j \u02d8\n` nl \u00ff\nj\u201c1 a`ijP ` g plq j \u02d8 ,\nP ` h pl`1q i \u02d8\n\u201c nl \u00ff\nj\u201c1 a`ijP ` f plq j \u02d8\n` nl \u00ff\nj\u201c1 a\u00b4ijP ` g plq j \u02d8\n` tbieu,\nwhere aij , bi are entries of the weight matrix Apl`1q P Znl`1\u02c6nl and bias vector bpl`1q P Rnl`1 , and e :\u201c p0, . . . , 0, 1q P Rd`1.\nA conclusion of Lemma 6.2 is that zonotopes are the building blocks in the tropical geometry of neural networks. Zonotopes are studied extensively in convex geometry and, among other things, are intimately related to hyperplane arrangements (Greene & Zaslavsky, 1983; Guibas et al., 2003; McMullen, 1971; Holtz & Ron, 2011). Lemma 6.2 connects neural networks to this extensive body of work but its full implication remains to be explored. In Section C.2 of the supplement, we show how one may build these polytopes for a two-layer neural network."}, {"heading": "6.3. Geometric complexity of deep neural networks", "text": "We apply the tools in Section 3 to study the complexity of a neural network, showing that a deep network is much more expressive than a shallow one. Our measure of complexity is geometric: we will follow (Montufar et al., 2014; Raghu et al., 2017) and use the number of linear regions of a piecewise linear function \u03bd : Rd \u00d1 Rp to measure the complexity of \u03bd.\nWe would like to emphasize that our upper bound below does not improve on that obtained in (Raghu et al., 2017) \u2014 in fact, our version is more restrictive given that it applies only to neural networks satisfying (a)\u2013(c). Nevertheless our goal here is to demonstrate how tropical geometry may be used to derive the same bound.\nTheorem 6.3. Let \u03bd : Rd \u00d1 R be an L-layer real-valued feedforward neural network satisfying (a)\u2013(c). Let tpLq \u201c\n\u00b48 and nl \u011b d for all l \u201c 1, . . . , L \u00b4 1. Then \u03bd \u201c \u03bdpLq has at most\nL\u00b41 \u017a\nl\u201c1\nd \u00ff i\u201c0 pnl i q\nlinear regions. In particular, if d \u010f n1, . . . , nL\u00b41 \u010f n, the number of linear regions of \u03bd is bounded by O ` ndpL\u00b41q \u02d8 .\nProof. If L \u201c 2, this follows directly from Lemma 6.2 and Corollary 3.4. The case of L \u011b 3 is in Section D.7 in the supplement.\nAs was pointed out in (Raghu et al., 2017), this upper bound closely matches the lower bound \u2126 ` pn{dqpL\u00b41qdnd \u02d8 in (Montufar et al., 2014, Corollary 5) when n1 \u201c \u00a8 \u00a8 \u00a8 \u201c nL\u00b41 \u201c n \u011b d. Hence we surmise that the number of linear regions of the neural network grows polynomially with the width n and exponentially with the number of layers L."}, {"heading": "7. Conclusion", "text": "We argue that feedforward neural networks with rectified linear units are, modulo trivialities, nothing more than tropical rational maps. To understand them we often just need to understand the relevant tropical geometry.\nIn this article, we took a first step to provide a proof-ofconcept: questions regarding decision boundaries, linear regions, how depth affect expressiveness, etc, can be translated into questions involving tropical hypersurfaces, dual subdivision of Newton polygon, polytopes constructed from zonotopes, etc.\nAs a new branch of algebraic geometry, the novelty of tropical geometry stems from both the algebra and geometry as well as the interplay between them. It has connections to many other areas of mathematics. Among other things, there is a tropical analogue of linear algebra (Butkovic\u030c, 2010) and a tropical analogue of convex geometry (Gaubert & Katz, 2006). We cannot emphasize enough that we have only touched on a small part of this rich subject. We hope that further investigation from this tropical angle might perhaps unravel other mysteries of deep neural networks."}, {"heading": "Acknowledgments", "text": "The authors thank Ralph Morrison, Yang Qi, Bernd Sturmfels, and the anonymous referees for their very helpful comments. The work in this article is generously supported by DARPA D15AP00109, NSF IIS 1546413, the Eckhardt Faculty Fund, and a DARPA Director\u2019s Fellowship."}, {"heading": "B. Tropical power", "text": "As in Section 2, we write xa \u201c xda; aside from this slight abuse of notation, \u2018 and d denote tropical sum and product, ` and \u00a8 denote standard sum and product in all other contexts. Tropical power evidently has the following properties:\n\u2022 For x, y P R and a P R, a \u011b 0,\npx\u2018 yqa \u201c xa \u2018 ya and pxd yqa \u201c xa d ya.\nIf a is allowed negative values, then we lose the first property. In general px\u2018 yqa \u2030 xa \u2018 ya for a \u0103 0. \u2022 For x P R,\nx0 \u201c 0.\n\u2022 For x P R and a, b P N, pxaqb \u201c xa\u00a8b.\n\u2022 For x P R and a, b P Z, xa d xb \u201c xa`b.\n\u2022 For x P R and a, b P Z, xa \u2018 xb \u201c xa d pxa\u00b4b \u2018 0q \u201c xa d p0\u2018 xa\u00b4bq."}, {"heading": "C. Examples", "text": "C.1. Examples of tropical curves and dual subdivision of Newton polygon\nLet f P Polp2, 1q \u201c Trx1, x2s, i.e., a bivariate tropical polynomial. It follows from our discussions in Section 3 that the tropical hypersurface T pfq is a planar graph dual to the dual subdivision \u03b4pfq in the following sense:\n(i) Each two-dimensional face in \u03b4pfq corresponds to a vertex in T pfq. (ii) Each one-dimensional edge of a face in \u03b4pfq corresponds to an edge in T pfq. In particular, an edge from the Newton\npolygon \u2206pfq corresponds to an unbounded edge in T pfq while other edges correspond to bounded edges.\nFigure 2 illustrates how we may find the dual subdivision for the tropical polynomial fpx1, x2q \u201c 1d x21 \u2018 1d x22 \u2018 2d x1x2 \u2018 2d x1 \u2018 2d x2 \u2018 2. First, find the convex hull\nPpfq \u201c Convtp2, 0, 1q, p0, 2, 1q, p1, 1, 2q, p1, 0, 2q, p0, 1, 2q, p0, 0, 2qu.\nThen, by projecting the upper envelope of Ppfq to R2, we obtain \u03b4pfq, the dual subdivision of the Newton polygon.\nC.2. Polytopes of a two-layer neural network\nWe illustrate our discussions in Section 6.2 with a two-layer example. Let \u03bd : R2 \u00d1 R be with n0 \u201c 2 input nodes, n1 \u201c 5 nodes in the first layer, and n2 \u201c 1 nodes in the output:\ny \u201c \u03bdp1qpxq \u201c max\n$\n\u2019 \u2019 \u2019 \u2019 &\n\u2019 \u2019 \u2019 \u2019 %\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2013\n\u00b41 1 1 \u00b43 1 2 \u00b44 1 3 2\nfi\nffi ffi ffi ffi fl \u201e x1 x2  `\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2013\n1 \u00b41\n2 0 \u00b42\nfi\nffi ffi ffi ffi fl , 0\n,\n/ / / / .\n/ / / / -\n,\n\u03bdp2qpyq \u201c maxty1 ` 2y2 ` y3 \u00b4 y4 \u00b4 3y5, 0u.\nWe first express \u03bdp1q and \u03bdp2q as tropical rational maps,\n\u03bdp1q \u201c F p1q mGp1q, \u03bdp2q \u201c f p2q m gp2q,\nwhere\ny :\u201c F p1qpxq \u201c Hp1qpxq \u2018Gp1qpxq,\nz :\u201c Gp1qpxq \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2013 x1 x32 0 x41 0\nfi\nffi ffi ffi ffi fl , Hp1qpxq \u201c\n\u00bb\n\u2014 \u2014 \u2014 \u2014 \u2013\n1d x2 p\u00b41q d x1 2d x1x22\nx2 p\u00b42q d x31x22\nfi\nffi ffi ffi ffi fl ,\nand\nf p2qpxq \u201c gp2qpxq \u2018 hp2qpxq, gp2qpxq \u201c y4 d y35 d z1 d z22 d z3 \u201c px2 \u2018 x41q d pp\u00b42q d x31x22 \u2018 0q3 d x1 d px32q2, hp2qpxq \u201c y1 d y22 d y3 d z4 d z35\n\u201c p1d x2 \u2018 x1q d pp\u00b41q d x1 \u2018 x32q2 d p2d x1x22 \u2018 0q d x41.\nWe will write F p1q \u201c pf p1q1 , . . . , f p1q 5 q and likewise for Gp1q and Hp1q. The monomials occurring in g p1q j pxq and h p1q j pxq are all of the form cxa11 x a2 2 . Therefore Ppg p1q j q and Pph p1q j q, j \u201c 1, . . . , 5, are points in R3.\nSince F p1q \u201c Gp1q \u2018Hp1q, Ppf p1qj q is a convex hull of two points, and thus a line segment in R3. The Newton polygons associated with f p1qj , equal to their dual subdivisions in this case, are obtained by projecting these line segments back to the plane spanned by a1, a2, as shown on the left in Figure C.1.\nThe line segments Ppf p1qj q, j \u201c 1, . . . , 5, and points Ppg p1q j q, j \u201c 1, . . . , 5, serve as building blocks for Pphp2qq and Ppgp2qq, which are constructed as weighted Minkowski sums:\nPphp2qq \u201c Ppf p1q4 q ` 3Ppf p1q 5 q ` Ppg p1q 1 q ` 2Ppg p1q 2 q ` Ppg p1q 3 q, Ppgp2qq \u201c Ppf p1q1 q ` 2Ppf p1q 2 q ` Ppf p1q 3 q ` Ppg p1q 4 q ` 3Ppg p1q 5 q.\nPpgp2qq and the dual subdivision of its Newton polygon are shown on the right in Figure C.1. Pphp2qq and the dual subdivision of its Newton polygon are shown on the left in Figure C.2. Ppf p2qq is the convex hull of the union of Ppgp2qq and Pphp2qq. The dual subdivision of its Newton polygon is obtained by projecting the upper faces of Ppf p2qq to the plane spanned by a1, a2. These are shown on the right in Figure C.2."}, {"heading": "D. Proofs", "text": ""}, {"heading": "D.1. Proof of Corollary 3.4", "text": "Proof. Let V1 and V2 be the sets of vertices on the upper and lower envelopes of P respectively. By Theorem 3.3, P has\nn1 :\u201c 2 d \u00ff j\u201c0 pm\u00b4 1j q\nvertices in total. By construction, we have |V1 Y V2| \u201c n1. It is well-known that zonotopes are centrally symmetric and so there are equal number of vertices on the upper and lower envelopes, i.e., |V1| \u201c |V2|. Let P 1 :\u201c \u03c0pP q be the projection of P into Rd. Since the projected vertices are assumed to be in general positions, P 1 must be a d-dimensional zonotope generated by m nonparallel line segments. Hence, by Theorem 3.3 again, P 1 has\nn2 :\u201c 2 d\u00b41 \u00ff j\u201c0 pm\u00b4 1j q\nvertices. For any vertex v P P , \u03c0pvq is a vertex of P 1 if and only if v belongs to both the upper and lower envelopes, i.e., v P V1 X V2. Therefore the number of vertices on P 1 equals |V1 X V2|. By construction, we have |V1 X V2| \u201c n2. Consequently the number of vertices on the upper envelope is\n|V1| \u201c 1\n2 p|V1 Y V2| \u00b4 |V1 X V2|q ` |V1 X V2| \u201c\n1 2 pn1 \u00b4 n2q ` n2 \u201c\nd \u00ff j\u201c0 pmj q ."}, {"heading": "D.2. Proof of Proposition 5.1", "text": "Proof. Writing A \u201c A` \u00b4A\u00b4, we have\n\u03c1pl`1qpxq \u201c ` A` \u00b4A\u00b4 \u02d8` F plqpxq \u00b4Gplqpxq \u02d8 ` b \u201c ` A`F plqpxq `A\u00b4Gplqpxq ` b \u02d8 \u00b4 ` A`G plqpxq `A\u00b4F plqpxq \u02d8\n\u201c Hpl`1qpxq \u00b4Gpl`1qpxq, \u03bdpl`1qpxq \u201c max \u03c1pl`1qpyq, t (\n\u201c max Hpl`1qpxq \u00b4Gpl`1qpxq, t (\n\u201c max Hpl`1qpxq, Gpl`1qpxq ` t ( \u00b4Gpl`1qpxq \u201c F pl`1qpxq \u00b4Gpl`1qpxq."}, {"heading": "D.3. Proof of Theorem 5.4", "text": "Proof. It remains to establish the \u201conly if\u201d part. We will write \u03c3tpxq :\u201c maxtx, tu. Any tropical monomial bix\u03b1i is clearly such a neural network as\nbix \u03b1i \u201c p\u03c3\u00b48 \u02dd \u03c1iqpxq \u201c maxt\u03b1Tix` bi,\u00b48u.\nIf two tropical polynomials p and q are represented as neural networks with lp and lq layers respectively,\nppxq \u201c ` \u03c3\u00b48 \u02dd \u03c1plpqp \u02dd \u03c30 \u02dd . . . \u03c30 \u02dd \u03c1p1qp \u02d8 pxq, qpxq \u201c `\n\u03c3\u00b48 \u02dd \u03c1plqqq \u02dd \u03c30 \u02dd . . . \u03c30 \u02dd \u03c1p1qq \u02d8 pxq,\nthen pp\u2018 qqpxq \u201c maxtppxq, qpxqu can also be written as a neural network with maxtlp, lqu ` 1 layers:\npp\u2018 qqpxq \u201c \u03c3\u00b48 ` r\u03c30 \u02dd \u03c11spypxqq ` r\u03c30 \u02dd \u03c12spypxqq \u00b4 r\u03c30 \u02dd \u03c13spypxqq \u02d8 ,\nwhere y : Rd \u00d1 R2 is given by ypxq \u201c pppxq, qpxqq and \u03c1i : R2 \u00d1 R, i \u201c 1, 2, 3, are linear functions defined by\n\u03c11pyq \u201c y1 \u00b4 y2, \u03c12pyq \u201c y2, \u03c13pyq \u201c \u00b4y2.\nThus, by induction, any tropical polynomial can be written as a neural network with ReLU activation. Observe also that if a tropical polynomial is the tropical sum of r monomials, then it can be written as a neural network with no more than rlog2 rs` 1 layers.\nNext we consider a tropical rational function ppm qqpxq \u201c ppxq \u00b4 qpxq where p and q are tropical polynomials. Under the same assumptions, we can represent pm q as\nppm qqpxq \u201c \u03c3\u00b48 ` r\u03c30 \u02dd \u03c14spypxqq \u00b4 r\u03c30 \u02dd \u03c15spypxqq ` r\u03c30 \u02dd \u03c16spypxqq \u00b4 r\u03c30 \u02dd \u03c17spypxqq \u02d8\nwhere \u03c1i : R2 \u00d1 R2, i \u201c 4, 5, 6, 7, are linear functions defined by\n\u03c14pyq \u201c y1, \u03c15pyq \u201c \u00b4y1, \u03c16pyq \u201c \u00b4y2, \u03c17pyq \u201c y2.\nTherefore pm q is also a neural network with at most maxtlp, lqu ` 1 layers.\nFinally, if f and g are tropical polynomials that are respectively tropical sums of rf and rg monomials, then the discussions above show that pf m gqpxq \u201c fpxq \u00b4 gpxq is a neural network with at most maxtrlog2 rf s, rlog2 rgsu ` 2 layers."}, {"heading": "D.4. Proof of Proposition 5.5", "text": "Proof. It remains to establish the \u201cif\u201d part. Let Rd be divided into N polyhedral region on each of which \u03bd restricts to a linear function `ipxq \u201c aTix` bi, ai P Zd, bi P R, i \u201c 1, . . . , L, i.e., for any x P Rd, \u03bdpxq \u201c `ipxq for some i P t1, . . . , Lu. It follows from (Tarela & Martinez, 1999) that we can find N subsets of t1, . . . , Lu, denoted by Sj , j \u201c 1, . . . , N , so that \u03bd has a representation\n\u03bdpxq \u201c max j\u201c1,...,N min iPSj `i.\nIt is clear that each `i is a tropical rational function. Now for any tropical rational functions p and q,\nmintp, qu \u201c \u00b4maxt\u00b4p,\u00b4qu \u201c 0m rp0m pq \u2018 p0m qqs \u201c rpd qs m rp\u2018 qs.\nSince pd q and p\u2018 q are both tropical rational functions, so is their tropical quotient. By induction, miniPSj `i is a tropical rational function for any j \u201c 1, . . . , N , and therefore so is their tropical sum \u03bd."}, {"heading": "D.5. Proof of Proposition 5.6", "text": "Proof. For a one-layer neural network \u03bdpxq \u201c maxtAx ` b, tu \u201c p\u03bd1pxq, . . . , \u03bdppxqq with A P Rp\u02c6d, b P Rp, x P Rd, t P pRY t\u00b48uqp, we have\n\u03bdkpxq \u201c \u02c6 bk d d \u00e4\nj\u201c1 x akj j\n\u02d9 \u2018 tk \u201c \u02c6 bk d d \u00e4\nj\u201c1 x akj j\n\u02d9 \u2018 \u02c6 tk d d \u00e4\nj\u201c1 x0j\n\u02d9\n, k \u201c 1, . . . , p.\nSo for any k \u201c 1, . . . , p, if we write b\u03041 \u201c bk, b\u03042 \u201c tk, a\u03041j \u201c akj , a\u03042j \u201c 0, j \u201c 1, . . . , d, then\n\u03bdkpxq \u201c 2 \u00e0\ni\u201c1 b\u0304i\nd \u00e4 j\u201c1 x a\u0304ij j\nis clearly a tropical signomial function. Therefore \u03bd is a tropical signomial map. The result for arbitrary number of layers then follows from using the same recurrence as in the proof in Section D.2, except that now the entries in the weight matrix are allowed to take real values, and the maps Hplqpxq, Gplqpxq, F plqpxq are tropical signomial maps. Hence every layer can be written as a tropical rational signomial map \u03bdplq \u201c F plq mGplq."}, {"heading": "D.6. Proof of Proposition 6.1", "text": "We prove a slightly more general result.\nProposition D.1 (Level sets). Let f m g P Ratpd, 1q \u201c Tpx1, . . . , xdq.\n(i) Given a constant c \u0105 0, the level set B :\u201c tx P Rd : fpxq m gpxq \u201c cu\ndivides Rd into at most N pfq connected polyhedral regions where fpxq m gpxq \u0105 c, and at most N pgq such regions where fpxq m gpxq \u0103 c.\n(ii) If c P R is such that there is no tropical monomial in fpxq that differs from any tropical monomial in gpxq by c, then the level set B is contained in a tropical hypersurface,\nB \u010e T pmaxtfpxq, gpxq ` cuq \u201c T pcd g \u2018 fq.\nProof. We show that the bounds on the numbers of connected positive (i.e., above c) and negative (i.e., below c) regions are as we claimed in (i). The tropical hypersurface of f divides Rd into N pfq convex regions C1, . . . , CN pfq such that f is linear on each Ci. As g is piecewise linear and convex over Rd, f m g \u201c f \u00b4 g is piecewise linear and concave on each Ci. Since the level set tx : fpxq \u00b4 gpxq \u201c cu and the superlevel set tx : fpxq \u00b4 gpxq \u011b cu must be convex by the concavity of f \u00b4 g, there is at most one positive region in each Ci. Therefore the total number of connected positive regions cannot exceed N pfq. Likewise, the tropical hypersurface of g divides Rd into N pgq convex regions on each of which f m g is convex. The same argument shows that the number of connected negative regions does not exceed N pgq.\nWe next address (ii). Upon rearranging terms, the level set becomes\nB \u201c x P Rd : fpxq \u201c gpxq ` c ( .\nSince fpxq and gpxq ` c are both tropical polynomial, we have\nfpxq \u201c b1x\u03b11 \u2018 \u00a8 \u00a8 \u00a8 \u2018 brx\u03b1r , gpxq ` c \u201c c1x\u03b21 \u2018 \u00a8 \u00a8 \u00a8 \u2018 csx\u03b2s ,\nwith appropriate multiindices \u03b11, . . . , \u03b1r, \u03b21, . . . , \u03b2s, and real coefficients b1, . . . , br, c1, . . . , cs. By the assumption on the monomials, we have that x0 P B only if there exist i, j so that \u03b1i \u2030 \u03b2j and bix\u03b1i0 \u201c cjx \u03b2j 0 . This completes the proof since if we combine the monomials of fpxq and gpxq ` c by (tropical) summing them into a single tropical polynomial, maxtfpxq, gpxq ` cu, the above implies that on the level set, the value of the combined tropical polynomial is attained by at least two monomials and therefore x0 P T pmaxtfpxq, gpxq ` cuq.\nProposition 6.1 follows immediately from Proposition D.1 since the decision boundary tx P Rd : \u03bdpxq \u201c s\u00b41pcqu is a level set of the tropical rational function \u03bd."}, {"heading": "D.7. Proof of Theorem 6.3", "text": "The linear regions of a tropical polynomial map F P Polpd,mq are all convex but this is not necessarily the case for a tropical rational map F P Ratpd, nq. Take for example a bivariate real-valued function fpx, yq whose graph in R3 is a pyramid with base tpx, yq P R2 : x, y P r\u00b41, 1su and zero everywhere else, then the linear region where f vanishes is R2ztpx, yq P R2 : x, y P r\u00b41, 1su, which is nonconvex. The nonconvexity invalidates certain geometric arguments that only apply in the convex setting. Nevertheless there is a way to subdivide each of the nonconvex linear regions into convex ones to get ourselves back into the convex setting. We will start with the number of convex linear regions for tropical rational maps although later we will deduce the required results for the number of linear regions (without imposing convexity).\nWe first extend the notion of tropical hypersurface to tropical rational maps: Given a tropical rational map F P Ratpd,mq, we define T pF q to be the boundaries between adjacent linear regions. When F \u201c pf1, . . . , fmq P Polpd,mq, i.e., a tropical polynomial map, this set is exactly the union of tropical hypersurfaces T pfiq, i \u201c 1, . . . ,m. Therefore this definition of T pF q extends Definition 3.1.\nFor a tropical rational map F , we will examine the smallest number of convex regions that form a refinement of T pF q. For brevity, we will call this the convex degree of F ; for consistency, the number of linear regions of F we will call its linear degree. We define convex degree formally below. We will write F |C to mean the restriction of map F to C \u010e Rd. Definition D.1. The convex degree of a tropical rational map F P Ratpd, nq is the minimum division of Rd into convex regions over which F is linear, i.e.\nNcpF q :\u201c min n : C1 Y \u00a8 \u00a8 \u00a8 Y Cn \u201c Rd, Ci convex, F |Ci linear ( .\nNote that C1, . . . , CNcpF q either divide Rd into the same regions as T pF q or form a refinement.\nFor m \u010f d, we will denote by NcpF | mq the maximum convex degree obtained by restricting F to an m-dimensional affine subspace in Rd, i.e.,\nNcpF | mq :\u201c max NcpF |\u2126q : \u2126 \u010e Rd is an m-dimensional affine space ( .\nFor any F P Ratpd, nq, there is at least one tropical polynomial map that subdivides T pF q, and so convex degree is welldefined (e.g., if F \u201c pp1 m q1, . . . , pn m qnq P Ratpd, nq, then we may choose P \u201c pp1, . . . , pn, q1, . . . , qnq P Polpd, 2nq). Since the linear regions of a tropical polynomial map are always convex, we have N pF q \u201c NcpF q for any F P Polpd, nq.\nLet F \u201c pf1, . . . , fnq P Ratpd, nq and \u03b1 \u201c pa1, . . . , anq P Zn. Consider the tropical rational function3\nF\u03b1 :\u201c \u03b1TF \u201c a1f1 ` \u00a8 \u00a8 \u00a8 ` anfn \u201c n \u00e4\nj\u201c1 f aj j P Ratpd, 1q.\nFor some \u03b1, F\u03b1 may have fewer linear regions than F , e.g, \u03b1 \u201c p0, . . . , 0q. As such, we need the following notion. Definition D.2. \u03b1 \u201c pa1, . . . , anq P Zn is said to be a general exponent of F P Ratpd, nq if the linear regions of F\u03b1 and the linear regions of F are identical.\nWe show that general exponent always exists for any F P Ratpd, nq and may be chosen to have all entries nonnegative. Lemma D.2. Let F P Ratpd, nq. Then\n(i) N pF\u03b1q \u201c N pF q if and only if \u03b1 is a general exponent; (ii) F has a general exponent \u03b1 P Nn.\nProof. It follows from the definition of tropical hypersuface that T pF\u03b1q and T pF q comprise respectively the points x P Rd at which F\u03b1 and F are not differentiable. Hence T pF\u03b1q \u010e T pF q, which implies that N pF\u03b1q \u0103 N pF q unless T pF\u03b1q \u201c T pF q. This concludes (i).\nFor (ii), we need to show that there always exists an \u03b1 P Nn such that F\u03b1 divides its domain Rd into the same set of linear regions as F . In other words, for every pair of adjacent linear regions of F , the pd \u00b4 1q-dimensional face in T pF q that separates them is also present in T pF\u03b1q and so T pF\u03b1q \u011a T pF q.\nLet L and M be adjacent linear regions of F . The differentials of F |L and F |M must have integer coordinates, i.e., dF |L, dF |M P Zn\u02c6d. Since L and M are distinct linear regions, we must have dF |L \u2030 dF |M (or otherwise L and M can be merged into a single linear region). Note that the differentials of F\u03b1|L and F\u03b1|M are given by \u03b1TdF |L and \u03b1TdF |M .\nTo ensure the pd\u00b4 1q-dimensional face separating L and M still exists in T pF\u03b1q, we need to choose \u03b1 so that \u03b1TdF |L \u2030 \u03b1TdF |M . Observe that the solution to pdF |L \u00b4 dF |M qT\u03b1 \u201c 0 is contained in a one-dimensional subspace of Rn.\nLet ApF q be the collection of all pairs of adjacent linear regions of F . Since the set of \u03b1 that degenerates two adjacent linear regions into a single one, i.e.,\nS :\u201c \u010f\npL,MqPApF q\n\u03b1 P Nn : pdF |L \u00b4 dF |M q T\u03b1 \u201c 0q\n(\n,\nis contained in a union of a finite number of hyperplanes in Rn, S cannot cover the entire lattice of nonnegative integers Nn. Therefore the set Nn X pRnzSq is nonempty and any of its element is a general exponent for F .\nLemma D.2 shows that we may study the linear degree of a tropical rational map by studying that of a tropical rational function, for which the results in Section 3.1 apply.\nWe are now ready to prove a key result on the convex degree of composition of tropical rational maps.\nTheorem D.3. Let F \u201c pf1, . . . , fmq P Ratpn,mq and G P Ratpd, nq. Define H \u201c ph1, . . . , hmq P Ratpd,mq by\nhi :\u201c fi \u02ddG, i \u201c 1, . . . ,m.\nThen N pHq \u010f NcpHq \u010f NcpF | dq \u00a8NcpGq.\nProof. Only the upper bound requires a proof. Let k \u201c NcpGq. By the definition of NcpGq, there exist convex sets C1, . . . , Ck \u010e Rd whose union is Rd and on each of which G is linear. So G|Ci is some affine function \u03c1i. For any i,\nNcpF \u02dd \u03c1iq \u010f NcpF | dq, 3This is in the sense of a tropical power but we stay consistent to our slight abuse of notation and write F\u03b1 instead of Fd\u03b1.\nby the definition of NcpF | dq. Since F \u02ddG \u201c F \u02dd \u03c1i on Ci, we have\nNcpF \u02ddGq \u010f k \u00ff\ni\u201c1 NcpF \u02dd \u03c1iq.\nHence\nNcpF \u02ddGq \u010f k \u00ff\ni\u201c1 NcpF \u02dd \u03c1iq \u010f\nk \u00ff i\u201c1 NcpF | dq \u201c NcpF | dq \u00a8NcpGq.\nWe now apply our observations on tropical rational functions to neural networks. The next lemma follows directly from Corollary 3.4.\nLemma D.4. Let \u03c3plq \u02dd \u03c1plq : Rnl\u00b41 \u00d1 Rnl where \u03c3plq and \u03c1plq are the affine transformation and activation of the lth layer of a neural network. If d \u010f nl, then\nNcp\u03c3plq \u02dd \u03c1plq | dq \u010f d \u00ff i\u201c0 pnl i q .\nProof. Ncp\u03c3plq \u02dd \u03c1plq | dq is the maximum convex degree of a tropical rational map F \u201c pf1, . . . , fnlq : Rd \u00d1 Rnl of the form\nfipxq :\u201c \u03c3plqi \u02dd \u03c1 plq \u02dd pb1 d x\u03b11 , . . . , bnl\u00b41 d x \u03b1nl\u00b41 q, i \u201c 1, . . . , nl.\nFor a general affine transformation \u03c1plq,\n\u03c1plqpb1 d x\u03b11 , . . . , bnl\u00b41 d x \u03b1nl\u00b41 q \u201c ` b11 d x\u03b1 1 1 , . . . , b1nl d x \u03b11nl \u02d8 \u201c: Gpxq\nfor some \u03b111, . . . , \u03b1 1 nl and b11, . . . , b 1 nl , and we denote this map by G : Rd \u00d1 Rnl . So fi \u201c \u03c3plqi \u02ddG. By Theorem D.3, we have Ncp\u03c3plq \u02dd \u03c1plq | dq \u201c Ncp\u03c3plq | dq \u00a8NcpGq \u201c Ncp\u03c3plq | dq; note that NcpGq \u201c 1 as G is a linear function.\nWe have thus reduced the problem to determining a bound on the convex degree of a single layer neural network with nl nodes \u03bd \u201c p\u03bd1, . . . , \u03bdnlq : Rd \u00d1 Rnl . Let \u03b3 \u201c pc1, . . . , cnlq P Nnl be a nonnegative general exponent for \u03bd. Note that\nnl \u00e4 j\u201c1 \u03bd cj j \u201c nl \u00e4 j\u201c1 \u201e\u02c6 d \u00e4 i\u201c1 bi d xa ` ji \u02d9 \u2018 \u02c6 d \u00e4 i\u201c1 xa \u00b4 ji \u02d9 d tj cj \u00b4 nl \u00e4 j\u201c1 \u02c6 d \u00e4 i\u201c1 xa \u00b4 ji \u02d9cj .\nSince the last term is linear in x, we may drop it without affecting the convex degree of the entire expression. It remains to determine an upper bound for the number of linear regions of the tropical polynomial\nhpxq \u201c nl \u00e4\nj\u201c1\n\u201e\u02c6 d \u00e4\ni\u201c1 bi d xa\n` ji\n\u02d9 \u2018 \u02c6 d \u00e4\ni\u201c1 xa \u00b4 ji\n\u02d9 d tj cj ,\nwhich we will obtain by counting vertices of the polytope Pphq. By Propositions 3.1 and 3.2 the polytope Pphq is given by a weighted Minkowski sum\nnl \u00ff j\u201c1 cjP \u201e\u02c6 d \u00e4 i\u201c1 bi d xa ` ji \u02d9 \u2018 \u02c6 d \u00e4 i\u201c1 xa \u00b4 ji \u02d9 d tj  .\nBy Proposition 3.2 again,\nP \u201e\u02c6 d \u00e4\ni\u201c1 bi d xa\n` ji\n\u02d9 \u2018 \u02c6 d \u00e4\ni\u201c1 xa \u00b4 ji\n\u02d9 d tj  \u201c Conv ` VpPpfqq Y VpPpgqq \u02d8\nwhere\nfpxq \u201c d \u00e4\ni\u201c1 bi d xa\n` ji and gpxq \u201c\n\u02c6 d \u00e4\ni\u201c1 xa \u00b4 ji\n\u02d9\nd tj\nare tropical monomials. Therefore Ppfq, Ppgq are just points in Rd`1 and Conv ` VpPpfqq Y VpPpgqq \u02d8 is a line in Rd`1. Hence Pphq is a Minkowski sum of nl line segments in Rd`1, i.e., a zonotope, and Corollary 3.4 completes the proof.\nUsing Lemma D.4, we obtain a bound on the number of linear regions created by one layer of a neural network.\nTheorem D.5. Let \u03bd : Rd \u00d1 RnL be an L-layer neural network satisfying assumptions (a)\u2013(c) with F plq, Gplq,Hplq, and \u03bdplq as defined in Proposition 5.1. Let nl \u011b d for all l \u201c 1, . . . , L. Then\nNcp\u03bdp1qq \u201c N pGp1qq \u201c N pHp1qq \u201c 1, Ncp\u03bdpl`1qq \u010f Ncp\u03bdplqq \u00a8 d \u00ff i\u201c0 pnl`1 i q .\nProof. The l \u201c 1 case follows from the fact that Gp1qpxq \u201c Ap1q\u00b4 x and Hp1qpxq \u201c A p1q ` x` bp1q are both linear, which in turn forces Ncp\u03bdp1qq \u201c 1 as in the proof of Lemma D.4. Since \u03bdplq \u201c p\u03c3plq \u02dd \u03c1plqq \u02dd \u03bdpl\u00b41q, the recursive bound follows from Theorem D.3 and Lemma D.4.\nTheorem 6.3 follows from applying Theorem D.5 recursively."}], "year": 2018, "references": [{"title": "Understanding deep neural networks with rectified linear units", "authors": ["Arora", "Raman", "Basu", "Amitabh", "Mianjy", "Poorya", "Mukherjee", "Anirbit"], "venue": "International Conference on Learning Representations,", "year": 2018}, {"title": "Neural machine translation by jointly learning to align and translate", "authors": ["Bahdanau", "Dzmitry", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "year": 2014}, {"title": "On the expressive power of deep architectures", "authors": ["Bengio", "Yoshua", "Delalleau", "Olivier"], "venue": "In International Conference on Algorithmic Learning Theory,", "year": 2011}, {"title": "Convex optimization", "authors": ["Boyd", "Stephen", "Vandenberghe", "Lieven"], "year": 2004}, {"title": "Max-linear systems: theory and algorithms", "authors": ["Butkovi\u010d", "Peter"], "venue": "Springer Science & Business Media,", "year": 2010}, {"title": "Shallow vs. deep sum-product networks", "authors": ["Delalleau", "Olivier", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2011}, {"title": "The power of depth for feedforward neural networks", "authors": ["Eldan", "Ronen", "Shamir", "Ohad"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "Max-plus convex geometry", "authors": ["Gaubert", "St\u00e9phane", "Katz", "Ricardo"], "venue": "In International Conference on Relational Methods in Computer Science. Springer,", "year": 2006}, {"title": "On the interpretation of whitney numbers through arrangements of hyperplanes, zonotopes, non-radon partitions, and orientations of graphs", "authors": ["Greene", "Curtis", "Zaslavsky", "Thomas"], "venue": "Transactions of the American Mathematical Society,", "year": 1983}, {"title": "Minkowski addition of polytopes: computational complexity and applications to gr\u00f6bner bases", "authors": ["Gritzmann", "Peter", "Sturmfels", "Bernd"], "venue": "SIAM Journal on Discrete Mathematics,", "year": 1993}, {"title": "Zonotopes as bounding volumes", "authors": ["Guibas", "Leonidas J", "Nguyen", "An", "Zhang", "Li"], "venue": "In Proceedings of 14th Annual ACM-SIAM Symposium on Discrete Algorithms,", "year": 2003}, {"title": "On functions representable as a difference of convex functions", "authors": ["Hartman", "Philip"], "venue": "Pacific Journal of Mathematics,", "year": 1959}, {"title": "Tropical algebraic geometry, volume 35", "authors": ["Itenberg", "Ilia", "Mikhalkin", "Grigory", "Shustin", "Eugenii I"], "venue": "Springer Science & Business Media,", "year": 2009}, {"title": "Recurrent continuous translation models", "authors": ["Kalchbrenner", "Nal", "Blunsom", "Phil"], "venue": "In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,", "year": 2013}, {"title": "Imagenet classification with deep convolutional neural networks", "authors": ["Krizhevsky", "Alex", "Sutskever", "Ilya", "Hinton", "Geoffrey E"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Introduction to tropical geometry, volume 161", "authors": ["Maclagan", "Diane", "Sturmfels", "Bernd"], "venue": "American Mathematical Society,", "year": 2015}, {"title": "On the number of linear regions of deep neural networks", "authors": ["Montufar", "Guido F", "Pascanu", "Razvan", "Cho", "Kyunghyun", "Bengio", "Yoshua"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Exponential expressivity in deep neural networks through transient chaos", "authors": ["Poole", "Ben", "Lahiri", "Subhaneil", "Raghu", "Maithra", "SohlDickstein", "Jascha", "Ganguli", "Surya"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "On the expressive power of deep neural networks", "authors": ["Raghu", "Maithra", "Poole", "Ben", "Kleinberg", "Jon", "Ganguli", "Surya", "Sohl-Dickstein", "Jascha"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "The dc (difference of convex functions) programming and dca revisited with dc models of real world nonconvex optimization problems", "authors": ["Tao", "Pham Dinh", "Hoai An", "Le Thi"], "venue": "Annals of Operations Research,", "year": 2005}, {"title": "Region configurations for realizability of lattice piecewise-linear models", "authors": ["JM Tarela", "Martinez", "MV"], "venue": "Mathematical and Computer Modelling,", "year": 1999}, {"title": "Understanding deep learning requires rethinking generalization", "authors": ["Zhang", "Chiyuan", "Bengio", "Samy", "Hardt", "Moritz", "Recht", "Benjamin", "Vinyals", "Oriol"], "year": 2016}], "id": "SP:52b767e22072f323121ea14358e215ac7ce2487d", "authors": [{"name": "Liwen Zhang", "affiliations": []}, {"name": "Gregory Naitzat", "affiliations": []}, {"name": "Lek-Heng Lim", "affiliations": []}], "abstractText": "We establish, for the first time, connections between feedforward neural networks with ReLU activation and tropical geometry \u2014 we show that the family of such neural networks is equivalent to the family of tropical rational maps. Among other things, we deduce that feedforward ReLU neural networks with one hidden layer can be characterized by zonotopes, which serve as building blocks for deeper networks; we relate decision boundaries of such neural networks to tropical hypersurfaces, a major object of study in tropical geometry; and we prove that linear regions of such neural networks correspond to vertices of polytopes associated with tropical rational functions. An insight from our tropical formulation is that a deeper network is exponentially more expressive than a shallow network.", "title": "Tropical Geometry of Deep Neural Networks"}