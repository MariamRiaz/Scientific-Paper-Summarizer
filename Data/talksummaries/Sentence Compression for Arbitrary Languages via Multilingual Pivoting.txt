0	24	Sentence compression aims to produce a summary of a single sentence that retains the most important information while preserving its fluency.
1	88	The task has attracted much attention due to its potential for applications such as text summarization (Jing, 2000; Madnani et al., 2007; Woodsend and Lapata, 2010; Berg-Kirkpatrick et al., 2011), subtitle generation (Vandeghinste and Pan, 2004; Luotolahti and Ginter, 2015), and the display of text on small-screens (Corston-Oliver, 2001).
2	67	The bulk of research on sentence compression has focused on a simplification of the task involving exclusively word deletion (Knight and Marcu, 2002; Riezler et al., 2003; Turner and Charniak, 2005; McDonald, 2006; Clarke and Lapata, 2008; Cohn and Lapata, 2009), whereas a few approaches view sentence compression as a more general text rewriting problem (Galley and McKeown, 2007; Woodsend and Lapata, 2010; Cohn and Lapata, 2013).
3	13	Irrespective of how the compression task is formulated, most previous work relies on syntactic information such as parse trees to help decide what to delete from a sentence or which rules to learn in order to rewrite a sentence using less words.
5	20	Filippova et al. (2015) focus on deletion-based sentence compression which they model as a sequence labeling problem using a recurrent neural network with long short-term memory units (LSTM; Hochreiter and Schmidhuber 1997).
13	34	Since large scale compression datasets do not occur naturally, they must be somehow approximated, e.g., by pairing headlines with the first sentence of a news article (Filippova and Altun, 2013; Rush et al., 2015).
26	41	A few models have been proposed for Japanese (Hori and Furui, 2004; Hirao et al., 2009; Harashima and Kurohashi, 2012), including a neural network model (Hasegawa et al., 2017) which repurposes Filippova and Altun’s (2013) data construction method for Japanese.
27	25	There is a compression corpus available for French (de Loupy et al., 2010), however, we are not aware of any modeling work on this language.
28	12	Overall, there are no standardized datasets in languages other than English, either for training or testing.
29	173	Our contributions in this work are three-fold: a novel application of bilingual pivoting to sentence compression; corroborated by empirical results showing that our model scales across languages and text genres without additional supervision over and above what is available in the bilingual parallel data; and the release of a multilingual, multi-reference compression corpus which can be effectively used to gain insight in the compression task and facilitate further research in compression modeling.
30	13	In our pivot-based sentence compression model an input sequence is first translated into a foreign language, and then back into the source language.
31	18	Unlike previous paraphrasing pivoting models (Mallinson et al., 2017), we parameterize our translation models with a length feature, which allows us to produce compressed output.
62	46	In Figure 1 we illustrate how the pivot-based model sketched above can successfully control the output of the generated compressions.
63	111	We show the output of a single-step compression model on three languages initialized with varying compression rates3 (see Section 4 for details on how the models were trained and tested).
64	27	The compression rate (CR) is used to determine length parameter of Equation (8): Ty′ = Tx ·CR (11) The figure shows how the output length varies compared to a vanilla encoder-decoder system which uses pivoting to backtranslate the source language (Mallinson et al., 2017).
65	17	We can see that the majority of sentences are generated with length close to the desired compression rate.
66	112	For evaluation purposes, we created a multilingual sentence compression corpus in English, German, and French.
67	16	The corpus was collated from existing document and sentence aligned multilingual datasets which vary both in terms of topic and genre.
72	29	The News Commentary Parallel Corpus contains articles downloaded from Project Syndicate, an international media organization that publishes commentary on global topics (e.g., economics, world affairs).
81	14	As can be seen, Europarl contains the longest sentences across languages (see column SL), TED contains the shortest sentences, while the other two corpora are somewhere in-between.
83	54	Overall, French speakers seem more conservative when shortening sentences compared to English and German.
86	14	We used TER to compute the (average) number of edits required to change a long sentence to shorter output.
90	30	Neural Machine Translation Training Nematus (Sennrich et al., 2017) was used as the machine translation system for all our experiments.
92	14	All networks have a hidden layer size of 1,000, and an embedding layer size of 512.
104	18	The compression rate (see Equation (8)) was tuned experimentally on the validation set which consists of one document from each domain (20 source sentences; 100 compression-pairs).
108	18	The dataset consists of approximately 4 million pairs of the first sentence from each source document and its headline.
109	38	We also trained LenInit (Kikuchi et al., 2016) on the same corpus which is conceptually similar to ABS but additionally controls the output length using a length embedding vector (as described in Section 2.2).5 Unfortunately, we could not train these models for French or German, since there are no monolingual sentence compression datasets available at a similar scale.
111	50	As the quality of the translation is relatively poor, we also translated German or French into English, compressed it with ABS and LenInit trained on the Gigaword corpus, and then translated the compressions back to French or German.
113	11	MOSS Evaluation We assessed model performance using three automatic metrics which represent different aspects of the compression task and have been found to correlate well with human judgments (Toutanova et al., 2016; Clarke and Lapata, 2006).
118	42	All pivot-based models perform compression in a single step (see Section 2.3).
