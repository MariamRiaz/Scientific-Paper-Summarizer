23	36	In recent NLP work the term reproducibility was used when trying to get identical results on the same data (Névéol et al., 2016; Marrese-Taylor and Matsuo, 2017).
32	21	In Sections 5 and 6 we demonstrate how to apply the proposed frameworks to two synthetic data toy examples and four NLP applications: multidomain dependency parsing, multilingual POS tagging, cross-domain sentiment classification, and word similarity prediction with word embedding models.
63	44	In contrast, our goal is to count and identify the datasets for which one algorithm significantly outperforms the other, which provides more intricate information, especially when the datasets come from different sources.
69	19	We start by formulating a general hypothesis testing framework for a comparison between two algorithms.
70	41	This is a common type of hypothesis testing framework applied in NLP, its detailed formulation will help us develop our ideas.
78	26	We define the difference in performance between two algorithms, A and B, according to the measure Mj on the dataset Xi as: δj(X i) =Mj(A,Xi)−Mj(B,Xi).
83	24	Rejection of the null hypothesis when it is true is termed type I error, and non-rejection of the null hypothesis when the alternative is true is termed type II error.
84	37	The classical approach to hypothesis testing is to find a test that guarantees that the probability of making a type I error is upper bounded by a predefined constant α, the test significance level, while achieving as low probability of type II error as possible, a.k.a “achieving as high power as possible”.
91	72	Denoting the total number of type I errors as V , we can see below that if the test statistics are independent then the probability of making at least one incorrect rejection is 0.994: P(V > 0) = 1− P(V = 0) = 1− 100∏ i=1 P(no type I error in i) =1− (1− 0.05)100.
94	23	The multiple testing literature proposes various procedures for bounding the probability of making at least one type I error, as well as other, less restrictive error criteria (see a survey in Farcomeni (2007)).
96	19	While identifying the datasets gives more information when compared to just declaring their number, we consider these two questions separately.
99	34	We start by reformulating the set of hypothesis testing problems of Equation 1 as a unified hypothesis testing problem.
125	21	Below, we describe two such methods and their properties.
130	39	Our recommendation is hence to use the Bonferroni’s method when the datasets are dependent and to use the more powerful Fisher’s method when the datasets are independent.
132	38	The partial conjunction p−values are: p u/N Bonferroni = (N − u+ 1)p(u) (2) p u/N Fisher = P ( χ22(N−u+1) ≥ −2 N∑ i=u ln p(i) ) (3) where χ22(N−u+1) denotes a chi-squared random variable with 2(N − u+ 1) degrees of freedom.
138	52	Fisher’s method rejects the global null for large values of −2∑Ni=1 ln p(i), or equivalently for small values of∏N i=1 pi.
140	31	Fisher’s method requires a small enough product of p−values as evidence that at least one null hypothesis is false.
161	19	This is a simple p−value based procedure that is concordant with the partial conjunction analysis when pu/NBonferroni is used in that analysis.
165	25	The Holm procedure for identifying the datasets with a significant effect is given below.
166	39	Procedure Holm 1) Let k be the minimal index such that p(k) > α N+1−k .
174	108	Each p-value is compared to a threshold which is smaller or equal to α and depends on the number of evaluation datasets N. The dependence of the thresholds on N can be intuitively explained as follows: the probability of making one or more erroneous claims may increase with N, as demonstrated in Section 3.2.
179	76	Based on Section 4.3 we suggest to answer the identification question of Section 1 by reporting the rejection list returned by the Holm procedure.
182	82	Indeed, to the best of our knowledge, the nature of the dependency between dependent test sets in NLP work has not been analyzed before.
189	53	We sample the 100 p−values from a standard uniform distribution, which is the p−value distribution under the null hypothesis, repeating the simulation 1,000 times.
195	28	To consider a scenario where a dependency between the participating datasets does exist, we consider a second toy example.
199	22	We estimate the probability that k̂ > k = 0 for the three k̂ estimators based on the 1000 repetitions and get the values of: P̂ (k̂count > k) = 0.943, P̂ (k̂Bonferroni > k) = 0.046 and P̂ (k̂Fisher > k) = 0.234.
200	168	This simulation demonstrates the importance of using Bonferroni’s method rather than Fisher’s method when the datasets are dependent, even if some of the datasets are independent.
201	24	In this section we demonstrate the potential impact of replicability analysis on the way experimental results are analyzed in NLP setups.
202	101	We explore four NLP applications: (a) two where the datasets are independent: multi-domain dependency parsing and multilingual POS tagging; and (b) two where dependency between the datasets does exist: cross-domain sentiment classification and word similarity prediction with word embedding models.
208	68	Train and test set size (in sentences) range from 6672 to 34,492 and from 280 to 2327, respectively (see Table 1 of Choi et al. (2015)).
