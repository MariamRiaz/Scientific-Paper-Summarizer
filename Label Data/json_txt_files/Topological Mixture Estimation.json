{"sections": [{"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Background", "text": "Density functions that represent sample data are often multimodal, i.e. they exhibit more than one maximum. Typically this behavior indicates that the underlying data deserves a more detailed representation as a mixture of densities with individually simpler structure. The usual specification of a component density is quite restrictive, with log-concave the most general case considered in the literature, and Gaussian the overwhelmingly typical case. It is also necessary to determine the number of mixture components a priori, and much art is devoted to this.\nIn this paper we detail how to efficiently determine a topologically and information-theoretically optimal mixture of generic unimodal component densities directly from a onedimensional input density and without any auxiliary information whatsoever. The topological criterion is a natural qualitative alternative to more traditional quantitative model selection criteria (e.g., information criteria) and is computed at the outset of computation, then subsequently preserved, while the information-theoretical criterion optimally sepa-\n1BAE Systems FAST Labs, Arlington, VA, USA. Correspondence to: Steve Huntsman <steve.huntsman@baesystems.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nrates component densities. We further show how to optimally smooth the mixture when the input density itself is being estimated. Topological persistence (which operationally amounts to the assignment of significance to topological features that persist as a function of scale) is the essential ingredient in both the \u201cmodel selection\u201d and smoothing."}, {"heading": "1.2. Formal Motivation", "text": "To give some formal motivation, letD(Rd) denote a suitable space of continuous probability densities (henceforth merely called densities) on Rd. A mixture on Rd with M components is a pair (\u03c0, p) \u2208 \u2206\u25e6M \u00d7 D(Rd)M , where \u2206\u25e6M := {\u03c0 \u2208 (0, 1]M : \u2211 m \u03c0m = 1}; we write |(\u03c0, p)| := M , and note that \u03c0 cannot have any components equal to zero. The corresponding mixture density is \u3008\u03c0, p\u3009 := \u2211M m=1 \u03c0mpm. The Jensen-Shannon divergence of (\u03c0, p) is (Brie\u0308t & Harremoe\u0308s, 2009)\nJ(\u03c0, p) := H (\u3008\u03c0, p\u3009)\u2212 \u3008\u03c0,H(p)\u3009 (1) where H(p)m := H(pm) and H(f) := \u2212 \u222b f log f dx is the entropy of f .\nNow J(\u03c0, p) is the mutual information between the random variables \u039e \u223c \u03c0 and X \u223c \u3008\u03c0, p\u3009. Since mutual information is always nonnegative, the same is true of J . The concavity of H gives the same result, i.e. H (\u3008\u03c0, p\u3009) \u2265 \u3008\u03c0,H(p)\u3009. If M := |(\u03c0, p)| > 1, \u03c0\u0302 := (\u03c01, . . . , \u03c0M\u22122, \u03c0M\u22121 + \u03c0M ), and p\u0302 := ( p1, . . . , pM\u22122,\n\u03c0M\u22121pM\u22121+\u03c0MpM \u03c0M\u22121+\u03c0M\n) , then is easy\nto show that J(\u03c0\u0302, p\u0302) \u2264 J(\u03c0, p).\nWe say that a density f \u2208 D(Rd) is unimodal if f\u22121([y,\u221e)) is either empty or contractible (i.e., topologically equivalent to a point in the sense of homotopy) for all y. For d = 1, this simply means that any nonempty sets f\u22121([y,\u221e)) are intervals and agrees with intuition. We call a mixture (\u03c0, p) unimodal iff each of the component densities pm is unimodal. The unimodal category ucat(f) is the smallest number of components of any unimodal mixture (\u03c0, p) that satisfies \u3008\u03c0, p\u3009 = f . Figure 1 shows that the unimodal category can be much less than the number of maxima. In the event that \u3008\u03c0, p\u3009 = f and |(\u03c0, p)| = ucat(f), we write (\u03c0, p) |= f : the symbol |= is called \u201cmodels.\u201d The unimodal category is a topological invariant that generalizes and relates to other classical invariants (Baryshnikov & Ghrist, 2011; Ghrist, 2014).\nThe preceding constructions naturally lead us to consider the unimodal Jensen-Shannon divergence\nJ\u2229(f) := sup (\u03c0,p)|=f J(\u03c0, p) (2)\nas a simultaneous measure of both the topological and information-theoretical complexity of f , and\n(\u03c0\u2229, p\u2229) := arg max (\u03c0,p)|=f J(\u03c0, p) (3)\nas a topologically and information-theoretically optimal topological mixture estimate (TME).\nThe natural questions are if such an estimate exists (is the supremum attained?), is unique, and if so, how to perform TME in practice. In this paper we address these questions for the case d = 1, and we demonstrate the utility of TME in examples (see Figures 6-10).\nAfter reviewing related work in \u00a72, we cover the basic algorithm of TME in \u00a73. The proof therein that Algorithm 1 computes (3) is nearly trivial with Lemmas 1 and 2 in hand: these are respectively in appendices \u00a7A and \u00a7B. Next, in \u00a74 we review the related technique of topological density estimation (TDE) before showing in \u00a75 how blurring and deblurring mixture estimates can usefully couple TDE and TME. Finally, in \u00a76 we produce examples of TME in action before making some closing remarks in \u00a77."}, {"heading": "2. Related Work", "text": "While density estimation enables various clustering techniques (Li et al., 2007; Azzalini & Menardi, 2014; Xu & Tian, 2015), mixture estimation is altogether more powerful than clustering: e.g., it is possible to have mixture components that significantly and meaningfully overlap. For example, a cluster with a bimodal density will usually be considered as arising from two unimodal mixture components that are individually of interest. In this light and in view of its totally nonparametric nature, our approach can be seen as particularly powerful, particularly when coupled with TDE and deblurring/reblurring (see \u00a74 and \u00a75).\nStill, even for clustering (even in one dimension, where an optimal solution to k-means can be computed efficiently (Wang & Song, 2011; Nielsen & Nock, 2014; Gr\u00f8nlund et al., 2017)), determining the number of clusters in data (Feng & Hamerly, 2007; Mirkin, 2011) is as much an art as a science. All of the techniques we are aware of either require some ad hoc determination to be made, require auxiliary information (e.g., (Tibshirani et al., 2001)) or are parametric in at least a limited sense (e.g., (Sugar & James, 2003)). While a parametric approach allows likelihoods and thus various information criteria (Burnham & Anderson, 2003) or their ilk to be computed for automatically determining the number of clusters, this comes at the cost of a strong modeling assumption, and criteria values themselves are difficult to compare meaningfully (Melnykov & Maitra, 2010).\nThese shortcomings\u2013including determining the number of mixture components\u2013carry over to the more difficult problem of mixture estimation. (McLachlan & Peel, 2004; Melnykov & Maitra, 2010; McLachlan & Rathnayake, 2014) As an example, an ad hoc and empirically derived unimodal mixture estimation technique that requires one of a few common functional forms for the mixture components has been recently employed in (Mints & Hekker, 2017). Univariate model-based mixtures of skew distributions admit EM-type algorithms and can outperform Gaussian mixture models (Lin et al., 2007; Basso et al., 2010). Though these generalize to the multivariate case quite effectively (see, e.g., (Lee & McLachlan, 2016)), the EM-type algorithms are generically vulnerable to becoming trapped in local minima without good initial parameter values, and they require some model selection criterion to determine the number of mixture components, though the parameter learning and model selection steps can be integrated as in (Figueiredo & Jain, 2002). A Bayesian nonparametric mixture model that incorporates many\u2013but not arbitrary\u2013unimodal distributions is considered in (Rodr\u0131\u0301guez & Walker, 2014). Principled work has been done on estimating mixtures of log-concave distributions (Walther, 2009) and (Chan et al., 2013) describes how densities of discrete unimodal mixtures can be estimated. However, actually estimating generic unimodal mixtures themselves appears to be unaddressed in the literature, even in one dimension. Indeed, even estimating individual modes and their associated uncertainties or significances has only been addressed recently (Genovese et al., 2016; Mukhopadhyay, 2017)."}, {"heading": "3. The Basic Algorithm", "text": "Given f , the \u201csweep\u201d algorithm of (Baryshnikov & Ghrist, 2011) yields (\u03c0, p) |= f . We will repeatedly perturb (\u03c0, p) to obtain (3) using Lemmas 1 and 2, which are respectively in \u00a7A and \u00a7B. Lemma 1 states that that J is convex under perturbations of (\u03c0, p) that preserve \u3008\u03c0, p\u3009. Lemma 2 is\nAlgorithm 1 Topological Mixture Estimation (TME) Input: function data {xk, f(xk)} Initialize (\u03c0, p) |= f as in (Baryshnikov & Ghrist, 2011) repeat\nfor each evaluation point and pair of components do Greedily perturb (\u03c0, p) to (\u03c0\u2032, p\u2032) using Lemma 2 end for Update (\u03c0, p) = arg max J(\u03c0\u2032, p\u2032)\nuntil (\u03c0, p) and/or J(\u03c0, p) converge Output: (\u03c0, p)\na characterization of perturbations of two components of a piecewise affine and continuous (or piecewise constant) mixture that preserve the predicate (\u03c0, p) |= f , i.e., that preserve unimodality (as in Figure 2) and the mixture density. Together, these results entail Theorem 1, which establishes that greedy unimodality- and density-preserving local perturbations of pairs of mixture components converge to (3).\nTheorem 1. Let \u2212\u221e = x\u22121 < x0 < \u00b7 \u00b7 \u00b7 < xN < xN+1 = \u221e and f be piecewise constant (or affine) over each [xk, xk+1]. Then Algorithm 1 efficiently computes (3).\nProof. By Lemma 1 (see \u00a7A), greedily and locally perturbing the mixture (\u03c0, p) |= f according to Lemma 2 (see \u00a7B), then updating the mixture according to the perturbation which optimizes J gives the desired result in O(MN) iterations. This result is unique by convexity. Each iteration requires O(M2N) trial perturbations, each of which in turn requires O(MN) arithmetic operations to evaluate J ."}, {"heading": "4. Topological Density Estimation", "text": "The obvious situation of practical interest for TME is that a density has been obtained from a preliminary estimation process involving some sample data. There is a natural approach to this preliminary estimation process called topological density estimation (TDE) (Huntsman, 2017) that naturally dovetails with TME.\nAlgorithm 2 Topological Density Estimation (TDE) Input: {Xj} for each proposed bandwidth h do\nCompute uX(h) using (5) end for Compute m\u0302X using (6) Output: h\u0302X using (7)"}, {"heading": "4.1. Idea", "text": "We recall the basic idea here (for pseudocode, see Algorithm 2). Given a kernel K and sample data Xj for 1 \u2264 j \u2264 n, and for each proposed bandwidth h, compute the kernel density estimate (Silverman, 1986; Chen, 2017)\nf\u0302h;X := 1\nn n\u2211 j=1 KXj ,h (4)\nwhere K\u00b5,\u03c3(x) := 1\u03c3K( x\u2212\u00b5 \u03c3 ). Next, compute\nuX(h) := ucat(f\u0302h;X) (5)\nand estimate the unimodal category of the PDF that X is sampled from via\nm\u0302X := arg max m\n\u00b5(u\u22121X (m)) (6)\nwhere \u00b5 denotes an appropriate measure (nominally counting measure or the pushforward of Lebesgue measure under the transformation h 7\u2192 1/h).\n(6) gives the most prevalent and topologically persistent (Ghrist, 2014; Oudot, 2015) value of the unimodal category, i.e., this is a topologically robust estimate of the number of components required to produce the PDF that X is sampled from as a mixture. While any element of u\u22121X (m\u0302X) is a bandwidth consistent with the estimate (6), considerations of robustness lead us to typically make the more detailed nominal specification\nh\u0302X := median\u00b5(u\u22121X (m\u0302X)). (7)"}, {"heading": "4.2. Performance", "text": "TDE turns out to be very computationally efficient relative to the traditional technique of cross-validation (CV). On highly multimodal densities, TDE is competitive or at least reasonably performant relative to CV and other nonparametric density estimation approaches with respect to traditional statistical evaluation criteria. Moreover, TDE outperforms other approaches when qualitative criteria such as the number of local maxima and the unimodal category itself are considered (see Figures 3-5). In practice, such qualitative\ncriteria are generally of paramount importance. For example, precisely estimating the shape of a density is generally less important than determining if it has multiple modes.\nAs an illustration, consider \u00b5(j,m) := jm+1 , \u03c3(k,m) := 2\u2212(k+2)(m+ 1)\u22122 and the family of distributions\nfkm := 1\nm m\u2211 j=1 K\u00b5(j,m),\u03c3(k,m) (8)\nfor 1 \u2264 k \u2264 3 and 1 \u2264 m \u2264 10, and where here K is the standard Gaussian density: see Figure 3. Exhaustive details relating to the evaluation of TDE on this family and other densities are in the software package and test suite (BAE Systems, 2017): here, we merely show performance data for (8) in Figures 4 and 5.\nTDE has the very useful feature (shared by essentially no high-performing density estimation technique other than\nCV) that it requires no free parameters or assumptions. Indeed, TDE can be used to evaluate its own suitability: for unimodal distributions, it is often not an ideal choice\u2013but it is good at detecting this situation in the first place. Furthermore, TDE is very efficient computationally.\nIn situtations of practical interest, it is tempting to couple TDE and TME in the obvious way: i.e., perform them sequentially and indepdently. This yields a completely nonparametric estimate of a mixture from sample data alone. However, there is a much better way to couple these techniques, as we shall see in the sequel."}, {"heading": "5. Blurring and Deblurring", "text": ""}, {"heading": "5.1. Blurring", "text": "Recall that a log-concave function is unimodal, and moreover that a function is log-concave iff its convolutions with unimodal functions are identically unimodal (Ibragimov, 1956; Keilson & Gerber, 1971; Bertin et al., 2013). This observation naturally leads to the following question: if (\u03c0, p) |= f , how good of an approximation to the \u03b4 distribution must a log-concave density g be in order to have (\u03c0, p\u2217g) |= f \u2217g? In particular, suppose that g is a Gaussian density: what bandwidth must it have? An answer to this question of how much blurring a minimal unimodal mixture model can sustain defines a topological scale (viz., the persistence of the unimodal category under blurring) that we proceed to illustrate in light of TDE.\nIn this paragraph we assume thatK is the standard Gaussian density, so that K\u00b5,h \u2217 K\u00b5\u2032,h\u2032 = K\u00b5+\u00b5\u2032,(h2+h\u20322)1/2 and f\u0302h;X \u2217K0,h\u2032 = f\u0302(h2+h\u20322)1/2;X . Write h\u0302X for the bandwidth obtained via TDE, whether via the nominal specification (7) or any other: by construction we have that inf u\u22121X (m\u0302X) \u2264\nh\u0302X \u2264 supu\u22121X (m\u0302X). Now if (\u03c0, p) |= f\u0302h\u0302X ;X , then m\u0302X = uX(h\u0302X) = |(\u03c0, p)|. In order to have (\u03c0, p \u2217 K0,h\u2032) |= f \u2217 K0,h\u2032 , it must be that m\u0302X = uX(h\u0302X) = |(\u03c0, p)| = |(\u03c0, p \u2217K0,h\u2032)| = uX((h\u03022X + h\u20322)1/2), i.e.,\nh\u2032 \u2264 ([ supu\u22121X (m\u0302X) ]2 \u2212 h\u03022X)1/2 . (9)\nIn particular, we have the weaker inequality involving a purely topological scale:\nh\u2032 \u2264 ([ supu\u22121X (m\u0302X) ]2 \u2212 [inf u\u22121X (m\u0302X)]2)1/2 . (10)\nThe preceding considerations generalize straightforwardly if we define uf (h) := ucat(f \u2217K0,h), where once againK is a generic kernel. This generalizes (5) so long as we associate sample data with a uniform average of \u03b4 distributions. Under reasonable conditions, we can write uf (0) = ucat(f), and it is easy to see that the analogous bound is\nh\u2032 \u2264 supu\u22121f (uf (0)). (11)\nOf course, (11) merely restates the triviality that the blurred mixture ceases to be minimal precisely when the number of mixture components exceeds the unimodal category of the mixture density. Meanwhile, the special case furnished by TDE with the standard Gaussian kernel affords sufficient structure for a slightly less trivial statement."}, {"heading": "5.2. Deblurring/reblurring", "text": "The considerations of \u00a75.1 suggest how to couple TDE and TME in a much more effective way than performing them sequentially and independently. The idea is to use a Gaussian kernel and instead of (7), pick the bandwidth\nh\u0302 (\u2212) X := inf\u00b5(u \u22121 X (m\u0302X)) (12)\nand then perform TME; finally, convolve the results with K0,\u2206h where\n\u2206h := ( h\u03022X \u2212 [ h\u0302 (\u2212) X ]2)1/2 . (13)\nThis preserves the result of TDE while giving a smoother, less artificial, and more practically useful mixture estimate than the information theoretically optimal result.\nOf course, a similar tactic can be performed directly on a density f by considering its Fourier deconvolution F\u22121(Ff/FK0,h\u2032), where F denotes the Fourier transform and h\u2032 is as in (11): however, any a priori justification for such a tactic is necessarily context-dependent in general, and our experience suggests that its implementation would be delicate and/or prone to aliasing. Nevertheless, this would\nAlgorithm 3 Reblurred Topological Mixture Estimation Input: {Xj} for each proposed bandwidth h do\nCompute uX(h) using (5) end for Compute m\u0302X using (6) Compute h\u0302(\u2212)X using (12) Compute (\u03c0, p) from f\u0302\nh\u0302 (\u2212) X ;X using Algorithm 1 Update p = p \u2217K0,\u2206h with \u2206h as in (13) Output: (\u03c0, p)\nbe particularly desirable in the context of heavy-tailed distributions, where kernel density estimation requires much larger sample sizes in order to achieve acceptable results. In this context it would also be worth considering the use of a symmetric stable density (Uchaikin & Zolotarev, 1999; Nolan, 2018) (e.g., a Cauchy density) as a kernel with the aim of recapturing the essence of (13)."}, {"heading": "6. Examples", "text": "We present two phenomenologically illustrative examples. First, in Figures 6 and 7 we consider the n = 272 waiting times between eruptions of the Old Faithful geyser from the data set in (Ha\u0308rdle, 2012). Then, in Figures 8 and 9 we consider the n = 2107 Sloan Digitial Sky Survey g \u2212 r color indices accessed from the VizieR database (Ochsen-\nbein et al., 2000) at http://cdsarc.u-strasbg. fr/viz-bin/Cat?J/ApJ/700/523 and discussed in (An et al., 2009); the latter example is replicated in (BAE Systems, 2018).\nAs suggested, several phenomena are readily apparent from these examples. First, mixtures obtained via the sweep algorithm are manifestly parity-dependent, i.e., the direction of sweeping matters; second, mixtures obtained via TME alone exhibit artificial anti-overlapping behavior; third, deblurring followed by reblurring preserves unimodality, the overall density, a topologically persistent invariant (viz., the unimodal category) and the spirit of information-theoretical optimality while producing an obviously better behaved mixture; fourth and finally, the various techniques involved here can significantly shift classification/decision boundaries based on the dominance of various mixture components.\nWhile the data in Figures 6 and 7 is at least qualitatively approximated by a two-component Gaussian mixture, it\nis clear that a three-component Gaussian mixture cannot capture the highly oscillatory behavior of the density in Figures 8 and 9. Indeed, this example illustrates how such oscillatory behavior can actually arise from a unimodal mixture with many fewer components than might naively appear to be required.\nFigure 10 shows that there are strong and independent grounds to conclude that the color index data of Figure 9 is produced by a unimodal mixture of three components, with componentwise modes as suggested by TME, and furthermore that CV undersmooths this data. For each density estimate shown, each componentwise maximum of the corresponding mixture (3) is virtually identical to one of the local maxima of the density estimate itself: this is a consequence of the anti-overlapping tendency described above.\nIn particular, the fourth panel of Figure 10 illustrates that it is possible and potentially advantageous to use TME as an alternative mode identification technique in the LPMode algorithm of (Mukhopadhyay, 2017). Furthermore, while we have not implemented a reliable Fourier deconvolution/reblurring algorithm of the sort hinted at in \u00a75.2, the fifth and sixth panels of Figure 10 suggest that this is not particularly important for the narrowly defined task of mode finding/bump hunting."}, {"heading": "7. Remarks", "text": "While the O(M4N3) arithmetic operations of TME as implemented in Algorithm 1 might seem uncomfortably high, in practice M is generally quite small and it is reasonable to enforce N = 102 as in fact we do in the MATLAB implementation (BAE Systems, 2018) and \u00a76. Furthermore, avoiding redundant perturbations and incrementally computing J would dramatically reduce the computational complexity. Still, even without the benefit of any such refinements, the color index example in \u00a76 runs in less than 40 seconds.\nNote also that TDE and de/reblurring as respectively implemented in Algorithms 2 and 3 are relatively computationally inexpensive. The former case is helped by resampling data to on the order of 103 quantiles, which has no material effect on the output in cases where kernels are appropriate to use. That said, using a fast generalized Gauss transform (Spivak et al., 2010) would dramatically accelerate TDE.\nEven though TME is presently limited to one dimension, it is still very useful due to the preponderance of onedimensional problems. For instance, TME is a good candidate to improve on some of the best practical unsupervised image thresholding techniques (Kapur et al., 1985; Kittler & Illingworth, 1986; Sezgin & Sankur, 2004), and has prospects for enhancing data/sensor fusion, informationtheoretical analysis of time series (by using sliding time windows to define samples), and many other tasks.\nFurthermore, the one-dimensional framework can be used with random projections of high-dimensional data in a way that is likely to yield improvements for model selection (Feng & Hamerly, 2007) (cf. (Kalai et al., 2012)) and anomaly detection (Pevny\u0301, 2016). We plan to explore these topics in future work, with the associated intent of gauging the art of the possible with respect to determining unimodal decompositions in dimension > 1.\nThat said, the extension of TME to dimension > 1 will require effort and mathematical tools well beyond those used in this paper. One reason is that computing a unimodal decomposition is algorithmically undecidable in high dimensions, a property inherited from the problem of determining contractibility of simplicial complexes (Tancer, 2016) and the geometric realization theorem (Edelsbrunner & Harer, 2010) applied to level and upper excursion sets. Therefore, extending the constructions of this paper will require some modification of the notion of unimodal category in dimension > 1, approximations and/or heuristics.\nFor example, restricting to convex versus contractible upper excursion sets is probably desirable on intuitive as well as computational grounds. However, even in two dimensions the corresponding problem of constructing minimal convex partitions of polygons is still NP-hard. On the other hand, the case without interior holes is efficiently solvable (O\u2019Rourke et al., 2017) and there is a quasi-polynomial time approximation scheme for the general case (Bandyapadhyay et al., 2015). The heuristic of (Liu et al., 2010) seems to be a good starting point for exploring relevant tradeoffs.\nAn appropriate tactic for directly leveraging the onedimensional framework en route to higher dimensions appears to be tomography in the spirit of the topological Radon transform (Ghrist, 2014). Besides working with random projections in this vein, another sensible approach (suggested to the author by Robert Ghrist) is to foliate (Lawson Jr., 1974) the domain of a sample (in practice this would just mean taking a family of parallel lines) and perform TME on data in tubular neighborhoods of nearby leaves of the foliation, then assemble the results, essentially by interpolating. Here topological tools such as sheaves and Morse theory seem inevitably to be required in order to do things in a globally coherent way. A suitable member of the class of metrics on mixtures introduced in (Liu & Huang, 2000) that provides data relevant for assembly as a byproduct will likely also be necessary.\nFinally, we note that there are prospects for recursively coupling TME and TDE: the idea here is to pull mixture components back to weighted subsamples, then re-run TDE (or in the unimodal case, CV) on these individually. The resulting variable-bandwidth mixture estimator would give a multiresolution description of data that would be be likely to yield further improvements in many applications."}, {"heading": "A. Convexity", "text": "The following lemma shows that J is convex as we gradually shift part of one mixture component to another.\nLemma 1. Let |(\u03c0, p)| = 3 and define\n\u03c012,t := \u03c01 + (1\u2212 t)\u03c02; \u03c023,t := t\u03c02 + \u03c03; p12,t := \u03c01p1 + (1\u2212 t)\u03c02p2\n\u03c012,t ;\np23,t := t\u03c02p2 + \u03c03p3\n\u03c023,t ,\nso that \u3008(\u03c012,t, \u03c023,t) , (p12,t, p23,t)\u3009 = \u3008\u03c0, p\u3009. The function g\u03c0,p : [0, 1]\u2192 [0,\u221e) defined by\ng\u03c0,p(t) := J ((\u03c012,t, \u03c023,t) , (p12,t, p23,t)) (14)\nsatisfies\ng\u03c0,p(t) \u2264 t \u00b7 g\u03c0,p(1) + (1\u2212 t) \u00b7 g\u03c0,p(0). (15)\nProof. We have that g\u03c0,p(t) = H(\u3008\u03c0, p\u3009) \u2212 \u3008(\u03c012,t, \u03c023,t) , (H(p12,t), H(p23,t))\u3009. Furthermore, if we write \u03c012 := \u03c01 + \u03c02, \u03c023 := \u03c02 + \u03c03, p12 := \u03c01p1+\u03c02p2\u03c012 , and p23 := \u03c02p2+\u03c03p3\u03c023 , then\n\u03c012,t = t\u03c01 + (1\u2212 t)\u03c012; \u03c023,t = t\u03c023 + (1\u2212 t)\u03c03; p12,t = t\u03c01p1 + (1\u2212 t)\u03c012p12\n\u03c012,t ;\np23,t = t\u03c023p23 + (1\u2212 t)\u03c03p3\n\u03c023,t .\nIt is well known that H is a concave functional: from this it follows that\nH(p12,t) \u2265 t\u03c01 \u03c012,t H(p1) + (1\u2212 t)\u03c012 \u03c012,t H(p12);\nH(p23,t) \u2265 t\u03c023 \u03c023,t H(p23) + (1\u2212 t)\u03c03 \u03c023,t H(p3).\nTherefore\ng\u03c0,p(t) = H(\u3008\u03c0, p\u3009) \u2212\u3008(\u03c012,t, \u03c023,t) , (H(p12,t), H(p23,t))\u3009\n\u2264 H(\u3008\u03c0, p\u3009)\u2212 t\u03c01H(p1)\u2212 (1\u2212 t)\u03c012H(p12) \u2212t\u03c023H(p23)\u2212 (1\u2212 t)\u03c03H(p3)\n= tH(\u3008\u03c0, p\u3009)\u2212 t\u3008(\u03c01, \u03c023), (H(p1), H(p23))\u3009 +(1\u2212 t) \u00b7H(\u3008\u03c0, p\u3009) \u2212(1\u2212 t) \u00b7 \u3008(\u03c012, \u03c03), (H(p12), H(p3))\u3009\n= t \u00b7 g\u03c0,p(1) + (1\u2212 t) \u00b7 g\u03c0,p(0)\nas claimed."}, {"heading": "B. Preserving Unimodality", "text": "Suppose that (\u03c0, p) is a unimodal mixture on R with |(\u03c0, p)| > 1. We would like to determine how we can perturb two components of this mixture so that the result is still unimodal and yields the same density. In the event that the mixture is piecewise affine and continuous (or piecewise constant) the space of permissible perturbations can be characterized by the following\nLemma 2. For 0 \u2264 k \u2264 N , let yk \u2208 [0,\u221e) be such that y0 = 0 = yN and there are integers `, u satisfying 0 < ` \u2264 u < N with\ny0 \u2264 \u00b7 \u00b7 \u00b7 \u2264 y`\u22121 < y` = \u00b7 \u00b7 \u00b7 = yu > yu+1 \u2265 \u00b7 \u00b7 \u00b7 \u2265 yN . (16) (That is, y1, . . . , yN\u22121 is a nonnegative, nontrivial unimodal sequence.) Then for 1 \u2264 r \u2264 N\u22121 and \u03b5\u2212r \u2265 0, yk\u2212\u03b4kr\u03b5\u2212r is nonnegative and unimodal iff\n\u03b5\u2212r \u2264 yr \u2212min{yr\u22121, yr+1}. (17)\nSimilarly, for \u03b5+r \u2265 0, yk + \u03b4kr\u03b5+r is nonnegative and unimodal iff\n\u03b5+r \u2264 { \u221e if `\u2212 1 \u2264 k \u2264 u+ 1 max{yr\u22121, yr+1} \u2212 yr otherwise.\n(18)\nProof (sketch). We first sketch ((17),\u21d0). Nonegativity follows from 0 \u2264 min{yr\u22121, yr+1} \u2264 yr \u2212 \u03b5\u2212r . Unimodality follows from a series of trivial checks for the cases 1 \u2264 r < `\u22121, r = `\u22121, r = `, and ` < r < u: the remaining cases r = u, r = u+ 1, and u+ 1 < r \u2264 N \u2212 1 follow from symmetry. For example, in the case 1 \u2264 r < ` \u2212 1, we only need to show that yr\u22121 \u2264 yr \u2212 \u03b5\u2212r \u2264 yr+1.\nA sketch of ((17),\u21d2) amounts to using the same cases and symmetry argument to perform equally trivial checks. For example, in the case 1 \u2264 r < ` \u2212 1, we have \u03b5\u2212r \u2264 yr \u2212 yr\u22121 \u2264 yr \u2212min{yr\u22121, yr+1}.\nThe proof of (18) is mostly similar to that of (17): the key difference here is that any argument adjacent to or at a point where the maximum is attained can have its value increased arbitrarily without affecting unimodality (or nonnegativity).\nThe example in Figure 2 is probably more illuminating than filling in the details of the proof sketch above."}, {"heading": "Acknowledgements", "text": "The author thanks Adelchi Azzalini, Robert Ghrist, Subhadeep Mukhopadhyay, and John Nolan for their helpful and patient discussions, and reviewers for their comments and advice.\nThis material is based upon work supported by the Defense Advanced Research Projects Agency (DARPA) and the Air Force Research Laboratory (AFRL). Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA or AFRL."}], "year": 2018, "references": [{"title": "Galactic globular and open clusters in the Sloan Digital Sky Survey. II. Test of theoretical stellar isochrones", "authors": ["D. An", "M.H. Pinsonneault", "T. Masseron", "F. Delahaye", "J.A. Johnson", "D.M. Terndrup", "T.C. Beers", "I.I. Ivans", "\u017d. Ivezi\u0107"], "venue": "The Astrophysical Journal,", "year": 2009}, {"title": "Clustering via nonparametric density estimation: The R package pdfCluster", "authors": ["A. Azzalini", "G. Menardi"], "venue": "Journal of Statistical Software, Articles,", "year": 2014}, {"title": "Approximation schemes for partitioning: convex decomposition and surface approximation", "authors": ["S. Bandyapadhyay", "S. Bhowmick", "K. Varadarajan"], "venue": "In Proceedings of SODA,", "year": 2015}, {"title": "Unimodal category and topological statistics", "authors": ["Y. Baryshnikov", "R. Ghrist"], "venue": "In Proceedings of NOLTA,", "year": 2011}, {"title": "Robust mixture modeling based on scale mixtures of skew-normal distributions", "authors": ["R.M. Basso", "V.H. Lachos", "C.R.B. Cabral", "P. Ghosh"], "venue": "Computational Statistics & Data Analysis,", "year": 2010}, {"title": "Properties of classical and quantum Jensen-Shannon divergence", "authors": ["J. Bri\u00ebt", "P. Harremo\u00ebs"], "venue": "Physical Review A,", "year": 2009}, {"title": "Model Selection and Multimodel Inference: A Practical Information-theoretic Approach", "authors": ["K.P. Burnham", "D.R. Anderson"], "year": 2003}, {"title": "Learning mixtures of structured distributions over discrete domains", "authors": ["Chan", "S.-O", "I. Diakonikolas", "R.A. Servedio", "X. Sun"], "venue": "In Proceedings of SODA,", "year": 2013}, {"title": "A tutorial on kernel density estimation and recent advances", "authors": ["Chen", "Y.-C"], "year": 2017}, {"title": "PG-means: learning the number of clusters in data", "authors": ["Y. Feng", "G. Hamerly"], "venue": "In Proceedings of NIPS,", "year": 2007}, {"title": "Unsupervised learning of finite mixture models", "authors": ["M.A.T. Figueiredo", "A.K. Jain"], "venue": "IEEE Transactions on Pattern Analysis and Machine Intelligence,", "year": 2002}, {"title": "Non-parametric inference for density modes", "authors": ["C.R. Genovese", "M. Perone-Pacifico", "I. Verdinelli", "L. Wasserman"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2016}, {"title": "Fast exact k-means, k-medians and Bregman divergence clustering in 1d", "authors": ["A. Gr\u00f8nlund", "K.G. Larsen", "A. Mathiasen", "J.S. Nielsen"], "year": 2017}, {"title": "Smoothing Techniques: With Implementation in S", "authors": ["W. H\u00e4rdle"], "venue": "Springer Science & Business Media,", "year": 2012}, {"title": "Topological density estimation", "authors": ["S. Huntsman"], "venue": "In Proceedings of NOLTA,", "year": 2017}, {"title": "On the composition of unimodal distributions", "authors": ["I.A. Ibragimov"], "venue": "Theory of Probability & Its Applications,", "year": 1956}, {"title": "A new method for gray-level picture thresholding using the entropy of the histogram", "authors": ["J.N. Kapur", "P.K. Sahoo", "A.K.C. Wong"], "venue": "Computer Vision, Graphics, and Image Processing,", "year": 1985}, {"title": "Some results for discrete unimodality", "authors": ["J. Keilson", "H. Gerber"], "venue": "Journal of the American Statistical Association,", "year": 1971}, {"title": "Minimum error thresholding", "authors": ["J. Kittler", "J. Illingworth"], "venue": "Pattern Recognition,", "year": 1986}, {"title": "Finite mixtures of canonical fundamental skew t-distributions", "authors": ["S.X. Lee", "G.J. McLachlan"], "venue": "Statistics and Computing,", "year": 2016}, {"title": "A nonparametric statistical approach to clustering via mode identification", "authors": ["J. Li", "S. Ray", "B.G. Lindsay"], "venue": "Journal of Machine Learning Research,", "year": 2007}, {"title": "Robust mixture modeling using the skew t distribution", "authors": ["T.I. Lin", "J.C. Lee", "W.J. Hsieh"], "venue": "Statistics and Computing,", "year": 2007}, {"title": "Convex shape decomposition", "authors": ["H. Liu", "W. Liu", "L.J. Latecki"], "venue": "In Proceedings of CVPR,", "year": 2010}, {"title": "A new distance measure for probability distribution function of mixture type", "authors": ["Z. Liu", "Q. Huang"], "venue": "In Proceedings of ICASSP,", "year": 2000}, {"title": "Finite Mixture Models", "authors": ["G.J. McLachlan", "D. Peel"], "year": 2004}, {"title": "On the number of components in a Gaussian mixture model", "authors": ["G.J. McLachlan", "S. Rathnayake"], "venue": "Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,", "year": 2014}, {"title": "Finite mixture models and model-based clustering", "authors": ["V. Melnykov", "R. Maitra"], "venue": "Statistics Surveys,", "year": 2010}, {"title": "A unified tool to estimate distances, ages, and masses (UniDAM) from spectrophotometric data", "authors": ["A. Mints", "S. Hekker"], "venue": "Astronomy & Astrophysics,", "year": 2017}, {"title": "Choosing the number of clusters. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery", "authors": ["B. Mirkin"], "year": 2011}, {"title": "Large-scale mode identification and data-driven sciences", "authors": ["S. Mukhopadhyay"], "venue": "Electronic Journal of Statistics,", "year": 2017}, {"title": "Optimal interval clustering: Application to Bregman clustering and statistical mixture learning", "authors": ["F. Nielsen", "R. Nock"], "venue": "IEEE Signal Processing Letters,", "year": 2014}, {"title": "Stable Distributions: Models for Heavy Tailed Data", "authors": ["J.P. Nolan"], "year": 2018}, {"title": "The VizieR database of astronomical catalogues", "authors": ["F. Ochsenbein", "P. Bauer", "J. Marcout"], "venue": "Astronomy and Astrophysics Supplement Series,", "year": 2000}, {"title": "Persistence Theory: From Quiver Representations to Data Analysis", "authors": ["S.Y. Oudot"], "venue": "American Mathematical Society,", "year": 2015}, {"title": "Loda: Lightweight on-line detector of anomalies", "authors": ["T. Pevn\u00fd"], "venue": "Machine Learning,", "year": 2016}, {"title": "Univariate Bayesian nonparametric mixture modeling with unimodal kernels", "authors": ["C.E. Rodr\u0131\u0301guez", "S.G. Walker"], "venue": "Statistics and Computing,", "year": 2014}, {"title": "Survey over image thresholding techniques and quantitative performance evaluation", "authors": ["M. Sezgin", "B. Sankur"], "venue": "Journal of Electronic Imaging,", "year": 2004}, {"title": "Density Estimation for Statistics and Data Analysis", "authors": ["B.W. Silverman"], "venue": "CRC Press,", "year": 1986}, {"title": "The fast generalized Gauss transform", "authors": ["M. Spivak", "S.K. Veerapaneni", "L. Greengard"], "venue": "SIAM Journal on Scientific Computing,", "year": 2010}, {"title": "Finding the number of clusters in a dataset: An information-theoretic approach", "authors": ["C.A. Sugar", "G.M. James"], "venue": "Journal of the American Statistical Association,", "year": 2003}, {"title": "Recognition of collapsible complexes is NPcomplete", "authors": ["M. Tancer"], "venue": "Discrete and Computational Geometry,", "year": 2016}, {"title": "Estimating the number of clusters in a data set via the gap statistic", "authors": ["R. Tibshirani", "G. Walther", "T. Hastie"], "venue": "Journal of the Royal Statistical Society: Series B (Statistical Methodology),", "year": 2001}, {"title": "Chance and Stability: Stable Distributions and Their Applications", "authors": ["V.V. Uchaikin", "V.M. Zolotarev"], "venue": "Walter de Gruyter,", "year": 1999}, {"title": "Inference and modeling with log-concave distributions", "authors": ["G. Walther"], "venue": "Statistical Science, pp", "year": 2009}, {"title": "Ckmeans.1d.dp: optimal k-means clustering in one dimension by dynamic programming", "authors": ["H. Wang", "M. Song"], "venue": "The R Journal,", "year": 2011}, {"title": "A comprehensive survey of clustering algorithms", "authors": ["D. Xu", "Y. Tian"], "venue": "Annals of Data Science,", "year": 2015}], "id": "SP:d15584b10f2f8b636443f6ba4c4a3bc94dc78201", "authors": [{"name": "Steve Huntsman", "affiliations": []}], "abstractText": "We introduce topological mixture estimation, a completely nonparametric and computationally efficient solution to the problem of estimating a one-dimensional mixture with generic unimodal components. We repeatedly perturb the unimodal decomposition of Baryshnikov and Ghrist to produce a topologically and information-theoretically optimal unimodal mixture. We also detail a smoothing process that optimally exploits topological persistence of the unimodal category in a natural way when working directly with sample data. Finally, we illustrate these techniques through examples.", "title": "Topological Mixture Estimation"}