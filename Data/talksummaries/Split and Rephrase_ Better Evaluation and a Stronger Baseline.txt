0	27	Processing long, complex sentences is challenging.
1	24	This is true either for humans in various circumstances (Inui et al., 2003; Watanabe et al., 2009; De Belder and Moens, 2010) or in NLP tasks like parsing (Tomita, 1986; McDonald and Nivre, 2011; Jelı́nek, 2014) and machine translation (Chandrasekar et al., 1996; Pouget-Abadie et al., 2014; Koehn and Knowles, 2017).
2	78	An automatic system capable of breaking a complex sentence into several simple sentences that convey the same meaning is very appealing.
3	34	A recent work by Narayan et al. (2017) introduced a dataset, evaluation method and baseline systems for the task, naming it “Split-andRephrase”.
12	16	Digging further, we find that 99% of the simple sentences (more than 89% of the unique ones) in the validation and test sets also appear in the training set, which—coupled with the good memorization capabilities of SEQ2SEQ models and the relatively small number of distinct simple sentences—helps to explain the high BLEU score.
13	13	To aid further research on the task, we propose a more challenging split of the data.
19	26	The code and data to reproduce our results are available on Github.1 We encourage future work on the split-and-rephrase task to use our new data split or the v1.0 split instead of the original one.
20	90	Task Definition In the split-and-rephrase task we are given a complex sentence C, and need to produce a sequence of simple sentences T1, ..., Tn, n ≥ 2, such that the output sentences convey all and only the information in C. As additional supervision, the split-and-rephrase dataset associates each sentence with a set of RDF triples that describe the information in the sentence.
23	13	For evaluation we follow Narayan et al. (2017) and compute the averaged individual multi-reference BLEU score for each prediction.2 We split each prediction to sentences3 and report the average number of simple sentences in each prediction, and the average number of tokens for each simple sentence.
24	12	We train vanilla sequence-to-sequence models with attention (Bahdanau et al., 2015) as implemented in the OPENNMT-PY toolkit (Klein et al., 2017).4 Our models only differ in the LSTM cell size (128, 256 and 512, respectively).
27	22	HYBRIDSIMPL and SEQ2SEQ are text-to-text models, while the other reported baselines additionally use the RDF information.
34	32	All the unsupported facts seem to be related to entities mentioned in the source sentence.
35	62	Inspecting the attention weights (Figure 1) reveals a worrying trend: throughout the prediction, the model focuses heavily on the first word in of the first entity (“A wizard of Mars”) while paying little attention to other cues like “hardcover”, “Diane” and references of a specific complex sentence, and then average these numbers.
36	12	This explains the abundance of “hallucinated” unsupported facts: rather than learning to split and rephrase, the model learned to identify entities, and spit out a list of facts it had memorized about them.
40	18	To further illustrate the model’s recognize-andspit strategy, we compose inputs containing an entity string which is duplicated three times, as shown in the bottom two rows of Table 3.
41	22	As expected, the model predicted perfectly phrased and correct facts about the given entities, although these facts are clearly not supported by the input.
44	76	We split the data into train, development and test sets by randomly dividing the 5,554 distinct complex sentences across the sets, while using the provided RDF information to ensure that: 1.
45	15	Every possible RDF relation (e.g., BORNIN, LOCATEDIN) is represented in the training set (and may appear also in the other sets).
55	19	Copying is modeled using a “copy switch” probability p(z) computed by a sigmoid over a learned composition of the decoder state, the context vector and the last output embedding.
56	14	It interpolates the psoftmax distribution over the target vocabulary and a copy distribution pcopy over the source sentence tokens.
67	18	On the new data-split, as expected, the performance degrades for all models, as they are required to generalize to sentences not seen during training.
68	14	The copy-augmented models perform better than the baselines in this case as well, with a larger relative gap which can be explained by the lower lexical overlap between the train and the test sets in the new split.
70	14	Analysis We inspect the models’ predictions for the first 20 complex sentences of the original and new validation sets in Table 7.
71	18	We mark each simple sentence as being “correct” if it contains all and only relevant information, “unsupported” if it contains facts not present in the source, and “repeated” if it repeats information from a previous sentence.
76	39	On the original split, while SEQ2SEQ128 mainly suffers from missing information, perhaps due to insufficient memorization capacity, SEQ2SEQ512 generated the most unsupported sentences, due to overfitting or memorization.
78	11	We demonstrated that a SEQ2SEQ model can obtain high scores on the original split-and-rephrase task while not actually learning to split-andrephrase.
79	26	We propose a new and more challenging data-split to remedy this, and demonstrate that the cheating SEQ2SEQ models fail miserably on the new split.
80	30	Augmenting the SEQ2SEQ models with a copy-mechanism improves performance on both data splits, establishing a new competitive baseline for the task.
81	56	Yet, the split-and-rephrase task (on the new split) is still far from being solved.
82	36	We strongly encourage future research to evaluate on our proposed split or on the recently released version 1.0 of the dataset, which is larger and also addresses the overlap issues mentioned here.
