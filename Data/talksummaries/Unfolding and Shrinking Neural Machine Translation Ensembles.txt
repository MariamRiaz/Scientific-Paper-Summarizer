0	79	The top systems in recent machine translation evaluation campaigns on various language pairs use ensembles of a number of NMT systems (Bojar et al., 2016; Sennrich et al., 2016a; Chung et al., 2016; Neubig, 2016; Wu et al., 2016; Cromieres et al., 2016; Durrani et al., 2017).
1	27	Ensembling (Dietterich, 2000; Hansen and Salamon, 1990) of neural networks is a simple yet very effective technique to improve the accuracy of NMT.
3	27	The ensemble decoder computes predictions from each of the individual models which are then combined using the arithmetic average (Sutskever et al., 2014) or the geometric average (Cromieres et al., 2016).
13	23	Shrinking the unfolded network leads to a smaller model which consumes less space on the disk and in the memory; a crucial factor on mobile devices.
20	40	Depending on the aggressiveness of shrinking, we report either a gain of 2.2 BLEU at the same decoding speed, or a 3.4× CPU decoding speed up with only a minor drop in BLEU compared to the original single NMT system.
21	19	Furthermore, it is often much easier to stage a single NMT system than an ensemble in a commercial MT workflow, and it is crucial to be able to optimize quality at specific speed and memory constraints.
32	44	The first neurons in the hidden layer of the combined network correspond to the hidden layer in the first single network, and the others to the hidden layer of the second network.
34	29	The weight matrices in the unfolded network can be constructed by stacking the corresponding weight matrices (either horizontally or vertically) in network 1 and 2.
35	26	This kind of aggregation of multiple networks with the same topology is not only possible for single-layer feedforward architectures but also for complex networks consisting of multiple GRU layers and attention.
39	29	We denote the size of a layer in the individual models as s(d).
42	34	We explicitly allow d1 and d2 to be non-consecutive or reversed to be able to model recurrent networks.
54	55	After constructing the weight matrices of the unfolded network we reduce the size of it by iteratively shrinking layer sizes.
55	60	In this section we denote the incoming weight matrix of the layer to shrink as U ∈ Rmin×m and the outgoing weight matrix as V ∈ Rm×mout .
56	28	Our procedure is inspired by the method of Srinivas and Babu (2015).
59	24	If the incoming weight vectors U:,i and U:,j are exactly the same for two neurons i and j, we can remove the neuron j and add its outgoing connections to neuron i (Vi,: ← Vi,: + Vj,:) without changing the output.1 This holds since the activity in neuron j will always be equal to the activity in neuron i.
60	34	In practice, Srinivas and Babu use a distance measure based on the difference of the incoming weight vectors to search for similar neurons as exact matches are very rare.
61	21	The second intuition of the criterion used by Srinivas and Babu (2015) is that neurons with small outgoing weights contribute very little overall.
62	65	Therefore, they search for a pair of neurons i, j ∈ [1,m] according the following term and remove the j-th neuron.2 arg min i,j∈[1,m] ||U:,i − U:,j ||22||Vj,:||22 (1) Neuron j is selected for removal if (1) there is another neuron i which has a very similar set of incoming weights and if (2) j has a small outgoing weight vector.
96	33	Datafree shrinking uses the similarities between incoming weights, and data-bound shrinking uses neuron activities recorded during training.
124	31	Shrinking the Unfolded Network First, we investigate which shrinking methods are effective for which layers.
149	88	However, more aggressive shrinking yields a BLEU score of 25.3 when combining three systems (row (g)) – 1.8 BLEU better than the single system, but 0.6 BLEU worse than the 3- ensemble.
152	35	3 plots the BLEU score when isolated layers are shrunk even below their size in the original NMT network.
156	17	Adjusting the Target Sizes of Layers Based on our previous experiments we revise our approach to shrink the 3-Unfold system in Tab.
159	40	3-Unfold-Normal has the same number of parameters as the original NMT networks (size factor: 1.0), 3-UnfoldSmall is only half their size (size factor: 0.5), and 3-Unfold-Tiny reduces the size by two thirds (size factor: 0.33).
161	34	5 we observe that 3-Unfold-Normal yields a gain of 2.2 BLEU with respect to the original single system and a slight improvement in decoding speed at the same time.7 Networks with the size factor 1.0 like 3-Unfold-Normal are very likely to yield about the same decoding speed as the Single network regardless of the decoder implementation, machine learning framework, and hardware.
162	150	Therefore, we think that similar results are possible on other platforms as well.
163	37	CPU decoding speed directly benefits even more from smaller setups – 3-Unfold-Tiny is only 0.3 BLEU worse than Single but decoding on a single CPU is 3.4 times faster (row (a) vs. row (e) in Tab.
165	54	Our initial experiments in Tab.
188	160	Our approach involves unfolding an ensemble of multiple systems into a single large neural network and shrinking this network by removing redundant neurons.
189	52	Our best results on Japanese-English either yield a gain of 2.2 BLEU compared to the original single NMT network at about the same decoding speed, or a 3.4×CPU decoding speed up with only a minor drop in BLEU.
