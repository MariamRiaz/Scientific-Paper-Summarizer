0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 670–680 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics
Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1.",text,[0],[0]
"Distributed representations of words (or word embeddings) (Bengio et al., 2003; Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) have shown to provide useful features for various tasks in natural language processing and computer vision.",1 Introduction,[0],[0]
"While there seems to be a consensus concerning the usefulness of word embeddings and how to learn them, this is not yet clear with regard to representations that carry the meaning of a full sentence.",1 Introduction,[0],[0]
"That is, how to capture the
1https://www.github.com/ facebookresearch/InferSent
relationships among multiple words and phrases in a single vector remains an question to be solved.
",1 Introduction,[0],[0]
"In this paper, we study the task of learning universal representations of sentences, i.e., a sentence encoder model that is trained on a large corpus and subsequently transferred to other tasks.",1 Introduction,[0],[0]
"Two questions need to be solved in order to build such an encoder, namely: what is the preferable neural network architecture; and how and on what task should such a network be trained.",1 Introduction,[0],[0]
"Following existing work on learning word embeddings, most current approaches consider learning sentence encoders in an unsupervised manner like SkipThought (Kiros et al., 2015) or FastSent (Hill et al., 2016).",1 Introduction,[0],[0]
"Here, we investigate whether supervised learning can be leveraged instead, taking inspiration from previous results in computer vision, where many models are pretrained on the ImageNet (Deng et al., 2009) before being transferred.",1 Introduction,[0],[0]
"We compare sentence embeddings trained on various supervised tasks, and show that sentence embeddings generated from models trained on a natural language inference (NLI) task reach the best results in terms of transfer accuracy.",1 Introduction,[0],[0]
"We hypothesize that the suitability of NLI as a training task is caused by the fact that it is a high-level understanding task that involves reasoning about the semantic relationships within sentences.
",1 Introduction,[0],[0]
"Unlike in computer vision, where convolutional neural networks are predominant, there are multiple ways to encode a sentence using neural networks.",1 Introduction,[0],[0]
"Hence, we investigate the impact of the sentence encoding architecture on representational transferability, and compare convolutional, recurrent and even simpler word composition schemes.",1 Introduction,[0],[0]
"Our experiments show that an encoder based on a bi-directional LSTM architecture with max pooling, trained on the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015), yields state-of-the-art sentence embeddings com-
670
pared to all existing alternative unsupervised approaches like SkipThought or FastSent, while being much faster to train.",1 Introduction,[0],[0]
We establish this finding on a broad and diverse set of transfer tasks that measures the ability of sentence representations to capture general and useful information.,1 Introduction,[0.9529074935364661],"['We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.']"
"Transfer learning using supervised features has been successful in several computer vision applications (Razavian et al., 2014).",2 Related work,[0],[0]
"Striking examples include face recognition (Taigman et al., 2014) and visual question answering (Antol et al., 2015), where image features trained on ImageNet (Deng et al., 2009) and word embeddings trained on large unsupervised corpora are combined.
",2 Related work,[0],[0]
"In contrast, most approaches for sentence representation learning are unsupervised, arguably because the NLP community has not yet found the best supervised task for embedding the semantics of a whole sentence.",2 Related work,[0],[0]
"Another reason is that neural networks are very good at capturing the biases of the task on which they are trained, but can easily forget the overall information or semantics of the input data by specializing too much on these biases.",2 Related work,[0],[0]
Learning models on large unsupervised task makes it harder for the model to specialize.,2 Related work,[0],[0]
"Littwin and Wolf (2016) showed that co-adaptation of encoders and classifiers, when trained end-to-end, can negatively impact the generalization power of image features generated by an encoder.",2 Related work,[0],[0]
"They propose a loss that incorporates multiple orthogonal classifiers to counteract this effect.
",2 Related work,[0],[0]
"Recent work on generating sentence embeddings range from models that compose word embeddings (Le and Mikolov, 2014; Arora et al., 2017; Wieting et al., 2016b) to more complex neural network architectures.",2 Related work,[0],[0]
"SkipThought vectors (Kiros et al., 2015) propose an objective function that adapts the skip-gram model for words (Mikolov et al., 2013) to the sentence level.",2 Related work,[0],[0]
"By encoding a sentence to predict the sentences around it, and using the features in a linear model, they were able to demonstrate good performance on 8 transfer tasks.",2 Related work,[0],[0]
"They further obtained better results using layer-norm regularization of their model in (Ba et al., 2016).",2 Related work,[0],[0]
Hill et al. (2016) showed that the task on which sentence embeddings are trained significantly impacts their quality.,2 Related work,[0],[0]
"In addition to unsupervised methods, they included supervised training in their comparison—namely, on
machine translation data (using the WMT’14 English/French and English/German pairs), dictionary definitions and image captioning data from the COCO dataset (Lin et al., 2014).",2 Related work,[0],[0]
"These models obtained significantly lower results compared to the unsupervised Skip-Thought approach.
",2 Related work,[0],[0]
"Recent work has explored training sentence encoders on the SNLI corpus and applying them on the SICK corpus (Marelli et al., 2014), either using multi-task learning or pretraining (Mou et al., 2016; Bowman et al., 2015).",2 Related work,[0],[0]
"The results were inconclusive and did not reach the same level as simpler approaches that directly learn a classifier on top of unsupervised sentence embeddings instead (Arora et al., 2017).",2 Related work,[0.9525301987643194],"['These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.']"
"To our knowledge, this work is the first attempt to fully exploit the SNLI corpus for building generic sentence encoders.",2 Related work,[0],[0]
"As we show in our experiments, we are able to consistently outperform unsupervised approaches, even if our models are trained on much less (but humanannotated) data.",2 Related work,[0],[0]
"This work combines two research directions, which we describe in what follows.",3 Approach,[0],[0]
"First, we explain how the NLI task can be used to train universal sentence encoding models using the SNLI task.",3 Approach,[0],[0]
"We subsequently describe the architectures that we investigated for the sentence encoder, which, in our opinion, covers a suitable range of sentence encoders currently in use.",3 Approach,[0],[0]
"Specifically, we examine standard recurrent models such as LSTMs and GRUs, for which we investigate mean and maxpooling over the hidden representations; a selfattentive network that incorporates different views of the sentence; and a hierarchical convolutional network that can be seen as a tree-based method that blends different levels of abstraction.",3 Approach,[0.9508006575589272],"['Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.']"
"The SNLI dataset consists of 570k humangenerated English sentence pairs, manually labeled with one of three categories: entailment, contradiction and neutral.",3.1 The Natural Language Inference task,[0],[0]
"It captures natural language inference, also known in previous incarnations as Recognizing Textual Entailment (RTE), and constitutes one of the largest high-quality labeled resources explicitly constructed in order to require understanding sentence semantics.",3.1 The Natural Language Inference task,[0],[0]
"We hypothesize that the semantic nature of NLI makes it a good candidate for learning universal sentence
embeddings in a supervised way.",3.1 The Natural Language Inference task,[0],[0]
"That is, we aim to demonstrate that sentence encoders trained on natural language inference are able to learn sentence representations that capture universally useful features.
",3.1 The Natural Language Inference task,[0],[0]
"Models can be trained on SNLI in two different ways: (i) sentence encoding-based models that explicitly separate the encoding of the individual sentences and (ii) joint methods that allow to use encoding of both sentences (to use cross-features or attention from one sentence to the other).
",3.1 The Natural Language Inference task,[0],[0]
"Since our goal is to train a generic sentence encoder, we adopt the first setting.",3.1 The Natural Language Inference task,[0],[0]
"As illustrated in Figure 1, a typical architecture of this kind uses a shared sentence encoder that outputs a representation for the premise u and the hypothesis v.",3.1 The Natural Language Inference task,[0],[0]
"Once the sentence vectors are generated, 3 matching methods are applied to extract relations between u and v : (i) concatenation of the two representations (u, v); (ii) element-wise product u ∗ v; and (iii) absolute element-wise difference |u− v|.",3.1 The Natural Language Inference task,[0],[0]
"The resulting vector, which captures information from both the premise and the hypothesis, is fed into a 3-class classifier consisting of multiple fullyconnected layers culminating in a softmax layer.",3.1 The Natural Language Inference task,[0],[0]
"A wide variety of neural networks for encoding sentences into fixed-size representations exists, and it is not yet clear which one best captures generically useful information.",3.2 Sentence encoder architectures,[0],[0]
"We compare 7 different architectures: standard recurrent encoders with either Long Short-Term Memory (LSTM) or Gated Recurrent Units (GRU), concatenation of last hidden states of forward and backward GRU, Bi-directional LSTMs (BiLSTM)
with either mean or max pooling, self-attentive network and hierarchical convolutional networks.",3.2 Sentence encoder architectures,[0],[0]
"Our first, and simplest, encoders apply recurrent neural networks using either LSTM (Hochreiter and Schmidhuber, 1997) or GRU (Cho et al., 2014) modules, as in sequence to sequence encoders (Sutskever et al., 2014).",3.2.1 LSTM and GRU,[0.9526667419242145],"['For determining the sentiment, we used Stanford CoreNLP (Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).']"
"For a sequence of T words (w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ), the network computes a set of T hidden representations h1, . . .",3.2.1 LSTM and GRU,[0],[0]
", hT , with ht = −−−−→ LSTM(w1, . . .",3.2.1 LSTM and GRU,[0],[0]
", wT ) (or using GRU units instead).",3.2.1 LSTM and GRU,[0],[0]
"A sentence is represented by the last hidden vector, hT .
",3.2.1 LSTM and GRU,[0],[0]
"We also consider a model BiGRU-last that concatenates the last hidden state of a forward GRU, and the last hidden state of a backward GRU to have the same architecture as for SkipThought vectors.",3.2.1 LSTM and GRU,[0],[0]
"For a sequence of T words {wt}t=1,...,T , a bidirectional LSTM computes a set of T vectors {ht}t.",3.2.2 BiLSTM with mean/max pooling,[0],[0]
For t ∈,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", T ], ht, is the concatenation of a forward LSTM and a backward LSTM that read the sentences in two opposite directions:
−→ ht = −−−−→ LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )←−
",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"ht = ←−−−− LSTMt(w1, . . .",3.2.2 BiLSTM with mean/max pooling,[0],[0]
", wT )",3.2.2 BiLSTM with mean/max pooling,[0],[0]
ht =,3.2.2 BiLSTM with mean/max pooling,[0],[0]
"[ −→ ht , ←− ht ]
We experiment with two ways of combining the varying number of {ht}t to form a fixed-size vector, either by selecting the maximum value over each dimension of the hidden units (max pooling) (Collobert and Weston, 2008) or by considering the average of the representations (mean pooling).",3.2.2 BiLSTM with mean/max pooling,[0],[0]
"The self-attentive sentence encoder (Liu et al., 2016; Lin et al., 2017) uses an attention mechanism over the hidden states of a BiLSTM to generate a representation u of an input sentence.",3.2.3 Self-attentive network,[0],[0]
"The attention mechanism is defined as :
h̄i = tanh(Whi + bw)
αi = eh̄",3.2.3 Self-attentive network,[0],[0]
T,3.2.3 Self-attentive network,[0],[0]
"i uw∑
",3.2.3 Self-attentive network,[0],[0]
"i e h̄Ti uw u = ∑
t
αihi
where {h1, . . .",3.2.3 Self-attentive network,[0],[0]
", hT } are the output hidden vectors of a BiLSTM.",3.2.3 Self-attentive network,[0],[0]
"These are fed to an affine transformation (W , bw) which outputs a set of keys (h̄1, . . .",3.2.3 Self-attentive network,[0],[0]
", h̄T ).",3.2.3 Self-attentive network,[0],[0]
The {αi} represent the score of similarity between the keys and a learned context query vector uw.,3.2.3 Self-attentive network,[0],[0]
"These weights are used to produce the final representation u, which is a weighted linear combination of the hidden vectors.
",3.2.3 Self-attentive network,[0],[0]
"Following Lin et al. (2017) we use a selfattentive network with multiple views of the input sentence, so that the model can learn which part of the sentence is important for the given task.",3.2.3 Self-attentive network,[0],[0]
"Concretely, we have 4 context vectors u1w, u 2 w, u 3 w, u 4 w which generate 4 representations that are then concatenated to obtain the sentence representation u. Figure 3 illustrates this architecture.",3.2.3 Self-attentive network,[0],[0]
"One of the currently best performing models on classification tasks is a convolutional architecture termed AdaSent (Zhao et al., 2015), which concatenates different representations of the sentences
at different level of abstractions.",3.2.4 Hierarchical ConvNet,[0],[0]
"Inspired by this architecture, we introduce a faster version consisting of 4 convolutional layers.",3.2.4 Hierarchical ConvNet,[0],[0]
"At every layer, a representation ui is computed by a max-pooling operation over the feature maps (see Figure 4).
",3.2.4 Hierarchical ConvNet,[0],[0]
The final representation u =,3.2.4 Hierarchical ConvNet,[0],[0]
"[u1, u2, u3, u4] concatenates representations at different levels of the input sentence.",3.2.4 Hierarchical ConvNet,[0],[0]
The model thus captures hierarchical abstractions of an input sentence in a fixed-size representation.,3.2.4 Hierarchical ConvNet,[0],[0]
"For all our models trained on SNLI, we use SGD with a learning rate of 0.1 and a weight decay of 0.99.",3.3 Training details,[0],[0]
"At each epoch, we divide the learning rate by 5 if the dev accuracy decreases.",3.3 Training details,[0],[0]
We use minibatches of size 64 and training is stopped when the learning rate goes under the threshold of 10−5.,3.3 Training details,[0],[0]
"For the classifier, we use a multi-layer perceptron with 1 hidden-layer of 512 hidden units.",3.3 Training details,[0],[0]
We use opensource GloVe vectors trained on Common Crawl 840B2 with 300 dimensions as fixed word embeddings.,3.3 Training details,[0],[0]
"Our aim is to obtain general-purpose sentence embeddings that capture generic information that is
2https://nlp.stanford.edu/projects/ glove/
useful for a broad set of tasks.",4 Evaluation of sentence representations,[0],[0]
"To evaluate the quality of these representations, we use them as features in 12 transfer tasks.",4 Evaluation of sentence representations,[0],[0]
We present our sentence-embedding evaluation procedure in this section.,4 Evaluation of sentence representations,[0],[0]
We constructed a sentence evaluation tool3 to automate evaluation on all the tasks mentioned in this paper.,4 Evaluation of sentence representations,[0],[0]
"The tool uses Adam (Kingma and Ba, 2014) to fit a logistic regression classifier, with batch size 64.
",4 Evaluation of sentence representations,[0],[0]
"Binary and multi-class classification We use a set of binary classification tasks (see Table 1) that covers various types of sentence classification, including sentiment analysis (MR, SST), question-type (TREC), product reviews (CR), subjectivity/objectivity (SUBJ) and opinion polarity (MPQA).",4 Evaluation of sentence representations,[0],[0]
We generate sentence vectors and train a logistic regression on top.,4 Evaluation of sentence representations,[0],[0]
"A linear classifier requires fewer parameters than an MLP and is thus suitable for small datasets, where transfer learning is especially well-suited.",4 Evaluation of sentence representations,[0],[0]
"We tune the L2 penalty of the logistic regression with grid-search on the validation set.
",4 Evaluation of sentence representations,[0],[0]
Entailment and semantic relatedness We also evaluate on the SICK dataset for both entailment (SICK-E) and semantic relatedness (SICK-R).,4 Evaluation of sentence representations,[0],[0]
We use the same matching methods as in SNLI and learn a Logistic Regression on top of the joint representation.,4 Evaluation of sentence representations,[0],[0]
"For semantic relatedness evaluation, we follow the approach of (Tai et al., 2015) and learn to predict the probability distribution of relatedness scores.",4 Evaluation of sentence representations,[0],[0]
"We report Pearson correlation.
",4 Evaluation of sentence representations,[0],[0]
"STS14 - Semantic Textual Similarity While semantic relatedness is supervised in the case of SICK-R, we also evaluate our embeddings on the 6 unsupervised SemEval tasks of STS14 (Agirre et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"This dataset includes subsets of news articles, forum discussions, image descriptions and headlines from news articles containing pairs of sentences (lower-cased), labeled with
3https://www.github.com/ facebookresearch/SentEval
a similarity score between 0 and 5.",4 Evaluation of sentence representations,[0],[0]
"These tasks evaluate how the cosine distance between two sentences correlate with a human-labeled similarity score through Pearson and Spearman correlations.
",4 Evaluation of sentence representations,[0],[0]
Paraphrase detection The Microsoft Research Paraphrase Corpus is composed of pairs of sentences which have been extracted from news sources on the Web.,4 Evaluation of sentence representations,[0],[0]
Sentence pairs have been human-annotated according to whether they capture a paraphrase/semantic equivalence relationship.,4 Evaluation of sentence representations,[0],[0]
"We use the same approach as with SICK-E, except that our classifier has only 2 classes.
",4 Evaluation of sentence representations,[0],[0]
"Caption-Image retrieval The caption-image retrieval task evaluates joint image and language feature models (Hodosh et al., 2013; Lin et al., 2014).",4 Evaluation of sentence representations,[0],[0]
"The goal is either to rank a large collection of images by their relevance with respect to a given query caption (Image Retrieval), or ranking captions by their relevance for a given query image (Caption Retrieval).",4 Evaluation of sentence representations,[0],[0]
"We use a pairwise rankingloss Lcir(x, y): ∑ y ∑ k
max(0, α− s(V y, Ux) + s(V y, Uxk))",4 Evaluation of sentence representations,[0],[0]
"+∑ x ∑ k′ max(0, α− s(Ux, V y) + s(Ux, V yk′))
",4 Evaluation of sentence representations,[0],[0]
"where (x, y) consists of an image y with one of its associated captions x, (yk)k and (yk′)k′ are negative examples of the ranking loss, α is the margin and s corresponds to the cosine similarity.",4 Evaluation of sentence representations,[0],[0]
U and V are learned linear transformations that project the caption x and the image y to the same embedding space.,4 Evaluation of sentence representations,[0],[0]
We use a margin α = 0.2 and 30 contrastive terms.,4 Evaluation of sentence representations,[0],[0]
"We use the same splits as in (Karpathy and Fei-Fei, 2015), i.e., we use 113k images from the COCO dataset (each containing 5 captions) for training, 5k images for validation and 5k images for test.",4 Evaluation of sentence representations,[0],[0]
"For evaluation, we split the 5k images in 5 random sets of 1k images on which we compute Recall@K, with K ∈ {1, 5, 10} and
median (Med r) over the 5 splits.",4 Evaluation of sentence representations,[0],[0]
"For fair comparison, we also report SkipThought results in our setting, using 2048-dimensional pretrained ResNet101 (He et al., 2016) with 113k training images.",4 Evaluation of sentence representations,[0],[0]
"In this section, we refer to ”micro” and ”macro” averages of development set (dev) results on transfer tasks whose metrics is accuracy: we compute a ”macro” aggregated score that corresponds to the classical average of dev accuracies, and the ”micro” score that is a sum of the dev accuracies, weighted by the number of dev samples.",5 Empirical results,[0],[0]
Model We observe in Table 3 that different models trained on the same NLI corpus lead to different transfer tasks results.,5.1 Architecture impact,[0],[0]
The BiLSTM-4096 with the max-pooling operation performs best on both SNLI and transfer tasks.,5.1 Architecture impact,[0],[0]
"Looking at the micro and macro averages, we see that it performs significantly better than the other models LSTM, GRU, BiGRU-last, BiLSTM-Mean, inner-attention and the hierarchical-ConvNet.
",5.1 Architecture impact,[0],[0]
"Table 3 also shows that better performance on the training task does not necessarily translate in better results on the transfer tasks like when comparing inner-attention and BiLSTM-Mean for instance.
",5.1 Architecture impact,[0],[0]
We hypothesize that some models are likely to over-specialize and adapt too well to the biases of a dataset without capturing general-purpose information of the input sentence.,5.1 Architecture impact,[0],[0]
"For example, the inner-attention model has the ability to focus only on certain parts of a sentence that are useful for the SNLI task, but not necessarily for the transfer tasks.",5.1 Architecture impact,[0],[0]
"On the other hand, BiLSTM-Mean does not make sharp choices on which part of the sentence is more important than others.",5.1 Architecture impact,[0],[0]
"The difference between the results seems to come from the different abilities of the models to incorporate general information while not focusing too much on specific features useful for the task at hand.
",5.1 Architecture impact,[0],[0]
"For a given model, the transfer quality is also sensitive to the optimization algorithm: when training with Adam instead of SGD, we observed that the BiLSTM-max converged faster on SNLI (5 epochs instead of 10), but obtained worse results on the transfer tasks, most likely because of the model and classifier’s increased capability to over-specialize on the training task.
",5.1 Architecture impact,[0],[0]
"Embedding size Figure 5 compares the overall performance of different architectures, showing the evolution of micro averaged performance with
Model MR CR SUBJ MPQA SST TREC MRPC SICK-R SICK-E STS14 Unsupervised representation training (unordered sentences) Unigram-TFIDF 73.7 79.2 90.3 82.4 - 85.0 73.6/81.7 - - .58/.57",5.1 Architecture impact,[0],[0]
ParagraphVec (DBOW) 60.2 66.9 76.3 70.7 - 59.4 72.9/81.1 - - .42/.43 SDAE 74.6 78.0,5.1 Architecture impact,[0],[0]
90.8 86.9 - 78.4 73.7/80.7 - - .37/.38 SIF (GloVe + WR) - - - - 82.2 - - - 84.6 .69/ - word2vec BOW† 77.7 79.8 90.9 88.3 79.7 83.6 72.5/81.4 0.803 78.7,5.1 Architecture impact,[0],[0]
.65/.64 fastText BOW†,5.1 Architecture impact,[0],[0]
76.5 78.9 91.6 87.4 78.8 81.8 72.4/81.2 0.800 77.9 .63/.62,5.1 Architecture impact,[0],[0]
GloVe BOW†,5.1 Architecture impact,[0],[0]
78.7 78.5 91.6 87.6 79.8 83.6 72.1/80.9 0.800 78.6 .54/.56 GloVe Positional Encoding† 78.3 77.4 91.1 87.1 80.6 83.3 72.5/81.2 0.799 77.9,5.1 Architecture impact,[0],[0]
.51/.54,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (untrained)† 77.5 81.3 89.6 88.7 80.7 85.8 73.2/81.6 0.860 83.4 .39/.48
Unsupervised representation training (ordered sentences) FastSent 70.8 78.4 88.7 80.6 - 76.8 72.2/80.3 - - .63/.64 FastSent+AE 71.8 76.7 88.8 81.5 - 80.4 71.2/79.1 - - .62/.62 SkipThought 76.5 80.1 93.6 87.1 82.0 92.2 73.0/82.0 0.858 82.3 .29/.35",5.1 Architecture impact,[0],[0]
"SkipThought-LN 79.4 83.1 93.7 89.3 82.9 88.4 - 0.858 79.5 .44/.45
Supervised representation training CaptionRep (bow) 61.9 69.3 77.4 70.8 - 72.2 - - - .46/.42 DictRep (bow) 76.7 78.7 90.7 87.2 - 81.0 68.4/76.8 - - .67/.70",5.1 Architecture impact,[0],[0]
NMT En-to-Fr 64.7 70.1 84.9 81.5 - 82.8 - - .43/.42 Paragram-phrase - - - - 79.7 - - 0.849 83.1 .71/ - BiLSTM-Max (on SST)† (*) 83.7 90.2 89.5 (*) 86.0 72.7/80.9 0.863 83.1 .55/.54 BiLSTM-Max (on SNLI)† 79.9 84.6 92.1 89.8 83.3 88.7 75.1/82.3 0.885 86.3 .68/.65,5.1 Architecture impact,[0],[0]
"BiLSTM-Max (on AllNLI)† 81.1 86.3 92.4 90.2 84.6 88.2 76.2/83.1 0.884 86.3 .70/.67
Supervised methods (directly trained for each task – no transfer) Naive Bayes - SVM 79.4 81.8 93.2 86.3 83.1 - - - - - AdaSent 83.1 86.3 95.5 93.3 - 92.4 - - - - TF-KLD - - - - - - 80.4/85.9 - - - Illinois-LH - - - - - - - - 84.5 - Dependency Tree-LSTM - - - - - - - 0.868 - -
Table 4: Transfer test results for various architectures trained in different ways.",5.1 Architecture impact,[0],[0]
"Underlined are best results for transfer learning approaches, in bold are best results among the models trained in the same way.",5.1 Architecture impact,[0],[0]
"† indicates methods that we trained, other transfer models have been extracted from (Hill et al., 2016).",5.1 Architecture impact,[0],[0]
"For best published supervised methods (no transfer), we consider AdaSent (Zhao et al., 2015), TF-KLD (Ji and Eisenstein, 2013), Tree-LSTM (Tai et al., 2015) and Illinois-LH system (Lai and Hockenmaier, 2014).",5.1 Architecture impact,[0],[0]
(*),5.1 Architecture impact,[0],[0]
"Our model trained on SST obtained 83.4 for MR and 86.0 for SST (MR and SST come from the same source), which we do not put in the tables for fair comparison with transfer methods.
regard to the embedding size.
",5.1 Architecture impact,[0],[0]
"Since it is easier to linearly separate in high dimension, especially with logistic regression, it is not surprising that increased embedding sizes lead to increased performance for almost all models.",5.1 Architecture impact,[0],[0]
"However, this is particularly true for some models (BiLSTM-Max, HConvNet, inner-att), which demonstrate unequal abilities to incorporate more information as the size grows.",5.1 Architecture impact,[0],[0]
We hypothesize that such networks are able to incorporate information that is not directly relevant to the objective task (results on SNLI are relatively stable with regard to embedding size) but that can nevertheless be useful as features for transfer tasks.,5.1 Architecture impact,[0],[0]
We report in Table 4 transfer tasks results for different architectures trained in different ways.,5.2 Task transfer,[0],[0]
We group models by the nature of the data on which they were trained.,5.2 Task transfer,[0],[0]
The first group corresponds to models trained with unsupervised unordered sentences.,5.2 Task transfer,[0],[0]
"This includes bag-of-words models such as word2vec-SkipGram, the UnigramTFIDF model, the Paragraph Vector model (Le and Mikolov, 2014), the Sequential Denoising Auto-Encoder (SDAE) (Hill et al., 2016) and the SIF model (Arora et al., 2017), all trained on the Toronto book corpus (Zhu et al., 2015).",5.2 Task transfer,[0],[0]
"The second group consists of models trained with unsu-
pervised ordered sentences such as FastSent and SkipThought (also trained on the Toronto book corpus).",5.2 Task transfer,[0],[0]
We also include the FastSent variant “FastSent+AE” and the SkipThought-LN version that uses layer normalization.,5.2 Task transfer,[0],[0]
"We report results from models trained on supervised data in the third group, and also report some results of supervised methods trained directly on each task for comparison with transfer learning approaches.
",5.2 Task transfer,[0],[0]
"Comparison with SkipThought The best performing sentence encoder to date is the SkipThought-LN model, which was trained on a very large corpora of ordered sentences.",5.2 Task transfer,[0],[0]
"With much less data (570k compared to 64M sentences) but with high-quality supervision from the SNLI dataset, we are able to consistently outperform the results obtained by SkipThought vectors.",5.2 Task transfer,[0],[0]
We train our model in less than a day on a single GPU compared to the best SkipThought-LN network trained for a month.,5.2 Task transfer,[0],[0]
"Our BiLSTM-max trained on SNLI performs much better than released SkipThought vectors on MR, CR, MPQA, SST, MRPC-accuracy, SICK-R, SICK-E and STS14 (see Table 4).",5.2 Task transfer,[0],[0]
"Except for the SUBJ dataset, it also performs better than SkipThought-LN on MR, CR and MPQA.",5.2 Task transfer,[0],[0]
We also observe by looking at the STS14 results that the cosine metrics in our embedding space is much more semantically informative than in SkipThought embedding space (pearson score of 0.68 compared to 0.29 and 0.44 for ST and ST-LN).,5.2 Task transfer,[0],[0]
"We hypothesize that this is namely linked to the matching method of SNLI models which incorporates a notion of distance (element-wise product and absolute difference) during training.
",5.2 Task transfer,[0],[0]
"NLI as a supervised training set Our findings indicate that our model trained on SNLI obtains much better overall results than models trained on other supervised tasks such as COCO, dictionary definitions, NMT, PPDB (Ganitkevitch et al., 2013) and SST.",5.2 Task transfer,[0],[0]
"For SST, we tried exactly the same models as for SNLI; it is worth noting that SST is smaller than NLI.",5.2 Task transfer,[0],[0]
Our representations constitute higher-quality features for both classification and similarity tasks.,5.2 Task transfer,[0],[0]
"One explanation is that the natural language inference task constrains the model to encode the semantic information of the input sentence, and that the information required to perform NLI is generally discriminative and informative.
",5.2 Task transfer,[0],[0]
Domain adaptation on SICK tasks Our transfer learning approach obtains better results than previous state-of-the-art on the SICK task - can be seen as an out-domain version of SNLI - for both entailment and relatedness.,5.2 Task transfer,[0],[0]
"We obtain a pearson score of 0.885 on SICK-R while (Tai et al., 2015) obtained 0.868, and we obtain 86.3% test accuracy on SICK-E while previous best handengineered models (Lai and Hockenmaier, 2014) obtained 84.5%.",5.2 Task transfer,[0],[0]
"We also significantly outperformed previous transfer learning approaches on SICK-E (Bowman et al., 2015) that used the parameters of an LSTM model trained on SNLI to fine-tune on SICK (80.8% accuracy).",5.2 Task transfer,[0],[0]
"We hypothesize that our embeddings already contain the information learned from the in-domain task, and that learning only the classifier limits the number of parameters learned on the small out-domain task.
",5.2 Task transfer,[0],[0]
"Image-caption retrieval results In Table 5, we report results for the COCO image-caption retrieval task.",5.2 Task transfer,[0],[0]
We report the mean recalls of 5 random splits of 1K test images.,5.2 Task transfer,[0],[0]
"When trained with
ResNet features and 30k more training data, the SkipThought vectors perform significantly better than the original setting, going from 33.8 to 37.9 for caption retrieval R@1, and from 25.9 to 30.6 on image retrieval R@1.",5.2 Task transfer,[0],[0]
"Our approach pushes the results even further, from 37.9 to 42.4 on caption retrieval, and 30.6 to 33.2 on image retrieval.",5.2 Task transfer,[0],[0]
"These results are comparable to previous approach of (Ma et al., 2015) that did not do transfer but directly learned the sentence encoding on the imagecaption retrieval task.",5.2 Task transfer,[0],[0]
"This supports the claim that pre-trained representations such as ResNet image features and our sentence embeddings can achieve competitive results compared to features learned directly on the objective task.
",5.2 Task transfer,[0],[0]
MultiGenre NLI,5.2 Task transfer,[0],[0]
"The MultiNLI corpus (Williams et al., 2017) was recently released as a multi-genre version of SNLI.",5.2 Task transfer,[0],[0]
"With 433K sentence pairs, MultiNLI improves upon SNLI in its coverage: it contains ten distinct genres of written and spoken English, covering most of the complexity of the language.",5.2 Task transfer,[0],[0]
We augment Table 4 with our model trained on both SNLI and MultiNLI (AllNLI).,5.2 Task transfer,[0],[0]
We observe a significant boost in performance overall compared to the model trained only on SLNI.,5.2 Task transfer,[0],[0]
"Our model even reaches AdaSent performance on CR, suggesting that having a larger coverage for the training task helps learn even better general representations.",5.2 Task transfer,[0],[0]
"On semantic textual similarity STS14, we are also competitive with PPDB based paragramphrase embeddings with a pearson score of 0.70.",5.2 Task transfer,[0],[0]
"Interestingly, on caption-related transfer tasks such as the COCO image caption retrieval task, training our sentence encoder on other genres from MultiNLI does not degrade the performance compared to the model trained only SNLI (which contains mostly captions), which confirms the generalization power of our embeddings.",5.2 Task transfer,[0],[0]
This paper studies the effects of training sentence embeddings with supervised data by testing on 12 different transfer tasks.,6 Conclusion,[0],[0]
We showed that models learned on NLI can perform better than models trained in unsupervised conditions or on other supervised tasks.,6 Conclusion,[0],[0]
"By exploring various architectures, we showed that a BiLSTM network with max pooling makes the best current universal sentence encoding methods, outperforming existing approaches like SkipThought vectors.
",6 Conclusion,[0],[0]
We believe that this work only scratches the surface of possible combinations of models and tasks for learning generic sentence embeddings.,6 Conclusion,[0],[0]
Larger datasets that rely on natural language understanding for sentences could bring sentence embedding quality to the next level.,6 Conclusion,[0],[0]
"Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features.",abstractText,[0],[0]
"Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful.",abstractText,[0],[0]
Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted.,abstractText,[0],[0]
"In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks.",abstractText,[0],[0]
"Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks.",abstractText,[0],[0]
Our encoder is publicly available1.,abstractText,[0],[0]
Supervised Learning of Universal Sentence Representations from Natural Language Inference Data,title,[0],[0]
"Proceedings of the SIGDIAL 2016 Conference, pages 242–251, Los Angeles, USA, 13-15 September 2016. c©2016 Association for Computational Linguistics",text,[0],[0]
"Current virtual personal assistants (PAs) require users to either formulate complex intents in one utterance (e.g., “call Peter Miller on his mobile phone”) or go through tedious sub-dialogues (e.g., “phone call” – who would you like to call? – “Peter Miller” – I have a mobile number and a work number.",1 Introduction,[0],[0]
Which one do you want?).,1 Introduction,[0],[0]
"This is not how one would interact with a human assistant, where the request would be naturally structured into smaller chunks that individually get acknowledged (e.g., “Can you make a connection for me?” – sure – “with Peter Miller” - uh huh",1 Introduction,[0],[0]
- “on his mobile” - dialling now).,1 Introduction,[0],[0]
"Current PAs signal ongoing understanding by displaying the state of
the recognised speech (ASR) to the user, but not their semantic interpretation of it.",1 Introduction,[0],[0]
Another type of assistant system forgoes enquiring user intent altogether and infers likely intents from context.,1 Introduction,[0],[0]
"GoogleNow, for example, might present traffic information to a user picking up their mobile phone at their typical commute time.",1 Introduction,[0],[0]
"These systems display their “understanding” state, but do not allow any type of interaction with it apart from dismissing the provided information.
",1 Introduction,[0],[0]
"In this work, we explore adding a graphical user interface (GUI) modality that makes it possible to see these interaction styles as extremes on a continuum, and to realise positions between these extremes and present a mixed graphical/voice enabled PA that can provide feedback of understanding to the user incrementally as the user’s utterance unfolds–allowing users to make requests in instalments instead of fully thought-out requests.",1 Introduction,[0],[0]
It does this by signalling ongoing understanding in an intuitive tree-like GUI that can be displayed on a mobile device.,1 Introduction,[0],[0]
"We evaluate our system by directing users to perform tasks using it under nonincremental (i.e., ASR endpointing) and incremental conditions and then compare the two conditions.",1 Introduction,[0],[0]
"We further compare a non-adaptive with an adaptive (i.e., infers likely events) version of our system.",1 Introduction,[0],[0]
"We report that the users found the interface intuitive and easy to use, and that users were able to perform tasks more efficiently with incremental as well as adaptive variants of the system.",1 Introduction,[0],[0]
This work builds upon several threads of previous research: Chai et al. (2014),2 Related Work,[0],[0]
"addressed misalignments in understanding (i.e., common ground (Clark and Schaefer, 1989)) between robots and humans by informing the human of the internal system state via speech.",2 Related Work,[0],[0]
"We take this idea and ap-
242
ply it to a PA by displaying the internal state of the system to the user via a GUI (explained in Section 3.5), allowing the user to determine if system understanding has taken place–a way of providing feedback and backchannels to the user.",2 Related Work,[0],[0]
"Dethlefs et al. (2016) provide a good review of work that show how backchannels facilitate grounding, feedback, and clarifications in human spoken dialogue, and apply an information density approach to determine when to backchannel using speech.",2 Related Work,[0],[0]
"Because we don’t backchannel using speech here, there is no potential overlap between the user and the system; rather, our system can display backchannels and ask clarifications without frustrating the user through inadvertent overlaps.
",2 Related Work,[0],[0]
"Though different in many ways, our work is similar in some regards to Larsson et al. (2011), which displays information to the user and allows the user to navigate the display itself (e.g., by saying up or down in a menu list)–functionality that we intend to apply to our GUI in future work.",2 Related Work,[0],[0]
"Our work is also comparable to SDS toolkits such as IrisTK (Skantze and Moubayed, 2012) and OpenDial (Lison, 2015) which enable SDS designers to visualise the internal state of their systems, though not for end user interpretability.
",2 Related Work,[0],[0]
"Some of the work here is inspired by the Microsoft Language Understanding Intelligent Service (LUIS) project (Williams et al., 2015).",2 Related Work,[0],[0]
"While our system by no means achieves the scale that LUIS does, we offer here an additional contribution of an open source LUIS-like system (with the important addition of the graphical interface) that is authorable (using JSON files; we leave authoring using a web interface like that of LUIS to future work), extensible (affordances can be easily added), incremental (in that respect going beyond LUIS), trainable (i.e., can learn from examples, but can still function well without examples), and can learn through interacting (here we apply a user model that learns during interaction).",2 Related Work,[0],[0]
"This section introduces and describes our SDS, which is modularised into four main components: ASR, natural language understanding (NLU), dialogue management (DM), and the graphical user interface (GUI) which, as explained below, is visualised as a right-branching tree.",3 System Description,[0],[0]
The overall system is represented in Figure 1.,3 System Description,[0],[0]
"For the remainder of this section, each module is explained in
turn.",3 System Description,[0],[0]
"As each module processes input incrementally (i.e., word for word), we first explain our framework for incremental processing.",3 System Description,[0],[0]
An aspect of our SDS that sets it apart from others is the requirement that it process incrementally.,3.1 Incremental Dialogue,[0],[0]
"One potential concern with incremental processing is regarding informativeness: why act early when waiting might provide additional information, resulting in better-informed decisions?",3.1 Incremental Dialogue,[0],[0]
The trade off is naturalness as perceived by the user who is interacting with the SDS.,3.1 Incremental Dialogue,[0],[0]
"Indeed, it has been shown that human users perceive incremental systems as being more natural than traditional, turn-based systems (Aist et al., 2006; Skantze and Schlangen, 2009; Skantze and Hjalmarsson, 1991; Asri et al., 2014), offer a more human-like experience (Edlund et al., 2008) and are more satisfying to interact with than non-incremental systems (Aist et al., 2007).",3.1 Incremental Dialogue,[0.9513751390691803],"['Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b)) as features.']"
"Psycholinguistic research has also shown that humans comprehend utterances as they unfold and do not wait until the end of an utterance to begin the comprehension process (Tanenhaus et al., 1995; Spivey et al., 2002).
",3.1 Incremental Dialogue,[0],[0]
The trade-off between informativeness and naturalness can be reconciled when mechanisms are in place that allow earlier decisions to be repaired.,3.1 Incremental Dialogue,[0],[0]
"Such mechanisms are offered by the incremental unit (IU) framework for SDS (Schlangen and Skantze, 2011), which we apply here.",3.1 Incremental Dialogue,[0],[0]
"Following Kennington et al. (2014), the IU framework consists of a network of processing modules.",3.1 Incremental Dialogue,[0],[0]
"A typical module takes input, performs some kind of processing on that data, and produces output.
",3.1 Incremental Dialogue,[0],[0]
The data are packaged as the payload of incremental units (IUs) which are passed between modules.,3.1 Incremental Dialogue,[0],[0]
"The IUs themselves are interconnected via so-called same level links (SLL) and groundedin links (GRIN), the former allowing the linking of IUs as a growing sequence, the latter allowing that sequence to convey what IUs directly affect it (see Figure 2 for an example of incremental ASR).",3.1 Incremental Dialogue,[0],[0]
"Thus IUs can be added, but can be later revoked and replaced in light of new information.",3.1 Incremental Dialogue,[0],[0]
"The IU framework can take advantage of up-to-date information, but have the potential to function in such a way that users perceive as more natural.
",3.1 Incremental Dialogue,[0],[0]
The modules explained in the remainder of this section are implemented as IU-modules and process incrementally.,3.1 Incremental Dialogue,[0],[0]
Each will now be explained.,3.1 Incremental Dialogue,[0],[0]
The module that takes speech input from the user in our SDS is the ASR component.,3.2 Speech Recognition,[0],[0]
"Incremental ASR must transcribe uttered speech into words which must be forthcoming from the ASR as early as possible (i.e., the ASR must not wait for endpointing to produce output).",3.2 Speech Recognition,[0],[0]
"Each module that follows must also process incrementally, acting in lock-step upon input as it is received.",3.2 Speech Recognition,[0],[0]
"Incremental ASR is not new (Baumann et al., 2009) and many of the current freely-accessible ASR systems can produce output (semi-) incrementally.",3.2 Speech Recognition,[0],[0]
We opt for Google ASR for its vocabulary coverage of our evaluation language (German).,3.2 Speech Recognition,[0],[0]
"Following, Baumann et al. (2016), we package output from the Google service into IUs which are passed to the NLU module, which we now explain.",3.2 Speech Recognition,[0],[0]
We approach the task of NLU as a slot-filling task (a very common approach; see Tur et al. (2012)) where an intent is complete when all slots of a frame are filled.,3.3 Language Understanding,[0],[0]
"The main driver of the NLU in
our SDS is the SIUM model of NLU introduced in Kennington et al. (2013).",3.3 Language Understanding,[0],[0]
"SIUM has been used in several systems which have reported substantial results in various domains, languages, and tasks (Han et al., 2015; Kennington et al., 2015; Kennington and Schlangen, 2017)",3.3 Language Understanding,[0],[0]
"Though originally a model of reference resolution, it was always intended to be used for general NLU, which we do here.",3.3 Language Understanding,[0],[0]
"The model is formalised as follows:
",3.3 Language Understanding,[0],[0]
P (I|U) = 1 P (U) P (I) ∑ r∈R P (U |R = r)P,3.3 Language Understanding,[0],[0]
"(R = r|I) (1)
That is, P (I|U) is the probability of the intent",3.3 Language Understanding,[0],[0]
"I (i.e., a frame slot) behind the speaker’s (ongoing) utterance U .",3.3 Language Understanding,[0],[0]
"This is recovered using the mediating variable R, a set of properties which map between aspects of U and aspects of I .",3.3 Language Understanding,[0],[0]
"We opt for abstract properties here (e.g., the frame for restaurant might be filled by a certain type of cuisine intent such as italian which has properties like pasta, mediterranean, vegetarian, etc.).",3.3 Language Understanding,[0],[0]
Properties are pre-defined by a system designer and can match words that might be uttered to describe the intent in question.,3.3 Language Understanding,[0],[0]
"For P (R|I), probability is distributed uniformly over all properties that a given intent is specified to have.",3.3 Language Understanding,[0],[0]
"(If other information is available, more informative priors could be used as well.)",3.3 Language Understanding,[0],[0]
The mapping between properties and aspects of U can be learned from data.,3.3 Language Understanding,[0],[0]
"During application, R is marginalised over, resulting in a distribution over possible intents.1",3.3 Language Understanding,[0],[0]
"This occurs at each word increment, where the distribution from the previous increment is combined via P (I), keeping track of the distribution over time.
",3.3 Language Understanding,[0],[0]
We further apply a simple rule to add in apriori knowledge: if some r ∈ R and w ∈ U are such that r,3.3 Language Understanding,[0],[0]
".= w (where .= is string equality; e.g., an intent has the property of pasta and the word pasta is uttered), then we set C(U=w|R=r)=1.",3.3 Language Understanding,[0],[0]
"To allow for possible ASR confusions, we also apply C(U=w|R=r)= 1",3.3 Language Understanding,[0],[0]
"− ld(w, r)/max(len(w), len(r)), where ld is the Levenshtein distance (but we only apply this if the calculated value is above a threshold of 0.6; i.e., the two strings are mostly similar).",3.3 Language Understanding,[0],[0]
"For all otherw, C(w|r)=0.",3.3 Language Understanding,[0],[0]
"This results in a distribution C, which we renormalise and blend with learned distribution to yield P (U |R).
",3.3 Language Understanding,[0],[0]
"1In Kennington et al. (2013) the authors apply Bayes’ Rule to allow P (U |R) to produce a distribution over properties, which we adopt here.
",3.3 Language Understanding,[0],[0]
We apply an instantiation of SIUM for each slot.,3.3 Language Understanding,[0],[0]
"The candidate slots which are processed depends on the state of the dialogue; only slots represented by visible nodes are considered, thereby reducing the possible frames that could be predicted.",3.3 Language Understanding,[0],[0]
"At each word increment, the updated slots (and their corresponding) distributions are given to the DM, which will now be explained.",3.3 Language Understanding,[0],[0]
"The DM plays a crucial role in our SDS: as well as determining how to act, the DM is called upon to decide when to act, effectively giving the DM the control over timing of actions rather than relying on ASR endpointing–further separating our SDS from other systems.",3.4 Dialogue Manager,[0],[0]
"The DM policy is based on a confidence score derived from the NLU (in this case, we used the distribution’s argmax value) using thresholds for the actions (see below), set by hand (i.e., trial and error).",3.4 Dialogue Manager,[0],[0]
"At each word and resulting distribution from NLU, the DM needs to choose one of the following:
• wait – wait for more information (i.e., for the next word)
• select – as the NLU is confident enough, fill the slot can with the argmax from NLU
• request – signal a (yes/no) clarification request on the current slot and the proposed filler
• confirm – act on the confirmation of the user; in effect, select the proposed slot value
Though the thresholds are statically set, we applied OpenDial (Lison, 2015) as an IU-module to perform the task of the DM with the future goal that these values could be adjusted through reinforcement learning (which OpenDial could provide).",3.4 Dialogue Manager,[0],[0]
"The DM processes and makes a decision for each slot, with the assumption that only one slot out of all that are processed will result in an non-wait action (though this is not enforced).",3.4 Dialogue Manager,[0],[0]
The goal of the GUI is to intuitively inform the user about the internal state of the ongoing understanding.,3.5 Graphical User Interface,[0],[0]
"One motivation for this is that the user can determine if the system understood the user’s intent before providing the user with a response
(e.g., a list of restaurants of a certain type); i.e., if any misunderstanding takes place, it happens before the system commits to an action and is potentially more easily repaired.
",3.5 Graphical User Interface,[0],[0]
"The display is a rightbranching tree, where the branches directly off the root node display the affordances of the system (i.e., what domains of things it can understand and do something about).",3.5 Graphical User Interface,[0],[0]
"When the first tree is displayed, it represents a state of the NLU where none of the slots are filled, as in Figure 3.
",3.5 Graphical User Interface,[0],[0]
"When a user verbally selects a domain to ask about, the tree is adjusted to make that domain the only one displayed and
the slots that are required for that domain are shown as branches.",3.5 Graphical User Interface,[0],[0]
"The user can then fill those slots (i.e., branches) by uttering the displayed name, or, alternatively, by uttering the item to fill the slot directly.",3.5 Graphical User Interface,[0],[0]
"For example, at a minimum, the user could utter the name of the domain then an item for each slot (e.g., food Thai downtown) or the speech could be more natural (e.g., I’m quite hungry, I am looking for some Thai food maybe in the downtown area).",3.5 Graphical User Interface,[0],[0]
"Crucially, the user can also hesitate within and between chunks, as advancement is not triggered by silence thresholding, but rather semantically.",3.5 Graphical User Interface,[0],[0]
"When something is uttered that falls into the request state of the DM as explained above, the display expands the subtree under question and marks the item with a question mark (see Figure 4).",3.5 Graphical User Interface,[0],[0]
"At this point, the user can utter any kind of confirmation.",3.5 Graphical User Interface,[0],[0]
A positive confirmation fills the slot with the item in question.,3.5 Graphical User Interface,[0],[0]
"A negative confirmation retracts the question, but leaves the branch expanded.",3.5 Graphical User Interface,[0],[0]
The expanded branches are displayed according to their rank as given by the NLU’s probability distribution.,3.5 Graphical User Interface,[0],[0]
"Though a branch in the display can theoretically display an unlimited number of children, we opted to only show 7 children; if a branch had more, the final child displayed as an ellipsis.
",3.5 Graphical User Interface,[0],[0]
"A completed branch is collapsed, visually marking its corresponding slot as filled.",3.5 Graphical User Interface,[0],[0]
"At any
time, a user can backtrack by saying no (or equivalent) or start the entire interaction over from the beginning with a keyword, e.g., restart.",3.5 Graphical User Interface,[0],[0]
"To aid the user’s attention, the node under question is marked in red, where completed slots are represented by outlined nodes, and filled nodes represent candidates for the current slot in question (see examples of all three in Figure 4).",3.5 Graphical User Interface,[0],[0]
"For cases where the system is in the wait state for several words (during which there is no change in the tree), the system signals activity at each word by causing the red node in question to temporarily change to white, then back to red (i.e., appearing as a blinking node to the user).",3.5 Graphical User Interface,[0],[0]
"Figure 5 shows a filled frame, represented as tree with one branch for each filled slot.
",3.5 Graphical User Interface,[0],[0]
Figure 5: Example tree where all of the slots are filled.,3.5 Graphical User Interface,[0],[0]
"(i.e., domain:food, location:university, type:thai)
",3.5 Graphical User Interface,[0],[0]
Such an interface clearly shows the internal state of the SDS and whether or not it has understood the request so far.,3.5 Graphical User Interface,[0],[0]
"It is designed to aid the user’s attention to the slot in question, and clearly indicates the affordances that the system has.",3.5 Graphical User Interface,[0],[0]
"The interface is currently a read-only display that is purely speech-driven, but it could be augmented with additional functionalities, such as tapping a node for expansion or typing input that the system might not yet display.",3.5 Graphical User Interface,[0],[0]
"It is currently implemented as a web-based interface (using the JavaScript D3 library), allowing it to be usable as a web application on any machine or mobile device.
",3.5 Graphical User Interface,[0],[0]
"Adaptive Branching The GUI as explained affords an additional straight-forward extension: in order to move our system towards adaptivity on the above-mentioned continuum, the GUI can be used to signal what the system thinks the user might say next.",3.5 Graphical User Interface,[0],[0]
"This is done by expanding a branch and displaying a confirmation on that branch, signalling that the system predicts that the user will choose that particular branch.",3.5 Graphical User Interface,[0],[0]
"Alternatively, if the system is confident that a user will fill a slot with a particular value, that particular slot can be filled without confirmation.",3.5 Graphical User Interface,[0],[0]
This is displayed as a collapsed tree branch.,3.5 Graphical User Interface,[0],[0]
"A system that perfectly predicts a user’s intent would fill an entire tree (i.e., all slots) only requiring the user to confirm once.",3.5 Graphical User Interface,[0],[0]
A more careful system would confirm at each step (such an interaction would only require the user to utter confirmations and nothing else).,3.5 Graphical User Interface,[0],[0]
We applied this adaptive variant of the tree in one of our experiments explained below.,3.5 Graphical User Interface,[0],[0]
"In this section, we describe two experiments where we evaluated our system.",4 Experiments,[0],[0]
It is our primary goal to show that our GUI is useful and signals understanding to the user.,4 Experiments,[0],[0]
We also wish to show that incremental presentation of such a GUI is more effective than an endpointed system.,4 Experiments,[0],[0]
We further want to show that an adaptive system is more effective than a non-adaptive system (though both would process incrementally).,4 Experiments,[0],[0]
"In order to best evaluate our system, we recruited participants to interact with our system in varied settings to compare endpointed (i.e., non-incremental) and nonadaptive as well as adaptive versions.",4 Experiments,[0],[0]
"We describe how the data were collected from the participants, then explain each experiment and give results.",4 Experiments,[0],[0]
The participants were seated at a desk and given written instructions indicating that they were to use the system to perform as many tasks as possible in the allotted time.,4.1 Task & Procedure,[0],[0]
Figure 6 shows some example tasks as they would be displayed (one at a time) to the user.,4.1 Task & Procedure,[0],[0]
"A screen, tablet, and keyboard were on the desk in front of the user (see Figure",4.1 Task & Procedure,[0],[0]
7).2,4.1 Task & Procedure,[0],[0]
"The user was instructed to convey the task presented on the screen to the system such
2We used a Samsung 8.4 Pro tablet turned to its side to show a larger width for the tree to grow to the right.",4.1 Task & Procedure,[0],[0]
"The tablet only showed the GUI; the SDS ran on a separate computer.
that the GUI on the tablet would have a completed tree (e.g., as in Figure 5).",4.1 Task & Procedure,[0],[0]
"When the participant was satisfied that the system understood her intent, she was to press space bar on the keyboard which triggered a new task to be displayed on the screen and reset the tree to its start state on the tablet (as in Figure 3).
",4.1 Task & Procedure,[0],[0]
"The possible task domains were call, which had a single slot for name to be filled (i.e., one out of the 22 most common German given names); message which had a slot for name and a slot for the message (which, when invoked, would simply fill in directly from the
ASR until 1 second of silence was detected); eat which had slots for type (in this case, 6 possible types) and location (in this case, 6 locations based around the city of Bielefeld); route which had slots for source city and the destination city (which shared the same list of the top 100 most populous German cities); and reminder which had a slot for message.
",4.1 Task & Procedure,[0],[0]
"For each task, the domain was first randomly chosen from the 5 possible domains, and then each slot value to be filled was randomly chosen (the message slot for the name and message domains was randomly selected from a list of 6 possible “messages”, each with 2-3 words; e.g., feed the cat, visit grandma, etc.).",4.1 Task & Procedure,[0],[0]
The system kept track of which tasks were already presented to the participant.,4.1 Task & Procedure,[0],[0]
"At any time after the first task, the system could choose a task that was previously presented and present it again to the participant (with a 50% chance) so the user would often see tasks that she had seen before (with the assumption that humans who use PAs often do perform similar, if not the same, tasks more than once).
",4.1 Task & Procedure,[0],[0]
"The participant was told that she would interact with the system in three different phases, each for 4 minutes, and to accomplish as many tasks as possible in that time allotment.",4.1 Task & Procedure,[0],[0]
The participant was not told what the different phases were.,4.1 Task & Procedure,[0],[0]
"The experiments described in Sections 4.2 and
4.3 respectively describe and report a comparison first between the Phase 1 and 2 (denoted as the endpointed and incremental variants of the system) in order to establish whether or not the incremental variant produced better results than the endpointed variant.",4.1 Task & Procedure,[0],[0]
We also report a comparison between Phase 2 and 3 (incremental and incremental-adaptive phases).,4.1 Task & Procedure,[0],[0]
Phase 1 and Phase 3 are not directly comparable to each other as Phase 3 is really a variant of Phase 2.,4.1 Task & Procedure,[0],[0]
"Because of this, we fixed the order of the phase presentation for all participants.",4.1 Task & Procedure,[0],[0]
Each of these phases are described below.,4.1 Task & Procedure,[0],[0]
"Before the participant began Phase 1, they were able to try it out for up to 4 minutes (in Phase 1 settings) and ask for help from the experimenter, allowing them to get used to the Phase 1 interface before the actual experiment began.",4.1 Task & Procedure,[0],[0]
"After this trial phase, the experiment began with Phase 1.
",4.1 Task & Procedure,[0],[0]
"Phase 1: Non-incremental In this phase, the system did not appear to work incrementally; i.e., the system displayed tree updates after ASR endpointing (of 1.2 seconds–a reasonable amount of time to expect a response from a commercial spoken PA).",4.1 Task & Procedure,[0],[0]
The system displayed the ongoing ASR on the tablet as it was recognised (as is often done in commercial PAs).,4.1 Task & Procedure,[0],[0]
"At the end of Phase 1, a pop up window notified the user that the phase was complete.",4.1 Task & Procedure,[0],[0]
"They then moved onto Phase 2.
",4.1 Task & Procedure,[0],[0]
"Phase 2: Incremental In this phase, the system displayed the tree information incrementally without endpointing.",4.1 Task & Procedure,[0],[0]
"The ASR was no longer displayed; only the tree provided feedback in understanding, as explained in Section 3.5.
",4.1 Task & Procedure,[0],[0]
"After Phase 2, a 10-question questionnaire was displayed on the screen for the participant to fill out comparing Phase 1 and Phase 2.",4.1 Task & Procedure,[0],[0]
"For each question, they had the choice of Phase 1, Phase
2, Both, and Neither.",4.1 Task & Procedure,[0],[0]
(See Appendix for full list of questions.),4.1 Task & Procedure,[0],[0]
"After completing the questionnaire, they moved onto Phase 3.
",4.1 Task & Procedure,[0],[0]
"Phase 3: Incremental-adaptive In this phase, the incremental system was again presented to the participant with an added user model that “learned” about the user.",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than once, the user model would predict that, if the user chose that task domain again (e.g., route) then the system would automatically ask a clarification using the previously filled values (except for the message slot, which the user always had to fill).",4.1 Task & Procedure,[0],[0]
"If the user saw a task more than 3 times, the system skipped asking for clarifications and filled in the domain slots completely, requiring the user only to press the space bar to confirm it was the correct one (i.e., to complete the task).",4.1 Task & Procedure,[0],[0]
"An example progression might be as follows: a participant is presented with the task route from Bielefeld to Berlin, then the user would attempt to get the system to fill in the tree (i.e., slots) with those values.",4.1 Task & Procedure,[0],[0]
"After some interaction in other domains, the user sees the same task again, and now after indicating the intent type route, the user must only say “yes” for each slot to confirm the system’s prediction.",4.1 Task & Procedure,[0],[0]
"Later, if the task is presented a third time, when entering that domain (i.e, route), the two slots would already be filled.",4.1 Task & Procedure,[0],[0]
"If later a different route task was presented, e.g., route from Bielefeld to Hamburg, the system would already have the two slots filled, but the user could backtrack by saying “no, to Hamburg” which would trigger the system to fill the appropriate slot with the corrected value.",4.1 Task & Procedure,[0],[0]
"Later interactions within the route domain would ask for a clarification on the destination slot since it has had several possible values given by the participant, but continue to fill the from slot with Bielefeld.
",4.1 Task & Procedure,[0],[0]
"After Phase 3, the participants were presented with another questionnaire on the screen to fill out with the same questions (plus two additional questions), this time comparing Phase 2 and Phase 3.",4.1 Task & Procedure,[0],[0]
"For each item, they had the choice of Phase 2, Phase 3, Both, and Neither.",4.1 Task & Procedure,[0],[0]
"At the end of the three phases and questionnaires, the participants were given a final questionnaire to fill out by hand on their general impressions of the systems.
",4.1 Task & Procedure,[0],[0]
We recruited 14 participants for the evaluation.,4.1 Task & Procedure,[0],[0]
"We used the Mint tools data collection framework (Kousidis et al., 2012) to log the interactions.",4.1 Task & Procedure,[0],[0]
"Due to some technical issues, one of the participants
did not log interactions.",4.1 Task & Procedure,[0],[0]
"We collected data from 13 participants, post-Phase 2 questionnaires from 12 participants, post-Phase 3 questionnaires from all 14 participants, and general questionnaires from all 14 participants.",4.1 Task & Procedure,[0],[0]
"In the experiments that follow, we report objective and subjective measures to determine the settings that produced superior results.
",4.1 Task & Procedure,[0],[0]
Metrics We report the subjective results of the participant questionnaires.,4.1 Task & Procedure,[0],[0]
We only report those items that were statistically significant (see Appendix for a full list of the questions).,4.1 Task & Procedure,[0],[0]
"We further report objective measures for each system variant: total number of completed tasks, fully correct frames, average frame f-score, and average time elapsed (averages are taken over all participants for each variant; we only used the 10 participants who fully interacted with all three phases).",4.1 Task & Procedure,[0],[0]
Discussion is left to the end of this section.,4.1 Task & Procedure,[0],[0]
"In this section we report the results of the evaluation between the endpointed (i.e., nonincremental; Phase 1) variant vs the incremental (Phase 2) variant of our system.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Subjective Results We applied a multinomial test of significance to the results, treating all four possible answers as equally likely (with Bonferroni correction of 10).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item The interface was useful and easy to understand with the answer of Both was significant (χ2 (4, N = 12)",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"= 9.0, p < .005), as was The assistant was easy and intuitive to use also with the answer Both (χ2 (4, N = 12) = 9.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"The item I always understood what the system wanted from me was also answered Both significantly more times than other answers (χ2 (4, N = 14) = 9.0, p< .005), similarly for It was sometimes unclear to me if the assistant understood me with the answer of Both (χ2 (4, N = 12) = 10.0, p < .005).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"These responses tell us that though the participants did not report preference for either system variant, they reported a general positive impression of the GUI (in both variants).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a nice result; the GUI could be used in either system with benefit to the users.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
Objective Results The endpointed (Phase 1) and incremental (Phase 2) columns in Table 1 show the results of the objective evaluation.,4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Though the average time per task and fscore for the endpointed variant are better than those of the
incremental variant, the total number of tasks for the incremental variant was higher.
",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"Manual inspection of logs indicate that participants took advantage of the system’s flexibility of understanding instalments (i.e., filling frames incrementally).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is evidenced in that participants often uttered words understood by the system as being negative (e.g., nein/no), either as a result of an explicit confirmation request by the system (e.g., Thai?) or after a slot was incorrectly filled (something very easily determined through the GUI).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"This is a desired outcome of using our system; participants were able to repair local areas of misunderstanding as they took place instead of needing to correct an entire intent (i.e., frame).",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"However, we cannot fully empirically measure these tendencies given our data.",4.2 Experiment 1: Endpointed vs. Incremental,[0],[0]
"In this section we report results for the evaluation between the incremental (Phase 2) and incremental-adaptive (henceforth just adaptive; Phase 3) systems.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Subjective Results We applied the same significance test as Experiment 1 (with Bonferroni correction of 12).,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"The item The interface was useful and easy to understand was answered with Both significantly (χ2 (4, N = 14)",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"= 10.0, p < .0042), The item I had the feeling that the assistant attempted to learn about me was answered with Neither (χ2 (4, N = 14) = 8.0, p < .0042), though Phase 3 was also marked (6 times).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
All other items were not significant.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
Here again we see that there is a general positive impression of the GUI under all conditions.,4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"If anyone noticed that a system variant was attempting to learn a user model at all, they noticed that it was in Phase 3, as expected.
",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"Objective Results The incremental (Phase 2) and adaptive (Phase 3) columns in Table 1 show
the results for the objective evaluation for this experiment.",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0],[0]
"There is a clear difference between the two variants, with the adaptive showing more completed tasks, more fully correct frames, and a higher average fscore (all three likely due to the fact that frames were potentially pre-filled).",4.3 Experiment 2: Incremental vs. Incremental-Adaptive,[0.9505882921396734],"['Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.']"
"While the responses don’t express any preference for a particular system variant, the overall impression of the GUI was positive.",4.4 Discussion,[0],[0]
"The objective measures show that there are gains to be made when the system signals understanding at a more finegrained interval than at the utterance level, due to the higher number of completed tasks and locallymade repairs.",4.4 Discussion,[0],[0]
"There are further gains to be made when the system applies simple user modelling (i.e., adaptivity) by attempting to predict what the user might want to do in a chosen domain, decreasing the possibility of user error and allowing the system to accurately and quickly complete more tasks.",4.4 Discussion,[0],[0]
"Participants also didn’t just get used to the system over time, as the average time per episode was fairly similar in all three phases.
",4.4 Discussion,[0],[0]
The open-ended questionnaire sheds additional light.,4.4 Discussion,[0],[0]
"Most of the suggestions for improvement related to ASR misrecognition and speed (i.e., not about the system itself).",4.4 Discussion,[0],[0]
Two participants suggested an ability to add “free input” or select alternatives from the tree.,4.4 Discussion,[0],[0]
"Two participants suggested that the system be more responsive (i.e., in wait states), and give more feedback (i.e., backchannels) more often.",4.4 Discussion,[0],[0]
"For those participants that expressed preference to the non-incremental system (Phase 1), none of them had used a speech-based PA before, whereas those that expressed preference to the incremental versions (Phases 2 and 3) use them regularly.",4.4 Discussion,[0],[0]
"We conjecture that people without SDS experience equate understanding with ASR, whereas those that are more familiar with PAs know that perfect ASR doesn’t translate to perfect understanding–hence the need for a GUI.",4.4 Discussion,[0],[0]
"A potential remedy would be to display ASR with the tree, signalling understanding despite ASR errors.",4.4 Discussion,[0],[0]
"Given the results and analysis, we conclude that an intuitive presentation that signals a system’s ongoing understanding benefits end users who perform simple tasks which might be performed by a PA.",5 Conclusion & Future Work,[0],[0]
"The GUI that we provided, using a right-branching
tree, worked well; indeed, the participants who used it found it intuitive and easy to understand.",5 Conclusion & Future Work,[0],[0]
There are gains to be made when the system signals understanding at finer-grained levels than just at the end of a pre-formulated utterance.,5 Conclusion & Future Work,[0],[0]
There are further gains to be made when a PA attempts to learn (even a rudimentary) user model to predict what the user might want to do next.,5 Conclusion & Future Work,[0],[0]
"The adaptivity moves our system from one extreme of the continuum–simple slot filling–closer towards the extreme that is fully predictive, with the additional benefit of being able to easily correct mistakes in the predictions.
",5 Conclusion & Future Work,[0],[0]
"For future work, we intend to provide simple authoring tools for the system to make building simple PAs using our GUI easy.",5 Conclusion & Future Work,[0],[0]
We want to improve the NLU and scale to larger domains.3,5 Conclusion & Future Work,[0],[0]
"We also plan on implementing this as a standalone application that could be run on a mobile device, which could actually perform the tasks.",5 Conclusion & Future Work,[0],[0]
"It would further be beneficial to compare the GUI with a system that responds with speech (i.e., without a GUI).",5 Conclusion & Future Work,[0],[0]
"Lastly, we will investigate using touch as an additional input modality to select between possible alternatives that are offered by the system.
",5 Conclusion & Future Work,[0],[0]
Acknowledgements Thanks to the anonymous reviewers who provided useful comments and suggestions.,5 Conclusion & Future Work,[0],[0]
Thanks also to Julian Hough for helping with experiments.,5 Conclusion & Future Work,[0],[0]
"We acknowledge support by the Cluster of Excellence “Cognitive Interaction Technology” (CITEC; EXC 277) at Bielefeld University, which is funded by the German Research Foundation (DFG), and the BMBF KogniHome project.
",5 Conclusion & Future Work,[0],[0]
"Appendix The following questions were asked on both questionnaires following Phase 2 and Phase 3 (comparing the two most latest used system versions; as translated into English):
•",5 Conclusion & Future Work,[0],[0]
The interface was useful and easy to understand.,5 Conclusion & Future Work,[0],[0]
• The assistant was easy and intuitive to use.,5 Conclusion & Future Work,[0],[0]
• The assistant understood what I wanted to say.,5 Conclusion & Future Work,[0],[0]
• I always understood what the system wanted from me.,5 Conclusion & Future Work,[0],[0]
•,5 Conclusion & Future Work,[0],[0]
The assistant made many mistakes.,5 Conclusion & Future Work,[0],[0]
• The assistant did not respond while I spoke.,5 Conclusion & Future Work,[0],[0]
"3Kennington and Schlangen (2017) showed that our chosen NLU approach can scale fairly well, but the GUI has some limits when applied to larger domains with thousands of items.",5 Conclusion & Future Work,[0],[0]
"We leave improved scaling to future work.
",5 Conclusion & Future Work,[0],[0]
"• It was sometimes unclear to me if the assistant understood me.
",5 Conclusion & Future Work,[0],[0]
• The assistant responded while I spoke.,5 Conclusion & Future Work,[0],[0]
• The assistant sometimes did things that I did not expect.,5 Conclusion & Future Work,[0],[0]
"• When the assistant made mistakes, it was easy for me
to correct them.
",5 Conclusion & Future Work,[0],[0]
"In addition to the above 10 questions, the following were also asked on the questionnaire following Phase 3: • I had the feeling that the assistant attempted to learn
about me.
",5 Conclusion & Future Work,[0],[0]
"• I had the feeling that the assistant made incorrect guesses.
",5 Conclusion & Future Work,[0],[0]
The following questions were used on the general questionnaire:,5 Conclusion & Future Work,[0],[0]
"• I regularly use personal assistants such as Siri, Cortana,
Google now or Amazon Echo:",5 Conclusion & Future Work,[0],[0]
"Yes/No
• I have never used a speech-based personal assistant: Yes/No
• What was your general impression of our personal assistants?
",5 Conclusion & Future Work,[0],[0]
• Would you use one of these assistants on a smart phone or tablet if it were available?,5 Conclusion & Future Work,[0],[0]
"If yes, which one?
• Do you have suggestions that you think would help us improve our assistants?
",5 Conclusion & Future Work,[0],[0]
"• If you have used other speech-based interfaces before, do you prefer this interface?",5 Conclusion & Future Work,[0],[0]
"Arguably, spoken dialogue systems are most often used not in hands/eyes-busy situations, but rather in settings where a graphical display is also available, such as a mobile phone.",abstractText,[0],[0]
We explore the use of a graphical output modality for signalling incremental understanding and prediction state of the dialogue system.,abstractText,[0],[0]
"By visualising the current dialogue state and possible continuations of it as a simple tree, and allowing interaction with that visualisation (e.g., for confirmations or corrections), the system provides both feedback on past user actions and guidance on possible future ones, and it can span the continuum from slot filling to full prediction of user intent (such as GoogleNow).",abstractText,[0],[0]
"We evaluate our system with real users and report that they found the system intuitive and easy to use, and that incremental and adaptive settings enable users to accomplish more tasks.",abstractText,[0],[0]
Supporting Spoken Assistant Systems with a Graphical User Interface that Signals Incremental Understanding and Prediction State,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 640–645 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
640",text,[0],[0]
"In structured input-output models as used in tasks like translation and image captioning, the attention variable decides which part of the input aligns to the current output.",1 Introduction,[0],[0]
"Many attention mechanisms have been proposed (Xu et al., 2015; Bahdanau et al., 2014; Luong et al., 2015; Martins and Astudillo, 2016) but the de facto standard is a soft attention mechanism that first assigns attention weights to input encoder states, then computes an attention weighted ’soft’ aligned input state, which finally derives the output distribution.",1 Introduction,[0],[0]
"This method is end to end differentiable and easy to implement.
",1 Introduction,[0],[0]
Another less popular variant is hard attention that aligns each output to exactly one input state but requires intricate training to teach the network to choose that state.,1 Introduction,[0],[0]
"When successfully trained, hard attention is often found to be more accurate (Xu et al., 2015; Zaremba and Sutskever, 2015).",1 Introduction,[0],[0]
"In NLP, a recent success has been in a monotonic hard attention setting in morphological inflection tasks (Yu et al., 2016; Aharoni and Goldberg, 2017).",1 Introduction,[0],[0]
"For general seq2seq learning, methods like SparseMax (Martins and Astudillo, 2016) and local attention (Luong et al., 2015) were proposed to bridge the gap between soft and hard attention.
",1 Introduction,[0],[0]
"∗Both authors contributed equally to this work
In this paper we propose a surprisingly simpler alternative based on the original joint distribution between output and attention, of which existing soft and hard attention mechanisms are approximations.",1 Introduction,[0],[0]
"The joint model couples input states individually to the output like in hard attention, but it combines the advantage of end-to-end trainability of soft attention.",1 Introduction,[0],[0]
"When the number of input states is large, we propose to use a simple approximation of the full joint distribution called Beam-joint.",1 Introduction,[0],[0]
"This approximation is also easily trainable and does not suffer from the high variance of Monte-Carlo sampling gradients of hard attention.
",1 Introduction,[0],[0]
"We evaluated our model on five translation tasks and increased BLEU by 0.8 to 1.7 over soft attention, which in turn was better than hard and the recent Sparsemax (Martins and Astudillo, 2016) attention.",1 Introduction,[0],[0]
"More importantly, the training process was as easy as soft attention.",1 Introduction,[0],[0]
"For further support, we also evaluate on two morphological inflection tasks and got gains over soft and hard attention.",1 Introduction,[0],[0]
For sequence to sequence (seq2seq) learning the encoder-decoder model is the standard and we review it here.,2 Background and Related Work,[0],[0]
We then review related work on attention mechanisms on these models.,2 Background and Related Work,[0],[0]
"Let x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm denote the tokens in the input sequence that have been transformed by an encoder network to state vectors x1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", xm, which we jointly denote as x1...m. Let y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn denote the output tokens in the target sequence.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"The Encoder-Decoder (ED) network factorizes Pr(y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
", yn|x1...m) as ∏n t=1 Pr(yt|x1...m, st) where st is a decoder state summarizing y1, . . .",2.1 Attention-based Encoder Decoder Model,[0],[0]
yt−1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"For each t, a hidden attention variable at is used to denote which part of x1...m aligns with yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Let P (at = j|x1...m, st) denote the
probability that encoder state xj is relevant for output yt.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Typically this is estimated using a softmax function over attention scores computed from xj and decoder state st as follows.
",2.1 Attention-based Encoder Decoder Model,[0],[0]
"P (at = j|x1...m, st) =",2.1 Attention-based Encoder Decoder Model,[0],[0]
"eAθ(xj ,st)∑m r=1",2.1 Attention-based Encoder Decoder Model,[0],[0]
"e Aθ(xr,st) (1)
where Aθ(., .) is the attention unit that scores each input state xj as per the decoder state st.",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Thereafter, in the popular soft-attention mechanism, the attention weighted sum of the input states is used to model log likelihood for each yt as
log Pr(yt|x1...m) = log Pr(yt| ∑ a Pt(a)xa) (2)
where Pt(at = j) is the short form for P (at = j|x1...m, st).",2.1 Attention-based Encoder Decoder Model,[0],[0]
"Also, here and in the rest of the paper we drop st from P (yt) and Pt(a) for ease of notation.",2.1 Attention-based Encoder Decoder Model,[0],[0]
The weighted sum ∑ a Pt(a)xa is called an input context ct which is fed to the decoder RNN along with yt for computing the next state st+1.,2.1 Attention-based Encoder Decoder Model,[0],[0]
"We next review existing attention types.
",2.2 Related Work,[0],[0]
"Soft Attention is the attention method described in the previous section and is the current standard for seq2seq learning (Xu Chen, 2018; Koehn, 2017).",2.2 Related Work,[0],[0]
"It was proposed for translation in (Bahdanau et al., 2014) and refined further in (Luong et al., 2015).",2.2 Related Work,[0],[0]
"As shown in Eq 2, here each output is derived from an attention averaged input.",2.2 Related Work,[0],[0]
This diffuses the coupling between the input and output.,2.2 Related Work,[0],[0]
"The advantage of soft attention is end to end differentiability, and fast training and inference.
",2.2 Related Work,[0],[0]
"Hard Attention was proposed in its current form in (Xu et al., 2015) and attends to exactly one input state for an output1.",2.2 Related Work,[0],[0]
"During training, log-likelihood is an expectation over sampled attentions:
logPt(yt|x1...m) = M∑ l=1 logPt(yt|xãl) (3)
where ã1, . . .",2.2 Related Work,[0],[0]
", ãM are sampled from the multinomial Pt(a).",2.2 Related Work,[0],[0]
"Because of the sampling, the gradient has to be computed by Monte Carlo gradient/REINFORCE (Williams, 1992) and is subject to high variance.",2.2 Related Work,[0],[0]
"Many tricks are required to train
1Note, attention on a single input encoder state does not imply attention on a single input token because RNNs or selfattention capture the context around the token.
hard attention and there is little standardization across implementations.",2.2 Related Work,[0],[0]
Xu et al (2015) use a combination of REINFORCE and soft attention.,2.2 Related Work,[0],[0]
Zaremba et al(2015) uses curriculum learning that starts as soft-attention and gradually becomes discrete.,2.2 Related Work,[0],[0]
"Ling& Rush (2017) aggregates multiple samples during training, and a single sampled attention while testing.",2.2 Related Work,[0],[0]
"However, once trained well the sharp focus on memory provided by hard-attention has been found to yield superior performance (Xu et al., 2015; Shankar and Sarawagi, 2018).
",2.2 Related Work,[0],[0]
Sparse/Local Attention Many attempts have been made to bridge the gap between soft and hard attention.,2.2 Related Work,[0],[0]
Luong et al (2015) proposes local attention that averages a window of input.,2.2 Related Work,[0],[0]
"This has been refined later to include syntax (Chen et al., 2017; Sennrich and Haddow, 2016; Chen et al., 2018).",2.2 Related Work,[0],[0]
"Another idea is to replace the softmax in soft attention with sparsity inducing operators (Martins and Astudillo, 2016; Niculae and Blondel, 2017).",2.2 Related Work,[0],[0]
"However, all sparse/local attention methods continue to compute P (y) from an attention weighted sum of inputs (Eq: 2) unlike hard attention.",2.2 Related Work,[0],[0]
"We start from an explicit joint representation of the uncertainty of the attention and output variables.
",3 Joint Attention-Output Models,[0],[0]
logPt(yt|x1...m) = log ∑ a Pt(a)Pt(yt|xa),3 Joint Attention-Output Models,[0],[0]
"(4)
The joint model directly couples individual input states to the output, and thus is a type of hard attention.",3 Joint Attention-Output Models,[0],[0]
"Also, by taking an expectation, instead of a single hard attention, it enjoys differentiability as in soft-attention.",3 Joint Attention-Output Models,[0],[0]
"We call this the full-joint method.
",3 Joint Attention-Output Models,[0],[0]
"Unfortunately, either when the vocabulary or the number of encoder states (m) is large, full-joint is not practical.",3 Joint Attention-Output Models,[0],[0]
Existing hard and soft attentions can be viewed as its approximations that either marginalize early or hard select attention.,3 Joint Attention-Output Models,[0],[0]
We show a surprisingly simple alternative approximation that provides hard attention without its training complexity.,3 Joint Attention-Output Models,[0],[0]
"Our method called Beam-joint deterministically selects the top-k highest attention values and approximates the full joint log probability as
logPt(yt|x1...m)",3 Joint Attention-Output Models,[0],[0]
"≈ log ∑
a∈TopK(Pt(a))
Pt(a)Pt(yt|xa)",3 Joint Attention-Output Models,[0],[0]
"(5)
Thus, in beam-joint, we first compute the multinomial attention distribution in O(m) time using
Eq 1, select the Top-K input positions from the multinomial, next with hard attention on each position compute K output softmax, and finally compute the attention weighted output mixture distribution.",3 Joint Attention-Output Models,[0],[0]
The number of output softmax is K times in normal soft-attention but the actual running time overhead is only 20–30% for translation tasks.,3 Joint Attention-Output Models,[0],[0]
We used the default pass-through TopK operator (which is not differentiable) and optimize the beamapproximation directly.,3 Joint Attention-Output Models,[0],[0]
"We also experimented with a version which smoothly shifts from soft-attention to beam-attention, but found that training the beamapproximation directly leads to best results.
",3 Joint Attention-Output Models,[0],[0]
We show empirically that this very simple scheme is surprisingly effective compared to existing hard and soft attention over several translation tasks.,3 Joint Attention-Output Models,[0],[0]
"Unlike sampling and variational methods that require careful tuning and exotic tricks during training, this simple scheme trains as easily as softattention, without significant increase in training time because even K = 6 works well enough.
",3 Joint Attention-Output Models,[0],[0]
"Another reason why our ’sum of probabilities’ form performs better could be the softmax barrier effect highlighted in (Yang et al., 2018).",3 Joint Attention-Output Models,[0],[0]
The authors argue that the richness of natural language cannot be captured in normal softmax due to the low rank constraint it imposes on input-to-output matrix.,3 Joint Attention-Output Models,[0],[0]
They improve performance using a Mixture of Softmax model.,3 Joint Attention-Output Models,[0],[0]
Our beam-joint also is a mixture of softmax and possibly achieves higher rank than a single softmax.,3 Joint Attention-Output Models,[0],[0]
"However their mixture requires learning multiple softmax matrices, whereas ours are due to varying attention and we do not learn any extra parameters than soft attention.",3 Joint Attention-Output Models,[0],[0]
We compare attention models on two NLP tasks: machine translation and morphological inflection.,4 Experiments,[0],[0]
We experiment on five language pairs from three datasets:,4.1 Machine translation,[0],[0]
"IWSLT15 English↔Vietnamese (Cettolo et al., 2015) which contains 133k train, 1.5k validation(tst2012) and 1.2k test(tst2013) sentence pairs respectively; IWSLT14 German↔English (Cettolo et al., 2014) which contains 160k train, 7.2k validation and 6.7k test sentence pairs respectively ; Workshop on Asian Translation 2017 Japanese→English",4.1 Machine translation,[0],[0]
"(Nakazawa et al., 2016) which contains 2M train, 1.8k validation and 1.8k test sentence pairs respectively.",4.1 Machine translation,[0],[0]
"We use a 2 layer bi-
directional encoder and a 2 layer unidirectional decoder with 512 hidden LSTM units and 0.2 dropout rate with vanilla SGD optimizer.",4.1 Machine translation,[0],[0]
We base our implementation2 on the NMT code3 in Tensorflow.,4.1 Machine translation,[0],[0]
"We did no special hyper-parameter tuning and used standard-softmax tuned parameters on a batch size of 64.
",4.1 Machine translation,[0],[0]
Comparing attention models We compare beam-joint (default K = 6) with standard soft and hard attention.,4.1 Machine translation,[0],[0]
"To further dissect the reasons behind beam-joint’s gains, we compare beam-joint with a sampling based approximation of full-joint called Sample-Joint that replaces the TopK in Eq 5 with K attention weighted samples.",4.1 Machine translation,[0],[0]
We train samplejoint as well as hard-attention with REINFORCE with 6-samples.,4.1 Machine translation,[0],[0]
"Also to ascertain that our gains are not explained by sparsity alone, we compare with Sparsemax (Martins and Astudillo, 2016).
",4.1 Machine translation,[0],[0]
In Table 1 we show perplexity and BLEU with three beam sizes (B).,4.1 Machine translation,[0],[0]
"Beam-joint significantly outperforms all other variants, including the standard soft attention by 0.8 to 1.7 BLEU points.",4.1 Machine translation,[0],[0]
The perplexity shows even a more impressive drop in all five datasets.,4.1 Machine translation,[0],[0]
"Also we observe training times for beam-joint to be only 20–30% higher than softattention, establishing that beam-joint is both practical and more accurate.
",4.1 Machine translation,[0],[0]
Sample-joint is much worse than beam-joint.,4.1 Machine translation,[0],[0]
"Apart from the problem of high variance of gradients in the reinforce step, another problem is that sampling repeats states whereas TopK in beamjoint gets distinct states.",4.1 Machine translation,[0],[0]
"Hard attention too faces training issues and performs worse than soft attention, explaining why it is not commonly used in NMT.",4.1 Machine translation,[0],[0]
"Sample-joint is better than Hard attention, further highlighting the merits of the joint distribution.",4.1 Machine translation,[0],[0]
Sparsemax is competitive but marginally worse than soft attention.,4.1 Machine translation,[0],[0]
"This is concordant with the recent experiments of (Niculae and Blondel, 2017).
",4.1 Machine translation,[0],[0]
Comparison with Full Joint Next we evaluate the impact of our beam-joint approximation against full-joint and soft attention.,4.1 Machine translation,[0],[0]
"Full-joint cannot scale to large vocabularies, therefore we only compare on En-Vi with a batch size of 32.",4.1 Machine translation,[0],[0]
Figure 1a shows final BLEU of these methods as well as BLEU against increasing training steps.,4.1 Machine translation,[0],[0]
"Beam-joint both converges faster and to a higher score than soft-
2https://github.com/sid7954/beam-joint-attention 3https://github.com/tensorflow/nmt
attention.",4.1 Machine translation,[0],[0]
"For example by 10000 steps ( 5 epochs), beam-joint has surpassed soft-attention by almost 2 BLEU points (20 vs 22).",4.1 Machine translation,[0],[0]
"Moreover beam-joint tracks full-joint well, and both converge finally to similar BLEUs near 27 against 26 for soft attention.",4.1 Machine translation,[0],[0]
"This shows that an attention-beam of size 6 suffices to approximate full joint almost perfectly.
",4.1 Machine translation,[0],[0]
"Next, in Figure 1b, we compare beam-joint (solid lines) and soft attention (dotted lines) for convergence rates on three other datasets.",4.1 Machine translation,[0],[0]
"For each dataset beam-joint trains faster with a consistent improvement of more than 1 BLEU.
",4.1 Machine translation,[0],[0]
Effect of K in Beam-joint We show the effect of K used in TopK of beam-joint in Figure 2 on the En-Vi and De-En tasks.,4.1 Machine translation,[0],[0]
On En-Vi BLEU increases from 16.0 to 25.7 to 26.5 as K increases from 1 to 2 to 3; and then saturates quickly.,4.1 Machine translation,[0],[0]
Similar behavior is observed in the other dataset.,4.1 Machine translation,[0],[0]
"This shows that small K values like 6 suffice for translation.
",4.1 Machine translation,[0],[0]
We further evaluate whether the performance gain of beam-joint is due to the softmax barrier alone in Table 2.,4.1 Machine translation,[0],[0]
"We used our models trained with K=6, and deployed them for test-time greedy decoding with K set to 1.",4.1 Machine translation,[0],[0]
"Since the output now has only a single softmax component, this model faces the same bottleneck as soft-attention.",4.1 Machine translation,[0],[0]
"One can observe that as expected these results are worse than beam-joint with K=6, however they still exceed soft-attention by a significant margin, demonstrating that the performance gain is not solely due to the effect of ensembling or softmax-barrier.",4.1 Machine translation,[0],[0]
"To demonstrate the use of this approach beyond translation, we next consider two morphological
inflection tasks.",4.2 Morphological Inflection,[0],[0]
"We use (Durrett and DeNero, 2013)’s dataset containing 8 inflection forms for German Nouns (de-N) and 27 forms for German Verbs (de-V).",4.2 Morphological Inflection,[0],[0]
The number of training words is 2364 and 1627 respectively while the validation and test words are 200 each.,4.2 Morphological Inflection,[0],[0]
"We train a one layer encoder and decoder with 128 hidden LSTM units each with a dropout rate of 0.2 using Adam(Kingma and Ba, 2014) and measure 0/1 accuracy for soft, hard and full-joint attention models.",4.2 Morphological Inflection,[0],[0]
"Due to limited input length and vocabulary, we were able to run directly the full-joint model.",4.2 Morphological Inflection,[0],[0]
"We also ran the 100 units wide two layer LSTM with hard-monotonic attention provided by (Aharoni and Goldberg, 2017) labeled Hard-Mono4.",4.2 Morphological Inflection,[0],[0]
The table below shows that even for this task full-joint scores over existing attention models5.,4.2 Morphological Inflection,[0],[0]
"The generic full-joint attention provides slight gains even over the task specific hard-monotonic attention.
",4.2 Morphological Inflection,[0],[0]
"Dataset Soft Hard HardMono
FullJoint
de-N 85.50 85.13 85.65 85.81 de-V 94.91 95.04 95.31 95.52
Conclusion
",4.2 Morphological Inflection,[0],[0]
In this paper we showed a simple yet effective approximation of the joint attention-output distribution in sequence to sequence learning.,4.2 Morphological Inflection,[0],[0]
Our joint model consistently provides higher accuracy without significant running time overheads in five translation and two morphological inflection tasks.,4.2 Morphological Inflection,[0],[0]
"An interesting direction for future work is to extend beam-joint to multi-head attention architectures as in (Vaswani et al., 2017; Xu Chen, 2018).
",4.2 Morphological Inflection,[0],[0]
"Acknowledgements We thank NVIDIA Corporation for supporting this research by the donation of Titan X GPU.
4https://github.com/roeeaharoni/morphologicalreinflection
5Our numbers are lower than earlier reported because ours use a single model whereas (Aharoni and Goldberg, 2017) and others report from an ensemble of five models.",4.2 Morphological Inflection,[0],[0]
"In this paper we show that a simple beam approximation of the joint distribution between attention and output is an easy, accurate, and efficient attention mechanism for sequence to sequence learning.",abstractText,[0],[0]
The method combines the advantage of sharp focus in hard attention and the implementation ease of soft attention.,abstractText,[0],[0]
On five translation and two morphological inflection tasks we show effortless and consistent gains in BLEU compared to existing attention mechanisms.,abstractText,[0],[0]
Surprisingly Easy Hard-Attention for Sequence to Sequence Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 93–104 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
93
We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations. To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data. To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals. Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task. We provide comprehensive analysis that indicates significant opportunities for future research.",text,[0],[0]
"When we read a story, we bring to it a large body of implicit knowledge about the physical world.",1 Introduction,[0],[0]
"For instance, given the context “on stage, a woman takes a seat at the piano,” shown in Table 1, we can easily infer what the situation might look like: a woman is giving a piano performance, with a crowd watching her.",1 Introduction,[0],[0]
"We can furthermore infer her likely next action: she will most likely set her fingers on the piano keys and start playing.
",1 Introduction,[0],[0]
"This type of natural language inference requires commonsense reasoning, substantially broadening the scope of prior work that focused primarily on
linguistic entailment (Chierchia and McConnellGinet, 2000).",1 Introduction,[0],[0]
"Whereas the dominant entailment paradigm asks if two natural language sentences (the ‘premise’ and the ‘hypothesis’) describe the same set of possible worlds (Dagan et al., 2006; Bowman et al., 2015), here we focus on whether a (multiple-choice) ending describes a possible (future) world that can be anticipated from the situation described in the premise, even when it is not strictly entailed.",1 Introduction,[0],[0]
"Making such inference necessitates a rich understanding about everyday physical situations, including object affordances (Gibson, 1979) and frame semantics (Baker et al., 1998).
",1 Introduction,[0],[0]
A first step toward grounded commonsense inference with today’s deep learning machinery is to create a large-scale dataset.,1 Introduction,[0],[0]
"However, recent work has shown that human-written datasets are susceptible to annotation artifacts: unintended stylistic patterns that give out clues for the gold labels (Gururangan et al., 2018; Poliak et al., 2018).",1 Introduction,[0],[0]
"As a result, models trained on such datasets with hu-
man biases run the risk of over-estimating the actual performance on the underlying task, and are vulnerable to adversarial or out-of-domain examples (Wang et al., 2018; Glockner et al., 2018).
",1 Introduction,[0],[0]
"In this paper, we introduce Adversarial Filtering (AF), a new method to automatically detect and reduce stylistic artifacts.",1 Introduction,[0],[0]
We use this method to construct Swag: an adversarial dataset with 113k multiple-choice questions.,1 Introduction,[0],[0]
"We start with pairs of temporally adjacent video captions, each with a context and a follow-up event that we know is physically possible.",1 Introduction,[0],[0]
We then use a state-of-theart language model fine-tuned on this data to massively oversample a diverse set of possible negative sentence endings (or counterfactuals).,1 Introduction,[0],[0]
"Next, we filter these candidate endings aggressively and adversarially using a committee of trained models to obtain a population of de-biased endings with similar stylistic features to the real ones.",1 Introduction,[0],[0]
"Finally, these filtered counterfactuals are validated by crowd workers to further ensure data quality.
",1 Introduction,[0],[0]
"Extensive empirical results demonstrate unique contributions of our dataset, complementing existing datasets for natural langauge inference (NLI) (Bowman et al., 2015; Williams et al., 2018) and commonsense reasoning (Roemmele et al., 2011; Mostafazadeh et al., 2016; Zhang et al., 2017).",1 Introduction,[0],[0]
"First, our dataset poses a new challenge of grounded commonsense inference that is easy for humans (88%) while hard for current state-ofthe-art NLI models (<60%).",1 Introduction,[0],[0]
"Second, our proposed adversarial filtering methodology allows for cost-effective construction of a large-scale dataset while substantially reducing known annotation artifacts.",1 Introduction,[0],[0]
"The generality of adversarial filtering allows it to be applied to build future datasets, ensuring that they serve as reliable benchmarks.
",1 Introduction,[0],[0]
"2 Swag: Our new dataset
We introduce a new dataset for studying physically grounded commonsense inference, called Swag.1",1 Introduction,[0],[0]
Our task is to predict which event is most likely to occur next in a video.,1 Introduction,[0],[0]
"More formally, a model is given a context c = (s,n): a complete sentence s and a noun phrase n that begins a second sentence, as well as a list of possible verb phrase sentence endings V = {v1, . . .",1 Introduction,[0],[0]
",v4}.",1 Introduction,[0],[0]
"See Figure 1 for an example triple (s,n,vi).",1 Introduction,[0],[0]
"The model must then select the most appropriate verb phrase vî ∈ V .
1Short for Situations With Adversarial Generations.
",1 Introduction,[0],[0]
"Overview Our corpus consists of 113k multiple choice questions (73k training, 20k validation, 20k test) and is derived from pairs of consecutive video captions from ActivityNet Captions (Krishna et al., 2017; Heilbron et al., 2015) and the Large Scale Movie Description Challenge (LSMDC; Rohrbach et al., 2017).",1 Introduction,[0],[0]
The two datasets are slightly different in nature and allow us to achieve broader coverage: ActivityNet contains 20k YouTube clips containing one of 203 activity types (such as doing gymnastics or playing guitar); LSMDC consists of 128k movie captions (audio descriptions and scripts).,1 Introduction,[0],[0]
"For each pair of captions, we use a constituency parser (Stern et al., 2017) to split the second sentence into noun and verb phrases (Figure 1).2 Each question has a human-verified gold ending and 3 distractors.",1 Introduction,[0],[0]
"In this section, we outline the construction of Swag.",3 A solution to annotation artifacts,[0],[0]
"We seek dataset diversity while minimizing annotation artifacts, conditional stylistic patterns such as length and word-preference biases.",3 A solution to annotation artifacts,[0],[0]
"For many NLI datasets, these biases have been shown to allow shallow models (e.g. bag-of-words) obtain artificially high performance.
",3 A solution to annotation artifacts,[0],[0]
"To avoid introducing easily “gamed” patterns, we present Adversarial Filtering (AF), a generallyapplicable treatment involving the iterative refinement of a set of assignments to increase the entropy under a chosen model family.",3 A solution to annotation artifacts,[0],[0]
"We then discuss how we generate counterfactual endings, and
2We filter out sentences with rare tokens (≤3 occurrences), that are short (l ≤ 5), or that lack a verb phrase.
",3 A solution to annotation artifacts,[0],[0]
Algorithm 1 Adversarial filtering (AF) of negative samples.,3 A solution to annotation artifacts,[0],[0]
"During our experiments, we set Neasy = 2 for refining a population ofN− = 1023 negative examples to k = 9, and used a 80%/20% train/test split.
while convergence not reached do • Split the dataset D randomly up into training and testing portions Dtr and Dte. •",3 A solution to annotation artifacts,[0],[0]
Optimize a model fθ on Dtr. for index i in Dte do •,3 A solution to annotation artifacts,[0],[0]
Identify easy indices:,3 A solution to annotation artifacts,[0],[0]
Aeasyi = {j ∈ Ai : fθ(x,3 A solution to annotation artifacts,[0],[0]
+ i ) > fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,j)}
• Replace N easy easy indices j ∈ Aeasyi with adversarial indices k 6∈",3 A solution to annotation artifacts,[0],[0]
Ai satisfying fθ(x,3 A solution to annotation artifacts,[0],[0]
"− i,k) > fθ(x",3 A solution to annotation artifacts,[0],[0]
"− i,j).
end for end while
finally, the models used for filtering.",3 A solution to annotation artifacts,[0],[0]
"In this section, we formalize what it means for a dataset to be adversarial.",3.1 Formal definition,[0],[0]
"Intuitively, we say that an adversarial dataset for a model f is one on which f will not generalize, even if evaluated on test data from the same distribution.",3.1 Formal definition,[0],[0]
"More formally, let our input space be X and the label space be Y .",3.1 Formal definition,[0],[0]
"Our trainable classifier f , taking parameters θ is defined as fθ : X → R|Y|.",3.1 Formal definition,[0],[0]
"Let our dataset of size N be defined as D = {(xi, yi)}1≤i≤N , and let the loss function over the dataset be L(fθ,D).",3.1 Formal definition,[0],[0]
"We say that a dataset is adversarial with respect to f if we expect high empirical error I over all leave-one-out train/test splits (Vapnik, 2000):
I(D, f)",3.1 Formal definition,[0],[0]
"= 1 N N∑ i=1 L(fθ?i , {(xi, yi)}), (1)
where θ?i = argmin θ L(fθ,D \ {(xi, yi)}), (2)
",3.1 Formal definition,[0],[0]
with regularization terms omitted for simplicity.,3.1 Formal definition,[0],[0]
"In this section, we outline an approach for generating an adversarial dataset D, effectively maximizing empirical error I with respect to a family of trainable classifiers f .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"Without loss of generality, we consider the situation where we have N contexts, each associated with a single positive example (x+i , 1)∈X ×Y , and a large population of context-specific negative examples (x−i,j , 0)∈X ×Y , where 1≤j≤N− for each i. For instance, the negative examples could be incorrect relations in knowledge-base completion (Socher et al., 2013), or all words in a dictionary for a
single-word cloze task (Zweig and Burges, 2011).",3.2 Adversarial filtering (AF) algorithm,[0],[0]
Our goal will be to filter the population of negative examples for each instance i to a size of k N−.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This will be captured by returning a set of assignments A, where for each instance the assignment will be a k-subset Ai = [1 . . .",3.2 Adversarial filtering (AF) algorithm,[0],[0]
N−]k.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"The filtered dataset will then be:
DAF = {(xi, 1), {(x−i,j , 0)}j∈Ai}1≤i≤N (3)
Unfortunately, optimizing I(DAF , f) is difficult as A is global and non-differentiable.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To address this, we present Algorithm 1.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"On each iteration, we split the data into dummy ‘train’ and ‘test’ splits.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"We train a model f on the training portion and obtain parameters θ, then use the remaining test portion to reassign the indices of A.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"For each context, we replace some number of ‘easy’ negatives in A that fθ classifies correctly with ‘adversarial’ negatives outside ofA that fθ misclassifies.
",3.2 Adversarial filtering (AF) algorithm,[0],[0]
"This process can be thought of as increasing the overall entropy of the dataset: given a strong model fθ that is compatible with a random subset of the data, we aim to ensure it cannot generalize to the held-out set.",3.2 Adversarial filtering (AF) algorithm,[0],[0]
We repeat this for several iterations to reduce the generalization ability of the model family f over arbitrary train/test splits.,3.2 Adversarial filtering (AF) algorithm,[0],[0]
"To generate counterfactuals for Swag, we use an LSTM (Hochreiter and Schmidhuber, 1997) language model (LM), conditioned on contexts from video captions.",3.3 Generating candidate endings,[0],[0]
"We first pretrain on BookCorpus (Zhu et al., 2015), then finetune on the video caption datasets.",3.3 Generating candidate endings,[0],[0]
The architecture uses standard best practices and was validated on held-out perplexity of the video caption datasets; details are in the appendix.,3.3 Generating candidate endings,[0],[0]
"We use the LM to sample N−=1023 unique endings for a partial caption.3
Importantly, we greedily sample the endings, since beam search decoding biases the generated endings to be of lower perplexity (and thus easily distinguishable from found endings).",3.3 Generating candidate endings,[0],[0]
"We find this process gives good counterfactuals: the generated endings tend to use topical words, but often make little sense physically, making them perfect for our task.",3.3 Generating candidate endings,[0],[0]
"Further, the generated endings are marked as “gibberish” by humans only 9.1% of the time (Sec 3.5); in that case the ending is filtered out.
",3.3 Generating candidate endings,[0],[0]
"3To ensure that the LM generates unique endings, we split the data into five validation folds and train five separate LMs, one for each set of training folds.",3.3 Generating candidate endings,[0],[0]
This means that each LM never sees the found endings during training.,3.3 Generating candidate endings,[0],[0]
"In creating Swag, we designed the model family f to pick up on low-level stylistic features that we posit should not be predictive of whether an event happens next in a video.",3.4 Stylistic models for adversarial filtering,[0],[0]
"These stylistic features are an obvious case of annotation artifacts (Cai et al., 2017; Schwartz et al.,",3.4 Stylistic models for adversarial filtering,[0],[0]
2017).4 Our final classifier is an ensemble of four stylistic models:,3.4 Stylistic models for adversarial filtering,[0],[0]
1.,3.4 Stylistic models for adversarial filtering,[0],[0]
A multilayer perceptron (MLP) given LM perplexity features and context/ending lengths.,3.4 Stylistic models for adversarial filtering,[0],[0]
2.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bag-of-words model that averages the word embeddings of the second sentence as features.,3.4 Stylistic models for adversarial filtering,[0],[0]
3.,3.4 Stylistic models for adversarial filtering,[0],[0]
"A one-layer CNN, with filter sizes ranging from 2-5, over the second sentence.",3.4 Stylistic models for adversarial filtering,[0],[0]
4.,3.4 Stylistic models for adversarial filtering,[0],[0]
A bidirectional LSTM over the 100 most common words in the second sentence; uncommon words are replaced by their POS tags.,3.4 Stylistic models for adversarial filtering,[0],[0]
We ensemble the models by concatenating their final representations and passing it through an MLP.,3.4 Stylistic models for adversarial filtering,[0],[0]
"On every adversarial iteration, the ensemble is trained jointly to minimize cross-entropy.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"The accuracies of these models (at each iteration, evaluated on a 20% split of the test dataset before indices of A get remapped) are shown in Figure 2.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Performance decreases from 60% to close to random chance; moreover, confusing the perplexity-based MLP is not sufficient to lower performance of the ensemble.",3.4 Stylistic models for adversarial filtering,[0],[0]
"Only once the other stylistic models are added does the ensemble accuracy drop substantially, suggesting that our approach is effective at reducing stylistic artifacts.
",3.4 Stylistic models for adversarial filtering,[0],[0]
"4A broad definition of annotation artifacts might include aspects besides lexical/stylistic features: for instance, certain events are less likely semantically regardless of the context (e.g. riding a horse using a hose).",3.4 Stylistic models for adversarial filtering,[0],[0]
"For this work, we erred more conservatively and only filtered based on style.",3.4 Stylistic models for adversarial filtering,[0],[0]
The final data-collection step is to have humans verify the data.,3.5 Human verification,[0],[0]
"Workers on Amazon Mechanical Turk were given the caption context, as well as six candidate endings: one found ending and five adversarially-sampled endings.",3.5 Human verification,[0],[0]
The task was twofold:,3.5 Human verification,[0],[0]
"Turkers ranked the endings independently as likely, unlikely, or gibberish, and selected the best and second best endings (Fig 3).
",3.5 Human verification,[0],[0]
We obtained the correct answers to each context in two ways.,3.5 Human verification,[0],[0]
"If a Turker ranks the found ending as either best or second best (73.7% of the time), we add the found ending as a gold example, with negatives from the generations not labelled best or gibberish.",3.5 Human verification,[0],[0]
"Further, if a Turker ranks a generated ending as best, and the found ending as second best, then we have reason to believe that the generation is good.",3.5 Human verification,[0],[0]
"This lets us add an additional training example, consisting of the generated best ending as the gold, and remaining generations as negatives.5 Examples with ≤3 nongibberish endings were filtered out.6
We found after 1000 examples that the annotators tended to have high agreement, also generally choosing found endings over generations (see Table 2).",3.5 Human verification,[0],[0]
"Thus, we collected the remaining 112k examples with one annotator each, periodically verifying that annotators preferred the found endings.",3.5 Human verification,[0],[0]
"In this section, we evaluate the performance of various NLI models on Swag.",4 Experiments,[0],[0]
"Recall that models
5These two examples share contexts.",4 Experiments,[0],[0]
"To prevent biasing the test and validation sets, we didn’t perform this procedure on answers from the evaluation sets’ context.
",4 Experiments,[0],[0]
"6To be data-efficient, we reannotated filtered-out examples by replacing gibberish endings, as well as generations that outranked the found ending, with candidates from A.
for our dataset take the following form: given a sentence and a noun phrase as context c = (s,n), as well as a list of possible verb phrase endings V = {v1, . . .",4 Experiments,[0],[0]
",v4}, a model fθ must select a verb î that hopefully matches igold:
î =",4 Experiments,[0],[0]
"argmax i fθ(s,n,vi) (4)
To study the amount of bias in our dataset, we also consider models that take as input just the ending verb phrase vi, or the entire second sentence (n,vi).",4 Experiments,[0],[0]
"For our learned models, we train f by minimizing multi-class cross-entropy.",4 Experiments,[0],[0]
"We consider three different types of word representations: 300d GloVe vectors from Common Crawl (Pennington et al., 2014), 300d Numberbatch vectors retrofitted using ConceptNet relations (Speer et al., 2017), and 1024d ELMo contextual representations that show improvement on a variety of NLP tasks, including standard NLI (Peters et al., 2018).",4 Experiments,[0],[0]
"We follow the final dataset split (see Section 2) using two training approaches: training on the found data, and the found and highly-ranked generated data.",4 Experiments,[0],[0]
See the appendix for more details.,4 Experiments,[0],[0]
"The following models predict labels from a single span of text as input; this could be the ending only, the second sentence only, or the full passage.",4.1 Unary models,[0],[0]
a.,4.1 Unary models,[0],[0]
"fastText (Joulin et al., 2017):",4.1 Unary models,[0],[0]
"This library models a single span of text as a bag of n-grams, and tries to predict the probability of an ending being correct or incorrect independently.7",4.1 Unary models,[0],[0]
"b. Pretrained sentence encoders We consider two types of pretrained RNN sentence encoders, SkipThoughts (Kiros et al., 2015) and InferSent
7The fastText model is trained using binary cross-entropy; at test time we extract the prediction by selecting the ending with the highest positive likelihood under the model.
",4.1 Unary models,[0],[0]
"(Conneau et al., 2017).",4.1 Unary models,[0],[0]
"SkipThoughts was trained by predicting adjacent sentences in book data, whereas InferSent was trained on supervised NLI data.",4.1 Unary models,[0],[0]
"For each second sentence (or just the ending), we feed the encoding into an MLP.",4.1 Unary models,[0],[0]
c. LSTM sentence encoder,4.1 Unary models,[0],[0]
"Given an arbitrary span of text, we run a two-layer BiLSTM over it.",4.1 Unary models,[0],[0]
"The final hidden states are then max-pooled to obtain a fixed-size representation, which is then used to predict the potential for that ending.",4.1 Unary models,[0],[0]
The following models predict labels from two spans of text.,4.2 Binary models,[0],[0]
"We consider two possibilties for these models: using just the second sentence, where the two text spans are n,vi, or using the context and the second sentence, in which case the spans are s, (n,vi).",4.2 Binary models,[0],[0]
The latter case includes many models developed for the NLI task.,4.2 Binary models,[0],[0]
"d. Dual Bag-of-Words For this baseline, we treat each sentence as a bag-of-embeddings (c,vi).",4.2 Binary models,[0],[0]
We model the probability of picking an ending i using a bilinear model: softmaxi(cWvTi ).,4.2 Binary models,[0],[0]
"8 e. Dual pretrained sentence encoders Here, we obtain representations from SkipThoughts or InferSent for each span, and compute their pairwise compatibility using either 1) a bilinear model or 2) an MLP from their concatenated representations.",4.2 Binary models,[0],[0]
"f. SNLI inference Here, we consider two models that do well on SNLI (Bowman et al., 2015): Decomposable Attention (Parikh et al., 2016) and ESIM (Chen et al., 2017).",4.2 Binary models,[0],[0]
"We use pretrained versions of these models (with ELMo embeddings) on SNLI to obtain 3-way entailment, neutral, and contradiction probabilities for each example.",4.2 Binary models,[0],[0]
We then train a log-linear model using these 3-way probabilities as features.,4.2 Binary models,[0],[0]
g. SNLI models (retrained),4.2 Binary models,[0],[0]
"Here, we train ESIM and Decomposable Attention on our dataset: we simply change the output layer size to 1 (the potential of an ending vi) with a softmax over i.",4.2 Binary models,[0],[0]
We also considered the following models:,4.3 Other models,[0],[0]
h. Length:,4.3 Other models,[0],[0]
"Although length was used by the adversarial classifier, we want to verify that human validation didn’t reintroduce a length bias.",4.3 Other models,[0],[0]
"For this baseline, we always choose the shortest ending.",4.3 Other models,[0],[0]
i. ConceptNet,4.3 Other models,[0],[0]
"As our task requires world knowledge, we tried a rule-based system on top of the
8We also tried using an MLP, but got worse results.
",4.3 Other models,[0],[0]
"ConceptNet knowledge base (Speer et al., 2017).",4.3 Other models,[0],[0]
"For an ending sentence, we use the spaCy dependency parser to extract the head verb and its dependent object.",4.3 Other models,[0],[0]
The ending score is given by the number of ConceptNet causal relations9 between synonyms of the verb and synonyms of the object.,4.3 Other models,[0],[0]
"j. Human performance To benchmark human performance, five Mechanical Turk workers were asked to answer 100 dataset questions, as did an ‘expert’ annotator (the first author of this paper).",4.3 Other models,[0],[0]
Predictions were combined using a majority vote.,4.3 Other models,[0],[0]
We present our results in Table 3.,4.4 Results,[0],[0]
"The best model that only uses the ending is the LSTM sequence model with ELMo embeddings, which obtains 43.6%.",4.4 Results,[0],[0]
"This model, as with most models studied, greatly improves with more context: by 3.1% when given the initial noun phrase, and by an ad-
9We used the relations ‘Causes’, ‘CapableOf’, ‘ReceivesAction’, ‘UsedFor’, and ‘HasSubevent’.",4.4 Results,[0],[0]
"Though their coverage is low (30.4% of questions have an answer with≥1 causal relation), the more frequent relations in ConceptNet, such as ‘IsA’, at best only indirectly relate to our task.
",4.4 Results,[0],[0]
ditional 4% when also given the first sentence.,4.4 Results,[0],[0]
Further improvement is gained from models that compute pairwise representations of the inputs.,4.4 Results,[0],[0]
"While the simplest such model, DualBoW, obtains only 35.1% accuracy, combining InferSent sentence representations gives 40.5% accuracy (InferSent-Bilinear).",4.4 Results,[0],[0]
"The best results come from pairwise NLI models: when fully trained on Swag, ESIM+ELMo obtains 59.2% accuracy.
",4.4 Results,[0],[0]
"When comparing machine results to human results, we see there exists a lot of headroom.",4.4 Results,[0],[0]
"Though there likely is some noise in the task, our results suggest that humans (even untrained) converge to a consensus.",4.4 Results,[0],[0]
"Our in-house “expert” annotator is outperformed by an ensemble of 5 Turk workers (with 88% accuracy); thus, the effective upper bound on our dataset is likely even higher.",4.4 Results,[0],[0]
"5.1 Swag versus existing NLI datasets The past few years have yielded great advances in NLI and representation learning, due to the availability of large datasets like SNLI and MultiNLI
(Bowman et al., 2015; Williams et al., 2018).",5 Analysis,[0],[0]
"With the release of Swag, we hope to continue this trend, particularly as our dataset largely has the same input/output format as other NLI datasets.",5 Analysis,[0],[0]
"We observe three key differences between our dataset and others in this space:
First, as noted in Section 1, Swag requires a unique type of temporal reasoning.",5 Analysis,[0],[0]
"A state-of-theart NLI model such as ESIM, when bottlenecked through the SNLI notion of entailment (SNLIESIM), only obtains 36.1% accuracy.10 This implies that these datasets necessitate different (and complementary) forms of reasoning.
",5 Analysis,[0],[0]
"Second, our use of videos results in wide coverage of dynamic and temporal situations Compared with SNLI, with contexts from Flickr30K (Plummer et al., 2017) image captions, Swag has more active verbs like ‘pull’ and ‘hit,’ and fewer static verbs like ‘sit’ and ‘wear’ (Figure 4).11
Third, our dataset suffers from few lexical biases.",5 Analysis,[0],[0]
"Whereas fastText, a bag of n-gram model, obtains 67.0% accuracy on SNLI versus a 34.3% baseline (Gururangan et al., 2018), fastText obtains only 29.0% accuracy on Swag.12",5 Analysis,[0],[0]
"We sought to quantify how human judgments differ from the best studied model, ESIM+ELMo.",5.2 Error analysis,[0],[0]
"We randomly sampled 100 validation questions
10The weights of SNLI-ESIM pick up primarily on entailment probability (0.59), as with neutral (0.46), while contradiction is negatively correlated (-.42).
",5.2 Error analysis,[0],[0]
"11Video data has other language differences; notably, character names in LSMDC were replaced by ‘someone’
12The most predictive individual words on SWAG are infrequent in number: ‘dotted‘ with P(+|dotted) = 77% with 10.3 counts, and P(−|similar)",5.2 Error analysis,[0],[0]
= 81% with 16.3 counts.,5.2 Error analysis,[0],[0]
"(Counts from negative endings were discounted 3x, as there are 3 times as many negative endings as positive endings).
",5.2 Error analysis,[0],[0]
"that ESIM+ELMo answered incorrectly, for each extracting both the gold ending and the model’s preferred ending.",5.2 Error analysis,[0],[0]
"We asked 5 Amazon Mechanical Turk workers to pick the better ending (of which they preferred the gold endings 94% of the time) and to select one (or more) multiple choice reasons explaining why the chosen answer was better.
",5.2 Error analysis,[0],[0]
"The options, and the frequencies, are outlined in Table 4.",5.2 Error analysis,[0],[0]
"The most common reason for the turkers preferring the correct answer is situational (52.3% of the time), followed by weirdness (17.5%) and plausibility (14.4%).",5.2 Error analysis,[0],[0]
"This suggests that ESIM+ELMo already does a good job at filtering out weird and implausible answers, with the main bottleneck being grounded physical understanding.",5.2 Error analysis,[0],[0]
"The ambiguous percentage is also relatively low (12.0%), implying significant headroom.",5.2 Error analysis,[0],[0]
"Last, we show several qualitative examples in Table 5.",5.3 Qualitative examples,[0],[0]
"Though models can do decently well by identifying complex alignment patterns between the two sentences (e.g. being “up a tree” implies that “tree” is the end phrase), the incorrect model predictions suggest this strategy is insuffi-
cient.",5.3 Qualitative examples,[0],[0]
"For instance, answering “An old man rides a small bumper car” requires knowledge about bumper cars and how they differ from regular cars: bumper cars are tiny, don’t drive on roads, and don’t work in parking lots, eliminating the alternatives.",5.3 Qualitative examples,[0],[0]
"However, this knowledge is difficult to extract from existing corpora: for instance, the ConceptNet entry for Bumper Car has only a single relation: bumper cars are a type of vehicle.",5.3 Qualitative examples,[0],[0]
"Other questions require intuitive physical reasoning: e.g, for “he pours the raw egg batter into the pan,” about what happens next in making an omelet.",5.3 Qualitative examples,[0],[0]
Our results suggest that Swag is a challenging testbed for NLI models.,5.4 Where to go next?,[0],[0]
"However, the adversarial models used to filter the dataset are purely stylistic and focus on the second sentence; thus, subtle artifacts still likely remain in our dataset.",5.4 Where to go next?,[0],[0]
"These patterns are ostensibly picked up by the NLI models (particularly when using ELMo features), but the large gap between machine and human performance suggests that more is required to solve the dataset.",5.4 Where to go next?,[0],[0]
"As models are developed for commonsense inference, and more broadly as the field of NLP advances, we note that AF can be used again to create a more adversarial version of Swag using better language models and AF models.",5.4 Where to go next?,[0],[0]
"Entailment NLI There has been a long history of NLI benchmarks focusing on linguistic entailment (Cooper et al., 1996; Dagan et al., 2006; Marelli et al., 2014; Bowman et al., 2015; Lai et al., 2017; Williams et al., 2018).",6 Related Work,[0],[0]
"Recent NLI datasets in particular have supported learning broadly-applicable sentence representations (Conneau et al., 2017); moreover, models trained on these datasets were used as components
for performing better video captioning (Pasunuru and Bansal, 2017), summarization (Pasunuru and Bansal, 2018), and generation (Holtzman et al., 2018), confirming the importance of NLI research.",6 Related Work,[0],[0]
"The NLI task requires a variety of commonsense knowledge (LoBue and Yates, 2011), which our work complements.",6 Related Work,[0],[0]
"However, previous datasets for NLI have been challenged by unwanted annotation artifacts, (Gururangan et al., 2018; Poliak et al., 2018) or scale issues.",6 Related Work,[0],[0]
"Our work addresses these challenges by constructing a new NLI benchmark focused on grounded commonsense reasoning, and by introducing an adversarial filtering mechanism that substantially reduces known and easily detectable annotation artifacts.
",6 Related Work,[0],[0]
"Commonsense NLI Several datasets have been introduced to study NLI beyond linguistic entailment: for inferring likely causes and endings given a sentence (COPA; Roemmele et al., 2011), for choosing the most sensible ending to a short story (RocStories; Mostafazadeh et al., 2016; Sharma et al., 2018), and for predicting likelihood of a hypothesis by regressing to an ordinal label (JOCI; (Zhang et al., 2017)).",6 Related Work,[0],[0]
These datasets are relatively small: 1k examples for COPA and 10k cloze examples for RocStories.13 JOCI increases the scale by generating the hypotheses using a knowledge graph or a neural model.,6 Related Work,[0],[0]
"In contrast to JOCI where the task was formulated as a regression task on the degree of plausibility of the hypothesis, we frame commonsense inference as a multiple choice question to reduce the potential ambiguity in the labels and to allow for direct comparison between machines and humans.",6 Related Work,[0],[0]
"In addition, Swag’s use of adversarial filtering increases diversity of situations and counterfactual generation quality.
",6 Related Work,[0],[0]
"13For RocStories, this was by design to encourage learning from the larger corpus of 98k sensible stories.
",6 Related Work,[0],[0]
"Last, another related task formulation is sentence completion or cloze, where the task is to predict a single word that is removed from a given context (Zweig and Burges, 2011; Paperno et al., 2016).14 Our work in contrast requires longer textual descriptions to reason about.
",6 Related Work,[0],[0]
Vision datasets Several resources have been introduced to study temporal inference in vision.,6 Related Work,[0],[0]
"The Visual Madlibs dataset has 20k image captions about hypothetical next/previous events (Yu et al., 2015); similar to our work, the test portion is multiple-choice, with counterfactual answers retrieved from similar images and verified by humans.",6 Related Work,[0],[0]
"The question of ‘what will happen next?’ has also been studied in photo albums (Huang et al., 2016), videos of team sports, (Felsen et al., 2017) and egocentric dog videos (Ehsani et al., 2018).",6 Related Work,[0],[0]
"Last, annotation artifacts are also a recurring problem for vision datasets such as Visual Genome (Zellers et al., 2018) and Visual QA (Jabri et al., 2016); recent work was done to create a more challenging VQA dataset by annotating complementary image pairs (Goyal et al., 2016).
",6 Related Work,[0],[0]
"Reducing gender/racial bias Prior work has sought to reduce demographic biases in word embeddings (Zhang et al., 2018) as well as in image recognition models (Zhao et al., 2017).",6 Related Work,[0],[0]
"Our work has focused on producing a dataset with minimal annotation artifacts, which in turn helps to avoid some gender and racial biases that stem from elicitation (Rudinger et al., 2017).",6 Related Work,[0],[0]
"However, it is not perfect in this regard, particularly due to biases in movies (Schofield and Mehr, 2016; Sap et al., 2017).",6 Related Work,[0],[0]
"Our methodology could potentially be extended to construct datasets free of (possibly intersectional) gender or racial bias.
",6 Related Work,[0],[0]
"Physical knowledge Prior work has studied learning grounded knowledge about objects and verbs: from knowledge bases (Li et al., 2016), syntax parses (Forbes and Choi, 2017), word embeddings (Lucy and Gauthier, 2017), and images and dictionary definitions (Zellers and Choi, 2017).",6 Related Work,[0],[0]
"An alternate thread of work has been to learn scripts: high-level representations of event chains (Schank and Abelson, 1975; Chambers and Jurafsky, 2009).",6 Related Work,[0],[0]
"Swag evaluates both of these strands.
",6 Related Work,[0],[0]
14Prior work on sentence completion filtered negatives with heuristics based on LM perplexities.,6 Related Work,[0],[0]
"We initially tried something similar, but found the result to still be gameable.",6 Related Work,[0],[0]
We propose a new challenge of physically situated commonsense inference that broadens the scope of natural language inference (NLI) with commonsense reasoning.,7 Conclusion,[0],[0]
"To support research toward commonsense NLI, we create a large-scale dataset Swag with 113k multiple-choice questions.",7 Conclusion,[0],[0]
"Our dataset is constructed using Adversarial Filtering (AF), a new paradigm for robust and cost-effective dataset construction that allows datasets to be constructed at scale while automatically reducing annotation artifacts that can be easily detected by a committee of strong baseline models.",7 Conclusion,[0],[0]
"Our adversarial filtering paradigm is general, allowing potential applications to other datasets that require human composition of question answer pairs.",7 Conclusion,[0],[0]
"We thank the anonymous reviewers, members of the ARK and xlab at the University of Washington, researchers at the Allen Institute for AI, and Luke Zettlemoyer for their helpful feedback.",Acknowledgements,[0],[0]
We also thank the Mechanical Turk workers for doing a fantastic job with the human validation.,Acknowledgements,[0],[0]
"This work was supported by the National Science Foundation Graduate Research Fellowship (DGE-1256082), the NSF grant (IIS1524371, 1703166), the DARPA CwC program through ARO (W911NF-15-1-0543), the IARPA DIVA program through D17PC00343, and gifts by Google and Facebook.",Acknowledgements,[0],[0]
"The views and conclusions contained herein are those of the authors and should not be interpreted as representing endorsements of IARPA, DOI/IBC, or the U.S. Government.",Acknowledgements,[0],[0]
"Given a partial description like “she opened the hood of the car,” humans can reason about the situation and anticipate what might come next (“then, she examined the engine”).",abstractText,[0],[0]
"In this paper, we introduce the task of grounded commonsense inference, unifying natural language inference and commonsense reasoning.",abstractText,[0],[0]
"We present Swag, a new dataset with 113k multiple choice questions about a rich spectrum of grounded situations.",abstractText,[0],[0]
"To address the recurring challenges of the annotation artifacts and human biases found in many existing datasets, we propose Adversarial Filtering (AF), a novel procedure that constructs a de-biased dataset by iteratively training an ensemble of stylistic classifiers, and using them to filter the data.",abstractText,[0],[0]
"To account for the aggressive adversarial filtering, we use state-of-theart language models to massively oversample a diverse set of potential counterfactuals.",abstractText,[0],[0]
"Empirical results demonstrate that while humans can solve the resulting inference problems with high accuracy (88%), various competitive models struggle on our task.",abstractText,[0],[0]
We provide comprehensive analysis that indicates significant opportunities for future research.,abstractText,[0],[0]
Swag: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 856–861 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
856
In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",text,[0],[0]
Data augmentation algorithms generate extra data points from the empirically observed training set to train subsequent machine learning algorithms.,1 Introduction and Related Work,[0],[0]
"While these extra data points may be of lower quality than those in the training set, their quantity and diversity have proven to benefit various learning algorithms (DeVries and Taylor, 2017; Amodei et al., 2016).",1 Introduction and Related Work,[0],[0]
"In image processing, simple augmentation techniques such as flipping, cropping, or increasing and decreasing the contrast of the image are both widely utilized and highly effective (Huang et al., 2016; Zagoruyko and Komodakis, 2016).
",1 Introduction and Related Work,[0],[0]
"However, it is nontrivial to find simple equivalences for NLP tasks like machine translation, because even slight modifications of sentences can result in significant changes in their semantics, or
*: Equal contributions.
require corresponding changes in the translations in order to keep the data consistent.",1 Introduction and Related Work,[0],[0]
"In fact, indiscriminate modifications of data in NMT can introduce noise that makes NMT systems brittle (Belinkov and Bisk, 2018).
",1 Introduction and Related Work,[0],[0]
"Due to such difficulties, the literature in data augmentation for NMT is relatively scarce.",1 Introduction and Related Work,[0],[0]
"To our knowledge, data augmentation techniques for NMT fall into two categories.",1 Introduction and Related Work,[0],[0]
"The first category is based on back-translation (Sennrich et al., 2016b; Poncelas et al., 2018), which utilizes monolingual data to augment a parallel training corpus.",1 Introduction and Related Work,[0],[0]
"While effective, back-translation is often vulnerable to errors in initial models, a common problem of self-training algorithms (Chapelle et al., 2009).",1 Introduction and Related Work,[0.952291091742252],"['In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).']"
The second category is based on word replacements.,1 Introduction and Related Work,[0],[0]
"For instance, Fadaee et al. (2017) propose to replace words in the target sentences with rare words in the target vocabulary according to a language model, and then modify the aligned source words accordingly.",1 Introduction and Related Work,[0],[0]
"While this method generates augmented data with relatively high quality, it requires several complicated preprocessing steps, and is only shown to be effective for low-resource datasets.",1 Introduction and Related Work,[0],[0]
"Other generic word replacement methods include word dropout (Sennrich et al., 2016a; Gal and Ghahramani, 2016), which uniformly set some word embeddings to 0 at random, and Reward Augmented Maximum Likelihood (RAML; Norouzi et al. (2016)), whose implementation essentially replaces some words in the target sentences with other words from the target vocabulary.
",1 Introduction and Related Work,[0],[0]
"In this paper, we derive an extremely simple and efficient data augmentation technique for NMT.",1 Introduction and Related Work,[0],[0]
"First, we formulate the design of a data augmentation algorithm as an optimization problem, where we seek the data augmentation policy that maximizes an objective that encourages two desired properties: smoothness and diversity.",1 Introduction and Related Work,[0],[0]
"This optimization problem has a tractable analytic solution,
which describes a generic framework of which both word dropout and RAML are instances.",1 Introduction and Related Work,[0],[0]
"Second, we interpret the aforementioned solution and propose a novel method: independently replacing words in both the source sentence and the target sentence by other words uniformly sampled from the source and the target vocabularies, respectively.",1 Introduction and Related Work,[0],[0]
"Experiments show that this method, which we name SwitchOut, consistently improves over strong baselines on datasets of different scales, including the large-scale WMT 15 English-German dataset, and two medium-scale datasets: IWSLT 2016 German-English and IWSLT 2015 EnglishVietnamese.",1 Introduction and Related Work,[0],[0]
"We use uppercase letters, such as X , Y , etc., to denote random variables and lowercase letters such as x, y, etc., to denote the corresponding actual values.",2.1 Notations,[0],[0]
"Additionally, since we will discuss a data augmentation algorithm, we will use a hat to denote augmented variables and their values, e.g. bX , bY , bx, by, etc.",2.1 Notations,[0],[0]
"We will also use boldfaced characters, such as p, q, etc., to denote probability distributions.",2.1 Notations,[0],[0]
We facilitate our discussion with a probabilistic framework that motivates data augmentation algorithms.,2.2 Data Augmentation,[0],[0]
"With X , Y being the sequences of words in the source and target languages (e.g. in machine translation), the canonical MLE framework maximizes the objective
JMLE(✓) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(X,Y )",2.2 Data Augmentation,[0],[0]
"[logp✓(y|x)] .
",2.2 Data Augmentation,[0],[0]
"Here bp(X,Y ) is the empirical distribution over all training data pairs (x, y) and p
✓ (y|x) is a parameterized distribution that we aim to learn, e.g. a neural network.",2.2 Data Augmentation,[0],[0]
"A potential weakness of MLE is the mismatch between bp(X,Y ) and the true data distribution p(X,Y ).",2.2 Data Augmentation,[0],[0]
"Specifically, bp(X,Y ) is usually a bootstrap distribution defined only on the observed training pairs, while p(X,Y ) has a much larger support, i.e. the entire space of valid pairs.",2.2 Data Augmentation,[0],[0]
"This issue can be dramatic when the empirical observations are insufficient to cover the data space.
",2.2 Data Augmentation,[0],[0]
"In practice, data augmentation is often used to remedy this support discrepancy by supplying additional training pairs.",2.2 Data Augmentation,[0],[0]
"Formally, let q( bX, bY ) be the augmented distribution defined on a larger support than the empirical distribution bp(X,Y ).",2.2 Data Augmentation,[0],[0]
"Then,
MLE training with data augmentation maximizes
JAUG(✓) = Ebx,by⇠q( bX,bY )",2.2 Data Augmentation,[0],[0]
"[logp✓(by|bx)] .
",2.2 Data Augmentation,[0],[0]
"In this work, we focus on a specific family of q, which depends on the empirical observations by
q( bX, bY ) =",2.2 Data Augmentation,[0],[0]
"E x,y⇠bp(x,y)
h q( bX, bY |x, y) i .
",2.2 Data Augmentation,[0],[0]
"This particular choice follows the intuition that an augmented pair (bx, by) that diverges too far from any observed data is more likely to be invalid and thus harmful for training.",2.2 Data Augmentation,[0],[0]
The reason will be more evident later.,2.2 Data Augmentation,[0],[0]
"Certainly, not all q are equally good, and the more similar q is to p, the more desirable q will be.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Unfortunately, we only have access to limited observations captured by bp.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Hence, in order to use q to bridge the gap between bp and p, it is necessary to utilize some assumptions about p. Here, we exploit two highly generic assumptions, namely:
• Diversity: p(X,Y ) has a wider support set, which includes samples that are more diverse than those in the empirical observation set.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"• Smoothness: p(X,Y ) is smooth, and similar (x, y) pairs will have similar probabilities.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"To formalize both assumptions, let s(bx, by;x, y) be a similarity function that measures how similar an augmented pair (bx, by) is to an observed data pair (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Then, an ideal augmentation policy q( bX, bY |x, y) should have two properties.",2.3 Diverse and Smooth Augmentation,[0],[0]
"First, based on the smoothness assumption, if an augmented pair (bx, by) is more similar to an empirical pair (x, y), it is more likely that (bx, by) is sampled under the true data distribution p(X,Y ), and thus q( bX, bY |x, y) should assign a significant amount of probability mass to (bx, by).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Second, to quantify the diversity assumption, we propose that the entropy H[q( bX, bY |x, y)] should be large, so that the support of q( bX, bY ) is larger than the support of bp and thus is closer to the support p(X,Y ).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Combining these assumptions implies that q( bX, bY |x, y) should maximize the objective
J(q;x, y) = Ebx,by⇠q( bX,bY |x,y) ⇥",2.3 Diverse and Smooth Augmentation,[0],[0]
"s(bx, by;x, y) ⇤
+ ⌧H(q( bX, bY |x, y)), (1)
where ⌧ controls the strength of the diversity objective.",2.3 Diverse and Smooth Augmentation,[0],[0]
"The first term in (1) instantiates the smoothness assumption, which encourages q to draw samples that are similar to (x, y).",2.3 Diverse and Smooth Augmentation,[0],[0]
"Meanwhile, the second term in (1) encourages more diverse samples from q. Together, the objective J(q;x, y) extends the information in the “pivotal” empirical sample (x, y) to a diverse set of similar cases.",2.3 Diverse and Smooth Augmentation,[0],[0]
"This echoes our particular parameterization of q in Section 2.2.
",2.3 Diverse and Smooth Augmentation,[0],[0]
"The objective J(q;x, y) in (1) is the canonical maximum entropy problem that one often encounters in deriving a max-ent model (Berger et al., 1996), which has the analytic solution:
q⇤(bx, by|x, y) = exp {s(bx, by;x, y)/⌧}P bx0,by0 exp {s(bx0, by0;x, y)/⌧}
(2) Note that (2) is a fairly generic solution which is agnostic to the choice of the similarity measure s. Obviously, not all similarity measures are equally good.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Next, we will show that some existing algorithms can be seen as specific instantiations under our framework.",2.3 Diverse and Smooth Augmentation,[0],[0]
"Moreover, this leads us to propose a novel and effective data augmentation algorithm.",2.3 Diverse and Smooth Augmentation,[0],[0]
Word Dropout.,2.4 Existing and New Algorithms,[0],[0]
"In the context of machine translation, Sennrich et al. (2016a) propose to randomly choose some words in the source and/or target sentence, and set their embeddings to 0 vectors.",2.4 Existing and New Algorithms,[0],[0]
"Intuitively, it regards every new data pair generated by this procedure as similar enough and then includes them in the augmented training set.",2.4 Existing and New Algorithms,[0],[0]
"Formally, word dropout can be seen as an instantiation of our framework with a particular similarity function s(x̂, ŷ;x, y) (see Appendix A.1).
RAML.",2.4 Existing and New Algorithms,[0],[0]
"From the perspective of reinforcement learning, Norouzi et al. (2016) propose to train the model distribution to match a target distribution proportional to an exponentiated reward.",2.4 Existing and New Algorithms,[0],[0]
"Despite the difference in motivation, it can be shown (c.f. Appendix A.2) that RAML can be viewed as an instantiation of our generic framework, where the similarity measure is s(bx, by;x, y) = r(by; y) if bx = x and 1 otherwise.",2.4 Existing and New Algorithms,[0],[0]
"Here, r is a task-specific reward function which measures the similarity between by and y. Intuitively, this means that RAML only exploits the smoothness property on the target side while keeping the source side intact.
SwitchOut.",2.4 Existing and New Algorithms,[0],[0]
"After reviewing the two existing augmentation schemes, there are two immediate
insights.",2.4 Existing and New Algorithms,[0],[0]
"Firstly, augmentation should not be restricted to only the source side or the target side.",2.4 Existing and New Algorithms,[0],[0]
"Secondly, being able to incorporate prior knowledge, such as the task-specific reward function r in RAML, can lead to a better similarity measure.
",2.4 Existing and New Algorithms,[0],[0]
"Motivated by these observations, we propose to perform augmentation in both source and target domains.",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we separately measure the similarity between the pair (bx, x) and the pair (by, y) and then sum them together, i.e.
s(bx, by;x, y)/⌧ ⇡ r x (bx, x)/⌧ x + r y (by, y)/⌧ y , (3)
where r x and r y are domain specific similarity functions and ⌧
x , ⌧ y are hyper-parameters that absorb the temperature parameter ⌧ .",2.4 Existing and New Algorithms,[0],[0]
"This allows us to factor q⇤(bx, by|x, y) into:
q⇤(bx, by|x, y) = exp {rx(bx, x)/⌧x}P bx0 exp {rx(bx0, x)/⌧x}
⇥",2.4 Existing and New Algorithms,[0],[0]
"exp {ry(by, y)/⌧y}P by0 exp {ry(by0, y)/⌧y}
(4)
",2.4 Existing and New Algorithms,[0],[0]
"In addition, notice that this factored formulation allows bx and by to be sampled independently.
",2.4 Existing and New Algorithms,[0],[0]
Sampling Procedure.,2.4 Existing and New Algorithms,[0],[0]
"To complete our method, we still need to define r
x and r y , and then design a practical sampling scheme from each factor in (4).",2.4 Existing and New Algorithms,[0],[0]
"Though non-trivial, both problems have been (partially) encountered in RAML (Norouzi et al., 2016; Ma et al., 2017).",2.4 Existing and New Algorithms,[0],[0]
"For simplicity, we follow previous work to use the negative Hamming distance for both r
x and r y .",2.4 Existing and New Algorithms,[0],[0]
"For a more parallelized implementation, we sample an augmented sentence bs from a true sentence s as follows:
1.",2.4 Existing and New Algorithms,[0],[0]
"Sample bn 2 {0, 1, ..., |s|} by p(bn) /",2.4 Existing and New Algorithms,[0],[0]
"e bn/⌧ .
",2.4 Existing and New Algorithms,[0],[0]
2.,2.4 Existing and New Algorithms,[0],[0]
"For each i 2 {1, 2, ..., |s|}, with probability bn/ |s|, we can replace s
i by a uniform bs",2.4 Existing and New Algorithms,[0],[0]
"i 6= s i .
",2.4 Existing and New Algorithms,[0],[0]
"This procedure guarantees that any two sentences bs1 and bs2 with the same Hamming distance to s have the same probability, but slightly changes the relative odds of sentences with different Hamming distances to s from the true distribution by negative Hamming distance, and thus is an approximation of the actual distribution.",2.4 Existing and New Algorithms,[0.9524810914412942],"['The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’ The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.']"
"However, this efficient sampling procedure is much easier to implement while achieving good performance.
",2.4 Existing and New Algorithms,[0],[0]
"Algorithm 1 illustrates this sampling procedure, which can be applied independently and in parallel for each batch of source sentences and target
sentences.",2.4 Existing and New Algorithms,[0],[0]
"Additionally, we open source our implementation in TensorFlow and in PyTorch (respectively in Appendix A.5 and A.6).
",2.4 Existing and New Algorithms,[0],[0]
Algorithm 1: Sampling with SwitchOut.,2.4 Existing and New Algorithms,[0],[0]
"Input : s: a sentence represented by vocab integral ids,
⌧ : the temperature, V : the vocabulary Output : bs: a sentence with words replaced
1 Function HammingDistanceSample(s, ⌧ , |V |): 2 Let Z(⌧) P|s| n=0 e
n/⌧ be the partition function.",2.4 Existing and New Algorithms,[0],[0]
"3 Let p(n) e n/⌧/Z(⌧) for n = 0, 1, ..., |s|.",2.4 Existing and New Algorithms,[0],[0]
4 Sample bn ⇠ p(n).,2.4 Existing and New Algorithms,[0],[0]
5,2.4 Existing and New Algorithms,[0],[0]
"In parallel, do: 6 Sample a
i ⇠ Bernoulli(bn/ |s|).",2.4 Existing and New Algorithms,[0],[0]
7,2.4 Existing and New Algorithms,[0],[0]
"if a
i = 1 then 8 bs
i Uniform(V \{s i })",2.4 Existing and New Algorithms,[0],[0]
.,2.4 Existing and New Algorithms,[0],[0]
"9 else
10 bs",2.4 Existing and New Algorithms,[0],[0]
i s i .,2.4 Existing and New Algorithms,[0],[0]
11 end 12 return bs,2.4 Existing and New Algorithms,[0],[0]
Datasets.,3 Experiments,[0],[0]
We benchmark SwitchOut on three translation tasks of different scales: 1) IWSLT 2015 English-Vietnamese (en-vi); 2) IWSLT 2016 German-English (de-en); and 3) WMT 2015 English-German (en-de).,3 Experiments,[0],[0]
All translations are wordbased.,3 Experiments,[0],[0]
"These tasks and pre-processing steps are standard, used in several previous works.",3 Experiments,[0],[0]
"Detailed statistics and pre-processing schemes are in Appendix A.3.
",3 Experiments,[0],[0]
Models and Experimental Procedures.,3 Experiments,[0],[0]
"Our translation model, i.e. p
✓ (y|x), is a Transformer network (Vaswani et al., 2017).",3 Experiments,[0],[0]
"For each dataset, we first train a standard Transformer model without SwitchOut and tune the hyper-parameters on the dev set to achieve competitive results.",3 Experiments,[0],[0]
(w.r.t.,3 Experiments,[0],[0]
Luong and Manning (2015); Gu et al. (2018); Vaswani et al. (2017)),3 Experiments,[0],[0]
.,3 Experiments,[0],[0]
"Then, fixing all hyper-parameters, and fixing ⌧
y = 0, we tune the ⌧ x rate, which controls how far we are willing to let bx deviate from x.",3 Experiments,[0],[0]
"Our hyper-parameters are listed in Appendix A.4.
Baselines.",3 Experiments,[0],[0]
"While the Transformer network without SwitchOut is already a strong baseline, we also compare SwitchOut against two other baselines that further use existing varieties of data augmentation: 1) word dropout on the source side with the dropping probability of word = 0.1; and 2) RAML on the target side, as in Section 2.4.",3 Experiments,[0],[0]
"Additionally, on the en-de task, we compare SwitchOut against back-translation (Sennrich et al., 2016b).
",3 Experiments,[0],[0]
SwitchOut vs. Word Dropout and RAML.,3 Experiments,[0],[0]
"We report the BLEU scores of SwitchOut, word dropout, and RAML on the test sets of the tasks in Table 1.",3 Experiments,[0],[0]
"To account for variance, we run each experiment multiple times and report the median BLEU.",3 Experiments,[0],[0]
"Specifically, each experiment without SwitchOut is run for 4 times, while each experiment with SwitchOut is run for 9 times due to its inherently higher variance.",3 Experiments,[0],[0]
"We also conduct pairwise statistical significance tests using paired bootstrap (Clark et al., 2011), and record the results in Table 1.",3 Experiments,[0],[0]
"For 4 of the 6 settings, SwitchOut delivers significant improvements over the best baseline without SwitchOut.",3 Experiments,[0],[0]
"For the remaining two settings, the differences are not statistically significant.",3 Experiments,[0],[0]
The gains in BLEU with SwitchOut over the best baseline on WMT 15 en-de are all significant (p < 0.0002).,3 Experiments,[0],[0]
"Notably, SwitchOut on the source demonstrates as large gains as these obtained by RAML on the target side, and SwitchOut delivers further improvements when combined with RAML.
SwitchOut vs. Back Translation.",3 Experiments,[0],[0]
"Traditionally, data-augmentation is viewed as a method to enlarge the training datasets (Krizhevsky et al., 2012; Szegedy et al., 2014).",3 Experiments,[0],[0]
"In the context of neural MT, Sennrich et al. (2016b) propose to use artificial data generated from a weak back-translation model, effectively utilizing monolingual data to enlarge the bilingual training datasets.",3 Experiments,[0],[0]
"In connection, we compare SwitchOut against back translation.",3 Experiments,[0],[0]
"We only compare SwitchOut against back translation on the en-de task, where the amount of bilingual training data is already sufficiently large2.",3 Experiments,[0],[0]
"The
2We add the extra monolingual data from http://data.statmt.org/rsennrich/wmt16_ backtranslations/en-de/
BLEU scores with back-translation are reported in Table 2.",3 Experiments,[0],[0]
These results provide two insights.,3 Experiments,[0],[0]
"First, the gain delivered by back translation is less significant than the gain delivered by SwitchOut.",3 Experiments,[0],[0]
"Second, SwitchOut and back translation are not mutually exclusive, as one can additionally apply SwitchOut on the additional data obtained from back translation to further improve BLEU scores.
",3 Experiments,[0],[0]
Effects of ⌧ x and ⌧ y .,3 Experiments,[0],[0]
We empirically study the effect of these temperature parameters.,3 Experiments,[0],[0]
"During the tuning process, we translate the dev set of the tasks and report the BLEU scores in Figure 1.",3 Experiments,[0],[0]
"We observe that when fixing ⌧
y , the best performance is always achieved with a non-zero ⌧
.
",3 Experiments,[0],[0]
Where does SwitchOut Help the Most?,3 Experiments,[0],[0]
"Intuitively, because SwitchOut is expanding the support of the training distribution, we would expect that it would help the most on test sentences that are far from those in the training set and would thus benefit most from this expanded support.",3 Experiments,[0.9604242055620856],"['This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.']"
"To test this hypothesis, for each test sentence we find its most similar training sample (i.e. nearest neighbor), then bucket the instances by the distance to their
nearest neighbor and measure the gain in BLEU afforded by SwitchOut for each bucket.",3 Experiments,[0],[0]
"Specifically, we use (negative) word error rate (WER) as the similarity measure, and plot the bucket-by-bucket performance gain for each group in Figure 2.",3 Experiments,[0],[0]
"As we can see, SwitchOut improves increasingly more as the WER increases, indicating that SwitchOut is indeed helping on examples that are far from the sentences that the model sees during training.",3 Experiments,[0],[0]
This is the desirable effect of data augmentation techniques.,3 Experiments,[0],[0]
"In this paper, we propose a method to design data augmentation algorithms by solving an optimization problem.",4 Conclusion,[0],[0]
"These solutions subsume a few existing augmentation schemes and inspire a novel augmentation method, SwitchOut.",4 Conclusion,[0],[0]
SwitchOut delivers improvements over translation tasks at different scales.,4 Conclusion,[0],[0]
"Additionally, SwitchOut is efficient and easy to implement, and thus has the potential for wide application.",4 Conclusion,[0],[0]
"We thank Quoc Le, Minh-Thang Luong, Qizhe Xie, and the anonymous EMNLP reviewers, for their suggestions to improve the paper.
",Acknowledgements,[0],[0]
This material is based upon work supported in part by the Defense Advanced Research Projects Agency Information Innovation Office (I2O) Low Resource Languages for Emergent Incidents (LORELEI) program under Contract No. HR0011-15-C0114.,Acknowledgements,[0],[0]
"The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government.",Acknowledgements,[0],[0]
The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.,Acknowledgements,[0],[0]
"In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT).",abstractText,[0],[0]
"We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution.",abstractText,[0],[0]
"This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies.",abstractText,[0],[0]
We name this method SwitchOut.,abstractText,[0],[0]
"Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a).",abstractText,[0],[0]
Code to implement this method is included in the appendix.,abstractText,[0],[0]
SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3772–3782 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
3772",text,[0],[0]
"As algorithms for the semantic analysis of natural language sentences have developed, the role of syntax has been repeatedly revisited.",1 Introduction,[0],[0]
"Linguistic theories have argued for a very tight integration of syntactic and semantic processing (Steedman, 2000; Copestake and Flickinger, 2000), and many systems have used syntactic dependency or phrase-based parsers as preprocessing for semantic analysis (Gildea and Palmer, 2002; Punyakanok et al., 2008; Das et al., 2014).",1 Introduction,[0],[0]
"Meanwhile, some recent methods forgo explicit syntactic processing altogether (Zhou and Xu, 2015; He et al., 2017; Lee et al., 2017; Peng et al., 2017).
",1 Introduction,[0],[0]
"Because annotated training datasets for semantics will always be limited, we expect that syntax—which offers an incomplete but potentially useful view of semantic structure—will continue to offer useful inductive bias, encouraging semantic models toward better generalization.",1 Introduction,[0],[0]
"We address the central question: is there a way for semantic analyzers to benefit from syntax without the computational cost of syntactic parsing?
",1 Introduction,[0],[0]
"We propose a multitask learning approach to incorporating syntactic information into learned
representations of neural semantics models (§2).",1 Introduction,[0],[0]
"Our approach, the syntactic scaffold, minimizes an auxiliary supervised loss function, derived from a syntactic treebank.",1 Introduction,[0],[0]
"The goal is to steer the distributed, contextualized representations of words and spans toward accurate semantic and syntactic labeling.",1 Introduction,[0],[0]
"We avoid the cost of training or executing a full syntactic parser, and at test time (i.e., runtime in applications)",1 Introduction,[0],[0]
the semantic analyzer has no additional cost over a syntax-free baseline.,1 Introduction,[0],[0]
"Further, the method does not assume that the syntactic treebank overlaps the dataset for the primary task.
",1 Introduction,[0],[0]
"Many semantic tasks involve labeling spans, including semantic role labeling (SRL; Gildea and Jurafsky, 2002) and coreference resolution (Ng, 2010) (tasks we consider in this paper), as well as named entity recognition and some reading comprehension and question answering tasks (Rajpurkar et al., 2016).",1 Introduction,[0],[0]
These spans are usually syntactic constituents (cf.,1 Introduction,[0],[0]
"PropBank; Palmer et al., 2005), making phrase-based syntax a natural choice for a scaffold.",1 Introduction,[0],[0]
See Figure 1 for an example sentence with syntactic and semantic annotations.,1 Introduction,[0],[0]
"Since the scaffold task is not an end in itself, we relax the syntactic parsing problem to a collection of independent span-level predictions, with no constraint that they form a valid parse tree.",1 Introduction,[0],[0]
"This means we never need to run a syntactic parsing algorithm.
",1 Introduction,[0],[0]
Our experiments demonstrate that the syntactic scaffold offers a substantial boost to state-of-theart baselines for two SRL tasks (§5) and coreference resolution (§6).,1 Introduction,[0],[0]
"Our models use the strongest available neural network architectures for these tasks, integrating deep representation learning (He et al., 2017) and structured prediction at the level of spans (Kong et al., 2016).",1 Introduction,[0],[0]
"For SRL, the base-
line itself is a novel globally normalized structured conditional random field, which outperforms the previous state of the art.1 Syntactic scaffolds result in further improvements over prior work— 3.6 absolute F1 in FrameNet SRL, 1.1 absolute F1 in PropBank SRL, and 0.6 F1 in coreference resolution (averaged across three standard scores).",1 Introduction,[0],[0]
Our code is open source and available at https: //github.com/swabhs/scaffolding.,1 Introduction,[0],[0]
"Multitask learning (Caruana, 1997) is a collection of techniques in which two or more tasks are learned from data with at least some parameters shared.",2 Syntactic Scaffolds,[0],[0]
"We assume there is only one task about whose performance we are concerned, denoted T1 (in this paper, T1 is either SRL or coreference resolution).",2 Syntactic Scaffolds,[0],[0]
"We use the term “scaffold” to refer to a second task, T2, that can be combined with T1 during multitask learning.",2 Syntactic Scaffolds,[0],[0]
"A scaffold task is only used during training; it holds no intrinsic interest beyond biasing the learning of T1, and after learning is completed, the scaffold is discarded.
",2 Syntactic Scaffolds,[0],[0]
"A syntactic scaffold is a task designed to steer the (shared) model toward awareness of syntactic
1This excludes models initialized with deep, contextualized embeddings (Peters et al., 2018), an approach orthogonal to ours.
structure.",2 Syntactic Scaffolds,[0],[0]
It could be defined through a syntactic parser that shares some parameters with T1’s model.,2 Syntactic Scaffolds,[0],[0]
"Since syntactic parsing is costly, we use simpler syntactic prediction problems (discussed below) that do not produce whole trees.
",2 Syntactic Scaffolds,[0],[0]
"As with multitask learning in general, we do not assume that the same data are annotated with outputs for T1 and T2.",2 Syntactic Scaffolds,[0],[0]
"In this work, T2 is defined using phrase-structure syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).",2 Syntactic Scaffolds,[0],[0]
We experiment with three settings: one where the corpus for T2 does not overlap with the training datasets for T1 (frame-SRL) and two where there is a complete overlap (PropBank SRL and coreference).,2 Syntactic Scaffolds,[0],[0]
"Compared to approaches which require multiple output labels over the same data, we offer the major advantage of not requiring any assumptions about, or specification of, the relationship between T1 and T2 output.",2 Syntactic Scaffolds,[0],[0]
"We briefly contrast the syntactic scaffold with existing alternatives.
Pipelines.",3 Related Work,[0],[0]
"In a typical pipeline, T1 and T2 are separately trained, with the output of T2 used to define the inputs to T1 (Wolpert, 1992).",3 Related Work,[0],[0]
"Using syntax as T2 in a pipeline is perhaps the most
common approach for semantic structure prediction (Toutanova et al., 2008; Yang and Mitchell, 2017; Wiseman et al., 2016).2 However, pipelines introduce the problem of cascading errors (T2’s mistakes affect the performance, and perhaps the training, of T1; He et al., 2013).",3 Related Work,[0],[0]
"To date, remedies to cascading errors are so computationally expensive as to be impractical (e.g., Finkel et al., 2006).",3 Related Work,[0],[0]
"A syntactic scaffold is quite different from a pipeline since the output of T2 is never explicitly used.
",3 Related Work,[0],[0]
Latent variables.,3 Related Work,[0],[0]
Another solution is to treat the output of T2 as a (perhaps structured) latent variable.,3 Related Work,[0],[0]
This approach obviates the need of supervision for T2 and requires marginalization (or some approximation to it) in order to reason about the outputs of T1.,3 Related Work,[0],[0]
Syntax as a latent variable for semantics was explored by Zettlemoyer and Collins (2005) and Naradowsky et al. (2012).,3 Related Work,[0],[0]
"Apart from avoiding marginalization, the syntactic scaffold offers a way to use auxiliary syntacticallyannotated data as direct supervision for T2, and it need not overlap the T1 training data.
",3 Related Work,[0],[0]
Joint learning of syntax and semantics.,3 Related Work,[0],[0]
"The motivation behind joint learning of syntactic and semantic representations is that any one task is helpful in predicting the other (Lluı́s and Màrquez, 2008; Lluı́s et al., 2013; Henderson et al., 2013; Swayamdipta et al., 2016).",3 Related Work,[0],[0]
"This typically requires joint prediction of the outputs of T1 and T2, which tends to be computationally expensive at both training and test time.
",3 Related Work,[0],[0]
Part of speech scaffolds.,3 Related Work,[0],[0]
"Similar to our work, there have been multitask models that use partof-speech tagging as T2, with transition-based dependency parsing (Zhang and Weiss, 2016) and CCG supertagging (Søgaard and Goldberg, 2016) as T1.",3 Related Work,[0],[0]
Both of the above approaches assumed parallel input data and used both tasks as supervision.,3 Related Work,[0],[0]
"Notably, we simplify our T2, throwing away the structured aspects of syntactic parsing, whereas part-of-speech tagging has very little structure to begin with.",3 Related Work,[0],[0]
"While their approach results in improved token-level representations learned via supervision from POS tags, these must still be composed to obtain span representations.",3 Related Work,[0],[0]
"In-
2",3 Related Work,[0],[0]
"There has been some recent work on SRL which completely forgoes syntactic processing (Zhou and Xu, 2015), however it has been shown that incorporating syntactic information still remains useful (He et al., 2017).
",3 Related Work,[0],[0]
"stead, our approach learns span-level representations from phrase-type supervision directly, for semantic tasks.",3 Related Work,[0],[0]
"Additionally, these methods explore architectural variations in RNN layers for including supervision, whereas we focus on incorporating supervision with minimal changes to the baseline architecture.",3 Related Work,[0],[0]
"To the best of our knowledge, such simplified syntactic scaffolds have not been tried before.
",3 Related Work,[0],[0]
Word embeddings.,3 Related Work,[0],[0]
"Our definition of a scaffold task almost includes stand-alone methods for estimating word embeddings (Mikolov et al., 2013; Pennington et al., 2014; Peters et al., 2018).",3 Related Work,[0],[0]
"After training word embeddings, the tasks implied by models like the skip-gram or ELMo’s language model become irrelevant to the downstream use of the embeddings.",3 Related Work,[0],[0]
"A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of T1 through a multitask objective.
",3 Related Work,[0],[0]
Multitask learning.,3 Related Work,[0],[0]
"Neural architectures have often yielded performance gains when trained for multiple tasks together (Collobert et al., 2011; Luong et al., 2015; Chen et al., 2017; Hashimoto et al., 2017).",3 Related Work,[0],[0]
"In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks (FitzGerald et al., 2015; Peng et al., 2017, 2018).",3 Related Work,[0],[0]
"Contemporaneously with this work, Hershcovich et al. (2018) proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics (Abend and Rappoport, 2013).",3 Related Work,[0],[0]
"Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.",3 Related Work,[0],[0]
"We assume two sources of supervision: a corpusD1 with instances x annotated for the primary task’s outputs y (semantic role labeling or coreference resolution), and a treebankD2 with sentences x, each with a phrase-structure tree z.",4 Syntactic Scaffold Model,[0],[0]
"Each task has an associated loss, and we seek to minimize the combination of task losses,∑
(x,y)∈D1 L1(x, y) + δ ∑ (x,z)∈D2 L2(x, z) (1)
with respect to parameters, which are partially shared, where δ is a tunable hyperparameter.",4.1 Loss,[0],[0]
"In
the rest of this section, we describe the scaffold task.",4.1 Loss,[0],[0]
"We define the primary tasks in Sections 5–6.
",4.1 Loss,[0],[0]
"Each input is a sequence of tokens, x = 〈x1, x2, . . .",4.1 Loss,[0],[0]
", xn〉, for some n. We refer to a span of contiguous tokens in the sentence as xi: j = 〈xi, xi+1, . . .",4.1 Loss,[0],[0]
", x",4.1 Loss,[0],[0]
"j〉, for any 1 6 i 6 j 6 n. In our experiments we consider only spans up to a maximum length D, resulting in O(nD) spans.
",4.1 Loss,[0],[0]
"Supervision comes from a phrase-syntactic tree z for the sentence, comprising a syntactic category zi: j ∈ C for every span xi: j in x (many spans are given a null label).",4.1 Loss,[0],[0]
"We experiment with different sets of labels C (§4.2).
",4.1 Loss,[0],[0]
"In our model, every span xi: j is represented by an embedding vector vi: j (see details in §5.3).",4.1 Loss,[0],[0]
"A distribution over the category assigned to zi: j is derived from vi: j:
p(zi: j = c | xi: j) = softmax c wc · vi: j (2)
where wc is a parameter vector associated with category c.",4.1 Loss,[0],[0]
"We sum the log loss terms for all the spans in a sentence to give its loss:
L2(x, z) =",4.1 Loss,[0],[0]
"− ∑
16i6 j6n j−i6D
log p(zi: j | xi: j).",4.1 Loss,[0],[0]
(3),4.1 Loss,[0],[0]
"Different kinds of syntactic labels can be used for learning syntactically-aware span representations: • Constituent identity: C = {0, 1}; is a span a
constituent, or not?",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal: c is the category of a span,
including a null for non-constituents.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Non-terminal and parent: c is the category
of a span, concatenated with the category of its immediate ancestor.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"null is used for nonconstituents, and for empty ancestors.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"• Common non-terminals: Since a majority
of semantic arguments and entity mentions are labeled with a small number of syntactic categories,3 we experiment with a threeway classification among (i) noun phrase (or prepositional phrase, for frame SRL); (ii) any other category; and (iii) null.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"In Figure 1, for the span “encouraging them”, the constituent identity scaffold label is 1, the nonterminal label is S|VP, the non-terminal and parent label is S|VP+par=PP, and the common nonterminals label is set to OTHER.
",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
"3In the OntoNotes corpus, which includes both syntactic and semantic annotations, 44% of semantic arguments are noun phrases and 13% are prepositional phrases.",4.2 Labels for the Syntactic Scaffold Task,[0],[0]
We contribute a new SRL model which contributes a strong baseline for experiments with syntactic scaffolds.,5 Semantic Role Labeling,[0],[0]
"The performance of this baseline itself is competitive with state-of-the-art methods (§7).
",5 Semantic Role Labeling,[0],[0]
FrameNet.,5 Semantic Role Labeling,[0],[0]
"In the FrameNet lexicon (Baker et al., 1998), a frame represents a type of event, situation, or relationship, and is associated with a set of semantic roles, called frame elements.",5 Semantic Role Labeling,[0],[0]
"A frame can be evoked by a word or phrase in a sentence, called a target.",5 Semantic Role Labeling,[0],[0]
"Each frame element of an evoked frame can then be realized in the sentence as a sentential span, called an argument (or it can be unrealized).",5 Semantic Role Labeling,[0],[0]
"Arguments for a given frame do not overlap.
",5 Semantic Role Labeling,[0],[0]
PropBank.,5 Semantic Role Labeling,[0],[0]
PropBank similarly disambiguates predicates and identifies argument spans.,5 Semantic Role Labeling,[0],[0]
"Targets are disambiguated to lexically specific senses rather than shared frames, and a set of generic roles is used for all targets, reducing the argument label space by a factor of 17.",5 Semantic Role Labeling,[0],[0]
"Most importantly, the arguments were annotated on top of syntactic constituents, directly coupling syntax and semantics.",5 Semantic Role Labeling,[0],[0]
"A detailed example for both formalisms is provided in Figure 1.
",5 Semantic Role Labeling,[0],[0]
"Semantic structure prediction is the task of identifying targets, labeling their frames or senses, and labeling all their argument spans in a sentence.",5 Semantic Role Labeling,[0],[0]
"Here we assume gold targets and frames, and consider only the SRL task.
",5 Semantic Role Labeling,[0],[0]
"Formally, a single input instance for argument identification consists of: an n-word sentence x = 〈x1, x2, . . .",5 Semantic Role Labeling,[0],[0]
", xn〉, a single target span t = 〈tstart, tend〉, and its evoked frame, or sense, f .",5 Semantic Role Labeling,[0],[0]
"The argument labeling task is to produce a segmentation of the sentence: s = 〈s1, s2, . . .",5 Semantic Role Labeling,[0],[0]
", sm〉 for each input x.",5 Semantic Role Labeling,[0],[0]
A segment s = 〈,5 Semantic Role Labeling,[0],[0]
"i, j, yi: j〉 corresponds to a labeled span of the sentence, where the label yi: j ∈ Y f ∪ {null} is either a role that the span fills, or null if the span does not fill any role.",5 Semantic Role Labeling,[0],[0]
"In the case of PropBank, Y f consists of all possible roles.",5 Semantic Role Labeling,[0],[0]
The segmentation is constrained so that argument spans cover the sentence and do not overlap (ik+1 = 1 + jk for sk; i1 = 1; jm = n).,5 Semantic Role Labeling,[0],[0]
Segments of length 1 such that i = j are allowed.,5 Semantic Role Labeling,[0],[0]
A separate segmentation is predicted for each target annotation in a sentence.,5 Semantic Role Labeling,[0],[0]
"In order to model the non-overlapping arguments of a given target, we use a semi-Markov conditional random field (semi-CRF; Sarawagi et al., 2004).",5.1 Semi-Markov CRF,[0],[0]
"Semi-CRFs define a conditional distribution over labeled segmentations of an input sequence, and are globally normalized.",5.1 Semi-Markov CRF,[0],[0]
A single target’s arguments can be neatly encoded as a labeled segmentation by giving the spans in between arguments a reserved null label.,5.1 Semi-Markov CRF,[0],[0]
"Semi-Markov models are more powerful than BIO tagging schemes, which have been used successfully for PropBank SRL (Collobert et al., 2011; Zhou and Xu, 2015, inter alia), because the semi-Markov assumption allows scoring variable-length segments, rather than fixed-length label n-grams as under an (n − 1)-order Markov assumption.",5.1 Semi-Markov CRF,[0],[0]
Computing the marginal likelihood with a semi-CRF can be done using dynamic programming in O(n2) time (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"By filtering out segments longer than D tokens, this is reduced to O(nD).
",5.1 Semi-Markov CRF,[0],[0]
"Given an input x, a semi-CRF defines a conditional distribution p(s | x).",5.1 Semi-Markov CRF,[0],[0]
Every segment s = 〈,5.1 Semi-Markov CRF,[0],[0]
"i, j, yi: j〉 is given a real-valued score, ψ(〈i, j, yi: j = r〉, xi: j) = wr · vi: j, where vi: j is an embedding of the span (§5.3) and wr is a parameter vector corresponding to its label.",5.1 Semi-Markov CRF,[0],[0]
"The score of the entire segmentation s is the sum of the scores of its segments: Ψ(x, s) =",5.1 Semi-Markov CRF,[0],[0]
"∑m k=1 ψ(sk, xik: jk ).",5.1 Semi-Markov CRF,[0],[0]
These scores are exponentiated and normalized to define the probability distribution.,5.1 Semi-Markov CRF,[0],[0]
The sum-product variant of the semi-Markov dynamic programming algorithm is used to calculate the normalization term (required during learning).,5.1 Semi-Markov CRF,[0],[0]
"At test time, the maxproduct variant returns the most probable segmentation, ŝ = arg max sΨ(s, x).
",5.1 Semi-Markov CRF,[0],[0]
The parameters of the semi-CRF are learned to maximize a criterion related to the conditional loglikelihood of the gold-standard segments in the training corpus (§5.2).,5.1 Semi-Markov CRF,[0],[0]
"The learner evaluates and adjusts segment scores ψ(sk, x) for every span in the sentence, which in turn involves learning embedded representations for all spans (§5.3).",5.1 Semi-Markov CRF,[0],[0]
Typically CRF and semi-CRF models are trained to maximize a conditional log-likelihood objective.,5.2 Softmax-Margin Objective,[0],[0]
"In early experiments, we found that incorporating a structured cost was beneficial; we do so by using a softmax-margin training objective (Gimpel and Smith, 2010), a “cost-aware” variant
of log-likelihood:
L1 = − ∑
(x,s∗)∈D1 log
exp Ψ(s∗, x) Z(x, s∗) , (4)
Z(x, s∗) =",5.2 Softmax-Margin Objective,[0],[0]
"∑
s exp {Ψ(s, x) +",5.2 Softmax-Margin Objective,[0],[0]
"cost(s, s∗)}.",5.2 Softmax-Margin Objective,[0],[0]
"(5)
We design the cost function so that it factors by predicted span, in the same way Ψ does:
cost(s, s∗)",5.2 Softmax-Margin Objective,[0],[0]
"= ∑ s∈s cost(s, s∗) = ∑ s∈s I(s < s∗).",5.2 Softmax-Margin Objective,[0],[0]
"(6)
The softmax-margin criterion, like log-likelihood, is globally normalized over all of the exponentially many possible labeled segmentations.",5.2 Softmax-Margin Objective,[0],[0]
"The following zeroth-order semi-Markov dynamic program (Sarawagi et al., 2004) efficiently computes the new partition function:
α j = ∑
s=〈i, j,yi: j〉 j−i6D
αi−1 exp{Ψ(s, x) + cost(s, s∗)}, (7)
where Z = αn, under the base case α0 = 1.",5.2 Softmax-Margin Objective,[0],[0]
"The prediction under the model can be calculated using a similar dynamic program with the following recurrence where γ0 = 1:
γ j = max s=〈i, j,yi: j〉
j−i6D
γi−1 exp Ψ(s, x).",5.2 Softmax-Margin Objective,[0],[0]
"(8)
Our model formulation enforces that arguments do not overlap.",5.2 Softmax-Margin Objective,[0],[0]
"We do not enforce any other SRL constraints, such as non-repetition of core frame elements (Das et al., 2012).",5.2 Softmax-Margin Objective,[0],[0]
"This section describes the neural architecture used to obtain the span embedding, vi: j, corresponding to a span xi: j and the target in consideration, t = 〈tstart, tend〉.",5.3 Input Span Representation,[0],[0]
"For the scaffold task, since the syntactic treebank does not contain annotations for semantic targets, we use the last verb in the sentence as a placeholder target, wherever target features are used.",5.3 Input Span Representation,[0],[0]
"If there are no verbs, we use the first token in the sentence as a placeholder target.",5.3 Input Span Representation,[0],[0]
"The parameters used to learn v are shared between the tasks.
",5.3 Input Span Representation,[0],[0]
"We construct an embedding for the span using • hi and h j: contextualized embeddings for the
words at the span boundary (§5.3.1), • ui: j: a span summary that pools over the con-
tents of the span (§5.3.2), and
• ai: j: and a hand-engineered feature vector for the span (§5.3.3).
",5.3 Input Span Representation,[0],[0]
"This embedding is then passed to a feedforward layer to compute the span representation, vi: j.",5.3 Input Span Representation,[0],[0]
"To obtain contextualized embeddings of each token in the input sequence, we run a bidirectional LSTM (Graves, 2012) with ` layers over the full input sequence.",5.3.1 Contextualized Token Embeddings,[0],[0]
"To indicate which token is a predicate, a linearly transformed one-hot embedding v is used, following Zhou and Xu (2015) and He et al. (2017).",5.3.1 Contextualized Token Embeddings,[0],[0]
The input vector representing the token at position q in the sentence is the concatenation of a fixed pretrained embedding xq and vq.,5.3.1 Contextualized Token Embeddings,[0],[0]
"When given as input to the bidirectional LSTM, this yields a hidden state vector hq representing the qth token in the context of the sentence.",5.3.1 Contextualized Token Embeddings,[0],[0]
Tokens within a span might convey different amounts of information necessary to label the span as a semantic argument.,5.3.2 Span Summary,[0],[0]
"Following Lee et al. (2017), we use an attention mechanism (Bahdanau et al., 2014) to summarize each span.",5.3.2 Span Summary,[0],[0]
"Each contextualized token in the span is passed through a feed-forward network to obtain a weight, normalized to give σk = softmax
i6k6 j whead · hk, where whead
is a learned parameter.",5.3.2 Span Summary,[0],[0]
"The weights σ are then used to obtain a vector that summarizes the span, ui: j = ∑ i6k6 j; j−i<D σk · hk.",5.3.2 Span Summary,[0],[0]
"We use the following three features for each span: • width of the span in tokens (Das et al., 2014) • distance (in tokens) of the span from the tar-
get (Täckström et al., 2015) • position of the span with respect to the tar-
get (before, after, overlap) (Täckström et al., 2015)
",5.3.3 Span Features,[0],[0]
"Each of these features is encoded as a one-hotembedding and then linearly transformed to yield a feature vector, ai: j.",5.3.3 Span Features,[0],[0]
Coreference resolution is the task of determining clusters of mentions that refer to the same entity.,6 Coreference Resolution,[0],[0]
"Formally, the input is a document x = x1, x2, . . .",6 Coreference Resolution,[0],[0]
", xn consisting of n words.",6 Coreference Resolution,[0],[0]
"The goal is to predict a set of clusters c = {c1, c2, . . .",6 Coreference Resolution,[0],[0]
"}, where each cluster c = {s1, s2, .",6 Coreference Resolution,[0],[0]
. .,6 Coreference Resolution,[0],[0]
"} is a set of spans and
each span s = 〈i, j〉 is a pair of indices such that 1 6 i 6 j 6 n.
As a baseline, we use the model of Lee et al. (2017), which we describe briefly in this section.",6 Coreference Resolution,[0],[0]
This model decomposes the prediction of coreference clusters into a series of span classification decisions.,6 Coreference Resolution,[0],[0]
"Every span s predicts an antecedent ws ∈ Y(s) = {null, s1, s2, . . .",6 Coreference Resolution,[0],[0]
", sm}.",6 Coreference Resolution,[0],[0]
"Labels s1 to sm indicate a coreference link between s and one of the m spans that precede it, and null indicates that s does not link to anything, either because it is not a mention or it is in a singleton cluster.",6 Coreference Resolution,[0],[0]
"The predicted clustering of the spans can be recovered by aggregating the predicted links.
",6 Coreference Resolution,[0],[0]
"Analogous to the SRL model (§5), every span s is represented by an embedding vs, which is central to the model.",6 Coreference Resolution,[0],[0]
"For each span s and a potential antecedent a ∈ Y(s), pairwise coreference scores Ψ(vs, va, φ(s, a)) are computed via feedforward networks with the span embeddings as input.",6 Coreference Resolution,[0],[0]
"φ(s, a) are pairwise discrete features encoding the distance between span s and span a and metadata, such as the genre and speaker information.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for the details of the scoring function.
",6 Coreference Resolution,[0],[0]
"The scores from Ψ are normalized over the possible antecedents Y(s) of each span to induce a probability distribution for every span:
p(ws = a) = softmax a∈Y(s) Ψ(vs, va, φ(s, a))",6 Coreference Resolution,[0],[0]
"(9)
In learning, we minimize the negative loglikelihood marginalized over the possibly correct antecedents:
L1 = − ∑ s∈D log ∑ a∗∈G(s)∩Y(s) p(ws = a∗) (10)
whereD is the set of spans in the training dataset, and G(s) indicates the gold cluster of s if it belongs to one and {null} otherwise.
",6 Coreference Resolution,[0],[0]
"To operate under reasonable computational requirements, inference under this model requires a two-stage beam search, which reduces the number of span pairs considered.",6 Coreference Resolution,[0],[0]
"We refer the reader to Lee et al. (2017) for details.
",6 Coreference Resolution,[0],[0]
Input span representation.,6 Coreference Resolution,[0],[0]
"The input span embedding, vs for coreference resolution and its syntactic scaffold follow the definition used in §5.3, with the key difference of using no target features.",6 Coreference Resolution,[0],[0]
"Since there is a complete overlap of input sentences betweenDsc andDpr as the coreference annotations are also from OntoNotes (Pradhan et al.,
2012), we reuse the v for the scaffold task.",6 Coreference Resolution,[0],[0]
"Additionally, instead of the entire document, each sentence in it is independently given as input to the bidirectional LSTMs.",6 Coreference Resolution,[0],[0]
We evaluate our models on the test set of FrameNet 1.5 for frame SRL and on the test set of OntoNotes for both PropBank SRL and coreference.,7 Results,[0],[0]
"For the syntactic scaffold in each case, we use syntactic annotations from OntoNotes 5.0 (Weischedel et al., 2013; Pradhan et al., 2013).4 Further details on experimental settings and datasets have been elaborated in the supplemental material.
",7 Results,[0],[0]
Frame SRL.,7 Results,[0],[0]
Table 1 shows the performance of all the scaffold models on frame SRL with respect to prior work and a semi-CRF baseline (§5.1) without a syntactic scaffold.,7 Results,[0],[0]
"We follow the official evaluation from the SemEval shared task for frame-semantic parsing (Baker et al., 2007).
",7 Results,[0],[0]
"Prior work for frame SRL has relied on predicted syntactic trees, in two different ways: by using syntax-based rules to prune out spans of text that are unlikely to contain any frame’s argument; and by using syntactic features in their statistical model (Das et al., 2014; Täckström",7 Results,[0],[0]
"et al., 2015; FitzGerald et al., 2015; Kshirsagar et al., 2015).
",7 Results,[0],[0]
The best published results on FrameNet 1.5 are due to Yang and Mitchell (2017).,7 Results,[0],[0]
"In their sequential model (seq), they treat argument identification as a sequence-labeling problem using a deep bidirectional LSTM with a CRF layer.",7 Results,[0],[0]
"In their relational model (Rel), they treat the same problem as a span classification problem.",7 Results,[0],[0]
"Finally, they introduce an ensemble to integerate both models, and use an integer linear program for inference satisfying SRL constraints.",7 Results,[0],[0]
"Though their model does not do any syntactic pruning, it does use syntactic features for argument identification and labeling.5
Notably, all prior systems for frame SRL listed in Table 1 use a pipeline of syntax and semantics.",7 Results,[0],[0]
"Our semi-CRF baseline outperforms all prior work, without any syntax.",7 Results,[0],[0]
"This highlights the ben-
4http://cemantix.org/data/ontonotes.html 5Yang and Mitchell (2017) also evaluated on the full frame-semantic parsing task, which includes frame-SRL as well as identifying frames.",7 Results,[0],[0]
"Since our frame SRL performance improves over theirs, we expect that incorporation into a full system (e.g., using their frame identification module) would lead to overall benefits as well; this experiment is left to future work.
",7 Results,[0],[0]
"efits of modeling spans and of global normalization.
",7 Results,[0],[0]
"Turning to scaffolds, even the most coarsegrained constituent identity scaffold improves the performance of our syntax-agnostic baseline.",7 Results,[0],[0]
"The nonterminal and nonterminal and parent scaffolds, which use more detailed syntactic representations, improve over this.",7 Results,[0],[0]
"The greatest improvements come from the scaffold model predicting common nonterminal labels (NP and PP, which are the most common syntactic categories of semantic arguments, vs. others): 3.6% absolute improvement in F1 measure over prior work.
",7 Results,[0],[0]
"Contemporaneously with this work, Peng et al. (2018) proposed a system for joint frame-semantic and semantic dependency parsing.",7 Results,[0],[0]
"They report results for joint frame and argument identification, and hence cannot be directly compared in Table 1.",7 Results,[0],[0]
"We evaluated their output for argument identification only; our semi-CRF baseline model exceeds their performance by 1 F1, and our common nonterminal scaffold by 3.1 F1.6
6This result is not reported in Table 1 since Peng et al. (2018) used a preprocessing which renders the test set slightly larger — the difference we report is calculated using their test set.
",7 Results,[0],[0]
PropBank SRL.,7 Results,[0],[0]
"We use the OntoNotes data from the CoNLL shared task in 2012 (Pradhan et al., 2013) for Propbank SRL.",7 Results,[0],[0]
"Table 2 reports results using gold predicates.
",7 Results,[0],[0]
"Recent competitive systems for PropBank SRL follow the approach of Zhou and Xu (2015), employing deep architectures, and forgoing the use of any syntax.",7 Results,[0],[0]
"He et al. (2017) improve on those results, and in analysis experiments, show that constraints derived using syntax may further improve performance.",7 Results,[0],[0]
Tan et al. (2018) employ a similar approach but use feed-forward networks with selfattention.,7 Results,[0],[0]
"He et al. (2018a) use a span-based classification to jointly identify and label argument spans.
",7 Results,[0],[0]
"Our syntax-agnostic semi-CRF baseline model improves on prior work (excluding ELMo), showing again the value of global normalization in semantic structure prediction.",7 Results,[0],[0]
We obtain further improvement of 0.8 absolute F1 with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"This indicates that a syntactic inductive bias is beneficial even when using sophisticated neural architectures.
",7 Results,[0],[0]
"He et al. (2018a) also provide a setup where initialization was done with deep contextualized embeddings, ELMo (Peters et al., 2018), resulting in 85.5 F1 on the OntoNotes test set.",7 Results,[0],[0]
"The improvements from ELMo are methodologically orthogonal to syntactic scaffolds.
",7 Results,[0],[0]
"Since the datasets for learning PropBank semantics and syntactic scaffolds completely overlap, the performance improvement cannot be attributed to a larger training corpus (or, by extension, a larger vocabulary), though that might be a factor for frame SRL.
",7 Results,[0],[0]
"A syntactic scaffold can match the performance of a pipeline containing carefully extracted syntactic features for semantic prediction (Swayamdipta et al., 2017).",7 Results,[0],[0]
"This, along with other recent ap-
proaches (He et al., 2017, 2018b) show that syntax remains useful, even with strong neural models for SRL.
Coreference.",7 Results,[0],[0]
"We report the results on four standard scores from the CoNLL evaluation: MUC, B3 and CEAFφ4 , and their average F1 in Table 3.",7 Results,[0],[0]
"Prior competitive coreference resolution systems (Wiseman et al., 2016; Clark and Manning, 2016b,a) all incorporate synctactic information in a pipeline, using features and rules for mention proposals from predicted syntax.
",7 Results,[0],[0]
"Our baseline is the model from Lee et al. (2017), described in §6.",7 Results,[0],[0]
"Similar to the baseline model for frame SRL, and in contrast with prior work, this model does not use any syntax.
",7 Results,[0],[0]
We experiment with the best syntactic scaffold from the frame SRL task.,7 Results,[0],[0]
"We used NP, OTHER, and null as the labels for the common nonterminals scaffold here, since coreferring mentions are rarely prepositional phrases.",7 Results,[0],[0]
The syntactic scaffold outperforms the baseline by 0.6 absolute F1.,7 Results,[0],[0]
"Contemporaneously, Lee et al. (2018) proposed a model which takes in account higher order inference and more aggressive pruning, as well as initialization with ELMo embeddings, resulting in 73.0 average F1.",7 Results,[0],[0]
"All the above are orthogonal to our approach, and could be incorporated to yield higher gains.",7 Results,[0],[0]
"To investigate the performance of the syntactic scaffold, we focus on the frame SRL results, where we observed the greatest improvement with respect to a non-syntactic baseline.
",8 Discussion,[0],[0]
"We consider a breakdown of the performance by the syntactic phrase types of the arguments, provided in FrameNet7 in Figure 2.",8 Discussion,[0],[0]
"Not surpris-
7We used FrameNet syntactic phrase annotations for analysis only, and not in our models, since they are annotated only for the gold arguments.
",8 Discussion,[0],[0]
"ingly, we observe large improvements in the common nonterminals used (NP and PP).",8 Discussion,[0],[0]
"However, the phrase type annotations in FrameNet do not correspond exactly to the OntoNotes phrase categories.",8 Discussion,[0],[0]
"For instance, FrameNet annotates nonmaximal (A) and standard adjective phrases (AJP), while OntoNotes annotations for noun-phrases are flat, ignore the underlying adjective phrases.",8 Discussion,[0],[0]
"This explains why the syntax-agnostic baseline is able to recover the former while the scaffold is not.
",8 Discussion,[0],[0]
"Similarly, for frequent frame elements, scaffolding improves performance across the board, as shown in Fig. 3.",8 Discussion,[0],[0]
"The largest improvements come for Theme and Goal, which are predominantly realized as noun phrases and prepositional phrases.",8 Discussion,[0],[0]
"We introduced syntactic scaffolds, a multitask learning approach to incorporate syntactic bias into semantic processing tasks.",9 Conclusion,[0],[0]
"Unlike pipelines and approaches which jointly model syntax and semantics, no explicit syntactic processing is required at runtime.",9 Conclusion,[0],[0]
"Our method improves the performance of competitive baselines for semantic role labeling on both FrameNet and PropBank, and for coreference resolution.",9 Conclusion,[0],[0]
"While our focus was on span-based tasks, syntactic scaffolds could be applied in other settings (e.g., dependency and graph representations).",9 Conclusion,[0],[0]
"Moreover, scaffolds need not be syntactic; we can imagine, for example, semantic scaffolds being used to improve NLP applications with limited annotated data.",9 Conclusion,[0],[0]
"It remains an open empirical question to determine the relative merits of different kinds of scaffolds and multitask learners, and how they can be most produc-
tively combined.",9 Conclusion,[0],[0]
Our code is publicly available at https://github.com/swabhs/scaffolding.,9 Conclusion,[0],[0]
"We thank several members of UW-NLP, particularly Luheng He, as well as David Weiss and Emily Pitler for thoughtful discussions on prior versions of this paper.",Acknowledgments,[0],[0]
We also thank the three anonymous reviewers for their valuable feedback.,Acknowledgments,[0],[0]
This work was supported in part by NSF grant IIS1562364 and by the NVIDIA Corporation through the donation of a Tesla GPU.,Acknowledgments,[0],[0]
"We introduce the syntactic scaffold, an approach to incorporating syntactic information into semantic tasks.",abstractText,[0],[0]
"Syntactic scaffolds avoid expensive syntactic processing at runtime, only making use of a treebank during training, through a multitask objective.",abstractText,[0],[0]
"We improve over strong baselines on PropBank semantics, frame semantics, and coreference resolution, achieving competitive performance on all three tasks.",abstractText,[0],[0]
Syntactic Scaffolds for Semantic Structures,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 2061–2071 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
2061",text,[0],[0]
"Semantic role labeling (SRL), namely semantic parsing, is a shallow semantic parsing task, which aims to recognize the predicate-argument structure of each predicate in a sentence, such as who did what to whom, where and when, etc.",1 Introduction,[0],[0]
"Specifically, we seek to identify arguments and label their semantic roles given a predicate.",1 Introduction,[0],[0]
"SRL is an impor-
∗ These authors made equal contribution.† Corresponding author.",1 Introduction,[0],[0]
"This paper was partially supported by National Key Research and Development Program of China (No. 2017YFB0304100), National Natural Science Foundation of China (No. 61672343 and No. 61733011), Key Project of National Society Science Foundation of China (No. 15- ZDA041), The Art and Science Interdisciplinary Funds of Shanghai Jiao Tong University (No. 14JCRZ04).
tant method to obtain semantic information beneficial to a wide range of natural language processing (NLP) tasks, including machine translation (Shi et al., 2016), question answering (Berant et al., 2013; Yih et al., 2016) and discourse relation sense classification (Mihaylov and Frank, 2016).
",1 Introduction,[0],[0]
"There are two formulizations for semantic predicate-argument structures, one is based on constituents (i.e., phrase or span), the other is based on dependencies.",1 Introduction,[0],[0]
"The latter proposed by the CoNLL-2008 shared task (Surdeanu et al., 2008) is also called semantic dependency parsing, which annotates the heads of arguments rather than phrasal arguments.",1 Introduction,[0],[0]
"Generally, SRL is decomposed into multi-step classification subtasks in pipeline systems, consisting of predicate identification and disambiguation, argument identification and classification.
",1 Introduction,[0],[0]
"In prior work of SRL, considerable attention has been paid to feature engineering that struggles to capture sufficient discriminative information, while neural network models are capable of extracting features automatically.",1 Introduction,[0],[0]
"In particular, syntactic information, including syntactic tree feature, has been show extremely beneficial to SRL since a larger scale of empirical verification of Punyakanok et al. (2008).",1 Introduction,[0],[0]
"However, all the work had to take the risk of erroneous syntactic input, leading to an unsatisfactory performance.
",1 Introduction,[0],[0]
"To alleviate the above issues, Marcheggiani et al. (2017) propose a simple but effective model for dependency SRL without syntactic input.",1 Introduction,[0],[0]
"It seems that neural SRL does not have to rely on syntactic features, contradicting with the belief that syntax is a necessary prerequisite for SRL as early as Gildea and Palmer (2002).",1 Introduction,[0],[0]
"This dramatic contradiction motivates us to make a thorough exploration on syntactic contribution to SRL.
",1 Introduction,[0],[0]
"This paper will focus on semantic dependency parsing and formulate SRL as one or two se-
quence tagging tasks with predicate-specific encoding.",1 Introduction,[0],[0]
"With the help of the proposed k-order argument pruning algorithm over syntactic tree, our model obtains state-of-the-art scores on the CoNLL benchmarks for both English and Chinese.
",1 Introduction,[0],[0]
"In order to quantitatively evaluate the contribution of syntax to SRL, we adopt the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies introduced by CoNLL2008 Shared Task1 as evaluation metric.",1 Introduction,[0],[0]
"Considering that various syntactic parsers contribute different syntactic inputs with various range of quality levels, the ratio provides a fairer comparison between syntactically-driven SRL systems, which will be surveyed by our empirical study.",1 Introduction,[0],[0]
"To fully disclose the predicate-argument structure, typical SRL systems have to step by step perform four subtasks.",2 Model,[0],[0]
"Since the predicates in CoNLL2009 (Hajič et al., 2009) corpus have been preidentified, we need to tackle three other subtasks, which are formulized into two-step pipeline in this work, predicate disambiguation and argument labeling.",2 Model,[0],[0]
"Namely, we do the work of argument identification and classification in one model.
",2 Model,[0],[0]
Argument structure for each known predicate will be disclosed by our argument labeler over a sequence including possible arguments (candidates).,2 Model,[0],[0]
"There are two ways to determine the sequence, one is to simply input the entire sentence as a syntax-agnostic SRL system does, the other is to select words according to syntactic parse tree around the predicate as most previous SRL systems did.",2 Model,[0],[0]
The latter strategy usually works through a syntactic tree based argument pruning algorithm.,2 Model,[0],[0]
"We will use the proposed k-order argument pruning algorithm (Section 2.1) to get a sequence w = (w1, . . .",2 Model,[0],[0]
", wn) for each predicate.",2 Model,[0],[0]
"Then, we represent each word wi ∈ w as xi (Section 2.2).",2 Model,[0],[0]
"Eventually, we obtain contextual features with sequence encoder (Section 2.3).",2 Model,[0],[0]
The overall role labeling model is depicted in Figure 1.,2 Model,[0],[0]
"As pointed out by Punyakanok et al. (2008), syntactic information is most relevant in identifying
1CoNLL-2008 is an English-only task, while CoNLL2009 extends to a multilingual one.",2.1 Argument Pruning,[0],[0]
"Their main difference is that predicates have been beforehand indicated for the latter.
",2.1 Argument Pruning,[0],[0]
"the arguments, and the most crucial contribution of full parsing is in the pruning stage.",2.1 Argument Pruning,[0],[0]
"In this paper, we propose a k-order argument pruning algorithm inspired by Zhao et al. (2009b).",2.1 Argument Pruning,[0],[0]
"First of all, for node n and its descendant nd in a syntactic dependency tree, we define the order to be the distance between the two nodes, denoted as D(n, nd).",2.1 Argument Pruning,[0],[0]
"Then we define k-order descendants of given node satisfying D(n, nd) = k, and k-order traversal that visits each node from the given node to its descendant nodes within k-th order.",2.1 Argument Pruning,[0],[0]
"Note that the definition of k-order traversal is somewhat different from tree traversal in terminology.
",2.1 Argument Pruning,[0],[0]
A brief description of the proposed k-order pruning algorithm is given as follow.,2.1 Argument Pruning,[0],[0]
"Initially, we set a given predicate as the current node in a syntactic dependency tree.",2.1 Argument Pruning,[0],[0]
"Then, collect all its argument candidates by the strategy of k-order traversal.",2.1 Argument Pruning,[0],[0]
"Afterwards, reset the current node to its syntactic head and repeat the previous step till the root of the tree.",2.1 Argument Pruning,[0],[0]
"Finally, collect the root and stop.",2.1 Argument Pruning,[0],[0]
The k-order argument algorithm is presented in Algorithm 1 in detail.,2.1 Argument Pruning,[0],[0]
"An example of a syntactic dependency tree for sentence She began to trade the art for money is shown in Figure 2.
",2.1 Argument Pruning,[0],[0]
"The main reasons for applying the extended korder argument pruning algorithm are two-fold.
",2.1 Argument Pruning,[0],[0]
Algorithm 1 k-order argument pruning algorithm,2.1 Argument Pruning,[0],[0]
"Input: A predicate p, the root node r given a syn-
tactic dependency tree T , the order k Output:",2.1 Argument Pruning,[0],[0]
"The set of argument candidates S
1: initialization set p as current node c, c = p 2: for each descendant ni of c in T do 3: if D(c, ni) ≤ k",2.1 Argument Pruning,[0],[0]
"and ni /∈ S then 4: S = S + ni 5: end if 6: end for 7: find the syntactic head ch of c, and let c = ch 8: if c = r then 9: S = S + r
10: else 11: goto step 2 12: end if 13: return argument candidates set S
First, previous standard pruning algorithm may hurt the argument coverage too much, even though indeed arguments usually tend to surround their predicate in a close distance.",2.1 Argument Pruning,[0],[0]
"As a sequence tagging model has been applied, it can effectively handle the imbalanced distribution between arguments and non-arguments, which is hardly tackled by early argument classification models that commonly adopt the standard pruning algorithm.",2.1 Argument Pruning,[0],[0]
"Second, the extended pruning algorithm provides a better trade-off between computational cost and performance by carefully tuning k.",2.1 Argument Pruning,[0],[0]
"We produce a predicate-specific word representation xi for each word wi, where i stands for the word position in an input sequence, following Marcheggiani et al. (2017).",2.2 Word Representation,[0],[0]
"However, we differ by (1) leveraging a predicate-specific indicator embedding, (2) using deeper refined representation, including character and dependency relation embeddings, and (3) applying recent advances in RNNs, such as highway connections (Srivastava et al., 2015).
",2.2 Word Representation,[0],[0]
"In this work, word representation xi is the concatenation of four types of features: predicatespecific feature, character-level, word-level and linguistic features.",2.2 Word Representation,[0],[0]
"Unlike previous work, we leverage a predicate-specific indicator embedding xiei rather than directly using a binary flag either 0 or 1.",2.2 Word Representation,[0],[0]
"At character level, we exploit convolutional neural network (CNN) with bidirectional LSTM (BiLSTM) to learn character embedding
xcei .",2.2 Word Representation,[0],[0]
"As shown in Figure 1, the representation calculated by the CNN is fed as input to BiLSTM.",2.2 Word Representation,[0],[0]
"At word level, we use a randomly initialized word embedding xrei and a pre-trained word embedding xpei .",2.2 Word Representation,[0],[0]
"For linguistic features, we employ a randomly initialized lemma embedding xlei and a randomly initialized POS tag embedding xposi .",2.2 Word Representation,[0],[0]
"In order to incorporate more syntactic information, we adopt an additional feature, the dependency relation to syntactic head.",2.2 Word Representation,[0],[0]
"Likewise, it is a randomly initialized embedding xdei .",2.2 Word Representation,[0],[0]
The resulting word representation is concatenated as xi =,2.2 Word Representation,[0],[0]
"[x ie i , x ce i , x re i , x pe",2.2 Word Representation,[0],[0]
"i , x le i , x pos",2.2 Word Representation,[0],[0]
"i , x de i ].",2.2 Word Representation,[0],[0]
"As Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) have shown significant representational effectiveness to NLP tasks, we thus use BiLSTM as the sentence encorder.",2.3 Sequence Encoder,[0],[0]
"Given an input sequence x = (x1, . . .",2.3 Sequence Encoder,[0],[0]
", xn), BiLSTM processes the sequence in both forward and backward direction to obtain two separated hidden states, −→ h i which handles data from x1 to xi and ←− h i which tackles data from xn to xi for each word representation.",2.3 Sequence Encoder,[0],[0]
"Finally, we get a contextual representation hi =",2.3 Sequence Encoder,[0],[0]
"[ −→ h i, ←− h",2.3 Sequence Encoder,[0],[0]
i] by concatenating the states of BiLSTM networks.,2.3 Sequence Encoder,[0],[0]
"To get the final predicted semantic roles, we exploit a multi-layer perceptron (MLP) with highway connections on the top of BiLSTM networks, which takes as input the hidden representation hi
of all time steps.",2.3 Sequence Encoder,[0],[0]
The MLP network consists of 10 layers with highway connections and we employ ReLU activations for the hidden layers.,2.3 Sequence Encoder,[0],[0]
"Finally, we use a softmax layer over the outputs to maximize the likelihood of labels.",2.3 Sequence Encoder,[0],[0]
"Although predicates have been identified given a sentence, predicate disambiguation is an indispensable task, which aims to determine the predicate-argument structure for an identified predicate in a particular context.",2.4 Predicate Disambiguation,[0],[0]
"Here, we also use the identical model (BiLSTM composed with MLP) for predicate disambiguation, in which the only difference is that we remove the syntactic dependency relation feature in corresponding word representation (Section 2.2).",2.4 Predicate Disambiguation,[0],[0]
"Exactly, given a predicate p, the resulting word representation is pi =",2.4 Predicate Disambiguation,[0],[0]
"[p ie i , p ce",2.4 Predicate Disambiguation,[0],[0]
"i , p re i , p pe",2.4 Predicate Disambiguation,[0],[0]
"i , p le i , p pos",2.4 Predicate Disambiguation,[0],[0]
i ].,2.4 Predicate Disambiguation,[0],[0]
"Our model2 is evaluated on the CoNLL-2009 shared task both for English and Chinese datasets, following the standard training, development and test splits.",3 Experiments,[0],[0]
"The hyperparameters in our model were selected based on the development set, and are summarized in Table 1.",3 Experiments,[0.963934059452529],"['The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.']"
Note that the parameters of predicate model are the same as these in argument model.,3 Experiments,[0],[0]
"All real vectors are randomly initialized, and the pre-trained word embeddings for English are GloVe vectors (Pennington et al., 2014).",3 Experiments,[0],[0]
"For Chinese, we exploit Wikipedia documents to train Word2Vec embeddings (Mikolov
2The code is available at https://github.com/ bcmi220/srl_syn_pruning.
",3 Experiments,[0],[0]
"et al., 2013).",3 Experiments,[0],[0]
"During training procedures, we use the categorical cross-entropy as objective, with Adam optimizer (Kingma and Ba, 2015).",3 Experiments,[0],[0]
We train models for a maximum of 20 epochs and obtain the nearly best model based on development results.,3 Experiments,[0],[0]
"For argument labeling, we preprocess corpus with k-order argument pruning algorithm.",3 Experiments,[0],[0]
"In addition, we use four CNN layers with singlelayer BiLSTM to induce character representations derived from sentences.",3 Experiments,[0],[0]
"For English3, to further enhance the representation, we adopt CNNBiLSTM character embedding structure from AllenNLP toolkit (Peters et al., 2018).",3 Experiments,[0],[0]
"During the pruning of argument candidates, we use the officially predicted syntactic parses provided by CoNLL-2009 shared-task organizers on both English and Chinese.",3.1 Preprocessing,[0],[0]
Figure 3 shows changing curves of coverage and reduction following k on the English train set.,3.1 Preprocessing,[0],[0]
"According to our statistics, the number of non-arguments is ten times more than that of arguments, where the data distribution is fairly unbalanced.",3.1 Preprocessing,[0],[0]
"However, a proper pruning strategy could alleviate this problem.",3.1 Preprocessing,[0],[0]
"Accordingly, the first-order pruning reduces more than 50% candidates at the cost of missing 5.5% true ones on average, and the second-order prunes about 40% candidates with nearly 2.0% loss.",3.1 Preprocessing,[0],[0]
"The coverage of third-order has achieved 99% and it reduces approximately 1/3 corpus size.
",3.1 Preprocessing,[0],[0]
"It is worth noting that as k is larger than 19,
3For Chinese, we do not use character embedding.
",3.1 Preprocessing,[0],[0]
"there will come full coverage on all argument candidates for English training set, which let our high order pruning algorithm degrade into a syntaxagnostic setting.",3.1 Preprocessing,[0],[0]
"In this work, we use the tenthorder pruning for pursuing the best performance.",3.1 Preprocessing,[0],[0]
"Our system performance is measured with the official script from CoNLL-2009 benchmarks, combining the output of our predicate disambiguation with our semantic role labeling.",3.2 Results,[0],[0]
"Our predicate disambiguation model achieves the accuracy of 95.01% and 95.58%4 on development and test sets, respectively.",3.2 Results,[0],[0]
"We compare our model performance with the state-of-the-art models for dependency SRL.5 Noteworthily, our model is local and single without reranking, which neither includes global inference nor combines multiple models.",3.2 Results,[0],[0]
"The experimental results on the English in-domain (WSJ) and out-of-domain (Brown) test sets are shown in Tables 2 and 3, respectively.
",3.2 Results,[0],[0]
"For English, our syntax-aware model outperforms previously published best single model, scoring 89.5% F1 with 1.5% absolute improvement on the in-domain (WSJ) test data.",3.2 Results,[0],[0]
"Compared
4Note that we give a slightly better predicate model than Roth and Lapata (2016), with 94.77% and 95.47% accuracy on development and test sets, respectively.
5Here, we do not compare against span-based SRL models, which annotate roles for entire argument spans instead of semantic dependencies.
with ensemble models, our single model even provides better performance (+0.4% F1) than the system (Marcheggiani and Titov, 2017), and significantly surpasses all the rest models.",3.2 Results,[0],[0]
"In the syntaxagnostic setting (without pruning and dependency relation embedding), we also reach the new stateof-the-art, achieving a performance gain of 1% F1.
",3.2 Results,[0],[0]
"On the out-of-domain (Brown) test set, we achieve the new best results of 79.3% (syntaxaware) and 78.8% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"Moreover, our syntax-aware model performs better than the syntax-agnostic one.
",3.2 Results,[0],[0]
Table 4 presents the results on Chinese test set.,3.2 Results,[0],[0]
"Even though we use the same parameters as for English, our model also outperforms the best reported results by 0.3% (syntax-aware) and 0.6% (syntax-agnostic) in F1 scores.",3.2 Results,[0],[0]
"To evaluate the contributions of key factors in our method, a series of ablation studies are performed on the English development set.
",3.3 Analysis,[0],[0]
"In order to demonstrate the effectiveness of our k-order pruning algorithm, we report the SRL performance excluding predicate senses in evaluation, eliminating the performance gain from predicate disambiguation.",3.3 Analysis,[0],[0]
Table 5 shows the results from our syntax-aware model with lower order argument pruning.,3.3 Analysis,[0],[0]
"Compared to the best previous model, our system still yields an increment in recall by more than 1%, leading to improvements in F1 score.",3.3 Analysis,[0],[0]
"It demonstrates that refining syntactic parser tree based candidate pruning does help in argument recognition.
",3.3 Analysis,[0],[0]
"Table 6 presents the performance of our syntaxagnostic SRL system with a basic configuration, which removes components, including indicator and character embeddings.",3.3 Analysis,[0],[0]
"Note that the first row is the results of BiLSTM (removing MLP from basic model), whose encoding is the same as Marcheggiani et al. (2017).",3.3 Analysis,[0],[0]
"Experiments show that both enhanced representations improve over our basic model, and our adopted labeling model is superior to the simple BiLSTM.
Figure 4 shows F1 scores in different k-order pruning together with our syntax-agnostic model.",3.3 Analysis,[0],[0]
"It also indicates that the least first-order pruning fails to give satisfactory performance, the best performing setting coming from a moderate setting of k = 10, and the largest k shows that our argu-
ment pruning falls back to syntax-agnostic type.",3.3 Analysis,[0],[0]
"Meanwhile, from the best k setting to the lower order pruning, we receive a much faster performance drop, compared to the higher order pruning until the complete syntax-agnostic case.",3.3 Analysis,[0],[0]
"The proposed k-order pruning algorithm always works even it reaches the syntax-agnostic setting, which empirically explains why the current syntax-aware and syntax-agnostic SRL models hold little performance difference, as maximum k-order pruning actually removes few words just like syntaxagnostic model.",3.3 Analysis,[0],[0]
"In this work, we consider additional model that integrates predicate disambiguation and argument labeling into one sequence labeling model.",3.4 End-to-end SRL,[0],[0]
"In order to implement an end-to-end model, we introduce a virtual root (VR) for predicate disambiguation similar to Zhao et al. (2013) who handled the entire SRL task as word pair classification.",3.4 End-to-end SRL,[0],[0]
"Concretely, we add a predicate sense feature to the input sequence by concatenating a VR.",3.4 End-to-end SRL,[0],[0]
The word representation of VR is randomly initialized during training.,3.4 End-to-end SRL,[0],[0]
"In Figure 5, we give an example sequence with the labels for the given sentence.
",3.4 End-to-end SRL,[0],[0]
We also report results of our end-to-end model on CoNLL-2009 test set with syntax-aware and syntax-agnostic settings.,3.4 End-to-end SRL,[0],[0]
"As shown in Table 7, our end-to-end model yields slightly weaker performance compared with our pipeline.",3.4 End-to-end SRL,[0],[0]
"A reasonable account for performance degradation is that the training data has completely different genre distributions over predicate senses and argument roles, which may be somewhat confusing for integrative model to make classification decisions.",3.4 End-to-end SRL,[0],[0]
"For a full SRL task, the predicate identification subtask is also indispensable, which has been included in CoNLL-2008 shared task.",3.5 CoNLL-2008 SRL Setting,[0],[0]
"We thus evaluate our model in terms of data and setting of the CoNLL-2008 benchmark (WSJ).
",3.5 CoNLL-2008 SRL Setting,[0],[0]
"To identify predicates, we train the BiLSTMMLP sequence labeling model with same parameters in Section 2.4 to tackle the predicate identification and disambiguation subtasks in one shot, and the only difference is that we remove the predicate-specific indicator feature.",3.5 CoNLL-2008 SRL Setting,[0],[0]
The F1 score of our predicate labeling model is 90.53% on indomain (WSJ) data.,3.5 CoNLL-2008 SRL Setting,[0],[0]
"Compared with the best reported results, we observe absolute improvements in semantic F1 of 0.8% (in Table 8).",3.5 CoNLL-2008 SRL Setting,[0],[0]
"Note that as predicate identification is introduced, our same model shows about 6% performance loss for either syntax-agnostic or syntax-aware case, which indicates that predicate identification should be carefully handled, as it is very needed in a complete practical SRL system.",3.5 CoNLL-2008 SRL Setting,[0],[0]
Syntactic information plays an informative role in semantic role labeling.,4 Syntactic Contribution,[0],[0]
"However, few studies were done to quantitatively evaluate the syntactic contribution to SRL.",4 Syntactic Contribution,[0],[0]
"Furthermore, we observe that most of the above compared neural SRL systems took the syntactic parser of (Björkelund et al., 2010) as syntactic inputs instead of the one from CoNLL-2009 shared task, which adopted a much weaker syntactic parser.",4 Syntactic Contribution,[0],[0]
"Especially (Marcheggiani and Titov, 2017), adopted an external syntactic
parser with even higher parsing accuracy.",4 Syntactic Contribution,[0],[0]
"Contrarily, our SRL model is based on the automatically predicted parse with moderate performance provided by CoNLL-2009 shared task, but outperforms their models.
",4 Syntactic Contribution,[0],[0]
This section thus attempts to explore how much syntax contributes to dependency-based SRL in deep learning framework and how to effectively evaluate relative performance of syntax-based SRL.,4 Syntactic Contribution,[0],[0]
"To this end, we conduct experiments for empirical analysis with different syntactic inputs.
",4 Syntactic Contribution,[0],[0]
"Syntactic Input In order to obtain different syntactic inputs, we design a faulty syntactic tree generator (refer to STG hereafter), which is able to produce random errors in the output parse tree like a true parser does.",4 Syntactic Contribution,[0],[0]
"To simplify implementation, we construct a new syntactic tree based on the gold standard parse tree.",4 Syntactic Contribution,[0],[0]
"Given an input error probability distribution estimated from a true parser output, our algorithm presented in Algorithm 2 stochastically modifies the syntactic heads of nodes on the premise of a valid tree.
",4 Syntactic Contribution,[0],[0]
"Evaluation Measure For SRL task, the primary evaluation measure is the semantic labeled F1 score.",4 Syntactic Contribution,[0],[0]
"However, the score is influenced by the quality of syntactic input to some extent, leading to unfaithfully reflecting the competence of syntax-based SRL system.",4 Syntactic Contribution,[0],[0]
"Namely, this is not the outcome of a true and fair quantitative comparison for these types of SRL models.",4 Syntactic Contribution,[0],[0]
"To normalize the semantic score relative to syntactic parse, we take into account additional evaluation measure to estimate the actual overall performance of SRL.",4 Syntactic Contribution,[0],[0]
"Here, we use the ratio between labeled F1 score for semantic dependencies (Sem-F1) and the labeled attachment score (LAS) for syntactic dependencies
Algorithm 2 Faulty Syntactic Tree Generator Input: A gold standard syntactic tree GT , the
specific error probability p Output: The new generative syntactic tree NT
1: N denotes the number of nodes in GT 2: for each node n ∈ GT do 3: r = random(0, 1), a random number 4: if r < p then 5: h = random(0, N ), a random integer 6: find the syntactic head nh of n in GT 7: modify nh = h, and get a new tree NT 8: if NT is a valid tree then 9: break
10: else 11: goto step 5 12: end if 13: end if 14: end for 15: return the new generative tree NT
proposed by Surdeanu et al. (2008) as evaluation metric.6 The benefits of this measure are twofold: quantitatively evaluating syntactic contribution to SRL and impartially estimating the true performance of SRL, independent of the performance of the input syntactic parser.
",4 Syntactic Contribution,[0],[0]
Table 9 reports the performance of existing models7 in term of Sem-F1/LAS ratio on CoNLL2009 English test set.,4 Syntactic Contribution,[0],[0]
"Interestingly, even though our system has significantly lower scores than others by 3.8% LAS in syntactic components, we
6The idea of ratio score in Surdeanu et al. (2008) actually was from author of this paper, Hai Zhao, which has been indicated in the acknowledgement part of Surdeanu et al. (2008).
7Note that several SRL systems without providing syntactic information are not listed in the table.
obtain the highest results both on Sem-F1 and the Sem-F1/LAS ratio, respectively.",4 Syntactic Contribution,[0.9558685473238925],"['Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.']"
These results show that our SRL component is relatively much stronger.,4 Syntactic Contribution,[0],[0]
"Moreover, the ratio comparison in Table 9 also shows that since the CoNLL-2009 shared task, most SRL works actually benefit from the enhanced syntactic component rather than the improved SRL component itself.",4 Syntactic Contribution,[0],[0]
"All post-CoNLL SRL systems, either traditional or neural types, did not exceed the top systems of CoNLL-2009 shared task, (Zhao et al., 2009c) (SRL-only track using the provided predicated syntax) and (Zhao et al., 2009a)",4 Syntactic Contribution,[0],[0]
(Joint track using self-developed parser).,4 Syntactic Contribution,[0],[0]
"We believe that this work for the first time reports both higher Sem-F1 and higher Sem-F1/LAS ratio since CoNLL-2009 shared task.
",4 Syntactic Contribution,[0],[0]
"We also perform our first and tenth order pruning models with different erroneous syntactic inputs generated from STG and evaluate their per-
formance using the Sem-F1/LAS ratio.",4 Syntactic Contribution,[0],[0]
Figure 6 shows Sem-F1 scores at different quality of syntactic parse inputs on the English test set whose LAS varies from 85% to 100%.,4 Syntactic Contribution,[0],[0]
"Compared to previous state-of-the-arts (Marcheggiani and Titov, 2017).",4 Syntactic Contribution,[0],[0]
"Our tenth-order pruning model gives quite stable SRL performance no matter the syntactic input quality varies in a broad range, while our firstorder pruning model yields overall lower results (1-5% F1 drop), owing to missing too many true arguments.",4 Syntactic Contribution,[0],[0]
These results show that high-quality syntactic parses may indeed enhance dependency SRL.,4 Syntactic Contribution,[0],[0]
"Furthermore, it indicates that our model with an accurate enough syntactic input as Marcheggiani and Titov (2017), namely, 90% LAS, will give a Sem-F1 exceeding 90% for the first time in the research timeline of semantic role labeling.",4 Syntactic Contribution,[0],[0]
Semantic role labeling was pioneered by Gildea and Jurafsky (2002).,5 Related Work,[0],[0]
"Most traditional SRL models rely heavily on feature templates (Pradhan et al., 2005; Zhao et al., 2009b; Björkelund et al., 2009).",5 Related Work,[0],[0]
"Among them, Pradhan et al. (2005) combined features derived from different syntactic parses based on SVM classifier, while Zhao et al. (2009b) presented an integrative approach for dependency SRL by greedy feature selection algorithm.",5 Related Work,[0],[0]
"Later, Collobert et al. (2011) proposed a convolutional neural network model of inducing word embeddings substituting for hand-crafted features, which was a breakthrough for SRL task.
",5 Related Work,[0],[0]
"With the impressive success of deep neural networks in various NLP tasks (Zhang et al., 2016; Qin et al., 2017; Cai et al., 2017), a series of neural SRL systems have been proposed.",5 Related Work,[0],[0]
"Foland and Martin (2015) presented a dependency semantic role labeler using convolutional and time-domain neural networks, while FitzGerald et al. (2015) exploited neural network to jointly embed arguments and semantic roles, akin to the work (Lei et al., 2015), which induced a compact feature representation applying tensor-based approach.",5 Related Work,[0],[0]
"Recently, researchers consider multiple ways to effectively integrate syntax into SRL learning.",5 Related Work,[0],[0]
Roth and Lapata (2016) introduced dependency path embedding to model syntactic information and exhibited a notable success.,5 Related Work,[0],[0]
Marcheggiani and Titov (2017) leveraged the graph convolutional network to incorporate syntax into neural models.,5 Related Work,[0],[0]
"Differently, Marcheggiani et al. (2017) proposed a
syntax-agnostic model using effective word representation for dependency SRL, which for the first time achieves comparable performance as stateof-the-art syntax-aware SRL models.
",5 Related Work,[0],[0]
"However, most neural SRL works seldom pay much attention to the impact of input syntactic parse over the resulting SRL performance.",5 Related Work,[0],[0]
"This work is thus more than proposing a high performance SRL model through reviewing the highlights of previous models, and presenting an effective syntactic tree based argument pruning.",5 Related Work,[0],[0]
"Our work is also closely related to (Punyakanok et al., 2008; He et al., 2017).",5 Related Work,[0],[0]
"Under the traditional methods, Punyakanok et al. (2008) investigated the significance of syntax to SRL system and shown syntactic information most crucial in the pruning stage.",5 Related Work,[0],[0]
"He et al. (2017) presented extensive error analysis with deep learning model for span SRL, including discussion of how constituent syntactic parser could be used to improve SRL performance.",5 Related Work,[0],[0]
"This paper presents a simple and effective neural model for dependency-based SRL, incorporating syntactic information with the proposed extended k-order pruning algorithm.",6 Conclusion and Future Work,[0],[0]
"With a large enough setting of k, our pruning algorithm will result in a syntax-agnostic setting for the argument labeling model, which smoothly unifies syntax-aware and syntax-agnostic SRL in a consistent way.",6 Conclusion and Future Work,[0],[0]
"Experimental results show that with the help of deep enhanced representation, our model outperforms the previous state-of-the-art models in both syntaxaware and syntax-agnostic situations.
",6 Conclusion and Future Work,[0],[0]
"In addition, we consider the Sem-F1/LAS ratio as a mean of evaluating syntactic contribution to SRL, and true performance of SRL independent of the quality of syntactic parser.",6 Conclusion and Future Work,[0],[0]
"Though we again confirm the importance of syntax to SRL with empirical experiments, we are aware that since (Pradhan et al., 2005), the gap between syntax-aware and syntax-agnostic SRL has been greatly reduced, from as high as 10% to only 1-2% performance loss in this work.",6 Conclusion and Future Work,[0],[0]
"However, maybe we will never reach a satisfying conclusion, as whenever one proposes a syntax-agnostic SRL system which can outperform all syntax-aware ones at then, always there comes argument that you have never fully explored creative new method to effectively exploit the syntax input.",6 Conclusion and Future Work,[0],[0]
Semantic role labeling (SRL) is dedicated to recognizing the predicate-argument structure of a sentence.,abstractText,[0],[0]
Previous studies have shown syntactic information has a remarkable contribution to SRL performance.,abstractText,[0],[0]
"However, such perception was challenged by a few recent neural SRL models which give impressive performance without a syntactic backbone.",abstractText,[0],[0]
This paper intends to quantify the importance of syntactic information to dependency SRL in deep learning framework.,abstractText,[0],[0]
We propose an enhanced argument labeling model companying with an extended korder argument pruning algorithm for effectively exploiting syntactic information.,abstractText,[0],[0]
"Our model achieves state-of-the-art results on the CoNLL-2008, 2009 benchmarks for both English and Chinese, showing the quantitative significance of syntax to neural SRL together with a thorough empirical survey over existing models.",abstractText,[0.9557316204775046],['The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.']
"Syntax for Semantic Role Labeling, To Be, Or Not To Be",title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 55–64, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Simultaneous interpretation is challenging because it demands both quality and speed.,1 Introduction,[0],[0]
Conventional batch translation waits until the entire sentence is completed before starting to translate.,1 Introduction,[0],[0]
This merely optimizes translation quality and often introduces undesirable lag between the speaker and the audience.,1 Introduction,[0],[0]
Simultaneous interpretation instead requires a tradeoff between quality and speed.,1 Introduction,[0],[0]
A common strategy is to translate independently translatable segments as soon as possible.,1 Introduction,[0],[0]
"Various segmentation methods (Fujita et al., 2013; Oda et al., 2014) reduce translation delay; they are limited, however, by the unavoidable word reordering between languages with drastically different word orders.",1 Introduction,[0],[0]
We show an example of Japanese-English translation in Figure 1.,1 Introduction,[0],[0]
"Consider the batch translation: in English, the verb change comes immediately after the subject",1 Introduction,[0],[0]
"We, whereas in Japanese it comes at the end
of the sentence; therefore, to produce an intelligible English sentence, we must translate the object after the final verb is observed, resulting in one large and painfully delayed segment.
",1 Introduction,[0],[0]
"To reduce structural discrepancy, we can apply syntactic transformations to make the word order of one language closer to the other.",1 Introduction,[0],[0]
Consider the monotone translation in Figure 1.,1 Introduction,[0],[0]
"By passivizing the English sentence, we can cache the subject and begin translating before observing the final verb.",1 Introduction,[0],[0]
"Furthermore, by using the English possessive, we mimic the order of the Japanese genitive construction.",1 Introduction,[0],[0]
"These transformations enable us to divide the input into shorter segments, thus reducing translation delay.
",1 Introduction,[0],[0]
"To produce such monotone translations, a straightforward approach is to incorporate interpretation data into the learning of a machine translation (MT) system, because human interpreters use a variety of strategies (Shimizu et al., 2014; Camayd-Freixas, 2011; Tohyama and Matsubara, 2006) to fine-tune the word order.",1 Introduction,[0],[0]
Shimizu et al. (2013) shows that this approach improves the speed-accuracy tradeoff.,1 Introduction,[0],[0]
"However, existing parallel simultaneous interpretation corpora (Shimizu et al., 2014; Matsubara et al., 2002; Bendazzoli and Sandrelli, 2005) are often small, and collecting new data is expensive due to the inherent costs of recording and transcribing speeches (Paulik and Waibel, 2010).",1 Introduction,[0],[0]
"In addition, due to the intense time pressure during interpretation, human interpretation has the disadvantage of simpler, less precise diction (Camayd-Freixas, 2011; Al-Khanji et al., 2000) compared to human translations done at the translator’s leisure, allowing for more introspection and precise word choice.
",1 Introduction,[0],[0]
We aim to address the data scarcity problem and combine translators’ lexical precision and interpreters’ syntactic flexibility.,1 Introduction,[0],[0]
"We propose to rewrite the reference translation in a way that uses the original lexicon, obeys standard grammar rules of
55
the target language, preserves the original semantics, and yields more monotonic translations.",1 Introduction,[0],[0]
We then train the MT system with the rewritten references so that it learns how to produce low-latency translations from the data.,1 Introduction,[0],[0]
A data-driven approach to learning these rewriting rules is hampered by the dearth of parallel data: we have few examples of text that have been both interpreted and translated.,1 Introduction,[0],[0]
"Therefore, we design syntactic transformation rules based on linguistic analysis of the source and the target languages.",1 Introduction,[0],[0]
We apply these rules to parsed text and decide whether to accept the rewritten sentence based on the amount of delay reduction.,1 Introduction,[0],[0]
"In this work, we focus on Japanese to English translation, because (i) Japanese and English have significantly different word orders (SOV vs. SVO); and consequently, (ii) the syntactic constituents required earlier by an English sentence often come late in the corresponding Japanese sentence.
",1 Introduction,[0],[0]
We evaluate our approach using standard machine translation data (the Reuters newsfeed Japanese-English corpus) in a simultaneous translation setting.,1 Introduction,[0],[0]
Our experimental results show that including the rewritten references into the learning of a phrase-based MT system results in a better speed-accuracy tradeoff against both the original and the rewritten reference translations.,1 Introduction,[0],[0]
Simultaneous interpretation has two goals: producing good translations and producing them promptly.,2 The Problem of Delay Reduction,[0],[0]
"However, most existing parallel corpora and MT systems do not address the issue of delay during translation.",2 The Problem of Delay Reduction,[0],[0]
We explicitly adapt the training data by rewriting rules to reduce delay.,2 The Problem of Delay Reduction,[0],[0]
We first define translation delay and describe—in general terms— our rewriting rules.,2 The Problem of Delay Reduction,[0],[0]
"In the next section, we describe the rules in more detail.
",2 The Problem of Delay Reduction,[0],[0]
"While we are motivated by real-time interpretation, to simplify our problem, we assume that we have perfect text input.",2 The Problem of Delay Reduction,[0],[0]
"Given this constraint, a typical simultaneous interpretation system (Sridhar et al., 2013; Fujita et al., 2013; Oda et al., 2014) produces partial translations of consecutive segments in the source sentence and concatenates them to produce a complete translation.",2 The Problem of Delay Reduction,[0],[0]
We define the translation delay of a sentence as the average number of tokens the system has to observe between translation of two consecutive segments (denoted by # words/seg).1,2 The Problem of Delay Reduction,[0],[0]
"For instance, the minimum delay of 1 word/seg is achieved when we translate immediately upon hearing a word.",2 The Problem of Delay Reduction,[0],[0]
"At test time, when the input is segmented, the delay is the average segment length.",2 The Problem of Delay Reduction,[0],[0]
"During the data preprocessing step of rewriting, we calculate delay from word alignments (Section 4).
",2 The Problem of Delay Reduction,[0],[0]
"Given a reference batch translation x, we apply a set of rewriting rulesR to x to minimize its delay.",2 The Problem of Delay Reduction,[0],[0]
"A rewriting rule r ∈ R is a mapping that takes the constituent parse tree of x as input and outputs a modified parse tree, which specifies a rewritten sentence x′.",2 The Problem of Delay Reduction,[0],[0]
"The tree-editing operation includes node deletion, insertion, and swapping, as well as induced changes of word form and node label.",2 The Problem of Delay Reduction,[0],[0]
"A valid transformation rule should rearrange constituents in x to follow the word order of the input sentence as closely as possible, subject to grammatical constraints and preservation of the original meaning.
",2 The Problem of Delay Reduction,[0],[0]
"1Ideally, delay should be based on time lapse.",2 The Problem of Delay Reduction,[0],[0]
"However, timestamping is not applicable to typical MT corpus, therefore we approximate it by number of tokens and ignore decoding time.",2 The Problem of Delay Reduction,[0],[0]
We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences.,3 Transformation Rules,[0],[0]
"Our rules cover verb, noun, and clause reordering.",3 Transformation Rules,[0],[0]
"While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",3 Transformation Rules,[0],[0]
The most significant difference between Japanese and English is that the head of a verb phrase comes at the end of Japanese sentences.,3.1 Verb Phrases,[0],[0]
"In English, it occupies one of the initial positions.",3.1 Verb Phrases,[0],[0]
"We now introduce rules that can postpone a head verb.
",3.1 Verb Phrases,[0],[0]
Passivization and Activization,3.1 Verb Phrases,[0],[0]
"In Japanese, the standard structure of a sentence is NP1 NP2 verb, where case markers following the verb indicate the voice of the sentence.",3.1 Verb Phrases,[0],[0]
"However, in English, we have NP1 verb NP2, where the form of the verb indicates its voice.",3.1 Verb Phrases,[0],[0]
Changing the voice is particularly useful when NP2 (object in an active-voice sentence and subject in a passive-voice sentence) is long.,3.1 Verb Phrases,[0],[0]
"By reversing positions of verb and NP2, we are not held back by the upcoming verb and can start to translate NP2 immediately.",3.1 Verb Phrases,[0],[0]
"Figure 1 shows an example in which passive voice can help make the target and source word orders more compatible, but it is not the case that passivizing every sentence would be a good idea; sometimes making a passive sentence active makes the word orders more compatible if the objects are relatively short:
",3.1 Verb Phrases,[0],[0]
O: The talk was denied by the boycott group spokesman.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The boycott group spokesman denied the talk.
",3.1 Verb Phrases,[0],[0]
"Quotative Verbs Quotative verbs are verbs that, syntactically and semantically, resemble said and often start an independent clause.",3.1 Verb Phrases,[0],[0]
"Such verbs are frequent, especially in news, and can be moved to the end of a sentence:
",3.1 Verb Phrases,[0],[0]
O: They announced that the president will restructure the division.,3.1 Verb Phrases,[0],[0]
R:,3.1 Verb Phrases,[0],[0]
"The president will restructure the division, they announced.
",3.1 Verb Phrases,[0],[0]
"In addition to quotative verbs, candidates typically include factive (e.g., know, realize, observe), factive-like (e.g., announce, determine), belief (e.g., believe, think, suspect), and antifactive (e.g., doubt, deny) verbs.",3.1 Verb Phrases,[0],[0]
"When these verbs are followed by a
clause (S or SBAR), we move the verb and its subject to the end of the clause.
",3.1 Verb Phrases,[0],[0]
"While some exploratory work automatically extracts factive verbs, to our knowledge, an exhaustive list does not exist.",3.1 Verb Phrases,[0],[0]
"To obtain a list with reasonable coverage, we exploit the fact that Japanese has an unambiguous quotative particle, to, that precedes such verbs.2 We identify all of the verbs in the Kyoto corpus (Neubig, 2011) marked by the quotative particle and translate them into English.",3.1 Verb Phrases,[0],[0]
We then use these as our quotative verbs.3,3.1 Verb Phrases,[0],[0]
Another difference between Japanese and English lies in the order of adjectives and the nouns they modify.,3.2 Noun Phrases,[0],[0]
"We identify two situations where we can take advantage of the flexibility of English grammar to favor sentence structures consistent with positions of nouns in Japanese.
",3.2 Noun Phrases,[0],[0]
"Genitive Reordering In Japanese, genitive constructions always occur in the form of X no Y, where Y belongs to X.",3.2 Noun Phrases,[0],[0]
"In English, however, the order may be reversed through the of construction.",3.2 Noun Phrases,[0],[0]
"Therefore, we transform constructions NP1 of NP2 to possessives using the apostrophe-s, NP2’(s) NP1 (Figure 1).",3.2 Noun Phrases,[0],[0]
We use simple heuristics to decide if such a transformation is valid.,3.2 Noun Phrases,[0],[0]
"For example, when X / Y contains proper nouns (e.g., the City of New York), numbers (e.g., seven pounds of sugar), or pronouns (e.g., most of them), changing them to the possessive case is not legal.
that Clause In English, clauses are often modified through a pleonastic pronoun.",3.2 Noun Phrases,[0],[0]
"E.g., It is ADJP to/that SBAR/S.",3.2 Noun Phrases,[0],[0]
"In Japanese, however, the subject (clause) is usually put at the beginning.",3.2 Noun Phrases,[0],[0]
"To be consistent with the Japanese word order, we move the modified clause to the start of the sentence: To S/SBAR is ADJP.",3.2 Noun Phrases,[0],[0]
"The rewritten English sentence is still grammatical, although its structure is less frequent in common English usage.",3.2 Noun Phrases,[0],[0]
"For example,
O: It is important to remain watchful.",3.2 Noun Phrases,[0],[0]
"R: To remain watchful is important.
",3.2 Noun Phrases,[0],[0]
2We use a morphological analyzer to distinguish between the conjunction and quotative particles.,3.2 Noun Phrases,[0],[0]
"Examples of words marked by this particle include 見られる (expect), 言う (say), 思われる (seem), する (assume), 信じる (believe) and so on.
",3.2 Noun Phrases,[0],[0]
3We also include the phrase It looks like.,3.2 Noun Phrases,[0],[0]
"In Japanese, clausal conjunctions are often marked at the end of the initial clause of a compound sentence.",3.3 Conjunction Clause,[0],[0]
"In English, however, the order of clauses is more flexible.",3.3 Conjunction Clause,[0],[0]
We can therefore reduce delay by reordering the English clauses to mirror how they typically appear in Japanese.,3.3 Conjunction Clause,[0],[0]
"Below we describe rules reversing the order of clauses connected by these conjunctions:
• Clausal conjunctions: because (of), in order to • Contrastive conjunctions: despite, even though, although • Conditionals: (even) if, as a result (of) •",3.3 Conjunction Clause,[0],[0]
"Misc: according to
In standard Japanese, such conjunctions include no de, kara, de mo and so on.",3.3 Conjunction Clause,[0],[0]
"The sentence often appears in the form of S2 conj, S1.",3.3 Conjunction Clause,[0],[0]
"In English, however, two common constructions are
S1 conj S2: We should march because winter is coming.",3.3 Conjunction Clause,[0],[0]
"conj S2, S1: Because winter is coming, we should march.
",3.3 Conjunction Clause,[0],[0]
"To follow the Japanese clause order, we adapt the above two constructions to
S2, conj’ S1: Winter is coming, because of this, we should march.
",3.3 Conjunction Clause,[0],[0]
Here conj’ represents the original conjunction word appended with simple pronouns/phrases to refer to S2.,3.3 Conjunction Clause,[0],[0]
"For example, because → because of this, even if → even if this is the case.",3.3 Conjunction Clause,[0],[0]
We now turn our attention to the implementation of the syntactic transformation rules described above.,4 Sentence Rewriting Process,[0],[0]
"Applying a transformation consists of three steps:
1.",4 Sentence Rewriting Process,[0],[0]
Detection:,4 Sentence Rewriting Process,[0],[0]
Identify nodes in the parse tree for which the transformation is applicable; 2.,4 Sentence Rewriting Process,[0],[0]
Modification: Transform nodes and labels; 3.,4 Sentence Rewriting Process,[0],[0]
"Evaluation: Compute delay reduction, and
decide whether to accept the rewritten sentence.
",4 Sentence Rewriting Process,[0],[0]
Figure 2 illustrates the process using passivization as an example.,4 Sentence Rewriting Process,[0],[0]
"In the detection step, we find the subtree that satisfies the condition of applying a rule.",4 Sentence Rewriting Process,[0],[0]
"In this case, we look for an S node whose children include an NP (denoted by NP1), the subject, and a VP to its right, such that the VP node has a leaf VB*, the main verb,4 followed by another NP (denoted by NP2), the object.",4 Sentence Rewriting Process,[0],[0]
We allow the parent nodes (S and VP) to have additional children besides the matched ones.,4 Sentence Rewriting Process,[0],[0]
They are not affected during the transformation.,4 Sentence Rewriting Process,[0],[0]
"In the modification step, we swap the subject node and object node; we add the verb be in its correct form by checking the tense of the verb and the form of NP2;5and we add the preposition by before the subject.",4 Sentence Rewriting Process,[0],[0]
"The process is executed recursively throughout the parse tree.
",4 Sentence Rewriting Process,[0],[0]
"4The main verb excludes be and have when it indicates tense (e.g., have done).
",4 Sentence Rewriting Process,[0],[0]
"5We use the Nodebox linguistic library (https://www. nodebox.net/code) to detect and modify word forms.
",4 Sentence Rewriting Process,[0],[0]
"Although our rules are designed to minimize long range reordering, there are exceptions.6 Thus applying a rule does not always reduce delay.",4 Sentence Rewriting Process,[0],[0]
"In the evaluation step, we compare translation delay before and after applying the rule.",4 Sentence Rewriting Process,[0],[0]
"We accept a rewritten sentence if its delay is reduced; otherwise, we revert to the input sentence.",4 Sentence Rewriting Process,[0],[0]
"Since we do not segment sentences during rewriting, we must estimate the delay.
",4 Sentence Rewriting Process,[0],[0]
"To estimate the delay, we use word alignments.",4 Sentence Rewriting Process,[0],[0]
Figure 2c shows the source Japanese sentence in its word-for-word English translation and alignments from the target words to the source words.,4 Sentence Rewriting Process,[0],[0]
"The first English word, We, is aligned to the first Japanese word; it can thus be treated as an independent segment and translated immediately.",4 Sentence Rewriting Process,[0],[0]
"The second English word, love, is aligned to the last Japanese word, which means the system cannot start to translate until four more Japanese words are revealed.",4 Sentence Rewriting Process,[0],[0]
This alignment therefore forms a segment with delay of four words/seg.,4 Sentence Rewriting Process,[0],[0]
"Alignments of the following words come before the source word aligned to love; hence, they are already translated in the previous segment and we do not double count their delay.",4 Sentence Rewriting Process,[0],[0]
"In this example, the delay of the original sentence is 2.5 word/seg; after rewriting, it is reduced to 1.7 word/seg.",4 Sentence Rewriting Process,[0],[0]
"Therefore, we accept the rewritten sentence.",4 Sentence Rewriting Process,[0],[0]
"However, when the subject phrase is long and the object phrase is short, a swap may not reduce delay.
",4 Sentence Rewriting Process,[0],[0]
We can now formally define the delay.,4 Sentence Rewriting Process,[0],[0]
Let ei be the ith target word in the input sentence x and ai be the maximum index among indices of source words that ei aligned to.,4 Sentence Rewriting Process,[0],[0]
"We define the delay of ei as di = max(0, ai−maxj<i aj).",4 Sentence Rewriting Process,[0],[0]
The delay of x is then ∑N i=1,4 Sentence Rewriting Process,[0],[0]
"di/N , where the sum is over all aligned words except punctuation and stopwords.
",4 Sentence Rewriting Process,[0],[0]
"Given a set of rules, we need to decide which rules to apply and in what order.",4 Sentence Rewriting Process,[0],[0]
"Fortunately, our rules have little interaction with each other, and the order of application has a negligible effect.",4 Sentence Rewriting Process,[0],[0]
"We apply the rules, roughly, sequentially in order of complexity: if the output of current rule is not accepted, the sentence is reverted to the last accepted version.",4 Sentence Rewriting Process,[0],[0]
"We evaluate our method on the Reuters JapaneseEnglish corpus of news articles (Utiyama and Isahara, 2003).",5 Experiments,[0],[0]
"For training the MT system, we also include the EIJIRO dictionary entries and the accompanying example sentences.7 Statistics of the dataset are shown in Table 1.",5 Experiments,[0],[0]
"The rewritten translation is generally slightly longer than the gold translation because our rewriting often involves inserting pronouns (e.g. it, this) for antecedents.
",5 Experiments,[0],[0]
"We use the TreebankWordTokenizer from NLTK (Bird et al., 2009) to tokenize English sentences and Kuromoji Japanese morphological analyzer8 to tokenize Japanese sentences.",5 Experiments,[0],[0]
"Our phrase-based MT system is trained by Moses (Koehn et al., 2003) with standard parameters settings.",5 Experiments,[0],[0]
We use GIZA++,5 Experiments,[0],[0]
"(Och and Ney, 2003) for word alignment and k-best batch MIRA (Cherry and Foster, 2012) for tuning.",5 Experiments,[0],[0]
"The translation quality is evaluated by BLEU (Papineni et al., 2002) and RIBES (Isozaki et al., 2010).9 To obtain the parse trees for English sentences, we use the Stanford Parser (Klein and Manning, 2003) and the included English model.",5 Experiments,[0],[0]
"After applying the rewriting rules (Section 4), Table 2 shows the percentage of sentences that are candidates and how many rewrites are accepted.",5.1 Quality of Rewritten Translations,[0],[0]
The most generalizable rules are passivization and delaying quotative verbs.,5.1 Quality of Rewritten Translations,[0],[0]
"We rewrite 32.2% of sentences, reducing the delay from 9.9 words/seg to 6.3 words/seg per segment for rewritten sentences and from 7.8 words/seg to 6.7 words/seg overall.
",5.1 Quality of Rewritten Translations,[0],[0]
"6For example, in clause transformation, the Japanese conjunction moshi, which is clause initial, may appear at the beginning of a sentence to emphasize conditionals, although its appearance is relatively rare.
",5.1 Quality of Rewritten Translations,[0],[0]
"7Available at http://eijiro.jp 8Available at http://www.atilika.org/ 9In contrast to BLEU, RIBES is an order-sensitive metric commonly used for translation between Japanese and English.
",5.1 Quality of Rewritten Translations,[0],[0]
We evaluate the quality of our rewritten sentences from two perspectives: grammaticality and preserved semantics.,5.1 Quality of Rewritten Translations,[0],[0]
"To examine how close the rewritten sentences are to standard English, we train a 5-gram language model using the English data from the Europarl corpus, consisting of 46 million words, and use it to compute perplexity.",5.1 Quality of Rewritten Translations,[0],[0]
Rewriting references increases the perplexity under the language model only slightly: from 332.0 to 335.4.,5.1 Quality of Rewritten Translations,[0],[0]
"To ensure that rewrites leave meaning unchanged, we use the SEMAFOR semantic role labeler (Das et al., 2014) on the original and modified sentence; for each role-labeled token in the reference sentence, we examine its corresponding role in the rewritten sentence and calculate the average accuracy acrosss all sentences.",5.1 Quality of Rewritten Translations,[0],[0]
"Even ignoring benign lexical changes—for example, he becoming him in a passivized sentence—95.5% of the words retain their semantic roles in the rewritten sentences.
",5.1 Quality of Rewritten Translations,[0],[0]
"Although our rules are conservative to minimize corruption, some errors are unavoidable propagation of parser errors.",5.1 Quality of Rewritten Translations,[0],[0]
"For example, the sentence the London Stock Exchange closes at 1230 GMT today is parsed as:10 (S (NP the London Stock Exchange) (VP (VBZ closes)
(PP at 1230) (NP GMT today)))
",5.1 Quality of Rewritten Translations,[0],[0]
GMT today is separated from the PP as an NP and is mistaken as the object.,5.1 Quality of Rewritten Translations,[0],[0]
The passive version is then GMT today is closed at 1230 by the London Stock Exchange.,5.1 Quality of Rewritten Translations,[0],[0]
"Such errors could be reduced by skipping nodes with low inside/outside scores given by the parser, or skipping low-frequency patterns.",5.1 Quality of Rewritten Translations,[0],[0]
"However, we leave this for future work.",5.1 Quality of Rewritten Translations,[0],[0]
"At test time, we use right probability (Fujita et al., 2013, RP) to decide when to start translating a
10For simplicity we show the shallow parse only.
sentence.",5.2 Segmentation,[0],[0]
"As we read in the source Japanese sentence, if the input segment matches an entry in the learned phrase table, we query the RP of the Japanese/English phrase pair.",5.2 Segmentation,[0],[0]
A higher RP indicates that the English translation of this Japanese phrase will likely be followed by the translation of the next Japanese phrase.,5.2 Segmentation,[0],[0]
"In other words, translation of the two consecutive Japanese phrases is monotonic, thus, we can begin translating immediately.",5.2 Segmentation,[0],[0]
"Following (Fujita et al., 2013), if the RP of the current phrase is lower than a fixed threshold, we cache the current phrase and wait for more words from the source sentence; otherwise, we translate all cached phrases.",5.2 Segmentation,[0],[0]
"Finally, translations of segments are concatenated to form a complete translation of the input sentence.",5.2 Segmentation,[0],[0]
"To show the effect of rewritten references, we compare the following MT systems:
• GD: only gold reference translations; • RW: only rewritten reference translations; • RW+GD: both gold and the rewritten refer-
ences; and • RW-LM+GD: using gold reference transla-
tions but using the rewritten references for training the LM and for tuning.
",5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD and RW-LM+GD, we interpolate the language models of GD and RW.",5.3 Speed/Accuracy Trade-off,[0],[0]
The interpolating weight is tuned with the rewritten sentences.,5.3 Speed/Accuracy Trade-off,[0],[0]
"For RW+GD, we combine the translation models (phrase tables and reordering tables) of RW and GD by fill-up combination (Bisazza et al., 2011), where all entries in the tables of RW are preserved and entries from the tables of GD are added if new.
Increasing the RP threshold increases interpretation delay but improves the quality of the translation.",5.3 Speed/Accuracy Trade-off,[0],[0]
"We set the RP threshold at 0.0, 0.2, 0.4, 0.8 and finally 1.0 (equivalent to batch translation).",5.3 Speed/Accuracy Trade-off,[0],[0]
Figure 3 shows the BLEU/RIBES scores vs. the number of words per segement as we increase the threshold.,5.3 Speed/Accuracy Trade-off,[0],[0]
Rewritten sentences alone do not significantly improve over the baseline.,5.3 Speed/Accuracy Trade-off,[0],[0]
"We suspect this is because the transformation rules sometimes generate ungrammatical sentences due to parsing errors, which impairs learning.",5.3 Speed/Accuracy Trade-off,[0],[0]
"However, combining RW and GD results in a better speed-accuracy tradeoff: the RW+GD curve completely dominates other curves in Figure 3a, 3c.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Thus, using more monotone translations improves simultaneous machine translation, and because RW-LM+GD is about
0 5 10 15 20 25 30 35
Average # of words per segment
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(a) BLEU w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD RW-LM+GD RW GD
(b) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"gold ref
0 5 10 15 20 25 30 35
Average # of words per segment
10
11
12
13
14
15
16
17
18
B LE
U
RW+GD RW-LM+GD RW GD
(c) BLEU",5.3 Speed/Accuracy Trade-off,[0],[0]
w.r.t.,5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
0 5 10 15 20 25 30 35
Average # of words per segment
59.5
60.0
60.5
61.0
61.5
62.0
62.5
R IB
E S
RW+GD",5.3 Speed/Accuracy Trade-off,[0],[0]
"RW-LM+GD RW GD
(d) RIBES w.r.t.",5.3 Speed/Accuracy Trade-off,[0],[0]
"rewritten ref
Figure 3: Speed/accuracy tradeoff curves: BLEU (left) /",5.3 Speed/Accuracy Trade-off,[0],[0]
"RIBES (right) versus translation delay (average number of words per segment).
",5.3 Speed/Accuracy Trade-off,[0],[0]
"the same as GD, the major improvement likely comes from the translation model from rewritten sentences.
",5.3 Speed/Accuracy Trade-off,[0],[0]
The right two plots recapitulate the evaluation with the RIBES metric.,5.3 Speed/Accuracy Trade-off,[0],[0]
"This result is less clear, as MT systems are optimized for BLEU and RIBES penalizes word reordering, making it difficult to compare systems that intentionally change word order.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Nevertheless, RW is comparable to GD on gold references and superior to the baseline on rewritten references.",5.3 Speed/Accuracy Trade-off,[0],[0]
"Rewriting training data not only creates lower latency simultaneous translations, but it also improves batch translation.",5.4 Effect on Verbs,[0],[0]
One reason is that SOV to SVO translation often drops the verb because of long range reordering.,5.4 Effect on Verbs,[0],[0]
"(We see this for Japanese here, but this is also true for German.)",5.4 Effect on Verbs,[0],[0]
"Similar word orders in the source and target results in less reordering and improves phrase-based MT (Collins
et al., 2005; Xu et al., 2009).",5.4 Effect on Verbs,[0],[0]
"Table 3 shows the number of verbs in the translations of the test sentences produced by GD, RW, RW+GD, as well as the number in the gold reference translation.",5.4 Effect on Verbs,[0],[0]
"Both RW and RW+GD produce more verbs (a statistically significant result), although RW+GD captures the most verbs.",5.4 Effect on Verbs,[0],[0]
Table 4 compares translations by GD and RW.,5.5 Error Analysis,[0],[0]
"RW correctly puts the verb said at the end, while GD drops the final verb.",5.5 Error Analysis,[0],[0]
"However, RW still produces he at the beginning (also the first word in the Japanese source sentence).",5.5 Error Analysis,[0],[0]
This is because our current segmentation strategy do not preserve words for later translation—a note-taking strategy used by human interpreters.,5.5 Error Analysis,[0],[0]
Previous approaches to simultaneous machine translation have employed explicit interpretation strategies for coping with delay.,6 Related Work,[0],[0]
"Two major approaches are segmentation and prediction.
",6 Related Work,[0],[0]
"Most segmentation strategies are based on heuristics, such as pauses in speech (Fügen et al., 2007; Bangalore et al., 2009), comma prediction (Sridhar et al., 2013) and phrase reordering probability (Fujita et al., 2013).",6 Related Work,[0],[0]
Learning-based methods have also been proposed.,6 Related Work,[0],[0]
Oda et al. (2014) find segmentations that maximize the BLEU score of the final concatenated translation by dynamic programming.,6 Related Work,[0],[0]
Grissom II et al. (2014) formulate simultaneous translation as a sequential decision making problem and uses reinforcement learning to decide when to translate.,6 Related Work,[0],[0]
"One limitation of these methods is that when learning with standard batch MT corpus, their gain can be restricted by natural word reordering between the source and the target sentences, as explained in Section 1.
",6 Related Work,[0],[0]
"In an SOV-SVO context, methods to predict unseen words are proposed to alleviate the above restriction.",6 Related Work,[0],[0]
Matsubara et al. (1999) predict the English verb in the target sentence and integrates it syntactically.,6 Related Work,[0],[0]
"Grissom II et al. (2014) predict the final verb in the source sentence and decide when to use the predicted verb with reinforcement learning.
",6 Related Work,[0],[0]
"Nevertheless, unless the predictor considers contextual and background information, which human interpreters often rely on for prediction (Hönig, 1997; Camayd-Freixas, 2011), such a prediction task is inherently hard.
",6 Related Work,[0],[0]
"Unlike previous approaches to simultaneous translation, we directly adapt the training data and transform a translated sentence to an “interpreted” one.",6 Related Work,[0],[0]
"We can, therefore, take advantage of the abundance of parallel batch-translated corpora for training a simultaneous MT system.",6 Related Work,[0],[0]
"In addition, as a data preprocessing step, our approach is orthogonal to the others, with which it can be easily combined.
",6 Related Work,[0],[0]
"This work is also related to preprocessing reordering approaches (Xu et al., 2009; Collins et al., 2005; Galley and Manning, 2008; Hoshino et al., 2013; Hoshino et al., 2014) in batch MT for language pairs with substantially different word orders.",6 Related Work,[0],[0]
"However, our problem is different in several ways.",6 Related Work,[0],[0]
"First, while the approaches resemble each other, our motivation is to reduce translation delay.",6 Related Work,[0],[0]
"Second, they reorder the source sentence, which is nontrivial and time-consuming when the sentence is incrementally revealed.",6 Related Work,[0],[0]
"Third, rewriting the target sentence requires the output to be grammatical (for it to be used as reference translation), which is not a concern when rewriting source sentences.",6 Related Work,[0],[0]
Training MT systems with more monotonic (interpretation-like) sentences improves the speedaccuracy tradeoff for simultaneous machine translation.,7 Conclusion,[0],[0]
"By designing syntactic transformations and rewriting batch translations into more monotonic translations, we reduce the translation delay.",7 Conclusion,[0],[0]
"MT systems trained on the rewritten reference translations learn interpretation strategies implicitly from the data.
",7 Conclusion,[0],[0]
Our rewrites are based on linguistic knowledge and inspired by techniques used by human interpreters.,7 Conclusion,[0],[0]
"They cover a wide range of reordering phenomena between Japanese and English, and more generally, between SOV and SVO languages.",7 Conclusion,[0],[0]
A natural extension is to automatically extract such rules from parallel corpora.,7 Conclusion,[0],[0]
"While there exist approaches that extract syntactic tree transformation rules automatically, one of the difficulties is that most parallel corpora is dominated by lexical paraphrasing instead of syntactic paraphrasing.",7 Conclusion,[0],[0]
This work was supported by NSF grant IIS1320538.,Acknowledgments,[0],[0]
Boyd-Graber is also partially supported by NSF grants CCF-1409287 and NCSE-1422492.,Acknowledgments,[0],[0]
Daumé III,Acknowledgments,[0],[0]
and He are also partially supported by NSF grant IIS-0964681.,Acknowledgments,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor.",Acknowledgments,[0],[0]
Divergent word order between languages causes delay in simultaneous machine translation.,abstractText,[0],[0]
We present a sentence rewriting method that generates more monotonic translations to improve the speedaccuracy tradeoff.,abstractText,[0],[0]
We design grammaticality and meaning-preserving syntactic transformation rules that operate on constituent parse trees.,abstractText,[0],[0]
We apply the rules to reference translations to make their word order closer to the source language word order.,abstractText,[0],[0]
"On Japanese-English translation (two languages with substantially different structure), incorporating the rewritten, more monotonic reference translation into a phrase-based machine translation system enables better translations faster than a baseline system that only uses gold reference translations.",abstractText,[0],[0]
Syntax-based Rewriting for Simultaneous Machine Translation,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1325–1337 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
1325",text,[0],[0]
Dependency parsing is a core task in natural language processing (NLP).,1 Introduction,[0],[0]
"Given a sentence, a dependency parser produces a dependency tree, which specifies the typed head-modifier relations between pairs of words.",1 Introduction,[0],[0]
"While supervised dependency parsing has been successful (McDonald and Pereira, 2006; Nivre, 2008; Kiperwasser and Goldberg, 2016), unsupervised parsing can hardly produce useful parses (Mareček, 2016).",1 Introduction,[0],[0]
So it is extremely helpful to have some treebank of supervised parses for training purposes.,1 Introduction,[0],[0]
"Unfortunately, manually constructing a treebank for a new target language is expensive (Böhmová et al., 2003).",1.1 Past work: Cross-lingual transfer,[0],[0]
"As an alternative, cross-lingual transfer parsing (McDonald et al., 2011) is sometimes possible, thanks to the recent development of multi-lingual treebanks (McDonald et al., 2013; Nivre et al., 2015; Nivre et al., 2017).",1.1 Past work: Cross-lingual transfer,[0],[0]
The idea is to parse the sentences of the target language with a supervised parser trained on the treebanks of one or more source languages.,1.1 Past work: Cross-lingual transfer,[0],[0]
"Although the parser cannot be expected to know the words of the target language, it can make do with parts of
speech (POS) (McDonald et al., 2011; Täckström",1.1 Past work: Cross-lingual transfer,[0],[0]
"et al., 2013; Zhang and Barzilay, 2015) or crosslingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016).",1.1 Past work: Cross-lingual transfer,[0],[0]
"A more serious challenge is that the parser may not know how to handle the word order of the target language, unless the source treebank comes from a closely related language (e.g., using German to parse Luxembourgish).",1.1 Past work: Cross-lingual transfer,[0],[0]
"Training the parser on trees from multiple source languages may mitigate this issue (McDonald et al., 2011) because the parser is more likely to have seen target part-of-speech sequences somewhere in the training data.",1.1 Past work: Cross-lingual transfer,[0],[0]
"Some authors (Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016) have shown additional improvements by preferring source languages that are “close” to the target language, where the closeness is measured by distance between POS language models trained on the source and target corpora.",1.1 Past work: Cross-lingual transfer,[0],[0]
"We will focus on delexicalized dependency parsing, which maps an input POS tag sequence to a dependency tree.",1.2 This paper: Tailored synthetic data,[0],[0]
"We evaluate single-source transfer—train a parser on a single source language, and evaluate it on the target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"This is the setup of Zeman and Resnik (2008) and Søgaard (2011a).
",1.2 This paper: Tailored synthetic data,[0],[0]
"Our novel ingredient is that rather than seek a close source language that already exists, we create one.",1.2 This paper: Tailored synthetic data,[0],[0]
How?,1.2 This paper: Tailored synthetic data,[0],[0]
"Given a dependency treebank of a possibly distant source language, we stochastically permute the children of each node, according to some distribution that makes the permuted language close to the target language.
",1.2 This paper: Tailored synthetic data,[0],[0]
And how do we find this distribution?,1.2 This paper: Tailored synthetic data,[0],[0]
We adopt the tree-permutation model of Wang and Eisner (2016).,1.2 This paper: Tailored synthetic data,[0],[0]
"We design a dynamic programming algorithm which, for any given distribution p in Wang and Eisner’s family, can compute the expected counts of all POS bigrams in the permuted source treebank.",1.2 This paper: Tailored synthetic data,[0],[0]
"This allows us to evaluate p by computing the divergence between the bigram POS language model formed by these expected counts,
and the one formed by the observed counts of POS bigrams in the unparsed target language.",1.2 This paper: Tailored synthetic data,[0],[0]
"In order to find a p that locally minimizes this divergence, we adjust the model parameters by stochastic gradient descent (SGD).",1.2 This paper: Tailored synthetic data,[0],[0]
Better measures of surface closeness between two languages might be devised.,1.3 Key limitations in this paper,[0],[0]
"However, even counting the expected POS N -grams is moderately expensive, taking time exponential in N if done exactly.",1.3 Key limitations in this paper,[0],[0]
"So we compute only these local statistics, and only for N = 2.",1.3 Key limitations in this paper,[0],[0]
We certainly need N > 1 because the 1-gram distribution is not affected by permutation at all.,1.3 Key limitations in this paper,[0],[0]
"N = 2 captures useful bigram statistics: for example, to mimic a verb-final language with prenominal modifiers, we would seek constituent permutations that result in matching its relatively high rate of VERB–PUNCT and ADJ–NOUN bigrams.",1.3 Key limitations in this paper,[0],[0]
"While N > 2 might have improved the results, it was too slow for our large-scale experimental design.",1.3 Key limitations in this paper,[0],[0]
"§7 discusses how richer measures could be used in the future.
",1.3 Key limitations in this paper,[0],[0]
"We caution that throughout this paper, we assume that our corpora are annotated with gold POS tags, even in the target language (which lacks any gold training trees).",1.3 Key limitations in this paper,[0],[0]
This is an idealized setting that has often been adopted in work on unsupervised and cross-lingual transfer.§7 discusses a possible avenue for doing without gold tags.,1.3 Key limitations in this paper,[0],[0]
We begin by motivating the idea of tree permutation.,2 Modeling Surface Realization,[0],[0]
Let us suppose that the dependency tree for a sentence starts as a labeled graph—a tree in which siblings are not yet ordered with respect to their parent or one another.,2 Modeling Surface Realization,[0],[0]
Each language has some systematic way to realize its unordered trees as surface strings:1 it imposes a particular order on the tree’s word tokens.,2 Modeling Surface Realization,[0],[0]
"More precisely, a language specifies a distribution p(string | unordered tree) over a tree’s possible realizations.
",2 Modeling Surface Realization,[0],[0]
"As an engineering matter, we now make the strong assumption that the unordered dependency trees are similar across languages.",2 Modeling Surface Realization,[0],[0]
"That is, we suppose that different languages use similar underlying syntactic/semantic graphs, but differ in how they realize this graph structure on the surface.
",2 Modeling Surface Realization,[0],[0]
"1Modeling this process was the topic of the recent Surface Realization Shared Task (Mille et al., 2018).",2 Modeling Surface Realization,[0],[0]
"Most relevant is work on tree linearization (Filippova and Strube, 2009; Futrell and Gibson, 2015; Puzikov and Gurevych, 2018).
",2 Modeling Surface Realization,[0],[0]
"Thus, given a gold POS corpus u of the unknown target language, we may hope to explain its distribution of surface POS bigrams as the result of applying some target-language surface realization model to the distribution of cross-linguistically “typical” unordered trees.",2 Modeling Surface Realization,[0],[0]
"To obtain samples of the latter distribution, we use the treebanks of one or more other languages.",2 Modeling Surface Realization,[0],[0]
The present paper evaluates our method when only a single source treebank is used.,2 Modeling Surface Realization,[0],[0]
"In the future, we could try tuning a mixture of all available source treebanks.",2 Modeling Surface Realization,[0],[0]
We presume that the target language applies the same stochastic realization model to all trees.,2.1 Realization is systematic,[0],[0]
All that we can optimize is the parameter vector of this model.,2.1 Realization is systematic,[0],[0]
"Thus, we deny ourselves the freedom to realize each individual tree in an ad hoc way.",2.1 Realization is systematic,[0],[0]
"To see why this is important, suppose the target language is French, whose corpus u contains many NOUN–ADJ bigrams.",2.1 Realization is systematic,[0],[0]
"We could achieve such a bigram from the unordered source tree
DET NOUN VERB PROPN ADJ
the cake made Sue sleepy
det nsubj dobj xcomp
by ordering
it to yield DET NOUN ADJ VERB PROPN the cake sleepy made Sue
det dobjxcomp nsubj
.",2.1 Realization is systematic,[0],[0]
"However, that realization is not in fact appropriate for French, so that ordered tree would not be a useful training tree for French.",2.1 Realization is systematic,[0],[0]
"Our approach should disprefer this tempting but incorrect realization, because any model with a high probability of this realization would, if applied systematically over the whole corpus, also yield sentences like He sleepy made Sue, with unwanted PRON–ADJ bigrams that would not match the surface statistics of French.",2.1 Realization is systematic,[0],[0]
"We hope our approach will instead choose the realization model that is correct for French, in which the NOUN–ADJ bigrams arise instead from source trees where the ADJ is a dependent of the NOUN, yielding (e.g.)
DET NOUN ADJ VERB PROPN the cake tasty pleased Sue
dobjdet amod nsubj
.",2.1 Realization is systematic,[0],[0]
"This has the same POS sequence as the example above (as it happens), but now assigns the correct tree to it.",2.1 Realization is systematic,[0],[0]
"As our family of realization distributions, we adopt the log-linear model used for this purpose by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
"The model assumes that the root node a of the unordered dependency tree selects an ordering π(a) of the na nodes consisting
of a and its na − 1 dependent children.",2.2 A parametric realization model,[0],[0]
The procedure is repeated recursively at the child nodes.,2.2 A parametric realization model,[0],[0]
"This method can produce only projective trees.
",2.2 A parametric realization model,[0],[0]
"Each node a draws its ordering π(a) independently according to
pθ(π | a) = 1
Z(a) exp ∑ 1≤i<j≤na θ · f(π, i, j) (1)
which is a distribution over the na! possible orderings.",2.2 A parametric realization model,[0],[0]
Z(a) is a normalizing constant.,2.2 A parametric realization model,[0],[0]
"f is a feature vector extracted from the ordered pair of nodes πi, πj , and θ is the model’s parameter vector of feature weights.",2.2 A parametric realization model,[0],[0]
"See Appendix A for the feature templates, which are a subset of those used by Wang and Eisner (2016).",2.2 A parametric realization model,[0],[0]
These features are able to examine the tree’s node labels (POS tags) and edge labels (dependency relations).,2.2 A parametric realization model,[0],[0]
"Thus, when a is a verb, the model can assign a positive weight to “subject precedes verb” or “subject precedes object,” thus preferring orderings with these features.
",2.2 A parametric realization model,[0],[0]
"Following Wang and Eisner (2016, §3.1), we choose new orderings for the noun and verb nodes only,2 preserving the source treebank’s order at all other nodes a.",2.2 A parametric realization model,[0],[0]
"Given a source treebank B and some parameters θ, we can use equation (1) to randomly sample realizations of the trees inB.",2.3 Generating training data,[0],[0]
The effect is to reorder dependent phrases within those trees.,2.3 Generating training data,[0],[0]
The resulting permuted treebank B′ can be used to train a parser for the target language.,2.3 Generating training data,[0],[0]
So how do we choose θ that works for the target language?,2.4 Choosing parameters θ,[0],[0]
"Suppose u is a corpus of targetlanguage POS sequences, using the same set of POS tags as B. We evaluate parameters θ according to whether POS tag sequences in B′ will be distributed like POS tag sequences in u.
To do this, first we estimate a bigram language model q̂ from the actual distribution q of POS sequences observed in u. Second, let pθ denote the distribution of POS sequences that we expect to see in B′, that is, POS sequences obtained by
2Specifically, the 93% of nodes tagged with NOUN, PROPN, PRON or VERB in Universal Dependencies format.",2.4 Choosing parameters θ,[0],[0]
"In retrospect, this restriction was unnecessary in our setting, but it skipped only 4.4% of nodes on average (from 2% to 11% depending on language).",2.4 Choosing parameters θ,[0],[0]
"The remaining nodes were nouns, verbs, or childless.
stochastically realizing observed trees in B according to θ.",2.4 Choosing parameters θ,[0],[0]
"We estimate another bigram model p̂θ from this distribution pθ.
",2.4 Choosing parameters θ,[0],[0]
"We then try to set θ, using SGD, to minimize a divergence D(p̂θ, q̂) that we will define below.",2.4 Choosing parameters θ,[0],[0]
"Estimating q̂ is straightforward: q̂(t | s) = cq(st)/cq(s), where cq(st) is the count of POS bigram st in the average3 sentence of u and cq(s) =∑
t′ cq(st ′).",2.4.1 Estimation of bigram models,[0],[0]
"We estimate p̂θ in the same way, where cp(st) denotes the expected count of st in a random POS sequence y ∼ pθ.",2.4.1 Estimation of bigram models,[0],[0]
"This is equivalent to choosing q̂, p̂θ to minimize the KL-divergences KL(q ||",2.4.1 Estimation of bigram models,[0],[0]
"q̂),KL(pθ || p̂θ).",2.4.1 Estimation of bigram models,[0],[0]
"It ensures that each model’s expected bigram counts match those in the POS sequences.
",2.4.1 Estimation of bigram models,[0],[0]
"However, these maximum-likelihood estimates might overfit on our finite data, u and B. We therefore smooth both models by first adding λ = 0.1 to all bigram counts cq(st) and cp(st).4",2.4.1 Estimation of bigram models,[0],[0]
We need a metric to evaluate θ.,2.4.2 Divergence of bigram models,[0],[0]
"If p and q are bigram language models over POS sequences y (sentences), their Kullback-Leibler divergence is
KL(p || q) def=",2.4.2 Divergence of bigram models,[0],[0]
"Ey∼p[log p(y)− log q(y)] (2) = ∑ s,t cp(st) (3)
· (log p(t | s)− log q(t | s))
where y ranges over POS sequences and st ranges over POS bigrams.",2.4.2 Divergence of bigram models,[0],[0]
"These include bigrams where s = BOS (“beginning of sequence”) or t = EOS (“end of sequence”), which are boundary tags that we take to surround y.
All quantities in equation (3) can be determined directly from the (expected) bigram counts given by cp and cq.",2.4.2 Divergence of bigram models,[0],[0]
"No other model estimation is needed.
",2.4.2 Divergence of bigram models,[0],[0]
A concern about equation (3) is that a single bigram st that is badly underrepresented in q may contribute an arbitrarily large term log p(t|s)q(t|s) .,2.4.2 Divergence of bigram models,[0],[0]
"To limit this contribution to at most log 1α , for some small α ∈ (0, 1), we define KLα(p || q) by a variant of equation (3) in which q(t | s) has been replaced by q̃(t | s) def= αp(t",2.4.2 Divergence of bigram models,[0],[0]
"| s) + (1− α)q(t | s).5
3A more familiar definition of cq would use the total count in u. Our definition, which yields the same bigram probabilities, is analogous to our definition of cp.",2.4.2 Divergence of bigram models,[0],[0]
"This cp is needed for KL(p || q) in (3), and cq symmetrically for KL(q || p).
",2.4.2 Divergence of bigram models,[0],[0]
"4Ideally one should tune λ to minimize the language model perplexity on held-out data (e.g., by cross-validation).
",2.4.2 Divergence of bigram models,[0],[0]
"5This is inspired by the α-skew divergence of Lee (1999,
Our final divergence metric D(p̂θ, q̂) defines D as a linear combination of exclusive and inclusive KLα divergences, which respectively emphasize pθ’s precision and recall at matching q’s bigrams:
D(p, q) =",2.4.2 Divergence of bigram models,[0],[0]
(1−β)·KLα1(p || q) Ey∼p[ |y| ] +β·KLα2(q ||,2.4.2 Divergence of bigram models,[0],[0]
"p) Ey∼q[ |y| ] (4) where β, α1, α2 are tuned by cross-validation to maximize the downstream parsing performance.",2.4.2 Divergence of bigram models,[0],[0]
"The division by average sentence length converts KL from nats per sentence to nats per word,6 so that the KL values have comparable scale even if B has much longer or shorter sentences than u.",2.4.2 Divergence of bigram models,[0],[0]
"We now present a polynomial-time algorithm for computing the expected bigram counts cp under pθ (or equivalently p̂θ), for use above.",3.1 Efficiently computing expected counts,[0],[0]
"This averages expected counts from each unordered tree x ∈ B. Algorithm 1 in the supplement gives pseudocode.
",3.1 Efficiently computing expected counts,[0],[0]
"The insight is that rather than sampling a single realization of x (as B′ does), we can use dynamic programming to sum efficiently over all of its exponentially many realizations.",3.1 Efficiently computing expected counts,[0],[0]
This gives an exact answer.,3.1 Efficiently computing expected counts,[0],[0]
"It algorithmically resembles tree-to-string machine translation, which likewise considers the possible reorderings of a source tree and incorporates a language model by similarly tracking their surface N -grams (Chiang, 2007, §5.3.2).
",3.1 Efficiently computing expected counts,[0],[0]
"For each node a of the tree x, let the POS string ya be the realization of the subtree rooted at a. Let ca(st) be the expected count of bigram st in ya, whose distribution is governed by equation (1).",3.1 Efficiently computing expected counts,[0],[0]
"We allow s = BOS or t = EOS as defined in §2.4.2.
",3.1 Efficiently computing expected counts,[0],[0]
The ca function can be represented as a sparse map from POS bigrams to reals.,3.1 Efficiently computing expected counts,[0],[0]
We compute ca at each node a of x in a bottom-up order.,3.1 Efficiently computing expected counts,[0],[0]
"The final step computes croot, giving the expected bigram counts in x’s realization y",3.1 Efficiently computing expected counts,[0],[0]
"(that is, cp in §2.4).
",3.1 Efficiently computing expected counts,[0],[0]
We find ca as follows.,3.1 Efficiently computing expected counts,[0],[0]
"Let n = na and recall from §2.2 that π(a) is an ordering of a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an, where a1, . . .",3.1 Efficiently computing expected counts,[0],[0]
", an−1 are the child nodes of a, and an is a dummy node representing a’s head token.
2001).",3.1 Efficiently computing expected counts,[0],[0]
"Indeed, we may regard KLα(p || q) as the α-skew divergence between the unigram distributions p(· | s) and q(· | s), averaged over all s in proportion to cp(s).",3.1 Efficiently computing expected counts,[0],[0]
"In principle, we could have used the α-skew divergence between the distributions p(·) and q(·) over POS sequences y, but computing that would have required a sampling-based approximation (§7).
6Recall that the units of negated log-probability are called bits for log base 2, but nats for log base e.
Also, let a0 and an+1 be dummy nodes that always appear at the start and end of any ordering.
",3.1 Efficiently computing expected counts,[0],[0]
For all 0 ≤,3.1 Efficiently computing expected counts,[0],[0]
i ≤ n,3.1 Efficiently computing expected counts,[0],[0]
and 1 ≤ j ≤ n,3.1 Efficiently computing expected counts,[0],[0]
"+ 1, let pa(i, j) denote the expected count of the aiaj node bigram—the probability that π(a) places node ai immediately before node aj .",3.1 Efficiently computing expected counts,[0],[0]
"These node bigram probabilities can be obtained by enumerating all possible orderings π, a matter we return to below.
",3.1 Efficiently computing expected counts,[0],[0]
"It is now easy to compute ca:
ca(st) =",3.1 Efficiently computing expected counts,[0],[0]
"c within a (st) + c between a (st) (5)
",3.1 Efficiently computing expected counts,[0],[0]
"cwithina (st) =
{∑n i=1 cai(st)",3.1 Efficiently computing expected counts,[0],[0]
"if s 6= BOS, t 6=",3.1 Efficiently computing expected counts,[0],[0]
"EOS
0 otherwise
cacrossa (st) = n∑ i=0 n+1∑ j=1 pa(i, j)cai(s EOS)caj (BOS t)
",3.1 Efficiently computing expected counts,[0],[0]
"That is, ca inherits all non-boundary bigrams st that fall within its child constituents (via cwithina ).",3.1 Efficiently computing expected counts,[0],[0]
"It also counts bigrams st that cross the boundary between consecutive nodes (via cacrossa ), where nodes ai and aj are consecutive with probability pa(i, j).
",3.1 Efficiently computing expected counts,[0],[0]
"When computing ca via (5), we will have already computed ca1 , . . .",3.1 Efficiently computing expected counts,[0],[0]
", can−1 bottom-up.",3.1 Efficiently computing expected counts,[0],[0]
"As for the dummy nodes, an is realized by the length-1 string hwhere h is the head token of node a, while a0 and an+1 are each realized by the empty string.",3.1 Efficiently computing expected counts,[0],[0]
"Thus, can simply assigns count 1 to the bigrams BOS h and h EOS, and ca0 and can+1 each assign expected count 1 to BOS EOS.",3.1 Efficiently computing expected counts,[0],[0]
"(Notice that thus, cacrossa (st) counts ya’s boundary bigrams—the bigrams stwhere s = BOS or t = EOS—when i = 0 or j = n+ 1 respectively.)",3.1 Efficiently computing expected counts,[0],[0]
"The main challenge above is computing the node bigram probabilities pa(i, j).",3.2 Efficient enumeration over permutations,[0],[0]
"These are marginals of p(π | a) as defined by (1), which unfortunately is intractable to marginalize: there is no better way than enumerating all n! permutations.
",3.2 Efficient enumeration over permutations,[0],[0]
"That said, there is a particularly efficient way to enumerate the permutations.",3.2 Efficient enumeration over permutations,[0],[0]
"The SteinhausJohnson-Trotter (SJT) algorithm (Sedgewick, 1977) does so in O(1) time per permutation, obtaining each permutation by applying a single swap to the previous one.",3.2 Efficient enumeration over permutations,[0],[0]
Only the features that are affected by this swap need to be recomputed.,3.2 Efficient enumeration over permutations,[0],[0]
"For our features (Appendix A), this cuts the runtime per permutation from O(n2) to O(n).
",3.2 Efficient enumeration over permutations,[0],[0]
"Furthermore, the single swap of adjacent nodes only changes 3 bigrams (possibly including boundary bigrams).",3.2 Efficient enumeration over permutations,[0],[0]
"As a result, it is possible to
obtain the marginal probabilities with O(1) additional work per permutation.",3.2 Efficient enumeration over permutations,[0],[0]
"When a node bigram is destroyed, we increment its marginal probability by the total probability of permutations encountered since the node bigram was last created.",3.2 Efficient enumeration over permutations,[0],[0]
This can be found as a difference of partial sums.,3.2 Efficient enumeration over permutations,[0],[0]
"The final partial sum is the normalizing constant Z(a), which can be applied at the end.",3.2 Efficient enumeration over permutations,[0],[0]
"Pseudocode is given in supplementary material as Algorithm 2.
",3.2 Efficient enumeration over permutations,[0],[0]
"When we train the parameters θ (§2.4), we must back-propagate through the whole computation of equation (4), which depends on tag bigram counts ca(st), which depend via (5) on expected node bigram counts pa(i, j), which depend via Algorithm 2 on the permutation probabilities p(π | a), which depend via (1) on the feature weights θ.",3.2 Efficient enumeration over permutations,[0],[0]
"As a further speedup, we only train on trees with number of words < 40 and maxa na ≤ 5, so na!",4.1 Pruning high-degree trees,[0],[0]
≤,4.1 Pruning high-degree trees,[0],[0]
120.7 We then produce the synthetic treebank B′,4.1 Pruning high-degree trees,[0],[0]
(§2.3) by drawing a single realization of each tree in B for which maxa na ≤ 7.,4.1 Pruning high-degree trees,[0],[0]
"This requires sampling from up to 7! = 5040 candidates per node, again using SJT.8
That is, in this paper we run exact algorithms (§3), but only on a subset of B. The subset is not necessarily representative.",4.1 Pruning high-degree trees,[0],[0]
"An improvement would use importance sampling, with a proposal distribution that samples the slower trees less often during SGD but upweights them to compensate.",4.1 Pruning high-degree trees,[0],[0]
"§7 suggests a future strategy that would run on all trees in B via approximate, sampling-based algorithms.",4.1 Pruning high-degree trees,[0],[0]
The exact methods would remain useful for calibrating the approximation quality.,4.1 Pruning high-degree trees,[0],[0]
"To minimize (4), we use the Adam variant of SGD (Kingma and Ba, 2014), with learning rate 0.01 chosen by cross-validation (§5.1).
",4.2 Minibatch estimation of cp,[0],[0]
SGD requires a stochastic estimate of the gradient of the training objective.,4.2 Minibatch estimation of cp,[0],[0]
"Ordinarily this is done by replacing an expectation over the entire training set with an expectation over a minibatch.
",4.2 Minibatch estimation of cp,[0],[0]
"7We found that this threshold worked much better than ≤ 4 and about as well as the much slower ≤ 6.
8This pruning heuristic retains 36.1% of the trees (averaging over the 20 development treebanks (§5.1)) for training, and 66.6% for actual realization.",4.2 Minibatch estimation of cp,[0],[0]
"The latter restriction follows Wang and Eisner (2016, §4.2): they too discarded trees with nodes having na ≥ 8.
",4.2 Minibatch estimation of cp,[0],[0]
Equation (2) with p = p̂θ is indeed an expectation over sentences of B. It can be stochastically estimated as (3) where cp gives the expected bigram counts averaged over only the sentences in a minibatch of B. These are found using §3’s algorithms with the current θ.,4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately, the term log p(t | s) depends on bigram counts that should be derived from the entire corpus B in the same way.",4.2 Minibatch estimation of cp,[0],[0]
Our solution is to simply reuse the minibatch estimate of cp for the latter counts.,4.2 Minibatch estimation of cp,[0],[0]
"We use a large minibatch of 500 sentences from B so that this drop-in estimate does not introduce too much bias into the stochastic gradient: after all, we only need to estimate bigram statistics on 17 POS types.9
By contrast, the cq values that are used for the expectation in the second term of (4) and in log q(t | s) do not change during optimization, so we simply compute them once from all of u.",4.2 Minibatch estimation of cp,[0],[0]
"Unfortunately the objective (4) is not convex, so the optimizer is sensitive to initialization (see §5.3 below for empirical discussion).",4.3 Informed initialization,[0],[0]
Initializing θ = 0,4.3 Informed initialization,[0],[0]
(so that p(π | a) is uniform),4.3 Informed initialization,[0],[0]
gave poor results in pilot experiments.,4.3 Informed initialization,[0],[0]
"Instead, we initially choose θ to be the realization parameters of the source language, as estimated from the source",4.3 Informed initialization,[0],[0]
treebank B.,4.3 Informed initialization,[0],[0]
"This is at least a linguistically realistic θ, although it may not be close to the target language.10
For this initial estimation, we follow Wang and Eisner (2016) and perform supervised training on B of the log-linear realization model (1), by maximizing the conditional log-likelihood of B, namely ∑ (x,t)∈B log pθ(t | x), where (x, t) are an unordered tree and its observed ordering in B. This initial objective is convex.11",4.3 Informed initialization,[0],[0]
We performed a large-scale experiment requiring hundreds of thousands of CPU-hours.,5 Experiments,[0],[0]
"To our knowledge, this is the largest study of parsing transfer yet attempted.
",5 Experiments,[0],[0]
"9We also used the minibatch to estimate the average sentence length Ey∼p[ |y| ] in (4), although here we could have simply used all of B since this value does not change.
",5 Experiments,[0],[0]
"10As an improvement, one could also try initial realization parameters for B that are estimated from treebanks of other languages.",5 Experiments,[0],[0]
"Concretely, the optimizer could start by selecting a “galactic” treebank from Wang and Eisner (2016) that is already close to the target language, according to (4), and try to make it even closer.",5 Experiments,[0],[0]
"We leave this to future work.
",5 Experiments,[0],[0]
"11Unfortunately, we did not regularize it, which probably resulted in initializing some parameters too close to ±∞ for the optimizer to change them meaningfully.",5 Experiments,[0],[0]
"As our main dataset, we use Universal Dependencies version 1.2 (Nivre et al., 2015)—a set of 37 dependency treebanks for 33 languages, with a unified POS-tag set and relation label set.
",5.1 Data and setup,[0],[0]
Our evaluation metric was unnormalized attachment score (UAS) when parsing a target treebank with a parser trained on a (possibly permuted) source treebank.,5.1 Data and setup,[0],[0]
"For both evaluation and training, we used only the training portion of each treebank.
",5.1 Data and setup,[0],[0]
"Our parser was Yara (Rasooli and Tetreault, 2015), a fast and accurate transition-based dependency parser that can be rapidly retrained.",5.1 Data and setup,[0],[0]
We modified Yara to ignore the input words and use only the input gold POS tags (see §1.3).,5.1 Data and setup,[0],[0]
"To train the Yara parser on a (possibly permuted) source treebank, we first train on 80% of the trees and use the remaining 20% to tune Yara’s hyperparameters.",5.1 Data and setup,[0],[0]
"We then retrain Yara on 100% of the source trees and evaluate it on the target treebank.
Similar to Wang and Eisner (2017), we use 20 treebanks (18 distinct languages) as development data, and hold out the remaining 17 treebanks for the final evaluation.",5.1 Data and setup,[0],[0]
"We chose the hyperparameters (α1, α2, β) of (4) to maximize the target-language UAS, averaged over all 376 transfer experiments where the source and target treebanks were development treebanks of different languages.12 (See Appendix C for details.)
",5.1 Data and setup,[0],[0]
The next few sections perform some exploratory analysis on these 376 experiments.,5.1 Data and setup,[0],[0]
"Then, for the final test in §5.4, we will evaluate UAS on all 337 transfer experiments where the source is a development treebank and the target is a test treebank of a different language.13",5.1 Data and setup,[0],[0]
We have assumed that a smaller divergence between source and target treebanks results in better transfer parsing accuracy.,5.2 Exploratory analysis,[0],[0]
"Figure 1 shows that these quantities are indeed correlated, both for the original source treebanks and for their “made to order” permuted versions.
",5.2 Exploratory analysis,[0],[0]
"12We have 19*20=380 pairs in total, minus the four excluded pairs (grc, grc proiel), (grc proiel, grc), (la proiel, la itt) and (la itt, la proiel).",5.2 Exploratory analysis,[0],[0]
"Unlike Wang and Eisner (2017), we exclude duplicated languages in development and testing.
",5.2 Exploratory analysis,[0],[0]
"13Specifically, there are 3 duplicated sets: {grc, grc proiel}, {la, la proiel, la itt}, and {fi, fi ftb}.",5.2 Exploratory analysis,[0],[0]
"Whenever one treebank is used as the target language, we exclude the other treebanks in the same set.
",5.2 Exploratory analysis,[0],[0]
"15According to the family (and sub-family) information at http://universaldependencies.org.
",5.2 Exploratory analysis,[0],[0]
"Thus, we hope that the optimizer will find a systematic permutation that reduces the divergence.",5.2 Exploratory analysis,[0],[0]
Does it?,5.2 Exploratory analysis,[0],[0]
"Yes: Figures 5 and 6 in the supplementary material show that the optimizer almost always manages to reduce the objective on training data, as expected.
",5.2 Exploratory analysis,[0],[0]
"One concern is that our divergence metric might misguide us into producing dysfunctional languages whose trees cannot be easily recovered from their surface strings, i.e., they have no good parser.",5.2 Exploratory analysis,[0],[0]
"In such a language, the word order might be extremely free (e.g., θ = 0), or common constructions might be syntactically ambiguous.",5.2 Exploratory analysis,[0],[0]
"Fortunately, Appendix D shows that our synthetic languages appear natural with respect to their their parsability.
",5.2 Exploratory analysis,[0],[0]
The above findings are promising.,5.2 Exploratory analysis,[0],[0]
So does permuting the source language in fact result in better transfer parsing of the target language?,5.2 Exploratory analysis,[0],[0]
"We experiment on the 376 development pairs.
",5.2 Exploratory analysis,[0],[0]
"The solid lines in Figure 2 show our improvements on the dev data, with a simpler scatterplot given by in Figure 7 in the supplementary material.",5.2 Exploratory analysis,[0],[0]
The upshot is that the synthetic source treebanks yield a transfer UAS of 52.92 on average.,5.2 Exploratory analysis,[0],[0]
This is not yet a result on held-out test data: recall that 52.92 was the best transfer UAS achieved by any hyperparameter setting.,5.2 Exploratory analysis,[0],[0]
"That said, it is 1.00 points better than transferring from the original source treebanks, a significant difference (paired permutation test by language pair, p < 0.01).
",5.2 Exploratory analysis,[0],[0]
"Figure 2 shows that this average improvement is mainly due to the many cases where the source and target languages come from different families.
",5.2 Exploratory analysis,[0],[0]
Permutation tends to improve source languages that were doing badly to start with.,5.2 Exploratory analysis,[0],[0]
"However, it tends to hurt a source language that is already in the target language family.
",5.2 Exploratory analysis,[0],[0]
A hypothetical experiment shows that permuting the source does have good potential to help (or at least not hurt) in both cases.,5.2 Exploratory analysis,[0],[0]
"The dashed lines in Figure 2—and the scatterplot in Figure 8— show the potential of the method, by showing the improvement we would get from permuting each source treebank using an “oracle” realization policy—the supervised realization parameters θ that are estimated from the actual target treebank.",5.2 Exploratory analysis,[0],[0]
"The usefulness of this oracle-permuted source varies depending on the source language, but it is usually much better than the automaticallypermuted version of the same source.
",5.2 Exploratory analysis,[0],[0]
This shows that large improvements would be possible if we could only find the best permutation policy allowed by our model family.,5.2 Exploratory analysis,[0],[0]
"The question for future work is whether such gains can be achieved by a more sensitive permutation model than (1), a better divergence objective than (4), or a better search algorithm than §4.2.",5.2 Exploratory analysis,[0],[0]
"Identifying the best available source treebank, or the best mixture of all source treebanks, would also help greatly.",5.2 Exploratory analysis,[0],[0]
Figure 2 makes clear that performance of the synthetic source treebanks is strongly correlated with that of their original versions.,5.3 Sensitivity to initializer,[0],[0]
Most points in Figure 7 lie near the diagonal (Kendall’s τ = 0.85).,5.3 Sensitivity to initializer,[0],[0]
"Even with oracle permutation in Figure 8, the correlation remains strong (τ = 0.59), suggesting that the choice of source treebank is important even beyond its effect on search initialization.
",5.3 Sensitivity to initializer,[0],[0]
"We suspected that when “made to order” source treebanks (more than the oracle versions) have performance close to their original versions, this is in part because the optimizer can get stuck near the initializer (§4.3).",5.3 Sensitivity to initializer,[0],[0]
"To examine this, we experimented with random restarts, as follows.",5.3 Sensitivity to initializer,[0],[0]
"In addition to informed initialization (§4.3), we optimized from 5 other starting points θ ∼ N (0, I).",5.3 Sensitivity to initializer,[0],[0]
"From these 6 runs, we selected the final parameters that achieved the best divergence (4).",5.3 Sensitivity to initializer,[0],[0]
"As shown by
Figure 9 in the supplement, greater gains appear to be possible with more aggressive search methods of this sort, which we leave to future work.",5.3 Sensitivity to initializer,[0],[0]
"We could also try non-random restarts based on the realization parameters of other languages, as suggested in footnote 10.",5.3 Sensitivity to initializer,[0],[0]
"For our final evaluation (§5.1), we use the same hyperparameters (Appendix C) and report on single-source transfer to the 17 held-out treebanks.
",5.4 Final evaluation on the test languages,[0],[0]
The development results hold up in Figure 3.,5.4 Final evaluation on the test languages,[0],[0]
"Using the synthetic languages yields 50.36 UAS on average—1.75 points over the baseline, which is significant (paired permutation test, p < 0.01).
",5.4 Final evaluation on the test languages,[0],[0]
"In the supplementary material (Appendix E), we include some auxiliary experiments on multisource transfer.",5.4 Final evaluation on the test languages,[0],[0]
"Unsupervised parsing has remained challenging for decades (Mareček, 2016).",6.1 Unsupervised parsing,[0],[0]
"Classical grammar induction approaches (Lari and Young, 1990; Carroll and Charniak, 1992; Klein and Manning, 2004; Headden III et al., 2009; Naseem et al., 2010) estimate a generative grammar to explain the sentences, for example by the ExpectationMaximization (EM) algorithm, and then use it to parse.",6.1 Unsupervised parsing,[0],[0]
Some such approaches try to improve the grammar model.,6.1 Unsupervised parsing,[0],[0]
"For example, Klein and Manning (2004)’s dependency model with valence was the first to beat a trivial baseline; later improvements considered higher-order effects and punctuation (Headden III et al., 2009; Spitkovsky et al., 2012).",6.1 Unsupervised parsing,[0],[0]
"Other approaches try to avoid search error, using strategies like convexified objectives (Wang et al., 2008; Gimpel and Smith, 2012), informed initialization (Klein and Manning, 2004; Mareček and Straka, 2013), search bias (Smith and Eisner, 2005, 2006; Naseem et al., 2010; Gillenwater et al., 2010), branch-and-bound search (Gormley and Eisner, 2013), and switching objectives (Spitkovsky et al., 2013).
",6.1 Unsupervised parsing,[0],[0]
"The alternative of cross-lingual transfer has recently flourished thanks to the development of consistent cross-lingual datasets of POS-tagged (Petrov et al., 2012) and dependency-parsed (McDonald et al., 2013) sentences.",6.1 Unsupervised parsing,[0],[0]
McDonald et al. (2011) showed a significant improvement over grammar induction by simply using the delexicalized parser trained on other language(s).,6.1 Unsupervised parsing,[0],[0]
"Subsequent improvements have come from re-weighting source languages (Søgaard, 2011b; Rosa and Žabokrtský, 2015a,b; Wang and Eisner, 2016), adapting the model to the target language using WALS (Dryer and Haspelmath, 2013) features (Naseem et al., 2012; Täckström",6.1 Unsupervised parsing,[0],[0]
"et al., 2013;
Zhang and Barzilay, 2015; Ammar et al., 2016), and improving the lexical representations via multilingual word embeddings (Duong et al., 2015; Guo et al., 2016; Ammar et al., 2016) and synthetic data generation (§6.2).",6.1 Unsupervised parsing,[0],[0]
Our novel proposal ties into the recent interest in data augmentation in supervised machine learning.,6.2 Synthetic data generation,[0],[0]
"In unsupervised parsing, the most widely
adopted synthetic data method has been annotation projection, which generates synthetic analyses of target-language sentences by “projecting” the analysis from a source-language translation.",6.2 Synthetic data generation,[0],[0]
"Of course, this requires bilingual corpora as an additional resource.",6.2 Synthetic data generation,[0],[0]
"Annotation projection was proposed by Yarowsky et al. (2001), gained promising results on sequence labelling tasks, and was later developed for unsupervised parsing (Hwa et al., 2005; Ganchev et al., 2009; Smith and Eisner, 2009; Tiedemann, 2014; Ma and Xia, 2014; Tiedemann et al., 2014).",6.2 Synthetic data generation,[0],[0]
"Recent work in this vein has mainly focused on improving the synthetic data, including reweighting the training trees (Agić et al., 2016) or pruning those that cannot be aligned well (Rasooli and Collins, 2015, 2017; Lacroix et al., 2016).
",6.2 Synthetic data generation,[0],[0]
"On the other hand, Wang and Eisner (2016) proposed to permute source language treebanks using word order realization models trained on other source languages.",6.2 Synthetic data generation,[0],[0]
"They generated on the order of 50,000 synthetic languages by “mixing and matching” a few dozen source languages.",6.2 Synthetic data generation,[0],[0]
"Their idea was that with a large set of synthetic languages, they could use them as supervised examples to train an unsupervised structure discovery system that could analyze any new language.",6.2 Synthetic data generation,[0],[0]
"Systems built with this dataset were competitive in single-source parser transfer (Wang and Eisner, 2016), typology
prediction (Wang and Eisner, 2017), and parsing unknown languages (Wang and Eisner, 2018).
",6.2 Synthetic data generation,[0],[0]
Our work in this paper differs in that our synthetic treebanks are “made to order.”,6.2 Synthetic data generation,[0],[0]
"Rather than combine aspects of different treebanks and hope to get at least one combination that is close to the target language, we “combine” the source treebank with a POS corpus of the target language, which guides our customized permutation of the source.
",6.2 Synthetic data generation,[0],[0]
"Beyond unsupervised parsing, synthetic data has been used for several other tasks.",6.2 Synthetic data generation,[0],[0]
"In NLP, it has been used for complex tasks such as questionanswering (QA) (Serban et al., 2016) and machine reading comprehension (Weston et al., 2016; Hermann et al., 2015; Rajpurkar et al., 2016), where highly expressive neural models are used and not enough real data is available to train them.",6.2 Synthetic data generation,[0],[0]
"In the playground of supervised parsing, Gulordava and Merlo (2016) conduct a controlled study on the parsibility of languages by generating treebanks with short dependency length and low variability of word order.",6.2 Synthetic data generation,[0],[0]
We have shown how cross-lingual transfer parsing can be improved by permuting the source treebank to better resemble the target language on the surface (in its distribution of gold POS bigrams).,7 Conclusion & Future Work,[0],[0]
The code is available at https://github. com/wddabc/ordersynthetic.,7 Conclusion & Future Work,[0],[0]
"Our work is grounded in the notion that by trying to explain the POS bigram counts in a target corpus, we can discover a stochastic realization policy for the target language, which correctly “translates” the source trees into appropriate target trees.
",7 Conclusion & Future Work,[0],[0]
"We formulated an objective for evaluating such a policy, based on KL-divergence between bigram models.",7 Conclusion & Future Work,[0],[0]
"We showed that the objective could be computed efficiently by dynamic programming, thanks to the limitation to bigram statistics.
",7 Conclusion & Future Work,[0],[0]
"Experimenting on the Universal Dependencies treebanks v1.2, we showed that the synthetic treebanks were—on average—modestly but significantly better than the corresponding real treebanks for single-source transfer (and in Appendix E, on multi-source transfer).
",7 Conclusion & Future Work,[0],[0]
"On the downside, Figure 7 shows that with our current method, permuting the source language to be more like the target language is helpful (on average) only when the source language is from a different language family.",7 Conclusion & Future Work,[0],[0]
"This contrast would be
even more striking if we had a better optimizer:
Figure 9 shows that SGD’s initialization bias limits permutation’s benefit for cross-family training, as well as its harm for within-family training.
",7 Conclusion & Future Work,[0],[0]
Several opportunities for future work have already been mentioned throughout the paper.,7 Conclusion & Future Work,[0],[0]
"We are also interested in experimenting with richer families of permutation distributions, as well as “conservative” distributions that tend to prefer the original source order.",7 Conclusion & Future Work,[0],[0]
"We could use entropy regularization (Grandvalet and Bengio, 2005) to encourage more “deterministic” patterns of realization in the synthetic languages.
",7 Conclusion & Future Work,[0],[0]
"We would also like to consider more sensitive divergence measures that go beyond bigrams, for example using recurrent neural network language models (RNNLMs) for q̂ and p̂θ.",7 Conclusion & Future Work,[0],[0]
"This means abandoning our exact dynamic programming methods; we would also like to abandon exact exhaustive enumeration in order to drop §4.1’s bounds on n. Fortunately, there exist powerful MCMC methods (Eisner and Tromble, 2006) that can sample from interesting distributions over the space of n!",7 Conclusion & Future Work,[0],[0]
"permutations, even for large n. Thus, we could approximately sample from pθ by drawing permuted versions of each tree in B.
Given this change, a very interesting direction would be to graduate from POS language models to word language models, using cross-lingual unsupervised word embeddings (Ruder et al., 2017).",7 Conclusion & Future Work,[0],[0]
This would eliminate the need for the gold POS tags that we unrealistically assumed in this paper (which are typically unavailable for a low-resource target language).,7 Conclusion & Future Work,[0],[0]
"Furthermore, it would enable us to harness richer lexical information beyond the 17 UD POS tags.",7 Conclusion & Future Work,[0],[0]
"After all, even a (gold) POS corpus might not be sufficient to determine the word order of the target language: “NOUN VERB NOUN” could be either subject-verb-object or object-verbsubject.",7 Conclusion & Future Work,[0],[0]
"However, “water drink boy” is presumably object-verb-subject.",7 Conclusion & Future Work,[0],[0]
"Thus, using crosslingual embeddings, we would try to realize the unordered source trees so that their word strings, with few edits, can achieve high probability under a neural language model of the target.",7 Conclusion & Future Work,[0],[0]
This work was supported by National Science Foundation Grants 1423276 & 1718846.,Acknowledgements,[0],[0]
"We are grateful to the state of Maryland for the Maryland Advanced Research Computing Center, a crucial resource.",Acknowledgements,[0],[0]
"We thank Shijie Wu and Adithya Renduchintala for early discussion, Argo lab members for further discussion, and the 3 reviewers for quality comments.",Acknowledgements,[0],[0]
"To approximately parse an unfamiliar language, it helps to have a treebank of a similar language.",abstractText,[0],[0]
But what if the closest available treebank still has the wrong word order?,abstractText,[0],[0]
We show how to (stochastically) permute the constituents of an existing dependency treebank so that its surface part-of-speech statistics approximately match those of the target language.,abstractText,[0],[0]
The parameters of the permutation model can be evaluated for quality by dynamic programming and tuned by gradient descent (up to a local optimum).,abstractText,[0],[0]
This optimization procedure yields trees for a new artificial language that resembles the target language.,abstractText,[0],[0]
We show that delexicalized parsers for the target language can be successfully trained using such “made to order” artificial languages.,abstractText,[0],[0]
Synthetic Data Made to Order: The Case of Parsing,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 752–757 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
752",text,[0],[0]
"Story comprehension has been one of the longestrunning ambitions in artificial intelligence (Dijk, 1980; Charniak, 1972).",1 Introduction,[0],[0]
One of the challenges in expanding the field had been the lack of a solid evaluation framework and datasets on which comprehension models can be trained and tested.,1 Introduction,[0],[0]
"Mostafazadeh et al. (2016) introduced the Story Cloze Test (SCT) evaluation framework to address
*",1 Introduction,[0],[0]
"This work was performed at University of Rochester.
this issue.",1 Introduction,[0],[0]
"This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’",1 Introduction,[0.9884319076557287],"['This test evaluates a story comprehension system where the system is given a foursentence short story as the ‘context’ and two alternative endings and to the story, labeled ‘right ending’ and ’wrong ending.’ Then, the system’s task is to choose the right ending.']"
"Then, the system’s task is to choose the right ending.",1 Introduction,[0],[0]
"In order to support this task, Mostafazadeh et al. also provide the ROC Stories dataset, which is a collection of crowd-sourced complete five sentence stories through Amazon Mechanical Turk (MTurk).",1 Introduction,[0],[0]
"Each story follows a character through a fairly simple series of events to a conclusion.
",1 Introduction,[0],[0]
"Several shallow and neural models, including the state-of-the-art script learning approaches, were presented as baselines (Mostafazadeh et al., 2016) for tackling the task, where they show that all their models perform only slightly better than a random baseline suggesting that richer models are required for tackling this task.",1 Introduction,[0],[0]
"A variety of new systems were proposed (Mihaylov and Frank, 2017; Schenk and Chiarcos, 2017; Schwartz et al., 2017b; Roemmele et al., 2017) as a part of the first shared task on SCT at LSDSem’17 workshop (Mostafazadeh et al., 2017).",1 Introduction,[0],[0]
"Surprisingly, one of the models made a staggering improvement of 15% to the accuracy, partially due to using stylistic features isolated in the ending choices (Schwartz et al., 2017b), discarding the narrative context.",1 Introduction,[0],[0]
"Clearly, this success does not seem to reflect the intent of the original task, where the systems should leverage narrative understanding as opposed to the statistical biases in the data.",1 Introduction,[0],[0]
"In this paper, we study the effect of such biases between the ending choices and present a new scheme to reduce such stylistic artifacts.
",1 Introduction,[0],[0]
"The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety
of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).",1 Introduction,[0.9999999523340093],['The contribution of this paper is threefold: (1) we provide an extensive analysis of the SCT dataset to shed some light on the ending data characteristics (Section 3) (2) we develop a new strong classifier for tackling the SCT that uses a variety of features inspired by all the top-performing systems on the task (Section 4) (3) we design a new crowd-sourcing scheme that yields a new SCT dataset; we benchmark various models on the new dataset (Section 5).']
"The results show that the topperforming SCT system on the the leaderboard1 (Chaturvedi et al., 2017) fails to keep up the performance on our new dataset.",1 Introduction,[0],[0]
We discuss the implications of this experiment to the greater research community in terms of data collection and benchmarking practices in Section 6.,1 Introduction,[0],[0]
All the code and datasets for this paper will be released to the public.,1 Introduction,[0],[0]
We hope that the availability of the new evaluation set can further support the continued research on story understanding.,1 Introduction,[0],[0]
"This paper mainly extends the work on creating the Story Cloze Test set (Mostafazadeh et al., 2016), hereinafter SCT-v1.0.",2 Related Work,[0],[0]
"The SCT-v1.0 dataset was created as follows: full five-sentence stories from the ROC Stories corpus were sampled, then, the initial four sentences were shown to a set of MTurk2 crowd workers who were prompted to author ‘right’ and ‘wrong’ endings.",2 Related Work,[0],[0]
"Mostafazadeh et al. (Mostafazadeh et al., 2016) give special care to make sure there were no boundary cases for ‘right’ and ‘wrong’ endings by implementing extra rounds of data filtering.",2 Related Work,[0],[0]
"The resulting SCT-v1.0 dataset had a validation (hereinafter, SCT-v1.0 Val) and a test set (SCT-v1.0 test), each with 1,871 cases.",2 Related Work,[0],[0]
Table 1 shows two example story cloze test cases from SCT-v1.0 corpus.,2 Related Work,[0],[0]
"As for positive training data, they had provided a collection of 100K five sentence stories.",2 Related Work,[0],[0]
"Human performance is reported to be 100% on SCT-v1.0.
",2 Related Work,[0],[0]
"Mostafazadeh et al. (2016) provide a variety of baseline models for SCT-v1.0, with the best model performing with an accuracy of 59%.",2 Related Work,[0],[0]
"The first
1As of 15th February 2018.",2 Related Work,[0],[0]
"2http://mturk.com
shared task on SCT-v1.0 was conducted at the LSDSem’17 workshop (Mostafazadeh et al., 2017), where most of the models performed with 60- 70% accuracy.",2 Related Work,[0],[0]
"One of the top-performing models, msap (Schwartz et al., 2017b,a), built a classifier using linguistic features that have been previously useful in authorship style detection, using only the ending sentences.",2 Related Work,[0],[0]
"They used stylistic features such as sentence length, word, and character level n-grams for each ending (fully discarding the context), achieving an accuracy of 72%.",2 Related Work,[0],[0]
"In conjunction with their work, Cai et al., (Cai et al., 2017) reported similar observations separately, exposing that features such as sentiment, negation, and length are different between the right and wrong endings.",2 Related Work,[0],[0]
"The best model on SCT-v1.0 to this date is cogcomp, which is a linear model that uses event sequences, sentiment trajectory, and topical consistency as features, and performs with an accuracy of 77.6%.
",2 Related Work,[0],[0]
This paper takes all their analysis further and introduces a model aggregating all the pinpointed features to shed more light into the stylistic biases isolated in SCT-v1.0 endings.,2 Related Work,[0],[0]
"Despite all the efforts made in the original SCT paper, there was never an extensive analysis of the features isolated in the endings of the stories.",3 Stylistic Feature Analysis,[0],[0]
"We explored the differences among stylistic features such as word-token count, sentiment, and the sentence complexity between the endings, to determine a composite score for identifying sources of bias.",3 Stylistic Feature Analysis,[0],[0]
"For determining the sentiment, we used Stanford CoreNLP",3 Stylistic Feature Analysis,[0],[0]
"(Manning et al., 2014) and the VADER sentiment analyzer (Hutto and Gilbert, 2014).",3 Stylistic Feature Analysis,[0],[0]
"For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).",3 Stylistic Feature Analysis,[1.0],"['For measuring the syntactic complexity, we used Yngve and Frazier metrics (Yngve, 1960; Frazier, 1985).']"
Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.,3 Stylistic Feature Analysis,[1.0],['Table 2 compares these statistics between the right and wrong endings in the SCTv1.0 dataset.']
"The feature distribution plots can be found in the supplementary material.
",3 Stylistic Feature Analysis,[0.9999999350680123],['The feature distribution plots can be found in the supplementary material.']
"Furthermore, we conducted an extensive ngram analysis, using word tokens, characters, partof-speech, and token-POS (similar to Schwartz et al. (Schwartz et al., 2017b))",3 Stylistic Feature Analysis,[0],[0]
as features.,3 Stylistic Feature Analysis,[0],[0]
"We see char-grams such as “sn’t” and “not” appear more commonly in the ‘wrong endings’, suggesting heavy negation.",3 Stylistic Feature Analysis,[0],[0]
"In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.",3 Stylistic Feature Analysis,[1.0],"['In ‘right endings’, pronouns are used more frequently versus proper nouns used in ‘wrong endings’.']"
"Artifacts such as ‘pizza’ are common in ’wrong endings,’ which could suggest that for a given topic, the authors may replace an object in a right ending with a wrong one and quickly think up a common item such as pizza to create a ‘wrong’ one.",3 Stylistic Feature Analysis,[0],[0]
"An extensive analysis of these features, including the n-gram analysis, can be found in the supplementary material.",3 Stylistic Feature Analysis,[0],[0]
"Following the analysis above, we developed a Story Cloze model, hereinafter EndingReg, that only uses the ending features while disregarding the story context for choosing the right ending.",4 Model,[0],[0]
We expanded each Story Cloze Test case’s ending options into a set of two single sentences.,4 Model,[0],[0]
"Then, for each sentence, we created the following features:
1.",4 Model,[0],[0]
Number of tokens 2.,4 Model,[0],[0]
VADER composite sentiment score 3.,4 Model,[0],[0]
Yngve complexity score 4.,4 Model,[0],[0]
Token-POS n-grams 5.,4 Model,[0],[0]
POS n,4 Model,[0],[0]
-grams 6.,4 Model,[0],[0]
"Four length character-grams
All n-gram features needed to appear at least five times throughout the dataset.",4 Model,[0],[0]
The features were collected for each five-sentence story and then fed into a logistic regression classifier.,4 Model,[0],[0]
"As an initial experiment, we trained this model using the SCTv1.0 validation set and tested on the SCT-v1.0 test set.",4 Model,[0],[0]
"An L2 regularization penalty was used to enforce a Gaussian prior on the feature-space, where a grid search was conducted for hyper-parameter tuning.",4 Model,[0],[0]
This model achieves an accuracy of 71.5% on the SCT-v1.0 dataset which is on par with the highest score achieved by any model using only the endings.,4 Model,[0],[0]
"Table 3 shows the accuracies ob-
tained by models using only those particular features.",4 Model,[0],[0]
"We achieve minimal but sometimes important classification using token count, VADER, and Yngve in combination alone, better classification using POS or char-grams alone, and best classification using n-grams alone.",4 Model,[0],[0]
By combining all of them we achieve the overall best results.,4 Model,[0],[0]
"Based on the findings above, a new test set for the SCT was deemed necessary.",5 Data Collection,[0],[0]
"The premise of predicting an ending to a short story, as opposed to predicting say a middle sentence, enables a more systematic evaluation where human can agree on the cases 100%.",5 Data Collection,[0],[0]
"Hence, our goal was to come up with a data collection scheme that overcomes the data collection biases, while keeping the original evaluation format.",5 Data Collection,[0],[0]
"As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.",5 Data Collection,[1.0],"['As the data analysis revealed, the token count, sentiment, and the complexity are not as important features for classification as the ending n-grams are.']"
We set the following goals for sourcing the new ‘right’ and ‘wrong’ endings.,5 Data Collection,[0],[0]
"They both should:
1.",5 Data Collection,[0],[0]
Contain a similar number of tokens 2.,5 Data Collection,[0],[0]
"Have similar distributions of token n-grams
and char-grams 3.",5 Data Collection,[1.0000000642958506],['Have similar distributions of token n-grams and char-grams 3.']
"Occur as standalone events with the same
likelihood to occur, with topical, sentimental, or emotion consistencies when applicable.
",5 Data Collection,[0],[0]
"First, we crowdsourced 5,000 new five-sentence stories through Amazon Mechanical Turk.",5 Data Collection,[0],[0]
We prompted the users in the same manner described in Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"In order to source new ‘wrong’ endings, we tried two different methods.",5 Data Collection,[0],[0]
"In Method #1, we kept the original ending sourcing format of Mostafazadeh et al., but imposed some further restrictions.",5 Data Collection,[0],[0]
"This was done
by taking the first four sentences of the newly collected stories and asking an MTurker to write a ‘right’ and ‘wrong’ ending for each.",5 Data Collection,[0],[0]
"The new restrictions were: ‘Each sentence should stay within the same subject area of the story,’ and ‘The number of words in the Right and Wrong sentences should not differ by more than 2 words,’ and ‘When possible, the Right and Wrong sentences should try to keep a similar tone/sentiment as one another.’",5 Data Collection,[0.9560970355744433],"['Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.']"
"The motivation behind this technique was to reduce the statistical differences by asking the user to be mindful of considerations.
",5 Data Collection,[0],[0]
"In Method #2, we took the five sentences stories and prompted a second set of MTurk workers to modify the fifth sentence in order to make a resulting five-sentence story non-sensible.",5 Data Collection,[0],[0]
"Here, the prompt instructs the workers to make sure the new ‘wrong ending’ sentence makes sense standalone, that it does not differ in the number of words from the original sentence by more than three words, and that the changes cannot be as simple as e.g., putting the word ‘not’ in front of a description or a verb.",5 Data Collection,[0],[0]
"As a result, the workers had much less flexibility for changing the underlying linguistic structures which can help tackle the authorship style differences between the ‘right’ and ‘wrong’ endings.
",5 Data Collection,[0],[0]
"The results in Table 4, which show classification accuracy when using EndingReg on the two new data sources, show that Method #2 is a slightly better data sourcing scheme in reducing the bias, since the EndingReg model’s performance is slightly worse.",5 Data Collection,[0],[0]
The set was further filtered through human verification similar to Mostafazadeh et al. (2016).,5 Data Collection,[0],[0]
"The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.",5 Data Collection,[1.0],"['The filtering was done by splitting each SCT-v1.0’s two alternative endings into two independent five-sentence stories and asking three different MTurk users to categorize the story as either: one where the story made complete sense, one where the story made sense until the last sentence and one where the story does not make sense for another reason.']"
Stories were only selected if all the three MTurk users verified that the story with the ‘right ending’ and the corresponding story with the ‘wrong ending’ were verified to be indeed right and wrong respectively.,5 Data Collection,[0],[0]
This ensured a higher quality of data and eliminating boundary cases.,5 Data Collection,[0],[0]
"This entire process resulted in creating the Story Cloze Test v1.5 (SCT-v1.5) dataset, consisting of 1,571 stories for each validation and test sets.",5 Data Collection,[0],[0]
"In order to test the decrease in n-gram bias, which was the most salient feature for the classification task using only the endings, we compare the variance between the n-gram counts from SCT-v1.0 to SCT-v1.5.",6 Results,[0],[0]
"The results are presented in Table 5, which indicates the drop in the standard deviations in our new dataset.",6 Results,[0],[0]
Table 6 shows the classification results of various models on SCT-v1.5.,6 Results,[0],[0]
"The drop in accuracy of the EndingReg model between the SCT-v1.0 and SCT-v1.5 shows a significant improvement on the statistical weight of the stylistic features generated by the model.
",6 Results,[0],[0]
"Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.",6 Results,[1.0],"['Since the main features used are the token length and the various n-grams, this suggests that the new ‘right endings’ and ‘wrong endings’ have much more similar token n-gram, pos n-gram, postoken n-gram and char-gram overlap.']"
"Furthermore, the CogComp model’s performance has significantly dropped on SCT-v1.5.",6 Results,[0],[0]
"Although this model seems to be using story comprehension features such as event sequencing, since the endings are included in the sequences, the biases within the endings have influenced the predictions and the weak performance of the model in SCT-v1.5 suggest that this model had picked up on the biases of SCT-v1.0 as opposed to really understanding the context.",6 Results,[0],[0]
"In particular, the posterior probabilities for each ending choice using their features are quite similar on the SCT-v1.5.",6 Results,[0],[0]
"These results place the classification accuracy of this top performing model on par with or worse than the models that did not use the ending features of the old SCT-v1.0 dataset (Mostafazadeh et al., 2017), which suggest that the gap that once was held by models using the ending biases seems to be corrected for.",6 Results,[0],[0]
"Al-
though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.
",6 Results,[0.9999999692148713],"['Al- though we did not get to test all the other models published on SCT-v1.0 directly, we predict similar trends.']"
It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.,6 Results,[1.0],['It is important to point out that the 64.4% performance attained by our EndingReg model is still high for a model which completely discards the context.']
"This indicates that although we could correct for some of the stylistic biases, there are some other hidden patterns in the new endings that would not have been accounted for without having the EndingReg baseline.",6 Results,[0],[0]
"This showcases the importance of maintaining benchmarks that evolve and improve over time, where systems should not be optimized for particular narrow test sets.",6 Results,[0],[0]
"We propose the community to report accuracies on both SCT-v1.0 and SCT-v1.5, both of which still have a huge gap between the best system and the human performance.",6 Results,[0],[0]
"In this paper, we presented a comprehensive analysis of the stylistic features isolated in the endings of the original Story Cloze Test (SCT-v1.0).",7 Conclusion,[0],[0]
"Using that analysis, along with a classifier we developed for testing new data collection schemes, we created a new SCT dataset, SCT-v1.5, which overcomes some of the biases.",7 Conclusion,[0],[0]
"Based on the results presented in this paper, we believe that our SCT-v1.5 is a better benchmark for story comprehension.",7 Conclusion,[0],[0]
"However, as shown in multiple AI tasks (Ettinger et al., 2017; Antol et al., 2015; Jabri et al., 2016; Poliak et al., 2018), no collected dataset is entirely without its inherent biases and often the biases in datasets go undiscovered.",7 Conclusion,[0],[0]
We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.,7 Conclusion,[1.0],['We believe that evaluation benchmarks should evolve and improve over time and we are planning to incrementally update the Story Cloze Test benchmark.']
"All the new versions, along with a leader-board showcasing the stateof-the-art results, will be tracked via CodaLab
https://competitions.codalab.org/ competitions/15333.
",7 Conclusion,[0],[0]
The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.,7 Conclusion,[1.0],['The success of our modified data collection method shows how extreme care must be given for sourcing new datasets.']
"We suggest the next SCT challenges to be completely blind, where the participants cannot deliberately leverage any particular data biases.",7 Conclusion,[0],[0]
"Along with this paper, we are releasing the datasets and the developed models to the community.",7 Conclusion,[0],[0]
"All the announcements, new supplementary material, and datasets can be accessed through http://cs.",7 Conclusion,[0],[0]
rochester.edu/nlp/rocstories/.,7 Conclusion,[1.0],['rochester.edu/nlp/rocstories/.']
We hope that this work ignites further interest in the community for making progress on story understanding.,7 Conclusion,[1.0],['We hope that this work ignites further interest in the community for making progress on story understanding.']
We would like to thank Roy Schwartz for his valuable feedback regarding some of the experiments.,Acknowledgement,[0],[0]
"We also thank the amazing crowd workers, without the work of whom this work would have been impossible.",Acknowledgement,[0],[0]
This work was supported in part by grant W911NF15-1-0542 with the US Defense Advanced Research Projects Agency (DARPA) as a part of the Communicating with Computers (CwC) program.,Acknowledgement,[0],[0]
The Story Cloze Test (SCT) is a recent framework for evaluating story comprehension and script learning.,abstractText,[0],[0]
There have been a variety of models tackling the SCT so far.,abstractText,[0],[0]
"Although the original goal behind the SCT was to require systems to perform deep language understanding and commonsense reasoning for successful narrative understanding, some recent models could perform significantly better than the initial baselines by leveraging human-authorship biases discovered in the SCT dataset.",abstractText,[0],[0]
"In order to shed some light on this issue, we have performed various data analysis and analyzed a variety of top performing models presented for this task.",abstractText,[0],[0]
"Given the statistics we have aggregated, we have designed a new crowdsourcing scheme that creates a new SCT dataset, which overcomes some of the biases.",abstractText,[0],[0]
We benchmark a few models on the new dataset and show that the topperforming model on the original SCT dataset fails to keep up its performance.,abstractText,[0],[0]
Our findings further signify the importance of benchmarking NLP systems on various evolving test sets.,abstractText,[0],[0]
Tackling the Story Ending Biases in The Story Cloze Test,title,[0],[0]
