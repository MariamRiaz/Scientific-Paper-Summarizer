0	27	Structured prediction can be thought of as a generalization of binary classification to structured outputs, where the goal is to jointly predict several dependent variables.
1	41	Predicting complex, structured data is of great significance in various application domains including computer vision (e.g., image segmentation, multiple object tracking), natural language processing (e.g., part-of-speech tagging, named entity recognition) and computational biology (e.g. protein structure prediction).
2	25	However, unlike binary classification, structured prediction presents a set of unique computational and statistical challenges.
4	16	For instance, in translation tasks, the number of parse trees of a sentence is exponential in the length of the sentence.
6	68	The key computational challenge in structured prediction stems from the inference problem, where a decoder, parameterized by a vector w of weights, predicts (or decodes) the latent structured output y given an observed input x.
7	27	With the exception of a few special cases, the general inference problem in structured prediction is intractable.
8	47	For instance in many cases the inference problem reduces to the maximum acyclic subgraph problem which is NP-hard and hard to approximate to within a factor of 1/2 of the optimal solution (Guruswami et al., 2008), or cardinality-constrained submodular maximization, which is also NP-hard and hard to compute a solution better than the (1 1/")-approximate solution returned by a greedy algorithm (Nemhauser et al., 1978).
10	40	Hardness of max-margin learning (SVM) was shown by (Sontag et al., 2010).
12	18	In these approaches, learning the parameter w of the decoder involves minimizing a loss function L(w, S) over a data set S of m training pairs {(x i , y i )}m i=1 .
14	34	McAllester (McAllester, 2007) showed, using the PAC-Bayesian framework, that the commonly used max-margin loss (Taskar et al., 2003) upper bounds the expected Gibbs loss over the data distribution, upto statistical error.
15	28	Therefore, minimizing the max-margin loss provides a principled way for learning the parameters of a structured decoder.
18	67	The perturb-and-MAP framework introduced by (Papandreou & Yuille, 2011), and henceforth referred to as MAP perturbation, provides an efficient way to generate samples from the Gibbs distribution by injecting random noise (that do not depend on the weights of the decoder w) in the potential or score function of the decoder and then computing the most likely assignment or energy configuration (MAP).
34	19	Lastly, the main conceptual contribution of our work is to demonstrate that it is possible to efficiently learn the parameters of a structured decoder with generalization guarantees without solving the inference problem exactly.
35	17	We begin this section by introducing our notations and formalizing the problem of learning MAP-perturbation models.
50	16	Under Gumbel perturbations, f w, (x i ) is distributed according to following conditional random field (CRF) distribution Q(x i , w) with pmf q( · ;x i , w) (Gumbel, 1954; Papandreou & Yuille, 2011): q(y i ;x i , w) = Pr ⇠Gr( ) {fw, (xi) = yi} = exp( h (xi,yi),wi/ ) Z(w, x i ) , (5) where Z(w, x i ) = P y2Y(x) exp( h (xi,y),wi/ ) is the partition function.
70	17	This is due to the need for computing the partition function Z(w, x) which is an NP-hard problem (Barahona, 1982).
76	31	Note that the partition function Z w,x,T0 can be computed in time linear in n, since |T0| = n. Now, let T = {T i | x i 2 S} be the collection of n structured outputs sampled for each x i in the data set, from the product distribution R(S, w) def= ⇥m i=1 (R(x i ) n ).
86	21	(9) As opposed to the loss function given by (6), the loss in (9) can be computed efficiently for small n. Our next result shows that the randomized augmented loss lower bounds the full CRF loss L(w, S) as long as ¯T i is a set, i.e., contains only unique elements.
90	19	Next, we will show that an algorithm that learns the parameter w of the MAP-perturbation model, by sampling a small number of structured outputs for each x i and minimizing the empirical loss given by (9), generalizes under various choices of the proposal distribution R. Our first step in that direction would be to obtain uniform convergence guarantees for the stochastic loss (9).
93	18	From Lemma 1 it is clear that the proposal distribution plays a crucial role in determining how far the surrogate loss L(w, S, ¯T) is from the CRF loss L(w, S).
101	22	If the scale parameter of the Gumbel perturbations satisfies:  min( kwk 1 /logm,wmin/log((r 1)(pm 1))) for all w 6= 0, and n m0.5 c, then under Assumption 1 A(w, S)  " 1 (m,n,w), where " 1 (m,n,w) def = kwk 1p m + 1 1 + p m , and w min = min{|w j | | |w j | 6= 0, j 2 [d]}.
109	30	Taking log on both sides we get: > h (x i , y i ) (x i , y0), wi log((r 1)(pm 1)) Since y i is the unique maximizer of the score h (x i , y i ), wi, (x i , y0) and (x i , y i ) must differ on at least one element in the support set of w. This implies, from above and the assumption that the minimum non-zero element of (x, y) is at least 1: > w min log((r 1)(pm 1)) , which violates Assumption 1.
127	29	We want to construct proposal distributions of the form given by Definition 1 that satisfy Assumption 1 with a large enough constant c. Additionally, for our randomized procedure to run in polynomial time we want the proposal distribution to sample a structured output in constant time.
145	23	The exact CRF loss (L(w, S)) can similarly be minimized by using ¯T i = Y(x i ), for all x i 2 S, in the above.
175	21	CRF_RAND significantly outperformed other algorithms in both the test set loss and test set hamming loss, while being ⇡ 6 times faster than the exact method (CRF_ALL) for DAGs, ⇡ 20 times faster for trees, and⇡ 3 times faster for sets.
178	19	lRVV DAGV 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 7rDLn lRVV 7HVt lRVV 7HVt HDmm.
180	27	lRVV C5F_5A1D C5F_ALL 6V0_5A1D 6V0_ALL 7rDining time DAGs 0.0 50.0 100.0 150.0 200.0 6 e c o n G s 10.04 58.10 10.19 149.91 7rDining time 7rees 16.91 339.76 15.22 934.94 7rDining time 6ets 21.15 57.59 7.32 147.70 Figure 1.
183	18	(Right) Training time in seconds for the various methods.
196	74	While in this work we showed that one can learn with approximate inference, it would be interesting to analyze approximate inference for prediction on an independent test set.
197	49	Another avenue for future work would be to develop more powerful proposal distributions that allow for more finer-grained control over the parameter c by exploiting problem specific structure like submodularity.
