{"sections": [{"heading": "1. Introduction", "text": "Variational inference (VI) is a family of methods to approximate an intractable target distribution (typically known only up to a constant) with a tractable surrogate distribution (Blei et al., 2016; Jordan et al., 1999; Wainwright & Jordan, 2008). VI procedures typically minimize the Kullback-Leibler (KL) divergence between the approximation and target distributions by maximizing a tractable lower bound on the marginal likelihood. The approximating family is often fixed, and typically excludes the neighborhood surrounding the target distribution, which prevents the approximation from becoming arbitrarily close to the true posterior. In the context of Bayesian inference, this mismatch between the variational family and the true posterior often manifests as underestimating the posterior variances of the model parameters and the inability to capture posterior correlations (Wainwright & Jordan, 2008).\nAn alternative approach to posterior inference uses Markov\n1Harvard University, Cambridge, MA, USA 2University of Washington, Seattle, WA, USA 3Google Brain, Cambridge, MA, USA. Correspondence to: Andrew C. Miller <acm@seas.harvard.edu>, Nicholas J. Foti <nfoti@uw.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\nchain Monte Carlo (MCMC) methods that approximate a target distribution with samples drawn from a Markov chain constructed to admit the target distribution as the stationary distribution. MCMC enables a trade-off between computation and accuracy: drawing more samples makes the approximation closer to the target distribution. However, MCMC algorithms typically must be run iteratively and it can be difficult to assess convergence to the true target. Furthermore, correctly specifying MCMC moves can be more algorithmically restrictive than optimization-based approaches.\nTo alleviate the mismatch between tractable variational approximations and complicated posterior distributions, we propose a variational inference method that iteratively allows the approximating family of distributions to become more complex. Under certain conditions, the proposed approximations are eventually expressive enough to represent the true target arbitrarily well (though we do not prove our algorithm attains such a universal approximation here). Thus, the practitioner can trade time fitting a posterior approximation for increased accuracy of posterior estimates. Our algorithm grows the complexity of the approximating class in two ways: 1) incorporating rich covariance structure, and 2) sequentially adding new components to the approximating distribution. Our method builds on black-box variational inference methods using the re-parameterization trick by adapting it to be used with mixture distributions. This allows our method to be applied to a variety of target distributions including those arising from non-conjugate model specifications (Kingma & Welling, 2014; Ranganath et al., 2014; Salimans & Knowles, 2013). We demonstrate empirically that our algorithm improves posterior estimates over other variational methods for several practical Bayesian models."}, {"heading": "2. Variational Inference", "text": "Given a target distribution with density1 \u03c0(x) for a continuous random variable x \u2208 X \u2286 RD, variational inference approximates \u03c0(x) with a tractable distribution, q(x;\u03bb), from which we can efficiently draw samples and form sample-based estimates of functions of x. Variational methods minimize the KL-divergence, KL(q||\u03c0), between q(\u00b7;\u03bb) and the true \u03c0 as a function of variational parameter \u03bb (Bishop, 2006). Although direct minimization of\n1We assume \u03c0(x) is known up to a constant, \u03c0\u0303(x) = C\u03c0(x).\nKL(q||\u03c0) is often intractable, we can derive a tractable objective based on properties of the KL-divergence. This objective is known as the evidence lower bound (ELBO):\nL(\u03bb) = Eq\u03bb [ln\u03c0(x)\u2212 ln q(x;\u03bb)] + ln C = ln C \u2212KL(q\u03bb||\u03c0) \u2264 ln C = ln \u222b \u03c0\u0303(x)dx\nwhich, due to the positivity of KL(q||\u03c0), is a lower bound on C = log \u03c0(x), i.e., the marginal likelihood.\nVariational methods typically fix a family of distributions Q = {q(\u00b7;\u03bb) : \u03bb \u2208 \u039b} parameterized by \u03bb, and maximize the ELBO with respect to \u03bb \u2208 \u039b. Often there exists some (possibly non-unique) \u03bb\u2217 \u2208 \u039b for which KL(q||\u03c0) is minimized. However, when the family Q does not include \u03c0 then KL(q\u03bb\u2217 ||\u03c0) > 0 which will result in biased estimates of functions f(x), Ex\u223cq\u03bb\u2217 [f(x)] 6= Ex\u223c\u03c0[f(x)].\nThe primary alternative to variational methods for approximate inference is Markov chain Monte Carlo (MCMC), which constructs a Markov chain such that the target distribution remains invariant. Expectations with respect to the target distribution can be calculated as an average with respect to these correlated samples. MCMC typically enjoys nice asymptotic properties; as the number of samples grows, MCMC samplers represent the true target distribution with increasing fidelity. However, rules for constructing correct Markov steps are restrictive. With a few exceptions, most MCMC algorithms require evaluating a log-likelihood that touches all data at each step in the chain (Maclaurin & Adams, 2014; Welling & Teh, 2011). This becomes problematic during statistical analyses of large amounts of data \u2014 MCMC is often considered unusable because of this computational bottleneck. Notably, variational methods can avoid this bottleneck by sub-sampling the data (Ranganath et al., 2016a), as unbiased estimates of the log-likelihood can often be straight-forwardly used with optimization methods.\nIn the next section, we propose an algorithm that iteratively grows the approximating class Q and reframes the VI procedure as a series of optimization problems, resulting in a practical inference method that can both represent arbitrarily complex distributions and scale to large data sets."}, {"heading": "3. Variational Boosting", "text": "We define our class of approximating distributions to be mixtures of C simpler component distributions:\nq(C)(x;\u03bb, \u03c1) = C\u2211 c=1 \u03c1cqc(x;\u03bbc) , s.t. \u03c1c \u2265 0, \u2211 c \u03c1c = 1,\nwhere we denote the full mixture as q(C), mixing proportions \u03c1 = (\u03c11, . . . , \u03c1C), and component distributions qc(\u00b7;\u03bbc) parameterized by \u03bb = (\u03bb1, . . . , \u03bbC). The\nqc(\u00b7;\u03bbc)s can be any distribution over X \u2286 RD from which we can efficiently draw samples using a continuous mapping parameterized by \u03bbc (e.g., multivariate normal (Jaakkola & Jordan, 1998), or a composition of invertible maps (Rezende & Mohamed, 2015)).\nWhen posterior expectations and variances are of interest, mixture distributions provide tractable summaries. Expectations are easily expressed in terms of component expectations:\nEq(C) [f(x)] = \u222b q(C)(x)f(x)dx = \u2211 c \u03c1cEqc [f(x)].\nIn the case of multivariate normal components, the mean and covariance of a mixture are easy to compute, as are marginal distributions along any set of dimensions.\nVariational boosting (VBoost) begins with a single mixture component, q(1)(x;\u03bb) = q1(x;\u03bb1) with C = 1. We fix \u03c11 = 1 and use existing black-box variational inference methods to fit the first component parameter, \u03bb1. At the next iteration C = 2, we fix \u03bb1 and introduce a new component into the mixture, q2(x;\u03bb2). We define a new ELBO objective as a function of new component parameters, \u03bb2, and a new mixture weight, \u03c12. We then optimize this objective with respect to \u03bb2 and \u03c12 until convergence. At each subsequent round, c, we introduce new component parameters and a mixing weight, (\u03bbc, \u03c1c), which are then optimized according to a new ELBO objective. The name variational boosting is inspired by methods that iteratively construct strong learners from ensembles of weak learners. We apply VBoost to target distributions via black-box variational inference with the re-parameterization trick to fit each component and mixture weights (Kingma & Welling, 2014; Ranganath et al., 2014; Salimans & Knowles, 2013). However, using mixtures as the variational approximation complicates the use of the re-parameterization trick."}, {"heading": "3.1. The re-parameterization trick and mixtures", "text": "The re-parameterization trick is used to compute an unbiased estimate of the gradient of an objective that is expressed as an intractable expectation with respect to a continuousvalued random variable. This situation arises in variational inference when the ELBO cannot be evaluated analytically. We form an unbiased estimate as:\nL(\u03bb) = Eq [ln\u03c0(x)\u2212 ln q(x;\u03bb)] (1)\n\u2248 1 L L\u2211 `=1 [ ln\u03c0(x(`))\u2212 ln q(x(`);\u03bb) ] (2)\nwhere x(`) \u223c q(x;\u03bb). To obtain a Monte Carlo estimate of the gradient of L(\u03bb) using the re-parameterization trick, we first separate the randomness needed to generate x(`) from the parameters \u03bb, by defining a deterministic map x(`) , fq( ;\u03bb) such that \u223c p( ) implies\nx(`) \u223c q(x;\u03bb). Note that p( ) does not depend on \u03bb. We then differentiate Eq. (2) with respect to \u03bb through the map fq to obtain an estimate of\u2207\u03bbL(\u03bb).\nWhen q(\u00b7;\u03bb) is a mixture, applying the re-parameterization trick is not straightforward. The typical sampling procedure for a mixture model includes a discrete random variable that indicates a mixture component, which complicates differentiation. We circumvent this by re-writing the variational objective as a weighted combination of expectations with respect to individual mixture components:\nL(\u03bb, \u03c1) = \u222b ( C\u2211\nc=1\n\u03c1cqc(x;\u03bbc)\n) [ln\u03c0(x)\u2212 ln q(x;\u03bb)] dx\n= C\u2211 c=1 \u03c1c \u222b qc(x;\u03bbc) [ln\u03c0(x)\u2212 ln q(x;\u03bb)] dx\n= C\u2211 c=1 \u03c1cEqc [ln\u03c0(x)\u2212 ln q(x;\u03bb)]\nwhich is a weighted sum of component-specific ELBOs. If the qc are continuous and there exists some function fc( ;\u03bb) such that x = fc( ;\u03bb) and x \u223c qc(\u00b7;\u03bb) when \u223c p( ), then we can apply the re-parameterization trick to each component to obtain gradients of the ELBO : \u2207\u03bbcL(\u03bb, \u03c1) = \u2207\u03bbc C\u2211 c=1 \u03c1cEx\u223cq(x;\u03bb) [ln\u03c0(x)\u2212 ln q(x;\u03bb)]\n= C\u2211 c=1 \u03c1cE \u223cp( ) [ \u2207\u03bbc ln\u03c0(fc( ;\u03bbc))\n\u2212\u2207\u03bbc ln q(fc( ;\u03bbc);\u03bb) ] .\nVBoost leverages the above formulation of \u2207\u03bbcL(\u03bb, \u03c1) to use the re-parameterization trick in a component-bycomponent manner, allowing us to improve the variational approximation as we incorporate new components."}, {"heading": "3.2. Incorporating New Components", "text": "Next, we describe how to incrementally add components during the VBoost procedure.\nThe first component VBoost starts by fitting a approximation to \u03c0(x) consisting of a single component, q1. We do this by maximizing the first ELBO objective\nL(1)(\u03bb1) = Eq1 [ln\u03c0(x)\u2212 ln q1(x;\u03bb1)] (3) \u03bb\u22171 = arg max\n\u03bb1\nL(1)(\u03bb1) . (4)\nDepending on the forms of \u03c0 and q1, optimizing L(1) can be accomplished by various methods \u2014 an obvious choice being black-box VI with the re-parameterization trick. After convergence we fix \u03bb1 to be \u03bb\u22171.\nComponent C + 1 After iteration C, our current approximation to \u03c0(x) is a mixture distribution with C components:\nq(C)(x;\u03bb, \u03c1) = C\u2211 c=1 \u03c1cqc(x;\u03bbc). (5)\nAdding a component to Eq. (5) introduces a new component parameter, \u03bbC+1, and a new mixing weight, \u03c1C+1. In this section, the mixing parameter \u03c1C+1 \u2208 [0, 1] mixes between the new component, qC+1(\u00b7;\u03bbC+1) and the existing approximation, q(C). The new approximate distribution is\nq(C+1)(x;\u03bb, \u03c1)\n= (1\u2212 \u03c1C+1)q(C)(x) + \u03c1C+1qC+1(x;\u03bbC+1) .\nThe new ELBO, as a function of \u03c1C+1 and \u03bbC+1, is:\nL(C+1)(\u03c1C+1, \u03bbC+1) = Ex\u223cq(C+1) [ ln\u03c0(x)\u2212 ln q(C+1)(x;\u03bbC+1, \u03c1C+1) ] = (1\u2212 \u03c1C+1)Eq(C) [ ln\u03c0(x)\u2212 ln q(C+1)(x;\u03bbC+1, \u03c1C+1)\n] + \u03c1C+1EqC+1 [ ln\u03c0(x)\u2212 ln q(C+1)(x;\u03bbC+1, \u03c1C+1) ] .\nCrucially, we have separated out two expectations: one with respect to the existing approximation, q(C) (which is fixed), and the other with respect to the new component distribution, qC+1. Because we have fixed q(C), we only need to optimize the new component parameters, \u03bbC+1 and \u03c1C+1, allowing us to use the re-parameterization trick to obtain gradients of L(C+1). Note that evaluating the gradient requires sampling from the existing components which may result in larger variance than typical black-box variational methods. To mitigate the extra variance we use many samples to estimate the gradient and leave variance reduction to future work.\nFigure 1 illustrates the algorithm on a simple onedimensional example \u2014 the initialization of a new component and the resulting mixture after optimizing the second objective, L(2)(\u03c12, \u03bb2). Figure 2 depicts the result of VBoost on a two-dimensional, multi-modal target distribution. In both cases, the component distributions are Gaussians with diagonal covariance."}, {"heading": "3.3. Structured Multivariate Normal Components", "text": "Though our method can use any component distribution that can be sampled using a continuous mapping, a sensible choice of component distribution is a multivariate normal\nq(x;\u03bb) = N (x;\u00b5\u03bb,\u03a3\u03bb) = |2\u03c0\u03a3\u03bb|\u22121/2 exp ( \u2212 12 (x\u2212 \u00b5\u03bb) \u1d40\u03a3\u22121\u03bb (x\u2212 \u00b5\u03bb) )\nwhere the variational parameter \u03bb is transformed into a mean vector \u00b5\u03bb and covariance matrix \u03a3\u03bb.\nSpecifying the structure of the covariance matrix is a choice that largely depends on the dimensionality of X \u2286 RD and the correlation structure of the target distribution. A common choice of covariance is a diagonal matrix, \u03a3\u03bb = diag(\u03c321 , . . . , \u03c3 2 D), which implies that x is independent across dimensions. When the approximation only consists of one component, this structure is commonly referred to as the mean field family. While computationally efficient, mean field approximations cannot model posterior correlations, which often leads to underestimation of marginal variances. Additionally, when diagonal covariances are used as the component distributions in Eq. (5) the resulting mixture may require a large number of components to represent the strong correlations (see Fig. 2). Furthermore, independence constraints can actually introduce local optima in the variational objective (Wainwright & Jordan, 2008).\nOn the other end of the spectrum, we can parameterize the entire covariance matrix using the Cholesky decomposition, L, such that LL\u1d40 = \u03a3. This allows \u03a3 to be any positive semi-definite matrix, enabling q to have the full flexibility of a D-dimensional multivariate normal distribution. However, this introduces D(D + 1)/2 parameters, which can be computationally cumbersome when D is even moderately large. Furthermore, only a few pairs of variables may exhibit posterior correlations, particularly in multi-level models or neural networks where different parameter types may be nearly independent in the posterior.\nAs such, we would like to incorporate some capacity to capture correlations between dimensions of x without overparameterizing the approximation. The next subsection discusses a covariance specification that provides this tradeoff, while remaining computationally tractable.\nLow-rank plus diagonal covariance Black-box variational inference methods with the re-parameterization trick\nrequire sampling from the variational distribution and efficiently computing (or approximating) the entropy of the variational distribution. For multivariate normal distributions, the entropy is a function of the determinant of the covariance matrix, \u03a3, while computing the log likelihood requires computing \u03a3\u22121. When the dimensionality of the target, D, is large, computing determinants and inverses will have O(D3) time complexity and therefore may be prohibitively expensive to compute at every iteration.\nHowever, it may be unnecessary to represent allD(D\u22121)/2 possible correlations in the target distribution, particularly if certain dimensions are close to independent. One way to increase the capacity of q(x;\u03bb) is to model the covariance as a low-rank plus diagonal (LR+D) matrix\n\u03a3 = FF \u1d40 + diag(exp(v)) (6)\nwhere F \u2208 RD\u00d7r is a matrix of off diagonal factors, and v \u2208 RD is the log-diagonal component. This is effectively approximating the target via a factor analysis model.\nThe choice of the rank r presents a tradeoff: with a larger rank, the variational approximation can be more flexible; with a lower rank, the computations necessary for fitting the variational approximation are more efficient. As a concrete example, in Section 4 we present a D = 37 dimensional posterior resulting from a non-conjugate hierarchical model, and we show that a \u201crank r = 2 plus diagonal\u201d covariance does an excellent job capturing all D(D \u2212 1)/2 = 780 pairwise correlations and D marginal variances. Incorporating more components using the VBoost framework further improves the approximation of the distribution.\nTo use the re-parameterization trick with this low rank covariance, we can simulate from q in two steps\nz(lo) \u223c N (0, Ir) z(hi) \u223c N (0, ID) x = Fz(lo) + \u00b5+ I(v/2)z(hi)\nwhere z(lo) generates the randomness due to the low-rank structure, and z(hi) generates the randomness due to the diagonal structure. We use the operator I(a) = diag(exp(a)) for notational brevity. This generative process can be differentiated, yielding Monte Carlo estimates of the gradient with respect to F and v suitable for stochastic optimization.\nIn order to use LR+D covariance structure within VBoost, we will need to efficiently compute the determinant and inverse of \u03a3. The matrix determinant lemma expresses the determinant of \u03a3 as the product of two determinants\n|FF \u1d40 + I(v))| = |I(v))||Ir + F \u1d40I(\u2212v)F |\n= exp (\u2211 d vd ) |Ir + F \u1d40I(\u2212v)F |\nwhere the left term is simply the product of the diagonal component, and the right term is the determinant of an r \u00d7 r matrix, computable in O(r3) time (Harville, 1997).\nTo compute \u03a3\u22121, the Woodbury matrix identity states that\n(FF \u1d40 + I(v))\u22121\n= I(\u2212v)\u2212 I(\u2212v)F (Ir + F \u1d40I(\u2212v)F )\u22121F \u1d40I(\u2212v)\nwhich involves the inversion of a smaller, r \u00d7 r matrix and can be done in O(r3) time (Golub & Van Loan, 2013). Importantly, for r D the above operations are efficiently differentiable and amenable for use in the BBVI framework.\nFitting the rank To specify the ELBO objective, we need to choose a rank r for the component covariance. There are many ways to decide on the rank of the variational approximation, some more appropriate for certain settings given dimensionality and computational constraints. For instance, we can greedily incorporate new rank components. Alternatively, we can fit a sequence of ranks r = 1, 2, . . . , rmax, and choose the best result (in terms of KL). In the Bayesian neural network model, we report results for a fixed schedule of ranks. In the hierarchical Poisson model, we monitor the change in marginal variances to decide the appropriate rank. See Section B of the supplement for further discussion.\nInitializing new component parameters When we add a new component, we must first initialize the component parameters. We find that the VBoost optimization procedure can be sensitive to initialization, so we devise a cheap importance sampling-based algorithm to generate a good starting point. This initialization procedure is detailed in Section A and Algorithm 1 of the supplement."}, {"heading": "3.4. Related Work", "text": "Mixtures of mean field approximations were introduced in Jaakkola & Jordan (1998) where mean field-like updates were developed using a bound on the entropy term and\nmodel-specific parameter updates. Nonparametric variational inference, introduced in Gershman et al. (2012), is a black-box variational inference algorithm that approximates a target distribution with a mixture of equally-weighted isotropic normals. The authors use a lower-bound on the entropy term in the ELBO to make the optimization procedure tractable. Similarly, Salimans & Knowles (2013) present a method for fitting mixture distributions as an approximation. However, their method is restricted to mixture component distributions within the exponential family, and a joint optimization procedure. Mixture distributions are a type of hierarchical variational model (Ranganath et al., 2016b), where the component identity can be thought of as latent variables in the variational distribution. While in Ranganath et al. (2016b), the authors optimize a lower bound on the ELBO to fit general hierarchical variational models, our approach integrates out the discrete latent variables, allowing us to directly optimize the ELBO.\nSequential maximum-likelihood estimation of mixture models has been studied previously where the error between the sequentially learned model and the optimal model where all components and weights are jointly learned is bounded by O(1/C) where C is the number of mixture components (Li & Barron, 1999; Li, 1999; Rakhlin et al., 2006). A similar bound was proven in Zhang (2003) using arguments from convex analysis. More recently, sequentially constructing a mixture of deep generative models has been shown to achieve the same O(1/C) error bound when trained using an adversarial approach (Tolstikhin et al., 2016). Though these ideas show promise for deriving error bounds for variational boosting, there are difficulties in applying them.\nIn concurrent work, Guo et al. (2016) developed a boosting procedure to construct flexible approximations to posterior distributions. In particular, they use gradient-boosting to determine candidate component distributions and then optimize the mixture weight for the new component (Friedman, 2000). However, Guo et al. (2016) assume that the gradientboosting procedure is able to find the optimal new component so that the arguments in Zhang (2003) apply, which is not true in general. We note that if we make the similar assumption that at each step of VBoost the component parameters \u03bb\u2217C are found exactly, then the optimization of \u03c1C is convex and can be optimized exactly. We can then appeal to the same arguments in Zhang (2003) and obtain an O(1/C) error bound. The work in Guo et al. (2016) provides important first steps in the theoretical development of boosting methods applied to variational inference, however, we note that developing a comprehensive theory that deals with the difficulties of multimodality and the non-joint-convexity of KL divergence in \u03bb and \u03c1 is still needed. Recently, Moore (2016) began to address issues of multimodality from model symmetry in variational inference. However, the question remains whether the entire distribution is being explored.\nSeeger (2010) explored the use of low-rank covariance Gaussians as variational approximations using a PCA-like algorithm. Additionally, concurrent work has proposed the use a LR+D matrices as the covariances of Gaussian posterior approximations (Ong et al., 2017). We have also found that though the LR+D covariance approximation is useful for capturing posterior correlations, combining the idea with boosting new components to capture non-Gaussian posteriors yields superior posterior inferences."}, {"heading": "4. Experiments and Analysis", "text": "To supplement the previous synthetic examples, we use VBoost to approximate various challenging posterior distributions arising from real statistical models of interest.2\nBinomial Regression We first apply VBoost to a nonconjugate hierarchical binomial regression model.3 The model describes the binomial rates of success (batting averages) of baseball players using a hierarchical model (Efron & Morris, 1975), parameterizing the \u201cskill\u201d of each player:\n\u03b8j \u223c Beta(\u03c6 \u00b7 \u03ba, (1\u2212 \u03c6) \u00b7 \u03ba) player j prior yj \u223c Binomial(Kj , \u03b8j) player j hits ,\nwhere yj is the number of successes (hits) player j has attempted in Kj attempts (at bats). Each player has a latent success rate \u03b8j , which is governed by two global variables \u03ba and \u03c6. We specify the priors \u03c6 \u223c Unif(0, 1) and \u03ba \u223c Pareto(1, 1.5). There are 18 players in this example, creating a posterior distribution with D = 20 parameters. For each round of VBoost, we estimate\u2207\u03bb,\u03c1L(C+1) using 2Code available at https://github.com/andymiller/vboost. 3Model and data from the mc-stan case studies\n400 samples each for qC+1 and qC . We use 1,000 iterations of adam with default parameters to update \u03c1C+1 and \u03bbC+1 (Kingma & Ba, 2014).\nIn all experiments, we use autograd to obtain gradients with respect to new component parameters (Maclaurin et al., 2015b;a). To highlight the fidelity of our method, we compare VBoost with rank-1 components to mean field VI (MFVI) and the No-U-Turn Sampler (NUTS) (Hoffman & Gelman, 2014). The empirical distribution resulting from 20k NUTS samples is considered the \u201cground truth\u201d posterior in this example. Figure 3a compares a selection of univariate and bivariate posterior marginals. We see that VBoost is able to closely match the NUTS posteriors, improving upon the MFVI approximation. Figure 3b compares the VBoost covariance estimates to the \u201cground truth\u201d estimates of MCMC at various stages of the algorithm. We see that VBoost is able to capture pairwise covariances with increasing accuracy as the number of components increases.\nMulti-level Poisson GLM We use VBoost to approximate the posterior of a hierarchical Poisson GLM, a common non-conjugate Bayesian model. Here, we focus on a specific model that was formulated to measure the relative rates of stop-and-frisk events for different ethnicities and in different precincts (Gelman et al., 2007), and has been used as an illustrative example of multi-level modeling (Gelman & Hill, 2006). The model uses a precinct and ethnicity effect to describe the relative rate of stop-and-frisk events\n\u03b1e \u223c N (0, \u03c32\u03b1) ethnicity effect \u03b2p \u223c N (0, \u03c32\u03b2) precinct effect\nln\u03bbep = \u00b5+ \u03b1e + \u03b2p + lnNep log rate Yep \u223c P(\u03bbep) stop-and-frisk events\nwhere Yep are the number of stop-and-frisk events within ethnicity group e and precinct p over some fixed period of time; Nep is the total number of arrests of ethnicity group e in precinct p over the same period of time; \u03b1e and \u03b2p are the ethnicity and precinct effects. The prior over the mean offset and group variances is given by \u00b5, ln\u03c32\u03b1, ln\u03c3 2 \u03b2 \u223c N (0, 102).\nAs before, we simulate 20k NUTS samples, and compare various variational approximations. Because of the high posterior correlations present in this example, VBoost with diagonal covariance components is inefficient in its representation of this structure. As such, this example relies on the low-rank approximation to shape the posterior. Figure 4 shows how posterior accuracy is affected by incorporating covariance structure (left) and adding more components (right). Figures 6 and 7 in the supplement compare VBoost covariances to MCMC samples, showing that increased posterior rank capacity and number of components yield more accurate marginal variance and covariance estimates. These results indicate that while incorporating covariance structure increases the accuracy of estimating marginal variances, the non-Gaussianity afforded by the use\nof mixture components allows for a better posterior approximation translating into more accurate moment estimates.\nBayesian Neural Network We apply our method to a Bayesian neural network (BNN) regression model, which admits a high-dimensional, non-Gaussian posterior. We compare predictive performance of VBoost to Probabilistic Backpropagation (PBP) (Hern\u00e1ndez-Lobato & Adams, 2015). Mimicking the experimental setup of Hern\u00e1ndezLobato & Adams (2015), we use a single 50-unit hidden layer, with ReLU activation functions. We place a normal prior over each weight in the neural network, governed by the same variance and an inverse Gamma prior over the observation variance yielding the model:\nwi \u223c N (0, 1/\u03b1) weights y|x,w, \u03c4 \u223c N (\u03c6(x,w), 1/\u03c4) output distribution\nwhere w = {wi} is the set of weights, and \u03c6(x,w) is a multi-layer perceptron that maps input x to output y as a function of parameters w. Both \u03b1 and \u03c4 are given Gamma(1, .1) priors. We denote the set of parameters as \u03b8 , (w,\u03b1, \u03c4). We approximate the posterior p(w,\u03b1, \u03c4 |D),\nwhere D is the training set of {xn, yn}Nn=1 input-output pairs. We then use the posterior predictive distribution to compute the distribution for a new input x\u2217\np(y|x\u2217,D) = \u222b p(y|x\u2217, \u03b8)p(\u03b8|D)d\u03b8 (7)\n\u2248 1 L L\u2211 `=1 p(y|x\u2217, \u03b8(`)) , \u03b8(`) \u223c p(\u03b8|D) (8)\nand report average predictive log probabilities for held out data, p(Y = y\u2217|x\u2217,D). For a dataset with input dimension P , the posterior has dimensionD = (P+2)\u00b750+3 (between D = 303 and D = 753 for the data sets considered).\nWe report held-out predictive performance for different approximate posteriors for six datasets. For each dataset, we perform the following training procedure 20 times. First, we create a random partition into a 90% training set and 10% testing set. We then apply VBoost, adding rank 5 components. We allow each additional component only 200 iterations. To save time on initialization, we draw 100 samples from the existing approximation, and initialize the new component with the sample with maximum weight. For comparison, Probabilistic back-propagation is given 1000 passes over the training data \u2014 empirically, sufficient for the algorithm to converge.\nTable 3 in the supplement presents out-of-sample log probability for single-component multivariate Gaussian approximations with varying rank structure. Table 1 presents outof-sample log probability for additional rank 5 components added using VBoost. We note that though we do not see much improvement as rank structure is added, we do see predictive improvement as components are added. Our results suggest that incorporating and adapting new mixture components is a recipe for a more expressive posterior approximation, translating into better predictive results. In fact, for all datasets we see that incorporating a new component improves test log probability, and we see further improvement with additional components for most of the datasets. Furthermore, in five of the datasets we see predictive performance surpass probabilistic back-propagation as new components are added. This highlights VBoost\u2019s ability to trade computation for improved accuracy. These empirical results suggest that augmenting a Gaussian approximation to include additional capacity can improve predictive performance in a BNN while retaining computational tractability."}, {"heading": "4.1. Comparison to NPVI", "text": "We also compare VBoost to nonparametric variational inference (NPVI) (Gershman et al., 2012), a similar mixture based black-box variational method. NPVI derives a tractable lower bound to the ELBO which is then approximately maximized. NPVI requires computing the Hessian\nof the model for the ELBO approximation, so we limit our comparison to the lower dimensional hierarchical models.\nWe also note that the NPVI ELBO approximation does not fully integrate the ln\u03c0(x) term against the variational approximation, q(x;\u03bb) when optimizing the mean parameters of the approximation components. When we applied NPVI to the baseball model, we discovered an instability in the optimization of these mean parameters (which we verified by finding that map optimization diverges). Black box VI, VBoost, and MCMC were not susceptible to this pathology. Consequently, we only compare NPVI to VBoost on the frisk model. Because NPVI uses diagonal components, we restrict VBoost to use purely diagonal components (r = 0). In Table 2 we show marginal likelihood lower bounds, comparing NPVI to VBoost with a varying number of components. Even with a single component, the NPVI objective tends to underperform. The NPVI component variance is spherical, limiting its capacity to represent posterior correlations. Further, NPVI is approximately optimizing a looser lower bound to the marginal likelihood. These two factors explain why NPVI fails to match MFVI and VBoost."}, {"heading": "5. Discussion and Conclusion", "text": "We proposed VBoost, a practical variational inference method that constructs an increasingly expressive posterior approximation and is applicable to a variety of Bayesian models. We demonstrated the ability of VBoost to learn rich representations of complex, high-dimensional posteriors on a variety of real world statistical models. One avenue for future work is incorporating flexible component distributions such as compositions of invertible maps (Rezende & Mohamed, 2015) or auxiliary variable variational models (Maal\u00f8e et al., 2016). We also plan to study approximation guarantees of the VBoost method and variance reduction techniques for our reparameterization gradient approach. Also, when optimizing parameters of a variational family, recent work has shown that the natural gradient can be more robust and lead to better optima (Hoffman et al., 2013; Johnson et al., 2016). Deriving and applying natural gradient updates for mixture approximations could make VBoost more efficient."}, {"heading": "Acknowledgements", "text": "The authors would like to thank Arjumand Masood, Mike Hughes, and Finale Doshi-Velez for helpful feedback. ACM is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. NJF is supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering and Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation."}], "references": [{"title": "Pattern Recognition and Machine Learning", "authors": ["C. Bishop"], "year": 2006}, {"title": "Variational inference: A review for statisticians", "authors": ["D.M. Blei", "A. Kucukelbir", "J.D. McAuliffe"], "venue": "arXiv preprint arXiv:1601.00670,", "year": 2016}, {"title": "Maximum likelihood from incomplete data via the EM algorithm", "authors": ["A.P. Dempster", "N.M. Laird", "D.B. Rubin"], "venue": "Journal of the Royal Statistical Society. Series B (methodological),", "year": 1977}, {"title": "Data analysis using Stein\u2019s estimator and its generalizations", "authors": ["B. Efron", "C. Morris"], "venue": "Journal of the American Statistical Association,", "year": 1975}, {"title": "Greedy function approximation: A gradient boosting machine", "authors": ["J.H. Friedman"], "venue": "Annals of Statistics,", "year": 2000}, {"title": "Data Analysis Using Regression and Multilevel/Hierarchical Models", "authors": ["A. Gelman", "J. Hill"], "year": 2006}, {"title": "An analysis of the NYPD\u2019s stop-and-frisk policy in the context of claims of racial bias", "authors": ["A. Gelman", "J. Fagan", "A. Kiss"], "venue": "Journal of the American Statistical Association,", "year": 2007}, {"title": "Nonparametric variational inference", "authors": ["S. Gershman", "M. Hoffman", "D.M. Blei"], "venue": "In International Conference on Machine Learning,", "year": 2012}, {"title": "Matrix Algebra from a Statistician\u2019s", "authors": ["D.A. Harville"], "venue": "Perspective. Springer-Verlag,", "year": 1997}, {"title": "Probabilistic backpropagation for scalable learning of Bayesian neural networks", "authors": ["J.M. Hern\u00e1ndez-Lobato", "R.P. Adams"], "year": 2015}, {"title": "The No-U-turn sampler: adaptively setting path lengths in Hamiltonian monte carlo", "authors": ["M.D. Hoffman", "A. Gelman"], "venue": "Journal of Machine Learning Research,", "year": 2014}, {"title": "Stochastic variational inference", "authors": ["M.D. Hoffman", "D.M. Blei", "C. Wang", "J.W. Paisley"], "venue": "Journal of Machine Learning Research,", "year": 2013}, {"title": "Improving the mean field approximation via the use of mixture distributions", "authors": ["T.S. Jaakkola", "M.I. Jordan"], "venue": "In Learning in Graphical Models,", "year": 1998}, {"title": "Composing graphical models with neural networks for structured representations and fast inference", "authors": ["M.J. Johnson", "D.K. Duvenaud", "A.B. Wiltschko", "S.R. Datta", "R.P. Adams"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "An introduction to variational methods for graphical models", "authors": ["M.I. Jordan", "Z. Ghahramani", "T.S. Jaakkola", "L.K. Saul"], "venue": "Machine learning,", "year": 1999}, {"title": "Adam: A method for stochastic optimization", "authors": ["D. Kingma", "J. Ba"], "venue": "In International Conference on Learning Representations,", "year": 2014}, {"title": "Auto-encoding variational Bayes", "authors": ["D.P. Kingma", "M. Welling"], "venue": "In International Conference on Learning Representations,", "year": 2014}, {"title": "Estimation of Mixture Models", "authors": ["Q. Li"], "venue": "PhD thesis,", "year": 1999}, {"title": "Mixture density estimation", "authors": ["Q.J. Li", "A.R. Barron"], "venue": "In Advances in Neural Information Processing Systems,", "year": 1999}, {"title": "Auxiliary deep generative models", "authors": ["L. Maal\u00f8e", "C.K. S\u00f8nderby", "S.K. S\u00f8nderby", "O. Winther"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Firefly Monte Carlo: Exact MCMC with subsets of data", "authors": ["D. Maclaurin", "R.P. Adams"], "venue": "In Uncertainty in Artificial Intelligence,", "year": 2014}, {"title": "Autograd: Reverse-mode differentiation of native python", "authors": ["D. Maclaurin", "D. Duvenaud", "R.P. Adams"], "venue": "ICML workshop on Automatic Machine Learning,", "year": 2015}, {"title": "Autograd: Reverse-mode differentiation of native Python, 2015b. URL http://github.com/HIPS/ autograd", "authors": ["D. Maclaurin", "D. Duvenaud", "M. Johnson", "R.P. Adams"], "year": 2015}, {"title": "Symmetrized variational inference", "authors": ["D.A. Moore"], "venue": "In NIPS Workshop on Advances in Approximate Bayesian Inferece,", "year": 2016}, {"title": "Gaussian variational approximation with factor covariance structure", "authors": ["Ong", "V.M.-H", "D.J. Nott", "M.S. Smith"], "venue": "arXiv preprint arXiv:1701.03208,", "year": 2017}, {"title": "Risk bounds for mixture density estimation", "authors": ["A. Rakhlin", "Panchenko", "Mukherjee"], "venue": "ESAIM: Probability and Statistics,", "year": 2006}, {"title": "Black box variational inference", "authors": ["R. Ranganath", "S. Gerrish", "D.M. Blei"], "venue": "In International Conference on Artificial Intelligence and Statistics,", "year": 2014}, {"title": "Operator variational inference", "authors": ["R. Ranganath", "J. Altosaar", "D. Tran", "D.M. Blei"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Hierarchical variational models", "authors": ["R. Ranganath", "D. Tran", "D.M. Blei"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Variational inference with normalizing flows", "authors": ["D. Rezende", "S. Mohamed"], "venue": "In International Conference on Machine Learning,", "year": 2015}, {"title": "Fixed-form variational posterior approximation through stochastic linear regression", "authors": ["T. Salimans", "D.A. Knowles"], "venue": "Bayesian Analysis,", "year": 2013}, {"title": "Gaussian covariance and scalable variational inference", "authors": ["M.W. Seeger"], "venue": "In International Conference on Machine Learning,", "year": 2010}, {"title": "Adagan: Boosting generative models", "authors": ["I. Tolstikhin", "S. Gelly", "O. Bousquet", "Simon-Gabriel", "C.-J", "B. Schoelkopf"], "venue": "arXiv preprint arXiv:1701.02386,", "year": 2016}, {"title": "Graphical models, exponential families, and variational inference", "authors": ["M.J. Wainwright", "M.I. Jordan"], "venue": "Foundations and Trends in Machine Learning,", "year": 2008}, {"title": "Bayesian learning via stochastic gradient Langevin dynamics", "authors": ["M. Welling", "Y.W. Teh"], "venue": "In International Conference on Machine Learning,", "year": 2011}, {"title": "Sequential greedy approximation for certain convex optimization problems", "authors": ["T. Zhang"], "venue": "IEEE Transactions on Information Theory,", "year": 2003}], "id": "SP:51ddc46563379cf0fcd96e7b8be62ce318771f9a", "authors": [{"name": "Andrew C. Miller", "affiliations": []}, {"name": "Nicholas J. Foti", "affiliations": []}, {"name": "Ryan P. Adams", "affiliations": []}], "abstractText": "We propose a black-box variational inference method to approximate intractable distributions with an increasingly rich approximating class. Our method, variational boosting, iteratively refines an existing variational approximation by solving a sequence of optimization problems, allowing a trade-off between computation time and accuracy. We expand the variational approximating class by incorporating additional covariance structure and by introducing new components to form a mixture. We apply variational boosting to synthetic and real statistical models, and show that the resulting posterior inferences compare favorably to existing variational algorithms.", "title": "Variational Boosting: Iteratively Refining Posterior Approximations"}