0	41	Speech recognition is one of the success stories of language technology.
1	19	It works remarkably well in a range of practical settings.
3	17	Humans are able to learn to recognize and understand speech from notably weaker and noisier supervision: they manage to learn to extract structure and meaning from speech by simply being exposed to utterances situated and grounded in their daily sensory experience.
7	24	Gelderloos and Chrupała (2016) use the image captioning dataset MS COCO (Lin et al., 2014) to mimic the setting of grounded language learning: the sensory input consists of images of natural scenes, while the language input are phonetically transcribed descriptions of these scenes.
17	30	We also release a dataset of synthetically spoken image captions based on MS COCO, available at https://doi.org/10.5281/zenodo.400926.
52	26	Recurrent neural networks are designed for modeling sequential data, and gated variants (GRUs, LSTMs) are widely used with speech and text in both cognitive modeling and engineering contexts.
57	54	The model consists of two parts: an utterance encoder, and an image encoder.
58	28	The utterance encoder starts from MFCC speech features, while the image encoder starts from features extracted with a VGG-16 pre-trained on ImageNet.
59	13	Our loss function attempts to make the cosine distance between encodings of matching utterances and images greater than the distance between encodings of mismatching utterance/image pairs, by a margin: (1) ∑ u,i (∑ u′ max[0, α+d(u, i)−d(u′, i)] + ∑ i′ max[0, α+ d(u, i)− d(u, i′)] ) where d(u, i) is the cosine distance between the encoded utterance u and encoded image i.
99	12	Five thousand images each are held out for validation and test.
101	17	For the MS COCO captions we extracted only plain MFCC and total energy features, and did not add deltas in order to keep the amount of computation manageable given the size of the dataset.
102	36	We evaluate our model on the task of ranking images given a spoken utterance, such that highly ranked images contain scenes described by the utterance.
104	12	We compare the speech models to models trained on written sentences split into words.
108	28	The Speech RHN model scores substantially higher than model of Harwath and Glass (2015) on the same data.
109	28	However the large gap between its perfomance and the scores of the text model suggests that Flickr8K is rather small for the speech task.
113	16	While the MS COCO text model is overall better than the speech model, there are cases where it outperforms the text model.
120	13	One of them is the 36-word- long utterance depicted in Figure 3, with ranks 470 and 2 for text and speech respectively.
121	15	We suspect that the speech model’s attention mechanism enables it to cherry pick key fragments of such monster utterances, while the text model lacking this mechanism may struggle.
124	14	Since the length of an utterance directly corresponds to how long it takes to articulate, we also use the number of time steps5 as a feature and expect it to provide the upper bound for our task, especially for synthetic speech.
125	36	We use a Ridge Regression model for predicting utterance length using each set of features.
146	12	All results using features extracted from the model are above chance (0.5), with the average unit activations of the hidden layers yielding the best results (0.65 for Flickr8K on layer 3, and 0.79 for COCO on layer 4).
148	29	The observed trend is similar to the previous task: average unit activations on the higher-level hidden layers are more informative for this task than the utterance embeddings, but the performance plateaus before the topmost layer.
155	47	We then correlate these cosine similarities with • semantic relatedness according to human ratings • cosine similarities according to z-score transformed embeddings from COCO Text RHN • edit similarities, a measure of how similar the sentences are in form, specifically, 1−normalized Levenshtein distance over character sequences Figure 6 shows a boxplot over 10,000 bootstrap samples for all correlations.
158	13	The overall growing correlation with both human semantic similarity ratings and the COCO Text RHN indicate that higher layers learn to represent semantic knowledge.
162	15	These increased monotonically, in support of our conjecture.
163	37	Next we simulate the task of distinguishing between pairs of homonyms, i.e. words with the same acoustic form but different meaning.
174	58	We present a multi-layer recurrent highway network model of language acquisition from visually grounded speech signal.
175	21	Through detailed analysis we uncover how information in the input signal is transformed as it flows through the network: formal aspects of language such as word identities that not directly present in the input are discovered and encoded low in the layer hierarchy, while semantic information is most strongly expressed in the topmost layers.
176	76	Going forward we would like to compare the representations learned by our model to the brain activity of people listening to speech in order to determine to what extent the patterns we found correspond to localized processing in the human cortex.
177	27	This will hopefully lead to a better understanding of language learning and processing by both artificial and neural networks.
