{"sections": [{"heading": "1. Introduction", "text": "In recent years, submodular functions (Fujishige, 2005) have been used to address an increasingly wide variety of problems in machine learning and artificial intelligence. This includes energy functions in probabilistic models (Kohli et al., 2007; Gotovos et al., 2015; Djolonga et al., 2016), influence in social network (Kempe et al., 2003; Mossel & Roch, 2007), crowd teaching (Singla et al., 2014), non-parametric\n1Google AI 2Kakao Mobility 3University of Washington, Seattle, work done while at Google AI. Correspondence to: Andrew Cotter <acotter@google.com>, Jeff Bilmes <bilmes@uw.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nBayesian estimation (Reed & Ghahramani, 2013), document and speech summarization (Lin et al., 2009; Lin & Bilmes, 2011; Li et al., 2012), image summarization (Tschiatschek et al., 2014; Singla et al., 2016), and clustering (Narasimhan et al., 2005).\nIn this paper, we introduce and apply a new submodular optimization problem related to partitioning, covering, and packing (Schrijver, 2003). Given a set function f : 2V \u2192 R+, it may be normalized (i.e., f(\u2205) = 0), monotone non-decreasing (i.e., f(A) \u2264 f(B) whenever A \u2286 B), and/or submodular (i.e., \u2200A,B \u2286 V , f(A) + f(B) \u2265 f(A \u222a B) + f(A \u2229 B)). A function g is said to be supermodular if \u2212g is submodular. A function m is modular if it is both submodular and supermodular. An m-partition of V is a set of m subsets, called blocks, that are non-intersecting (A\u03c0i \u2229A\u03c0j = \u2205 for all i 6= j) and covering (\u222aiA\u03c0i = V ). An m-covering is a set of blocks that is required only to be covering. In an m-covering, we might also have a multiplicity constraint which is expressed as a positive integer valued vector k = (kv : v \u2208 V ) where kv \u2208 Z+. To be a (k,m)covering, we must have an m-covering with no multiplicity violations, i.e., |{i \u2208 [m] : v \u2208 A\u03c0i }| \u2264 kv,\u2200v \u2208 V . A packing is a set of blocks that is required only to be nonintersecting. When we wish to refer collectively either to a partition, a covering, or a packing, we use the term grouping.\nGiven a finite set V of size n = |V |, a non-negative integer m \u2208 [1, n], and m monotone non-decreasing submodular functions fi : 2V \u2192 R+ for i \u2208 [m], the problem we study finds a feasible m-partition, or m-covering, or m-packing \u03c0 of V into m blocks A\u03c01 , A \u03c0 2 , . . . , A \u03c0 m that are \u201cgood\u201d in a way to be described below.\nFeasibility of our groupings is expressed using matroids, which are powerful combinatorial objects that can express many useful constraints over sets. A matroid (Oxley, 2006) M = (V, I) consists of a finite countable set V and a non-empty set of \u201cindependent\u201d subsets I = {I1, I2, . . .}, where Ii \u2286 V , that is down-closed (A \u2286 B \u2208 I \u21d2 A \u2208 I) and where all maximally independent sets have the same size (i.e., \u2200A,B \u2208 I with |A| < |B|, \u2203v \u2208 B \\ A having A+ v \u2208 I).\nFor a feasible grouping to be good, it must have several properties. First, the blocks in the grouping should both be individually highly diverse and also all be highly diverse\non average, where diversity is measured by the functions {fi}i\u2208[m]. For example, suppose the elements of the ground set include animal names: \u201cgoldfish\u201d and \u201ccarp\u201d have a similar meaning, as do \u201ccrow\u201d and \u201craven\u201d. But fish are very different from birds, so each of the sets {goldfish, crow} and {carp, raven} are diverse. Second, and more importantly, different blocks should not be redundant w.r.t. each other. This avoids the case where two different blocks convey the same information but in different ways, something that might happen even if the two blocks are disjoint. For example, the sets A1 = {goldfish, crow} and A2 = {carp, raven} are similar and hence redundant, despite being disjoint, and thus would be undesirable blocks when chosen together. This distinction between disjointedness and nonredundancy is particularly relevant in the context of submodular scoring functions where, as measured by a submodular function f , redundancy between A1 and A2 would mean that f(A1 \u222a A2) \u2248 f(A1) and/or f(A1 \u222a A2) \u2248 f(A2). We show below that this idea has a number of natural applications, one of which we evaluate in our case study section.\nThe paper is organized as follows. In the remainder of this section, we formally define our contributions (Section 1.1) and outline their utility in practice (Section 1.2). This section also defines objectives that, when constrainedly optimized, achieve our stated grouping goals. Section 2 places this paper in the context of previous work, and demonstrates that our contributions are novel. Section 3 formally outlines our approach and Section 4 details how we achieve cross-block diversity. Section 5 provides algorithms for constrainedly optimizing our objectives. Notably, we show a bi-criterion multiplicative approximation ratio guarantee for a fast polynomial time deterministic algorithm (Theorem 1), and also provide a randomized version of the algorithm giving a guarantee with high probability (using Lemma 4). The guarantees themselves are rather complex, and they are best appreciated in context, so we refer the reader to Theorem 1 and Lemma 4 for their statement. Lastly, Section 6 explores a case study application where we show our approach can be used to produce ensembles of machine learning models. We demonstrate that our approach improves on previous state-of-the-art results and moreover the groupings achieve the aforementioned desired properties."}, {"heading": "1.1. Contributions and Objectives", "text": "Our starting point is a recently introduced objective (Wei et al., 2015) that takes a convex combination of a robust and an average objective and finds a grouping \u03c0 that scores highly w.r.t.:\nFra(\u03c0) = \u03bb1 min i\u2208[m]\nfi(A \u03c0 i ) + \u03bb2 m \u2211 i\u2208[m] fi(A \u03c0 i ), (1)\nwhere the \u03bbis are non-negative coefficients and the fis may be distinct submodular functions. Our first contribution is\nthat, unlike Wei et al. (2015), which only handles partitions, we also handle coverings and packings. Our second contribution is the inclusion of more general block-specific constraints, expressed as intersections of matroids on an expanded ground set. For example, we may wish for blocks to not exceed a certain size, or for each block to correspond to a sub-tree of some graph (Section 3).\nOur third contribution, and the most significant, is the introduction of cross-block interaction terms, enabling us to avoid groupings containing pairs of blocks that jointly score poorly. Our final objective is:\nF (\u03c0) =Fra(\u03c0) + \u03bb3 min i,j\u2208[m],i<j\nFi,j(A \u03c0 i , A \u03c0 j ) (2)\n+ \u03bb4 1( m 2 ) \u2211 i,j\u2208[m],i<j Fi,j(A \u03c0 i , A \u03c0 j ).\nWhile there are four \u03bbis in this objective, typically only two\u2014one for scoring individual blocks, and the other for pairwise interactions\u2014will be nonzero. We interpret the extra cross-block terms Fi,j as rewarding inter-block diversity. For example, we could cause our objective function to prefer blocks with large pairwise symmetric differences by taking Fi,j(A\u03c0i , A \u03c0 j ) =\n\u2223\u2223A\u03c0i4A\u03c0j \u2223\u2223. Alternatively, in the partitioning or packing setting, we could define Fi,j(A \u03c0 i , A \u03c0 j ) = f(A \u03c0 i \u222a A\u03c0j ), in which case if there are two blocks A\u03c0i , A \u03c0 j with either f(A \u03c0 i \u222a A\u03c0j ) \u2248 f(A\u03c0i ) or f(A\u03c0i \u222aA\u03c0j ) \u2248 f(A\u03c0j ), then, under an interpretation of f as a diversity measure, the two blocks would be redundant, a situation we would prefer to avoid. We study several possible cross-block interactions, based on unions, intersections and symmetric differences, and {sub,super}modular functions thereof, and show that cross-block diversity preserves submodularity in an expanded ground set under various set-to-set mappings (Section 4).\nFinally, we offer an approach that reduces the above problem to either non-monotone (without robust terms, i.e. \u03bb1 = 0 and \u03bb3 = 0) or iterative monotone (with one robust term, i.e. only one of \u03bb1 or \u03bb3 is nonzero) submodular maximization subject to multiple matroid constraints (Section 5)."}, {"heading": "1.2. Applications", "text": "There are several applications in machine learning and data science that fit naturally into this setting, two of which we outline here.\nConstructing ensembles of machine learning models: Let V index a set of features, with subsets of V corresponding to subsets of features on which a model will be trained. The classical feature selection problem would be to choose a single set of features that result in a good model. We\u2019re interested instead in the problem of finding an ensemble of models, each trained on a different subset of features, that together achieve good performance (Canini et al., 2016).\nThis can be done by grouping V into A\u03c01 , A \u03c0 2 , . . . , A \u03c0 m, from which we form an ensemble of m models, the results of which are aggregated together e.g. by averaging, voting, or taking the minimum. Given a submodular function f : V \u2192 R+ measuring the \u201cquality\u201d of a single feature subset, one natural goal would be to choose each A\u03c0i to be individually high-quality, according to f . However, since the ensemble outputs are being aggregated, it would be purposeless to have redundant models\u2014many results (e.g. Kittler et al., 1998) suggest that when aggregating models, it is best for them to be as diverse as possible (so that the errors they make are independent, thereby improving accuracy and reducing variance). This motivates us to seek blocks that are as different from each other as possible. Individual model quality combined with diversity is exactly what maximizing Eq. (2) encourages. Our case study (Section 6) was performed in this setting and supports the benefits of aggregating diverse models.\nMultiple mutually diverse summaries: Data summarization involves finding a small but representative subset of a large set. There are some cases where it is useful to have multiple mutually diverse summaries, each of which is representative of the whole. For example, in parallel machine learning, where training data might need to be partitioned onto multiple machines distributed across a network, it can be useful to ensure that each subset is representative (so that local computations are accurate) but also diverse (since if two subsets are redundant, than so will the work that each processor performs). As another example, consider the problem of document summarization. It can be useful to produce multiple representative but distinct summaries of a collection of documents, as this ensures all concepts are covered but different perspectives are preserved."}, {"heading": "2. Previous Work", "text": "Special cases of our problem have previously been studied. For example, maximizing Eq. (1) with 1 = \u03bb1 = 1 \u2212 \u03bb2 over the space of all otherwise unconstrained partitions corresponds to the submodular fair allocation (SFA) problem. It is possible to achieve a O(1/( \u221a m log3m)) approximation (Asadpour & Saberi, 2010) via iteratively rounding an LP solution when the fi\u2019s are all modular, although the problem is NP-hard to 1/2 + approximate for any > 0 (Golovin, 2005). For submodular fi\u2019s, (Golovin, 2005) also gives a matching-based algorithm with a factor 1/(n \u2212m + 1) approximation. A binary search algorithm (Khot & Ponnuswami, 2007) has a better factor of 1/(2m \u2212 1) that is independent of n. A less practical approach uses an ellipsoid approximation (Goemans et al., 2009) of each submodular function and reduces SFA to its modular version yielding an approximation factor of 1/( \u221a nm1/4 log n log3/2m). (Wei et al., 2015) shows that\na greedy algorithm has a 1/m approximation when all fi\u2019s are the same. Fair allocation problems are also studied with sometimes non-submodular objectives (Ghodsi et al., 2017). Maximizing Eq. (1) with 0 = \u03bb1 = 1 \u2212 \u03bb2 over the space of all partitions corresponds to the submodular welfare problem (SW), which can be reduced to submodular maximization on an expanded ground set under a partition matroid constraint (Vondra\u0301k, 2008) using a greedy algorithm, an approach having a 1/2 guarantee (Fisher et al., 1978). The multi-linear extension of a submodular function can be used in a continuous greedy approach that solves SW with a (1\u2212 1/e) tight approximation factor (Vondra\u0301k, 2008). When \u03bb1 > 0, \u03bb2 > 0, (Wei et al., 2015) offers two approaches. The first takes the best of the two solutions computed under \u03bb1 = 1 = 1 \u2212 \u03bb2 and \u03bb1 = 0 = 1 \u2212 \u03bb2 to provide a max( \u03b2\u03b1(\u03bb1)\u03b2+\u03b1 , \u03bb2\u03b2) guarantee, where \u03b1 is the approximation factor for the SFA problem and \u03b2 the factor for the SW problem. A second binary-search approach, the inspiration for Algorithm 1, finds a partition whose block objective value is at least max( \u03b41\u2212\u03b1+\u03b4 , \u03bb2\u03b1)(OPT\u2212 ) for an \u03b1\u2212\u03b4 fraction of the blocks, where \u03b1 is the approximation factor of a SW solver and 0 < \u03b4 < \u03b1.\nThe novelty of our optimization problem is that: (i) we may form not only partitions but also (k,m)-coverings and packings; (ii) we utilize a set of m matroids to define the feasibility of the individual blocks in a grouping; and (iii) we explicitly incorporate cross-block interaction terms."}, {"heading": "3. Approach", "text": "As with the strategy for the submodular welfare problem (Vondra\u0301k, 2008), our approach to maximizing Eq. (2) starts by defining an expanded ground set V\u00d7 (Figure 1), consisting of m disjoint unions of the original ground set V , i.e., the product set, defined as:\nV\u00d7 , m\u228e i=1 V (i) = \u228e v\u2208V R(v) = {(v, i) : v \u2208 V, i \u2208 [m]}\nwhere |V\u00d7| = nm and where ] is the disjoint union operator. V\u00d7 can be viewed as indexing into a n \u00d7m matrix with V (i) (isomorphic to V ) being the ith column, and R(v) (isomorphic to [m]) being the vth row.\nWe also define a mapping from subsets S \u2286 V\u00d7 to the original ground set, and another mapping that selects the original ground set elements corresponding to those in the ith column as follows:\nabs (S) ,{v \u2208 V : \u2203i \u2208 [m] with (v, i) \u2208 S} col (S, i) , abs ( S \u2229 V (i) ) Given S \u2286 V\u00d7, a grouping \u03c0 is obtained by setting A\u03c0i = col (S, i) for all i \u2208 [m].\nUsing these mappings, we will ultimately (Eq. (5)) define a new objective F\u00d7 : 2V\n\u00d7 \u2192 R on the expanded ground set that produces the valuation of a set S \u2286 V\u00d7 indirectly, via submodular functions defined on the original ground set, using col (S, i) to map subsets of V\u00d7 to subsets of V ."}, {"heading": "3.1. Partitionings, Packings, and Coverings", "text": "During optimization, we will take the feasible set F\u00d7 \u2286 2V \u00d7\nto be the intersection of the independent sets of zero or more matroids over V\u00d7. Such matroid independence constraints can be used to ensure that any feasible solution maps back to a partitioning, packing, or covering over V . When we wish for a partition, we can maximize F\u00d7 subject to a partition matroid on V\u00d7 whose independent sets are defined based on the \u201crows\u201d. That is, the independent sets are defined as follows:\nIk = { S \u2286 V\u00d7 : \u2200v \u2208 V, \u2223\u2223\u2223S \u2229R(v)\u2223\u2223\u2223 \u2264 kv}, (3) where k = (kv1 , kv2 , . . . , kvn) and \u2200v, kv = 1. In words, at most one \u201ccopy\u201d of each element of the original ground set may be present. To express (k,m)-covering constraints on V , we allow \u2200v, kv \u2265 1. A covering and partition is obtained when maximizing a monotone F\u00d7, since any candidate solution that is not yet a covering or partition can be made so by adding elements until all constraints are met with equality.\nLikewise, a packing constraint can be expressed using a `-uniform matroid with independent sets:\nI` = { S \u2286 V\u00d7 : |S| \u2264 ` } . (4)\nIf we set F\u00d7 = I` \u2229Ik, where k = 1 is the vector of all 1s, this expresses a packing constraint, and is the intersection of two matroids defined on V\u00d7. In fact, the intersection of a matroid and a `-uniform matroid is still a single matroid, called its `-truncation (Schrijver, 2003), and hence I` \u2229 Ik constitutes only a single matroid."}, {"heading": "3.2. Block Constraints", "text": "Having discussed how matroid constraints on the rows of V\u00d7 can be used to express the partitioning, packing and covering problems, we now turn our attention to how matroid constraints on the columns can be used to represent more general constraints on the blocks. Imagine that each block is required to satisfy its own matroid independence constraint: we are given m matroids {(V, Ii)}i\u2208[m] with independent sets Ii for i \u2208 [m], where each matroid is defined over the original ground set V . Using the expanded ground set and taking S \u2286 V\u00d7, we have that A\u03c0i \u2208 Ii if and only if col (S, i) \u2208 Ii.\nGiven a size-m set of matroids {Mi}i\u2208[m] where Mi = (V, Ii), the matroid union theorem (Schrijver, 2003, Thm. 42.1a) states that a new matroid can be defined on V\u00d7 with independent sets Ib = {I1 ] I2 ] . . . ] Im : Ii \u2208 Ii,\u2200i \u2208 [m]}. Despite there being a matroid for each block, the disjoint union of these matroids is a single matroid on V\u00d7.\nOne of the simplest examples of such constraints, and the one that we use in our case study (Section 6), simply places an upper bound on the number of elements within each block. The resulting matroid is a column-based analogue of Eq. (3)."}, {"heading": "4. Cross-block Interaction", "text": "In order to define the expanded objective F\u00d7 in terms of submodular functions on the original ground set V , we will define set functions via mappings from subsets of an expanded ground set to subsets of the original ground set. The next result shows that in some cases, composition and setto-set mappings preserve submodularity or supermodularity.\nLemma 1. Let V \u2032, V be two ground sets and define a set-toset mapping functionG : 2V\n\u2032 \u2192 2V . Also, let f : 2V \u2192 R+ be monotone non-decreasing and submodular, and let g : 2V \u2192 R+ be monotone non-decreasing and supermodular. Then:\n1. If G is monotone non-decreasing (i.e. G(S) \u2286 G(T ) whenever S \u2286 T ), then f \u25e6 G and g \u25e6 G are both\nmonotone non-decreasing.1\n2. If \u2200S, T \u2286 V \u2032, G(S \u222aT ) = G(S)\u222aG(T ) and G(S \u2229 T ) \u2286 G(S) \u2229 G(T ), then f \u25e6 G : 2V \u2032 \u2192 R+ is submodular.\n3. If \u2200S, T \u2286 V \u2032, G(S \u222aT ) \u2287 G(S)\u222aG(T ) and G(S \u2229 T ) = G(S) \u2229 G(T ), then g \u25e6 G : 2V \u2032 \u2192 R+ is supermodular.\nProof. In Appendix C. The objective defined in Eq. (2) involves cross block interaction terms via Fi,j(A\u03c0i , A \u03c0 j ) for all i, j \u2208 [m]. The ground set expansion defined in Section 3, combined with the above lemma, surprisingly allows many such interaction terms to be handled in a way that preserves submodularity. To this end, we define three additional set-to-set (i.e. 2V \u00d7 \u2192 2V ) mappings corresponding to union, intersection, or symmetric difference of the ith and jth mapped subsets:\nG i,j (S) , col (S, i) col (S, j)\nwhere = \u222a,\u2229 or4. The following lemma, which largely follows from Lemma 1, shows that these can be used in a way that preserves sub/supermodularity:\nLemma 2. Let f : 2V \u2192 R be monotone non-decreasing submodular, m : 2V \u2192 R be non-negative modular, and g : 2V \u2192 R be monotone non-decreasing supermodular. Then f \u25e6G\u222ai,j : 2V\n\u00d7 \u2192 R is monotone non-decreasing submodular, g \u25e6G\u2229i,j : 2V\n\u00d7 \u2192 R is monotone non-decreasing supermodular, and m \u25e6 G4i,j : 2V\n\u00d7 \u2192 R is non-negative submodular.\nProof. In Appendix C.\nIf one wishes to reduce the number of common elements in pairs of blocks, a useful cross-block interaction term is |V | \u2212 \u2223\u2223G\u2229i,j (S)\u2223\u2223. Because the cardinality function is non-decreasing and modular,\n\u2223\u2223G\u2229i,j (S)\u2223\u2223 is non-decreasing supermodular by Lemma 2. A submodular function (including the constant |V |) minus a supermodular function is submodular, and thus, the above interaction function is submodular and non-increasing.\nIn the partitioning and packing settings, if we have a nondecreasing submodular function f that measures the diversity of a set, one could use f ( G\u222ai,j (S) ) , which is both submodular and non-decreasing, as the cross-block interaction term, to encourage pairwise diversity. If one wants blocks to have large pairwise differences, then a natural cross-block interaction term is |G4i,j (S)|. This function is again submodular, but is non-monotone (it is neither non-increasing nor non-decreasing).\n1Note that \u201c\u25e6\u201d denotes function composition."}, {"heading": "4.1. Submodular Approximation of f \u25e6G4i,j", "text": "Unfortunately, f \u25e6G4i,j is not necessarily submodular, even for a submodular f , and has no obvious difference representation (Iyer & Bilmes, 2012). However, we can derive (non-monotone) submodular bounds based on the curvature. A normalized monotone submodular function f : 2V \u2192 R has curvature c if f(v|S) \u2265 (1 \u2212 c)f(v) for all S \u2286 V and V 3 v /\u2208 S, where f(v|S) , f(S \u222a {v}) \u2212 f(S) is the gain. Curvature is easily computed in O(n) time since c = 1 \u2212minv\u2208V f(v|V \\ v)/f(v), where c \u2208 [0, 1]. Modular functions have c = 0, fully curved functions have c = 1, and submodular function classes can be restricted to those having a particular c, since many useful submodular functions have non-extreme curvature, e.g. sums of nonasymptoting concave functions composed with non-negative modular functions (Stobbe & Krause, 2010).\nGiven a set X \u2286 V , a normalized monotone submodular function f with curvature c, and any ordering \u03c3 = (\u03c31, \u03c32, . . . , \u03c3n) of V such that X = { \u03c31, \u03c32, . . . , \u03c3|X| } , a modular subgradient mXf of f can be obtained (Fujishige, 2005) where mXf (X) = f(X), \u2200Y,mXf (Y ) \u2264 f(Y ), and where mXf (\u03c3i) = f(\u03c3i|\u03c31, \u03c32, . . . , \u03c3i\u22121). Since f is monotone, the subgradient is non-negative. Also, for any Y , submodularity ensures that mXf (v) \u2265 (1 \u2212 c)f(v) and hence f(Y ) \u2265 mXf (Y ) \u2265 (1 \u2212 c)f(Y ) for any X,Y , where the second inequality follows since f is normalized submodular. This enables us to obtain a non-monotone submodular lower bound via f \u25e6G4i,j(S) \u2265 mXf \u25e6G 4 i,j(S) \u2265 (1\u2212c)f \u25e6G 4 i,j(S) for any X \u2208 V that approximates the original problem: Lemma 3. Given any algorithm that produces a solution S\u0302 having the property that F (S\u0302) + mXf \u25e6 G 4 i,j(S\u0302) \u2265\n\u03b1maxS\u2208F\u00d7 ( F (S) +mXf \u25e6G 4 i,j(S) ) for \u03b1 > 0, then S\u0302 also has the property that F (S\u0302) + f \u25e6 G4i,j(S\u0302) \u2265 \u03b1(1 \u2212 c) maxS\u2208F\u00d7 ( F (S) + f \u25e6G4i,j(S) ) = \u03b1(1\u2212 c)OPT.\nProof. In Appendix C."}, {"heading": "5. Algorithms and Optimization", "text": "Eq. (2) can now be written in terms of the expanded ground set as F\u00d7 : 2V \u00d7 \u2192 R:\nF\u00d7(S) = (5)\n\u03bb1 min i fi (col (S, i)) + \u03bb2 m \u2211 i\u2208[m] fi (col (S, i))\n+ \u03bb3 min i,j\u2208[m],i<j F\u00d7i,j(S) + \u03bb4( m 2 ) \u2211 i,j\u2208[m],i<j F\u00d7i,j(S)\nOur approach in maximizing Eq. (2) subject to grouping constraints is to optimize Eq. (5) subject to the intersection\nAlgorithm 1 Adaptation of the GeneralGreedSAT algorithm of Wei et al. (2015) to handle matroid constraints, crossblock interactions, and coverings/packings as well as partitions. The functions f\u00d71 , . . . , f \u00d7 m : 2\nV\u00d7 \u2192 R are monotone non-decreasing, non-negative and submodular, \u03b7f\u00d7 is a uniform upper bound on the f \u00d7 i \u2019s, each I1, . . . , Ik \u2286 2V \u00d7 is the set of independent sets of a matroid on V\u00d7, and CALLBACK is a helper function that returns an \u03b1-approximation to the submodular maximization problem argmaxS\u2208\u22c2ki=1 Ii F\u00d7c (S) in polynomial time.\nBisection ( , V\u00d7,m, f\u00d71 , . . . , f \u00d7 m, \u03b7f\u00d7 , k, I1, . . . , Ik,CALLBACK, \u03b1 ) :\n1 Define F\u00d7c (S) = (\u2211m i=1 min { c, f\u00d7i (S) }) /m 2 Initialize cmin = 0, cmax = \u03b7f\u00d7 , S = \u2205 3 While cmax \u2212 cmin > : 4 Let c = (cmax \u2212 cmin) /2 and Sc = CALLBACK (V\u00d7, F\u00d7c , k, I1, . . . , Ik) 5 If F\u00d7c (Sc) < \u03b1c, then let cmax = c, else let cmin = c and S = Sc 6 Return S\nof multiple matroid constraints defined on the expanded ground set. The matroid constraints ensure that the solution can be transformed back to the original ground set, while preserving the approximation ratio. The algorithm used depends both on which terms are present (i.e. have nonzero associated \u03bbs), and whether the submodular functions are monotone.\nWhen the overall objective is non-monotone submodular (so \u03bb1 = 0 and \u03bb3 = 0), the problem of maximizing Eq. (5) becomes one of non-monotone submodular maximization subject to matroid constraints. One can use algorithms such as Lee et al. (2010); Ward (2012), or the more recent, faster, and scalable approach given in Feldman et al. (2017). When using f \u25e6G4i,j(S) as a non-submodular block pair reward for non-robust coverings, a modular approximation adjusts any guarantees by 1 \u2212 c (Lemma 3). The non-robust packing or partitioning problem reduces to monotone submodular maximization subject to two matroid constraints, for which there are a variety of good solutions. For example, the efficient greedy algorithm (Nemhauser & Wolsey, 1978) solves this problem with a 1/3 guarantee while more recent but also more complicated approaches, such as Ward (2012), can solve this with an approximation ratio of (k + 3)/2 + for ` matroids (here k = 2).\nWhen all of the involved submodular functions are monotone non-decreasing, a single robust term can be handled (one of \u03bb1 or \u03bb3 may be nonzero). Here, we are inspired by an approach originally used for robust submodular optimization (Krause et al., 2008) where the goal is to find the max of the min over a set of submodular functions subject to a cardinality constraint. In Wei et al. (2015), this was extended to apply to a mixed robust/average objective over partitions. It turns out that essentially the same idea\u2014this is Algorithm 1\u2014applies to the more general case of coverings and packings, as well as when there are matroids constraining each block individually, and also when the blocks\u2019 scores interact.\nIn brief, this algorithm proceeds by iteratively optimizing inner submodular optimization problems using a provided CALLBACK function, which is assumed to run in polynomial time, and return an \u03b1-approximation. The final algorithm achieves a nearly constant-factor approximation for a constant fraction of the blocks. As the fraction of the blocks shrinks, the guarantee for those blocks grows, and the guarantee holds simultaneously for a range over the fractions.\nTheorem 1. A call to Algorithm 1 will perform dlog2 (\u03b7f/ )e calls to CALLBACK, each of which will perform polynomially many evaluations of F\u00d7c , each of which evaluates all m f\u00d7i s once.\nThe resulting set S will satisfy the constraints, and for any \u03b3 \u2208 (0, \u03b1) there will exist at least dm (\u03b1\u2212 \u03b3) / (1\u2212 \u03b3)e indices i \u2208 [m] for which f\u00d7i (S) > \u03b3 (OPT\u2212 ), where OPT = maxS\u2208\u22c2ki=1 Ii F\u00d7 (S) is the optimum. Proof. The proof technique follows that of Wei et al. (2015, Theorem 11), but we include it in Appendix C for completeness, and to show that it applies to our more general case (i.e., covers, packings, block-specific matroid constraints, and cross-block interactions).\nThis algorithm depends on a CALLBACK that deterministically returns an \u03b1-approximation to an inner submodular optimization problem. However, many submodular maximization algorithms are randomized, and have approximation guarantees that hold only in expectation. With a bit of extra work, such an algorithm can be used as well. First, we must convert the in-expectation guarantee into a high-probability guarantee using the following lemma that requires only an approximation bound:\nLemma 4. Let A be a randomized algorithm for submodular maximization that has an \u03b1-approximation guarantee in expectation, i.e. for which E [f (S)] \u2265 \u03b1f (S\u2217), where f is the submodular function we wish to maximize, S is the result of algorithm A, and S\u2217 is the maximizer of f . For\nparameters \u03b2, \u03b4 \u2208 (0, 1), suppose that we run algorithm A k times, where k = \u2308( ln 1\u03b4 ) / ( ln 1\u2212\u03b1\u03b21\u2212\u03b1 )\u2309 , yielding results S1, S2, . . . , Sk. Take S = argmaxSi:i\u2208[k] f (Si) to be the best of these results. Then S will have an approximation ratio of \u03b1\u03b2, i.e. f (S) \u2265 \u03b1\u03b2f (S\u2217), with probability 1\u2212 \u03b4.\nProof. In Appendix C.\nAs was shown in Theorem 1, CALLBACK will be called at most dlog2 (\u03b7f/ )e times, so it follows from the union bound that if we use the procedure of Lemma 4, then, with probability 1 \u2212 \u03b4 dlog2 (\u03b7f/ )e, every call to CALLBACK will return an \u03b1\u03b2-approximation, and the result of Theorem 1 will hold (with \u03b1\u03b2 substituted for \u03b1).\nThe ability to handle a mixed robust/average objective, however, comes at a cost. Because Theorem 1 only applies when all of the involved submodular functions are monotone non-decreasing, we cannot use the intersection and symmetric-difference-based cross-block interaction terms\u2014 only the union-based terms are possible. Non-monotone interaction terms can only be used with a non-robust objective. Finding an algorithm that can handle both robustness and non-monotone interactions is therefore an interesting open problem that we leave to future work."}, {"heading": "6. Case Study", "text": "We validate our proposed approach with a case study in the setting of Canini et al. (2016), in which the task is to construct an ensemble-of-lattices machine learning model. Each lattice model (Gupta et al., 2016) in the ensemble is defined on a subset of the features\u2014intuitively, two features interact non-linearly if they are included in the same lattice, and interact only linearly if they are not\u2014so our primary goal is to choose subsets of features that interact well with each other, with our secondary goal being to reduce redundancy in pairs of subsets. Notice that this is not a feature selection problem\u2014typically, every feature will be included in at least one lattice\u2014the task is to determine which features should interact non-linearly. For more details, please see Appendix A.\nOur goals are to demonstrate that (i) we can successfully find good approximate maximizers of the proposed objective function, and (ii) the inclusion of pairwise diversity terms results in improved diversity.\nWe compare to two baselines, the \u201cCrystals\u201d and \u201cRandom Tiny Lattice (RTL)\u201d algorithms of Canini et al. (2016). The first of these\u2014the current state-of-the-art\u2014is essentially a heuristic for choosing diverse ensembles, while the second simply chooses each lattice\u2019s features uniformly at random.\nThe dataset contains 463 154 samples with 29 informative\nfeatures plus a binary label indicating whether a particular visual element should be displayed on a web page. The dataset was randomly partitioned into training, validation and testing subsets containing 80%, 10% and 10% of the data, respectively (the validation set was only used for hyperparameter optimization of the baseline Crystals algorithm)."}, {"heading": "6.1. Choice of f", "text": "Our ground set V consists of the n = 29 features. We began by finding a submodular function f : 2V \u2192 R+ for which f (S) represents roughly how well a single lattice model on the features in S would perform. To this end, we chose f to have the form f (S) = \u03b2 + \u2211 A\u2208A \u03b1A \u221a |A \u2229 S|, where A consists of all 1- and 2-element subsets of V . Observe that \u221a |A \u2229 S| is submodular, non-negative, and monotone non-decreasing, so if \u03b2 and \u03b1A are non-negative, then f will likewise be submodular, non-negative, and monotone.\nBased on 9 191 random subsets of sizes between two and ten, we learned the \u03b2 and \u03b1A parameters to minimize the squared error between f (S) and the training accuracy of a lattice model trained on the features contained in S. The result is the f that we use throughout."}, {"heading": "6.2. Covering", "text": "Our goal here is essentially identical to Canini et al. (2016)\u2014 we seek to choose m = 8 lattices, each containing up to 8 features (via a matroid constraint), by finding the S \u2286 V\u00d7 maximizing:\u2211\ni\u2208[m]\nf (col (S, i)) + \u03bb4 |V | \u2211 i,j\u2208[m]\u2227i 6=j \u2223\u2223\u2223G4i,j(S)\u2223\u2223\u2223 (6) The 8 lattices together should have good performance (the first term), and the feature subsets should be relatively pairwise distinct, i.e. have large symmetric differences (this is the second term). We henceforth refer to the first (intrablock diversity) term, representing the individual quality of the lattices, as the \u201cquality\u201d term, and the second (not including the \u03bb4-scaling), representing the inter-block diversity, as the \u201cdiversity\u201d term.\nWe optimized Eq. (6) for various choices of \u03bb4 using the randomized algorithm of Feldman et al. (2017) combined with the procedure of Lemma 4, with \u03b2 = 0.5 and \u03b4 = 0.1. Each optimization took between 2 and 30 seconds on a Xeon E5-2690. The results are shown in Figure 2. The left-hand plot shows that, as the trade-off parameter \u03bb4 increases, the relative magnitude of the quality term decreases, and of the diversity term increases, as expected. The right-hand plot shows that, when the \u03bb4 parameter is sufficiently large, the diversity term is \u201cbalanced\u201d with the quality term (or is larger), and the ensembles found by our approach outperform those of both the state-of-the-\nart Crystals algorithm (which uses heuristics to encourage diversity) and the 90th percentile of RTLs, albeit by a small amount. More importantly, the leftmost points in the righthand plot of Figure 2, in which the diversity portion of the objective is essentially zero, have significantly worse testing accuracies than those with larger \u03bb4s. This indicates that the use of pairwise diversity terms may be broadly beneficial to submodular grouping problems."}, {"heading": "6.3. Partitioning and Packing", "text": "Appendix B contains additional case studies exploring the efficacy of the mixed robust/average objective for partitioning and packing, where the task is to maximize:\u2211\ni\u2208[m]\nf (col (S, i)) + \u03bb3 min i,j\u2208[m]\u2227i 6=j\nf ( G\u222ai,j(S) ) (7)\nUnlike in Eq. (6), the \u201cdiversity\u201d term is a minimum over monotone non-decreasing submodular functions, instead of a sum over non-monotone submodular functions. The results demonstrate that Algorithm 1 is effective at optimizing this objective, but also reveal that, for this problem and data set, the above diversity term is not helpful\u2014and can be harmful if the quality term is overpowered. The lesson is that cross-block diversity is not a magic bullet\u2014it must be chosen appropriately for the problem."}, {"heading": "6.4. Discussion", "text": "We have introduced a new class of submodular optimization problems involving grouping ground elements together into multiple sets and the first, as far as we know, to involve\nblock-block interaction terms as well as general (matroid intersection) block constraints.\nAnother potential application of our method is sensitivity analysis of machine learning systems (i.e., does an ML model vary greatly when trained on different representative but mutually diverse subsets of the training data?) and also a form of robustness analysis (i.e., how does an ML system perform when tested on different representative but mutually diverse subsets of test data?). These are important questions, considering for example the recent interest in adversarial examples in ML.\nAlso, since convex combinations of submodular components preserve submodularity, it might also in the future be interesting to consider some form of Pareto frontier of solution sets for different convex mixtures.\nAcknowledgments: This work was done in part during a time when author Bilmes was visiting Google AI research and also visiting the Simons Institute for the Theory of Computing. This material is based upon work supported by the National Science Foundation under Grant No. IIS1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, a Facebook, and an Intel research award. This work was supported in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA"}], "year": 2018, "references": [{"title": "An approximation algorithm for max-min fair allocation of indivisible goods", "authors": ["A. Asadpour", "A. Saberi"], "venue": "In SICOMP,", "year": 2010}, {"title": "Fast and flexible monotonic functions with ensembles of lattices", "authors": ["K. Canini", "A. Cotter", "M. Gupta", "M. Milani Fard", "J. Pfeifer"], "venue": "In NIPS,", "year": 2016}, {"title": "Variational inference in mixed probabilistic submodular models", "authors": ["J. Djolonga", "S. Tschiatschek", "A. Krause"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Greed is good: Near-optimal submodular maximization via greedy optimization", "authors": ["M. Feldman", "C. Harshaw", "A. Karbasi"], "venue": "arXiv preprint arXiv:1704.01652,", "year": 2017}, {"title": "An analysis of approximations for maximizing submodular set functions\u2014 II", "authors": ["M. Fisher", "G. Nemhauser", "L. Wolsey"], "venue": "In Polyhedral combinatorics,", "year": 1978}, {"title": "Submodular functions and optimization, volume 58", "authors": ["S. Fujishige"], "year": 2005}, {"title": "Fair allocation of indivisible goods: Improvement and generalization", "authors": ["M. Ghodsi", "M. HajiAghayi", "M. Seddighin", "S. Seddighin", "H. Yami"], "venue": "arXiv preprint arXiv:1704.00222,", "year": 2017}, {"title": "Approximating submodular functions everywhere", "authors": ["M. Goemans", "N. Harvey", "S. Iwata", "V. Mirrokni"], "venue": "In SODA,", "year": 2009}, {"title": "Max-min fair allocation of indivisible goods", "authors": ["D. Golovin"], "venue": "Technical Report CMU-CS-05-144,", "year": 2005}, {"title": "Sampling from probabilistic submodular models", "authors": ["A. Gotovos", "S.H. Hassani", "A. Krause"], "venue": "In Neural Information Processing Systems (NIPS),", "year": 2015}, {"title": "Monotonic calibrated interpolated look-up", "authors": ["M.R. Gupta", "A. Cotter", "J. Pfeifer", "K. Voevodski", "K. Canini", "A. Mangylov", "W. Moczydlowski", "A. van Esbroeck"], "venue": "tables. JMLR,", "year": 2016}, {"title": "Algorithms for approximate minimization of the difference between submodular functions, with applications", "authors": ["R.K. Iyer", "J.A. Bilmes"], "venue": "CoRR, abs/1207.0560,", "year": 2012}, {"title": "Maximizing the spread of influence through a social network", "authors": ["D. Kempe", "J. Kleinberg", "\u00c9. Tardos"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2003}, {"title": "Approximation algorithms for the max-min allocation problem", "authors": ["S. Khot", "A. Ponnuswami"], "venue": "In APPROX,", "year": 2007}, {"title": "On combining classifiers", "authors": ["J. Kittler", "M. Hatef", "R.P.W. Duin", "J. Matas"], "venue": "IEEE Trans. Pattern Anal. Mach. Intell.,", "year": 1998}, {"title": "Robust submodular observation selection", "authors": ["A. Krause", "B. McMahan", "C. Guestrin", "A. Gupta"], "venue": "In JMLR,", "year": 2008}, {"title": "Submodular maximization over multiple matroids via generalized exchange properties", "authors": ["J. Lee", "M. Sviridenko", "J. Vondr\u00e1k"], "venue": "Math. Oper. Res.,", "year": 2010}, {"title": "Multi-document summarization via submodularity", "authors": ["J. Li", "L. Li", "T. Li"], "venue": "Applied Intelligence,", "year": 2012}, {"title": "Graph-based submodular selection for extractive summarization", "authors": ["H. Lin", "J. Bilmes", "S. Xie"], "venue": "In Automatic Speech Recognition & Understanding,", "year": 2009}, {"title": "On the submodularity of influence in social networks", "authors": ["E. Mossel", "S. Roch"], "venue": "In Proceedings of the thirty-ninth annual ACM symposium on Theory of computing,", "year": 2007}, {"title": "Best algorithms for approximating the maximum of a submodular set function", "authors": ["G. Nemhauser", "L. Wolsey"], "venue": "Mathematics of Operations Research,", "year": 1978}, {"title": "Matroid theory", "authors": ["J. Oxley"], "year": 2006}, {"title": "Scaling the Indian buffet process via submodular maximization", "authors": ["C. Reed", "Z. Ghahramani"], "year": 2013}, {"title": "Combinatorial optimization: polyhedra and efficiency, volume 24", "authors": ["A. Schrijver"], "year": 2003}, {"title": "Near-optimally teaching the crowd to classify", "authors": ["A. Singla", "I. Bogunovic", "G. Bartok", "A. Karbasi", "A. Krause"], "venue": "In ICML, pp", "year": 2014}, {"title": "Noisy submodular maximization via adaptive sampling with applications to crowdsourced image collection summarization", "authors": ["A. Singla", "S. Tschiatschek", "A. Krause"], "venue": "In Thirtieth AAAI Conference on Artificial Intelligence,", "year": 2016}, {"title": "Efficient minimization of decomposable submodular functions", "authors": ["P. Stobbe", "A. Krause"], "venue": "In NIPS,", "year": 2010}, {"title": "Learning mixtures of submodular functions for image collection summarization", "authors": ["S. Tschiatschek", "R.K. Iyer", "H. Wei", "J.A. Bilmes"], "venue": "In Advances in neural information processing systems,", "year": 2014}, {"title": "Optimal approximation for the submodular welfare problem in the value oracle model", "authors": ["J. Vondr\u00e1k"], "venue": "In STOC,", "year": 2008}, {"title": "A (k+ 3)/2-approximation algorithm for monotone submodular k-set packing and general k-exchange systems", "authors": ["J. Ward"], "venue": "In LIPIcs-Leibniz International Proceedings in Informatics,", "year": 2012}, {"title": "Mixed robust/average submodular partitioning: Fast algorithms, guarantees, and applications", "authors": ["K. Wei", "R.K. Iyer", "S. Wang", "W. Bai", "J.A. Bilmes"], "venue": "In NIPS,", "year": 2015}], "id": "SP:1d1c92017d595ca0af15205b2016c542fb3fa182", "authors": [{"name": "Andrew Cotter", "affiliations": []}, {"name": "Mahdi Milani Fard", "affiliations": []}, {"name": "Seungil You", "affiliations": []}, {"name": "Maya Gupta", "affiliations": []}, {"name": "Jeff Bilmes", "affiliations": []}], "abstractText": "We introduce the problem of grouping a finite set V into m blocks where each block is a subset of V and where: (i) the blocks are individually highly valued by a submodular function f (both robustly and in the average case) while satisfying block-specific matroid constraints; and (ii) block scores interact where blocks are jointly scored highly via f , thus making the blocks mutually non-redundant. Submodular functions are good models of information and diversity; thus, the above can be seen as grouping V into matroid constrained blocks that are both intraand inter-diverse. Potential applications include forming ensembles of classification/regression models, partitioning data for parallel processing, and summarization. In the non-robust case, we reduce the problem to non-monotone submodular maximization subject to multiple matroid constraints. In the mixed robust/average case, we offer a bicriterion guarantee for a polynomial time deterministic algorithm and a probabilistic guarantee for randomized algorithm, as long as the involved submodular functions (including the inter-block interaction terms) are monotone. We close with a case study in which we use these algorithms to find high quality diverse ensembles of classifiers, showing good results.", "title": "Constrained Interacting Submodular Groupings"}