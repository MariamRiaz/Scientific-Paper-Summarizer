40	13	First, we introduce a novel Bayesian optimization methodology able to leverage conditional dependencies between hyperparameters.
63	24	Further, let Dn = {(xi, yi)}ni=1 be the set of observations.
65	18	Note that xi∈Xpi since the input domain may vary from one leaf to another.
77	27	Let y ∈ Rn be the vector of all observations and g ∈ Rn the vector of latent function values at {xi}ni=1.
78	29	Further, let Ip = {i | pi = p}, noting that np = |Ip|.
79	47	We partition the data accordingly, so that yp = [yi]i∈Ip , and similarly gp = [gp(xi)]i∈Ip .
90	15	We can also obtain an interesting interpretation for the induced kernel of the marginal likelihood by computing it in a different way.
100	35	Let Vp ⊆ V be the set of inner nodes on the path from the root to leaf p. Concatenating the rv’s, we define the induced feature vector zp = [rv]v∈Vp such that c>zp = ∑ v∈Vp c > v rv.
104	37	In Section 4, we will use rv to encode both numerical (i.e., dv = 1) and categorical parameters (via one-hot representations, so that dv equals the number of categories).
105	47	In our deep learning use case, parameters such as the learning rate, the number of units and the type of activation functions are encoded via the rv’s (see Figure 3, bottom).
106	33	We will refer to the parameter associated with rv as a shared parameter since it is shared across all the leaves whose paths contain v.
107	114	Bayesian optimization generally proceeds by discretizing the search space X into a set of anchor points, for example by using quasi-random sequences (Sobol, 1967).
108	39	We then maximize an acquisition function starting from the most promising anchor point(s), typically with a numerical solver like L-BFGS (Nocedal & Wright, 2006).
110	63	Frequently used choices include Thompson sampling (Thompson, 1933), probability of improvement (PI) (Kushner, 1964), expected improvement (EI) (Mockus et al., 1978), or GP-UCB (Srinivas et al., 2010).
111	29	We will focus on EI in the sequel as it has been shown to perform better than PI.
112	27	Our initial experiments also showed that Thompson sampling was not performing well.
113	35	The naive approach of globally optimizing EI over anchor points does not scale well with a high-dimensional X .
114	49	In the previous section, we specified a tree-structured model for the (random) surrogate function, with which the evaluation of an acquisition function at some x ∈ X is sped up.
118	18	The naive approach ignores structure in the search space, using a surrogate model based on a global kernel, like the one proposed by Swersky et al. (2014a).
123	16	The independent model also fails to represent dependencies between the leaf nodes, so that a larger total number of evaluations may be required to reach a good solution.
128	88	In practice, the set of leaf nodes P can become large, in which case the requirement to search in every leaf node can be costly.
130	70	Namely, our model implies a path acquisition function α(p|Dn).
142	16	As noted above, we could optimize α(x, p|Dn) at all leaves and pick the overall winner.
143	37	Instead, we propose a two-step approach, based on a path EI acquisition function: α(p|Dn) = E { [ymin − bp − z>p c]+ } , (6) where the expectation is taken with respect to z>p c + bp ∼ N (z>p Λ −1 c fc + bp, z > p Λ −1 c zp).
148	16	In this section, we conduct two sets of experiments.
174	134	At this juncture, we would also like to emphasize that while independent catches up with tree in some cases, it is more wasteful of resources as it requires to score every leaf at each iteration unlike tree (see also Table 1).
177	20	Approach not based on GPs: smac is known to be stateof-the-art for optimization tasks in presence of conditional relationships (Eggensperger et al., 2013).
180	31	We now focus our attention on the tuning of a multilayer perceptron (MLP) for binary classification.
181	28	The setting we consider is reminiscent of that proposed by Swersky et al. (2014a).
184	38	The optimization task can be specified in various ways, resulting in different topologies for the trees of conditional relationships.We consider the two instantiations of conditional relationships illustrated in Figure 3.
