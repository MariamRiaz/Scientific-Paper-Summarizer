{"sections": [{"heading": "1. Introduction", "text": "Submodular optimization is fast becoming a primary tool in machine learning. The power of submodularity as a model has been demonstrated in numerous applications, including document summarization (Lin & Bilmes, 2011), clustering (Gomes & Krause, 2010), active learning (Golovin & Krause, 2011; Guillory & Bilmes, 2011; Hoi et al., 2006), graph and network inference (Gomez Rodriguez et al., 2010; Rodriguez & Scho\u0308lkopf, 2012; Defazio & Caetano, 2012), and information diffusion in networks (Kempe et al., 2003). Crucial to the success of these methods is the fact that optimizing submodular functions can be done efficiently and with provable guarantees (Krause & Golovin, 2014).\nIn many cases, however, the true function cannot be accessed, and instead a surrogate function is learned from data (Balkanski et al., 2017). To this end, PMAC learning (Balcan & Harvey, 2011) offers a framework for analyzing the learnability of submodular functions, as well as algo-\n1Harvard University 2Tel Aviv University. Correspondence to: Nir Rosenfeld <nirr@g.harvard.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nrithms for learning in practice. Encouraging results show that in many cases submodular functions can be efficiently learned from data (Balcan & Harvey, 2011; Iyer et al., 2013; Feldman & Kothari, 2014; Feldman & Vondrak, 2016). A natural approach in this setting is to first learn a surrogate function from samples, and then optimize it, hoping that the estimated optimum will be close to the true one. A recent line of work has been devoted to this setting of optimization from samples (OPS) (Balkanski et al., 2016; 2017).\nThe main result of OPS is unfortunately discouraging: for maximizing a submodular function under a cardinality constraint, no algorithm can obtain a constant factor approximation guarantee given polynomially-many samples from any distribution (Balkanski et al., 2017). Thus, optimizing over learned surrogates does not provide any meaningful guarantees with respect to the true function.\nThe hardness of OPS is, however, a worst-case result. The hardness stems from the discrepancy between how the algorithm gains access to information (via samples) and how it is evaluated (globally). In contrast, machine learning objectives are typically concerned with expected outcomes, and are evaluated over the same distribution from which data is acquired (Valiant, 1984). In this paper, we build on this motivation and propose an alternative framework for optimizing from samples. The objective we propose, called distributional optimization from samples (DOPS), circumvents the above difficulties by considering a distribution-dependent objective. In general, a function class F is in \u03b1-DOPS if an \u03b1-approximation of the empirical argmax can be found with arbitrarily high probability using polynomially many samples, for any distribution D and for any f \u2208 F . Formally: Definition 1 (\u03b1-DOPS). Let F = {f : 2[n] \u2192 R+} be a class of set functions over n elements. We say that F is \u03b1-distributionally optimizable from samples if there is an algorithm A that, for every distribution D over 2[n], every f \u2208 F , and every , \u03b4 \u2208 [0, 1], when A is given as input a sample set S = {(Si, f(Si))}Mi=1 where Si\niid\u223c D, with probability of at least 1\u2212 \u03b4 over S it holds that:\nPT \u223cDm [ f ( A(T ) ) \u2265 1 \u03b1 max S\u2208T f(S) ] \u2265 1\u2212 (1)\nwhere T = {(Sj)}mj=1, A(T ) \u2208 T is the output of the algorithm, and S is of size M \u2208 poly(n,m, 1/ , 1/\u03b4, \u03b1).\nThe criterion in Eq. (1) relaxes the OPS objective to hold in expectation over D. This is achieved by replacing the entire combinatorial domain with a sampled subset T of size m, allowing for a distribution-agnostic notion of approximation. As m increases, satisfying Eq. (1) is expected to be harder. When m\u2192\u221e, DOPS recovers OPS.\nOur first goal in this paper is to establish the hardness of DOPS. In general, classic approximation results do not necessarily transfer to statistical settings (Balkanski et al., 2017). Nonetheless, our main theoretical result establishes a tight equivalence between DOPS and PMAC learning (Balcan & Harvey, 2011), meaning that any F that is learnable is also optimizable, and vice versa. This demonstrates an intriguing link between learning and optimizing submodular functions, which are known to be PMAC-learnable (Balcan & Harvey, 2011). The equivalence result is constructive, and gives a general optimization algorithm which can utilize any PMAC learner as a black box for DOPS, and vice versa. While our main focus in this paper is on submodular functions, these results hold for any family of combinatorial functions.\nIn practice, however, optimizing via PMAC algorithms has several drawbacks (Balcan & Harvey, 2011; Feldman & Kothari, 2014; Feldman & Vondrak, 2016). Our second goal in this paper is hence to design an efficient and scalable DOPS algorithm for several classes of interest. Our algorithm optimizes a loss function whose minimization provides a sufficient condition for DOPS. We prove that the minimizer of the empirical loss can be used for recovering an approximate argmax. In this sense, the framework we propose is one in which the algorithm \u201clearns to optimize\u201d. We show how the loss can be minimized efficiently and with guarantees for several submodular function classes, including coverage functions, cut functions, and unit demand.\nAn additional benefit of our approach is that it provides guarantees even when the output of the algorithm is restricted to a set of sampled alternatives. This setting is especially prevalent in cases where both sets and their values are generated by human users. For example, in the problem of influence maximization (Kempe et al., 2003), the goal is to choose a \u201cseed\u201d set of users such that, when exposed to certain content, will maximize its expected propagation. However, targeting arbitrary subsets of users is in most cases impossible, and the algorithm must choose between the sets of users sharing currently trending items. In the last part of the paper we demonstrate the empirical utility of our approach on this task using real data from Twitter."}, {"heading": "2. Distributional optimization and learning", "text": "In this section we give a tight characterization of function classes in DOPS by showing that a class F is in DOPS if and only if it is PMAC-learnable. This involves two steps. In\nthe first, we show that if F is \u03b1-PMAC learnable with sample complexity MPMAC, then it is \u03b1-DOPS. We augment this result with tight sample complexity bounds for \u03b1-DOPS. In the second part, we show that PMAC learnability is not only sufficient but also necessary for distributional optimization from samples. We show that if F is not \u03b1-PMAC learnable, then it is not (\u03b1\u2212 )-DOPS for any constant > 0, which is tight. This result is obtained by constructing a novel PMAC algorithm based on a DOPS black-box, and may thus be of separate interest in PMAC analysis. Overall, our results determine the hardness of DOPS by establishing a connection between the approximability and learnability of function classes.\nWe begin by reviewing the notion of PMAC learnability:\nDefinition 2 (PMAC, Balcan & Harvey (2011)). A class F is \u03b1-PMAC-learnable if there is an algorithm such that for every distribution D, every f \u2208 F , and every , \u03b4 \u2208 [0, 1],\nPS\u223cD [ f\u0303(S) \u2264 f(S) \u2264 \u03b1f\u0303(S) ] \u2265 1\u2212 (2)\nwhere the input of the algorithm is a set S of size M \u2208 poly(n, 1/ , 1/\u03b4, \u03b1), the output is a mapping f\u0303 : 2[n] \u2192 R+, and Eq. (2) holds w.p. at least 1\u2212 \u03b4 over S .\nIntuitively, PMAC generalizes the standard notion of PAC learning by considering a loss which penalizes predictions that are not within a factor of \u03b1 of their true value.\nWe are now ready to prove our main theoretical results.\n2.1. If F is PMAC-learnable then F is in DOPS\nWe show that if F is \u03b1-PMAC learnable with sample complexity MPMAC(n, \u03b4, , \u03b1), then it is \u03b1-DOPS with sample complexity MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1), and this sample complexity is tight. A PMAC algorithm learns a surrogate function f\u0303 . In our reduction, the corresponding DOPS algorithm simply outputs argmaxS\u2208T f\u0303(S). The technical part of this result is in showing the sample complexity tightness. Intuitively, the sample complexity is exactly the number of samples that are needed so that, with high probability, f\u0303 obtains a good approximation on all S \u2208 T . We begin by showing that MPMAC(n, \u03b4, 1 \u2212 (1 \u2212 )1/m, \u03b1) is sufficient, which follows from the definition of PMAC.\nTheorem 1. Assume F is \u03b1-PMAC-learnable with sample complexity MPMAC(n, \u03b4, , \u03b1), then F is \u03b1-DOPS with sample complexity at most MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1), i.e.,\nMDOPS(n,m, \u03b4, , \u03b1) \u2264MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1).\nProof. Let f \u2208 F , D be some distribution, S = {(Si, f(Si))}Mi=1 and T = {Si}mi=1 be the train and test sets, andA be an algorithm that constructs f\u0303 which \u03b1-PMAC learns f with sample complexity MPMAC(n, \u03b4, , \u03b1).\nThe DOPS algorithm that we analyze constructs f\u0303 with algorithm A using S and returns\nS\u0303? = argmax S\u2208T f\u0303(S).\nFix , \u03b4 > 0 and \u03b1 > 1 and consider M = MPMAC(n, \u03b4, 1\u2212 (1 \u2212 )1/m, \u03b1). By the definition of \u03b1-PMAC, we get that with probability 1\u2212 \u03b4 over S,\nPr S\u223cD\n[ f\u0303(S) \u2264 f(S) \u2264 \u03b1 \u00b7 f\u0303(S) ] \u2265 (1\u2212 )1/m.\nNext, we obtain\nPr T\n[ f\u0303(S) \u2264 f(S) \u2264 \u03b1 \u00b7 f\u0303(S) : \u2200S \u2208 T ] = (\nPr S\u223cD\n[ f\u0303(S) \u2264 f(S) \u2264 \u03b1 \u00b7 f\u0303(S) ])m \u2265 1\u2212 .\nwhere the equality is due to the sets S \u2208 T being drawn i.i.d. fromD, and the inequality holds with probability 1\u2212\u03b4 over S. We define S? = argmaxS\u2208T f(S) and obtain that with probability 1\u2212 over T and 1\u2212 \u03b4 over S,\nf(S\u0303?) \u2265 f\u0303(S\u0303?) \u2265 f\u0303(S?) \u2265 \u03b1\u22121f(S?).\nWe conclude that withM = MPMAC(n, \u03b4, 1\u2212(1\u2212 )1/m, \u03b1),\nf(S\u0303?) \u2265 1 \u03b1 \u00b7max S\u2208T f(S)\nwith probability 1\u2212 over T and 1\u2212 \u03b4 over S.\nFor tightness, we give an information-theoretic lower bound by constructing a difficult class F that cannot be in \u03b1-DOPS with less than MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1) samples. Theorem 2. For all \u03b1 > 1 and , \u03b4 > 0, for m sufficiently large, there exists a family of functions F and a function MPMAC(\u00b7) such that\n\u2022 for all \u2032, \u03b4\u2032 > 0: F is \u03b1-PMAC-learnable with sample complexity MPMAC(n, \u03b4\u2032, \u2032, \u03b1), and\n\u2022 given strictly less than MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1) samples, F is not \u03b1-DOPS, i.e.,\nMDOPS(n,m, \u03b4, , \u03b1) \u2265MPMAC(n, \u03b4, 1\u2212(1\u2212 )1/m, \u03b1).\nProof Sketch (see supp. material for full proof). For each f in the difficult F , only a single set S? has a high value, while all others have low values. We consider a uniformly random function f \u2208 F and the corresponding randomized subclass F \u2032 \u2286 F which consists of all functions f \u2032 such that S? is in the test set but not in the train set.\nInformally, an algorithm which aims to optimize f \u2208 F \u2032 cannot use the train set to learn which S \u2208 T is S?. More\nprecisely, if f \u2208 F \u2032, the decisions of the algorithm are independent of the randomization of f , conditioned on f \u2208 F \u2032. Thus, if f \u2208 F \u2032, the algorithm does not obtain an \u03b1approximation because of the gap between the value of S? and the other sets.\nWe construct F and D such that S? is in the test set w.p. greater than 1 \u2212 . This implies that to satisfy DOPS, the algorithm must observe enough samples so that S? is in the train set w.p. at least 1\u2212 \u03b4. We then argue that this number of samples is at least MPMAC(n, \u03b4, 1\u2212 (1\u2212 )1/m, \u03b1).\n2.2. If F is not PMAC-learnable then F is not in DOPS\nA simple intuition for Theorem 1 is that if one can accurately predict the values of all S \u2208 T , then it is possible to find the empirical argmax. The main result in this section, which is perhaps less intuitive, shows that the reverse implication also holds. Namely, if one can find the the empirical argmax, then it is possible to infer the values of all sets in T . The contrapositive of this result is that if F is not PMAC-learnable, then F is not in DOPS. Combining both results provides a full characterization of distributional optimization in terms of learnability.\nTo construct a PMAC learner from a DOPS algorithm, we first randomly partition S into \u201ctrain\u201d and \u201ctest\u201d sets. We then train the DOPS algorithm on the train set, and use it to generate pairwise comparisons with test elements. The learned value for S is given by the maximum value of a test sample that S \u201cbeats\u201d (via the inferred comparisons). At a high level, the analysis uses the DOPS guarantees and a bucketing argument to satisfy the PMAC requirements. Theorem 3. Let \u00b5 = maxS f(S)/minS:f(S)>0 f(S), c be any constant such that 1 \u2264 c \u2264 \u03b1, and M\u00b5 = 8 log \u00b5 log c ( 1 + 2 log ( 1 \u03b4 )) . If a class F is in \u03b1/cDOPS with sample complexity MDOPS(n,m, , \u03b4, \u03b1/c), then it is \u03b1-PMAC-learnable with sample complexity M\u00b5 + MDOPS(n, 2, /M\u00b5, \u03b4/M\u00b5, \u03b1/c), i.e.,\nMPMAC(n, , \u03b4, \u03b1) \u2264M\u00b5+MDOPS(n, 2, /M\u00b5, \u03b4/M\u00b5, \u03b1/c).\nProof. Fix , \u03b4 > 0 and \u03b1 > 1. Let S = {(Si, f(Si))}Mi=1 be the samples from D that are given as input. We partition the samples in S uniformly at random into S1 and S2 of sizes M1 and M2, respectively. For some S \u223c D, the goal is to predict f\u0303(S) such that f\u0303(S) \u2264 f(S) \u2264 \u03b1 \u00b7 f\u0303(S).\nFor each Si \u2208 S2, define S2,i := {Si, S}. Since F is in DOPS, with M1 = MDOPS(n, 2, /M2, \u03b4/M2, \u03b1/c) samples, the algorithm outputs S?i \u2208 S2,i such that with probabilities 1\u2212 \u03b4/M2 over S1 and 1\u2212 /M2 over S2,i,\nf(S?i ) \u2265 c\n\u03b1 max(f(S), f(Si)).\nBy a union bound, this holds for all i \u2208M2 with probability 1\u2212 \u03b4 over S1 and probability 1\u2212 over S and S2.\nWe say that S \u201cbeats\u201d Si if the \u03b1-DOPS algorithm outputs S when given S2,i. Let S\u22122 be the collection of sets Si in S2 such that S beats Si. The learning algorithm is\nf\u0303(S) = c\n\u03b1 \u00b7 max Si\u2208S\u22122 f(Si).\nLet fmin = minS f(S) and fmax = maxS f(S). We partition the sets into buckets defined as follows:\nBi := {S : fmin \u00b7 ci\u22121 \u2264 f(S) < fminci}\nfor i \u2265 1 and B0 = {S : f(S) = 0}. With \u03b2 := log\u00b5/ log c buckets, all sets S are in a bucket since fmin \u2264 f(S) \u2264 fmax. We define a bucket Bi to be dense if a random set S \u223c D has non-negligible probability to be in Bi, otherwise it is sparse. More precisely, Bi is dense if PrS\u223cD [S \u2208 Bi] \u2265 /2\u03b2.\nThe set S is in a dense bucket Bi with probability at least 1 \u2212 2 since there are at most \u03b2 buckets that are not dense and S is in each of them with probability at most 2\u03b2 by the definition of dense bucket. With m samples, the expected number of samples in Bi is at least m 2\u03b2 and by a standard concentration bound,\nPr [ |Bi| \u2264 m\n2 2\u03b2\n] \u2264 e\u2212 m 16\u03b2\nWe assume that |Bi| \u2265 m2 2\u03b2 for the remainder of the proof. There is at most one set in bucket Bi that is beaten by all the other sets. Since the set S has equal probability to be any of the sets in Bi,1 there is at least one other set S\u2212 in Bi which S beats with probability 1/|Bi| \u2264 4\u03b2/m .\nWith \u03b4 \u2265 e\u2212 m 16\u03b2 (and hence m \u2265 log(1/\u03b4)16\u03b2 ), with probability of at least 1\u2212 \u03b4, the number of samples in Bi is at least m 4\u03b2 . With /2 \u2265 4\u03b2/m (and hence m \u2265 8\u03b2/\n2), with probability of at least 1\u2212 over S \u223c D, S is in a dense bucket and beats at least one other S\u2212 \u2208 S\u22122 in that bucket.\nWe get that:\nf\u0303(S) = c\n\u03b1 \u00b7 max Si\u2208S\u22122\nf(Si) \u2265 c \u03b1 \u00b7 f(S\u2212) \u2265 1 \u03b1 \u00b7 f(S)\nwhere the equality is by the definition of f\u0303(S), the first inequality is since S\u2212 \u2208 S\u22122 , and the last is since S and S\u2212 are in the same bucket. We also have\nf(S) \u2265 c \u03b1 \u00b7 max Si\u2208S\u22122 f(Si) = f\u0303(S)\nwhere the inequality is by the definition of S\u22122 and the equality by definition of f\u0303(S). Thus, f\u0303(S) \u2264 f(S) \u2264 \u03b1f\u0303(S) and with M2 = m \u2265 8 log \u00b5 log c ( 1 + 2 log ( 1 \u03b4 )) = M\u00b5, the sample complexity is M\u00b5 +MDOPS(n, 2, /M\u00b5, \u03b4/M\u00b5, \u03b1/c).\n1We assume that the DOPS algorithm breaks ties in a consistent manner, i.e., it cannot be adversarial and break ties depending on whether S is the set we wish to learn or if S \u2208 S2.\nAlgorithm 1 DOPS(S = {(Si, zi)}Mi=1, m, \u03b1) 1: Randomly partition [M ] into N = bMm c sets A1, . . . , AN 2: Create m-tuple sample set S = {(Si, zi)}Ni=1 from S\nwhere Si = {Sj}j\u2208Ai and z i = {zj}j\u2208Ai\n3: Compute \u03b1(zi) = {y \u2208 [m] : ziy \u2265 1\u03b1 max z i} \u2200 i \u2208 [N ]\n4: \u03b8\u0302 = argmin \u03b8\u2208\u0398 N\u2211 i=1 max y [1{y 6\u2208\u03b1(zi)} + f\u03b8(S i y)\u2212 \u03c8\u03b8(Si, zi)]+\nwhere \u03c8\u03b8(S, z) = 1|\u03b1(z)| \u2211 y\u2208\u03b1(z) f\u03b8(Sy)\n5: Return h \u03b8\u0302 (T ) = argmax S\u2208T f \u03b8\u0302 (S)"}, {"heading": "3. Learning to Optimize at Scale", "text": "In this section we give an efficient DOPS algorithm that applies to several interesting parametric submodular subclasses F\u0398 = {f\u03b8 : \u03b8 \u2208 \u0398}. Our general technique includes two steps. First, we identify a loss function whose minimization provides a sufficient condition for DOPS (Eq. (1)), but is in general hard to optimize. Then, we show that for the function classes we consider, a transformation of the inputs reveals structure which can be exploited for efficiently optimizing a convex surrogate loss. Note that in principle, due to Thm. 1, any PMAC algorithm can be used for DOPS. This, however, has several practical disadvantages, which we comment on in Sec. 3.5.\nWe begin by illustrating our approach for coverage functions with parametric weights. We then describe our algorithm, prove its correctness, and show how it can be applied to other classes such as graph cuts, unit demand, and coverage functions with parametric cover sets."}, {"heading": "3.1. Learning to optimize coverage functions", "text": "Coverage functions are a simple but important class of submodular functions, and have been used in applications such as computational linguistics (Sipos et al., 2012), algorithmic game theory (Dughmi & Vondra\u0301k, 2015), and influence maximization in social networks (Kempe et al., 2003). Let U be a ground set of d items, and C = {C1, . . . , Cn} a collection of subsets where Ci \u2286 U . For a set of non-negative item weights \u03b8 = {\u03b81, . . . , \u03b8d}, a function f\u03b8 : 2[n] \u2192 R is a coverage function if:\nf\u03b8(S) = \u2211\nu\u2208C(S) \u03b8u, C(S) = \u22c3 i\u2208S Ci (3)\nWhile apparently simple, coverage functions are quite expressive, and optimizing them from samples is known to be hard (Balkanski et al., 2017). One reason is that, as a\nfunction of their inputs S, coverage functions can be highly non-linear. Meanwhile, as a function of their parameters, they become linear via a simple transformation of the inputs:\nf\u03b8(S) = \u3008\u03c6(S), \u03b8\u3009, \u03c6u(S) = 1{\u2203 i\u2208S s.t. u\u2208Ci} (4)\nThis structure allows our algorithm to efficiently find the approximate empirical argmax of any given T with high probability. The output of the algorithm is a function h \u2208 H for choosing one S out of the m candidates in T , where:\nH = {h\u03b8(T ) = argmax S\u2208T f\u03b8(S) : \u03b8 \u2208 \u0398} (5)\nIn this sense, our algorithm learns an \u201dempirical optimizer\u201d that is guaranteed to correctly optimize collections of size m drawn from Dm."}, {"heading": "3.2. Algorithm", "text": "Pseudocode of our algorithm is given in Algorithm 1. The following theorem establishes its correctness for any parametric class of functions F that can be made linear in their parameters using some transformation \u03c6, namely F\u0398 = {f\u03b8(S) = \u3008\u03c6(S), \u03b8\u3009 : \u03b8 \u2208 \u0398}. As we show, this holds for several interesting submodular sub-classes, including the coverage functions in Sec. 3.1 as well as all other classes presented in Sec. 3.3.\nTheorem 4. Let m \u2208 N and , \u03b4 \u2208 [0, 1], and let f = f\u03b8\u2217 with \u03b8\u2217 \u2208 \u0398. For a given \u03b1 > 0, let h be the output of Algorithm 1 when given S = {(Si, zi)}Mi=1, m, and \u03b1 as input, where z = f\u03b8(S) and S\niid\u223c D. Then, with probability of at least 1\u2212 \u03b4 over S, it holds that:\nPT \u223cDm [ f ( h(T ) ) \u2265 1 \u03b1 max S\u2208T f(S) ] \u2265 1\u2212 (6)\nfor M \u2265 O\u0303(m(RB/ )2), R = maxS \u2016\u03c6(S)\u2016, B = \u2016\u03b8\u2217\u2016.\nProof. We begin with some notation. Let S = {S1, . . . , Sm} be a set of m examples with corresponding values z = {z1, . . . , zm} where zy = f(Sy). Algorithm 1 returns a function h that chooses a set Sy \u2208 S. It will be convenient to instead view h as a mapping from S to indices y \u2208 [m]. Denote the set of \u03b1-approximate solutions by:\n\u03b1(z) = {y \u2208 [m] : zy \u2265 1\n\u03b1 max z} (7)\nOur analysis makes use of the following loss function:\n\u2206\u03b1(z, y) = 1{y 6\u2208 \u03b1(z)} (8)\nEq. (8) is useful since L(h) := E[\u2206\u03b1(z, h(S))] \u2264 implies that h satisfies Eq. (6). We therefore focus on bounding\nL(h). As we do not have access toD, our algorithm chooses an h \u2208 H which instead minimizes the empirical loss. Note that while \u2206\u03b1 is defined over m-tuples, S contains individual sets. To ensure a consistent empirical loss, we randomly partition [M ] into N = M/m distinct sets A1, . . . , AN , and define anm-tuple sample set S = {(Si, zi)}Ni=1, where Si = {Sy}y\u2208Ai and zi = {zy}y\u2208Ai . The loss is now:\nL\u0302(h;S) = 1 N N\u2211 i=1 \u2206\u03b1(z i, y\u0302i), y\u0302i = h(Si) (9)\nSince \u2206\u03b1 is not convex, the algorithm instead optimizes a surrogate convex upper bound. There are many ways to do this; here we use an average hinge surrogate:\nmax y\u2208[m]\n[\u2206\u03b1(z i, y) + f\u03b8(S i y)\u2212 \u03c8\u03b8(Si, zi)]+ (10)\nwhere [a]+ = max{0, a} and:\n\u03c8\u03b8(S, z) = 1 |\u03b1(z)| \u2211 y\u2208\u03b1(z) f\u03b8(Sy) (11)\nEq. (10) is similar in spirit to the loss in (Lapin et al., 2015), and is tight w.r.t. Eq. (9) whenever L\u0302 = 0, Intuitively, minimizing Eq. (10) pushes \u03b8 towards values for which the true argmax is scored higher than all others by a margin. Note that the average in Eq. (11) can be replaced with a max to attain a tighter (though no longer convex) surrogate.\nSince S is labeled by some f\u03b8\u2217 \u2208 F\u0398, we have that L(h\u03b8\u2217) = 0. This means that there is some \u03b8 \u2208 \u0398 such that with L\u0302(h\u03b8;S) = 0, and due to the tightness of Eq. (10), L\u0303(h\u03b8;S) = 0 as well. This is sufficient for applying the following generalization bound (Collins, 2004):\nL(h) \u2264 O\n(\u221a m\nM\n( (RB logM)2 + log 1\n\u03b4\n)) (12)\nPlugging in M gives L(h) \u2264 , concluding the proof.\nEq. (10) is convex whenever f\u03b8 is linear in \u03b8 for some representation \u03c6. This holds for coverage functions (Eq. (4)) as well as for the other classes we consider in Sec. 3.3. Eq. (10) can then be optimized using standard convex solvers, or with highly efficient and scalable solvers such as the cutting plane method of Joachims et al. (2009)."}, {"heading": "3.3. Other submodular classes", "text": "We now discuss how our method can be extended to other submodular function classes. For each class, we give a transformation \u03c6 of the inputs under which the function becomes linear in its parameters. Thm. 4 and Algorithm 1 can then be applied with the appropriate f\u03b8(S) = \u3008\u03c6(S), \u03b8\u3009.\nGraph k-cuts: Let G = (V,E) be an undirected graph, and let \u03b8 \u2208 R|E|+ be edge weights. For a partition P \u2208 [k]|V | of the nodes into k groups, its value is given by:\nf\u03b8(P ) = 1\n2 \u2211 (u,v)\u2208E Pu 6=Pv \u03b8uv\nWhile k-cut functions are known to be hard to optimize over P , they become linear in \u03b8 with the transformation:\n\u03c6uv(P ) = 1{Pu 6= Pv} \u2200 (u, v) \u2208 E\nUnit demand: Let \u03b8 \u2208 Rn+ be a set of item weights. The value of a subset S \u2286 [n] is given by:\nf\u03b8(S) = max u\u2208S \u03b8u\nAlthough it is possible to write f\u03b8 = \u3008\u03b8, \u03c6(S)\u3009 with \u03c6u(S) = 1{\u03b8u\u2265\u03b8v \u2200v\u2208S}, this representation requires \u03b8, which is unknown. Nonetheless, a similar data-dependent construction can still be used to obtain some \u03b8\u2032 which minimizes the loss. To see why, let S\u0304 \u2208 S be the set with the highest value f\u03b8(S\u0304) in S. For this S\u0304, there must exist some u \u2208 S\u0304 that is not in any other S \u2208 S with f\u03b8(S) < f\u03b8(S\u0304). By setting \u03c6v(S\u0304) = 1{u=v} and \u03b8\u2032u = f\u03b8(S\u0304), we ensure that f\u03b8(S\u0304) = \u3008\u03b8\u2032, \u03c6(S\u0304)\u3009. Note that this does not necessarily imply that \u03b8\u2032u = \u03b8u. In a similar fashion, by setting:\n\u03c6u(Si) = 1{u \u2208 Si \u2227 @ j 6= i s.t. u \u2208 Sj \u2227 zj < zi}\nfor every i \u2208M , we get that f\u03b8(Si) = \u3008\u03b8\u2032, \u03c6(Si)\u3009 for some \u03b8\u2032, which guarantees L\u0302 = 0. Note that generalization here concerns \u03c6 as applied to examples in both S and T .\nCoverage with parametrized cover sets: Let U = [N ] be a ground set of items with unit weights. The parameters are a collection item subsets {C1, . . . , Cn} with Ci \u2286 U . We use \u03beiu = 1{u \u2208 Ci} and denote the maximal overlap by d = maxu \u2211 i \u03beiu. For a subset S \u2208 [n], its value is:\nfC(S) = \u2223\u2223\u2223\u22c3\ni\u2208S Ci \u2223\u2223\u2223 While fC is not linear over C, it can be linearized over a different parameterization. For xi = 1{i \u2208 S}, we have:\nfC(S) = \u2211 u\u2208\u2126 ( 1\u2212 n\u220f i=1 (1\u2212 xi\u03beiu) )\nSince fC is a polynomial of degree at most d, the explicit size of \u03c6 (and hence of the corresponding \u03b8) is nd. For computational efficiency, we can consider the dual form and implicitly define \u03c6 via the kernalized inner product:\n\u3008\u03c6(S), \u03c6(S\u2032)\u3009 = ( \u3008xS , xS\u2032\u3009+ 1 )d"}, {"heading": "3.4. Reducing the sample-complexity cost of m", "text": "Interestingly, at the cost of a small additional additive error, the dependence of the generalization bound on m can be removed by considering an alternative loss function. Fix some q \u2208 [0, 1]. Given S, define Q to be the set of examples in the top q-quantile. The idea here is to learn \u03b8 so that f\u03b8 will score top-quantile examples S \u2208 Q above low-quantile examples S 6\u2208 Q. The corresponding loss is therefore defined over example pairs:\n\u2206q(S, S \u2032, f\u03b8) = { 1{f\u03b8(S)<f\u03b8(S\u2032)} if S \u2208 Q \u2227 S\u2032 6\u2208 Q 0 otherwise\n(13) Note that, in a similar fashion to \u2206\u03b1, the empirical loss L\u0302q over \u2206q can be optimized efficiently, and the optimal \u03b8 gives L\u0302q = 0. For any S \u2208 S, the probability of having at least one S \u2208 S \u2229Q is 1\u2212 qm. Applying the generalization bound in Agarwal & Niyogi (2009) gives:\n\u2264 qm + O\u0303\n( B\n\u03bbMq +\n( B2\n\u03bb + Z\n)\u221a ln(1/\u03b4)\nMq\n) (14)\nwhere Z = supS f(S) and \u03bb controls an additional regularizer. In Sec. 4 we use a stricter variant of this formulation, in which high-quantile items are binned separately.\n3.5. Using PMAC algorithms in practice\nIn principle, the reduction in Sec. 2.1 shows that any PMAC algorithm can be used for DOPS. Practically, however, this approach has several disadvantages. The root cause of this is that most current PMAC algorithms are designed for general submodular functions.2 As such, they must adhere to demanding lower bounds (Balcan & Harvey, 2011; Feldman & Vondrak, 2016) which hold even for simple distributions (e.g., uniform). When considering specific submodular subclasses, these algorithms can therefore be suboptimal (and in fact quite costly) in terms of runtime, sample complexity, and/or approximation ratio. Additionally, virtually all current PMAC algorithms provide guarantees for either uniform or product distributions. Even in this setting, PMAC algorithms either guarantee a fixed approximation ratio, or are exponential in \u03b1 (Feldman & Vondrak, 2016), making them difficult to use for \u03b1-DOPS with arbitrarily small \u03b1. The only known result for arbitrary distributions is the \u221a n+ 1- PMAC algorithm of Balcan & Harvey (2011), which give a matching \u2126\u0303(n1/3) lower bound on \u03b1.\n2 A notable exception to this is Feldman & Kothari (2014) which specifically considers PMAC learning of coverage functions with unknown cover sets."}, {"heading": "4. Experiments", "text": "In this section we evaluate the performance of our method on the task of optimally choosing trending items in social media platforms. Of the countless items that are continuously created and shared by users in such platforms, only a handful will become widespread (Goel et al., 2012). A key challenge faced daily by platform administrators is that of identifying potential trending content as early as possible. Trending items can then be marked, used for generating recommendations, or promoted to the public front page."}, {"heading": "4.1. Optimizing trending items", "text": "For a given social platform, let n be the number of users, and \u2126 be the set of spreading content items. When a user u \u2208 [n] is observed to have been exposed to an item \u03c9 \u2208 \u2126, we say that u adopted \u03c9. This can happen, for instance, when u views, shares, comments, or votes on \u03c9. A crucial factor in the successful spread of an item is the identity of its early adopters (Rogers, 1962; Goldenberg et al., 2002). We therefore represent each content item \u03c9 at a certain time point by the set of users that have adopted it up to that time, which we denote by S\u03c9 \u2286 [n]. We will be interested in the final number of adopters z\u03c9 as a function of the set of adopting users, namely z\u03c9 = f(S\u03c9). For simplicity we assume that all items are considered at the time when adopted by exactly k users, so that |S\u03c9| = k for all \u03c9 \u2208 \u2126. Under the above representation, targeting a successful item can be thought of as optimizing over the set of adopting users under a cardinality constraint. The task is therefore to choose the set S\u03c9 for which f(S\u03c9) is maximal.\nThe above optimization task has two clear restrictions. First, f cannot be accessed or queried, and any information regarding the value of subsets is available only via samples, namely past items and their adopting users. Second, an algorithm cannot output any user subset S \u2286 [n], but must rather choose from a set of currently available items. In addition, the task of choosing the top trending item is performed repeatedly, each time over a different collection of content items. For example, for a front page that is updated hourly, a new trending item must be selected from the set of currently propagating content items for each update. Note that in such systems, the available subsets and their eventual value are primarily determined by the system\u2019s users. Online social platforms are therefore a prime example of a setting where an optimization algorithm has only statistical access to data."}, {"heading": "4.2. Experimental setup", "text": "We evaluate the performance of our method on a benchmark dataset of propagating Twitter hashtags (Weng et al., 2013). Data was gathered by monitoring the sharing (tweeting and retweeting) of hashtags across users over the course of a month. The dataset includes 612,355 users who shared 226,488 distinct hashtags, with a total of 1,687,704 sharing activities. For each hashtag, the data describes the sequence of adopting users and the corresponding timestamps. These are used to construct a \u201cretweet\u201d social network G = (V,E) where (u, v) \u2208 E if v retweeted u. A user is considered to be active if she shared at least 20 hashtags. We focus on the 11,815 active users and on the 4,155 hashtags that include at least one active user. If a user retweeted the same hashtag more than once, we consider only the first tweet.\nSamples were generated in the following manner. For each hashtag \u03c9, the user set S\u03c9 was defined to include the first k \u2208 {5, . . . , 15} active adopting users, and z\u03c9 was set to be the number of eventual adopters. All pairs (S\u03c9, z\u03c9) were randomly partitioned into a train set S and a global test set T \u2032 using a 90:10 split. All methods were given S as input, and were evaluated on 1,000 random subsets T \u2286 T \u2032 of size m, where m \u2208 {100, . . . , 500}. This was repeated 100 times, and average results are reported. All methods we consider return an element S\u0302 \u2208 T by computing argmaxS\u2208T g(S) for some score function g, which is typically learned from the data. Hyper-parameters were tuned using cross validation for all relevant methods.\nDOPS model: We implement the DOPS algorithm using coverage functions as the base class. Specifically, given the social network graph G = (V,E), we use V as the ground set, and construct a cover set Cv = u : (v, u) \u2208 E) for every v \u2208 V . The coverage function we learn is:\nf\u03b8,\u03b7(S) = \u2211 v\u2208S \u03b8v + \u2211 u\u2208C(S) \u03b7u (15)\nwhereC(S) = \u22c3 v\u2208S Cv . The idea behind this model is that, given that user v adopted, each of her neighbors can also adopt (with some probability). Figure 1 illustrates this idea. Thus, the two terms in Eq. (15) quantify the contributions of the adopting nodes and of their neighbors, respectfully, to the overall score. The coverage formulation takes into account the potential overlap in neighboring nodes, which can often be considerable (Holland & Leinhardt, 1971; Watts & Strogatz, 1998). We note that G is constructed using training data alone, and incoming edges were only considered for nodes with at least 10 shares. Eq. (10) was optimized using the cutting-plane method of Joachims et al. (2009).\nBaselines: We compare to the following methods:\n\u2022 SLOPE: A first-order extrapolation where we first estimate the slope of the diffusion curve, and then choose the subset with the highest value.\n\u2022 LINREG: We first run linear regression with `2 regularization, and then choose the subset with the highest predicted value.\n\u2022 OPS: A variant of the OPS (Balkanski et al., 2016), where instead of returning a global argmax, a given subset is scored based on the sum of marginal estimates. Note that under certain conditions, this algorithm is optimal for the setting of optimization from samples.\n\u2022 PMAC: A soft version of the distribution-independent PMAC algorithm of Balcan & Harvey (2011). Since the original algorithm assumes separability (which does not hold here), we instead use an agnostic classifier.\nResults: Figures 2(a) and 2(b) compare the value (number of adopters) for the chosen output of each method. As can be seen, DOPS clearly outperforms other methods by a margin. Note that when k increases, average output values are likely to increase as well, since the algorithms are given more\ninformation as input. When m increases, however, it is not clear a-priori how the average output values should change. This is because larger test sets are more likely to include higher-valued items, but at the same time have more lowvalued alternatives. Interestingly, while the performance of most baselines does not improve (or even degrades) as m increases, the performance of DOPS improves steadily."}, {"heading": "5. Conclusions", "text": "In this work, we proposed an optimization criterion for settings where the algorithm is limited to statistical access of the objective function. We argue that this setting is pervasive, and in fact, believe that in most applications it is the common rule rather than the exception. Previous results have been generally negative, but mostly due to demanding worst-case requirements. Drawing inspiration from learning theory, our solution relaxes these requirements to hold in expectation.\nOur main theoretical result shows an equivalence between optimization in this setting and learning. This highlights intriguing connections between the computational and statistical structure of function classes. An interesting corollary is that analyzing hardness of computation and approximation can now be done using statistical tools, and vice versa.\nSeveral of the functions classes we explored are notoriously hard to optimize, but have a surprisingly simple structure as a function of their parameters. This allowed us to use simple learning strategies to produce powerful optimization mechanisms. We hypothesize that there are many other classes that posses these properties. An additional avenue for further exploration, hinted by our equivalence result, is the reverse: are there classes that are seemingly hard-tolearn, but due to their optimizational properties, can actually be learned efficiently? We leave this for future work."}, {"heading": "Acknowledgements", "text": "This research was supported by a Google PhD Fellowship, NSF grant CAREER CCF-1452961, BSF grant 2014389, NSF USICCS proposal 1540428, ISF Centers of Excellence grant, a Google research award, and a Facebook research award."}], "year": 2018, "references": [{"title": "Generalization bounds for ranking algorithms via algorithmic stability", "authors": ["Agarwal", "Shivani", "Niyogi", "Partha"], "venue": "Journal of Machine Learning Research,", "year": 2009}, {"title": "Learning submodular functions", "authors": ["Balcan", "Maria-Florina", "Harvey", "Nicholas JA"], "venue": "In Proceedings of the forty-third annual ACM symposium on Theory of computing,", "year": 2011}, {"title": "The power of optimization from samples", "authors": ["Balkanski", "Eric", "Rubinstein", "Aviad", "Singer", "Yaron"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "The limitations of optimization from samples", "authors": ["Balkanski", "Eric", "Rubinstein", "Aviad", "Singer", "Yaron"], "venue": "In Proceedings of the 49th Annual ACM SIGACT Symposium on Theory of Computing,", "year": 2017}, {"title": "Strong regularities in growth and decline of popularity of social media services", "authors": ["Bauckhage", "Christian", "Kersting", "Kristian"], "venue": "arXiv preprint arXiv:1406.6529,", "year": 2014}, {"title": "Parameter estimation for statistical parsing models: Theory and practice of distribution-free methods", "authors": ["Collins", "Michael"], "venue": "New developments in parsing technology,", "year": 2004}, {"title": "A convex formulation for learning scale-free networks via submodular relaxation", "authors": ["Defazio", "Aaron", "Caetano", "Tiberio S"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2012}, {"title": "Limitations of randomized mechanisms for combinatorial auctions", "authors": ["Dughmi", "Shaddin", "Vondr\u00e1k", "Jan"], "venue": "Games and Economic Behavior,", "year": 2015}, {"title": "Learning coverage functions and private release of marginals", "authors": ["Feldman", "Vitaly", "Kothari", "Pravesh"], "venue": "In Conference on Learning Theory, pp", "year": 2014}, {"title": "Optimal bounds on approximation of submodular and XOS functions by juntas", "authors": ["Feldman", "Vitaly", "Vondrak", "Jan"], "venue": "SIAM Journal on Computing,", "year": 2016}, {"title": "The structure of online diffusion networks", "authors": ["Goel", "Sharad", "Watts", "Duncan J", "Goldstein", "Daniel G"], "venue": "In Proceedings of the 13th ACM conference on electronic commerce,", "year": 2012}, {"title": "Riding the saddle: How cross-market communications can create a major slump in sales", "authors": ["Goldenberg", "Jacob", "Libai", "Barak", "Muller", "Eitan"], "venue": "Journal of Marketing,", "year": 2002}, {"title": "Adaptive submodularity: Theory and applications in active learning and stochastic optimization", "authors": ["Golovin", "Daniel", "Krause", "Andreas"], "venue": "Journal of Artificial Intelligence Research,", "year": 2011}, {"title": "Budgeted nonparametric learning from data streams", "authors": ["Gomes", "Ryan", "Krause", "Andreas"], "venue": "In ICML, pp", "year": 2010}, {"title": "Inferring networks of diffusion and influence", "authors": ["Gomez Rodriguez", "Manuel", "Leskovec", "Jure", "Krause", "Andreas"], "venue": "In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2010}, {"title": "Simultaneous learning and covering with adversarial noise", "authors": ["Guillory", "Andrew", "Bilmes", "Jeff A"], "venue": "In ICML,", "year": 2011}, {"title": "Batch mode active learning and its application to medical image classification", "authors": ["Hoi", "Steven CH", "Jin", "Rong", "Zhu", "Jianke", "Lyu", "Michael R"], "venue": "In Proceedings of the 23rd international conference on Machine learning,", "year": 2006}, {"title": "Transitivity in structural models of small groups", "authors": ["Holland", "Paul W", "Leinhardt", "Samuel"], "venue": "Comparative Group Studies,", "year": 1971}, {"title": "Curvature and optimal algorithms for learning and minimizing submodular functions", "authors": ["Iyer", "Rishabh K", "Jegelka", "Stefanie", "Bilmes", "Jeff A"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Cutting-plane training of structural SVMs", "authors": ["T. Joachims", "T. Finley", "Yu", "Chun-Nam"], "venue": "Machine Learning,", "year": 2009}, {"title": "Maximizing the spread of influence through a social network", "authors": ["Kempe", "David", "Kleinberg", "Jon", "Tardos", "\u00c9va"], "venue": "In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining,", "year": 2003}, {"title": "Submodular function maximization", "authors": ["Krause", "Andreas", "Golovin", "Daniel"], "year": 2014}, {"title": "Topk multiclass svm", "authors": ["Lapin", "Maksim", "Hein", "Matthias", "Schiele", "Bernt"], "venue": "In Advances in Neural Information Processing Systems, pp", "year": 2015}, {"title": "Submodular inference of diffusion networks from multiple trees", "authors": ["Rodriguez", "Manuel Gomez", "Sch\u00f6lkopf", "Bernhard"], "venue": "arXiv preprint arXiv:1205.1671,", "year": 2012}, {"title": "Diffusion of innovations", "authors": ["E.M. Rogers"], "venue": "Free Press of Glencoe,", "year": 1962}, {"title": "A theory of the learnable", "authors": ["Valiant", "Leslie G"], "venue": "Communications of the ACM,", "year": 1984}, {"title": "Virality prediction and community structure in social networks", "authors": ["Weng", "Lilian", "Menczer", "Filippo", "Ahn", "Yong-Yeol"], "venue": "Scientific reports,", "year": 2013}], "id": "SP:5c22e93e1b6fe23880bd7a83419ab3dd1b4e9bec", "authors": [{"name": "Nir Rosenfeld", "affiliations": []}, {"name": "Eric Balkanski", "affiliations": []}, {"name": "Amir Globerson", "affiliations": []}, {"name": "Yaron Singer", "affiliations": []}], "abstractText": "Submodular functions have become a ubiquitous tool in machine learning. They are learnable from data, and can be optimized efficiently and with guarantees. Nonetheless, recent negative results show that optimizing learned surrogates of submodular functions can result in arbitrarily bad approximations of the true optimum. Our goal in this paper is to highlight the source of this hardness, and propose an alternative criterion for optimizing general combinatorial functions from sampled data. We prove a tight equivalence showing that a class of functions is optimizable if and only if it can be learned. We provide efficient and scalable optimization algorithms for several function classes of interest, and demonstrate their utility on the task of optimally choosing trending social media items.", "title": "Learning to Optimize Combinatorial Functions"}