4	30	Most computational models for sarcasm detection have considered utterances in isolation (Davidov et al., 2010; González-Ibáñez et al., 2011; Liebrecht et al., 2013; Riloff et al., 2013; Maynard and Greenwood, 2014; Joshi et al., 2015; Ghosh et al., 2015; Joshi et al., 2016; Ghosh and Veale, 2016).
5	32	In many instances, even humans have difficulty in recognizing sarcastic intent when considering an utterance in isolation (Wallace et al., 2014).
6	85	In this paper, we investigate the role of conversation context in detecting sarcasm in social media discussions (Twitter conversations and discussion forums).
7	31	Table 1 shows some examples of sarcastic replies taken from two media platforms (userB 186 and userD’s posts, respectively) and a minimum unit of conversation context given by the prior turn (userA and userC’s posts, respectively).
8	60	We address two specific issues: (1) does modeling of conversation context help in sarcasm detection and (2) can we understand what part of conversation context triggered the sarcastic reply (e.g., which sentence(s) from userC’s comment triggered userD’s sarcastic reply).
12	23	We make all datasets and code available.2
13	50	One goal of our investigation is to comparatively study two types of social media platforms that have been considered individually for sarcasm detection: discussion forums and Twitter.
15	32	Oraby et al. (2016) have introduced the Sarcasm Corpus V2, a subset of the Internet Argument Corpus that consists of discussion forum data.
16	35	This corpus consists of sarcastic responses and their context (quotes to which the posts are replies to).
17	59	The annotation of sarcastic vs. non-sarcastic replies was done using crowdsourcing, where annotators were asked to label a reply as sarcastic if any part of the reply contained sarcasm (thus annotation is done at the reply/comment level and not sentence level).
35	25	To assess the effect of conversation context (c) on labeling a reply (r) as sarcastic or not sarcastic, we consider two binary classification tasks.
42	26	3.1 SVM with discrete features (SVMbl) For features, we used n-grams, lexicon-based features, and sarcasm indicators that are commonly used in the existing sarcasm detection approaches (Tchokni et al., 2014; González-Ibáñez et al., 2011; Riloff et al., 2013; Joshi et al., 2015; Ghosh et al., 2015; Muresan et al., 2016).
52	57	We use morpho-syntactic features such as interjections (e.g., “uh”, “oh”, “yeah”), tag questions (e.g., “is not it?”, “don’t they”), exclamation marks (e.g., “!”, “?”); typographic features such as capitalization of words, quotation marks, emoticons; tropes such as superlative and intensifiers words (e.g., “greatest”, “best”, “really”) that often occur in sarcastic utterances (Camp, 2012).
82	33	In other words, LSTMr is conditioned on the representation of LSTMc that is built on the context.
85	22	For Twitter we used the skip-gram word-embeddings (100-dimension) used in (Ghosh et al., 2015) that was built using over 2.5 million tweets.4 For discussion forums, we use the standard Google n-gramword2vec pre-trained model (300- dimension) (Mikolov et al., 2013).
93	30	LSTMconditional is the conditional encoding model (no attention).
94	65	Discussion Forums: Table 2 shows the classification results on the discussion forum dataset.
120	31	We designed an Amazon Mechanical Turk task (for brevity, MTurk) framed as follow: Given a pair of context c and a sarcastic reply r from the discussion forum dataset, identify one or more sentences in c that may trigger the sarcastic reply r. Turkers could select one or more sentences from the context c, including the entire context.
129	102	We found that 41% of times the sentence with the highest attention weight is also the one picked by Turkers.
130	74	Figure 2 shows side by side the heat maps of the attention weights of LSTM models (LHS) and Turkers’ choices when picking up sentences from context that they thought triggered the sarcastic reply (RHS).
135	33	Looking at this example it seems the model pays attention to output vectors that are semantically coherent between c and r. The sarcastic response of this example contains a single sentence – “.
142	44	Incongruity between context and reply The meaning incongruity is an inherent characteristic of irony and sarcasm and have been extensively studied in linguistics, philosophy, communication science (Grice et al., 1975; Attardo, 2000; Burgers et al., 2012) as well as recently in NLP (Riloff et al., 2013; Joshi et al., 2015).
143	32	For instance, Riloff et al. (2013) pointed out that identifying the incongruity between positive sentiment towards a negative situation is a key characteristic of sarcasm detection in social media.
152	43	Word-models seem to also work well when words in the context/reply are semantically incongruous but connected via deeper semantics (“bums” and “welfare” in context: “someone needs to remind these bums they work for the people” and reply: “feels like we are paying them welfare” (Figure 6).
153	41	Attention weights and sarcasm markers Looking just at attention weights in reply, we notice the models are giving highest weight to sentences that contain sarcasm markers, such as emoticons (i.e., “:p”, “:)”) and interjections (i.e., “ah”, “hmm”).
158	34	Rocktäschel et al. (2015) have argued that interpretations based on attentions weights have to be taken with care since the classification task is not forced to solely rely on the attentions weights.
172	32	In particular, conditional LSTM networks (Rocktäschel et al., 2015) and LSTM networks with sentence level attention achieved significant improvement (e.g., 6-11% F1 for discussion forums and Twitter messages).
174	58	We also showed that attention-based models are able to identify inherent characteristics of sarcasm (i.e., sarcasm markers and sarcasm factors such as context in- congruity).
175	32	In future, we plan to study larger context, such as the full thread in a discussion forum that consider also the responses to the sarcastic comment, when available.
176	30	We are also interested in analyzing sarcastic replies that do not contain sarcasm markers or explicit incongruence (i.e., opposing sentiment between the context and the reply).
