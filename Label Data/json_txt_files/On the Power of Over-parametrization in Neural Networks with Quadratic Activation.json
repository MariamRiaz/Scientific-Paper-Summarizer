{"sections": [{"text": "\u221a 2n, over-parametrization\nenables local search algorithms to find a globally optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when k \u2265 \u221a 2n, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks."}, {"heading": "1. Introduction", "text": "Neural networks have achieved a remarkable impact on many applications such computer vision, reinforcement learning and natural language processing. Though neural networks are successful in practice, their theoretical properties are not yet well understood. Specifically, there are two intriguing empirical observations that existing theories cannot explain.\n\u2022 Optimization: Despite the highly non-convex nature of the objective function, simple first-order algorithms like stochastic gradient descent are able to minimize the training loss of neural networks. Researchers have conjectured that the use of over-parametrization (Livni et al., 2014; Safran and Shamir, 2017) is the primary\n1Machine Learning Department, Carnegie Mellon University 2Department of Data Sciences and Operations, University of Southern California. Correspondence to: Simon S. Du <ssdu@cs.cmu.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nreason why local search algorithms can achieve low training error. The intuition is over-parametrization alters the loss function to have a large manifold of globally optimal solutions, which in turn allows local search algorithms to more easily find a global optimal solution.\n\u2022 Generalization: From the statistical point of view, over-parametrization may hinder effective generalization, since it greatly increases the number of parameters to the point of having number of parameters exceed the sample size. To address this, practitioners often use explicit forms of regularization such as weight decay, dropout, or early stopping to improve generalization. However in the non-convex setting, theoretically, we do not have a good quantitative understanding on how these regularizations help generalization for neural network models.\nIn this paper, we provide new theoretical insights into the optimization landscape and generalization ability of overparametrized neural networks. Specifically we consider the neural network of the following form:\nf(x,W) = k\u2211 j=1 ai\u03c3 (\u3008wj ,x\u3009) . (1)\nIn the above x \u2208 Rd is the input vector, W \u2208 Rd\u00d7k with wj \u2208 Rd denotes the j-th row of W and ai\u2019s are the weights in the second layer. Finally \u03c3 (\u00b7) : R \u2192 R denotes the activation function applied to each hidden node. When the neural network is over-parameterized, the number of hidden notes k can be very large compared with input dimension d or the number of training samples.\nIn our setting, we fix the second layer to be a = (1, . . . , 1). Although it is simpler than the case where the second layer is not fixed, the effect of over-parameterization can be studied in this setting as well because we do not have any restriction on the number of hidden nodes.\nWe focus on quadratic activation function \u03c3 (z) = z2. Though quadratic activations are rarely used in practice, stacking multiple such two-layer blocks can be used to simulate higher-order polynomial neural networks and sigmodial\nactivated neural networks (Livni et al., 2014; Soltani and Hegde, 2017).\nIn practice, we have n training samples {xi, yi}ni=1 and solve the following optimization problem to learn a neural network\nmin W\n1\nn n\u2211 i=1 ` (f(xi,W), yi)\nwhere `(\u00b7, \u00b7) is some loss function such as `2 or logistic loss. For gradient descent we use the following update\nW\u2190W \u2212 \u03b7 n\u2211 i=1 \u2207W` (f(xi), yi)\nwhere \u03b7 is the step size.\nTo improve the generalization ability, we often add explicit regularization. In this paper, we focus on a particular regularization technique, weight decay for which we slightly change the gradient descent algorithm to\nW\u2190W \u2212 \u03b7 n\u2211 i=1 \u2207W` (f(xi,W), yi)\u2212 \u03b7\u03bbW.\nwhere \u03bb is the decay rate. Note this algorithm is equivalent to applying the gradient descent algorithm on the regularized loss\nmin W\nL (W) = 1\nn n\u2211 i=1 ` (f(xi), yi) + \u03bb 2 \u2016W\u20162F . (2)\nIn this setup, we make the following theoretical contributions to explain why over-parametrization helps optimization and still allows for generalization."}, {"heading": "1.1. Main Contributions", "text": "Over-parametrization Helps Optimization. We analyze two kinds of over-parameterization. First we show that for\nk \u2265 d,\nthen all local minima in Problem (2) is global and all saddle points are strict. This properties together with recent algorithmic advances in non-convex optimization (Lee et al., 2016) imply gradient descent can find a globally optimal solution with random initialization. This is a minor generalization of results in (Soltanolkotabi et al., 2017) which only includes `2 loss, and (Haeffele and Vidal, 2015; Haeffele et al., 2014) which only include k \u2265 d+ 1.\nSecond, we consider another form of over-parametrization,\nk(k + 1)\n2 > n.\nThis condition on the amount of over-parameterization is much milder than k \u2265 n, a condition used in many previous papers (Nguyen and Hein, 2017a;b). Further in practice, k(k + 1)/2 > n is a much milder requirement than k \u2265 d, since if k \u2248 \u221a 2n and n << d2 then k << d. In this setting, we consider the perturbed version of the Problem (2):\nmin W\nLC (W) = 1\nn n\u2211 i=1 ` (f(xi), yi) + \u03bb 2 \u2016W\u20162F + \u3008C,W>W\u3009 (3)\nwhere C is a random positive semidefinite matrix with arbitrarily small Frobenius norm. We show that if k(k+1)2 > n, Problem (3) also has the desired properties that all local minima are global and all saddle points are strict with probability 1. Since C has small Frobenius norm, the optimal value of Problem (3) is very close to that of Problem (2). See Section 3 for the precise statement.\nTo prove this surprising fact, we bring forward ideas from smoothed analysis in constructing the perturbed loss function (3), which we believe is useful for analyzing the landscape of non-convex losses.\nWeight-decay Helps Generalization. We show because of weight-decay, the optimal solution of Problem (2) also generalizes well. The major observation is weight-decay ensures the solution of Problem (2) has low Frobenius norm, which is equivalent to matrix W>W having low nuclear norm (Srebro et al., 2005). This observation allows us to use theory of Rademacher complexity to directly obtain quantitative generalization bounds. Our theory applies to a wide range of data distribution and in particular, does not need to assume the model is realizable. Further, the generalization bound does not depend on the number of epochs SGD runs or the number of hidden nodes.\nTo sum up, in this paper we justify the following folklore.\nOver-parametrization allows us to find global optima and with weight decay, the solution also generalizes well."}, {"heading": "1.2. Organization", "text": "This paper is organized as follows. In Section 2 we introduce necessary background and definitions. In Section 3 we present our main theorems on why over-parametrization helps optimization when k \u2265 d or k(k+1)2 > n. In Section 4, we give quantitative generalization bounds to explain why weight decay helps generalization in the presence of over-parametrization. In Section 5, we prove our main theorems. We conclude and list future works in Section 6."}, {"heading": "1.3. Related Works", "text": "Neural networks have enjoyed great success in many practical applications (Krizhevsky et al., 2012; Dauphin et al., 2016; Silver et al., 2016). To explain this success, many works have studied the expressiveness of neural networks. The expressive ability of shallow neural network dates back to 90s (Barron, 1994). Recent results give more refined analysis on deeper models (Bo\u0308lcskei et al., 2017; Telgarsky, 2016; Wiatowski et al., 2017).\nHowever, from the point of view of learning theory, it is well known that training a neural network is hard in the worst case (Blum and Rivest, 1989). Despite the worst-case pessimism, local search algorithms such as gradient descent are very successful in practice. With some additional assumptions, many works tried to design algorithms that provably learn a neural network (Goel et al., 2016; Sedghi and Anandkumar, 2014; Janzamin et al., 2015). However these algorithms are not gradient-based and do not provide insight on why local search algorithm works well.\nFocusing on gradient-based algorithms, a line of research (Tian, 2017; Brutzkus and Globerson, 2017; Zhong et al., 2017a;b; Li and Yuan, 2017; Du et al., 2017b;c) analyzed the behavior of (stochastic) gradient descent with a structural assumption on the input distribution. The major drawback of these papers is that they all focus on the regression setting with least-squares loss and further assume the model is realizable meaning the label is the output of a neural network plus a zero mean noise, which is unrealistic. In the case of more than one hidden unit, the papers of (Li and Yuan, 2017; Zhong et al., 2017b) further require a stringent initialization condition to recover the true parameters.\nFinding the optimal weights of a neural network is nonconvex problem. Recently, researchers found that if the objective functions satisfy the following two key properties: (1) all local minima are global and (2) all saddle points and local maxima are strict, then first order method like gradient descent (Ge et al., 2015; Jin et al., 2017; Levy, 2016; Du et al., 2017a; Lee et al., 2016) can find a global minimum.\nThis motivates the research of studying the landscape of neural networks (Kawaguchi, 2016; Choromanska et al., 2015; Freeman and Bruna, 2016; Zhou and Feng, 2017; Nguyen and Hein, 2017a;b; Ge et al., 2017; Safran and Shamir, 2017; Soltanolkotabi et al., 2017; Poston et al., 1991; Haeffele and Vidal, 2015; Haeffele et al., 2014; Soudry and Hoffer, 2017) In particular, Haeffele and Vidal (2015); Poston et al. (1991); Nguyen and Hein (2017a;b)studied the effect of over-parameterization on training the neural networks. These results require a large amount of overparameterization that the width of one of the hidden layers has to be greater than the number of training examples, which is unrealistic in commonly used neural networks. Re-\ncently, Soltanolkotabi et al. (2017) showed for shallow neural networks, the number of hidden nodes is only required to be larger or equal to the input dimension for `2-loss. In comparison, our theorems work for general loss functions with regularization under the same assumption. Further we also propose a new form of over-parameterization, namely as long as k \u2265 \u221a 2n, the loss function also admits a benign landscape.\nWe now turn our attention to generalization ability of learned neural networks. It is well known that the classical learning theory cannot explain the generalization ability because VCdimension of neural networks is large (Harvey et al., 2017; Zhang et al., 2016). A line of research tries to explain this phenomenon by studying the implicit regularization from stochastic gradient descent algorithm (Hardt et al., 2015; Pensia et al., 2018; Mou et al., 2017; Brutzkus et al., 2017; Li et al., 2017). However, the generalization bounds of these papers often depend on the number of epochs SGD runs, which is large in practice. Another direction is to study the generalization ability based on the norms of weight matrices in neural networks (Neyshabur et al., 2015; 2017a;b; Bartlett et al., 2017; Liang et al., 2017; Golowich et al., 2017; Dziugaite and Roy, 2017; Wu et al., 2017). Our theorem on generalization ability also uses this idea but is more specialized to the network architecture (1).\nAfter the initial submission of this manuscript, we became aware of concurrent work of (Bhojanapalli et al., 2018), which also considered the smoothed analysis technique to solve semi-definite programs in penalty form. The mathematical techniques in our work and (Bhojanapalli et al., 2018) are similar, but the focus is on two distinct problems of solving semi-definite programs and quadratic activation neural networks."}, {"heading": "2. Preliminaries", "text": "We use bold-faced letters for vectors and matrices. For a vector v, we use \u2016v\u20162 to denote the Euclidean norm. For a matrix M, we denote \u2016M\u20162 the spectral norm and \u2016M\u2016F the Frobenius norm. We let N (M) to denote the left nullspace of M, i.e.\nN (M) = { v : v>M = 0 } .\nWe use \u03a3 (M) to denote the set of matrices with Frobenius norm bounded by M and \u03a31 (1) to denote the set of rank-1 matrices with spectral norm bounded by 1. We also denote Sd the set of d\u00d7d symmetric positive semidefinite matrices.\nIn this paper, we characterize the landscape of overparameterized neural networks. More specifically we study the properties of critical points of empirical loss. Here for a loss function L (W), a critical point W\u2217 satisfies \u2207L (W\u2217) = 0. A critical point can be a local minimum or\na saddle point.1 If W\u2217 is a local minimum, then there is a neighborhood O around W\u2217 such that L (W\u2217) \u2264 L (W) for all W \u2208 O. If W\u2217 is a saddle point, then for all neighborhood O around W\u2217, there is a W \u2208 O such that L (W) < L (W\u2217).\nIdeally, we would like a loss function that satisfies the following two geometric properties.\nProperty 2.1 (All local minima are global). If W\u2217 is a local minimum of L (\u00b7) it is also the global minimum, i.e., W\u2217 \u2208 argminWL (W). Property 2.2 (All saddles are strict). At a saddle point Ws, there is a direction U \u2208 Rk\u00d7d such that\nvect (U)>\u22072L (Ws) vect (U) < 0.\nIf a loss function L (\u00b7) satisfies Property 2.1 and Property 2.2, recent algorithmic advances in non-convex optimization show randomly initialized gradient descent algorithm or perturbed gradient descent can find a global minimum (Lee et al., 2016; Ge et al., 2015; Jin et al., 2017; Du et al., 2017a).\nLastly, standard applications of Rademacher complexity theory will be used to derive generalization bounds.\nDefinition 2.1 (Definition of Rademacher Complexity). Given a sample S = (x1, . . . ,xn), the empirical Rademacher complexity of a function class F is defined as\nRS (F) = 1\nn E\u03c3 [ sup f\u2208F n\u2211 i=1 \u03c3if(xi) ]\nwhere \u03c3 = (\u03c31, . . . , \u03c3m) are independent random varaibles drawn from the Rademacher distribution, i.e., P (\u03c3i = 1) = P (\u03c3i = \u22121) = 1/2 for i = 1, . . . , n."}, {"heading": "3. Overparametrization Helps Optimization", "text": "In this section we present our main results on explaining why over-parametrization helps local search algorithms find a global optimal solution. We consider two kinds of overparameterization, k \u2265 d and k(k+1)2 > n. We begin with the simpler case when k \u2265 d. Theorem 3.1. Assume we have an arbitrary data set of input/label pairs xi \u2208 Rd and yi \u2208 R for i = 1, . . . , n and a convex C2 loss `(y\u0302, y). Consider a neural network of the form f(x,W) = \u2211k j=1 \u03c3 ( w>j x ) with \u03c3 (z) = z2 and W \u2208 Rk\u00d7d denoting the weights connecting input to hidden layers. Suppose k \u2265 d. Then, the training loss as a\n1We do not differentiate between saddle points and local maxima in this paper.\nfunction of weight W of the hidden layers\nL (W) = 1\n2n ` (f(xi,W), yi) +\n\u03bb 2 \u2016W\u20162F\nobeys Property 2.1 and Property 2.2.\nThe above result states that given an arbitrary data set, the optimization landscape has benign properties that facilitate finding globally optimal neural networks. In particular, by setting the last layer to be the average pooling layer, all local minima are global minima and all saddles have a direction of negative curvature. This in turn implies that gradient descent on the first layer weights, when initialized at random, converges to a global optimum. These desired properties hold as long as the hidden layer is wide (k \u2265 d).\nAn interesting and perhaps surprising aspect of Theorem 3.1 is its generality. It applies to arbitrary data set of any size with any convex differentiable loss function.\nNow we consider the second case when k(k+1)2 > n. As mentioned earlier, in practice this is often a milder requirement than k \u2265 d, and one of the main novelties of this paper.\nTheorem 3.2. Assume we have an arbitrary data set of input/label pairs xi \u2208 Rd and yi \u2208 R for i = 1, . . . , n, and a convex C2 loss `(y\u0302, y). Consider a neural network of the form f(x) = \u2211k j=1 \u03c3 ( w>j x ) with \u03c3 (z) = z2 and W \u2208 Rk\u00d7d denoting the weights connecting input to hidden layers. Suppose k(k+1)2 > n, k < d and C is a random positive semidefinite matrix with \u2016C\u2016F \u2264 \u03b4 whose distribution is absolutely continuous with respect to Lebesgue measure. Then, the training loss as a function of weight W of the hidden layers\nL (W) = 1\n2n ` (f(xi), yi) +\n\u03bb 2 \u2016W\u20162F + \u3008C,W >W\u3009 (4)\nobeys Property 2.1 and Property 2.2. Further, any global optimal solution W\u0302of Problem (4) satisfies\n1\n2n n\u2211 i=1 ` ( f(xi,W\u0302), yi ) + \u03bb 2 \u2225\u2225\u2225W\u0302\u2225\u2225\u22252 F\n\u2264 1 2n n\u2211 i=1 ` (f(xi,W \u2217), yi) + \u03bb 2 \u2016W\u2217\u2016F2 + \u03b4 \u2016W \u2217\u20162F\nwhere\nW\u2217 \u2208 argminW n\u2211 i=1 1 2n ` (f(xi,W), yi) + \u03bb 2 \u2016W\u2217\u20162F .\nSimilar to Theorem 3.1, Theorem 3.2 states that if k(k+1)2 > n, then for an arbitrary data set, the perturbed objective\nfunction (3) has the desired properties that enable local search heuristics to find globally optimal solution for a general class of loss functions. Further, we can choose this perturbation to be arbitrarily small so the minimum of (3) is close to (2).\nThe proof of theorem is inspired by a line of literature started by Pataki (1998; 2000); Burer and Monteiro (2003); Boumal et al. (2016). In summary, Boumal et al. (2016) showed that for \u201calmost all\u201d semidefinite programs, every local minima of the rank r non-convex formulation of an SDP is a global minimum of the original SDP. However, this theorem applies with the important caveat of only applying to semidefinite programs that do not fall into a measure zero set. Our primary contribution is to develop a procedure that exploits this by a) constructing a perturbed objective to avoid the measure zero set, b) proving that the perturbed objective has Property 2.1 and 2.2, and c) showing the optimal value of the perturbed objective is close to the original objective. Further note that the analysis of (Boumal et al., 2016) does not apply since our loss functions, such as the logistic loss, are not semi-definite representable. We refer readers to Section 5.2 for more technical insights."}, {"heading": "4. Weight-decay Helps Generalization", "text": "In this section we switch our focus to the generalization ability of the learned neural network. Since we use weightdecay or equivalently `2 regularization in (2), the Frobenius norm of learned weight W is bounded. Therefore, in this section we focus weight matrix in bounded Frobenius norm space, i.e., \u2016W\u2016F \u2208 \u03a3 (M).\nTo derive the generalization bound, we first recall the classical generalization bound based on Rademacher complexity bound (c.f. Theorem 2 of (Koltchinskii and Panchenko, 2002)).\nTheorem 4.1. Assume each data point is sampled i.i.d from some distribution P , i.e.,\n(xi, yi) \u223c P for i = 1, . . . , n.\nWe denote S = {xi, yi}ni=1 and Ltr (W) = 1 n \u2211n i=1 ` (f(xi,W), yi) and Lte (W) = E(x,y)\u223cP [` (f(xi,W), yi)]. Suppose loss function `(\u00b7, \u00b7) is L-Lipschitz in the first argument, then for all W \u2208 \u03a3 (M), we have with high probability\nLte (W)\u2212 Ltr (W) \u2264 CL \u00b7RS (\u03a3 (M))\nwhere C > 0 is an absolute constant and RS (\u03a3 (M)) is the Rademacher complexity of \u03a3 (M).\nWith Theorem 4.1 at hand, we only need to bound the Rademacher complexity of \u03a3 (M). Note that Rademacher complexity is a distribution dependent quantity. If the data\nis arbitrary, we cannot have any guarantee. We begin with a theorem for bounded input domain. Theorem 4.2. Suppose input is sampled from a bounded ball in Rd, i.e., \u2016x\u20162 \u2264 b for some b > 0 and E [\u2225\u2225xx>\u2225\u22252\n2\n] \u2264 B, then the Rademacher complexity sat-\nisfies\nRS (\u03a3 (M)) \u2264 \u221a 2b4M4 log d\nn .\nCombining Theorem 4.1 and Theorem 4.2 we can obtain a generalization bound. Theorem 4.3. Under the same assumptions of Theorem 4.1 and Theorem 4.2, we have\nLte (W)\u2212 Ltr (W) \u2264 CLM2b2 \u221a log d\nn .\nfor some absolute constant C > 0.\nWhile Theorem 4.2 is a valid bound, it is rather pessimistic because we only assume x is bounded. Consider the following scenario in which each input is sampled from a standard Gaussian distribution xi \u223c N (0, I). Then ignoring the logarithmic factors, using standard Gaussian concentration bound we can show with high probability \u2016xi\u20162 = O\u0303 (\u221a d )\n. 2 Plugging in this bound we have\nLte (W)\u2212 Ltr (W) \u2264 CLM2 \u221a d2 log d\nn (5)\nNote in this bound, it has a quadratic dependency on the dimension, so we need to have n = \u2126 ( d2 )\nto have a meaningful bound.\nIn fact, for specific distributions like Gaussian using Theorem 5.2, we can often derive a stronger generalization bound. Corollary 4.1. Suppose xi \u223c N(0, I) for i = 1, . . . , n. If the number of samples satisfies n \u2265 d log d, we have with high probability that Rademacher complexity satisfies\nRS (\u03a3 (M)) \u2264 C \u221a M4d\nn .\nfor some absolute constant C > 0.\nAgain, combining Theorem 4.1 and Corollary 4.1 we obtain the following generalization bound for Gaussian input distribution Theorem 4.4. Under the same assumptions of Theorem 4.1 and Corollary 4.1, we have\nLte (W)\u2212 Ltr (W) \u2264 CLM2 \u221a d\nn .\nfor some absolute constant C > 0.\n2O\u0303(\u00b7) hides logarithmic factors.\nComparing Theorem 4.4 with generalization bound (5), Theorem 4.4 has an O( \u221a d) advantage. Theorem 4.4 has the\u221a\nd/n dependency, which is the usual parametric rate. Further in practice, number of training samples and input dimension are often of the same order for common datasets and architectures (Zhang et al., 2016).\nCorollary 4.1 is a special case of the more general Theorem 5.2 which only requires a bound on the fourth moment,\u2225\u2225\u2211n\ni=1(xix > i )\n2 \u2225\u2225\n2 \u2264 s. In general, our theorems suggest if\nthe Frobenius norm of weight matrix W is small and the input x is sampled from a benign distribution with controlled 4th moments, then we have good generalization.\nAs a concrete scenario, consider a favorable setting where the true data can be correctly classified by a small network using only k0 k hidden units. The weights W \u2217 are nonzero only in the first k0 rows and maxj\u2208[k0]\n\u2225\u2225e>j W \u2217\u2225\u22252 \u2264 R. From Theorem 4.4 to reach generalization gap of , we have sample complexity of n \u2265 1 2C\n2L2R4dk20 , which only depends on the effective number of hidden units k0 k. The same result can be reached for more general input distributions by using Theorem 5.2 in place of Theorem 4.4."}, {"heading": "5. Proofs", "text": ""}, {"heading": "5.1. Proof of Theorem 3.1 and Theorem 3.2", "text": "Our proofs of over-parametrization helps optimization build upon existing geometric characterization on matrix factorization. We first cite a useful Theorem by Haeffele et al. (2014).3\nTheorem 5.1 (Theorem 2 of (Haeffele et al., 2014) adapted to our setting). Let ` (\u00b7, \u00b7) be a twice differentiable convex function in the first argument. If the function L (W) defined in (2) at a rank-deficient matrix W satisfies\n\u2207L (W) = 0 and \u22072L (W) < 0,\nthen W is a global minimum.\nProof of Theorem 3.1. We prove Property 2.1 and Property 2.2 simultaneously by showing if a W satisfy\n\u2207L (W) = 0 and \u22072L (W) < 0\nthen it is a global minimum.\nIf rank (W) < d, we can directly apply Theorem 5.1. Thus it remains to consider the case rank (W) = d. We first\n3Theorem 2 of (Haeffele et al., 2014) assumes W is a local minimum, but scrutinizing its proof, we can see that the assumption can be relaxed to\u2207L (W) = 0 and\u22072L (W) < 0.\nnotice that \u2207L (W) = 0 is equivalent to\nW\n( 1\nn n\u2211 i=1 \u2202` (y\u0302i, yi) \u2202y\u0302i xix > i + \u03bbI\n) = 0 (6)\nwhere y\u0302i = f(xi,W). Since rank (W) = d and k \u2265 d, we know W has a left pseudo-inverse, i.e., there exists W\u2020 such that W\u2020W = I. Multiplying W\u2020 on the left in Equation (6), we have\n1\nn n\u2211 i=1 \u2202` (y\u0302i, yi) \u2202y\u0302i xix > i + \u03bbI = 0. (7)\nTo prove Theorem 3.1, the key idea is to consider the follow reference optimization problem.\nmin M:M\u2208Sd\nL (M) = 1\nn n\u2211 i=1 ` ( x>i Mxi, yi ) + \u03bb \u2016M\u2016\u2217 . (8)\nProblem (8) is a convex optimization problem in M and has the same global minimum as the original problem. Now we denote y\u0303i = x>i Mxi. Since this is a convex function, the first-order optimality condition for global optimality is\n0 \u2208 1 n n\u2211 i=1 \u2202` (y\u0303i, yi) \u2202y\u0303i xix > i + \u03bb\u2202 \u2016M\u2016\u2217 ,\nM is a global minimum.\nUsing Equation (7), we know M = W>W achieves the global minimum in Problem (8). The proof is thus complete."}, {"heading": "5.2. Proof of Theorem 3.2", "text": "Proof. We first prove LC (W) satisfies Property 2.1 and Property 2.2. Similar to the proof of Theorem 3.1, we prove these two properties simultaneously by showing if a W satisfy\n\u2207LC (W) = 0 and \u22072LC (W) < 0 (9)\nthen it is a global minimum. Because of Theorem 5.1, we only need to show that if W satisfy condition (9), it is rank deficient, i.e. rank (W) < k.\nFor the gradient condition, we have\nW\n( 1\nn n\u2211 i=1 \u2202` (y\u0302i, yi) \u2202y\u0302i xix > i + \u03bbI\n) + WC = 0.\nFor simplicity we denote vi = \u2202`(y\u0302i,yi) \u2202y\u0302i where y\u0302i = x>i W >Wxi and S (v) = \u2211n i=1 vixix > i . Using the first order condition we know W is in the null space of (S (v) + \u03bbI + C). Thus, we can bound the rank of W by\nrank (W) \u2264dimN (S (v) + \u03bbI + C) \u2264max\nv dimN (S (v) + \u03bbI + C) .\nWe prove by contradiction. Assume rank (W) \u2265 k, we must have\nk \u2264 max v N (S (v) + \u03bbI + C) .\nNow define M = S (v\u2217) + \u03bbI + C with\nv\u2217 = argmax dimN (S (v) + \u03bbI + C) .\nThus we have following conditions\nC =M\u2212 S (v\u2217)\u2212 \u03bbI, dimN (M) \u2265k.\nThe key idea is to use these two conditions to upper bound the dimension of C. To this end, we first define the set\nB` = { A : A = M\u2212 S (v)\u2212 \u03bbI,M \u2208 Sd,v \u2208 Rn,\ndimN (M\u2212 \u03bbI) = `} .\nNote that the dimension of the manifold B` is( d(d+ 1)\n2 \u2212 ` (`+ 1) 2\n) + n\nwhere the first term is the dimension of d\u00d7 d matrices, the second term is the dimension of the null space and the last term is dimension of Range(S (v)) for v \u2208 Rn, which is upper bounded by n.\nNext note that B`1 \u2282 B`2 for `1 \u2265 `2. Therefore, we can compute the dimension of the union\ndim ( \u222ad`=kB` ) =\n( d(d+ 1)\n2 \u2212 k(k + 1) 2\n) + n.\nNote because we assume k(k+1)2 > n, we have dim ( \u222ad`=kB` ) < d(d+1)2 . However, recall C \u2208 \u222a d `=kB` by definition, so we have C is sampled from a low-dimensional manifold which has Lebesuge measure 0. Since we sample C from a distribution that is absolute continuous with respect to the Lebesgue measure, the event { C \u2208 \u222ad`=kB`\n} happens with probability 0. Therefore, with probability 1, rank (W) < k. The proof of the first part of Theorem 3.2 is complete.\nFor the second part. Let W\u0302 = argminWLC (W) and W\u2217 = argminW (W). Therefore we have\nL ( W\u0302 ) + \u3008C,W\u0302>W\u0302\u3009\n\u2264L (W\u2217) + \u3008C, (W\u2217)>W\u2217\u3009\n\u2264L (W\u2217) + \u03b4 \u2016W\u2217\u20162F .\nNote because C and W\u0302>W\u0302 are both positive semidefinite, we have \u3008C,W\u0302>W\u0302\u3009 \u2265 0. Thus L ( W\u0302 ) \u2264 L (W\u2217) +\n\u03b4 \u2016W\u2217\u20162F ."}, {"heading": "5.3. Proof of Theorem 4.2 and Theorem 4.4", "text": "Our proof is inspired by (Srebro and Shraibman, 2005) which exploits the structure of nuclear norm bounded space. We first prove a general Theorem that only depends on the property of the fourth-moment of input random variables. Theorem 5.2. Suppose the input random variable satisfies\u2225\u2225\u2225\u2211ni=1 (xix>i )2\u2225\u2225\u2225 2 \u2264 s. Then the Rademacher complexity of \u03a3 (M) is bounded by\nRS (\u03a3 (M)) \u2264 \u221a 2M4s log d\nn .\nProof. For a given set of inputs S = {xi}ni=1 in our context, we can write Rademacher complexity as\nRS (\u03a3 (M)) = 1\nn E\u03c3\n[ sup\nW\u2208\u03a3(M) n\u2211 i=1 \u03c3ix > i W >Wxi\n]\nSince Rademacher complexity does not change when taking convex combinations, we can first bound Rademacher complexity of the class of rank-1 matrices with spectral norm bounded by 1 and then take convex hull and scale by M . Note for W \u2208 \u03a31 (1), we can write W = vw> with \u2016w\u20162 \u2264 1 and \u2016v\u20162 = 1. Using this expression, we can obtain an explicit formula of Rademacher complexity.\nRS (\u03a31 (1))\n= 1\nn E\u03c3\n[ sup\nW\u2208\u03a31(1) n\u2211 i=1 \u03c3ix > i W >Wxi\n]\n= 1\nn E\u03c3\n[ sup\nW\u2208\u03a31(1) n\u2211 i=1 \u03c3ix > i ( w>v )> ( vw> ) xi\n]\n= 1\nn E\u03c3\n[ sup\nw:\u2016w\u20162\u22641 n\u2211 i=1 \u03c3ix > i ww >xi\n]\n= 1\nn E\u03c3\n[ sup\nw:\u2016w\u20162\u22641 n\u2211 i=1 \u03c3iw >x>i xiw\n]\n= 1\nn E\u03c3 [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03c3ixix > i \u2225\u2225\u2225\u2225\u2225 2 ] .\nNow, to bound E\u03c3 [\u2225\u2225\u2211n i=1 \u03c3ixix > i \u2225\u2225 2 ] , we can use the results from random matrix theory on Rademacher series. Recall that we assume\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 ( xix > i )2\u2225\u2225\u2225\u2225\u2225 2 \u2264 s\nand notice that\nE\u03c3 [ n\u2211 i=1 \u03c3ixix > i ] = 0.\nApplying Rademacher matrix series expectation bound (Theorem 4.6.1 of (Tropp et al., 2015)), we have\nE\u03c3 [\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 \u03c3ixix > i \u2225\u2225\u2225\u2225\u2225 2 ] \u2264 \u221a\u221a\u221a\u221a2 \u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 ( xix>i )2\u2225\u2225\u2225\u2225\u2225 2 log d\n\u2264 \u221a 2s log d.\nNow taking the convex hull and, scaling by M we obtain the desired result.\nWith Theorem 5.2 at hand, for different distributions, we only need to bound \u2225\u2225\u2225\u2211ni=1 (xix>i )2\u2225\u2225\u2225 2 .\nProof of Theorem 4.3. Since we assume \u2016xi\u20162 \u2264 b, we directly have\u2225\u2225\u2225\u2225\u2225 n\u2211 i=1 ( xix > i )2\u2225\u2225\u2225\u2225\u2225 2 \u2264 n\u2211 i=1 \u2225\u2225\u2225(xix>i )2\u2225\u2225\u2225 2\n= n\u2211 i=1 \u2016xi\u201622 \u2225\u2225xix>i \u2225\u22252 \u2264nb4.\nPlugging this bound in Theorem 5.2 we obtain the desired inequality.\nProof of Corollary 4.1 and Theorem 4.4. To prove Corollary 4.1, we use Theorem 5.2 and Lemma 4.7 in (Soltanolkotabi et al., 2017) to upper bound s =\u2225\u2225\u2225\u2211ni=1 \u2016xi\u20162 xix>i \u2225\u2225\u2225. By letting A = I in Lemma 4.7, we find that s \u2264 Cnd, with probability at least 1\u2212 Cd . This completes the proof of Corollary 4.1.\nUsing this bound in Theorem 5.2 comletes the proof of Theorem 4.4."}, {"heading": "6. Conclusion and Future Works", "text": "In this paper we provided new theoretical results on overparameterized neural networks. Using smoothed analysis, we showed as long as the number of hidden nodes is bigger than the input dimension or square root of the number of training data, the loss surface has benign properties that enable local search algorithms to find global minima. We further use the theory of Rademacher complexity to show the learned neural can generalize well.\nOur next step is consider neural networks with other activation functions and how over-parametrization allows for efficient local-search algorithms to find near global minimzers. Another interesting direction to extend our results to deeper model."}, {"heading": "7. Acknowledgment", "text": "S.S.D. was supported by NSF grant IIS1563887, AFRL grant FA8750-17-2-0212 and DARPA D17AP00001. J.D.L. acknowledges support of the ARO under MURI Award W911NF-11-1-0303. This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative."}], "year": 2018, "references": [{"title": "Approximation and estimation bounds for artificial neural networks", "authors": ["Andrew R Barron"], "venue": "Machine learning,", "year": 1994}, {"title": "Spectrally-normalized margin bounds for neural networks", "authors": ["Peter L Bartlett", "Dylan J Foster", "Matus J Telgarsky"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Smoothed analysis for low-rank solutions to semidefinite programs in quadratic penalty form", "authors": ["Srinadh Bhojanapalli", "Nicolas Boumal", "Prateek Jain", "Praneeth Netrapalli"], "venue": "arXiv preprint arXiv:1803.00186,", "year": 2018}, {"title": "Training a 3-node neural network is NP-complete", "authors": ["Avrim Blum", "Ronald L Rivest"], "venue": "In Advances in neural information processing systems,", "year": 1989}, {"title": "Optimal approximation with sparsely connected deep neural networks", "authors": ["Helmut B\u00f6lcskei", "Philipp Grohs", "Gitta Kutyniok", "Philipp Petersen"], "venue": "arXiv preprint arXiv:1705.01714,", "year": 2017}, {"title": "The non-convex burer-monteiro approach works on smooth semidefinite programs", "authors": ["Nicolas Boumal", "Vlad Voroninski", "Afonso Bandeira"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2016}, {"title": "Globally optimal gradient descent for a convnet with gaussian inputs", "authors": ["Alon Brutzkus", "Amir Globerson"], "venue": "arXiv preprint arXiv:1702.07966,", "year": 2017}, {"title": "Sgd learns over-parameterized networks that provably generalize on linearly separable data", "authors": ["Alon Brutzkus", "Amir Globerson", "Eran Malach", "Shai Shalev-Shwartz"], "venue": "arXiv preprint arXiv:1710.10174,", "year": 2017}, {"title": "A nonlinear programming algorithm for solving semidefinite programs via low-rank factorization", "authors": ["Samuel Burer", "Renato DC Monteiro"], "venue": "Mathematical Programming,", "year": 2003}, {"title": "The loss surfaces of multilayer networks", "authors": ["Anna Choromanska", "Mikael Henaff", "Michael Mathieu", "G\u00e9rard Ben Arous", "Yann LeCun"], "venue": "In Artificial Intelligence and Statistics,", "year": 2015}, {"title": "Language modeling with gated convolutional networks", "authors": ["Yann N Dauphin", "Angela Fan", "Michael Auli", "David Grangier"], "venue": "arXiv preprint arXiv:1612.08083,", "year": 2016}, {"title": "Gradient descent can take exponential time to escape saddle points", "authors": ["Simon S Du", "Chi Jin", "Jason D Lee", "Michael I Jordan", "Barnabas Poczos", "Aarti Singh"], "venue": "arXiv preprint arXiv:1705.10412,", "year": 2017}, {"title": "When is a convolutional filter easy to learn", "authors": ["Simon S Du", "Jason D Lee", "Yuandong Tian"], "venue": "arXiv preprint arXiv:1709.06129,", "year": 2017}, {"title": "Gradient descent learns one-hiddenlayer cnn: Don\u2019t be afraid of spurious local minima", "authors": ["Simon S Du", "Jason D Lee", "Yuandong Tian", "Barnabas Poczos", "Aarti Singh"], "venue": "arXiv preprint arXiv:1712.00779,", "year": 2017}, {"title": "Computing nonvacuous generalization bounds for deep (stochastic) neural networks with many more parameters than training data", "authors": ["Gintare Karolina Dziugaite", "Daniel M Roy"], "venue": "arXiv preprint arXiv:1703.11008,", "year": 2017}, {"title": "Topology and geometry of half-rectified network optimization", "authors": ["C Daniel Freeman", "Joan Bruna"], "venue": "arXiv preprint arXiv:1611.01540,", "year": 2016}, {"title": "Escaping from saddle points\u2212 online stochastic gradient for tensor decomposition", "authors": ["Rong Ge", "Furong Huang", "Chi Jin", "Yang Yuan"], "venue": "In Proceedings of The 28th Conference on Learning Theory,", "year": 2015}, {"title": "Learning onehidden-layer neural networks with landscape design", "authors": ["Rong Ge", "Jason D Lee", "Tengyu Ma"], "venue": "arXiv preprint arXiv:1711.00501,", "year": 2017}, {"title": "Reliably learning the ReLU in polynomial time", "authors": ["Surbhi Goel", "Varun Kanade", "Adam Klivans", "Justin Thaler"], "venue": "arXiv preprint arXiv:1611.10258,", "year": 2016}, {"title": "Sizeindependent sample complexity of neural networks", "authors": ["Noah Golowich", "Alexander Rakhlin", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1712.06541,", "year": 2017}, {"title": "Structured low-rank matrix factorization: Optimality, algorithm, and applications to image processing", "authors": ["Benjamin Haeffele", "Eric Young", "Rene Vidal"], "venue": "In International Conference on Machine Learning,", "year": 2014}, {"title": "Global optimality in tensor factorization, deep learning, and beyond", "authors": ["Benjamin D Haeffele", "Ren\u00e9 Vidal"], "venue": "arXiv preprint arXiv:1506.07540,", "year": 2015}, {"title": "Train faster, generalize better: Stability of stochastic gradient descent", "authors": ["Moritz Hardt", "Benjamin Recht", "Yoram Singer"], "venue": "arXiv preprint arXiv:1509.01240,", "year": 2015}, {"title": "Nearlytight vc-dimension bounds for piecewise linear neural networks", "authors": ["Nick Harvey", "Chris Liaw", "Abbas Mehrabian"], "venue": "arXiv preprint arXiv:1703.02930,", "year": 2017}, {"title": "Beating the perils of non-convexity: Guaranteed training of neural networks using tensor methods", "authors": ["Majid Janzamin", "Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1506.08473,", "year": 2015}, {"title": "How to escape saddle points efficiently", "authors": ["Chi Jin", "Rong Ge", "Praneeth Netrapalli", "Sham M. Kakade", "Michael I. Jordan"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Deep learning without poor local minima", "authors": ["Kenji Kawaguchi"], "venue": "In Advances In Neural Information Processing Systems,", "year": 2016}, {"title": "Empirical margin distributions and bounding the generalization error of combined classifiers", "authors": ["Vladimir Koltchinskii", "Dmitry Panchenko"], "venue": "Annals of Statistics,", "year": 2002}, {"title": "Imagenet classification with deep convolutional neural networks. In Advances in neural information processing", "authors": ["Alex Krizhevsky", "Ilya Sutskever", "Geoffrey E Hinton"], "year": 2012}, {"title": "Gradient descent only converges to minimizers", "authors": ["Jason D Lee", "Max Simchowitz", "Michael I Jordan", "Benjamin Recht"], "venue": "In Conference on Learning Theory,", "year": 2016}, {"title": "The power of normalization: Faster evasion of saddle points", "authors": ["Kfir Y Levy"], "venue": "arXiv preprint arXiv:1611.04831,", "year": 2016}, {"title": "Convergence analysis of twolayer neural networks with ReLU activation", "authors": ["Yuanzhi Li", "Yang Yuan"], "venue": "arXiv preprint arXiv:1705.09886,", "year": 2017}, {"title": "Algorithmic regularization in over-parameterized matrix recovery", "authors": ["Yuanzhi Li", "Tengyu Ma", "Hongyang Zhang"], "venue": "arXiv preprint arXiv:1712.09203,", "year": 2017}, {"title": "Fisher-rao metric, geometry, and complexity of neural networks", "authors": ["Tengyuan Liang", "Tomaso Poggio", "Alexander Rakhlin", "James Stokes"], "venue": "arXiv preprint arXiv:1711.01530,", "year": 2017}, {"title": "On the computational efficiency of training neural networks", "authors": ["Roi Livni", "Shai Shalev-Shwartz", "Ohad Shamir"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2014}, {"title": "Generalization bounds of sgld for non-convex learning: Two theoretical viewpoints", "authors": ["Wenlong Mou", "Liwei Wang", "Xiyu Zhai", "Kai Zheng"], "venue": "arXiv preprint arXiv:1707.05947,", "year": 2017}, {"title": "Norm-based capacity control in neural networks", "authors": ["Behnam Neyshabur", "Ryota Tomioka", "Nathan Srebro"], "venue": "In Conference on Learning Theory, pages 1376\u20131401,", "year": 2015}, {"title": "A pac-bayesian approach to spectrally-normalized margin bounds for neural networks", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nathan Srebro"], "venue": "arXiv preprint arXiv:1707.09564,", "year": 2017}, {"title": "Exploring generalization in deep learning", "authors": ["Behnam Neyshabur", "Srinadh Bhojanapalli", "David McAllester", "Nati Srebro"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "The loss surface of deep and wide neural networks", "authors": ["Quynh Nguyen", "Matthias Hein"], "venue": "arXiv preprint arXiv:1704.08045,", "year": 2017}, {"title": "The loss surface and expressivity of deep convolutional neural networks", "authors": ["Quynh Nguyen", "Matthias Hein"], "venue": "arXiv preprint arXiv:1710.10928,", "year": 2017}, {"title": "On the rank of extreme matrices in semidefinite programs and the multiplicity of optimal eigenvalues", "authors": ["G\u00e1bor Pataki"], "venue": "Mathematics of operations research,", "year": 1998}, {"title": "The geometry of semidefinite programming", "authors": ["G\u00e1bor Pataki"], "venue": "In Handbook of semidefinite programming,", "year": 2000}, {"title": "Generalization error bounds for noisy, iterative algorithms", "authors": ["Ankit Pensia", "Varun Jog", "Po-Ling Loh"], "venue": "arXiv preprint arXiv:1801.04295,", "year": 2018}, {"title": "Local minima and back propagation", "authors": ["Timothy Poston", "C-N Lee", "Y Choie", "Yonghoon Kwon"], "venue": "In Neural Networks,", "year": 1991}, {"title": "Spurious local minima are common in two-layer relu neural networks", "authors": ["Itay Safran", "Ohad Shamir"], "venue": "arXiv preprint arXiv:1712.08968,", "year": 2017}, {"title": "Provable methods for training neural networks with sparse connectivity", "authors": ["Hanie Sedghi", "Anima Anandkumar"], "venue": "arXiv preprint arXiv:1412.2693,", "year": 2014}, {"title": "Towards provable learning of polynomial neural networks using low-rank matrix estimation", "authors": ["Mohammadreza Soltani", "Chinmay Hegde"], "year": 2017}, {"title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "authors": ["Mahdi Soltanolkotabi", "Adel Javanmard", "Jason D Lee"], "venue": "arXiv preprint arXiv:1707.04926,", "year": 2017}, {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "authors": ["Daniel Soudry", "Elad Hoffer"], "venue": "arXiv preprint arXiv:1702.05777,", "year": 2017}, {"title": "Rank, trace-norm and max-norm", "authors": ["Nathan Srebro", "Adi Shraibman"], "venue": "In International Conference on Computational Learning Theory,", "year": 2005}, {"title": "Maximum-margin matrix factorization", "authors": ["Nathan Srebro", "Jason Rennie", "Tommi S Jaakkola"], "venue": "In Advances in neural information processing systems,", "year": 2005}, {"title": "Benefits of depth in neural networks", "authors": ["Matus Telgarsky"], "venue": "arXiv preprint arXiv:1602.04485,", "year": 2016}, {"title": "An analytical formula of population gradient for two-layered ReLU network and its applications in convergence and critical point analysis", "authors": ["Yuandong Tian"], "venue": "arXiv preprint arXiv:1703.00560,", "year": 2017}, {"title": "Energy propagation in deep convolutional neural networks", "authors": ["Thomas Wiatowski", "Philipp Grohs", "Helmut B\u00f6lcskei"], "venue": "IEEE Transactions on Information Theory,", "year": 2017}, {"title": "Towards understanding generalization of deep learning: Perspective of loss landscapes", "authors": ["Lei Wu", "Zhanxing Zhu"], "venue": "arXiv preprint arXiv:1706.10239,", "year": 2017}, {"title": "Understanding deep learning requires rethinking generalization", "authors": ["Chiyuan Zhang", "Samy Bengio", "Moritz Hardt", "Benjamin Recht", "Oriol Vinyals"], "venue": "arXiv preprint arXiv:1611.03530,", "year": 2016}, {"title": "Learning non-overlapping convolutional neural networks with multiple kernels", "authors": ["Kai Zhong", "Zhao Song", "Inderjit S Dhillon"], "venue": "arXiv preprint arXiv:1711.03440,", "year": 2017}, {"title": "Recovery guarantees for one-hiddenlayer neural networks", "authors": ["Kai Zhong", "Zhao Song", "Prateek Jain", "Peter L Bartlett", "Inderjit S Dhillon"], "venue": "arXiv preprint arXiv:1706.03175,", "year": 2017}, {"title": "The landscape of deep learning algorithms", "authors": ["Pan Zhou", "Jiashi Feng"], "venue": "arXiv preprint arXiv:1705.07038,", "year": 2017}], "id": "SP:9aef40ca3dd916fe68a81239a0e05bf01f755a39", "authors": [{"name": "Simon S. Du", "affiliations": []}, {"name": "Jason D. Lee", "affiliations": []}], "abstractText": "We provide new theoretical insights on why overparametrization is effective in learning neural networks. For a k hidden node shallow network with quadratic activation and n training data points, we show as long as k \u2265 \u221a 2n, over-parametrization enables local search algorithms to find a globally optimal solution for general smooth and convex loss functions. Further, despite that the number of parameters may exceed the sample size, using theory of Rademacher complexity, we show with weight decay, the solution also generalizes well if the data is sampled from a regular distribution such as Gaussian. To prove when k \u2265 \u221a 2n, the loss function has benign landscape properties, we adopt an idea from smoothed analysis, which may have other applications in studying loss surfaces of neural networks.", "title": "On the Power of Over-parametrization in Neural Networks with Quadratic Activation"}