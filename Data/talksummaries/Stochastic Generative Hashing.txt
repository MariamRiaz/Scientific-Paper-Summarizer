1	21	Formally, given a reference dataset X = {x i }N i=1 with x 2 X ⇢ Rd, we want to re- trieve similar items from X for a given query y according to some similarity measure sim(x, y).
2	36	When the negative Euclidean distance is used, i.e., sim(x, y) = kx yk2, this corresponds to L2 Nearest Neighbor Search (L2NNS) problem; when the inner product is used, i.e., sim(x, y) = x>y, it becomes a Maximum Inner Product Search (MIPS) problem.
3	5	In this work, we focus on L2NNS for simplicity, however our method handles MIPS problems as well, as shown in the supplementary material D. Brute-force linear search is expensive for large datasets.
4	80	To alleviate the time and storage bottlenecks, two research directions have been studied extensively: (1) partition the dataset so that only a subset of data points is searched; (2) represent the data as codes so that similarity computation can be carried out more efficiently.
7	19	In this work, we focus on speeding up search via binary hashing.
8	20	Hashing for similarity search was popularized by influential works such as Locality Sensitive Hashing (Indyk and Motwani, 1998; Gionis et al., 1999; Charikar , 2002).
9	188	The crux of binary hashing is to utilize a hash function, f(·) : X !
10	38	{0, 1}l, which maps the original samples in X 2 Rd to l-bit binary vectors h 2 {0, 1}l while preserving the similarity measure, e.g., Euclidean distance or inner product.
11	17	Search with such binary representations can be efficiently conducted using Hamming distance computation, which is supported via POPCNT on modern CPUs and GPUs.
12	1	Quantization based techniques (Babenko and Lempitsky, 2014; Jegou et al., 2011; Zhang et al., 2014b) have been shown to give stronger empirical results but tend to be less efficient than Hamming search over binary codes (Douze et al., 2015; He et al., 2013).
13	252	Data-dependent hash functions are well-known to perform better than randomized ones (Wang et al., 2014).
14	37	Learning hash functions or binary codes has been discussed in several papers, including spectral hashing (Weiss et al., 2009), semi-supervised hashing (Wang et al., 2010), iterative quantization (Gong and Lazebnik, 2011), and others (Liu et al., 2011; Gong et al., 2013; Yu et al., 2014; Shen et al., 2015; Guo et al., 2016).
15	263	The main idea behind these works is to optimize some objective function that captures the preferred properties of the hash function in a supervised or unsupervised fashion.
16	37	Even though these methods have shown promising performance in several applications, they suffer from two main drawbacks: (1) the objective functions are often heuristically constructed without a principled characterization of goodness of hash codes, and (2) when optimizing, the binary constraints are crudely handled through some relaxation, leading to inferior results (Liu et al., 2014).
17	27	In this work, we introduce Stochastic Generative Hashing (SGH) to address these two key issues.
18	56	We propose a generative model which captures both the encoding of binary codes h from input x and the decoding of input x from h. This provides a principled hash learning framework, where the hash function is learned by Minimum Description Length (MDL) principle.
20	74	Such a generative model also enables us to optimize distributions over discrete hash codes without the necessity to handle discrete variables.
