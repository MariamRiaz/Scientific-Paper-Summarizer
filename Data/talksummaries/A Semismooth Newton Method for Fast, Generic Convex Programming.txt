0	42	Conic optimization problems (or cone programs) are convex optimization problems of the form minimize x∈Rn cTx subject to b−Ax ∈ K, (1) where c ∈ Rn, A ∈ Rm×n, b ∈ Rm, K are problem data, specified by the user, and K is a proper cone (Nesterov & Nemirovskii, 1994; Ben-Tal & Nemirovski, 2001; Boyd & Vandenberghe, 2004); we give a formal treatment of proper cones in Section 2, but a simple example of a proper cone, for now, is the nonnegative orthant, i.e., the set of all points in Rm with nonnegative components.
1	70	These problems are quite general, encapsulating a number of standard problem classes: e.g., taking K as the nonnegative orthant yields a linear program; taking K as the positive semidefinite cone, i.e., the space of m×m positive semidefinite matrices Sm+ , yields a semidefinite program; and taking K as the secondorder (or Lorentz) cone {(x, y) ∈ Rm−1 × R : ‖x‖2 ≤ y} yields a second-order cone program (a quadratic program is a special case).
2	50	Due, in part, to their generality, cone programs have been the focus of much recent work, and additionally form the basis of many convex optimization modeling frameworks, e.g., sdpsol (Wu & Boyd, 2000), YALMIP (Lofberg, 2005), and the CVX family of frameworks (Grant, 2004; Diamond & Boyd, 2016; Udell et al., 2014).
3	95	These frameworks generally make it easy to quickly solve small and mediumsized convex optimization problems to high accuracy; they work by allowing the user to specify a generic convex optimization problem in a way that resembles its mathematical representation, then convert the problem into a form similar to (1), and finally solve the problem.
6	38	In recent work, O’Donoghue et al. (2016) use the alternating direction method of multipliers (ADMM) (Boyd et al., 2011) to solve generic cone programs; operator splitting methods (e.g., ADMM, Peaceman-Rachford splitting (Peaceman & Rachford, 1955), Douglas-Rachford splitting (Douglas & Rachford, 1956), and dual decomposition) generally converge to modest accuracy in just a few iterations, so the approach (called the splitting conic solver, or SCS) is scalable, and also has a number of other benefits, e.g., provding certificates of primal or dual infeasibility.
7	43	In this paper, we introduce a new method (called “NewtonADMM”) for solving large-scale, generic cone programs rapidly to high accuracy.
8	114	The basic idea is to view the usual ADMM recurrence relation as a fixed point iteration, and then use a truncated, nonsmooth Newton method to find a fixed point; to justify the approach, we extend the theory of semismooth operators, coming out of the applied mathematics literature over the last two decades (Mifflin, 1977; Qi & Sun, 1993; Martı́nez & Qi, 1995; Facchinei et al., 1996), although it has received little attention from the machine learning community (Ferris & Munson, 2004).
10	26	We show, under regularity conditions, that Newton-ADMM is quadratically convergent; empirically, Newton-ADMM is significantly faster than SCS, on a number of problems.
11	20	Also, Newton-ADMM has essentially no tuning parameters, and generates certificates of infeasibility, helpful in diagnosing problem misspecification.
12	29	The rest of the paper is organized as follows.
20	42	Finally, we give an overview of semismoothness (Mifflin, 1977), a generalization of smoothness, central to our Newton method.
21	55	We say that a set C is a cone if, for all x ∈ C, and θ ≥ 0, we get that θx ∈ C. The dual cone C∗, associated with the cone C, is defined as the set {y : yTx ≥ 0, ∀x ∈ C}.
22	29	Additionally, a cone C is a convex cone if, for all x, y ∈ C, and θ1, θ2 ≥ 0, we get that θ1x + θ2y ∈ C. A cone C is a proper cone if it is (i) convex; (ii) closed; (iii) solid, i.e., its interior is nonempty; and (iv) pointed, i.e., if both x,−x ∈ C, then we get that x = 0.
24	63	The exponential cone (see, e.g., Serrano (2015)),Kexp, is a three-dimensional proper cone, defined as the closure of the epigraph of the perspective of exp(x), with x ∈ R: Kexp = {(x, y, z) : x ∈ R, y > 0, z ≥ y exp(x/y)} ∪ {(x, 0, z) : x ≤ 0, z ≥ 0} .
25	23	Cone programs resembling (1) were first described by Nesterov & Nemirovskii (1994, page 67), although special cases were, of course, considered earlier.
76	35	The preceding results lay the groundwork for us to use a semismooth Newton method (Qi & Sun, 1993), applied to F , where we replace the usual Jacobian with any element of the generalized Jacobian (14); however, as many have observed (Khan & Barton, 2017), it is not always straightforward to compute an element of the generalized Jacobian.
79	22	Using the lemma, an element J ∈ R3k×3k of the generalized Jacobian of the map F ∈ R3k is then just J =  I +Q −I −IJu I −I 0  , (17) where Ju =  −I 0 0 I 0 0 I 0 00 −JPK∗ 0 0 I 0 0 JPK∗ 0 0 0 −` 0 0 1 0 0 `  (18) is a (k×3k)-dimensional matrix forming the second row of J ; ` equals 1 if ũτ − vκ ≥ 0 and 0 otherwise; and JPK∗ ∈ Rm×m is the Jacobian of the projection onto the dual cone K∗.
81	26	Later, we discuss computing JPK∗ , the Jacobian of the projection onto the dual cone K∗, for various cones K; these pieces let us compute an element J , given in (17) – (18), of the generalized Jacobian of the map F , defined in (16), which we use instead of the usual Jacobian, in a semismooth Newton method; below, we describe a way to scale the method to larger problems (i.e., values of n).
95	59	Since the nonnegative orthant is self-dual, we can simply find a subgradient of each component in (9), to get that JPK∗ is diagonal with, say, (JPK∗ )ii set to 1 if (ũy−vs)i ≥ 0 and 0 otherwise, for i = 1, .
96	43	Write z = (z1, z2), z1 ∈ Rm−1, z2 ∈ R. The second-order cone is self-dual, as well, so we can find subgradients of (10), to get that JPK∗ =  0, ‖z1‖2 ≤ −z2 I, ‖z1‖2 ≤ z2 D, otherwise, (21) where D is a low-rank matrix (details in the supplement).
104	14	Here, we give some convergence results for NewtonADMM, the method presented in Algorithm 1.
156	49	,m. In the left panel of Figure 2, we compare a specialized Newton-ADMM applied directly to the lasso problem (25), with the ADMM algorithm for (26) – (28), a proximal gradient method (Beck & Teboulle, 2009), and a heavilyoptimized implementation of coordinate descent (Friedman et al., 2007); we set p = 400, N = 200, λ = 10, ρ = 1.
157	57	Here, the specialized Newton-ADMM is quite competitive with these strong baselines; the specialized NewtonADMM outperforms Newton-ADMM applied to the cone program (2), so we omit the latter from the comparison.
159	22	In the right panel of Figure 2, we present a similar comparison, for sparse inverse covariance estimation, with the QUIC method of Hsieh et al. (2014); Newton-ADMM clearly performs best (p = N = 1, 000, λ = ρ = 1, details in the supplement).
160	40	We introduced Newton-ADMM, a new method for generic convex programming.
161	59	The basic idea is use a nonsmooth Newton method to find a fixed point of the residuals of the consecutive ADMM iterates generated by SCS, a state-ofthe-art solver for cone programs; we showed that the basic idea is fairly general, and can be applied to accelerate (virtually) any ADMM-based algorithm.
162	20	We presented theoretical and empirical support that Newton-ADMM converges rapidly (i.e., quadratically) to a solution, outperforming SCS across several problems.
163	40	AA was supported by the DoE Computational Science Graduate Fellowship DE-FG0297ER25308.
164	63	EW was supported by DARPA, under award number FA8750-17-2-0027.
165	145	We thank Po-Wei Wang and the referees for a careful proof-reading.
