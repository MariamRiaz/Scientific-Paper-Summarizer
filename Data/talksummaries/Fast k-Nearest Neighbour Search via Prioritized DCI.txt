0	29	The method of k-nearest neighbours is a fundamental building block of many machine learning algorithms and also has broad applications beyond artificial intelligence, including in statistics, bioinformatics and database systems, e.g. (Biau et al., 2011; Behnam et al., 2013; Eldawy & Mokbel, 2015).
1	28	Consequently, since the problem of nearest neighbour search was first posed by Minsky & Papert (1969), it has for decades intrigued the artificial intelligence and theoretical computer science communities alike.
2	56	Unfortunately, the myriad efforts at devising efficient algorithms have encountered a recurring obstacle: the curse of dimensionality, which describes the phenomenon of query time complexity depending exponentially on dimensionality.
3	42	As a result, even on datasets with moderately high dimensionality, practitioners often have resort to naı̈ve exhaustive search.
5	17	The more familiar notion, ambient dimensionality, refers to the dimensionality of the space data points are embedded in.
9	18	Most existing methods suffer from some form of curse of dimensionality.
11	60	Later methods (Krauthgamer & Lee, 2004; Beygelzimer et al., 2006; Dasgupta & Freund, 2008) overcame the exponential dependence on ambient dimensionality, but have not been able to escape from an exponential dependence on intrinsic dimensionality.
13	34	Recently, Li & Malik (2016) proposed an approach known as Dynamic Continuous Indexing (DCI) that successfully reduces the dependence on intrinsic dimensionality from exponential to sublinear, thereby making high-dimensional nearest neighbour search more practical.
14	24	The key observation is that the difficulties encountered by many existing methods, including k-d trees and Locality-Sensitive Hashing (LSH) (Indyk & Motwani, 1998), may arise from their reliance on space partitioning, which is a popular divideand-conquer strategy.
15	27	It works by partitioning the vector space into discrete cells and maintaining a data structure 2 c, where c is the expansion rate introduced in (Karger & Ruhl, 2002).
16	30	that keeps track of the points lying in each cell.
17	59	At query time, these methods simply look up of the contents of the cell containing the query and possibly adjacent cells and perform brute-force search over points lying in these cells.
20	26	First, because the volume of space grows exponentially in dimensionality, either the number or the volumes of cells must grow exponentially.
21	20	Second, the discretization of the space essentially limits the “field of view” of the algorithm, as it is unaware of points that lie in adjacent cells.
22	56	This is especially problematic when the query lies near a cell boundary, as there could be points in adjacent cells that are much closer to the query.
24	11	Fourth, when the dataset exhibits varying density across space, choosing a good partitioning is non-trivial.
33	27	Specifically, we show a remarkable result: a linear increase in intrinsic dimensionality, which could mean an exponential increase in the number of points near a query, can be mostly counteracted with a corresponding linear increase in the number of indices.
37	78	In particular, compared to LSH, it achieves a 14- to 116-fold reduction in the number of distance evaluations and a 21-fold reduction in the memory usage.
73	25	DCI constructs a data structure consisting of multiple composite indices of data points, each of which in turn consists of a number of simple indices.
74	36	Each simple index orders data points according to their projections along a particular random direction.
76	23	The true distances from the query to every candidate point are evaluated and the ones that are among the k clos- est to the query are returned.
77	79	More concretely, each simple index is associated with a random direction and stores the projections of every data point along the direction.
78	64	They are implemented using standard data structures that maintain one-dimensional ordered sequences of elements, like self-balancing binary search trees (Bayer, 1972; Guibas & Sedgewick, 1978) or skip lists (Pugh, 1990).
79	17	At query time, the algorithm projects the query along the projection directions associated with each simple index and finds the position where the query would have been inserted in each simple index, which takes logarithmic time.
80	54	It then iterates over, or visits, data points in each simple index in the order of their distances to the query under projection, which takes constant time for each iteration.
93	20	The priority of a simple index is set to the negative absolute difference between the query projection and the next data point projection in the index.
94	21	Algorithm 2 k-nearest neighbour querying procedure Require: Query point q in Rd, binary search trees/skip lists and their associated projection vectors {(Tjl, ujl)}j2[m],l2[L], the number of points to retrieve k 0 and the number of points to visit k 1 in each composite index function QUERY(q, {(Tjl, ujl)}j,l, k0, k1) Cl array of size n with entries initialized to 0 8l 2 [L] qjl hq, ujli 8j 2 [m], l 2 [L] Sl ; 8l 2 [L] Pl empty priority queue 8l 2 [L] for l = 1 to L do for j = 1 to m do (p(1)jl , h (1) jl ) the node in Tjl whose key is the closest to qjl Insert (p(1)jl , h (1) jl ) with priority |p (1) jl qjl| into Pl end for end for for i0 = 1 to k 1 1 do for l = 1 to L do if |Sl| < k0 then (p(i)jl , h (i) jl ) the node with the highest priority in Pl Remove (p(i)jl , h (i) jl ) from Pl and insert the node in Tjl whose key is the next closest to qjl, which is denoted as (p(i+1)jl , h (i+1) jl ), with priority |p(i+1)jl qjl| into Pl Cl[h (i) jl ] Cl[h (i) jl ] + 1 if Cl[h (i) jl ] = m then Sl Sl [ {h(i)jl } end if end if end for end for return k points in S l2[L] Sl that are the closest in Euclidean distance in Rd to q end function Intuitively, this ensures data points are visited in the order of their distances to the query under projection.
95	36	Because data points are only retrieved from a composite index when they have been visited in all constituent simple indices, data points are retrieved in the order of the maximum of their distances to the query along multiple projection directions.
96	20	Since distance under projection forms a lower bound on the true distance, the maximum projected distance approaches the true distance as the number of projection directions increases.
102	24	Notably, the first term of the query complexity, which dominates when the ambient dimensionality d is large, has a more favourable dependence on the intrinsic dimensionality d0 than the query complexity of standard DCI.
