0	43	Theories of language origin identify the combination of language and nonverbal behaviors (vision and acoustic modality) as the prime form of communication utilized by humans throughout evolution (Müller, 1866).
1	61	In natural language processing, this form of language is regarded as human multimodal language.
2	23	Modeling multimodal language has recently become a centric research direction in both NLP and multimodal machine learning (Hazarika et al., 2018; Zadeh et al., 2018a; Poria et al., 2017a; Baltrušaitis et al., 2017; Chen et al., 2017).
3	32	Studies strive to model the dual dynamics of multimodal language: intra-modal dynamics (dynamics within each modality) and cross-modal dynamics (dynamics across different modalities).
4	35	However, from a resource perspective, previous multimodal language datasets have severe shortcomings in the following aspects: Diversity in the training samples: The diversity in training samples is crucial for comprehensive multimodal language studies due to the complexity of the underlying distribution.
5	28	This complexity is rooted in variability of intra-modal and crossmodal dynamics for language, vision and acoustic modalities (Rajagopalan et al., 2016).
6	25	Previously proposed datasets for multimodal language are generally small in size due to difficulties associated with data acquisition and costs of annotations.
8	40	Models trained on only few topics generalize poorly as language and nonverbal behaviors tend to change based on the impression of the topic on speakers’ internal mental state.
10	21	Training models on only few speakers can lead to degenerate solutions where models learn the identity of speakers as opposed to a generalizable model of multimodal language (Wang et al., 2016).
11	31	Variety in annotations Having multiple labels to predict allows for studying the relations between labels.
66	24	Each video inherently contains three modalities: language in the form of spoken text, visual via perceived gestures and facial expressions, and acoustic through intonations and prosody.
91	22	Ekman emotions (Ekman et al., 1980) of {happiness, sadness, anger, fear, disgust, surprise} are annotated on a [0,3] Likert scale for presence of emotion x: [0: no evidence of x, 1: weakly x, 2: x, 3: highly x].
124	70	Once the model is trained end to end, we analyze the efficacies in the DFG to study the fusion mechanism learned for modalities in multimodal language.
132	26	For example, {l} ⊂ {l, v} which results in a connection between < language > and < language, vision >.
151	69	We replace the Delta-memory Attention Network with DFG and refer to the modified model as Graph Memory Fusion Network (Graph-MFN).
154	31	cl, cv, and ca represent the memory of LSTMs for language, vision and acoustic modalities respectively.
157	68	The DFG models cross-modal interactions and encodes the cross-modal representations in its output vertex Tt for storage in the Multi-view Gated Memory ut.
168	55	Graph-MFN shows superior performance in sentiment analysis and competitive performance in emotion recognition.
169	31	Therefore, DFG is both an effective and interpretable model for multimodal fusion.
171	22	Multimodal Fusion has a Volatile Nature: The first observation is that the structure of the DFG is changing case by case and for each case over time.
173	29	For example, in case (I) where all modalities are informative, all efficacies seem to be high, imply- ing that the DFG is able to find useful information in unimodal, bimodal and trimodal interactions.
177	153	For example the model always seems to prioritize fusion between language and audio in (l → l, a), and (a → l, a).
180	30	In the presence of informative visual information, the model increases the efficacies of (v → τ) although the values of other visual efficacies also increase.
186	41	However, unlike language, the acoustic modality also appears to fuse with the visual modality if both modalities are meaningful, such as in case (I).
189	31	Both of these cases show unchanging behaviors which we believe DFG has learned as natural priors of human communicative signal.
190	51	With these observations, we believe that DFG has successfully learned how to manage its internal structure to model human communication.
191	31	In this paper we presented the largest dataset of multimodal sentiment analysis and emotion recognition called CMU Multimodal Opinion Sentiment and Emotion Intensity (CMU-MOSEI).
192	88	CMUMOSEI consists of 23,453 annotated sentences from more than 1000 online speakers and 250 different topics.
193	43	The dataset expands the horizons of Human Multimodal Language studies in NLP.
194	170	One such study was presented in this paper where we analyzed the structure of multimodal fusion in sentiment analysis and emotion recognition.
