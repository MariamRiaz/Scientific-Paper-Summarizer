0,1,label2,summary_sentences
"Decades of research have been dedicated to heuristics for speeding up inference in natural language processing tasks, such as constituency parsing (Pauls and Klein, 2009; Caraballo and Charniak, 1998) and machine translation (Petrov et al., 2008; Xu et al., 2013).",1 Introduction,[0.9533204928641658],"['We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013), word embeddings tailored to representing sentences (Kenter et al., 2016), and skip-thoughts sentence embeddings (Kiros et al., 2015).']"
"Such research is necessary because of a trend toward richer models, which improve accuracy at the cost of slower inference.",1 Introduction,[0],[0]
"For example, state-of-theart constituency parsers use grammars with millions of rules, while dependency parsers routinely use millions of features.",1 Introduction,[0],[0]
"Without heuristics, these parsers take minutes to process a single sentence.
",1 Introduction,[0],[0]
"To speed up inference, we will learn a pruning policy.",1 Introduction,[0],[0]
"During inference, the pruning policy is invoked to decide whether to keep or prune various parts of the search space, based on features of the input and (potentially) the state of the inference process.
",1 Introduction,[0.9524850489108434],"['Salton et al., on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.']"
"Our approach searches for a policy with maximum end-to-end performance (reward) on training data, where the reward is a linear combination of problemspecific measures of accuracy and runtime, namely reward = accuracy−λ · runtime.",1 Introduction,[0],[0]
"The parameter λ ≥ 0
specifies the relative importance of runtime and accuracy.",1 Introduction,[0],[0]
"By adjusting λ, we obtain policies with different speed-accuracy tradeoffs.
",1 Introduction,[0],[0]
"For learning, we use Locally Optimal Learning to Search (LOLS) (Chang et al., 2015b), an algorithm for learning sequential decision-making policies, which accounts for the end-to-end performance of the entire decision sequence jointly.",1 Introduction,[0],[0]
"Unfortunately, executing LOLS naively in our setting is prohibitive because it would run inference from scratch millions of times under different policies, training examples, and variations of the decision sequence.",1 Introduction,[0],[0]
"Thus, this paper presents efficient algorithms for repeated inference, which are applicable to a wide variety of NLP tasks, including parsing, machine translation and sequence tagging.",1 Introduction,[0],[0]
"These algorithms, based on change propagation and dynamic programming, dramatically reduce time spent evaluating similar decision sequences by leveraging problem structure and sharing work among evaluations.
",1 Introduction,[0],[0]
We evaluate our approach by learning pruning heuristics for constituency parsing.,1 Introduction,[0],[0]
"In this setting, our approach is the first to account for end-to-end performance of the pruning policy, without making independence assumptions about the reward function, as in prior work (Bodenstab et al., 2011).",1 Introduction,[0],[0]
"In the larger context of learning-to-search for structured prediction, our work is unusual in that it learns to control a dynamic programming algorithm (i.e., graphbased parsing) rather than a greedy algorithm (e.g., transition-based parsing).",1 Introduction,[0],[0]
Our experiments show that accounting for end-to-end performance in training leads to better policies along the entire Pareto frontier of accuracy and runtime.,1 Introduction,[0],[0]
"A simple yet effective approach to speeding up parsing was proposed by Bodenstab et al. (2011), who trained a pruning policy π to classify whether or not spans of the input sentence w1 · · ·wn form plausible
263
Transactions of the Association for Computational Linguistics, vol. 5, pp.",2 Weighted CKY with pruning,[0],[0]
"263–278, 2017.",2 Weighted CKY with pruning,[0],[0]
Action Editor: Marco Kuhlmann.,2 Weighted CKY with pruning,[0],[0]
"Submission batch: 5/2016; Revision batch: 9/2016; Published 8/2017.
",2 Weighted CKY with pruning,[0],[0]
c©2017 Association for Computational Linguistics.,2 Weighted CKY with pruning,[0],[0]
"Distributed under a CC-BY 4.0 license.
constituents based on features of the input sentence.",2 Weighted CKY with pruning,[0],[0]
"These predictions enable a parsing algorithm, such as CKY, to skip expensive steps during its execution: unlikely constituents are pruned.",2 Weighted CKY with pruning,[0],[0]
"Only plausible constituents are kept, and the parser assembles the highest-scoring parse from the available constituents.
",2 Weighted CKY with pruning,[0],[0]
Alg. 1 provides pseudocode for weighted CKY with pruning.,2 Weighted CKY with pruning,[0],[0]
"Weighted CKY aims to find the highestscoring derivation (parse tree) of a given sentence, where a given grammar specifies a non-negative score for each derivation rule and a derivation’s score is the product of the scores of the rules it uses.1 CKY uses a dynamic programming strategy to fill in a three-dimensional array β, known as the chart.",2 Weighted CKY with pruning,[0],[0]
The score βikx is the score of the highest-scoring subderivation with fringe wi+1 . . .,2 Weighted CKY with pruning,[0],[0]
wk and root,2 Weighted CKY with pruning,[0],[0]
x.,2 Weighted CKY with pruning,[0],[0]
This value is computed by looping over the possible ways to assemble such a subderivation from smaller subderivations with scores βijy and βjkz (lines 17–22).,2 Weighted CKY with pruning,[0],[0]
"Additionally, we track a witness (backpointer) for each βikx, so that we can easily reconstruct the corresponding subderivation at line 23.",2 Weighted CKY with pruning,[0],[0]
"The chart is initialized with lexical grammar rules (lines 3–9), which derive words from grammar symbols.
",2 Weighted CKY with pruning,[0],[0]
"The key difference between pruned and unpruned CKY is an additional “if” statement (line 14), which queries the pruning policy π to decide whether to compute the several values βikx associated with a span (i, k).",2 Weighted CKY with pruning,[0],[0]
Note that width-1 and width-n spans are always kept because all valid parses require them.,2 Weighted CKY with pruning,[0],[0]
Bodenstab et al. (2011) train their pruning policy as a supervised classifier of spans.,3 End-to-end training,[0],[0]
"They derive direct supervision as follows: try to keep a span if it appears in the gold-standard parse, and prune it otherwise.",3 End-to-end training,[0],[0]
They found that using an asymmetric weighting scheme helped find the right balance between false positives and false negatives.,3 End-to-end training,[0],[0]
"Intuitively, failing to prune is only a slight slowdown, whereas pruning a good item can ruin the accuracy of the parse.
",3 End-to-end training,[0],[0]
"1As is common practice, we assume the grammar has been binarized.",3 End-to-end training,[0],[0]
"We focus on pre-trained grammars, leaving coadaptation of the grammar and pruning policy to future work.",3 End-to-end training,[0],[0]
"As indicated at lines 6 and 19, a rule’s score may be made to depend on the context in which that rule is applied (Finkel et al., 2008), although the pre-trained grammars in our present experiments are ordinary PCFGs for which this is not the case.
",3 End-to-end training,[0],[0]
"Algorithm 1 PARSE: Weighted CKY with pruning 1: Input: grammar G, sentence w, policy π
Output: completed chart β, derivation d 2: .",3 End-to-end training,[0],[0]
"Initialize chart 3: β := 0 4: for k := 1 to n : 5: for x such that (x→ wk) ∈ rules(G) : 6: s := G(x→ wk | w, k) 7: if s > βk−1,k,x : 8: βk−1,k,x := s 9: witness(k−1, k, x) := (k−1, k, wk)
10: for width := 2 to n : 11: for i := 0 to n− width : 12: k := i+ width .",3 End-to-end training,[0],[0]
"Current span is (i, k) 13: .",3 End-to-end training,[0],[0]
"Policy determines whether to fill in this span 14: if π(w, i, k) = prune : 15: continue 16: .",3 End-to-end training,[0],[0]
Fill in span by considering each split point j 17: for j := i+ 1 to k,3 End-to-end training,[0],[0]
"− 1 : 18: for (x→ y z) ∈ rules(G) : 19: s := βijy ·βjkz ·G(x→ y z | w, i, j, k) 20: if s > βikx : 21: βikx := s 22: witness(i, k, x) := (j, y, z) 23: d̂ := follow backpointers from (0, n,ROOT) 24: return (β, d̂)
",3 End-to-end training,[0],[0]
"Our end-to-end training approach improves upon asymmetric weighting by jointly evaluating the sequence of pruning decisions, measuring its effect on the test-time evaluation metric by actually running pruned CKY (Alg. 1).",3 End-to-end training,[0],[0]
"To estimate the value of a pruning policy π, we call PARSE(G,w(i), π) on each training sentence w(i), and apply the reward function, r = accuracy−λ · runtime.",3 End-to-end training,[0],[0]
"The empirical value of a policy is its average reward on the training set:
R(π) = 1 m
m∑
i=1
E",3 End-to-end training,[0],[0]
"[ r(PARSE(G,w(i), π)) ]",3 End-to-end training,[0],[0]
"(1)
The expectation in the definition may be dropped if PARSE, π, and r are all deterministic, as in our setting.2 Our definition of r depends on the user parameter λ ≥ 0, which specifies the amount of accuracy the user would sacrifice to save one unit of
2Parsers may break ties randomly or use Monte Carlo methods.",3 End-to-end training,[0],[0]
"The reward function r can be nondeterministic when it involves wallclock time or human judgments.
runtime.",3 End-to-end training,[0],[0]
"Training under a range of values for λ gives rise to policies covering a number of operating points along the Pareto frontier of accuracy and runtime.
",3 End-to-end training,[0],[0]
End-to-end training gives us a principled way to decide what to prune.,3 End-to-end training,[0],[0]
"Rather than artificially labeling each pruning decision as inherently good or bad, we evaluate its effect in the context of the particular sentence and the other pruning decisions.",3 End-to-end training,[0],[0]
"Actions that prune a gold constituent are not equally bad—some cause cascading errors, while others are “worked around” in the sense that the grammar still selects a mostly-gold parse.",3 End-to-end training,[0],[0]
"Similarly, actions that prune a non-gold constituent are not equally good—some provide more overall speedup (e.g., pruning narrow constituents prevents wider ones from being built), and some even improve accuracy by suppressing an incorrect but high-scoring parse.
",3 End-to-end training,[0],[0]
"More generally, the gold vs. non-gold distinction is not even available in NLP tasks where one is pruning potential elements of a latent structure, such as an alignment (Xu et al., 2013) or a finer-grained parse (Matsuzaki et al., 2005).",3 End-to-end training,[0],[0]
"Yet our approach can still be used in such settings, by evaluating the reward on the downstream task that the latent structure serves.
",3 End-to-end training,[0],[0]
Past work on optimizing end-to-end performance is discussed in §8.,3 End-to-end training,[0],[0]
"One might try to scale these techniques to learning to prune, but in this work we take a different approach.",3 End-to-end training,[0],[0]
"Given a policy, we can easily find small ways to improve it on specific sentences by varying individual pruning actions (e.g., if π currently prunes a span then try keeping it instead).",3 End-to-end training,[0],[0]
"Given a batch of improved action sequences (trajectories), the remaining step is to search for a policy which produces the improved trajectories.",3 End-to-end training,[0],[0]
"Conveniently, this can be reduced to a classification problem, much like the asymmetric weighting approach, except that the supervised labels and misclassification costs are not fixed across iterations, but rather are derived from interaction with the environment (i.e., PARSE and the reward function).",3 End-to-end training,[0.9575921138256341],"['Comparing the word2vec model with and without the canonical form feature, we see that, when this feature is used, there is a relatively larger increase in precision and recall (and F1 score) for the literal class, than for the idiomatic class.']"
"This idea is formalized as a learning algorithm called Locally Optimal Learning to Search (Chang et al., 2015b), described in §4.
",3 End-to-end training,[0],[0]
The counterfactual interventions we require— evaluating how reward would change if we changed one action—can be computed more efficiently using our novel algorithms (§5) than by the default strategy of running the parser repeatedly from scratch.,3 End-to-end training,[0],[0]
"The key is to reuse work among evaluations, which is
possible because LOLS only makes tiny changes.",3 End-to-end training,[0],[0]
Pruned inference is a sequential decision process.,4 Learning algorithm,[0],[0]
The process begins in an initial state s0.,4 Learning algorithm,[0],[0]
"In pruned CKY, s0 specifies the state of Alg.",4 Learning algorithm,[0],[0]
"1 at line 10, after the chart has been initialized from some selected sentence.",4 Learning algorithm,[0],[0]
"Next, the policy is invoked to choose action a0 = π(s0)—in",4 Learning algorithm,[0],[0]
our case at line 14—which affects what the parser does next.,4 Learning algorithm,[0],[0]
"Eventually the parser reaches some state s1 from which it calls the policy to choose action a1 = π(s1), and so on.",4 Learning algorithm,[0],[0]
"When the policy is invoked at state st, it selects action at based on features extracted from the current state st—a snapshot of the input sentence, grammar and parse chart at time t.3",4 Learning algorithm,[0],[0]
"We call the state-action sequence s0 a0 s1 a1 · · · sT a trajectory, where T is the trajectory length.",4 Learning algorithm,[0],[0]
"At the final state, the reward function is evaluated, r(sT ).
",4 Learning algorithm,[0],[0]
The LOLS algorithm for learning a policy is given in Alg.,4 Learning algorithm,[0],[0]
"2,4 with a graphical illustration in Fig. 1.",4 Learning algorithm,[0],[0]
"At a high level, LOLS alternates between evaluating and improving the current policy πi.
",4 Learning algorithm,[0],[0]
"The evaluation phase first samples a trajectory from πi, called a roll-in: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi).",4 Learning algorithm,[0],[0]
"In our setting, s0 is derived from a randomly sampled training sentence, but the rest of the trajectory is then deterministically computed by πi given s0.",4 Learning algorithm,[0],[0]
"Then we revisit each state s in the roll-in (line 7), and try each available action ā∈A(s)",4 Learning algorithm,[0],[0]
"(line 9), executing πi thereafter—a rollout—to measure the resulting reward r̂[ā] (line 10).",4 Learning algorithm,[0],[0]
"Our parser is deterministic, so a single rollout is an unbiased, 0-variance estimate of the expected reward.",4 Learning algorithm,[0],[0]
"This process is repeated many times, yielding a large list Q̂i of pairs 〈s, r̂〉, where s is a state that was encountered in some roll-in and r̂ maps the possible actions A(s) in that state to their measured rewards.
",4 Learning algorithm,[0],[0]
"The improvement phase now trains a new policy πi+1 to try to choose high-reward actions, seeking a policy that will “on average” get high rewards r[πi+1(s)].",4 Learning algorithm,[0],[0]
"Good generalization is important: the policy must select high-reward actions even in states s that are not represented in Q̂i, in case they are
3Our experiments do not make use of the current state of the chart.",4 Learning algorithm,[0],[0]
"We discuss this decision in §8.
4Alg.",4 Learning algorithm,[0],[0]
"2 is simpler than in Chang et al. (2015b) because it omits oracle rollouts, which we do not use in our experiments.
",4 Learning algorithm,[0],[0]
Algorithm 2 LOLS algorithm for learning to prune.,4 Learning algorithm,[0],[0]
1: π1 := INITIALIZEPOLICY(. . . ),4 Learning algorithm,[0],[0]
2: for i := 1 to number of iterations : 3: .,4 Learning algorithm,[0],[0]
Evaluate: Collect dataset for πi 4: Q̂i := ∅ 5: for j := 1 to minibatch size : 6: s0 a0 s1 a1 · · · sT ∼ ROLL-IN(πi) .,4 Learning algorithm,[0],[0]
Sample 7: for t := 0 to T−1 : 8: .,4 Learning algorithm,[0],[0]
Intervene: Evaluate each action at st 9: for āt ∈ A(st) : .,4 Learning algorithm,[0],[0]
"Possible actions
10: r̂t[āt] ∼ ROLLOUT(πi, st, āt) 11: Q̂i.append(〈st, r̂t 〉) 12: .",4 Learning algorithm,[0],[0]
"Improve: Train with dataset aggregation
13: πi+1 ← TRAIN",4 Learning algorithm,[0],[0]
(,4 Learning algorithm,[0],[0]
"⋃i k=1 Q̂k )
14: .",4 Learning algorithm,[0],[0]
Finalize: Pick the best policy over all iterations 15: return argmaxi′ R(πi′) encountered when running the new policy πi+1 (or when parsing test sentences).,4 Learning algorithm,[0],[0]
"Thus, beyond just regularizing the training objective, we apply dataset aggregation (Ross et al., 2011): we take the training set to include not just Q̂i but also the examples from previous iterations (line 13).",4 Learning algorithm,[0],[0]
"This also ensures that the sequence of policies π1, π2, . .",4 Learning algorithm,[0],[0]
".will be “stable” (Ross and Bagnell, 2011) and will eventually converge.
",4 Learning algorithm,[0],[0]
"So line 13 seeks to find a good classifier πi+1 using a training set: a possible classifier π would receive from each training example 〈s, r̂〉 a reward of r̂[π(s)].",4 Learning algorithm,[0],[0]
"In our case, where A(s) = {keep, prune}, this cost-sensitive classification problem is equivalent to training an ordinary binary classifier, after converting each training example 〈s, r̂〉 to 〈s, argmaxa",4 Learning algorithm,[0],[0]
"r̂[a]〉 and giving this example a weight of |r̂t,keep− r̂t,prune|.",4 Learning algorithm,[0],[0]
"Our specific classifier is described in §6.
",4 Learning algorithm,[0],[0]
"In summary, the evaluation phase of LOLS collects training data for a cost-sensitive classifier, where the
inputs (states), outputs (actions), and costs are obtained by interacting with the environment.",4 Learning algorithm,[0],[0]
"LOLS concocts a training set and repeatedly revises it, similar to the well-known Expectation-Maximization algorithm.",4 Learning algorithm,[0],[0]
This enables end-to-end training of systems with discrete decisions and nondecomposable reward functions.,4 Learning algorithm,[0],[0]
LOLS gives us a principled framework for deriving (nonstationary) “supervision” even in tricky cases such as latent-variable inference (mentioned in §3).,4 Learning algorithm,[0],[0]
"LOLS has strong theoretical guarantees, though in pathological cases, it may take exponential time to converge (Chang et al., 2015b).
",4 Learning algorithm,[0],[0]
"The inner loop of the evaluation phase performs roll-ins, interventions and rollouts.",4 Learning algorithm,[0],[0]
Roll-ins ensure that the policy is (eventually) trained under the distribution of states it tends to encounter at test time.,4 Learning algorithm,[0],[0]
Interventions and rollouts force πi to explore the effect of currently disfavored actions.,4 Learning algorithm,[0],[0]
"Unlike most applications of LOLS and related algorithms, such as SEARN (Daumé III, 2006) and DAGGER (Ross et al., 2011), executing the policy is a major bottleneck in training.",5 Efficient rollouts,[0],[0]
"Because our dynamic programming parser explores many possibilities (unlike a greedy, transition-based decoder) its trajectories are quite long.",5 Efficient rollouts,[0],[0]
"This not only slows down each rollout: it means we must do more rollouts.
",5 Efficient rollouts,[0],[0]
"In our case, the trajectory has length T = n·(n+1)
2",5 Efficient rollouts,[0],[0]
− 1− n,5 Efficient rollouts,[0],[0]
"for a sentence of length n, where T is also the number of pruning decisions: one for each span other than the root and width-1 spans.",5 Efficient rollouts,[0],[0]
LOLS must then perform T rollouts on this example.,5 Efficient rollouts,[0],[0]
"This means that to evaluate policy πi, we must parse each sentence in the minibatch hundreds of times (e.g., 189 for n=20, 434 for n=30, and 779 for n=40).
",5 Efficient rollouts,[0],[0]
"We can regard each policy π as defining a pruning
mask m, an array that maps each of the T spans (i, k) to a decision mik (1 = keep, 0 = prune).",5 Efficient rollouts,[0],[0]
"Each rollout tries flipping a different bit in this mask.
",5 Efficient rollouts,[0],[0]
We could spend less time on each sentence by sampling only some of its T rollouts (see §6).,5 Efficient rollouts,[0],[0]
"Regardless, the rollouts we do on a given sentence are related: in this section we show how to get further speedups by sharing work among them.",5 Efficient rollouts,[0],[0]
"In §5.2, we leverage the fact that rollouts will be similar to one another (differing by a single pruning decision).",5 Efficient rollouts,[0],[0]
"In §5.3, we show that the reward of all T rollouts can be computed simultaneously by dynamic programming under some assumptions about the structure of the reward function (described later).",5 Efficient rollouts,[0],[0]
We found these algorithms to be crucial to training in a “reasonable” amount of time (see the empirical comparison in §7.2).,5 Efficient rollouts,[0],[0]
"It is convenient to present our efficient rollout algorithms in terms of the hypergraph structure of Alg. 1 (Klein and Manning, 2001; Huang, 2008; Li and Eisner, 2009; Eisner and Blatz, 2007).",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph describes the information flow among related quantities in a dynamic programming algorithm.,5.1 Background: Parsing as hypergraphs,[0],[0]
"Many computational tricks apply generically to hypergraphs.
",5.1 Background: Parsing as hypergraphs,[0],[0]
A hypergraph edge e (or hyperedge) is a “generalized arrow” e.head ≺ e.Tail with one output and a list of inputs.,5.1 Background: Parsing as hypergraphs,[0],[0]
"We regard each quantity βikx,mik, or G(. . .)",5.1 Background: Parsing as hypergraphs,[0],[0]
"in Alg. 1 as the value of a corresponding hypergraph vertex β̇ikx, ṁik, or Ġ(. . .).",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus, value(v̇) = v for any vertex v̇. Each ṁik’s value is computed by the policy π or chosen by a rollout intervention.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Each Ġ’s value is given by the grammar.
",5.1 Background: Parsing as hypergraphs,[0],[0]
"Values of β̇ikx, by contrast, are computed at line 19 if k − i > 1.",5.1 Background: Parsing as hypergraphs,[0],[0]
"To record the dependence of βikx on other quantities, our hypergraph includes the hyperedge β̇ikx ≺",5.1 Background: Parsing as hypergraphs,[0],[0]
"(β̇ijy, β̇jkz, ṁik, ġ) for each 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
i < j < k ≤ n,5.1 Background: Parsing as hypergraphs,[0],[0]
"and (x→ y z) ∈ rules(G), where ġ denotes the vertex Ġ(x→ y z | w, i, j, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"If k − i = 1, then values of βikx are instead computed at line 6, which does not access any other β values or the pruning mask.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Thus our hypergraph includes the hyperedge vikx ≺(ġ) whenever i = k−1, 0 ≤",5.1 Background: Parsing as hypergraphs,[0],[0]
"i < k ≤ n, and (x→ wk) ∈ rules(G), with ġ = Ġ(x→ wk | w, k).
",5.1 Background: Parsing as hypergraphs,[0],[0]
"With this setup, the value βikx is the maximum score of any derivation of vertex β̇ikx (a tree rooted at β̇ikx, representing a subderivation), where the score
of a derivation is the product of its leaf values.",5.1 Background: Parsing as hypergraphs,[0],[0]
Alg. 1 computes it by considering hyperedges β̇ikx ≺ T and the previously computed values of the vertices in the tail T .,5.1 Background: Parsing as hypergraphs,[0],[0]
"For a vertex v̇, we write In(v̇) and Out(v̇) for its sets of incoming and outgoing hyperedges.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Our algorithms follow these hyperedges implicitly, without the overhead of materializing or storing them.",5.1 Background: Parsing as hypergraphs,[0],[0]
"Change propagation is an efficient method for incrementally re-evaluating a computation under a change to its inputs (Acar and Ley-Wild, 2008; Filardo and Eisner, 2012).",5.2 Change propagation (CP),[0],[0]
"In our setting, each roll-in at Alg. 2 line 6 evaluates the reward r(PARSE(G, xi, π)) from (1), which involves computing an entire parse chart via Alg. 1.",5.2 Change propagation (CP),[0],[0]
"The inner loop at line 10 performs T interventions per roll-in, which ask how reward would have changed if one bit in the pruning maskm had been different.",5.2 Change propagation (CP),[0],[0]
"Rather than reparsing from scratch (T times) to determine this, we can simply adjust the initial roll-in computation (T times).
",5.2 Change propagation (CP),[0],[0]
CP is efficient when only a small fraction of the computation needs to be adjusted.,5.2 Change propagation (CP),[0],[0]
"In principle, flipping a single pruning bit can change up to 50% of the chart, so one might expect the bookkeeping overhead of CP to outweigh the gains.",5.2 Change propagation (CP),[0],[0]
"In practice, however, 90% of the interventions change < 10% of the β values in the chart.",5.2 Change propagation (CP),[0],[0]
"The reason is that βikx is a maximum over many quantities, only one of which “wins.”",5.2 Change propagation (CP),[0],[0]
"Changing a given βijy rarely affects this maximum, and so changes are unlikely to propagate from vertex β̇ijy to β̇ikx.",5.2 Change propagation (CP),[0],[0]
"Since changes are not very contagious, the “epidemic of changes” does not spread far.
",5.2 Change propagation (CP),[0],[0]
Alg. 3 provides pseudocode for updating the highest-scoring derivation found by Alg. 1.,5.2 Change propagation (CP),[0],[0]
"We remark that the RECOMPUTE is called only when we flip a bit from keep to prune, which removes hyperedges and potentially decreases vertex values.",5.2 Change propagation (CP),[0],[0]
"The reverse flip only adds hyperedges, which increases vertex values via a running max (lines 12–14).
",5.2 Change propagation (CP),[0],[0]
"After determining the effect of flipping a bit, we must restore the original chart before trying a different bit (the next rollout).",5.2 Change propagation (CP),[0],[0]
The simplest approach is to call Alg. 3 again to flip the bit,5.2 Change propagation (CP),[0],[0]
"back.5
5Our implementation uses a slightly faster method which accumulates an “undo list” of changes that it makes to the chart to quickly revert the modified chart to the original roll-in state.
",5.2 Change propagation (CP),[0],[0]
Algorithm 3 Change propagation algorithm 1: Global: Alg.,5.2 Change propagation (CP),[0],[0]
"1’s vertex values/witnesses (roll-in) 2: procedure CHANGE(v̇, v) 3: .",5.2 Change propagation (CP),[0],[0]
Change the value of a leaf vertex v̇ to v 4: value(v̇) := v ; witness(v̇) = LEAF 5: Q := ∅; Q.push(v̇) .,5.2 Change propagation (CP),[0],[0]
Work queue (“agenda”) 6: while Q 6= ∅ : .,5.2 Change propagation (CP),[0],[0]
Propagate until convergence 7: u̇,5.2 Change propagation (CP),[0],[0]
:= Q.pop() .,5.2 Change propagation (CP),[0],[0]
Narrower constituents first 8: if witness(u̇) = NULL : .,5.2 Change propagation (CP),[0],[0]
Value is unknown 9: RECOMPUTE(u̇) .,5.2 Change propagation (CP),[0],[0]
"Get value & witness
10: for e ∈ Out(u̇) : .",5.2 Change propagation (CP),[0],[0]
Propagate new value of u̇ 11: ṡ := e.head; s := ∏ u̇′∈e.,5.2 Change propagation (CP),[0],[0]
"Tail value(u̇
′) 12: if s > value(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Increase value 13: value(ṡ) := s; witness(ṡ) := e 14: Q.push(ṡ) 15: else if witness(ṡ) = e and s < value(ṡ): 16: witness(ṡ) := NULL .Value,5.2 Change propagation (CP),[0],[0]
may decrease 17: Q.push(ṡ) .,5.2 Change propagation (CP),[0],[0]
"so, recompute upon pop 18: procedure RECOMPUTE(ṡ) 19: for e ∈ In(ṡ) : .",5.2 Change propagation (CP),[0],[0]
Max over incoming hyperedges 20: s := ∏ u̇∈e.,5.2 Change propagation (CP),[0],[0]
Tail value(u̇) 21: if s > value(ṡ) : 22: value(ṡ) = s; witness(ṡ) =,5.2 Change propagation (CP),[0],[0]
e,5.2 Change propagation (CP),[0],[0]
The naive rollout algorithm runs the parser T times— once for each variation of the pruning mask.,5.3 Dynamic programming (DP),[0],[0]
"The reader may be reminded of the finite difference approximation to the gradient of a function, which also measures the effects from perturbing each input value individually.",5.3 Dynamic programming (DP),[0],[0]
"In fact, for certain reward functions, the naive algorithm can be precisely regarded as computing a gradient—and thus we can use a more efficient algorithm, back-propagation, which finds the entire gradient vector of reward as fast (in the big-O sense) as computing the reward once.",5.3 Dynamic programming (DP),[0],[0]
"The overall algorithm is O(|E| + T ) where |E| is the total number of hyperedges, whereas the naive algorithm is O(|E′|·T ) where |E′| ≤ |E| is the maximum number of hyperedges actually visited on any rollout.
",5.3 Dynamic programming (DP),[0],[0]
What accuracy measure must we use?,5.3 Dynamic programming (DP),[0],[0]
Let r(d) denote the recall of a derivation d—the fraction of gold constituents that appear as vertices in the derivation.,5.3 Dynamic programming (DP),[0],[0]
"A simple accuracy metric would be 1-best recall, the recall r(d̂) of the highest-scoring derivation d̂ that was not pruned.",5.3 Dynamic programming (DP),[0],[0]
"In this section, we relax that to ex-
pected recall,6 r̄= ∑
d p(d)r(d).",5.3 Dynamic programming (DP),[0],[0]
"Here we interpret the pruned hypergraph’s values as an unnormalized probability distribution over derivations, where the probability p(d) =",5.3 Dynamic programming (DP),[0],[0]
p̃(d)/Z of a derivation is proportional to its score p̃(d) =,5.3 Dynamic programming (DP),[0],[0]
"∏ u̇∈leaves(d) value(u̇).
",5.3 Dynamic programming (DP),[0],[0]
"Though r̄ is not quite our evaluation metric, it captures more information about the parse forest, and so may offer some regularizing effect when used in a training criterion (see §7.1).",5.3 Dynamic programming (DP),[0],[0]
"In any case, r̄ is close to r(d̂) when probability mass is concentrated on a few derivations, which is common with heavy pruning.
",5.3 Dynamic programming (DP),[0],[0]
"We can re-express r̄ as r̃/Z, where
r̃ = ∑
d
p̃(d)r(d) Z = ∑
d
p̃(d) (2)
These can be efficiently computed by dynamic programming (DP), specifically by a variant of the inside algorithm (Li and Eisner, 2009).",5.3 Dynamic programming (DP),[0],[0]
"Since p̃(d) is a product of rule weights and pruning mask bits at d’s leaves (§5.1), each appearing at most once, both r̃ and Z vary linearly in any one of these inputs provided that all other inputs are held constant.",5.3 Dynamic programming (DP),[0],[0]
"Thus, the exact effect on r̃ or Z of changing an input mik can be found from the partial derivatives with respect to it.",5.3 Dynamic programming (DP),[0],[0]
"In particular, if we increased mik by ∆ ∈ {−1, 1} (to flip this bit), the new value of r̄ would be exactly
r̃ + ∆ · ∂r̃/∂mik",5.3 Dynamic programming (DP),[0],[0]
"Z + ∆ · ∂Z/∂mik
(3)
",5.3 Dynamic programming (DP),[0],[0]
It remains to compute these partial derivatives.,5.3 Dynamic programming (DP),[0],[0]
"All partials can be jointly computed by back-propagation, which equivalent to another dynamic program known as the outside algorithm (Eisner, 2016).
",5.3 Dynamic programming (DP),[0],[0]
"The inside algorithm only needs to visit the |E′| unpruned edges, but the outside algorithm must also visit some pruned edges, to determine the effect of “unpruning” them (changing their mik input from 0 to 1) by finding ∂r̃/∂mik and ∂Z/∂mik.",5.3 Dynamic programming (DP),[0],[0]
"On the other hand, these partials are 0 when some other input to the hyperedge is 0.",5.3 Dynamic programming (DP),[0],[0]
"This case is common when the hypergraph is heavily pruned (|E′| |E|), and means that back-propagation need not descend further through that hyperedge.
",5.3 Dynamic programming (DP),[0],[0]
"6In theory, we could anneal from expected to 1-best recall (Smith and Eisner, 2006).",5.3 Dynamic programming (DP),[0],[0]
"We experimented extensively with annealing but found it to be too numerically unstable for our purposes, even with high-precision arithmetic libraries.
",5.3 Dynamic programming (DP),[0],[0]
Note that the DP method computes only the accuracies of rollouts—not the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"In this paper, we will combine DP with a very simple runtime measure that is trivial to roll out (see §7).",5.3 Dynamic programming (DP),[0],[0]
An alternative would be to use CP to roll out the runtimes.,5.3 Dynamic programming (DP),[0],[0]
"This is very efficient: to measure just runtime, CP only needs to update the record of which constituents or edges are built, and not their scores, so the changes are easier to compute than in §5.2, and peter out more quickly.
6 Parser details7
Setup: We use the standard English parsing setup: the Penn Treebank (Marcus et al., 1993) with the standard train/dev/test split, and standard tree normalization.8 For efficiency during training, we restrict the length of sentences to ≤ 40.",5.3 Dynamic programming (DP),[0],[0]
We do not restrict the length of test sentences.,5.3 Dynamic programming (DP),[0],[0]
"We experiment with two grammars: coarse, the “no frills” left-binarized treebank grammar, and fine, a variant of the Berkeley split-merge level-6 grammar (Petrov et al., 2006) as provided by Dunlop (2014, ch. 5).",5.3 Dynamic programming (DP),[0],[0]
The parsing algorithms used during training are described in §5.,5.3 Dynamic programming (DP),[0],[0]
"Our test-time parsing algorithm uses the left-child loop implementation of CKY (Dunlop et al., 2010).",5.3 Dynamic programming (DP),[0],[0]
All algorithms allow unary rules (though not chains).,5.3 Dynamic programming (DP),[0],[0]
"We evaluate accuracy at test time with the F1 score from the official EVALB script (Sekine and Collins, 1997).
",5.3 Dynamic programming (DP),[0],[0]
Training:,5.3 Dynamic programming (DP),[0],[0]
Note that we never retrain the grammar weights—we train only the pruning policy.,5.3 Dynamic programming (DP),[0],[0]
"To TRAIN our classifiers (Alg. 2 line 13), we use L2-regularized logistic regression, trained with L-BFGS optimization.",5.3 Dynamic programming (DP),[0],[0]
"We always rescale the example weights in the training set to sum to 1 (otherwise as LOLS proceeds, dataset aggregation overwhelms the regularizer).",5.3 Dynamic programming (DP),[0],[0]
"For the baseline (defined in next section), we determine the regularization coefficient by sweeping {2−11, 2−12, 2−13, 2−14, 2−15} and picking the best value (2−13) based on the dev frontier.",5.3 Dynamic programming (DP),[0],[0]
We re-used this regularization parameter for LOLS.,5.3 Dynamic programming (DP),[0],[0]
"The number of LOLS iterations is determined by a 6-day training-time limit9 (meaning some jobs run many
7Code for experiments is available at http://github.",5.3 Dynamic programming (DP),[0],[0]
"com/timvieira/learning-to-prune.
",5.3 Dynamic programming (DP),[0],[0]
8Data train/dev/test split (by section) 2–21 / 22 / 23.,5.3 Dynamic programming (DP),[0],[0]
"Normalization operations: Remove function tags, traces, spurious unary edges (X → X), and empty subtrees left by other operations.",5.3 Dynamic programming (DP),[0],[0]
"Relabel ADVP and PRT|ADVP tags to PRT.
",5.3 Dynamic programming (DP),[0],[0]
"9On the 7th day, LOLS rested and performance was good.
",5.3 Dynamic programming (DP),[0],[0]
fewer iterations than others).,5.3 Dynamic programming (DP),[0],[0]
For LOLS minibatch size we use 10K on the coarse grammar and 5K on the fine grammar.,5.3 Dynamic programming (DP),[0],[0]
"At line 15 of Alg. 2, we return the policy that maximized reward on development data, using the reward function from training.
",5.3 Dynamic programming (DP),[0],[0]
"Features: We use similar features to Bodenstab et al. (2011), but we have removed features that depend on part-of-speech tags.",5.3 Dynamic programming (DP),[0],[0]
"We use the following 16 feature templates for span (i, k) with 1 < k−i < N : bias, sentence length, boundary words, conjunctions of boundary words, conjunctions of word shapes, span shape, width bucket.",5.3 Dynamic programming (DP),[0],[0]
"Shape features map a word or phrase into a string of character classes (uppercase, lowercase, numeric, spaces); we truncate substrings of identical classes to length two; punctuation chars are never modified in any way.",5.3 Dynamic programming (DP),[0],[0]
"Width buckets use the following partition: 2, 3, 4, 5, [6, 10], [11, 20], [21,∞).",5.3 Dynamic programming (DP),[0],[0]
"We use feature hashing (Weinberger et al., 2009) with MurmurHash3 (Appleby, 2008) and project to 222 features.",5.3 Dynamic programming (DP),[0],[0]
"Conjunctions are taken at positions (i−1, i), (k, k+1), (i−1, k+1) and (i, k).",5.3 Dynamic programming (DP),[0],[0]
"We use special begin and end symbols when a template accesses positions beyond the sentence boundary.
",5.3 Dynamic programming (DP),[0],[0]
Hall et al. (2014) give examples motivating our feature templates and show experimentally that they are effective in multiple languages.,5.3 Dynamic programming (DP),[0],[0]
Boundary words are strong surface cues for phrase boundaries.,5.3 Dynamic programming (DP),[0],[0]
Span shape features are also useful as they (minimally) check for matched parentheses and quotation marks.,5.3 Dynamic programming (DP),[0],[0]
Reward functions and surrogates: Each user has a personal reward function.,7 Experimental design and results,[0],[0]
"In this paper, we choose to specify our true reward as accuracy − λ · runtime, where accuracy is given by labeled F1 percentage and runtime by mega-pushes (mpush), millions of calls per sentence to lines 6 and 19 of Alg. 1, which is in practice proportional to seconds per sentence (correlation > 0.95) and is more replicable.",7 Experimental design and results,[0],[0]
We evaluate accordingly (on test data)—but during LOLS training we approximate these metrics.,7 Experimental design and results,[0],[0]
"We compare:
• rCP (fast): Use change propagation (§5.2) to compute accuracy on a sentence as F1 of just that sentence, and to approximate runtime as ||β||0,
the number of constituents that were built.10
• rDP (faster): Use dynamic programming (§5.3) to approximate accuracy on a sentence as expected recall.11 This time we approximate runtime more crudely as ||m||0, the number of nonzeros in the pruning mask for the sentence (i.e., the number of spans whose constituents the policy would be willing to keep if they were built).
",7 Experimental design and results,[0],[0]
We use these surrogates because they admit efficient rollout algorithms.,7 Experimental design and results,[0],[0]
"Less important, they preserve the training objective (1) as an average over sentences.",7 Experimental design and results,[0],[0]
"(Our true F1 metric on a corpus cannot be computed in this way, though it could reasonably be estimated by averaging over minibatches of sentences in (1).)
",7 Experimental design and results,[0],[0]
"Controlled experimental design: Our baseline system is an adaptation of Bodenstab et al. (2011) to learning-to-prune, as described in §3 and §6.",7 Experimental design and results,[0],[0]
Our goal is to determine whether such systems can be improved by LOLS training.,7 Experimental design and results,[0],[0]
"We repeat the following design for both reward surrogates (rCP and rDP) and for both grammars (coarse and fine).
",7 Experimental design and results,[0],[0]
¬ We start by training a number of baseline models by sweeping the asymmetric weighting parameter.,7 Experimental design and results,[0],[0]
"For the coarse grammar we train 8 such models, and for the fine grammar 12.
 ",7 Experimental design and results,[0],[0]
"For each baseline policy, we estimate a value of λ for which that policy is optimal (among baseline policies) according to surrogate reward.12
10When using rCP, we speed up LOLS by doing≤ 2n rollouts per sentence of length n. We sample these uniformly without replacement from the T possible rollouts (§5), and compensate by upweighting the resulting training examples by T/(2n).
",7 Experimental design and results,[0],[0]
"11Considering all nodes in the binarized tree, except for the root, width-1 constituents, and children of unary rules.
",7 Experimental design and results,[0],[0]
"12We estimate λ by first fitting a parametric model yi = h(xi) , ymax · sigmoid(a · log(xi + c) + b) to the baseline runtime-accuracy measurements on dev data (shown in green in Fig. 2) by minimizing mean squared error.",7 Experimental design and results,[0],[0]
"We then use the fitted curve’s slope h′ to estimate each λi = h′(xi), where xi is the runtime of baseline i. The resulting choice of reward function y−λi",7 Experimental design and results,[0],[0]
"·x increases along the green arrow in Fig. 2, and is indeed maximized (subject to y ≤ h(x), and in the region where h is concave) at x = xi.",7 Experimental design and results,[0],[0]
"As a sanity check, notice since λi is a derivative of the function y = h(x), its units are in units of y (accuracy) per unit of x (runtime), as appropriate for use in the expression",7 Experimental design and results,[0],[0]
y,7 Experimental design and results,[0],[0]
− λi · x.,7 Experimental design and results,[0],[0]
"Indeed, this procedure will construct the same reward function regardless of the units we use to express x.",7 Experimental design and results,[0],[0]
"Our specific parametric model h is a sigmoidal curve, with
® For each baseline policy, we run LOLS with the same surrogate reward function (defined by λ) for which that baseline policy was optimal.",7 Experimental design and results,[0],[0]
We initialize LOLS by setting π0 to the baseline policy.,7 Experimental design and results,[0],[0]
"Furthermore, we include the baseline policy’s weighted training set Q̂0 in the ⋃ at line 13.
",7 Experimental design and results,[0],[0]
"Fig. 2 shows that LOLS learns to improve on the baseline, as evaluated on development data.
¯",7 Experimental design and results,[0],[0]
But do these surrogate reward improvements also improve our true reward?,7 Experimental design and results,[0],[0]
"For each baseline policy, we use dev data to estimate a value of λ for which that policy is optimal according to our true reward function.",7 Experimental design and results,[0],[0]
"We use blind test data to compare the baseline policy to its corresponding LOLS policy on this true reward function, testing significance with a paired permutation test.",7 Experimental design and results,[0],[0]
"The improvements hold up, as shown in Fig. 3.
",7 Experimental design and results,[0],[0]
"The rationale behind this design is that a user who actually wishes to maximize accuracy−λ·runtime, for some specific λ, could reasonably start by choosing the best baseline policy for this reward function, and then try to improve that baseline by running LOLS with the same reward function.",7 Experimental design and results,[0],[0]
"Our experiments show this procedure works for a range of λ values.
",7 Experimental design and results,[0],[0]
"In the real world, a user’s true objective might instead be some nonlinear function of runtime and accuracy.",7 Experimental design and results,[0],[0]
"For example, when accuracy is “good enough,” it may be more important to improve runtime, and vice-versa.",7 Experimental design and results,[0],[0]
LOLS could be used with such a nonlinear reward function as well.,7 Experimental design and results,[0],[0]
"In fact, a user does not even have to quantify their global preferences by writing down such a function.",7 Experimental design and results,[0],[0]
"Rather, they could select manually among the baseline policies, choosing one with an attractive speed-accuracy tradeoff, and then specify λ to indicate a local direction of desired improvement (like the green arrows in Fig. 2), modifying this direction periodically as LOLS runs.",7 Experimental design and results,[0],[0]
"As previous work has shown, learning to prune gives us excellent parsers with less than < 2% overhead
accuracy → ymax asymptotically as runtime → ∞. It obtains an excellent fit by placing accuracy and runtime on the loglogit scale—that is, log(xi + c) and logit(yi/ymax) transforms are used to convert our bounded random variables xi and yi to unbounded ones—and then assuming they are linearly related.
for deciding what to prune (i.e., pruning feature extraction and span classification).",7.1 Discussion,[0],[0]
"Even the baseline pruner has access to features unavailable to the grammar, and so it learns to override the grammar, improving an unpruned coarse parser’s accuracy from 61.1 to as high as 70.1% F1 on test data (i.e., beneficial search error).",7.1 Discussion,[0],[0]
"It is also 8.1x faster!13 LOLS simply does a better job at figuring out where to prune, raising accuracy 2.1 points to 72.2 (while maintaining a 7.4x speedup).",7.1 Discussion,[0],[0]
"Where pruning is more aggressive,
13We measure runtime as best of 10 runs (recommended by Dunlop (2014)).",7.1 Discussion,[0],[0]
"All parser timing experiments were performed on a Linux laptop with the following specs: Intel® Core™ i5-2540M 2.60GHz CPU, 8GB memory, 32K/256K/3072K L1/L2/L3 cache.",7.1 Discussion,[0],[0]
"Code is written in the Cython language.
",7.1 Discussion,[0],[0]
"LOLS has even more impact on accuracy.
",7.1 Discussion,[0],[0]
"Even on the fine grammar, where there is less room to improve accuracy, the most accurate LOLS system improves an unpruned parser by +0.16% F1 with a 8.6x speedup.",7.1 Discussion,[0],[0]
"For comparison, the most accurate baseline drops −0.03% F1 with a 9.7x speedup.
",7.1 Discussion,[0],[0]
"With the fine grammar, we do not see much improvement over the baseline in the accuracy > 85 regions.",7.1 Discussion,[0],[0]
This is because the supervision specified by asymmetric weighting is similar to what LOLS surmises via rollouts.,7.1 Discussion,[0],[0]
"However, in lower-accuracy regions we see that LOLS can significantly improve reward over its baseline policy.",7.1 Discussion,[0],[0]
"This is because the baseline supervision does not teach which plausible
constituents are “safest” to prune, nor can it learn strategies such as “skip all long sentences.”",7.1 Discussion,[0],[0]
"We discuss why LOLS does not help as much in the high accuracy regions further in §7.3.
",7.1 Discussion,[0],[0]
"In a few cases in Fig. 2, LOLS finds no policy that improves surrogate reward on dev data.",7.1 Discussion,[0],[0]
"In these cases, surrogate reward does improve slightly on training data (not shown), but early stopping just keeps the initial (baseline) policy since it is just as good on dev data.",7.1 Discussion,[0],[0]
"Adding a bit of additional random exploration might help break out of this initialization.
",7.1 Discussion,[0],[0]
"Interestingly, the rDP LOLS policies find higheraccuracy policies than the corresponding rCP policies, despite a greater mismatch in surrogate accuracy definitions.",7.1 Discussion,[0],[0]
"We suspect that rDP’s approach of trying to improve expected accuracy may provide a useful regularizing effect, which smooths out the reward signal and provides a useful bias (§5.3).
",7.1 Discussion,[0],[0]
"The most pronounced qualitative difference due to LOLS training is substantially lower rates of parse failure in the mid- to high- λ-range on both grammars
(not shown).",7.1 Discussion,[0],[0]
"Since LOLS does end-to-end training, it can advise the learner that a certain pruning decision catastrophically results in no parse being found.",7.1 Discussion,[0],[0]
Part of the contribution of this paper is faster algorithms for performing LOLS rollouts during training (§5).,7.2 Training speed and convergence,[0],[0]
"Compared to the naive strategy of running the parser from scratch T times, rCP achieves speedups of 4.9–6.6x on the coarse grammar and 1.9–2.4x on the fine grammar.",7.2 Training speed and convergence,[0],[0]
"rDP is even faster, 10.4–11.9x on coarse and 10.5–13.8x on fine.",7.2 Training speed and convergence,[0],[0]
"Most of the speedup comes from longer sentences, which take up most of the runtime for all methods.",7.2 Training speed and convergence,[0],[0]
Our new algorithms enable us to train on fairly long sentences (≤ 40).,7.2 Training speed and convergence,[0],[0]
"We note that our implementations of rCP and rDP are not as highly optimized as our test-time parser, so there may be room for improvement.
",7.2 Training speed and convergence,[0],[0]
Orthogonal to the cost per rollout is the number of training iterations.,7.2 Training speed and convergence,[0],[0]
"LOLS may take many steps to converge if trajectories are long (i.e., T is large)
because each iteration of LOLS training attempts to improve the current policy by a single action.",7.2 Training speed and convergence,[0],[0]
"In our setting, T is quite large (discussed extensively in §5), but we are able to circumvent slow convergence by initializing the policy (via the baseline method).",7.2 Training speed and convergence,[0],[0]
This means that LOLS can focus on fine-tuning a policy which is already quite good.,7.2 Training speed and convergence,[0],[0]
"In fact, in 4 cases, LOLS did not improve from its initial policy.
",7.2 Training speed and convergence,[0],[0]
We find that when λ is large—the cases where we get meaningful improvements because the initial policy is far from locally optimal—LOLS steadily and smoothly improves the surrogate reward on both training and development data.,7.2 Training speed and convergence,[0],[0]
"Because these are fast parsers, LOLS was able to run on the order of 10 (fine grammar) or 100 (coarse grammar) epochs within our 6-day limit; usually it was still improving when we terminated it.",7.2 Training speed and convergence,[0],[0]
"By contrast, for the slower and more accurate small-λ parsers (which completed fewer training epochs), LOLS still improves surrogate reward on training data, but without systematically improving on development data—often the reward on development fluctuates, and early stopping simply picks the best of this small set of “random” variants.",7.2 Training speed and convergence,[0],[0]
"In §3, we argued that LOLS gives a more appropriate training signal for pruning than the baseline method of consulting the gold parse, because it uses rollouts to measure the full effect of each pruning decision in the context of the other decisions made by the policy.
",7.3 Understanding the LOLS training signal,[0],[0]
"To better understand the results of our previous experiments, we analyze how often a rollout does determine that the baseline supervision for a span is suboptimal, and how suboptimal it is in those cases.
",7.3 Understanding the LOLS training signal,[0],[0]
We specifically consider LOLS rollouts that evaluate the rCP surrogate (because rDP is a cruder approximation to true reward).,7.3 Understanding the LOLS training signal,[0],[0]
"These rollouts Q̂i tell us what actions LOLS is trying to improve in its current policy πi for a given λ, although there is no guarantee that the learner in §4 will succeed at classifying Q̂i correctly (due to limited features, regularization, and the effects of dataset aggregation).
",7.3 Understanding the LOLS training signal,[0],[0]
We define regret of the baseline oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"Let best(s) , argmaxaROLLOUT(π, s, a) and regret(s) , (ROLLOUT(π, s, best(s) − ROLLOUT(π, s, gold(s)))).",7.3 Understanding the LOLS training signal,[0],[0]
"Note that regret(s)≥0 for all s, and let diff(s) be the event that regret(s) > 0 strictly.",7.3 Understanding the LOLS training signal,[0],[0]
"We are interested in analyzing the expected regret over all gold and
non-gold spans, which we break down as
E[regret] = p(diff) (4) · ( p(gold | diff) · E[regret | gold, diff] + p(¬ gold | diff) · E[regret | ¬ gold, diff] )
where expectations are taken over s ∼ ROLL-IN(π).",7.3 Understanding the LOLS training signal,[0],[0]
"Empirical analysis of regret: To show where the benefit of the LOLS oracle comes from, Fig. 4 graphs the various quantities that enter into the definition (4) of baseline regret, for different π, λ, and grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"The LOLS oracle evolves along with the policy π, since it identifies the best action given π.",7.3 Understanding the LOLS training signal,[0],[0]
"We thus evaluate the oracle baseline against two LOLS oracles: the one used at the start of LOLS training (derived from the initial policy π1 that was trained on baseline supervision), and the one obtained at the end (derived from the LOLS-trained policy π∗ selected by early stopping).",7.3 Understanding the LOLS training signal,[0],[0]
"These comparisons are shown by solid and dashed lines respectively.
",7.3 Understanding the LOLS training signal,[0],[0]
"Class imbalance (black curves): In all graphs, the aggregate curves primarily reflect the non-gold spans, since only 8% of spans are gold.
",7.3 Understanding the LOLS training signal,[0],[0]
"Gold spans (gold curves): The top graphs show that a substantial fraction of the gold spans should be pruned (whereas the baseline tries to keep them all), although the middle row shows that the benefit of pruning them is small.",7.3 Understanding the LOLS training signal,[0],[0]
"In most of these cases, pruning a gold span improves speed but leaves accuracy unchanged—because that gold span was missed anyway by the highest-scoring parse.",7.3 Understanding the LOLS training signal,[0],[0]
Such cases become both more frequent and more beneficial as λ increases and we prune more heavily.,7.3 Understanding the LOLS training signal,[0],[0]
"In a minority of cases, however, pruning a gold span also improves accuracy (through beneficial search error).
",7.3 Understanding the LOLS training signal,[0],[0]
"Non-gold spans (purple curves): Conversely, the top graphs show that a few non-gold spans should be kept (whereas the baseline tries to prune them all), and the middle row shows a large benefit from keeping them.",7.3 Understanding the LOLS training signal,[0],[0]
"They are needed to recover from catastrophic errors and get a mostly-correct parse.
",7.3 Understanding the LOLS training signal,[0],[0]
Coarse vs. fine (left vs. right):,7.3 Understanding the LOLS training signal,[0],[0]
"The two grammars differ mainly for small λ, and this difference comes especially from the top row.",7.3 Understanding the LOLS training signal,[0],[0]
"With a fine grammar and small λ, the baseline parses are more accurate, so LOLS has less room for improvement: fewer
gold spans go unused, and fewer non-gold spans are needed for recovery.
",7.3 Understanding the LOLS training signal,[0],[0]
"Effect of λ: Aggressive pruning (large λ) reduces accuracy, so its effect on the top row is similar to that of using a coarse grammar.",7.3 Understanding the LOLS training signal,[0],[0]
"Aggressive pruning also has an effect on the middle row: there is more benefit to be derived from pruning unused gold spans (surprisingly), and especially from keeping those non-gold spans that are helpful (presumably they enable recovery from more severe parse errors).",7.3 Understanding the LOLS training signal,[0],[0]
"These effects are considerably sharper with rDP reward (not shown here), which more smoothly evaluates the entire weighted pruned parse forest rather than trying to coordinate actions to ensure a good single 1-best tree; the baseline oracle is excellent at choosing the action that gets the better forest when the forest is mostly present (small λ) but not when it is mostly pruned (large λ).
",7.3 Understanding the LOLS training signal,[0],[0]
Effect on retraining the policy: The black lines in the bottom graphs show the overall regret (on training data) if we were to perfectly follow the baseline oracle rather than the LOLS oracle.,7.3 Understanding the LOLS training signal,[0],[0]
"In practice, retraining the policy to match the oracle will not match it perfectly in either case.",7.3 Understanding the LOLS training signal,[0],[0]
"Thus the baseline method has a further disadvantage: when it trains a policy, its training objective weights all gold or all non-gold examples equally, whereas LOLS invests greater effort in matching the oracle on those states where doing so would give greater downstream reward.",7.3 Understanding the LOLS training signal,[0],[0]
Our experiments have focused on using LOLS to improve a reasonable baseline.,8 Related work,[0],[0]
Fig. 5 shows that our resulting parser fits reasonably among state-of-the-art constituency parsers trained and tested on the Penn Treebank.,8 Related work,[0],[0]
These parsers include a variety of techniques that improve speed or accuracy.,8 Related work,[0],[0]
"Many are quite orthogonal to our work here—e.g., the SpMV method (which is necessary for Bodenstab’s parser to beat ours) is a set of cache-efficient optimizations (Dunlop, 2014) that could be added to our parser (just as it was added to Bodenstab’s), while Hall et al. (2014) and Fernández-González and Martins (2015) replace the grammar with faster scoring models that have more conditional independence.",8 Related work,[0],[0]
"Overall, other fast parsers could also be trained using LOLS, so that
they quickly find parses that are accurate, or at least helpful to the accuracy of some downstream task.
",8 Related work,[0],[0]
"Pruning methods14 can use classifiers not only to select spans but also to prune at other granularities (Roark and Hollingshead, 2008; Bodenstab et al., 2011).",8 Related work,[0],[0]
"Prioritization methods do not prune substructures, but instead delay their processing until they are needed—if ever (Caraballo and Charniak, 1998).
",8 Related work,[0],[0]
This paper focuses on learning pruning heuristics that have trainable parameters.,8 Related work,[0],[0]
"In the same way, Stoyanov and Eisner (2012) learn to turn off unneeded factors in a graphical model, and Jiang et al. (2012) and Berant and Liang (2015) train prioritization heuristics (using policy gradient).",8 Related work,[0],[0]
"In both of those 2012 papers, we explicitly sought to maximize accuracy − λ · runtime as we do here.",8 Related work,[0],[0]
"Some previous “coarse-to-fine” work does not optimize heuris-
14We focus here on parsing, but pruning is generally useful in structured prediction.",8 Related work,[0],[0]
"E.g., Xu et al. (2013) train a classifier to prune (latent) alignments in a machine translation system.
tics directly but rather derives heuristics for pruning (Charniak et al., 2006; Petrov and Klein, 2007; Weiss and Taskar, 2010; Rush and Petrov, 2012) or prioritization (Klein and Manning, 2003; Pauls and Klein, 2009) from a coarser version of the model.",8 Related work,[0],[0]
"Combining these automatic methods with LOLS would require first enriching their heuristics with trainable parameters, or parameterizing the coarse-to-fine hierarchy itself as in the “feature pruning” work of He et al. (2013) and Strubell et al. (2015).
",8 Related work,[0],[0]
Dynamic features are ones that depend on previous actions.,8 Related work,[0],[0]
"In our setting, a policy could in principle benefit from considering the full state of the chart at Alg. 1 line 14.",8 Related work,[0],[0]
"While coarse-to-fine methods implicitly use certain dynamic features, training with dynamic features is a fairly new goal that is challenging to treat efficiently.",8 Related work,[0],[0]
"It has usually been treated with some form of simple imitation learning, using a heuristic training signal much as in our baseline (Jiang, 2014; He et al., 2013).",8 Related work,[0],[0]
"LOLS would be a more principled way to train such features, but for efficiency, our present paper restricts to static features that only access the state via π(w, i, k).",8 Related work,[0],[0]
This permits our fast CP and DP rollout algorithms.,8 Related work,[0],[0]
"It also reduces the time and space cost of dataset aggregation.15
LOLS attempts to do end-to-end training of a sequential decision-making system, without falling back on black-box optimization tools (Och, 2003; Chung and Galley, 2012) that ignore the sequential structure.",8 Related work,[0],[0]
"In NLP, sequential decisions are more commonly trained with step-by-step supervision
15LOLS repeatedly evaluates actions given (w, i, k).",8 Related work,[0],[0]
"We consolidate the resulting training examples by summing their reward vectors r̂, so the aggregated dataset does not grow over time.
",8 Related work,[0],[0]
"(Kuhlmann et al., 2011), using methods such as local classification (Punyakanok and Roth, 2001) or beam search with early update (Collins and Roark, 2004).",8 Related work,[0],[0]
LOLS tackles the harder setting where the only training signal is a joint assessment of the entire sequence of actions.,8 Related work,[0],[0]
"It is an alternative to policy gradient, which does not scale well to our long trajectories because of high variance in the estimated gradient and because random exploration around (even good) pruning policies most often results in no parse at all.",8 Related work,[0],[0]
"LOLS uses controlled comparisons, resulting in more precise “credit assignment” and tighter exploration.
",8 Related work,[0],[0]
"We would be remiss not to note that current transition-based parsers—for constituency parsing (Zhu et al., 2013; Crabbé, 2015) as well as dependency parsing (Chen and Manning, 2014)—are both incredibly fast and surprisingly accurate.",8 Related work,[0],[0]
"This may appear to undermine the motivation for our work, or at least for its application to fast parsing.16",8 Related work,[0],[0]
"However, transition-based parsers do not produce marginal probabilities of substructures, which can be useful features for downstream tasks.",8 Related work,[0],[0]
"Indeed, the transitionbased approach is essentially greedy and so it may fail on tasks with more ambiguity than parsing.",8 Related work,[0],[0]
"Current transition-based parsers also require step-by-step supervision, whereas our method can also be used to train in the presence of incomplete supervision, latent structure, or indirect feedback.",8 Related work,[0],[0]
"Our method could also be used immediately to speed up dynamic programming methods for MT, synchronous parsing, parsing with non-context-free grammar formalisms, and other structured prediction problems for which transition systems have not (yet) been designed.",8 Related work,[0],[0]
We presented an approach to learning pruning policies that optimizes end-to-end performance on a userspecified speed-accuracy tradeoff.,9 Conclusions,[0],[0]
We developed two novel algorithms for efficiently measuring how varying policy actions affects reward.,9 Conclusions,[0],[0]
"In the case of parsing, given a performance criterion and a good baseline policy for that criterion, the learner consistently manages to find a higher-reward policy.",9 Conclusions,[0],[0]
"We hope this work inspires a new generation of fast and accurate structured prediction models with tunable runtimes.
16Of course, LOLS can also train transition-based parsers (Chang et al., 2015a), or even vary their beam width dynamically.",9 Conclusions,[0],[0]
This material is based in part on research sponsored by the National Science Foundation under Grant No. 0964681 and DARPA under agreement number FA8750-13-2-0017 (DEFT program).,Acknowledgments,[0],[0]
"We’d like to thank Nathaniel Wesley Filardo, Adam Teichert, Matt Gormley and Hal Daumé III for helpful discussions.",Acknowledgments,[0],[0]
"Finally, we thank TACL action editor Marco Kuhlmann and the anonymous reviewers and copy editor for suggestions that improved this paper.",Acknowledgments,[0],[0]
Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing.,abstractText,[0],[0]
"Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy.",abstractText,[0],[0]
"This poses a difficult machine learning problem, which we tackle with the LOLS algorithm.",abstractText,[0],[0]
"LOLS training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms.",abstractText,[0],[0]
"We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.",abstractText,[0],[0]
Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing,title,[0],[0]
Deep neural networks (DNNs) have been widely used for machine learning applications due to their powerful capacity for modeling complex input patterns.,1. Introduction,[0],[0]
"Despite their success, it has been shown that DNNs are prone to training set biases, i.e. the training set is drawn from a joint distribution p(x, y) that is different from the distribution p(xv, yv) of the evaluation set.",1. Introduction,[0],[0]
"This distribution mismatch could have many
1Uber Advanced Technologies Group, Toronto ON, CANADA 2Department of Computer Science, University of Toronto, Toronto ON, CANADA.",1. Introduction,[0],[0]
"Correspondence to: Mengye Ren <mren3@uber.com>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
different forms.",1. Introduction,[0],[0]
Class imbalance in the training set is a very common example.,1. Introduction,[0],[0]
"In applications such as object detection in the context of autonomous driving, the vast majority of the training data is composed of standard vehicles but models also need to recognize rarely seen classes such as emergency vehicles or animals with very high accuracy.",1. Introduction,[0],[0]
"This will sometime lead to biased training models that do not perform well in practice.
",1. Introduction,[0],[0]
Another popular type of training set bias is label noise.,1. Introduction,[0],[0]
"To train a reasonable supervised deep model, we ideally need a large dataset with high-quality labels, which require many passes of expensive human quality assurance (QA).",1. Introduction,[0],[0]
"Although coarse labels are cheap and of high availability, the presence of noise will hurt the model performance, e.g. Zhang et al. (2017) has shown that a standard CNN can fit any ratio of label flipping noise in the training set and eventually leads to poor generalization performance.
",1. Introduction,[0],[0]
"Training set biases and misspecification can sometimes be addressed with dataset resampling (Chawla et al., 2002), i.e. choosing the correct proportion of labels to train a network on, or more generally by assigning a weight to each example and minimizing a weighted training loss.",1. Introduction,[0],[0]
"The example weights are typically calculated based on the training loss, as in many classical algorithms such as AdaBoost (Freund & Schapire, 1997), hard negative mining (Malisiewicz et al., 2011), self-paced learning (Kumar et al., 2010), and other more recent work (Chang et al., 2017; Jiang et al., 2017).
",1. Introduction,[0],[0]
"However, there exist two contradicting ideas in training loss based approaches.",1. Introduction,[0],[0]
"In noisy label problems, we prefer examples with smaller training losses as they are more likely to be clean images; yet in class imbalance problems, algorithms such as hard negative mining (Malisiewicz et al., 2011) prioritize examples with higher training loss since they are more likely to be the minority class.",1. Introduction,[0],[0]
"In cases when the training set is both imbalanced and noisy, these existing methods would have the wrong model assumptions.",1. Introduction,[0],[0]
"In fact, without a proper definition of an unbiased test set, solving the training set bias problem is inherently ill-defined.",1. Introduction,[0],[0]
"As the model cannot distinguish the right from the wrong, stronger regularization can usually work surprisingly well in certain synthetic noise settings.",1. Introduction,[0],[0]
"Here we argue that in order to learn general forms of training set biases, it is necessary to have a small unbiased validation to guide training.",1. Introduction,[0.9525978897123979],"['Our experimental results show that this leads to substantial improve- ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations.']"
"It is actually
not uncommon to construct a dataset with two parts - one relatively small but very accurately labeled, and another massive but coarsely labeled.",1. Introduction,[0],[0]
"Coarse labels can come from inexpensive crowdsourcing services or weakly supervised data (Cordts et al., 2016; Russakovsky et al., 2015; Chen & Gupta, 2015).
",1. Introduction,[0],[0]
"Different from existing training loss based approaches, we follow a meta-learning paradigm and model the most basic assumption instead: the best example weighting should minimize the loss of a set of unbiased clean validation examples that are consistent with the evaluation procedure.",1. Introduction,[0],[0]
"Traditionally, validation is performed at the end of training, which can be prohibitively expensive if we treat the example weights as some hyperparameters to optimize; to circumvent this, we perform validation at every training iteration to dynamically determine the example weights of the current batch.",1. Introduction,[0],[0]
"Towards this goal, we propose an online reweighting method that leverages an additional small validation set and adaptively assigns importance weights to examples in every iteration.",1. Introduction,[0],[0]
We experiment with both class imbalance and corrupted label problems and find that our approach significantly increases the robustness to training set biases.,1. Introduction,[0],[0]
The idea of weighting each training example has been well studied in the literature.,2. Related Work,[0],[0]
"Importance sampling (Kahn & Marshall, 1953), a classical method in statistics, assigns weights to samples in order to match one distribution to another.",2. Related Work,[0],[0]
"Boosting algorithms such as AdaBoost (Freund & Schapire, 1997), select harder examples to train subsequent classifiers.",2. Related Work,[0],[0]
"Similarly, hard example mining (Malisiewicz et al., 2011), downsamples the majority class and exploits the most difficult examples.",2. Related Work,[0],[0]
"Focal loss (Lin et al., 2017) adds a soft weighting scheme that emphasizes harder examples.
",2. Related Work,[0],[0]
Hard examples are not always preferred in the presence of outliers and noise processes.,2. Related Work,[0],[0]
Robust loss estimators typically downweigh examples with high loss.,2. Related Work,[0],[0]
"In selfpaced learning (Kumar et al., 2010), example weights are obtained through optimizing the weighted training loss encouraging learning easier examples first.",2. Related Work,[0],[0]
"In each step, the learning algorithm jointly solves a mixed integer program that iterates optimizing over model parameters and binary example weights.",2. Related Work,[0],[0]
"Various regularization terms on the example weights have since been proposed to prevent overfitting and trivial solutions of assigning weights to be all zeros (Kumar et al., 2010; Ma et al., 2017; Jiang et al., 2015).",2. Related Work,[0],[0]
Wang et al. (2017) proposed a Bayesian method that infers the example weights as latent variables.,2. Related Work,[0],[0]
"More recently, Jiang et al. (2017) proposed to use a meta-learning LSTM to output the weights of the examples based on the training loss.",2. Related Work,[0],[0]
"Reweighting examples is also related to curriculum learning (Bengio et al., 2009), where the model reweights
among many available tasks.",2. Related Work,[0],[0]
"Similar to self-paced learning, typically it is beneficial to start with easier examples.
",2. Related Work,[0],[0]
One crucial advantage of reweighting examples is robustness against training set bias.,2. Related Work,[0],[0]
"There has also been a multitude of prior studies on class imbalance problems, including using dataset resampling (Chawla et al., 2002; Dong et al., 2017), cost-sensitive weighting (Ting, 2000; Khan et al., 2015), and structured margin based objectives (Huang et al., 2016).",2. Related Work,[0],[0]
"Meanwhile, the noisy label problem has been thoroughly studied by the learning theory community (Natarajan et al., 2013; Angluin & Laird, 1988) and practical methods have also been proposed (Reed et al., 2014; Sukhbaatar & Fergus, 2014; Xiao et al., 2015; Azadi et al., 2016; Goldberger & Ben-Reuven, 2017; Li et al., 2017; Jiang et al., 2017; Vahdat, 2017; Hendrycks et al., 2018).",2. Related Work,[0],[0]
"In addition to corrupted data, Koh & Liang (2017); Muñoz-González et al. (2017) demonstrate the possibility of a dataset adversarial attack (i.e. dataset poisoning).
",2. Related Work,[0],[0]
"Our method improves the training objective through a weighted loss rather than an average loss and is an instantiation of meta-learning (Thrun & Pratt, 1998; Lake et al., 2017; Andrychowicz et al., 2016), i.e. learning to learn better.",2. Related Work,[0],[0]
"Using validation loss as the meta-objective has been explored in recent meta-learning literature for few-shot learning (Ravi & Larochelle, 2017; Ren et al., 2018; Lorraine & Duvenaud, 2018), where only a handful of examples are available for each class.",2. Related Work,[0],[0]
"Our algorithm also resembles MAML (Finn et al., 2017) by taking one gradient descent step on the meta-objective for each iteration.",2. Related Work,[0],[0]
"However, different from these meta-learning approaches, our reweighting method does not have any additional hyperparameters and circumvents an expensive offline training stage.",2. Related Work,[0],[0]
"Hence, our method can work in an online fashion during regular training.",2. Related Work,[0],[0]
"In this section, we derive our model from a meta-learning objective towards an online approximation that can fit into any regular supervised training.",3. Learning to Reweight Examples,[0],[0]
We give a practical implementation suitable for any deep network type and provide theoretical guarantees under mild conditions that our algorithm has a convergence rate of O(1/ 2).,3. Learning to Reweight Examples,[0],[0]
Note that this is the same as that of stochastic gradient descent (SGD).,3. Learning to Reweight Examples,[0],[0]
"Let (x, y) be an input-target pair, and {(xi, yi), 1 ≤",3.1. From a meta-learning objective to an online approximation,[0],[0]
i ≤ N} be the training set.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"We assume that there is a small unbiased and clean validation set {(xvi , yvi ), 1 ≤ i ≤M}, and M N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Hereafter, we will use superscript v to denote validation set and subscript i to denote the ith data.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We also assume
that the training set contains the validation set; otherwise, we can always add this small validation set into the training set and leverage more information during training.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let Φ(x, θ) be our neural network model, and θ be the model parameters.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"We consider a loss function C(ŷ, y) to minimize during training, where ŷ = Φ(x, θ).
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In standard training, we aim to minimize the expected loss for the training set: 1N ∑N i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"C(ŷi, yi) = 1 N ∑N i=1 fi(θ), where each input example is weighted equally, and fi(θ) stands for the loss function associating with data xi.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Here we aim to learn a reweighting of the inputs, where we minimize a weighted loss:
θ∗(w) = arg min θ N∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"wifi(θ), (1)
with wi unknown upon beginning.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Note that {wi}Ni=1 can be understood as training hyperparameters, and the optimal selection of w is based on its validation performance:
w∗ = arg min w,w≥0
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi,3.1. From a meta-learning objective to an online approximation,[0],[0]
(θ ∗(w)).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(2)
It is necessary that wi ≥ 0 for all i, since minimizing the negative training loss can usually result in unstable behavior.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Online approximation Calculating the optimal wi requires two nested loops of optimization, and every single loop can be very expensive.",3.1. From a meta-learning objective to an online approximation,[0],[0]
The motivation of our approach is to adapt online w through a single optimization loop.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"For each training iteration, we inspect the descent direction of some training examples locally on the training loss surface and reweight them according to their similarity to the descent direction of the validation loss surface.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"For most training of deep neural networks, SGD or its variants are used to optimize such loss functions.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"At every step t of training, a mini-batch of training examples {(xi, yi), 1 ≤ i ≤ n} is sampled, where n is the mini-batch size, n N .",3.1. From a meta-learning objective to an online approximation,[0],[0]
Then the parameters are adjusted according to the descent direction of the expected loss on the mini-batch.,3.1. From a meta-learning objective to an online approximation,[0],[0]
"Let’s consider vanilla SGD:
θt+1 = θt − α∇
( 1
n n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi(θt)
) , (3)
where α is the step size.
",3.1. From a meta-learning objective to an online approximation,[0],[0]
We want to understand what would be the impact of training example,3.1. From a meta-learning objective to an online approximation,[0],[0]
"i towards the performance of the validation set at training step t. Following a similar analysis to Koh & Liang (2017), we consider perturbing the weighting by i for each
training example in the mini- batch,
fi, (θ) = ifi(θ), (4)
θ̂t+1( ) = θt − α∇ n∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
"fi, (θ) ∣∣∣ θ=θt .",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(5)
We can then look for the optimal ∗ that minimizes the validation loss fv locally at step t:
∗t = arg min
1
M M∑ i=1",3.1. From a meta-learning objective to an online approximation,[0],[0]
fvi (θt+1( )).,3.1. From a meta-learning objective to an online approximation,[0],[0]
"(6)
Unfortunately, this can still be quite time-consuming.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To get a cheap estimate of wi at step t, we take a single gradient descent step on a mini-batch of validation samples wrt. t, and then rectify the output to get a non-negative weighting:
ui,t = −η ∂
∂",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t
1
m m∑ j=1 fvj (θt+1( )) ∣∣∣",3.1. From a meta-learning objective to an online approximation,[0],[0]
"i,t=0 , (7)
w̃i,t = max(ui,t, 0).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"(8)
where η is the descent step size on .
",3.1. From a meta-learning objective to an online approximation,[0],[0]
"To match the original training step size, in practice, we can consider normalizing the weights of all examples in a training batch so that they sum up to one.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In other words, we choose to have a hard constraint within the set {w : ‖w‖1 = 1} ∪ {0}.
wi,t = w̃i,t ( ∑ j w̃j,t) + δ",3.1. From a meta-learning objective to an online approximation,[0],[0]
"( ∑ j w̃j,t) , (9)
where δ(·) is to prevent the degenerate case when all wi’s in a mini-batch are zeros, i.e. δ(a) = 1 if a = 0, and equals to 0 otherwise.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Without the batch-normalization step, it is possible that the algorithm modifies its effective learning rate of the training progress, and our one-step look ahead may be too conservative in terms of the choice of learning rate (Wu et al., 2018).",3.1. From a meta-learning objective to an online approximation,[0],[0]
"Moreover, with batch normalization, we effectively cancel the meta learning rate parameter η.",3.1. From a meta-learning objective to an online approximation,[0],[0]
"In this section, we study how to compute wi,t in a multilayer perceptron (MLP) network.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
One of the core steps is to compute the gradients of the validation loss wrt.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"the local perturbation , We can consider a multi-layered network where we have parameters for each layer θ = {θl}Ll=1, and at every layer, we first compute zl the pre-activation, a weighted sum of inputs to the layer, and afterwards we apply a non-linear activation function σ to obtain z̃l the post-activation:
zl = θ > l z̃l−1, (10)
z̃l = σ(zl).",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(11)
During backpropagation, let gl be the gradients of loss wrt. zl, and the gradients wrt.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θl is given by z̃l−1g>l .,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"We can further express the gradients towards as a sum of local dot products.
∂ ∂",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t E [ fv(θt+1( )) ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"i,t=0 ] ∝− 1
m m∑ j=1 ∂fvj (θ) ∂θ ∣∣∣",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
>,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
θ=θt ∂fi(θ) ∂θ ∣∣∣ θ,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"=θt
=− 1 m m∑ j=1 L∑ l=1",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(z̃vj,l−1 >z̃i,l−1)(g v j,l >gi,l).
",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"(12)
Detailed derivations can be found in Supplementary Materials.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
Eq. 12 suggests that the meta-gradient on is composed of the sum of the products of two terms: z>zv and g>gv.,3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"The first dot product computes the similarity between the training and validation inputs to the layer, while the second computes the similarity between the training and validation gradient directions.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In other words, suppose that a pair of training and validation examples are very similar, and they also provide similar gradient directions, then this training example is helpful and should be up-weighted, and conversely, if they provide opposite gradient directions, this training example is harmful and should be downweighed.",3.2. Example: learning to reweight examples in a multi-layer perceptron network,[0],[0]
"In an MLP and a CNN, the unnormalized weights can be calculated based on the sum of the correlations of layerwise activation gradients and input activations.",3.3. Implementation using automatic differentiation,[0],[0]
"In more general networks, we can leverage automatic differentiation techniques to compute the gradient of the validation loss wrt.",3.3. Implementation using automatic differentiation,[0],[0]
the example weights of the current batch.,3.3. Implementation using automatic differentiation,[0],[0]
"As shown in Figure 1, to get the gradients of the example weights, one needs to first unroll the gradient graph of the training batch, and then use backward-on-backward automatic differentiation to take a second order gradient
pass (see Step 5 in Figure 1).",3.3. Implementation using automatic differentiation,[0],[0]
We list detailed step-bystep pseudo-code in Algorithm 1.,3.3. Implementation using automatic differentiation,[0],[0]
"This implementation can be generalized to any deep learning architectures and can be very easily implemented using popular deep learning frameworks such as TensorFlow (Abadi et al., 2016).
",3.3. Implementation using automatic differentiation,[0],[0]
"Algorithm 1 Learning to Reweight Examples using Automatic Differentiation Require: θ0, Df , Dg , n, m Ensure: θT
1: for t = 0 ...",3.3. Implementation using automatic differentiation,[0],[0]
T,3.3. Implementation using automatic differentiation,[0],[0]
"− 1 do 2: {Xf , yf} ← SampleMiniBatch(Df , n) 3: {Xg, yg} ← SampleMiniBatch(Dg , m) 4: ŷf ← Forward(Xf , yf , θt) 5: ← 0; lf ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"iC(yf,i, ŷf,i) 6: ∇θt ← BackwardAD(lf , θt) 7: θ̂t ← θt − α∇θt 8: ŷg",3.3. Implementation using automatic differentiation,[0],[0]
"← Forward(Xg, yg, θ̂t) 9: lg ← 1m ∑m i=1",3.3. Implementation using automatic differentiation,[0],[0]
"C(yg,i, ŷg,i)
10: ∇ ← BackwardAD(lg, ) 11:",3.3. Implementation using automatic differentiation,[0],[0]
"w̃ ← max(−∇ , 0); w ← w̃∑
j w̃+δ( ∑ j w̃)
12: l̂f ← ∑n i=1",3.3. Implementation using automatic differentiation,[0],[0]
"wiC(yi, ŷf,i) 13: ∇θt",3.3. Implementation using automatic differentiation,[0],[0]
"← BackwardAD(l̂f , θt) 14: θt+1 ← OptimizerStep(θt,∇θt) 15: end for
Training time Our automatic reweighting method will introduce a constant factor of overhead.",3.3. Implementation using automatic differentiation,[0],[0]
"First, it requires two full forward and backward passes of the network on training and validation respectively, and then another backward on backward pass (Step 5 in Figure 1), to get the gradients to the example weights, and finally a backward pass to minimize the reweighted objective.",3.3. Implementation using automatic differentiation,[0],[0]
"In modern networks, a backwardon-backward pass usually takes about the same time as a forward pass, and therefore compared to regular training, our method needs approximately 3× training time; it is also possible to reduce the batch size of the validation pass for speedup.",3.3. Implementation using automatic differentiation,[0],[0]
"We expect that it is worthwhile to spend the extra time to avoid the irritation of choosing early stopping, finetuning schedules, and other hyperparameters.",3.3. Implementation using automatic differentiation,[0],[0]
"Convergence results of SGD based optimization methods are well-known (Reddi et al., 2016).",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However it is still meaningful to establish a convergence result about our method since it involves optimization of two-level objectives (Eq. 1, 2) rather than one, and we further make some firstorder approximation by introducing Eq. 7.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Here, we show theoretically that our method converges to the critical point of the validation loss function under some mild conditions, and we also give its convergence rate.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"More detailed proofs can be found in the Supplementary Materials.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Definition 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"A function f(x) : Rd → R is said to be Lipschitz-smooth with constant L if
‖∇f(x)−∇f(y)‖ ≤",3.4. Analysis: convergence of the reweighted training,[0],[0]
"L‖x− y‖,∀x, y ∈ Rd.
Definition 2. f(x) has σ-bounded gradients if ‖∇f(x)‖ ≤ σ for all x ∈ Rd.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
"In most real-world cases, the high-quality validation set is really small, and thus we could set the mini-batch size m to be the same as the size of the validation set M .",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Under this condition, the following lemma shows that our algorithm always converges to a critical point of the validation loss.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"However, our method is not equivalent to training a model only on this small validation set.",3.4. Analysis: convergence of the reweighted training,[0],[0]
Because directly training a model on a small validation set will lead to severe overfitting issues.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"On the contrary, our method can leverage useful information from a larger training set, and still converge to an appropriate distribution favored by this clean and balanced validation dataset.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"This helps both generalization and robustness to biases in the training set, which will be shown in our experiments.
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Lemma 1.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose the validation loss function is Lipschitzsmooth with constant L, and the train loss function fi of training data xi have σ-bounded gradients.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Let the learning rate αt satisfies αt ≤ 2nLσ2 , where n is the training batch size.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"Then, following our algorithm, the validation loss always monotonically decreases for any sequence of training batches, namely,
G(θt+1) ≤ G(θt), (13)
where G(θ) is the total validation loss
G(θ) = 1
M M∑ i=1",3.4. Analysis: convergence of the reweighted training,[0],[0]
fvi (θt+1( )).,3.4. Analysis: convergence of the reweighted training,[0],[0]
"(14)
Furthermore, in expectation, the equality in Eq. 13 holds only when the gradient of validation loss becomes 0 at some time step t, namely Et [G(θt+1)]",3.4. Analysis: convergence of the reweighted training,[0],[0]
"= G(θt) if and only if ∇G(θt) = 0, where the expectation is taking over possible training batches at time step t.
Moreover, we can prove the convergence rate of our method to be O(1/ 2).
",3.4. Analysis: convergence of the reweighted training,[0],[0]
Theorem 2.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"Suppose G, fi and αt satisfy the aforementioned conditions, then Algorithm 1 achieves E",3.4. Analysis: convergence of the reweighted training,[0],[0]
[ ‖∇G(θt)‖2 ] ≤ in O(1/ 2) steps.,3.4. Analysis: convergence of the reweighted training,[0],[0]
"More specifically,
min 0<t<T
E [ ‖∇G(θt)‖2 ] ≤ C√
T , (15)
where C is some constant independent of the convergence process.",3.4. Analysis: convergence of the reweighted training,[0],[0]
"To test the effectiveness of our reweighting algorithm, we designed both class imbalance and noisy label settings, and a combination of both, on standard MNIST and CIFAR benchmarks for image classification using deep CNNs.",4. Experiments,[0],[0]
We use the standard MNIST handwritten digit classification dataset and subsample the dataset to generate a class imbalance binary classification task.,4.1. MNIST data imbalance experiments,[0],[0]
"We select a total of 5,000 images of size 28×28 on class 4 and 9, where 9 dominates the training data distribution.",4.1. MNIST data imbalance experiments,[0],[0]
We train a standard LeNet on this task and we compare our method with a suite of commonly used tricks for class imbalance: 1) PROPORTION weights each example by the inverse frequency 2),4.1. MNIST data imbalance experiments,[0],[0]
"RESAMPLE samples a class-balanced minibatch for each iteration 3) HARD MINING selects the highest loss examples from the majority class and 4) RANDOM is a random example weight baseline that assigns weights based on a rectified Gaussian distribution:
wrndi = max(zi, 0)∑",4.1. MNIST data imbalance experiments,[0],[0]
"i max(zi, 0) , where zi ∼ N (0, 1).",4.1. MNIST data imbalance experiments,[0],[0]
"(16)
To make sure that our method does not have the privilege of training on more data, we split the balanced validation set of 10 images directly from the training set.",4.1. MNIST data imbalance experiments,[0],[0]
"The network is trained with SGD with a learning rate of 1e-3 and mini-batch size of 100 for a total of 8,000 steps.
",4.1. MNIST data imbalance experiments,[0],[0]
Figure 2 plots the test error rate across various imbalance ratios averaged from 10 runs with random splits.,4.1. MNIST data imbalance experiments,[0],[0]
Note that our method significantly outperforms all the baselines.,4.1. MNIST data imbalance experiments,[0],[0]
"With class imbalance ratio of 200:1, our method only reports a small increase of error rate around 2%, whereas other methods suffer terribly under this setting.",4.1. MNIST data imbalance experiments,[0],[0]
"Compared with resampling and hard negative mining baselines, our approach does not throw away samples based on its class or training loss - as long as a sample is helpful towards the validation loss, it will be included as a part of the training loss.",4.1. MNIST data imbalance experiments,[0],[0]
Reweighting algorithm can also be useful on datasets where the labels are noisy.,4.2. CIFAR noisy label experiments,[0],[0]
"We study two settings of label noise here:
• UNIFORMFLIP: All label classes can uniformly flip to any other label classes, which is the most studied in the literature.",4.2. CIFAR noisy label experiments,[0],[0]
• BACKGROUNDFLIP:,4.2. CIFAR noisy label experiments,[0],[0]
All label classes can flip to a single background class.,4.2. CIFAR noisy label experiments,[0],[0]
This noise setting is very realistic.,4.2. CIFAR noisy label experiments,[0],[0]
"For instance, human annotators may not have recognized all the positive instances, while the
rest remain in the background class.",4.2. CIFAR noisy label experiments,[0],[0]
"This is also a combination of label imbalance and label noise since the background class usually dominates the label distribution.
",4.2. CIFAR noisy label experiments,[0],[0]
"We compare our method with prior work on the noisy label problem.
",4.2. CIFAR noisy label experiments,[0],[0]
"• REED, proposed by Reed et al. (2014), is a bootstrapping technique where the training target is a convex combination of the model prediction and the label.
",4.2. CIFAR noisy label experiments,[0],[0]
"• S-MODEL, proposed by Goldberger & Ben-Reuven (2017), adds a fully connected softmax layer after the regular classification output layer to model the noise transition matrix.
",4.2. CIFAR noisy label experiments,[0],[0]
"• MENTORNET, proposed by Jiang et al. (2017), is an RNN-based meta-learning model that takes in a sequence of loss values and outputs the example weights.",4.2. CIFAR noisy label experiments,[0],[0]
"We compare numbers reported in their paper with a base model that achieves similar test accuracy under 0% noise.
",4.2. CIFAR noisy label experiments,[0],[0]
"In addition, we propose two simple baselines: 1) RANDOM, which assigns weights according to a rectified Gaussian (see Eq. 16); 2) WEIGHTED, designed for BACKGROUNDFLIP, where the model knows the oracle noise ratio for each class and reweights the training loss proportional to the percentage of clean images of that label class.
",4.2. CIFAR noisy label experiments,[0],[0]
"Clean validation set For UNIFORMFLIP, we use 1,000 clean images in the validation set; for BACKGROUNDFLIP, we use 10 clean images per label class.",4.2. CIFAR noisy label experiments,[0],[0]
"Since our method uses information from the clean validation, for a fair comparison, we conduct an additional finetuning on the clean data based on the pre-trained baselines.",4.2. CIFAR noisy label experiments,[0],[0]
"We also study the effect on the size of the clean validation set in an ablation study.
",4.2. CIFAR noisy label experiments,[0],[0]
"Hyper-validation set For monitoring training progress and tuning baseline hyperparameters, we split out another
5,000 hyper-validation set from the 50,000 training images.",4.2. CIFAR noisy label experiments,[0],[0]
"We also corrupt the hyper-validation set with the same noise type.
",4.2. CIFAR noisy label experiments,[0],[0]
"Experimental details For REED model, we use the best β reported in Reed et al. (2014) (β = 0.8 for hard bootstrapping and β = 0.95 for soft bootstrapping).",4.2. CIFAR noisy label experiments,[0],[0]
"For the S-MODEL, we explore two versions to initialize the transition weights: 1) a smoothed identity matrix; 2) in background flip experiments we consider initializing the transition matrix with the confusion matrix of a pre-trained baseline model (S-MODEL +CONF).",4.2. CIFAR noisy label experiments,[0],[0]
"We find baselines can easily overfit the training noise, and therefore we also study early stopped versions of the baselines to provide a stronger comparison.",4.2. CIFAR noisy label experiments,[0],[0]
"In contrast, we find early stopping not necessary for our method.
",4.2. CIFAR noisy label experiments,[0],[0]
"To make our results comparable with the ones reported in MENTORNET and to save computation time, we exchange their Wide ResNet-101-10 with a Wide ResNet28-10 (WRN-28-10) (Zagoruyko & Komodakis, 2016) with dropout 0.3 as our base model in the UNIFORMFLIP experiments.",4.2. CIFAR noisy label experiments,[0],[0]
We find that test accuracy differences between the two base models are within 0.5% on CIFAR datasets under 0% noise.,4.2. CIFAR noisy label experiments,[0],[0]
"In the BACKGROUNDFLIP experiments, we use a ResNet-32 (He et al., 2016) as our base model.
",4.2. CIFAR noisy label experiments,[0],[0]
"We train the models with SGD with momentum, at an initial learning rate 0.1 and a momentum 0.9 with mini-batch size 100.",4.2. CIFAR noisy label experiments,[0],[0]
"For ResNet-32 models, the learning rate decays×0.1 at 40K and 60K steps, for a total of 80K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"For WRN and early stopped versions of ResNet-32 models, the learning rate decays at 40K and 50K steps, for a total of 60K steps.",4.2. CIFAR noisy label experiments,[0],[0]
"Under regular 0% noise settings, our base ResNet-32 gets 92.5% and 68.1% classification accuracy on CIFAR-10 and 100, and the WRN-28-10 gets 95.5% and 78.2%.",4.2. CIFAR noisy label experiments,[0],[0]
"For the finetuning stage, we run extra 5K steps of training on the
CLEAN ONLY 15.90 ± 3.32 8.06 ± 0.76 BASELINE +FT 82.82 ± 0.93 54.23 ± 1.75 BASELINE +ES +FT 85.19 ± 0.46 55.22 ± 1.40 WEIGHTED +FT 85.98 ± 0.47 53.99 ± 1.62 S-MODEL +CONF +FT 81.90 ± 0.85 53.11 ± 1.33 S-MODEL +CONF +ES +FT 85.86 ± 0.63 55.75 ± 1.26
OURS 86.73 ± 0.48 59.30 ± 0.60
limited clean data.
",4.2. CIFAR noisy label experiments,[0],[0]
"We report the average test accuracy for 5 different random splits of clean and noisy labels, with 95% confidence interval in Table 1 and 2.",4.2. CIFAR noisy label experiments,[0],[0]
"The background classes for the 5 trials are [0, 1, 3, 5, 7] (CIFAR-10) and [7, 12, 41, 62, 85] (CIFAR-100).",4.2. CIFAR noisy label experiments,[0],[0]
"The first result that draws our attention is that “Random” performs surprisingly well on the UNIFORMFLIP benchmark, outperforming all historical methods that we compared.",4.3. Results and Discussion,[0],[0]
"Given that its performance is comparable with Baseline on BACKGROUNDFLIP and MNIST class imbalance, we hypothesize that random example weights act as a strong regularizer and under which the learning objective on UNIFORMFLIP is still consistent.
",4.3. Results and Discussion,[0],[0]
"Regardless of the strong baseline, our method ranks the top on both UNIFORMFLIP and BACKGROUNDFLIP, showing our method is less affected by the changes in the noise type.",4.3. Results and Discussion,[0],[0]
"On CIFAR-100, our method wins more than 3% compared to the state-of-the-art method.
",4.3. Results and Discussion,[0],[0]
Understanding the reweighting mechanism It is beneficial to understand how our reweighting algorithm contributes to learning more robust models during training.,4.3. Results and Discussion,[0],[0]
"First, we use a pre-trained model (trained at half of the total iterations without learning rate decay) and measure the example weight distribution of a randomly sampled batch of validation images, which the model has never seen.",4.3. Results and Discussion,[0],[0]
"As shown in the left figure of Figure 3, our model correctly
pushes most noisy images to zero weights.",4.3. Results and Discussion,[0],[0]
"Secondly, we conditioned the input mini-batch to be a single nonbackground class and randomly flip 40% of the images to the background, and we would like to see how well our model can distinguish clean and noisy images.",4.3. Results and Discussion,[0],[0]
"As shown in Figure 3 right, the model is able to reliably detect images that are flipped to the background class.
",4.3. Results and Discussion,[0],[0]
"Robustness to overfitting noise Throughout experimentation, we find baseline models can easily overfit to the noise in the training set.",4.3. Results and Discussion,[0],[0]
"For example, shown in Table 2, applying early stopping (“ES”) helps the classification performance of “S-Model” by over 10% on CIFAR-10.",4.3. Results and Discussion,[0],[0]
"Figure 6 compares the final confusion matrices of the baseline and the proposed algorithm, where a large proportion of noise transition probability is cleared in the final prediction.",4.3. Results and Discussion,[0],[0]
Figure 7 shows training curves on the BACKGROUNDFLIP experiments.,4.3. Results and Discussion,[0],[0]
"After the first learning rate decay, both “Baseline” and “SModel” quickly degrade their validation performance due to overfitting, while our model remains the same validation accuracy until termination.",4.3. Results and Discussion,[0],[0]
"Note that here “S-Model” knows the oracle noise ratio in each class, and this information is
not available in our method.
",4.3. Results and Discussion,[0],[0]
Impact of the noise level We would like to investigate how strongly our method can perform on a variety of noise levels.,4.3. Results and Discussion,[0],[0]
"Shown in Figure 5, our method only drops 6% accuracy when the noise ratio increased from 0% to 50%;
whereas the baseline has dropped more than 40%.",4.3. Results and Discussion,[0],[0]
"At 0% noise, our method only slightly underperforms baseline.",4.3. Results and Discussion,[0],[0]
"This is reasonable since we are optimizing on the validation set, which is strictly a subset of the full training set, and therefore suffers from its own subsample bias.
",4.3. Results and Discussion,[0],[0]
"Size of the clean validation set When the size of the clean validation set grows larger, fine-tuning on the validation set will be a reasonble approach.",4.3. Results and Discussion,[0],[0]
"Here, we make an attempt to explore the tradeoff and understand when fine-tuning becomes beneficial.",4.3. Results and Discussion,[0],[0]
Figure 4 plots the classification performance when we varied the size of the clean validation on BACKGROUNDFLIP.,4.3. Results and Discussion,[0],[0]
"Surprisingly, using 15 validation images for all classes only results in a 2% drop in performance, and the overall classification performance does not grow after having more than 100 validation images.",4.3. Results and Discussion,[0],[0]
"In comparison, we observe a significant drop in performance when only fine-tuning on these 15 validation images for the baselines, and the performance catches up around using 1,000 validation images (100 per class).",4.3. Results and Discussion,[0],[0]
"This phenomenon suggests that in our method the clean validation acts more like a regularizer rather than a data source for parameter finetuning, and potentially our method can be complementary with fine-tuning based method when the size of the clean set grows larger.",4.3. Results and Discussion,[0],[0]
"In this work, we propose an online meta-learning algorithm for reweighting training examples and training more robust deep learning models.",5. Conclusion,[0],[0]
"While various types of training set biases exist and manually designed reweighting objectives have their own bias, our automatic reweighting algorithm shows superior performance dealing with class imbalance, noisy labels, and both.",5. Conclusion,[0],[0]
Our method can be directly applied to any deep learning architecture and is expected to train end-to-end without any additional hyperparameter search.,5. Conclusion,[0],[0]
"Validating on every training step is a novel setting and we show that it has links with model regularization, which can be a fruitful future research direction.",5. Conclusion,[0],[0]
Deep neural networks have been shown to be very powerful modeling tools for many supervised learning tasks involving complex input patterns.,abstractText,[0],[0]
"However, they can also easily overfit to training set biases and label noises.",abstractText,[0],[0]
"In addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional hyperparameters, such as example mining schedules and regularization hyperparameters.",abstractText,[0],[0]
"In contrast to past reweighting methods, which typically consist of functions of the cost value of each example, in this work we propose a novel meta-learning algorithm that learns to assign weights to training examples based on their gradient directions.",abstractText,[0],[0]
"To determine the example weights, our method performs a meta gradient descent step on the current mini-batch example weights (which are initialized from zero) to minimize the loss on a clean unbiased validation set.",abstractText,[0],[0]
"Our proposed method can be easily implemented on any type of deep network, does not require any additional hyperparameter tuning, and achieves impressive performance on class imbalance and corrupted label problems where only a small amount of clean validation data is available.",abstractText,[0],[0]
Learning to Reweight Examples for Robust Deep Learning,title,[0],[0]
"Many natural language processing (NLP) and computer vision problems necessitate predicting structured outputs such as labeled sequences, trees or general graphs (Smith, 2010; Nowozin & Lampert, 2011).",1. Introduction,[0],[0]
Such tasks require modeling both input-output relationships and the interactions between predicted outputs to capture correlations.,1. Introduction,[0],[0]
"Across the various structured prediction formulations (Lafferty et al., 2001; Taskar et al., 2003; Chang et al., 2012), prediction requires solving inference problems by searching for scoremaximizing output structures.",1. Introduction,[0],[0]
"The search space for inference is typically large (e.g., all parse trees), and grows with input size.",1. Introduction,[0],[0]
"Exhaustive search can be prohibitive and standard alternatives are either: (a) perform exact inference with a large computational cost or, (b) approximate inference to sacrifice accuracy in favor of time.
",1. Introduction,[0],[0]
"1School of Computing, University of Utah, Salt Lake City, Utah, USA.",1. Introduction,[0],[0]
"Correspondence to: Xingyuan Pan <xpan@cs.utah.edu>, Vivek Srikumar <svivek@cs.utah.edu>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"In this paper, we focus on the computational cost of inference.",1. Introduction,[0],[0]
"We argue that naturally occurring problems have remarkable regularities across both inputs and outputs, and traditional formulations of inference ignore them.",1. Introduction,[0],[0]
"For example, parsing an n-word sentence will cost a standard head-driven lexical parser O(n5) time.",1. Introduction,[0],[0]
Current practice in NLP is to treat each new sentence as a fresh discrete optimization problem and pay the computational price each time.,1. Introduction,[0],[0]
"However, this practice is not only expensive, but also wasteful!",1. Introduction,[0],[0]
"We ignore the fact that slight changes to inputs often do not change the output, or even the sequence of steps taken to produce it.",1. Introduction,[0],[0]
"Moreover, not all outputs are linguistically meaningful structures; as we make more predictions, we should be able to learn to prune the output space.
",1. Introduction,[0],[0]
The motivating question that drives our work is: Can we design inference schemes that learn to make a trained structured predictor faster without sacrificing output quality?,1. Introduction,[0],[0]
"After training, the structured classifier can be thought as a black-box.",1. Introduction,[0],[0]
"Typically, once deployed, it is never modified over its lifetime of classifying new examples.",1. Introduction,[0],[0]
"Subsequently, we can view each prediction of the black-box classifier as an opportunity to learn how to navigate the output space more efficiently.",1. Introduction,[0],[0]
"Thus, if the classifier sees a previously encountered situation, it could make some decisions without needless computations.
",1. Introduction,[0],[0]
We formalize this intuition by considering the trained models as solving arbitrary integer linear programs (ILPs) for combinatorial inference.,1. Introduction,[0],[0]
"We train a second, inexpensive speedup classifier which acts as a heuristic for a searchbased inference algorithm that mimics the more expensive black-box classifier.",1. Introduction,[0],[0]
The speedup heuristic is a function that learns regularities among predicted structures.,1. Introduction,[0],[0]
"We present a mistake bound algorithm that, over the classifier’s lifetime, learns to navigate the feasible regions of the ILPs.",1. Introduction,[0],[0]
"By doing so, we can achieve a reduction in inference time.
",1. Introduction,[0],[0]
We further identify inference situations where the learned speedup heuristic alone can correctly label parts of the outputs without computing the corresponding input features.,1. Introduction,[0],[0]
"In such situations, the search algorithm can safely ignore parts of inputs if the corresponding outputs can be decided based on the sub-structures constructed so far.",1. Introduction,[0],[0]
"Seen this way, the speedup classifier can be seen as a statistical cache of past decisions made by the black-box classifier.
",1. Introduction,[0],[0]
We instantiate our strategy to the task of predicting entities and relations from sentences.,1. Introduction,[0],[0]
"Using an ILP based black-box classifier, we show that the trained speedup classifier mimics the reference inference algorithm to obtain improvements in running time, and also recovers its accuracy.",1. Introduction,[0],[0]
"Indeed, by learning to ignore input components when they will not change the prediction, we show that learned search strategy outperforms even greedy search in terms of speed.
",1. Introduction,[0],[0]
"To summarize, the main contribution of this paper is the formalization of the problem of learning to make structured output classifiers faster without sacrificing accuracy.",1. Introduction,[0],[0]
We develop a learning-to-search framework to train a speedup classifier with a mistake-bound guarantee and a sufficient condition to safely avoid computing input-based features.,1. Introduction,[0],[0]
"We show empirically on an entity-relation extraction task that we can learn a speedup classifier that is (a) faster than both the state-of-the-art Gurobi optimizer and greedy search, and (b) does not incur a loss in output quality.",1. Introduction,[0],[0]
"First, we will define the notation used in this paper with a running example that requires of identifying entity types and their relationships in text.",2. Notation and Preliminaries,[0],[0]
"The input to the problem consists of sentences such as:
Colin went back home in Ordon Village.
",2. Notation and Preliminaries,[0],[0]
"These inputs are typically preprocessed — here, we are given spans of text (underlined) corresponding to entities.",2. Notation and Preliminaries,[0],[0]
"We will denote such preprocessed inputs to the structured prediction problem as x.
We seek to produce a structure y ∈",2. Notation and Preliminaries,[0],[0]
"Yx (e.g., labeled trees, graphs) associated with these inputs.",2. Notation and Preliminaries,[0],[0]
"Here, Yx is the set of all possible output structures for the input x.",2. Notation and Preliminaries,[0],[0]
"In the example problem, our goal is to assign types to the entities and also label the relationships between them.",2. Notation and Preliminaries,[0],[0]
"Suppose our task has three types of entities: person, location and organization.",2. Notation and Preliminaries,[0],[0]
"A pair of entities can participate in one of five possible directed relations: Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",2. Notation and Preliminaries,[0],[0]
"Additionally, there is a special entity label NoEnt meaning a text span is not an entity, and a special relation label NoRel indicating that two spans are unrelated.",2. Notation and Preliminaries,[0],[0]
"Figure 1 shows a plausible structure for the example sentence as per this scheme.
",2. Notation and Preliminaries,[0],[0]
A standard way to model the prediction problem requires learning a model that scores all structures in Yx and searching for the score-maximizing structure.,2. Notation and Preliminaries,[0],[0]
"Linear models are commonly used as scoring functions, and require a feature vector characterizing input-output relationships Φ (x,y).",2. Notation and Preliminaries,[0],[0]
We will represent the model by a weight vector α.,2. Notation and Preliminaries,[0],[0]
"Every structure y associated with an input x is scored as the dot product α · Φ (x,y).",2. Notation and Preliminaries,[0],[0]
"The goal of prediction is to find the
structure y∗ that maximizes this score.",2. Notation and Preliminaries,[0],[0]
"That is,
y∗ = arg max y∈Yx α ·",2. Notation and Preliminaries,[0],[0]
"Φ (x,y) .",2. Notation and Preliminaries,[0],[0]
"(1)
Learning involves using training data to find the best weight vector α.
",2. Notation and Preliminaries,[0],[0]
"In general, the output structure y is a set of K categorical inference variables {y1, y2, · · · , yK} , each of which can take a value from a predefined set of n labels.",2. Notation and Preliminaries,[0],[0]
"That is, each yk ∈ y takes a value from {l1, l2, · · · , ln}.1 In our running example, the inference variables correspond to the four decisions that define the structure: the labels for the two entities, and the relations in each direction.",2. Notation and Preliminaries,[0],[0]
"The feature function Φ decomposes into a sum of features over each yk, each denoted by Φk, giving us the inference problem:
y∗ = arg max y∈Yx K∑",2. Notation and Preliminaries,[0],[0]
k=1 α ·,2. Notation and Preliminaries,[0],[0]
"Φk ( x, yk ) .",2. Notation and Preliminaries,[0],[0]
"(2)
The dependencies between the yk’s specify the nature of the output space.",2. Notation and Preliminaries,[0],[0]
Determining each yk in isolation greedily does not typically represent a viable inference strategy because constraints connecting the variables are ignored.,2. Notation and Preliminaries,[0],[0]
"In this spirit, the problem of finding the best structure can be viewed as a combinatorial optimization problem.
",2. Notation and Preliminaries,[0],[0]
"In this paper, we consider the scenario in which we have already trained a model α.",2. Notation and Preliminaries,[0],[0]
"We focus on solving the inference problem (i.e.,Eq.",2. Notation and Preliminaries,[0],[0]
(2)) efficiently.,2. Notation and Preliminaries,[0],[0]
We conjecture that it should be possible to observe a black-box inference algorithm over its lifetime to learn to predict faster without losing accuracy.,2. Notation and Preliminaries,[0],[0]
One common way to solve inference is by designing efficient dynamic programming algorithms that exploit problem structure.,2.1. Black-box Inference Mechanisms,[0],[0]
"While effective, this approach is limited to special cases where the problem admits efficient decoding, thus placing restrictions on factorization and feature design.
",2.1. Black-box Inference Mechanisms,[0.9552547844210827],"['canonical form feature itself performs relatively poorly on literal usages, it provides information that enables the word2vec model to better identify literal usages.']"
"In this paper, we seek to reason about the problem of predicting structures in the general case.",2.1. Black-box Inference Mechanisms,[0],[0]
"Since inference is essentially a combinatorial optimization problem, without loss
1We make this choice for simplicity of notation.",2.1. Black-box Inference Mechanisms,[0],[0]
"In general, K depends on the size of the input x, and categorical variables may take values from different label sets.
of generality, we can represent any inference problem as an integer linear programming (ILP) instance (Schrijver, 1998).",2.1. Black-box Inference Mechanisms,[0],[0]
To represent the inference task in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
"(2) as an ILP instance, we will define indicator variables of the form zki ∈ {0, 1}, which stands for the decision that the categorical variable yk is assigned the ith label among the n labels.",2.1. Black-box Inference Mechanisms,[0],[0]
"That is, zki = 1 if yk = li, and 0 otherwise.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using this notation, we can write the cost of any structure y in terms of the indicators as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
cki,2.1. Black-box Inference Mechanisms,[0],[0]
z,2.1. Black-box Inference Mechanisms,[0],[0]
k,2.1. Black-box Inference Mechanisms,[0],[0]
i .,2.1. Black-box Inference Mechanisms,[0],[0]
"(3)
Here, cki is a stand in for −α ·",2.1. Black-box Inference Mechanisms,[0],[0]
"Φk (x, li), namely the cost (negative score) associated with this decision.2",2.1. Black-box Inference Mechanisms,[0],[0]
"In our example, suppose the first categorical variable y1 corresponds to the entity Colin, and it has possible labels {person,location, . . .",2.1. Black-box Inference Mechanisms,[0],[0]
}.,2.1. Black-box Inference Mechanisms,[0],[0]
"Then, assigning person to Colin would correspond to setting z11 = 1, and z 1 i = 0 for all i 6= 1.",2.1. Black-box Inference Mechanisms,[0],[0]
"Using the labels enumerated in §2, there will be 20 indicators for the four categorical decisions.
",2.1. Black-box Inference Mechanisms,[0],[0]
"Of course, arbitrary assignments to the indicators is not allowed.",2.1. Black-box Inference Mechanisms,[0],[0]
We can define the set of feasible structures using linear constraints.,2.1. Black-box Inference Mechanisms,[0],[0]
"Clearly, each categorical variable can take exactly one label, which can be expressed via:
n∑ i=1 zki = 1, for all k. (4)
",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, we can define the set of valid structures Yx using a collection of m linear constraints, the jth one of which can be written as
K∑ k=1 n∑ i=1",2.1. Black-box Inference Mechanisms,[0],[0]
"Akjiz k i = bj , for all j. (5)
",2.1. Black-box Inference Mechanisms,[0],[0]
These structural constraints characterize the interactions between the categorical variables.,2.1. Black-box Inference Mechanisms,[0],[0]
"For example, if a directed edge in our running example is labeled as LiveIn, then, its source and target must be a person and a location respectively.",2.1. Black-box Inference Mechanisms,[0],[0]
"While Eq.(5) only shows equality constraints, in practice, inequality constraints can also be included.
",2.1. Black-box Inference Mechanisms,[0],[0]
The inference problem in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(2) is equivalent to the problem of minimizing the objective in Eq.,2.1. Black-box Inference Mechanisms,[0],[0]
(3) over the 0-1 indicator variables subject to the constraints in Eqs.,2.1. Black-box Inference Mechanisms,[0],[0]
"(4) and (5).
",2.1. Black-box Inference Mechanisms,[0],[0]
We should note the difference between the ability to write an inference problem as an ILP instance and actually solving it as one.,2.1. Black-box Inference Mechanisms,[0],[0]
"The former gives us the ability to reason about inference in general, and perhaps using other methods (such as Lagrangian relaxation (Lemaréchal, 2001)) for inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"However, solving problems with industrial strength ILP
2The negation defines an equivalent minimization problem and makes subsequent description of the search framework easier.
solvers such as the Gurobi solver3 is competitive with other approaches in terms of inference time, even though they may not directly exploit problem structure.
",2.1. Black-box Inference Mechanisms,[0],[0]
"In this work, we use the general structure of the ILP inference formulation to develop the theory for speeding up inference.",2.1. Black-box Inference Mechanisms,[0],[0]
"In addition, because of its general applicability and fast inference speed, we use the Gurobi ILP solver as our black-box classifier, and learn a speedup heuristic to make even faster inference.",2.1. Black-box Inference Mechanisms,[0],[0]
Directly applying the black-box solver for the large output spaces may be impractical.,2.2. Inference as Search,[0],[0]
An alternative general purpose strategy for inference involves framing the maximization in Eq.,2.2. Inference as Search,[0],[0]
"(2) as a graph search problem.
",2.2. Inference as Search,[0],[0]
"Following Russell & Norvig (2003); Xu et al. (2009), a general graph search problem requires defining an initial search node I , a successor function s(·), and a goal test.",2.2. Inference as Search,[0],[0]
The successor function s(·) maps a search node to its successors.,2.2. Inference as Search,[0],[0]
The goal test determines whether a node is a goal node.,2.2. Inference as Search,[0],[0]
"Usually, each search step is associated with a cost function, and we seek to find a goal node with the least total cost.
",2.2. Inference as Search,[0],[0]
We can define the search problem corresponding to inference as follows.,2.2. Inference as Search,[0],[0]
"We will denote a generic search node in the graph as v, which corresponds to a set of partially assigned categorical variables.",2.2. Inference as Search,[0],[0]
"Specifically, we will define the search node v as a set of pairs {(k, i)}, each element of which specifies that the variable yk is assigned the ith label.",2.2. Inference as Search,[0],[0]
The initial search node I is the empty set since none of the variables has been assigned when the search begins.,2.2. Inference as Search,[0],[0]
"For a node v, its successors s(v) is a set of nodes, each containing one more assigned variable than v. A node is a goal node if all variables yk’s have been assigned.",2.2. Inference as Search,[0],[0]
"The size of any goal node is K, the number of categorical variables.
",2.2. Inference as Search,[0],[0]
"In our running example, at the start of search, we may choose to assign the first label l1 (person) to the variable y1 – the entity Colin – leading us to the successor {(1, 1)}.",2.2. Inference as Search,[0],[0]
Every search node specifies a partial or a full assignment to all the entities and relations.,2.2. Inference as Search,[0],[0]
"The goal test simply checks if we arrive at a full assignment, i.e., all the entity and relation candidates have been assigned a label.
",2.2. Inference as Search,[0],[0]
"Note that goal test does not test the quality of the node, it simply tests whether the search process is finished.",2.2. Inference as Search,[0],[0]
"The quality of the goal node is determined by the path cost from the initial node to the goal node, which is the accumulated cost of each step along the way.",2.2. Inference as Search,[0],[0]
The step cost for assigning label li to a variable yk is the same cki we defined for the ILP objective in Eq.,2.2. Inference as Search,[0],[0]
(3).,2.2. Inference as Search,[0],[0]
"Finding a shortest path in such a search space is equivalent to the original ILP problem
3http://www.gurobi.com
without the structural constraints in Eq.",2.2. Inference as Search,[0],[0]
(5).,2.2. Inference as Search,[0],[0]
The uniquelabel constraints in Eq.,2.2. Inference as Search,[0],[0]
"(4) are automatically satisfied by our formulation of the search process.
",2.2. Inference as Search,[0],[0]
"Indeed, solving inference without the constraints in Eq.(5) is trivial.",2.2. Inference as Search,[0],[0]
"For each categorical variable yk, we can pick the label li that has the lowest value of cki .",2.2. Inference as Search,[0],[0]
"This gives us two possible options for solving inference as search: We can (a) ignore the constraints that make inference slow to greedily predict all the labels, or, (b) enforce constraints at each step of the search, and only consider search nodes that satisfy all constraints.",2.2. Inference as Search,[0],[0]
"The first option is fast, but can give us outputs that are invalid.",2.2. Inference as Search,[0],[0]
"For example, we might get a structure that mandates that the person Colin lives in a person called Ordon Village.",2.2. Inference as Search,[0],[0]
"The second option will give us structurally valid outputs, but can be prohibitively slow.
",2.2. Inference as Search,[0],[0]
Various graph search algorithms can be used for performing inference.,2.2. Inference as Search,[0],[0]
"For efficiency, we can use beam search with a fixed beam width b.",2.2. Inference as Search,[0],[0]
When search begins the beam B0 contains only the initial node B0 =,2.2. Inference as Search,[0],[0]
[I].,2.2. Inference as Search,[0],[0]
"Following Collins & Roark (2004); Xu et al. (2009), we define the function BreadthExpand which takes the beam Bt at step t and generates the candidates Ct+1 for the next beam:
Ct+1 = BreadthExpand(Bt) = ∪v∈Bts(v)
The next beam is given by Bt+1 = Filter(Ct+1), where Filter takes top b nodes according to some priority function p(v).",2.2. Inference as Search,[0],[0]
"In the simplest case, the priority of a node v is the total path cost of reaching that node.",2.2. Inference as Search,[0],[0]
"More generally, the priority function can be informed not only by the path cost, but also by a heuristic function as in the popular A∗ algorithm.",2.2. Inference as Search,[0],[0]
"In the previous section, we saw that using a black-box ILP solver may be slower than greedy search which ignores constraints, but produces valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"However, over its lifetime, a trained classifier predicts structures for a large number of inputs.",3. Speeding up Structured Prediction,[0],[0]
"While the number of unique inputs (e.g. sentences) may be large, the number of unique structures that actually occur among the predictions is not only finite, but also small.",3. Speeding up Structured Prediction,[0],[0]
"This observation was exploited by Srikumar et al. (2012); Kundu et al. (2013) for amortizing inference costs.
",3. Speeding up Structured Prediction,[0],[0]
"In this paper, we are driven by the need for an inference algorithm that learns regularities across outputs to become faster at producing structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"In order to do so, we will develop an inference-as-search scheme that inherits the speed of greedy search, but learns to produce structurally valid outputs.",3. Speeding up Structured Prediction,[0],[0]
"Before developing the algorithmic aspects of such an inference scheme, let us first see a proofof-concept for such a scheme.",3. Speeding up Structured Prediction,[0],[0]
Our goal is to incorporate the structural constraints from Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(5) as a heuristic for greedy or beam search.,3.1. Heuristics for Structural Validity,[0],[0]
"To do so, at each step during search, we need to estimate how likely an assignment can lead to a constraint violation.",3.1. Heuristics for Structural Validity,[0],[0]
"This information can be characterized by using a heuristic function h(v), which will be used to evaluated a node v during search.
",3.1. Heuristics for Structural Validity,[0],[0]
The dual form the ILP in Eqs.,3.1. Heuristics for Structural Validity,[0],[0]
(3) to (5) help justify the idea of capturing constraint information using a heuristic function.,3.1. Heuristics for Structural Validity,[0],[0]
We treat the unique label constraints in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
"(4) as defining the domain in which each 0-1 variable zki lives, and the only real constraints are given by Eq. (5).
Let uj represent the dual variable for the jth constraint.",3.1. Heuristics for Structural Validity,[0],[0]
"Thus, we obtain the Lagrangian4
L(z, u) =",3.1. Heuristics for Structural Validity,[0],[0]
K∑,3.1. Heuristics for Structural Validity,[0],[0]
k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
cki z,3.1. Heuristics for Structural Validity,[0],[0]
k i,3.1. Heuristics for Structural Validity,[0],[0]
− m∑ j=1 uj (,3.1. Heuristics for Structural Validity,[0],[0]
K∑ k=1 n∑ i=1,3.1. Heuristics for Structural Validity,[0],[0]
Akjiz k i,3.1. Heuristics for Structural Validity,[0],[0]
"− bj )
",3.1. Heuristics for Structural Validity,[0],[0]
"= ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j ujA k ji  zki +∑ j bjuj
",3.1. Heuristics for Structural Validity,[0],[0]
"The dual function θ(u) = minz L(z, u), where the minimization is over the domain of the z variables.
",3.1. Heuristics for Structural Validity,[0],[0]
Denote u∗ = arg max θ(u) as the solution to the dual problem.,3.1. Heuristics for Structural Validity,[0],[0]
"In the case of zero duality gap, the theory of Lagrangian relaxation (Lemaréchal, 2001) tells us that solving the following relaxed minimization problem will solve the original ILP:
min ∑ k,i cki",3.1. Heuristics for Structural Validity,[0],[0]
"−∑ j u∗jA k ji  zki (6)∑ i zki = 1, for all k (7) zki ∈ {0, 1}, for all k, i (8)
This new optimization problem does not have any structural constraints and can be solved greedily for each k if we know the optimal dual variables u∗.
To formulate the minimization in Eqs (6) to (8) as a search problem, we define the priority function p(v) for ranking the nodes as p(v) = g(v) + h∗(v), where the path cost g(v) and heuristic function h∗(v) are given by
g(v) = ∑
(k,i)∈v
cki , (9)
h∗(v) =",3.1. Heuristics for Structural Validity,[0],[0]
"− ∑
(k,i)∈v ∑ j Akjiu ∗ j (x).",3.1. Heuristics for Structural Validity,[0],[0]
"(10)
Since Eq. (6) is a minimization problem, smaller priority value p(v) means higher ranking during search.",3.1. Heuristics for Structural Validity,[0],[0]
"Note that
4We omit the ranges of the summation indices",3.1. Heuristics for Structural Validity,[0],[0]
"i, j, k hereafter.
even though heuristic function defined in this way is not always admissible, greedy search with ranking function p(v) will lead to the exact solution of Eqs.",3.1. Heuristics for Structural Validity,[0],[0]
(6) to (8).,3.1. Heuristics for Structural Validity,[0],[0]
"In practice, however, we do not have the optimal values for the dual variables u∗.",3.1. Heuristics for Structural Validity,[0],[0]
"Indeed, when Lagrangian relaxation is used for inference, the optmial dual variables are computed using subgradient optimization for each example because their value depends on the original input via the c’s.
",3.1. Heuristics for Structural Validity,[0],[0]
"Instead of performing expensive gradient based optimization for every input instance, we will approximate the heuristic function as a classifier that learns to prioritize structurally valid outputs.",3.1. Heuristics for Structural Validity,[0],[0]
"In this paper, we use a linear model based on a weight vector w to approximate the heuristic as
h(v) = −w · φ(v) (11)
",3.1. Heuristics for Structural Validity,[0],[0]
"For an appropriate choice of node features φ(v), the heuristic h(v) in Eq.(10) is indeed a linear function.5",3.1. Heuristics for Structural Validity,[0],[0]
"In other words, there exists a linear heuristic function that can guide graph search towards creating structurally valid outputs.
",3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the priority function p(v) for each node is determined by two components: the path cost g(v) from the initial node to the current node, and the learned heuristic cost h(v), which is an estimate of how good the current node is.",3.1. Heuristics for Structural Validity,[0],[0]
"Because the purpose of the heuristic is to help improve inference speed, we call φ(v) speedup features.",3.1. Heuristics for Structural Validity,[0],[0]
The speedup features can be different from the original model features in Eq.,3.1. Heuristics for Structural Validity,[0],[0]
(2).,3.1. Heuristics for Structural Validity,[0],[0]
In particular it can includes features for partial assignments made so far which were not available in the original model features.,3.1. Heuristics for Structural Validity,[0],[0]
"In this setting, the goal of speedup learning is to find suitable weight vector w over the black-box classifier’s lifetime.",3.1. Heuristics for Structural Validity,[0],[0]
"In this section, we will describe a mistake-bound algorithm to learn the weight vector w of the speedup classifier.",4. Learning the Speedup Classifier,[0],[0]
"The design of this algorithm is influenced by learning to search algorithms such as LaSO (Daumé III & Marcu, 2005; Xu et al., 2009).",4. Learning the Speedup Classifier,[0],[0]
"We assume that we have access to a trained black-box ILP solver called Solve, which can solve the structured prediction problems, and we have a large set of examples {xi}Ni=1.",4. Learning the Speedup Classifier,[0],[0]
Our goal is to use this set to train a speedup classifier to mimic the ILP solver while predicting structures for this set of examples.,4. Learning the Speedup Classifier,[0],[0]
"Subsequently, we can use the less expensive speedup influenced search procedure to replace the ILP solver.
",4. Learning the Speedup Classifier,[0],[0]
"To define the algorithm, we will need additional terminology.",4. Learning the Speedup Classifier,[0],[0]
"Given a reference solution y, we define a node v to be ygood, if it can possibly lead to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"If a node v is y-good, then the already assigned variables have the same labels as in the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"We define a
5See supplementary material for an elaboration.
",4. Learning the Speedup Classifier,[0],[0]
"Algorithm 1 Learning a speedup classifier using examples {xi}Ni=1, and a black-box Solver Solve.
1: Initialize the speedup weight vector w← 0 2: for epoch = 1 . .",4. Learning the Speedup Classifier,[0],[0]
.M,4. Learning the Speedup Classifier,[0],[0]
do 3: for i = 1 . . .,4. Learning the Speedup Classifier,[0],[0]
N,4. Learning the Speedup Classifier,[0],[0]
do 4: y← Solve(xi) 5: Initialize the beam B ←,4. Learning the Speedup Classifier,[0],[0]
"[I] 6: while B is y-good and v̂ is not goal do 7: B ← Filter(BreadthExpand(B)) 8: end while 9: if B is not y-good then
10: v∗ ← SetGood(v̂) 11: w←",4. Learning the Speedup Classifier,[0],[0]
w + φ(v∗)− 1|B| ∑ v∈B φ(v) 12: else if v̂ is not y-good then 13: v∗ ← SetGood(v̂) 14: w←,4. Learning the Speedup Classifier,[0],[0]
"w + φ(v∗)− φ(v̂) 15: end if 16: end for 17: end for
beam B is y",4. Learning the Speedup Classifier,[0],[0]
-good if it contains at least one y-good node to represent the notion that search is still viable.,4. Learning the Speedup Classifier,[0],[0]
"We denote the first element (the highest ranked) in a beam by v̂. Finally, we define an operator SetGood, which takes a node that is not y-good, and return its corresponding y-good node by fixing the incorrect assignments according to the reference solution.",4. Learning the Speedup Classifier,[0],[0]
"The unassigned variables are still left unassigned by the SetGood operator.
",4. Learning the Speedup Classifier,[0],[0]
The speedup-learning algorithm is listed as Algorithm 1.,4. Learning the Speedup Classifier,[0],[0]
It begins by initializing the weight w to the zero vector.,4. Learning the Speedup Classifier,[0],[0]
We iterate over the examples for M epochs.,4. Learning the Speedup Classifier,[0],[0]
"For each example xi, we first solve inference using the ILP solver to obtain the reference structure y (line 4).",4. Learning the Speedup Classifier,[0],[0]
Next a breadth-expand search is performed (lines 5-8).,4. Learning the Speedup Classifier,[0],[0]
"Every time the beam B is updated, we check if the beam contains at least one ygood node that can possibly lead to the reference solution y. Search terminates if the beam is not y-good, or if the highest ranking node v̂ is a goal.",4. Learning the Speedup Classifier,[0],[0]
"If the beam is not y-good, we compute the corresponding y-good node v∗ from v̂, and perform a perceptron style update to the speedup weights (line 9-11).",4. Learning the Speedup Classifier,[0],[0]
"In other words, we update the weight vector by adding feature vector of φ(v∗), and subtracting the average feature vector of all the nodes in the beam.",4. Learning the Speedup Classifier,[0],[0]
Otherwise v̂ must be a goal node.,4. Learning the Speedup Classifier,[0],[0]
We then check if v̂ agrees with the reference solution (lines 12-15).,4. Learning the Speedup Classifier,[0],[0]
"If not, we perform a similar weight update, by adding the feature vector of φ(v∗), and subtracting φ(v̂).
",4. Learning the Speedup Classifier,[0],[0]
"Mistake bound Next, we show that the Algorithm 1 has a mistake bound.",4. Learning the Speedup Classifier,[0],[0]
"Let Rφ be a positive constant such that for every pair of nodes (v, v′), we have ‖φ(v)− φ(v′)‖ ≤ Rφ.",4. Learning the Speedup Classifier,[0],[0]
"Let Rg be a positive constant such that for every pair of
search nodes (v, v′), we have |g(v)− g(v′)| ≤",4. Learning the Speedup Classifier,[0],[0]
Rg .,4. Learning the Speedup Classifier,[0],[0]
"Finally we define the level margin of a weight vector w for a training set as
γ = min",4. Learning the Speedup Classifier,[0],[0]
"{(v,v′)}
",4. Learning the Speedup Classifier,[0],[0]
"w · ( φ(v)− φ(v′) ) (12)
",4. Learning the Speedup Classifier,[0],[0]
"Here, the set {(v, v′)} contains any pair such that v is ygood, v′ is not y-good, and v and v′ are at the same search level.",4. Learning the Speedup Classifier,[0],[0]
"The level margin denotes the minimum score gap between a y-good and a y-bad node at the same search level.
",4. Learning the Speedup Classifier,[0],[0]
The priority function used to rank the search nodes is defined as pw(v) = g(v)−w,4. Learning the Speedup Classifier,[0],[0]
·φ(v).,4. Learning the Speedup Classifier,[0],[0]
Smaller priority function value ranks higher during search.,4. Learning the Speedup Classifier,[0],[0]
With these definitions we have the following theorem: Theorem 1 (Speedup mistake bound).,4. Learning the Speedup Classifier,[0],[0]
"Given a training set such that there exists a weight vector w with level margin γ > 0 and ‖w‖ = 1, the speedup learning algorithm (Algorithm 1) will converge with a consistent weight vector after making no more than R2φ+2Rg
γ2 weight updates.
",4. Learning the Speedup Classifier,[0],[0]
Proof.,4. Learning the Speedup Classifier,[0],[0]
The complete proof is in the supplementary material of the paper.,4. Learning the Speedup Classifier,[0],[0]
"So far, we have shown that a structured prediction problem can be converted to a beam search problem.",4.1. Avoiding Computing the Input Features,[0],[0]
The priority function for ranking search nodes is determined by p(v) = g(v) + h(v).,4.1. Avoiding Computing the Input Features,[0],[0]
We have seen how the h function be trained to enforce structural constraints.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, there are other opportunities for speeding up as well.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Computing the path cost g(v) involves calculating the corresponding ILP coefficients, which in turn requires feature extraction using the original trained model.",4.1. Avoiding Computing the Input Features,[0],[0]
"This is usually a time-consuming step (Srikumar, 2017), thus motivating the question of whether we can avoid calculating them without losing accuracy.",4.1. Avoiding Computing the Input Features,[0],[0]
"If a search node is strongly preferred by the heuristic function, the path cost is unlikely to reverse the heuristic function’s decision.",4.1. Avoiding Computing the Input Features,[0],[0]
"In this case, we can rank the candidate search nodes with heuristic function only.
",4.1. Avoiding Computing the Input Features,[0],[0]
"Formally, given a fixed beam size b and the beam candidates Ct at step t from which we need to select the beam Bt, we can rank the nodes in Ct from smallest to largest according to the heuristic function value h(v).",4.1. Avoiding Computing the Input Features,[0],[0]
"Denote the bth smallest node as vb and the (b+1)th smallest node as vb+1, we define the heuristic gap ∆t as
∆t = h(vb+1)− h(vb).",4.1. Avoiding Computing the Input Features,[0],[0]
"(13)
If the beam Bt is selected from Ct only according to heuristic function, then ∆t is the gap between the last node in the beam and the first node outside the beam.",4.1. Avoiding Computing the Input Features,[0],[0]
"Next we define the path-cost gap δt as
δt = max v,v′∈Ct
(v − v′) (14)
",4.1. Avoiding Computing the Input Features,[0],[0]
With these definitions we immediately have the following theorem: Theorem 2.,4.1. Avoiding Computing the Input Features,[0],[0]
"Given the beam candidates Ct with heuristic gap ∆t and path-cost gap δt, if ∆t > δt, then using only heuristic function to select the beam Bt will have the same set of nodes selected as using the full priority function up to their ordering in the beam.
",4.1. Avoiding Computing the Input Features,[0],[0]
"If the condition of Theorem 2 holds, then we can rank the candidates using only heuristic function without calculating the path cost.",4.1. Avoiding Computing the Input Features,[0],[0]
This will further save computation time.,4.1. Avoiding Computing the Input Features,[0],[0]
"However, without actually calculating the path cost there is no way to determine the path-cost gap δt at each step.",4.1. Avoiding Computing the Input Features,[0],[0]
"In practice we can treat δt as an empirical parameter θ and define the following priority function
pθ(v) = { h(v), if ∆t > θ, g(v) + h(v), otherwise.
",4.1. Avoiding Computing the Input Features,[0],[0]
(15),4.1. Avoiding Computing the Input Features,[0],[0]
We empirically evaluate the speedup based inference scheme described in Section 4 on the problem of predicting entities and relations (i.e. our running example).,5. Experiments,[0],[0]
"In this task, we are asked to label each entity, and the relation between each pair of the entities.",5. Experiments,[0],[0]
"We assume the entity candidates are given, either from human annotators or from a preprocessing step.",5. Experiments,[0],[0]
"The goal of inference is to determine the types of the entity spans, and the relations between them, as opposed to identify entity candidates.",5. Experiments,[0],[0]
"The research questions we seek to resolve empirically are:
1.",5. Experiments,[0],[0]
Does using a learned speedup heuristic recover structurally valid outputs without paying the inference cost of the integer linear program solver?,5. Experiments,[0],[0]
2.,5. Experiments,[0],[0]
"Can we construct accurate outputs without always computing input features and using only the learned heuristic to guide search?
",5. Experiments,[0],[0]
The dataset we used is from the previous work by Roth & Yih (2004).,5. Experiments,[0],[0]
It contains 1441 sentences.,5. Experiments,[0],[0]
"Each sentence contains several entities with labels, and the labeled relations between every pair of entity.",5. Experiments,[0],[0]
"There are three types of entities, person, location and organization, and five types of relations, Kill, LiveIn, WorkFor, LocatedAt and OrgBasedIn.",5. Experiments,[0],[0]
"There are two constraints associated with each relation type, specifying the allowed source and target arguments.",5. Experiments,[0],[0]
"For example, if the relation label is LiveIn, the source entity must be person and the target entity must be location.",5. Experiments,[0],[0]
"There is also another kind of constraint which says for every pair of entities, they can not have a relation label in both directions between them, i.e., one of the direction must be labeled as NoRel.
",5. Experiments,[0],[0]
"We re-implemented the model from the original work using the same set of features as for the entity and relation scoring
functions.",5. Experiments,[0],[0]
"We used 70% of the labeled data to train an ILPbased inference scheme, which will become our black-box solver for learning the speedup classifier.",5. Experiments,[0],[0]
"The remaining 30% labeled data are held out for evaluations.
",5. Experiments,[0],[0]
"We use 29950 sentences from the Gigaword corpus (Graff et al., 2003) to train the speedup classifier.",5. Experiments,[0],[0]
"The entity candidates are extracted using the Stanford Named Entity Recognizer (Manning et al., 2014).",5. Experiments,[0],[0]
"We ignore the entity labels, however, since our task requires determining the type of the entities and relations.",5. Experiments,[0],[0]
"The features we use for the speedup classifiers are counts of the pairs of labels of the form (source label, relation label), (relation label, target label), and counts of the triples of labels of the form (source label, relation label, target label).",5. Experiments,[0],[0]
"We run Algorithm 1 over this unlabeled dataset, and evaluate the resulting speedup classifier on the held out test set.",5. Experiments,[0],[0]
"In all of our speedup search implementations, we first assign labels to the entities from left to right, then the relations among them.
",5. Experiments,[0],[0]
We evaluate the learned speedup classifier in terms of both accuracy and speed.,5. Experiments,[0],[0]
"The accuracy of the speedup classifier can be evaluated using three kinds of metrics: F-1 scores against gold labels, F-1 scores against the ILP solver’s prediction, and the validity ratio, which is the percentage of the predicted examples agreeing with all constraints.6",5. Experiments,[0],[0]
Our first set of experiments evaluates the impact of Algorithm 1.,5.1. Evaluation of Algorithm 1,[0],[0]
These results are shown in Table 1.,5.1. Evaluation of Algorithm 1,[0],[0]
We see the ILP solver achieves perfect entity and relation F-1 when compared with ILP model itself.,5.1. Evaluation of Algorithm 1,[0],[0]
It guarantees all constraints are satisfied.,5.1. Evaluation of Algorithm 1,[0],[0]
Its accuracy against gold label and its prediction time becomes the baselines of our speedup classifiers.,5.1. Evaluation of Algorithm 1,[0],[0]
We also provide two search baselines.,5.1. Evaluation of Algorithm 1,[0],[0]
The first search baseline just uses greedy search without any constraint considerations.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this setting each label is assigned independently, since the step cost of assigning a label to an entity or a relation variable depends only on the corresponding coefficients in the ILP objectives.",5.1. Evaluation of Algorithm 1,[0],[0]
"In this case, a structured prediction problem becomes several independent multi-class classification problems.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is faster than ILP but the validity ratio is rather low (0.29).,5.1. Evaluation of Algorithm 1,[0],[0]
The second search baseline is greedy search with constraint satisfaction.,5.1. Evaluation of Algorithm 1,[0],[0]
The constraints are guaranteed to be satisfied by using the standard arc-consistency search.,5.1. Evaluation of Algorithm 1,[0],[0]
"The prediction takes much longer than the ILP solver (844 ms vs. 239 ms.).
",5.1. Evaluation of Algorithm 1,[0],[0]
We trained a speedup classifier with two different beam sizes.,5.1. Evaluation of Algorithm 1,[0],[0]
"Even with beam width b = 1, we are able to obtain > 95% validity ratio, and the prediction time is much faster
6All our experiments were conducted on a server with eight Intel i7 3.40 GHz cores and 16G memory.",5.1. Evaluation of Algorithm 1,[0],[0]
"We disabled multithreaded execution in all cases for a fair comparison.
than the ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"Furthermore, we see that the F-1 score evaluated against gold labels is only slightly worse than ILP model.",5.1. Evaluation of Algorithm 1,[0],[0]
"With beam width b = 2, we recover the ILP model accuracy when evaluated against gold labels.",5.1. Evaluation of Algorithm 1,[0],[0]
The prediction time is still much less than the ILP solver.,5.1. Evaluation of Algorithm 1,[0],[0]
"In this section, we empirically verify the idea that we do not always need to compute the path cost, if the heuristic gap ∆t is large.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
We use the evaluation function pθ(v) in Eq.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
(15) with different values of θ to rank the search nodes.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"The results are given in Table 2.
",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"For both beam widths, θ = 0 is the case in which the original model is completely ignored.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
All the nodes are ranked using the speedup heuristic function only.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
"Even though it has perfect validity ratio, the result is rather poor when evaluated on F-1 scores.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
"When θ increases, the entity and relation F-1 scores quickly jump up, essentially getting back the same accuracy as the speedup classifiers in Table 1.",5.2. Experiments on Ignoring the Model Cost,[0],[0]
But the prediction time is lowered compared to the results from Table 1.,5.2. Experiments on Ignoring the Model Cost,[0],[0]
The idea of learning memo functions to make computation more efficient goes back to Michie (1968).,6. Discussion and Related Work,[0],[0]
"Speedup learning has been studied since the eighties in the context of general problem solving, where the goal is to learn a problem solver that becomes faster as opposed to becoming more accurate as it sees more data.",6. Discussion and Related Work,[0],[0]
Fern (2011) gives a broad survey of this area.,6. Discussion and Related Work,[0],[0]
"In this paper, we presented a variant of this idea that is more concretely applied to structured output prediction.
",6. Discussion and Related Work,[0],[0]
Efficient inference is a central topic in structured prediction.,6. Discussion and Related Work,[0],[0]
"In order to achieve efficiency, various strategies are adopted in the literature.",6. Discussion and Related Work,[0],[0]
Search based strategies are commonly used for this purpose and several variants abound.,6. Discussion and Related Work,[0],[0]
"The idea of framing a structured prediction problem as a search problem has been explored by several previous works (Collins & Roark, 2004; Daumé III & Marcu, 2005; Daumé III et al., 2009; Huang et al., 2012; Doppa et al., 2014).",6. Discussion and Related Work,[0],[0]
"It usually admits incorporating arbitrary features more easily than fully global structured prediction models like conditional random fields (Lafferty et al., 2001), structured perceptron (Collins, 2002), and structured support vector machines (Taskar et al., 2003; Tsochantaridis et al., 2004).",6. Discussion and Related Work,[0],[0]
"In such cases too, inference can be solved approximately using heuristic search.",6. Discussion and Related Work,[0],[0]
"Either a fixed beam size (Xu et al., 2009), or a dynamicallysized beam (Bodenstab et al., 2011) can be used.",6. Discussion and Related Work,[0],[0]
In our work we fix the beam size.,6. Discussion and Related Work,[0],[0]
The key difference from previous work is that our ranking function combines information from the trained model with the heuristic function which characterizes constraint information.,6. Discussion and Related Work,[0],[0]
"Closely related to the
work described in this paper are approaches that learn to prune the search space (He et al., 2014; Vieira & Eisner, 2016) and learn to select features (He et al., 2013).
",6. Discussion and Related Work,[0],[0]
Another line of recent related work focuses on discovering problem level regularities across the inference space.,6. Discussion and Related Work,[0],[0]
"These amortized inference schemes are designed using deterministic rules for discovering when a new inference problem can re-use previously computed solutions (Srikumar et al., 2012; Kundu et al., 2013) or in the context of a Bayesian network by learning a stochastic inverse network that generates outputs (Stuhlmüller et al., 2013).
",6. Discussion and Related Work,[0],[0]
"Our work is also related to the idea of imitation learning (Daumé III et al., 2009; Ross et al., 2011; Ross & Bagnell, 2014; Chang et al., 2015).",6. Discussion and Related Work,[0],[0]
"In this setting, we are given a reference policy, which may or may not be a good policy.",6. Discussion and Related Work,[0],[0]
"The goal of learning is to learn another policy to imitate the given policy, or even learn a better one.",6. Discussion and Related Work,[0],[0]
Learning usually proceeds in an online fashion.,6. Discussion and Related Work,[0],[0]
"However, imitation learning requires learning a new policy which is independent of the given reference policy, since during test time the reference policy is no longer available.",6. Discussion and Related Work,[0],[0]
"In our case, we can think of the black-box solver as a reference policy.",6. Discussion and Related Work,[0],[0]
"During prediction we always have this solver at our disposal, what we want is avoiding unnecessary calls to the solver.",6. Discussion and Related Work,[0],[0]
"Following recent successes in imitation learning, we expect that we can replace the linear heuristic function with a deep network to avoid feature design.
",6. Discussion and Related Work,[0],[0]
"Also related is the idea of knowledge distillation (Bucilă et al., 2006; Hinton et al., 2015; Kim & Rush, 2016), that seeks to train a student classifier (usually a neural network) to compress and mimic a larger teacher network, thus improve prediction speed.",6. Discussion and Related Work,[0],[0]
The primary difference with the speedup idea of this paper is that our goal is to be more efficient at constructing internally self-consistent structures without explicitly searching over the combinatorially large output space with complex constraints.,6. Discussion and Related Work,[0],[0]
"In this paper, we asked whether we can learn to make inference faster over the lifetime of a structured output classifier.",7. Conclusions,[0],[0]
"To address this question, we developed a search-based strategy that learns to mimic a black-box inference engine but is substantially faster.",7. Conclusions,[0],[0]
We further extended this strategy by identifying cases where the learned search algorithm can avoid expensive input feature extraction to further improve speed without losing accuracy.,7. Conclusions,[0],[0]
We empirically evaluated our proposed algorithms on the problem of extracting entities and relations from text.,7. Conclusions,[0],[0]
"Despite using an object-heavy JVM-based implementation of search, we showed that by exploiting regularities across the output space, we can outperform the industrial strength Gurobi integer linear program solver in terms of speed, while matching its accuracy.
",7. Conclusions,[0],[0]
Acknowledgments We thank the Utah NLP group members and the anonymous reviewers for their valuable feedback.,7. Conclusions,[0],[0]
Predicting structured outputs can be computationally onerous due to the combinatorially large output spaces.,abstractText,[0],[0]
"In this paper, we focus on reducing the prediction time of a trained black-box structured classifier without losing accuracy.",abstractText,[0],[0]
"To do so, we train a speedup classifier that learns to mimic a black-box classifier under the learning-to-search approach.",abstractText,[0],[0]
"As the structured classifier predicts more examples, the speedup classifier will operate as a learned heuristic to guide search to favorable regions of the output space.",abstractText,[0],[0]
We present a mistake bound for the speedup classifier and identify inference situations where it can independently make correct judgments without input features.,abstractText,[0],[0]
We evaluate our method on the task of entity and relation extraction and show that the speedup classifier outperforms even greedy search in terms of speed without loss of accuracy.,abstractText,[0],[0]
Learning to Speed Up Structured Output Prediction,title,[0],[0]
"In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated. However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics. In these settings, parametric ODE model cannot be formulated. Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge. We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism. We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.",text,[0],[0]
Dynamical systems modelling is a cornerstone of experimental sciences.,1. Introduction,[0],[0]
"In biology, as well as in physics and chemistry, modelers attempt to capture the dynamical behavior of a given system or a phenomenon in order to improve its understanding and make predictions about its future state.",1. Introduction,[0],[0]
Systems of coupled ordinary differential equations (ODEs) are undoubtedly the most widely used models in science.,1. Introduction,[0],[0]
"Even simple ODE functions can describe complex dynamical behaviours (Hirsch et al., 2004).",1. Introduction,[0],[0]
"Typically, the dynamics are firmly grounded in physics with only a few parameters to be estimated from data.",1. Introduction,[0],[0]
"However, equally ubiquitous are the cases where the governing dynamics are partially or completely unknown.
",1. Introduction,[0],[0]
"We consider the dynamics of a system governed by multi-
*Equal contribution 1Aalto University, Finland 2Helsinki Institute of Information Technology HIIT, Finland.",1. Introduction,[0],[0]
Correspondence to: Markus Heinonen,1. Introduction,[0],[0]
<,1. Introduction,[0],[0]
"markus.o.heinonen@aalto.fi>.
",1. Introduction,[0],[0]
"Proceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018.",1. Introduction,[0],[0]
"Copyright 2018 by the author(s).
",1. Introduction,[0],[0]
"variate ordinary differential functions:
ẋ(t) = dx(t)
dt = f(x(t)) (1)
where x(t) ∈ X",1. Introduction,[0],[0]
"= RD is the state vector of a Ddimensional dynamical system at time t, and the ẋ(t) ∈",1. Introduction,[0],[0]
Ẋ =,1. Introduction,[0],[0]
"RD is the first order time derivative of x(t) that drives the state x(t) forward, and where f :",1. Introduction,[0],[0]
RD → RD is the vector-valued derivative function.,1. Introduction,[0],[0]
"The ODE solution is determined by
x(t) = x0 + ∫",1. Introduction,[0],[0]
t 0,1. Introduction,[0],[0]
"f(x(τ))dτ, (2)
where we integrate the system state from an initial state x(0) = x0 for time t forward.",1. Introduction,[0],[0]
We assume that f(·) is completely unknown and we only observe one or several multivariate time series Y =,1. Introduction,[0],[0]
"(y1, . . .",1. Introduction,[0],[0]
",yN )",1. Introduction,[0],[0]
"T ∈ RN×D obtained from an additive noisy observation model at observation time points T = (t1, . . .",1. Introduction,[0],[0]
", tN ) ∈ RN ,
y(t)",1. Introduction,[0],[0]
= x(t) +,1. Introduction,[0],[0]
"εt, (3)
where εt ∼ N (0,Ω) follows a stationary zero-mean multivariate Gaussian distribution with diagonal noise variances Ω = diag(ω21 , . . .",1. Introduction,[0],[0]
", ω 2 D).",1. Introduction,[0],[0]
The observation time points do not need to be equally spaced.,1. Introduction,[0],[0]
"Our task is to learn the differential function f(·) given observations Y , with no prior knowledge of the ODE system.
",1. Introduction,[0],[0]
"There is a vast literature on conventional ODEs (Butcher, 2016) where a parametric form for function f(x;θ, t) is assumed to be known, and its parameters θ are subsequently optimised with least squares or Bayesian approach, where the expensive forward solution xθ(ti)",1. Introduction,[0],[0]
"=∫ ti 0
f(x(τ);θ, t)dτ is required to evaluate the system responses xθ(ti) from parameters θ against observations y(ti).",1. Introduction,[0],[0]
"To overcome the computationally intensive forward solution, a family of methods denoted as gradient matching (Varah, 1982; Ellner et al., 2002; Ramsay et al., 2007) have proposed to replace the forward solution by matching f(yi)",1. Introduction,[0],[0]
"≈ ẏi to empirical gradients ẏi of the data instead, which do not require the costly integration step.",1. Introduction,[0],[0]
"Recently several authors have proposed embedding a parametric differential function within a Bayesian or Gaussian process (GP) framework (Graepel, 2003; Calderhead et al., 2008;
Dondelinger et al., 2013; Wang and Barber, 2014; Macdonald, 2017) (see Macdonald et al. (2015) for a review).",1. Introduction,[0],[0]
"GPs have been successfully applied to model linear differential equations as they are analytically tractable (Gao et al., 2008; Raissi et al., 2017).
",1. Introduction,[0],[0]
"However, conventional ODE modelling can only proceed if a parametric form of the driving function f(·) is known.",1. Introduction,[0],[0]
"Recently, initial work to handle unknown or non-parametric ODE models have been proposed, although with various limiting approximations.",1. Introduction,[0],[0]
"Early works include spline-based smoothing and additive functions ∑D j fj(xj) to infer gene regulatory networks (De Hoon et al., 2002; Henderson and Michailidis, 2014).",1. Introduction,[0],[0]
"Äijö and Lähdesmäki (2009) proposed estimating the unknown nonlinear function with GPs using either finite time differences, or analytically solving the derivative function as a function of only time, ẋ(t) = f(t) (Äijö et al., 2013).",1. Introduction,[0],[0]
"In a seminal technical report of Heinonen and d’Alche Buc (2014) a full vector-valued kernel model f(x) was proposed, however using a gradient matching approximation.",1. Introduction,[0],[0]
"To our knowledge, there exists no model that can learn non-linear ODE functions ẋ(t) = f(x(t)) over the state x against the true forward solutions x(ti).
",1. Introduction,[0],[0]
"In this work we propose NPODE1: the first ODE model for learning arbitrary, and a priori completely unknown nonparametric, non-linear differential functions f : X → Ẋ from data in a Bayesian way.",1. Introduction,[0],[0]
"We do not use gradient matching or other approximative models, but instead propose to directly optimise the exact ODE system with the fully forward simulated responses against data.",1. Introduction,[0],[0]
"We parameterise our model as an augmented Gaussian process vector field with inducing points, while we propose sensitivity equations to efficiently compute the gradients of the system.",1. Introduction,[0],[0]
"Our model can forecast continuous-time systems arbitrary amounts to future, and we demonstrate the state-of-the-art performance in human motion datasets.",1. Introduction,[0],[0]
"The differential function f(x) to be learned defines a vector field2 f , that is, an assignment of a gradient vector f(x) ∈ RD to every state x ∈ RD.",2. Nonparametric ODE Model,[0],[0]
"We model the vector field as a vector-valued Gaussian process (Rasmussen and Williams, 2006)
f(x) ∼ GP(0,K(x,x′)), (4)
which defines a priori distribution over function values f(x) whose mean and covariances are
E[f(x)]",2. Nonparametric ODE Model,[0],[0]
"= 0 (5) cov[f(x), f(x′)] = K(x,x′), (6)
1The implementation is publicly available in http://www. github.com/cagatayyildiz/npode
2We use vector field and differential function interchangeably.
and where the kernel K(x,x′) ∈ RD×D is matrixvalued.",2. Nonparametric ODE Model,[0],[0]
A GP prior defines that for any collection of states X =,2. Nonparametric ODE Model,[0],[0]
"(x1, . . .",2. Nonparametric ODE Model,[0],[0]
",xN )",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D, the function values F = (f(x1), . . .",2. Nonparametric ODE Model,[0],[0]
", f(xN ))",2. Nonparametric ODE Model,[0],[0]
"T ∈ RN×D follow a matrixvalued normal distribution,
p(F ) = N (vec(F )|0,K(X,X)), (7)
where K(X,X) =",2. Nonparametric ODE Model,[0],[0]
"(K(xi,xj))Ni,j=1 ∈",2. Nonparametric ODE Model,[0],[0]
"RND×ND is a block matrix of matrix-valued kernels K(xi,xj).",2. Nonparametric ODE Model,[0],[0]
"The key property of Gaussian processes is that they encode functions where similar states x,x′ induce similar differentials f(x), f(x′), and where the state similarity is defined by the kernel K(x,x′).
",2. Nonparametric ODE Model,[0],[0]
"In standard GP regression we would obtain the posterior of the vector field by conditioning the GP prior with the data (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
In ODE models the conditional f(x)|Y of a vector field is intractable due to the integral mapping (2) between observed states y(ti) and differentials f(x).,2. Nonparametric ODE Model,[0],[0]
"Instead, we resort to augmenting the Gaussian process with a set of M inducing points z ∈ X and u ∈",2. Nonparametric ODE Model,[0],[0]
"Ẋ , such that f(z) = u (Quiñonero-Candela and
Rasmussen, 2005).",2. Nonparametric ODE Model,[0],[0]
"We choose to interpolate the differential function between the inducing points as (See Figure 1)
f(x) , Kθ(x, Z)Kθ(Z,Z) −1vec(U), (8)
which supports the function f(x) with inducing locations Z = (z1, . . .",2. Nonparametric ODE Model,[0],[0]
", zM ), inducing vectors U = (u1, . . .",2. Nonparametric ODE Model,[0],[0]
",uM ), and θ are the kernel parameters.",2. Nonparametric ODE Model,[0],[0]
"The function above corresponds to a vector-valued kernel function (Alvarez et al., 2012), or to a multi-task Gaussian process conditional mean without the variance term (Rasmussen and Williams, 2006).",2. Nonparametric ODE Model,[0],[0]
This definition is then compatible with the deterministic nature of the ODE formalism.,2. Nonparametric ODE Model,[0],[0]
"Due to universality of several kernels and kernel functions (Shawe-Taylor and Cristianini, 2004), we can represent arbitrary vector fields with appropriate inducing point and kernel choices.",2. Nonparametric ODE Model,[0],[0]
"The vector-valued kernel function (8) uses operator-valued kernels, which result in matrix-valued kernels Kθ(z, z′) ∈ RD×D for real valued states x, z, while the kernel matrix over data points becomes Kθ = (K(zi, zj))Mi,j=1 ∈ RMD×MD (See Alvarez et al. (2012) for a review).",2.1. Operator-valued Kernels,[0],[0]
"Most straightforward operator-valued kernel is the identity decomposable kernel Kdec(z, z′) = k(z, z′) ·",2.1. Operator-valued Kernels,[0],[0]
"ID, where the scalar Gaussian kernel
Kθ(z, z ′) =",2.1. Operator-valued Kernels,[0],[0]
σ2f exp −1 2 D∑ j=1 (zj − z′j)2,2.1. Operator-valued Kernels,[0],[0]
"`2j  (9) with differential variance σ2f and dimension-specific lengthscales ` = (`1, . . .",2.1. Operator-valued Kernels,[0],[0]
", `D) are expanded into a diagonal matrix of size D × D. We collect the kernel parameters as θ = (σf , `).
",2.1. Operator-valued Kernels,[0],[0]
We note that more complex kernels can also be considered given prior information of the underlying system characteristics.,2.1. Operator-valued Kernels,[0],[0]
"The divergence-free matrix-valued kernel induces vector fields that have zero divergence (Wahlström et al., 2013; Solin et al., 2015).",2.1. Operator-valued Kernels,[0],[0]
"Intuitively, these vector fields do not have sinks or sources, and every state always finally returns to itself after sufficient amount of time.",2.1. Operator-valued Kernels,[0],[0]
"Similarly, curl-free kernels induce curl-free vector fields that can contain sources or sinks, that is, trajectories can accelerate or decelerate.",2.1. Operator-valued Kernels,[0],[0]
"For theoretical treatment of vector field kernels, see (Narcowich and Ward, 1994; Bhatia et al., 2013; Fuselier and Wright, 2017).",2.1. Operator-valued Kernels,[0],[0]
"Non-stationary vector fields can be modeled with input-dependent lengthscales (Heinonen et al., 2016), while spectral kernels can represent stationary (Wilson et al., 2013) or non-stationary (Remes et al., 2017) recurring patterns in the differential function.",2.1. Operator-valued Kernels,[0],[0]
"We assume a Gaussian likelihood over the observations yi and the corresponding simulated responses x(ti) of Equation (2),
p(Y |x0, U, Z,ω) = N∏ i=1 N",2.2. Joint Model,[0],[0]
"(yi|x(ti),Ω), (10)
where x(ti) are forward simulated responses using the integral Equation (2) and differential Equation (8), and Ω = diag(ω21 . .",2.2. Joint Model,[0],[0]
.,2.2. Joint Model,[0],[0]
", ω 2 D) collects the dimension-specific noise variances.
",2.2. Joint Model,[0],[0]
"The inducing vectors have a Gaussian process prior
p(U |Z,θ) = N",2.2. Joint Model,[0],[0]
"(vec(U)|0,Kθ(Z,Z)).",2.2. Joint Model,[0],[0]
"(11)
",2.2. Joint Model,[0],[0]
"The model posterior is then
p(U,x0,θ,ω|Y ) ∝",2.2. Joint Model,[0],[0]
"p(Y |x0, U,ω)p(U |θ) = L, (12)
where we have for brevity omitted the dependency on the locations of the inducing points Z and also the parameter hyperpriors p(θ) and p(ω) since we assume them to be uniform, unless there is specific domain knowledge of the priors.
",2.2. Joint Model,[0],[0]
"The model parameters are the initial state x03, the inducing vectors U , the noise standard deviations ω = (ω1, . . .",2.2. Joint Model,[0],[0]
", ωD), and the kernel hyperparameters θ = (σf , `1, . . .",2.2. Joint Model,[0],[0]
", `D).",2.2. Joint Model,[0],[0]
"We apply a latent parameterisation using Cholesky decomposition LθLTθ = Kθ(Z,Z), which maps the inducing vectors to whitened domain (Kuss and Rasmussen, 2005)
",2.3. Noncentral Parameterisation,[0],[0]
"U = LθŨ , Ũ = L −1 θ U. (13)
",2.3. Noncentral Parameterisation,[0],[0]
The latent variables Ũ are projected on the kernel manifold Lθ to obtain the inducing vectors U .,2.3. Noncentral Parameterisation,[0],[0]
"This non-centered parameterisation (NCP) transforms the hierarchical posterior L of Equation (12) into a reparameterised form
p(x0, Ũ ,θ,ω|Y ) ∝",2.3. Noncentral Parameterisation,[0],[0]
"p(Y |x0, Ũ ,ω,θ)p(Ũ), (14)
where all variables to be optimised are decoupled, with the latent inducing vectors having a standard normal prior Ũ ∼ N (0, I).",2.3. Noncentral Parameterisation,[0],[0]
Optimizing Ũ and θ is now more efficient since they have independent contributions to the vector field via U = LθŨ .,2.3. Noncentral Parameterisation,[0],[0]
"The gradients of the whitened posterior can be retrieved analytically as (Heinonen et al., 2016)
",2.3. Noncentral Parameterisation,[0],[0]
∇Ũ logL = L T,2.3. Noncentral Parameterisation,[0],[0]
"θ∇U logL. (15)
3In case of multiple time-series, we will use one initial state for each time-series.
",2.3. Noncentral Parameterisation,[0],[0]
"Finally, we find a maximum a posteriori (MAP) estimate for the initial state x0, latent vector field Ũ , kernel parameters θ and noise variances ω by gradient ascent,
x0,MAP, ŨMAP,θMAP,ωMAP = arg max x0,Ũ ,θ,ω
logL, (16)
while keeping the inducing locations Z fixed on a sufficiently dense grid (See Figure 1).",2.3. Noncentral Parameterisation,[0],[0]
"The partial derivatives of the posterior with respect to noise parameters ω can be found analytically, while the derivative with respect to σf is approximated with finite differences.",2.3. Noncentral Parameterisation,[0],[0]
We select the optimal lengthscales ` by cross-validation.,2.3. Noncentral Parameterisation,[0],[0]
"The key term to carry out the MAP gradient ascent optimization is the likelihood
log p(Y |x0, Ũ ,ω)
that requires forward integration and computing the partial derivatives with respect to the whitened inducing vectors Ũ .",3. Sensitivity Equations,[0],[0]
Given Equation (15) we only need to compute the gradients with respect to the inducing vectors u = vec(U) ∈,3. Sensitivity Equations,[0],[0]
"RMD,
d log p(Y |x0,u,ω)",3. Sensitivity Equations,[0],[0]
"du
= N∑ s=1 d logN (ys|x(ts,u),Ω)",3. Sensitivity Equations,[0],[0]
"dx dx(ts,u) du .",3. Sensitivity Equations,[0],[0]
"(17)
",3. Sensitivity Equations,[0],[0]
"This requires computing the derivatives of the simulated system response x(t,u) against the vector field parameters u,
dx(t,u)
du ≡ S(t) ∈ RD×MD, (18)
which we denote by Sij(t)",3. Sensitivity Equations,[0],[0]
"= ∂x(t,u)i
∂uj , and expand the no-
tation to make the dependency of x on u explicit.",3. Sensitivity Equations,[0],[0]
"Approximating these with finite differences is possible in principle, but is highly inefficient and has been reported to cause unstability (Raue et al., 2013).",3. Sensitivity Equations,[0],[0]
"We instead turn to sensitivity equations for u and x0 that provide computationally efficient, analytical gradients S(t) (Kokotovic and Heller, 1967; Fröhlich et al., 2017).
",3. Sensitivity Equations,[0],[0]
"The solution for dx(t,u)du can be derived by differentiating the full nonparametric ODE system with respect to u by
d
du
dx(t,u)
dt =
d
du f(x(t,u)).",3. Sensitivity Equations,[0],[0]
"(19)
The sensitivity equation for the given system can be obtained by changing the order of differentiation on the left hand side and carrying out the differentiation on the right hand side.
",3. Sensitivity Equations,[0],[0]
"The resulting sensitivity equation can then be expressed in the form
Ṡ(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"d
dt
dx(t,u)
du =
J(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂x
S(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"dx(t,u)
",3. Sensitivity Equations,[0],[0]
"du +
R(t)︷ ︸︸ ︷",3. Sensitivity Equations,[0],[0]
"∂f(x(t,u))
",3. Sensitivity Equations,[0],[0]
"∂u ,
(20)
",3. Sensitivity Equations,[0],[0]
"where J(t) ∈ RD×D, R(t), Ṡ(t) ∈ RD×MD (See Supplements for detailed specification).",3. Sensitivity Equations,[0],[0]
"For our nonparametric ODE system the sensitivity equation is fully determined by
J(t) = ∂K(x, Z)
∂x K(Z,Z)−1u",3. Sensitivity Equations,[0],[0]
"(21)
R(t) = K(x, Z)K(Z,Z)−1. (22)
",3. Sensitivity Equations,[0],[0]
The sensitivity equation provides us with an additional ODE system which describes the time evolution of the derivatives with respect to the inducing vectors S(t).,3. Sensitivity Equations,[0],[0]
"The sensitivities are coupled with the actual ODE system and, thus both systems x(t) and S(t) are concatenated as the new augmented state that is solved jointly by Equation (2) driven by the differentials ẋ(t) and Ṡ(t) (Leis and Kramer, 1988).",3. Sensitivity Equations,[0],[0]
The initial sensitivities are computed as S(0) = dx0du .,3. Sensitivity Equations,[0],[0]
"In our implementation, we merge x0 with u for sensitivity analysis to obtain the partial derivatives with respect to the initial state which is estimated along with the other parameters.",3. Sensitivity Equations,[0],[0]
"We use the CVODES solver from the SUNDIALS package (Hindmarsh et al., 2005) to solve the nonparametric ODE models and the corresponding gradients numerically.",3. Sensitivity Equations,[0],[0]
"The sensitivity equation based approach is superior to the finite differences approximation because we have exact formulation for the gradients of state over inducing points, which can be solved up to the numerical accuracy of the ODE solver.",3. Sensitivity Equations,[0],[0]
"As first illustration of the proposed nonparametric ODE method we consider three simulated differential systems: the Van der Pol (VDP), FitzHugh-Nagumo (FHN) and Lotka-Volterra (LV) oscillators of form
VDP : ẋ1 = x2 ẋ2 =",4. Simple Simulated Dynamics,[0],[0]
(1− x21)x2,4. Simple Simulated Dynamics,[0],[0]
− x1 FHN :,4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 3(x1 − x31 3 + x2),4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = 0.2− 3x1 − 0.2x2
3 LV :",4. Simple Simulated Dynamics,[0],[0]
ẋ1 = 1.5x1,4. Simple Simulated Dynamics,[0],[0]
− x1x2,4. Simple Simulated Dynamics,[0],[0]
"ẋ2 = −3x2 + x1x2.
",4. Simple Simulated Dynamics,[0],[0]
"In the conventional ODE case the coefficients of these equations can be inferred using standard statistical techniques if sufficient amount of time series data is available (Girolami, 2008; Raue et al., 2013).",4. Simple Simulated Dynamics,[0],[0]
"Our main goal is to infer unknown dynamics, that is, when these equations are unavailable and we instead represent the dynamics with a nonparametric
vector field of Equation (8).",4. Simple Simulated Dynamics,[0],[0]
"We use these simulated models to only illustrate our model behavior against the true dynamics.
",4. Simple Simulated Dynamics,[0],[0]
"We employ 25 data points from one cycle of noisy observation data from VDP and FHN models, and 25 data points from 1.7 cycles from the LV model with a noise variance of σ2n = 0.1
2.",4. Simple Simulated Dynamics,[0],[0]
"We learn the npODE model with five training sequences using M = 62 inducing locations on a fixed grid, and forecast between 4 and 8 future cycles starting from true initial state x0 at time 0.",4. Simple Simulated Dynamics,[0],[0]
Training takes approximately 100 seconds per oscillator.,4. Simple Simulated Dynamics,[0],[0]
"Figure 2 (bottom) shows the training datasets (grey regions), initial states, true trajectories (black lines) and the forecasted trajectory likelihoods (colored regions).",4. Simple Simulated Dynamics,[0],[0]
"The model accurately learns the dynamics from less than two cycles of data and can reproduce them reliably into future.
",4. Simple Simulated Dynamics,[0],[0]
Figure 2 (top) shows the corresponding true vector field (black arrows) and the estimated vector field (grey arrows).,4. Simple Simulated Dynamics,[0],[0]
"The vector field is a continuous function, which is plotted on a 8x8 grid for visualisation.",4. Simple Simulated Dynamics,[0],[0]
"In general the most difficult part of the system is learning the middle of the loop (as seen in the FHN model), and learning the most outermost regions (bottom left in the LV model).",4. Simple Simulated Dynamics,[0],[0]
"The model learns the
underlying differential f(x) accurately close to observed points, while making only few errors in the border regions with no data.",4. Simple Simulated Dynamics,[0],[0]
"Next, we illustrate how the model estimates realistic, unknown dynamics from noisy observations y(t1), . . .",5. Unknown System Estimation,[0],[0]
",y(tN ).",5. Unknown System Estimation,[0],[0]
"As in Section 4, we make no assumptions on the structure or form of the underlying system, and capture the underlying dynamics with the nonparameteric system alone.",5. Unknown System Estimation,[0],[0]
"We employ no subjective priors, and assume no inputs, controls or other sources of information.",5. Unknown System Estimation,[0],[0]
"The task is to infer the underlying dynamics f(x), and interpolate or extrapolate the state trajectory outside the observed data.
",5. Unknown System Estimation,[0],[0]
We use a benchmark dataset of human motion capture data from the Carnegie Mellon University motion capture (CMU mocap) database.,5. Unknown System Estimation,[0],[0]
"Our dataset contains 50-dimensional pose measurements y(ti) from humans walking, where each pose dimension records a measurement in different parts of the body during movement (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
We apply the preprocessing of Wang et al. (2008) by downsampling the datasets by a factor of four and centering the data.,5. Unknown System Estimation,[0],[0]
"This resulted in a total of 4303 datapoints spread across 43 trajec-
tories with on average 100 frames per trajectory.",5. Unknown System Estimation,[0],[0]
"In order to tackle the problem of dimensionality, we project the original dataset with PCA to a three dimensional latent space where the system is specified, following Damianou et al. (2011) and Wang et al. (2006).",5. Unknown System Estimation,[0],[0]
"We place M = 53 inducing vectors on a fixed grid, and optimize our model starting from 100 different initial values, which we set by perturbing the projected empirical differences y(ti)−y(ti−1) to the inducing vectors.",5. Unknown System Estimation,[0],[0]
We use an L-BFGS optimizer in Matlab.,5. Unknown System Estimation,[0],[0]
"The whole inference takes approximately few minutes per trajectory.
",5. Unknown System Estimation,[0],[0]
We evaluate the method with two types of experiments: imputing missing values and forecasting future cycles.,5. Unknown System Estimation,[0],[0]
"For the forecasting the first half of the trajectory is reserved for model training, and the second half is to be forecasted.",5. Unknown System Estimation,[0],[0]
"For imputation we remove roughly 20% of the frames from the middle of the trajectory, which are to be filled by the models.",5. Unknown System Estimation,[0],[0]
We perform model selection for lengthscales ` with crossvalidation split of 80/20.,5. Unknown System Estimation,[0],[0]
"We record the root mean square error (RMSE) over test points in the original feature space in both cases, where we reconstruct the original dimensions from the latent space trajectories.
",5. Unknown System Estimation,[0],[0]
"Due to the current lack of ODE methods suitable for this nonparametric inference task, we instead compare our method to the state-of-the-art state-space models where such problems have been previously considered (Wang et al., 2008).",5. Unknown System Estimation,[0],[0]
In a state-space or dynamical model a transition function x(tk+1) = g(x(tk)) moves the system forward in discrete steps.,5. Unknown System Estimation,[0],[0]
"With sufficiently high sampling rate, such models can estimate and forecast finite approximations of smooth dynamics.",5. Unknown System Estimation,[0],[0]
"In Gaussian process dynamical model (Wang et al., 2006; Frigola et al., 2014; Svensson et al., 2016)",5. Unknown System Estimation,[0],[0]
"a GP transition function is inferred in a latent space, which can be inferred with a standard GPLVM (Lawrence, 2004) or with a dependent GPLVM (Zhao and Sun, 2016).",5. Unknown System Estimation,[0],[0]
"In dynamical systems the transition function is replaced by a GP interpolation (Damianou et al., 2011).",5. Unknown System Estimation,[0],[0]
"The discrete time state-space models emphasize inference of a low-dimensional manifold as an explanation of the high-dimensional measurement trajectories.
",5. Unknown System Estimation,[0],[0]
"We compare our method to the dynamical model GPDM of Wang et al. (2006) and to the dynamical system VGPLVM of Damianou et al. (2011), where we directly apply the implementations provided by the authors at inverseprobability.com/vargplvm and dgp.",5. Unknown System Estimation,[0],[0]
toronto.edu/˜jmwang/gpdm.,5. Unknown System Estimation,[0],[0]
"Both methods optimize their latent spaces separately, and they are thus not directly comparable.",5. Unknown System Estimation,[0],[0]
"In the forecasting task we train all models with the first half of the trajectory, while forecasting the second half starting from the first frame.",5.1. Forecasting,[0],[0]
"The models are trained and forecasted
within a low-dimensional space, and subsequently projected back into the original space via inverting the PCA or with GPLVM mean predictions.",5.1. Forecasting,[0],[0]
"As all methods optimize their latent spaces separately, they are not directly comparable.",5.1. Forecasting,[0],[0]
"Thus, the mean errors are computed in the original highdimensional space.",5.1. Forecasting,[0],[0]
"Note that the low-dimensional representation necessarily causes some reconstruction errors.
",5.1. Forecasting,[0],[0]
Figure 3 illustrates the models on one of the trajectories 35 12.amc.,5.1. Forecasting,[0],[0]
"The top part (a) shows the training data in the PCA space for npODE, and optimized training data representation for GPDM and VGPLVM (black points).",5.1. Forecasting,[0],[0]
"The colored lines (npODE) and points (GPDM, VGPLVM) indicate the future forecast.",5.1. Forecasting,[0],[0]
The bottom part (b) shows the first 9 reconstructed original pose dimensions reconstructed from the latent forecasted trajectories.,5.1. Forecasting,[0],[0]
"The training data is shown in grey background, while test data is shown with circles.
",5.1. Forecasting,[0],[0]
"The VGPLVM has most trouble forecasting future points, and reverts quickly after training data to a value close to zero, failing to predict future points.",5.1. Forecasting,[0],[0]
"The GPDM model produces more realistic trajectories, but fails to predict any of the poses accurately.",5.1. Forecasting,[0],[0]
"Finally, npODE can accurately predict five poses, and still retains adequate performance on remaining poses, except for pose 2.
",5.1. Forecasting,[0],[0]
"Furthermore, Table 1 indicates that npODE is also best performing method on average over the whole dataset in the forecasting.",5.1. Forecasting,[0],[0]
In the imputation task we remove approximately 20% of the training data from the middle of the trajectory.,5.2. Imputation,[0],[0]
The goals are to learn a model with the remaining data and to forecast the missing values.,5.2. Imputation,[0],[0]
Figure 4 highlights the performance of the three models on the trajectory 07 07.amc.,5.2. Imputation,[0],[0]
"The top part (a) shows the training data (black points) in the PCA space (npODE) or optimized training locations in the latent space (GPDM, VGPLVM).",5.2. Imputation,[0],[0]
The middle part imputation is shown with colored points or lines.,5.2. Imputation,[0],[0]
"Interestingly both npODE and GPDM operate on cyclic representations, while VGPLVM is not cyclic.
",5.2. Imputation,[0],[0]
"The bottom panel (b) shows the first 9 reconstructed pose
dimensions from the three models.",5.2. Imputation,[0],[0]
"The missing values are shown in circles, while training points are shown with black dots.",5.2. Imputation,[0],[0]
"All models can accurately reproduce the overall trends, while npODE seems to fit slightly worse than the other methods.",5.2. Imputation,[0],[0]
The PCA projection causes the seemingly perfect fit of the npODE prediction (at the top) to lead to slightly warped reconstructions (at the bottom).,5.2. Imputation,[0],[0]
All methods mostly fit the missing parts as well.,5.2. Imputation,[0],[0]
Table 1 shows that on average the npODE and VGPLVM have approximately equal top performance on the imputing missing values task.,5.2. Imputation,[0],[0]
"We proposed the framework of nonparametric ODE model that can accurately learn arbitrary, nonlinear continuos-time dynamics from purely observational data without making assumptions of the underlying system dynamics.",6. Discussion,[0],[0]
We demonstrated that the model excels at learning dynamics that can be forecasted into the future.,6. Discussion,[0],[0]
"We consider this work as the
first in a line of studies of nonparametric ODE systems, and foresee several aspects as future work.",6. Discussion,[0],[0]
"Currently we do not handle non-stationary vector fields, that is time-dependent differentials ft(x).",6. Discussion,[0],[0]
"Furthermore, an interesting future avenue is the study of various vector field kernels, such as divergence-free, curl-free or spectral kernels (Remes et al., 2017).",6. Discussion,[0],[0]
"Finally, including inputs or controls to the system would allow precise modelling in interactive settings, such as robotics.
",6. Discussion,[0],[0]
"The proposed nonparametric ODE model operates along a continuous-time trajectory, while dynamic models such as hidden Markov models or state-space models are restricted to discrete time steps.",6. Discussion,[0],[0]
"These models are unable to consider system state at arbitrary times, for instance, between two successive timepoints.
",6. Discussion,[0],[0]
"Conventional ODE models have also been considered from the stochastic perspective with stochastic differential equation (SDE) models that commonly model the deterministic
system drift and diffusion processes separately leading to a distribution of trajectories p(x(t))",6. Discussion,[0],[0]
"(Archambeau et al., 2007; Garcı́a et al., 2017).",6. Discussion,[0],[0]
"As future work we will consider stochastic extensions of our nonparametric ODE model, as well as MCMC sampling of the inducing point posterior p(U |Y ), leading to trajectory distribution as well.
Acknowledgements.",6. Discussion,[0],[0]
The data used in this project was obtained from mocap.cs.cmu.edu.,6. Discussion,[0],[0]
The database was created with funding from NSF EIA-0196217.,6. Discussion,[0],[0]
"This work has been supported by the Academy of Finland Center of Excellence in Systems Immunology and Physiology, the Academy of Finland grants no. 284597, 311584, 313271, 299915.",6. Discussion,[0],[0]
In conventional ODE modelling coefficients of an equation driving the system state forward in time are estimated.,abstractText,[0],[0]
"However, for many complex systems it is practically impossible to determine the equations or interactions governing the underlying dynamics.",abstractText,[0],[0]
"In these settings, parametric ODE model cannot be formulated.",abstractText,[0],[0]
"Here, we overcome this issue by introducing a novel paradigm of nonparametric ODE modelling that can learn the underlying dynamics of arbitrary continuous-time systems without prior knowledge.",abstractText,[0],[0]
"We propose to learn non-linear, unknown differential functions from state observations using Gaussian process vector fields within the exact ODE formalism.",abstractText,[0],[0]
We demonstrate the model’s capabilities to infer dynamics from sparse data and to simulate the system forward into future.,abstractText,[0],[0]
Learning unknown ODE models with Gaussian processes,title,[0],[0]
"Translating words between languages, or more generally inferring bilingual dictionaries, is a long-studied research direction with applications including machine translation (Lample et al., 2017), multilingual word embeddings (Klementiev et al., 2012), and knowledge transfer to low resource languages (Guo et al., 2016).",1 Introduction,[0],[0]
"Research here has a long history under the guise of decipherment (Knight et al., 2006).",1 Introduction,[0],[0]
"Current contemporary methods have achieve effective word translation through theme-aligned corpora (Gouws et al., 2015), or seed dictionaries (Mikolov et al., 2013).
",1 Introduction,[0],[0]
"Mikolov et al. (2013) showed that monolingual word embeddings exhibit isomorphism across languages, and can be aligned with a simple linear transformation.",1 Introduction,[0],[0]
"Given two sets word vectors learned independently from monolingual corpora, and a dictionary of seed pairs to learn a linear transformation for alignment; they were able to
estimate a complete bilingual lexicon.",1 Introduction,[0.9522455089666882],"['We use a Siamese CBOW model that was pretrained on a snapshot of Wikipedia from November 2012 using randomly initialized word embeddings.5 Similarly to the word2vec model, to embed a given sentence containing a VNC instance, we average the word embeddings for each word in the sentence.']"
"Many studies have since followed this approach, proposing various improvements such as orthogonal mappings (Artetxe et al., 2016) and improved objectives (Lazaridou et al., 2015).
",1 Introduction,[0],[0]
Obtaining aligned corpora or bilingual seed dictionaries is nevertheless not straightforward for all language pairs.,1 Introduction,[0],[0]
"This has motivated a wave of very recent research into unsupervised word translation: inducing bilingual dictionaries given only monolingual word embeddings (Conneau et al., 2018; Zhang et al., 2017b,a; Artetxe et al., 2017).",1 Introduction,[0],[0]
"The most successful have leveraged ideas from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014).",1 Introduction,[0],[0]
"In this approach the generator provides the cross-modal mapping, taking embeddings of dictionary words in one language and ‘generating’ their translation in another.",1 Introduction,[0],[0]
The discriminator tries to distinguish between this ‘fake’ set of translations and the true dictionary of embeddings in the target language.,1 Introduction,[0],[0]
"The two play a competitive game, and if the generator learns to fool the discriminator, then its cross-modal mapping should be capable of inducing a complete dictionary, as per Mikolov et al. (2013).
",1 Introduction,[0],[0]
"Despite these successes, such adversarial methods have a number of well-known drawbacks (Arjovsky et al., 2017):",1 Introduction,[0],[0]
"Due to the nature of their min-max game, adversarial training is very unstable, and they are prone to divergence.",1 Introduction,[0],[0]
"It is extremely hyper-parameter sensitive, requiring problem-specific tuning.",1 Introduction,[0],[0]
"Convergence is also hard to diagnose and does not correspond well to efficacy of the generator in downstream tasks (Hoshen and Wolf, 2018).
",1 Introduction,[0],[0]
"In this paper, we propose an alternative statistical dependency-based approach to unsupervised word translation.",1 Introduction,[0],[0]
"Specifically, we propose to search for the cross-lingual word pairing that maximizes statistical dependency in terms of squared
loss mutual information (SMI) (Yamada et al., 2015; Suzuki and Sugiyama, 2010).",1 Introduction,[0],[0]
"Compared to prior statistical dependency-based approaches such as Kernelized Sorting (KS) (Quadrianto et al., 2009) we advance: (i) through use of SMI rather than their Hilbert Schmidt Independence Criterion (HSIC) and (ii) through jointly optimising cross-modal pairing with representation learning within each view.",1 Introduction,[0],[0]
"In contrast to prior work that uses a fixed representation, by non-linearly projecting monolingual world vectors before matching, we learn a new embedding where statistical dependency is easier to establish.",1 Introduction,[0],[0]
"Our method: (i) achieves similar unsupervised translation performance to recent adversarial methods, while being significantly easier to train and (ii) clearly outperforms prior non-adversarial methods.",1 Introduction,[0],[0]
"Let dataset D contain two sets of unpaired monolingual word embeddings from two languages D = ({xi}ni=1, {yj}nj=1) where x,y ∈ Rd.",2.1 Deep Distribution Matching,[0],[0]
"Let π be a permutation function over {1, 2, . . .",2.1 Deep Distribution Matching,[0],[0]
",",2.1 Deep Distribution Matching,[0],[0]
"n}, and Π the corresponding permutation indicator matrix: Π ∈ {0, 1}n×n,Π1n",2.1 Deep Distribution Matching,[0],[0]
"= 1n, and Π>1n = 1n.",2.1 Deep Distribution Matching,[0],[0]
Where 1n is the n-dimensional vector with all ones.,2.1 Deep Distribution Matching,[0],[0]
"We aim to optimize for both the permutation Π (bilingual dictionary), and non-linear transformations gx(·) and gy(·) of the respective wordvectors, that maximize statistical dependency between the views.",2.1 Deep Distribution Matching,[0],[0]
While regularising by requiring the original word embedding information is preserved through reconstruction using decoders fx(·) and fy(·).,2.1 Deep Distribution Matching,[0],[0]
"Our overall loss function is:
min Θx,Θy ,Π Ω(D; Θx,Θy)︸ ︷︷ ︸ Regularizer −λDΠ(D; Θx,Θy)︸ ︷︷ ︸ Dependency ,
DΠ(D; Θx,Θy) = DΠ({gx(xi), gy(yπ(i))}ni=1),
Ω(D; Θx,Θy) = n∑ i=1",2.1 Deep Distribution Matching,[0],[0]
"‖xi − fx(gx(xi))‖22
+ ‖yi",2.1 Deep Distribution Matching,[0],[0]
− fy(gy(yi))‖22 +R(Θx),2.1 Deep Distribution Matching,[0],[0]
"+R(Θy).
(1)
where Θs parameterize the encoding and reconstruction transformations, R(·) is a regularizer (e.g., `2-norm and `1-norm), and DΠ(·, ·) is a statistical dependency measure.",2.1 Deep Distribution Matching,[0],[0]
"Crucially compared to prior methods such as matching CCA (Haghighi
et al., 2008), dependency measures such as SMI do not need comparable representations to get started, making the bootstrapping problem less severe.",2.1 Deep Distribution Matching,[0],[0]
Squared-Loss Mutual Information (SMI),2.2 Dependence Estimation,[0],[0]
"The squared loss mutual information between two random variables x and y is defined as (Suzuki and Sugiyama, 2010):
SMI = ∫∫ ( p(x,y) p(x)p(y)",2.2 Dependence Estimation,[0],[0]
"− 1 )2 p(x)p(y)dxdy,
which is the Pearson divergence (Pearson, 1900) from p(x,y) to p(x)p(y).",2.2 Dependence Estimation,[0],[0]
"The SMI is an f - divergence (Ali and Silvey, 1966).",2.2 Dependence Estimation,[0],[0]
"That is, it is a non-negative measure and is zero only if the random variables are independent.
",2.2 Dependence Estimation,[0],[0]
"To measure SMI from a set of samples we take a direct density ratio estimation approach (Suzuki and Sugiyama, 2010), which leads (Yamada et al., 2015) to the estimator:
ŜMI({(xi,yi)}ni=1)",2.2 Dependence Estimation,[0],[0]
"= 1 2n tr (diag (α̂)KL)− 1 2 ,
where K ∈ Rn×n and L ∈ Rn×n are the gram matricies for x and y respectively, and
Ĥ = 1
n2 (KK>) ◦",2.2 Dependence Estimation,[0],[0]
"(LL>),
ĥ = 1
n",2.2 Dependence Estimation,[0],[0]
"(K ◦L)1n, α̂ =
( Ĥ + λIn )−1 ĥ,
λ > 0 is a regularizer and In ∈ Rn×n is the identity matrix.
",2.2 Dependence Estimation,[0],[0]
"SMI for Matching SMI computes the dependency between two sets of variables, under an assumption of known correspondence.",2.2 Dependence Estimation,[0],[0]
In our application this corresponds to a measure of dependency between two aligned sets of monolingual wordvectors.,2.2 Dependence Estimation,[0],[0]
"To exploit SMI for matching, we introduce a permutation variable Π by replacing L→ Π>LΠ in the estimator:
ŜMI({(xi,yπ(i))}n1 )",2.2 Dependence Estimation,[0],[0]
= 1 2n tr ( diag (α̂Π)KΠ >LΠ ),2.2 Dependence Estimation,[0],[0]
"− 1 2 ,
that will enable optimizing Π to maximize SMI.",2.2 Dependence Estimation,[0],[0]
"To initialize Θx and Θy, we first independently estimate them using autoencoders.",2.3 Optimization of parameters,[0],[0]
Then we employ an alternative optimization on Eq.,2.3 Optimization of parameters,[0],[0]
"(1) for
(Θx,Θy) and Π until convergence.",2.3 Optimization of parameters,[0],[0]
We use 3 layer MLP neural networks for both f and g. Algorithm 1 summarises the steps.,2.3 Optimization of parameters,[0],[0]
"Optimization for Θx and Θy With fixed permutation matrix Π (or π), the objective function
min Θx,Θy
Ω(D; Θx,Θy)− λDΠ(D; Θx,Θy) (2)
is an autoencoder optimization with regularizer DΠ(·), and can be solved with backpropagation.",2.3 Optimization of parameters,[0],[0]
"Optimization for Π To find the permutation (word matching) Π that maximizes SMI given fixed encoding parameters Θx,Θy, we only need to optimize the dependency term DΠ in Eq.",2.3 Optimization of parameters,[0],[0]
(1).,2.3 Optimization of parameters,[0],[0]
"We employ the LSOM algorithm (Yamada et al., 2015).",2.3 Optimization of parameters,[0],[0]
"The estimator of SMI for samples {gx(xi), gy(yπ(i))}ni=1 encoded with gx, gy is:
ŜMI = 1 2n tr ( diag (α̂Θ,Π)KΘxΠ >LΘyΠ )",2.3 Optimization of parameters,[0],[0]
"− 1 2 .
",2.3 Optimization of parameters,[0],[0]
"Which leads to the optimization problem:
max Π∈{0,1}n×n
tr (
diag (α̂Θ,Π)KΘxΠ >LΘyΠ ) s.t. Π1n",2.3 Optimization of parameters,[0],[0]
"= 1n,Π>1n = 1n.",2.3 Optimization of parameters,[0],[0]
"(3)
Since the optimization problem is NP-hard, we iteratively solve the relaxed problem (Yamada et al., 2015):
",2.3 Optimization of parameters,[0],[0]
"Πnew = (1− η)Πold+
η argmax Π
tr ( diag ( α̂Θ,Πold ) KΘxΠ >LΘyΠ old ) ,
where 0 <",2.3 Optimization of parameters,[0],[0]
η ≤ 1 is a step size.,2.3 Optimization of parameters,[0],[0]
The optimization problem is a linear assignment problem (LAP).,2.3 Optimization of parameters,[0],[0]
"Thus, we can efficiently solve the algorithm by using the Hungarian method (Kuhn, 1955).",2.3 Optimization of parameters,[0],[0]
"To get discrete Π, we solve the last step by setting η = 1.
",2.3 Optimization of parameters,[0],[0]
"Intuitively, this can be seen as searching for the permutation Π for which the data in the two (initially unsorted views) have a matching withinview affinity (gram) matrix, where matching is defined by maximum SMI.",2.3 Optimization of parameters,[0],[0]
"In this section, we evaluate the efficacy of our proposed method against various state of the art methods for word translation.",3 Experiments,[0],[0]
Implementation Details,3 Experiments,[0],[0]
Our autoencoder consists of two layers with dropout and a tanh nonlinearity.,3 Experiments,[0],[0]
"We use polynomial kernel to compute
Algorithm 1 SMI-based unsupervised word translation Input: Unpaired word embeddings D = ({xi}ni=1, {yj}nj=1).
1: Init: weights Θx, Θy, permutation matrix Π. 2: while not converged do 3: Update Θx,Θy given Π: Backprop (2).",3 Experiments,[0],[0]
"4: Update Π given Θx,Θy: LSOM (3).",3 Experiments,[0],[0]
"5: end while
Output: Permutation Matrix Π. Params Θx, Θy.
the gram matrices K and L.",3 Experiments,[0],[0]
"For all pairs of languages, we fix the number of training epochs to 20.",3 Experiments,[0],[0]
All the word vectors are `2 unit normalized.,3 Experiments,[0],[0]
For CSLS we set the number of neighbors to 10.,3 Experiments,[0],[0]
"For optimizing Π at each epoch, we set the step size η = 0.75 and use 20 iterations.",3 Experiments,[0],[0]
"For the regularization R(Θ), we use the sum of the Frobenius norms of weight matrices.",3 Experiments,[0],[0]
"We train Θ using full batch gradient-descent, with learning rate 0.05.",3 Experiments,[0],[0]
"Datasets We performed experiments on the publicly available English-Italian, EnglishSpanish and English-Chinese datasets released by (Dinu and Baroni, 2015; Zhang et al., 2017b; Vulic and Moens, 2013).",3 Experiments,[0],[0]
We name this collective set of benchmarks BLI.,3 Experiments,[0],[0]
"We also conduct further experiments on a much larger recent public benchmark, MUSE (Conneau et al., 2018)1.",3 Experiments,[0],[0]
"Setting and Metrics We evaluate all methods in terms of Precision@1, following standard practice.",3 Experiments,[0],[0]
"We note that while various methods in the literature were initially presented as fully supervised (Mikolov et al., 2013), semi-supervised (using a seed dictionary)",3 Experiments,[0],[0]
"(Haghighi et al., 2008), or unsupervised (Zhang et al., 2017b), most of them can be straightforwardly adapted to run in any of these settings.",3 Experiments,[0],[0]
"Therefore we evaluate all methods both in the unsupervised setting in which we are primarily interested, and also the commonly evaluated semi-supervised setting with 500 seed pairs.",3 Experiments,[0],[0]
"Competitors: Non-Adversarial In terms of competitors that, like us, do not make use of GANs, we evaluate: Translation Matrix (Mikolov et al., 2013), which alternates between estimating a linear transformation by least squares and matching by nearest neighbour (NN).",3 Experiments,[0],[0]
"Multilingual Correlation (Faruqui and Dyer, 2014), and Matching CCA (Haghighi et al., 2008), which alternates between matching and estimat-
1https://github.com/facebookresearch/MUSE/
ing a joint linear subspace.",3 Experiments,[0],[0]
"Kernelized Sorting (Quadrianto et al., 2009), which directly uses HSIC-based statistical dependency to match heterogeneous data points.",3 Experiments,[0],[0]
"Self Training (Artetxe et al., 2017)",3 Experiments,[0],[0]
"A recent state of the art method that alternate between estimating an orthonormal transformation, and NN matching.
",3 Experiments,[0],[0]
"Competitors: Adversarial In terms of competitors that do make use of adversarial training, we compare: W-GAN and EMDOT (Zhang et al., 2017b) make use of adversarial learning using Wasserstein GAN and Earth Movers Distance respectively.",3 Experiments,[0],[0]
"GAN-NN (Conneau et al., 2018) uses adversarial learning to train an orthogonal transformation, along with some refinement steps and an improvement to the conventional NN matching procedure called ‘cross-domain similarity lo-
cal scaling’ (CSLS).",3 Experiments,[0],[0]
"Since this is a distinct step, we also evaluate our method with CSLS.
",3 Experiments,[0],[0]
"We use the provided code for GAN-NN and Self-Train, while re-implementing EDOT/WGAN to avoid dependency on theano.",3 Experiments,[0],[0]
Fully Unsupervised Table 1 presents comparative results for unsupervised word translation on BLI and MUSE.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
Our method (bottom) is consistently and significantly better than non-adversarial alternatives (top).,3.1 Results,[0],[0]
(ii),3.1 Results,[0],[0]
"Compared to adversarial alternatives Deep-SMI performs comparably.
",3.1 Results,[0],[0]
All methods generally perform better on the MUSE dataset than BLI.,3.1 Results,[0],[0]
"These differences are due to a few factors: MUSE is a significantly
larger dataset than BLI, benefitting methods that can exploit a large amount of training data.",3.1 Results,[0],[0]
"In the ground-truth annotation, BLI contains 1-1 translations while MUSE contains more realistic 1-many translations (if any correct translation is picked, a success is counted), making it easier to reach a higher score.
",3.1 Results,[0],[0]
Semi-supervised Results using a 500-word bilingual seed dictionary are presented in Table 2.,3.1 Results,[0],[0]
From these we observe: (i),3.1 Results,[0],[0]
"The conventional methods’ performances (top) jump up, showing that they are more competitive if at least some sparse data is available.",3.1 Results,[0],[0]
"(ii) Deep-SMI performance also improves, and still outperforms the classic methods significantly overall.",3.1 Results,[0],[0]
"(iii) Again, we perform comparably to the GAN methods.",3.1 Results,[0],[0]
Figure 1 shows the convergence process of DeepSMI.,3.2 Discussion,[0],[0]
"From this we see that: (i) Unlike the adversarial methods, our objective (Eq. (1)) improves smoothly over time, making convergence much easier to assess.",3.2 Discussion,[0],[0]
"(ii) Unlike the adversarial methods, our accuracy generally mirrors the model’s loss.",3.2 Discussion,[0],[0]
"In contrast, the various losses of the adversarial approaches do not well reflect translation accuracy, making model selection or early stopping a challenge in itself.",3.2 Discussion,[0.9609604008880464],"['In particular, the relatively low recall for the literal class indicates that many literal usages occur in a canonical form.']"
"Please compare our Figure 1 with Fig 3 in Zhang et al. (2017b), and Fig 2 in Conneau et al. (2018).
",3.2 Discussion,[0],[0]
There are two steps in our optimization: matching permutation Π and representation weights Θ.,3.2 Discussion,[0],[0]
"Although this is an alternating optimization, it is analogous to an EM-type algorithm optimizing latent variables (Π) and parameters (Θ).",3.2 Discussion,[0],[0]
"While local minima are a risk, every optimisation step for either variable reduces our objective Eq.",3.2 Discussion,[0],[0]
"(1).
",3.2 Discussion,[0],[0]
"There is no min-max game, so no risk of divergence as in the case of adversarial GAN-type methods.
",3.2 Discussion,[0],[0]
Our method can also be understood as providing an unsupervised Deep-CCA type model for relating heterogeneous data across two views.,3.2 Discussion,[0],[0]
"This is in contrast to the recently proposed unsupervised shallow CCA (Hoshen and Wolf, 2018), and conventional supervised Deep-CCA (Chang et al., 2018) that requires paired data for training; and using SMI rather than correlation as the optimisation objective.",3.2 Discussion,[0],[0]
We have presented an effective approach to unsupervised word translation that performs comparably to adversarial approaches while being significantly easier to train and diagnose; as well as outperforming prior non-adversarial approaches.,4 Conclusion,[0],[0]
"Word translation, or bilingual dictionary induction, is an important capability that impacts many multilingual language processing tasks.",abstractText,[0],[0]
"Recent research has shown that word translation can be achieved in an unsupervised manner, without parallel seed dictionaries or aligned corpora.",abstractText,[0],[0]
"However, state of the art methods for unsupervised bilingual dictionary induction are based on generative adversarial models, and as such suffer from their well known problems of instability and hyperparameter sensitivity.",abstractText,[0],[0]
We present a statistical dependency-based approach to bilingual dictionary induction that is unsupervised – no seed dictionary or parallel corpora required; and introduces no adversary – therefore being much easier to train.,abstractText,[0],[0]
Our method performs comparably to adversarial alternatives and outperforms prior non-adversarial methods.,abstractText,[0],[0]
Learning Unsupervised Word Translations Without Adversaries,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1024–1034 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics",text,[0],[0]
Learning word representations has become a fundamental problem in processing natural languages.,1 Introduction,[0],[0]
"These semantic representations, which map a word into a point in a linear space, have been widely applied in downstream applications, including named entity recognition (Guo et al., 2014), document ranking (Nalisnick et al., 2016), sentiment analysis (Irsoy and Cardie, 2014), question answering (Antol et al., 2015), and image captioning (Karpathy and Fei-Fei, 2015).
",1 Introduction,[0],[0]
"Over the past few years, various approaches have been proposed to learn word vectors (e.g., (Pennington et al., 2014; Mikolov et al., 2013a; Levy and Goldberg, 2014b; Ji et al., 2015)) based on co-occurrence information between words observed on the training corpus.",1 Introduction,[0],[0]
"The intuition behind this is to represent words with similar vectors if
they have similar contexts.",1 Introduction,[0],[0]
"To learn a good word embedding, most approaches assume a large collection of text is freely available, such that the estimation of word co-occurrences is accurate.",1 Introduction,[0],[0]
"For example, the Google Word2Vec model (Mikolov et al., 2013a) is trained on the Google News dataset, which contains around 100 billion tokens, and the GloVe embedding (Pennington et al., 2014) is trained on a crawled corpus that contains 840 billion tokens in total.",1 Introduction,[0],[0]
"However, such an assumption may not hold for low-resource languages such as Inuit or Sindhi, which are not spoken by many people or have not been put into a digital format.",1 Introduction,[0],[0]
"For those languages, usually, only a limited size corpus is available.",1 Introduction,[0],[0]
"Training word vectors under such a setting is a challenging problem.
",1 Introduction,[0],[0]
One key restriction of the existing approaches is that they often mainly rely on the word pairs that are observed to co-occur on the training data.,1 Introduction,[0],[0]
"When the size of the text corpus is small, most word pairs are unobserved, resulting in an extremely sparse co-occurrence matrix (i.e., most entries are zero)1.",1 Introduction,[0],[0]
"For example, the text82 corpus has about 17,000,000 tokens and 71,000 distinct words.",1 Introduction,[0],[0]
"The corresponding co-occurrence matrix has more than five billion entries, but only about 45,000,000 are non-zeros (observed on the training corpus).",1 Introduction,[0],[0]
"Most existing approaches, such as Glove and Skip-gram, cannot handle a vast number of zero terms in the co-occurrence matrix; therefore, they only sub-sample a small subset of zero entries during the training.
",1 Introduction,[0],[0]
"In contrast, we argue that the unobserved word pairs can provide valuable information for training a word embedding model, especially when the co-occurrence matrix is very sparse.",1 Introduction,[0],[0]
"Inspired
1Note that the zero term can mean either the pairs of words cannot co-occur or the co-occurrence is not observed in the training corpus.
2http://mattmahoney.net/dc/text8.zip
1024
by the success of Positive-Unlabeled Learning (PU-Learning) in collaborative filtering applications (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015), we design an algorithm to effectively learn word embeddings from both positive (observed terms) and unlabeled (unobserved/zero terms) examples.",1 Introduction,[0],[0]
"Essentially, by using the square loss to model the unobserved terms and designing an efficient update rule based on linear algebra operations, the proposed PULearning framework can be trained efficiently and effectively.
",1 Introduction,[0],[0]
We evaluate the performance of the proposed approach in English3 and other three resourcescarce languages.,1 Introduction,[0],[0]
"We collected unlabeled language corpora from Wikipedia and compared the proposed approach with popular approaches, the Glove and the Skip-gram models, for training word embeddings.",1 Introduction,[0],[0]
"The experimental results show that our approach significantly outperforms the baseline models, especially when the size of the training corpus is small.
",1 Introduction,[0],[0]
"Our key contributions are summarized below.
",1 Introduction,[0],[0]
"• We propose a PU-Learning framework for learning word embedding.
",1 Introduction,[0],[0]
"• We tailor the coordinate descent algorithm (Yu et al., 2017b) for solving the corresponding optimization problem.
",1 Introduction,[0],[0]
• Our experimental results show that PULearning improves the word embedding training in the low-resource setting.,1 Introduction,[0],[0]
Learning word vectors.,2 Related work,[0],[0]
"The idea of learning word representations can be traced back to Latent Semantic Analysis (LSA) (Deerwester et al., 1990) and Hyperspace Analogue to Language (HAL) (Lund and Burgess, 1996), where word vectors are generated by factorizing a worddocument and word-word co-occurrence matrix, respectively.",2 Related work,[0],[0]
"Similar approaches can also be extended to learn other types of relations between words (Yih et al., 2012; Chang et al., 2013) or entities (Chang et al., 2014).",2 Related work,[0],[0]
"However, due to the limitation of the use of principal component analysis,
3Although English is not a resource-scarce language, we simulate the low-resource setting in an English corpus.",2 Related work,[0],[0]
"In this way, we leverage the existing evaluation methods to evaluate the proposed approach.
",2 Related work,[0],[0]
these approaches are often less flexible.,2 Related work,[0],[0]
"Besides, directly factorizing the co-occurrence matrix may cause the frequent words dominating the training objective.
",2 Related work,[0],[0]
"In the past decade, various approaches have been proposed to improve the training of word embeddings.",2 Related work,[0],[0]
"For example, instead of factorizing the co-occurrence count matrix, Bullinaria and Levy (2007); Levy and Goldberg (2014b) proposed to factorize point-wise mutual information (PMI) and positive PMI (PPMI) matrices as these metrics scale the co-occurrence counts (Bullinaria and Levy, 2007; Levy and Goldberg, 2014b).",2 Related work,[0],[0]
"Skipgram model with negative-sampling (SGNS) and Continuous Bag-of-Words models (Mikolov et al., 2013b) were proposed for training word vectors on a large scale without consuming a large amount of memory.",2 Related work,[0],[0]
"GloVe (Pennington et al., 2014) is proposed as an alternative to decompose a weighted log co-occurrence matrix with a bias term added to each word.",2 Related work,[0],[0]
"Very recently, WordRank model (Ji et al., 2015) has been proposed to minimize a ranking loss which naturally fits the tasks requiring ranking based evaluation metrics.",2 Related work,[0],[0]
Stratos et al. (2015) also proposed CCA (canonical correlation analysis)-based word embedding which shows competitive performance.,2 Related work,[0],[0]
"All these approaches focus on the situations where a large text corpus is available.
",2 Related work,[0],[0]
"Positive and Unlabeled (PU) Learning: Positive and Unlabeled (PU) learning (Li and Liu, 2005) is proposed for training a model when the positive instances are partially labeled and the unlabeled instances are mostly negative.",2 Related work,[0],[0]
"Recently, PU learning has been used in many classification and collaborative filtering applications due to the nature of “implicit feedback” in many recommendation systems—users usually only provide positive feedback (e.g., purchases, clicks) and it is very hard to collect negative feedback.
",2 Related work,[0],[0]
"To resolve this problem, a series of PU matrix completion algorithms have been proposed (Pan et al., 2008; Hu et al., 2008; Pan and Scholz, 2009; Qin et al., 2010; Paquet and Koenigstein, 2013; Hsieh et al., 2015; Yu et al., 2017b).",2 Related work,[0],[0]
The main idea is to assign a small uniform weight to all the missing or zero entries and factorize the corresponding matrix.,2 Related work,[0],[0]
"Among them, Yu et al. (2017b) proposed an efficient algorithm for matrix factorization with PU-learning, such that the weighted matrix is constructed implicitly.",2 Related work,[0],[0]
"In this paper, we
W, C vocabulary of central and context words m,n vocabulary sizes k dimension of word vectors W,H m× k and n×",2 Related work,[0],[0]
"k latent matrices Cij weight for the (i, j) entry Aij value of the PPMI matrix Qij value of the co-occurrence matrix wi,hj i-th row of W and j-th row of H b, b̂ bias term λi, λj regularization parameters | · | the size of a set Ω Set of possible word-context pairs Ω+ Set of observed word-context pairs Ω− Set of unobserved word-context pairs
Table 1: Notations.
design a new approach for training word vectors by leveraging the PU-Learning framework and existing word embedding techniques.",2 Related work,[0],[0]
"To the best of our knowledge, this is the first work to train word embedding models using the PU-learning framework.",2 Related work,[0],[0]
"Similar to GloVe and other word embedding learning algorithms, the proposed approach consists of three steps.",3 PU-Learning for Word Embedding,[0],[0]
The first step is to construct a cooccurrence matrix.,3 PU-Learning for Word Embedding,[0],[0]
"Follow the literature (Levy and Goldberg, 2014a), we use the PPMI metric to measure the co-occurrence between words.",3 PU-Learning for Word Embedding,[0],[0]
"Then, in the second step, a PU-Learning approach is applied to factorize the co-occurrence matrix and generate word vectors and context vectors.",3 PU-Learning for Word Embedding,[0],[0]
"Finally, a post-processing step generates the final embedding vector for each word by combining the word vector and the context vector.
",3 PU-Learning for Word Embedding,[0],[0]
We summarize the notations used in this paper in Table 1 and describe the details of each step in the remainder of this section.,3 PU-Learning for Word Embedding,[0],[0]
Various metrics can be used for estimating the co-occurrence between words in a corpus.,3.1 Building the Co-Occurrence Matrix,[0],[0]
"PPMI metric stems from point-wise mutual information (PMI) which has been widely used as a measure of word association in NLP for various tasks (Church and Hanks, 1990).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In our case, each entry PMI(w, c) represents the relevant measure between a word w and a context word c by calculating the ratio between their joint probability (the
chance they appear together in a local context window) and their marginal probabilities (the chance they appear independently) (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"More specifically, each entry of PMI matrix can be defined by
PMI(w, c) = log P̂ (w, c)
P̂ (w) ·",3.1 Building the Co-Occurrence Matrix,[0],[0]
"P̂ (c) , (1)
where P̂ (w), P̂ (c) and P̂ (w, c) are the the frequency of word w, word c, and word pairs (w, c), respectively.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"The PMI matrix can be computed based on the co-occurrence counts of word pairs, and it is an information-theoretic association measure which effectively eliminates the big differences in magnitude among entries in the cooccurrence matrix.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Extending from the PMI metric, the PPMI metric replaces all the negative entries in PMI matrix by 0:
PPMI(w, c) = max(PMI(w, c), 0).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"(2)
The intuition behind this is that people usually perceive positive associations between words (e.g. “ice” and “snow”).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In contrast, the negative association is hard to define (Levy and Goldberg, 2014b).",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Therefore, it is reasonable to replace the negative entries in the PMI matrix by 0, such that the negative association is treated as “uninformative”.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Empirically, several existing works (Levy et al., 2015; Bullinaria and Levy, 2007) showed that the PPMI metric achieves good performance on various semantic similarity tasks.
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"In practice, we follow the pipeline described in Levy et al. (2015) to build the PPMI matrix and apply several useful tricks to improve its quality.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"First, we apply a context distribution smoothing mechanism to enlarge the probability of sampling a rare context.",3.1 Building the Co-Occurrence Matrix,[0.9565258414326406],"['To embed a given a sentence containing a VNC token instance, we average the word embeddings for each word in the sentence, including stopwords.4 Prior to averaging, we normalize each embedding to have unit length.']"
"In particular, all context counts are scaled to the power of α.4:
PPMIα(w, c) = max
( log P̂ (w, c)
P̂ (w)P̂α(c) , 0
)
P̂α(c) = #(c)α∑ c̄ #(c̄) α ,
where #(w) denotes the number of times word w appears.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"This smoothing mechanism effectively
4Empirically, α = 0.75 works well (Mikolov et al., 2013b).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"alleviates PPMI’s bias towards rare words (Levy et al., 2015).
",3.1 Building the Co-Occurrence Matrix,[0],[0]
"Next, previous studies show that words that occur too frequent often dominate the training objective (Levy et al., 2015) and degrade the performance of word embedding.",3.1 Building the Co-Occurrence Matrix,[0],[0]
"To avoid this issue, we follow Levy et al. (2015) to sub-sample words with frequency more than a threshold twith a probability p defined as:
p = 1− √ t
P̂ (w) .",3.1 Building the Co-Occurrence Matrix,[0],[0]
We proposed a matrix factorization based word embedding model which aims to minimize the reconstruction error on the PPMI matrix.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"The lowrank embeddings are obtained by solving the following optimization problem:
min W,H
∑
i,j∈Ω",3.2 PU-Learning for Matrix Factorization,[0],[0]
Cij(Aij −wTi hj,3.2 PU-Learning for Matrix Factorization,[0],[0]
"− bi − b̂j)2 + ∑
i
λi‖wi‖2 + ∑
j
λj‖hj‖2, (3)
where W and H are m× k and n× k latent matrices, representing words and context words, respectively.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The first term in Eq.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(3) aims for minimizing reconstruction error, and the second and third terms are regularization terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
λi and λj are weights of regularization term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"They are hyperparameters that need to be tuned.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The zero entries in co-occurrence matrix denote that two words never appear together in the current corpus, which also refers to unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
The unobserved term can be either real zero (two words shouldn’t be co-occurred even when we use very large corpus) or just missing in the small corpus.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In contrast to SGNS sub-sampling a small set of zero entries as negative samples, our model will try to use the information from all zeros.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The set Ω includes all the |W| × |C| entries— both positive and zero entries:
Ω = Ω+ ∪ Ω−. (4)
Note that we define the positive samples Ω+ to be all the (w, c) pairs that appear at least one time in the corpus, and negative samples Ω− are word pairs that never appear in the corpus.
Weighting function.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Eq (3) is very similar to the one used in previous matrix factorization approaches such as GloVe, but we propose a new way to set the weights Cij .",3.2 PU-Learning for Matrix Factorization,[0],[0]
"If we set equal weights for all the entries, then Cij = constant, and the model is very similar to conducting SVD for the PPMI matrix.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Previous work has shown that this approach often suffers from poor performance (Pennington et al., 2014).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"More advanced methods, such as GloVe, set non-uniform weights for observed entries to reflect their confidence.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"However, the time complexity of their algorithm is proportional to number of nonzero weights (|(i, j) | Cij 6= 0|), thus they have to set zero weights for all the unobserved entries (Cij = 0 for Ω−), or try to incorporate a small set of unobserved entries by negative sampling.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We propose to set the weights for Ω+ and Ω−
differently using the following scheme:
Cij =   (Qij/xmax)",3.2 PU-Learning for Matrix Factorization,[0],[0]
"α, if Qij ≤ xmax, and (i, j) ∈ Ω+
1,",3.2 PU-Learning for Matrix Factorization,[0],[0]
"if Qij > xmax, and (i, j) ∈ Ω+",3.2 PU-Learning for Matrix Factorization,[0],[0]
"ρ, (i, j) ∈ Ω−
(5)
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Here xmax and α are re-weighting parameters, and ρ is the unified weight for unobserved terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We will discuss them later.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω+, we set the non-uniform weights as in GloVe (Pennington et al., 2014), which assigns larger weights to context word that appears more often with the given word, but also avoids overwhelming the other terms.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"For entries in Ω−, instead of setting their weights to be 0, we assign a small constant weight ρ.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"The main idea is from the literature of PU-learning (Hu et al., 2008; Hsieh et al., 2015): although missing entries are highly uncertain, they are still likely to be true 0, so we should incorporate them in the learning process but multiplying with a smaller weight according to the uncertainty.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Therefore, ρ in (5) reflects how confident we are to the zero entries.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In our experiments, we set xmax = 10, α = 3/4 according to (Pennington et al., 2014), and let ρ be a parameter to tune.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Experiments show that adding weighting function obviously improves the performance especially on analogy tasks.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
Bias term.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"Unlike previous work on PU matrix completion (Yu et al., 2017b; Hsieh et al., 2015), we add the bias terms for word and context word
vectors.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Instead of directly using w>i hj to approximate Aij , we use
Aij ≈ w>i hj + bi + b̂j .
Yu et al. (2017b) design an efficient columnwise coordinate descent algorithm for solving the PU matrix factorization problem; however, they do not consider the bias term in their implementations.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"To incorporate the bias term in (3), we propose the following training algorithm based on the coordinate descent approach.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Our algorithm does not introduce much overhead compared to that in (Yu et al., 2017b).
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"We augment each wi,hj ∈ Rk into the following (k + 2) dimensional vectors:
w′i =   wi1 ...",3.2 PU-Learning for Matrix Factorization,[0],[0]
"wik 1 bi   h′j =   hj1 ... hjk b̂j 1  
Therefore, for each word and context vector, we have the following equality
〈w′i,h′j〉 = 〈wi,hj〉+ bi + b̂j ,
which means the loss function in (3) can be written as
∑
i,j∈Ω Cij(Aij −w′>i h′j)2.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"Also, we denote W ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[w′1,w ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",w ′ n] > and H ′ =",3.2 PU-Learning for Matrix Factorization,[0],[0]
"[h′1,h ′ 2, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
",h ′ n]",3.2 PU-Learning for Matrix Factorization,[0],[0]
>.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the column-wise coordinate descent method, at each iteration we pick a t ∈ {1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", (k+2)}, and update the t-th column of W ′ and H ′. The updates can be derived for the following two cases:
a. When t ≤ k, the elements in the t-th column is w1t, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", wnt and we can directly use the update rule derived in Yu et al. (2017b) to update them.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
b.,3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k + 1, we do not update the corresponding column of W ′ since the elements are all 1, and we use the similar coordinate descent update to update the k+ 1-th column of H ′ (corresponding to b̂1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", b̂n).",3.2 PU-Learning for Matrix Factorization,[0],[0]
"When t = k+2, we do not update the corresponding column of H ′",3.2 PU-Learning for Matrix Factorization,[0],[0]
(they are all 1) and we update the k+ 2-th column of W ′,3.2 PU-Learning for Matrix Factorization,[0],[0]
"(corresponding to b1, . . .",3.2 PU-Learning for Matrix Factorization,[0],[0]
", bn) using coordinate descent.
",3.2 PU-Learning for Matrix Factorization,[0],[0]
"With some further derivations, we can show that the algorithm only requires O(nnz(A) + nk) time to update each column,5 so the overall complexity is O(nnz(A)k + nk2) time per epoch, which is only proportional to number of nonzero terms in A. Therefore, with the same time complexity as GloVe, we can utilize the information from all the zero entries in A instead of only sub-sampling a small set of zero entries.",3.2 PU-Learning for Matrix Factorization,[0],[0]
"In the PU-Learning formulation, ρ represents the unified weight that assigned to the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"Intuitively, ρ reflects the confidence on unobserved entries—larger ρmeans that we are quite certain about the zeroes, while small ρ indicates the many of unobserved pairs are not truly zero.",3.3 Interpretation of Parameters,[0],[0]
"When ρ = 0, the PU-Learning approach reduces to a model similar to GloVe, which discards all the unobserved terms.",3.3 Interpretation of Parameters,[0],[0]
"In practice, ρ is an important parameter to tune, and we find that ρ = 0.0625 achieves the best results in general.",3.3 Interpretation of Parameters,[0],[0]
"Regarding the other parameter, λ is the regularization term for preventing the embedding model from overfitting.",3.3 Interpretation of Parameters,[0],[0]
"In practice, we found the performance is not very sensitive to λ as long as it is resonably small.",3.3 Interpretation of Parameters,[0],[0]
"More discussion about the parameter setting can be found in Section 5.
",3.3 Interpretation of Parameters,[0],[0]
Post-processing of Word/Context Vectors The PU-Learning framework factorizes the PPMI matrix and generates two vectors for each word,3.3 Interpretation of Parameters,[0],[0]
"i, wi ∈ Rk and hi ∈ Rk.",3.3 Interpretation of Parameters,[0],[0]
The former represents the word when it is the central word and the latter represents the word when it is in context.,3.3 Interpretation of Parameters,[0],[0]
Levy et al. (2015) shows that averaging these two vectors (uavgi = wi + hi) leads to consistently better performance.,3.3 Interpretation of Parameters,[0],[0]
The same trick of constructing word vectors is also used in GloVe.,3.3 Interpretation of Parameters,[0],[0]
"Therefore, in the experiments, we evaluate all models with uavg.",3.3 Interpretation of Parameters,[0],[0]
Our goal in this paper is to train word embedding models for low-resource languages.,4 Experimental Setup,[0],[0]
"In this section, we describe the experimental designs to evaluate the proposed PU-learning approach.",4 Experimental Setup,[0],[0]
We first describe the data sets and the evaluation metrics.,4 Experimental Setup,[0],[0]
"Then, we provide details of parameter tuning.
",4 Experimental Setup,[0],[0]
5Here we assume m = n for the sake of simplicity.,4 Experimental Setup,[0],[0]
"And, nnz(A) denotes the number of nonzero terms in the matrix A.",4 Experimental Setup,[0],[0]
"We consider two widely used tasks for evaluating word embeddings, the word similarity task and the word analogy task.",4.1 Evaluation tasks,[0],[0]
"In the word similarity task, each question contains a word pairs and an annotated similarity score.",4.1 Evaluation tasks,[0],[0]
The goal is to predict the similarity score between two words based on the inner product between the corresponding word vectors.,4.1 Evaluation tasks,[0],[0]
"The performance is then measured by the Spearmans rank correlation coefficient, which estimates the correlation between the model predictions and human annotations.",4.1 Evaluation tasks,[0],[0]
"Following the settings in literature, the experiments are conducted on five data sets, WordSim353 (Finkelstein et al., 2001), WordSim Similarity (Zesch et al., 2008), WordSim Relatedness (Agirre et al., 2009), Mechanical Turk (Radinsky et al., 2011) and MEN (Bruni et al., 2012).
",4.1 Evaluation tasks,[0],[0]
"In the word analogy task, we aim at solving analogy puzzles like “man is to woman as king is to ?”, where the expected answer is “queen.”",4.1 Evaluation tasks,[0],[0]
"We consider two approaches for generating answers to the puzzles, namely 3CosAdd and 3CosMul (see (Levy and Goldberg, 2014a) for details).",4.1 Evaluation tasks,[0],[0]
"We evaluate the performances on Google analogy dataset (Mikolov et al., 2013a) which contains 8,860 semantic and 10,675 syntactic questions.",4.1 Evaluation tasks,[0],[0]
"For the analogy task, only the answer that exactly matches the annotated answer is counted as correct.",4.1 Evaluation tasks,[0],[0]
"As a result, the analogy task is more difficult than the similarity task because the evalu-
ation metric is stricter and it requires algorithms to differentiate words with similar meaning and find the right answer.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performances of models in the low-resource setting, we train word embedding models on Dutch, Danish, Czech and, English data sets collected from Wikipedia.",4.1 Evaluation tasks,[0],[0]
"The original Wikipedia corpora in Dutch, Danish, Czech and English contain 216 million, 47 million, 92 million, and 1.8 billion tokens, respectively.",4.1 Evaluation tasks,[0],[0]
"To simulate the low-resource setting, we sub-sample the Wikipedia corpora and create a subset of 64 million tokens for Dutch and Czech and a subset of 32 million tokens for English.",4.1 Evaluation tasks,[0],[0]
"We will demonstrate how the size of the corpus affects the performance of embedding models in the experiments.
",4.1 Evaluation tasks,[0],[0]
"To evaluate the performance of word embeddings in Czech, Danish, and Dutch, we translate the English similarity and analogy test sets to the other languages by using Google Cloud Translation API6.",4.1 Evaluation tasks,[0],[0]
"However, an English word may be translated to multiple words in another language (e.g., compound nouns).",4.1 Evaluation tasks,[0],[0]
We discard questions containing such words (see Table 3 for details).,4.1 Evaluation tasks,[0],[0]
"Because all approaches are compared on the same test set for each language, the comparisons are fair.",4.1 Evaluation tasks,[0],[0]
"We compare the proposed approach with two baseline methods, GloVe and SGNS.",4.2 Implementation and Parameter Setting,[0],[0]
"The imple-
6https://cloud.google.com/translate
mentations of Glove7 and SGNS8 and provided by the original authors, and we apply the default settings when appropriate.",4.2 Implementation and Parameter Setting,[0],[0]
The proposed PULearning framework is implemented based on Yu et al. (2017a).,4.2 Implementation and Parameter Setting,[0],[0]
"With the implementation of efficient update rules, our model requires less than 500 seconds to perform one iteration over the entire text8 corpus, which consists of 17 million tokens 9.",4.2 Implementation and Parameter Setting,[0],[0]
"All the models are implemented in C++.
",4.2 Implementation and Parameter Setting,[0],[0]
"We follow Levy et al. (2015)10 to set windows size as 15, minimal count as 5, and dimension of word vectors as 300 in the experiments.",4.2 Implementation and Parameter Setting,[0],[0]
Training word embedding models involves selecting several hyper-parameters.,4.2 Implementation and Parameter Setting,[0],[0]
"However, as the word embeddings are usually evaluated in an unsupervised setting (i.e., the evaluation data sets are not seen during the training), the parameters should not be tuned on each dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"To conduct a fair comparison, we tune hyper-parameters on the text8 dataset.",4.2 Implementation and Parameter Setting,[0],[0]
"For GloVe model, we tune the discount parameters xmax and find that xmax = 10 per-
7https://nlp.stanford.edu/projects/glove 8https://code.google.com/archive/p/word2vec/ 9http://mattmahoney.net/dc/text8.zip
10https://bitbucket.org/omerlevy/hyperwords
forms the best.",4.2 Implementation and Parameter Setting,[0],[0]
SGNS has a natural parameter k which denotes the number of negative samples.,4.2 Implementation and Parameter Setting,[0],[0]
"Same as Levy et al. (2015), we found that setting k to 5 leads to the best performance.",4.2 Implementation and Parameter Setting,[0],[0]
"For the PU-learning model, ρ and λ are two important parameters that denote the unified weight of zero entries and the weight of regularization terms, respectively.",4.2 Implementation and Parameter Setting,[0],[0]
We tune ρ in a range from 2−1 to 2−14 and λ in a range from 20 to 2−10.,4.2 Implementation and Parameter Setting,[0],[0]
We analyze the sensitivity of the model to these hyper-parameters in the experimental result section.,4.2 Implementation and Parameter Setting,[0],[0]
The best performance of each model on the text8 dataset is shown in the Table 2.,4.2 Implementation and Parameter Setting,[0],[0]
It shows that PU-learning model outperforms two baseline models.,4.2 Implementation and Parameter Setting,[0],[0]
"We compared the proposed PU-Learning framework with two popular word embedding models – SGNS (Mikolov et al., 2013b) and Glove (Pennington et al., 2014) on English and three other languages.",5 Experimental Results,[0],[0]
The experimental results are reported in Table 4.,5 Experimental Results,[0],[0]
The results show that the proposed PULearning framework outperforms the two baseline approaches significantly in most datasets.,5 Experimental Results,[0],[0]
"This re-
sults confirm that the unobserved word pairs carry important information and the PU-Learning model leverages such information and achieves better performance.",5 Experimental Results,[0],[0]
"To better understand the model, we conduct detailed analysis as follows.
",5 Experimental Results,[0],[0]
"Performance v.s. Corpus size We investigate the performance of our algorithm with respect to different corpus size, and plot the results in Figure 1.",5 Experimental Results,[0],[0]
"The results in analogy task are obtained by 3CosMul method (Levy and Goldberg, 2014a).",5 Experimental Results,[0],[0]
"As the corpus size grows, the performance of all models improves, and the PU-learning model consistently outperforms other methods in all the tasks.",5 Experimental Results,[0],[0]
"However, with the size of the corpus increases, the difference becomes smaller.",5 Experimental Results,[0],[0]
"This is reasonable as when the corpus size increases the number of nonzero terms becomes smaller and the PU-learning approach is resemblance to Glove.
",5 Experimental Results,[0],[0]
Impacts of ρ and λ,5 Experimental Results,[0],[0]
"We investigate how sensitive the model is to the hyper-parameters, ρ and λ.",5 Experimental Results,[0],[0]
"Figure 2 shows the performance along with various values of λ and ρ when training on the text8 corpus, respectively.",5 Experimental Results,[0],[0]
Note that the x-axis is in log scale.,5 Experimental Results,[0],[0]
"When ρ is fixed, a big λ degrades the performance of the model significantly.",5 Experimental Results,[0],[0]
This is because when λ is too big the model suffers from underfitting.,5 Experimental Results,[0],[0]
"The model is less sensitive when λ is small and in general, λ = 2−11 achieves consistently good performance.
",5 Experimental Results,[0],[0]
"When λ is fixed, we observe that large ρ (e.g., ρ ≈ 2−4) leads to better performance.",5 Experimental Results,[0],[0]
"As ρ represents the weight assigned to the unobserved term, this result confirms that the model benefits from using the zero terms in the co-occurrences matrix.",5 Experimental Results,[0],[0]
"In this paper, we presented a PU-Learning framework for learning word embeddings of lowresource languages.",6 Conclusion,[0],[0]
"We evaluated the proposed approach on English and other three languages and showed that the proposed approach outperforms other baselines by effectively leveraging the information from unobserved word pairs.
",6 Conclusion,[0],[0]
"In the future, we would like to conduct experiments on other languages where available text corpora are relatively hard to obtain.",6 Conclusion,[0],[0]
"We are also interested in applying the proposed approach to domains, such as legal documents and clinical notes, where the amount of accessible data is small.",6 Conclusion,[0],[0]
"Besides, we plan to study how to leverage other information to facilitate the training of word embeddings under the low-resource setting.
",6 Conclusion,[0],[0]
"Acknowledge
This work was supported in part by National Science Foundation Grant IIS-1760523, IIS-1719097 and an NVIDIA Hardware Grant.",6 Conclusion,[0],[0]
Word embedding is a key component in many downstream applications in processing natural languages.,abstractText,[0],[0]
Existing approaches often assume the existence of a large collection of text for learning effective word embedding.,abstractText,[0],[0]
"However, such a corpus may not be available for some low-resource languages.",abstractText,[0],[0]
"In this paper, we study how to effectively learn a word embedding model on a corpus with only a few million tokens.",abstractText,[0],[0]
"In such a situation, the co-occurrence matrix is sparse as the co-occurrences of many word pairs are unobserved.",abstractText,[0],[0]
"In contrast to existing approaches often only sample a few unobserved word pairs as negative samples, we argue that the zero entries in the co-occurrence matrix also provide valuable information.",abstractText,[0],[0]
We then design a Positive-Unlabeled Learning (PU-Learning) approach to factorize the co-occurrence matrix and validate the proposed approaches in four different languages.,abstractText,[0],[0]
Learning Word Embeddings for Low-resource Languages by PU Learning,title,[0],[0]
"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 4829–4833 Brussels, Belgium, October 31 - November 4, 2018. c©2018 Association for Computational Linguistics
4829",text,[0],[0]
Co-reference resolution requires models to cluster mentions that refer to the same physical entities.,1 Introduction,[0],[0]
The models based on neural networks typically require different levels of semantic representations of input sentences.,1 Introduction,[0],[0]
"The models usually need to calculate the representations of word spans, or mentions, given pre-trained character and wordlevel embeddings (Turian et al., 2010; Pennington et al., 2014) before predicting antecedents.",1 Introduction,[0],[0]
"The mention-level embeddings are used to make coreference decisions, typically by scoring mention pairs and making links (Lee et al., 2017; Clark and Manning, 2016a; Wiseman et al., 2016).",1 Introduction,[0],[0]
"Long short-term memories (LSTMs) are often used to encode the syntactic and semantic information of input sentences.
",1 Introduction,[0],[0]
Articles and conversations include more than one sentences.,1 Introduction,[0],[0]
"Considering the accuracy and efficiency of co-reference resolution models, the encoder LSTM usually processes input sentences separately as a batch (Lee et al., 2017).",1 Introduction,[0],[0]
"The disadvantage of this method is that the models do not consider the dependency among words from different sentences, which plays a significant role in word representation learning and co-reference predicting.",1 Introduction,[0],[0]
"For example, pronouns are often linked to entities mentioned in other sentences, while their initial word vectors lack dependency information.",1 Introduction,[0],[0]
"As a result, a word representation model cannot learn an informative embedding of a pronoun without considering cross-sentence dependency in this case.
",1 Introduction,[0],[0]
It is also problematic if we encode the input document considering cross-sentence dependency and treat the entire document as one sentence.,1 Introduction,[0],[0]
An input article or conversation can be too long for a single LSTM cell to memorize.,1 Introduction,[0],[0]
"If the LSTM updates itself for too many steps, gradients will vanish or explode (Pascanu et al., 2013), and the coreference resolution model will be very difficult to optimize.",1 Introduction,[0],[0]
"Regarding the entire input corpus as one sequence instead of a batch also significantly increases the time complexity of the model.
",1 Introduction,[0],[0]
"To solve the problem that traditional LSTM encoders, which treat the input sentences as a batch, lack an ability to capture cross-sentence dependency, and to avoid the time complexity and difficulties of training the model concatenating all input sentences, we propose a cross-sentence encoder for end-to-end co-reference (E2E-CR).",1 Introduction,[0],[0]
"Borrowing the idea of an external memory module from Sukhbaatar et al. (2015), an external memory block containing syntactic and semantic information from context sentences is added to the standard LSTM model.",1 Introduction,[0],[0]
"With this context memory block, the proposed model is able to encode
input sentences as a batch, and also calculate the representations of input words by taking both target sentences and context sentences into consideration.",1 Introduction,[0],[0]
Experiments showed that this approach improved the performance of co-reference resolution models.,1 Introduction,[0],[0]
"A popular method of co-reference resolution is mention ranking (Durrett and Klein, 2013).",2.1 Co-reference Resolution,[0],[0]
"Reading each mention, the model calculates coreference scores for all antecedent mentions, and picks the mention with the highest positive score to be its co-reference.",2.1 Co-reference Resolution,[0],[0]
Many recent works are based on this approach.,2.1 Co-reference Resolution,[0],[0]
Durrett and Klein (2013) designed a set of feature templates to improve the mention-ranking model.,2.1 Co-reference Resolution,[0],[0]
Peng et al. (2015) proposed a mention-ranking model by jointly learning mention heads and co-references.,2.1 Co-reference Resolution,[0],[0]
Clark and Manning (2016a) proposed a reinforcement learning framework for the mention ranking approach.,2.1 Co-reference Resolution,[0],[0]
"Based on similar ideas but without using parsing features, the authors of Lee et al. (2017) proposed the current state-of-the-art model which uses neural networks to embed mentions and calculate mention and antecedent scores.",2.1 Co-reference Resolution,[0],[0]
"Lee et al. (2018) applied ELMo embeddings (Peters et al., 2018) to improve within-sentence dependency modeling and word representation learning.",2.1 Co-reference Resolution,[0],[0]
Wiseman et al. (2016) and Clark and Manning (2016b) proposed models using global entity-level features.,2.1 Co-reference Resolution,[0],[0]
"Distributed word embeddings has been used as the basic unit of language representation for over a decade (Bengio et al., 2003).",2.2 Language Representation Learning,[0],[0]
"Pre-trained word embeddings, for example GloVe (Pennington et al., 2014) and Skip-Gram (Mikolov et al., 2013) are widely used as the input of natural language processing models.
",2.2 Language Representation Learning,[0],[0]
"Long short-term memory (LSTM) networks (Hochreiter and Schmidhuber, 1997) are widely used for sentence modeling.",2.2 Language Representation Learning,[0],[0]
"A single-layer LSTM network was applied in the previous state-of-theart co-reference model (Lee et al., 2017) to generate word and mention representations.",2.2 Language Representation Learning,[0],[0]
"To capture dependency of longer distances, Campos et al. (2017) proposed a recurrent model that outputs hidden states by skipping input tokens.
",2.2 Language Representation Learning,[0],[0]
"Recently, memory networks (Sukhbaatar et al.,
2015) have been applied in language modeling (Cheng et al., 2016; Tran et al., 2016).",2.2 Language Representation Learning,[0],[0]
"Applying an attention mechanism on memory cells, memory networks allow the model to focus on significant words or segments for classification and generation tasks.",2.2 Language Representation Learning,[0],[0]
"Previous works have shown that applying memory blocks in LSTMs also improves longdistance dependency extraction (Yogatama et al., 2018).",2.2 Language Representation Learning,[0],[0]
"To improve the word representation learning model for better co-reference resolution performance, we propose two word representation models that learn cross-sentence dependency.",3 Learning Cross-Sentence dependency,[0],[0]
"Instead of treating the entire input document as separate sentences and encode the sentences as a batch with an LSTM, the most direct way to consider cross-sentence dependency is to initialize LSTM states with the encodings of adjacent sentences.",3.1 Linear Sentence Linking,[0],[0]
"We name this method linear sentence linking (LSL).
",3.1 Linear Sentence Linking,[0],[0]
"In LSL, we encode input sentences with a 2- layer bidirectional LSTM.",3.1 Linear Sentence Linking,[0],[0]
"Give input sentences [s1, s2 . . .",3.1 Linear Sentence Linking,[0],[0]
"sn], the outputs of the first layer are [[−→s 1;←−s 1],",3.1 Linear Sentence Linking,[0],[0]
"[−→s 2;←−s 2], . . .",3.1 Linear Sentence Linking,[0],[0]
[−→s n;←−s n]].,3.1 Linear Sentence Linking,[0],[0]
"In the second LSTM layer, the initial state of the forward LSTM of si is initialized as
−→ S i =",3.1 Linear Sentence Linking,[0],[0]
"[ −→c 20; [−→s i−1;←−s i−1]]
while the backward state is initialized as
←−",3.1 Linear Sentence Linking,[0],[0]
"S i = [ ←−c 20; [−→s i−1;←−s i−1]]
where ci0 stands for the initial cell of the ith layer, and x stands for the final output of the LSTMs in first layer.",3.1 Linear Sentence Linking,[0],[0]
We then concatenate the outputs of the forward and backward LSTMs in the second layer as the word representations for coreference prediction.,3.1 Linear Sentence Linking,[0],[0]
It is difficult for LSTMs to embed enough information about a long sentence into a lowdimensional distributed vector.,3.2 Attentional Sentence Linking,[0],[0]
"To collect richer knowledge from neighbor sentences, we propose a long short-term recurrent memory module and an attention mechanism to improve sentence linking.
",3.2 Attentional Sentence Linking,[0],[0]
"To describe the architecture of the proposed model, we focus on adjacent input sentences si−1
and si.",3.2 Attentional Sentence Linking,[0],[0]
"We present the input embeddings of the j-th word in the i-th sentence with xi,j .",3.2 Attentional Sentence Linking,[0],[0]
"To solve the traditional recurrent neural networks, Hochreiter and Schmidhuber (1997) proposed the LSTM architecture.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
"The detail of recurrent state updating in LSTMs ht = flstm(xt, ht−1, ct−1) is shown in following equations.
",3.2.1 Long Short-Term Memory RNNs,[0],[0]
it = σ(Wxixt +Whiht−1 + bi) ft = σ(Wxfxt,3.2.1 Long Short-Term Memory RNNs,[0],[0]
+Whfht−1 + bf ),3.2.1 Long Short-Term Memory RNNs,[0],[0]
ct = ft ct−1 + it tanh(Wxcxt +Whcht−1 + bc) ot = σ(Wxoxt +Whoht−1 + bo),3.2.1 Long Short-Term Memory RNNs,[0],[0]
"ht = ot tanh(ct)
where xt is the input embedding and ht is the output representation of the t-th word.",3.2.1 Long Short-Term Memory RNNs,[0],[0]
We design an LSTM module with cross-sentence attention for capturing cross-sentence dependency.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
We name this method attentional sentence linking (ASL).,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"Considering input word xi,t in the ith sentence and all words from the previous sentence Xi−1 =",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"[xi−1,1, xi−1,2, . . .",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
", xi−1,m], we regard the matrix Xi−1 as an external memory module and calculate an attention on its cells, where each cell contains a word embedding.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"αj = ecj∑ k e ck (1)
ck = fc([xi,t;ht−1;xi−1,k] T ) (2)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"With the attention distribution α, we can get a vector summarizing related information from si−1,
vi−1 = ∑ j αj · xi−1,j (3)
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The model decides if it needs to pay more attention on the current input or cross-sentence information with a context gate.
",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"gt = σ(fg([xi,t;ht−1; vi−1] T )) (4)
x̂i,t = gt · xi,t + (1− gt) · vi−1 (5)
σ",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
(·) stands for the Sigmoid function.,3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"The word representation of the target word is calculated as
hi,t = flstm(x̂i,t, hi,t−1, ci,t−1) (6)
where flstm stands for standard LSTM update described in section 3.2.1.",3.2.2 LSTMs with Cross-Sentence Attention,[0],[0]
"In this work, we apply the mention-ranking endto-end co-reference resolution (E2E-CR) model proposed by Lee et al. (2017) for co-reference prediction.",3.3 Co-reference Prediction,[0],[0]
The word representations applied in E2ECR model is formed by concatenating pre-trained word embeddings and the outputs of LSTMs.,3.3 Co-reference Prediction,[0],[0]
"In our work, we represent words by concatenating pre-trained word embeddings and the outputs of LSL- and ASL-LSTMs.",3.3 Co-reference Prediction,[0],[0]
"We train and evaluate our model on the English corpus of the CoNLL-2012 shared task (Pradhan et al., 2012).",4 Experiments,[0],[0]
"We implement our model based on the published implementation of the baseline E2ECR model (Lee et al., 2017) 1.",4 Experiments,[0],[0]
Our implementation is also available online for reproducing the results reported in this paper 2.,4 Experiments,[0],[0]
"In this section, we first describe our hyperparameter setup, and then show the experimental results of previous work and our proposed models.",4 Experiments,[0],[0]
"In practice, the LSTM modules applied in our model have 200 output units.",4.1 Model and Hyperparameter Setup,[0],[0]
"In ASL, we calculate cross-sentence dependency using a multilayer perceptron with one hidden layer consisting of 150 hidden units.",4.1 Model and Hyperparameter Setup,[0],[0]
The initial learning rate is set as 0.001 and decays 0.001% every 100 steps.,4.1 Model and Hyperparameter Setup,[0],[0]
"The model is optimized with the Adam algorithm (Kingma and Ba, 2014).",4.1 Model and Hyperparameter Setup,[0],[0]
We randomly select up to 40 continuous sentences for training if the input is too long.,4.1 Model and Hyperparameter Setup,[0],[0]
"In co-reference prediction, we select 250 candidate antecedents as our baseline model.",4.1 Model and Hyperparameter Setup,[0],[0]
We evaluate our model on the test set of the CoNLL-2012 shared task.,4.2 Experiment Results and Discussion,[0],[0]
The performance of previous work and our model are shown in Table 1.,4.2 Experiment Results and Discussion,[0],[0]
"We mainly focus on the average F1 score of MUC, B3, and CEAF metrics.",4.2 Experiment Results and Discussion,[0],[0]
"Comparing with the baseline model that achieved 67.2% F1 score, the ASL model improved the performance by 0.6% and achieved 67.8% average F1.",4.2 Experiment Results and Discussion,[0],[0]
"Experiments
1https://github.com/kentonl/e2e-coref 2https://github.com/luohongyin/
coatt-coref
show that the models that consider cross-sentence dependency significantly outperform the baseline model, which encodes each sentence from the input document separately.
",4.2 Experiment Results and Discussion,[0],[0]
"Experiments also indicated that the ASL model has better performance than the LSL model, since it summarizes extracts context information with an attention mechanism instead of simply viewing sentence-level embeddings.",4.2 Experiment Results and Discussion,[0],[0]
"This gives the model a better ability to model cross-sentence dependency.
",4.2 Experiment Results and Discussion,[0],[0]
Examples for comparing the performance of the ASL model and the baseline are shown in Table 2.,4.2 Experiment Results and Discussion,[0],[0]
Each example contains two continuous sentences with co-references distritubed in different sentences.,4.2 Experiment Results and Discussion,[0],[0]
Underlined spans in bold are target mentions and annotated co-references.,4.2 Experiment Results and Discussion,[0],[0]
"Spans in
green are ASL predictions, and spans in red are baseline predictions.",4.2 Experiment Results and Discussion,[0],[0]
"A prediction on “-” means that no mention is predicted as a co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"Table 2 shows that the baseline model, which does not consider cross-sentence dependency, has difficulty in learning the semantics of pronouns whose co-references are not in the same sentence.",4.2 Experiment Results and Discussion,[0],[0]
The pretrained embeddings of pronouns are not informative enough.,4.2 Experiment Results and Discussion,[0],[0]
"In the first example, “it” is not semantically similar with “SMS” in GloVe without any context, and in this case, “it” and “SMS” are in different sentences.",4.2 Experiment Results and Discussion,[0],[0]
"As a result, if reading this two sentences separately, it is hard for the encoder to represent “it” with the semantics of “SMS”.",4.2 Experiment Results and Discussion,[0],[0]
"This difficulty makes the co-reference resolution model either prediction a wrong antecedent mention, or cannot find any co-reference.
",4.2 Experiment Results and Discussion,[0],[0]
"However, with ASL, the model learns the semantics of pronouns with an attention to words in other sentences.",4.2 Experiment Results and Discussion,[0],[0]
"With the proposed context gate, ASL takes knowledge from context sentences if local inputs are not informative enough.",4.2 Experiment Results and Discussion,[0],[0]
"Based on word represents enhanced with cross-sentence dependency, the co-reference scoring model can make better predictions.",4.2 Experiment Results and Discussion,[0],[0]
We proposed linear and attentional sentence linking models for learning word representations that captures cross-sentence dependency.,5 Conclusion and Future Work,[0],[0]
"Experiments showed that the embeddings learned by proposed models successfully improved the performance of the state-of-the-art co-reference resolution model, indicating that cross-sentence dependency plays an important role in semantic learning in articles and conversations consists of multiple sentences.",5 Conclusion and Future Work,[0],[0]
"It worth exploring if our model can improve the performance of other natural language processing
applications whose inputs contain multiple sentences, for example, reading comprehension, dialog generation, and sentiment analysis.",5 Conclusion and Future Work,[0],[0]
"In this work, we present a word embedding model that learns cross-sentence dependency for improving end-to-end co-reference resolution (E2E-CR).",abstractText,[0],[0]
"While the traditional E2ECR model generates word representations by running long short-term memory (LSTM) recurrent neural networks on each sentence of an input article or conversation separately, we propose linear sentence linking and attentional sentence linking models to learn crosssentence dependency.",abstractText,[0],[0]
Both sentence linking strategies enable the LSTMs to make use of valuable information from context sentences while calculating the representation of the current input word.,abstractText,[0],[0]
"With this approach, the LSTMs learn word embeddings considering knowledge not only from the current sentence but also from the entire input document.",abstractText,[0],[0]
"Experiments show that learning cross-sentence dependency enriches information contained by the word representations, and improves the performance of the co-reference resolution model compared with our baseline.",abstractText,[0],[0]
Learning Word Representations with Cross-Sentence Dependency for End-to-End Co-reference Resolution,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 506–517 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1047",text,[0],[0]
"Automatically discovering words and other elements of linguistic structure from continuous speech has been a longstanding goal in computational linguists, cognitive science, and other speech processing fields.",1.1 Problem Statement and Motivation,[0],[0]
"Practically all humans acquire language at a very early age, but this task has proven to be an incredibly difficult problem for computers.",1.1 Problem Statement and Motivation,[0],[0]
"While conventional automatic speech recognition (ASR) systems have a long history and have recently made great strides thanks to the revival of deep neural networks (DNNs), their reliance on highly supervised training paradigms has essentially restricted their application to the major languages of the world, accounting for a small fraction of the more than 7,000 human languages spoken worldwide (Lewis et al., 2016).",1.1 Problem Statement and Motivation,[0],[0]
"The main
reason for this limitation is the fact that these supervised approaches require enormous amounts of very expensive human transcripts.",1.1 Problem Statement and Motivation,[0],[0]
"Moreover, the use of the written word is a convenient but limiting convention, since there are many oral languages which do not even employ a writing system.",1.1 Problem Statement and Motivation,[0],[0]
"In constrast, infants learn to communicate verbally before they are capable of reading and writing - so there is no inherent reason why spoken language systems need to be inseparably tied to text.
",1.1 Problem Statement and Motivation,[0],[0]
The key contribution of this paper has two facets.,1.1 Problem Statement and Motivation,[0],[0]
"First, we introduce a methodology capable of not only discovering word-like units from continuous speech at the waveform level with no additional text transcriptions or conventional speech recognition apparatus.",1.1 Problem Statement and Motivation,[0],[0]
"Instead, we jointly learn the semantics of those units via visual associations.",1.1 Problem Statement and Motivation,[0],[0]
"Although we evaluate our algorithm on an English corpus, it could conceivably run on any language without requiring any text or associated ASR capability.",1.1 Problem Statement and Motivation,[0],[0]
"Second, from a computational perspective, our method of speech pattern discovery runs in linear time.",1.1 Problem Statement and Motivation,[0],[0]
"Previous work has presented algorithms for performing acoustic pattern discovery in continuous speech (Park and Glass, 2008; Jansen et al., 2010; Jansen and Van Durme, 2011) without the use of transcriptions or another modality, but those algorithms are limited in their ability to scale by their inherent O(n2) complexity, since they do an exhaustive comparison of the data against itself.",1.1 Problem Statement and Motivation,[0],[0]
Our method leverages correlated information from a second modality - the visual domain - to guide the discovery of words and phrases.,1.1 Problem Statement and Motivation,[0],[0]
"This enables our method to run in O(n) time, and we demonstrate it scalability by discovering acoustic patterns in over 522 hours of audio.",1.1 Problem Statement and Motivation,[0],[0]
"A sub-field within speech processing that has garnered much attention recently is unsupervised
506
speech pattern discovery.",1.2 Previous Work,[0],[0]
"Segmental Dynamic Time Warping (S-DTW) was introduced by Park and Glass (2008), which discovers repetitions of the same words and phrases in a collection of untranscribed acoustic data.",1.2 Previous Work,[0],[0]
"Many subsequent efforts extended these ideas (Jansen et al., 2010; Jansen and Van Durme, 2011; Dredze et al., 2010; Harwath et al., 2012; Zhang and Glass, 2009).",1.2 Previous Work,[0],[0]
"Alternative approaches based on Bayesian nonparametric modeling (Lee and Glass, 2012; Ondel et al., 2016) employed a generative model to cluster acoustic segments into phoneme-like categories, and related works aimed to segment and cluster either reference or learned phonemelike tokens into higher-level units (Johnson, 2008; Goldwater et al., 2009; Lee et al., 2015).
",1.2 Previous Work,[0],[0]
"While supervised object detection is a standard problem in the vision community, several recent works have tackled the problem of weaklysupervised or unsupervised object localization (Bergamo et al., 2014; Cho et al., 2015; Zhou et al., 2015; Cinbis et al., 2016).",1.2 Previous Work,[0],[0]
"Although the focus of this work is discovering acoustic patterns, in the process we jointly associate the acoustic patterns with clusters of image crops, which we demonstrate capture visual patterns as well.
",1.2 Previous Work,[0],[0]
The computer vision and NLP communities have begun to leverage deep learning to create multimodal models of images and text.,1.2 Previous Work,[0],[0]
"Many works have focused on generating annotations or text captions for images (Socher and Li, 2010; Frome et al., 2013; Socher et al., 2014; Karpathy et al., 2014; Karpathy and Li, 2015; Vinyals et al., 2015; Fang et al., 2015; Johnson et al., 2016).",1.2 Previous Work,[0],[0]
"One interesting intersection between word induction from phoneme strings and multimodal modeling of images and text is that of Gelderloos and Chrupaa (2016), who uses images to segment words within captions at the phoneme string level.",1.2 Previous Work,[0],[0]
"Other work has taken these ideas beyond text, and attempted to relate images to spoken audio captions directly at the waveform level (Roy, 2003; Harwath and Glass, 2015; Harwath et al., 2016).",1.2 Previous Work,[0],[0]
"The work of (Harwath et al., 2016) is the most similar to ours, in which the authors learned embeddings at the entire image and entire spoken caption level and then used the embeddings to perform bidirectional retrieval.",1.2 Previous Work,[0],[0]
"In this work, we go further by automatically segmenting and clustering the spoken captions into individual word-like units, as well as the images into object-like categories.",1.2 Previous Work,[0],[0]
"We employ a corpus of over 200,000 spoken captions for images taken from the Places205 dataset (Zhou et al., 2014), corresponding to over 522 hours of speech data.",2 Experimental Data,[0],[0]
"The captions were collected using Amazon’s Mechanical Turk service, in which workers were shown images and asked to describe them verbally in a free-form manner.",2 Experimental Data,[0],[0]
"The data collection scheme is described in detail in Harwath et al. (2016), but the experiments in this paper leverage nearly twice the amount of data.",2 Experimental Data,[0],[0]
"For training our multimodal neural network as well as the pattern discovery experiments, we use a subset of 214,585 image/caption pairs, and we hold out a set of 1,000 pairs for evaluating the multimodal network’s retrieval ability.",2 Experimental Data,[0],[0]
"Because we lack ground truth text transcripts for the data, we used Google’s Speech Recognition public API to generate proxy transcripts which we use when analyzing our system.",2 Experimental Data,[0],[0]
"Note that the ASR was only used for analysis of the results, and was not involved in any of the learning.",2 Experimental Data,[0],[0]
"We first train a deep multimodal embedding network similar in spirit to the one described in Harwath et al. (2016), but with a more sophisticated architecture.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is trained to map entire image frames and entire spoken captions into a shared embedding space; however, as we will show, the trained network can then be used to localize patterns corresponding to words and phrases within the spectrogram, as well as visual objects within the image by applying it to small sub-regions of the image and spectrogram.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The model is comprised of two branches, one which takes as input images, and the other which takes as input spectrograms.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The image network is formed by taking the off-the-shelf VGG 16 layer network (Simonyan and Zisserman, 2014) and replacing the softmax classification layer with a linear transform which maps the 4096-dimensional activations of the second fully connected layer into our 1024-dimensional multimodal embedding space.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"In our experiments, the weights of this projection layer are trained, but the layers taken from the VGG network below it are kept fixed.",3 Audio-Visual Embedding Neural Networks,[0],[0]
The second branch of our network analyzes speech spectrograms as if they were black and white images.,3 Audio-Visual Embedding Neural Networks,[0],[0]
"Our spectrograms are computed using 40 log Mel
filterbanks with a 25ms Hamming window and a 10ms shift.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The input to this branch always has 1 color channel and is always 40 pixels high (corresponding to the 40 Mel filterbanks), but the width of the spectrogram varies depending upon the duration of the spoken caption, with each pixel corresponding to approximately 10 milliseconds worth of audio.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The architecture we use is entirely convolutional and shown below, where C denotes the number of convolutional channels, W is filter width, H is filter height, and S is pooling stride.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
1.,3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=128, W=1, H=40, ReLU 2.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=256, W=11, H=1, ReLU 3.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 4.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 5.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 6.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=512, W=17, H=1, ReLU 7.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Maxpool: W=3, H=1, S=2 8.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Convolution: C=1024, W=17, H=1, ReLU 9.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Meanpool over entire caption
10.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"L2 normalization In practice during training, we restrict the caption spectrograms to all be 1024 frames wide (i.e., 10sec of speech) by applying truncation or zero padding.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Additionally, both the images and spectrograms are mean normalized before training.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The overall multimodal network is formed by tying together the image and audio branches with a layer which takes both of their output vectors and computes an inner product between them, representing the similarity score between a given image/caption pair.",3 Audio-Visual Embedding Neural Networks,[0.9508152576930219],"['+CF further incorporates lexico-syntactic knowledge of canonical forms into each model by concatenating the embedding representing each VNC token instance with a one-dimensional vector which is one if the VNC occurs in its canonical form, and zero otherwise.']"
"We train the network to assign high scores to matching image/caption pairs, and lower scores to mismatched pairs.
",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Within a minibatch of B image/caption pairs, let Spj , j = 1, . . .",3 Audio-Visual Embedding Neural Networks,[0],[0]
", B denote the similarity score of the jth image/caption pair as output by the neural network.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Next, for each pair we randomly sample one impostor caption and one impostor image from the same minibatch.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Let Sij denote the similarity score between the jth caption and its impostor image, and Scj be the similarity score between the jth image and its impostor caption.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"The total loss for the entire minibatch is then computed as
L(θ) = B∑
j=1
[max(0, Scj − Spj + 1)
+ max(0,",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Sij − Spj + 1)] (1)
We train the neural network with 50 epochs of stochastic gradient descent using a batch size B =
128, a momentum of 0.9, and a learning rate of 1e5 which is set to geometrically decay by a factor between 2 and 5 every 5 to 10 epochs.",3 Audio-Visual Embedding Neural Networks,[0],[0]
"Although we have trained our multimodal network to compute embeddings at the granularity of entire images and entire caption spectrograms, we can easily apply it in a more localized fashion.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the case of images, we can simply take any arbitrary crop of an original image and resize it to 224x224 pixels.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The audio network is even more trivial to apply locally, because it is entirely convolutional and the final mean pooling layer ensures that the output will be a 1024-dim vector no matter the extent of the input.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The bigger question is where to locally apply the networks in order to discover meaningful acoustic and visual patterns.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Given an image and its corresponding spoken audio caption, we use the term grounding to refer to extracting meaningful segments from the caption and associating them with an appropriate subregion of the image.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"For example, if an image depicted a person eating ice cream and its caption contained the spoken words “A person is enjoying some ice cream,” an ideal set of groundings would entail the acoustic segment containing the word “person” linked to a bounding box around the person, and the segment containing the word “ice cream” linked to a box around the ice cream.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use a constrained brute force ranking scheme to evaluate all possible groundings (with a restricted granularity) between an image and its caption.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Specifically, we divide the image into a grid, and extract all of the image crops whose boundaries sit on the grid lines.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Because we are mainly interested in extracting regions of interest and not high precision object detection boxes, to keep the number of proposal regions under control we impose several restrictions.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"First, we use a 10x10 grid on each image regardless of its original size.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Second, we define minimum and maximum aspect ratios as 2:3 and 3:2 so as not to introduce too much distortion and also to reduce the number of proposal boxes.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Third, we define a minimum bounding width as 30% of the original image width, and similarly a minimum height as 30% of the original image height.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In practice, this results in a few thousand proposal regions per image.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To extract proposal segments from the audio
caption spectrogram, we similarly define a 1-dim grid along the time axis, and consider all possible start/end points at 10 frame (pixel) intervals.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We impose minimum and maximum segment length constraints at 50 and 100 frames (pixels), implying that our discovered acoustic patterns are restricted to fall between 0.5 and 1 second in duration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The number of proposal segments will vary depending on the caption length, and typically number in the several thousands.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Note that when learning groundings we consider the entire audio sequence, and do not incorporate the 10sec duration constraint imposed during training.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Once we have extracted a set of proposed visual bounding boxes and acoustic segments for a given image/caption pair, we use our multimodal network to compute a similarity score between each unique image crop/acoustic segment pair.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Each triplet of an image crop, acoustic segment, and similarity score constitutes a proposed grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"A naive approach would be to simply keep the top N groundings from this list, but in practice we ran into two problems with this strategy.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"First, many proposed acoustic segments capture mostly silence due to pauses present in natural speech.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We solve this issue by using a simple voice activity detector (VAD) which was trained on the TIMIT corpus(Garofolo et al., 1993).",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the VAD estimates that 40% or more of any proposed acoustic segment is silence, we discard that entire grounding.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
The second problem we ran into is the fact that the top of the sorted grounding list is dominated by highly overlapping acoustic segments.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"This makes sense, because highly informative content words will show up in many different groundings with slightly perturbed start or end times.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"To alleviate this issue, when evaluating a grounding from the top of the proposal list we compare the interval intersection over union (IOU) of its acoustic segment against all acoustic segments already accepted for further consideration.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"If the IOU exceeds a threshold of 0.1, we discard the new grounding and continue moving down the list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We stop accumulating groundings once the scores fall to below 50% of the top score in the “keep” list, or when 10 groundings have been added to the “keep” list.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Figure 1 displays a pictorial example of our grounding procedure.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"Once we have completed the grounding procedure, we are left with a small set of regions of interest in each image and caption spectrogram.
",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We use the respective branches of our multimodal network to compute embedding vectors for each grounding’s image crop and acoustic segment.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
We then employ k-means clustering separately on the collection of image embedding vectors as well as the collection of acoustic embedding vectors.,4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"The last step is to establish an affinity score between each image cluster I and each acoustic cluster A; we do so using the equation
Affinity(I,A) = ∑
i∈I
∑ a∈A",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"i>a · Pair(i,a) (2)
where i is an image crop embedding vector, a is an acoustic segment embedding vector, and Pair(i,a) is equal to 1",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"when i and a belong to the same grounding pair, and 0 otherwise.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"After clustering, we are left with a set of acoustic pattern clusters, a set of visual pattern clusters, and a set of linkages describing which acoustic clusters are associated with which image clusters.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"In the next section, we investigate these clusters in more detail.",4 Finding and Clustering Audio-Visual Caption Groundings,[0],[0]
"We trained our multimodal network on a set of 214,585 image/caption pairs, and vetted it with an image search (given caption, find image) and annotation (given image, find caption) task similar to the one used in Harwath et al. (2016); Karpathy et al. (2014); Karpathy and Li (2015).",5 Experiments and Analysis,[0],[0]
"The image annotation and search recall scores on a 1,000 image/caption pair held-out test set are shown in Table 1.",5 Experiments and Analysis,[0],[0]
"Also shown in this table are the scores
achieved by a model which uses the ASR text transcriptions for each caption instead of the speech audio.",5 Experiments and Analysis,[0],[0]
"The text captions were truncated/padded to 20 words, and the audio branch of the network was replaced with a branch with the following architecture:
1.",5 Experiments and Analysis,[0],[0]
"Word embedding layer of dimension 200
2.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=512, W=3, ReLU 3.",5 Experiments and Analysis,[0],[0]
"Temporal Convolution: C=1024, W=3 4.",5 Experiments and Analysis,[0],[0]
Meanpool over entire caption 5.,5 Experiments and Analysis,[0],[0]
"L2 normalization
One would expect that access to ASR hypotheses should improve the recall scores, but the performance gap is not enormous.",5 Experiments and Analysis,[0],[0]
"Access to the ASR hypotheses provides a relative improvement of approximately 21.8% for image search R@10 and 12.5% for annotation R@10 compared to using no transcriptions or ASR whatsoever.
",5 Experiments and Analysis,[0],[0]
"We performed the grounding and pattern clustering steps on the entire training dataset, which resulted in a total of 1,161,305 unique grounding pairs.",5 Experiments and Analysis,[0],[0]
"For evaluation, we wish to assign a label to each cluster and cluster member, but this is not completely straightforward since each acoustic segment may capture part of a word, a whole word, multiple words, etc.",5 Experiments and Analysis,[0],[0]
"Our strategy is to forcealign the Google recognition hypothesis text to the audio, and then assign a label string to each acoustic segment based upon which words it overlaps in time.",5 Experiments and Analysis,[0],[0]
"The alignments are created with the help of a Kaldi (Povey et al., 2011) speech recognizer
Table 3:",5 Experiments and Analysis,[0],[0]
Top 50 clusters with k = 500 sorted by increasing variance.,5 Experiments and Analysis,[0],[0]
"Legend: |Cc| is acoustic cluster size, |Ci| is associated image cluster size, Pur. is acoustic cluster purity, σ2 is acoustic cluster variance, and Cov. is acoustic cluster coverage.",5 Experiments and Analysis,[0],[0]
"A dash (-) indicates a cluster whose majority label is silence.
",5 Experiments and Analysis,[0],[0]
"Trans |Cc| |Ci| Pur. σ2 Cov. Trans |Cc| |Ci| Pur. σ2 Cov. - 1059 3480 0.70 0.26 - snow 4331 3480 0.85 0.26 0.45
desert 1936 2896 0.82 0.27 0.67 kitchen 3200 2990 0.88 0.28 0.76 restaurant 1921 2536 0.89 0.29 0.71 mountain 4571 2768 0.86 0.30 0.38
black 4369 2387 0.64 0.30 0.17 skyscraper 843 3205 0.84 0.30 0.84 bridge 1654 2025 0.84 0.30 0.25 tree 5303 3758 0.90 0.30 0.16 castle 1298 2887 0.72 0.31 0.74 bridge 2779 2025 0.81 0.32 0.41
- 2349 2165 0.31 0.33 - ocean 2913 3505 0.87 0.33 0.71 table 3765 2165 0.94 0.33 0.23 windmill 1458 3752 0.71 0.33 0.76 window 1890 2795 0.85 0.34 0.21 river 2643 3204 0.76 0.35 0.62 water 5868 3204 0.90 0.35 0.27 beach 1897 2964 0.79 0.35 0.64 flower 3906 2587 0.92 0.35 0.67 wall 3158 3636 0.84 0.35 0.23
sky 4306 6055 0.76 0.36 0.34 street 2602 2385 0.86 0.36 0.49 golf course 1678 3864 0.44 0.36 0.63 field 3896 3261 0.74 0.36 0.37
tree 4098 3758 0.89 0.36 0.13 lighthouse 1254 1518 0.61 0.36 0.83 forest 1752 3431 0.80 0.37 0.56 church 2503 3140 0.86 0.37 0.72 people 3624 2275 0.91 0.37 0.14 baseball 2777 1929 0.66 0.37 0.86 field 2603 3922 0.74 0.37 0.25 car 3442 2118 0.79 0.38 0.27
people 4074 2286 0.92 0.38 0.17 shower 1271 2206 0.74 0.38 0.82 people walking 918 2224 0.63 0.38 0.25 wooden 3095 2723 0.63 0.38 0.28
mountain 3464 3239 0.88 0.38 0.29 tree 3676 2393 0.89 0.39 0.11 - 1976 3158 0.28 0.39 - snow 2521 3480 0.79 0.39 0.24
water 3102 2948 0.90 0.39 0.14 rock 2897 2967 0.76 0.39 0.26 - 2918 3459 0.08 0.39 - night 3027 3185 0.44 0.39 0.59
station 2063 2083 0.85 0.39 0.62 chair 2589 2288 0.89 0.39 0.22 building 6791 3450 0.89 0.40 0.21 city 2951 3190 0.67 0.40 0.50
Figure 2:",5 Experiments and Analysis,[0],[0]
"Scatter plot of audio cluster purity weighted by log cluster size vs variance for k = 500 (least-squares line superimposed).
based on the standard WSJ recipe and trained using the Google ASR hypothesis as a proxy for the transcriptions.",5 Experiments and Analysis,[0],[0]
Any word whose duration is overlapped 30% or more by the acoustic segment is included in the label string for the segment.,5 Experiments and Analysis,[0],[0]
We then employ a majority vote scheme to derive the overall cluster labels.,5 Experiments and Analysis,[0],[0]
"When computing the purity of a
cluster, we count a cluster member as matching the cluster label as long as the overall cluster label appears in the member’s label string.",5 Experiments and Analysis,[0],[0]
"In other words, an acoustic segment overlapping the words “the lighthouse” would receive credit for matching the overall cluster label “lighthouse”.",5 Experiments and Analysis,[0],[0]
A breakdown of the segments captured by two clusters is shown in Table 2.,5 Experiments and Analysis,[0],[0]
"We investigated some simple schemes for predicting highly pure clusters, and found that the empirical variance of the cluster members (average squared distance to the cluster centroid) was a good indicator.",5 Experiments and Analysis,[0],[0]
Figure 2 displays a scatter plot of cluster purity weighted by the natural log of the cluster size against the empirical variance.,5 Experiments and Analysis,[0],[0]
"Large, pure clusters are easily predicted by their low empirical variance, while a high variance is indicative of a garbage cluster.
",5 Experiments and Analysis,[0],[0]
"Ranking a set of k = 500 acoustic clusters by their variance, Table 3 displays some statistics for the 50 lowest-variance clusters.",5 Experiments and Analysis,[0],[0]
"We see that most of the clusters are very large and highly pure, and their labels reflect interesting object categories being identified by the neural network.",5 Experiments and Analysis,[0],[0]
"We additionally compute the coverage of each cluster by counting the total number of instances of the clus-
ter label anywhere in the training data, and then compute what fraction of those instances were captured by the cluster.",5 Experiments and Analysis,[0],[0]
"There are many examples of high coverage clusters, e.g. the “skyscraper” cluster captures 84% of all occurrences of the word “skyscraper”, while the “baseball” cluster captures 86% of all occurrences of the word “baseball”.",5 Experiments and Analysis,[0],[0]
"This is quite impressive given the fact that no conventional speech recognition was employed, and neither the multimodal neural network nor the grounding algorithm had access to the text transcripts of the captions.
",5 Experiments and Analysis,[0],[0]
"To get an idea of the impact of the k parameter as well as a variance-based cluster pruning threshold based on Figure 2, we swept k from 250 to 2000 and computed a set of statistics shown in Table 4.",5 Experiments and Analysis,[0],[0]
We compute the standard overall cluster purity evaluation metric in addition to the average coverage across clusters.,5 Experiments and Analysis,[0],[0]
"The table shows the natural tradeoff between cluster purity and redun-
dancy (indicated by the average cluster coverage) as k is increased.",5 Experiments and Analysis,[0],[0]
"In all cases, the variance-based cluster pruning greatly increases both the overall purity and average cluster coverage metrics.",5 Experiments and Analysis,[0],[0]
"We also notice that more unique cluster labels are discovered with a larger k.
Next, we examine the image clusters.",5 Experiments and Analysis,[0],[0]
"Figure 3 displays the 9 most central image crops for a set of 10 different image clusters, along with the majority-vote label of each image cluster’s associated audio cluster.",5 Experiments and Analysis,[0],[0]
"In all cases, we see that the image crops are highly relevant to their audio cluster label.",5 Experiments and Analysis,[0],[0]
"We include many more example image clusters in Appendix A.
In order to examine the semantic embedding space in more depth, we took the top 150 clusters from the same k = 500 clustering run described in Table 3 and performed t-SNE (van der Maaten and Hinton, 2008) analysis on the cluster centroid vectors.",5 Experiments and Analysis,[0],[0]
"We projected each centroid down to 2 di-
mensions and plotted their majority-vote labels in Figure 4.",5 Experiments and Analysis,[0],[0]
"Immediately we see that different clusters which capture the same label closely neighbor one another, indicating that distances in the embedding space do indeed carry information discriminative across word types (and suggesting that a more sophisticated clustering algorithm than kmeans would perform better).",5 Experiments and Analysis,[0],[0]
"More interestingly, we see that semantic information is also reflected in these distances.",5 Experiments and Analysis,[0],[0]
"The cluster centroids for “lake,” “river,” “body,” “water,” “waterfall,” “pond,” and “pool” all form a tight meta-cluster, as do “restaurant,” “store,” “shop,” and “shelves,” as well as “children,” “girl,” “woman,” and “man.”",5 Experiments and Analysis,[0],[0]
"Many other semantic meta-clusters can be seen in Figure 4, suggesting that the embedding space is capturing information that is highly discriminative both acoustically and semantically.
",5 Experiments and Analysis,[0],[0]
"Because our experiments revolve around the discovery of word and object categories, a key question to address is the extent to which the supervision used to train the VGG network constrains or influences the kinds of objects learned.",5 Experiments and Analysis,[0],[0]
"Because the 1,000 object classes from the ILSVRC2012 task (Russakovsky et al., 2015) used to train the VGG network were derived from WordNet synsets (Fellbaum, 1998), we can measure the semantic similarity between the words
learned by our network and the ILSVRC2012 class labels by using synset similarity measures within WordNet.",5 Experiments and Analysis,[0],[0]
"We do this by first building a list of the 1,000 WordNet synsets associated with the ILSVRC2012 classes.",5 Experiments and Analysis,[0],[0]
"We then take the set of unique majority-vote labels associated with the discovered word clusters for k = 500, filtered by setting a threshold on their variance (σ2 ≤ 0.65) so as to get rid of garbage clusters, leaving us with 197 unique acoustic cluster labels.",5 Experiments and Analysis,[0],[0]
"We then look up each cluster label in WordNet, and compare all noun senses of the label to every ILSVRC2012 class synset according to the path similarity measure.",5 Experiments and Analysis,[0],[0]
"This measure describes the distance between two synsets in a hyponym/hypernym hierarchy, where a score of 1 represents identity and lower scores indicate less similarity.",5 Experiments and Analysis,[0],[0]
We retain the highest score between any sense of the cluster label and any ILSVRC2012 synset.,5 Experiments and Analysis,[0],[0]
"Of the 197 unique cluster labels, only 16 had a distance of 1 from any ILSVRC12 class, which would indicate an exact match.",5 Experiments and Analysis,[0],[0]
"A path similarity of 0.5 indicates one degree of separation in the hyponym/hypernym hierarchy - for example, the similarity between “desk” and “table” is 0.5.",5 Experiments and Analysis,[0],[0]
"47 cluster labels were found to have a similarity of 0.5 to some ILSVRC12 class, leaving 134 cluster labels whose highest similarity to any ILSVRC12 class was less than 0.5.",5 Experiments and Analysis,[0],[0]
"In
other words, more than two thirds of the highly pure pattern clusters learned by our network were dissimilar to all of the 1,000 ILSVRC12 classes used to pretrain the VGG network, indicating that our model is able to generalize far beyond the set of classes found in the ILSVRC12 data.",5 Experiments and Analysis,[0],[0]
We display the labels of the 40 lowest variance acoustic clusters labels along with the name and similarity score of their closest ILSVRC12 synset in Table 5.,5 Experiments and Analysis,[0],[0]
"In this paper, we have demonstrated that a neural network trained to associate images with the waveforms representing their spoken audio captions can successfully be applied to discover and
cluster acoustic patterns representing words or short phrases in untranscribed audio data.",6 Conclusions and Future Work,[0],[0]
"An analogous procedure can be applied to visual images to discover visual patterns, and then the two modalities can be linked, allowing the network to learn, for example, that spoken instances of the word “train” are associated with image regions containing trains.",6 Conclusions and Future Work,[0],[0]
"This is done without the use of a conventional automatic speech recognition system and zero text transcriptions, and therefore is completely agnostic to the language in which the captions are spoken.",6 Conclusions and Future Work,[0],[0]
"Further, this is done in O(n) time with respect to the number of image/caption pairs, whereas previous stateof-the-art acoustic pattern discovery algorithms which leveraged acoustic data alone run in O(n2) time.",6 Conclusions and Future Work,[0],[0]
"We demonstrate the success of our methodology on a large-scale dataset of over 214,000 image/caption pairs comprising over 522 hours of spoken audio data, which is to our knowledge the largest scale acoustic pattern discovery experiment ever performed.",6 Conclusions and Future Work,[0],[0]
"We have shown that the shared multimodal embedding space learned by our model is discriminative not only across visual object categories, but also acoustically and semantically across spoken words.
",6 Conclusions and Future Work,[0],[0]
The future directions in which this research could be taken are incredibly fertile.,6 Conclusions and Future Work,[0],[0]
"Because our method creates a segmentation as well as an alignment between images and their spoken captions, a generative model could be trained using these alignments.",6 Conclusions and Future Work,[0],[0]
"The model could provide a spoken caption for an arbitrary image, or even synthesize an image given a spoken description.",6 Conclusions and Future Work,[0],[0]
"Modeling improvements are also possible, aimed at the goal of incorporating both visual and acoustic localization into the neural network itself.",6 Conclusions and Future Work,[0],[0]
"The same framework we use here could be extended to video, enabling the learning of actions, verbs, environmental sounds, and the like.",6 Conclusions and Future Work,[0],[0]
"Additionally, by collecting a second dataset of captions for our images in a different language, such as Spanish, our model could be extended to learn the acoustic correspondences for a given object category in both languages.",6 Conclusions and Future Work,[0],[0]
"This paves the way for creating a speech-to-speech translation model not only with absolutely zero need for any sort of text transcriptions, but also with zero need for directly parallel linguistic data or manual human translations.",6 Conclusions and Future Work,[0],[0]
"beach cliff pool desert field
chair table staircase statue stone
church forest mountain skyscraper trees
waterfall windmills window city bridge
flowers man wall archway baseball
boat shelves cockpit girl children
building rock kitchen plant hallway",A Additional Cluster Visualizations,[0],[0]
"Given a collection of images and spoken audio captions, we present a method for discovering word-like acoustic units in the continuous speech signal and grounding them to semantically relevant image regions.",abstractText,[0],[0]
"For example, our model is able to detect spoken instances of the words “lighthouse” within an utterance and associate them with image regions containing lighthouses.",abstractText,[0],[0]
"We do not use any form of conventional automatic speech recognition, nor do we use any text transcriptions or conventional linguistic annotations.",abstractText,[0],[0]
"Our model effectively implements a form of spoken language acquisition, in which the computer learns not only to recognize word categories by sound, but also to enrich the words it learns with semantics by grounding them in images.",abstractText,[0],[0]
Learning word-like units from joint audio-visual analysis,title,[0],[0]
"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1817–1827, Lisbon, Portugal, 17-21 September 2015. c©2015 Association for Computational Linguistics.",text,[0],[0]
Unsupervised word alignment (WA) on bilingual sentence pairs serves as an essential foundation for building most statistical machine translation (SMT) systems.,1 Introduction,[0],[0]
A lot of methods have been proposed to raise the accuracy of WA in an effort to improve end-to-end translation quality.,1 Introduction,[0],[0]
"This paper contributes to this effort through refining the widely used expectation-maximization (EM) algorithm for WA (Dempster et al., 1977; Brown et al., 1993b; Och and Ney, 2000).
∗",1 Introduction,[0],[0]
"The author now is affiliated with Google, Japan.
",1 Introduction,[0],[0]
The EM algorithm for WA has a great influence in SMT.,1 Introduction,[0],[0]
"Many well-known toolkits including GIZA++ (Och and Ney, 2003), the Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007), Fast Align (Dyer et al., 2013) and SyMGIZA++ (Junczys-Dowmunt and Sza, 2012), all employ this algorithm.",1 Introduction,[0],[0]
GIZA++,1 Introduction,[0],[0]
"in particular is frequently used in systems participating in many shared tasks (Goto et al., 2011; Cettolo et al., 2013; Bojar et al., 2013).
",1 Introduction,[0],[0]
"However, the EM algorithm for WA is wellknown for introducing “garbage collector effects.”",1 Introduction,[0],[0]
"Rare words have a tendency to collect garbage, that is they have a tendency to be erroneously aligned to untranslated words (Brown et al., 1993a; Moore, 2004; Ganchev et al., 2008; V Graça et al., 2010).",1 Introduction,[0],[0]
"Figure 1(a) shows a real sentence pair, denoted s, from the GALE ChineseEnglish Word Alignment and Tagging Training corpus (GALE WA corpus)1 with it’s humanannotated word alignment.",1 Introduction,[0],[0]
"The Chinese word “HE ZHANG,” denoted wr, which means river custodian, only occurs once in the whole corpus.",1 Introduction,[0],[0]
"We performed EM training using GIZA++ on this corpus concatenated with 442,967 training sentence pairs from the NIST Open Machine Translation (OpenMT) 2006 evaluation2.",1 Introduction,[0],[0]
The resulting alignment is shown in Figure 1(b).,1 Introduction,[0],[0]
"It can be seen that wr is erroneously aligned to multiple English words.
",1 Introduction,[0],[0]
"To find the cause of this, we checked the alignments in each iteration i of s, denoted ais.",1 Introduction,[0],[0]
"We found that in a1s , wr together with the other source-side words were aligned with uniform probability to all the target-side words since the alignment models provided no prior information.",1 Introduction,[0],[0]
"However, in a2s , wr became erroneously aligned,
1Released by Linguistic Data Consortium, catalog number LDC2012T16, LDC2012T20, LDC2012T24 and LDC2013T05.
2http://www.itl.nist.gov/iad/mig/ tests/mt/2006/
1817
because the alignment distribution3 of wr was only learned from a1s , thus consisted of non-zero values only for generating the target-side words in s. Therefore, the alignment probabilities from the rare word wr to the unaligned words in s were extraordinarily high, since almost all of the probability mass was distributed among them.",1 Introduction,[0],[0]
"In other words, the story behind these garbage collector effects is that erroneous alignments are able to provide support for themselves; the probability distribution learned only from s is re-applied to s. In this way, these “garbage collector effects” are a form of over-fitting.
",1 Introduction,[0],[0]
"Motivated by this observation, we propose a leave-one-out EM algorithm for WA in this paper.",1 Introduction,[0],[0]
"Recently this technique has been applied to avoid over-fitting in kernel density estimation (Roux and Bach, 2011); instead of performing maximum likelihood estimation, maximum leaveone-out likelihood estimation is performed.",1 Introduction,[0],[0]
Figure 1(c) shows the effect of using our technique on the example.,1 Introduction,[0],[0]
"The garbage collection has not occurred, and the alignment of the word “HE ZHANG” is identical to the human annotation.",1 Introduction,[0],[0]
"The most related work to this paper is training phrase translation models with leave-one-out forced alignment (Wuebker et al., 2010; Wuebker et al., 2012).",2 Related Work,[0],[0]
"The differences are that their work operates at the phrase level, and their aim is to improve translation models; while our work operates at the word level, and our aim is to provide better word alignment.",2 Related Work,[0],[0]
"As word alignment is a foundation of most MT systems, our method have a wider application.
",2 Related Work,[0],[0]
"Recently, better estimation methods during the maximization step of EM have been proposed to avoid the over-fitting in WA, such as using Kneser-Ney Smoothing to back-off the expected counts (Zhang and Chiang, 2014) or integrating the smoothed l0 prior to the estimation of probability (Vaswani et al., 2012).",2 Related Work,[0],[0]
"Our work differs from theirs by addressing the over-fitting directly in the EM algorithm by adopting a leave-one-out approach.
",2 Related Work,[0],[0]
"Bayesian methods (Gilks et al., 1996; Andrieu et al., 2003; DeNero et al., 2008; Neubig et al.,
3The probability distribution of generating target language words from wr .",2 Related Work,[0],[0]
"The description here is only based on IBM model1 for simplicity, and the other alignment models are similar.
",2 Related Work,[0],[0]
"(a)
2011), also attempt to address the issue of overfitting, however EM algorithms related to the proposed method have been shown to be more efficient (Wang et al., 2014).",2 Related Work,[0],[0]
"This section first formulates the standard EM for WA, then presents the leave-one-out EM for WA, and finally briefly discusses handling singletons and effecient implementation.",3 Methodology,[0],[0]
The main notation used in this section is shown in Table 1.,3 Methodology,[0],[0]
"To perform WA through EM, the parallel corpus is taken as observed data, the alignments are taken as latent data.","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"In order to maximize the likelihood of the alignment model θ given the data S, the following two steps are conducted iteratively (Brown et al., 1993b; Och and Ney, 2000; Och and Ney, 2003),
Expectation Step (E step): calculating the conditional probability of alignments for each sentence pair, P (a|s, θ) = ∏Jj=1 θali(aj |aj−1, I)θlex(fj |eaj ),(1) where θali(i|i′, I) is the alignment probability and θlex(f |e) is the translation probability.","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Note that
(1) is a general form for IBM model 1, model 2 and the HMM model.
","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Maximization step (M step): re-estimating the probability models,
θali(i|i′, I)","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑
s Ni|i′,I(s)∑ s Ni′,I(s)
(2) θlex(f |e)","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑
s Nf |e(s)∑ s ne(s)
(3)
where Ni′,I(s) is the marginal number of times ei′ is aligned to some foreign word if the length of e is I , or 0 otherwise; Ni|i′,I(s) is the marginal number of times the next alignment position after i′ is i in a if the length of e is I , or 0 otherwise; ne(s) is the count of e in e; Nf |e(s,a) is the marginal number of times e is aligned to f .","3.1 Standard EM for IBM Models 1, 2 and HMM Model",[0],[0]
Leave-one-out EM for WA differs from standard EM in the way the alignment and translation probabilities are calculated.,"3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Each sentence pair will
have its own alignment and translation probability models calculated by excluding the sentence pair itself.","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"More formally, leave-one-out EM for WA are formulated as follows,
Leave-one-out E step: employing leave-oneout models for each s to calculate the conditional probability of alignments
P (a|s, θs̄) = ∏J j=1 θ s̄ ali(aj |aj−1, I)θs̄lex(fj |eaj ),(4)
where θs̄ali(i|i′, I) and θs̄lex(fj |eaj ) are the leaveone-out alignment probability and translation probability, respectively.
","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"Leave-one-out M step: re-estimating leaveone-out probability models,
θs̄ali(i|i′, I)","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑ s′ 6=s Ni|i′,I(s ′)∑
s′ 6=s Ni′,I(s′) (5) θs̄lex(f |e)","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"← ∑ s′ 6=s Nf |e(s ′)∑
s′ 6=s ne(s′) .","3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
(6),"3.2 Leave-one-out EM for IBM Models 1, 2 and HMM Model",[0],[0]
"The framework of the standard EM for IBM Model 4 is similar with the one for IBM Models 1, 2 and HMM Model, but the calculation of alignment probability is more complicated.
",3.3 Standard EM for IBM Model 4,[0],[0]
"E step: calculating the conditional probability through the reverted alignment (Och and Ney, 2003),
P (a|s, θ) = P (B0|B1, . .",3.3 Standard EM for IBM Model 4,[0],[0]
.,3.3 Standard EM for IBM Model 4,[0],[0]
", BI)·",3.3 Standard EM for IBM Model 4,[0],[0]
"I∏
i=1
P (Bi|Bi−1, ei) · I∏
i=1",3.3 Standard EM for IBM Model 4,[0],[0]
∏,3.3 Standard EM for IBM Model 4,[0],[0]
"j∈Bi θlex(fj |ei), (7)
where B0 means the set of foreign words aligned with the empty word; P (B0|B1, . .",3.3 Standard EM for IBM Model 4,[0],[0]
.,3.3 Standard EM for IBM Model 4,[0],[0]
", BI) is assumed to be a binomial distribution for the size of B0 (Brown et al., 1993b) or an modified distribution to relieve deficiency (Och and Ney, 2003).
",3.3 Standard EM for IBM Model 4,[0],[0]
"The distribution P (Bi|Bi−1, ei) is decomposed as
P (Bi|Bi−1, ei) = θfer(φi|ei)· θhea(Bi,1 −Bρi |Eρi) · φi∏
k=2
θoth(Bi,k −Bi,k−1),
(8)
where θfer is a fertility model; θhea is a probability model for the head (first) aligned foreign word; θoth is a probability model for the other aligned foreign words.",3.3 Standard EM for IBM Model 4,[0],[0]
"θhea is assumed to be conditioned
on the word class Eρi , following the paper of (Och and Ney, 2003) and the implementation of GIZA++ and CICADA.
",3.3 Standard EM for IBM Model 4,[0],[0]
"M step: re-estimating the probability models, θfer(φ|e) ← ∑
s Nφ|e(s)∑ s ∑ φ′ Nφ′|e(s)
(9)
θhea(∆i|E)",3.3 Standard EM for IBM Model 4,[0],[0]
"← ∑ s N hea ∆i|E(s)∑
s ∑ ∆i′ N hea ∆i′|E(s)
(10)
θoth(∆i) ← ∑ s N oth ∆i (s)∑
s ∑ ∆i′ N oth ∆i′(s) , (11)
where ∆i is a difference of the indexes of two foreign words.",3.3 Standard EM for IBM Model 4,[0],[0]
"The leave-one-out treatment were applied to the three component probability models θfer, θhea and θoth of IBM model 4.
",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"Leave-one-out E step: calculating the conditional probability through leave-one-out probability models
P (a|s, θs̄) = P (B0|B1, . .",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
.,3.4 Leave-one-out EM for IBM Model 4,[0],[0]
", BI)·",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"I∏
i=1
P s̄(Bi|Bi−1, ei) · I∏
i=1",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
∏,3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"j∈Bi θs̄lex(fj |ei), (12)
P s̄(Bi|Bi−1, ei) = θs̄fer(φi|ei)·
θs̄hea(Bi,1 −Bρi |Eρi) · φi∏
k=2
θs̄oth(Bi,k −Bi,k−1).
",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"(13)
Leave-one-out M step: re-estimating the leaveone-out probability models,
θs̄fer(φ|e) ← ∑ s′ 6=s Nφ|e(s ′)∑
s′ 6=s ∑ φ′ Nφ′|e(s′) (14)
θs̄hea(∆i|E)",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
"← ∑ s′ 6=s N hea ∆i|E(s
′)∑ s′ 6=s ∑ ∆i′ N hea ∆i′|E(s ′) (15)
θs̄oth(∆i) ← ∑ s′ 6=s N oth ∆i (s
′)∑ s′ 6=s ∑ ∆i′ N oth ∆i′(s ′) .",3.4 Leave-one-out EM for IBM Model 4,[0],[0]
(16),3.4 Leave-one-out EM for IBM Model 4,[0],[0]
Singletons are the words that occur only once in corpora.,3.5 Handling Singletons,[0],[0]
Singletons cause problems when applying leave-one-out to lexicalized models such as the translation model θs̄lex and the fertility model θ s̄ fer.,3.5 Handling Singletons,[0],[0]
"When calculating (6) and (14) for singletons, the
denominators become zero, thus the probabilities are undefined.
",3.5 Handling Singletons,[0],[0]
"For singletons, there is no prior information to guide their alignment, so we back off to uniform distributions.",3.5 Handling Singletons,[0],[0]
"In that case, the alignments are primarily determined by the rest of the sentence.
",3.5 Handling Singletons,[0],[0]
"In addition, singletons can be in the target side of the translation model θs̄lex.",3.5 Handling Singletons,[0],[0]
"In that case, the probabilities become zero.",3.5 Handling Singletons,[0],[0]
"This is handled by setting a minimum probability value of 1.0× 10−12, which was decided by pilot experiments.",3.5 Handling Singletons,[0],[0]
"To alleviate memory requirements and increase speed, our implementation did not build or store the local alignment models explicitly for each sentence pair.",3.6 Implementation Details,[0],[0]
"The following formula was used to efficiently calculate (5), (6) and (14–16) to build temporary probability models,∑
s′ 6=s Nx(s′) =",3.6 Implementation Details,[0],[0]
"( ∑ s′ Nx(s′))−Nx(s), (17)
where x is a alignment event.",3.6 Implementation Details,[0],[0]
"Our implementation maintained global counts of all alignment events ∑ s′ Nx(s
′), and (considerably smaller) local counts Nx(s) from each sentence pair s.
Take the translation model θs̄lex for example.",3.6 Implementation Details,[0],[0]
For a sentence pair s = (f1 . . .,3.6 Implementation Details,[0],[0]
"fJ , e1 . . .",3.6 Implementation Details,[0],[0]
"eI), it is cauclulated as,
θs̄lex(fj |ei) =",3.6 Implementation Details,[0],[0]
"( ∑ s′ N(fj |ei)(s ′))−N(fj |ei)(s) ( ∑ s′ nei(s′))− nei(s) .
",3.6 Implementation Details,[0],[0]
"(18)
",3.6 Implementation Details,[0],[0]
"The global counts to be maintained are∑ s′ N(fj |ei)(s
′) and nei(s′), and the local counts are ∑ s N(fj |ei)(s) and nei(s).",3.6 Implementation Details,[0],[0]
"Therefore the memory cost is,
|E| · (|F|+ 1) + ∑ s Is(Js + 1), (19)
where |E| is the size of English vocabulary, |F| is the size of foreign language vocabulary, Is is the length of the English sentence of s, and Js is the length of the foreign sentence of s.
",3.6 Implementation Details,[0],[0]
"The calculation of the leave-one-out translation model is performed for each English word and foreign word in s. Therefore, the time cost is,∑
s
Is(Js + 1).",3.6 Implementation Details,[0],[0]
"(20)
In addition, because the local counts N(fj |ei)(s) and nei(s) are read in order, storing them in a external memory such as a hard disk will not slow down the running speed much.",3.6 Implementation Details,[0],[0]
"This will reduce the memory cost to
|E| · (|F|+ 1).",3.6 Implementation Details,[0],[0]
"(21) This cost is independent to the number of sentence pairs4.
",3.6 Implementation Details,[0],[0]
The speed of the proposed method can be boosted through parallelism.,3.6 Implementation Details,[0],[0]
These calculations on each sentence pair can be performed independently.,3.6 Implementation Details,[0],[0]
"We found empirically that when our implementation of the proposed method is run on a 16-core computer, it finishes the task earlier than GIZA++5.",3.6 Implementation Details,[0],[0]
The proposed WA method was tested on two language pairs: Chinese-English and JapaneseEnglish (Table 2).,4 Experiments,[0],[0]
"Performance was measured both directly using the agreement with reference to manual WA annotations, and indirectly using the BLEU score in end-to-end machine translation tasks.",4 Experiments,[0],[0]
GIZA++ and our own implementation of standard EM were used as baselines.,4 Experiments,[0],[0]
The Chinese-English experimental data consisted of the GALE WA corpus and the OpenMT corpus.,4.1 Experimental Settings,[0],[0]
"They are from the same domain, both contain newswire texts and web blogs.",4.1 Experimental Settings,[0],[0]
"The OpenMT evaluation 2005 was used as a development set for MERT tuning (Och, 2003), and the OpenMT evaluation 2006 was used as a test set.",4.1 Experimental Settings,[0],[0]
"The JapaneseEnglish experimental data was the Kyoto Free Translation Task (Neubig, 2011)6.",4.1 Experimental Settings,[0],[0]
"The corpus contains a set of 1,235 sentence pairs that are manually word aligned.
",4.1 Experimental Settings,[0],[0]
The corpora were processed using a standard procedure for machine translation.,4.1 Experimental Settings,[0],[0]
"The English texts were tokenized with the tokenization script released with Europarl corpus (Koehn, 2005) and converted to lowercase; the Chinese texts were segmented into words using the Stanford Word Segmenter (Xue et al., 2002)7; the Japanese texts
4We found the memory of our server is large enough, so we did not implement it
5We plan to make our code public available.",4.1 Experimental Settings,[0],[0]
"6http://www.phontron.com/kftt/ 7http://nlp.stanford.edu/software/
segmenter.shtml
were segmented into words using the Kyoto Text Analysis Toolkit (KyTea8).",4.1 Experimental Settings,[0],[0]
"Sentences longer than 100 words or those with foreign/English word length ratios between larger than 9 were filtered out.
",4.1 Experimental Settings,[0],[0]
"GIZA++ was run with the default Moses settings (Koehn et al., 2007).",4.1 Experimental Settings,[0],[0]
"The IBM model 1, HMM model, IBM model 3 and IBM model 4 were run with 5, 5, 3 and 3 iterations.",4.1 Experimental Settings,[0],[0]
"We implemented the proposed leave-one-out EM and standard EM in IBM model 1, HMM model and IBM model 4.",4.1 Experimental Settings,[0],[0]
"In the original work (Och and Ney, 2003) this combination of models achieved comparable performance to the default Moses settings.",4.1 Experimental Settings,[0],[0]
"They were run with 5, 5 and 6 iterations.
",4.1 Experimental Settings,[0],[0]
"The standard EM was re-implemented as a baseline to provide a solid basis for comparison, because GIZA++ contains many undocumented details.",4.1 Experimental Settings,[0],[0]
"Our implementation is based on the toolkit of CICADA (Watanabe and Sumita, 2011; Watanabe, 2012; Tamura et al., 2013)9.",4.1 Experimental Settings,[0],[0]
"We named the implemented aligner AGRIPPA, to support our inhouse decoders OCTAVIAN and AUGUSTUS.
",4.1 Experimental Settings,[0],[0]
"In all experiments, WA was performed independently in two directions: from foreign languages to English, and from English to foreign languages.",4.1 Experimental Settings,[0],[0]
"Then the grow-diag-final-and heuristic was used to combine the two alignments from both directions to yield the final alignments for evaluation (Och and Ney, 2000; Och and Ney, 2003).",4.1 Experimental Settings,[0],[0]
"Word alignment accuracy of the baseline and the proposed method is shown in Table 3 in terms of precision, recall and F1 (Och and Ney, 2003).",4.2 Word Alignment Accuracy,[0],[0]
The proposed method gave rise to higher quality alignments in all our experiments.,4.2 Word Alignment Accuracy,[0],[0]
"The improvement in F1, precision and recall based on IBM Model 4 is in the range 8.3% to 9.1% compared with the GIZA++ baseline, and in the range 5.0% to 17.2% compared with our own baseline.
",4.2 Word Alignment Accuracy,[0],[0]
"The most meaningful result comes from the comparison of the models trained using standard EM log-likelihood training, and the proposed EM leave-one-out log-likelihood training.",4.2 Word Alignment Accuracy,[0],[0]
These models are identical except for way in which the model likelihood is calculated.,4.2 Word Alignment Accuracy,[0],[0]
In all our experiments the proposed method gave rise to higher quality alignments.,4.2 Word Alignment Accuracy,[0],[0]
"The standard EM implementation achieved
8http://www.phontron.com/kytea/ 9http://www2.nict.go.jp/univ-com/multi trans/cicada/
alignment performance approximately comparable to GIZA++, whereas the proposed method exceeded the performance of both implementations.",4.2 Word Alignment Accuracy,[0],[0]
"BLEU scores achieved by the phrase-based and hierachical SMT systems10 which were trained from different alignment results, are shown in Table 4.",4.3 End-to-end Translation Quality,[0],[0]
Each experiment was conducted three times to mitigate the variance in the results due to MERT.,4.3 End-to-end Translation Quality,[0],[0]
The results show that the proposed alignment method achieved the highest BLEU score in all experiments.,4.3 End-to-end Translation Quality,[0],[0]
"The improvement over the baseline is in range 0.03 to 1.03 for phrase-based systems, and ranged from 0.43 to 1.30 for hierarchical systems.
",4.3 End-to-end Translation Quality,[0],[0]
Hierarchical systems benifit more from the proposed method than phrase-based systems.,4.3 End-to-end Translation Quality,[0],[0]
We think this is because that hierarchical systems are more sensitive to word alignment quality than phrase-based systems.,4.3 End-to-end Translation Quality,[0],[0]
"Phrase-based systems only
10from the Moses toolkit
take contiguous parallel phrase pairs as translation rules, while hierarchical systems also use patterns made by subtracting (inner) short parallel phrases from (outer) longer parallel phrases.",4.3 End-to-end Translation Quality,[0],[0]
Both the outer and inner phrases typically need to be noisefree in order to produce high quality rules.,4.3 End-to-end Translation Quality,[0],[0]
This puts a high demand on the alignment quality.,4.3 End-to-end Translation Quality,[0],[0]
"Training corpora of different sizes were employed to perform unsupervised WA experiments and MT experiments (see Tables 5 and 6).
",4.4 Effect of Training Corpus Size,[0],[0]
The training corpora were randomly sampled from the Chinese-English manual WA corpora and the parallel training corpus.,4.4 Effect of Training Corpus Size,[0],[0]
"The manual WA corpus has a priority for being sampled so that the gold WA annotation is available for MT experi-
ments.",4.4 Effect of Training Corpus Size,[0],[0]
The settings of the unsupervised WA experiments and the MT experiments are the same with the previous experiments.,4.4 Effect of Training Corpus Size,[0],[0]
"In the WA experiments, GIZA++, our implemented standard EM and the proposed leave-one-out EM are applied to training corpora with the same parameter settings as the previous.",4.4 Effect of Training Corpus Size,[0],[0]
"In the MT experiments, the WA results of different methods and the gold WA (if available) are employed to extract translation rules; the rest settings including language models, development and test corpus, and parameters are the same as the previous.
",4.4 Effect of Training Corpus Size,[0],[0]
"On word alignment accuracy, the proposed method achieved improvements of F1 from 0.041 to 0.090 under the different training corpora (Table 5.",4.4 Effect of Training Corpus Size,[0],[0]
"The maximum improvement compared with GIZA++ is 0.069 when the training corpus has 4,000 sentence pairs.",4.4 Effect of Training Corpus Size,[0],[0]
"The maximum improvement compared with our own implement is 0.090 when the training corpus has 64,000 sentence pairs.
",4.4 Effect of Training Corpus Size,[0],[0]
"Figure 2 shows that the extent of improvements slightly changes under different training corpora, but they are all quite stable and obvious.
",4.4 Effect of Training Corpus Size,[0],[0]
"On translation quality, the proposed method achieved improvements of BLEU under the different training corpora.",4.4 Effect of Training Corpus Size,[0],[0]
The improvements ranged from 0.19 to 1.72 for phrase-based MT and ranged from 0.25 to 3.02 (see Table 5).,4.4 Effect of Training Corpus Size,[0],[0]
"The improvements are larger under smaller training corpora (see Figure 3).
",4.4 Effect of Training Corpus Size,[0],[0]
"In addition, the BLEUs achieved by the proposed method is close to the ones achieved by gold WA annotations.",4.4 Effect of Training Corpus Size,[0],[0]
"The proposed method slightly outperforms the gold WA annotations when using the full manual WA corpus of 18,057 sentence pairs.
",4.4 Effect of Training Corpus Size,[0],[0]
"4.5 Comparison to l0-Normalization and Kneser-Ney Smoothing Methods
The proposed leave-one-word word alignment method was empirically compared to l0-normalized GIZA++",4.4 Effect of Training Corpus Size,[0],[0]
"(Vaswani et al., 2012)11 and Kneser-Ney smoothed GIZA++",4.4 Effect of Training Corpus Size,[0],[0]
"(Zhang and Chiang, 2014)12.",4.4 Effect of Training Corpus Size,[0],[0]
l0-normalization and KneserNey smoothing methods are established methods to overcome the sparse problem.,4.4 Effect of Training Corpus Size,[0],[0]
This enables the probability distributions on rare words to be estimated more effectively.,4.4 Effect of Training Corpus Size,[0],[0]
"In this way, these two GIZA++ variants are related to the proposed method.
",4.4 Effect of Training Corpus Size,[0],[0]
"l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++ were run with the same settings as GIZA++, which came from the default settings of MOSES.",4.4 Effect of Training Corpus Size,[0],[0]
For the settings of l0-normalized GIZA++ that are not in common with GIZA++ were the default settings.,4.4 Effect of Training Corpus Size,[0],[0]
"As for Kneser-Ney smoothed GIZA++, the smooth switches of IBM models 1 – 4 and HMM model
11http://www.isi.edu/˜avaswani/ giza-pp-l0.html
12https://github.com/hznlp/giza-kn
were turned on.
",4.4 Effect of Training Corpus Size,[0],[0]
The experimental results are presented in Table 7.,4.4 Effect of Training Corpus Size,[0],[0]
The experiments were run on the ChineseEnglish language pair.,4.4 Effect of Training Corpus Size,[0],[0]
The word alignment quality was evaluated separately for all words and for various levels of rare words.,4.4 Effect of Training Corpus Size,[0],[0]
"The leave-one-out method outperformed related methods in terms of precision, recall and F1 when evaluated on all words.
",4.4 Effect of Training Corpus Size,[0],[0]
Rare words were categorized based on the number of occurences in the source-language text of the training data.,4.4 Effect of Training Corpus Size,[0],[0]
The evaluations were carried out on the subset of alignment links that had a rare word on the source side.,4.4 Effect of Training Corpus Size,[0],[0]
"Table 7 presents the results for thresholds 1, 2, 5 and 10.",4.4 Effect of Training Corpus Size,[0],[0]
"The proposed method achieved much higher precision on rare words than the other methods, but performed poorly on recall.",4.4 Effect of Training Corpus Size,[0],[0]
The Kneser-Ney Smoothed GIZA++ had higher recall.,4.4 Effect of Training Corpus Size,[0],[0]
"The explanation might be that the leave-one-out method punishes rare words more than the Kneser-Ney smoothing method, by totally removing the derived expected counts of current sentence pair from the alignment models.",4.4 Effect of Training Corpus Size,[0],[0]
This leads to rare words being passively aligned.,4.4 Effect of Training Corpus Size,[0],[0]
"In other words, the leave-one-out method would align rare words unless the confidence is high.",4.4 Effect of Training Corpus Size,[0],[0]
"Therefore, we plan to seek a method to integrate Kneser-Ney smoothing into the proposed leave-one-out method in the future work.
",4.4 Effect of Training Corpus Size,[0],[0]
The BLEU scores achieved by phrase-based SMT and hierarchical SMT for different alignment methods are presented in Table 7.,4.4 Effect of Training Corpus Size,[0],[0]
The proposed method outperforms the other methods.,4.4 Effect of Training Corpus Size,[0],[0]
The Kneser-Ney Smoothed GIZA++ performed the second best.,4.4 Effect of Training Corpus Size,[0],[0]
"We tried to further analyze the relation between word alignment and BLEU, but found the analysis was obscured by the many processing stages.",4.4 Effect of Training Corpus Size,[0],[0]
"These stages include paral-
lel phrase extraction (or translation rule extraction from hierarchical SMT), log-linear model, MERT tuning and practical decoding where a lot of pruning happened.",4.4 Effect of Training Corpus Size,[0],[0]
This paper proposes a leave-one-out EM algorithm for WA to overcome the over-fitting problem that occurs when using standard EM for WA.,5 Conclusion,[0],[0]
"The experimental results on Chinese-English and Japanese-English corpora show that both the WA accuracy and the end-to-end translation are improved.
",5 Conclusion,[0],[0]
"In addition, we have a interesting finding about the effect of manual WA annotations on training MT systems.",5 Conclusion,[0],[0]
"In a Chinese-English parallel training corpus of 18,057 sentence pairs, the manual WA annotation outperformed the unsupervised WA results produced by standard EM algorithms.",5 Conclusion,[0],[0]
"However, the unsupervised WA results produced by proposed leave-one-out EM algorithm outperformed the manual WA annotation.
",5 Conclusion,[0],[0]
Our future work will focus on increasing the gains in end-to-end translation quality through the proposed leave-one-out aligner.,5 Conclusion,[0],[0]
It is a interesting question why GIZA++ achieved competitive BLEU scores though its alignment accuracy measured by F1 was substantially lower.,5 Conclusion,[0],[0]
The answer to this question which may reveal essence of good word alignment for MT and eventually help to improve MT.,5 Conclusion,[0],[0]
"In addition, we plan to improve the proposed method by integrating Kneser-Ney smoothing.",5 Conclusion,[0],[0]
We appreciated the valuable comments from the reviewers.,Acknowledgments,[0],[0]
"Expectation-maximization algorithms, such as those implemented in GIZA++ pervade the field of unsupervised word alignment.",abstractText,[0],[0]
"However, these algorithms have a problem of over-fitting, leading to “garbage collector effects,” where rare words tend to be erroneously aligned to untranslated words.",abstractText,[0],[0]
This paper proposes a leave-one-out expectationmaximization algorithm for unsupervised word alignment to address this problem.,abstractText,[0],[0]
The proposed method excludes information derived from the alignment of a sentence pair from the alignment models used to align it.,abstractText,[0],[0]
This prevents erroneous alignments within a sentence pair from supporting themselves.,abstractText,[0],[0]
"Experimental results on Chinese-English and Japanese-English corpora show that the F1, precision and recall of alignment were consistently increased by 5.0% – 17.2%, and BLEU scores of end-to-end translation were raised by 0.03 – 1.30.",abstractText,[0],[0]
The proposed method also outperformed l0-normalized GIZA++ and Kneser-Ney smoothed GIZA++.,abstractText,[0],[0]
Leave-one-out Word Alignment without Garbage Collector Effects,title,[0],[0]
"Proceedings of NAACL-HLT 2018, pages 1090–1100 New Orleans, Louisiana, June 1 - 6, 2018. c©2018 Association for Computational Linguistics
Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data. However, there is no standard and effective metric to evaluate the quality of multilingual topics. We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications. Importantly, we also study evaluation for low-resource languages. Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.",text,[0],[0]
"Topic models provide a high-level view of the main themes of a document collection (Boyd-Graber et al., 2017).",1 Introduction,[0],[0]
"Document collections, however, are often not in a single language, driving the development of multilingual topic models.",1 Introduction,[0],[0]
"These models discover topics that are consistent across languages, providing useful tools for multilingual text analysis (Vulić et al., 2015), such as detecting cultural differences (Gutiérrez et al., 2016) and bilingual dictionary extraction (Liu et al., 2015).
",1 Introduction,[0],[0]
"Monolingual topic models can be evaluated through likelihood (Wallach et al., 2009b) or coherence (Newman et al., 2010), but topic model evaluation is not well understood in multilingual settings.",1 Introduction,[0],[0]
Our contributions are two-fold.,1 Introduction,[0],[0]
"We introduce an improved intrinsic evaluation metric for multilingual topic models, called Crosslingual Normalized Pointwise Mutual Information (CNPMI, Section 2).",1 Introduction,[0],[0]
We explore the behaviors of CNPMI at both the model and topic levels with six language pairs and varying model specifications.,1 Introduction,[0],[0]
"This metric
correlates well with human judgments and crosslingual classification results (Sections 5 and 6).
",1 Introduction,[0],[0]
"We also focus on evaluation in low-resource languages, which lack large parallel corpora, dictionaries, and other tools that are often used in learning and evaluating topic models.",1 Introduction,[0],[0]
"To adapt CNPMI to these settings, we create a coherence estimator (Section 3) that extrapolates statistics derived from antiquated, specialized texts like the Bible: often the only resource available for many languages.",1 Introduction,[0],[0]
A multilingual topic contains one topic for each language.,2 Evaluating Multilingual Coherence,[0],[0]
"For a multilingual topic to be meaningful to humans (Figure 1), the meanings should be consistent across the languages, in addition to coherent within each language (i.e., all words in a topic are related).
",2 Evaluating Multilingual Coherence,[0],[0]
This section describes our approach to evaluating the quality of multilingual topics.,2 Evaluating Multilingual Coherence,[0],[0]
"After defining the multilingual topic model, we describe topic model evaluation extending standard monolingual approaches to multilingual settings.",2 Evaluating Multilingual Coherence,[0],[0]
"Probabilistic topic models associate each document in a corpus with a distribution over latent topics, while each topic is associated with a distribution over words in the vocabulary.",2.1 Multilingual Topic Modeling,[0],[0]
"The most widely used topic model, latent Dirichlet allocation (Blei et al., 2003, LDA), can be extended to connect languages.",2.1 Multilingual Topic Modeling,[0],[0]
"These extensions require additional knowledge to link languages together.
",2.1 Multilingual Topic Modeling,[0],[0]
"One common encoding of multilingual knowledge is document links (indicators that documents are parallel or comparable), used in polylingual topic models (Mimno et al., 2009; Ni et al., 2009).",2.1 Multilingual Topic Modeling,[0],[0]
"In these models, each document d indexes a tuple of parallel/comparable language-specific documents,
1090
d(`), and the language-specific “views” of a document share the document-topic distribution θd.",2.1 Multilingual Topic Modeling,[0],[0]
"The generative story for the document-links model is:
1 for each topic k and each language ` do 2 Draw a distribution over words φ`k ∼ Dirichlet(β); 3 for each document tuple d = ( d(1), . . .",2.1 Multilingual Topic Modeling,[0],[0]
", d(L) )",2.1 Multilingual Topic Modeling,[0],[0]
"do 4 Draw a distribution over topics θd ∼ Dirichlet(α); 5 for each language ` = 1, . . .",2.1 Multilingual Topic Modeling,[0],[0]
",L do 6 for each token t ∈ d(`) do 7 Draw a topic zn ∼ θd; 8 Draw a word wn ∼ φ`z;
Alternatively, word translations (Jagarlamudi and Daumé III, 2010), concept links (Gutiérrez et al., 2016; Yang et al., 2017), and multi-level priors (Krstovski et al., 2016) can also provide multilingual knowledges.",2.1 Multilingual Topic Modeling,[0],[0]
"Since the polylingual topic model is the most common approach for building multilingual topic models (Vulić et al., 2013, 2015; Liu et al., 2015; Krstovski and Smith, 2016), our study will focus on this model.",2.1 Multilingual Topic Modeling,[0],[0]
"Most automatic topic model evaluation metrics use co-occurrence statistics of word pairs from a reference corpus to evaluate topic coherence, assuming that coherent topics contain words that often appear together (Newman et al., 2010).",2.2 Monolingual Evaluation,[0],[0]
"The most successful (Lau et al., 2014) is normalized pointwise mutual information (Bouma, 2009, NPMI).",2.2 Monolingual Evaluation,[0],[0]
"NPMI compares the joint probability of words appearing together Pr(wi,wj) to their probability assuming independence Pr(wi) Pr(wj), normalized by the joint probability:
NPMI(wi,wj) = log
Pr(wi,wj) Pr(wi) Pr(wj)
log Pr(wi,wj) .",2.2 Monolingual Evaluation,[0],[0]
"(1)
The word probabilities are calculated from a reference corpus, R, typically a large corpus such as Wikipedia that can provide meaningful cooccurrence patterns that are independent of the target dataset.
",2.2 Monolingual Evaluation,[0],[0]
"The quality of topic k is the average NPMI of all word pairs (wi,wj) in the topic:
NPMIk = −1",2.2 Monolingual Evaluation,[0],[0]
"( C 2 ) ∑ i∈W(k,C) ∑ j 6=i NPMI(wi,wj), (2)
where W(k,C) are the C most probable words in the topic-word distribution φk (the number of words is the topic’s cardinality).",2.2 Monolingual Evaluation,[0],[0]
Higher NPMIk means the topic’s top words are more coupled.,2.2 Monolingual Evaluation,[0],[0]
"While automatic evaluation has been well-studied for monolingual topic models, there are no robust evaluations for multilingual topic models.",2.3 Existing Multilingual Evaluations,[0],[0]
"We first consider two straightforward metrics that could be used for multilingual evaluation, both with limitations.",2.3 Existing Multilingual Evaluations,[0],[0]
"We then propose an extension of NPMI that addresses these limitations.
",2.3 Existing Multilingual Evaluations,[0],[0]
Internal Coherence.,2.3 Existing Multilingual Evaluations,[0],[0]
A simple adaptation of NPMI is to calculate the monolingual NPMI score for each language independently and take the average.,2.3 Existing Multilingual Evaluations,[0],[0]
We refer this as internal NPMI (INPMI) as it evaluates coherence within a language.,2.3 Existing Multilingual Evaluations,[0],[0]
"However, this metric does not consider whether the topic is coherent across languages—that is, whether a language-specific word distribution φ`1k is related to the corresponding distribution in another language, φ`2k.
Crosslingual Consistency.",2.3 Existing Multilingual Evaluations,[0],[0]
"Another straightforward measurement is Matching Translation Accuracy (Boyd-Graber and Blei, 2009, MTA), which counts the number of word translations in a topic between two languages using a bilingual dictionary.",2.3 Existing Multilingual Evaluations,[0],[0]
"This metric can measure whether a topic is well-aligned across languages literally, but cannot capture non-literal more holistic similarities across languages.",2.3 Existing Multilingual Evaluations,[0],[0]
"We extend NPMI to multilingual models, with a metric we call crosslingual normalized pointwise mutual information (CNPMI).",2.4 New Metric: Crosslingual NPMI,[0],[0]
"This metric will be the focus of our experiments.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"A multilingually coherent topic means that if wi,`1 in language `1 and wj,`2 in language `2 are in the same topic, they should appear in similar contexts in comparable or parallel corporaR(`1,`2).
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Our adaptation of NPMI is based on the same principles as the monolingual version, but focuses on the co-occurrences of bilingual word pairs.",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Given a bilingual word pair (wi,`1 ,wj,`2) the co-occurrence of this word pair is the event where word wi,`1 appears in a document in language `1 and the word wj,`2 appears in a comparable or parallel document in language `2.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"The co-occurrence probability of each bilingual word pair is:
Pr (wi,`1 ,wj,`2) , ∣∣{d : wi,`1 ∈ d(`1),wj,`2 ∈ d(`2) }∣∣ ∣∣R(`1,`2) ∣∣ , (3)
where d = ( d(`1), d(`2) ) is a pair of parallel/comparable documents in the reference corpus R(`1,`2).",2.4 New Metric: Crosslingual NPMI,[0],[0]
"When one or both words in a bilingual pair do not appear in the reference corpus, the cooccurrence score is zero.
",2.4 New Metric: Crosslingual NPMI,[0],[0]
"Similar to monolingual settings, CNPMI for a bilingual topic k is the average of the NPMI scores of all C2 bilingual word pairs,
CNPMI(`1, `2, k) =
∑C",2.4 New Metric: Crosslingual NPMI,[0],[0]
"i,j NPMI (wi,`1 ,wj,`2)
C2 .",2.4 New Metric: Crosslingual NPMI,[0],[0]
"(4)
It is straightforward to generalize CNPMI from a language pair to multiple languages by averaging CNPMI(`i, `j , k) over all language pairs (`i, `j).",2.4 New Metric: Crosslingual NPMI,[0],[0]
CNPMI needs a reference corpus for co-occurrence statistics.,3 Adapting to Low-Resource Languages,[0],[0]
"Wikipedia, which has good coverage of topics and vocabularies is a common choice (Lau and Baldwin, 2016).",3 Adapting to Low-Resource Languages,[0],[0]
"Unfortunately, Wikipedia is often unavailable or not large enough for lowresource languages.",3 Adapting to Low-Resource Languages,[0],[0]
"It only covers 282 languages,1 and only 249 languages have more than 1,000 pages: many of pages are short or unlinked to
1 https://meta.wikimedia.org/wiki/List_of_Wikipedias
a high-resource language.",3 Adapting to Low-Resource Languages,[0],[0]
"Since CNPMI requires comparable documents, the usable reference corpus is defined by paired documents.
",3 Adapting to Low-Resource Languages,[0],[0]
"Another option for a parallel reference corpus is the Bible (Resnik et al., 1999), which is available in most world languages;2 however, it is small and archaic.",3 Adapting to Low-Resource Languages,[0],[0]
"It is good at evaluating topics such as family and religion, but not “modern” topics like biology and Internet.",3 Adapting to Low-Resource Languages,[0],[0]
"Without reference co-occurrence statistics relevant to these topics, CNPMI will fail to judge topic coherence—it must give the ambiguous answer of zero.",3 Adapting to Low-Resource Languages,[0],[0]
"Such a score could mean a totally incoherent topic where each word pair never appears together (Topics 6 in Figure 1), or an unjudgeable topic (Topic 5).
",3 Adapting to Low-Resource Languages,[0],[0]
Our goal is to obtain a reliable estimation of topic coherence for low-resource languages when the Bible is the only reference.,3 Adapting to Low-Resource Languages,[0],[0]
We propose a model that can correct the drawbacks of a Bible-derived CNPMI.,3 Adapting to Low-Resource Languages,[0],[0]
"While we assume bilingual topics paired with English, our approach can be applied to any high-resource/low-resource language pair.
",3 Adapting to Low-Resource Languages,[0],[0]
We take Wikipedia’s CNPMI from high-resource languages as accurate estimations.,3 Adapting to Low-Resource Languages,[0],[0]
"We then build a coherence estimator on topics from high-resource languages, with the Wikipedia CNPMI as the target output.",3 Adapting to Low-Resource Languages,[0],[0]
We use linear regression using the below features.,3 Adapting to Low-Resource Languages,[0],[0]
"Given a topic in low-resource language, the estimator produces an estimated coherence (Figure 2).",3 Adapting to Low-Resource Languages,[0],[0]
The key to the estimator is to find features that capture whether we should trust the Bible.,3.1 Estimator Features,[0],[0]
"For generality, we focus on features independent of the available resources other than the Bible.",3.1 Estimator Features,[0],[0]
"This section describes the features, which we split into four groups.
",3.1 Estimator Features,[0],[0]
Base Features (BASE),3.1 Estimator Features,[0],[0]
"Our base features include information we can collect from the Bible and the topic model: cardinality C, CNPMI and INPMI, MTA, and topic word coverage (TWC), which counts the percentage of topic words in a topic that appear in a reference corpus.
Crosslingual Gap (GAP) A low CNPMI score could indicate a topic pair where each language has a monolingually coherent topic but that are not about the same theme (Topic 6 in Figure 1).",3.1 Estimator Features,[0],[0]
"Thus, we add two features to capture this information
2The Bible is available in 2,530 languages.
using the Bible: mismatch coefficients (MC) and internal comparison coefficients (ICC):
MC(`1; `2, k) = CNPMI(`1, `2, k)
INPMI(`1, k) + α , (5)
",3.1 Estimator Features,[0],[0]
"ICC(`1, `2, k) = INPMI(`1, k) + α
INPMI(`2, k) + α , (6)
where α is a smoothing factor (α = 0.001 in our experiments).",3.1 Estimator Features,[0],[0]
"MC recognizes the gap between crosslingual and monolingual coherence, so a higher MC score indicates a gap between coherence within and across languages.",3.1 Estimator Features,[0],[0]
"Similarly, ICC compares monolingual coherence to tell if both languages are coherent: the closer to 1 the ICC is, the more comparable internal coherence both languages have.
",3.1 Estimator Features,[0],[0]
"Word Era (ERA) Because the Bible’s vocabulary is unable to evaluate modern topics, we must tell the model what the modern words are.",3.1 Estimator Features,[0],[0]
The word era features are the earliest usage year 3 for each word in a topic.,3.1 Estimator Features,[0],[0]
"We use both the mean and standard deviation as features.
",3.1 Estimator Features,[0],[0]
Meaning Drift (DRIFT).,3.1 Estimator Features,[0],[0]
The meaning of a word can expand and drift over time.,3.1 Estimator Features,[0],[0]
"For example, in the Bible, “web” appears in Isaiah 59:5:
They hatch cockatrice’ eggs, and weave the spider’s web.
3 https://oxforddictionaries.com/
The word “web” could be evaluated correctly in an animal topic.",3.1 Estimator Features,[0],[0]
"For modern topics, however, Bible fails to capture modern meanings of “web”, as in Topic 5 (Figure 1).
",3.1 Estimator Features,[0],[0]
"To address this meaning drift, we use a method similar to Hamilton et al. (2016).",3.1 Estimator Features,[0],[0]
"For each English word, we calculate the context vector from Bible and from Wikipedia with a window size of five and calculate the cosine similarity between them as word similarity.",3.1 Estimator Features,[0],[0]
Similar context vectors mean that the usage in the Bible is consistent with Wikipedia.,3.1 Estimator Features,[0],[0]
We calculate word similarities for all the English topic words in a topic and use the average and standard deviation as features.,3.1 Estimator Features,[0],[0]
"In Figure 3, Topic 1 is coherent while Topic 8 is not.",3.2 Example,[0],[0]
"From left to right, we incrementally add new feature sets, and show how the estimated topic coherence scores (dashed lines) approach the ideal CNPMI (dotted lines).",3.2 Example,[0],[0]
"When only using the BASE features, the estimator gives a higher prediction to Topic 8 than to Topic 1.",3.2 Example,[0],[0]
Their low MTA and TWC prevent accurate evaluations.,3.2 Example,[0],[0]
Adding GAP does not help much.,3.2 Example,[0],[0]
"However, ICC(EN, AM, k = 1) is much smaller, which might indicate a large gap of internal coherence between the two languages.
",3.2 Example,[0],[0]
Adding ERA makes the estimated scores flip between the two topics.,3.2 Example,[0],[0]
"Topic 1 has word era of 1823, much older than Topic 8’s word era of 1923, in-
dicating that Topic 8 includes modern words the Bible lacks (e.g., “computer”).",3.2 Example,[0],[0]
"Using all the features, the estimator gives more accurate topic coherence evaluations.",3.2 Example,[0],[0]
"We experiment on six languages (Table 1) from three corpora: Romanian (RO) and Swedish (SV) from EuroParl as representative of well-studied and rich-resource languages (Koehn, 2005); Amharic (AM) and Tagalog (TL) from collected news, as lowresource languages (Huang et al., 2002a,b); and Chinese (ZH) and Turkish (TR) from TED Talks 2013 (Tiedemann, 2012), adding language variety to our experiments.",4 Experiments: Bible to Wikipedia,[0],[0]
"Each language is paired with English as a bilingual corpus.
",4 Experiments: Bible to Wikipedia,[0],[0]
"Typical preprocessing methods (stemming, stop word removal, etc.) are often unavailable for lowresource languages.",4 Experiments: Bible to Wikipedia,[0],[0]
"For a meaningful comparison across languages, we do not apply any stemming or lemmatization strategies, including English, except removing digit numbers and symbols.",4 Experiments: Bible to Wikipedia,[0],[0]
"However, we remove words that appear in more than 30% of documents for each language.
",4 Experiments: Bible to Wikipedia,[0],[0]
"Each language pair is separately trained using the MALLET (McCallum, 2002) implementation of the polylingual topic model.",4 Experiments: Bible to Wikipedia,[0],[0]
"Each experiment runs five Gibbs sampling chains with 1,000 iterations per chain with twenty topics.",4 Experiments: Bible to Wikipedia,[0],[0]
"The hyperparameters are set to the default values (α = 0.1, β = 0.01), and are optimized every 50 iterations in MALLET using slice sampling (Wallach et al., 2009a).",4 Experiments: Bible to Wikipedia,[0],[0]
We use Wikipedia and the Bible as reference corpora for calculating co-occurrence statistics.,4.1 Evaluating Multilingual Topics,[0],[0]
"Different numbers of Wikipedia articles are available for each language pair (Table 1), while the Bible contains a complete set of 1,189 chapters for all of its translations (Christodoulopoulos and Steed-
Are these two groups of words talking about the same thing?
man, 2015).",4.1 Evaluating Multilingual Topics,[0],[0]
We use Wiktionary as the dictionary to calculate MTA.,4.1 Evaluating Multilingual Topics,[0],[0]
"In addition to experimenting on Wikipedia-based CNPMI, we also re-evaluate the topics’ Bible coherence using our estimator.",4.2 Training the Estimator,[0],[0]
"In the following experiments, we use an AdaBoost regressor with linear regression as the coherence estimator (Friedman, 2002; Collins et al., 2000).",4.2 Training the Estimator,[0],[0]
"The estimator takes a topic and low-quality CNPMI score as input and outputs (hopefully) an improved CNPMI score.
",4.2 Training the Estimator,[0],[0]
"To make our testing scenario more realistic, we treat one language as our estimator’s test language and train on multilingual topics from the other languages.",4.2 Training the Estimator,[0],[0]
"We use three-fold cross-validation over languages to select the best hyperparameters, including the learning rate and loss function in AdaBoost.",4.2 Training the Estimator,[0],[0]
"R2 (Drucker, 1997).",4.2 Training the Estimator,[0],[0]
We first study CNPMI at the topic level: does a particular topic make sense?,5 Topic-Level Evaluation,[0],[0]
"An effective evaluation should be consistent with human judgment of the topics (Chang et al., 2009).",5 Topic-Level Evaluation,[0],[0]
"In this section, we measure gold-standard human interpretability of multilingual topics to establish which automatic measures of topic interpretability work best.",5 Topic-Level Evaluation,[0],[0]
"Following monolingual coherence evaluations (Lau et al., 2014), we present topic pairs to bilingual CrowdFlower users.",5.1 Task Design,[0],[0]
Each task is a topic pair with the top ten topic words (C = 10) for each language.,5.1 Task Design,[0],[0]
"We ask if both languages’ top words in a multilingual topic are talking about the same concept (Figure 4), and make a judgment on a three-point scale—coherent (2 points), somewhat coherent (1 point), and incoherent (0 points).",5.1 Task Design,[0],[0]
"To ensure the users have adequate language competency, we insert several topics that are easily identifiable as incoherent as a qualification test.
",5.1 Task Design,[0],[0]
"We randomly select sixty topics from each language pair (360 topics total), and each topic is judged by five users.",5.1 Task Design,[0],[0]
We take the average of the judgment points and calculate Pearson correlations with the proposed evaluation metrics (Table 2).,5.1 Task Design,[0],[0]
NPMI-based scores are separately calculated from each reference corpus.,5.1 Task Design,[0],[0]
"CNPMI (the extended metric) has higher correlations with human judgments than INPMI (the naive adaptation of monolingual NPMI), while MTA (matching translation accuracy) correlations are comparable to CNPMI.
",5.2 Agreement with Human Judgments,[0],[0]
"Unsurprisingly, when using Wikipedia as the reference, the correlations are usually higher than when using the Bible.",5.2 Agreement with Human Judgments,[0],[0]
"The Bible’s archaic content limits its ability to estimate human judgments in modern corpora (Section 3).
",5.2 Agreement with Human Judgments,[0],[0]
"Next, we compare CNPMI to two baselines: INPMI and MTA.",5.2 Agreement with Human Judgments,[0],[0]
"As expected, CNPMI outperforms INPMI regardless of reference corpus overall, because INPMI only considers monolingual coherence.",5.2 Agreement with Human Judgments,[0],[0]
"MTA has higher correlations than CNPMI
scores from the Bible, because the Bible fails to give accurate estimates due to limited topic coverage.",5.2 Agreement with Human Judgments,[0],[0]
"MTA, on the other hand, only depends on dictionaries, which are more comprehensive than the Bible.",5.2 Agreement with Human Judgments,[0],[0]
"It is also possible that users are judging coherence based on translations across a topic pair, rather than the overall coherence, which would closely correlate with MTA.",5.2 Agreement with Human Judgments,[0],[0]
The Bible—by itself—produces CNPMI values that do not correlate well with human judgments (Table 2).,5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"After training an estimator (Section 4.2), we calculate Pearson’s correlation between Wikipedia’s CNPMI and the estimated topic coherence score (Table 3).",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"A higher correlation with Wikipedia’s CNPMI means more accurate coherence.
",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"As a baseline, the correlation of Bible-based CNPMI without adaptation has negative and nearzero correlations with Wikipedia;4 it does not capture coherence.",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"After training the estimator, the correlations become stronger, indicating the estimated scores are closer to Wikipedia’s CNPMI.",5.3 Re-Estimating Topic-Level Coherence,[0],[0]
"We analyze MTA from two aspects—the inability to capture semantically-related non-translation topic words, and insensitivity to cardinality—to show why MTA is not an ideal measurement, even though it correlates well with human judgments.
",5.4 When MTA Falls Short,[0],[0]
Semantics We take two examples with EN-ZH (Topic 1) and EN-TL (Topic 2) in Figure 5.,5.4 When MTA Falls Short,[0],[0]
"Topic 1 has fewer translation pairs than Topic 2, which leads to a lower MTA score for Topic 1.",5.4 When MTA Falls Short,[0],[0]
"However, all words in Topic 1 talk about art, while it is hard to interpret Topic 2.",5.4 When MTA Falls Short,[0],[0]
"Wikipedia CNPMI scores reveals
4Normally one would not estimate CNPMI on rich-resource languages using low-resource languages.",5.4 When MTA Falls Short,[0],[0]
"For completeness, however, we also include these situations.
",5.4 When MTA Falls Short,[0],[0]
Topic 1 is more coherent.,5.4 When MTA Falls Short,[0],[0]
"Because our experiments are on datasets with little divergence between the themes discussed across languages, this is uncommon for us but could appear in noisier datasets.
",5.4 When MTA Falls Short,[0],[0]
"Cardinality Increasing cardinality diminishes a topic’s coherence (Lau and Baldwin, 2016).",5.4 When MTA Falls Short,[0],[0]
We vary the cardinality of topics from ten to fifty at intervals of ten (Figure 6).,5.4 When MTA Falls Short,[0],[0]
"As cardinality increases, more low-probability and irrelevant words appear the topic, which lowers CNPMI scores.",5.4 When MTA Falls Short,[0],[0]
"However, MTA stays stable or increases with increasing cardinality.",5.4 When MTA Falls Short,[0],[0]
"Thus, MTA fails to fulfill a critical property of topic model evaluation.
",5.4 When MTA Falls Short,[0],[0]
"Finally, MTA requires a comprehensive multilingual dictionary, which may be unavailable for lowresource languages.",5.4 When MTA Falls Short,[0],[0]
"Additionally, most languages often only have one dictionary, which makes it problematic to use the same resource (a language’s single multilingual dictionary) for training and evaluating models that use a dictionary to build multilingual topics (Hu et al., 2014).",5.4 When MTA Falls Short,[0],[0]
"Given these concerns, we continue the paper’s focus on CNPMI as a data-driven alternative to MTA.",5.4 When MTA Falls Short,[0],[0]
"However, for many applications MTA may suffice as a simple, adequate evaluation metric.",5.4 When MTA Falls Short,[0],[0]
"While the previous section looked at individual topics, we also care about how well CNPMI characterizes the quality of models through an average of a model’s constituent topics.",6 Model-Level Evaluation,[0],[0]
"Adding more knowledge to multilingual topic models improves topics (Hu et al., 2014), so an effective evaluation should reflect this improvement as knowlege is added to the model.",6.1 Training Knowledge,[0],[0]
"For polylingual topic models, this knowledge takes the form of the number of linked documents.
",6.1 Training Knowledge,[0],[0]
We start by experimenting with no multilingual knowledge: no document pairs share a topic distribution,6.1 Training Knowledge,[0],[0]
θd (but the documents are in the collection as unlinked documents).,6.1 Training Knowledge,[0],[0]
We then increase the number of document pairs that share θd from 20% of the corpus to 100%.,6.1 Training Knowledge,[0],[0]
"Fixing the topic cardinality at ten, CNPMI captures the improvements in models (Figure 7) through a higher coherence score.",6.1 Training Knowledge,[0],[0]
"Topic models are often used as a feature extraction technique for downstream machine learning
applications, and topic model evaluations should reflect whether these features are useful (Ramage et al., 2009).",6.2 Agreement with Machines,[0],[0]
"For each model, we apply a document classifier trained on the model parameters to test whether CNPMI is consistent with classification accuracy.
",6.2 Agreement with Machines,[0],[0]
"Specifically, we want our classifier to transfer information from training on one language to testing on another (Smet et al., 2011; Heyman et al., 2016).",6.2 Agreement with Machines,[0],[0]
"We train a classifier on one language’s documents, where each document’s feature vector is the document-topic distribution θd.",6.2 Agreement with Machines,[0],[0]
"We apply this to TED Talks, where each document is labeled with multiple categories.",6.2 Agreement with Machines,[0],[0]
"We choose the most frequent seven categories across the corpus as labels,5 and only have labeled documents in one side of a bilingual topic model.",6.2 Agreement with Machines,[0],[0]
"CNPMI has very strong correlations with classification results, though using the Bible as the reference corpus gives slightly lower correlation—with higher variance— than Wikipedia (Figure 8).",6.2 Agreement with Machines,[0],[0]
"In Section 5.3, we improve Bible-based CNPMI scores for individual topics.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"Here, we show the estimator also improves model-level coherence.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"We apply the estimator on the models created in Section 6.2 and calculate the correlation between estimated scores and Wikipedia’s CNPMI (Table 4).
",6.3 Re-Estimating Model-Level Coherence,[0],[0]
The coherence estimator substantially improves scores except for Turkish: the correlation is better before applying the estimator (0.911).,6.3 Re-Estimating Model-Level Coherence,[0],[0]
"We suspect a lack of overlap between topics between Turkish and languages other than Chinese is to blame (Figure 9); the features used by the estimator do not generalize well to other kinds of features; training on many languages pairs would hopefully solve this
5design, global issues, art, science, technology, business, and culture
issue.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"Turkish is also morphologically rich, and our preprocessing completely ignores morphology.",6.3 Re-Estimating Model-Level Coherence,[0],[0]
"One challenge with low-resource languages is that even if Wikipedia is available, it may have too few documents to accurately calculate coherence.",6.4 Reference Size,[0],[0]
"As a final analysis, we examine how the reliability of CNPMI degrades with a smaller reference corpus.
",6.4 Reference Size,[0],[0]
"We randomly sample 20% to 100% of document pairs from the reference corpora and evaluate the polylingual topic model with all document links (Figure 10), again fixing the cardinality as 10.
",6.4 Reference Size,[0],[0]
"CNPMI is stable across different amounts of ref-
erence documents, as long as the number of reference documents is sufficiently large.",6.4 Reference Size,[0],[0]
"If there are too few reference documents (for example, 20% of Amharic Wikipedia is only 316 documents), then CNPMI degrades.",6.4 Reference Size,[0],[0]
Topic Coherence Many coherence metrics based on co-occurrence statistics have been proposed besides NPMI.,7 Related Work,[0],[0]
"Similar metrics—such as asymmetrical word pair metrics (Mimno et al., 2011) and combinations of existing measurements (Lau et al., 2014; Röder et al., 2015)— correlate well with human judgments.",7 Related Work,[0],[0]
"NPMI has been the current gold standard for evaluation and improvements of monolingual topic models (Pecina, 2010; Newman et al., 2011).
",7 Related Work,[0],[0]
"External Tasks Another approach is to use a model for predictive tasks: the better the results are on external tasks, the better a topic model is assumed to be.",7 Related Work,[0],[0]
"A common task is held-out likelihood (Wallach et al., 2009b; Jagarlamudi and Daumé III, 2010; Fukumasu et al., 2012), but as Chang et al. (2009) show, this does not always reflect human interpretability.",7 Related Work,[0],[0]
"Other specific tasks have also been used, such as bilingual dictionary extraction (Liu et al., 2015; Ma and Nasukawa, 2017), cultural difference deteciton (Gutiérrez et al., 2016), and crosslingual document clustering (Vulić et al., 2015).
",7 Related Work,[0],[0]
"Representation Learning Topic models are one example of a broad class of techniques of learning representations of documents (Bengio et al., 2013).",7 Related Work,[0],[0]
"Other approaches learn respresentations at the word (Klementiev et al., 2012; Vyas and Carpuat, 2016), paragraph (Mogadala and Rettinger, 2016), or corpus level (Søgaard et al., 2015).",7 Related Work,[0],[0]
"However, neural representation learning approaches are often data hungry and not adaptable to low-resource languages.",7 Related Work,[0],[0]
"The approaches here could help improve the evaluation of all multilingual representation learning algorithms (Schnabel et al., 2015).",7 Related Work,[0],[0]
"We have provided a comprehensive analysis of topic model evaluation in multilingual settings, including for low-resource languages.",8 Conclusion,[0],[0]
"While evaluation is an important area of topic model research, no previous work has studied evaluation of multilingual topic models.",8 Conclusion,[0],[0]
"Our work provided two primary contributions to this area, including a new intrinsic evaluation metric, CNPMI, as well as a model for adapting this metric to low-resource languages without large reference corpora.
",8 Conclusion,[0],[0]
"As the first study on evaluation for multilingual topic models, there is still room for improvement and further applications.",8 Conclusion,[0],[0]
"For example, human judgment is more difficult to measure than in monolingual settings, and it is still an open question on how to design a reliable and accurate survey for multilingual quality judgments.",8 Conclusion,[0],[0]
"As a measurement of multilingual coherence, we plan to extend CNPMI to high-dimensional representations, e.g., multilingual word embeddings, particularly in low-resource languages (Ruder et al., 2017).",8 Conclusion,[0],[0]
We thank the anonymous reviewers for their insightful and constructive comments.,Acknowledgement,[0],[0]
"Hao has been supported under subcontract to Raytheon BBN Technologies, by DARPA award HR0011-15-C-0113.",Acknowledgement,[0],[0]
Boyd-Graber and Paul were supported by NSF grant IIS-1564275.,Acknowledgement,[0],[0]
"Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsors.",Acknowledgement,[0],[0]
Multilingual topic models enable document analysis across languages through coherent multilingual summaries of the data.,abstractText,[0],[0]
"However, there is no standard and effective metric to evaluate the quality of multilingual topics.",abstractText,[0],[0]
We introduce a new intrinsic evaluation of multilingual topic models that correlates well with human judgments of multilingual topic coherence as well as performance in downstream applications.,abstractText,[0],[0]
"Importantly, we also study evaluation for low-resource languages.",abstractText,[0],[0]
"Because standard metrics fail to accurately measure topic quality when robust external resources are unavailable, we propose an adaptation model that improves the accuracy and reliability of these metrics in low-resource settings.",abstractText,[0],[0]
Lessons from the Bible on Modern Topics: Low-Resource Multilingual Topic Model Evaluation,title,[0],[0]
"Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 741–752 Vancouver, Canada, July 30 - August 4, 2017. c©2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1069",text,[0],[0]
The importance of understanding political discourse on social media platforms is becoming increasingly clear.,1 Introduction,[0],[0]
"In recent U.S. presidential elections, Twitter was widely used by all candidates to promote their agenda, interact with supporters, and attack their opponents.",1 Introduction,[0],[0]
Social interactions on such platforms allow politicians to quickly react to current events and gauge interest in and support for their actions.,1 Introduction,[0],[0]
These dynamic settings emphasize the importance of constructing automated tools for analyzing this content.,1 Introduction,[0],[0]
"However, these same dynamics make constructing such tools difficult, as the language used to discuss new events and political agendas continuously changes.",1 Introduction,[0],[0]
"Consequently, the rich social interactions on Twitter can be leveraged to help support such analysis by providing alternatives to direct supervision.
",1 Introduction,[0],[0]
"In this paper we focus on political framing, a very nuanced political discourse analysis task, on
a variety of issues frequently discussed on Twitter.",1 Introduction,[0],[0]
"Framing (Entman, 1993; Chong and Druckman, 2007) is employed by politicians to bias the discussion towards their stance by emphasizing specific aspects of the issue.",1 Introduction,[0],[0]
"For example, the debate around increasing the minimum wage can be framed as a quality of life issue or as an economic issue.",1 Introduction,[0],[0]
"While the first frame supports increasing minimum wage because it improves workers’ lives, the second frame, by conversely emphasizing the costs involved, opposes the increase.",1 Introduction,[0],[0]
"Using framing to analyze political discourse has gathered significant interest over the last few years (Tsur et al., 2015; Card et al., 2015; Baumer et al., 2015) as a way to automatically analyze political discourse in congressional speeches and political news articles.",1 Introduction,[0],[0]
"Different from previous works which focus on these longer texts or single issues, our dataset includes tweets authored by all members of the U.S. Congress from both parties, dealing with several policy issues (e.g., immigration, ACA, etc.).",1 Introduction,[0],[0]
"These tweets were annotated by adapting the annotation guidelines developed by Boydstun et al. (2014) for Twitter.
",1 Introduction,[0],[0]
Twitter issue framing is a challenging multilabel prediction task.,1 Introduction,[0],[0]
"Each tweet can be labeled as using one or more frames, out of 17 possibilities, while only providing 140 characters as input to the classifier.",1 Introduction,[0],[0]
The main contribution of this work is to evaluate whether the social and behavioral information available on Twitter is sufficient for constructing a reliable classifier for this task.,1 Introduction,[0],[0]
"We approach this framing prediction task using a weakly supervised collective classification approach which leverages the dependencies between tweet frame predictions based on the interactions between their authors.
",1 Introduction,[0],[0]
These dependencies are modeled by connecting Twitter users who have social connections or behavioral similarities.,1 Introduction,[0],[0]
"Social connections are di-
741
rected dependencies that represent the followers of each user as well as retweeting behavior (i.e., user A retweets user B’s content).",1 Introduction,[0],[0]
"Interestingly, such social connections capture the flow of influence within political parties; however, the number of connections that cross party lines is extremely low.",1 Introduction,[0],[0]
"Instead, we rely on capturing behavioral similarity between users to provide this information.",1 Introduction,[0],[0]
"For example, users whose Twitter activity peaks at similar times tend to discuss issues in similar ways, providing indicators of their frame usage for those issues.",1 Introduction,[0],[0]
"In addition to using social and behavioral information, our approach also incorporates each politician’s party affiliation and the frequent phrases (e.g., bigrams and trigrams) used by politicians on Twitter.
",1 Introduction,[0],[0]
"These lexical, social, and behavioral features are extracted from tweets via weakly supervised models and then declaratively compiled into a graphical model using Probabilistic Soft Logic (PSL), a recently introduced probabilistic modeling framework.1 As described in Section 4, PSL specifies high level rules over a relational representation of these features.",1 Introduction,[0],[0]
"These rules are then compiled into a graphical model called a hingeloss Markov random field (Bach et al., 2013), which is used to make the frame prediction.",1 Introduction,[0],[0]
"Instead of direct supervision we take a bootstrapping approach by providing a small seed set of keywords adapted from Boydstun et al. (2014), for each frame.
",1 Introduction,[0],[0]
"Our experiments show that modeling social and behavioral connections improves F1 prediction scores in both supervised and unsupervised settings, with double the increase in the latter.",1 Introduction,[0],[0]
We apply our unsupervised model to our entire dataset of tweets to analyze framing patterns over time by both party and individual politicians.,1 Introduction,[0],[0]
"Our analysis provides insight into the usage of framing for identification of aisle-crossing politicians, i.e., those politicians who vote against their party.",1 Introduction,[0],[0]
"Issue framing is related to the broader challenges of biased language analysis (Recasens et al., 2013; Choi et al., 2012; Greene and Resnik, 2009) and subjectivity (Wiebe et al., 2004).",2 Related Work,[0],[0]
"Several previous works have explored framing in public statements, congressional speeches, and news articles (Fulgoni et al., 2016; Tsur et al., 2015; Card
1http://psl.cs.umd.edu
et al., 2015; Baumer et al., 2015).",2 Related Work,[0],[0]
"Our approach builds upon the previous work on frame analysis of Boydstun et al. (2014), by adapting and applying their annotation guidelines for Twitter.
",2 Related Work,[0],[0]
In recent years there has been growing interest in analyzing political discourse.,2 Related Work,[0],[0]
"Most previous work focuses on opinion mining and stance prediction (Sridhar et al., 2015; Hasan and Ng, 2014; Abu-Jbara et al., 2013; Walker et al., 2012; Abbott et al., 2011; Somasundaran and Wiebe, 2010, 2009).",2 Related Work,[0],[0]
"Analyzing political tweets has also attracted considerable interest: a recent SemEval task looked into stance prediction,2 and more related to our work, Tan et al. (2014) have shown how wording choices can affect message propagation on Twitter.",2 Related Work,[0],[0]
"Two recent works look into predicting stance (at user and tweet levels respectively) on Twitter using PSL (Johnson and Goldwasser, 2016; Ebrahimi et al., 2016).",2 Related Work,[0],[0]
"Frame classification, however, has a finer granularity than stance classification and describes how someone expresses their view on an issue, not whether they support the issue.",2 Related Work,[0],[0]
"Other works focus on identifying and measuring political ideologies (Iyyer et al., 2014; Bamman and Smith, 2015; Sim et al., 2013), policies (Nguyen et al., 2015), and voting patterns (Gerrish and Blei, 2012).
",2 Related Work,[0],[0]
"Exploiting social interactions and group structure for prediction has also been explored (Sridhar et al., 2015; Abu-Jbara et al., 2013; West et al., 2014).",2 Related Work,[0],[0]
"Works focusing on inferring signed social networks (West et al., 2014), stance classification (Sridhar et al., 2015), social group modeling (Huang et al., 2012), and collective classification using PSL (Bach et al., 2015) are closest to our approach.",2 Related Work,[0],[0]
"Unsupervised and weakly supervised models of Twitter data for several various tasks have been suggested, including: profile (Li et al., 2014b) and life event extraction (Li et al., 2014a), conversation modeling (Ritter et al., 2010), and methods for dealing with the unique language used in microblogs (Eisenstein, 2013).
",2 Related Work,[0],[0]
"Predicting political affiliation and other characteristics of Twitter users has been explored (Volkova et al., 2015, 2014; Yano et al., 2013; Conover et al., 2011).",2 Related Work,[0],[0]
"Others have focused on sentiment analysis (Pla and Hurtado, 2014; Bakliwal et al., 2013), predicting ideology (Djemili et al., 2014), automatic polls
2http://alt.qcri.org/semeval2016/ task6/
based on Twitter sentiment and political forecasting using Twitter (Bermingham and Smeaton, 2011; O’Connor et al., 2010; Tumasjan et al., 2010), as well as distant supervision applications (Marchetti-Bowick and Chambers, 2012).
",2 Related Work,[0],[0]
"Several works from political and social science research have studied the role of Twitter and framing in shaping public opinion of certain events, e.g. the Vancouver riots (Burch et al., 2015) and the Egyptian protests (Harlow and Johnson, 2011; Meraz and Papacharissi, 2013).",2 Related Work,[0],[0]
"Others have covered framing and sentiment analysis of opponents (Groshek and Al-Rawi, 2013) and network agenda modeling (Vargo et al., 2014) in the 2012 U.S. presidential election.",2 Related Work,[0],[0]
Jang and Hart (2015) studied frames used by the general population specific to global warming.,2 Related Work,[0],[0]
"In contrast to these works, we predict the issue-independent general frames of tweets, by U.S. politicians, which discuss six different policy issues.",2 Related Work,[0],[0]
"Data Collection and Preprocessing: We collected 184,914 of the most recent tweets of members of the U.S. Congress (both the House of Representatives and Senate).",3 Data Collection and Annotation,[0],[0]
"Using an average of ten keywords per issue, we filtered out tweets not related to the following six issues of interest: (1) limiting or gaining access to abortion, (2) debates concerning the Affordable Care Act (i.e., ACA or Obamacare), (3) the issue of gun rights versus gun control, (4) effects of immigration policies, (5) acts of terrorism, and (6) issues concerning the LGBTQ community.",3 Data Collection and Annotation,[0],[0]
"Forty politicians (10 Republicans and 10 Democrats, from both the House and Senate), were chosen randomly for annotation.",3 Data Collection and Annotation,[0],[0]
"Table 1 presents the statistics of our congressional tweets dataset, which is available for the community.3 Appendix A contains more details of our dataset and preprocessing steps.
",3 Data Collection and Annotation,[0],[0]
Data Annotation: Two graduate students were trained in the use of the Policy Frames Codebook developed by Boydstun et al. (2014) for annotating each tweet with a frame.,3 Data Collection and Annotation,[0],[0]
The general aspects of each frame are shown in Table 2.,3 Data Collection and Annotation,[0],[0]
Frames are designed to generalize across issues and overlap of multiple frames is possible.,3 Data Collection and Annotation,[0],[0]
"Additionally, the Codebook is typically applied to newspaper ar-
3The dataset and PSL scripts are available at: http://purduenlp.cs.purdue.edu/projects/ twitterframing.
",3 Data Collection and Annotation,[0],[0]
ticles where discussion of policy can encompass other frames in the text.,3 Data Collection and Annotation,[0],[0]
"Consequently, annotators using the Codebook are advised to be careful when assigning Frame 13 to a text.
",3 Data Collection and Annotation,[0],[0]
"Based on this guidance and the difficulty of labeling tweets (as discussed in Card et al. (2015)), annotators were instructed to use the following procedure: (1) attempt to assign a primary frame to the tweet if possible, (2) if not possible, assign two frames to the tweet where the first frame is chosen as the more accurate of the two frames, (3) when assigning frames 12 through 17, double check that the tweet cannot be assigned to any other frames.",3 Data Collection and Annotation,[0],[0]
Annotators spent one month labeling the randomly chosen tweets.,3 Data Collection and Annotation,[0],[0]
"For all tweets with more than one frame, annotators met to come to a consensus on whether the tweet should have one frame or both.",3 Data Collection and Annotation,[0],[0]
"The labeled dataset has an inter-annotator agreement, calculated using Cohen’s Kappa statistic, of 73.4%.
",3 Data Collection and Annotation,[0],[0]
Extensions of the Codebook for Twitter Use: The first 14 frames outlined in Table 2 are directly applicable to the tweets of U.S. politicians.,3 Data Collection and Annotation,[0],[0]
"In our labeled set, Frame 15 (Other) was never used.",3 Data Collection and Annotation,[0],[0]
"Therefore, we drop its analysis from this paper.",3 Data Collection and Annotation,[0],[0]
"From our observations, we propose the addition of the 3 frames at the bottom of Table 2 for Twitter analysis: Factual, (Self) Promotion, and Personal Sympathy and Support.",3 Data Collection and Annotation,[0],[0]
"Tweets that present a fact, with no detectable political spin or twists, are labeled as having the Factual frame (15).",3 Data Collection and Annotation,[0],[0]
"Tweets that discuss a politician’s appearances, speeches, statements, or refer to political friends are considered to have the (Self) Promotion frame.",3 Data Collection and Annotation,[0],[0]
"Finally, tweets where a politician offers their “thoughts and prayers”, condolences, or stands in support of others, are considered to have the Personal frame.
",3 Data Collection and Annotation,[0],[0]
"We find that for many tweets, one frame is not enough.",3 Data Collection and Annotation,[0],[0]
"This is caused by the compound nature of many tweets, e.g., some tweets are two separate sentences, with each sentence having a different frame or tweets begin with one frame and end with another.",3 Data Collection and Annotation,[0],[0]
"A final problem, that may also be relevant to longer text articles, is that of subframes within a larger frame.",3 Data Collection and Annotation,[0],[0]
"For example, the tweet “We must bolster the security of our borders and craft an immigration policy that grows our economy.”",3 Data Collection and Annotation,[0],[0]
has two frames: Security & Defense and Economic.,3 Data Collection and Annotation,[0],[0]
"However, both frames could fall under Frame 13 (Policy), if this tweet as a whole was a rebuttal point about an immigration policy.",3 Data Collection and Annotation,[0],[0]
"The lack of
available context for short tweets can make it difficult to determine if a tweet should have one primary frame or is more accurately represented by multiple frames.",3 Data Collection and Annotation,[0],[0]
"Due to the dynamic nature of political discourse on Twitter, our approach is designed to require as little supervision as possible.",4 Global Models of Twitter Language and Activity,[0],[0]
We implement 6 weakly supervised models which are datadependent and used to extract and format information from tweets into input for PSL predicates.,4 Global Models of Twitter Language and Activity,[0],[0]
These predicates are then combined into the probabilistic rules of each model as shown in Table 3.,4 Global Models of Twitter Language and Activity,[0],[0]
"The only sources of supervision these models require includes: unigrams related to the issues, unigrams adapted from the Boydstun et al. (2014) Codebook for frames, and political party of the author of the tweets.",4 Global Models of Twitter Language and Activity,[0],[0]
"PSL is a declarative modeling language which can be used to specify weighted, first-order logic rules.",4.1 Global Modeling Using PSL,[0],[0]
"These rules are compiled into a hinge-loss Markov random field which defines a probability distribution over possible continuous value assignments to the random variables of the model (Bach et al.,
2015).4 This probability density function is represented as:
P (Y | X) = 1 Z exp
MX
r=1
r r(Y , X)
!
where Z is a normalization constant, is the weight vector, and
r(Y, X) =",4.1 Global Modeling Using PSL,[0],[0]
"(max{lr(Y, X), 0})⇢r
is the hinge-loss potential specified by a linear function lr.",4.1 Global Modeling Using PSL,[0],[0]
"The exponent ⇢r 2 1, 2 is optional.",4.1 Global Modeling Using PSL,[0],[0]
"Each potential represents the instantiation of a rule, which takes the following form:
1 : P1(x) ^",4.1 Global Modeling Using PSL,[0],[0]
"P2(x, y) !",4.1 Global Modeling Using PSL,[0],[0]
P3(y) 2 : P1(x) ^,4.1 Global Modeling Using PSL,[0],[0]
"P4(x, y) !",4.1 Global Modeling Using PSL,[0],[0]
"¬P3(y)
P1, P2, P3, and P4 are predicates (e.g., political party, issue, frame, and presence of n-grams) and x, y are variables.",4.1 Global Modeling Using PSL,[0],[0]
Each rule has a weight which reflects that rule’s importance and is learned using the Expectation-Maximization algorithm in our unsupervised experiments.,4.1 Global Modeling Using PSL,[0],[0]
"Using concrete constants a, b (e.g., tweets and words) which instantiate the variables x, y, model atoms are mapped
4Unlike other probabilistic logical models, e.g. MLNs, in which the model’s random variables are strictly true or false.
",4.1 Global Modeling Using PSL,[0],[0]
"to continuous [0,1] assignments.",4.1 Global Modeling Using PSL,[0],[0]
"More important rules (i.e., those with larger weights) are given preference by the model.",4.1 Global Modeling Using PSL,[0],[0]
"Unigrams: Using the guidelines provided in the Policy Frames Codebook (Boydstun et al., 2014), we adapted a list of expected unigrams for each frame.",4.2 Language Based Models,[0],[0]
"For example, unigrams that should be related to Frame 12 (Political Factors & Implications) include: filibuster, lobby, Democrats, Republicans.",4.2 Language Based Models,[0],[0]
"We expect that if a tweet and frame contain a matching unigram, then that frame is likely present in that tweet.",4.2 Language Based Models,[0],[0]
"The information that tweet T has expected unigram U of frame F is represented with the PSL predicate: UNIGRAMF (T, U).",4.2 Language Based Models,[0],[0]
"This knowledge is then used as input to PSL Model 1 via the rule: UNIGRAMF (T, U) !",4.2 Language Based Models,[0],[0]
"FRAME(T, F) (shown in line 1 of Table 3).
",4.2 Language Based Models,[0],[0]
"However, not every tweet will have a unigram that matches those in this list.",4.2 Language Based Models,[0],[0]
"Under the intuition that at least one unigram in a tweet should be similar to a unigram in the list, we designed the following MaxSim metric to compute the maximum similarity between a word in a tweet and a word from the list of unigrams.
MAXSIM(T, F) = arg max u2F,w2T SIMILARITY(W,U)
(1) T is a tweet, W is each word in T, and U is each unigram in the list of expected unigrams (per frame).",4.2 Language Based Models,[0],[0]
SIMILARITY is the computed word2vec similarity (using pretrained embeddings) of each word in the tweet with every unigram in the list of unigrams for each frame.,4.2 Language Based Models,[0],[0]
"The frame F of the maximum scoring unigram is input to the PSL predicate: MAXSIMF (T, F), which indicates that tweet T has the highest similarity to frame F.
Bigrams and Trigrams:",4.2 Language Based Models,[0],[0]
"In addition to unigrams, we also explored the effects of political party slogans on frame prediction.",4.2 Language Based Models,[0],[0]
Slogans are common catch phrases or sayings that people typically associate with different U.S. political parties.,4.2 Language Based Models,[0],[0]
"For example, Republicans are known for using the phrase “repeal and replace” when they discuss the ACA.",4.2 Language Based Models,[0],[0]
"Similarly, in the 2016 U.S. presidential election, Secretary Hillary Clinton’s campaign slogan became “Love Trumps Hate”.",4.2 Language Based Models,[0],[0]
"To visualize slogan usage by parties for different issues, we used the entire tweets dataset, including all unlabeled tweets, to extract the top bigrams
and trigrams per party for each issue.",4.2 Language Based Models,[0],[0]
The histograms in Figure 1 show these distributions for the top 100 bigrams and trigrams.,4.2 Language Based Models,[0],[0]
"Based on these results, we use the top 20 bigrams (e.g., women’s healthcare and immigration reform) and trigrams (e.g. prevent gun violence) as input to PSL predicates BIGRAMIP (T, B) and TRIGRAMIP (T, TG).",4.2 Language Based Models,[0],[0]
These rules represent that tweet T has bigram B or trigram TG from the respective issue I phrase lists of either party P.,4.2 Language Based Models,[0],[0]
"In addition to language based features of tweets, we also exploit the behavioral and social features of Twitter including similarities between temporal activity and network relationships.
",4.3 Twitter Behavior Based Models,[0],[0]
Temporal Similarity: We construct a temporal histogram for each politician which captures their Twitter activity over time.,4.3 Twitter Behavior Based Models,[0],[0]
When an event happens politicians are most likely to tweet about that event within hours of its occurrence.,4.3 Twitter Behavior Based Models,[0],[0]
"Similarly, most politicians tweet about the event most frequently the day of the event and this frequency decreases over time.",4.3 Twitter Behavior Based Models,[0],[0]
"From these temporal histograms, we observed that the frames used the day of an event were similar and gradually changed over time.",4.3 Twitter Behavior Based Models,[0],[0]
"For example, once the public is notified of a shooting, politicians respond with Frame 17 to offer sympathy to the victims and their families.",4.3 Twitter Behavior Based Models,[0],[0]
"Over the next days or weeks, both parties slowly transition to using additional frames, e.g. Democrats use Frame 7 to argue for gun control legislation.",4.3 Twitter Behavior Based Models,[0],[0]
"To capture this behavior we use the PSL predicate SAMETIME(T1, T2).",4.3 Twitter Behavior Based Models,[0],[0]
"This indicates that tweet T1 occurs around the same time as tweet
T2.5 This information is used in Model 4 via rules such as: SAMETIME(T1, T2) & FRAME(T1, F) !",4.3 Twitter Behavior Based Models,[0],[0]
"FRAME(T2, F), as shown in line 4 of Table 3.
",4.3 Twitter Behavior Based Models,[0],[0]
"Network Similarity: Finally, we expect that politicians who share ideologies, and thus are likely to frame issues similarly, will retweet and/or follow each other on Twitter.",4.3 Twitter Behavior Based Models,[0],[0]
"Due to the compound nature of tweets, retweeting with additional comments can add more frames to the original tweet.",4.3 Twitter Behavior Based Models,[0],[0]
"Additionally, politicians on Twitter are more likely to follow members of their own party or similar non-political entities than those of the opposing party.",4.3 Twitter Behavior Based Models,[0],[0]
"To capture this network-based behavior we use two PSL predicates: RETWEETS(T1, T2) and FOLLOWS(T1, T2).",4.3 Twitter Behavior Based Models,[0],[0]
"These predicates indicate that the content of tweet T1 includes a retweet of tweet T2 and that the author of T1 follows the author of T2 on Twitter, respectively.",4.3 Twitter Behavior Based Models,[0],[0]
The last two lines of Table 3 show examples of how network similarity is incorporated into PSL rules.,4.3 Twitter Behavior Based Models,[0],[0]
"Evaluation Metrics: Since each tweet can have more than one frame, our prediction task is a multilabel classification task.",5 Experiments,[0],[0]
"The precision of a multilabel model is the ratio of how many predicted labels are correct:
Precision = 1
T
TX
t=1
|Yt \ h(xt)|",5 Experiments,[0],[0]
"|h(xt)|
(2)
",5 Experiments,[0],[0]
"The recall of this model is the ratio of how many of the actual labels were predicted:
Recall = 1
T
TX
t=1
|Yt \ h(xt)|",5 Experiments,[0],[0]
"|Yt|
(3)
5We conducted experiments with different hour and day limits and found that using a time frame of one hour results in the best accuracy while limiting noise.
",5 Experiments,[0],[0]
"In both formulas, T is the number of tweets, Yt is the true label for tweet t, xt is a tweet example, and h(xt) are the predicted labels for that tweet.",5 Experiments,[0],[0]
The F1 score is computed as the harmonic mean of the precision and recall.,5 Experiments,[0],[0]
"Additionally, in Tables 4, 5, and 6 the reported average is the micro-weighted average F1 scores over all frames.
",5 Experiments,[0],[0]
Experimental Settings: We provide an analysis of our PSL models under both supervised and unsupervised settings.,5 Experiments,[0],[0]
"In the PSL supervised experiments, we used five-fold cross validation with randomly chosen splits.
",5 Experiments,[0],[0]
"Previous works typically use an SVM, with bagof-words features, which is not used in a multilabel prediction, i.e., each frame is predicted individually.",5 Experiments,[0],[0]
The results of this approach on our dataset are shown in column 2 of Table 4.,5 Experiments,[0],[0]
"In this scenario, the SVM tends to prefer the majority class, which results in many incorrect labels.",5 Experiments,[0],[0]
Column 3 shows the results of using an SVM with bag-of-words features to perform multilabel classification.,5 Experiments,[0],[0]
This approach decreases the F1 score for a majority of frames.,5 Experiments,[0],[0]
"Both SVMs also result in F1 scores of 0 for some frames, further lowering the overall performance.",5 Experiments,[0],[0]
"Finally, columns 4 and 5 show the results of using our worst and best PSL models, respectively.",5 Experiments,[0],[0]
"PSL Model 1, which uses our adapted unigram features instead of the bag-of-words features for multilabel classification, serves as our baseline to improve upon.",5 Experiments,[0],[0]
"Additionally, Model 6 of the supervised, collective network setting represents the best results we can achieve.
",5 Experiments,[0],[0]
We also explore the results of our PSL models in an unsupervised setting because the highly dynamic nature of political discourse on Twitter makes it unrealistic to expect annotated data to generalize to future discussions.,5 Experiments,[0],[0]
The only source of supervision comes from the initial unigrams lists and party information as described in Section 4.,5 Experiments,[0],[0]
The labeled tweets are used for evaluation only.,5 Experiments,[0],[0]
"As seen in Table 4, we are able to improve
the best unsupervised model to within an F1 score of 7.36 points of the unigram baseline of 66.02, and 19.13 points of the best supervised score of 77.79.
",5 Experiments,[0],[0]
Analysis of Supervised Experiments: Table 5 shows the results of our supervised experiments.,5 Experiments,[0],[0]
"Here we can see that by adding Twitter behavior (beginning with Model 4), our behaviorbased models achieve the best F1 scores across all frames.",5 Experiments,[0],[0]
"Model 4 achieves the highest results on two frames, suggesting retweeting and network follower information do not help improve the prediction score for these frames.",5 Experiments,[0],[0]
"Similarly, Model 5 achieves the highest prediction for 5 of the frames, suggesting network follower information cannot further improve the score for these frames.",5 Experiments,[0],[0]
"Overall, the Twitter behavior based models are able to outperform language based models alone, including the best performing language model (Model 3) which combines unigrams, bigrams, and trigrams together to collectively infer the correct frames.
",5 Experiments,[0],[0]
"Analysis of Unsupervised Experiments: In the unsupervised setting, Model 6, the combination of language and Twitter behavior features achieves the best results on 16 of the 17 issues, as shown in Table 6.",5 Experiments,[0],[0]
There are a few interesting aspects of the unsupervised setting which differ from the supervised setting.,5 Experiments,[0],[0]
"Six of the frame predictions do worse in Model 2, which is double that of the supervised version.",5 Experiments,[0],[0]
"This is likely due to the presence of overlapping bigrams across frames and issues, e.g., “women’s healthcare” could appear in both Frames 4 and 8 and the issues of ACA and abortion.",5 Experiments,[0],[0]
"However, all six are able to improve with the addition of trigrams (Model 3), whereas only 1 of 3 frames improves in the supervised setting.",5 Experiments,[0],[0]
This suggests that bigrams may not be as useful as trigrams in an unsupervised setting.,5 Experiments,[0],[0]
"Finally, in Model 5, which adds retweet behaviors, we notice that 5 of the frames decrease in F1 score and 11
of the frames have the same score as the previous model.",5 Experiments,[0],[0]
These results suggest that retweet behaviors are not as useful as the follower network relationships in an unsupervised setting.,5 Experiments,[0],[0]
"To explore the usefulness of frame identification in political discourse analysis, we apply our best performing model (Model 6) on the unlabeled dataset to determine framing patterns over time, both by party and individual.",6 Qualitative Analysis,[0],[0]
Figure 2 shows the results of our frame analysis for both parties over time for two issues: ACA and terrorism.6 We compiled the predicted frames for tweets from 2014 to 2016 for each party.,6 Qualitative Analysis,[0],[0]
"Figure 3 presents the results of frame prediction for 2015 tweets of aisle-crossing individual politicians for these two issues.
",6 Qualitative Analysis,[0],[0]
"Party Frames: From Figure 2(a) we can see that Democrats mainly use Frames 1, 4, 8, 9, and 15 to discuss ACA, while Figure 2(c) shows that Republicans predominantly use Frames 1, 8, 9, 12, and 13.",6 Qualitative Analysis,[0],[0]
"Though the parties use similar frames, they are used to express different agendas.",6 Qualitative Analysis,[0],[0]
"For example, Democrats use Frame 8 to indicate the positive effect that the ACA has had in granting more Americans health care access.",6 Qualitative Analysis,[0],[0]
"Republicans, however, use Frame 8 (and Frame 13) to indicate their party’s agenda to replace the ACA with access to different options for health care.",6 Qualitative Analysis,[0],[0]
"Additionally, Democrats use the Fairness & Equality Frame (Frame 4) to convey that the ACA gives minority groups a better chance at accessing health care.
6Due to space, we omit the other 4 issues.",6 Qualitative Analysis,[0],[0]
"These 2 were chosen because they are among the most frequently discussed issues in our dataset.
",6 Qualitative Analysis,[0],[0]
They also use Frame 15 to express statistics about enrollment of Americans under the ACA.,6 Qualitative Analysis,[0],[0]
"Finally, Republicans use Frames 12 and 13 to bring attention to their own party’s actions to “repeal and replace” the ACA with different policies.
",6 Qualitative Analysis,[0],[0]
Figures 2(b) and 2(d) show the party-based framing patterns over time for terrorism related tweets.,6 Qualitative Analysis,[0],[0]
"For this issue both parties use similar frames: 3, 7, 10, 14, 16, and 17, but to express different views.",6 Qualitative Analysis,[0],[0]
"For example, Democrats use Frame 3 to indicate a moral responsibility to fight ISIS.",6 Qualitative Analysis,[0],[0]
Republicans use Frame 3 to frame terrorists or their attacks as a result of “radical Islam”.,6 Qualitative Analysis,[0],[0]
An interesting pattern to note is seen in Frames 10 and 14 for both parties.,6 Qualitative Analysis,[0],[0]
"In 2015 there is a large in-
crease in the usage of this frame.",6 Qualitative Analysis,[0],[0]
"This seems to indicate that parties possibly adopt new frames simultaneously or in response to the opposing party, perhaps in an effort to be in control of the way the message is delivered through that frame.
",6 Qualitative Analysis,[0],[0]
"Individual Frames: In addition to entire party analysis, we were interested in seeing if frames could shed light on the behavior of aisle-crossing politicians.",6 Qualitative Analysis,[0],[0]
"These are politicians who do not vote the same as the majority vote of their party (i.e., they vote the same as the opposing party).",6 Qualitative Analysis,[0],[0]
"Identifying such politicians can be useful in governments which are heavily split by party, i.e., governments such as the recent U.S. Congress (2015 to 2017), where politicians tend to vote the same
as the rest of their party members.",6 Qualitative Analysis,[0],[0]
"For this analysis, we collected five 2015 votes from the House of Representatives on both issues and compiled a list of the politicians who voted opposite to their party.",6 Qualitative Analysis,[0],[0]
The most important descriptor we noticed was that all aisle-crossing politicians tweet less frequently on the issue than their fellow party members.,6 Qualitative Analysis,[0],[0]
This is true for both parties.,6 Qualitative Analysis,[0],[0]
"This behavior could indicate lack of desire to draw attention to one’s stance on the particular issue.
",6 Qualitative Analysis,[0],[0]
Figure 3(a) shows the framing patterns of aislecrossing Republicans on ACA votes from 2015.,6 Qualitative Analysis,[0],[0]
"Recall from Figure 2 that Democrats mostly use Frames 1, 4, 8, 9, and 15, while Republicans mainly use Frames 1, 8, and 9.",6 Qualitative Analysis,[0],[0]
"In this example, these Republicans are considered aislecrossing votes because they have voted the same as Democrats on this issue.",6 Qualitative Analysis,[0],[0]
"The most interesting pattern to note here is that these Republicans use the same framing patterns as the Republicans (Frames 1, 8, and 9), but they also use the frames that are unique to Democrats: Frames 4 and 15.",6 Qualitative Analysis,[0],[0]
These latter two frames appear significantly less in the Republican tweets of our entire dataset as well.,6 Qualitative Analysis,[0],[0]
"These results suggest that to predict aisle-crossing
Republicans it would be useful to check for usage of typically Democrat-associated frames, especially if those frames are infrequently used by Republicans.
",6 Qualitative Analysis,[0],[0]
Figure 3(b) shows the predicted frames for aisle-crossing Democrats on terrorism-related votes.,6 Qualitative Analysis,[0],[0]
"We see here that there are very few tweets from these Democrats on this issue and that overall they use the same framing patterns as seen previously: Frames 3, 7, 10, 14, 16, and 17.",6 Qualitative Analysis,[0],[0]
"However, given the small scale of these tweets, we can also consider Frames 12 and 13 to show peaks for this example.",6 Qualitative Analysis,[0],[0]
This suggests that for aisle-crossing Democrats the use of additional frames not often used by their party for discussing an issue might indicate potentially different voting behaviors.,6 Qualitative Analysis,[0],[0]
In this paper we present the task of collective classification of Twitter data for framing prediction.,7 Conclusion,[0],[0]
"We show that by incorporating Twitter behaviors such as similar activity times and similar networks, we can increase F1 score prediction.",7 Conclusion,[0],[0]
"We provide an analysis of our approach in both supervised and unsupervised settings, as well as a real world analysis of framing patterns over time.",7 Conclusion,[0],[0]
"Finally, our global PSL models can be applied to other domains, such as politics in other countries, simply by changing the initial unigram keywords to reflect the politics of those countries.",7 Conclusion,[0],[0]
We thank the anonymous reviewers for their thoughtful comments and suggestions.,Acknowledgments,[0],[0]
"In this section we provide additional information about our congressional tweets dataset, as well as the lists of keywords and phrases used to filter tweets by issue and the unigrams used to extract information used for the Unigram and MaxSim PSL predicates.",A Supplementary Material,[0],[0]
"It is important to note that during preprocessing capitalization, stop words, URLs, and punctuation have been removed from tweets in our dataset.",A Supplementary Material,[0],[0]
"Additional word lists along with our PSL scripts and dataset are available at: http://purduenlp.cs.purdue.edu/ projects/twitterframing.
",A Supplementary Material,[0],[0]
Dataset Statistics:,A Supplementary Material,[0],[0]
Figure 4 shows the coverage of the labeled frames by party.,A Supplementary Material,[0],[0]
"From this, general patterns can be observed.",A Supplementary Material,[0],[0]
"For example, Republicans use Frames 12 and 17 more frequently than Democrats, while Democrats tend to use Frames 4, 9, 10, and 11.",A Supplementary Material,[0],[0]
"Table 7 shows the count of each type of frame that appears in each issue in our labeled dataset.
",A Supplementary Material,[0],[0]
Word Lists: Table 8 lists the keywords or phrases used to filter the entire dataset to only tweets related to the six issues studied in this paper.,A Supplementary Material,[0],[0]
"Table 9 lists the unigrams that were designed based on the descriptions for Frames 1 through 14
provided in the Policy Frames Codebook (Boydstun et al., 2014).",A Supplementary Material,[0],[0]
These unigrams provide the initial supervision for our models as described in Section 4.,A Supplementary Material,[0],[0]
Framing is a political strategy in which politicians carefully word their statements in order to control public perception of issues.,abstractText,[0],[0]
"Previous works exploring political framing typically analyze frame usage in longer texts, such as congressional speeches.",abstractText,[0],[0]
"We present a collection of weakly supervised models which harness collective classification to predict the frames used in political discourse on the microblogging platform, Twitter.",abstractText,[0],[0]
"Our global probabilistic models show that by combining both lexical features of tweets and network-based behavioral features of Twitter, we are able to increase the average, unsupervised F1 score by 21.52 points over a lexical baseline alone.",abstractText,[0],[0]
Leveraging Behavioral and Social Information for Weakly Supervised Collective Classification of Political Discourse on Twitter,title,[0],[0]
"Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 345–350 Melbourne, Australia, July 15 - 20, 2018. c©2018 Association for Computational Linguistics
345",text,[0],[0]
"Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (Baldwin and Kim, 2010).",1 Introduction,[1.0],"['Multiword expressions (MWEs) are combinations of multiple words that exhibit some degree of idiomaticity (Baldwin and Kim, 2010).']"
"Verb–noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (Fazly et al., 2009).",1 Introduction,[1.0],"['Verb–noun combinations (VNCs), consisting of a verb with a noun in its direct object position, are a common type of semantically-idiomatic MWE in English and cross-lingually (Fazly et al., 2009).']"
"Many VNCs are ambiguous between MWEs and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage (i.e., an MWE), while 2 is a literal combination.1
1.",1 Introduction,[0.999999947554715],"['Many VNCs are ambiguous between MWEs and literal combinations, as in the following examples of see stars, in which 1 is an idiomatic usage (i.e., an MWE), while 2 is a literal combination.1 1.']"
"Hereford United were seeing stars at Gillingham after letting in 2 early goals
2.",1 Introduction,[1.0000000514897418],['Hereford United were seeing stars at Gillingham after letting in 2 early goals 2.']
"Look into the night sky to see the stars 1These examples, and idiomaticity judgements, are taken
from the VNC-Tokens dataset (Cook et al., 2008).
",1 Introduction,[0.9999998851424748],"['Look into the night sky to see the stars 1These examples, and idiomaticity judgements, are taken from the VNC-Tokens dataset (Cook et al., 2008).']"
"MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010), and must be able to make such distinctions.",1 Introduction,[1.0],"['MWE identification is the task of automatically determining which word combinations at the token-level form MWEs (Baldwin and Kim, 2010), and must be able to make such distinctions.']"
"This is particularly important for applications such as machine translation (Sag et al., 2002), where the appropriate meaning of word combinations in context must be preserved for accurate translation.
",1 Introduction,[0.999999963770474],"['This is particularly important for applications such as machine translation (Sag et al., 2002), where the appropriate meaning of word combinations in context must be preserved for accurate translation.']"
"In this paper, following prior work (e.g., Salton et al., 2016), we frame token-level identification of VNCs as a supervised binary classification problem, i.e., idiomatic vs. literal.",1 Introduction,[0],[0]
"We consider a range of approaches to forming distributed representations of the context in which a VNC occurs, including word embeddings (Mikolov et al., 2013), word embeddings tailored to representing sentences (Kenter et al., 2016), and skip-thoughts sentence embeddings (Kiros et al., 2015).",1 Introduction,[0],[0]
We then train a support vector machine (SVM) on these representations to classify unseen VNC instances.,1 Introduction,[0],[0]
"Surprisingly, we find that an approach based on representing sentences as the average of their word embeddings performs comparably to, or better than, the skip-thoughts based approach previously proposed by Salton et al. (2016).
VNCs exhibit lexico-syntactic fixedness.",1 Introduction,[0],[0]
"For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars.",1 Introduction,[1.0],"['For example, the idiomatic interpretation in example 1 above is typically only accessible when the verb see has active voice, the determiner is null, and the noun star is in plural form, as in see stars or seeing stars.']"
"Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation.
",1 Introduction,[1.0000000114381162],"['Usages with a determiner (as in example 2), a singular noun (e.g., see a star), or passive voice (e.g., stars were seen) typically only have the literal interpretation.']"
In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs — automatically acquired from corpora using the method of Fazly et al. (2009) — into our various embedding-based approaches.,1 Introduction,[1.0],['In this paper we further incorporate knowledge of the lexico-syntactic fixedness of VNCs — automatically acquired from corpora using the method of Fazly et al. (2009) — into our various embedding-based approaches.']
"Our experimental results show that this leads to substantial improve-
ments, indicating that this rich linguistic knowledge is complementary to that available in distributed representations.",1 Introduction,[0.9852454893337298],['Our findings indicate that this rich linguistic knowledge is complementary to that available in distributed representations.']
"Much research on MWE identification has focused on specific kinds of MWEs (e.g., Patrick and Fletcher, 2005; Uchiyama et al., 2005), including English VNCs (e.g., Fazly et al., 2009; Salton et al., 2016), although some recent work has considered the identification of a broad range of kinds of MWEs (e.g., Schneider et al., 2014; Brooke et al., 2014; Savary et al., 2017).
",2 Related work,[0],[0]
"Work on MWE identification has leveraged rich linguistic knowledge of the constructions under consideration (e.g., Fazly et al., 2009; Fothergill and Baldwin, 2012), treated literal and idiomatic as two senses of an expression and applied approaches similar to word-sense disambiguation (e.g., Birke and Sarkar, 2006; Hashimoto and Kawahara, 2008), incorporated topic models (e.g., Li et al., 2010), and made use of distributed representations of words (Gharbieh et al., 2016).
",2 Related work,[0],[0]
"In the most closely related work to ours, Salton et al. (2016) represent token instances of VNCs by embedding the sentence that they occur in using skip-thoughts (Kiros et al., 2015) — an encoder– decoder model that can be viewed as a sentencelevel counterpart to the word2vec (Mikolov et al., 2013) skip-gram model.",2 Related work,[0],[0]
"During training the target sentence is encoded using a recurrent neural network, and is used to predict the previous and next sentences.",2 Related work,[0],[0]
"Salton et al. then use these sentence embeddings, representing VNC token instances, as features in a supervised classifier.",2 Related work,[0],[0]
"We treat this skip-thoughts based approach as a strong baseline to compare against.
",2 Related work,[0],[0]
"Fazly et al. (2009) formed a set of eleven lexicosyntactic patterns for VNC instances capturing the voice of the verb (active or passive), determiner (e.g., a, the), and number of the noun (singular or plural).",2 Related work,[0],[0]
"They then determine the canonical form, C(v, n), for a given VNC as follows:2
C(v, n)",2 Related work,[0],[0]
=,2 Related work,[0],[0]
"{ptk ∈ P |z(v, n, ptk) >",2 Related work,[0],[0]
"Tz} (1) where P is the set of patterns, Tz is a predetermined threshold, which is set to 1, and z(v, n, ptk) is calculated as follows:
z(v, n, ptk) = f(v, n,",2 Related work,[0],[0]
"ptk)− f
s (2)
2In a small number of cases a VNC is found to have a small number of canonical forms, as opposed to just one.
",2 Related work,[0],[0]
"where f(·) is the frequency of a VNC occurring in a given pattern in a corpus,3 and f and s are the mean and standard deviations for all patterns for the given VNC, respectively.
",2 Related work,[0],[0]
"Fazly et al. (2009) showed that idiomatic usages of a VNC tend to occur in that expression’s canonical form, while literal usages do not.",2 Related work,[0],[0]
"This approach provides a strong, linguistically-informed, unsupervised baseline, referred to as CForm, for predicting whether VNC instances are idiomatic or literal.",2 Related work,[0],[0]
"In this paper we incorporate knowledge of canonical forms into embedding-based approaches to VNC token classification, and show that this linguistic knowledge can be leveraged to improve such approaches.",2 Related work,[0],[0]
We describe the models used to represent VNC token instances below.,3 Models,[1.0],['We describe the models used to represent VNC token instances below.']
"For each model, a linear SVM classifier is trained on these representations.",3 Models,[1.0],"['For each model, a linear SVM classifier is trained on these representations.']"
"We trained word2vec’s skip-gram model (Mikolov et al., 2013) on a snapshot of Wikipedia from September 2015, which consists of approximately 2.6 billion tokens.",3.1 Word2vec,[0],[0]
We used a window size of ±8 and 300 dimensions.,3.1 Word2vec,[0],[0]
"We ignore all words that occur less than fifteen times in the training corpus, and did not set a maximum vocabulary size.",3.1 Word2vec,[0],[0]
We perform negative sampling and set the number of training epochs to five.,3.1 Word2vec,[0],[0]
"We used batch processing with approximately 10k words in each batch.
",3.1 Word2vec,[0],[0]
"To embed a given a sentence containing a VNC token instance, we average the word embeddings for each word in the sentence, including stopwords.4 Prior to averaging, we normalize each embedding to have unit length.",3.1 Word2vec,[0],[0]
"The Siamese CBOW model (Kenter et al., 2016) learns word embeddings that are better able to represent a sentence through averaging than conventional word embeddings such as skip-gram or CBOW.",3.2 Siamese CBOW,[1.0],"['The Siamese CBOW model (Kenter et al., 2016) learns word embeddings that are better able to represent a sentence through averaging than conventional word embeddings such as skip-gram or CBOW.']"
"We use a Siamese CBOW model that was pretrained on a snapshot of Wikipedia from November 2012 using randomly initialized word
3Fazly et al. (2009) used the British National Corpus (Burnard, 2000).
",3.2 Siamese CBOW,[0],[0]
"4Preliminary experiments showed that models performed better when stopword removal was not applied.
embeddings.5 Similarly to the word2vec model, to embed a given sentence containing a VNC instance, we average the word embeddings for each word in the sentence.",3.2 Siamese CBOW,[0],[0]
"We use a publicly-available skip-thoughts model, that was pre-trained on a corpus of books.6 We represent a given sentence containing a VNC instance using the skip-thoughts encoder.",3.3 Skip-thoughts,[0],[0]
"Note that this approach is our re-implementation of the skipthoughts based method of Salton et al. (2016), and we use it as a strong baseline for comparison.",3.3 Skip-thoughts,[0],[0]
"In this section, we discuss the dataset used in our experiments, and the evaluation of our models.",4 Data and evaluation,[0],[0]
"We use the VNC-Tokens dataset (Cook et al., 2008) — the same dataset used by Fazly et al. (2009) and Salton et al. (2016) — to train and evaluate our models.",4.1 Dataset,[0],[0]
"This dataset consists of sentences containing VNC usages drawn from the British National Corpus (Burnard, 2000),7 along with a label indicating whether the VNC is an idiomatic or literal usage (or whether this cannot be determined, in which case it is labelled “unknown”).
",4.1 Dataset,[0],[0]
VNC-Tokens is divided into DEV and TEST sets that each include fourteen VNC types and a total of roughly six hundred instances of these types annotated as literal or idiomatic.,4.1 Dataset,[0],[0]
"Following Salton et al. (2016), we use DEV and TEST, and ignore all token instances annotated as “unknown”.
",4.1 Dataset,[0],[0]
Fazly et al. (2009) and Salton et al. (2016) structured their experiments differently.,4.1 Dataset,[0],[0]
Fazly et al. report results over DEV and TEST separately.,4.1 Dataset,[0],[0]
In this setup TEST consists of expressions that were not seen during model development (done on DEV).,4.1 Dataset,[0],[0]
"Salton et al., on the other hand, merge DEV and TEST, and create new training and testing sets, such that each expression is present in the training and testing data, and the ratio of idiomatic to literal usages of each expression in the training data is roughly equal to that in the testing data.
",4.1 Dataset,[0],[0]
We borrowed ideas from both of these approaches in structuring our experiments.,4.1 Dataset,[0],[0]
"We retain
5https://bitbucket.org/TomKenter/ siamese-cbow
6https://github.com/ryankiros/ skip-thoughts
7http://www.natcorp.ox.ac.uk
the type-level division of Fazly et al. (2009) into DEV and TEST.",4.1 Dataset,[0],[0]
"We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as Salton et al. (2016).",4.1 Dataset,[1.0],"['We then divide each of these into training and testing sets, using the same ratios of idiomatic to literal usages for each expression as Salton et al. (2016).']"
"This allows us to develop and tune a model on DEV, and then determine whether, when retrained on instances of unseen VNCs in (the training portion of) TEST, that model is able to generalize to new VNCs without further tuning to the specific expressions in TEST.",4.1 Dataset,[0],[0]
The proportion of idiomatic usages in the testing portions of both DEV and TEST is 63%.,4.2 Evaluation,[0],[0]
We therefore use accuracy to evaluate our models following Fazly et al. (2009) because the classes are roughly balanced.,4.2 Evaluation,[0],[0]
"We randomly divide both DEV and TEST into training and testing portions ten times, following Salton et al. (2016).",4.2 Evaluation,[0],[0]
"For each of the ten runs, we compute the accuracy for each expression, and then compute the average accuracy over the expressions.",4.2 Evaluation,[0],[0]
We then report the average accuracy over the ten runs.,4.2 Evaluation,[0],[0]
"In this section we first consider the effect of tuning the cost parameter of the SVM for each model on DEV, and then report results on DEV and TEST using the tuned models.",5 Experimental results,[0],[0]
"We tune the SVM for each model on DEV by carrying out a linear search for the penalty cost from 0.01–100, increasing by a factor of ten each time.",5.1 Parameter tuning,[0],[0]
Results for this parameter tuning are shown in Table 1.,5.1 Parameter tuning,[0],[0]
These results highlight the importance of choosing an appropriate setting for the penalty cost.,5.1 Parameter tuning,[0],[0]
"For example, the accuracy of the word2vec model ranges from 0.619–0.830 depending on the cost setting.",5.1 Parameter tuning,[0],[0]
"In subsequent experiments, for each
model, we use the penalty cost that achieves the highest accuracy in Table 1.",5.1 Parameter tuning,[0],[0]
"In Table 2 we report results on DEV and TEST for each model, as well as the unsupervised CForm model of Fazly et al. (2009), which simply labels a VNC as idiomatic if it occurs in its canonical form, and as literal otherwise.",5.2 DEV and TEST results,[1.0],"['In Table 2 we report results on DEV and TEST for each model, as well as the unsupervised CForm model of Fazly et al. (2009), which simply labels a VNC as idiomatic if it occurs in its canonical form, and as literal otherwise.']"
We further consider each model (other than CForm) in two setups.,5.2 DEV and TEST results,[0],[0]
−CF corresponds to the models as described in Section 3.,5.2 DEV and TEST results,[0],[0]
"+CF further incorporates lexico-syntactic knowledge of canonical forms into each model by concatenating the embedding representing each VNC token instance with a one-dimensional vector which is one if the VNC occurs in its canonical form, and zero otherwise.
",5.2 DEV and TEST results,[0],[0]
We first consider results for the −CF setup.,5.2 DEV and TEST results,[1.0],['We first consider results for the −CF setup.']
"On both DEV and TEST, the accuracy achieved by each supervised model is higher than that of the unsupervised CForm approach, except for Siamese CBOW on TEST.",5.2 DEV and TEST results,[0],[0]
"The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively.",5.2 DEV and TEST results,[1.0],"['The word2vec model achieves the highest accuracy on DEV and TEST of 0.830 and 0.804, respectively.']"
"The difference between the word2vec model and the next-best model, skip-thoughts, is significant using a bootstrap test (Berg-Kirkpatrick et al., 2012) with 10k repetitions for DEV (p = 0.006), but not for TEST (p = 0.051).",5.2 DEV and TEST results,[0],[0]
"Nevertheless, it is remarkable that the relatively simple approach to averaging word embeddings used by word2vec performs as well as, or better than, the much more complex skipthoughts model used by Salton et al. (2016).8
8The word2vec and skip-thoughts models were trained on different corpora, which could contribute to the differences in results for these models.",5.2 DEV and TEST results,[0],[0]
"We therefore carried out an additional experiment in which we trained word2vec on BookCorpus, the corpus on which skip-thoughts was trained.",5.2 DEV and TEST results,[0],[0]
"This new word2vec model achieved accuracies of 0.825 and 0.809, on DEV and TEST, respectively, which are also higher accu-
Turning to the +CF setup, we observe that, for both DEV and TEST, each model achieves higher accuracy than in the −CF setup.9",5.2 DEV and TEST results,[0],[0]
All of these differences are significant using a bootstrap test (p < 0.002 in each case).,5.2 DEV and TEST results,[0],[0]
"In addition, each method outperforms the unsupervised CForm approach on both DEV and TEST.",5.2 DEV and TEST results,[0],[0]
"These findings demonstrate that the linguistically-motivated, lexico-syntactic knowledge encoded by the canonical form feature is complementary to the information from a wide range of types of distributed representations.",5.2 DEV and TEST results,[0],[0]
"In the +CF setup, the word2vec model again achieves the highest accuracy on both DEV and TEST of 0.854 and 0.852, respectively.10 The difference between the word2vec model and the next-best model, again skip-thoughts, is significant for both DEV and TEST using a bootstrap test (p < 0.05 in each case).
",5.2 DEV and TEST results,[0],[0]
"To better understand the impact of the canonical form feature when combined with the word2vec model, we compute the average precision, recall, and F1 score for each MWE for both the positive (idiomatic) and negative (literal) classes, for each run on TEST.11 For a given run, we then compute the average precision, recall, and F1 score across all MWEs, and then the average over all ten runs.",5.2 DEV and TEST results,[0],[0]
"We do this using CForm, and the word2vec model with and without the canonical form feature.",5.2 DEV and TEST results,[0],[0]
Results are shown in Table 3.,5.2 DEV and TEST results,[0],[0]
"In line with the findings of Fazly et al. (2009), CForm achieves higher precision and recall on idiomatic usages than literal ones.",5.2 DEV and TEST results,[0],[0]
"In particular, the relatively low recall for the literal class indicates that many literal usages occur in a canonical form.",5.2 DEV and TEST results,[0],[0]
"Comparing the word2vec model with and without the canonical form feature, we see that, when this feature is used, there is a relatively larger increase in precision and recall (and F1 score) for the literal class, than for the idiomatic class.",5.2 DEV and TEST results,[0],[0]
"This indicates that, although the
racies than those obtained by the skip-thoughts model.",5.2 DEV and TEST results,[0],[0]
"9In order to determine that this improvement is due to the information about canonical forms carried by the additional feature in the +CF setup, and not due to the increase in number of dimensions, we performed additional experiments in which we concatenated the embedding representations with a random binary feature, and with a randomly chosen value between 0 and 1.",5.2 DEV and TEST results,[0],[0]
"For each model, neither of these approaches outperformed that model using the +CF setup.
",5.2 DEV and TEST results,[0],[0]
"10In the +CF setup, the word2vec model using embeddings that were trained on the same corpus as skip-thoughts achieved accuracies of 0.846 and 0.851, on DEV and TEST, respectively.",5.2 DEV and TEST results,[0],[0]
"These are again higher accuracies than the corresponding setup for the skip-thoughts model.
",5.2 DEV and TEST results,[0],[0]
11We carried out the same analysis on DEV.,5.2 DEV and TEST results,[0],[0]
"The findings were similar.
canonical form feature itself performs relatively poorly on literal usages, it provides information that enables the word2vec model to better identify literal usages.",5.2 DEV and TEST results,[0],[0]
"Determining whether a usage of a VNC is idiomatic or literal is important for applications such as machine translation, where it is vital to preserve the meanings of word combinations.",6 Conclusions,[0],[0]
In this paper we proposed two approaches to the task of classifying VNC token instances as idiomatic or literal based on word2vec embeddings and Siamese CBOW.,6 Conclusions,[0],[0]
"We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (Salton et al., 2016).",6 Conclusions,[1.0],"['We compared these approaches against a linguistically-informed unsupervised baseline, and a model based on skip-thoughts previously applied to this task (Salton et al., 2016).']"
"Our experimental results show that a comparatively simple approach based on averaging word embeddings performs at least as well as, or better than, the approach based on skip-thoughts.",6 Conclusions,[0],[0]
"We further proposed methods to combine linguistic knowledge of the lexico-syntactic fixedness of VNCs — socalled “canonical forms”, which can be automatically acquired from corpora via statistical methods — with the embedding based approaches.",6 Conclusions,[1.0],"['We further proposed methods to combine linguistic knowledge of the lexico-syntactic fixedness of VNCs — socalled “canonical forms”, which can be automatically acquired from corpora via statistical methods — with the embedding based approaches.']"
"Our findings indicate that this rich linguistic knowledge is complementary to that available in distributed representations.
",6 Conclusions,[0],[0]
"Alternative approaches to embedding sentences containing VNC instances could also be considered, for example, FastSent (Hill et al., 2016).",6 Conclusions,[0],[0]
"However, all of the models we used represent the context of a VNC by the sentence in which it occurs.",6 Conclusions,[0],[0]
"In future work we therefore also intend to consider approaches such as context2vec (Melamud et al., 2016) which explicitly encode the context in which a token occurs.",6 Conclusions,[0],[0]
"Finally, one known challenge of VNC token classification is to develop models that are able to generalize to VNC types that were not seen during training (Gharbieh et al., 2016).",6 Conclusions,[0],[0]
"In future work we plan to explore
this experimental setup.",6 Conclusions,[0],[0]
"Verb–noun combinations (VNCs) — e.g., blow the whistle, hit the roof, and see stars — are a common type of English idiom that are ambiguous with literal usages.",abstractText,[0],[0]
"In this paper we propose and evaluate models for classifying VNC usages as idiomatic or literal, based on a variety of approaches to forming distributed representations.",abstractText,[0],[0]
"Our results show that a model based on averaging word embeddings performs on par with, or better than, a previously-proposed approach based on skip-thoughts.",abstractText,[0],[0]
Idiomatic usages of VNCs are known to exhibit lexico-syntactic fixedness.,abstractText,[0],[0]
"We further incorporate this information into our models, demonstrating that this rich linguistic knowledge is complementary to the information carried by distributed representations.",abstractText,[0],[0]
Leveraging distributed representations and lexico-syntactic fixedness for token-level prediction of the idiomaticity of English verb-noun combinations,title,[0],[0]
