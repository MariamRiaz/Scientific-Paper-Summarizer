0	29	Recent work on deep learning syntactic parsing models has achieved notably good results, e.g., Dyer et al. (2016) with 92.4 F1 on Penn Treebank constituency parsing and Vinyals et al. (2015) with 92.8 F1.
1	15	In this paper we borrow from the approaches of both of these works and present a neural-net parse reranker that achieves very good results, 93.8 F1, with a comparatively simple architecture.
5	144	Formally, a language model (LM) is a probability distribution over strings of a language: P (x) = P (x1, · · · , xn) = n∏ t=1 P (xt|x1, · · · , xt−1), (1) where x is a sentence and t indicates a word position.
6	18	The efforts in language modeling go into computing P (xt|x1, · · · , xt−1), which as described next is useful for parsing as well.
8	33	If we think of a tree (x,y) as a sequence (z) (Vinyals et 2331 al., 2015) as illustrated in Figure 1, we can define a probability distribution over (x,y) as follows: P (x,y) = P (z) = P (z1, · · · , zm) = m∏ t=1 P (zt|z1, · · · , zt−1), (2) which is equivalent to Equation (1).
19	21	Recurrent Neural Network Grammars (RNNG), a generative parsing model, defines a joint distribution over a tree in terms of actions the model takes to generate the tree (Dyer et al., 2016): P (x,y) = P (a) = m∏ t=1 P (at|a1, · · · , at−1), (3) where a is a sequence of actions whose output precisely matches the sequence of symbols in z, which implies Equation (3) is the same as Equation (2).
22	15	Due to lack of an algorithm that searches through an exponentially large phrase-structure space, we use an n-best parser to reduce Y(x) to Y ′(x), whose size is polynomial, and use LSTM-LM to find y that satisfies argmax y′∈Y ′(x) P (x,y′).
23	42	The model has three LSTM layers with 1,500 units and gets trained with truncated backpropagation through time with mini-batch size 20 and step size 50.
30	32	We use the Wall Street Journal (WSJ) of the Penn Treebank (Marcus et al., 1993) for training (2-21), development (24) and testing (23) and millions of auto-parsed “silver” trees (McClosky et al., 2006; Huang et al., 2010; Vinyals et al., 2015) for tritraining.
31	52	To obtain silver trees, we parse the entire section of the New York Times (NYT) of the fifth Gigaword (Parker et al., 2011) with a product of eight Berkeley parsers (Petrov, 2010)2 and ZPar (Zhu et al., 2013) and select 24 million trees on which both parsers agree (Li et al., 2014).
42	5	We address this problem in Section 5.3.
48	15	We compare LSTM-LM (GS) to two very strong semi-supervised NN parsers: an ensemble of five MTPs trained on 11 million trees of the highconfidence corpus4 (HC) (Vinyals et al., 2015); and an ensemble of six one-to-many sequence models trained on the HC and 4.5 millions of EnglishGerman translation sentence pairs (Luong et al., 2016).
49	8	We also compare LSTM-LM (GS) to best performing non-NN parsers in the literature.
50	45	Parsers’ parsing performance along with their training data is reported in Table 3.
52	32	Due to search errors – good trees are missing in 50-best trees – in Charniak (G), our supervised and semi-supervised models do not exhibit their full potentials when Charniak (G) provides Y ′(x).
54	29	As shown in Table 3, both LSTM-LM (G) and LSTM-LM (GS) are affected by the quality of Y ′(x).
55	25	A single LSTM-LM (GS) together with Charniak (GS) reaches 93.6 and an ensemble of eight LSTM-LMs (GS) with Charniak (GS) achieves a new state of the art, 93.8 F1.
56	46	When trees are converted to Stanford dependencies,5 UAS and LAS are 95.9% and 94.1%,6 more than 1% higher than those of the state of the art dependency parser (Andor et al., 2016).
57	57	Why an indirect method (converting trees to dependencies) is more accurate than a direct one (dependency parsing) remains unanswered (Kong and Smith, 2014).
58	15	The generative parsing model we presented in this paper is very powerful.
59	133	In fact, we see that a generative parsing model, LSTM-LM, is more effective than discriminative parsing models (Dyer et al., 2016).
60	74	We suspect building large models with character embeddings would lead to further improvement as in language modeling (Kim et al., 2016; Jozefowicz et al., 2016).
61	118	We also wish to develop a complete parsing model using the LSTMLM framework.
