41	1	While the theorem above concerns nonparametric estimation, and hence does not take into account how G is parametrised, it forms the basis for a consistency proof of CNCE.
69	1	From (12), we know that at the optimum of J (θ), G(u1,u2;θ) matches log pd(u1) − log pd(u2).
70	2	The values which the arguments u1 and u2 take during the minimisation are determined by the conditional noise distribution.
86	2	In order to investigate the potential benefit of the adaptive noise of CNCE, we used the following more challenging “ring model” where the data lie in lower dimensional manifold.
88	3	(17) The model is best understood in polar coordinates: the angular components are uniformly distributed and the radial direction is Gaussian with mean µr and precision γr.
91	1	As often done in NCE, a Gaussian noise is chosen to match the mean and covariance of the data distribution.
92	1	Because of the manifold structure of the data, the NCE noise is concentrated in areas where the data distribution takes small values, which is in contrast to the CNCE noise that well covers the data manifold.
93	1	Figures 2a and 2b show the estimation error as a function of the number of data points N .
94	1	For both the Gaussian and ICA models, the CNCE error decreases linearly in the loglog domain as the sample size increases, which indicates convergence in quadratic mean, and hence consistency.
95	3	Furthermore, as the number of noise-per-data points κ grows, the error appears to approach the MLE error.
103	2	Figure 3 shows the results for the ring model using κ = 10.
113	3	The sampled image patches were vectorised and both the ensemble mean and local mean (DC component) were subtracted.
114	2	The resulting data were then whitened and their dimensionality reduced to D = 600 by principal component analysis (Murphy, 2012, Chapter 12.2), retaining 98% of the variance.
120	1	In order to better learn about the non-Gaussian properties of natural images, we define the conditional noise distribution as log pc(u2|u1) = log p̃c(u2|u1)− 1 2 u2 · u2 + const, (19) where p̃c is the Gaussian noise distribution in (13).
131	1	The log nonlinearity counteracts the squaring, leading to an approximation of the max operation (Gutmann & Hyvärinen, 2013).
133	1	(24) The pooling weights q(4)j are restricted to be non-negative, which is enforced as for the second layer.
134	2	We here work with a simpler pooling model than in Equation (23).
137	1	Following (Gutmann & Hyvärinen, 2012; 2013) we used log φ̃(L)(u(1);θ) = K(L)∑ j=1 fth ( z (L) j + b (L) j ) (25) for L = 2, 3, 4 where fth is a smooth rectifying linear unit1 and b(L)j threshold parameters that are also learned from the data.
139	2	In the case L = 1, the outputs z(1)j were passed through the additional nonlinearity log((·)2 + 1) prior to thresholding.
140	2	This corresponds to computing the 2nd layer outputs with the 2nd layer weights fixed to correspond together to the identity matrix.
141	1	We learned the weights hierarchically one layer at a time, e.g. after learning of the 1st layer weights, we kept them fixed and learned the second layer weight vector w(2)j etc.
148	2	The polar plot is centred on the probing location, and the maximal radius is an indicator of the envelope and hence spatial frequency of the Gabor stimulus (larger circles correspond to lower spatial frequencies).
156	1	In this paper, we addressed the problem of density estimation for unnormalised models where the normalising partition function cannot be computed.
163	1	We provided theoretical and empirical arguments that CNCE provides a consistent estimator and proved that score matching emerges as a limiting case.
164	1	As score matching makes more stringent assumptions but does not rely on sampling, it is an open question whether we can use this result to e.g. devise a hybrid approach where parts of the model are automatically estimated with the more suitable method.
165	28	We further found that the relative performances of NCE and CNCE are model dependent, but that CNCE has an advantage in the important case where the data lie in a lower dimensional manifold.
166	10	An inherent limitation of empirical comparisons, and hence also those performed here, is that the results depend on the models and noise distributions used.
167	18	However, given the adaptive nature of CNCE, simple Gaussian conditional noise distributions are likely widely useful, as exemplified by our results on unsupervised deep learning of a neural image model.
168	135	The proposed method further allows one to iteratively adapt the conditional noise distribution to make the classification task successively more challenging, as it was done in some simulations for NCE (Gutmann & Hyvärinen, 2010), and generally for learning in generative latent variable models (Gutmann et al., 2014; Goodfellow et al., 2014).
169	133	This is an interesting direction of future work on CNCE.
