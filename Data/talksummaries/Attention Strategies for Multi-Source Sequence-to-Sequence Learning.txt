0	30	Sequence-to-sequence (S2S) learning with attention mechanism recently became the most successful paradigm with state-of-the-art results in machine translation (MT) (Bahdanau et al., 2014; Sennrich et al., 2016a), image captioning (Xu et al., 2015; Lu et al., 2016), text summarization (Rush et al., 2015) and other NLP tasks.
3	12	In this work, we focus on a special case of S2S learning with multiple input sequences of possibly different modalities and a single output-generating recurrent decoder.
5	8	The existing approaches to this problem do not explicitly model different importance of the inputs to the decoder (Firat et al., 2016; Zoph and Knight, 2016).
7	23	In automatic post-editing (APE), where a sentence in a source language and its automatically generated translation are on the input, we might want to attend to the source text only in case the model decides that there is an error in the translation.
8	12	We propose two interpretable attention strategies that take into account the roles of the individual source sequences explicitly—flat and hierarchical attention combination.
9	41	This paper is organized as follows: In Section 2, we review the attention mechanism in single-source S2S learning.
10	11	Section 3 introduces new attention combination strategies.
14	11	Inspired by content-based addressing in Neural Turing Machines (Graves et al., 2014), the attention mechanism estimates a probability distribution over the encoder hidden states in each decoding step.
22	6	Analogically to Equation 1, we compute a scalar energy term for the sentinel: eψi = v > a tanh ( Wasi + U (ψ) a (ψi si) ) (5) where Wa, U (ψ) a are the projection matrices, va is the weight vector, and ψi si is the sentinel vector.
29	48	A widely adopted technique for combining multiple attention models in a decoder is concatenation of the context vectors c(1)i , .
31	12	As mentioned in Section 1, this setting forces the model to attend to each encoder independently and lets the attention combination to be resolved implicitly in the subsequent network layers.
35	17	Flat attention combination projects the hidden states of all encoders into a shared space and then computes an arbitrary distribution over the projections.
36	14	The difference between the concatenation of the context vectors and the flat attention combination is that the αi coefficients are computed jointly for all encoders: α (k) ij = exp(e (k) ij ) ∑N n=1 ∑T (n)x m=1 exp ( e (n) im ) (6) where T (n)x is the length of the input sequence of the n-th encoder and e(k)ij is the attention energy of the j-th state of the k-th encoder in the i-th decoding step.
37	10	These attention energies are computed as in Equation 1.
41	11	The matrices U (k)c project the hidden states into a common vector space.
43	8	In our experiments, we explore both options.
47	7	We divide the computation of the attention distribution into two steps: First, we compute the context vector for each encoder independently using Equation 3.
48	60	Second, we project the context vectors (and optionally the sentinel) into a common space (Equation 8), we compute another distribution over the projected context vectors (Equation 9) and their corresponding weighted average (Equation 10): e (k) i = v > b tanh(Wbsi + U (k) b c (k) i ), (8) β (k) i = exp(e (k) i )∑N n=1 exp(e (n) i ) , (9) ci = N∑ k=1 β (k) i U (k) c c (k) i (10) where c(k)i is the context vector of the k-th encoder, additional trainable parameters vb and Wb are shared for all encoders, and U (k)b and U (k) c are encoder-specific projection matrices, that can be set equal and shared, similarly to the case of flat attention combination.
49	7	We evaluate the attention combination strategies presented in Section 3 on the tasks of multimodal translation (Section 4.1) and automatic post-editing (Section 4.2).
50	20	The models were implemented using the Neural Monkey sequence-to-sequence learning toolkit (Helcl and Libovický, 2017).12 In both setups, we process the textual input with bidirectional GRU network (Cho et al., 2014) with 300 units in the hidden state in each direction and 300 units in embeddings.
69	12	Automatic post-editing is a task of improving an automatically generated translation given the source sentence where the translation system is treated as a black box.
71	10	Each triplet contains an English source sentence, an automatically generated German translation of the source sentence, and a manually post-edited German sentence as a reference.
72	44	In case of this dataset, the MT outputs are almost perfect in and only little effort was required to post-edit the sentences.
75	22	By this technique, we prevent the model from paraphrasing the input sentences.
78	17	The models were able to slightly, but significantly improve over the baseline – leaving the MT output as is (HTER 24.8).
95	32	We introduced two new strategies of combining attention in a multi-source sequence-to-sequence setup.
96	22	Both methods are based on computing a joint distribution over hidden states of all encoders.
97	36	We conducted experiments with the proposed strategies on multimodal translation and automatic post-editing tasks, and we showed that the flat and hierarchical attention combination can be applied to these tasks with maintaining competitive score to previously used techniques.
98	12	Unlike the simple context vector concatenation, the introduced combination strategies can be used with the conditional GRU units in the decoder.
99	8	On top of that, the hierarchical combination strategy exhibits faster learning than than the other strategies.
