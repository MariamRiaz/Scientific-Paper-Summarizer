0	18	Programming robots is very difficult, in part because the real world is inherently rich and—to some degree— unpredictable.
3	48	Because of these complexities, relying on end-users to provide instructions to robots programmatically seems destined to fail.
5	18	Furthermore, real-world animal training is an existence proof that people can train complex behavior using these simple signals.
16	17	Then, to validate that COACH scales to complex problems, we train five different behaviors on a TurtleBot robot.
28	29	In this work, a human-centered reinforcement-learning (HCRL) problem is a learning problem in which an agent is situated in an environment described by an MDP but in which rewards are generated by a human trainer instead of from a stationary MDP reward function that the agent is meant to maximize.
35	25	In particular, a trainer must take care that the feedback they give contains no unanticipated exploits, constraining the feedback strategies they can use.
36	48	Indeed, prior research has shown that interpreting human feedback like a reward function often induces positive reward cycles that lead to unintended behaviors (Knox, 2012; Ho et al., 2015).
37	17	The issues with interpreting feedback as reward have led to the insight that human feedback is better interpreted as commentary on the agent’s behavior; for example, positive feedback roughly corresponds to “that was good” and negative feedback roughly corresponds to “that was bad.” In the next section, we review existing HCRL approaches that build on this insight.
67	18	Consequently, we were interested in investigating which model better fits human feedback.
71	28	To further make the case for human feedback being policy dependent, we provide a stronger result showing that trainers—for the same state– action pair—choose positive or negative feedback depending on their perception of the learner’s behavior.
86	22	For all three conditions, the dog’s behavior in the final episode was “alright,” regardless of any prior feedback.
88	45	In the first two episodes, users observed bad behavior in the improving condition (improving to alright); alright behavior in the steady condition; and good behavior in the degrading condition.
89	13	If feedback is policy-dependent, we would expect more positive feedback in the final episode for the improving condition, but not for policy-independent feedback since it was the same final behavior for all conditions.
98	33	COACH is based on the insight that the advantage function is a good model of human feedback and that actor–critic algorithms update a policy using the critic’s TD error, which is an unbiased estimate of the advantage function.
99	16	Consequently, an agent’s policy can be directly modified by human feedback without a critic component.
104	14	(1) Roughly speaking, the advantage function describes how much better or worse an action selection is compared to the agent’s performance under policy π.
105	17	The function is closely related to the update used in policy iteration (Puterman, 1994): defining π′(s) = argmaxaA π(s, a) is guaranteed to produce an improvement over π whenever π is suboptimal.
106	14	It can also be used in policy gradient methods to gradually improve the performance of a policy, as described later.
162	22	This type of feedback serves TAMER well.
165	50	Each combination of algorithm and feedback strategy was run 99 times with the median value of the number of steps needed to reach the goal reported.
168	13	The figure shows that TAMER can fail to learn in this setting.
174	32	Both TAMER and COACH perform well with this feedback strategy.
181	12	TAMER, on the other hand, performs very badly at first.
182	26	While the median score in the plot shows TAMER suddenly performing more comparably to COACH after about 10 episodes, 29% of our training trials completely failed to improve and timed-out across all 100 episodes.
203	50	The hide behavior has the TurtleBot back away from the ball when it is near and turn away from it when it is far.
210	63	For alternate, the agent was first trained to navigate to the ball when it sees it, and then turn away when it is near.
214	62	In cylinder navigation, they first trained the ball to be a lure, used it to guide the TurtleBot to the cylinder, and finally gave a +4 reward to reinforce the behaviors it took when following the ball (turning to face the cylinder, moving toward it, and stopping upon reaching it).
245	28	There are a number of exciting future directions to extend this work.
246	39	In particular, because COACH is built on the actor-critic paradigm, it should be possible to combine it straightforwardly with learning from demonstration and environmental rewards, allowing an agent to be trained in a variety of ways.
247	67	Second, because people give policydependent feedback, investigating how people model the current policy of the agent and how their model differs from the agent’s actual policy may produce even greater gains.
