{"sections": [{"heading": "1. Introduction", "text": "Many important problems in machine learning require learning functions in the presence of noise. For example, in reinforcement learning (RL), the transition dynamics of a system is often stochastic. Ideally, a model for these systems should be able to both express such randomness but also to account for the uncertainty in its parameters.\nBayesian neural networks (BNN) are probabilistic models that place the flexibility of neural networks in a Bayesian framework (Blundell et al., 2015; Gal, 2016). In particular, recent work has extended BNNs with latent input variables (BNN+LV) to estimate functions with complex stochasticity such as bimodality or heteroscedasticity (Depeweg et al., 2016). This model class can describe complex stochastic patterns via a distribution over the latent input variables (aleatoric uncertainty), while, at the same time, account for model uncertainty via a distribution over weights (epistemic uncertainty).\n1Siemens AG 2TU Munich 3University of Cambridge 4Harvard University. Correspondence to: Stefan Depeweg <stdepewe@gmail.com>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nIn this work we show how to perform and utilize a decomposition of uncertainty in aleatoric and epistemic components for decision making purposes. Our contributions are:\n\u2022 We derive two decompositions that extract epistemic and aleatoric uncertainties from the predictive distribution of BNN+LV (Section 3). \u2022 We demonstrate that with this uncertainty decomposition, the BNN+LV identifies informative regions even in bimodal and heteroscedastic cases, enabling efficient active learning in the presence of complex noise (Section 4). \u2022 We derive a novel risk-sensitive criterion for model-\nbased RL based on the uncertainty decomposition, enabling a domain expert to trade-off the risks of reliability, which originates from model bias and the risk induced by stochasticity (Section 5).\nWhile using uncertainties over transition probabilities to avoid worst-case behavior has been well-studied in discrete MDPs (e.g. (Shapiro & Kleywegt, 2002; Nilim & El Ghaoui, 2005; Bagnell et al., 2001), to our knowledge, our work is the first to consider continuous non-linear functions with complex noise."}, {"heading": "2. Background: BNN+LV", "text": "We review a recent family of probabilistic models for multioutput regression. These models were previously introduced by Depeweg et al. (2016), we refer to them as Bayesian Neural Networks with latent variables (BNN+LV).\nGiven a dataset D = {xn,yn}Nn=1, formed by feature vectors xn \u2208 RD and targets yn \u2208 RK , we assume that yn = f(xn, zn;W) + n, where f(\u00b7, \u00b7;W) is the output of a neural network with weights W and K output units. The network receives as input the feature vector xn and the latent variable zn \u223c N (0, \u03b3). We choose rectifiers, \u03d5(x) = max(x, 0), as activation functions for the hidden layers and the identity function, \u03d5(x) = x, for the output layer. The network output is corrupted by the additive noise variable n \u223c N (0,\u03a3) with diagonal covariance matrix \u03a3. The role of the latent variable zn is to capture unobserved stochastic features that can affect the network\u2019s output in complex ways. Without zn, randomness\nar X\niv :1\n71 0.\n07 28\n3v 4\n[ st\nat .M\nL ]\n1 5\nJu n\n20 18\nwould only be given by the additive Gaussian observation noise n, which can only describe limited stochastic patterns. The network has L layers, with Vl hidden units in layer l, and W = {Wl}Ll=1 is the collection of Vl \u00d7 (Vl\u22121 + 1) weight matrices. The +1 is introduced here to account for the additional per-layer biases. We approximate the exact posterior p(W, z | D) with:\nq(W, z) =\n  L\u220f\nl=1\nVl\u220f\ni=1\nVl\u22121+1\u220f\nj=1\nN (wij,l|mwij,l, vwij,l)\n \n\ufe38 \ufe37\ufe37 \ufe38 q(W)\n\u00d7\n[ N\u220f\nn=1\nN (zn |mzn, vzn)\n]\n\ufe38 \ufe37\ufe37 \ufe38 q(z)\n. (1)\nThe parameters mwij,l, v w ij,l and m z n, v z n are determined by minimizing a divergence between p(W, z | D) and the approximation q. The reader is referred to the work of Herna\u0301ndez-Lobato et al. (2016); Depeweg et al. (2016) for more details on this. In our experiments, we tune q using black-box \u03b1-divergence minimization with \u03b1 = 1.0. While other values of \u03b1 are possible, this specific value produced better uncertainty decompositions in practice: see Section 4 and the supplementary material for results with \u03b1 = 0.5 and \u03b1 = 0 (variational Bayes).\nBNN+LV can capture complex stochastic patterns, while at the same time account for model uncertainty. They achieve this by jointly learning q(z), which describes the values of the latent variables that were used to generate the training data, and q(W), which represents uncertainty about model parameters. The result is a flexible Bayesian approach for learning conditional distributions with complex stochasticity, e.g. bimodal or heteroscedastic noise."}, {"heading": "3. Uncertainty Decomposition in BNN+LV", "text": "Let us assume that the targets are one-dimensional, that is, K = 1. The predictive distribution of a BNN+LV for the target variable y? associated with the test data point x? is\np(y?|x?) = \u222b p(y?|W,x?, z?)p(z?)q(W) dz? dW . (2)\nwhere p(y?|W,x?, z?) = N (y?|f(x?, z?;W),\u03a3) is the likelihood function, p(z?) = N (z?|0, \u03b3) is the prior on the latent variables and q(W) is the approximate posterior for W given D. In this expression q(z) is not used since the integration with respect to z? must be done using the prior p(z?). The reason for this is that the y? associated with x? is unknown and consequently, there is no other evidence on z? than the one coming from p(z?).\nIn Eq. (2), the randomness or uncertainty on y? has its origin inW \u223c q(W), z? \u223c p(z?) and \u223c N (0, \u03c32). This means\nthat there are two types of uncertainties entangled in our predictons for y?: aleatoric and epistemic (Der Kiureghian & Ditlevsen, 2009; Kendall & Gal, 2017). The aleatoric uncertainty originates from the randomness of z? and and cannot be reduced by collecting more data. By contrast, the epistemic uncertainty originates from the randomness of W and can be reduced by collecting more data, which will typically shrink the approximate posterior q(W).\nEq. (2) is the tool to use when making predictions for y?. However, there are many settings in which, for decision making purposes, we may be interested in separating the two forms of uncertainty present in this distribution. We now describe two decompositions , each one differing in the metric used to quantify uncertainty: the first one is based on the entropy, whereas the second one uses the variance.\nLet H(\u00b7) compute the differential entropy of a probability distribution. The total uncertainty present in Eq. (2) can then be quantified as H(y?|x?). Furthermore, assume that we do not integrateW out in Eq. (2) and, instead, we just condition on a specific value of this variable. The result is then p(y?|W,x?) = \u222b p(y?|W,x?, z?)p(z?) dz? with corresponding uncertainty H(y?|W,x?). The expectation of this quantity under q(W), that is, Eq(W)[H(y?|W,x?)], can then be used to quantify the overall uncertainty in Eq. (2) coming from z? and . Therefore, Eq(W)[H(y?|W,x?)], measures the aleatoric uncertainty. We can then quantify the epistemic part of the uncertainty in Eq. (2) by computing the difference between total and aleatoric uncertainties:\nH[y?|x?]\u2212Eq(W)[H(y?|W,x?)] = I(y?,W) , (3)\nwhich is the mutual information between y? andW .\nInstead of the entropy, we can use the variance as a measure of uncertainty. Let \u03c32(\u00b7) compute the variance of a probability distribution. The total uncertainty present in Eq. (2) is then \u03c32(y?|x?). This quantity can then be decomposed using the law of total variance:\n\u03c32(y?|x?) = \u03c32q(W)(E[y?|W,x?]) +Eq(W)[\u03c32(y?|W,x?)] .\nwhere E[y?|W,x?] and \u03c32[y?|W,x?] are, respectively, the mean and variance of y? according to p(y?|W,x?). In the expression above, \u03c32q(W)(E[y?|W,x?]) is the variance of E[y?|W,x?] when W \u223c q(W). This term ignores any contribution to the variance of y? from z? and and only considers the effect ofW . Therefore, it corresponds to the epistemic uncertainty in Eq. (2). By contrast, the term Eq(W)[\u03c32(y?|W,x?)] represents the average value of \u03c32(y?|W,x?) when W \u223c q(W). This term ignores any contribution to the variance of y? fromW and, therefore, it represents the aleatoric uncertainty in Eq. (2).\nIn some cases, working with variances can be undesirable because they have square units. To avoid this problem, we\ncan work with the square root of the previous terms. For example, we can represent the total uncertainty using \u03c3(y?|x?) ={\n\u03c32q(W)(E[y?|W,x?]) +Eq(W)[\u03c32(y?|W,x?)] } 1 2 . (4)"}, {"heading": "4. Active Learning with Complex Noise", "text": "Active learning is the problem of iteratively collecting data so that the final gains in predictive performance are as high as possible (Settles, 2012). We consider the case of actively learning arbitrary non-linear functions with complex noise. To do so, we apply the information-theoretic framework for active learning described by MacKay (1992), which is based on the reduction of entropy in the model\u2019s posterior distribution. Below, we show that this framework naturally results in the entropy-based uncertainty decomposition from Section 3. Next, we demonstrate how this framework, applied to BNN+LV enables data-efficient learning in the presence of heteroscedastic and bimodal noise.\nAssume a BNN+LV is used to describe a batch of training data D = {(x1,y1), \u00b7 \u00b7 \u00b7 , (xN ,yN )}. The expected reduction in posterior entropy forW that would be obtained when collecting the unknown target y? for the input x? is\nH(W|D)\u2212Ey?|x?,D [H(W|D \u222a {x?,y?})] = I(W,y?) = H(y?|x?)\u2212Eq(W) [H(y?|W,x?)] . (5)\nNote that this is the epistemic uncertainty that we introduced in Section 3, which has arisen naturally in this setting: the most informative x? for which to collect y? next is the one for which the epistemic uncertainty in the BNN+LV predictive distribution is the highest.\nThe epistemic uncertainty in Eq. (5) can be approximated using standard entropy estimators, e.g. nearest-neighbor methods (Kozachenko & Leonenko, 1987; Kraskov et al., 2004; Gao et al., 2016). For that, we repeatedly sample W and z? and do forward passes through the BNN+LV to sample y?. The resulting samples of y? can then be used to approximate the respective entropies for each x? using the nearest-neighbor approach:\nH(y?|x?)\u2212Eq(W) [H(y?|W,x?)]\n\u2248 H\u0302(y1?, . . . ,yL? )\u2212 1\nM M\u2211 i=1 [ H\u0302(y1,Wi? , . . . ,y L,Wi ? ) ] . (6)\nwhere H\u0302(\u00b7) is a nearest-neighbor entropy estimate given an empirical sample of points, y1?, . . . ,y L ? are sampled from p(y?|x?) according to Eq. (2),W1, . . . ,WM \u223c q(W) and y1,Wi? , . . . ,y L,Wi ? \u223c p(y?|Wi,x?) for i = 1, . . . ,M .\nThere are alternative ways to estimate the entropy, e.g. with histograms or using kernel density estimation (KDE) (Beirlant et al., 1997). We choose nearest neighbor methods\nbecause they tend to work well in low dimensions, are fast to compute (compared to KDE) and do not require much hyperparameter tuning (compared to histograms). However, we note that for high-dimensional problems, estimating entropy is a difficult problem."}, {"heading": "4.1. Experiments", "text": "We evaluate the active learning procedure on three problems. In each of them, we first train a BNN+LV with 2 hidden layers and 20 units per layer. Afterwards, we approximate the epistemic uncertainty as outlined in the previous section. Hyper-parameter settings and details for replication can be found in the supplementary material, which includes results for three other inference methods: Hamiltonian Monte Carlo (HMC), black-box \u03b1- divergence minimization with \u03b1 = 0.5 and \u03b1 = 0 (variational Bayes). These results show that the decomposition of uncertainty produced by \u03b1 = 1 and the gold standard HMC are similar, but for lower values of \u03b1 this is not the case. Our main findings are:\nThe decomposition of uncertainty allows us to identify informative inputs when the noise is heteroscedastic. We consider a regression problem with heteroscedastic noise where y = 7 sin(x) + 3| cos(x/2)| with \u223c N (0, 1). We sample 750 values of the input x from a mixture of three Gaussians with mean parameters {\u00b51 = \u22124, \u00b52 = 0, \u00b53 = 4}, variance parameters {\u03c31 = 25 , \u03c32 = 0.9, \u03c33 = 2 5} and with each Gaussian component having weight equal to 1/3 in the mixture. Figure 1a shows the data. We have many points at the borders and in the center, but few in between.\nFigure 1 shows the results obtained (see caption for details). The resulting decomposition of predictive uncertainty is very accurate: the epistemic uncertainty (Figure 1f), is inversely proportional to the density used to sample the data (Figure 1b). This makes sense, since in this toy problem the most informative inputs are located in regions where data is scarce. However, this may not be the case in more complicated settings. Finally, we note that the total predictive uncertainty (Figure 1d) fails to identify informative regions.\nThe decomposition of uncertainty allows us to identify informative inputs when the noise is bimodal. Next we consider a toy problem given by a regression task with bimodal data. We define x \u2208 [\u22120.5, 2] and y = 10 sin(x) + with probability 0.5 and y = 10 cos(x) + , otherwise, where \u223c N (0, 1) and is independent of x. We sample 750 values of x from an exponential distribution with \u03bb = 2. Figure 2a shows the data. We have many points on the left, but few on the right.\nFigure 2 shows the results obtained (see caption for details). Figure 2c shows that the BNN+LV has learned the bimodal structure in the data and Figure 2d shows how the total predictive uncertainty increases on the right, where data is scarce. The aleatoric uncertainty (Figure 2e), by contrast,\nhas an almost symmetric form around x = 0.75, taking lower values at this location. This makes sense since the data generating process is symmetric around x = 0.75 and the noise changes from bimodal to unimodal when one gets closer to x = 0.75. Figure 2f shows an estimate of the epistemic uncertainty, which as expected increases with x.\nThe decomposition of uncertainty identifies informative inputs when noise is both heteroscedastic and bimodal. We consider data sampled from a 2D stochastic system called the wet-chicken (Hans & Udluft, 2009; Depeweg et al., 2016), see the supplementary material for details. The wet-chicken transition dynamics exhibit complex stochastic patterns: bimodality, heteroscedasticity and truncation (the agent cannot move beyond the boundaries of state space: [0, 5]2). The data are 7, 500 state transitions collected by random exploration. Figure 3a shows the states visited. For each transition, the BNN+LV predicts the next state given the current one and the action applied. Figure 3d shows that the epistemic uncertainty is highest in the top right corner,\nwhile data is most scarce in the bottom right corner. The reason for this result is that the wet-chicken dynamics bring the agent back to y = 0 whenever the agent goes beyond y = 5, but this does not happen for y = 0 where the agent just bounces back. Therefore, learning the dynamics is more difficult and requires more data at y = 5 than at y = 0. The epistemic uncertainty captures this property, but the total predictive uncertainty (Figure 3b) does not.\nActive learning with BBN+LV is improved by using the uncertainty decomposition. We evaluate the gains obtained by using Eq. (5) with BNN+LV, when collecting\ndata in three toy active learning problems. We refer to this method as Ibb-\u03b1(W, y?) and compare it with two baselines. The first one also uses a BNN+LV to describe data, but does not perform a decomposition of predictive uncertainty, that is, this method uses H(y?|x?) instead of Eq. (5) for active learning. We call this method Hbb-\u03b1(y?|x?). The second baseline is given by a Gaussian process (GP) model which collects data according to H(y?|x?) since in this case the uncertainty decomposition is not necessary because the GP model does not include latent variables. The GP model assumes Gaussian noise and is not able to capture complex stochastic patterns.\nThe three problems considered correspond to the datasets from Figures 1, 2 and 3. The general set-up is as follows. We start with the available data shown in the previous figures. At each iteration, we select a batch of data points to label from a pool set which is sampled uniformly at random in input space. The selected data is then included in the training set and the log-likelihood is evaluated on a separate test set. This process is performed for 150 iterations and we repeat all experiments 5 times.\nTable 1 shows the results obtained. Overall, BNN+LV outperform GPs in terms of predictive performance. We also see significant gains of Ibb-\u03b1(W, y?) over H(y?|x?) on the heteroscedastic and wet-chicken tasks, whereas their results are similar on the bimodal task. The reason for this is that, in the latter task, the epistemic and the total uncertainty have a similar behaviour as shown in Figures 2d and 2f. Finally, we note that heteroscedastic GPs (Le et al., 2005) will likely perform similar to BNN+LV in the heteroscedastic task from Figure 2, but they will fail in the other two settings considered (Figures 2 and 3)."}, {"heading": "5. Risk-sensitive Reinforcement Learning", "text": "We propose an extension of the \u201crisk-sensitive criterion\u201d for safe model-based RL (Garc\u0131\u0301a & Ferna\u0301ndez, 2015) to balance the risks produced by epistemic and aleatoric uncertainties.\nWe focus on batch RL with continuous state and action spaces (Lange et al., 2012): we are given a batch of state transitions D = {(st,at, st+1)} formed by triples contain-\ning the current state st, the action applied at and the next state st+1. In addition toD, we are also given a cost function c. The goal is to obtain from D a policy in parametric form that minimizes c on average under the system dynamics.\nIn model-based RL, the first step consists in learning a dynamics model fromD. We assume that the true dynamical system can be expressed by an unknown neural network with latent variables:\nst = ftrue(st\u22121,at\u22121, zt;Wtrue) , zt \u223c N (0, \u03b3) , (7)\nwhereWtrue denotes the weights of the network and st\u22121, at\u22121 and zt are the inputs to the network. We use BNN+LV from Section 2 to approximate a posterior q(W, z) using the batch D (Depeweg et al., 2016).\nIn model-based policy search we optimize a policy given by a deterministic neural network with weightsW\u03c0. This parametric policy returns an action at as a function of st, that is, at = \u03c0(st;W\u03c0). The policy parameters W\u03c0 can be tuned by minimizing the expectation of the cost C =\u2211T t=1 ct over a finite horizon T with respect to the belief q(W), where ct = c(st). This expected cost is obtained by averaging across multiple virtual roll-outs as described next.\nGiven s0, we sampleW \u223c q and simulate state trajectories for T steps using the model st+1 = f(st,at, zt;W) + t+1 with policy at = \u03c0(st;W\u03c0), input noise zt \u223c N (0, \u03b3) and additive noise t+1 \u223c N (0,\u03a3). By averaging across these roll-outs, we obtain a Monte Carlo approximation of the expected cost given the initial state s0:\nJ(W\u03c0) = E [C] = E [\u2211T t=1 ct ] , (8)\nwhere, E[\u00b7] denotes here an average across virtual roll-outs starting from s0 and, to simplify our notation, we have made the dependence on s0 implicit. The policy search algorithm optimizes the expectation of Eq. (8) when s0 is sampled uniformly from D, that is, Es0\u223cD[J(W\u03c0)]. This quantity can be easily approximated by Monte Carlo and if model, policy, and cost function are differentiable, we are able to tuneW\u03c0 by stochastic gradient descent.\nThe \u201crisk-sensitive criterion\u201d (Garc\u0131\u0301a & Ferna\u0301ndez, 2015)\nchanges Eq. (8) to atain a balance between expected cost and risk, where the risk typically penalizes the deviations of C from E[C] during the virtual roll-outs with initial state s0. For example, the risk-sensitive objective could be\nJ(W\u03c0) = E [C] + \u03b2\u03c3(C) , (9)\nwhere \u03c3(C) is the standard deviation of C across virtual roll-outs starting from s0 and the risk-sensitive parameter \u03b2 determines the amount of risk-avoidance (\u03b2 \u2265 0) or riskseeking behavior (\u03b2 < 0) when optimizingW\u03c0 .\nInstead of working directly with the risk on the final cost C, we consider the sum of risks on the individual costs c1, . . . , cT . The reason for this is that the latter is a more restrictive criterion since low risk on the ct will imply low risk on C, but not the other way around. Let \u03c3(ct) denote the standard deviation of ct over virtual roll-outs starting from s0. We can then explicitly write \u03c3(ct) in terms of its aleatoric and epistemic components by using the decomposition of uncertainty from Eq. (4). In particular,\n\u03c3(ct) = { \u03c32q(W)(E[ct|W]) +Eq(W)[\u03c32(ct|W)] } 1 2 , (10)\nwhere E[ct|W] and \u03c32(ct|W) denote the mean and variance of ct under virtual roll-outs from s0 performed with policy W\u03c0 and under the dynamics of a BNN+LV with parameters W . In a similar manner as in Eq. (4), the operators Eq(W)[\u00b7] and \u03c32q(W)(\u00b7) in Eq. (10) compute the mean and variance of their arguments whenW \u223c q(W).\nThe two terms inside the square root in Eq. (10) have a clear interpretation. The first one, Eq(W)[\u03c32(ct|W)], represents the risk originating from the sampling of z and in the virtual roll-outs. We call this term the aleatoric risk. The second term, \u03c32q(W)(E[ct|W]), encodes the risk originating from the sampling ofW in the virtual roll-outs. We call this term the epistemic risk.\nWe can now extend the objective in Eq. (8) with a new risk term that balances the epistemic and aleatoric risks. This term is obtained by first using risk-sensitive parameters \u03b2 and \u03b3 to balance the epistemic and aleatoric components in Eq. (10), and then summing the resulting expression for t = 1, . . . , T :\n\u03c3(\u03b3, \u03b2) = T\u2211 t=1 { \u03b22\u03c32q(W)(E[ct|W]) + \u03b32Eq(W)[\u03c32(ct|W)] } 1 2 .\nTherefore, our \u2018risk-sensitive criterion\u201d uses the function J(W\u03c0) = E [C] + \u03c3(\u03b3, \u03b2), which can be approximated via Monte Carlo and optimized using stochastic gradient descent. The Monte Carlo approximation is generated by performing M \u00d7N roll-outs with starting state s0 sampled uniformly fromD. For this,W is sampled from q(W) a total of M times and then, for each of these samples, N roll-outs are performed withW fixed and sampling only the latent\nvariables and the additive Gaussian noise in the BBN+LV. Let zm,nt and m,n t be the samples of the latent variables and the additive Gaussian noise at step t during the n-th roll-out for them-th sample ofW , which we denote byWm. Then cm,n(t) = c(s Wm,{zm,n1 ,...,z m,n t },{ m,n 1 ,..., m,n t },W\u03c0\nt ) denotes the cost obtained at time t in that roll-out. All these cost values obtained at time t are stored in the M \u00d7N matrix C(t). The Monte Carlo estimate of J(W\u03c0) is then\nJ(W\u03c0) \u2248 T\u2211 t=1 { 1TC(t)1 MN +\n{ \u03b22\u03c3\u03022 [C(t)1/N ] + \u03b32 1\nM M\u2211 m=1 \u03c3\u03022 [C(t)m,\u00b7]\n} 1 2 }\n, (11)\nwhere 1 denotes a vector with all of its entries equal to 1, C(t)m,\u00b7 is a vector with the m-th row of C(t) and \u03c3\u03022[x] returns the empirical variance of the entries in vector x.\nBy setting \u03b2 and \u03b3 to specific values in Eq. (11), the user can choose different trade-offs between cost, aleatoric and epistemic risk: for \u03b3 = 0 the term inside the square root is \u03b22 times \u03c3\u03022 [C(t)1/N ] which is a Monte Carlo approximation of the epistemic risk in Eq. (10). Similarly, for \u03b2 = 0, inside the square root we obtain \u03b32 times 1M \u2211M m=1 \u03c3\u0302\n2 [C(t)m,\u00b7] which approximates the aleatoric risk. For \u03b2 = \u03b3 the standard risk criterion \u03c3(ct) is obtained, weighted by \u03b2."}, {"heading": "5.1. Model-bias and Noise aversion", "text": "The epistemic risk term in Eq. (10) can be connected with the concept of model-bias in model-based RL. A policy with W\u03c0 is optimized on the model but executed on the ground truth system. The more model and ground truth differ, the more the policy is \u2019biased\u2019 by the model (Deisenroth & Rasmussen, 2011). Given the initial state s0, we can quantify this bias with respect to the policy parametersW\u03c0 as\nb(W\u03c0) = T\u2211 t=1 (Etrue[ct]\u2212E[ct])2 , (12)\nwhere Etrue[ct] is the expected cost obtained at time t across roll-outs starting at s0, under the ground truth dynamics and with policy \u03c0(st;W\u03c0). E[ct] is the same expectation but under BNN+LV dynamics sampled from q(W) on each individual roll-out.\nEq. (12) is impossible to compute in practice because we do not know the ground truth dynamics. However, as indicated in Eq. (7), we assume that the true dynamic is given by a neural network with latent variables and weightsWtrue. We can then rewrite Etrue[ct] as E[ct|Wtrue] and since we do not knowWtrue, we can further assume thatWtrue \u223c q(W). The expected model-bias is then\nE[b(W\u03c0)] = Eq(Wtrue) { T\u2211 t=1 (E[ct|Wtrue]\u2212E[ct])2 }\n= T\u2211 t=1 \u03c32q(Wtrue)(E[ct|Wtrue]) . (13)\nWe see that our definition of epistemic risk also represents an estimate of model-bias in model-based RL. This risk term will guide the policy to operate in areas of state space where model-bias is expected to be low.\nThe aleatoric risk term in Eq. (10) can be connected with the concept of noise aversion. Let \u03c32(ct|Wtrue) be the variance obtained at time t across roll-outs starting at s0, under the ground truth dynamics and with policy \u03c0(st;W\u03c0). Assuming Wtrue \u223c q(W), the expected variance is then Eq(Wtrue)[\u03c3\n2(ct|Wtrue)]. This term will guide the policy to operate in areas of state space where the stochasticity of the cost is low. Assuming a deterministic cost function this stochasticity is determined by the model\u2019s predictions that originate from zt and t."}, {"heading": "5.2. Experiments", "text": "We investigate the following questions: To what extent does our new risk criterion reduce model-bias? What trade-offs do we observe between average cost and model-bias? How does the decomposition compare to other simple methods?\nWe consider two model-based RL scenarios. The first one is given by the industrial benchmark (IB), a publicly available simulator with properties inspired by real industrial systems (Hein et al., 2017). The second RL scenario is a modified version of the HAWC2 wind turbine simulator (Larsen & Hansen, 2007), which is widely used for the study of wind turbine dynamics (Larsen et al., 2015).\nWe are given a batch of data formed by state transitions generated by a behavior policy \u03c0b, for example, from an already running system. The behavioral policy has limited randomness and will keep the system dynamics constrained to a reduced manifold in state space. This means that large portions of state space will be unexplored and uncertainty will be high in those regions. The supplementary material contains full details on the experimental setup and the \u03c0b.\nWe consider the risk-sensitive criterion from Section 5 for different choices of \u03b2 and \u03b3, comparing it with 3 baselines. The first baseline is obtained by setting \u03b2 = \u03b3 = 0. In this case, the policy optimization ignores any risk. The second baseline is obtained when \u03b2 = \u03b3. In this case, the risk criterion simplifies to \u03b2\u03c3(ct), which corresponds to the traditional risk-sensitive approach in Eq. (9), but applied to the individual costs c1, . . . , cT . The last baseline uses a deterministic neural network to model the dynamics and a nearest neighbor approach to quantify risk: for each state st generated in a roll-out, we calculate the Euclidean distance of that state to the nearest one in the training data. The average value of the distance metric for st across roll-outs is then an approximation to \u03c3(ct). To reduce computational\ncost, we summarize the training data using the centroids returned by an execution of the k-means clustering method. We denote this method as the nn-baseline.\nFigure 4a shows result on the industrial benchmark. The y-axis in the plot is the average total cost at horizon T obtained by the policy in the ground truth system. The xaxis is the average model-bias in the ground truth system according to Eq. (12). Each individual curve in the plot is obtained by fixing \u03b3 to a specific value (line colour) and then changing \u03b2 (circle colour). The policy that ignores risk (\u03b2 = \u03b3 = 0) results in both high model-bias and high cost when evaluated on the ground truth, which indicates overfitting. As \u03b2 increases, the policies put more emphasis on avoiding model-bias, but at the same time the average cost increases. The best tradeoff is obtained by the dark red curve with \u03b3 = 0. The risk criterion is then \u03b2 \u2211T t=1 \u03c3q(W)(E[ct|W]). In this problem, adding aleatoric risk by setting \u03b3 > 0 decreases performance. The nn-baseline shows a similar pattern as the BNN+LV approach, but the trade-off between model-bias and cost is worse.\nFigure 5 shows roll-outs for three different policies and a fixed initial state s0. Figure 5a shows results for a policy learned with \u03b3 = \u03b2 = 0. This policy ignores risk, and as a consequence, the mismatch between predicted performance on the model and on the ground truth increases after t = 20. This result illustrates how model-bias can lead to policies with high costs at test time. Figure 5b shows results for policy that was trained while penalizing epistemic risk (\u03b2 = 4, \u03b3 = 0). In this case, the average costs under the BNN+LV model and the ground truth are similar, and the overall ground truth cost is lower than in Figure 5a. Finally, Figure 5c shows results for a noise averse policy (\u03b2 = 0, \u03b3 = 4). In this case, the model bias is slightly higher than in the previous figure, but the stochasticity is lower.\nThe results for wind turbine simulator can be found in Figure 4b. In this case, the best trade-offs between expected cost and model-bias are obtained by the policies with \u03b3 = 7.5. These policies are noise averse and will try to avoid noisy regions in state space. This makes sense because in wind turbines, high noise regions in state space are those where the effect of wind turbulence will have a strong impact on the average cost."}, {"heading": "6. Related Work", "text": "The distinction between aleatoric and epistemic uncertainty has been recognized in many fields within machine learning, often within the context of specific subfields, models, and objectives. Kendall & Gal (2017) consider a decomposition of uncertainty in the context of computer vision with heteroscedastic Gaussian output noise, while McAllister (2016) consider a decomposition in GPs for model-based RL.\nWithin reinforcement learning, Bayesian notions of model uncertainty have a long history (Schmidhuber, 1991b;a; Dearden et al., 1999; Still & Precup, 2012; Sun et al., 2011; Maddison et al., 2017). The mentioned works typically consider the online case, where model uncertainty (a.k.a. curiosity) is used to guide exploration, e.g. in Houthooft et al. (2016) the uncertainty of a BNN model is used to guide exploration assuming deterministic dynamics. In contrast, we focus on the batch setting with stochastic dynamics.\nModel uncertainty is used in safe or risk-sensitive RL (Mihatsch & Neuneier, 2002; Garc\u0131\u0301a & Ferna\u0301ndez, 2015). In safe RL numerous other approaches exists for safe exploration (Joseph et al., 2013; Hans et al., 2008; Garcia & Ferna\u0301ndez, 2012; Berkenkamp et al., 2017). Uncertainties over transition probabilities have been studied in discrete MDPs since a long time (Shapiro & Kleywegt, 2002; Nilim & El Ghaoui, 2005; Bagnell et al., 2001) often with a focus on worst-case avoidance. Our work extends this to continuous state and action space using scalable probabilistic models. Our decomposition enables a practitioner to adjust the optimization criterion to specific decision making,.\nWithin active learning many approaches exist that follow an information theoretic approach (MacKay, 1992; Herna\u0301ndezLobato & Adams, 2015; Guo & Greiner, 2007). To our knowledge, all of these approaches however use deterministic methods (mostly GPs) as model class. Perhaps closest to our work is BALD (Houlsby et al., 2012), however because GPs are used, this approach cannot model problems with complex noise."}, {"heading": "7. Conclusion", "text": "We have described a decomposition of predictive uncertainty into its epistemic and aleatoric components when working with Bayesian neural networks with latent variables. We have shown how this decomposition of uncertainty can be used for active learning, where it naturally arises from an information-theoretic perspective. We have also used the decomposition to propose a novel risk- sensitive criterion for model-based reinforcement learning which decomposes risk into model-bias and noise aversion components. Our experiments illustrate how the described decomposition of uncertainty is useful for efficient and risk-sensitive learning."}, {"heading": "A. Hamiltonian Monte Carlo Solution to Toy Problems", "text": "The goal here is to show that the decomposition we obtained in the active learning examples is not a result of our approximation using black-box \u03b1-divergence minimization but a property of BNN+LV themselves. To that end we will approximate using HMC. After a burn-in of 500, 000 samples we sample from the posterior 200, 000 samples. We thin out 90% by only keeping every tenth sample."}, {"heading": "A.1. Heteroscedastic Problem", "text": "We define the stochastic function y = 7 sin(x) + 3| cos(x/2)| with \u223c N (0, 1). The data availability is limited to specific regions of x. In particular, we sample 750 values of x from a mixture of three Gaussians with mean parameters {\u00b51 = \u22124, \u00b52 = 0, \u00b53 = 4}, variance parameters {\u03c31 = 25 , \u03c32 = 0.9, \u03c33 = 2 5} and with each Gaussian component having weight equal to 1/3 in the mixture. Figure 6a shows the raw data. We have lots of points at both borders of the x axis and in the center, but little data available in between."}, {"heading": "A.2. Bimodal Problem", "text": "We consider a toy problem given by a regression task with bimodal data. We define x \u2208 [\u22120.5, 2] and y = 10 sin(x) + with probability 0.5 and y = 10 cos(x) + , otherwise, where \u223c N (0, 1) and is independent of x. The data availability is not uniform in x. In particular we sample 750 values of x from an exponential distribution with \u03bb = 2"}, {"heading": "B. Solutions to Toy Problems for different values of \u03b1", "text": "In the main document we pointed out that the decomposition of uncertainty does not work as good with other values of \u03b1. We will see in the following, that lower values of \u03b1 will put more and more emphasis on the latent variable z. We observe that the epistemic uncertainty will vanish as the \u03b1-divergence minimization approaches variational Bayes.\nB.1. \u03b1 = 0.5\nB.2. V B solutions to Toy Problems\nWe approximate the method variational Bayes by setting \u03b1 to 10\u22126."}, {"heading": "C. Experiments Specification", "text": ""}, {"heading": "C.1. Active Learning", "text": "All models start with the available described in the respective paragraphs. We train for 5000 epochs a BNN+LV with two-hidden layer and 20 hidden units per layer. We use Adam as optimizer with a learning rate of 0.001. For Gaussian\nprocesses(GPs) we use the standard RBF kernel using the python GPy implementation. For the entropy estimation we use a nearest-neighbor approach as explained in the main document with k = 25 and 500 samples of q(W ) and 500 samples of p(z).\nFor active learning we evaluate performance using a held-out test set of size 500 for the bimodal and heteroscedastic problem and 2500 for the wet-chicken problem. The test data is sampled uniformly in state (and action) space. In each of the n = 150 iterations we sample a pool set of size 50 uniformly in input space. In each iteration a method can decide to include 5 data points into the training set. After that, the models are re-trained (from scratch) and the performance is evaluated on the test set."}, {"heading": "C.2. Wet-chicken", "text": "We use the continuous two-dimensional version of the problem. A canoeist is paddling on a two-dimensional river. The canoeist\u2019s position at time t is (xt, yt). The river has width w = 5 and length l = 5 with a waterfall at the end, that is, at yt = l. The canoeist wants to move as close to the waterfall as possible because at time t he gets reward rt = \u2212(l \u2212 yt). However, going beyond the waterfall boundary makes the canoeist fall down, having to start back again at the origin (0, 0). At time t the canoeist can choose an action (at,x, at,y) \u2208 [\u22121, 1]2 that represents the direction and magnitude of his paddling. The river dynamics have stochastic turbulences st and drift vt that depend on the canoeist\u2019s position on the x axis. The larger xt, the larger the drift and the smaller xt, the larger the turbulences.\nThe underlying dynamics are given by the following system of equations. The drift and the turbulence magnitude are given by vt = 3xtw\u22121 and st = 3.5 \u2212 vt, respectively. The new location (xt+1, yt+1) is given by the current location (xt, yt) and current action (at,x, at,y) using\nxt+1 =    0 if xt + at,x < 0 0 if y\u0302t+1 > l w if xt + at,x > w xt + at,x otherwise , yt+1 =    0 if y\u0302t+1 < 0 0 if y\u0302t+1 > l y\u0302t+1 otherwise , (14)\nwhere y\u0302t+1 = yt + (at,y \u2212 1) + vt + st\u03c4t and \u03c4t \u223c Unif([\u22121, 1]) is a random variable that represents the current turbulence. As the canoeist moves closer to the waterfall, the distribution for the next state becomes increasingly bi-modal because when he is close to the waterfall, the change in the current location can be large if the canoeist falls down the waterfall and starts again at (0, 0). The distribution may also be truncated uniform for states close to the borders. Furthermore the system\nhas heteroscedastic noise, the smaller the value of xt the higher the noise variance."}, {"heading": "C.3. Reinforcement Learning", "text": "C.3.1. INDUSTRIAL BENCHMARK\nPolicies in the industrial benchmark specify changes \u2206v , \u2206g and \u2206s in three steering variables v (velocity), g (gain) and s (shift) as a function of st. In the behavior policy these changes are stochastic and sampled according to\n\u2206v =    zv , if v(t) < 40 \u2212zv , if v(t) > 60 uv , otherwise\n(15)\n\u2206g =    zg , if g(t) < 40 \u2212zg , if g(t) > 60 ug , otherwise\n(16)\n\u2206s = us , (17)\nwhere zv, zg \u223c N (0.5, 1\u221a3 ) and uv, ug, us \u223c U(\u22121, 1). The velocity v(t) and gain g(t) can take values in [0, 100]. Therefore, the data collection policy will try to keep these values only in the medium range given by the interval [40, 60]. Because of this, large parts of the state space will be unobserved. After collecting the data, the 30, 000 state transitions are used to train a BNN with latent variables with the same hyperparameters as in (Depeweg et al., 2016). Finally, we train different policies using the Monte Carlo approximation and we set the horizon of T = 100 steps, with M = 50 and N = 25 and a minibatch size of 1 for 750 epochs. The total training time on a single CPU is around 18 hours."}, {"heading": "C.3.2. WIND TURBINE SIMULATOR", "text": "In this problem we observe the turbine state s(t), with features such as current wind speed and currently produced power. Our actions a(t) adjust the turbine\u2019s behavior, with known upper and lower bounds. The goal is to maximize energy output over a T -step horizon.\nWe are given a batch of around 5,000 state transitions generated by a behavior policy \u03c0b. The policy does limited exploration around the neutral action a(t) = 0.\nThe system is expected to be highly stochastic due to the unpredictability of future wind dynamics. Furthermore the dimensionality of state observation is much higher than the action dimensionality, so, with the limited dataset that we have, we expect it to be very challenging to accurately learn the influence of the action on the reward.\nFirst we train a BNN with two hidden layer and 50 hidden units per layer on the available batch using \u03b1-divergence minimization with \u03b1 = 1.0. In the second step, using the model, we train a policy with 20 hidden units on each of the two layers in the usual way, using the Monte Carlo estimate. The total training time on a single CPU is around 8 hours."}], "year": 2018, "references": [{"title": "Solving uncertain markov decision processes", "authors": ["J.A. Bagnell", "A.Y. Ng", "J.G. Schneider"], "year": 2001}, {"title": "Nonparametric entropy estimation: An overview", "authors": ["J. Beirlant", "E.J. Dudewicz", "L. Gy\u00f6rfi", "E.C. Van der Meulen"], "venue": "International Journal of Mathematical and Statistical Sciences,", "year": 1997}, {"title": "Safe model-based reinforcement learning with stability guarantees", "authors": ["F. Berkenkamp", "M. Turchetta", "A.P. Schoellig", "A. Krause"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2017}, {"title": "Weight uncertainty in neural networks", "authors": ["C. Blundell", "J. Cornebise", "K. Kavukcuoglu", "D. Wierstra"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Model based bayesian exploration", "authors": ["R. Dearden", "N. Friedman", "D. Andre"], "venue": "In Proceedings of the Fifteenth conference on Uncertainty in artificial intelligence (UAI),", "year": 1999}, {"title": "Pilco: A model-based and data-efficient approach to policy search", "authors": ["M. Deisenroth", "C.E. Rasmussen"], "venue": "In Proceedings of the 28th International Conference on machine learning", "year": 2011}, {"title": "Learning and policy search in stochastic dynamical systems with bayesian neural networks", "authors": ["S. Depeweg", "J.M. Hern\u00e1ndez-Lobato", "F. Doshi-Velez", "S. Udluft"], "venue": "International Conference on Learning Representations (ICLR),", "year": 2016}, {"title": "Aleatory or epistemic? does it matter", "authors": ["A. Der Kiureghian", "O. Ditlevsen"], "venue": "Structural Safety,", "year": 2009}, {"title": "Uncertainty in deep learning", "authors": ["Y. Gal"], "venue": "PhD thesis, University of Cambridge,", "year": 2016}, {"title": "Breaking the bandwidth barrier: Geometrical adaptive entropy estimation", "authors": ["W. Gao", "S. Oh", "P. Viswanath"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Safe exploration of state and action spaces in reinforcement learning", "authors": ["J. Garcia", "F. Fern\u00e1ndez"], "venue": "Journal of Artificial Intelligence Research,", "year": 2012}, {"title": "A comprehensive survey on safe reinforcement learning", "authors": ["J. Garc\u0131\u0301a", "F. Fern\u00e1ndez"], "venue": "The Journal of Machine Learning Research,", "year": 2015}, {"title": "Optimistic active-learning using mutual information", "authors": ["Y. Guo", "R. Greiner"], "venue": "In International Joint Conference on Artificial Intelligence (IJCAI),", "year": 2007}, {"title": "Efficient uncertainty propagation for reinforcement learning with limited data", "authors": ["A. Hans", "S. Udluft"], "venue": "In International Conference on Artificial Neural Networks (ICANN),", "year": 2009}, {"title": "Safe exploration for reinforcement learning", "authors": ["A. Hans", "D. Schneega\u00df", "A.M. Sch\u00e4fer", "S. Udluft"], "venue": "In ESANN,", "year": 2008}, {"title": "A benchmark environment motivated by industrial control problems", "authors": ["D. Hein", "S. Depeweg", "M. Tokic", "S. Udluft", "A. Hentschel", "T.A. Runkler", "V. Sterzing"], "venue": "IEEE Symposium on Computational Intelligence (IEEE SSCI),", "year": 2017}, {"title": "Probabilistic backpropagation for scalable learning of bayesian neural networks", "authors": ["J.M. Hern\u00e1ndez-Lobato", "R. Adams"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2015}, {"title": "Black-box \u03b1-divergence minimization", "authors": ["J.M. Hern\u00e1ndez-Lobato", "Y. Li", "M. Rowland", "D. Hern\u00e1ndez-Lobato", "T. Bui", "R.E. Turner"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2016}, {"title": "Collaborative gaussian processes for preference learning", "authors": ["N. Houlsby", "F. Huszar", "Z. Ghahramani", "J.M. Hern\u00e1ndez-Lobato"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2012}, {"title": "VIME: Variational information maximizing exploration", "authors": ["R. Houthooft", "X. Chen", "Y. Duan", "J. Schulman", "F. De Turck", "P. Abbeel"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2016}, {"title": "Reinforcement learning with misspecified model classes", "authors": ["J. Joseph", "A. Geramifard", "J.W. Roberts", "J.P. How", "N. Roy"], "venue": "In International Conference on Robotics and Automation (ICRA),", "year": 2013}, {"title": "What uncertainties do we need in bayesian deep learning for computer vision", "authors": ["A. Kendall", "Y. Gal"], "venue": "In Advances in Neural Information Processing Systems (NIPS),", "year": 2017}, {"title": "Sample estimate of the entropy of a random vector", "authors": ["L. Kozachenko", "N.N. Leonenko"], "venue": "Problemy Peredachi Informatsii,", "year": 1987}, {"title": "Estimating mutual information", "authors": ["A. Kraskov", "H. St\u00f6gbauer", "P. Grassberger"], "venue": "Physical review E,", "year": 2004}, {"title": "Batch reinforcement learning", "authors": ["S. Lange", "T. Gabel", "M. Riedmiller"], "venue": "In Reinforcement learning,", "year": 2012}, {"title": "How 2 hawc2, the user\u2019s manual", "authors": ["T.J. Larsen", "A.M. Hansen"], "venue": "Technical report, Ris\u00f8 National Laboratory,", "year": 2007}, {"title": "Comparison of measured and simulated loads for the siemens swt2.3 operating in wake conditions at the lillgrund wind farm using hawc2 and the dynamic wake meander model", "authors": ["T.J. Larsen", "G. Larsen", "H.A. Madsen", "K. Thomsen", "S.M. Pedersen"], "venue": "EWEA offshore 2015,", "year": 2015}, {"title": "Heteroscedastic gaussian process regression", "authors": ["Q.V. Le", "A.J. Smola", "S. Canu"], "venue": "In International Conference on Machine Learning (ICML),", "year": 2005}, {"title": "Information-based objective functions for active data selection", "authors": ["D.J. MacKay"], "venue": "Neural computation,", "year": 1992}, {"title": "Particle value functions", "authors": ["C.J. Maddison", "D. Lawson", "G. Tucker", "N. Heess", "A. Doucet", "A. Mnih", "Y.W. Teh"], "venue": "arXiv preprint arXiv:1703.05820,", "year": 2017}, {"title": "Bayesian Learning for Data-Efficient Control", "authors": ["R. McAllister"], "venue": "PhD thesis, Kings College,", "year": 2016}, {"title": "Risk-sensitive reinforcement learning", "authors": ["O. Mihatsch", "R. Neuneier"], "venue": "Machine learning,", "year": 2002}, {"title": "Robust control of markov decision processes with uncertain transition matrices", "authors": ["A. Nilim", "L. El Ghaoui"], "venue": "Operations Research,", "year": 2005}, {"title": "Curious model-building control systems", "authors": ["J. Schmidhuber"], "venue": "In IEEE International Joint Conference on Neural Networks,", "year": 1991}, {"title": "A possibility for implementing curiosity and boredom in model-building neural controllers", "authors": ["J. Schmidhuber"], "venue": "In From animals to animats: Proceedings of the first international conference on simulation of adaptive behavior,", "year": 1991}, {"title": "Active learning", "authors": ["B. Settles"], "venue": "Synthesis Lectures on Artificial Intelligence and Machine Learning,", "year": 2012}, {"title": "Minimax analysis of stochastic problems", "authors": ["A. Shapiro", "A. Kleywegt"], "venue": "Optimization Methods and Software,", "year": 2002}, {"title": "An information-theoretic approach to curiosity-driven reinforcement learning", "authors": ["S. Still", "D. Precup"], "venue": "Theory in Biosciences, pp", "year": 2012}, {"title": "Planning to be surprised: Optimal bayesian exploration in dynamic environments", "authors": ["Y. Sun", "F. Gomez", "J. Schmidhuber"], "venue": "In International Conference on Artificial General Intelligence,", "year": 2011}], "id": "SP:ed58b2c02ab669e9e6018cb7cce5efce80a152ac", "authors": [{"name": "Stefan Depeweg", "affiliations": []}, {"name": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato", "affiliations": []}, {"name": "Finale Doshi-Velez", "affiliations": []}, {"name": "Steffen Udluft", "affiliations": []}], "abstractText": "Bayesian neural networks with latent variables are scalable and flexible probabilistic models: They account for uncertainty in the estimation of the network weights and, by making use of latent variables, can capture complex noise patterns in the data. We show how to extract and decompose uncertainty into epistemic and aleatoric components for decision-making purposes. This allows us to successfully identify informative points for active learning of functions with heteroscedastic and bimodal noise. Using the decomposition we further define a novel risk-sensitive criterion for reinforcement learning to identify policies that balance expected cost, model-bias and noise aversion.", "title": "Decomposition of Uncertainty in Bayesian Deep Learning for Efficient and Risk-sensitive Learning"}