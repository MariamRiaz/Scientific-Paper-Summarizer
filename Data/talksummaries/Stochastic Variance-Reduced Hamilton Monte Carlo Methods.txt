49	1	bn (an & bn) if an is less than (larger than) bn up to a constant.
84	4	, n}, and exj is a snapshot of xk that is only updated every m iterations such that k = jm+ l for some l = 0, .
86	4	The idea of semi-stochastic gradient has been successfully used in stochastic optimization in machine learning to reduce the variance of stochastic gradient and obtains faster convergence rates (Johnson & Zhang, 2013; Xiao & Zhang, 2014; Reddi et al., 2016; Allen-Zhu & Hazan, 2016; Lei & Jordan, 2016; Lei et al., 2017).
91	6	,m 1 5: k = jm+ l 6: Uniformly sample ik 2 [n] 7: gk = rfik(xk) rfik(exj) + eg 8: xk+1 = xk + ⌘vk + ✏xk 9: vk+1 = vk ⌘vk ⌘ugk + ✏vk.
97	3	We first present the convergence rate and gradient complexity of SVR-HMC when f is smooth and strongly convex, i.e., the target distribution ⇡ / e f is smooth and strongly log-concave.
112	2	Under the same conditions as in Theorem 4.3, let m = n and ⌘ = O ✏/( 1d 1/2) ^ ✏ 2/3 /(1/3d1/3n2/3) .
113	1	Then the output of Algorithm 1 satisfies W2 P (xK),⇡  ✏ after eO ✓ n+  2 d 1/2 ✏ + n 2/3  4/3 d 1/3 ✏2/3 ◆ (4.3) stochastic gradient evaluations.
118	3	d 3/✏2, the gradient complexity of our algorithm is dominated by eO(n2/34/3d1/3/✏2/3).
121	1	• When n & 4d/✏2, i.e., the sample size is super large, the gradient complexity of our algorithm is dominated by eO(n).
125	3	Moreover, from Corollary 4.5 we know that the optimal learning rate for SVR-HMC is in the order of O(✏2/3/(1/3d1/3n2/3)), while the optimal learning rate for SG-HMC is in the order of O(✏2/( 2d))), which is smaller than the learning rate of SVR-HMC when n  d 3 /✏ 2 (Dalalyan, 2017).
127	1	In this section, we will extend the analysis of the proposed algorithm SVR-HMC to sampling from distributions which are only general log-concave but not strongly log-concave.
135	9	Before we present our theoretical characterization on this distance, we first lay down the following assumption.
146	1	To compare the convergence rates for different MCMC algorithms, we conduct the experiments on both synthetic data and real data.
150	1	Then it can be observed that the target density ⇡ / exp 1/n Pn i=1 fi(x) = exp (x ā)>⌃(x ā)/2 is a multivariate Gaussian distribution with mean ā = 1/n Pn i=1 ai and covariance matrix ⌃.
152	13	In our simulation, we investigate different dimension d and number of component functions n, and show the 2- Wasserstein distance between the target distribution ⇡ and that of the output from different algorithms with respect to the number of data passes.
156	2	This phenomenon is well-aligned with our theoretical analysis, since the gradient complexity of our algorithm can be worse than SG-HMC when the sample size n is extremely large.
157	4	Now, we apply our algorithm to the Bayesian logistic regression problems.
160	10	Considering the prior p(x) = N (0, 1I), the posterior distribution takes the form p(x|A,Y ) / p(Y |A,x)p(x) = nY i=1 p(yi|ai, )p(x).
161	3	The posterior distribution can be written as p(x|A,Y ) / e Pn i=1 fi(x), where each fi(x) is in the following form fi(x) = n log 1 + exp( yix>ai) + /2kxk22.
172	3	We also apply our algorithm to Bayesian linear regression, and make comparison with the baseline algorithms.
173	1	Similar to Bayesian classification, given i.i.d.
174	1	examples {ai, yi}i=1,...,n with yi 2 R, the likelihood of Bayessian linear regression is p(yi|ai,x) = N (x>ai, 2a) and the prior is N (0, 1I).
175	2	We use 4 datasets, which are summarized in Table 4.
178	2	Similarly, we compute the sample path average while treating the first 50 iterates as burn in.
179	1	We report the mean square errors on the test data on these 4 datasets in Figure 3 for different algorithms.
181	18	We propose a stochastic variance reduced Hamilton Monte Carlo (HMC) method, for sampling from a smooth and strongly log-concave distribution.
182	23	We show that, to achieve ✏ accuracy in 2-Wasserstein distance, our algorithm enjoys a faster rate of convergence and better gradient complexity than state-of-the-art HMC and stochastic gradient HMC methods in a wide regime.
183	51	We also extend our algorithm for sampling from smooth and general log-concave distributions.
184	131	Experiments on both synthetic and real data verified the superior performance of our algorithm.
185	125	In the future, we will extend our algorithm to non-log-concave distributions and study the symplectic integration techniques such as Leap-frog integration for Bayesian posterior sampling.
