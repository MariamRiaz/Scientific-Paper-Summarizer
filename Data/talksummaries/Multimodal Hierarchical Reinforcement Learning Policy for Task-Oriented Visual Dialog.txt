6	25	The agent aims to learn a dialog policy that can guess the correct image through question answering using the minimum number of turns.
7	51	Previous work on visual dialogs (Das et al., 2017a,b; Chattopadhyay et al., 2017) focused mainly on vision-to-language understanding and generation instead of dialog policy learning.
8	40	They let an agent ask a fixed number of questions to rank the images or let humans make guesses at the end of the conversations.
9	20	However, such setting is not realistic in real-world task-oriented applications, because in task-oriented applications, not only completing the task successfully is important but also completing it efficiently.
10	29	In addition, the agent should also be informed of the wrong guesses, so that it becomes more aware of the vision context.
13	45	We propose a multimodal hierarchical reinforcement learning framework that allows learning visual dialog state tracking and dialog policy jointly to complete visual dialog tasks efficiently.
15	27	In our case, it decomposes the decision into two steps: a first step where a master policy selects between verbal task (information query) and vision task (image retrieval), and a second step where a primitive action (question or im- age) is chosen from the selected task.
45	36	Figure 2 shows an overview of the multimodal hierarchical reinforcement learning framework and the simulated environment.
47	44	The visual dialog semantic embedding module learns a multimodal dialog state representation to support the visual dialog state tracking module with attention signals.
48	19	Then the hierarchical policy learning module takes the visual dialog state as the input to optimize the high-level control policy between question selection and image retrieval.
60	24	Given the QA pairs from the simulated environ- ment, the output of this module can also be used for the image retrieval sub-task.
67	58	The vision belief state is the output of the visual dialog semantic embedding module, which captures the internal multimodal information of the agent.
93	66	We define RQ as the pseudo reward for the sub-task of question selection as RQ = At −At−1 (5) At = sigmoid(VBr,t · Itarget) (6) where t refers to the dialog turn and affinity scores (At andAt−1) are the outputs of the sigmoid function that scales the similarity score (0-1) of the vision belief state and the target image vector.
107	20	To make the task setting meaningful and the training time manageable, we pre-process and select 1000 sets of games consisting of 20 similar images.
110	20	A game is terminated when one of the three conditions is fulfilled: 1) the agent guesses the correct answer, 2) the max number of guesses is reached (three guesses) or 3) the max number of dialog turns is reached.
116	14	- Random Question+DQN (Rnd+DQN): The agent randomly selects a question but a DQN is used to optimize the hierarchical decision of making a guess or asking a question.
128	34	Specifically, we enlarge the number of questions by including 200 human generated questions for the 20 images, and use a pre-trained visual question answer model to generate answers with respect to the target image.
129	15	In the last experiment, we further automate the process by generating questions given the 20 images using a pretrained visual question generation model.
133	36	We terminate the dialog after ten turns.
135	19	HRL+SAR achieves the best win rate with statistical significance.
159	15	One thing we need to point out is that the proposed method also received extra information about whether the guess is correct or not from the environment.
164	17	We follow the supervised training scheme discussed in (Das et al., 2017b) to train the visual question generation module offline.
178	23	However, the proposed HRL+SAR is still more resilient to the noise and achieves a higher win rate and less average number of turns compared to other baselines.
180	33	On the other hand, in Experiment 3, the agent reacts to the generated question and answers slower to complete the task.
182	27	It hints that there is a possible limitation of using the VisDial dataset, because the dialog is constructed by users who casually talk about MS COCO images (Chen et al., 2015) instead of exchanging with an explicit contextual goal in the dialog.
183	40	We develop a framework for task-oriented visual dialog systems and demonstrate the efficacy of integrating multimodal state representation with hierarchical decision learning in an image guessing game.
203	38	The reward discount factor was set to be 0.99.
204	25	Algorithm 1 Hierarchical Policy Learning 1: Initialize Double DQN(online network parameters θ and target network parameters θ−) and DRRN(network parameters θ+) with small random weights and corresponding replay memory EDQN and EDRRN to capacity N. 2: Initialize game simulator and load dictionary.
205	60	3: for episode r = 1, ..., M do 4: Restart game simulator.
206	66	5: Receive image caption and candidate images from the simulator, and convert them to represen- tation via pre-trained visual dialog semantic embedding layer, denoted as initial state Sr,0 6: for t = 1, ..., T do 7: sample high-level action from DQN, At ∼ πDQN (Sr,t) 8: if Ar,t = Q(asking a question) then 9: Compute Q(V Ct, qi) for the list of questions Qr,t using DRRN forward activation and select the question qr,t with the max Q-value, and keep track the next available question pool Qr,t+1 10: if Ar,t = G (guessing an image) then 11: Select the image gr,t with the smallest cosine distance between an image vector Ii and current image belief state VBr,t 12: Execute action qr,t or gr,t in the simulator and get the next visual dialog state representation Sr,t+1 and reward signal Rr,t 13: Store the transition (Sr,t, Ar,t, Sr,t+1, Rr,t) into EDQN and if asking a question, also store the transition (V Cr,t, qr,t, V Cr,t+1, Rr,t, Qr,t+1) into EDRRN 14: Sample random mini-batch of transitions (Sk, Ak, Sk+1, Rk) from EDQN 15: Set yDQN = { Rk if terminal state Rk + γQDQN (Sk+1, argmaxa′Q(Sk+1, a ′; θ); θ−) if else 16: Sample random mini-batch of transitions (V Cl, ql, V Cl+1, Rl, Ql+1) from EDRRN 17: Set yDRRN = { Rl if terminal state Rl + γmaxa′∈Ql+1QDRRN (V Cl+1, a ′; θ+) if else 18: Perform gradient steps for DQN with loss ‖ yDQN −QDQN (Sk, Ak; θ) ‖2 with respect to θ and DRRN with loss ‖ yDRRN −QDRRN (V Cl, ql; θ+) ‖2 with respect to θ+ 19: Replace target parameters θ− ← θ for every N steps.
