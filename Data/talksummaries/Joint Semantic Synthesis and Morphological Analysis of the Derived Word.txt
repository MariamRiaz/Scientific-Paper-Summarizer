10	15	face segmentation, our model performs canonical segmentation (Cotterell et al., 2016a; Cotterell et al., 2016b; Kann et al., 2016), i.e., it allows the induction of orthographic changes together with the segmentation, which is not typical.
11	28	For the example questionably, our model can restore the deleted characters le, yielding the canonical segments question, able and ly.
13	19	We experimentally investigate three novel aspects of our model.
14	15	• First, we show that jointly modeling continuous representations of the semantics of morphemes and words allows us to improve morphological analysis.
17	14	• Second, we explore improved models of vector composition for synthesizing word meaning.
27	12	Consider the example of the English noun discontentedness, which is derived from the adjective discontented.
28	11	It is true that both words share a close semantic relationship, but the transformation is clearly more than a simple inflectional marking of syntax.
29	58	Indeed, we can go one step further and define a chain of words content 7→ contented 7→ discontented 7→ discontentedness.
30	21	In the computational literature, derivational morphology has received less attention than inflectional.
36	11	Productivity and Semantic Coherence.
52	12	Implicit in such a treatment is the desire to only segment a word if the segmentation is derived from a productive process.
58	11	From an NLP perspective, canonical segmentation (Naradowsky and Goldwater, 2009; Cotterell et al., 2016b) is the task that seeks to algorithmically decompose a word into its canonical sequence of morphemes.
79	18	The first factor we consider is the transduction factor: exp ( g(u,w)>ω ) , which scores a surface representation (SR) w, the character string observed in raw text, and an underlying representation (UR), a character string with orthographic processes reversed.
89	17	The goal of this factor is to score a segmentation s of a UR u.
107	21	The goal of the composition function Cβ(s, l) is to stitch together morpheme embeddings to approximate the vector of the entire word.
108	12	The simplest form of the composition function Cβ(s, l) is add, an additive model of the morphemes.
120	19	Rather than considering all underlying orthographic forms u and segmentations s, we sample from a tractable proposal distribution q—a distribution over canonical segmentations.
130	15	Concretely, we train two proposal distributions q1(u | w) and q2(l, s | u) that take the form of a WFST and a semi-CRF, respectively, using features identical to the joint model.
185	11	We perform approximate MAP inference with importance sampling—taking the sample with the highest score.
213	13	This baseline tells us what happens if we make the incorrect assumption that derivation behaves like inflection and is not meaning-changing.
233	20	The RNN+DEPs model is .23 better than the BOW5 models (.81 vs. .58), .14 better than the BOW2 models (.81 vs. .67) and .25 better than Lazaridou et al.’s best model (.81 vs. .56).
240	25	Recently, running a recurrent net over the character stream has become a popular way of incorporating subword information into a model—empirical gains have been observed in a diverse set of NLP tasks: POS tagging (dos Santos and Zadrozny, 2014; Ling et al., 2015), parsing (Ballesteros et al., 2015) and language modeling (Kim et al., 2016).
242	13	Given a vector v for a word form w, we seek a function to minimize the following objective 1 2 ||v − hN ||22, (10) where hN is the final hidden state of a recurrent neural architecture, i.e., hi = σ(Ahi−1 +Bwi), (11) where σ is a non-linearity and wi is the ith character in w, hi−1 is the previous hidden state and A and B are matrices.
243	12	While we have defined the architecture for a vanilla RNN, we experiment with two more advanced recurrent architectures: GRUs (Cho et al., 2014b) and LSTMs (Hochreiter and Schmidhuber, 1997) as well as deep variants (Sutskever et al., 2014; Gillick et al., 2016; Firat et al., 2016).
258	18	(i) RNN+DEPs attains an average cosine similarity of around .80 for English.
292	13	We have presented a model of the semantics and structure of derivationally complex words.
295	22	Also, our models show state-of-theart performance on the derivational vector approximation task introduced by Lazaridou et al. (2013).
296	19	Future work will focus on the extension of the method to more complex instances of derivational morphology, e.g., compounding and reduplication, and on the extension to additional languages.
298	60	The first author was supported by a DAAD Long-Term Research Grant and an NDSEG fellowship and the second by a Volkswagenstiftung Opus Magnum grant.
299	144	We would also like to thank action editor Regina Barzilay for suggesting several changes we incorporated into the work and the three anonymous reviewers.
