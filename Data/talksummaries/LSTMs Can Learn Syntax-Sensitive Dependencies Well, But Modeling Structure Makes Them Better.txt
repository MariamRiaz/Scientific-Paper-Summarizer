2	55	Here we revisit the question asked by Linzen et al. (2016): as RNNs model word sequences without explicit notions of hierarchical structure, to what extent are these models able to learn non-local syntactic dependencies in natural language?
20	63	As we observe different patterns in number agreement, this demonstrates that while perplexity can be a useful diagnostic tool, it may not be sensitive enough for comparing models in terms of how well they capture grammatical intuitions.
21	41	We revisit the number agreement task with LSTMs trained on language modeling objectives, as proposed by Linzen et al. (2016).
23	23	We summarize the corpus statistics of the dataset, along with the test set distribution of the number of attractors, in Table 1.
27	30	We report performance of a few different LSTM hidden layer configurations, while other hyper-parameters are selected based on a grid search.2 Following Linzen et al. (2016), we include the results of our replication3 of the large-scale language model of Jozefowicz et al. (2016) that was trained on the One Billion Word Benchmark.4 Hyper-parameter tuning is based on validation set perplexity.
28	53	Table 2 indicates that, given enough capacity, LSTM language models without explicit syntactic supervision are able to perform well in number agreement.
29	200	For cases with multiple attractors, we observe that the LSTM language model with 50 hidden units trails behind its larger counterparts by a substantial margin despite comparable performance for zero attractor cases, suggesting that network capacity plays an especially important role in propagating relevant structural information across a large number of steps.5 Our experiment independently derives the same finding as the recent work of Gulordava et al. (2018), who also find that LSTMs trained with language modeling objectives are able to learn number agreement well; here we additionally identify model capacity as one of the reasons for the discrepancy with the Linzen et al. (2016) results.
32	30	In the vast majority of cases, structural dependencies between subjects and verbs highly overlap with sequential dependencies (Table 1).
40	65	Second, by nature of modeling characters, non-local structural dependencies are sequentially further apart than in the wordbased language model.
44	39	To some extent, our finding demonstrates the limitations that character LSTMs face in learning structure from language modeling objectives, despite earlier evidence that character LSTM language models are able to implicitly acquire a lexicon (Le Godais et al., 2017).
47	54	Our choice of RNNGs is motivated by the findings of Kuncoro et al. (2017), who find evidence for syntactic headedness in RNNG phrasal representations.
49	42	In some sense, the composition operator can be understood as injecting a structural recency bias into the model design, as subjects and verbs that are sequentially apart are encouraged to be close together in the RNNGsâ€™ representation.
50	28	RNNGs (Dyer et al., 2016) are language models that estimate the joint probability of string terminals and phrase-structure tree nonterminals.
51	151	Here we use stack-only RNNGs that achieve better perplexity and parsing performance (Kuncoro et al., 2017).
58	34	At training time, we use these predicted trees to derive action sequences on the training set, and train the RNNG model on these sequences.9 At test time, we compare the probabilities of the correct and incorrect verb forms given the prefix, which now includes both nonterminal and terminal symbols.
59	25	An example of the stack contents (i.e. the prefix) when predicting the verb is provided in Fig.
61	121	2 shows that RNNGs (rightmost) achieve much better number agreement accuracy compared to LSTM language models (leftmost) for difficult cases with four and five attractors, with around 30% error rate reductions, along with a 13% error rate reduction (from 9% to 7.8%) for three attractors.
64	78	3(a)) before the verb itself is introduced, while LSTM language models benefit from shorter dependencies for zero and one attractor cases.
66	37	First, RNNGs have access to predicted syntactic annotations, while LSTM language models operate solely on word sequences.
74	62	Our finding is consistent with the recent work of Yogatama et al. (2018), who find that introducing elements of hierarchical modeling through a stackstructured memory is beneficial for number agreement, outperforming LSTM language models and attention-augmented variants by increasing margins as the number of attractor grows.
75	112	In order to better interpret the results, we conduct further analysis into the perplexities of each model, followed by a discussion on the effect of incrementality constraints on the RNNG when predicting number agreement.
76	46	To what extent does the success of RNNGs in the number agreement task with multiple attractors correlate with better performance under the perplexity metric?
80	114	As the syntactic prefix was derived from a discriminative model that has access to unprocessed words, one potential concern is that this prefix might violate the incrementality constraints and benefit the RNNG over the LSTM language model.
88	89	This stepby-step arrangement extends to the derived string as well; since all variants generate words from left to right, the models can be compared using number agreement as a diagnostic.14 Here we state our hypothesis on why the build order matters.
89	29	The three generation strategies represent different chain rule decompositions of the joint probability of strings and phrase-structure trees, thereby imposing different biases on the learner.
90	35	Earlier work in parsing has characterized the plausibility of top-down, left-corner, and bottom-up strategies as viable candidates of human sentence processing, especially in terms of memory constraints and human difficulties with center embedding constructions (Johnson-Laird, 1983; Pulman, 1986; Abney and Johnson, 1991; Resnik, 1992, inter alia), along with neurophysiological evidence in human sentence processing (Nelson et al., 2017).
91	31	Here we cast the three strategies as models of language generation (Manning and Carpenter, 1997), and focus on the empirical question: which generation order has the most appropriate bias in modeling non-local structural dependencies in English?
99	67	As the commitment to label the nonterminal type of a phrase is delayed until its constituents are complete, this means that the generation of a child node cannot condition on the label of its parent node.
104	25	Once the extent of the new nonterminal has been decided, we parameterize the decision of the nonterminal label type; in Fig.
122	61	Nevertheless, the top-down variant outperforms both left-corner and bottom-up strategies for difficult cases with three or more attractors, suggesting that the top-down strategy is most appropriately biased to model difficult number agreement dependencies in English.
