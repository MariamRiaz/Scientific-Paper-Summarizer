22	32	This is somewhat analogous to the training process used in AlphaGo (Silver et al., 2016) for the game of Go.
26	12	The experimental results demonstrate the effectiveness of our latent intention model which achieves state-of-the-art performance on both automatic corpus-based evaluation and human evaluation.
29	60	Based on the search result, the model needs to summarise its findings and reply with an appropriate response mt in natural language.
31	13	It comprises three components: (1) Representation Construction; (2) Policy Network; and (3) Generator, as shown in Figure 1.
32	31	To capture the user’s intent and match it against the system’s knowledge, a dialogue state vector st = ut ⊕ bt ⊕ xt is derived from the user input ut and the knowledge base KB: ut is the distributed utterance representation, which is formed by encoding the user utterance2 ut with a bidirectional LSTM (Hochreiter & Schmidhuber, 1997) and concatenating the final stage hidden states together, ut = biLSTMΘ(ut).
33	16	(1) The belief vector bt, which is a concatenation of a set of probability distributions over domain specific slot-value pairs, is extracted by a set of pre-trained RNN-CNN belief trackers (Wen et al., 2017; Mrkšić et al., 2017), in which ut and mt−1 are processed by two different CNNs as shown in Figure 1, bt = RNN-CNN(ut,mt−1,bt−1) (2) where mt−1 is the preceding machine response and bt−1 is the preceding belief vector.
36	23	Q is then used to search the internal KB and return a vector xt representing the degree of matching in the KB.
39	15	For more details about the belief trackers and database operation refer to Wen et al (2016; 2017).
40	9	Conditioning on the state st, the policy network parameterises the latent intention zt by a single layer MLP, πΘ(zt|st) = softmax(Wᵀ2 · tanh(W ᵀ 1st +b1) +b2) (3) where W1, b1, W2, b2 are model parameters.
42	15	A latent intention z(n)t (or an action in the reinforcement learning literature) can then be sampled from the conditional distribution, z (n) t ∼ πΘ(zt|st).
43	73	(4) This sampled intention (or action) z(n)t and the state vector st can then be combined into a control vector dt, which is used to govern the generation of the system response based on a conditional LSTM language model, dt = W ᵀ 4zt ⊕ [sigmoid(W ᵀ 3zt + b3) ·W ᵀ 5st] (5) pΘ(mt|st, zt) = ∏ j p(wtj+1|wtj ,htj−1,dt) (6) where b3 and W3∼5 are parameters, zt is the 1-hot representation of z(n)t , w t j is the last output token (i.e. a word, a delexicalised2 slot name or a delexicalised2 slot value), and htj−1 is the decoder’s last hidden state.
47	10	To carry out inference for the LIDM, we introduce an inference network qΦ(zt|st,mt) to approximate the posterior distribution p(zt|st,mt) so that we can optimise the variational lower bound of the joint probability in a neural variational inference framework (Miao et al., 2016).
50	30	The inference network qΦ(zt|st,mt) is then constructed by qΦ(zt|st,mt) = Multi(ot) = softmax(W6ot) (9) ot = MLPΦ(bt,xt,ut,mt) (10) ut = biLSTMΦ(ut),mt = biLSTMΦ(mt) (11) where ot is the joint representation, and both ut and mt are modeled by a bidirectional LSTM network.
52	8	Based on the samples z(n)t ∼ qΦ(zt|st,mt), we use different strategies to alternately optimise the parameters Θ and Φ against the variational lower bound (Equation 8).
59	13	To reduce the variance during inference, we follow the REINFORCE algorithm (Mnih et al., 2014; Mnih & Gregor, 2014) and introduce two baselines b and b(st), the centered learning signal and input dependent baseline respectively to help reduce the variance.
66	50	Standard clustering algorithms can therefore be used to pre-process the corpus and generate automatic labels ẑt for part of the training examples (mt, st, ẑt) ∈ L. Then when the model is trained on the unlabeled examples (mt, st) ∈ U, we optimise it against the modified variational lower bound given in Equation 8 L1 = ∑ (mt,st)∈U Eqφ(zt|st,mt) [log pθ(mt|zt, st)] − λDKL(qφ(zt|st,mt)||πθ(zt|st)) (18) However, when the model is updated based on examples from the labeled set (mt, st, ẑt) ∈ L, we treat the labeled intention ẑt as an observed variable and train the model by maximising the joint log-likelihood, L2 = ∑ (mt,ẑt,st)∈L log [pΘ(mt|ẑt, st)πΘ(ẑt|st)qΦ(ẑt|st,mt)] (19) The final joint objective function can then be written as L′ = αL1 + L2, where α controls the trade-off between the supervised and unsupervised examples.
69	46	Since πΘ(zt|st) is a parameterised policy network itself, any policy gradient-based reinforcement learning algorithm (Williams, 1992; Konda & Tsitsiklis, 2003) can be used to fine-tune the initial policy against other objective functions that we are more interested in.
75	16	The corpus was collected based on a modified Wizard of Oz (Kelley, 1984) online data collection.
79	12	The database contains 99 unique restaurants.
83	7	All the system components were trained jointly by fixing the pre-trained belief trackers and the discrete database operator with the model’s latent intention size I set to 50, 70, and 100, respectively.
92	8	During reinforcement fine-tuning, we generated a sentence mt from the model to replace the ground truth m̂t at each turn and define an immediate reward as whether mt can improve the dialogue success (Su et al., 2015) relative to m̂t, plus the sentence BLEU score (Auli & Gao, 2014), rt = η · sBLEU(mt, m̂t) +  1 mt improves −1 mt degrades 0 otherwise (21) where the constant η was set to 0.5.
107	9	At the end of each conversation the judges were asked to rate and compare the model’s performance.
110	25	During human evaluation, we sampled from the top-5 intentions of the LIDM models and decoded a response based on the sample.
118	90	Each induced latent intention is shown by a tuple (index, probability) followed by a decoded response, and the sample dialogues were produced by following the responses highlighted in bold.
128	16	While in reinforcement learning, the latent distribution (i.e. policy network) is updated by the rewards from dialogue success and sentence BLEU score.
129	43	Hence, the latent variable bridges the different learning paradigms such as Bayesian learning and reinforcement learning and brings them together under the same framework.
130	10	This framework provides a more robust neural network-based approach than previous approaches because it does not depend solely on sequence-to-sequence learning but instead explicitly models the hidden dialogue intentions underlying the user’s utterances and allows the agent to directly learn a dialogue policy through interaction.
150	11	We have shown that the LIDM can discover an effective initial policy from the underlying data distribution and is capable of revising its strategy based on an external reward using reinforcement learning.
151	10	We believe this is a promising step forward for building autonomous dialogue agents since the learnt discrete latent variable interface enables the agent to perform learning using several differing paradigms.
152	31	The experiments showed that the proposed LIDM is able to communicate with human subjects and outperforms previous published results.
