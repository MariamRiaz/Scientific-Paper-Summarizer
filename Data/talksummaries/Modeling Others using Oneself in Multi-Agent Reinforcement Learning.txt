0	10	Reasoning about other agents’ intentions and being able to predict their behavior is important in multi-agent systems, in which the agents might have different, and sometimes competing, goals.
1	30	In this paper, we introduce a new approach for estimating other agents’ unknown goals from their behavior and using those estimates to choose actions.
2	20	We demonstrate that in the proposed tasks, using an explicit model of the other player leads to better performance than simply considering the other agent as part of the environment.
3	67	We frame the problem as a two-player stochastic game (Shapley, 1953), in which each agent is randomly assigned a different goal from a fixed set, which is shared between the agents.
4	8	Players have full visibility of the environment, but no direct knowledge of the other’s goal and no communication channel.
6	17	The key idea of this work is that as a first approximation of understanding what the other player is trying to achieve, an agent should ask itself “what would be my goal if I had acted as the other player had?”.
8	5	As the agent plays the game, it uses its own policy (with the input expressed in the other agent’s frame of reference) to maximize the likelihood of the other’s observed actions and optimize directly over the goal representation to infer the other agent’s unknown goal.
9	12	In contrast with the current literature, our approach does not require building any model of the other agent in order to infer its intention and predict its behavior.
13	34	In this work, we consider both cooperative and adversarial settings.
16	52	To decide an action and to estimate the value of a state, we use a neural network f that takes as input its own goal zself , an estimate of the other player’s goal z̃other, and the observation state sself , and outputs a probability distribution over actions π and a value estimate V , such that for each agent i playing the game we have:[ πi V i ] = f(siself , z i self , z̃ i other; θ i) .
17	8	Here θi are agent i’s parameters for f , which has one softmax output for the policy, one linear output for the value function, and all the non-output layers shared.
18	6	The actions are sampled from policy πi.
27	14	Then, the agent uses supervision of the other’s true action to backpropagate through fother (without updating its paramters) and directly optimize over its input z̃other, the estimate of the other agent’s goal.
37	44	We consider a continuous vector z̃other of the same dimension as zself , such that the estimate of the other agent’s Algorithm 1 SOM training for one episode 1: procedure SELF OTHER-MODELING 2: for k := 1, num players do 3: z̃kother ← 1ngoals1ngoals 4: game.reset() 5: for step := 1, episode length do 6: siself = s j other ← game.get state() 7: z̃OH,iother = one hot(argmax(softmax(z̃ i other)) 8: πiself , V i self ← f iself (siself , ziself , z̃ OH,i other; θ i) 9: aiself ∼ πiself 10: game.action(aiself ) 11: for k : = 1, num inference steps do 12: z̃GS,jother = gumbel soft(softmax(z̃ j other)) 13: π̃jother← f j other(s j other, z̃ GS,j other, z j self ; θ j) 14: loss = cross entropy loss(π̃jother, a i self ) 15: loss.backward() 16: update(z̃jother) 17: for k := 1, num players do 18: policy.update(θk) goal is a sample from the Categorical distribution with class probabilities softmax(z̃other).
42	6	The agents’ policies are parametrized by long short-term memory (LSTM) cells (Hochreiter & Schmidhuber, 1997) with two fully-connected linear layers, and exponential linear unit (ELU) (Clevert et al., 2015) activations.
82	5	NO-OTHER-MODEL (NOM): The first baseline we use only takes as inputs the observation states sself and its own goal zself .
83	8	NOM has the same architecture as the one used for SOM’s policy network, fself .
86	7	Besides the A3C loss used to train the policy of this network, we also add a cross-entropy loss to train the prediction of the other agent’s action, using observations of its true actions.
123	7	Our model clearly outperforms all other baselines on this task.
124	22	We also show the empirical upper bound on the reward using the model which takes as input the true color assigned to the other agent.
125	17	Figure 2 analyzes the strategies of the different models by looking at the proportion of coins of each type collected by the agents.
127	27	Due to the randomness in the initial conditions (in particular, the locations of coins in the environment), this amounts to each agent collecting an equal number of coins of its own and of the other’s color on average, across a large number of episodes (i.e. n̄selfCself = n̄ self Cother ).
128	8	Indeed, this is the strategy learned by the model with perfect information of the other agent’s goal (TOG).
129	17	SOM also learns to collect significantly more Other than Neither coins (although not as many as Self coins), indicating its ability to distinguish between the two types, at least during some of the episodes.
130	12	This means that SOM can accurately infer the other agent’s goal early enough during the episode and use that information to collect more Other Coins, thus gaining more reward than if it were only using its own goal to direct its actions.
131	21	In contrast, the agents trained with the three baseline models collect significantly more Self coins, and as many Other as Neither coins on average.
133	8	Even if IPP and SPP are able to predict the actions of the other player with an accuracy of about 50%, they do not learn to distinguish between the coins that would increase (Other) and those that would decrease (Neither) their reward.
135	10	Agents in adversarial scenarios have competing goals, so the ability of inferring the opponent’s goal could better inform the agent’s actions.
136	12	With this motivation in mind, we evaluate our model on a game in which the agents have to craft certain compositional recipes, each containing multiple items found in the environment.
138	67	The resources in the environment are scarce, so only one of the agents can craft its recipe within one episode.
