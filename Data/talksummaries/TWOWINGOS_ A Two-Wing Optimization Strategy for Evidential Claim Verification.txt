1	47	While the task of claim verification will not tell us the absolute truth of this claim, it is expected to determine whether the claim is supported by evidence in a given text collection.
2	18	Specifically, given a claim and a text corpus, evidential claim verification, demonstrated in 1cogcomp.org/page/publication_view/847 Figure 1, aims at identifying text snippets in the corpus that act as evidence that supports or refutes the claim.
9	11	First, to locate text snippets in the given corpus that can potentially be used to determine the truth value of the given claim.
12	16	This motivates us to develop an approach that can transfer knowledge from claim verification to evidence identification.
13	117	Second, the evidence for a claim might require aggregating information from multiple sentences and even multiple documents (rf.
16	9	The discussion above suggests that claim verification and evidence identification are tightly coupled.
17	30	Claim should influence the identification of appropriate evidence, and “trusted evidence boosts the claim’s veracity” (Vydiswaran et al., 2011).
21	44	Prior work mostly approached this problem as a pipeline procedure – first, given a claim x, determine Se by some similarity matching; then, conduct textual entailment over (Se, x) pairs.
22	27	Our framework, TWOWINGOS, optimizes the two subtasks jointly, so that both claim verification and evidence identification can enhance each other.
25	9	Our analysis shows that (i) entity mentions in claims provide a strong clue for retrieving relevant passages; (ii) composition of evidence clues across sentences helps claim verification; and that (iii) the joint training scheme provides significant benefits of a pipeline architecture.
46	52	Next, we use two separate subsections to elaborate the process of evidence identification (i.e., optimize p to q) and the claim verification (i.e., optimize o to z).
47	7	A simple approach to identifying evidence is to detect the top-k sentences that are lexically similar to the claim, as some pipeline systems (Roth et al., 2009; Thorne et al., 2018) do.
52	9	We first stack a vanilla CNN (convolution & max-pooling) (LeCun et al., 1998) over T to get a representation for t. As a result, each evidence candidate si has a representation si, and the claim x has a representation x.
56	12	Instead of directly employing the sentence-level representations, here we explore claim-aware representations for each word in sentence si, then compose them as the sentence representation ri, inspired by the Attentive Convolution (Yin and Schütze, 2017).
59	13	A convolution encoder generates its claim-aware representation iji : iji = tanh(W · [s j−1 i , s j i , s j+1 i , c j i ] + b) (3) where parameters W ∈ Rd×4d, b ∈ Rd.
62	14	With a claim-aware representation ri, the sentence si subsequently gets a probability, acting as the evidence, αi ∈ (0, 1) via a non-linear sigmoid function: αi = sigmoid(v · rTi ) (6) where parameter vector v has the same dimensionality as ri.
64	8	pi indicates si is evidence or not.
65	8	All {si} with pi = 1 act as evidence set Se.
73	13	It does not model the interactions within the evidence nor the interactions between the evidence and the claim.
81	18	By “two-channel,” we mean that each evidence si is aware of two kinds of context, one from the claim x, the other from the remaining evidences.
85	21	The reason we add s j i and c j i is motivated by a simple experience: Assume the claim “Lily lives in the biggest city in Canada”, and one sentence contains a clue “· · · Lily lives in Toronto · · · ” and another sentence contains a clue “· · · Toronto is Canada’s largest city· · · ”.
86	20	The most simple yet effective approach to aggregating the two clues is to sum up their representation vectors (Blacoe and Lapata, 2012) (we do not concatenate them, as those clues have no consistent textual order across different sji ).
87	15	After updating the representation of each word in si, we perform the aforementioned “singlechannel fine-grained representation” between the updated si and the claim x, generating [e, x].
89	42	Given the loss lev in evidence identification and the loss lcv in claim verification, the overall training loss is represented by: l = lev + lcv (15) To ensure that we jointly train the two coupled subtasks with intensive knowledge communication instead of simply putting two pipeline neural networks together, our TWOWINGOS has following configurations: • Both subsystems share the same set of word embeddings as parameters; the vanilla CNNs for learning sentence and claim representations share parameters as well.
90	34	• The output binary vector p by the evidence identification module is forwarded to the module of claim verification, as shown in Equations 8-10.
93	12	The claims in FEVER were generated from the introductory parts of about 50K Wikipedia pages of a June 2017 dump.
98	17	Each claim is labeled as SUPPORTED, REFUTED or NOTENOUGHINFO (NEI).
102	334	We can see that roughly 28% of the evidence covers more than one sentence, and approximately 16.3% of the evidence covers more than one wiki page.
103	47	This task has three evaluations: (i) NOSCOREEV – accuracy of claim verification, neglecting the validity of evidence; (ii) SCOREEV – accuracy of claim verification with a requirement that the predicted evidence fully covers the gold evidence for SUPPORTED and REFUTED; (iii) F1 – between the predicted evidence sentences and the ones chosen by annotators.
105	26	For each claim, we search in the given dictionary of wiki pages in the form of {title: sentence list}, and keep the top-5 ranked pages for fair comparison with Thorne et al. (2018).
