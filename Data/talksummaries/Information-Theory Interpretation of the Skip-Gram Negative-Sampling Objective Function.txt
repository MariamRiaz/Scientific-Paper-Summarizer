14	25	In Section 3, we show that the value of the SGNS objective computed at the PMI matrix is this information measure.
15	17	We then derive an explicit expression for the information loss caused by the low-dimensional embedding learned by the SGNS algorithm.
16	6	Finally, in Section 4, we illustrate this by computing the information loss caused by actual SGNS embeddings learned on a standard text corpus.
20	11	The Kullback-Leibler (KL) divergence of a distribution p from a distribution q is defined as follows: KL(p||q) =∑i∈A pi log piqi .
21	36	The mutual information between two jointly distributed random variables X and Y is defined as the KL divergence of the joint distribution p(x, y) from the product p(x)p(y) of the marginal distributions of X and Y, i.e. I(X;Y ) = KL(p(x, y)||p(x)p(y)).
25	13	We define the Jensen-Shannon Mutual information (JSMI) as follows: JSMIα(X,Y ) = JSα(p(x, y), p(x)p(y)).
28	18	Assume we choose between the two distributions, p(x, y) and the product of marginal distributions p(x)p(y), according to a binary random variableZ, such that p(Z = 1) = α.
30	12	(3) The divergence measure JSMIα(X,Y ) can be alternatively defined in terms of mutual information between W and Z.
31	18	(4) Applying Bayes rule we obtain: p(Z=1|W =(x, y)) (5) = αp(x, y) αp(x, y) + (1−α)p(x)p(y) = 1 1 + exp(− log( αp(x,y)(1−α)p(x)p(y))) = σ(pmix,y) such that σ(u) = 11+exp(−u) is the sigmoid function and pmix,y = log p(x, y) p(x)p(y) + log α 1−α (6) is a shifted version of the PMI function.
34	30	We can represent a given d-dimensional embedding by a matrix m, such that m(x, y) = ~x · ~y.
39	8	(8) Note that the term h( 1k+1), which does not appear in the original SGNS objective function (Mikolov et al., 2013), is a constant number that was added here to simplify the following presentation.
43	8	The actual SGNS score, then, is: S(m) ≈ h( 1 k+1 ) + 1 k+1 · 1 n n∑ t=1 (log σ(~xt · ~yt) + k∑ i=1 log σ(−~xt · ~yti)).
44	11	(9) such that t goes over all the word-context pairs in a given corpus.
54	18	(7)), we exactly obtain the SGNS score (8) at the PMI matrix.
55	7	2 Levy and Goldberg (2014) showed that SGNS’s objective achieves its maximal value at the PMI matrix.
56	11	However, this result reveals nothing about the more interesting lower dimensional case, where the PMI matrix factorization is forced to compress the joint distribution and thereby learn a meaningful embedding.
58	15	Given the word co-occurrences joint distribution p(x, y), we obtained in Eq.
60	30	In a similar way, given any matrixm, we can define a conditional distribution pm on the alphabet of (Z,W ) as follows: pm(Z=1|W =(x, y)) = σ(mx,y).
65	20	In this case, the true distribution is the pmi-based classifier ppmi(Z|W ).
67	26	The difference JSMIα(X,Y )− S(m) is the information loss caused by the lowdimensional embedding.
68	14	We can view it as a Jensen-Shannon variant of the information bottleneck principle (Tishby et al., 1999; Globerson et al., 2007) that is defined in terms of the KL divergence.
72	37	Previous works suggested other criteria for matrix factorization such as least-squares (Eckart and Young, 1936) and KL-divergence between the original matrix and the low-rank matrix approximation (Lee and Seung, 2000).
73	7	We have shown that the SGNS algorithm factorizes the PMI matrix based on the JSMI-based criterion stated in Eq.
78	9	A version of this dataset is available from Tomas Mikolov.1 It consists of 929K training words with a 10K word vocabulary, which we used to train our models.
86	5	As can be seen, the embeddings score gets close to the optimal value using higher dimensionality and more training iterations, but doesn’t surpass it.
87	8	In this study, we developed a new correlation measure between random variables, denoted JSMI.
88	5	This measure is based on the JS divergence and differs from the standard mutual information measure that is based on the KL divergence.
89	41	We showed that the optimization of skip-gram embeddings with negative sampling finds the best low-dimensional approximation of the JSMI measure.
90	49	Thus, we provided an information theory framework that hopefully contributes to a better understanding of this embedding algorithm.
91	17	Furthermore, although we focused here on the case of word-context joint distributions, the connection we haven shown between the PMI matrix and the JSMI function is valid for every joint distribution of two random variables.
