5	28	DNC is a collection of NLI problems, each requiring a model to perform a unique type of reasoning.
9	25	The tasks include event factuality, named entity recognition, gendered anaphora resolution, sentiment analysis, relationship extraction, pun detection, and lexicosyntactic inference.
10	60	Currently, the DNC contains over half a million labeled examples.
13	22	In short, this work answers a recent plea to the community to test “more kinds of inference” than in previous challenge sets (Chatzikyriakidis et al., 2017).
15	82	NLU Insights Popular NLI datasets, e.g. Stanford Natural Language Inference (SNLI) (Bowman et al., 2015) and its successor MultiNLI (Williams et al., 2017), were created by eliciting hypotheses from humans.
16	69	Crowd-source workers were tasked with writing one sentence each that is entailed, neutral, and contradicted by a caption extracted from the Flickr30k corpus (Young et al., 2014).
17	85	Although these datasets are widely used to train and evaluate sentence representations, a high accuracy is not indicative of what types of reasoning NLI models perform.
19	23	Such datasets cannot be used to determine how well an NLI model captures many desired capabilities of language understanding systems, e.g. paraphrastic inference, complex anaphora resolution (White et al., 2017), or compositionality (Pavlick and Callison-Burch, 2016; Dasgupta et al., 2018).
20	48	By converting prior annotation of a specific phenomenon into NLI examples, recasting allows us to create a diverse NLI benchmark that tests a model’s ability to perform distinct types of reasoning.
40	27	Previous Recast NLI Example sentences in RTE1 (Dagan et al., 2006) were extracted from MT, IE, and QA datasets, with the process referred to as ‘recasting’ in the thesis by Glickman (2006).
63	22	Given a sentence annotated with NER tags, we recast the annotations by preserving the original sentences as contexts and creating hypotheses using the template “NP is a Label.”6 For ENTAILED hypotheses we replace Label with the correct NER label of the NP; for NOT-ENTAILED hypotheses, we choose an incorrect label from the prior distribution of NER tags for the given phrase.
80	19	(2) a. Jo didn’t remember that she ate b. Jo didn’t remember to eat This small change in the syntactic structure gives rise to large changes in the inferences that are licensed: (2a) presupposes that Jo ate while (2b) entails that Jo didn’t eat.
81	31	We recast data from three datasets that are relevant to these sorts of lexicosyntactic interactions.
91	21	(5) a. Michael swatted the fly b. cause(E, Agent) c. Agent caused the E d. Michael caused the swatting We use the Berkeley Parser (Petrov et al., 2006) to match tokens in an example sentence with the thematic roles and then fill in the templates with the matched tokens (5d).
137	25	If the original sentence contained positive (negative) sentiment, the (8a)-(8b) pair is labeled as ENTAILED (NOT-ENTAILED) and (8a)(8c) is labeled as NOT-ENTAILED (ENTAILED).
142	28	Our experiments demonstrate how these recast datasets may be used to evaluate how well models capture different types of semantic reasoning necessary for general language understanding.
146	20	InferSent independently encodes a context and hypothesis with a bi-directional LSTM and combines the sentence representations by concatenating the individual sentence representations, 13“Her teeth was cared for” or “Floss were used”.
159	26	Interestingly, the hypothesis-only model outperforms InferSent on the recast RE.
167	25	Across all of the recast datasets, updating the pretrained model’s parameters during training improves InferSent’s accuracies more than keeping the model’s parameters fixed.
168	41	When updating a model pre-trained on the entire DNC, we see the largest improvements on VN (+9.15).
170	24	We posit that if a model, trained on and performing well on Multi-NLI, does not perform well on our recast datasets, then Multi-NLI might not evaluate a model’s ability to understand the “full complexity” of language as argued.17 When trained on Multi-NLI, our InferSent model achieves an accuracy of 70.22% on (matched) Multi-NLI.18 When we test the model on the recast datasets (without updating the parameters), we see significant drops.19 On the datasets testing a model’s lexicosyntactic inference capabilities, the model performs below the majority class baseline.
173	71	These results might suggest that Multi-NLI does not evaluate whether sentence representations capture these distinct semantic phenomena.
178	24	On five of the recast datasets, using a model pre-trained on DNC outperforms a model pre-trained on Multi-NLI.
181	27	From this, it is unclear whether pre-training on DNC is better than Multi-NLI.
182	31	Size of Pre-trained DNC Data We randomly sample 10K and 20K examples from each datasets’ training set to investigate what happens if we train our models on a subsample of each training set instead of the entire DNC.
202	31	We described how we recast a wide range of semantic phenomena from many NLP datasets into labeled NLI sentence pairs.
203	79	These examples serve as a diverse NLI framework that may help diagnose whether NLU models capture and perform distinct types of reasoning.
204	22	Our experiments demonstrate how to use this framework as an NLU benchmark.
206	96	We encourage dataset creators to recast their datasets in NLI and invite them to add their recast datasets into the DNC.
207	21	The collection, along with baselines and trained models are available online at http://www.decomp.net.
