1	43	While the associated alignment problem could be solved with access to large amounts of parallel data, broader applicability relies on the ability to do so with largely mono-lingual data, from Part-of-Speech (POS) tagging (Zhang et al., 2016), dependency parsing (Guo et al., 2015), to machine translation (Lample et al., 2018).
2	60	The key subtask of bilingual lexical induction, for example, while long standing as a problem (Fung, 1995; Rapp, 1995, 1999), has been actively pursued recently (Artetxe et al., 2016; Zhang et al., 2017a; Conneau et al., 2018).
3	36	Current methods for learning cross-domain correspondences at the word level rely on distributed representations of words, building on the observation that mono-lingual word embeddings exhibit similar geometric properties across languages Mikolov et al. (2013).
4	38	While most early work assumed some, albeit minimal, amount of parallel data (Mikolov et al., 2013; Dinu et al., 2014; Zhang et al., 2016), recently fully-unsupervised methods have been shown to perform on par with their supervised counterparts (Conneau et al., 2018; Artetxe et al., 2018).
8	34	OT is a general mathematical toolbox used to evaluate correspondence-based distances and establish mappings between probability distributions, including discrete distributions such as point-sets.
21	31	The associated training procedure will later be used for extending unsupervised alignments (Section 3.2).
23	59	Let X and Y be the matrices whose columns are vectors x(i) and y(j), respectively.
33	48	Optimal transport formalizes the problem of finding a minimum cost mapping between two point sets, viewed as discrete distributions.
34	21	Specifically, we assume two empirical distributions over embeddings, e.g., µ = n∑ i=1 piδx(i) , ν = m∑ j=1 qjδy(i) (3) where p and q are vectors of probability weights associated with each point set.
49	16	Such degrees of freedom can dramatically change the entries of the cost matrix Cij = ‖x(i) − y(j)‖ and the resulting transport map.
54	18	We adopt a theoretically well-founded generalization of optimal transport for pairs of points (their distances), thus in line with how the embeddings are estimated in the first place.
58	57	Such a metric may not be available, for example, when the sample sets to be matched do not belong to the same metric space (e.g., different dimension).
59	51	The Gromov-Wasserstein distance (Mémoli, 2011) generalizes optimal transport by comparing the metric spaces directly instead of samples across the spaces.
68	32	Furthermore, for suitable choices of loss function L, Peyré et al. (2016) show that instead of the O(N21N 2 2 ) complexity implied by naive fourthorder tensor product, this computation reduces to O(N21N2 + N1N 2 2 ) cost.
69	25	Their approach consists of solving (5) by projected gradient descent, which yields iterations that involve projecting onto Π(p,q) a pseudo-cost matrix of the form ĈΓ(C,C ′,Γ) = Cxy − h1(C)Γh2(C′)> (9) where Cxy = f1(C)p1 > m + 1nq >f2(C ′)> and f1, f2, h2, h2 are functions that depend on the loss L. We provide an explicit algorithm for the case L = L2 at the end of this section.
73	17	The transportation coupling Γ, being normalized by construction, requires no such artifacts.
75	24	With the choice L = L2, GW 1 2 is a distance on the space of metric measure spaces.
76	37	Solving problem (8) therefore yields a fascinating accompanying notion: the GromovWasserstein distance between languages, a measure of semantic discrepancy purely based on the relational characterization of their word embeddings.
81	22	In order to provide a fair comparison to previous work, throughout our experiments we use uniform distributions so as to avoid providing our method with additional information not available to others.
82	22	While the pure Gromov-Wasserstein approach leads to high quality solutions, it is best suited to small-to-moderate vocabulary sizes,2 since its optimization becomes prohibitive for very large problems.
84	30	Formally, suppose we solve problem (8) for a reduced matrices X1:k and Yi:k consisting of the first columns k of X and Y, respectively, and let Γ∗ be the optimal coupling.
106	19	Here, we focus on the language pairs for which they report results: English (EN) from/to Spanish (ES), French (FR), German (DE), Russian (RU) and simplified Chinese (ZH).
113	25	As previously mentioned, our approach involves only two optimization choices, one of which is required only for very large settings.
120	17	In addition, we found that computing GW distances between closer languages (such as EN and FR) leads to faster convergence than for more distant ones (such as EN and RU, in Fig.
124	25	Indeed, as pointed out by Artetxe et al. (2018), the FASTTEXT embeddings provided in this task are trained on very large and highly comparable—across languages— corpora (Wikipedia), and focuses on closely related pairs of languages.
130	101	In our case, it suffices to normalize the pairwise similarity matrices to the same range to obtain substantially better results.
134	37	As mentioned earlier, Theorem 3.1 implies that the optimal value of the Gromov-Wasserstein problem can be legitimately interpreted as a distance between languages, or more explicitly, between their word embedding spaces.
156	18	The Gromov-Wasserstein distance is well-suited for this task as it performs a relational comparison of word-vectors across languages rather than wordvectors directly.
159	68	While directly solving Gromov-Wasserstein problems of reasonable size is feasible, scaling up to large vocabularies made it necessary to learn an explicit mapping via Procrustes.
160	65	GPU computations or stochastic optimization could help avoid this secondary step.
