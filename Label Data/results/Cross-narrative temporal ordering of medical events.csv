0,1,label2,summary_sentences
"Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2832–2838 Copenhagen, Denmark, September 7–11, 2017. c©2017 Association for Computational Linguistics",text,[0],[0]
"Bidirectional Long Short-Term Memory (BLSTM) based models (Graves and Schmidhuber, 2005), along with word embeddings and character embeddings, have shown competitive performance on Part-of-Speech (POS) tagging given sufficient amount of training examples (Ling et al., 2015; Lample et al., 2016; Plank et al., 2016; Yang et al., 2017).
",1 Introduction,[0],[0]
"Given insufficient training examples, we can improve the POS tagging performance by cross-
lingual POS tagging, which exploits affluent POS tagging corpora from other source languages.",1 Introduction,[0],[0]
"This approach usually requires linguistic knowledge or resources about the relation between the source language and the target language such as parallel corpora (Täckström et al., 2013; Duong et al., 2013; Kim et al., 2015a; Zhang et al., 2016), morphological analyses (Hana et al., 2004), dictionaries (Wisniewski et al., 2014), and gaze features (Barrett et al., 2016).
",1 Introduction,[0],[0]
"Given no linguistic resources between the source language and the target language, transfer learning methods can be utilized instead.",1 Introduction,[0],[0]
"Transfer learning for cross-lingual cases is a type of transductive transfer learning, where the input domains of the source and the target are different (Pan and Yang, 2010) since each language has its own vocabulary space.",1 Introduction,[0],[0]
"When the input space is the same, lower layers of hierarchical models can be shared for knowledge transfer (Collobert et al., 2011; Kim et al., 2015b; Yang et al., 2017), but that approach is not directly applicable when the input spaces differ.
",1 Introduction,[0],[0]
Yang et al. (2017) used shared character embeddings for different languages as a cross-lingual transfer method while using different word embeddings for different languages.,1 Introduction,[0],[0]
"Although the approach showed improved performance on Named Entity Recognition, it is limited to character-level representation transfer and it is not applicable for knowledge transfer between languages without overlapped alphabets.
",1 Introduction,[0],[0]
"In this work, we introduce a cross-lingual transfer learning model for POS tagging requiring no cross-lingual resources, where knowledge transfer is made in the BLSTM layers on top of word embeddings and character embeddings.",1 Introduction,[0],[0]
"Inspired by Kim et al. (2016)’s multi-task slot-filling model, our model utilizes a common BLSTM for representing language-generic information, which al-
2832
lows knowledge transfer from other languages, and private BLSTMs for representing languagespecific information.",1 Introduction,[0],[0]
"The common BLSTM is additionally encouraged to be language-agnostic with language-adversarial training (Chen et al., 2016) so that the language-general representations to be more compatible among different languages.
",1 Introduction,[0],[0]
"Evaluating on POS datasets from 14 different target languages with English as the source language in the Universal Dependencies corpus 1.4 (Nivre et al., 2016), the proposed model showed significantly better performance when the source language and the target language are in the same language family, and competitive performance when the language families are different.",1 Introduction,[0],[0]
Cross-Lingual Training Figure 1 shows the overall architecture of the proposed model.,2 Model,[0],[0]
"The baseline POS tagging model is similar to Plank et al. (2016)’s model, and it corresponds to having only word+char embeddings, common BLSTM, and Softmax Output in Figure 1.",2 Model,[0],[0]
"Given an input
word sequence, a BLSTM is used for the character sequence of each word, where the outputs of the ends of the character sequences from the forward LSTM and the backward LSTM are concatenated to the word vector of the current word to supplement the word representation.",2 Model,[0],[0]
"These serve as an input to a BLSTM, and an output layer are used for POS tag prediction.
",2 Model,[0],[0]
"For the cross-lingual transfer learning, the character embedding, the BLSTM with the character embedding (Yang et al., 2017),1 and the common BLSTM are shared for all the given languages while word embeddings and private BLSTMs have different parameters for different languages.
",2 Model,[0],[0]
The outputs of the common BLSTM and the private BLSTM of the current language are summed to be used as the input to the softmax layer to predict the POS tags of given word sequences.,2 Model,[0],[0]
"The loss function of the POS tagging can be formulate as:
Lp = − S∑
i=1",2 Model,[0],[0]
"N∑ j=1 pi,j log (p̂i,j) , (1)
where S is the number of sentences in the current minibatch,N is the number of words in the current sentence, pi,j is the label of the j-th tag of the i-th sentence in the minibatch, and p̂i,j is the predicted tag.",2 Model,[0],[0]
"In addition to this main objective, two more objectives for improving the transfer learning are described in the following subsections.
",2 Model,[0],[0]
"Language-Adversarial Training We encourage the outputs of the common BLSTM to be language-agnostic by using language-adversarial training (Chen et al., 2016) inspired by domainadversarial training (Ganin et al., 2016; Bousmalis et al., 2016).",2 Model,[0],[0]
"First, we encode a BLSTM output sequence as a single vector using a CNN/MaxPool encoder, which is implemented the same as a CNN for text classification (Kim, 2014).",2 Model,[0],[0]
"The encoder is with three convolution filters whose sizes are 3, 4, and 5.",2 Model,[0],[0]
"For each filter, we pass the BLSTM output sequence as the input sequence and obtain a single vector from the filter output by using max pooling, and then tanh activation function is used for transforming the vector.",2 Model,[0],[0]
"Then, the vector outputs of the three filters are concatenated and forwarded to the language discriminator through the gradient reversal layer.",2 Model,[0],[0]
"The discriminator is implemented
1We also tried isolated character-level modules but the overall performance was worse.
as a fully-connected neural network with a single hidden layer, whose activation function is Leaky ReLU (Maas et al., 2013), where we multiply 0.2 to negative input values as the outputs.
",2 Model,[0],[0]
"Since the gradient reversal layer is below the language classifier, the gradients minimizing language classification errors are passed back with opposed sign to the sentence encoder, which adversarially encourages the sentence encoder to be language-agnostic.",2 Model,[0],[0]
"The loss function of the language classifier is formulated as:
La = − S∑
i=1
li log l̂i, (2)
where S is the number of sentences, li is the language of the i-th sentence, and l̂i is the softmax output of the tagging.",2 Model,[0],[0]
"Note that though the language classifier is optimized to minimize the language classification error, the gradient from the language classifier is negated so that the bottom layers are trained to be language-agnostic.
",2 Model,[0],[0]
"Bidirectional Language Modeling Rei (2017) showed the effectiveness of the bidirectional language modeling objective, where each time step of the forward LSTM outputs predicts the word of the next time step, and each of the backward LSTM outputs predicts the previous word.",2 Model,[0],[0]
"For example, if the current sentence is “I am happy”, the forward LSTM predicts “am happy <eos>” and the backward LSTM predicts “<bos> I am”.",2 Model,[0],[0]
"This objective encourages the BLSTM layers and the embedding layers to learn linguistically general-purpose representations, which are also useful for specific downstream tasks (Rei, 2017).",2 Model,[0],[0]
"We adopted the bidirectional language modeling objective, where the sum of the common BLSTM and the private BLSTM is used as the input to the language modeling module.",2 Model,[0],[0]
"It can be formulated as:
Ll = − S∑
i=1",2 Model,[0],[0]
N∑ j=1 log (P (wj+1|fj)),2 Model,[0],[0]
"+
log (P (wj−1|bj)) , (3)
where fj and bj represent the j-th outputs of the forward direction and the backward direction, respectively, given the output sum of the common BLSTM and the private BLSTM.
",2 Model,[0],[0]
"All the three loss functions are added to be optimized altogether as:
L = ws",2 Model,[0],[0]
"(Lp + λLa + λLl) , (4)
where λ is gradually increased from 0 to 1 as epoch increases so that the model is stably trained with auxiliary objectives (Ganin et al., 2016).",2 Model,[0],[0]
ws is used to give different weights to the source language and the target language.,2 Model,[0],[0]
"Since the source language has a larger train set and we are focusing on improving the performance of the target language, ws is set to 1 when training the target language.",2 Model,[0],[0]
"For the source language, instead, it is set as the size of the target train set divided by the size of the source train set.",2 Model,[0],[0]
"For the evaluation, we used the POS datasets from 14 different languages in Universal Dependencies corpus 1.4 (Nivre et al., 2016).",3 Experiments,[0],[0]
"We used English as the source language, which is with 12,543 training sentences.2",3 Experiments,[0],[0]
We chose datasets with 1k to 14k training sentences.,3 Experiments,[0],[0]
"The number of tag labels differs for each language from 15 to 18 though most of them are overlapped within the languages.
",3 Experiments,[0],[0]
"Table 1 shows the POS tagging accuracies of different transfer learning models when we limited the number of training sentences of the target languages to be the same as 1,280 for fair comparison among different languages.",3 Experiments,[0],[0]
The remainder training examples of the target languages are still used for both language-adversarial training and bidirectional language modeling since the objectives do not require tag labels.,3 Experiments,[0],[0]
Training with only the train sets in the target languages (c) showed 91.61% on average.,3 Experiments,[0],[0]
"When bidirectional language modeling objective is used (c, l), the accuracies were significantly increased to 92.82% on average.",3 Experiments,[0],[0]
"Therefore, we used the bidirectional language modeling for all the transfer learning evaluations.
",3 Experiments,[0],[0]
"With transfer learning, the three cases of using only the common BLSTM (c), using only the private BLSTMs (p), and using both (c, p) were evaluated.",3 Experiments,[0],[0]
"They showed better average accuracies than target only cases, but they showed mixed results.",3 Experiments,[0],[0]
"However, our proposed model (c, p, l + a), which utilizes both the common BLSTM with language-adversarial training and the private BLSTMs, showed the highest average score, 93.26%.",3 Experiments,[0],[0]
"For all the Germanic languages, where the source language also belongs to, the accuracies are significantly higher than those of
2The accuracies of English POS tagging are 94.01 and 94.33 for models without the bidirectional language modeling and with it, respectively.
",3 Experiments,[0],[0]
other transfer learning models.,3 Experiments,[0],[0]
"For the languages belonging to Slavic, Romance, or Indo-Iranian, our model shows competitive performance with the highest average accuracies among the compared models.",3 Experiments,[0],[0]
"Since languages in the same family are more likely to be similar and compatible, it is expected that the gain from the knowledge transfer to the languages in the same family to be higher than transferring to the languages in different families, which was shown in the results.",3 Experiments,[0],[0]
"This shows that utilizing both language-general representations that are encouraged to be more language-agnostic and language-specific representations effectively helps improve the POS tagging performance with transfer learning.
",3 Experiments,[0],[0]
Table 2 shows the results when using 320 taglabeled training sentences.,3 Experiments,[0],[0]
"In this case, transfer learning methods still show better accuracies than target-only approaches on average.",3 Experiments,[0],[0]
"However, the performance gain is weakened compared to using 1,280 labeled training sentences and there are some mixed results.",3 Experiments,[0],[0]
"In several cases, just utilizing private BLSTMs without the common BLSTM showed better accuracies than utilizing the common BLSTM.
",3 Experiments,[0],[0]
"When training with only 32 tag-labeled sentences, which is an extremely low-resourced setting, transfer learning methods still showed better accuracies than target-only methods on average.",3 Experiments,[0],[0]
"However, not using the common BLSTM
in transfer learning models showed better performance than using it on average.3",3 Experiments,[0],[0]
The main reason would be that we are not given a sufficient number of labeled training sentences to train both the common BLSTM and the private BLSTMs.,3 Experiments,[0],[0]
"In this case, just having private BLSTMs without the common BLSTM can show better performance.",3 Experiments,[0],[0]
"We also evaluated the opposite cases, which use all the tag-labeled training sentences in the target languages, and they showed mixed results.",3 Experiments,[0],[0]
"For example, the accuracy of German with the target only model is 93.31% while that of the proposed model is 93.04%.",3 Experiments,[0],[0]
"This is expected since transfer learning is effective when the target train set is small.
",3 Experiments,[0],[0]
An extension of this work is utilizing multiple languages as the source languages.,3 Experiments,[0],[0]
"Since we have four languages for each of Germanic, Slavic, and Romance language families, we evaluated the performance of those languages using the other languages in the same families as the source languages expecting that languages in the same language family are more likely to be helpful each other.",3 Experiments,[0],[0]
"For the efficiency, we performed multi-task learning for multiple languages rather than differentiating the targets from sources.",3 Experiments,[0],[0]
"When we tried to use 1,280, 320, and 32 tag-labeled training sentences for each language in the multi-source settings, the results showed noticeably better per-
3The results in detail are shown in the first authors dissertation Kim (2017).
formance than the results of using English as a single source language.",3 Experiments,[0],[0]
"Considering that utilizing 1,280*3=3,840, 320*3=960, or 32*3=96 tag labels from three other languages showed better results than using 12,543 English tag labels as the source, we can see that the knowledge transfer from multiple languages can be more helpful than that from single resource-rich source language.",3 Experiments,[0],[0]
"We also tried to use Wasserstein distance (Arjovsky et al., 2017) for the adversarial training in the multi-source settings, but there were no significant differences on average.4
Implementation Details All the models were optimized using ADAM (Kingma and Ba, 2015)5 with minibatch size 32 for total 100 epochs and we picked the parameters showing the best accuracy on the development set to report the score on the test set.",3 Experiments,[0],[0]
The dimensionalites of all the BLSTM related layers follow Plank et al. (2016)’s model.,3 Experiments,[0],[0]
Each word vector is 128 dimensional and each character vector is 100 dimensional.,3 Experiments,[0],[0]
"They are randomly initialized with Xavier initialization (Glorot and Bengio, 2010).",3 Experiments,[0],[0]
"For stable training, we use gradient clipping, where the threshold is set to 5.",3 Experiments,[0],[0]
"The dimensionality of each hidden output of LSTMs is 100, and the hidden outputs of both forward LSTM and backward LSTM are concatenated, thereby the output of each BLSTM for each time step is 200.",3 Experiments,[0],[0]
"Therefore, the input to the common BLSTM and the private BLSTM is 128+200=328
4The extended work in detail are shown in Kim (2017).",3 Experiments,[0],[0]
"5learning rate=0.001, β1 = 0.9, β2 = 0.999, = 1e− 8.
dimensional.",3 Experiments,[0],[0]
"The inputs and the outputs of the BLSTMs are regularized with dropout rate 0.5 (Pham et al., 2014).",3 Experiments,[0],[0]
"For the consistent dropout usages, we let the dropout masks to be identical for all the time steps of each sentence (Gal and Ghahramani, 2016).",3 Experiments,[0],[0]
"For all the BLSTMs, forget biases are initialized with 1 (Jozefowicz et al., 2015) and the other biases are initialized with 0.",3 Experiments,[0],[0]
"Each convolution filter output for the sentence encoding is 64 dimensional, and the three filter outputs are concatenated to represent each sentence with a 192 dimensional vector.",3 Experiments,[0],[0]
We introduced a cross-lingual transfer learning model for POS tagging which uses separate BLSTMs for language-general and languagespecific representations.,4 Conclusion,[0],[0]
"Evaluating on 14 different languages, including the source language improved tagging accuracies in almost all the cases.",4 Conclusion,[0],[0]
"Specifically, our model showed noticeably better performance when the source language and the target languages belong to the same language family, and competitively performed with the highest average accuracies for target languages in different families.",4 Conclusion,[0],[0]
We thank the anonymous reviewers for their helpful comments.,Acknowledgments,[0],[0]
All the experiments in this work were conducted with machines at Ohio Supercomputer Center (1987).,Acknowledgments,[0],[0]
Training a POS tagging model with crosslingual transfer learning usually requires linguistic knowledge and resources about the relation between the source language and the target language.,abstractText,[0],[0]
"In this paper, we introduce a cross-lingual transfer learning model for POS tagging without ancillary resources such as parallel corpora.",abstractText,[0],[0]
"The proposed cross-lingual model utilizes a common BLSTM that enables knowledge transfer from other languages, and private BLSTMs for language-specific representations.",abstractText,[0],[0]
The cross-lingual model is trained with language-adversarial training and bidirectional language modeling as auxiliary objectives to better represent language-general information while not losing the information about a specific target language.,abstractText,[0],[0]
"Evaluating on POS datasets from 14 languages in the Universal Dependencies corpus, we show that the proposed transfer learning model improves the POS tagging performance of the target languages without exploiting any linguistic knowledge between the source language and the target language.",abstractText,[0],[0]
Cross-Lingual Transfer Learning for POS Tagging without Cross-Lingual Resources,title,[0],[0]
"Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 998–1008, Baltimore, Maryland, USA, June 23-25 2014. c©2014 Association for Computational Linguistics",text,[0],[0]
"Discourse structure, logical flow of sentences, and context play a large part in ordering medical events based on temporal relations within a clinical narrative.",1 Introduction,[0],[0]
"However, cross-narrative temporal relation ordering is a challenging task as it is difficult to learn temporal relations among medical events which are not part of the logically coherent discourse of a single narrative.",1 Introduction,[0],[0]
"Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.",1 Introduction,[1.0],"['Resolving crossnarrative temporal relationships between medical events is essential to the task of generating an event timeline from across unstructured clinical narratives such as admission notes, radiology reports, history and physical reports and discharge summaries.']"
"Such a timeline has multiple applications in clinical trial recruitment (Luo et al., 2011), medical document summarization (Bramsen et al.,
2006, Reichert et al., 2010) and clinical decision making (Demner-Fushman et al., 2009).
",1 Introduction,[0],[0]
"Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?",1 Introduction,[1.0],"['Given multiple temporally ordered medical event sequences generated from each clinical narrative in a patient record, how can we combine the events to create a timeline across all the narratives?']"
"The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).",1 Introduction,[1.0],"['The tendency to copy-paste text and summarize past information in newly generated clinical narratives leads to multiple mentions of the same medical event across narratives (Cohen et al., 2013).']"
These cross-narrative coreferences act as important anchors for reasoning with information across narratives.,1 Introduction,[0],[0]
We leverage crossnarrative coreference information along with confident cross-narrative temporal relation predictions and learn to align and temporally order medical event sequences across longitudinal clinical narratives.,1 Introduction,[0],[0]
We model the problem as a sequence alignment task and propose solving this using two approaches.,1 Introduction,[0],[0]
"First, we use weighted finite state machines to represent medical events sequences, thus enabling composition and search to obtain the most probable combined sequence of medical events.",1 Introduction,[0],[0]
"As a contrast, we adapt dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) used to produce global and local alignments for aligning sequences of medical events across narratives.",1 Introduction,[0],[0]
"We also compare the proposed methods with an Integer Linear Programming (ILP) based method for timeline construction (Do et al., 2012).",1 Introduction,[0],[0]
"The cross-narrative coreference and temporal relation scores used in both these approaches are learned from a corpus of patient narratives from The Ohio State University Wexner Medical Center.
",1 Introduction,[0],[0]
The main contribution of this paper is a general framework that allows aligning multiple event sequences using cascaded weighted finite state transducers (WFSTs) with the help of efficient composition and decoding.,1 Introduction,[0],[0]
"Moreover, we demonstrate that this method can be used for more accurate multiple sequence alignment when compared to
998
dynamic programming or other ILP-based methods proposed in literature.",1 Introduction,[0],[0]
"In the areas of summarization and text-to-text generation, there has been prior work on several ordering strategies to order pieces of information extracted from different input documents (Barzilay et al., 2002, Lapata, 2003, Bollegala et al., 2010).",2 Related Work,[0],[0]
"In this paper, we focus on temporal ordering of information, as discussed next.
",2 Related Work,[0],[0]
"Recent state-of-the art research has focused on the problem of temporal relation learning within the same document, and in many cases within the same sentence (Mani et al., 2006, Verhagen et al., 2009, Lapata and Lascarides, 2011).",2 Related Work,[0],[0]
"Chambers and Jurafsky (2009) describe a process to induce a partially ordered set of events related by a common protagonist by using an unsupervised distributional method to learn relations between events sharing coreferring arguments, followed by temporal classification to induce partial order.",2 Related Work,[0],[0]
"The task was carried out on the Timebank newswire corpus, but was limited to an intra-document setting.",2 Related Work,[0],[0]
"More recently, (Do et al., 2012) proposed an ILP-based method to combine the outputs of an event-interval and an event-event classifier for timeline construction on the ACE 2005 corpus.",2 Related Work,[0],[0]
"However, this approach is also restricted to events within documents and requires annotations for event intervals.",2 Related Work,[0],[0]
We empirically compare our methods for timeline creation from longitudinal clinical narratives to such an ILP-based approach in Section 7.,2 Related Work,[0],[0]
"While a lot of this work has been done in the news domain, there is also some recent work in rule-based algorithms (Zhou et al., 2006) and machine learning (Roberts et al., 2008) applied to temporal relations between medical events in clinical text.",2 Related Work,[0],[0]
"Clinical narratives are written in a distinct sub-language with domain specific terminology and temporal characteristics, making them markedly different from newswire text.
",2 Related Work,[0],[0]
There is limited prior work in learning relations across documents.,2 Related Work,[0],[0]
"Ji and Grishman (2008) extended the one sense per discourse idea (Yarowsky, 1995) to multiple topically related documents and propagate consistent event arguments across sentences and documents.",2 Related Work,[0],[0]
Barzilay and McKeown (2005) propose a text-to-text generation technique for synthesizing common information across documents using sentence fusion.,2 Related Work,[0],[0]
"This involves multisequence dependency tree alignment to identify phrases conveying sim-
ilar information and statistical generation to combine common phrases into a sentence.",2 Related Work,[0],[0]
"Along with syntactic features, they combine knowledge from resources like WordNet to find similar sentences.",2 Related Work,[0],[0]
"In case of clinical narratives and medical event alignment, the objective is to identify a unique sequence of temporally ordered medical events from across longitudinal clinical data.
",2 Related Work,[0],[0]
"To the best of our knowledge, there is no prior work on cross-document alignment of event sequences.",2 Related Work,[0],[0]
"Multiple sequence alignment is a problem that arises in a variety of domains including gene/protein alignments in bioinformatics (Notredame, 2002), word alignments in machine translation (Kumar and Byrne, 2003), and sentence alignments for summarization (Lacatusu et al., 2004).",2 Related Work,[0],[0]
"Dynamic programming algorithms have been popularly leveraged to produce pairwise and global genetic alignments, where edit distance based metrics are used to compute the cost of insertions, deletions and substitutions.",2 Related Work,[0],[0]
"We use dynamic programming to compute the best alignment, given the temporal and coreference information between medical events across these sequences.",2 Related Work,[0],[0]
"More importantly, we propose a cascaded WFST-based framework for crossdocument temporal ordering of medical event sequences.",2 Related Work,[0],[0]
"Composition and search operations can be used to build a single transducer that integrates these components, directly mapping from input states to desired outputs, and obtain the best alignment (Mohri et al., 2000).",2 Related Work,[0],[0]
"In natural language processing, WFSTs have seen varied applications in machine translation (Kumar and Byrne, 2003), morphology (Sproat, 2006), named entity recognition (Krstev et al., 2011) and biological sequence alignment / generation (Whelan et al., 2010) among others.",2 Related Work,[0],[0]
We demonstrate that the WFST-based approach outperforms popularly used dynamic programming algorithms for multiple sequence alignment.,2 Related Work,[0],[0]
"Medical events are temporally-associated concepts in clinical text that describe a medical condition affecting the patient’s health, or procedures performed on a patient.",3 Problem Description,[0],[0]
We represent medical events by splitting each event into a start and a stop.,3 Problem Description,[0],[0]
"When there is insufficient information to discern the start or stop of an event, it is represented as a single concept.",3 Problem Description,[0],[0]
"If only the start is known then the stop is set to +∞, whereas when only the stop is known , the start is set to the date of birth of the
patient.1 Often, for chronic ailments like hypertension, we would only associate a start with the medical event and set the stop to +∞. The start of hypertension may be associated with the temporal expression history of in the narrative.",3 Problem Description,[0],[0]
"This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.",3 Problem Description,[1.0],"['This, when considered along with the admission date, allows us to relatively order hypertension with respect to other medical events.']"
"A medical event occurrence like chest pain may be associated with a start and a stop, where the start may be determined by the mention of “patient was complaining of chest pain yesterday” in the narrative text.",3 Problem Description,[0],[0]
"Further, the narrative may state that “he continued to have chest pain on admission, but currently he is chest pain free”; this may be used to infer the relative stop of chest pain.",3 Problem Description,[0],[0]
"Medical events may also be instantaneous, for e.g., injected with antibiotic.",3 Problem Description,[0],[0]
Such events are represented with the start and stop as being the same.,3 Problem Description,[0],[0]
Temporal relations exist between the start and stop of events as shown in Figure 1.,3 Problem Description,[0],[0]
"Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.",3 Problem Description,[1.0],"['Learning temporal relations before, after and simultaneous between the medical event starts and stops corresponds to learning all of Allen’s temporal relations (Allen, 1981) between the medical events.']"
"Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.",3 Problem Description,[1.0],"['Following our previous work (Raghavan et al., 2012c), such a representation allows us to temporally order the event starts and stops within each clinical narrative by learning to rank them in relative order of time.']"
"The problem definition is as follows:
1Patient date of birth, admission/ discharge date are usually available in the metadata associated with a clinical narrative.
",3 Problem Description,[0],[0]
Input: Sequences of temporally ordered medical event starts and stops.,3 Problem Description,[0],[0]
"This corresponds to N1, N2, and N3 in Figure 2.",3 Problem Description,[0],[0]
Each sequence corresponds to a clinical narrative.,3 Problem Description,[1.0],['Each sequence corresponds to a clinical narrative.']
"The total number of sequences correspond to the number of clinical narratives for a patient.
",3 Problem Description,[1.0000000022336084],['The total number of sequences correspond to the number of clinical narratives for a patient.']
Problem:,3 Problem Description,[0],[0]
"Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.
",3 Problem Description,[0.9946273619465908],"['Problem: Combine medical events across these sequences to generate a timeline i.e., a single comprehensive sequence of medical events over all clinical narratives of the patient.']"
Expected Output:,3 Problem Description,[0],[0]
"In the example shown in Figure 2, the output would be as follows:",3 Problem Description,[0],[0]
"Timeline (N1, N2, N3)= {cocaine usestart < hypertensionstart = hypertensionstart < admission1 < chest painstart ∼ palpitationsstart < chest painstop < heart attackstart = myocardial infarctionstart <",3 Problem Description,[0],[0]
"admission2 < infectionstart < MRSAstart < admission3 < woundsstart}.
",3 Problem Description,[0],[0]
The goal of multiple sequence alignment is to find an alignment that maximizes some overall alignment score.,3 Problem Description,[0],[0]
"Thus, in order to align event sequences, we need to compute scores corresponding to cross-narrative medical event coreference resolution and cross-narrative temporal relations.",3 Problem Description,[0],[0]
"The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[1.0],"['The first approach to learning a temporal ordering of medical events across all clinical narratives is to consider all pairs of events across all narratives and learn to classify them as sharing one of Allen’s temporal relations (Allen, 1981) using a single learning model.']"
"Alternatively, a ranking ap-
proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0.9999999996838496],"['Alternatively, a ranking ap- proach, similar to the one used to generate intranarrative temporal ordering, can also be extended to the cross-narrative case.']"
"However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[1.0],"['However, the features related to narrative structure and relative and implicit temporal expressions used for temporal ordering within a clinical narrative may not be applicable across narratives.']"
"For instance, a history and physical report may have sections like “past medical history”, “history of present illness”, “assessment and plan”, and a certain logical pattern to the flow of text within and across these sections.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Further, temporal cues like “thereafter”, “subsequently”, follow from the context around an event mention.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"The absence of such features in the cross-narrative case does not allow such a model to generate accurate temporal relation predictions.
",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Thus, for use in our sequence alignment models, we learn two independent classifiers for medical event coreference and temporal relation learning across narratives.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
We train a classifier to resolve cross-narrative coreferences by extracting semantic and temporal relatedness feature sets for each pair of medical concepts.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Extracting these feature sets helps us train a classifier to predict medical event coreferences (Raghavan et al., 2012a).",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
"Another classifier is then trained to classify pairs of medical event starts and stops across narratives as sharing temporal relations {before, after, overlaps}.",4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
The learned cross-narrative coreference predictions can then be used along with confident temporal relation predictions to derive a joint probability to enable cross-narrative temporal ordering.,4 Cross-Narrative Coreference Resolution and Temporal Relation Learning,[0],[0]
Sequence alignment algorithms have been developed and popularly used in bioinformatics.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"However, multiple sequence alignment (MSA) has been shown to be NP complete (Wang and Jiang, 1994) and various heuristic algorithms have been proposed to solve this problem (Notredame, 2002).",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"We propose a novel WFST-based representation that enables accurate decoding for MSA when compared to popularly used dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) or other state of the art methods (Do et al., 2012).
",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"In the problem of aligning events across multiple narrative sequences, we want to align temporally ordered medical events corresponding to clinical narratives of a patient.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Unlike problems in biological sequence alignment where the sym-
bols to be aligned across sequences are restricted to a fixed set, our symbol set is not fixed or certain because the symbols correspond to medical events in clinical narratives.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Moreover, we cannot have fixed scores for symbol transformations since our transformations correspond to coreference and temporal relations between the medical events across sequences.",5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
The computation of these scores is described next.,5 Narrative Sequence Alignment for Cross-narrative Temporal Ordering,[0],[0]
"Let us assume a, b are medical events in the first clinical narrative and have been temporally ordered so a < b.",5.1 Scoring Scheme,[0],[0]
"Similarly, x, y are medical events in the second clinical narrative such that x",5.1 Scoring Scheme,[0],[0]
< y.,5.1 Scoring Scheme,[0],[0]
"There exists a match or an alignment between a pair of medical events, across the sequences, in the following cases:
1.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and coreferring, denoted as a = x.
2.",5.1 Scoring Scheme,[0],[0]
"If the medical events are simultaneous and non-coreferring, denoted as a ∼ x.
3.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is before a medical event from another sequence, denoted as a < x.
4.",5.1 Scoring Scheme,[0],[0]
"If the a medical event from one sequence is after a medical event from another sequence, denoted as a > x.
We now illustrate how the scores for candidate aligned sequences are computed using the learned cross-narrative coreference and temporal probabilities for the following three scenarios:
• The medical events across sequences are simultaneous and corefer as illustrated in Figure 3.",5.1 Scoring Scheme,[0],[0]
"The joint score considers the probability of event temporal relations simultaneous conditioned on coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
Some medical events across sequences are simultaneous but do not corefer as illustrated in Figure 4.,5.1 Scoring Scheme,[0],[0]
"Here, the joint score considers the joint probability of temporal relations simultaneous or before and no-coreference.
",5.1 Scoring Scheme,[0],[0]
•,5.1 Scoring Scheme,[0],[0]
The medical events across sequences are not simultaneous and do not corefer as illustrated in Figure 5.,5.1 Scoring Scheme,[0],[0]
"In this case, the joint score considers the probability of the temporal relation before and no coreference.
",5.1 Scoring Scheme,[0.9999998932300748],"['In this case, the joint score considers the probability of the temporal relation before and no coreference.']"
"Thus, the coreference and temporal relation scores can be leveraged for aligning sequences of medical events.",5.1 Scoring Scheme,[0],[0]
"These scores are used in both the WFSTbased representation and decoding, as well as for dynamic programming.",5.1 Scoring Scheme,[0],[0]
"A weighted finite-state transducer (WFST) is an automaton in which each transition between states
is associated with an input symbol, an output symbol, and a weight (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
WFSTs can be used to efficiently represent and combine sequences of medical events based coreference and temporal relation information.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST representation gives us the ability to talk about the global joint probability derived from coreference and temporal relation scores described in Section 5.1.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
It allows us to build a weighted lattice of sequences that can be searched for the most probable sequence of medical events from across all clinical narratives of a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We use unweighted FSAs to represent the input described in Section 3, i.e. temporally ordered sequences of medical events corresponding to clinical narratives.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"This corresponds to N1 and N2 in Figure 6.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on whether we want to align the sequences purely based on coreference scores or both coreference and temporal relation scores, the arc weights for the WFST can be determined.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
M c12 is a WFST that maps input symbols from N1 to output symbols inN2 and is weighted by the probability of coreference or no-coreference between medical events across N1 and N2.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The representation in WFST M c+t12 shown in Figure 7 allows us to align N1 and N2 based on both coreference as well as temporal relation probabilities.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST has transitions to accommodate insertion and deletion of medical events when combining the sequences.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
Deletions correspond to the case when an event in the first sequence does not map to any event in the second sequence; similarly insertions correspond to the case where an event in the second sequence does not map to any event in the first sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.,5.2 Alignment using a Weighted Finite State Representation,[1.0],['The WFST composition operation allows the outputs of one WFST to be fed to the inputs of a second WFST or FSA.']
"Thus, we build our final machine by composing the three sub-machines as,
D = N1 ◦M i12 ◦N2.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
(1) where i = c or i = c + t.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This gives us a combined weighted graph by mapping the output symbols of the first medical event sequence to the input symbols of the second medical event sequence.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t.
In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.",5.2 Alignment using a Weighted Finite State Representation,[1.0000000508655145],"['The scores on the decoding graph are derived from only the coreference probabilities if i = c and both coreference and temporal relation probabilities if i = c+ t. In the medical event sequence alignment problem, we want to align multiple sequences of medical events that correspond to multiple clinical narratives of a patient.']"
"Since we want to now combine
all narrative chains belonging to the same patient, the composition cascade to build the final combined sequence will be as,
Df = N1◦M i12◦N2◦M i23◦N3◦M i34...◦",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Nn (2)
where i = c",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
or i = c + t and n is the number of medical event sequences corresponding to clinical narratives for a patient.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"During composition we retain intermediate paths like M i23 utilizing the ability to do lazy composition (Mohri and Pereira, 1998) in order to facilitate beam search through the multi-alignment.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.,5.2 Alignment using a Weighted Finite State Representation,[1.0],['The best hypothesis corresponds to the highest scoring path which can be obtained using shortest path algorithms like Djikstra’s algorithm.']
"The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.
",5.2 Alignment using a Weighted Finite State Representation,[1.0000000563333145],['The best path corresponds to the best alignment across all medical event sequences based on the joint probability of cross-narrative medical event coreferences and temporal relations across the narrative sequences.']
"The complexity of decoding increases exponentially with the number of narrative sequences in
the composition, and exact decoding becomes infeasible.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.",5.2 Alignment using a Weighted Finite State Representation,[1.0],"['One solution to this problem is to do the alignment greedily pairwise, starting from the most recent medical event sequences, finding the best path, and iteratively moving on to the next sequence, and proceeding until the oldest medical event sequence.']"
"The disadvantage of such a method is that it does not take into account constraints between medical events across multiple event sequences and may lead to a less accurate solution.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
An alternative method is to use lazy composition to perform more efficient composition as it allows practical memory usage.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use beam search to make for an efficient approximation to the best-path computation (Mohri et al., 2005).",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
This allows accommodating constraints from across multiple sequences and generates a more accurate best path.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Thus, this method generates more accurate alignments when we have more than two sequences to be aligned.
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"For instance, instance say a, b ∈ N1, x, y ∈ N2, and m,",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"n ∈ N3 are temporally medical event sequences corresponding to narratives N1, N2 and N3.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Based on the learned pairwise temporal relations, if we have the following constraints a < x, m > x, m < a.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Aligning N1 and N2 greedily pairwise may give us the best combined sequence as a, x, b, y ∈ N12.",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"Now in aligning N12 with N3, we won’t be able to accommodate m > x",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
and m < a.,5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.",5.2 Alignment using a Weighted Finite State Representation,[1.0],"['However, performing a beam search over the composed WFST in equation 2 allows us to accommodate such constraints across multiple sequences.']"
"The complexity of composing two transducers is O(V1V2D1(logD2 + M2)) where each edge from the first sequence matches every edge in the second sequence and Vi is the number of states, Di is the maximum out-degree and Mi maximum multiplicity for the ith FST (Mohri et al., 2005).
",5.2 Alignment using a Weighted Finite State Representation,[0],[0]
"We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.",5.2 Alignment using a Weighted Finite State Representation,[1.0],"['We also use popular dynamic programming algorithms (Needleman et al., 1970, Smith and Waterman, 1981) for sequence alignment of medical events across narratives and compare it to the WFST-based representation and decoding.']"
"As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).",5.3 Pairwise Alignment using Dynamic Programming,[1.0],"['As a contrast, we adapt two dynamic programming algorithms for sequence alignment: global alignment using the Needleman Wunsch algorithm (NW) (Needleman et al., 1970) and local alignment using the Smith-Waterman algorithm (SW) (Smith and Waterman, 1981).']"
NW allows us to align all events in one sequence with all events in another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
A drawback of NW is that short and highly similar sequences maybe missed because they get overweighted by the rest of the sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
NW is suitable when the two sequences are of similar length with significant degree of similarity throughout.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"On the other hand, SW gives the longest sub-sequence pair that yields maximum degree of similarity between the two original sequences.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
It does not force all events in a sequence to align with another sequence.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
SW is useful in aligning sequences that differ in length and have short patches of similarity.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of these methods for sequences of length m and n are O(mn).
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The scoring scheme described earlier is used to update the scoring matrix for dynamic programming.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In order to accommodate the temporal relations before and after, we insert a null symbol after every medical event in each sequence in the scoring matrix.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"A vertical or horizontal gap arises when cases 1, 2, 3 and 4 in Section 5.1 mentioned
above are not true.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"If the medical events are not simultaneous, not before or not after, the medical events will not align.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, the value of each cell in the scoring matrix is determined by computing the maximum score at each position C(i, j) as,
max{(C(i−1, j−1)+Sij), (C(i, j−1)+w), (C(i− 1, j) + w)} (3)
where, Sij = max{P (i = j), P (i < j), P (i > j)}, and w = max{(1",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"− P (i = j)), (1 − P (i < j)), (1 − P (i > j))}.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Here, C(i − 1, j − 1) corresponds to a match, whereas C(i, j − 1) and C(i − 1, j) correspond to a gaps in sequence one and two.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"In case of the SW algorithm, the negative scoring matrix cells are set to zero, thus making the positively scoring local alignments visible.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Backtracking starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, yielding the highest scoring local alignment.
",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
The time and space complexity grows exponentially with the number of sequences to be aligned and finding the global optimum has been shown to be a NP-complete problem.,5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"The time complexity of aligning N sequences of length L is O(2NLN ) (Wang and Jiang, 1994).",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
"Thus, for MSA using dynamic programming, we use a heuristic method where we combine pairwise alignments iteratively starting with the latest narrative and progressing towards the oldest narrative.",5.3 Pairwise Alignment using Dynamic Programming,[0],[0]
Corpus Description.,6 Experiments and Evaluation,[0],[0]
The corpus consists of a dataset of clinical narratives obtained from the [redacted] medical center.,6 Experiments and Evaluation,[0],[0]
"The corpus has a total of 2060 patients, and 100704 clinical narratives.",6 Experiments and Evaluation,[0],[0]
"We gathered a gold standard set of seven patients (80 clinical narratives overall) with manual annotation of all medical events mentioned in the narratives, coreferences, and medical event sequence information.",6 Experiments and Evaluation,[0],[0]
"The annotation agreement across annotators is high, with 89.5% agreement corresponding to inter-annotator Cohen’s kappa statistic of 0.86 (Raghavan et al., 2012b).",6 Experiments and Evaluation,[0],[0]
"The types of clinical narratives included 27 discharge summaries, 30 history and physical reports, 15 radiology reports and 8 pathology reports.",6 Experiments and Evaluation,[0],[0]
The distribution of the number of medical event sequences and unique medical events across patients is shown in Table 1.,6 Experiments and Evaluation,[0],[0]
"The annotated dataset is used to crossvalidate and train our coreference and temporal relation learning models and to evaluate our crossnarrative medical event timeline.
",6 Experiments and Evaluation,[0],[0]
Evaluation Metric.,6 Experiments and Evaluation,[0],[0]
"For each patient and each method (WFST or dynamic programming), the output timeline to evaluate is the highest scoring candidate hypothesis derived as described above.",6 Experiments and Evaluation,[0],[0]
Accuracy of the timeline is calculated as the number of transformations required to obtain the reference sequence in the annotated gold-standard from the one generated by our system.,6 Experiments and Evaluation,[0],[0]
"Transformations are measured in terms of the minimum edit distance, insertions, deletions, and substitutions of medical events.
",6 Experiments and Evaluation,[0],[0]
Experiments and Results.,6 Experiments and Evaluation,[0],[0]
"We first temporally order medical events within each clinical narrative by learning to rank them in relative order of occurence as described in our previous work (Raghavan et al., 2012c).",6 Experiments and Evaluation,[0],[0]
The overall accuracy of ranking medical events using leave-one-out cross validation is 82.1%.,6 Experiments and Evaluation,[0],[0]
"The resulting medical event sequences serve as the input to the problem of crossnarrative sequence alignment.
",6 Experiments and Evaluation,[0],[0]
The cross-narrative coreference and temporal relation pairwise classification models described in Section 4 are trained using a Maximum entropy classifier.,6 Experiments and Evaluation,[0],[0]
The coreference resolution performs with 71.5% precision and 82.3% recall.,6 Experiments and Evaluation,[0],[0]
The temporal relation classifier performs with 60.2% precision and 76.3% recall.,6 Experiments and Evaluation,[0],[0]
"The learned pairwise coreference and temporal relation probabilities are now used to derive the score for the WFST and dynamic programming approaches.
WFST representation and decoding.",6 Experiments and Evaluation,[0],[0]
We build finite-state machines using the open source OpenFST,6 Experiments and Evaluation,[0],[0]
library.2,6 Experiments and Evaluation,[0],[0]
We use a tropical semi-ring weighted using the negative log-likelihood of the computed scores.,6 Experiments and Evaluation,[0],[0]
"OpenFST provides tools that can search for the highest scoring sequences accepted by the machine, and can sample from highscoring sequences probabilistically, by treating the
2www.openfst.org
scores of each transition within the machine as a negative log probability.",6 Experiments and Evaluation,[0],[0]
The decoding process to compute the most likely combined medical event sequence can be defined as searching for the best path in the combined graph representation (Equation 2).,6 Experiments and Evaluation,[0],[0]
The best path is the one that minimizes the total weight on a path (since the arcs are negative log probabilities).,6 Experiments and Evaluation,[0],[0]
"In searching for the best path, the beam size is set to 5.",6 Experiments and Evaluation,[0],[0]
"The accuracy of the WFST-based representation and beam search across all sequences using the coreference and temporal relation scores to obtain the combined aligned sequence is 78.9%.
",6 Experiments and Evaluation,[0],[0]
Dynamic Programming.,6 Experiments and Evaluation,[0],[0]
We use the NW and SW algorithms described in Section 5.3 to produce local and global alignments respectively.,6 Experiments and Evaluation,[0],[0]
We use the scoring scheme described in Section 5.1 to update the cost matrix for dynamic programming and implement the algorithms as described in Section 5.3.,6 Experiments and Evaluation,[0],[0]
The overall accuracy of sequence alignment with both coreference and temporal relation scores using NW is 68.7% whereas SW gives an accuracy of 72.1%.,6 Experiments and Evaluation,[0],[0]
"In case of aligning just two sequences, both methods yield the same results.",6 Experiments and Evaluation,[0],[0]
"The accuracy of cross-narrative MSA for each patient, for each method, using cross validation, is shown in Table 1.",6 Experiments and Evaluation,[0],[0]
Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).,6 Experiments and Evaluation,[1.0],['Results indicate that the WFSTbased method outperforms the dynamic programming approach for multi-sequence alignment (statistical significance p<0.05).']
"Morever, the results using both coreference and temporal realtion scores for alignment outperform using only coreference scores for alignment using all approaches.",6 Experiments and Evaluation,[0],[0]
This indicates that cross-narrative temporal relations are important for accurately aligning medical event sequences across narratives.,6 Experiments and Evaluation,[0],[0]
"We propose and evaluate different approaches to multiple sequence alignment of medical events.
",7 Discussion,[0],[0]
Approaches to multi-alignment.,7 Discussion,[0],[0]
We address the problem of aligning medical event sequences using a novel WFST-based framework and empirically demonstrate that it outperforms pairwise progressive alignment using dynamic programming.,7 Discussion,[0],[0]
"This is mainly because the WFST-based allows us to consider temporal constraints from across multiple sequences when performing the alignment.
",7 Discussion,[0],[0]
"Moreover, it also outperforms the integer linear programming (ILP) method for timeline construction proposed in (Do et al., 2012).",7 Discussion,[0],[0]
We implemented the proposed method that also allows combining the output of classifiers subject to some constraints.,7 Discussion,[0],[0]
We derive intervals from event starts and stops and learn two perceptron classifiers for classifying the temporal relations between events and assigning events to intervals.,7 Discussion,[0],[0]
The classifier probabilities are then used to solve the optimization problem using the lpsolve solver.3,7 Discussion,[0],[0]
We also use intra-document coreference information to resolve coreference before performing the global optimization.,7 Discussion,[0],[0]
"We observe that in case of MSA, the optimal solution using ILP is still intractable as the number of constraints increases exponentially with the number of sequences.",7 Discussion,[0],[0]
Aligning pairwise iteratively gives us an overall average accuracy of 68.2% similar to dynamic programming.,7 Discussion,[0],[0]
"While this is comparable to the dynamic programming performance, the WFST-based method significantly outperforms this in case of multialignments for cross-narrative temporal ordering.
",7 Discussion,[0],[0]
Performance and error analysis.,7 Discussion,[0],[0]
"We perform multi-alignments over medical event sequences for a patient, where each sequence corresponds to temporally ordered medical events in a clinical narrative generated using the ranking model described in (Raghavan et al., 2012c).",7 Discussion,[0],[0]
The accuracy of intra-narrative temporal ordering is 82.1%.,7 Discussion,[1.0],['The accuracy of intra-narrative temporal ordering is 82.1%.']
The errors in performing this intra-narrative ordering may propagate to the cross-narrative model resulting in reduced accuracy.,7 Discussion,[0],[0]
"This may be addressed by considering n-best temporally ordered medical event sequences, generated by the ranking process, and aligning the n-best sequences using the WFST-based framework.",7 Discussion,[0],[0]
"This could be feasible as, practically, the WFST-based method for multialignment takes only a few secs to align a pair of medical event sequences with average length 40.
",7 Discussion,[0],[0]
The accuracy of alignments across multiple medical event sequences is also affected by the error induced by the coreference and temporal relation scores.,7 Discussion,[0],[0]
"Often, insufficient temporal cues leads
3http://lpsolve.sourceforge.net/5.5/
to misclassification of events incorrectly as sharing the “simultaneous” temporal relation and often as coreferring.",7 Discussion,[0],[0]
This induces errors in the score calculation and hence the alignments.,7 Discussion,[0],[0]
"Better methods to address the challenging problem of crossdocument temporal relation learning, perhaps with the help of structured data from the patient record, could improve the accuracy of alignments.
",7 Discussion,[0],[0]
"There is no clear trend with respect to the number of medical events and narratives for a patient (Table 1.), and the alignment accuracy.",7 Discussion,[0],[0]
"In future work, it would be interesting to examine any such correlation and also study the scalability of the WFST-based method for sequence alignment on longer medical event sequences and a larger dataset of patients.",7 Discussion,[0],[0]
"Further, the WFST-based method may be used to model multi-alignment tasks in other speech and language problems as well.",7 Discussion,[0],[0]
We propose a novel framework for aligning medical event sequences across clinical narratives based on coreference and temporal relation information using cascaded WFSTs.,8 Conclusion,[0],[0]
FSTs provide a convenient and flexible framework to model sequences of temporally ordered medical events and compose them into a combined graph representation.,8 Conclusion,[0],[0]
Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.,8 Conclusion,[1.0],['Decoding this graph allows us to jointly maximize coreference as well as temporal relation probabilities to derive a timeline of the most likely temporal ordering of medical events.']
This approach to aligning multiple sequences of medical events significantly outperforms other approaches such as dynamic programming.,8 Conclusion,[0],[0]
"Moreover, we demonstrate the importance of learning temporal relations for the task timeline generation from across multiple clinical narratives by empirically proving that decoding using both coreference and temporal relation scores is far more accurate than decoding with only coreference scores.",8 Conclusion,[0],[0]
The project was supported by Award Number Grant R01LM011116 from the National Library of Medicine.,Acknowledgments,[0],[0]
The content is solely the responsibility of the authors and does not necessarily represent the official views of the National Library of Medicine or the National Institutes of Health.,Acknowledgments,[0],[0]
The authors would like to thank Yanzhang He for his input on the WFST-based model.,Acknowledgments,[0],[0]
Cross-narrative temporal ordering of medical events is essential to the task of generating a comprehensive timeline over a patient’s history.,abstractText,[0],[0]
"We address the problem of aligning multiple medical event sequences, corresponding to different clinical narratives, comparing the following approaches: (1) A novel weighted finite state transducer representation of medical event sequences that enables composition and search for decoding, and (2) Dynamic programming with iterative pairwise alignment of multiple sequences using global and local alignment algorithms.",abstractText,[0],[0]
The cross-narrative coreference and temporal relation weights used in both these approaches are learned from a corpus of clinical narratives.,abstractText,[0],[0]
We present results using both approaches and observe that the finite state transducer approach performs performs significantly better than the dynamic programming one by 6.8% for the problem of multiple-sequence alignment.,abstractText,[0],[0]
Cross-narrative temporal ordering of medical events,title,[0],[0]
