{"sections": [{"text": "\u03b2\u0302 := arg min \u03b2\n\u2211n j=1`(x > j \u03b2; yj) + \u03bbR(\u03b2), (1)\nwhere xi \u2208 Rp and yi \u2208 R denote the ith feature and response variable respectively. Let ` and R be the loss function and regularizer, \u03b2 denote the unknown weights, and \u03bb be a regularization parameter. Finding the optimal choice of \u03bb is a challenging problem in high-dimensional regimes where both n and p are large. We propose two frameworks to obtain a computationally efficient approximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our two frameworks are based on the primal and dual formulations of (1). We prove the equivalence of the two approaches under smoothness conditions. This equivalence enables us to justify the accuracy of both methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization, and support vector machines. We empirically demonstrate the effectiveness of our results for non-differentiable cases."}, {"heading": "1. Introduction", "text": ""}, {"heading": "1.1. Motivation", "text": "Consider a standard prediction problem in which a dataset {(yj ,xj)}nj=1 \u2282 R\u00d7 Rp is employed to learn a model for inferring information about new datapoints that are yet to be observed. One of the most popular classes of learning\n*Equal contribution 1Department of Statistics, Columbia University, New York, USA 2Mathematics Department and Operation Research Center, Massachusetts Institute of Technology, Massachusetts, USA 3Google Research, New York, USA. Correspondence to: Shuaiwen Wang <sw2853@columbia.edu>, Wenda Zhou <wz2335@columbia.edu>.\nProceedings of the 35 th International Conference on Machine Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 by the author(s).\nschemes, especially in high-dimensional settings, studies the following optimization problem:\n\u03b2\u0302 := arg min \u03b2 n\u2211 j=1 `(x>j \u03b2; yj) + \u03bbR(\u03b2), (2)\nwhere ` : R2 \u2192 R is the loss function, R : Rp \u2192 R is the regularizer, and \u03bb is the tuning parameter that specifies the amount of regularization. By applying an appropriate regularizer in (2), we are able to achieve better bias-variance trade-off and pursue special structures such as sparsity and low rank structure. However, the performance of such techniques hinges upon the selection of tuning parameters.\nThe most generally applicable tuning method is cross validation (Stone, 1974). One common choice is k-fold cross validation, which however presents potential bias issues in high-dimensional settings where n is comparable to p. For instance, the phase transition phenomena that happen in such regimes (Amelunxen et al., 2014; Donoho et al., 2009; Donoho & Tanner, 2005) indicate that any data splitting may cause dramatic effects on the solution of (2) (see Figure 1 for an example). Hence, the risk estimates obtained from k-fold cross validation may not be reliable. The bias issues of k-fold cross validation may be alleviated by choosing the number of folds k to be large. However, such schemes are computationally demanding and may not be useful for emerging high-dimensional applications. An alternative choice of cross validation is LOOCV, which is unbiased in high-dimensional problems. However, the computation of LOOCV requires training the model n times, which is unaffordable for large datasets.\nThe high computational complexity of LOOCV has motivated researchers to propose computationally less demanding approximations of the quantity. Early examples offered approximations for the case R(\u03b2) = 12\u2016\u03b2\u2016 2 2 and the loss function being smooth (Allen, 1974; O\u2019sullivan et al., 1986; Le Cessie & Van Houwelingen, 1992; Cawley & Talbot, 2008; Meijer & Goeman, 2013; Opper & Winther, 2000). In (Beirami et al., 2017), the authors considered such approximations for smooth loss functions and smooth regularizers. In this line of work, the accuracy of the approximations was either not studied or was only studied in the n large, p fixed regime. In a recent paper, (Rad & Maleki, 2018) employed a similar approximation strategy to obtain approx-\nimate leave-one-out formulas for smooth loss functions and smooth regularizers. They show that under some mild conditions, such approximations are accurate in high-dimensional settings. Unfortunately, the approximations offered in (Rad & Maleki, 2018) only cover twice differentiable loss functions and regularizers. On the other hand, numerous modern regularizers, such as generalized LASSO and nuclear norm, and also many loss functions are not smooth.\nIn this paper, we propose two powerful frameworks for calculating an approximate leave-one-out estimator (ALO) of the LOOCV risk that are capable of offering accurate parameter tuning even for non-differentiable losses and regularizers. Our first approach is based on the smoothing and quadratic approximation of the primal problem (2). The second approach is based on the approximation of the dual of (2). While the two approaches consider different approximations that happen in different domains, we will show that when both ` and r are twice differentiable, the two frameworks produce the same ALO formulas, which are also the same as the formulas proposed in (Rad & Maleki, 2018).\nWe use our platforms to obtain concise formulas for several popular examples including generalized LASSO, support vector machine (SVM) and nuclear norm minimization. As will be clear from our examples, despite of the equivalence of the two frameworks for smooth loss functions and regularizers, the technical aspects of the derivations involved for obtaining ALO formulas have major variations in different examples. Finally, we present extensive simulations to confirm the accuracy of our formulas on various important machine learning models. Code is available at github.com/wendazhou/alocv-package."}, {"heading": "1.2. Other Related Work", "text": "The importance of parameter tuning in learning systems has encouraged many researchers to study the problem from different perspectives. In addition to cross validation, other approaches have been proposed including Stein\u2019s unbiased risk estimate (SURE), Akaike information criterion (AIC), and Mallow\u2019s Cp. While AIC is designed for smooth para-\nmetric models, SURE has been extended to emerging optimization problems, such as generalized LASSO and nuclear norm minimization (Candes et al., 2013; Dossal et al., 2013; Tibshirani et al., 2012; Vaiter et al., 2017; Zou et al., 2007).\nUnlike cross validation which approximates the out-ofsample prediction error, SURE, AIC, and Cp offer estimates for in-sample prediction error (Hastie et al., 2009). This makes cross validation more appealing for many learning systems. Furthermore, unlike ALO, both SURE andCp only work on linear models (and not generalized linear models) and their unbiasedness is only guaranteed under the Gaussian model for the errors. There has been little success in extending SURE beyond this model (Efron, 2004).\nAnother class of parameter tuning schemes are based on approximate message passing (Bayati et al., 2013; Mousavi et al., 2017; Obuchi & Kabashima, 2016). As pointed out in (Obuchi & Kabashima, 2016), this approach is intuitively related to LOOCV. It offers consistent parameter tuning in high-dimensions (Mousavi et al., 2017), but the results strongly depend on the independence of the elements ofX ."}, {"heading": "1.3. Notation", "text": "Lowercase and uppercase bold letters denote vectors and matrices, respectively. For subsets A \u2282 {1, 2, . . . , n} and B \u2282 {1, 2, . . . , p} of indices and a matrixX , letXA,\u00b7 and X\u00b7,B denote the submatrices that include only rows of X in A, and columns of X in B respectively. Let {ai}i\u2208S denote the vector whose components are ai for i \u2208 S. We may omit S, in which case we consider all indices valid in the context. For a function f : R \u2192 R, let f\u0307 , f\u0308 denote its 1st and 2nd derivatives. For a vector a, we use diag[a] to denote a diagonal matrix A with Aii = ai. Finally, let \u2207R and\u22072R denote the gradient and Hessian of a function R : Rp \u2192 R."}, {"heading": "2. Preliminaries", "text": ""}, {"heading": "2.1. Problem Description", "text": "In this paper, we study the statistical learning models in form (2). For each value of \u03bb, we evaluate the following LOOCV risk estimate with respect to some error function d:\nloo\u03bb := 1\nn n\u2211 i=1 d(yi,x > i \u03b2\u0302 /i), (3)\nwhere \u03b2\u0302/i is the solution of the leave-i-out problem\n\u03b2\u0302/i := arg min \u03b2 \u2211 j 6=i `(x>j \u03b2; yj) + \u03bbR(\u03b2). (4)\nCalculating (4) requires training the model n times, which may be time-consuming in high-dimensions. As an alternative, we propose an estimator \u03b2\u0303/i to approximate \u03b2\u0302/i based\non the full-data estimator \u03b2\u0302 to reduce the computational complexity. We consider two frameworks for obtaining \u03b2\u0303/i, and denote the corresponding risk estimate by:\nalo\u03bb := 1\nn n\u2211 i=1 d(yi,x > i \u03b2\u0303 /i). (5)\nThe estimates we obtain will be called approximated leaveone-out (ALO) throughout the paper."}, {"heading": "2.2. Primal and Dual Correspondence", "text": "The objective function of penalized regression problem with loss ` and regularizer R is given by:\nP (\u03b2) := n\u2211 j=1 `(x>j \u03b2; yj) +R(\u03b2). (6)\nHere and subsequently, we absorb the value of \u03bb into R to simplify the notation. We also consider the Lagrangian dual problem, which can be written in the form:\nmin \u03b8\u2208Rn D(\u03b8) := n\u2211 j=1 `\u2217(\u2212\u03b8j ; yj) +R\u2217(X>\u03b8), (7)\nwhere `\u2217 and R\u2217 denote the Fenchel conjugates 1 of ` and R respectively. See the derivation in Appendix A.\nIt is known that under mild conditions, (6) and (7) are equivalent (Boyd & Vandenberghe, 2004). In this case, we have the primal-dual correspondence relating the primal optimal \u03b2\u0302 and the dual optimal \u03b8\u0302:\n\u03b2\u0302 \u2208 \u2202R\u2217(X>\u03b8\u0302), X>\u03b8\u0302 \u2208 \u2202R(\u03b2\u0302),\nx>j \u03b2\u0302 \u2208 \u2202`\u2217(\u2212\u03b8\u0302j ; yj), \u2212\u03b8\u0302j \u2208 \u2202`(x>j \u03b2\u0302; yj), (8)\nwhere \u2202f denotes the set of subgradients of a function f . Below we will use both primal and dual perspectives for approximating loo\u03bb."}, {"heading": "3. Approximation in the Dual Domain", "text": ""}, {"heading": "3.1. The First Example: LASSO", "text": "Let us first start with a simple example that illustrates our dual method in deriving an approximate leave-one-out (ALO) formula for the standard LASSO. The LASSO estimator, first proposed in (Tibshirani, 1996), can be formulated as the penalized regression framework in (6) by setting `(\u00b5; y) = (\u00b5\u2212 y)2/2, and R(\u03b2) = \u03bb\u2016\u03b2\u20161.\nWe recall the general formulation of the dual for penalized regression problems (7), and note that in the case of the\n1The Fenchel conjugate f\u2217 of a function f is defined as f\u2217(x) := supy{\u3008x, y\u3009 \u2212 f(y)}.\nLASSO we have:\n`\u2217(\u03b8i; yi) = 1\n2 (\u03b8i\u2212yi)2, R\u2217(\u03b2) = { 0 if \u2016\u03b2\u2016\u221e \u2264 \u03bb, +\u221e otherwise.\nIn particular, we note that the solution of the dual problem (7) can be obtained from:\n\u03b8\u0302 = \u03a0\u2206X (y). (9)\nHere \u03a0\u2206X denotes the projection onto \u2206X , where \u2206X is the polytope given by:\n\u2206X = {\u03b8 \u2208 Rn : \u2016X>\u03b8\u2016\u221e \u2264 \u03bb}.\nLet us now consider the leave-i-out problem. Unfortunately, the dimension of the dual problem is reduced by 1 for the leave-i-out problem, making it difficult to leverage the information from the full-data solution to approximate the leave-i-out solution. We augment the leave-i-out problem with a virtual ith observation that does not affect the result of the optimization, but restores the dimensionality of the problem.\nMore precisely, let ya be the same as y, except that its ith coordinate is replaced by y\u0302/ii = x > i \u03b2\u0302\n/i, the leave-i-out predicted value. We note that the leave-i-out solution \u03b2\u0302/i is also the solution for the following augmented problem:\nmin \u03b2\u2208Rp n\u2211 j=1 `(x>j \u03b2; ya,j) +R(\u03b2). (10)\nLet \u03b8\u0302/i be the corresponding dual solution of (10). Then, by (9), we know that\n\u03b8\u0302/i = \u03a0\u2206X (ya).\nAdditionally, the primal-dual correspondence (8) gives that \u03b8\u0302/i = ya \u2212X\u03b2\u0302/i, which is the residual in the augmented problem, and hence that \u03b8\u0302/ii = 0. These two features allow us to characterize the leave-i-out predicted value y\u0302/ii :\ne>i \u03a0\u2206X ( y \u2212 (yi \u2212 y\u0302/ii )ei ) = 0 (11)\nwhere ei denotes the ith standard vector. Solving exactly for the above equation is in general a procedure that is computationally comparable to fitting the model, which may be expensive. However, we may attempt to obtain an approximate solution of (11) by linearizing the projection operator at the full data solution \u03b8\u0302, or equivalently performing a single Newton step to solve the leave-i-out problem from the full data solution. The approximate leave-i-out fitted value y\u0303 /i i is thus given by:\ny\u0303 /i i = yi \u2212 \u03b8\u0302i Jii , (12)\nwhere J denotes the Jacobian of the projection operator \u03a0\u2206X at the full data problem y. We can substitute the x>i \u03b2\u0303\n/i in (5) with the y\u0303/i found above to obtain our ALO risk estimates. Note that \u2206X is a polytope, and thus the projection onto \u2206X is almost everywhere locally affine (Tibshirani et al., 2012). Furthermore, it is straightforward to calculate the Jacobian of \u03a0\u2206X . Let E = {j : |X>j \u03b8\u0302| = \u03bb} be the equicorrelation set (where Xj denotes the jth column of X). Then the projection at the full data problem y is locally given by a projection onto the orthogonal complement of the span of X\u00b7,E , thus giving J = I\u2212X\u00b7,E(X>\u00b7,EX\u00b7,E)\u22121X>\u00b7,E . We can then obtain y\u0303/i by plugging J in (12). Finally, by replaceing x>i \u03b2\u0303 /i with y\u0303 /i i in (5) we obtain an estimate of the risk."}, {"heading": "3.2. General Case", "text": "In this section we extend the dual approach outlined in Section 3.1 to more general loss functions and regularizers.\nGeneral regularizers Let us first extend the dual approach to other regularizers, while the loss function remains `(\u00b5, y) = (\u00b5\u2212 y)2/2. In this case the dual problem (7) has the following form:\nmin \u03b8\n1\n2 n\u2211 j=1 (\u03b8j \u2212 yj)2 +R\u2217(X>\u03b8). (13)\nWe note that the optimal value of \u03b8 is by definition the value of the proximal operator of R\u2217(X>\u00b7) at y:\n\u03b8\u0302 = proxR\u2217(X>\u00b7)(y).\nFollowing the argument of Section 3.1, we obtain\ny\u0303 /i i = yi \u2212 \u03b8\u0302i Jii , (14)\nwith J now denoting the Jacobian of proxR\u2217(X>\u00b7). We note that the Jacobian matrix J exists almost everywhere, because the non-expansiveness of the proximal operator guarantees its almost-everywhere differentiability (Combettes & Pesquet, 2011). In particular, if y has distribution which is absolutely continuous with respect to the Lebesgue measure, J exists with probability 1. This approach is particularly useful when R is a norm, as its Fenchel conjugate is then the convex indicator of the unit ball of the dual norm, and the proximal operator reduces to a projection operator.\nGeneral smooth loss Let us now assume we have a convex smooth loss in (6), such as those that appear in generalized linear models. As we are arguing from a secondorder perspective by considering Newton\u2019s method, we will expand the loss as a quadratic form around the full data\nsolution. We will thus consider the approximate problem obtained by expanding `\u2217 around the dual optimal \u03b8\u0302:\nmin \u03b8\n1\n2 n\u2211 j=1 \u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj)\n( \u03b8j\u2212\u03b8\u0302j\u2212\n\u02d9\u0300\u2217(\u2212\u03b8\u0302j ; yj) \u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj)\n)2 +R\u2217(X>\u03b8).\n(15)\nThe constant term has been removed from (15) for simplicity. We have reduced the problem to that of a weighted `2 loss which may be further reduced to a simple `2 problem by a change of variable and a rescaling of X . Indeed, let K\nbe the diagonal matrix such that Kjj = \u221a\n\u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj), and note that we have: \u02d9\u0300\u2217(\u2212\u03b8\u0302j ; yj) = x>j \u03b2\u0302 := y\u0302j by the primaldual correspondence (8). Consider the change of variable u = K\u03b8 to obtain:\nmin u\n1\n2 n\u2211 j=1 uj \u2212 \u03b8\u0302j \u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj) + y\u0302j\u221a \u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj) 2+R\u2217(X>K\u22121u). We may thus reduce to the `2 loss case in (13) with a modifiedX and y:\nXu = K \u22121X, yu =  \u03b8\u0302j \u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj) + y\u0302j\u221a\u00a8\u0300\u2217(\u2212\u03b8\u0302j ; yj)  j . (16)\nSimilar to (14), the ALO formula in the case of general smooth loss can be obtained as y\u0303/ii = Kiiy\u0303 /i u,i, with\ny\u0303 /i u,i = yu,i \u2212 Kii\u03b8\u0302i Jii , (17)\nwhere J is the Jacobian of proxR\u2217(X>u \u00b7)."}, {"heading": "4. Approximation in the Primal Domain", "text": ""}, {"heading": "4.1. Smooth Loss and Regularizer", "text": "To obtain loo\u03bb we need to solve\n\u03b2\u0302/i := arg min \u03b2 \u2211 j 6=i `(x>j \u03b2; yj) +R(\u03b2). (18)\nAssuming \u03b2\u0302/i is close to \u03b2\u0302, we can take a Newton step from \u03b2\u0302 towards \u03b2\u0302/i to obtain its approximation \u03b2\u0303/i as:\n\u03b2\u0303/i = \u03b2\u0302+ [\u2211 j 6=i xjx > j \u00a8\u0300(x>j \u03b2\u0302; yj)+\u22072R(\u03b2\u0302) ]\u22121 xi \u02d9\u0300(x > i \u03b2\u0302; yi).\n(19)\nWe have by the matrix inversion lemma (Hager, 1989):\nx>i \u03b2\u0303 /i = x>i \u03b2\u0302 +\nHii\n1\u2212Hii \u00a8\u0300(x>i \u03b2\u0302; yi) \u02d9\u0300(x>i \u03b2\u0302; yi), (20)\nH = X[X>diag[{\u00a8\u0300(x>i \u03b2\u0302; yi)}i]X +\u22072R(\u03b2\u0302)]\u22121X>. (21)\nThis is the formula reported in (Rad & Maleki, 2018). By calculating \u03b2\u0302 and H in advance, we can cheaply approximate the leave-i-out prediction for all i and efficiently evaluate the LOOCV risk. On the other hand, in order to use the above strategy, twice differentiability of both the loss and the regularizer is necessary in a neighborhood of \u03b2\u0302. However, this assumption is violated for many machine learning models including LASSO, Nuclear norm, and SVM. In the next two sections, we introduce a smoothing technique which lifts the scope of the above primal approach to nondifferentiable losses and regularizers."}, {"heading": "4.2. Nonsmooth Loss and Smooth Regularizer", "text": "In this section we study the piecewise smooth loss functions and twice differentiable regularizers. Such problems arise in SVM (Cortes & Vapnik, 1995) and robust regression (Huber, 1973). Before proceeding further, we clarify our assumptions on the loss function.\nDefinition 4.1. A singular point of a function is called qth order, if at this point the function is q times differentiable, but its (q + 1)th order derivative does not exist.\nBelow we assume the loss ` is piecewise twice differentiable with k zero-order singularities v1, . . . , vk \u2208 R. The existence of singularities prohibits us from directly applying strategies in (19) and (20), where twice differentiability of ` and R is necessary. A natural solution is to first smooth the loss `, then apply the framework in Section 4.1 to the smoothed version and finally reduce the smoothness to recover the ALO formula for the original nonsmooth problem.\nAs the first step, consider the following smoothing idea:\n`h(\u00b5; y) =: 1\nh\n\u222b `(u; y)\u03c6((\u00b5\u2212 u)/h)du,\nwhere h > 0 is fixed and \u03c6 is a smooth symmetric function with the following properties:\nNormalization: \u222b \u03c6(w)dw = 1, \u03c6(w) \u2265 0, \u03c6(0) > 0;\nCompact support: supp(\u03c6) = [\u2212C,C] for some C > 0.\nNow plug in this smooth version `h into (18) to obtain the following formula from (19):\nGh := \u2211 j 6=i xjx > j \u00a8\u0300 h(x > j \u03b2\u0302h; yj) +\u22072R(\u03b2\u0302h),\n\u03b2\u0303 /i h := \u03b2\u0302h +G \u22121 h xi \u02d9\u0300 h(x > i \u03b2\u0302h; yi).\n(22)\nwhere \u03b2\u0302h is the minimizer on the full data from loss `h and R. \u03b2\u0303/ih is a good approximation to the leave-i-out estimator \u03b2\u0302/ih based on smoothed loss `h. Setting h \u2192 0, we have `h(\u00b5, y) converge to `(\u00b5, y) uniformly in the region of interest (see Appendix C.1 for the proof), implying\nthat limh\u21920 \u03b2\u0303 /i h serves as a good estimator of limh\u21920 \u03b2\u0302 /i h , which is heuristically close to the true leave-i-out \u03b2\u0302/i. Equation (22) can be simplified in the limit h \u2192 0. We define the sets of indices V and S for the samples at singularities and smooth parts respectively:\nV := {j : x>j \u03b2\u0302 = vt for some t \u2208 {1, . . . , k}}, S := {1, . . . , n} \\ V.\nWe characterize the limit of x>i \u03b2\u0303 /i h below. Theorem 4.1. Under some mild conditions, as h\u2192 0,\nx>i \u03b2\u0303 /i h \u2192 x > i \u03b2\u0302 + aig`,i\nwhere\nai =  Wii 1\u2212Wii \u00a8\u0300(x>i \u03b2\u0302;yi) if i \u2208 S,\n1 [(XV \u00b7Y \u22121X>V \u00b7) \u22121]ii if i \u2208 V,\nY = \u22072R(\u03b2\u0302) +X>S\u00b7diag[{\u00a8\u0300(x>j \u03b2\u0302)}j\u2208S ]XS\u00b7, Wii = x > i Y \u22121xi \u2212 x>i Y \u22121X>V,\u00b7(XV,\u00b7Y \u22121X>V,\u00b7)\u22121XV,\u00b7Y \u22121xi.\nFor i \u2208 S, g`,i = \u02d9\u0300(x>i \u03b2\u0302; yi), and for i \u2208 V , we have:\ng`,V = (XV,\u00b7X > V,\u00b7) \u22121XV,\u00b7[\u2207R(\u03b2\u0302)\u2212 \u2211 j\u2208S xj \u02d9\u0300(x > j \u03b2\u0302; yj)].\nWe can obtain the ALO estimate of prediction error by plugging x>i \u03b2\u0302 + aig`,i instead of x > i \u03b2\u0303\n/i in (5). The conditions and proof of Theorem 4.1 can be found in Appendix C.3."}, {"heading": "4.3. Nonsmooth Regularizer and Smooth Loss", "text": "The smoothing technique proposed in the last section can also handle many nonsmooth regularizers. In this section we focus on separable regularizers R, defined as R(\u03b2) = \u2211p l=1 r(\u03b2l), where r : R \u2192 R is piecewise twice differentiable with finite number of zero-order singularities in v1, . . . , vk \u2208 R. (Examples on non-separable regularizers are studied in Section 6.) We further assume the loss function ` to be twice differentiable and denote by A = {l : \u03b2\u0302l 6= vt, for any t \u2208 {1, . . . , k}} the active set.\nFor the coordinates of \u03b2\u0302 that lie inA, our objective function, constrained to these coordinates, is locally twice differentiable. Hence we expect \u03b2\u0302/iA to be well approximated by the ALO formula using \u03b2\u0302A. On the other hand, components not in A are trapped at singularities. As long as they are not on the boundary of being in or out of the singularities, we expect these locations of \u03b2\u0302/i to stay at the same values.\nTechnically, consider a similar smoothing scheme for r:\nrh(w) = 1\nh\n\u222b r(u)\u03c6((w \u2212 u)/h)du,\nand let Rh(\u03b2) = \u2211p l=1 rh(\u03b2l). We then consider the ALO formula of Model (18) with regularizer Rh:\nGh := \u2211 j 6=i xjx > j \u00a8\u0300(x>j \u03b2\u0302h; yj) +\u22072Rh(\u03b2\u0302h),\n\u03b2\u0303 /i h := \u03b2\u0302h +G \u22121 h xi \u02d9\u0300 h(x > i \u03b2\u0302; yi).\n(23)\nSetting h\u2192 0, (23) reduces to a simplified formula which heuristically serves as a good approximation to the true leave-i-out estimator \u03b2\u0302/i, stated as the following theorem:\nTheorem 4.2. Under some mild conditions, as h\u2192 0,\nx>i \u03b2\u0303 /i h \u2192 x > i \u03b2\u0302 +\nHii \u02d9\u0300(x > i \u03b2\u0302; yi)\n1\u2212Hii \u00a8\u0300(x>i \u03b2\u0302; yi) ,\nwith\nH =X\u00b7,A[X > \u00b7,Adiag[{\u00a8\u0300(x>i \u03b2\u0302; yi)}i]X\u00b7,A+\u22072R(\u03b2\u0302A)]\u22121X>\u00b7,A.\nThe conditions and proof of Theorem 4.2 can be found in the Appendix C.2.\nRemark 4.1. For nonsmooth problems, higher order singularities do not cause issues: the set of tuning values which cause \u03b2\u0302l (for regularizer) or x>j \u03b2\u0302 (for loss) to fall at those higher order singularities has measure zero.\nRemark 4.2. For both nonsmooth losses and regularizers, we need to invert some matrices in the ALO formula. Although the invertibility does not seem guaranteed in the general formula, as we apply ALO to specific models, the structures of the loss and/or the regularizer ensures this invertibility. For example, for LASSO, we have that the size of the active set |E| \u2264 min(n, p). Remark 4.3. We note that the dual approach is typically powerful for models with smooth losses and norm-type regularizers, such as the SLOPE norm and the generalized LASSO. On the other hand, the primal approach is valuable for models with nonsmooth loss or when the Hessian of the regularizer is feasible to calculate. Such regularizers often exhibit some type of separability or symmetry, such as in the case of SVM or nuclear norm."}, {"heading": "5. Equivalence Between Primal and Dual Methods", "text": "Although the primal and dual methods may be harder or easier to carry out depending on the specific problem at hand, one may wonder if they always obtain the same result. In this section, we outline a unifying view for both methods, and state an equivalence theorem.\nAs both the primal and dual methods are based on a firstorder approximation strategy, we will study them not as approximate solutions to the leave-i-out problem, but will\ninstead show that they are exact solutions to a surrogate leave-i-out problem. Indeed, recall that the leave-i-out problem is given by (4), which cannot be solved in closed form. However, we note that the solution does exist in closed form in the case where both ` and R are quadratic functions.\nWe may thus consider the approximate leave-i-out problem, where both ` and R have been replaced in the leave-i-out problem (4) by their quadratic expansion at \u03b2\u0302:\nmin \u03b2/i \u2211 j 6=i \u02dc\u0300(x>j \u03b2 /i; yj) + R\u0303(\u03b2 /i). (24)\nWhen both ` and R are twice differentiable at the full data solution, \u02dc\u0300and R\u0303 correspond to their respective second order Taylor expansions at \u03b2\u0302. When ` or R is not twice differentiable at the full data solution, we have seen that it is still possible to obtain an ALO estimator through the proximal map (in the case of the dual) or through smoothing arguments (in the case of the primal). The corresponding quadratic surrogates may then be formulated as partial quadratic functions, i.e. convex quadratic functions restricted to an affine subspace. However, due to space limitations we only focus on twice differentiable losses and regularizers here.\nThe way we obtain \u03b2\u0303/i in (19) indicates that the primal formula in (20) and (21) are the exact leave-i-out solution of the surrogate primal problem (24). On the other hand, we may also wish to consider the surrogate dual problem, by replacing `\u2217 andR\u2217 by their quadratic expansion at full data dual solution \u03b8\u0302 in the dual problem (7). One may possibly worry that the surrogate dual problem is then different from the dual of the surrogate primal problem (24). This does not happen, and we have the following theorem. Theorem 5.1. Let ` and R be twice differentiable convex functions. Let \u02dc\u0300and R\u0303 denote the quadratic surrogates of the loss and regularizer at \u03b2\u0302, and let \u02dc\u0300\u2217D and R\u0303 \u2217 D denote the quadratic surrogates of the conjugate loss and regularizer at the dual full data solution \u03b8\u0302. We have that the following problems are equivalent (have the same minimizer):\nmin\u03b8 \u2211n j=1 \u02dc\u0300\u2217(\u2212\u03b8j ; yj) + R\u0303\u2217(X>\u03b8), (25)\nmin\u03b8 \u2211n j=1 \u02dc\u0300\u2217 D(\u2212\u03b8j ; yj) + R\u0303\u2217D(X>\u03b8). (26)\nAdditionally, we note that the dual method described in Section 3 solves the surrogate dual problem (26).\nTheorem 5.2. LetXu, yu be as in (16), and let y\u0303 /i u,i be the transformed ALO obtained in (17). Let y\u0303a be the same as yu except y\u0303a,i = y\u0303 /i u,i. Then y\u0303a satisfies\n[proxg\u0303(y\u0303a)]i = 0, (27)\nwhere g\u0303(u) = R\u0303\u2217(X>u u) and R\u0303 denotes the quadratic surrogate of the regularizer. In particular, y\u0303/ii = Kiiy\u0303 /i u,i is the exact leave-i-out predicted value for the surrogate problem in Theorem 5.1.\nWe refer the reader to Appendix B for the proofs. These two theorems imply that for twice differentiable losses and regularizers, the frameworks we laid out in Sections 3 and 4 lead to exactly the same ALO formulas."}, {"heading": "6. Applications", "text": ""}, {"heading": "6.1. Generalized LASSO", "text": "The generalized LASSO (Tibshirani & Taylor, 2011) is a generalization of the LASSO problem which captures many applications such as the fused LASSO (Tibshirani et al., 2005), `1 trend filtering (Kim et al., 2009) and wavelet smoothing in a unified framework. The generalized LASSO problem solves the following penalized regression problem:\nmin \u03b2\n1\n2 n\u2211 j=1 (yj \u2212 x>j \u03b2)2 + \u03bb\u2016D\u03b2\u20161. (28)\nwhere the regularizer is parameterized by a fixed matrix D \u2208 Rm\u00d7p which captures the desired structure in the data. We note that the regularizer is a semi-norm. Hence we can formulate the dual problem as a projection. In fact, a dual formulation of (28) can be obtained as (see Appendix D):\nmin \u03b8,u\n1 2 \u2016\u03b8 \u2212 y\u201622 s.t. \u2016u\u2016\u221e \u2264 \u03bb andX>\u03b8 = D>u.\nThe dual optimal solution satisfies \u03b8\u0302 = \u03a0\u2206X (y), where \u2206X is the polytope given by:\n\u2206X = {\u03b8 \u2208 Rn : \u2203u, \u2016u\u2016\u221e \u2264 \u03bb andX>\u03b8 = D>u}.\nThe projection onto the polytope C = {D>u : \u2016u\u2016\u221e \u2264 \u03bb} is given in (Tibshirani & Taylor, 2011) as locally being the projection onto the affine space orthogonal to the nullspace of D\u00b7,\u2212E , where E = {i : |u\u0302i| = \u03bb} and \u2212E = {1, . . . , p} \\ E. Since \u2206X = [X>]\u22121C is the inverse image of C under the linear map given byX>, the projection onto \u2206X is given locally by the projection onto the affine space normal to the space spanned by the columns of [X>]+nullD\u00b7,\u2212E , provided X has full column rank. Here, [X>]+ denotes the Moore-Penrose pseudoinverse of X>. To obtain a spanning set of this space, we consider A = XB, where B is a set of vectors spanning the nullspace ofD\u00b7,\u2212E . This allows us to computeH = AA+, the projection onto the normal space required to compute the ALO."}, {"heading": "6.2. Nuclear Norm", "text": "Consider the following matrix sensing problem\nB\u0302 := arg min B\n1\n2 n\u2211 j=1 (yj \u2212 \u3008Xj ,B\u3009)2 + \u03bb\u2016B\u2016\u2217 (29)\nwith B,Xj \u2208 Rp1\u00d7p2 . \u3008X,B\u3009 = trace(X>B) denotes the inner product. We use \u2016 \u00b7 \u2016\u2217 for nuclear norm, which is defined as the sum of the singular values of a matrix. The nuclear norm is a unitarily invariant function of the matrix (Lewis, 1995). Such functions are only indirectly related to the components of the matrix, making their analysis difficult even when they are smooth, and exacerbating the difficulties when they are non-smooth such as in the case of the nuclear norm. In particular, the smoothing framework described in Section 4.3 cannot be applied directly.\nWe are nonetheless able to leverage the specific structure of such functions and apply the smoothing trick to the singular values to obtain the following theorem. For more details on the derivation, please refer to Appendix E.3. Theorem 6.1. Consider the nuclear-norm penalized matrix regression problem (29), and let B\u0302 = U\u0302diag[\u03c3\u0302]V\u0302 > be the SVD of the full data estimator B\u0302, with U\u0302 \u2208 Rp1\u00d7p1 , V\u0302 \u2208 Rp2\u00d7p2 . Let m = rank(B\u0302) be the number of nonzero \u03c3\u0302j\u2019s for B\u0302. Let B\u0303 /i h denote the approximate of B\u0302\n/i obtained from the smoothed problem. Then, as h\u2192 0\n\u3008Xi, B\u0303/ih \u3009 \u2192 \u3008Xi, B\u0302\u3009+ Hii\n1\u2212Hii (\u3008Xi, B\u0302\u3009 \u2212 yi),\nH = X \u00b7,E [X>\u00b7,EX \u00b7,E + \u03bbG]\u22121X > \u00b7,E ,\nwith X \u2208 Rn\u00d7p1p2 and G \u2208 Rm(p1+p2\u2212m)\u00d7m(p1+p2\u2212m) a symmetric matrix given by:\nX j,kl =U\u0302>\u00b7,kXjV\u0302\u00b7,l,\nGkl,st =  0 s = t = k = l \u2264 m 1 \u03c3\u0302s+\u03c3\u0302t 1 \u2264 s 6= t \u2264 m, (k, l) = (s, t) 1 \u03c3\u0302s 1 \u2264 s \u2264 m < t \u2264 p2, (k, l) = (s, t) 1 \u03c3\u0302t 1 \u2264 t \u2264 m < s \u2264 p1, (k, l) = (s, t) \u2212 1\u03c3\u0302s+\u03c3\u0302t 1 \u2264 s 6= t \u2264 m, (k, l) = (t, s) \u2212 gr[\u03c3\u0302t]\u03c3\u0302s 1 \u2264 s \u2264 m < t \u2264 p2, (k, l) = (t, s) \u2212 gr[\u03c3\u0302s]\u03c3\u0302t 1 \u2264 t \u2264 m < s \u2264 p2, (k, l) = (t, s) 0 otherwise.\n(30)\nwhere for t > m, \u03c3\u0302t = 0 and gr[\u03c3\u0302t] is the corresponding subgradient at this singular value, which can be obtained through the SVD of 1\u03bb \u2211n j=1(yj \u2212 \u3008Xj , B\u0302\u3009)Xj . The set E is then defined as:\nE = {(k, l) : k \u2264 m or l \u2264 m}.\nNote that the indices of G and the index set E are consistent."}, {"heading": "6.3. Linear SVM", "text": "The linear SVM optimization can be written as\narg min \u03b2 n\u2211 j=1 (1\u2212 yjx>j \u03b2)+ + \u03bb 2 \u2016\u03b2\u201622,\nwith yj \u2208 {\u22121, 1} and (\u00b7)+ = max{\u00b7, 0}. Note that this is a special case of the problem we studied in Section 4.2. Here, `(u; yj) = (1\u2212 yju)+ has only one zero order singularity at yj . Using Theorem 4.1 and simplifying the expressions, we obtain the following ALO formula for SVM:\nx>i \u03b2\u0303 /i = x>i \u03b2\u0302 + aig`,i,\nwhere\nai =\n{ 1 \u03bbx > i (Ip \u2212X>V,\u00b7(XV,\u00b7X>V,\u00b7)\u22121XV,\u00b7)xi i \u2208 S,(\n\u03bb[(XV,\u00b7X > V,\u00b7) \u22121]ii\n)\u22121 i \u2208 V,\nand for i \u2208 S, g`,i = \u2212yi if yix>i \u03b2\u0302 < 1, g`,i = 0 if yix > i \u03b2\u0302 > 1, and for i \u2208 V\ng`,V = (XV,\u00b7X > V,\u00b7) \u22121XV,\u00b7[\u03bb\u03b2\u0302 + \u2211 j:yjx>j \u03b2\u0302<1 yjxj ].\nRecall that V = {j : x>j \u03b2\u0302 = yj} and S = [1, . . . , n]\\V ."}, {"heading": "7. Numerical Experiments", "text": "We illustrate the performance of ALO through three experiments. The first two compare the ALO risk estimate with that of LOOCV. The third experiment compares the computational complexity of ALO with that of LOOCV. We have also evaluated the performance of ALO on real-world datasets. Due to lack of space, these results are presented in Appendix F.2. For the first experiment (Figure 2a), we run ALO and LOOCV for the three models studied in Section 6 (using fused LASSO (Tibshirani et al., 2005) as a special case of generalized LASSO) and compare their risk estimates under the settings n > p and n < p respectively. The full details of the experiments are provided in Appendix F.\nFor the second experiment (Figure 2b), we consider the risk estimates for LASSO from ALO and LOOCV under settings with model mis-specification, heavy-tail noise and correlated design. For all three cases, ALO approximates LOOCV well.\nIn general, we observe that the estimates given by ALO are close to LOOCV, although the performance may deteriorate\nfor very small values of \u03bb, as is clear in the fused-LASSO (n < p) example. These values of \u03bb correspond to \u201cdense\u201d solutions, and are far from the optimal choice. Hence, such inaccuracies do not harm the parameter tuning algorithm.\nOur last experiment compares the computational complexity of ALO with that of LOOCV. In Table 1, we provide the timing of LASSO for different values of n and p. The time required by ALO, which involves a single fit and a matrix inversion (in the construction ofH matrix), is in all experiments no more than twice that of a single fit. We refer the reader to Appendix F for the details of this experiment."}, {"heading": "8. Discussion", "text": "ALO offers a highly efficient approach for parameter tuning and risk estimation for a large class of statistical machine learning models. We focus on nonsmooth models and propose two general frameworks for calculating ALO. One is from the primal perspective, the other from the dual.\nBy approximating LOOCV, ALO inherits desirable properties of LOOCV in high-dimensional settings where n and p are comparable. In particular, ALO can overcome the bias issues that k-fold cross validation displays in these settings."}, {"heading": "Acknowledgements", "text": "We acknowledge computing resources from Columbia University\u2019s Shared Research Computing Facility project, which is supported by NIH Research Facility Improvement Grant 1G20RR030893-01, and associated funds from the New York State Empire State Development, Division of Science Technology and Innovation (NYSTAR) Contract C090171, both awarded April 15, 2010."}], "year": 2018, "references": [{"title": "The relationship between variable selection and data agumentation and a method for prediction", "authors": ["D.M. Allen"], "year": 1974}, {"title": "Living on the edge: Phase transitions in convex programs with random data", "authors": ["D. Amelunxen", "M. Lotz", "M.B. McCoy", "J.A. Tropp"], "venue": "Information and Inference: A Journal of the IMA,", "year": 2014}, {"title": "Estimating lasso risk and noise level", "authors": ["M. Bayati", "M.A. Erdogdu", "A. Montanari"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2013}, {"title": "Templates for convex cone problems with applications to sparse signal recovery", "authors": ["S.R. Becker", "E.J. Cand\u00e8s", "M.C. Grant"], "venue": "Math. Program. Comput.,", "year": 2011}, {"title": "On optimal generalizability in parametric learning", "authors": ["A. Beirami", "M. Razaviyayn", "S. Shahrampour", "V. Tarokh"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Convex optimization", "authors": ["S. Boyd", "L. Vandenberghe"], "year": 2004}, {"title": "Unbiased risk estimates for singular value thresholding and spectral estimators", "authors": ["E.J. Candes", "C.A. Sing-Long", "J.D. Trzasko"], "venue": "IEEE transactions on signal processing,", "year": 2013}, {"title": "Efficient approximate leaveone-out cross-validation for kernel logistic regression", "authors": ["G.C. Cawley", "N.L. Talbot"], "venue": "Machine Learning,", "year": 2008}, {"title": "Libsvm: a library for support vector machines", "authors": ["Chang", "C.-C", "Lin", "C.-J"], "venue": "ACM transactions on intelligent systems and technology (TIST),", "year": 2011}, {"title": "Proximal Splitting Methods in Signal Processing, pp. 185\u2013212", "authors": ["P.L. Combettes", "Pesquet", "J.-C"], "year": 2011}, {"title": "Neighborliness of randomly projected simplices in high dimensions", "authors": ["D.L. Donoho", "J. Tanner"], "venue": "Proceedings of the National Academy of Sciences of the United States of America,", "year": 2005}, {"title": "Messagepassing algorithms for compressed sensing", "authors": ["D.L. Donoho", "A. Maleki", "A. Montanari"], "venue": "Proceedings of the National Academy of Sciences,", "year": 2009}, {"title": "The degrees of freedom of the lasso for general design matrix", "authors": ["C. Dossal", "M. Kachour", "M. Fadili", "G. Peyr\u00e9", "C. Chesneau"], "venue": "Statistica Sinica,", "year": 2013}, {"title": "The estimation of prediction error: covariance penalties and cross-validation", "authors": ["B. Efron"], "venue": "Journal of the American Statistical Association,", "year": 2004}, {"title": "CVX: Matlab software for disciplined convex programming, version 2.1", "authors": ["M. Grant", "S. Boyd"], "year": 2014}, {"title": "Result analysis of the nips 2003 feature selection challenge", "authors": ["I. Guyon", "S. Gunn", "A. Ben-Hur", "G. Dror"], "venue": "In Advances in neural information processing systems,", "year": 2005}, {"title": "Updating the inverse of a matrix", "authors": ["W.W. Hager"], "venue": "SIAM review,", "year": 1989}, {"title": "Elements of Statistical Learning, chapter Model Assessment and Selection", "authors": ["T. Hastie", "R. Tishirani", "J. Friedman"], "venue": "Springer-Verlag New York,", "year": 2009}, {"title": "Robust regression: asymptotics, conjectures and monte carlo", "authors": ["P.J. Huber"], "venue": "The Annals of Statistics,", "year": 1973}, {"title": "Primal-dual firstorder methods withO(1/ ) iteration-complexity for cone programming", "authors": ["G. Lan", "Z. Lu", "R.D.C. Monteiro"], "venue": "Mathematical Programming,", "year": 2011}, {"title": "Ridge estimators in logistic regression", "authors": ["S. Le Cessie", "J.C. Van Houwelingen"], "venue": "Applied statistics,", "year": 1992}, {"title": "The convex analysis of unitarily invariant matrix functions", "authors": ["A.S. Lewis"], "venue": "Journal of Convex Analysis,", "year": 1995}, {"title": "Efficient approximate k-fold and leave-one-out cross-validation for ridge regression", "authors": ["R.J. Meijer", "J.J. Goeman"], "venue": "Biometrical Journal,", "year": 2013}, {"title": "Symmetric gauge functions and unitarily invariant norms", "authors": ["L. Mirsky"], "venue": "Quarterly Journal of Mathematics,", "year": 1960}, {"title": "Consistent parameter estimation for lasso and approximate message passing", "authors": ["A. Mousavi", "A. Maleki", "Baraniuk", "R. G"], "venue": "The Annals of Statistics,", "year": 2017}, {"title": "Cross validation in lasso and its acceleration", "authors": ["T. Obuchi", "Y. Kabashima"], "venue": "Journal of Statistical Mechanics: Theory and Experiment,", "year": 2016}, {"title": "Gaussian processes and svm: Mean field results and leave-one-out", "authors": ["M. Opper", "O. Winther"], "year": 2000}, {"title": "Automatic smoothing of regression functions in generalized linear models", "authors": ["F. O\u2019sullivan", "B.S. Yandell", "W.J. Raynor Jr."], "venue": "Journal of the American Statistical Association,", "year": 1986}, {"title": "URL http://www", "authors": ["J. Qian", "T. Hastie", "J. Friedman", "R. Tibshirani", "Simon", "N. Glmnet for matlab"], "venue": "stanford. edu/ \u0303 hastie/glmnet matlab, 2013.", "year": 2013}, {"title": "A scalable estimate of the extrasample prediction error via approximate leave-one-out", "authors": ["K. Rad", "A. Maleki"], "venue": "arXiv preprint arXiv:1801.10243,", "year": 2018}, {"title": "Convex analysis", "authors": ["R.T. Rockafellar"], "year": 1970}, {"title": "Cross-validatory choice and assessment of statistical predictions", "authors": ["M. Stone"], "venue": "Journal of the royal statistical society. Series B (Methodological), pp", "year": 1974}, {"title": "Regression shrinkage and selection via the lasso", "authors": ["R. Tibshirani"], "venue": "Journal of the Royal Statistical Society. Series B (Methodological), pp", "year": 1996}, {"title": "Sparsity and smoothness via the fused lasso", "authors": ["R. Tibshirani", "M. Saunders", "S. Rosset", "J. Zhu", "K. Knight"], "venue": "J. R. Stat. Soc. Ser. B Stat. Methodol.,", "year": 2005}, {"title": "The solution path of the generalized lasso", "authors": ["R.J. Tibshirani", "J. Taylor"], "venue": "Ann. Statist.,", "year": 2011}, {"title": "Degrees of freedom in lasso problems", "authors": ["R.J. Tibshirani", "J Taylor"], "venue": "The Annals of Statistics,", "year": 2012}, {"title": "The degrees of freedom of partly smooth regularizers", "authors": ["S. Vaiter", "C. Deledalle", "J. Fadili", "G. Peyr\u00e9", "C. Dossal"], "venue": "Annals of the Institute of Statistical Mathematics,", "year": 2017}, {"title": "Das asymptotische verteilungsgestez der eigenwert linearer partieller differentialgleichungen (mit einer anwendung auf der theorie der hohlraumstrahlung)", "authors": ["L. Weyl"], "venue": "Mathematische Annalen,", "year": 1912}, {"title": "Inverting modified matrices", "authors": ["M.A. Woodbury"], "venue": "Memorandum report,", "year": 1950}, {"title": "On the degrees of freedom of the lasso", "authors": ["H. Zou", "T. Hastie", "R Tibshirani"], "venue": "The Annals of Statistics,", "year": 2007}], "id": "SP:efd6f6109e4fbef727ea9ccb54aa374ef6e3510a", "authors": [{"name": "Shuaiwen Wang", "affiliations": []}, {"name": "Wenda Zhou", "affiliations": []}, {"name": "Haihao Lu", "affiliations": []}, {"name": "Arian Maleki", "affiliations": []}, {"name": "Vahab Mirrokni", "affiliations": []}], "abstractText": "Consider the following class of leaning schemes: \u03b2\u0302 := arg min \u03b2 \u2211n j=1`(x > j \u03b2; yj) + \u03bbR(\u03b2), (1) where xi \u2208 R and yi \u2208 R denote the i feature and response variable respectively. Let ` and R be the loss function and regularizer, \u03b2 denote the unknown weights, and \u03bb be a regularization parameter. Finding the optimal choice of \u03bb is a challenging problem in high-dimensional regimes where both n and p are large. We propose two frameworks to obtain a computationally efficient approximation ALO of the leave-one-out cross validation (LOOCV) risk for nonsmooth losses and regularizers. Our two frameworks are based on the primal and dual formulations of (1). We prove the equivalence of the two approaches under smoothness conditions. This equivalence enables us to justify the accuracy of both methods under such conditions. We use our approaches to obtain a risk estimate for several standard problems, including generalized LASSO, nuclear norm regularization, and support vector machines. We empirically demonstrate the effectiveness of our results for non-differentiable cases.", "title": "Approximate Leave-One-Out for Fast Parameter Tuning in High Dimensions"}