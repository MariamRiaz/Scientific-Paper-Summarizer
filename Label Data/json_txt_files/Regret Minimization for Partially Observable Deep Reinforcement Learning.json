{"sections": [{"heading": "1. Introduction", "text": "Many reinforcement learning problems of practical interest have the property of partial observability, where observations of state are generally non-Markovian. Practical deep reinforcement learning algorithms fall into two broad classes, neither of which satisfactorily deals with partial observability despite the prevalance of partial observations in the real world. One class of algorithms consists of value function-based methods such as deep Q-learning (Mnih et al., 2013; 2015), which are known to be highly sample efficient but generally assume a Markovian observation space. The other class of algorithms consists of Monte Carlo policy gradient methods such as trust region policy optimization (Schulman et al., 2015), which do not need to assume Markovian observations but are less sample efficient than value function-based methods. Some policy gradient methods\n1Department of Electrical Engineering and Computer Sciences, University of California, Berkeley. Correspondence to: Peter Jin <phj@eecs.berkeley.edu>.\nsuch as advantage actor-critic (Mnih et al., 2016) introduce the Markov assumption through a critic or state-dependent baseline to improve sample efficiency.\nThere are two common workarounds for the problem of partial observation: (a) learning policies and value functions on finite length observation histories, and (b) learning recurrent policies and recurrent value functions. Finite length observation histories concatenate the most recent raw observations into a stack of observations, and are a simple but effective technique to approximate the appearance of full observability. Recurrent functions can potentially incorporate an infinite observation history, but they can be harder to optimize. When using either finite length observation histories or recurrent functions, the same value function-based methods and policy gradient methods are employed with their respective tradeoffs in partially observable domains.\nWe are interested in developing methods that combine the best of both value function-based methods and policy gradient methods for partial observable domains. That is, can we develop methods that are sample efficient but are also robust to partial observation spaces?\nOur contribution is a new model-free deep reinforcement learning algorithm based on the principle of regret minimization which does not require access to a Markovian state. Our method learns a policy by estimating an advantage-like function which approximates a quantity called the counterfactual regret. Counterfactual regret is central to the family of counterfactual regret minimization (CFR) (Zinkevich et al., 2007) algorithms for solving incomplete information games. Hence we call our algorithm \u201cadvantage-based regret minimization\u201d (ARM).\nWe evaluate our approach on three partially observable visual reinforcement learning tasks: first-person 3D navigation in Doom and Minecraft (Kempka et al., 2016; Johnson et al., 2016), avoiding partially observed objects in Doom, and playing Pong with partial observation perturbations (Bellemare et al., 2013). In our experiments, we find that our method is more robust and offers substantial improvement over prior methods on partially observable tasks.\nar X\niv :1\n71 0.\n11 42\n4v 2\n[ cs\n.L G\n] 2\n5 O\nct 2\n01 8"}, {"heading": "2. Related Work", "text": "Deep reinforcement learning algorithms have been demonstrated to achieve excellent results on a range of complex tasks, including playing games (Mnih et al., 2015; Oh et al., 2016) and continuous control (Schulman et al., 2015; Lillicrap et al., 2016; Levine et al., 2016). Prior deep reinforcement learning algorithms either learn state or state-action value functions (Mnih et al., 2013), learn policies using policy gradients (Schulman et al., 2015), or perform a combination of the two using actor-critic architectures (Mnih et al., 2016). Policy gradient methods typically do not need to assume a Markovian state, but tend to suffer from poor sample complexity, due to their inability to use off-policy data. Methods based on learning Q-functions can use replay buffers to include off-policy data, accelerating learning (Lillicrap et al., 2016). However, learning Q-functions with Bellman error minimization typically requires a Markovian state space. When learning from partial observations such as images, the inputs might not be Markovian. Mnih et al. (2013) introduced the concatenation of short observation sequences. Prior methods (Hausknecht & Stone, 2017; Oh et al., 2016; Mnih et al., 2016; Heess et al., 2015) have proposed to mitigate this issue by learning recurrent critics and Q-functions, that depend on entire histories of observations. The recurrent deterministic policy gradient (Heess et al., 2015) for partially observable control uses a similar DDPGstyle estimator as used in ARM (see Section 3.3). However, all of these changes increase the size of the input space, increase variance, or make the optimization problem more complex. Our method instead learns cumulative advantage functions that depend only on the current observation, but can still handle non-Markovian problems.\nThe form of our advantage function update resembles positive temporal difference methods (Peng et al., 2016; van Hasselt & Wiering, 2007). Additionally, our update rule for a modified cumulative Q-function resembles the average Qfunction (Anschel et al., 2017) used for variance reduction in Q-learning. In both cases, the theoretical foundations of our method are based on cumulative regret minimization, and the motivation is substantively different. Previous work by Ross et al. (2011); Ross & Bagnell (2014) has connected regret minimization to reinforcement learning, imitation learning, and structured prediction, although not with counterfactual regret minimization. Regression regret matching (Waugh et al., 2015) is based on a closely related idea, which is to directly approximate the regret with a linear regression model, however the use of a linear model is limited in representation compared to deep function approximation.\nFinally, regret and value functions both typically take the form of expectations in reinforcement learning. An alternative view of value functions in RL is through the lens of value distributions (Bellemare et al., 2017). A connec-\ntion between regret and distributional RL could lead to very interesting future work."}, {"heading": "3. Advantage-based Regret Minimization", "text": "In this section, we provide some background on counterfactual regret minimization, describe ARM in detail, and give some intuition for why ARM works."}, {"heading": "3.1. Counterfactual Regret Minimization (CFR)", "text": "First, we give a reinforcement learning-centric exposition of counterfactual regret minimization (CFR) (Zinkevich et al., 2007; Bowling et al., 2015).\nThe CFR model for partial observation is as follows. Let S be the state space. Let I be the space of information sets: an information set I \u2208 I is a set of states s \u2208 I , where s \u2208 S, such that only the information set I is directly observable, and the individual states contained in I are hidden. An information set I is therefore a kind of aggregate state.\nIn CFR, an extensive-form game is repeatedly played between N players, indexed by i. We consider an iterative learning setting, where at the t-th learning iteration, the i-th player follows a fixed policy \u03c0it, choosing actions a \u2208 A conditioned on information sets I \u2208 I according to their policy \u03c0it(a|I). Let \u03c3t denotes the players\u2019 joint policy (their strategy profile). For any strategy profile \u03c3, the i-th player has an expected value for playing the game, J i(\u03c3). Let \u03c3\u2212it represent the joint policy of all players except the i-th player; similarly, let the tuple (\u03c0it, \u03c3 \u2212i t ) be the product of the i-th player\u2019s policy \u03c0it and the other players\u2019 joint policy \u03c3\u2212it .\nThe i-th player\u2019s overall regret after T learning iterations is defined:\n(RiT ) (overall) = max\n\u03c0i\u2217 T\u2211 t=1 J i(\u03c0i\u2217, \u03c3 \u2212i t )\u2212 J i(\u03c0it, \u03c3\u2212it ). (1)\nWhat is the interpretation of the overall regret? It is essentially how much better the i-th player could have done, had it always followed an optimal policy in hindsight instead of the actual sequence of policies \u03c0it it executed. Intuitively, the difference J i(\u03c0i\u2217, \u03c3 \u2212i t )\u2212J i(\u03c0it, \u03c3t) is the suboptimality of \u03c0it compared to \u03c0 i \u2217, and the sum of the suboptimalities over learning iterations yields the area inside the learning curve. In other words, a smaller overall regret implies better sample efficiency.\nLet Qi\u03c3t(I, a) be the counterfactual value of the i-th player, where the i-th player is assumed to reach I and always chooses the action a in the aggregate state I , and otherwise follows the policy \u03c0it, while all other players follow the strategy profile \u03c3\u2212it . Similarly, let the expected counterfactual value be calculated as V i\u03c3t(I) = \u2211 a\u2208A \u03c0 i t(a|I)Qi\u03c3t(I, a).\nLet (RiT ) (CF)(I, a) be the counterfactual regret of the i-th player, which is the sum of the advantage-like quantities Qi\u03c3t(I, a)\u2212 V i \u03c3t(I) after T learning iterations:\n(RiT ) (CF)(I, a) = T\u2211 t=1 Qi\u03c3t(I, a)\u2212 V i \u03c3t(I). (2)\nSimilarly, the immediate counterfactual regret can be obtained from the counterfactual regret by maximization over the player\u2019s action space:\n(RiT ) (immCF)(I) = max\na\u2208A T\u2211 t=1 Qi\u03c3t(I, a)\u2212 V i \u03c3t(I). (3)\nThe immediate counterfactual regret (Equation (3)) possesses a similar interpretation to the overall regret (Equation (1)), except that the immediate counterfactual regret and its constituent terms are additionally functions of the aggregate state I .\nSuppose that one were to briefly consider each aggregate state I as a separate, independent subproblem. By naively treating the counterfactual regret (RiT )\n(CF)(I, a) for each I as analogous to regret in an online learning setting, then at each learning iteration one may simply plug the counterfactual regret into a regret matching policy update (Hart & Mas-Colell, 2000):\n(\u03c0it+1) RM(a|I) = max(0, (R\ni t) (CF)(I, a))\u2211 a\u2032\u2208Amax(0, (R i t) (CF)(I, a\u2032)) . (4)\nIn the online learning setting, the regret matching policy achieves regret that increases with rate O( \u221a T ) in the number of iterations T (Cesa-Bianchi & Lugosi, 2003; Gordon, 2007); we can then say that the regret matching policy is regret minimizing.\nIt turns out that updating players\u2019 policies in the extensive game setting by iteratively minimizing the immediate counterfactual regrets according to Equation (4) will also minimize the overall regret (RiT )\n(overall) with upper bound O(|I| \u221a |A|T ). The overall regret\u2019s O( \u221a T ) dependence on the number of iterations T is not impacted by the structure of the information set space I, which is why CFR can be said to be robust to a certain kind of partial observability. This result forms the basis of the counterfactual regret minimization algorithm and was proved by Zinkevich et al. (2007).\nSince we are interested in the application of CFR to reinforcement learning, we can write down \u201c1-player\u201d versions of the components above: the counterfactual value, reinterpreted as a stationary state-action value function Q\u03c0|I 7\u2192a(I, a), where the action a is always chosen in the aggregate state I , and the policy \u03c0 is otherwise followed\n(Bellemare et al., 2016); the counterfactual regret, including its recursive definition:\nR(CF)T (I, a) = T\u2211 t=1 Q\u03c0t|I 7\u2192a(I, a)\u2212 V\u03c0t|I(I) (5)\n= R(CF)T\u22121(I, a) +Q\u03c0T |I 7\u2192a(I, a)\u2212 V\u03c0T |I(I) (6)\nwhere V\u03c0t|I(I) = \u2211 a\u2208A \u03c0t(a|I)Q\u03c0t|I 7\u2192a(I, a); and the regret matching policy update:\n\u03c0RMt+1(a|I) = max(0, R(CF)t (I, a))\u2211\na\u2032\u2208Amax(0, R (CF) t (I, a\n\u2032)) . (7)"}, {"heading": "3.2. CFR+", "text": "CFR+ (Tammelin, 2014) consists of a modification to CFR, in which instead of calculating the full counterfactual regret as in Equation (6), instead the counterfactual regret is recursively positively clipped:\nR(CF+)T (I, a) (8)\n= [R(CF+)T\u22121 (I, a)]+ +Q\u03c0T |I 7\u2192a(I, a)\u2212 V\u03c0T |I(I)\nwhere [x]+ = max(0, x) is the positive clipping operator. Comparing Equation (6) with Equation (8), the only difference in CFR+ is that the previous iteration\u2019s quantity is positively clipped in the recursion. This simple change turns out to yield a large practical improvement in the performance of the algorithm (Bowling et al., 2015). One intuition for why the positive clipping of CFR+ can improve upon CFR is that because [R(CF+)T\u22121 ]+ is nonnegative, it provides a kind of \u201coptimism under uncertainty,\u201d adding a bonus to some transitions while ignoring others. The CFR+ update has also been shown to do better than CFR when the best action in hindsight changes frequently (Tammelin et al., 2015). In the rest of this work we will solely build upon the CFR+ update."}, {"heading": "3.3. From CFR and CFR+ to ARM", "text": "Recall that CFR and CFR+ model partial observation in extensive games with a space I of information sets or aggregate states. Because we are interested in general observation spaces O in partially observed Markov decision processes (MDPs), we naively map CFR and CFR+ onto MDPs and replace I with O. It is known that Markov or stochastic games can be converted to extensive form (Littman, 1994; Sandholm & Singh, 2012; Kroer & Sandholm, 2014), and an MDP is a 1-player Markov game. Because we are also interested in high dimensional observations such as images, we estimate a counterfactual regret function approximation (Waugh et al., 2015) parameterized as a neural network.\nThe exact form of the counterfactual regret (either Equation (6) or Equation (8)), after translating aggregate states I to\nobservations o, utilizes a stationary action-value function Q\u03c0|o 7\u2192a(o, a). It is unclear how to learn a truly stationary action-value function, although Bellemare et al. (2016) propose a family of consistent Bellman operators for learning locally stationary action-value functions. We posit that the approximation Q\u03c0|o7\u2192a(o, a) \u2248 Q\u03c0(o, a), where Q\u03c0(o, a) is the usual action-value function, is acceptable when observations are rarely seen more than once in a typical trajectory.\nThe above series of approximations finally yields a learnable function using deep reinforcement learning; this function is a \u201cclipped cumulative advantage function\u201d:\nA+T (o, a) = max(0, A + T\u22121(o, a)) +A\u03c0T (o, a) (9)\nwhere A\u03c0(o, a) is the usual advantage function. Advantagebased regret minimization (ARM) is then the resulting batchmode deep RL algorithm that updates the policy to the regret matching distribution on the cumulative clipped advantage function:\n\u03c0t+1(a|o) = max(0, A+t (o, a))\u2211\na\u2032\u2208Amax(0, A + t (o, a\n\u2032)) . (10)\nAt the t-th batch iteration of ARM, a batch of data is collected by sampling trajectories using the current policy \u03c0t, followed by two processing steps: (a) fit A+t using Equation (9), then (b) set the next iteration\u2019s policy \u03c0t+1 using Equation (10).\nBelow, we will use the subscript k to refer to a timestep within a trajectory, while the subscript t refers to the batch iteration. To implement Equation (9) with deep function approximation, we define two value function approximations, V\u03c0t(ok; \u03b8t) and Q + t (ok, ak;\u03c9t), as well as a target value function V \u2032(ok;\u03d5), where \u03b8t, \u03c9t, and \u03d5 are the learnable parameters. The cumulative clipped advantage function is represented as A+t (ok, ak) = Q + t (ok, ak;\u03c9t)\u2212V\u03c0t(ok; \u03b8t). Within each sampling iteration, the value functions are fitted using stochastic gradient descent by sampling minibatches and performing gradient steps. In practice, we use Adam to perform the optimization (Kingma & Ba, 2015). The statevalue function V\u03c0t(ok; \u03b8t) is fit using n-step returns with a moving target value function V \u2032(ok+n;\u03d5), essentially using the estimator of the deep deterministic policy gradient (DDPG) (Lillicrap et al., 2016). In the same minibatch, Q+t (ok, ak; \u03b8t) is fit to a similar loss, but with an additional target reward bonus that incorporates the previous iteration\u2019s cumulative clipped advantage, max(0, A+t\u22121(ok, ak)). The regression targets vk and q+k are defined in terms of the n-step returns gnk = \u2211k+n\u22121 k\u2032=k \u03b3 k\u2032\u2212krk\u2032 + \u03b3 nV \u2032(ok+n;\u03d5):\nvk , g n k (11) qk , rk + \u03b3g n\u22121 k+1 (12) \u03c6k , Q + t\u22121(ok, ak;\u03c9t\u22121)\u2212 V\u03c0t\u22121(ok; \u03b8t\u22121) (13) q+k , max(0, \u03c6k) + qk. (14)\nAltogether, each minibatch step of the optimization subproblem consists of the following three parameter updates:\n\u03b8 (`+1) t \u2190 \u03b8 (`) t \u2212\n\u03b1 2 \u2207 \u03b8 (`) t (V\u03c0t(ok; \u03b8 (`) t )\u2212 vk)2 (15)\n\u03c9 (`+1) t \u2190 \u03c9 (`) t \u2212\n\u03b1 2 \u2207 \u03c9 (`) t (Q+t (ok, ak;\u03c9 (`) t )\u2212 q+k ) 2 (16)\n\u03d5(`+1) \u2190 \u03d5(`) + \u03c4(\u03b8(`+1)t \u2212 \u03d5(`)). (17)\nThe advantage-based regret minimization algorithm is summarized in Algorithm 1.\nWe note again that we use a biased value function estimator, whereas only the full returns are guaranteed to be unbiased in non-Markovian settings. In the high-dimensional domains we evaluated on, the most practical choice of deep advantage or value function approximation is based on biased but lower variance estimation, such as the n-step returns we use in practice (Schulman et al., 2016; Gu et al., 2017). We also note that ARM is essentially CFR with different design choices to facilitate function approximation. CFR is proven to converge in domains with non-Markovian information set spaces when using tabular representations, suggesting that our use of biased advantage estimation does not fundamentally preclude the applicability or effectiveness of ARM on non-Markovian domains.\nAlgorithm 1 Advantage-based regret minimization (ARM). initialize \u03c01 \u2190 uniform, \u03b80, \u03c90 \u2190 arbitrary for t in 1, . . . do\ncollect batch of trajectory data Dt \u223c \u03c0t initialize \u03b8t \u2190 \u03b8t\u22121, \u03c9t \u2190 \u03c9t\u22121, \u03d5\u2190 \u03b8t\u22121 for ` in 0, . . . do\nsample transitions: (ok, ak, rk, ok+1) \u223c Dt \u03b4k \u2190 1\u2212 I[ok is terminal] calculate n-step returns: gnk \u2190 \u2211k+n\u22121 k\u2032=k \u03b3 k\u2032\u2212krk\u2032 + \u03b3 n\u03b4k+nV\n\u2032(ok+n;\u03d5) calculate target values: if t = 1 then \u03c6k \u2190 0 else \u03c6k \u2190 Q+t\u22121(ok, ak;\u03c9t\u22121)\u2212 V\u03c0t\u22121(ok; \u03b8t\u22121) end if v(ok)\u2190 gnk q+(ok, ak)\u2190 max(0, \u03c6k) + rk + \u03b3gn\u22121k+1 update \u03b8t (Equation (15)) update \u03c9t (Equation (16)) update \u03d5 (Equation (17)) end for set \u03c0t+1(a|o) \u221d max(0, Q+t (o, a;\u03c9t)\u2212 V\u03c0t(o; \u03b8t))\nend for"}, {"heading": "3.4. ARM vs. Existing Policy Gradient Methods", "text": "We note that the fundamental operation in CFR-like algorithms, including ARM, is exemplified by the counterfactual regret update in Equation (6) which superficially looks quite similar to a policy gradient-style update: the update is in the direction of a Q-value minus a baseline. However, we can take the analogy between ARM and policy gradient methods further and show that ARM represents an inherently different update compared to existing policy gradient methods.\nRecent work has shown that policy gradient methods and Qlearning methods are connected via maximum entropy RL (O\u2019Donoghue et al., 2017; Haarnoja et al., 2017; Nachum et al., 2017; Schulman et al., 2017). One such perspective is from the soft policy iteration framework for batch-mode reinforcement learning (Haarnoja et al., 2018), where at the t-th batch iteration the updated policy is obtained by minimizing the average KL-divergence between the parametric policy class \u03a0 and a target policy ft. Below is the soft policy iteration update, where the subscript t refers to the batch iteration:\n\u03c0t+1 = arg min \u03c0\u2208\u03a0 Eo\u223c\u03c1t [DKL(\u03c0(\u00b7|o)\u2016ft(\u00b7|o))]\n= arg min \u03c0\u2208\u03a0 Eo\u223c\u03c1t,a\u223c\u03c0(\u00b7|o)[log(\u03c0(a|o))\u2212 log(ft(a|o))].\n(18)\nUsing the connection between policy gradient methods and Q-learning, we define the policy gradient target policy as a Boltzmann or softmax distribution on the entropy regularized advantage function A\u03b2-soft:\nfPGt (a|o) , exp(\u03b2A\u03b2-softt (o, a))\u2211\na\u2032\u2208A exp(\u03b2A \u03b2-soft t (o, a\n\u2032)) . (19)\nNow, parameterizing the policy \u03c0 in terms of an explicit parameter \u03b8, we obtain the expression for the existing policy gradient update, where b(o) is a baseline function:\n\u2206\u03b8PG \u221d Eo\u223c\u03c1t,a\u223c\u03c0(\u00b7|o;\u03b8) [ \u2207\u03b8 log(\u03c0(o|a; \u03b8))\u00b7 (20)(\n\u2212 (1/\u03b2) log(\u03c0(o|a; \u03b8)) +A\u03b2-softt (o, a)\u2212 b(o) )] .\nThe non-entropy-regularized policy gradient arises in the limit \u03b2 \u2192\u221e, at which point (1/\u03b2) vanishes.\nNote that an alternative choice of target policy ft will lead to a different kind of policy gradient update. A policy gradient algorithm based on ARM instead proposes a target policy based on the regret matching distribution:\nfARMt (a|o) , max(0, A+t (o, a))\u2211\na\u2032\u2208Amax(0, A + t (o, a\n\u2032)) . (21)\nSimilarly, we can express the ARM-like policy gradient update, where again b(o) is a baseline:\n\u2206\u03b8ARM = Eo\u223c\u03c1t,a\u223c\u03c0(\u00b7|o;\u03b8) [ \u2207\u03b8 log(\u03c0(o|a; \u03b8))\u00b7 (22)(\n\u2212 log(\u03c0(o|a; \u03b8)) + log(max(0, A+t (o, a)))\u2212 b(o) )] .\nComparing Equations (20) and (22), we see that the ARMlike policy gradient (Equation (22)) has a logarithmic dependence on the clipped advantage-like function max(0, A+), whereas the existing policy gradient (Equation (20)) is only linearly dependent on the advantage function A\u03b2-soft. This difference in logarithmic vs. linear dependence is responsible for a large part of the inherent distinction of ARM from existing policy gradient methods. In particular, the logarithmic dependence in an ARM-like update may be less sensitive to advantage overestimation compared to existing policy gradient methods, perhaps serving a similar purpose to the double Q-learning estimator (van Hasselt et al., 2016) or consistent Bellman operators (Bellemare et al., 2016).\nWe also see that for the existing policy gradient (Equation (20)), the \u2212(1/\u03b2) log(\u03c0(a|o; \u03b8)) term, which arises from the policy entropy, is vanishing for large \u03b2. On the other hand, for the ARM-like policy gradient (Equation (22)), there is no similar vanishing effect on the equivalent policy entropy term, suggesting that ARM may perform a kind of entropy regularization by default.\nIn practice we cannot implement an ARM-like policy gradient exactly as in Equation (22), because the positive clipping max(0, A+) can yield log(0). However we believe this is not an intrinsic obstacle and leave the question of how to implement an ARM-like policy gradient to future work."}, {"heading": "3.5. Comparing ARM with Other Methods in Partially Observable Domains", "text": "The regret matching policy which is fundamental to ARM can be interpreted as a more nuanced form of exploration compared to the epsilon-greedy policy used with Q-learning. In partially observable domains, the optimal policy may generally be stochastic (Jaakkola et al., 1994). So - greedy policies which put a substantial probability mass on arg maxaQ(o, a), e.g. by setting = 0.01, can be suboptimal, especially compared to more general distributions on discrete actions, such as ARM\u2019s regret matching policy. The softmax policy typically learned by policy gradient methods is also quite general, but can still put too much probability mass on one action without compensation by an explicit entropy bonus as done in maximum entropy reinforcement learning.\nPolicy gradient methods have an overall regret bound of RT \u2264 B2/\u03b7 + \u03b7G2T derived from stochastic gradient de-\nscent, where B is an upper bound on the policy parameter `2-norm, G2 is an upper bound on the second moments of the stochastic gradients, and \u03b7 is the learning rate (Dick, 2015), Assuming an optimal learning rate \u03b7 = B/(G \u221a T ), the policy gradient regret bound becomes RT \u2264 2BG \u221a T .\nCFR has an overall regret bound of RT \u2264 \u2206|I| \u221a |A|T , where \u2206 is the positive range of returns, and |I| and |A| are cardinalities of the information set space and action space, respectively (Zinkevich et al., 2007). Both regret bounds have O( \u221a T ) dependence on T . As mentioned earlier, policy gradient methods can also be robust to partial observation, but unlike in the case of CFR, the dependence of the policy gradient regret bound on an optimal learning rate and on gradient variance may affect policy gradient convergence in the presence of estimation error. On the other hand, the constants in the CFR regret bound are constant properties of the environment.\nFor Q-learning per se, we are not aware of any known regret bound. Szepesva\u0301ri proved that an upper bound on the convergence rate of Q-learning (specifically, the convergence rate of \u2016QT (s, a)\u2212Q\u2217(s, a)\u2016\u221e), assuming a fixed exploration strategy, depends on an exploration condition number, which is the ratio of minimum to maximum state-action occupation frequencies (Szepesva\u0301ri, 1998), and which describes how \u201cbalanced\u201d the exploration strategy is. If partial observability leads to imbalanced exploration due to confounding of states from perceptual aliasing (McCallum, 1997), then Q-learning should be negatively affected in convergence and possibly in absolute performance."}, {"heading": "4. Experiments", "text": "Because we hypothesize that ARM should perform well in partially observable domains, we conduct our experiments on visual tasks that naturally provide partial observations of state. Our evaluations use feedforward convnets with frame history inputs; our hyperparameters are listed in Section A1 of the Supplementary Material. We are interested in comparing ARM with other advantage-structured methods, primarily: (a) double deep Q-learning with dueling network streams (van Hasselt et al., 2016; Wang et al., 2016), which possesses an advantage-like parameterization of its Q-function and assumes Markovian observations; and (b) TRPO (Schulman et al., 2015; 2016), which estimates an empirical advantage using a baseline state-value function and can handle non-Markovian observations."}, {"heading": "4.1. Learning First-person 3D Navigation", "text": "We first evaluate ARM on the task of learning first-person navigation in 3D maze-like tasks from two domains: ViZDoom (Kempka et al., 2016) based on the game Doom, and Malmo\u0308 (Johnson et al., 2016) based on the game Minecraft. Doom and Minecraft both feature an egocentric viewpoint,\n3D perspective, and rich visual textures. We expect that both domains exhibit a substantial degree of partial observability since only the immediate field-of-view of the environment is observable by the agent due to first-person perspective.\nIn Doom MyWayHome, the agent is randomly placed in one of several rooms connected in a maze-like arrangement, and the agent must reach an item that has a fixed visual appearance and is in a fixed location before time runs out. For Minecraft, we adopt the teacher-student curriculum learning task of Matiisen et al. (2017), consisting of 5 consecutive \u201clevels\u201d that successively increase the difficulty of completing the simple task of reaching a gold block: the first level (\u201cL1\u201d) consists of a single room; the intermediate levels (\u201cL2\u201d\u2013\u201cL4\u201d) consist of a corridor with lava-bridge and wallgap obstacles; and the final level (\u201cL5\u201d) consists of a 2\u00d7 2 arrangement of rooms randomly separated by lava-bridge or wall-gap obstacles. Examples of the MyWayHome and the Minecraft levels are shown in Figure 1.\nOur results on Doom and Minecraft are in Figures 2 and 3. Unlike previous evaluations which augmented raw pixel observations with extra information about the game state, e.g. elapsed time ticks or remaining health (Kempka et al., 2016; Dosovitskiy & Koltun, 2017), in our evaluation we forced all networks to learn using only visual input. Despite this restriction, ARM is still able to quickly learn policies with minimal tuning of hyperparameters and to reach close to the maximum achievable score in under 1 million simulator steps, which is quite sample efficient. On MyWayHome, we observed that ARM generally learned a well-performing policy more quickly than other methods. Additionally, we found that ARM is able to take advantage of an off-policy replay memory when learning on MyWayHome by storing the trajectories of previous sampling batches and applying an importance sampling correction to the n-step return estimator; please see Section A2 in the Supplementary Material.\nWe performed our Minecraft experiments using fixed curriculum learning schedules to evaluate the sample efficiency of different algorithms: the agent is initially placed in the first level (\u201cL1\u201d), and the agent is advanced to the next level whenever a preselected number of simulator steps have elapsed, until the agent reaches the last level (\u201cL5\u201d). We found that ARM and dueling double DQN both were able to learn on an aggressive \u201cfast\u201d schedule of only 62500 simulator steps between levels. TRPO required a \u201cslow\u201d schedule of 93750 simulator steps between levels to reliably learn. ARM was able to consistently learn a well performing policy on all of the levels, whereas double DQN learned more slowly on some of the intermediate levels. ARM also more consistently reached a high score on the final, most difficult level (\u201cL5\u201d)."}, {"heading": "4.2. Learning with Partially Observed Objects", "text": "Previously in Section 3.5, we argued that the convergence results for CFR suggest that reducing the size of the observation space could improve the convergence of CFR compared to methods based on policy gradients or Q-learning. We would expect that by controlling the degree of partial observability in a fixed task, one could expect the relative sample efficiency or performance of ARM to improve compared to other methods.\nWhereas the navigation tasks in Section 4.1 only dealt with first-person motion in static 3D environments, tasks that add other objects which may move and are not always visible to the agent intuitively ought to be less observable. To test the effect of the degree of partial observability on fixed tasks with objects, we conduct experiments with at least two variants of each task: one experiment is based on the unmodified task, but the other experiments occlude or mask essential pixels in observed frames to raise the difficulty of identifying relevant objects.\nWe evaluate ARM and other methods on two tasks with objects: Doom Corridor via ViZDoom, and Atari Pong via the Arcade Learning Environment (Bellemare et al., 2013). In Corridor, the agent is placed in a narrow 3D hallway and must avoid getting killed by any of several shooting monsters scattered along the route while trying to reach the end of the hallway. In Pong, the agent controls a paddle and must hit a moving ball past the computer opponent\u2019s paddle on the other side of the screen.\nIn our implementation of Corridor, which we call \u201cCorridor+,\u201d we restrict the action space of the agent to only be able to turn left or right and to move to one side. Because\nthe agent spawns facing the end of the hallway, the agent has to learn to first turn to its side to orient itself sideways before moving, while also trying to avoid getting hit from behind by monsters in close proximity. To induce partial observability in Corridor, we mask out a large square region in the center of the frame buffer. An example of this occlusion is shown in Figure 4. Because the only way for the agent to progress toward the end of the hallway is by moving sideways, the monsters that are closest to the agent, and hence the most dangerous, will appear near the center of the agent\u2019s field-of-view. So the center pixels are the most natural ones to occlude in Corridor+.\nFor Pong, we choose a rectangular area in the middle of the frame between the two players\u2019 paddles, then we set all pixels in the rectangular area to the background color. An illustration of the occlusion is shown in Figure 4. When the ball enters the occluded region, it completely disappears from view so its state cannot be reconstructed using a limited frame history. Intuitively, the agent needs to learn to anticipate the trajectory of the ball in the absence of visual cues of the ball\u2019s position and velocity.\nIt is also known that for Atari games in general, with only 4 observed frames as input, it is possible to predict hundreds of frames into the future on some games using only a feedforward dynamics model (Oh et al., 2015). By default, all of our networks receive as input a frame history of length 4. This suggests that limiting the frame history length in Pong is another effective perturbation for reducing observability. When the frame history length is limited to one, then reconstructing the velocity of moving objects in Pong becomes more difficult (Hausknecht & Stone, 2017).\nOur results on Corridor+ are shown in Figure 5. Among all our experiments, it is on Corridor+ that TRPO performs the best compared to all other methods. One distinguishing feature of TRPO is its empirical full return estimator, which is unbiased on non-Markovian observations, but which can have greater variance than the n-step return estimator used in deep Q-learning and ARM. On Corridor+, there appears to be a benefit to using the unbiased full returns over the\nbiased n-step returns, which speaks to the non-Markovian character of the Corridor+ task. It is also evident on Corridor+ that the performance of deep Q-learning suffers when the observability is reduced by occlusion, and that by comparison ARM is relatively unaffected, despite both methods using biased n-step returns with the same value of n (n = 5). This suggests that even when ARM is handicapped by the bias of its return estimator, ARM is intrinsically more robust to the non-Markovian observations that arise from partial observability. One possible direction to improve ARM is through alternative return estimators (Schulman et al., 2016; Wang et al., 2017).\nOur results on Pong are shown in Figure 6. The convergence of ARM on all three variants of Pong suggests that ARM is not affected much by partial observation in this domain. As expected, when observability is reduced in the limited frame history and occlusion experiments, deep Qlearning performs worse and converges more slowly than ARM. TRPO is generally less sample efficient than either ARM or DQN, although like ARM it seems to be resilient to the partial observation perturbations."}, {"heading": "5. Discussion", "text": "In this paper, we presented a novel deep reinforcement learning algorithm based on counterfactual regret minimization (CFR). We call our method advantage-based regret minimization (ARM). Similarly to prior methods that learn state or state-action value functions, our method learns a cumulative clipped advantage function of observation and action. However, in contrast to these prior methods, ARM is well suited to partially observed or non-Markovian environments, making it an appealing choice in a number of difficult domains. When compared to baseline methods, including deep Q-learning and TRPO, on partially observable tasks such as first-person navigation in Doom and Minecraft and interacting with partially observed objects in Doom and Pong, ARM is robust to the degree of partial observability and can achieve substantially better sample efficiency and performance. This illustrates the value of ARM for partially observable problems. In future work, we plan to further explore applications of ARM to more complex tasks, including continuous action spaces."}, {"heading": "Acknowledgements", "text": "We thank Tambet Matiisen for providing the Malmo\u0308 curriculum learning tasks, Deepak Pathak for help with ViZDoom experiments, and Tuomas Haarnoja, Richard Li, and Haoran Tang for helpful discussions. This work was partially supported by Berkeley DeepDrive, ADEPT Lab industrial sponsors and affiliates Intel, Google, Siemens, and SK Hynix, DARPA Award Number HR0011-12-2-0016, and ASPIRE Lab industrial sponsors and affiliates Intel, Google, HPE, Huawei, LGE, Nokia, NVIDIA, Oracle, and Samsung."}, {"heading": "A1. Experimental Details", "text": "For each experiment, we performed 5 trials for each method (some TRPO experiments on Pong are for 3 trials)."}, {"heading": "A1.1. Hyperparameters", "text": "Our hyperparameter choices for each method are listed below. For all methods using Adam, we roughly tuned the learning rate to find the largest that consistently converged in unmodified variants of tasks.\nA1.1.1. ARM\nPlease see Tables 1 and 2 for hyperparameters used with ARM. Note that our choice of ARM hyperparameters yields an equivalent number of minibatch gradient steps per sample as used by deep Q-learning, i.e. 1 Adam minibatch gradient step per 4 simulator steps; c.f. Table 4 for the deep Qlearning hyperparameters. We kept hyperparameters (other than the learning rate and the number of steps n) constant across tasks."}, {"heading": "HYPERPARAMETER ATARI DOOM MINECRAFT", "text": ""}, {"heading": "HYPERPARAMETER DOOM", "text": "A1.1.2. A2C\nPlease see Table 3 for hyperparameters used with A2C. We found that increasing the number of steps n used to calculate the n-step returns was most important for getting A2C/A3C to converge on Doom MyWayHome.\nA1.1.3. DQN\nPlease see Table 4 for hyperparameters used with deep Qlearning. Dueling double DQN uses the tuned hyperpa-"}, {"heading": "HYPERPARAMETER DOOM", "text": "rameters (van Hasselt et al., 2016; Wang et al., 2016). In particular, we found that dueling double DQN generally performed better and was more stable when learning on Atari with the tuned learning rate 6.25\u00d7 10\u22125 \u2248 6\u00d7 10\u22125 from Wang et al. (2016), compared to the slightly larger learning rate of 1\u00d7 10\u22124 used by ARM.\nA1.1.4. TRPO\nPlease see Table 5 for hyperparameters used with TRPO. We generally used the defaults, such as the KL step size of 0.01 which we found to be a good default. Decreasing the batch size improved sample efficiency on Doom and Minecraft without adversely affecting the performance of the learned policies."}, {"heading": "HYPERPARAMETER ATARI DOOM MINECRAFT", "text": ""}, {"heading": "A1.2. Environment and Task Details", "text": "Our task-specific implementation details are described below.\nA1.2.1. ATARI\nFor the occluded variant of Pong, we set the middle region of the 160 \u00d7 210 screen with x, y pixel coordinates [55 . . . 105), [34 . . . 194) to the RGB color (144, 72, 17). The image of occluded Pong in Figure 4 from the main text has a slightly darker occluded region for emphasis.\nWe use the preprocessing and convolutional network model of Mnih et al. (2013). Specifically, we view every 4th emulator frame, convert the raw frames to grayscale, and perform downsampling to generate a single observed frame. The input observation of the convnet is a concatenation of the most recent frames (either 4 frames or 1 frame). The convnet consists of an 8\u00d7 8 convolution with stride 4 and 16 filters followed by ReLU, a 4 \u00d7 4 convolution with stride 2 and 32 filters followed by ReLU, a linear map with 256 units followed by ReLU, and a linear map with |A| units where |A| is the action space cardinality (|A| = 6 for Pong).\nA1.2.2. DOOM\nOur modified environment \u201cDoom Corridor+\u201d is very closely derived from the default \u201cDoom Corridor\u201d environment in ViZDoom. We primarily make two modifications: (a) first, we restrict the action space to the three keys {MoveRight, TurnLeft, TurnRight}, for a total of 23 = 8 discrete actions; (b) second, we set the difficulty (\u201cDoom skill\u201d) to the maximum of 5.\nFor the occluded variant of Corridor+, we set the middle region of the 160\u00d7 120 screen with x, y pixel coordinates [30 . . . 130), [10 . . . 110) to black, i.e. (0, 0, 0).\nFor Corridor+, we scaled rewards by a factor of 0.01. We did not scale rewards for MyWayHome.\nThe Doom screen was rendered at a resolution of 160\u00d7 120 and downsized to 84\u00d7 84. Only every 4th frame was rendered, and the input observation to the convnet is a concatenation of the last 4 rendered RGB frames for a total of 12 input channels. The convnet contains 3 convolutions with 32 filters each: the first is size 8\u00d7 8 with stride 4, the second is size 4\u00d7 4 with stride 2, and the third is size 3\u00d7 3 with stride 1. The final convolution is followed by a linear map with 1024 units. A second linear map yields the output. Hidden activations are gated by ReLUs."}, {"heading": "A1.2.3. MINECRAFT", "text": "Our Minecraft tasks are based on the tasks introduced by Matiisen et al. (2017), with a few differences. Instead of using a continuous action space, we used a discrete action space with 4 move and turn actions. To aid learning on the last level (\u201cL5\u201d), we removed the reward penalty upon episode timeout and we increased the timeout on \u201cL5\u201d from 45 seconds to 75 seconds due to the larger size of the envi-\nronment. We scaled rewards for all levels by 0.001.\nWe use the same convolutional network architecture for Minecraft as we use for ViZDoom. The Minecraft screen was rendered at a resolution of 320\u00d7 240 and downsized to 84\u00d7 84. Only every 5th frame was rendered, and the input observation of the convnet is a concatenation of the last 4 rendered RGB frames for a total of 12 input channels."}, {"heading": "A2. Off-policy ARM via Importance Sampling", "text": "Our current approach to running ARM with off-policy data consists of applying an importance sampling correction directly to the n-step returns. Given the behavior policy \u00b5 under which the data was sampled, the current policy \u03c0t under which we want to perform estimation, and an importance sampling weight clip c for variance reduction, the corrected n-step return we use is:\ngnk (\u00b5\u2016\u03c0t) = k+n\u22121\u2211 k\u2032=k \u03b3k \u2032\u2212k  k\u2032\u220f `=k w\u00b5\u2016\u03c0t(a`|o`)  rk\u2032 (23) + \u03b3nV \u2032(ok+n;\u03d5)\nwhere the truncated importance weight w\u00b5\u2016\u03c0t(a|o) is defined:\nw\u00b5\u2016\u03c0t(a|o) = min ( c, \u03c0t(a|o) \u00b5(a|o) ) . (24)\nNote that the target value function V \u2032(ok+n;\u03d5) does not require an importance sampling correction because V \u2032 already approximates the on-policy value function V\u03c0t(ok+n; \u03b8t). Our choice of c = 1 in our experiments was inspired by Wang et al., (2017). We found that c = 1 worked well but note other choices for c may also be reasonable.\nWhen applying our importance sampling correction, we preserve all details of the ARM algorithm except for two aspects: the transition sampling strategy (a finite memory of previous batches are cached and uniformly sampled) and the regression targets for learning the value functions. Specifically, the regression targets vk and q+k (Equations (11)\u2013(14) in the main text) are modified to the following:\nvk = g n k (\u00b5\u2016\u03c0t) (25) qk = (1\u2212 w\u00b5\u2016\u03c0t(ak|ok))rk + g n k (\u00b5\u2016\u03c0t) (26) \u03c6k = Q + t\u22121(ok, ak;\u03c9t\u22121)\u2212 V\u03c0t\u22121(ok; \u03b8t\u22121) (27) q+k = max(0, \u03c6k) + qk. (28)"}, {"heading": "A3. Additional Experiments", "text": ""}, {"heading": "A3.1. Recurrence in Doom MyWayHome", "text": "We evaluated the effect of recurrent policy and value function estimation in the maze-like MyWayHome scenario of\nViZDoom. For the recurrent policy and value function, we replaced the first fully connected operation with an LSTM featuring an equivalent number of hidden units (1024). We found that recurrence has a small positive effect on the convergence of A2C, but was much less significant than the choice of algorithm; compare Figure 2 in the main text with Figure 7 below.\nA3.2. Atari 2600 games\nAlthough our primary interest is in partially observable reinforcement learning domains, we also want to check that ARM works in nearly fully observable and Markovian environments, such as Atari 2600 games. We consider two baselines: double deep Q-learning, and double deep fitted Q-iteration which is a batch counterpart to double DQN. We find that double deep Q-learning is a strong baseline for learning to play Atari games, although ARM still successfully learns interesting policies. One major benefit of Q-learning-based methods is the ability to utilize a large off-policy replay memory. Our results on a suite of Atari games are in Figure 8.\n0 1 2 steps 1e7\n0\n1000\n2000\n3000\n4000\n5000\nm ea\nn ep\niso de\nre tu\nrn\nBeam Rider\n0 1 2 steps 1e7\n0\n100\n200\n300\n400\nm ea\nn ep\niso de\nre tu\nrn\nBreakout\n0 1 2 steps 1e7\n0\n100\n200 300 m ea n ep iso de re\ntu rn\nEnduro\n0 1 2 steps 1e7\n20\n10\n0\n10\n20\nm ea\nn ep\niso de\nre tu\nrn\nPong\n0 1 2 steps 1e7\n0\n2000\n4000\n6000\n8000\n10000\n12000\nm ea\nn ep\niso de\nre tu\nrn\nQ*Bert\n0 1 2 steps 1e7\n0\n2000\n4000\n6000\n8000\n10000\nm ea\nn ep\niso de\nre tu\nrn\nSeaquest\n0 1 2 steps 1e7\n0\n200\n400\n600\n800\n1000\nm ea\nn ep\niso de\nre tu\nrn\nSpace Invaders"}, {"heading": "DDQN DDFQI ARM", "text": "Figure 8. Comparing double deep Q-learning (orange), double deep fitted Q-iteration (red), and ARM (blue) on a suite of seven Atari games from the Arcade Learning Environment."}], "year": 2018, "references": [{"title": "Averaged-DQN: Variance Reduction and Stabilization for Deep Reinforcement Learning", "authors": ["O. Anschel", "N. Baram", "N. Shimkin"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "The Arcade Learning Environment: An evaluation platform for general agents", "authors": ["M.G. Bellemare", "Y. Naddaf", "J. Veness", "M. Bowling"], "venue": "Journal of Artificial Intelligence Research,", "year": 2013}, {"title": "Increasing the Action Gap: New Operators for Reinforcement Learning", "authors": ["M.G. Bellemare", "G. Ostrovski", "A. Guez", "P.S. Thomas", "R. Munos"], "venue": "In AAAI Conference on Artificial Intelligence,", "year": 2016}, {"title": "A Distributional Perspective on Reinforcement Learning", "authors": ["M.G. Bellemare", "W. Dabney", "R. Munos"], "venue": "In Proceedings of the 34th International Conference on Machine Learning,", "year": 2017}, {"title": "Heads-up limit hold\u2019em poker is", "authors": ["M. Bowling", "N. Burch", "M. Johanson", "O. Tammelin"], "venue": "solved. Science,", "year": 2015}, {"title": "Potential-Based Algorithms in On-Line Prediction and Game Theory", "authors": ["N. Cesa-Bianchi", "G. Lugosi"], "venue": "Machine Learning,", "year": 2003}, {"title": "Policy Gradient Reinforcement Learning Without Regret", "authors": ["T. Dick"], "venue": "Master\u2019s thesis, University of Alberta,", "year": 2015}, {"title": "Learning to Act by Predicting the Future", "authors": ["A. Dosovitskiy", "V. Koltun"], "venue": "In International Conference on Learning Representations,", "year": 2017}, {"title": "No-regret Algorithms for Online Convex Programs", "authors": ["G.J. Gordon"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2007}, {"title": "Interpolated Policy Gradient: Merging On-Policy and Off-Policy Gradient Estimation for Deep Reinforcement Learning", "authors": ["S. Gu", "T. Lillicrap", "Z. Ghahramani", "R.E. Turner", "B. Sch\u00f6lkopf", "S. Levine"], "venue": "arXiv preprint arXiv:1706.00387,", "year": 2017}, {"title": "Reinforcement Learning with Deep Energy-Based Policies", "authors": ["T. Haarnoja", "H. Tang", "P. Abbeel", "S. Levine"], "venue": "In International Conference on Machine Learning,", "year": 2017}, {"title": "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor", "authors": ["T. Haarnoja", "A. Zhou", "P. Abbeel", "S. Levine"], "venue": "In Proceedings of the 35th International Conference on Machine Learning,", "year": 2018}, {"title": "A Simple Adaptive Procedure Leading to Correlated Equilibrium", "authors": ["S. Hart", "A. Mas-Colell"], "year": 2000}, {"title": "Deep Recurrent QLearning for Partially Observable MDPs", "authors": ["M. Hausknecht", "P. Stone"], "venue": "arXiv preprint arXiv:1507.06527,", "year": 2017}, {"title": "Memorybased control with recurrent neural networks", "authors": ["N. Heess", "J.J. Hunt", "T.P. Lillicrap", "D. Silver"], "venue": "arXiv preprint arXiv:1512.04455,", "year": 2015}, {"title": "Reinforcement Learning Algorithm for Partially Observable Markov Decision Problems", "authors": ["T. Jaakkola", "S.P. Singh", "M.I. Jordan"], "venue": "In Advances in Neural Information Processing Systems,", "year": 1994}, {"title": "The Malmo Platform for Artificial Intelligence Experimentation", "authors": ["M. Johnson", "K. Hofmann", "T. Hutton", "D. Bignell"], "venue": "In Proceedings of the 25th International Joint Conference on Artificial Intelligence,", "year": 2016}, {"title": "ViZDoom: A Doom-based AI Research Platform for Visual Reinforcement Learning", "authors": ["M. Kempka", "M. Wydmuch", "G. Runc", "J. Toczek", "W. Ja\u015bkowski"], "year": 2016}, {"title": "Adam: A Method for Stochastic Optimization", "authors": ["D.P. Kingma", "J.L. Ba"], "venue": "In International Conference on Learning Representations,", "year": 2015}, {"title": "Extensive-Form Game Abstraction With Bounds", "authors": ["C. Kroer", "T. Sandholm"], "venue": "In Proceedings of the 15th ACM Conference on Economics and Computation,", "year": 2014}, {"title": "End-toend training of deep visuomotor policies", "authors": ["S. Levine", "C. Finn", "T. Darrell", "P. Abbeel"], "venue": "The Journal of Machine Learning Research,", "year": 2016}, {"title": "Continuous control with deep reinforcement learning", "authors": ["T.P. Lillicrap", "J.J. Hunt", "A. Pritzel", "N. Heess", "T. Erez", "Y. Tassa", "D. Silver", "D. Wierstra"], "venue": "In International Conference on Learning Representations,", "year": 2016}, {"title": "Markov games as a framework for multiagent reinforcement learning", "authors": ["M.L. Littman"], "venue": "In Proceedings of the 11th International Conference on Machine Learning,", "year": 1994}, {"title": "Efficient Exploration in Reinforcement Learning with Hidden State", "authors": ["A.K. McCallum"], "venue": "In AAAI Fall Symposium on Model-directed Autonomous Systems,", "year": 1997}, {"title": "Playing Atari with Deep Reinforcement Learning", "authors": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A. Graves", "I. Antonoglou", "D. Wierstra", "M. Riedmiller"], "venue": "arXiv preprint arXiv:1312.5602,", "year": 2013}, {"title": "Human-level control through deep reinforcement learning", "authors": ["V. Mnih", "K. Kavukcuoglu", "D. Silver", "A.A. Rusu", "J. Veness", "M.G. Bellemare", "A. Graves", "M. Riedmiller", "A.K. Fidjeland", "G Ostrovski"], "year": 2015}, {"title": "Asynchronous methods for deep reinforcement learning", "authors": ["V. Mnih", "A.P. Badia", "M. Mirza", "A. Graves", "T. Lillicrap", "T. Harley", "D. Silver", "K. Kavukcuoglu"], "venue": "In International Conference on Machine Learning,", "year": 2016}, {"title": "Bridging the Gap Between Value and Policy Based Reinforcement Learning", "authors": ["O. Nachum", "M. Norouzi", "K. Xu", "D. Schuurmans"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2017}, {"title": "Combining policy gradient and Q-learning", "authors": ["B. O\u2019Donoghue", "R. Munos", "K. Kavukcuoglu", "V. Mnih"], "venue": "arXiv preprint arXiv:1611.01626,", "year": 2017}, {"title": "ActionConditional Video Prediction using Deep Networks in Atari Games", "authors": ["J. Oh", "X. Guo", "H. Lee", "R.L. Lewis", "S. Singh"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2015}, {"title": "Control of memory, active perception, and action in minecraft", "authors": ["J. Oh", "V. Chockalingam", "S. Singh", "H. Lee"], "venue": "In Proceedings of The 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "TerrainAdaptive Locomotion Skills Using Deep Reinforcement Learning", "authors": ["X.B. Peng", "G. Berseth", "M. van de Penne"], "venue": "ACM Transactions on Graphics,", "year": 2016}, {"title": "Reinforcement and Imitation Learning via Interactive No-Regret Learning", "authors": ["S. Ross", "J.A. Bagnell"], "venue": "arXiv preprint arXiv:1406.5979,", "year": 2014}, {"title": "A Reduction of Imitation Learning and Structured Prediction to NoRegret Online Learning", "authors": ["S. Ross", "G.J. Gordon", "J.A. Bagnell"], "venue": "In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics,", "year": 2011}, {"title": "Lossy Stochastic Game Abstraction with Bounds", "authors": ["T. Sandholm", "S. Singh"], "venue": "In Proceedings of the 13th ACM Conference on Economics and Computation,", "year": 2012}, {"title": "Trust region policy optimization", "authors": ["J. Schulman", "S. Levine", "P. Abbeel", "M. Jordan", "P. Moritz"], "venue": "In Proceedings of the 32nd International Conference on Machine Learning", "year": 2015}, {"title": "High-Dimensional Continuous Control Using Generalized Advantage Estimation", "authors": ["J. Schulman", "P. Moritz", "S. Levine", "M.I. Jordan", "P. Abbeel"], "venue": "arXiv preprint arXiv:1506.02438,", "year": 2016}, {"title": "Equivalence Between Policy Gradients and Soft Q-Learning", "authors": ["J. Schulman", "X. Chen", "P. Abbeel"], "venue": "arXiv preprint arXiv:1704.06440,", "year": 2017}, {"title": "The Asymptotic Convergence-Rate of Qlearning", "authors": ["C. Szepesv\u00e1ri"], "venue": "In Advances in Neural Information Processing Systems,", "year": 1998}, {"title": "Solving Large Imperfect Information Games Using CFR+", "authors": ["O. Tammelin"], "venue": "arXiv preprint arXiv:1407.5042,", "year": 2014}, {"title": "Solving Heads-up Limit Texas Hold\u2019em", "authors": ["O. Tammelin", "N. Burch", "M. Johanson", "M. Bowling"], "venue": "In Proceedings of the 24th International Joint Conference on Artificial Intelligence,", "year": 2015}, {"title": "Reinforcement Learning in Continuous Action Spaces", "authors": ["H. van Hasselt", "M.A. Wiering"], "venue": "In Proceedings of the 2007 IEEE Symposium on Approximate Dynamic Programming and Reinforcement Learning,", "year": 2007}, {"title": "Deep Reinforcement Learning and Double Q-Learning", "authors": ["H. van Hasselt", "A. Guez", "D. Silver"], "venue": "In AAAI Conference on Artificial Intelligence,", "year": 2016}, {"title": "Dueling Network Architectures for Deep Reinforcement Learning", "authors": ["Z. Wang", "T. Schaul", "M. Hessel", "H. van Hasselt", "M. Lanctot", "N. de Freitas"], "venue": "In Proceedings of the 33rd International Conference on Machine Learning,", "year": 2016}, {"title": "Solving Games with Functional Regret Estimation", "authors": ["K. Waugh", "D. Morrill", "J.A. Bagnell", "M. Bowling"], "venue": "In Workshops at the Twenty-Ninth AAAI Conference on Artificial Intelligence,", "year": 2015}, {"title": "Regret Minimization in Games with Incomplete Information", "authors": ["M. Zinkevich", "M. Johanson", "M.H. Bowling", "C. Piccione"], "venue": "In Advances in Neural Information Processing Systems,", "year": 2007}, {"title": "2016), compared to the slightly larger learning rate of 1\u00d7 10\u22124 used by ARM", "authors": ["Wang"], "year": 2016}, {"title": "2017), with a few differences", "authors": ["Matiisen"], "year": 2017}, {"title": "We found that c = 1 worked well", "authors": ["Wang"], "year": 2017}], "id": "SP:7946d9bcd8cdb4800afe90ccd26abe65762a77c9", "authors": [{"name": "Peter Jin", "affiliations": []}, {"name": "Kurt Keutzer", "affiliations": []}, {"name": "Sergey Levine", "affiliations": []}], "abstractText": "Deep reinforcement learning algorithms that estimate state and state-action value functions have been shown to be effective in a variety of challenging domains, including learning control strategies from raw image pixels. However, algorithms that estimate state and state-action value functions typically assume a fully observed state and must compensate for partial observations by using finite length observation histories or recurrent networks. In this work, we propose a new deep reinforcement learning algorithm based on counterfactual regret minimization that iteratively updates an approximation to an advantage-like function and is robust to partially observed state. We demonstrate that this new algorithm can substantially outperform strong baseline methods on several partially observed reinforcement learning tasks: learning first-person 3D navigation in Doom and Minecraft, and acting in the presence of partially observed objects in Doom and Pong.", "title": "Regret Minimization for Partially Observable Deep Reinforcement Learning"}