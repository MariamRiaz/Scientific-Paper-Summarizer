0	35	In this work, we study a basic question that arises in the study of high dimensional vector representations: given a dataset D of vectors and a query q, estimate the number of points within a specified distance threshold of q.
1	15	Such density estimates are important building blocks in non-parametric clustering, determining the popularity of topics, search and recommendation systems, the analysis of the neighborhoods of nodes in social networks, and in outlier detection, where geometric representations of data are frequently used.
3	7	Our questions have been studied in the context of spherical range counting.
4	15	One class of solution methods arising in the computational geometry literature, such as hierarchical splitting via trees, (Arya et al., 2010) have performance guarantees that depend exponentially on dimension.
5	26	These are unsuitable for the higher dimensional models that machine learning methods are increasingly shifting towards e.g. word embeddings (Pennington et al., 2014; Mikolov et al., 2013) and graph embeddings (Perozzi et al., 2014; Tang et al., 2015; Cao et al., 2015; Grover & Leskovec, 2016; Yang et al., 2016; Wang et al., 2017; Hamilton et al., 2017).
6	18	Over-parameterized models are oftentimes easier to train (Livni et al., 2014), and perform just as well, if not better (Zhang et al., 2016).
9	25	Angular distance, which corresponds to Euclidean distance for data points on the unit sphere is commonly used in applications related to word and document embeddings, and image and video search (Jegou et al., 2011) (Huang et al., 2012).
11	11	Our approach uses indexing and search via locality sensitive hashing (LSH) functions in order to estimate the size of the neighborhood in a more efficient manner than retrieving the neighbors within the given radius of similarity.
15	26	The closest works to ours in terms of solution method that we are aware of is that of (Spring & Shrivastava, 2017), giving an LSH based estimator to compute the partition function of a log-linear model, and (Charikar & Siminelakis, 2017), adapting LSH to solve a class of kernel density estimation problems.
20	31	As we show in our experimental study on GLOVE embeddings, our estimate of the number of elements that are 60 degrees from a query q (which corresponds to synonyms and/or related words to q in the English vocabulary), achieves multiple orders of magnitude improved accuracy over competing methods, subject to reasonable and practical resource constraints.
23	44	vn ∈ Rd on the unit sphere, a query q ∈ Rd also on the unit sphere, and a range of angles of interestA, for example 0-60 degrees, how many elements v in D are such that the angle between q and v, denoted θqv, are within range A?
25	13	Our goal is to preprocess D in order to estimate the cardinality of this set, denoted |Aq|, efficiently for any given q.
34	38	In order to compensate for the concentrated sampling, we adjust the value of each sample by the inverse of the probability that the sample lands in the target buckets.
43	39	This is also a consideration in our sampling scheme; we want to sample from buckets of hamming distances I that have a high probability of containing elements that are within angle A of q.
44	125	Our sampling scheme picks elements over K hash tables from buckets that are at hamming distance I to the query, where I is tuned to A.
45	9	Given a sample, x, we compute the angular distance θqx = cos−1(q · x).
48	32	(1) where Ckq (I) is the total number of elements in buckets of hamming distance I from q’s bucket in table k. We take S samples and construct Z1, Z2, .
52	18	As noted previously, both works only look at a single hash bucket in each hash table, leading to a high storage overhead.
53	41	We establish the following theoretical bounds on the storage and sample complexity of our estimator in order to achieve a (1±ε)-approximation to the true count with high probability.
54	10	For a given angular distance range of interest A and a given query q, with probability 1 − δ, our estimator returns a (1 ± ε)approximation to |Aq|, the true number of elements within angle A to q using O ( 1 ε2 min x∈Aq p(x) log( 1 δ ) ) tables and O ( E(Cq(I)) ε2|Aq|· min x∈Aq p(x) log( 1 δ ) ) samples.
56	34	It can be shown through a standard BernoulliChernoff argument that the sample complexity for random sampling is O( n|Aq|ε2 ln ( 1 δ ) ), where n|Aq| is the inverse proportion of elements of interest in the overall population.
58	13	This ratio of expectations seems intuitive – one would expect to get such an expression if our scheme took one sample per table.
62	7	We choose I so that min x∈Aq p(x) is high (this probability can be high even for a small set of hamming distances I, since p(x) is the cumulative probability mass of I successes in t trials, and binomial distributions in t concentrate in an O( √ t) sized interval around the mean), and E(Cq(I)) to be small (to filter out elements that are not interesting).
63	12	There are certain tradeoffs to choosing I .
65	11	The optimal choice for I is to choose the hamming distances that substantially increase min x∈Aq p(x) yet do not substantially increase E(Cq(I)) (so not too many uninteresting elements are infiltrating those buckets).
66	17	In the following sections, we explain our scheme further and present our experimental results.
67	11	The preprocessing step contributes 3 key ingredients to the overall estimation scheme: Hash Tables: Given a family of bit-wise hash functions H, define a function family G = {g : D → {0, 1}t} such that g(v) = (h1(v), .
70	38	We store each v ∈ D in bucket gk(v) for k = 1, 2 .
80	59	We want to characterize the bias of our importance sampling scheme in relation to the contents of the buckets of our hash tables.
81	36	We let Bkq (I) denote the set of hash buckets that are at hamming distance I from the hash address of query q for table k. Next, we introduce an intermediate random variable: W = 1 K K∑ k=1 ∑ x∈Aq 1(x ∈ Bkq (I)) p(x) .
