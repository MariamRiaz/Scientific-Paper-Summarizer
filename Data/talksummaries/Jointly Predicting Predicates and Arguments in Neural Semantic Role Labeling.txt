0	88	Semantic role labeling (SRL) captures predicateargument relations, such as “who did what to whom.” Recent high-performing SRL models (He et al., 2017; Marcheggiani et al., 2017; Tan et al., 2018) are BIO-taggers, labeling argument spans for a single predicate at a time (as shown in Figure 1).
1	44	They are typically only evaluated with gold predicates, and must be pipelined with error-prone predicate identification models for deployment.
2	25	We propose an end-to-end approach for predicting all the predicates and their argument spans in one forward pass.
6	14	The final graph is simply the union of predicted SRL roles (edges) and their associated text spans (nodes).
8	10	The span representations also generalize the token-level representations in BIObased models, letting the model dynamically decide which spans and roles to include, without using previously standard syntactic features (Punyakanok et al., 2008; FitzGerald et al., 2015).
9	15	To the best of our knowledge, this is the first span-based SRL model that does not assume that predicates are given.
12	54	We consider the space of possible predicates to be all the tokens in the input sentence, and the space of arguments to be all continuous spans.
17	17	, wj) | 1 ≤ i ≤ j ≤ n} contains all the spans (arguments), and L is the space of semantic role labels, including a null label indicating no relation.
19	27	We then define a set of random variables, where each random variable yp,a corresponds to a predicate p ∈ P and an argument a ∈ A, taking value from the discrete label space L. The random variables yp,a are conditionally independent of each other given the input X: P (Y | X) = ∏ p∈P,a∈A P (yp,a | X) (1) P (yp,a = l | X) = exp(φ(p, a, l))∑ l′∈L exp(φ(p, a, l′)) (2) Where φ(p, a, l) is a scoring function for a possible (predicate, argument, label) combination.
20	71	φ is decomposed into two unary scores on the predicate and the argument (defined in Section 3), as well as a label-specific score for the relation: φ(p, a, l) = Φa(a) + Φp(p) + Φ (l) rel (a, p) (3) The score for the null label is set to a constant: φ(p, a, ) = 0, similar to logistic regression.
21	8	Learning For each input X , we minimize the negative log likelihood of the gold structure Y ∗: J (X) =− logP (Y ∗ | X) (4) Beam pruning As our model deals with O(n2) possible argument spans and O(n) possible predicates, it needs to consider O(n3|L|) possible relations, which is computationally impractical.
22	53	To overcome this issue, we define two beams Ba and Bp for storing the candidate arguments and predicates, respectively.
25	21	Elements that fall out of the beam do not participate in computing the edge factors Φ(l)rel , reducing the overall number of relational factors evaluated by the model to O(n2|L|).
27	30	Our model builds contextualized representations for argument spans a and predicate words p based on BiLSTM outputs (Figure 2) and uses feedforward networks to compute the factor scores in φ(p, a, l) described in Section 2 (Figure 3).
31	11	The argument representation contains the following: end points from the BiLSTM outputs (x̄START(a), x̄END(a)), a soft head word xh(a), and embedded span width features f(a), similar to Lee et al. (2017).
33	11	g(a) =[x̄START(a); x̄END(a); xh(a); f(a)] (5) g(p) =x̄INDEX(p) (6) The soft head representation xh(a) is an attention mechanism over word inputs x in the argument span, where the weights e(a) are computed via a linear layer over the BiLSTM outputs x̄.
37	6	In the end-to-end setup, a system takes a tokenized sentence as input, and predicts all the predicates and their arguments.
39	7	For comparison with previous systems, we also report results with gold predicates, in which the complete set of predicates in the input sentence is given as well.
43	19	In Table 1 and 2, we organize all the results into two categories: the comparable single model systems, and the mod- els augmented with ELMo or ensembling (in the PoE rows).
44	8	End-to-end results As shown in Table 1,2 our joint model outperforms the previous best pipeline system (He et al., 2017) by an F1 difference of anywhere between 1.3 and 6.0 in every setting.
45	12	The improvement is larger on the Brown test set, which is out-of-domain, and the CoNLL 2012 test set, which contains nominal predicates.
47	20	Results with gold predicates To compare with additional previous systems, we also conduct experiments with gold predicates by constraining our predicate beam to be gold predicates only.
51	6	To better understand our model’s strengths and weaknesses, we perform three analyses following Lee et al. (2017) and He et al. (2017), studying (1) the effectiveness of beam pruning, (2) the ability to capture long-range dependencies, (3) agreement with syntactic spans, and (4) the ability to predict globally consistent SRL structures.
58	32	Our model is better at accurately predicting arguments that are farther away from the predicates, even compared to an ensemble model (He et al., 2017) that has a higher overall F1.
66	7	For example, our model predicts duplicate core arguments6 (shown in the U column in Table 3) more often than previous work.
67	6	This is due to the fact that our model uses independent classifiers to label each predicate-argument pair, making it difficult for them to implicitly track the decisions made for several arguments with the same predicate.
70	7	We proposed a new SRL model that is able to jointly predict all predicates and argument spans, generalized from a recent coreference system (Lee et al., 2017).
72	195	Empirically, the model does better at longrange dependencies and agreement with syntactic boundaries, but is weaker at global consistency, due to our strong independence assumption.
73	61	In the future, we could incorporate higher-order inference methods (Lee et al., 2018) to relax this assumption.
74	35	It would also be interesting to combine our span-based architecture with the selfattention layers (Tan et al., 2018; Strubell et al., 2018) for more effective contextualization.
