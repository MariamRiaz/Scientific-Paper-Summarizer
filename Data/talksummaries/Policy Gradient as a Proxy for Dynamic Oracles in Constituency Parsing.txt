0	16	Many recent state-of-the-art models for constituency parsing are transition based, decomposing production of each parse tree into a sequence of action decisions (Dyer et al., 2016; Cross and Huang, 2016; Liu and Zhang, 2017; Stern et al., 2017), building on a long line of work in transition-based parsing (Nivre, 2003; Yamada and Matsumoto, 2003; Henderson, 2004; Zhang and Clark, 2011; Chen and Manning, 2014; Andor et al., 2016; Kiperwasser and Goldberg, 2016).
1	10	However, models of this type, which decompose structure prediction into sequential decisions, can be prone to two issues (Ranzato et al., 2016; Wiseman and Rush, 2016).
2	48	The first is exposure bias: if, at training time, the model only observes states resulting from correct past decisions, it will not be prepared to recover from its own mistakes during prediction.
3	32	Second is the loss mismatch between the action-level loss used at training and any structure-level evaluation metric, for example F1.
4	23	A large family of techniques address the exposure bias problem by allowing the model to make mistakes and explore incorrect states during training, supervising actions at the resulting states using an expert policy (Daumé III et al., 2009; Ross et al., 2011; Choi and Palmer, 2011; Chang et al., 2015); these expert policies are typically referred to as dynamic oracles in parsing (Goldberg and Nivre, 2012; Ballesteros et al., 2016).
5	30	While dynamic oracles have produced substantial improvements in constituency parsing performance (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), they must be custom designed for each transition system.
6	11	To address the loss mismatch problem, another line of work has directly optimized for structurelevel cost functions (Goodman, 1996; Och, 2003).
10	54	We compare against training with a dynamic oracle (both to supervise exploration and provide loss-augmentation) where one is available, including a novel dynamic oracle that we define for the top-down transition system of Dyer et al. (2016).
13	17	In the process, we obtain new state-of-the-art results for single-model discriminative transition-based parsers trained on the English PTB (92.6 F1), French Treebank (83.5 F1), and Penn Chinese Treebank Version 5.1 (87.0 F1).
14	40	The transition-based parsers we use all decompose production of a parse tree y for a sentence x into a sequence of actions (a1, .
15	17	aT ) and resulting states (s1, .
16	53	Actions at are predicted sequentially, conditioned on a representation of the parser’s current state st and parameters θ: p(y|x; θ) = T∏ t=1 p(at | st; θ) (1) We investigate four parsers with varying transition systems and methods of encoding the current state and sentence: (1) the discriminative Recurrent Neural Network Grammars (RNNG) parser of Dyer et al. (2016), (2) the In-Order parser of Liu and Zhang (2017), (3) the Span-Based parser of Cross and Huang (2016), and (4) the Top-Down parser of Stern et al. (2017).1 We refer to the original papers for descriptions of the transition systems and model parameterizations.
17	15	Likelihood training without exploration maximizes Eq.
19	46	Dynamic oracle methods are known to improve on this training procedure for a variety of parsers (Coavoux and Crabbé, 2016; Cross and Huang, 2016; Stern et al., 2017; González and Gómez-Rodrı́guez, 2018), supervising exploration during training by providing the parser with the best action to take at each explored state.
20	10	We describe how policy gradient can be applied as an oracle-free alternative.
21	64	We then compare to several variants of dynamic oracle training which focus on addressing exposure bias, loss mismatch, or both.
22	34	Given an arbitrary cost function ∆ comparing structured outputs (e.g. negative labeled F1, for trees), we use the risk objective: R(θ) = N∑ i=1 ∑ y p(y | x(i); θ)∆(y,y(i)) which measures the model’s expected cost over possible outputs y for each of the training examples (x(1),y(1)), .
23	35	Minimizing a risk objective has a long history in structured prediction (Povey and Woodland, 2002; Smith and Eisner, 2006; Li and Eisner, 2009; Gimpel and Smith, 2010) but often relies on the cost function decomposing according to the output structure.
25	16	The policy gradient method we apply is a simple variant of REINFORCE (Williams, 1992).
26	52	We perform mini-batch gradient descent on the gradient of the risk objective: ∇R(θ) = N∑ i=1 ∑ y p(y|x(i))∆(y,y(i))∇ log p(y|x(i); θ) ≈ N∑ i=1 ∑ y∈Y(x(i)) ∆(y,y(i))∇ log p(y|x(i); θ) where Y(x(i)) is a set of k candidate trees obtained by sampling from the model’s distribution for sentence x(i).
29	16	Following Shen et al. (2016), we also found better performance when including the gold tree y(i) in the set of k candidates Y(x(i)), and do so for all experiments reported here.2
30	14	For a given parser state st, a dynamic oracle defines an action a∗(st) which should be taken to incrementally produce the best tree still reachable from that state.3 Dynamic oracles provide strong supervision for training with exploration, but require custom design for a given transition system.
31	39	Cross and Huang (2016) and Stern et al. (2017) defined optimal (with respect to F1) dynamic oracles for their respective transition systems, and below we define a novel dynamic oracle for the top-down system of RNNG.
41	22	While we do not claim that this dynamic oracle is optimal with respect to F1, we find that it still helps substantially in supervising exploration (Section 5).
42	42	Likelihood Training with Exploration Past work has differed on how to use dynamic oracles to guide exploration during oracle training (Ballesteros et al., 2016; Cross and Huang, 2016; Stern et al., 2017).
43	126	We use the same sample-based method of generating candidate sets Y as for policy gradient, which allows us to control the dynamic oracle and policy gradient methods to perform an equal amount of exploration.
44	18	Likelihood training with exploration then maximizes the sum of the log probabilities for the oracle actions for all states composing the candidate trees: LE(θ) = N∑ i=1 ∑ y∈Y(x(i)) ∑ s∈y log p(a∗(s) | s) where a∗(s) is the dynamic oracle’s action for state s. Softmax Margin Softmax margin loss (Gimpel and Smith, 2010; Auli and Lopez, 2011) addresses loss mismatch by incorporating task cost into the training loss.
45	17	Since trees are decomposed into a sequence of local action predictions, we cannot use a global cost, such as F1, directly.
46	10	As a proxy, we rely on the dynamic oracles’ action-level supervision.
47	58	In all models we consider, action probabilities (Eq.
