0	50	We examine the task of Natural Sentence Grounding in Video (NSGV).
1	41	Given an untrimmed video and a natural sentence, the goal is to determine the start and end timestamps of the segment in the video which corresponds to the given sentence, as shown in Figure 1 (a).
2	38	Comparing with the other video researches, such as bidirectional video-sentence retrieval (Xu et al., 2015b), video attractiveness prediction (Chen et al., 2018, 2016), and video captioning (Pasunuru and Bansal, 2017; Wang et al., 2018a,b), NSGV needs to model not only the characteristics of sentence and video but also the fine-grained interactions between the two modalities, which is even more challenging.
5	60	Recently, several related works (Gao et al., 2017; Hendricks et al., 2017) leverage one temporal sliding window approach over video sequences to generate video segment candidates, which are then independently combined (Gao et al., 2017) or compared (Hendricks et al., 2017) with the given sentence to make the grounding prediction.
7	12	First, existing methods project the video segment and sentence into one common space, as shown in Figure 1 (b), where the two generated embedding vectors are used to perform the matching between video segment and sentence.
9	19	Second, in order to handle the diverse temporal scales and locations of the candidate segments, exhaustive matching between the large amount of overlapping segments and the sentence is required.
11	24	In order to tackle the above two limitations, we introduce a novel Temporal GroundNet (TGN) model, the first dynamic single-stream deep architecture for the NSGV task that takes full advantage of fine-grained interactions between video frames and words in a sentence, as shown in Figure 1 (c).
32	36	Given a long and untrimmed video sequence V and a natural sentence S, the NSGV task is to localize a video segment Vs = {ft}tet=tb from V , beginning at tb and ending at te, which corresponds to and expresses the same semantic meaning as the given sentence S. In order to perform the grounding, each video is represented as V = {ft}Tt=1, where T is the total number of frames and ft denotes the feature representation of the t-th video frame.
33	14	Similarly, each sentence is represented as S = {wn}Nn=1, where wn is the embedding vector of the n-th word in the sentence andN denotes the total number of words.
36	18	1) Encoder: visual and textual encoders are used to compose the video frame representations and word embeddings, respectively.
38	12	3) Grounder: a grounder generates the temporal localization in one single pass.
44	30	First, the frame-specific sentence feature is generated through summarizing the sentence hidden states by considering their relationships with the specific video frame at each time step.
47	26	Inspired by (Wang and Jiang, 2016a; Feng et al., 2018), we introduce one novel frame-specific sentence feature, which adaptively summarizes the hidden states of the sentence {hsn}Nn=1 with respect to the t-th video frame: Hst = N∑ n=1 αnt h s n, (1) where Hst denotes the summarized sentence representation specified by the t-th video frame.
49	14	The attention weight αnt encodes the degree to which the n-th word in the sentence is aligned with the t-th video frame.
51	16	As such, the generated framespecific sentence features {Hst}Tt=1 consider the frame-by-word relationships between all the video frames and all the words in the sentence.
58	14	In order to well capture the complicated temporal interactions between the video and sentence, at each time step t, the input of the i-LSTM is formed by concatenating the t-th video hidden state hvt and the t-th frame-specific sentence feature Hst as: rt = h v t ‖ Hst .
71	18	In this section, we introduce the grounder, which works on the yielded interaction status hrt from i-LSTM, to localize the video segment that corresponds to the sentence.
73	27	As shown in Figure 2, at each time step t, the grounder efficiently scores a set of K grounding candidates by considering multiple time scales (Buch et al., 2017) that end at time step t. Specifically, we use different K for different datasets, which is determined by the distribution of the lengths of all ground-truth groundings in a certain dataset.
77	14	Specifically, at each time step t, the grounder will classify each temporal candidate in consideration as a positive grounding or a negative one with respect to the given sentence.
78	18	Considering multiple time scales, the grounder will generate the confidence scores Ct = (c1t , c 2 t , ..., c K t ) that correspond to the set of K visual grounding candidates, all ending at time step t. The hidden state hrt generated by i-LSTM at time t, representing the interaction status between the sentence and video sequence up to the current position, is naturally suited to yield the confidence scores for the different time scales ending at time step t. In this paper, the confidence scores, indicating the sentence grounding, are generated by a fullyconnected layer with sigmoid nonlinearity: Ct = σ(W Khrt + b r t ), (5) where WK and brt are the corresponding parameters, and σ denotes the nonlinear sigmoid function.
87	12	Our TGN backpropagates at every time step t to learn all the parameters of the fully-coupled three modules: encoder, interactor, and grounder.
104	19	We compare our proposed TGN against the following two state-of-the-art models, specifically, the MCN (Hendricks et al., 2017), CTRL (Gao et al., 2017), visual-semantic alignment with LSTM (VSA-RNN) (Karpathy and Li, 2015), and visual-semantic alignment with skip thought vector (VSA-STV) (Kiros et al., 2015).
125	29	First, TGN with different features can significantly outperforms the “prior baseline” MFP, which retrieves segments corresponding to the most common start and end points in the dataset.
145	21	Second, with the same visual feature, specifically C3D, TGN-C3D significantly outperforms CTRL-C3D.
178	21	This experiment is designed to verify whether the frameby-word attention mechanism in interactor is useful to highlight the representative concepts in the sentence.
179	13	The attention weights α for two testing samples in DiDeMo are illustrated in Figure 4, where the darker the color is, the larger the attention weight is.
185	17	The efficiency is measured by frames per second (FPS) as shown in Table 5.
186	23	Please not that the feature extraction time is excluded.
188	18	The reason mainly attributes to that the proposed TGN only process each video in one single pass without processing overlapped sliding windows.
190	22	Towards this task, we proposed an end-to-end Temporal GroundNet (TGN) by incorporating the evolving fine-grained frame-by-word interactions across video-sentence modalities to generate a visual grounding tailored to each given natural sentence.
