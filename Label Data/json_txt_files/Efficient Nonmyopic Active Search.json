{"sections": [{"heading": "1. Introduction", "text": "In many real-world applications, the process of analyzing data incurs some cost; for example, obtaining a label may require a laborious experiment, a human action, or depletion of some other expensive resource. In these scenarios, carefully selecting data to label can often help us achieve our goals more efficiently than, e.g., random sampling. This is the motivation behind active learning.\nNaturally, which data examples are the most useful to label might change according to our objective. Traditionally, much of the active learning literature has focused on train-\n1Washington University in St. Louis, St. Louis, MO, USA 2Simpson College, Indianola, IA, USA 3University of South Carolina, Columbia, SC, USA. Correspondence to: Shali Jiang <jiang.s@wustl.edu>.\nProceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).\ning a model to have high generalization performance with few training examples. Here, we consider a special and atypical realization of active learning: the active search problem (Garnett et al., 2012). In active search, we seek to sequentially inspect data so as to discover members of a rare, desired class. The labels are not known a priori but can be revealed by querying a costly labeling oracle. The goal is to design an policy to sequentially query points to find as many valuable points as possible under a labeling budget. Several real-world problems can be naturally posed in terms of active search; drug discovery, fraud detection, and product recommendation are a few examples. A successful active search policy faces the fundamental dilemma between exploration and exploitation; i.e., whether to search for new regions of valuable points (exploration) or take advantage of the currently most-promising regions (exploitation).\nPrevious work developed policies for active search by appealing to Bayesian decision theory (Garnett et al., 2011; 2012). Garnett et al. (2012) derived the optimal policy in this framework with a natural utility function. Not surprisingly, realizing this policy in the general case requires exponential computation. To overcome this intractability, the authors of that work proposed using myopic lookahead policies in practice, which compute the optimal policy only up to a limited number of steps into the future. This defines a family of policies ranging in complexity from completely greedy one-step lookahead to the optimal policy, which looks ahead to the depletion of the entire budget. The authors demonstrated improved performance on active search over the greedy policy even when looking just two steps into the future, including in a drug-discovery setting (Garnett et al., 2015). The main limitation of these strategies is that they completely ignore what can happen beyond the chosen horizon, which for typical problems is necessarily limited to ` \u2264 3, even with aggressive pruning.\nThe contributions of this paper are two-fold. First, we prove that no polynomial time policy for active search can have nontrivial approximation ratio with respect to the optimal policy in terms of expected utility. This extends the result by Garnett et al. (2012) that myopic approximations to the optimal policy cannot approximate the optimal policy. The proof of this theorem is constructive, creating a family of explicitly difficult active search instances and showing that no polynomial time algorithm can perform well compared\nto the optimal (exponential cost) policy on these.\nSecond, we introduce a novel nonmyopic policy for active search that considers not only the potential immediate contribution of each unlabeled point but also its potential impact on the remaining points that could be chosen afterwards. Our policy automatically balances exploitation against exploration consistent with the labeling budget without requiring any parameters controlling this tradeoff. We also develop an effective strategy for pruning unlabeled points by bounding their potential impact on the search problem. We compare our method with several baselines by conducting experiments on numerous real datasets spanning several domains including citation networks, materials science, and drug discovery. Our results thoroughly demonstrate that our policy typically significantly outperforms previously proposed active search approaches."}, {"heading": "2. Active Search and the Optimal Policy", "text": "Suppose we are given a finite domain of elementsX , {xi}. We know that there is a rare subset R \u2282 X , the members of which are considered valuable, but their identities are unknown a priori. We will call the elements of R targets or positive items. Assume that there is an oracle that can determine whether a specified element x \u2208 X is a target, producing the binary output y , 1{x \u2208 R}. The oracle, however, is assumed to be expensive and may only be queried t times. We seek to design a policy to sequentially query elements to maximize the number of targets found.\nWe will express our preference over different sets of observations D , { (xi, yi) } through a simple utility:\nu(D) , \u2211\nyi\u2208D yi, (1)\nwhich simply counts the number of targets in D. Then, the problem is to sequentially construct a set of t observed points D with the goal of maximizing u(D). Throughout this work, we use a subscript to specify a set of observed data after i \u2264 t queries, defining Di , { (xj , yj) }i j=1 ."}, {"heading": "2.1. The Bayesian Optimal Policy", "text": "Following previous work, we consider the active search problem in the standard Bayesian framework. Assume we have a probabilistic classification model that provides the posterior probability of a point x belonging to R, given observed data D: Pr(y = 1 | x,D).\nRecall that we are allowed to perform t labeling queries, and suppose we are at some iteration i for i \u2264 t; having already observed i\u2212 1 examples, Di\u22121. We wish to submit the ith item to the oracle. Bayesian decision theory compels us to select the item that if we evaluate next maximizes the\nexpected utility of the final observed dataset:\nx\u2217i = argmax xi\u2208X\\Di\u22121\nE [ u(Dt) | xi,Di\u22121 ] . (2)\nIn other words, we choose a point x\u2217i maximizing the expected number of targets found at termination. Unfortunately, as we shall see later, computing E [ u(Dt) | xi,Di\u22121 ] is computationally impractical.\nTo better understand the optimal policy, consider the case i = t, so we already have t\u2212 1 observations Dt\u22121 and there is only one more query left. The expected utility is\nE [ u(Dt) | xt,Dt\u22121 ] = \u2211\nyt u(Dt) Pr(yt | xt,Dt\u22121)\n= u(Dt\u22121) + Pr(yt = 1 | xt,Dt\u22121). (3)\nNote u(Dt\u22121) is a constant, since Dt\u22121 was already observed. Thus, when there is one query remaining, the optimal decision is to greedily choose the remaining point with maximum probability of being a target.\nWhen two or more queries are left, the optimal policy is not as trivial. The challenge is that after the first choice, the probability model changes, affecting all future decisions. Below, we show the expected utility for i = t\u2212 1.\nE [ u(Dt) | xt\u22121,Dt\u22122 ] = u(Dt\u22122) +\nPr(yt\u22121 = 1 | xt\u22121,Dt\u22122) + Eyt\u22121 [ max xt Pr(yt = 1 | xt,Dt\u22121) ] . (4)\nThis expression has an intuitive interpretation. First, we have the reward for the data already observed, u(Dt\u22122). The second term is the expected reward contribution from the point xt\u22121 under consideration, Pr(yt\u22121 = 1 | xt\u22121,Dt\u22122). Finally, the last term is the expected future reward, which is the expected reward to be gathered on the next step; from our previous analysis, we know that this will be maximized by a greedy selection (3). These latter two terms can be interpreted as encouraging exploitation and exploration, respectively, with the optimal second-to-last query.\nIn general, we can compute expected utility (2) at time i \u2264 t recursively as (Garnett et al., 2012):\nE [ u(Dt) | xi,Di\u22121 ] = u(Di\u22121) +\nPr(yi = 1 | xi,Di\u22121)\ufe38 \ufe37\ufe37 \ufe38 exploitation, < 1 +\nEyi [ maxx\u2032 E [ u(Dt \\ Di) | x\u2032,Di ]] \ufe38 \ufe37\ufe37 \ufe38\nexploration, <t\u2212i\n. (5)\nIt is easy to show that the time complexity for computing Eq. (5) is O ( (2n)` ) , where ` = t\u2212 i+ 1 is the lookahead and n is the total number of unlabeled points.\nThis exponential running time complexity makes the Bayeisan optimal policy infeasible to compute, even for small-scale applications. A typical workaround is to pretend there are only a few steps left in the search problem at each iteration, and sequentially apply a myopic policy (e.g., (3) or (4)). We will refer to these policies as the one-step and two-step myopic policies, respectively, and more generally to the `-step myopic policy, with ` < t\u2212 i+ 1.\nSince these myopic approaches cannot plan more than ` steps ahead, they can underestimate the potential benefit of exploration. In particular, the potential magnitude of the exploration term in (5) depends linearly on the budget, whereas in an `-step myopic policy, the magnitude of the equivalent term can go no higher than a fixed upper bound of `. In fact, Garnett et al. (2012) showed via an explicit construction that the expected performance of the `-step policy can be arbitrarily worse than any m-step policy with ` < m, exploiting this inability to \u201csee past\u201d the horizon. When following this suggestion, we must thus trade off the potential benefits of nonmyopia and the rapidly increasing computational burden of lookahead when choosing a policy."}, {"heading": "2.2. Hardness of Approximation", "text": "We extend the above hardness result to show that no polynomial-time active search policy can be a (constant factor) approximation algorithm with respect to the optimal policy, in terms of expected utility. In particular, under the assumption that algorithms only have access to a unit cost conditional marginal probability Pr(y = 1 | x,D) for any x and D, where |D| is less than the budget,1 then:\nTheorem 1. There is no polynomial-time active search policy with a constant factor approximation ratio for optimizing the expected utility.\nWe prove this theorem in the appendix. The main idea is to construct a class of instances where a small \u201csecret\u201d set of elements encodes the locations of a large \u201ctreasure\u201d of targets. The probability of revealing the treasure is vanishingly small without discovering the secret set; however, it is extremely unlikely to observe any information about this secret set with polynomial-time effort.\nDespite the negative result of Theorem 1, we may still search for policies that are empirically effective on real problems. In the next section, we propose a novel alternative approximation to the optimal policy (2) that is nonmyopic, computationally efficient, and shows impressive empirical performance.\n1The optimal policy operates under these restrictions."}, {"heading": "3. Efficient Nonmyopic Active Search", "text": "We have seen above how to myopically approximate the Bayesian optimal policy using an `-step-lookahead approximate policy (5). Such an approximation, however, effectively assumes that the search procedure will terminate after the next ` evaluations, which does not reward exploratory behavior that improves performance beyond that horizon. We propose to continue to exactly compute the expected utility to some fixed horizon, but to approximate the remainder of the search differently. We will approximate the expected utility from any remaining portion of the search by assuming that any remaining points, {xi+1, xi+2, . . . , xt}, in our budget will be selected simultaneously in one big batch. One rationale is if we assume that after observing Di, the labels of all remaining unlabeled points are conditionally independent, then this approximation recovers the Bayesian optimal policy exactly. By exploiting linearity of expectation, it is easy to work out the optimal policy for selecting such a simultaneous batch observation: we simply select the points with the highest probability of being valuable. The resulting approximation is\nmax x\u2032\nE [ u(Dt\\Di) | x\u2032,Di ] \u2248 \u2211\u2032\nt\u2212i Pr(y = 1 | x,Di), (6)\nwhere the summation-with-prime symbol \u2211\u2032\nk indicates that we only sum the largest k values.\nOur proposed policy selects points by maximizing the approximate final expected utility using:\nE [ u(Dt) | xi,Di\u22121 ] \u2248 u(Di\u22121) +\nPr(yi = 1 | xi,Di\u22121) + Eyi [\u2211\u2032 t\u2212i Pr ( y = 1 | x,Di )] \ufe38 \ufe37\ufe37 \ufe38\nexploration, <t\u2212i\n. (7)\nWe will call this policy efficient nonmyopic search (ENS). As in the optimal policy, we can interpret (7) naturally as rewarding both exploitation and exploration, where the exploration benefit is judged by a point\u2019s capability to increase the top probabilities among currently unlabeled points. We note further that in (7) the reward for exploration naturally decreases over time as the budget is depleted, exactly as in the optimal policy. In particular, the very last point xt is chosen greedily by maximizing probability, agreeing with the true optimal policy. The second-to-last point is also guaranteed to match the optimal policy.\nNote that we may also use the approximation in (6) as part of a finite-horizon lookahead with ` > 1, producing a family of increasingly expensive but higher-fidelity approximations to the optimal policy, all retaining the same budget consciousness. The approximation in (7) is equivalent to a one-step maximization of (6). We will see in our experiments that this is often enough to show massive gains in performance, and\nthat even this policy shows clear awareness of the remaining budget throughout the search process, automatically and dynamically trading off exploration and exploitation."}, {"heading": "3.1. Nonmyopic Behavior", "text": "To illustrate the nonmyopic behavior of our policy, we have adapted the toy example presented by Garnett et al. (2012). Let I , [0, 1]2 be the unit square. We repeated the following experiment 100 times. We selected 500 points i.i.d. uniformly at random from I to form the input space X . We create an active search problem by defining the set of targets R \u2286 X to be all points within Euclidean distance 1/4 from either the center or any corner of I . We took the closest point to the center (always a target) as an initial training set. We then applied ENS and the two-step-lookahead (4) policies to sequentially select 200 further points for labeling.\nFigure 1 shows a kernel density estimate of the distribution of locations selected by both methods during two time intervals. Figures 1(a\u2013b) correspond to our method; Figures 1(c\u2013d) to two-step lookahead. Figures 1(a, c) consider the distribution of the first 100 selected locations; Figures 1(b, d) consider the last 100. The qualitative difference between these strategies is clear. The myopic policy focused on collecting all targets around the center (Figure 1(c)), whereas our policy explores the boundaries of the center clump with considerable intensity, as well as some of the corners (Figure 1(a)). As a result, our policy is capable of finding some of targets in the corners, whereas two-step lookahead hardly ever can (Figure 1(d)). We can also see that the highest probability mass in Figure 1(b) is the center, which shows that our policy typically saves many high-probability points until the end. On average, the ENS policy found about 40 more targets at termination than the two-step-lookahead policy."}, {"heading": "3.2. Implementation and Time Complexity", "text": "The complexity of our policy (7) isO ( n ( 2(n+n log n) )) = O(n2 log n), for n = |X |, because we need to compute the approximate expected utility for all n points, evaluate an expectation over its label, conditioning the model and sorting the posterior probabilities in the expectation. However, for some classification models Pr(y = 1 | x,D), observing one point will only affect the probabilities on a small portion of the other points (e.g., in a k-nn model). We can exploit such structure to reduce the complexity of our method by avoiding unnecessary computation.\nSpecifically, suppose that after observing a point we only need to update the probabilities of at-most m other points. We can avoid repeatedly sorting the probabilities of every unlabeled point when computing the score of each candidate point. Once the current probabilities are sorted (O(n log n)), we only need to update m probabilities and sort these as well (O(m logm)); now we can merge both\nlists to get the top t \u2212 i posterior probabilities in time O(t\u2212 i), where i is the index of current iteration. In summary, these tricks can reduce the computational complexity to O ( n(log n + m logm + t) ) . We can see the complexity is about the same as two-step lookahead, which is O ( n(log n+m) ) when using the same tricks."}, {"heading": "3.3. Pruning the Search Space", "text": "To further reduce the computational complexity, we can use a similar strategy as suggested by Garnett et al. (2012) to bound the score function (7) and prune points that cannot possibly maximize our score. We consider the same two assumptions proposed by these authors. First, observing a new negative point will not raise the probability of any other point being a target. Second, we are able to bound the maximum probability of the unlabeled points after conditioning on a given number of additional targets; that is, we assume there is a function p\u2217(n,D) such that\np\u2217(n,D) \u2265 max x\u2208X\\D\nPr(y = 1 | x,D\u222aD\u2032, \u2211\ny\u2032\u2208D\u2032y \u2032 \u2264 n).\nThat is, the probability of any unlabeled point can become at most p\u2217(n,D) after further conditioning on n or fewer additional target points.\nConsider an unlabeled point x at time i, and define \u03c0(x) = Pr(y = 1 | x,Di) for the remainder of this discussion. The\nscore (7), denoted f(x) here for simplicity, can be upper bounded by\nf(x) \u2264 \u03c0 \u00b7 ( 1 + (t\u2212 i)p\u2217(1,Di) ) +\n(1\u2212 \u03c0) \u00b7 (\u2211\u2032\nt\u2212i Pr(y \u2032 = 1 | x\u2032,Di)\n) , U(\u03c0).\nNote this upper bound is only a function of the current probability \u03c0. Let x+ be the point with maximum probability. Then f(x+) is certainly a lower bound of maxx f(x). Hence, those points satisfying U ( \u03c0(x) ) < f(x+) can be safely removed from consideration. Solving this inequality, we have\n\u03c0(x) < f(x+)\u2212\n\u2211\u2032 t\u2212i Pr(y\n\u2032 = 1 | x\u2032,D) 1 + p\u2217(1,D)(t\u2212 i)\u2212 \u2211\u2032 t\u2212i Pr(y \u2032 = 1 | x\u2032,D) .\n(8) Then, all points with current probability lower than the RHS of (8) can be removed from consideration. We will show empirically that a large fraction of points can often be pruned on massive datasets."}, {"heading": "4. Related Work", "text": "Our method falls into the broader framework of active learning. The particular setting of finding elements of a valuable class is rather unusual in active learning, which typically considers the goal of training a high-fidelity model (Lewis & Gale, 1994). For an exhaustive introduction to active learning, we refer the reader to Settles (2010).\nThe multi-armed bandit (MAB) problem shares some similarities with active search, where selecting an item can understood as \u201cpulling an arm.\u201d However, in active search the items are correlated, and, critically, they can never be played twice. Despite the difference, we note that our ENS policy is somewhat similar to the knowledge gradient policy introduced by Frazier et al. (2008).\nActive search can be seen as a special case of Bayesian optimization (Brochu et al., 2010; Snoek et al., 2012) with binary observations and cumulative reward. Several nonmyopic policies have been proposed for Bayesian optimization in the regression setting (e.g., Ling et al. (2016)), and our method is spiritually similar to the recently propopsed GLASSES algorithm (Gonz\u00e1lez et al., 2016).\nVanchinathan et al. (2015) proposed a method called GPSELECT to solve a class of problems the authors call \u201cadaptive valuable item discovery,\u201d which generalizes active search to the regression setting. GP-SELECT employs a Gaussian process regression model in a manner inspired by the Gaussian process upper confidence bound (GP-UCB) algorithm (Srinivas et al., 2010). A parameter must be specified to balance exploration and exploitation, whereas our method automatically and dynamically trades off these quantities. The method is also critically tied to Gaussian process\nregression as the underlying model, which is inappropriate for classification. Our decision-theoretic approach does not make any assumptions about the classification model.\nActive search can also be seen as a special case of (partially observable) Markov decision processes ((PO)MDPs), for which there are known hardness results. Sabbadin et al. (2007), for example, defined the class of so-called \u201cpurely epistemic\u201d MDPs (EMDPs), where the state does not evolve over time. The authors showed that the optimal policy for these problems cannot admit polynomial-time constant approximations. Unfortunately, these hardness results, for the very rich class of EMDPs are not trivially transferred to the more-specific active search problem.\nOur proposed approximation is similar in nature to the active search policy proposed by Wang et al. (2013), which only considered the effect of raising probabilities after observing a positive label, and did not consider the budget. Rather, the proposed score always encourages maximal exploration, in opposition to the optimal policy.\nThere has been some attention to active search in the graph setting where the input domain X is the nodes of a graph (Garnett et al., 2011; Wang et al., 2013; Pfeiffer III et al., 2014; Ma et al., 2015a). Our method does not restrict the input space. Further, the classification models used in these settings are often difficult to scale to large datasets, e.g., requiring the pseudoinverse of the graph Laplacian.\nFinally, variations on the active search problem have also been considered. Ma et al. (2014) proposed the active area search problem, wherein a continuous function is sampled to discover regions with large mean value, and Ma et al. (2015b) extended this idea to define the more-general active pointillistic pattern search problem. These settings do not allow querying for labels directly and offer no insight to the core active search problem."}, {"heading": "5. Experiments", "text": "We implemented our approximation to the Bayesian optimal policy with the MATLAB active learning toolbox,2 and have compared the performance of our proposed ENS policy with several baselines. First we compare with the myopic onestep (greedy) and two-step approximations to the Bayesian optimal policy, presented in (3\u20134). Note that Garnett et al. (2012) and Garnett et al. (2015) thoroughly compared the one- and two-step policies, with the finding that the lessmyopic two-step algorithm usually performs better in terms of targets found, as one would expect. In our experiments we will mainly focus on comparing our algorithm with myopic two-step approximate policy.\nWe also consider a simple baseline which we call RANDOM-\n2https://github.com/rmgarnett/active_learning\nGREEDY (RG). Here we randomly select points to query (exploration) during the first half of the budget, and select the remainder using greedy selection (exploitation). Although na\u00efve, this policy adapts to the budget.\nWe further compare with the score function proposed by Wang et al. (2013), which we refer to as IMS:\nIMS(x) = Pr(y = 1 | x,D) ( 1 + \u03b1 IM(x) ) ; (9)\nwhere IM(x) measures the \u201cexpected impact\u201d, the sum of the raised probabilities x results in if it is positive. Note that it is difficult to determine the tradeoff parameter \u03b1 without (expensive) cross validation. The empirical results in (Wang et al., 2013) indicate that \u03b1 = 10\u22124 performs well on average; we will fix this value in our experiments.\nFinally, we have also considered the following UCB-style (Auer, 2002) score function: \u03b1(x,D) = \u03c0 + \u03b3 \u221a \u03c0(1\u2212 \u03c0), where \u03c0 = Pr(y = 1 | x,D) and \u03b3 is a tradeoff parameter. The UCB score function is very popular and is the essence of the methods in (Vanchinathan et al., 2015; Srinivas et al., 2010) developed for Gaussian processes, including GP-SELECT. We considered various \u03b3 values and our experiments show that it is no better than two-step lookahead, so we present these results in the appendix due to space.\nThe probability model Pr(y = 1 | x,D) we will adopt is the k-nearest-neighbor (k-NN) classifier as described in Section 7 of (Garnett et al., 2012). This model, while being rather simple, shows reasonable generalization error, is nonparameteric, and can be rapidly updated given new training data, an important property in the active setting we consider here. We will also adopt the probability bound (8) for this model described in that work. Note IMS was proposed together (but orthogonally) with a graph model for the probability, which is computationally infeasible (O(n3)) for our datasets. So we also use k-NN model for IMS.\n5.1. CiteSeerx Data\nFor our first real data experiment, we consider a subset of the CiteSeerx citation network, first described in (Garnett et al., 2012). This dataset comprises 39 788 computer science papers published in the top-50 most-popular computer science venues. We form an undirected citation network from these papers. The target class is papers published in the NIPS proceedings; there are 2 190 such papers, 5.5% of the whole dataset. Note that distinguishing NIPS papers in the citation network is not an easy task, because many other highly related venues such as ICML, AAAI, IJCAI, etc. are also among the most-popular venues. A feature vector for each paper is computed by performing graph principal component analysis (Fouss et al., 2007) on the citation network and retaining the first 20 principal components.\nWe select a single target (i.e., a NIPS paper) uniformly at\nrandom to form an initial training set. The budget is set to t = 500, and we use k = 50 in the k-NN model. These parameters match the choices in (Garnett et al., 2012). We use each policy to sequentially select t papers for labeling. The experiment was repeated 20 times, varying the initial seed target. Figure 2 shows the average number of targets found for each method as a function of the number of queries. We first observe that the ranking of the performance is ENS, two-step, IMS, one-step, and RG, and our policy outperforms the two-step policy in this task by a large margin. The mean difference in number of targets found at termination vs. twostep is 34.6 (189 vs. 155), an improvement on average of 22%. A two-sided paired t-test testing the hypothesis that the average difference of targets found is zero returns a pvalue of p < 10\u22124, and a 95% confidence interval on the increase in number of targets found of [19.80, 49.30].\nAnother interesting observation is that during the initial\u223c80 queries, ENS actually performs worse on average than all baseline policies except RG, after which it quickly outperforms them. This feature perfectly illustrates an automatic exploration\u2013exploitation transition made by our policy. As we are always cognizant of our budget, we spend the initial stage thoroughly exploring the domain, without immediate reward. Once complete, we exploit what we learned for the remainder of the budget. This tradeoff happens automatically and without any need for an explicit two-stage approach or arbitrary tuning parameters.\nVarying the Budget. A distinguishing feature of our method is that it always takes the remaining budget into consideration when selecting a point, so we would expect different behavior with different budgets. We repeated the above experiment for budgets t \u2208 {100, 300, 500, 700, 900}, and report in Table 1 the average number of targets found at these time points for each method. We have the following observations from the table. First, ENS performs better than\nall other baseline policies for every budget. Second, ENS is able to adapt to the specified budget. For example, when comparing performance after 100 queries, ENS\u2013100 has located many more targets than the ENS methods with greater budgets, which at that time are still strongly rewarding exploration. A similar pattern holds when comparing other pairs of ENS variations, with one minor exception."}, {"heading": "5.2. Finding Bulk Metallic Glasses", "text": "Our next dataset considers an application from materials science: discovering novel alloys forming bulk metallic glasses (BMGs). BMGs have numerous desirable properties, including high toughness and good wear resistance compared to crystalline alloys. We compiled a database of 118 678 known alloys from the materials literature (e.g., (Kawazoe et al., 1997; all)), an extension of the dataset from (Ward et al., 2016). Of these, 4 746 (\u223c4%) are known to exhibit glass-forming ability, which we define to be targets. We conduct the same experiments described for the CiteSeerx data above and show the results in Table 1. We can see the results again demonstrate our policy\u2019s superior performance over all other methods, and its ability of adapting to the remaining budget."}, {"heading": "5.3. Virtual Drug Screening Data", "text": "We further conduct experiments on a massive database of chemoinformatic data. The basic setting is to screen a large database of compounds searching for those that show binding activity against some biological target. This is a basic component of drug-discovery pipelines. The dataset comprises 120 activity classes of human biological importance selected from the Binding DB (Liu et al., 2007) database. For each activity class, there are a small number of compounds with significant binding activity; the number of targets varies\nfrom 200 to 1 488 across the activity classes. From these we define 120 different active search problems. There are also 100 000 presumed inactive compounds selected at random from the ZINC database (Sterling & Irwin, 2015); these are used as a shared negative class for each of these problems. For each compound, we consider two different feature representations, also known as chemoinformatic fingerprints, called ECFP4 and GpiDAPH3. These fingerprints are binary vectors encoding the relevant chemical characteristics of the compounds; see (Garnett et al., 2015) for more details.3 So in total we have 240 active search problems, each with more than 100 000 points, and with targets less than 1.5%.\nAs is standard in this setting, we compute fingerprint similarities via the Jaccard index (Jasial et al., 2016), which are used to define the weight matrix of the k-NN model from above, setting k = 100 for all the experiments. For active search policies, we again randomly select one positive as the initial training set, and sequentially query t = 500 further points. We also report the performance of a baseline where we randomly sample a stratified sample of size 5% of the database (\u223c5 000 points, more than 10 times the budget of the active search policies). From this sample, we train the same k-NN model, compute the active probability of the remaining points, and query the 500 points with the highest posterior activity probability. All experiments were repeated 20 times, varying the initial training point. Note we did not test IMS on these data due to computational expense. Our policy nominally has higher time complexity, but our pruning strategy can reduce the computation significantly in practice, as we show in Section 5.4.\nTable 2 summarizes the results. First we notice that all 3We did not conduct experiments on the MACCS fingerprint. It was inferior in the findings of Garnett et al. (2015). A reviewer of (Jasial et al., 2016) noted that it is no longer used, due to clear underperformance compared to, e.g., ECFP4 and GpiDAPH3.\nTable 2: Number of active compounds found by various active search policies at termination for each fingerprint, averaged over 120 active classes and 20 experiments. Also shown is the difference of performance between ENS and two-step lookahead and the results of the corresponding paired t-test.\npolicy t-test results\nfingerprint 100-NN RG one-step two-step ENS difference p-value 95% CI\nECFP4 189 189 289 297 303 5.29 1.76\u00d7 10\u22123 2.01 8.56 GpiDAPH3 134 170 255 261 276 14.8 3.90\u00d7 10\u221213 11.2 18.4\nactive search policies perform much better than the recall of a simple classification algorithm, even though they observe less than one-tenth the data. Interestingly, even the na\u00efve random-greedy (RG) policy performs much better than this baseline, albeit much worse than other active search policies. The two-step policy is again better than the greedy policy for both fingerprints, which is consistent with the results reported in (Garnett et al., 2015). The ENS policy performs significantly better than two-step lookahead; a two-sided paired t-test overwhelmingly rejects the hypothesis that the performance at termination is equal in both cases.\nFigure 3 shows the mean difference in cumulative targets found between ENS and the two-step policy for the ECFP4 fingerprint. Again, we very clearly observe the automatic trade-off between exploration and exploitation by our method. In the initial stage of the search, we explore the space without much initial reward, but around query 200, our algorithm switches automatically to exploitation, outperforming the myopic policy significantly at termination. The mean difference curves for the other fingerprint is similar, and can be found in the appendix, along with the individual learning curves of the first six activity classes of ECFP4."}, {"heading": "5.4. Effect of Pruning", "text": "To investigate how pruning can improve the efficiency of computing the policy, we computed the average number of pruned points across all 120 \u00d7 20 \u00d7 500 = 3 000 000 iterations of active search, for each fingerprint. On average about 93% of the unlabeled points are pruned, dramatically improving the computational efficiency by approximately a corresponding linear factor. The time for each experiment was effectively reduced from on the order of one day to that of one hour. See the appendix for detailed results."}, {"heading": "6. Conclusion", "text": "In this paper we proved the theoretical hardness of active search and proposed an well-motivated and empirically better-performing policy for solving this problem. In particular, we proved that no polynomial-time algorithm can approximate the expected utility of the optimal policy within a constant approximation ratio. We then proposed a novel method, efficient nonmyopic search (ENS), for the active search problem. Our method approximates the Bayesian optimal policy by computing, conditioned on the location of the next point, how many targets are expected at termination, if the remaining budget is spent simultaneously. By taking the remaining budget into consideration in each step, we are able to automatically balance exploration and exploitation. Despite being nonmyopic, ENS is efficient to compute because future steps are flattened into a single batch, in contrast to the recursive simulation required when computing the true expected utility. We also derived an effective pruning strategy that can reduce the number of candidate points we must consider at each step, which can further improve the efficiency dramatically in practice. We conducted a massive empirical evaluation that clearly demonstrated superior overall performance on various domains, as well as our automatic balance between exploration and exploitation.\nGiven the hardness result we proved, in general there is little point to require more of an algorithm than superior empirical performance. However, one exciting future direction is to understand, under what conditions (e.g., some assumption about the structure of problem instances) we can find efficient algorithms with guarantees."}, {"heading": "Acknowledgments", "text": "We would like to thank Brendan Juba for insightful discussion. SJ, GM, and RG were supported by the National Science Foundation (NSF) under award number IIA\u20131355406. GM was also supported by the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES). GC and AS were supported by NSF under award number CNS\u20131560191. BM was supported by a Google Research Award, a Yahoo Research Award, and by NSF under award number CCF\u20131617724."}], "year": 2017, "references": [{"title": "Using Confidence Bounds for Exploitation\u2013 Exploration Trade-offs", "authors": ["Auer", "Peter"], "venue": "Journal of Machine Learning Research,", "year": 2002}, {"title": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning", "authors": ["Brochu", "Eric", "Cora", "Vlad M", "de Freitas", "Nando"], "year": 2010}, {"title": "Random-Walk Computation of Similarities between Nodes of a Graph with Application to Collaborative Recommendation", "authors": ["Fouss", "Francois", "Pirotte", "Alain", "Renders", "Jean-Michel", "Saerens", "Marco"], "venue": "IEEE Transactions on Knowledge and Data Engineering,", "year": 2007}, {"title": "A Knowledge-Gradient Policy for Sequential Information Collection", "authors": ["Frazier", "Peter I", "Poweel", "Warren B", "Dayanik", "Savas"], "venue": "SIAM Journal on Control and Optimization,", "year": 2008}, {"title": "Bayesian Optimal Active Search on Graphs", "authors": ["Garnett", "Roman", "Krishnamurthy", "Yamuna", "Wang", "Donghan", "Schneider", "Jeff", "Mann", "Richard"], "venue": "In 9th Workshop on Mining and Learning with Graphs,", "year": 2011}, {"title": "Bayesian Optimal Active Search and Surveying", "authors": ["Garnett", "Roman", "Krishnamurthy", "Yamuna", "Xiong", "Xuehan", "Schneider", "Jeff G", "Mann", "Richard P"], "venue": "In Proceedings of the 29th International Conference on Machine Learning,", "year": 2012}, {"title": "Introducing the \u2018active search\u2019 method for iterative virtual screening", "authors": ["Garnett", "Roman", "G\u00e4rtner", "Thomas", "Vogt", "Martin", "Bajorath", "J\u00fcrgen"], "venue": "Journal of Computer-Aided Molecular Design,", "year": 2015}, {"title": "Activity-relevant similarity values for fingerprints and implications for similarity searching", "authors": ["Jasial", "Swarit", "Hu", "Ye", "Vogt", "Martin", "Bajorath", "J\u00fcrgen"], "venue": "F1000Research, 5(Chem Inf", "year": 2016}, {"title": "Nonequilibrium Phase Diagrams of Ternary Amorphous Alloys, volume 37A of Condensed Matter", "authors": ["Kawazoe", "Yoshiyuki", "Yu", "Jing-Zhi", "Tsai", "An-Pang", "Masumoto", "Tsuyoshi (eds"], "year": 1997}, {"title": "A Sequential Algorithm for Training Text Classifiers", "authors": ["Lewis", "David D", "Gale", "William A"], "venue": "In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval,", "year": 1994}, {"title": "BindingDB: A web-accessible database of experimentally determined protein\u2013ligand binding affinities", "authors": ["Liu", "Tiqing", "Lin", "Yuhmei", "Wen", "Xin", "Jorissen", "Robert N", "Gilson", "Michael K"], "venue": "Nucleic Acids Research,", "year": 2007}, {"title": "Active Area Search via Bayesian Quadrature", "authors": ["Ma", "Yifei", "Garnett", "Roman", "Schneider", "Jeff"], "venue": "In Proceedings of the 17th International Conference on Artificial Intelligence and Statistics,", "year": 2014}, {"title": "Active Search and Bandits on Graphs using Sigma-Optimality", "authors": ["Ma", "Yifei", "Huang", "Tzu-Kuo", "Schneider", "Jeff"], "venue": "In Proceedings of the 31st Conference on Uncertainty in Artificial Intelligence,", "year": 2015}, {"title": "Active Pointillistic Pattern Search", "authors": ["Ma", "Yifei", "Sutherland", "Dougal J", "Garnett", "Roman", "Schneider", "Jeff"], "venue": "In Proceedings of the 18th International Conference on Artificial Intelligence and Statistics,", "year": 2015}, {"title": "Active Exploration in Networks: Using Probabilistic Relationships for Learning and Inference", "authors": ["Pfeiffer III", "Joseph J", "Neville", "Jennifer", "Bennett", "Paul N"], "venue": "In Proceedings of the 23rd ACM International Conference on Information and Knowledge Management,", "year": 2014}, {"title": "Purely Epistemic Markov Decision Processes", "authors": ["Sabbadin", "R\u00e9gis", "Lang", "J\u00e9r\u00f4me", "Ravoanjanahary", "Nasolo"], "venue": "In Proceedings of the 22nd AAAI Conference on Artificial Intelligence,", "year": 2007}, {"title": "Active Learning Literature Survey", "authors": ["Settles", "Burr"], "venue": "Technical report, University of Wisconsin\u2013Madison,", "year": 2010}, {"title": "Practical Bayesian Optimization of Machine Learning Algorithms", "authors": ["Snoek", "Jasper", "Larochelle", "Hugo", "Adams", "Ryan P"], "venue": "In Advances in Neural Information Processing Systems", "year": 2012}, {"title": "Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design", "authors": ["Srinivas", "Niranjan", "Krause", "Andreas", "Kakade", "Sham", "Seeger", "Matthias W"], "venue": "In Proceedings of the 27th International Conference on Machine Learning,", "year": 2010}, {"title": "ZINC 15 \u2013 Ligand Discovery for Everyone", "authors": ["Sterling", "Teague", "Irwin", "John J"], "venue": "Journal of Chemical Information and Modeling,", "year": 2015}, {"title": "Active Search on Graphs", "authors": ["Wang", "Xuezhi", "Garnett", "Roman", "Schneider", "Jeff"], "venue": "In Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining,", "year": 2013}, {"title": "A General-Purpose Machine Learning Framework for Predicting Properties of Inorganic Materials", "authors": ["Ward", "Logan", "Agrawal", "Ankit", "Choudhary", "Alok", "Wolverton", "Christopher"], "year": 2016}], "id": "SP:e72207254282130820f9e8f9e68935749495b32b", "authors": [{"name": "Shali Jiang", "affiliations": []}, {"name": "Gustavo Malkomes", "affiliations": []}, {"name": "Geoff Converse", "affiliations": []}, {"name": "Alyssa Shofner", "affiliations": []}, {"name": "Benjamin Moseley", "affiliations": []}, {"name": "Roman Garnett", "affiliations": []}], "abstractText": "Active search is an active learning setting with the goal of identifying as many members of a given class as possible under a labeling budget. In this work, we first establish a theoretical hardness of active search, proving that no polynomial-time policy can achieve a constant factor approximation ratio with respect to the expected utility of the optimal policy. We also propose a novel, computationally efficient active search policy achieving exceptional performance on several real-world tasks. Our policy is nonmyopic, always considering the entire remaining search budget. It also automatically and dynamically balances exploration and exploitation consistent with the remaining budget, without relying on a parameter to control this tradeoff. We conduct experiments on diverse datasets from several domains: drug discovery, materials science, and a citation network. Our efficient nonmyopic policy recovers significantly more valuable points with the same budget than several alternatives from the literature, including myopic approximations to the optimal policy.", "title": "Efficient Nonmyopic Active Search"}