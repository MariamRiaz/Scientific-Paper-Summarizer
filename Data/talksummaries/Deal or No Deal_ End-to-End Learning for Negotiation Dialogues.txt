0	16	Intelligent agents often need to cooperate with others who have different goals, and typically use natural language to agree on decisions.
1	23	Negotiation is simultaneously a linguistic and a reasoning problem, in which an intent must be formulated and then verbally realised.
6	27	To study semi-cooperative dialogue, we gather a dataset of 5808 dialogues between humans on a negotiation task.
10	36	We therefore explore two methods for improving the model’s strategic reasoning skills— both of which attempt to optimise for the agent’s goals, rather than simply imitating humans: Firstly, instead of training to optimise likelihood, we show that our agents can be considerably improved using self play, in which pre-trained models practice negotiating with each other in order to optimise performance.
26	26	When one agent selects that an agreement has been made, both agents independently output what they think the agreed decision was.
29	41	Two agents are both shown the same collection of items, and instructed to divide them so that each item assigned to one agent.
32	24	These constraints enforce that it is not possible for both agents to receive a maximum score, and that no item is worthless to both agents, so the negotiation will be competitive.
33	53	After 10 turns, we allow agents the option to complete the negotiation with no agreement, which is worth 0 points to both users.
35	29	We collected a set of human-human dialogues using Amazon Mechanical Turk.
42	125	We propose a simple but effective baseline model for the conversational agent, in which a sequenceto-sequence model is trained to produce the complete dialogue, conditioned on an agent’s input.
47	20	Dialogue x is a list of tokens x0..T containing the turns of each agent interleaved with symbols marking whether a turn was written by the agent or their partner, terminating in a special token indicating one agent has marked that an agreement has been made.
60	16	h −→o t = GRU−→o (h −→o t−1, [Ext, ht]) (3) h ←−o t = GRU←−o (h ←−o t+1, [Ext, ht]) (4) hot = [h ←−o t , h −→o t ] (5) hat = W a[tanh(W hhot )] (6) αt = exp(w · hat )∑ t′ exp(w · hat′) (7) hs = tanh(W s[hg, ∑ t αtht]) (8) The output tokens are predicted using softmax: pθ(oi|x0..t, g) ∝ exp(W oihs) (9) The model is trained to minimize the negative log likelihood of the token sequence x0..T conditioned on the input goals g, and of the outputs o conditioned on x and g. The two terms are weighted with a hyperparameter α. L(θ) =− ∑ x,g ∑ t log pθ(xt|x0..t−1, g)︸ ︷︷ ︸ Token prediction loss − α ∑ x,g,o ∑ j log pθ(oj |x0..T , g)︸ ︷︷ ︸ Output choice prediction loss (10) Unlike the Neural Conversational Model (Vinyals and Le, 2015), our approach shares all parameters for reading and generating tokens.
65	38	The space of solutions is small enough to be tractably enumerated.
70	56	While the other agent B could be a human, in our experiments we used our fixed supervised model that was trained to imitate humans.
71	68	The second model is fixed as we found that updating the parameters of both agents led to divergence from human language.
72	39	In effect, agentA learns to improve by simulating conversations with the help of a surrogate forward model.
79	17	Let rA be the score agent A achieved in the completed dialogue, T be the length of the dialogue, γ be a discount factor that rewards actions at the end of the dialogue more strongly, and µ be a running average of completed dialogue rewards so far2.
82	22	For instance, an agent may be choosing between accepting an offer, or making a counter offer.
98	19	Rollout to end of dialogue 13: k ← k + 1 14: xk ∼ pθ(xk|x0..k−1, g) .
117	73	We compare the performance of the following: LIKELIHOOD uses supervised training and decoding (§3), RL is fine-tuned with goal-based selfplay (§4), ROLLOUTS uses supervised training combined with goal-based decoding using rollouts (§5), and RL+ROLLOUTS uses rollouts with a base model trained with reinforcement learning.
125	15	Agreement: The percentage of dialogues where both agents agreed on the same decision.
128	143	Firstly, we see that the RL and ROLLOUTS models achieve significantly better results when negotiating with the LIKELIHOOD model, particularly the RL+ROLLOUTS model.
145	14	Figure 5 shows an example of our goal-based model stubbornly negotiating until it achieves a good outcome.
150	70	One interesting question is whether our models are capable of generating novel sentences in the new circumstances they find themselves in, or if they simply repeat messages from the training data verbatim.
156	424	One common linguistic error we see RL+ROLLOUTS make is to start a message by indicating agreement (e.g. I agree or Deal), but then going on to propose a counter offer—a behaviour that human partners found frustrating.
157	85	One explanation is that the model has learnt that in the supervised data, messages beginning with I agree are often at the end of the dialogue, and partners rarely reply with further negotiation—so the models using rollouts and reinforcement learning believe this tactic will help their offer to be accepted.
183	29	We gathered a large dataset of human-human negotiations, which contain a variety of interesting tactics.
184	43	We have shown that it is possible to train dialogue agents end-to-end, but that their ability can be much improved by training and decoding to maximise their goals, rather than likelihood.
185	92	There remains much potential for future work, particularly in exploring other reasoning strategies, and in improving the diversity of utterances without diverging from human language.
186	38	We will also explore other negotiation tasks, to investigate whether models can learn to share negotiation strategies across domains.
